1
00:00:00,790 --> 00:00:02,260
Welcome back everyone.

2
00:00:03,170 --> 00:00:07,740
We are still in the building blocks of deep learning.

3
00:00:07,740 --> 00:00:15,120
So one of the key concept of new our network and how it learns.

4
00:00:15,140 --> 00:00:23,240
Let me just come back here under the optimizer is that uses what's called gradient base optimization.

5
00:00:23,600 --> 00:00:24,220
And.

6
00:00:24,830 --> 00:00:33,440
And another word for gradient really is actually slope for every layer deep learning neural network

7
00:00:33,500 --> 00:00:38,960
layer every layer ends with what's core and next by evasion function.

8
00:00:38,960 --> 00:00:43,910
Now this activation function comes in different a few times.

9
00:00:43,910 --> 00:00:45,320
You have the sigmoid.

10
00:00:45,320 --> 00:00:47,840
You have solved Max and you have really.

11
00:00:47,960 --> 00:00:50,440
Again I'm not explaining this.

12
00:00:50,990 --> 00:00:51,590
Sorry.

13
00:00:51,600 --> 00:00:52,890
Yeah yeah.

14
00:00:53,120 --> 00:00:54,590
I'm not explaining this for now.

15
00:00:54,590 --> 00:00:57,460
I will cover that and separate lesson.

16
00:00:57,590 --> 00:01:05,960
I just want to highlight that the the actual activation function is to actually squash the data so that

17
00:01:05,960 --> 00:01:09,220
it actually provide a specific output.

18
00:01:09,300 --> 00:01:10,230
Right.

19
00:01:10,520 --> 00:01:15,170
Let's walk through these one by one where W is the actual weights.

20
00:01:15,170 --> 00:01:16,580
We've seen that before.

21
00:01:16,580 --> 00:01:22,090
There's the weights for every note of the layer.

22
00:01:22,130 --> 00:01:24,170
B is the buyer's term.

23
00:01:24,170 --> 00:01:31,280
Now if you think of linear regression or why is gram x plus C the B here is actually the bias term which

24
00:01:31,280 --> 00:01:33,600
is to intercept.

25
00:01:34,160 --> 00:01:42,050
And then the initial weights when you actually training your neural network initially the weights are

26
00:01:42,440 --> 00:01:43,880
set randomly.

27
00:01:43,880 --> 00:01:50,390
Now of course that's not the best place to start but for now we will take that and add enough to that

28
00:01:51,410 --> 00:01:53,360
of course when you actually said a random.

29
00:01:53,360 --> 00:02:02,300
It's not going to have a really accurate prediction or a very accurate neural network that is able to

30
00:02:02,540 --> 00:02:07,910
predict everything accurately so the training part has to come in.

31
00:02:08,140 --> 00:02:18,470
Now we didn't need to make gradual not gradual graduate as gradual adjustment and it just gradual

32
00:02:21,580 --> 00:02:24,160
we need to make gradual adjustment.

33
00:02:26,270 --> 00:02:31,780
Or rather we need to gradually adjust and update to do the weight to reduce the error.

34
00:02:32,160 --> 00:02:33,780
So which is this portion here.

35
00:02:33,780 --> 00:02:39,360
So we need to actually continual adjust this so that the lost function gets smaller and smaller.

36
00:02:39,360 --> 00:02:42,640
Our prediction get closer and closer to Y.

37
00:02:42,960 --> 00:02:47,690
Here's an example of our definition of neural network.

38
00:02:47,730 --> 00:02:55,020
How you define your network in carriers whereby you have the input from models to sequential and also

39
00:02:55,020 --> 00:02:58,050
four layers we import dense and activation.

40
00:02:58,320 --> 00:03:06,210
So we have two dense layer here with 32 neurons and 10 neurons an input should be specified in the activation

41
00:03:06,210 --> 00:03:09,820
function is value here and self makes here.

42
00:03:09,840 --> 00:03:13,240
So these are somewhat the basic concepts.

43
00:03:13,260 --> 00:03:21,900
I will cover this grid in base optimization in a future lesson I'd want to actually touch on this for

44
00:03:21,900 --> 00:03:25,980
now and so that you are aware that this exist.

45
00:03:25,980 --> 00:03:35,070
That's the first thing and and you need to understand that this is the actual so core layout the other

46
00:03:35,070 --> 00:03:36,180
way to look at this.

47
00:03:36,180 --> 00:03:43,050
Let me just touch on this for those who are a little bit more mathematically inclined you can think

48
00:03:43,050 --> 00:03:51,120
of this portion here the inside part here the DOT w multiplied by input plus b you can actually write

49
00:03:51,120 --> 00:03:55,010
it as W x plus B.

50
00:03:55,050 --> 00:04:01,860
This is going to look very similar two ways to emacs plus see where this is the intercept him also call

51
00:04:01,860 --> 00:04:05,600
the bias term X is our input data M is our slope.

52
00:04:05,600 --> 00:04:09,950
You'll notice that they actually look very similar except that this is in the vector Rice format.

53
00:04:10,080 --> 00:04:17,430
This is usually not in their vector Rice format and of course with this it comes out in the terms of

54
00:04:17,730 --> 00:04:25,890
some sort of value and we have a sigmoid function that's being applied to F X after the value has been

55
00:04:25,890 --> 00:04:27,510
calculated.

56
00:04:27,510 --> 00:04:28,400
Okay.

57
00:04:28,540 --> 00:04:33,000
And will refrain from explaining what a signal more is or cover that in future lesson.

58
00:04:33,000 --> 00:04:44,310
But for now I just understand that through this formula okay is how the whole neural network gets trained.

59
00:04:44,840 --> 00:04:56,010
Okay book cover artist and let me walk through the training loops so that you can understand at least

60
00:04:56,040 --> 00:05:01,970
conceptually how it's done before we actually implement the actual training itself.

61
00:05:02,110 --> 00:05:04,760
Right you draw a batch of training samples.

62
00:05:05,400 --> 00:05:13,140
X and the corresponding targets y and you train the neural network on X to obtain prediction of Y recall

63
00:05:13,620 --> 00:05:22,280
you draw a batch of data X and Y corresponding y you train it to get it or obtain a prediction of y.

64
00:05:22,410 --> 00:05:25,550
Okay so we are up to this point.

65
00:05:25,620 --> 00:05:32,790
Step number two then you compute the laws or the errors of the neural network on the bench meaning your

66
00:05:32,790 --> 00:05:34,380
prediction y prediction.

67
00:05:34,380 --> 00:05:37,890
How far is it from the actual y.

68
00:05:37,890 --> 00:05:44,880
Step four then you compute the gradient of the loss with respect to the neural network parameters and

69
00:05:44,880 --> 00:05:47,240
that's what's called a backward pass.

70
00:05:47,370 --> 00:05:51,070
Again if you don't understand is that OK all right.

71
00:05:51,100 --> 00:05:53,030
That's not bad at all.

72
00:05:53,070 --> 00:05:57,880
We will actually cover that in the numerical example so that you can actually cross it a bit better.

73
00:05:58,440 --> 00:06:04,050
So with this this is partial portion here you are actually computing the gradient of the lost meaning

74
00:06:04,050 --> 00:06:11,010
you are calculating the slope with respect to the lost with respect to the neural net neural network

75
00:06:11,020 --> 00:06:15,870
parameters OK meaning the parameters here is X and also y.

76
00:06:16,340 --> 00:06:16,640
Okay.

77
00:06:16,670 --> 00:06:20,580
So that we are freezing X and Y and we actually

78
00:06:22,890 --> 00:06:32,470
changing w okay we're changing W which is the gradient here until we actually get the optimal output.

79
00:06:33,100 --> 00:06:35,130
So let me come back here.

80
00:06:35,200 --> 00:06:36,010
I've removed that.

81
00:06:36,040 --> 00:06:43,990
So let me help you recall what we did to adjust now is this.

82
00:06:44,000 --> 00:06:45,320
I draw this.

83
00:06:45,400 --> 00:06:56,460
Y is good too w x plus C or bias or if X is a W X Y spies.

84
00:06:57,250 --> 00:06:59,620
So let me just change the color a little bit.

85
00:07:05,030 --> 00:07:18,490
Black we are freezing this we are freezing this so what's left is this.

86
00:07:18,550 --> 00:07:22,830
This is our projection and then we have a actual y.

87
00:07:22,840 --> 00:07:30,680
So I can call this religious call this I have an actual effects which are actual value or sometimes

88
00:07:30,700 --> 00:07:38,920
you also call y and I have an effect set of Y head here because this is this is fixed.

89
00:07:38,920 --> 00:07:39,820
This is fix.

90
00:07:39,820 --> 00:07:41,400
These two parameters are fixed.

91
00:07:42,140 --> 00:07:53,730
Okay so what's left now is to actually change w so that our y prediction is closer and closer in value

92
00:07:54,060 --> 00:07:55,200
to Y.

93
00:07:55,200 --> 00:07:59,340
So that's really what this steps number four is.

94
00:07:59,340 --> 00:08:06,480
And finally step 5 is once we actually calculated the gradient we move our parliament is little in the

95
00:08:06,480 --> 00:08:08,550
opposite direction from the gradient.

96
00:08:08,910 --> 00:08:13,650
If it's a positive slope we'll put a minus mark so that it moves closer.

97
00:08:13,650 --> 00:08:19,050
If there's a negative slope we change to sign to positive so that it moves towards the right direction

98
00:08:19,470 --> 00:08:24,660
faster because it will move the parent is little in the opposite direction from the gradient taking

99
00:08:24,660 --> 00:08:28,220
one small step to reduce the loss on the bench.

100
00:08:28,230 --> 00:08:29,160
The step here

101
00:08:31,920 --> 00:08:32,880
where's my pen.

102
00:08:32,960 --> 00:08:33,840
Okay.

103
00:08:33,940 --> 00:08:45,080
The step here has another name core lending rate we've calculated the gradient from earlier so the the

104
00:08:45,080 --> 00:08:53,300
point now is that we actually take put a minus with a small Smalls lending rate in the direction of

105
00:08:53,300 --> 00:08:53,960
the gradient.

106
00:08:54,470 --> 00:08:54,710
Okay.

107
00:08:54,710 --> 00:09:00,350
The minus here meaning opposite that the tip of the direction of the gradient so that we move closer

108
00:09:00,350 --> 00:09:06,170
and closer our y prediction moves closer and closer to Y.

109
00:09:06,560 --> 00:09:09,600
Well I thought conceptually this is the training loop.

110
00:09:09,740 --> 00:09:11,830
This is how neural networks learn.

111
00:09:11,900 --> 00:09:19,060
I know that right now you're not convinced in terms of how this x2 can work out.

112
00:09:19,370 --> 00:09:23,350
For that I need to actually show you an example.

113
00:09:23,360 --> 00:09:29,240
But before we do that let me just explain one more concept which is to stochastic gradient descent and

114
00:09:29,240 --> 00:09:35,690
it is an optimizer which is along the line of but we looked at earlier

115
00:09:38,480 --> 00:09:39,760
it's not here.

116
00:09:39,800 --> 00:09:46,050
Can we have look at our immense prop this is another alternative which is called Stochastic gradient

117
00:09:46,050 --> 00:09:47,340
descent.

118
00:09:47,340 --> 00:09:47,900
Right.

119
00:09:47,940 --> 00:09:50,490
So a couple again some terminologies.

120
00:09:50,490 --> 00:09:54,490
The first one is a study which is the stochastic gradient descent.

121
00:09:54,540 --> 00:10:02,730
With this we actually draw one sample at a time and it is a mini batch stochastic gradient descent.

122
00:10:02,730 --> 00:10:09,270
We draw a batch of data or a sample sample set off the data shouldn't really be sample a sample set

123
00:10:09,270 --> 00:10:10,340
of the data.

124
00:10:10,350 --> 00:10:13,590
The common batch size is 1 2 8 5 1 2.

125
00:10:13,590 --> 00:10:16,890
And also another terminology is called batch as you did.

126
00:10:16,890 --> 00:10:18,280
This is all the data.

127
00:10:18,450 --> 00:10:26,130
So at the one extreme the stochastic gradient basically to run one forward past with one sample and

128
00:10:26,130 --> 00:10:34,920
then update the gradient at the other extreme is it get trained with all the data then it update the

129
00:10:35,550 --> 00:10:36,760
gradient.

130
00:10:36,810 --> 00:10:42,870
All right update degrading and the weight essentially mini batch is somewhere in the middle whereby

131
00:10:42,870 --> 00:10:51,840
it only draw a sample sample set or batch of data which is why the 1 2 or 2 5 6 5 1 2 all whatever you

132
00:10:51,840 --> 00:10:52,680
specify.

133
00:10:53,070 --> 00:10:58,560
So at the one extreme you have every time you have one data and then you actually update the weight

134
00:10:58,620 --> 00:11:01,340
which is computationally very expensive.

135
00:11:01,410 --> 00:11:08,640
The other end is all the data which means the update is quite slow so you have something in between

136
00:11:08,640 --> 00:11:14,020
which is the many big badge as G.D. which is something that's a lot more popular.

137
00:11:14,280 --> 00:11:22,290
There are the more sophisticated techniques to deal with momentum at a grid iron man's prop but will

138
00:11:22,290 --> 00:11:30,090
cover this in the left relevant lectures in the different segment of the advanced neural network models

139
00:11:30,600 --> 00:11:37,800
for now or you know need to know is that these exist and we will cover these in future lessons for now

140
00:11:38,370 --> 00:11:45,390
sufficient to know that we are looking at stochastic gradient descent or bench mini batch of SGI D.

141
00:11:45,920 --> 00:11:47,760
All right with that I hope that has been useful.

142
00:11:47,820 --> 00:11:49,430
Thank you once again for watching.
