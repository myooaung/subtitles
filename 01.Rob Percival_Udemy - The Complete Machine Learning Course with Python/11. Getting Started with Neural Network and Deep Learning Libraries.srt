1
00:00:05,570 --> 00:00:06,670
Welcome back everyone.

2
00:00:06,730 --> 00:00:11,960
In this session we're going to look at getting started with neural network.

3
00:00:11,960 --> 00:00:18,430
For a start I just want to revise some basic principles which is the anatomy of a neural network and

4
00:00:18,440 --> 00:00:24,260
the elements that you will typically see is number one you have an input data which is x here and then

5
00:00:24,260 --> 00:00:32,720
you have a target levels which are labeled as actual and then you have the layers which is this and

6
00:00:32,720 --> 00:00:33,520
also this.

7
00:00:33,530 --> 00:00:35,750
In this case we have two layers.

8
00:00:35,840 --> 00:00:42,830
So while when you combine them they are core network neural network and then you need to measure the

9
00:00:42,830 --> 00:00:50,490
accuracy of your prediction which is here and the actual measurement that we use is called a lost function.

10
00:00:50,600 --> 00:00:58,610
The output of the function calculation is core a last call it informs you how far away is your prediction

11
00:00:58,610 --> 00:01:05,700
from the actual scores and because obviously in the beginning the weights is actually origin just randomized.

12
00:01:05,730 --> 00:01:06,330
No.

13
00:01:06,440 --> 00:01:14,390
We did an optimizer of salt to actually continuously tune our neural network until the weights are accurate

14
00:01:14,450 --> 00:01:22,220
or able to allow our neural network to predict accurately optimizes some example this stochastic gradient

15
00:01:22,220 --> 00:01:22,570
descent.

16
00:01:22,580 --> 00:01:28,340
We discussed a few of last time and the picture below really is an example of what a neural network

17
00:01:28,880 --> 00:01:29,570
may look like.

18
00:01:29,570 --> 00:01:36,080
You have two inputs you have layer 1 layer 2 These are the two hidden layers and then this is the output

19
00:01:36,140 --> 00:01:39,410
a single output layer.

20
00:01:39,660 --> 00:01:40,180
All right.

21
00:01:40,190 --> 00:01:44,560
So it's time to actually have a look at some of the really popular Deep Learning libraries.

22
00:01:44,570 --> 00:01:47,080
I've only highlighted three here.

23
00:01:47,080 --> 00:01:49,730
There are actually a lot more piano.

24
00:01:49,820 --> 00:01:54,110
I guess you can call that the grandfather of Deep Learning libraries.

25
00:01:54,110 --> 00:01:57,510
It's no longer commercially supported but it's still hugely popular.

26
00:01:57,530 --> 00:02:05,290
Before any of these previous libraries to or is really more or less the first and the most popular Deep

27
00:02:05,330 --> 00:02:10,900
Learning libraries that it's been used ever since tens before was launched in 2015.

28
00:02:11,060 --> 00:02:12,740
It basically took off.

29
00:02:12,800 --> 00:02:14,210
It's now the most popular.

30
00:02:14,210 --> 00:02:21,680
This is obviously supported by Google and carers is supported by chocolate and quite a few of the engineers

31
00:02:21,680 --> 00:02:23,570
from Google as well.

32
00:02:23,960 --> 00:02:31,910
Interestingly carers was actually launched in March 2015 whereas intensive flow was launched late 2015

33
00:02:31,910 --> 00:02:33,320
around November.

34
00:02:33,410 --> 00:02:41,250
If I remember correctly pine torch is actually supported by Facebook and but these three are really

35
00:02:41,250 --> 00:02:48,900
the most popular of all the different various Deep Learning libraries as you can see from the are from

36
00:02:48,900 --> 00:02:50,070
various metrics.

37
00:02:50,070 --> 00:02:52,200
One that I've shown here is just google search.

38
00:02:54,670 --> 00:02:54,960
Right.

39
00:02:54,970 --> 00:03:01,330
Finally in terms of the workflow that I just want to discuss with you before we go into looking at some

40
00:03:01,330 --> 00:03:02,770
examples.

41
00:03:03,420 --> 00:03:08,320
Chris intensive law is what we are using this course.

42
00:03:08,590 --> 00:03:14,860
We are using carrots because it's really probably one of the easiest to grasp in terms of the concept

43
00:03:15,400 --> 00:03:17,830
rather than get down to the nitty gritty of it.

44
00:03:17,860 --> 00:03:22,560
You can actually visualize it from previously I've shown you how it's done.

45
00:03:22,570 --> 00:03:27,110
We're gonna go into a few more examples this time.

46
00:03:27,280 --> 00:03:31,110
Looking at the workflow typically you will need to define your training data.

47
00:03:31,120 --> 00:03:33,550
You have your input x and your target y.

48
00:03:33,640 --> 00:03:39,040
So you need to take your input and also the the so-called result that you or you want the neural network

49
00:03:39,040 --> 00:03:45,760
to learn and the next thing you need to do is define your network architecture from the complexity of

50
00:03:45,760 --> 00:03:46,620
the task you are.

51
00:03:46,690 --> 00:03:54,960
You have to make choices in terms of how deep your layer is and how many neurons to learn and the next

52
00:03:54,960 --> 00:04:00,780
thing is do you need to select the appropriate lost function optimizer as well as performance matrix

53
00:04:01,410 --> 00:04:08,140
in order to actually measure the performance of your neural network and you repeat this task until you

54
00:04:08,160 --> 00:04:13,110
finally preset for specified performance is reached.

55
00:04:13,110 --> 00:04:18,060
Those are the basic workflow and that comes with a neural network and with that I'm going to stop here

56
00:04:18,080 --> 00:04:21,930
and I'm going to go into example and the next video.

57
00:04:21,930 --> 00:04:22,620
Thank you once again.
