WEBVTT
1
00:00:01.330 --> 00:00:02.040
Welcome back everyone.

2
00:00:02.040 --> 00:00:09.700
We're looking at data proof processing which is really a very important lesson for you in terms of the

3
00:00:10.920 --> 00:00:14.760
workflow of building machine learning models.

4
00:00:14.760 --> 00:00:19.030
Let's start with the basic importing the usual importing data.

5
00:00:19.050 --> 00:00:26.610
Preparing it and plotting the data itself so we're looking at just basically Alstead again the nonlinear

6
00:00:27.480 --> 00:00:34.180
characteristics of it and just have a good comparison of course if you are familiar with the gradient

7
00:00:34.200 --> 00:00:37.040
descent this is what we're really doing here.

8
00:00:37.320 --> 00:00:45.150
But rather than going through the extreme details of talking about the mechanics of gradient descent

9
00:00:46.220 --> 00:00:53.270
I'm just going to show you the illustration of what happened if you are not or if you have not pre process

10
00:00:53.270 --> 00:00:58.410
your data and the whole idea of gradient descent is two continuous.

11
00:00:58.440 --> 00:01:04.980
It's an algorithm to identify what we call the laws errors.

12
00:01:05.000 --> 00:01:14.210
So obviously it's a method to descent down meaning the idea as you can tell from the name is gradient

13
00:01:14.270 --> 00:01:15.070
descent.

14
00:01:15.170 --> 00:01:23.630
So as following the actual grading herself to go downhill and the actual gradient is measure of the

15
00:01:23.720 --> 00:01:24.730
losses.

16
00:01:24.740 --> 00:01:25.000
Okay.

17
00:01:25.040 --> 00:01:31.610
So you're going from high loss to lower loss to lower loss and the steeper the gradient meaning the

18
00:01:31.850 --> 00:01:33.990
fast faster the rate of drop.

19
00:01:34.100 --> 00:01:39.650
That's the path that it chooses until it reaches a point whereby it cannot go any lower.

20
00:01:39.650 --> 00:01:46.490
That was what we called the minima and the idea of doing this is that you are trying to lower the loss

21
00:01:46.900 --> 00:01:48.250
okay or lower.

22
00:01:48.290 --> 00:01:54.920
The difference between your prediction versus the actual real world real world value.

23
00:01:54.950 --> 00:02:01.190
So it's really trying to minimize your losses or come to a point where by your prediction is really

24
00:02:01.190 --> 00:02:06.290
close to the real world or the real or some poor data that you have.

25
00:02:06.290 --> 00:02:12.110
So that is a one minute explanation of grandiose end I wanted to get in the way of you understanding

26
00:02:12.770 --> 00:02:16.640
the pre processing the idea of pre processing.

27
00:02:16.640 --> 00:02:18.680
I'm trying to actually explain here.

28
00:02:18.690 --> 00:02:24.770
Now when it comes to pre processing I'm going to compare now with that piece of processing vs. with

29
00:02:24.770 --> 00:02:29.870
pre processing and the speed is one although I can't illustrate here.

30
00:02:30.200 --> 00:02:33.890
And the second thing really is the spot I will call the

31
00:02:36.670 --> 00:02:46.400
you might not get to your ultimate goal of getting an optimal estimates of your coefficient or your

32
00:02:46.790 --> 00:02:49.120
you know your value for your machine learning model.

33
00:02:49.130 --> 00:02:52.580
If you don't pre processing done pretty process your data.

34
00:02:52.610 --> 00:02:52.790
All right.

35
00:02:52.790 --> 00:03:00.110
So let's just plot this out and you will know that as we actually go through these so-called epoch epoch.

36
00:03:00.350 --> 00:03:06.530
Each cycle of looking at the data to estimate the so called coefficient.

37
00:03:06.650 --> 00:03:08.100
You noticed it.

38
00:03:08.280 --> 00:03:17.270
It seems to be okay until we reach while closer and closer to our final estimate of our coefficient.

39
00:03:17.300 --> 00:03:25.160
Companies point it explode and the explosion really shows you how unstable the coefficient that we have

40
00:03:26.240 --> 00:03:28.370
come to derived.

41
00:03:28.370 --> 00:03:30.930
When we don't pre process the data.

42
00:03:31.010 --> 00:03:33.690
Compare this to the pre processed data.

43
00:03:33.710 --> 00:03:35.480
It starts with high error.

44
00:03:35.480 --> 00:03:41.960
This is sums of squares of error and it continued the descent lower and lower to the point whereby it

45
00:03:41.960 --> 00:03:43.130
just stable.

46
00:03:43.130 --> 00:03:53.930
And this is the kind of behavior that we prefer to see when it comes to estimating our coefficient and

47
00:03:54.290 --> 00:04:00.470
thus to comparison Hopper made a point of trying to pre process or data is always a good practice of

48
00:04:00.470 --> 00:04:06.270
pre processing data beforehand before you start building your machine learning model.

49
00:04:08.010 --> 00:04:08.500
Okay.

50
00:04:08.540 --> 00:04:14.620
So that really gives you a very quick rundown of one of the benefits of performing data processing and

51
00:04:14.620 --> 00:04:19.000
there are others as we go through the idea of pre processing.

52
00:04:19.010 --> 00:04:26.600
You will be able to see more and more the difference between them and just an extension of the pre process

53
00:04:26.600 --> 00:04:27.150
data.

54
00:04:27.170 --> 00:04:35.790
We have the histogram of X the original data looks from zero to 40 and after scaling.

55
00:04:36.360 --> 00:04:42.890
You'll notice that it actually range from minus four to four and with the Center mean off around zero

56
00:04:42.920 --> 00:04:49.050
and the mean of this is around 10 for the DL.

57
00:04:49.130 --> 00:04:50.120
The Alstead.

58
00:04:50.270 --> 00:04:53.260
Okay so this is the actual data itself.

59
00:04:53.630 --> 00:04:53.930
Right.

60
00:04:53.960 --> 00:04:55.880
So let's look at data processing.

61
00:04:55.880 --> 00:04:57.560
There are four types.

62
00:04:58.370 --> 00:05:00.300
There is the mean removal.

63
00:05:00.590 --> 00:05:09.050
Basically you are looking to get to shift your data to mean of zero and a deviation of 1 min max is

64
00:05:09.050 --> 00:05:10.670
basically scaling it to a range.

65
00:05:10.670 --> 00:05:13.040
Quite often is between 0 and 1.

66
00:05:13.070 --> 00:05:16.360
Then you have the added 2 which is normalization and binary ization.

67
00:05:16.360 --> 00:05:21.790
I'll go into a little bit more detail when we actually get to that point.

68
00:05:21.900 --> 00:05:26.720
There are a couple of things here is that there are some assumptions that comes with pre processing

69
00:05:26.720 --> 00:05:27.650
that every processing.

70
00:05:27.650 --> 00:05:36.980
And these are economic assumptions that have been made and this is cyclic learn the documentation stating

71
00:05:37.010 --> 00:05:40.820
how it works in real life.

72
00:05:41.000 --> 00:05:46.210
They asked a couple assumptions first is that it follows a normal distribution and we know how well

73
00:05:46.250 --> 00:05:53.450
basically home possible that the data that we have real will come close or remotely come close to normal

74
00:05:53.450 --> 00:05:54.800
distribution.

75
00:05:54.800 --> 00:05:59.050
Second thing is that the C is that most method are based on linear assumptions of course.

76
00:05:59.060 --> 00:06:03.540
This is going to be broken from time to time and quite often actually.

77
00:06:03.590 --> 00:06:08.300
And the third one is most machine learning requires the data to be standard normally distributed and

78
00:06:08.320 --> 00:06:14.690
Gorgon which is normal really in other words we have zero mean the mean of zero and unit variance which

79
00:06:14.690 --> 00:06:17.810
is another word for variance is 6 to 1.

80
00:06:17.890 --> 00:06:22.520
And when it comes to cycle the documentation from the practitioner basically say they quite often ignore

81
00:06:22.520 --> 00:06:29.360
the shape of the distribution and just transform the data center it by removing the mean value removing

82
00:06:29.360 --> 00:06:31.970
mean value means we have a bias.

83
00:06:32.200 --> 00:06:35.570
The bias here is not in the English word of the word bias.

84
00:06:35.570 --> 00:06:42.980
Bias here means basically we have the value Y is both zero or below zero meaning it deviates from zero.

85
00:06:42.980 --> 00:06:49.790
So we're trying to center it so that Y which is a dependent value is zero and s go up by dividing non

86
00:06:49.790 --> 00:06:52.780
constant features by dest and deviations or Artemis.

87
00:06:52.790 --> 00:06:58.270
That means we are converting the standard deviation down to 1 or variance of 1 to be more exact.

88
00:07:00.480 --> 00:07:07.920
And let me just show you from Sakhalin and the training data here is really the stat we are looking

89
00:07:07.920 --> 00:07:08.820
at specifically.

90
00:07:08.850 --> 00:07:19.050
So you have 1 minus 1 2 2 0 2 and 0 1 0 and the mean here the axis is 0 0 meaning along this.

91
00:07:19.100 --> 00:07:19.690
OK.

92
00:07:19.800 --> 00:07:24.840
And along this and along this so as 1 2 0 0 minus 0 1 and 2 0 1.

93
00:07:24.840 --> 00:07:29.010
So as you can tell from here that's why this mean is zero.

94
00:07:29.040 --> 00:07:32.050
There also excess it could a 1 which is actually along.

95
00:07:32.230 --> 00:07:36.100
OK so this is vertical instead of horizontal.

96
00:07:36.120 --> 00:07:40.560
Let me show you one by one each of the four different types that we talked about here.

97
00:07:40.560 --> 00:07:46.850
You have sanitization slash win removal slash variance scaling.

98
00:07:47.130 --> 00:07:49.740
I'll leave you to do so and do the math.

99
00:07:49.740 --> 00:07:54.620
That's not important for the purpose of machine learning.

100
00:07:54.690 --> 00:08:00.280
It is useful to know that important to know as well.

101
00:08:00.320 --> 00:08:08.240
But it's not as important when it comes to the actual practical application of it to understand how

102
00:08:08.240 --> 00:08:09.320
it's used in a real war.

103
00:08:09.920 --> 00:08:15.860
But I still strongly encourage you to actually dig into the data itself or dig into the algorithm what

104
00:08:15.860 --> 00:08:21.470
it's done and the implications of not doing it versus the implications of doing it and the pitfalls

105
00:08:21.470 --> 00:08:21.830
as well.

106
00:08:21.830 --> 00:08:26.180
Those are the things that to keep in mind for now the focus of this lesson really is how do you actually

107
00:08:26.390 --> 00:08:28.010
process the data.

108
00:08:28.010 --> 00:08:31.790
So we have the pre processing which we actually import here.

109
00:08:31.790 --> 00:08:34.080
This is the actual library itself.

110
00:08:34.100 --> 00:08:37.410
Remember these are all original raw data.

111
00:08:37.730 --> 00:08:47.780
And after your pre process it you notice that the mean is now 0 for all of the vertical and the standard

112
00:08:47.780 --> 00:08:49.730
deviation is or 1.

113
00:08:49.760 --> 00:08:55.620
So this is under standardization or mean remove or variance scaling so this is really the first method

114
00:08:55.620 --> 00:08:58.850
that I wanted to cover with you.

115
00:08:59.120 --> 00:09:04.250
There's one thing to keep in mind though is that when you are performing machine learning so far we

116
00:09:04.250 --> 00:09:10.840
haven't talked about topic or splitting your data into training data versus test data.

117
00:09:11.060 --> 00:09:19.690
You need to be mindful of the process of preparing a data and booting the machine learning model.

118
00:09:19.690 --> 00:09:26.100
And when you are actually pre processing that data you should not process all your data.

119
00:09:26.120 --> 00:09:35.230
You should only prepare says your training data OK and using the same scaler.

120
00:09:35.620 --> 00:09:40.690
Okay you don't fit it again when it comes to working with the test data.

121
00:09:40.690 --> 00:09:48.580
You only need to transform your test data otherwise the score creep forward or you are peeping at your

122
00:09:48.580 --> 00:09:49.570
answer.

123
00:09:49.600 --> 00:09:51.000
Start to creep in.

124
00:09:51.010 --> 00:09:55.680
OK so that's the one part to be very mindful of.

125
00:09:56.170 --> 00:10:05.790
So we first of all fit not this is just the training data and these are the mean these are the scale

126
00:10:05.880 --> 00:10:09.180
and you're just transform your training data.

127
00:10:09.180 --> 00:10:14.570
You don't actually go and fit your test data.

128
00:10:14.610 --> 00:10:14.910
Okay.

129
00:10:14.910 --> 00:10:16.400
That part is important.

130
00:10:16.800 --> 00:10:19.740
And where we plotted out this is what it looks like.

131
00:10:19.890 --> 00:10:24.290
And now you're only utilized to transform for your test data.

132
00:10:24.290 --> 00:10:24.660
Okay.

133
00:10:24.660 --> 00:10:27.680
The part is really crucial to understand ours.

134
00:10:27.690 --> 00:10:35.340
You're gonna make the cardinal mistakes of allowing your algorithm to be trained on your test data.

135
00:10:35.550 --> 00:10:35.810
Okay.

136
00:10:35.820 --> 00:10:41.310
It's just like taking an exam you have already seen the test questions as well as the answer.

137
00:10:41.310 --> 00:10:44.900
Of course you're going to perform well but what's the point of doing that.

138
00:10:45.030 --> 00:10:45.890
You have done that.

139
00:10:45.880 --> 00:10:46.350
It doesn't.

140
00:10:47.190 --> 00:10:50.660
You have no idea whether you've learned the subject matter or not.

141
00:10:51.090 --> 00:10:58.310
So that's the whole idea of make ensuring you don't fit your test data.

142
00:10:58.380 --> 00:11:03.790
The second method is really on what we call mimics or scaling the feature to a range.

143
00:11:03.810 --> 00:11:08.520
In this case we are scaling it to between 0 and 1.

144
00:11:08.700 --> 00:11:12.560
So in the training data this is what are looking at.

145
00:11:12.570 --> 00:11:16.500
Once you scale it and transform it and if you look at the actual data.

146
00:11:16.530 --> 00:11:20.720
So I've noticed that by and large it's between 0 and 1.

147
00:11:21.300 --> 00:11:26.760
Once you perform your fitting and transformation of your training data.

148
00:11:26.760 --> 00:11:35.950
Notice that we're emphasizing on the training data then the data of the transform training data should

149
00:11:35.950 --> 00:11:39.920
be should only be between 0 and 1 Okay.

150
00:11:39.980 --> 00:11:45.670
It doesn't have to be zero does not to be is but it needs to be in that range of 0 to 1.

151
00:11:45.670 --> 00:11:50.480
There shouldn't be any values that below zero nor should it be any value as above.

152
00:11:50.480 --> 00:11:59.490
1 That is not the case however when you apply this Chinese form algorithm that you have fitted on the

153
00:11:59.490 --> 00:12:08.770
training data and apply it to the test data in the test data it can go below zero or above one.

154
00:12:08.790 --> 00:12:09.050
All right.

155
00:12:09.060 --> 00:12:11.140
So that's the one key part here.

156
00:12:11.160 --> 00:12:16.860
So looking at our original data is very much between minus one and two.

157
00:12:16.860 --> 00:12:19.570
If you look at here we have minus three.

158
00:12:19.570 --> 00:12:20.640
Now we have a four.

159
00:12:20.700 --> 00:12:20.960
OK.

160
00:12:20.970 --> 00:12:25.220
So obviously that these exceed the training data range.

161
00:12:25.350 --> 00:12:32.970
And if I provide it as a test data and transform it if you look at test data noticed it it sees a and

162
00:12:33.090 --> 00:12:34.490
it does exceed one.

163
00:12:34.590 --> 00:12:36.720
It does go below zero.

164
00:12:36.720 --> 00:12:38.490
So that part is important.

165
00:12:38.760 --> 00:12:45.710
It's not a characteristic that is not unexpected is something that you need to be mindful of.

166
00:12:45.960 --> 00:12:47.780
It makes absolute scalar.

167
00:12:48.000 --> 00:12:54.510
So this works in very similar fashion but scales in a way that the training data lies between the range

168
00:12:54.510 --> 00:13:01.490
of minus 1 and 1 whereas before we have 0 and 1 so for this is minus 1 and 1.

169
00:13:03.160 --> 00:13:03.450
Okay.

170
00:13:03.480 --> 00:13:09.130
So there is some underlying assumptions with this is that it's meant for data that's already center

171
00:13:09.130 --> 00:13:09.990
at zero.

172
00:13:10.050 --> 00:13:14.100
Okay so that all sparse data but at this center around zero.

173
00:13:14.430 --> 00:13:19.830
So that the one part that you need to keep in mind and if you haven't center your data then you're going

174
00:13:19.830 --> 00:13:24.900
to have a math problem that doesn't actually center for you.

175
00:13:25.020 --> 00:13:30.660
This is part of the reason why we don't always use this is we use another method was to our come to

176
00:13:30.660 --> 00:13:31.470
in a minute.

177
00:13:31.470 --> 00:13:33.440
So just a quick revision why have we cover.

178
00:13:33.430 --> 00:13:43.150
We've cover standardization and we have cover min max and we have cover Max absolute color.

179
00:13:44.120 --> 00:13:49.050
I'm going to lightly touch on these two scaling sparse data.

180
00:13:49.550 --> 00:13:56.870
Quite often when you sent a sparse data you destroy the sparse construction of data and it's not exactly

181
00:13:56.900 --> 00:13:58.740
the more sensible things to do.

182
00:13:59.070 --> 00:14:03.520
Okay so that's one thing to keep in mind.

183
00:14:03.780 --> 00:14:10.780
Sometimes it makes sense to scale sparse inputs especially of features on different scale.

184
00:14:10.780 --> 00:14:10.960
Okay.

185
00:14:10.970 --> 00:14:16.940
So you can actually use max apps color or Max apps underscore scalar for that.

186
00:14:17.030 --> 00:14:17.330
Okay.

187
00:14:17.900 --> 00:14:19.770
But I won't go into details for this.

188
00:14:19.820 --> 00:14:24.670
Please refer the link to provide in this notebook.

189
00:14:24.680 --> 00:14:31.460
The other thing is scaling versus highlighting sometimes is not enough to sentence go the features independently

190
00:14:32.300 --> 00:14:40.940
since the model itself can take and make further assumptions on the linear independent of the feature.

191
00:14:40.940 --> 00:14:48.880
So sometimes to address it you do need to actually apply principle component analysis to remove remove

192
00:14:48.890 --> 00:14:52.380
the what we call linear correlation across features.

193
00:14:52.430 --> 00:14:58.610
Now what do I mean by that sometimes between features down correlations and sometimes this is called

194
00:14:58.610 --> 00:15:02.860
multi continuity especially if is actually very strong.

195
00:15:03.170 --> 00:15:06.920
We often apply the composition using PCH to remove that.

196
00:15:06.950 --> 00:15:12.440
So that's the one thing to keep in mind as another arsenal in your tool box for you to pull it out.

197
00:15:12.440 --> 00:15:15.470
Should you actually observe that right.

198
00:15:15.480 --> 00:15:23.140
Normalization is the process of scaling your individual sample to have unit Norm okay.

199
00:15:23.240 --> 00:15:30.170
And this is the actual algorithm itself but there are two types of far normalization L1 normalization

200
00:15:30.200 --> 00:15:39.740
and now to the L1 is basically what we call least absolute deviations and the L2 is least squares.

201
00:15:39.740 --> 00:15:46.670
So one is using the absolute operator while the other one is using the brackets square or least squares

202
00:15:47.330 --> 00:15:48.920
normalization technique

203
00:15:51.810 --> 00:15:55.420
so let's look at the two different types of normalization.

204
00:15:55.410 --> 00:15:57.240
An example of them.

205
00:15:57.240 --> 00:16:03.560
Remember L1 is really least absolute deviation L2 is least square.

206
00:16:04.290 --> 00:16:06.310
So running on the first one.

207
00:16:06.450 --> 00:16:09.560
This is normalize using the L to NOM.

208
00:16:09.750 --> 00:16:10.920
You get this.

209
00:16:11.220 --> 00:16:12.840
OK.

210
00:16:12.870 --> 00:16:17.730
The reality is that you are not going to be able to really differentiate between L1 L2.

211
00:16:17.740 --> 00:16:24.670
It's just the actual tools and methods of normalizing your data.

212
00:16:24.690 --> 00:16:27.670
Just let me remind you what normalization is.

213
00:16:27.870 --> 00:16:31.980
Normalization really is to actually compress our data.

214
00:16:32.400 --> 00:16:32.670
Okay.

215
00:16:32.670 --> 00:16:35.350
Scaling it to have unit known.

216
00:16:35.420 --> 00:16:35.680
Okay.

217
00:16:35.700 --> 00:16:40.900
So that's the part that we are really really aiming to perform here.

218
00:16:40.920 --> 00:16:43.000
There are other ways to do this.

219
00:16:43.080 --> 00:16:46.920
It does provide an API to fit and transform your data.

220
00:16:47.400 --> 00:16:53.070
Not crucial but it's something that you can actually utilize and make use of.

221
00:16:53.060 --> 00:16:56.620
And essentially this is actually same as this line anyway.

222
00:16:56.640 --> 00:16:58.440
So it's just a different way of doing it.

223
00:16:59.360 --> 00:16:59.630
Okay.

224
00:16:59.640 --> 00:17:08.580
The last part really is by narration feature by normalization is really the process of setting a threshold

225
00:17:08.880 --> 00:17:14.090
and then converting your data using that threat threshold to decide whether there should be a zero.

226
00:17:14.130 --> 00:17:15.530
Or it should be a one.

227
00:17:15.540 --> 00:17:15.790
Okay.

228
00:17:15.790 --> 00:17:19.980
So meaning converting it to boolean 0 or 1 or slightly different.

229
00:17:21.060 --> 00:17:21.310
Okay.

230
00:17:21.330 --> 00:17:29.660
So looking at this here what we can probably have done here is really just withdraw data and using the

231
00:17:29.660 --> 00:17:33.840
binaries a notice that is become switching in the 0 1.

232
00:17:33.860 --> 00:17:39.620
And what we do want to know is that what exactly is the threshold and the threshold here is present

233
00:17:39.650 --> 00:17:45.380
at zero and above zero then is a one if it's less than zero then it's a minus one.

234
00:17:45.380 --> 00:17:47.310
So if it's less than zero.

235
00:17:47.930 --> 00:17:49.130
Dennis zero.

236
00:17:49.220 --> 00:17:50.930
So that's really the whole idea.

237
00:17:50.930 --> 00:17:55.360
Of course you can actually change the threshold and transform the data.

238
00:17:55.790 --> 00:18:00.800
Changing the threshold depends on the situation and the projects that you work on.

239
00:18:00.810 --> 00:18:02.020
You can actually do that.

240
00:18:02.030 --> 00:18:07.790
And notice that there are some variation especially when it comes to this zero is both minus zero point

241
00:18:07.790 --> 00:18:08.200
five.

242
00:18:08.210 --> 00:18:11.250
Hence it's actually a one.

243
00:18:11.730 --> 00:18:11.940
Right.

244
00:18:12.030 --> 00:18:18.890
How about when you're working with categorical features non numeric data to show you two.

245
00:18:18.890 --> 00:18:29.310
One is called labeling counter liberal code just encode your SO core categorical feature into a number.

246
00:18:29.310 --> 00:18:32.370
The process really is from the pre processing library.

247
00:18:32.370 --> 00:18:38.460
You instantiate a version of the label in Qatar and here I call a label underscore and even see you

248
00:18:38.460 --> 00:18:43.470
can actually use another name that fits your purpose and then you're fit and transformed the source

249
00:18:43.470 --> 00:18:46.670
data which is this one here and perform that.

250
00:18:46.770 --> 00:18:51.750
And this is just a code to illustrate that for Australia 0 Hong Kong and so on.

251
00:18:51.750 --> 00:18:59.580
Using it to a Singapore tree and if I provide the test data of these in different order from theirs

252
00:18:59.880 --> 00:19:05.730
and I want to see what happens if I encode just transform my test data what would the result be.

253
00:19:05.730 --> 00:19:10.250
Hong Kong is one coming back here as looking at my map.

254
00:19:10.440 --> 00:19:16.230
Yes Hong Kong and indeed one New Zealand is two and do this or two.

255
00:19:16.350 --> 00:19:21.130
I'll let you check out the Singapore and Australia has been transformed correctly or not.

256
00:19:22.140 --> 00:19:30.020
Okay so this is really label and coder Kong coding your Kilgore conical value into numerical number

257
00:19:30.030 --> 00:19:32.080
but they are there's no meaning to it.

258
00:19:32.130 --> 00:19:39.000
Okay so that's the part that's important and one hot encoding or one of K is actually converting it

259
00:19:39.000 --> 00:19:44.290
into our sparse matrix which is just purely binary result 0 or 1.

260
00:19:44.520 --> 00:19:51.030
So let's bring back source of that reminder and the actual SAIC meaning the number that we used to encode

261
00:19:51.030 --> 00:19:59.070
these categorical value we have to import one hot encoding and look at the so-called output or the outcome

262
00:19:59.070 --> 00:19:59.420
of it.

263
00:19:59.430 --> 00:20:07.540
Just remember we have to instantiate first and then the way that it needs to be taken in or transform

264
00:20:07.540 --> 00:20:13.370
the data has to be in a specific manner then we've performed the fit transform and then when we print

265
00:20:13.370 --> 00:20:15.810
out the one on encoding This is what it looks like.

266
00:20:15.810 --> 00:20:17.500
Notice that they're all unique.

267
00:20:17.640 --> 00:20:23.290
This is 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0.

268
00:20:23.280 --> 00:20:24.510
They're all different.

269
00:20:25.090 --> 00:20:25.400
Okay.

270
00:20:25.440 --> 00:20:27.010
So that part is important then.

271
00:20:27.010 --> 00:20:35.820
And if I reverse perform reversion meaning taking one number and go backward what I've done here is

272
00:20:35.820 --> 00:20:46.350
that I've taken one hot encoding taking the So core zero slice meaning this is roll zero which is this

273
00:20:46.350 --> 00:20:54.210
one here an inverse transform it so that I can actually see what my original value is at the first value

274
00:20:54.210 --> 00:20:55.730
is actually Australia.

275
00:20:55.800 --> 00:21:00.330
And sure enough it is Australia and you can do another one for the last value.

276
00:21:00.330 --> 00:21:02.490
Happens to be three here.

277
00:21:02.490 --> 00:21:08.710
So that's really the last portion of the basic idea under cover when it comes to data processing.

278
00:21:08.730 --> 00:21:15.420
It is a very important step of machine building a machine learning model is really the data pre processing

279
00:21:15.790 --> 00:21:22.230
what I've shown you here is what it looks like before and with and without pre processing and before

280
00:21:22.230 --> 00:21:27.660
you're trying to scale your data and after you scale your data what happened and I told took also talked

281
00:21:27.660 --> 00:21:31.800
about the four different types there is desensitization mean removal.

282
00:21:31.800 --> 00:21:33.620
This is the most common type.

283
00:21:33.960 --> 00:21:39.710
You have the min max meaning scaling it to a range and then you have the normalization and also binary

284
00:21:39.710 --> 00:21:48.270
ization and one part that is not highlighted here is labelling encode your categorical value as well

285
00:21:48.270 --> 00:21:50.940
as one hot encoding but I'm going to stop.

286
00:21:50.940 --> 00:21:54.450
Thank you for watching and appreciate you following along the lesson.
