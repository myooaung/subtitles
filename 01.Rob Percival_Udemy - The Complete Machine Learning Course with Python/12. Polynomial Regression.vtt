WEBVTT
1
00:00:03.500 --> 00:00:04.400
Welcome back everyone.

2
00:00:04.450 --> 00:00:10.130
In this lesson we're gonna go into polynomial regression polynomial regression is particularly useful

3
00:00:10.130 --> 00:00:14.570
when you're dealing with non-linear relationship which is what we're going to explore.

4
00:00:14.570 --> 00:00:19.310
Again I'm going to skip through the math without going into depth on those.

5
00:00:19.310 --> 00:00:26.870
I just want to illustrate with example the basic concept of how to do with non linear relationship but

6
00:00:26.990 --> 00:00:29.020
using polynomial regression.

7
00:00:29.240 --> 00:00:35.570
Again we use import the usual library and the basic psychic line and this is just to illustrate to you

8
00:00:35.570 --> 00:00:38.070
which version of an umpire panders.

9
00:00:38.180 --> 00:00:45.200
And the rest of the packages and libraries I'm using now more to the point really is this portion here.

10
00:00:45.200 --> 00:00:51.240
This formula this method Miller as you can tell here is X to the power of three clearly is not linear.

11
00:00:51.470 --> 00:00:57.590
Plus 100 which means we are raising the offset by 100 hundred plus error item.

12
00:00:58.760 --> 00:01:05.140
And I'm going to model that using random numbers and we're gonna do it 100 samples linear space here

13
00:01:05.180 --> 00:01:13.790
basically means that we're breaking the x from zero to a 10 100 subsegments and we're generating 100

14
00:01:13.790 --> 00:01:16.640
random numbers here by 100.

15
00:01:16.640 --> 00:01:25.610
Multiply it by 100 just to increase the scale and for this is why is extra the power three plus random

16
00:01:25.610 --> 00:01:28.230
number that we've generated here plus 100.

17
00:01:28.250 --> 00:01:30.380
So basically that's this formula here.

18
00:01:30.380 --> 00:01:36.860
So extra bar three random number and 100 and I'm plotting it out just so that you can actually see what

19
00:01:36.860 --> 00:01:39.090
it looks like using a scatter diagram.

20
00:01:39.470 --> 00:01:46.980
Looking at this you can see that the score it is not linear.

21
00:01:47.030 --> 00:01:49.190
That's the first thing that you can observe.

22
00:01:49.190 --> 00:01:57.290
Second of all you probably can't see clearly here is that the so-called line has been raised by 1 100.

23
00:01:57.310 --> 00:01:57.610
OK.

24
00:01:57.650 --> 00:01:59.060
So it is 100.

25
00:01:59.090 --> 00:02:05.210
The reason that we can't really see the clear line here is because what I have done is this function

26
00:02:05.210 --> 00:02:06.020
here.

27
00:02:06.080 --> 00:02:06.680
All right.

28
00:02:06.740 --> 00:02:14.720
This multiply by 100 created substantial amount of noise which I want the machine learning model to

29
00:02:15.170 --> 00:02:16.880
actually model that.

30
00:02:16.910 --> 00:02:17.270
OK.

31
00:02:17.270 --> 00:02:24.110
To to see through the noise and to be able to identify the underlying patterns that's being generated

32
00:02:24.730 --> 00:02:25.640
now in real life.

33
00:02:25.640 --> 00:02:28.150
You're not going to have access to this formula.

34
00:02:28.240 --> 00:02:33.800
Okay although you're going to see is when you plot a vision I get a diagram I'll visualize that you're

35
00:02:33.800 --> 00:02:35.500
going to see something that looks like this.

36
00:02:36.110 --> 00:02:41.060
And from that you're going to have to deduce that well this is what there's no way that the relationship

37
00:02:41.060 --> 00:02:45.090
between X and Y is going to be linear no matter how I draw a straight line.

38
00:02:45.110 --> 00:02:47.210
It's just not going to fit the data.

39
00:02:47.270 --> 00:02:54.190
There is a certain underlying pattern here which goes fairly flat and then start to accelerate upwards.

40
00:02:54.230 --> 00:02:56.900
S x increases.

41
00:02:57.070 --> 00:03:01.970
All right so those are the things that you have to observe Of course this will come of intuition and

42
00:03:01.970 --> 00:03:09.710
as you are exposed to more more examples you start to get the idea of how to actually tackle the problem

43
00:03:09.710 --> 00:03:11.050
directly.

44
00:03:11.060 --> 00:03:16.910
Now I'm going to just show you something here just so that it's clear now what happened if I am more

45
00:03:16.910 --> 00:03:22.940
removed the error term and how you removed that is just basically remove this term altogether like that.

46
00:03:22.940 --> 00:03:23.280
Okay.

47
00:03:23.300 --> 00:03:27.730
Just cut it away and you can actually run this again.

48
00:03:27.990 --> 00:03:32.360
You can see right the clear pattern that comes with it.

49
00:03:32.480 --> 00:03:38.440
And if you look carefully this is actually well it should ought to be 100 really expert apology past

50
00:03:38.460 --> 00:03:45.380
100 so the starting point should really be 100 but visually it doesn't look like it but it should be.

51
00:03:45.950 --> 00:03:49.900
And the second thing is that you can see that this is not linear.

52
00:03:50.410 --> 00:03:50.780
OK.

53
00:03:50.810 --> 00:03:57.920
The noise kind of mask could just removed that and just make it less so.

54
00:03:57.920 --> 00:04:04.610
So that is actually in this formula here extra laboratory plus 100 plus random number is generated and

55
00:04:04.610 --> 00:04:11.510
from there as you can actually see the So core it is not linear technically is extra power three but

56
00:04:11.510 --> 00:04:12.580
you don't know that.

57
00:04:12.590 --> 00:04:18.650
So the only way to do that is just visualized and then start generating model and using performance

58
00:04:18.650 --> 00:04:24.110
evaluation to see how close we are to the the empirical data they will observe.

59
00:04:24.740 --> 00:04:33.580
So let's go back to what we had before which is really the noise and what we did here is that we generate

60
00:04:33.610 --> 00:04:39.460
100 sample based on what we specify here and then we multiply number by 100.

61
00:04:39.460 --> 00:04:45.180
So the scale of those random number generated are magnified by 100 times.

62
00:04:45.620 --> 00:04:46.020
Okay.

63
00:04:46.020 --> 00:04:53.890
They need random number as you're probably aware is a general one hundred but as between 0 and 1 by

64
00:04:53.890 --> 00:04:55.720
multiplying by 100.

65
00:04:55.740 --> 00:04:58.730
So this is should be minus 1 to 1.

66
00:04:58.910 --> 00:05:00.260
Okay.

67
00:05:00.280 --> 00:05:01.410
With the mean of 0.

68
00:05:01.420 --> 00:05:03.450
Multiply by 100 and magnify it.

69
00:05:03.460 --> 00:05:09.610
So you can see that from 100 but go below and we go above depending on whether the number is positive

70
00:05:09.610 --> 00:05:11.850
or negative 0 5.

71
00:05:11.980 --> 00:05:21.730
Gone through that at length the next part then is really looking at how we model the data using linear

72
00:05:21.730 --> 00:05:22.850
models.

73
00:05:22.870 --> 00:05:23.120
All right.

74
00:05:23.140 --> 00:05:31.780
So let's start with the matrix of Our Square and you have this you have seen before generating in terms

75
00:05:31.780 --> 00:05:37.870
of creating a model selecting the model predicting the model from the data that we have and try to see

76
00:05:37.870 --> 00:05:45.610
if we are able to well come close to modeling the data using the underlying machine model from our square

77
00:05:45.610 --> 00:05:47.200
says 76 percent.

78
00:05:47.230 --> 00:05:54.830
Well close as always there's no way for you to tell which is the best male model.

79
00:05:54.940 --> 00:06:01.210
So the best the next best way says you don't have something to compare against is to actually create

80
00:06:01.240 --> 00:06:02.290
another model.

81
00:06:02.290 --> 00:06:03.710
Just as a comparison.

82
00:06:03.770 --> 00:06:09.520
Now I mentioned before looking at the underlying just purely the visualized as visualization aspect

83
00:06:09.520 --> 00:06:14.290
of it you should have been to actually identify that it is not normal.

84
00:06:14.290 --> 00:06:14.700
Right.

85
00:06:14.800 --> 00:06:16.360
Meaning is not linear.

86
00:06:16.390 --> 00:06:23.980
So what we do here is we import from psychic land pre processing import what's called a polynomial features

87
00:06:24.610 --> 00:06:34.120
and we stated that we want to actually go up to second degree meaning extra power to and we perform

88
00:06:34.120 --> 00:06:36.190
fit transform of x.

89
00:06:36.190 --> 00:06:36.470
All right.

90
00:06:36.490 --> 00:06:39.700
Using poly rigged which we instantiate here.

91
00:06:39.850 --> 00:06:44.300
Now basically we are actually transforming the X itself.

92
00:06:44.360 --> 00:06:50.680
And again we perform my linear regression this time where you called this model Lin underscore regression

93
00:06:50.680 --> 00:06:53.310
underscore too this is the second model.

94
00:06:53.380 --> 00:06:57.800
This was the first model that has the first model.

95
00:06:57.820 --> 00:07:05.820
So fit the actual x poly which we actually generated here and then Y and then perform our so-called

96
00:07:06.250 --> 00:07:10.810
prediction M plotting that out.

97
00:07:10.810 --> 00:07:14.330
It fits the data substantially better.

98
00:07:14.350 --> 00:07:15.670
Okay.

99
00:07:15.790 --> 00:07:23.990
Now remembering our formula which we use to generate this data is X to the power of three.

100
00:07:24.010 --> 00:07:26.930
Okay let's come back here next to the power three.

101
00:07:26.950 --> 00:07:33.470
Now when we actually stated polynomial features degree is good too.

102
00:07:33.470 --> 00:07:33.940
All right.

103
00:07:33.940 --> 00:07:35.320
So what does that actually mean.

104
00:07:35.320 --> 00:07:42.610
So let me just go over the cycle and polynomial features go to the documentation.

105
00:07:43.280 --> 00:07:43.640
Okay.

106
00:07:43.710 --> 00:07:48.860
So you can see here that it actually states how the default is too.

107
00:07:49.650 --> 00:07:59.210
Okay and the degree allows you to state so extra about zero extra power one extra power two extra policy.

108
00:07:59.230 --> 00:08:03.130
Let me come back here to look at the actual underlying data itself.

109
00:08:03.140 --> 00:08:09.050
Remember that we are looking at feature generating this feature.

110
00:08:09.050 --> 00:08:10.630
Degrees good too.

111
00:08:10.650 --> 00:08:10.920
All right.

112
00:08:10.920 --> 00:08:16.490
So just remember X capital x.

113
00:08:16.700 --> 00:08:20.480
Okay let me just show you with the first five.

114
00:08:21.080 --> 00:08:21.860
Okay.

115
00:08:22.110 --> 00:08:36.240
Let's try X poly cap ex capital x p and we'll look at the first five value looking at this.

116
00:08:38.100 --> 00:08:45.660
We are generating a constant term followed by the original data term followed by the X to the power

117
00:08:45.690 --> 00:08:47.710
of to term.

118
00:08:47.730 --> 00:08:50.560
Okay so this is X to the power to.

119
00:08:50.730 --> 00:08:58.730
Now if you want to go one step further meaning degree three okay.

120
00:08:59.220 --> 00:09:07.230
Then you can poly normal features and then the specified degree is equal to three and we generate something

121
00:09:07.230 --> 00:09:10.470
that similar to this.

122
00:09:10.510 --> 00:09:18.740
Okay but we're going to call this degree 3 an example.

123
00:09:18.870 --> 00:09:19.140
All right.

124
00:09:19.170 --> 00:09:22.810
So basically fit transforming the actual X..

125
00:09:23.220 --> 00:09:31.740
Again just to show you now if we look at example and look at the first five values it's gonna be a repeat

126
00:09:31.890 --> 00:09:33.610
of this.

127
00:09:33.630 --> 00:09:40.360
You have the constant term you have the original term extra power of 0 which is 1 really.

128
00:09:40.390 --> 00:09:45.290
So the actual underlying value itself then you have the extra the power.

129
00:09:45.330 --> 00:09:49.050
And then you have the extra the past three with the example.

130
00:09:49.050 --> 00:09:52.570
Hopefully that is a little bit clearer.

131
00:09:54.890 --> 00:09:55.740
Damaged.

132
00:09:55.750 --> 00:10:01.910
Just to go one step further just to ensure that you really really understand this.

133
00:10:02.020 --> 00:10:04.250
I'm gonna go one step further.

134
00:10:04.390 --> 00:10:05.320
I'm gonna I'm gonna use

135
00:10:12.490 --> 00:10:16.230
zero one two three four five.

136
00:10:16.640 --> 00:10:17.050
Okay.

137
00:10:17.050 --> 00:10:19.300
That ought to be pretty clear now.

138
00:10:19.720 --> 00:10:26.570
And the next thing is we're going to just directly make use of the polynomial degree.

139
00:10:26.690 --> 00:10:30.900
Okay so result to fit.

140
00:10:30.920 --> 00:10:31.420
Transform.

141
00:10:31.450 --> 00:10:35.060
Except that this time we have for transforming our yes.

142
00:10:35.140 --> 00:10:38.140
And so you can see the detail right away.

143
00:10:40.470 --> 00:10:47.300
OK so looking at past years which is Rio 0 1 2 3 4 5.

144
00:10:47.360 --> 00:10:50.750
You can see here we have the constant terms you're one two three four five.

145
00:10:50.750 --> 00:10:52.200
Now this is the square.

146
00:10:52.910 --> 00:10:58.930
I hope that's substantially clearer just to go one step further so that we can actually take this off

147
00:10:58.970 --> 00:11:02.580
the ball park with clarity.

148
00:11:02.630 --> 00:11:06.220
And finally there's not Polly rigged.

149
00:11:06.230 --> 00:11:10.080
This is core degree tree.

150
00:11:11.150 --> 00:11:18.190
And if I run that you should see that this one and then the value itself to the power to her battery.

151
00:11:18.580 --> 00:11:19.200
OK.

152
00:11:19.310 --> 00:11:20.560
Having done that let's.

153
00:11:20.630 --> 00:11:22.760
We've already seen this.

154
00:11:23.390 --> 00:11:30.420
This is substantially closer than the linear that we saw before which is over here.

155
00:11:30.420 --> 00:11:33.850
And if you go to extra power to it's substantially closer.

156
00:11:33.890 --> 00:11:40.950
We know the answer is X for the power three within the range that we're looking at from zero to 10.

157
00:11:40.970 --> 00:11:45.620
It's not so obvious once you actually have data going to 20.

158
00:11:45.820 --> 00:11:50.960
Execute a 20 extra 240 extra to 100 then it will becomes very obvious.

159
00:11:51.380 --> 00:11:53.810
But within this range is substantially close.

160
00:11:53.990 --> 00:11:54.640
Okay.

161
00:11:54.800 --> 00:12:02.810
That's the problem of the fact that we haven't gathered enough data and to narrow a range which is why

162
00:12:02.900 --> 00:12:09.860
we weren't able to observe empirically that this model is not quite the best one.

163
00:12:10.360 --> 00:12:10.670
Okay.

164
00:12:10.670 --> 00:12:13.860
Although it's better than linear but it's not the best one.

165
00:12:14.060 --> 00:12:15.680
It should really be extra policy.

166
00:12:15.690 --> 00:12:22.050
So these you have to actually as you collect more data you might have to revise a model over time.

167
00:12:22.930 --> 00:12:27.120
But having done that would just go one step further and look at real data.

168
00:12:28.310 --> 00:12:36.320
And I want to look at this to a variable here this and NOx and just to illustrate the concept slightly

169
00:12:36.320 --> 00:12:44.220
different this notice that this is a decreasing function as it get closer and closer to point 4 or 0

170
00:12:44.590 --> 00:12:46.280
is start to flatten out.

171
00:12:46.310 --> 00:12:46.600
OK.

172
00:12:46.620 --> 00:12:50.780
The the actual speed of decline decreases.

173
00:12:51.230 --> 00:12:54.120
Clearly this is not linear either.

174
00:12:54.230 --> 00:13:00.140
But we can actually start with the linear modeling and you can see the R squares point five nine.

175
00:13:00.140 --> 00:13:01.970
Now I'm not going to go through this.

176
00:13:02.060 --> 00:13:07.100
I'll leave it to you to actually go through the line because really is a repeat of before we've gone

177
00:13:07.100 --> 00:13:11.490
through there except my cone now is substantially more compact.

178
00:13:11.600 --> 00:13:16.820
This is the actual the first tree line is the modeling the following for line really is just plotting

179
00:13:18.870 --> 00:13:26.820
and the best fit line of best best fit is here and you can see to ask where is 59 if we try to the power

180
00:13:26.820 --> 00:13:27.360
to

181
00:13:29.970 --> 00:13:36.650
OK to the power to really again is that we have the polynomial features for transform and then instantiate

182
00:13:36.650 --> 00:13:40.160
the model for day to a data itself.

183
00:13:40.160 --> 00:13:48.320
And this part here really is rearranging so that we have we extract the minimum and we extract the largess

184
00:13:48.350 --> 00:13:56.660
with the step of one just so that we can actually have the x fitted x axis data so that we can actually

185
00:13:56.660 --> 00:13:58.910
perform the prediction.

186
00:13:58.910 --> 00:13:59.140
All right.

187
00:13:59.150 --> 00:14:06.470
So this where the y underscore prediction is about now the reason for doing this step is so that I am

188
00:14:06.470 --> 00:14:15.170
really trying to top up X in an even interval and I'm going to use that as my independent data to predict

189
00:14:15.260 --> 00:14:22.580
my dependent variable which is why in this case ultimately up the you created the model you want to

190
00:14:22.580 --> 00:14:27.040
actually run the prediction just to see what the outcome looks like.

191
00:14:27.050 --> 00:14:32.170
How close is it to the actual origin or data in our case.

192
00:14:32.180 --> 00:14:40.970
Because from my intuition of looking at it is that I just want to actually have even these base x so

193
00:14:40.970 --> 00:14:45.760
that I can see run the prediction to generate and see what the Y is mapped to.

194
00:14:45.920 --> 00:14:52.820
And then compared to the original data and then after that really is just a matter of visualizing it.

195
00:14:53.420 --> 00:14:59.330
And if you look at this well I don't ask where it tells you it is better.

196
00:14:59.330 --> 00:15:01.070
This part is a problem.

197
00:15:01.080 --> 00:15:09.740
OK so that means we need to actually use something else maybe log or some other method to you know to

198
00:15:09.740 --> 00:15:15.920
it to model this better but nevertheless I wanted to illustrate two things.

199
00:15:15.920 --> 00:15:18.250
Just using polynomial might not solve your problem.

200
00:15:18.260 --> 00:15:19.520
That's the first thing.

201
00:15:19.520 --> 00:15:25.230
Second thing is that ask where although has improve might not be the best thing I.

202
00:15:25.340 --> 00:15:31.190
It may have worked between the range from 0 to 8 but beyond that it actually is a terrible model.

203
00:15:31.190 --> 00:15:35.990
And this is also part of building the machine learning model is that you need to understand the range

204
00:15:36.580 --> 00:15:44.550
that your M.L. is good at and keep it working within the optimal zone.

205
00:15:44.550 --> 00:15:50.830
Beyond that you might want to actually consider not utilizing the model at all.

206
00:15:51.630 --> 00:16:00.120
And quite often in real world you use multiple models one you know basically car like benchmarking and

207
00:16:00.210 --> 00:16:07.020
always having multiple M.L. to compete against each other so that there is a check and balances against

208
00:16:07.050 --> 00:16:07.700
each other.

209
00:16:09.060 --> 00:16:14.040
Let's try Kubik so I'm meaning to the degree of three and run this whole exercise again.

210
00:16:14.280 --> 00:16:15.480
And as you plot it out.

211
00:16:16.030 --> 00:16:16.750
Okay.

212
00:16:17.010 --> 00:16:26.820
The Oscar improve in a very marginally 0.01 and Fs you can see it this did improve those start to dip

213
00:16:27.070 --> 00:16:28.720
twisters and here.

214
00:16:28.860 --> 00:16:36.590
Now this is the one on the problem of using higher polynomial meaning X the poetry and higher you're

215
00:16:36.590 --> 00:16:43.160
gonna have to fit all the fittings but to kick in it's a concept that I haven't talked about much it's

216
00:16:43.170 --> 00:16:50.580
something that we'll start to talk about in future lessons I just want to wrap up this lesson to just

217
00:16:50.580 --> 00:16:58.500
give you basically the whole idea of this lesson is to give you another perspective that you can utilize

218
00:16:58.500 --> 00:17:07.050
and another you know another tool or technique set in your armory to solve the problem that you have

219
00:17:07.050 --> 00:17:13.680
to model after the real world data and the polynomial is quite often a good way to do it because it's

220
00:17:13.710 --> 00:17:24.330
technically not what you will call higher dimensional and that in this sense basically is too linear

221
00:17:24.540 --> 00:17:33.180
the model they have created is too linear whereas the difference is that you are just adding the so-called

222
00:17:33.330 --> 00:17:40.260
x term by using the original data use but this original data you created extra to power to extra power

223
00:17:40.260 --> 00:17:44.210
three so that they are a bit more flexibility.

224
00:17:44.670 --> 00:17:50.820
Okay so that's really the basic idea polynomial just really extending the linear models to to it so

225
00:17:50.820 --> 00:17:53.090
that it becomes more flexible and more powerful.

226
00:17:53.690 --> 00:17:55.410
I hope this lesson has been useful.

227
00:17:55.600 --> 00:17:57.510
And thank you for watching.
