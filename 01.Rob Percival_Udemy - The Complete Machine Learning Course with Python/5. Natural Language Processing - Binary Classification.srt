1
00:00:00,930 --> 00:00:09,860
You might be forgiven for thinking that deep learning can really only be used for integers or value

2
00:00:09,870 --> 00:00:12,810
BS problems.

3
00:00:12,810 --> 00:00:16,430
What about text and sequence of tax.

4
00:00:16,590 --> 00:00:17,700
Two years ago.

5
00:00:17,700 --> 00:00:25,110
Probably not a well-known fact by now it is fairly common knowledge that deep learning can be applied

6
00:00:25,650 --> 00:00:33,010
to taxpayers problem just as easily as integer based or value based problems.

7
00:00:33,160 --> 00:00:39,860
I was gonna have a look at one such problem which is binary classification we are applying this to the

8
00:00:39,870 --> 00:00:42,450
I am D.B. database.

9
00:00:42,450 --> 00:00:43,830
What is I am D.B..

10
00:00:43,950 --> 00:00:52,010
I am VB is actually the sole core repository I guess you can call it or the website that collect all

11
00:00:52,020 --> 00:01:01,230
the viewers view of a movie and they do provide these data for for you to utilize is the really rich

12
00:01:01,230 --> 00:01:10,230
database where you have a lot of information and ratings and reviews and people write comments about

13
00:01:10,230 --> 00:01:10,360
it.

14
00:01:10,620 --> 00:01:17,880
If I dig into cable this is one example of it whereby everyone say is one of the other reviewers as

15
00:01:17,880 --> 00:01:20,170
mentioned that after watching just added a lover.

16
00:01:20,370 --> 00:01:26,930
Then you actually have the same core labelling whether it is positive or negative.

17
00:01:27,210 --> 00:01:39,560
So essentially these data allows you to have the so-called perspective of written texts versus the label

18
00:01:39,560 --> 00:01:40,760
to go with it.

19
00:01:40,760 --> 00:01:49,700
With that what you can do is you have essentially a simple vies machine learning problem and the question

20
00:01:49,730 --> 00:01:59,090
then of course for most people will be that how do I actually provide that tags into the machine learning

21
00:01:59,090 --> 00:02:05,230
algorithm for it to decipher the meaning of those sentences or attacks or the words.

22
00:02:05,390 --> 00:02:08,230
The idea is fairly simple and then help here.

23
00:02:08,240 --> 00:02:09,160
This is essentially.

24
00:02:09,170 --> 00:02:13,820
This is essentially a natural language processing problem or door.

25
00:02:13,820 --> 00:02:19,440
It has been simplify in that somewhat the pre processing the steps has already been done for you.

26
00:02:19,520 --> 00:02:26,210
The word has already been converted into integer and so you wouldn't actually have that have to perform

27
00:02:26,210 --> 00:02:27,830
that additional step itself.

28
00:02:27,840 --> 00:02:33,380
But if you are given in totality just pure text and the label then what you will have had to do is that

29
00:02:33,410 --> 00:02:33,860
you can.

30
00:02:33,890 --> 00:02:43,160
You will have to make use of some of the libraries to it to actually convert the word into unique token

31
00:02:44,750 --> 00:02:45,050
right.

32
00:02:45,050 --> 00:02:52,040
Having said all that essentially what we are doing here is still what's called a binary classification

33
00:02:52,040 --> 00:02:53,150
problem.

34
00:02:53,150 --> 00:03:00,650
Of course the features are slightly different now the features we are using the word as of features

35
00:03:01,130 --> 00:03:07,820
and we are using the word as of course you can tell from this that is a really really long and the massive

36
00:03:08,240 --> 00:03:15,340
matrix and substantial and the size in terms of the features.

37
00:03:15,410 --> 00:03:22,280
Of course the target or the label is do very simple as just whether it is positive or negative 0 or

38
00:03:22,280 --> 00:03:23,440
1.

39
00:03:23,450 --> 00:03:29,600
You can read more about the MTV Movie Review sentiment classification that is said here is twenty five

40
00:03:29,590 --> 00:03:37,730
thousand rather than the full fifty thousand and let's run the library and import the data.

41
00:03:37,750 --> 00:03:41,280
So this is x2 farm that carries data says I am D.B..

42
00:03:41,770 --> 00:03:47,980
We're going to just limit the so-called training here so that it doesn't run for too long.

43
00:03:47,990 --> 00:03:53,720
Just make sure that you do have the GPL and also Python 3 running well and looking at fifty five thousand

44
00:03:54,110 --> 00:03:56,680
you can increase that more if you like.

45
00:03:56,720 --> 00:04:03,530
We are saying that we are really looking at the maximum number of features of only 5000 looking at the

46
00:04:03,530 --> 00:04:05,060
data itself.

47
00:04:05,060 --> 00:04:07,830
We have 25000 droves of data.

48
00:04:07,850 --> 00:04:14,240
If you look at the actual underlying data itself the first 200 is what you can see here is that these

49
00:04:14,240 --> 00:04:21,260
are or in numeric format which is what I mentioned before the words has already been converted to introduce

50
00:04:21,300 --> 00:04:28,680
or is basically a sequence of words and each integer represent a specific word and where you have 0

51
00:04:28,680 --> 00:04:34,400
1 1 0 stand for a negative review 1 stands for positive review.

52
00:04:34,400 --> 00:04:40,300
Again looking at the first 2 this is will be a positive review of the second entry will be what's called

53
00:04:40,330 --> 00:04:43,480
a negative review of course from time to time.

54
00:04:43,490 --> 00:04:49,250
You may want to dig in and have a look at the individual word and the actual index to go with it.

55
00:04:49,250 --> 00:04:50,270
This is rather long.

56
00:04:50,570 --> 00:04:57,400
I'm gonna hide it after that so he can see here sorted is 1 8 5 2 2.

57
00:04:57,680 --> 00:05:01,240
Plug in is 5 fifty two thousand two hundred and ninety four.

58
00:05:01,400 --> 00:05:03,370
Let me hide that.

59
00:05:03,380 --> 00:05:11,840
And of course you may want to be looking at it the other way because you all you have is the 16 here

60
00:05:11,840 --> 00:05:13,880
you say OK what the 16 stand for.

61
00:05:13,880 --> 00:05:19,790
Then you can actually look up the value and and the corresponding word to go with it.

62
00:05:19,790 --> 00:05:28,100
The next step that you might want to do is plug this in to pandas and have it sorted in numerical order

63
00:05:28,780 --> 00:05:31,930
for quick searching right.

64
00:05:32,120 --> 00:05:36,020
So you're going to run the this one more thing that we need to do.

65
00:05:36,020 --> 00:05:42,670
Is that because the the features are in the form of integers.

66
00:05:42,920 --> 00:05:52,580
What we want to do is want hot and coded to a matrix of 0 and 1 and 0 when no such word exist.

67
00:05:52,580 --> 00:05:59,510
1 when the word exists in the data itself that way we have the same length.

68
00:05:59,510 --> 00:06:08,060
That's the first thing and the second thing is that this is a lot more friendly for the algorithm to

69
00:06:08,060 --> 00:06:09,410
actually be trained.

70
00:06:09,410 --> 00:06:13,260
So essentially this is how we function.

71
00:06:13,370 --> 00:06:20,410
You provide the data and you define the maximum features which we defined previously as 5000.

72
00:06:20,420 --> 00:06:30,920
So we're going to basically start off with creating a dimension of 5000 zeros in a row of 0 and based

73
00:06:30,920 --> 00:06:34,340
on the length of data 25000 remember currently.

74
00:06:34,340 --> 00:06:44,430
So basically we're creating now a matrix of 25000 by 5000 25000 long rolls 5000 wide.

75
00:06:44,430 --> 00:06:47,040
In terms of the features itself.

76
00:06:47,980 --> 00:06:48,440
Okay.

77
00:06:48,450 --> 00:06:56,390
And we do that for training and tests and would do that and for why we're converting it into a float.

78
00:06:56,730 --> 00:06:57,630
Okay.

79
00:06:57,800 --> 00:06:58,010
Right.

80
00:06:58,010 --> 00:07:01,590
The important part the architecture itself.

81
00:07:01,590 --> 00:07:03,680
We have the first layer.

82
00:07:04,050 --> 00:07:06,790
It's a dense layer okay.

83
00:07:07,150 --> 00:07:08,850
Our input shape is 5000.

84
00:07:08,850 --> 00:07:17,820
It goes into the first layer which is actually a hidden layer cost the employer is 5000 rolls followed

85
00:07:17,820 --> 00:07:26,080
by first hidden layer is 16 neurons with activation is really second layer of hidden layer is also another

86
00:07:26,080 --> 00:07:26,840
realm too.

87
00:07:26,880 --> 00:07:32,020
And finally the output there is the sigmoid because we want it either 0 or 1.

88
00:07:32,100 --> 00:07:37,440
We're using this across the ground descend for lost function being binary cross entropy.

89
00:07:37,440 --> 00:07:43,240
Looking at this we just reset this restart the runtime.

90
00:07:43,240 --> 00:07:44,290
Yes.

91
00:07:44,290 --> 00:07:55,010
And then I'm gonna clear output and stop running from the top.

92
00:07:55,430 --> 00:07:56,990
OK.

93
00:07:57,300 --> 00:07:58,580
All right so just a touch.

94
00:07:58,600 --> 00:07:59,990
Quick notes here.

95
00:08:00,010 --> 00:08:02,530
What are the activation function that we're using here.

96
00:08:02,530 --> 00:08:05,190
We have really two as I mentioned before is nonlinear.

97
00:08:05,200 --> 00:08:11,490
So the neural network can learn non-linear feature now also will be a purely what's called Alien aid

98
00:08:11,640 --> 00:08:19,150
or linear or fine in nature which is not very useful because in the problem that we have is clearly

99
00:08:19,150 --> 00:08:20,620
non linear.

100
00:08:20,620 --> 00:08:27,040
So this is looking at the data here we have the employer followed by 16 neuron first hit in their second

101
00:08:27,050 --> 00:08:34,750
hidden area is also another 60 neurons and finally the output is is a sigmoid whether it's positive

102
00:08:34,750 --> 00:08:37,120
or negative review.

103
00:08:37,330 --> 00:08:44,230
Splitting the data again to train and validation fitting the model with 50 epochs and the batch update

104
00:08:44,710 --> 00:08:53,960
is 5 to off or let that run and we also provide the validation in terms of the tuple expel and wave

105
00:09:00,010 --> 00:09:05,290
ok I know that the training is done looking at the accuracy of training around eighty six to eighty

106
00:09:05,320 --> 00:09:08,150
eight and the validation is right.

107
00:09:08,160 --> 00:09:10,440
Five to eighty six.

108
00:09:10,480 --> 00:09:12,250
Not too bad.

109
00:09:13,000 --> 00:09:15,550
Let's look at the performance.

110
00:09:15,790 --> 00:09:18,830
Plot you're not.

111
00:09:18,830 --> 00:09:23,830
Did you notice that the training lost continue to go down.

112
00:09:23,850 --> 00:09:29,010
You're starting to see the separation between the validation laws and also the training laws.

113
00:09:29,130 --> 00:09:38,760
They are some of a fitting going on here now and looks like our we need to actually work out how to

114
00:09:38,760 --> 00:09:46,620
actually solve this problem either by increasing the amount of data that we have or creating different

115
00:09:46,620 --> 00:09:48,540
features.

116
00:09:48,540 --> 00:09:53,410
The training and not the validation accuracy paint the same picture.

117
00:09:53,440 --> 00:10:01,740
Well just to summarize we've looked at the application of neural network and this lesson here on non

118
00:10:01,740 --> 00:10:09,030
numeric data which is text based data and basically came in the form of sentences with with the rating

119
00:10:09,030 --> 00:10:11,070
or target being positive or negative.

120
00:10:11,100 --> 00:10:13,930
That labelling part has to be done by human of course.

121
00:10:14,130 --> 00:10:22,780
The sentences each words has been converted and converted to a token and then we convert it into a matrix

122
00:10:22,780 --> 00:10:26,450
so that we can fit it into our neural network.

123
00:10:26,580 --> 00:10:29,730
And there's pretty much everything that we've covered.

124
00:10:29,730 --> 00:10:35,520
Of course the slight variation now is that we have a slightly more complex neural network with two layers

125
00:10:35,770 --> 00:10:36,440
meters high.

126
00:10:36,450 --> 00:10:41,280
This coming back to architecture neural network architecture.

127
00:10:41,280 --> 00:10:49,770
This is for a go with if you take out the input an output we have to hit in the years for this neural

128
00:10:49,770 --> 00:10:52,860
network and that's all for this lesson.

129
00:10:52,860 --> 00:10:53,280
Thank you.
