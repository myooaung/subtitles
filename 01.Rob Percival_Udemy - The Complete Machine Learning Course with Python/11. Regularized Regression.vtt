WEBVTT
1
00:00:01.950 --> 00:00:03.240
Good day everyone.

2
00:00:03.690 --> 00:00:09.670
We're going to venture into a very unique method but very popular as well.

3
00:00:09.930 --> 00:00:16.350
Sometimes it's not utilize enough I believe especially when you're dealing with linear regression that's

4
00:00:16.410 --> 00:00:23.050
not working with data that's normal at all and there's lots and lots of outliers.

5
00:00:23.130 --> 00:00:30.770
This method is called regularized methods for regression without going into a low in math.

6
00:00:30.780 --> 00:00:36.150
I just want to highlight three different types of regularization method and when to use them.

7
00:00:36.150 --> 00:00:39.800
And I think that's probably as deep as I want a goal.

8
00:00:40.080 --> 00:00:44.980
The Times pass I have a tendency to probably explore the math part of it.

9
00:00:45.060 --> 00:00:46.470
I'm a math geek.

10
00:00:46.530 --> 00:00:47.890
What do you do.

11
00:00:48.240 --> 00:00:52.470
And that probably 10 a fair few people off.

12
00:00:52.870 --> 00:00:53.610
I don't want to do that.

13
00:00:53.610 --> 00:00:59.780
I want you to be able to make use of the method and not get bogged down by the details at all.

14
00:00:59.820 --> 00:01:07.170
Ultimately none of us really want to know how the car engine work and you know and the so-called percentage

15
00:01:07.200 --> 00:01:08.990
efficiency of it.

16
00:01:09.720 --> 00:01:11.700
And also what's burning.

17
00:01:12.190 --> 00:01:19.650
And IBM and all of the details most of them most of us just want to get in a car start the car and get

18
00:01:19.650 --> 00:01:23.990
me from point A to Point B that's really all that we want to know.

19
00:01:24.090 --> 00:01:33.720
So I'm going to focus here in this lesson on three race regression lease square shrink kitchen selection

20
00:01:33.750 --> 00:01:41.280
operator which is a really terrible name which is called Lysol or a SS or in the fast last one is called

21
00:01:41.280 --> 00:01:47.230
elastic net so that those are the three that I'm really going to just throw out in there.

22
00:01:47.280 --> 00:01:50.830
Let me just highlight the difference between rich regression and loss.

23
00:01:51.210 --> 00:01:55.920
What let Rich regression trying to do is minimize this term.

24
00:01:55.980 --> 00:01:57.020
Okay.

25
00:01:57.360 --> 00:02:00.290
This is what's core the square.

26
00:02:00.570 --> 00:02:02.580
Get to the bar of two.

27
00:02:02.610 --> 00:02:05.750
Okay so this is what we're actually trying to minimize here.

28
00:02:05.970 --> 00:02:06.520
It is okay.

29
00:02:06.600 --> 00:02:10.640
Also called the L to penalize method compared this to last.

30
00:02:10.640 --> 00:02:12.900
So notice there's no square.

31
00:02:12.990 --> 00:02:18.230
This is the L 1 penalize regularize.

32
00:02:18.840 --> 00:02:26.340
Whereas a elastic net combined the two which you have the square as well as no square which is it's

33
00:02:27.150 --> 00:02:29.420
fairly straightforward or highlighted here.

34
00:02:29.460 --> 00:02:33.850
Not exactly straightforward but you can see what I mean.

35
00:02:34.130 --> 00:02:36.420
K going in to T.

36
00:02:36.530 --> 00:02:39.450
Integration development pot.

37
00:02:39.780 --> 00:02:42.010
I want to show you the outliers okay.

38
00:02:42.030 --> 00:02:45.300
The actual impact of it not foam.

39
00:02:46.950 --> 00:02:48.340
Okay.

40
00:02:48.480 --> 00:02:55.320
And just illustrate you know if we just do a straight linear regression this is a simple linear regression.

41
00:02:55.470 --> 00:02:59.230
Okay I'm gonna add some outliers.

42
00:02:59.230 --> 00:03:01.900
If you look at the outliers that's been added here.

43
00:03:01.940 --> 00:03:11.370
Notice that the coefficient goes from zero point four seven to one point five the whole line has been

44
00:03:11.430 --> 00:03:18.090
tilted pushed down this way and push up this way because of this outlier here.

45
00:03:18.090 --> 00:03:26.040
Now if we look at rich regression it doesn't actually well it didn't it did more for us to move but

46
00:03:26.040 --> 00:03:28.770
not as bad as linear regression.

47
00:03:28.770 --> 00:03:35.610
If we come back to the fact of the outlier is from point 4 7 to 1.

48
00:03:35.790 --> 00:03:39.330
Okay not as bad as the one point five.

49
00:03:39.330 --> 00:03:39.570
Okay.

50
00:03:39.570 --> 00:03:40.920
So this is risk regression.

51
00:03:40.920 --> 00:03:49.800
So this this regularized method basically restrict the actual movement it doesn't allow it to flow so

52
00:03:49.890 --> 00:03:56.700
easily that way it's not so poorly or badly penalized by this outlier and this outlier.

53
00:03:57.890 --> 00:03:58.140
Okay.

54
00:03:58.140 --> 00:04:05.340
So let me just look at lasso last so likewise is very similar to rich regression.

55
00:04:05.340 --> 00:04:13.480
It wasn't it's still affected but not as badly affected as the linear regression on its own.

56
00:04:13.950 --> 00:04:20.700
So if we look at Elastic net and you will see that it is 60 it performed the best of the two.

57
00:04:20.700 --> 00:04:23.340
Basically it actually combines the two method altogether.

58
00:04:23.340 --> 00:04:29.190
So from point four seven is to a slightly affected point for seven point four nine for memory or moved

59
00:04:29.190 --> 00:04:32.950
to point seven four instead of one point five.

60
00:04:32.970 --> 00:04:40.020
So really in summary when should I use loss or ritual lasting and I've got this answer from Stack Exchange

61
00:04:40.020 --> 00:04:44.310
you can Google it and you will be able to actually get the answer as well.

62
00:04:44.310 --> 00:04:50.160
Now there is regression can zero out coefficients you either end up including all the fish coefficient

63
00:04:50.160 --> 00:04:57.240
in the model or none of them less so on the other hand does both permit shrinkage and variable selection

64
00:04:57.360 --> 00:04:58.230
automatically.

65
00:04:58.940 --> 00:04:59.740
Okay.

66
00:04:59.880 --> 00:05:05.160
And if some of your covariance are highly correlated you may want to look at Elastic net which is a

67
00:05:05.160 --> 00:05:15.820
combination of lasso as well as the bridge itself so I hope that actually provides some foundation for

68
00:05:15.820 --> 00:05:19.570
you to explore not necessarily answer all your questions.

69
00:05:19.720 --> 00:05:26.250
You might not even have a really deep understanding of the rage or loss or I highly doubt that you have.

70
00:05:26.410 --> 00:05:34.060
But at the very least I think this lesson hopefully will provide you with some exposure to understand

71
00:05:34.060 --> 00:05:39.970
that in the future if should you come across lots and lots of outliers you few that is a city having

72
00:05:39.970 --> 00:05:43.060
a major impact on your model.

73
00:05:43.150 --> 00:05:49.810
You can stop think about Hey Anthony mentioned that there is such a thing called regularized regression

74
00:05:49.810 --> 00:05:50.330
method.

75
00:05:51.850 --> 00:05:57.270
I can remember when there was rich or Lasso of elastic net.

76
00:05:57.370 --> 00:06:02.890
Let me go back to this lesson and dig into it a little bit more or if I can find this lesson anymore

77
00:06:02.920 --> 00:06:06.810
I can google rich or regularized regression.

78
00:06:06.880 --> 00:06:13.150
So if I have achieved that that's really what I want you to X to get out of this lesson.

79
00:06:13.150 --> 00:06:20.080
Not so much the intricacy of it but just develop enough intuition and understanding so that you remember

80
00:06:20.080 --> 00:06:27.550
there's such a thing called regularized regression to solve the problem of outliers problem with it.

81
00:06:27.640 --> 00:06:28.530
Thank you.

82
00:06:28.600 --> 00:06:29.730
Really appreciate that.

83
00:06:30.130 --> 00:06:35.980
You pay attention rights road is probably not easy and I know it's actually not easy to follow along

84
00:06:36.400 --> 00:06:43.300
all that math especially you are not mathematically inclined and I hope you know this lesson help you

85
00:06:43.330 --> 00:06:47.840
somewhat in solving some of the common problems in machine learning.

86
00:06:48.460 --> 00:06:49.200
Thank you so much.
