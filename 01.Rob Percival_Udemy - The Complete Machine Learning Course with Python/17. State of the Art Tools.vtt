WEBVTT
1
00:00:01.140 --> 00:00:02.390
Welcome back everyone.

2
00:00:02.670 --> 00:00:04.880
Good to see you here in this session.

3
00:00:04.890 --> 00:00:07.430
I'm going to talk about where to from here.

4
00:00:07.440 --> 00:00:13.890
It's been a really exciting journey so far and what we've done so far is that we've covered some of

5
00:00:13.890 --> 00:00:21.930
the state of the art models all the way including veggie which is very deep convolution all networks

6
00:00:21.930 --> 00:00:24.090
for large scale image recognition.

7
00:00:24.120 --> 00:00:26.600
In fact I do have a notebook in there.

8
00:00:26.610 --> 00:00:29.870
Call rest net 2016.

9
00:00:29.910 --> 00:00:40.050
Now the question that you're naturally going to ask is that what happened between 2016 and two now there's

10
00:00:40.050 --> 00:00:46.380
quite a lot of development and there are quite a lot of state of the art design and the CNN that's been

11
00:00:46.680 --> 00:00:47.650
created.

12
00:00:47.670 --> 00:00:51.000
I just want to give you a brief overview of it right.

13
00:00:51.030 --> 00:00:57.750
So if you look at this what you can see here is that you have top 1 top 1 accuracy.

14
00:00:57.780 --> 00:01:02.980
So the meaning the higher these are the better it is Inception is the high is followed by Inception

15
00:01:02.980 --> 00:01:08.320
be three resonate for 1 5 to resonate 1 to 1 and rest at 34.

16
00:01:08.330 --> 00:01:10.500
I encourage you to go and have a look at the rest.

17
00:01:10.530 --> 00:01:12.490
It won't be walking through that.

18
00:01:12.490 --> 00:01:15.770
That was on the feature extraction.

19
00:01:16.050 --> 00:01:17.370
The notebook is there.

20
00:01:17.400 --> 00:01:22.230
It's just that I haven't actually run walk through with with you a bit mainly because it's actually

21
00:01:22.230 --> 00:01:29.460
very similar to Jeju 16 and then you have the rest at 16 Google an ADD IN IT AND THEN YOU HAVE SOME

22
00:01:29.460 --> 00:01:35.200
OF THE ALEXA net over here the rigid you 16 that's the one that we spend a fair bit of time on.

23
00:01:35.670 --> 00:01:41.100
And one thing that this picture doesn't show is that as you move towards the right the length of time

24
00:01:41.100 --> 00:01:42.170
it takes to train.

25
00:01:42.180 --> 00:01:46.160
The model goes up substantially OK.

26
00:01:46.170 --> 00:01:51.600
So there are a couple of things that spin massive advancements.

27
00:01:51.610 --> 00:01:54.000
There's the fine tuning that works.

28
00:01:54.220 --> 00:01:57.660
There is also the adaptive learning rate.

29
00:01:57.670 --> 00:02:00.990
Now what exactly is fine tuning networks.

30
00:02:01.000 --> 00:02:07.720
What we've done so far is actually feature extraction we've left all the weights that's in the model

31
00:02:07.750 --> 00:02:10.120
untouched meaning we didn't actually touch it at all.

32
00:02:10.720 --> 00:02:17.110
But what you can do is actually start opening up the layers one by one for fine tuning.

33
00:02:17.110 --> 00:02:19.290
That's the basic idea there.

34
00:02:19.300 --> 00:02:22.320
What about adaptive learning rate so far.

35
00:02:22.360 --> 00:02:25.300
We carers does allow you to change your learning rate.

36
00:02:25.300 --> 00:02:31.360
What we've done so far is that we just use a pretty standard learning rate and we didn't actually alter

37
00:02:31.360 --> 00:02:32.770
it that.

38
00:02:33.160 --> 00:02:39.860
And one of the ways that you can actually do to this it's a more advanced technique is it which require

39
00:02:39.860 --> 00:02:45.860
a lot more babysitting of your model is to change your learning rate as to accuracy you start to get

40
00:02:45.860 --> 00:02:52.160
better and better and you ought to and should slow down your learning rate so that it doesn't shoot

41
00:02:52.220 --> 00:02:52.820
overshoot.

42
00:02:52.820 --> 00:02:56.110
The the the so.

43
00:02:56.720 --> 00:02:59.570
I do or the lowest loss.

44
00:02:59.570 --> 00:02:59.840
OK.

45
00:02:59.860 --> 00:03:06.630
Because ultimately the whole or the models are looking at the lowest loss or minimize the error.

46
00:03:06.650 --> 00:03:10.920
There are a lot of advancement that's been made on the optimization model.

47
00:03:11.060 --> 00:03:16.870
The one that we talked about a little bit is est JD or stochastic gradient descent.

48
00:03:16.940 --> 00:03:22.970
We use our immense prop quite a fair bit although I didn't dig into it and explaining what they are

49
00:03:23.480 --> 00:03:31.590
for now is sufficient for you to actually just use the model to actually train your neural network there

50
00:03:31.590 --> 00:03:35.130
are some more advanced or state of the art architecture.

51
00:03:35.130 --> 00:03:42.960
We talked about video net rather than just rather than just using built it up from ground up.

52
00:03:43.110 --> 00:03:49.220
You have to Alex that you have the Google cool on it which is also called Inception and rest on it.

53
00:03:49.380 --> 00:03:56.960
So the picture source is so is an analysis of deep neural network models for practical application.

54
00:03:56.970 --> 00:04:03.310
So please do look up that if you want to X do you find out a little bit more.

55
00:04:03.630 --> 00:04:08.730
There are some would say of the art tools that I just want to highlight so that you can actually look

56
00:04:08.730 --> 00:04:15.390
them up to all these UN endorsement is just really to highlight to you that they they do exist and do

57
00:04:15.390 --> 00:04:21.580
have to look at them some of them does make a life a lot easier and so it will help immensely.

58
00:04:21.600 --> 00:04:26.420
So H2O deep condition meaning you don't actually have to code at all.

59
00:04:26.430 --> 00:04:33.200
You can actually start building a neural network without writing code as we did so far.

60
00:04:33.310 --> 00:04:37.210
Robyn miner is another one where it is completely gooey based.

61
00:04:37.290 --> 00:04:40.480
You have nine hour tricks and data robot.

62
00:04:40.530 --> 00:04:45.750
There are some compute free compute that is provided that you can actually make use of.

63
00:04:46.080 --> 00:04:49.830
If you don't have G.P. you and you want to actually expedite your training process.

64
00:04:49.830 --> 00:04:51.980
You can use Google Colette.

65
00:04:52.110 --> 00:04:59.820
Okay collab research Cagle as well as notebooks dot as you read that I'm going gonna stop this video.

66
00:04:59.820 --> 00:05:04.390
Thank you once again for watching and I look forward to seeing you in the future lessons.
