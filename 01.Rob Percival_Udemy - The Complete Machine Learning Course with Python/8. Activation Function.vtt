WEBVTT
1
00:00:00.270 --> 00:00:01.350
Welcome back everyone.

2
00:00:01.350 --> 00:00:08.100
And this segment we're going to look at the some of the common activation function.

3
00:00:08.130 --> 00:00:11.900
A lot of these should be quite familiar to you.

4
00:00:12.210 --> 00:00:14.540
In some ways hopefully.

5
00:00:14.940 --> 00:00:17.530
Let's start with the sigmoid function.

6
00:00:17.640 --> 00:00:30.150
And notice that the squeeze the output to 0 and 1 2 between 0 and 1 and sigmoid is quite often use in

7
00:00:30.210 --> 00:00:39.960
binary classification tangent Hage on the other hand squeeze the output to between minus 1 and 1 and

8
00:00:39.960 --> 00:00:45.540
read Lou for those who come from the finance background we'll look at this and say That looks a lot

9
00:00:45.540 --> 00:00:48.690
like a call option where you are indeed right.

10
00:00:48.720 --> 00:00:56.710
It basically looks exactly like a call option with the strike of 0 any thing less than zero.

11
00:00:56.750 --> 00:01:04.520
It would just convert the value to zero in terms of the output enough to that just increase in a gradual

12
00:01:04.520 --> 00:01:06.650
manner leaky really.

13
00:01:06.680 --> 00:01:14.240
On the other hand noticed that there are some negative values being like double quote through and followed

14
00:01:14.240 --> 00:01:16.110
by E. Lou.

15
00:01:16.130 --> 00:01:20.220
Okay exponential linear unit.

16
00:01:20.450 --> 00:01:32.200
And let's look at some example in terms of the mathematical formulation is equal to Max bracket 0 or

17
00:01:32.200 --> 00:01:32.920
b 3.

18
00:01:32.930 --> 00:01:39.740
So anything less than zero it will default to zero F is greater than zero.

19
00:01:39.760 --> 00:01:42.640
It will be the value in B3.

20
00:01:43.020 --> 00:01:45.940
So that's basically how this actually works.

21
00:01:45.940 --> 00:01:51.460
So as you move along that's basically the the actual.

22
00:01:51.610 --> 00:01:51.910
Sorry.

23
00:01:52.060 --> 00:01:58.160
Well the function is being applied to each and every one of these scalar value here.

24
00:01:58.360 --> 00:02:07.140
Sigma is the same in terms of the formula is one divided by one plus exponent of minus X..

25
00:02:07.370 --> 00:02:15.990
So that's essentially the sigmoid function and Rallo on the other hand applies the same.

26
00:02:15.990 --> 00:02:17.730
And then you have leaky value.

27
00:02:17.800 --> 00:02:19.750
Where would you saw led by.

28
00:02:19.830 --> 00:02:20.120
Really.

29
00:02:20.140 --> 00:02:21.720
So let's get lucky.

30
00:02:21.950 --> 00:02:24.810
The liqueur low has zero point one.

31
00:02:24.840 --> 00:02:30.860
Multiply by the X input so that this is the leaky part.

32
00:02:30.990 --> 00:02:36.010
But the point one is up to you to set quite often is being set to zero point one.

33
00:02:36.090 --> 00:02:39.080
You can't actually have lower value if you want to.

34
00:02:39.150 --> 00:02:40.880
And the basic idea here.

35
00:02:40.890 --> 00:02:47.500
So that there are some value being leaked through here in this spreadsheet here.

36
00:02:47.910 --> 00:02:51.180
I provided all the pictures.

37
00:02:51.310 --> 00:02:57.510
Well if you're 1 black and white this that one if you want color is that one you can actually go in

38
00:02:57.510 --> 00:03:03.000
and look at how I actually calculate all of these the like you really do is provided here as well.

39
00:03:03.000 --> 00:03:05.600
And then as far as the EU.

40
00:03:05.780 --> 00:03:06.080
Okay.

41
00:03:06.110 --> 00:03:09.210
So with that that's really what I wanted to highlight.

42
00:03:09.270 --> 00:03:10.930
Thank you again for watching.

43
00:03:10.930 --> 00:03:16.260
I hope this really quick run through gives you a sense of it.

44
00:03:16.260 --> 00:03:18.470
I won't go into the equation itself.

45
00:03:18.480 --> 00:03:25.050
You can do you find that quite easily just doing a simple googling Deep Learning activation function

46
00:03:25.080 --> 00:03:29.970
and then type the one that you're looking forward to elude like you will or rarely do.

47
00:03:30.540 --> 00:03:38.850
But the basic idea here is to look at the output function the output of the activation function and

48
00:03:38.850 --> 00:03:48.390
get some sense of what these functions attempt to do with the with your input and what this does is

49
00:03:48.390 --> 00:03:56.250
actually it's expedite the process and actually extract how you two actually converted to zero if it's

50
00:03:56.250 --> 00:04:05.610
negative so that the delay that's being applied to your input data later on can perform the computation

51
00:04:05.610 --> 00:04:11.880
better and some of these in particular are really which is extremely popular is used quite a lot more

52
00:04:11.880 --> 00:04:19.320
now because it actually allows brought back propagation to be calculated quickly instead of using the

53
00:04:19.320 --> 00:04:23.040
original tangent hedge right.

54
00:04:23.070 --> 00:04:29.530
A little bit more on the activation function if we pass the output layer as shown on the right.

55
00:04:29.560 --> 00:04:32.020
True Value activation function.

56
00:04:32.050 --> 00:04:33.060
So the output layer.

57
00:04:33.210 --> 00:04:37.020
This one here you will obtain the same result.

58
00:04:37.410 --> 00:04:46.380
That's because red blue is how Rela works is there is zero any negative numbers and leave positive numbers

59
00:04:46.470 --> 00:04:51.440
alone usually follow straight after convolution or layer.

60
00:04:51.690 --> 00:04:56.330
Often that is not specified in the architecture itself.

61
00:04:56.430 --> 00:05:01.290
If we go back to the carers let me just see if I can go back here.

62
00:05:01.290 --> 00:05:03.670
This one here.

63
00:05:03.800 --> 00:05:05.900
Notice these are the activation.

64
00:05:06.120 --> 00:05:09.700
Is it good to read though you can specify others if you want to.

65
00:05:09.810 --> 00:05:15.990
But in essence this is actually straight after the convolution there is usually not treated as a separate

66
00:05:15.990 --> 00:05:23.330
layer it's just ask the output that comes out the convolution layer below function is applied to it.

67
00:05:23.340 --> 00:05:38.260
You'll notice that in this summary of the CNN architecture there's no mention of red blue.

68
00:05:39.210 --> 00:05:40.670
OK.

69
00:05:41.340 --> 00:05:41.890
Right.

70
00:05:41.990 --> 00:05:45.530
These are the are the so-called activation function.

71
00:05:45.530 --> 00:05:52.230
This are the output plus the activation function with the different values that come out of it.

72
00:05:52.250 --> 00:05:54.830
I've given a couple examples here.

73
00:05:54.830 --> 00:05:57.530
The first one is the one that you've seen earlier.

74
00:05:57.560 --> 00:06:03.030
The second one is to illustrate to you when you feet negative value into value you get 0 0.

75
00:06:03.050 --> 00:06:05.450
Because they are negative values here.

76
00:06:05.600 --> 00:06:11.900
If you apply these two sigmoid a squeeze it two between 0 and 1 and you've if you apply this to lick

77
00:06:11.900 --> 00:06:12.720
you reload.

78
00:06:12.760 --> 00:06:15.870
This is actually the output that you see.

79
00:06:16.080 --> 00:06:20.320
No no longer does it actually convert it to 0.

80
00:06:20.390 --> 00:06:26.870
It does allow some of the negative values being leak through.

81
00:06:27.050 --> 00:06:27.360
All right.

82
00:06:27.360 --> 00:06:32.190
That's pretty much what I wanted to cover when it comes to activation function.

83
00:06:32.400 --> 00:06:35.520
When we come back we can look at the pulling layer.
