WEBVTT
1
00:00:00.420 --> 00:00:01.740
Welcome back.

2
00:00:01.740 --> 00:00:08.970
Well in this lesson we're going to look at what we've learned so far and look at how does the neural

3
00:00:08.970 --> 00:00:11.940
networks get trained.

4
00:00:11.940 --> 00:00:20.520
Now the key thing that we cover so far is that machine learning is really having an input as well as

5
00:00:20.670 --> 00:00:21.510
a target.

6
00:00:21.510 --> 00:00:31.380
And the actual algorithm itself learned the so-called patterns and the statistical representations that

7
00:00:31.380 --> 00:00:34.480
link the input to the target.

8
00:00:34.630 --> 00:00:41.280
Now the key thing is about finding the question marks here to link the inputs to targets in order for

9
00:00:41.310 --> 00:00:48.060
the algorithm to learn neural network to learned is you do need to provide many examples in order to

10
00:00:48.270 --> 00:00:53.280
provide to allow it to learn the patterns from the previous lives.

11
00:00:53.280 --> 00:00:56.640
We can see that there are many layers to deep learning neural network.

12
00:00:56.660 --> 00:01:03.320
Now this layers learn the relationship between the input and target from being exposed to many examples.

13
00:01:03.510 --> 00:01:07.140
Hence the picture there illustrate.

14
00:01:07.140 --> 00:01:16.320
Now the question that naturally arose arise is how does the algorithm learn.

15
00:01:16.320 --> 00:01:22.100
We cover this but I just want to show you another different picture.

16
00:01:22.110 --> 00:01:25.590
Now we've looked at this from a slightly different perspective.

17
00:01:25.590 --> 00:01:31.680
We've seen it from inputs and we also look at it from outputs.

18
00:01:31.680 --> 00:01:34.590
In this case we call it predictions.

19
00:01:34.590 --> 00:01:40.690
Now in between are the layers that we discovered or we explored earlier.

20
00:01:40.740 --> 00:01:44.030
The difference now is that there are these two elements here.

21
00:01:44.160 --> 00:01:48.720
These two elements are what determines the score.

22
00:01:48.810 --> 00:01:56.760
The learned contents of the neural network we will go into a little bit more of these in depth a little

23
00:01:56.760 --> 00:02:07.160
later but for now just understand that these are basically the actual input with the weights and and

24
00:02:07.180 --> 00:02:11.580
in order to get the actual so-called predicted output.

25
00:02:11.620 --> 00:02:14.590
Now this is what is also called feed forward.

26
00:02:14.590 --> 00:02:23.050
Let me just highlighted this way and I'm going to give you an example as an illustration right.

27
00:02:23.060 --> 00:02:27.280
They just have a pen and we use blue.

28
00:02:27.610 --> 00:02:32.480
Let's just say we have an output of one point two.

29
00:02:32.530 --> 00:02:32.770
Okay.

30
00:02:32.800 --> 00:02:35.200
So that's the target output now.

31
00:02:35.230 --> 00:02:38.350
And let's make our life a little bit easier.

32
00:02:38.380 --> 00:02:48.140
The input ledger say is zero point two and let's just say the so-called weight that is given here along

33
00:02:48.200 --> 00:02:55.680
this level all this layer is zero point one and four this is zero point two.

34
00:02:55.680 --> 00:03:05.270
Okay assuming that we are when we are looking at these neural network to the so core provided input

35
00:03:05.270 --> 00:03:07.490
here is the just say

36
00:03:16.330 --> 00:03:24.300
if we look at a simple example of let's just say choose a pen here.

37
00:03:24.540 --> 00:03:30.120
The input is zero point two and then we have a we'd of just say one to begin with.

38
00:03:30.540 --> 00:03:37.080
So when you multiply the two together what you get is zero point two and then just say you have another

39
00:03:37.080 --> 00:03:43.340
weights here the output has zero point two that you say we don't actually have an activation layer.

40
00:03:43.370 --> 00:03:50.280
So ignore what I just said in terms of the details we don't have this level let's just say it's point

41
00:03:50.280 --> 00:03:51.610
two straightforward.

42
00:03:51.660 --> 00:03:53.740
So the point to get fed forward.

43
00:03:53.850 --> 00:04:01.920
So two times one point two two times zero point two gives you one point two and let's just say the so-called

44
00:04:07.460 --> 00:04:15.940
the actual value that we are looking and aiming for is let us assume that is actually 2.0.

45
00:04:15.950 --> 00:04:17.810
So this is the actual value.

46
00:04:20.860 --> 00:04:28.690
So right now based on the weights of 1 and 2 here let's just call this weights of 1 and the weights

47
00:04:28.690 --> 00:04:31.230
of 2 is zero point two.

48
00:04:31.240 --> 00:04:37.690
How then do we learn get the neural network to learn this so that it actually achieve this zero point

49
00:04:37.690 --> 00:04:39.410
two.

50
00:04:39.420 --> 00:04:42.290
Now we know that the input is fixed.

51
00:04:42.390 --> 00:04:47.550
This value is determined by the weight as is this value is determined by the weight.

52
00:04:48.150 --> 00:04:53.490
So the key difference now really comes down to the actual weights itself.

53
00:04:53.550 --> 00:04:59.260
Indeed for neural network the weights are the actual memory of the neural network.

54
00:04:59.370 --> 00:05:05.450
They are the key for what is being learned by the neural networks.

55
00:05:05.460 --> 00:05:11.010
Just as the picture that we saw earlier in terms of the filters the different futures let me go back

56
00:05:11.720 --> 00:05:14.130
to these pictures.

57
00:05:14.130 --> 00:05:19.330
These are the weights the actual weights that's learned by the neural network layer.

58
00:05:19.350 --> 00:05:26.670
The difference is that we instead of scalar value which is like what we given here point to a 1.0 what

59
00:05:26.670 --> 00:05:35.610
we see here is a matrix of numbers which when you look at it has a seems to have a pattern that part

60
00:05:35.610 --> 00:05:38.910
of it is because our eyes are always looking for pattern.

61
00:05:39.120 --> 00:05:40.630
So let's come back here.

62
00:05:40.860 --> 00:05:47.220
Now the question is how do we actually get the neural network to learn such that it will eventually

63
00:05:47.220 --> 00:05:50.390
achieve a good accuracy.

64
00:05:50.640 --> 00:05:54.050
And that's where I'd like to introduce you to this picture here

65
00:05:57.230 --> 00:06:08.120
the picture that you saw earlier are these portion OK so you looked at this portion so far you have

66
00:06:08.120 --> 00:06:13.640
the input come to the neural network layer and the weights to it and then another neuron lower layer

67
00:06:13.790 --> 00:06:20.470
and then multiplied the weights and gives you the prediction on this site we have the actual y and decide

68
00:06:20.540 --> 00:06:25.470
you have the prediction of course unless you are lucky.

69
00:06:27.190 --> 00:06:31.400
It's not very likely that the first time round when the neural networks see the data it's going to be

70
00:06:31.400 --> 00:06:33.010
able to predict exactly.

71
00:06:33.110 --> 00:06:35.960
So there will always be some difference.

72
00:06:35.960 --> 00:06:41.780
And this is where I'd like to introduce to you what is called a lost function which measure the difference

73
00:06:41.780 --> 00:06:44.630
between actual and predictions.

74
00:06:44.690 --> 00:06:47.570
And this is what is core the lost function measure.

75
00:06:47.570 --> 00:06:56.740
The actual difference and the score is the output now in terms of the scores concern it tells you what

76
00:06:56.740 --> 00:07:04.330
your errors are meaning how far away is your model from providing a good prediction that still doesn't

77
00:07:04.330 --> 00:07:08.650
solve our problem of how to actually improve the model.

78
00:07:08.650 --> 00:07:15.970
And that's where the optimizer comes in to optimizer is the algorithm that actually feedback to change

79
00:07:16.000 --> 00:07:17.870
the weight for both.

80
00:07:18.130 --> 00:07:24.730
For all of the ways in each of the layers so that ended up closing the whole loop by itself.

81
00:07:24.760 --> 00:07:28.360
So you have the this portion here is called feed forward

82
00:07:31.380 --> 00:07:36.320
okay FTD EDF or Ardi f f or RWA R D.

83
00:07:36.330 --> 00:07:37.230
So feed forward.

84
00:07:37.230 --> 00:07:45.120
So this is part is the feed forward in terms of the poor writing the data and the actual prediction.

85
00:07:45.120 --> 00:07:57.010
Now the portion where it goes back to actually improve using the optimizer is core the back propagation.

86
00:07:57.750 --> 00:08:06.850
And the in order for it to continuously improve you need the error function or loss function to measure

87
00:08:06.850 --> 00:08:07.730
the error.

88
00:08:07.810 --> 00:08:14.770
And this is the actual value loss score is the actual value and tells you the distance between your

89
00:08:14.770 --> 00:08:17.760
actual value and also the prediction.

90
00:08:17.770 --> 00:08:21.250
So that's basically how a neural network learn.

91
00:08:21.310 --> 00:08:28.790
And it's actually very mathematical in that sense we will actually go through an example.

92
00:08:28.930 --> 00:08:34.450
That example will make use of quite a fair bit of math if you want to skip that.

93
00:08:34.450 --> 00:08:35.710
That's fine.

94
00:08:35.710 --> 00:08:42.230
But in terms of the application of it you don't really need to understand the actual math of it.

95
00:08:42.220 --> 00:08:48.180
But the math part I will provide nevertheless just so that to get off for those who are curious who

96
00:08:48.180 --> 00:08:54.830
wants to learn how the actual number actually works.

97
00:08:55.020 --> 00:08:55.260
Right.

98
00:08:55.290 --> 00:09:01.050
So the last slide down to cover and this lesson really is shallow vs. Deep Learning shallow learning

99
00:09:01.080 --> 00:09:07.560
is the Costco machine learning such as your linear regression logistic regression even your support

100
00:09:07.560 --> 00:09:11.790
vector machine and also decision trees.

101
00:09:11.790 --> 00:09:14.360
Those are all shallow machine learning.

102
00:09:14.610 --> 00:09:23.830
It's what we call the way that it learns is it learned greedily meaning in successive manner it's stacked.

103
00:09:23.850 --> 00:09:31.680
Now if you stepped out of the shallow machine learning algorithm you may think that it will actually

104
00:09:31.740 --> 00:09:38.280
achieve the same as how a neural network will perform because it does seem like that if you stack them

105
00:09:39.690 --> 00:09:43.180
it does seem like that if you stack them just come back here.

106
00:09:43.560 --> 00:09:45.460
If you stack them it looks like that.

107
00:09:45.630 --> 00:09:48.060
It will look like a normal neural network.

108
00:09:48.300 --> 00:09:53.820
But the truth is that is not the same because stacked stacking multiple shell learning together will

109
00:09:53.820 --> 00:09:55.500
not achieve the same result.

110
00:09:55.500 --> 00:10:01.380
The reason being the shallow machine learning learn independent from each other.

111
00:10:01.770 --> 00:10:08.580
Even if you stack them and successive layers they will learn on their own whereby deep learning the

112
00:10:08.580 --> 00:10:12.000
layers learn the patterns jointly.

113
00:10:12.080 --> 00:10:19.080
Okay so that means they can actually learn much more complex structure and patterns from the underlying

114
00:10:19.080 --> 00:10:20.340
data itself.

115
00:10:20.340 --> 00:10:25.820
Then lies the difference between shallow machine learning and deep learning.

116
00:10:25.820 --> 00:10:28.550
With that I'm going to stop here in this lesson.

117
00:10:28.550 --> 00:10:35.100
Basically we have cover really is we review what we've covered.

118
00:10:35.140 --> 00:10:41.620
We looked at how a neural network works the feet forward the measurement of the lost all the errors

119
00:10:41.980 --> 00:10:49.330
and also using optimizer to continuously improve the learning improved the machine learning algorithm

120
00:10:49.570 --> 00:10:55.060
and the accuracy of it tuning the weight so that becomes more and more accurate and that's the basic

121
00:10:55.060 --> 00:10:59.440
idea of how neural network actually learn.

122
00:10:59.440 --> 00:11:01.660
And once again thank you for watching this video.

123
00:11:01.660 --> 00:11:04.030
I look forward to seeing you in the future.
