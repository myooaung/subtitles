WEBVTT
1
00:00:01.790 --> 00:00:02.900
Welcome back everyone.

2
00:00:02.900 --> 00:00:11.000
In this segment of the training we're going to start looking at very simple new rent work.

3
00:00:11.030 --> 00:00:16.610
This is really to get you started on the different components of neural networks.

4
00:00:16.610 --> 00:00:19.970
And we will start with this very simple problem statement.

5
00:00:20.450 --> 00:00:23.720
So let me just hide the table content.

6
00:00:23.810 --> 00:00:27.190
You can bring it up using by clicking on that.

7
00:00:27.260 --> 00:00:32.190
Let me start with this really simple problem statement and then we've built on that.

8
00:00:32.570 --> 00:00:40.840
To begin with we have some really simple sample data generated by the following function.

9
00:00:40.850 --> 00:00:43.730
Why is a good two three x plus 50.

10
00:00:43.730 --> 00:00:48.330
And I hope that your high school math doesn't fail you.

11
00:00:48.470 --> 00:00:51.690
Why really is the dependent variable.

12
00:00:51.800 --> 00:00:56.960
Three is the slope and fifty is the intercept in machine learning.

13
00:00:57.080 --> 00:00:59.930
We also call it bias.

14
00:00:59.930 --> 00:01:04.010
Let me just bring it back to the simple function here.

15
00:01:04.010 --> 00:01:10.250
Let me just bring that up just so that you can see how this was actually calculated.

16
00:01:10.490 --> 00:01:10.930
Okay.

17
00:01:10.970 --> 00:01:11.640
Let me just.

18
00:01:12.640 --> 00:01:13.340
Right.

19
00:01:13.570 --> 00:01:13.820
Right.

20
00:01:13.820 --> 00:01:18.890
We have x value and the intercept is 50 right throughout.

21
00:01:18.890 --> 00:01:27.190
And the dependent variable or y that we are looking to predict as I mentioned the formula is Y is good

22
00:01:27.190 --> 00:01:37.600
at three x plus 50 and start to start off X is minus 20 multiplied by three gives you minus 60 plus

23
00:01:37.600 --> 00:01:39.560
50 gives you a minus 10.

24
00:01:39.730 --> 00:01:47.250
So on and so forth for all of these images bring that up hopefully that's a little bit clearer now that

25
00:01:47.250 --> 00:01:48.200
I've enlarged it.

26
00:01:49.120 --> 00:01:53.080
So essentially we created all of these data.

27
00:01:53.110 --> 00:02:04.000
So if you look across this I think as eight we will use the last row of data this raw data as the core

28
00:02:05.290 --> 00:02:10.590
value that we want the model to predict these will be the value that we will provide.

29
00:02:10.720 --> 00:02:20.010
Meaning X and also y were provided to our algorithm as the sample data.

30
00:02:20.110 --> 00:02:23.390
Now coming back to the core lab now.

31
00:02:23.450 --> 00:02:29.530
Of course in this case is a contrived example or one that we created.

32
00:02:29.620 --> 00:02:37.300
And with the exact data in real life however we have no knowledge of the real functions in our case

33
00:02:37.300 --> 00:02:43.140
we defined a function ourself in real life we don't have any knowledge of that.

34
00:02:43.190 --> 00:02:49.640
We are of course only have the data and the data that we usually see is the input data which is this

35
00:02:49.640 --> 00:02:50.960
column of value here.

36
00:02:50.990 --> 00:02:56.560
Let me just highlight in green and the so core corresponding dependent variable.

37
00:02:56.570 --> 00:03:02.750
So this is these are basically the only two columns of data that we have we basically have minus 20

38
00:03:03.050 --> 00:03:07.140
minus 10 minus five that 5 1 and 53.

39
00:03:07.190 --> 00:03:12.710
We only see the mapping of course the X could be many features.

40
00:03:12.710 --> 00:03:16.300
But let's just focus on one feature for now which is one x.

41
00:03:16.310 --> 00:03:17.480
It could be x two extra.

42
00:03:17.480 --> 00:03:18.660
In real life.

43
00:03:18.660 --> 00:03:24.220
So in real life we only have the data available to us.

44
00:03:24.770 --> 00:03:29.740
Of course in a situation when we know the real function you can just write the code for it.

45
00:03:29.770 --> 00:03:31.460
This nice and simple.

46
00:03:31.460 --> 00:03:34.410
But of course we don't.

47
00:03:34.460 --> 00:03:35.480
Okay.

48
00:03:35.960 --> 00:03:41.720
We have to estimate the function using the data given and that is really the challenge.

49
00:03:41.750 --> 00:03:46.180
And we are using machine learning in the specific case that we're doing here.

50
00:03:46.340 --> 00:03:51.130
Neural networks or deep learning to solve this problem.

51
00:03:51.200 --> 00:03:55.000
So let's go through these lumbar line which to start with.

52
00:03:55.000 --> 00:04:00.080
We import the libraries and before we do them.

53
00:04:00.110 --> 00:04:05.540
Let's just check our runtime is using Python 3 and GP you okay.

54
00:04:05.720 --> 00:04:09.840
So once we run that we basically need to import No I.

55
00:04:10.070 --> 00:04:11.310
And also tensor flow.

56
00:04:11.330 --> 00:04:13.840
We're using tensor floor 2.0.

57
00:04:15.080 --> 00:04:18.970
And next thing is the data preparing the data.

58
00:04:19.070 --> 00:04:28.970
The carriers which is what we actually using here under tends of flow expect the data to be in an array

59
00:04:28.970 --> 00:04:29.840
format.

60
00:04:29.930 --> 00:04:32.030
The NUM pi array format.

61
00:04:32.030 --> 00:04:38.590
And so we need to restructure these x and y pair into this format here.

62
00:04:38.720 --> 00:04:43.200
So X is none of my array and we actually set the data type as float.

63
00:04:43.310 --> 00:04:45.540
So let's just run that.

64
00:04:46.400 --> 00:04:51.620
Let's start with some really basic definitions that you might find useful.

65
00:04:51.630 --> 00:04:55.540
I talked about X just now in machine learning language or jargon.

66
00:04:55.550 --> 00:04:58.280
We call this feature.

67
00:04:58.280 --> 00:05:01.760
It's also sometimes called the independent independent variable.

68
00:05:01.760 --> 00:05:09.860
If you come from the statistics space it is sometimes also called the inputs and the Y is our dependent

69
00:05:09.860 --> 00:05:10.910
variable.

70
00:05:10.910 --> 00:05:13.100
Quite often it is called the label.

71
00:05:13.100 --> 00:05:20.390
Also the core targets and sometimes call the output to the actual model itself.

72
00:05:20.390 --> 00:05:26.570
So if you read these terminology don't get thrown off just understand that there are multiple names

73
00:05:26.570 --> 00:05:32.460
for the same thing and the symbol or example is really the pair.

74
00:05:32.650 --> 00:05:39.950
The pairing of the feature then the label or the X and Y provided to the machine learning algorithm

75
00:05:40.370 --> 00:05:42.500
during training.

76
00:05:42.500 --> 00:05:49.490
So in our situation here we have two four six seven pairs of feature as well as the label which we will

77
00:05:49.490 --> 00:05:56.810
feed to the machine learning algorithm to train it and hopefully it will be able to extract the pattern

78
00:05:57.260 --> 00:06:02.960
and be able to generalize and predict the answer right.

79
00:06:02.960 --> 00:06:05.550
The next step then is to define our model.

80
00:06:05.570 --> 00:06:09.380
We will just start very simply with just a dense layer.

81
00:06:09.380 --> 00:06:11.750
I don't explain what the dense layer means.

82
00:06:11.750 --> 00:06:20.540
In the next lecture and we will only use one neuron for this right in terms of the model.

83
00:06:20.550 --> 00:06:22.980
We do need to specify layers.

84
00:06:23.100 --> 00:06:28.060
OK I'll come back to that a little bit with the pictorial example.

85
00:06:29.470 --> 00:06:32.920
The step then first of all is to create an instance.

86
00:06:32.990 --> 00:06:42.500
Also core instantiate a layer by calling tensor flow TFC a shot for tensor flow dot terrorist dot layers

87
00:06:42.530 --> 00:06:51.350
dot dense and we also need to specify the input shape meaning the input data that we have we need to

88
00:06:51.350 --> 00:06:55.850
let us know what is the shape of it.

89
00:06:56.200 --> 00:06:56.480
Okay.

90
00:06:56.510 --> 00:07:03.140
Without specifying it carries will not know it we're just drawn on error so we do need to provide the

91
00:07:03.500 --> 00:07:04.330
information.

92
00:07:04.340 --> 00:07:12.400
What shape what dimension in our case there's only one single value of which is afloat and the units

93
00:07:12.400 --> 00:07:13.620
are equal to one.

94
00:07:13.760 --> 00:07:21.350
Specify that we are creating a neural network with only one neural OK one neuron.

95
00:07:21.350 --> 00:07:23.740
Sometimes this is called internal variable.

96
00:07:23.840 --> 00:07:31.400
Of course the more complex our problem is the more neuron that you need to specify in neuron can be

97
00:07:31.700 --> 00:07:33.800
with by with meaning.

98
00:07:33.800 --> 00:07:42.530
Each layer has however many that you define could be one to four and upwards or multiple layers meaning

99
00:07:42.590 --> 00:07:43.480
each.

100
00:07:44.410 --> 00:07:51.950
You can add one layer of five neurons or you can have three layers each having five neurons.

101
00:07:51.950 --> 00:07:56.960
So that really is defined using the units itself.

102
00:07:57.570 --> 00:08:01.430
That each the units define how many neurons in one day.

103
00:08:02.510 --> 00:08:10.100
Okay for our problem we will only make use of one layer the input shape is our first layer.

104
00:08:10.580 --> 00:08:10.920
Okay.

105
00:08:10.940 --> 00:08:16.850
So without specifying it as one we are defining our neural networks to take only a single value which

106
00:08:16.850 --> 00:08:21.630
is a floating point number again because this is our only layer.

107
00:08:21.680 --> 00:08:23.190
It is also I'll finally.

108
00:08:23.210 --> 00:08:25.880
The units then specify the number.

109
00:08:26.030 --> 00:08:32.520
Output from our numeric which is also a single value which is defined here is 1.

110
00:08:32.580 --> 00:08:35.180
Again this is a single float point number.

111
00:08:35.180 --> 00:08:42.170
Now you need to take care in a multi layer net was the size and shape of the output layer as defined

112
00:08:42.170 --> 00:08:43.910
by the units will need to match.

113
00:08:43.940 --> 00:08:52.460
So if you have multiple layers so when you go to layer two for example and layer 2 takes 3 as the input

114
00:08:52.460 --> 00:08:59.840
shape then your units here at the end of your layer 1 will also need to be 3 of course layer to follow

115
00:09:00.020 --> 00:09:02.110
what was defined in Layer 1.

116
00:09:02.210 --> 00:09:09.740
If Layer 1 specify the unit is 5 then your layer 2 input will needs to be the matching exactly the same

117
00:09:09.740 --> 00:09:10.160
as 5.

118
00:09:10.700 --> 00:09:13.970
So let's start with defining our layers.

119
00:09:13.970 --> 00:09:22.700
We are only we only have one layers and this layer we call Layer 1 layer underscore one tensor flow

120
00:09:22.720 --> 00:09:30.470
dot carers dot layers dot dense and the unit is equal to 1 and the input shape is equal to 1 as well.

121
00:09:30.710 --> 00:09:41.000
OK and we provide this OK as a pyramid to 2 tensor flow dot carers dot sequential and have that in a

122
00:09:41.000 --> 00:09:44.570
bracket and instantiate this and call this model.

123
00:09:44.570 --> 00:09:51.260
So I'll layer while we could lay underscore 1 and I'll model because the model again let's just run

124
00:09:51.260 --> 00:09:53.870
this okay.

125
00:09:53.900 --> 00:09:54.560
You can actually.

126
00:09:54.620 --> 00:09:56.570
There's two ways to define this.

127
00:09:56.570 --> 00:10:02.970
You can do it separately defining the layer separately from the model or you can actually have it inline

128
00:10:03.020 --> 00:10:09.830
like this tensor flow dot carries dot sequential like this part and the inside here you just typed out

129
00:10:09.830 --> 00:10:10.570
the whole thing.

130
00:10:10.580 --> 00:10:16.850
Tens of low dot carries dot layers dot tents dens defining the units as one and also input Shavers one

131
00:10:18.470 --> 00:10:19.390
all right.

132
00:10:19.400 --> 00:10:21.200
That's really defining it.

133
00:10:21.290 --> 00:10:28.350
Nothing has happened in the background so the next part then we use to actually compile our model in

134
00:10:28.390 --> 00:10:32.320
the compilation of our model we need to specify two things.

135
00:10:32.480 --> 00:10:40.080
One is a lost function the add on is the optimizer function the last function is a performance matrix.

136
00:10:40.430 --> 00:10:47.270
It is used to measure the errors of our model against the actual outcome it is also a core lost function

137
00:10:47.390 --> 00:10:51.760
because of the difference between our prediction and actual is called loss.

138
00:10:51.830 --> 00:11:00.380
So essentially the last function measure the scale or the difference between the prediction and the

139
00:11:00.380 --> 00:11:07.780
actual of course the smaller the better the because not ideal but the difference is really what's called

140
00:11:07.780 --> 00:11:15.760
the lost and hence we call this function to measure it called loss function and the optimizer function

141
00:11:15.880 --> 00:11:21.100
is the function or the mathematic function that is used to adjust the model Paramount is to improve

142
00:11:21.100 --> 00:11:23.170
prediction accuracy.

143
00:11:23.170 --> 00:11:32.390
Okay so that's the function that we use to adjust the internal value of our neural network so that the

144
00:11:32.400 --> 00:11:37.780
accuracy can be improved as time as the learning progresses.

145
00:11:37.800 --> 00:11:38.300
Right.

146
00:11:38.350 --> 00:11:44.350
There's one more thing that we need to define which is not a pyramid that is extra high per parameter.

147
00:11:44.440 --> 00:11:45.740
Let's call hyper pyramid.

148
00:11:45.790 --> 00:11:53.020
Because there's nothing to do with the actual machine learning model but it does have an impact on how

149
00:11:53.020 --> 00:11:55.760
fast or how slow our model learn.

150
00:11:55.990 --> 00:11:59.510
So that's where that's why it's core hyper parameter.

151
00:11:59.560 --> 00:12:06.910
It's due impact on the learning but is not one of the parameters of the actual machine learning model

152
00:12:08.800 --> 00:12:10.220
for the lending rate.

153
00:12:10.360 --> 00:12:13.180
You define it as zero point one.

154
00:12:13.180 --> 00:12:19.720
I've also tried zero point zero zero one which converge really slowly and insufficient in the time that

155
00:12:19.720 --> 00:12:25.960
we want to do zero point zero one is faster and zero point one is the fastest.

156
00:12:25.980 --> 00:12:26.220
Okay.

157
00:12:26.230 --> 00:12:27.460
So right.

158
00:12:27.490 --> 00:12:33.490
So in terms of the model model dot compile we need to provide as I mentioned before the lost function

159
00:12:34.570 --> 00:12:42.660
in this case we used to mean square error which we will discuss and let the lesson and for the optimizer

160
00:12:42.760 --> 00:12:48.750
we used to tend to flow carers optimizer the optimization algorithm.

161
00:12:48.760 --> 00:12:53.350
Call Adam in terms of the learning rate we provide at zero point one.

162
00:12:53.350 --> 00:12:56.860
So again let's run this and compile our model.

163
00:12:56.940 --> 00:13:03.700
I provided a bit more so-called links for you to refer to one is about selecting the right evaluation

164
00:13:03.860 --> 00:13:04.730
metric.

165
00:13:04.810 --> 00:13:12.310
It is dependent upon the nature of your problem whether it is a classification problem or whether it

166
00:13:12.310 --> 00:13:15.230
is a regression problem.

167
00:13:15.340 --> 00:13:22.520
So the metric that you use does matter and the Adam optimizes is one of the many optimization algorithm

168
00:13:22.520 --> 00:13:23.780
that you can use again.

169
00:13:23.790 --> 00:13:29.950
I'll provide a link here for you to explore a little bit more that's suitable for your problem.

170
00:13:31.570 --> 00:13:31.900
Alright.

171
00:13:31.930 --> 00:13:37.560
The next step then is to actually proceed to train our model in sensible Paris.

172
00:13:37.570 --> 00:13:41.480
The way to actually train it is core fit method.

173
00:13:41.530 --> 00:13:48.220
Now just remember that our actual model was why is good the three X plus 50 the slope history and intercept

174
00:13:48.300 --> 00:13:51.600
is the slope history and the intercept is 50.

175
00:13:52.210 --> 00:13:57.970
These are the two coefficients that we are asking the neural network to estimate because we didn't give

176
00:13:57.970 --> 00:14:03.490
it the actual formula and in real well we don't know the formula anyway so we need the actual machine

177
00:14:03.490 --> 00:14:08.260
learning model algorithm to actually estimate it for us.

178
00:14:08.260 --> 00:14:12.580
So the neural network what it does is that it starts with a random value since it has no idea where

179
00:14:12.580 --> 00:14:13.030
to start.

180
00:14:13.030 --> 00:14:20.260
Anyway so it just have to use a random value and this random value is also called weights and use it

181
00:14:21.310 --> 00:14:25.050
and the input y to compute x.

182
00:14:25.060 --> 00:14:25.310
Okay.

183
00:14:25.330 --> 00:14:31.540
So we will combine B and multiplication by multiplying x plus to buyers to compute y.

184
00:14:32.200 --> 00:14:37.000
So basically is using it in combination with the input barometer to calculate y.

185
00:14:37.840 --> 00:14:43.420
So because the weights start over the random value of course the prediction will start off with a large

186
00:14:43.420 --> 00:14:44.890
errors.

187
00:14:45.160 --> 00:14:46.280
That's the first then.

188
00:14:46.470 --> 00:14:52.810
Then what the algorithm does is that it takes the arrow value on the loss and then it calculated using

189
00:14:52.810 --> 00:14:55.450
the lost function or have one too many errors here.

190
00:14:59.280 --> 00:15:02.130
And the prediction and the actual value.

191
00:15:02.160 --> 00:15:05.900
So basically using lost function a calculate the lost value.

192
00:15:06.090 --> 00:15:13.290
Take into account of what the prediction is versus the actual value of the next step then is the optimizer

193
00:15:13.950 --> 00:15:15.340
then make an adjustment.

194
00:15:15.710 --> 00:15:16.050
Okay.

195
00:15:16.080 --> 00:15:21.660
Of course as this object to the lending rate that we defined okay and make the adjustment to the weight

196
00:15:21.900 --> 00:15:31.050
and the ultimate goal is to reduce the loss and each time that we allow the model to see all of the

197
00:15:31.110 --> 00:15:33.600
data set is core and epoch.

198
00:15:33.600 --> 00:15:42.090
Okay e.g. pot is the fourth iteration of the example that's provided or that we have and with each epoch

199
00:15:42.540 --> 00:15:50.940
the algorithm then start over again in terms of its learning therefore parameters that we need to provide

200
00:15:51.390 --> 00:15:58.530
for this fit method would need to provide our independent variable X our dependent variable which is

201
00:15:58.530 --> 00:16:06.810
what we were trying to predict Y and epochs specify sorry epochs specify how many cycles should be run

202
00:16:07.860 --> 00:16:12.840
and then we have another argument which has nothing to do with the model itself but more to do with

203
00:16:12.840 --> 00:16:21.210
the method that we're using is to output the so the results and display it onto the console.

204
00:16:21.210 --> 00:16:23.010
In this case we'll put false.

205
00:16:23.010 --> 00:16:25.880
So that we are not bombarded by the result.

206
00:16:26.190 --> 00:16:30.090
We're doing 1000 epochs and we need to provide the x and y.

207
00:16:30.090 --> 00:16:36.630
So let's just run this and store this into the results in the variable core history right.

208
00:16:37.510 --> 00:16:38.100
Okay.

209
00:16:38.250 --> 00:16:45.330
Right now having run that we have no idea whether it's done well whether the model is good or bad or

210
00:16:45.330 --> 00:16:46.870
how did the learning go.

211
00:16:46.920 --> 00:16:52.110
So what we do next is to actually visualize the model training.

212
00:16:52.110 --> 00:16:57.190
The good thing is that carers comes with the history as I mentioned before.

213
00:16:57.270 --> 00:17:04.020
You can actually make use of history to visualise our performance and what we usually do is that we

214
00:17:04.020 --> 00:17:12.790
measure the loss or the of magnitude and measure it or plotted against the number epochs that we run.

215
00:17:12.790 --> 00:17:13.070
Okay.

216
00:17:13.080 --> 00:17:19.500
And from there then we look at how the performance of the last curve looks like.

217
00:17:19.500 --> 00:17:27.000
So if you look at this it started off as I mentioned before because the ways the internal variable started

218
00:17:27.000 --> 00:17:29.550
off with random value.

219
00:17:29.670 --> 00:17:36.870
Of course the loss will be large as you start learning at jobs okay as we go through more and more epoch

220
00:17:37.080 --> 00:17:39.480
the actual loss drops.

221
00:17:39.480 --> 00:17:39.920
Okay.

222
00:17:39.940 --> 00:17:47.230
And basically what our algorithm has done is the learning curve is learning to minimize the loss and

223
00:17:47.530 --> 00:17:53.190
after adjusting the weight it continue until the losses start to actually taper off.

224
00:17:54.540 --> 00:18:01.110
Of course the lending is still got goes on but the point here is that our own this portion here about

225
00:18:01.230 --> 00:18:06.130
80 just shy of 100 most of the lending is done.

226
00:18:06.180 --> 00:18:15.780
Continued to learn and the next step then is for us to actually not just plot this the next step then

227
00:18:15.780 --> 00:18:23.560
is to actually look at the look under the hood in terms of the prediction recall.

228
00:18:23.700 --> 00:18:30.810
You probably want X was 100 when X was 100 the y value is 350.

229
00:18:31.050 --> 00:18:37.560
And if you look at we're providing X is 100 this input here and we're asking the model to perform a

230
00:18:37.560 --> 00:18:44.400
prediction it comes up with a value that's pretty close to a hundred and fifty one instead of 350.

231
00:18:44.400 --> 00:18:49.380
So the errors is actually very very narrow at this point.

232
00:18:49.590 --> 00:18:54.510
And if we actually look under the hood and look at the actual underlying weights I noticed that there

233
00:18:54.510 --> 00:18:56.340
are two variables here.

234
00:18:56.370 --> 00:18:57.920
Both of them are flawed.

235
00:18:58.020 --> 00:19:03.950
The first variable is our slope that we want to actually wanted it to to estimate comes up at three

236
00:19:03.950 --> 00:19:10.160
point 0 8 versus 23 which is really really close and 44 here versus 50.

237
00:19:10.170 --> 00:19:15.620
Again it's really close considering that we only really provided it with seven data points.

238
00:19:15.720 --> 00:19:23.790
It is able to actually come up to predict and also so I was able to estimate so closely.

239
00:19:23.790 --> 00:19:25.500
That's quite remarkable indeed.

240
00:19:25.510 --> 00:19:28.170
Even though there are not a lot of data for this.

241
00:19:28.590 --> 00:19:36.990
So that's really the basic that I want to explain to you in terms of what the machine learning model

242
00:19:37.020 --> 00:19:39.520
or the neural network that we've done here.

243
00:19:39.540 --> 00:19:43.890
The simple process here is to highlight to you how powerful it is.

244
00:19:43.890 --> 00:19:45.300
That's the first thing.

245
00:19:45.300 --> 00:19:53.880
Second thing is that it was able to actually estimate the view function really very closely.

246
00:19:53.880 --> 00:20:01.180
And the third thing that I want to highlight is that the in terms of the dense layer which is what we

247
00:20:01.180 --> 00:20:05.060
are going to talk about names we're only we don't even have multiple layers.

248
00:20:05.070 --> 00:20:06.780
We only have one dense layer.

249
00:20:07.110 --> 00:20:13.140
And from this that one single dense layer we were able to estimate the algorithm itself.

250
00:20:13.770 --> 00:20:14.040
OK.

251
00:20:14.070 --> 00:20:16.990
So we talk about the dense layer next.

252
00:20:16.990 --> 00:20:20.110
Finally with walk through the process.

253
00:20:20.180 --> 00:20:20.500
OK.

254
00:20:20.520 --> 00:20:25.440
From importing our libraries preparing the training data.

255
00:20:25.440 --> 00:20:26.000
All right.

256
00:20:26.100 --> 00:20:33.180
And defining our model in terms of the step by step process and after defining our model is the compilation

257
00:20:33.300 --> 00:20:38.670
providing the last function as walls to optimize the actual fitting or training it.

258
00:20:38.850 --> 00:20:46.110
And finally visualizing it to see how the learning has been has taken place or how it has done and a

259
00:20:46.110 --> 00:20:48.240
bit on the prediction side.

260
00:20:48.430 --> 00:20:54.060
And hopefully these five things that we've cover here has been helpful and I hope that you have a better

261
00:20:54.090 --> 00:21:02.190
and deeper understanding of the what we've done and yeah play around with it the particular the one

262
00:21:02.190 --> 00:21:05.850
part that I would like you to play with it is this learning right here.

263
00:21:05.910 --> 00:21:14.320
I had to do all of these and what I've noticed is that and this is not this no one bullet fits all.

264
00:21:14.370 --> 00:21:20.310
It's something that you have to actually experiment with to actually find what the ideal learning rate.

265
00:21:20.790 --> 00:21:22.290
So with that I'm going to stop.

266
00:21:22.320 --> 00:21:23.430
Thank you for watching.
