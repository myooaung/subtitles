1
00:00:01,260 --> 00:00:06,870
Good day everyone in this lesson we're gonna go on to a non-linear relationships once we get to this

2
00:00:06,870 --> 00:00:12,480
point whereby the underlying data is clearly not really really nonlinear.

3
00:00:12,500 --> 00:00:20,340
Now we have two weeks to start moving away from the simple ordinarily square or linear regression or

4
00:00:20,370 --> 00:00:23,810
the polynomial as best as we can.

5
00:00:23,820 --> 00:00:30,300
We tried to make use of those as well even as the baseline but once you actually stopped looking at

6
00:00:30,300 --> 00:00:36,660
data there's substantially more complex and a lot more interesting features.

7
00:00:36,660 --> 00:00:40,360
I suppose you can say it then we have to start looking at models.

8
00:00:40,490 --> 00:00:46,680
There are a lot more complicated and a lot more rich to describe and tried to model use use those to

9
00:00:46,680 --> 00:00:49,260
model our underlying data.

10
00:00:49,260 --> 00:00:56,440
The usual drill here is importing the libraries that we need for the purpose of our lesson.

11
00:00:56,970 --> 00:01:03,150
Loading the data have a preview of the data and just basically read our data.

12
00:01:03,720 --> 00:01:04,210
Okay.

13
00:01:04,370 --> 00:01:08,090
That is running in the background the first regress.

14
00:01:08,200 --> 00:01:11,490
Then we're going to make use of this court decision tree.

15
00:01:11,550 --> 00:01:20,520
This may not be very common knowledge to a lot of people decision trees aren't just purely for classification

16
00:01:20,520 --> 00:01:24,320
based machine learning.

17
00:01:24,320 --> 00:01:28,700
It can be used for regression base.

18
00:01:28,920 --> 00:01:34,830
A quick differentiation between regression versus classification really is that classification is often

19
00:01:36,240 --> 00:01:46,280
binary in nature or things are boxed into different classes apples orange dogs cats.

20
00:01:46,280 --> 00:01:52,730
Those are distinct classes on their own zero and one is obviously another example.

21
00:01:52,880 --> 00:01:59,750
Whereas if you're talking about regression it's the numbers that you're dealing with are continuous

22
00:01:59,780 --> 00:02:01,030
in nature.

23
00:02:01,220 --> 00:02:08,380
But the example we've been looking at so far is house price projection or the median value that we're

24
00:02:08,390 --> 00:02:08,990
looking at.

25
00:02:08,990 --> 00:02:09,560
Modeling.

26
00:02:09,580 --> 00:02:12,730
And with that you can see that the value is continuous.

27
00:02:12,720 --> 00:02:16,420
There isn't a distinct category to it.

28
00:02:16,640 --> 00:02:22,910
So that's the basic idea here is the psychic and the psychic then there's a decision tree library that's

29
00:02:22,910 --> 00:02:24,080
built in.

30
00:02:24,080 --> 00:02:30,160
And yes you can use decision tree except that you do need to import the decision tree regress.

31
00:02:31,310 --> 00:02:38,750
And we are looking specifically is just one of the feature in this case our state.

32
00:02:38,780 --> 00:02:46,460
I just want to use this as an illustration lesson for instructional purpose rather than for on model

33
00:02:46,460 --> 00:02:47,690
building.

34
00:02:47,690 --> 00:02:56,410
For now we need to understand what is it that we are working with the actual features and the star lies.

35
00:02:58,310 --> 00:03:05,030
Characteristics of Decision Tree regress rather than full on modelling of the underlying data.

36
00:03:05,410 --> 00:03:13,220
What are the so-called parameters that we need to specify is how deep or how big we want to build our

37
00:03:13,220 --> 00:03:14,410
tree.

38
00:03:14,450 --> 00:03:18,070
In this case we specify a maximum depth of five.

39
00:03:18,110 --> 00:03:22,700
We have no way of knowing whether that's too deep or not.

40
00:03:22,700 --> 00:03:30,650
We have to iterate this process until we develop some intuition of that underlying data.

41
00:03:30,770 --> 00:03:32,980
So let's try that and then fit it.

42
00:03:33,260 --> 00:03:34,090
Okay.

43
00:03:34,130 --> 00:03:35,730
These are the output.

44
00:03:35,750 --> 00:03:39,340
There are a lot of parameters that you can actually tune out for now.

45
00:03:39,410 --> 00:03:47,740
We just really want to make use of just one which is max depth and the next thing is just plot it.

46
00:03:48,350 --> 00:04:00,320
The blue dots are the underlying data and the black line is our predicted prediction using the decision

47
00:04:00,320 --> 00:04:02,010
tree.

48
00:04:02,480 --> 00:04:04,740
That's where the prediction part comes in here.

49
00:04:04,750 --> 00:04:13,700
No this function here is just really X is it comes in an array.

50
00:04:13,720 --> 00:04:19,000
What we wanted to do is just flatten it so that it becomes a while.

51
00:04:19,180 --> 00:04:24,730
Originally when it can come in in the form of vector we want to just flatten it so this is just a continuous.

52
00:04:24,730 --> 00:04:30,190
It's just an array and then next we want to do is actually just sorted from the lowest to the largest.

53
00:04:30,190 --> 00:04:35,110
Otherwise it gets a bit problematic when it comes to the plotting.

54
00:04:35,110 --> 00:04:37,150
So that's really the basic idea there.

55
00:04:37,240 --> 00:04:43,870
And as we actually performing the prediction this is the x value and this is the y value.

56
00:04:43,870 --> 00:04:52,090
And as you can see here is if you plot a linear regression it would have been like this and this is

57
00:04:52,090 --> 00:04:52,780
not what you will.

58
00:04:52,780 --> 00:05:04,080
Normally core normal model and you can see that is all over the place at this is what we call a telltale

59
00:05:04,080 --> 00:05:12,360
sign of the model of fitting we might have given it far too many degrees of freedom and is not fitting

60
00:05:12,360 --> 00:05:12,810
this way.

61
00:05:12,810 --> 00:05:17,740
And one other way to actually reduce that is just to change it to just say two.

62
00:05:18,450 --> 00:05:21,480
And you can see that is substantially better.

63
00:05:21,480 --> 00:05:22,930
But what might we have.

64
00:05:22,940 --> 00:05:28,000
What what what we might have done in doing that exercises that we might have restricted it so much.

65
00:05:28,020 --> 00:05:31,290
Kind of like a regular Rick.

66
00:05:32,400 --> 00:05:39,450
The regularize adjustment technique that we talked about earlier having is a bit tongue tied today.

67
00:05:40,760 --> 00:05:45,100
Just too restricted from being able to model the data too much.

68
00:05:45,170 --> 00:05:49,970
And three years seems OK but I just want to go back to five.

69
00:05:49,970 --> 00:05:56,540
Just to illustrate to you as you can see this is quite a lengthy process and one that you need to iterate

70
00:05:56,540 --> 00:06:00,950
through just so that you actually know underlie understand the model that you've built.

71
00:06:00,950 --> 00:06:05,930
So we've look at two we looked at three and looked at five and since those three is probably one of

72
00:06:05,930 --> 00:06:07,160
the best.

73
00:06:07,340 --> 00:06:11,580
And we also highlight this whole exercise here using two

74
00:06:15,560 --> 00:06:21,820
Decision Tree is one we can also look at another which is call random forest.

75
00:06:22,040 --> 00:06:31,030
We're going to make use of this model to begin with I'm splitting the data into train and test.

76
00:06:31,190 --> 00:06:38,620
I'll explain that in a minute and keeping 30 percent as our test and we input the random forest regress.

77
00:06:38,650 --> 00:06:40,660
This is an ensemble.

78
00:06:40,660 --> 00:06:46,060
It's a different type of model random forest technically is combinations of decision trees.

79
00:06:46,090 --> 00:06:49,870
It's not just one tree but many of them.

80
00:06:50,230 --> 00:06:57,820
And COBOL criterion is that we need to provide for now don't need to worry about these essentially just

81
00:06:57,860 --> 00:07:04,320
I'm saying look estimated 500 times using means great errors.

82
00:07:04,390 --> 00:07:09,760
The worry about the random state I just want to fix it so that I get the same output and also use all

83
00:07:09,790 --> 00:07:15,230
the all the threads my computer cores to.

84
00:07:15,240 --> 00:07:21,310
She does the computation because if you were working with lots of data and a forest is actually computationally

85
00:07:21,310 --> 00:07:29,130
quite intensive fit the data predict perform the prediction for training data and it perform the prediction

86
00:07:29,130 --> 00:07:37,430
for test data and look at the main square error kind of hard to tell whether that's good or bad.

87
00:07:37,560 --> 00:07:43,030
If you look at the ask where is ninety seven point eighty seven for the training and then in the whole

88
00:07:43,030 --> 00:07:45,670
of the sample the test is eighty seven percent.

89
00:07:45,870 --> 00:07:48,760
And if you look at the misery error in sample is one point eighty seven.

90
00:07:48,770 --> 00:07:50,670
And test is substantially worse.

91
00:07:50,790 --> 00:07:56,420
So that tells you as you know our models do not stay in our grade.

92
00:07:56,460 --> 00:08:01,570
There seem to be something else that's actually at play here.

93
00:08:02,850 --> 00:08:08,300
But ask where does tell you that it is an improvement or slightly somewhat better.

94
00:08:08,970 --> 00:08:13,380
And the next model that want to look at is score and a boost.

95
00:08:13,590 --> 00:08:16,380
Also an ensemble method.

96
00:08:16,410 --> 00:08:23,360
So let's run this and look at the direct comparison between the miss gray area as well so ask.

97
00:08:23,390 --> 00:08:30,470
You notice that the Atavus did not perform as well as the random forest.

98
00:08:30,810 --> 00:08:35,950
So in this case we may revert to random for us quite often in real life.

99
00:08:35,960 --> 00:08:43,680
You will still continue to run that at a boost on the side ESA check especially if it is useful to help

100
00:08:43,680 --> 00:08:50,850
you to evaluate random furrows especially if both models are deteriorating concurrently it tells you

101
00:08:50,850 --> 00:08:54,720
that something is changing the underlying data brought with it.

102
00:08:54,720 --> 00:09:01,530
I'm gonna stop and just to summarize the lesson here When we're dealing with non related non-linear

103
00:09:01,530 --> 00:09:09,210
relationships we may deviate from the normal linear regressions or polynomial regression model.

104
00:09:09,210 --> 00:09:17,220
When we start looking at Decision Trees random forests as well as a edibles and down more but that ultimately

105
00:09:17,220 --> 00:09:24,410
this is just to show you these models that you can estimate use Office for regression as well.
