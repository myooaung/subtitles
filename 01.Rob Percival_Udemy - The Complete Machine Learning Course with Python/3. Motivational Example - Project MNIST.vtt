WEBVTT
1
00:00:02.560 --> 00:00:06.830
And the previous lessons what we did was to look at densely.

2
00:00:06.940 --> 00:00:10.930
And we looked at a regression based problem.

3
00:00:10.960 --> 00:00:19.180
So in this lesson we're going to step aside and look at a classification problem and add a few new concepts

4
00:00:19.180 --> 00:00:19.600
onto it.

5
00:00:19.990 --> 00:00:23.260
So essentially we're going to cover we're really cover regression.

6
00:00:23.260 --> 00:00:26.450
So in this lesson we're going to cover classification.

7
00:00:26.530 --> 00:00:35.260
We've talked about another type of layer or method called flattening in the previous lesson we talked

8
00:00:35.260 --> 00:00:37.000
about a dense layer.

9
00:00:37.000 --> 00:00:46.300
We'll also talk about what's called an activation function value as well as soft Max will cover regularization

10
00:00:46.300 --> 00:00:48.420
method called dropout.

11
00:00:48.430 --> 00:00:49.660
And while we would do it.

12
00:00:49.990 --> 00:00:53.400
And finally training and testing as a concept.

13
00:00:53.590 --> 00:00:58.430
But we won't actually use it or looked at it in this lesson.

14
00:00:58.430 --> 00:01:00.700
But these are the basic concepts that were talked about.

15
00:01:00.820 --> 00:01:03.040
We will highlight as well.

16
00:01:03.220 --> 00:01:12.190
I want to just talk a little bit about the purpose of flattening in the data sets that we're going to

17
00:01:12.190 --> 00:01:15.140
be looking at it's core amnesty data set.

18
00:01:15.380 --> 00:01:20.030
It's provided and created by the queen.

19
00:01:20.350 --> 00:01:24.990
And you can actually follow this link along.

20
00:01:25.030 --> 00:01:31.720
It is core amnesty database is prepared as I mentioned by Yann McKeown as you basically have sixty thousand

21
00:01:31.720 --> 00:01:35.050
examples and ten thousand examples.

22
00:01:35.050 --> 00:01:42.360
And it's what it's called handwritten digits from 0 1 to all the way to 9.

23
00:01:42.730 --> 00:01:48.150
And it has a corresponding corresponding label to go with it as well.

24
00:01:48.150 --> 00:01:54.670
It has the image of the handwriting or the digits as well as the label itself.

25
00:01:54.670 --> 00:01:58.710
So these are all the different tests and that people must perform.

26
00:01:58.720 --> 00:02:03.710
You can also find this in cargo and we just go there.

27
00:02:03.700 --> 00:02:14.140
All right digits recognize it's very famous it's one that people use to actually used to be the Hello

28
00:02:14.150 --> 00:02:23.680
well nowadays is pretty much the the baseline for people to actually understand how it works anyway.

29
00:02:23.830 --> 00:02:25.660
So this is the digit recognize.

30
00:02:25.690 --> 00:02:32.920
So this is a good place to X to get started led you to read you read a little bit more about this and

31
00:02:32.920 --> 00:02:37.790
I'll come back to the actual color code itself right.

32
00:02:39.250 --> 00:02:42.470
So these are the concert we're going to cover so let's get started.

33
00:02:42.470 --> 00:02:48.670
For a start we will usually as we always do import the libraries the next part is that we do need to

34
00:02:48.670 --> 00:02:50.160
import the data.

35
00:02:50.200 --> 00:02:55.720
The good thing is that the data is provided in the carer's data sets as on the tensor floor carers data

36
00:02:55.720 --> 00:02:56.830
sets amnesty.

37
00:02:56.830 --> 00:03:04.270
We just need to import that by loading the data and then the data is being sorted into training set

38
00:03:04.330 --> 00:03:06.300
as well as to test it.

39
00:03:06.820 --> 00:03:13.000
Just to give you a little bit more metadata about these data sets The X train has 60000 in terms of

40
00:03:13.000 --> 00:03:14.050
shapes.

41
00:03:14.050 --> 00:03:15.660
There's 28 by 28.

42
00:03:15.700 --> 00:03:19.480
These are the features as you can see these are the X train.

43
00:03:19.480 --> 00:03:23.890
These are the features in the previous problem that were looked at.

44
00:03:23.890 --> 00:03:33.280
We only had one feature the one feature was the flow data point that we provide whereby for this it's

45
00:03:33.460 --> 00:03:39.950
pixel we have 28 by 28 meaning 28 pixel high by 28 pixel y.

46
00:03:39.970 --> 00:03:44.050
So if you might apply that out this 784.

47
00:03:44.110 --> 00:03:51.670
So this is the actual shape of the data that we have for both the x train Expo as the X test for the

48
00:03:51.670 --> 00:03:57.880
Y train because it's only just label 0 1 2 3 or the way to 9.

49
00:03:57.880 --> 00:04:03.600
So this why it does not have a dimension of the actual shape itself is actually one.

50
00:04:03.610 --> 00:04:06.490
And likewise for the test as well.

51
00:04:06.880 --> 00:04:12.610
So if you look at the unique value for y train in white as is zero or the way to nine as I mentioned

52
00:04:13.620 --> 00:04:13.940
right.

53
00:04:13.960 --> 00:04:18.070
The next part is just to let you see what does the image look like.

54
00:04:18.070 --> 00:04:27.100
The X strain here is we're looking at the first data point on the train so you can see that this shape

55
00:04:27.310 --> 00:04:28.890
what looks like a five.

56
00:04:28.900 --> 00:04:35.580
And if you want to verify that you look at the corresponding label provide the index which is 0 as well.

57
00:04:35.800 --> 00:04:41.400
If you print that out and it will show you at the actual label is 5 indeed.

58
00:04:41.590 --> 00:04:48.040
The one thing that we do need to do is do what's called data preparation because our data come in the

59
00:04:48.040 --> 00:04:51.100
form of unsigned integer of 8.

60
00:04:51.100 --> 00:04:53.410
We do want in float.

61
00:04:53.440 --> 00:05:01.600
So what we do is actually we add because the unsigned comes in the 0 2 5 5 it is in grayscale.

62
00:05:01.690 --> 00:05:10.330
So we do need to combat it to float so of between 0 and 1 by dividing by 250 5.0 which is a float it

63
00:05:10.330 --> 00:05:13.480
forces the whole answer to become a float.

64
00:05:13.790 --> 00:05:14.020
OK

65
00:05:16.880 --> 00:05:17.810
yep or you run there.

66
00:05:17.810 --> 00:05:26.810
So the first step that we do is that we flatten the pixel itself the pixel and Eminem's come in the

67
00:05:26.810 --> 00:05:31.220
form of 28 by 28 28 high end 28 wide.

68
00:05:31.220 --> 00:05:37.100
So what we do is that we converted from a 2D array to a one the array of seven hundred and eighty four

69
00:05:37.100 --> 00:05:37.640
pixels.

70
00:05:37.640 --> 00:05:41.150
That's really what the flattening performed.

71
00:05:41.150 --> 00:05:44.600
The second a year is a hidden layer.

72
00:05:44.720 --> 00:05:47.990
It's a densely connected layer of phone and of neurons.

73
00:05:48.000 --> 00:05:54.550
Now naturally this fight and two off can be defined as another number.

74
00:05:54.680 --> 00:06:03.100
In this case in terms of our net model of this fight an 84 maybe somewhat on the high side.

75
00:06:03.170 --> 00:06:10.070
This is something that you need to play around with to actually come to an optimal architecture that

76
00:06:10.070 --> 00:06:11.700
works for you.

77
00:06:11.810 --> 00:06:19.310
Now they are fun two off neurons here and each neurons takes input from all of these seven hundred and

78
00:06:19.370 --> 00:06:23.310
eighty four nodes before recall.

79
00:06:23.390 --> 00:06:35.000
What we have was as the first layer of inputs 784 pixel and each pixel was 256 considers value 256 but

80
00:06:35.000 --> 00:06:43.860
once it converted to float takes a float instead of the 0 2 5 5 value.

81
00:06:44.150 --> 00:06:46.490
And once we actually have flattened it.

82
00:06:46.490 --> 00:06:54.710
Now what we have is actually a fully connected leaf from 7 8 4 to 5 and 2 of the output layer is another

83
00:06:54.710 --> 00:06:55.450
dense layer.

84
00:06:55.460 --> 00:07:03.080
And this time round what we have is a 10 node what's called soft Max play layer each note represent

85
00:07:03.200 --> 00:07:10.580
each of the digital class because we have 0 1 2 or the way to name would really have 10 classes for

86
00:07:10.590 --> 00:07:18.590
yourself Max because soft Max generate the probability of each class based on the actual input itself.

87
00:07:18.590 --> 00:07:23.500
So that's really what's nice about soft Max and that's why we use it.

88
00:07:23.590 --> 00:07:23.870
Right.

89
00:07:23.900 --> 00:07:28.290
Just correct a typo here should be legal.

90
00:07:28.910 --> 00:07:32.560
All right so let's compile our model.

91
00:07:32.680 --> 00:07:38.740
So this is the portion where we have Cara sequential we'll have to flatten where we defined that the

92
00:07:38.740 --> 00:07:40.840
input shape is 28 by 28.

93
00:07:40.900 --> 00:07:43.690
We have a dense layer of fine and twelve units.

94
00:07:43.690 --> 00:07:50.140
There is the activation function core value as well as the activation function course of Max.

95
00:07:50.830 --> 00:07:57.400
I like our first example from the previous lesson where we don't have any activation layer at all.

96
00:07:57.400 --> 00:08:03.280
Here we have to do the activation the last layer.

97
00:08:03.280 --> 00:08:08.430
It's about generating the probability so it doesn't actually have any.

98
00:08:08.440 --> 00:08:16.160
So core learning per say in it but really low on the other hand has a lot of value to go with it.

99
00:08:16.210 --> 00:08:23.920
The reason that we are looking at value instead of no real low or no activation function like our first

100
00:08:24.670 --> 00:08:29.830
model that we looked at in the previous lesson is this the first model was very simple and isn't linear

101
00:08:29.830 --> 00:08:40.180
by nature because it is linear by nature there's no nonlinear core functionality or the there's no non-linear

102
00:08:40.240 --> 00:08:44.540
features or characteristics that we needed to learn.

103
00:08:44.560 --> 00:08:51.580
So a really basic simple algorithm with no activation is sufficient in our case however if you look

104
00:08:51.580 --> 00:08:56.770
at these you can clearly see there are a lot of nonlinear by and large it's actually non-linear rather

105
00:08:56.770 --> 00:09:00.960
than linear in nature hence we actually use.

106
00:09:01.030 --> 00:09:01.480
Really.

107
00:09:01.900 --> 00:09:02.220
Okay.

108
00:09:02.240 --> 00:09:05.800
Now I say a lot about rare Lu You probably have no idea what I'm talking about.

109
00:09:06.100 --> 00:09:06.760
So let me.

110
00:09:07.330 --> 00:09:12.540
I didn't have a link here for real which is I believe up here so let me just put it up.

111
00:09:12.910 --> 00:09:14.680
Bring this back to where we were.

112
00:09:16.300 --> 00:09:17.280
Which is right here.

113
00:09:17.290 --> 00:09:17.730
Ready.

114
00:09:18.300 --> 00:09:19.510
OK.

115
00:09:19.740 --> 00:09:25.090
There's a nice write up here by then.

116
00:09:25.110 --> 00:09:25.280
Yeah.

117
00:09:25.300 --> 00:09:33.070
By then and here introduce and talked about really in detail and you can see that if you're a finance

118
00:09:33.070 --> 00:09:38.740
train or come from an accounting or with exposure to finance you can see that this is a little bit like

119
00:09:38.740 --> 00:09:39.260
option.

120
00:09:39.280 --> 00:09:48.560
In fact the actual function is actually an option function as anything below zero or negative value.

121
00:09:48.610 --> 00:09:50.890
It will automatically default to zero.

122
00:09:51.190 --> 00:09:55.270
Anything above zero would take the value of its own.

123
00:09:55.270 --> 00:10:03.790
So this is actually what you can see that this is non-linear in nature and it into DOS what's core interactions

124
00:10:03.790 --> 00:10:05.110
and non linearity.

125
00:10:05.110 --> 00:10:10.310
So I will leave that for you to explore a little bit more.

126
00:10:10.580 --> 00:10:12.880
And yeah it's an interesting function.

127
00:10:12.880 --> 00:10:17.350
There are other activation functions that you can make use of but in this case we are making use of

128
00:10:17.350 --> 00:10:18.780
value as myself.

129
00:10:18.790 --> 00:10:21.630
Max we printed their motto summary here.

130
00:10:21.670 --> 00:10:26.500
So you have the first layer which is really the flat and layer then you have the hit in the year which

131
00:10:26.500 --> 00:10:29.270
is fine which has fine until of neurons.

132
00:10:29.410 --> 00:10:34.780
And finally we have the dense layer with an output of 10 which is a 10 classes that we have.

133
00:10:34.780 --> 00:10:44.410
So this is by design it's really relate to what you're what you won your output to be OK in terms of

134
00:10:44.410 --> 00:10:46.110
the model compilation.

135
00:10:46.220 --> 00:10:52.930
We've used our immense prop as the optimizer and also a sparse categorical cross entropy.

136
00:10:52.930 --> 00:11:01.240
This is the right type of lost function for the 4 classification and we can of course try using Adam

137
00:11:01.750 --> 00:11:02.970
as the optimizer.

138
00:11:03.010 --> 00:11:05.240
I'll leave that for you to try that out.

139
00:11:05.470 --> 00:11:08.800
Some of these is just really again trial and error.

140
00:11:09.580 --> 00:11:17.530
And finally with the matrix that we use to measure the performance is core accuracy and in terms of

141
00:11:17.530 --> 00:11:23.350
the fitting would provide the X training data y training data we are only training it for ten epochs

142
00:11:23.440 --> 00:11:27.230
and the batch size we set is 256.

143
00:11:27.400 --> 00:11:33.630
The publisher of the best batch size is to inform the fitting method.

144
00:11:33.790 --> 00:11:40.000
How often should it actually read compute and change the weighting so that's what the batch size is

145
00:11:40.000 --> 00:11:44.190
actually for what I already added a common here.

146
00:11:44.200 --> 00:11:46.810
Basically update the model.

147
00:11:46.810 --> 00:11:57.010
Some of them are hidden variables after every batch of 256 image notice that each epoch we train sixty

148
00:11:57.130 --> 00:11:58.300
thousand images.

149
00:11:58.300 --> 00:12:03.990
That's the sixty thousand images that we have the training images and looking at the lost.

150
00:12:04.000 --> 00:12:09.420
It started off quite high and it drops all the way to point 1 3 in terms of accuracy is a total of ninety

151
00:12:09.420 --> 00:12:14.060
one per cent richest ninety nine point six six per.

152
00:12:14.290 --> 00:12:18.310
You can argue that is 60 really really high and looks really good.

153
00:12:18.520 --> 00:12:24.400
But as I mentioned before there may be a danger of over fitting and we shall see when we compare against

154
00:12:24.400 --> 00:12:36.660
our second model and in terms of the last magnitude only 10 epoch were really close to well 100 percent

155
00:12:36.730 --> 00:12:37.590
really.

156
00:12:37.700 --> 00:12:45.680
And if we look at the Lost performance of the accuracy and if you look at this to test accuracy based

157
00:12:45.680 --> 00:12:49.470
on the test data for the model it's 98 now.

158
00:12:49.540 --> 00:12:56.900
That here doesn't seem like a big deal but when you look at your accuracy in training is ninety nine

159
00:12:56.900 --> 00:12:58.090
point six six.

160
00:12:58.100 --> 00:13:03.800
You will be x2 is somewhat disappointed when you see your value as ninety eight point one is a good

161
00:13:03.860 --> 00:13:09.770
one one and a half percentage drop and this is what I'm trying to highlight to you is that you should

162
00:13:09.770 --> 00:13:15.890
actually pay attention to these small details as it's highlighting to you that your model is likely

163
00:13:15.890 --> 00:13:20.070
to be although fitting your training data.

164
00:13:20.170 --> 00:13:21.820
Let me just sidetrack a little bit.

165
00:13:21.830 --> 00:13:29.330
Much like how we learn when we take an edge or go for education be in primary secondary school and tertiary

166
00:13:29.330 --> 00:13:31.030
Of course.

167
00:13:31.280 --> 00:13:38.240
As you learn the materials as the teacher provide you with the example when you actually go into an

168
00:13:38.240 --> 00:13:43.990
exam the last thing that you want is to teach it to give you the exact same question as the tutorial

169
00:13:44.000 --> 00:13:45.010
questions.

170
00:13:45.170 --> 00:13:50.840
Well the why is that that the reason will be that that's called memorization doesn't actually test your

171
00:13:50.840 --> 00:13:56.690
ability that you test your ability and test that you have actually learned the concept.

172
00:13:57.350 --> 00:14:02.960
What you can essentially do to survive or do well is that if they are all the same questions that you've

173
00:14:02.960 --> 00:14:04.190
seen before.

174
00:14:04.250 --> 00:14:07.070
Basically what does that situation will be.

175
00:14:07.070 --> 00:14:11.090
What we call memory like memorization which is not learning at all.

176
00:14:11.300 --> 00:14:17.870
What you really won is that you learned a different set of questions and tutorial come.

177
00:14:17.870 --> 00:14:27.140
The exam is similar but different Okay similar concepts or concepts being tested are the same.

178
00:14:27.140 --> 00:14:33.470
But the question is damn how the wordings are the numbers everything should be totally different.

179
00:14:33.920 --> 00:14:40.730
And that will be a proper test of the learner or the student's ability to have it grasp the concept

180
00:14:40.730 --> 00:14:49.240
rather than memorizing the answer from what from answer or training better that they've seen previously

181
00:14:50.740 --> 00:14:53.660
so this is really the first part of the side.

182
00:14:53.680 --> 00:15:00.040
I want to highlight is the same concept when it comes to training your machine learning model.

183
00:15:00.040 --> 00:15:02.140
Now let's look at a second model.

184
00:15:02.140 --> 00:15:03.600
Model number two.

185
00:15:03.790 --> 00:15:09.760
And if you looked at it you will see that there's a slight variation here which is that we have a smaller

186
00:15:09.820 --> 00:15:12.760
architecture rather than fallen and two off neurons.

187
00:15:12.760 --> 00:15:15.900
Now we only have 128.

188
00:15:15.940 --> 00:15:22.600
We also have another additional layer here called dropout with the value of point two.

189
00:15:22.600 --> 00:15:23.380
The rest are the same.

190
00:15:23.630 --> 00:15:24.120
OK.

191
00:15:24.240 --> 00:15:24.990
OK.

192
00:15:25.090 --> 00:15:27.730
Adam's used rather than as prop.

193
00:15:27.790 --> 00:15:32.780
Other than that everything else is exactly the same OK.

194
00:15:32.800 --> 00:15:35.610
Why those change.

195
00:15:35.610 --> 00:15:46.230
This is obviously a smaller architecture so its ability to learn is obviously less than the 512 neurons

196
00:15:46.710 --> 00:15:49.920
but it may be as efficient for our task here.

197
00:15:49.920 --> 00:15:55.310
Fine until might be a bit too flexible to learn the problems that we are facing.

198
00:15:55.650 --> 00:16:03.360
And the other part about the drop out is that a 20 point to here is 20 percent.

199
00:16:03.390 --> 00:16:14.540
It's forcing carers during the training phase 20 percent of the neurons get dropped randomly.

200
00:16:14.600 --> 00:16:18.920
What that's actually forcing the neuron to do is obviously to work harder.

201
00:16:18.950 --> 00:16:19.700
That's the first thing.

202
00:16:19.700 --> 00:16:25.970
Second thing is that it does not allow it to memorize the.

203
00:16:25.970 --> 00:16:29.120
These are called training data has seen before.

204
00:16:29.120 --> 00:16:32.910
So that's what the 20 percent drop out is for.

205
00:16:33.110 --> 00:16:43.070
And that kind of variation the drop out of 20 percent variation is to actually what we call regularization.

206
00:16:43.070 --> 00:16:50.900
It's a way to make make sure your model is lot more robust to avoid the memorization problem that are

207
00:16:50.900 --> 00:16:51.570
highlighted.

208
00:16:51.950 --> 00:16:57.470
And the second thing is that you really are driving it to learn the general concept of the be able to

209
00:16:57.470 --> 00:17:02.110
generalize towards it when you actually start looking at the test data.

210
00:17:02.180 --> 00:17:04.520
So let's run this.

211
00:17:05.330 --> 00:17:05.830
Okay.

212
00:17:05.840 --> 00:17:14.070
The Adam is just a change just to see if there is a speed or change in terms of the optimization.

213
00:17:14.080 --> 00:17:19.340
That doesn't seem to be of course because partially because our training is not really it's not a large

214
00:17:19.340 --> 00:17:22.130
dataset and also is not very complex today.

215
00:17:22.130 --> 00:17:27.980
So if you look at the time one second 18 microseconds at exactly the same there's hardly any difference

216
00:17:28.760 --> 00:17:39.440
but the part that comes out quite strongly is the accuracy is only at 97 percent is still climbing although

217
00:17:39.590 --> 00:17:46.730
this is still climbing but you can see that the climb change the percentage change from each epoch has

218
00:17:46.730 --> 00:17:54.740
slowed down whereas for this S2 at ninety seven said seem like there's still more room to has more room

219
00:17:54.860 --> 00:18:00.140
to grow or train and let's plot this out to see.

220
00:18:00.240 --> 00:18:04.250
Yep this is definitely a lot steeper than what we looked at.

221
00:18:04.250 --> 00:18:11.750
Let's look at the model out our sample accuracy is ninety seven point eight two which is actually very

222
00:18:11.750 --> 00:18:12.380
interesting.

223
00:18:12.860 --> 00:18:19.370
Let me just hide this so that we can see you'd notice that because of the regularization that we did

224
00:18:19.370 --> 00:18:27.500
which is why drop out and also the smaller architecture you can see that the accuracy within the training

225
00:18:27.950 --> 00:18:30.650
and also the Alo sample is very very close.

226
00:18:30.650 --> 00:18:36.140
Ninety seven point eight to ninety seven point eight two is basically no change at all.

227
00:18:36.560 --> 00:18:43.100
And that's telling me that number one this is the appropriate amount of learning.

228
00:18:43.130 --> 00:18:54.290
Maybe more but definitely not over fitted as compared to this earlier one that we saw where within the

229
00:18:54.290 --> 00:18:59.730
training is ninety nine point six six percent versus ninety eight point one eight percent.

230
00:18:59.750 --> 00:19:08.990
So the difference inform you that the first model is all of a train and and basically and basically

231
00:19:08.990 --> 00:19:11.970
has gone into all the fitting.

232
00:19:11.990 --> 00:19:15.580
All right let's go one step further in terms of prediction.

233
00:19:15.590 --> 00:19:20.220
Let's look at the first data the X test.

234
00:19:20.240 --> 00:19:21.830
The index that we're looking at here.

235
00:19:21.830 --> 00:19:29.720
The shape we just need to reshape it to just basically a one so that the because the the architecture

236
00:19:29.750 --> 00:19:33.870
is X expecting 784.

237
00:19:34.130 --> 00:19:37.790
And if you just feed 28 by 28 without informing it.

238
00:19:37.790 --> 00:19:40.310
This is one sample it will throw in an error.

239
00:19:41.270 --> 00:19:46.820
So we've reshaped this and we fit it into the prediction.

240
00:19:46.820 --> 00:19:51.290
And what you can see here is that of course this is 0 1 2 3.

241
00:19:51.890 --> 00:20:00.440
But what is telling you here is that it's ninety nine point ninety percent sure this is a 7 and it's

242
00:20:01.100 --> 00:20:02.810
1 percent.

243
00:20:02.970 --> 00:20:08.890
And I'm sure that it is a 2 0 1 2 3 okay.

244
00:20:08.980 --> 00:20:14.010
This 5 percent probability that this is actually a 3.

245
00:20:14.060 --> 00:20:19.300
Let's have a look at the prediction it is actually a seven.

246
00:20:19.300 --> 00:20:26.800
Based on the highest value and if you look at the actual labor itself it is seven as well.

247
00:20:26.800 --> 00:20:27.060
Okay.

248
00:20:27.070 --> 00:20:30.460
So that's really the prediction step.

249
00:20:30.460 --> 00:20:32.310
Let me just quickly summarize.

250
00:20:32.480 --> 00:20:36.360
Well the cover and some of the concept with touched on.

251
00:20:36.610 --> 00:20:42.370
We've talked about the fact that we looked at a regression example in the previous lesson and this is

252
00:20:42.460 --> 00:20:46.850
a classification problem and it's a 10 class classification problem.

253
00:20:46.870 --> 00:20:49.570
We looked at what is a flattening layer.

254
00:20:49.600 --> 00:20:53.770
We also looked at the activation value as well as soft Max.

255
00:20:53.920 --> 00:20:56.650
And we talked about the purpose of drop out.

256
00:20:56.680 --> 00:21:03.160
We briefly touch on training and testing where we'll go into a little bit more depth in the future lesson

257
00:21:03.550 --> 00:21:06.930
on training as well as testing.

258
00:21:06.940 --> 00:21:08.860
With that I'm going to stop.

259
00:21:08.860 --> 00:21:10.480
Thank you for watching.

260
00:21:10.480 --> 00:21:13.120
I hope you have learned something all of this lesson.
