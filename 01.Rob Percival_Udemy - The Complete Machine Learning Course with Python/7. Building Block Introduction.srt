1
00:00:00,230 --> 00:00:01,330
Hey welcome back.

2
00:00:01,480 --> 00:00:06,990
And this lesson we're going to start looking at some of the building blocks of neural networks.

3
00:00:06,990 --> 00:00:16,530
Now some of the key foundation that you do need to know not to really grasp the concepts is linear algebra.

4
00:00:16,530 --> 00:00:22,530
That's the first one second one is probability as well as information theory and you do need to actually

5
00:00:22,530 --> 00:00:28,260
have a fairly good grasp of numerical computation as well as literacy.

6
00:00:28,260 --> 00:00:34,610
Now the the so-called codes that I showed you in the last lesson.

7
00:00:34,620 --> 00:00:37,860
It requires a certain level thinking of course.

8
00:00:37,860 --> 00:00:44,760
But if you study it carefully and if you actually study it diligently you'll find that the actual sequence

9
00:00:44,760 --> 00:00:50,180
and the actual approach is is it's it's OK.

10
00:00:50,250 --> 00:00:55,470
You can exceed come to Ken come to an understanding of it without much problem for some of you you will

11
00:00:55,470 --> 00:00:57,540
be able to pick up very easily.

12
00:00:57,690 --> 00:01:02,760
It's just a second nature to you and maybe because you have developed excuse over the years for some

13
00:01:02,760 --> 00:01:08,280
of you don't lose heart it just do not you will still be able to get hold of it.

14
00:01:08,320 --> 00:01:16,340
I just need to give it a bit more chance and give yourself a bit more chance as well just to revise

15
00:01:16,340 --> 00:01:16,830
a little bit.

16
00:01:16,830 --> 00:01:25,170
But we actually covered in the last example and code walk through we looked at the immense dataset and

17
00:01:25,170 --> 00:01:31,690
the steps that we covered pretty much is the actual standard steps for all neural network.

18
00:01:32,010 --> 00:01:34,320
And the training of any models.

19
00:01:34,530 --> 00:01:36,630
The first thing is loading the data.

20
00:01:36,900 --> 00:01:44,370
The second one will be defining your neural network architecture before you compile it with the selection

21
00:01:44,400 --> 00:01:52,710
of your optimizer as well as your lost function in the last one what we did use is our mess prop which

22
00:01:52,800 --> 00:01:54,690
did not explain what it is.

23
00:01:54,750 --> 00:02:01,890
OK we will explain that in the later future lesson as well as the lost function the lost function is

24
00:02:02,160 --> 00:02:09,510
across entropy that we use across entropy is for multi class in our case we have zero one two three

25
00:02:09,510 --> 00:02:15,870
four or the eight to nine so we have ten classes that is what you will use which is cross entropy if

26
00:02:15,870 --> 00:02:18,300
it's binary then you use binary.

27
00:02:18,390 --> 00:02:20,000
OK.

28
00:02:20,400 --> 00:02:27,390
The the lost function and after that we do need to compile it this the compilation part which I just

29
00:02:27,390 --> 00:02:28,530
explain.

30
00:02:28,530 --> 00:02:34,080
And before we feed our data through we do need to actually pre process and prepare the data so that

31
00:02:34,080 --> 00:02:40,110
it actually makes it more amenable for the training of the neural network what we did in the previous

32
00:02:40,110 --> 00:02:48,390
lesson to X is to convert or data to or squeeze it to between 0 and 1 it actually speeds up the training

33
00:02:48,780 --> 00:02:55,370
and you don't actually have an error errors drawing everywhere the next thing is that we do need to

34
00:02:55,370 --> 00:03:04,190
prepare our label or targets to ensure that it is appropriately labeled because when it comes to the

35
00:03:05,480 --> 00:03:11,330
machine learning again it doesn't work well with tanks or strings you do have to convert it to integers

36
00:03:11,420 --> 00:03:19,610
ideally in one hard encoding format and the last second lasting is to actually train the model to an

37
00:03:19,610 --> 00:03:24,080
acceptable accuracy level before continue on.

38
00:03:24,080 --> 00:03:29,420
Once you actually happy with it then you actually move on to the actual prediction.

39
00:03:29,510 --> 00:03:37,430
Now this process that I've just described here is not a one off process it's one that you do need to

40
00:03:37,430 --> 00:03:41,660
actually continuously train your data as new data comes in.

41
00:03:41,660 --> 00:03:48,290
You do need to train your model because there's one of those things when it comes to machine learning

42
00:03:48,290 --> 00:03:56,780
algorithm is that it does go through decay or new were so-called algorithm better algorithm comes along

43
00:03:56,780 --> 00:04:05,060
that you do need to improve it and also sometimes if you're working with financial Data's the taste

44
00:04:05,060 --> 00:04:08,600
and the preferences of your customers may change.

45
00:04:08,600 --> 00:04:10,790
So those are the things that them.

46
00:04:11,030 --> 00:04:17,630
It's a it's a broader issue to think about and to consider one that you do need to train your neural

47
00:04:17,630 --> 00:04:23,190
network on a continuous basis so that it actually stay relevant.

48
00:04:23,190 --> 00:04:25,850
All right so that's really what I want to covered when we come back.

49
00:04:25,880 --> 00:04:33,140
We're going to go into a little bit more about the actual tensor itself which is a description that

50
00:04:33,140 --> 00:04:39,290
we use to describe the data and then I'm going to stop and thank you once again for watching this.
