1
00:00:00,300 --> 00:00:01,560
Welcome back everyone.

2
00:00:01,560 --> 00:00:10,260
In this segment we can start looking at a few more distinguished and unique characteristics of several

3
00:00:10,260 --> 00:00:11,850
types of layers.

4
00:00:11,910 --> 00:00:15,740
This is towards the end of the different type of layers.

5
00:00:15,870 --> 00:00:22,770
In this segment we're going to look at putting layer the fully connected and the flatten just to wrap

6
00:00:22,770 --> 00:00:27,420
it up so let's start with pulling layer.

7
00:00:27,640 --> 00:00:36,550
This is an example of Max pulling down to common times average pulling or Max pulling.

8
00:00:36,940 --> 00:00:42,460
The idea here applies to each activation map independently.

9
00:00:42,520 --> 00:00:48,720
The purpose of pulling is to reduce the size or spatial size or the representation.

10
00:00:48,970 --> 00:00:54,270
It is speeds up the computation layer so that there's not so many calculation that needs to be done.

11
00:00:54,490 --> 00:00:57,880
And it helps over control of the fitting as well.

12
00:00:57,880 --> 00:01:01,630
Now if you think about what is the purpose of Max.

13
00:01:01,900 --> 00:01:09,670
Max pulling the basic idea is that you take the maximum value in.

14
00:01:09,790 --> 00:01:13,500
In this case in two by two matrix.

15
00:01:13,630 --> 00:01:21,100
What that is doing is that you are actually just taking the mole most outstanding value and take that

16
00:01:21,160 --> 00:01:27,910
as the representation of that small four pixels or two by two matrix.

17
00:01:27,910 --> 00:01:36,860
Looking at the pictures that we have here there are two outputs the top one with the stride of to the

18
00:01:36,860 --> 00:01:38,480
bottom one with the strata one.

19
00:01:38,480 --> 00:01:45,890
So let's look at the one with the stride of two and you do need to follow along with the color and the

20
00:01:45,890 --> 00:01:50,020
color represent the origin or origin of the maximum value.

21
00:01:50,060 --> 00:01:56,420
So if we take this portion here this is the first application of the max pulling within this two by

22
00:01:56,420 --> 00:01:57,250
two matrix.

23
00:01:57,260 --> 00:02:03,470
You can see that value is chaff as the high is hands you have that because the stride is too you skip

24
00:02:03,470 --> 00:02:10,430
this and you come over here and again you look at this two by two matrix and tech the highest value

25
00:02:10,760 --> 00:02:17,480
which is five and now you have because the stride is too you skip this row and you come to this raw

26
00:02:17,540 --> 00:02:23,790
you're looking at this then you notice that the maximum value here is twenty two followed by thirty

27
00:02:23,790 --> 00:02:24,290
two.

28
00:02:24,290 --> 00:02:31,810
So this basically makes pulling with two by two filter with the stride of two what happen if this stride

29
00:02:31,820 --> 00:02:33,490
is actually one.

30
00:02:34,160 --> 00:02:39,950
So you will the first one will be the same but you move along to twelve and you know looking at this

31
00:02:40,280 --> 00:02:41,170
matrix here.

32
00:02:41,440 --> 00:02:41,810
OK.

33
00:02:41,840 --> 00:02:47,600
This matrix the highest value is twelve hands is off and you move along by one the highest value is

34
00:02:47,600 --> 00:02:53,860
five hence you have five and you moved down here within this two by two matrix the highest value is

35
00:02:53,870 --> 00:02:58,120
twenty two and is twenty two and then let me just move along one more.

36
00:02:58,510 --> 00:03:03,710
Just basically finished this row and you look at this two by two the highest value is twelve hence is

37
00:03:03,720 --> 00:03:09,230
two of and if you look at this two by two the highest values ten then you move down by one pixel looking

38
00:03:09,230 --> 00:03:16,520
at this the highest values twenty two so looking at a stride one you get a lot more overlaps that's

39
00:03:16,520 --> 00:03:22,070
why when you look at the so-called output the future of this being repeated there there's few twenty

40
00:03:22,070 --> 00:03:23,750
two there's a few thirty two.

41
00:03:23,750 --> 00:03:30,050
These are all repetition if you really want to reduce the size then you choose a larger stride like

42
00:03:30,080 --> 00:03:37,340
the one on top whereby or the highest value is represented those are the so called high intensity I

43
00:03:37,340 --> 00:03:44,000
suppose you can call it from a slightly different angle but the idea here is that it picks up the key

44
00:03:44,000 --> 00:03:49,190
features so that's basically what I want to cover in regards to pulling.

45
00:03:49,200 --> 00:03:55,140
Let's move to the next one which is flatten the basic idea of flatten.

46
00:03:55,140 --> 00:03:58,980
Is that the classifier that we will apply next.

47
00:03:59,070 --> 00:04:05,430
Which could be a sigmoid could be a linear regression logistic regression or soft Max and the likes

48
00:04:06,270 --> 00:04:12,020
this allowed the classifier to to to convert it into a class.

49
00:04:12,510 --> 00:04:18,180
Otherwise if you have a matrix structure then these functions can handle it.

50
00:04:18,720 --> 00:04:23,420
So hence we actually convert it into a one dimensional format.

51
00:04:23,610 --> 00:04:30,960
So where you have logistic this and Z a two by two which is char 5 and then followed by another row

52
00:04:30,990 --> 00:04:40,260
20 to 30 to become 520 232 one long string of numbers and you apply the sigmoid or the soft Max depending

53
00:04:40,260 --> 00:04:50,990
on your depending on your intention or whether there is actually multiple classes or only binary classification.

54
00:04:51,540 --> 00:04:55,350
OK so that's pretty much for the flatten layer.

55
00:04:55,500 --> 00:05:02,430
And we have the fully connected layer which is very common right through our artificial neural network

56
00:05:03,050 --> 00:05:04,220
in then.

57
00:05:04,290 --> 00:05:12,030
And if you look at this fully connected layer it's also called dense layer whereby every neuron from

58
00:05:12,030 --> 00:05:18,780
previous layer is connected to every neuron in the current layer and the next layer and on and on it

59
00:05:18,780 --> 00:05:19,520
goes.

60
00:05:19,560 --> 00:05:23,560
If you look at this you can see that is extremely busy in terms of the picture.

61
00:05:23,560 --> 00:05:27,870
So this connects to all the neuron.

62
00:05:27,870 --> 00:05:30,090
Likewise this connects to all four neurons.

63
00:05:30,090 --> 00:05:36,420
If you have fifty two neurons you can see what it means or why did they actually used to call this dense

64
00:05:36,420 --> 00:05:36,770
layer.

65
00:05:36,780 --> 00:05:42,780
And also the rationale of calling it fully connected because they are connected to every single one

66
00:05:42,780 --> 00:05:52,160
of them normally in the modern day architecture of CNN you usually observe these at the end of the neural

67
00:05:52,160 --> 00:05:55,000
network right.

68
00:05:55,030 --> 00:05:58,760
So we come to the end of all the different layers.

69
00:05:58,870 --> 00:06:06,540
There is this convolution neural network but this one is written in Java Script by cup of tea cup of

70
00:06:06,540 --> 00:06:09,970
the was the professor in Stanford.

71
00:06:10,040 --> 00:06:18,520
Two pops in every now and then I believe but now he's if I follow along correctly as it's in 2018 he

72
00:06:18,520 --> 00:06:24,170
is in Tesla working on the self-driving autonomous cars.

73
00:06:24,700 --> 00:06:29,500
So this is what he created when he was actually teaching in Stanford.

74
00:06:29,500 --> 00:06:35,800
This is based on the CFR 10 which is 10 classes data sets it's all pictures.

75
00:06:36,390 --> 00:06:45,790
And at this Scripps here are the java scripts demonstrate to you how CNN actually work.

76
00:06:45,790 --> 00:06:53,320
It's quite a good visualization technique allows you to see end to end how it works.

77
00:06:53,470 --> 00:07:00,870
A You can let me just zoom in here in terms of the loss functions this year.

78
00:07:00,920 --> 00:07:02,460
You can choose the lending rate.

79
00:07:02,470 --> 00:07:06,820
You can also choose the actual momentum which we haven't talked about.

80
00:07:06,820 --> 00:07:13,990
So instead of stochastic gradient descent that applies momentum we've trained to it.

81
00:07:13,990 --> 00:07:16,430
And then you can choose to bench size.

82
00:07:16,450 --> 00:07:18,950
You can also change the way decay.

83
00:07:18,970 --> 00:07:25,480
This is to actually allow the so-called fine and green training as well.

84
00:07:25,510 --> 00:07:31,600
This is training from grown up meaning starting from zero.

85
00:07:31,600 --> 00:07:39,250
You can also load pre train networks just by clicking this and wait for a while.

86
00:07:39,330 --> 00:07:46,910
It would just refresh and start again and the preacher network has accuracy of approximate 80 percent.

87
00:07:47,110 --> 00:07:51,760
This is what the so-called layer looked like neural network architecture.

88
00:07:51,790 --> 00:07:58,120
Look like you have the input you have the convolution layer pulling layer convolution pulling convolution

89
00:07:58,120 --> 00:08:05,360
pulling and then soft Max finally in terms of visualization you have the input.

90
00:08:05,970 --> 00:08:06,460
Okay.

91
00:08:06,460 --> 00:08:10,990
And this is what the activation function or pictures looks like.

92
00:08:10,990 --> 00:08:16,330
And you notice that these are the different futures and these are the gradient.

93
00:08:16,390 --> 00:08:17,850
These are the weights.

94
00:08:18,040 --> 00:08:23,080
And this again that Renu again activation gradient gradient.

95
00:08:23,110 --> 00:08:25,040
And this is the max pulling.

96
00:08:25,240 --> 00:08:31,150
This is convolution and on and on it goes it actually highlight all the multiple layers and what it

97
00:08:31,150 --> 00:08:32,110
looks like.

98
00:08:32,320 --> 00:08:34,520
And to help you to actually see that.

99
00:08:34,620 --> 00:08:37,900
And these are the so-called example prediction.

100
00:08:37,900 --> 00:08:38,190
OK.

101
00:08:38,260 --> 00:08:41,220
Got the truck correct the DOT correct.

102
00:08:41,440 --> 00:08:42,070
There's a dot.

103
00:08:42,070 --> 00:08:43,290
Yes.

104
00:08:43,360 --> 00:08:51,430
This is the ship and also shows you how confident it is something like this is not very confident.

105
00:08:51,430 --> 00:08:52,330
This is a horse.

106
00:08:52,370 --> 00:08:53,770
Notice they got it wrong.

107
00:08:53,770 --> 00:08:54,850
This is a frog.

108
00:08:54,850 --> 00:08:57,660
What it takes to call it a ship.

109
00:08:57,940 --> 00:08:58,110
Yeah.

110
00:08:58,120 --> 00:09:03,310
So it tells you the first three classifications the top three classification whether they're right or

111
00:09:03,310 --> 00:09:04,240
wrong.

112
00:09:04,240 --> 00:09:07,470
Green means is actually correct.

113
00:09:07,480 --> 00:09:09,550
That's the correct class of his top.

114
00:09:09,550 --> 00:09:12,490
That means it takes to classify that correctly.

115
00:09:12,490 --> 00:09:12,760
Yeah.

116
00:09:12,790 --> 00:09:19,500
So there are a whole lot of intuition that you can develop out of playing around with this.

117
00:09:19,520 --> 00:09:21,500
I highly highly encourage you.

118
00:09:21,530 --> 00:09:24,070
I mean it's not very complicated architecture.

119
00:09:24,070 --> 00:09:32,410
CNN architecture but it definitely will help you in terms of developing your intuition with it.

120
00:09:32,410 --> 00:09:34,570
So it's really screaming.

121
00:09:34,850 --> 00:09:41,370
My my my Mac is screaming now because he's actually using my CPO to actually do the calculation.

122
00:09:41,440 --> 00:09:45,790
So one two three convolution that pulling in between

123
00:09:48,580 --> 00:09:49,870
and the activation function.

124
00:09:49,870 --> 00:09:50,600
All right.

125
00:09:51,010 --> 00:09:58,750
So one convolution one value and then you have a pulling convolution to pulling convolution really pulling

126
00:09:59,020 --> 00:10:01,730
and then fully connected layer and solve Max.

127
00:10:02,050 --> 00:10:02,640
Right.

128
00:10:02,680 --> 00:10:09,850
With that let me just close it off because it's really heating up my my end mic and them.

129
00:10:09,940 --> 00:10:10,150
Yeah.

130
00:10:10,180 --> 00:10:18,670
So this is basically what we've covered right throughout this whole series of lessons in terms of the

131
00:10:18,670 --> 00:10:20,140
different layers.

132
00:10:20,140 --> 00:10:28,040
Let me go back to the architecture that we looked at.

133
00:10:28,230 --> 00:10:34,390
We covered the employer which is the image that is made out pixels of its gray scale.

134
00:10:34,410 --> 00:10:39,890
There is only one layer if its color then you have to be three layers or three channels.

135
00:10:39,960 --> 00:10:43,050
We talked about convolution We talked about pulling layer.

136
00:10:43,380 --> 00:10:46,230
We talked about the flatten the purpose of flatten.

137
00:10:46,350 --> 00:10:49,160
And we also talked about the densely connected layer.

138
00:10:49,350 --> 00:10:53,880
And yeah that's pretty much everything that we cover.

139
00:10:53,880 --> 00:10:58,920
I hope you have found that extremely useful.

140
00:10:58,930 --> 00:11:01,330
These are all the different layers we covered.

141
00:11:01,330 --> 00:11:03,160
Thank you once again for watching.

142
00:11:03,160 --> 00:11:05,260
I look forward to seeing you in the next lesson.
