1
00:00:00,210 --> 00:00:01,650
Welcome back everyone.

2
00:00:01,650 --> 00:00:03,410
Good to see you here again.

3
00:00:03,690 --> 00:00:10,480
In this portion we're going to look at over fitting as well as under fitting for your neuro networks.

4
00:00:10,490 --> 00:00:15,480
Then there is always a tradeoff between the fitting and the fitting.

5
00:00:15,480 --> 00:00:21,630
The goal of training your neural network is so that it can learn from the pattern of the data it is

6
00:00:21,990 --> 00:00:22,830
under fitting.

7
00:00:22,830 --> 00:00:24,850
It's a waste of the data.

8
00:00:25,020 --> 00:00:28,310
So we tend to continue to fit the data.

9
00:00:28,320 --> 00:00:34,470
The only problem is that it can over fit your training data and is not able to generalize.

10
00:00:34,500 --> 00:00:36,910
So that part is important.

11
00:00:37,290 --> 00:00:43,050
Now the fitting process there's another term for it was mentioned here is core optimization.

12
00:00:43,050 --> 00:00:47,330
Your model will always start off with being under fitted.

13
00:00:47,430 --> 00:00:55,740
Now as you continuously expose your data your model to the training data many times it will continue

14
00:00:55,740 --> 00:01:01,890
to improve but there will come a tipping point whereby your model will over learn and or over learn

15
00:01:01,890 --> 00:01:05,150
from the training data and wasn't able to generalize.

16
00:01:05,160 --> 00:01:12,170
And at that point is actually you should and ought to have stopped the training.

17
00:01:12,780 --> 00:01:17,510
Now the fitting is when your model has not learned fully from the training data in this situation both

18
00:01:17,510 --> 00:01:22,690
the training and test loss will be falling on each epoch.

19
00:01:22,890 --> 00:01:28,410
So we demonstrated that in some of our examples in fact all of the examples that we've talked about

20
00:01:28,410 --> 00:01:30,770
that this is for the lost score.

21
00:01:30,860 --> 00:01:35,910
Now in order to improve both your training and test score you can increase the number of times your

22
00:01:35,910 --> 00:01:40,950
model epoxy the data e.g. pull it sees all the data.

23
00:01:40,950 --> 00:01:47,790
This process is also a core optimization all fitting is when the law score in the training and test

24
00:01:47,880 --> 00:01:49,620
start to diverge.

25
00:01:49,620 --> 00:01:55,200
The training law score continue to fall while the test scores start to rise.

26
00:01:55,470 --> 00:01:57,240
So rising is bad.

27
00:01:57,480 --> 00:01:57,850
OK.

28
00:01:57,870 --> 00:01:59,710
So that means the error errors is going up.

29
00:02:00,240 --> 00:02:04,740
So in this situation the bottle is starting to over fit your training data and unable to generalize

30
00:02:04,770 --> 00:02:06,870
it will not perform well with data.

31
00:02:06,860 --> 00:02:12,800
It has not seen before the data that hasn't been seen before is also called test score.

32
00:02:12,840 --> 00:02:15,230
How do you overcome a fitting.

33
00:02:15,510 --> 00:02:19,660
There's a few ways you can get more data.

34
00:02:19,710 --> 00:02:31,030
You can also perform one score regularization meaning forcing your weights to be within a range.

35
00:02:31,030 --> 00:02:31,350
Okay.

36
00:02:31,370 --> 00:02:33,530
So that is sexually constrained.

37
00:02:34,140 --> 00:02:36,150
So regularization reduced.

38
00:02:36,180 --> 00:02:37,350
There's a few ways.

39
00:02:37,360 --> 00:02:38,000
There's.

40
00:02:38,160 --> 00:02:39,890
Reduce your network size.

41
00:02:39,930 --> 00:02:44,430
Meaning don't have too many neurons or don't have too many layers.

42
00:02:44,430 --> 00:02:52,320
Weight regularization is pertaining to the weight meaning constrain it to within a range.

43
00:02:52,320 --> 00:03:01,380
Drop out is something that we will look at essentially as we are running our training from one level

44
00:03:01,380 --> 00:03:02,450
to another level.

45
00:03:02,460 --> 00:03:12,450
We dropped some of the nodes or the neuron data and Don pass it on but it's randomly dropped doing so

46
00:03:12,660 --> 00:03:18,880
improve the model's ability to generalize OK.

47
00:03:18,890 --> 00:03:26,150
So when these are the key a few key points reducing the network size I cover before I the managed to

48
00:03:26,150 --> 00:03:29,680
number of layers or managed a number of notes players.

49
00:03:30,110 --> 00:03:33,200
These are also core learnable parameters.

50
00:03:33,200 --> 00:03:36,940
This dictates the capacity of the model.

51
00:03:36,960 --> 00:03:39,120
Now this part is more art than science.

52
00:03:39,210 --> 00:03:49,400
Hans is a good idea to have a GP you or use or use cloud based computer to improve that with this part.

53
00:03:49,400 --> 00:03:55,240
I'm going to show you three examples or two examples I should say when the model is undersized and the

54
00:03:55,250 --> 00:03:56,240
model is too big.

55
00:03:56,600 --> 00:04:06,260
And using benchmark as a reference and for weight regularization you forced to wait to be restricted

56
00:04:06,290 --> 00:04:09,930
to within the range or constrain to within the range.

57
00:04:09,950 --> 00:04:13,730
This is done by applying a costume to the last function.

58
00:04:13,730 --> 00:04:14,990
What is this costume.

59
00:04:14,990 --> 00:04:16,820
There are two types of costume.

60
00:04:16,820 --> 00:04:23,030
There is the L1 regularization and the other one's called out to regularization.

61
00:04:23,030 --> 00:04:29,990
The difference between the two is the L1 is the absolute value L2 is the squared value.

62
00:04:30,080 --> 00:04:32,570
So let's look at L1 regularization.

63
00:04:32,740 --> 00:04:39,140
The costume is proportional to the absolute value of the weight coefficient meaning it is constraining

64
00:04:39,140 --> 00:04:48,360
it based on the absolute value whereby the second one is relative to the squared value but Dropout is

65
00:04:48,600 --> 00:04:53,760
the more common and and much more popular method.

66
00:04:53,910 --> 00:04:56,080
This is created by Hinton again.

67
00:04:56,280 --> 00:05:04,020
It's also called the godfather of AA and he's the one person that is persistent been persistent and

68
00:05:04,020 --> 00:05:05,880
pursuing his goal.

69
00:05:06,240 --> 00:05:13,650
I guess you can call him dogged as well but without him we will not have a two day drop out is the most

70
00:05:13,650 --> 00:05:16,350
commonly used applied to a layer.

71
00:05:16,650 --> 00:05:22,770
Basically you're randomly dropping meaning zero setting meaning setting the weight to zero a number

72
00:05:22,770 --> 00:05:25,190
of output features of the layer during training.

73
00:05:25,200 --> 00:05:26,320
Sorry not weights.

74
00:05:26,490 --> 00:05:29,020
It's in the output not the weight.

75
00:05:29,310 --> 00:05:35,310
The weight is retained but the output from layer to leave is not passed on meaning zero before I move

76
00:05:35,310 --> 00:05:37,180
on to the next layer.

77
00:05:37,260 --> 00:05:45,510
Now according to Charlotte dropout rate usually is set between zero point two and zero point five during

78
00:05:45,510 --> 00:05:49,500
the test time however no drop out is applied

79
00:05:52,370 --> 00:05:52,740
all right.

80
00:05:52,800 --> 00:06:04,230
Look at some examples here of of the so-called validation lost that I have extracted for you and let's

81
00:06:04,230 --> 00:06:05,760
look at this.

82
00:06:05,760 --> 00:06:06,480
What do you see.

83
00:06:06,480 --> 00:06:14,490
You have the plus blue plus is the original model when you look at the blue plus is consistently low

84
00:06:15,210 --> 00:06:23,940
and you notice that the smaller model is consistently higher meaning it has a larger validation lost.

85
00:06:23,940 --> 00:06:33,480
What that is saying is that the smaller model smaller neural network is not able to learn the the features

86
00:06:33,570 --> 00:06:35,860
or the richness of the features.

87
00:06:36,420 --> 00:06:40,350
In the data itself handset there is always a gap.

88
00:06:40,430 --> 00:06:42,870
It wasn't able to learn.

89
00:06:43,290 --> 00:06:50,190
Not only was it able not to learn from the training data in the validation data it wasn't able to actually

90
00:06:50,220 --> 00:06:51,350
generalize as well.

91
00:06:51,360 --> 00:06:56,370
It just wasn't able to to provide a better prediction.

92
00:06:56,700 --> 00:07:05,850
So that therein lies as hens I'm using validation laws rather than the training loss let's have a look

93
00:07:05,850 --> 00:07:07,780
an example of a bigger model.

94
00:07:07,860 --> 00:07:12,690
If you look at this bigger model this is zooming in it's the same original model that we're looking

95
00:07:12,690 --> 00:07:15,710
at except the scale is a bit different now.

96
00:07:16,200 --> 00:07:23,250
If you look at this what you can see is that the larger models start all with a better validation loss

97
00:07:23,370 --> 00:07:32,940
because the initial few epochs and it's able to actually well from if you look at this right you can

98
00:07:32,940 --> 00:07:33,620
see is that.

99
00:07:33,990 --> 00:07:38,090
Let me just use a pen so that you can follow along my argument.

100
00:07:38,130 --> 00:07:41,580
Let me just change the color to black because we have

101
00:07:44,410 --> 00:07:49,670
because we have let's use black right.

102
00:07:49,690 --> 00:07:54,190
This is the original model and this is the larger model.

103
00:07:54,190 --> 00:08:01,280
OK so the larger model is started off with a better value validation lost and then we have the crossover

104
00:08:01,510 --> 00:08:11,790
here where the original model goes down the validation lost for the larger model goes up and that's

105
00:08:11,790 --> 00:08:23,250
when the larger model start to all the fit the the the training data and then wasn't able to actually

106
00:08:26,540 --> 00:08:27,470
predict as well.

107
00:08:27,480 --> 00:08:31,430
And data that is not seen but that's not the key point of this light.

108
00:08:31,430 --> 00:08:38,720
What are the key slide that I wanted to actually highlight is that the larger model is able to learn

109
00:08:39,110 --> 00:08:47,330
the features better hence it actually has better lost up to around let's just call it a PoC.

110
00:08:47,960 --> 00:08:48,220
OK.

111
00:08:48,230 --> 00:08:55,220
So that's the benefit of having a bigger model is that it is able to learn the data better and also

112
00:08:55,370 --> 00:08:56,870
able to predict the better.

113
00:08:56,870 --> 00:09:05,720
So therein lies the contrast between the smaller model the original model as well as the larger model.

114
00:09:05,750 --> 00:09:15,000
Now if we look at the original was this the larger model training lost you'll notice that they are continuously

115
00:09:15,120 --> 00:09:22,440
falling it's continued to adapt and learn from the actual training data itself from here you cannot

116
00:09:22,440 --> 00:09:25,830
tell that it is actually over fitting.

117
00:09:25,830 --> 00:09:33,720
We know from this previous slide from the number 9 onwards epoch 9 onwards the larger model is already

118
00:09:33,720 --> 00:09:40,020
all fitting and hence you always have a validation data to inform you of that.

119
00:09:40,630 --> 00:09:40,880
OK.

120
00:09:40,890 --> 00:09:48,210
I also want to show you how you can actually perform weight regularization as well ignore those commented

121
00:09:48,270 --> 00:09:49,890
code.

122
00:09:50,670 --> 00:09:56,070
If you look at the data or rather the line of code.

123
00:09:56,070 --> 00:10:04,590
This is an out to regularization and how you actually apply the regularization is that you have this

124
00:10:04,590 --> 00:10:05,760
line here.

125
00:10:05,760 --> 00:10:11,910
Core kernel underscore regularly the equals to regularize is dot L2.

126
00:10:11,940 --> 00:10:13,470
This is L2.

127
00:10:13,550 --> 00:10:15,260
Okay so that's out too.

128
00:10:15,330 --> 00:10:20,340
And the the value that you use is zero point zero zero one.

129
00:10:20,340 --> 00:10:26,140
So this is l to zero point zero zero one.

130
00:10:26,810 --> 00:10:27,190
Okay.

131
00:10:27,210 --> 00:10:29,190
And the rest are exactly the same.

132
00:10:29,230 --> 00:10:35,330
Notice that this is obviously a larger model with 2048 note.

133
00:10:35,680 --> 00:10:44,110
Basically it means that you know in your neural network so you say you have tree input and you have

134
00:10:44,260 --> 00:10:51,520
to layer and then you have legislate 3 output and what you have here is it's basically two thousand

135
00:10:51,940 --> 00:10:54,360
neural network neural nodes.

136
00:10:54,680 --> 00:10:55,650
Okay neurons here.

137
00:10:55,660 --> 00:10:59,200
So religious from 1 all the way to 2048.

138
00:10:59,230 --> 00:11:01,820
So that's how many there is here now.

139
00:11:01,840 --> 00:11:04,510
So that's an example out to what about L1.

140
00:11:04,510 --> 00:11:11,320
This is an L1 and the actual difference between the both line and this is just that for this you have

141
00:11:11,350 --> 00:11:15,140
l 1 but you have to write it as it's lower cap again.

142
00:11:15,220 --> 00:11:17,030
Upper cap when you're actually writing the code.

143
00:11:17,050 --> 00:11:22,030
I'm just changing into upper case so that you can actually see that easily.

144
00:11:22,030 --> 00:11:25,780
So that's basically for L1 and for L1 and L2.

145
00:11:25,780 --> 00:11:32,950
This is what you do L1 on this go out to and L 1 is equal to zero point zero zero one.

146
00:11:32,950 --> 00:11:40,760
So basically you're actually providing the L1 the value that you want to regularize it for.

147
00:11:40,960 --> 00:11:42,820
So that's pretty much it.

148
00:11:43,150 --> 00:11:48,010
And you do apply regularize us for each of the layer as well.

149
00:11:48,010 --> 00:11:53,230
And apart from that the rest are exactly the same.

150
00:11:53,360 --> 00:11:55,580
I hope that as it's clear.

151
00:11:55,880 --> 00:12:03,090
Let's have a look at the application of weight regularization on the larger model.

152
00:12:03,290 --> 00:12:07,640
And if you look at this carefully this is validation lost.

153
00:12:07,640 --> 00:12:12,450
You have the L2 regularize model here and you notice that it.

154
00:12:13,160 --> 00:12:18,880
Well it's it's basically being forced constrain.

155
00:12:19,190 --> 00:12:22,710
So hopes.

156
00:12:22,770 --> 00:12:24,030
Sorry.

157
00:12:24,300 --> 00:12:28,860
This is the we'll just bring on the pen.

158
00:12:30,600 --> 00:12:36,790
So this is the L to regularize model and this hasn't actually overfed the data.

159
00:12:37,000 --> 00:12:42,340
In contrast to the larger model which is our own here and it jumps.

160
00:12:42,340 --> 00:12:49,900
It has of a fit but in this situation what happened is that because the regularization does apply here

161
00:12:49,990 --> 00:12:58,450
as constrain the model so much is that it did not perform as well as either the original or the regularized

162
00:12:58,450 --> 00:12:59,260
model.

163
00:12:59,380 --> 00:12:59,920
And.

164
00:13:00,520 --> 00:13:09,540
And this is the part whereby you need to tune the the regularization here is what's called a hyper parameter.

165
00:13:09,580 --> 00:13:16,210
It's not part of the neural network parameters hence is called hyper primitives.

166
00:13:16,240 --> 00:13:26,020
So if I look at dropout as as an alternative to applying weight regularization what do we see.

167
00:13:26,080 --> 00:13:26,290
Right.

168
00:13:26,320 --> 00:13:30,260
So we have a couple models once again some images bring out my pen.

169
00:13:30,280 --> 00:13:32,820
You have the green one is your larger model.

170
00:13:32,830 --> 00:13:37,430
You have to drop out let's look at the larger model there.

171
00:13:37,490 --> 00:13:42,390
This is the strict stop plus is your larger model.

172
00:13:43,240 --> 00:13:45,090
Okay let's just change color.

173
00:13:52,520 --> 00:13:53,980
OK so let's just draw this.

174
00:13:53,990 --> 00:14:01,300
This is our larger model unconstrained without any regularization or dropout applied.

175
00:14:01,310 --> 00:14:06,560
So let me just change to paint color and this time this use yellow.

176
00:14:06,620 --> 00:14:10,320
This is our dropout models started off didn't perform as well.

177
00:14:10,750 --> 00:14:11,060
Okay.

178
00:14:11,060 --> 00:14:14,430
And then the validation is better then the larger model.

179
00:14:14,510 --> 00:14:20,200
And around here is do better as was is better better.

180
00:14:20,270 --> 00:14:29,150
And notice that it continues to do better even as we allow the data the more data is being applied in

181
00:14:29,150 --> 00:14:35,740
fact when you look carefully and actually perform as well as the original model.

182
00:14:36,020 --> 00:14:42,200
And it seems to be stable of course you can continue to run this but this require a fair bit of tuning

183
00:14:42,230 --> 00:14:51,440
a fair bit of of you know playing around with it until you actually get a at an ideal or optimal neural

184
00:14:51,440 --> 00:14:51,860
network.

185
00:14:51,860 --> 00:15:02,240
That part takes time and I hope that this portion on dropout wait regularization has been useful and

186
00:15:02,240 --> 00:15:07,790
I hope that you've gained much all of this on how to manage and understood also what is all the fitting

187
00:15:07,790 --> 00:15:13,700
and under fitting and how to actually and what to do with it when you actually do come across this situation.

188
00:15:13,700 --> 00:15:15,080
Thank you once again for watching.

189
00:15:15,080 --> 00:15:16,910
I look forward to seeing you in the next video.
