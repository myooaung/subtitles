1
00:00:00,720 --> 00:00:07,380
Welcome back everyone to this lecture on Ellis T.M. and G are you so a common issue that recurrent neural

2
00:00:07,380 --> 00:00:13,260
networks face is that after a while especially if you're training the network on a really large sequence

3
00:00:13,650 --> 00:00:19,920
then the recurring neural network is going to begin to forget those very first inputs the later and

4
00:00:19,920 --> 00:00:24,900
later training batches of later inputs things that come towards the end of the text document that's

5
00:00:24,900 --> 00:00:30,240
going to start kind of overwriting the weights of the very beginning inputs of the beginning of the

6
00:00:30,240 --> 00:00:31,200
text.

7
00:00:31,200 --> 00:00:36,690
So we want to make sure is that we're not beginning to forget those first inputs as information is lost

8
00:00:36,900 --> 00:00:39,460
that each step going through their current neural network.

9
00:00:39,480 --> 00:00:43,730
So what we really need is we need some sort of long term memory for our networks.

10
00:00:43,720 --> 00:00:48,990
We've balanced both the short term memory of the networks the data that it was recently trained on versus

11
00:00:48,990 --> 00:00:54,510
the long term memory of the networks all the data and the very first data that was being trained on.

12
00:00:54,820 --> 00:01:00,240
So the Ellis team or long short term memory cell was created to help address these recurrent neural

13
00:01:00,240 --> 00:01:01,410
network issues.

14
00:01:01,410 --> 00:01:07,830
And while long short term memory cells aren't exactly state of the art they're still quite new considering

15
00:01:07,830 --> 00:01:12,420
that they've really only been around in Paris for just the past few years.

16
00:01:12,480 --> 00:01:15,960
So we're gonna do is go through how an Elysium cell works.

17
00:01:15,960 --> 00:01:18,380
Keep in mind there's actually a lot of mathematics here.

18
00:01:18,390 --> 00:01:22,530
There's really just a mathematical theory lecture where we're actually dealing with Caris it's gonna

19
00:01:22,560 --> 00:01:24,510
be a simple import call.

20
00:01:24,660 --> 00:01:27,180
So check out the resource link for a fool.

21
00:01:27,210 --> 00:01:32,250
Really great breakdown in fact that's a super common link when discussing Ellis Thames and then we'll

22
00:01:32,250 --> 00:01:35,110
also discuss the related G are you sell.

23
00:01:35,220 --> 00:01:40,380
But for our use case I really want you to focus on Ellis Tam because that's what's most commonly used

24
00:01:40,680 --> 00:01:41,860
for tech generation.

25
00:01:41,940 --> 00:01:47,980
So let's get started learning how this Ellis Tam cell actually works but quickly recall that a typical

26
00:01:47,980 --> 00:01:53,250
recurrent neuron basically has this sort of structure where we have some input at T minus one.

27
00:01:53,380 --> 00:02:00,930
Then we have the output a T minus one and that output is also fed back in along the input at T so as

28
00:02:00,930 --> 00:02:06,630
a quick note these outputs are often called Hidden so we can have instead of saying output at T minus

29
00:02:06,630 --> 00:02:06,960
one.

30
00:02:06,950 --> 00:02:12,420
We can say h of T minus one and then that also gets fitted with the input at t that gets passed through

31
00:02:12,420 --> 00:02:15,300
some sort of activation function like the hyperbolic tangent.

32
00:02:15,450 --> 00:02:21,210
Then that gives the output h of T etc. So when you see h of t just kind of think of that as the typical

33
00:02:21,240 --> 00:02:23,700
output of a recurrent neuron cell.

34
00:02:23,790 --> 00:02:29,430
All right so here we can see the entire model of a long short term memory cell and I know it can look

35
00:02:29,430 --> 00:02:34,140
really complicated if you see it in this format but it's actually not so bad when you break it down

36
00:02:34,170 --> 00:02:35,470
in parts.

37
00:02:35,610 --> 00:02:40,650
So let's go ahead and first take a look at what's being input and what's being output it here.

38
00:02:40,680 --> 00:02:44,120
We still have those original inputs from a normal recurrent neuron.

39
00:02:44,130 --> 00:02:47,160
Those h of T minus one and exit T.

40
00:02:47,160 --> 00:02:51,260
But note here we have this third input and we'll call that the cell state.

41
00:02:51,330 --> 00:02:57,360
So right now we're receiving the cell state at T minus one and then the outputs we again output h of

42
00:02:57,360 --> 00:03:00,120
T and then we also have the cell states.

43
00:03:00,150 --> 00:03:02,490
That's the current cell state C of T.

44
00:03:02,490 --> 00:03:07,830
So what we're going to end up doing is we're gonna take in h of T minus one and exit T as well as the

45
00:03:07,830 --> 00:03:14,670
previous cell state C of T minus one and we'll output h of T as well as the current cell state C of

46
00:03:14,670 --> 00:03:15,330
T.

47
00:03:15,330 --> 00:03:22,890
So we're gonna do this step by step so the very first step is called the forgets Gates layer and this

48
00:03:22,890 --> 00:03:29,010
is going to be the first step or we decide what information are we going to forget or throw away from

49
00:03:29,010 --> 00:03:30,450
the cell state.

50
00:03:30,450 --> 00:03:38,040
So what we end up doing here is you pass an h of T minus one and x of T and we pass that in after performing

51
00:03:38,040 --> 00:03:42,980
a linear transformation with some weights and biased terms into a sigmoid function.

52
00:03:42,990 --> 00:03:48,120
And remember because this is essentially a sigmoid layer it's always going to output a number between

53
00:03:48,180 --> 00:03:55,420
0 and 1 and a 1 is going to represent to keep it and a 0 is going to represent to forget about it or

54
00:03:55,440 --> 00:03:56,660
get rid of it.

55
00:03:56,670 --> 00:04:02,430
So if we think back to maybe a language model where we're trying to predict the very next word based

56
00:04:02,430 --> 00:04:08,550
on previous ones a cell state might include the gender of the present subject so that you in picking

57
00:04:08,550 --> 00:04:13,830
the correct pronoun so when you end up seeing a new subject you want to forget about the gender of the

58
00:04:13,890 --> 00:04:14,870
old subject.

59
00:04:14,910 --> 00:04:22,720
So that may be a use for a forget gate layer when working with a natural language as a sequence.

60
00:04:22,800 --> 00:04:28,640
Now the very next step is to decide what new information are you going to store in the cell state.

61
00:04:28,740 --> 00:04:30,580
So we order the what we're going to forget.

62
00:04:30,630 --> 00:04:34,300
Now we need to decide what are we actually going to store in the cell state.

63
00:04:34,320 --> 00:04:36,330
Remember that's C of T.

64
00:04:36,450 --> 00:04:38,970
So the first part is a sigmoid layer.

65
00:04:39,000 --> 00:04:41,480
And the second part is a hyperbolic tangent layer.

66
00:04:41,670 --> 00:04:45,120
So let's go ahead and tackle that first part that sigmoid layer.

67
00:04:45,330 --> 00:04:48,420
So let's sigmoid layer is called the input gate layer.

68
00:04:48,450 --> 00:04:52,230
So we'll say that's equal to I F T for input gate layer.

69
00:04:52,230 --> 00:04:57,180
And again we take an h of T minus 1 and x of t do a linear transformation on it.

70
00:04:57,180 --> 00:05:03,150
With they'll be a vi plus b a vi we pass that into a sigmoid function and again now we have a bunch

71
00:05:03,150 --> 00:05:06,250
of values between zero and ones.

72
00:05:06,270 --> 00:05:11,820
Then the second part of this is the hyperbolic tangent layer and that's again going to take h of T minus

73
00:05:11,820 --> 00:05:17,520
one and exit t do that linear transformation and then pass it through a hyperbolic tangent so that ends

74
00:05:17,520 --> 00:05:21,330
up creating a vector of what we call new candidate values.

75
00:05:21,330 --> 00:05:24,150
So that's that C of T with a little tilt above it.

76
00:05:24,600 --> 00:05:29,060
So these are candidate values that could be added to the state in the next step.

77
00:05:29,070 --> 00:05:33,900
We're going to combine these two to create an update to the cell state.

78
00:05:33,900 --> 00:05:38,910
So if we think back to an example of a language model we essentially want to add the gender of the new

79
00:05:38,910 --> 00:05:45,920
subject to the cell state and replace the old one that we've already decided we're forgetting so now

80
00:05:45,920 --> 00:05:48,560
it's time to update the old cell state.

81
00:05:48,590 --> 00:05:54,110
Remember the old cell state is C of T minus one and we eventually want to update that to the new cell

82
00:05:54,110 --> 00:06:01,080
state C of T so we can pass that on to the T plus one state of the cell.

83
00:06:01,460 --> 00:06:06,050
So in the previous steps we already decided what we're going to forget and we also already decided what

84
00:06:06,050 --> 00:06:07,190
we're going to store.

85
00:06:07,310 --> 00:06:11,670
So now what we need to do is just perform or execute these actions.

86
00:06:11,720 --> 00:06:19,100
So what we end up doing is we multiply the old state C of T minus one by F of T so we end up forgetting

87
00:06:19,100 --> 00:06:21,910
the things that we essentially decided we're going to forget.

88
00:06:22,010 --> 00:06:29,000
Based off that first sigmoid layer then we're going to add I have t the input gate layer times those

89
00:06:29,000 --> 00:06:31,400
candidate values C of t with tilled on top.

90
00:06:31,940 --> 00:06:37,130
So these are the new candidate values and now they're being scaled by how much we decided to up they

91
00:06:37,220 --> 00:06:39,050
each state value.

92
00:06:39,050 --> 00:06:43,790
So if we think back to a case of a language model this is where we would actually drop the information

93
00:06:44,090 --> 00:06:50,330
about that old subject's gender and add in new information based off what we decided in the previous

94
00:06:50,330 --> 00:06:51,850
steps.

95
00:06:51,860 --> 00:06:56,590
Now our final decision is going to be what do we output for h of T.

96
00:06:56,840 --> 00:06:59,500
So this output is gonna be based off your cell state.

97
00:06:59,540 --> 00:07:01,450
That is just a filter diversion.

98
00:07:01,550 --> 00:07:03,080
So it's actually pretty straightforward now.

99
00:07:03,080 --> 00:07:10,310
We just using h of T minus 1 and x of t we pass that after a linear transformation into the sigmoid

100
00:07:10,370 --> 00:07:15,720
layer and that's going to decide what parts of the cell state we're going to be outputting.

101
00:07:15,730 --> 00:07:21,560
Then we put the cell state through a hyperbolic tangent so that's gonna push the values to be between

102
00:07:21,650 --> 00:07:27,890
negative 1 and the 1 and we're going to then multiply it by the output of that sigmoid gate so that

103
00:07:27,890 --> 00:07:30,520
we only output the parts we decided to.

104
00:07:30,530 --> 00:07:33,180
So that's what's occurring here in this red line.

105
00:07:33,190 --> 00:07:38,390
Again we're taking h of T minus one and exit t doing a linear transformation on it passing it through

106
00:07:38,390 --> 00:07:44,090
the sigmoid and then once we have that output so we're going to say that's o of T output of T We're

107
00:07:44,090 --> 00:07:51,020
then going to multiply by the hyperbolic tangent of the C of T or that current cell state and that gives

108
00:07:51,020 --> 00:07:52,480
us h of T.

109
00:07:52,670 --> 00:07:56,610
So that's how an Ellis team cell works.

110
00:07:56,740 --> 00:07:59,360
Now there's different variants on an Alice T.M. cell.

111
00:07:59,380 --> 00:08:02,950
So there's a variance called the peephole variance.

112
00:08:03,040 --> 00:08:05,440
So this is actually quite popular.

113
00:08:05,530 --> 00:08:09,970
And the reason for that is because it adds peepholes to all the gates.

114
00:08:09,970 --> 00:08:20,200
So basically it allows these FTI of T and o of t to be able to see the previous cell state or C of T

115
00:08:20,200 --> 00:08:21,000
minus one.

116
00:08:21,040 --> 00:08:26,950
So you can see here now instead of being a function of just h of T minus one an X of T we're also passing

117
00:08:26,950 --> 00:08:32,530
in C of T minus one and then for the output we're also passing in a sea of T directly.

118
00:08:32,650 --> 00:08:39,340
So that's called an Ellis team with peoples another variation of the Ellis T.M. cell is called the gated

119
00:08:39,340 --> 00:08:41,260
recurrent unit or G are you.

120
00:08:41,350 --> 00:08:46,540
And this was actually introduced quite recently around 2014 and what it ends up doing is it actually

121
00:08:46,540 --> 00:08:53,260
simplifies things a bit by combining the forget and input gates into a single what they call update

122
00:08:53,260 --> 00:08:53,880
gates.

123
00:08:53,950 --> 00:08:58,110
And it also merges the cell state and hidden state and it makes a few other changes.

124
00:08:58,120 --> 00:09:04,450
So this resulting model is actually simpler than standard Ellis models and because of that it's been

125
00:09:04,450 --> 00:09:08,550
growing increasingly popular over the last few years that it's been around.

126
00:09:08,650 --> 00:09:13,600
So there's actually lots of other slight variations off of this and they're being introduced all the

127
00:09:13,600 --> 00:09:19,630
time there was another one called depth gated recurrent neural networks and that was introduced in 2015.

128
00:09:19,780 --> 00:09:24,310
And I mean who knows maybe in a few years we'll see another variation that further improves on this

129
00:09:24,310 --> 00:09:25,060
model.

130
00:09:25,060 --> 00:09:31,300
But the main idea is to understand how Ellis teams work and that allows you to quickly learn how these

131
00:09:31,300 --> 00:09:32,680
variations work off of it.

132
00:09:32,770 --> 00:09:33,140
All right.

133
00:09:33,190 --> 00:09:39,350
Now that we understand Ellis teams and G use we can now use them with Caris.

134
00:09:39,370 --> 00:09:44,870
Now fortunately Caris has a super nice API that makes Ellis Tam and RNA and really easy to work with.

135
00:09:45,040 --> 00:09:50,130
And typically from the research I've done protects generation Ellis teams work best.

136
00:09:50,140 --> 00:09:51,860
So that's what we're going to be using.

137
00:09:51,940 --> 00:09:56,960
Coming up next we're gonna take a several part process in order to learn how to format our data.

138
00:09:56,980 --> 00:09:59,430
That is our text data for recurrent neural networks.

139
00:09:59,440 --> 00:10:04,060
And again Caris has a lot of convenient built in tools for this and then we'll see how to use Ellis

140
00:10:04,060 --> 00:10:05,940
TAM for tech's generation.

141
00:10:05,980 --> 00:10:08,830
This is a really awesome project so I hope you're super excited for it.

142
00:10:09,190 --> 00:10:10,240
I'll see you at the next lecture.

