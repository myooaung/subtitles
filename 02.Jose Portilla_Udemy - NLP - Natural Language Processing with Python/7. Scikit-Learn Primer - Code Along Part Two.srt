1
00:00:05,310 --> 00:00:11,310
Welcome back here we are where we left off last time fitting our model to our ex trainee and white training

2
00:00:11,310 --> 00:00:12,410
data set.

3
00:00:12,420 --> 00:00:18,330
Let's go ahead and learn how to test the accuracy of the model using the test data set to do this.

4
00:00:18,330 --> 00:00:25,060
We're going to say from S-K learn import metrics.

5
00:00:25,290 --> 00:00:29,860
Then we're going to grab the features from the test data set.

6
00:00:29,880 --> 00:00:32,340
Remember that was X test.

7
00:00:32,340 --> 00:00:38,130
Now we're going to predict on this the exact same way we would predict on brand new data we would simply

8
00:00:38,130 --> 00:00:45,470
call our train model called up predict and we passen new data that the model hasn't seen before.

9
00:00:45,530 --> 00:00:50,990
In this case for evaluation we're going to pass an X test against his brand new data the model hasn't

10
00:00:50,990 --> 00:00:51,830
seen before.

11
00:00:52,040 --> 00:00:57,970
But lucky for us we know the correct answers the correct answers are why test.

12
00:00:58,060 --> 00:01:02,520
So we're going to do is pass an X test and we're going to ask the model to predict on this test set

13
00:01:03,070 --> 00:01:04,820
and then see what their predictions are.

14
00:01:06,370 --> 00:01:09,360
So now we have predictions that look like this.

15
00:01:09,370 --> 00:01:15,790
HAM HAM and et cetera there's some spam in there and we have the true values the true values are why

16
00:01:15,790 --> 00:01:18,660
test and here are these true values.

17
00:01:18,670 --> 00:01:24,190
So now we're going to compare our predictions against Why test just as we explain in the slides during

18
00:01:24,190 --> 00:01:25,540
the previous lectures.

19
00:01:25,540 --> 00:01:27,190
There's various ways we can do this.

20
00:01:27,190 --> 00:01:35,130
One way is actually build out the confusion matrix ourselves so we could say metrics dots and then call

21
00:01:35,130 --> 00:01:43,950
confusion matrix and then pasan why test and predictions and then print out the result of this and we

22
00:01:43,950 --> 00:01:46,520
actually get back our confusion matrix.

23
00:01:46,560 --> 00:01:48,790
So our confusion matrix looks like this.

24
00:01:48,870 --> 00:01:54,090
And your confusion matrix may look slightly different depending on your train test split or how you

25
00:01:54,090 --> 00:01:55,590
actually train your model.

26
00:01:55,620 --> 00:02:01,730
But here we can see that overall it looks like we're not able to predict well on spam and you could

27
00:02:01,740 --> 00:02:06,830
actually make the confusion matrix a little better by passing it into a data frame.

28
00:02:07,050 --> 00:02:11,870
And I'm going to copy and paste the line of code that does this from the notes.

29
00:02:12,270 --> 00:02:16,290
So the line of code that does it's from the notes again you can just open up the notebook and scroll

30
00:02:16,290 --> 00:02:19,290
down until you get to test accuracy of the model.

31
00:02:19,290 --> 00:02:24,360
It's right here it is the line I just copied but we'll come back here and we're just passing in this

32
00:02:24,360 --> 00:02:31,660
confusion matrix into a Pandurs data frame and then we're labeling the index in the columns HAMOND spam.

33
00:02:31,830 --> 00:02:37,680
That way if actually the splay this data frame that I could see the labeled confusion matrix.

34
00:02:37,680 --> 00:02:43,200
So it looks like I only correctly classified 5 for the spam.

35
00:02:43,440 --> 00:02:49,890
And here you can see incorrectly classified 44 spam that were actually him and I incorrectly classify

36
00:02:49,960 --> 00:02:58,020
219 spam as him and I correctly classified 1404 for my test dataset correctly as him.

37
00:02:58,020 --> 00:02:59,890
So these results are definitely not good.

38
00:02:59,900 --> 00:03:04,680
Now we're going to get much better results when we take into account the text data set itself.

39
00:03:04,680 --> 00:03:08,660
That means that it's to say the actual text messages the raw text data.

40
00:03:08,670 --> 00:03:13,290
But we haven't learned about text feature extraction yet so we're going to leave that aside for now.

41
00:03:13,290 --> 00:03:15,810
Next I want to show you besides the confusion matrix.

42
00:03:15,810 --> 00:03:22,710
You can also print out a classification report and the way you do this is you say metric starts and

43
00:03:22,710 --> 00:03:29,940
then you call classification report and again you pass your test against the predictions that you made

44
00:03:30,920 --> 00:03:31,850
and you print this out.

45
00:03:33,290 --> 00:03:36,560
And you get back this classification report which tells you precision.

46
00:03:36,580 --> 00:03:39,580
Recall an EF 1 score for each of the categories.

47
00:03:39,600 --> 00:03:45,000
So here you can see I'm getting pretty good precision and recall on my ham but very poor precision and

48
00:03:45,000 --> 00:03:46,820
recall on the spam.

49
00:03:46,830 --> 00:03:54,180
So overall my model is pretty good at the testing Hahm but it's horrible at detecting spam text messages

50
00:03:54,720 --> 00:04:02,710
and I can print out the overall accuracy of the model by saying metrics accuracy score and then passing

51
00:04:02,950 --> 00:04:11,940
tests and predictions and I can print these out and here I see my model is eighty four point three percent

52
00:04:11,940 --> 00:04:13,110
accuracy.

53
00:04:13,110 --> 00:04:18,270
Now what you can also do is choose different machine learning models and see those perform better.

54
00:04:18,270 --> 00:04:24,360
Right now we've only ran a logistic regression model but we could try other models and we do that just

55
00:04:24,360 --> 00:04:26,640
to show you the syntax is actually the same.

56
00:04:26,700 --> 00:04:32,550
You'll say from S-K learn thought and then choose a family of models and to decide what family models

57
00:04:32,550 --> 00:04:37,430
to look at you can just look at the sikat learn documentation but you would say something like naive

58
00:04:37,440 --> 00:04:39,230
Beys import.

59
00:04:39,630 --> 00:04:48,550
And then we're going to use a multinomial Navy base model here it will say and B model is equal to multinomial

60
00:04:48,580 --> 00:04:53,210
and b c a very common classifier for text data sets.

61
00:04:53,500 --> 00:05:01,660
And then just like before we take our model now we fit it to the training data so we pass it x train

62
00:05:01,720 --> 00:05:02,810
and y train.

63
00:05:03,070 --> 00:05:13,040
And just as before we say predictions is equal to our model and then we predict off the excess data

64
00:05:13,040 --> 00:05:24,910
set and then we can printout the confusion matrix we can say confusion matrix and then pasan why test

65
00:05:25,030 --> 00:05:27,490
against these new predictions.

66
00:05:27,490 --> 00:05:33,670
So notice here how this code essentially looks all was exactly the same as the code we use for the logistic

67
00:05:33,670 --> 00:05:34,610
regression here.

68
00:05:34,810 --> 00:05:40,870
We just imported the model create an instance of the model fit the model and then predict using the

69
00:05:40,870 --> 00:05:41,320
model.

70
00:05:41,350 --> 00:05:44,410
And those are really the key steps we want to understand for this lecture.

71
00:05:44,590 --> 00:05:50,790
The fact that we can basically use the same syntax for a wide variety of models with sikat learn import

72
00:05:50,800 --> 00:05:56,290
the model create an instance of it fit the model and then predict what the model and you can use those

73
00:05:56,290 --> 00:05:59,360
predictions with the confusion matrix per classification report.

74
00:06:00,930 --> 00:06:04,850
So running this we see this model also does not perform well.

75
00:06:04,920 --> 00:06:09,700
In fact now we're not able to quickly than to fight any of the spam which means we're essentially doing

76
00:06:09,740 --> 00:06:13,250
zero precision and zero recall for spam.

77
00:06:13,350 --> 00:06:23,160
So we can actually prove this by printing out metrics or Doc classification report and saying why test

78
00:06:23,190 --> 00:06:29,430
and predictions run that and the spam we're failing to detect any spam correctly which is denoted here

79
00:06:29,460 --> 00:06:30,770
by that 0.

80
00:06:30,780 --> 00:06:36,010
So let's try another model and often you can experiment with various models quite easily.

81
00:06:36,060 --> 00:06:43,670
Like learn we can say from S-K learn let's try a vector machine model we're going to import as we see

82
00:06:43,690 --> 00:06:52,130
a support vector classification model will say as a VC model is equal to as we see in this case and

83
00:06:52,160 --> 00:06:57,990
only one of the parameters you can always shift tab on these models after you've imported them so you

84
00:06:57,990 --> 00:07:02,910
have to run this import line first and then you're able to shift tab here in order to see the various

85
00:07:02,910 --> 00:07:03,830
parameters.

86
00:07:03,870 --> 00:07:07,010
One of the things going to change here is just make sure that gamma is set to auto.

87
00:07:07,010 --> 00:07:08,140
It's actually at the fault.

88
00:07:08,250 --> 00:07:10,370
So in that case I don't need to set it.

89
00:07:10,380 --> 00:07:12,780
But if you want a different gamma you could.

90
00:07:13,140 --> 00:07:15,650
So there's Gamma's equal to auto default values.

91
00:07:15,750 --> 00:07:24,960
And then I going say support vector model and I'm going to fit it to the training data and just like

92
00:07:24,960 --> 00:07:34,020
before we're going to see predictions as equal to as we see model predict of X test and then print out

93
00:07:34,110 --> 00:07:37,800
the off of metrics.

94
00:07:37,800 --> 00:07:44,560
The confusion matrix passing in Y test and passing them predictions again the exact same process for

95
00:07:44,560 --> 00:07:46,580
a completely different machine learning model.

96
00:07:46,650 --> 00:07:51,030
If you understand these five lines of code then you basically understand the main point of the series

97
00:07:51,030 --> 00:07:57,510
of lectures for the syntax of psychic learn you import the model create the model fit the model and

98
00:07:57,510 --> 00:08:01,250
then predict using the model and then evaluate your predictions.

99
00:08:01,260 --> 00:08:05,900
So if you run this you're able to see how well the support vector classifier worked.

100
00:08:06,090 --> 00:08:06,910
OK.

101
00:08:06,990 --> 00:08:13,890
So again keep in mind this is really just the syntax we need to know and the way we can test our train

102
00:08:13,890 --> 00:08:19,470
models on new data simply by calling that predict and then getting those predictions.

103
00:08:19,470 --> 00:08:23,940
Now what we haven't learned yet is if we go all the way back to the top Remember we still have the actual

104
00:08:24,030 --> 00:08:25,730
text message itself.

105
00:08:25,740 --> 00:08:30,690
What we need to learn is how to actually extract features from the text information because right now

106
00:08:30,960 --> 00:08:36,780
the length and punctuation that numerical data is not enough to make an accurate assessment or an accurate

107
00:08:36,780 --> 00:08:37,510
model.

108
00:08:37,530 --> 00:08:41,370
We need to somehow convert this text message into numerical information.

109
00:08:41,400 --> 00:08:45,070
So coming up next we're going to learn about feature extraction from text.

110
00:08:45,090 --> 00:08:45,700
We'll see if they're.

