WEBVTT
ï»¿1
00:00:00.240 --> 00:00:01.360
Welcome back everyone.

2
00:00:01.440 --> 00:00:06.780
In this lecture we're going to show you how to use voter sentiment analysis with Python and L2.

3
00:00:07.240 --> 00:00:10.030
OK let's begin by importing an el T.K..

4
00:00:10.740 --> 00:00:15.910
And then what you need to do is download the Vader lexicon and you only need to do this once.

5
00:00:16.200 --> 00:00:25.580
So we'll say an 80 k download and then pass in Vader underscore lexicon.

6
00:00:25.760 --> 00:00:26.950
Go ahead and run that.

7
00:00:27.140 --> 00:00:30.690
And then if you've already downloaded it it should be almost instantaneous.

8
00:00:30.800 --> 00:00:34.100
If not it may take a little bit of time to download the data from the Internet.

9
00:00:34.100 --> 00:00:38.020
Make sure you check your firewall and make sure that's not blocking your download.

10
00:00:38.030 --> 00:00:43.600
So once you've downloaded the Vader lexicon you should be able to import it using from an A.K..

11
00:00:43.610 --> 00:00:55.260
That sentiment Darth Vader import and then it's sentiment intensity analyzer and then create an instance

12
00:00:55.260 --> 00:00:55.780
of it.

13
00:00:56.010 --> 00:00:56.880
We call it S.A.T.

14
00:01:00.020 --> 00:01:05.480
and what Vader's sentiment intensity analyzer does is it simply takes in the string and returns a dictionary

15
00:01:05.480 --> 00:01:12.200
of scores in four categories negative neutral positive and then a compound score which is computed by

16
00:01:12.200 --> 00:01:15.190
normalizing the negative neutral and positive scores.

17
00:01:15.260 --> 00:01:17.310
So it's create a really simple string.

18
00:01:17.390 --> 00:01:27.000
We'll say this is a good movie and then we'll say essay The Dot and off of this we will call polarity

19
00:01:27.030 --> 00:01:33.840
underscore scores and then pass in the string and you get back this dictionary which has some negative

20
00:01:33.840 --> 00:01:39.570
value a neutral value a positive value and then a compound value which essentially normalizing these

21
00:01:39.570 --> 00:01:40.880
three values here.

22
00:01:40.890 --> 00:01:44.870
So as we expect there is no negative value since it's this is a good movie.

23
00:01:44.970 --> 00:01:49.290
It has some neutral words or tones in it and then it has also some positive tones.

24
00:01:49.380 --> 00:01:53.220
And the max value for any of these is one point zero.

25
00:01:53.220 --> 00:01:56.490
So now let's try a more complicated string.

26
00:01:56.610 --> 00:02:07.140
We'll say this was the best comma most awesome movie and then we're going to capitalize ever made and

27
00:02:07.140 --> 00:02:08.690
have three exclamation points.

28
00:02:08.790 --> 00:02:14.070
And as we previously mentioned Vader is smart enough to understand things like repeated punctuation

29
00:02:14.610 --> 00:02:16.230
and capitalization.

30
00:02:16.230 --> 00:02:17.580
So go ahead and run that.

31
00:02:17.580 --> 00:02:21.280
And then again call aside the polarity scores.

32
00:02:21.390 --> 00:02:23.040
Make sure we spell that right.

33
00:02:23.220 --> 00:02:24.340
Pass on the string.

34
00:02:24.540 --> 00:02:28.910
And here we can see it's again more positive than the previous one.

35
00:02:29.220 --> 00:02:34.420
And we can see here that the compound score is much more positive because neutral also dropped.

36
00:02:34.500 --> 00:02:37.430
Finally let's go ahead and have a very negative string.

37
00:02:37.440 --> 00:02:47.560
So we'll say this was the worst movie I have ever or that has ever disgraces seen.

38
00:02:47.600 --> 00:02:56.120
Let's say that has ever disgraced disgraced the screen OK.

39
00:02:56.160 --> 00:02:57.900
So quite a negative review.

40
00:02:57.900 --> 00:03:04.040
Let's see if the Vader picks it up we'll say S.A.T. polarity scores pass an A.

41
00:03:04.240 --> 00:03:09.430
And here we can see that now there is no positive it's just neutral and negative and so happens is the

42
00:03:09.430 --> 00:03:15.400
compound score then becomes negative so we can see here a compound score of zero would be completely

43
00:03:15.400 --> 00:03:15.960
neutral.

44
00:03:16.270 --> 00:03:22.420
A compound score above zero indicates some sort of positive score and then a compound score below zero

45
00:03:22.540 --> 00:03:24.870
indicates some sort of negative score.

46
00:03:25.030 --> 00:03:30.160
So that we're going to do is show you how you can use Vader to analyze Amazon reviews.

47
00:03:30.190 --> 00:03:35.260
So in our text files folder we have an amazon reviews the TSC file.

48
00:03:35.260 --> 00:03:38.560
I went ahead and moved it to the same location as my notebook here.

49
00:03:38.860 --> 00:03:45.470
And we're going to read it in using pandas will say import pandas as PD and they will say ADF is equal

50
00:03:45.470 --> 00:03:50.440
to PD read CSB and already moved the file here.

51
00:03:50.450 --> 00:03:53.500
So it's simply called Amazon reviews that TSB.

52
00:03:53.510 --> 00:03:58.870
It's again located under text files folder and it's tab separated.

53
00:03:58.940 --> 00:04:04.140
So you need to also indicate that the separator is backslash T for Tab separation.

54
00:04:04.190 --> 00:04:08.400
Once you read that in you should be able to view it by simply calling the head of that data frame.

55
00:04:08.570 --> 00:04:10.560
And essentially what we have here are labels.

56
00:04:10.600 --> 00:04:15.260
They see their process for positive or energy for a negative and then we have the actual text of the

57
00:04:15.260 --> 00:04:16.100
review.

58
00:04:16.100 --> 00:04:22.400
So if we wanted to get an idea of how many positive or negative labels we want we have we can say the

59
00:04:22.400 --> 00:04:29.630
F pass passing the label column and then simply call value underscore counts and we can see here we

60
00:04:29.630 --> 00:04:35.250
have slightly more negative reviews than positive reviews but overall it looks like we have around 10000

61
00:04:35.660 --> 00:04:36.520
reviews.

62
00:04:36.680 --> 00:04:42.110
So what we're gonna do now is do a little bit of cleaning of the data just to double check that we have

63
00:04:42.110 --> 00:04:47.150
no empty records and then we're going to run a first review through Vader.

64
00:04:47.150 --> 00:04:52.460
So quite simply what we're gonna do here is simply say the F that drop N A.

65
00:04:52.580 --> 00:04:55.960
And we'll say in place is equal to true.

66
00:04:56.060 --> 00:05:02.060
That's going to drop anything that's missing and then we're going to do is drop anything that has a

67
00:05:02.150 --> 00:05:04.250
empty whitespace value.

68
00:05:04.250 --> 00:05:08.600
Now for your data sets depending where you get them you may or may not have this but it's always a good

69
00:05:08.600 --> 00:05:08.950
idea.

70
00:05:08.960 --> 00:05:12.480
So I'm just saying for index for label and for review.

71
00:05:12.500 --> 00:05:20.510
So those are kind of place holders there and Def dots and then I'm going to call it or tuples so essentially

72
00:05:20.510 --> 00:05:27.140
here everything is just going to be returned as a tuple where I have the index the label and then the

73
00:05:27.140 --> 00:05:28.820
review text.

74
00:05:28.820 --> 00:05:39.910
So for I label and review I'm going to say if the type of the review is equal to the string type then

75
00:05:39.910 --> 00:05:47.480
I'm going to check that the review is space essentially checking whether or not it's a space there.

76
00:05:47.860 --> 00:05:55.410
And if it's true I'm going to take a list of blanks which will go ahead and define it will say blanks

77
00:05:56.260 --> 00:06:01.510
and we've already seen this kind of thing before but I'll simply say blanks append and then we'll plan

78
00:06:01.570 --> 00:06:03.330
that index position.

79
00:06:03.430 --> 00:06:06.000
So if we run this let's go ahead and check on blanks.

80
00:06:06.010 --> 00:06:07.340
See if we had any blanks.

81
00:06:07.450 --> 00:06:08.140
It looks like we did it.

82
00:06:08.230 --> 00:06:09.010
This list is empty.

83
00:06:09.010 --> 00:06:14.860
So we don't need to drop anything but if we did have some index positions that were brand blanks we

84
00:06:14.860 --> 00:06:21.590
simply need to say DLF drop blanks and then we could say in place is equal to true.

85
00:06:21.730 --> 00:06:25.360
But again since we don't have any we don't actually need to run that line.

86
00:06:25.360 --> 00:06:30.490
So now we're going to do is continue on and run a first review through Vader.

87
00:06:30.670 --> 00:06:38.630
So say SD called polarity scores and we're going to just run the first review on it so we'll say DFT

88
00:06:38.630 --> 00:06:46.170
dot and we'll call I 0 and then call that particular review.

89
00:06:46.250 --> 00:06:51.620
So all I'm doing is if we were to check out this one line here that's essentially just grabbing the

90
00:06:51.620 --> 00:06:54.620
text of the first review so we can see here it's quite positive.

91
00:06:54.620 --> 00:07:01.040
The soundtrack was beautiful game music exclamation points best music etc. so shook up the polarity

92
00:07:01.040 --> 00:07:06.620
score here and then we get back this dictionary and it looks like it has a very small amount of negativity

93
00:07:06.980 --> 00:07:13.040
that Vader picked up and it could be small phrases that get confusing for Vader things like anyone who

94
00:07:13.040 --> 00:07:18.200
cares to listen may be kind of negative in a slight sense but it's actually a very small negativity.

95
00:07:18.320 --> 00:07:23.690
In fact most of it is neutral or slightly positive which means a compound score is extremely positive

96
00:07:23.990 --> 00:07:28.650
which if we take a look at that first label was positive so the first few reviews are positive.

97
00:07:28.730 --> 00:07:31.520
So looks like Vader is actually able to select that.

98
00:07:32.090 --> 00:07:35.740
So now let's go ahead and as scores and labels to the data frame.

99
00:07:35.900 --> 00:07:46.860
So we're going to say the F scores is equal to def review and then we're going to call in apply method

100
00:07:47.310 --> 00:07:54.000
in order to essentially apply SD polarity scores to every single review in our data frame so we'll say

101
00:07:54.720 --> 00:08:02.630
lambda take that review and then apply SD polarity scores to that particular review.

102
00:08:02.980 --> 00:08:07.270
So we run that and this may take a little bit of time because it is running this whole polarity of course

103
00:08:07.270 --> 00:08:08.830
function on every single review.

104
00:08:09.010 --> 00:08:13.300
But once you have that you can go ahead and check out the ahead of the data frame and then you'll get

105
00:08:13.300 --> 00:08:18.730
back in your column scores that contains this dictionary and often you really just want to be of the

106
00:08:18.730 --> 00:08:19.780
compound score.

107
00:08:19.810 --> 00:08:25.240
So let's go ahead and create a new compound column we'll say the F compound and then we'll set that

108
00:08:25.240 --> 00:08:30.100
equal to the F scores and similar idea here.

109
00:08:30.100 --> 00:08:37.270
We're simply going to apply a lambda and we're gonna pass in that dictionary and then just gram compound

110
00:08:37.350 --> 00:08:38.410
off of that dictionary.

111
00:08:38.410 --> 00:08:43.050
So every item in scores is a dictionary source since just reading the score for compound.

112
00:08:43.180 --> 00:08:45.370
So we'll go ahead and run that.

113
00:08:45.910 --> 00:08:49.350
That's going to be much faster since the dictionary is just existing there.

114
00:08:49.690 --> 00:08:54.260
And then we can get the compound score so notice these first five labels they're all positive and it

115
00:08:54.270 --> 00:08:56.650
looks like the compound score is also all positive.

116
00:08:57.040 --> 00:09:02.620
So let's go ahead and based off this compound score do a little bit of logic and say if it's greater

117
00:09:02.620 --> 00:09:04.450
than zero then it's positive.

118
00:09:04.450 --> 00:09:10.090
If it's less than zero it's negative and then we'll compare these compound scores to the true labels

119
00:09:10.090 --> 00:09:18.670
that we already know so we're going to say one last column of our creation comp score and say it's equal

120
00:09:18.670 --> 00:09:27.760
to D F compound we're going to apply essentially a little bit of logic here.

121
00:09:27.810 --> 00:09:35.400
We'll take in that score and we'll say return back the string peo us for positive.

122
00:09:35.660 --> 00:09:43.760
If the score is greater than or equal to zero else return the string negative.

123
00:09:44.090 --> 00:09:48.950
Essentially changing the score into a string that matches our current label.

124
00:09:48.950 --> 00:09:53.010
So if we run this and then again check out the head of our data frame.

125
00:09:53.030 --> 00:09:54.670
Now we have a label.

126
00:09:54.770 --> 00:10:01.520
The review the full scores dictionary the compound offer that scores dictionary and our converted compound

127
00:10:01.850 --> 00:10:03.860
to a score or a label.

128
00:10:03.920 --> 00:10:06.980
P.S. so looks like we're matching up on the first five.

129
00:10:07.110 --> 00:10:13.520
But let's go ahead and have an overall report on the accuracy comparing the Vader compound score labels

130
00:10:13.880 --> 00:10:23.240
to the manual labels from this dataset and we can do that simply by saying from SDK learn that metrics

131
00:10:23.630 --> 00:10:31.820
import and we'll do an accuracy score a classification report as well as a confusion matrix.

132
00:10:31.910 --> 00:10:33.680
So we've already seen quite a few of these already.

133
00:10:33.680 --> 00:10:42.760
Let's first just get the accuracy score and we can do that by simply saying the F label sets the correct

134
00:10:42.820 --> 00:10:46.630
value compared to the F comp score.

135
00:10:46.630 --> 00:10:52.590
So essentially we're comparing how well that Vader perform against what was manually labeled.

136
00:10:52.600 --> 00:10:57.280
So this first label was manual label essentially a person read these reviews and decided whether or

137
00:10:57.280 --> 00:10:59.430
not they're positive or negative.

138
00:10:59.500 --> 00:11:04.270
So if we run their accuracy score we get an accuracy of zero point seven.

139
00:11:04.300 --> 00:11:09.400
If we were to randomly choose positives and negatives we'd be probably getting an accuracy score of

140
00:11:09.400 --> 00:11:10.720
around zero point five.

141
00:11:10.840 --> 00:11:15.460
So we can see we're doing better than random guessing which is quite good given the fact that we're

142
00:11:15.460 --> 00:11:20.130
essentially just running one line of code appear of inside the polarity scores.

143
00:11:20.140 --> 00:11:25.420
So it's definitely not bad considering how simple it is to run this process.

144
00:11:25.420 --> 00:11:30.650
Let's go ahead and print the classification report we'll say print classification report.

145
00:11:30.650 --> 00:11:38.480
And just like before we'll pass in the true label that we know and then our calculated comp score so

146
00:11:38.480 --> 00:11:45.500
we'll run this and then we can see our precision recall an F1 score and we can also compare it to versus

147
00:11:45.500 --> 00:11:47.000
negative versus positive.

148
00:11:47.000 --> 00:11:54.030
So it looks like the Vader has a little bit of trouble with negative reviews versus positive reviews.

149
00:11:54.110 --> 00:12:00.250
And if you take a look at some of these Amazon reviews some of these strings and some of the text is

150
00:12:00.260 --> 00:12:05.600
sometimes a bit hard to read and sometimes it's also sarcastic which means it's really hard to detect

151
00:12:05.740 --> 00:12:09.830
so sarcasm is almost impossible to detect for something like Vader.

152
00:12:09.830 --> 00:12:17.490
And then finally let's print out a confusion matrix so say confusion matrix and then D F label and pass

153
00:12:17.490 --> 00:12:19.880
in what Vader said.

154
00:12:19.910 --> 00:12:26.090
So the comp score here and here we can see our confusion matrix where we have the correctly classified

155
00:12:26.090 --> 00:12:30.160
instance a positive and negative versus the incorrectly classified.

156
00:12:30.290 --> 00:12:33.640
So you can see here again it's not performing well with the negative Varese.

157
00:12:34.120 --> 00:12:34.760
OK.

158
00:12:34.940 --> 00:12:40.520
So this does tell us that Vader correctly identified in Amazon review as positive or negative roughly

159
00:12:40.520 --> 00:12:43.020
around 71 percent of the time.

160
00:12:43.040 --> 00:12:46.450
That's definitely not bad considering how simple the process is.

161
00:12:46.550 --> 00:12:52.460
But it's also not excellent compared to maybe some state of the art deep learning methods for sentiment

162
00:12:52.550 --> 00:12:58.110
analysis but overall for simple import they can quickly apply to any text dataset.

163
00:12:58.130 --> 00:12:59.590
It's quite good.

164
00:12:59.600 --> 00:13:00.270
OK.

165
00:13:00.290 --> 00:13:03.890
Coming up next we'll go ahead and run a sentiment analysis project.

166
00:13:03.890 --> 00:13:04.460
We'll see you there.

