WEBVTT
﻿1
00:00:05.670 --> 00:00:12.080
Welcome back to this lecture on parts of speech basics so I've already seen a little bit of parts of

2
00:00:12.080 --> 00:00:13.970
speech in previous lectures.

3
00:00:13.970 --> 00:00:19.730
But let's go and take a deeper dive into POS or parts of speech and what that really means and how we

4
00:00:19.730 --> 00:00:23.100
can use space to actually grab parts of speech tags.

5
00:00:23.180 --> 00:00:29.450
It's important to understand that most words themselves are actually rare in occurrence inside the context

6
00:00:29.510 --> 00:00:35.330
of an entire document and it's common for words that look completely different to mean almost the same

7
00:00:35.330 --> 00:00:40.400
thing and the same words in a different order can actually then and that meaning something completely

8
00:00:40.400 --> 00:00:41.300
different.

9
00:00:41.300 --> 00:00:46.460
So really there's a lot of nuance to the way that words are being used and that is why we need to also

10
00:00:46.460 --> 00:00:51.300
look at the part of speech that word is being used in not just the word itself.

11
00:00:52.900 --> 00:00:58.690
So when you're doing things like splitting text into useful word like units like tokenization that itself

12
00:00:58.690 --> 00:01:00.580
can be difficult in many languages.

13
00:01:00.580 --> 00:01:05.680
And while it's possible to solve some problems starting from only the raw characters or those raw tokens

14
00:01:06.070 --> 00:01:12.100
it's usually better to use linguistic knowledge to add a useful information to know if you mean a noun

15
00:01:12.130 --> 00:01:17.860
versus a verb versus an adjective that part of speech information is really crucial to taking your natural

16
00:01:17.860 --> 00:01:20.100
language processing to the next level.

17
00:01:21.380 --> 00:01:23.610
So that's exactly what space is designed to do.

18
00:01:23.660 --> 00:01:29.090
You put in the raw text and you get back this document object that comes with a variety of annotations.

19
00:01:29.090 --> 00:01:34.580
In this lecture we're going to be taking a closer look at the actual speech tax that spacey gives us

20
00:01:34.640 --> 00:01:35.780
and it gives us two types.

21
00:01:35.810 --> 00:01:38.400
It gives us a course part of speech tags.

22
00:01:38.420 --> 00:01:43.580
Those are your basic parts of speech things like nouns verbs and adjectives but it also gives you fine

23
00:01:43.580 --> 00:01:49.760
grained tags more nuanced information like whether something's a plural noun or a past tense for or

24
00:01:49.760 --> 00:01:53.440
a superlative adjective and as a quick note.

25
00:01:53.630 --> 00:01:58.760
Keep in mind English is an incredibly complex language with many rules and even more exceptions to the

26
00:01:58.760 --> 00:01:59.660
rules.

27
00:01:59.660 --> 00:02:04.820
Teaching English and the various grammatical rules it's outside the scope of this course you can check

28
00:02:04.820 --> 00:02:09.230
out the resource references to learn more about the various parts of speech tags and what they mean

29
00:02:09.230 --> 00:02:12.320
in English especially if English isn't your first language.

30
00:02:12.320 --> 00:02:18.890
Keep in mind I really wouldn't expect people to be a super in tune with the very nuanced details of

31
00:02:18.890 --> 00:02:24.670
grammar such as like a superlative adjective you may need to look that one up but you should be familiar

32
00:02:24.670 --> 00:02:29.030
with things like a noun versus a verb versus an adjective to really get the most out of this part of

33
00:02:29.030 --> 00:02:29.940
the course.

34
00:02:31.060 --> 00:02:34.920
OK let's go ahead and explore part of speech tags in more detail.

35
00:02:34.950 --> 00:02:38.110
Spacey I want to jump over to Jupiter notebook.

36
00:02:38.110 --> 00:02:42.430
All right if you take a look at the parts of speech tacking folder the very first note book we're going

37
00:02:42.430 --> 00:02:44.380
over is called POS basics.

38
00:02:44.560 --> 00:02:48.410
And if you open up that notebook and scroll through it I would highly encourage you to check out this

39
00:02:48.480 --> 00:02:54.400
notebook as if you this lecture because we've provided full tables that discuss the Course great speech

40
00:02:54.400 --> 00:02:54.990
tags.

41
00:02:55.090 --> 00:02:58.780
Those are the basic ones that every token has a POS tag assigned to it.

42
00:02:58.810 --> 00:03:04.780
So we have the code adjective position adverb etc. in description and then some examples in this table

43
00:03:04.780 --> 00:03:05.710
for you at checkout.

44
00:03:06.010 --> 00:03:08.850
But there's also fine grained parts of speech tags.

45
00:03:08.980 --> 00:03:14.770
So tokens are then subsequently after giving a speech tag given a fine green tag and then we have a

46
00:03:14.770 --> 00:03:18.900
description there as well as some morphology information about that particular tag.

47
00:03:18.910 --> 00:03:21.160
So these tables are really useful to reference.

48
00:03:21.250 --> 00:03:24.930
So let's go ahead and start going through this notebook.

49
00:03:24.970 --> 00:03:30.190
We're going to start by saying import Spacey and then we're going to load up the English language library

50
00:03:30.220 --> 00:03:39.170
by saying entropy is equal to speccy load and we'll call it an underscore core web underscore S.M. so

51
00:03:39.170 --> 00:03:44.860
that loads up the English language library for us Spacey and let's create a document.

52
00:03:45.160 --> 00:03:55.130
We're going to passen a unicode string and we'll just say the quick brown fox jumped over the lazy dog's

53
00:03:55.130 --> 00:03:58.070
back.

54
00:03:58.100 --> 00:03:58.750
All right.

55
00:03:58.880 --> 00:04:04.760
Now we already know that if I want to print out the entire document text I just need to say print document

56
00:04:04.760 --> 00:04:07.520
text and that prints out that string text.

57
00:04:07.850 --> 00:04:12.140
And you should recall that you can obtain particular tokens by index position.

58
00:04:12.140 --> 00:04:18.520
So if I want to just grab a particular token like jumped I just need to call that tokens in the Exposition

59
00:04:19.000 --> 00:04:25.700
at index 4 and I get back jumped and then I can call various parts of this token.

60
00:04:25.700 --> 00:04:32.120
So for example I can say something like grab the token text which is what I just saw here or I can grab

61
00:04:32.120 --> 00:04:42.500
the speech tag such as verb or if I want to grab the fine green tag I can say t a g underscore and that

62
00:04:42.500 --> 00:04:44.020
gives me that fine grained tags.

63
00:04:44.030 --> 00:04:49.580
So if we come back up here to this table again remember we have the fine grained part of speech tat.

64
00:04:49.820 --> 00:04:54.590
So I could end up looking for the fine grained tag and start looking up what it actually means.

65
00:04:54.590 --> 00:04:58.020
So in this case jumped has a tag of VB.

66
00:04:58.460 --> 00:05:06.040
And let's check out again what it's course tag was by saying Doc 4 and then POS underscore.

67
00:05:06.040 --> 00:05:06.790
And it was a verb.

68
00:05:06.800 --> 00:05:09.030
So it's a verb with a verb.

69
00:05:09.140 --> 00:05:12.320
So we come back down here to the fine grained speech.

70
00:05:12.530 --> 00:05:16.390
We can scroll down until we find verb and then we're looking for VB.

71
00:05:16.580 --> 00:05:23.150
And it says it's a past tense verb so spacey is smart enough to know that just means it was in the past

72
00:05:23.150 --> 00:05:23.690
tense.

73
00:05:23.690 --> 00:05:28.490
The fox historically jump it jumped in the past over the lazy duck's back.

74
00:05:28.490 --> 00:05:33.190
So we have both the speech this broader or Corser part a speech tag.

75
00:05:33.350 --> 00:05:38.660
And then the fine detailed tack underscore and if you don't provide the underscore it's just going to

76
00:05:38.690 --> 00:05:42.280
give you the numerical ID for that particular tag.

77
00:05:42.470 --> 00:05:49.420
So sometimes it's more useful to have the numerical ID specifically for cases of looking up information.

78
00:05:49.630 --> 00:05:56.500
Now what's nice is you can make yourself a little for loop by saying for token and document and then

79
00:05:56.500 --> 00:06:00.740
you can print out a little table of all this information using string literals.

80
00:06:00.850 --> 00:06:02.430
So we can say something like.

81
00:06:02.620 --> 00:06:09.460
Print out the token text and then print out the token part of speech.

82
00:06:10.930 --> 00:06:17.630
Then print out the token tag and then print out an explanation.

83
00:06:17.630 --> 00:06:22.180
So if you want an explanation you can just call from Spacey explain.

84
00:06:22.280 --> 00:06:23.730
Remember we've seen this before.

85
00:06:23.930 --> 00:06:26.800
And then we can explain that actual tag tag underscore.

86
00:06:27.110 --> 00:06:29.850
And if you run this your formatting may be a little off.

87
00:06:29.850 --> 00:06:31.340
So it's all put together here.

88
00:06:31.340 --> 00:06:33.160
Let's go ahead and give it a little bit of spacing.

89
00:06:34.510 --> 00:06:39.740
By adding this colon tend to a lot of these guys will say call and turn on this one.

90
00:06:39.730 --> 00:06:41.380
CONAN And on that one.

91
00:06:41.650 --> 00:06:46.250
Clinton here and now let's run this and we get an nicer looking table.

92
00:06:46.510 --> 00:06:53.710
So here we can see that took a text the speech the fine grained part of speech tag and then space explanation

93
00:06:54.070 --> 00:06:58.500
for that particular part of speech specifically the token tag.

94
00:06:58.720 --> 00:07:04.960
So this explanation goes with this fine grained tag be can pass in any token speech or a token text

95
00:07:05.320 --> 00:07:07.980
and see a space you can explain it for you.

96
00:07:08.010 --> 00:07:13.530
So we just saw we have the coarse green part of speech tags as well as the fine grained part of speech.

97
00:07:13.530 --> 00:07:18.280
Now in the English language as we mention that same string of characters can have different meanings.

98
00:07:18.360 --> 00:07:28.880
So let's create a documents that says we're going to make it a unicode string here that says I read

99
00:07:28.880 --> 00:07:31.010
books on an LP

100
00:07:34.490 --> 00:07:41.610
and we're actually going to say that the word we're examining is the word read.

101
00:07:41.640 --> 00:07:43.940
So I'm going to grab that read tokin.

102
00:07:43.950 --> 00:07:48.430
So if I check out word text that's that string read.

103
00:07:48.450 --> 00:07:53.480
So just looking at the token Right now let's go ahead and check out the explanation for it.

104
00:07:53.520 --> 00:07:56.540
I'm going to copy and paste this line right here.

105
00:07:59.840 --> 00:08:02.150
And this is printing out that token information.

106
00:08:02.370 --> 00:08:08.640
Let's go ahead and just redefine token as the word we're looking at and run this.

107
00:08:08.890 --> 00:08:10.590
And in this context I can see.

108
00:08:10.620 --> 00:08:15.180
I read books on an LP read is a verb which makes sense.

109
00:08:15.180 --> 00:08:17.480
The person is reading the book.

110
00:08:17.670 --> 00:08:21.170
But now let's create another document.

111
00:08:21.180 --> 00:08:22.730
It's going to look really similar.

112
00:08:22.740 --> 00:08:26.380
There's going to be slightly different.

113
00:08:27.150 --> 00:08:35.860
We're going to say I read a book on an LP and there is a difference here.

114
00:08:35.880 --> 00:08:42.030
This person reads books on an LP versus I read a book on an LP.

115
00:08:42.030 --> 00:08:43.570
So we're going to run that again.

116
00:08:44.280 --> 00:08:47.950
And now we're going to grab that cell information.

117
00:08:49.190 --> 00:08:56.230
Paste it in here and let's go ahead and read the fine the word again.

118
00:08:56.450 --> 00:09:01.730
So same thing I'm grabbing read except now the context I've read is different and the context is so

119
00:09:01.760 --> 00:09:04.010
my new as far as the difference here.

120
00:09:04.010 --> 00:09:04.760
What I'm saying.

121
00:09:04.820 --> 00:09:06.860
I read books on an LP.

122
00:09:06.880 --> 00:09:13.920
The other one saying I read a book on an LP and when you run this you'll notice that Spacey is smart

123
00:09:13.920 --> 00:09:19.020
enough to understand that when you say I read a book on an LP you're referring to something in the past

124
00:09:19.020 --> 00:09:24.540
tense versus this first one I read books on an LP is actually in the present.

125
00:09:24.600 --> 00:09:28.430
This person currently reads books on natural language processing and the second one.

126
00:09:28.470 --> 00:09:32.340
This person in the past had read a book on natural language processing.

127
00:09:32.400 --> 00:09:37.320
So this is kind of amazing the fact that spacey can take all of this context based off the surrounding

128
00:09:37.320 --> 00:09:37.990
words.

129
00:09:38.160 --> 00:09:42.210
It's really quite incredible what Spacey has done here next.

130
00:09:42.220 --> 00:09:45.510
Let's show you how you can actually count parts of speech tags.

131
00:09:45.610 --> 00:09:51.640
So the document object has a count by a method that accepts a specific token attribute as its argument

132
00:09:52.000 --> 00:09:56.950
and then returns a frequency count of the given attribute as a dictionary object.

133
00:09:56.950 --> 00:10:00.340
So let's show you this.

134
00:10:00.440 --> 00:10:01.390
I'm going to.

135
00:10:01.430 --> 00:10:06.360
Again let's actually use the same string as before where we said the lazy dog.

136
00:10:06.380 --> 00:10:08.780
So let's just copy this cell right here.

137
00:10:09.900 --> 00:10:12.630
And I'm going to put that in.

138
00:10:12.630 --> 00:10:17.970
So read the fine or document as the quick brown fox jumped over the lazy dog's back and let me show

139
00:10:17.970 --> 00:10:21.010
you how to get parts of speech counts.

140
00:10:21.150 --> 00:10:31.490
I can grab that doc and then say call count underscore by and we'll say Spacey thought Archie us are

141
00:10:31.520 --> 00:10:39.250
82 years excuse me for attributes and then dot P O S and what this is going to do is it's going to return

142
00:10:39.250 --> 00:10:42.770
a dictionary so speech counts.

143
00:10:42.910 --> 00:10:47.050
And then you may be wondering when you get back to dictionary what it's actually representing.

144
00:10:47.050 --> 00:10:53.170
Well we asked for a count by so we can tell that the values appeared to be some sort of count but these

145
00:10:53.170 --> 00:10:56.320
numbers aren't very useful by themselves.

146
00:10:56.320 --> 00:10:59.250
These numbers are actually the parts of speech code.

147
00:10:59.260 --> 00:11:05.440
Recall that when we came back up here if we actually didn't include an underscore we wouldn't get the

148
00:11:05.440 --> 00:11:07.210
name of the speech we'd get back.

149
00:11:07.220 --> 00:11:09.200
It's numerical identifier.

150
00:11:09.220 --> 00:11:16.500
Now we can always look up numerical identifiers simply by calling the documents vocab and then passing

151
00:11:16.500 --> 00:11:18.000
in whatever number we're interested in.

152
00:11:18.180 --> 00:11:20.190
So let's say I want to know what 83 stands for.

153
00:11:20.220 --> 00:11:27.830
I'll just pass an 83 and then ask for the text of that and I'll return back that it's an adjective again

154
00:11:27.840 --> 00:11:32.110
recall that if we just grab any token off of a document.

155
00:11:32.160 --> 00:11:41.510
So for example let's go ahead and just to make sure we run this let's grab Brown So here's document

156
00:11:41.520 --> 00:11:42.240
to.

157
00:11:42.330 --> 00:11:43.890
It's token brown.

158
00:11:44.030 --> 00:11:48.450
I can ask for the parts of speech underscore and it returns back a..

159
00:11:48.650 --> 00:11:52.960
Or if I just say POS it returns back its actual numerical code.

160
00:11:53.030 --> 00:11:54.560
That's what the speech counts.

161
00:11:54.580 --> 00:11:59.600
Dictionary is doing it's returning back the number and then how many times it showed up.

162
00:11:59.600 --> 00:12:04.910
So here we can tell that additives there's three adjectives inside of this document.

163
00:12:04.910 --> 00:12:08.590
The quick brown fox jumped over the lazy dog's back.

164
00:12:08.600 --> 00:12:13.610
Now if you want to actually create a frequency list of of speech tags from the entire document you can

165
00:12:13.610 --> 00:12:16.630
do that by simply making a for loop that looks something like this.

166
00:12:16.630 --> 00:12:30.490
We can say for key value in that we can call sordid POS counts items which is a essentially a list of

167
00:12:30.490 --> 00:12:34.090
tuples we can print out.

168
00:12:34.110 --> 00:12:36.740
Well let's use a little extra formatting here.

169
00:12:36.760 --> 00:12:42.350
We'll print out the key and then do a little space.

170
00:12:42.600 --> 00:12:47.070
Well say Doc vocab at that particular key.

171
00:12:47.880 --> 00:12:52.230
Print out the text and then add in a little bit of formatting here.

172
00:12:52.990 --> 00:12:55.360
The string literal formatting.

173
00:12:55.360 --> 00:13:00.090
And then we can passen the the actual value for it.

174
00:13:00.090 --> 00:13:03.050
So let's make sure we then commit a typo here run this.

175
00:13:03.230 --> 00:13:04.010
And there we go.

176
00:13:04.010 --> 00:13:09.760
So it's telling us that for numerical code 83 which is an adjective we have three adjectives.

177
00:13:09.980 --> 00:13:13.470
We have three nouns one punctuation and so on.

178
00:13:14.790 --> 00:13:20.160
And keep in mind you can do this not just for the course of speech tags but for the fine grained tags

179
00:13:20.220 --> 00:13:21.090
as well.

180
00:13:21.090 --> 00:13:26.820
Recall again from our table that right now we're just looking at this part of speech this more course

181
00:13:26.820 --> 00:13:27.310
tad.

182
00:13:27.480 --> 00:13:31.920
But there's also these fine grained tags and we could do the same thing for it so we can come back here

183
00:13:32.460 --> 00:13:36.780
and essentially copy and paste this.

184
00:13:37.200 --> 00:13:43.920
But then we're going to do is instead of saying POS counts we're going to say tag counts and let's create

185
00:13:43.950 --> 00:13:53.350
that tag counts object will say tag counts as equal to the documents and we'll say count by an also

186
00:13:53.390 --> 00:14:01.410
a Speccy €80 for attributes and instead of saying POS we'll say tag and then if you run that now you

187
00:14:01.410 --> 00:14:03.180
get to see the tags.

188
00:14:03.210 --> 00:14:09.870
So again notice here we have the numerical value the fine grained part speech and then how many times

189
00:14:09.870 --> 00:14:11.020
it showed up.

190
00:14:11.040 --> 00:14:16.530
Now you're probably wondering why are these numbers so much larger than the numbers for the basic tags

191
00:14:16.530 --> 00:14:18.690
like adjective and noun.

192
00:14:18.690 --> 00:14:26.370
Well the reason for that is because in speccy certain text values are hard coded into the document vocab.

193
00:14:26.550 --> 00:14:32.550
So that's the DRC that vocab and they take up the first several hundred ID numbers so the most common

194
00:14:32.550 --> 00:14:35.190
ones are the ones that are closer up in the list.

195
00:14:35.190 --> 00:14:39.180
So things like nouns and punctuations those are really commonly.

196
00:14:39.180 --> 00:14:43.830
So that's why they're kind of smaller numbers are further up in that list closer to that first item

197
00:14:43.830 --> 00:14:48.090
in the list the ones that don't get used that often they get pushed way further down.

198
00:14:48.120 --> 00:14:52.310
I remember Doc vocab also has essentially all the English words available to it.

199
00:14:52.350 --> 00:14:59.270
Remember the length of that vocab if we check it out it's actually this really big vocabulary of fifty

200
00:14:59.270 --> 00:15:01.180
seven thousand eight hundred sixty three.

201
00:15:01.190 --> 00:15:05.480
So for purposes of efficiency the most common ones are placed closer to the front of the list.

202
00:15:05.480 --> 00:15:07.610
That way when you search for them it's more efficient.

203
00:15:07.610 --> 00:15:14.810
And the ones that use less those get put down with kind of larger hashes and that really has to do more

204
00:15:14.820 --> 00:15:19.280
with the way that space works internally through its own internal operations.

205
00:15:19.290 --> 00:15:22.230
The thing is you're going to see as a user on your end.

206
00:15:22.440 --> 00:15:28.320
So as a user on our end work in a space the size of this number it's kind of irrelevant to us we just

207
00:15:28.320 --> 00:15:35.730
care that there is a number identifier for each particular text code of either a speech tag or a particular

208
00:15:35.730 --> 00:15:40.800
fine green tag and we can do the exact same thing for syntactic dependencies.

209
00:15:40.830 --> 00:15:48.530
So again we can copy and paste this code here and we can say DGP counts for syntactic dependencies.

210
00:15:48.690 --> 00:15:50.250
Look up the epi.

211
00:15:50.280 --> 00:15:56.580
So basically changing tags everywhere to the AP run that and you can see the syntactic dependency tags

212
00:15:56.950 --> 00:15:57.770
again.

213
00:15:57.810 --> 00:16:01.910
Kind of less common ones having larger numbers here and so on.

214
00:16:01.920 --> 00:16:05.510
OK so that's all there is for our discussion right now.

215
00:16:05.550 --> 00:16:09.290
Of course POS tags versus fine grained POS tags.

216
00:16:09.360 --> 00:16:14.700
The main thing I understand from this lecture is that you can always call the tags off any token by

217
00:16:14.700 --> 00:16:20.550
saying POS underscore and tag underscore if you forget the underscore it's just going to show you that

218
00:16:20.550 --> 00:16:27.120
raw number and you should also keep in mind they can also call spaces that explain to explain its token

219
00:16:27.120 --> 00:16:29.020
tags or its token part of speech.

220
00:16:29.100 --> 00:16:35.610
And we also have this table that kind of lines them up all nicely for us then as a quick reminder if

221
00:16:35.610 --> 00:16:40.710
you ever want to do a count of how many parts of speech you have you can say take your document call

222
00:16:40.710 --> 00:16:45.520
count by and then call Spacey the attributes whatever attribute you want it counts.

223
00:16:45.570 --> 00:16:48.830
Coming up next we're going to review how to visualize parts of speech.

224
00:16:48.900 --> 00:16:49.510
We'll see if they're.

