1
00:00:05,160 --> 00:00:08,850
Go over the solutions for the text classification Assessment Project.

2
00:00:09,090 --> 00:00:11,640
OK here we have the text classification assessment.

3
00:00:11,740 --> 00:00:18,910
So it's for a long and complete the tasks we're first going to import Pandurs as PD and then say DNA

4
00:00:18,970 --> 00:00:28,970
is equal to PD read CSB and then inside the text files folder you should be able to find movie reviews

5
00:00:29,020 --> 00:00:35,630
to ts V and remember because it's a TSB we want to indicate that it's a tab separated file.

6
00:00:35,870 --> 00:00:40,430
So after we run that we can check the head of the data frame just to confirm that the data frame is

7
00:00:40,430 --> 00:00:40,730
correct.

8
00:00:40,730 --> 00:00:44,180
So we see the labels and then we see the actual text review.

9
00:00:44,180 --> 00:00:50,790
Next we want to check for missing values so we can do this by saying DSF is null and then taking the

10
00:00:50,810 --> 00:00:53,280
sum of that.

11
00:00:53,460 --> 00:00:55,330
And it looks like we do have some missing values.

12
00:00:55,350 --> 00:00:58,410
In fact 20 of the reviews are missing.

13
00:00:58,530 --> 00:01:01,440
So we're going to want to make sure to remove those values.

14
00:01:01,500 --> 00:01:04,340
Now we also want to make sure to check for white string spaces.

15
00:01:04,590 --> 00:01:07,560
Next we're also going to check for whitespace strings.

16
00:01:07,740 --> 00:01:12,640
And then after we check for whitespace strings we'll go ahead and remove null values.

17
00:01:12,690 --> 00:01:13,270
It's up to you.

18
00:01:13,280 --> 00:01:14,570
Or do you want to do this in.

19
00:01:14,820 --> 00:01:19,350
And we're going to show you a caveat you have to keep in mind because we haven't removed the null values

20
00:01:19,350 --> 00:01:19,740
yet.

21
00:01:20,070 --> 00:01:21,090
So keep that in mind.

22
00:01:21,200 --> 00:01:26,500
We're going to create a list called blanks that's going to hold the actual index positions of any whitespace

23
00:01:26,500 --> 00:01:38,390
strings we encounter and then we'll say for the index label and review in ZF or tuples this is going

24
00:01:38,390 --> 00:01:41,280
to go and iterate through the data frame we'll do the following.

25
00:01:41,330 --> 00:01:48,080
If the review itself if the type of the review happens to be a string then we're going to check to see

26
00:01:48,080 --> 00:01:49,430
if it's an empty string.

27
00:01:49,430 --> 00:01:52,850
The reason I'm doing this check is because we know we have no values.

28
00:01:52,850 --> 00:01:56,720
In fact many of these reviews are not street types but there are no values.

29
00:01:56,720 --> 00:02:04,390
So if that happens to be the case that it's actually string then I can check if the review is space

30
00:02:04,900 --> 00:02:12,330
then I will say blanks append that particular index position.

31
00:02:12,420 --> 00:02:16,910
So then if I run this and actually check the length of blanks you realize that 0.

32
00:02:16,920 --> 00:02:18,780
So there actually are no whitespace strings.

33
00:02:18,810 --> 00:02:20,960
There's only these missing values that take care of.

34
00:02:21,050 --> 00:02:27,090
Now we can do that easily by just saying the F drop and a and then we'll say in place is equal to true

35
00:02:28,920 --> 00:02:31,430
task force to take a quick look at the label column.

36
00:02:31,440 --> 00:02:32,500
So let's go out and check it out.

37
00:02:32,670 --> 00:02:40,500
We'll say the label and let's call value counts in here we can see we have just as many positive reviews

38
00:02:40,500 --> 00:02:44,000
as we have negative reviews so it's a very well perfectly balanced data set.

39
00:02:44,280 --> 00:02:48,560
And so task five is that split the data into treating sets and test sets.

40
00:02:49,380 --> 00:02:52,090
We'll say from S-K learn model selection

41
00:02:54,810 --> 00:03:06,520
import train test split and then I will set x to be the review and Y to be the label and then finally

42
00:03:06,520 --> 00:03:08,850
we will call train to split.

43
00:03:09,070 --> 00:03:15,700
And then if you put this in a new cell and run the import You should then be able to shift tab and then

44
00:03:16,390 --> 00:03:17,470
expand on this.

45
00:03:17,470 --> 00:03:22,410
Scroll down until you find extreme excess weight training weightiest.

46
00:03:22,570 --> 00:03:29,130
So we'll go ahead and copy and paste those sort of to train test split and or passenger X or Y.

47
00:03:29,410 --> 00:03:35,290
And then our test size go ahead and set that equal to zero point three three and we'll set a random

48
00:03:35,290 --> 00:03:39,610
state of 42 so we run that.

49
00:03:39,610 --> 00:03:43,730
And now we can build a pipeline to vectorize the data then train and fit the model.

50
00:03:43,870 --> 00:03:45,610
So let's go ahead and do that.

51
00:03:45,740 --> 00:03:52,750
We will say from S-K learn and we're going to do it through a pipeline we'll import pipeline singular

52
00:03:54,100 --> 00:04:05,170
import pipeline and then from S-K learn thought feature extraction thought text import T.F. IDF vector

53
00:04:05,190 --> 00:04:16,080
Isar then we'll save from Eski learn as SVM import linear SABC that you will say create a text class

54
00:04:16,080 --> 00:04:24,350
for object with our pipeline object and this pipeline objects going to taken a list of tuples or say

55
00:04:25,040 --> 00:04:33,990
T.F. idea and then insert here a Tina Fey Effect raiser and then the next step in our pipeline will

56
00:04:33,990 --> 00:04:39,100
just be the classifier itself which we're here we're just going to use a linear support vector classifier.

57
00:04:39,720 --> 00:04:45,240
So say linear support vector classifier and then will actually fit the data.

58
00:04:46,390 --> 00:04:49,720
We'll call it on our next train.

59
00:04:49,930 --> 00:04:53,590
And there are white train that we can call extreme why train directly because we're going to have x

60
00:04:53,590 --> 00:04:55,200
rays within the pipeline.

61
00:04:55,570 --> 00:05:00,420
So let's make sure we didn't commit any typos and actually check to make sure this worked by running

62
00:05:00,420 --> 00:05:02,330
it.

63
00:05:02,400 --> 00:05:06,650
OK so don't worry if you get a warning as long as you see the actual output you should be good to go.

64
00:05:07,010 --> 00:05:09,740
So let's run the predictions and analyze the results.

65
00:05:09,800 --> 00:05:17,510
So we're going to say predictions is equal to text classifier or text CNF and we're going to predict

66
00:05:17,600 --> 00:05:23,960
off our test set and then we're going to report a confusion matrix which means we need to import it

67
00:05:24,400 --> 00:05:32,510
will say from S-K learn thought metrics in import a confusion matrix A classification report and we'll

68
00:05:32,510 --> 00:05:38,450
go in for accuracy score and then we'll just print all these out will print out the confusion matrix

69
00:05:39,580 --> 00:05:41,530
with the predictions versus the white.

70
00:05:41,530 --> 00:05:46,320
True so we'll say why test the true values against the predictions.

71
00:05:47,830 --> 00:05:48,700
So let's run that.

72
00:05:48,700 --> 00:05:50,900
So here we can see your confusion matrix.

73
00:05:51,310 --> 00:05:58,100
Let's check out the classification report and then see why test against the predictions.

74
00:05:58,200 --> 00:05:59,630
Looks like it's doing pretty darned good.

75
00:05:59,700 --> 00:06:01,350
And they need 2 percent precision recall.

76
00:06:01,370 --> 00:06:07,990
Let's put out the overall accuracy we'll say metrics and then accuracy score.

77
00:06:08,100 --> 00:06:13,570
We are actually in pretty accurate or we just call it directly and then pass y test of the predictions

78
00:06:13,930 --> 00:06:16,370
Alzugaray the 92 percent accuracy.

79
00:06:16,630 --> 00:06:17,300
OK.

80
00:06:17,350 --> 00:06:22,030
Hopefully this whole process now feels pretty straightforward to you we've seen it several times.

81
00:06:22,270 --> 00:06:26,200
But now you understand what each step is doing you understand is going on behind the scenes with the

82
00:06:26,200 --> 00:06:27,220
vectorization.

83
00:06:27,430 --> 00:06:33,090
And now you're able to form text classification using the raw text data as a major feature.

84
00:06:33,100 --> 00:06:34,650
Thanks and we'll see you at the next section.

