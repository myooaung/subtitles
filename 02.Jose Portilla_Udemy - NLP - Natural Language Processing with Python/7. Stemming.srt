1
00:00:05,540 --> 00:00:12,110
Welcome everyone to this lecture on stemming often when searching text for a certain keyword it helps

2
00:00:12,160 --> 00:00:14,840
if the search returns variations of the word.

3
00:00:14,840 --> 00:00:20,740
For instance searching for the term boat might also return things like boats and boating here.

4
00:00:20,750 --> 00:00:28,280
The stem boat is going to be the stem for words like boat boat or boating and boats which are conveying

5
00:00:28,280 --> 00:00:35,960
the idea related to boat stemming is a somewhat crude method for cataloguing related words.

6
00:00:36,070 --> 00:00:40,830
It essentially just chops off letters from the end until the stem is actually reached.

7
00:00:40,840 --> 00:00:47,230
This works fairly well in most cases but unfortunately English has so many exceptions where a more sophisticated

8
00:00:47,230 --> 00:00:49,170
process is usually required.

9
00:00:50,190 --> 00:00:55,580
In fact it's such a rough process that Spacey doesn't even include a Stemmer.

10
00:00:55,620 --> 00:01:00,510
Instead they opt for the more sophisticated method known as limitation which we're going to cover in

11
00:01:00,510 --> 00:01:06,210
the next lecture but you'll often see stemming in natural language processing discussion.

12
00:01:06,390 --> 00:01:11,520
So it's they do have an understanding of the basics of studying before you move onto amortisation.

13
00:01:11,620 --> 00:01:17,370
There is a link in the note book to a discussion on why the maintainer's is spacey the decide not to

14
00:01:17,370 --> 00:01:22,420
include a summer in favor of limitation in case you're interested in the history of the development

15
00:01:22,420 --> 00:01:23,070
of space.

16
00:01:23,240 --> 00:01:28,760
So you can check out the new book for that link discussion now because of this decision not to include

17
00:01:28,760 --> 00:01:29,880
stemming in space.

18
00:01:30,020 --> 00:01:34,170
We're now going to jump over to using an old Teekay and learn about various DeMars.

19
00:01:34,190 --> 00:01:39,270
In fact we're going to show you both the porter Stemmer and the snowball Stemmer.

20
00:01:39,300 --> 00:01:44,550
So one of the most common and effective stemming tools is Porter's algorithm developed by Martin Porter

21
00:01:44,580 --> 00:01:49,190
in 1980 and the algorithm employs five phases of word reduction.

22
00:01:49,440 --> 00:01:52,900
Each with its own set of mapping rules.

23
00:01:52,970 --> 00:01:57,800
So in the first phase simple suffixed mapping rules are the find such as the following.

24
00:01:57,800 --> 00:02:04,450
So here we have a table with S1 going to s to essentially going from one word to a step.

25
00:02:04,460 --> 00:02:11,630
So you can see here that the ending of one s s e s will get reduced to s s or caresses gets reduced

26
00:02:11,630 --> 00:02:18,830
to caress or I guess it gets reduced to ice so ponys gets reduced to P and I and Tai's gets reduced

27
00:02:18,830 --> 00:02:19,390
to t.

28
00:02:19,420 --> 00:02:28,980
I so from a given set of stemming rules only one rule is applied based off the longest suffix S1 thus

29
00:02:28,980 --> 00:02:32,250
caresses reduces to caress but not to CARES.

30
00:02:32,400 --> 00:02:37,140
That way there's no confusion and mixing up different types of words to the wrong stem.

31
00:02:37,140 --> 00:02:42,420
Now there are some more sophisticated phrases that consider the length and complexity of the word before

32
00:02:42,480 --> 00:02:50,760
applying a rule such as trying to map relational or national to their stem relates and national or agreed

33
00:02:50,820 --> 00:02:53,850
and feed to their other see them agree and feed.

34
00:02:54,000 --> 00:02:58,260
Notice that we're going from e d to e or agonal to 80.

35
00:02:58,440 --> 00:03:03,830
So there's lots of different complex rules that the porter Stemmer is going to implement.

36
00:03:05,450 --> 00:03:10,460
Now snowball is a name of a Stebbing language that was also developed by Martin porter and the algorithm

37
00:03:10,460 --> 00:03:16,580
here is actually sometimes called the English Stemmer or porter to Stemmer and it offers a slight improvement

38
00:03:16,610 --> 00:03:19,820
over the original Porter Stemmer both in logic and speed.

39
00:03:19,940 --> 00:03:25,090
So you can essentially think of it as an improvement over the original Porter Stemmer.

40
00:03:25,160 --> 00:03:30,370
So let's use Python and NLC can actually show how to use the Strummer's with Python.

41
00:03:30,440 --> 00:03:31,650
Jump over to a notebook now.

42
00:03:32,590 --> 00:03:37,940
All right the first thing I'm going to do is import an LTE K for the natural language toolkit and then

43
00:03:37,940 --> 00:03:41,800
from an LTE K that's them.

44
00:03:42,020 --> 00:03:50,640
Porter I'm going to import the porter Stemmer and then I'm going to create an instance of this porter

45
00:03:50,660 --> 00:03:57,340
Stemmer ogic and I'm just using tap to autocomplete there and let's create a list of words.

46
00:03:57,340 --> 00:04:05,440
So I'm going to have run and runner and then I'm going to have also ran.

47
00:04:05,470 --> 00:04:07,900
So notice technically these are all conveying the same idea.

48
00:04:07,930 --> 00:04:09,640
They just had different tenses to them.

49
00:04:09,700 --> 00:04:13,750
So run runner and ran actually one is a noun.

50
00:04:13,990 --> 00:04:18,460
These are verbs but you can see they're both conveying are all three of them are conveying the same

51
00:04:18,460 --> 00:04:26,410
idea of running and we're going to add one more runs and let's add in a few more words like easily.

52
00:04:26,680 --> 00:04:32,050
And we'll also put in fairly just we can get some more examples.

53
00:04:32,320 --> 00:04:38,570
And the way you stem down these words is you just pass them into the stem method of this porter Stemmer.

54
00:04:38,830 --> 00:04:47,560
So the way this works is we can say for word in words we can print out the original word and then I'm

55
00:04:47,560 --> 00:04:51,700
going to concatenate an arrow to show you what it stems to.

56
00:04:52,390 --> 00:04:59,520
And then off our Stemmer object so off piece Stemmer you call them and then pasand that particular word.

57
00:04:59,530 --> 00:05:03,900
So all I'm doing here is I'm going to print out the original word from this list then I get a print

58
00:05:03,900 --> 00:05:09,840
out an arrow with a string and then I'm going to show the stem the version of the word using the porter

59
00:05:09,840 --> 00:05:11,490
Stemmer.

60
00:05:11,530 --> 00:05:18,790
So when I run this I can see that run goes to run run or goes the runner ran goes to ran runs however

61
00:05:18,880 --> 00:05:22,990
does get turned down to runs and that's according to the porter Stemmer rules.

62
00:05:23,050 --> 00:05:27,800
We should chop off the s and now run and runs get treated the same.

63
00:05:27,880 --> 00:05:33,890
What's interesting here is that easily and fairly they get them down to anything of an eye which is

64
00:05:33,890 --> 00:05:35,480
kind of interesting here.

65
00:05:35,480 --> 00:05:44,420
So the Stemmer is actually able to recognize that a runner is a noun not a verb form or a adverb.

66
00:05:44,660 --> 00:05:47,660
And these adverbs are here easily and fairly.

67
00:05:47,690 --> 00:05:53,280
Their stem to the unusual root of easily and I am fairly with an eye as well.

68
00:05:53,630 --> 00:05:59,840
So keep that in mind for use in the porter Stemmer as we mention there's a more sophisticated and slightly

69
00:05:59,840 --> 00:06:02,770
better version of the Stemmer called the snowball Stemmer.

70
00:06:02,930 --> 00:06:07,920
And in order to use it you just say T.K. that's them.

71
00:06:08,050 --> 00:06:17,400
That snowball import and called the snowball Stemmer and then you can create an instance of that Stemmer

72
00:06:18,540 --> 00:06:24,560
by saying snowball Stemmer and this one also requires you to tell you what language it's in.

73
00:06:24,570 --> 00:06:29,820
So we're just going to pass in English as the language parameter and we're going to use the same words

74
00:06:29,880 --> 00:06:31,020
as before.

75
00:06:31,080 --> 00:06:42,150
So now we're going to say for word in words print out the word and then we'll say print out in concatenation

76
00:06:42,960 --> 00:06:44,610
what the snowball Stemmer steps to.

77
00:06:44,640 --> 00:06:53,090
So as Stemmer stem and then word and when you run this you notice that the stemming performs slightly

78
00:06:53,090 --> 00:06:56,710
differently run and runs both gets them down to run.

79
00:06:56,870 --> 00:07:01,880
But now fairly just goes down to fair which is interesting because maybe that performs better for things

80
00:07:01,880 --> 00:07:02,830
like fairness.

81
00:07:02,930 --> 00:07:05,620
So fairly and fairness may both go down to fair.

82
00:07:05,630 --> 00:07:14,330
In fact we can check that by adding in some more words like fairness and if we run this again there's

83
00:07:14,340 --> 00:07:19,830
that fairness goes down to fair fairly goes down to fair leave and I let's try with this snowball Stemmer

84
00:07:20,830 --> 00:07:23,560
and now fairly in fairness both go down to fair.

85
00:07:23,590 --> 00:07:25,010
Isn't the snowball Stemmer.

86
00:07:25,170 --> 00:07:30,110
And typically it's not really a matter of predicting what the stem is going to be the actual stem itself.

87
00:07:30,120 --> 00:07:31,470
It's not super important.

88
00:07:31,530 --> 00:07:36,510
What's important is that you understand the actual process that's happening here.

89
00:07:36,540 --> 00:07:41,130
There is some set of algorithmic rules that these numbers are following to try to reduce down these

90
00:07:41,130 --> 00:07:46,000
words to some sort of root idea or root word.

91
00:07:46,120 --> 00:07:49,710
And I would encourage you to play around with distempers and see how they differ.

92
00:07:49,780 --> 00:07:51,270
Words of your own choice.

93
00:07:51,320 --> 00:07:56,590
So we're going to show one more list here of words kind of as a fun example we're going to type in the

94
00:07:56,590 --> 00:08:08,560
word generous and then run I write in generation and then we're going to write in generously.

95
00:08:08,670 --> 00:08:14,700
And let's also say generate So some of these are similar words and some of them are not so generous

96
00:08:14,790 --> 00:08:18,070
and generation don't really have anything to do if each other.

97
00:08:18,120 --> 00:08:22,590
Yeah technically these kind of the same Latin root but really they're conveying that for ideas to be

98
00:08:22,590 --> 00:08:28,490
generous is not the same thing as actually generating something however generously and generous or conveying

99
00:08:28,530 --> 00:08:34,330
extremely similar ideas and generation and generate are probably also pretty similar ideas.

100
00:08:34,530 --> 00:08:39,630
So let's run that and let's actually copy the snowball Stemmer for this and run the snowball Stemmer

101
00:08:39,660 --> 00:08:45,510
on that list of words and you can see here that generous and generously actually get reduced both to

102
00:08:45,570 --> 00:08:52,380
generous and then Generation and generates both get reduced to Genner rot ending of a T.

103
00:08:52,440 --> 00:08:56,490
So hopefully that gives an idea of how you can use an LTE system words.

104
00:08:56,490 --> 00:09:01,560
And often when you're reading books on natural language processing people like to perform stemming before

105
00:09:01,560 --> 00:09:06,390
they run some sort of analysis in order to try to reduce words to their root.

106
00:09:06,390 --> 00:09:12,450
Now as I mentioned Spacey didn't have stemming because it figures limitation is a much more effective

107
00:09:12,450 --> 00:09:14,340
way of actually reducing these words.

108
00:09:14,340 --> 00:09:19,530
So coming up next we're going to learn about limitation and see why it's probably a more effective way

109
00:09:19,890 --> 00:09:22,260
of reducing these words to their roots.

110
00:09:22,260 --> 00:09:23,280
We'll see at the next lecture.

