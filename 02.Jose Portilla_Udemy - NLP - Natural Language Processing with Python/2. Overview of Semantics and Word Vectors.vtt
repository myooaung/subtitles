WEBVTT
ï»¿1
00:00:00.240 --> 00:00:01.860
Welcome back everyone.

2
00:00:01.860 --> 00:00:08.980
Let's now discuss semantics and word vectors but first in order to actually use spaces embedded word

3
00:00:08.990 --> 00:00:12.960
vectors we must download the larger Spacey English models.

4
00:00:13.030 --> 00:00:17.680
Full details about the models and as well as how to install them can be found at the link right here.

5
00:00:17.680 --> 00:00:21.880
Space is the IO forward slash usage forward slash models.

6
00:00:21.970 --> 00:00:27.910
Recall that when we first installed Spacey not only did we have to do an environmental install either

7
00:00:27.910 --> 00:00:33.580
using content install or pip install but we also at the command line had to install the full English

8
00:00:33.640 --> 00:00:35.280
language models.

9
00:00:35.290 --> 00:00:40.840
Now if you went ahead and downloaded the default model that would be the small model and that actually

10
00:00:41.020 --> 00:00:46.570
doesn't contain the word vectors because the word vectors themselves is actually a large enough piece

11
00:00:46.570 --> 00:00:51.740
of data that Spacey has medium size and larger size models for that.

12
00:00:51.790 --> 00:00:57.490
So in order to download either the medium or large space the English models at your command line you

13
00:00:57.490 --> 00:01:06.160
need to type out Python space dash M. space Spacey space download space and then e n underscore core

14
00:01:06.340 --> 00:01:13.410
underscore web underscore and then either M.D. for the medium English model or LG for the large spaces

15
00:01:13.420 --> 00:01:18.970
English model and the difference with medium and large for the context of these lectures doesn't really

16
00:01:18.970 --> 00:01:20.040
make a huge difference.

17
00:01:20.050 --> 00:01:21.850
So you can choose either one.

18
00:01:21.940 --> 00:01:24.850
I went ahead and downloaded the large English model.

19
00:01:24.910 --> 00:01:28.450
Just keep in mind these are really large files so they may take a while to download.

20
00:01:28.870 --> 00:01:35.470
So run either of these two lines at your command line in order to install the medium or large spacing

21
00:01:35.470 --> 00:01:36.580
English models.

22
00:01:36.580 --> 00:01:43.990
And again you can check out the link that usage slash models for full details on this installation process.

23
00:01:44.050 --> 00:01:49.180
So now that you have the larger models that contain the actual word vectors let's discuss how word vectors

24
00:01:49.240 --> 00:01:58.340
are created words of Vec is a two layer neural net that processes text its input as a text corpus and

25
00:01:58.340 --> 00:02:05.280
its output is a set of vectors which are essentially just feature vectors for words in that corpus the

26
00:02:05.280 --> 00:02:11.040
purpose and usefulness of where to Vec is to group the vectors of similar words together in this vector

27
00:02:11.040 --> 00:02:12.000
space.

28
00:02:12.000 --> 00:02:16.890
And that is it's able to the text similarities mathematically and we'll discuss that in further detail

29
00:02:17.010 --> 00:02:17.780
in just a little bit.

30
00:02:19.480 --> 00:02:24.820
But first try to understand that words of Vec is really just creating vectors that are distributed numerical

31
00:02:24.820 --> 00:02:30.880
representations of words features features such as the context of individual words and what's really

32
00:02:30.880 --> 00:02:35.340
interesting it's able to do all of this without any sort of human intervention.

33
00:02:35.470 --> 00:02:38.590
It's just learning naturally from a very large corpus of texts.

34
00:02:38.770 --> 00:02:46.460
The context of different words and which words are similar to other words so theoretically given enough

35
00:02:46.460 --> 00:02:48.750
data usage and contexts.

36
00:02:48.770 --> 00:02:53.480
So we're dealing for a really large corpus for example all of Wikipedia.

37
00:02:53.480 --> 00:02:58.470
Once you feed that into the words civic algorithm words that can make highly accurate guesses about

38
00:02:58.470 --> 00:03:03.500
a word's meaning based on past appearances and the context surrounding that particular word.

39
00:03:03.530 --> 00:03:08.190
So those guesses can then be used to establish a words association with other words.

40
00:03:08.240 --> 00:03:14.040
So after you actually train a word to model the vectors themselves will carry information and it's going

41
00:03:14.040 --> 00:03:19.550
to be able to understand associations such as man is the boy as woman is the girl.

42
00:03:19.610 --> 00:03:24.590
So while others understand that there's some sort of a relationship of age there that man and woman

43
00:03:24.590 --> 00:03:31.490
tend to be the older words you use versus boy and girl tend to be the younger words you use now where

44
00:03:31.490 --> 00:03:37.370
to Vec trains words against other words that neighbor them in the input corpus and a can actually do

45
00:03:37.370 --> 00:03:44.270
this one of two ways either using context to predict a target word and it is known as a continuous backwards

46
00:03:44.300 --> 00:03:49.380
approach or CB O W or C bow or the other method.

47
00:03:49.390 --> 00:03:56.650
That's also common is using a word to predict a target context which is called Skip Graham so here we

48
00:03:56.650 --> 00:03:59.710
see a diagram of the two possible approaches.

49
00:03:59.710 --> 00:04:04.160
And basically the kind of inverse of each other for the CBO approach.

50
00:04:04.240 --> 00:04:11.140
You have several input words and then your projection is essentially trying to predict what is the highest

51
00:04:11.140 --> 00:04:13.630
probability word to show up.

52
00:04:13.630 --> 00:04:19.750
Given the context of those surrounding words now the skip Graham method takes a little longer to train

53
00:04:19.840 --> 00:04:25.780
and to develop because it's essentially doing the opposite given an input of a single word using the

54
00:04:25.780 --> 00:04:32.590
auto encoder neural network projection try to output the weighted probabilities of the other words that

55
00:04:32.590 --> 00:04:36.430
are going to show up around the context of this input word.

56
00:04:36.430 --> 00:04:42.430
So again you have kind of two inverse methods here either given the context words and then predicting

57
00:04:42.430 --> 00:04:50.450
the output word or inputting the word and then trying to predict the output context words so at the

58
00:04:50.450 --> 00:04:51.200
end of all this.

59
00:04:51.230 --> 00:04:57.710
Recall that each word is now going to be represented by a vector and in Spacey each of these vectors

60
00:04:57.710 --> 00:05:02.110
has three hundred dimensions while you're actually trading training the auto encoder.

61
00:05:02.150 --> 00:05:04.430
If you were to implement or to vector yourself.

62
00:05:04.430 --> 00:05:09.920
Granted it takes a very long time on a very large corpus so it's unusual for people to actually not

63
00:05:09.920 --> 00:05:12.640
use some sort of built in embedded word vectors.

64
00:05:12.770 --> 00:05:17.870
But if you did want to actually train your own auto encoder for word to Vec then you could actually

65
00:05:17.870 --> 00:05:21.100
theoretically choose either less or more dimensions.

66
00:05:21.170 --> 00:05:27.140
Typically the ranges are between 100 to 1000 dimensions the higher number of dimensions the longer the

67
00:05:27.140 --> 00:05:32.510
training time but you should have then more context around each of these words since you have more dimensions

68
00:05:32.510 --> 00:05:35.650
to hold more information.

69
00:05:35.650 --> 00:05:41.080
Now what this means is since we have each word mapped to a vector in this three hundred dimensional

70
00:05:41.110 --> 00:05:47.050
space we can use cosine similarity to measure how similar word vectors are to each other.

71
00:05:47.050 --> 00:05:51.790
So cosine similarity is essentially just checking the distance between two vectors.

72
00:05:51.790 --> 00:05:54.850
And here we see a simple diagram in a two dimensional space.

73
00:05:54.850 --> 00:05:58.600
But this expands out to end dimensions in our case.

74
00:05:58.600 --> 00:06:04.120
We'll be taking several 300 dimensional vectors and then calculating the cosine similarity between them

75
00:06:04.450 --> 00:06:07.210
to see what vectors are most similar to each other.

76
00:06:07.300 --> 00:06:10.550
And in this case the actual vectors represent words.

77
00:06:10.570 --> 00:06:16.360
Now what's really interesting is this means we can outperform vector arithmetic with the new word vectors

78
00:06:16.510 --> 00:06:23.110
so we can calculate a brand new vector by performing arithmetic that is adding or subtracting different

79
00:06:23.110 --> 00:06:28.720
vectors so I can take the vector that three hundred dimensional vector for King and then subtract the

80
00:06:28.720 --> 00:06:32.360
vectors for man and then add the vector for a woman.

81
00:06:32.410 --> 00:06:37.150
So that creates a brand new vector that's not actually directly associated with a single word.

82
00:06:37.570 --> 00:06:43.360
Instead we performed an arithmetic between the vectors of several words and then when I can attempt

83
00:06:43.360 --> 00:06:49.750
to do is find the most similar existing vectors to this new vector so hopefully after you do something

84
00:06:49.750 --> 00:06:55.330
like King minus man policewoman that new vector its closest existing word vector could be something

85
00:06:55.330 --> 00:06:56.420
like Queen.

86
00:06:56.650 --> 00:07:03.550
Essentially understanding the context of royalty along one dimension and then moving along another dimension

87
00:07:03.550 --> 00:07:06.270
for gender.

88
00:07:06.430 --> 00:07:12.340
So this is able to actually establish really interesting relationships between the word vectors including

89
00:07:12.340 --> 00:07:14.460
relationships between male versus female.

90
00:07:14.470 --> 00:07:21.040
Like we just explained earlier or even dimensions for verb tense so I can understand that walking is

91
00:07:21.040 --> 00:07:28.760
to walked as swimming is the swim so let's begin to explore Spacey word vectors with Python.

92
00:07:28.790 --> 00:07:29.840
I'll see you at the next lecture.

