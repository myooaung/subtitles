WEBVTT
ï»¿1
00:00:05.370 --> 00:00:10.560
Welcome back everyone in this lecture we're going to discuss non-negative matrix factorization and how

2
00:00:10.560 --> 00:00:12.390
we can apply it to topic modeling.

3
00:00:13.830 --> 00:00:19.890
Non-negative matrix factorization is an unsupervised algorithm that simultaneously performs dimensionality

4
00:00:19.920 --> 00:00:22.230
reduction and clustering.

5
00:00:22.230 --> 00:00:27.300
We can use it in conjunction with T.F. idea to model topic's across various documents.

6
00:00:28.610 --> 00:00:33.840
So because the general mathematics behind non-negative matrix factorization are an image.

7
00:00:34.120 --> 00:00:38.380
And that we'll show you how you can apply it with a document matrix.

8
00:00:38.480 --> 00:00:43.910
So first you're given a non-negative matrix A which is what we're going to be able to create using T.F.

9
00:00:43.940 --> 00:00:44.570
idea.

10
00:00:44.900 --> 00:00:50.540
And then we want to find a K that image an approximation in terms of two other non-negative factors

11
00:00:50.640 --> 00:00:52.080
w an h.

12
00:00:52.130 --> 00:00:58.340
So you can have some matrix A which has and by M some rows features and columns or objects and we want

13
00:00:58.340 --> 00:01:06.820
to perform factorization to essentially approximate a with the matrix multiplication of W and H where

14
00:01:06.830 --> 00:01:14.820
we have and by K for W and K by M for H and again we are going to approximate each object.

15
00:01:14.860 --> 00:01:23.550
That is the columns of a by a linear combination of K reduce them mentions or basis vectors in W. each

16
00:01:23.550 --> 00:01:29.100
Bass's vector can then be interpreted as a cluster and the membership's of objects in these clusters

17
00:01:29.190 --> 00:01:30.770
are encoded by H.

18
00:01:32.580 --> 00:01:36.810
So the way this is going to work again is we have an input that we're providing a non-negative data

19
00:01:36.810 --> 00:01:38.570
matrix which is a.

20
00:01:38.730 --> 00:01:40.910
And in this case that's going to be your T.F. idea.

21
00:01:41.010 --> 00:01:47.310
So your term frequency inverse document frequency for the documents versus the actual words and those

22
00:01:47.400 --> 00:01:52.920
across all the documents then we're going to have a number of Bass's vectors or k.

23
00:01:53.190 --> 00:01:57.830
In our case for topic modeling the k is going to be the number of topics we choose.

24
00:01:57.840 --> 00:02:04.420
So just like before with LDA we actually have to decide on how many topics we want to try to discover.

25
00:02:04.440 --> 00:02:10.290
So we get to choose the value OK and then we're going to initialize some values for the factors W H

26
00:02:10.740 --> 00:02:17.460
and these start off as random matrices then it's our job to essentially First get some sort of measurement

27
00:02:17.580 --> 00:02:21.080
of the reconstruction error between a and the approximation.

28
00:02:21.320 --> 00:02:25.900
H and then we're going to do is optimize.

29
00:02:26.140 --> 00:02:32.890
So we have an expectation maximisation optimization to essentially refine W and H in order to minimize

30
00:02:32.890 --> 00:02:39.310
that objective function essentially keep updating those coefficients until the approximation of W. H

31
00:02:39.550 --> 00:02:41.590
is actually going to make sense for a.

32
00:02:41.860 --> 00:02:47.170
So a common approach is to essentially iterate between two multiplicative update rules until convergence

33
00:02:47.350 --> 00:02:53.330
updating H then updated w and then repeating that process and to Convergys.

34
00:02:53.340 --> 00:02:59.550
So the way this works for our particular use case is we first construct a vector space model for documents

35
00:02:59.880 --> 00:03:02.160
after we do things like filter out Stop words.

36
00:03:02.190 --> 00:03:08.420
And that results in a term document matrix for a or a document termit tricks DTM.

37
00:03:08.610 --> 00:03:14.940
Then we apply T.F. IDF term weight normalization to a and then normalized T.F. idea of vectors to unit

38
00:03:14.940 --> 00:03:15.920
length.

39
00:03:15.930 --> 00:03:19.750
Keep in mind those first three steps even though it sounds like a lot.

40
00:03:19.890 --> 00:03:24.740
Sikat learn is essentially doing all of that for us when we call it T.F. idea of vector Isar.

41
00:03:24.960 --> 00:03:29.820
So we don't have to do things like remove the Stoppard's ourselves and call count vectorizing and then

42
00:03:29.820 --> 00:03:34.220
call the term frequency and then figure out inverse document frequency.

43
00:03:34.290 --> 00:03:40.710
Recall that all of that from text feature extraction is essentially done under the hood for us by calling

44
00:03:41.070 --> 00:03:44.410
the T.F. idea of vector Isar circular.

45
00:03:44.520 --> 00:03:50.540
So once we've done that or we're going to do is we're going to initialize the factors using an ant and

46
00:03:50.710 --> 00:03:58.510
the SVT or non-negative double single singular value decomposition on the A matrix.

47
00:03:58.650 --> 00:04:06.660
And the we're going to apply a projected gradient non-negative matrix factorization to a.

48
00:04:06.670 --> 00:04:10.080
So then what we end up discovering are the basis vectors.

49
00:04:10.180 --> 00:04:15.550
Those are the topics or clusters in the data and then we have a coefficient matrix which is the membership

50
00:04:15.550 --> 00:04:18.590
weights for documents relative to each topic.

51
00:04:18.790 --> 00:04:21.880
In this case each cluster.

52
00:04:21.940 --> 00:04:27.160
So again all we're doing is we're going to create a documentary matrix with T.F. idea factorization.

53
00:04:27.190 --> 00:04:28.270
So we've done that before.

54
00:04:28.270 --> 00:04:32.860
It looks something like this here instead of numerical values we're just representing the various values

55
00:04:32.890 --> 00:04:35.100
by how dark blue there is.

56
00:04:36.120 --> 00:04:39.170
And then we're going to have a resulting w an H.

57
00:04:39.210 --> 00:04:40.630
And so the basics are vectors.

58
00:04:40.770 --> 00:04:42.160
Those columns in W..

59
00:04:42.180 --> 00:04:47.790
Those are your topics and we can see how words relate to particular topics.

60
00:04:47.790 --> 00:04:51.230
So looks like Topic one has TV shows and actors.

61
00:04:51.300 --> 00:04:57.140
Topic 2 has to do a sports clubs footballs and then topic three tenths of do business banks money and

62
00:04:57.150 --> 00:05:04.230
finance and then h is going to have coefficients and that's going to be the membership's for each document.

63
00:05:04.260 --> 00:05:08.770
So we have co-efficient values for each document and how they relate to each topic.

64
00:05:09.030 --> 00:05:15.480
So the results are pretty similar as far as interpretation to what we saw before we get some sort of

65
00:05:15.480 --> 00:05:20.470
relationship between the documents and the topics and the words and the topics.

66
00:05:21.400 --> 00:05:28.360
Now keep in mind just like LDK were still going to need to select the number of expected topics beforehand.

67
00:05:28.510 --> 00:05:33.550
So we need to actually again choose that value of k and say something like I believe there are 12 topics

68
00:05:33.580 --> 00:05:36.640
across all these articles or I believe there are seven topics.

69
00:05:36.640 --> 00:05:43.060
So you have to initialize with some sort of value of k and also just like with L.D. a we all have to

70
00:05:43.060 --> 00:05:48.440
interpret these topics based on the coefficient values of the words per topic.

71
00:05:48.640 --> 00:05:54.550
Keep in mind these matrix coefficient values are not probabilities that can be interpreted like we did

72
00:05:54.550 --> 00:05:55.750
with LDK.

73
00:05:55.750 --> 00:06:01.670
We're still going to be able to interpret a relationship between a word to a topic of the coefficient

74
00:06:01.750 --> 00:06:07.650
and between a document to a topic with the coefficient but it's not like a probability value.

75
00:06:07.690 --> 00:06:15.600
Lta gave us it's just the coefficient value that the non negative matrix factorization solved for now.

76
00:06:15.610 --> 00:06:22.270
Luckily due to sikat learns uniform syntax switching out LDK for MF is actually very simple.

77
00:06:22.270 --> 00:06:28.480
In fact we're going to be able to kind of run through the last notebook and very easily adapt it for

78
00:06:28.540 --> 00:06:34.840
an F so we're going to repeat our analysis of the article dataset and show you how you can do topic

79
00:06:34.840 --> 00:06:37.170
modeling and discover topics.

80
00:06:37.450 --> 00:06:43.120
And we're going to be able to go through it a lot faster because we actually just have to swap out a

81
00:06:43.120 --> 00:06:45.530
couple of things in order to run an MF.

82
00:06:45.640 --> 00:06:49.630
So we're going to use the same notebook as we did before and then just show you what you need to swap

83
00:06:49.630 --> 00:06:52.010
to perform non-negative matrix factorization.

84
00:06:52.300 --> 00:06:54.170
OK we'll see you at the next lecture.

