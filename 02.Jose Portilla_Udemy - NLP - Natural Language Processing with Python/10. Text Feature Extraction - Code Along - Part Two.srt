1
00:00:05,300 --> 00:00:06,240
Welcome back everyone.

2
00:00:06,270 --> 00:00:11,090
Or you can continue right where we left off last time by showing you how to use T.F. idea with sikat

3
00:00:11,090 --> 00:00:11,600
learn.

4
00:00:11,870 --> 00:00:14,510
Let's head back to the notebook in the previous lecture.

5
00:00:14,510 --> 00:00:20,060
We went ahead and performed count vectorization as we discussed during the theory lecture.

6
00:00:20,060 --> 00:00:25,910
We can actually use T.F. idea or term frequency inversed document frequency in order to actually give

7
00:00:25,910 --> 00:00:32,720
words that are more important more weight one way to do this is by simply passing in our count vectorization

8
00:00:33,140 --> 00:00:35,490
into ATF IDF transformer.

9
00:00:35,650 --> 00:00:45,980
We can say from S-K learn that feature extraction thought text import T.F. IDF transformer.

10
00:00:45,980 --> 00:00:49,240
We'll talk about TAF IDF vectorize or object and just a second.

11
00:00:50,510 --> 00:00:59,240
First let's create an instance of the T.F. idea of transformer will say T.F. IDF transformer is equal

12
00:00:59,240 --> 00:01:08,690
to and here we'll call an instance of it and then we can simply take X train underscore T.F. IDF and

13
00:01:08,690 --> 00:01:18,010
set equal to the T.F. IDF transformer and then call transform on X train counts the result of the count

14
00:01:18,020 --> 00:01:19,310
vectorization.

15
00:01:19,310 --> 00:01:24,970
Now don't worry if you see a future warning show up that excess do of sikat learn and they will update

16
00:01:25,030 --> 00:01:27,080
and fix that in a future release.

17
00:01:27,170 --> 00:01:28,300
So you don't need to worry.

18
00:01:28,310 --> 00:01:31,150
It still works even if you get this warning.

19
00:01:31,160 --> 00:01:34,540
Next we're going to do is just quickly check the shape of this object.

20
00:01:34,610 --> 00:01:36,600
The extraneous idea.

21
00:01:37,750 --> 00:01:39,020
And here we can check the shape.

22
00:01:39,020 --> 00:01:42,350
It's still the same shape but it's no longer just counts.

23
00:01:42,350 --> 00:01:47,690
Instead we've taken in the term frequency and multiplied it by its inverse document frequency.

24
00:01:47,840 --> 00:01:55,340
Now because it's so common to perform a count vectorization followed by T.F. IDF transformation that

25
00:01:55,400 --> 00:02:02,090
sikat learn provides a T.F. IDF vectorization and that actually combines the two previous steps of Count

26
00:02:02,090 --> 00:02:06,510
vectorization A.F. IDF transformation into one single step.

27
00:02:06,890 --> 00:02:18,480
So we can do in the future is just say from S-K learn the feature extraction text import T.F. IDF victimiser

28
00:02:19,220 --> 00:02:22,360
and then you can create an instance of this vector laser.

29
00:02:22,390 --> 00:02:32,540
I was going to call vectorize or T If IDF victimiser and then we can say X train t If IDF is equal to

30
00:02:33,470 --> 00:02:38,840
and we call her vectorize her and then we can call that transform directly on X train.

31
00:02:38,840 --> 00:02:45,440
Remember this is our original x training data set and this essentially combines the process of both

32
00:02:45,470 --> 00:02:49,040
count vectorization and T.F. idea of transformation.

33
00:02:49,250 --> 00:02:52,130
So for convenience there's this T.F. idea of vectorize.

34
00:02:52,520 --> 00:02:55,070
So we can run that and we would get back the same result.

35
00:02:55,070 --> 00:02:58,520
Notice again I get a warning here at the Pentagon what version you are using.

36
00:02:58,550 --> 00:02:59,990
You may or may not get a warning.

37
00:02:59,990 --> 00:03:02,400
It's OK if it's just the warning.

38
00:03:02,420 --> 00:03:11,540
Next we're going to do is we're going to train a classifier So we will say from S-K learn SVM import

39
00:03:12,320 --> 00:03:19,250
and we'll be importing a linear support vector classifier will create an instance by saying CNF is equal

40
00:03:19,250 --> 00:03:30,830
to linear s v c and then we will say our classifier and we're going to fit it to the vectorized training

41
00:03:30,830 --> 00:03:31,340
data.

42
00:03:31,580 --> 00:03:38,000
So I'll say X underscore train t f f and then we passen the corresponding y.

43
00:03:38,000 --> 00:03:43,460
Training labels you run this and I'll report back the model that was used.

44
00:03:43,530 --> 00:03:49,710
Now remember only our training set has been vectorize into a full vocabulary in order to perform an

45
00:03:49,710 --> 00:03:51,520
analysis on our test set.

46
00:03:51,540 --> 00:03:54,990
We would actually have to then repeat all the same procedures.

47
00:03:54,990 --> 00:03:59,770
Now that can actually get a bit tiresome especially if you have a very long process.

48
00:03:59,880 --> 00:04:06,040
What sikat learn provides is a pipeline class that essentially behaves like a compound classifier.

49
00:04:06,090 --> 00:04:10,140
It can perform both the vectorization and the classification.

50
00:04:10,140 --> 00:04:16,740
So instead of having to do this sort of thing transform and count vectorization on your test data in

51
00:04:16,740 --> 00:04:22,830
order to predict what we can do is actually combine everything we just saw into one single pipelined

52
00:04:22,830 --> 00:04:32,370
step so the way we do this is we say from as Kaylor that pipeline import and we import this pipeline

53
00:04:32,430 --> 00:04:37,020
object and then we say text or whatever variable you want.

54
00:04:37,020 --> 00:04:42,750
We're going to call this my text classifier and you create an instance of the pipeline object and the

55
00:04:42,750 --> 00:04:47,460
pipeline object is going to take a tuple or a list of tuples excuse me.

56
00:04:47,460 --> 00:04:52,300
So you're going to pass in a list of tuples each tuple.

57
00:04:52,350 --> 00:04:57,060
What it's going to have is going to have a string name that either side or what to call this step in

58
00:04:57,060 --> 00:04:58,130
the pipeline.

59
00:04:58,170 --> 00:05:02,580
So I'm going to say T.F. IDF is my first step of the pipeline.

60
00:05:02,790 --> 00:05:04,890
And then you actually call what you want to occur.

61
00:05:04,920 --> 00:05:10,640
So I can pass in my vectorize or object there as the first item in my tuple.

62
00:05:10,650 --> 00:05:20,550
Then I can passen in other tuple pair so I can say actually perform classification using a linear as

63
00:05:20,550 --> 00:05:25,770
we see those means of open and close parentheses here and there.

64
00:05:25,790 --> 00:05:28,340
We can actually create our pipeline.

65
00:05:28,340 --> 00:05:35,000
So this pipeline object behaves just like a normal classifier of sikat learn our normal machine learning

66
00:05:35,000 --> 00:05:35,570
model.

67
00:05:35,810 --> 00:05:41,840
However what's convenient about this pipeline object is that it actually can perform all these steps

68
00:05:41,840 --> 00:05:48,050
for you in a single call then that means you can directly provide the original extraneous data and have

69
00:05:48,050 --> 00:05:52,780
it be both vectorized and run the classifier on it in a single step.

70
00:05:52,970 --> 00:05:57,080
So this is what you see often use especially in natural language processing where you can have a really

71
00:05:57,080 --> 00:06:04,090
long pipeline of things like text feature extraction or moving stop words tokenization limits ization

72
00:06:04,080 --> 00:06:05,160
et cetera.

73
00:06:05,240 --> 00:06:07,690
So the pipeline is a really convenient way to do this.

74
00:06:07,760 --> 00:06:09,590
Right now we have a very kind of short pipeline.

75
00:06:09,620 --> 00:06:14,580
It's just performing vectorization and the support or the support vector classifier you can imagine

76
00:06:14,580 --> 00:06:18,310
that we can provide a lot more steps inside this list of tuples.

77
00:06:18,320 --> 00:06:24,350
So the way to use the pipeline is exactly the same way you would using normal model you simply call

78
00:06:24,350 --> 00:06:27,410
your pipeline object and then say that fit.

79
00:06:27,680 --> 00:06:33,010
But now I can pass in my raw treating data X train and y train.

80
00:06:33,110 --> 00:06:35,640
And this is a lot more convenient than what we did before.

81
00:06:35,750 --> 00:06:42,230
Previously I had to call the vectorize or create an instance of the vectorize or do a fit transform

82
00:06:42,230 --> 00:06:48,560
on my original x train dataset then import the classifier create an instance of the classifier and then

83
00:06:48,560 --> 00:06:53,760
fit on the transformed or vectorize version of the ex-Marines dataset pipeline.

84
00:06:53,940 --> 00:06:55,180
Does all of this for us.

85
00:06:55,310 --> 00:06:59,790
It creates the pipeline and then you get to decide what steps to take.

86
00:06:59,870 --> 00:07:05,250
Do some vectorization first then perform the classification next step so that I can simply say that

87
00:07:05,270 --> 00:07:07,700
fit on the original x train.

88
00:07:07,730 --> 00:07:08,930
Why train data.

89
00:07:09,230 --> 00:07:14,930
So I run that fit here and again or if you get this warning you should eventually see something that

90
00:07:14,930 --> 00:07:17,640
says pipeline and then report back the steps it took.

91
00:07:17,840 --> 00:07:21,110
And then we can test the classifier and display results.

92
00:07:21,110 --> 00:07:27,830
So I can say my predictions is equal to and here I'm just going to call my pipeline object and then

93
00:07:27,860 --> 00:07:34,580
call predict just like I would with any other machine learning model and passing in the raw test data.

94
00:07:34,580 --> 00:07:38,740
So X test remember if we actually take a look at this.

95
00:07:38,750 --> 00:07:43,410
So for instance go below and take a look at what X test looks like.

96
00:07:43,420 --> 00:07:46,200
Remember these are the raw text messages.

97
00:07:46,370 --> 00:07:51,260
And what I'm doing is I'm going to pass in the raw text messages straight into this predictive function.

98
00:07:51,260 --> 00:08:00,170
So now I have these predictions and then I can say from S-K learn that metrics import and I can import

99
00:08:00,260 --> 00:08:07,270
a confusion matrix and a classification report and then see how well I performed.

100
00:08:07,460 --> 00:08:14,800
So I can say Prince the confusion matrix of why test versus my predictions

101
00:08:17,460 --> 00:08:22,710
and here I can see my confusion matrix Lotus's or performing much much better than our first attempt

102
00:08:22,710 --> 00:08:24,300
on this dataset.

103
00:08:24,600 --> 00:08:27,310
And then I can actually print out the classification report as well.

104
00:08:28,630 --> 00:08:33,550
So I can say why test predictions run that.

105
00:08:33,620 --> 00:08:36,070
And here I could see the classification report.

106
00:08:36,080 --> 00:08:38,380
So again notice what's happening here.

107
00:08:38,390 --> 00:08:44,540
I'm calling this all off of this text classifier which was this pipeline object for convenience and

108
00:08:44,540 --> 00:08:51,440
now we're able to see that we get a much better performing classifier when actually vectorize and taken

109
00:08:51,440 --> 00:08:56,450
the message to go ahead and compare these results for the confusion matrix and classification port to

110
00:08:56,450 --> 00:08:57,800
our previous results.

111
00:08:57,800 --> 00:09:00,100
We can also do is check out accuracy.

112
00:09:00,380 --> 00:09:11,420
So I can say from S-K learn import metrics and then I can call metrics accuracy score and then pasan.

113
00:09:11,510 --> 00:09:20,650
My lips are lower case why my tests and predictions and I can see I am achieving almost 90 percent accuracy.

114
00:09:20,900 --> 00:09:26,550
So if you ever want to predict on a new message you just use the same predict method here though he

115
00:09:26,550 --> 00:09:28,630
is on the X test data set.

116
00:09:29,120 --> 00:09:30,940
Let's quickly show you how you can do that.

117
00:09:31,070 --> 00:09:37,910
So I'm going to grab my classifier object text underscore C L F and let's have a predicate on a new

118
00:09:37,910 --> 00:09:40,820
message that I make up whether or not it's spam.

119
00:09:40,820 --> 00:09:45,220
So I'm going to say hi and pass this as a list.

120
00:09:45,530 --> 00:09:46,400
Just a single message.

121
00:09:46,400 --> 00:09:50,270
Hi how are you doing today.

122
00:09:51,470 --> 00:09:57,260
I'm going to run that and it's predicting that it's him that is a legitimate message which makes sense.

123
00:09:57,260 --> 00:09:59,460
This does look like a legitimate message string.

124
00:09:59,510 --> 00:10:01,170
Notice I'm passing it inside of a list.

125
00:10:01,460 --> 00:10:04,070
Let's try predicting on something that is more Spammy.

126
00:10:04,130 --> 00:10:07,150
So you can say congratulations

127
00:10:09,280 --> 00:10:14,760
you've been selected as a winner.

128
00:10:14,760 --> 00:10:20,720
Text 1 2 4 4 or whatever some sort of number.

129
00:10:20,910 --> 00:10:22,350
Congratulations.

130
00:10:22,350 --> 00:10:25,450
Free entry to contest.

131
00:10:25,500 --> 00:10:25,840
Let's see.

132
00:10:25,890 --> 00:10:26,990
Let's see if that's spammy enough.

133
00:10:27,000 --> 00:10:31,530
We run that and it looks like it does predict that is a spam text message.

134
00:10:31,530 --> 00:10:33,500
Keep mine depending on what string you pass.

135
00:10:33,510 --> 00:10:38,280
It may or may not predict that it's spam and spam is kind of harder to make a spam message.

136
00:10:38,280 --> 00:10:39,270
It looks bad enough.

137
00:10:39,510 --> 00:10:43,350
But if you pass in an oral message it should hopefully predict ham.

138
00:10:43,470 --> 00:10:43,970
OK.

139
00:10:44,160 --> 00:10:49,230
So we just learn how to actually build out a pipeline train a classifier display the results and predict

140
00:10:49,440 --> 00:10:55,000
on our X test dataset as well as predicting on single new messages coming up next.

141
00:10:55,020 --> 00:10:58,680
We're going to go through an entire code along Tech's classification project.

142
00:10:58,710 --> 00:10:59,330
We'll see if there.

