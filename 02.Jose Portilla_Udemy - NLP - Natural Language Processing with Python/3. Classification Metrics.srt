1
00:00:05,430 --> 00:00:10,410
Welcome back everyone to this lecture on classification metrics.

2
00:00:10,540 --> 00:00:16,840
We just learn that after a machine learning process is complete we use performance metrics to evaluate

3
00:00:16,870 --> 00:00:18,700
how well our model did.

4
00:00:18,700 --> 00:00:25,270
Remember in that train test split step we ended up running our train model on some test data and then

5
00:00:25,270 --> 00:00:27,220
evaluating its performance.

6
00:00:27,220 --> 00:00:32,530
In this lecture we're actually going to discuss in more detail how we actually evaluate the performance

7
00:00:32,590 --> 00:00:33,410
of a model.

8
00:00:33,630 --> 00:00:38,700
And that comes in discussion with classification metrics.

9
00:00:38,850 --> 00:00:45,360
The key classification metrics we need to understand our accuracy recall precision and EF 1 score which

10
00:00:45,360 --> 00:00:49,310
is essentially a combination of recall and precision.

11
00:00:49,340 --> 00:00:53,210
But first we should really understand the reasoning behind these metrics and how they actually work

12
00:00:53,210 --> 00:00:54,310
in the real world.

13
00:00:54,350 --> 00:01:01,250
So we're going to follow along a little bit in a process with some more realistic example such as classifying

14
00:01:01,280 --> 00:01:10,130
a spam versus ham message so typically in any classification task your model can only achieve two results.

15
00:01:10,310 --> 00:01:15,540
Either your model was correct in its prediction or your model was incorrect in its prediction.

16
00:01:16,600 --> 00:01:19,120
Unfortunately incorrect versus correct.

17
00:01:19,120 --> 00:01:21,660
That expands to situations where you have multiple classes.

18
00:01:21,790 --> 00:01:26,500
It doesn't matter if you're trying to predict categories with eight different types of classes or eight

19
00:01:26,500 --> 00:01:27,950
different types of categories.

20
00:01:27,970 --> 00:01:32,820
Your model fundamentally only has two outputs either a correct output or incorrect output.

21
00:01:32,860 --> 00:01:37,690
When it comes classification now for the purposes of explaining the metrics we're going to imagine a

22
00:01:37,750 --> 00:01:46,610
binary classification situation where we only have two available classes spam versus him in our example.

23
00:01:46,630 --> 00:01:52,630
We're going to be attempting to predict if a text message is spam or ham ham being another word for

24
00:01:52,630 --> 00:01:54,660
a legitimate message.

25
00:01:54,910 --> 00:02:01,300
Since this is a supervised learning process we were going to first fit or train a model on training

26
00:02:01,300 --> 00:02:02,140
data.

27
00:02:02,170 --> 00:02:05,300
Then we will test the model on testing data.

28
00:02:05,530 --> 00:02:11,560
Once we have the models predictions from the XT test data we compare it to the true y values that is

29
00:02:11,560 --> 00:02:17,590
the correct labels because remember in a supervised learning process we're dealing with historical information

30
00:02:17,890 --> 00:02:21,720
that we already have the labels for.

31
00:02:21,730 --> 00:02:27,970
Keep in mind there are a few steps to convert the raw text message information into a format that the

32
00:02:27,970 --> 00:02:29,640
machine learning model can understand.

33
00:02:29,830 --> 00:02:34,390
So whenever we're dealing with raw text we're gonna have to do a little bit of work in vectorization

34
00:02:34,750 --> 00:02:40,900
in order to translate the raw text into numerical information the machine learning model can understand.

35
00:02:40,900 --> 00:02:46,000
So whether it's a text message from a cell phone whether it's an email whether it's a movie review that's

36
00:02:46,000 --> 00:02:51,010
been written down there will be some steps that we're going to cover later on in actually converting

37
00:02:51,010 --> 00:02:55,960
that raw text information into numerical information rather discuss these methods in much more detail

38
00:02:55,960 --> 00:02:56,580
later on.

39
00:02:56,710 --> 00:03:01,960
So just to give you a brief overview of kind of the process of vectorization going to wave our hands

40
00:03:01,960 --> 00:03:02,660
a little bit here.

41
00:03:02,680 --> 00:03:08,520
But essentially what we're doing is we're taking some sort of rock text message from the testing dataset

42
00:03:08,590 --> 00:03:13,550
that is the X test that we saw in the previous lecture we passes through some vectorize.

43
00:03:13,630 --> 00:03:16,300
And then we have a vectorized version of that.

44
00:03:16,300 --> 00:03:21,220
So to give you just a brief idea of what that actually is or that would look like Essentially we would

45
00:03:21,220 --> 00:03:22,920
have some raw text information.

46
00:03:22,930 --> 00:03:26,050
So here's a text message from a cell phone saying hey how are you doing.

47
00:03:26,050 --> 00:03:27,880
I've been doing well et cetera.

48
00:03:27,910 --> 00:03:32,620
Theoretically after we pass it through the vector laser it should be formatted in some sort of vectorize

49
00:03:32,620 --> 00:03:37,090
format that has a numerical matrix that the machine learning model can then understand.

50
00:03:37,090 --> 00:03:41,860
You can imagine that we can do things such as count the number of times certain words show up and that

51
00:03:41,860 --> 00:03:46,660
would be numerical information and we would format it in a way that the machine learning model can understand

52
00:03:47,100 --> 00:03:48,700
or to learn about this entire process.

53
00:03:48,700 --> 00:03:54,220
In a lot more detail things like term frequency inverse document frequency bag of words.

54
00:03:54,220 --> 00:03:59,270
Those are all different processes that we can employ in order to vectorize text information the way

55
00:03:59,270 --> 00:04:05,470
to learn about that in the feature or text feature extraction lecture right now who can kind of imagine

56
00:04:05,470 --> 00:04:09,600
behind the scenes or some sort of vectorization process taking place.

57
00:04:09,700 --> 00:04:11,820
So we set up this vectorization in a pipeline.

58
00:04:12,040 --> 00:04:16,480
And again there's many ways of transforming that raw text into murky information.

59
00:04:16,480 --> 00:04:21,640
For right now since we want to focus on understanding classification metrics we're just going to assume

60
00:04:21,640 --> 00:04:28,370
that there were some underlying vectorization process that took place OK so it's actually jump right

61
00:04:28,430 --> 00:04:33,440
into the middle of that machine learning process pipeline that we saw in the previous lecture and assume

62
00:04:33,800 --> 00:04:38,900
that we have a trained model and the model is trained on the training data.

63
00:04:38,930 --> 00:04:43,730
So we took in our next train and our white train and then we train the model.

64
00:04:43,730 --> 00:04:46,760
Now it comes time to actually evaluate the models performance.

65
00:04:46,760 --> 00:04:52,960
So what we need to do is taken our test dataset and then pass it into the train model.

66
00:04:52,970 --> 00:05:00,190
So for example I go ahead and grab a test message from X test and if you're wondering what X test means

67
00:05:00,200 --> 00:05:02,560
go ahead and watch the previous lecture we explain that there.

68
00:05:02,840 --> 00:05:08,540
So we have this text message from a cell phone and it's from the X test dataset and we pass it into

69
00:05:08,540 --> 00:05:15,740
the train model that I remember because a test message and this is supervised learning with historical

70
00:05:15,740 --> 00:05:17,210
labeled information.

71
00:05:17,330 --> 00:05:21,680
We actually already know whether or not this is a ham or spam.

72
00:05:21,680 --> 00:05:26,540
So keep in mind we always already know the correct label for this particular test data point that's

73
00:05:26,540 --> 00:05:32,620
going to allow sexually compare the output or the train model whether it's correct or incorrect.

74
00:05:32,830 --> 00:05:37,780
So we pass it through a train model and let's say in this particular instance the train model said I

75
00:05:37,780 --> 00:05:42,810
think this text message is ham or it's a legitimate test message.

76
00:05:42,940 --> 00:05:49,090
Then all we do is we simply compare the correct label that we already know to be true against the prediction

77
00:05:49,090 --> 00:05:50,690
that the train model output.

78
00:05:50,710 --> 00:05:52,510
So here we say it was hand equal to him.

79
00:05:52,540 --> 00:05:53,830
And in this case it's correct.

80
00:05:53,830 --> 00:05:56,880
So here we get a correct prediction.

81
00:05:56,890 --> 00:06:02,080
Keep in mind that for other data points the train model could be incorrect.

82
00:06:02,110 --> 00:06:05,520
It might say Oh I think this text message is spam.

83
00:06:05,680 --> 00:06:09,040
And in that case it would be incorrect for action.

84
00:06:09,910 --> 00:06:15,790
So what we're going to do is we just repeat this process for all the text messages inside of our test

85
00:06:15,790 --> 00:06:16,620
data.

86
00:06:16,870 --> 00:06:22,950
At the end we're going to have a count of correct matches and accounts of incorrect matches.

87
00:06:22,960 --> 00:06:28,420
Here's the absolute key realization probably the most fundamental part of this particular lecture in

88
00:06:28,420 --> 00:06:30,100
real world situations.

89
00:06:30,100 --> 00:06:36,970
Not all incorrect or correct matches hold equal value which is why we actually have those various classification

90
00:06:36,970 --> 00:06:37,710
metrics.

91
00:06:37,720 --> 00:06:44,380
It's not enough to understand that you got a particular count of correct versus incorrect its various

92
00:06:44,380 --> 00:06:48,670
ratios that we need to take into account.

93
00:06:48,700 --> 00:06:52,510
So in the real world a single metric won't tell the complete story.

94
00:06:52,510 --> 00:06:56,380
So to understand all of this we're going to bring back those four metrics we mentioned at the beginning

95
00:06:56,800 --> 00:07:01,480
and see how they're actually calculated and we could actually organize or predicted values compared

96
00:07:01,480 --> 00:07:04,090
to the real values in what's known as a confusion matrix.

97
00:07:04,240 --> 00:07:10,690
And the next lecture were to have a whole discussion of the confusion matrix the freight Now let's look

98
00:07:10,690 --> 00:07:18,280
at accuracy accuracy in classification problems is the number of correct predictions made by the model

99
00:07:18,550 --> 00:07:21,310
divided by the total number of predictions.

100
00:07:21,310 --> 00:07:27,700
So again what we do is we count up our Telia how many times the model was correct and then we divide

101
00:07:27,700 --> 00:07:30,570
it by the total number of predictions the model made.

102
00:07:30,670 --> 00:07:38,920
For example if the test dataset was 100 messages in our model correctly predicted 80 of those messages

103
00:07:38,920 --> 00:07:45,400
from the test dataset then we would have 80 out of a total of 100 predictions that were done correctly.

104
00:07:45,400 --> 00:07:48,680
And that means we were 0.8 or 80 percent accurate.

105
00:07:50,810 --> 00:07:54,610
Accuracy is really useful when target classes are well balanced.

106
00:07:54,620 --> 00:07:59,510
So in that example it would mean that we have roughly the same amount of spam messages in our test data

107
00:07:59,870 --> 00:08:03,640
as we have ham or legitimate messages.

108
00:08:03,650 --> 00:08:05,170
Now here's a problem.

109
00:08:05,180 --> 00:08:09,520
Accuracy is not a good choice with unbalanced classes.

110
00:08:09,830 --> 00:08:17,240
Let's imagine a situation where we had ninety nine legitimate ham messages in a one spam text message.

111
00:08:17,330 --> 00:08:20,990
If our model was a simple line of code that's always predicted him.

112
00:08:21,140 --> 00:08:26,750
That means we would get ninety nine percent accuracy even though it was incorrect on every single instance

113
00:08:26,810 --> 00:08:28,000
of spam.

114
00:08:28,070 --> 00:08:34,420
We got unlucky because we had an unbalanced class situation and our test data set.

115
00:08:34,540 --> 00:08:36,840
So in this situation we're going to want to understand.

116
00:08:36,850 --> 00:08:42,670
Recall and precision that's going to give us a better understanding of how it performs.

117
00:08:42,790 --> 00:08:50,580
On the other smaller class so let's quickly go over some formal definitions of precision.

118
00:08:50,580 --> 00:08:55,790
Recall an EF 1 score if one score is simply a combination of precision recall.

119
00:08:56,010 --> 00:08:59,400
We're going to go over their formal definitions and I think usually they make a lot more sense when

120
00:08:59,400 --> 00:09:02,070
you see them in combination with the confusion matrix.

121
00:09:02,070 --> 00:09:06,430
So keep in mind in the next lecture you'll see a confusion matrix which should clear up precision.

122
00:09:06,450 --> 00:09:09,020
Recall an EF 1 score with further detail.

123
00:09:09,030 --> 00:09:10,720
Let's start with free call.

124
00:09:10,800 --> 00:09:16,910
Recall is the ability of the machine learning model to find all the relevant cases within a data set

125
00:09:17,610 --> 00:09:23,460
and the actual mathematical formula for definition of recall is going to be the number of true positives

126
00:09:23,940 --> 00:09:28,280
divided by the number of true positives plus the number of false negatives.

127
00:09:28,350 --> 00:09:31,650
And when we see this in combination with the confusion matrix they'll make a lot more sense.

128
00:09:31,650 --> 00:09:35,640
So keep in mind the next lecture we're actually going to see the formulas and how they're calculated.

129
00:09:35,820 --> 00:09:38,340
So don't worry too much about these formal definitions.

130
00:09:39,540 --> 00:09:45,530
Precision is the ability of a classification model to identify only the relevant data points.

131
00:09:45,540 --> 00:09:50,390
Precision is defined as a number of true positives divided by the number of true positives plus the

132
00:09:50,390 --> 00:09:56,100
number of false positives in the next picture we'll talk more about true positives false positives true

133
00:09:56,130 --> 00:09:57,700
negatives and false negatives.

134
00:09:59,510 --> 00:10:05,420
So keep in mind often you have a tradeoff between recall a precision wall recall expresses the ability

135
00:10:05,720 --> 00:10:12,530
to find all relevant instances in the data set precision expresses the proportion of the data points.

136
00:10:12,530 --> 00:10:16,210
Our model says was relevant that actually were relevant.

137
00:10:18,190 --> 00:10:24,380
And in cases where we want to find an optimal blend of precision and recall we can combine the two metrics

138
00:10:24,410 --> 00:10:31,810
using what is called the EF 1 score the one score is the harmonic mean of precision and recall taking

139
00:10:31,840 --> 00:10:35,120
both metrics into account in the following equation.

140
00:10:35,170 --> 00:10:41,290
We say if one score is equal to two times and in the numerator we have precision times recall and then

141
00:10:41,290 --> 00:10:45,730
in denominator we have position of plus recall this is known as taking the harmonic mean in order to

142
00:10:45,730 --> 00:10:47,710
say normal mean.

143
00:10:47,910 --> 00:10:53,510
And the reason we use the harmonic mean instead of a simple average is because it punishes extreme values.

144
00:10:53,730 --> 00:10:59,530
Let's imagine we had a classifier for a precision of one point zero but a horrible recall of 0.0.

145
00:10:59,700 --> 00:11:05,070
If we were to perform the normal mean we'd end up getting in a score of 0.5.

146
00:11:05,070 --> 00:11:06,900
So that's just a simple average.

147
00:11:06,990 --> 00:11:11,790
If we take the harmonic mean we can be a little smarter about it and really punish this model for having

148
00:11:11,790 --> 00:11:17,670
a horrible recall because we're going to say precision times recall then that's going to say one time

149
00:11:17,670 --> 00:11:18,780
zero is zero.

150
00:11:18,790 --> 00:11:23,130
So that means the score automatically becomes very small or zero.

151
00:11:23,340 --> 00:11:28,220
If one of these values either precision or recall happens to be very poor.

152
00:11:28,230 --> 00:11:33,930
So again it's a better display of the optimal blend than just the simple average because it's punishing

153
00:11:34,140 --> 00:11:35,580
extreme values.

154
00:11:37,260 --> 00:11:42,090
So persistent recall as I keep mentioning typically make more sense in the context of the confusion

155
00:11:42,090 --> 00:11:42,820
matrix.

156
00:11:42,840 --> 00:11:47,640
So in this next lecture coming up we're going explore his confusion matrix and again talk again in more

157
00:11:47,640 --> 00:11:53,170
detail about the different classification metrics we'll be using throughout this section of the course.

158
00:11:53,280 --> 00:11:54,300
We'll see at the next lecture.

