1
00:00:00,300 --> 00:00:01,290
Welcome back.

2
00:00:01,290 --> 00:00:07,260
Let's get started by learning and exploring word vectors that come already prepackaged for us with Spacey

3
00:00:07,350 --> 00:00:08,700
and python.

4
00:00:08,700 --> 00:00:13,920
Please remember to watch the previous lecture in order to download the larger Spacey English language

5
00:00:13,920 --> 00:00:16,380
models that actually contain the word vectors.

6
00:00:16,380 --> 00:00:18,690
The smaller models do not contain the word vectors.

7
00:00:19,110 --> 00:00:21,830
OK let's jump to the Jupiter notebook.

8
00:00:21,870 --> 00:00:22,100
OK.

9
00:00:22,110 --> 00:00:24,690
Let's begin by importing Spacey.

10
00:00:24,960 --> 00:00:30,840
And then we're going to load the large English or medium English download that we discussed in the previous

11
00:00:30,840 --> 00:00:32,010
lecture.

12
00:00:32,010 --> 00:00:39,300
So go ahead and say Spacey don't load and then even underscore core underscore web underscore and depending

13
00:00:39,300 --> 00:00:45,720
on how you proceeded in the previous lecture for dialing the libraries I would do M.D. or LG or underscore

14
00:00:46,530 --> 00:00:51,460
LG I went ahead and downloaded the large library so I'll go ahead and do that.

15
00:00:51,480 --> 00:00:57,510
Keep in mind the medium core English library and the large English core library We're both trained using

16
00:00:57,510 --> 00:01:00,240
the same familiar word to that family of algorithms.

17
00:01:00,360 --> 00:01:02,150
So they should be pretty similar.

18
00:01:02,280 --> 00:01:06,660
Once this is loaded up and keep in mind if you're loading up the large one it may take a while but once

19
00:01:06,660 --> 00:01:12,950
that's loaded up all you need to do is say LP and then pass in a unicode string.

20
00:01:12,990 --> 00:01:16,130
So for example we can say lion.

21
00:01:16,200 --> 00:01:22,130
And then once you do that you should be able to access an attribute for vector off of that.

22
00:01:22,140 --> 00:01:27,420
And so now here we have the actual vector components for the string lion.

23
00:01:27,420 --> 00:01:29,810
You'll notice that there's a ton of dimensions here.

24
00:01:29,850 --> 00:01:35,150
So that means we have a lot of information contained in this vector version of the word.

25
00:01:35,160 --> 00:01:41,310
Now what's interesting is that Doc and span objects themselves also have vectors and these vectors are

26
00:01:41,310 --> 00:01:46,620
then derived from the averages of the individual token vectors and that essentially allows you to not

27
00:01:46,620 --> 00:01:52,110
just preferred or perform a word to vector but actually document to check where the document itself

28
00:01:52,410 --> 00:01:54,800
is an average of all the words.

29
00:01:54,870 --> 00:02:01,470
So just to show you this you could say something like the quick brown fox jumped and if you run that

30
00:02:01,560 --> 00:02:04,170
and then ask for the vector notice here what happens.

31
00:02:04,200 --> 00:02:09,600
We get back a vector and we can check up the shape of this vector since it's essentially an umpire.

32
00:02:10,890 --> 00:02:16,560
And there's three hundred different dimensions to this vector that the formatting was slightly different

33
00:02:16,650 --> 00:02:17,710
than a single word.

34
00:02:17,940 --> 00:02:20,560
So it can say an LP you hear.

35
00:02:20,650 --> 00:02:25,810
Now let's just say Fox lost for the vector and ask for the shape here.

36
00:02:25,950 --> 00:02:27,240
But the dimensions are the same.

37
00:02:27,270 --> 00:02:29,070
So we have three hundred by three hundred.

38
00:02:29,850 --> 00:02:34,710
So what that basically means is there's three dimensions to the vector for this essentially a document

39
00:02:34,710 --> 00:02:40,560
here and through dimensions for the single word this document is just the average of all the singular

40
00:02:40,560 --> 00:02:42,660
vectors for all the words there.

41
00:02:42,660 --> 00:02:45,870
So now let's try identifying similar vectors.

42
00:02:45,870 --> 00:02:51,210
The best way to expose vector relationships is through the dots similarity method of the actual document

43
00:02:51,210 --> 00:02:51,990
tokens.

44
00:02:51,990 --> 00:03:00,070
So I'm going to create some tokens by simply saying an LP and then we're gonna pass in some similar

45
00:03:00,070 --> 00:03:05,490
words but not the exact same words we'll say lion cat and pet.

46
00:03:05,560 --> 00:03:09,610
So just naturally we can tell there's probably some sort of relationship between things like cat and

47
00:03:09,610 --> 00:03:10,260
pet.

48
00:03:10,300 --> 00:03:16,360
Cats are often pets and we also know that lions and cats are similar just because a lion is kind of

49
00:03:16,360 --> 00:03:17,070
a form of a cat.

50
00:03:17,080 --> 00:03:20,350
They're essentially from the same family of animals.

51
00:03:20,350 --> 00:03:29,230
So then we're going to do is say for token one in tokens and we'll have a nested for loop then for talking

52
00:03:29,290 --> 00:03:33,830
to in tokens essentially comparing each word to every other word.

53
00:03:34,060 --> 00:03:45,790
We'll say print out the token one text then the token to text and then grab the first token and check

54
00:03:45,790 --> 00:03:50,250
its similarity with the second token to a token too.

55
00:03:50,520 --> 00:03:55,190
And then as here you just grab similarly directly off the token not off the text.

56
00:03:55,210 --> 00:04:01,560
So what's interesting here is on the outputs you'll notice that line line cat cat and pet pet have 100

57
00:04:01,660 --> 00:04:03,310
percent similarity between each other.

58
00:04:03,310 --> 00:04:07,180
So the similarity values are going to be between 0 and 1.

59
00:04:07,270 --> 00:04:13,450
So it makes sense that a word is totally similar to itself but what's really interesting here is that

60
00:04:13,510 --> 00:04:19,790
the word vectors contain enough information to realize that line and cat do have some similarity.

61
00:04:19,840 --> 00:04:21,460
They're at zero point five too.

62
00:04:21,880 --> 00:04:24,400
So that means they're above zero point five similarity.

63
00:04:24,400 --> 00:04:29,500
And what's really interesting is that cat and pet tend to have a very high similarity because most cats

64
00:04:29,590 --> 00:04:35,600
are gonna be pets and pets are essentially at least Tyrion I'd say it's either cats dogs or birds.

65
00:04:35,620 --> 00:04:40,150
So it makes sense that they have a very high similarity and it makes even more sense that line and pet

66
00:04:40,450 --> 00:04:42,420
have a similarity less than zero point five.

67
00:04:42,430 --> 00:04:47,440
It's probably not the best idea to have a line as a pet although some people do have that.

68
00:04:47,500 --> 00:04:51,460
So we can already see that we have established relationships just from the word of actors themselves

69
00:04:51,820 --> 00:04:59,410
essentially all this is doing is checking the cosine similarity between token one and token to something

70
00:04:59,410 --> 00:05:05,020
you should keep in mind is that words that have opposite meaning but that often appear in the same context

71
00:05:05,140 --> 00:05:13,320
may actually have similar vectors as well so let's put in three other words here we'll say like love

72
00:05:13,800 --> 00:05:19,860
and then hate so you can imagine that love and hate are very different words of very different meanings

73
00:05:20,220 --> 00:05:24,340
however they're used often in the same typical context.

74
00:05:24,450 --> 00:05:28,080
You either love a movie or you hate the movie Love a book or hate a book.

75
00:05:28,410 --> 00:05:33,300
So they're very similar words in that aspect in that they are often used in similar contexts.

76
00:05:33,300 --> 00:05:37,380
So words like that can often have very similar vectors.

77
00:05:37,380 --> 00:05:43,170
So we're going to run this tokens again and rerun this cell and so we can see again like and like love

78
00:05:43,170 --> 00:05:50,400
and love and hate and hate are very similar but you'll notice that like and love and like and hate almost

79
00:05:50,400 --> 00:05:57,700
have the exact same similarity relationship as well as love and hate here are also quite similar.

80
00:05:57,750 --> 00:06:04,110
So keep in mind that if the words are used in similar contexts they may also be similar even if in normal

81
00:06:04,140 --> 00:06:07,190
English they have the opposite meaning.

82
00:06:07,390 --> 00:06:14,680
Now it's sometimes helpful to aggregate three hundred dimensions into a Euclidean L2 norm.

83
00:06:14,680 --> 00:06:20,470
And so basically what that means is we're going compute this as the square root of the sum of squared

84
00:06:20,620 --> 00:06:27,390
vectors and this is actually accessible as an attribute of the token called vector underscore norm.

85
00:06:27,640 --> 00:06:35,880
And there's a couple other helpful attributes including has vector and is Ovi or is out of vocabulary.

86
00:06:35,950 --> 00:06:43,450
So if we want to take a look at our current vocabulary we can just say MLP dot vocab dot vectors.

87
00:06:43,560 --> 00:06:46,590
And so this is the list of all the vectors in the vocabulary.

88
00:06:46,590 --> 00:06:52,860
So we could just check the length of this and here we see six hundred eighty four thousand eight hundred

89
00:06:52,860 --> 00:06:53,880
thirty one.

90
00:06:53,910 --> 00:06:58,980
So that means there is essentially around six hundred eighty five thousand unique words in the vocabulary

91
00:06:59,280 --> 00:07:00,690
that we have vectors for.

92
00:07:00,810 --> 00:07:03,190
So if we were to check the shape of this.

93
00:07:03,210 --> 00:07:08,820
So learning to check the length just the shape of how this is held you'll let us it's this many words

94
00:07:08,880 --> 00:07:13,320
by three hundred dimensions for each particular word vector.

95
00:07:13,320 --> 00:07:19,800
So out of those around six or eight five thousand words we're gonna be able to understand if we have

96
00:07:19,800 --> 00:07:22,020
a new word that is outside of the vocabulary.

97
00:07:22,080 --> 00:07:24,610
So that could be someone's proper name.

98
00:07:24,600 --> 00:07:28,390
Maybe there's an unusual name so that one's not going to have a vector.

99
00:07:28,440 --> 00:07:30,350
We'll go ahead and check that out.

100
00:07:30,390 --> 00:07:35,070
We're going to say tokens is equal to an LP.

101
00:07:35,180 --> 00:07:42,600
Now let's provide a string with dog cat and then I'm going to kind of make something up so normal.

102
00:07:42,660 --> 00:07:43,240
There you go.

103
00:07:43,320 --> 00:07:47,680
And we'll say for token in tokens prints.

104
00:07:47,880 --> 00:07:50,840
We'll print out the token text.

105
00:07:51,150 --> 00:07:58,380
We'll print out has vector essentially answering true or false thus this sexual word have a vector associated

106
00:07:58,380 --> 00:08:02,930
with it and then we'll say token dot vector underscore.

107
00:08:03,060 --> 00:08:12,640
Norm and they will say token thought is underscore O O V which stands for out of vocabulary.

108
00:08:12,640 --> 00:08:20,230
So when we run this as we may expect the dog and cat tokens do have vectors true and true and they have

109
00:08:20,230 --> 00:08:22,250
these normalized representations.

110
00:08:22,300 --> 00:08:26,580
Essentially the sum of the squares of all three hundred dimensions.

111
00:08:26,590 --> 00:08:32,290
Now how interpretable that is is sometimes a little hard reducing three dimensions just one scalar value.

112
00:08:32,320 --> 00:08:37,210
So it may not be very interpretable but keep in mind it is kind of there for convenience.

113
00:08:37,210 --> 00:08:42,850
And then this last attribute is out of vocabulary is false for the first two because dog and cat are

114
00:08:42,850 --> 00:08:47,230
in the vocabulary of these six hundred eighty four thousand ish words.

115
00:08:47,260 --> 00:08:49,870
However this kind of made up word is false.

116
00:08:49,870 --> 00:08:54,940
It doesn't have a vector so that means it returns back a norm of zero point zero and is it out of the

117
00:08:54,940 --> 00:08:55,810
vocabulary.

118
00:08:55,870 --> 00:08:56,860
It's true.

119
00:08:56,860 --> 00:09:01,040
So keep in mind common names may actually be vector.

120
00:09:01,060 --> 00:09:06,130
So if we just put John in there that actually does have a vector associated with it and even uncommon

121
00:09:06,130 --> 00:09:06,420
names.

122
00:09:06,430 --> 00:09:11,770
So if I were to put in my middle name that actually is in the vocabulary which is kind of interesting.

123
00:09:11,860 --> 00:09:16,480
But if we kind of misspell this as something stranger that's gonna be false.

124
00:09:16,520 --> 00:09:22,210
Okay so let's say you can check and understand that Spacey doesn't actually have a problem with words

125
00:09:22,240 --> 00:09:26,710
in an MLP documents that it doesn't have a vector for they'll just go ahead and assign them a vector

126
00:09:26,710 --> 00:09:31,450
norm of zero point zero and then inform you that it is outside of its vocabulary and it doesn't have

127
00:09:31,450 --> 00:09:32,610
a vector.

128
00:09:32,710 --> 00:09:35,820
So the last thing you really want to explore here is vector arithmetic.

129
00:09:35,860 --> 00:09:41,380
So believe or not we can actually calculate a new vector by adding and subtracting related vectors from

130
00:09:41,380 --> 00:09:48,040
these words and a really famous example is king minus man plus woman is equal to Queen and we talked

131
00:09:48,040 --> 00:09:49,380
about this previously.

132
00:09:49,480 --> 00:09:52,110
So let's go ahead and try it out ourselves.

133
00:09:52,120 --> 00:09:59,120
So in order to do this we need to be able to calculate the cosine similarity ourselves.

134
00:09:59,680 --> 00:10:08,320
So we're going to say from side pi import spatial and then we're going to essentially write the formula

135
00:10:08,590 --> 00:10:14,080
for cosine similarity and we're going to write it as a lambda expression just for convenience.

136
00:10:14,080 --> 00:10:22,480
So say lambda and we'll say that one Vec two and we'll say 1 minus.

137
00:10:22,750 --> 00:10:28,990
And this is why we're importing side PI will say spatial thought distance thought cosine so essentially

138
00:10:29,020 --> 00:10:32,960
cosine function there and we'll say Vec 1 vector.

139
00:10:33,520 --> 00:10:39,490
So this essentially just returns back cosine so one minus that cosine similarity.

140
00:10:39,490 --> 00:10:44,950
So how this little land the expression for cosine similarity and then we're going to do is we'll create

141
00:10:44,950 --> 00:10:47,220
three variables one for King.

142
00:10:47,410 --> 00:10:56,440
So out of our vocab we'll go ahead and grab King and then ask for its vector and we'll do the same for

143
00:10:56,800 --> 00:11:06,420
man so MLP the vocab man ask for that vector and then we'll say the same for women.

144
00:11:06,500 --> 00:11:13,820
So MLP that vocab woman and then grab that vector.

145
00:11:13,910 --> 00:11:21,380
So the idea that we're going to do here is we're going to ask for the king vector minus the man vector

146
00:11:22,070 --> 00:11:28,790
plus the woman vector and we hope that somewhere along the most similar vectors to this like brand new

147
00:11:28,790 --> 00:11:36,740
vector we hope that this new vector is somehow similar to Queen or possibly princess or Highness or

148
00:11:36,740 --> 00:11:37,840
something like that.

149
00:11:38,000 --> 00:11:44,000
Essentially we're assuming that somewhere in those three hundred dimensions there is some sort of understanding

150
00:11:44,120 --> 00:11:50,870
of maybe formality or royalty with the words King and there's some sort of understanding of the words

151
00:11:51,230 --> 00:11:58,340
gender so we understand that kings are men and if we were to subtract the gender dimension from King

152
00:11:58,520 --> 00:12:04,820
and then add woman to it maybe we get something that has royalty as well as those aspects of something

153
00:12:04,820 --> 00:12:07,220
female like Queen Princess or Highness.

154
00:12:07,220 --> 00:12:08,130
So that's our hope.

155
00:12:08,150 --> 00:12:15,600
Again this isn't kind of a perfect science so we'll try it out who will say our new vector is equal

156
00:12:15,600 --> 00:12:20,260
to and here we're just gonna say King minus man plus woman.

157
00:12:20,400 --> 00:12:26,190
So I can just add and subtract these vectors so there's our new vector and then we're going to say computed

158
00:12:27,610 --> 00:12:36,610
similarities is equal to an empty list and we'll do a little for loop here we'll say for word in an

159
00:12:36,610 --> 00:12:43,530
LP dot vocab so notice I'm going through the entire vocabulary and I'm going to search for words that

160
00:12:43,540 --> 00:12:45,660
have very similar values.

161
00:12:45,660 --> 00:12:51,790
This new vector essentially checking for the cosine similarity for between this new vector and all the

162
00:12:51,790 --> 00:12:53,590
words in my current vocabulary.

163
00:12:53,650 --> 00:13:01,720
So essentially for all words in my vocab so all six hundred and or a hundred and eighty four six or

164
00:13:01,750 --> 00:13:06,670
eighty four thousand ish words what we're going to do is say four word in that vocab.

165
00:13:06,670 --> 00:13:17,520
I'll say if that word actually has a vector associated with it I'll say if that word is lowercase and

166
00:13:17,640 --> 00:13:25,140
then I'll do one more thing if that word is Alpha which essentially means it's not like a number or

167
00:13:25,140 --> 00:13:26,490
something.

168
00:13:26,640 --> 00:13:33,640
Then what I'm going to do is I'm going to calculate its similarity and I will say cosine essentially

169
00:13:33,640 --> 00:13:40,420
calling my lambda function on between my new vector that I just calculated using my custom formula of

170
00:13:40,420 --> 00:13:50,680
King minus me and policewoman and then that current words vector and then I will say computed similarities

171
00:13:51,720 --> 00:13:58,760
and I am going to append and what I'm going to a pen whips is a tuple so I'll put another set of parentheses

172
00:13:58,760 --> 00:14:08,680
there and I'm going to pass in the word itself as well as the similarity value OK so let's make sure

173
00:14:08,680 --> 00:14:12,340
I don't have a typo there whoops.

174
00:14:12,350 --> 00:14:14,330
It should be has vector.

175
00:14:14,390 --> 00:14:14,960
There we go.

176
00:14:14,990 --> 00:14:15,820
Run that.

177
00:14:15,860 --> 00:14:21,470
So keep in mind this may take a little bit of time since we are running it across around six or a four

178
00:14:21,560 --> 00:14:28,610
thousand words but once that's done running well we're going to do is we're going to sort this list

179
00:14:28,610 --> 00:14:29,780
of tuples.

180
00:14:29,780 --> 00:14:40,340
So I'm going to say my computed similarities so computed similarities is going to be equal to sort in

181
00:14:42,130 --> 00:14:45,430
computed similarities.

182
00:14:45,430 --> 00:14:51,010
And then what I'm going to do is specify there's actually a special form of sorting if you have a list

183
00:14:51,010 --> 00:14:56,120
of tuples or we can sort by the very first item it's the way we do this.

184
00:14:56,380 --> 00:14:58,100
We simply say lambda is

185
00:15:01,030 --> 00:15:07,380
lambda item and then we'll say minus item one.

186
00:15:07,400 --> 00:15:11,270
So what that means is essentially going to come in descending order.

187
00:15:11,270 --> 00:15:12,740
That's what this minus is for.

188
00:15:12,740 --> 00:15:20,150
So descending order of the item at index 1 which is this similarity essentially is kind of a fancy way

189
00:15:20,210 --> 00:15:24,800
of sorting by these tuples based on their similarity value in descending order.

190
00:15:24,800 --> 00:15:28,250
If you didn't have this minus here it would do him in ascending order and then you would get the least

191
00:15:28,250 --> 00:15:29,660
similar words.

192
00:15:29,660 --> 00:15:34,520
So we're gonna go ahead and sort that so run that sorting that one shouldn't take too long.

193
00:15:34,670 --> 00:15:42,000
And then let's go ahead and print out the top 10 most similar words so we're going to say print and

194
00:15:41,990 --> 00:15:44,090
we can do this with list comprehension or before loop.

195
00:15:44,090 --> 00:15:45,800
What are you more comfortable with.

196
00:15:45,800 --> 00:15:52,940
We'll say remember we have a tuple here of a word and similarity so gram the first item in the tuple

197
00:15:52,940 --> 00:16:00,800
which is the word and then ask for its text and we'll do that for tuple in computed similarities.

198
00:16:00,800 --> 00:16:03,460
And we're just going to go through the top 10.

199
00:16:03,560 --> 00:16:05,520
So colon 10.

200
00:16:05,630 --> 00:16:08,140
So let's run that and there we go.

201
00:16:08,920 --> 00:16:16,030
So we have king queen Prince Kings princess royal throne Queen's monarch and King them.

202
00:16:16,030 --> 00:16:21,560
So in this case what's interesting is that the word King is still the most similar.

203
00:16:21,760 --> 00:16:25,110
When you do this new vector of King minus man plus woman.

204
00:16:25,210 --> 00:16:31,210
So trying to remove the man gender and then adding the woman gender still doesn't remove enough information

205
00:16:31,210 --> 00:16:37,120
about King to make a new vector the most similar however knows what happens the second most similar

206
00:16:37,540 --> 00:16:38,680
is queen.

207
00:16:38,680 --> 00:16:39,080
OK.

208
00:16:39,130 --> 00:16:41,950
I hope you thought this was super cool basically.

209
00:16:41,950 --> 00:16:48,100
Now we understand that there's a wealth of information in these word vectors and you can add or subtract

210
00:16:48,100 --> 00:16:53,500
word vectors and you can even expand this to documents so you could later on add and subtract documents

211
00:16:53,650 --> 00:16:58,130
to find more similar documents after your experimentations.

212
00:16:58,210 --> 00:17:03,280
So if you want go ahead and play around with this go ahead and choose a couple of new vocab words create

213
00:17:03,280 --> 00:17:06,790
your new vector and then compute the most similar vectors.

214
00:17:06,850 --> 00:17:07,090
All right.

215
00:17:07,090 --> 00:17:10,160
Coming up next we're going to discuss sentiment analysis.

216
00:17:10,170 --> 00:17:10,740
I'll see you there.

