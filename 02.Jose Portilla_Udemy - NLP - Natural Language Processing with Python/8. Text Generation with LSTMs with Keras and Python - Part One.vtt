WEBVTT
ï»¿1
00:00:05.470 --> 00:00:10.860
Welcome back everyone to part one of tuks generation of python and keris in part 1.

2
00:00:10.870 --> 00:00:17.750
We're going to process the tax cleaned the text and took the tax increase sequences with Keres pre-processing.

3
00:00:17.860 --> 00:00:18.670
Let's get started.

4
00:00:19.580 --> 00:00:26.690
OK let's begin by creating a simple function that can read in files we'll call it read files and when

5
00:00:26.690 --> 00:00:32.900
you give a file path it's going to do is simply return as a single string everything in that particular

6
00:00:32.900 --> 00:00:33.500
document.

7
00:00:33.650 --> 00:00:38.510
So we're simply using with open file path as.

8
00:00:38.870 --> 00:00:44.670
Go ahead and grab all the text string text read it in his own giant string.

9
00:00:44.780 --> 00:00:45.950
And after you've done opening it.

10
00:00:45.980 --> 00:00:49.480
Go ahead and return String text.

11
00:00:49.610 --> 00:00:51.680
So we have this convenience function here.

12
00:00:51.680 --> 00:00:58.260
So if we call read a file and we're going to be using the Moby Dick story.

13
00:00:58.550 --> 00:01:02.520
So we have the first four chapters of Moby Dick is the file We're going to be reading in.

14
00:01:02.600 --> 00:01:07.100
And if you just run this without saying anything you should get one giant string here of the first four

15
00:01:07.100 --> 00:01:08.580
chapters of Moby Dick.

16
00:01:08.660 --> 00:01:13.580
And keep in mind is not that there's a lot of new lines because of the way this is formatted inside

17
00:01:13.580 --> 00:01:14.730
this text file.

18
00:01:15.200 --> 00:01:18.000
Ok so I can't comment that out will be using it later.

19
00:01:18.290 --> 00:01:23.240
And the next step is to tokenize and clean the text and we can use spaces to do this.

20
00:01:23.240 --> 00:01:31.070
There's lots of different ways but we'll see import Spacey and then it will say P is equal to spacey

21
00:01:31.190 --> 00:01:34.670
the load will go ahead and just load English.

22
00:01:35.060 --> 00:01:38.060
And I really just want to use speccy for tokenization.

23
00:01:38.060 --> 00:01:45.080
I don't need to do parsing tagging or named entity recognition so I can add in this able and I can add

24
00:01:45.080 --> 00:01:51.970
a list of things I want disabled so I can disable the parser disable the tagger and disable names and

25
00:01:52.050 --> 00:01:55.970
the recognition that's going to help our tokenization process go a little faster.

26
00:01:56.830 --> 00:01:57.550
Okay.

27
00:01:57.740 --> 00:02:06.110
And then the last thing I want to make sure I'm going to set an LP max length equal to a larger number

28
00:02:06.110 --> 00:02:07.480
than 1 million.

29
00:02:07.490 --> 00:02:11.100
So there's quite a bit of words inside these first four chapters.

30
00:02:11.270 --> 00:02:15.290
And if you want to do it across the entire Melleville text.

31
00:02:15.380 --> 00:02:21.320
So if you do read file here and start typing out Melleville we do actually have the entirety of Moby

32
00:02:21.380 --> 00:02:23.480
Dick text in case you want to work with that.

33
00:02:23.660 --> 00:02:28.250
So in case you want to work with a really large piece of text sometimes a.p complains that it doesn't

34
00:02:28.250 --> 00:02:34.070
have enough of the maximum length so you can provide a larger number so that number that should work

35
00:02:34.070 --> 00:02:36.030
for the entirety of the text file.

36
00:02:36.110 --> 00:02:39.470
We have this in the notebook so I'm going to try and copy and paste it here.

37
00:02:39.530 --> 00:02:46.010
That should give you as many words as there are inside this Moby Dick text file.

38
00:02:46.010 --> 00:02:52.930
So again in case you want to run it on the entirety of the text you may need to set this max limit higher.

39
00:02:52.950 --> 00:03:00.120
So once we've done that we're going to create another function called Separate underscore punctuation

40
00:03:00.960 --> 00:03:08.340
and it takes in some document text as a string and then it's going to grab the text tokens.

41
00:03:08.340 --> 00:03:13.080
If they're not some particular type of punctuation or if they're not you line.

42
00:03:13.230 --> 00:03:19.620
So we're simply going to do this with list comprehension we'll say token text and we're going to lowercase

43
00:03:19.620 --> 00:03:26.260
it for a token in an p doc underscore text.

44
00:03:26.740 --> 00:03:35.840
Ok so we have tooken that text that lower for token an LP text and we're going to say if that text is

45
00:03:35.840 --> 00:03:39.870
not in and then we're going to have a string here.

46
00:03:39.870 --> 00:03:43.460
So let me zoom out just quickly to explain the logic what's going on.

47
00:03:43.460 --> 00:03:49.280
Member and LNP I can get the actual tokens using an LP on that particular document.

48
00:03:49.290 --> 00:03:55.510
So the idea is only to read in this entire thing as a string passing in to an LP and then iterate through

49
00:03:55.530 --> 00:03:58.240
tokens grabbing their text and then lowercase it.

50
00:03:58.480 --> 00:04:03.240
However I want to get rid of things that are probably not going to be very helpful for training purposes

51
00:04:03.540 --> 00:04:09.620
like periods or new lines because they show up so often in the actual text especially new lines that

52
00:04:09.630 --> 00:04:14.670
I want to make sure my text generation neural network doesn't overfit that sort of punctuation.

53
00:04:14.670 --> 00:04:18.120
Otherwise you may just get a bunch of periods or a bunch of new lines at the end.

54
00:04:18.210 --> 00:04:21.810
Since those are common enough that the neural network overfit to them.

55
00:04:22.020 --> 00:04:27.430
So what I can do is say go for every token and then check that the token text is not in.

56
00:04:27.630 --> 00:04:31.710
And then I can decide and just put a string of things that I want to make sure are not in there so I

57
00:04:31.710 --> 00:04:37.760
can do just random punctuation I can add in new lines and anything I pass on to the string.

58
00:04:37.770 --> 00:04:40.570
It's not going to be added to this list of tokens.

59
00:04:40.740 --> 00:04:47.660
So I'm going to copy and paste a string that we've provided and we'll show you later on.

60
00:04:47.660 --> 00:04:53.180
It's actually provided by keris as a convenience method for a bunch of punctuation but I've also added

61
00:04:53.180 --> 00:04:57.140
things like double newlines and triple newlines and spaces to this.

62
00:04:57.170 --> 00:05:01.700
So I copied and pasted this from the note book that goes along with this text generation of neural networks

63
00:05:02.000 --> 00:05:07.520
and all we're doing here is we're saying if the character or word basically if the token happens to

64
00:05:07.520 --> 00:05:12.740
be any of these things like an at symbol square braces a new line punctuation and like periods or double

65
00:05:12.740 --> 00:05:14.380
dashes go ahead and get rid of it.

66
00:05:14.390 --> 00:05:15.260
We're not going to need it.

67
00:05:15.410 --> 00:05:17.930
We don't want to over train or train on that information.

68
00:05:17.960 --> 00:05:20.420
We're really interested in the relationship between words.

69
00:05:20.570 --> 00:05:24.580
So we're going to go ahead and create that separate punctuation function.

70
00:05:26.000 --> 00:05:31.220
And then I'm going to read in and this case all read in.

71
00:05:31.530 --> 00:05:34.120
Make sure I have this as a string.

72
00:05:34.240 --> 00:05:38.860
I'll read it in Moby Dick the first four chapters entry dealing with the first four chapters you shouldn't

73
00:05:38.860 --> 00:05:43.630
have an issue with the max length the max length should only be an issue of the entirety of the text.

74
00:05:43.630 --> 00:05:47.380
Keep in mind if you want to train on the entirety of text that's going to take a while and even four

75
00:05:47.380 --> 00:05:49.250
chapters going to take some serious amount of time.

76
00:05:50.300 --> 00:05:52.700
We'll say tokens is equal to.

77
00:05:53.240 --> 00:05:59.540
And then we're going to separate punctuation and you run this and then you're going to have the tokens

78
00:05:59.540 --> 00:06:00.400
themselves.

79
00:06:00.710 --> 00:06:05.990
So if you check that out now we can see the tokens come Ishmail some years ago etc. etc. so I have this

80
00:06:05.990 --> 00:06:11.300
giant list of all the tokens and we checked the length of this.

81
00:06:11.440 --> 00:06:16.120
It's eleven thousand three hundred ninety four tokens for the first four chapters.

82
00:06:16.120 --> 00:06:20.980
Next we're going to do is create sequence of tokens so the general idea that we're going to run through

83
00:06:20.980 --> 00:06:28.600
here is that we're going to pass in the first 24 words of a sentence and have our network predicts the

84
00:06:28.600 --> 00:06:30.020
25th word.

85
00:06:30.070 --> 00:06:32.270
So I'm going to say.

86
00:06:32.530 --> 00:06:40.480
So what we're going to do here is we're going to pass in 25 words and then have our network the neural

87
00:06:40.480 --> 00:06:43.880
network that is predicts the very next word after.

88
00:06:43.900 --> 00:06:51.380
So essentially we're number 26 so he pasand 25 words in a sentence and then the target word is the next

89
00:06:51.380 --> 00:06:52.770
word after those 25.

90
00:06:53.060 --> 00:06:57.680
And depending on what kind of documents and what kind of text generation you want you may want to make

91
00:06:57.680 --> 00:07:00.320
that list of words shorter or longer.

92
00:07:00.320 --> 00:07:06.770
The idea here is that 25 words is long enough to grab the structure of a sentence but not short enough

93
00:07:06.860 --> 00:07:11.240
where you're missing general context so hopefully that helps her generation.

94
00:07:11.240 --> 00:07:15.740
Again there is no right or wrong answer but you can imagine that if you're dealing with for instance

95
00:07:15.830 --> 00:07:20.710
a haiku dataset you probably don't need 25 words to predict number 26.

96
00:07:20.720 --> 00:07:26.060
In fact haikus probably don't have 25 words per line so you probably go for shorter maybe half three

97
00:07:26.060 --> 00:07:28.690
words and predicts word number four.

98
00:07:28.700 --> 00:07:33.050
Same thing if to generate song lyrics or something that you probably want to go shorter if you're trying

99
00:07:33.050 --> 00:07:37.850
to predict something like Shakespeare you may have to go up to 50 words and for the word number 51 because

100
00:07:37.850 --> 00:07:43.010
you have things like characters being introduced and who's speaking things like action scenes etc..

101
00:07:43.100 --> 00:07:47.150
So again no right or wrong answer here just something you have to play around with depending on what

102
00:07:47.150 --> 00:07:48.850
documents you're looking at.

103
00:07:48.860 --> 00:07:57.720
So we're going to do say training length and zoom back in here is equal to 25 plus one.

104
00:07:57.950 --> 00:08:07.900
And then I'm going to make an empty list called Text sequences and I will say for I in range of the

105
00:08:07.900 --> 00:08:17.910
training length up to the length of the tokens go ahead and create a sequence which is going to be tokens.

106
00:08:18.100 --> 00:08:25.920
I minus train length up to I and then we're going to add that.

107
00:08:26.080 --> 00:08:30.810
Ticker text sequences and we will pin that particular sequence.

108
00:08:30.820 --> 00:08:35.880
So this is a little confusing at first but once you see the actual result it makes a lot more sense.

109
00:08:35.890 --> 00:08:37.260
So go ahead and run that.

110
00:08:37.720 --> 00:08:42.520
And then if you check out your text sequences it's just a giant list.

111
00:08:42.540 --> 00:08:44.320
So it was.

112
00:08:44.420 --> 00:08:45.850
So it's a giant list.

113
00:08:45.850 --> 00:08:51.670
Let's go ahead and grab the first item in that list and it says Call me Ishmael some years ago and it

114
00:08:51.670 --> 00:08:54.260
goes all the way to interest me on.

115
00:08:54.440 --> 00:09:00.950
And now the next sequence is essentially one word over or one token over.

116
00:09:01.000 --> 00:09:04.170
It starts with me Ishmael and then it keeps going on.

117
00:09:04.360 --> 00:09:09.710
So we've essentially created these sequences for the entirety of the document.

118
00:09:10.000 --> 00:09:20.530
And if you actually want to view these we can do is say space dot join tech sequences for any sequence

119
00:09:20.530 --> 00:09:24.220
such as 0 and you'll see what this looks like says Call me Ishmael.

120
00:09:24.280 --> 00:09:29.090
And then if we copy and paste this and go for the next sequence run that.

121
00:09:29.110 --> 00:09:31.330
You notice it's just one token over.

122
00:09:31.540 --> 00:09:34.800
So go again and again it's one token over.

123
00:09:34.870 --> 00:09:39.780
So we're doing this across the entire document attack just creating these text sequences that happens

124
00:09:39.780 --> 00:09:45.910
to be in our case the training length because we're dealing with 25 words here that we want to train

125
00:09:45.910 --> 00:09:49.200
on and then our ideas to predict this word on.

126
00:09:49.210 --> 00:09:53.230
So essentially the networks to say OK given these 25 words can I predict.

127
00:09:53.230 --> 00:09:53.910
Sure.

128
00:09:54.130 --> 00:09:57.600
Given these 25 words can I predict I.

129
00:09:57.760 --> 00:09:58.390
OK.

130
00:09:58.390 --> 00:10:02.510
So we've created our text sequences again just using a simple for loop.

131
00:10:02.530 --> 00:10:08.560
So we're using slicing here to actually grab these sequences and we're just upping the original starting

132
00:10:08.560 --> 00:10:14.000
position minus the length all the way to I want to have that ready to go.

133
00:10:14.150 --> 00:10:21.320
Well we need to do is use Cara's tokenization to format this into a numerical system that Chris can

134
00:10:21.320 --> 00:10:22.100
understand.

135
00:10:22.340 --> 00:10:24.800
And luckily this is actually pretty straightforward.

136
00:10:24.980 --> 00:10:36.760
We'll see from Cara's dot pre-processing text import tokenizer and then we create the tokenizer object.

137
00:10:36.780 --> 00:10:40.810
We'll say tokenizer is equal to an instance of tokenizer.

138
00:10:41.010 --> 00:10:45.840
And there are different arguments you can pass in the tokenizer and it is tokenizer itself already has

139
00:10:45.870 --> 00:10:50.470
some filters for things like punctuation and Stoppard's et cetera.

140
00:10:50.730 --> 00:10:52.400
Well it really just punctuation.

141
00:10:52.530 --> 00:10:57.720
And what I did was I copied and pasted this into our original tokenizer up here.

142
00:10:57.780 --> 00:10:59.630
So that's where I got the string from.

143
00:10:59.730 --> 00:11:04.500
But I also added my own stuff that I knew was particular to this document like double newlines spaces

144
00:11:04.560 --> 00:11:09.340
and triple new lines as well as these double dashes that appear throughout the text.

145
00:11:09.390 --> 00:11:11.260
So that's where I got it from.

146
00:11:11.310 --> 00:11:17.430
We have the tokenizer ready to go and then we're going to take our tokenizer and I'm going to call fit

147
00:11:17.520 --> 00:11:19.110
on Tex for this.

148
00:11:19.110 --> 00:11:20.250
So it.

149
00:11:20.400 --> 00:11:27.430
Underscore on underscore tecs and then we just provide our text sequences.

150
00:11:27.480 --> 00:11:36.290
So you run that and then we'll grab our sequences and it will be tokenizer dot and then call text 2

151
00:11:36.840 --> 00:11:38.900
and it can actually go directly to Matrix.

152
00:11:38.910 --> 00:11:46.620
But I want to show you what it looks like when you go the sequences and then pasand text sequences.

153
00:11:46.620 --> 00:11:50.040
So if I take a look at what these sequences actually look like.

154
00:11:51.100 --> 00:11:54.430
You notice that it just looks like like a series of numbers.

155
00:11:54.430 --> 00:11:59.710
And notice the starting ones here 9 6 4 14 and 2 6 5.

156
00:11:59.710 --> 00:12:03.430
Let's look at the next entry and sequences notice.

157
00:12:03.430 --> 00:12:06.300
Now it's 14 to 6 5.

158
00:12:06.310 --> 00:12:12.070
So what we've done here is we've essentially replaced our original text sequences which are just sequences

159
00:12:12.070 --> 00:12:19.300
of text 26 long two sequences that happen to have numbers in place of those words.

160
00:12:19.300 --> 00:12:24.400
And now the idea is that each of these numbers is an ID for a particular word.

161
00:12:24.640 --> 00:12:32.330
And if you want to figure out the relationship you simply call tokenizer dot index word and you can

162
00:12:32.330 --> 00:12:37.010
see a picture here that connects every single value to a particular word.

163
00:12:37.010 --> 00:12:41.170
Because here one of the most common ones one is the two is and et cetera.

164
00:12:41.540 --> 00:12:49.500
And there's your actual tokenization so you can easily go back and forth between these numeric sequences

165
00:12:49.560 --> 00:12:58.520
and these text sequences using this tokenizer index where it can simply say something like four and

166
00:12:59.000 --> 00:13:11.800
sequences 0 go ahead and print out just using string formatting I can print out the actual ID a space

167
00:13:12.220 --> 00:13:18.850
and this essentially acts like a dictionary so I can say tokenizer dot index word at that particular

168
00:13:18.880 --> 00:13:19.440
eye.

169
00:13:19.720 --> 00:13:23.520
So if you actually print that out let's go and take a look at it.

170
00:13:23.560 --> 00:13:29.170
It just prints them both out 9:06 for is call 14 as me just smile.

171
00:13:29.210 --> 00:13:30.020
And so on.

172
00:13:30.290 --> 00:13:33.580
So if we see a word that repeats I'm not sure if there is one here.

173
00:13:33.800 --> 00:13:34.570
We can scroll down.

174
00:13:34.610 --> 00:13:35.890
So we have on.

175
00:13:36.050 --> 00:13:40.760
We actually don't have anything repeating here but something I want to stress is that 14 is a unique

176
00:13:40.760 --> 00:13:42.290
ID to the word me.

177
00:13:42.290 --> 00:13:46.940
So if you ever see me again in another sentence again I'm going to get the index 14.

178
00:13:46.940 --> 00:13:48.180
This is not exactly a count.

179
00:13:48.230 --> 00:13:51.350
It's just a unique ID for this particular word.

180
00:13:51.350 --> 00:13:51.780
OK.

181
00:13:52.040 --> 00:13:59.680
So we've conducted the tokenization and we can check now if we want from tokenizer.

182
00:13:59.680 --> 00:14:00.910
It has actually a lot of information.

183
00:14:00.910 --> 00:14:04.900
It also has things like word counts which essentially counts how many times these words show up.

184
00:14:04.930 --> 00:14:10.430
So the word me shows up 2004 and 71 times Ishmail hundred thirty three times some seventieth.

185
00:14:10.470 --> 00:14:12.050
So 58 et cetera.

186
00:14:12.130 --> 00:14:13.260
Or a dictionary here.

187
00:14:13.270 --> 00:14:17.080
So in case you want to do things like word count essentially factorization there's some information

188
00:14:17.080 --> 00:14:18.370
here on the tokenizer.

189
00:14:18.370 --> 00:14:26.320
They can easily access so let's go ahead and get a size of our vocabulary by simply saying vocabulary.

190
00:14:27.250 --> 00:14:35.320
Size is equal to length of tokenizer underscore word counts.

191
00:14:35.360 --> 00:14:37.420
So if I checked my vocabulary size.

192
00:14:37.490 --> 00:14:43.140
There's 2700 and nine unique words across those first four chapters.

193
00:14:43.140 --> 00:14:43.590
OK.

194
00:14:43.920 --> 00:14:50.080
So the last thing you need to do is convert these sequences remember sequences right now.

195
00:14:50.160 --> 00:14:55.740
We have our text sequences which is the actual text in our sequences which is the ID numbers for each

196
00:14:55.740 --> 00:14:56.910
word.

197
00:14:56.910 --> 00:15:04.680
Right now if I check the type of this it's a list where every item in the list is another list of these

198
00:15:04.680 --> 00:15:05.860
actual numbers.

199
00:15:05.900 --> 00:15:09.450
I'd like to do is format that to be an umpire matrix.

200
00:15:09.450 --> 00:15:17.270
And what I can do is I could simply cast it in poor umpires and the and then just call sequences and

201
00:15:17.270 --> 00:15:26.120
make that equal to the array version of the sequences and then if I look at sequences it's nicely formatted.

202
00:15:26.120 --> 00:15:31.760
So essentially each of these rows represents a single line in the text.

203
00:15:31.770 --> 00:15:34.880
Again notice how we're essentially shifting one word over.

204
00:15:34.880 --> 00:15:40.410
So I go 9 6 4 14 2 6 5 and then 14 becomes the first one of this line.

205
00:15:40.490 --> 00:15:43.870
5:46 5:51 and so on et cetera et cetera.

206
00:15:43.880 --> 00:15:49.320
For all the text so there are as many sequences as there are words in the actual document.

207
00:15:49.350 --> 00:15:53.730
We're just going one by one essentially shifting over one word at a time.

208
00:15:53.960 --> 00:15:56.480
And then we have the actual target that we're trying to predict.

209
00:15:56.480 --> 00:16:03.040
So given these ID numbers for each word what is the expected word to come after those first 25 words.

210
00:16:03.050 --> 00:16:07.270
So we already have our features here as well as our label.

211
00:16:07.340 --> 00:16:11.450
And later on we'll be performing a train test split with that functionality which is why we kind of

212
00:16:11.450 --> 00:16:14.480
needed it in this NUMP hiree.

213
00:16:14.540 --> 00:16:15.020
OK.

214
00:16:15.230 --> 00:16:20.880
So that's it for part 1 part 2 will create the elist based model and then we'll perform the train test

215
00:16:21.290 --> 00:16:22.350
and train the model.

216
00:16:22.490 --> 00:16:23.000
We'll see it there.

