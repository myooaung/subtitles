1
00:00:05,430 --> 00:00:09,040
Welcome back everyone to this lecture on an overview of machine learning.

2
00:00:10,630 --> 00:00:15,570
Before we dive into text classification let's work on understanding the general machine learning process

3
00:00:15,570 --> 00:00:16,770
we're going to be using.

4
00:00:17,050 --> 00:00:20,700
The specific case of machine learning we're going to be conducting in this particular section of the

5
00:00:20,710 --> 00:00:23,520
course is known as supervised learning.

6
00:00:23,530 --> 00:00:26,570
So we're going to be focusing on that supervised learning process.

7
00:00:27,290 --> 00:00:30,050
Because different students have different backgrounds and math.

8
00:00:30,050 --> 00:00:34,570
We're going to be keeping the mathematics behind the machine learning algorithms on the lighter side.

9
00:00:36,070 --> 00:00:40,740
A great textbook on General machine learning especially if you are interested in the mathematics is

10
00:00:40,750 --> 00:00:44,800
called an introduction to statistical learning by Gareth James's companion book.

11
00:00:45,040 --> 00:00:50,070
It's freely available online as a PDA and it's released for free by the authors.

12
00:00:50,200 --> 00:00:53,590
You can simply Google search the title of the book to find the link.

13
00:00:53,590 --> 00:00:58,360
So again just google search I guess L.R. or introduction to statistical learning and you'll be able

14
00:00:58,360 --> 00:01:00,390
to easily find this book for free online.

15
00:01:02,390 --> 00:01:07,580
So let's begin by answering the question what is machine learning machine learning as a method of data

16
00:01:07,580 --> 00:01:14,270
analysis that automates analytical model building using algorithms that it's really learned from data

17
00:01:14,270 --> 00:01:15,330
itself.

18
00:01:15,350 --> 00:01:20,570
Machine learning allows computers to find hidden insights without being explicitly programmed where

19
00:01:20,570 --> 00:01:21,420
to look.

20
00:01:21,440 --> 00:01:26,000
So the machine learning algorithm on its own is going to figure out what patterns and what features

21
00:01:26,000 --> 00:01:26,990
are important.

22
00:01:28,740 --> 00:01:30,840
Now what does machine learning actually used for.

23
00:01:30,840 --> 00:01:35,070
As you may already know it's used across a variety of tasks and industries.

24
00:01:35,220 --> 00:01:41,310
Anything from fraud detection to credit scoring loan applications or the things we're going to be focusing

25
00:01:41,310 --> 00:01:45,770
on things like tech sentiment analysis or e-mail spam filtering.

26
00:01:45,780 --> 00:01:50,510
So this is a small subset here of what machine learning applications can do for us.

27
00:01:52,080 --> 00:01:58,960
Most talk about supervised learning supervised learning algorithms are trained using labeled examples.

28
00:01:58,990 --> 00:01:59,720
That's a key word.

29
00:01:59,720 --> 00:02:07,610
They're labeled so a labeled example is an example where the input and desired output is known.

30
00:02:07,620 --> 00:02:14,610
For example if you have a data point as a segment a text it could have a categorical label such as the

31
00:02:14,610 --> 00:02:21,390
text of an email being labelled as spam versus legitimate or the text of a movie review being labeled

32
00:02:21,420 --> 00:02:25,040
as a positive movie review or a negative movie review.

33
00:02:26,880 --> 00:02:32,160
The learning algorithm or machine learning algorithm is going to receive a set of inputs along with

34
00:02:32,160 --> 00:02:37,650
the corresponding correct outputs and the algorithm is going to learn by comparing its actual output

35
00:02:37,920 --> 00:02:40,470
with the correct outputs in order to find an error.

36
00:02:40,590 --> 00:02:45,300
And then it's going to modify the internal parameters of the machine learning algorithm accordingly

37
00:02:45,600 --> 00:02:51,100
to achieve the best results on its own without you explicitly programming it what it should change.

38
00:02:52,760 --> 00:02:58,280
Supervised learning is commonly used applications where historical data points predict likely future

39
00:02:58,280 --> 00:03:04,550
events such as historical information of previous emails you've received that someone has labeled as

40
00:03:04,820 --> 00:03:07,830
legitimate or illegitimate or spam.

41
00:03:07,910 --> 00:03:13,650
The historical data can be used in order to provide a future spam filter for future emails.

42
00:03:15,360 --> 00:03:20,160
So let's talk about the supervised machine learning process and this is probably the most fundamental

43
00:03:20,160 --> 00:03:22,750
thing to get out of this particular lecture.

44
00:03:23,040 --> 00:03:25,890
So we're going to go through all these steps individually.

45
00:03:25,890 --> 00:03:28,800
Now the first step is to actually quiet your data.

46
00:03:28,800 --> 00:03:30,570
And there's many different ways to do this.

47
00:03:30,630 --> 00:03:33,450
In this particular course the acquisition is pretty simple.

48
00:03:33,450 --> 00:03:38,280
We actually just provide the data for you but depending on where you're actually working or what you're

49
00:03:38,280 --> 00:03:43,480
doing for your personal machine learning projects you can acquire that data through a variety of sources.

50
00:03:43,500 --> 00:03:48,240
You can check out the resource links in this lecture for lots of free data sources or if you're working

51
00:03:48,240 --> 00:03:53,160
on a company you can work internally to acquire data but your customers or even from like things like

52
00:03:53,160 --> 00:03:54,870
physical sensors et cetera.

53
00:03:54,930 --> 00:03:59,790
So we're going to assume then that you've acquired that data then the next step is to clean and format

54
00:03:59,790 --> 00:04:00,480
your data.

55
00:04:00,570 --> 00:04:05,490
And in particular if you're working with text data for natural language processing you'll have to perform

56
00:04:05,490 --> 00:04:11,000
things like vectorization on that data which are going to cover in the lecture text feature extraction.

57
00:04:11,310 --> 00:04:16,590
But you get to do things like remove missing data points from your data and then convert the raw text

58
00:04:16,770 --> 00:04:20,340
into numerical vectors that machine learning models can understand.

59
00:04:20,580 --> 00:04:24,410
So that's a whole process that we're really going to focus on on in another lecture on the future.

60
00:04:24,580 --> 00:04:27,920
But you've acquired that data and you've cleaned in a format of your data.

61
00:04:29,800 --> 00:04:34,990
Then once you have the clean and formatted data you're going to split the data into training data or

62
00:04:34,990 --> 00:04:40,410
each training set and then test data or a test set and know what you're going to do is on the training

63
00:04:40,410 --> 00:04:47,320
data you're going to grab your machine learning model and have it learn or train off that training data.

64
00:04:47,370 --> 00:04:50,730
And that's also known as fitting the model to the training data.

65
00:04:50,790 --> 00:04:55,070
So the machinery model is going to attempt to learn as best it can.

66
00:04:55,440 --> 00:04:59,820
The different application that you're looking for so if you're trying to build an email spam filter

67
00:04:59,940 --> 00:05:05,820
right now it's working on that training set of data and the training data is commonly about 70 percent

68
00:05:05,880 --> 00:05:07,380
of the total data.

69
00:05:07,380 --> 00:05:10,950
So your training your model and other model has learned on your training data.

70
00:05:10,980 --> 00:05:15,060
Now it's time to evaluate your model's performance and for evaluation.

71
00:05:15,060 --> 00:05:19,370
It makes sense to use data that the model has never seen before that you're not cheating.

72
00:05:19,530 --> 00:05:21,720
That's why we have the separate test data set.

73
00:05:21,840 --> 00:05:28,500
Around 30 percent of our data so we take the model we just trade on and then we test it against that

74
00:05:28,500 --> 00:05:33,870
test dataset and then we can evaluate how well the model actually performed using different evaluation

75
00:05:33,870 --> 00:05:34,490
metrics.

76
00:05:34,530 --> 00:05:38,350
And we're going to talk about evaluation metrics in a lecture in that section.

77
00:05:38,370 --> 00:05:44,400
So after evaluating our model we can decide to improve the model maybe edits some parameters of the

78
00:05:44,400 --> 00:05:50,040
original machine learning algorithm so we can go back to the model and training and building phase adjust

79
00:05:50,040 --> 00:05:55,800
some parameters and essentially repeat this process until we're satisfied for a modest performance and

80
00:05:55,800 --> 00:05:56,910
then we can deploy the model

81
00:05:59,600 --> 00:06:05,150
so for tech's classification and recognition this is really common and widely applicable use of machine

82
00:06:05,150 --> 00:06:05,740
learning.

83
00:06:06,110 --> 00:06:10,340
And later on in this section of the course we're going to be learning about the sikat learn library

84
00:06:10,460 --> 00:06:16,850
in order to use Python to conduct machine learning text classification.

85
00:06:16,850 --> 00:06:22,100
Now let's take a quick moment to focus on that idea of the train to split that I just mentioned and

86
00:06:22,100 --> 00:06:25,070
that we just saw her and we're going to learn a few key terms.

87
00:06:25,070 --> 00:06:31,730
So let's imagine a full data set of what is known as ham versus spam text messages and ham is just another

88
00:06:31,730 --> 00:06:35,080
word for legit or correct text messages.

89
00:06:35,240 --> 00:06:41,710
Basically the opposite of spam so we're going to imagine a full dataset and our dataset is going to

90
00:06:41,710 --> 00:06:42,790
look something like this.

91
00:06:42,790 --> 00:06:46,390
Notice that we have a label and the actual raw text message.

92
00:06:46,390 --> 00:06:51,160
So you can see the one in the middle there that's a spam text message offering some sort of bogus free

93
00:06:51,160 --> 00:06:52,950
entry and some weekly competition.

94
00:06:53,170 --> 00:06:56,980
The other text messages are him or legitimate text messages.

95
00:06:56,980 --> 00:07:02,560
So our idea here is that we're going to build a machine learning model that can detect given a new text

96
00:07:02,560 --> 00:07:07,200
message whether or not something is ham or spam.

97
00:07:07,310 --> 00:07:10,400
Now before the split we're going to have labels and features.

98
00:07:10,400 --> 00:07:15,540
And in this case the label is again the historical label ham or spam.

99
00:07:15,590 --> 00:07:22,310
So that means that someone in or someone manually had to go in and read the message and then label it

100
00:07:22,430 --> 00:07:25,250
as a legitimate message or spam message.

101
00:07:25,250 --> 00:07:31,140
So that's a key part of supervised learning is they have to have historical labeled data and often labeling

102
00:07:31,140 --> 00:07:33,260
your data as a manual process.

103
00:07:33,260 --> 00:07:37,510
Now in this course you're not going have to label any data we provide label data for you.

104
00:07:37,910 --> 00:07:44,330
So we have what's known as the y label and the X features features is the actual data points.

105
00:07:44,420 --> 00:07:46,970
They're going to use to predict the white label.

106
00:07:47,180 --> 00:07:52,730
And the reason we have y and x is because in linear algebra notation the way the mathematics work out

107
00:07:52,790 --> 00:07:56,630
usually have an X matrix of features and white label.

108
00:07:56,630 --> 00:07:58,770
And often you'll see why lower cased.

109
00:07:58,820 --> 00:08:00,870
So this is right before the split.

110
00:08:00,890 --> 00:08:04,760
We still have labels and features we haven't actually than a train test split yet.

111
00:08:05,000 --> 00:08:09,710
What we want to do is probably take about 70 percent of these points both the labels and the corresponding

112
00:08:09,710 --> 00:08:15,200
messages and save those for the training data and then take the other 30 percent and have that be our

113
00:08:15,200 --> 00:08:15,980
test data.

114
00:08:17,600 --> 00:08:25,240
So once we have our white labels next features before we fit the model we'll split the data so we're

115
00:08:25,230 --> 00:08:27,270
at take this data and we're going to split.

116
00:08:27,270 --> 00:08:31,260
So this is a really small example of a dataset just five points that we've already shown that we're

117
00:08:31,260 --> 00:08:32,050
splitting it.

118
00:08:32,160 --> 00:08:37,500
So you notice here a total data or splitting it into a test set and a training set.

119
00:08:37,500 --> 00:08:40,920
Remember that we still have a white label on X features.

120
00:08:40,920 --> 00:08:46,790
So we just split the data and we have now our test set in our training set.

121
00:08:48,040 --> 00:08:53,560
And now we can do is we can individually label the subsets of y s and x.

122
00:08:53,570 --> 00:08:58,880
So what you're going to see when you do the actual train test split of python and sikat learn is for

123
00:08:58,910 --> 00:08:59,710
outputs.

124
00:08:59,900 --> 00:09:05,930
You have your y labels your y tests in y train and then you have your X features your X features for

125
00:09:05,930 --> 00:09:08,890
the test set and your X features for the training set.

126
00:09:09,080 --> 00:09:15,860
So don't be confused when you run the train to split and see for outputs why test X test Y train extreme.

127
00:09:16,070 --> 00:09:19,580
All it is is the data set being split into two pieces.

128
00:09:19,610 --> 00:09:26,340
Your tests and your training set and then those two pieces being split along the y labels and X features.

129
00:09:26,390 --> 00:09:30,620
So that's all that is those four terms mean.

130
00:09:30,660 --> 00:09:34,170
So again we have those four components and you're going to see this components often throughout the

131
00:09:34,170 --> 00:09:36,820
course.

132
00:09:36,960 --> 00:09:41,220
So these four components are simply the result of the train to split groups being separated between

133
00:09:41,220 --> 00:09:42,840
features and labels.

134
00:09:42,840 --> 00:09:48,090
Coming up next we're to continue to understand the classification process in more detail and importantly

135
00:09:48,390 --> 00:09:51,860
metrics in order to evaluate a classification process.

136
00:09:51,870 --> 00:09:52,980
We'll see you at the next lecture.

