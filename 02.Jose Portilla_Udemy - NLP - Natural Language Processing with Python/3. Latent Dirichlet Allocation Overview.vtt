WEBVTT
ï»¿1
00:00:05.320 --> 00:00:07.370
Welcome back everyone in this lecture.

2
00:00:07.390 --> 00:00:13.180
We're going to give an explanatory overview of how LDA or later there are only allocation for topic

3
00:00:13.180 --> 00:00:14.230
modeling works.

4
00:00:15.810 --> 00:00:21.450
Johanne there slate was a German mathematician in the 1800s who contributed widely to the field of modern

5
00:00:21.450 --> 00:00:22.680
mathematics.

6
00:00:22.680 --> 00:00:27.510
And there is a probability distribution named after him called that there is Slate distribution and

7
00:00:27.510 --> 00:00:34.990
this is the distribution that is used later on in LDK so late to Thursley allocation is based off this

8
00:00:34.990 --> 00:00:36.730
probability distribution.

9
00:00:36.940 --> 00:00:43.300
And in 2003 Elda was actually first published as a graphical model for topic discovery in the Journal

10
00:00:43.300 --> 00:00:44.920
of machine learning research.

11
00:00:44.920 --> 00:00:49.330
So keep in mind even though there are Schley's name is attached to this particular method for topic

12
00:00:49.330 --> 00:00:54.400
modeling it really just stems from the fact that it uses that there is slape probability distribution

13
00:00:54.760 --> 00:01:00.850
not that Thursley himself actually invented the LTA for topic modeling the actual method is relatively

14
00:01:01.000 --> 00:01:01.650
new.

15
00:01:01.690 --> 00:01:04.150
From 2003.

16
00:01:04.390 --> 00:01:08.490
So we're going to get a high level overview of how LDA works for topic modeling.

17
00:01:08.620 --> 00:01:12.900
But I would really encourage you to also take a look at the original publication paper.

18
00:01:12.920 --> 00:01:19.190
Now there are two main assumptions we're going to make in order to actually apply LDA for topic modeling.

19
00:01:19.190 --> 00:01:25.610
The first one is that documents with similar topics use similar groups of words and that's a pretty

20
00:01:25.610 --> 00:01:31.100
reasonable assumption because that basically saying that if you have various documents covering a similar

21
00:01:31.100 --> 00:01:37.070
topic like a bunch of documents covering the topic of business or economy that they should end up using

22
00:01:37.070 --> 00:01:42.510
similar words like money price market stocks etc..

23
00:01:42.950 --> 00:01:48.260
The other Sumption are going to make is that latent topic's can then be found by searching for groups

24
00:01:48.260 --> 00:01:53.930
of words that frequently occur together in documents across the corpus and that's going to the assumption

25
00:01:53.960 --> 00:01:56.690
that we're really going to dive into the details later on.

26
00:01:56.690 --> 00:02:00.830
So again these are the two assumptions and they're both actually quite reasonable for the way humans

27
00:02:00.830 --> 00:02:01.970
write documents.

28
00:02:02.270 --> 00:02:06.980
And we can actually think of these two assumptions mathematically the way we kind of model these assumptions

29
00:02:07.220 --> 00:02:08.630
is the following.

30
00:02:08.630 --> 00:02:14.780
We can say that documents are probability distributions over some underlying late and topics and then

31
00:02:14.780 --> 00:02:19.300
topics themselves are probability distributions over overworks.

32
00:02:19.310 --> 00:02:21.610
So let's see how each of those actually plays out.

33
00:02:22.510 --> 00:02:29.500
We can imagine that any particular document is going to have a probability distribution over a given

34
00:02:29.500 --> 00:02:31.250
amount of topics.

35
00:02:31.330 --> 00:02:37.990
So let's say we decide that there are five and topics across various documents than any particular document

36
00:02:38.140 --> 00:02:42.110
is going to have a probability of belonging to each topic.

37
00:02:42.160 --> 00:02:47.810
So here we can see document one has the highest probability of belonging to topic number two.

38
00:02:47.830 --> 00:02:52.630
So we have this discrete probability distribution across the topics for each document.

39
00:02:52.630 --> 00:02:56.450
Then we can look at another document such as document number 2.

40
00:02:56.530 --> 00:03:01.870
In this case it does have probabilities of belonging to other topics but we're going to say that it

41
00:03:01.870 --> 00:03:05.540
has the highest probability of belonging to Topic four.

42
00:03:05.560 --> 00:03:11.500
Notice here we're not saying definitively that document 1 belongs to any particular topic or document

43
00:03:11.500 --> 00:03:13.420
to belong to any particular topic.

44
00:03:13.420 --> 00:03:20.100
Instead we're modeling them as having a probability distribution over a variety of topics.

45
00:03:22.270 --> 00:03:27.880
And then if we look at the topics themselves those are simply going to be modeled as probability distributions

46
00:03:27.970 --> 00:03:29.290
over words.

47
00:03:29.290 --> 00:03:35.350
So for example we can define topic one as different probabilities belonging to each of these words as

48
00:03:35.350 --> 00:03:36.650
belonging to that topic.

49
00:03:36.940 --> 00:03:43.510
So we can see here that it has a low probability of the word he belong a topic one low probability of

50
00:03:43.510 --> 00:03:50.230
food belong a topic one etc. and then we can see that word such as cat and dog have a higher probability

51
00:03:50.500 --> 00:03:52.450
of belonging to topic one.

52
00:03:52.510 --> 00:03:57.940
And here is where we're actually going to begin as a user trying to understand what this topic is actually

53
00:03:57.940 --> 00:03:59.210
representative of.

54
00:03:59.230 --> 00:04:04.360
So if we were to get this sort of probability distribution across all the vocabulary of all the words

55
00:04:04.360 --> 00:04:10.390
in the corpus but we would end up doing is asking for maybe the top 10 highest probability words for

56
00:04:10.390 --> 00:04:16.000
topic 1 and then we would try to realize what the actual underlying topic was.

57
00:04:16.000 --> 00:04:21.580
So in this case we could make an educated guess that topic one happened to do with pets and we would

58
00:04:21.580 --> 00:04:24.160
say that topic one has to do with pets.

59
00:04:24.210 --> 00:04:29.710
Again the LDA or unsupervised learning technique is not going to be able to tell you that directly.

60
00:04:29.830 --> 00:04:35.140
It's up to the user to interpret these probability distributions as topics and we'll actually get hands

61
00:04:35.140 --> 00:04:38.940
on practice with that when we perform LDH ourselves of Python.

62
00:04:40.970 --> 00:04:49.590
So LTA represents documents as mixtures of topics that spit out words with certain probabilities and

63
00:04:49.590 --> 00:04:53.340
it's going to assume that documents are produced in the following fashion.

64
00:04:53.340 --> 00:05:00.540
It's first going to decide on the number of words and that that document will have then we choose a

65
00:05:00.540 --> 00:05:06.290
topic mixture that documents according to a slate distribution over a fixed set of topics.

66
00:05:06.300 --> 00:05:10.110
So that's where that slide distribution comes to effect.

67
00:05:10.110 --> 00:05:16.440
So for example we start off and say this document is 60 percent business 20 percent politics and 10

68
00:05:16.440 --> 00:05:17.340
percent foods.

69
00:05:17.340 --> 00:05:24.160
So that's her actual distribution and then what we're going to do is we're going to generate each word

70
00:05:24.160 --> 00:05:30.270
in the document by first picking a topic according to the multinomial distribution that we sampled previously.

71
00:05:30.310 --> 00:05:34.840
So we picked words 60 percent of them from the business topic 20 percent of them from politics and then

72
00:05:34.840 --> 00:05:40.910
10 percent from the food topic and then using the topic to generate the word itself.

73
00:05:40.910 --> 00:05:46.370
So again according to the topics own multinomial distribution across the words.

74
00:05:46.370 --> 00:05:51.800
So for example if we selected the food topic we might generate the word apple a 60 percent probability

75
00:05:52.110 --> 00:05:58.670
and another word home with less probability like 30 percent probability and so on.

76
00:05:58.700 --> 00:06:05.060
Assuming this sort of generative model for a collection of documents LDA is actually going to then try

77
00:06:05.060 --> 00:06:11.180
to backtrack from the documents to find the topics that are likely to have generated the collection.

78
00:06:11.180 --> 00:06:17.560
So again this process here that we just went over LTI is assuming that that's how you built the documents.

79
00:06:17.570 --> 00:06:22.160
Now obviously in the real world you're not actually building documents with this sort of frame of mind

80
00:06:22.490 --> 00:06:28.520
but it's a very useful construct of the way topics can be mixed throughout various documents and the

81
00:06:28.520 --> 00:06:31.670
way words can be mixed throughout various topics.

82
00:06:31.670 --> 00:06:36.880
So what we're going to do is attempt to backtrack that sort of process.

83
00:06:37.020 --> 00:06:41.460
So let's actually show you what else is going to do since it's assuming that that's how you built the

84
00:06:41.460 --> 00:06:42.600
documents.

85
00:06:42.600 --> 00:06:48.120
So we're can imagine we have a set of documents and the first that we have to do is actually choose

86
00:06:48.420 --> 00:06:53.460
some fixed number of topics to discover and you should note that very carefully that this is actually

87
00:06:53.460 --> 00:06:54.630
really hard.

88
00:06:54.630 --> 00:07:01.170
In order for LDA to work you as a user need to decide how many topics are going to be discovered.

89
00:07:01.170 --> 00:07:07.440
So even before you start LDA you need to have some sort of intuition over how many topics.

90
00:07:07.440 --> 00:07:13.560
So we choose some fixed number of topics to discover and then we're going to want to use LDA to learn

91
00:07:13.560 --> 00:07:18.190
the topic representation of each document and the words associated to each topic.

92
00:07:19.970 --> 00:07:24.920
Then we're going to go through each document and we're going to randomly assign each word in the document

93
00:07:25.220 --> 00:07:27.050
to one of the K topics.

94
00:07:27.050 --> 00:07:33.320
So keep in mind the very first pass this random assignment actually already gives you both topic representations

95
00:07:33.350 --> 00:07:38.960
of all the documents and word distributions of all the topics and we've assigned everything randomly

96
00:07:38.960 --> 00:07:44.180
at the very first pass so we're technically not done yet because these initial ran and topics won't

97
00:07:44.210 --> 00:07:44.980
really make sense.

98
00:07:44.990 --> 00:07:50.150
They're going to be really poor representations of topics since you essentially just assign every word

99
00:07:50.150 --> 00:07:51.130
around that topic.

100
00:07:51.140 --> 00:07:57.950
So now it's time to iterate over this and see if we can figure out how to fix these sort of assignments.

101
00:07:58.210 --> 00:08:03.190
So we're going to iterate over every word in every document to improve these topics and we're going

102
00:08:03.190 --> 00:08:08.300
to do it for every word in every document and for each topic.

103
00:08:08.650 --> 00:08:14.260
We're going to that the following we're going to calculate the proportion of words in document D that

104
00:08:14.260 --> 00:08:17.970
are currently assigned to topic T.

105
00:08:18.190 --> 00:08:23.980
Then we're also going to calculate the proportion of assignments that topic t over all documents that

106
00:08:23.980 --> 00:08:32.740
come from this particular word w and then we're going to reassign w a new topic where we choose topic

107
00:08:32.800 --> 00:08:40.980
t with probability of topic T given document the times probability of word w given topic T.

108
00:08:41.260 --> 00:08:48.460
So this is essentially the probability that topic t generated the word W and after repeating that previous

109
00:08:48.460 --> 00:08:54.730
step a large number of times we eventually reach a roughly steady state where the assignments for the

110
00:08:54.730 --> 00:09:02.140
topics are acceptable these words and topics don't start changing that often they become pretty steady.

111
00:09:02.140 --> 00:09:08.350
So at the end what we have is each document being assigned to a topic and then all we can do is we can

112
00:09:08.350 --> 00:09:15.540
then search for the words that have the highest probability of being assigned a topic so we end up with

113
00:09:15.540 --> 00:09:17.630
an output such as this will say.

114
00:09:17.640 --> 00:09:23.000
After running through all the documents and performing LDA you pass in one particular document and then

115
00:09:23.000 --> 00:09:25.580
report back LDA will report back.

116
00:09:25.590 --> 00:09:30.750
I think this document is assigned to topic number four and we actually don't know yet what topic number

117
00:09:30.750 --> 00:09:37.590
4 presents but we can ask LDA is what are the most common words in topic for what words have the highest

118
00:09:37.590 --> 00:09:40.190
probability of showing up a topic for.

119
00:09:40.320 --> 00:09:48.000
And then you may get results like cat that birds dog food home etc. and then given this list of high

120
00:09:48.000 --> 00:09:53.390
probability words for this particular topic it's up to the user to interpret what topic is.

121
00:09:53.550 --> 00:09:58.740
And for this I think a reasonable interpretation would be that these particular documents assigns a

122
00:09:58.740 --> 00:10:04.620
topic for probably have to do something with pets and we would say OK I think topic 4 is pets.

123
00:10:04.740 --> 00:10:06.290
Now is that the right answer.

124
00:10:06.300 --> 00:10:11.490
There's really no way of knowing because we didn't have the right answer to begin with but it's a reasonable

125
00:10:11.490 --> 00:10:18.820
assumption to make given the high probability of words showing up for topic for so again two important

126
00:10:18.820 --> 00:10:21.710
notes here before we continue and actually apply this with Python.

127
00:10:21.940 --> 00:10:26.920
The first important note is that the user themselves must decide on the amount of topics present in

128
00:10:26.920 --> 00:10:33.430
the document before even beginning this process and the second one is that the user themselves must

129
00:10:33.430 --> 00:10:37.350
interpret what the topics are are and what they're actually representing.

130
00:10:37.670 --> 00:10:43.390
OK in the next lecture we're going to explore how to actually implement LDA with Python and secular.

131
00:10:43.390 --> 00:10:44.100
I'll see it there.

