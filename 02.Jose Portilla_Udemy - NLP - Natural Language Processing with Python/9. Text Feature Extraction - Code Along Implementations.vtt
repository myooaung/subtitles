WEBVTT
ï»¿1
00:00:05.300 --> 00:00:09.880
Welcome back everyone to feature extraction from text in this lecture.

2
00:00:09.890 --> 00:00:14.660
We're going to be learning about a basic manual implementation of building out a vocabulary just to

3
00:00:14.660 --> 00:00:19.690
give you a little bit of more intuition behind building out a vocabulary across multiple documents.

4
00:00:19.850 --> 00:00:25.130
Then we'll learn how to use the sikat learn library for Victoria Station as well as how to build pipelines

5
00:00:25.130 --> 00:00:27.640
for natural language processing with sikat Hitler.

6
00:00:27.740 --> 00:00:29.770
Let's open up the notebook and get started.

7
00:00:29.780 --> 00:00:30.350
All right.

8
00:00:30.400 --> 00:00:35.600
If you check out the text classification folder there is a notebook labeled feature extraction from

9
00:00:35.600 --> 00:00:41.050
text and if you open up that feature extraction from text notebook it's divided into two sections.

10
00:00:41.210 --> 00:00:46.400
The first section is basically showing you how to build out a natural language processor from scratch

11
00:00:46.640 --> 00:00:53.510
particularly how to build out a vocabulary giving each word a unique ID will briefly walk through that.

12
00:00:53.540 --> 00:00:57.810
We're not actually going to live code anything there and then we'll actually show you how I perform

13
00:00:57.800 --> 00:01:00.940
these steps using the real psychic learned tools.

14
00:01:00.960 --> 00:01:05.910
So just to give you a little bit more intuition of what's happening at the very base level behind every

15
00:01:05.910 --> 00:01:10.650
natural language processor is the fact they need to build out a vocabulary.

16
00:01:10.810 --> 00:01:15.400
So we're going to start off with some documents and keep my you don't really need to memorize this code.

17
00:01:15.400 --> 00:01:17.540
You never actually implement this manually in real life.

18
00:01:17.630 --> 00:01:23.530
Again just to build out an intuition here we're going to write two text files and keep mine.

19
00:01:23.530 --> 00:01:28.390
These two text files if you're not using Jupiter they're available right here as to x files inside the

20
00:01:28.390 --> 00:01:30.060
text classification folder.

21
00:01:30.770 --> 00:01:32.700
And then we can build out a vocabulary.

22
00:01:32.900 --> 00:01:37.970
So this is essentially the first step in natural language processing regardless of what method you're

23
00:01:37.970 --> 00:01:39.020
going to be using.

24
00:01:39.020 --> 00:01:45.800
You do have to assign some sort of vocabulary across all your documents and the way this works is we're

25
00:01:45.800 --> 00:01:53.270
going to build out a dictionary and then set an ID counter then we're going to open each text file and

26
00:01:53.270 --> 00:01:59.300
we're going to read in lowercase sing all the words so that we have a list of all the words then for

27
00:01:59.300 --> 00:02:05.070
every word in that list of words if that word is already in the vocabulary dictionary will just continue

28
00:02:05.630 --> 00:02:08.590
if that word is not present a cross over cavalry.

29
00:02:08.690 --> 00:02:14.380
We'll go ahead and assign it a unique ID number and then we'll add one to the IDs tracker.

30
00:02:14.420 --> 00:02:19.570
That way we go ahead and make sure that each next word gets a unique ID.

31
00:02:19.580 --> 00:02:22.480
Again we're not really counting the number of occurrences yet.

32
00:02:22.550 --> 00:02:29.440
We're just giving each word a unique ID so we can print out the vocabulary dictionary and we see this

33
00:02:29.480 --> 00:02:33.820
is one is is two is three stories four and so on.

34
00:02:33.830 --> 00:02:41.180
So each of these strings or words is getting a unique ID then we do the same thing for the second textfile

35
00:02:41.450 --> 00:02:44.300
using the same vocabulary dictionary as before.

36
00:02:44.300 --> 00:02:48.230
And notice we're again checking if that word is already present in the vocabulary.

37
00:02:48.230 --> 00:02:55.970
So for a word like story shows up in both documents we know that it has a unique ID across all the documents

38
00:02:56.030 --> 00:03:03.470
so story ID is for or in this location for and then we can store it that way and know that even though

39
00:03:03.680 --> 00:03:10.370
2.6 C has 15 words we ended up only adding 7 new words to that vocabulary dictionary.

40
00:03:10.470 --> 00:03:16.200
And so we've essentially encapsulated our entire language across all the documents in a dictionary so

41
00:03:16.200 --> 00:03:20.940
we can then begin to perform feature extraction on each of our original documents.

42
00:03:20.970 --> 00:03:27.120
So what we could do is for each of these documents we can create an empty vector space for each word

43
00:03:27.120 --> 00:03:28.380
in the vocabulary.

44
00:03:28.410 --> 00:03:33.600
So all I'm doing here is I'm saying one that ticks he has a string and then I'm adding a bunch of zeros

45
00:03:33.690 --> 00:03:38.230
for each vocabulary word that's present in this dictionary right here.

46
00:03:38.290 --> 00:03:43.360
That one is going to do is going to map the frequencies of each word across a text document.

47
00:03:43.620 --> 00:03:50.520
So again reading in all those words are now actually going to count the words and then I'm going to

48
00:03:50.520 --> 00:03:56.820
count them based off their exposition so we can see here that these particular words at these positions

49
00:03:57.210 --> 00:04:01.980
end up showing one time that I have a word here that shows up twice and then the rest of these words

50
00:04:02.010 --> 00:04:05.750
don't show up and the times they're only showing up in the other documents.

51
00:04:05.760 --> 00:04:10.930
So most of the time you're going to get a bunch of zeros and we explain this before.

52
00:04:10.950 --> 00:04:15.210
That means we're going to create a sparse matrix as we end up doing this for the other documents.

53
00:04:15.450 --> 00:04:18.690
So if we do this the same for the second document you get something that looks like this.

54
00:04:18.780 --> 00:04:22.400
Again a bunch of zeros for words that weren't used in the second document.

55
00:04:22.410 --> 00:04:27.320
So now we can begin to compare the vectors and we see that some words are common to both.

56
00:04:27.360 --> 00:04:29.120
Some appear only one that ticks.

57
00:04:29.220 --> 00:04:31.200
Others appear only in two that ticks T.

58
00:04:31.350 --> 00:04:36.120
And we're essentially going to extend this logic to tens of thousands of documents and we begin to see

59
00:04:36.120 --> 00:04:41.190
the vocabulary Dictionnaire grow to hundreds of thousands of words and the vectors again were mostly

60
00:04:41.190 --> 00:04:44.400
contain zero values making them sparse matrices.

61
00:04:44.400 --> 00:04:50.600
So now let's explore how we can learn about bag of words and T.F. IDF as well as using stop words Bortz

62
00:04:50.620 --> 00:04:51.140
them.

63
00:04:51.150 --> 00:04:53.600
Tokenization and tagging.

64
00:04:53.790 --> 00:04:57.110
So you can go ahead and read these descriptions here.

65
00:04:57.120 --> 00:04:59.290
A lot of this what we went over in that previous lecture.

66
00:04:59.610 --> 00:05:02.730
But we're going to now begin feature extraction from text.

67
00:05:02.730 --> 00:05:07.160
So for this I'm going to be again working with this messaging dataset.

68
00:05:07.290 --> 00:05:11.280
But now we're working directly with the message data.

69
00:05:11.280 --> 00:05:19.290
So I'm going to open up a brand new notebook and then follow along with that actual data set and using

70
00:05:19.290 --> 00:05:22.150
sikat learn to perform this feature extraction.

71
00:05:22.170 --> 00:05:30.930
The first thing to do is I'm going to import non-players MP and import panderers as PD and then we'll

72
00:05:30.930 --> 00:05:38.780
read in that file by saying read underscore CXXVI and then underneath the text files I have the Esam

73
00:05:38.790 --> 00:05:40.240
as spam collection.

74
00:05:40.410 --> 00:05:45.250
It's a TSB file meaning it's separated by tabs.

75
00:05:46.260 --> 00:05:51.710
And we can confirm this by checking out the head of that document.

76
00:05:51.780 --> 00:05:57.000
Previously we learned about sikat learn and how we can build a machine learning classifier given numerical

77
00:05:57.000 --> 00:05:57.910
information.

78
00:05:58.080 --> 00:06:03.690
What we want to do now is take this raw text information and vectorize it now is always a good idea

79
00:06:03.690 --> 00:06:05.070
to check for missing values.

80
00:06:05.070 --> 00:06:14.690
So one way to do that is saying DMF is null and taking some of that and they'll return back zeros if

81
00:06:14.690 --> 00:06:16.310
there are no missing values.

82
00:06:16.310 --> 00:06:21.990
Keep in mind you may also want to be a little more clever in this and check for strings that are empty.

83
00:06:22.130 --> 00:06:26.000
Maybe a missing value is not actually a completely missing value.

84
00:06:26.000 --> 00:06:29.870
It could just be a string of one space or something like that.

85
00:06:29.870 --> 00:06:31.690
Now we'll talk about that more in your assessment.

86
00:06:31.700 --> 00:06:33.230
But keep that in mind.

87
00:06:34.300 --> 00:06:34.840
OK.

88
00:06:34.970 --> 00:06:43.170
So we're going to do is take again a quick look at the label column and then call value counts here.

89
00:06:43.400 --> 00:06:50.830
You can see we have seven hundred forty seven instances of spam and 4000 825 instances of him.

90
00:06:50.850 --> 00:06:55.140
Next we're going to split the data into a training set and a test set.

91
00:06:55.200 --> 00:07:00.330
So we will say from S-K learn the model selection

92
00:07:02.930 --> 00:07:11.070
import train test split and then we're going to actually just look at the text.

93
00:07:11.080 --> 00:07:15.830
So I'm not even going to bother with the length of the message or the punctuation.

94
00:07:15.850 --> 00:07:19.390
I'm only going to be using the raw text as the feature itself.

95
00:07:19.600 --> 00:07:32.720
So I will say x is equal to the F message and Y is equal to if label and just by convention X is capitalized

96
00:07:32.810 --> 00:07:39.320
and Y is lower case that has to do with the fact that X kind of represents a larger matrix and Y is

97
00:07:39.320 --> 00:07:45.110
essentially a one dimensional array of just a bunch of labels Hamers spam.

98
00:07:45.110 --> 00:07:49.840
So an often question I get is why is X capitalized and why is lowercase.

99
00:07:49.850 --> 00:07:52.070
So the reason why just convention.

100
00:07:52.130 --> 00:07:58.820
Next we're going to actually perform the train split so when called Train to split do shift tab on this

101
00:07:59.000 --> 00:08:05.240
after you've imported it and you should be able to scroll down here and copy and paste right here.

102
00:08:05.480 --> 00:08:12.250
This command in order to save yourself a little bit of typing and we've already covered how the train

103
00:08:12.250 --> 00:08:13.200
to split works.

104
00:08:13.210 --> 00:08:20.630
So just going to pass in our X and r y we will choose a test size that's appropriate so we can say test

105
00:08:20.660 --> 00:08:22.410
size is equal to.

106
00:08:22.450 --> 00:08:27.400
And again there's no right or wrong answer but 30 percent or 33 percent a third of your data is pretty

107
00:08:27.400 --> 00:08:28.660
common.

108
00:08:28.660 --> 00:08:34.060
And then we will say a random state to make sure you're trained to split is randomized the same way

109
00:08:34.060 --> 00:08:35.280
minus or following on.

110
00:08:35.290 --> 00:08:36.310
Exactly.

111
00:08:36.310 --> 00:08:41.430
And you can just choose an arbitrary number so I'll choose 42 and you can choose 42 as well.

112
00:08:41.440 --> 00:08:44.330
So you get the same split I do next.

113
00:08:44.330 --> 00:08:47.720
It's time to actually perform count vectorization.

114
00:08:47.720 --> 00:08:50.510
So Tex pre-processing tokenizer.

115
00:08:50.540 --> 00:08:56.030
And the ability to filter out Stop words are all included in count victimiser which builds a dictionary

116
00:08:56.060 --> 00:09:01.500
of features and transforms documents to feature vectors and to use Skillern for this.

117
00:09:01.550 --> 00:09:13.130
We simply say from Learn the feature extraction text import and if hit tab here you can essentially

118
00:09:13.130 --> 00:09:18.110
scroll down and see the various things that are available like T.F. idea of transformer vectorize or

119
00:09:18.410 --> 00:09:20.690
vectorize or hashing vectorize or et cetera.

120
00:09:20.930 --> 00:09:28.280
Right now we're just going to use the Count vector Isar then we create an instance of the count vector

121
00:09:28.280 --> 00:09:37.120
Isar So I'd say count underscore that is equal to count vectorize or.

122
00:09:37.360 --> 00:09:39.790
So now you have this instance of this count vector Isar.

123
00:09:39.790 --> 00:09:46.300
So what I need to do is if I take a look at X it's right now still in raw text form what I want to do

124
00:09:46.420 --> 00:09:51.710
is pass it into the count vector Isar and then transform this.

125
00:09:51.730 --> 00:09:53.710
So there's two ways of doing this.

126
00:09:53.770 --> 00:10:01.950
The first thing I need to do is actually fit the vector Roser to the data and what that does is it does

127
00:10:01.950 --> 00:10:12.710
things like build a vocabulary and then it also does things like count the number of words and so on

128
00:10:13.400 --> 00:10:24.240
then the next step I need to do is transform the original text message to the vector.

129
00:10:24.240 --> 00:10:29.200
So this is known as a fit transform and there's two ways of doing this.

130
00:10:29.220 --> 00:10:32.970
One way is to first do your fitting and then do your transform.

131
00:10:33.240 --> 00:10:41.810
So you can do something like this or you can say count vectorize or fit to the training data and then

132
00:10:41.810 --> 00:10:52.250
say that my vectorized X train counts is equal to count vectorize victimiser and then call transform

133
00:10:53.820 --> 00:10:55.470
on your X training data.

134
00:10:55.470 --> 00:11:00.810
So that's one way to do it to fit essentially building on your vocabulary counting the words and then

135
00:11:00.960 --> 00:11:02.790
actually doing the transformation.

136
00:11:02.820 --> 00:11:06.990
So fitting by itself doesn't do the transformation you have to actually call that transform.

137
00:11:06.990 --> 00:11:15.020
So because almost always you do a fit and the transform together there's a convenience method that sikat

138
00:11:15.020 --> 00:11:21.410
learn has which is called if you call counter-act It's called Fit transform and essentially does these

139
00:11:21.410 --> 00:11:22.830
two steps for you.

140
00:11:23.090 --> 00:11:28.220
So you may see it both ways when you read the documentation you may see it as two separate steps fitting

141
00:11:28.430 --> 00:11:33.710
and then transforming or you may see it done in one line with this transform because it is so common

142
00:11:34.070 --> 00:11:36.500
to do these operations in conjunction.

143
00:11:36.680 --> 00:11:39.170
I'm going to comment this out and do it in one step.

144
00:11:39.200 --> 00:11:44.720
Since that's a little more efficient and we're just going to pass in our extremely data and this is

145
00:11:44.720 --> 00:11:50.420
going to fit essentially learn the vocabulary cut the number of words and then transform X train.

146
00:11:50.420 --> 00:11:59.230
So that's actually a vector numerical vectors so say X train counts and so that equal to the count vector

147
00:11:59.230 --> 00:12:02.110
Isar fit transform on the extreme data.

148
00:12:02.710 --> 00:12:08.590
So I run that and now I'm extremely data victory's or has fit and train on it and then transformed the

149
00:12:08.590 --> 00:12:09.630
original data.

150
00:12:09.670 --> 00:12:17.450
So if I take a look at x train counts notice that I actually can't view it because it's a huge sparse

151
00:12:17.540 --> 00:12:18.320
matrix.

152
00:12:18.530 --> 00:12:26.420
So sikat learn is actually smart enough to compress this into a compressed sparse Rove format.

153
00:12:26.420 --> 00:12:31.490
The way this works is you can imagine that if you have a bunch of zeros in a row it would make sense

154
00:12:31.490 --> 00:12:36.200
to actually compress this information and just count the number of rows you have or count the number

155
00:12:36.200 --> 00:12:37.530
of zeros you have in a row.

156
00:12:37.820 --> 00:12:39.300
So extreme counts.

157
00:12:39.320 --> 00:12:44.050
Notice it's three thousand seven hundred thirty three by seven thousand eighty two.

158
00:12:44.240 --> 00:12:51.320
So if you actually take a look at the original x train data and check the shape of it it has three thousand

159
00:12:51.630 --> 00:12:53.530
733 messages.

160
00:12:53.570 --> 00:12:59.910
So across all those messages there was seven thousand eighty two unique words.

161
00:12:59.930 --> 00:13:02.990
So a lot of those words are not going to show up in most of the text messages.

162
00:13:03.010 --> 00:13:09.170
You have a ton of zeros because of this sikat learn a PI are able to compress using the information

163
00:13:09.170 --> 00:13:12.890
that there's so many zeros there in order to save space in them or in your computer.

164
00:13:13.070 --> 00:13:18.050
So we're not going be able to directly see the sparse matrix unless we particularly call for it but

165
00:13:18.050 --> 00:13:23.340
that's usually not a good idea because you don't want to use that space in memory because it's not needed.

166
00:13:23.390 --> 00:13:25.660
We can let some PI be smarter about this.

167
00:13:25.670 --> 00:13:26.250
OK.

168
00:13:26.510 --> 00:13:31.240
So we have our original X-Trace shape those messages.

169
00:13:31.460 --> 00:13:38.270
And now if we Cecco X counts and asked for the shape of that we now see all those messages and then

170
00:13:38.420 --> 00:13:41.590
the vocab count that remember the vocab count we saw earlier.

171
00:13:41.630 --> 00:13:46.760
We come back to this feature extraction from text as well as the previous slide that right now is just

172
00:13:46.760 --> 00:13:48.110
looking something like this.

173
00:13:48.140 --> 00:13:54.440
Essentially all those counts across all those text messages where most of them are going to be zeros.

174
00:13:54.660 --> 00:14:00.870
Next we want to do is transform the counts to frequencies with T.F. IDF then we'll combine the steps

175
00:14:00.960 --> 00:14:05.280
with ATF IDF vectorize or we'll train the classifier and build the pipeline.

176
00:14:05.490 --> 00:14:08.770
We'll go ahead and continue right where we left off in the next lecture.

177
00:14:08.790 --> 00:14:16.440
But as a quick recap so far we've just read in our data we've imported and ran a train test split on

178
00:14:16.440 --> 00:14:17.400
this.

179
00:14:17.430 --> 00:14:22.620
We did a count of vectorization and then fit transform on the training data.

180
00:14:22.620 --> 00:14:23.780
I'll see you in the next lecture.

181
00:14:23.790 --> 00:14:29.160
We continue by combining these steps with term frequency inversed document frequency analysis.

