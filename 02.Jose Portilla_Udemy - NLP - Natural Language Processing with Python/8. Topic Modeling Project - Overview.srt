1
00:00:05,630 --> 00:00:07,430
Welcome back everyone in this lecture.

2
00:00:07,430 --> 00:00:11,410
We're going to have a brief overview of the assessment for the topic modeling section.

3
00:00:11,600 --> 00:00:15,010
Let's go ahead and open up the topic modeling Assessment Project notebook.

4
00:00:15,350 --> 00:00:15,770
OK.

5
00:00:15,770 --> 00:00:20,630
Underneath the topic modeling folder you should be able to find the Assessment Project notebook as well

6
00:00:20,630 --> 00:00:25,430
as the Assessment Project solutions and going to overview the Assessment Project and in the next lecture

7
00:00:25,460 --> 00:00:27,860
we'll go over the solutions together.

8
00:00:27,960 --> 00:00:31,750
So the Assessment Project is a topic modeling Assessment Project.

9
00:00:31,830 --> 00:00:37,020
And for this category you're going to be working over 400000 questions from Quora.

10
00:00:37,020 --> 00:00:42,150
Quora is a question answer site where people can post questions and then crowdsource from the community

11
00:00:42,150 --> 00:00:43,070
for answers.

12
00:00:43,230 --> 00:00:48,810
And what we're going to do is to ask that these questions though actually have any labeled category.

13
00:00:48,840 --> 00:00:53,250
And if you're looking for the questions it's simply this Quora underscore questions thought see a tweet

14
00:00:53,250 --> 00:00:53,830
file.

15
00:00:54,060 --> 00:00:58,520
So essentially this is a list of questions that look something like this.

16
00:00:58,580 --> 00:01:04,860
So once asking What's step by step guide to invest in something mentally very lonely which one way to

17
00:01:04,860 --> 00:01:09,880
dissolve water quickly sugar cetera and you can see here that there's a wide variety of topics.

18
00:01:09,900 --> 00:01:15,030
And right now we only have the questions so maybe nice if we can build some sort of automated system

19
00:01:15,390 --> 00:01:21,690
that automatically tries to guess the topic of an incoming question and then we be able to put it in

20
00:01:21,690 --> 00:01:24,180
the correct location on the court Web site.

21
00:01:24,180 --> 00:01:28,770
So we're going to do the things that we did before import pantless read in the CSC file.

22
00:01:28,770 --> 00:01:30,250
You do some preprocessing.

23
00:01:30,300 --> 00:01:37,530
And for this particular project I want you to use non-negative matrix factorization if you want.

24
00:01:37,530 --> 00:01:40,280
You can also perform this with LDA.

25
00:01:40,410 --> 00:01:45,900
But because of the size of the data set it's quite a bit larger than the articles dataset.

26
00:01:45,960 --> 00:01:52,260
And since MF is faster than LDA we only expect you to use an MF for this project.

27
00:01:52,290 --> 00:01:58,740
Again it's optional if you want to use LDK but we only expect an MF so perform T.F. IDF vectorization

28
00:01:59,130 --> 00:02:01,930
explore the Maxy and mehndi parameters.

29
00:02:01,980 --> 00:02:06,210
There's no right or wrong parameters here but you can use the same ones we used in the lecture.

30
00:02:06,210 --> 00:02:08,450
You should get a sparse matrix that looks something like this.

31
00:02:08,460 --> 00:02:11,210
Again notice it's quite a bit larger than NPR.

32
00:02:11,220 --> 00:02:13,050
NPR it was around 12000 articles.

33
00:02:13,050 --> 00:02:16,350
Here we're dealing with hundred and four thousand questions.

34
00:02:16,350 --> 00:02:18,230
Granted the questions are shorter.

35
00:02:18,330 --> 00:02:24,150
So that has to do a little bit with the size difference but these There's way more questions and there

36
00:02:24,150 --> 00:02:27,170
are articles so this data is bigger.

37
00:02:27,390 --> 00:02:32,850
And then you can reform non-avian matrix factorization and you're going to ask for 20 expected components

38
00:02:32,940 --> 00:02:35,580
essentially try to find 20 topics here.

39
00:02:35,580 --> 00:02:41,070
And I also encourage you to use random state 42 so you can compare the same topics that we get to the

40
00:02:41,070 --> 00:02:46,050
ones you get and then print out the top 15 most common words for each of the 20 topics.

41
00:02:46,050 --> 00:02:49,120
So here are the various topics that I got with random state.

42
00:02:49,140 --> 00:02:55,410
You can see some of these are learning languages and books things on the election things on businesses

43
00:02:55,410 --> 00:03:04,050
countries Russia Pakistan things on movies songs job work engineering things on India looks like resolutions

44
00:03:04,080 --> 00:03:05,570
New Year's etc..

45
00:03:05,700 --> 00:03:07,770
So there's a bunch of different topics there.

46
00:03:08,160 --> 00:03:12,210
And then you're going to add a new column to the original court data frame that labels each question

47
00:03:12,390 --> 00:03:13,940
into one of the 20 topics.

48
00:03:13,980 --> 00:03:18,360
So you're going to perform from our original data frame that has a list of questions to something that

49
00:03:18,360 --> 00:03:21,930
looks like this some sort of topics and obviously if you want.

50
00:03:21,990 --> 00:03:26,100
You can add a you know topic name and map some sort of topic names.

51
00:03:26,100 --> 00:03:29,460
Each of these topics that's not really necessary because there's two main topics I would expect you

52
00:03:29,460 --> 00:03:31,190
to manually name them all.

53
00:03:31,200 --> 00:03:35,230
But again essentially at the end of the day we should have something that looks like this.

54
00:03:35,250 --> 00:03:35,910
All right.

55
00:03:35,970 --> 00:03:38,070
So best of luck with that assessment project.

56
00:03:38,130 --> 00:03:41,170
Coming up next we're going to show you the solutions.

57
00:03:41,180 --> 00:03:41,880
We'll see you there.

