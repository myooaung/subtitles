1
00:00:05,280 --> 00:00:06,080
Welcome back.

2
00:00:06,270 --> 00:00:10,780
Let's go ahead and learn how to use sikat learn through this code long lecture and open up Jupiter notebook

3
00:00:10,860 --> 00:00:11,710
now.

4
00:00:11,730 --> 00:00:12,020
All right.

5
00:00:12,030 --> 00:00:19,440
So of to begin we're going to import some pie as NPE and we're also going to be importing a library

6
00:00:19,440 --> 00:00:25,530
called pandas and pand is going to allow us to actually read in CXXVI files which are comma separated

7
00:00:25,530 --> 00:00:31,230
values files and other file types like text files or tab separated files and so on.

8
00:00:31,230 --> 00:00:35,910
So we're going to be using panels a little bit and we'll be showing you a quick look about a few various

9
00:00:35,910 --> 00:00:43,080
Pandurs methods that are really useful in conjuncture sikat learn so Pancho's can actually read in files

10
00:00:43,170 --> 00:00:47,390
into what's known as a data frame object and it does this through the command.

11
00:00:47,390 --> 00:00:52,260
PD read underscore see s the.

12
00:00:52,360 --> 00:00:55,320
And then we pass in the file path to the file.

13
00:00:55,570 --> 00:00:58,990
So we're going to be opening the up underneath text files.

14
00:00:58,990 --> 00:01:06,740
There is an Esam as spam collection that TSB file and it's a TSB meaning it's tabs separated.

15
00:01:06,740 --> 00:01:10,590
So we're going to pass in one more parameter here which is going to be backslash T.

16
00:01:10,610 --> 00:01:16,640
And that indicates a tab just telling Panne those in Python that this particular file isn't separated

17
00:01:16,640 --> 00:01:22,520
by commas for each column but separated by tabs and then we're going to assign this to a variable called

18
00:01:22,520 --> 00:01:26,120
the F and then off of this variable.

19
00:01:26,260 --> 00:01:28,820
If you hit that and then hit tab once it's been defined.

20
00:01:28,930 --> 00:01:34,300
So if you run that cell that actually defined if you hit tab you'll notice there's quite a few attributes

21
00:01:34,390 --> 00:01:35,250
and methods.

22
00:01:35,380 --> 00:01:39,040
We actually don't need to worry about too many of these but the one we're going to see right now is

23
00:01:39,040 --> 00:01:40,130
called head.

24
00:01:40,450 --> 00:01:43,990
And this is going to show us the first five rows.

25
00:01:44,110 --> 00:01:46,920
So these are actual data set and we've seen this before.

26
00:01:46,990 --> 00:01:53,650
We have a label indicating if a text message is ham or spam and then we have the actual text of the

27
00:01:53,650 --> 00:01:58,980
message and then we have the length of the message and then we have the punctuation.

28
00:01:58,990 --> 00:02:01,950
So how many actual punkish pieces of punctuation.

29
00:02:01,960 --> 00:02:03,730
Does this message contain.

30
00:02:03,730 --> 00:02:07,660
We still don't know how to extract features from the text.

31
00:02:07,690 --> 00:02:13,180
So instead we're just going to be using these direct numerical features and then later on we'll realize

32
00:02:13,300 --> 00:02:19,540
and learn how to actually convert this text message that the most important feature here into numerical

33
00:02:19,540 --> 00:02:22,760
information using text feature extraction.

34
00:02:22,960 --> 00:02:27,880
But for this lecture we're going to be focusing on things that are already numbers in this dataset in

35
00:02:27,880 --> 00:02:33,620
order to use them to actually perform a text classification machine learning model.

36
00:02:33,670 --> 00:02:38,170
Now machinery models usually require complete data.

37
00:02:38,170 --> 00:02:45,490
So one easy way to check if your data is missing anything is by taking this data from object and then

38
00:02:45,490 --> 00:02:47,130
calling is null.

39
00:02:47,200 --> 00:02:53,740
And if you just run is no by itself it returns back in other data frame of booleans in the killing true

40
00:02:53,740 --> 00:02:56,730
or false is a null or missing value.

41
00:02:56,740 --> 00:03:03,020
So what you can do is offer this simply take the sum and it's going to treat these falses as zeros.

42
00:03:03,190 --> 00:03:08,380
And if there's anything missing they'll say it's true that it is null and ill treat that as a 1.

43
00:03:08,410 --> 00:03:11,510
So if you say is no and then hold that sum.

44
00:03:11,820 --> 00:03:17,340
Then if you ever see a one here then you know there's a missing value in this case everything is zero.

45
00:03:17,500 --> 00:03:19,910
So actually no we're not missing any data.

46
00:03:20,870 --> 00:03:26,510
Now we can also check information about this data frame like how many rows it has by simply checking

47
00:03:26,510 --> 00:03:31,700
the length of state of frame and it returns back that currently we have five thousand five hundred and

48
00:03:31,700 --> 00:03:33,020
seventy two rows.

49
00:03:33,020 --> 00:03:35,850
So we have five thousand five hundred seventy two text messages.

50
00:03:35,990 --> 00:03:40,880
Each of them labeled as well as understanding their length and they're basically how much punctuation

51
00:03:40,880 --> 00:03:43,240
this particular message has.

52
00:03:43,290 --> 00:03:49,200
Now we're going to do is take a quick look at the ham and spam label columns and the way you can access

53
00:03:49,200 --> 00:03:56,160
columns off a data frame is by calling data frame and then in square braces passing in the name of the

54
00:03:56,160 --> 00:04:02,600
column the name of the column is right here symbol to see the label message length or PNC.

55
00:04:02,730 --> 00:04:03,860
So those are a column names.

56
00:04:03,870 --> 00:04:11,240
Let's go ahead and check in the label by passing it in as a string so here I just grab the label column

57
00:04:11,540 --> 00:04:18,880
and if I say that unique as a method it's going to return back the actually unique values.

58
00:04:18,910 --> 00:04:20,380
So here you have two unique values.

59
00:04:20,410 --> 00:04:21,690
Ham and spam.

60
00:04:22,180 --> 00:04:24,040
And then I can call off the same column

61
00:04:26,920 --> 00:04:34,060
the value underscore counts and it will actually count how many of each unique values we have.

62
00:04:34,280 --> 00:04:40,730
So it looks like we are four thousand eight hundred twenty five him text messages and 747 spam text

63
00:04:40,730 --> 00:04:44,180
messages.

64
00:04:44,180 --> 00:04:51,080
Now what we're going to attempt to do is try to build a very simple machine learning model that attempts

65
00:04:51,080 --> 00:04:57,050
to predict whether a message is ham or spam based solely on these two numerical features the length

66
00:04:57,110 --> 00:05:00,770
of the message and the punctuation of the message.

67
00:05:00,770 --> 00:05:02,700
And we can actually visualize this.

68
00:05:02,840 --> 00:05:08,390
In fact if you check out the notebook that goes along this lecture if you scroll down we show some code

69
00:05:08,480 --> 00:05:14,950
that allows us to visualize ham versus spam and the actual punctuation or the length of the message.

70
00:05:14,990 --> 00:05:16,690
We can produce histograms for it.

71
00:05:16,910 --> 00:05:22,130
So don't worry too much about what this actual code does because this isn't really a visualization course

72
00:05:22,220 --> 00:05:24,300
if you are interested in what this code is doing.

73
00:05:24,320 --> 00:05:26,090
You can check among other courses on that.

74
00:05:26,180 --> 00:05:29,670
But really here we're just interested in this visualization.

75
00:05:29,960 --> 00:05:33,690
So I'm going to just copy and paste the code that's here.

76
00:05:34,860 --> 00:05:36,980
And then explain what it's actually doing.

77
00:05:36,990 --> 00:05:43,800
So this is producing two histograms one for the hand messages and then one for the spam messages.

78
00:05:43,800 --> 00:05:48,660
So right now what this is doing is it's looking at the length column and it's actually showing on a

79
00:05:48,660 --> 00:05:50,220
logarithmic scale.

80
00:05:50,310 --> 00:05:57,810
And notice here that the actual distribution of ham text message length versus spam text message length

81
00:05:57,870 --> 00:05:59,100
is quite different.

82
00:05:59,100 --> 00:06:07,260
Spam text messages look to be longer in length than ham text messages and intuitively that actually

83
00:06:07,260 --> 00:06:08,270
makes sense.

84
00:06:08,310 --> 00:06:13,800
A lot of ham text messages are legitimate text messages are going to be short they're just going to

85
00:06:13,800 --> 00:06:20,580
say things like ok or on my way or see you there while spam text messages typically are going to be

86
00:06:20,580 --> 00:06:26,160
quite longer because there's a lot more information to convey because there's more spam so we can check

87
00:06:26,160 --> 00:06:31,740
it out here by actually seeing something like this free entry and a two weekly competition et cetera.

88
00:06:32,010 --> 00:06:35,530
Most likely if it's a spam text message going to be quite a bit longer.

89
00:06:35,550 --> 00:06:40,350
So this one is 155 versus these other hand ones tend to be a little shorter.

90
00:06:40,350 --> 00:06:45,750
That's not to say that ham or legitimate text messages can't be long as well but the distribution of

91
00:06:45,750 --> 00:06:47,330
each of these is quite different.

92
00:06:47,580 --> 00:06:53,130
So that probably in the Cate's that's a good numerical feature to provide to our machine learning model.

93
00:06:53,200 --> 00:06:58,180
It's still simplistic and we're really just focusing on how to use the sikat learn syntax.

94
00:06:58,180 --> 00:07:02,740
So don't worry too much about this being the most robust machine learning model ever.

95
00:07:02,890 --> 00:07:07,420
Now also we're going to do is we're going to take a look at the punctuation column and see if it has

96
00:07:07,510 --> 00:07:09,220
maybe a similar behavior.

97
00:07:09,220 --> 00:07:11,070
So again just coming from our notebook.

98
00:07:11,070 --> 00:07:13,890
We're going to scroll down here and then copy the code.

99
00:07:13,960 --> 00:07:21,100
It's essentially the same thing but now looking at the punctuation column on a logarithmic scale and

100
00:07:21,160 --> 00:07:24,660
you see here these are two histograms for ham versus spam.

101
00:07:24,660 --> 00:07:30,150
Punctuation wise and here that doesn't tend to be any distinct behavior.

102
00:07:30,180 --> 00:07:37,250
It looks like spam tends to have a little bit more relative to other hand messages.

103
00:07:37,350 --> 00:07:43,340
It it's a little more punctuation but tear the behavior is not as clear or the stink as it is here.

104
00:07:43,350 --> 00:07:49,670
So in general we can say that it looks like spam tends to have a higher range of values.

105
00:07:49,710 --> 00:07:55,380
It looks like spam messages tend to be longer and there's less of a range of overall values.

106
00:07:55,380 --> 00:08:00,380
They all tend to be pretty long versus him has quite a large range of length.

107
00:08:00,510 --> 00:08:05,370
So we're going to be attempting to use again these two numerical features along the label.

108
00:08:05,400 --> 00:08:10,920
Again we're really just focusing on sikat learn later on we'll actually learn how to use this text message

109
00:08:11,190 --> 00:08:12,670
data.

110
00:08:12,720 --> 00:08:18,100
So what we're going to do here is learn how to actually conduct a machine learning model of sikat learn.

111
00:08:18,270 --> 00:08:22,220
So the first thing to do is to split the data into a training set and a test.

112
00:08:22,440 --> 00:08:32,960
And the way we do this is we simply say from S-K learn the model selection and he should be able to

113
00:08:32,980 --> 00:08:38,830
tab a complete that we're going to import train test split and we've seen this before.

114
00:08:39,200 --> 00:08:45,530
And then we're going to call train test split and we're going to pasan x and y.

115
00:08:45,670 --> 00:08:49,470
But first we need to define X and Y actually are.

116
00:08:49,730 --> 00:08:57,510
So what we're going to do is we're going to say that X is our featured data.

117
00:08:57,730 --> 00:09:02,430
And why is our label so for feature data.

118
00:09:02,760 --> 00:09:08,210
We're going to say x is equal to the F and I'm going to use two sets of brackets here.

119
00:09:08,220 --> 00:09:17,250
I'm passing in a list of column names and will be passing in the length column and the punctuation column

120
00:09:17,320 --> 00:09:17,640
again.

121
00:09:17,640 --> 00:09:23,820
Note the use of double brackets I'm passing in a list of columns which is why I have two sets of brackets

122
00:09:24,430 --> 00:09:30,520
and then we're going to pass in our label which in this case that column is just called label.

123
00:09:30,570 --> 00:09:33,540
So have X feature data and white label data.

124
00:09:34,140 --> 00:09:36,850
Then what we want to do is pass that into train test split.

125
00:09:36,960 --> 00:09:41,960
And what I encourage you to do is if you do shift tab in Jupiter you can see the docstring for train

126
00:09:41,960 --> 00:09:48,300
test split and if you scroll down inside of this eventually see this nice example you can scroll down

127
00:09:48,450 --> 00:09:56,850
and essentially copy and paste this tuple that says X train x test Y train and white test because using

128
00:09:56,850 --> 00:10:00,030
tuple and packing we can see what is actually returned here.

129
00:10:00,150 --> 00:10:06,360
Train to split as we mentioned previously in the other lectures is going to return your training set

130
00:10:06,600 --> 00:10:12,450
and your X test set your wily Wil's for the training set and your y labels for the test set and then

131
00:10:12,450 --> 00:10:16,790
it's up to you to decide what the test size should be.

132
00:10:16,800 --> 00:10:21,060
So we're going to say that 30 percent of our data is going to the test data set.

133
00:10:21,970 --> 00:10:28,480
And then the other thing you may want to do is say a random state because this train split happens randomly.

134
00:10:28,690 --> 00:10:34,540
So it's going to grab random rows that way in case your data sorted that won't affect the actual train

135
00:10:34,540 --> 00:10:35,700
test split.

136
00:10:35,710 --> 00:10:41,290
So if for if you want to be able to repeat this sort of randomness and the train test split you can

137
00:10:41,290 --> 00:10:49,810
provide a random states and then choose an arbitrary integer value to in the actual value itself is

138
00:10:49,810 --> 00:10:50,900
kind of meaningless.

139
00:10:50,920 --> 00:10:56,020
What matters is that you use the same value if you want to repeat this train to split.

140
00:10:56,350 --> 00:11:01,360
So we're going to pass 42 and if you pass and 42 that means you'll get the same train to split that

141
00:11:01,520 --> 00:11:02,060
do.

142
00:11:02,230 --> 00:11:06,720
If you pass in a different random state that's fine you just won't get the same split that I do.

143
00:11:06,760 --> 00:11:10,090
So I encourage you to fall exactly and type in 42.

144
00:11:10,090 --> 00:11:12,110
Again that's just the random state for the split.

145
00:11:13,670 --> 00:11:18,930
So we're going to run that and now we can check out the shape of this data by simply saying X train

146
00:11:18,960 --> 00:11:19,340
sheep

147
00:11:22,140 --> 00:11:27,510
notice here X train is three thousand nine hundred rows and two columns.

148
00:11:27,540 --> 00:11:32,800
So the two columns there are length and punctuation X test.

149
00:11:32,860 --> 00:11:41,410
Check out the shape of that is 1672 rows with two columns so extreme that's going to be our training

150
00:11:41,410 --> 00:11:47,080
data for the features the length of punctuation X test that's going to be the test features.

151
00:11:47,080 --> 00:11:53,740
And remember it's still like the punctuation but it's only 30 percent of the data and we can see why

152
00:11:53,740 --> 00:11:54,820
test.

153
00:11:54,820 --> 00:11:58,280
And if you check out the shape of that that's essentially the labels right now.

154
00:11:58,310 --> 00:11:58,930
So no.

155
00:11:58,950 --> 00:12:06,820
1 6 7 2 because it's the labels that correspond with X tests so ham ham spam and the index position

156
00:12:07,240 --> 00:12:08,820
is kept in memory.

157
00:12:08,830 --> 00:12:12,470
So that's just the index that corresponds with X test.

158
00:12:12,640 --> 00:12:18,550
So if we delete that thought shape and look at X test notice that we have these index positions and

159
00:12:18,550 --> 00:12:22,510
that's going to allow us to match up the Y test to the X test.

160
00:12:22,870 --> 00:12:29,710
And that how this is not in order because we randomly selected them using train test split.

161
00:12:29,730 --> 00:12:34,590
OK so we have our X test our test our X train and are y train.

162
00:12:34,590 --> 00:12:38,940
Now it's time to actually create and train a machine learning model.

163
00:12:39,000 --> 00:12:44,310
So I'm going to show you how to train multiple models and you hopefully realize that the exact process

164
00:12:44,370 --> 00:12:47,940
is really similar regardless of what model you choose.

165
00:12:47,940 --> 00:12:51,150
So the general syntax is the following.

166
00:12:51,150 --> 00:12:57,300
You say from escolar dots and then the family of models and if you hit tab here you'll notice that there's

167
00:12:57,360 --> 00:12:58,250
a lot of options.

168
00:12:58,270 --> 00:12:59,940
Now all of these are families.

169
00:13:00,030 --> 00:13:03,120
The first we're going to show you is a logistic regression model.

170
00:13:03,420 --> 00:13:10,670
So we say he learned that linear model import and we're going to say logistic regression.

171
00:13:10,770 --> 00:13:13,190
So that's step number one in putting the model.

172
00:13:13,380 --> 00:13:15,800
Then it's time to create an instance of the model.

173
00:13:16,020 --> 00:13:20,550
We will say well our model is equal to.

174
00:13:20,550 --> 00:13:26,160
And then you create an instance of a logistic regression and if he does shift tab here you'll notice

175
00:13:26,160 --> 00:13:30,810
that you have a ton of different parameters that you can choose.

176
00:13:30,840 --> 00:13:33,430
Often the default values are actually quite good.

177
00:13:33,900 --> 00:13:39,000
If you want to learn more about how to actually edit these parameters what things like seemin or what

178
00:13:39,000 --> 00:13:40,460
solver's to use.

179
00:13:40,500 --> 00:13:45,030
You can check out the reading an introduction to the learning this the book we previously mentioned

180
00:13:45,360 --> 00:13:50,430
which goes into a lot more detail of the mathematics behind a lot of these parameters but these are

181
00:13:50,430 --> 00:13:55,580
the sort of parameters you'd be editing after you test your model against the test data.

182
00:13:55,590 --> 00:13:59,630
So just for example we could edit this particular solver.

183
00:13:59,700 --> 00:14:04,950
So if you scroll down here there's a lot of explanations over what these actually do these different

184
00:14:04,950 --> 00:14:05,830
parameters.

185
00:14:05,850 --> 00:14:09,870
So if you wanted to edit the solver you were to scroll down here until you see solver's.

186
00:14:10,020 --> 00:14:14,550
And this shows you the different algorithms it uses as well as what options you have.

187
00:14:14,640 --> 00:14:17,740
So you have this Newton option.

188
00:14:17,810 --> 00:14:19,350
Elby Yes et cetera.

189
00:14:19,380 --> 00:14:24,630
And that actually even gives me advice so for small data sets lib Liniers is a good choice for multiclass

190
00:14:24,630 --> 00:14:25,450
problems.

191
00:14:25,560 --> 00:14:27,910
These can handle multinomial loss etc..

192
00:14:28,170 --> 00:14:32,670
So just to show you an example of how you would actually do this you could just for instance choose

193
00:14:32,670 --> 00:14:39,770
a different solver like Elby G-S and then say for this particular logistic regression I want solver

194
00:14:39,770 --> 00:14:42,580
to be LBG of us.

195
00:14:42,590 --> 00:14:47,110
Once you've completed that step it's time to actually fit the model to the training data.

196
00:14:47,540 --> 00:14:55,000
So you just take your model called that fit and then pass your X training data and the labels that corresponds

197
00:14:55,000 --> 00:14:56,120
to that data set.

198
00:14:56,120 --> 00:14:58,590
Notice that I'm not setting this equal to a variable.

199
00:14:58,760 --> 00:15:00,880
I'm simply just calling that fit.

200
00:15:00,920 --> 00:15:03,470
I don't need to actually set the sequel to any variable.

201
00:15:03,480 --> 00:15:09,710
I'm calling that fit on the training data and then it will report back the actual model that was used

202
00:15:10,040 --> 00:15:11,890
along all the parameters that was used.

203
00:15:11,960 --> 00:15:14,750
And now the model is ready to predict.

204
00:15:14,780 --> 00:15:19,550
So coming up in the next lecture we're going to show you how to actually test the accuracy of the model

205
00:15:19,640 --> 00:15:21,410
against the test data set.

206
00:15:21,470 --> 00:15:22,160
We'll see if.

