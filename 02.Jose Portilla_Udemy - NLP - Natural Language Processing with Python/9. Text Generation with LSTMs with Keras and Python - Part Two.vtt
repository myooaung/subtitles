WEBVTT
ï»¿1
00:00:05.560 --> 00:00:10.210
Welcome everyone to part to touch generation with Python and Cara's.

2
00:00:10.340 --> 00:00:14.410
So for this lecture we're going to show you how to create the TAM based model.

3
00:00:14.540 --> 00:00:18.170
But before we do that we want to split the data into features and labels.

4
00:00:18.320 --> 00:00:22.550
So we're going to have our X features which is the first N-words of the sequence and then we'll have

5
00:00:22.550 --> 00:00:26.220
our white label which is the very next word after that entire sequence.

6
00:00:26.270 --> 00:00:30.710
Once you've done that we'll be able to fit the model on that feature to predict the next word in the

7
00:00:30.710 --> 00:00:31.560
sequence.

8
00:00:31.880 --> 00:00:33.850
OK let's jump to the notebook.

9
00:00:34.150 --> 00:00:38.910
OK so here we are where we left off last time with these actual sequences in a matrix format.

10
00:00:38.990 --> 00:00:45.440
What I want to do is perform a train test split and the train display is going to go ahead and separate

11
00:00:45.950 --> 00:00:48.780
these first columns as features.

12
00:00:48.890 --> 00:00:52.100
And then this last column as the target we want to predict.

13
00:00:52.100 --> 00:00:54.890
So we're going to do a couple of things here.

14
00:00:55.100 --> 00:01:02.800
We'll say from Kurus dot utils import to categorical.

15
00:01:03.380 --> 00:01:10.580
And again we're going to then take our sequences and grab everything but the last words.

16
00:01:10.780 --> 00:01:14.440
So we'll say for all rows starting from the very beginning.

17
00:01:14.490 --> 00:01:19.390
Grab everything but that very last column.

18
00:01:19.390 --> 00:01:21.150
So again if I take a look at what this means.

19
00:01:21.160 --> 00:01:25.980
Notice we have 14 24 9 6 5 here that's exactly we have here on the end.

20
00:01:26.080 --> 00:01:30.070
So essentially grabbing for every row all the columns except the very last column.

21
00:01:30.100 --> 00:01:33.010
That's what the slice notation is doing with no pie.

22
00:01:33.010 --> 00:01:36.610
So these are the first X-amount of words of your sequence length.

23
00:01:36.670 --> 00:01:39.040
In this case we happened to choose a sequence length 25.

24
00:01:39.040 --> 00:01:42.890
Again that's something you can edit as you go along and play this yourself.

25
00:01:42.940 --> 00:01:49.420
But here in this case we have the first twenty five words and the very last word of these sequences

26
00:01:50.490 --> 00:01:52.570
essentially is going to be that last column.

27
00:01:52.600 --> 00:01:56.960
So we're going to say OK grab all the rows and I'll just grab that last column.

28
00:01:57.250 --> 00:02:00.270
So that is 24 9 6 5 5 and so on.

29
00:02:00.310 --> 00:02:04.760
So notice what the values are here are the original sequences 24 9 6 5 and so on.

30
00:02:04.870 --> 00:02:13.510
And then 2 2 7 0 9 26 and here we have 2 2 7 9 26 so hopefully that's clear that now I can simply say

31
00:02:13.900 --> 00:02:17.700
X are my features those first X-amount of words.

32
00:02:17.830 --> 00:02:20.710
And then the label is what I'm looking for.

33
00:02:20.710 --> 00:02:29.920
So in this case that very last word and what I'm going to do is then change that y into a two categorical.

34
00:02:29.920 --> 00:02:35.900
So I'll say why is equal to two categorical Y.

35
00:02:35.960 --> 00:02:44.240
And then the number of classes is simply the vocabulary size we discussed earlier plus one because of

36
00:02:44.240 --> 00:02:45.680
the way Keres padding works.

37
00:02:45.680 --> 00:02:49.950
It needs an extra one to hold eight zero.

38
00:02:49.980 --> 00:02:58.300
OK so we passed that into two categorical and then to set our sequence length we simply asked well what

39
00:02:58.300 --> 00:03:01.450
was the shape of x.

40
00:03:01.640 --> 00:03:03.010
Index 1.

41
00:03:03.010 --> 00:03:08.830
So if I take a look at the shape of my features right now it's eleven thousand three hundred sixty eight.

42
00:03:08.920 --> 00:03:12.420
That means I have 11000 368 sequences.

43
00:03:12.460 --> 00:03:18.640
So those were essentially those shifted 25 words sentences and then how many words are in each sentence

44
00:03:18.640 --> 00:03:20.290
will I know it's 25.

45
00:03:20.290 --> 00:03:23.710
So that means my sequence length is this right here 25.

46
00:03:23.710 --> 00:03:28.270
So the reason we said it using X thought shape is that where you can later come back up here and then

47
00:03:28.270 --> 00:03:32.160
play around with the 2005 value if you come back up here.

48
00:03:32.170 --> 00:03:35.520
Remember we chose our training length to be 25 plus one.

49
00:03:35.530 --> 00:03:36.840
That one we're going to predict.

50
00:03:36.850 --> 00:03:41.800
So if you play around with that value that should be taken care of automatically as you come down here

51
00:03:41.950 --> 00:03:43.510
and then play around the shape.

52
00:03:43.570 --> 00:03:49.840
So now I have my sequence length expected is 25 predict the very next word after those 25 words.

53
00:03:50.050 --> 00:03:54.910
Now that we've organized our data and done a trained split on the data it's time to actually create

54
00:03:54.910 --> 00:03:55.850
the model.

55
00:03:56.250 --> 00:04:04.330
And for this we're simply going to say from cars that models import sequential and then from carious

56
00:04:04.540 --> 00:04:11.990
that lawyers import and we'll be importing a dense layer in Ellis T.M. layer to the all the sequences

57
00:04:12.260 --> 00:04:15.840
and in embedding layer to deal with the vocabulary.

58
00:04:15.860 --> 00:04:23.970
And we're going to functionals this so I'll say T.F. we'll call it create model and it takes in a vocabulary

59
00:04:23.970 --> 00:04:28.360
size as well as that sequence length that we just defined earlier.

60
00:04:30.170 --> 00:04:37.310
Well start off as we always do creating an instance of that sequential model and then we will add to

61
00:04:37.310 --> 00:04:38.110
this model.

62
00:04:38.180 --> 00:04:46.440
So we'll say embedding and we'll pasan a vocabulary size passen the sequence length.

63
00:04:46.440 --> 00:04:49.740
In fact we can just pasand what we passed and their sequence length.

64
00:04:49.860 --> 00:04:55.960
And then again we're going to define that the input length is equal to the sequence length.

65
00:04:56.010 --> 00:05:00.780
So it's actually going on here for this embedding is we're defining an input dimension for the embedding

66
00:05:00.840 --> 00:05:03.240
which is the entirety of the vocabulary.

67
00:05:03.240 --> 00:05:07.170
We're also defining the output dimension which is just the sequence length.

68
00:05:07.170 --> 00:05:11.750
And then we're defining the input length which is also equal to the sequence length.

69
00:05:12.040 --> 00:05:12.520
OK.

70
00:05:12.690 --> 00:05:16.870
So again that's what the embedding is doing and if front to figure out more about it it's really all

71
00:05:16.880 --> 00:05:25.010
it's doing here is it turns positive integers or indexes into a dense vectors of a fixed size.

72
00:05:25.050 --> 00:05:30.670
So essentially transforming something like this into this fixed size based vocabulary.

73
00:05:30.960 --> 00:05:34.340
And this has to be the first layer in the model OK.

74
00:05:34.400 --> 00:05:38.900
And you can see more examples of all this embedding essentially just allowing us to deal of that text

75
00:05:38.900 --> 00:05:39.610
data.

76
00:05:40.050 --> 00:05:47.030
OK so we have the imbedding and now we can add in some LACMA units to actually be trained on the sequence.

77
00:05:47.120 --> 00:05:53.500
So at Ellis T.M. and that LACMA you decide how many neurons are going to go in there or how many units.

78
00:05:53.550 --> 00:05:59.240
Again no right answer here but it's usually a good idea to make some sort of multiple off of your sequence

79
00:05:59.240 --> 00:06:00.130
length.

80
00:06:00.170 --> 00:06:06.030
So I'm going to go and go a little larger on this or just for the purposes of video.

81
00:06:06.050 --> 00:06:07.110
I'll go 150.

82
00:06:07.250 --> 00:06:10.180
But we provided some models are trained with more neurons.

83
00:06:10.190 --> 00:06:13.780
Again the more neurons we add here it's going to take slightly longer to train.

84
00:06:13.790 --> 00:06:17.000
So let's just go with two acts or sequence length so 15.

85
00:06:17.120 --> 00:06:21.650
And in fact if you really wanted to you could simply find this based off your sequence length something

86
00:06:21.650 --> 00:06:26.470
like sequence sequence length times 2 or something like that for the number of neurons.

87
00:06:26.780 --> 00:06:27.860
It's really up to you.

88
00:06:28.400 --> 00:06:30.240
We'll decide on 15 year olds.

89
00:06:30.290 --> 00:06:33.450
This will give us the best results but it should train a lot faster.

90
00:06:33.710 --> 00:06:39.940
And then we're going to ask for the return sequences to be true.

91
00:06:40.180 --> 00:06:46.690
And then we can add in another layer of elist T.M. again choosing some out of neurons for that one.

92
00:06:47.850 --> 00:06:50.820
And then at the end we're going to add in.

93
00:06:51.190 --> 00:06:58.450
But then Slayer this one you probably want to be a little larger but we'll go ahead and do 50 this network

94
00:06:58.450 --> 00:07:01.960
itself probably will give you such great results since it's a little smaller but they also want to take

95
00:07:01.960 --> 00:07:03.000
forever to train.

96
00:07:03.340 --> 00:07:05.820
In general it's going to take a really long time to train.

97
00:07:05.820 --> 00:07:09.000
I think the one we provide you took about three hours to train.

98
00:07:09.130 --> 00:07:13.880
So we have that's activation rectified linear unit at the very end.

99
00:07:13.960 --> 00:07:18.480
We want to translate this into a vocabulary word because it's just going to spit out a number.

100
00:07:18.640 --> 00:07:28.090
So we want a dense network of our vocabulary size where the activation is soft Max and the less we need

101
00:07:28.090 --> 00:07:32.900
to do here is simply say model and we're going to compile this.

102
00:07:33.010 --> 00:07:39.680
We're going to give a loss of categorical cross entropy categorical underscore cross entropy.

103
00:07:39.910 --> 00:07:42.750
Be careful not to make a typo here it's really easy to do so.

104
00:07:43.510 --> 00:07:47.700
And basically that means we're treating each vocabulary word as its own individual category.

105
00:07:49.200 --> 00:07:56.660
When you add the final optimizer leaves the atom optimizer and then for metrics will use accuracy.

106
00:07:58.110 --> 00:08:04.750
Then once that's done go ahead and give a summary of your model such a print out a little summary and

107
00:08:04.750 --> 00:08:06.370
then return your model.

108
00:08:06.370 --> 00:08:07.720
So we have a create model function.

109
00:08:07.720 --> 00:08:11.430
Let's go ahead to make sure we passed it incorrectly.

110
00:08:11.440 --> 00:08:18.040
So we'll say create model and we'll pass in our vocabulary size that we define earlier.

111
00:08:18.060 --> 00:08:23.970
Remember this vocabulary size we just define appear previously using the length of the word counts.

112
00:08:23.980 --> 00:08:26.250
So how many unique words are there.

113
00:08:26.300 --> 00:08:31.780
So 2 7 0 9 that particular case because of the way the embedding works with padding that we'll see later

114
00:08:31.780 --> 00:08:33.060
on for Pead sequences.

115
00:08:33.280 --> 00:08:36.320
We actually need to add one more space to essentially hold a zero.

116
00:08:36.340 --> 00:08:42.720
So we're going to see if we'll cover size plus one and then we're going to pasan our sequence length.

117
00:08:42.880 --> 00:08:44.890
And this is going to be our model.

118
00:08:45.130 --> 00:08:48.640
Go ahead and run this and make sure you get this summary output.

119
00:08:48.640 --> 00:08:52.310
If you got an error here it probably means you made a typo somewhere along here.

120
00:08:52.330 --> 00:08:55.690
In that case just go ahead and copy the function we created for you in the notebooks.

121
00:08:55.780 --> 00:08:58.480
But really easy to make a typo especially on this compile line.

122
00:08:58.900 --> 00:09:01.220
But if you saw the summary then you are good to go.

123
00:09:01.530 --> 00:09:01.990
OK.

124
00:09:02.290 --> 00:09:05.940
So coming up next we're going to generate new text.

125
00:09:06.010 --> 00:09:09.570
So the last thing to do here is actually train and fit our model.

126
00:09:09.850 --> 00:09:12.490
So what we do is we're going to save from pixel

127
00:09:15.590 --> 00:09:22.040
import dump Khama load that's going to allow us to actually save the file and then load it up later

128
00:09:22.040 --> 00:09:23.600
on.

129
00:09:23.600 --> 00:09:29.720
So we're going to say from pickle import dump load and then say model that fit are going to fit on our

130
00:09:29.750 --> 00:09:40.750
XT data in our y data here here we'll say batch size and then we'll say.

131
00:09:41.410 --> 00:09:43.220
The amount of epoxy we want to train for.

132
00:09:43.230 --> 00:09:47.910
I'll go ahead and just train for two epochs in this video and then say verbose is equal to one.

133
00:09:47.920 --> 00:09:49.510
So explain what's going on here.

134
00:09:50.540 --> 00:09:54.860
Now previously I was using the terms train test split it wasn't really a train split it was actually

135
00:09:54.860 --> 00:09:55.930
features labels split.

136
00:09:55.940 --> 00:10:00.900
So I apologize for using those words but essentially we have X and Y we don't truly have a train to

137
00:10:00.930 --> 00:10:01.850
split with testate.

138
00:10:01.880 --> 00:10:07.610
Because there's nothing to Testament's there's kind of no right answer as far as what text generated

139
00:10:07.610 --> 00:10:08.670
should look like.

140
00:10:08.810 --> 00:10:15.950
Instead we are really just texting or testing these features against the predicted label the batch size

141
00:10:16.040 --> 00:10:18.580
is how many sequences you want to passen at a time.

142
00:10:18.710 --> 00:10:20.600
You don't want to pasan everything at a time.

143
00:10:20.600 --> 00:10:23.200
Otherwise the neural network won't be able to handle that.

144
00:10:23.210 --> 00:10:25.400
So you only want to pass on a certain amount of sequences.

145
00:10:25.690 --> 00:10:29.810
128 was a value that I kind of just chose arbitrarily and worked well for me.

146
00:10:29.820 --> 00:10:34.640
E POCs to you POCs is not going to be nearly enough to generate any text that makes sense so you may

147
00:10:34.640 --> 00:10:37.870
just see a bunch of the most common words like the other repeated.

148
00:10:37.880 --> 00:10:42.820
But go ahead and treat it on a little bit of pocks just so you can tell that it worked or not and then

149
00:10:42.820 --> 00:10:45.260
verbose one is just going to be the output port.

150
00:10:45.260 --> 00:10:46.310
Let's go with this.

151
00:10:46.340 --> 00:10:48.120
Make sure it runs like this.

152
00:10:48.140 --> 00:10:49.750
Mostly just to check if it runs.

153
00:10:49.790 --> 00:10:55.600
You should probably be training for at least like 200 POCs to get something that's reasonable.

154
00:10:55.670 --> 00:10:58.610
So if you're able to run this and this should be good.

155
00:10:58.640 --> 00:11:01.190
Again our accuracy is absolutely horrible which makes sense.

156
00:11:01.190 --> 00:11:08.330
Right now it's probably just predicting the word to be the most common word but once done training especially

157
00:11:08.330 --> 00:11:15.050
if you trained for a really long time you're going to want to save this so you're going to want to take

158
00:11:15.050 --> 00:11:26.900
the model and save it and say something like my underscore Moby-Dick model thought age 5.

159
00:11:27.060 --> 00:11:32.390
So that saves the model but the other thing we're actually going to say is the tokenizer as well.

160
00:11:32.550 --> 00:11:36.910
And in order to do that we're going to use that pixel functionality.

161
00:11:36.920 --> 00:11:39.470
So we're going to say this is a pickle file.

162
00:11:39.470 --> 00:11:43.330
Now we're going to save in our tokenizer memory that tokenizer has information across the entire vocabulary

163
00:11:43.340 --> 00:11:44.200
like word counts.

164
00:11:44.210 --> 00:11:49.750
We'll need that later on though we don't get into tokenize text again so I'll say dump tokenizer.

165
00:11:49.940 --> 00:11:58.660
And this is where you would make up a name for it tokenizer like my simple tokenizer and then say WB

166
00:11:58.660 --> 00:11:59.060
here.

167
00:11:59.080 --> 00:12:04.240
So what this does is it will automatically create a file for you called my simple tokenizer and then

168
00:12:04.240 --> 00:12:09.340
write binary to it in order to dump this tokenizer object to that file.

169
00:12:09.340 --> 00:12:14.980
Notice there's no extension there saying go ahead and run that as well and save it and now you successfully

170
00:12:15.460 --> 00:12:22.790
train your model and then you save it as well as saved your tokenizer.

171
00:12:22.910 --> 00:12:25.350
Coming up next we're going to show you how to generate new text.

172
00:12:25.370 --> 00:12:25.890
We'll see it there.

