WEBVTT
1
00:00:01.080 --> 00:00:04.050
Hello and welcome back to the course on artificial intelligence.

2
00:00:04.050 --> 00:00:09.810
Today we continue our journey into the world every three see and we're talking about the synchronous

3
00:00:09.870 --> 00:00:13.530
side of aither C so there we have our abbreviation of synchronous advantage.

4
00:00:13.550 --> 00:00:14.430
Active critic.

5
00:00:14.430 --> 00:00:19.040
And today we're going to find out what a synchronous here stands for what it means.

6
00:00:19.080 --> 00:00:20.990
And let's go back a step.

7
00:00:21.000 --> 00:00:27.270
Let's look at what we started this whole course for enforcement learning what it's all about that the

8
00:00:27.270 --> 00:00:29.050
Asian is in a certain state.

9
00:00:29.340 --> 00:00:30.690
They observe the state.

10
00:00:30.730 --> 00:00:37.080
They make certain decisions they take actions in that state and then the state is changed so they get

11
00:00:37.080 --> 00:00:39.010
into a new state plus they get reward.

12
00:00:39.150 --> 00:00:46.830
So the reward for taking that action or some sort of reward which could be a penalty as well and they

13
00:00:46.830 --> 00:00:47.780
end up in a new state.

14
00:00:47.790 --> 00:00:50.180
And based on that now they take another action again.

15
00:00:50.250 --> 00:00:56.280
They get a reward and end up in a new state and they take another action and so on and so that is the

16
00:00:56.280 --> 00:00:59.670
basis behind all of reinforcement learning.

17
00:00:59.700 --> 00:01:06.030
And that's what we've been using in learning in deep learning and deep convolutional keep learning and

18
00:01:06.030 --> 00:01:10.660
that has allowed our agents to beat gradually more complex and more complex environments.

19
00:01:10.770 --> 00:01:18.310
But now we're going to introduce a even better concept and even for take this even further level.

20
00:01:18.720 --> 00:01:25.530
What A-3 see introduces through this and synchronous element is instead of having one agent attack the

21
00:01:25.520 --> 00:01:26.730
environment.

22
00:01:26.790 --> 00:01:34.140
They have three agents or whatever number of agents or several agents attacking the same environment.

23
00:01:34.290 --> 00:01:39.360
And the key here is that's why it's called the synchronises because they're initialized differently

24
00:01:39.360 --> 00:01:40.920
so their star inputs are different.

25
00:01:40.920 --> 00:01:46.520
So for instance as you'll see from practical sources you set a random seed and you set it differently

26
00:01:46.530 --> 00:01:47.980
for each of the agents.

27
00:01:48.030 --> 00:01:51.150
And that way because their starting points are different.

28
00:01:51.270 --> 00:01:55.260
They're going to first go through environments in different ways and then they're going to explore in

29
00:01:55.260 --> 00:01:58.620
different ways and then in the next iterations are also going to explore in different ways.

30
00:01:58.620 --> 00:02:01.190
And so for instance we have three agents.

31
00:02:01.300 --> 00:02:06.390
You're all of a sudden you're getting triple the amount of experience instead of just one age and going

32
00:02:06.390 --> 00:02:12.480
through and exploring the environment and trying to understand how to operate it in that environment.

33
00:02:12.570 --> 00:02:18.780
You now have three or however many of them going through that and getting this experience and so there

34
00:02:18.930 --> 00:02:25.140
so that each one of them is learning for this bigger experience and apart from being just giving a broader

35
00:02:25.140 --> 00:02:31.300
range of experience it also reduces the chances of one agent getting stuck in a local maximum.

36
00:02:31.320 --> 00:02:38.040
So for instance if one agent finds a way to beat the environment which is not the most optimal because

37
00:02:38.130 --> 00:02:43.470
if it deviates a left to the right from that solution that it found it always gets like gets more penalized

38
00:02:43.470 --> 00:02:45.710
it might get stuck in a local maximum.

39
00:02:45.710 --> 00:02:49.530
It might just keep doing that thinking that that's the optimal solution where where it's actually not.

40
00:02:49.680 --> 00:02:58.830
Well the likelihood of several agents getting stuck in that same local maximum is decreases over decreases

41
00:02:58.830 --> 00:03:04.440
with the number of agents so the probability of one agent getting stuck in a certain local maximum might

42
00:03:04.440 --> 00:03:07.990
be high but Or might might be a certain value.

43
00:03:08.130 --> 00:03:11.730
But the probability when you have three of them of all three of them getting stuck in that local maximum

44
00:03:11.730 --> 00:03:13.020
is much lower.

45
00:03:13.020 --> 00:03:18.630
And as long as they share experience between each other they can help each other out so if one of them

46
00:03:18.630 --> 00:03:23.370
gets stuck for instance it's in a local maximum and just simply think that that's the best and that's

47
00:03:23.370 --> 00:03:25.700
the best that's the best solution all the time and keeps doing that.

48
00:03:25.830 --> 00:03:30.740
Well as long as it interacts with the other agents So let's say this guy gets stuck in a calm action

49
00:03:30.770 --> 00:03:35.710
as long as interacts with other agents through the way we build our whole algorithm through cellular

50
00:03:35.710 --> 00:03:37.620
and they will help him out.

51
00:03:37.620 --> 00:03:42.960
They will give him knowledge that actually you know hey you should explore this or he will be likely

52
00:03:43.020 --> 00:03:44.630
more likely to get out of that.

53
00:03:44.760 --> 00:03:50.370
And also overall the environment will know that hey even though this a great maximum these other ages

54
00:03:50.370 --> 00:03:55.230
have seen better options and we should keep exploring because it looks like there are better options.

55
00:03:55.230 --> 00:04:00.750
So in a in a very short kind of rough intuitive understanding that's that those are some of the advantages

56
00:04:00.750 --> 00:04:05.970
of having these are synchronous agents for so you have more experience to choose from and to learn from.

57
00:04:06.210 --> 00:04:12.750
You could get to the solution faster and generally speaking if there's a lesser chance of getting stuck

58
00:04:13.080 --> 00:04:16.640
in a CRN local maximum.

59
00:04:16.680 --> 00:04:18.710
So let's see how this all plays out.

60
00:04:18.720 --> 00:04:24.300
In this model that we've built so far so is remember this is what we've gotten so far through the actual

61
00:04:24.300 --> 00:04:29.730
critic and this is like we're all teasing this is so far as you remember from the first to turtle we

62
00:04:29.730 --> 00:04:33.030
did introduce this you know we had this already even in deep ocean.

63
00:04:33.030 --> 00:04:38.340
Q learning's So we just named the X now but now we've introduced critic but so far doesn't really make

64
00:04:38.340 --> 00:04:43.620
sense what's the point of having this critic and measuring the value of the state or predicting the

65
00:04:43.620 --> 00:04:48.320
value of a stage using the same neural networks or the same approach.

66
00:04:48.510 --> 00:04:52.350
But now is this is this is the partner's going to start making more sense.

67
00:04:52.350 --> 00:04:57.750
What we're going to do is we're going to replicate this because now we have multiple agents So if multiple

68
00:04:57.750 --> 00:04:59.410
agents this is this is what it looks like.

69
00:04:59.410 --> 00:05:07.380
So the first way of imagining it is now we have these three days well remember what we said about them

70
00:05:07.380 --> 00:05:09.210
sharing their experience between each other.

71
00:05:09.210 --> 00:05:12.270
So this is actually like right now they're all independent.

72
00:05:12.260 --> 00:05:15.420
You have one playing the game and other than playing the game another play in the game.

73
00:05:15.450 --> 00:05:20.500
It's like it's like launching your agent on three different computers you put three different computers

74
00:05:20.500 --> 00:05:23.030
next to each other and you launch them and you know that's great.

75
00:05:23.050 --> 00:05:29.100
Like indeed you like you'll get you'll get more experience you'll get more variety especially if they're

76
00:05:29.100 --> 00:05:29.760
initialized.

77
00:05:29.790 --> 00:05:33.600
So we can assume from here that they're ill initial always initialized before even though we have the

78
00:05:33.600 --> 00:05:34.770
same picture here.

79
00:05:34.870 --> 00:05:39.900
Are we going to know that they're actually initialized differently so it's not going to be like identical

80
00:05:39.900 --> 00:05:43.460
training identical learning from this game.

81
00:05:43.890 --> 00:05:47.700
And so even if you like you put three computer side by side and you launch them yes you're going to

82
00:05:47.700 --> 00:05:55.560
have more experience because you're going to have three agents playing and also you're going to have

83
00:05:55.680 --> 00:05:58.600
a bigger variety of possible solutions.

84
00:05:58.620 --> 00:06:00.120
So that's true.

85
00:06:00.120 --> 00:06:03.840
But the problem is that they're not sharing our experience among each other or not learning from each

86
00:06:03.840 --> 00:06:04.120
other.

87
00:06:04.220 --> 00:06:06.840
So they don't have that synergy.

88
00:06:06.840 --> 00:06:11.670
They don't have the advantage or the extra power that they would get if they were compering you know

89
00:06:11.670 --> 00:06:17.930
like how if you have if you have a team of people they work better together than each one of them separately.

90
00:06:17.920 --> 00:06:20.650
So like in a team here you got one plus one plus one.

91
00:06:20.730 --> 00:06:25.200
It's three but in a team one plus one to spawn and not three is like the three because they leverage

92
00:06:25.200 --> 00:06:29.220
each other's strengths and mitigate each other's weaknesses and same thing here.

93
00:06:29.220 --> 00:06:34.080
So if you put these two computer side by side yes you'll you'll have more experience memory and possibly

94
00:06:34.150 --> 00:06:35.580
someone will get a better solution.

95
00:06:35.580 --> 00:06:39.710
Another one that's great but it'll be even better if they start sharing that experience.

96
00:06:39.780 --> 00:06:41.120
And how do they do that.

97
00:06:41.130 --> 00:06:47.820
Well it's through this Wii that we calculate it so this Wii value that's the output of our network is

98
00:06:47.820 --> 00:06:49.550
actually like that.

99
00:06:49.560 --> 00:06:54.960
So they have this same every so every time.

100
00:06:54.960 --> 00:06:58.050
All these agents they're contributing to the same critic.

101
00:06:58.050 --> 00:07:04.290
They don't have separate critics they have a common critic and that's the key of how the actor critic

102
00:07:04.290 --> 00:07:06.270
ties in with their synchronous.

103
00:07:06.270 --> 00:07:09.850
So there's one critic that's watching us as they get experience.

104
00:07:09.870 --> 00:07:12.420
So how do we calculate the Wii.

105
00:07:12.570 --> 00:07:13.920
We've got to get the Wii through.

106
00:07:14.220 --> 00:07:20.190
As you remember we can get TV through the values that we get so the rewards that we get through the

107
00:07:20.190 --> 00:07:20.750
environment.

108
00:07:20.760 --> 00:07:28.310
And so as the agents explore their environment they are calculi they're predicting the Wii.

109
00:07:28.320 --> 00:07:30.710
Plus they have the Wii that they can calculate.

110
00:07:30.720 --> 00:07:35.010
This is this is all all ties back into what we've already discussed in the previous sections of the

111
00:07:35.010 --> 00:07:35.730
scores.

112
00:07:35.850 --> 00:07:42.870
So they already have a Wii that they that they can predict like expect through the rewards that they

113
00:07:42.870 --> 00:07:48.700
know that exist in this maze and that they've already explored and as they explore them of course that

114
00:07:48.750 --> 00:07:49.770
that value can change.

115
00:07:49.890 --> 00:07:55.410
But also they have the Wii that this is the output of the neural network so as they're going through

116
00:07:55.410 --> 00:08:01.530
this they're going to be adjusting their neural networks in order to better match that expected.

117
00:08:01.530 --> 00:08:10.080
So basically this is shared the critic part is shared between the agents and that is how they share

118
00:08:10.080 --> 00:08:15.490
the information between each other that's how they are able to kind of see what is going on in the environment

119
00:08:15.490 --> 00:08:20.890
shared with each other and then use that as we'll see further in the next part in advantage.

120
00:08:20.930 --> 00:08:25.450
So use that in order to optimize how they're behaving the environment.

121
00:08:25.710 --> 00:08:27.960
And the other thing to note here is.

122
00:08:28.080 --> 00:08:29.510
So this was a through C.

123
00:08:29.520 --> 00:08:33.150
This is like the core of A-3 see up to here.

124
00:08:33.150 --> 00:08:38.610
This is a type of version of 08:30 But there's an actually even better implementation of this.

125
00:08:38.610 --> 00:08:45.450
A through C which you'll actually hear I'd love to talk about in the one of the first tutorials and

126
00:08:45.450 --> 00:08:52.680
the practical side of things and what he'll be talking about is how the creator of Pi torche actually

127
00:08:52.980 --> 00:08:58.530
made an adjustment to one of the codes that was shared and get hub where he took all of these as you

128
00:08:58.530 --> 00:09:03.420
can see right now they have separate neural networks and they showed the Wii that adjustment that was

129
00:09:03.420 --> 00:09:09.000
made was actually to take all of these neural networks and put them into one take them and put them

130
00:09:09.000 --> 00:09:09.300
together.

131
00:09:09.300 --> 00:09:15.100
So ultimately there's only one neural network here shared among the agents.

132
00:09:15.120 --> 00:09:21.180
So before they had each one of them had one neural network which were shared for the actor and for the

133
00:09:21.180 --> 00:09:25.790
critic one neural network Shelfer actual for the critic one neural network share for accuracy.

134
00:09:25.800 --> 00:09:31.730
Now they all have one neural network which is shared for the actor or critic actual critic x or critic.

135
00:09:31.980 --> 00:09:35.130
And then the critic is here in common.

136
00:09:35.310 --> 00:09:36.690
So let's see let's.

137
00:09:36.690 --> 00:09:39.840
Let's move these pictures to the left here so make some space.

138
00:09:40.100 --> 00:09:47.430
And this is basically the architecture or the structure that we're going to be using in the practical

139
00:09:47.430 --> 00:09:48.250
tutorials.

140
00:09:48.300 --> 00:09:55.020
I know that like this may sound a bit overwhelming at this stage but we've got one more to talk about

141
00:09:55.020 --> 00:09:59.370
which is advantage and there we'll see it better in action.

142
00:09:59.370 --> 00:10:02.780
How does going so we'll talk about the intuition in action.

143
00:10:02.870 --> 00:10:05.680
But generally speaking this is this is what it is.

144
00:10:05.700 --> 00:10:10.640
This is there's one network which each of the agents use or they share.

145
00:10:10.640 --> 00:10:15.820
Basically what that means is that they share the weights the weights of the network are shared between

146
00:10:15.840 --> 00:10:19.920
ages and when they update it they update the whole network not just their own network.

147
00:10:20.480 --> 00:10:26.270
And then they have outputs they have like these actions for each agent and then they have the critic

148
00:10:26.270 --> 00:10:27.710
that is shared which is going to be monitored.

149
00:10:27.700 --> 00:10:34.280
So I know all of this is kind of like like there's a lot of stuff right now but hopefully it's slowly

150
00:10:34.850 --> 00:10:35.900
coming together at least.

151
00:10:35.900 --> 00:10:39.660
The main takeaway from here is that the critic because it's shared.

152
00:10:39.670 --> 00:10:47.810
That's how the agents are able to make sure that they are cooperating together in order to get the result

153
00:10:47.810 --> 00:10:48.660
much faster.

154
00:10:48.860 --> 00:10:52.690
And then in the next tutorial we'll see even further how all of this adds up.

155
00:10:52.700 --> 00:10:53.650
All of this comes together.

156
00:10:53.900 --> 00:11:00.920
And for now there's like I would like to recommend or we would like to recommend you a an additional

157
00:11:00.920 --> 00:11:01.210
reading.

158
00:11:01.210 --> 00:11:06.780
So this is a blog by Jaromir Jansch.

159
00:11:06.860 --> 00:11:11.570
It's called Let's make an A3 see implantations is actually two parts implementation and theory.

160
00:11:11.820 --> 00:11:19.010
There's the link and it's very similar to what Adlon will be implementing in the practical side of the

161
00:11:19.010 --> 00:11:25.880
tutorial so it's not specifically for this tutorial just not just for Sutro but it's for this whole

162
00:11:25.880 --> 00:11:27.200
section.

163
00:11:27.200 --> 00:11:30.900
Encouragement there some additional information some additional insights there.

164
00:11:31.040 --> 00:11:33.260
And so that's that's why we're bringing it up here.

165
00:11:33.320 --> 00:11:38.030
But nevertheless in the next tutorial we're going to start pulling all of this together.

166
00:11:38.030 --> 00:11:39.040
Everything we've discussed.

167
00:11:39.200 --> 00:11:40.590
And I look forward to seeing you next time.

168
00:11:40.590 --> 00:11:42.200
And until then enjoy I.
