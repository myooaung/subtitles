WEBVTT
1
00:00:00.270 --> 00:00:06.230
Hello and welcome to this pipe and to Torro now we have to define the five variables of this function

2
00:00:06.240 --> 00:00:09.560
that is the three convolutions and the temporal connections.

3
00:00:09.810 --> 00:00:11.100
So let's start with the first one.

4
00:00:11.250 --> 00:00:18.690
Convolutional one applies convolution to the input images so that's the original images and now you're

5
00:00:18.690 --> 00:00:22.710
going to see how everything will become so simple to create this convolution.

6
00:00:22.770 --> 00:00:29.070
Well what we have to do is actually create a subject of some specific class and this class is taken

7
00:00:29.070 --> 00:00:38.800
from and then and then the classes come to the to because we're working with 2D images and now as you

8
00:00:38.800 --> 00:00:40.930
can see we need to put several arguments.

9
00:00:40.930 --> 00:00:43.140
First one is in channels.

10
00:00:43.190 --> 00:00:45.910
Let's put it in channels.

11
00:00:45.970 --> 00:00:49.210
The second one is out channels.

12
00:00:49.330 --> 00:00:57.070
The third one is Kerno size and the rest of them are stride padding the dilation groups and bias.

13
00:00:57.130 --> 00:00:59.120
And we have different values for all these ones.

14
00:00:59.260 --> 00:01:02.290
So we're not going to but them we're going to keep the default values.

15
00:01:02.470 --> 00:01:08.500
But what's important is these three arguments in channels and channels and kernel size and so do I guess

16
00:01:08.500 --> 00:01:09.740
what they correspond to.

17
00:01:09.880 --> 00:01:15.730
Well very simply in general correspond to the input of the convolution and all channels correspond to

18
00:01:15.730 --> 00:01:17.900
the output of the conclusion.

19
00:01:17.920 --> 00:01:19.940
So what is it going to be.

20
00:01:20.110 --> 00:01:24.460
Well very simply that's going to be the number of channels in our images.

21
00:01:24.610 --> 00:01:30.460
And actually we are going to work with black and white images because basically we don't colors to recognize

22
00:01:30.460 --> 00:01:31.360
the monsters.

23
00:01:31.450 --> 00:01:35.310
The AI is totally capable of recognizing the monsters in black and white.

24
00:01:35.440 --> 00:01:39.200
So we don't see the colors at all just recognize them by their shape.

25
00:01:39.250 --> 00:01:44.380
Therefore we're going to use one channel so one channel is when you have black and white images and

26
00:01:44.380 --> 00:01:46.820
three channels is when you have called images.

27
00:01:46.930 --> 00:01:51.320
And therefore since we're working with black and white images in channels it's going to be equal to

28
00:01:51.340 --> 00:01:59.730
one then our channels so our channels it's going to be equal to the images you want to have in the convolutional

29
00:01:59.730 --> 00:02:02.970
there which is the output of this constitution one.

30
00:02:02.970 --> 00:02:08.820
And so basically this is equal to the number of features you want to the text in your original images

31
00:02:09.240 --> 00:02:15.040
because who will create one image per feature we want to detect because basically you know how it works.

32
00:02:15.120 --> 00:02:21.390
We applied one feature detector to the input image to detect a specific feature in the input image and

33
00:02:21.390 --> 00:02:26.450
therefore the number of output images here is the number of features we want to detect.

34
00:02:26.460 --> 00:02:30.110
So now the question is how many features do we want to detect.

35
00:02:30.240 --> 00:02:38.730
Well a common practice is to start with 32 feature detectors and so that will lead us to 32 percent

36
00:02:38.820 --> 00:02:46.170
images in this first convolutional layer so the input is one black and white image a real image and

37
00:02:46.170 --> 00:02:53.010
the output in the first convolutional there is 32 processed images and by processed I mean of course

38
00:02:53.380 --> 00:02:59.750
that the conclusion was applied to the input image to get 32 new images with detected features.

39
00:03:00.240 --> 00:03:06.990
And then we need to specify a kernel size which is nothing else than the dimensions of the square that

40
00:03:07.080 --> 00:03:09.450
will go through the original image.

41
00:03:09.600 --> 00:03:15.350
And in common practice we use either to buy two or three wide three or five by five.

42
00:03:15.660 --> 00:03:22.170
And for the first one we're going to use a five by five feature detector that is a feature detector

43
00:03:22.170 --> 00:03:24.580
that will have five by five 10 engines.

44
00:03:24.810 --> 00:03:29.290
And then we will reduce the size of this kernel for the next convolutional layers.

45
00:03:29.340 --> 00:03:32.320
And speaking of it this is exactly what we're going to do now.

46
00:03:32.390 --> 00:03:40.860
We are going to copy this to define the second convolution and therefore I'm basing that here and now

47
00:03:40.890 --> 00:03:43.370
it's very funny and very easy it's like a domino.

48
00:03:43.500 --> 00:03:49.260
The input channel of the second convolutional layer is the output channel of the first convolutional

49
00:03:49.260 --> 00:03:49.810
there.

50
00:03:49.950 --> 00:03:55.300
So this number of output 32 Here is the same number of inputs 32 here.

51
00:03:55.420 --> 00:04:01.290
And that's because we have 32 images in the input convolutional layer of the second convolution.

52
00:04:01.440 --> 00:04:09.830
And so the second convolution is applied to this second convolutional layer to return a third convolutional

53
00:04:09.830 --> 00:04:10.340
layer.

54
00:04:10.530 --> 00:04:13.280
And so now the question is how many new images do we want.

55
00:04:13.410 --> 00:04:19.820
Well same that create 32 new images 32 is actually a very common number in convolutional neural networks

56
00:04:19.830 --> 00:04:23.710
if you look at the architectures you will find 32 in many of them.

57
00:04:24.030 --> 00:04:29.910
And then for the kernel size Well we need to reduce the kernel size that is the dimensions of our feature

58
00:04:29.910 --> 00:04:30.610
detector.

59
00:04:30.780 --> 00:04:37.680
And so now we're going to go from five to four or even three and then we'll go even smaller.

60
00:04:37.920 --> 00:04:40.790
All right so our second convolution is ready.

61
00:04:40.830 --> 00:04:43.830
It takes as inputs 32 process images.

62
00:04:43.890 --> 00:04:51.130
Each one in a first feature of the original input image and it creates 32 new images.

63
00:04:51.270 --> 00:04:54.990
Thanks to this reduced dimensions of the feature detector.

64
00:04:55.120 --> 00:04:57.250
And so now let's push this even more.

65
00:04:57.300 --> 00:05:05.450
So I'm copying this and pasting that here to create a third convolution to detect some features.

66
00:05:05.470 --> 00:05:08.010
And so now that's the same the input channels.

67
00:05:08.010 --> 00:05:13.800
Here is the number of input images at the left of deconvolution connection and that is the number of

68
00:05:13.800 --> 00:05:17.740
precess images that went to the right of the previous convolution connections.

69
00:05:17.740 --> 00:05:18.710
That's 32.

70
00:05:18.750 --> 00:05:20.030
Therefore we keep sorry to hear.

71
00:05:20.030 --> 00:05:20.990
That's perfect.

72
00:05:21.220 --> 00:05:25.220
And now the question is again how many new images do we want to detect.

73
00:05:25.310 --> 00:05:31.130
We are going to take now 64 and therefore 64 outputs precess images.

74
00:05:31.260 --> 00:05:36.800
And of course now we take a smaller kernel size and we're going to take two.

75
00:05:36.800 --> 00:05:42.660
And so that's a very classic architecture of a convolutional there and it's very efficient to have a

76
00:05:42.660 --> 00:05:45.840
high level of feature detection inside images.

77
00:05:46.200 --> 00:05:53.010
All right and so now that we have our three convolutional there's are to our three convolution connections

78
00:05:53.010 --> 00:05:53.510
here.

79
00:05:53.640 --> 00:05:59.970
Well now it's time to get our toothful connections that I remind we'll take this huge vector that we

80
00:05:59.970 --> 00:06:07.710
obtain after flattening all the 64 times 32 times 32 again images that we got from all these convolutions

81
00:06:08.160 --> 00:06:13.920
so we flattened all the pixels of these images and we can one huge vector that will become the input

82
00:06:14.190 --> 00:06:16.590
of a new fully connected neural network.

83
00:06:16.860 --> 00:06:22.230
And so that's when we have to make these connections between first this huge vector and a hidden layer

84
00:06:22.470 --> 00:06:27.710
and then a second full connection between the hidden layer and the output they're composed of the output

85
00:06:27.730 --> 00:06:28.270
neurons.

86
00:06:28.320 --> 00:06:31.940
Each one corresponding to a cube value of the possible actions.

87
00:06:31.980 --> 00:06:33.960
So let's make these two connections.

88
00:06:33.960 --> 00:06:35.220
You know how to do that.

89
00:06:35.220 --> 00:06:37.550
That's exactly what we did for the self-driving car.

90
00:06:37.560 --> 00:06:38.800
So let's do that again.

91
00:06:39.000 --> 00:06:46.890
Well first we take our Maggio then we take the Lynnie our class because again the connection we create

92
00:06:46.920 --> 00:06:49.060
is an object of the ruling class.

93
00:06:49.260 --> 00:06:50.550
And then in parenthesis.

94
00:06:50.670 --> 00:06:58.620
Well that's the same for put the input features that is the number of them then the output features.

95
00:06:58.850 --> 00:07:03.110
And so the input features for the first full connection what is it going to be.

96
00:07:03.340 --> 00:07:10.300
Well that's going to be equal to the number of pixels there are in this huge vector change after flattening

97
00:07:10.480 --> 00:07:13.830
all the process images after the three convolutions.

98
00:07:13.830 --> 00:07:15.110
And so what does this number.

99
00:07:15.220 --> 00:07:17.350
Well actually there is a trick here.

100
00:07:17.380 --> 00:07:19.620
This number is actually hard to get.

101
00:07:19.660 --> 00:07:22.770
We actually need to make a function to compute that number.

102
00:07:22.960 --> 00:07:25.570
We don't have a variable that will get us this number.

103
00:07:25.570 --> 00:07:29.040
We have to compute it and therefore what we're going to do now.

104
00:07:29.150 --> 00:07:34.780
And now it's very important to understand the mindset of programming that we must have and trying to

105
00:07:35.200 --> 00:07:40.930
bring to you the mindset that is what you must be thinking right now to do to overcome this obstacle

106
00:07:41.110 --> 00:07:45.690
because the first time you might say hey I don't have this number of neurons in the Flaten vector.

107
00:07:45.730 --> 00:07:46.650
What should I do.

108
00:07:46.660 --> 00:07:47.600
I'm stuck here.

109
00:07:47.800 --> 00:07:55.720
Well no actually because what you can do now is simply input any name here that will represent the number

110
00:07:55.720 --> 00:08:01.690
of neurons so uncommon that number neurons number of neurons and then we will simply make a function

111
00:08:01.690 --> 00:08:05.130
that will return and this number of neurons variable.

112
00:08:05.180 --> 00:08:07.100
This number of pixels we're looking for.

113
00:08:07.330 --> 00:08:12.760
So we can totally do that we can totally put this very vocal of course will get a warning because it

114
00:08:12.760 --> 00:08:17.090
doesn't exist yet but we will create it afterwards with a function.

115
00:08:17.350 --> 00:08:21.140
And we are totally allowed to do that even if the function comes afterwards.

116
00:08:21.170 --> 00:08:25.970
That's a typical programming thinking you must have when you get that kind of obstacle.

117
00:08:26.020 --> 00:08:29.710
Well you can make a function to get what you're missing.

118
00:08:29.740 --> 00:08:32.640
All right and then our features and our future.

119
00:08:32.650 --> 00:08:37.830
That's the number of neurons in a hidden layer and that this time is up to you.

120
00:08:37.870 --> 00:08:41.230
That depends on the architecture of the new network you want to create.

121
00:08:41.230 --> 00:08:44.230
And so a good number would be not a small number.

122
00:08:44.230 --> 00:08:46.960
So for example 40 neurons might be fine.

123
00:08:46.960 --> 00:08:48.660
We can try to increase that.

124
00:08:48.790 --> 00:08:51.280
If the training is not too slow you can try to increase that.

125
00:08:51.340 --> 00:08:56.750
Maybe that will improve the predictions but let's start with 40 maybe we'll increase that afterwards.

126
00:08:57.130 --> 00:09:04.690
All right that's it for the first full connection then we'll copy this paste that here for the second

127
00:09:04.710 --> 00:09:09.300
full connection that is the connection between the hidden layer and the output layer.

128
00:09:09.370 --> 00:09:15.720
And so the features here becomes the out features of the previous layer and that is 40.

129
00:09:15.790 --> 00:09:18.020
So here we can put 40.

130
00:09:18.190 --> 00:09:20.500
That's of course the number of neurons in a layer.

131
00:09:20.860 --> 00:09:27.280
And our future is going to be equal to the number of output neurons there should be no neural network.

132
00:09:27.340 --> 00:09:33.360
And since each output neuron corresponds to one new value and one Cuvee and response to one action while

133
00:09:33.370 --> 00:09:38.440
the number of output neurons here is of course the number of actions and we have one variable for this

134
00:09:38.740 --> 00:09:47.320
which is number actions and therefore here input number actions and there we go congratulations.

135
00:09:47.390 --> 00:09:53.850
We did find the architecture of our neural network our neural network is composed of three convolutional

136
00:09:53.850 --> 00:09:55.960
layers and one hidden layer.

137
00:09:56.130 --> 00:10:03.020
All this in one big CNN and this CNN will detect features in the game so that the AI will know where

138
00:10:03.020 --> 00:10:06.550
it has to do where it has to go and where it needs to shoot.

139
00:10:06.900 --> 00:10:08.240
So then we go for this step.

140
00:10:08.340 --> 00:10:10.720
That's a first very important step done.

141
00:10:10.720 --> 00:10:16.020
Now we're going to move onto the next step which is of course to get this number of neurons that is

142
00:10:16.020 --> 00:10:17.010
still missing.

143
00:10:17.010 --> 00:10:21.930
That's actually why we have the warning here and the phone number neurons but no worries.

144
00:10:21.960 --> 00:10:26.970
Now we will make a function that will return the number of neurons in this huge vector and we will put

145
00:10:26.970 --> 00:10:30.210
that number in a variable that will call a number of neurons.

146
00:10:30.270 --> 00:10:32.070
So let's do this in the next tutorial.

147
00:10:32.070 --> 00:10:33.130
That's our next step.

148
00:10:33.180 --> 00:10:34.790
And until then enjoy AI.
