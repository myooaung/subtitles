1
00:00:00,330 --> 00:00:05,940
Hello and welcome to this tutorial this special toy is going to be super exciting because we are getting

2
00:00:05,940 --> 00:00:08,540
closer to the A.S.C. algorithm.

3
00:00:08,700 --> 00:00:14,190
You're going to see that what we're about to implement and that is called eligibility trace or Sarsour

4
00:00:14,520 --> 00:00:20,250
is actually an algorithm of the synchronous active critics agents algorithms that we cannot consider

5
00:00:20,250 --> 00:00:20,380
it.

6
00:00:20,380 --> 00:00:25,290
And we see because we're still going to have one agent but still you're going to see that what we're

7
00:00:25,290 --> 00:00:32,010
about to implement is actually taken from the following paper which is this paper as Synchronoss method

8
00:00:32,130 --> 00:00:38,280
for deep reinforcement learning and it is in this paper that you will find A-3 algorithms that we will

9
00:00:38,280 --> 00:00:40,840
implement as the final bonus of this course.

10
00:00:41,010 --> 00:00:47,580
But as I said we are getting closer to it because the model that will implement right now is actually

11
00:00:48,420 --> 00:00:55,620
this one the synchronous and secularised that's the one that's almost the A3 C which is doing after

12
00:00:55,620 --> 00:01:03,490
that but with one agent and the powerful thing about this is this and step Cunanan we're going to learn

13
00:01:03,490 --> 00:01:09,520
to accumulate rewards and learn the cumulative target on end steps instead of one step like Priestley.

14
00:01:09,690 --> 00:01:15,250
And that's what will make the training much more performance and therefore are much more powerful.

15
00:01:15,300 --> 00:01:21,080
So we actually have the pseudo code for this algorithm it's this algorithm as to right here.

16
00:01:21,180 --> 00:01:23,230
So let's click on it and there we go.

17
00:01:23,250 --> 00:01:25,740
That's the algorithm we about to implement.

18
00:01:25,890 --> 00:01:30,960
But remember with only one agent the difference is that here they take an action.

19
00:01:30,990 --> 00:01:37,380
80 according to the president Greely policy based on the q values for the current state and the action

20
00:01:37,380 --> 00:01:38,020
played.

21
00:01:38,190 --> 00:01:42,050
But in our case we didn't implement an excellent green policy.

22
00:01:42,090 --> 00:01:43,770
We implemented a soft Max.

23
00:01:43,890 --> 00:01:46,110
But the rest is the same as you can see.

24
00:01:46,110 --> 00:01:50,210
We are going to compute the cumulative we worked on and steps actually 10 steps.

25
00:01:50,220 --> 00:01:52,340
Remember the steps is equal to 10.

26
00:01:52,440 --> 00:01:58,170
And so we will implement this line of code in our algorithm that we're about to implement right now.

27
00:01:58,170 --> 00:01:59,010
We're going to get this.

28
00:01:59,010 --> 00:02:02,150
And mostly we are going to implement this as well.

29
00:02:02,160 --> 00:02:07,590
You'll see that we will get the maximum of the q values for the current state and the current action

30
00:02:07,590 --> 00:02:08,820
in this theta.

31
00:02:08,820 --> 00:02:11,150
Here is just a target parameter.

32
00:02:11,430 --> 00:02:15,350
So let's do this let's attack this algorithm.

33
00:02:15,360 --> 00:02:18,730
This one is called the synchronous and stick learning.

34
00:02:18,900 --> 00:02:24,490
But we don't have the right to say synchronous as far as we're concerned because we only have one engine.

35
00:02:24,670 --> 00:02:29,830
But therefore we can call it and learning eligibility trace or even Sarsour.

36
00:02:30,270 --> 00:02:31,780
All right so let's do this.

37
00:02:31,800 --> 00:02:33,080
It's going to be pretty fun.

38
00:02:33,090 --> 00:02:37,460
We can basically follow the code here and that's what we're going to do.

39
00:02:37,460 --> 00:02:43,980
And so as you can see a parameter that we'll need is again up the parameter that is the decay parameter

40
00:02:44,280 --> 00:02:50,820
and therefore we will start by introducing a variable for this Gahanna parameter and choosing them.

41
00:02:50,820 --> 00:02:51,840
So let's do this.

42
00:02:51,900 --> 00:02:57,040
We actually don't need a classroom tremendous We can simply implement this with a function because you

43
00:02:57,040 --> 00:03:01,830
know we don't really need to create objects for this to be to trace model a function will be enough

44
00:03:02,190 --> 00:03:08,500
because basically what we want to do is to return the inputs and the target so that later when training

45
00:03:08,500 --> 00:03:14,460
the AI we are ready to minimize the distance between the predictions and the target and to get the predictions

46
00:03:14,470 --> 00:03:19,950
we need the inputs because we are going to apply our brain on the input to get the output signals that

47
00:03:19,950 --> 00:03:21,300
will be our predictions.

48
00:03:21,300 --> 00:03:26,730
And then once we have our predictions and our targets we will be ready to train the AI by trying to

49
00:03:26,730 --> 00:03:30,680
minimize this great distance between the predictions and the toilets.

50
00:03:30,690 --> 00:03:32,780
So that's the whole point of doing this right now.

51
00:03:32,880 --> 00:03:38,130
We're implementing this function to be able to return these inputs in the Soviets so that we can be

52
00:03:38,130 --> 00:03:43,010
ready for the training to minimize the squared distance predictions manage toilets.

53
00:03:43,020 --> 00:03:46,400
All right so let's do this as we said we want to implement functions.

54
00:03:46,410 --> 00:03:47,910
We start with this.

55
00:03:47,940 --> 00:03:52,680
This we're going to call it eligibility underscored trace.

56
00:03:52,860 --> 00:03:54,210
You can also call it Sarsour.

57
00:03:54,240 --> 00:04:00,660
You can also call it step to ring whatever you want but let's call it eligibility trace and this function

58
00:04:00,660 --> 00:04:07,530
is going to take one argument which is going to be a batch and why that it's because we're going to

59
00:04:07,530 --> 00:04:13,460
get some inputs and some targets because we're going to train the AI on batches.

60
00:04:13,530 --> 00:04:19,170
And so the inputs and the targets will go inside some batches and therefore the input argument here

61
00:04:19,410 --> 00:04:25,090
is this batch that will contain several inputs and then several targets that will compute.

62
00:04:25,500 --> 00:04:26,730
So there we go.

63
00:04:26,730 --> 00:04:28,310
That's the only argument we need.

64
00:04:28,500 --> 00:04:32,280
Now let's go inside a function and let's define what we needed to do.

65
00:04:32,370 --> 00:04:40,140
So as we saw in the Basilica of the paper we need again the parameters so as we said we start by introducing

66
00:04:40,440 --> 00:04:45,390
this gamma parameter semi-close and we can already decipher value.

67
00:04:45,450 --> 00:04:51,780
And we're going to choose four point ninety nine that's a classic good value for the Ghana and Norreys.

68
00:04:51,780 --> 00:04:55,640
I checked that this is a good value for our AI.

69
00:04:55,650 --> 00:05:04,120
All right then next step next step is to prepare our input and our targets because that's exactly what

70
00:05:04,120 --> 00:05:05,260
we want to return.

71
00:05:05,260 --> 00:05:08,980
We want to return the inputs into targets to prepare the training.

72
00:05:08,980 --> 00:05:15,520
And so we can already initialize them with an empty list because of course these inputs Inside the best

73
00:05:15,790 --> 00:05:21,130
we're going to have several input all into a list and that's when initializing the inputs as a list

74
00:05:21,430 --> 00:05:25,000
as well as the targets that we go.

75
00:05:25,000 --> 00:05:31,420
So we initialized our and put in our targets and in the end this eligibility trace function will return

76
00:05:31,540 --> 00:05:33,320
exactly these inputs.

77
00:05:33,330 --> 00:05:36,180
And these yes were of course Filton.

78
00:05:36,290 --> 00:05:42,300
We have several inputs and the associated several targets in what will be returned by the function.

79
00:05:42,300 --> 00:05:48,820
All right next up next step is to start a loop and that's exactly because we're following the slow code

80
00:05:48,880 --> 00:05:50,290
of the paper.

81
00:05:50,290 --> 00:05:51,400
This sort of code.

82
00:05:51,490 --> 00:05:58,420
And as you can see there is this repeat code section and repeated exactly a full loop in the code.

83
00:05:58,470 --> 00:06:03,870
We are going to compute the cumulative reward right here accumulated over the 10 steps.

84
00:06:03,940 --> 00:06:05,130
And how is it computed.

85
00:06:05,290 --> 00:06:10,480
Well in each step that is not the last step we're going to get the maximum of the core values of the

86
00:06:10,480 --> 00:06:13,070
currency that we're in during this and steps run.

87
00:06:13,240 --> 00:06:17,790
And if we reach the last State of the 10 steps well this will be equal to zero.

88
00:06:17,890 --> 00:06:19,810
That is we don't want to do it anymore.

89
00:06:20,080 --> 00:06:23,320
And then we have this for loop which is going to be another up.

90
00:06:23,470 --> 00:06:28,390
They don't say repeat here but that's the same it's going to be the second full loop in our algorithm.

91
00:06:28,600 --> 00:06:34,570
Well we will have at that we were this way by multiplying it by the decay parameter gamma and adding

92
00:06:34,840 --> 00:06:35,660
the word.

93
00:06:36,010 --> 00:06:41,510
So let's do this let's go back to Python and let's start our full so.

94
00:06:41,980 --> 00:06:44,920
And what is going to be the iterative variable.

95
00:06:45,070 --> 00:06:47,820
Well that's going to be our 10 step series.

96
00:06:47,890 --> 00:06:55,510
You know our series of 10 transitions so we're going to call this variable series that represent a series

97
00:06:55,600 --> 00:06:58,820
of 10 transitions like a sequence of 10 transitions.

98
00:06:58,860 --> 00:07:00,910
So for series in.

99
00:07:01,210 --> 00:07:02,400
And then what do you think.

100
00:07:02,620 --> 00:07:05,570
Well our series will be down to our batch.

101
00:07:05,680 --> 00:07:12,040
There is the batches on which will train the AI and so forth series in batch that is for all the series

102
00:07:12,040 --> 00:07:14,540
of 10 transitions in our input batch.

103
00:07:14,800 --> 00:07:20,710
Well where are we going to do well to get a cumulative reward you will see in the silica that we need

104
00:07:20,950 --> 00:07:26,560
the State of the first transition of the series and also the state of the last transition of the series.

105
00:07:26,740 --> 00:07:33,040
So what we have to do right now is get these input states and so we are going to put these two states

106
00:07:33,130 --> 00:07:39,380
into a viable that were going to call input and we will get these two input states.

107
00:07:39,400 --> 00:07:46,590
The first one of the series and the last one that we're going to put into a non-pilot array but no worries

108
00:07:46,800 --> 00:07:51,880
will not stay with this and Ampyra will of course convert that into a horrible but the first step is

109
00:07:51,880 --> 00:07:56,930
to put these two states the first one in the last one into an empire.

110
00:07:57,240 --> 00:08:03,850
And so right here in this array we add the first input which is the input stage of the first transition

111
00:08:03,850 --> 00:08:10,820
of the series and that is Series and then to take it for a transition we take the index zero of the

112
00:08:10,820 --> 00:08:17,780
series thats the first transition and then we can access it by taking its attributes which is state

113
00:08:18,290 --> 00:08:24,200
and thats because in our experience replay file we did find a special structure for each of the transition

114
00:08:24,440 --> 00:08:25,660
and you know the structure.

115
00:08:25,730 --> 00:08:29,380
Each transition is composed of a state an action word.

116
00:08:29,570 --> 00:08:34,970
But then the last element which is done so this special structure that we are allowed to use right now

117
00:08:35,240 --> 00:08:39,250
comes from the way we defined the transition and experience replay.

118
00:08:39,290 --> 00:08:45,380
All right so with this we get the input state of the first transition and now lets get also the input

119
00:08:45,380 --> 00:08:48,640
stage of the last transition of the series.

120
00:08:48,800 --> 00:08:57,200
And to do this that's to saying we can just copy this and paste it and replace as you are here by the

121
00:08:57,200 --> 00:09:04,040
last index of the series which we can access with this trick minus one series minus one that state will

122
00:09:04,100 --> 00:09:07,420
get the input state of the last transition of the series.

123
00:09:08,210 --> 00:09:16,310
All right then we need to put these two elements inside some square brackets because that's what is

124
00:09:16,310 --> 00:09:23,120
expected by the umpire a function and then an important thing to do since we are going to convert that

125
00:09:23,210 --> 00:09:26,140
into a torch answer in a torch variable.

126
00:09:26,330 --> 00:09:31,810
Well remember a torch tensor is by definition a special array containing one single type.

127
00:09:31,940 --> 00:09:34,640
And so we need to force having one single type.

128
00:09:34,760 --> 00:09:42,020
And as usual we're going to choose the float type and so on adding this parameter here D type equals

129
00:09:42,750 --> 00:09:45,030
and P that float.

130
00:09:45,320 --> 00:09:52,380
So that you can take this one and now we can convert that into a torch tensor in a torch voivode.

131
00:09:52,550 --> 00:09:54,640
So let's do this do this.

132
00:09:54,770 --> 00:09:57,470
Well first let's convert that into a torch sensor.

133
00:09:57,590 --> 00:10:06,880
And remember we can use torche that from non-prime that we go and we put all the array of the two input

134
00:10:06,900 --> 00:10:15,180
states inside this torch dancer with the torch from them by function perfect that will convert these

135
00:10:15,250 --> 00:10:18,230
arrays of the two input state into a torch sensor.

136
00:10:18,420 --> 00:10:26,560
And now we put this torch the answer into a torch very well using the variable class so input will be

137
00:10:26,560 --> 00:10:28,400
an object of the valuable class.

138
00:10:28,480 --> 00:10:35,380
And in fact as you understood this variable class takes all this as an argument and that creates the

139
00:10:35,380 --> 00:10:36,680
object.

140
00:10:36,680 --> 00:10:38,420
All right so now we should be good.

141
00:10:38,450 --> 00:10:41,000
We have our two inputs that we need.

142
00:10:41,000 --> 00:10:45,440
That is the input state of the first transition and then input say that the last transition.

143
00:10:45,770 --> 00:10:51,890
And now now that we have the inputs Well what can we get we can get the output signal of the brain of

144
00:10:51,890 --> 00:10:52,560
the AI.

145
00:10:52,640 --> 00:10:56,160
That is the prediction that we're going to call it outputs.

146
00:10:56,630 --> 00:10:57,950
That's the output signal.

147
00:10:58,130 --> 00:10:59,280
And to get the outputs.

148
00:10:59,330 --> 00:11:04,670
Well now that's very easy because we already have a brain created which is our convolutional neural

149
00:11:04,670 --> 00:11:05,330
network.

150
00:11:05,570 --> 00:11:14,120
And so we can simply take our brain CNN applied to the inputs which will return the prediction that

151
00:11:14,120 --> 00:11:16,650
is the output as simple as that.

152
00:11:16,820 --> 00:11:19,510
And now we are already ready to move on to the next step.

153
00:11:20,730 --> 00:11:24,890
And the next step is to start to compute this community if you want.

154
00:11:25,090 --> 00:11:30,690
So now we're going to do exactly the same as our as to algorithm the Sarsour or should we call it and

155
00:11:30,760 --> 00:11:32,300
steps to learning.

156
00:11:32,290 --> 00:11:39,990
We're going to introduce the cumulative reward variable which will be the cumulative reward let's go

157
00:11:39,990 --> 00:11:45,120
back to the paper as you can see right now what we have to do to get this community reward which is

158
00:11:45,210 --> 00:11:46,000
our here.

159
00:11:46,170 --> 00:11:53,160
Well and each step of the 10 steps run we need to update it by adding a zero to this community if we

160
00:11:53,160 --> 00:11:59,910
were if we reached the last stage of the series or the maximum of the core values if we haven't reached

161
00:12:00,000 --> 00:12:03,410
the last stage of the series that is for all the statics that lasted.

162
00:12:03,750 --> 00:12:05,890
So that's simply an bonanzas.

163
00:12:06,000 --> 00:12:07,580
Let's go back to Piscean.

164
00:12:07,650 --> 00:12:16,490
So this community reward as we just saw is going to be equal to zero point zero if we reached the last

165
00:12:16,490 --> 00:12:23,070
state and we can write this condition this way if series of index minus 1.

166
00:12:23,070 --> 00:12:25,940
That is the last transition of the series.

167
00:12:26,250 --> 00:12:32,280
Then we add that done because done actually is an attribute of you know this transitional structure

168
00:12:32,280 --> 00:12:38,400
that we defined in experience we play our experience replay foul and this done comes from actually the

169
00:12:38,400 --> 00:12:44,550
opening structures because if we go to the open Allergan website which is actually right here I prepared

170
00:12:44,550 --> 00:12:45,240
it.

171
00:12:45,280 --> 00:12:47,160
That's the do good or vizir.

172
00:12:47,340 --> 00:12:52,300
And if we go to documentation and then if we.

173
00:12:52,380 --> 00:12:55,240
That's the tutorial I really encourage you to have a look at it.

174
00:12:55,470 --> 00:13:01,530
You can run an environment that Knowsley you can see that our observations that these are transitions

175
00:13:01,950 --> 00:13:04,690
are defined by an observation.

176
00:13:04,790 --> 00:13:12,150
We want this done here and this done means exactly that a transition or a step is over.

177
00:13:12,150 --> 00:13:15,440
And so we're going to use this done here for our IF condition.

178
00:13:15,610 --> 00:13:23,460
Therefore iSeries madness when that done means if the last transition of the series is over is completed.

179
00:13:23,730 --> 00:13:29,290
And so this cumulative reward is going to be equal to zero if the last transition of the series is done.

180
00:13:29,330 --> 00:13:37,290
Else if we haven't reached the last transition well cumulative reward is going to be updated with as

181
00:13:37,290 --> 00:13:40,950
we said the maximum of the key values.

182
00:13:41,220 --> 00:13:47,460
And since this output here is the output of the brain that is the predictions of the neural network.

183
00:13:47,520 --> 00:13:53,640
And as you know the predictions of the neural network are the predicted values Well this output contains

184
00:13:53,920 --> 00:13:55,040
the values.

185
00:13:55,110 --> 00:14:01,130
And since we need to take the max of the q values well we need to add first this index because this

186
00:14:01,130 --> 00:14:04,620
structure contains two key values and the next one.

187
00:14:04,620 --> 00:14:07,830
And then we need to add data to access the data.

188
00:14:07,830 --> 00:14:12,150
This output structure you know it has the special structure of a torch voivode.

189
00:14:12,270 --> 00:14:17,910
So with this we get our core values and then we want to take the maximum of our cue values and so we

190
00:14:17,910 --> 00:14:20,470
add that Max.

191
00:14:20,490 --> 00:14:27,820
And now we get exactly what we want as in the paper this maximum of the cube values for the non-terminal

192
00:14:27,820 --> 00:14:30,240
States as to perfect.

193
00:14:30,280 --> 00:14:33,460
And so now what we're going to do is make the second fold.

194
00:14:33,930 --> 00:14:40,830
That is for the 10 steps of the series we are going to update the cumulative went this way by multiplying

195
00:14:40,830 --> 00:14:46,170
first by Gamma the decay parameter which we already have and then add the B word.

196
00:14:46,410 --> 00:14:47,540
So let's do this.

197
00:14:47,610 --> 00:14:52,260
We are actually going to do exactly the same as in the pseudocode as you can notice they start from

198
00:14:52,260 --> 00:14:52,970
the right.

199
00:14:53,040 --> 00:14:59,010
So they're not starting with the first step and go into the last test they start with the last step.

200
00:14:59,040 --> 00:15:02,280
T-minus 1 to the first step to start.

201
00:15:02,340 --> 00:15:06,520
That's exactly what we're going to do and that's because we want to get in the end the cumulative reward

202
00:15:06,610 --> 00:15:15,840
equal to our equals or zero plus gamma or 1 plus gamma squared or two plus that added plus gamma at

203
00:15:15,840 --> 00:15:24,420
the power of 10 or 10 where are 1 or 2 that are 10 are the word obtained in each of the steps of the

204
00:15:24,420 --> 00:15:25,580
series.

205
00:15:25,590 --> 00:15:29,870
So let's take a quick break before taking the second full loop and I'll see you in the next tutorial.

206
00:15:30,030 --> 00:15:31,530
Until then enjoy AI.
