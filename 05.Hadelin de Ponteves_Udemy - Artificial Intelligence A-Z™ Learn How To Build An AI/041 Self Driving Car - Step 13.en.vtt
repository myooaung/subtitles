WEBVTT
1
00:00:00.450 --> 00:00:02.670
Hello and welcome to this pattern it's horror.

2
00:00:02.850 --> 00:00:06.890
All right so we just updated the memory after reaching the new state.

3
00:00:06.900 --> 00:00:09.010
And now let's take care of the next day.

4
00:00:09.330 --> 00:00:12.810
According to you now what is going to be the next update.

5
00:00:12.810 --> 00:00:20.040
Well basically we're done with one transition we have dated the last element of the transition which

6
00:00:20.040 --> 00:00:20.960
is the new state.

7
00:00:21.150 --> 00:00:23.340
So now it's like we're starting all over again.

8
00:00:23.520 --> 00:00:25.530
And when we were starting all over again.

9
00:00:25.650 --> 00:00:31.320
It's like you know we are in this new state of the environment and so what do we need to do now naturally.

10
00:00:31.500 --> 00:00:37.170
Well of course it's to play an action because we already got the observation of the news States.

11
00:00:37.410 --> 00:00:43.290
Now the thing that we have to do is play an action and therefore what we need to do now is of course

12
00:00:43.290 --> 00:00:46.960
use the select action function to play the action.

13
00:00:47.010 --> 00:00:54.320
So let's do it let's create a new Voivode action and let's play the action with the select action so

14
00:00:54.320 --> 00:00:55.240
I'm taking.

15
00:00:55.380 --> 00:01:03.150
Well first self to specify that the select action function is a method of the object of the class that

16
00:01:03.150 --> 00:01:04.300
will be created.

17
00:01:04.470 --> 00:01:08.610
So a self that selects action.

18
00:01:08.640 --> 00:01:09.520
Here we go.

19
00:01:09.560 --> 00:01:11.090
So that's the next action.

20
00:01:11.310 --> 00:01:17.560
And then of course since the select action function takes the state as input because of course the select

21
00:01:17.560 --> 00:01:24.270
action function will return the output of the neural network when the current input state entered the

22
00:01:24.270 --> 00:01:25.220
neural network.

23
00:01:25.500 --> 00:01:31.650
So we have to input the input stage here and since that's the states that we just reached in the environment

24
00:01:31.650 --> 00:01:37.350
right now where the input state is of course you state because this state that we just reached at the

25
00:01:37.350 --> 00:01:40.070
time we right now is Newstead.

26
00:01:40.290 --> 00:01:45.140
So in this select action function I mean putting new sticks.

27
00:01:45.150 --> 00:01:51.710
All right so with this line of code we simply play the new action after reaching the new state.

28
00:01:52.140 --> 00:01:58.780
OK and now that we played an action Well we get the reward and therefore we get feedback with the reward.

29
00:01:59.010 --> 00:02:04.550
And therefore if we have more than 100 elements in the memory Well it would be time to learn.

30
00:02:04.860 --> 00:02:10.830
And therefore what we must do now is what logically comes after selecting an action which is of course

31
00:02:10.830 --> 00:02:13.830
to lower the AI needs to start learning.

32
00:02:13.950 --> 00:02:15.620
If it is doing the things the right way.

33
00:02:15.870 --> 00:02:22.530
And now since it just played the action well we're going to make the AI learn from its actions in the

34
00:02:22.530 --> 00:02:23.940
last 100 events.

35
00:02:24.130 --> 00:02:31.320
But before we apply this learned function we have to make this condition to make sure that we already

36
00:02:31.320 --> 00:02:37.380
have reached more than 100 events because you know we're learning from the random samples of the memory.

37
00:02:37.530 --> 00:02:41.110
You know we have this huge memory of 10000 elements.

38
00:02:41.190 --> 00:02:48.930
We're taking some random samples of the memory of 100 elements and the AI is learning from the information

39
00:02:48.990 --> 00:02:52.850
contained in this sample of 100 random transitions.

40
00:02:52.860 --> 00:03:00.690
So let's just make this if condition to make sure that the number of elements of the memory of that

41
00:03:00.810 --> 00:03:06.390
memory and then be careful just a little trick here self-taught memory is the object of your replay

42
00:03:06.390 --> 00:03:12.380
memory class but then the replay memory class has an attribute which is memory.

43
00:03:12.510 --> 00:03:21.780
So in fact we need to take some of the memory that memory the first memory is the object of the replay

44
00:03:21.780 --> 00:03:28.170
memory class and the second memory is the attribute here self that memory.

45
00:03:28.200 --> 00:03:37.350
So if the number of elements in the memory is well we want it to be larger than 100 then Cullin And

46
00:03:37.860 --> 00:03:38.800
then what happens.

47
00:03:38.970 --> 00:03:46.320
Well we can learn but for learning we need to get this random sample of 100 transitions and this we

48
00:03:46.320 --> 00:03:48.840
can get with the simple function.

49
00:03:48.840 --> 00:03:55.140
And since the simple function returns the different batches to states at time t this data 20 plus one

50
00:03:55.290 --> 00:03:57.950
the actions of time t and we was at 20.

51
00:03:58.200 --> 00:04:03.360
Well what we need to do now is create some new revivals which are going to be the batch of the states

52
00:04:03.360 --> 00:04:08.580
at time t the batch of the next dates the batch of the words and the batch of the actions and we can

53
00:04:08.580 --> 00:04:16.030
simply give the same names as we gave for the arguments here and they are here.

54
00:04:16.200 --> 00:04:24.720
And these variables will be equal to what the simple function returns because it returns exactly these

55
00:04:24.720 --> 00:04:28.320
batches and the state's next takes words and actions.

56
00:04:28.320 --> 00:04:35.570
So what we simply need to do now is get first our memory object and then from this memory object we

57
00:04:35.620 --> 00:04:40.300
are going to use the simple method which will take as inputs.

58
00:04:40.500 --> 00:04:46.370
Well the number of transitions we want our AI to learn from that is 100.

59
00:04:46.620 --> 00:04:50.540
That's why we made sure that the we had more than one hundred transitions.

60
00:04:50.610 --> 00:04:54.780
So it's going to learn from 100 transitions of the memory.

61
00:04:54.840 --> 00:04:56.500
So the learning will be much better.

62
00:04:56.610 --> 00:04:59.560
And so now let's make this really happen.

63
00:04:59.700 --> 00:05:04.560
Well since the learn method is a method of our in class.

64
00:05:04.830 --> 00:05:11.180
Well we need to access this Learn method from the future objects that will be created from a different

65
00:05:11.190 --> 00:05:14.300
class and therefore what we need to take is self.

66
00:05:14.430 --> 00:05:21.960
Self refers to that objective to do during class and then learn as this learned method learned method

67
00:05:22.470 --> 00:05:29.340
to which when put Of course these guys here the bad state the Belgian state the natural world and the

68
00:05:29.340 --> 00:05:30.530
Bachche action.

69
00:05:30.630 --> 00:05:38.730
These are our batches sampled from our memory and we get 100 of them because we have 100 transitions

70
00:05:39.330 --> 00:05:47.750
from these 100 transitions we take 100 States 100 next states 100 reward and 100 actions let's face

71
00:05:47.780 --> 00:05:51.530
the here and there we go now the learning will happen.

72
00:05:51.850 --> 00:05:54.490
It will happen from all these random batches.

73
00:05:54.520 --> 00:05:55.850
Perfect.

74
00:05:55.960 --> 00:06:03.310
And now what we need to do is the very last updates after you know reaching a new state and playing

75
00:06:03.310 --> 00:06:04.140
in action.

76
00:06:04.330 --> 00:06:08.890
Well we got you actually to play but we still didn't have that reaction.

77
00:06:08.920 --> 00:06:11.590
That is our self that last action voivode.

78
00:06:11.770 --> 00:06:13.730
So let's make sure we don't forget this.

79
00:06:13.780 --> 00:06:15.160
Let's do it right now.

80
00:06:15.190 --> 00:06:24.610
We will update the last action self that last action equals and of course action the action that we

81
00:06:24.610 --> 00:06:27.020
just stay here with this select action function.

82
00:06:27.020 --> 00:06:30.520
All right now the last section is updated then.

83
00:06:30.520 --> 00:06:31.890
Same for the new state.

84
00:06:31.930 --> 00:06:39.190
We reached the new state but we haven't updated the last date yet because of course the last was before

85
00:06:39.280 --> 00:06:40.590
the state at time t.

86
00:06:40.750 --> 00:06:44.870
But since now we reached the new state surplus when it's time to pass one.

87
00:06:45.070 --> 00:06:48.180
Well the last it becomes this you said here.

88
00:06:48.190 --> 00:06:50.290
Therefore we need updated as well.

89
00:06:50.290 --> 00:06:57.070
Self that last state equals our new state.

90
00:06:57.280 --> 00:06:58.020
There we go.

91
00:06:58.210 --> 00:06:59.620
And now what do we need to date.

92
00:06:59.830 --> 00:07:01.640
Well there is only one thing left.

93
00:07:01.660 --> 00:07:08.070
That's of course the word and the word is exactly the word we get in reality.

94
00:07:08.290 --> 00:07:15.910
So that will be the argument of this function which if we made the connection to our map will be the

95
00:07:15.910 --> 00:07:17.160
last word.

96
00:07:17.200 --> 00:07:23.250
That is the word we get right after playing the action in this reached new states.

97
00:07:23.470 --> 00:07:30.260
So if we go on to some sound this last word will be that minus one if we get further from the go we

98
00:07:30.290 --> 00:07:35.580
will get a slightly bad word minus 0.2 if we get closer to the goal.

99
00:07:35.660 --> 00:07:38.680
We will get a slightly good reward 0.1.

100
00:07:38.830 --> 00:07:43.370
And if we get too close to an edge of the map well that will be about punishment.

101
00:07:43.510 --> 00:07:45.330
We will get minus one for each.

102
00:07:45.440 --> 00:07:47.110
So that's the last word we get.

103
00:07:47.110 --> 00:07:50.700
In reality that is when that happens for real on the map.

104
00:07:50.830 --> 00:07:53.550
And this will be the argument of the function.

105
00:07:53.600 --> 00:07:54.690
The last word here.

106
00:07:54.700 --> 00:07:56.200
That's exactly this one.

107
00:07:56.250 --> 00:08:01.840
And so since this is the argument of the update function well that corresponds to this we weren't here

108
00:08:02.170 --> 00:08:11.590
and therefore our self that last word variable initialized at the beginning in this function becomes

109
00:08:12.070 --> 00:08:20.570
the new word we get in reality that is the word or that's the same last word.

110
00:08:20.730 --> 00:08:21.360
All right.

111
00:08:21.410 --> 00:08:23.680
Now we updated our last word.

112
00:08:23.990 --> 00:08:27.240
And now since we just got our last word.

113
00:08:27.500 --> 00:08:29.470
Well we can now date the world.

114
00:08:29.850 --> 00:08:37.140
You remember the war when they we initialized here as one of the variable of the object of our class.

115
00:08:37.190 --> 00:08:42.500
That's the window that's going to keep track of how this train is going by taking the average of the

116
00:08:42.500 --> 00:08:44.270
last 100 reward.

117
00:08:44.300 --> 00:08:50.180
So you know it will be like a sliding window showing us how the mean of the world is evolving.

118
00:08:50.180 --> 00:08:52.730
And so since we just got our last word.

119
00:08:52.910 --> 00:08:56.910
Well we can update the we were window into how do we update it.

120
00:08:57.080 --> 00:09:03.680
Well we simply need to append this last word to the window and therefore what I'm going to do is take

121
00:09:03.980 --> 00:09:12.350
my war window self that we word when they hear this and I'm going to use the append function.

122
00:09:12.530 --> 00:09:18.830
And inside the open function we need to input the element we want to append to the we were when doing

123
00:09:18.830 --> 00:09:21.480
that of course do we want.

124
00:09:21.500 --> 00:09:22.610
All right perfect.

125
00:09:22.610 --> 00:09:29.480
And then since this war window is going to have a fixed size you know it's not going to be a growing

126
00:09:29.480 --> 00:09:35.300
window it's going to be a window of fixed size sliding with time to show the evolution of the world.

127
00:09:35.500 --> 00:09:38.590
And so now we need to decide for a size of this winter.

128
00:09:38.750 --> 00:09:43.600
And it's simply the number of means of the reward we will have in this window.

129
00:09:43.900 --> 00:09:49.330
And so for example let's get you know the last 1000 means of the last 100 words.

130
00:09:49.340 --> 00:09:58.730
And so to make sure of it we're going to add if then plan then we take our work window and we simply

131
00:09:58.740 --> 00:10:05.810
add here if the number of elements in the window is larger than 1000.

132
00:10:05.990 --> 00:10:14.780
Well what we want to do is delete the first element of this who our window and the first element of

133
00:10:14.780 --> 00:10:19.070
this were window has to index zero.

134
00:10:19.150 --> 00:10:19.550
All right.

135
00:10:19.550 --> 00:10:24.620
Now we make sure that this war window will never get more than 1000 elements.

136
00:10:24.620 --> 00:10:31.130
There is one thousand means of the last 100 words that's perfect this will be a window of fixed size

137
00:10:31.310 --> 00:10:34.360
so that we can see if the mean of the word is increasing.

138
00:10:34.460 --> 00:10:39.560
And therefore if the training is going well and accordingly the court does what we want.

139
00:10:39.800 --> 00:10:40.760
Perfect.

140
00:10:41.000 --> 00:10:46.010
And now one tiny little thing to do left according to you what is it going to be.

141
00:10:46.190 --> 00:10:52.130
Well remember this update function not only updates the different elements of the transition in the

142
00:10:52.140 --> 00:10:57.960
war window but also it returns the action that was played when reaching this new state.

143
00:10:58.190 --> 00:11:05.270
That's why we have and then that action equals bring that date that we're less signal and therefore

144
00:11:05.360 --> 00:11:10.760
it's supposed to return something and the something that is supposed to return is of course the action.

145
00:11:10.820 --> 00:11:18.950
So the simple last thing we need to do here is just return action the action that was just played when

146
00:11:18.950 --> 00:11:20.180
reaching the new stage.

147
00:11:20.600 --> 00:11:23.670
And that's our update function is ready.

148
00:11:23.840 --> 00:11:29.110
It's going to do all the required updates and it will turn the action when reaching the new stage.

149
00:11:29.480 --> 00:11:30.650
That's perfect.

150
00:11:30.650 --> 00:11:35.230
That was the last difficult action to make for all this a process.

151
00:11:35.240 --> 00:11:37.120
Now the rest will be good stuff.

152
00:11:37.220 --> 00:11:42.170
We will just make a core function to return the means that we want in the window.

153
00:11:42.200 --> 00:11:47.450
Then we will make a safe function to save the brain of the car whenever you want to quit the application

154
00:11:47.600 --> 00:11:48.840
and go back to it.

155
00:11:48.890 --> 00:11:53.720
And of course since you want to be able to load the brain of your car when you get back to it get back

156
00:11:53.720 --> 00:11:54.730
to the application.

157
00:11:55.040 --> 00:12:01.370
Well we will end up by making a load function which will load your model after you save your model with

158
00:12:01.370 --> 00:12:02.800
the same function.

159
00:12:02.840 --> 00:12:06.470
So three functions to do that but it's going to be simple.

160
00:12:06.500 --> 00:12:12.760
And then we'll have the most exciting section of this first module that is the demo we will see if the

161
00:12:12.770 --> 00:12:13.840
air works.

162
00:12:13.850 --> 00:12:19.430
We will see if the car reaches the goals and we will see how we can improve it and then eventually you

163
00:12:19.430 --> 00:12:21.980
will have to build your first AI.

164
00:12:22.160 --> 00:12:24.010
So I can't wait to start the demo.

165
00:12:24.140 --> 00:12:27.580
Let's make this three functions first and until then I.
