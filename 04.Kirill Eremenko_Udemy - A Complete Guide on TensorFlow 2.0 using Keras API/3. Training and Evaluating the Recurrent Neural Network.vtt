WEBVTT
1
00:00:00.360 --> 00:00:00.670
All right.

2
00:00:00.690 --> 00:00:01.580
Let's do this.

3
00:00:01.770 --> 00:00:06.900
Let's train our record a new network and let's find out about the final accuracy.

4
00:00:07.560 --> 00:00:13.470
So this time compiling the model will be quite different because we'll have a different optimizer a

5
00:00:13.470 --> 00:00:18.750
different laws and a different metrics as opposed to the two previous new networks you know the CNN

6
00:00:18.840 --> 00:00:25.820
and the CNN which had the exact same compiling but this time indeed we still use the compound method

7
00:00:26.070 --> 00:00:29.460
but to which we input different arguments.

8
00:00:29.460 --> 00:00:37.050
The first one optimizer is going to be the army's prop optimizer which is indeed the most recommended

9
00:00:37.170 --> 00:00:39.750
optimizer for record new networks.

10
00:00:39.780 --> 00:00:46.440
You could still use an atom optimizer but in most of the cases using an iron in the arm as prop leads

11
00:00:46.440 --> 00:00:53.720
to better results then the loss is not going to be as before sparse categorical cross entropy.

12
00:00:53.850 --> 00:00:57.540
But binary cross entropy I'm sure you know why.

13
00:00:57.760 --> 00:01:05.270
That's because this time we're predicting a binary result as opposed to a multi class result like before

14
00:01:05.280 --> 00:01:09.390
you know we had to predict a class among 10 possible outcomes.

15
00:01:09.390 --> 00:01:16.440
And here the outcome can only be 0 or 1 in 0 0 4 negative review 1 for positive review and therefore

16
00:01:16.440 --> 00:01:23.450
in that case when we are doing basically binary classification well the less used is binary cross entropy.

17
00:01:23.460 --> 00:01:29.700
So now you know this and then for the metrics same we're not going to use like before sparse categorical

18
00:01:29.760 --> 00:01:32.710
accuracy but simply accuracy.

19
00:01:32.780 --> 00:01:38.400
That's the simple parameters name given when you are doing binary classification.

20
00:01:38.550 --> 00:01:42.210
So you know it's very simple when you're doing binary classification.

21
00:01:42.210 --> 00:01:45.750
The last is binary cross entropy and the matrix is accuracy.

22
00:01:45.810 --> 00:01:51.540
And when you're doing multi class classification the list is sparse categorical cross entropy and the

23
00:01:51.540 --> 00:01:54.330
metric is both categorical accuracy.

24
00:01:54.330 --> 00:01:54.630
Right.

25
00:01:54.630 --> 00:02:00.870
So it's good that you see both situations and now basically you can do any classification with any neural

26
00:02:00.870 --> 00:02:01.700
networks.

27
00:02:01.800 --> 00:02:07.650
And so there you go that connects your model to the right optimizer you know the most recommended one

28
00:02:07.980 --> 00:02:11.170
the appropriate laws and the appropriate accuracy metric.

29
00:02:11.460 --> 00:02:17.100
And so just before we train a moral Well let's have a look at its summary by just calling the summary

30
00:02:17.100 --> 00:02:23.280
function from our model and indeed we get a sequential model you know as a sequence of layers starting

31
00:02:23.280 --> 00:02:31.680
with a first and bearing layer composed of 128 columns encoding each word into this embedding matrix

32
00:02:31.800 --> 00:02:33.500
representation of words.

33
00:02:33.570 --> 00:02:40.040
Then the second layer is an Alice gem with 128 units or Elysium cells.

34
00:02:40.080 --> 00:02:46.920
And finally the output layer fully connected to the previous LC M layer with only one unit corresponding

35
00:02:46.920 --> 00:02:50.280
of course to the final output the predictions.

36
00:02:50.280 --> 00:02:55.830
And so in total we will have two million six hundred ninety one thousand seven hundred and thirteen

37
00:02:56.070 --> 00:02:57.180
parameters two trains.

38
00:02:57.180 --> 00:03:03.180
These are of course the weights that will be updated by the arms optimizer during stochastic gradient

39
00:03:03.180 --> 00:03:09.300
descent went back propagating the binary cross entropy plus during back propagation then we have the

40
00:03:09.300 --> 00:03:15.330
same number of trainable parameters because we have no hyper parameter insider will network.

41
00:03:15.330 --> 00:03:21.180
All right so we compiled our moral the right way and now let's train it and you're going to see that

42
00:03:21.180 --> 00:03:27.270
this time the way we train it is slightly different from before because we are going to include a batch

43
00:03:27.270 --> 00:03:28.130
size.

44
00:03:28.200 --> 00:03:29.100
There you go.

45
00:03:29.100 --> 00:03:34.400
You know before we didn't have a batch size we just provided a certain number of epochs.

46
00:03:34.500 --> 00:03:38.910
And of course the inputs and the targets in the real classes with the ground truth.

47
00:03:39.180 --> 00:03:44.820
And so this time well I included a batch size because indeed I wanted to get great results to give you

48
00:03:44.820 --> 00:03:48.290
some challenging exercises for you to try and improve this.

49
00:03:48.300 --> 00:03:49.280
So there you go.

50
00:03:49.320 --> 00:03:55.950
We have a batch size of 128 meaning that you're going to feed the neural network with different batches

51
00:03:56.250 --> 00:04:00.580
each one having one hundred and twenty eight input reviews all tatted of course.

52
00:04:00.960 --> 00:04:07.920
So instead of feeding the new network with either single observations or the full training set well

53
00:04:07.950 --> 00:04:14.580
you're going to feed the record into one network with batches of 128 reviews and all these batches will

54
00:04:14.580 --> 00:04:17.020
be extracted of course from exchange.

55
00:04:17.130 --> 00:04:19.800
And so let's do this let's run this cell.

56
00:04:19.800 --> 00:04:21.810
It's already executed of course.

57
00:04:21.810 --> 00:04:29.760
And we get the three following epochs with the following accuracies in the first epoch we already reached

58
00:04:29.820 --> 00:04:33.030
a pretty high accuracy with 78 percent.

59
00:04:33.450 --> 00:04:34.080
But that's not all.

60
00:04:34.080 --> 00:04:42.090
The second epoch led to a great improvement with 88 percent accuracy and the third epoch showed another

61
00:04:42.090 --> 00:04:46.250
improvement because it went up to ninety one percent accuracy.

62
00:04:46.260 --> 00:04:52.770
So again you should try with more bugs you should get some higher accuracy as you can see the results

63
00:04:52.860 --> 00:04:53.630
are really good.

64
00:04:53.790 --> 00:04:56.270
But don't forget let's not scream victory yet.

65
00:04:56.340 --> 00:05:02.010
We have to check the accuracy on the evaluation set as well and mostly on the evaluation set and we

66
00:05:02.010 --> 00:05:07.890
get you know still by calling the same evaluate method which takes the input of course the test set

67
00:05:07.890 --> 00:05:12.930
composed of X test and Y test and returning the teslas and the test accuracy.

68
00:05:12.930 --> 00:05:21.930
And so by executing this we get a superb accuracy of 85 percent so the accuracy is quite much lower

69
00:05:22.020 --> 00:05:23.520
than the one in training set.

70
00:05:23.520 --> 00:05:29.580
So maybe that should give you some ideas of what to do to improve the accuracy on the evaluation set.

71
00:05:30.120 --> 00:05:34.530
Maybe we have a little bit of overfishing so you know the solution to overfishing.

72
00:05:34.530 --> 00:05:37.650
You'll try that in the exercises but there you go.

73
00:05:37.650 --> 00:05:38.730
That's still very good.

74
00:05:38.730 --> 00:05:40.020
Eighty five percent.

75
00:05:40.020 --> 00:05:42.090
Well that's surely a great job.

76
00:05:42.090 --> 00:05:48.570
But remember that the aim to be movie reviews data set as clearly said here is intended to serve as

77
00:05:48.570 --> 00:05:56.010
a benchmark for sentiment classification and you might imagine that this 85 percent accuracy is way

78
00:05:56.010 --> 00:05:58.080
below the leader board.

79
00:05:58.080 --> 00:06:03.450
So that's also the purpose of using this data set it is to check the power and the performance of your

80
00:06:03.450 --> 00:06:09.090
morale in order to try new Arnold models with different architectures and different hybrid parameters

81
00:06:09.090 --> 00:06:15.720
and tools like the optimizer or a tunable hyper parameter like the batch size to improve the results.

82
00:06:15.720 --> 00:06:16.280
All right.

83
00:06:16.320 --> 00:06:22.050
And as usual I added this print just to get the test accuracy which is of course the same 85 percent.

84
00:06:22.140 --> 00:06:28.140
So try to be this or simply do the exercises that will still be a great thing for your skills.

85
00:06:28.140 --> 00:06:31.260
And then as soon as you feel confident with record neural networks.

86
00:06:31.320 --> 00:06:35.850
Well let us move on to the next section of this course about deep learning.

87
00:06:35.850 --> 00:06:39.610
So we are entering a branch closer to artificial intelligence.

88
00:06:39.720 --> 00:06:45.330
And this is where also we'll start some real world applications we can do with tons of load 2.0.

89
00:06:45.810 --> 00:06:51.510
So these applications will be done with the CO instructor of this course Luca and is seen I already

90
00:06:51.510 --> 00:06:54.150
know he's super pumped to do these applications with you.

91
00:06:54.480 --> 00:07:01.070
So you will have fun with him and on my side was a pleasure to share these new knowledge about sensor

92
00:07:01.070 --> 00:07:07.290
flow 2.0 with you with the three classic neural networks Aon and CNN and Arnon and I wish you to have

93
00:07:07.290 --> 00:07:12.690
great success and also fun not only with Luca and the rest of this course but also in your projects

94
00:07:12.930 --> 00:07:13.930
and in your career.

95
00:07:13.980 --> 00:07:15.420
So I'll see you in the next course.

96
00:07:15.420 --> 00:07:16.510
I look forward to.

97
00:07:16.510 --> 00:07:19.680
And until then enjoy deep learning in A.I. with tens of.
