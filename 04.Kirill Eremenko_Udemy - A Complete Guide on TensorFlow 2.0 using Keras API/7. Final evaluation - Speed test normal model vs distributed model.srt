1
00:00:00,500 --> 00:00:06,650
Hello everyone and welcome to despite an editorial in the final video of distributed training project

2
00:00:07,400 --> 00:00:08,140
in this video.

3
00:00:08,150 --> 00:00:15,960
We are going to compare the training time for the normal model and the distributed one before our lesson.

4
00:00:15,960 --> 00:00:23,010
I prepared only two lines of code where I define this starting time before our training process is started

5
00:00:23,460 --> 00:00:29,760
and the simple string formatting where I'm going to print out how many seconds a training process took.

6
00:00:31,000 --> 00:00:36,790
I've done that twice once for a distributed model and other one for the normal one.

7
00:00:36,820 --> 00:00:43,090
Right now we are going to ride the same training process for both models on the same data set with the

8
00:00:43,080 --> 00:00:50,530
same parameters but only different strategies one is distributed one and other one is on the single

9
00:00:50,590 --> 00:00:51,010
GPO.

10
00:00:51,280 --> 00:00:58,000
Let's start by calling model distributed that we defined above and let's call FID function on it.

11
00:00:58,090 --> 00:01:04,110
We are going to provide X train and Y train that we had in Emily's data set.

12
00:01:04,210 --> 00:01:11,370
It is followed by epochs which is going to be equal to 10 and batch size which is going to be twenty

13
00:01:11,480 --> 00:01:12,830
five.

14
00:01:12,870 --> 00:01:16,420
We are going to do the same thing for our normal model.

15
00:01:16,420 --> 00:01:24,340
Copy the line and paste it in a dedicated cell for our normal model instead of model of distributed

16
00:01:24,750 --> 00:01:27,260
change to model normal.

17
00:01:27,270 --> 00:01:33,310
Now we are ready to execute these two cells and start the train process for a distributed model and

18
00:01:33,310 --> 00:01:34,860
the normal one.

19
00:01:34,860 --> 00:01:42,240
Are you ready for that guys execute the cells and by doing that we are going to start the board train

20
00:01:42,240 --> 00:01:48,980
processes firstly it is going to complete the training process of our distributed model and then we

21
00:01:48,980 --> 00:01:51,310
are going to jump in a normal model of training.

22
00:01:52,160 --> 00:01:58,200
I will skip to the part where the training process is done.

23
00:01:58,230 --> 00:01:59,100
Welcome back.

24
00:01:59,100 --> 00:02:04,410
We finally have the results for our distributed and normal model training.

25
00:02:04,650 --> 00:02:07,710
Let's see which one performed better.

26
00:02:07,850 --> 00:02:14,750
We did expect to have our distributed system work better and to have better performance than our normal

27
00:02:14,750 --> 00:02:22,960
case let's check if that is the case really as you can see here distributed training took around one

28
00:02:22,960 --> 00:02:25,200
hundred and seventy three seconds.

29
00:02:25,450 --> 00:02:30,640
Well the non distributed one took only one hundred and fifteen nine seconds.

30
00:02:30,640 --> 00:02:39,350
Why is that the case well in this scenario we have the Depew and the CPI in the device pool and the

31
00:02:39,350 --> 00:02:45,120
CPO works worse therefore it will slow down the GPO system.

32
00:02:45,410 --> 00:02:52,760
If you have a single drip you in most cases it is going to perform better and faster without distributing

33
00:02:52,760 --> 00:02:53,950
it to you.

34
00:02:54,020 --> 00:03:00,600
And the CPO combination I would say that this was the edge case where the normal training performance

35
00:03:00,600 --> 00:03:02,410
is better than the distributed one.

36
00:03:02,780 --> 00:03:08,870
But if you have two or more deep use you're most certainly going to have better results by combining

37
00:03:08,900 --> 00:03:11,360
speeds of both the GP use.

38
00:03:11,360 --> 00:03:18,210
So if you find yourself in a situation where you have two or more deep viewers use this to be the training.

39
00:03:18,320 --> 00:03:24,510
This outcome was probably unexpected to many of you but these were results and that's it for now.

40
00:03:24,620 --> 00:03:29,000
If you have any questions or comments please post them in the comments section.

41
00:03:29,000 --> 00:03:30,860
Otherwise I assume the next tutorial.
