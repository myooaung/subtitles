WEBVTT
1
00:00:01.150 --> 00:00:04.690
Hello and welcome back to the course on artificial intelligence.

2
00:00:04.690 --> 00:00:08.020
Today we're talking about the Temporal Difference.

3
00:00:08.020 --> 00:00:14.260
Now it's very important to trial because Temporal Difference is the heart and soul of the queue learning

4
00:00:14.290 --> 00:00:15.020
algorithm.

5
00:00:15.040 --> 00:00:21.910
This is actually how everything we've learned so far comes together into play inside key learning.

6
00:00:22.360 --> 00:00:23.840
So let's have a look.

7
00:00:23.860 --> 00:00:28.360
Remember the time when we talked about deterministic versus non deterministic search.

8
00:00:28.360 --> 00:00:34.910
And remember how we said in this case it's when the agent wants to go up he definitely goes up and when.

9
00:00:35.020 --> 00:00:38.710
In this case he wants to go up there's a 10 percent chance he'll go or left 10 percent chance he'll

10
00:00:38.710 --> 00:00:42.060
go right and an 80 percent chance he'll go right go straight up.

11
00:00:42.400 --> 00:00:46.360
Well these numbers are of course arbitrary and can be different.

12
00:00:46.360 --> 00:00:50.620
And this whole concept is could be different in different problems.

13
00:00:50.650 --> 00:00:55.030
So doesn't have to concern which way he's moving is that there's some randomness.

14
00:00:55.030 --> 00:01:01.300
Something as out of the control of the agent happening inside this environment and what effect that

15
00:01:01.300 --> 00:01:02.530
had is.

16
00:01:02.530 --> 00:01:09.190
As you remember was that in the deterministic example it was very easy to calculate the V values.

17
00:01:09.190 --> 00:01:10.980
Well not necessarily always very easy.

18
00:01:10.990 --> 00:01:16.150
But in our case we could just simply calculate them by using the bellman equation and we we had the

19
00:01:16.150 --> 00:01:24.430
exact values and then as you remember I very carefully mentioned that these values for the non deterministic

20
00:01:24.430 --> 00:01:27.750
search example are off the top of my head.

21
00:01:27.770 --> 00:01:31.870
They're not calculus we know lost at that time I said we know we're just not going to calculate them

22
00:01:31.870 --> 00:01:37.330
because it's very complex but the computer could do it and we just went along with these values that

23
00:01:37.330 --> 00:01:39.550
are just values that I made up.

24
00:01:39.550 --> 00:01:42.730
But they did get the job done they helped us understand the concept.

25
00:01:43.240 --> 00:01:48.130
Well now we're going to return to that a little bit and understand what exactly is going on here why

26
00:01:48.130 --> 00:01:55.330
is it so much harder to calculate these values in the non deterministic example or generally speaking

27
00:01:55.330 --> 00:01:59.490
in these problems in these environments and the agent going through them.

28
00:01:59.500 --> 00:02:00.130
Why is it.

29
00:02:00.460 --> 00:02:02.970
Why can it be so hard to calculate these values.

30
00:02:02.980 --> 00:02:09.070
Well when you think about it because when the agent moves for for instance from here to the right he

31
00:02:09.070 --> 00:02:14.860
doesn't necessarily always move that way sometimes as a chance that he'll go to and to when instead

32
00:02:14.860 --> 00:02:18.600
of going straight so let's call these NSW.

33
00:02:18.600 --> 00:02:26.440
So in 0 9 instead of going west the agent might sometimes go south and for instance from here instead

34
00:02:26.440 --> 00:02:28.920
of going north he might sometimes go east.

35
00:02:29.410 --> 00:02:30.220
So sorry.

36
00:02:30.220 --> 00:02:35.410
So here instead of going east he might sometimes go south and here instead of going north he might sometimes

37
00:02:35.410 --> 00:02:40.980
go east or west and here instead of going north he might sometimes go west east or west and so on and

38
00:02:40.990 --> 00:02:46.960
so and therefore so in order to calculate this value you would need to know what this values but the

39
00:02:46.960 --> 00:02:51.060
interesting thing is that in order to calculate this value you need to know what this value is.

40
00:02:51.070 --> 00:02:56.740
So there is a lot of recursion happening here and therefore you cannot just decide to define what these

41
00:02:56.740 --> 00:02:57.310
values are.

42
00:02:57.310 --> 00:03:01.100
And on top of that this recursion is not deterministic.

43
00:03:01.120 --> 00:03:06.130
It is sometimes it happens this way Sometimes it's up uphill go right sometimes instead of going up

44
00:03:06.580 --> 00:03:10.480
he'll go left sometimes it's there when he wants to go up he will go up.

45
00:03:10.480 --> 00:03:17.080
So it is subject to chance and so maybe many times the agent will go through this path and he'll go

46
00:03:17.080 --> 00:03:21.850
up up up up up and he'll think that from here he always kind of goes up and so the value of the state

47
00:03:21.850 --> 00:03:27.550
will go will be good and then all of a sudden he'll drop into the pit and this value will go down.

48
00:03:27.550 --> 00:03:33.280
And so therefore you can see how there is some stochastic city or randomness to this whole calculation

49
00:03:33.280 --> 00:03:35.210
on these values because they're all interlinked.

50
00:03:35.260 --> 00:03:40.840
Plus on top you've got that randomness in this inherent in the environment because there's a mark of

51
00:03:40.840 --> 00:03:42.490
decision process.

52
00:03:42.490 --> 00:03:47.650
So that's where all of this comes together and that's where we're going to introduce the concept of

53
00:03:47.650 --> 00:03:52.450
the Temporal Difference which will allow the agent to calculate these values.

54
00:03:52.450 --> 00:03:57.540
And here we were dealing with these values and since then we've already moved on to Q values.

55
00:03:57.550 --> 00:04:01.880
So that's what we're going to be working with we're going to be looking at Q values.

56
00:04:01.960 --> 00:04:06.130
So as you recall this is our Belman equation for Q values.

57
00:04:06.130 --> 00:04:08.390
So a Q value or the.

58
00:04:08.450 --> 00:04:16.510
The value of performing a certain action a state s is equal to the reward that you get after performing

59
00:04:16.510 --> 00:04:25.600
that actions immediately after that action plus you get the maximum you get the gamma of the sum of

60
00:04:25.600 --> 00:04:26.760
all the possible.

61
00:04:26.860 --> 00:04:30.970
So you kind of get the expected value of the state that you will end up in.

62
00:04:31.600 --> 00:04:36.940
So as you recall that was our formula for the Beldon accurate equation and now just for simplicity's

63
00:04:36.940 --> 00:04:43.240
sake we're going to rewrite it in the old fashioned way in the way that we used to talk about the Belmont

64
00:04:43.240 --> 00:04:45.870
equation before we knew about the success the city.

65
00:04:45.900 --> 00:04:53.050
So remember this was our Belmont equation in the sense of a deterministic search example because here

66
00:04:53.050 --> 00:04:58.450
you don't have that expected value you don't have the somewhere across all probabilities you just have

67
00:04:58.450 --> 00:05:03.480
that as if it's determined you're going to end up what states you're going to end up and then you're

68
00:05:03.480 --> 00:05:05.100
taking the max in that one state.

69
00:05:05.520 --> 00:05:12.120
And the reason we're rewriting it is simply the only reason is because it is just easier to write it

70
00:05:12.150 --> 00:05:14.520
and it'll be easier just to fall along with the formula.

71
00:05:14.520 --> 00:05:21.390
So we're going to just remember that we replaced this part with this part and also you'll find this

72
00:05:21.630 --> 00:05:27.270
notation in a lot of literature so it'll be easier for you to follow along with other sources if you're

73
00:05:27.270 --> 00:05:33.870
standing those but do remember that in fact what we mean is this probabilistic approach here instead

74
00:05:33.870 --> 00:05:39.480
of this notation it's just easier for us to operate this and understand what's going on and just kind

75
00:05:39.480 --> 00:05:42.500
of like look at the equation so that they're not too cluttered.

76
00:05:42.750 --> 00:05:48.210
But once again just remember that in fact what we mean is this probabilistic approach over here.

77
00:05:48.210 --> 00:05:51.990
And so we're actually near Newtown so let's have a look at what's going on.

78
00:05:52.110 --> 00:05:56.370
So here is our blank state of the maze.

79
00:05:56.370 --> 00:06:02.790
We don't have any Q values let's see or we may but let's just keep it blank for now let's just look

80
00:06:02.790 --> 00:06:10.020
at one of the states either one of the cells and this one specifically and here we have for instance

81
00:06:10.020 --> 00:06:16.260
for the action of going up we have a Q value that we calculate so it's not that we don't have any Q

82
00:06:16.260 --> 00:06:21.960
values yet we worry we do but we're just not illustrating anything we're just keeping a blank for simplicity's

83
00:06:21.960 --> 00:06:28.260
sake but we have the edge has been walking around for some time and let's say hypothetically somehow

84
00:06:28.260 --> 00:06:36.480
he's calculated this Q value of going up or north from this state from this specific cell and the values

85
00:06:36.480 --> 00:06:38.090
Q S and a.

86
00:06:38.100 --> 00:06:40.170
And so now what we have.

87
00:06:40.170 --> 00:06:47.070
So he is currently with his blue arrows pointing to the agent is sitting in this cell and now he needs

88
00:06:47.070 --> 00:06:48.540
to make a choice where is he going to go.

89
00:06:48.540 --> 00:06:56.340
And he knows the value of this of action going north and that is Q and A and here I'm saying before

90
00:06:57.110 --> 00:07:01.550
and the reason for that is because he that is before he takes action he hasn't taken action yet.

91
00:07:01.560 --> 00:07:10.260
So he's still in the cell and before he's taken action the value here is Q an estimate and now he actually

92
00:07:10.260 --> 00:07:11.330
takes the action.

93
00:07:11.340 --> 00:07:16.800
So let's say he decides this the best one he takes action and he moves up to this sell well.

94
00:07:17.070 --> 00:07:24.510
Now what happens is now comes after so after he's taken action we can measure what is this value let's

95
00:07:24.510 --> 00:07:31.230
just calculate this value the value of the reward of for taking that action plus gamma times the maximum

96
00:07:31.260 --> 00:07:35.590
of this new state that he's just gotten into s prime.

97
00:07:35.590 --> 00:07:38.930
And so the maximum across all possible actions and aspirin.

98
00:07:39.030 --> 00:07:44.760
And so what we have here is the value before in of that action.

99
00:07:44.760 --> 00:07:47.610
And then we've calculated this metric afterwards.

100
00:07:47.610 --> 00:07:50.760
But as you can recall from the previous formula.

101
00:07:50.790 --> 00:07:58.500
So if we could go back very quickly from the previous formula where we just calculated is indeed the

102
00:07:58.500 --> 00:08:01.320
value that is how Q of S&amp;P is calculated.

103
00:08:02.130 --> 00:08:09.690
So this right Pa we've just calculated it separately but after we've taken action so as Avon's again

104
00:08:09.690 --> 00:08:15.780
before we knew a cue of an estimated value something that we've calculated through our iterations produces

105
00:08:15.780 --> 00:08:16.640
something.

106
00:08:16.920 --> 00:08:19.920
So a value that's stored in our memory.

107
00:08:19.920 --> 00:08:26.940
So just like a number that we know and now after the actions being performed we know what reward he

108
00:08:26.940 --> 00:08:30.390
actually got what reward the agent actually got.

109
00:08:30.390 --> 00:08:33.270
And we can calculate this new value.

110
00:08:33.270 --> 00:08:39.630
So in essence we're kind of recalculating this value but now with new information the new information

111
00:08:39.630 --> 00:08:45.240
is the reward that we got and plus what state we ended up in and what the maximum across that state

112
00:08:45.630 --> 00:08:50.520
what that this new value is for that specific date that we're looking at.

113
00:08:50.520 --> 00:08:54.460
So what's the value of that being in that state.

114
00:08:54.480 --> 00:08:56.420
So basically the Q of NSN.

115
00:08:56.460 --> 00:09:06.090
But given the new information and now the Temporal Difference is defined as t d of a n s of these two

116
00:09:06.240 --> 00:09:07.670
of the difference between these two.

117
00:09:07.670 --> 00:09:11.730
So here the first element is your after value.

118
00:09:11.730 --> 00:09:19.080
So the kind of like Q of S and a but calculated afterwards and the previous Q of NSA and a which you

119
00:09:19.080 --> 00:09:21.730
had stored in your memory.

120
00:09:22.020 --> 00:09:24.240
And so the question is are they different.

121
00:09:24.240 --> 00:09:26.160
So ideally they should be the same.

122
00:09:26.190 --> 00:09:31.670
Ideally this should be the same as is simply because this is the formula for calculating this.

123
00:09:31.710 --> 00:09:34.920
But the thing is that this is not something we collect.

124
00:09:34.950 --> 00:09:39.790
This is something that we have from empirical evidence something that we have from just going through

125
00:09:39.790 --> 00:09:45.270
the maze many times and calculate so this is something we've come up with so far it's not related to

126
00:09:45.270 --> 00:09:46.770
the current iteration.

127
00:09:46.770 --> 00:09:51.450
It's something that we came up with previously a long time not a long time ago but in one of our previous

128
00:09:51.450 --> 00:09:56.610
iterations going through the maze whereas this is something we've calculated just now adds there is

129
00:09:56.610 --> 00:10:02.430
no guarantee that they're going to be the same and because of the randomness that exists in the maze.

130
00:10:02.440 --> 00:10:08.230
Because this could have been calculated and saw some certain random events were triggered and this can

131
00:10:08.230 --> 00:10:10.870
be called to a different random events have been triggered.

132
00:10:11.680 --> 00:10:14.020
And so now let's rewrite that over here.

133
00:10:14.020 --> 00:10:15.070
Let's just move it up there.

134
00:10:15.640 --> 00:10:16.840
So how do we use this.

135
00:10:16.840 --> 00:10:17.590
The question is.

136
00:10:17.590 --> 00:10:17.820
OK.

137
00:10:17.850 --> 00:10:20.410
So we have this temporal difference.

138
00:10:20.410 --> 00:10:23.260
How do we use this and why is it called a temporal difference.

139
00:10:23.530 --> 00:10:28.930
Well the reason is called the temporal difference is because you're basically calculating the same thing

140
00:10:28.960 --> 00:10:35.470
you're calculating queue of S.A. so the cue value of that action you calculate here and you're calculating

141
00:10:35.470 --> 00:10:35.910
it here.

142
00:10:36.250 --> 00:10:38.190
But the difference is time.

143
00:10:38.260 --> 00:10:41.610
This is your queue of assigned a previously.

144
00:10:41.830 --> 00:10:44.080
This is your queue of S and a.

145
00:10:44.080 --> 00:10:46.080
Now your new curious.

146
00:10:46.150 --> 00:10:49.030
And the question is it has there been a difference.

147
00:10:49.030 --> 00:10:54.420
Have there's been a shift between them in time and how can we use this to our advantage.

148
00:10:54.430 --> 00:10:56.910
If there is indeed has been this shift in time.

149
00:10:56.980 --> 00:11:03.160
Well one thing would we could do is we could say OK well you know our queue of S and a doesn't this

150
00:11:03.160 --> 00:11:07.450
new value doesn't equal old so we're going to get rid of the old we'll forget about the old and we'll

151
00:11:07.450 --> 00:11:09.650
just use this as a new value.

152
00:11:09.880 --> 00:11:11.880
But that would not be smart.

153
00:11:11.890 --> 00:11:17.590
And the reason for that is that in our environment the random events can have sometimes happen.

154
00:11:18.070 --> 00:11:24.850
And what if our old QSL Afsane was something that you know consistently happens like 80 percent of the

155
00:11:24.850 --> 00:11:25.490
time.

156
00:11:25.720 --> 00:11:28.650
And then like was represented by what happens 80 percent of the time.

157
00:11:28.690 --> 00:11:33.060
And then this new one just what happened due to randomness.

158
00:11:33.220 --> 00:11:39.820
In that case we're gonna throw away the the one that is responsible for the bulk of the situation and

159
00:11:39.820 --> 00:11:43.680
we're going to replace it with something that happens only 10 or 20 percent of the time.

160
00:11:43.840 --> 00:11:46.920
That wouldn't be the best approach to go.

161
00:11:46.930 --> 00:11:51.970
And that's why that's exactly why we don't want to completely change our Q values.

162
00:11:51.970 --> 00:11:56.860
We want to use like change in step by step a little bit by little bit.

163
00:11:56.860 --> 00:12:00.760
And that's why we're going to use this temporal difference in a specific way.

164
00:12:00.760 --> 00:12:06.490
So we're going to say Here's a formula we're going to take our cue of SLA and we're going to update

165
00:12:06.490 --> 00:12:12.400
it in such a way we're going to take the old value of KUSA and we are going to add alpha time the temporal

166
00:12:12.400 --> 00:12:13.260
difference.

167
00:12:13.360 --> 00:12:15.640
So Alpha is going to be our learning rate.

168
00:12:15.640 --> 00:12:19.410
That's a new parameter that we're introducing that's how quickly is the algorithm learning.

169
00:12:20.020 --> 00:12:26.290
So basically we're taking this difference and whatever it is we're adding it on to our previous kill

170
00:12:26.290 --> 00:12:27.150
in the Senate.

171
00:12:27.160 --> 00:12:30.520
Now this formula probably doesn't make any sense or just by looking.

172
00:12:30.520 --> 00:12:33.980
It doesn't make any sense because I got curious and a here and give us an A here.

173
00:12:34.000 --> 00:12:34.660
It's the same thing.

174
00:12:34.660 --> 00:12:40.260
So probably should negate each other but we want to rewrite this in a bit of a different way.

175
00:12:40.330 --> 00:12:41.620
So I'm just going to show you again.

176
00:12:41.670 --> 00:12:48.010
I'm just adding time to these formulas so here is Q T minus one the previous year.

177
00:12:48.010 --> 00:12:50.270
Q T minus one the previous here's Kutty.

178
00:12:50.310 --> 00:12:53.020
Then you this should be a circle here a circle here as well.

179
00:12:53.060 --> 00:12:54.030
But never mind.

180
00:12:54.160 --> 00:12:56.500
And here we've got Alpha temporal difference.

181
00:12:56.510 --> 00:13:00.550
The new the current temperature difference so you can see what we're doing.

182
00:13:00.550 --> 00:13:08.170
We're saying OK let's take our current cue is going to be equal to all previous Q Plus whatever temporal

183
00:13:08.170 --> 00:13:09.560
difference we found.

184
00:13:09.580 --> 00:13:16.210
Times alpha this formula here is the heart and soul of the cube learning algorithm.

185
00:13:16.240 --> 00:13:21.940
This is how the cube values are update and it's good that we've already learned what Q values are with

186
00:13:21.940 --> 00:13:25.240
Gamma is what R is and what all of this stuff is.

187
00:13:25.360 --> 00:13:30.350
And now all we need to see is that you have a previous Q value.

188
00:13:30.430 --> 00:13:31.910
Yes that's good.

189
00:13:31.930 --> 00:13:37.810
And then what can happen is that when you taken when you actually do take the action when the agent

190
00:13:37.810 --> 00:13:42.540
takes action he will know he will get a reward and he'll end up in a state.

191
00:13:42.550 --> 00:13:46.360
And so based on that he can calculate aha.

192
00:13:46.390 --> 00:13:46.720
OK.

193
00:13:46.720 --> 00:13:53.500
So what is what would have what should have been the Q value of that move that I made.

194
00:13:53.500 --> 00:13:56.350
And now that is this part of the equation.

195
00:13:56.410 --> 00:13:58.300
Subtract the old Q value gets you.

196
00:13:58.300 --> 00:13:59.600
Temporal Difference.

197
00:13:59.710 --> 00:14:05.950
And now you need to take a alpha time simple difference and that's how you get adjusted to value that's

198
00:14:05.950 --> 00:14:07.440
what you're going to adjust Q value.

199
00:14:08.090 --> 00:14:10.220
And I'll just to finish off this.

200
00:14:10.240 --> 00:14:14.830
This is kind of like this is sufficient to understand what's going on but just to clarify things even

201
00:14:14.830 --> 00:14:18.400
more or perhaps maybe confuse things even more.

202
00:14:18.400 --> 00:14:21.990
What do we need to do is we going to take this Temporal Difference or this temporal difference over

203
00:14:22.000 --> 00:14:24.160
here or plug it into this formula.

204
00:14:24.160 --> 00:14:29.860
So we're going to take all of this part and plug it into this formula and end up with a huge equation.

205
00:14:29.860 --> 00:14:32.530
So here we go there is our equation.

206
00:14:32.560 --> 00:14:38.530
So this is the full equation with the temporal difference written out completely.

207
00:14:38.530 --> 00:14:43.480
And the reason I wrote us this out as well first of all you'll probably find this in other literature

208
00:14:43.510 --> 00:14:45.450
if you study it.

209
00:14:45.700 --> 00:14:48.570
And the second thing is that it makes some things a bit more complex.

210
00:14:48.600 --> 00:14:52.240
Yes the form is longer but also makes some things a bit clearer.

211
00:14:52.240 --> 00:14:55.880
So for instance you can see here the role Alpha place.

212
00:14:55.900 --> 00:14:58.210
You can see it better because look at this.

213
00:14:58.210 --> 00:15:00.730
Here you go Q T minus one.

214
00:15:00.730 --> 00:15:01.340
And here you go.

215
00:15:01.350 --> 00:15:03.340
Q T minus one with a negative side.

216
00:15:03.720 --> 00:15:06.880
So if you plug in alpha equals to 1.

217
00:15:06.900 --> 00:15:12.110
If you put a one in here then this will negate with this.

218
00:15:12.150 --> 00:15:15.770
So they will destroy each other and all you'll have left is this park.

219
00:15:16.440 --> 00:15:21.020
And what that means is exactly that situation where we said all right.

220
00:15:21.030 --> 00:15:24.780
So you've got a new value which it should have been.

221
00:15:24.780 --> 00:15:27.200
Let's update our Q value with the new value.

222
00:15:27.230 --> 00:15:32.730
And forget about whatever we had previously and as we discussed isn't the best approach because there

223
00:15:32.850 --> 00:15:39.180
are random events here and we want to update things step by step and on the other hand if you said Alpha

224
00:15:39.180 --> 00:15:40.910
equal to zero.

225
00:15:41.130 --> 00:15:47.250
What happens then is that you completely forget about this whole part and your q t the new one or the

226
00:15:47.250 --> 00:15:49.470
current one is going to be always equal to the previous one.

227
00:15:49.470 --> 00:15:54.480
So you're not gonna be learning anything and that means whatever is happening in the maze doesn't matter

228
00:15:54.510 --> 00:15:59.170
because you've decided and you Kutty value a long time ago and you're just going to keep it.

229
00:15:59.190 --> 00:16:01.670
So that's why alphas shouldn't be zero or should be one.

230
00:16:01.810 --> 00:16:06.870
It should be somewhere in between and it's going to allow you to learn slowly step by step is going

231
00:16:06.870 --> 00:16:12.930
to allow you to as your or the agent as it goes through the maze is gonna get this temporal difference.

232
00:16:12.930 --> 00:16:17.790
And slowly but surely this value is going to get updated and update I'm.

233
00:16:17.970 --> 00:16:25.470
And what will happen eventually is that at some point hopefully the algorithm will converge.

234
00:16:25.650 --> 00:16:30.900
And what that means is that this temporal difference will start becoming closer and closer to zero and

235
00:16:30.900 --> 00:16:35.190
eventually we just will very close to zero or even zero zero zero zero.

236
00:16:35.490 --> 00:16:42.600
And what that means is that every single time your your new acuity value or your new calculated value

237
00:16:43.320 --> 00:16:44.370
what it should have been.

238
00:16:44.400 --> 00:16:49.740
So not this one but what it hypothetically should have enough to take this step will be just equal to

239
00:16:49.740 --> 00:16:54.340
your previous duty value and then one then it's zero and that means when your temporal difference is

240
00:16:54.340 --> 00:17:02.400
zero means your algorithm has converged and it's not really necessary to continue updating what's going

241
00:17:02.400 --> 00:17:02.640
on.

242
00:17:02.640 --> 00:17:06.200
It does necessary to continue updating your Q values.

243
00:17:06.210 --> 00:17:12.720
The caveat here is that the only time probably one of the only times when you would still want to continue

244
00:17:12.750 --> 00:17:19.260
performing this whole you know updating of your Q values if the environment is constantly changing if

245
00:17:19.260 --> 00:17:23.030
not just is not that it just has some randoms to casting events in it.

246
00:17:23.160 --> 00:17:30.150
But the environment itself is modifying is is morphing is changing with time so you continuously need

247
00:17:30.150 --> 00:17:35.850
to learn because it's not possible for you to learn everything and come up with the optimal policy because

248
00:17:35.850 --> 00:17:39.180
the optimal policy is also changing with the environment all the time.

249
00:17:39.180 --> 00:17:44.850
In that case you will need to continue calculating Temporal Difference and calculate the Q values but

250
00:17:44.850 --> 00:17:46.760
other than that that's kind of like an extra complication.

251
00:17:46.770 --> 00:17:49.320
Other than that this is how Q values of data.

252
00:17:49.320 --> 00:17:55.950
So this is the main formula of the Q learning algorithm and this is kind of like the expanded version

253
00:17:55.950 --> 00:18:01.770
of that and now it should all come together makes sense why we have the Belmont equation and not only

254
00:18:01.800 --> 00:18:10.230
what it represents to Google's but also how the agent goes about updating its Q values and finding exactly

255
00:18:10.500 --> 00:18:14.330
what is going on in that environment so it can come up with the optimal policy.

256
00:18:14.580 --> 00:18:21.150
So I know quite a lot to take in but hopefully you enjoyed today cereal and hopefully you were able

257
00:18:21.150 --> 00:18:28.620
to take away the underlying concepts and intuition behind Q values and what's the whole notion of Temporal

258
00:18:28.620 --> 00:18:36.930
Difference is and why it's important why it helps us slowly train our agents and get them to understand

259
00:18:36.990 --> 00:18:38.910
their environments that they're operating in.

260
00:18:39.210 --> 00:18:45.480
And if you'd like to learn a bit more about temporal differences then a very popular paper is learning

261
00:18:45.480 --> 00:18:52.510
to predict by the methods of temporal differences by Richard Sutton of nineteen eighty eight.

262
00:18:52.590 --> 00:18:56.820
We've already had a reference by Richard Sutton as well but this isn't just another one and actually

263
00:18:56.820 --> 00:18:57.480
has a book.

264
00:18:57.480 --> 00:19:04.890
So if you if you get into his writing style and his style of communication then check out his book as

265
00:19:04.890 --> 00:19:05.620
well.

266
00:19:05.750 --> 00:19:08.610
It is kind of like a more expanded version of all of these things.

267
00:19:08.610 --> 00:19:11.590
I haven't read the book but that's what I'm imagining.

268
00:19:11.850 --> 00:19:17.820
At the same time this is the link to this to the paper and you can learn a bit more about or probably

269
00:19:17.820 --> 00:19:21.030
a lot more about temporal differences there.

270
00:19:21.240 --> 00:19:24.080
And I hope you enjoyed today's tutorial and look forward to you next.

271
00:19:24.210 --> 00:19:26.180
Until then enjoy a.
