WEBVTT
1
00:00:00.890 --> 00:00:04.130
Hello and welcome back to the course on artificial intelligence.

2
00:00:04.130 --> 00:00:09.020
All right so I hope you're enjoying a tutorial so far we're nearly done with the intention you soon

3
00:00:09.020 --> 00:00:12.890
very soon get to the practical side of things we've just got a few little things that we need to cover

4
00:00:12.890 --> 00:00:13.290
off.

5
00:00:13.450 --> 00:00:20.780
All right so previously we talked about how we add neural networks into this whole equation of Q learning

6
00:00:20.870 --> 00:00:25.640
and tequila into the next step and turn it into deep kill learning.

7
00:00:25.640 --> 00:00:32.900
And today we're going to add a an extra important feature which you will be coding in the practical

8
00:00:32.900 --> 00:00:38.870
side of things so headline and I decided that it's important for us to cover it often in the intuition

9
00:00:38.870 --> 00:00:42.340
side of things so that you're more prepared for it when it comes in the coding side of things.

10
00:00:42.350 --> 00:00:46.390
So as we discuss we've got the network there.

11
00:00:46.430 --> 00:00:47.900
There's two parts that happen.

12
00:00:47.900 --> 00:00:54.620
First of all it's the learning so the network actually learns with every new state and it slowly updates

13
00:00:54.620 --> 00:00:58.780
its weights to get better and better and better at dealing with this environment.

14
00:00:58.820 --> 00:01:06.560
And then there is the acting inside the state so after the key values have been counted in the state.

15
00:01:06.560 --> 00:01:08.180
Then once you value selected.

16
00:01:08.180 --> 00:01:14.720
So today we're still going to talk about the learning part we're going to come up with a interesting

17
00:01:14.720 --> 00:01:16.330
feature that's going to.

18
00:01:16.340 --> 00:01:22.700
Well we're not going to come up with this feature ourselves but we'll talk about a feature that is very

19
00:01:23.000 --> 00:01:29.150
important for a deep Q learning and that feature is called Experience replay.

20
00:01:29.650 --> 00:01:29.960
All right.

21
00:01:29.990 --> 00:01:31.820
So here is our network.

22
00:01:31.820 --> 00:01:36.850
So we've just copied it over here we've got that a loss that is calculated.

23
00:01:36.860 --> 00:01:43.070
The bottom is back propagate through a network and let's have a look at an example of what happens to

24
00:01:43.070 --> 00:01:45.170
understand the problem that we're dealing with a bit better.

25
00:01:45.620 --> 00:01:53.020
So here is an example actually from this course this is a screenshot shot exactly from this course.

26
00:01:53.030 --> 00:01:54.750
This is what you'll be programming.

27
00:01:54.770 --> 00:02:02.270
This is a self-driving car that is driving through this thing along this road and it has to learn how

28
00:02:02.270 --> 00:02:03.720
to navigate this road.

29
00:02:03.740 --> 00:02:08.060
And so what it is as we discussed previously what is this.

30
00:02:08.060 --> 00:02:09.240
In what is this state.

31
00:02:09.260 --> 00:02:15.890
And of course the state is going to be x1 x2 alone will describe it bit in a lot more detail what the

32
00:02:15.890 --> 00:02:24.200
state is it is going to be a couple of parameters which relate to the angle of the car and some relative

33
00:02:24.350 --> 00:02:28.880
parameters what the sensors are reading and so on so there's going to be more parameters than that to

34
00:02:28.880 --> 00:02:29.770
describe the state.

35
00:02:29.780 --> 00:02:34.070
But nevertheless it's going to be a vector of values going to go through a neural network and then on

36
00:02:34.070 --> 00:02:36.720
the output you're going to have some Q values again.

37
00:02:36.740 --> 00:02:39.790
There'll be a different depending on the environment.

38
00:02:39.800 --> 00:02:44.290
It can be a different number of actions possible actions.

39
00:02:44.450 --> 00:02:49.070
But we're just going to for simplicity's sake leave it at for just for us two people to understand a

40
00:02:49.070 --> 00:02:50.420
bit better what's going on here.

41
00:02:50.750 --> 00:02:55.610
So in this case what is the question is so far what is this.

42
00:02:55.610 --> 00:03:03.470
This inputs into this neural network or more specifically how often do we trigger this neural net.

43
00:03:03.470 --> 00:03:05.060
How often does this neural network go through.

44
00:03:05.060 --> 00:03:11.360
Well every time the car ends up in a new state so the car makes a move it ends up in a new state and

45
00:03:11.480 --> 00:03:12.600
then everything go.

46
00:03:12.620 --> 00:03:17.360
All that data all that information from about the state goes through the network key values are calculated

47
00:03:17.590 --> 00:03:24.110
error is this error is calculated based on what we discussed in previous tutorials this error is back

48
00:03:24.110 --> 00:03:26.050
propagated through a network which are updated.

49
00:03:26.080 --> 00:03:32.090
Then the car selects which action wants to take makes that move ends up in a in a new state in the new

50
00:03:32.090 --> 00:03:32.530
state.

51
00:03:32.540 --> 00:03:34.400
Everything starts over again.

52
00:03:34.400 --> 00:03:38.000
And so basically this happens every time the car is in a new state.

53
00:03:38.390 --> 00:03:39.800
Well let's have a look at this example.

54
00:03:39.830 --> 00:03:46.190
I specifically took this screenshot because it looks it's very well illustrates the problem that is

55
00:03:46.190 --> 00:03:51.380
addressed through experience replay and expands replay is not just something that we use in this course

56
00:03:51.380 --> 00:03:58.370
or in this specific problem it is something that you will see used throughout like on and on and over

57
00:03:58.370 --> 00:04:05.080
and over again in artificial intelligence algorithms because it is so powerful and it's so important.

58
00:04:05.090 --> 00:04:11.360
So look at this car this car in this problem or in this environment its goal is come from go from here

59
00:04:11.360 --> 00:04:12.350
to here and back.

60
00:04:12.350 --> 00:04:17.640
Its goal is to navigate its way here here without crossing these walls which are made of sand.

61
00:04:17.750 --> 00:04:19.580
And so the car starts over here.

62
00:04:19.580 --> 00:04:25.040
It went down and like its reward is based on you know how close it is to study.

63
00:04:25.070 --> 00:04:29.490
So the car went from here it went down and kept going like this like this like this like this.

64
00:04:29.490 --> 00:04:31.350
Along this wall along this wall.

65
00:04:31.490 --> 00:04:34.860
And what is it going to do next is going to turn is going to keep going.

66
00:04:34.880 --> 00:04:37.220
Well what we wanted to do is keep going here.

67
00:04:37.610 --> 00:04:39.390
But let's think about it for a second.

68
00:04:39.500 --> 00:04:44.180
Once it got to this wall every single time it moves forward it moves forward.

69
00:04:44.180 --> 00:04:47.990
It moves forward it moves forward moves forward it moves forward it moves forward and so on it moves

70
00:04:47.990 --> 00:04:48.300
forward.

71
00:04:48.530 --> 00:04:53.150
So there might be light depending on the structure environment it could be like a hundred moves here

72
00:04:53.150 --> 00:04:54.850
or 50 moves here.

73
00:04:54.920 --> 00:04:59.910
It just keeps moving forward forward forward forward forward forward and nothing changes nothing really

74
00:04:59.910 --> 00:05:03.290
changes yes it gets a way further away from this started closer to this target.

75
00:05:03.290 --> 00:05:04.110
That's lovely.

76
00:05:04.110 --> 00:05:09.960
But in terms of their surrounding environment not many things are changing it's still that same wall.

77
00:05:10.020 --> 00:05:15.390
If you sitting in the car you've probably seen the situation when you're driving in the whatever you're

78
00:05:15.390 --> 00:05:21.150
seeing is like the environment is so monotonous that you're just seeing kind of the same thing is just

79
00:05:21.150 --> 00:05:21.960
passing by.

80
00:05:22.320 --> 00:05:26.640
Because I imagine you're driving through a desert and you're just seeing the same thing it's the same

81
00:05:26.640 --> 00:05:32.790
sign that's the same sound nothing is happening nothing is changing and so best but every single time

82
00:05:32.910 --> 00:05:36.940
we're putting that state that new state into here.

83
00:05:36.970 --> 00:05:41.850
Yes of course something might be changing for us as you're driving the car and your G.P.S. is showing

84
00:05:41.850 --> 00:05:43.490
you're closer to your destination.

85
00:05:43.500 --> 00:05:49.230
So one of these inputs the strategy but a lot of these other inputs the sensors for instance which are

86
00:05:49.230 --> 00:05:55.320
on the car they're not changing and therefore as you're driving through in this day to put input the

87
00:05:55.320 --> 00:06:01.620
inputs into your neural network here here here here here here here and here and here all the time the

88
00:06:01.620 --> 00:06:02.910
inputs are pretty much the same.

89
00:06:03.210 --> 00:06:10.380
And so if you keep inputting the same inputs the same values the same vector or very similar vectors

90
00:06:10.470 --> 00:06:17.910
into your network because there is no variety the car will learn very well one thing you'll learn very

91
00:06:17.910 --> 00:06:21.640
well how to drive along this wall which is on its right.

92
00:06:21.690 --> 00:06:26.940
And so that's how the network will update and will get rewarded will slowly start getting rewarded for

93
00:06:26.940 --> 00:06:32.490
driving so well it will be like OK so from here we'll be start learning Oh I'm doing so good I'm doing

94
00:06:32.490 --> 00:06:34.330
him better I'm doing it better it will.

95
00:06:34.440 --> 00:06:41.850
It will have this this false perception that it's actually doing very well even though it only learns

96
00:06:41.880 --> 00:06:43.340
how to drive along this wall.

97
00:06:43.440 --> 00:06:48.480
And so the neural network will become very adapted to driving along this wall and then all of a sudden

98
00:06:48.480 --> 00:06:54.720
there's this curve and the car doesn't know what to do and it completely doesn't fit in with this neural

99
00:06:54.720 --> 00:06:55.270
network.

100
00:06:55.380 --> 00:07:01.800
And even if it does adjust somehow let's hypothetically say it passes a spa and then it ends up on this

101
00:07:01.800 --> 00:07:02.150
wall.

102
00:07:02.190 --> 00:07:05.270
Same thing is gonna happen is gonna drive from here here here.

103
00:07:05.280 --> 00:07:05.520
Okay.

104
00:07:05.520 --> 00:07:10.800
Now the neural network is restructuring itself to adapt to this wall and then bam this thing happens

105
00:07:10.860 --> 00:07:16.020
and then even if somehow it gets past that it will drive past this thing and then same thing along these

106
00:07:16.020 --> 00:07:16.230
lines.

107
00:07:16.230 --> 00:07:23.490
So basically this is a very vivid example of the problem that we are what we have is that because the

108
00:07:23.490 --> 00:07:29.550
way we're using the neural net updating it with every single state once we have lots of consecutive

109
00:07:29.550 --> 00:07:30.830
steps they don't even have to be the same.

110
00:07:30.840 --> 00:07:40.590
But there is in environments it's normal that consecutive states are somehow correlated or somehow interdependent

111
00:07:40.950 --> 00:07:45.500
and we don't want that interdependency to bias our network.

112
00:07:45.510 --> 00:07:51.840
We don't want the card to just learn how to drive along like a straight line or along a curved line

113
00:07:52.230 --> 00:08:01.710
or like anything that you think you can think of in in life where an agent would be navigate environment

114
00:08:01.710 --> 00:08:10.560
where we can think of correlated or interdependent states that come after another that can really mess

115
00:08:10.560 --> 00:08:12.080
up your neural network.

116
00:08:12.150 --> 00:08:17.880
If you're just going to let the agent learn from that and that's where experience replay comes in what

117
00:08:18.060 --> 00:08:21.810
what happens and experience replay is these experiences.

118
00:08:21.810 --> 00:08:29.430
So these states that it's in one two three however many 50 states here in a row they don't get put through

119
00:08:29.430 --> 00:08:31.450
the network right away.

120
00:08:31.450 --> 00:08:36.000
They're actually saved into memory of the agent.

121
00:08:36.120 --> 00:08:41.040
And so for instance it saves all these and saves all these and some at some point once it reaches a

122
00:08:41.040 --> 00:08:46.020
certain threshold we shall be able to code and Alan we'll show you how to do that once it reaches a

123
00:08:46.020 --> 00:08:50.310
certain threshold then the agent decides for itself.

124
00:08:50.330 --> 00:08:50.560
OK.

125
00:08:50.590 --> 00:08:51.240
Sam to learn.

126
00:08:51.240 --> 00:08:56.760
I have I have this batch of experiences that I have and now I'm going to learn from that batch and so

127
00:08:56.820 --> 00:09:03.870
randomly selects a uniformly distributed and uniformly is key is important here because that's something

128
00:09:03.870 --> 00:09:06.640
we'll we'll talk about on the next slide.

129
00:09:06.750 --> 00:09:07.390
We'll book.

130
00:09:07.410 --> 00:09:12.390
We'll mention that but it takes a uniformly distributed sample.

131
00:09:12.390 --> 00:09:15.630
So basically all experiences are considered to be equal.

132
00:09:15.630 --> 00:09:23.280
It takes a uniformly distributed sample from that batch of experiences that it has and then it goes

133
00:09:23.280 --> 00:09:24.690
through them and it learns from them.

134
00:09:24.690 --> 00:09:29.000
So it doesn't take all the extremes it just takes it uniformly distribute samples it might take a couple

135
00:09:29.020 --> 00:09:34.980
from here a couple from here a couple from here and each experience is characterized by the state it

136
00:09:34.980 --> 00:09:43.650
was in the action that it took the state it ended up in and the reward it it achieved through that action

137
00:09:43.650 --> 00:09:44.760
in that specific state.

138
00:09:44.760 --> 00:09:50.100
So four elements in each experience state one action state 2 and reward.

139
00:09:50.100 --> 00:09:54.690
And so it takes all those experiences and then it passes them through the network and it learns and

140
00:09:54.690 --> 00:10:05.170
that way it it breaks the pattern of that bias which comes from the sequential nature of the experience

141
00:10:05.220 --> 00:10:07.800
if you were to put them through the network one after the other.

142
00:10:08.280 --> 00:10:11.850
So that's the main focus of experience replay.

143
00:10:11.850 --> 00:10:17.670
That's that's what the problem it addresses and another benefit of exchange replay is that sometimes

144
00:10:17.670 --> 00:10:22.340
in an environment like this you might have very valuable rare experiences.

145
00:10:22.350 --> 00:10:23.940
So for instance I don't know.

146
00:10:23.940 --> 00:10:26.280
Let's say let's look at this corner right.

147
00:10:26.280 --> 00:10:28.660
This is this is a right corner all right.

148
00:10:28.680 --> 00:10:29.690
And a very sharp one.

149
00:10:29.690 --> 00:10:30.800
How many is sharp.

150
00:10:30.810 --> 00:10:32.590
So it'll be coming from here.

151
00:10:33.000 --> 00:10:35.580
Assuming it's going to be hugging this corner.

152
00:10:35.580 --> 00:10:40.890
So having you sharp right corner is do we have in this in this where we only have one right corner here

153
00:10:40.890 --> 00:10:43.480
and one right corner here.

154
00:10:43.620 --> 00:10:43.890
Right.

155
00:10:43.890 --> 00:10:46.320
So when it's coming this way that's the right corner.

156
00:10:46.320 --> 00:10:48.540
And then when it's going back it's a sharp right corner here.

157
00:10:48.570 --> 00:10:53.010
So and this one's not sharp this one is sharp so there's only one opportunity in the whole environment

158
00:10:53.550 --> 00:10:56.910
to learn from a sharp right corner.

159
00:10:56.920 --> 00:11:03.030
And that's a very important experience because it might get really good at driving along straight lines

160
00:11:03.030 --> 00:11:08.850
get really good at doing like soft corners like that like that but and then it will keep messing up

161
00:11:08.850 --> 00:11:14.880
this sharp right corner simply because simply because it's own it doesn't have that much opportunity

162
00:11:14.880 --> 00:11:18.690
to learn from it and so therefore it will learn everything else pretty quickly but it'll take a long

163
00:11:18.690 --> 00:11:20.150
time to learn this right corner.

164
00:11:20.160 --> 00:11:25.980
It's a very simplified example is a very simplified explanation but it illustrates the concept that

165
00:11:26.220 --> 00:11:30.120
sometimes they're rare experiences which are which can be valuable.

166
00:11:30.210 --> 00:11:35.820
And if you're just doing a simple neural network where you're putting in your values here and you know

167
00:11:35.820 --> 00:11:40.860
they're going through and you know like even if we forget about that problem of the sequential nature

168
00:11:40.860 --> 00:11:46.080
of experiences and how they can be interdependent or correlated and even forget about that for a second

169
00:11:46.770 --> 00:11:51.810
what happens is once you put an experience in it goes through then it looks updated then you instantly

170
00:11:51.810 --> 00:11:52.230
forget forgive.

171
00:11:52.260 --> 00:11:54.360
Forget about that experience you move on to the next one.

172
00:11:54.360 --> 00:11:56.050
That's just how the neural networks.

173
00:11:56.160 --> 00:11:59.550
Then you move on to the next state the next state the next state the next experience next experience

174
00:11:59.550 --> 00:12:01.080
their experience and so on.

175
00:12:01.110 --> 00:12:05.700
So this right corner as soon as it goes through a network it's gone there and you don't have any memory

176
00:12:05.700 --> 00:12:07.500
of that valuable experience.

177
00:12:07.500 --> 00:12:14.160
Whereas we've experienced replay because you're putting these experiences into batches you can organize

178
00:12:14.160 --> 00:12:15.630
your bash as a rolling window.

179
00:12:15.630 --> 00:12:20.300
So for instance you could have like 100 batches so 100 experiences in your batch.

180
00:12:20.300 --> 00:12:27.130
So when it's coming back from here it's as soon as it has this recorded this experience in its batch.

181
00:12:27.330 --> 00:12:30.330
Then like at some point it's right runs.

182
00:12:30.330 --> 00:12:35.160
It takes a uniform distribution from its batch of experiences and then there's a rolling window sort

183
00:12:35.200 --> 00:12:37.950
forgets these experiences but then it keeps these experiences.

184
00:12:37.950 --> 00:12:43.890
And then again it learns from once it's here it learns from this batch and then once it's here if it

185
00:12:43.890 --> 00:12:47.970
gets all the way up to here but then it has a batch of experiences like that.

186
00:12:47.970 --> 00:12:55.500
So therefore a node learns from these experiences and that way what you are getting is that this right

187
00:12:55.500 --> 00:13:01.050
hand corner might come up several times in its learning process because it was in that batch when the

188
00:13:01.050 --> 00:13:06.600
batch was like this around there then it was in the batch here in the batch here so it came up in several

189
00:13:06.600 --> 00:13:11.380
batches because of a batch might be updated as a rolling window of experience.

190
00:13:11.400 --> 00:13:14.570
So the older experiences get kicked out the newer experiences are added.

191
00:13:14.580 --> 00:13:16.140
And then again older recipients get it.

192
00:13:16.380 --> 00:13:23.010
So an experience it stays in the batch for quite some time and the car or agent can learn from that

193
00:13:23.010 --> 00:13:24.120
experience several times.

194
00:13:24.150 --> 00:13:27.210
So that's another advantage of experience replay.

195
00:13:27.510 --> 00:13:33.420
And of course the final advantage is experience replay gives you an opportunity to learn from more experiences

196
00:13:34.170 --> 00:13:39.600
than if you were just learning for one at a time because you have that batch and therefore and a rolling

197
00:13:39.600 --> 00:13:47.100
window and therefore even if your environment is limited to experience your experience replay approach

198
00:13:47.130 --> 00:13:53.730
can help you learn faster and instead of just redoing their version many many many times you can learn

199
00:13:53.730 --> 00:13:57.750
faster because you don't have to redo it you have those experiences saved.

200
00:13:57.750 --> 00:13:59.850
So those are the main advantages of expanded replay.

201
00:13:59.850 --> 00:14:06.320
Let's recap on then we've got that we're breaking that pattern of independence and correlation of sequential

202
00:14:06.320 --> 00:14:07.820
experiences.

203
00:14:07.820 --> 00:14:13.520
We save rare experiences which might be important and therefore we can learn from them more often and

204
00:14:14.300 --> 00:14:23.360
we can learn in environments we can learn foster environments which are experience which have a shortage

205
00:14:23.360 --> 00:14:28.310
of experiences which don't have that many experiences that the agent goes through and still we can be

206
00:14:28.310 --> 00:14:28.930
able to learn it.

207
00:14:29.300 --> 00:14:32.410
So that is what experience replay is all about.

208
00:14:32.420 --> 00:14:38.240
If you'd like to read a bit more than this there's an interesting article published by deep mind in

209
00:14:38.240 --> 00:14:46.640
2016 is called prioritized experience replay and it talks about why why are we using a uniform distribution

210
00:14:46.640 --> 00:14:50.450
to select our experiences from the experienced batch.

211
00:14:50.450 --> 00:14:55.490
Why don't we find a better way to select our experiences and prioritize some of the experiences which

212
00:14:55.490 --> 00:14:57.010
we feel that are important.

213
00:14:57.050 --> 00:14:58.140
So it's quite an interesting thing.

214
00:14:58.140 --> 00:15:07.670
So in this case you will be able to not only reinforce or not only reinforce your knowledge on experience

215
00:15:07.670 --> 00:15:14.450
replay but you'll actually be able to move with the cutting edge of technology so this is 2016 and published

216
00:15:14.450 --> 00:15:15.070
by deep minds.

217
00:15:15.070 --> 00:15:21.530
It's a very recent very powerful paper so you'll be able to actually explore the limits or explore even

218
00:15:21.530 --> 00:15:24.440
further this algorithm and take it to the next level.

219
00:15:24.470 --> 00:15:30.830
So I'll leave it up to you to find out why and how we can change the uniform submission to a different

220
00:15:30.830 --> 00:15:33.890
approach to expense replay from this paper if you'd like.

221
00:15:33.890 --> 00:15:39.170
And I hope you enjoyed this material and now we know what experience replay is and we can confidently

222
00:15:39.170 --> 00:15:42.890
use it in our practical circles and I look forward to see you next time.

223
00:15:42.890 --> 00:15:44.460
Until then enjoy a.
