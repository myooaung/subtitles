1
00:00:00,550 --> 00:00:03,160
Hello everyone and welcome to this python tutorial.

2
00:00:04,150 --> 00:00:11,530
In this part of this section we are going to be focusing on a new technique called fine tuning which

3
00:00:11,530 --> 00:00:14,360
is directly connected to the transfer learning.

4
00:00:14,440 --> 00:00:22,060
In a nutshell it is used after the transfer learning and sometimes it can improve upon the results achieved

5
00:00:22,090 --> 00:00:25,750
using only France for learning for transfer learning.

6
00:00:25,750 --> 00:00:32,680
We used a pre train network that we have frozen and just trained the custom part of it to be suited

7
00:00:32,680 --> 00:00:34,720
for the task that we're trying to solve.

8
00:00:36,300 --> 00:00:43,320
By the base network at this point it doesn't have any features connected to our specific task if we

9
00:00:43,320 --> 00:00:45,940
have very specific task to solve.

10
00:00:46,080 --> 00:00:55,480
It is handy to have features from the data set inside the base network as well enter fine tuning in

11
00:00:55,480 --> 00:00:56,730
the fine tuning step.

12
00:00:56,770 --> 00:01:03,880
We use the same heat texture that we did in the transfer learning although we unfreeze a few top players

13
00:01:03,940 --> 00:01:05,550
from the base network.

14
00:01:05,680 --> 00:01:11,590
This way we allow the network to learn even further about the custom data set.

15
00:01:11,620 --> 00:01:16,180
There are a few pointers that I can give you a critical one.

16
00:01:16,180 --> 00:01:19,930
Do not use fine tuning on the whole network.

17
00:01:20,110 --> 00:01:22,670
Only a few top layers are enough.

18
00:01:23,050 --> 00:01:25,800
In most cases they are more specialized.

19
00:01:25,840 --> 00:01:35,290
When CNN is trained lower layers are going to learn general features lines edges corners and so on but

20
00:01:35,290 --> 00:01:40,840
the top players will get more abstract features about the data set as well.

21
00:01:40,840 --> 00:01:44,660
For example eyes tails ears and so on.

22
00:01:45,680 --> 00:01:53,830
And the goal of fine tuning is to adopt those specific parts of the network to our custom data set and

23
00:01:53,830 --> 00:01:59,800
the second point their data can give you is start with the fine tuning after you're finished with the

24
00:01:59,800 --> 00:02:07,120
transfer learning step if we try to perform fine tuning immediately in our custom head is not trained

25
00:02:07,810 --> 00:02:16,170
gradients will be much different between custom had and our unfrozen layers from the base network to

26
00:02:16,170 --> 00:02:18,670
perform the fine tuning of the network.

27
00:02:18,690 --> 00:02:24,060
The first step is to unfreeze a few top players and to achieve this.

28
00:02:24,210 --> 00:02:34,720
We are going to unfreeze the whole thing so right base model dot trainable equals to true before we

29
00:02:34,720 --> 00:02:38,010
choose how many layers to fine tune.

30
00:02:38,080 --> 00:02:42,880
Let us count how many layers we have in our mobile Net.

31
00:02:42,980 --> 00:02:50,100
You can copy this simple print statement from me or write her own execute a cell and as you can see

32
00:02:50,160 --> 00:02:59,370
in our base model alone we have one hundred fifty five layers which is a lot define a variable called

33
00:02:59,370 --> 00:03:07,170
fine tuned at this will store an index of the starting layer from which point on we are going to fine

34
00:03:07,170 --> 00:03:09,670
tune for this task.

35
00:03:09,960 --> 00:03:19,920
Let's set this to 100 so we are going to find you in all layers from 100 to 150 5 and to achieve this.

36
00:03:19,960 --> 00:03:22,810
If you remember we unfrozen a whole network.

37
00:03:22,810 --> 00:03:29,920
We need to freeze all areas before that so let's create the for loop that will iterate fruit all layers

38
00:03:29,920 --> 00:03:31,560
in our base network.

39
00:03:31,780 --> 00:03:43,710
So write for layer in base model both layers and in brackets just set column fine tuned at in this for

40
00:03:43,710 --> 00:03:44,600
loop right.

41
00:03:44,640 --> 00:03:48,780
Layer that trainable equals to false.

42
00:03:48,780 --> 00:03:54,060
This code will freeze all layers up to 100 and that's it for this video.

43
00:03:54,150 --> 00:03:58,410
If you have any questions or comments please post them in the comments section.

44
00:03:58,590 --> 00:04:00,720
Otherwise I've seen the next tutorial.
