1
00:00:00,780 --> 00:00:02,780
Welcome back.

2
00:00:02,910 --> 00:00:07,500
In the next couple of lessons I've got a really exciting topic for you.

3
00:00:07,530 --> 00:00:15,840
We're gonna be talking about an LP natural language processing natural language processing is a huge

4
00:00:15,840 --> 00:00:16,910
field.

5
00:00:16,980 --> 00:00:23,340
It used to be a subfield of artificial intelligence actually but it moved more and more into the domain

6
00:00:23,400 --> 00:00:32,340
of machine learning and an LP is big business to all sorts of things fall into the domain of natural

7
00:00:32,340 --> 00:00:33,960
language processing.

8
00:00:34,050 --> 00:00:41,940
For example it covers things like search sentiment analysis of tweets or reviews Google AdWords automatic

9
00:00:41,940 --> 00:00:51,990
translation spellcheck auto correct Siri Alexa you name it as you can guess an LP is what most of Google's

10
00:00:52,050 --> 00:00:54,190
earnings actually depend on.

11
00:00:54,480 --> 00:01:02,300
Now how are we going to use an LP for our naive based classifier Well we're gonna use it to prepare

12
00:01:02,480 --> 00:01:06,130
a piece of text for our learning algorithm.

13
00:01:06,260 --> 00:01:14,400
We have to convert our email bodies into a form that the algorithm can understand and this means pre

14
00:01:14,400 --> 00:01:17,540
processing our text.

15
00:01:17,580 --> 00:01:20,740
Now what kind of things do I mean by pre processing.

16
00:01:21,360 --> 00:01:23,690
Here's the high level overview.

17
00:01:23,700 --> 00:01:29,040
First off we're gonna start converting all our text to lower case.

18
00:01:29,070 --> 00:01:35,400
Second we're gonna token ize our text meaning we're gonna split up the individual words in a sentence.

19
00:01:35,400 --> 00:01:41,790
Third we're gonna be removing the stop words by stop words I mean very common English words like the

20
00:01:41,790 --> 00:01:48,740
word the which is the to convey grammar rather than meaning next we're also going to strip out the H

21
00:01:48,740 --> 00:01:51,800
to male tags that are in the emails.

22
00:01:52,100 --> 00:01:57,870
A lot of the emails are not written in plain text but contain a lot of H2 mail formatting which we're

23
00:01:57,870 --> 00:02:00,120
not going to feed into our algorithm.

24
00:02:00,120 --> 00:02:06,560
Next we're gonna do some words stemming and that means converting the individual words to their STEM

25
00:02:06,560 --> 00:02:07,710
word.

26
00:02:07,710 --> 00:02:16,350
So for example if you have the words going goes and go then all of these words actually share the same

27
00:02:16,530 --> 00:02:23,340
words then it's only really the grammar that changes their spelling by stemming the words we're able

28
00:02:23,340 --> 00:02:25,750
to treat them all as the same word.

29
00:02:25,770 --> 00:02:31,830
And lastly we're also going to remove the punctuation and that is because as you can tell our naive

30
00:02:31,860 --> 00:02:34,880
based classifier will ignore the grammar.

31
00:02:34,980 --> 00:02:39,270
Now without further ado let's get started.

32
00:02:39,280 --> 00:02:39,940
All right.

33
00:02:39,940 --> 00:02:47,500
So I'm going to add a few markdown cells once again in Jupiter so that we can find this section really

34
00:02:47,500 --> 00:02:49,380
easily when we're scrolling through it.

35
00:02:49,510 --> 00:03:02,160
So I'll call the first heading natural language processing with two s's not three and then I'll add

36
00:03:02,220 --> 00:03:10,100
a subheading that reads text pre processing.

37
00:03:10,170 --> 00:03:15,870
Now the first step is normalizing the casing of the letters.

38
00:03:15,870 --> 00:03:24,340
Very often the case of the words should not matter if I search for what is the airspeed velocity of

39
00:03:24,460 --> 00:03:33,630
an unlit and swallow then if I were to type what is the airspeed velocity of an unlabeled swallow.

40
00:03:33,630 --> 00:03:40,460
Now now even though that is horrible to read the answer to this vitally important question should not

41
00:03:40,460 --> 00:03:44,940
depend on the upper or lower casing of my letters.

42
00:03:45,320 --> 00:03:51,080
And you can also verified home that when you type in a search query Google completely ignores the upper

43
00:03:51,080 --> 00:03:57,690
casing of your letters the casing of the letters doesn't affect our search results.

44
00:03:57,690 --> 00:04:05,330
And similarly for our spam classifier we will treat the words loan or Viagra the same way regardless

45
00:04:05,390 --> 00:04:11,990
whether they're spelled with uppercase letters or lower case letters so coming back to our Python code

46
00:04:12,830 --> 00:04:14,150
suppose we have a message.

47
00:04:14,210 --> 00:04:26,100
We have some sort of string that reads all work and no play makes Jack a dull boy full stop.

48
00:04:27,210 --> 00:04:32,700
How can we convert all of these letters to lowercase.

49
00:04:32,700 --> 00:04:42,210
How can we ignore the casing of the words in this string Hall Python strings have a handy little function

50
00:04:42,570 --> 00:04:53,880
called lower so MSE dot lower parentheses will actually convert all the letters in the string to lower

51
00:04:53,880 --> 00:05:03,570
case so you can see that Jack becomes lowercase and the word for all also becomes lowercase so converting

52
00:05:03,570 --> 00:05:11,400
to lower case is one kind of text pre processing that you can do now for a lot of other pre processing

53
00:05:11,400 --> 00:05:20,370
that we're going to do we're going to use a python package called The Natural Language toolkit or an

54
00:05:20,430 --> 00:05:28,950
L T.K. the Web site for this module looks like this it's on an al T.K. dot org and this is actually

55
00:05:28,950 --> 00:05:38,040
a package that almost every professional in the MLP field will use at some point for their natural language

56
00:05:38,040 --> 00:05:39,420
processing needs.

57
00:05:39,660 --> 00:05:46,380
The NL T.K. package can do a huge number of things and we're gonna start using it with some of the fundamentals

58
00:05:46,920 --> 00:05:52,030
namely pre processing our text so that our machine learning algorithm can use it.

59
00:05:52,410 --> 00:05:58,680
Now since we're gonna be using the multi K resources I'm going to add a very quick section heading here

60
00:05:59,190 --> 00:06:13,230
to download the multi K resources and those include something called a token Nisar and a list of stop

61
00:06:13,230 --> 00:06:21,820
words amongst other things but before I do that I'm going to import the package itself along with a

62
00:06:21,820 --> 00:06:31,710
couple of the tools someone to come up here to my notebook imports and then I'd say input an L T K and

63
00:06:31,710 --> 00:06:38,900
then from an LTE K Dot stem we're going to import the port of.

64
00:06:40,670 --> 00:06:45,350
From multi K Dot corpus

65
00:06:49,010 --> 00:06:51,800
we're going to import stop words

66
00:06:57,480 --> 00:07:09,070
and from an LTE K Dot token eyes we're going to import word underscore token nice.

67
00:07:09,200 --> 00:07:13,080
I think this will do for now when it import the package as a whole.

68
00:07:13,130 --> 00:07:19,520
And then we're going to import three additional pieces of functionality the port toss them a stop words

69
00:07:19,970 --> 00:07:30,650
and a word token either so when it shift into on the cell and and scroll down here to our section where

70
00:07:30,650 --> 00:07:36,530
I'm going to show you how to download the healthy K resources and this is what we're going to do in

71
00:07:36,530 --> 00:07:40,770
the next lesson two token eyes outwards.

72
00:07:41,240 --> 00:07:41,960
I'll see you there.
