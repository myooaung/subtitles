1
00:00:00,500 --> 00:00:09,960
Okay so step one was calculating the basic probabilities it was gathering information on the frequencies

2
00:00:09,960 --> 00:00:15,600
of an event and figuring out the probability of that event happening.

3
00:00:15,600 --> 00:00:23,280
Based on that in step two we're going to talk about a concept called the joint probability.

4
00:00:23,280 --> 00:00:29,610
So you flip a coin twice what are the chances that you get heads both times.

5
00:00:29,610 --> 00:00:34,270
What's the probability that you get heads two times in a row.

6
00:00:34,650 --> 00:00:41,610
The way you see this written in mathematical syntax is with this little upside down use symbol this

7
00:00:41,610 --> 00:00:43,440
symbol stands for.

8
00:00:43,440 --> 00:00:50,960
And also every textbook out there will refer to an event like the result of a coin flip with a letter.

9
00:00:51,060 --> 00:00:59,610
So you'll see this written as what's the probability of A and B A is getting heads on the first flip

10
00:00:59,970 --> 00:01:03,630
and B is getting heads on the second flip.

11
00:01:03,630 --> 00:01:09,900
In our case if probability is a new topic for you then maybe just quickly pause the video and figure

12
00:01:09,900 --> 00:01:15,050
out what the probability is of getting heads two times in a row.

13
00:01:15,130 --> 00:01:17,140
Did you have a go.

14
00:01:17,150 --> 00:01:20,470
If so how did you figure this out.

15
00:01:20,480 --> 00:01:21,870
What was your approach.

16
00:01:22,280 --> 00:01:28,250
One way that you can solve this kind of problem is to draw matrix.

17
00:01:28,250 --> 00:01:36,980
This matrix has all the possible combinations in it and there you can see that there's a total of four

18
00:01:37,160 --> 00:01:42,710
possible outcomes with the coin flipping problem from our little chart here.

19
00:01:42,710 --> 00:01:52,370
We can see that both coins showing heads only happens 1 in 4 times so therefore the probability of getting

20
00:01:52,370 --> 00:01:56,590
heads on two flips in a row is 25 percent.

21
00:01:57,050 --> 00:01:58,470
But you know what.

22
00:01:58,520 --> 00:02:03,260
There's a better way to calculate a joint probability.

23
00:02:03,260 --> 00:02:12,530
All we have to do in our coin flipping example is to multiply the probability of getting heads times

24
00:02:12,830 --> 00:02:20,960
the probability of getting heads since we have a 50/50 chance of getting heads it's zero point five

25
00:02:21,050 --> 00:02:27,200
times zero point five which is zero point to five or 25 percent.

26
00:02:27,200 --> 00:02:34,260
In other words we're multiplying the probability of A times the probability of B.

27
00:02:34,550 --> 00:02:40,270
And that's how we can get the probability of A and B.

28
00:02:40,290 --> 00:02:45,590
Now I think this formula is really really handy when you don't want to draw a huge matrix because say

29
00:02:45,590 --> 00:02:52,580
you want to calculate the probability of getting heads on a coin flip four times in a row.

30
00:02:52,700 --> 00:02:58,850
Well simply by applying this formula you know it's zero point five times zero point five times zero

31
00:02:58,850 --> 00:03:04,950
point five times the 2.5 which is equal to about six point to 5 percent.

32
00:03:04,970 --> 00:03:13,310
So now let's use this technique with a dice pause the video and as a challenge work out the probability

33
00:03:13,580 --> 00:03:20,360
of rolling a 6 on a dice like this three times in a row.

34
00:03:20,480 --> 00:03:24,340
Did you have a go with a six sided die.

35
00:03:24,360 --> 00:03:28,290
You essentially have a 1 in 6 chance to roll any particular number.

36
00:03:28,890 --> 00:03:40,690
So rolling a 6 3 times in a row has a joint probability of 1 6th times 1 6 times 1 6th and that's equal

37
00:03:40,870 --> 00:03:44,700
to approximately zero point for six percent.

38
00:03:45,250 --> 00:03:47,680
So less than one half of a percent.

39
00:03:47,770 --> 00:03:50,830
Very very unlikely.

40
00:03:50,860 --> 00:03:56,170
Now I know these were some very very simple examples just now but I hope you can see how rolling more

41
00:03:56,170 --> 00:04:02,040
and more six years in a row gets less and less likely and the same with flipping coins right.

42
00:04:02,050 --> 00:04:07,990
It would be strange to see somebody get heads after heads after heads after heads on a fair coin that

43
00:04:07,990 --> 00:04:10,110
is now I think personally.

44
00:04:10,150 --> 00:04:17,320
This is like the very friendly and simple side of probability but it's still interesting because let

45
00:04:17,320 --> 00:04:19,150
me ask you a question.

46
00:04:19,210 --> 00:04:26,410
Imagine you and I are hanging out on a Friday night chilling flipping coins and talking about probability

47
00:04:26,620 --> 00:04:28,190
as as we do.

48
00:04:28,510 --> 00:04:33,000
And I've just flipped my coin twice both times.

49
00:04:33,220 --> 00:04:34,660
I got heads.

50
00:04:34,660 --> 00:04:36,680
What's the probability that I could.

51
00:04:36,680 --> 00:04:39,890
Heads on my third Flip as well.

52
00:04:40,240 --> 00:04:46,860
Is it 1 1/2 times 1 1/2 times one half or zero point 1 2 5.

53
00:04:46,930 --> 00:04:48,190
No no no no no.

54
00:04:48,190 --> 00:04:49,330
It is not.

55
00:04:49,330 --> 00:04:51,260
Most definitely not.

56
00:04:51,280 --> 00:04:59,860
And the reason is is that the past coin flips don't affect the next coin flip each coin flip is independent

57
00:05:00,580 --> 00:05:05,350
the probability of me getting heads on that third flip is still 50 percent.

58
00:05:06,160 --> 00:05:14,470
How this might seem obvious to you but intuitively I've seen a lot of people just sort of accept this

59
00:05:14,470 --> 00:05:17,690
idea of independence.

60
00:05:18,100 --> 00:05:19,620
Intelligent people too.

61
00:05:19,750 --> 00:05:23,300
It's like you know people don't want to accept this.

62
00:05:23,520 --> 00:05:29,860
And if you don't believe me you can actually observe this behavior for yourself.

63
00:05:29,860 --> 00:05:38,140
All you need to do is go to a roulette table in a crowded casino and just wait and watch just watch

64
00:05:38,140 --> 00:05:39,090
people.

65
00:05:39,160 --> 00:05:44,980
Now if you've never played roulette or you're not familiar with this game in the casino people spin

66
00:05:44,980 --> 00:05:52,570
this wheel and people win when they've predicted where the ball will land people can bet on a number

67
00:05:52,570 --> 00:05:58,480
of things but they can also bet on the colors right the color that the ball will land on.

68
00:05:58,480 --> 00:06:02,950
So in this game a lot of people love betting on a particular color.

69
00:06:02,950 --> 00:06:03,250
Right.

70
00:06:03,250 --> 00:06:04,210
Red or black.

71
00:06:04,210 --> 00:06:08,460
Will the ball end up on the red or on the black for example.

72
00:06:08,740 --> 00:06:16,030
Now when the ball has been landing on black for a few times in a row you will see that people increasingly

73
00:06:16,090 --> 00:06:22,120
start betting on red because they think it's the more likely outcome for the next spin.

74
00:06:22,240 --> 00:06:28,480
Intuitively many people think it's been black four times in a row on the next spin the ball just has

75
00:06:28,480 --> 00:06:29,450
to land on red.

76
00:06:29,500 --> 00:06:29,800
Right.

77
00:06:30,520 --> 00:06:34,750
But the truth is red is not more likely.

78
00:06:34,750 --> 00:06:35,720
Why.

79
00:06:35,770 --> 00:06:38,940
Because each spin of the roulette wheel is independent.

80
00:06:39,040 --> 00:06:44,050
The wheel itself doesn't have any memory of the previous outcomes.

81
00:06:44,050 --> 00:06:44,330
OK.

82
00:06:44,350 --> 00:06:50,510
So that's a real world example of this concept of independence in action.

83
00:06:50,650 --> 00:06:55,740
And we've actually covered quite a few important concepts in probabilities so far.

84
00:06:55,750 --> 00:07:02,860
Already we've covered basic probability which was very much based on observing the frequency of a particular

85
00:07:02,860 --> 00:07:09,790
event we've covered this idea of independence where two events have nothing to do with one another.

86
00:07:09,800 --> 00:07:17,510
And finally we've also covered this idea of joint probability and what we saw was that for independent

87
00:07:17,510 --> 00:07:26,780
events like rolls of dice and coin tosses the formula for both A and B happening was simply by multiplying

88
00:07:26,960 --> 00:07:32,420
the probability of A times the probability of B.

89
00:07:32,420 --> 00:07:42,350
Now let's take this discussion back to spam classification in order to work out the probability of an

90
00:07:42,350 --> 00:07:48,890
email being spam we should look at something other than the frequency of spam emails out there in the

91
00:07:48,890 --> 00:07:49,280
wild.

92
00:07:49,280 --> 00:07:50,200
Right.

93
00:07:50,270 --> 00:07:55,510
We should look at the contents of each individual email.

94
00:07:55,520 --> 00:08:01,720
This means looking at the message itself which is well where it gets interesting.

95
00:08:01,820 --> 00:08:08,720
Now if you fire up Hotmail or e-mail right now and you look through your e-mail spam folder what kind

96
00:08:08,720 --> 00:08:15,400
of words do you tend to see in the message bodies for these e-mails.

97
00:08:15,410 --> 00:08:17,810
I bet you can kind of spot a pattern.

98
00:08:17,930 --> 00:08:22,910
You can spot certain themes coming up over and over again in your spam emails.

99
00:08:23,150 --> 00:08:31,280
Certain words are just much more likely to come up in spam e-mail than in regular legitimate e-mail.

100
00:08:31,970 --> 00:08:42,340
I'm thinking of words like free access Viagra loan online pharmacy adult great offer winner casino Bitcoin.

101
00:08:42,410 --> 00:08:49,160
What have you like when was the last time a friend of yours send you a legitimate email about an online

102
00:08:49,160 --> 00:08:52,530
pharmacy or free bitcoins for example.

103
00:08:52,670 --> 00:09:00,230
So clearly there's going to be a clue in the message body whether an email is going to be a spam email

104
00:09:00,380 --> 00:09:04,200
or are not just purely based on what the emails about.

105
00:09:05,000 --> 00:09:08,090
So imagine a new email arrives in your inbox.

106
00:09:08,210 --> 00:09:12,000
The e-mail body contains the word VIAGRA.

107
00:09:12,230 --> 00:09:16,280
Should this e-mail be classified as spam.

108
00:09:16,280 --> 00:09:23,130
Let me rephrase that question given that this e-mail contains the word VIAGRA.

109
00:09:23,150 --> 00:09:30,500
What's the probability of this email being spam this question that I just posed.

110
00:09:30,630 --> 00:09:34,340
It's all about conditional probability.

111
00:09:34,740 --> 00:09:38,520
And this is where things are going to start getting pretty wild.

112
00:09:38,520 --> 00:09:47,760
Conditional Probability is in my personal opinion both something unintuitive and difficult and we're

113
00:09:47,760 --> 00:09:52,620
about to get our hands dirty with some of these meaty statistics.

114
00:09:52,620 --> 00:09:53,880
Oh yes.

115
00:09:53,880 --> 00:10:00,960
You see this is really really worthwhile doing actually many of the basic statistical concepts like

116
00:10:00,960 --> 00:10:07,410
probability independence joint probability and conditional probability are used everywhere in machine

117
00:10:07,410 --> 00:10:08,280
learning.

118
00:10:08,280 --> 00:10:10,130
Even if you don't see it right away.

119
00:10:10,410 --> 00:10:16,180
Even our box standard linear regression models come straight out of a statistics textbook.

120
00:10:16,200 --> 00:10:22,140
The nice thing about the naive bayes classifier is that we don't use statistics and probability under

121
00:10:22,140 --> 00:10:31,110
the hood we're gonna use probability explicitly our naive bayes algorithm uses statistics in its raw

122
00:10:31,230 --> 00:10:39,600
and pure format to classify email spam and conditional probability is at the very heart of it conditional

123
00:10:39,600 --> 00:10:47,790
probability measures the probability of some event given that another event has occurred.

124
00:10:47,790 --> 00:10:50,460
The mathematical syntax looks like this.

125
00:10:50,460 --> 00:10:58,500
What we have here reads the probability of a given B for example what's the probability of me getting

126
00:10:58,500 --> 00:11:07,210
heads on my next coin flip given that my last coin flip was heads or we just talked about this the first

127
00:11:07,210 --> 00:11:09,590
coin flip doesn't affect the second coin flip.

128
00:11:09,670 --> 00:11:13,520
Sort of two events are conditionally independent.

129
00:11:13,900 --> 00:11:19,660
The probability of me getting heads on that next flip is still 50 percent.

130
00:11:19,930 --> 00:11:26,200
But but but what if the two events are in fact dependent.

131
00:11:26,380 --> 00:11:34,300
Going back to our email example what's the probability of an email being spam given that it contains

132
00:11:34,300 --> 00:11:42,530
the word VIAGRA in this case the probability of the email being spam actually depends on the probability

133
00:11:43,130 --> 00:11:46,870
that the email contains the word via Gra.

134
00:11:46,910 --> 00:11:47,920
What.

135
00:11:47,930 --> 00:11:50,350
That sounds weird right now.

136
00:11:50,360 --> 00:11:53,720
I don't expect you to accept this at face value.

137
00:11:53,720 --> 00:11:58,190
Let me try and explain this with a different example where the intuition is is less murky.

138
00:11:58,190 --> 00:12:00,140
It's it's much more clear.

139
00:12:00,140 --> 00:12:03,330
Say you look out the window and it's cloudy outside.

140
00:12:03,440 --> 00:12:06,600
What's the probability of it raining today.

141
00:12:06,620 --> 00:12:09,980
In other words given that it is cloudy.

142
00:12:09,980 --> 00:12:14,470
What is the probability of rain now in this picture.

143
00:12:14,480 --> 00:12:20,570
It looks like it's about to rain any minute but why rely on eyesight when you can have a mathematical

144
00:12:20,570 --> 00:12:21,380
formula.

145
00:12:21,680 --> 00:12:23,920
You see like all good things.

146
00:12:23,960 --> 00:12:32,510
Conditional Probability has a mathematical formula associated with it and here it is the probability

147
00:12:32,510 --> 00:12:33,380
of rain.

148
00:12:33,380 --> 00:12:43,400
Given that it is cloudy is equal to the probability of rain and it being cloudy divided by the probability

149
00:12:43,880 --> 00:12:46,000
of it being cloudy hmm.

150
00:12:46,390 --> 00:12:53,660
OK this looks like a very very strange formula indeed but important learning point here is that the

151
00:12:53,660 --> 00:13:01,250
probability of it raining depends on the probability of it being cloudy and that's the bottom part of

152
00:13:01,250 --> 00:13:02,570
the fraction here.

153
00:13:02,630 --> 00:13:06,870
The bottom part of the fraction is the probability of it being cloudy.

154
00:13:07,040 --> 00:13:16,750
If it was cloudy for 250 days a year last year then this part of the fraction is 250 divided by 365.

155
00:13:16,770 --> 00:13:20,790
Now what about the top part of the fraction.

156
00:13:20,800 --> 00:13:22,360
Well guess what.

157
00:13:22,360 --> 00:13:25,900
This is our old friend joint probability.

158
00:13:26,080 --> 00:13:33,490
It's the amount of overlap between the days that were cloudy and the days that it rained so in London.

159
00:13:33,610 --> 00:13:37,310
It rains on approximately 107 days per year.

160
00:13:37,480 --> 00:13:44,400
But you know what two of those rainy days were actually fairly sunny and I saw a rainbow.

161
00:13:44,440 --> 00:13:52,250
So then number of days where it was both raining and cloudy was only one hundred and five.

162
00:13:52,330 --> 00:13:59,200
So the top part of that fraction is going gonna be 105 out of 365 now doing this calculation gives us

163
00:13:59,290 --> 00:14:08,220
a conditional probability of approximately 42 percent the probability of it raining given that it is

164
00:14:08,220 --> 00:14:13,350
cloudy outside is 42 percent according to this formula.

165
00:14:14,430 --> 00:14:21,030
I know this may sound a little bit trivial but the really really cool thing is that what we've just

166
00:14:21,030 --> 00:14:30,210
done is calculate the probability of something knowing something else and that sounds a lot like fundamental

167
00:14:30,210 --> 00:14:31,140
machine learning right.

168
00:14:31,980 --> 00:14:35,870
Knowing the movie budget we can calculate expected movie revenue.

169
00:14:36,060 --> 00:14:40,620
Knowing the number of rooms we can calculate the expected property price.

170
00:14:40,680 --> 00:14:48,250
Think of it being cloudy outside has a feature and think of it raining today as your target your wine.

171
00:14:48,750 --> 00:14:55,680
So I hope this example shows how conceptually conditional probability is actually widely applicable

172
00:14:55,950 --> 00:15:03,300
to machine learning and if you open any statistics textbook out there you'll see that conditional probability

173
00:15:03,300 --> 00:15:06,710
is expressed in very general terms just like this.

174
00:15:07,020 --> 00:15:17,480
The probability of a given B is equal to the probability of A and B over the probability of B what would

175
00:15:17,480 --> 00:15:25,590
this formula look like in our spam example substituting for A and B we get something like this the probability

176
00:15:26,010 --> 00:15:36,030
of an email being spam given that it contains the word by Agora is equal to the probability of an email

177
00:15:36,090 --> 00:15:45,780
being spam and the probability of the email containing the word VIAGRA over the probability of an email

178
00:15:45,780 --> 00:15:48,590
containing the word VIAGRA.

179
00:15:48,780 --> 00:15:56,520
Looking at this formula there's one problem though and that's the top term in this fraction the joint

180
00:15:56,820 --> 00:16:04,750
probability calculating the joint probability was super easy when we were dealing with dice and with

181
00:16:04,750 --> 00:16:08,470
coins because each event was independent.

182
00:16:08,580 --> 00:16:14,920
In that case we could just multiply the probabilities of the events together but with dependent events

183
00:16:15,130 --> 00:16:24,460
like clouds and rain or email and the word by Agora this calculation is no longer so simple and that's

184
00:16:24,460 --> 00:16:31,690
why to calculate the conditional probability and create our spam classifier we have to take it to the

185
00:16:31,690 --> 00:16:32,990
next level.

186
00:16:33,280 --> 00:16:35,770
We have to use Bayes theorem.
