1
00:00:00,210 --> 00:00:01,460
Welcome back.

2
00:00:01,470 --> 00:00:09,330
Having just completed some very beautiful visualizations let's get back to pre processing our data for

3
00:00:09,330 --> 00:00:11,640
our base classifier.

4
00:00:11,720 --> 00:00:18,620
Now there's lots of individual words among the five thousand eight hundred odd emails that constitute

5
00:00:18,770 --> 00:00:20,300
our dataset.

6
00:00:20,300 --> 00:00:26,360
We won't actually use every single word that came up in these email bodies.

7
00:00:26,390 --> 00:00:32,720
We're just going to use to two thousand five hundred most frequent words and when add a markdown cell

8
00:00:32,720 --> 00:00:43,990
here to commemorate this and it's going to read generate vocabulary and dictionary the two thousand

9
00:00:43,990 --> 00:00:51,110
five hundred most frequent words in our dataset are going to form our vocabulary and we will generate

10
00:00:51,470 --> 00:00:59,000
this vocabulary from our stemmed list of words to get our STEM list of words we're once again going

11
00:00:59,000 --> 00:01:05,010
to call our clean message no h t a malfunction that we've created earlier.

12
00:01:05,090 --> 00:01:08,570
Now I know that for the word cloud we commented out this line.

13
00:01:08,570 --> 00:01:14,960
So if you have this commented out commented packin because we actually do understand the words this

14
00:01:14,960 --> 00:01:20,820
time around and if you made this change make sure that you do two things.

15
00:01:20,900 --> 00:01:27,680
If you comment this line back in make sure you comment this one back out and also press shift enter

16
00:01:27,830 --> 00:01:29,090
on this cell.

17
00:01:29,090 --> 00:01:33,470
Otherwise you're going to get some very unexpected results later on.

18
00:01:33,530 --> 00:01:33,830
All right.

19
00:01:33,860 --> 00:01:39,200
So let's use our apply method and call this function right here.

20
00:01:39,200 --> 00:01:47,870
Create a variable called stemmed underscore nested on the score list set that equal to data dot message

21
00:01:47,930 --> 00:01:56,460
dot apply and then I'll feed in the name of our function clean on a core message underscore no underscore

22
00:01:56,570 --> 00:01:58,310
HMO.

23
00:01:58,640 --> 00:02:08,000
Because this is a nested list I'm going to flatten this list and store it under flat stemmed list and

24
00:02:08,000 --> 00:02:12,760
set that equal to the result of some Python list comprehension.

25
00:02:13,190 --> 00:02:23,750
Item 4 sub list in stemmed nested list for item in sub list.

26
00:02:24,620 --> 00:02:27,420
So far nothing new.

27
00:02:27,440 --> 00:02:31,040
Let's run the cell and move on the next step.

28
00:02:31,280 --> 00:02:34,500
We'll be getting a unique set of words.

29
00:02:34,550 --> 00:02:37,250
This is gonna make up our vocabulary.

30
00:02:37,250 --> 00:02:42,980
The easiest way to do this I think is to generate a penned a series and then use the values Kant's method.

31
00:02:42,980 --> 00:02:45,590
Once again you can ignore this warning here.

32
00:02:45,590 --> 00:02:51,140
This is aimed at people who are trying to use beautiful soup to open a URL now to create that series

33
00:02:51,230 --> 00:02:52,700
of unique words.

34
00:02:52,850 --> 00:03:02,910
I'll create a variable really quickly that that's called unique words and set that equal to PD dot series

35
00:03:03,480 --> 00:03:11,970
parentheses provide the flat stemmed list that we created a minute ago and then I'm going to call a

36
00:03:11,970 --> 00:03:16,230
method by the name of value and a score counts.

37
00:03:16,320 --> 00:03:18,130
Let me show you what we've just done here.

38
00:03:18,210 --> 00:03:27,240
The number of unique words I can print out using the shape of my variable so I'll see number of unique

39
00:03:27,240 --> 00:03:37,620
words is going to be in a print statement that a comma and I'll say unique words dot shape square brackets

40
00:03:37,870 --> 00:03:38,710
zero.

41
00:03:38,820 --> 00:03:45,480
This will print out the number of unique words in this series and to look at the first five rows the

42
00:03:45,480 --> 00:03:47,140
first five entries in the series.

43
00:03:47,310 --> 00:03:50,770
I'll see unique words dot head.

44
00:03:51,180 --> 00:03:59,760
See what we get and what we see here is that after cleaning and stemming we are left with twenty seven

45
00:03:59,760 --> 00:04:07,920
thousand three hundred and twenty words in our dataset twenty seven thousand unique words across all

46
00:04:07,920 --> 00:04:09,590
our email bodies.

47
00:04:09,630 --> 00:04:17,430
Now this is an absolutely huge number and we're actually only going to train our classifier with a subset

48
00:04:17,880 --> 00:04:25,490
of this number namely to two thousand five hundred most frequent words you might be wondering why HP

49
00:04:25,530 --> 00:04:26,540
is up here.

50
00:04:26,580 --> 00:04:30,350
Well HDP pretty much precedes every single year l.

51
00:04:30,410 --> 00:04:37,910
So this goes to show how many hyperlinks people have included in their emails and how to get the two

52
00:04:37,910 --> 00:04:42,640
thousand five hundred most frequent words I wanna throw this over to you as a challenge.

53
00:04:42,910 --> 00:04:49,860
And the reason is this is another good opportunity to practice sub setting and working with these series.

54
00:04:49,860 --> 00:04:58,610
Can you create a subset of this unique world series and store it in a variable called frequent underscore

55
00:04:58,680 --> 00:05:05,520
words which will only contain the most frequent two thousand five hundred words out of the total.

56
00:05:05,730 --> 00:05:09,870
And then afterwards print out the top 10 words.

57
00:05:09,990 --> 00:05:14,700
These of course are going to overlap with the top five words that you see above.

58
00:05:14,700 --> 00:05:17,490
I'll give you a few seconds to pause the video and give this a go.

59
00:05:20,430 --> 00:05:22,590
Here's the solution.

60
00:05:22,740 --> 00:05:31,350
Frequent underscore words is equal to unique words and then we use that square bracket notation zero

61
00:05:31,380 --> 00:05:35,500
to two thousand five hundred.

62
00:05:35,580 --> 00:05:40,040
That's how we create a subset and to print out the top 10.

63
00:05:40,140 --> 00:05:50,620
We'll say print most common words and then I'm even going to use a skip character and a new line comma

64
00:05:51,760 --> 00:05:52,990
frequent words.

65
00:05:53,320 --> 00:06:00,250
And once again when I create a subset this time from the beginning so I'm even gonna leave out the zero

66
00:06:00,730 --> 00:06:01,900
to 10.

67
00:06:01,900 --> 00:06:05,620
These are the top 10 words and here they are.

68
00:06:06,070 --> 00:06:12,670
With this notation when you're creating a subset you're setting a starting point and an ending point.

69
00:06:12,670 --> 00:06:18,700
And with this notation of creating a subset you're going from the beginning to an end point.

70
00:06:18,700 --> 00:06:21,400
So I hope this was a useful review.

71
00:06:21,430 --> 00:06:26,800
One thing that we can do to improve our code though is removing some of these magic numbers that we

72
00:06:26,800 --> 00:06:27,830
see here.

73
00:06:27,850 --> 00:06:35,230
So what I like is instead of having two thousand five hundred float around in my code here I'd like

74
00:06:35,230 --> 00:06:36,820
to define a constant.

75
00:06:36,820 --> 00:06:47,440
At the very top called vocab under school size and set that equal to the size of the vocabulary that

76
00:06:47,440 --> 00:06:53,290
I'm gonna use in my code later on that way if I ever want to make a change all I have to do is change

77
00:06:53,470 --> 00:07:02,760
this number here and it will filter through as long as I replace this number here with my constant vocab

78
00:07:02,840 --> 00:07:05,100
under school size.

79
00:07:05,150 --> 00:07:09,430
Now you've got to remember if you've changed a cell up top get a press shift enter.

80
00:07:09,580 --> 00:07:12,580
Otherwise you're gonna get an error.

81
00:07:13,120 --> 00:07:17,040
Now with frequent words we're working with a series right.

82
00:07:17,350 --> 00:07:27,790
So type parentheses frequent words we can see that we have a panda's series and not only that if we

83
00:07:27,790 --> 00:07:36,940
look at frequent words we see that this bit here the actual words form our index and the number of occurrences

84
00:07:37,210 --> 00:07:40,610
are actually the values in this series.

85
00:07:40,630 --> 00:07:47,320
Let's practice how we would go between a series and a data frame and how to work with these indices.

86
00:07:47,380 --> 00:07:54,040
We're also going to take this opportunity to assign a word I.D. to each word similar to how we assigned

87
00:07:54,100 --> 00:08:04,490
a dark I.D. in an earlier lesson body markdowns sell here real quick that's gonna read create vocabulary

88
00:08:05,780 --> 00:08:15,680
data frame with a word on a school I.D. Now our word I.D. is just gonna be integers that gonna be ranging

89
00:08:15,680 --> 00:08:21,980
from zero to two thousand four hundred and ninety nine meaning we're gonna work again with this range

90
00:08:21,980 --> 00:08:31,190
object and we can create a range very easily with range parentheses zero comma and then going up to

91
00:08:31,790 --> 00:08:33,720
our vocab size right.

92
00:08:33,740 --> 00:08:39,630
This is how we create our range and how we can do is store all these numbers in a list.

93
00:08:39,710 --> 00:08:47,090
So I'll wrap this call to the range into a list and I'm actually gonna store this in a variable called

94
00:08:47,150 --> 00:08:48,470
word I.D..

95
00:08:48,470 --> 00:08:56,210
So where do these is equal to list parentheses range parentheses zero comma vocab size and then closing

96
00:08:56,210 --> 00:08:57,740
the parentheses.

97
00:08:57,740 --> 00:09:06,410
Now let's create our data frame with PD dot data frame open parentheses and then what I want to do is

98
00:09:06,480 --> 00:09:14,130
I want to provide a dictionary so I'll have those two curly braces our dictionary is gonna consist of

99
00:09:14,130 --> 00:09:15,580
a key and a value.

100
00:09:15,780 --> 00:09:22,810
So the key will be whatever I want that column heading to read vocab underscore a word.

101
00:09:22,860 --> 00:09:24,830
Sounds good to me.

102
00:09:24,960 --> 00:09:31,530
Now the values I want those to be the actual words scrolling up a little bit.

103
00:09:31,830 --> 00:09:36,240
The words are here in our series.

104
00:09:36,240 --> 00:09:45,630
So this means that if I use frequent words like so I'm actually accessing the frequencies the numbers

105
00:09:46,400 --> 00:09:51,190
what I need to do to get the words is work with our index right.

106
00:09:51,240 --> 00:10:00,810
So index dot values will be the way I can store all these different strengths in a column for our data

107
00:10:00,810 --> 00:10:02,010
frame.

108
00:10:02,010 --> 00:10:03,270
So far so good.

109
00:10:03,390 --> 00:10:04,200
Let's see what we've got.

110
00:10:04,740 --> 00:10:13,750
Let me hit shift enter on the cell at the moment our data frame looks like this fair enough.

111
00:10:13,750 --> 00:10:21,520
Let's add our word I.D. explicitly to the state of frame so we can do that with setting the data frames

112
00:10:21,610 --> 00:10:22,270
Index.

113
00:10:22,300 --> 00:10:22,710
Right.

114
00:10:22,810 --> 00:10:30,720
So index is equal to word I.D. and then we can also give that index a name.

115
00:10:30,860 --> 00:10:39,990
But first let me give our data frame a name as well so say vocab is equal to PD dot data frame and on

116
00:10:39,990 --> 00:10:40,700
the line below.

117
00:10:40,710 --> 00:10:51,890
I'll say vocab dot index dot name is equal to single quotes word on a score I d let's look at the first

118
00:10:52,010 --> 00:10:58,500
five rows in our data frame with vocab dot head and then shift into.

119
00:10:58,610 --> 00:10:59,510
There we go.

120
00:10:59,510 --> 00:11:00,670
Fantastic.

121
00:11:00,740 --> 00:11:07,040
We've generated our vocabulary that we're going to train our classifier with and previously we've had

122
00:11:07,040 --> 00:11:15,080
a pending state of frame and we've used this to Jason functionality to save a file in the Jason format

123
00:11:15,260 --> 00:11:16,410
to our disk.

124
00:11:16,430 --> 00:11:22,640
Now to Jason it's very well and good but of course printers can save many different file types.

125
00:11:22,730 --> 00:11:29,750
A common one that you're gonna be working with a lot is a CSP file a comma separated value.

126
00:11:29,750 --> 00:11:35,060
This is a file format that can easily be opened and nicely formatted with Microsoft Excel or google

127
00:11:35,060 --> 00:11:36,290
sheets.

128
00:11:36,290 --> 00:11:43,340
Now you probably already surmised that you're going to need a file path for this to CSC function that

129
00:11:43,340 --> 00:11:43,970
we're gonna call.

130
00:11:44,780 --> 00:11:52,610
So let's go back up to constants and create a concept that will hold on to our file path and our file

131
00:11:52,610 --> 00:11:56,840
name for this CSP file that we're gonna create a copy.

132
00:11:56,870 --> 00:11:59,900
This constant right here pasted below.

133
00:11:59,900 --> 00:12:02,270
And then make a few changes to it of course.

134
00:12:02,300 --> 00:12:09,710
So I'll call this one in all caps would on a school I.D. on a score file and we're still gonna save

135
00:12:09,710 --> 00:12:19,400
it in our processing folder but we're going to call it would hyphen by hyphen I.D. and then add the

136
00:12:19,610 --> 00:12:23,210
don't C S the extension to it.

137
00:12:23,210 --> 00:12:32,420
So that's our file path and I'd shift into to make sure this is saved and then down here gonna add a

138
00:12:32,660 --> 00:12:45,470
quick section heading it's gonna read save the vocabulary as a CSA file as you can guess will access

139
00:12:45,650 --> 00:12:57,970
R to CSB method from our vocab object so vocab dot to underscore CSP parentheses and then we're gonna

140
00:12:57,980 --> 00:13:02,920
pass in r word I.D. file path and name.

141
00:13:02,960 --> 00:13:10,700
Now if I hit shift tab on this I can see some of these other parameters that I can specify and that

142
00:13:10,700 --> 00:13:16,190
includes a header and an index.

143
00:13:16,190 --> 00:13:19,300
Now what does this mean coming down here.

144
00:13:19,400 --> 00:13:29,020
I can see that our header can be a list of strings and our index label can also be a string by default.

145
00:13:29,120 --> 00:13:31,520
No index label is provided.

146
00:13:31,850 --> 00:13:40,430
So let's provide these two additional inputs to our two CSB method call so that a comma here and the

147
00:13:40,430 --> 00:13:43,440
first thing I'll do is provide the index label.

148
00:13:43,430 --> 00:13:50,390
Now I can provide it as a string with single quotes and c word on the score I.D. but what I could also

149
00:13:50,390 --> 00:13:54,580
do is instead of typing this out and risk making a typo.

150
00:13:55,160 --> 00:14:02,690
If I want to make sure it matches what I've got in my data frame I can access this directly with vocab

151
00:14:03,080 --> 00:14:10,010
dot index dot name and for our header it's actually the very same thing.

152
00:14:10,550 --> 00:14:20,350
I could either write single quotes vocab on the school word which is our header right here but alternatively

153
00:14:20,620 --> 00:14:23,520
I can also grab our column.

154
00:14:23,590 --> 00:14:28,110
So with vocab dot vocab underscore a word.

155
00:14:28,120 --> 00:14:34,010
Don't name this will accomplish the very very same thing.

156
00:14:34,050 --> 00:14:36,660
Now I'm not gonna hit shifting around this right now.

157
00:14:36,660 --> 00:14:43,330
What I want to do instead is bring up my folder here on the right hand side and then hit shift enter

158
00:14:43,560 --> 00:14:46,550
so you can see the file appearing.

159
00:14:46,620 --> 00:14:47,670
Here you go.

160
00:14:47,670 --> 00:14:49,110
There it is.

161
00:14:49,320 --> 00:14:57,330
Now you can open this in a text editor say like atom and the CSB file will be formatted something like

162
00:14:57,330 --> 00:14:58,140
this.

163
00:14:58,140 --> 00:15:01,350
It's not a particularly impressive.

164
00:15:01,470 --> 00:15:08,880
But if you have a spreadsheet program like Microsoft Excel or google sheets or in my case this uh numbers

165
00:15:08,880 --> 00:15:15,810
program that comes with Mac then you'll see the values formatted nicely in these columns.

166
00:15:15,810 --> 00:15:20,640
So as you can see the CRC format is very very handy.

167
00:15:20,640 --> 00:15:25,090
Now we're covering quite a lot of stuff in these lessons and with programming.

168
00:15:25,170 --> 00:15:27,710
The best way of learning it is by doing it.

169
00:15:28,020 --> 00:15:33,930
So the next two lessons will consist of some very quick exercises to review some of the concepts that

170
00:15:34,110 --> 00:15:36,040
we've talked about.

171
00:15:36,090 --> 00:15:39,420
I'm off to grab some more coffee and then I'll see you there.
