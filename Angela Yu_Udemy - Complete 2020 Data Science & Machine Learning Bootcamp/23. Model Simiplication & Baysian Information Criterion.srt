1
00:00:00,560 --> 00:00:05,480
In this lesson we're going to talk about simplifying our model.

2
00:00:05,480 --> 00:00:11,510
And that's because all else equal simpler models are preferable to complex ones.

3
00:00:11,510 --> 00:00:20,040
Remember the Zen of Python simple is better than a complex and complex is better than complicated and

4
00:00:20,070 --> 00:00:21,930
this doesn't just apply to programming.

5
00:00:22,110 --> 00:00:26,250
The same thing can be said about our regression model.

6
00:00:26,250 --> 00:00:33,450
By the way you can always bring up this little gem of programming philosophy when you type import this

7
00:00:33,600 --> 00:00:40,760
in Jupiter notebook simple models really are preferable to complex ones.

8
00:00:40,760 --> 00:00:42,180
All else equal.

9
00:00:42,530 --> 00:00:47,090
So the question is how can we simplify our model.

10
00:00:47,210 --> 00:00:55,900
One of the easiest ways is to remove some of the explanatory variables but can we just drop some features.

11
00:00:55,900 --> 00:00:57,980
Is that a wise thing to do.

12
00:00:58,030 --> 00:01:02,380
And if so which features should we drop.

13
00:01:02,380 --> 00:01:09,440
What about the features that we're not highly correlated with property prices in the correlation matrix.

14
00:01:09,460 --> 00:01:17,670
We saw that distance from employment centers only had a zero point two five correlation with our target.

15
00:01:17,760 --> 00:01:25,960
DHS had a low correlation with price but also it had a high correlation with our industry factor of

16
00:01:26,080 --> 00:01:28,950
minus zero point seven one.

17
00:01:29,470 --> 00:01:37,210
At the time we were wondering how much value that distance from employment centers feature really added.

18
00:01:37,480 --> 00:01:40,140
But now we know scrolling down.

19
00:01:40,270 --> 00:01:46,960
We've got the p values which test for the significance of our different factors and we can see that

20
00:01:46,960 --> 00:01:50,890
distance is actually very statistically significant.

21
00:01:50,890 --> 00:01:57,800
So we should probably keep DHS around now on the other hand looking back up at our industry factor.

22
00:01:57,910 --> 00:02:04,510
This has a p value of around zero point 4 4 meaning it is not statistically significant.

23
00:02:05,730 --> 00:02:11,560
The threshold for p values if you recall was zero point zero five.

24
00:02:11,610 --> 00:02:18,340
So now the question is should we try removing the industry factor from the model.

25
00:02:18,430 --> 00:02:24,770
And the thing is it is really really tempting to remove insignificant predictors.

26
00:02:24,770 --> 00:02:32,030
But even dropping statistically insignificant features is not something people do lightly because even

27
00:02:32,030 --> 00:02:39,500
a feature with a low p value can add value to the model as a whole by providing some kind of information

28
00:02:39,710 --> 00:02:43,190
that the other features do not provide.

29
00:02:43,190 --> 00:02:49,670
Actually deciding on what to keep and what to throw away from a machine learning model is a bit of an

30
00:02:49,670 --> 00:02:57,200
art and it gets into this whole topic of feature selection which is a very very big topic indeed and

31
00:02:57,200 --> 00:03:01,250
one that we will continue to tackle throughout this course.

32
00:03:01,250 --> 00:03:09,230
The goal of this lesson is to introduce you to this topic a feature selection in the context of a regression

33
00:03:09,230 --> 00:03:15,900
model and we will be looking at a metric that you can use to help you make your decisions.

34
00:03:16,190 --> 00:03:24,050
And that metric is called Bayesian information criterion or B I see the patient information criterion

35
00:03:24,380 --> 00:03:27,350
is a way you can measure complexity.

36
00:03:27,350 --> 00:03:31,690
It's basically a number that allows you to compare two different models.

37
00:03:31,790 --> 00:03:37,250
So what you end up doing is you run a regression with model number one and then you run a regression

38
00:03:37,580 --> 00:03:46,290
with Model Number two now model number one might have a big value of 1 4 8 and model number 2 might

39
00:03:46,290 --> 00:03:53,610
have a big value of 1 5 for the actual number doesn't by itself mean very much.

40
00:03:53,760 --> 00:04:03,880
What matters is which one is lower because all else equal a lower big number is better so this measure

41
00:04:04,000 --> 00:04:08,350
can help you pick between two or more models.

42
00:04:08,350 --> 00:04:15,290
So what models will we compare Well for starters let's compare the model that includes the industry

43
00:04:15,290 --> 00:04:19,840
feature and the model that excludes the industry feature.

44
00:04:20,120 --> 00:04:26,740
Let's commemorate this with another markup sell in Jupiter notebook so I I change my sell here to markdown

45
00:04:27,010 --> 00:04:29,830
and then put a section heading here.

46
00:04:29,830 --> 00:04:41,150
Model simplification and the Bayesian information criterion to calculate the patient information criterion.

47
00:04:41,260 --> 00:04:47,630
We're once again going to use our stats model modules regression capabilities instead of psychic learn

48
00:04:48,130 --> 00:04:54,330
so let's copy the sell here and paste that below.

49
00:04:54,500 --> 00:04:58,770
I'm going to delete these comments and I'm going to add a new comment up here.

50
00:05:00,090 --> 00:05:10,360
That reads our original model with log prices and all features and the data frame that I've got here.

51
00:05:10,470 --> 00:05:19,890
I'm going to store inside a variable so going to see original on a score QF is equal to PD dot data

52
00:05:19,890 --> 00:05:27,810
frame and so on and now what I want to do is I want to add some additional print statements and in these

53
00:05:27,810 --> 00:05:35,640
print statements will output both the patient information criterion value and the r squared for the

54
00:05:35,640 --> 00:05:37,240
regression.

55
00:05:37,260 --> 00:05:44,280
Now previously we've used psychics learn score method to print out the r squared but things work a little

56
00:05:44,280 --> 00:05:51,870
differently with the stats model module and I think this is a good time to practice making sense of

57
00:05:51,870 --> 00:05:54,560
the official documentation.

58
00:05:54,990 --> 00:06:04,350
So as a challenge can you look up the stats model docs for the regression results and figure out how

59
00:06:04,350 --> 00:06:12,220
to print out both the patient information criterion value for this regression as well as the r squared.

60
00:06:12,270 --> 00:06:16,780
I'll give you a few seconds to pause the video and give this a shot.

61
00:06:16,890 --> 00:06:17,580
You ready.

62
00:06:18,550 --> 00:06:19,890
Here's the solution.

63
00:06:20,200 --> 00:06:26,170
Being able to read and interpret the official documentation for all a lot of these Python modules is

64
00:06:26,170 --> 00:06:30,310
one of the key skills and becoming a better programmer.

65
00:06:30,310 --> 00:06:36,700
If I click on my results object and press shift tab on my keyboard to bring up the quick documentation

66
00:06:37,120 --> 00:06:41,200
I don't actually get anything useful.

67
00:06:41,320 --> 00:06:47,710
I just see that to get further information I need to look at the regression results documentation.

68
00:06:47,710 --> 00:06:53,590
The same thing is true if I click here and I find out that I'm just dealing with a data frame or when

69
00:06:53,590 --> 00:06:54,260
I click here.

70
00:06:54,610 --> 00:07:03,010
So in all these cases the quick documentation isn't actually helping me all that much I'm just not having

71
00:07:03,010 --> 00:07:05,790
any luck on getting the relevant information.

72
00:07:05,890 --> 00:07:10,600
So what I'm going to have to do is I don't have to google the documentation myself.

73
00:07:10,750 --> 00:07:18,880
The best keywords to enter into that white text box are stats model regression results and that should

74
00:07:18,880 --> 00:07:22,200
pretty much take you to one of the stats model pages.

75
00:07:22,390 --> 00:07:29,380
Now out of these three that I've got here the one I'm looking for is the page for the documentation

76
00:07:29,860 --> 00:07:32,280
on the regression results object.

77
00:07:32,910 --> 00:07:34,360
So it's the third one down here.

78
00:07:36,380 --> 00:07:40,430
This is the regression results object from the linear model.

79
00:07:40,430 --> 00:07:44,260
This is the documentation page that you want to be looking for.

80
00:07:44,300 --> 00:07:51,050
The other reason why the regression results page that I've clicked on is the relevant one is because

81
00:07:51,560 --> 00:08:00,140
this particular object the regression results wrapper inherits most of the methods and attributes from

82
00:08:00,320 --> 00:08:06,980
regression results meaning the capabilities of a regression results wrapper object which we've got and

83
00:08:07,040 --> 00:08:10,900
a regression results object are pretty much the same.

84
00:08:10,940 --> 00:08:18,620
They have a lot in common a lot of the methods and attributes are inherited from this particular object.

85
00:08:18,620 --> 00:08:20,820
Yeah they're closely linked.

86
00:08:21,200 --> 00:08:24,670
So this is why I'm looking on this page now.

87
00:08:24,680 --> 00:08:30,470
This is an incredibly long page if you look at it it's very Hey comprehensive but the interesting thing

88
00:08:30,470 --> 00:08:38,000
is that we can find both the parameter attribute and p values attribute on this page.

89
00:08:38,000 --> 00:08:44,330
So if I search for parameters on this page I can find parameters listed as one of the attributes and

90
00:08:44,330 --> 00:08:50,630
I can also find p values listed as one of the attributes and you probably already spotted it at the

91
00:08:50,630 --> 00:08:58,850
top of the page is the p i c attribute that Bayes information criteria to be I C is the name of the

92
00:08:58,850 --> 00:09:05,840
attribute for the Bayes information criteria and the way we access this attribute is simply by writing

93
00:09:05,840 --> 00:09:06,650
results.

94
00:09:06,810 --> 00:09:07,960
Don't be.

95
00:09:07,970 --> 00:09:15,830
I see when I if I had shift into I can see what the value of this actually is it's negative one hundred

96
00:09:16,250 --> 00:09:23,730
and thirty nine point eight five so about negative one hundred and forty and what about the R squared.

97
00:09:23,850 --> 00:09:33,360
Going back to the documentation and scrolling down squared unsurprisingly has the attribute name r squared

98
00:09:33,450 --> 00:09:44,260
all lowercase and in one word so results dot r squared will bring up the R squared for this regression

99
00:09:44,470 --> 00:09:48,040
which is zero point 7 9 3.

100
00:09:48,100 --> 00:09:50,890
Let's have both of these lines of code in a print statement.

101
00:09:50,890 --> 00:10:03,800
So what have print B I see is in the single quotes comma results done by C closing parenthesis and then

102
00:10:04,010 --> 00:10:04,880
I'll have print

103
00:10:07,650 --> 00:10:15,230
r squared is comma results start ask what closing parentheses.

104
00:10:15,420 --> 00:10:16,350
There we go.

105
00:10:16,410 --> 00:10:23,700
Okay so now we have both the r squared and R pick printed out the comforting thing to see is that the

106
00:10:23,730 --> 00:10:28,000
stats model and socket learn is exactly the same R squared.

107
00:10:28,050 --> 00:10:35,070
So we're doing things right now in this case the patient information criterion is actually a negative

108
00:10:35,280 --> 00:10:38,540
number and that's absolutely fine.

109
00:10:38,550 --> 00:10:42,550
What matters is how does number stacks up to our next model.

110
00:10:42,570 --> 00:10:48,190
So I'm going to copy this paste it and then I want to modify my comment.

111
00:10:48,390 --> 00:11:00,040
And when I write here reduced Model Number One excluding in this and then what I'll say is that X on

112
00:11:00,040 --> 00:11:11,020
a score including on a score constant is equal to X on a score including constant don't drop parentheses

113
00:11:11,710 --> 00:11:20,120
square brackets single quotes in this all caps comma axis equals 1.

114
00:11:20,150 --> 00:11:27,560
So in this line of code I'm redefining the data frame of features by overwriting what's stored inside

115
00:11:27,560 --> 00:11:28,370
this variable.

116
00:11:28,370 --> 00:11:36,720
I'm dropping the in this column from the data frame and storing that as the new feature data frame.

117
00:11:36,740 --> 00:11:44,600
So on this line when it comes to training our model we are excluding the index feature next I'm going

118
00:11:44,600 --> 00:11:52,100
to change the name of this variable to Cliff underscore minus on a school in this.

119
00:11:52,100 --> 00:11:56,900
So that way we're not overwriting the coefficient data frame from the cell above.

120
00:11:57,370 --> 00:12:00,140
And I'm going to delete this comment here.

121
00:12:00,200 --> 00:12:05,990
Now let me hit shift enter and refresh this cell.

122
00:12:06,220 --> 00:12:09,070
So this result is already quite interesting.

123
00:12:09,070 --> 00:12:15,040
What we can see is that our patient information criterion has gotten more negative.

124
00:12:15,040 --> 00:12:20,150
We've now got the value minus one 4 five point two.

125
00:12:20,170 --> 00:12:22,970
So this is an even lower number than before.

126
00:12:22,990 --> 00:12:29,560
So we have an improvement in terms of reducing complexity but at the same time it's also really nice

127
00:12:29,560 --> 00:12:34,180
to see that the r squared at zero point seven nine.

128
00:12:34,240 --> 00:12:36,430
Pretty much stays where it is.

129
00:12:36,460 --> 00:12:43,540
So even though we have removed one feature from our dataset it hasn't really impacted our fit in a material

130
00:12:43,540 --> 00:12:44,490
way.

131
00:12:44,500 --> 00:12:46,600
This is actually very encouraging.

132
00:12:46,630 --> 00:12:51,200
Let's go back up to our p values and experiment with removing something else.

133
00:12:51,220 --> 00:12:59,440
Let's experiment with removing age coming back down I'm going to copy the cell paste that changed my

134
00:12:59,440 --> 00:13:08,340
comment here to reduce Model Number Two excluding endorse and age and then in this line I'm going to

135
00:13:08,340 --> 00:13:14,990
have to add the single quotes and age between the square brackets.

136
00:13:14,990 --> 00:13:25,150
I'm also going to rename our data frame of coefficients to maybe reduced co F and now when it shift

137
00:13:25,400 --> 00:13:33,280
into and what we actually see is a further improvement based on the Bayesian information criterion.

138
00:13:33,550 --> 00:13:40,990
So we get an even lower pick number at minus one for nine and a half but we see no material change in

139
00:13:40,990 --> 00:13:42,210
the r squared.

140
00:13:42,400 --> 00:13:47,910
So this makes me think that removing both innocent age is actually a beneficial thing.

141
00:13:48,010 --> 00:13:54,190
We can probably safely drop these two features simplifying our model without incurring too much of a

142
00:13:54,190 --> 00:13:58,130
cost in terms of lost information and a worse fit.

143
00:13:58,360 --> 00:14:04,120
How even though I just gave you two examples where removing a feature improved the patient information

144
00:14:04,120 --> 00:14:07,870
criterion and left the r square pretty much unchanged.

145
00:14:07,870 --> 00:14:16,740
This isn't always the case if I change age to one of the other features say maybe zone Z in and press

146
00:14:16,740 --> 00:14:20,880
shift into what we see is not really all that clear cut.

147
00:14:20,970 --> 00:14:25,620
In this case we have a higher big number and a lower ask word than before.

148
00:14:26,010 --> 00:14:28,760
So this is probably not the direction we want to go.

149
00:14:29,130 --> 00:14:33,420
Same thing if I change this to our tax feature.

150
00:14:33,420 --> 00:14:35,780
Again we're making our model worse.

151
00:14:35,890 --> 00:14:43,080
And same thing if I change this to the else that feature removing else that actually makes our model

152
00:14:43,200 --> 00:14:44,910
much much worse.

153
00:14:44,910 --> 00:14:50,860
You can see how much the Bayesian information criterion jumped and how much lower our r squared is.

154
00:14:50,880 --> 00:14:58,290
In this case so else that is actually a very important to keep in the model changes back to age and

155
00:14:58,290 --> 00:15:03,010
press shift and so that we're back to where we started.

156
00:15:03,030 --> 00:15:03,310
Okay.

157
00:15:03,330 --> 00:15:04,020
So where are we now.

158
00:15:04,020 --> 00:15:06,690
We've made two small tweaks to our model.

159
00:15:06,690 --> 00:15:13,260
We've removed two of the features which were not statistically significant and we've looked at the Bayesian

160
00:15:13,260 --> 00:15:20,610
information criterion and the r squared to provide additional justification for leaving them out and

161
00:15:20,610 --> 00:15:28,710
simplifying our model and by doing so we've managed to improve our pick number from around minus 140

162
00:15:28,910 --> 00:15:31,070
to minus 150.

163
00:15:31,380 --> 00:15:38,100
So we get about a 9 10 point improvement in the big number but we don't incur a material penalty on

164
00:15:38,100 --> 00:15:39,040
the r squared.

165
00:15:39,060 --> 00:15:41,480
We're still on zero point seven nine.

166
00:15:41,580 --> 00:15:42,120
Cool.

167
00:15:42,120 --> 00:15:49,680
So that about wraps up our introduction to thinking about feature selection and one thing that we can

168
00:15:49,680 --> 00:15:53,280
do now is we can link this lesson to our previous one.

169
00:15:53,400 --> 00:16:01,050
We can link this to the discussion on multi calling parity and looking for stability in the theta estimates

170
00:16:01,260 --> 00:16:09,520
for our features because we've made quite a few tweaks to our model and we said that one of the symptoms

171
00:16:09,910 --> 00:16:13,920
of multi calling parity are unstable coefficients.

172
00:16:14,080 --> 00:16:20,740
Having run three different versions of our regression and having stored our coefficients in some variables

173
00:16:21,310 --> 00:16:27,160
we can now look at them side by side and double check if there are any strange developments.

174
00:16:27,250 --> 00:16:29,430
Now I'd be surprised if we saw any.

175
00:16:29,430 --> 00:16:34,100
Because we have no indication of MultiClient parity so far.

176
00:16:34,330 --> 00:16:36,450
But take a look at this.

177
00:16:36,520 --> 00:16:43,020
If I create a variable called frames and make that equal to a list of our data frames.

178
00:16:43,050 --> 00:16:53,740
So we had the original coefficient we had the QF underscore minus underscore in this data frame.

179
00:16:54,490 --> 00:17:01,610
And then finally we had you reduced on a score coiffed data frame of coefficients to put them all side

180
00:17:01,610 --> 00:17:02,720
by side.

181
00:17:02,750 --> 00:17:12,110
I would use Panda's con cap method so PD dot com cat and then in the parentheses to provide my list

182
00:17:12,320 --> 00:17:18,260
of data frames and an axis I want this to be concatenated along the columns.

183
00:17:18,260 --> 00:17:25,630
So side by side as opposed to top to bottom right so axis is gonna be equal to 1 instead of 0.

184
00:17:25,640 --> 00:17:26,900
Let's see what we get.

185
00:17:26,930 --> 00:17:28,320
Fantastic.

186
00:17:28,340 --> 00:17:36,910
Now what you can also see here in this table here is how Python treats missing values in a data frame.

187
00:17:36,920 --> 00:17:45,510
You see these nans n n Nan stands for not a number meaning there is a missing value here.

188
00:17:45,590 --> 00:17:52,280
So this column here is a reduced model without the age and without the industry feature.

189
00:17:52,310 --> 00:17:56,250
So we've got nans in place of these rows.

190
00:17:56,630 --> 00:18:03,200
Looking at this table is actually very very encouraging because what I'm seeing is that despite tweaking

191
00:18:03,200 --> 00:18:06,290
the model all the coefficients.

192
00:18:06,360 --> 00:18:07,550
Yeah for all three.

193
00:18:07,550 --> 00:18:14,990
So like Charles River crime they all are remarkably consistent so the numbers change somewhat but they

194
00:18:14,990 --> 00:18:15,970
don't switch signs.

195
00:18:16,010 --> 00:18:17,520
They don't change drastically.

196
00:18:17,540 --> 00:18:23,840
So these are all very very stable coefficient estimates and the same actually holds true.

197
00:18:23,840 --> 00:18:29,180
If you were to remove tax which we suspected was a potential problem source.

198
00:18:29,180 --> 00:18:36,620
So even removing tax and rerunning this you'd see that the data estimates are nice and stable between

199
00:18:36,620 --> 00:18:42,830
the three models and that brings us to the end of this lesson in the next one.

200
00:18:42,890 --> 00:18:47,060
We're gonna take our evaluation of our regression even further.

201
00:18:47,120 --> 00:18:53,930
We're going to look at how far off our models predictions were from their true values.

202
00:18:53,930 --> 00:19:00,090
We're gonna be looking and analyzing how our regression residuals how.

203
00:19:00,150 --> 00:19:06,570
As an aside while putting this lesson together for you I found myself googling the word big hand trying

204
00:19:06,570 --> 00:19:11,920
to read up on the information criterion a hand for a split second I was quite confused.

205
00:19:11,930 --> 00:19:18,900
Well hi all I got on the front page was information about pens hand the website of the Bournemouth International

206
00:19:18,900 --> 00:19:19,840
Centre.

207
00:19:20,130 --> 00:19:25,960
The other time that happened to me was when I read up on nested tables then I was confronted with this.

208
00:19:26,400 --> 00:19:31,020
So yeah if you have any stories like this to share please put them in the comments section for this

209
00:19:31,020 --> 00:19:31,650
video.

210
00:19:31,740 --> 00:19:33,930
Laughter here at CNN next lesson.
