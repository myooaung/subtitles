1
00:00:00,600 --> 00:00:01,070
All right.

2
00:00:01,100 --> 00:00:06,460
So in this lesson we will save and export our machine learning model.

3
00:00:06,480 --> 00:00:07,910
Why is this important.

4
00:00:07,980 --> 00:00:09,330
Why bother saving a model.

5
00:00:09,840 --> 00:00:16,940
Well first off this can massively cut down on trading times if your model takes 20 minutes to train.

6
00:00:17,130 --> 00:00:22,050
Then saving the model at the end of training and loading it next time will save you 20 minutes.

7
00:00:22,050 --> 00:00:27,420
But if your model takes 10000 gp hours to train then saving is even more important because it allows

8
00:00:27,420 --> 00:00:30,030
you to pick up where you left off.

9
00:00:30,030 --> 00:00:35,610
But even we had this problem in our Jupiter notebooks every time we closed our computer or restarted.

10
00:00:35,610 --> 00:00:40,860
Then we had to rerun all the cells in our notebook and retrain our models from scratch.

11
00:00:40,890 --> 00:00:44,860
So saving models to save on training time is really really helpful.

12
00:00:44,970 --> 00:00:48,070
But in our case we want to take it one step further still.

13
00:00:48,090 --> 00:00:51,890
We want to be able to save our models that we can actually run it on the web.

14
00:00:51,990 --> 00:00:53,210
Right.

15
00:00:53,220 --> 00:00:57,990
We want our model to be able to protect live data from our Web site.

16
00:00:58,320 --> 00:01:02,410
And that means we need a way of being able to export the entire model.

17
00:01:02,460 --> 00:01:08,450
We got to have a train model running on this Web site so that we can make these predictions and intensive

18
00:01:08,450 --> 00:01:11,520
flow parlance this is called serving the model.

19
00:01:11,520 --> 00:01:17,400
So let's dive right into Jupiter and take the first step namely saving our model to fire up Jupiter.

20
00:01:17,520 --> 00:01:25,940
If you're on Windows simply open the anaconda prompt and then type Jupiter notebook and hit enter if

21
00:01:25,940 --> 00:01:26,910
you're on Mac.

22
00:01:26,930 --> 00:01:35,510
You can go here and bring up terminal and type the very same command Jupiter notebook.

23
00:01:35,510 --> 00:01:40,880
And what this should do it should fire up Jupiter and drop you off where you can see your folders.

24
00:01:40,990 --> 00:01:47,090
Now I'm going to go to documents and then e-mail projects and here I've got the notebook that I've worked

25
00:01:47,090 --> 00:01:49,130
on in the previous module.

26
00:01:49,130 --> 00:01:53,880
Number eleven neural networks hyphen TAF handwriting recognition.

27
00:01:53,930 --> 00:01:59,620
This is the notebook that we created to predict the handwritten digits for the endless dataset.

28
00:01:59,690 --> 00:02:08,660
If you've got this notebook right here simply go to duplicate right here click duplicate and then what

29
00:02:08,660 --> 00:02:15,280
you can do is simply select the copy and click rename and what we're gonna do is gonna give this notebook

30
00:02:15,460 --> 00:02:16,050
and you name.

31
00:02:16,080 --> 00:02:20,540
We're gonna call it 12th T F saved model.

32
00:02:20,920 --> 00:02:28,230
Export now if you don't have the code for the previous module you can also download this notebook from

33
00:02:28,230 --> 00:02:32,190
the lesson resources and simply upload it to Jupiter right here.

34
00:02:34,120 --> 00:02:39,240
Then simply open the notebook and then go to sell and click Run all.

35
00:02:39,560 --> 00:02:46,120
And while Jupiter's doing its thing let's have a look on google and see how we can save our tensor flow

36
00:02:46,120 --> 00:02:47,260
models.

37
00:02:47,320 --> 00:02:50,850
Let's pull up the documentation from tensor flow.

38
00:02:50,890 --> 00:02:53,700
The first link should take you to a page like this.

39
00:02:53,710 --> 00:02:58,920
This is a really long and detailed article and it's actually not all that clear.

40
00:02:59,140 --> 00:03:04,750
What we need to do to make sense of this document we need to understand a little bit more about what

41
00:03:04,750 --> 00:03:06,110
our options are.

42
00:03:06,130 --> 00:03:08,790
So let me show you a bit of a high level overview.

43
00:03:08,890 --> 00:03:12,460
There are actually two different types of saving intensive flow.

44
00:03:12,460 --> 00:03:18,790
The first has to do with checkpoints and the second has to do with something called a saved model.

45
00:03:18,880 --> 00:03:20,200
But what's the difference.

46
00:03:20,200 --> 00:03:24,610
Well these two types of saving are actually used for very different purposes.

47
00:03:24,610 --> 00:03:30,010
So the checkpoint is used to save variables what variables.

48
00:03:30,010 --> 00:03:33,970
Well things like connection weights that you've estimated in your neural network.

49
00:03:33,970 --> 00:03:39,850
The saved model on the other hand is also used to save variables but it also saves the entire graph

50
00:03:40,270 --> 00:03:43,110
and the metadata about the graph.

51
00:03:43,120 --> 00:03:48,580
This means Yes it saves the connection weights but it also saves the structure for all the calculations

52
00:03:48,580 --> 00:03:52,350
that take place and even the names of these calculations.

53
00:03:52,540 --> 00:03:54,770
What are the checkpoints useful for.

54
00:03:54,820 --> 00:04:01,120
Well the checkpoints are primarily used during training say while you're running your loop and iterating

55
00:04:01,150 --> 00:04:02,590
over those epochs.

56
00:04:02,590 --> 00:04:06,590
Perhaps you want to save your progress every 10000 steps or so.

57
00:04:06,730 --> 00:04:10,000
This allows you to resume training from a checkpoint.

58
00:04:10,120 --> 00:04:16,720
If things go wrong a saved model on the other hand is primarily used after training has been completed

59
00:04:17,890 --> 00:04:23,710
and since her goal is to create our website our math garden and have our model running there we're gonna

60
00:04:23,710 --> 00:04:30,340
go down the saved model route because the saved model is suitable for serving the saved model.

61
00:04:30,370 --> 00:04:34,570
Not only gives us the variables but it also gives us the graph and the metadata.

62
00:04:35,200 --> 00:04:40,750
So now that we have a little bit of an overview over the two types of saving let's go back to the documentation

63
00:04:41,110 --> 00:04:43,690
and have a look what it says there.

64
00:04:43,750 --> 00:04:49,030
If I look here on the right hand side I see that there is actually a section on how to build and load

65
00:04:49,210 --> 00:04:50,990
a saved model.

66
00:04:51,370 --> 00:04:57,550
And if I go there then in the first section I can see that the easiest way to create a saved model is

67
00:04:57,550 --> 00:05:04,480
to use a function called simple Save this function even has the word simple in it because it just needs

68
00:05:04,480 --> 00:05:06,700
for things to accomplish this goal.

69
00:05:07,000 --> 00:05:13,810
And those four things are a session an export directory some inputs and some outputs.

70
00:05:13,810 --> 00:05:15,530
So let's talk about these in turn.

71
00:05:15,580 --> 00:05:17,110
What's the session.

72
00:05:17,140 --> 00:05:23,710
Well the session that we have to provide is just a session object that we're using to train our model.

73
00:05:23,710 --> 00:05:29,860
And you can see the session object right here in our Jupiter notebook so what about the second thing

74
00:05:29,860 --> 00:05:30,380
here.

75
00:05:30,400 --> 00:05:32,410
The export directory.

76
00:05:32,410 --> 00:05:36,460
Well let's pull up some more details about this function by clicking here.

77
00:05:38,130 --> 00:05:45,510
You can also find all of these links in the lesson resources scrolling down we can see that the second

78
00:05:45,510 --> 00:05:51,860
argument the export directory is simply the path to which the saved model is stored.

79
00:05:52,080 --> 00:05:54,580
Saving a model will actually create a number of files.

80
00:05:54,600 --> 00:05:58,310
So what we're gonna do is we're just going to provide a folder name here.

81
00:05:58,320 --> 00:06:03,500
Now what about the inputs and the outputs to understand what these are.

82
00:06:03,510 --> 00:06:10,490
Let's first pull up 10 support somewhere or create a new window in my terminal and there and when a

83
00:06:10,500 --> 00:06:18,690
fire up 10 support by typing tensor board dash dash log here and point tensor board to where my log

84
00:06:18,690 --> 00:06:26,160
files are being saved the simplest way to get this project path is simply to open up the email project

85
00:06:26,160 --> 00:06:35,260
folder in Windows Explorer or on Mac finder and just drag this into your window.

86
00:06:35,330 --> 00:06:40,290
Now hit enter and then copy this address here.

87
00:06:41,410 --> 00:06:44,770
And navigate to it in your browser.

88
00:06:44,860 --> 00:06:50,540
Now click on graphs and make sure you've got your most recent run selected.

89
00:06:50,620 --> 00:06:53,910
If you take a look at this graph can you spot the inputs.

90
00:06:54,100 --> 00:06:58,510
Or can you spot what the inputs should be to our simple save function.

91
00:06:58,510 --> 00:07:03,300
The input in fact is this little X at the bottom.

92
00:07:03,310 --> 00:07:06,370
This X here represents our features.

93
00:07:06,370 --> 00:07:13,720
This tensor is going to hold all the pixel values for the hand drawn digits it's these inputs that feed

94
00:07:13,780 --> 00:07:15,280
into our old neural network.

95
00:07:15,400 --> 00:07:20,650
Starting with the input layer going through the dropout layer then our second hidden layer and then

96
00:07:20,650 --> 00:07:23,010
finally our output layer.

97
00:07:23,080 --> 00:07:26,500
So where are these inputs in our Jupiter notebook.

98
00:07:26,500 --> 00:07:30,370
Well they're right at the top where we created our tensor flow placeholder.

99
00:07:30,400 --> 00:07:37,540
It's this tensor that holds onto the input data for our neural network but what about the output of

100
00:07:37,540 --> 00:07:39,050
our neural network.

101
00:07:39,100 --> 00:07:42,170
Remember the output of the neural network that we're after.

102
00:07:42,340 --> 00:07:44,270
It's gonna be our prediction.

103
00:07:44,290 --> 00:07:46,250
Let's take another look at transport.

104
00:07:46,330 --> 00:07:53,310
Where would we find the output well the first place I would look is in our third hidden layer.

105
00:07:53,460 --> 00:07:55,070
We've even named this layer out.

106
00:07:55,770 --> 00:07:59,050
So is this layer the output that we should save.

107
00:07:59,910 --> 00:08:02,500
Well close but not quite.

108
00:08:02,520 --> 00:08:09,270
This is the last layer of the neural network which is true but what we're actually after is the prediction

109
00:08:09,570 --> 00:08:11,370
of our model.

110
00:08:11,370 --> 00:08:17,760
If we take a look back into our Jupiter notebook we can see that our last layer our output layer is

111
00:08:17,760 --> 00:08:19,190
defined here.

112
00:08:19,300 --> 00:08:25,770
We're calling this function setup layer and what we're doing there is we're multiplying our weights

113
00:08:26,190 --> 00:08:28,500
and we're adding our biases.

114
00:08:28,500 --> 00:08:33,900
And then for the activation function we're specifying these soft Max activation function.

115
00:08:34,250 --> 00:08:40,020
The soft Max activation function if you remember will give us a probability for each of the different

116
00:08:40,020 --> 00:08:47,040
handwritten digits it might say something like this a 5 percent chance that this digit is a one or a

117
00:08:47,040 --> 00:08:51,860
20 percent chance that it is a two and a 30 percent chance that it's a 3.

118
00:08:51,960 --> 00:09:00,240
And so on but even this is actually not the calculation that will give us the final prediction because

119
00:09:00,240 --> 00:09:06,690
the final prediction will be the largest of these probabilities from the soft max function.

120
00:09:06,690 --> 00:09:08,180
So where do we get that.

121
00:09:08,220 --> 00:09:11,950
Where did we get the largest probability from the output.

122
00:09:11,970 --> 00:09:17,610
Well we're actually got the largest probability from our output right here where we're calculating the

123
00:09:17,730 --> 00:09:25,110
accuracy of our model and this line of code we're checking if the true value the actual value of the

124
00:09:25,110 --> 00:09:28,530
label is equal to the predicted value.

125
00:09:28,530 --> 00:09:35,910
This here is our prediction and we're checking for equality between our prediction and the actual label.

126
00:09:36,180 --> 00:09:41,540
What we can actually do is modify this code a little bit to make this even more explicit.

127
00:09:41,610 --> 00:09:47,850
So I'm actually going to create a little variable here called Model underscore prediction and I want

128
00:09:47,850 --> 00:09:50,920
to use it to split out that line of code below.

129
00:09:51,110 --> 00:09:59,130
I'm going to set this equal to TAF dot org Max and that's actually just going to be equal to this quantity

130
00:09:59,370 --> 00:10:00,540
right here.

131
00:10:00,900 --> 00:10:08,400
Right now what I can do is I can replace this with model underscore prediction.

132
00:10:08,400 --> 00:10:14,820
I think that makes it a little bit more explicit as to what calculation in our tensor flow graph actually

133
00:10:14,820 --> 00:10:18,960
corresponds to the final prediction of our model.

134
00:10:19,020 --> 00:10:24,240
And just to make it show up on our graph very very explicitly which calculation is actually the prediction

135
00:10:24,480 --> 00:10:30,180
I'm going to give this particular calculation a name and when I do that by providing this extra argument

136
00:10:30,190 --> 00:10:36,350
here name is equal to single quotes prediction all lowercase.

137
00:10:36,360 --> 00:10:42,240
Now what I can do is come back up here where I'm setting up my tensor flow graph and go to sell run

138
00:10:42,240 --> 00:10:43,470
all below.

139
00:10:43,470 --> 00:10:51,040
And what this will do is it'll refresh the graph in 10 support allowing me to press refresh here and

140
00:10:51,040 --> 00:10:59,970
in selecting my latest run this one right here if I select my accuracy calculation here and just double

141
00:10:59,970 --> 00:11:05,380
click on it to expand it then what I see is my prediction right here.

142
00:11:05,520 --> 00:11:12,720
The prediction of my model with the name prediction all lowercase this year is the output that we're

143
00:11:12,720 --> 00:11:16,380
gonna provide to these simple save function.

144
00:11:16,430 --> 00:11:21,350
Now the really nice thing about tensor board is that it makes the full name of this calculation very

145
00:11:21,350 --> 00:11:21,860
very clear.

146
00:11:22,550 --> 00:11:27,880
We can see that the full name is accuracy underscore calc slash prediction.

147
00:11:28,040 --> 00:11:29,450
We're going to need this later on.

148
00:11:29,450 --> 00:11:34,970
So what I want to do is I want to click on it and I want to copy it and I encourage you to do the same.

149
00:11:35,060 --> 00:11:40,480
Copy this name verbatim so that you can use it later on.

150
00:11:40,490 --> 00:11:45,770
Now if you look back at the documentation for the simple save function we can make more sense of the

151
00:11:45,770 --> 00:11:54,170
description for the inputs and the outputs both the inputs and the outputs need to be a dictionary mapping

152
00:11:54,200 --> 00:12:03,890
the string input names to tenses the string input name is gonna be the name of our calculation and the

153
00:12:03,920 --> 00:12:07,580
tensor is going to be a model underscore prediction.

154
00:12:08,180 --> 00:12:15,690
So now let's write the code to save our model insert a cell after you finish training.

155
00:12:15,690 --> 00:12:20,660
Make this a markdown cell and just write saving the model.

156
00:12:21,180 --> 00:12:26,960
And here what we're gonna do is we're in a create a variable called outputs.

157
00:12:27,140 --> 00:12:29,700
We're gonna set that equal to a dictionary.

158
00:12:29,700 --> 00:12:30,150
Right.

159
00:12:30,180 --> 00:12:36,270
So curly braces and it's going to be a dictionary mapping the String output names.

160
00:12:36,270 --> 00:12:45,030
So this named right here to a tensor so in our case the name is the name of the calculation and the

161
00:12:45,030 --> 00:12:50,930
tensor it's gonna be model underscore prediction creating the dictionary for the inputs.

162
00:12:50,940 --> 00:12:53,570
Luckily it's much much easier there.

163
00:12:53,580 --> 00:12:55,910
The name is just capital X..

164
00:12:56,400 --> 00:12:59,610
And our tensor is also just capital X..

165
00:12:59,850 --> 00:13:03,270
So I'll create a variable here called inputs.

166
00:13:03,270 --> 00:13:09,720
And this is gonna be a dictionary mapping the string name capital X to our tensor.

167
00:13:09,840 --> 00:13:15,330
Also capital X. Now the spelling here is very very important.

168
00:13:15,540 --> 00:13:19,340
These names they have to match the names that you see intensive.

169
00:13:19,440 --> 00:13:21,080
Exactly.

170
00:13:21,090 --> 00:13:23,670
So now let's call the simple save function.

171
00:13:23,670 --> 00:13:25,620
How would we do that.

172
00:13:25,650 --> 00:13:32,210
Well simple save is the short name but what's the full name the long name.

173
00:13:32,240 --> 00:13:36,170
Well in this case it be TAF dot saved on a scroll model.

174
00:13:36,170 --> 00:13:37,590
Dot simple safe right.

175
00:13:38,990 --> 00:13:45,260
And normally I would use this long name but if I scroll down a little bit then I get this little box

176
00:13:45,260 --> 00:13:45,400
here.

177
00:13:45,420 --> 00:13:52,250
This little red box warning me that there's some changes coming up in tensor flow tensor flow like every

178
00:13:52,250 --> 00:13:58,130
other bit of software is constantly being updated and new versions are always coming out to make sure

179
00:13:58,130 --> 00:14:00,410
our code doesn't break in the future.

180
00:14:00,440 --> 00:14:05,230
What we have to do is we have to access the simple save function this way.

181
00:14:05,360 --> 00:14:07,610
We have to use this long name for it.

182
00:14:07,610 --> 00:14:14,570
TMF dot com Pat dot V1 dot saved underscore a model that's simple safe.

183
00:14:14,600 --> 00:14:20,270
So what I'm gonna do is I'm going to copy the long name right here and then in Jupiter I want a pasted

184
00:14:20,270 --> 00:14:26,270
in and at the end I'm going to provide those four inputs and provide our session.

185
00:14:26,270 --> 00:14:28,880
I want to provide an export directory.

186
00:14:28,880 --> 00:14:36,980
Someone is gonna call this saved model in single quotes and then I'm going to provide the inputs and

187
00:14:37,100 --> 00:14:41,760
outputs the two dictionaries that we've created on the two lines above.

188
00:14:42,230 --> 00:14:43,280
And that's it.

189
00:14:43,280 --> 00:14:44,700
Now it's time to run our code.

190
00:14:45,230 --> 00:14:52,490
But before we do let's bring up our windows explorer or our Mac finder here side by side with our Jupiter

191
00:14:52,490 --> 00:14:53,470
notebook.

192
00:14:53,480 --> 00:15:00,920
That way we can see exactly how this folder is going to be created and what files are going to be saved

193
00:15:01,040 --> 00:15:02,900
for a model.

194
00:15:02,960 --> 00:15:07,910
So when I just go here to Colonel restart and run all.

195
00:15:08,240 --> 00:15:13,140
And then what I'm gonna do is going to keep an eye on this directory here.

196
00:15:13,310 --> 00:15:18,680
The reason that I'm actually running all my cells again is because in order to save our model we actually

197
00:15:18,680 --> 00:15:22,880
have to have a working running session.

198
00:15:22,880 --> 00:15:25,430
My previous session was actually closed.

199
00:15:25,460 --> 00:15:28,830
So that's why I reran the cells in my notebook.

200
00:15:29,330 --> 00:15:36,110
And when all goes well you should see your saved model directory appear here and inside you have a PDB

201
00:15:36,110 --> 00:15:44,770
file and you have some more files for your variables so congratulations you've not successfully saved

202
00:15:44,770 --> 00:15:47,500
your train model including all of its variables.

203
00:15:47,500 --> 00:15:54,100
And the graph itself in the next lesson I want to show you how to load this model in a fresh notebook

204
00:15:54,490 --> 00:15:58,310
and be able to make predictions using this model right away.

205
00:15:58,920 --> 00:16:02,630
For all of that and more I'll see you in the next lesson.

206
00:16:02,640 --> 00:16:03,190
Take care.
