1
00:00:00,480 --> 00:00:08,410
Okay so previously we've split and shuffled our data and put everything into a data frame.

2
00:00:08,430 --> 00:00:12,780
Now let's create that sparse matrix to do that.

3
00:00:12,870 --> 00:00:18,930
We will use three things we will use our extreme data frame.

4
00:00:18,930 --> 00:00:28,860
We will use our why train penned a series and we will also use our vocabulary words.

5
00:00:28,860 --> 00:00:29,820
Remember those.

6
00:00:30,030 --> 00:00:31,530
They look like this.

7
00:00:31,950 --> 00:00:38,190
Half vocabulary of two thousand five hundred words is stored in a data frame where the index are the

8
00:00:38,190 --> 00:00:45,140
word ideas and the individual strings are in a column called vocab on a school word.

9
00:00:45,180 --> 00:00:53,130
Now if we've got this data frame here and we want to know which word has word idea number three then

10
00:00:53,130 --> 00:00:55,140
we can find that really really easily.

11
00:00:55,140 --> 00:01:03,750
And we've done this before because all you'd have to specify is the index and the column and then you'd

12
00:01:03,750 --> 00:01:11,620
get a string that reads e-mail but see we know the word and you want to know the word I.D..

13
00:01:12,300 --> 00:01:14,390
Now we're asking this question in reverse.

14
00:01:14,460 --> 00:01:16,830
We're asking it the other way around.

15
00:01:16,950 --> 00:01:22,420
For example what is the word I.D. for the string Email.

16
00:01:22,500 --> 00:01:30,070
An easy way to answer this question with some Python code is to create an index from this column here

17
00:01:30,780 --> 00:01:39,600
from the vocab word column and then look up the position of a particular string in the index.

18
00:01:39,600 --> 00:01:41,430
Let me show you what I mean.

19
00:01:41,750 --> 00:01:54,360
I'll add a quick mark down somehow that reads create a sparse matrix for the training data.

20
00:01:54,360 --> 00:02:01,020
Now to turn a particular column of our data frame into an index all we'd have to do is select our data

21
00:02:01,020 --> 00:02:11,400
frame select the column and then wrap that whole thing into some parentheses and put it inside PD dot

22
00:02:12,270 --> 00:02:13,800
index.

23
00:02:13,800 --> 00:02:22,440
This will create an index from a particular column in a data frame that store this in a variable called

24
00:02:22,440 --> 00:02:23,360
word on a score.

25
00:02:23,370 --> 00:02:34,640
Index so word underscore index is equal to PD dot index parentheses vocab dot vocab underscore word.

26
00:02:34,710 --> 00:02:41,940
Now we know we're dealing with a index with a penis index because the type of word on the school index

27
00:02:42,420 --> 00:02:52,970
is pen does not call a dot indexes dot based on index and this index is composed of strings individual

28
00:02:52,970 --> 00:02:57,090
strings like h TTP email get and so on.

29
00:02:57,110 --> 00:03:05,690
The ones we saw earlier and I can verify this if I say pull up the C fourth word at index position number

30
00:03:05,900 --> 00:03:12,590
three and check the type of this would end there you can see that indeed we're dealing with strings

31
00:03:12,950 --> 00:03:22,910
inside our index if I come back up here and take a look at our first email hit hint ha X underscore

32
00:03:22,920 --> 00:03:32,300
train data frame I can see that it's probably the stemmed word for Thursday right THQ now if I wanted

33
00:03:32,300 --> 00:03:45,120
to note the word I d for t h you in r word and X I can simply take our index and use the get underscore

34
00:03:45,530 --> 00:03:55,820
location method with t h you passed in as an argument and I see that this would that position of a three

35
00:03:55,820 --> 00:03:58,510
hundred and ninety five.

36
00:03:58,610 --> 00:04:05,690
So here's what we're gonna do we're going to take our X on unescorted train data frame and we'll take

37
00:04:05,690 --> 00:04:12,590
the information contained therein to create our sparse matrix let's walk through how we would do it

38
00:04:12,710 --> 00:04:18,730
with the first row that I've shown here the document idea for this row is four thousand eight hundred

39
00:04:18,760 --> 00:04:26,620
and forty four we would be able to retrieve this information from our X train using our index since

40
00:04:26,620 --> 00:04:32,830
our document ideas are stored as the index values since four thousand eight hundred forty four is the

41
00:04:32,830 --> 00:04:39,550
very first entry the very first value in the index we would be able to just say give us the value of

42
00:04:39,550 --> 00:04:47,710
the index at position 0 that's what we can use to fill in our document daddy of the sparse matrix now

43
00:04:47,770 --> 00:04:52,560
what about the label what about the category that this email belongs to.

44
00:04:52,660 --> 00:05:00,340
For that we simply look towards the Y on a squad train panda series there at the entry named four thousand

45
00:05:00,430 --> 00:05:05,200
eight hundred and forty four we would either get a 1 or a 0.

46
00:05:05,260 --> 00:05:11,070
So this email actually is a non spam email so it would have the label zero.

47
00:05:11,080 --> 00:05:16,930
Now let's tackle that first stemmed would THQ for Thursday.

48
00:05:16,930 --> 00:05:19,170
Here are word index comes into play.

49
00:05:19,450 --> 00:05:26,930
We can get the word I.D. for this string THQ from our word index using that get location method.

50
00:05:27,340 --> 00:05:34,930
And as we saw earlier Pete is at position number three hundred and ninety five for that last column

51
00:05:35,050 --> 00:05:38,380
in the sparse matrix will simply add a 1.

52
00:05:38,480 --> 00:05:44,410
And that's simply because we've counted one occurrence in fact on our first pass we'll add one for all

53
00:05:44,410 --> 00:05:51,220
the occurrences will combine the occurrences later let's move on to that next would J you L. short for

54
00:05:51,430 --> 00:05:57,970
July the document I.D. and the label of course status same four thousand eight hundred and forty four

55
00:05:58,150 --> 00:06:06,370
and non spam and then we simply again use our word index and get the location for this particular string

56
00:06:07,390 --> 00:06:11,920
and this string has the word I.D. four hundred and ninety four.

57
00:06:11,920 --> 00:06:18,960
And again that cause a single time now because we've actually saved all our word I.D. as a CSB file

58
00:06:19,470 --> 00:06:25,410
we can actually verify the word I.D. and Microsoft Excel or Google Sheets if I double click on this

59
00:06:25,410 --> 00:06:29,460
file and I scroll down to what do we see.

60
00:06:29,460 --> 00:06:33,710
Position four hundred and ninety four.

61
00:06:33,840 --> 00:06:37,560
Then I see my stemmed word right here.

62
00:06:37,600 --> 00:06:41,590
Next up it's that third would rodent.

63
00:06:41,590 --> 00:06:50,940
So let's see what happens when we check whether the word rodent is part of how word index if we do this

64
00:06:51,500 --> 00:07:01,470
word on this call index don't get location parentheses single quotes rodent we will actually get an

65
00:07:01,470 --> 00:07:09,530
error and that's because the word rodent doesn't occur frequently enough to have made it into our vocabulary.

66
00:07:09,780 --> 00:07:14,120
In other words the word rodent will not be added to our sparse matrix.

67
00:07:14,160 --> 00:07:16,980
Instead we move on to the next word.

68
00:07:17,610 --> 00:07:26,390
Our next word is in fact this one right here checking our index we find that the word I.D. is two thousand

69
00:07:26,390 --> 00:07:28,830
three hundred eighty six.

70
00:07:28,970 --> 00:07:33,710
This is essentially the workflow of how we will build up our sparse matrix.

71
00:07:33,710 --> 00:07:39,050
We're going to put all of this work into a loop and then wrap all of that into a function.

72
00:07:39,110 --> 00:07:40,690
So let's get on it.

73
00:07:40,730 --> 00:07:42,530
Here's what our function will look like.

74
00:07:42,900 --> 00:07:49,330
As always we're going to start out with RDF keyword define to create our function we'll call this function.

75
00:07:49,520 --> 00:07:55,660
Make under school sparse on the school matrix a very imaginative name.

76
00:07:55,730 --> 00:08:01,970
But it's very clear and I reckon this function should take three inputs a data frame an index for the

77
00:08:01,970 --> 00:08:07,510
word I.D. So let's call this one indexed on a school Woods.

78
00:08:07,820 --> 00:08:12,330
And then third it should take in the labels namely the y values.

79
00:08:12,430 --> 00:08:14,110
So that'll be our third input.

80
00:08:14,110 --> 00:08:15,810
But a colon at the end.

81
00:08:16,250 --> 00:08:20,650
And now we can add a quick description of what this function should do.

82
00:08:20,660 --> 00:08:28,400
Three double quotes hence a doc string and we'll provide a pretty very quick description right.

83
00:08:28,400 --> 00:08:36,320
Returns sparse matrix as data from our inputs are gonna be as follows.

84
00:08:36,380 --> 00:08:38,000
F is

85
00:08:40,500 --> 00:08:47,160
a data frame with words in the columns

86
00:08:52,000 --> 00:09:08,700
a document I.D. as an index parentheses x train or X on a school test then the indexed underscore words

87
00:09:09,270 --> 00:09:22,730
parameter it's gonna be an index of words ordered by word I.D. the labels should be that category as

88
00:09:22,730 --> 00:09:23,990
a series.

89
00:09:24,020 --> 00:09:30,770
In other words why train or why the school test.

90
00:09:30,770 --> 00:09:33,020
I think that do for doctoring.

91
00:09:33,030 --> 00:09:35,410
Now let's add the body of the function.

92
00:09:35,660 --> 00:09:41,480
Now as you can see from the docs string this function should work regardless of whether we feed in the

93
00:09:41,540 --> 00:09:49,190
X unescorted train data frame or the X underscore test data frame so let's capture the kind of dimensions

94
00:09:49,190 --> 00:09:53,950
of the data frame that is coming in as an input ahead of time.

95
00:09:54,240 --> 00:10:02,930
I'll create a variable called number of rows and I'll be equal to the input.

96
00:10:03,040 --> 00:10:12,220
Def dodge shape square brackets 0 and the number of columns and R underscore calls.

97
00:10:12,220 --> 00:10:19,430
It's gonna be equal to Def dot shape parentheses what so that's that.

98
00:10:20,160 --> 00:10:25,290
Now within the body of this function I know that I'm going to be doing a lot of lookups I'm going to

99
00:10:25,290 --> 00:10:30,960
be checking if the words in the data frame are part of our vocabulary list.

100
00:10:30,960 --> 00:10:33,620
There's a lot of checks they're going gonna be running as part of our loop.

101
00:10:33,900 --> 00:10:39,000
So I want to be working with a data structure called a python set.

102
00:10:39,000 --> 00:10:50,790
As you recall so I'll say word on the score set as equal to set parentheses indexed on a school words.

103
00:10:51,030 --> 00:10:59,750
Here I'm creating a python set from our index that is being fed into this function as an argument.

104
00:10:59,770 --> 00:11:06,520
Now I'm going to add a nested loop and within that loop I'm going to be adding dictionaries to a Python

105
00:11:06,520 --> 00:11:07,580
list.

106
00:11:07,630 --> 00:11:09,360
Let me have the outline of this loop first.

107
00:11:09,820 --> 00:11:19,150
So I create my empty list first I'll call it dict list and have two square brackets and at the very

108
00:11:19,150 --> 00:11:27,910
end of our function I'm going to return a panda's data frame that is going to be created from this list

109
00:11:28,000 --> 00:11:30,630
that we're gonna be creating inside our loop.

110
00:11:30,670 --> 00:11:34,790
Now in between these two lines of code it's gonna go the meat of our code.

111
00:11:34,930 --> 00:11:43,510
There'll be two loops for I in range parentheses number of rows.

112
00:11:43,660 --> 00:11:49,060
It'll be the outer loop and then there'll be an inner loop for J.

113
00:11:49,210 --> 00:11:53,320
In range number of columns.

114
00:11:53,320 --> 00:12:01,090
So we're gonna go through this data frame that we're feeding in row by row and column by column within

115
00:12:01,090 --> 00:12:01,740
this inner loop.

116
00:12:01,780 --> 00:12:09,080
We're gonna be spending a dictionary to our list every time the loop runs.

117
00:12:09,130 --> 00:12:10,720
Here's how it's gonna work.

118
00:12:10,990 --> 00:12:16,510
The very first thing that we're gonna do is we're gonna get hold of a particular string right.

119
00:12:16,720 --> 00:12:23,020
And by a particular string I mean value in a particular cell because we're gonna iterate through this

120
00:12:23,020 --> 00:12:24,040
data frame.

121
00:12:24,040 --> 00:12:35,780
Row by row and column by column to get hold of a particular word we'll say D F dot I and square brackets.

122
00:12:35,800 --> 00:12:45,460
I comma J in other words we'll be retrieving the word in the eighth row and the J column then we'll

123
00:12:45,460 --> 00:12:54,680
check if that word that we picked out of our data frame is in our words set and if it is then we should

124
00:12:54,680 --> 00:12:56,130
fetch the document Daddy.

125
00:12:56,180 --> 00:13:05,240
The word Ida and the category the document I.D. is gonna be equal to the value of the index in the eye

126
00:13:05,240 --> 00:13:10,210
through CPF dot index square brackets.

127
00:13:10,230 --> 00:13:17,750
I the word I.D. is gonna be equal to the indexed words

128
00:13:21,350 --> 00:13:32,000
get location parentheses word word is a string we can feed that into our get location method to retrieve

129
00:13:32,180 --> 00:13:38,440
the position off this word in our index of words and that will be equal to our word.

130
00:13:39,260 --> 00:13:41,140
Now it's time to get the category.

131
00:13:41,180 --> 00:13:48,660
The category is gonna be r y values at well at the document I.D..

132
00:13:48,680 --> 00:13:49,630
Right.

133
00:13:49,710 --> 00:13:59,550
The y values we said we'd feed in as this labels parameter here so we'll see labels dot at square brackets

134
00:14:00,330 --> 00:14:06,410
dock I.D. how we've got the three things that we need.

135
00:14:06,530 --> 00:14:12,540
And from that we can create a little dictionary to put them all into one data structure and I'll call

136
00:14:12,540 --> 00:14:27,080
that item equal to curly brackets single quotes label colon category comma single quotes doc on a school

137
00:14:27,080 --> 00:14:47,280
I.D. colon doc on a school I.D. comma single quotes occurrence colon one comma single quotes word on

138
00:14:47,280 --> 00:14:48,720
the score I.D..

139
00:14:48,720 --> 00:14:51,240
Colon word I.D.

140
00:14:54,970 --> 00:15:03,820
here I've created a dictionary with four entries the first one has the key label and it gets the y value

141
00:15:03,940 --> 00:15:06,230
the category spam or not spam.

142
00:15:06,310 --> 00:15:13,730
The second is our document I.D. which we'll get the document i.e. that we've extracted here the third

143
00:15:13,940 --> 00:15:18,880
occurrence which is always gonna be equal to one because we're kind of doing a first pass on this.

144
00:15:18,950 --> 00:15:25,480
And every time we discover a word that's part of our vocabulary we'll add it to our data frame.

145
00:15:25,790 --> 00:15:30,390
And the last one has the word I.D. which we've retrieved here.

146
00:15:30,440 --> 00:15:37,940
So now that we have a dictionary for a single item what we can do is take our dict underscore list and

147
00:15:38,360 --> 00:15:41,020
append our item.

148
00:15:41,670 --> 00:15:49,370
So appending all our dictionaries that we're creating as this loop runs individually to our list which

149
00:15:49,370 --> 00:15:56,000
initially starts off empty but then gets populated and the data frame that we're returning from this

150
00:15:56,000 --> 00:15:59,800
function gets created using this list.

151
00:15:59,840 --> 00:16:00,540
Fantastic.

152
00:16:01,340 --> 00:16:04,280
So here's the whole function body in its entirety.

153
00:16:04,280 --> 00:16:06,200
Let me press shift enter on this.

154
00:16:06,920 --> 00:16:14,900
And now let's try and run this baby My scroll down here into this next cell and the first thing there

155
00:16:14,900 --> 00:16:17,860
is actually at some micro benchmarking code.

156
00:16:18,170 --> 00:16:18,660
So.

157
00:16:18,680 --> 00:16:28,340
Percent percent time this will time how long this cell will take to run and how I'm going to store the

158
00:16:28,340 --> 00:16:37,190
results of this function call this data frame in a variable called spots on a school train on the school

159
00:16:37,730 --> 00:16:45,830
DLF and I'll set that equal to make sparse matrix and you guessed it I'm going to give it the training

160
00:16:45,830 --> 00:16:46,790
data right.

161
00:16:46,910 --> 00:17:00,980
X on a school train comma would index comma Y on the school train.

162
00:17:01,510 --> 00:17:04,140
And now let me hit shift enter and let's see what happens.

163
00:17:05,680 --> 00:17:13,630
Now this cell can take quite a long time to execute its passing a lot of data and it's going through

164
00:17:13,720 --> 00:17:19,300
another data frame that has thousands of cells and thousands of columns in it and it's going through

165
00:17:19,300 --> 00:17:24,240
it one entry at a time on the machine that I'm currently on.

166
00:17:24,280 --> 00:17:28,080
This cell takes between five to 10 minutes to run actually.

167
00:17:28,090 --> 00:17:35,420
So I typically step away and grab a cross or a coffee or something and come back when it's done.

168
00:17:35,420 --> 00:17:37,570
And I really encourage you to do the same.

169
00:17:37,570 --> 00:17:40,090
There's no point in like waiting around.

170
00:17:40,180 --> 00:17:46,120
This is actually one of those times where you'll see a dramatic performance difference whether or not

171
00:17:46,270 --> 00:17:50,140
you're using a set data structure or not.

172
00:17:50,140 --> 00:17:57,490
This check here in our inner loop runs thousands of times so any minimal difference in time that this

173
00:17:57,490 --> 00:18:03,430
look up this check takes you'll actually see that build up to quite a significant amount of time.

174
00:18:03,610 --> 00:18:10,540
If we go back up to our constants then you will spot another thing that really determines the size of

175
00:18:10,570 --> 00:18:13,060
the dataset that we're working with.

176
00:18:13,060 --> 00:18:18,850
Yes we imported approximately five thousand eight hundred e-mails that were passing and so on but one

177
00:18:18,850 --> 00:18:24,010
of the key inputs one of the key constraints that we set is actually our vocabulary size.

178
00:18:24,040 --> 00:18:31,750
We set this at 2500 and this vocabulary size will actually determine how big of a matrix we end up with

179
00:18:31,750 --> 00:18:32,930
at the end.

180
00:18:32,950 --> 00:18:39,910
The reason I picked 2500 is because it's relatively large it's going to make all computer work quite

181
00:18:39,910 --> 00:18:48,190
hard but it's nowhere near sort of a commercial application spam filter for a youth based model.

182
00:18:48,190 --> 00:18:54,270
If we were on an operating Hotmail or Gmail or something and we had to build this need based classifier

183
00:18:54,910 --> 00:19:02,230
we would typically set our vocabulary size at 10000 to 15000 words and you can imagine how much data

184
00:19:02,230 --> 00:19:09,120
we would have to crunch and how long we would have to run our machines for.

185
00:19:09,570 --> 00:19:11,990
All right pretty chuffed to see that it's all done now.

186
00:19:12,010 --> 00:19:15,470
It's six minutes and 28 seconds.

187
00:19:15,520 --> 00:19:21,290
This means that we can take a look at the results see if they make sense.

188
00:19:21,320 --> 00:19:30,410
Let's take a look at the first five rows here so sparse train underscored the F square brackets.

189
00:19:30,530 --> 00:19:32,340
On 5.

190
00:19:33,380 --> 00:19:39,110
Well give me the first five rows and here I can see my word ideas.

191
00:19:39,360 --> 00:19:44,830
Each of these words occurs only once and all of these words occur in email.

192
00:19:44,880 --> 00:19:50,790
Number four thousand eight hundred forty four which is a non spam email.

193
00:19:50,810 --> 00:19:59,650
Let's take a look at the shape of this data frame now so sparse on this quatrain on a squat f dot shape

194
00:20:00,460 --> 00:20:11,500
shift into shows us that we've got approximately four hundred and fifty thousand rows in this data frame.

195
00:20:11,500 --> 00:20:13,810
That's an absolutely huge amount.

196
00:20:13,810 --> 00:20:15,150
Almost half a million.

197
00:20:15,430 --> 00:20:22,970
This is one of the reasons why this whole thing took a good six minutes to run on my machine the last

198
00:20:22,970 --> 00:20:29,360
five rows of this data frame look like this spots on a squat train on the score D F square brackets

199
00:20:29,930 --> 00:20:35,670
minus five colon closing square brackets.

200
00:20:35,690 --> 00:20:36,950
Here you go.

201
00:20:37,100 --> 00:20:40,760
All of these rows pertain to email number eight hundred and sixty.

202
00:20:41,120 --> 00:20:43,610
Now one of the reasons why they're here.

203
00:20:43,610 --> 00:20:49,040
Four hundred and fifty thousand rows in this data frame is because we've put each and every single word

204
00:20:49,160 --> 00:20:54,430
from X on a squat train into a separate row.

205
00:20:54,470 --> 00:21:03,380
So if in the same email we have the word those they occur twice it has two separate rows in this data

206
00:21:03,380 --> 00:21:04,530
frame.

207
00:21:04,670 --> 00:21:08,800
What we're gonna do now is we're going to combine these occurrences right.

208
00:21:08,840 --> 00:21:15,460
If a word occurs more than one time in the same email we should combine it in the state of frame.

209
00:21:15,500 --> 00:21:19,010
We should have an occurrence of two for this particular word I.D..
