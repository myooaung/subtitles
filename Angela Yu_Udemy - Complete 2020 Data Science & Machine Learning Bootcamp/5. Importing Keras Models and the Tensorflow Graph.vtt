WEBVTT
1
00:00:00.640 --> 00:00:07.000
Now that we've converted our image into an array of numbers we can start looking more closely at the

2
00:00:07.000 --> 00:00:07.970
models.

3
00:00:08.110 --> 00:00:10.980
And something called the graph.

4
00:00:11.320 --> 00:00:18.550
If you head on over to Caris dot Io slash applications you can actually see all the models that are

5
00:00:18.550 --> 00:00:21.310
available to us with a few lines of code.

6
00:00:21.340 --> 00:00:27.910
Now on this list there is a model called Inception rez net version 2.

7
00:00:27.990 --> 00:00:34.970
So if I click on it I get to see all the documentation pertaining to this model and how to use it.

8
00:00:34.990 --> 00:00:42.310
So let's make sense of this documentation and then implement this in our Python code in this documentation.

9
00:00:42.370 --> 00:00:44.450
We can already see a couple of things.

10
00:00:44.500 --> 00:00:49.290
First off we see where we can import the model from.

11
00:00:49.330 --> 00:00:56.950
So this Inception resonant version to model lives under Kerry's start applications dot Inception and

12
00:00:56.940 --> 00:00:59.370
a score resonant version too.

13
00:00:59.500 --> 00:01:08.320
And what we also see is that we can use the pre train weights simply by specifying that weights should

14
00:01:08.320 --> 00:01:10.310
be equal to image net.

15
00:01:10.330 --> 00:01:16.060
This is what the documentation is saying down here it's giving us the choice of either training this

16
00:01:16.060 --> 00:01:22.330
model ourselves and trying to figure out our own weights or using the pre train weights that we're trained

17
00:01:22.420 --> 00:01:23.990
on image net.

18
00:01:24.010 --> 00:01:26.740
Now image Net itself is actually very interesting.

19
00:01:26.740 --> 00:01:33.970
So let me just quickly explain what image net is and why you might see its name come up a lot on anything

20
00:01:33.970 --> 00:01:41.470
to do with computer vision and machine learning image that is actually a high quality database of roughly

21
00:01:41.470 --> 00:01:49.760
around 14 million images created and maintained by Princeton and Stanford University.

22
00:01:49.960 --> 00:01:57.670
The goal of this image net project was to give researchers access to high quality images that were labeled

23
00:01:57.970 --> 00:01:59.320
and this is the key.

24
00:01:59.350 --> 00:02:07.330
The images on image net have labels on them so that these images can be then used to train machine learning

25
00:02:07.330 --> 00:02:08.840
models.

26
00:02:08.830 --> 00:02:15.640
In other words there is an annotation with each image that goes something like this image contains calls

27
00:02:16.060 --> 00:02:22.570
or there are no calls in this image and these annotations were actually done by hand for the millions

28
00:02:22.570 --> 00:02:25.220
of images in this dataset.

29
00:02:25.290 --> 00:02:31.090
Now if annotating millions of images sounds like a really boring job to you then ask you if you've ever

30
00:02:31.090 --> 00:02:33.550
seen any of these types of things before.

31
00:02:34.120 --> 00:02:41.260
So if you've ever had to sign into a Web site or sign into an app sometimes you're prompted by these

32
00:02:41.620 --> 00:02:45.550
prove that you're not a robot type of exercises.

33
00:02:45.550 --> 00:02:51.850
And these ones are actually called recapture and apparently the way we supposed to prove that we're

34
00:02:51.850 --> 00:03:00.490
not robots is by selecting the images that contain say street signs or storefronts or buses or what

35
00:03:00.490 --> 00:03:00.970
have you.

36
00:03:01.630 --> 00:03:03.400
Well guess what.

37
00:03:03.400 --> 00:03:08.380
Every time we do this we're actually annotating images for Google.

38
00:03:08.410 --> 00:03:09.130
That's right.

39
00:03:09.130 --> 00:03:16.180
We're doing work for Google so that they can then feed this database with annotated images into their

40
00:03:16.180 --> 00:03:20.260
own neural networks horror machine vision programs fun times.

41
00:03:20.260 --> 00:03:20.860
Right.

42
00:03:20.950 --> 00:03:27.940
But back to image net and Caris let's actually load this Inception dot resonant model into our core

43
00:03:27.940 --> 00:03:38.980
lab notebook as per the documentation we're going to see from Charisse dot applications dot Inception

44
00:03:39.290 --> 00:03:49.040
underscore resonant underscore V2 import Inception resonant V2 and you're going to pick this one here.

45
00:03:49.130 --> 00:03:56.800
Capital AI and one word now hit shift enter and then down here.

46
00:03:56.800 --> 00:04:07.690
We're going to insert a markdown cell that reads load Inception resonant I'm going to add some notebook

47
00:04:07.690 --> 00:04:08.470
magic with.

48
00:04:08.470 --> 00:04:16.420
Percent percent time because I want to show you how long this actually takes and then I'm going to store

49
00:04:16.630 --> 00:04:25.360
our model in a variable called Inception underscore model and I want to set that equal to Inception

50
00:04:25.710 --> 00:04:34.550
resonant v 2 and then open parentheses weights is equal to single quotes.

51
00:04:34.570 --> 00:04:37.310
Imagine it all lowercase.

52
00:04:37.390 --> 00:04:38.650
There we go.

53
00:04:38.650 --> 00:04:40.870
That's all we have to do.

54
00:04:41.020 --> 00:04:47.380
If we look back at the documentation we can see that for all the other arguments there's a default value

55
00:04:47.530 --> 00:04:52.870
and we can see that we don't have to supply any additional information to get started.

56
00:04:52.870 --> 00:05:00.220
So let me hit shift enter on the cell and let's watch what happens the very first thing that I see is

57
00:05:00.220 --> 00:05:08.330
a warning and scrolling over I can actually see this is some sort of deprecation warning.

58
00:05:08.500 --> 00:05:14.830
So the people maintaining Caris will probably have to do some work to update this in the future.

59
00:05:14.830 --> 00:05:20.470
And then what we see is actually that a download started and all of this took a total of 45 seconds

60
00:05:20.590 --> 00:05:21.730
until it finished.

61
00:05:22.420 --> 00:05:24.640
So what did it download Exactly.

62
00:05:24.640 --> 00:05:31.990
Well when you scroll over here you can see that the file is called Something something waits on a score

63
00:05:32.140 --> 00:05:35.990
tensor flow underscore time ordering underscore etc etc..

64
00:05:36.000 --> 00:05:38.040
Colonel Stone Age 5.

65
00:05:38.160 --> 00:05:46.270
This here is the name of the weights file that was trained on image net.

66
00:05:46.330 --> 00:05:51.780
This is what we've downloaded and applied to our inception Resnick model.

67
00:05:52.210 --> 00:05:57.560
Now Inception resonant is actually quite a complex neural network.

68
00:05:58.150 --> 00:06:03.460
It's a specialized type of neural network called a convolution all neural network.

69
00:06:04.030 --> 00:06:09.750
So it's definitely on the more complicated side and its structure actually looks something like this.

70
00:06:09.790 --> 00:06:14.440
The one thing I wanted to show you here is that there are certain subtypes of neural networks that have

71
00:06:14.440 --> 00:06:22.360
shown particular promise with particular tasks and the convolution all neural networks have shown particular

72
00:06:22.360 --> 00:06:29.200
promise when it came to computer vision which is why we're gonna be using this kind of model for our

73
00:06:29.200 --> 00:06:31.870
classification exercise here.

74
00:06:31.870 --> 00:06:38.660
Once we got our inception model loaded we can take our inception model into the next part of the setup.

75
00:06:38.950 --> 00:06:52.330
So Inception model dot graph is equal to T F tensor flow don't get default graph and parentheses at

76
00:06:52.330 --> 00:06:55.930
the end lets it shift enter.

77
00:06:55.950 --> 00:07:01.510
Now even though this looks like it didn't really do much setting up the graph intensive low is actually

78
00:07:01.510 --> 00:07:02.500
key.

79
00:07:02.590 --> 00:07:08.890
You see every time you work with tensor flow you're gonna be working with something called a graph tensor

80
00:07:08.890 --> 00:07:13.500
flow is what we're using that will do the calculations for us here in this case.

81
00:07:13.660 --> 00:07:19.780
Caris will provide the neural network but tensor flow will do our calculations and for tensor flow to

82
00:07:19.780 --> 00:07:27.400
do its calculations it will need to actually organize the computation and the way it does this is by

83
00:07:27.400 --> 00:07:32.280
putting all the computations into something called a graph.

84
00:07:32.290 --> 00:07:37.660
This is a really strange concept that refers to wrap your head around but we'll get a chance to talk

85
00:07:37.660 --> 00:07:40.830
about this a lot more detail for now.

86
00:07:40.840 --> 00:07:44.950
What I want to do is I want to show you a picture of what a graph actually looks like.

87
00:07:44.980 --> 00:07:49.990
This is not the graph for our current project but it would be the graph for our next project.

88
00:07:49.990 --> 00:07:52.980
So this would be a bit of a look into the future.

89
00:07:53.370 --> 00:08:00.250
What we're looking at here is the example graph for all the calculations that are taking place and our

90
00:08:00.250 --> 00:08:01.780
next project.

91
00:08:01.780 --> 00:08:08.020
The point is is that tensor flow will use something called a graph to organize its calculations.

92
00:08:08.020 --> 00:08:11.950
Some of these calculations might have to do with training the neural network.

93
00:08:11.950 --> 00:08:15.830
Like doing all the calculations inside the individual layers.

94
00:08:16.090 --> 00:08:21.910
Other bits of the graph pertain to doing all the calculations to estimate the loss and then other bits

95
00:08:21.910 --> 00:08:25.580
of the graph have to do with calculating the accuracy of the model.

96
00:08:25.600 --> 00:08:30.550
Basically the graph represents all the calculations that you're asking tensor flow to do.

97
00:08:30.550 --> 00:08:36.070
Now of course some of these calculations have to do with training the neural network but other bits

98
00:08:36.070 --> 00:08:40.840
might be related to the analytics or they might be related to something completely different.

99
00:08:40.840 --> 00:08:45.690
For example here there are some calculations that have to do with showing the image on screen.

100
00:08:46.030 --> 00:08:52.700
All of these things are represented as individual nodes inside this thing called the tensor flow graph.

101
00:08:52.720 --> 00:08:59.020
That's the reason I'm showing you this slide it nicely illustrates this concept and it allows us to

102
00:08:59.020 --> 00:09:04.060
get a bit more familiar with some of this tensor flow specific vocabulary.

103
00:09:04.060 --> 00:09:10.270
The good news is that we're pretty much done with our setup and now we get to use our model to make

104
00:09:10.270 --> 00:09:17.380
a classification using our sample images so we'll check that out in the next lesson and see how that

105
00:09:17.380 --> 00:09:17.990
works.

106
00:09:18.010 --> 00:09:18.810
I'll see you there.
