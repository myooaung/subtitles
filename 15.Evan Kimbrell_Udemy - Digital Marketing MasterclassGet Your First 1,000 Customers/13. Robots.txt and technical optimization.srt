1
00:00:00,400 --> 00:00:01,790
All right guys welcome back.

2
00:00:01,870 --> 00:00:06,750
OK now that we've talked to link structure and architecture in our last lecture there's a few more techie

3
00:00:06,750 --> 00:00:09,760
kind of best practices you should use to optimize your site.

4
00:00:09,850 --> 00:00:15,600
This lecture I'm going to go over the robot's text file and a few other things to prepare your site

5
00:00:15,630 --> 00:00:17,910
for those creepy crawlers that Google uses.

6
00:00:17,910 --> 00:00:23,160
So a robot stopped text file is a text file that you put on your Web site that tells webcrawler what

7
00:00:23,160 --> 00:00:25,290
to access and what not to access.

8
00:00:25,290 --> 00:00:30,720
On your Web site now Google sends out its web crawler its message is an algorithm that crawls through

9
00:00:30,770 --> 00:00:32,280
Web sites looks for links.

10
00:00:32,280 --> 00:00:37,260
The news there's links to go find other websites and just bounces around pretty much permanently making

11
00:00:37,260 --> 00:00:39,570
their massive index of Web sites.

12
00:00:39,600 --> 00:00:43,890
Anyway you were one of those crawlers hits a Web sites one of the first thing it does is look for a

13
00:00:43,890 --> 00:00:48,690
robot's text file and the file gives you a chance to tell Google a few different things.

14
00:00:48,690 --> 00:00:53,710
One tell Google which directories to index like where your great stuff is located.

15
00:00:53,850 --> 00:00:56,290
Number two you can also tell Google which directories to skip.

16
00:00:56,310 --> 00:01:01,680
Now these be the parts of your site with files that you don't want indexed or showing up on public search

17
00:01:01,680 --> 00:01:02,160
results.

18
00:01:02,160 --> 00:01:07,200
For example you have to skip directories where you developing new stuff or pages that aren't ready yet.

19
00:01:07,200 --> 00:01:12,210
Like say your production server or you know use Wordpress you don't want it to constantly keep displaying

20
00:01:12,210 --> 00:01:14,610
the log in page for your Administration panel.

21
00:01:14,610 --> 00:01:19,290
So goes without saying you might want to add an internal use pages onto that text file and some people

22
00:01:19,290 --> 00:01:24,930
also tell Google to skip directories of certain file types are stored like a directory full of say important

23
00:01:24,930 --> 00:01:26,620
PDX or video files.

24
00:01:26,790 --> 00:01:30,530
Especially if you're making lead pages where they have to sign up to get the file.

25
00:01:30,540 --> 00:01:33,900
Well kind of defeats the purpose of ego and Google search and just find the file.

26
00:01:33,900 --> 00:01:38,090
You can also use the robot's text file to tell Google where your sitemap is located.

27
00:01:38,160 --> 00:01:42,450
In our last lecture where we talked about creating that SML site map for your site is really easy to

28
00:01:42,450 --> 00:01:46,460
just use any free tool we use at SML Dasch sitemap dot com.

29
00:01:46,620 --> 00:01:52,600
While listing that in your robot's text file will tell Google where that sitemap is before it crawls

30
00:01:52,600 --> 00:01:53,630
and indexes anything.

31
00:01:53,670 --> 00:01:57,930
Now truth is you don't have any parts of your site where you're worried about Google finding.

32
00:01:57,930 --> 00:02:01,490
Well in the robot's text file really is not a big concern.

33
00:02:01,530 --> 00:02:03,380
It might not really even be necessary.

34
00:02:03,390 --> 00:02:06,540
My advice either way is to create one so you know it's there.

35
00:02:06,540 --> 00:02:08,980
In fact do you right now know if you have one.

36
00:02:09,010 --> 00:02:10,720
Well this is a good thing to check.

37
00:02:10,800 --> 00:02:16,620
Go to your you r l and then just put Flash robots taxed after the euro and that's generally where it

38
00:02:16,620 --> 00:02:20,260
should be located right in the main root directory of your website.

39
00:02:20,340 --> 00:02:24,890
So if something shows up some lines of text should show up in your browser window you're good.

40
00:02:24,930 --> 00:02:29,970
Very If you can do this for pretty much any Web site out there but some robot's text files are more

41
00:02:29,970 --> 00:02:31,080
elaborate than others.

42
00:02:31,080 --> 00:02:33,830
Here I'll show you an example of one even if you didn't ask for it.

43
00:02:33,930 --> 00:02:36,940
If you go to pita pit you say dot com.

44
00:02:37,140 --> 00:02:39,480
Easily a place I go to 10 times a day.

45
00:02:39,540 --> 00:02:42,060
You'll see the home page for pita pit whatever that is.

46
00:02:42,060 --> 00:02:44,280
They sell PITA's maybe out of the pit.

47
00:02:44,280 --> 00:02:50,010
I'm just I'm just guessing here but if you type in pita pit slash robots are taxed after the address

48
00:02:50,010 --> 00:02:50,340
bar.

49
00:02:50,370 --> 00:02:55,140
Well you'll see a super simple three line text file and you also see where PITA pit hides all their

50
00:02:55,140 --> 00:02:55,660
secrets.

51
00:02:55,710 --> 00:03:00,900
Looks like kind of like this like user agent disallow WordPress blah blah blah blah blah allow WordPress

52
00:03:00,960 --> 00:03:02,280
W admin admin.

53
00:03:02,400 --> 00:03:03,270
OK you get the point.

54
00:03:03,300 --> 00:03:08,520
Here's what the code means the user agent bit with the star after it basically says any crawler can

55
00:03:08,520 --> 00:03:13,530
come in here to crawl and index these pages and you can an attribute tell certain callers not to call

56
00:03:13,530 --> 00:03:16,150
the site or you know just do it in a different way.

57
00:03:16,170 --> 00:03:21,600
Less Crawley disallowed tells the user agent the crawler above not to crawl a certain your Arel and

58
00:03:21,600 --> 00:03:25,170
as you can see here it's a WordPress admin directory like I talked about before.

59
00:03:25,170 --> 00:03:26,430
Internal pages.

60
00:03:26,430 --> 00:03:31,080
They don't what their WordPress log in page for example to be indexed and listed on the search results.

61
00:03:31,080 --> 00:03:31,760
Kind of weird.

62
00:03:31,770 --> 00:03:36,600
Allow of course tells the user agent above definitely Crawl's specific you are Elen in this case it's

63
00:03:36,600 --> 00:03:40,050
a PH P file inside that's in a disallowed directory.

64
00:03:40,180 --> 00:03:42,960
So they've made an exception here for some reason.

65
00:03:42,990 --> 00:03:47,550
Now they thing Peter pick could add if they had it was a u r l of a sitemap.

66
00:03:47,580 --> 00:03:51,670
Now if they created and uploaded next XML sitemap Generally you'd add in a line of code to the bottom

67
00:03:51,670 --> 00:03:54,060
of the robot's text file and look something like this.

68
00:03:54,180 --> 00:03:59,640
Sitemap looks like PITA pit slash wherever sitemap is so pretty much an amateur move on the part of

69
00:03:59,880 --> 00:04:03,300
it so you might wonder if this is the entire robot's text file.

70
00:04:03,300 --> 00:04:07,090
Does that mean Google is only indexing one thing from the site that HP file.

71
00:04:07,110 --> 00:04:08,620
No that's not actually not what it means.

72
00:04:08,640 --> 00:04:12,070
All of the PITA pit people did was note a couple of exceptions.

73
00:04:12,090 --> 00:04:14,360
Don't cross this but do Krull this one.

74
00:04:14,430 --> 00:04:19,020
But aside from that Google will just crawl whatever I can find as it follows links throughout the site

75
00:04:19,050 --> 00:04:21,600
as long as they aren't listed in the no follow section.

76
00:04:21,600 --> 00:04:25,290
Now keep in mind if you don't have one of these robots textfiles at all.

77
00:04:25,290 --> 00:04:27,510
Kernels are just going to assume that everything is fair game.

78
00:04:27,510 --> 00:04:32,460
Now if you're in need of one of these magical text files for your site you also just use a free file

79
00:04:32,460 --> 00:04:35,460
generator from the peep's that it's SBO books Dot com.

80
00:04:35,460 --> 00:04:40,020
Just don't what you are or else you allow or disallow if you have any exceptions in mind and it'll put

81
00:04:40,020 --> 00:04:40,960
it all together for you.

82
00:04:41,040 --> 00:04:43,890
And only if do is just loaded onto the main director of your website.

83
00:04:43,890 --> 00:04:49,350
Now you might be ready to set up a text file and perhaps you know for a fact you a section of your site

84
00:04:49,350 --> 00:04:51,480
where personal information gets stored on pages.

85
00:04:51,480 --> 00:04:52,340
People's ages.

86
00:04:52,350 --> 00:04:55,020
I don't know names credit card numbers and I hope not.

87
00:04:55,020 --> 00:04:57,590
You should really be careful with that stuff specifically.

88
00:04:57,600 --> 00:04:59,150
Now send all that info to me.

89
00:04:59,160 --> 00:05:03,990
I promise I'll take it seriously if you have parts of your site where sensitive information is located

90
00:05:04,260 --> 00:05:09,390
you really do not want that index and then available through Google's ERP is imagine someone searching

91
00:05:09,390 --> 00:05:10,620
for credit card numbers.

92
00:05:10,620 --> 00:05:12,720
I can steal and your site comes up.

93
00:05:12,780 --> 00:05:13,830
That would suck anyway.

94
00:05:13,890 --> 00:05:18,810
In the case where you know pages of data like this that shouldn't be public it's recommended that use

95
00:05:18,810 --> 00:05:20,130
a different kind of code.

96
00:05:20,140 --> 00:05:23,760
Tell Google not to index it and it's called Big surprise.

97
00:05:23,760 --> 00:05:29,640
The no index file instead of listing these pages in the robot's text you go right to the page itself

98
00:05:29,670 --> 00:05:32,370
and go to the head section of your HMO code.

99
00:05:32,380 --> 00:05:36,930
Remember we were doing this before really and it's the title of the page well right in the same exact

100
00:05:36,930 --> 00:05:41,700
place you can add another line of code if you have a page that should not be indexed by adding this

101
00:05:41,700 --> 00:05:45,020
line the robots like Googlebot for example might cross your page.

102
00:05:45,030 --> 00:05:47,480
But when it sees this code it will not index it.

103
00:05:47,490 --> 00:05:50,390
So this way you protect the page and add stuff.

104
00:05:50,400 --> 00:05:51,060
So there you go.

105
00:05:51,150 --> 00:05:53,300
Robots dot tax and no index.

106
00:05:53,310 --> 00:05:55,350
Use them to prepare your site for the crawlers.

107
00:05:55,350 --> 00:06:00,030
Help speed up the process of indexing and protect any parts of your site that shouldn't be made public.

108
00:06:00,030 --> 00:06:06,900
Now one quick question which is should I make one of these even if I have nothing to provide as an exception

109
00:06:06,900 --> 00:06:09,130
for the crawlers and in general.

110
00:06:09,240 --> 00:06:11,460
No it doesn't really matter.

111
00:06:11,460 --> 00:06:15,850
It's not you're not going to get any Seob value inherently from having this file.

112
00:06:15,870 --> 00:06:21,960
It's just more so to help you get the most out of search engines and not to have some awkward situations

113
00:06:22,080 --> 00:06:27,180
and optimizing what shows up in their search results regardless of where it ranks is half the battle.

114
00:06:27,180 --> 00:06:29,340
So anyway that's it for now in the next lecture.
