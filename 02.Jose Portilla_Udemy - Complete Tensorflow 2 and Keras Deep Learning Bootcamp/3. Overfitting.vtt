WEBVTT
1
00:00:05.500 --> 00:00:09.940
Welcome back everyone to continue on our discussion of machine learning theory topics.

2
00:00:09.940 --> 00:00:14.890
We're not going to discuss over fitting and under fitting in how you can recognize if this is occurring

3
00:00:15.190 --> 00:00:21.860
in your own problems so we understand now the full process for supervised learning acquiring the data

4
00:00:21.920 --> 00:00:27.500
clean the data performing that sort of train test split or a trained test validation split and then

5
00:00:27.500 --> 00:00:33.140
training your model comparing its results on the validation data and then finally getting some performance

6
00:00:33.140 --> 00:00:37.350
metric on your test data and then deploying your model to the real world.

7
00:00:37.370 --> 00:00:44.040
What I want to do now is touch upon the important topics of over 50 and under fitting so overfishing

8
00:00:44.130 --> 00:00:47.340
is when the model fits too much to the noise from the data.

9
00:00:47.520 --> 00:00:53.820
And this often results in low error on training sets but high error on your test and validation sets

10
00:00:55.250 --> 00:00:58.230
so let's actually take a look at what overfishing would look like.

11
00:00:58.280 --> 00:01:04.550
For the example case of a really simple dataset where you just have one feature X and you're trying

12
00:01:04.550 --> 00:01:06.230
to predict Y.

13
00:01:06.290 --> 00:01:11.990
So here we have a bunch of training points and so there's some single feature X and at the end of the

14
00:01:11.990 --> 00:01:19.760
day we want to build out a model that can predict Y so a good model would try to fit the general trend

15
00:01:20.270 --> 00:01:26.030
of the actual data set here so we can see it appears that there's some sort of positive slope and it

16
00:01:26.030 --> 00:01:27.230
looks like it levels off.

17
00:01:27.230 --> 00:01:29.310
Later on.

18
00:01:29.340 --> 00:01:33.660
So what would happen if the model over fit to this training data.

19
00:01:33.690 --> 00:01:40.110
So when you actually overfished what's going to happen is you're actually going to fit too much to the

20
00:01:40.110 --> 00:01:41.570
noise of the data.

21
00:01:41.580 --> 00:01:44.730
So here we have what appears to be a really bad model.

22
00:01:45.060 --> 00:01:50.550
But keep in mind that this model is technically hitting every single one of those training points.

23
00:01:50.550 --> 00:01:54.060
So your error here is actually going to be extremely low.

24
00:01:54.080 --> 00:01:59.610
And in this specific example your error is actually zero because your model is accounting for every

25
00:01:59.610 --> 00:02:01.040
point in the training set.

26
00:02:01.140 --> 00:02:05.840
So your error just on the training set is actually really low when you're over Finning.

27
00:02:05.850 --> 00:02:10.980
So that's why it can be really deceptive and that's why we also need the validation and test sets in

28
00:02:10.980 --> 00:02:16.020
order to understand whether or not we're overfishing because just the training data alone won't tell

29
00:02:16.020 --> 00:02:17.580
us that information.

30
00:02:17.580 --> 00:02:22.810
So what happens when you're overfishing is you're fitting too much to the noise of the data here.

31
00:02:22.860 --> 00:02:29.640
And so if you were to get a new point and this would be a test or validation point if you're over fitting

32
00:02:29.700 --> 00:02:32.490
that means you're getting low error on the training set.

33
00:02:32.490 --> 00:02:37.860
But when you get a new test so that the model hasn't seen before you're going to get a much larger error

34
00:02:37.950 --> 00:02:40.660
off your model.

35
00:02:40.660 --> 00:02:45.640
Now let's think about under fitting under fitting is what a model does not capture the underlying trend

36
00:02:45.640 --> 00:02:51.280
that the data and does not fit the data well enough kind of the opposite of overfishing and this results

37
00:02:51.340 --> 00:02:54.470
in low variance by high bias and under fitting.

38
00:02:54.520 --> 00:03:01.360
It's often the result of an excessively simple model so in that same situation if we were to have the

39
00:03:01.360 --> 00:03:07.370
features x and the label y something that was under fitting would just not even be able to grab the

40
00:03:07.370 --> 00:03:08.250
general trend.

41
00:03:08.350 --> 00:03:14.200
So maybe it picks up on that first upward slope but doesn't really grasp the idea that this model should

42
00:03:14.200 --> 00:03:22.080
eventually level off for that value of increasing X now this particular data set was really easy to

43
00:03:22.080 --> 00:03:22.560
visualize.

44
00:03:22.560 --> 00:03:25.120
We just had one feature X and one label y.

45
00:03:25.560 --> 00:03:30.420
But how can we actually see under fitting and over fitting occur when dealing with multi-dimensional

46
00:03:30.420 --> 00:03:31.140
datasets.

47
00:03:31.350 --> 00:03:37.560
Well let's first imagine we train the model and then measured its error overtraining time.

48
00:03:37.560 --> 00:03:40.080
So let's begin with a thought experiment.

49
00:03:40.080 --> 00:03:43.500
Let's imagine we have what should be a good model.

50
00:03:43.500 --> 00:03:49.230
Well as you can imagine a good model when it first encounters the training data it's going to have kind

51
00:03:49.230 --> 00:03:54.540
of a large error because it hasn't actually seen this data before and has adjusted any internal parameters

52
00:03:54.540 --> 00:03:55.610
to this data.

53
00:03:55.620 --> 00:04:02.010
However as you train your model for more time on the training data you would expect the error to essentially

54
00:04:02.010 --> 00:04:06.660
go down until it levels off and converges to some sort of minimum error.

55
00:04:06.660 --> 00:04:08.830
So this is what a good model should look like.

56
00:04:08.970 --> 00:04:14.850
And when we're specifically talking about neural networks this training time actually has a specific

57
00:04:14.850 --> 00:04:20.130
word and depending if you pronounce it the American way which is epic or the British way epoch which

58
00:04:20.130 --> 00:04:27.090
is the way I pronounce it essentially one epoch is passing the entirety of your training data one time

59
00:04:27.510 --> 00:04:29.060
through your model.

60
00:04:29.460 --> 00:04:34.920
So we can see here as we do multiple passes on the entire set of the training data.

61
00:04:34.980 --> 00:04:38.980
Eventually our error will converge to some sort of minimum.

62
00:04:39.060 --> 00:04:43.720
That's a good model but what would a bad model do.

63
00:04:43.850 --> 00:04:44.950
A bad model.

64
00:04:45.110 --> 00:04:48.070
If you train it for more and more time would get increasing error.

65
00:04:48.080 --> 00:04:52.220
And that's not good because essentially your models not learning every time it goes through the train

66
00:04:52.220 --> 00:04:52.600
data.

67
00:04:52.640 --> 00:04:54.270
It's actually increase in the air.

68
00:04:54.470 --> 00:04:59.670
So we have the expected behavior of a good model and the expected behavior of a bad model.

69
00:04:59.690 --> 00:05:04.460
So how can we use this intuition that to understand what over fitting or under fitting would look like

70
00:05:04.850 --> 00:05:10.820
during the training of a model so in thinking about overfishing and under fitting we want to keep in

71
00:05:10.820 --> 00:05:16.730
mind the important relationship of model performance on the training set versus that test or validation

72
00:05:16.730 --> 00:05:23.230
set so let's imagine we've split up our data into a training set and a test set and we're gonna color

73
00:05:23.230 --> 00:05:25.090
them here.

74
00:05:25.110 --> 00:05:29.430
So first we would see the performance on the training set.

75
00:05:29.610 --> 00:05:35.490
And typically you're training set performance is going to decrease in error as you train it for more

76
00:05:35.490 --> 00:05:37.990
epochs.

77
00:05:38.000 --> 00:05:44.040
Next we check the performance on the test set and in an ideal situation you'll get similar behavior.

78
00:05:44.180 --> 00:05:49.970
The test set as you train for more epochs on the training data then your error on the test set will

79
00:05:50.030 --> 00:05:51.550
also decrease.

80
00:05:51.560 --> 00:05:52.760
So it should look the same.

81
00:05:52.760 --> 00:06:01.060
That's the ideal situation so ideally the model perform well on both with similar behavior but what

82
00:06:01.060 --> 00:06:04.860
would happen if we were to over fit on the training data.

83
00:06:04.930 --> 00:06:12.190
So recall from that simple example of just that single feature X and that label y if we were over fitting

84
00:06:12.190 --> 00:06:16.590
on that data we actually still get really good performance on our training data.

85
00:06:16.600 --> 00:06:21.400
However once we introduce that new test point or that new validation point that's where we start to

86
00:06:21.400 --> 00:06:23.380
see a large error occur.

87
00:06:23.380 --> 00:06:26.440
So that means we will start to perform poorly on new test data.

88
00:06:26.440 --> 00:06:32.620
If we were actually over fitting so often you'll see something like this occur where we have that red

89
00:06:32.620 --> 00:06:37.300
line as our training set and then that blue line as our test or validation set.

90
00:06:37.300 --> 00:06:40.680
And essentially this indicates that we're overfishing.

91
00:06:40.780 --> 00:06:45.980
So this is a good indication of training too much on that training dataset and you begin to overfished.

92
00:06:45.990 --> 00:06:50.760
You've done too many passes on that training set and now your model can't generalize to data.

93
00:06:50.760 --> 00:06:51.910
It hasn't seen before.

94
00:06:52.030 --> 00:06:56.070
It's only been able to predict now really well only on data it's seen.

95
00:06:56.110 --> 00:07:00.640
So what we want to do is want to make sure we don't train too much on that specific training set so

96
00:07:00.640 --> 00:07:06.430
we can still generalize to new test or validation data and what we can do is we can plot out this behavior

97
00:07:06.730 --> 00:07:12.790
on the training set versus the test set and then you should look to have a cutoff point on that training

98
00:07:12.790 --> 00:07:13.440
time.

99
00:07:13.510 --> 00:07:16.810
So as you can see the error on the test set start to increase.

100
00:07:16.840 --> 00:07:21.330
That's a good indication that that's where over fitting on the training data would begin.

101
00:07:21.340 --> 00:07:26.170
So you want to kind of choose that cutoff point and it's not always as clear as this where essentially

102
00:07:26.170 --> 00:07:27.070
the Intersect.

103
00:07:27.100 --> 00:07:35.230
But the main idea being if you see an increase in error on that test set as the training error is still

104
00:07:35.230 --> 00:07:38.940
going down that's a good indication that you should stop training there.

105
00:07:39.040 --> 00:07:45.430
And this is an idea I will touch upon later on in the course so as I mentioned we'll check on this idea

106
00:07:45.430 --> 00:07:48.370
again or actually beginning creating models.

107
00:07:48.370 --> 00:07:53.050
For now let's just go ahead and be aware of this possible issue and keep it in the back of our minds

108
00:07:53.110 --> 00:07:55.810
when we're actually creating and training our models.

109
00:07:55.810 --> 00:07:57.090
OK thanks.

110
00:07:57.130 --> 00:07:58.240
And I'll see you at the next lecture.
