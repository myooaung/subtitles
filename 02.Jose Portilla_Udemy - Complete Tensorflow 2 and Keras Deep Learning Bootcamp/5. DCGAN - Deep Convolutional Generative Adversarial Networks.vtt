WEBVTT
1
00:00:05.440 --> 00:00:12.970
Welcome back everyone to this lecture on deep compositional Ganz deep compositional gains are actually

2
00:00:13.030 --> 00:00:15.340
not a huge leap from what we just did.

3
00:00:15.430 --> 00:00:20.920
They basically just add in conversational layers to attempt to build Ganz better suited for image data.

4
00:00:21.100 --> 00:00:25.900
It makes sense that if we use convolution or layers for image classification we should probably also

5
00:00:25.900 --> 00:00:32.620
use them inside of our discriminator to classify fake versus real images and we should also take advantages

6
00:00:32.710 --> 00:00:37.380
of they are unique to the capabilities to actually generate images.

7
00:00:37.390 --> 00:00:43.690
So what we're going to do in this lecture is we're just going to take a look at the DC Gan notebook

8
00:00:43.720 --> 00:00:48.250
that we have for you and we're going to see what the changes are inside of the notebook.

9
00:00:48.250 --> 00:00:52.570
We're not going to actually code this all out because really that there's just a few changes as far

10
00:00:52.570 --> 00:00:57.750
as building out the models and then compiling the models everything else pretty much remains the same.

11
00:00:57.820 --> 00:01:00.480
It's just the actual layers you're putting in there.

12
00:01:00.610 --> 00:01:03.540
So it's just a couple of more imports and then setting up the layers.

13
00:01:03.580 --> 00:01:06.000
And these are things you can always experiment with.

14
00:01:06.010 --> 00:01:08.720
Let's go ahead and load up that notebook and walk you through it.

15
00:01:09.150 --> 00:01:09.520
OK.

16
00:01:09.520 --> 00:01:11.200
Here I am at the notebook.

17
00:01:11.200 --> 00:01:13.980
You'll notice that everything at the top is pretty much exactly the same.

18
00:01:13.990 --> 00:01:19.840
We load up the data however the one thing that we do need to change is reshaping and re scaling the

19
00:01:19.840 --> 00:01:28.060
images our generator at the very end uses a hyperbolic tangent activation function and a hyperbolic

20
00:01:28.060 --> 00:01:35.770
tangent if you actually plot this out the limits of a hyperbolic tangent go from negative 1 to 1 and

21
00:01:35.860 --> 00:01:41.410
our images either go from zero to turn fifty five or if you scale them down they go from zero to one

22
00:01:41.980 --> 00:01:48.340
we actually want to make sure that our training data is going to have the same range off of this hyperbolic

23
00:01:48.340 --> 00:01:50.890
tangent activation in that last layer.

24
00:01:50.950 --> 00:01:55.810
So we need to do is we need to reshape it to be between negative 1 to 1.

25
00:01:55.810 --> 00:01:59.010
So the way we do that is we just take all our training data.

26
00:01:59.200 --> 00:02:05.050
We divide it by two and fifty fives and then everything's between 0 and 1 and then we multiply that

27
00:02:05.050 --> 00:02:07.890
data after reshaping it times 2.

28
00:02:07.930 --> 00:02:14.580
And I have everything between 0 and 2 and then if you subtract 1 well then the lowest values from 0

29
00:02:14.620 --> 00:02:20.290
go to negative 1 and the highest values set to go to 1 and then your minimum becomes negative 1 and

30
00:02:20.290 --> 00:02:25.300
your maximum becomes 1 and if you just take a look at what the hyperbolic tangent looks like you'll

31
00:02:25.300 --> 00:02:26.930
see what I'm talking about there.

32
00:02:26.990 --> 00:02:33.180
OK then we continue just as we did before we we went ahead and filtered out only zeros if you want for

33
00:02:33.190 --> 00:02:38.810
these more complex compositional network based scans you can go ahead and just use all the data.

34
00:02:38.830 --> 00:02:41.290
Just keep in mind training time should take longer.

35
00:02:41.620 --> 00:02:46.660
And then we're going to import a couple more layers here in order to try to get the best performance

36
00:02:46.660 --> 00:02:47.710
possible.

37
00:02:47.740 --> 00:02:53.880
We still have dense and reshape but we're also doing is we're importing leaky rectified linear unit.

38
00:02:54.010 --> 00:02:58.270
It's an activation function that tends to work a little better than just the rectified linear unit.

39
00:02:58.270 --> 00:03:03.490
In this case of deep convolution gains and then we're also going to be performing batch normalization

40
00:03:04.000 --> 00:03:08.320
as well as getting these two compositional to the layers.

41
00:03:08.320 --> 00:03:09.690
And also this transpose layer.

42
00:03:10.090 --> 00:03:16.090
So we have these extra layers and essentially we just build out the generator and discriminator using

43
00:03:16.090 --> 00:03:17.140
those extra layers.

44
00:03:17.260 --> 00:03:23.200
So if we take a look at the generator me zoom in one more here we can see it starts off sequential.

45
00:03:23.200 --> 00:03:28.610
We also have this dense layer here we have seven by seven times one hundred and twenty eight.

46
00:03:28.630 --> 00:03:34.600
You can kind of choose how many neurons you want there by editing 128 and then we reshape this to be

47
00:03:34.780 --> 00:03:37.020
7 by 7 by 128.

48
00:03:37.030 --> 00:03:41.190
And for that batch we then perform batch normalization.

49
00:03:41.230 --> 00:03:46.810
Keep in mind you have a lot of leeway here over how many neurons you want in each layer or how many

50
00:03:46.810 --> 00:03:50.320
kernels there should be in the next convolution layer.

51
00:03:50.320 --> 00:03:56.850
So what we do here is we have a compositional to the transpose layer and then we decide to have sixty

52
00:03:56.850 --> 00:04:03.040
four kernels kernel size is five strides is to note that padding we keep the same lot of things you

53
00:04:03.040 --> 00:04:09.460
can experiment with here and then we add in another layer of batch normalization right before our second

54
00:04:09.850 --> 00:04:16.040
compositional to the layer here and at the very end we just have one output for that image.

55
00:04:16.060 --> 00:04:20.210
OK then we have discriminator which is a sequential model.

56
00:04:20.320 --> 00:04:25.060
And notice it's kind of like a mirrored image a bit of the generator.

57
00:04:25.060 --> 00:04:30.640
So we start off of a convolution layer of sixty four we add a little bit of drop out here 50 percent

58
00:04:30.640 --> 00:04:35.650
drop out to try to prevent over fitting hopefully making discriminator stronger between real images

59
00:04:35.650 --> 00:04:40.840
versus fake images and then we add in another compositional to the layer expanding the number of number

60
00:04:40.840 --> 00:04:46.540
of kernels or image kernels there and then we also add in a drop layer zero point five at the end we

61
00:04:46.540 --> 00:04:53.280
flatten all this out to a single dense neuron essentially saying is it a real image is it a fake image.

62
00:04:53.470 --> 00:05:01.240
And notice the activation is sigmoid because of that OK then we combine these two into the sequential

63
00:05:01.240 --> 00:05:08.260
generator and discriminatory game and then we compile the discriminator using lost binary cross entropy

64
00:05:08.290 --> 00:05:10.200
because it's essentially real or fake.

65
00:05:10.200 --> 00:05:11.080
It's binary.

66
00:05:11.260 --> 00:05:17.710
And then we set discriminative trainable equal to False before we compile the full again and the full

67
00:05:17.710 --> 00:05:22.260
again also takes binary cross entropy with optimizer atom.

68
00:05:22.270 --> 00:05:26.970
And from that it continues the same way we left off last time we can see the two layers.

69
00:05:26.980 --> 00:05:31.860
We can set up the training batches notice training batch set up just the same as before.

70
00:05:31.870 --> 00:05:38.290
One thing to note here is we're going to train this for a lot more epochs because these networks are

71
00:05:38.290 --> 00:05:42.270
a lot more complex going to take a lot more epochs to actually get good results.

72
00:05:42.340 --> 00:05:48.490
And because of that this is really where you need to be using GPA to have a reasonable amount of time

73
00:05:48.490 --> 00:05:49.540
for training.

74
00:05:49.540 --> 00:05:51.610
Training loop is pretty much exactly the same.

75
00:05:51.610 --> 00:05:54.270
So we see the exact same training loop as we did before.

76
00:05:54.460 --> 00:05:59.730
And if you scroll down I trained it for 20 epochs here you can see my results.

77
00:05:59.770 --> 00:06:05.500
Recall that our previous models suffered mode collapse and the actual images themselves weren't super

78
00:06:05.500 --> 00:06:11.690
great but using deep compositional Ganz we've only zeros we can see here we get much better results.

79
00:06:11.740 --> 00:06:17.110
And what's really nice here is we also get variety in our results so we don't just get the same zero

80
00:06:17.290 --> 00:06:18.570
for all these.

81
00:06:18.590 --> 00:06:22.980
OK so really interesting stuff and a lot of stuff you can play around with here.

82
00:06:23.020 --> 00:06:27.070
I would highly encourage you to play around with all the different hyper parameters that are available

83
00:06:27.070 --> 00:06:28.480
to you within this network.

84
00:06:28.510 --> 00:06:32.890
Just make sure to use a GP use you're not waiting forever for all your new designs.

85
00:06:32.900 --> 00:06:35.560
Okay thanks and we'll see you at the next lecture.
