WEBVTT
1
00:00:05.230 --> 00:00:10.270
Welcome back everyone to complete this section of the course we're going to briefly discuss tensor board

2
00:00:11.690 --> 00:00:17.570
tensor board is a visualization tool from Google designed to work in conjunction with tensor flow to

3
00:00:17.570 --> 00:00:20.440
visualize various aspects of your model.

4
00:00:20.620 --> 00:00:25.930
Here we will simply understand how to view the tensor board dashboard in our browser and analyze an

5
00:00:25.930 --> 00:00:27.270
existing model.

6
00:00:27.490 --> 00:00:33.250
A really important note here is that this lecture requires you that you understand file paths and the

7
00:00:33.250 --> 00:00:37.570
current location of either your notebook that you're working in or your dot pi file.

8
00:00:37.570 --> 00:00:38.710
So keep that in mind.

9
00:00:38.830 --> 00:00:43.450
We did a review some of these topics in the pan this input output lecture so you may want to review

10
00:00:43.450 --> 00:00:45.730
that.

11
00:00:45.750 --> 00:00:50.160
Now keep in mind tensor board is technically a separate library from tensor flow.

12
00:00:50.160 --> 00:00:52.740
So he aren't using our environment file.

13
00:00:52.740 --> 00:00:56.000
You may need to pip install or condense stored tensor board.

14
00:00:56.010 --> 00:01:02.820
In addition the tensor flow if you're using the Google collab notebook online the Google official guide

15
00:01:02.880 --> 00:01:07.440
actually has a premade notebook that you just run all the cells and it automatically uploads tensor

16
00:01:07.440 --> 00:01:08.130
board for you.

17
00:01:08.520 --> 00:01:13.410
I have the URL here but we're also going to just show you how you can find that and we link to it in

18
00:01:13.440 --> 00:01:18.930
our notebook or you can just google search tensor board in notebooks and I'll have that link as well.

19
00:01:18.930 --> 00:01:23.700
OK let's get started by heading over to our folder from our zip file.

20
00:01:23.730 --> 00:01:24.130
OK.

21
00:01:24.180 --> 00:01:27.010
Here I am underneath the aliens folder.

22
00:01:27.060 --> 00:01:32.600
Go ahead and open up the tensor board notebook that we have here for you at 0 5 dash tensor board.

23
00:01:32.760 --> 00:01:36.930
What we're going to be doing is we're going to be creating an intensive board running off the network.

24
00:01:36.930 --> 00:01:40.260
We originally created in our Caris classification lecture.

25
00:01:40.260 --> 00:01:43.900
So when you open up that tensor board notebook it should look something like this.

26
00:01:44.100 --> 00:01:49.290
And there's a link here for the full official tutorial that discusses a lot of cool aspects of tensor

27
00:01:49.290 --> 00:01:49.900
board.

28
00:01:49.920 --> 00:01:53.960
We're really just going to be going over the basics in this particular lecture.

29
00:01:54.030 --> 00:01:58.830
But if you click on that link it has this getting started link and what's really cool about this is

30
00:01:58.830 --> 00:02:03.780
if you're using Google collab all you need to do is you can essentially almost skip this lecture just

31
00:02:03.810 --> 00:02:08.610
open up this link that says running Google collab and it has this getting started with tensor board

32
00:02:08.610 --> 00:02:14.060
notebook that you can essentially run all the cells and it has everything done automatically for you.

33
00:02:14.070 --> 00:02:17.210
Keep in mind it's going to be using a slightly different model here.

34
00:02:17.250 --> 00:02:21.480
It's doing an M honest convolution on your own network that we'll see later on in the course.

35
00:02:21.480 --> 00:02:26.360
But the same ideas are the same you'll create a log directory create some callbacks and then fit it.

36
00:02:26.430 --> 00:02:30.060
And we're gonna be going over this as well in our lecture right now.

37
00:02:30.060 --> 00:02:35.010
So you can go ahead and watch our lecture and then just click on this link and then run this notebook

38
00:02:35.060 --> 00:02:40.500
and you should be able to actually see tensor board inside of your notebook so let's come back here

39
00:02:40.830 --> 00:02:42.080
and we'll show you how to run this.

40
00:02:42.090 --> 00:02:43.220
If you're running it locally.

41
00:02:43.350 --> 00:02:48.450
Again if you're uncle collab pretty much just click on that link and start running the cells but we're

42
00:02:48.510 --> 00:02:53.880
gonna import pansies and PDA num PI read in the data file that we work to that cancer classification

43
00:02:54.330 --> 00:02:59.670
we'll go ahead and grab those values perform the train test split and then scale the data.

44
00:02:59.910 --> 00:03:01.070
So far nothing different.

45
00:03:01.680 --> 00:03:04.260
And then we'll also create the model.

46
00:03:04.260 --> 00:03:05.750
So we'll create the model.

47
00:03:05.820 --> 00:03:12.210
I'm also going to add in the early stopping but one notice or one thing to notice here is that we're

48
00:03:12.510 --> 00:03:18.410
importing an additional callback so from carriers that callbacks you will also be importing tensor board.

49
00:03:18.450 --> 00:03:23.910
So even if you don't import early stopping for your particular model in order to actually compile and

50
00:03:23.910 --> 00:03:29.400
have tensor board keep track of those logs we're going to from Carousel callbacks import tensor board

51
00:03:30.110 --> 00:03:35.670
we'll go ahead and create the early stopping callback just as we did last time and then we're going

52
00:03:35.670 --> 00:03:38.580
to make sure we understand where this notebook is located.

53
00:03:38.580 --> 00:03:39.830
By running P.W..

54
00:03:40.050 --> 00:03:45.330
So in my case I have now the file path of where this notebook is actually located.

55
00:03:45.330 --> 00:03:45.570
All right.

56
00:03:46.170 --> 00:03:51.290
So now it's time to actually get to probably the most important part which is creating this tensor board

57
00:03:51.330 --> 00:03:52.230
callback.

58
00:03:52.230 --> 00:03:55.100
So again tensor board it's a visualization tool.

59
00:03:55.200 --> 00:04:00.700
And what's going to happen is this callback that we're going to create with this tensor board import.

60
00:04:00.870 --> 00:04:02.500
It's going to log a bunch of things.

61
00:04:02.580 --> 00:04:05.700
And it's up to you what it logs and what it doesn't log.

62
00:04:05.700 --> 00:04:11.580
So it can log things like a training graph visualization activation histogram is sampled profiling metric

63
00:04:11.580 --> 00:04:13.760
summary plots et cetera.

64
00:04:13.830 --> 00:04:20.010
So there are various arguments the pass when creating a tense report a callback variable to indicate

65
00:04:20.040 --> 00:04:22.370
whether or not you want to record something.

66
00:04:22.380 --> 00:04:24.490
So the most important one is right here.

67
00:04:24.570 --> 00:04:29.580
Log directory which is essentially the file path to the logs and we'll show you how to fill that in

68
00:04:29.660 --> 00:04:31.410
in just a second.

69
00:04:31.430 --> 00:04:35.680
The other ones are just whether or not you want to keep recording something.

70
00:04:35.720 --> 00:04:42.740
So the histogram frequency is the frequency in epochs at which to compute activation and weight histogram

71
00:04:42.800 --> 00:04:44.390
for the layers of the model.

72
00:04:44.390 --> 00:04:48.470
So what we're going to do is we're going to set this to 1.

73
00:04:48.560 --> 00:04:55.700
So that basically after every epoch of training we'll go ahead and calculate all the weights for our

74
00:04:55.700 --> 00:04:58.300
layers and then we'll create a histogram of it.

75
00:04:58.520 --> 00:05:02.820
And then the next epoch will do another histogram so we'll actually be able to see these histogram is

76
00:05:03.560 --> 00:05:08.840
essentially stacked on top of each other and we'll get this kind of cool 3d visualization that shows

77
00:05:08.840 --> 00:05:13.400
how the weights are essentially changing throughout the epochs of training.

78
00:05:13.400 --> 00:05:19.400
And then if you want you can also visualize the graph itself so we can set right graph and we can set

79
00:05:19.400 --> 00:05:21.080
that to true.

80
00:05:21.080 --> 00:05:23.660
We can also decide whether or not we want to write images.

81
00:05:23.660 --> 00:05:29.480
So whether to write model weights to visualize as an image and tensor board and then the update frequency

82
00:05:29.570 --> 00:05:31.850
is also something you can choose.

83
00:05:31.850 --> 00:05:36.560
Typically I would recommend updating per epoch it's usually easier to interpret that way.

84
00:05:36.770 --> 00:05:43.300
And then finally we can also specify a profile batch so we can profile the batch to sample compute characteristics.

85
00:05:43.730 --> 00:05:46.310
By default we can profile by the second batch.

86
00:05:46.310 --> 00:05:51.770
So we'll keep it on that default and then embedding is frequency is the frequency in epochs which embedding

87
00:05:51.770 --> 00:05:54.400
layers will be visualized if you set that to zero.

88
00:05:54.410 --> 00:05:56.560
You won't bother with visualizing those embedding.

89
00:05:57.020 --> 00:05:59.180
So lots of different things there.

90
00:05:59.180 --> 00:06:04.820
The other thing to note here is if you're running this model multiple times with different parameters

91
00:06:04.940 --> 00:06:09.910
and you want to make sure you have different directories for each time you run this model.

92
00:06:09.980 --> 00:06:14.290
What I recommend doing and this is also the exact same thing they setup in the official tutorial.

93
00:06:14.450 --> 00:06:17.840
You'll notice as you scroll down they'll set it up with a date timestamp.

94
00:06:17.840 --> 00:06:20.750
So right here they call it date time date time.

95
00:06:20.870 --> 00:06:25.640
Essentially what that does is if you say from date time import date time and then you say let me just

96
00:06:25.760 --> 00:06:31.040
zoom in here so you can see this clearly daytime thought now and then structure the time with this particular

97
00:06:31.040 --> 00:06:37.070
command you'll get this string which indicates today's date and the hour and what you can do is have

98
00:06:37.070 --> 00:06:42.230
that be the name your folder that way as long as you wait one minute before running your model again.

99
00:06:42.230 --> 00:06:47.180
You'll have a unique log for each time you run the model that we can go back at at the number of neurons

100
00:06:47.180 --> 00:06:52.490
in a layer et cetera and visualize each of those models with tensor board now.

101
00:06:52.520 --> 00:06:58.770
In our case we're only going to run this one time so we'll go ahead just set our log directory to logs.

102
00:06:58.910 --> 00:07:04.430
And since I'm on windows I'll say backslash backslash fit if you're a MacOS or Linux you may need to

103
00:07:04.430 --> 00:07:07.310
do just one backslash or even a forward slash.

104
00:07:07.310 --> 00:07:10.750
What I recommend doing is if you come back up here.

105
00:07:11.030 --> 00:07:14.540
Notice after you type TWD it actually reports back to you.

106
00:07:14.540 --> 00:07:16.100
The syntax you should be using.

107
00:07:16.160 --> 00:07:18.890
So I should be using double back slashes.

108
00:07:18.890 --> 00:07:21.410
So that's what I'm going to do down here.

109
00:07:21.410 --> 00:07:22.800
So you're logged directory.

110
00:07:22.970 --> 00:07:26.030
The base of it should always be logs.

111
00:07:26.030 --> 00:07:28.640
And then the next sub directory should be fit.

112
00:07:28.640 --> 00:07:34.310
So after I run this inside of my current location this ancient folder I will end up seeing a folder

113
00:07:34.310 --> 00:07:38.800
called logs be created and then a folder inside that called it.

114
00:07:38.900 --> 00:07:45.080
So we must say logs fit and then optionally you can add a timestamp for unique folder.

115
00:07:45.080 --> 00:07:48.660
So this is the code that automatically does that for you.

116
00:07:48.780 --> 00:07:54.190
OK so right now we have this variable log directory which is essentially saying where am I going to

117
00:07:54.190 --> 00:07:56.080
say these logs during training.

118
00:07:56.290 --> 00:07:58.740
And by default it should be logs.

119
00:07:59.410 --> 00:08:05.500
So I pass that in as my dirt log directory I'll say histogram frequency is one and then we'll go ahead

120
00:08:05.530 --> 00:08:06.250
say right graph.

121
00:08:06.310 --> 00:08:06.820
True.

122
00:08:06.820 --> 00:08:07.050
Right.

123
00:08:07.060 --> 00:08:08.300
Image is true.

124
00:08:08.350 --> 00:08:10.310
Update the frequencies every epoch.

125
00:08:10.390 --> 00:08:15.280
We'll keep profile back to two because that's the default and then embedding is frequency we'll keep

126
00:08:15.280 --> 00:08:18.000
that at one and you can play around with these options as well.

127
00:08:18.100 --> 00:08:20.990
Based off what you read here and what you actually want to record.

128
00:08:21.160 --> 00:08:24.510
So we'll go ahead and create that call back.

129
00:08:24.540 --> 00:08:27.710
And now just as before we'll go ahead and create the same model.

130
00:08:27.780 --> 00:08:30.840
You actually don't need to edit anything in your model.

131
00:08:30.840 --> 00:08:36.210
So we'll just run that model just as we did before where you actually add this in is underneath our

132
00:08:36.210 --> 00:08:40.730
callbacks list instead of just passing an early stop as we did before.

133
00:08:40.740 --> 00:08:47.470
We will also pass in this variable board which recall is this tensor board callback that we just created.

134
00:08:47.520 --> 00:08:53.000
So after you create the model go ahead and fit the model.

135
00:08:53.330 --> 00:08:55.150
OK so this is going to be training.

136
00:08:55.220 --> 00:08:59.660
And because I have early stopping here it shouldn't be training for the full six hundred epochs.

137
00:08:59.660 --> 00:09:03.440
I think last time it stopped at a little over one hundred and twenty.

138
00:09:03.590 --> 00:09:09.200
So we'll just go ahead and let this run for a little bit and eventually it should stop training once

139
00:09:09.200 --> 00:09:11.370
it reaches somewhere above that.

140
00:09:11.450 --> 00:09:11.700
OK.

141
00:09:11.720 --> 00:09:13.730
So here's 126 early stopping.

142
00:09:13.730 --> 00:09:14.270
Perfect.

143
00:09:14.780 --> 00:09:15.510
OK.

144
00:09:15.680 --> 00:09:17.810
So now we're going to run tensor board.

145
00:09:18.440 --> 00:09:22.460
So instead of running our own visualizations let's see what tensor board has to offer.

146
00:09:22.460 --> 00:09:27.950
The first thing to note here is that tensor board is going to be running locally in your browser local

147
00:09:27.950 --> 00:09:30.770
host 6 0 0 6.

148
00:09:30.770 --> 00:09:35.000
Go ahead and click that and open it in a new tab.

149
00:09:35.000 --> 00:09:40.100
Now because we haven't actually run tensor board yet when you run that it should just be empty you'll

150
00:09:40.130 --> 00:09:45.500
say something like this site can't be reached because tensor boards not running on that yet to actually

151
00:09:45.500 --> 00:09:50.070
run tensor board or we're going to do is we're going to run it through the command line.

152
00:09:50.150 --> 00:09:54.530
The main thing to keep in mind is what was your log directory recall.

153
00:09:54.560 --> 00:09:59.480
That's the variable we made that essentially indicates where we're actually saving our logs.

154
00:09:59.570 --> 00:10:06.170
Recall that we say the under logs that in fact if you come back to the aliens folder you'll notice we

155
00:10:06.170 --> 00:10:12.060
now have this logs folder and underneath that there's a fit of directory and underneath that there's

156
00:10:12.080 --> 00:10:15.540
train validation and the special file for tensor board.

157
00:10:15.740 --> 00:10:21.800
And these are essentially raw files that tensor board is going to use to display visualizations.

158
00:10:21.950 --> 00:10:25.910
And there's more files depending on how much you're actually recording.

159
00:10:25.910 --> 00:10:26.520
OK.

160
00:10:26.630 --> 00:10:31.850
So the main thing keep in mind is what was your log directory in our case we kept it simple just logs

161
00:10:31.940 --> 00:10:36.400
fit but you may have a longer timestamp associate of that as well.

162
00:10:36.500 --> 00:10:41.110
And then the next thing to recall is where you're actually located on your computer.

163
00:10:41.120 --> 00:10:43.550
So this is where I'm located on my computer.

164
00:10:43.580 --> 00:10:46.550
So we need to do is go to our command line.

165
00:10:46.550 --> 00:10:53.300
That's either the anaconda prompt or the command prompt for Windows users or for Mac OS or Ubuntu or

166
00:10:53.300 --> 00:10:54.560
Linux users.

167
00:10:54.560 --> 00:11:02.930
That will be your terminal or the anaconda prompt if you install that as a Mac OS or Linux user.

168
00:11:02.950 --> 00:11:05.480
Let's go ahead and open up our command line.

169
00:11:05.650 --> 00:11:11.500
So main things keep in mind is whereas this file currently located and what the name of our log directory

170
00:11:11.500 --> 00:11:18.610
was OK so I will open up the anaconda prompt and now I have my anaconda prompt open.

171
00:11:18.880 --> 00:11:25.510
The next step is to make sure that my course file or course environment is activated.

172
00:11:25.510 --> 00:11:33.250
So I will say Conda activate and then whatever the course environment happens to be called in my case

173
00:11:33.340 --> 00:11:36.880
I use a slightly different environment while I'm filming this lectures.

174
00:11:36.880 --> 00:11:42.530
So I will say kind of activate TMF To GP you however your course environment will be the one you set

175
00:11:42.530 --> 00:11:46.810
up during the installation lecture you should have already been activating your environment as you've

176
00:11:46.810 --> 00:11:47.700
gone through these lectures.

177
00:11:47.710 --> 00:11:49.480
So there shouldn't be new to you.

178
00:11:49.510 --> 00:11:50.160
OK.

179
00:11:50.320 --> 00:11:56.320
So you've activated the environment and now we need to do is we need to change directory or see into

180
00:11:56.410 --> 00:12:05.230
this particular folder that was typed out during PDL BD so I will see these into these period data courses.

181
00:12:05.260 --> 00:12:06.850
That's where I happen to be located.

182
00:12:06.850 --> 00:12:09.030
Your files may be different on your computer.

183
00:12:09.250 --> 00:12:14.560
And I would highly recommend that you just tab autocomplete this because if you spell anything wrong

184
00:12:14.860 --> 00:12:19.650
it won't work then type C the again and the next folder.

185
00:12:19.840 --> 00:12:22.160
My case is tensor float to boot camp.

186
00:12:22.180 --> 00:12:25.650
Again just use tab autocomplete to make sure spelling is right.

187
00:12:25.720 --> 00:12:31.260
And then finally see the into 0 3 and ends and there we have it.

188
00:12:31.390 --> 00:12:32.160
Okay.

189
00:12:32.370 --> 00:12:34.320
So now it's time for the command.

190
00:12:34.620 --> 00:12:40.800
So the command we use is make sure you've activated your environment you should be able to type tensor

191
00:12:40.800 --> 00:12:51.320
board space dash dash log D I R space and then your log directory.

192
00:12:51.410 --> 00:12:59.120
So in my case my log directory I printed it out here was logs backslash fit so that I'll be logs backslash

193
00:12:59.570 --> 00:13:05.770
fit and if you want to see it on one line I can stretch this out so here it is.

194
00:13:05.790 --> 00:13:08.290
So the steps was to activate my environment.

195
00:13:08.310 --> 00:13:14.220
Yours may be called different depending on your install lecture then S.D. into that folder where I happened

196
00:13:14.220 --> 00:13:21.460
to say those logs or that notebook was running and then run tensor board space dash dash log the IRR

197
00:13:21.540 --> 00:13:27.000
space and then the file path that was printed out when you printed out log directory appear in the notebook

198
00:13:27.570 --> 00:13:31.140
then go ahead and hit enter upon hitting enter.

199
00:13:31.140 --> 00:13:36.570
You should see some commands pop up and eventually it's going to happen is it's going to tell you hey

200
00:13:36.690 --> 00:13:43.610
tensor boards running at local host 6 0 0 6 recall that's the URL we opened previously.

201
00:13:43.740 --> 00:13:51.040
Whatever you do do not hit control plus C so don't do control policy to copy this local host otherwise

202
00:13:51.040 --> 00:13:56.220
that actually quits this so only use control policy when you're ready to quit or we're going to do is

203
00:13:56.220 --> 00:14:02.250
we're actually done with the anaconda prompt for now so we're going to come to our book in the browser

204
00:14:02.790 --> 00:14:10.830
recall that we already open local host 6 0 0 6 go ahead and open that tab again and if it says this

205
00:14:10.830 --> 00:14:16.590
site can't be reached you saw fit for an instance there just refresh and you should see a pop up and

206
00:14:16.590 --> 00:14:17.770
just in case it doesn't.

207
00:14:17.820 --> 00:14:20.560
You can always come here and then select scholars.

208
00:14:20.580 --> 00:14:24.870
However if you did everything like I did it should have automatically gone to scholars for you as you

209
00:14:24.870 --> 00:14:26.190
can see here in the euro.

210
00:14:26.760 --> 00:14:28.640
OK so let's explore some of these.

211
00:14:28.650 --> 00:14:34.210
I will zoom out just a little bit so we're not super zoomed into this tensor board and to start off

212
00:14:34.240 --> 00:14:39.970
we can see our loss both on the training set and our validation set and we get this really nice interactive

213
00:14:39.970 --> 00:14:45.370
plot and we can make it very smooth if we want just to get the general trends this pebble a little too

214
00:14:45.370 --> 00:14:50.600
smooth or we can turn something completely off to get the raw data as we've done before.

215
00:14:50.620 --> 00:14:55.840
So this is essentially an interactive version of the plots we were making before with pandas and we

216
00:14:55.840 --> 00:14:59.930
can start playing around by toggling the y axis to be logarithmic.

217
00:14:59.960 --> 00:15:01.270
We can turn that off and on.

218
00:15:01.360 --> 00:15:03.400
We can also fit the domain to the data.

219
00:15:03.430 --> 00:15:05.020
It actually does that automatically.

220
00:15:05.200 --> 00:15:08.110
And then you can also do things like make it larger and zoom in.

221
00:15:08.110 --> 00:15:11.260
So lots of different things could play around with if you only want to see validation.

222
00:15:11.290 --> 00:15:14.760
You can do a checkbox here to see it et cetera.

223
00:15:14.770 --> 00:15:17.000
So this is what we see under scholars.

224
00:15:17.020 --> 00:15:21.370
You can explore that then we have images in our case.

225
00:15:21.370 --> 00:15:28.930
We did actually decide to record these images and this is essentially showing you the images waits.

226
00:15:28.960 --> 00:15:34.120
And as far as interpreting this for our particular set since we're not dealing with image data it's

227
00:15:34.120 --> 00:15:36.040
not very clear how to interpret this.

228
00:15:36.040 --> 00:15:43.250
So essentially the weights as they get kind of darker or lighter depending on what actual air on.

229
00:15:43.280 --> 00:15:49.420
So we can see here the mapping but overall interpreting this isn't gonna help us that much because we're

230
00:15:49.420 --> 00:15:54.670
not actually dealing of image data and you can also play around with the contrast etc. as well as the

231
00:15:54.670 --> 00:15:55.520
brightness.

232
00:15:55.720 --> 00:15:57.500
Then there's the graph itself.

233
00:15:57.580 --> 00:15:59.230
So this is the graph we created.

234
00:15:59.230 --> 00:16:00.270
They can play around with.

235
00:16:00.280 --> 00:16:01.060
You can explore it.

236
00:16:01.210 --> 00:16:04.470
There's a legend here showing you what each of these notes are doing.

237
00:16:04.480 --> 00:16:06.900
But here's the sequential model that we created.

238
00:16:06.970 --> 00:16:07.920
We can expand on that.

239
00:16:07.930 --> 00:16:11.290
We can see here what we created et cetera.

240
00:16:11.320 --> 00:16:17.500
Then there's the distributions the range of weights throughout these different layers.

241
00:16:17.530 --> 00:16:19.060
So that's one that's two.

242
00:16:19.060 --> 00:16:20.500
And then the final one.

243
00:16:20.530 --> 00:16:25.360
Keep in mind as far as interpreting these directly it won't really help you that much for this particular

244
00:16:25.360 --> 00:16:26.210
dataset.

245
00:16:26.230 --> 00:16:29.740
This becomes a lot more helpful when you're dealing with things like convolution networks.

246
00:16:29.740 --> 00:16:35.980
We can see the histogram is again as far as interpreting this directly maybe not super useful but some

247
00:16:35.990 --> 00:16:40.150
of you could tell here is how much things are changing throughout time.

248
00:16:40.150 --> 00:16:45.910
So for example if we just take a look at this first dense one this is the bias right now.

249
00:16:46.300 --> 00:16:49.990
So these are the biased values per epoch throughout time.

250
00:16:49.990 --> 00:16:54.730
So you can see in the very beginning they're kind of changing wildly but as you start to get towards

251
00:16:54.730 --> 00:16:57.560
the stopping epoch notice you're not changing that much.

252
00:16:57.560 --> 00:17:02.170
These history grams of these biased values is not changing which makes sense which we should expect

253
00:17:02.170 --> 00:17:07.180
that as we get closer and closer to the early stopping point things are changing less and less and it's

254
00:17:07.180 --> 00:17:10.030
the same for the actual weight histogram as well.

255
00:17:10.030 --> 00:17:11.210
So these are histogram.

256
00:17:11.370 --> 00:17:14.420
In this case of bias and the weight for that first dense layer.

257
00:17:14.530 --> 00:17:19.780
So you can see they start changing rapidly in the beginning but then they start settling down towards

258
00:17:19.810 --> 00:17:21.620
whatever their final weights going to be.

259
00:17:21.740 --> 00:17:24.340
And we can view that for the layers.

260
00:17:24.400 --> 00:17:25.130
OK.

261
00:17:25.270 --> 00:17:30.290
And then depending on either side the record you can view things like profile and projector.

262
00:17:30.400 --> 00:17:32.950
These are going to be super useful for us right now.

263
00:17:32.950 --> 00:17:37.480
So we'll go ahead and skip these and we'll come back to tensor board when we're dealing with convolution

264
00:17:37.480 --> 00:17:41.350
on your networks because then it's going to be really helpful in exploring the network.

265
00:17:41.350 --> 00:17:46.030
But right now we can see that we've actually already created a lot of these images ourselves specifically

266
00:17:46.030 --> 00:17:46.910
the epoch loss.

267
00:17:46.930 --> 00:17:50.620
That's the one that really you're going to use a lot and we already know how to create this on our own

268
00:17:51.640 --> 00:17:56.260
these Instagram's while they're kind of interesting to look at for right now there's not much you can

269
00:17:56.260 --> 00:18:02.980
do as far as editing the hyper parameters on this artificial neural network just based off these diagrams

270
00:18:03.580 --> 00:18:08.110
as you get more advanced you can interpret these a little better but for right now the main idea here

271
00:18:08.170 --> 00:18:12.520
is that you know how to create tensor board and we're going to come back to this and explore these in

272
00:18:12.520 --> 00:18:18.400
more detail as we learn about more complex models we're interpreting these graphs can really help us.

273
00:18:18.400 --> 00:18:21.670
OK thanks everyone and I'll see you at the next section of the course.
