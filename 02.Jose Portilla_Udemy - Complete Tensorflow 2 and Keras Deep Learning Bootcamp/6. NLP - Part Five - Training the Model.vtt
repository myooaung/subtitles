WEBVTT
1
00:00:05.360 --> 00:00:11.200
Welcome back everyone to Part Five of this text Generation series of lectures and partners.

2
00:00:11.280 --> 00:00:14.670
Number five we're gonna be showing you and talking about training the model.

3
00:00:14.670 --> 00:00:18.960
We're gonna quickly show you an example of how to train the model but more importantly since this model

4
00:00:18.960 --> 00:00:23.940
has over three million parameters and it essentially requires a GP to train in a reasonable amount of

5
00:00:23.940 --> 00:00:24.580
time.

6
00:00:24.660 --> 00:00:27.870
We'll show you how to load our provided saved model file.

7
00:00:27.910 --> 00:00:32.220
We're also going to do is just to make sure everything is running correctly that will run the model

8
00:00:32.220 --> 00:00:33.470
without it being train.

9
00:00:33.510 --> 00:00:36.090
And we should expect just to see random characters show up.

10
00:00:36.450 --> 00:00:38.880
Let's head to notebook and see this all in action.

11
00:00:38.880 --> 00:00:39.140
All right.

12
00:00:39.150 --> 00:00:43.950
So here I am at my notebook and the main thing to notice right now is when we call our models summary

13
00:00:44.100 --> 00:00:47.990
in the previous set of lectures we're dealing with over three million parameters.

14
00:00:48.090 --> 00:00:53.130
And because this is a recurring neural network with over three million parameters we really need a GP

15
00:00:53.160 --> 00:00:54.240
to train this.

16
00:00:54.240 --> 00:00:59.220
So in case you don't have a GP you locally especially since you require an in video card and a correct

17
00:00:59.310 --> 00:01:04.730
installation of CUDA what I'd highly recommend is uploading this notebook to Google collab.

18
00:01:04.800 --> 00:01:07.980
So I'm filming this right now on a laptop that doesn't actually have a GP.

19
00:01:08.370 --> 00:01:13.020
So I'm going to come to Google collab and quickly show you how you could upload this notebook along

20
00:01:13.080 --> 00:01:14.540
the text data set.

21
00:01:14.550 --> 00:01:20.150
So just google search Google collab click on collab that research that Google dot com opened it up and

22
00:01:20.160 --> 00:01:21.470
it will come off a new notebook.

23
00:01:21.540 --> 00:01:28.500
And then what we can do here is we can say file upload a notebook and you may need to sign into Google.

24
00:01:28.500 --> 00:01:28.940
That's OK.

25
00:01:28.950 --> 00:01:35.750
Go ahead and just sign in if your Google account and then once you're signed then you should be able

26
00:01:35.750 --> 00:01:40.430
to then say file upload notebook and then choose a file.

27
00:01:40.430 --> 00:01:43.430
I'm going to choose the Untitled notebook that we've been working with.

28
00:01:43.460 --> 00:01:50.240
So go ahead and click choose file and then underneath right here in LP and text data I have my untitled

29
00:01:50.240 --> 00:01:50.810
notebook.

30
00:01:50.810 --> 00:01:54.580
If you want you can also upload our actual notebook that we provide for you.

31
00:01:54.650 --> 00:01:56.870
And continue with the lecture we've been leaving off.

32
00:01:56.870 --> 00:01:58.490
So I have this entitled one.

33
00:01:58.490 --> 00:02:03.370
I'll go ahead and open it up and upload it a fast forward so let's done OK.

34
00:02:03.380 --> 00:02:05.680
And we can see the entire notebook being uploaded.

35
00:02:05.870 --> 00:02:09.130
And here we can see everything that we've been doing so far.

36
00:02:09.230 --> 00:02:14.750
The last thing I need to upload is the actual text data set since Shakespeare that text isn't actually

37
00:02:14.750 --> 00:02:16.970
part of Google collab.

38
00:02:16.970 --> 00:02:23.820
So we're gonna expand on this right here and then click on files and in order to upload files and you

39
00:02:23.840 --> 00:02:25.120
connect to a runtime.

40
00:02:25.190 --> 00:02:27.150
So in case it hasn't already.

41
00:02:27.200 --> 00:02:28.030
Look at the top right.

42
00:02:28.040 --> 00:02:30.020
And make sure it's connected to a runtime.

43
00:02:30.020 --> 00:02:37.130
And then once that's done you can hit upload here and then upload this Shakespeare text file.

44
00:02:37.130 --> 00:02:38.610
So it's just called Shakespeare.

45
00:02:38.630 --> 00:02:43.700
Do not upload the start H5 that's actually the model file that we're using.

46
00:02:43.700 --> 00:02:46.990
If you want you can upload it as well in case you want to load the model.

47
00:02:47.000 --> 00:02:50.890
But if you're strictly just going to train Shakespeare is the actual text file.

48
00:02:50.910 --> 00:02:53.130
Shakespeare Gen H5.

49
00:02:53.210 --> 00:02:54.150
That's the model file.

50
00:02:54.170 --> 00:02:57.110
I'll go ahead with both of them since we will use both of them eventually.

51
00:02:57.140 --> 00:03:02.000
So go and open that and then I'll remind you that they'll get deleted once this run times over.

52
00:03:02.000 --> 00:03:03.170
That's OK.

53
00:03:03.170 --> 00:03:07.490
And then we can see here they're uploading going to go ahead and fast forward time until that's all

54
00:03:07.520 --> 00:03:08.840
done uploading.

55
00:03:08.840 --> 00:03:09.140
All right.

56
00:03:09.140 --> 00:03:15.620
Now that those files are uploaded have the text as well as the trained model which we're going to load

57
00:03:15.620 --> 00:03:16.910
up later on.

58
00:03:16.910 --> 00:03:22.940
We'll go ahead and close this and what we'll do is to make sure we're using a tensor flow to later on

59
00:03:22.940 --> 00:03:24.140
this is going to be the default.

60
00:03:24.140 --> 00:03:30.540
But let me zoom in here so you can see these commands clearly what we're going to do here is we're going

61
00:03:30.540 --> 00:03:40.150
to say per cent sine tensor flow version space two point X and then once we do that we're just going

62
00:03:40.150 --> 00:03:43.260
to hit runtime run all.

63
00:03:43.700 --> 00:03:51.230
So that will set tensor flow as version 2.0 and then once that's ready to go it'll read in the file

64
00:03:51.380 --> 00:03:54.610
and do all the encoding and decoding commands that we did before.

65
00:03:54.620 --> 00:03:57.560
So we should be able to then scroll down all the way to the bottom of this notebook.

66
00:03:57.560 --> 00:04:03.080
If you've been following along and you'll see the model summary pop up so again we're dealing with more

67
00:04:03.080 --> 00:04:05.930
than three million parameters which definitely means we need GPA.

68
00:04:06.350 --> 00:04:12.140
So make sure you confirm that you're running on GPA as shown in the original setup lecture for Google

69
00:04:12.140 --> 00:04:13.530
collab.

70
00:04:13.530 --> 00:04:18.170
But we're gonna do now is we'll go ahead and make sure everything is okay before we actually train our

71
00:04:18.170 --> 00:04:22.350
model by saying uh by recklessly running an input batch.

72
00:04:22.380 --> 00:04:37.450
So we'll say for input underscore example underscore batch comma target example batch in data set and

73
00:04:37.450 --> 00:04:42.880
then we're going to take one from this dataset and I'm going to zoom out just one level.

74
00:04:42.880 --> 00:04:50.240
So these icons don't take up too much so we're gonna do is we're gonna run through an example batch

75
00:04:50.330 --> 00:04:53.930
just one simple batch and then we're gonna pass it into our model.

76
00:04:53.930 --> 00:04:59.110
So that means the example batch predictions is going to be equal to the model.

77
00:04:59.180 --> 00:05:07.460
And so we're going to input the example batch so the input example batch recall that the input example

78
00:05:07.460 --> 00:05:13.580
batch that is the original sequence the target example batch is that sequence shifted one character

79
00:05:13.580 --> 00:05:14.360
forward.

80
00:05:14.390 --> 00:05:19.970
So right now if I even training the model I'm passing in an example input batch seeing what the model

81
00:05:19.970 --> 00:05:25.100
predicts if the initialization are truly random then it should just be a bunch of random characters

82
00:05:25.100 --> 00:05:28.890
that it predicts should be the sequence one timestamp to the future.

83
00:05:29.000 --> 00:05:35.230
So we go ahead and run that and then once that is finished running what I'm going to do is I'm going

84
00:05:35.230 --> 00:05:40.120
to check with those example batch predictions actually look like we'll say example batch predictions

85
00:05:40.180 --> 00:05:46.510
let's first check the shape of this so the shape is 120 by 128 by eighty four.

86
00:05:46.670 --> 00:05:48.770
And this is the batch size.

87
00:05:48.800 --> 00:05:52.880
So 128 is this batch size that we chose earlier.

88
00:05:52.880 --> 00:05:59.600
So the 120 is the sequence length of the batch 120 characters and then eighty four is the vocab size.

89
00:06:00.170 --> 00:06:07.610
So then let's go ahead and grab the very first of these predictions by saying index zero and you'll

90
00:06:07.610 --> 00:06:10.850
notice it's essentially a bunch of probability.

91
00:06:11.300 --> 00:06:16.680
So these are the probabilities that it assumes for each concurrent character.

92
00:06:16.700 --> 00:06:26.730
So what we can do here is the following We can say using some tensor flow commands TFR random the categorical

93
00:06:28.140 --> 00:06:34.010
and it takes in those logics which essentially just a term for the log odds or logarithmic probability

94
00:06:34.010 --> 00:06:42.690
here and then we're going to say just a number of samples to Gram is one so we'll just do that because

95
00:06:42.690 --> 00:06:53.080
I just want a single sample so say sampled indices is going to grab from that probability distribution

96
00:06:53.890 --> 00:07:03.150
and then we'll say we make a couple more cells here underneath here will say sampled indices and if

97
00:07:03.150 --> 00:07:08.280
we check out what that looks like it's essentially the actual indices of the characters it thinks it

98
00:07:08.280 --> 00:07:14.820
should be predicting and in order to pass in this sort of shaped array into our index to character sequence

99
00:07:14.880 --> 00:07:23.050
we're going to do here is the following we'll say sample indices is then equal to TFT squeeze

100
00:07:25.910 --> 00:07:36.500
and then we'll pass in sample indices access is equal to negative 1 num pi net essentially reshapes

101
00:07:36.500 --> 00:07:37.410
it.

102
00:07:37.670 --> 00:07:43.670
So now it's going to look like this which is exactly the format we wanted in.

103
00:07:43.740 --> 00:07:52.010
So I can pass it into this index to character array so go ahead pass it to the index the character a

104
00:07:55.000 --> 00:07:58.100
and running that gets you a bunch of random characters.

105
00:07:58.120 --> 00:08:02.890
So this is essentially its prediction and is just a bunch of random characters which makes sense because

106
00:08:02.890 --> 00:08:05.300
the model hasn't been trained at all.

107
00:08:05.320 --> 00:08:11.140
So if you wanted to train the model what you would do is simply say the following you would choose some

108
00:08:11.140 --> 00:08:16.570
number of epochs and I would highly recommend that at least 30 epochs to get realistic results on this

109
00:08:16.570 --> 00:08:17.840
particular model.

110
00:08:17.980 --> 00:08:24.200
So you you'd specify some number of epochs such as 30 and then simply say model that fit to the data

111
00:08:24.200 --> 00:08:31.270
set object we made earlier where epochs is equal to the number of epochs specified run that and then

112
00:08:31.330 --> 00:08:33.190
it will begin fitting your model.

113
00:08:33.190 --> 00:08:38.860
So in our case I'm going to go ahead and interrupt this training because this might take a really long

114
00:08:38.860 --> 00:08:42.070
time to run in my experience.

115
00:08:42.070 --> 00:08:45.600
It took about somewhere between 1 to 2 minutes per epoch.

116
00:08:45.790 --> 00:08:50.650
So it's running a little bit faster right now but it will take some time especially for such a large

117
00:08:50.650 --> 00:08:51.450
model.

118
00:08:51.460 --> 00:08:59.650
So what I'm going to do here is simply say run time interrupt execution so actually interrupt the training

119
00:08:59.650 --> 00:09:04.910
of this and then you'll see that keyboard interrupt pop up.

120
00:09:04.910 --> 00:09:06.220
That's OK.

121
00:09:06.530 --> 00:09:09.400
If you want you can kind of sit around wait for it to train.

122
00:09:09.440 --> 00:09:13.970
If you're really interested in that though we're gonna do is we're actually going to load up the model

123
00:09:14.030 --> 00:09:24.540
so we can do that by saying from tensor flow dark hair stop models import load model and in order to

124
00:09:24.540 --> 00:09:29.760
load the model make sure you upload it already this Shakespeare Gen the H5 file.

125
00:09:29.770 --> 00:09:31.770
So that's our model.

126
00:09:31.770 --> 00:09:36.840
So in part 6 We're going to do is we'll finish off this lecture by actually loading up the model we're

127
00:09:36.840 --> 00:09:40.920
going to do it in a specific way just to load the model weights first and then build it to a different

128
00:09:41.070 --> 00:09:41.810
batch shape.

129
00:09:42.060 --> 00:09:46.410
So we'll show that all in the very next lecture so coming up next we'll show you how to load the model

130
00:09:46.410 --> 00:09:47.730
in a very specific manner.

131
00:09:47.730 --> 00:09:48.200
I'll see you there.
