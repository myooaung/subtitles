WEBVTT
1
00:00:05.320 --> 00:00:06.480
Welcome back everyone.

2
00:00:06.490 --> 00:00:11.290
In this lecture we're going to go over the solutions for the completion of neural network exercise questions.

3
00:00:11.290 --> 00:00:13.210
Let's head to the notebook and get started.

4
00:00:13.220 --> 00:00:15.340
OK here we are at the exercise notebook.

5
00:00:15.340 --> 00:00:17.320
First thing I want to do is actually load up the data set.

6
00:00:17.320 --> 00:00:22.240
We can do this simply by running the cell from the exercise so tense flow carries data sets fashion

7
00:00:22.240 --> 00:00:26.200
eminence and actually want to use map Gottlieb to visualize the data.

8
00:00:26.200 --> 00:00:31.720
I mean we need to import map plot lib thought pie plot as BLT.

9
00:00:31.900 --> 00:00:37.810
And then if we take a look at the training data right now if we take a look at the shape of it it's

10
00:00:37.810 --> 00:00:40.660
sixty thousand images twenty eight by twenty eight.

11
00:00:40.690 --> 00:00:44.940
So let's go ahead and just grab one of those images.

12
00:00:44.970 --> 00:00:46.850
So now we have a single image there.

13
00:00:46.890 --> 00:00:52.700
And then to display just a single image we can say PDT M show.

14
00:00:52.990 --> 00:00:58.260
And that should display the image and in this case since the very first image item is an actual boot.

15
00:00:59.180 --> 00:00:59.850
OK.

16
00:01:00.000 --> 00:01:02.650
So we've done that we understand that we have this data.

17
00:01:02.700 --> 00:01:04.470
Now it's time to process the data.

18
00:01:04.530 --> 00:01:10.560
Luckily for us we can confirm that the maximum value is to fifty five by just saying that Max which

19
00:01:10.560 --> 00:01:18.430
means I can then do a simple pre processing by saying X train is equal to extra and divided by 255 and

20
00:01:18.430 --> 00:01:24.360
that x test is equal to x test divided by two fifty five.

21
00:01:24.370 --> 00:01:28.450
After that we can reshape the x rays to include that fourth dimension.

22
00:01:28.450 --> 00:01:41.300
So what we can do here is we can say X train is equal to x train reshape sixty thousand twenty eight

23
00:01:41.360 --> 00:01:46.970
by twenty eight by one to make sure we include that color channel or a singular color channel I should

24
00:01:46.970 --> 00:01:53.300
say and then x test is going to be x test reshape.

25
00:01:53.300 --> 00:01:55.300
And in this case it's ten thousand points.

26
00:01:55.550 --> 00:01:58.700
So it's twenty eight by twenty eight by one color channel.

27
00:01:58.760 --> 00:02:03.020
So you've reshaped it to include that color channel and then we also want to make sure we convert the

28
00:02:03.020 --> 00:02:11.720
labels to be categorical so we can say from tensor flow doctors thought utilize import to categorical

29
00:02:12.680 --> 00:02:21.130
run that and then we'll say why cat train is equal to this call of two categorical.

30
00:02:21.450 --> 00:02:22.780
And we can say why train.

31
00:02:22.850 --> 00:02:27.170
It should be able to infer that there's 10 classes but we can always make sure of that by passing it

32
00:02:27.170 --> 00:02:28.280
in as well.

33
00:02:28.280 --> 00:02:34.910
And then we'll say why cat test is equal to two categorical on why test and hopefully this feels pretty

34
00:02:34.910 --> 00:02:40.890
familiar given that we've already done almost this exact same process on the M this dataset after that

35
00:02:40.980 --> 00:02:42.950
it comes time to build out the model.

36
00:02:42.990 --> 00:02:46.920
So you want to use full and cares to build out this sequential model.

37
00:02:46.920 --> 00:02:55.830
So start of the imports we'll say from tensor flow that Kerry's import sequential and also from tensor

38
00:02:55.830 --> 00:02:58.650
flow that carries the layers import.

39
00:02:58.830 --> 00:03:07.450
And we're going to use dense compositional to these as well as a max pooling layer that's another to

40
00:03:07.450 --> 00:03:11.740
do there and then we also need to flatten out the results later on in our model.

41
00:03:11.810 --> 00:03:13.410
So we have that ready to go.

42
00:03:13.520 --> 00:03:23.680
We can create our model and then let's add in a conversational air that's compositional to the we'll

43
00:03:23.690 --> 00:03:25.310
go ahead and set filters equal to 32.

44
00:03:25.310 --> 00:03:27.160
This is a value you can play around with.

45
00:03:27.560 --> 00:03:30.080
But we'll set the defaults that we essentially did last time.

46
00:03:30.080 --> 00:03:31.440
So four by four col.

47
00:03:31.610 --> 00:03:38.120
And then we need to specify the input shape to be 28 by twenty eight by one.

48
00:03:38.360 --> 00:03:40.380
And we can also play around if the activation function.

49
00:03:40.400 --> 00:03:44.200
So I can say activation is equal to rectified linear unit.

50
00:03:44.480 --> 00:03:48.990
Once we've done that we'll go ahead and add in a pooling layer.

51
00:03:49.040 --> 00:03:54.980
So that's Max Poole to the where the pool size the default is two by two.

52
00:03:54.980 --> 00:03:59.060
So I could just leave by default or specify it myself but we'll go ahead.

53
00:03:59.050 --> 00:04:01.890
Just keep it to the bye to the.

54
00:04:02.490 --> 00:04:03.030
Next.

55
00:04:03.090 --> 00:04:06.380
For that we'll say a model ad and we'll flatten it out.

56
00:04:06.560 --> 00:04:08.610
You can add in more compositional layers if you want.

57
00:04:08.630 --> 00:04:10.590
But for use case that should be enough.

58
00:04:10.610 --> 00:04:20.550
And then we'll add in a dense layer call with an activation rectified linear unit last layer should

59
00:04:20.550 --> 00:04:25.320
be a dense layer that has the same number of neurons as number of classes.

60
00:04:25.320 --> 00:04:26.490
So that's 10 neurons.

61
00:04:26.730 --> 00:04:32.640
And we also have to make sure that it's soft Max because it's a multi class classification problem.

62
00:04:32.640 --> 00:04:38.590
And then after that we will compile this model if you want you can go ahead and add in things during

63
00:04:38.590 --> 00:04:40.930
training like an early stopping mechanism.

64
00:04:41.070 --> 00:04:51.420
But for analogies keep things simple we'll say categorical cross entropy.

65
00:04:51.540 --> 00:04:54.000
We'll go ahead and just choose an atom optimizer.

66
00:04:54.000 --> 00:04:55.190
See how that works.

67
00:04:55.500 --> 00:05:02.190
And then let's go ahead and keep track of accuracy as well so say accuracy is one of our metrics.

68
00:05:02.190 --> 00:05:11.420
We compile this we can confirm this three model summary call by saying model that summary and then we

69
00:05:11.420 --> 00:05:12.750
can see the results.

70
00:05:12.750 --> 00:05:14.950
OK then it comes time to train the model.

71
00:05:14.960 --> 00:05:17.130
The amount of epochs is really up to you.

72
00:05:17.150 --> 00:05:21.320
We'll have some results here so you can see that essentially after a couple of epochs the performance

73
00:05:21.590 --> 00:05:26.060
tends to hit a point ninety five even after just like three epochs.

74
00:05:26.060 --> 00:05:28.670
So we'll go ahead just train for let's say three epochs here.

75
00:05:28.670 --> 00:05:37.240
So a model fit fit on X train y cat train and as an option you can also pass and validation data.

76
00:05:37.250 --> 00:05:37.940
Let's go ahead and do that.

77
00:05:37.940 --> 00:05:44.310
We'll say validation data is X test with Y cat test.

78
00:05:44.370 --> 00:05:46.870
This wasn't required by the exercise.

79
00:05:46.870 --> 00:05:53.180
We can go and play around with that and let's say epochs is equal to three.

80
00:05:53.260 --> 00:05:54.280
So we won't train at that much.

81
00:05:54.310 --> 00:05:57.560
We should get pretty good results even out of three pucks and the one running it.

82
00:05:57.580 --> 00:06:01.090
You should start seeing it train for the second video I'm only doing three epochs.

83
00:06:01.090 --> 00:06:04.080
You can always add an early stopping and see how it performs.

84
00:06:04.120 --> 00:06:08.970
We can see here we're starting to reach 90 percent so it's pretty good accuracy and on one more hopefully

85
00:06:08.970 --> 00:06:10.430
a brief 90 percent perfect.

86
00:06:10.870 --> 00:06:16.780
If we train for a couple more epochs it's very likely that we would have hit ninety five percent okay.

87
00:06:16.840 --> 00:06:22.390
Following the model so lots of different ways we can do this we'll go ahead and just follow the same

88
00:06:22.390 --> 00:06:23.610
couple of metrics we did.

89
00:06:23.680 --> 00:06:28.240
Let's just ask what the model metrics were so we can do this by saying model metrics.

90
00:06:28.240 --> 00:06:32.560
Names here we can see loss and accuracy as a result.

91
00:06:32.710 --> 00:06:34.880
There's lots of different options we can do.

92
00:06:34.960 --> 00:06:40.370
I can say model history that history.

93
00:06:40.450 --> 00:06:47.120
That into a data frame and then say something like metrics is equal to that and make sure we import

94
00:06:47.120 --> 00:06:49.550
pandas in order to do this.

95
00:06:49.670 --> 00:06:51.600
Import pandas as PD.

96
00:06:51.710 --> 00:06:52.700
There we go.

97
00:06:52.850 --> 00:06:58.160
And then I should be able to plot out my metrics so nothing too crazy here.

98
00:06:58.240 --> 00:07:06.860
Gonna make sure we plot relevant metrics against each other so let's plot in loss vs. validation loss

99
00:07:08.390 --> 00:07:11.810
plot that out it looks like they were still both going down we probably couldn't train for more than

100
00:07:11.810 --> 00:07:16.910
three epochs but for the sake of this recording didn't want to waste time just training and then the

101
00:07:16.910 --> 00:07:24.080
other one we could do is accuracy versus validation accuracy and hopefully they're both also continuing

102
00:07:24.080 --> 00:07:29.150
to go up and there we go so clearly we could've been training for more epochs but let's go ahead and

103
00:07:29.150 --> 00:07:30.830
see how we did overall.

104
00:07:31.250 --> 00:07:40.720
We'll say from SDK learn metrics import a classification report and then what we can do here is say

105
00:07:40.720 --> 00:07:50.290
predictions is equal to model predict classes and we'll do that on our x test set and then we simply

106
00:07:50.290 --> 00:08:01.230
print out our classification report comparing y true or really y test to our predictions.

107
00:08:01.230 --> 00:08:06.420
Okay so based off the fact that you probably then train for enough epochs compared to the 10 epochs

108
00:08:06.420 --> 00:08:09.000
in the solution we're still actually performing pretty well.

109
00:08:09.000 --> 00:08:12.360
We have 90 percent accuracy which is essentially what we were aiming for.

110
00:08:12.570 --> 00:08:15.720
Lots of other things you can do here like experiment with compositional layers.

111
00:08:15.720 --> 00:08:21.750
Adding in more neurons etc. playing around kernel sizes but hopefully this was a good exercise in just

112
00:08:21.750 --> 00:08:26.310
you following the general workflow for setting up a compositional neural network.

113
00:08:26.810 --> 00:08:28.820
OK that's it for this section.

114
00:08:28.830 --> 00:08:30.270
Thanks and I'll see you at the next one.
