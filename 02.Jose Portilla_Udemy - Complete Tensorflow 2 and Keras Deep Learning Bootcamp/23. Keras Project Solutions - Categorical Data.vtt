WEBVTT
1
00:00:05.500 --> 00:00:06.840
Welcome back everyone.

2
00:00:06.910 --> 00:00:11.170
In this lecture we're going to show you how to work with the categorical data in our data set.

3
00:00:11.230 --> 00:00:13.030
So we just took care of the missing data.

4
00:00:13.030 --> 00:00:18.610
Now let's explore the categorical data and the string data and see if we either remove it or convert

5
00:00:18.610 --> 00:00:21.340
it to dummy variables using one hot encoding.

6
00:00:21.340 --> 00:00:24.130
Let's head back to the Jupiter notebook and continue where we left off.

7
00:00:24.290 --> 00:00:26.350
OK here I am at the notebook again.

8
00:00:26.350 --> 00:00:29.920
We've already taken care of all the known values I can see here from before.

9
00:00:29.920 --> 00:00:31.260
We have no more null values.

10
00:00:31.420 --> 00:00:34.450
So now we need to do is take care of categorical variables.

11
00:00:34.930 --> 00:00:39.490
So the first task is to list all the columns that are currently non numeric and there's two helpful

12
00:00:39.490 --> 00:00:45.040
links here that essentially show you how to quickly get it through a very useful method call and the

13
00:00:45.040 --> 00:00:53.200
way we can do that is if you check out those links it would tell you to run DFS select D types and we

14
00:00:53.200 --> 00:00:59.560
could have selected pass an object as a single item string and then called Dot columns and that only

15
00:00:59.560 --> 00:01:02.200
selects the essentially string columns.

16
00:01:02.200 --> 00:01:07.420
So now we have to do is we have to go through each of these term grades sub grade et cetera and see

17
00:01:07.420 --> 00:01:08.460
what we should do with them.

18
00:01:08.470 --> 00:01:12.500
Some of them will just remove others will be useful to us and we'll keep them.

19
00:01:12.640 --> 00:01:16.060
So we have to decide and the first we're going to start off with is term.

20
00:01:16.120 --> 00:01:23.050
So let's go ahead and check out term and if we check it out term in fact we can do a feature Information

21
00:01:23.050 --> 00:01:27.760
request on term from that function that we have for you is the number of payments on the loan.

22
00:01:27.760 --> 00:01:32.260
And values are in months and can either be 36 months or 60 months.

23
00:01:32.260 --> 00:01:38.620
And if we take a look at f term and call value counts.

24
00:01:38.620 --> 00:01:44.590
Notice it's essentially a binary column there's either thirty six months or six months which means we

25
00:01:44.590 --> 00:01:48.700
have a couple of options here because it's also numeric.

26
00:01:48.700 --> 00:01:55.810
I could convert this to be either 36 as an American integer or 60 as a numeric integer so directly just

27
00:01:56.080 --> 00:02:00.330
grab these basically first two numbers and convert them into an integer.

28
00:02:00.340 --> 00:02:05.800
The other thing I could do is because it's technically just two categories is I could one hot encode

29
00:02:06.400 --> 00:02:14.260
it to be either 36 months or not 36 months now because there is a numeric relationship between 36 and

30
00:02:14.260 --> 00:02:15.400
60 months.

31
00:02:15.400 --> 00:02:21.460
Let's go ahead and keep that numeric relationship essentially showing you the relationship between having

32
00:02:21.460 --> 00:02:23.290
more time to pay off the loan.

33
00:02:23.830 --> 00:02:30.330
So we're going to do is we'll map this directly to 36 and 60 and there's many ways you could do this.

34
00:02:30.440 --> 00:02:36.530
One way is to just create a dictionary called some mapping and then map 30 six month string to the numeric

35
00:02:36.560 --> 00:02:39.800
36 60 month string to the number 60.

36
00:02:39.920 --> 00:02:44.900
But another way we could do this is by grabbing those first two characters in the string and converting

37
00:02:44.900 --> 00:02:47.140
those into an actual number.

38
00:02:47.150 --> 00:02:52.810
So the way you would do that is by applying a function that does that Orland expression.

39
00:02:52.840 --> 00:02:56.380
So we're going to do is to save a little bit space we'll do a land expression.

40
00:02:56.380 --> 00:03:01.390
We're essentially going to go through the terms and for the terms we'll grab the first two characters

41
00:03:01.480 --> 00:03:09.000
sets everything up to bet on including index three and then we will convert that into an integer and

42
00:03:09.000 --> 00:03:15.480
then we'll say def term is equal to the F term and we apply that.

43
00:03:15.550 --> 00:03:17.890
So we'll go ahead and run that.

44
00:03:17.920 --> 00:03:24.450
Keep in mind you may get a warning but in this case that is OK is essentially warning you that you're

45
00:03:24.450 --> 00:03:27.650
taking an existing column and then reassigning it.

46
00:03:27.660 --> 00:03:32.930
So just telling you what you kind of already know and if you take a look at the f term now you'll notice

47
00:03:32.940 --> 00:03:34.750
now just 36 and 60.

48
00:03:34.770 --> 00:03:40.490
So it can actually run this value counts again and we see we essentially converted three six months

49
00:03:40.490 --> 00:03:43.310
60 months to the integers 36 and 60.

50
00:03:43.590 --> 00:03:48.550
So that column has now been taken care of it's now numeric column that we can pass into tensor flow.

51
00:03:48.780 --> 00:03:53.910
The next we're going to take a look at is the great feature and we actually already know that grade

52
00:03:54.120 --> 00:03:56.160
is simply a part of sub grade.

53
00:03:56.160 --> 00:04:00.990
So we can just drop the great feature because recall it sub grade already had that information if something

54
00:04:00.990 --> 00:04:05.810
was sub grade G1 then we already knew as part of the G grade.

55
00:04:05.820 --> 00:04:08.080
So that's essentially duplicate information.

56
00:04:08.130 --> 00:04:09.720
So we'll go ahead and drop it.

57
00:04:09.870 --> 00:04:17.980
Let's just say the f is equal to D F that drop and we'll say grade X is equal to 1.

58
00:04:18.000 --> 00:04:22.980
Keep in mind whenever you're running these drop method calls or function calls you can only run them

59
00:04:22.980 --> 00:04:23.400
once.

60
00:04:23.400 --> 00:04:27.010
Otherwise you can error because you can't drop something more than once.

61
00:04:27.030 --> 00:04:32.880
So we go ahead and run that and now we no longer have that column and now we're going to do is we're

62
00:04:32.880 --> 00:04:38.310
going to convert these sub grade column into dummy variables and then we'll concatenate these new columns

63
00:04:38.310 --> 00:04:39.610
to the original data frame.

64
00:04:39.810 --> 00:04:44.670
A couple of things to remember from the lectures is to drop the original site grade column since we

65
00:04:44.670 --> 00:04:50.010
won't need it after that and to add drop underscore first is equal to true in your get dummies call

66
00:04:50.220 --> 00:04:55.710
to prevent the multi variable trap essentially prevent encoding duplicate information.

67
00:04:55.710 --> 00:04:57.590
So let's go ahead and check this out.

68
00:04:57.660 --> 00:05:01.120
Lots of different ways you can do this but there's essentially three main steps.

69
00:05:01.170 --> 00:05:04.080
The first step is to actually get the dummy variables.

70
00:05:04.080 --> 00:05:13.200
So we'll say dummies is equal to PD dot get dummies and then I'm going to obtain them from the sub grade

71
00:05:13.890 --> 00:05:14.820
column.

72
00:05:14.820 --> 00:05:21.840
So I have my sub great column and recall since it's a column I need to also say here that drop first

73
00:05:22.410 --> 00:05:28.470
is equal to true and that eventually will stop us from encoding duplicate information.

74
00:05:28.490 --> 00:05:30.040
So that's why we dropped the first one.

75
00:05:30.120 --> 00:05:36.180
You can imagine in the case if we had some sort of a gender or sex column where it was male or female

76
00:05:36.510 --> 00:05:40.230
we don't need to encode both for them to be zeros and ones instead.

77
00:05:40.390 --> 00:05:45.300
That would just get converted to whether or not they're male 0 or 1.

78
00:05:45.300 --> 00:05:48.350
Otherwise you're duplicating the information and that's the same thing.

79
00:05:48.360 --> 00:05:53.190
If you start expanding to more categories so if you have ABC as your categories.

80
00:05:53.190 --> 00:05:59.630
So if you have ABC you don't need to encode that 2 0 1 0 or something like that.

81
00:05:59.760 --> 00:06:05.640
You just need the A and B columns because if it's not and if it's not b that's implied that it's going

82
00:06:05.640 --> 00:06:06.280
to be C..

83
00:06:06.330 --> 00:06:09.830
So that's why we have the drop first versus equal to true there.

84
00:06:09.840 --> 00:06:10.450
Okay.

85
00:06:10.560 --> 00:06:14.750
So after that we need to concatenate that with the actual data frame.

86
00:06:15.120 --> 00:06:23.290
So we're going to say D F and first we'll drop the original sub grade column because we won't need it

87
00:06:23.290 --> 00:06:24.970
anymore since we have the dummies.

88
00:06:25.150 --> 00:06:29.200
And then what we'll do is concatenate that with the dummies.

89
00:06:29.200 --> 00:06:32.180
So say PD concatenate this.

90
00:06:32.190 --> 00:06:38.520
We pass this as a list concatenate it with make sure it says X is equal to 1 with the dropping of the

91
00:06:38.520 --> 00:06:42.090
sub grade and then say sub grade

92
00:06:45.260 --> 00:06:46.550
actually just say dummies here

93
00:06:51.440 --> 00:06:55.380
and then we'll say axes is equal to 1.

94
00:06:55.430 --> 00:07:00.410
So we do that function call and then we set that to our existing data frame.

95
00:07:00.440 --> 00:07:03.050
So really just two main lines of code.

96
00:07:03.050 --> 00:07:08.330
First one is to get the dummies themselves off the sub great column and then the next one is to make

97
00:07:08.330 --> 00:07:15.240
sure to remove that original column and then concatenate it with the dummies along the columns.

98
00:07:15.350 --> 00:07:17.340
And that's essentially all there is to this.

99
00:07:17.810 --> 00:07:21.750
And you have to make sure that your brackets and parentheses match up.

100
00:07:21.890 --> 00:07:22.910
Looks like we're good here.

101
00:07:22.910 --> 00:07:24.980
So let's go ahead and run this.

102
00:07:25.220 --> 00:07:26.000
And there we go.

103
00:07:26.000 --> 00:07:33.200
So now if I take a look at the FDR columns you'll notice I have a lot more columns because now we have

104
00:07:33.200 --> 00:07:35.830
one high encoding for each of the possible upgrades.

105
00:07:35.840 --> 00:07:38.750
And we did drop the very first one which was 8 1.

106
00:07:38.750 --> 00:07:43.070
OK so there are my columns now and we're going to continue 1.

107
00:07:43.100 --> 00:07:49.490
I mean go ahead and comment that out and now we're going to essentially do the same thing for verification

108
00:07:49.490 --> 00:07:56.960
status application type initial status and the purpose column.

109
00:07:56.960 --> 00:08:01.430
So if you take a look at these other columns there are also good candidates for being converted into

110
00:08:01.430 --> 00:08:06.650
dummy variables because there's not a whole lot of categories and they're pretty much also for the most

111
00:08:06.650 --> 00:08:09.010
of these binary or just a few categories.

112
00:08:09.020 --> 00:08:14.510
So they're good candidates to go ahead and convert them into dummy variables so we can actually do this

113
00:08:14.570 --> 00:08:17.120
with essentially the exact same steps we did up here.

114
00:08:17.120 --> 00:08:24.300
So I can simply just copy and paste these and then put them in here and what I'm going to do is I just

115
00:08:24.300 --> 00:08:28.970
need to cover these columns and we set him up in a way here that you can just copy and paste.

116
00:08:29.040 --> 00:08:34.950
So we're going to convert and get dummies and what to get them is command you can pass in all of these

117
00:08:34.950 --> 00:08:41.860
guys at once and then we'll go ahead and also drop the original columns at once.

118
00:08:41.970 --> 00:08:43.620
So we'll go ahead and do that as well.

119
00:08:43.710 --> 00:08:45.090
And that's pretty much it.

120
00:08:45.090 --> 00:08:49.650
So it's actually the exact same commands before you just pass and a list of the columns now which is

121
00:08:49.650 --> 00:08:50.270
kind of nice.

122
00:08:50.340 --> 00:08:52.010
So no extra work we need to do.

123
00:08:52.110 --> 00:08:52.980
We'll go ahead and run that.

124
00:08:53.160 --> 00:08:54.520
Let me take a little bit more time.

125
00:08:54.570 --> 00:08:59.260
Case it was pretty fast here but we went ahead and took care of those columns.

126
00:08:59.300 --> 00:09:04.020
OK now for home ownership we'll go ahead and review the value counts here for the Home Ownership column

127
00:09:04.440 --> 00:09:06.380
to get something that looks like this.

128
00:09:06.380 --> 00:09:14.520
So say IDF homeownership and if you do value counts here what you'll end up noticing is that essentially

129
00:09:14.850 --> 00:09:21.720
most people are in either three categories mortgaging renting or owning and then just very few are and

130
00:09:21.780 --> 00:09:23.910
other none are any.

131
00:09:23.910 --> 00:09:29.610
So what we're going to do here is because there's so few people in none or any let's just go ahead and

132
00:09:29.670 --> 00:09:33.250
put these guys is twenty nine and three in the other category.

133
00:09:33.270 --> 00:09:38.700
So that way we reduce the number of actual feature columns because we don't want an entire feature column

134
00:09:39.030 --> 00:09:46.570
for just twenty nine plus three people instead we'll go ahead and put those in the other so to begin

135
00:09:46.570 --> 00:09:53.710
this we first want to replace none and any with other and the replace Link will actually show you this

136
00:09:53.920 --> 00:10:00.780
call which allows you to directly replace string values of other string values so we can see here there

137
00:10:00.790 --> 00:10:03.060
is documentation on how this actually works.

138
00:10:03.160 --> 00:10:05.260
But essentially the call looks like this.

139
00:10:05.260 --> 00:10:08.030
We say and there's many ways you could do this you could also just map it.

140
00:10:08.050 --> 00:10:15.090
You could say none in any get map to other or just do your custom apply function but the only thing

141
00:10:15.120 --> 00:10:20.100
actually have to do here is called homeownership and say replace and you can pass and a list of what

142
00:10:20.100 --> 00:10:20.760
you want to replace.

143
00:10:20.770 --> 00:10:23.040
So this case owner replace none.

144
00:10:23.220 --> 00:10:29.430
And any any pass as a second parameter which you want to replace them with in this case.

145
00:10:29.430 --> 00:10:33.840
I want to replace them with other so you could do this either if a dictionary mapping your custom apply

146
00:10:33.840 --> 00:10:40.710
function or with this convenience replace function lots of the four ways you could do this so go ahead

147
00:10:40.800 --> 00:10:42.890
and run that replacement.

148
00:10:42.900 --> 00:10:46.530
So now if I run value counts again.

149
00:10:46.530 --> 00:10:51.270
Notice those twenty nine plus three people have been added to the other column.

150
00:10:51.330 --> 00:10:56.340
So now we'll convert this into dummy variables and to do that I will simply copy paste what we have

151
00:10:56.340 --> 00:10:56.790
before.

152
00:10:56.800 --> 00:11:04.290
So let me copy this guy will come back down here to homeownership paste that in and replace some grade

153
00:11:04.950 --> 00:11:06.500
with homeownership.

154
00:11:06.510 --> 00:11:09.680
So put that guy in there and put that in there.

155
00:11:10.050 --> 00:11:12.040
OK so go ahead and run that.

156
00:11:12.150 --> 00:11:14.090
And now we've replaced homeownership.

157
00:11:14.100 --> 00:11:18.430
Keep in mind you should only be able to run these one time otherwise you'll start getting errors.

158
00:11:18.450 --> 00:11:21.570
Now the next column we're going to take a look at is the address column.

159
00:11:21.600 --> 00:11:28.340
So we're going to do is if we actually take a look at examples of the address column it looks like it's

160
00:11:28.350 --> 00:11:29.850
somebodies full address.

161
00:11:29.850 --> 00:11:33.800
So it's some sort of address number a street estate.

162
00:11:33.930 --> 00:11:39.230
And then it looks like we also have zip codes where we're going to be doing is we're going to be actually

163
00:11:39.240 --> 00:11:42.660
feature engineering by extracting the zip code from this.

164
00:11:42.690 --> 00:11:44.770
And there's many different ways you can do this.

165
00:11:44.790 --> 00:11:51.780
One way is to use the same technique we used up here when we were extracting the actual numbers from

166
00:11:51.780 --> 00:11:54.360
the 36 and 60 string.

167
00:11:54.360 --> 00:12:00.060
So notice that the zip code is always going to be the last five digits here.

168
00:12:00.060 --> 00:12:06.900
So we have these last five digits which means if I were to apply a function or lambda expression either

169
00:12:06.900 --> 00:12:09.380
one if I said lambda.

170
00:12:09.570 --> 00:12:16.370
Go ahead and take a look at that address and then grab starting from position negative five all the

171
00:12:16.370 --> 00:12:22.330
way to the end and run that that will actually grab that last item of information.

172
00:12:22.370 --> 00:12:33.880
So if I take a look at this I can say DLF ZIP code is equal to that extraction from the actual address

173
00:12:33.880 --> 00:12:34.850
column.

174
00:12:34.930 --> 00:12:40.150
So we run that and that we're going to do is take a look at making this zip code column into dummy variables

175
00:12:40.270 --> 00:12:43.680
so we can essentially make a category per zip code.

176
00:12:43.870 --> 00:12:52.570
And if I did def zip code and then called value counts on it you'll notice that there's actually not

177
00:12:52.570 --> 00:12:55.060
that many unique zip codes here.

178
00:12:55.060 --> 00:13:02.370
So we run this and it looks like there's a total of 1 2 3 4 5 6 7 8 9 10 zip codes.

179
00:13:02.470 --> 00:13:08.950
So that's not totally unreasonable to then create 10 dummy features off of this essentially nine extra

180
00:13:08.950 --> 00:13:11.260
columns since we're going to drop the first one.

181
00:13:11.260 --> 00:13:13.830
So we'll just copy and paste the same code that we've been doing up here.

182
00:13:14.590 --> 00:13:19.830
But now we will replace homeownership with this new zip code column that we created.

183
00:13:19.990 --> 00:13:28.380
So go ahead and grab that and then we're going to pass this in OK we will run that and then we're going

184
00:13:28.380 --> 00:13:33.980
to do is we'll go ahead and drop that original address column since we won't be needing anymore.

185
00:13:34.020 --> 00:13:42.120
So say DLF is equal to the F drop and then we'll go ahead pass and address there along axes is equal

186
00:13:42.120 --> 00:13:43.880
to 1.

187
00:13:43.880 --> 00:13:46.230
OK so that took care of the address column.

188
00:13:46.350 --> 00:13:49.120
Next one is issue underscore D.

189
00:13:49.200 --> 00:13:55.990
So if we take a look at the feature information for this guy and run it you'll notice that this feature

190
00:13:55.990 --> 00:13:59.610
information indicates the month which the loan was funded.

191
00:13:59.890 --> 00:14:06.490
And if we actually think about the problem we're trying to solve here is we're trying to specify based

192
00:14:06.490 --> 00:14:13.240
off someone's already known features whether or not they're going to pay back alone which ideally that's

193
00:14:13.240 --> 00:14:15.310
going to happen before we actually issue them alone.

194
00:14:15.310 --> 00:14:19.910
Because if we think they're not going to actually pay back the loan we will issue one in the first place.

195
00:14:19.960 --> 00:14:25.210
So we in reality won't have an issue date for when we're actually running the model.

196
00:14:25.240 --> 00:14:27.930
It's not like we're ever going to know that we issue them alone.

197
00:14:27.940 --> 00:14:30.900
Otherwise that kind of defeats the whole purpose of the model.

198
00:14:30.910 --> 00:14:36.010
So this is actually data leakage since realistically speaking upon application of the model you won't

199
00:14:36.010 --> 00:14:37.250
have an issue date.

200
00:14:37.270 --> 00:14:46.730
So we'll go ahead and just drop this column we'll say the f is equal to the F drop issue date along

201
00:14:46.730 --> 00:14:47.800
axis is equal to 1.

202
00:14:48.880 --> 00:14:49.560
OK.

203
00:14:49.810 --> 00:14:52.100
Next one is this earliest credit line.

204
00:14:52.180 --> 00:14:58.150
So if we take a look at this function or this feature closely in fact we can just call feature information

205
00:14:58.150 --> 00:15:04.270
here and pass in earliest credit line.

206
00:15:04.320 --> 00:15:08.730
This appears to be the month the borrower's earliest reported credit line was open.

207
00:15:08.820 --> 00:15:11.550
So there's a historical timestamp feature.

208
00:15:11.550 --> 00:15:18.360
So we're we're going to do is we're going to extract the year from this feature using a dot apply function.

209
00:15:18.360 --> 00:15:20.110
So there's lots of different ways you can do this.

210
00:15:20.280 --> 00:15:27.690
But for first going to convert it to some sort of date feature or we can actually just grab it based

211
00:15:27.690 --> 00:15:28.950
off its position.

212
00:15:29.000 --> 00:15:33.170
So let's take a look at the examples in the sexual column.

213
00:15:33.420 --> 00:15:36.090
We'll say earliest credit line run that.

214
00:15:36.240 --> 00:15:42.360
And notice right now it's essentially three letters for a month a dash and then the year of the earliest

215
00:15:42.360 --> 00:15:43.520
credit line.

216
00:15:43.530 --> 00:15:49.650
So what we'll do here is we can either convert this to a date time and then extract with an attribute

217
00:15:49.650 --> 00:15:56.630
call the year or just grab the last four characters in the string and convert that to an integer.

218
00:15:56.790 --> 00:15:58.760
That's by slightly easier to do.

219
00:15:58.860 --> 00:16:02.190
So we'll go ahead and do that approach although either one is totally valid.

220
00:16:02.910 --> 00:16:04.770
So we're would say apply lambda.

221
00:16:04.890 --> 00:16:11.940
And on this date go ahead and starting from position negative for go all the way to the end to grab

222
00:16:11.940 --> 00:16:19.140
those last four characters and then we will convert that to an integer and we'll say def earliest credit

223
00:16:19.140 --> 00:16:25.010
line is now equal to this.

224
00:16:25.160 --> 00:16:30.740
So we'll go ahead and run that and we'll say IDF earliest credit line.

225
00:16:30.740 --> 00:16:36.060
And now if we take a look at it we have this new column.

226
00:16:36.090 --> 00:16:39.220
Now keep in mind if you follow these task instructions exactly.

227
00:16:39.390 --> 00:16:43.260
We did ask you to create a new column called earliest credit year and then drop the earliest credit

228
00:16:43.260 --> 00:16:46.320
line feature is essentially the same thing if you just overwrite it.

229
00:16:46.350 --> 00:16:51.810
So you can overwrite it or assign it to some new column and then drop it really up to you main idea

230
00:16:51.810 --> 00:16:55.560
here is that we essentially just want the year since that's something we can work with.

231
00:16:55.650 --> 00:16:59.730
And in fact we can further explore this by doing value counts on it.

232
00:17:00.240 --> 00:17:04.950
And note here that we have value accounts that make sense and we don't need to convert this to dummy

233
00:17:04.950 --> 00:17:09.000
variables because the year itself can be treated as a continuous data type.

234
00:17:09.450 --> 00:17:10.320
OK.

235
00:17:10.410 --> 00:17:16.920
So moving along the next series of steps will just be pre processing things like train test splits as

236
00:17:16.920 --> 00:17:19.110
well as scaling the data.

237
00:17:19.110 --> 00:17:21.850
So we'll go ahead and take care of that in the next lecture.

238
00:17:21.870 --> 00:17:22.570
I'll see you there.
