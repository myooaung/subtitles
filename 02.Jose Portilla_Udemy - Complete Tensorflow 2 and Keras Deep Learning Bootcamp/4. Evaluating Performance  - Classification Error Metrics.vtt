WEBVTT
1
00:00:05.570 --> 00:00:11.180
Welcome everyone to this lecture on evaluating performance particularly for classification problems

2
00:00:11.570 --> 00:00:12.400
in the next lecture.

3
00:00:12.410 --> 00:00:18.960
We'll discuss evaluating performance for regression protests so we just learned that after a machine

4
00:00:18.960 --> 00:00:24.150
learning process is complete we're going to be using performance metrics to evaluate how our model actually

5
00:00:24.210 --> 00:00:25.170
did.

6
00:00:25.170 --> 00:00:29.910
And we keep mentioning the fact that we train our model on the training data and then we're gonna use

7
00:00:29.910 --> 00:00:34.720
some sort of metric to actually see how it performed on the test set or the validation set.

8
00:00:34.770 --> 00:00:37.240
So let's go ahead and discuss what that actually means.

9
00:00:37.260 --> 00:00:42.830
What classification metrics are we going to be using and the key classification metrics we should be

10
00:00:42.830 --> 00:00:50.860
understanding our accuracy recall precision and f1 score but first we should understand the reasoning

11
00:00:50.860 --> 00:00:57.890
behind these metrics and how they will actually work in the real world typically in any classification

12
00:00:57.890 --> 00:01:03.650
task your model can actually really only achieve two results either your model was correct in its prediction

13
00:01:04.070 --> 00:01:12.200
or your model was incorrect in its prediction and all classification metrics stem from this idea now

14
00:01:12.210 --> 00:01:18.480
fortunately incorrect versus correct also expands to situations where you have multiple classes such

15
00:01:18.480 --> 00:01:22.160
as trying to predict categories of more than two.

16
00:01:22.230 --> 00:01:28.380
For example you have Category A B C D You can either be correct in predicting the correct category or

17
00:01:28.410 --> 00:01:30.610
incorrect in predicting the right category.

18
00:01:30.720 --> 00:01:35.250
Now for the purpose of explaining the metrics we're gonna go ahead and simplify this to what's known

19
00:01:35.250 --> 00:01:39.150
as a binary classification situation binary meaning two.

20
00:01:39.180 --> 00:01:43.350
So there's only two available classes either class 0 or Class 1.

21
00:01:43.560 --> 00:01:47.400
And this idea is going to expand to multiple classes as well.

22
00:01:47.400 --> 00:01:54.610
But for simplification let's imagine just a binary classification situation so in our example we're

23
00:01:54.610 --> 00:02:00.750
going to attempt to predict if an image is a dog or a cat will actually perform this task later on during

24
00:02:00.750 --> 00:02:04.020
the convolution all neural network portion of this course.

25
00:02:04.020 --> 00:02:10.140
Now since it's a supervised learning problem where we're gonna need to do first is fit or train a model

26
00:02:10.200 --> 00:02:11.430
on training data.

27
00:02:11.430 --> 00:02:16.850
That means we're gonna have images that someone's already gone ahead and labeled dog or cat.

28
00:02:16.890 --> 00:02:22.470
So we know the correct answer on these images then we're going to test that model on the testing data.

29
00:02:22.560 --> 00:02:26.500
So we're gonna show new images that the model hasn't seen before.

30
00:02:26.580 --> 00:02:31.530
Get the models prediction and then compare the results of the model prediction to the correct answer

31
00:02:31.530 --> 00:02:33.240
that we already know.

32
00:02:33.240 --> 00:02:37.860
So once we have the models predictions from X test data we compare it to the true y values.

33
00:02:37.860 --> 00:02:43.380
The correct labels so let's actually diagram this process out.

34
00:02:43.490 --> 00:02:48.230
Let's imagine we've already trained our model on some training data and now it's time to actually evaluate

35
00:02:48.260 --> 00:02:49.540
the models performance.

36
00:02:49.550 --> 00:02:52.130
This is where our test dataset comes in.

37
00:02:52.310 --> 00:02:57.680
So we take a test image from where we're going to label x test X meaning feature.

38
00:02:57.680 --> 00:03:04.570
So the actual image itself is a feature and this is from the test set and there is a corresponding correct

39
00:03:04.630 --> 00:03:06.700
label from Y test.

40
00:03:06.820 --> 00:03:12.190
So we have the feature or image X and that's the test image and we also have the correct label from

41
00:03:12.220 --> 00:03:12.820
Y test.

42
00:03:12.820 --> 00:03:15.820
And in this case we can see that this image is an image of a dog.

43
00:03:16.360 --> 00:03:18.900
So we're going to do is we're just going to feed in the features.

44
00:03:18.940 --> 00:03:25.380
In this case the image into our already trained model and then the model is going to make some prediction.

45
00:03:25.500 --> 00:03:32.110
So the model predicts that this is a dog and then what we do is we compare the prediction to the correct

46
00:03:32.110 --> 00:03:32.890
label.

47
00:03:32.890 --> 00:03:40.940
So this dog equal dog in this case it was correct however maybe it predicted that this image was a cat.

48
00:03:40.940 --> 00:03:45.230
And in this case this comparison to the correct label would be incorrect.

49
00:03:45.230 --> 00:03:47.960
And these are essentially just the only two situations.

50
00:03:47.960 --> 00:03:55.370
Either you're right or you're wrong correct or incorrect so we're repeat this process for all the images

51
00:03:55.430 --> 00:03:56.910
in our x test data.

52
00:03:57.110 --> 00:04:02.240
And at the end we will have a count of correct matches and account of incorrect matches.

53
00:04:02.390 --> 00:04:08.570
And the key realization here that we need to make is that in the real world not all incorrect or correct

54
00:04:08.570 --> 00:04:10.910
matches hold equal value.

55
00:04:11.000 --> 00:04:16.640
So let's kind of hone in on what we mean by that so in the real world a single metric won't tell the

56
00:04:16.640 --> 00:04:22.460
complete story and to understand all this let's bring back those four metrics we mentioned and see how

57
00:04:22.460 --> 00:04:23.640
they're actually calculated.

58
00:04:23.960 --> 00:04:29.270
We could organize our predicted values compared to the real values and what is known as a confusion

59
00:04:29.360 --> 00:04:34.680
matrix so we'll touch upon the confusion matrix later on.

60
00:04:34.700 --> 00:04:40.820
But first let's talk about accuracy accuracy is one of the most common classification metrics.

61
00:04:40.960 --> 00:04:43.460
And luckily it's also the easiest to understand.

62
00:04:43.510 --> 00:04:48.940
It's just really intuitive what it's measuring accuracy and classification problems is the number of

63
00:04:48.940 --> 00:04:56.620
correct predictions made by the model divided by the number of or the total number of predictions.

64
00:04:56.620 --> 00:05:01.300
So again it's the number of correct predictions divided by the total number of predictions essentially

65
00:05:01.300 --> 00:05:07.230
answering the questions how many predictions does you get right as a percentage so for example let's

66
00:05:07.230 --> 00:05:12.900
imagine that X test set was 100 images and our model correctly predicted 80 images.

67
00:05:12.900 --> 00:05:21.070
Then we have 80 divided by 100 or zero point eight or 80 percent accuracy and accuracy is really useful

68
00:05:21.340 --> 00:05:23.860
when the target classes are well balanced.

69
00:05:23.860 --> 00:05:24.640
So what does that mean.

70
00:05:24.640 --> 00:05:26.750
Well balanced well in our example.

71
00:05:26.830 --> 00:05:32.890
That would mean we have roughly the same amount of cat images as we have dog images throughout our data.

72
00:05:32.890 --> 00:05:40.360
So it means the actual labels themselves are roughly equally represented in the dataset but let's imagine

73
00:05:40.390 --> 00:05:43.650
that we have what's known as an unbalanced class situation.

74
00:05:43.870 --> 00:05:47.640
In this case accuracy is actually not a good metric to use.

75
00:05:47.680 --> 00:05:49.570
So let's do a little thought experiment.

76
00:05:49.660 --> 00:05:56.440
Let's imagine that in this test set we had ninety nine images of dogs and only one image of a cat.

77
00:05:56.440 --> 00:06:03.130
Now if our model was simply a line that just always predicted dog no matter what image it saw then we

78
00:06:03.130 --> 00:06:08.320
would actually get ninety nine percent accuracy on this particular test set because by just always saying

79
00:06:08.320 --> 00:06:13.560
dog ninety nine images are dogs and one images of a cat then we only missed one.

80
00:06:13.600 --> 00:06:19.390
So that's why you have to be aware of the downside of accuracy and the downside really comes when you

81
00:06:19.390 --> 00:06:21.760
have an unbalanced class situation.

82
00:06:21.760 --> 00:06:28.180
If your classes are roughly balanced then accuracy is a very nice intuitive method or a metric to understand.

83
00:06:28.210 --> 00:06:32.590
However if you have an unbalanced class you can see in this particular situation where it starts to

84
00:06:32.590 --> 00:06:37.930
represent a problem and that's where the other metrics come into play.

85
00:06:37.950 --> 00:06:43.860
So again in this situation will want to understand recall and precision that helps us understand how

86
00:06:43.860 --> 00:06:51.190
well it's performing on unbalanced classes recall is the ability of a model to find all the relevant

87
00:06:51.190 --> 00:06:57.430
cases within a dataset and the precise definition of recall is the number of what's known as true positives

88
00:06:57.790 --> 00:07:00.670
and we'll kind of hone in on that later when we see the confusion matrix.

89
00:07:00.790 --> 00:07:05.590
But it's the number of true positives divided by the number of true positives plus the number of false

90
00:07:05.590 --> 00:07:13.060
negatives and the precision is the ability of a classification model to identify only the relevant data

91
00:07:13.060 --> 00:07:18.850
points where precision is defined as the number of true positives divided by the number of true positives

92
00:07:18.880 --> 00:07:22.710
plus a number of false positives now.

93
00:07:22.770 --> 00:07:29.160
Often you have a tradeoff between recall and precision while recall expresses the ability to find all

94
00:07:29.160 --> 00:07:35.040
the relevant instances in a dataset precision expresses the proportion of the data points our model

95
00:07:35.040 --> 00:07:42.670
says was relevant that actually were relevant and then F1 one score is essentially a combination of

96
00:07:42.670 --> 00:07:43.570
these two.

97
00:07:43.600 --> 00:07:49.420
In cases where we want to find optimal blend of precision and recall we can combine the two metrics

98
00:07:49.540 --> 00:07:58.040
using what is known as the F1 score and the F1 score is the harmonic mean of precision and recall taking

99
00:07:58.040 --> 00:08:00.980
both metrics into account in the following equation.

100
00:08:00.980 --> 00:08:07.160
So this isn't just taking the average of record precision it's taking the harmonic mean of them.

101
00:08:07.160 --> 00:08:11.320
And so here we have the formula for F1 score or F1 scores equal to two times.

102
00:08:11.480 --> 00:08:16.760
Precision times recall in the numerator and then divided by precision plus recall in the denominator

103
00:08:18.610 --> 00:08:23.110
so the reason we use the harmonic mean instead of just a simple mean or simple average is because it's

104
00:08:23.110 --> 00:08:25.210
going to punish extreme values.

105
00:08:25.240 --> 00:08:30.640
For example if you created a classifier that had a position of 1 meaning essentially perfect precision

106
00:08:31.000 --> 00:08:34.510
and a recall of zero essentially the worst record possible.

107
00:08:34.510 --> 00:08:39.360
If you were just to take the simple average of this then it would be zero point five.

108
00:08:39.370 --> 00:08:44.860
But the F1 score because it's taken a harmonic mean we have precision times recall there at the top

109
00:08:45.490 --> 00:08:49.820
which means you get zero times one of the top and you get an F1 score of zero.

110
00:08:50.290 --> 00:08:55.600
So this is really nice because it lets you punish extreme differences between the precision and recall

111
00:08:55.960 --> 00:09:00.520
so gives you kind of a fair assessment of that tradeoff between precision and recall.

112
00:09:00.520 --> 00:09:06.550
Now we can also view all correct classified versus incorrectly classified images in the form of a confusion

113
00:09:06.550 --> 00:09:07.690
matrix.

114
00:09:07.690 --> 00:09:10.400
So confusing matrix looks something like this.

115
00:09:10.510 --> 00:09:12.740
We have the underlying true conditions.

116
00:09:12.760 --> 00:09:18.580
That's the true correct label and we can think of condition as positive or negative such as actually

117
00:09:18.580 --> 00:09:25.840
being a dog or not being a dog or oftentimes is used in a medical diagnosis such as having presence

118
00:09:25.840 --> 00:09:30.910
of a disease versus not having presence of a disease and then we have the models predicted condition.

119
00:09:30.910 --> 00:09:36.370
So prediction positive or prediction negative and I think when it comes to confusion matrices it's often

120
00:09:36.370 --> 00:09:42.430
easiest to first get an intuition if you think of this as a diagnostic test for having some sort of

121
00:09:42.430 --> 00:09:48.010
disease present in a person after maybe you take a blood sample and run it through your model.

122
00:09:48.550 --> 00:09:54.580
So for example a true positive would be someone actually having that disease and your model correctly

123
00:09:54.580 --> 00:09:59.830
predicting that they have it a true negative would be someone not having the disease and your model

124
00:09:59.830 --> 00:10:02.500
predicting correctly that they do not have the disease.

125
00:10:02.620 --> 00:10:07.840
So have true positives and true negatives and those are both correct predictions and then we have essentially

126
00:10:07.840 --> 00:10:14.320
these two types of incorrect predictions a false positive and a false negative a false positive would

127
00:10:14.320 --> 00:10:19.720
be if the person doesn't have the disease and you predict that they do have it that's a false positive

128
00:10:20.080 --> 00:10:24.760
because you're falsely saying that they are positive for this disease or this condition a false negative

129
00:10:24.760 --> 00:10:30.760
is essentially the opposite of that where this person does have the disease present and then you report

130
00:10:30.760 --> 00:10:35.680
back using your model and your test that it's actually negative they don't actually have this disease

131
00:10:36.040 --> 00:10:42.650
and these are also sometimes called and statistics a type 1 error and a type 2 error and then off of

132
00:10:42.650 --> 00:10:47.350
these essentially true positive true negatives false negatives and false positives results.

133
00:10:47.530 --> 00:10:52.400
There's a lot of other metrics you can calculate so we can see some of them here such as on the bottom

134
00:10:52.400 --> 00:10:57.290
left corner we can see the actual calculation for accuracy it's the sum of true positives plus true

135
00:10:57.290 --> 00:11:02.510
negatives essentially the sum of what you got correct over the total population and there's a bunch

136
00:11:02.510 --> 00:11:08.300
of other ones like positive likelihood ratio false positive rate true positive rate prevalence false

137
00:11:08.300 --> 00:11:09.660
discovery rate etc

138
00:11:12.530 --> 00:11:17.330
now the main point to remember here is that the confusion matrix and the various calculated metrics

139
00:11:17.480 --> 00:11:22.940
is essentially they're all fundamentally ways of comparing the predicted value versus the true values

140
00:11:23.390 --> 00:11:28.330
and what constitutes good metrics actually really depends on the specific situation.

141
00:11:28.430 --> 00:11:32.840
A really common question I get from students is hey is this a good enough accuracy.

142
00:11:32.990 --> 00:11:38.090
Well that really depends on your situation and the context of the situation.

143
00:11:38.180 --> 00:11:40.990
Now if you're still confusing the confusion matrix it's no problem.

144
00:11:41.000 --> 00:11:42.410
Check out the wikipedia page for it.

145
00:11:42.740 --> 00:11:47.620
It has a really good diagram with all the foremost for all the metrics and throughout the training then

146
00:11:47.630 --> 00:11:52.310
this course were usually just going to print out metrics such as printing out basic accuracy.

147
00:11:52.340 --> 00:11:56.720
Now before we finish off this lecture I want to go back to that initial question I mentioned that students

148
00:11:56.720 --> 00:12:01.240
ask me all the time which is this idea of what is a good enough accuracy.

149
00:12:01.820 --> 00:12:07.010
And as always this really depends on the context of the situation you're running your model in.

150
00:12:07.160 --> 00:12:08.870
There's no single number I can give.

151
00:12:08.870 --> 00:12:13.320
To say something like 90 percent accuracy is good enough for every situation.

152
00:12:13.490 --> 00:12:18.350
We already saw the issue with unbalanced classes but there's also not really good remark I can give

153
00:12:18.350 --> 00:12:24.320
for what is a good enough precision or recall and we have to think of the context of the situation.

154
00:12:24.320 --> 00:12:29.950
So let's think back on maybe that model that is diagnostic for predicting the presence of a disease.

155
00:12:30.020 --> 00:12:34.310
So if we created a model to predict the presence of a disease first thing want to think about is are

156
00:12:34.310 --> 00:12:37.380
we going to have balance classes or unbalanced classes.

157
00:12:37.460 --> 00:12:42.830
So is the disease presence well-balanced in the general population and in pretty much most cases it's

158
00:12:42.830 --> 00:12:43.700
probably not.

159
00:12:43.700 --> 00:12:48.050
There's probably not a disease that's going to happen where about half the population is affected and

160
00:12:48.050 --> 00:12:50.120
half the population is not affected.

161
00:12:50.540 --> 00:12:54.470
So we can already tell we're gonna have an unbalanced class situation which means we're gonna have a

162
00:12:54.470 --> 00:13:02.360
precision recall tradeoff so we should also keep in mind that often these models are used as quick diagnostic

163
00:13:02.360 --> 00:13:06.010
tests to have before having a more invasive test.

164
00:13:06.170 --> 00:13:12.920
For example for prostate cancer models it's really common to just do a simple urine test before getting

165
00:13:12.920 --> 00:13:18.740
a biopsy because it's much easier to just get your urine tested than going through a full biopsy on

166
00:13:18.740 --> 00:13:19.880
the prostate.

167
00:13:19.880 --> 00:13:23.510
So we also need to consider then what is at stake if it's prostate cancer.

168
00:13:23.510 --> 00:13:25.500
The stakes are actually quite high.

169
00:13:25.640 --> 00:13:31.280
So as we mentioned we often have that precision recall tradeoff which means we essentially need to decide

170
00:13:31.610 --> 00:13:35.670
if the model should focus on fixing false positives versus false negatives.

171
00:13:35.690 --> 00:13:39.700
So at the cost of decreasing false negatives.

172
00:13:39.800 --> 00:13:43.110
Most likely we increase false positives and vice versa.

173
00:13:43.310 --> 00:13:49.550
And in disease diagnosis it's probably better to try to minimize the number of false negatives at the

174
00:13:49.550 --> 00:13:52.670
cost of increasing the number of false positives.

175
00:13:52.760 --> 00:13:55.030
So it goes in the direction of these false positives.

176
00:13:55.100 --> 00:13:56.780
And why do we actually want to do that.

177
00:13:56.780 --> 00:14:01.730
Well we want to make sure we correctly classify as many cases of the disease as possible.

178
00:14:01.850 --> 00:14:08.120
So at the cost of increasing false positives we try our best to decrease false negatives.

179
00:14:08.120 --> 00:14:12.270
In this context of disease diagnosis and why would we actually want to do that.

180
00:14:12.290 --> 00:14:18.350
Well we want to make sure that anyone that actually has some sort of presence of this disease or in

181
00:14:18.350 --> 00:14:20.410
this case of this example prostate cancer.

182
00:14:20.420 --> 00:14:24.650
If we're doing that urine test we wanna make sure that all those people to have the presence of the

183
00:14:24.650 --> 00:14:27.630
disease will actually go through to the next step.

184
00:14:27.740 --> 00:14:30.650
That is maybe a more invasive tests like a biopsy.

185
00:14:30.650 --> 00:14:35.300
And remember this kind of usually comes at a cost of increasing those false positives.

186
00:14:35.330 --> 00:14:39.090
So our models aren't gonna be perfect and eventually it's going to happen as someone who doesn't have

187
00:14:39.090 --> 00:14:43.940
the disease when we do our simple urine test or whatever our machine learning model is it's going to

188
00:14:44.060 --> 00:14:49.730
predict back that hey I'm sorry but you have the positive presence of this disease and it will be an

189
00:14:49.730 --> 00:14:51.980
error because it will be a false positive.

190
00:14:51.980 --> 00:14:57.560
However then when they get then more invasive tests then they'll realize Oh sorry it was a false positive.

191
00:14:57.650 --> 00:14:59.020
You're actually OK.

192
00:14:59.090 --> 00:15:04.640
You don't have this disease and that comes because we're trying to minimize the false negatives.

193
00:15:04.640 --> 00:15:10.310
What we really don't want when it comes as something as important as having cancer or having a disease

194
00:15:10.640 --> 00:15:16.290
we really don't want to turn someone away who actually has a disease and tell them they don't have the

195
00:15:16.280 --> 00:15:16.730
disease.

196
00:15:16.730 --> 00:15:19.640
So be a false negative when it comes to disease diagnosis.

197
00:15:19.640 --> 00:15:21.560
That's something we really want to minimize.

198
00:15:21.570 --> 00:15:26.150
We want to minimize those false negatives so we don't accidentally miss someone right off the bat with

199
00:15:26.180 --> 00:15:31.010
our diagnostic tool and tell them oh you don't have the disease or you don't have cancer when they actually

200
00:15:31.010 --> 00:15:31.810
do have it.

201
00:15:31.910 --> 00:15:37.280
So we can see for something as important as disease diagnosis really minimizing those false negatives

202
00:15:37.310 --> 00:15:43.790
becomes really important and pretty much all of this is to say machine learning is not going to be performed

203
00:15:43.970 --> 00:15:44.810
in a vacuum.

204
00:15:44.990 --> 00:15:50.810
And these performance metrics don't have some sort of universal truth that is true across all problems

205
00:15:51.230 --> 00:15:56.220
but machine learning is that it's a really collaborative process where we should always be consulting

206
00:15:56.220 --> 00:15:58.070
of experts in the domain.

207
00:15:58.070 --> 00:16:03.350
For example in the case of disease diagnosis we should probably be talking to metal cola doctors as

208
00:16:03.350 --> 00:16:07.920
far as what our acceptable standards for false negatives versus false positives.

209
00:16:08.030 --> 00:16:14.180
But as a general presence of this disease in the general population et cetera and this is going to change

210
00:16:14.180 --> 00:16:17.900
depending on the context of your situation so always keep that in mind.

211
00:16:17.930 --> 00:16:22.100
There's not gonna be an easy answer for me to say is this a good enough accuracy precision or recall

212
00:16:22.370 --> 00:16:26.540
because it all depends on the context and the domain of the situation.

213
00:16:26.660 --> 00:16:30.530
Okay so we just talked about how we can evaluate classification models.

214
00:16:30.530 --> 00:16:35.980
Let's go ahead and move on to the next lecture where we discuss evaluating regression tasks.

215
00:16:35.990 --> 00:16:36.470
I'll see you there.
