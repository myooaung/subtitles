1
00:00:05,560 --> 00:00:10,350
Welcome everyone to the final part of our next generation project in this part.

2
00:00:10,360 --> 00:00:14,570
We're going to load up the model but we'll only load the weights that way when we build the model.

3
00:00:14,620 --> 00:00:19,660
We can actually build it to accept a batch size of one and then we'll create a function that can generate

4
00:00:19,660 --> 00:00:20,170
text.

5
00:00:20,170 --> 00:00:24,370
Given the model let's get started and continue where we left off last time.

6
00:00:24,430 --> 00:00:24,840
All right.

7
00:00:24,850 --> 00:00:30,820
In order to actually load up our model or load up your model that you just trained First off make sure

8
00:00:30,820 --> 00:00:32,230
you've saved your model.

9
00:00:32,260 --> 00:00:38,170
So if you decided to actually train your own model go ahead and save it and then the next step to load

10
00:00:38,170 --> 00:00:38,440
it up.

11
00:00:38,440 --> 00:00:45,340
So has a batch size of one create a new model we'll say create model and then pass in everything we

12
00:00:45,340 --> 00:00:54,990
pass them before vocab size in bed size as in bed dimensions and then Ana neurons but finally for batch

13
00:00:54,990 --> 00:00:58,260
size say batch size is equal to one.

14
00:00:58,410 --> 00:01:05,650
So you'll create a model of batch size equal to one and then same model load weights and then pass in

15
00:01:06,450 --> 00:01:14,530
the actual file that you saved or our file which was Shakespeare underscored again that H5.

16
00:01:15,150 --> 00:01:21,110
And then after that we'll say model build and we'll build it passing in the input shape and the input

17
00:01:21,110 --> 00:01:33,840
shape stems from TAF tensor shape and then we'll say one by none so running these three lines of code

18
00:01:33,840 --> 00:01:40,170
here of creating a new instance of the model or batch size equal to 1 and instead of actually loading

19
00:01:40,170 --> 00:01:47,010
up the full model we will say load weights and then we will build out the model with that particular

20
00:01:47,010 --> 00:01:48,000
shape size.

21
00:01:48,010 --> 00:01:55,420
Okay so now when I say model summary same models before but notice instead of 120 as the batch size

22
00:01:55,600 --> 00:01:57,630
it expects just one.

23
00:01:57,640 --> 00:02:01,010
So that means I can create a function that will be able to generate text.

24
00:02:01,060 --> 00:02:07,450
So the way it's going to work is will say these f generate TEXT function it's going to take in the model

25
00:02:07,930 --> 00:02:14,050
it's going to take in some starting seed and then it will take a Jen size argument essentially how many

26
00:02:14,050 --> 00:02:15,570
characters you want to generate.

27
00:02:15,580 --> 00:02:18,000
Go ahead and generate 500 characters by default.

28
00:02:18,310 --> 00:02:20,610
And then we'll also specify a temperature argument.

29
00:02:20,680 --> 00:02:21,860
They can play around with.

30
00:02:21,940 --> 00:02:24,670
We'll set it to a default of one point zero.

31
00:02:24,670 --> 00:02:29,170
So the basic idea behind this function is to take in some seed text format it so that it's in the correct

32
00:02:29,170 --> 00:02:34,480
shape for a network and then loop the sequence as we keep adding our own predicted characters very similar

33
00:02:34,480 --> 00:02:38,240
to the work we did in our time series problem but for current year on networks.

34
00:02:38,410 --> 00:02:46,270
So we'll do here it will say the number to generate is equal to the size this is the number of characters

35
00:02:46,270 --> 00:02:50,110
that generate and then we're gonna start by vector rising the starting seed text.

36
00:02:50,110 --> 00:02:53,950
So the start seed will actually be a rostering that the user can pass then.

37
00:02:54,190 --> 00:03:04,410
So in order to do the actual evaluation we'll say input evaluation is equal to character to index s

38
00:03:04,530 --> 00:03:06,830
for s in start seed.

39
00:03:06,930 --> 00:03:11,160
Essentially for every character will go ahead and transform it to an index and then we'll have a list

40
00:03:11,160 --> 00:03:16,320
of those and then we want to expand this to match the batch format shape.

41
00:03:16,560 --> 00:03:26,680
So say INPUT eval is equal to T F expand dimensions we will say INPUT eval

42
00:03:30,850 --> 00:03:36,970
comma zero so it expands the dimensions so that it's in the correct shape and then we're gonna create

43
00:03:36,970 --> 00:03:44,440
finally an empty list called text generated which is going to hold our resulting generated text.

44
00:03:44,440 --> 00:03:50,350
Then finally we can specify a temperature so we can say temperature is equal to temp and the temperature

45
00:03:50,350 --> 00:03:52,150
is of value can play around with.

46
00:03:52,150 --> 00:03:58,960
You can make it lower or higher depending on basically how random or more surprising you want the results

47
00:03:58,960 --> 00:03:59,440
to be.

48
00:03:59,440 --> 00:04:03,910
So we'll show you in a second in our loop how that actually comes into play when generating the predictions

49
00:04:04,770 --> 00:04:07,770
for right now we'll go ahead and reset the state to the model.

50
00:04:07,780 --> 00:04:10,980
So same model reset states

51
00:04:15,510 --> 00:04:20,890
and then we're going to do here is say for i in range the number to generate

52
00:04:23,640 --> 00:04:25,440
will first generate some predictions.

53
00:04:25,440 --> 00:04:34,330
So we'll say predictions is equal to model and pass in that input excel that we just created and then

54
00:04:34,330 --> 00:04:40,450
we'll say predictions and we'll remove the batch shape I mentioned we can do this by simply saying TAF

55
00:04:41,380 --> 00:04:49,270
squeeze predictions zero it's almost essentially reversing the TAF shaping we used earlier that is to

56
00:04:49,270 --> 00:04:54,850
say this to expand dimensions so the opposite of squeezing them and then we want to use a categorical

57
00:04:54,940 --> 00:04:57,400
distribution to select the next character.

58
00:04:57,490 --> 00:04:59,980
So make sure we spell predictions right here.

59
00:05:01,320 --> 00:05:02,180
Perfect.

60
00:05:02,280 --> 00:05:15,040
Then we'll say offer this predictions is equal to predictions divided by temperature and then we'll

61
00:05:15,040 --> 00:05:20,180
say the predicted index or predicted I.D. is equal to just as we did before.

62
00:05:20,200 --> 00:05:32,110
TMF random the categorical predictions we only need one of these characters will say samples as one.

63
00:05:32,300 --> 00:05:36,800
And then finally because of the format does return then we'll do a little bit of indexing by saying

64
00:05:36,830 --> 00:05:45,600
negative 1 0 2 num pi is essentially just indexing formatting so we get the actual single idea which

65
00:05:45,600 --> 00:05:52,440
means then our input eval will say TMF will expand the dimensions back out so we'll see expand dimensions

66
00:05:53,680 --> 00:06:02,020
notice the braces there to predicted Ida comma zero and then we'll transform this back to the character

67
00:06:02,020 --> 00:06:04,300
letter by saying Tex generated

68
00:06:07,230 --> 00:06:14,770
that append index to character of the predicted Ida

69
00:06:18,410 --> 00:06:26,690
once that's all done we'll go ahead and make sure you check your indentation here will return start

70
00:06:26,690 --> 00:06:36,530
seed plus and then we'll join all the items together we'll say join the tax generated list all right.

71
00:06:37,290 --> 00:06:39,530
So let's go ahead and make sure we understand this.

72
00:06:39,570 --> 00:06:44,910
We take in the model the starting seed strain the number of characters we want to generate as well as

73
00:06:44,910 --> 00:06:50,490
the temperature then we assign some variables here number to generate as well as the temperature and

74
00:06:50,490 --> 00:06:57,270
then we're going to get our input evaluation essentially transform that string back into the index locations

75
00:06:57,330 --> 00:06:58,370
we'll expand this dimension.

76
00:06:58,380 --> 00:07:04,560
So it's in the right shape for our model we'll reset the states and then for Iron Range number generators

77
00:07:04,560 --> 00:07:10,260
or for each of these characters we'll get a prediction we'll do squeezing essentially undo this expand

78
00:07:10,260 --> 00:07:13,850
dimensions in order to actually grab the predictions.

79
00:07:13,980 --> 00:07:18,000
These are what these two lines are doing we can affect the actual probability those distributions based

80
00:07:18,000 --> 00:07:21,120
off a temperature you can play around of that value one point zero.

81
00:07:21,120 --> 00:07:26,070
Essentially the base essentially no adjustment since dividing by 1 is the same as leaving at the same

82
00:07:26,890 --> 00:07:31,620
we get predicted I.D. to random categorical it's actually grabbing it a little bit of indexing here

83
00:07:31,740 --> 00:07:37,620
it's converted to some PI so we get that single digit expand that back out so you can get the input

84
00:07:37,620 --> 00:07:42,780
eval shift it forward by your predictive text and then we get this text generated and we start appending

85
00:07:42,780 --> 00:07:42,900
it.

86
00:07:43,230 --> 00:07:48,510
Okay so let's run this make sure we then commit any typos and we'll go ahead and say print generate

87
00:07:48,510 --> 00:07:57,600
text pass in our model go ahead and pass in anything that seems like it should belong in a play by William

88
00:07:57,600 --> 00:08:06,820
Shakespeare such as Juliet in all caps and then let's go ahead and say Our Jen size is equal to 1000

89
00:08:06,900 --> 00:08:11,820
characters run that and this should take a little bit of time but eventually you should see something

90
00:08:11,820 --> 00:08:21,460
that hopefully looks Shakespearean in its output OK so here is what it came up with you can see Juliet

91
00:08:21,820 --> 00:08:26,350
and then excellent start going in all caps but again it's doing this character by character they can

92
00:08:26,350 --> 00:08:31,720
see King Richard how now mistress does the Mona help me pluck over the proud.

93
00:08:31,720 --> 00:08:38,500
No doubt us and you can see some of these are actually just kind of made up words but other ones actually

94
00:08:38,500 --> 00:08:39,630
make a lot of sense.

95
00:08:39,760 --> 00:08:44,980
So you notice it's doing a couple of typos here but on a character by character basis it's pretty amazing

96
00:08:45,310 --> 00:08:51,400
that it's able to start labeling scenes and after a scene as label it tells what characters are entering

97
00:08:51,520 --> 00:08:56,740
and then it really learned the structure of you have some character name in all caps and then what they

98
00:08:56,740 --> 00:09:00,670
say then another character name in all caps and then what they say et cetera.

99
00:09:00,730 --> 00:09:05,920
So pretty amazing work that just on a character by character basis is able to learn the structure of

100
00:09:05,920 --> 00:09:06,900
Shakespeare.

101
00:09:06,900 --> 00:09:12,790
OK definitely check out the rest of the resource links for more examples of this sort of recurrent neural

102
00:09:12,790 --> 00:09:15,820
network architecture being used to generate results.

103
00:09:15,820 --> 00:09:17,460
Thanks and we'll see you at the next section.
