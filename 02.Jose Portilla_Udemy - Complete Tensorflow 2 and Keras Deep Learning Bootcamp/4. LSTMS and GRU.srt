1
00:00:05,190 --> 00:00:06,360
Welcome back everyone.

2
00:00:06,360 --> 00:00:12,900
In this lecture we're going to discuss Ellis T.M. and G are you units many of the solutions previously

3
00:00:12,900 --> 00:00:18,000
presented for vanishing gradients can also apply to recurrent neural networks or the recurrent neuron

4
00:00:18,480 --> 00:00:23,460
that is things like using different activation functions such as rectified linear units performing batch

5
00:00:23,460 --> 00:00:30,570
normalization etc. However because of the typical length of a time series input especially if the time

6
00:00:30,570 --> 00:00:35,250
series happens to be really long what could happen is using those different activation functions or

7
00:00:35,250 --> 00:00:41,080
some of those other solutions that could actually slow down training time now a possible solution to

8
00:00:41,080 --> 00:00:44,880
that would just be to shorten the actual time step they're using for prediction.

9
00:00:44,950 --> 00:00:51,220
So maybe instead of looking back twenty four time steps for the 25th prediction you just look back three

10
00:00:51,220 --> 00:00:53,530
time steps to get the next prediction.

11
00:00:53,650 --> 00:00:56,990
However this makes the model worse at predicting longer trends.

12
00:00:57,070 --> 00:01:03,340
So we still want to be able to use a long time sequence in order to predict the next item in the sequence

13
00:01:03,370 --> 00:01:06,350
or element in the sequence.

14
00:01:06,360 --> 00:01:11,070
Now another issue that recurrent neural networks face in general is that after a while the network will

15
00:01:11,070 --> 00:01:16,470
begin to forget those first inputs as information is lost at each step going through the recurrent neural

16
00:01:16,470 --> 00:01:17,260
network.

17
00:01:17,320 --> 00:01:22,630
Well we need to do is to have some sort of long term memory for our networks.

18
00:01:22,720 --> 00:01:27,850
So as you can imagine this were the Ellis team memory cell or neuron comes into play and Elysium stands

19
00:01:27,850 --> 00:01:29,940
for Long Short term memory.

20
00:01:30,070 --> 00:01:34,450
So we're going to have both long term and short term memory going in and out of that cell.

21
00:01:34,600 --> 00:01:39,040
And this was created to help address these typical Recurrent Neural Networks issues.

22
00:01:39,040 --> 00:01:42,670
So let's go ahead and get an understanding of how an Ellis stem cell works.

23
00:01:42,670 --> 00:01:48,660
But first let's quickly review what's happening in a normal recurrent neuron so in a typical recurrent

24
00:01:48,720 --> 00:01:53,790
neural network where you don't have any specialized cell types you're taking in the input at some timestamp

25
00:01:54,150 --> 00:01:58,580
and then along with the output of that timestamp you're feeding that back in the next timestamp.

26
00:01:59,010 --> 00:02:02,740
So your input a T minus one produces an output a T minus one.

27
00:02:02,850 --> 00:02:06,800
You take that output and feed it back into the next time step along with the input.

28
00:02:06,810 --> 00:02:08,160
Then at time T.

29
00:02:08,160 --> 00:02:09,710
So essentially they have two inputs.

30
00:02:09,780 --> 00:02:16,300
You create a single output the pass in that same output into the next timestamp and so on so if we just

31
00:02:16,300 --> 00:02:21,790
take a look at what a single recurrent neuron would actually be doing is essentially it's taking in

32
00:02:22,060 --> 00:02:28,030
both the previous output and then the current input and then producing the next output.

33
00:02:28,150 --> 00:02:32,980
And what we'll do in order to label these equations is instead of saying output or input we'll simply

34
00:02:32,980 --> 00:02:37,720
refer to these as hidden state and then our current feature X going in.

35
00:02:37,750 --> 00:02:45,070
So basically we have the H of T minus one going in along with X of T and that produces h of T.

36
00:02:45,070 --> 00:02:48,580
And then what is actually happening inside of the neuron.

37
00:02:48,580 --> 00:02:51,430
So in a standard occurrence you're a network.

38
00:02:51,430 --> 00:02:56,710
Essentially what we do is we just have a single hyperbolic tangent function and then what we're doing

39
00:02:56,710 --> 00:03:02,680
is combining h of T minus 1 along of expertise multiplying it by some weight matrix and then adding

40
00:03:02,680 --> 00:03:07,600
a bias to it and then passing it through the hyperbolic tangent function and that gives us back our

41
00:03:07,660 --> 00:03:08,560
H of T.

42
00:03:08,560 --> 00:03:15,330
And then we just repeat that through the next recurrent neuron or an extra current layer so it Ellis

43
00:03:15,330 --> 00:03:18,000
team will also have this chain like structure.

44
00:03:18,180 --> 00:03:23,940
But the repeating module has a slight difference to it and instead of just having a single neural network

45
00:03:23,940 --> 00:03:29,610
layer there's actually going to be four layers working and interacting in a special way and the way

46
00:03:29,610 --> 00:03:35,250
we end up getting for is the fact that not only will we keep track of just a single historical memory

47
00:03:35,250 --> 00:03:41,340
with h of T minus one we're keeping track of both the long term memory input and the short term memory

48
00:03:41,370 --> 00:03:47,160
input and then creating a new long term memory output and then use short term memory output along with

49
00:03:47,160 --> 00:03:49,190
the current input at time T.

50
00:03:49,260 --> 00:03:51,540
And then we produce an output at time T.

51
00:03:51,840 --> 00:03:56,070
So don't worry too much about these details right now or we're going to be doing is walking through

52
00:03:56,070 --> 00:03:58,350
the Ellis Tam step by step.

53
00:03:58,350 --> 00:04:03,680
But right now let's just get comfortable with some of the notation we're going to be using so essentially

54
00:04:03,710 --> 00:04:06,780
we're going to have forming components inside the Ellis team.

55
00:04:06,920 --> 00:04:12,290
We have a forget gate and output gate and update gate and an input gate.

56
00:04:12,290 --> 00:04:17,240
So as you can imagine and we'll go over this in more detail later on but the forget gate will decide

57
00:04:17,270 --> 00:04:24,590
what to forget from the previous memory units the input gate will decide what to actually accept into

58
00:04:24,590 --> 00:04:30,200
the neuron and the update gate will update the memories and the output gate will actually output the

59
00:04:30,200 --> 00:04:34,050
new long term memory so a gate.

60
00:04:34,100 --> 00:04:38,220
Just to clarify this optionally lets some information through.

61
00:04:38,390 --> 00:04:42,250
And essentially we can just think of this as mathematically it's a sigmoid function.

62
00:04:42,440 --> 00:04:47,840
It's either going to end up being a squeeze between a 0 or 1 and if it's 0 we don't let that information

63
00:04:47,840 --> 00:04:48,190
through.

64
00:04:48,200 --> 00:04:51,800
And if it's a one we do okay.

65
00:04:51,930 --> 00:04:57,450
So again here's our general structure of the Elysium and we're accepting both long term memory and short

66
00:04:57,450 --> 00:05:02,910
term memory and we can kind of think of this as going to be passed in through conveyor belts inside

67
00:05:02,910 --> 00:05:03,830
of this neuron.

68
00:05:03,930 --> 00:05:09,150
And what ends up happening is it just ends up kind of running down straight the entire chain and has

69
00:05:09,150 --> 00:05:13,740
some kind of linear interactions with a few functions inside of the cell.

70
00:05:13,770 --> 00:05:19,620
Now for the purpose of a mathematical notation we're gonna relabel some of these and we're going to

71
00:05:19,620 --> 00:05:20,640
label it as such.

72
00:05:20,640 --> 00:05:26,740
We're gonna say we have on the bottom there an X of teh input leading out to your h of T.

73
00:05:26,880 --> 00:05:29,140
And notice then we have that short term memory.

74
00:05:29,190 --> 00:05:31,350
So we're called a short term memory is on the bottom here.

75
00:05:31,410 --> 00:05:34,800
So we're going to call that H of T minus one and each of T.

76
00:05:34,800 --> 00:05:40,620
And we're also going to not just have that hidden state but also C of T minus 1 and c of T which is

77
00:05:40,620 --> 00:05:44,440
going to refer essentially to our long term memory.

78
00:05:44,730 --> 00:05:50,140
OK so what are the actual linear interactions and functions going on inside of Elysium.

79
00:05:50,140 --> 00:05:52,750
Well here we can see the entire Elysium cell.

80
00:05:52,750 --> 00:05:56,620
So we're going to do is essentially go through the process step by step.

81
00:05:56,620 --> 00:05:56,940
All right.

82
00:05:56,950 --> 00:06:02,020
The first step in our LSD team is to decide what information are we going to throw away from the cell

83
00:06:02,020 --> 00:06:03,000
state essentially.

84
00:06:03,310 --> 00:06:05,340
What are you going to forget.

85
00:06:05,410 --> 00:06:09,920
So we end up creating is a forget gate layer or F of T.

86
00:06:10,060 --> 00:06:15,550
And remember those gates are essentially just passing things through a sigmoid function where the closer

87
00:06:15,550 --> 00:06:16,600
it is to 0.

88
00:06:16,630 --> 00:06:19,600
That means the less weight you're giving it the closer it is to 1.

89
00:06:19,600 --> 00:06:21,070
That means the more weight you're giving it.

90
00:06:21,280 --> 00:06:25,740
So in the context of a forget gate layer the closer it is to 0.

91
00:06:25,870 --> 00:06:27,970
Essentially means forget about this and get rid of it.

92
00:06:28,090 --> 00:06:31,540
The closer it is to 1 on this output means completely remember this.

93
00:06:31,570 --> 00:06:33,810
It's probably important that we remember this.

94
00:06:33,820 --> 00:06:36,550
So this is what this f of T is doing.

95
00:06:36,550 --> 00:06:42,370
And notice that it's essentially a linear combination of H of T minus one that previous hidden state

96
00:06:42,640 --> 00:06:45,250
combined with the input x of T.

97
00:06:45,250 --> 00:06:50,650
And then we have our own sets of weights for this forget gate layer plus a bias and then we pass it

98
00:06:50,650 --> 00:06:58,290
through an activation function and then we get F of t then the next step after this is to decide what

99
00:06:58,290 --> 00:07:02,520
new information are we going to store into the cell state.

100
00:07:02,520 --> 00:07:04,410
So this has two parts to it.

101
00:07:04,410 --> 00:07:06,420
First we can see here on top.

102
00:07:06,480 --> 00:07:11,730
We have a sigmoid layer that we're going to label the input gate layer which is essentially going to

103
00:07:11,740 --> 00:07:15,630
decide what values are we going to update next.

104
00:07:15,720 --> 00:07:21,820
We have a hyperbolic tangent layer that creates a vector of new candidate values which will say C of

105
00:07:21,840 --> 00:07:25,320
T but we'll label this with a tilde on top of it.

106
00:07:25,390 --> 00:07:31,050
Essentially say hey these are the new candidate values that will eventually hopefully in some sort of

107
00:07:31,050 --> 00:07:34,490
weighing be updating the cell state.

108
00:07:34,590 --> 00:07:39,660
So we have that input gate layer again deciding which values we're going to update in the hyperbolic

109
00:07:39,660 --> 00:07:43,250
tangent layer which is creating a vector of those new candidate values.

110
00:07:43,260 --> 00:07:48,600
Notice again there are essentially as linear combinations of exit T U of H of T minus one.

111
00:07:48,630 --> 00:07:52,770
And then we have them being multiplied by their own set of weights and we add a bias to it and we're

112
00:07:52,770 --> 00:08:01,700
passing them through activation functions then it's now time to update the old cell state which is C

113
00:08:01,700 --> 00:08:03,280
of team minus 1.

114
00:08:03,290 --> 00:08:09,200
So what we do is in order to calculate the new cell state C of t that we're going to end up outputting

115
00:08:09,710 --> 00:08:19,220
we have C A T is equal to f of T multiplied by C of T minus one and then we're going to add I have t

116
00:08:19,520 --> 00:08:21,500
times those new candidate values.

117
00:08:21,530 --> 00:08:27,650
So essentially what we're doing is we just multiply the old state by 50 forgetting the things that we

118
00:08:27,650 --> 00:08:31,610
decided weren't that important due to the forget gate layer.

119
00:08:31,610 --> 00:08:35,700
Then we added I have t times the new candidate values.

120
00:08:35,720 --> 00:08:41,390
So essentially these are the new candidate values for the cell state scaled by how much we decided to

121
00:08:41,450 --> 00:08:43,030
update each state value.

122
00:08:43,790 --> 00:08:51,000
So then finally after this we need to decide well what are we actually going to output so this output

123
00:08:51,000 --> 00:08:53,040
will be based on our cell state.

124
00:08:53,050 --> 00:08:56,140
That is actually a filtered version first.

125
00:08:56,220 --> 00:09:02,430
We end up doing is we run a sigmoid layer so that's that top equation which decides what parts of the

126
00:09:02,430 --> 00:09:04,980
cell state we're going to output.

127
00:09:04,980 --> 00:09:10,860
Then we put that cell state through a hyperbolic tangent function and what the hyperbolic tangent function

128
00:09:10,860 --> 00:09:16,710
does is it pushes all the values to be between negative 1 and 1 and then we're gonna multiply it by

129
00:09:16,710 --> 00:09:24,010
the output of that initial sigmoid gate of o of T so that we only output the parts that we decided to.

130
00:09:24,150 --> 00:09:30,300
Now what I've been describing so far is essentially just your standard normal LSD m but there are variations

131
00:09:30,300 --> 00:09:37,480
of this which I'll briefly mention there is the peephole variation where essentially what we end up

132
00:09:37,480 --> 00:09:44,590
doing is we have these people connections for added here as red lines and this means we let the multiple

133
00:09:44,590 --> 00:09:49,330
gate layers the F of T I have t and o of t look at these cell state.

134
00:09:49,750 --> 00:09:56,380
So here notice that we're essentially adding in C of T minus 1 or c t into these 3 gates.

135
00:09:56,380 --> 00:10:02,020
Now we previously didn't have that we only had H T minus one an equity but in some variations of Elysium

136
00:10:02,180 --> 00:10:10,570
they add in peoples which essentially also linearly combines the cell state inside of the LSD M in other

137
00:10:10,570 --> 00:10:14,060
variation off of this is the gated recurrent units.

138
00:10:14,140 --> 00:10:20,560
So what the gate it recurrent unit is is essentially it's going to combine the forget an input gates

139
00:10:20,830 --> 00:10:26,350
into what's known as a single update gate and it also merges the cell state and hidden state and makes

140
00:10:26,410 --> 00:10:27,880
some other changes.

141
00:10:27,880 --> 00:10:34,460
So what ends up happening is you have this model that's simpler than a standard LSD model and the panel

142
00:10:34,530 --> 00:10:35,440
applications.

143
00:10:35,440 --> 00:10:41,470
It's getting kind of popular but again the standard has really become Alice Tam and then get a recurrent

144
00:10:41,470 --> 00:10:47,920
unit is slowly picking up popularity but by the time you're seeing this who knows what the current cases

145
00:10:47,950 --> 00:10:50,620
because there's a field that moves very very fast.

146
00:10:50,620 --> 00:10:57,190
So if you want you can check out the links I have for this lecture to see a kind of in-depth explanation

147
00:10:57,580 --> 00:11:04,100
over which variants are the best and how their differences actually matter and when to use certain units.

148
00:11:04,120 --> 00:11:09,960
Okay now fortunately with our deep learning python library we simply just need to call the import for

149
00:11:09,960 --> 00:11:15,510
recurrent neural network or Ellis dam instead of needing to code all of this calculations out ourselves.

150
00:11:15,520 --> 00:11:20,080
So finally what we're going to do is let's go ahead and explore how do you use LSD Ms for deep learning

151
00:11:20,080 --> 00:11:21,380
with Python code.

152
00:11:21,460 --> 00:11:22,480
I'll see you at the next lecture.
