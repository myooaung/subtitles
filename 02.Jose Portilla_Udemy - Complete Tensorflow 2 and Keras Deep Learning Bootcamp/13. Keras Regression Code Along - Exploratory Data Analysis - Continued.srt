1
00:00:04,220 --> 00:00:10,170
OK now let's go ahead and start our feature engineering process as well as getting rid of features that

2
00:00:10,170 --> 00:00:12,090
are not going to be useful to us.

3
00:00:12,150 --> 00:00:16,740
So we're going to start working with this feature data and I always like to kind of review the data

4
00:00:16,740 --> 00:00:17,820
frame itself.

5
00:00:18,030 --> 00:00:22,710
So something we can drop right away is this idea this idea is some sort of unique idea.

6
00:00:22,710 --> 00:00:23,980
There's no trend to it.

7
00:00:24,090 --> 00:00:28,290
And if there is we probably don't really understand that based off this number itself.

8
00:00:28,710 --> 00:00:35,950
So I will just say DLF is equal to DFT drop and then go ahead and drop that idea column and that's along

9
00:00:36,030 --> 00:00:37,350
axis is equal to 1.

10
00:00:38,430 --> 00:00:44,620
So we go ahead and just drop that idea column and the next thing you're going to do is check out the

11
00:00:44,620 --> 00:00:46,210
date column.

12
00:00:46,210 --> 00:00:52,020
So if we take a look at this date column right now it looks to be just some sort of string.

13
00:00:52,270 --> 00:00:56,770
So we can convert it to a date time object by doing the following.

14
00:00:56,890 --> 00:01:05,600
I can say the FDA is equal to PD to underscore date time and pass in the FDA.

15
00:01:05,620 --> 00:01:12,640
And what this does is it's going to automatically convert this string to be a date time object and once

16
00:01:12,700 --> 00:01:18,730
it's a date time object that means I can extract information such as the month or year automatically.

17
00:01:19,480 --> 00:01:21,050
So I go ahead and run this.

18
00:01:21,180 --> 00:01:27,370
And now if I take a look at my date column note the formatting is different and it now reports back

19
00:01:27,400 --> 00:01:33,790
that it's a date time object which means I can begin to perform feature engineering off of this this

20
00:01:33,790 --> 00:01:40,000
date time by itself may not be useful but I can extract the year component and the month component and

21
00:01:40,000 --> 00:01:48,380
the way I can do that is through a couple of simple calls I can say the F year is equal to def date

22
00:01:49,470 --> 00:01:55,350
and I can apply a function that actually extracts the year from this date time.

23
00:01:55,350 --> 00:02:02,400
So if I have a date time object it's simply an attribute call to grab the year itself and I can do this

24
00:02:02,400 --> 00:02:10,200
with a simple lambda expression by saying lambda date and then say date month if you're unfamiliar with

25
00:02:10,200 --> 00:02:11,310
land expressions.

26
00:02:11,310 --> 00:02:16,470
That's essentially shorthand for a function so to show you what I mean by that this land expression

27
00:02:16,920 --> 00:02:23,280
would be the exact same thing as if I did this or create a function called the air extraction takes

28
00:02:23,280 --> 00:02:28,620
in some date and then it returns back date that year date that month.

29
00:02:28,620 --> 00:02:32,910
And in this case the should actually be a year in order to match up with what I'm calling the column

30
00:02:32,910 --> 00:02:33,830
here.

31
00:02:33,840 --> 00:02:38,550
So this land expression is the exact same thing as running this right here.

32
00:02:39,090 --> 00:02:44,520
So recall that when built in Python you have this date time object and you can grab the attributes of

33
00:02:44,520 --> 00:02:50,490
Year Month et cetera which is why we called PD to date time on the original string column and then this

34
00:02:50,490 --> 00:02:57,080
land expression is essentially shorthand for that and I'm going to copy and paste this for month as

35
00:02:57,080 --> 00:03:05,270
well and this is essentially feature engineering because these features were technically hidden inside

36
00:03:05,270 --> 00:03:11,180
of the string date and now I'm creating new columns to try to extract or engineer more information off

37
00:03:11,180 --> 00:03:12,370
my original features.

38
00:03:12,380 --> 00:03:15,300
This is a very common step especially if timestamps.

39
00:03:15,500 --> 00:03:17,140
So we'll go ahead and run this.

40
00:03:17,570 --> 00:03:24,470
And now if I take a look at the head of my data frame I have this newly formatted Date Time column and

41
00:03:24,470 --> 00:03:29,540
I'll scroll all the way to the right here and I should see my new year and month column and now I can

42
00:03:29,540 --> 00:03:34,610
do some sort of exploratory data analysis to see if these are useful features in general.

43
00:03:34,610 --> 00:03:39,350
So let's go in and see if there's any variation based off the month that this is selling it.

44
00:03:39,500 --> 00:03:45,590
So maybe we price things higher if we believe they're going to go on market in December versus in March.

45
00:03:46,190 --> 00:03:52,430
So I can do that by saying S.A. box plot and let's go ahead and see the distribution per month

46
00:03:55,800 --> 00:04:00,900
and another way I could do this is through grouping by month and then doing it describe on the price

47
00:04:00,900 --> 00:04:03,910
column as well which is essentially what a box spot does visually.

48
00:04:03,910 --> 00:04:06,080
Now let's make this just a little larger.

49
00:04:06,210 --> 00:04:14,150
We'll say Pulte figure fig size make this something like 10 by six.

50
00:04:14,180 --> 00:04:17,500
Go ahead and run this and I can see the distributions here.

51
00:04:17,570 --> 00:04:24,470
Now it's hard to tell just from this plot whether or not there is any significant distribution differences

52
00:04:24,860 --> 00:04:30,920
between the month that you're going to sell this house at and what might be easier to do is just see

53
00:04:30,920 --> 00:04:32,140
the numbers themselves.

54
00:04:32,180 --> 00:04:41,350
So I could do the F group by column month here and then just see what's the average price per month.

55
00:04:42,100 --> 00:04:48,640
So I'll take the mean after I do the group buy and then just explore the price and then here this maybe

56
00:04:48,730 --> 00:04:55,180
will allow me to actually see or read the numbers myself to see if there's some sort of significant

57
00:04:55,180 --> 00:04:58,150
difference between the months and if I want to see this visually.

58
00:04:58,150 --> 00:05:02,090
I could also just call that plot and this would show me the behavior.

59
00:05:02,140 --> 00:05:07,030
So it looks like maybe there's some sort of difference between the months if we actually see the total

60
00:05:07,030 --> 00:05:07,950
y range.

61
00:05:08,020 --> 00:05:13,570
It only goes from 5 million ish to around five and a half million ish.

62
00:05:13,570 --> 00:05:15,850
So not a huge difference here in the price.

63
00:05:15,850 --> 00:05:19,770
Excuse me actually five hundred ten thousand versus five hundred sixty thousand.

64
00:05:19,760 --> 00:05:25,330
So not a huge range but it looks like there's some behavior differences there in the months themselves.

65
00:05:25,330 --> 00:05:27,640
And we could do similar things for the year.

66
00:05:27,700 --> 00:05:33,610
So if I wanted to explore the year this plot definitely makes sense because if you look back at King

67
00:05:33,610 --> 00:05:38,920
County sales they're just increasing in price as time goes on and you expect that just naturally with

68
00:05:38,920 --> 00:05:42,940
inflation unless there's some sort of a major housing event.

69
00:05:42,950 --> 00:05:46,810
OK so we featured engineered year and month we'll go ahead.

70
00:05:46,820 --> 00:05:47,720
Just keep them in there.

71
00:05:47,720 --> 00:05:55,690
See if the model uses them or not and we'll drop out our date column will say EDF is equal to the drop

72
00:05:58,060 --> 00:06:05,930
and we'll drop off that original date column since it's no longer useful to us since we featured engineered.

73
00:06:05,930 --> 00:06:08,930
Now let's take a quick look again at what remaining columns we have.

74
00:06:08,930 --> 00:06:13,660
We can either do this with the F columns or it can simply just check the head of my data frame to begin

75
00:06:13,670 --> 00:06:14,870
exploring this.

76
00:06:14,870 --> 00:06:18,660
A lot of these features actually make sense already the way they are.

77
00:06:18,770 --> 00:06:22,670
The number of bedrooms bathrooms square foot living doesn't look like we have to do a lot of feature

78
00:06:22,670 --> 00:06:26,960
engineering here and even the waterfront and view type categories.

79
00:06:26,960 --> 00:06:29,570
Those are already in dummy variable form for us.

80
00:06:29,570 --> 00:06:36,200
It's either a 0 or 1 something that we want to maybe take note of is this zip code column.

81
00:06:36,200 --> 00:06:43,310
So there's the zip code is numerical and if we feed this directly into our model the model will assume

82
00:06:43,340 --> 00:06:50,930
that it's some sort of continuous feature that somehow ZIP Code 9 8 1 7 8 is greater than 9 8 1 2 5

83
00:06:51,500 --> 00:06:56,780
and this may or may not be the case that pending how the zip codes are actually mapped out on a real

84
00:06:56,780 --> 00:06:57,470
map.

85
00:06:57,470 --> 00:07:00,170
So this is where domain experience comes into play.

86
00:07:00,230 --> 00:07:05,300
You'd have to either look up the maps yourself so you can do a quick google search and find some sort

87
00:07:05,300 --> 00:07:10,280
of zip code mapping on top of King County and see if there actually is a relationship you could zoom

88
00:07:10,280 --> 00:07:15,830
in here and see if there's a really shift between the numbers zip codes are assigned versus maybe their

89
00:07:15,830 --> 00:07:17,420
latitude or longitude.

90
00:07:17,420 --> 00:07:22,640
They can also do that through some sort of correlation plot but if you take a closer look at this there

91
00:07:22,640 --> 00:07:29,210
doesn't seem to be a clear continuous distribution of these actual zip codes which means you would want

92
00:07:29,210 --> 00:07:32,180
to start treating this as a categorical variable.

93
00:07:32,180 --> 00:07:38,660
So again we'll come back to our notebook here and begin exploring this and see if it's actually viable

94
00:07:38,870 --> 00:07:48,300
to keep it as some sort of category so we can say something like the F zip code and call value counts

95
00:07:48,300 --> 00:07:53,730
on it and this will give us an idea of how many actual unique zip codes we have and what their distribution

96
00:07:53,730 --> 00:07:55,600
is like across the dataset.

97
00:07:55,740 --> 00:08:01,530
And it looks like we have 70 unique zip codes so it's probably too much to just call PD get dummies

98
00:08:01,530 --> 00:08:04,740
on this and then have 70 categories of zip codes.

99
00:08:04,950 --> 00:08:10,410
So we'll go ahead and for this particular case we will drop the zip code column because 70 categories

100
00:08:10,440 --> 00:08:11,660
is just too much for us.

101
00:08:11,730 --> 00:08:17,370
However what we could do is have a little bit of domain experience and domain knowledge maybe try to

102
00:08:17,400 --> 00:08:22,830
categorize this based off what we know are expensive zip codes versus maybe less expensive zip codes

103
00:08:23,190 --> 00:08:29,040
or we could do some sort of mapping or grouping to do zip codes in the south zip codes in the middle

104
00:08:29,040 --> 00:08:35,640
and zip codes in north east and west etc. But this will take then a lot more manual work here.

105
00:08:35,640 --> 00:08:39,340
And it also takes a lot more experience of what King County is actually like.

106
00:08:39,720 --> 00:08:45,930
So again a big part of machine learning and data science is to contact someone with domain experience

107
00:08:46,020 --> 00:08:49,560
and try to get those mappings and feature engineering in place.

108
00:08:49,560 --> 00:08:52,590
But for use case there's 70 categories here.

109
00:08:52,590 --> 00:08:55,350
We'll go ahead and just ignore them for now.

110
00:08:55,500 --> 00:09:00,450
However in a more realistic situation you'd probably want to take the time to look at a map and start

111
00:09:00,450 --> 00:09:04,220
mapping these out yourself manually.

112
00:09:04,460 --> 00:09:10,960
So we'll say EDF is equal to D F drop and kind of for the sake of time really we'll drop the zip code

113
00:09:10,970 --> 00:09:11,390
column

114
00:09:15,680 --> 00:09:20,960
and then there's other things that can take a look at another one that may be kind of troublesome is

115
00:09:20,960 --> 00:09:22,450
this year renovated.

116
00:09:22,610 --> 00:09:31,700
You'll notice that right year renovated if we take a look at it the F year renovated let's do a value

117
00:09:31,730 --> 00:09:32,620
count on that

118
00:09:36,330 --> 00:09:41,550
most of the values are actually zero which essentially implies that it was not renovated.

119
00:09:41,580 --> 00:09:47,480
So he's stuck in a zero there and then we have 2014 2013 etc..

120
00:09:47,650 --> 00:09:49,420
Now there's different approaches we can take here.

121
00:09:49,450 --> 00:09:55,900
As far as feature engineering is concerned one main issue here that we can tell is that zero is not

122
00:09:55,960 --> 00:09:57,610
actually a year.

123
00:09:57,610 --> 00:10:04,420
Instead it's basically an indication that the House was not renovated so it may make more sense to categorize

124
00:10:04,420 --> 00:10:10,630
this as renovated or not renovated essentially turn all these years using a custom to apply function

125
00:10:11,080 --> 00:10:18,280
into a kind of positive renovated call and then keep zero as just not renovated so we could do that

126
00:10:18,280 --> 00:10:20,150
through an apply function.

127
00:10:20,230 --> 00:10:25,330
However we can actually take advantage of the situation by thinking of it in the following fashion.

128
00:10:25,330 --> 00:10:30,940
Notice that it's most likely the more recent the year of renovation.

129
00:10:30,940 --> 00:10:37,300
Essentially the higher the value of this year renovated than the more likely that the House is going

130
00:10:37,300 --> 00:10:39,700
to have a higher sale price.

131
00:10:39,700 --> 00:10:44,830
And we can just think of that intuitively because the more recent the renovation the better.

132
00:10:44,830 --> 00:10:51,550
And since 0 actually follows along with this correlation it's almost like the lowest year possible.

133
00:10:51,700 --> 00:10:55,000
Then we should expect that to also have little value.

134
00:10:55,000 --> 00:11:01,480
So in this case we're actually kind of lucky and due to the scaling from zero to the highest year higher

135
00:11:01,480 --> 00:11:05,260
should correlate with more value and we can actually just keep this as is.

136
00:11:05,260 --> 00:11:11,200
And that's essentially the kind of lucky situation we find ourselves in that the years happen to go

137
00:11:11,380 --> 00:11:18,350
in a positive direction and the higher years tend to correlate with more value because the more recent

138
00:11:18,430 --> 00:11:22,780
renovation just intuitively you expect that to have more value.

139
00:11:22,780 --> 00:11:27,730
Now that won't always be the case for things that are years and then have zeros filled in.

140
00:11:27,880 --> 00:11:32,500
But in our case we got kind of lucky here and that makes sense to just leave it in.

141
00:11:32,650 --> 00:11:37,720
However you could perform more feature engineering off of this by categorizing it into renovated versus

142
00:11:37,720 --> 00:11:39,110
not renovated.

143
00:11:39,240 --> 00:11:39,950
Okay.

144
00:11:40,090 --> 00:11:46,120
And then other things we can do is we can take a look here at something like the square feet in the

145
00:11:46,120 --> 00:11:53,120
basement kind of a similar situation so I take us square feet and basement and do value counts on it.

146
00:11:55,320 --> 00:12:01,410
You'll notice that in a similar situation it just has zero there for a lot of the entries and zero most

147
00:12:01,410 --> 00:12:03,370
likely just means there is no basement there.

148
00:12:03,390 --> 00:12:06,720
So they just put in the zero four square feet and basement.

149
00:12:06,720 --> 00:12:14,400
Now we can see here again the values starting to go upwards etc. and these also basically make sense

150
00:12:14,520 --> 00:12:18,800
as a continuous variable because we would expect that there is no basement.

151
00:12:18,930 --> 00:12:21,620
It would have less value than having an extremely large basement.

152
00:12:21,720 --> 00:12:23,790
So we can also keep this as is.

153
00:12:23,790 --> 00:12:28,830
So a lot of times you'll have to make the decision in feature engineering if you want to make a continuous

154
00:12:28,830 --> 00:12:35,670
variable categorical or just keep it as continuous and you have to really think carefully if it's valid

155
00:12:35,670 --> 00:12:37,110
to keep it as continuous.

156
00:12:37,200 --> 00:12:42,840
And for these two it's not a crazy assumption to keep them as continuous and leave these zeros as kind

157
00:12:42,840 --> 00:12:44,530
of your bottom marker.

158
00:12:44,610 --> 00:12:44,840
All right.

159
00:12:45,420 --> 00:12:51,840
So that's it for a feature analysis and feature engineering section later on in your section exercise

160
00:12:52,200 --> 00:12:54,900
you'll be doing a lot more feature engineering than what we've done here.

161
00:12:55,410 --> 00:13:00,210
So keep that in mind feature engineering and exploratory data analysis is pretty much always a necessary

162
00:13:00,210 --> 00:13:03,810
part of any good data science or machine learning project.

163
00:13:03,810 --> 00:13:04,680
Coming up next.

164
00:13:04,680 --> 00:13:10,920
We're then going to focus on doing our train test split and scaling and pre processing our data in order

165
00:13:10,920 --> 00:13:13,410
to create our model to run off of it.

166
00:13:13,410 --> 00:13:15,080
Thanks and I'll see you at the next lecture.
