1
00:00:05,720 --> 00:00:07,240
Welcome back everyone.

2
00:00:07,250 --> 00:00:10,370
Now that we understand how image kernels and filters work.

3
00:00:10,370 --> 00:00:15,800
Let's go ahead and see how they play a role in the convolution layer portion of a conversational neural

4
00:00:15,800 --> 00:00:17,780
network.

5
00:00:17,780 --> 00:00:22,820
Recall that running an artificial neural network that was just a fully connected feed forward neural

6
00:00:22,820 --> 00:00:28,660
network for the endless dataset actually resulted in a network with relatively good accuracy.

7
00:00:28,670 --> 00:00:34,640
However there are some issues with always using an end models for image data.

8
00:00:34,640 --> 00:00:39,470
So if we were to always use artificial neural networks just normal fully connected networks for image

9
00:00:39,470 --> 00:00:43,020
data we would end up with a huge amount of parameters.

10
00:00:43,040 --> 00:00:48,650
Recall that when we actually checked the parameter count for our network on the end this dataset we

11
00:00:48,650 --> 00:00:50,530
had over 100000 parameters.

12
00:00:50,570 --> 00:00:58,160
And that was for tiny 28 by 20 images 28 by 28 pixel images are really small compared to your images

13
00:00:58,160 --> 00:00:59,620
that you're used to a normal life.

14
00:00:59,630 --> 00:01:05,630
For example you know that your resolution on just a normal monitor screen is probably like 10 ADP that's

15
00:01:05,630 --> 00:01:08,390
10 80 pixels or 1080 pixels.

16
00:01:08,390 --> 00:01:10,740
That's a lot larger than 28 by 28.

17
00:01:11,120 --> 00:01:16,400
So if we try to scale an artificial neural network to just normal sized images you're going to end up

18
00:01:16,400 --> 00:01:22,220
with not just 100000 parameters but a million parameters as a training time is going to take forever.

19
00:01:22,220 --> 00:01:27,170
The other problem with an artificial neural network is recall that we had to flatten out the data before

20
00:01:27,170 --> 00:01:29,100
actually feeding it into the network.

21
00:01:29,180 --> 00:01:33,590
And if we flatten out the data we basically lose all two dimensional information.

22
00:01:33,770 --> 00:01:40,420
And the other issue is an an and will actually only perform well where images are extremely similar.

23
00:01:40,460 --> 00:01:45,200
And if you take a closer look at the end this dataset you'll notice that essentially all the numbers

24
00:01:45,290 --> 00:01:47,090
are centered in the image.

25
00:01:47,270 --> 00:01:52,310
We want our convolution on neural networks or just our networks in general to be able to recognize a

26
00:01:52,310 --> 00:01:54,940
number regardless of where it is in the image.

27
00:01:54,950 --> 00:01:57,850
In artificial neural network will actually not be able to do that.

28
00:01:58,010 --> 00:02:04,820
If you were to take a 20 by 20 image of a handwritten digit but the digit was not centered maybe it

29
00:02:04,820 --> 00:02:10,160
was starting all the way on the right and feed it into the artificial neural network we trained you

30
00:02:10,160 --> 00:02:11,930
would actually not get good results.

31
00:02:11,990 --> 00:02:16,730
And it's most likely that the network would not be able to understand what it was actually looking at.

32
00:02:17,150 --> 00:02:21,980
So the M.A. data dataset is really special because all the numbers are perfectly centered in these really

33
00:02:21,980 --> 00:02:26,960
small Twenty by 20 images and that's why the artificial neural network was actually able to perform

34
00:02:26,960 --> 00:02:27,910
so well there.

35
00:02:28,100 --> 00:02:34,400
But later on we'll see that this network won't generalize well to just normal sized images especially

36
00:02:34,700 --> 00:02:37,270
when what we're looking for could be anywhere in the image.

37
00:02:37,310 --> 00:02:41,940
So we have a lot of restrictions for the artificial neural network.

38
00:02:42,260 --> 00:02:48,050
Now a CNN or convolution on your own network can use what's known as a convolution layer to help alleviate

39
00:02:48,140 --> 00:02:54,410
a lot of these issues a convolution of layers created when we apply multiple image filters to the input

40
00:02:54,470 --> 00:03:03,140
images the layer will then be trying to figure out the best filter weight values a CNN also helps reduce

41
00:03:03,140 --> 00:03:09,830
parameters by focusing on local connectivity and we'll actually see an example of this localized connections.

42
00:03:09,830 --> 00:03:14,780
So in a convolution on your own network specifically in this compositional air not all neurons will

43
00:03:14,780 --> 00:03:16,010
be fully connected.

44
00:03:16,160 --> 00:03:21,290
Instead neurons are only connected to a subset of local neurons in the next layer.

45
00:03:21,380 --> 00:03:27,580
And these actually end up becoming the filters so let's understand this local connectivity and its connection

46
00:03:27,580 --> 00:03:33,250
to filters by beginning with a very simplified one dimensional example and then later on we'll expand

47
00:03:33,250 --> 00:03:38,170
this to two dimensional inputs for a grayscale image and then later on to a three dimensional tensor

48
00:03:38,170 --> 00:03:46,370
input for a color image so here we have just a normal artificial neural network we see here on the left

49
00:03:46,370 --> 00:03:50,960
hand side we have some sort of input and then it's fully connected to all the neurons in the next layer

50
00:03:51,080 --> 00:03:56,270
and then that would continue on throughout the network so because this is fully connected.

51
00:03:56,300 --> 00:04:01,190
There are lots parameters which we've already decided is gonna be a problem for really large images

52
00:04:01,220 --> 00:04:06,470
or even just normal sized images because of training time to figure out all those parameters.

53
00:04:06,470 --> 00:04:13,200
So how do we reduce these number of parameters well a conversational layer what it does is it focuses

54
00:04:13,290 --> 00:04:14,740
on local connections.

55
00:04:14,910 --> 00:04:21,360
Notice now that the neurons are only connected to a local neuron in the next layer they're not connected

56
00:04:21,360 --> 00:04:23,860
to every other single neuron.

57
00:04:23,880 --> 00:04:29,190
So here we have these localized connections and this is just being shown on one dimension.

58
00:04:29,190 --> 00:04:35,940
Later on we'll expand this to two dimensions where we can see here that we actually have one filter.

59
00:04:36,060 --> 00:04:42,000
So what's gonna happen is because of this essentially a all layer with a localized connection these

60
00:04:42,000 --> 00:04:48,580
localized connections end up creating this filter and the network itself is going to figure out where

61
00:04:48,580 --> 00:04:55,050
are the best weights for this filter and so what we could do is then maybe add another filter and as

62
00:04:55,050 --> 00:05:00,120
you're training your compositional neural network you actually get to decide how many filters you want

63
00:05:00,120 --> 00:05:06,000
to apply to this image and essentially have the network learn the best weights for the filter.

64
00:05:06,000 --> 00:05:12,390
So previously we saw that really common filters such as a blur filter or an edge filter those weights

65
00:05:12,510 --> 00:05:13,660
are already known.

66
00:05:13,800 --> 00:05:18,960
But if you're working with maybe facial recognition data and you're trying to figure out well what filter

67
00:05:18,960 --> 00:05:23,610
values are going to be really good at recognizing eyebrows you yourself are probably not going be able

68
00:05:23,610 --> 00:05:29,010
to figure out those numbers but the convolution neural network will somewhere along the line probably

69
00:05:29,010 --> 00:05:33,980
be able to figure out some filter which is good at recognizing some component of that face.

70
00:05:33,990 --> 00:05:39,270
Keep in mind it's going to be difficult to actually open up so to speak the neural network and decide

71
00:05:39,270 --> 00:05:43,110
what each filter is actually looking at on those face images.

72
00:05:43,110 --> 00:05:50,400
But it's likely that the filters will begin to recognize higher and higher patterns related to a face

73
00:05:50,400 --> 00:05:57,270
like eyebrows noses ears mouths et cetera and we'll talk about interpreting the filters of a network

74
00:05:57,300 --> 00:05:59,810
and visualizing that later on in the course.

75
00:05:59,850 --> 00:06:04,950
But here we can get an idea that through these localized connections in this compositional neural network

76
00:06:05,220 --> 00:06:12,380
we can then have kind of a stack that no filters so that was a simple one dimensional example of convolution

77
00:06:12,830 --> 00:06:14,710
but recall these grayscale images.

78
00:06:14,750 --> 00:06:15,880
Those are two dimensional.

79
00:06:16,010 --> 00:06:20,920
And we mentioned that we wanted to preserve the two dimensional relational information in the convolution

80
00:06:20,920 --> 00:06:26,860
all layer and this is where our knowledge of those image filters or image kernels comes into play.

81
00:06:26,990 --> 00:06:29,300
So let's expand this down to two dimensions.

82
00:06:29,300 --> 00:06:31,230
And it's actually quite simple.

83
00:06:31,340 --> 00:06:32,810
Here we have an input image.

84
00:06:32,810 --> 00:06:34,060
It's a really simple image.

85
00:06:34,070 --> 00:06:36,090
It's just three by four pixels.

86
00:06:36,140 --> 00:06:41,690
And recall that since this is a grayscale image essentially each value falls somewhere between 0 and

87
00:06:41,690 --> 00:06:47,840
1 or the pending how it's normalized between negative 1 and 1 or 0 and 255 is another really common

88
00:06:48,500 --> 00:06:49,240
pixel value.

89
00:06:49,270 --> 00:06:54,290
But the main idea here is these are pixels and they're technically just numbers or a matrix or feeding

90
00:06:54,290 --> 00:06:55,110
in here.

91
00:06:55,130 --> 00:07:00,950
So here's our input image kind of just this really really simple Y shape we're showing here with darker

92
00:07:00,950 --> 00:07:07,550
zeros and lighter ones than what we're gonna do with this input image is recall that we have this filter

93
00:07:07,550 --> 00:07:12,440
that we can essentially scan across the image and other things to remember is that if we wanted to we

94
00:07:12,440 --> 00:07:17,290
could pad the image and we can also choose a stride length.

95
00:07:17,450 --> 00:07:24,080
So we're going to do is here in two dimensions we're going to focus on localized connections using this

96
00:07:24,080 --> 00:07:28,880
filter process and recall again that we can edit our stride size.

97
00:07:28,880 --> 00:07:34,460
And so eventually what we end up doing here is we get these localized connections from these input images

98
00:07:34,460 --> 00:07:41,120
or input features of these pixel values and connect them to only certain subsets of neurons in this

99
00:07:41,120 --> 00:07:42,110
next layer.

100
00:07:42,110 --> 00:07:47,120
And you'll notice that essentially what we're creating here is a filter and then it's it's up to the

101
00:07:47,120 --> 00:07:51,320
network to figure out what weights that should be applied to this filter during training to correctly

102
00:07:51,320 --> 00:07:52,850
classify the images.

103
00:07:52,850 --> 00:07:56,020
And if you want you can always do this with another set of filters.

104
00:07:56,030 --> 00:07:58,270
You can add on as many filters as you want here.

105
00:07:58,400 --> 00:08:00,960
So you do the same scanning process again and again.

106
00:08:01,130 --> 00:08:07,580
And here we have three filters on this input image but it's not uncommon to see a tens or hundreds of

107
00:08:07,580 --> 00:08:13,220
filters depending on the complexity of what you're actually trying to classify as well as how many different

108
00:08:13,220 --> 00:08:21,130
types of objects you're trying to classify so if we were kind to stack these filters together that's

109
00:08:21,130 --> 00:08:23,540
what begins creating our conversational air.

110
00:08:23,560 --> 00:08:29,170
It's essentially this stack of filters for the convolution all network to figure out the proper weights

111
00:08:29,590 --> 00:08:35,980
in each of these filters only has localized connections because we understand how the image kernel is

112
00:08:36,130 --> 00:08:40,360
involved across that image.

113
00:08:40,360 --> 00:08:44,360
Now keep in mind that was an example for technically just a grayscale image.

114
00:08:44,380 --> 00:08:51,970
But what about color images color images can be thought as three dimensional is consisting of red green

115
00:08:52,030 --> 00:08:59,890
and blue color channels additive color mixing allows us to represent a wide variety of colors by simply

116
00:08:59,890 --> 00:09:02,500
combining different amounts of red green and blue.

117
00:09:02,980 --> 00:09:08,230
So you've probably already heard of RG G.B. color coding red green blue color coding and the idea behind

118
00:09:08,230 --> 00:09:15,770
that is through some combination of red green and blue you can create any version of any color.

119
00:09:15,790 --> 00:09:21,250
Now keep in mind there are limits to this are RG B can't produce every single color available but it

120
00:09:21,250 --> 00:09:26,380
does produce a wide variety of colors that we can see on a human scale.

121
00:09:26,440 --> 00:09:31,780
So you're not going to see things like ultraviolet or infrared but arguably does allow you to create

122
00:09:32,260 --> 00:09:37,510
what's known here as this range of colors now.

123
00:09:37,520 --> 00:09:41,240
Each color channel will have intensity values and you've probably already seen this.

124
00:09:41,270 --> 00:09:47,190
If you've ever worked with representation of color pixels and other software of RG B sliders.

125
00:09:47,510 --> 00:09:54,200
So here we have our sliders and essentially we choose some value between 0 and 255 or red green and

126
00:09:54,200 --> 00:09:59,330
blue and you essentially mix them together as if there are paints and you get some sort of color out.

127
00:09:59,330 --> 00:10:04,580
The main idea being is that these colors can essentially be represented as a mix between red green and

128
00:10:04,580 --> 00:10:05,450
blue.

129
00:10:05,510 --> 00:10:09,320
So what does that mean for our actual images.

130
00:10:09,320 --> 00:10:15,200
So here we can see an example of breaking down a full color image into its color channels.

131
00:10:15,200 --> 00:10:20,330
So for a single color image we have these actual three dimensions.

132
00:10:20,330 --> 00:10:22,980
We have the height in the with something we're already familiar with.

133
00:10:23,120 --> 00:10:28,820
But then for every single color channel that is one for red one for green and one for blue we're actually

134
00:10:28,820 --> 00:10:36,040
going to have a separate array of pixels so this means when you actually read in an image and chicken

135
00:10:36,070 --> 00:10:37,920
shape it's going to look something like this.

136
00:10:37,930 --> 00:10:41,730
You'll have maybe twelve eighty by 720 by three.

137
00:10:41,740 --> 00:10:50,060
So that means that the single image has told 80 pixel with 720 pixel height and then three color channels.

138
00:10:50,110 --> 00:10:54,790
And keep in mind that each of these color channels is essentially if you were just to grab one of them

139
00:10:55,090 --> 00:10:56,960
it looks like a grayscale image.

140
00:10:56,980 --> 00:11:03,700
That's just values between zero and one or zero to fifty five or negative one and one they just represent

141
00:11:03,700 --> 00:11:06,420
the intensity for that particular color.

142
00:11:06,460 --> 00:11:09,520
So we can see here the intensity for red green and blue.

143
00:11:09,520 --> 00:11:14,530
And then when you combine those intensities as we previously discussed here you end up creating mixes

144
00:11:14,530 --> 00:11:19,090
of colors which produces this color image.

145
00:11:19,130 --> 00:11:23,540
So again we think of this as the height the width and then three color channels.

146
00:11:23,570 --> 00:11:25,960
And recall this is just for a single image.

147
00:11:26,090 --> 00:11:32,480
So now a single image is actually this three dimensional tensor.

148
00:11:32,490 --> 00:11:36,900
You should also keep in mind that the computer will actually know if a channel is red.

149
00:11:36,990 --> 00:11:40,260
It just knows that there are now three Intensity channels.

150
00:11:40,260 --> 00:11:45,150
So you should also keep that in mind in case your images happen to have different color encoding certain

151
00:11:45,150 --> 00:11:48,780
programs will actually encode instead of red green blue.

152
00:11:48,840 --> 00:11:51,190
They'll encode it as blue green red.

153
00:11:51,300 --> 00:11:55,800
That won't really matter that much for our use case of the convolution on you're on networks because

154
00:11:56,070 --> 00:11:59,400
the network doesn't really care what order the colors are being fed in.

155
00:11:59,400 --> 00:12:04,290
It just cares that there's three Intensity channels.

156
00:12:04,330 --> 00:12:05,890
Now the question arises.

157
00:12:05,890 --> 00:12:10,960
We just saw how to perform a convolution on a one dimensional array that we saw had a performance on

158
00:12:10,960 --> 00:12:12,370
a two dimensional array.

159
00:12:12,370 --> 00:12:17,300
How do we perform a convolution on this color image since it's a three dimensional tensor.

160
00:12:17,410 --> 00:12:23,870
So we actually end up with is a three dimensional filter with values for each color channel so essentially

161
00:12:24,350 --> 00:12:27,740
this filter now has three dimensions to it.

162
00:12:27,770 --> 00:12:32,140
We have the actual height and width of that filter shown as three by three.

163
00:12:32,150 --> 00:12:36,200
But then we have an instance for each dimension for the color channels.

164
00:12:36,200 --> 00:12:41,630
So there's one for the red one for the green and one for the blue and just as before we can have as

165
00:12:41,630 --> 00:12:43,130
many filters as we want.

166
00:12:43,310 --> 00:12:44,840
So we can create another filter.

167
00:12:44,840 --> 00:12:46,970
And again this filter is three dimensional.

168
00:12:46,970 --> 00:12:50,060
It has one little image kernel per color channel.

169
00:12:50,120 --> 00:12:52,690
So one for red one for green on blue for blue.

170
00:12:52,730 --> 00:12:54,840
And we can keep adding on these filters.

171
00:12:55,040 --> 00:12:59,860
So we should now be thinking of these filters for these colored images as three dimensional.

172
00:12:59,930 --> 00:13:06,140
And later on the course will actually expand from grayscale images and we'll move on to color images

173
00:13:06,170 --> 00:13:12,940
so we can get an idea of working with this extra dimension of data now often come volitional layers

174
00:13:13,030 --> 00:13:15,820
are fed into another compositional layer.

175
00:13:15,880 --> 00:13:21,490
So it's not uncommon to see it come volitional layer being directly stacked and fed into another compositional

176
00:13:21,490 --> 00:13:24,220
error and then stack them fed into another compositional layer.

177
00:13:24,250 --> 00:13:29,710
And what's really cool about this is this allows a network to discover patterns within patterns usually

178
00:13:29,710 --> 00:13:34,230
with more complexity for later compositional layers.

179
00:13:34,230 --> 00:13:39,860
Now we've learned a lot so far and we have one final theory topic to cover before coding our own accomplish

180
00:13:39,860 --> 00:13:40,640
on your own networks.

181
00:13:40,650 --> 00:13:45,640
And that is the pooling layer also known as the down sampling or sub sampling layer.

182
00:13:45,780 --> 00:13:49,800
Once you understand how the pulling layer works along with the compositional error then we have all

183
00:13:49,800 --> 00:13:54,780
the pieces we need to actually create our own compositional neural network and then we'll revisit that

184
00:13:54,830 --> 00:13:56,800
in this dataset problem.

185
00:13:56,820 --> 00:13:59,700
So in the next lecture let's go in and discuss pulling layers.

186
00:13:59,700 --> 00:14:00,300
I'll see you there.
