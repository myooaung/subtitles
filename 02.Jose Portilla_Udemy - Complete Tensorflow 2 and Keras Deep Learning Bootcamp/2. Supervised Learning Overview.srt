1
00:00:05,540 --> 00:00:12,190
Welcome back everyone to this lecture on supervised learning supervised learning algorithms are trained

2
00:00:12,250 --> 00:00:14,130
using labeled examples.

3
00:00:14,140 --> 00:00:19,390
And that's a keyword label such as an input where the desired output is known.

4
00:00:19,390 --> 00:00:24,990
That means within your dataset you're going to have some historical features with historical labels.

5
00:00:25,120 --> 00:00:30,150
So you already have that information such as a segment of text could have a category label.

6
00:00:30,250 --> 00:00:36,400
So you take a bunch of previous e-mails and someone has already gone by and classified them using the

7
00:00:36,400 --> 00:00:37,090
correct label.

8
00:00:37,090 --> 00:00:42,910
So they read the e-mail and classified it as spam versus legitimate or we have a bunch of movie reviews

9
00:00:43,210 --> 00:00:48,040
and someone has already gone and labeled movie reviews either positive to the movie or negative to the

10
00:00:48,040 --> 00:00:55,120
movie and then the idea would be for future text information such as a future e-mail using the historical

11
00:00:55,120 --> 00:01:01,300
label data the network or machine learning algorithm could learn off the historical data in order to

12
00:01:01,300 --> 00:01:07,570
predict for new data whether it belongs in the spam category or legitimate category or in the positive

13
00:01:07,570 --> 00:01:14,530
category or negative category for these movie reviews so the way this works is for neural networks the

14
00:01:14,530 --> 00:01:20,620
networks and receive a set of input data along with the corresponding correct outputs and then the algorithm

15
00:01:20,620 --> 00:01:25,960
or network will learn by comparing its actual output with correct outputs to find errors and then it

16
00:01:25,960 --> 00:01:31,320
will modify the model accordingly such as adjusting the weights and biased values in the network and

17
00:01:31,310 --> 00:01:32,870
the worry about those two key terms.

18
00:01:32,890 --> 00:01:39,020
We'll discuss those and a lot more detail when we talk about neuro network theory so supervised learning

19
00:01:39,140 --> 00:01:47,050
is used in applications where historical data predicts likely future events and the machine learning

20
00:01:47,050 --> 00:01:48,720
process for supervised learning.

21
00:01:48,730 --> 00:01:49,600
Looks like this.

22
00:01:49,600 --> 00:01:56,070
So let's go in and go through this step by step so the first thing to do is actually get data and depends

23
00:01:56,070 --> 00:01:59,220
on what domain you're working in where this data actually comes from.

24
00:01:59,280 --> 00:02:06,180
This can come from your customers or can come from collecting things into a database online or maybe

25
00:02:06,180 --> 00:02:09,290
it's physical data and it comes from sensors et cetera.

26
00:02:09,330 --> 00:02:16,080
So at some point the data has to actually be acquired once we actually acquire the data then we need

27
00:02:16,080 --> 00:02:21,580
to clean and format the data so that our neural network can actually process it and often we'll do this

28
00:02:21,580 --> 00:02:30,110
using a library called PANDAS then we split the data into training data and test data and we'll talk

29
00:02:30,110 --> 00:02:34,360
about this particular step in a little more detail towards the end of this lecture.

30
00:02:34,490 --> 00:02:40,520
But what we do here is we take some portion of our data maybe like 30 percent to be test data and then

31
00:02:40,610 --> 00:02:46,190
the larger majority of data like 70 percent to be our training data and what we're going to do here

32
00:02:46,400 --> 00:02:53,030
is we're going to use that specific training set on our network or model in order to fit a model to

33
00:02:53,030 --> 00:02:58,630
that training data then we want to know how well our model actually performed.

34
00:02:58,690 --> 00:03:06,370
So we then run that test data through the model and compare the model's prediction to the actual correct

35
00:03:06,430 --> 00:03:11,410
label that the test date ahead because remember we actually know the correct label for that test data

36
00:03:11,830 --> 00:03:17,050
so you can run that test data features through the model get our models predictions and compare it to

37
00:03:17,050 --> 00:03:21,370
the right answer and then we can evaluate the model and then maybe you want to go back based off that

38
00:03:21,370 --> 00:03:27,940
performance and adjust the model parameters maybe add more layers or more neurons to try to get a better

39
00:03:27,940 --> 00:03:29,540
fit onto that test data.

40
00:03:29,680 --> 00:03:35,750
And once we're satisfied of this we can then deploy the model to the real world now something to note

41
00:03:35,750 --> 00:03:41,720
here is what we just showed was technically a simplified approach to supervised learning and it does

42
00:03:41,720 --> 00:03:47,450
contain a key issue which we kind of touched upon during that test train split and the question arises

43
00:03:47,930 --> 00:03:55,490
Is it fair to use that single split of the data into one test set and one training set to actually evaluate

44
00:03:55,490 --> 00:03:57,200
your model's performance.

45
00:03:57,200 --> 00:04:03,230
So when you actually test your model on the test data you'll get some sort of performance metric for

46
00:04:03,230 --> 00:04:04,590
regression tax.

47
00:04:04,610 --> 00:04:08,510
It could be something like a root mean square error for a classification task.

48
00:04:08,540 --> 00:04:15,770
It could be something like the accuracy but is it actually fair to use the accuracy you get off the

49
00:04:15,770 --> 00:04:18,790
test data as your model's final performance metric.

50
00:04:18,920 --> 00:04:24,650
Since technically after all you were given the chance to update the model parameters again and again

51
00:04:25,010 --> 00:04:27,380
after evaluating your results on that test set.

52
00:04:28,190 --> 00:04:30,050
So how do we fix this conundrum.

53
00:04:31,280 --> 00:04:37,460
Well to fix this issue the data especially in neural networks and deep learning it's often split actually

54
00:04:37,520 --> 00:04:42,710
into three sets and we have training data validation data and then test data.

55
00:04:43,640 --> 00:04:48,030
So we kind of introduce this in-between step of this validation.

56
00:04:48,080 --> 00:04:52,430
And so what we end up doing is we have these three sets and we have the training data just as we did

57
00:04:52,430 --> 00:04:53,150
before.

58
00:04:53,330 --> 00:04:55,850
And this is used to train them on a parameters.

59
00:04:55,850 --> 00:05:02,660
So the model gets to look at the features look at the correct output and then fit to this training data

60
00:05:03,020 --> 00:05:08,400
and then the next step is our validation data which was kind of our test data from before.

61
00:05:08,570 --> 00:05:14,720
And so what we do this validation data is after training on the training data we check the performance

62
00:05:14,810 --> 00:05:20,900
on the validation data and maybe based off that performance we go back and adjust our models.

63
00:05:20,900 --> 00:05:27,060
Maybe adding more neurons or adding more layers changing the actual architecture of the network et cetera.

64
00:05:27,110 --> 00:05:31,070
And then you kind of repeat that process over and over again until you're satisfied.

65
00:05:31,070 --> 00:05:38,000
If your model's performance on the validation data and now it comes time to evaluate the true performance

66
00:05:38,120 --> 00:05:39,270
of your model.

67
00:05:39,440 --> 00:05:40,610
So what do we do.

68
00:05:40,850 --> 00:05:44,120
Well that's why we have that third split of test data.

69
00:05:44,120 --> 00:05:49,520
That the model has never seen before and what you use that final test data set is to actually get some

70
00:05:49,520 --> 00:05:51,410
final performance metric.

71
00:05:51,440 --> 00:05:56,960
Now the key thing to note here is that once you run the model through the test data that's gonna be

72
00:05:56,960 --> 00:06:01,730
the performance metric that you expect your model to actually perform with in the real world.

73
00:06:01,820 --> 00:06:08,150
Since you're not going to go back and adjust your models weights or parameters or anything else.

74
00:06:08,150 --> 00:06:14,270
Once you actually go onto that final test dataset you're technically not allowed to go back and adjust

75
00:06:14,270 --> 00:06:19,880
the model in order to try to refine your performance on that final test dataset so you can think of

76
00:06:19,880 --> 00:06:20,980
it this way.

77
00:06:21,350 --> 00:06:24,290
You train on the training data to actually fit your model.

78
00:06:24,470 --> 00:06:29,000
Then you use the validation data to see how your model performs and unseen data and then go back and

79
00:06:29,000 --> 00:06:30,510
adjust your hyper parameters.

80
00:06:30,590 --> 00:06:35,870
But when it comes time to actually kind of report to your boss how well will this model do in the real

81
00:06:35,870 --> 00:06:36,480
world.

82
00:06:36,560 --> 00:06:40,290
That's where your final test dataset comes into play and the test data set.

83
00:06:40,370 --> 00:06:44,390
Once you actually pass through test data and you get that performance metric that's it.

84
00:06:44,390 --> 00:06:47,060
You don't really get to go back and adjust the hyper parameters.

85
00:06:47,060 --> 00:06:51,950
Otherwise you're kind of cheating yourself again and understanding the models real performance on truly

86
00:06:52,010 --> 00:06:53,270
unseen data.

87
00:06:53,270 --> 00:06:59,030
So what this means is after we see that result on the final test set we don't get to go back and adjust

88
00:06:59,120 --> 00:07:00,710
any of those model parameters.

89
00:07:00,740 --> 00:07:08,050
This final measure is what we labeled the true performance of the model to be on unseen data now in

90
00:07:08,050 --> 00:07:08,710
this course.

91
00:07:08,710 --> 00:07:13,300
In general we're going to simplify our data by just doing that single train test split.

92
00:07:13,300 --> 00:07:18,550
While technically we could be doing another split to get both validation and a test set for the kind

93
00:07:18,550 --> 00:07:22,630
of problems and exercises we're doing in this course since we're not really deploying a lot of these

94
00:07:22,630 --> 00:07:28,360
models to the real world it's not a huge deal that we essentially just have a training set and a test

95
00:07:28,360 --> 00:07:34,000
set and we skip the sort of like in-between validation set so we will simply train and then evaluate

96
00:07:34,000 --> 00:07:38,560
on a test set and we'll leave the option to the students to go back and adjust the hyper parameters

97
00:07:38,770 --> 00:07:42,850
and then after going through this entire course you're going to be able to easily perform another split

98
00:07:43,240 --> 00:07:44,440
to get three data sets.

99
00:07:44,440 --> 00:07:50,620
If you desire so you'll be able to easily create a training set a test set and a validation set if that's

100
00:07:50,620 --> 00:07:55,300
what you want but just want to tell you in this course we'll keep things simple and just train on a

101
00:07:55,300 --> 00:08:00,160
training set and then evaluate on a test set and we won't really go back into just hyper parameters

102
00:08:00,160 --> 00:08:02,500
will kind of treat that test as the final.

103
00:08:02,530 --> 00:08:07,570
And again you can always go back and add in more neurons or more layers or more hyper parameters to

104
00:08:07,570 --> 00:08:13,300
your network they'll be really easy to do and we'll kind of leave that up to you as a student to decide

105
00:08:13,300 --> 00:08:15,120
if you want to continue with that.

106
00:08:15,220 --> 00:08:19,250
Okay let's go ahead and move on to discuss things like overfishing.

107
00:08:19,330 --> 00:08:20,290
I'll see you at the next lecture.
