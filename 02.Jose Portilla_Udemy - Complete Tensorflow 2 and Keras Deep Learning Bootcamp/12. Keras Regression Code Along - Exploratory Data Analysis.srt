1
00:00:05,390 --> 00:00:06,980
Welcome back everyone.

2
00:00:06,980 --> 00:00:12,890
Over the next series of lectures we're going to be coding along with a regression based project where

3
00:00:13,040 --> 00:00:18,860
based off the dataset for different housing features we're going to try to predict the price that a

4
00:00:18,860 --> 00:00:20,190
house should sell it.

5
00:00:20,420 --> 00:00:26,060
So based on things like the number of bedrooms number of bathrooms square footage et cetera we'll try

6
00:00:26,060 --> 00:00:30,110
to build a deep learning model that can actually predict the price of a house.

7
00:00:30,590 --> 00:00:35,510
So a big part of this is exploratory data analysis as well as feature engineering which is what are

8
00:00:35,510 --> 00:00:37,610
going to focus on in part one.

9
00:00:37,610 --> 00:00:39,920
Let's head over to a notebook and get started.

10
00:00:39,920 --> 00:00:45,620
I'm here in a notebook and I've already imported pandas number hi Matt put lib and Seabourn born as

11
00:00:45,620 --> 00:00:46,340
a quick note.

12
00:00:46,430 --> 00:00:52,550
If you go to our annex folder the notebook that corresponds to this lecture or series of lectures it's

13
00:00:52,550 --> 00:00:53,650
called Cash regression.

14
00:00:53,650 --> 00:00:58,190
It's right after the syntax basics and if you open it up you should be able to scroll down and then

15
00:00:58,190 --> 00:01:00,980
see a full description of all the feature columns.

16
00:01:01,040 --> 00:01:07,130
So some of these feature columns it may not be quite obvious what the actual feature column stands for.

17
00:01:07,130 --> 00:01:09,290
So here we have the actual definitions.

18
00:01:09,290 --> 00:01:10,310
In case you're interested in it.

19
00:01:10,790 --> 00:01:20,290
So we're gonna come back here and let's load up our data we'll say def is equal to PD dot read CSB and

20
00:01:20,290 --> 00:01:27,970
our data is located underneath our data folder so if we go back up a directory and is called Casey underscore

21
00:01:28,240 --> 00:01:34,150
House underscore data that CSP go ahead and read it in.

22
00:01:34,320 --> 00:01:38,310
And again if you check out our regression notebook we're actually using this data.

23
00:01:38,340 --> 00:01:43,600
So it's a rural dataset from a Kaggle link so you can check out the actual link right here.

24
00:01:43,650 --> 00:01:50,280
But basically what this is it's historical pricing data for house sales in King County USA and King

25
00:01:50,280 --> 00:01:53,080
County is essentially where Seattle is.

26
00:01:53,140 --> 00:01:58,860
So come back here to our notebook and we're going to export this data a little bit using some visualization

27
00:01:58,860 --> 00:01:59,940
techniques.

28
00:01:59,940 --> 00:02:02,010
So first let's see if we have any missing data.

29
00:02:02,010 --> 00:02:06,480
We'll say the f is no not some.

30
00:02:06,480 --> 00:02:10,360
And so what this does is first off the f is no.

31
00:02:10,590 --> 00:02:14,200
That just returns back true or false if something is null.

32
00:02:14,220 --> 00:02:17,640
So if something is missing it'll return back true.

33
00:02:17,640 --> 00:02:23,520
And I can actually then sum these up per column and it will treat false as zeros and truths as ones

34
00:02:23,790 --> 00:02:27,590
and that we can actually add a count of how many missing data points we have.

35
00:02:27,590 --> 00:02:31,630
And for this particular dataset we actually have no missing data which kind of makes sense.

36
00:02:31,650 --> 00:02:36,750
If a house was historically sold it's not like you wouldn't know the number of bedrooms the house had

37
00:02:36,810 --> 00:02:38,090
before you sold it.

38
00:02:38,100 --> 00:02:42,490
So for this particular data set it makes sense that there is no missing data later on in the section

39
00:02:42,510 --> 00:02:46,600
The course will show you how to deal with a dataset that does have missing data.

40
00:02:46,620 --> 00:02:52,530
And another thing I like to do after you check out how much data is missing is just do a quick describe

41
00:02:52,530 --> 00:02:57,660
call which will give you the statistical analysis on your dataset.

42
00:02:57,750 --> 00:03:04,020
And I personally like to transpose this so that I get to see the statistical means during deviation

43
00:03:04,110 --> 00:03:07,920
mean percentile values etc. For all the columns.

44
00:03:07,920 --> 00:03:09,780
Now some of these doesn't really mean anything.

45
00:03:09,810 --> 00:03:18,560
So later on if you actually see the head of the data frame you'll notice that one of them is just a

46
00:03:18,590 --> 00:03:19,610
unique idea.

47
00:03:19,610 --> 00:03:21,880
So this is some sort of unique idea for the sale.

48
00:03:21,920 --> 00:03:27,650
So it doesn't really make sense to look into any meaning for the mean of the I.D. our other things like

49
00:03:27,650 --> 00:03:29,200
the mean price could be really important.

50
00:03:29,780 --> 00:03:35,630
So we can see here that we have this scientific notation which basically means five point four times

51
00:03:35,870 --> 00:03:37,230
10 to the power of five.

52
00:03:37,250 --> 00:03:38,840
So essentially add in five zeros.

53
00:03:39,440 --> 00:03:41,770
OK so we have all that information.

54
00:03:41,840 --> 00:03:44,880
It's a little hard to just read this table and thoroughly understand it.

55
00:03:45,230 --> 00:03:49,970
Let's start actually describing it through visualization using see more in the map live and all those

56
00:03:49,970 --> 00:03:54,050
skills we learned in the data visualization crass course section.

57
00:03:54,050 --> 00:04:00,880
So something I could do especially for continuous labels is just do a distribution of the actual label

58
00:04:02,030 --> 00:04:07,910
so do a distribution plot essentially a histogram and something else I can do is just make this a little

59
00:04:07,910 --> 00:04:19,390
larger by calling Pulte that figure and setting fixed size to let's say 10 by 6.

60
00:04:19,460 --> 00:04:22,540
So I'll go ahead and run that and I get to see this distribution.

61
00:04:22,600 --> 00:04:28,850
So notice here it looks like most of our houses are falling somewhere between zero and maybe around

62
00:04:28,850 --> 00:04:34,040
one point five million dollars and we could have these extreme outliers here for the really expensive

63
00:04:34,040 --> 00:04:41,120
houses and it may actually make sense to drop those outliers in our analysis if they are just a few

64
00:04:41,120 --> 00:04:43,060
points that are very extreme.

65
00:04:43,130 --> 00:04:48,230
And so we can essentially build a model that realistically predicts the price of a house if its intended

66
00:04:48,230 --> 00:04:51,930
value is somewhere between let's say 0 and 2 million dollars.

67
00:04:51,950 --> 00:04:55,730
There's not going to be that many houses apparently on the market that are worth more than let's say

68
00:04:55,760 --> 00:04:56,790
3 million.

69
00:04:56,810 --> 00:05:02,090
So that's something to keep in mind here especially for applying this to a realistic situation.

70
00:05:02,210 --> 00:05:06,560
They were trying to build a model for a real estate agency since it's really not that many houses on

71
00:05:06,560 --> 00:05:08,290
the market that are that expensive.

72
00:05:08,510 --> 00:05:14,450
It may not be really useful to actually have our model train on these extreme outliers.

73
00:05:14,480 --> 00:05:18,270
Now we can go ahead and do similar analysis of different features.

74
00:05:18,410 --> 00:05:25,880
So for example for categorical ones such as number of bedrooms which is kind of continuous but you can't

75
00:05:25,880 --> 00:05:29,500
have really one point five bedrooms you can actually have that four bathrooms.

76
00:05:29,510 --> 00:05:32,120
But for the case of bedrooms we are in our data set.

77
00:05:32,120 --> 00:05:34,580
We don't really have actually one point five or five.

78
00:05:34,760 --> 00:05:37,520
So we can treat this as a count plot.

79
00:05:37,520 --> 00:05:42,600
So I could say something like go ahead and count bedrooms and then plot them out.

80
00:05:42,680 --> 00:05:48,950
And so here I can see what actually looks like almost like a similarly distribution where the vast majority

81
00:05:49,010 --> 00:05:54,530
of all these houses have somewhere between two to five bedrooms and it looks like there's a huge mansion

82
00:05:54,530 --> 00:05:56,750
somewhere in this that has 33 bedrooms.

83
00:05:56,750 --> 00:06:02,210
So it looks like eight through 33 probably just have like one instance themselves which is why they're

84
00:06:02,210 --> 00:06:02,780
showing up here.

85
00:06:02,780 --> 00:06:07,640
But we can't actually see that bar because no the actual rest of the bedrooms are in the thousands.

86
00:06:07,640 --> 00:06:12,200
So it makes sense that you wouldn't really see a color for something as small as one.

87
00:06:12,230 --> 00:06:19,190
Now what's also nice is just comparing your label to some sort of feature that you think has a high

88
00:06:19,190 --> 00:06:28,960
correlation and what you can't do is you can say data frame correlation run that and then you can begin

89
00:06:28,960 --> 00:06:31,960
to see what actually correlates with your label and offer this.

90
00:06:32,020 --> 00:06:39,040
I'm going to go ahead and grab my label let's just say price run that and let's go ahead and sort these

91
00:06:39,040 --> 00:06:48,400
values will say sort underscore values and here I can see things that are either highly positively correlated

92
00:06:48,430 --> 00:06:50,170
or highly negatively correlated.

93
00:06:50,170 --> 00:06:55,630
Obviously price is going to be perfectly correlated with price but it looks like these square feet of

94
00:06:55,630 --> 00:07:00,410
living space has a very high correlation with the actual price of the house.

95
00:07:00,640 --> 00:07:06,580
And what I would recommend doing is exploring highly correlated features with your label through a scatter

96
00:07:06,580 --> 00:07:06,850
plot.

97
00:07:07,300 --> 00:07:22,900
So for example I could say S.A. scatter plot and compare price to my square feet living space and say

98
00:07:22,900 --> 00:07:30,010
data is equal to D F I'll go ahead and run that and I can see here a very strong linear relationship.

99
00:07:30,010 --> 00:07:31,310
And if I need to expand this out.

100
00:07:31,330 --> 00:07:34,440
Notice that looks like the price is overlapping.

101
00:07:34,840 --> 00:07:41,950
I could say Pulte figure and just give it a little more space by saying think size is equal to something

102
00:07:41,950 --> 00:07:46,400
like 10 by 5 and that gives us a nice little space here.

103
00:07:46,460 --> 00:07:47,110
Okay.

104
00:07:47,270 --> 00:07:53,480
So again I always recommend checking out the correlations between your different features and your actual

105
00:07:53,480 --> 00:07:58,580
label and then exploring those correlations through or exploring those features through some sort of

106
00:07:58,580 --> 00:08:00,170
data visualization.

107
00:08:00,170 --> 00:08:05,780
So for example if we take a look here it looks like bedrooms also has some positive correlation as well

108
00:08:05,780 --> 00:08:10,850
as bathrooms and you can do count plots of those against the price as well or even box plots to see

109
00:08:10,850 --> 00:08:12,650
the distributions.

110
00:08:12,650 --> 00:08:20,720
For example I could say box plot where x is the number of bedrooms.

111
00:08:20,920 --> 00:08:24,510
Why is the price.

112
00:08:24,760 --> 00:08:27,370
And then my data is the f.

113
00:08:27,850 --> 00:08:30,540
I'll go ahead and make this a little larger as well.

114
00:08:31,030 --> 00:08:36,520
By doing a figure call here go ahead and make this 10 by 6 or something similar.

115
00:08:36,640 --> 00:08:43,250
And what this is showing me is the distribution of prices per bedrooms.

116
00:08:43,270 --> 00:08:49,420
So for example I can see that there's quite a bit of variation in bedrooms ranging between 3 and 7.

117
00:08:49,630 --> 00:08:54,160
And that also makes sense because if we took a look at our count plot from before it looks like the

118
00:08:54,160 --> 00:08:58,590
majority of the houses have bedrooms between maybe 3 and 7.

119
00:08:58,600 --> 00:09:02,880
So it also makes sense that there's quite a large variety in prices there.

120
00:09:03,760 --> 00:09:08,320
So there is no right or wrong way to perform exploratory data analysis.

121
00:09:08,320 --> 00:09:13,570
So feel free to continue exploring this dataset through any other features that you're interested in

122
00:09:13,780 --> 00:09:16,540
either doing box plots or count plots.

123
00:09:16,540 --> 00:09:26,800
We should notice however that in our dataset if we take a look at the columns we have this lat and long

124
00:09:26,800 --> 00:09:33,430
features and if we take a look back at our actual feature columns those actually stand for the latitude

125
00:09:33,640 --> 00:09:39,760
and longitude so it may be interesting to actually explore this by plotting it out and we can actually

126
00:09:39,760 --> 00:09:43,120
do a pretty good job of this just if a simple scatter plot.

127
00:09:43,120 --> 00:09:48,800
Now keep in mind Seabourn doesn't actually have built in geographical plotting capabilities.

128
00:09:48,800 --> 00:09:53,740
There is a little bit of that with map plot lib with some extension plug in libraries but we're actually

129
00:09:53,740 --> 00:09:58,550
not going to focus on trying to plot these points on top of a real world map.

130
00:09:58,570 --> 00:10:03,400
Instead we can actually gain a lot of information with a little bit of cursory knowledge of what King

131
00:10:03,400 --> 00:10:07,510
County actually looks like combined with a simple scatter plot call.

132
00:10:07,630 --> 00:10:12,880
So let's first see the distribution of prices per latitude vs. longitude.

133
00:10:13,420 --> 00:10:23,490
It's going to come back to my notebook here and all I'm going to do is see what do my prices look like.

134
00:10:23,490 --> 00:10:31,220
So we'll have price on the x axis and then is there some sort of differentiating factor just based off

135
00:10:31,220 --> 00:10:35,470
longitude so then we'll say data is DLF.

136
00:10:35,670 --> 00:10:40,140
And since I have so many points in my dataset I'm gonna make this figure quite a bit larger.

137
00:10:40,320 --> 00:10:46,110
We'll say Pulte figure and set fixed size equal to let's make it 12 by eight.

138
00:10:47,230 --> 00:10:55,240
So I can go ahead and run this and I would expect to see just essentially kind of a flat blob here.

139
00:10:55,330 --> 00:10:58,200
If there was no price differentiation based off longitude.

140
00:10:58,570 --> 00:11:04,870
But it looks like there tends to be some sort of price distribution at a certain longitude.

141
00:11:04,900 --> 00:11:09,190
So looks like at longitude negative one to two point two.

142
00:11:09,190 --> 00:11:11,560
That looks like an expensive housing area.

143
00:11:11,560 --> 00:11:13,680
You can see the distribution quite clearly here.

144
00:11:13,870 --> 00:11:22,450
And we can repeat this per latitude so we can go ahead and changes from long to lat and we can explore

145
00:11:22,450 --> 00:11:23,280
this as well.

146
00:11:23,350 --> 00:11:25,570
And the same behavior seems to pop up.

147
00:11:25,660 --> 00:11:30,650
It also seems that a particular latitude there's some sort of expensive housing area.

148
00:11:30,730 --> 00:11:35,770
And basically what this is telling us is that it looks like at a certain combination of latitude and

149
00:11:35,770 --> 00:11:39,340
longitude that tends to be an expensive area.

150
00:11:39,340 --> 00:11:44,650
So if we just look at a King County map we can begin to discern this.

151
00:11:44,650 --> 00:11:49,510
And what we're gonna do is we can see here essentially the city of Seattle and King County itself.

152
00:11:49,510 --> 00:11:55,210
Let's just plot out latitude versus longitude and plot out all these points and then later we can affect

153
00:11:55,210 --> 00:11:56,350
their hue.

154
00:11:56,350 --> 00:12:01,750
So we're going to come back to our notebook and we can already tell just from this evidence that there

155
00:12:01,750 --> 00:12:06,360
should be some hotspot on a map that has expensive houses.

156
00:12:06,400 --> 00:12:08,370
So we'll come back here and we'll do the following.

157
00:12:08,410 --> 00:12:16,730
I will scatter plot with X as my longitude and Y as my latitude.

158
00:12:16,850 --> 00:12:20,940
And it should be in this order in order for the map to make sense.

159
00:12:20,960 --> 00:12:27,380
Otherwise you'll end up flipping the coordinates of the map and then we'll say also make this larger

160
00:12:28,300 --> 00:12:33,780
penalty figure fig size is equal to twelve by eight.

161
00:12:34,700 --> 00:12:38,820
So this is just a simple scatter plot and we get something that looks like this.

162
00:12:39,050 --> 00:12:45,140
And if we compare this to our actual map of King County we can see that more or less they tend to match

163
00:12:45,140 --> 00:12:45,540
up.

164
00:12:45,590 --> 00:12:51,590
We can see here kind of the shapes of Seattle and we can see here the real map of King County.

165
00:12:51,590 --> 00:12:52,870
So keep that in mind.

166
00:12:53,110 --> 00:12:59,300
And what we're gonna do now is I'm going to start editing this to see if we can actually hone in on

167
00:12:59,300 --> 00:13:07,760
this expensive housing area and one way we can do this is by attempting to say Hugh is equal to price.

168
00:13:08,000 --> 00:13:13,340
And what that is going to do is it's actually going to color these points darker or lighter based off

169
00:13:13,340 --> 00:13:14,030
their price.

170
00:13:14,030 --> 00:13:19,220
And I can begin to see a little bit here of a darker area and it looks like it's actually matching up

171
00:13:19,610 --> 00:13:22,450
with our original estimates of the expensive longitude.

172
00:13:22,460 --> 00:13:25,940
So notice at negative one to two point two but keep going up.

173
00:13:25,940 --> 00:13:27,740
I eventually hit these darker points.

174
00:13:27,740 --> 00:13:34,730
Same at around forty seven point six which is kind of what we expected based off this latitude mapping.

175
00:13:34,910 --> 00:13:41,660
However it looks like I'm not getting quite a color gradient as I would like and that's because of those

176
00:13:41,660 --> 00:13:47,080
really expensive outlier housing as well as the fact that we still have a marker edge here.

177
00:13:47,150 --> 00:13:52,500
So let's see if we can actually clean up this map a little bit by maybe dropping some of these outliers.

178
00:13:52,610 --> 00:14:01,530
So we're going to do is I'm going to take a look at my data frame and I'm going to sort the values based

179
00:14:01,530 --> 00:14:07,130
off price and I will say ascending is equal to False.

180
00:14:07,290 --> 00:14:15,230
And let me just check out the top 20 most expensive houses and loops that should be sought values.

181
00:14:15,260 --> 00:14:16,390
There we go.

182
00:14:16,430 --> 00:14:22,310
So know that in my top 20 houses my most expensive house in this dataset is seven point seven million

183
00:14:22,310 --> 00:14:23,170
dollars.

184
00:14:23,240 --> 00:14:27,560
And as I keep going down you'll notice that it eventually kind of quickly drops off to something more

185
00:14:27,560 --> 00:14:29,240
reasonable like three point six.

186
00:14:29,420 --> 00:14:35,300
And if we take a look at our distribution of the prices of these houses it looks like I should probably

187
00:14:35,300 --> 00:14:40,160
have some reasonable cutoff at three million dollars because it looks like almost just that there's

188
00:14:40,160 --> 00:14:45,070
only 20 houses here that are above three million or maybe a little bit more than that.

189
00:14:45,080 --> 00:14:51,830
So something I can do is I can just sample out the maybe top one percent of all houses.

190
00:14:51,890 --> 00:14:59,600
So if I take a look at the link for my data frame I right now have around 21000 houses in my data frame

191
00:14:59,900 --> 00:15:07,840
which means 1 percent of this is 215 houses which is actually quite a lot of houses.

192
00:15:07,880 --> 00:15:16,790
So let's go ahead and create another data frame and we will call this non top 1 percent or you could

193
00:15:16,790 --> 00:15:19,280
relabel this as bottom ninety nine percent.

194
00:15:19,520 --> 00:15:25,400
And what we're gonna do here is we'll take this same data frame that we created the F sought values

195
00:15:25,400 --> 00:15:26,420
price ascending

196
00:15:29,150 --> 00:15:36,620
and what I will do is I will grab everything after the top 1 percent of houses which essentially means

197
00:15:37,310 --> 00:15:41,140
starting index integer location to 16.

198
00:15:41,150 --> 00:15:43,610
Go ahead and grab everything beyond that.

199
00:15:43,640 --> 00:15:49,370
So all I'm doing here is I'm grabbing the ninety nine percent bottom of houses so I'm not dropping that

200
00:15:49,370 --> 00:15:54,080
much information I'm only dropping 1 percent of information but hopefully that drops all those really

201
00:15:54,080 --> 00:16:01,570
expensive outlier houses and the reason for that is so I can get a more clear color distribution on

202
00:16:01,570 --> 00:16:03,340
this actual scatter plot.

203
00:16:03,340 --> 00:16:04,380
So we'll come back here.

204
00:16:04,390 --> 00:16:08,180
I have now my bottom 99 percent or non top 1 percent.

205
00:16:08,290 --> 00:16:10,840
And let's go ahead and try this out again.

206
00:16:10,840 --> 00:16:15,620
I'm going to copy and paste this code here.

207
00:16:15,910 --> 00:16:18,850
Except now my data is going to be equal to instead of the F

208
00:16:21,740 --> 00:16:29,780
that non top one percent data frame so I can run this and now I can definitely see a lot clearer color

209
00:16:29,780 --> 00:16:32,790
distribution and I can actually begin to play around with this.

210
00:16:32,810 --> 00:16:40,130
So for example maybe I don't want an edge color I don't want that white edge color I begin to say edge

211
00:16:40,130 --> 00:16:46,390
color is equal to none and since I have so many points stacked on top of each other.

212
00:16:46,640 --> 00:16:50,080
Also say Alpha is equal to zero point two.

213
00:16:50,210 --> 00:16:53,770
And finally I'll go ahead and choose a different color gradient.

214
00:16:54,140 --> 00:17:03,460
So this is something that's kind of totally optional for you but I'll choose a red yellow green color

215
00:17:03,460 --> 00:17:03,990
gradient.

216
00:17:03,990 --> 00:17:09,760
So this is are the y l GM and it's going to go from yellow red to yellow to green.

217
00:17:09,760 --> 00:17:13,490
And that's going to make it a little clearer where the expensive housing is.

218
00:17:13,570 --> 00:17:16,220
So I run this again make sure to check your commas.

219
00:17:16,360 --> 00:17:21,640
You can always copy and paste these lines of code from the notebook but now this is a much better plot

220
00:17:21,730 --> 00:17:27,640
at showing me where the expensive parts of King County is and I can see the distribution really clearly

221
00:17:27,640 --> 00:17:28,280
here.

222
00:17:28,390 --> 00:17:32,800
And you'll also notice that it almost looks like on the edge of the water there tends to be some lighter

223
00:17:32,800 --> 00:17:39,040
points which makes sense because usually a waterfront property is going to be more expensive than inland

224
00:17:39,040 --> 00:17:40,010
property.

225
00:17:40,030 --> 00:17:46,430
So this is a much better distribution and scatter plot map than our original mapping right here.

226
00:17:46,600 --> 00:17:51,250
While this does tell us a little bit of the information we can start playing around with the actual

227
00:17:51,310 --> 00:17:54,080
data frame that we're plotting out now we're still showing here.

228
00:17:54,100 --> 00:17:56,770
Ninety nine percent of all houses.

229
00:17:56,950 --> 00:18:01,750
And those outliers we can kind of assume are going to be somewhere either on the waterfront on this

230
00:18:01,750 --> 00:18:05,590
top Northern Edge or in this expensive area of King County.

231
00:18:05,590 --> 00:18:10,600
So lots of different things you can play around here to actually get better looking plots that are more

232
00:18:10,600 --> 00:18:12,610
informative to the user.

233
00:18:12,610 --> 00:18:17,920
And the other things we can show you here are things like doing a box plot on whether or not something

234
00:18:17,920 --> 00:18:18,820
is on the waterfront.

235
00:18:19,260 --> 00:18:31,970
So it's actually one of our features we can say X equals swaps waterfront and say Y is equal to price

236
00:18:35,110 --> 00:18:41,620
stay our original data frame and here we can see the distribution of prices whether or not they're on

237
00:18:41,620 --> 00:18:42,350
the waterfront.

238
00:18:42,370 --> 00:18:47,380
So it looks like if you are on the waterfront you are more likely to be more expensive which again kind

239
00:18:47,380 --> 00:18:48,270
of makes sense.
