WEBVTT
1
00:00:05.370 --> 00:00:12.190
Welcome back everyone to this lecture on multi class activation functions so previously we just saw

2
00:00:12.190 --> 00:00:15.570
that all the activation functions in the previous lecture.

3
00:00:15.580 --> 00:00:18.060
They make sense for something of a single output.

4
00:00:18.130 --> 00:00:23.440
Either you trying to predict a continuous label or you try to predict something with a binary classification

5
00:00:23.440 --> 00:00:24.400
problem.

6
00:00:24.400 --> 00:00:30.040
That means you either want a zero or 1 or something between 0 and 1 and assign a probability.

7
00:00:30.040 --> 00:00:34.950
But what do we do if we actually have a multi class situation in a multi class situation.

8
00:00:35.050 --> 00:00:41.860
The output layer is actually going to have multiple neurons so keep in mind there's two main types of

9
00:00:41.890 --> 00:00:43.870
multi class situations.

10
00:00:43.960 --> 00:00:47.430
There's the not exclusive class situation and not exclusive.

11
00:00:47.440 --> 00:00:52.480
Basically means that a single data point can actually have multiple classes or categories assigned to

12
00:00:52.480 --> 00:00:57.760
it a mutually exclusive class situation which typically we are learning about machine learning is the

13
00:00:57.760 --> 00:01:04.850
more common type is where you can only have one class per data point so let's go ahead and look at the

14
00:01:04.850 --> 00:01:07.920
activation functions for these two use cases.

15
00:01:07.940 --> 00:01:13.220
So first off for things like non-exclusive classes again that's where a data point can have multiple

16
00:01:13.220 --> 00:01:15.470
classes or categories assigned to it.

17
00:01:15.470 --> 00:01:20.380
So for example let's imagine your data happens to be a photo a photo can have multiple texts.

18
00:01:20.480 --> 00:01:26.030
It could be tagged Beach family vacation etc. So can have multiple classes or categories then assigned

19
00:01:26.030 --> 00:01:30.150
to a single photo for a mutually exclusive class.

20
00:01:30.160 --> 00:01:34.120
That data point can only have one class or category assigned to it.

21
00:01:34.120 --> 00:01:40.180
So for example photos can be Kateri categorized as being in grayscale black and white or full color.

22
00:01:40.320 --> 00:01:45.670
A photo can't really be both at the same time it's either in grayscale black and white or full color.

23
00:01:45.670 --> 00:01:49.780
You can't assign both of those classes to a single data point that wouldn't make sense.

24
00:01:49.780 --> 00:01:57.030
So that's mutually exclusive so we want to do is first figure out how do we actually organize data that

25
00:01:57.030 --> 00:01:58.740
contains multiple classes.

26
00:01:58.740 --> 00:02:05.420
And the easiest way to organize multiple classes is to simply have one output note per class so previously

27
00:02:05.430 --> 00:02:09.860
if we look back at that neural network that we are drawing out and illustrating we thought of that last

28
00:02:09.890 --> 00:02:16.320
output layer as a single node that single node can maybe output a continuous regression value or binary

29
00:02:16.320 --> 00:02:23.550
classification so or 1 or maybe some value between 0 or 1 however let's now expand this outer layer

30
00:02:23.580 --> 00:02:27.290
to work for the case of multi classification.

31
00:02:27.370 --> 00:02:31.420
So what this ends up looking like is we have our input layer at the very beginning.

32
00:02:31.480 --> 00:02:38.260
Then we have some hidden layers and then we have a final output layer where the final output layer is

33
00:02:38.260 --> 00:02:39.960
essentially a neuron per class.

34
00:02:40.240 --> 00:02:47.490
So there is one you're on for class one neuron for class two all the way till and classes so this means

35
00:02:47.490 --> 00:02:50.610
we need to organize categories for this output layer.

36
00:02:50.610 --> 00:02:55.680
We're not gonna be able to feed a neural network category it's like a string red blue or green.

37
00:02:55.710 --> 00:03:00.610
Recall that the neural network is gonna take an x values and those should be numbers.

38
00:03:00.750 --> 00:03:03.480
Then it can apply weights to them and add biases.

39
00:03:03.480 --> 00:03:09.220
You can't really multiply the words red blue or green and often real data points when we have classes.

40
00:03:09.230 --> 00:03:13.830
They're going to be things like string codes sought to figure out how do we actually transform our data

41
00:03:13.830 --> 00:03:20.040
correctly to be able to use neural networks for multi class situations.

42
00:03:20.050 --> 00:03:24.160
So what we can do is we can instead use something known as one hot encoding.

43
00:03:24.220 --> 00:03:27.270
You may also heard this called dummy variables.

44
00:03:27.430 --> 00:03:33.250
So let's take a look at what this looks like for mutually exclusive classes sort of for mutually exclusive

45
00:03:33.250 --> 00:03:37.560
classes you may have something that looks like this you have data points and then a class that belongs

46
00:03:37.560 --> 00:03:38.750
to each data point.

47
00:03:38.800 --> 00:03:43.830
Here we can see data point one as red then two is Green Three is blue all the way until data point n

48
00:03:43.870 --> 00:03:44.890
is red.

49
00:03:44.920 --> 00:03:49.810
Now we're not gonna be able to feed the string color codes red green and blue into our neural network.

50
00:03:49.810 --> 00:03:54.430
So how do we actually organize this especially considering that last output layer.

51
00:03:54.430 --> 00:04:00.040
Well we can do is actually just use binary classification for each class and what we do is when the

52
00:04:00.040 --> 00:04:06.190
building out a matrix so we can see here we see data point one is one for red and then zero for the

53
00:04:06.190 --> 00:04:07.200
rest of those classes.

54
00:04:07.210 --> 00:04:11.650
Then data point to is zero for red one on green and zero for blue.

55
00:04:11.750 --> 00:04:13.960
And you can see how we can essentially expand this idea.

56
00:04:13.990 --> 00:04:21.070
So again this is called One high encoding and other people like call this creating dummy variables now

57
00:04:21.070 --> 00:04:24.250
for not exclusive classes going to be slightly different.

58
00:04:24.250 --> 00:04:29.020
Recall that non-exclusive classes means each data point can actually have multiple classes assigned

59
00:04:29.020 --> 00:04:33.730
to it such as a photo having multiple tags or categories assigned to it.

60
00:04:33.730 --> 00:04:39.550
So we can see here data point one has classes and be assigned to a data point to as a data point three

61
00:04:39.550 --> 00:04:41.570
has CMB etc..

62
00:04:41.710 --> 00:04:48.340
So we just actually performed the same idea except now data points can have a value of one for multiple

63
00:04:48.340 --> 00:04:49.170
categories.

64
00:04:49.210 --> 00:04:50.470
So same idea here.

65
00:04:50.620 --> 00:04:55.950
You're essentially where that data point matches up with a class or category you assign a value of one

66
00:04:56.200 --> 00:05:02.560
every year or else you assign a value of zero essentially transforming or encoding that string information

67
00:05:03.010 --> 00:05:09.810
as one hot essentially showing that one means on and 0 is off.

68
00:05:09.830 --> 00:05:15.000
So now that we have data correctly organized and we're going to be organized and data in one hot including

69
00:05:15.020 --> 00:05:19.910
our data later on we're actually coding only you these Choose the correct classification activation

70
00:05:19.910 --> 00:05:27.480
function that the last output layer should have so if you're dealing with non exclusive then you can

71
00:05:27.480 --> 00:05:32.640
just use the sigmoid function because each neuron will output a value between 0 and 1 indicating the

72
00:05:32.640 --> 00:05:34.970
probability of having that class assigned to it.

73
00:05:36.690 --> 00:05:40.710
So what this looks like is we essentially we can have a neuron for each class.

74
00:05:40.710 --> 00:05:42.170
Again this is not exclusive.

75
00:05:42.180 --> 00:05:45.210
So data points can have multiple classes assigned to them.

76
00:05:45.210 --> 00:05:50.040
Then you just end up putting a sigmoid function recall a sigmoid function also known as the logistic

77
00:05:50.040 --> 00:05:50.730
function.

78
00:05:50.880 --> 00:05:56.730
It can go have values 1 0 and 1 and then all the outputs for those neurons end up lighting up somewhere

79
00:05:56.730 --> 00:05:58.180
between 0 and 1.

80
00:05:58.500 --> 00:06:04.560
And what you end up doing is you just say OK in this particular case this data point has an 80 percent

81
00:06:04.560 --> 00:06:10.230
likelihood of belong in the class 1 as well as having a 20 percent likelihood of having class 2 assigned

82
00:06:10.230 --> 00:06:10.680
to it.

83
00:06:10.920 --> 00:06:13.900
And then maybe a 30 percent likelihood of having class end.

84
00:06:14.220 --> 00:06:19.350
So we would probably just say All right go ahead and assign Class 1 to this data point.

85
00:06:19.350 --> 00:06:24.210
However keep in mind that when we're dealing with not exclusive classes you can see here that let's

86
00:06:24.210 --> 00:06:26.460
say ran another data point and I happen to say.

87
00:06:26.460 --> 00:06:32.540
Class 1 is after going through sigmoid function zero point eight class two zero point six for the non-exclusive

88
00:06:32.550 --> 00:06:33.210
case.

89
00:06:33.210 --> 00:06:38.760
If my cutoff point is at zero point five then I would actually assign here to classes or two categories.

90
00:06:38.760 --> 00:06:44.100
This data point I would say both class one in class two are gonna be assigned to this data point since

91
00:06:44.100 --> 00:06:46.470
I'm dealing with non exclusive classes.

92
00:06:46.470 --> 00:06:52.550
Again the classes on inclusive so data points cannot be assigned multiple categories or multiple classes.

93
00:06:52.620 --> 00:06:58.950
So in this case since class wanted to kind of breach that halfway cross at zero point five we assign

94
00:06:58.950 --> 00:07:02.320
both one and two to this particular data point.

95
00:07:02.370 --> 00:07:07.350
So as I just mentioned for non-exclusive classes where we're using that sigmoid function always keep

96
00:07:07.350 --> 00:07:12.930
in mind that that allows each neuron to output independently of the other classes so it allows for a

97
00:07:12.930 --> 00:07:18.120
single data point fed into the function to have multiple classes assigned to it.

98
00:07:18.140 --> 00:07:23.020
Now what do we do when each data point can actually only have one single class assigned to it.

99
00:07:23.030 --> 00:07:26.630
Well for this there's a really clever soft Max activation function.

100
00:07:27.950 --> 00:07:31.010
So the soft max function looks something like this.

101
00:07:31.010 --> 00:07:37.340
And essentially what you end up doing is recall here that we have Z as the actual inputs into the last

102
00:07:37.400 --> 00:07:42.020
output layer and we're gonna be able to do is with this particular soft max function.

103
00:07:42.020 --> 00:07:47.810
Notice that it actually has IE is equal to 1 then 2 all the way to K and K here stands for the number

104
00:07:47.810 --> 00:07:48.960
of categories.

105
00:07:49.010 --> 00:07:56.360
And essentially what this does is the SFX function calculates the probabilities distribution of the

106
00:07:56.360 --> 00:08:01.960
events over k different events and in this case gauges the different classes we have.

107
00:08:02.000 --> 00:08:06.980
So this functional calculate the probabilities of each target class over all possible target classes

108
00:08:08.260 --> 00:08:09.990
so the range will be 0 to 1.

109
00:08:10.120 --> 00:08:15.370
And what's really important here is when you're using the soft Max activation function the sum of all

110
00:08:15.370 --> 00:08:17.580
the probabilities will be equal to 1.

111
00:08:17.590 --> 00:08:24.400
So that means in the output layer when you're passing in through the soft max function the sum of all

112
00:08:24.460 --> 00:08:27.330
the probabilities in the outlier is equal to 1.

113
00:08:27.340 --> 00:08:31.220
So we end up doing is as the model returns the probabilities of each class.

114
00:08:31.360 --> 00:08:39.760
The target class is chosen by the neuron that has the highest probability so what does this actually

115
00:08:39.760 --> 00:08:40.390
look like.

116
00:08:40.390 --> 00:08:44.200
Well the main thing to keep mind is again we're dealing with mutually exclusive classes.

117
00:08:44.200 --> 00:08:48.550
So for that sort of problem after you apply the soft Max you get an output that looks something like

118
00:08:48.550 --> 00:08:49.270
this.

119
00:08:49.270 --> 00:08:54.820
You have some index position that matches up with those neurons in the output layer and then soft Max

120
00:08:55.240 --> 00:09:00.310
provides probabilities for each class so we can see here for this particular data point we had zero

121
00:09:00.310 --> 00:09:04.630
point one for red zero point six for green and zero point three for blue.

122
00:09:04.630 --> 00:09:10.540
You'll notice that the sum of all those probabilities is equal to 1 which makes sense because the sum

123
00:09:10.720 --> 00:09:13.680
over an entire probability space should be equal to 1.

124
00:09:13.780 --> 00:09:17.770
Because that basically answers a question that there is a 100 percent chance this data point belongs

125
00:09:17.770 --> 00:09:19.750
to one of these classes.

126
00:09:19.960 --> 00:09:24.400
But if we read into this further this basically says there is a 10 percent chance it belongs to red

127
00:09:24.760 --> 00:09:29.010
a 60 percent chance it belongs to green and a 30 percent chance it belongs to blue.

128
00:09:29.080 --> 00:09:34.120
So for this particular case we would choose green as our classification for this mutually exclusive

129
00:09:34.120 --> 00:09:40.150
class and say that this network believes that this particular data point is green and it gives it a

130
00:09:40.150 --> 00:09:41.970
60 percent chance of being green.

131
00:09:43.790 --> 00:09:48.420
And as I noted the probabilities for each class all sum up to 1 which is the highest probability as

132
00:09:48.510 --> 00:09:55.680
our assignment so for a review of the theory we've gone over so far we understand the basic precept

133
00:09:55.680 --> 00:10:00.990
trans how they can expand to a neural network model which stand weights and biases and how those get

134
00:10:00.990 --> 00:10:03.290
passed in with an activation function.

135
00:10:03.360 --> 00:10:07.710
And we also learn about particular activation functions for multi class situations.

136
00:10:07.710 --> 00:10:12.540
However we haven't really discussed how the network actually learns how do we update these weights and

137
00:10:12.540 --> 00:10:15.670
biases in order to improve our performance.

138
00:10:15.690 --> 00:10:19.110
Initially they all start off just random weights and ran the biased values.

139
00:10:19.140 --> 00:10:23.280
Eventually we're gonna figure out how this network is actually going to learn to choose correct weights

140
00:10:23.610 --> 00:10:28.140
and for that we did first learn about cost functions and then we can learn about things like gradient

141
00:10:28.140 --> 00:10:30.060
descent and back propagation.

142
00:10:30.060 --> 00:10:33.060
So let's go ahead and go over cost functions in the next lecture.

143
00:10:33.060 --> 00:10:33.930
I'll see you there.
