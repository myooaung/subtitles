WEBVTT
1
00:00:05.380 --> 00:00:06.610
Welcome back everyone.

2
00:00:06.610 --> 00:00:08.500
Here we are where we left off last time.

3
00:00:08.500 --> 00:00:11.700
We already have our time series generator and our data is scaled.

4
00:00:12.040 --> 00:00:14.040
Let's go ahead and now create the model.

5
00:00:14.140 --> 00:00:16.140
Well first start off of a couple of imports.

6
00:00:16.390 --> 00:00:21.710
We'll save from tensor flow that carries the models.

7
00:00:21.870 --> 00:00:26.790
We will import the sequential model that we've always been working with and then we'll import our specific

8
00:00:26.790 --> 00:00:38.020
layers from tensor flow that cares that layers will import dense and Ellis.

9
00:00:38.040 --> 00:00:42.530
Now we're only using one feature in our time series essentially the previous sales data.

10
00:00:42.750 --> 00:00:45.990
So the number of features right now is just equal to one.

11
00:00:46.210 --> 00:00:53.430
And let's create our model we'll see a model is equal to sequential and then we're going to add in our

12
00:00:53.430 --> 00:00:55.040
else Tam layer.

13
00:00:55.260 --> 00:00:59.310
And typically you can go pretty large with the number of neurons in this Ellis Tam layer we'll go ahead

14
00:00:59.310 --> 00:01:05.130
and do 100 and then we can also play around with things like activation functions.

15
00:01:05.190 --> 00:01:07.550
So previously we left a default activation function.

16
00:01:07.560 --> 00:01:08.810
That's also something you can do.

17
00:01:09.060 --> 00:01:13.260
But I do want to show you that if you do want to edit the activation function for an A-list him you

18
00:01:13.260 --> 00:01:17.680
can say activation and set it to something like rectified linear unit.

19
00:01:17.700 --> 00:01:20.970
So that's another aspect of an Alice TAM that you can play around with.

20
00:01:20.970 --> 00:01:27.270
And if you do shift tab here to check out the doc string you can also do lots of other things like set

21
00:01:27.360 --> 00:01:29.700
initialize hours regularization et cetera.

22
00:01:29.730 --> 00:01:33.280
And we'll talk about regularization and constraints later on in the course.

23
00:01:33.300 --> 00:01:36.610
And what's really nice here is you can also set dropouts OK.

24
00:01:36.660 --> 00:01:41.490
So right now we're keeping things simple this dataset is actually pretty straightforward so we don't

25
00:01:41.760 --> 00:01:45.140
need anything fancy to get a pretty good forecast.

26
00:01:45.210 --> 00:01:48.870
The thing that we do need to make sure that we get correct here and this is something that you don't

27
00:01:48.990 --> 00:01:54.120
have options for as the input shape the input shape should be the length of the batch by the number

28
00:01:54.120 --> 00:02:02.040
of features and then after that we also need to make sure that we add a final dense layer to have one

29
00:02:02.040 --> 00:02:09.540
final predictive output and let's go ahead just compile all this will choose the optimizer to be Adam.

30
00:02:09.610 --> 00:02:16.390
So we'll say optimizer is equal to an atom optimizer and the loss should be for a continuous variable

31
00:02:16.480 --> 00:02:21.820
since we're working on continuous sales data and that will go ahead and be mean squared error.

32
00:02:22.390 --> 00:02:27.430
And then let's get our model summary and there's our sequential model.

33
00:02:27.680 --> 00:02:28.480
Okay.

34
00:02:28.780 --> 00:02:33.710
Now as I mentioned before recurrent neural networks often take a really long time to train and they

35
00:02:33.710 --> 00:02:35.290
take many epochs for training.

36
00:02:35.410 --> 00:02:40.200
So it be nice here is to create an early stopping mechanism so I can just set an arbitrarily high number

37
00:02:40.200 --> 00:02:44.430
of epochs and let tensor flow do the work of figuring out when to stop training.

38
00:02:44.440 --> 00:02:53.200
We'll do that just as we did before by saying from tensor flow that carries that callbacks import early

39
00:02:53.200 --> 00:03:01.150
stopping and then let's create our early stop callback and that will be early stopping and what we're

40
00:03:01.150 --> 00:03:07.540
going to be monitoring here is our validation loss and for this particular case especially for current

41
00:03:07.540 --> 00:03:08.350
neural networks.

42
00:03:08.500 --> 00:03:13.180
I would recommend just having a little more patients than the default.

43
00:03:13.180 --> 00:03:18.100
So recall that if we take a look at what patients actually represents it's a number of epochs we want

44
00:03:18.100 --> 00:03:21.970
to wait beyond after which no improvement.

45
00:03:22.030 --> 00:03:26.960
So it's a number of epochs of no improvement after which training will be stopped recurrent narrow networks

46
00:03:26.980 --> 00:03:29.960
can definitely be pretty noisy in the first couple of epochs.

47
00:03:30.040 --> 00:03:34.810
So it's probably a good idea to not have the default patients is zero because it's very likely to end

48
00:03:34.810 --> 00:03:36.300
training probably a little too soon.

49
00:03:36.700 --> 00:03:44.570
So I will go ahead and say wait for at least two epochs OK so we have our early stop and it's time to

50
00:03:44.840 --> 00:03:51.120
create our validation generator recall since we're running fit generator.

51
00:03:51.190 --> 00:03:57.250
The validation set that's going to be passed in during training also needs to be a generator.

52
00:03:57.250 --> 00:04:01.990
So we'll see our validation generator is just before a time series generator.

53
00:04:01.990 --> 00:04:04.720
Except now it's on the scaled test data.

54
00:04:04.990 --> 00:04:10.720
So the skill test data is bought is both the source of our X and or Y the length should be the same

55
00:04:11.440 --> 00:04:16.020
as our training set and the batch size is going to be the same as well.

56
00:04:16.090 --> 00:04:16.510
Okay.

57
00:04:16.600 --> 00:04:20.950
So it's essentially a carbon copy of our previous time series generator except this time we're going

58
00:04:20.950 --> 00:04:22.600
to be passing in this test data.

59
00:04:22.600 --> 00:04:27.220
That way we can then have our validation lost during training and early stop if we need to.

60
00:04:27.310 --> 00:04:29.590
Then the last step is to fit our generator.

61
00:04:29.590 --> 00:04:36.670
So we'll save it generator passin generator which is our training generator and then we'll go and choose

62
00:04:36.670 --> 00:04:39.730
a large number of epochs hopefully doesn't need to train for that many.

63
00:04:39.910 --> 00:04:45.600
And then our validation data is equal to the validation generator that we just created.

64
00:04:45.700 --> 00:04:50.320
And then finally we want to make sure that we don't forget to add in our callbacks which is going to

65
00:04:50.320 --> 00:04:53.900
allow us to commit to an early stop here.

66
00:04:53.950 --> 00:04:55.220
So go ahead and run that.

67
00:04:55.240 --> 00:04:59.000
Make sure you don't get any typos here and there we can see it's running.

68
00:04:59.200 --> 00:05:02.650
So on my computer it took about eight epochs of training.

69
00:05:02.650 --> 00:05:05.560
So I will fast forward the time until this is done training.

70
00:05:05.610 --> 00:05:05.910
Okay.

71
00:05:05.920 --> 00:05:07.120
Finished training my model.

72
00:05:07.120 --> 00:05:10.710
So this time it took a little further and it went to eleven epochs.

73
00:05:10.720 --> 00:05:12.550
Let's go ahead and visualize the losses

74
00:05:15.440 --> 00:05:25.570
we can do this by saying the data frame and then passing and model history that history and then simply

75
00:05:25.570 --> 00:05:30.980
say losses plot and we can see here are loss in our validation loss.

76
00:05:31.080 --> 00:05:37.650
So it looks like we got quite a bit of noise and then we went back down to our original value that was

77
00:05:37.860 --> 00:05:40.120
kind of our minimum here around eight epochs.

78
00:05:40.140 --> 00:05:43.240
So as I mentioned in the beginning the training can be quite noisy.

79
00:05:43.260 --> 00:05:47.340
So it's probably better to have a little more patience now too much patience and you'll probably start

80
00:05:47.340 --> 00:05:49.440
getting too much noise and and too late.

81
00:05:49.440 --> 00:05:52.040
So keep in mind it is a bit of a balance there.

82
00:05:52.040 --> 00:05:52.510
OK.

83
00:05:52.650 --> 00:05:56.430
So let's actually see on some test data how he performed.

84
00:05:56.430 --> 00:06:02.490
So to do this or we're going to do is I'm going to copy and paste the same for loop that we created

85
00:06:02.670 --> 00:06:03.720
last time.

86
00:06:03.810 --> 00:06:08.730
So if you have any questions on this make sure to check out the previous lecture but we're using the

87
00:06:08.730 --> 00:06:12.660
exact same for loop that we used in the previous training lecture.

88
00:06:12.660 --> 00:06:13.670
So I'm going to go over it here.

89
00:06:13.680 --> 00:06:18.960
But again the full detailed explanation is in the previous lecture but I have my test predictions going

90
00:06:18.960 --> 00:06:21.810
to start off as an empty list and then I need to figure out.

91
00:06:21.840 --> 00:06:23.230
Well my first training batch.

92
00:06:23.250 --> 00:06:24.710
What's it going to look like.

93
00:06:24.750 --> 00:06:29.200
It's the very last batch points from my scaled training set.

94
00:06:29.220 --> 00:06:32.860
Recall that the length of my batches is 12 months.

95
00:06:33.000 --> 00:06:38.970
So I'm going to grab what is essentially the last 12 months of my training set to predict one month

96
00:06:39.030 --> 00:06:43.670
into the future which by definition then is the first point in my test set.

97
00:06:43.680 --> 00:06:49.170
So that's my first evaluation batch that I need to make sure to reshape it to the correct format that

98
00:06:49.170 --> 00:06:55.380
is expected by my recurrent neural network because I'm passing in just one item per batch and then it's

99
00:06:55.450 --> 00:06:58.240
12 months by one y feature.

100
00:06:58.350 --> 00:07:04.130
So I reshape it to that format and then I have my current batch then what I'm going to do is for the

101
00:07:04.130 --> 00:07:05.890
entire length of my test set.

102
00:07:05.900 --> 00:07:09.440
In this case I get the prediction one timestamp ahead.

103
00:07:09.440 --> 00:07:11.270
So zero is just grabbing that number.

104
00:07:11.300 --> 00:07:13.380
So I predict one timestamp ahead.

105
00:07:13.460 --> 00:07:18.680
I get my current prediction I append my current prediction to test predictions and then when I'm going

106
00:07:18.680 --> 00:07:25.760
to do is this indexing right here drops the very first value in that current batch and then I append

107
00:07:25.880 --> 00:07:31.100
my current prediction value essentially moving the current batch one month into the future.

108
00:07:31.100 --> 00:07:34.870
And as we keep moving into the future I'm have more and more predictions.

109
00:07:34.910 --> 00:07:37.130
Essentially predicting off predictions.

110
00:07:37.160 --> 00:07:39.270
So that's what a true forecast is.

111
00:07:39.560 --> 00:07:45.890
Let's go ahead and run this and again I would recommend that you just directly get this from our notebooks

112
00:07:45.890 --> 00:07:52.850
really easy to make typos here and then what we'll do is we have our test predictions values but recall

113
00:07:52.880 --> 00:07:53.750
they're scaled.

114
00:07:53.750 --> 00:07:56.580
So we need to do is inverse that transformation.

115
00:07:56.690 --> 00:08:05.000
So we'll say scalar inverse transform on my test predictions and set that to I'll call my true predictions.

116
00:08:05.000 --> 00:08:07.500
Basically my predictions at the correct scaling.

117
00:08:07.790 --> 00:08:18.660
So we run that we get our test and I'll say my predictions is equal to my true predictions and you may

118
00:08:18.660 --> 00:08:23.430
get a warning here it's basically just telling you hey you're overwriting test that already exists you

119
00:08:23.430 --> 00:08:28.740
can completely ignore this warning there's actually a whole caveat link to the documentation on this

120
00:08:28.740 --> 00:08:34.750
warning but you can completely ignore it it's totally OK and now when we check out our test we have

121
00:08:34.840 --> 00:08:38.180
our true sales data and then our predictions.

122
00:08:38.200 --> 00:08:43.480
Let's go ahead and see how these lineup on a plot to see how far off we are.

123
00:08:43.480 --> 00:08:49.460
So say test plot fake size is equal to twelve by eight.

124
00:08:49.540 --> 00:08:50.800
Go ahead and run that.

125
00:08:50.920 --> 00:08:54.700
And if we take a look our predictions they look pretty good.

126
00:08:54.700 --> 00:09:00.900
Granted they're not perfect but given the fact that at the end we're essentially forecasting off forecasts.

127
00:09:00.910 --> 00:09:07.360
The general seasonality trends do tend to line up and keep in mind that this network doesn't actually

128
00:09:07.360 --> 00:09:11.710
have a true understanding of things like December is a holiday.

129
00:09:11.710 --> 00:09:13.390
So there should be a lot of clothing sales.

130
00:09:13.600 --> 00:09:21.050
Instead it's learning based off the previous cycles what the trends should be so it's kind of impressive

131
00:09:21.060 --> 00:09:25.300
the fact that the network is able to perform this well at all.

132
00:09:25.310 --> 00:09:25.670
OK.

133
00:09:25.730 --> 00:09:29.970
So another thing you can experiment with is trying to use the default activation functions of Alice

134
00:09:29.970 --> 00:09:35.180
Tams instead of setting it to a rectified linear unit and see if that improves your results.

135
00:09:35.180 --> 00:09:39.590
So there's lots of different options you can play with removing or adding more neurons with LSD and

136
00:09:39.590 --> 00:09:41.090
layers et cetera.

137
00:09:41.210 --> 00:09:45.310
But let's go ahead and try to forecast into the unknown future.

138
00:09:45.530 --> 00:09:52.190
So what we'll do is we'll retrain on all our data and then again we'll do our scaling do a retraining

139
00:09:52.310 --> 00:09:54.110
and then we'll do a true forecast.

140
00:09:54.110 --> 00:09:59.240
So right now recall that these blue values are values that we actually know the true results for.

141
00:09:59.240 --> 00:10:05.540
So if we decide that Hey this is good enough to essentially forecast values then what we do into the

142
00:10:05.540 --> 00:10:07.190
future.

143
00:10:07.190 --> 00:10:11.910
So we'll say forecast actually before we do that when you actually start scaling our data.

144
00:10:12.080 --> 00:10:20.540
So I will say my full scalar is equal to men Max scalar and I will scale my entire dataset will say

145
00:10:20.630 --> 00:10:26.790
my full scale data is equal to full scalar dot.

146
00:10:26.860 --> 00:10:33.280
And here I can fit transform to all my data since the quote unquote forecast section doesn't really

147
00:10:33.280 --> 00:10:34.280
exist for me.

148
00:10:34.390 --> 00:10:37.200
So say fit transform def.

149
00:10:37.580 --> 00:10:41.250
OK so now my data frame is fully transformed.

150
00:10:41.260 --> 00:10:46.210
So now I have the scale the full data we'll use the same batches before say length is equal to twelve

151
00:10:47.710 --> 00:10:56.480
and then say generator here is equal to a time series generator and then we'll pass on my skillful data

152
00:10:57.540 --> 00:11:04.570
it in twice because it's both the source of our X and our y the length is equal to length and then the

153
00:11:04.570 --> 00:11:09.300
batch size is equal to one okay.

154
00:11:09.300 --> 00:11:10.830
So there's our new generator.

155
00:11:10.950 --> 00:11:14.100
It's time to add in our model.

156
00:11:14.100 --> 00:11:20.530
So we create a sequential model I can add in assuming that I was satisfied with the model before.

157
00:11:20.550 --> 00:11:23.020
Let's go ahead and just use the exact same one.

158
00:11:23.250 --> 00:11:24.530
And I'm not going to make claims here.

159
00:11:24.550 --> 00:11:26.700
That's the best model possible.

160
00:11:26.760 --> 00:11:28.610
In fact I would encourage you to play around with it.

161
00:11:28.650 --> 00:11:31.440
But the results were pretty reasonable given our plot.

162
00:11:31.440 --> 00:11:40.850
So we'll go ahead and just say input shape is equal to length by and features and then we'll say dance

163
00:11:40.970 --> 00:11:45.440
is one and we'll go ahead and compile this.

164
00:11:45.440 --> 00:11:54.170
So say optimizer is equal to Adam and then say loss is equal to mean squared error and then we'll say

165
00:11:54.170 --> 00:11:56.450
model fit generator

166
00:11:59.330 --> 00:12:04.870
is equal to generator and now I have to choose a number of epochs.

167
00:12:05.120 --> 00:12:11.340
Now a really common question I get from students at this phase is how many epochs should I be choosing.

168
00:12:11.390 --> 00:12:13.670
And why can't I just do an early stop.

169
00:12:13.670 --> 00:12:20.010
Call back again when I'm forecasting into the unknown future after retraining on my entire dataset.

170
00:12:20.210 --> 00:12:25.460
And it's a bit tricky to answer but let's just think about what we want to do recall if we come back

171
00:12:25.550 --> 00:12:32.060
up here the way the early stopping works is it evaluates your validation loss.

172
00:12:32.170 --> 00:12:38.890
Recall that that validation generator is validating your model and evaluating your model on data that

173
00:12:38.890 --> 00:12:41.220
the model does not get to train on.

174
00:12:41.230 --> 00:12:46.780
However by definition for this forecast we're training on the full data.

175
00:12:47.320 --> 00:12:53.050
So there is no chunk of data that we're setting aside that is fair game for evaluation because the model

176
00:12:53.050 --> 00:12:55.450
gets to see all the data at once.

177
00:12:55.450 --> 00:13:02.260
So it's a bit of a catch 22 because if you tried to implement an early stopping right now by passing

178
00:13:02.260 --> 00:13:08.410
in some sort of validation generator it's not going to be a very evaluation because you'll have to actually

179
00:13:08.410 --> 00:13:13.110
use some portion of data that your model is being trained on.

180
00:13:13.240 --> 00:13:18.430
And then if you decide to actually set that data apart then you're basically doing what we did before

181
00:13:18.520 --> 00:13:21.480
which was forecasting into a known test set.

182
00:13:21.580 --> 00:13:26.650
And what we really want to do is take advantage now of our full dataset assuming that we're satisfied

183
00:13:26.770 --> 00:13:29.050
by the performance against the test set.

184
00:13:29.200 --> 00:13:34.330
And this is what makes time series forecasting different than typical regression models that we saw

185
00:13:34.390 --> 00:13:38.410
earlier such as the regression model for pricing House data.

186
00:13:38.410 --> 00:13:43.600
It's the fact that we're forecasting into the unknown future so we should be using all the data right

187
00:13:43.600 --> 00:13:49.360
up to the present day which then doesn't really allow us to save anything for a test set for some sort

188
00:13:49.360 --> 00:13:50.860
of early stopping mechanism.

189
00:13:51.010 --> 00:13:57.850
Which is why it's so important to essentially do that step during the testing phase.

190
00:13:57.850 --> 00:14:02.410
And because of that you should be choosing a number of epochs essentially based off this chart right

191
00:14:02.410 --> 00:14:06.730
here and it looks like based off this chart we didn't have to train up to 10 epochs.

192
00:14:06.730 --> 00:14:10.570
We got some sort of minimum on that validation lost around 8 epochs.

193
00:14:10.690 --> 00:14:13.210
So that's what I'm going to go ahead and choose 8.

194
00:14:13.330 --> 00:14:18.490
And again we're not really able to initiate an early stopping here without essentially cheating.

195
00:14:18.490 --> 00:14:22.090
So that was the whole purpose of this phase right here.

196
00:14:22.130 --> 00:14:27.820
They basically initiating the early stopping being satisfied by our validation loss being satisfied

197
00:14:27.820 --> 00:14:31.960
by the performance of the model and then choosing that as our epochs here.

198
00:14:32.980 --> 00:14:38.860
OK so we'll go ahead and run that it's going to fit our generator and I'm going to fast forward in time

199
00:14:38.860 --> 00:14:42.190
until this is done fitting OK.

200
00:14:42.220 --> 00:14:44.890
This is now finished training for those eight epochs.

201
00:14:44.890 --> 00:14:48.850
The next step that we're going to do is forecast into the actual future.

202
00:14:48.910 --> 00:14:50.440
I'm just going to copy and paste a loop.

203
00:14:50.440 --> 00:14:53.470
This is the exact same loop that we did for a Test Evaluation.

204
00:14:53.470 --> 00:14:59.440
The main thing to notice here is I essentially renamed forecast to test predictions and really the real

205
00:14:59.440 --> 00:15:01.850
change here is this period's argument.

206
00:15:01.900 --> 00:15:06.980
Previously we were only forecasting into the length of the test set which happened to be 12 months.

207
00:15:07.030 --> 00:15:12.910
We can now theoretically forecast one month in the future so one period to the future 12 periods into

208
00:15:12.910 --> 00:15:19.630
the future 18 periods to the future etc. The one things keep in mind however is the longer your forecast

209
00:15:19.630 --> 00:15:25.910
length the more more noise you're going to introduce because essentially you're forecasting off predictions.

210
00:15:25.930 --> 00:15:31.240
So you're basically adding in more and more predictions and each prediction is going to carry with it

211
00:15:31.300 --> 00:15:33.530
a little bit of noise from the ground truth.

212
00:15:33.760 --> 00:15:39.470
And essentially you if you try to forecast 100 years as the future it's going to be very very noisy.

213
00:15:39.520 --> 00:15:42.730
So just keep that in mind that there is a limitation here.

214
00:15:42.730 --> 00:15:48.910
And typically your forecast length should be the exact same length as your initial test length so it's

215
00:15:48.910 --> 00:15:54.100
probably a good idea that the number of periods is going to be equal to your initial batch length because

216
00:15:54.100 --> 00:15:56.020
that's really what your models designed to do.

217
00:15:56.710 --> 00:16:02.140
OK so we'll go ahead and run this and again we're forecasting into the unknown future and then we're

218
00:16:02.140 --> 00:16:04.640
going to invert scale are forecasting.

219
00:16:04.660 --> 00:16:14.500
So say forecast dot excuse me forecast is equal to scalar that inverse transform on our forecast and

220
00:16:14.500 --> 00:16:16.420
now we have our new forecast values.

221
00:16:16.420 --> 00:16:22.800
So here's our original data frame going from nineteen ninety two all the way to 2019 in October.

222
00:16:22.840 --> 00:16:30.010
And if we take a look now at our actual forecast these are now the next series of values.

223
00:16:30.010 --> 00:16:33.400
So this is going into what is essentially 2020.

224
00:16:33.430 --> 00:16:37.210
Now we don't know if these values are true or not but that's our forecast for them.

225
00:16:37.210 --> 00:16:41.860
The tricky part is we want to be able to add in these values with a timestamp.

226
00:16:41.890 --> 00:16:43.420
So how do we actually do this.

227
00:16:43.420 --> 00:16:49.150
Luckily for us pandas has built in tools for this exact thing and we're going to build out a forecast

228
00:16:49.150 --> 00:16:50.080
index.

229
00:16:50.560 --> 00:16:56.420
And this is one of the main differences from our simple time series prediction on just the sine wave.

230
00:16:56.560 --> 00:17:04.630
So what we're going to do is we'll create that date range index by saying PD that date range and then

231
00:17:05.050 --> 00:17:09.430
we have to choose our starting date and the starting they shouldn't be the very next month off of this.

232
00:17:09.430 --> 00:17:14.430
So one period into the future if you're dealing with daily data then it would be one day into the future.

233
00:17:14.460 --> 00:17:17.770
But essentially what should the index value for this very first point be.

234
00:17:17.770 --> 00:17:20.650
Well it should be 20 19 eleven 1.

235
00:17:20.770 --> 00:17:25.700
So go ahead and say 20 19 11 0 1.

236
00:17:25.780 --> 00:17:27.910
So it starts at the very next month.

237
00:17:27.910 --> 00:17:33.500
And then how many periods are we actually going to predict for.

238
00:17:33.510 --> 00:17:37.620
Well how many periods was this We read the find that appear as twelve.

239
00:17:37.680 --> 00:17:43.330
So we just say periods is equal to periods so that makes that easy.

240
00:17:43.490 --> 00:17:48.530
And then the last parameter reads to tell pandas is with what frequency should we be creating this date

241
00:17:48.530 --> 00:17:49.170
range.

242
00:17:49.250 --> 00:17:54.560
Because if we start at 20 19 and we want to go 12 periods into the future we to pandas.

243
00:17:54.560 --> 00:17:56.150
Well what is a period is a period.

244
00:17:56.150 --> 00:17:57.290
Twelve days the future.

245
00:17:57.290 --> 00:18:03.160
Twelve hours twelve months etc. and we want the frequency to be a monthly start.

246
00:18:03.380 --> 00:18:09.620
So the string code for that is capital M S and if you're wondering well how do we actually figure out

247
00:18:09.620 --> 00:18:10.790
these string codes.

248
00:18:10.790 --> 00:18:15.140
The easiest way to do it is if you actually just google search pandas frequency strings.

249
00:18:15.290 --> 00:18:18.830
So here and a new tab I went ahead and search pandas frequency strings.

250
00:18:18.830 --> 00:18:23.210
There is a link to the documentation but the very first link is actually a stack overflow.

251
00:18:23.300 --> 00:18:27.650
So if you click on that it has the link and they're called offset aliases.

252
00:18:27.650 --> 00:18:30.080
So he has the link here to the official documentation.

253
00:18:30.080 --> 00:18:36.170
Well what's nice is they actually just set out a table here so we can see here that you can do weekly

254
00:18:36.170 --> 00:18:42.770
frequency monthly end frequency etc. We're doing month start frequency because if you notice we're always

255
00:18:42.800 --> 00:18:45.160
starting here at the first of the month.

256
00:18:45.170 --> 00:18:49.190
So that's the 0 1 which means the frequency code here should be monthly start.

257
00:18:49.190 --> 00:18:54.770
So you'll have to determine your frequency based off this dataset so there our forecast next.

258
00:18:54.770 --> 00:18:56.660
Make sure we spell that correctly.

259
00:18:56.660 --> 00:18:57.860
So we have our forecast index.

260
00:18:57.860 --> 00:18:59.370
Let's go ahead and run it.

261
00:18:59.410 --> 00:19:03.420
If I take a look at my forecast index now I have these points.

262
00:19:03.420 --> 00:19:08.430
So it's starting at 2019 11 0 1 going all the way to October of 2020.

263
00:19:08.430 --> 00:19:14.370
Now let's create a data frame that combines our forecast values with that forecast the next by saying

264
00:19:14.370 --> 00:19:20.780
forecast data frame is equal to PD data frame.

265
00:19:20.930 --> 00:19:27.990
That data is equal to our forecast the index is equal to that forecast index we just created.

266
00:19:27.990 --> 00:19:31.050
And let's go ahead and give the columns the name forecast

267
00:19:35.880 --> 00:19:37.470
forecast data frame.

268
00:19:37.470 --> 00:19:38.480
And there it is.

269
00:19:38.490 --> 00:19:40.700
So let's go ahead and plot these out.

270
00:19:40.890 --> 00:19:48.060
So you could plot these out separately by saying forecast data frame plot and the F plot and it will

271
00:19:48.060 --> 00:19:49.230
create two plots for you.

272
00:19:49.230 --> 00:19:53.410
One of your original sales data and then our forecast into the future.

273
00:19:53.640 --> 00:20:00.900
And if you want to see these on the same plot well you can do is you can say axes is equal to the F

274
00:20:00.900 --> 00:20:10.470
plot and then just say forecast def go ahead and plot on the same axes and that will combine these plots

275
00:20:10.470 --> 00:20:11.760
together onto the same plot.

276
00:20:12.150 --> 00:20:18.480
And if you want to zoom in then you simply zoom in using P.L. T X limb except this time instead of passing

277
00:20:18.480 --> 00:20:22.240
in a straight tick values you can actually pass on dates.

278
00:20:22.260 --> 00:20:24.030
So let's go ahead and zoom in.

279
00:20:24.030 --> 00:20:30.540
Starting from 2018 0 1 0 1 to the end of our dataset which was towards 2020.

280
00:20:30.570 --> 00:20:34.450
So we say comma go to just go to the end of 2020.

281
00:20:34.660 --> 00:20:42.300
12 01 run that and now we can see the zoom in and we can see right here that the forecast actually looks

282
00:20:42.300 --> 00:20:43.160
pretty good.

283
00:20:43.200 --> 00:20:48.250
And the reason there's no link here is because this is technically two separate data frames.

284
00:20:48.330 --> 00:20:52.470
You could concatenate these to make them into one data frame but then pandas wouldn't color the forecast

285
00:20:52.470 --> 00:20:54.070
differently from the sales data.

286
00:20:54.070 --> 00:20:57.030
And this does not look totally unreasonable.

287
00:20:57.030 --> 00:21:01.530
If you're just to show this is to a normal person and say this this forecast makes sense given the previous

288
00:21:01.530 --> 00:21:02.810
patterns it definitely does.

289
00:21:02.820 --> 00:21:04.380
It looks like pretty good.

290
00:21:04.380 --> 00:21:07.620
Then the question is well how good is this forecast.

291
00:21:07.620 --> 00:21:11.910
Unfortunately there's no true way of knowing that until the 20 20 months.

292
00:21:11.910 --> 00:21:17.270
So the only way to truly evaluate this forecast that wasn't valuing your model on the previous test

293
00:21:17.270 --> 00:21:23.880
set is to just wait it out and then basically see how well you performed in to 2020 which is basically

294
00:21:23.910 --> 00:21:29.160
the whole point of forecasting there is no value you can truly compare this to that will be correct.

295
00:21:29.160 --> 00:21:29.400
All right.

296
00:21:29.880 --> 00:21:34.710
So that's forecasting on true time series data coming up next we'll go ahead and test your new skills

297
00:21:34.830 --> 00:21:36.060
on an exercise.

298
00:21:36.060 --> 00:21:36.510
I'll see you there.
