WEBVTT
1
00:00:05.340 --> 00:00:06.450
Welcome back everyone.

2
00:00:06.450 --> 00:00:11.220
We're going to continue where we left off last time on the data pre processing working of missing data.

3
00:00:11.220 --> 00:00:12.650
Let's head back to that notebook.

4
00:00:12.660 --> 00:00:12.930
OK.

5
00:00:12.950 --> 00:00:14.140
Here we are back to the notebook.

6
00:00:14.340 --> 00:00:19.320
Let's go ahead and continue of the tasks first asking us to review the title column versus the purpose

7
00:00:19.320 --> 00:00:24.360
column and see if this is repeated information which is kind of a strong hint that it probably is.

8
00:00:24.570 --> 00:00:25.610
But go on and check this out.

9
00:00:25.620 --> 00:00:27.550
Will say DLF purpose.

10
00:00:27.570 --> 00:00:30.950
Let's go ahead and just check out what that actually looks like.

11
00:00:31.110 --> 00:00:35.820
It looks like this information on the purpose of the loan whether it was for vacation debt consolidation

12
00:00:36.110 --> 00:00:41.350
and we can always check this out by saying feature info on purpose.

13
00:00:41.400 --> 00:00:45.930
Run that and we'll be a category provided by the borrower themselves for the loan request.

14
00:00:46.320 --> 00:00:52.550
OK so we have that information and we can also check out the same for title by saying title dot head

15
00:00:53.220 --> 00:00:53.960
run that.

16
00:00:54.000 --> 00:00:57.360
And here we can see vacation debt consolidation et cetera.

17
00:00:57.360 --> 00:01:04.200
And let's go ahead and check out the feature information for a title run that and this is the loan TITLE

18
00:01:04.200 --> 00:01:05.310
PROVIDED BY THE BAR.

19
00:01:05.310 --> 00:01:08.610
So essentially we have both a category and a title.

20
00:01:08.670 --> 00:01:13.800
So the title column is simply a string or subcategory description of the purpose.

21
00:01:13.830 --> 00:01:18.840
So it makes sense to actually just drop the actual title column because if we take a look at some of

22
00:01:18.840 --> 00:01:25.710
these examples on the title column some of them are just basically the same information as what was

23
00:01:25.710 --> 00:01:26.780
on purpose.

24
00:01:26.820 --> 00:01:37.770
So we'll go in and drop that will say IDF is equal to DFT drop title along axis is equal to 1.

25
00:01:37.880 --> 00:01:43.730
OK so the next part here is one of the hardest parts of the project because it's filling in missing

26
00:01:43.730 --> 00:01:49.520
data based on the values of another column in the data frame.

27
00:01:49.550 --> 00:01:53.480
So I would highly recommend again refer to the solutions not book for getting stuck on this one which

28
00:01:53.540 --> 00:01:54.500
is why you're here.

29
00:01:54.620 --> 00:01:56.630
But let's go ahead and walk through this.

30
00:01:56.630 --> 00:02:01.600
The task is to find out what this Mort's underscore HCC feature represents.

31
00:02:01.640 --> 00:02:02.720
And if you run Ft.

32
00:02:02.720 --> 00:02:10.850
Information on this that is the feature information function on Mort's underscore HCC run that reports

33
00:02:10.850 --> 00:02:11.180
back.

34
00:02:11.180 --> 00:02:14.090
This is the number of mortgage accounts that people have.

35
00:02:14.090 --> 00:02:24.240
OK so it's go and create a value counts of that by saying def mortgage accounts and say value underscore

36
00:02:24.570 --> 00:02:26.220
counts here.

37
00:02:26.220 --> 00:02:31.680
So you run that and we'll realize here right now that it looks like the majority of people have zero

38
00:02:31.680 --> 00:02:35.630
other mortgage accounts and it looks like that's almost 25 percent of our data.

39
00:02:35.730 --> 00:02:40.910
And then it goes to 1 2 etc and some of these are kind of interesting extreme values that someone here

40
00:02:40.920 --> 00:02:43.250
this dataset somehow is 34 mortgage accounts.

41
00:02:43.320 --> 00:02:47.700
But we're going to leave those in now the main thing we're actually trying to figure out here is what

42
00:02:47.700 --> 00:02:49.760
do we actually do with this column.

43
00:02:50.010 --> 00:02:56.880
If we take a look back at how many missing values we have in this column almost 10 percent of all our

44
00:02:56.880 --> 00:03:03.220
values have something missing from this mortgage account number.

45
00:03:03.630 --> 00:03:05.490
So that means we cannot drop the rose.

46
00:03:05.490 --> 00:03:08.040
Otherwise we'd be dropping 10 percent of our data.

47
00:03:08.040 --> 00:03:13.880
So the other option is do we drop the actual feature itself.

48
00:03:13.890 --> 00:03:15.390
Well that one's up for the bait.

49
00:03:15.390 --> 00:03:17.210
There really is no right answer here.

50
00:03:17.250 --> 00:03:18.680
It's not unreasonable to drop it.

51
00:03:18.690 --> 00:03:22.620
But you're also not missing that much of the data you're only missing 10 percent.

52
00:03:22.620 --> 00:03:26.400
So we want to figure out is there a way we can fill in this data.

53
00:03:26.400 --> 00:03:31.110
So this is one of the hardest things to do if missing data is to figure out a reasonable way to try

54
00:03:31.110 --> 00:03:32.250
to fill it in.

55
00:03:32.250 --> 00:03:38.010
So one approach is to try to figure out which of these other features that we have all the information

56
00:03:38.010 --> 00:03:45.090
for correlates highly with this mortgage accounts and see if we can use that to fill in our information

57
00:03:45.180 --> 00:03:51.230
off of it we'll scroll back down and basically follow the tasks which attempt to guide you along.

58
00:03:51.230 --> 00:03:55.790
So again many ways we could all of this missing data or we're gonna do is we'll go ahead and review

59
00:03:55.790 --> 00:04:00.620
which columns are most highly correlated to our mortgage accounts column.

60
00:04:00.620 --> 00:04:05.120
So you want to check what is the correlation of the mortgage account column for all our current numerical

61
00:04:05.120 --> 00:04:07.290
columns to do this.

62
00:04:07.290 --> 00:04:17.920
We simply say F and then we'll go dot correlation and then just check the correlation against the mortgage

63
00:04:17.920 --> 00:04:26.460
account column and I will sort these values so I run that and I get the results here and of course mortgage

64
00:04:26.460 --> 00:04:28.680
accounts perfectly correlates of mortgage accounts.

65
00:04:28.700 --> 00:04:33.750
But what's interesting is they have this very similar other column total accounts and it's not a perfect

66
00:04:33.750 --> 00:04:38.990
correlation which means it's not duplicate data but it does have pretty good positive correlation.

67
00:04:39.150 --> 00:04:45.440
So you should be able to see from the series that this total account feature correlates with the mortgage

68
00:04:45.440 --> 00:04:50.160
count feature and that pretty much makes sense intuitively that the total accounts would be correlated

69
00:04:50.160 --> 00:04:51.940
to the number of mortgage accounts.

70
00:04:51.960 --> 00:04:55.350
So let's go out and try this with a feel and a approach.

71
00:04:55.440 --> 00:05:01.950
So we're going to do is we're going to group the data frame by total accounts and calculate the mean

72
00:05:01.950 --> 00:05:09.860
value for the mortgage accounts per total account entry to get the result that looks like this below.

73
00:05:09.870 --> 00:05:14.910
So basically what we're doing here is the total account we're going to group by it and then figure out

74
00:05:14.940 --> 00:05:21.120
what is the mean value there of the mortgage account column per total account category and then we'll

75
00:05:21.120 --> 00:05:29.040
be using this particular average to fill in the mortgage account column or missing in information.

76
00:05:29.070 --> 00:05:32.820
So it's a bit complicated right now but let's break it down into steps right now.

77
00:05:32.820 --> 00:05:37.380
I have missing data in my mortgage account column and what I want to do is figure out a reasonable way

78
00:05:37.590 --> 00:05:40.130
to fill it in based off this total account column.

79
00:05:40.260 --> 00:05:52.530
So I will do is say take my data frame grouped by the total account column and then take the average

80
00:05:52.530 --> 00:05:59.800
value for the total account column or when I group buy this so I can see the average loan amount per

81
00:05:59.800 --> 00:06:01.140
total number of total accounts.

82
00:06:01.150 --> 00:06:03.900
The average interest rate per total number of accounts here.

83
00:06:03.910 --> 00:06:08.650
So these are the averages across these different categories of total accounts.

84
00:06:08.680 --> 00:06:13.960
So then what I want is I'm really just as shown in filling in my mortgage account information so I'll

85
00:06:13.960 --> 00:06:22.880
grab that and now I can see the average mortgage account value per total account grouping.

86
00:06:22.960 --> 00:06:28.900
So here I can see that the average mortgage accounts if you only have two total accounts is going to

87
00:06:28.900 --> 00:06:30.020
be zero.

88
00:06:30.100 --> 00:06:36.280
So it makes sense that if I have a row or I'm missing the mortgage account information I will use this

89
00:06:36.280 --> 00:06:42.550
series as a look up and replace that missing mortgage account value based off the total account value

90
00:06:42.550 --> 00:06:46.540
that I know is not missing and fill it in with the average value there.

91
00:06:46.540 --> 00:06:47.870
So this is not unreasonable.

92
00:06:47.950 --> 00:06:51.060
So it will go ahead and do is figure out how we can do this.

93
00:06:51.250 --> 00:06:58.300
So I'm going to set this equal to a variable that I can then reference will say this is equal to total

94
00:06:58.900 --> 00:07:06.100
underscore account underscore averages or means is equal to this so we run that.

95
00:07:06.100 --> 00:07:08.990
So we're going to fill in the missing account values here.

96
00:07:09.010 --> 00:07:14.180
There's actually a helpful link here on how to do this but one way to do it is through a function call.

97
00:07:14.200 --> 00:07:15.760
So I'm gonna build out a function

98
00:07:18.820 --> 00:07:24.550
that's going to be called Fill in mortgage account and it takes in to values it takes in that person's

99
00:07:24.580 --> 00:07:29.800
total account value as well as that person's mortgage account value.

100
00:07:30.280 --> 00:07:32.880
And what I'm going to do is I'll do the following.

101
00:07:33.250 --> 00:07:40.620
If you can use P that is Nan to check if we have a missing value.

102
00:07:40.960 --> 00:07:45.810
So if I'm missing a value I'm going to return something.

103
00:07:45.830 --> 00:07:48.560
We'll just keep that commented for right now.

104
00:07:48.560 --> 00:07:55.040
However the easier one is if I'm not missing any mortgage account value I'll simply return the current

105
00:07:55.040 --> 00:07:55.850
mortgage cash value.

106
00:07:56.600 --> 00:07:59.750
So what if I am missing this mortgage can't value what I want to do.

107
00:08:00.260 --> 00:08:08.760
So what I want to do here is basically do a lookup call off the total account averages paste that in

108
00:08:09.150 --> 00:08:17.130
and then look up what that person's number of total accounts in and that will fill it in with this averaged

109
00:08:17.760 --> 00:08:20.460
mortgage account value off of this.

110
00:08:20.530 --> 00:08:23.510
Okay so that's what this function is going to do.

111
00:08:24.030 --> 00:08:31.320
And then I can simply apply it so I can say def apply and I can do this either a function call for land

112
00:08:31.320 --> 00:08:38.160
expression so do land that save a little bit of typing space but basically this lambda or we're going

113
00:08:38.160 --> 00:08:44.680
to do is call our fill mortgage account and this is actually directly lifted off this helpful link.

114
00:08:44.690 --> 00:08:50.430
So if you check out that helpful link the actual expression if you scroll down it's shown here for you

115
00:08:50.490 --> 00:08:51.590
section various ways.

116
00:08:51.600 --> 00:08:54.810
But here's the expression we were using.

117
00:08:54.810 --> 00:09:01.190
We'll come back here and fill it out as shown there will say x of total accounts

118
00:09:04.230 --> 00:09:14.280
and the X here of the mortgage accounts and we're doing this along axis is equal to one

119
00:09:18.250 --> 00:09:21.820
and we're going to apply that and then we'll set that equal to

120
00:09:24.600 --> 00:09:25.880
the mortgage account column.

121
00:09:25.910 --> 00:09:28.820
This is again one of the probably hardest things we do.

122
00:09:29.010 --> 00:09:32.130
Throughout this lecture series on the solutions.

123
00:09:32.130 --> 00:09:33.900
So let me break it down one more time.

124
00:09:33.900 --> 00:09:36.960
You can go and skip ahead if you understood what's happening here.

125
00:09:36.960 --> 00:09:41.640
But what we wanted to do is we need to figure out a way to fill in the missing values of this mortgage

126
00:09:41.640 --> 00:09:44.300
account column 10 percent of people were missing it.

127
00:09:44.340 --> 00:09:48.780
So we can't just drop those rows and it looks like it may be important enough to actually keep it as

128
00:09:48.780 --> 00:09:49.620
a feature.

129
00:09:49.650 --> 00:09:54.270
So if we decide not to drop that column and not drop that Rose we have to fill in the data.

130
00:09:54.270 --> 00:09:55.470
So how can we do that.

131
00:09:55.470 --> 00:10:00.130
Well we figured out what actual other features are highly correlated with mortgage accounts.

132
00:10:00.240 --> 00:10:02.850
It looks like total accounts is pretty well correlated.

133
00:10:02.850 --> 00:10:04.130
So go ahead and use that.

134
00:10:04.380 --> 00:10:10.200
And then we're going to do is based off total accounts we go ahead and group by total accounts and figure

135
00:10:10.200 --> 00:10:14.750
out what's the average mortgage account value for the different groupings of the total account breached

136
00:10:14.760 --> 00:10:16.160
total account category.

137
00:10:16.200 --> 00:10:22.410
Now we're going to use that as our lookup then if we go through the data frame and we realize hey we're

138
00:10:22.410 --> 00:10:24.420
missing the mortgage account value.

139
00:10:24.570 --> 00:10:30.170
Go ahead and look up the average value for that mortgage count based off their total account.

140
00:10:30.240 --> 00:10:35.400
Otherwise if it's not missing does return back the current value and that is then applied here based

141
00:10:35.400 --> 00:10:41.280
off the information off this helpful link of how to apply a function that basically is a function of

142
00:10:41.280 --> 00:10:43.230
two columns in a pan this data frame.

143
00:10:43.260 --> 00:10:45.520
That's what that Stack Overflow post is answering.

144
00:10:45.750 --> 00:10:49.470
So we'll go ahead and run this and depending on your computer this actually may take a little bit of

145
00:10:49.470 --> 00:10:53.810
time because it is doing calculations for every single row in the dataset.

146
00:10:54.180 --> 00:10:59.940
But once you've done that we should be able to confirm that it's been filled in by saying the F is no

147
00:11:00.780 --> 00:11:08.220
some run that you should notice now that the mortgage account column is now at a 0 which leaves us with

148
00:11:08.220 --> 00:11:14.520
just two left on missing data which is this rival you till and public record bankruptcies.

149
00:11:14.520 --> 00:11:20.220
However this is such a small percentage of our overall data set at most five hundred and thirty five

150
00:11:20.580 --> 00:11:22.230
that we might as well just drop those rows.

151
00:11:22.230 --> 00:11:23.390
Why don't you drop these features.

152
00:11:23.400 --> 00:11:29.340
We can just drop these 500 and so people that were missing their public record bankruptcies and these

153
00:11:29.400 --> 00:11:32.420
270 so people that were missing this particular feature.

154
00:11:32.580 --> 00:11:38.620
So to do that we simply say the f is equal to the F drop N A.

155
00:11:38.820 --> 00:11:46.090
And now if you're on the F is null some you should get back the result that you have no more missing

156
00:11:46.090 --> 00:11:46.590
data.

157
00:11:46.870 --> 00:11:47.470
OK.

158
00:11:47.710 --> 00:11:50.410
So that's it for the missing data part of the pre processing.

159
00:11:50.410 --> 00:11:55.540
Definitely when the hardest parts of this project and this is essentially what we just went through.

160
00:11:55.540 --> 00:11:59.950
Coming up next we're going to see how to deal of categorical variables and dummy variables.

161
00:11:59.950 --> 00:12:01.730
OK thanks and I'll see you there.
