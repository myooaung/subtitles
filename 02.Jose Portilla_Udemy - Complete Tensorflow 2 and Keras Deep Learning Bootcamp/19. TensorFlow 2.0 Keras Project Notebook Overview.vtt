WEBVTT
1
00:00:05.310 --> 00:00:06.580
Welcome back everyone.

2
00:00:06.690 --> 00:00:11.700
In this lecture we're going to give you an overview of the Project for the section of the course which

3
00:00:11.700 --> 00:00:17.530
is using sensor fool 2.0 Care's API to perform a classification task.

4
00:00:17.580 --> 00:00:20.760
Now this is one of the largest projects in the entire course.

5
00:00:20.760 --> 00:00:25.290
So let's go ahead and take the time to actually do an overview of what's in the notebook.

6
00:00:25.290 --> 00:00:25.520
All right.

7
00:00:25.560 --> 00:00:31.230
So if you open up the folder that contains R and a notebook and scroll down you'll see two project related

8
00:00:31.230 --> 00:00:31.830
notebooks.

9
00:00:31.830 --> 00:00:37.860
The first one is Kerry's project exercise and then the next one is Kerry's project exercise solutions.

10
00:00:37.860 --> 00:00:42.390
Right now we're just going to give you an overview of the actual exercise notebook and then coming up

11
00:00:42.510 --> 00:00:45.540
through the next series of lectures we'll be walking through the solutions.

12
00:00:46.080 --> 00:00:51.870
So go ahead and open up the cares project exercise and we're going to be using a subset of a lending

13
00:00:51.870 --> 00:00:55.830
club data set obtained from Kaggle and you can check out the link here.

14
00:00:55.860 --> 00:01:02.200
But keep in mind we recommend that you don't download the full zip file that's actually from this link.

15
00:01:02.250 --> 00:01:07.110
We provide a special version of this file that actually has some extra feature engineering for you to

16
00:01:07.110 --> 00:01:07.400
do.

17
00:01:07.770 --> 00:01:13.470
So we've added some extra features for you to then use to get practice on feature engineering and working

18
00:01:13.470 --> 00:01:15.080
with realistic looking data.

19
00:01:15.080 --> 00:01:16.080
To this real data.

20
00:01:16.080 --> 00:01:18.800
And we've actually added more information to a subset of it.

21
00:01:19.180 --> 00:01:19.760
OK.

22
00:01:19.890 --> 00:01:24.610
So lending club if you're not familiar with it it's basically a U.S. peer to peer lending company it's

23
00:01:24.780 --> 00:01:26.800
headquartered in service Sysco California.

24
00:01:26.880 --> 00:01:32.460
And what it does is it gives out loans to people and then they have to pay back that loan with a certain

25
00:01:32.460 --> 00:01:33.460
interest rate.

26
00:01:33.540 --> 00:01:35.850
And sometimes people won't pay off that loan.

27
00:01:35.970 --> 00:01:41.380
And the company is going to have to record that as what's known as a charge off essentially they have

28
00:01:41.370 --> 00:01:43.950
to write off the loan because it wasn't paid back to them.

29
00:01:43.980 --> 00:01:50.640
So the loan status column will contain our label for this particular class and we're going to try to

30
00:01:50.640 --> 00:01:57.930
predict based off historical data and the features of a potential customer potential bar whether or

31
00:01:57.930 --> 00:01:59.870
not they will default on the loan.

32
00:01:59.910 --> 00:02:05.370
So default is known as a charge off or will they fully pay back the loan fully paid.

33
00:02:05.370 --> 00:02:11.010
So keep in mind the classification metrics that you're going to be evaluating when evaluating the performance

34
00:02:11.010 --> 00:02:12.000
of your model.

35
00:02:12.090 --> 00:02:16.010
So the couple of first sells have already been filled in for you.

36
00:02:16.140 --> 00:02:21.510
And since there are actually many Lending Club datasets on Kaggle we have information on this particular

37
00:02:21.510 --> 00:02:24.680
dataset filled out for you in a table here in this cell.

38
00:02:24.870 --> 00:02:27.120
So you can go ahead and check out the various features.

39
00:02:27.120 --> 00:02:29.720
So for instance I anti underscore rate.

40
00:02:29.790 --> 00:02:30.800
Here's a description of it.

41
00:02:30.810 --> 00:02:32.120
It's the interest rate on the loan.

42
00:02:32.130 --> 00:02:36.870
So if the descriptions for the various features that are included in this data set in there this is

43
00:02:36.870 --> 00:02:39.840
quite a large dataset with many interesting features.

44
00:02:39.840 --> 00:02:45.300
So I spent a lot of time visualizing in the exploratory data analysis as well as performing basic feature

45
00:02:45.300 --> 00:02:52.230
engineering and we've filled out just a little bit of starter code for you to read in the file as well

46
00:02:52.290 --> 00:02:55.550
as a Little Feat underscore info.

47
00:02:55.830 --> 00:03:01.050
So you'll notice that we actually have this table and what we did is we built a little function for

48
00:03:01.050 --> 00:03:02.930
you called Ft.

49
00:03:03.060 --> 00:03:09.060
Underscore info which will print out feature information based off the string column you give it.

50
00:03:09.180 --> 00:03:15.780
So for example if you saw the string column Mort's underscore HCC then you could just pass that into

51
00:03:15.780 --> 00:03:16.800
this built in Ft.

52
00:03:16.800 --> 00:03:22.050
Underscore info and it will print back out for you what the actual description is which is number of

53
00:03:22.050 --> 00:03:23.220
mortgage accounts.

54
00:03:23.250 --> 00:03:28.890
So go ahead and run these first couple of cells will read in the file for you and also create this informational

55
00:03:29.220 --> 00:03:34.910
function call and then down here we'll be reading in the actual data for you.

56
00:03:35.040 --> 00:03:37.110
And then it looks something like this.

57
00:03:37.140 --> 00:03:40.040
Notice that there will be missing data that you will have to deal with.

58
00:03:40.140 --> 00:03:42.990
And we essentially guide you along with this project.

59
00:03:42.990 --> 00:03:49.680
So then we actually reach the project tasks section and so Section 1 is exploratory data analysis and

60
00:03:49.680 --> 00:03:52.650
we'll just ask you to be creating some of these visualizations.

61
00:03:52.650 --> 00:03:58.200
And keep in mind you may need to actually Google around and we provide helpful links on certain tasks

62
00:03:58.200 --> 00:04:02.000
that we may not have specifically gone over but you should be able to do them.

63
00:04:02.010 --> 00:04:06.380
Given what we've gone over in the data visualization crass coarse section.

64
00:04:06.480 --> 00:04:11.250
So for example we have a heat map for you to build out and in case you need any help with that we have

65
00:04:11.250 --> 00:04:13.250
links here for heat map information.

66
00:04:13.350 --> 00:04:15.550
Help with resizing et cetera.

67
00:04:15.810 --> 00:04:17.420
You can continue scrolling down.

68
00:04:17.420 --> 00:04:23.190
I want you to create some scatter plots some box plots that we want you to actually follow the instructions

69
00:04:23.190 --> 00:04:29.640
to build out a little bit of some bar plots here that are huge based off the loads that is whether it's

70
00:04:29.640 --> 00:04:31.120
fully paid or charged off.

71
00:04:31.230 --> 00:04:33.390
And there's descriptions here on what you can do.

72
00:04:33.990 --> 00:04:37.160
And then we want you to create a bar plot per category maybe start resizing it.

73
00:04:37.190 --> 00:04:41.580
So look something like this you can change the color palettes as well and et cetera.

74
00:04:41.580 --> 00:04:46.290
So we basically have various plots free to kind of play around with.

75
00:04:46.500 --> 00:04:51.240
And if we scroll down the next section is data pre processing and this has a little less to do with

76
00:04:51.570 --> 00:04:58.020
actually visualizing data and really our goal here is to one remove or fill any missing data.

77
00:04:58.020 --> 00:05:02.940
We want to remove unnecessary repetitive features and then we want to convert categorical string features

78
00:05:03.180 --> 00:05:04.350
to dummy variables.

79
00:05:04.350 --> 00:05:09.600
So we're we're going to be doing a lot of feature engineering and feature analysis so we'll first deal

80
00:05:09.600 --> 00:05:14.440
with the missing data and we have instructions here on for you to kind of follow along and how to deal

81
00:05:14.440 --> 00:05:20.830
with that missing data then after that we'll kind of examine the missing data see what we can keep but

82
00:05:20.840 --> 00:05:21.550
we should fill in.

83
00:05:21.580 --> 00:05:25.610
We should drop and then we'll also start dealing with categorical data.

84
00:05:25.960 --> 00:05:31.030
So if we start coming down here we start dealing categorical data and we assess and challenge tasks

85
00:05:31.030 --> 00:05:37.750
for you to follow along with and then down here we keep dealing with kind of feature by feature basis

86
00:05:38.000 --> 00:05:43.190
etc. And eventually after dealing all this feature engineering you should be ready for a train test

87
00:05:43.190 --> 00:05:49.630
split if you get stuck on any of the feature engineering tasks that we try to lay out for you above

88
00:05:49.630 --> 00:05:51.610
this train test split line.

89
00:05:51.610 --> 00:05:54.290
Go ahead and refer to our solutions notebook.

90
00:05:54.460 --> 00:05:59.020
And keep in mind you can actually just take this dataset and try to build a model on your own without

91
00:05:59.020 --> 00:06:02.610
necessarily following along with our exact steps that we've laid out here.

92
00:06:02.620 --> 00:06:07.210
The main goal this is to build a model that attempts to predict the actual label class.

93
00:06:07.210 --> 00:06:13.120
This is the binary classification so you perform your train test split and then Optionally you can grab

94
00:06:13.150 --> 00:06:19.660
a sample for training time so the sample contains about three hundred ninety five thousand entries.

95
00:06:19.690 --> 00:06:25.450
If you're doing a smaller Ram computer or something that's not GP you we have a little sample code here

96
00:06:25.450 --> 00:06:27.400
to take a fraction of the data set the train on.

97
00:06:28.200 --> 00:06:28.800
OK.

98
00:06:28.930 --> 00:06:34.060
So after you perform your train test split you want to normalize the data then you create your model

99
00:06:34.070 --> 00:06:41.200
we have some imports for you here you can create whatever model you feel necessary but we recommend

100
00:06:41.310 --> 00:06:46.200
or the solution actually uses 78 neurons the thirty nine neurons the 19 year olds the one you're on.

101
00:06:46.240 --> 00:06:47.230
This is totally optional.

102
00:06:47.230 --> 00:06:48.980
You essentially have unlimited options.

103
00:06:49.120 --> 00:06:53.230
You can mess around with adding and drop out layers etc. and we have links for you here to check out

104
00:06:53.290 --> 00:06:54.170
as well.

105
00:06:54.610 --> 00:06:59.320
So I would recommend adding in some dropout layers and in the solutions notebook you can check that

106
00:06:59.320 --> 00:07:00.250
out as well.

107
00:07:00.250 --> 00:07:06.340
We have this particular network structure with a zero point two or 20 percent dropout for each of those

108
00:07:06.340 --> 00:07:08.890
layers to try to prevent overfishing.

109
00:07:08.920 --> 00:07:13.400
They want you to fit that model for at least twenty five epochs and then you'll keep going here.

110
00:07:13.540 --> 00:07:18.220
Save the model evaluate your model performance see these plots just we've been doing.

111
00:07:18.220 --> 00:07:22.540
And then finally given this customer below would you offer this person alone.

112
00:07:23.080 --> 00:07:23.710
Okay.

113
00:07:24.010 --> 00:07:27.940
So then you can check against the results and that's the project.

114
00:07:27.940 --> 00:07:32.500
Thanks and we'll see you at the next series of lectures we'll be eventually working out through this

115
00:07:32.560 --> 00:07:33.460
solutions notebook.

116
00:07:33.660 --> 00:07:38.170
If you get stuck anywhere in this project exercise go ahead and just refer to the solutions notebook

117
00:07:38.500 --> 00:07:40.250
that's already filled out for you.

118
00:07:40.270 --> 00:07:40.800
I'll see you there.
