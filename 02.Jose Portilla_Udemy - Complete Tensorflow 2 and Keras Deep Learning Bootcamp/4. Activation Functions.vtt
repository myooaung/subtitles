WEBVTT
1
00:00:05.310 --> 00:00:12.220
Welcome back everyone to this lecture and activation functions so recall from our neural network models

2
00:00:12.550 --> 00:00:19.150
that inputs X have a weight assigned to them w and then we also add in a bias term attached to them

3
00:00:19.270 --> 00:00:25.300
in that perception or neuron model which means as formula we have the input x times there'll be plus

4
00:00:25.300 --> 00:00:32.730
b so clearly if we start thinking about this w just implies how much weight or strength we should be

5
00:00:32.730 --> 00:00:37.920
giving to the incoming input we can almost think of it as kind of how important is that input.

6
00:00:37.950 --> 00:00:43.230
You can imagine that if we have a absolute value of that weight being very large that input or that

7
00:00:43.230 --> 00:00:48.310
feature is probably really important we can also think of B as an offset value.

8
00:00:48.330 --> 00:00:55.650
We mentioned this before but basically that makes it so that X times W has to reach a certain threshold

9
00:00:55.650 --> 00:01:00.150
before having an effect and overcoming that B term.

10
00:01:00.170 --> 00:01:06.800
So for example if we have B is equal to negative 10 then the effects of X times w won't really start

11
00:01:06.800 --> 00:01:11.720
to overcome that B or bias term until their products are passes 10.

12
00:01:11.780 --> 00:01:18.500
So after that then the effect is solely based on that value of W thus the term bias for B.

13
00:01:19.010 --> 00:01:24.440
So you can essentially think of that bias term as a threshold that the neuron has placed in order for

14
00:01:24.440 --> 00:01:30.620
the input times the weight to start taking some sort of majority effect.

15
00:01:30.660 --> 00:01:35.970
So next we want to do is we want to set boundaries for the overall output value of that combination

16
00:01:36.060 --> 00:01:41.700
x times there'll be a plus b and to keep things simple we can do is we can state z is equal to x times

17
00:01:41.700 --> 00:01:49.550
w plus B and then we can pass that term Z through some sort of activation function to limit its value.

18
00:01:49.560 --> 00:01:54.060
Now keep in mind a lot of research has been done into activation functions and their effectiveness.

19
00:01:54.060 --> 00:01:59.100
We're gonna explore some really common activation functions and then also show the wikipedia page for

20
00:01:59.100 --> 00:02:05.860
activation functions which displays and lists a lot more so we're called The Simple neuron or perceptual

21
00:02:05.860 --> 00:02:07.320
model has an F of X..

22
00:02:07.450 --> 00:02:14.330
So we have those X's times are weights plus the bias of that neuron so you can imagine if we had a binary

23
00:02:14.330 --> 00:02:21.870
classification problem we would want an output of either 0 or 1 now as a quick note as I mentioned to

24
00:02:21.870 --> 00:02:28.530
avoid confusion I'm going to find the total inputs as this variable Z raises equal to w x plus B.

25
00:02:28.590 --> 00:02:33.690
So in this context of a neural network and passing in the inputs times a weight plus the bias into an

26
00:02:33.690 --> 00:02:39.490
activation function I'm really going to pass in that Z term into the activation function.

27
00:02:39.510 --> 00:02:43.770
Keep in mind you often see these variables capitalized in the literature such as function of capital

28
00:02:43.770 --> 00:02:45.990
C or function of capital Z.

29
00:02:45.990 --> 00:02:52.080
In regards to some capital X to denote that instead of a single X input you actually have a tensor input

30
00:02:52.110 --> 00:02:57.330
consisting of multiple values which means Z is also a tensor which means you have a tensor of weights

31
00:02:57.420 --> 00:02:58.110
etc..

32
00:02:58.290 --> 00:03:01.940
So don't get confused on the capitalization you may see if you're reading some books.

33
00:03:01.940 --> 00:03:07.140
Another thing that I get confused by often when people are writing out these activation functions it's

34
00:03:07.140 --> 00:03:12.030
just really common practice to write them out in regards to f since writing a function in regards to

35
00:03:12.030 --> 00:03:13.290
X is kind of the default.

36
00:03:13.680 --> 00:03:18.690
So so you may see some activation functions written out in regards to X especially on that wikipedia

37
00:03:18.690 --> 00:03:23.590
page or just in normal literature and books you're reading on deep learning let that confuse you.

38
00:03:23.610 --> 00:03:29.400
We're actually passing in w x plus B in its entirety to the activation functions.

39
00:03:29.550 --> 00:03:34.760
Okay so I just mentioned if we're doing a binary classification problem it would be really nice that

40
00:03:34.760 --> 00:03:37.590
the neuron always spits out either a 0 or 1.

41
00:03:37.820 --> 00:03:43.140
So the absolute simplest networks can rely on a basic step function that outputs 0 or 1.

42
00:03:43.250 --> 00:03:48.210
So all we do is dependent on the value of Z along here we can see the x axis.

43
00:03:48.290 --> 00:03:52.400
What we're gonna do is if it's value of Z is less than zero we are at zero.

44
00:03:52.400 --> 00:03:58.300
If value of C is greater than zero then we output 1 so regardless of the values.

45
00:03:58.300 --> 00:03:59.190
What's nice about this.

46
00:03:59.230 --> 00:04:05.480
It's always going to output 0 or 1 so this sort of function is releasable for classification.

47
00:04:05.480 --> 00:04:12.960
It will always output either 0 or 1 however this is a very quote unquote strong function since really

48
00:04:12.960 --> 00:04:15.070
small changes aren't reflected.

49
00:04:15.270 --> 00:04:19.320
You can see here there's just an immediate cut off that splits between 0 and 1.

50
00:04:19.320 --> 00:04:24.590
If the total output of Z happens to be less than zero we just defined that as zero.

51
00:04:24.630 --> 00:04:29.760
If the total output of Z happened to be greater than zero then when we pass it through the step function

52
00:04:29.820 --> 00:04:31.700
we kind of top it off at 1.

53
00:04:31.710 --> 00:04:36.840
So there is a floor at 0 a ceiling at 1 and the immediate cutoff just depends on what that total value

54
00:04:36.840 --> 00:04:39.600
of Z ended up being.

55
00:04:39.700 --> 00:04:44.710
It would be really nice however if instead of using such a dramatic step function we have a little bit

56
00:04:44.710 --> 00:04:50.180
more of a dynamic function for example that red line so lucky for us.

57
00:04:50.210 --> 00:04:53.510
And you're probably already familiar if this if you'd done any machine learning classes.

58
00:04:53.570 --> 00:04:55.490
This is actually the sigmoid function.

59
00:04:55.610 --> 00:05:01.370
It has the same lower bound and upper bound so 0 and 1 which is useful for binary classification but

60
00:05:01.370 --> 00:05:05.870
it does this in a little more of a moderate fashion than a simple cut off that a step function would

61
00:05:05.870 --> 00:05:06.620
do.

62
00:05:06.620 --> 00:05:12.050
And here we can see the formula for a sigmoid function also known as the logistic function which is

63
00:05:12.110 --> 00:05:18.410
f a Z in our case is equal to 1 divided by one plus E to power of negative Zi where Z.

64
00:05:18.410 --> 00:05:25.910
In our case would be equal to w x plus b so changing the activation function used in your neurons can

65
00:05:25.910 --> 00:05:29.360
be really beneficial depending on the task.

66
00:05:29.390 --> 00:05:34.070
Now keep in mind this still works for classification but what's really nice is it's going to be slightly

67
00:05:34.070 --> 00:05:40.640
more sensitive to small changes and if we want we can actually grab that output the sigmoid value to

68
00:05:40.640 --> 00:05:45.890
then treat it as a probability between zero and one since we can see here that there's actually values

69
00:05:45.920 --> 00:05:51.710
that it's going to output between 0 and 1 instead of solely 0 or solely 1 that red line is going to

70
00:05:51.710 --> 00:05:55.780
be able to report back something like zero point six or zero point four.

71
00:05:55.780 --> 00:06:01.670
We should then give you an idea of how sure the network is that it belongs into any particular class

72
00:06:03.630 --> 00:06:06.810
so let's discuss a few more activation functions that we're going to encounter.

73
00:06:07.770 --> 00:06:13.140
So some really common ones are things like the hyperbolic tangent written out as 10 H and Hebrew can

74
00:06:13.140 --> 00:06:16.090
see the formulas with respect to x on the right hand side.

75
00:06:16.200 --> 00:06:21.960
So there's hyperbolic cosine hyperbolic sine but the most common one is hyperbolic tangent which is

76
00:06:22.230 --> 00:06:26.450
hyperbolic sine divided by hyperbolic cosine.

77
00:06:26.610 --> 00:06:32.400
So what's nice about this it's going to output between negative 1 and 1 instead of 0 to 1 looks really

78
00:06:32.400 --> 00:06:33.810
similar to sigmoid function.

79
00:06:33.810 --> 00:06:39.150
Essentially the main difference is that lower bound that floor and we'll discuss y later on with certain

80
00:06:39.150 --> 00:06:42.960
neurons and certain networks it makes more sense to use a hyperbolic tangent.

81
00:06:43.320 --> 00:06:49.620
I want you to be aware that it's a really common option now in other really common action is the rectified

82
00:06:49.680 --> 00:06:55.920
linear unit shortened to R E L U and this is actually really a relatively simple function.

83
00:06:55.920 --> 00:07:02.190
Basically you can describe that as Max zero comma Z which essentially states that if the output of the

84
00:07:02.190 --> 00:07:05.220
value is less than zero we treat it as zero.

85
00:07:05.220 --> 00:07:12.510
Otherwise if it's greater than zero we go ahead and output the actual z value so rectified linear units

86
00:07:12.750 --> 00:07:17.670
have actually been found to have very good performance especially when dealing with the issue of vanishing

87
00:07:17.670 --> 00:07:23.370
gradient which is a term an issue we'll discuss in more detail in a future lecture because it's so common

88
00:07:23.370 --> 00:07:28.470
to use rectified linear units in literature we're often when we're building our networks going to default

89
00:07:28.830 --> 00:07:36.210
to a rectified linear unit as our activation function due to its overall good performance now for a

90
00:07:36.210 --> 00:07:38.250
full list of possible activation functions.

91
00:07:38.250 --> 00:07:42.100
Highly encourage you to check out the wikipedia page on activation functions.

92
00:07:42.120 --> 00:07:44.860
In fact let's go ahead and take a quick tour of it now.

93
00:07:44.910 --> 00:07:45.090
All right.

94
00:07:45.090 --> 00:07:49.950
Here I am on the wikipedia page for activation functions and we can see here that it actually shows

95
00:07:49.950 --> 00:07:53.320
you the logistic activation function has kind of its main image.

96
00:07:53.320 --> 00:07:58.270
But if you scroll down it talks about functions talks up biologically inspired neural networks.

97
00:07:58.290 --> 00:08:00.460
So here's the image we were showing you earlier.

98
00:08:00.570 --> 00:08:03.180
You can keep going down here some alternative structures.

99
00:08:03.180 --> 00:08:07.050
If you don't want to use an activation function but eventually we scroll down here.

100
00:08:07.240 --> 00:08:12.270
It's so do some things to compare activation functions against such as being long linear the range the

101
00:08:12.270 --> 00:08:15.000
upper bound the lower bounds if they're continuously differential.

102
00:08:15.330 --> 00:08:20.670
But here we can see this little table that displays a lot of commonly used activation functions as well

103
00:08:20.670 --> 00:08:24.540
as their derivative which is going to be important we're talking about things like back propagation.

104
00:08:24.540 --> 00:08:29.040
But the main thing keep mine is really just the name the plot and the equation which should give you

105
00:08:29.040 --> 00:08:32.570
kind of an overview of what the activation functions actually doing.

106
00:08:32.570 --> 00:08:35.760
There is a simple identity which is just whatever then you're on outputs.

107
00:08:35.760 --> 00:08:39.390
That's gonna be what it outputs with the activation function attached to it.

108
00:08:39.390 --> 00:08:44.040
There's the binary step essentially zero if you're less than zero one if you're greater than or equal

109
00:08:44.040 --> 00:08:44.980
to zero.

110
00:08:45.000 --> 00:08:47.510
The logistic otherwise known as a sigmoid or soft.

111
00:08:47.510 --> 00:08:53.220
That also very very common activation function to use especially for binary classification have things

112
00:08:53.220 --> 00:08:57.440
like the hyperbolic tangent which you just mentioned looks a lot like sigmoid except you can see here

113
00:08:57.440 --> 00:09:01.500
at the upper and lower bound specifically lower bound goes to negative one.

114
00:09:01.590 --> 00:09:05.590
There's arc tangent arcs hyperbolic sine lots of there are things here.

115
00:09:05.730 --> 00:09:10.020
Then if you keep scrolling down you see the rectified linear unit that has a whole Wikipedia page and

116
00:09:10.040 --> 00:09:11.220
sit in it.

117
00:09:11.430 --> 00:09:18.030
As I mentioned before it's it's a really common activation function to use so common has its own Wikipedia

118
00:09:18.060 --> 00:09:19.040
article on it.

119
00:09:19.110 --> 00:09:21.540
Then there's kind of more advanced rectified linear unit.

120
00:09:21.560 --> 00:09:26.460
So recently a lot people have been experimenting with different variations of the rectified linear unit

121
00:09:26.820 --> 00:09:28.080
specifically a really common one.

122
00:09:28.080 --> 00:09:35.520
Is this leaky rectified linear unit which instead of kind of flatlining at 0 4 values along the x axis

123
00:09:35.520 --> 00:09:36.670
that are less than zero.

124
00:09:36.750 --> 00:09:39.980
It kind of leaks a little bit and has this really small gradient.

125
00:09:39.990 --> 00:09:42.640
This is actually a little exaggerated here.

126
00:09:42.640 --> 00:09:47.160
You can see it's going to be zero point zero one x 4 x that is less than zero.

127
00:09:47.610 --> 00:09:53.010
So this is known as a leaky rectified linear unit and there's a bunch of other linear units they can

128
00:09:53.010 --> 00:09:53.740
check out here.

129
00:09:53.760 --> 00:09:57.960
So as you can see we have lots of different options here but typically what we're going to be doing

130
00:09:58.080 --> 00:10:04.110
is we'll be using things like sigmoid function the soft max function for multi class classification

131
00:10:04.110 --> 00:10:08.580
which we'll talk about in just a second and then rectified later units because overall they have pretty

132
00:10:08.580 --> 00:10:09.870
good performance.

133
00:10:09.870 --> 00:10:13.100
So keep in mind they encourage you to check out the activation function page here.

134
00:10:13.110 --> 00:10:18.480
This article's really interesting but the other thing I want you to keep in mind is right now we've

135
00:10:18.480 --> 00:10:24.120
only discussed things like binary classification keeping things between 0 and 1 and then maybe we have

136
00:10:24.120 --> 00:10:25.800
some sort of fixed probability there.

137
00:10:25.980 --> 00:10:28.920
But there's another set of issues that we have to kind of think about.

138
00:10:28.980 --> 00:10:34.680
If we're dealing with multi class classification so let's go ahead and discuss multi class classification

139
00:10:34.770 --> 00:10:37.860
activation functions and networks in the next lecture.

140
00:10:37.860 --> 00:10:38.750
I'll see you there.
