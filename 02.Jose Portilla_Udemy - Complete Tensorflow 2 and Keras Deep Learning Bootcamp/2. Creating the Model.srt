1
00:00:05,210 --> 00:00:08,830
Welcome everyone to our first lecture on the deployment series.

2
00:00:08,830 --> 00:00:13,270
So to begin to actually deploy something we need to actually create a model.

3
00:00:13,280 --> 00:00:18,140
So let's head over to first a Jupiter notebook and then we'll create a model there.

4
00:00:18,350 --> 00:00:19,360
Let's get started.

5
00:00:19,400 --> 00:00:19,660
OK.

6
00:00:19,670 --> 00:00:21,670
Here I am at a fresh notebook.

7
00:00:21,770 --> 00:00:25,580
I essentially want to start the process of deployment from the very beginning.

8
00:00:25,610 --> 00:00:32,240
So the first thing you need is a data set and I'm going to run import as an important Empire's MP and

9
00:00:32,240 --> 00:00:38,070
import pandas PD to get started and to use a data set they're all familiar with.

10
00:00:38,090 --> 00:00:44,240
What I'm going to do is reading the iris dataset So recall that the iris dataset is one of these kind

11
00:00:44,240 --> 00:00:49,970
of world famous data sets that everybody always starts off with in machine learning and it has a variety

12
00:00:49,970 --> 00:00:54,170
of features about flowers and then the class they belong to.

13
00:00:54,170 --> 00:01:00,140
So this is a supervised learning problem where we have these four measurements or these four features

14
00:01:00,320 --> 00:01:02,510
and then a species of flower.

15
00:01:02,540 --> 00:01:08,630
And so what I want to do is let's imagine in the future I have a researcher out in the field who picks

16
00:01:08,630 --> 00:01:14,800
a new flower and then takes the simple length and width and then the pedal length and with measurements.

17
00:01:14,990 --> 00:01:20,060
Eventually what I want them to be able to do is to log into a Web site enter those measurements and

18
00:01:20,060 --> 00:01:24,530
have a deep learning model produce a species prediction result.

19
00:01:24,650 --> 00:01:26,590
So that's kind of our final goal here.

20
00:01:26,720 --> 00:01:29,970
But in order to do all this when you start with historical data.

21
00:01:30,200 --> 00:01:34,790
So what's always important when you're deploying your model is think about your dataset as well as what

22
00:01:34,790 --> 00:01:38,350
the final product should look like or what question you're trying to answer.

23
00:01:38,390 --> 00:01:45,020
In our case we're trying to create a model that will given four features of a flower predict what actual

24
00:01:45,020 --> 00:01:47,280
species of Iris flower it is.

25
00:01:47,300 --> 00:01:52,160
So first we need to do is separate this out into features versus target.

26
00:01:52,160 --> 00:01:56,250
So that means my X should be Iris.

27
00:01:56,310 --> 00:02:01,790
I don't need to drop that species column and when he did drop a column so we say access is equal to

28
00:02:01,790 --> 00:02:11,340
1 and now x is just all those features and then what I need to do here is similarly for y I just need

29
00:02:11,340 --> 00:02:13,670
to do the opposite which is just grab the species column.

30
00:02:14,250 --> 00:02:14,630
Okay.

31
00:02:14,660 --> 00:02:20,070
So I have my X and Y and as I mentioned before typically X is capitalized because it's a two dimensional

32
00:02:20,070 --> 00:02:26,850
matrix and Y is lower case because it's just a one day vector what I want to do is I want to now make

33
00:02:26,850 --> 00:02:36,290
sure in order to do multi class classification if we take a look at why the unique values here are three

34
00:02:36,290 --> 00:02:41,360
classes which means in order to actually perform multi class classification with the way we're going

35
00:02:41,360 --> 00:02:45,830
to design our model essentially one neuron per class output.

36
00:02:45,860 --> 00:02:52,310
What I want to do is somehow either one hand code this or use some sort of a dummy variable to convert

37
00:02:52,610 --> 00:02:58,930
my actual y values one way I can do this quite easily is psychic learn so there's lots of different

38
00:02:58,930 --> 00:03:05,750
ways you can do this but secular does have one way we can say from Eskil or in the pre processing import

39
00:03:06,470 --> 00:03:18,790
label binary either run that import and then say encoder is equal to an instance of this label binaries

40
00:03:18,790 --> 00:03:19,530
or.

41
00:03:19,760 --> 00:03:27,500
And essentially what we can do say Y is equal to encoder that fit transform on Y.

42
00:03:27,500 --> 00:03:33,710
And then if I take a look at what y looks like now it's essentially one hot encoded so I have so Tosa

43
00:03:34,070 --> 00:03:40,950
and then Virginia and then Versa color here and later on we'll talk about how you would actually know

44
00:03:40,950 --> 00:03:41,830
the orders there.

45
00:03:41,850 --> 00:03:47,570
It's just the same orders as they're shown in what unique the Tosa versus color and Virginia and this

46
00:03:47,760 --> 00:03:50,940
kind of a more official way to do that which is actually off the encoder itself.

47
00:03:50,940 --> 00:03:57,810
So we'll show how to do that later on but right now we've essentially one high encoded y as needed.

48
00:03:57,810 --> 00:04:01,680
Now it's time to do a trade test split as well a scalar data.

49
00:04:02,040 --> 00:04:08,610
Hopefully this all seems very familiar so we'll say from scalar in that model selection import train

50
00:04:08,610 --> 00:04:15,610
test split and also say from S.K. learned that let's make sure we spell that right from S.K. learn that

51
00:04:15,620 --> 00:04:22,310
pre processing import min max scalar go ahead and run that.

52
00:04:22,440 --> 00:04:28,400
And now we say train to split and I like to do this little shift tab trick to make sure I don't have

53
00:04:28,400 --> 00:04:34,520
to type out too much scroll all the way down until you see that example here and let's go ahead and

54
00:04:34,520 --> 00:04:41,550
grab this guy OK so we have our extreme accessed why train and why test.

55
00:04:41,800 --> 00:04:42,670
Next thing you need to do.

56
00:04:42,730 --> 00:04:44,750
Let me zoom out so we can see the whole thing here.

57
00:04:44,860 --> 00:04:47,320
We just say X and Y.

58
00:04:47,320 --> 00:04:49,720
Let's go ahead and choose the test size of 20 percent.

59
00:04:49,750 --> 00:04:51,630
So zero point two.

60
00:04:51,640 --> 00:04:54,430
And then to make sure you get the exact same brand the split I do.

61
00:04:54,430 --> 00:04:59,220
Go ahead and say random state is equal to 1 0 1 you run that.

62
00:04:59,380 --> 00:05:01,630
And now you've created or train test split.

63
00:05:01,630 --> 00:05:09,370
Next step is to do scaling so we'll say scalar is equal to an instance of this min max scalar we take

64
00:05:09,370 --> 00:05:12,780
our scalar and we fit it only on the training set.

65
00:05:12,860 --> 00:05:19,430
We don't want to assume prior knowledge of our test set run that and now let's create our scaled X train

66
00:05:19,730 --> 00:05:33,310
which is scalar thought transform on X train and now let's create a scaled version of our x test sets

67
00:05:33,330 --> 00:05:40,540
scalar the transform on X test and you only need to scale your feature data loops.

68
00:05:40,590 --> 00:05:43,910
Make sure we say no there.

69
00:05:44,280 --> 00:05:45,480
Run that.

70
00:05:45,540 --> 00:05:47,880
Okay so we have our skilled features.

71
00:05:47,880 --> 00:05:50,190
And recall we only fit the training set.

72
00:05:50,400 --> 00:05:55,140
So later on we have to do is what we're gonna have to actually save this scalar object in order to use

73
00:05:55,140 --> 00:05:56,340
it for new incoming data.

74
00:05:56,340 --> 00:06:01,950
So keep that in mind after this next step is to actually create the model we'll say from tensor flow

75
00:06:03,400 --> 00:06:14,230
that Kerry's top models import sequential and then from tensor flow that carries that layers import

76
00:06:14,230 --> 00:06:15,980
dense.

77
00:06:16,100 --> 00:06:26,200
Let's go ahead and create a very simple model we'll say model is equal to sequential and let's add a

78
00:06:26,200 --> 00:06:32,920
dense layer of four units with an activation function of rectified linear unit.

79
00:06:32,920 --> 00:06:37,630
And here the input shape it's just 4 features and we're actually not going to worry about batches or

80
00:06:37,630 --> 00:06:43,810
anything so just the input shape is for comma we don't need to worry about the other dimensions since

81
00:06:43,810 --> 00:06:49,450
we're not dealing with any to the images and we're also not feeding and things batches so finally after

82
00:06:49,450 --> 00:06:55,750
this we'll say a model and go ahead and edit Dantzler here and this is will be our last layer which

83
00:06:55,750 --> 00:06:58,860
means a number of units should match the number of classes we have.

84
00:06:58,870 --> 00:07:02,320
Recall we have three classes so total of Virginia converse color.

85
00:07:02,470 --> 00:07:07,960
So units is equal to three and then the activation function here because it's a multi class classification

86
00:07:07,960 --> 00:07:08,860
algorithm.

87
00:07:08,860 --> 00:07:13,390
We need this to be soft Max okay.

88
00:07:13,480 --> 00:07:20,270
And then finally we will compile this model and we can do this with the atom optimizer again you have

89
00:07:20,270 --> 00:07:25,550
flexibility overall optimize you want to choose but what loss you should be choosing is categorical

90
00:07:26,640 --> 00:07:30,560
cross entropy.

91
00:07:30,660 --> 00:07:34,110
Now let's go ahead and also keep track of the accuracy and our metrics.

92
00:07:34,110 --> 00:07:43,920
So instead of just loss we'll say accuracy OK and make sure this is just compile without an s to make

93
00:07:43,920 --> 00:07:49,350
sure we don't commit any typos there run that and you should have a model created for you after this

94
00:07:49,410 --> 00:07:55,660
it's time to train the model so nothing you so far will say from terrorists or tense flow that carries

95
00:07:55,680 --> 00:08:01,440
that callbacks we'll go ahead and import early stopping that we don't need to worry about choosing the

96
00:08:01,440 --> 00:08:03,140
right number of epochs.

97
00:08:03,150 --> 00:08:11,570
Go ahead say early stop is equal to early stopping and we are actually going to use most of these defaults

98
00:08:11,570 --> 00:08:13,060
like what we're going to monitor.

99
00:08:13,340 --> 00:08:14,630
Minimum delta et cetera.

100
00:08:14,630 --> 00:08:21,230
Last thing I want to do is I'll have patients be a little larger than 0 so this is a very simple problem.

101
00:08:21,260 --> 00:08:23,070
We're just going to say patients is equal to 10.

102
00:08:23,090 --> 00:08:27,540
That way we can really see that curve flatten out for a bit and now it's time to fit our model will

103
00:08:27,550 --> 00:08:35,030
same model fit you pass an X which in this case is our scaled X train and that's corresponding y is

104
00:08:35,030 --> 00:08:40,070
y train then we'll go ahead and choose just a very large number of epochs since we have that early stopping

105
00:08:40,070 --> 00:08:46,490
mechanism to stop in cases over fitting we'll see epoxy the three hundred and then let's say our validation

106
00:08:46,490 --> 00:08:57,310
data is equal to scaled x test and Y test and then finally we'll go ahead and set our callbacks so callbacks

107
00:08:57,370 --> 00:09:01,260
is equal to early stop that we created earlier.

108
00:09:01,630 --> 00:09:05,990
We'll go ahead and run this and now we can see it's training.

109
00:09:05,990 --> 00:09:09,360
Training shouldn't take that long especially if the early stop.

110
00:09:09,380 --> 00:09:12,050
Let's go ahead and scroll down here.

111
00:09:12,110 --> 00:09:16,180
You may notice that it actually may take close to all the three hundred epochs.

112
00:09:16,190 --> 00:09:20,320
If it's getting better and better performance you should get pretty high accuracy here.

113
00:09:20,360 --> 00:09:24,360
So let's go ahead and then see how it performed on our actual data.

114
00:09:24,380 --> 00:09:33,360
Let's grab first those metrics will say metrics is equal to PD data frame and we can say model history

115
00:09:33,390 --> 00:09:37,500
that history run that.

116
00:09:37,630 --> 00:09:38,090
Take a look.

117
00:09:38,100 --> 00:09:43,230
Your metrics here can see loss accuracy value loss and about accuracy.

118
00:09:43,330 --> 00:09:53,340
Let's go ahead and plot first loss against validation loss run that plot.

119
00:09:53,550 --> 00:09:58,090
Okay so we can see here that both loss and validation loss were continuing to go down.

120
00:09:58,120 --> 00:09:59,800
In fact we could train for even further.

121
00:09:59,800 --> 00:10:02,780
You can see that it was still headed down.

122
00:10:02,780 --> 00:10:13,160
And let's go ahead and check out our accuracy metrics so we track metrics and compare accuracy versus

123
00:10:13,160 --> 00:10:15,880
validation accuracy.

124
00:10:15,980 --> 00:10:22,670
Go ahead and plot these out run that and it looks like in the beginning we were over fitting a little

125
00:10:22,670 --> 00:10:27,730
bit suddenly got a lower performance on accuracy but eventually the model learns what to do.

126
00:10:27,920 --> 00:10:33,410
And it looks like it flatlined for quite a bit from epochs like 100 or 200 but eventually start to learn

127
00:10:33,470 --> 00:10:38,630
a couple of those kind of edge cases till it was getting around 80 percent accuracy.

128
00:10:38,630 --> 00:10:44,180
So if you're running the models just like we did you should have achieved somewhere between 70 to 80

129
00:10:44,180 --> 00:10:49,270
percent accuracy and the worry if your validation accuracy is a little bit lower than ours.

130
00:10:49,280 --> 00:10:51,320
We're getting a pretty good fit on this model.

131
00:10:51,320 --> 00:10:51,930
Okay.

132
00:10:52,070 --> 00:11:00,050
So last thing you can also do as a model dot evaluate just look at that raw metrics data and we can

133
00:11:00,050 --> 00:11:09,490
pass in our scaled x test why test and then say verbose is equal to zero and then it will give you back

134
00:11:09,580 --> 00:11:13,100
the final loss as well as the final accuracy.

135
00:11:13,120 --> 00:11:18,520
So in our case we're getting 80 percent accuracy on a test set of this model.

136
00:11:18,520 --> 00:11:23,500
This is the point in time where you would have to decide if those performance metrics are good enough

137
00:11:23,500 --> 00:11:30,820
for you if we decide that our model is ready for deployment if we decide that 80 percent accuracy is

138
00:11:30,820 --> 00:11:34,680
doable then let's go ahead and ready our model for deployment.

139
00:11:34,750 --> 00:11:37,230
So how do we ready our model for deployment.

140
00:11:37,240 --> 00:11:42,280
Well when I deploy my model I want to make sure that I use all available data.

141
00:11:42,340 --> 00:11:48,880
So for the purposes of training and evaluation I do my train test split when it comes to the real world

142
00:11:49,300 --> 00:11:52,320
for that researcher out in the field who's going to pick that new flower.

143
00:11:52,330 --> 00:11:57,400
I want to make sure that I'm providing the best model possible by using all the data.

144
00:11:57,400 --> 00:12:02,890
So what I'm going to do is the following we'll say epochs is equal to length of metric.

145
00:12:02,890 --> 00:12:07,600
So we'll go ahead just run the same number of epochs recall that was three hundred and then I'm going

146
00:12:07,600 --> 00:12:19,400
to say scaled X is equal to scalar and if I wanted to I could then do a fit transform now on all my

147
00:12:19,400 --> 00:12:19,700
data.

148
00:12:21,560 --> 00:12:24,930
X so notice I'm no longer doing train test split.

149
00:12:25,100 --> 00:12:28,700
I've decided already that this performance is good enough for me.

150
00:12:28,730 --> 00:12:30,290
80 percent accuracy.

151
00:12:30,290 --> 00:12:35,690
So that wasn't the test set that test set recall is data that the model was not trained on.

152
00:12:35,690 --> 00:12:39,830
It's just evaluating it as it goes along now to provide the best model possible.

153
00:12:39,830 --> 00:12:43,410
I'm retraining on all my data so it's kind of a phase two.

154
00:12:43,580 --> 00:12:48,770
And so far in all our lectures we've kind of just been stopping on this evaluation phase when it comes

155
00:12:48,770 --> 00:12:49,590
to deployment.

156
00:12:49,610 --> 00:12:55,880
We want to retrain in everything so that's why I'm doing a fit and the transform on all my X data for

157
00:12:55,880 --> 00:12:59,530
my scaled X so go ahead and run that.

158
00:12:59,580 --> 00:13:03,120
And now let's go ahead and create a new model will say sequential.

159
00:13:03,120 --> 00:13:07,080
In fact that is going to essentially copy paste what we did before.

160
00:13:07,080 --> 00:13:12,650
So scroll up here can go ahead and copy and paste what is already there for us.

161
00:13:12,690 --> 00:13:18,520
Scroll down and now we have our model.

162
00:13:18,600 --> 00:13:27,680
Go ahead and redefine your model and then we're going to fit our model to our scaled X data our y data

163
00:13:28,070 --> 00:13:32,460
and epochs is equal to the previous number of epochs we chose which was three hundred.

164
00:13:32,480 --> 00:13:34,960
Go ahead and fit that model.

165
00:13:34,970 --> 00:13:35,660
Okay.

166
00:13:35,780 --> 00:13:42,140
So once that model is done fitting for those 300 epochs we're now ready to figure out how to save the

167
00:13:42,140 --> 00:13:42,860
model.

168
00:13:42,880 --> 00:13:49,490
So scroll down here we can see if we kind of scroll towards the very bottom how well the models performing.

169
00:13:49,490 --> 00:13:52,010
So it looks like it's getting 96 percent accuracy.

170
00:13:52,010 --> 00:13:57,570
However keep in mind that's the accuracy on all the data that it's being trained on.

171
00:13:57,590 --> 00:14:03,200
So it's not a true fair comparison to say your model is now in ninety six percent accurate it's ninety

172
00:14:03,390 --> 00:14:08,780
six percent accurate when it gets to see all the data train on data and evaluate itself on all that

173
00:14:08,780 --> 00:14:14,300
data the true accuracy measurement that you should be using is this 80 percent accuracy which is a much

174
00:14:14,300 --> 00:14:15,800
fairer evaluation of this model.

175
00:14:15,800 --> 00:14:22,070
So keep that in mind so we'll come down here and then we're going to do is there's two things you need

176
00:14:22,070 --> 00:14:22,920
to save.

177
00:14:22,920 --> 00:14:24,110
We need to save our model.

178
00:14:24,420 --> 00:14:26,400
Luckily we already know how to do that.

179
00:14:26,430 --> 00:14:37,280
So go ahead and say a model save and I'm going to save this as final IRS model dot H5.

180
00:14:37,330 --> 00:14:41,950
Keep in mind this is all the same file name that we save for you but it's OK if you want to rewrite

181
00:14:41,950 --> 00:14:46,480
it and then the next thing to do is save the scalar itself.

182
00:14:46,480 --> 00:14:52,480
New incoming data will have to be scaled in the same way that the model expects it to do this.

183
00:14:52,480 --> 00:14:56,790
Go ahead an import job lib or job library.

184
00:14:56,790 --> 00:15:02,180
This is actually built into Python so you're into download anything to run that and then say job live

185
00:15:02,200 --> 00:15:10,260
that dump and then the way this works is you pass in essentially the object that you want to persist

186
00:15:10,320 --> 00:15:13,110
as an arbitrary python object and then the file name.

187
00:15:13,110 --> 00:15:16,480
Essentially the file path that it's going to go into.

188
00:15:16,500 --> 00:15:24,840
So I want to save my scalar object and I will say this as Iris underscore scalar in the extension it

189
00:15:24,840 --> 00:15:28,330
should be scale if its P K L stands for a pickle file.

190
00:15:28,500 --> 00:15:32,680
Go ahead and run that and I'll basically report back the string name of where it was saved.

191
00:15:32,910 --> 00:15:36,370
OK so I saved this dot H5 file for my model.

192
00:15:36,510 --> 00:15:40,890
And if you end up using scaling for your model you need to say that scalar as well.

193
00:15:40,890 --> 00:15:42,790
Now let's see what this would look like.

194
00:15:42,810 --> 00:15:50,470
If I want to predict on a new single flower so we'll say from tensor flow doctors that models go ahead

195
00:15:50,470 --> 00:15:56,250
and import load model is essentially just to check that everything is working and then I'm going to

196
00:15:56,250 --> 00:16:05,990
create two new objects one is my flower model a new variable which loads up the final Iris model and

197
00:16:06,500 --> 00:16:14,450
I'm also going to say flower scalar and I'm going to say job live the load and I'm going to load up

198
00:16:14,570 --> 00:16:22,570
the iris scalar object as well and now what I should be able to do is based off this model predict something.

199
00:16:22,710 --> 00:16:29,220
So what we're gonna see in essentially part two of this continuation is how to use this new loaded model

200
00:16:29,370 --> 00:16:35,490
and this new loaded scalar object and create a function that takes in this model takes in the scalar

201
00:16:35,580 --> 00:16:42,150
and takes in an example Jason which is what our API is going to be working with when we actually deploy

202
00:16:42,150 --> 00:16:46,520
our model and return back a prediction for the actual classes.

203
00:16:46,530 --> 00:16:52,020
All right so I'll see you in the continuation of our deployment lecture series coming up next.
