WEBVTT
1
00:00:05.440 --> 00:00:06.480
Welcome back everyone to.

2
00:00:06.520 --> 00:00:10.450
Part three of tech generation of Python and cares in step three.

3
00:00:10.450 --> 00:00:15.490
We're going to focus on creating batches that is to say we understand the way the tech sequences are

4
00:00:15.490 --> 00:00:20.920
organized and shifted one character forward and then we'll actually use tensor flows built in data sets.

5
00:00:20.920 --> 00:00:25.300
Class Object to generate those batches and shuffle those batches for us.

6
00:00:25.300 --> 00:00:28.120
Let's head back to the notebook where we left off last time to check this all out.

7
00:00:28.960 --> 00:00:31.480
All right here I am at the notebook we left off last time.

8
00:00:31.570 --> 00:00:36.850
Previously we discovered how we could take our rostering data and encode it so that every character

9
00:00:37.180 --> 00:00:39.910
has a unique integer assigned to it.

10
00:00:39.910 --> 00:00:43.210
Next we need to do is actually create or training sequences.

11
00:00:43.210 --> 00:00:46.470
Those are the batches we're going to be training our model on.

12
00:00:46.600 --> 00:00:50.740
Now the next thing before we actually create those sequences though is to figure out how long should

13
00:00:50.740 --> 00:00:52.310
these sequences be.

14
00:00:52.390 --> 00:00:56.050
Recall that our model is a character based model.

15
00:00:56.050 --> 00:01:05.980
So for example if I were to grab a portion of the text grab the first 500 characters and print that

16
00:01:05.980 --> 00:01:06.580
out.

17
00:01:06.910 --> 00:01:10.420
Here we can see the general structure in this case of a sonnet.

18
00:01:10.450 --> 00:01:18.250
So let's go ahead and just grab a line from this so I'll grab the first line and let's say line is equal

19
00:01:18.250 --> 00:01:22.140
to and we'll just put in that single line.

20
00:01:22.240 --> 00:01:25.860
So how long is one line of text.

21
00:01:26.140 --> 00:01:30.030
So it looks like one line of text is 41 characters.

22
00:01:30.250 --> 00:01:35.140
But we want to do is make sure our training sequences are long enough that they'll actually be able

23
00:01:35.140 --> 00:01:41.110
to pick up the general structure of the text and likely Shakespeare has a very specific structure due

24
00:01:41.110 --> 00:01:42.590
to its iambic pentameter.

25
00:01:42.730 --> 00:01:45.020
As you can see here things tend to rhyme.

26
00:01:45.160 --> 00:01:46.130
Every other line.

27
00:01:46.240 --> 00:01:53.560
Not exactly but more or less we have increase with the C's die and memory fuel and cruel eyes and lies

28
00:01:53.590 --> 00:01:54.590
et cetera.

29
00:01:54.670 --> 00:02:00.120
So we should probably have at least three lines in order to try to understand that structure.

30
00:02:00.250 --> 00:02:08.270
So that should be around if we check out the copy paste here we can see lines and let's go ahead and

31
00:02:08.270 --> 00:02:09.170
put this in.

32
00:02:09.290 --> 00:02:14.150
Using triple quotes here because it's multi line let's check the length of those lines and it looks

33
00:02:14.150 --> 00:02:15.880
like about one hundred thirty three.

34
00:02:15.890 --> 00:02:21.680
So if on average each line is around 40 characters and three lines is one one hundred and thirty three

35
00:02:21.680 --> 00:02:27.030
characters probably let's go ahead and choose our sequence length of one hundred and twenty characters.

36
00:02:27.230 --> 00:02:30.740
Kind of a smaller set of three lines.

37
00:02:30.740 --> 00:02:36.680
Keep in mind your actual character sequences will depend on the actual structure of your text data and

38
00:02:36.800 --> 00:02:40.680
what trying what you're trying to achieve structure wise with your character.

39
00:02:40.680 --> 00:02:46.130
Current neural network but in our case in twenty characters seems to have enough information to actually

40
00:02:46.130 --> 00:02:52.250
capture everything but not so much that it's an it take a really long training time so we're gonna choose

41
00:02:52.250 --> 00:02:59.090
sequence length of 120 and then we're going to do here is calculate the total number of sequences in

42
00:02:59.090 --> 00:03:00.000
our text.

43
00:03:00.020 --> 00:03:08.740
So for this we can say the total number of sequences instead of text is merely the length of that text

44
00:03:08.750 --> 00:03:14.710
the full text and then we're going to use two forward slashes here so that it's classic division instead

45
00:03:14.740 --> 00:03:21.420
of truth vision essentially rounding this off and then we'll say sequence length plus one because of

46
00:03:21.420 --> 00:03:23.280
zero indexing here.

47
00:03:23.450 --> 00:03:30.350
Go ahead run that and we should see that we have about forty five thousand sequences total so let's

48
00:03:30.350 --> 00:03:36.350
go ahead and create the training sequences and we can do this with these special data set object from

49
00:03:36.350 --> 00:03:37.590
tensor flow.

50
00:03:38.190 --> 00:03:47.720
So I will say character data set is equal to and we say TAF data dot data set and then we're going to

51
00:03:47.720 --> 00:03:54.350
be constructing this from tensor slices on our encoded text not on the original text

52
00:03:57.810 --> 00:04:01.310
OK so go ahead and run that may take a little bit of time.

53
00:04:01.310 --> 00:04:05.840
The pin in your computer but if you took out the type of characters that essentially this is a special

54
00:04:06.020 --> 00:04:07.940
tensor slice data.

55
00:04:08.270 --> 00:04:14.540
And with that if we check out character data set and then do that tab you'll see that it has a bunch

56
00:04:14.540 --> 00:04:16.050
of method calls off of it.

57
00:04:16.250 --> 00:04:20.910
So one of them that's the most convenient we're going to be using is this batch call.

58
00:04:20.930 --> 00:04:26.780
So the batch method converts these individual character calls into sequences we can feed in as a batch

59
00:04:28.890 --> 00:04:31.390
so let me show you what that actually looks like.

60
00:04:31.410 --> 00:04:41.050
We'll say for item in character data set and then we're call this take method and this take method basically

61
00:04:41.050 --> 00:04:45.420
just creates a data set with at most count elements from this dataset.

62
00:04:45.430 --> 00:04:54.070
So we'll say 500 and then we'll go ahead and print those items but those items ought to be prints and

63
00:04:54.070 --> 00:04:55.550
it'll be converted to num pi.

64
00:04:55.670 --> 00:05:01.610
So if we just print out these items you'll notice essentially the encoded text which means I could say

65
00:05:02.650 --> 00:05:10.950
index to character and pass that in to that array and then you'll notice as you scroll down it's essentially

66
00:05:10.950 --> 00:05:16.710
character by character being printed out from various creatures we desire et cetera.

67
00:05:16.710 --> 00:05:19.950
So that's what's actually inside of that large character dataset.

68
00:05:20.610 --> 00:05:30.720
So what we can do now is create sequences from it so I can say sequences is equal to that character

69
00:05:30.760 --> 00:05:37.320
dataset and then I'm going to call this batch method and dispatch method combines consecutive elements

70
00:05:37.320 --> 00:05:41.340
of this dataset into batches that we can then feed into our neural network later on.

71
00:05:42.360 --> 00:05:48.630
So we're going to define the first item as sequence length plus one we're using plus one because of

72
00:05:48.630 --> 00:05:56.190
zero indexing here and then we also say drop remainder is equal to true.

73
00:05:56.290 --> 00:05:57.700
So as you probably suspected.

74
00:05:57.880 --> 00:06:04.960
Recall that our total number sequences if we actually divide it out with classic division is forty five

75
00:06:04.960 --> 00:06:06.500
thousand and five.

76
00:06:06.550 --> 00:06:11.530
But if we were to do this with true division and run that you'll notice that it's actually not a perfect

77
00:06:11.530 --> 00:06:17.740
fit which means the actual number of characters in our entire dataset is not perfectly divisible by

78
00:06:17.770 --> 00:06:20.700
our sequence length of 120 which makes sense.

79
00:06:20.710 --> 00:06:23.950
You would expect it to be perfectly divisible by 120.

80
00:06:24.130 --> 00:06:27.540
So that means you're gonna have some remainder characters at the very end.

81
00:06:27.580 --> 00:06:30.550
So we do here is we say drop remainder is equal to true.

82
00:06:30.670 --> 00:06:36.370
So it just discards those last couple of characters and in our case it's not a big deal to discard that

83
00:06:36.370 --> 00:06:42.490
because it's going to be either 1 19 or one hundred and nineteen characters or less out of a character

84
00:06:42.490 --> 00:06:45.150
set of more than five million characters.

85
00:06:45.160 --> 00:06:49.360
So dropping those last hundred nineteen or less is not a big deal.

86
00:06:49.510 --> 00:06:53.140
So we'll go ahead and run this and now we have our sequences.

87
00:06:53.140 --> 00:06:57.790
So now that we have our sequences we'll perform the following steps for each one of those to create

88
00:06:57.790 --> 00:06:59.250
our target text sequence.

89
00:06:59.380 --> 00:07:04.330
We're first going to grab the input text sequence assigned the target text sequence as the input text

90
00:07:04.330 --> 00:07:10.010
sequence shifted my one step forward and then group them together as a tuple to do this easily.

91
00:07:10.150 --> 00:07:19.600
I'll just create a function you'll say the F create sequence targets and so given a particular sequence

92
00:07:19.720 --> 00:07:29.070
essentially a sequence of 120 characters we're going to say the input text is equal to sequence starting

93
00:07:29.070 --> 00:07:32.280
from the beginning all the way up to Maine not including that last character.

94
00:07:32.310 --> 00:07:40.930
So it's colon negative 1 which means our target text is an equal to the sequence shifted 1 4.

95
00:07:40.950 --> 00:07:48.650
So starting at index 1 instead of index 0 go all the way to the end and then we'll return input text

96
00:07:50.390 --> 00:07:51.230
target text.

97
00:07:52.040 --> 00:07:59.210
So what this is doing is you can imagine that this is going to grab something like Hello my name without

98
00:07:59.210 --> 00:08:06.410
the E and this one is going to be e oh my name.

99
00:08:06.440 --> 00:08:12.200
So this is starting from all the way to being go all the way up to but not including that last character.

100
00:08:12.200 --> 00:08:17.180
Keep in mind it would be 120 characters and then this one starting an index one go all the way to the

101
00:08:17.180 --> 00:08:17.640
end.

102
00:08:17.660 --> 00:08:22.670
So essentially we get the same sequence shifted over by one time step or one character step into the

103
00:08:22.670 --> 00:08:24.200
future.

104
00:08:24.200 --> 00:08:31.580
So you run that and then we're going to map that to all the sequences so that my final dataset is sequences

105
00:08:31.790 --> 00:08:38.980
that map and we'll say create sequence targets go ahead and run that and then we're going to do just

106
00:08:38.980 --> 00:08:41.000
showing an example of what this actually looks like.

107
00:08:41.020 --> 00:08:49.090
So I'm going to say for input text comma target text in my dataset.

108
00:08:49.280 --> 00:08:51.590
Well it's going to take one example of this.

109
00:08:51.890 --> 00:08:59.480
We'll do the following we'll say go ahead and print input text as a name pyrite

110
00:09:02.620 --> 00:09:15.600
and then we'll prints the index two character of our input text name pie and let's go ahead and join

111
00:09:15.600 --> 00:09:17.160
that since that's just going to be a list.

112
00:09:17.160 --> 00:09:19.460
Or say join that.

113
00:09:19.660 --> 00:09:27.300
Make sure you check your braces and parentheses there we'll print a new line for some clarity and then

114
00:09:27.300 --> 00:09:28.830
we'll print Target text

115
00:09:32.590 --> 00:09:33.690
in terms of nut pie

116
00:09:36.490 --> 00:09:45.870
and then we'll say print and then we'll go ahead and join that as well say join clips index the character

117
00:09:46.410 --> 00:09:54.120
in this case the target text number pi so essentially showing you what the first batch will look like.

118
00:09:54.140 --> 00:09:57.450
So the very first batch essentially looks like this.

119
00:09:57.470 --> 00:10:03.190
No it goes 0 1 1 1 etc. all the way to seventy six seventy five.

120
00:10:03.260 --> 00:10:04.490
That's the first sequence.

121
00:10:04.620 --> 00:10:08.180
This what it actually looks like if you were to see it as characters.

122
00:10:08.180 --> 00:10:11.580
Keep in mind our network is going to see the numbers not these actual characters.

123
00:10:11.810 --> 00:10:14.650
And then the next batch notice it starts at 1.

124
00:10:14.660 --> 00:10:16.160
So here we see 0 1.

125
00:10:16.160 --> 00:10:19.990
This one's going to 1 and then it goes seventy six seventy five.

126
00:10:20.020 --> 00:10:25.530
But this one extends to seventy six seventy five and one we're 1 essentially just whitespace.

127
00:10:25.640 --> 00:10:27.620
So you can see here the string as well.

128
00:10:27.620 --> 00:10:30.480
So even those strings when they're printed out look actually similar.

129
00:10:30.530 --> 00:10:35.180
This one has extra whitespace and you can kind of tell by the fact that I'm highlighting it the highlighting

130
00:10:35.210 --> 00:10:35.840
ends at T.

131
00:10:35.870 --> 00:10:38.030
But here it ends at the whitespace.

132
00:10:38.120 --> 00:10:40.530
You can confirm that by saying we have that extra one.

133
00:10:40.550 --> 00:10:42.000
So this is the target.

134
00:10:42.020 --> 00:10:45.770
This is the input sequence and then this is Second once the target sequence.

135
00:10:45.770 --> 00:10:50.210
OK so let's go ahead and I'll show you how to actually generate those training batches.

136
00:10:50.210 --> 00:10:52.260
So now we have the actual sequences.

137
00:10:52.340 --> 00:10:55.820
We can create those training batches this choose a batch size.

138
00:10:55.910 --> 00:11:01.120
Go ahead and choose 128 and then we want to make sure we shuffle these.

139
00:11:01.120 --> 00:11:05.570
So the model doesn't learn on a particular ordering of the text.

140
00:11:05.590 --> 00:11:10.760
It should be able to take any random snippet of text and start generating the next sequence from it.

141
00:11:10.780 --> 00:11:17.330
So to do that we'll say data set is equal to data set shuffle.

142
00:11:17.480 --> 00:11:22.430
And just to make sure everything's not shuffled in memory we're going to only take batches at a time

143
00:11:22.520 --> 00:11:23.380
to shuffle.

144
00:11:23.390 --> 00:11:27.770
So we'll say buffer size is equal to ten thousand.

145
00:11:27.770 --> 00:11:33.890
So essentially what this means is just take in 10000 of those elements and then shuffle them around.

146
00:11:34.040 --> 00:11:39.890
Because if you don't actually specify a buffer size when you call shuffle ill attempt to read in the

147
00:11:39.890 --> 00:11:44.510
entire data set in memory and shuffle that and if you're dealing for a large dataset that could cause

148
00:11:44.510 --> 00:11:44.750
error.

149
00:11:44.750 --> 00:11:51.680
So we just say dataset that shuffle buffer size and that will help out if any memory potential issues

150
00:11:52.220 --> 00:12:00.160
and then the batch we pass in the batch size and then again we say drop remainder equal to true OK go

151
00:12:00.160 --> 00:12:01.420
ahead and run that.

152
00:12:01.420 --> 00:12:04.660
And now we should have our dataset ready to go.

153
00:12:04.650 --> 00:12:07.340
Notice the shapes it's 128 by 120.

154
00:12:07.360 --> 00:12:12.070
So one hundred and twenty eight different types of sequences where each sequence is one hundred and

155
00:12:12.070 --> 00:12:17.860
twenty long and then we have this tuple where we have the input sequence and then the target sequence.

156
00:12:18.200 --> 00:12:18.800
OK.

157
00:12:19.000 --> 00:12:22.160
This probably one of most complicated steps out of this entire process.

158
00:12:22.240 --> 00:12:27.040
But if you understand the main idea that your batches essentially look like this where you have one

159
00:12:27.040 --> 00:12:33.190
sequence of encoded numbers that relates to an original string and then the target is that sequence

160
00:12:33.250 --> 00:12:39.160
shifted over by one number again relating to some string then you essentially understand the actual

161
00:12:39.160 --> 00:12:43.120
process the technical parts of creating this with this data set object.

162
00:12:43.120 --> 00:12:47.320
That's probably the most complicated part but if you understand the fundamental idea of this encoded

163
00:12:47.650 --> 00:12:52.680
sequence being shifted at one time step forward then you pretty much got this section down.

164
00:12:52.680 --> 00:12:55.480
Note here that this 120 refers the batch size.

165
00:12:55.630 --> 00:13:00.040
So we're gonna be feeding in one hundred and twenty eight of these sequences at a time where each sequence

166
00:13:00.100 --> 00:13:01.700
is 120 characters long.

167
00:13:01.780 --> 00:13:07.090
So we have the input sequences and then the target sequences coming up next we'll focus on creating

168
00:13:07.090 --> 00:13:08.250
the actual model.

169
00:13:08.350 --> 00:13:09.390
Thanks and I'll see you there.
