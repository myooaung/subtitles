WEBVTT
1
00:00:05.210 --> 00:00:06.690
Welcome back everyone.

2
00:00:06.800 --> 00:00:11.040
In this series of lectures we're going to focus on two things.

3
00:00:11.110 --> 00:00:15.280
One is how to perform a classification task with tensor flow.

4
00:00:15.280 --> 00:00:21.130
The second is also focusing on how to identify and deal with overfishing through early stopping callback

5
00:00:21.130 --> 00:00:28.410
techniques as well as adding in dropout layers early stopping is something we can do if cares to automatically

6
00:00:28.410 --> 00:00:34.230
stop training based on a lost condition on the validation data that we pass in during the model that

7
00:00:34.230 --> 00:00:34.500
fit.

8
00:00:34.500 --> 00:00:44.500
Call dropout layers are layers that can be added to turn off neurons during training to prevent overfishing.

9
00:00:44.510 --> 00:00:51.830
Basically what we do is each drop layer will drop or turn off a user defined percentage of neuron units

10
00:00:52.130 --> 00:00:53.400
in the previous layer.

11
00:00:53.480 --> 00:00:54.530
Every batch.

12
00:00:54.740 --> 00:00:59.900
So that means certain neurons don't have their weights or biases affected during a batch.

13
00:00:59.900 --> 00:01:01.470
Instead they're just turned off.

14
00:01:02.000 --> 00:01:05.440
OK so let's see how this all works by heading over to Jupiter notebook.

15
00:01:05.660 --> 00:01:07.580
All right here I am at a Jupiter notebook.

16
00:01:07.580 --> 00:01:12.290
I've gone ahead and imported pandas num pi map polyp and sea born and the data set.

17
00:01:12.290 --> 00:01:18.230
We're going to be working with here is basically a bunch of measurements of tumors and then a classification

18
00:01:18.560 --> 00:01:20.450
of whether they're malignant or benign.

19
00:01:21.410 --> 00:01:23.600
And let's go ahead and load up this file.

20
00:01:23.600 --> 00:01:34.090
We're gonna say PD read CSP and then under data we're going to load up the file and it is called cancer

21
00:01:34.480 --> 00:01:41.230
classification so you can tab autocomplete that from the data folder and a quick note underneath our

22
00:01:41.290 --> 00:01:42.250
aliens folder.

23
00:01:42.250 --> 00:01:47.530
If you go to the carer's classification notebook and open it up you shouldn't be able to scroll through

24
00:01:47.530 --> 00:01:50.690
this and read the entire description of the dataset.

25
00:01:50.710 --> 00:01:57.370
It contains 30 numeric predictive attributes and then one extra class and it's a relatively small dataset

26
00:01:57.730 --> 00:02:00.290
about five hundred and sixty nine instances.

27
00:02:00.460 --> 00:02:05.890
So we'll come back here and let's go out and do a little bit of exploratory data analysis and then later

28
00:02:05.890 --> 00:02:12.400
on we'll move on to focusing on early stopping callbacks as well as adding in those dropout layers so

29
00:02:12.400 --> 00:02:17.710
something I would always recommend doing is just doing quick info call and this will lie to see if you

30
00:02:17.710 --> 00:02:19.580
have any null values right away.

31
00:02:19.720 --> 00:02:22.870
And if you scroll through this you'll notice are all five hundred sixty nine.

32
00:02:22.960 --> 00:02:26.380
So none of them are NOLs which is good for us.

33
00:02:26.380 --> 00:02:34.360
And then we can also perform a described call in order to begin exploring the statistical distribution

34
00:02:34.690 --> 00:02:36.010
of the various features.

35
00:02:36.130 --> 00:02:42.220
And I always like to transpose this just so it's a little easier to read with the index being the features

36
00:02:42.220 --> 00:02:43.210
themselves.

37
00:02:43.240 --> 00:02:43.650
All right.

38
00:02:43.870 --> 00:02:46.090
So lots of information here we can explore.

39
00:02:46.090 --> 00:02:50.800
Let's go ahead and do a little bit of exploratory data analysis through visualization for classification

40
00:02:50.800 --> 00:02:51.370
tasks.

41
00:02:51.370 --> 00:02:59.890
It's always a good idea to do a count plot of your actual label to see the number of instances per label

42
00:03:00.310 --> 00:03:02.380
and see if it's a well balanced problem or not.

43
00:03:02.860 --> 00:03:13.110
So the label column is called benign underscore zero underscore underscore M A L underscore one.

44
00:03:13.140 --> 00:03:17.520
So basically just indicates that benign is zero and then malignant is one.

45
00:03:17.530 --> 00:03:25.900
So here's the actual feature or excuse me label and then our data frame is def we'll go ahead and run

46
00:03:25.900 --> 00:03:31.370
that and this looks to be a relatively well balance so we definitely have more cases of malignant tumor

47
00:03:31.370 --> 00:03:32.750
is in this particular dataset.

48
00:03:32.920 --> 00:03:40.140
But the difference is not extreme here then what else we can do is check out the correlation between

49
00:03:40.140 --> 00:03:41.510
the features themselves.

50
00:03:41.730 --> 00:03:49.310
So we can say the F correlation and this will actually produce a correlation here.

51
00:03:49.490 --> 00:03:55.430
And sometimes what's nice to do is to actually just view this in respect to the label we're trying to

52
00:03:55.430 --> 00:03:59.480
predict which again is whether it's malignant or benign.

53
00:03:59.600 --> 00:04:06.770
So we can pass that in and that just gives us this right here and in that case we can then say sort

54
00:04:06.770 --> 00:04:13.230
values and then we can see what is highly positively correlated as well as what's highly negatively

55
00:04:13.230 --> 00:04:18.780
correlated and sometimes it's easier to actually plot this out and we can do this by simply calling

56
00:04:19.320 --> 00:04:22.200
plot with kind as equal to a bar plot.

57
00:04:22.320 --> 00:04:24.490
You could also do this with seaborne if you wanted to.

58
00:04:24.630 --> 00:04:30.390
And notice this very last one right here is the actual label itself which is perfectly correlated at

59
00:04:30.390 --> 00:04:31.160
1.

60
00:04:31.200 --> 00:04:33.460
So let's just go ahead and drop that one.

61
00:04:33.600 --> 00:04:41.370
So we'll go ahead and say sort values and after this we'll do the following we'll say grab everything

62
00:04:42.210 --> 00:04:47.560
but the last one and that will essentially drop that label column.

63
00:04:47.690 --> 00:04:52.700
So it looks like we have very correlated values but very highly negatively correlated so we should be

64
00:04:52.700 --> 00:04:57.350
able to get pretty strong predictions from this dataset just based off this analysis here.

65
00:04:57.530 --> 00:05:05.480
And we can also do a similar analysis of correlation between the actual variables themselves by calling

66
00:05:05.540 --> 00:05:11.060
a CNS heat map on this correlation and this returns a heat map.

67
00:05:11.070 --> 00:05:15.110
However this shows you the correlation with every feature compared to every other feature.

68
00:05:15.150 --> 00:05:22.080
And if it's a little small you can always expand on this by saying Pulte figure fig size and then choose

69
00:05:22.080 --> 00:05:25.050
a larger size twelve by twelve or whatever you see fit.

70
00:05:25.170 --> 00:05:27.180
So that's something you can explore as well.

71
00:05:27.180 --> 00:05:30.360
You can also change the color mappings to explore that further.

72
00:05:30.360 --> 00:05:36.060
But right now we'll go ahead and put exploratory data analysis the so we can get to our train test split

73
00:05:36.270 --> 00:05:37.460
and scaling the data.

74
00:05:37.650 --> 00:05:43.570
And then in part two we'll focus on creating the model and dealing with overfishing that could occur.

75
00:05:43.760 --> 00:05:51.390
So first let's do our train test split our X features we're going to say the F drop the benign column.

76
00:05:51.470 --> 00:05:58.010
In fact they should still have that copy pasted there and then that's along axes equals one and we'll

77
00:05:58.010 --> 00:06:00.610
just grab those values so that it's an umpire.

78
00:06:01.220 --> 00:06:05.690
And then y it's going to be equal to the F in this case.

79
00:06:05.690 --> 00:06:09.980
It's just that column and we'll grab those values.

80
00:06:09.980 --> 00:06:16.460
So then we will say from S K learn model selection and this should seem very familiar to you by now

81
00:06:16.910 --> 00:06:21.800
the train test split import that called train test split.

82
00:06:21.800 --> 00:06:27.110
Go ahead and expand the documentation string so you can scroll down here and then grab this example

83
00:06:28.690 --> 00:06:34.700
let's copy and paste that here and then what we're going to do is we'll simply change the test size

84
00:06:34.700 --> 00:06:38.090
to something a little smaller since we have that many points.

85
00:06:38.090 --> 00:06:42.560
We'll go ahead and change it to just 25 percent of our data and to keep things consistent.

86
00:06:42.620 --> 00:06:44.690
I'll set my random state to 1 0 1.

87
00:06:44.840 --> 00:06:46.610
Of course that's just an arbitrary choice.

88
00:06:46.610 --> 00:06:50.430
But if you want to get the same split I do go ahead and choose the same value.

89
00:06:50.870 --> 00:06:51.410
We run that.

90
00:06:51.410 --> 00:06:54.760
We get our split and then finally let's scale the data.

91
00:06:55.190 --> 00:07:04.010
We'll do this by using the pre processing from psychic learn and then we will import min max scalar

92
00:07:05.940 --> 00:07:15.040
create an instance of the scalar and we will fit transform our training data.

93
00:07:15.190 --> 00:07:22.450
So we'll go and say X. train is equal to fit transform X train and we will do the same for our test

94
00:07:22.450 --> 00:07:22.810
data.

95
00:07:22.810 --> 00:07:29.150
So scalar transform x test recall we don't want to actually fit the test data.

96
00:07:29.330 --> 00:07:33.100
We only want to fit to our training data to prevent data leakage.

97
00:07:33.100 --> 00:07:33.470
All right.

98
00:07:33.860 --> 00:07:35.840
So we did a little bit exploratory data analysis.

99
00:07:35.850 --> 00:07:37.120
There's definitely a lot more you can do.

100
00:07:37.120 --> 00:07:42.500
So feel free to create scatter plots distribution plots et cetera whatever you're interested in and

101
00:07:42.500 --> 00:07:47.240
then more importantly we went ahead and did a train test split as well as the pre processing to scale

102
00:07:47.240 --> 00:07:48.060
the data.

103
00:07:48.140 --> 00:07:54.200
So coming up next we're going to focus on creating the model and then showing you how to prevent overfishing

104
00:07:54.290 --> 00:07:59.840
and how to make sure you don't actually overrun on the training data set and then end up having a poor

105
00:07:59.840 --> 00:08:01.260
fit on your test data set.

106
00:08:01.280 --> 00:08:03.590
So we'll cover all that in part two.

107
00:08:03.620 --> 00:08:04.370
I'll see you there.
