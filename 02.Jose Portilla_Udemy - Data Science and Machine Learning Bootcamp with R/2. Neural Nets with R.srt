1

00:00:00,570  -->  00:00:01,550
Hello everyone.

2

00:00:01,710  -->  00:00:07,290
Now that we've gone over a light theoretical overview of the perception and how to build a neural network

3

00:00:07,320  -->  00:00:12,150
from a group of percept trons we're actually going to learn how to implement a neural network with our

4

00:00:12,420  -->  00:00:18,240
using the neural network package or library for our let's go ahead and jump to our studio to get started

5

00:00:18,240  -->  00:00:18,560
.

6

00:00:18,570  -->  00:00:20,630
OK here we are at our studio.

7

00:00:20,630  -->  00:00:22,940
We got some pretty cool stuff in store for this lecture.

8

00:00:22,950  -->  00:00:28,080
But before we begin we have to actually get our data in this case we're going to use the famous or popular

9

00:00:28,080  -->  00:00:32,490
Boston data set and it's from the mass package with our.

10

00:00:32,610  -->  00:00:36,530
And if you don't have the mass package all you need to do is go to your consul.

11

00:00:36,780  -->  00:00:42,840
And just like in the note book it says you can put in installed packages and capital M S S and that

12

00:00:42,840  -->  00:00:44,480
should give you the mass package.

13

00:00:44,700  -->  00:00:50,700
Once you have the mass package installed and downloaded you just call it library mass and you should

14

00:00:50,700  -->  00:00:53,400
be able to call the head of capital B.

15

00:00:53,560  -->  00:01:02,100
OS Boston let me go ahead and expand the council and scripts for a little bit and enter here and you'll

16

00:01:02,100  -->  00:01:07,360
see we have a bunch of features of houses in Boston.

17

00:01:07,410  -->  00:01:12,300
Keep in mind this housing data is from a publication that's from 1978.

18

00:01:12,480  -->  00:01:17,340
So it's pretty old but it'll do for what we're actually trying to do in this case.

19

00:01:17,340  -->  00:01:21,620
Each of these columns represents some sort of feature of a house for instance.

20

00:01:21,630  -->  00:01:29,340
C R I M is the per capita crime rate per town than we have this r.m which is the average number of rooms

21

00:01:29,340  -->  00:01:32,560
per dwelling in that particular area of Boston.

22

00:01:32,700  -->  00:01:36,870
You have the tax rate the ages of the houses et cetera.

23

00:01:36,870  -->  00:01:41,710
At the very end we have this median value of owner occupied homes in the thousands of dollars.

24

00:01:41,880  -->  00:01:47,550
We're going to be trying to do is essentially almost like a regression problem where we have a bunch

25

00:01:47,550  -->  00:01:52,980
of features and we're going to try to predict a continuous value which is the median value of owner

26

00:01:52,980  -->  00:01:55,540
occupied homes in the thousands of dollars.

27

00:01:55,890  -->  00:01:59,200
If you want more information on what each of these columns actually represents.

28

00:01:59,220  -->  00:02:03,720
Check out the note book where the description of each of the features of this data frame.

29

00:02:03,720  -->  00:02:11,760
Let's check out how large data frame is by calling structure member that's s tr Press in Boston and

30

00:02:11,760  -->  00:02:19,100
it looks like we have 506 observations and there's 14 variables meaning the 13 features and number 14

31

00:02:19,110  -->  00:02:22,690
we have what we're trying to predict which is the median value.

32

00:02:22,710  -->  00:02:27,150
Let's go ahead and just quickly check to make sure we don't have any missing data member the way we

33

00:02:27,150  -->  00:02:34,280
check for missing data and to clear the Consul is by saying NE is dot an A.

34

00:02:34,410  -->  00:02:39,390
And then you pass in your data in this case I'm just going to pass in that Boston and we get false which

35

00:02:39,390  -->  00:02:40,540
means false.

36

00:02:40,560  -->  00:02:45,210
There is no and a value is in Boston meaning we're not missing any information.

37

00:02:45,270  -->  00:02:46,560
So that's all great.

38

00:02:46,650  -->  00:02:55,050
Let's go ahead and clear the console make a variable data and assign Boston to it.

39

00:02:55,590  -->  00:03:00,150
That way we can mess around with data and it's a little more clear than just typing Boston all the time

40

00:03:00,150  -->  00:03:00,910
.

41

00:03:00,960  -->  00:03:06,900
We won't do any exploratory data analysis in this lecture but in your project that's coming after this

42

00:03:06,900  -->  00:03:11,770
lecture you will do a little bit of exploratory data analysis on a dataset you haven't seen before.

43

00:03:11,770  -->  00:03:16,140
For now we're going to focus on actually training a neural net model in order to do this we need to

44

00:03:16,140  -->  00:03:19,000
do two things we need to pre-process our data.

45

00:03:19,110  -->  00:03:21,820
And then you actually need to install the neural net library.

46

00:03:21,870  -->  00:03:26,990
Let's continue on the data path and actually preprocess our data.

47

00:03:27,180  -->  00:03:33,570
Whenever you're training a neural network it's a good practice to normalize your data and we can use

48

00:03:33,570  -->  00:03:41,510
the skill function in order to normalize our data depending on your dataset.

49

00:03:41,550  -->  00:03:47,790
Avoiding normalization may lead to useless results or to a very difficult training process meaning a

50

00:03:47,790  -->  00:03:52,950
lot of times the algorithm won't converge before the number of maximum iterations is allowed.

51

00:03:53,160  -->  00:03:59,700
But you can choose different methods to scale your data such as a min max scale Xen normalization a

52

00:03:59,700  -->  00:04:03,470
bunch of different typical methods for scaling and normalizing your data.

53

00:04:03,630  -->  00:04:08,460
Well I'm going to do is teach you how you can customize your arguments in the skill function and are

54

00:04:09,090  -->  00:04:10,400
in the upcoming project.

55

00:04:10,410  -->  00:04:15,360
I'll just have you use the default arguments for scale and that means it's going to be a lot more straightforward

56

00:04:15,570  -->  00:04:21,370
but I do want you to know how you can actually customize the arguments that we put into our scale function

57

00:04:21,380  -->  00:04:21,830
.

58

00:04:22,110  -->  00:04:27,090
Well we're going to do first is grab the mid and emacs values of each of the columns in our data frame

59

00:04:27,420  -->  00:04:35,520
and we're to do that using apply and we create a variable called Maxxis.

60

00:04:36,450  -->  00:04:43,050
And I'm going to call the Apply function I'm going to apply over my data and I'm going to add a second

61

00:04:43,080  -->  00:04:44,560
argument called margin.

62

00:04:44,590  -->  00:04:48,630
And if you want more information on what that second argument margin does you can just call help on

63

00:04:48,630  -->  00:04:53,850
apply and it should take the documentation apply functions over array margins.

64

00:04:53,850  -->  00:04:58,740
But basically what's going to happen is if you just put in a two it indicates that we want to apply

65

00:04:58,740  -->  00:05:04,320
this over the columns and that's exactly want because I want the maximum value of each of the columns

66

00:05:04,320  -->  00:05:09,900
which is why I'm going to pasan a two here as my margin argument and then I'm going to pass in a function

67

00:05:09,900  -->  00:05:12,140
that I want to apply over the columns.

68

00:05:12,160  -->  00:05:20,180
In this case the function I want to apply is Max let's go ahead and see that analysis checkup Max's

69

00:05:21,320  -->  00:05:27,620
notice now I get the maximum value for each of the columns mean the maximum value for the median value

70

00:05:27,710  -->  00:05:31,220
of a group of houses is $50000.

71

00:05:31,370  -->  00:05:33,950
We're going to do the same thing for the minimum values.

72

00:05:33,950  -->  00:05:40,010
Let's go ahead and say an argument mennes make sure you have an s at the end of Maxxis or mindes that

73

00:05:40,010  -->  00:05:44,060
we don't overwrite the actual max or min built in functions.

74

00:05:44,300  -->  00:05:46,420
We're going to apply over our data.

75

00:05:46,430  -->  00:05:53,690
That's the Boston dataset put into because we want to do this over the columns and then put in min for

76

00:05:53,690  -->  00:05:55,240
the minimum function.

77

00:05:55,530  -->  00:05:57,520
And now let's go ahead and check mins.

78

00:05:57,830  -->  00:06:02,870
And now we have the minimum values for each of the columns.

79

00:06:02,870  -->  00:06:09,460
Now what I'm going to do is use these men's and max values in order to scale my data or normalize my

80

00:06:09,470  -->  00:06:13,590
data may go ahead and clear the con..

81

00:06:13,970  -->  00:06:20,540
And what I want to do is go ahead and call help on the scale function and it should take you to a page

82

00:06:20,540  -->  00:06:23,070
that looks like this in the documentation.

83

00:06:23,090  -->  00:06:28,340
You'll notice that scale is just a generic function whose default method centers and or scales the columns

84

00:06:28,340  -->  00:06:31,340
of a numeric matrix and that tells you two things.

85

00:06:31,340  -->  00:06:36,800
One that it has default arguments and two it's going to return in numeric matrix meaning we have to

86

00:06:36,800  -->  00:06:39,350
transform it back into a data frame.

87

00:06:39,350  -->  00:06:46,360
Notice that these other arguments center and scale can be either logical values or numeric vectors.

88

00:06:46,610  -->  00:06:49,910
In your project you're just going to leave them as the default logical values.

89

00:06:49,910  -->  00:06:55,820
But right now on this show you can actually pass in the min and max values we just calculated in order

90

00:06:55,820  -->  00:06:58,530
to customize your normalization.

91

00:06:58,910  -->  00:07:05,630
What the first center argument is if we're going to pass in a numeric vector for center then each column

92

00:07:05,750  -->  00:07:09,350
of our data has the corresponding value from center.

93

00:07:09,340  -->  00:07:18,980
Subtract that from it meaning I'm going to do this on a cost scale on my data and say Center is equal

94

00:07:18,980  -->  00:07:26,360
to men's and that means each single data point in the column is going to have the minimum value subtracted

95

00:07:26,360  -->  00:07:27,360
from it.

96

00:07:27,620  -->  00:07:33,920
And then we have the second argument which is scale in if we pasand a numeric vector for scale each

97

00:07:33,920  -->  00:07:38,240
value in our dataset is going to be divided by the scale.

98

00:07:38,630  -->  00:07:45,380
And that means I want the scale to be Maxxis minus mindes.

99

00:07:45,470  -->  00:07:51,140
All I'm doing right now is I'm saying get my data in every data point subtract the minimum value of

100

00:07:51,140  -->  00:07:56,270
that column that it's in and then divide it by the maximum value minus the minimum value and that's

101

00:07:56,270  -->  00:07:58,250
going to normalize my data.

102

00:07:58,250  -->  00:08:07,520
Let me go ahead and set this equal to scaled data and a push over the documentation's.

103

00:08:07,510  -->  00:08:13,540
We have a little more room here and I have my scaled data.

104

00:08:13,600  -->  00:08:22,170
Remember this is a matrix and that means I want to actually say scaled or scaled up data and passing

105

00:08:22,170  -->  00:08:23,540
it as a data frame.

106

00:08:23,600  -->  00:08:34,700
Let's say as that data frame and person that matrix and then I can get back head of scaled and notice

107

00:08:34,790  -->  00:08:40,530
everything's been scaled now or normalized and that's exactly how we want it for a neural net.

108

00:08:40,640  -->  00:08:44,630
This is a really important step and you should always make sure you're normalizing your data whenever

109

00:08:44,630  -->  00:08:46,210
you're working with a neural net.

110

00:08:46,250  -->  00:08:51,230
Otherwise you'll get funky results or it just won't converge and they pushed this a little more to the

111

00:08:51,230  -->  00:08:51,830
right.

112

00:08:51,830  -->  00:08:53,470
So we have a little more room to work with.

113

00:08:53,780  -->  00:08:58,040
And if you want to confirm that this is actually scale you can just go ahead and check out the head

114

00:08:58,040  -->  00:09:04,620
of the built in Boston data frame and notice that we actually had different values here.

115

00:09:04,670  -->  00:09:06,400
You can you just check here in the median value.

116

00:09:06,420  -->  00:09:14,240
Notice everything's now between 0 and 1 versus in the original you actually had the real values.

117

00:09:14,810  -->  00:09:19,700
Now that we've standardized our data let's go ahead and split it into a training set and a testing set

118

00:09:19,700  -->  00:09:20,840
.

119

00:09:20,840  -->  00:09:23,080
Now we're already really familiar with this process.

120

00:09:23,120  -->  00:09:27,110
Hopefully you can just do this on your own by now since you've done it for basically every lecture and

121

00:09:27,110  -->  00:09:27,910
every project.

122

00:09:28,040  -->  00:09:39,980
Remember you call CA tools get the variables split call sample dot split and then our scaled data pasan

123

00:09:40,010  -->  00:09:40,820
any column.

124

00:09:40,820  -->  00:09:46,490
I always like passing in or trying to predict in that case it's the median value in our split ratio

125

00:09:46,530  -->  00:09:48,830
is just going to be point 7.

126

00:09:48,830  -->  00:09:57,590
That means 70 percent of the data be for training which means I will say train is a subset of scaled

127

00:09:58,260  -->  00:10:07,970
where the split is equal to true just put in capital-T and then test is the subset were scaled split

128

00:10:08,360  -->  00:10:11,630
is equal to false.

129

00:10:11,660  -->  00:10:13,420
Let's go ahead and run that

130

00:10:16,350  -->  00:10:17,970
it looks like everything's good.

131

00:10:18,140  -->  00:10:19,020
You get a warning message.

132

00:10:19,010  -->  00:10:22,150
But again that just has to do with what version the library is built on.

133

00:10:22,400  -->  00:10:28,490
But if I go ahead and say head of train looks like everything's working out I may go ahead and clear

134

00:10:28,500  -->  00:10:29,840
the con..

135

00:10:29,850  -->  00:10:32,340
Now we're ready to actually train our model.

136

00:10:32,420  -->  00:10:35,280
We're going to need the neural net package in order to do this.

137

00:10:35,460  -->  00:10:41,150
And that means if you don't have it yet go ahead and and your consul say install that packages and in

138

00:10:41,150  -->  00:10:45,900
quotes person that's one word word neural net.

139

00:10:45,960  -->  00:10:56,070
Once you've done that you should be able to call library neural net and have it all come out for you

140

00:10:56,070  -->  00:10:58,660
.

141

00:10:58,760  -->  00:10:59,100
All right.

142

00:10:59,100  -->  00:11:05,450
Now that that's installed Let's go ahead and actually use the neural function from the library to train

143

00:11:05,460  -->  00:11:12,090
a neural network for some odd reasons this neural net function won't accept the formula in the classic

144

00:11:12,090  -->  00:11:18,860
form that we're used to or we just say why and untilled and Dadds it doesn't except that for whatever

145

00:11:18,870  -->  00:11:24,090
reason you have to actually manually put in call 1 column to etc..

146

00:11:24,260  -->  00:11:29,340
Usually we're used to being able to avoid that process by just using the dot.

147

00:11:29,470  -->  00:11:32,460
And this time we're actually gonna have to manually put those all in.

148

00:11:32,460  -->  00:11:38,040
I'm going to go ahead and show you a little trick that will help you avoid that manual labor of having

149

00:11:38,030  -->  00:11:39,590
to type all those out.

150

00:11:39,600  -->  00:11:45,250
Go ahead and ask for the names of the training data set the column names.

151

00:11:45,360  -->  00:11:50,610
That we want to call and we see all the column names and in your note book you have a piece of code

152

00:11:50,610  -->  00:11:50,630
.

153

00:11:50,630  -->  00:11:52,790
I'm going to go ahead and copy and paste it from the notebook.

154

00:11:52,830  -->  00:11:58,100
Now you grab it copy paste.

155

00:11:58,190  -->  00:12:05,050
And what this does is it automatically joins all of these column names together into a formula.

156

00:12:05,180  -->  00:12:10,800
Let me go ahead and just quickly break this down what we're doing is we're just grabbing every single

157

00:12:10,790  -->  00:12:15,040
column name pasting it together with a plus in between.

158

00:12:15,170  -->  00:12:21,090
And then at the at the very beginning we're saying MATV for the median value tilde and then we're going

159

00:12:21,100  -->  00:12:27,080
to go ahead and paste in as long as it's not an MTV and then casting it as that formula.

160

00:12:27,120  -->  00:12:32,630
And that's going to save you some time just using this names function and then using this formula little

161

00:12:33,000  -->  00:12:35,670
cheat code that I put in the note book for you.

162

00:12:35,930  -->  00:12:40,850
And then when you call f you'll see that this formula actually has already been written out for you

163

00:12:41,390  -->  00:12:46,020
and that way you can avoid the process of having to manually type out each of these column names.

164

00:12:46,560  -->  00:12:50,780
Hopefully that's useful to you but that's where you're going to have to do since you can't use the dot

165

00:12:50,780  -->  00:12:53,840
notation for the neural net function.

166

00:12:53,850  -->  00:13:00,150
Let me go ahead and clear the console now and actually train our neural net and create a variable called

167

00:13:00,240  -->  00:13:04,980
an N called in your own that function.

168

00:13:04,980  -->  00:13:09,290
It takes in a formula again and we couldn't do something like this.

169

00:13:09,290  -->  00:13:11,090
We have to type in the whole formula.

170

00:13:11,350  -->  00:13:15,050
In this case RLT save that as f is going to put an F..

171

00:13:15,090  -->  00:13:21,720
I'm just going to train on my training data that Sika arguments that data and then we have an argument

172

00:13:21,720  -->  00:13:22,950
called hidden.

173

00:13:23,120  -->  00:13:24,820
And what the hidden argument does.

174

00:13:24,870  -->  00:13:31,120
It's a vector of integers specifying the number of hidden neurons in each layer.

175

00:13:31,160  -->  00:13:36,500
Go ahead and check out the Wikipedia articles as far as maybe what are reasonable values to choose here

176

00:13:36,500  -->  00:13:36,810
.

177

00:13:36,990  -->  00:13:43,820
But I'm going to go ahead and just choose a first hidden layer of five neurons and a second hidden layer

178

00:13:43,910  -->  00:13:46,560
of three neurons you could keep adding to this.

179

00:13:46,590  -->  00:13:49,230
Right now we'll just leave it at that.

180

00:13:49,430  -->  00:13:54,380
And then since we're not doing classification we're actually doing some sort of continuous value.

181

00:13:54,570  -->  00:14:00,710
Well we're going to go ahead and say is our final argument linear output equals true.

182

00:14:01,250  -->  00:14:06,870
And you would set this to false if you're actually performing a classification problem your neural net

183

00:14:06,880  -->  00:14:07,030
.

184

00:14:07,160  -->  00:14:11,790
Well let's go ahead and train our neural net.

185

00:14:11,780  -->  00:14:12,460
OK.

186

00:14:12,570  -->  00:14:16,620
Now what's really cool here is we're actually going to be able to plot out the neural net that we just

187

00:14:16,620  -->  00:14:17,160
created.

188

00:14:17,150  -->  00:14:18,800
Let me go ahead and show you how you can do that.

189

00:14:18,810  -->  00:14:24,060
I'm going to expand the visualization window a little bit.

190

00:14:24,670  -->  00:14:28,140
Put it all the way out so we can clear when your output nicely.

191

00:14:28,130  -->  00:14:28,810
All right.

192

00:14:29,250  -->  00:14:35,300
And then all you do is say plots and pass the variable that is your neural net and you will see the

193

00:14:35,310  -->  00:14:38,170
plot of your neural nets go in and zoom in on it.

194

00:14:38,340  -->  00:14:42,130
And this is one of the good things and nice things about a neural net.

195

00:14:42,300  -->  00:14:48,000
Basically a neural net you cannot treat it a bit of a black box because it's really hard to interpret

196

00:14:48,020  -->  00:14:54,240
what each of these actual weighted vectors really means in reference to the column values of your data

197

00:14:54,240  -->  00:14:54,750
.

198

00:14:54,810  -->  00:14:59,730
We can see here that the black lines show the connections between each layer and the weights on each

199

00:14:59,730  -->  00:15:06,050
connection and these blue lines that we see showed the bias term added in each step.

200

00:15:06,060  -->  00:15:10,070
Again the bias can be thought almost as the intercept of a linear model.

201

00:15:10,160  -->  00:15:12,650
The net here is essentially a black box.

202

00:15:12,680  -->  00:15:17,590
That means we cannot say that much about the fitting the weights or even the model.

203

00:15:18,410  -->  00:15:22,740
But we can't say is that this training algorithm converged and therefore the models ready to be used

204

00:15:22,740  -->  00:15:25,650
on some sort of testing data.

205

00:15:26,070  -->  00:15:30,270
All right let's go ahead and move on to actually creating predictions with our model.

206

00:15:30,280  -->  00:15:33,010
I'm going to hope you thought this paralyzation was pretty cool.

207

00:15:33,180  -->  00:15:41,910
Even if it's not super interpretable and I go in and close the shed over to the right and time for some

208

00:15:41,900  -->  00:15:50,250
predictions I'll just go ahead and do this all in the con. I'm going to make a variable called predicted

209

00:15:50,360  -->  00:15:58,740
and an values and for the neural net function you just use the compute function and you use compute

210

00:15:58,830  -->  00:15:59,640
and sort of predict.

211

00:15:59,630  -->  00:16:06,830
But we're usually used to you passing your neural net model and then you pass in your test data with

212

00:16:06,960  -->  00:16:08,790
out the labels for it.

213

00:16:08,940  -->  00:16:10,830
And that means columns 1 through 13.

214

00:16:10,830  -->  00:16:15,370
Remember there were 14 variables or features in that column day to frame.

215

00:16:15,480  -->  00:16:19,250
We just want the first 13 of them because we don't want that actual label.

216

00:16:19,870  -->  00:16:24,590
And we compute those values if we check out the structure of what came out of that.

217

00:16:24,650  -->  00:16:27,960
This predicted and end values.

218

00:16:27,960  -->  00:16:33,740
Notice we get back a list a list of two there's neurons in the net result.

219

00:16:34,050  -->  00:16:37,350
Well we actually want is the net result.

220

00:16:37,960  -->  00:16:42,450
But remember we scaled and normalized our data for the training model.

221

00:16:42,480  -->  00:16:47,830
What we have to do is basically undo those operations in order to get the true predictions.

222

00:16:47,970  -->  00:16:58,200
And that means I'm going to make a variable called True predictions and say predicted and end values

223

00:16:59,660  -->  00:17:07,650
net results and I'm going to go ahead and just copy this formula from the notebook and then quickly

224

00:17:07,640  -->  00:17:09,290
explain it.

225

00:17:09,300  -->  00:17:17,070
Remember that we were subtracting from the center value and then dividing by that scale value or performed

226

00:17:17,070  -->  00:17:18,920
our normalization operation.

227

00:17:18,920  -->  00:17:21,110
And basically here we're just inverting that.

228

00:17:21,120  -->  00:17:27,150
Notice that I'm saying instead of dividing by Max minus min of that median value of our data I'm saying

229

00:17:27,170  -->  00:17:32,340
multiply it by the max minus the men and then instead of subtracting from that center value I'm saying

230

00:17:32,340  -->  00:17:37,890
go ahead and add that center value and I'm just grabbing the Maximin from that specific median value

231

00:17:37,880  -->  00:17:38,820
column.

232

00:17:38,820  -->  00:17:46,440
And this will essentially and the scale or get non-skilled predictions let me go ahead and also convert

233

00:17:46,430  -->  00:17:47,820
the test data.

234

00:17:47,990  -->  00:17:50,650
I'm going to go ahead and just copy those.

235

00:17:51,140  -->  00:17:56,300
Going to say testor are in basically doing the exact same thing.

236

00:17:56,690  -->  00:18:01,880
And the reason I'm doing that is in order to actually get some sort of mean squared error from this

237

00:18:02,490  -->  00:18:04,810
and I say mean squared error.

238

00:18:04,840  -->  00:18:19,010
M S E and say and then remember that's the sum of our test minus our true predictions.

239

00:18:19,020  -->  00:18:24,390
And the reason I call that true there should be a minus sign is because these are the non-skilled predictions

240

00:18:24,400  -->  00:18:24,690
.

241

00:18:25,110  -->  00:18:31,410
And then what I'm going to go ahead and do is square this and then divide it by the number of rows in

242

00:18:31,400  -->  00:18:32,490
our test set.

243

00:18:32,760  -->  00:18:36,000
And that's the main square.

244

00:18:36,270  -->  00:18:40,640
Go ahead and call it now and figure out the mean square error.

245

00:18:40,640  -->  00:18:46,200
Well we can also do is visualizes error in G-G plot 2 by actually just graphically showing the true

246

00:18:46,220  -->  00:18:51,000
predictions plotted by the test values.

247

00:18:50,990  -->  00:19:00,000
All right let's go ahead and create a data frame air Dia and i will call the data frame function and

248

00:19:00,060  -->  00:19:08,490
pasan my test values going to call and test R and then my true unskilled predictions.

249

00:19:08,840  -->  00:19:12,080
Let me go ahead and check the head of that error.

250

00:19:12,120  -->  00:19:14,040
D f.

251

00:19:14,180  -->  00:19:14,610
All right.

252

00:19:14,610  -->  00:19:18,560
And this is basically how we have the test value for that particular house.

253

00:19:18,570  -->  00:19:20,540
This was Row 2 row 4.

254

00:19:20,550  -->  00:19:25,530
Notice these aren't sequential because we did that random train test split and then we have these true

255

00:19:25,530  -->  00:19:26,150
predictions.

256

00:19:26,150  -->  00:19:28,880
These are the unskilled predictions of our neural net.

257

00:19:29,070  -->  00:19:30,480
Looks like we're not doing too bad.

258

00:19:30,490  -->  00:19:31,850
Let's go ahead and visualize.

259

00:19:31,910  -->  00:19:38,470
Visualize this by saying the test the test values on the y axis and the true predictions on the X-axis

260

00:19:38,560  -->  00:19:39,090
.

261

00:19:39,200  -->  00:19:42,280
I'm going to call me go to clear the consul here.

262

00:19:42,290  -->  00:19:50,960
When ecology as you plot to which are already very familiar with say G-G plot ere the F Pescara data

263

00:19:52,010  -->  00:19:59,990
and as the æsthetics I'm going to go ahead and say X is the test are and why is those true predictions

264

00:19:59,990  -->  00:20:00,940
.

265

00:20:01,010  -->  00:20:05,270
I want this to be a scatterplot and a go ahead and say GM underscore points

266

00:20:07,940  -->  00:20:16,440
and then I also want to go ahead and Panay that smooth line just to see a little more on the visual

267

00:20:16,430  -->  00:20:16,770
side.

268

00:20:16,760  -->  00:20:21,830
How we did is go ahead and run that.

269

00:20:21,890  -->  00:20:30,800
And that should have been smooth not state smooth Let's cool this plot out and there it is.

270

00:20:30,810  -->  00:20:32,880
Go ahead and click zoom.

271

00:20:32,970  -->  00:20:39,140
Remember that a perfect prediction would just basically be a straight linear line with a slope of one

272

00:20:39,150  -->  00:20:39,260
.

273

00:20:39,360  -->  00:20:44,990
And here is our true predictions meaning Menier unskilled prediction of values versus the actual test

274

00:20:44,990  -->  00:20:45,950
data.

275

00:20:46,010  -->  00:20:48,530
Looks like there were some houses that threw off our model.

276

00:20:48,540  -->  00:20:54,000
But overall it's not looking too bad especially considering all we have to do is normalize our data

277

00:20:54,040  -->  00:20:54,130
.

278

00:20:54,360  -->  00:20:59,690
And we treated the neural net as essentially just a total black box and that means we can conclude a

279

00:20:59,690  -->  00:21:06,140
couple of things that neural networks resemble black boxes a lot explaining their actual outcome and

280

00:21:06,140  -->  00:21:11,990
how they interpret the features is much more difficult than explaining the outcome of a simple simpler

281

00:21:12,090  -->  00:21:14,610
model such as like a linear regression.

282

00:21:14,660  -->  00:21:17,020
Therefore the pending on the kind of application you need.

283

00:21:17,100  -->  00:21:19,980
You may want to take this into account as well.

284

00:21:19,970  -->  00:21:26,390
That means if you're doing something at work or your own project where you need to specifically interpret

285

00:21:26,660  -->  00:21:32,940
the importance of each of the variables a neural network that is little more black boxy may not be as

286

00:21:32,930  -->  00:21:35,310
good as a simpler model.

287

00:21:35,310  -->  00:21:40,430
And then furthermore as you've seen here extra care is needed to fit and neural that work in order to

288

00:21:40,430  -->  00:21:44,560
actually use that data and that means stuff like normalization of your data.

289

00:21:45,090  -->  00:21:47,300
Ok I hope you enjoy this lecture.

290

00:21:47,310  -->  00:21:50,270
Coming up next is your neural network project.

291

00:21:50,280  -->  00:21:52,380
Thanks everyone and I'll see you at the next lecture.
