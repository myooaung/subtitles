1

00:00:01,230  -->  00:00:02,160
Hello everyone.

2

00:00:02,160  -->  00:00:06,900
Welcome to the decision trees and ran them for us with our lecture this lecture.

3

00:00:06,930  -->  00:00:12,440
We're going to learn how to implement decision trees and run the course with our on an example dataset

4

00:00:12,510  -->  00:00:12,950
.

5

00:00:12,990  -->  00:00:15,930
Let's go ahead and jump to our studio to get started.

6

00:00:15,930  -->  00:00:18,510
All right so here we are at our studio.

7

00:00:18,510  -->  00:00:23,220
We're going to go ahead and do is install the our parts package that we're going to need in order to

8

00:00:23,220  -->  00:00:26,050
create this trees in random force.

9

00:00:26,160  -->  00:00:32,340
So go ahead and type in install the packages our parts are going to go ahead and jump to the finished

10

00:00:32,400  -->  00:00:35,660
downloaded installation of this.

11

00:00:35,700  -->  00:00:37,950
OK what's the our part library.

12

00:00:37,950  -->  00:00:43,570
We're going to be able to build a decision tree model we're going to go ahead and show you the our part

13

00:00:43,650  -->  00:00:48,610
library and the function when we go in and clear this console.

14

00:00:48,750  -->  00:00:59,070
If you call Library our parts it will then allow you to use the r part function and call help on it

15

00:00:59,070  -->  00:01:00,950
.

16

00:01:00,990  -->  00:01:05,880
The our part function is what's going to allow us to build a recursive partitioning and regression tree

17

00:01:05,880  -->  00:01:06,630
.

18

00:01:06,630  -->  00:01:09,970
This is just a fancy way of saying good decision tree.

19

00:01:10,500  -->  00:01:14,900
This our per function is we're going to be using to pass in the formula and pass in the data.

20

00:01:14,910  -->  00:01:16,290
Like we've been doing before.

21

00:01:16,550  -->  00:01:20,510
For other models let's go ahead and continue.

22

00:01:20,610  -->  00:01:24,320
Hopefully you take the time to just browse through the documentation for our part.

23

00:01:24,460  -->  00:01:29,390
But we're going to go ahead and run through the example that's in the note book for this lecture.

24

00:01:29,490  -->  00:01:31,920
Let me go ahead and clear this.

25

00:01:32,190  -->  00:01:38,100
We're going to be using the chi whosis data frame which has 81 rows and four columns.

26

00:01:38,100  -->  00:01:40,010
Let me go ahead and shoot the structure of it.

27

00:01:40,290  -->  00:01:42,580
It's built into our.

28

00:01:43,190  -->  00:01:46,010
That way you can just go ahead and call psychosis.

29

00:01:46,170  -->  00:01:53,250
And you'll notice that it has an age number and start the closest feature is the actual label indicating

30

00:01:53,250  -->  00:02:01,380
whether proposed This is absent or present in catharsis is a type of deformation of the spine.

31

00:02:01,380  -->  00:02:05,730
Then you have a page which indicates the age of the children in months.

32

00:02:05,730  -->  00:02:09,980
So don't worry they're not 158 years old 158 months old.

33

00:02:10,150  -->  00:02:12,400
These are relatively young people.

34

00:02:12,600  -->  00:02:17,370
And then the number in the case the number of vertebrae involved because this is a deformation of the

35

00:02:17,370  -->  00:02:17,980
spine.

36

00:02:18,300  -->  00:02:23,560
And then the star is the number of the first or top most vertebrae operated on.

37

00:02:24,150  -->  00:02:27,570
Let's go ahead and check out the head of this data frame

38

00:02:31,590  -->  00:02:37,110
that we will notice we have absent and present the actual label the age in months of this child.

39

00:02:37,230  -->  00:02:45,320
The number again of vertebrae involved and then start which indicates the number or of the first top

40

00:02:45,360  -->  00:02:47,600
most vertebrate operated on.

41

00:02:48,300  -->  00:02:53,040
And basically this dataset the way it came about is this is data on children.

42

00:02:53,040  -->  00:03:00,870
They've all had a corrective spinal surgery and the label is whether the condition was present or absent

43

00:03:00,990  -->  00:03:02,180
after the surgery.

44

00:03:02,410  -->  00:03:08,730
And that's why we have the starts which in the case the number of the first or top most vertebrae that

45

00:03:08,730  -->  00:03:14,880
the operation was started on let's go in and continue by just building a tree model off of this data

46

00:03:14,890  -->  00:03:15,370
.

47

00:03:15,380  -->  00:03:16,980
Everything's already correct.

48

00:03:16,980  -->  00:03:20,170
They're all in integers and we have our label as a factor.

49

00:03:20,220  -->  00:03:22,810
We can just simply call our model.

50

00:03:22,850  -->  00:03:30,080
I'm going to make a variable called tree and call our parts in order to build that tree.

51

00:03:30,270  -->  00:03:37,440
And then I pass in the formula in this case I want to predict the Infosys notice it's a capital K.

52

00:03:38,330  -->  00:03:46,110
Now I want to predict Infosys absent or present off of all the features but tilde period comma I pass in

53

00:03:46,190  -->  00:03:50,670
mephitic class because this is a classification.

54

00:03:51,090  -->  00:03:57,090
And then I can go ahead and say data equals and here's why I passen lower case i Phasis the actual data

55

00:03:57,090  -->  00:03:59,160
frame.

56

00:03:59,610  -->  00:04:03,730
And now there's actually lots of functions you can use to examine your tree model.

57

00:04:03,750  -->  00:04:09,630
Let me go ahead and jump to the notebook for this lecture to show you just a few of the examples.

58

00:04:09,630  -->  00:04:13,130
All right here's the notebook and you have access to this.

59

00:04:13,170  -->  00:04:15,680
There are lots of functions you can use to examine your model.

60

00:04:15,810  -->  00:04:18,530
You can plot the cross-validation results.

61

00:04:18,630  -->  00:04:24,030
You can just print the results you can actually plot the decision tree or label the decision tree plots

62

00:04:24,480  -->  00:04:29,730
and you can even plot the approximate R-squared relative error for different splits and that creates

63

00:04:29,730  -->  00:04:30,930
two watts.

64

00:04:30,930  -->  00:04:35,730
However that's only appropriate for the Nova method where you try to do some sort of regression at time

65

00:04:35,730  -->  00:04:36,550
series indels.

66

00:04:36,690  -->  00:04:38,690
Let's go ahead and explore a few of these.

67

00:04:38,700  -->  00:04:42,810
I'm going to go back to our studio right behind this.

68

00:04:42,810  -->  00:04:52,150
I'm going to go ahead and print C.P. which will display ECP table for this and you just pasand the actual

69

00:04:52,150  -->  00:04:54,730
model be called Tree.

70

00:04:54,730  -->  00:04:55,050
Right.

71

00:04:55,090  -->  00:04:57,260
And this is a classification tree.

72

00:04:57,280  -->  00:05:02,020
It reports back to me that you use the variables actually used in the tree construction.

73

00:05:02,020  -->  00:05:07,170
This case we used age and start the root node air meaning right off the bat.

74

00:05:07,210  -->  00:05:09,210
What is our error when we do that initial split.

75

00:05:09,230  -->  00:05:14,500
It's about 20 percent and then it gives you some more error information a lot of times you'll actually

76

00:05:14,500  -->  00:05:18,230
just want to visualize your tree and you can actually plot out your tree.

77

00:05:18,280  -->  00:05:25,360
You can say plots your tree or whatever the site it's called Decision Tree model and I'm going to put

78

00:05:25,360  -->  00:05:31,910
in uniform is equal to true and Main is just the main title.

79

00:05:31,940  -->  00:05:38,800
Go ahead and say Hi Folks this tree.

80

00:05:38,800  -->  00:05:44,440
Go ahead and click enter and you'll notice you'll get an error if your margins are too large.

81

00:05:44,470  -->  00:05:48,460
You're going to want to expand this first and then call the line

82

00:05:52,420  -->  00:05:54,080
and notice if you just call plot.

83

00:05:54,100  -->  00:05:58,900
You actually won't see anything which you have to do after that is called text as well

84

00:06:01,990  -->  00:06:08,320
and the text function is going to add text to this going to and pass in tree Kessin used.

85

00:06:08,380  -->  00:06:16,970
And as true and pass in all true and this just puts in labels into that decision tree.

86

00:06:17,260  -->  00:06:18,300
There it is.

87

00:06:18,400  -->  00:06:23,020
And the reason I did this in multiple steps was just to show you a few of the issues if you just call

88

00:06:23,020  -->  00:06:25,440
plot and text off of that tree.

89

00:06:25,450  -->  00:06:30,430
Couple of issues are you already need to have the plot window expanded then you need to plot the tree

90

00:06:30,520  -->  00:06:32,520
and plot the text separately.

91

00:06:32,530  -->  00:06:37,810
There's actually a much nicer library for this and it's just the our parts the plot library let me go

92

00:06:37,810  -->  00:06:39,800
ahead and show you how to install it.

93

00:06:39,880  -->  00:06:42,510
You can just go ahead and say install the packages.

94

00:06:42,760  -->  00:06:49,660
And as a string say our parts dot plots and this is a much nicer library her plotting out this decision

95

00:06:49,660  -->  00:06:53,120
trees going to go ahead and jump to the finished download.

96

00:06:53,740  -->  00:06:54,670
There it is.

97

00:06:55,150  -->  00:07:04,960
And let's go ahead and call Library our parts the plots and now we're going to do is just call the p

98

00:07:04,990  -->  00:07:11,770
r p function and the PR P function is our part up plots way of plotting a decision tree and then just

99

00:07:11,770  -->  00:07:17,260
passing your model is going to be a much nicer plot than what we've seen before and we can zoom in on

100

00:07:17,260  -->  00:07:17,500
this.

101

00:07:17,500  -->  00:07:21,940
And now this is really intuitive to interpret what's really cool about this.

102

00:07:21,940  -->  00:07:28,420
It will show you what it split on and what the actual split criteria was and then what the classes fill

103

00:07:28,450  -->  00:07:31,860
under this can get really awesome when you have a lot more features.

104

00:07:31,870  -->  00:07:35,310
But just keep in mind if you have a ton of features this could get complicated.

105

00:07:35,620  -->  00:07:38,500
But again a much nicer way of plotting these decision trees.

106

00:07:38,710  -->  00:07:44,590
Instead of using the double plot and then text method you can just call the our part up plot library

107

00:07:44,680  -->  00:07:47,770
and then call p r p on the tree.

108

00:07:47,770  -->  00:07:50,520
That model we just built to get this much nicer plot.

109

00:07:50,530  -->  00:07:55,490
That's what I highly recommend doing there want to plot out or visualize her decision trees.

110

00:07:55,600  -->  00:07:58,570
Let's go ahead and move on to random force.

111

00:07:58,750  -->  00:08:04,690
Remember that ran the Chloris improve predictive accuracy by generating a large number of bootstrap

112

00:08:04,720  -->  00:08:11,170
trees based on random samples of variables classifying a case using history in the New Forest and the

113

00:08:11,170  -->  00:08:18,100
sighting a final predicted outcome by combining the results across all of the trees and in classification

114

00:08:18,100  -->  00:08:20,500
it's a majority vote of the trees.

115

00:08:20,770  -->  00:08:29,990
Well we need to do is go ahead and install the random forest library or package go ahead and say install

116

00:08:29,990  -->  00:08:37,480
the packages and then type in random capital F. forest and this is the package we're going to be using

117

00:08:37,600  -->  00:08:40,070
for random forest.

118

00:08:40,090  -->  00:08:41,120
All right.

119

00:08:41,230  -->  00:08:45,460
Again go ahead and install that and then what you're going to do is just like the tree model we built

120

00:08:45,460  -->  00:08:45,690
.

121

00:08:45,880  -->  00:08:49,020
You can call ran the force.

122

00:08:49,030  -->  00:08:58,840
Let's go ahead and say our F model ran the forest model then just call random forest let's make sure

123

00:08:58,840  -->  00:09:03,460
we actually call the library ran the forest first.

124

00:09:05,410  -->  00:09:05,900
OK.

125

00:09:05,920  -->  00:09:08,860
Now it's called Let's actually start that again.

126

00:09:08,860  -->  00:09:12,570
Our model ran them for us.

127

00:09:12,590  -->  00:09:19,100
There's our function and this is going to take just like your tree then to take the formula.

128

00:09:19,190  -->  00:09:24,360
Now this should be a capital K because it's the actual column name filled dots.

129

00:09:24,760  -->  00:09:31,030
And then we're going to put in the data I Phasis that's the data frame.

130

00:09:31,570  -->  00:09:39,310
And now let's go ahead and just print out the model and this will basically give us an estimate of the

131

00:09:39,310  -->  00:09:41,360
error rates.

132

00:09:41,440  -->  00:09:45,490
Let me go ahead and clear this and call it one more time.

133

00:09:46,420  -->  00:09:48,100
Print R.F. model

134

00:09:51,070  -->  00:09:57,080
in something I want to point out is that if you actually look at the documentation for random force

135

00:09:57,100  -->  00:10:04,270
function you'll notice a list of arguments but also a list of values that are held inside a random force

136

00:10:04,290  -->  00:10:04,600
.

137

00:10:04,840  -->  00:10:09,740
That means there's a lot of components that you can actually grab out of this ran of course model since

138

00:10:09,740  -->  00:10:14,590
this random force model is essentially just a list of components.

139

00:10:14,620  -->  00:10:20,530
For example you want to grab the model put in the dollar sign and there's all these items in the list

140

00:10:20,530  -->  00:10:26,470
call type predicted we can call predicted in a report back for each of these values would it actually

141

00:10:26,470  -->  00:10:29,140
predict it when it ran the random forest.

142

00:10:29,550  -->  00:10:36,480
The other information for instance a number of trees grown you can call your model or model and type

143

00:10:36,480  -->  00:10:43,150
in and tree and it will tell you it grew it with 500 trees which is the default value of trees you can

144

00:10:43,150  -->  00:10:48,520
possibly lower your error or misclassification rate by adding more trees but at a certain point adding

145

00:10:48,520  -->  00:10:53,590
more trees is not going to help you a good way of deciding that it's like plotting out your number of

146

00:10:53,590  -->  00:11:01,140
trees versus your misclassification rate and you can even do things like call the confusion matrix offer

147

00:11:01,140  -->  00:11:03,940
your model.

148

00:11:03,940  -->  00:11:08,740
And again this is just for the training data that it was trained on.

149

00:11:08,740  -->  00:11:15,220
All right go ahead and explore all the different components that are inside the rain forest.

150

00:11:15,400  -->  00:11:18,150
More likely in real life scenarios.

151

00:11:18,160  -->  00:11:22,120
You're going to be using random force instead of just single decision trees.

152

00:11:22,120  -->  00:11:27,970
Just because a random force is such a powerful algorithm and it's so easy to use implements Plus it's

153

00:11:27,970  -->  00:11:35,030
really nice for distributed computing because you can just distribute some of the trees to one server

154

00:11:35,030  -->  00:11:38,490
or some of the trees to another server and they're all in the pivot of each other.

155

00:11:38,520  -->  00:11:43,030
That we don't need to communicate to each other until the aggregate the actual votes.

156

00:11:43,030  -->  00:11:43,720
All right.

157

00:11:43,720  -->  00:11:50,230
That's it for performing decision trees and ran them force in our hope those useful to go ahead and

158

00:11:50,230  -->  00:11:51,040
check up the notes.

159

00:11:51,040  -->  00:11:56,530
If you want more information on random force and the trees coming up next is your machine learning project

160

00:11:56,530  -->  00:11:56,700
.

161

00:11:56,820  -->  00:11:59,260
We're going to practice all of this.

162

00:11:59,290  -->  00:12:01,060
Thanks everyone and I'll see at the next lecture
