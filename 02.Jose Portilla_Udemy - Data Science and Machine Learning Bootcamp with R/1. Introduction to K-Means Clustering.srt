1

00:00:00,360  -->  00:00:04,920
Hello everyone and welcome to an introduction to k means clustering in this lecture.

2

00:00:04,950  -->  00:00:11,460
We're going to discuss that k means clustering algorithm which will allow us to cluster and label data

3

00:00:11,520  -->  00:00:14,740
in an unsupervised machine learning algorithm.

4

00:00:14,820  -->  00:00:17,240
You want to fool mathematics behind this algorithm.

5

00:00:17,310  -->  00:00:19,780
Go ahead and read Chapter 10 of an introduction.

6

00:00:19,830  -->  00:00:27,930
Just learning k means clustering is an unsupervised learning algorithm meaning it takes an unlabeled

7

00:00:28,020  -->  00:00:33,040
data it's going to attempt to group similar clusters together in your data.

8

00:00:33,060  -->  00:00:35,600
So what is the typical clustering problem look like.

9

00:00:35,850  -->  00:00:41,100
Well a few examples may be things like clustering similar documents together clustering customers based

10

00:00:41,100  -->  00:00:47,130
off of their features trying to perform market segmentation or even trying to identify similar physical

11

00:00:47,130  -->  00:00:48,910
groups.

12

00:00:48,930  -->  00:00:54,780
The overall goal is to divide data into distinct groups such that observations within each group are

13

00:00:54,780  -->  00:00:55,740
similar.

14

00:00:55,740  -->  00:01:03,420
Here you can see a diagram of unlabelled training data and k means clustering trying to attempt to cluster

15

00:01:03,420  -->  00:01:09,250
that data into five distinct clock colored clusters here.

16

00:01:09,330  -->  00:01:11,090
How does the algorithm actually work.

17

00:01:11,350  -->  00:01:18,180
Well you first choose a number of clusters K and we'll discuss how to choose k later on then you randomly

18

00:01:18,180  -->  00:01:25,230
assign each point in your data to a specific cluster then until the clusters stop changing you repeat

19

00:01:25,290  -->  00:01:30,990
the following steps for each cluster you're going to compute the cluster centroid by taking the mean

20

00:01:30,990  -->  00:01:33,270
vector of points in the cluster.

21

00:01:33,360  -->  00:01:38,830
Then you assign each data point to the cluster for which the centroid is the closest.

22

00:01:39,120  -->  00:01:45,170
Let's go ahead and take a look at the state of visualization or this plotted out visualization of k

23

00:01:45,170  -->  00:01:47,460
means clustering algorithm.

24

00:01:47,460  -->  00:01:52,740
You first start off of just the observations that's shown on the top left then in top center we have

25

00:01:52,740  -->  00:01:54,510
Step 1 of the algorithm.

26

00:01:54,510  -->  00:02:01,610
Each observation is randomly assigned to a cluster in the top right of step 2 A The cluster centroid

27

00:02:01,680  -->  00:02:03,120
are then computed.

28

00:02:03,180  -->  00:02:05,850
These are shown as large colored disks.

29

00:02:05,880  -->  00:02:11,070
Initially these centroid are almost completely overlapping because the initial cluster assignments were

30

00:02:11,070  -->  00:02:18,660
chosen at random in the bottom left and step to be each observation is assigned to the nearest centroid

31

00:02:19,340  -->  00:02:20,370
in the bottom center.

32

00:02:20,370  -->  00:02:22,760
Step two is once again perform.

33

00:02:22,770  -->  00:02:29,100
You can see that we've led to new cluster centroid and we basically keep repeating these steps until

34

00:02:29,160  -->  00:02:35,520
there's no new clusters meaning data points aren't being reassigned to a new cluster centroid at the

35

00:02:35,520  -->  00:02:36,060
bottom right.

36

00:02:36,060  -->  00:02:40,440
We have the results obtained after about 10 iterations.

37

00:02:41,460  -->  00:02:46,620
Let's discuss choosing a k value as we just saw to perform Kamins clustering.

38

00:02:46,680  -->  00:02:49,870
We as it decide how many clusters we expect in the data.

39

00:02:49,890  -->  00:02:54,440
The problem of selecting K isn't actually that simple.

40

00:02:54,480  -->  00:02:57,600
There's no easy answer for choosing a best case value.

41

00:02:57,630  -->  00:03:02,430
However one way to try to do it is using something known as the elbow method.

42

00:03:02,430  -->  00:03:07,560
First of all you compute the sum of squared error otherwise known as s s e.

43

00:03:07,680  -->  00:03:12,540
For some values of k for example 2 4 6 8 etc..

44

00:03:12,570  -->  00:03:18,030
The sum of squared errors is defined as the sum of the squared distance between each member of the cluster

45

00:03:18,120  -->  00:03:19,780
and its centroid.

46

00:03:20,550  -->  00:03:27,270
If you plot K against the SS e you will see that the error decreases as K gets larger.

47

00:03:27,270  -->  00:03:30,640
This is because in the number of clusters increases they should be smaller.

48

00:03:30,780  -->  00:03:33,040
So distortion is also smaller.

49

00:03:33,090  -->  00:03:39,090
The idea of the Elbel method is to choose the K at which the s s e decreases abruptly.

50

00:03:39,090  -->  00:03:44,040
This produces an elbow effect in the graph as you can see in the following picture.

51

00:03:44,070  -->  00:03:49,680
Here we can see an example of the plotted out Eldo method on the X horizontal axis.

52

00:03:49,680  -->  00:03:53,980
We have the K value which stands for the number of clusters we choose for.

53

00:03:54,000  -->  00:04:01,380
The algorithm on the y axis you have the within groups sum of squares you want to try to choose a K

54

00:04:01,830  -->  00:04:07,900
where you won't get that much more information by increasing the number of clusters.

55

00:04:07,920  -->  00:04:14,310
Many are not going to significantly decrease the within groups sum of squared error by increasing number

56

00:04:14,310  -->  00:04:15,530
of clusters.

57

00:04:15,630  -->  00:04:17,820
And that's why this is called the elbow method.

58

00:04:18,000  -->  00:04:23,870
As you can see an elbow occurring around the number let's say six to seven clusters.

59

00:04:23,880  -->  00:04:27,130
That means that's a good number to choose for your clusters.

60

00:04:27,150  -->  00:04:29,640
Again there's no correct value.

61

00:04:29,730  -->  00:04:34,350
So you want to use your domain experience or domain knowledge in whatever field you're working with

62

00:04:34,710  -->  00:04:40,310
in order to try to choose a correct or reasonable value.

63

00:04:40,320  -->  00:04:43,820
Let's go ahead and jump to our studio and begin to explore an example.

64

00:04:43,860  -->  00:04:48,180
Then you'll have a project to test your own understanding of the Kamins algorithm.

65

00:04:48,180  -->  00:04:50,630
Thanks everyone and I'll see you at the next lecture.
