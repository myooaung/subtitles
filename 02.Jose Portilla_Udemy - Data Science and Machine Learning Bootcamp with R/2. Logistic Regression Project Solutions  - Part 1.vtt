WEBVTT
1

00:00:00.870  -->  00:00:06.510
Hello everyone and welcome to the logistic regression project solutions lecture video is probably going

2

00:00:06.510  -->  00:00:12.390
to be the first part of a series of solutions video lectures for the logistic regression project.

3

00:00:12.390  -->  00:00:15.320
Let's go ahead and get started by jumping to our studio.

4

00:00:15.320  -->  00:00:15.670
All right.

5

00:00:15.690  -->  00:00:17.370
So here in our studio.

6

00:00:17.490  -->  00:00:22.690
First thing we have to make sure is that you have the adult underscore Souled Out CSB that adult salary

7

00:00:22.690  -->  00:00:29.930
see every file in your working directory or just know the entire path to it so you can pass it in.

8

00:00:30.030  -->  00:00:36.110
The first thing we had to do is actually read in the CSP file.

9

00:00:36.180  -->  00:00:39.120
Hopefully by now you're pretty comfortable doing this.

10

00:00:39.330  -->  00:00:44.170
And then let's go ahead and check the head of adults and explore it a bit.

11

00:00:44.880  -->  00:00:49.790
Let's run that push this over to the Rykes we won't be using it for now.

12

00:00:50.140  -->  00:00:53.630
And you're in the summertime so we use all that console space.

13

00:00:53.670  -->  00:00:54.870
Here we go.

14

00:00:54.930  -->  00:00:56.000
So what do we notice here.

15

00:00:56.010  -->  00:00:58.380
We have the age of the adult.

16

00:00:58.470  -->  00:01:07.940
The type of employer this SNL WGC column that column stands for final sampling weight.

17

00:01:08.010  -->  00:01:13.010
You can go ahead and check out the U.S. Census Web site for more information on what that actually is

18

00:01:13.020  -->  00:01:13.330
.

19

00:01:13.410  -->  00:01:18.660
But let's go ahead and focus on things that are easier to just interpret directly such as education

20

00:01:18.660  -->  00:01:24.000
level education which is basically a number of their educational level.

21

00:01:24.000  -->  00:01:29.010
They also have some mercial data whether they were married or never married divorce etc..

22

00:01:29.280  -->  00:01:36.960
Occupation which is another factor level feature relationship their race their gender or sex capital

23

00:01:36.960  -->  00:01:38.340
gain capital loss.

24

00:01:38.370  -->  00:01:42.430
Hours worked per week country and income.

25

00:01:42.780  -->  00:01:48.870
Something to notice right away is that we basically accidentally have a double indexing of this X column

26

00:01:48.870  -->  00:01:48.960
.

27

00:01:48.960  -->  00:01:50.090
That's pretty common.

28

00:01:50.130  -->  00:01:58.500
If you're writing and reading CSFB files with our and forgetting to ignore the index it will automatically

29

00:01:58.500  -->  00:02:02.460
place an index where you can go ahead and remove that with the plier

30

00:02:05.210  -->  00:02:14.700
you can just say call library the player and then set adults to select pasand the data frame adult.

31

00:02:14.700  -->  00:02:18.970
And we can either put in all the columns you want to select or put negative signs against the column

32

00:02:18.980  -->  00:02:19.040
.

33

00:02:19.050  -->  00:02:23.570
We don't want to select and that we'll go ahead and get rid of it.

34

00:02:23.700  -->  00:02:31.930
You can go ahead and also check out the structure of the data sets as well as on the go and just as

35

00:02:32.880  -->  00:02:38.250
the summary of the data set as well.

36

00:02:38.250  -->  00:02:41.550
It's going to run that right.

37

00:02:41.600  -->  00:02:46.920
We hear we have about thirty two thousand thirty two thousand five hundred sixty one observations of

38

00:02:46.920  -->  00:02:48.630
15 variables.

39

00:02:48.630  -->  00:02:50.770
Some of them are factors which make sense.

40

00:02:50.790  -->  00:02:54.520
Others are not which are most of them also make sense.

41

00:02:54.630  -->  00:03:00.510
Something to note here is that a lot of these columns have a ton of factor levels something like country

42

00:03:00.510  -->  00:03:07.170
with 42 factor levels is going to be a large amount of levels to actually be useful for logistic regression

43

00:03:07.180  -->  00:03:07.360
.

44

00:03:07.710  -->  00:03:12.660
Something we all want to do is try to use feature engineering to reduce the number of levels some single

45

00:03:12.660  -->  00:03:13.820
digit count is OK.

46

00:03:13.830  -->  00:03:19.080
But usually when you're looking at double digit factor levels you may want to consider feature engineering

47

00:03:19.350  -->  00:03:24.120
to actually group some of these levels together and we'll explore how to do that through data cleaning

48

00:03:24.140  -->  00:03:24.410
.

49

00:03:24.660  -->  00:03:30.150
Let's go ahead and use table to check out the frequency of the type employer column.

50

00:03:30.190  -->  00:03:32.010
They're going to show you how to do that.

51

00:03:32.070  -->  00:03:36.230
I'm going to switch over to doing a lot of things in the con..

52

00:03:36.240  -->  00:03:37.550
So keep that in mind.

53

00:03:37.740  -->  00:03:43.820
I'm going to use table on the type employer and essentially get a frequency count.

54

00:03:43.850  -->  00:03:48.970
Here we want to know is how many know values are there for type underscore employer.

55

00:03:48.990  -->  00:03:50.660
What are the two smallest groups.

56

00:03:50.910  -->  00:03:55.490
Well we have 100 way about one thousand eight hundred thirty six no values.

57

00:03:55.800  -->  00:03:58.500
In this case the knowledge is represented by a question marks.

58

00:03:58.500  -->  00:04:01.660
Keep that in mind it's not an actual and a it's a question mark.

59

00:04:01.830  -->  00:04:03.660
That means it's a string that's just a question mark.

60

00:04:03.720  -->  00:04:08.880
We'll have to transform it to an end a later on if we want it to be null and then we also have two small

61

00:04:08.880  -->  00:04:13.290
groups never worked which is seven without pay which is 14.

62

00:04:13.350  -->  00:04:17.350
It'll be nice to actually combine never works with without pay.

63

00:04:17.520  -->  00:04:22.200
And combine these two small groups into a single group called unemployed because basically if you're

64

00:04:22.320  -->  00:04:26.390
if you're never works or if you're not being paid you're essentially unemployed.

65

00:04:26.430  -->  00:04:30.330
There's lots of ways to do this so feel free to get creative as far as joining these groups together

66

00:04:30.330  -->  00:04:30.680
.

67

00:04:30.690  -->  00:04:36.020
I'm going to go ahead and use a function to join them.

68

00:04:36.060  -->  00:04:43.820
Let's go ahead and switch back to the script and say This section is called data cleaning.

69

00:04:44.790  -->  00:04:46.650
First thing I want to do is

70

00:04:49.440  -->  00:04:53.370
combine employer type

71

00:04:56.780  -->  00:05:02.980
I'm going to create a function called an MP you can call it whatever you want and it will take in some

72

00:05:02.980  -->  00:05:06.180
variable job.

73

00:05:06.970  -->  00:05:15.100
And what I'm going to do is go ahead and make sure that job is a character and I can do that by just

74

00:05:15.100  -->  00:05:20.980
stating let me expand this a little bit by stating job is now equal to as character Job and this ensures

75

00:05:20.980  -->  00:05:27.190
me that it's a character that we can do string comparisons so I can do something like this now I'll

76

00:05:27.190  -->  00:05:35.260
say if the job is equal to the string never works.

77

00:05:35.260  -->  00:05:46.420
So if it's equal to this or and we use that pipe operator job is equal to without pay

78

00:05:49.990  -->  00:05:51.550
or return unemployed

79

00:05:56.530  -->  00:05:58.480
else what we're going to do is just return

80

00:06:01.480  -->  00:06:07.860
the actual job itself and then we're going to apply this to the type employer column.

81

00:06:08.350  -->  00:06:12.360
Let's go ahead and then do that.

82

00:06:13.360  -->  00:06:16.540
Scroll down a bit and create some more lines here.

83

00:06:16.630  -->  00:06:20.930
Say apply.

84

00:06:23.320  -->  00:06:36.010
Type employer as supply call adult type employer and apply the an amp which is SAS for unemployed and

85

00:06:36.010  -->  00:06:41.860
then let's go ahead and print outs adults type employer.

86

00:06:41.890  -->  00:06:48.670
Just to make sure that worked and let's print out the actual table of this that we can see the frequency

87

00:06:48.670  -->  00:06:48.940
count.

88

00:06:48.940  -->  00:06:51.010
So keep in mind this is the previous one.

89

00:06:51.100  -->  00:06:52.970
We have never worked without pay.

90

00:06:53.110  -->  00:06:57.880
Hopefully after applying this function and we call that same table we will see these to combine to an

91

00:06:57.910  -->  00:06:59.400
unemployed column.

92

00:06:59.450  -->  00:07:01.450
Let's go ahead and run that.

93

00:07:02.550  -->  00:07:05.350
Okay perfect 7 plus 14 is 21.

94

00:07:05.460  -->  00:07:11.620
Now and implied that these are both unemployed variables instead of never working without pay.

95

00:07:11.740  -->  00:07:16.810
Since you are doing it now is feature engineering slash data cleaning because we're reducing the number

96

00:07:16.810  -->  00:07:22.060
of factory levels which will hopefully make our code more efficient later on and easier to interpret

97

00:07:22.060  -->  00:07:23.060
.

98

00:07:23.070  -->  00:07:27.790
There are some other columns that are suitable for combining such as the state and local government

99

00:07:27.790  -->  00:07:33.930
jobs into a category maybe for local government and maybe self-employed jobs as well.

100

00:07:33.940  -->  00:07:38.070
So we have self amp ink and self Ampe not Inc.

101

00:07:38.230  -->  00:07:42.180
So self-employed incorporated self-employed not incorporated.

102

00:07:42.250  -->  00:07:44.310
However these are both still self-employment.

103

00:07:44.470  -->  00:07:49.090
Let's go ahead and do a similar process to join the self-employment columns and then let's go ahead

104

00:07:49.090  -->  00:07:52.300
and join local government and state government.

105

00:07:52.450  -->  00:07:56.770
This is going to be a really similar job to this function.

106

00:07:57.130  -->  00:08:05.450
So I'm going to go ahead and copy those functions from the notes we remove that.

107

00:08:07.270  -->  00:08:08.040
Let's go ahead.

108

00:08:08.050  -->  00:08:21.620
In group self employed and states and local.

109

00:08:22.240  -->  00:08:25.140
I'm going to go ahead and copy and paste the function here.

110

00:08:25.150  -->  00:08:28.870
This function is essentially doing the exact same thing as you did earlier except for checking for different

111

00:08:28.870  -->  00:08:31.540
character strings.

112

00:08:31.540  -->  00:08:37.240
Something to keep in mind that this function is I'm no longer saying job as that character because I

113

00:08:37.240  -->  00:08:40.030
know since I already applied the on ramp function.

114

00:08:40.240  -->  00:08:45.020
Every job is already as a character and I can check that by using structure.

115

00:08:45.130  -->  00:08:50.440
So I don't have to call that anymore so I skip that line and I say if job is to go to a local government

116

00:08:50.440  -->  00:08:56.290
job or state government job go ahead and return s l Pashka which is going to stand for state local government

117

00:08:56.290  -->  00:09:02.170
job or else the job is self-employed incorporated or self-employed not incorporated.

118

00:09:02.170  -->  00:09:07.790
Go ahead and just return the basic label self dashed empy else return job.

119

00:09:08.380  -->  00:09:22.310
Let's go ahead and then apply that means we're gonna say adult type employer as apply adult type employer

120

00:09:22.320  -->  00:09:22.490
.

121

00:09:22.720  -->  00:09:29.810
And this one's called Group MP and let's go ahead and recheck that table.

122

00:09:29.860  -->  00:09:37.050
I will say Prince table of adults type employer.

123

00:09:37.060  -->  00:09:41.760
Let's run this now expand the council a little bit so you can see it.

124

00:09:42.310  -->  00:09:43.030
OK looking good.

125

00:09:43.030  -->  00:09:49.000
We have the federal government jobs private jobs self-employed jobs state and local government jobs

126

00:09:49.090  -->  00:09:51.080
and unemployed people.

127

00:09:51.160  -->  00:09:52.530
Perfect.

128

00:09:52.540  -->  00:09:56.640
Let's go ahead and leave this column alone for an Now we'll come back to it later on.

129

00:09:56.650  -->  00:09:58.620
Dealing with missing data points.

130

00:09:58.760  -->  00:10:03.970
But now let's go ahead and look at the Marchal column.

131

00:10:04.060  -->  00:10:08.400
You go ahead to label that part of the script will say group self-employed apply.

132

00:10:08.770  -->  00:10:14.690
Combine employer types just so it's a nice label.

133

00:10:14.830  -->  00:10:19.030
This falls under data cleaning combine employer types and now we're going to say

134

00:10:22.450  -->  00:10:33.370
marital status stat says and do a similar thing I'm going to go ahead and say Prince we can actually

135

00:10:33.370  -->  00:10:35.290
do this straight in the con..

136

00:10:35.560  -->  00:10:40.370
Check out the table of adults marital.

137

00:10:41.320  -->  00:10:41.930
OK.

138

00:10:42.100  -->  00:10:47.890
What do we have here we have divorced people never married people separated people widowed people marry

139

00:10:47.990  -->  00:10:52.200
their spouses absent married and they live with their spouse et cetera.

140

00:10:52.260  -->  00:10:58.120
What we want to do is just reduce this to three basic groups married not married and never married.

141

00:10:58.120  -->  00:11:03.250
And we're going to do a very similar operation just by using a function and applying it to our column

142

00:11:03.250  -->  00:11:04.060
.

143

00:11:04.120  -->  00:11:08.890
I want to go in and copy and paste function so don't waste a bunch of time just rewriting the notes

144

00:11:09.840  -->  00:11:10.790
.

145

00:11:11.020  -->  00:11:15.640
Let's walk through this function before we actually apply it here.

146

00:11:15.640  -->  00:11:21.820
I created a function called Group underscore marital and marital thing you want to pronounce it it takes

147

00:11:21.820  -->  00:11:28.300
in their status transforms it to a character and then checks if they're not married if they're separated

148

00:11:28.300  -->  00:11:35.180
divorced or widowed That means they are currently not married if they're never been married.

149

00:11:35.200  -->  00:11:40.360
We just check if they have been married and we apply that label never married since that's already a

150

00:11:40.360  -->  00:11:42.880
label we're just going to go ahead and return as they are.

151

00:11:43.000  -->  00:11:48.640
Otherwise if they're not separated divorced widowed or never been married then they are married either

152

00:11:48.670  -->  00:11:52.780
happily or unhappily so we'll go ahead and continue on by applying this

153

00:11:57.060  -->  00:12:03.200
and it creates some new lines as you can see our our code is starting to get quite a few lines in it

154

00:12:03.200  -->  00:12:03.220
.

155

00:12:03.220  -->  00:12:13.220
So just keep that in mind and where we're going to go out and do say adult marital supply.

156

00:12:13.590  -->  00:12:14.560
Passen the funk.

157

00:12:14.560  -->  00:12:23.620
The column that we want to affect and then the function that we just created and we'll go ahead and

158

00:12:23.620  -->  00:12:31.170
do a quick check by printing out the table of that column.

159

00:12:31.390  -->  00:12:34.590
Go ahead and run this.

160

00:12:36.550  -->  00:12:37.300
Okay perfect.

161

00:12:37.300  -->  00:12:40.670
We were able to transform all of these factors into three factors.

162

00:12:40.670  -->  00:12:43.440
Married never married and not married.

163

00:12:43.690  -->  00:12:48.910
Let's go ahead and continue this sort of data cleaning aspect.

164

00:12:48.910  -->  00:12:51.120
Hopefully you realize the theme here.

165

00:12:51.220  -->  00:12:55.650
Basically create a function and apply it in trying to reduce the number of factors.

166

00:12:55.660  -->  00:13:02.950
This case we're going to check out the country column.

167

00:13:03.070  -->  00:13:11.100
Let's start by moving over to the con. And just performing at the table off of the country column.

168

00:13:11.500  -->  00:13:12.080
OK.

169

00:13:12.130  -->  00:13:17.560
You can see here that this was one of the ones I had the most levels we have countries from all over

170

00:13:17.560  -->  00:13:18.960
the world.

171

00:13:19.150  -->  00:13:23.790
What we want to do is group these countries together and you can group them in any way you see fit.

172

00:13:23.800  -->  00:13:27.280
I gave you flexibility here because there's really no right or wrong way.

173

00:13:27.280  -->  00:13:30.550
One possible way to do it is grouping by continents.

174

00:13:30.550  -->  00:13:33.980
You should be able to reduce number of groups here significantly though.

175

00:13:34.060  -->  00:13:37.630
Go ahead and explore the different ways you could group these.

176

00:13:37.810  -->  00:13:40.810
I grouped them by more or less continents.

177

00:13:40.960  -->  00:13:45.430
Except I also made an exception exception that of North America or South America.

178

00:13:45.550  -->  00:13:48.930
I did North America versus Latin and South America.

179

00:13:49.210  -->  00:13:56.740
I'm going to go ahead and copy the groups I did and walk through this grouping or this particular application

180

00:13:56.830  -->  00:14:04.440
of data cleaning first thing I did was I created vectors based off of the names of the countries.

181

00:14:04.480  -->  00:14:06.990
And this took a while because it kind of had a manually type them out.

182

00:14:07.060  -->  00:14:14.590
I just copied and pasted from this table and grouped them into Asia category North America category

183

00:14:14.890  -->  00:14:19.970
Europe category Latin and South America category and that other category.

184

00:14:20.110  -->  00:14:23.000
I couldn't tell what South was as a label.

185

00:14:23.120  -->  00:14:28.130
There was a South column here few years 80 countries and South but I'm not sure what that was.

186

00:14:28.270  -->  00:14:32.990
So I went ahead instead of putting them to Asia North America Europe or Latin and South America.

187

00:14:33.010  -->  00:14:36.020
I just put them on other other.

188

00:14:36.580  -->  00:14:40.600
Maybe if you wanted to you could continue to further split up these groups.

189

00:14:40.600  -->  00:14:47.050
Maybe you have a West Europe versus an Eastern Europe because of the factors of England France and Germany

190

00:14:47.050  -->  00:14:52.390
are usually different than countries such as Poland or Hungary Yugoslavia I should mention the state

191

00:14:52.400  -->  00:14:57.420
assets are quite old from about 1996 and then we also have Latin and South America.

192

00:14:57.460  -->  00:15:02.610
Maybe you could have separated it to Latin America and South Americas two separate groups you could

193

00:15:02.610  -->  00:15:06.190
have done Asia as Middle East versus East Asia etc..

194

00:15:06.250  -->  00:15:08.510
Here I just did some pretty simple groupings.

195

00:15:08.620  -->  00:15:11.040
Hopefully that works out.

196

00:15:11.050  -->  00:15:17.210
Let's go ahead and apply this and I use the in operator to save some typing space.

197

00:15:17.330  -->  00:15:25.260
I want to go ahead and copy from the notes the actual apply function for the countries.

198

00:15:25.270  -->  00:15:29.680
That means I created a function called Group underscore country.

199

00:15:29.680  -->  00:15:36.700
It takes in the city or Y the country value and essentially shuts about it just a bunch of if an ELSE

200

00:15:36.790  -->  00:15:43.120
IF statements checking if the country is in one of these vectors that I created is going to check if

201

00:15:43.120  -->  00:15:43.930
it's in Asia.

202

00:15:43.930  -->  00:15:48.120
If it is it returns Asia checks if it's in North America etc..

203

00:15:48.190  -->  00:15:50.130
Europe Latin and South America.

204

00:15:50.170  -->  00:15:52.440
Otherwise it just returns other.

205

00:15:52.720  -->  00:15:56.100
That means there's actually no check to check if it's an other.

206

00:15:56.110  -->  00:15:58.960
It just doesn't else statement for that in case we miss anything.

207

00:15:58.960  -->  00:16:00.700
It just goes under other.

208

00:16:00.700  -->  00:16:06.590
Let's go ahead and apply it.

209

00:16:06.600  -->  00:16:20.090
I'm going to go ahead and say adults country lips as apply and I'm going to go out and say adults country

210

00:16:20.620  -->  00:16:27.280
and the name of a function was Group country and then I'm going to confirm that the groupings actually

211

00:16:27.280  -->  00:16:34.540
work by just printing out the table of adults underscore a country.

212

00:16:34.720  -->  00:16:36.180
Let's go ahead and run this.

213

00:16:36.200  -->  00:16:38.030
See if it worked out.

214

00:16:38.350  -->  00:16:42.250
Something to note is that basically every time I'm running it I'm performing all the operations just

215

00:16:42.250  -->  00:16:43.820
listed in the script.

216

00:16:43.870  -->  00:16:47.310
So I get slower and slower as we add more functions to this.

217

00:16:47.530  -->  00:16:50.110
But it's not going to slow down too much.

218

00:16:50.110  -->  00:16:56.380
All right perfect we were able to reduce this to just five groups of Asia Europe Latin and South America

219

00:16:56.440  -->  00:16:58.960
North America and other verses.

220

00:16:58.990  -->  00:17:04.060
All of these various countries that also didn't have too many data points within them separately but

221

00:17:04.060  -->  00:17:08.190
grouped together we can actually get substantial amounts of data points in each one.

222

00:17:08.320  -->  00:17:11.760
And hopefully that's better for our logistic regression model later on.

223

00:17:12.130  -->  00:17:14.830
Let's go ahead and check the structure of adults again

224

00:17:18.970  -->  00:17:20.860
just doing this in the con..

225

00:17:21.550  -->  00:17:28.510
And if we take notice here the columns we affected such as type employer and country and the marital

226

00:17:28.510  -->  00:17:31.710
status have all been changed to character columns.

227

00:17:31.810  -->  00:17:37.510
But we're going to want to do is go ahead and call factor on these columns to refactor them and make

228

00:17:37.510  -->  00:17:42.670
sure they don't have these old levels but these new ones that we created it's that I'm going to go ahead

229

00:17:42.670  -->  00:17:51.820
and just copy the code for this from the notebook where essentially just applying factor and I should

230

00:17:51.820  -->  00:17:53.680
note there's only two ways to do this.

231

00:17:53.680  -->  00:17:57.810
You can just call factor on the column or you can use as apply on a factor.

232

00:17:57.850  -->  00:17:59.430
There are essentially the same thing.

233

00:17:59.590  -->  00:18:05.140
Previously we've done it with the factory method you just call factor on the column itself but you can

234

00:18:05.140  -->  00:18:08.380
also do as apply and factor.

235

00:18:08.380  -->  00:18:09.250
Either way it works.

236

00:18:09.250  -->  00:18:14.610
I just want to show you that there's multiple ways of doing it.

237

00:18:14.770  -->  00:18:20.110
Usually I would probably just call the factor function itself but I want to show you what's possible

238

00:18:20.110  -->  00:18:22.990
with a supply by calling it this way.

239

00:18:22.990  -->  00:18:29.800
And then finally we'll go ahead and print the structure of adults again just to make sure it works.

240

00:18:30.130  -->  00:18:34.580
If we take a look at adult right now we can see we have these character columns.

241

00:18:34.750  -->  00:18:38.660
Let's run the script from the source.

242

00:18:38.660  -->  00:18:40.880
Check out that print statement.

243

00:18:41.200  -->  00:18:43.870
It's going to print out the structure and a little bit.

244

00:18:44.380  -->  00:18:45.350
There it is.

245

00:18:45.700  -->  00:18:52.720
And we noticed that we now have the factor of six levels and country is a factor of five levels and

246

00:18:52.720  -->  00:18:54.490
the marriage satisfactory or three levels.

247

00:18:54.490  -->  00:18:54.880
Perfect.

248

00:18:54.880  -->  00:18:58.560
That's exactly what it wanted to occur.

249

00:18:58.820  -->  00:19:03.280
We could do still play around with education occupation to try to reduce number of factors for those

250

00:19:03.280  -->  00:19:04.010
columns.

251

00:19:04.270  -->  00:19:09.250
But for now go ahead and move on to dealing with missing data in part two of this logistic regression

252

00:19:09.250  -->  00:19:10.170
project.

253

00:19:10.300  -->  00:19:13.740
Feel free to group those columns as well and see how they affect your model.

254

00:19:13.750  -->  00:19:18.730
It's totally up to you on how much you actually want to clean and mess around this data and perform

255

00:19:18.730  -->  00:19:20.140
feature engineering on it.

256

00:19:20.230  -->  00:19:25.030
For instance there's still quite a few factors in education and there's still quite a few factors in

257

00:19:25.360  -->  00:19:26.280
occupation as well.

258

00:19:26.280  -->  00:19:27.720
They're both still in double digits.

259

00:19:27.880  -->  00:19:29.270
I'll leave them like this for now.

260

00:19:29.320  -->  00:19:34.320
It's not a huge deal to leave him in like that but you can go ahead and explore play around with and

261

00:19:34.320  -->  00:19:36.430
clean your data a little more.

262

00:19:36.430  -->  00:19:37.540
All right.

263

00:19:37.540  -->  00:19:38.810
Hopefully that was helpful to you.

264

00:19:38.860  -->  00:19:42.250
Let's just do a very quick brief overview of everything we just did.

265

00:19:42.250  -->  00:19:48.040
We went ahead and read in our data we dropped that repeated index and then essentially we've only been

266

00:19:48.040  -->  00:19:53.200
cleaning our data so far by combining factory levels so that they're a little simpler to understand

267

00:19:53.200  -->  00:19:58.170
a little more interpretable and also so we have more data points per factor level.

268

00:19:58.430  -->  00:20:01.270
Okay thanks everyone and I will see you at the next lecture
