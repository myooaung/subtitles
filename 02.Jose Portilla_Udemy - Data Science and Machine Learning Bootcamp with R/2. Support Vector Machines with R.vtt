WEBVTT
1

00:00:01.590  -->  00:00:07.020
Hello everyone and welcome to support vector machines with our lecture in this lecture we're just going

2

00:00:07.020  -->  00:00:13.620
to quickly show you how to actually create a support machine model with our well we'll use the iris

3

00:00:13.620  -->  00:00:17.560
data set that we're already familiar with from the nearest neighbors lectures.

4

00:00:17.580  -->  00:00:21.150
Let's go ahead and jump to our studio and get started.

5

00:00:21.150  -->  00:00:21.580
OK.

6

00:00:21.600  -->  00:00:23.520
Here we are in our studio.

7

00:00:23.650  -->  00:00:31.050
We're going to go ahead and call library I S L R and then we're going to just remind ourselves what

8

00:00:31.050  -->  00:00:35.080
the iris dataset looks like by printing out the head of it.

9

00:00:35.880  -->  00:00:36.140
OK.

10

00:00:36.150  -->  00:00:44.040
There's a sample length sample with pedaling pedal with and three possible species to predict before

11

00:00:44.040  -->  00:00:49.380
we can actually use support vector machines with our what we have to do is install the correct package

12

00:00:49.680  -->  00:00:55.800
that has support vector machines in it which are going to have to do is say install packages and we're

13

00:00:55.800  -->  00:00:59.720
going to install the e 1 0 7 1 package.

14

00:00:59.850  -->  00:01:04.190
Make sure to pass this in as a string.

15

00:01:04.380  -->  00:01:08.890
I'm going to go ahead and jump to the finished downloading and installation of this package.

16

00:01:09.360  -->  00:01:12.320
OK here we are successfully impact and installed.

17

00:01:12.540  -->  00:01:18.780
Once you've actually installed this package what you should go ahead and use your control I'll call

18

00:01:18.780  -->  00:01:22.300
the library e 1 0 7 1.

19

00:01:23.310  -->  00:01:28.110
And then don't worry about the warning message it just tells you what version of our was built in versus

20

00:01:28.110  -->  00:01:29.960
the current version we're using.

21

00:01:30.180  -->  00:01:37.980
And then go ahead and say help and pasan s the M.

22

00:01:38.760  -->  00:01:42.490
This should bring up the help documentation for support vector machines.

23

00:01:42.510  -->  00:01:47.730
I would really recommend that you actually just briefly look over the support documentation for support

24

00:01:47.730  -->  00:01:49.040
vector machines.

25

00:01:49.050  -->  00:01:51.040
You'll go ahead and see the arguments.

26

00:01:51.060  -->  00:01:57.810
This particular algorithm function is going to take the formula and data method that we're already familiar

27

00:01:57.810  -->  00:01:58.450
with.

28

00:01:58.470  -->  00:02:03.240
That means we'll be passing in a formula of what we're trying to predict and the data frame that we're

29

00:02:03.240  -->  00:02:05.020
trying to use for the prediction.

30

00:02:05.070  -->  00:02:10.590
You can also then pasand things such as the kernel we saw earlier in the lecture that a very simple

31

00:02:10.590  -->  00:02:16.470
linear kernel just draws a line trying to separate the data but then we can also do polynomial kernels

32

00:02:16.470  -->  00:02:20.270
radial basis kernel and sigmoid kernel by default.

33

00:02:20.280  -->  00:02:23.660
It's going to choose the radial basis kernel which is the most common kernel.

34

00:02:23.820  -->  00:02:29.280
And again that was the idea of seeing things higher they mention in order to create that hyperplane

35

00:02:29.280  -->  00:02:30.940
split.

36

00:02:30.990  -->  00:02:36.150
You also notice you can pass in other parameters such as the cost and gamma that you're going to use

37

00:02:36.150  -->  00:02:37.770
if your support vector machine.

38

00:02:37.770  -->  00:02:42.810
If you did the reading in introduction to Stichel learning you'll have an understanding of what gamma

39

00:02:42.930  -->  00:02:50.130
and cost are usually however well under We'll have an intuition as to what gamma and cost you should

40

00:02:50.130  -->  00:02:56.150
initially use and I'll show you a way to nicely tune the parameters of your support vector machine.

41

00:02:56.430  -->  00:03:00.500
But let's go ahead and get started by actually creating that model.

42

00:03:00.650  -->  00:03:05.960
We're going to do control L and create a support vector machine model.

43

00:03:06.000  -->  00:03:07.980
This is actually very simple to do.

44

00:03:08.280  -->  00:03:11.740
You just go ahead and type in which you want to predict.

45

00:03:11.820  -->  00:03:18.500
I want to predict species and then the tilt sign and I don't use all the features in my data frame and

46

00:03:18.510  -->  00:03:21.530
say My data is the iris data set.

47

00:03:22.200  -->  00:03:26.280
Hopefully by now you're very familiar with this sort of tactic as far as building a training a model

48

00:03:26.280  -->  00:03:26.510
.

49

00:03:26.580  -->  00:03:32.670
We just say the model name SVM function passing your formula and passing your data later on we'll see

50

00:03:32.670  -->  00:03:38.940
some more parameters we can pass with SVM this case the school had and train that model and then call

51

00:03:38.940  -->  00:03:39.990
the summary of that model

52

00:03:43.260  -->  00:03:45.830
and you'll get an output that looks like this.

53

00:03:45.900  -->  00:03:51.300
Here you'll see the parameters that you use in this case for doing classification with for radio kernel

54

00:03:51.750  -->  00:03:56.090
with a cost of 1 and a gamma of 0.2 5.

55

00:03:56.190  -->  00:04:01.460
And we're trying to predict out of three classes and there was 51 support vectors.

56

00:04:02.160  -->  00:04:07.350
And as a quick reminder if you wanted to actually predict values using your model you would just use

57

00:04:07.350  -->  00:04:09.170
the Predict function.

58

00:04:09.180  -->  00:04:12.900
Let me go ahead and copy this code to the script so you can reference it

59

00:04:16.000  -->  00:04:20.440
or clear the console right now if you want it to predict some values.

60

00:04:20.460  -->  00:04:22.110
You would just use a predicate function.

61

00:04:22.440  -->  00:04:27.090
You would say predict you would Pessin your model and then your test data.

62

00:04:27.120  -->  00:04:31.410
In this case and I did a do a test train split but I could do something like just pass in my training

63

00:04:31.410  -->  00:04:32.240
data.

64

00:04:32.310  -->  00:04:34.070
You should never really do this.

65

00:04:34.230  -->  00:04:38.370
As far as predicting off the data you're trained on because you're going to do very well since your

66

00:04:38.370  -->  00:04:41.540
model was literally trained on that data.

67

00:04:41.880  -->  00:04:48.300
But then you could just say something like table the predicted values versus the actual values and in

68

00:04:48.300  -->  00:04:53.620
this case the actual values is going to be the species column.

69

00:04:53.760  -->  00:04:59.790
And here you can see it performs almost perfectly except it accidently gets fooled by some noise between

70

00:04:59.790  -->  00:05:02.590
versicolor and virginica and mislabel.

71

00:05:02.640  -->  00:05:05.810
Two of those flower species.

72

00:05:06.000  -->  00:05:06.750
All right.

73

00:05:06.910  -->  00:05:07.600
Moving along.

74

00:05:07.620  -->  00:05:15.490
Let's focus now on tuning in this is where we're going to actually specialize our understanding of support

75

00:05:15.490  -->  00:05:18.680
vector machines as far as creating them in our.

76

00:05:18.880  -->  00:05:24.490
We noticed that when we called summary of model let me go and show that again one more time we had these

77

00:05:24.490  -->  00:05:26.100
parameters.

78

00:05:26.110  -->  00:05:32.740
Now after doing the reading introductions to learning you hopefully understand the cost and gamma parameters

79

00:05:32.740  -->  00:05:40.960
for support vector machines there's just a very brief explanation of what cost and gamma our cost is

80

00:05:40.990  -->  00:05:47.780
basically what allows the support vector machines to have what's known as a soft margin in the example

81

00:05:47.770  -->  00:05:53.670
of visualization as I showed during the lecture we had a hard margin noting that no training labels

82

00:05:53.680  -->  00:05:55.560
could cross over the margin.

83

00:05:55.560  -->  00:06:00.930
But if you actually have a cost value you can play around with it and allow for soft margins meaning

84

00:06:00.940  -->  00:06:06.350
that the SBM allows some examples to be ignored or placed on the wrong side of that margin.

85

00:06:06.550  -->  00:06:10.190
And that sort of innovation actually often leads to a better fit.

86

00:06:10.620  -->  00:06:15.840
So the cost is the parameter for the soft margin cost function which controls the influence of each

87

00:06:15.940  -->  00:06:17.570
individual support vector.

88

00:06:17.700  -->  00:06:21.960
And this process involves trading error penalty for stability.

89

00:06:21.970  -->  00:06:27.450
Again make sure you read about the introduction of statistical learning's chapter on support vector

90

00:06:27.450  -->  00:06:30.240
machines to fully understand that cost function.

91

00:06:30.390  -->  00:06:36.370
But just as a brief overview it essentially just allows certain training labels to cross over that soft

92

00:06:36.370  -->  00:06:37.590
margin.

93

00:06:38.110  -->  00:06:44.050
The gamma parameter is a little more complicated because it has to do with non-linear kernel functions

94

00:06:44.050  -->  00:06:44.520
.

95

00:06:44.520  -->  00:06:50.050
Again I was showing visualisations where we just did a linear split of a line but you can use these

96

00:06:50.050  -->  00:06:54.460
kernel tricks and involve nonlinear kernel functions.

97

00:06:54.490  -->  00:07:02.650
And this was actually invented around 1992 but basically with the radio kernel function which is a non-linear

98

00:07:02.640  -->  00:07:09.640
kernel function the gamma is the free parameter of the Gaussian radial basis function was essentially

99

00:07:09.630  -->  00:07:17.020
just a formula and a small gamma means a Gaussian if a large variance so the influence of the support

100

00:07:17.050  -->  00:07:23.620
vector is more and a small gamma implies that the class of this support vector is going to have an influence

101

00:07:23.680  -->  00:07:27.270
on the side in the class of the vector.

102

00:07:27.270  -->  00:07:32.770
Even if the distance between them is large that basically means if gamma is large then the variance

103

00:07:32.760  -->  00:07:37.720
is small implying the support vector does not have a widespread influence.

104

00:07:37.840  -->  00:07:43.780
Technically speaking large gamma leads to high bias and low variance models and vice versa.

105

00:07:43.770  -->  00:07:45.250
Again I can't stress enough.

106

00:07:45.260  -->  00:07:50.310
Make sure read introduction to Siskel learning section on support vector machines to truly understand

107

00:07:50.800  -->  00:07:53.860
how to interpret gamma and cost.

108

00:07:53.880  -->  00:07:58.860
Right now we're going to show you how to use R to actually tune those parameters and have a computer

109

00:07:59.130  -->  00:08:02.720
choose the best cost and gamma values for you.

110

00:08:02.740  -->  00:08:08.470
Let's go ahead and clear the console and show you how to use tune.

111

00:08:08.620  -->  00:08:12.310
You can use tune in the following manner.

112

00:08:12.310  -->  00:08:16.530
You go ahead and say tune results as your variable name.

113

00:08:16.560  -->  00:08:20.080
Are going to save your result set and these the tune function.

114

00:08:20.070  -->  00:08:24.480
And again this generic function just tune's hyper parameters of statistical methods.

115

00:08:24.500  -->  00:08:26.990
Specially using a grid search.

116

00:08:27.850  -->  00:08:30.030
You pass in your model that you want to use in this case.

117

00:08:30.060  -->  00:08:32.390
Only use SVM.

118

00:08:33.100  -->  00:08:38.520
Now this is just the actual function of the model of the train the model because we're going to be training

119

00:08:38.520  -->  00:08:46.510
a bunch of SD models and seeing which one has the best fits that are going to see train that x is equal

120

00:08:46.500  -->  00:08:47.700
to my training data.

121

00:08:47.710  -->  00:08:52.860
In this case this is Irus columns 1 through 4.

122

00:08:53.430  -->  00:08:54.780
Then I say training.

123

00:08:54.810  -->  00:09:00.010
Why is my next argument that's going to be the actual label that I'm training for.

124

00:09:01.410  -->  00:09:05.830
I'm going to specify the kernel I want in this case.

125

00:09:05.830  -->  00:09:08.140
I'm just going to say the kernel is the radio kernel

126

00:09:11.690  -->  00:09:12.090
loops

127

00:09:14.740  -->  00:09:23.020
and I'm also going to put in ranges in ranges where you actually start putting in the range of gamma

128

00:09:23.010  -->  00:09:24.250
values and cost values.

129

00:09:24.270  -->  00:09:26.040
You want to do so ranges.

130

00:09:26.050  -->  00:09:35.280
Takes any list arguments of parameters and each parameter that is going to actually be a vector of values

131

00:09:35.910  -->  00:09:37.610
so you can pass in a vector.

132

00:09:37.680  -->  00:09:40.450
For instance we can pasan for cost.

133

00:09:40.440  -->  00:09:52.620
Let's go ahead and test out 0.1 1 and 10 and then for gamma we can also go ahead and test out any other

134

00:09:52.620  -->  00:10:01.380
value we pass and a vector of values we want to test for gamma 0.5 1 to and when you go ahead and run

135

00:10:01.380  -->  00:10:05.450
that depending on your computer that may take a while.

136

00:10:05.460  -->  00:10:07.070
So just keep that in mind.

137

00:10:07.260  -->  00:10:16.650
Then you call a summary of your results and you will get some information as far as the error versus

138

00:10:16.650  -->  00:10:24.450
the cost and gamma combinations and this summary will report back the best cost and gamma values in

139

00:10:24.450  -->  00:10:31.050
this case the best cost and gamma combination was cost of one gamma of 0.5.

140

00:10:31.320  -->  00:10:35.880
Let me go ahead and clear this and explain one more time where we're actually doing here because it

141

00:10:35.880  -->  00:10:41.510
is a little complicated to use tuning if you're not super familiar with support vector machines.

142

00:10:42.240  -->  00:10:43.480
What we have here.

143

00:10:43.530  -->  00:10:48.940
We're going to go ahead and say Let me just show tune ups.

144

00:10:49.440  -->  00:10:53.870
We have the tune function which is going to essentially do a grid search.

145

00:10:53.880  -->  00:10:58.920
And what that means is just try a bunch of different combinations of the support vector machine parameters

146

00:10:59.160  -->  00:11:02.430
and see which one has the lowest error rate.

147

00:11:02.550  -->  00:11:09.950
So you pass in the model you say train the X pass in your features that you want to train on say train

148

00:11:09.950  -->  00:11:17.910
that Y and the actual labels and then I'm going to just test it on the radio kernel and then you say

149

00:11:17.910  -->  00:11:24.840
ranges pasand a list of a vector of Castaldi is that you want to test and a vector of gamma values that

150

00:11:24.840  -->  00:11:26.110
you want to test.

151

00:11:26.200  -->  00:11:31.290
Well last time we just ran this we saw that one and 0.5 were the best.

152

00:11:31.290  -->  00:11:34.700
That means maybe you want to start choosing values around those.

153

00:11:34.710  -->  00:11:43.510
Or I could say 0.1 zero point five and 0.7 as my gamma values and then I'll start choosing around 1

154

00:11:43.510  -->  00:11:43.850
.

155

00:11:44.100  -->  00:11:52.740
Maybe I want to do 0.5 for cost 1 and then 1.5 and then I'll check out those results.

156

00:11:52.880  -->  00:11:56.990
Run that again that may take a while depending on your computer.

157

00:11:57.690  -->  00:12:00.090
And then I'll ask for the summary of the tune results.

158

00:12:00.090  -->  00:12:06.450
And now notice that I actually have another set of best parameters ends up the cost 1.5 and Gamma's

159

00:12:06.450  -->  00:12:12.830
or 0.1 is a better combination than the previous one which was 1.0.

160

00:12:12.870  -->  00:12:16.000
And I believe 0.5.

161

00:12:16.020  -->  00:12:16.560
All right.

162

00:12:16.560  -->  00:12:25.470
And then once you know the parameters in this case we know we want costs to be 1.5 and we want gamma

163

00:12:25.560  -->  00:12:29.190
to be 0.1

164

00:12:31.710  -->  00:12:35.940
and we could keep messing around with more combinations but was going to leave it like that for now

165

00:12:35.940  -->  00:12:36.280
.

166

00:12:36.540  -->  00:12:43.890
You can go ahead and say tuna SVM call the SBM function passing your formula in this case and trying

167

00:12:43.880  -->  00:12:52.830
to predict species off of all the data on my data frame say my data is Irus in your case you would use

168

00:12:52.830  -->  00:12:55.920
your training data if you're doing a train test split.

169

00:12:56.100  -->  00:12:59.110
I'll say my kernel is radial.

170

00:12:59.310  -->  00:13:04.740
Note that that's the default will say cost is equal to one looks.

171

00:13:04.770  -->  00:13:06.890
I mean 1.5.

172

00:13:06.930  -->  00:13:10.830
And then we'll say gamma is equal to 0.1.

173

00:13:11.540  -->  00:13:13.360
And now we have a two SVM.

174

00:13:13.380  -->  00:13:17.580
So I can call the summary off my tuned SVM

175

00:13:20.630  -->  00:13:23.910
and don't get too confused here because there are two separate steps.

176

00:13:23.910  -->  00:13:29.200
One step is that Toona function which returns what cost gamma to use.

177

00:13:29.310  -->  00:13:35.040
And the next step is you actually redoing your model with those particular values and again you can

178

00:13:35.040  -->  00:13:40.410
go in and check the notes or the documentation for tune if you want more help on how to actually use

179

00:13:40.410  -->  00:13:42.480
those functions.

180

00:13:42.480  -->  00:13:45.460
All right that's it for this lecture.

181

00:13:45.480  -->  00:13:50.850
Go ahead and review the notes if you need any more help on these topics what you should have gone out

182

00:13:51.030  -->  00:13:57.510
gotten out of this lecture is the ability to use SVM and the ability to use the tune function if you

183

00:13:57.510  -->  00:13:58.500
want more help on those.

184

00:13:58.500  -->  00:14:04.920
I really suggest one that you read the SVM section of the introduction to Stichel learning book and

185

00:14:04.920  -->  00:14:13.030
also go ahead and check out the documentation for SVM by calling Help SVM and reading through it.

186

00:14:13.050  -->  00:14:15.650
There's also an example at the bottom of this.

187

00:14:15.650  -->  00:14:20.140
If you scroll the way down you'll see some example code using the iris data set.

188

00:14:20.430  -->  00:14:22.170
And there some links for you to check out.

189

00:14:22.210  -->  00:14:29.360
And also I would also recommend that you check out the help for the tune function which is just premiÃ¨re

190

00:14:29.370  -->  00:14:31.620
tuning a functions using grid search.

191

00:14:31.620  -->  00:14:36.110
Again all that's doing is it's basically you put in a bunch of combinations for the parameters.

192

00:14:36.120  -->  00:14:41.760
It tests them all and see what's the best combination the results and the lowest error.

193

00:14:41.760  -->  00:14:43.460
OK thanks everyone.

194

00:14:43.470  -->  00:14:44.580
Hope those things for you.

195

00:14:44.730  -->  00:14:48.670
And coming up next is your support machine project.

196

00:14:48.690  -->  00:14:49.110
Thanks.

197

00:14:49.110  -->  00:14:50.570
I'll see you at the next lecture.
