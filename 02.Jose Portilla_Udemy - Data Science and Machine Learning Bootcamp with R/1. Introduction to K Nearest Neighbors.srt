1

00:00:01,260  -->  00:00:06,810
Hello everyone and welcome to an introduction to the nearest neighbors lecture and this lecture will

2

00:00:06,810  -->  00:00:13,980
discuss the Kinnear's neighbor's algorithm and how to use it for classification problems to understand

3

00:00:13,980  -->  00:00:16,280
the mathematics behind this algorithm.

4

00:00:16,350  -->  00:00:23,640
Go ahead and finish reading Chapter 4 of an introduction to statistical learning cannoneers neighbors

5

00:00:23,730  -->  00:00:30,110
sometimes known as k and an is a classification algorithm that operates on a very simple principle.

6

00:00:30,300  -->  00:00:32,970
It's actually just best shown through example.

7

00:00:32,970  -->  00:00:38,910
Imagine we have some imaginary data on dogs and horses with heights and weights of these dog and horse

8

00:00:39,000  -->  00:00:41,250
data points.

9

00:00:41,250  -->  00:00:43,830
If we plot them out it looks something like this.

10

00:00:43,890  -->  00:00:47,990
Again this is just imaginary data so don't worry about the weights and the heights.

11

00:00:48,000  -->  00:00:54,270
Obviously you're not going to have a 500 pound dog but here we have red points indicating horses and

12

00:00:54,270  -->  00:00:56,430
blue points the kitting dogs.

13

00:00:56,430  -->  00:01:01,710
This is our training data but we want to know is if we get an animal where we only know the height and

14

00:01:01,710  -->  00:01:06,540
weight we want to predict if that new data point is a horse or a dog.

15

00:01:06,540  -->  00:01:10,650
Again we're just trying to classify this new data point.

16

00:01:10,650  -->  00:01:17,020
Here we can kind of already see that based on where the green point lies in association fits neighboring

17

00:01:17,040  -->  00:01:17,860
points.

18

00:01:17,880  -->  00:01:20,390
It's kind of easy to predict certain points.

19

00:01:20,460  -->  00:01:27,510
For example this top point in green would probably be a horse since all the points around it are red

20

00:01:27,630  -->  00:01:28,920
horse points.

21

00:01:28,920  -->  00:01:35,490
Likewise the points all the way at the bottom left that's green would likely be a dog since the nearest

22

00:01:35,490  -->  00:01:41,790
points to it are all blue points indicating their dogs want to expand on this idea and formalize it

23

00:01:41,790  -->  00:01:42,990
.

24

00:01:43,560  -->  00:01:46,860
That's where K the neighbor's algorithm comes to play.

25

00:01:46,860  -->  00:01:48,810
The training algorithm is very simple.

26

00:01:48,840  -->  00:01:53,970
You simply store all the data and the prediction algorithm for new test points.

27

00:01:53,970  -->  00:01:55,090
Works like this.

28

00:01:55,200  -->  00:02:02,730
You calculate the distance from X to all the points in your data X indicating that particular new data

29

00:02:02,730  -->  00:02:03,620
point.

30

00:02:03,690  -->  00:02:08,620
Then you sort the points in your data by increasing distance from X.

31

00:02:08,820  -->  00:02:12,830
Then you predict the majority label of the K K being a number.

32

00:02:12,840  -->  00:02:19,800
Closest points choosing a k it will affect what class a new point is assigned to.

33

00:02:19,800  -->  00:02:22,740
Let's go ahead and look at another example here.

34

00:02:22,740  -->  00:02:27,470
I've plotted out our training data which is just yellow points in class A.

35

00:02:27,600  -->  00:02:29,980
And purple points in class B.

36

00:02:30,000  -->  00:02:37,920
This red star indicates a few points I want to predict whether this point belongs to class A or class

37

00:02:37,950  -->  00:02:39,230
B.

38

00:02:39,510  -->  00:02:46,080
If I choose k equals 3 then I look at the three nearest neighbors to this new point.

39

00:02:46,080  -->  00:02:54,270
In this case I have two purple ones and one yellow one are a class a so with equal to three I will predict

40

00:02:54,270  -->  00:02:58,190
that this you redstart point belongs to class B purple.

41

00:02:58,230  -->  00:03:05,820
However if I set equal to 6 I will actually have a majority of yellow or class 8 points meaning if I

42

00:03:05,820  -->  00:03:12,330
have capless 6 I would predict that the new point belongs to a class A choosing a K will affect what

43

00:03:12,330  -->  00:03:14,540
class a new point is assigned to.

44

00:03:14,550  -->  00:03:22,270
So if we look back on our horse vs dog we can here see the plotted effects of various k values by choose

45

00:03:22,270  -->  00:03:25,990
cake was to 1 and then actually pick up a lot of noise.

46

00:03:26,190  -->  00:03:33,540
But as I go larger and larger k values you'll notice I smooth out and create more of a bias in my model

47

00:03:34,230  -->  00:03:40,830
or I get a cleaner cut off at the cost of mislabeling some points.

48

00:03:40,830  -->  00:03:45,120
The prose of the cannoneers neighbors algorithm is that it's very simple.

49

00:03:45,150  -->  00:03:47,810
The training is very trivial you just sort your data.

50

00:03:47,940  -->  00:03:50,130
It works if any number of classes.

51

00:03:50,130  -->  00:03:53,830
It's really easy to add more data and it has very few parameters.

52

00:03:53,850  -->  00:04:00,030
The only parameters being that K which is how many points are you going to look at that are next to

53

00:04:00,030  -->  00:04:04,110
your new test point and then whatever distance metric you're using.

54

00:04:04,320  -->  00:04:08,420
If you want to learn more about distance metrics go ahead and finish reading Chapter 4.

55

00:04:08,610  -->  00:04:12,100
But the distance metric is just how you're actually defining mathematically.

56

00:04:12,210  -->  00:04:16,890
The distance between your new test point and the old training points.

57

00:04:16,890  -->  00:04:22,690
The cons of this algorithm is that there's a high prediction cost and it's worse for large data sets

58

00:04:23,190  -->  00:04:26,160
because you're having to sort this entire large dataset.

59

00:04:26,190  -->  00:04:32,100
There's a high prediction cost and testing out what points are closest to this new data point.

60

00:04:32,100  -->  00:04:37,960
It's also not good with high dimensional data as your features increase and you get higher that mentions

61

00:04:37,970  -->  00:04:38,970
year data.

62

00:04:38,970  -->  00:04:44,870
It's going to throw off your ability to measure distances in various dimensions.

63

00:04:44,910  -->  00:04:47,340
Also categorical features don't work well.

64

00:04:47,350  -->  00:04:49,580
K nearest neighbors.

65

00:04:49,980  -->  00:04:53,760
Let's go ahead and jump to our studio and begin to explore an example.

66

00:04:53,880  -->  00:04:58,660
Then you're going to have a project to test your understanding of the nearest neighbors algorithm thinks

67

00:04:58,740  -->  00:05:00,650
everyone will see at the next lecture.
