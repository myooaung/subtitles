WEBVTT
1

00:00:00.930  -->  00:00:07.260
Hello everyone and welcome to logistically Russian project solutions part 3 lecture video in the first

2

00:00:07.260  -->  00:00:12.030
two sections part 1 and Part Two of the logistic regression project solutions walk through.

3

00:00:12.060  -->  00:00:15.370
We went ahead and mainly cleaned our data and explored our data.

4

00:00:15.450  -->  00:00:20.820
Finally in Perth 3 we're going to be focusing on training and building a logistic regression model.

5

00:00:20.820  -->  00:00:23.610
Let's go ahead and jump to our studio and get started.

6

00:00:23.610  -->  00:00:25.550
OK here we are our studio.

7

00:00:25.590  -->  00:00:28.350
Go ahead and shift this a little over to the right.

8

00:00:28.350  -->  00:00:31.630
It's finally time to start building our logistic regression model.

9

00:00:31.660  -->  00:00:37.470
Remember a logistic regression is a type of classification model and in classification models we attempt

10

00:00:37.470  -->  00:00:41.000
to predict the outcomes of categorical dependent variables.

11

00:00:41.040  -->  00:00:47.080
In this case we are trying to predict whether someone makes more or less than $50000 a year.

12

00:00:47.280  -->  00:00:51.780
That means the logistic regression we're using is based off of the logistic function which always takes

13

00:00:51.780  -->  00:00:55.400
values between 0 and 1.

14

00:00:55.410  -->  00:01:03.010
Let's go ahead and take a quick final look at the head of our adult data frame.

15

00:01:04.590  -->  00:01:07.910
Looks like our income variable looks good.

16

00:01:07.920  -->  00:01:10.340
We have region data hours per week.

17

00:01:10.830  -->  00:01:16.300
And let's go in and check the structure of our data to make sure it's also okay.

18

00:01:17.670  -->  00:01:17.940
All right.

19

00:01:17.940  -->  00:01:21.240
Looks like ours is an integer we have factory levels where we need them.

20

00:01:21.240  -->  00:01:23.690
Everything is looking ready to go.

21

00:01:23.940  -->  00:01:27.280
Let's go ahead and clear the console and begin.

22

00:01:27.580  -->  00:01:30.440
We want to do first is do a train test split.

23

00:01:30.480  -->  00:01:34.150
We want to split the data into a training set and a testing set.

24

00:01:35.130  -->  00:01:41.130
So I'm going to say train test split up more lines.

25

00:01:41.130  -->  00:01:45.360
And the best way to do this is using the CAA tools library.

26

00:01:45.360  -->  00:01:49.650
By the end of this course you're going to be super familiar of using the CA tools library for train

27

00:01:49.650  -->  00:01:50.760
test split.

28

00:01:50.790  -->  00:01:56.660
And this is going to be about the second or third time you've seen this library itself.

29

00:01:56.980  -->  00:02:02.120
So go ahead and set the seeds to 101 if you want your splits to be exactly like mine.

30

00:02:02.130  -->  00:02:07.440
First Step we import that library then we set a random seed to our results to make sure that the same

31

00:02:07.440  -->  00:02:13.770
as mine that you're going to split up the sample going to create a variable called sample say sample

32

00:02:13.770  -->  00:02:17.710
dot splits and you can pass in any column.

33

00:02:17.760  -->  00:02:22.890
I always like to pass in the column we're trying to predict this case it's income and I'm going to go

34

00:02:22.890  -->  00:02:26.130
ahead and say split ratio will say 0.7

35

00:02:29.520  -->  00:02:37.240
and then we're going to have our training data we'll call it train and it will be the subset of adults

36

00:02:37.310  -->  00:02:45.750
where samples equal to true are put in just the capital-T and then we have tests where these subset

37

00:02:45.840  -->  00:02:55.820
of adults sample is equal to false and should be used equals for a quality here so it should be double

38

00:02:55.850  -->  00:02:57.550
equal signs.

39

00:02:57.690  -->  00:03:04.200
All right let's go ahead and run this and make sure it works out ok.

40

00:03:04.260  -->  00:03:05.950
Looks like everything worked out.

41

00:03:05.970  -->  00:03:08.360
The next thing I wanted you to do was just go ahead and call.

42

00:03:08.360  -->  00:03:16.650
Help on G.L. em to explore the documentation of general linearized models documentation reading is always

43

00:03:16.650  -->  00:03:23.100
a great idea especially once your first time working with a particular function are has great documentation

44

00:03:23.190  -->  00:03:25.840
and the fact that it has a help window right here in our studio.

45

00:03:26.040  -->  00:03:28.700
You could always be checking that out and reading it.

46

00:03:28.860  -->  00:03:34.470
Once you've done that let's go ahead and use all the features to train a G.L. and model on the training

47

00:03:34.550  -->  00:03:36.570
data set.

48

00:03:36.570  -->  00:03:38.220
That means we're going to do the following.

49

00:03:38.280  -->  00:03:47.880
We're going to say our model is equal to or assign the G.L. m function person income on to predict income

50

00:03:47.880  -->  00:03:48.150
.

51

00:03:48.210  -->  00:03:50.040
Put the tilt sign and then I put a period.

52

00:03:50.040  -->  00:03:50.500
It's.

53

00:03:50.530  -->  00:03:59.550
All the features to predict that I'm going to go ahead and say family is equal to binomial in this case

54

00:03:59.550  -->  00:04:00.840
I can say whip's

55

00:04:03.810  -->  00:04:06.010
link is equal to logic.

56

00:04:06.270  -->  00:04:08.760
I can also just pass in logic by itself.

57

00:04:08.760  -->  00:04:16.410
I could just say for instance you could also just say that if you want it to it's really up to you.

58

00:04:16.440  -->  00:04:18.130
Whatever is more readable.

59

00:04:18.570  -->  00:04:21.560
And then I'm going to go and say data is equal to train.

60

00:04:21.690  -->  00:04:24.290
Since I'm just using my training set.

61

00:04:24.840  -->  00:04:25.100
All right.

62

00:04:25.110  -->  00:04:27.050
And that's training the model right now.

63

00:04:27.420  -->  00:04:27.850
OK.

64

00:04:27.900  -->  00:04:30.030
You'll probably notice this warning message.

65

00:04:30.030  -->  00:04:35.700
All this warning message just saying is that the model was able to fit probabilities with a class of

66

00:04:35.870  -->  00:04:38.730
0 percent or 100 percent chance of occurring.

67

00:04:38.730  -->  00:04:43.620
Basically that means that for certain data points it predicted that it has a zero percent chance of

68

00:04:43.620  -->  00:04:49.950
being over $50000 a year or a hundred percent chance of being over $50000 a year.

69

00:04:50.040  -->  00:04:57.420
Don't worry too much since we're not dealing with a quote unquote very real large noisy data set this

70

00:04:57.420  -->  00:04:59.430
is more of a toy set.

71

00:04:59.490  -->  00:05:04.440
It is a related set but it's from 1996 and we've done enough to it to clean it up that it's not too

72

00:05:04.440  -->  00:05:07.470
surprising that we get numerically zero or one.

73

00:05:07.500  -->  00:05:12.510
You can go ahead and just ignore that warning for now if you're dealing with a real large noisy data

74

00:05:12.510  -->  00:05:18.750
set and you get this warning message go ahead and just do a quick reality check and make sure that a

75

00:05:18.750  -->  00:05:24.210
lot of your data points are 0 0 percent or 100 percent chance of occurring.

76

00:05:24.270  -->  00:05:27.300
But other than that don't worry too much about this message.

77

00:05:27.300  -->  00:05:29.080
It will probably show up quite often.

78

00:05:29.080  -->  00:05:31.120
Or using logistic regression.

79

00:05:31.200  -->  00:05:38.370
Go ahead and chuck a summary of the model so we can call summary model code expand this window a little

80

00:05:38.370  -->  00:05:43.530
bit and check out the summary itself.

81

00:05:43.560  -->  00:05:51.210
Notice that since we actually didn't split up education into less factor groups we have quite a few

82

00:05:51.210  -->  00:05:54.490
features here under education that are three stars.

83

00:05:54.510  -->  00:05:59.210
What might be interesting is to try to reduce the number of factory levels for education and try to

84

00:05:59.220  -->  00:06:02.340
also reduce the number of factory levels for occupation.

85

00:06:02.340  -->  00:06:08.420
You'll notice here that there's quite a bit of three stars or highly significant features.

86

00:06:08.490  -->  00:06:09.640
So just keep that in mind.

87

00:06:09.660  -->  00:06:17.640
It may be a good idea to go back this data set and make these into lower factor level numbers for features

88

00:06:17.640  -->  00:06:19.570
themselves.

89

00:06:19.620  -->  00:06:21.140
Again we solve a lot of features.

90

00:06:21.140  -->  00:06:22.970
Some are important some not so much.

91

00:06:23.020  -->  00:06:29.070
However our comes of a really awesome function called step and it looks like this going to clear the

92

00:06:29.070  -->  00:06:30.710
console.

93

00:06:31.650  -->  00:06:38.070
Now with this step function does is it really tries to remove predictive variables from the model in

94

00:06:38.070  -->  00:06:41.980
an attempt to delete variables that do not significantly add to the fit.

95

00:06:42.510  -->  00:06:43.850
And it uses an.

96

00:06:43.850  -->  00:06:45.460
I see.

97

00:06:45.480  -->  00:06:48.890
And that stands for the key key information criterion.

98

00:06:48.900  -->  00:06:56.670
It was based off of a Japanese petitions work in the 1970s called hero to a kiki and he basically just

99

00:06:56.670  -->  00:07:03.120
invented this criterion of AIC and this criterion allows us to compare different logistic regression

100

00:07:03.120  -->  00:07:09.630
models and what the step function is going to do is basically try a bunch of different combinations

101

00:07:09.720  -->  00:07:15.690
of the variable features in the logistic regression models and keep the ones that thinks are significant

102

00:07:15.690  -->  00:07:16.560
.

103

00:07:16.560  -->  00:07:21.640
Again if you recall back to that summer we just saw a lot of them or significant.

104

00:07:21.660  -->  00:07:28.200
So it may be the case that even if we try the step function we will still get the exact same features

105

00:07:28.260  -->  00:07:35.490
as important features in order to use step function we can go ahead and do say new step model and just

106

00:07:35.490  -->  00:07:43.530
pass on your old model into the step function and you will slowly start to get AIC values for the various

107

00:07:43.530  -->  00:07:47.580
combinations depending on how fast your computer is.

108

00:07:47.580  -->  00:07:54.540
This may take a very long time and it'll take even longer if you have a ton of features to go through

109

00:07:54.540  -->  00:07:55.010
.

110

00:07:55.020  -->  00:07:59.730
You'll also probably get a bunch of warning messages about those fitted probabilities numerically zero

111

00:07:59.760  -->  00:08:02.320
or one occurring.

112

00:08:02.340  -->  00:08:07.140
Notice how I'm building off the different models right here.

113

00:08:07.170  -->  00:08:09.430
So it's going to continue to build new models.

114

00:08:09.630  -->  00:08:14.370
I'm going to go ahead and essentially fast forward to the end of this.

115

00:08:14.680  -->  00:08:14.930
OK.

116

00:08:14.940  -->  00:08:18.480
I went ahead and jumped to the step function being done.

117

00:08:18.630  -->  00:08:23.910
This is what it looks like when your output of the step function is complete and what you can do to

118

00:08:23.910  -->  00:08:26.910
go ahead and check all this is just going to clear this.

119

00:08:27.050  -->  00:08:32.480
And you can call for a summary off of the new data step model or whatever the side of the coin you just

120

00:08:32.520  -->  00:08:32.930
that model.

121

00:08:32.940  -->  00:08:36.900
And this will show you what features that side keep and which the side of throw away.

122

00:08:36.900  -->  00:08:41.910
You'll notice however in our particular case the step function decided to keep all the features we just

123

00:08:41.910  -->  00:08:42.780
used.

124

00:08:42.780  -->  00:08:48.120
And this is because we're using the AIC criteria to compare the various logistic regression models.

125

00:08:48.150  -->  00:08:52.680
There's a lot of other criteria you could use this has do some math that's outside of the scope of this

126

00:08:52.680  -->  00:08:53.370
course.

127

00:08:53.490  -->  00:08:58.230
You can check out the notes and links for the Wikipedia pages describing things such as the variable

128

00:08:58.230  -->  00:09:05.520
inflation factor of the I-F to explore other methods of comparing models to each other.

129

00:09:05.520  -->  00:09:12.870
Let's go ahead and move on to create a confusion matrix off our predicted values or do this first.

130

00:09:12.980  -->  00:09:14.700
We're going to need some particular values.

131

00:09:14.700  -->  00:09:22.450
Let's go ahead and say test and we'll go ahead and say predicted income.

132

00:09:22.450  -->  00:09:31.860
So I'm making a new column off the test and we're going to say it equals to predicts model my new data

133

00:09:31.860  -->  00:09:33.580
is equal to test.

134

00:09:33.630  -->  00:09:42.480
And then since I'm doing a classification I want the type to be a response and notice that you'll get

135

00:09:42.480  -->  00:09:48.120
a warning message indicating that you have a rank deficient it don't worry about that too much right

136

00:09:48.120  -->  00:09:53.760
now it may occur and that's just a warning that their predictions actually might be a little misleading

137

00:09:53.760  -->  00:09:54.140
.

138

00:09:54.150  -->  00:09:59.490
You can go ahead and check out the explanation in The Notebook and the stock exchange link for a more

139

00:09:59.490  -->  00:10:04.440
thorough explanation of why that may be happening and why it may or may not actually affect your results

140

00:10:04.440  -->  00:10:04.790
.

141

00:10:04.840  -->  00:10:08.400
For now go ahead and just don't worry about that particular warning it's an error.

142

00:10:08.430  -->  00:10:12.590
Just the warning and move on to creating the confusion matrix itself.

143

00:10:12.600  -->  00:10:14.590
This case it's pretty easy.

144

00:10:14.760  -->  00:10:18.200
You just call the column you were trying to predict.

145

00:10:18.270  -->  00:10:27.360
We were trying to predict the income and then you put in test income and in this case whip's test predicted

146

00:10:27.360  -->  00:10:27.780
income.

147

00:10:27.780  -->  00:10:33.440
So the predictive values that are greater than 0.5.

148

00:10:35.760  -->  00:10:38.050
And there is our confusion matrix.

149

00:10:38.100  -->  00:10:42.990
Let's go ahead and get a few measurements off of this confusion matrix such as the accuracy of the model

150

00:10:43.430  -->  00:10:48.900
number the accuracy of say a CCC or a CC.

151

00:10:48.900  -->  00:10:50.940
Sorry one too many CS there.

152

00:10:51.360  -->  00:10:59.190
That's going to go ahead and be equal to six thousand three hundred seventy two and you can review the

153

00:10:59.580  -->  00:11:05.520
logistic regression lecture at the very beginning of this section for an overview of what the confusion

154

00:11:05.520  -->  00:11:08.600
matrix looks like and that it's going to be over everything.

155

00:11:08.610  -->  00:11:15.390
Basically we just want for accuracy measurements the true positives Plus the true negatives divided

156

00:11:15.390  -->  00:11:21.280
by everything else which in this case is going to be all the other data points.

157

00:11:21.340  -->  00:11:23.230
China's going to copy and paste there.

158

00:11:23.610  -->  00:11:25.790
Let's go in and check out the accuracy.

159

00:11:25.800  -->  00:11:30.180
Looks like we have an eighty four point six percent accuracy model.

160

00:11:30.390  -->  00:11:36.390
Let's go ahead and calculate other metrics of performance such as recall going to go ahead and just

161

00:11:36.810  -->  00:11:37.720
enter that.

162

00:11:37.920  -->  00:11:41.620
Looks like we have a 97 percent recall in our precision.

163

00:11:41.680  -->  00:11:45.810
We go ahead and check that is 93 percent.

164

00:11:45.810  -->  00:11:50.900
So in your opinion was this a good model or a bad model was the final question.

165

00:11:51.180  -->  00:11:54.690
And the other question was What other context would you like to know before answering that question

166

00:11:54.690  -->  00:11:55.420
.

167

00:11:55.530  -->  00:12:02.580
The correct answer is that given the information there is no real way to get a correct evaluation of

168

00:12:02.880  -->  00:12:04.250
how good this model is.

169

00:12:04.260  -->  00:12:11.610
What we need to know is the cost associated with accuracy precision and recall since we don't actually

170

00:12:11.610  -->  00:12:18.180
know we are going to be using this model for now we're using the data for as a final product or process

171

00:12:18.180  -->  00:12:18.480
.

172

00:12:18.480  -->  00:12:22.700
We don't actually have a good opinion on whether or not this was a good model.

173

00:12:22.830  -->  00:12:30.150
We'll have to do is figure out what is the cost associated with accuracy versus recall versus precision

174

00:12:30.660  -->  00:12:36.930
because some models you are maybe trying to maximize recall or precision versus other models you're

175

00:12:36.930  -->  00:12:39.430
trying to maximize accuracy.

176

00:12:39.450  -->  00:12:40.040
Okay.

177

00:12:40.380  -->  00:12:41.790
I hope that was helpful for you.

178

00:12:41.790  -->  00:12:47.490
Go ahead and explore the notebook if you want more information on how to create the logistic regression

179

00:12:47.490  -->  00:12:48.020
model.

180

00:12:48.090  -->  00:12:50.670
But overall it's a pretty straightforward process.

181

00:12:50.670  -->  00:12:56.940
You just do the train test split then call it g l n we go ahead and take a look at our history you can

182

00:12:56.940  -->  00:13:02.700
see that but you just called G.L. and on your model and that will be it they'll create a logistic regression

183

00:13:02.700  -->  00:13:03.340
model.

184

00:13:03.780  -->  00:13:08.040
Hopefully you found this exercise useful and I will see you at the next lecture.

185

00:13:08.040  -->  00:13:09.090
Thanks everyone.
