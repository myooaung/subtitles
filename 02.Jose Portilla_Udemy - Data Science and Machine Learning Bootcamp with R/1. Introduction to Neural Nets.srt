1

00:00:00,390  -->  00:00:04,310
Hello everyone and welcome to the introduction to neural networks lecture.

2

00:00:04,410  -->  00:00:09,780
I'm super excited for this lecture because after discussing supervised learning topics and unsupervised

3

00:00:09,780  -->  00:00:15,840
learning topics we're finally at reinforcement learning neural networks is our first stepping stone

4

00:00:15,840  -->  00:00:19,770
to understanding things such as deep learning.

5

00:00:20,090  -->  00:00:24,660
There's no official reading assignment for this lecture but I would still suggest you go ahead and browse

6

00:00:24,660  -->  00:00:29,880
through the Wikipedia articles on neural networks and reinforcement learning before viewing this lecture

7

00:00:29,880  -->  00:00:30,500
.

8

00:00:30,510  -->  00:00:32,600
Let's go ahead and get started.

9

00:00:32,760  -->  00:00:38,130
Your own networks are modeled after biological neural networks and attempt to allow computers to learn

10

00:00:38,160  -->  00:00:41,730
in a similar manner to humans through reinforcement learning.

11

00:00:41,730  -->  00:00:44,060
There's lots of use cases for neural networks.

12

00:00:44,100  -->  00:00:48,460
A few examples would be things like pattern recognition time series predictions.

13

00:00:48,480  -->  00:00:53,970
Signal Processing anomaly detection and later on through the use of neuron that works can even do things

14

00:00:53,970  -->  00:00:57,810
like controlling self-driving vehicles.

15

00:00:58,020  -->  00:01:03,480
The human brain has interconnected neurons with dendrites that received inputs and then based off those

16

00:01:03,510  -->  00:01:09,300
inputs produce an electrical signal output to another neuron and that's the concept we're going to be

17

00:01:09,300  -->  00:01:13,340
trying to recreate through artificial neural networks or a an end.

18

00:01:13,710  -->  00:01:18,350
Let's talk about why we want to even develop neural networks to begin with.

19

00:01:18,600  -->  00:01:22,410
There are problems that are difficult for humans but easy for computers.

20

00:01:22,410  -->  00:01:27,900
Things such as multiplying two large numbers together are easy for computers to solve but difficult

21

00:01:27,900  -->  00:01:29,760
for humans to do in their head.

22

00:01:29,760  -->  00:01:35,340
Then there are problems that are easier for humans but difficult for computers such as recognising pictures

23

00:01:35,340  -->  00:01:42,060
of people or giving an image and describing the image and what's inside that picture.

24

00:01:42,060  -->  00:01:47,610
Neural networks attempt to solve problems that would normally be easy for humans but hard for computers

25

00:01:47,620  -->  00:01:47,900
.

26

00:01:48,150  -->  00:01:51,420
Let's start by looking at the simplest neural network possible.

27

00:01:51,450  -->  00:02:00,470
The Perceptor from a preset truck consists of one or more inputs a processor and a single output.

28

00:02:00,570  -->  00:02:06,690
A perception follows the feed forward model meaning the inputs are sent into the neuron are processed

29

00:02:06,780  -->  00:02:08,700
and then result in an output.

30

00:02:08,700  -->  00:02:11,690
So for this figure below you would read this from left to right.

31

00:02:11,760  -->  00:02:18,150
You have input 0 input 1 going into the preprocessor and then being output it is some sort of result

32

00:02:18,210  -->  00:02:20,740
after being processed.

33

00:02:21,270  -->  00:02:24,510
A perception process follows four main steps.

34

00:02:24,510  -->  00:02:31,410
First you receive the inputs to you put some sort of weights on those inputs three you sum those inputs

35

00:02:31,500  -->  00:02:36,770
and then for you process those inputs and generates some sort of output.

36

00:02:36,780  -->  00:02:39,480
Let's go ahead and walk through the very simple example.

37

00:02:39,690  -->  00:02:48,270
Say we have a perception of two inputs we'll say Input 1 is x 1 equals 12 we'll say Input 1 is x 2 equal

38

00:02:48,270  -->  00:02:56,340
to for each input that is sent into the NIRA must first be weighted which means going to be multiplied

39

00:02:56,340  -->  00:02:57,420
by some value.

40

00:02:57,420  -->  00:03:04,020
Often this value is just between negative 1 and 1 when creating a percent Tron will typically begin

41

00:03:04,020  -->  00:03:06,080
by just assigning random weights.

42

00:03:06,220  -->  00:03:13,380
In this case we'll go ahead and randomly assign weight 0 to 0.5 in weight 1 to negative 1.

43

00:03:13,410  -->  00:03:17,150
That means we take each input and multiply it by it's weights.

44

00:03:17,190  -->  00:03:23,880
That means we'll say input 0 times weight 0 is 12 times zero point by six and input one tempts one is

45

00:03:23,880  -->  00:03:30,930
four times negative one equals negative for the output of a perception is generated by passing that

46

00:03:30,930  -->  00:03:31,650
sum.

47

00:03:31,680  -->  00:03:34,020
There are some sort of activation function.

48

00:03:34,020  -->  00:03:39,270
In the case of a simple binary output that activation function is what tells preset Tron whether to

49

00:03:39,270  -->  00:03:40,530
fire or not.

50

00:03:40,800  -->  00:03:46,290
There are many activation functions to choose from functions and processing such as logistic functions

51

00:03:46,320  -->  00:03:52,370
Trigon to metric functions such as sinusoidal functions or tanh functions step functions etc..

52

00:03:52,530  -->  00:03:54,630
Let's go ahead and make the activation function.

53

00:03:54,660  -->  00:03:56,030
The sign of the sun.

54

00:03:56,040  -->  00:03:58,490
So a very simple activation function.

55

00:03:58,530  -->  00:04:03,250
In other words if the sum is a positive number we'll have the output be 1.

56

00:04:03,360  -->  00:04:07,940
If it's a negative number we'll have the output be negative 1.

57

00:04:07,980  -->  00:04:10,680
One more thing to consider is bias.

58

00:04:10,680  -->  00:04:13,680
Imagine that both inputs were equal to zero.

59

00:04:13,710  -->  00:04:19,320
That means any sum no matter what multiplicative weight you put on it would also be zero that would

60

00:04:19,320  -->  00:04:22,650
cause problems because it would be very common to have a point.

61

00:04:22,680  -->  00:04:30,600
That may just happen to land on x 1 is 0 x 2 0 or X is 0 y is zero etc. in order to solve that problem

62

00:04:31,050  -->  00:04:32,800
we add a bias.

63

00:04:32,820  -->  00:04:36,930
This third input is known as a bias and it starts with a value of 1.

64

00:04:36,930  -->  00:04:44,350
This avoids that zero issue to actually train the percept Tron we use the following steps.

65

00:04:44,400  -->  00:04:48,650
We provide the perception of inputs for which there is a known answer.

66

00:04:48,660  -->  00:04:51,690
We asked the precept trons to guess an answer.

67

00:04:51,690  -->  00:04:57,870
Then we compute the error which is basically asking how far off from the correct answer are we.

68

00:04:57,870  -->  00:05:02,640
We adjust all the weights according to that error and we return to step 1 and just keep repeating this

69

00:05:02,640  -->  00:05:03,950
process.

70

00:05:04,020  -->  00:05:06,340
That means these five steps laid out.

71

00:05:06,340  -->  00:05:11,700
Here is how you actually trained precept Tron and you'll basically layer a bunch of percent trons and

72

00:05:11,700  -->  00:05:13,910
create a neural net.

73

00:05:14,850  -->  00:05:20,540
We repeat that until we reach an error that we're satisfied with and usually set that beforehand.

74

00:05:20,580  -->  00:05:24,440
That is how a single percept trundled work and that or create a neural network.

75

00:05:24,460  -->  00:05:30,810
Again you have to do is link many preset trons together in layers you'll have an input layer and an

76

00:05:30,900  -->  00:05:37,170
output layer any layers in between are known as hidden layers because you don't directly see anything

77

00:05:37,200  -->  00:05:43,180
but the input or output you may have heard of the term deep learning.

78

00:05:43,350  -->  00:05:47,390
That's just a neural network with many hidden layers causing it to be deep.

79

00:05:47,580  -->  00:05:51,140
You may be wondering well how deep is a typical deep learning network.

80

00:05:51,160  -->  00:05:54,350
There's no good answer for how many hidden layers you should use.

81

00:05:54,480  -->  00:06:00,410
But as a quick example Microsoft state of the art vision recognition uses something like 152 layers

82

00:06:00,410  -->  00:06:02,310
.

83

00:06:02,430  -->  00:06:05,800
Let's go ahead and go to our studio and begin to explore for example.

84

00:06:05,880  -->  00:06:09,950
Then finally you'll have a project to test your understanding of creating your own networks.

85

00:06:09,990  -->  00:06:13,260
In our thanks everyone and I'll see you at the next lecture
