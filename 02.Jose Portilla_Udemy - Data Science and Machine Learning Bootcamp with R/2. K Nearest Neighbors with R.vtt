WEBVTT
1

00:00:01.230  -->  00:00:05.470
Hello everyone and welcome to the nearest neighbors with our lecture.

2

00:00:05.550  -->  00:00:10.230
This lecture we're going to learn how to implement the nearest neighbors classification algorithm in

3

00:00:10.240  -->  00:00:12.540
our let's go ahead and get started.

4

00:00:12.540  -->  00:00:12.800
All right.

5

00:00:12.800  -->  00:00:15.480
So I went ahead and jumped to our studio to get started.

6

00:00:15.570  -->  00:00:21.320
The first step we're need to do is get the data and the data were going to be playing around with a

7

00:00:21.330  -->  00:00:25.460
caravan dataset that comes with the SLR package.

8

00:00:25.500  -->  00:00:28.740
Now in order to actually get this data you'll need to install the package.

9

00:00:28.890  -->  00:00:36.310
So go in and say installed up packages in as a character string type in I s l r.

10

00:00:36.990  -->  00:00:40.860
I just put it here as a comment because I actually already have the Eissler package.

11

00:00:41.070  -->  00:00:46.170
Once you've download it you should be able to call all our library.

12

00:00:46.170  -->  00:00:51.570
And once you've called the library you'll have access to the care of an dataset Let's go ahead and check

13

00:00:51.570  -->  00:00:54.650
out the structure of this current data set.

14

00:00:55.110  -->  00:01:03.090
This current data set is about 6000 observations on 6000 customers and they have 86 features or variables

15

00:01:03.090  -->  00:01:04.590
in this data frame.

16

00:01:04.590  -->  00:01:10.380
And if you scroll all the way down which you'll notice is that there's a final purchase variable with

17

00:01:10.380  -->  00:01:11.920
factors of two levels.

18

00:01:11.940  -->  00:01:13.150
Yes or no.

19

00:01:13.440  -->  00:01:20.730
Basically this is just a bunch of customer data measured across eighty five predictors of the demographic

20

00:01:20.730  -->  00:01:25.890
characteristics with the final one being whether or not they purchased insurance from this care event

21

00:01:25.890  -->  00:01:26.680
company.

22

00:01:26.820  -->  00:01:28.520
And that's why it's called the care of data set.

23

00:01:28.530  -->  00:01:35.220
The insurance policy company is called Caravan and this is a bunch of data off customers indicating

24

00:01:35.340  -->  00:01:40.080
whether or not they actually purchased the insurance policy in this dataset.

25

00:01:40.080  -->  00:01:46.020
Only about 6 percent of people purchased the insurance policy and you can check that just by asking

26

00:01:46.020  -->  00:01:48.960
for the summary of the purchase columns.

27

00:01:49.000  -->  00:01:56.310
I can say summary care of an dollar sign and purchase is the target that we're going to try to predict

28

00:01:56.310  -->  00:01:58.120
later on when we split this.

29

00:01:58.170  -->  00:02:06.150
You can see here it's about 5400 nos and only about 350 people actually purchased the insurance.

30

00:02:06.150  -->  00:02:08.050
Let's go ahead and move along.

31

00:02:08.340  -->  00:02:12.810
Let's see if we actually need to clean the data so we're just using this data as a simple example.

32

00:02:12.810  -->  00:02:15.300
We won't worry too much about feature engineering.

33

00:02:15.300  -->  00:02:21.000
Let's just go ahead and remove any A values by dropping the rows with them in order to check if there's

34

00:02:21.000  -->  00:02:22.320
any any values.

35

00:02:22.320  -->  00:02:27.320
I'm just going to say any is that a pass on the entire data frame.

36

00:02:27.750  -->  00:02:30.090
And you'll notice we get false so it looks like we're good.

37

00:02:30.330  -->  00:02:35.820
Let's go ahead and move on to a very important topic when dealing with KNM or cannoneers neighbors and

38

00:02:35.820  -->  00:02:42.360
that's to standardize the variables because the K and classifier predicts the class of a given test

39

00:02:42.360  -->  00:02:46.330
observation by 10 to find the observations that are nearest to it.

40

00:02:46.350  -->  00:02:49.290
The scale of the variables really matters.

41

00:02:49.470  -->  00:02:54.960
Any variables that are on a large scale will have a much larger effect on the distance between observations

42

00:02:55.410  -->  00:02:57.910
versus variables that are on a small scale.

43

00:02:57.930  -->  00:03:04.230
For example let's go ahead and check the variance of two features will check the variance of the first

44

00:03:04.230  -->  00:03:05.280
feature.

45

00:03:05.520  -->  00:03:14.700
So caravan the very first column going to say comma 1 the variance of the first column is 165 and the

46

00:03:14.700  -->  00:03:18.420
variance of the second column

47

00:03:21.210  -->  00:03:26.540
is 0.1 6 so clearly or scales are really different for each of the variables.

48

00:03:26.700  -->  00:03:31.680
Well we're going to do now is standardize all the variables except for the purchase one because the

49

00:03:31.680  -->  00:03:37.650
purchase an actual label that we're trying to predict the purchase variable is column 86 or the last

50

00:03:37.650  -->  00:03:39.200
column of our data set.

51

00:03:39.210  -->  00:03:43.500
So we're going to need to do is save it as a separate variable because the K and then a function will

52

00:03:43.500  -->  00:03:45.870
need it as a separate argument.

53

00:03:45.870  -->  00:03:51.950
Let me go ahead and start by doing this will say purchase and I'm writing this now.

54

00:03:51.960  -->  00:03:59.220
A script is going to be caravan and it's going to be the very last column.

55

00:03:59.550  -->  00:04:02.690
And there's different ways to index the actual purchase column.

56

00:04:02.690  -->  00:04:04.510
You could have also just index that by name.

57

00:04:04.620  -->  00:04:10.320
But here I know it's the last column and then what I'm going to do is standardize the dataset using

58

00:04:10.320  -->  00:04:14.320
the scale function of r and that's actually really easy.

59

00:04:14.340  -->  00:04:16.430
That's something that's really nice about our.

60

00:04:16.740  -->  00:04:26.010
I'm going to call this standardized dot caravan and you can just go ahead and use the scale function

61

00:04:26.100  -->  00:04:31.650
and scale is just a generic function whose default method centers are scales the columns of a numeric

62

00:04:31.710  -->  00:04:32.270
matrix.

63

00:04:32.280  -->  00:04:34.890
And in this case we're going to do it on care of.

64

00:04:35.800  -->  00:04:40.160
And I want to do it on everything but the eighty sixth column.

65

00:04:40.260  -->  00:04:42.870
So I'm going to go ahead and say minus 86 here

66

00:04:46.020  -->  00:04:50.640
and then I'm going to go ahead and print the variants

67

00:04:53.720  -->  00:04:58.080
of the first and second column just to make sure everything worked.

68

00:04:58.080  -->  00:04:59.560
Let me go ahead and copy this line

69

00:05:04.220  -->  00:05:10.210
an input to hear let's go ahead and run this and noticed we get one and one.

70

00:05:10.320  -->  00:05:16.050
So now that we can see that we've actually standardized all the columns which is good because of K nearest

71

00:05:16.060  -->  00:05:16.660
neighbors.

72

00:05:16.830  -->  00:05:22.410
It would have been an issue if we're dealing with stuff on really different scales.

73

00:05:22.410  -->  00:05:29.610
Let's go ahead and continue by doing the train test split on our data.

74

00:05:29.620  -->  00:05:31.560
I'm not going to use a tool here.

75

00:05:31.560  -->  00:05:39.480
I'm just going to do a very simple split and just grab the first 1000 rows as a test set.

76

00:05:39.720  -->  00:05:41.220
Let me go ahead and show you how we can do that.

77

00:05:41.220  -->  00:05:45.150
I'm just going to say test index is 1 through 1000.

78

00:05:45.150  -->  00:05:53.670
So a vector of the integers 1 through 1000 that is going to go out and say test data grab it from the

79

00:05:53.670  -->  00:05:58.500
standardized caravan and pass in my test index.

80

00:05:59.070  -->  00:06:02.190
And this is just another example of how you can do a train test split.

81

00:06:02.190  -->  00:06:05.630
However I would always recommend using the CAA tools random method.

82

00:06:05.820  -->  00:06:10.950
But for right now this is a pretty simple example because just Kinnear's neighbors by default is a pretty

83

00:06:11.550  -->  00:06:13.050
basic algorithm.

84

00:06:13.050  -->  00:06:17.240
We can just go ahead and do this.

85

00:06:20.030  -->  00:06:20.900
OK.

86

00:06:21.000  -->  00:06:25.320
So that's my test data and I'm going to do the same for my training data.

87

00:06:25.400  -->  00:06:26.760
And I say train that data

88

00:06:30.310  -->  00:06:40.380
standardized caravan and say negative test index and that means the Spraberry thing that's not in that

89

00:06:40.380  -->  00:06:41.570
test index.

90

00:06:41.850  -->  00:06:50.440
And then I'm going to say train that purchase is going to be equal to purchase of negative test the

91

00:06:50.440  -->  00:06:51.930
index.

92

00:06:52.710  -->  00:06:53.270
OK.

93

00:06:53.520  -->  00:06:56.860
So we have our training data and our testing data.

94

00:06:56.880  -->  00:07:03.490
Now it's time to use K and N or the K nearest neighbors algorithm and as a reminder we're trying to

95

00:07:03.630  -->  00:07:08.730
come up for model to predict whether someone will purchase the insurance policy you're not going to

96

00:07:08.740  -->  00:07:18.860
go ahead and make a little cut in my script and start the section of billing the canon and model.

97

00:07:19.510  -->  00:07:22.350
Let me go ahead and continue down.

98

00:07:22.360  -->  00:07:29.700
First off we're going to need to do is call the class library and the class library is library that

99

00:07:29.700  -->  00:07:32.140
contains the KNM function.

100

00:07:32.190  -->  00:07:39.210
I'm going to go ahead and set the seed to 101 because there may be some random variants while using

101

00:07:39.210  -->  00:07:44.650
the canon function that way I'll just set the seed to make sure you get the same results I do

102

00:07:48.270  -->  00:07:54.760
and I'm going to create a variable called predicted purchase and then I'll call Canon.

103

00:07:54.780  -->  00:08:02.190
So that's the nearest neighbor classification the nearest neighbor function works a little differently

104

00:08:02.190  -->  00:08:08.030
than what we've seen before which you have to do is you passing in your training data then you pass

105

00:08:08.030  -->  00:08:16.670
in your test data and then you pass in your training labels.

106

00:08:16.680  -->  00:08:24.350
So notice the order here we have our training data or test data and then the labels are the purchase

107

00:08:24.390  -->  00:08:26.190
yes or no for the training data.

108

00:08:26.190  -->  00:08:32.970
Now then what this will attempt to do is predict the purchase for the test data and it's a little different

109

00:08:32.980  -->  00:08:38.480
than what we've seen before as far as putting in a formula and then we'll go ahead and say is equal

110

00:08:38.500  -->  00:08:40.130
to 1 for now.

111

00:08:40.490  -->  00:08:48.630
And let me go ahead and show you what this outputs by printing the head of the predicted purchase.

112

00:08:48.880  -->  00:08:50.780
Let me run this.

113

00:08:51.730  -->  00:08:52.170
Whoops

114

00:08:54.690  -->  00:08:57.160
OK if we take a look we'll get.

115

00:08:57.170  -->  00:08:58.780
No no no no no.

116

00:08:58.800  -->  00:08:59.770
But we have levels.

117

00:08:59.760  -->  00:09:00.900
No and yes.

118

00:09:01.090  -->  00:09:01.970
Looking good.

119

00:09:02.160  -->  00:09:06.230
Let's go ahead and delete these print statements.

120

00:09:06.250  -->  00:09:11.200
We've gotten the data we've done the train test split and we did the KNM model.

121

00:09:11.190  -->  00:09:16.750
Again something to note here how you actually use the k and end function you pass in your training data

122

00:09:16.840  -->  00:09:18.330
without a label.

123

00:09:18.450  -->  00:09:21.430
You pass your test data again without a label.

124

00:09:21.420  -->  00:09:27.430
Then you actually pass in your training data label points and then you pass the cave all you want to

125

00:09:27.420  -->  00:09:32.130
use and you'll get the actual predicted labels on your test data.

126

00:09:32.130  -->  00:09:38.660
Let's go ahead and evaluate the model with our misclassification error rate nor to do that.

127

00:09:38.670  -->  00:09:44.520
I can just say this I can say miss class error

128

00:09:47.210  -->  00:09:58.510
is and that's going to be the average of where my test purchase member that was the actual label points

129

00:09:58.500  -->  00:10:03.610
on whether or not they purchased the insurance policy for my test data whether or not this is not equal

130

00:10:03.610  -->  00:10:07.360
to well say predicted purchase.

131

00:10:07.960  -->  00:10:14.460
And let's go ahead and print the missed class error.

132

00:10:17.190  -->  00:10:24.480
Ok looks like a misclassified about eleven point six percent of my test data points that's not so great

133

00:10:24.480  -->  00:10:29.130
but again it really depends on what your data looks like what your features look like cetera.

134

00:10:29.130  -->  00:10:33.710
Right now the focus of this lecture is just to get you feeling comfortable using the K and and function

135

00:10:33.710  -->  00:10:34.370
.

136

00:10:34.410  -->  00:10:37.500
Let's go ahead and start the next step.

137

00:10:37.560  -->  00:10:44.400
That's always really important when you are using Kinnear's neighbors and that's choosing a k value

138

00:10:44.400  -->  00:10:45.510
.

139

00:10:45.510  -->  00:10:53.400
And we're going to do is actually implements the elbow method we discussed during the lecture and I'll

140

00:10:53.400  -->  00:10:56.520
show you our intuition behind the process we're going to do.

141

00:10:56.520  -->  00:11:00.680
Let's go ahead and just quickly change this to say K equals 3.

142

00:11:01.080  -->  00:11:08.280
And again let's go ahead and print out the missed class error member or previous miss Clouser was about

143

00:11:08.280  -->  00:11:10.490
eleven point six percent.

144

00:11:10.780  -->  00:11:16.710
If I run it now I get 7.4 percent when I say Keiko's with the three That's looking pretty good.

145

00:11:16.800  -->  00:11:20.540
Let's go hands up at teacake was 5.

146

00:11:20.940  -->  00:11:24.070
It looks like it's going further down 6.6 percent.

147

00:11:24.090  -->  00:11:28.330
The question is how do we actually know when to stop as far as increase in this k value.

148

00:11:28.710  -->  00:11:34.250
What I could do is just manually change K and see which k gives us the minimal classification misclassification

149

00:11:34.250  -->  00:11:34.780
rate.

150

00:11:35.010  -->  00:11:39.470
But since we have a computer we're just going to do a for loop for this process.

151

00:11:39.780  -->  00:11:42.550
And let me go ahead and start that off.

152

00:11:42.810  -->  00:11:54.790
I'm going to say predictive purchase is null and then I'm going to go ahead and say error rates is also

153

00:11:54.790  -->  00:11:55.850
a no.

154

00:11:56.640  -->  00:12:02.850
And the reason I'm sending them as no instead of any values is because predictive purchase is eventually

155

00:12:02.850  -->  00:12:10.770
going to be the vector of predicted purchases and the error rate is going to be a vector of all the

156

00:12:10.770  -->  00:12:13.300
error rates.

157

00:12:15.000  -->  00:12:15.870
And I'll show you what I mean.

158

00:12:15.870  -->  00:12:16.770
Just a second.

159

00:12:17.100  -->  00:12:26.250
We'll go ahead and go for I from 1 to 20.

160

00:12:27.360  -->  00:12:35.700
I'm going to set seed 101 just to make sure any random variations are the same for you and me and then

161

00:12:35.700  -->  00:12:37.620
I'm going to say predict that purchase

162

00:12:40.260  -->  00:12:50.160
is canon pass in my train data person my test data and then passing the labels for the training data

163

00:12:50.160  -->  00:12:50.180
.

164

00:12:50.190  -->  00:12:54.830
So train that purchase and then I say k is equal to I.

165

00:12:56.070  -->  00:13:05.850
And then once that's all calculated out going to say the error rates of I is the mean of test up purchase

166

00:13:06.420  -->  00:13:16.680
not equal to predicted that purchase and then we're going to go ahead and do is print out the error

167

00:13:16.680  -->  00:13:18.050
rate.

168

00:13:18.180  -->  00:13:20.840
Let's go ahead and run this.

169

00:13:20.880  -->  00:13:23.070
Note that this may take a while.

170

00:13:23.100  -->  00:13:24.870
So just keep that in mind.

171

00:13:24.960  -->  00:13:29.240
I'm going to go ahead and jump to this completion.

172

00:13:29.280  -->  00:13:29.730
All right.

173

00:13:29.730  -->  00:13:35.790
That took just about 10 more seconds to complete but looks like now we have all the misclassification

174

00:13:35.820  -->  00:13:39.800
or error rates for various values of k from 1 to 20.

175

00:13:39.810  -->  00:13:44.580
Let's go ahead and use G-G plot to actually plot out the elbow here.

176

00:13:45.420  -->  00:13:47.220
So going we start a new section called

177

00:13:49.410  -->  00:13:55.990
visualize K elbow method

178

00:13:58.790  -->  00:14:05.520
and you all these notes are in the notebook as far as all the code and we'll review it right for we

179

00:14:05.520  -->  00:14:06.540
and the lecture.

180

00:14:06.880  -->  00:14:17.130
But I'm going to call library G-G plot 2 set K values equal 1 through 20 and I'm going to create a data

181

00:14:17.130  -->  00:14:23.250
frame of my errors with K as one column and then the error rate that goes if that K is another column

182

00:14:23.250  -->  00:14:24.060
.

183

00:14:24.060  -->  00:14:34.830
So I going to go ahead and say data frame pasan my error rates and then pass in the case value vector

184

00:14:34.830  -->  00:14:36.050
.

185

00:14:36.420  -->  00:14:39.030
And just to show you what this looks like.

186

00:14:39.630  -->  00:14:45.560
Let me go ahead and put all this in the consul and to clear this.

187

00:14:45.810  -->  00:14:46.650
Put it in the console.

188

00:14:46.650  -->  00:14:50.200
That way we don't run that huge for loop again.

189

00:14:50.960  -->  00:14:58.650
And if I print out what the area looks like you'll notice I have this I have the error rates and the

190

00:14:58.650  -->  00:15:01.430
K value associated with it.

191

00:15:01.440  -->  00:15:04.200
Now let's go ahead and just plot this out.

192

00:15:04.530  -->  00:15:06.960
Easy and easy plot.

193

00:15:06.960  -->  00:15:10.090
That means I'm going to say G-G plot.

194

00:15:10.870  -->  00:15:14.410
In my data area that the if the aesthetic I want to do.

195

00:15:14.550  -->  00:15:25.680
I want to plot the K values on the x axis versus the error rates on the y axis and then I want to do

196

00:15:26.520  -->  00:15:34.190
a scatterplot and I'm going to go ahead and add a connecting line with Chiam underscore line.

197

00:15:35.340  -->  00:15:42.480
And I want to go ahead and say the line is a dotted line and it's going to be red.

198

00:15:42.510  -->  00:15:44.320
You don't have to actually add all this in.

199

00:15:44.490  -->  00:15:50.160
This is just optional stuff to make the plot look a little better or a little nicer.

200

00:15:50.640  -->  00:15:53.350
Let's go ahead and make sure this all works.

201

00:15:53.930  -->  00:15:55.880
And let's zoom in on this plot.

202

00:15:56.160  -->  00:15:57.640
And there we have it.

203

00:15:57.720  -->  00:16:05.070
Now you can see that increasing k will drop your error rates or your misclassification rate.

204

00:16:05.340  -->  00:16:14.130
But once you get to about 9 as your K value it doesn't help your misclassification rate to increase

205

00:16:14.130  -->  00:16:14.910
the K value.

206

00:16:14.910  -->  00:16:18.370
In fact you can see it basically flatlines after 9.

207

00:16:18.420  -->  00:16:21.180
And that's where our elbow is.

208

00:16:21.180  -->  00:16:27.600
So if you were doing this in real life after plotting this data out you would choose k equals 9 as your

209

00:16:27.630  -->  00:16:29.460
optimal k value.

210

00:16:29.460  -->  00:16:32.240
Given what you've done for the training set.

211

00:16:32.490  -->  00:16:35.940
All right that's it for the nearest neighbors lecture.

212

00:16:35.940  -->  00:16:41.130
Again that should have been pretty straightforward because conures neighbors is a really simple algorithm

213

00:16:41.150  -->  00:16:41.380
.

214

00:16:41.580  -->  00:16:46.860
Let's go ahead and just take these final few minutes to go over everything we did.

215

00:16:46.860  -->  00:16:49.070
First thing we have to do is get the data.

216

00:16:49.170  -->  00:16:54.850
That means you had to install the SLR package and call the Eissler library.

217

00:16:55.030  -->  00:16:57.850
Then we could check out the care event package.

218

00:16:57.870  -->  00:17:04.050
We didn't do any exploratory data analysis or visualization off the caravan package itself because we're

219

00:17:04.050  -->  00:17:09.180
just focusing on learning how to use the Kinnear's neighbor implementation and our we went ahead and

220

00:17:09.180  -->  00:17:16.770
got the data we got the purchase Colomb out of the data and then we standardized or scaled the rest

221

00:17:16.770  -->  00:17:21.230
of the data as it was normalized which is really important when you're working Kinnear's neighbors.

222

00:17:21.570  -->  00:17:23.810
Then we did a very simple train test split.

223

00:17:23.820  -->  00:17:26.910
We didn't do a random s.a.a tools train to split.

224

00:17:26.910  -->  00:17:34.260
We just went ahead and grabbed the first 1000 data points as our test Index said that toward test data

225

00:17:34.380  -->  00:17:37.350
and then we grabbed the test purchase this test labels.

226

00:17:37.350  -->  00:17:40.440
We did the same thing for the training data.

227

00:17:40.440  -->  00:17:44.940
Then to use the k and model we did the following is probably the most important part of this lecture

228

00:17:45.420  -->  00:17:53.560
you call library class then you call that an end function and you pass in your trained data.

229

00:17:54.120  -->  00:18:01.820
Your test data and then you're trained up purchase and then you pass in your K value.

230

00:18:01.890  -->  00:18:07.620
In this case you just Pessin like I mentioned the training data and the test data and then you pass in

231

00:18:07.650  -->  00:18:13.320
the labels for your training data in order to get out the labels for your test data.

232

00:18:13.320  -->  00:18:19.230
And this makes sense because if you remember back from the K N and lecture the training of the data

233

00:18:19.350  -->  00:18:23.540
as or the training of the cane and model is just giving it and sorting that data.

234

00:18:23.580  -->  00:18:28.530
That's why it doesn't have the same sort of formula that the other models we've been working with have

235

00:18:28.530  -->  00:18:29.550
.

236

00:18:29.580  -->  00:18:32.220
Then we went ahead and grabbed the misclassification error.

237

00:18:32.310  -->  00:18:35.930
You could also use table to start making it confusion matrix off of this thing.

238

00:18:36.270  -->  00:18:40.480
And then we went ahead and used a for loop to try to choose the best value.

239

00:18:40.560  -->  00:18:43.280
And then we visualized it using the elbow method.

240

00:18:43.440  -->  00:18:48.750
That's really all there is to it as far as using the K and an algorithm in our.

241

00:18:48.750  -->  00:18:53.670
This is a really simple algorithm but it's a pretty powerful tool given how simple and intuitive it

242

00:18:53.670  -->  00:18:57.930
is to actually perform classifications on a data set.

243

00:18:57.930  -->  00:18:59.600
All right I hope those are useful.

244

00:18:59.610  -->  00:19:04.380
Remember to reference notes in case anything was confusing and I will see you at the next lecture.

245

00:19:04.380  -->  00:19:05.760
Thanks everyone.
