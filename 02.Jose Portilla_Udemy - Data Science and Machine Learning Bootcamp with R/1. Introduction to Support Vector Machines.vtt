WEBVTT
1

00:00:00.690  -->  00:00:05.750
Hello everyone and welcome to the introduction to support vector machines lecture this lecture will

2

00:00:05.760  -->  00:00:10.140
discuss the formal definition of support vector machines and then try to get an understanding of the

3

00:00:10.140  -->  00:00:12.300
intuition behind support vector machines.

4

00:00:12.330  -->  00:00:19.170
Or as the EMS if you want the mathematics behind this algorithm go ahead and read Chapter 9 of an introduction

5

00:00:19.170  -->  00:00:26.460
to stickle learning support vector machines or SVM as are also known or supervised learning models of

6

00:00:26.460  -->  00:00:30.460
associated learning algorithms that analyze data and recognize patterns.

7

00:00:30.520  -->  00:00:33.380
Their use for classification and regression analysis.

8

00:00:33.600  -->  00:00:40.350
In this lecture we'll be talking about their use for classification given a set of training examples

9

00:00:40.470  -->  00:00:43.370
each marked for belonging to one of two categories.

10

00:00:43.470  -->  00:00:49.340
So binary classification and SBM training algorithm builds a model that assigns new examples.

11

00:00:49.340  -->  00:00:53.010
Are these test data points into one category or the other.

12

00:00:53.010  -->  00:01:00.690
Making it a non probabilistic binary linear classifier in SVM model is a representation of the examples

13

00:01:00.720  -->  00:01:07.020
as points in space maps so that the examples of the separate categories are divided by a clear gap that

14

00:01:07.020  -->  00:01:12.990
is as wide as possible and that's going to begin to set the intuition for an SVM that we'll see later

15

00:01:12.990  -->  00:01:15.390
on through some plots and diagrams.

16

00:01:15.390  -->  00:01:21.180
New examples are then mapped into that same space now predicted to belong to the category based on which

17

00:01:21.180  -->  00:01:24.030
side of that gap they fall on.

18

00:01:24.030  -->  00:01:29.040
All right let's go ahead and try to understand the basic intuition through looking at some diagrams

19

00:01:29.130  -->  00:01:31.160
and some plotted out data.

20

00:01:31.190  -->  00:01:34.400
Imagine we have the training data below.

21

00:01:34.450  -->  00:01:36.480
Here we have two classes.

22

00:01:36.480  -->  00:01:39.160
We have a blue class and a pink class.

23

00:01:39.270  -->  00:01:44.070
Now what we're going to try to do is a binary classification for new points.

24

00:01:44.070  -->  00:01:51.480
We want to put a new point on this plot or the horizontal axis is some feature one and the y vertical

25

00:01:51.480  -->  00:01:53.470
axis is a feature too.

26

00:01:53.820  -->  00:01:59.030
When we put a new point we wanted the term and does it belong to the blue class or the pink class.

27

00:01:59.190  -->  00:02:03.780
Intuitively what we could do is draw a separating hyperplane.

28

00:02:03.810  -->  00:02:08.720
In the case of two that mentions is just a line between the classes.

29

00:02:08.720  -->  00:02:15.390
However we have lots of options of hyperplane that separate these two classes perfectly.

30

00:02:15.390  -->  00:02:22.290
Notice that the pink black and green line would all separate the blue and pink training points perfectly

31

00:02:22.290  -->  00:02:23.430
.

32

00:02:23.430  -->  00:02:29.570
The question that arises how do we actually choose the line that separates these classes the best.

33

00:02:29.910  -->  00:02:35.740
What we would like to do is choose a hyperplane that maximizes the margin between the classes.

34

00:02:36.120  -->  00:02:37.350
So you'll see a diagram.

35

00:02:37.350  -->  00:02:42.840
Typically it looks like this where you have a separating hyperplane here denoted as the dotted line

36

00:02:43.080  -->  00:02:49.830
and then a margin that extends out from that hyperplane the vector points at the margin line touch are

37

00:02:49.830  -->  00:02:55.080
known as support vectors and that's where the name support vector machines come from.

38

00:02:55.080  -->  00:02:59.280
Here we can see the incircle points that are the support doctors.

39

00:02:59.280  -->  00:03:04.470
Those are the training points that actually touch those margined lines.

40

00:03:04.980  -->  00:03:11.310
We can expand this idea to nonlinearly separable data through the use of a kernel trick.

41

00:03:11.310  -->  00:03:18.000
That means if you take a look at the left hand plot in two dimensions here you may have a X label on

42

00:03:18.000  -->  00:03:24.930
y label and you'll notice that this data is not linearly separable because it's a loose circle in the

43

00:03:24.930  -->  00:03:30.930
middle of blue triangles and red outer circle of red circles and there's no way we can draw a straight

44

00:03:30.930  -->  00:03:33.210
line to separate these classes.

45

00:03:33.210  -->  00:03:38.970
However through the kernel trick what we do is we end up viewing this in a higher dimension.

46

00:03:39.000  -->  00:03:43.540
In this case we look at a third Z label over on the right.

47

00:03:43.680  -->  00:03:49.020
Now it can see that this is separable in the third dimension through another hyperplane.

48

00:03:49.020  -->  00:03:54.000
You can check out YouTube for a really nice 3D visualization videos explaining this idea and if you

49

00:03:54.000  -->  00:03:58.890
want the mathematics behind the kernel trick again go ahead and check out Chapter 9 of an introduction

50

00:03:58.980  -->  00:04:01.250
to school learning.

51

00:04:02.490  -->  00:04:07.320
Let's go ahead now and jump to our studio and begin to explore an example and then you'll have a project

52

00:04:07.320  -->  00:04:10.770
to test your understanding of using support vector machines.

53

00:04:10.770  -->  00:04:13.270
Thanks everyone and I'll see at the next lecture.
