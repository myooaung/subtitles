WEBVTT
1

00:00:01.140  -->  00:00:06.720
Hello everyone and welcome to linear regression with our part 3 in this final lecture of the linear

2

00:00:06.720  -->  00:00:12.000
regression series we're going to be looking at some advanced visualizations capabilities by just plotting

3

00:00:12.000  -->  00:00:16.010
your model and creating predictions off our test set.

4

00:00:16.260  -->  00:00:20.190
Let's go ahead and jump to our studio and get started.

5

00:00:20.190  -->  00:00:26.250
OK so here in our studio again we've already split the date into training and test set we ran our model

6

00:00:26.340  -->  00:00:31.550
and we interpreted the model and we went ahead and plotted the residuals in the last lecture.

7

00:00:31.680  -->  00:00:37.020
Something I want to show you is if you go ahead and run this model scripts so that you have your model

8

00:00:37.350  -->  00:00:47.100
train advanced visualization should perform with your model such as Q-Q plots or plotting your residuals

9

00:00:47.100  -->  00:00:49.320
versus the actual values.

10

00:00:49.320  -->  00:00:54.110
You can just say plot model enter and you'll get four plots.

11

00:00:54.120  -->  00:00:54.930
You hit return.

12

00:00:54.930  -->  00:00:56.630
See the next plot.

13

00:00:56.620  -->  00:00:57.920
Can Know how to do that now.

14

00:00:57.960  -->  00:01:02.990
And there's various plots you can see there's residuals Russa's fitted values.

15

00:01:03.260  -->  00:01:11.340
There's the normal Q-Q plot the scale location plot and the residuals versus Leveridge plot.

16

00:01:11.340  -->  00:01:16.650
These are all more advanced plots for you to check out if you're already very familiar to stick's behind

17

00:01:16.830  -->  00:01:18.220
linear regression.

18

00:01:18.240  -->  00:01:22.950
If you're not and you want to figure out how to actually interpret these plots go ahead and check out

19

00:01:22.950  -->  00:01:27.220
the link in the note book which is the Wikipedia page on regression validation.

20

00:01:27.240  -->  00:01:31.960
Also make sure you do the reading and I SLR because a lot of this is covered there as well.

21

00:01:32.250  -->  00:01:36.880
But again this is mostly just residual plots that you can get directly off the model.

22

00:01:37.140  -->  00:01:42.900
The way to do that one more time is just by saying plot model let's go ahead and move along by actually

23

00:01:42.900  -->  00:01:51.210
getting predictions off our model then go ahead and clear the consul here in order to actually see how

24

00:01:51.210  -->  00:01:57.990
well our models performing what we want to do is not just a summary of model but use Riddick to grab

25

00:01:57.990  -->  00:01:59.950
predictions off our testing set.

26

00:02:00.180  -->  00:02:04.340
And this is basically the whole reason we were using the test train split.

27

00:02:04.450  -->  00:02:11.400
I don't want to train my model on the training data and then test the model for new predictions using

28

00:02:11.400  -->  00:02:12.760
the test data set.

29

00:02:12.870  -->  00:02:19.570
It doesn't really make sense to predict the new data points off the training set because the models

30

00:02:19.590  -->  00:02:22.840
actually already been trained and has seen all those training points.

31

00:02:23.130  -->  00:02:31.330
I want to try to predict new values based off the test set something the models never seen before.

32

00:02:31.440  -->  00:02:34.440
Let's go ahead and get started with predictions.

33

00:02:34.440  -->  00:02:36.050
I'm going to make a variable.

34

00:02:36.090  -->  00:02:37.910
Let me move this down a bit.

35

00:02:38.010  -->  00:02:45.810
Move this over to the right and then make a variable called G-3 dot predictions.

36

00:02:45.810  -->  00:02:50.730
And I want to use the Predict function from stats in order to do this to use to predict function.

37

00:02:50.730  -->  00:02:57.090
All you have to do is pasan the model he created as a first arguments and then the second argument is

38

00:02:57.090  -->  00:02:58.610
you're testing it.

39

00:02:58.860  -->  00:03:02.830
You may be wondering will how does predict actually know what value I want to predict.

40

00:03:02.970  -->  00:03:05.880
In this case that information is already in your model.

41

00:03:06.000  -->  00:03:11.340
Your model already knows the formula and it knows you're trying to predict G-3 based off all the features

42

00:03:11.640  -->  00:03:13.370
of your training data set.

43

00:03:13.440  -->  00:03:17.700
That means when you are using the Predict function need to pass in the model that's already been trained

44

00:03:18.180  -->  00:03:23.240
and your test set should basically look at exactly the same as your training set.

45

00:03:23.370  -->  00:03:28.950
As far as formatting it should have the same columns and it should have the same factored features of

46

00:03:28.950  -->  00:03:32.800
the columns etc. not the actual same data just the same format.

47

00:03:33.630  -->  00:03:35.600
That's how you can grab the predictions.

48

00:03:35.610  -->  00:03:41.190
Now what I want to do is grab the root mean squared error and I also want to be able to easily work

49

00:03:41.190  -->  00:03:42.010
with this.

50

00:03:42.150  -->  00:03:47.070
That means what I'm going to do is actually transform this into a data frame.

51

00:03:47.180  -->  00:03:49.400
Let me go ahead and do that.

52

00:03:49.470  -->  00:03:50.780
We have our predictions.

53

00:03:50.790  -->  00:04:01.950
When I'm going to do is say results and I'm going to do a it and see binds with G-3 predictions and

54

00:04:01.950  -->  00:04:11.730
the actual real values member and trying to predict the G-3 values that I'm going to name those columns

55

00:04:11.730  -->  00:04:16.850
in my results and I'll pass in a vector of strings.

56

00:04:17.040  -->  00:04:29.820
Call it predicted and actual and then I'm going to set that as a data frame as that data frame results

57

00:04:29.820  -->  00:04:36.420
so I transform that matrix into a data frame and then go ahead and print the head of results make sure

58

00:04:36.420  -->  00:04:39.100
that all works out OK.

59

00:04:39.210  -->  00:04:41.740
That means if I check this out I move this up a bit.

60

00:04:41.850  -->  00:04:46.550
What I've done is it created the data frame for each of the test data points.

61

00:04:46.560  -->  00:04:52.680
Note that these numbers are inAmerica order because this was a random grab of the data frame that original

62

00:04:52.680  -->  00:04:53.180
data.

63

00:04:53.280  -->  00:04:54.340
That's why these aren't.

64

00:04:54.340  -->  00:04:55.590
One two three four.

65

00:04:55.620  -->  00:04:57.180
This is actually data point number four.

66

00:04:57.180  -->  00:04:58.610
Happened to go to test data.

67

00:04:58.640  -->  00:05:01.190
They do point number five up the go to testate etc..

68

00:05:01.530  -->  00:05:07.770
These are the predicted values and these are the actual values of their final period three G-3 score

69

00:05:07.770  -->  00:05:08.470
.

70

00:05:08.520  -->  00:05:09.050
OK.

71

00:05:09.800  -->  00:05:15.440
Something I want to note here is that when we plotted out the residual values we noticed that we had

72

00:05:15.440  -->  00:05:18.650
a large amount of larger negative values.

73

00:05:18.710  -->  00:05:25.320
And the reason for that was because our model was predicting negative final score test values or negative

74

00:05:25.310  -->  00:05:27.160
final period values.

75

00:05:27.170  -->  00:05:30.010
Now the lowest value you could get is a zero.

76

00:05:30.050  -->  00:05:34.250
That means we're going to do is take care of those negative predictions.

77

00:05:34.790  -->  00:05:45.950
I'm going to go ahead and say to zero is a function and we'll go to label this as take care of negative

78

00:05:45.950  -->  00:05:49.380
values.

79

00:05:51.560  -->  00:06:01.360
And my function is going to take in some X and it's going to say if X is less than zero return zero

80

00:06:01.520  -->  00:06:06.530
then it's eventually going to apply this to my data frame.

81

00:06:06.530  -->  00:06:10.510
Else if it's not negative I'm just going to return the actual value.

82

00:06:10.550  -->  00:06:15.410
This case X. And that's my function there to zero.

83

00:06:15.800  -->  00:06:23.760
And I'm going to go ahead and apply zero function.

84

00:06:23.810  -->  00:06:28.910
There's lots of ways you could have done this but here's a very readable way to do it by just creating

85

00:06:28.910  -->  00:06:29.870
this function.

86

00:06:29.990  -->  00:06:36.260
Again this function is just to take care of negative prediction values and I'll show you what I mean

87

00:06:36.250  -->  00:06:40.500
by that if we just say men of results.

88

00:06:40.580  -->  00:06:46.940
Notice I have negative 2.7 which doesn't make sense that minimum should be zero.

89

00:06:47.000  -->  00:06:56.140
So I'm going to go ahead and say results predicted is equal to as supply results predicted.

90

00:06:56.500  -->  00:06:59.360
And I'm going to apply that to zero function.

91

00:06:59.360  -->  00:07:03.840
The reason again I'm only doing it on the predicted is because the actual scores only go to zero.

92

00:07:04.000  -->  00:07:08.690
I want to make sure any negative predictions get pushed up to zero.

93

00:07:08.690  -->  00:07:14.070
That way we can improve our accuracy without actually detailing of our model.

94

00:07:14.780  -->  00:07:15.490
OK.

95

00:07:15.710  -->  00:07:21.730
There's lots of ways to evaluate the prediction values in one example for this is that mean square error

96

00:07:22.410  -->  00:07:28.010
in the way we actually do this song O.S. mean squared error.

97

00:07:28.040  -->  00:07:30.680
And this is kind of a measure of how off you are.

98

00:07:30.680  -->  00:07:47.150
I can say MSCE the mean and my error is going to be my actual results minus my predicted results

99

00:07:49.860  -->  00:07:51.580
squared.

100

00:07:52.630  -->  00:07:58.880
And then take the mean that that's the mean square to Arendt's and then prints the MSEE out.

101

00:07:58.880  -->  00:08:06.200
And if you want it can also sometimes do the root mean square error so we can do something like Prince

102

00:08:07.250  -->  00:08:12.260
Amasia the power of negative or power of 0.5 which is the same as taking a square root of something

103

00:08:13.090  -->  00:08:14.780
and go ahead and run this.

104

00:08:14.780  -->  00:08:18.920
So control shift as to run that.

105

00:08:20.140  -->  00:08:27.140
Let me go ahead and print a little notes here so say Prince.

106

00:08:27.350  -->  00:08:35.350
Well first off we won't print this results and instead will just say here right before we print the

107

00:08:35.360  -->  00:08:37.020
actual mean square error.

108

00:08:37.120  -->  00:08:41.400
You going to put a little label here prince or Misi

109

00:08:44.190  -->  00:08:50.620
Krantz square roots of MSCE.

110

00:08:50.960  -->  00:08:53.220
Now let's go ahead and run that's a little cleaner.

111

00:08:53.450  -->  00:08:56.340
And here we have three point ninety nine and one point ninety nine.

112

00:08:56.360  -->  00:09:03.750
That kind of gives you an idea of your means greater of how off you are from these actual scores.

113

00:09:04.040  -->  00:09:09.950
Something that may be a little easier to interpret is just the R-squared of value for the predictions

114

00:09:09.960  -->  00:09:15.700
so how well does your model fit the predicted values.

115

00:09:15.710  -->  00:09:18.360
Let's go ahead and run that.

116

00:09:19.010  -->  00:09:26.370
Son going to go ahead and say the sum of these squared errors.

117

00:09:26.450  -->  00:09:39.880
So again that's going to be a results predicted minus results actual square that's.

118

00:09:40.120  -->  00:09:43.480
Now that we have S S E which is the sum of squared error.

119

00:09:43.490  -->  00:09:51.730
What I want is the s s t value which is the sum of squared total and essentially the formula for R-Squared

120

00:09:51.830  -->  00:09:55.460
is 1 minus SHC divided by SS t..

121

00:09:55.490  -->  00:10:00.460
And again you could check out these formulas both an SLR and in the notes.

122

00:10:00.470  -->  00:10:08.580
So here at ss t you are going to go ahead and do say some the mean of DSF G-3

123

00:10:10.930  -->  00:10:15.340
minus results of actual.

124

00:10:15.350  -->  00:10:20.740
And we're going to square that and then to order in order to get the actual R squared value we'll call

125

00:10:20.750  -->  00:10:27.320
it R2 she's going to be one of minus SS C divided by SS t..

126

00:10:27.620  -->  00:10:32.770
And then we'll go ahead and prints are squared.

127

00:10:34.570  -->  00:10:38.890
And let's go ahead and just prints that are square to value are two.

128

00:10:39.160  -->  00:10:40.810
Let's run that.

129

00:10:41.360  -->  00:10:43.650
All right we check out our squared value.

130

00:10:43.630  -->  00:10:45.730
It looks like it's 0.8.

131

00:10:45.800  -->  00:10:51.140
That's not so bad for the testate it means we're explaining about 80 percent variance on the test data

132

00:10:51.150  -->  00:10:51.670
.

133

00:10:52.150  -->  00:10:57.730
Hopefully by now you should feel comfortable with the our syntax for actually creating a model of linear

134

00:10:57.740  -->  00:10:58.970
regression.

135

00:10:58.970  -->  00:11:03.680
If some of the plots or math did not make sense to you don't worry too much about it.

136

00:11:03.770  -->  00:11:08.870
If you're interested in that go ahead and review Eisele are the introduction to school learning and

137

00:11:08.870  -->  00:11:14.750
the relevant Wikipedia pages that are linked in the notebook and throughout the lectures that we've

138

00:11:14.750  -->  00:11:15.890
been covering.

139

00:11:15.880  -->  00:11:21.740
There is no real substitute for just taking the time to read about the material and understand the mathematics

140

00:11:21.740  -->  00:11:25.800
behind it in order to fully understand how to interpret the results.

141

00:11:25.970  -->  00:11:31.910
If you want you can treat these functions as just black boxes in order to do some quick and dirty analysis

142

00:11:31.900  -->  00:11:32.890
on your data.

143

00:11:32.990  -->  00:11:38.540
But for true data science and understanding the statistics behind it what it really takes is just taking

144

00:11:38.540  -->  00:11:43.070
the time to read through the books read through the material and read through the Wikipedia articles

145

00:11:43.080  -->  00:11:43.290
.

146

00:11:43.460  -->  00:11:44.140
OK.

147

00:11:44.450  -->  00:11:45.470
I hope you enjoy that.

148

00:11:45.470  -->  00:11:46.690
Hope it was useful.

149

00:11:46.700  -->  00:11:50.970
Up next is an exercise to test your knowledge on everything we just covered.

150

00:11:51.290  -->  00:11:53.200
Thanks everyone and I'll see you at the next lecture
