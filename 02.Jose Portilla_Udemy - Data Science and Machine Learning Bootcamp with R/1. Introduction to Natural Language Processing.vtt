WEBVTT
1

00:00:01.530  -->  00:00:06.630
Hello everyone and welcome to the introduction to natural language processing lecture.

2

00:00:07.140  -->  00:00:11.520
While there's no official reading assignment if you would like I would suggest reading the Wikipedia

3

00:00:11.520  -->  00:00:15.820
article on natural language processing to get some context for this lecture.

4

00:00:15.850  -->  00:00:17.990
However you don't have to read the whole article.

5

00:00:17.990  -->  00:00:21.110
Just go ahead and browse through it.

6

00:00:21.120  -->  00:00:25.160
Let's talk about natural language processing and why you may want to use it.

7

00:00:25.230  -->  00:00:32.130
Natural Language Processing or an LP serves a lot of use cases when you're dealing with text or unstructured

8

00:00:32.130  -->  00:00:33.720
text data.

9

00:00:33.720  -->  00:00:39.780
Imagine you work for Google News and you want to group news articles by topic or maybe you work for

10

00:00:39.780  -->  00:00:44.700
a legal firm and you need to sift through thousands of pages of legal documents to find the relevant

11

00:00:44.700  -->  00:00:45.720
ones.

12

00:00:45.720  -->  00:00:49.730
This is where natural language processing can help.

13

00:00:49.770  -->  00:00:55.920
We will want to compile the documents in some fashion get features from them so we'll have to featuritis

14

00:00:55.920  -->  00:01:00.110
those documents and then compare their features.

15

00:01:00.120  -->  00:01:02.200
Let's go through a simple example.

16

00:01:02.220  -->  00:01:04.790
Imagine you have two very simple documents.

17

00:01:04.830  -->  00:01:10.810
In this case the document is just blue house and then the second document is Red House.

18

00:01:10.890  -->  00:01:14.680
That means it's just a document of basically a single sentence.

19

00:01:14.730  -->  00:01:20.610
So the first sentence is Blue House document one second sentence red houses document to a simple way

20

00:01:20.610  -->  00:01:25.140
to featuritis text documents to feature is based off a word count.

21

00:01:25.140  -->  00:01:30.350
So we transform blue house into a vectorized word count.

22

00:01:30.390  -->  00:01:36.120
We basically create a vector count of all the possible words throw all the documents in this case they're

23

00:01:36.120  -->  00:01:43.100
red blue and a house and then we just count how many times those words occur in each document.

24

00:01:43.140  -->  00:01:49.920
That means in this case for document one blue house we get 0 1 1 since Red occurs 0 times blue occurs

25

00:01:49.920  -->  00:01:52.470
once house occurs once.

26

00:01:52.610  -->  00:02:02.490
Similarly her red house we get 1 0 1 because red occurs once blue 0 times in house one time a document

27

00:02:02.490  -->  00:02:06.810
represented as a vector of word counts is called a bag of words.

28

00:02:06.930  -->  00:02:12.740
And hopefully you saw that bag of words in the Wikipedia article if you decided to read up on it.

29

00:02:12.930  -->  00:02:18.990
Once we have these bags of words vectors you can use co-signed similarity on the vectors to determine

30

00:02:18.990  -->  00:02:22.070
similarity of the documents themselves.

31

00:02:22.080  -->  00:02:27.900
This is really useful because we're basically treating each document as a vector of features meaning

32

00:02:27.930  -->  00:02:33.120
we can perform mathematical operations such as the coastline similarity taking their dot products and

33

00:02:33.120  -->  00:02:38.730
then dividing it by the multiplication of their magnitudes or other similarity metrics in order to figure

34

00:02:38.730  -->  00:02:43.690
out how similar two text documents are to each other.

35

00:02:43.980  -->  00:02:49.440
We can improve on backwords by adjusting word counts based on their frequency in the corpus and the

36

00:02:49.440  -->  00:02:56.220
corpus just stands for the group of all the documents bickies something called T.F. IDF or at term frequency

37

00:02:56.300  -->  00:02:57.880
inversed document frequency.

38

00:02:57.960  -->  00:03:02.580
To do this let's go over some definitions.

39

00:03:02.640  -->  00:03:06.450
Term frequency is the importance of the term within that document.

40

00:03:06.670  -->  00:03:13.350
So will denote T.F. term frequency as a function taking in D and T were the number of occurrences of

41

00:03:13.350  -->  00:03:16.030
term t in document D.

42

00:03:16.350  -->  00:03:21.510
Then we have the inverse document frequency which is the importance of the term in the corpus itself

43

00:03:21.510  -->  00:03:21.540
.

44

00:03:21.540  -->  00:03:30.810
So all the documents will call the IDF of T equals log capital D divided by lower case t where capital

45

00:03:30.810  -->  00:03:39.030
D is the total number of documents and lowercase t is equal to a number of documents with the term mathematically

46

00:03:39.030  -->  00:03:39.060
.

47

00:03:39.060  -->  00:03:44.730
Then we can express T.F. IDF as the following equation.

48

00:03:44.730  -->  00:03:53.790
Here we're saying term X within document Y so we can say T F X common Y is the frequency of x and y

49

00:03:54.180  -->  00:03:59.880
and then we multiply that by the log of the total number of documents divided by the number of documents

50

00:03:59.880  -->  00:04:01.830
containing x.

51

00:04:01.890  -->  00:04:08.880
The reason we do this is so that we can get not just a word count but also some sort of notation on

52

00:04:08.880  -->  00:04:15.060
how important a word is not just relevant to the document but to the entire corpus of all the documents

53

00:04:15.070  -->  00:04:15.330
.

54

00:04:15.570  -->  00:04:20.370
Let's go ahead and jump to our studio and begin to actually explore these concepts through code and

55

00:04:20.370  -->  00:04:22.140
through that project where we data mine.

56

00:04:22.170  -->  00:04:24.960
Twitter thanks everyone and I'll see you at the next lecture
