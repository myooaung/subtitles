WEBVTT
1

00:00:00.540  -->  00:00:04.980
Hello everyone and welcome to k means clustering with our video lecture.

2

00:00:04.980  -->  00:00:10.530
This video lecture learn how to implement Kamins clustering algorithm with R and we'll be working with

3

00:00:10.530  -->  00:00:16.170
the iris data set that we've seen before where you had four features of flowers they're peddling pedal

4

00:00:16.170  -->  00:00:20.310
widths etc. and we're trying to predict one out of three species.

5

00:00:20.550  -->  00:00:24.540
In this case in this case means is an unsupervised learning algorithm.

6

00:00:24.540  -->  00:00:28.900
Let's go over a few caveats as to using the iris data set.

7

00:00:28.920  -->  00:00:34.140
Keep in mind that although we already have the labels for the iris data set we won't actually let the

8

00:00:34.140  -->  00:00:36.720
Kamins algorithm see those labels.

9

00:00:36.720  -->  00:00:42.180
The Kamins algorithm doesn't take in labels it just takes in the features and tries to create clusters

10

00:00:42.450  -->  00:00:43.700
based off the cave Valley.

11

00:00:43.710  -->  00:00:50.220
You say for Kamins algorithm we will only use these labels later to evaluate how well the algorithm

12

00:00:50.280  -->  00:00:52.830
actually clustered the correct values.

13

00:00:52.890  -->  00:00:57.300
You should note that usually you don't have the luxury of label data when you're using an unsupervised

14

00:00:57.300  -->  00:01:03.060
learning algorithm such as k means clustering but since we're trying to learn about Kamins clustering

15

00:01:03.300  -->  00:01:09.030
we'll go ahead and keep the labels out so we can use them to check how well we did later on.

16

00:01:09.030  -->  00:01:12.680
All right let's go ahead and jump to our studio to get started.

17

00:01:12.690  -->  00:01:14.660
All right here we're our studio.

18

00:01:14.730  -->  00:01:19.050
I'm going to go ahead and call the ISO our library

19

00:01:22.590  -->  00:01:28.290
and then just print out the head of the iris data set so we can remind ourselves what it looks like

20

00:01:28.290  -->  00:01:30.150
.

21

00:01:30.210  -->  00:01:35.350
Notice we get see blank Sipple with pedaling pedal with and then the species label.

22

00:01:35.460  -->  00:01:40.740
And as I just mentioned one reason that Kamins algorithms it's unsupervised we're only going to allow

23

00:01:40.740  -->  00:01:42.570
it to see the features.

24

00:01:42.660  -->  00:01:47.930
Usually you won't have an actual label or species column but in this case we can use this to our advantage

25

00:01:47.940  -->  00:01:52.630
just to see how well the Kamins clustering algorithm actually perform.

26

00:01:52.680  -->  00:01:54.610
Let's go ahead and just plot out this data.

27

00:01:54.660  -->  00:02:00.540
A quick scatterplot to get an idea of what we might expect when trying to cluster these species of flowers

28

00:02:00.540  -->  00:02:02.610
.

29

00:02:02.670  -->  00:02:07.860
I'm going to call Egidia plot two and say G-G plot.

30

00:02:07.860  -->  00:02:08.610
Iris

31

00:02:12.370  -->  00:02:17.660
pasand pedal length fonix paddle with on y.

32

00:02:17.880  -->  00:02:22.910
And the note that the color is going to be equal to species going to go ahead and set.

33

00:02:22.930  -->  00:02:24.660
That's a P.L..

34

00:02:25.110  -->  00:02:30.410
And then and going to Prince P.L. Plus GM underscore points.

35

00:02:30.570  -->  00:02:36.180
And I'm going to go ahead to make the size of those points a little bigger for let's go ahead and run

36

00:02:36.180  -->  00:02:39.290
this and get the plot out.

37

00:02:39.290  -->  00:02:42.680
All right here's a plot and a click zoom so we can see it.

38

00:02:43.500  -->  00:02:50.250
OK just looking at this plot quickly we can see that the Tosa species itself is much more separated

39

00:02:50.250  -->  00:02:55.410
than the versicolor and virginica species based off of pedal length and pedal with.

40

00:02:55.410  -->  00:03:00.870
That means we're actually creating the Kamins clustering algorithm implementation Kamins clustering

41

00:03:00.870  -->  00:03:02.950
is probably going to have an easier time clustering.

42

00:03:02.980  -->  00:03:09.770
Tosa altogether versus virginica and versicolor this noise or this crossover over here between versicolor

43

00:03:09.820  -->  00:03:15.660
virginica are green and blue is probably going to lead to some clustering mistakes when we actually

44

00:03:15.660  -->  00:03:16.940
check out the labels.

45

00:03:16.950  -->  00:03:22.980
But again I should note we usually don't have the luxury of knowing those labels before hand for unsupervised

46

00:03:22.980  -->  00:03:24.850
learning algorithms.

47

00:03:25.650  -->  00:03:27.330
OK we explore that data a little bit.

48

00:03:27.330  -->  00:03:33.110
Let's get on to the meat of the lecture which is actually implementing Kamins clustering and to do this

49

00:03:33.110  -->  00:03:34.190
in the con..

50

00:03:34.530  -->  00:03:40.410
First thing I want to know is since K means clustering randomly starts the centroid and randomly assigns

51

00:03:40.410  -->  00:03:42.680
points in order to get the same clusters.

52

00:03:42.690  -->  00:03:46.660
I do go ahead and set your random seed to 101.

53

00:03:46.660  -->  00:03:53.310
That means just put in set that seed 1 0 1 the K means function is already built into the stats library

54

00:03:53.310  -->  00:03:53.920
of R.

55

00:03:53.970  -->  00:04:00.870
So you don't need to download or install any other package on them to make an object called Iris dot

56

00:04:01.050  -->  00:04:01.970
.

57

00:04:03.170  -->  00:04:08.020
Well to say Iris cluster.

58

00:04:08.300  -->  00:04:13.330
You can call this whatever you want and this will actually be the case means implementation.

59

00:04:13.510  -->  00:04:16.110
We're going to go ahead and pass pass in our data.

60

00:04:16.110  -->  00:04:18.870
Remember since this is an unsupervised learning algorithm.

61

00:04:18.900  -->  00:04:21.150
Your data would usually not have a label.

62

00:04:21.300  -->  00:04:24.410
That means I want to go ahead and just pasand the first four columns.

63

00:04:24.420  -->  00:04:31.150
Second to say comma one colon for and then the next argument you put in is centers and centers.

64

00:04:31.150  -->  00:04:32.510
It's just your K value.

65

00:04:32.700  -->  00:04:36.420
How many clusters or cluster centers do you actually expect.

66

00:04:36.450  -->  00:04:37.760
We're fortunate that we know it.

67

00:04:37.760  -->  00:04:39.430
We expect three species.

68

00:04:39.540  -->  00:04:44.610
However a lot of times you're not going to know how many labels or clusters you would expect.

69

00:04:44.610  -->  00:04:48.330
That means you're gonna have to use your domain knowledge or business experience to choose a reasonable

70

00:04:48.330  -->  00:04:48.860
value.

71

00:04:48.900  -->  00:04:55.900
OK we're going to go ahead and put in 3 there and then something else we can put in is and start and

72

00:04:55.960  -->  00:04:56.320
start.

73

00:04:56.340  -->  00:05:03.010
It's just the random or the number of random starts we can do which is go ahead and put in 20.

74

00:05:03.020  -->  00:05:04.140
All right we have clusters.

75

00:05:04.140  -->  00:05:11.040
We can go ahead and prints out the iris cluster object and we'll get a bunch of information off of it

76

00:05:11.060  -->  00:05:11.780
.

77

00:05:11.790  -->  00:05:15.360
Note that we get the cluster means for each of the features.

78

00:05:15.360  -->  00:05:16.910
We have three clusters here.

79

00:05:16.980  -->  00:05:19.240
Now also tells you how many points are in each cluster.

80

00:05:19.320  -->  00:05:21.290
62 50 38.

81

00:05:21.630  -->  00:05:24.700
We have the cluster means we are the clustering vector.

82

00:05:24.750  -->  00:05:30.540
We also get back the cluster sum of squares by cluster and we get the available components for the actual

83

00:05:30.540  -->  00:05:32.950
object.

84

00:05:33.210  -->  00:05:38.550
If we want to evaluate how well are clustering algorithm actually performed we can check it against

85

00:05:38.550  -->  00:05:43.290
the actual labels that we know to be true.

86

00:05:43.290  -->  00:05:52.830
And that means you can call something like table Irus cluster call the cluster column and then check

87

00:05:52.830  -->  00:06:01.840
it against Iris species column the label and notice you get these results.

88

00:06:01.890  -->  00:06:07.530
It looks like we mislabeled a few of them but also noticed just like we thought the Tosa were all labeled

89

00:06:07.530  -->  00:06:09.060
into a single class.

90

00:06:09.060  -->  00:06:11.120
Now this is cluster number two.

91

00:06:11.130  -->  00:06:16.860
Again you should note here that the clusters here are n named but we can tell since we already know

92

00:06:16.860  -->  00:06:17.770
the labels.

93

00:06:17.880  -->  00:06:24.060
That it looks like cluster 3 is what k means thought were most of the virginica flowers closer to the

94

00:06:24.140  -->  00:06:30.870
Kamins algorithm thought this a total flowers were and Heyman's group cluster one was the versicolor

95

00:06:30.870  -->  00:06:35.160
flowers although there was some noise between virginica and versicolor.

96

00:06:35.160  -->  00:06:40.950
However it was able to cluster a Tosa all correctly and I know I've said this a couple of times already

97

00:06:41.070  -->  00:06:46.770
but usually you won't have the luxury of knowing these labels and you're not quilt to call a table like

98

00:06:46.770  -->  00:06:50.430
this when you're using unsupervised learning algorithms.

99

00:06:50.430  -->  00:06:54.110
Let's go ahead and briefly briefly discuss cluster visualization.

100

00:06:54.510  -->  00:07:01.110
You can plot out these clusters on just two of the features and I will show you how to do that.

101

00:07:01.110  -->  00:07:02.610
Let me go to and clear this.

102

00:07:02.610  -->  00:07:12.210
You can call the cluster library and there's a function called clus plots and clus plot just draws a

103

00:07:12.210  -->  00:07:17.490
two dimensional clustering plot and it just has a generic partitioning method.

104

00:07:17.500  -->  00:07:19.450
Let me go ahead and show what that looks like.

105

00:07:19.740  -->  00:07:27.810
You're going to call clus plot which is from the cluster of library you can pass in the data.

106

00:07:27.810  -->  00:07:32.460
In this case in a pass in are labeled data so we can actually see the true points and then which can

107

00:07:32.460  -->  00:07:42.960
also do is passen our iris cluster cluster argument and then add some additional arguments here we can

108

00:07:42.960  -->  00:07:45.120
go ahead and put color equal.

109

00:07:45.120  -->  00:07:55.590
True we can add some shade equals true and we'll go ahead and say labels equals zero and lines equals

110

00:07:55.590  -->  00:07:56.490
zero.

111

00:07:56.960  -->  00:08:01.710
And that's just some formatting stuff for the actual plot function.

112

00:08:02.010  -->  00:08:06.200
And then when you call clus plot you should get a plot that looks something like this.

113

00:08:06.720  -->  00:08:11.940
And basically what this is doing is its plotting the cluster's against the two components that explain

114

00:08:11.940  -->  00:08:13.740
the most variability.

115

00:08:13.740  -->  00:08:18.960
And that means this component one component to these two components explain about ninety five point

116

00:08:18.950  -->  00:08:21.900
zero two points per percent of the variable.

117

00:08:22.230  -->  00:08:25.080
And clearly here we can see the Tosa cluster.

118

00:08:25.170  -->  00:08:30.540
It's all by itself it is clearly just state one cluster and then we see some crossing of the clusters

119

00:08:30.540  -->  00:08:36.810
between the virginica and versicolor species of flowers and you can go ahead and explore more about

120

00:08:36.810  -->  00:08:38.730
this clus plot library if you want to.

121

00:08:38.750  -->  00:08:44.640
By saying Help clus plot and you can check out the documentation.

122

00:08:44.640  -->  00:08:47.940
This is basically just a buy very plot device.

123

00:08:47.940  -->  00:08:49.650
You could also have done with G.G. plot.

124

00:08:49.860  -->  00:08:55.590
I should note though it's not going to be super useful for you if you have a data set with a ton of

125

00:08:55.590  -->  00:08:58.990
features because you can only really plot two features at a time.

126

00:08:59.090  -->  00:09:00.690
On to them missional plot.

127

00:09:00.710  -->  00:09:06.540
That means if you have something like 50 features you may only be explaining 20 percent of the variability

128

00:09:06.960  -->  00:09:08.850
with two features but close.

129

00:09:08.850  -->  00:09:13.960
Plot attempts to do is plot against the two features that explain the most variability.

130

00:09:14.430  -->  00:09:15.120
All right.

131

00:09:15.120  -->  00:09:16.280
Hope those useful to you.

132

00:09:16.350  -->  00:09:21.990
But remember the main idea is just how to use the means function and if you ever need more help on that

133

00:09:21.990  -->  00:09:24.850
you can just say help Kamins.

134

00:09:25.320  -->  00:09:30.630
All right thanks everyone and I will see you at the next lecture where we will be discussing your project

135

00:09:30.690  -->  00:09:33.060
for the Kamins clustering section of this course.
