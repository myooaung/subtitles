WEBVTT
1

00:00:00.240  -->  00:00:05.310
Hello everyone and welcome to the introduction to logistic regression lecture and this lecture we'll

2

00:00:05.310  -->  00:00:11.010
discuss classification problems how to use logistic regression to solve them and how to interpret the

3

00:00:11.010  -->  00:00:16.050
results of the logistic regression to wreak confusion matrix.

4

00:00:16.050  -->  00:00:19.630
If you want to understand the full mathematics behind logistic regression.

5

00:00:19.890  -->  00:00:26.570
Go ahead and read sections 4 through 4.3 of introduction to Stichel learning curve James.

6

00:00:27.180  -->  00:00:33.420
Moving along we want to learn about logistic regression as a method for classification and machine learning

7

00:00:33.420  -->  00:00:34.230
and statistics.

8

00:00:34.230  -->  00:00:41.370
Classification is the problem of identifying to which of a set of categories a new observation belongs

9

00:00:41.370  -->  00:00:44.270
to based off of training data.

10

00:00:44.280  -->  00:00:51.390
Some examples of classification problems include trying to detect spam vs ham emails trying to detect

11

00:00:51.420  -->  00:00:56.530
whether someone's going to default on their loan or not or even trying to diagnose a disease.

12

00:00:56.910  -->  00:01:01.050
For example trying to tell if someone has cancer or not.

13

00:01:01.050  -->  00:01:09.000
These above examples are all examples a binary classification meaning we have two classes so far in

14

00:01:09.000  -->  00:01:09.590
our study.

15

00:01:09.600  -->  00:01:14.640
We've only seen regression problems where we try to predict a continuous value such as the price of

16

00:01:14.640  -->  00:01:18.090
a house although the name may be confusing at first.

17

00:01:18.090  -->  00:01:23.790
Logistic Regression allows us to solve classification problems where we're trying to predict discrete

18

00:01:23.850  -->  00:01:25.050
categories.

19

00:01:25.050  -->  00:01:31.310
The convention for binary classification is to have two classes zero and 1.

20

00:01:32.730  -->  00:01:37.640
However we can't use a normal linear regression model on a binary group.

21

00:01:37.650  -->  00:01:40.340
It won't lead to a good fit.

22

00:01:40.530  -->  00:01:46.290
For example if we take a look at this plot below we have a Y axis which represents the probability of

23

00:01:46.290  -->  00:01:48.480
belonging to a particular group.

24

00:01:48.480  -->  00:01:54.000
Let's go ahead and imagine that this example plot is trying to predict likelihood of paying back a loan

25

00:01:54.000  -->  00:01:54.630
.

26

00:01:54.630  -->  00:02:00.420
We'll go ahead and label 0 percent probability as defaulting on their loan meaning they have a zero

27

00:02:00.420  -->  00:02:06.540
percent probability of being able to pay back their loan and at the top we have one or a 100 percent

28

00:02:06.540  -->  00:02:09.270
probability as fully paying back their loan.

29

00:02:09.630  -->  00:02:14.370
We'll go ahead and mark the X axis as some sort of paycheck value.

30

00:02:14.370  -->  00:02:20.490
That means if we go ahead and look at this data as your paycheck goes lower you have a closer to zero

31

00:02:20.490  -->  00:02:25.680
percent probability that you're going to be able to pay back your loan as your paycheck value gets higher

32

00:02:25.680  -->  00:02:25.910
.

33

00:02:25.920  -->  00:02:29.750
You didn't have close to 100 percent probability of paying back your loan.

34

00:02:30.000  -->  00:02:35.670
The reason these yellow dashes are all either on 0 percent or 100 percent is because this is training

35

00:02:35.670  -->  00:02:37.230
data.

36

00:02:37.230  -->  00:02:42.540
Now if this was trading data and we try to use a linear regression model on it we would get a very bad

37

00:02:42.540  -->  00:02:43.100
fit.

38

00:02:43.110  -->  00:02:48.060
We would actually end up predicting probabilities below zero percent which doesn't really make any sense

39

00:02:48.060  -->  00:02:49.800
.

40

00:02:49.800  -->  00:02:55.590
Instead we can transform our linear regression to a logistic regression curve and you'll notice our

41

00:02:55.590  -->  00:03:02.130
logistic regression curve can only go between 0 and 1 and that's going to be the key to understanding

42

00:03:02.400  -->  00:03:03.260
classification.

43

00:03:03.270  -->  00:03:06.970
Using a logistic regression curve the sigmoid.

44

00:03:06.990  -->  00:03:12.720
Also known as logistic function is going to be the key to understanding using logistic regression to

45

00:03:12.750  -->  00:03:15.040
perform a classification.

46

00:03:15.180  -->  00:03:21.210
The key secret to this function is that it can take in any value and its output is going to be between

47

00:03:21.330  -->  00:03:23.450
0 and 1.

48

00:03:23.530  -->  00:03:26.520
We take a look at the equation here on this plot.

49

00:03:26.520  -->  00:03:33.200
We have the sigmoid function plotted out on the z axis is going to be the bottom line.

50

00:03:33.210  -->  00:03:34.500
Usually the x x.

51

00:03:34.560  -->  00:03:41.910
They're here without noting it as theta of Z and the formula is theta of Z so the function of z is equal

52

00:03:41.910  -->  00:03:46.510
to 1 over 1 plus E to the power negative Z.

53

00:03:46.860  -->  00:03:53.730
The key thing to notice here is that it doesn't matter what value of Z you put in to the logistic function

54

00:03:53.730  -->  00:03:55.310
or the sigmoid function.

55

00:03:55.350  -->  00:03:58.640
You'll always get a value between 0 and 1.

56

00:03:58.920  -->  00:04:04.170
So again if you take a look at this plot it doesn't matter that whatever value you put in for Z the

57

00:04:04.260  -->  00:04:10.860
output along the vertical axis is always going to be between 0 and 1 and that's the key to the sigmoid

58

00:04:10.860  -->  00:04:12.900
function.

59

00:04:12.900  -->  00:04:19.170
This means that we can take her linear regression solution and place it into this sigmoid function and

60

00:04:19.170  -->  00:04:20.760
that's going to look like this.

61

00:04:20.760  -->  00:04:26.100
Remember our linear model followed a basic y equals x plus B principle.

62

00:04:26.190  -->  00:04:32.640
Here we have a linear model as y equals beta plus Beta 1 times X..

63

00:04:32.700  -->  00:04:39.750
If we take that linear model and put it into the sigmoid function we finally are able to transform this

64

00:04:39.810  -->  00:04:46.200
linear regression to a logistic model meaning it doesn't matter what whatever the value of the linear

65

00:04:46.200  -->  00:04:48.230
model output actually is.

66

00:04:48.330  -->  00:04:55.200
It's always going to be between 0 and 1 when we place it into the logistic model or the sigmoid function

67

00:04:55.200  -->  00:04:55.770
.

68

00:04:55.950  -->  00:05:01.890
Again if you want more of you on this mathematics make sure to read sections 4 through 4.3 of introduction

69

00:05:02.100  -->  00:05:04.120
to Stichel learning.

70

00:05:04.510  -->  00:05:10.160
But the basic premise of all of this is that this results in a probability from zero to one of belonging

71

00:05:10.190  -->  00:05:16.670
in the one class again doesn't matter what we put in on the horizontal axis on the vertical axis will

72

00:05:16.730  -->  00:05:20.740
always get some sort of probability between 0 and 1.

73

00:05:20.870  -->  00:05:29.930
That means we can set a cutoff point usually at 0.5 and we'll say if anything below results in 0.5 or

74

00:05:29.930  -->  00:05:33.410
below 0.5 that will go to class 0.

75

00:05:33.440  -->  00:05:36.460
Anything above belongs the class 1.

76

00:05:36.470  -->  00:05:42.640
So we're going to transform that 0.5 probability as a cutoff point.

77

00:05:42.640  -->  00:05:47.290
Let's go ahead and do a quick recap overview of what we just discussed.

78

00:05:47.320  -->  00:05:52.790
We can use the logistic function to output a value ranging from 0 to 1.

79

00:05:52.790  -->  00:05:59.440
Again it doesn't matter what we put along the horizontal axis we get a value from 0 to 1 based off of

80

00:05:59.440  -->  00:06:07.370
this probability we will assign a class by putting a cutoff point 0.5 which makes sense because we want

81

00:06:07.370  -->  00:06:14.150
to say if the probability is 50 percent or less of belonging to class 1 then we should classify this

82

00:06:14.140  -->  00:06:17.590
as Class 0 in our binary classification.

83

00:06:17.960  -->  00:06:24.170
If we have a probability of 0.5 or above of belonging to a class 1 we'll go ahead and assign this new

84

00:06:24.170  -->  00:06:27.200
point to class 1.

85

00:06:27.190  -->  00:06:27.920
All right.

86

00:06:27.920  -->  00:06:34.670
So let's go ahead and talk about modeling valuation and using a confusion matrix after you train a logistic

87

00:06:34.670  -->  00:06:37.620
regression model to classify some training data.

88

00:06:37.810  -->  00:06:41.870
You're going to want to evaluate your model's performance on some test data.

89

00:06:42.110  -->  00:06:49.810
You can use a confusion matrix to evaluate classification models a confusion matrix is a table that

90

00:06:49.810  -->  00:06:55.070
is often used to describe the performance of the classification model on a set of test data for which

91

00:06:55.060  -->  00:06:57.640
the true values are already known.

92

00:06:57.640  -->  00:07:03.410
The confusion matrix itself is actually relatively simple to understand but sometimes the related terminology

93

00:07:03.410  -->  00:07:04.630
can be confusing.

94

00:07:04.940  -->  00:07:07.290
Let's go ahead and walk through this example.

95

00:07:07.550  -->  00:07:11.220
In this case we have a binary classification problem.

96

00:07:11.330  -->  00:07:17.360
So in this example we're testing for the presence of a disease where no is a negative test which is

97

00:07:17.360  -->  00:07:19.190
false equals zero.

98

00:07:19.460  -->  00:07:24.470
Yes is a positive test where true is equal to one.

99

00:07:24.560  -->  00:07:26.190
What can we learn from this matrix.

100

00:07:26.240  -->  00:07:28.540
Well there are two possible predictive classes.

101

00:07:28.610  -->  00:07:29.540
Yes and no.

102

00:07:29.810  -->  00:07:34.540
If we were predicting the presence of a disease for example yes it would mean that they have disease

103

00:07:34.520  -->  00:07:34.700
.

104

00:07:34.730  -->  00:07:36.800
No it would mean that they don't have the.

105

00:07:37.190  -->  00:07:43.880
The classifier made a total of a hundred and sixty five predictions meaning 165 patients were tested

106

00:07:43.880  -->  00:07:45.850
for the presence of the disease.

107

00:07:45.860  -->  00:07:53.380
Out of those 165 cases the classifier predicted a yes 110 times and no 55 times.

108

00:07:53.540  -->  00:08:00.620
In reality meaning we already have label test data 105 patients in the sample have the disease and 60

109

00:08:00.620  -->  00:08:02.200
patients do not.

110

00:08:02.210  -->  00:08:08.450
Now let's go ahead and define the most basic terms the basic terms are the whole number it terms so

111

00:08:08.450  -->  00:08:15.490
not rates just hold numbers and those terms are true positives true negatives false positives and false

112

00:08:15.520  -->  00:08:16.260
negatives.

113

00:08:16.390  -->  00:08:18.500
You may already be familiar with this terminology.

114

00:08:18.620  -->  00:08:25.520
If you've ever had a deal of studies related to vaccine checks or disease checks a true positive.

115

00:08:25.620  -->  00:08:29.270
Are the cases in which we predicted yes that they have disease.

116

00:08:29.500  -->  00:08:32.080
And in reality they do have the disease.

117

00:08:32.210  -->  00:08:34.590
That's T.P. true positive.

118

00:08:34.630  -->  00:08:40.870
In this case you can find that here at the bottom quadrant where T-P is equal to 100 true negatives

119

00:08:41.240  -->  00:08:42.290
are where we predicted.

120

00:08:42.300  -->  00:08:48.220
Know that they don't have disease and in reality again they don't actually have.

121

00:08:48.470  -->  00:08:52.320
That's on the upper left hand corner T.N. and that's equal to 50.

122

00:08:52.420  -->  00:08:58.670
So those are true positives and true negatives that don't have false positives and false negatives false

123

00:08:58.660  -->  00:09:00.280
positives are where we predicted.

124

00:09:00.290  -->  00:09:05.770
Yes that they have Zeese but in reality they don't actually have disease.

125

00:09:05.770  -->  00:09:12.590
This is also known as a type 1 error that only have false negatives where we predicted.

126

00:09:12.590  -->  00:09:19.460
No they do not have the disease but in reality they actually did have a disease that's not as a type

127

00:09:19.880  -->  00:09:21.990
2 error.

128

00:09:23.540  -->  00:09:26.050
Then we can talk about rates.

129

00:09:26.060  -->  00:09:29.410
The first rate we can discuss is accuracy.

130

00:09:29.570  -->  00:09:33.770
What this is actually getting at is overall how often is it correct.

131

00:09:33.760  -->  00:09:39.200
A lot of times when you hear reports on studies they'll just tell you the accuracy and the accuracy

132

00:09:39.190  -->  00:09:44.830
is calculated by the number of true positives plus the number of true negatives over the total.

133

00:09:44.870  -->  00:09:50.230
In this case our model is 91 percent accurate.

134

00:09:50.240  -->  00:09:54.130
Then we have the misclassification rate which is answering the question.

135

00:09:54.190  -->  00:09:57.740
Overall how often is the model wrong.

136

00:09:57.760  -->  00:10:02.980
This is going to be calculated by the number of false positives plus a number of false negatives divided

137

00:10:02.990  -->  00:10:04.330
by that total.

138

00:10:04.390  -->  00:10:07.930
So that's 15 divided by a sixty five.

139

00:10:07.970  -->  00:10:12.380
Overall this is 9 percent error rate or misclassification rate.

140

00:10:12.380  -->  00:10:18.340
Now let me discuss a nice quick way to remember false positives versus false negatives or your type

141

00:10:18.350  -->  00:10:21.230
1 error versus your type 2 error.

142

00:10:21.470  -->  00:10:25.000
You remember are false positives that I'm pointing out here of the laser.

143

00:10:25.010  -->  00:10:27.670
If P is equal to 10 that's where we predicted.

144

00:10:27.670  -->  00:10:30.260
Yes but they didn't actually have the disease.

145

00:10:30.290  -->  00:10:32.130
That's Type 1 error.

146

00:10:32.650  -->  00:10:34.620
False negatives is what we predicted.

147

00:10:34.630  -->  00:10:39.370
No but they actually do has a disease known as type 2 error.

148

00:10:39.670  -->  00:10:42.210
The go ahead and move along.

149

00:10:42.400  -->  00:10:49.010
We can think of type 1 error and type 2 errors as this funny little diagram as a nice way to remember

150

00:10:49.000  -->  00:10:49.690
it.

151

00:10:49.690  -->  00:10:53.630
So Type 1 error is where you're telling a man they're pregnant again.

152

00:10:53.620  -->  00:10:54.340
You're predicting.

153

00:10:54.350  -->  00:11:00.340
Yes they're pregnant but they actually don't have the pregnancy type 2 error is a false negative.

154

00:11:00.380  -->  00:11:03.710
Here you're saying someone that's obviously pregnant that they're not pregnant.

155

00:11:03.700  -->  00:11:08.870
So you're predicting No but they actually are pregnant and that's a difference at Type 1 err a type

156

00:11:08.870  -->  00:11:09.570
2 error.

157

00:11:09.760  -->  00:11:16.040
False positives vs. false negatives and statistics are commonly referred to as type 1 or Type 2 instead

158

00:11:16.040  -->  00:11:17.060
of their actual names.

159

00:11:17.060  -->  00:11:18.900
False positive or false negative.

160

00:11:19.310  -->  00:11:25.280
Hopefully this is a nice helpful and funny reminder on how to actually memorize these terms.

161

00:11:25.310  -->  00:11:30.950
Let's go ahead and jump to our studio and begin to explore an actual example of using a general logistic

162

00:11:30.940  -->  00:11:32.030
regression model.

163

00:11:32.120  -->  00:11:34.610
And then you'll have a project to test your understanding.

164

00:11:34.930  -->  00:11:37.030
Thanks everyone and I'll see at the next lecture
