WEBVTT
1

00:00:00.750  -->  00:00:05.880
Hello everyone and welcome to the lecture guide to using tidy ear in this lecture we're going to discuss

2

00:00:05.880  -->  00:00:11.730
how to clean and tidy up our data using the tidy package and how to install the tidy package as well

3

00:00:11.730  -->  00:00:13.860
as what data tables are.

4

00:00:13.880  -->  00:00:16.350
Let's good and jump to our studio to get started.

5

00:00:16.830  -->  00:00:19.010
OK so here we are at our studio.

6

00:00:19.140  -->  00:00:24.570
Let's go ahead and begin by installing Thaiday ear and the data that table packages in order to do this

7

00:00:24.570  -->  00:00:32.130
we just say installed packages and pass in quotes T I D Y R for tinier.

8

00:00:32.850  -->  00:00:38.750
Wait till that's finished and tidy or is going to help us clean data.

9

00:00:38.790  -->  00:00:41.860
So what do we actually mean by a clean or tidy data set.

10

00:00:42.090  -->  00:00:48.320
Well a tidy data set is where every rose an observation every column is a feature or variable.

11

00:00:48.330  -->  00:00:53.450
This way the data is organized in such a way where every cell is a value for a specific variable or

12

00:00:53.460  -->  00:00:55.940
feature of a specific observation.

13

00:00:56.220  -->  00:01:01.350
This is usually how we've been working with data with the built in data frames such as the empty car's

14

00:01:01.350  -->  00:01:07.480
data frame where every row was a specific observation or vehicle model in every column.

15

00:01:07.470  -->  00:01:13.850
What specific feature or variable of cars such as MPG etc..

16

00:01:14.210  -->  00:01:14.920
Okay.

17

00:01:15.300  -->  00:01:22.230
The other package you want to install is called data table and it's a bit of a complimentary package

18

00:01:22.230  -->  00:01:24.520
to the plier anti-teacher.

19

00:01:25.070  -->  00:01:29.760
So you may be wondering what's different Suena data that table in a data that frame command.

20

00:01:29.760  -->  00:01:35.320
Well loosely speaking you can think of a data table as a data frame with just a few extra features.

21

00:01:35.610  -->  00:01:38.660
Data tables just the package that extends data frames.

22

00:01:38.760  -->  00:01:43.180
And two of its most notable features are usually speed and cleaner syntax.

23

00:01:43.420  -->  00:01:48.510
Hillary should note that the syntax for a data table can be really really similar to the syntax for

24

00:01:48.510  -->  00:01:49.440
a data frame.

25

00:01:49.620  -->  00:01:54.510
So you may be reading some code that already has data tables and set of data frames inside of it and

26

00:01:54.510  -->  00:01:58.170
you may not even notice the difference.

27

00:01:58.170  -->  00:02:04.560
The main difference in a data table in a data frame is usually the speed of the operation data that

28

00:02:04.560  -->  00:02:11.140
table won't do internal copying needlessly and as you usually a lot faster for some basic operations

29

00:02:11.150  -->  00:02:12.980
so just keep that in mind.

30

00:02:13.080  -->  00:02:17.740
We'll revisit this package in a lot more detail later on the course for now.

31

00:02:17.910  -->  00:02:23.100
Let's go ahead and just clear the console going to go out and just copy and paste these install commands

32

00:02:23.100  -->  00:02:27.330
in case you need to reference them and I will clear the console and let's just start by talking about

33

00:02:27.330  -->  00:02:29.760
tight ear nerve to use tight ear.

34

00:02:29.790  -->  00:02:39.120
I'm going to say Library CD t id why are you enter in for good measure also just say data that table

35

00:02:39.330  -->  00:02:41.970
as well.

36

00:02:41.970  -->  00:02:42.500
All right.

37

00:02:42.750  -->  00:02:48.600
So we're going to cover the form most useful functions anti-teacher and that's gather spread separate

38

00:02:48.720  -->  00:02:49.910
and unite.

39

00:02:50.610  -->  00:02:54.710
So first we'll start by talking about gather.

40

00:02:54.870  -->  00:03:00.120
So what I gathered was it's going to collapse multiple columns to keep their values and we're going

41

00:03:00.120  -->  00:03:06.840
to go ahead and do is create some fake data that we need to clean using tidy ear and it is going to

42

00:03:06.840  -->  00:03:11.370
get a copy and paste some code from the notes.

43

00:03:11.370  -->  00:03:20.130
So here I'm just saying make some columns comp a year column from 1988 to 2000 and then let's say four

44

00:03:20.130  -->  00:03:26.880
quarter columns that are just random numbers essentially and combine these to just create essentially

45

00:03:27.120  -->  00:03:31.790
a data frame of some sort of quarterly return for a company.

46

00:03:31.800  -->  00:03:34.860
Let's go ahead and run this.

47

00:03:34.860  -->  00:03:41.330
All right so now if I check my data frame if I have a basically data for it looks like this.

48

00:03:41.700  -->  00:03:45.790
Again you can go ahead and just copy this code from the notes in case you want to create this data frame

49

00:03:45.790  -->  00:03:46.340
.

50

00:03:46.830  -->  00:03:50.390
Continuing on let's talk about the gather function.

51

00:03:50.790  -->  00:03:53.480
So the gather function will collapse multiple columns.

52

00:03:53.490  -->  00:03:55.400
It's a key pair of values.

53

00:03:55.560  -->  00:03:58.790
The data frame here is considered white.

54

00:03:59.040  -->  00:04:02.080
So w i d e wide.

55

00:04:02.190  -->  00:04:07.320
And the reason it's considered wide is because the time variable is represented as quarters.

56

00:04:07.380  -->  00:04:11.680
It's structured in a way that each quarter represents a variable or feature.

57

00:04:11.730  -->  00:04:17.130
What we really want to do is restructure the time component of this data as just a single individual

58

00:04:17.130  -->  00:04:17.880
variable.

59

00:04:18.120  -->  00:04:23.790
Now we can gather each quarter within one column variable and also gather the values associated with

60

00:04:23.790  -->  00:04:26.100
each quarter in a second column variable.

61

00:04:26.400  -->  00:04:29.310
So we're going to show you how we can do this with the gather function.

62

00:04:29.310  -->  00:04:33.760
First thing we're going to do is passenger data frame or data set here ZF.

63

00:04:34.320  -->  00:04:45.600
And then I'm going to go ahead and creates a quarter column and a revenue column and then I'm just going

64

00:04:45.600  -->  00:04:47.870
to pass in the actual columns.

65

00:04:47.910  -->  00:04:50.300
I want to gather together.

66

00:04:50.310  -->  00:04:59.700
So in this case it's the cutesie or one column and I can slice all the way to the cutesie are for column

67

00:05:01.430  -->  00:05:04.010
enough I gather these and see what the results are.

68

00:05:04.120  -->  00:05:05.300
It looks like this.

69

00:05:05.380  -->  00:05:14.170
I've been able to gather these four quarter columns of time information into two columns one for quarter

70

00:05:14.200  -->  00:05:16.130
and then one for revenue.

71

00:05:16.150  -->  00:05:23.920
So what is gather doing it's collapsing these last columns the last argument here in two key hair value

72

00:05:23.930  -->  00:05:24.320
.

73

00:05:24.670  -->  00:05:32.500
So if you want to learn more about how he can use gather we can just say help gather and go him pop

74

00:05:32.500  -->  00:05:33.990
out the help tab here.

75

00:05:34.240  -->  00:05:36.430
You know descrive how it gathers columns.

76

00:05:36.430  -->  00:05:38.910
Like I mentioned it's a key value pairs.

77

00:05:39.040  -->  00:05:46.750
So you're going to say gather with scroll back up here gather your data and then you put in your key

78

00:05:46.840  -->  00:05:51.430
and your value pair and then you pass in the actual columns that you want to gather.

79

00:05:51.430  -->  00:05:57.430
So some specification of the columns and you can scroll down here and see that it actually has a few

80

00:05:57.430  -->  00:06:03.280
examples similar to what we were just doing this sort of operations really useful for stock information

81

00:06:03.280  -->  00:06:09.640
or stock prices quarterly sales data and you want to maybe gather all the time series information into

82

00:06:09.640  -->  00:06:13.780
a single column through some sort of key pair of value Association.

83

00:06:13.780  -->  00:06:19.900
Well it might be a little tricky to understand at first is this sort of slicing notation of the columns

84

00:06:20.200  -->  00:06:26.530
but essentially all I'm saying with this command quarter 1 through quarter 4 is just grabbing these

85

00:06:26.530  -->  00:06:27.940
last four columns.

86

00:06:27.940  -->  00:06:38.050
Q1 Q2 Q3 Q4 and what ends up happening is that these values here actually become the revenue.

87

00:06:38.050  -->  00:06:45.010
So my key is quarter and the paired value is revenue which is just the actual cell value here and this

88

00:06:45.010  -->  00:06:52.290
sort of tidy up our data to have quarter as one variable or one feature and revenue as another instead

89

00:06:52.300  -->  00:07:00.430
of these time components and then revenue here is just some unmarked unlabelled value in that cell.

90

00:07:00.430  -->  00:07:07.450
So the reason that this formatting is tighter or cleaner than this first formatting is that quarter

91

00:07:07.450  -->  00:07:13.800
one isn't actually a feature or a variable it's some sort of time constraint.

92

00:07:14.020  -->  00:07:19.900
And if we see something like seventy point ninety six that doesn't actually mean a value of quarter

93

00:07:20.080  -->  00:07:22.260
it's a revenue value in this case.

94

00:07:22.270  -->  00:07:27.850
So we want to label it as such as we did here and the second data frame and we can use gather to do

95

00:07:27.850  -->  00:07:33.430
that sort of key value pairs association and we can do the same sort of operation just by using the

96

00:07:33.430  -->  00:07:34.950
pipe operator.

97

00:07:35.020  -->  00:07:43.780
So if we say death and then we have the pipe operator and then we just call it gather and then we can

98

00:07:43.780  -->  00:07:52.870
skip passing in the data center using our key value pair revenue and then the actual columns we want

99

00:07:52.870  -->  00:07:59.030
to pass in and we get the same result.

100

00:07:59.830  -->  00:08:00.220
Okay.

101

00:08:00.220  -->  00:08:02.610
So let's go in and move on to spread.

102

00:08:02.620  -->  00:08:07.730
So spread is essentially the complement of gather and that's how actually gets the name.

103

00:08:07.900  -->  00:08:13.870
Let me go ahead and copy some code from the notes to create our data that we're going to work with.

104

00:08:14.110  -->  00:08:21.760
I'm going to go ahead and clear this copy and paste this stock information which is essentially just

105

00:08:21.760  -->  00:08:29.420
going to make up some stock information for us may go in and run the source here clear the console cycle

106

00:08:29.530  -->  00:08:35.440
stocks get some centrally random stock information.

107

00:08:35.590  -->  00:08:41.350
So has some time data to it so some sort of time stamp by day and then three features or variables x

108

00:08:41.440  -->  00:08:42.610
y and z.

109

00:08:42.850  -->  00:08:47.190
So let me just go ahead and show the head of stocks now while we work with this.

110

00:08:47.230  -->  00:08:48.840
So head of stocks.

111

00:08:49.180  -->  00:08:50.580
There it is.

112

00:08:50.620  -->  00:08:56.380
OK the first thing I'm going to do here is actually gather the data to go ahead and reinforce what we

113

00:08:56.380  -->  00:08:58.030
just learned about gathering data.

114

00:08:58.060  -->  00:09:01.900
I'm going to go ahead and make a new variable called stocks

115

00:09:04.390  -->  00:09:13.630
gathered in a sign it stocks pipe operator and then I want to go ahead and call that gather function

116

00:09:13.630  -->  00:09:13.750
.

117

00:09:13.750  -->  00:09:17.410
I don't have to pass in my data right now because I'm using the pipe operator.

118

00:09:17.410  -->  00:09:23.320
So I'll go ahead and go straight to the key value pair the key value pair that I'll create is stock

119

00:09:24.130  -->  00:09:30.070
and then price and then the columns I want to do this with are X through Z.

120

00:09:30.520  -->  00:09:38.110
So instead of actually doing something like this x colon Z just to make it even more clear all passen

121

00:09:38.950  -->  00:09:41.020
X comma y come easy.

122

00:09:41.020  -->  00:09:42.840
So this is the exact same thing.

123

00:09:43.120  -->  00:09:48.730
Just hopefully this may be a clear notation for you in passing in the columns and this is a key value

124

00:09:48.730  -->  00:09:55.210
pair for gather like go ahead and take a look at what the head of stocks gathered looks like.

125

00:09:57.140  -->  00:10:01.130
Looks something like this hopefully this looks really similar to the quarter information we're just

126

00:10:01.130  -->  00:10:09.140
working with I have time and then have a key value pair with stock and price as key value Perry created

127

00:10:09.620  -->  00:10:12.610
and then X versus the actual amount there.

128

00:10:12.650  -->  00:10:18.110
And so the reason again that this is tidy or clean data is that this actually is the actual cell value

129

00:10:18.120  -->  00:10:18.160
.

130

00:10:18.170  -->  00:10:21.020
Now corresponds to price.

131

00:10:21.020  -->  00:10:24.980
Otherwise we have this cell value corresponding to a variable.

132

00:10:24.980  -->  00:10:31.490
In this case X which may or may not actually reference the value of whatever's in that cell here it's

133

00:10:31.490  -->  00:10:36.810
quite clear that this does represent the price of a stock labeled x.

134

00:10:37.380  -->  00:10:45.260
Okay let's go ahead and continue on by spreading this data back out so I can call spread using a pipe

135

00:10:45.260  -->  00:10:47.060
operator if I wanted to.

136

00:10:47.060  -->  00:10:53.420
So I'll say stocks that gathered and I'm going to go ahead and use a pipe operator to call a spread

137

00:10:53.420  -->  00:10:54.600
on it.

138

00:10:54.860  -->  00:10:58.610
And usually you would pass in the data first and the spread of that the same reason the pipe operator

139

00:10:58.610  -->  00:10:58.640
.

140

00:10:58.670  -->  00:10:59.550
I don't need to.

141

00:10:59.870  -->  00:11:05.360
And I just actually put in the columns are the key value pair that I want to spread by this case.

142

00:11:05.360  -->  00:11:11.110
It was stock price and there we were able to spread the data.

143

00:11:11.240  -->  00:11:20.150
If I didn't actually want to use this pipe operator I could've just said spread stocks gathered or stock

144

00:11:20.150  -->  00:11:26.960
stock gathered Khama and then it could have passed in the stock column and the price column and I would

145

00:11:26.960  -->  00:11:29.090
have gotten the same results.

146

00:11:29.100  -->  00:11:34.380
So it's spread by more than just that we could have also spread maybe buy time and price.

147

00:11:34.490  -->  00:11:40.100
It look a little weirder but essential make all the rows the X Y and Z of the stocks.

148

00:11:40.100  -->  00:11:41.950
So I'm going to show how you can do that.

149

00:11:41.960  -->  00:11:46.910
I'll go ahead and call spread Passons stock stock gather.

150

00:11:47.630  -->  00:11:56.210
And then I will spread by the time column and the price column and I notice I get something quite a

151

00:11:56.210  -->  00:12:02.280
bit different cent. Now my rose each represents a stock either X Y or Z.

152

00:12:02.310  -->  00:12:07.090
We can imagine these are some sort of stock ticker and that each column is an actual dates.

153

00:12:07.160  -->  00:12:09.170
And here we have a value.

154

00:12:09.230  -->  00:12:14.360
Now whether or not this is tidy or good data is kind of up to you but I would say there's still not

155

00:12:14.360  -->  00:12:19.630
so great because the actual value in this case will say negative 0.6 3.

156

00:12:19.730  -->  00:12:25.220
That's not some sort of time stamp reformations that's not a feature of this date column it's an actual

157

00:12:25.220  -->  00:12:26.090
price.

158

00:12:26.120  -->  00:12:32.900
So we want to try to have the sort of sell information line up with the column name like it did in the

159

00:12:32.960  -->  00:12:35.660
original spread information.

160

00:12:35.660  -->  00:12:40.030
Here are the gathered information here where we have this value is an actual price.

161

00:12:40.220  -->  00:12:47.240
And this character is an actual stock versus over here we have this character is under the date column

162

00:12:47.270  -->  00:12:51.440
but it's actually a price feature but we don't have any price labels anywhere.

163

00:12:51.440  -->  00:12:57.050
So that's the main difference between something that is tidy and some data that is not tidy or not perfectly

164

00:12:57.050  -->  00:12:58.180
clean.

165

00:12:58.220  -->  00:13:02.230
Again it really depends on your situation on what works best for you.

166

00:13:02.240  -->  00:13:06.890
So just keep in mind what situation you're in how you want your data to be presented and what works

167

00:13:06.890  -->  00:13:08.170
best for your analysis.

168

00:13:08.180  -->  00:13:11.840
You don't have to tidy up your data if it doesn't work for you.

169

00:13:12.230  -->  00:13:14.870
In any case you're used to work with Excel a lot.

170

00:13:14.870  -->  00:13:21.560
Or maybe some other sacer SPSS programming like language we can think of these as sort of an analogous

171

00:13:21.560  -->  00:13:23.740
to a pivot table in Excel.

172

00:13:24.280  -->  00:13:29.780
All right finally we're going to learn about the two last functions for tinier which is separate separate

173

00:13:29.870  -->  00:13:30.940
and unite.

174

00:13:31.490  -->  00:13:33.830
OK so let's start by learning about separate.

175

00:13:34.070  -->  00:13:40.100
So separate is going to be given either regular expressions or a vector of character positions.

176

00:13:40.310  -->  00:13:44.980
And what separates going to allow us to do is turn a single character column into multiple columns.

177

00:13:45.350  -->  00:13:53.570
And to start by creating a data frame with all say data that frame all Pessin a vector.

178

00:13:53.570  -->  00:14:03.560
Let's say we just call this vector x or this column X and it's going to be a vector and they will say

179

00:14:03.560  -->  00:14:10.720
that x B that y z z.

180

00:14:10.730  -->  00:14:15.130
So again this wouldn't just make this really obvious by saying this is new.

181

00:14:15.370  -->  00:14:19.530
Well if I called ZF I get my new column.

182

00:14:19.550  -->  00:14:25.450
First row was a null value and then have a that Expwy that Y see that Z as characters.

183

00:14:25.520  -->  00:14:32.120
And what I can go ahead and do is call separate to try to turn a single character column into multiple

184

00:14:32.120  -->  00:14:35.930
columns and I'll show you an example of that.

185

00:14:36.320  -->  00:14:41.060
I call separate pass in my data frame.

186

00:14:41.510  -->  00:14:45.560
And then when I go ahead and do is pass in my column name that I want to separate.

187

00:14:45.590  -->  00:14:53.540
In this case I want to pass that call to separate and then I pass in a vector of the new column names

188

00:14:53.570  -->  00:14:56.900
in this case are going to try to separate this into two new columns.

189

00:14:56.930  -->  00:14:59.610
First column I want to call ABC.

190

00:14:59.810  -->  00:15:03.140
Second call me I want to call X Y Z.

191

00:15:03.560  -->  00:15:10.460
So if I go ahead and run this notice able to actually automatically separate this column the single

192

00:15:10.460  -->  00:15:14.890
columns need that call into two columns ABC and x y z.

193

00:15:15.170  -->  00:15:21.560
So you may be wondering now how did this separate function no to separate the single column value into

194

00:15:21.560  -->  00:15:22.590
C and z.

195

00:15:22.610  -->  00:15:24.450
Or did it know to get rid of this period.

196

00:15:24.770  -->  00:15:31.970
Well by default the separator argument that you can pass into separate will try to separate non alphanumeric

197

00:15:31.970  -->  00:15:38.940
characters but you can signify what you want the separate or separation value to be.

198

00:15:38.960  -->  00:15:42.840
So I want to go ahead and recreate that data frame.

199

00:15:42.860  -->  00:15:56.570
Well let's go ahead an instead of a dot X I will go ahead and say a dash x b dash Y and see Dash Z.

200

00:15:56.690  -->  00:16:03.140
So if I take a look at my data frame DSF I have a null value that Scheck's be that NYC dash city etc.

201

00:16:03.690  -->  00:16:07.010
Let's call separate or separate again.

202

00:16:07.910  -->  00:16:12.410
First thing to do is pass in my data and I'm going to go and actually just label these arguments.

203

00:16:12.410  -->  00:16:13.250
So it's really clear.

204

00:16:13.260  -->  00:16:17.820
So data is going to pass in the call.

205

00:16:17.870  -->  00:16:20.210
So the bear column name.

206

00:16:20.210  -->  00:16:25.670
So if we look above remember that was new that calls to the column again is new that call it's the that's

207

00:16:25.670  -->  00:16:32.850
the column actually want to separate and then I'm going to say into is the third official argument.

208

00:16:33.080  -->  00:16:40.330
So last time I just passed in the vector of the new column names that stands for the into.

209

00:16:40.730  -->  00:16:47.840
So we can think of this as taking this data take this new column or this column value and separate it

210

00:16:48.110  -->  00:16:52.430
into these columns and this case is separated into two columns.

211

00:16:52.430  -->  00:16:59.660
First column is going to be well say ABC which put them in lowercase this time.

212

00:16:59.660  -->  00:17:08.240
Second column is going to be lowercase x y z and then finally I can indicate a SEP argument which is

213

00:17:08.240  -->  00:17:10.490
what I want to separate by this case.

214

00:17:10.490  -->  00:17:14.180
I want to separate by a dash and there we go.

215

00:17:14.180  -->  00:17:22.730
It was able to separate this column just like that if I did it add that Sep arguments and still try

216

00:17:22.730  -->  00:17:24.380
to separate this.

217

00:17:24.440  -->  00:17:29.960
It still would have worked because that dash is a non alphanumeric character meaning it's not a number

218

00:17:30.140  -->  00:17:32.120
or it's not a letter.

219

00:17:32.120  -->  00:17:37.640
So automatically separate on those kind of characters if your data doesn't really work well for some

220

00:17:37.640  -->  00:17:39.510
sort of automatic separation like that.

221

00:17:39.620  -->  00:17:44.990
You can go ahead and pass in whatever pattern or regular expression you want to separate by.

222

00:17:44.990  -->  00:17:51.080
Again the basic syntax is you say separate in the data frame that you're working with the column that

223

00:17:51.080  -->  00:17:56.520
you want to separate in the columns you want to separate it into and then if you want or need to.

224

00:17:56.540  -->  00:18:02.920
You can as Sep arguments and then pass in the character you want to actually split on or separate.

225

00:18:03.460  -->  00:18:04.100
OK.

226

00:18:04.310  -->  00:18:06.780
And then finally a complement to this is unite.

227

00:18:06.800  -->  00:18:11.680
So you're right it's just a convenience function to paste together multiple columns into one.

228

00:18:12.470  -->  00:18:18.380
And the syntax is essentially going to be exactly the same as this separate call except it's going to

229

00:18:18.380  -->  00:18:26.210
do the reverse sort of saying into it's going to take away those columns and put them into a new column

230

00:18:26.210  -->  00:18:26.240
.

231

00:18:26.240  -->  00:18:28.460
So it's going to unite them together.

232

00:18:28.700  -->  00:18:31.170
It's going to show you how we can do this.

233

00:18:31.310  -->  00:18:39.080
I'm going to take the separate results and store it into DFT Sep.

234

00:18:39.350  -->  00:18:46.480
So if I call the SEP I get that separated value don't worry about those warnings.

235

00:18:46.500  -->  00:18:51.770
They're going to clear this and say DFA Sep.

236

00:18:51.770  -->  00:19:00.200
All right so we can unite these columns together by saying unite pass in the data frame I'm working

237

00:19:00.200  -->  00:19:06.650
with in this case it's DFA dot Sep that separated data frame and then I add in the name of the column

238

00:19:06.650  -->  00:19:08.040
that I want to add.

239

00:19:08.060  -->  00:19:16.570
So we'll go ahead and call this column let's say let's call it just new joined us.

240

00:19:16.640  -->  00:19:25.610
Well and then I pass in the list of columns I want to join this case it's ABC and x y z and there we

241

00:19:25.610  -->  00:19:25.930
have it.

242

00:19:25.940  -->  00:19:31.090
There is my new joint column and note that by default it joined if an underscore.

243

00:19:31.310  -->  00:19:40.040
But you can actually add a third argument in here Sep equals an earlier using separate.

244

00:19:40.070  -->  00:19:46.130
That indicated what we wanted to separate on if we add it to the Unite call indicate how we want to

245

00:19:46.130  -->  00:19:48.800
join them if we want to put anything in between them.

246

00:19:48.800  -->  00:19:53.190
So let's go out and put three dashes in between them when we join them.

247

00:19:53.630  -->  00:19:55.870
And there we have it we have these three dashes.

248

00:19:55.880  -->  00:19:58.120
Note that this is no longer a no value.

249

00:19:58.130  -->  00:20:02.230
It's a character string and a dash dash dash and a.

250

00:20:02.960  -->  00:20:03.310
OK.

251

00:20:03.320  -->  00:20:06.070
So those are the basics of type here.

252

00:20:06.290  -->  00:20:08.350
Hopefully you won't have to use Tidy are too much.

253

00:20:08.390  -->  00:20:11.710
If your data set is already in a format you expect it in.

254

00:20:11.900  -->  00:20:17.360
But in case you need to manipulate or play around for data to get in a good format for maybe a machine

255

00:20:17.360  -->  00:20:22.940
learning algorithm or your basic analysis or visualization protocols you can use these four functions

256

00:20:22.940  -->  00:20:27.030
of time here and look up the notes on how best to use them to clean your data.

257

00:20:27.410  -->  00:20:30.170
OK thanks everyone and I'll see you at the next lecture
