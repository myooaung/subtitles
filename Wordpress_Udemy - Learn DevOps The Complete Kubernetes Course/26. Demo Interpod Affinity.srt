1
1

00:00:00,598  -->  00:00:01,839
<v Instructor>I will now show you</v>
2

2

00:00:01,839  -->  00:00:04,720
a demo of Interpod affinity.
3

3

00:00:04,720  -->  00:00:06,806
So I am here now in the affinity folder,
4

4

00:00:06,806  -->  00:00:10,045
of my current discourse Interpod Tree.
5

5

00:00:10,045  -->  00:00:14,475
and here I have found pod Affinity dot E M O.
6

6

00:00:14,475  -->  00:00:16,206
Let's have a look at this,
7

7

00:00:16,206  -->  00:00:19,024
it's going to deploy two deployments:
8

8

00:00:19,024  -->  00:00:22,269
First deployment is pod affinity one,
9

9

00:00:22,269  -->  00:00:24,820
second one is pod affinity two.
10

10

00:00:24,820  -->  00:00:27,191
This first deployment is just a container.
11

11

00:00:27,191  -->  00:00:29,787
No pod affinity defined.
12

12

00:00:29,787  -->  00:00:34,036
it's the second one that has a pod affinity defined,
13

13

00:00:34,036  -->  00:00:36,859
and here we're going to say affinity pod affinity,
14

14

00:00:36,859  -->  00:00:40,625
required during scheduling, ignored during exclusion.
15

15

00:00:40,625  -->  00:00:44,766
We have the label sector F, in pod affinity one.
16

16

00:00:44,766  -->  00:00:45,599
If this is true,
17

17

00:00:45,599  -->  00:00:48,019
then the pod will be scaled on the note
18

18

00:00:48,019  -->  00:00:50,748
that has the same host name.
19

19

00:00:50,748  -->  00:00:51,865
So this is Redis,
20

20

00:00:51,865  -->  00:00:54,728
so this means that the Redis pod,
21

21

00:00:54,728  -->  00:00:58,003
is always going to be scheduled next to the first pod,
22

22

00:00:58,003  -->  00:01:00,855
the pod affinity one pod.
23

23

00:01:00,855  -->  00:01:04,188
Let's kube CTL create this pod affinity.
24

24

00:01:07,746  -->  00:01:09,163
Two pods created,
25

25

00:01:10,761  -->  00:01:12,344
container creating,
26

26

00:01:13,248  -->  00:01:14,081
let's use O wide,
27

27

00:01:14,081  -->  00:01:17,610
and then we can see on what nodes they are created.
28

28

00:01:17,610  -->  00:01:19,138
Both of them are running,
29

29

00:01:19,138  -->  00:01:22,712
both of them are on the same node.
30

30

00:01:22,712  -->  00:01:24,176
Maybe that's a coincidence.
31

31

00:01:24,176  -->  00:01:27,185
So let's scale this deployment,
32

32

00:01:27,185  -->  00:01:28,852
replicas four maybe,
33

33

00:01:30,092  -->  00:01:31,592
of our deployment,
34

34

00:01:32,579  -->  00:01:33,996
pod affinity two.
35

35

00:01:37,524  -->  00:01:38,857
Now it's scaled.
36

36

00:01:42,924  -->  00:01:47,692 line:15% 
All the new pods are on the same node, so you see,
37

37

00:01:47,692  -->  00:01:52,094 line:15% 
pod affinity, make sure that the new pod, the second one,
38

38

00:01:52,094  -->  00:01:56,790 line:15% 
pod affinity two is always scaled next to the first one.
39

39

00:01:56,790  -->  00:01:59,731 line:15% 
We can use different labels as well,
40

40

00:01:59,731  -->  00:02:03,125 line:15% 
So let's say we that we kube CTL delete,
41

41

00:02:03,125  -->  00:02:07,292 line:15% 
this pod affinity to delete those two deployments.
42

42

00:02:08,991  -->  00:02:11,033 line:15% 
Let's have a look at our labels.
43

43

00:02:11,033  -->  00:02:13,033 line:15% 
Get nodes, scribe nodes.
44

44

00:02:14,598  -->  00:02:16,348
We'll use our labels.
45

45

00:02:17,923  -->  00:02:21,821 line:15% 
So here these are all the labels that we can use.
46

46

00:02:21,821  -->  00:02:22,654 line:15% 
For example,
47

47

00:02:22,654  -->  00:02:26,956 line:15% 
if I want to use a failure domain the zone,
48

48

00:02:26,956  -->  00:02:29,123 line:15% 
then we can also use this.
49

49

00:02:30,639  -->  00:02:32,056 line:15% 
And pod affinity,
50

50

00:02:33,251  -->  00:02:36,977 line:15% 
here we can just change it to plodgy key,
51

51

00:02:36,977  -->  00:02:39,200 line:15% 
in the failure domain.
52

52

00:02:39,200  -->  00:02:40,867 line:15% 
This will also work.
53

53

00:02:42,375  -->  00:02:44,221 line:15% 
I only launched in one zone,
54

54

00:02:44,221  -->  00:02:47,589 line:15% 
so you will not see that it doesn't get scaled
55

55

00:02:47,589  -->  00:02:49,854 line:15% 
in one of the nodes because it's all in the same zone.
56

56

00:02:49,854  -->  00:02:51,932 line:15% 
But you will see now they will be scaled,
57

57

00:02:51,932  -->  00:02:54,361 line:15% 
all the nodes that are in the same region,
58

58

00:02:54,361  -->  00:02:56,789 line:15% 
in the same availability zone.
59

59

00:02:56,789  -->  00:02:58,622 line:15% 
Let's create it again,
60

60

00:02:59,708  -->  00:03:01,708 line:15% 
let's scale this to five
61

61

00:03:06,149  -->  00:03:08,240 line:15% 
kube CTL get nodes,
62

62

00:03:08,240  -->  00:03:09,073 line:15% 
O wide,
63

63

00:03:10,638  -->  00:03:13,388 line:15% 
get pods is what I'm looking for.
64

64

00:03:15,529  -->  00:03:18,946 line:15% 
And you see now they're getting scaled
65

65

00:03:18,946  -->  00:03:20,529 line:15% 
on different nodes.
66

66

00:03:21,607  -->  00:03:24,416 line:15% 
Because now you're using the failure domain
67

67

00:03:24,416  -->  00:03:26,262 line:15% 
and not the host name,
68

68

00:03:26,262  -->  00:03:27,844 line:15% 
so that's it for this demo.
69

69

00:03:27,844  -->  00:03:31,971 line:15% 
In the next demo I'll show you how pod anti-affinity works.
