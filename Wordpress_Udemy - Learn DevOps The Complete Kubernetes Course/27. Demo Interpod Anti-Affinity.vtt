WEBVTT
1
1

00:00:00.160  -->  00:00:00.993
<v Instructor>In this demo,</v>
2

2

00:00:00.993  -->  00:00:03.703
I'm going to show you how Pod Anti-Affinity works.
3

3

00:00:05.730  -->  00:00:07.250
So I'm in the Affinity folder
4

4

00:00:07.250  -->  00:00:11.143
and here I have an Pod Anti-Afinity yaml file.
5

5

00:00:12.570  -->  00:00:16.410
I begin my first deployment to Pod Affinity
6

6

00:00:16.410  -->  00:00:19.670
and then I have a second one, Pod Affinity two,
7

7

00:00:19.670  -->  00:00:21.570
that is the same one as the previous one.
8

8

00:00:21.570  -->  00:00:25.600
So it's Pod Affinity, app, in Pod Affinity one.
9

9

00:00:25.600  -->  00:00:29.000
So this one will be collocated to the first pod,
10

10

00:00:29.000  -->  00:00:30.965
is the Redis container and then I have another one,
11

11

00:00:30.965  -->  00:00:35.210
the Pod Affinity three and the Pod Affinity three
12

12

00:00:35.210  -->  00:00:37.373
has Pod Anti-Affinity.
13

13

00:00:38.240  -->  00:00:42.430
And Anti-Affinity is, if the app is Pod Affinity one,
14

14

00:00:42.430  -->  00:00:45.443
then don't scale on the same hostname.
15

15

00:00:46.384  -->  00:00:47.623
So let's try this one.
16

16

00:00:48.670  -->  00:00:52.353
Kube C-T-L create, Pod Anti-Affinity.
17

17

00:00:54.760  -->  00:00:57.553
There's a fourth one but I will first show you the third one
18

18

00:00:57.553  -->  00:01:00.660
and then I will show you how the fourth one works.
19

19

00:01:00.660  -->  00:01:04.260
Kube C-T-L get Pod, minus O wide.
20

20

00:01:06.200  -->  00:01:10.380
Pod Affinity one is running on one-eight-one.
21

21

00:01:10.380  -->  00:01:15.380
Pod Affinity two, those three are running, all of them,
22

22

00:01:15.730  -->  00:01:19.410
on one-eight-one because these are collocated
23

23

00:01:19.410  -->  00:01:22.690
and the next one, the Pod Affinity three,
24

24

00:01:22.690  -->  00:01:26.340
also three of them are scaled on another node
25

25

00:01:26.340  -->  00:01:30.140
because they cannot be on the same node as Pod Affinity one.
26

26

00:01:30.140  -->  00:01:31.940
So this is all working.
27

27

00:01:31.940  -->  00:01:36.940
This one, Pod Affinity three is using Anti-Affinity
28

28

00:01:37.640  -->  00:01:39.500
to make sure that it doesn't get held
29

29

00:01:39.500  -->  00:01:42.233
on the same node as Pod Affinity one.
30

30

00:01:44.640  -->  00:01:47.423
Let's have a look at the Pod Affinity four.
31

31

00:01:47.423  -->  00:01:49.660
I have one more here.
32

32

00:01:49.660  -->  00:01:51.810
This Pod Affinity four assess.
33

33

00:01:51.810  -->  00:01:54.890
I have Pod Anti-Affinity, the match expression is,
34

34

00:01:54.890  -->  00:01:58.090
app in Pod Affinity one and three,
35

35

00:01:58.090  -->  00:02:02.940
so this pod cannot be scaled on a node
36

36

00:02:02.940  -->  00:02:06.050
of the same node host name of Pod Affinity one
37

37

00:02:06.050  -->  00:02:07.393
and Pod Affinity three.
38

38

00:02:08.640  -->  00:02:11.893
So if you have a look at kube C-T-L get nodes,
39

39

00:02:13.350  -->  00:02:15.233
you only have two nodes here.
40

40

00:02:15.233  -->  00:02:17.590
This is the first node.
41

41

00:02:17.590  -->  00:02:19.540
Pod Affinity one is running on it
42

42

00:02:19.540  -->  00:02:23.250
and then we have the second node, that ends with 46.
43

43

00:02:23.250  -->  00:02:25.810
And we have Pod Affinity three running on it.
44

44

00:02:25.810  -->  00:02:29.430
So Pod Affinity four needs to be not on...
45

45

00:02:30.430  -->  00:02:32.070
a node where Pod Affinity one is running
46

46

00:02:32.070  -->  00:02:35.030
and not on a node where Pod Affinity three is running.
47

47

00:02:35.030  -->  00:02:38.480
So there are no nodes, because I only have two nodes.
48

48

00:02:38.480  -->  00:02:43.480
So if I do kube C_T_L describe Pod, Pod Affinity four,
49

49

00:02:44.110  -->  00:02:47.440
then zero out of three nodes are available.
50

50

00:02:47.440  -->  00:02:51.700
The first one is a master so I cannot scale on that one
51

51

00:02:51.700  -->  00:02:55.110
and the other ones are because of the Anti-Affinity rule.
52

52

00:02:55.110  -->  00:02:59.640
I cannot scale on any node because it's just not possible.
53

53

00:02:59.640  -->  00:03:04.640
Now the Pod doesn't scale because we used the required flag.
54

54

00:03:06.270  -->  00:03:11.183
We have log Pod Anti-Affinity dot yaml,
55

55

00:03:12.220  -->  00:03:15.430
we used required during scheduling ignored during execution.
56

56

00:03:15.430  -->  00:03:18.690
So let's say if we change this into the preferred one,
57

57

00:03:18.690  -->  00:03:19.840
what would happen then?
58

58

00:03:21.080  -->  00:03:23.090
I have this already prepared
59

59

00:03:23.090  -->  00:03:26.110
in Pod Anti-Affinity five dot yaml.
60

60

00:03:26.110  -->  00:03:28.540
Here we have Pod Affinity five defined
61

61

00:03:29.380  -->  00:03:32.040
and the only thing that's different here, is that I have
62

62

00:03:32.040  -->  00:03:35.650
preferred during scheduling ignored during execution.
63

63

00:03:35.650  -->  00:03:36.890
You have to give the weight,
64

64

00:03:36.890  -->  00:03:39.320
you have to use a cue of Pod Affinity term
65

65

00:03:39.320  -->  00:03:41.690
and then we have a label sector.
66

66

00:03:41.690  -->  00:03:43.690
Expression is exactly the same.
67

67

00:03:43.690  -->  00:03:46.290
App in Pod Affinity one and three.
68

68

00:03:46.290  -->  00:03:48.170
So because this is Anti-Affinity
69

69

00:03:48.170  -->  00:03:50.670
if there's already a Pod one or Pod three
70

70

00:03:50.670  -->  00:03:52.620
it cannot be scaled on that node
71

71

00:03:53.690  -->  00:03:56.173
because we're using host name as topology key.
72

72

00:03:57.500  -->  00:03:58.350
Let's run this.
73

73

00:03:58.350  -->  00:04:03.350
Kube C-T-L create Pod Anti-Affinity number five dot yaml.
74

74

00:04:05.390  -->  00:04:09.213
Let's have a look at our pods, O wide.
75

75

00:04:13.000  -->  00:04:17.030
This is our four, it's pending and this is our fifth,
76

76

00:04:17.030  -->  00:04:19.760
which is running as we can see,
77

77

00:04:19.760  -->  00:04:24.760
it is now running on the same node as our pod affinity two
78

78

00:04:25.790  -->  00:04:28.430
because we have preferred instead of required,
79

79

00:04:28.430  -->  00:04:31.583
so it still gets scaled even though it's not an ideal match.
80

80

00:04:32.470  -->  00:04:34.790
So that's it for Pod Anti-Affinity.
81

81

00:04:34.790  -->  00:04:36.640
So you can play around with it a bit
82

82

00:04:36.640  -->  00:04:39.100
and if you have any questions you can always use the Q and A
83

83

00:04:39.100  -->  00:04:41.710
to ask me questions about Pod Affinity
84

84

00:04:41.710  -->  00:04:44.403
or Pod Anti-Affinity or if you get stuck somewhere.
