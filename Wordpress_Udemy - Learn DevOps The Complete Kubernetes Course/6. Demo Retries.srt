1
1

00:00:00,310  -->  00:00:04,490
<v Edward>One of the great features of Istio is retries.</v>
2

2

00:00:04,490  -->  00:00:06,490
If one of your pods stops working,
3

3

00:00:06,490  -->  00:00:09,260
Istio can redirect traffic to another pod
4

4

00:00:09,260  -->  00:00:11,513
without the client seeing an impact.
5

5

00:00:12,460  -->  00:00:15,480
In this demo, I will deploy version three
6

6

00:00:15,480  -->  00:00:17,323
of our hello world app.
7

7

00:00:19,040  -->  00:00:21,680
Version three will be two deployments.
8

8

00:00:21,680  -->  00:00:23,870
We'll have two pods in the first deployment
9

9

00:00:23,870  -->  00:00:26,970
and the one pod in the second deployment.
10

10

00:00:26,970  -->  00:00:30,500
The difference is that the pod is the second deployment
11

11

00:00:30,500  -->  00:00:33,090
will have a pod latency of five seconds.
12

12

00:00:33,090  -->  00:00:36,210
So the boss and arrival variable
13

13

00:00:36,210  -->  00:00:38,860
to tell our application that it has to wait
14

14

00:00:38,860  -->  00:00:41,083
five seconds before excluding the codes.
15

15

00:00:42,180  -->  00:00:47,170
In our virtual service, we want a timeout of 10 seconds.
16

16

00:00:47,170  -->  00:00:49,540
So within 10 seconds, we need to see a reply.
17

17

00:00:49,540  -->  00:00:53,255
Otherwise, we just give up on the request.
18

18

00:00:53,255  -->  00:00:56,040
Within those 10 seconds, we're going to retry
19

19

00:00:56,040  -->  00:00:58,712
two times, so we're going to define two retry items
20

20

00:00:58,712  -->  00:01:02,640
and the per retry timeout is two seconds.
21

21

00:01:02,640  -->  00:01:06,819
So if a pod doesn't reply within two seconds,
22

22

00:01:06,819  -->  00:01:11,420
then Istio will try using another pod.
23

23

00:01:11,420  -->  00:01:14,770
So our pod latency is five seconds.
24

24

00:01:14,770  -->  00:01:18,280
Our hello v3 will never make it within
25

25

00:01:18,280  -->  00:01:21,070
this retries amount and we should only get
26

26

00:01:21,070  -->  00:01:23,650
the request from the first two pods.
27

27

00:01:23,650  -->  00:01:24,773
Let's try this out.
28

28

00:01:26,070  -->  00:01:29,943
Let's have a look at hello world v3.yaml.
29

29

00:01:31,390  -->  00:01:32,790
This is my first deployment.
30

30

00:01:33,960  -->  00:01:36,030
Hello v3 is the name of it.
31

31

00:01:36,030  -->  00:01:39,593
Two replicas and I'm just going to echo hello,
32

32

00:01:39,593  -->  00:01:42,690
this is my pod name so that I know what pod
33

33

00:01:42,690  -->  00:01:44,310
I am reaching.
34

34

00:01:44,310  -->  00:01:48,070
This defines my pod name, so it comes from the metadata.
35

35

00:01:48,070  -->  00:01:50,443
These will then just reply the name of the pod.
36

36

00:01:53,290  -->  00:01:57,000
My second deployment is called hello v3 latency.
37

37

00:01:57,000  -->  00:02:00,470
It's only one replica and it's also going to reply
38

38

00:02:00,470  -->  00:02:02,710
hello, this is and the pod name.
39

39

00:02:02,710  -->  00:02:05,223
But it will introduce a latency of five seconds.
40

40

00:02:06,660  -->  00:02:10,883
This pod will still show text but only after five seconds.
41

41

00:02:13,290  -->  00:02:17,560
We're then going to define a new destination rule
42

42

00:02:17,560  -->  00:02:18,800
or change the current one.
43

43

00:02:18,800  -->  00:02:21,363
We're going to add v3 to it.
44

44

00:02:24,795  -->  00:02:27,640
We're then going to define a new virtual service.
45

45

00:02:27,640  -->  00:02:32,620
Hello world v3 with the host hello v3 example.com.
46

46

00:02:32,620  -->  00:02:37,620
In this hello v3, we'll write everything to the v3
47

47

00:02:38,570  -->  00:02:40,990
version of our pods, so it's routed to the
48

48

00:02:40,990  -->  00:02:44,750
two deployments but we are going to say here
49

49

00:02:44,750  -->  00:02:47,910
timeout 10 seconds, retries attempts two,
50

50

00:02:47,910  -->  00:02:50,563
and per try timeout is two seconds.
51

51

00:02:53,680  -->  00:02:58,680
Let's apply this kube C-T-L apply hello world v3.yaml.
52

52

00:03:03,200  -->  00:03:06,313
This will create two new deployments.
53

53

00:03:07,910  -->  00:03:11,510
It will reconfigure our hello destination rule.
54

54

00:03:11,510  -->  00:03:13,510
We have a look at this destination rule.
55

55

00:03:16,320  -->  00:03:21,320
Destination rule, so we have the hello destination rule.
56

56

00:03:22,500  -->  00:03:23,803
So describe this one.
57

57

00:03:27,110  -->  00:03:30,510
Now we have v1, v2, and v3.
58

58

00:03:30,510  -->  00:03:34,090
If you do get virtual service,
59

59

00:03:34,090  -->  00:03:36,170
you'll now have a virtual service for hello world
60

60

00:03:36,170  -->  00:03:38,490
from our previous example and now we have
61

61

00:03:38,490  -->  00:03:39,523
the hello world v3.
62

62

00:03:40,960  -->  00:03:42,463
Let's have a look at the pods.
63

63

00:03:46,570  -->  00:03:50,698
These are our v3 pods, hello world v3,
64

64

00:03:50,698  -->  00:03:52,263
v3, and v3 latency.
65

65

00:03:54,070  -->  00:03:58,470
Let's take this end point, hello v3 example.com
66

66

00:03:58,470  -->  00:04:02,620
is our host in this case, so this is going to
67

67

00:04:02,620  -->  00:04:04,140
then give us the
68

68

00:04:05,036  -->  00:04:10,036
hello, this hello v3 c5, so it's our first deployment.
69

69

00:04:11,430  -->  00:04:14,263
And it's this pod that is replying.
70

70

00:04:16,030  -->  00:04:18,310
We'll need to do it a couple of times
71

71

00:04:18,310  -->  00:04:20,223
so let's use our for loop again.
72

72

00:04:23,380  -->  00:04:26,110
Hello world v3 for the for loop and let's
73

73

00:04:26,110  -->  00:04:30,370
also add time, so do time curl.
74

74

00:04:30,370  -->  00:04:34,120
If you do time, then we'll see the time
75

75

00:04:34,120  -->  00:04:37,880
that it needs to execute this curl command.
76

76

00:04:37,880  -->  00:04:40,310
Because what's gonna happen is that if
77

77

00:04:40,310  -->  00:04:45,310
we first hit this v3 latency, then our request
78

78

00:04:45,860  -->  00:04:48,240
will take two seconds longer because the retry
79

79

00:04:48,240  -->  00:04:51,770
timeout for this pod is two seconds.
80

80

00:04:51,770  -->  00:04:54,680
And because this pod only answers after five seconds,
81

81

00:04:54,680  -->  00:04:58,060
after two seconds, it still will retry one of those pods.
82

82

00:04:58,060  -->  00:05:00,500
So our time should be a little bit more than
83

83

00:05:00,500  -->  00:05:04,021
two seconds if we hit this one, but if we directly
84

84

00:05:04,021  -->  00:05:08,890
one of these two, then it should be immediately.
85

85

00:05:08,890  -->  00:05:11,723
So let's execute this and let's look for that result.
86

86

00:05:17,140  -->  00:05:18,990
Here we immediately see it happening.
87

87

00:05:25,460  -->  00:05:29,560
First of all, we never see these latency pods here.
88

88

00:05:29,560  -->  00:05:31,620
So the only pods that can give us output
89

89

00:05:31,620  -->  00:05:34,273
are the pods from the first deployment.
90

90

00:05:35,540  -->  00:05:39,260
Then, if we immediately hit the first deployment,
91

91

00:05:39,260  -->  00:05:41,860
then you see that the real time to execute
92

92

00:05:41,860  -->  00:05:42,873
is very low.
93

93

00:05:43,840  -->  00:05:48,670
You see here and here and here is 0.0 something.
94

94

00:05:48,670  -->  00:05:52,723
If we hit the latency pod, then you see
95

95

00:05:52,723  -->  00:05:56,183
that there is a two second timeout that is added.
96

96

00:05:57,100  -->  00:05:59,340
So for two seconds, it's trying to get a reply
97

97

00:05:59,340  -->  00:06:00,660
from this pod.
98

98

00:06:00,660  -->  00:06:03,330
It gave up, it went to another pod,
99

99

00:06:03,330  -->  00:06:05,330
which is a pod from the first deployment
100

100

00:06:06,280  -->  00:06:09,840
and then this replied very fast within less than a second
101

101

00:06:10,910  -->  00:06:13,960
and this gives us two seconds and just something
102

102

00:06:13,960  -->  00:06:16,810
where the two seconds comes from the latency pod
103

103

00:06:16,810  -->  00:06:20,420
and your time comes from doing the request itself
104

104

00:06:20,420  -->  00:06:24,493
and also the execution of hello v3 and then this pod.
105

105

00:06:26,640  -->  00:06:29,910
If we wouldn't have retry, we would always have
106

106

00:06:29,910  -->  00:06:33,720
to wait five seconds, so I can change the retry
107

107

00:06:33,720  -->  00:06:35,470
or I can remove the retry
108

108

00:06:35,470  -->  00:06:37,343
to show you what would then happen.
109

109

00:06:38,580  -->  00:06:41,467
Hello world v3.yaml.
110

110

00:06:41,467  -->  00:06:44,510
Just remove the retry altogether, so I'll just
111

111

00:06:44,510  -->  00:06:45,593
come at it for now.
112

112

00:06:48,730  -->  00:06:53,227
Let's apply this again, hello world v3.yaml
113

113

00:06:55,390  -->  00:06:57,260
and now the retry is gone.
114

114

00:06:57,260  -->  00:07:00,421
So what are we going to see now is that
115

115

00:07:00,421  -->  00:07:03,760
we will get the request from our latency pod
116

116

00:07:03,760  -->  00:07:05,590
and it will not be retried but the latency
117

117

00:07:05,590  -->  00:07:06,703
will be a lot higher.
118

118

00:07:07,870  -->  00:07:11,713
Let's do time curl on this hello v3 host.
119

119

00:07:13,510  -->  00:07:15,900
Hello, this is v3 and is not a latency pod.
120

120

00:07:15,900  -->  00:07:17,133
This one's really quick.
121

121

00:07:18,670  -->  00:07:21,710
Now I do it again and you see now we are waiting
122

122

00:07:23,120  -->  00:07:26,230
and it is now five seconds, more than five seconds.
123

123

00:07:26,230  -->  00:07:28,630
And the reply comes from this hello latency pod.
124

124

00:07:29,643  -->  00:07:33,000
Using retries, you can make sure that you
125

125

00:07:33,000  -->  00:07:35,703
regress our retries and route it to a pod
126

126

00:07:35,703  -->  00:07:38,663
that can answer within the retry latency.
127

127

00:07:39,700  -->  00:07:42,310
This can avoid that an end user has to wait
128

128

00:07:42,310  -->  00:07:45,990
a long time for one pod to respond to its request.
129

129

00:07:45,990  -->  00:07:49,630
If the pod doesn't respond, then it can be retried
130

130

00:07:49,630  -->  00:07:52,530
using another pod that doesn't have this latency.
131

131

00:07:52,530  -->  00:07:55,840
That is maybe not handling as many requests as
132

132

00:07:55,840  -->  00:07:58,690
the other pod and therefore, it can respond much quicker.
133

133

00:08:00,230  -->  00:08:02,410
So this is something very useful to implement
134

134

00:08:02,410  -->  00:08:04,563
if your applications support it.
