WEBVTT
1
00:00:00.300 --> 00:00:04.640
This lecture will be about node maintenance.

2
00:00:04.680 --> 00:00:13.010
It is the node controller that is responsible for managing the node objects it assigns IP space to the node

3
00:00:13.100 --> 00:00:15.550
when a new node is launched.

4
00:00:15.820 --> 00:00:19.570
It keeps a node list up to date with the available machines.

5
00:00:20.580 --> 00:00:24.600
The Node Controller is also monitoring the health of the node.

6
00:00:25.050 --> 00:00:33.580
If a node is unhealthy it gets deleted. Pods running unhealthy nodes will get rescheduled.

7
00:00:33.580 --> 00:00:34.280
When adding a new node.

8
00:00:34.370 --> 00:00:37.760
The kubelet will attempt to register itself.

9
00:00:37.760 --> 00:00:41.210
This is called self-registration and is a default behavior.

10
00:00:41.390 --> 00:00:46.530
You can do manual registration, but self-registration is obviously more handy.

11
00:00:46.640 --> 00:00:52.210
It allows you to easily add more nodes to the cluster without making API changes yourself.

12
00:00:53.040 --> 00:01:01.400
A new node object is automatically created with the metadata, name, IP hostname, and labels.

13
00:01:01.440 --> 00:01:09.210
For instance, the cloud region, availability zone, and instance size. A node also has a node condition,

14
00:01:09.200 --> 00:01:12.690
for instance, it can be Ready, OutOfDisk, when the disk is full.

15
00:01:13.670 --> 00:01:18.080
When you want to decommission a node you want to do it gracefully.

16
00:01:18.080 --> 00:01:20.280
You just don't want to shut it down immediately.

17
00:01:20.330 --> 00:01:25.520
You want to instruct Kubernetes to move first all the running away from it.

18
00:01:25.710 --> 00:01:31.760
So, you drain a node before you shut it down or take it out of the cluster. To drain a node

19
00:01:31.770 --> 00:01:38.470
you can use the following command, "kubectl drain" and the node name, it can also pass a "--grace-period".

20
00:01:38.730 --> 00:01:45.180
So, a period that it will take to, to gracefully be decommissioned. If the node runs pods not managed by

21
00:01:45.180 --> 00:01:49.340
a controller like replication controller or a replication set.

22
00:01:49.500 --> 00:01:56.430
But if it's just a single pod then you need to force it, because those pods will then be lost.

23
00:01:56.520 --> 00:01:57.470
In the next short demo

24
00:01:57.510 --> 00:01:58.950
I will show you how to drain works.
