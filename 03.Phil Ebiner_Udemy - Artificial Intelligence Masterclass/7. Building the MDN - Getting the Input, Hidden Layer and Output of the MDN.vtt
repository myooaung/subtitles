WEBVTT

00:00.300 --> 00:06.210
Hello and welcome to this new tutorial and also to this new code section because as you can see now

00:06.210 --> 00:11.230
we're about to build the NBN part of the NBN iron model.

00:11.310 --> 00:14.910
Indeed we're done with the classic creation of the iron.

00:15.000 --> 00:21.690
You know we've just got at the end here the deterministic output by the iron and thanks to this dynamic

00:21.720 --> 00:22.910
on function.

00:23.130 --> 00:29.910
And now we're about to associate it with what we've created here meaning the matrix of wait up w and

00:29.910 --> 00:37.830
The Wanted tensor biases output B to create this extra hidden layer which is exactly what we see here

00:37.860 --> 00:45.090
or right here which are the Gaussian mixtures which will return as the caustic prediction of the output

00:45.090 --> 00:46.110
side of the island.

00:46.410 --> 00:50.590
And we're going to do this thanks to our five Gaussian mixtures.

00:50.670 --> 00:56.700
Remember came X equals five because remember that creates basically five separate distributions of the

00:56.700 --> 00:58.700
output as we can see here.

00:58.740 --> 01:05.340
And so now what we're gonna do is first reshape that final deterministic output that we've just got

01:05.670 --> 01:14.280
and which remember is a 3D tensor of 100 samples in a batch this correspond to the batch 1000 frames

01:14.370 --> 01:21.330
in the sequence meaning a sequence of 1000 frames representing a part of an episode when we play the

01:21.330 --> 01:28.650
game so that we can catch the time dependencies happening in these successive 1000 frames.

01:28.650 --> 01:37.440
And for each of the 1000 frames we have this output vector of 256 elements which basically encode what's

01:37.440 --> 01:39.240
happening inside the environment.

01:39.240 --> 01:45.450
So you have to understand that the reason why we take these 1000 frames here is just to catch the time

01:45.480 --> 01:52.470
related dependencies of some parts of the game and all these time related dependencies are besides encoded

01:52.650 --> 01:55.710
by these vectors of 256 elements.

01:56.220 --> 01:56.490
All right.

01:56.510 --> 02:02.460
So this is the actual shape of our outputs and now we're about to reshape it because what we want to

02:02.460 --> 02:11.460
do now is to as we mentioned before multiplied the output by the matrix of weight output W but in order

02:11.460 --> 02:20.310
to do that since we're going to use that function g f does n n that x w plus B which will do a matrix

02:20.310 --> 02:26.580
multiplication of X here which will be our output which will be at the left of the matrix multiplication

02:27.060 --> 02:33.570
and the weight which will be at the right of the matrix multiplication plus the biases and therefore

02:33.690 --> 02:41.430
since the actual shape of the matrix of weights output W is as we can see so I'm going to execute this

02:41.850 --> 02:51.690
is as we can see 256 by 480 so 480 corresponds to that and out number here which is the multiplication

02:51.690 --> 02:54.020
of out with that K mix the three.

02:54.060 --> 03:01.140
So basically it has two hundred and fifty six lines and 480 columns and therefore since we're multiplying

03:01.140 --> 03:09.270
the output by the left to that matrix of weights output w well therefore we need to flatten the output

03:09.510 --> 03:19.050
so that it has 256 columns which will be multiplied by the 256 rows of the matrix of weight output W

03:19.590 --> 03:26.040
and therefore what we're going to do right now since we have one hundred samples of 1000 frames each

03:26.040 --> 03:33.030
one being encoded by a vector of 256 elements which is I remind on in size Well what we're gonna do

03:33.030 --> 03:34.710
is flatten all this.

03:34.740 --> 03:43.980
Basically we're going to take each of the 100 times 1000 vectors of 256 elements and gather all of them

03:44.040 --> 03:53.760
in one same huge to do tensor where we're going to have 100 times 1000 rows and 256 columns and these

03:53.970 --> 04:03.000
256 guns will then be able to be multiplied to the 256 rows of the matrix of weights output w 2 which

04:03.000 --> 04:05.420
then we're going to add the basis.

04:05.460 --> 04:11.040
So let's do this the way we're going to do this is thanks to the reshape function by tensor flow which

04:11.040 --> 04:18.660
we've already used for our v e and so since we're about to modify the output well I'm going to reintroduce

04:18.660 --> 04:26.730
this output variable and now I'm going to modify it by first called intensive flow to f from which we're

04:26.730 --> 04:34.560
going to get that reshape function and to which we're going to input first what we want to reshape the

04:34.560 --> 04:40.080
tensor we want to reshape which is of course so far are 3D sensor outputs.

04:40.150 --> 04:41.250
So here we go.

04:41.250 --> 04:48.450
That's what we want to reshape and then we have to specify in square brackets the format of how we want

04:48.450 --> 04:49.530
to reshape it.

04:49.620 --> 04:56.100
And the trick to flatten basically we're flattening these two dimensions here you know by gathering

04:56.100 --> 05:03.150
in one same dimension these 100 samples of the batch times 1000 frames of the sequence length.

05:03.470 --> 05:09.530
And the trick to do that is by you know adding a minus one here which will basically gather in one same

05:09.530 --> 05:16.880
dimension the first two dimensions and to specify that what's one together in you know that merge of

05:16.880 --> 05:22.850
the dimensions are those two hundred and fifty six elements of each of the vectors of each of the 1000

05:22.850 --> 05:31.070
frames for each of the 100 samples and therefore we'll specify here HP as that ion in size because that's

05:31.070 --> 05:39.200
exactly this 256 number here which is not only the size of the ion Enbrel so the number of elements

05:39.200 --> 05:42.800
in the output vector and you're going to see that by doing this well.

05:42.860 --> 05:46.400
Output is now going to have the dimensions we're looking for.

05:46.400 --> 05:49.850
So I'm going to execute this right.

05:50.060 --> 05:53.910
I don't need to go back to on values we can just do it directly this way.

05:53.990 --> 05:54.720
Here we go.

05:55.010 --> 05:56.620
That's our new output.

05:56.640 --> 05:59.300
Now let's see its new shape.

05:59.300 --> 06:00.970
I'm going to execute this.

06:01.100 --> 06:01.760
And here we go.

06:01.760 --> 06:08.710
Now indeed it is a 2D tensor of remember 100 times 1000 elements.

06:08.760 --> 06:15.700
One hundred thousand elements in total that's resulting from the merge of the two first dimensions here

06:15.710 --> 06:21.020
the number of samples in the batch and the number of friends in the sequence length and of course we

06:21.020 --> 06:29.780
kept these vectors of 256 elements for each of the 1000 frames of each of the 100 samples of the batch.

06:30.230 --> 06:30.530
All right.

06:30.530 --> 06:37.820
So now we get the right shape and now we can indeed do this matrix multiplication of the output which

06:37.820 --> 06:45.590
is that X here in this x w plus b function you know computing the matrix multiplication of X by the

06:45.600 --> 06:54.110
weights plus the biases because indeed now we can clearly see it the output has 256 columns and the

06:54.110 --> 07:02.600
matrix of weights has 256 rows which allows this matrix multiplication of X times w output times output

07:02.600 --> 07:03.660
W.

07:03.880 --> 07:04.240
All right.

07:04.250 --> 07:04.700
Perfect.

07:04.700 --> 07:10.130
So maybe I'll specify somewhere in the course that it's probably better to refresh our memories of linear

07:10.130 --> 07:16.420
algebra and specialty matrix multiplication but you'll have no problem finding some quick refresh on

07:16.430 --> 07:22.370
line so I'm not too worried and therefore let's move on to the next step which is.

07:22.400 --> 07:28.220
Well now that we have the right shapes of the output and the matrix of weights output W and the one

07:28.220 --> 07:35.570
the tensor of bias is output B Well we are ready to make that matrix multiplication which will create

07:35.570 --> 07:40.110
indeed that frame of the Indian output layer.

07:40.220 --> 07:47.990
That extra hidden layer which will give us the possibility to get the stochastic prediction of the output

07:47.990 --> 07:48.990
set.

07:49.040 --> 07:54.730
All right so let's do this and therefore we're directly going to go here and we're going to take all

07:54.740 --> 08:01.100
this just this so that we can get the right name of the function and now well we're just going to do

08:01.100 --> 08:06.830
the saying we're going to modify the output again because basically what we've just done here you know

08:06.830 --> 08:13.580
is not only to be able to do that matrix multiplication This is actually the input of the NDA and you

08:13.580 --> 08:18.080
know it's like what is entering here in this part.

08:18.080 --> 08:25.900
You know this is the hidden layer and it takes as input the deterministic outcome of the classic Ironman

08:26.150 --> 08:30.440
which is exactly what we've obtained here and now.

08:30.440 --> 08:35.960
Now the two have just got the input of the engine Well we're gonna get the hidden layer if you want

08:36.160 --> 08:38.420
of the MGM which we're gonna get.

08:38.420 --> 08:44.650
Thanks to that matrix multiplication of the outputs of the classic RNA which is the input of the MGM

08:44.990 --> 08:51.130
multiplied by this matrix of weight output w plus this one the tensor of Baez's output.

08:51.770 --> 08:58.460
And so as we said we're going to modify output again which will now become that hidden layer of the

08:58.510 --> 09:04.910
MGM and this is now they have to paste what I've just copied in the tensor float documentation that's

09:04.940 --> 09:12.310
close parenthesis and now let's input the arguments so let's see you know I don't know the tensor flow

09:12.320 --> 09:15.610
functions by heart I always look at the documentation.

09:15.620 --> 09:19.070
So this is like I'm coding something new with you.

09:19.100 --> 09:26.690
So the first argument that it takes is X which is of course are output the output of the classic ion

09:26.690 --> 09:29.450
which is also the input of the MGM.

09:29.450 --> 09:35.870
So here we have to input of course first output good then it's going to be so easy.

09:35.870 --> 09:42.860
The next argument is the weights and which is of course that output w matrix of weights which you've

09:42.860 --> 09:50.310
created before in the iron in scope so pacing there right here and then of course very easily we're

09:50.330 --> 09:56.870
going to input the last argument which is this one the tensor of biases when did tensor biases which

09:56.870 --> 10:05.640
is of course well output B so this is going to be what comes after the plus output beat which of course

10:05.640 --> 10:10.710
must have the same number of elements that we have here you know because this the number that we need

10:10.710 --> 10:16.590
to have for the matrix multiplication but then this is what needs to be aligned with the biases.

10:16.830 --> 10:23.910
So if we select this output being executed we should have indeed four hundred and eighty as number of

10:23.910 --> 10:27.030
elements of this one unique dimension.

10:27.030 --> 10:28.020
So this is all good.

10:28.020 --> 10:34.500
This all makes sense mathematically to compute this and therefore now we have this hidden layer of the

10:34.500 --> 10:41.580
engine that is this part exactly that we see right here at MGM by the hidden layer between the input

10:41.580 --> 10:48.210
of the MGM which is the output of the classic iron and stochastic output or stochastic prediction of

10:48.210 --> 10:53.730
Z which is our final output in which is what we're about to get right now and the way we're going to

10:53.730 --> 11:01.230
get it is through another reshape because indeed if we execute this now well you're going to see that

11:01.620 --> 11:03.740
the shape of the output has changed.

11:03.750 --> 11:04.460
Here we go.

11:04.460 --> 11:08.190
Now I'm going to select this and execute.

11:08.190 --> 11:15.600
And now of course the shape of the output has changed from a to day tensor one hundred thousand rows

11:15.600 --> 11:22.800
and 256 columns to add to the tensor of one hundred thousand rows and four hundred and eighty columns

11:22.800 --> 11:28.510
of course because that results from the matrix multiplication to which we added the biases and now what

11:28.510 --> 11:31.980
we're going to do to finish this is very similar to what we did here.

11:31.980 --> 11:37.310
So I'm just going to copy this and right here just below I'm going to paste it.

11:37.530 --> 11:38.330
And here we go.

11:38.340 --> 11:44.910
So we're modifying again the output by taking the previous output which is this output here which is

11:44.910 --> 11:53.010
the hidden layer of the NBN and that at this stage had a 2D tensor shape of hundred thousand rows and

11:53.030 --> 11:54.850
480 columns.

11:54.960 --> 12:05.100
And now we're about to reshape by just replacing this HP as that aren't in size by K mix times three

12:05.450 --> 12:11.280
and we'll get that final stochastic output of the engine Arnon but then we'll get the MTM coefficients

12:11.550 --> 12:12.360
in the next story.

12:12.390 --> 12:18.600
But before we get to it well let's execute this again to you know have a final look at what we've created.

12:18.600 --> 12:21.150
You know that final shape of the output.

12:21.150 --> 12:28.380
Here we go executing this and now I'm going to select this execute it and indeed we have created this

12:28.380 --> 12:35.430
2D sensor of the output with all these number of rows each one getting the number of mixtures times

12:35.790 --> 12:42.210
the number of actions that is five times three 15 and that's exactly the shape that is expected in the

12:42.210 --> 12:46.350
full world model in that specific engine or in in part.

12:46.380 --> 12:48.590
So that's why I wanted to show you this.

12:48.720 --> 12:52.520
This is exactly the shape that we need for the full world model.

12:52.560 --> 12:59.610
And now just before we move on to the next sartorial let's keep in an object void will that last date

12:59.790 --> 13:03.200
which was returned to us by the dynamic on end function.

13:03.330 --> 13:10.290
We're going to put that in the final object variable which we're going to call self that final state

13:10.950 --> 13:13.550
and that will be equal to of course last date.

13:13.560 --> 13:19.800
And now we can move on to the next material to compute the Indian coefficients and we'll do that inside

13:19.890 --> 13:20.840
a function.

13:20.970 --> 13:22.190
Until then enjoy a.
