WEBVTT

00:00.310 --> 00:03.000
Hello and welcome back to the course on deep learning.

00:03.090 --> 00:05.970
So we've learned quite a lot in this section of the course.

00:06.000 --> 00:08.520
Let's summarize what we've talked about.

00:08.520 --> 00:09.900
All right so here we go.

00:10.050 --> 00:16.170
We started with an input image to which we applied multiple different feature detectors or also called

00:16.170 --> 00:18.950
filters to create these feature maps.

00:19.080 --> 00:21.560
And this comprises our convolutional lair.

00:21.570 --> 00:29.070
Then on top of that Warshel there we applied the loo or rectified linear unit to remove any energy or

00:29.070 --> 00:31.980
increase non-linearity in our images.

00:31.980 --> 00:36.910
Then we applied a puling Alair to our kind of delusional there.

00:36.930 --> 00:44.880
So from every single feature map we created a puled feature map and basically the pulling Lehre has

00:44.880 --> 00:45.810
lots of advantages.

00:45.810 --> 00:54.090
The main purpose of the pooling Lair is to make sure that we have a special spatial invariance in our

00:54.270 --> 00:54.630
images.

00:54.630 --> 01:01.830
So basically if something tilts or twists or is a bit different to the ideal scenario then we can still

01:01.830 --> 01:08.520
pick up that feature plus pulling significantly reduces the size of our images are also pooling helps

01:08.970 --> 01:16.290
with avoiding any kind of overfitting of our data or overall model to the data because it just simply

01:16.290 --> 01:18.160
gets rid of a lot of that data.

01:18.390 --> 01:24.270
But at the same time pooling preserves the main features that we're after just because the way instruction

01:24.270 --> 01:26.710
and the pooling were used was Max pooling.

01:26.940 --> 01:35.430
Then we flattened all of the pooled images into one long a vector or a column of all of these values

01:35.490 --> 01:41.030
and we put that into an artificial neural network and that was Step Three flattening and Step four is

01:41.030 --> 01:47.520
a fully connected artificial neural network where all of these features are processed through a network

01:47.550 --> 01:54.720
and then we have this final Lehre final fully collected Lehre which performs the voting towards the

01:54.720 --> 02:01.590
classes that we're after and then all of this is trained through a forward propagation and back propagation

02:01.770 --> 02:02.450
process.

02:02.490 --> 02:09.960
And lots of lots of iterations and a pox and in the end we have a very well defined neural network and

02:10.040 --> 02:10.580
the computer.

02:10.680 --> 02:14.820
Another important thing is not only the weights are trained in artificial neural network part but also

02:15.090 --> 02:22.530
the feature detectors are trained and adjusted in that same and gradient decent process and that allows

02:22.530 --> 02:23.890
us to come up with the best feature maps.

02:23.910 --> 02:31.050
And in the end we get a fully trained convolutional neural network which can recognize images and classify

02:31.050 --> 02:31.650
them.

02:31.710 --> 02:32.330
So there we go.

02:32.350 --> 02:35.450
That's how can the volitional neural networks work.

02:35.710 --> 02:42.150
And now you should be totally comfortable with this concept and ready to proceed to the practical applications.

02:42.270 --> 02:51.280
If you'd like to do some additional reading then there's a great blog by added to disband from 2016.

02:51.390 --> 02:53.340
You can see the link up there at the bottom.

02:53.340 --> 02:58.300
So the blog is called The Nine deep learning papers you need to know about understanding CNN's part

02:58.380 --> 03:04.530
three and this blog actually gives you a short overview of nine different CNN's that have been created

03:04.530 --> 03:10.530
by people like licken and others which you can then go ahead and study further.

03:10.530 --> 03:17.940
So there will be a lot of new things that will be totally new to you and that you will have to get your

03:17.940 --> 03:22.460
head around but just keep this blog in mind are these nine papers in mind.

03:22.470 --> 03:27.090
And even if you're not ready to go through them right now maybe after the practical tutorials maybe

03:27.330 --> 03:33.480
after you do some additional training in the space of deep learning slowly you can then reference these

03:33.480 --> 03:39.510
works and ideally I think you will get a lot of value through looking through other people's neural

03:39.510 --> 03:44.940
networks and how they structured their own relational nets and that will help you understand what are

03:44.940 --> 03:49.950
the best practices and why people did certain things in a certain way and that will help you with your

03:49.950 --> 03:57.240
architecture of neural networks because neural networks and convolutional neural networks are not an

03:57.240 --> 03:57.960
exception.

03:57.960 --> 04:05.610
They are like an architecture challenge you have to come up with a idea and then structure and then

04:05.610 --> 04:11.720
adjust it and tweak it to get the best possible design and the best possible and optimal performance.

04:11.730 --> 04:12.430
So there we go.

04:12.450 --> 04:13.380
That's us for today.

04:13.390 --> 04:16.700
I hope you enjoyed today's tutorial and this whole section and I look for.

04:16.700 --> 04:17.670
See you next time.

04:17.670 --> 04:19.390
Until then enjoy deep learning.
