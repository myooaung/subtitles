WEBVTT

00:00.480 --> 00:02.750
Hello and welcome back to the course in deep learning.

00:02.760 --> 00:06.080
Today we're finally stepped up before full connection.

00:06.270 --> 00:08.140
So what does this step all about.

00:08.370 --> 00:16.940
Well in this step we're adding a whole artificial neural network to our convolutional neural network

00:16.950 --> 00:22.470
so to all of the things that we've done so far which are convolution pooling and flattening.

00:22.470 --> 00:28.940
Now we're adding a whole new age and on the back of that how intense is that.

00:28.950 --> 00:32.550
That is just that is something that is definitely something.

00:32.550 --> 00:36.710
And so here we've got the input layer we've got a fully connected lab.

00:36.720 --> 00:42.540
I'll put there and by the way the fully connected Lehre in the artificial neural networks we used to

00:42.540 --> 00:43.810
call them hidden Lares.

00:44.040 --> 00:49.050
And here we are calling them fully connected because they are hidden lairs but at the same time they're

00:49.080 --> 00:55.030
a more specific type of fiddleheads they're a fully connected layer in artificial neural networks.

00:55.440 --> 00:59.910
Hidden layers don't have to be fully connected whereas in convolutional neural networks we're going

00:59.910 --> 01:05.450
to be using fully connected letters and that's why they're generally called fully connected Lares.

01:05.720 --> 01:11.580
And so basically that whole column or vector of outputs that we have after the flattening we're passing

01:11.580 --> 01:18.090
it into the input learned here we've got a very simplified example just for illustration purposes.

01:18.090 --> 01:25.980
And what's the main purpose of the artificial neural network is is to combine our features into more

01:25.980 --> 01:28.920
attributes that predict the Klaas's even better.

01:28.920 --> 01:37.620
So we already in our vector of outputs in the flattening of the flattened result from what we've already

01:37.620 --> 01:43.710
done we have some features encoded in the numbers in that vector and they can already do probably a

01:43.710 --> 01:51.690
pretty good job at predicting what's what's Clauss we're looking at whether it's a dog or a cat or whether

01:51.690 --> 01:53.800
it's a tumor or not a tumor and so on.

01:53.850 --> 02:00.570
But at the same time we know that we have this structure called artificial neural network which is designed

02:00.570 --> 02:07.770
which which has a purpose of dealing with attributes and coming out or dealing with features and coming

02:07.770 --> 02:16.050
up with new attributes and combining attributes together to even better predict things that we're trying

02:16.050 --> 02:16.640
to predict.

02:16.800 --> 02:20.400
And we know that from the previous part so why not leverage that.

02:20.400 --> 02:22.710
And that's exactly what the plan here is.

02:22.710 --> 02:29.070
So how about we pass on those values into an artificial neural network and let it even further optimize

02:29.070 --> 02:30.580
everything that we're doing.

02:30.600 --> 02:31.830
And so that's what we're going to be doing.

02:31.830 --> 02:36.320
But let's look at a more realistic example because this one is a bit too simple.

02:36.540 --> 02:43.950
So here we've got a better looking artificial neural network where we have five attributes on the inputs

02:43.950 --> 02:51.260
that we have in the first even have six neurons in the second or in the second fully connected Larry

02:51.300 --> 02:55.440
have eight neurons and then we have two outputs one for dog and one for cat.

02:55.560 --> 03:02.190
And so an important thing to talk of for us to talk about here is that why do we have two outputs.

03:02.200 --> 03:09.030
We're kind of used to having only one output in our artificial neural networks Well one output is for

03:09.060 --> 03:13.150
kind of when you're predicting a numerical value when you're pretty.

03:13.150 --> 03:19.800
When you're running a regression type of problem but when you're doing classification you need an output

03:19.870 --> 03:25.020
Proclus except for the exception is when you have just two classes like we have two classes here dog

03:25.020 --> 03:25.440
and cat.

03:25.620 --> 03:31.710
And we could have just done one output and made it a binary output and said One is a dog and zeros a

03:31.710 --> 03:37.260
cat and that would have worked totally fine and actually what you'll see had lunch do that in the practical

03:37.260 --> 03:39.210
tutorials and that's how they will be structured.

03:39.210 --> 03:46.020
But at the same time if you have more than two categories for instance dogs cats and birds then you

03:46.020 --> 03:52.380
have to have a neuron per every category and that's why we're going to practice with two categories

03:52.380 --> 03:58.180
in this example so that we know what to expect if we ever have more than two categories.

03:58.470 --> 03:59.970
And so was going to be happening here.

03:59.970 --> 04:05.220
So we've already done all the groundwork we've done the evolution we've done the pulling and the flattening

04:05.550 --> 04:09.480
and now the information is going to go through the artificial neural network.

04:09.480 --> 04:12.240
So let's have a look at how that all happens.

04:12.300 --> 04:15.150
There is information going through from the very start.

04:15.150 --> 04:22.460
From the moment when the image is processed and convoluted convolved then puled flattened and then through

04:22.480 --> 04:29.160
the artificial neural network all four steps and then a prediction is made and we'll see how this happens.

04:29.160 --> 04:30.690
In a moment we'll be very very interesting.

04:30.690 --> 04:32.870
But for now let's just say a prediction is made.

04:32.880 --> 04:35.910
And for instance 80 percent that is a dog.

04:36.030 --> 04:40.590
But it turns out to be a cat and then an error is calculated.

04:40.590 --> 04:40.980
A.

04:41.160 --> 04:47.690
Well what we used to call a cost function in a artificial neural network and we used the mean squared

04:47.700 --> 04:51.280
error there or in coalman illusional neural networks.

04:51.390 --> 04:58.500
It's called a loss function and we use a cross entropy function for that and we'll talk about cross

04:58.500 --> 04:59.800
entropy and mean squared errors.

05:00.070 --> 05:02.740
In a separate tutorial and how all that happens.

05:02.760 --> 05:08.670
But for now let's just say we have a loss type of function which tells us how well our network is performing

05:08.670 --> 05:13.540
and we're trying to optimize optimize it or minimize that function to optimize on effort.

05:13.720 --> 05:19.410
So the error is calculated and then it's back propagated through the network just like we had in artificial

05:19.410 --> 05:21.520
neural networks is back propagated.

05:21.530 --> 05:28.530
And the some things are adjusted in the network to help optimize the performance and the things that

05:28.530 --> 05:32.410
are just that are as usual the weights in the artificial neural network are part of those.

05:32.450 --> 05:41.070
The blue lines that you see here the syllabuses then also another thing that is adjusted is the feature

05:41.070 --> 05:46.090
detectors so we know that we're looking for features but what if we're looking for the wrong features.

05:46.080 --> 05:51.510
What if this didn't work out because the features are incorrect and so the feature detectors those remember

05:51.510 --> 06:00.300
those little matrices that we had that's the three by three matrices they are adjusted so that maybe

06:00.390 --> 06:01.920
next time it'll be better.

06:01.920 --> 06:03.200
And let's see what happens.

06:03.300 --> 06:03.850
Type of thing.

06:03.850 --> 06:10.980
And but of course it's all done with a lot of science in the background of a lot of math and it's all

06:10.980 --> 06:14.520
done through a great gradient descent with back propagation.

06:14.520 --> 06:20.840
So it's all it's all not just random perturbations it's actually very thought through how it's done.

06:21.150 --> 06:27.570
But nevertheless the feature detectors are adjusted the weights are adjusted and this whole process

06:27.570 --> 06:32.640
happens again and then again the air is back propagate and this keeps going on and on and on.

06:32.730 --> 06:37.890
And that's how our network is optimized that's how our network trains on the data.

06:37.900 --> 06:42.750
So and then the important thing here is that the data goes through the whole area from the very start

06:43.140 --> 06:44.370
to the very end.

06:44.370 --> 06:49.920
Then the error is compared so the error is calculated and then is back propagate.

06:49.920 --> 06:56.460
So same story as with artificial neural networks just a bit longer because of that whole for the first

06:56.460 --> 06:58.280
three steps that we already had.

06:58.980 --> 07:04.380
And now let's have a look at the interesting part the really interesting part how do these two classes

07:04.380 --> 07:10.020
work because Or how do these two output neurons work because before we've always kind of had one output

07:10.020 --> 07:11.780
neuron what happens when we have two.

07:11.790 --> 07:17.440
How does how does this situation of cluster additional images play out.

07:17.610 --> 07:21.940
Well let's start with the top neuron first going to start with the dog.

07:22.020 --> 07:28.890
How do we the main purpose what we need to do first is we need to understand what weights to assign

07:28.890 --> 07:36.480
to all of these Cynapsus that connect to the dog so that we know which of the previous neurons are actually

07:36.840 --> 07:38.870
important for the dog and let's see how that is done.

07:38.870 --> 07:46.430
So let's say hypothetically we've got these numbers in our previous layer of previous fully connected.

07:46.470 --> 07:47.930
In the final fully connected there.

07:48.060 --> 07:50.970
And again these numbers can be absolutely anything.

07:50.970 --> 07:56.430
They don't have to be that they can be any numbers but just for argument's sake we're going to agree

07:56.430 --> 08:01.830
that we are looking specifically at numbers between 0 and 1.

08:02.220 --> 08:09.780
So it's easier for us to argue these things and understand and one means that that neuron was very confident

08:09.780 --> 08:15.900
that it found this feature and zero is going to mean that that neuron didn't find a feature is looking

08:15.900 --> 08:23.270
for so because at the end of the day these neurons are like anything else on this from on this left

08:23.270 --> 08:25.440
side is just looking at features at an image.

08:25.440 --> 08:30.990
This is already very very process but still it's detecting a certain feature or a combination of features

08:31.410 --> 08:36.740
on the image right before we in the convolve step we had kind of recognizable features in the pools

08:36.740 --> 08:41.100
that they're less recognizable then they become even less recognizable in the flattened image and then

08:41.370 --> 08:42.450
they get combine and so on.

08:42.540 --> 08:48.680
But nevertheless this we're talking about here certain features that are present image or their combination.

08:48.680 --> 08:54.360
So and one which has been passed and this is in part has been passed to both the dog and the cat.

08:54.360 --> 08:57.070
At the same time to both the output neurons.

08:57.140 --> 09:06.120
So one means that for us for our argument it means that this neuron has is firing up its its really

09:06.120 --> 09:11.790
rapidly detecting that feature that you know might be an eyebrow it might be detecting this eyebrow

09:11.830 --> 09:13.890
for again for simplicity's sake.

09:13.890 --> 09:19.080
Is it taking this eyebrow and is communicating that to the dog run to the cat neuron saying I can see

09:19.080 --> 09:20.280
my eyebrow I can see my eyebrow.

09:20.280 --> 09:25.230
And then it's up to the dog and the cat neuron to understand what that means for them.

09:25.230 --> 09:25.680
Right.

09:25.830 --> 09:30.760
And so in this case which neurons are firing up these three neurons are firing up the eyebrow and they

09:30.770 --> 09:36.130
say the nose is saying I can see I can see a big nose and I can see floppy ears.

09:36.210 --> 09:40.490
So it and it's saying that to the dog and to the cat and then what the dog.

09:40.530 --> 09:43.350
And then what happens is we know that this is a dog.

09:43.410 --> 09:49.890
So the dog neuron knows that the answer is it is actually a dog because at the end we're comparing to

09:49.890 --> 09:53.580
the picture or the label on the picture and when the dog.

09:53.580 --> 09:56.280
So basically the dog neuron is going to say Aha.

09:56.280 --> 09:58.770
So I should be triggered in this case.

09:58.770 --> 10:04.740
So these are neurons they're telling this signal that they're sending to both to me to the dog and the

10:04.740 --> 10:08.940
cat is actually a indication for me that it is the dog.

10:08.960 --> 10:13.890
And throughout these lots and lots and lots of iterations of this happens many times the dog will learn

10:13.890 --> 10:19.530
that these neurons do indeed fire up when the feature belongs to a dog.

10:19.620 --> 10:24.210
On the other hand the cat neuron will know that it's not a cat and it will know that this feature is

10:24.210 --> 10:28.180
firing up and this neuron is telling me it can see floppy ears floppy ears floppy ears.

10:28.320 --> 10:30.990
But at the same time it's not a cat.

10:30.990 --> 10:35.590
So basically to me that's a signal that I should ignore this neuron.

10:35.760 --> 10:41.910
And the more that happens the more the cat neuron is going to ignore this neuron about the floppy ears.

10:42.390 --> 10:49.040
And so basically that that's how through lots and lots of iterations if this happens often.

10:49.040 --> 10:53.490
So this is just one example but if this happens often maybe a one maybe a a point eight point nine maybe

10:53.490 --> 11:00.750
sometimes it won't fire but overall an average this neuron is lighting up very often when it is indeed

11:00.750 --> 11:05.870
a dog the dog neuron will start attributing higher importance to this neuron.

11:05.880 --> 11:06.530
And so there we go.

11:06.530 --> 11:08.380
That's that's how we're going to signify it.

11:08.400 --> 11:14.520
We can't just say that these three neurons through this iterative process with may have many many many

11:14.520 --> 11:19.890
many samples and many many airports remember so a sample is a row in your data set an epoch is when

11:19.890 --> 11:25.050
you go through your whole dataset again and again and again there are lots and lots of iterations.

11:25.170 --> 11:33.960
This dog neuron learned that this eyebrowed neuron and this big nose neuron and this floppy ear neuron

11:34.290 --> 11:42.990
they all seem to really contribute very well to the classification of what it's looking for and which

11:42.990 --> 11:44.430
is a dog.

11:44.430 --> 11:45.680
So that's how it works.

11:45.690 --> 11:55.080
And again these ears and nose and eyebrows those are very very approximate or like very far fetched

11:55.080 --> 12:01.590
examples because by this stage in this whole convolution conventional neural network it is completely

12:01.590 --> 12:07.350
unrecognizable what they're looking for but at the same time it is something in the features of dogs

12:07.350 --> 12:09.040
or cats or whatever you classify it.

12:09.360 --> 12:11.090
And then so let's move on to the next.

12:11.100 --> 12:15.810
Now we're going to look at the cat neuron but these We're going to remember that these weights are you

12:15.810 --> 12:17.840
know how we've sorted out the dog.

12:17.850 --> 12:22.920
So the dog is kind of like pretty much ignoring all these other neurons one two three four or five but

12:22.920 --> 12:26.460
it's really paying attention to what these three neurons are saying.

12:26.520 --> 12:28.270
Now what is the cat's listening to.

12:28.440 --> 12:30.760
Well whenever it is actually a cat.

12:30.990 --> 12:32.500
Right.

12:32.680 --> 12:35.550
This is this is an example of a situation when it's actually a cat.

12:35.550 --> 12:42.930
So you'll see that this these three neurons 0.9 0.9 and one they're saying something they're saying

12:42.930 --> 12:44.540
something to both the dog and the cat.

12:44.550 --> 12:49.460
And this is again important remember this output signal goes both ways it's the same right.

12:49.470 --> 12:54.810
It's it's saying one to the dog saying one to the cat but then it's up to the dog and the cat to decide

12:55.170 --> 13:00.160
whether to take into account that signal and learn from it or not.

13:00.450 --> 13:05.760
And both the dog and the cat can see that this is a photo I should of put a photo of a cat here but

13:05.760 --> 13:09.980
basically imagine a photo of a cat both a dog and the cat can see that this is actually a cat.

13:10.140 --> 13:20.100
So basically the dog is like OK so these whiskers and these pointy triangle ears and this small size

13:20.380 --> 13:27.180
yes or or maybe the this type you know how cats have these things in their eyes their eyes are like

13:27.750 --> 13:28.240
little.

13:28.270 --> 13:33.300
They're not circles or lines or something like cat eyes.

13:33.300 --> 13:37.410
Basically these cat eyes they're definitely not working for me.

13:37.410 --> 13:43.110
They're not helping me out predict because every time these neurons light up the prediction is not what

13:43.110 --> 13:44.160
I'm looking for.

13:44.190 --> 13:46.850
On the other hand the cat is like hmm that's interesting.

13:46.850 --> 13:51.570
Every time these this one lights up it's more most of the time it lights up.

13:51.570 --> 13:55.260
It matches my expectation it matches what I'm looking for.

13:55.260 --> 13:58.000
OK I'm going to listen to this guy more than this one.

13:58.110 --> 14:02.650
This one same thing every time it lights up or most of the times it lights up.

14:02.760 --> 14:09.050
I happened to get a good I happened to be rewarded for my prediction because I get it right.

14:09.060 --> 14:09.710
It's a cat.

14:09.720 --> 14:10.000
OK.

14:10.020 --> 14:11.380
So I'm going to listen to him more.

14:11.400 --> 14:17.880
You know this one useless to me because he's not actually you know like he's he's not even lighting

14:17.880 --> 14:20.980
up it's a cat but it's he's not lighting up so the opposite is happening.

14:21.000 --> 14:24.360
And this one is well it's a cad but he's not letting up so I'm not going to listen to him.

14:24.360 --> 14:29.500
But this one when he went what is this the eyes the cat eyes light up.

14:29.820 --> 14:30.570
We can see.

14:30.570 --> 14:31.800
I can see that it's a cat.

14:31.800 --> 14:36.390
It matches most of the time so I'm going to learn from that and I'm going to listen to these three guys

14:36.930 --> 14:38.700
more often than not.

14:38.700 --> 14:42.870
And so basically the cat is listening to these three and it's ignoring the other five.

14:43.200 --> 14:53.490
And that is how these final neurons learn which neurons in the final fully connected Lehre to listen

14:53.490 --> 14:58.400
to the output neurons learn which of the fully or which are the final fully connected.

14:58.650 --> 15:05.160
Are neurons to listen to and that's how they understand basically that's how the features are propagated

15:05.160 --> 15:08.900
through the network and conveyed to the output.

15:08.940 --> 15:14.010
And so even though these features of course don't have that much meaning to them like floppy ears or

15:14.010 --> 15:21.480
whiskers at the same time they do have some distinctive they are a distinctive feature of that specific

15:21.480 --> 15:26.850
class and that's how the network is trained because we also during remember during the back propagation

15:26.850 --> 15:32.970
process we also adjust the feature detectors so if a feature is useless to the output it's going to

15:33.420 --> 15:38.690
it's going to probably be disregarded because this doesn't happen to one or two stories has happened

15:38.700 --> 15:40.930
through thousands and thousands of iterations.

15:40.980 --> 15:46.550
So with time a feature that is useless to the network is going to be disregarded replaceable feature

15:46.560 --> 15:52.770
is useful and so at the end of the day in this final layer of neurons you are likely to have lots of

15:53.010 --> 15:59.670
features or combinations of features from the image that are indeed representative or descriptive of

15:59.670 --> 16:01.260
dogs and cats.

16:01.650 --> 16:06.610
And so then once your network is trained up then this is how it's applied.

16:06.610 --> 16:08.500
So this is the next step like we've already trained up on.

16:08.510 --> 16:12.980
So will this happen let's see what happens when the this network is applied.

16:12.980 --> 16:15.560
So let's say we pass on an image of a dog.

16:16.350 --> 16:20.380
The values are propagated through a network we get certain values.

16:20.580 --> 16:26.670
And so this time the dog and the cat neurons don't know they don't have the image of the dog here.

16:26.690 --> 16:32.610
They don't know that it's a dog or a cat they have no idea what it is but they have learned to listen

16:32.910 --> 16:35.310
to what is being shown here.

16:35.310 --> 16:35.630
Right.

16:35.640 --> 16:40.380
They have learned to listen to dog ear and listen to these three neurons the cat neuron listens to these

16:40.380 --> 16:40.740
three.

16:40.890 --> 16:44.790
And so the dog neuron looks at one two three and says aha these are pretty high.

16:44.880 --> 16:50.380
So my probability is going to be high that it's a dog the cat neuron looks at these three and says OK

16:50.440 --> 16:53.610
these this one is pretty high but these are pretty low.

16:53.610 --> 16:54.270
Interesting.

16:54.270 --> 16:56.840
So my probability is going to be 0.05.

16:57.090 --> 16:58.910
And and then and that's.

16:58.910 --> 17:00.060
And that's where you get a prediction.

17:00.060 --> 17:04.460
So then your first choice for this neural network is dog.

17:04.470 --> 17:06.870
Second choice is cat and that's pretty much it.

17:06.870 --> 17:11.620
So the answer is dog and same thing happens when you pass an image of a cat.

17:11.850 --> 17:16.550
You get new values and you can see that even though this one's high these ones are low.

17:16.710 --> 17:20.560
And for the cat This was high this was high and this is a bit low.

17:20.610 --> 17:25.800
So the probability here might not be as great as previously but still you can see that it's a cat of

17:25.800 --> 17:26.700
79 percent.

17:26.910 --> 17:30.200
And so therefore the neural network is going to vote that it's a cat.

17:30.230 --> 17:33.270
And so basically all the neural networks are going to conclude that it's a cat.

17:33.300 --> 17:39.610
Voting is a term that is used for these guys so these neurons in the final fully connected directly

17:39.740 --> 17:42.770
where they get to vote and these are their votes.

17:42.810 --> 17:47.130
And again we are just for argument's sake putting values between 0 and 1 here.

17:47.130 --> 17:54.420
These could be any values but they get to vote and then these weights are the importance of their vote.

17:54.420 --> 18:00.480
So this is these are these purple weights are how the dog neuron views their votes.

18:00.480 --> 18:04.770
How much importance is it assigns to these neurons and to those votes.

18:04.770 --> 18:12.660
And this is how much importance the cat near an Apple assigns to these votes to the votes of these neurons

18:12.670 --> 18:18.450
and so these neurons vote the dog and the cat based on their learned weights they can decide who to

18:18.450 --> 18:23.310
listen to and then they make their predictions and then hold neural network concludes that this is in

18:23.310 --> 18:28.770
this case a cat and then that's then that's your conclusion and that's how you get images like this

18:28.770 --> 18:36.750
where you have a cheetah and then you have a cheetah claws who you know like a high high probability

18:36.760 --> 18:39.920
So this is you know the probability that the network has predicted.

18:40.020 --> 18:44.370
And these are laws but these still exist because they're still kind of like a small chance the other

18:44.370 --> 18:49.650
neurons are also listening to their voters and they're saying oh maybe it's actually a leopard and a

18:49.650 --> 18:50.520
bullet train.

18:50.520 --> 18:51.360
Very very probable.

18:51.380 --> 18:52.380
Here scissors.

18:52.410 --> 18:57.540
You know this one one but hand-glass was very close second and then fivepence stethoscope because you

18:57.540 --> 19:03.930
could see like this guy this this neuron the scissors neuron the output Circes neuron listened to its

19:03.930 --> 19:07.040
voters and it had the predominant vote overall.

19:07.050 --> 19:10.150
But then the hand-glass had a good outcome as well.

19:10.170 --> 19:16.390
So there we go that's how the full connection works and how this is all this all plays out together.

19:16.620 --> 19:18.750
I hope you enjoyed this tutorial.

19:18.750 --> 19:22.830
We're going to summarize all of this in the summary as well and I'll see you next time.

19:22.830 --> 19:24.660
Until then enjoy deep learning.
