WEBVTT

00:00.250 --> 00:04.640
Hello and welcome back to the course on deep learning today we've got a very exciting tutorial ahead.

00:04.640 --> 00:09.530
We're talking about a long a short term memory or elist Yemens for short.

00:09.660 --> 00:10.980
So let's get started.

00:10.980 --> 00:16.540
It's actually going to be quite a saturated trial so we go to our own little plan of attack for today.

00:16.650 --> 00:19.390
Today we're going to first of all look at a bit of history.

00:19.410 --> 00:24.720
So word came from what was the main idea behind it white people invented it and then we're going to

00:24.720 --> 00:27.320
be talking about Ellis the stem architecture.

00:27.360 --> 00:30.580
There's going to be the bulk of our tutorial today so be prepared for that.

00:30.690 --> 00:33.420
And then we're going to have an example walk through.

00:33.440 --> 00:36.090
Hopefully we'll have enough time for that as well.

00:36.090 --> 00:36.630
All right.

00:36.630 --> 00:38.490
So let's get started here.

00:38.490 --> 00:46.170
We've got a problem which we identified in the previous tutorials we talked about the vanishing gradient

00:46.170 --> 00:47.470
problem.

00:47.490 --> 00:53.490
So in short what happens is as we propagate the error through the network it has to go through the unravelled

00:53.670 --> 01:01.180
temporal loop and as it does that it goes through these layers of neurons which are connected to themselves.

01:01.290 --> 01:06.030
These hidden layers which are connected to themselves and they're connected by the means of a always

01:06.030 --> 01:13.200
called the w w w recurrent weight and because the weight is applied many many times on top of itself

01:13.500 --> 01:19.650
that causes the gradient to decline rapidly meaning that the weight of the layers on the very far left

01:19.680 --> 01:25.530
are updated much slower than the waist on the other layers on the far right and this creates a domino

01:25.530 --> 01:32.370
effect because the weights of the far left legs are very important because they dictate the outputs

01:32.370 --> 01:36.840
of those layers which are the inputs to the far right layers and therefore the whole training of the

01:36.840 --> 01:37.890
network suffers.

01:37.890 --> 01:40.300
And that is called The problem of the vanishing gradient.

01:40.440 --> 01:46.750
And as a rule of thumb we could see here that if X is small then we have vanishing greatly and if x

01:46.890 --> 01:49.880
is large then we have exploding gradient.

01:49.950 --> 01:54.870
So now how do you solve this problem we've talked about we talked about a couple of possible solutions

01:54.870 --> 02:00.540
we talked about clipping the gradient or penalize the gradient for exploding gradients.

02:00.540 --> 02:07.710
We talked about smartly selecting the weights or the ex-state networks which we didn't go into detail

02:07.710 --> 02:10.040
on for the vanishing gradient.

02:10.170 --> 02:15.720
And also we talked about the LSD for the vanishing brain just if you separate yourself from all this

02:15.720 --> 02:18.470
information on all this theory and knowledge.

02:18.660 --> 02:20.190
How would you solve a problem like this.

02:20.190 --> 02:22.530
What's the easiest and Fossa solution to it.

02:22.530 --> 02:29.260
So if we have w Reyk is in simple terms less than one then we have vanishment gradient if we have W.

02:29.270 --> 02:35.400
wreck greater than one we've got exploding green What's the first thing that comes to mind to solve

02:35.400 --> 02:36.130
this problem.

02:36.360 --> 02:40.530
Well the first thing comes to mind is to make W Raechel one.

02:40.590 --> 02:43.680
And that's exactly what was done in the Ellis DMAs.

02:43.710 --> 02:49.840
This is a very simplified explanation and there's definitely more to it than just w technicals one.

02:49.860 --> 02:55.760
But in general that's the idea and that's all it took.

02:55.770 --> 03:00.960
And when I saw this for the first time I was so excited that you know the solution is so simple it's

03:00.960 --> 03:06.420
really a genius solution that completely gets rid of the vanishing gradient problem.

03:06.510 --> 03:09.900
And so who are the people behind this.

03:09.900 --> 03:18.000
Here are two gentlemen we've talked about Seppala writer and the second person is Jurgen Schmid Hober

03:18.240 --> 03:26.140
who is who was his supervisor during SEPs research or Ph.D..

03:26.310 --> 03:33.780
And so basically they wrote a paper on it in 1987 about Elliston's and that's when LACMA were introduced

03:34.140 --> 03:35.600
to the world for the first time.

03:35.670 --> 03:36.930
Very exciting topic.

03:36.930 --> 03:39.830
So let's have a look at what they actually are.

03:40.050 --> 03:45.620
So we've got a recurrent neural network right here on raveled temporal loop.

03:45.630 --> 03:47.060
This is what it looks like.

03:47.220 --> 03:54.900
If you dig in inside the recurrent role or network and right here I wanted to do a quick shout out to

03:55.740 --> 03:57.520
Crista for all law.

03:57.600 --> 04:00.490
So here is his blog right here.

04:00.820 --> 04:02.950
A very well-written blog.

04:02.970 --> 04:05.410
Amazing images as you can see.

04:05.460 --> 04:10.590
And we're going to be using images from this blog in our explanation here so thank you very much to

04:11.250 --> 04:20.380
Christopher for making this publicly available and allowing the use of his images in other works.

04:20.400 --> 04:24.120
So we'll definitely reference this at the end of today's tutorial.

04:24.540 --> 04:27.020
And going back to our presentation.

04:27.070 --> 04:33.020
So here we've got the recurrent neural network and this is what it looks like inside.

04:33.030 --> 04:34.330
And this is where the problem lies.

04:34.320 --> 04:41.320
So this operation that happens here is actually a neural network or operation as will see further down

04:41.970 --> 04:50.460
but simply put as you have outputs coming into your module into this module this operation is applied

04:50.490 --> 04:52.980
and then goes into the next module oppression is applied and so on.

04:52.980 --> 04:57.570
So as you apply this approach and when you back propagate it goes through all of this and that's where

04:57.570 --> 04:59.910
the weights are applied that's where the wreck is sitting.

05:00.440 --> 05:05.180
As that weight is applied as it's applied is applied the gradient vanishes vanishes and vanishes and

05:05.540 --> 05:12.830
that means that the weights cannot be updated properly or fast enough to train the network properly

05:12.830 --> 05:18.400
and so the further away you try to look the more problems you have in the more of the bashin gradient

05:18.410 --> 05:19.700
you have.

05:19.700 --> 05:27.230
This is the standard Arnon and this is what the Ellis T.M. version looks like and I know you might be

05:27.230 --> 05:30.210
thinking that well this is super complex.

05:30.460 --> 05:32.050
This There's so much going on here.

05:32.060 --> 05:37.350
And indeed it is a bit more complex than the standard Arnon But check this out.

05:37.370 --> 05:44.630
This is how elist teams are normally displayed in literature and in papers and so on.

05:44.630 --> 05:53.120
So definitely this is the same thing as you saw before previous before but is just a much more convoluted

05:53.120 --> 05:54.880
explanation or representation.

05:54.880 --> 06:01.520
So definitely this option is better and we're going to stick with this one and more so the it looks

06:01.640 --> 06:07.400
it looks difficult now but the goal is that by the end of this tutorial you are completely comfortable

06:07.400 --> 06:09.420
what's going on here and I think that's pretty exciting.

06:09.410 --> 06:13.390
And this even though it may seem a bit complex now.

06:13.850 --> 06:18.400
Towards the end of the trial hopefully you'll be able to navigate LACMA quite well.

06:18.710 --> 06:25.190
And so before we go deep into what is going on here I wanted to highlight the main point so remember

06:25.190 --> 06:27.170
we said Jurek equals 1.

06:27.230 --> 06:33.380
Well that's this line over here that pipeline at the top as you can see there's nothing much not much

06:33.380 --> 06:33.860
going on here.

06:33.860 --> 06:40.460
There's only like two point pointwise operations as well understand for the down and there's no complex

06:42.260 --> 06:47.960
neural network led operations are all brought out to this part and that's this is the essence if you're

06:47.960 --> 06:55.740
going to take away one thing from today's tutorial then this is that you have a memory cell which is

06:55.740 --> 06:56.890
called the memory cell.

06:56.930 --> 07:02.190
I call the memory pipelined Which just goes through time.

07:02.210 --> 07:08.540
And this is as going through time and it can vary freely of flow through time sometimes it might be

07:09.560 --> 07:13.390
removed and erased sometimes things might be added into it.

07:13.550 --> 07:14.560
But that's pretty much it.

07:14.600 --> 07:19.400
Otherwise it flows through times freely and therefore when you back propagate through these Alice jams

07:19.700 --> 07:24.440
you don't have that problem of your vanishing gradient.

07:24.470 --> 07:29.530
That's that's the essence of LSD.

07:29.660 --> 07:35.090
All right so now let's dig in in a bit more detail so we're going to replace these modules on the left

07:35.090 --> 07:35.640
on the right.

07:35.640 --> 07:43.120
We have something more simple is going to replace them with our representation.

07:43.130 --> 07:46.770
So C stands for memory.

07:47.270 --> 07:48.020
Memory.

07:48.050 --> 07:49.280
Yes.

07:49.400 --> 07:53.610
Is your output so you can see your output going out into the world.

07:53.630 --> 07:58.650
And here you got your output going into the next module into the next block.

07:58.760 --> 08:00.730
And then here you've got your input XTi.

08:00.740 --> 08:05.660
So basically this is the output from the previous module which also went into the world but also is

08:05.660 --> 08:06.700
coming here.

08:06.740 --> 08:07.190
So there we go.

08:07.190 --> 08:14.360
So an Ellis T.M. module takes in three inputs and has two outputs.

08:14.390 --> 08:17.500
So City NHT because these are the same.

08:17.530 --> 08:23.290
And the important thing to understand and remember is that everything here is a vector.

08:23.300 --> 08:28.150
So all of these are this is not just one neuron not just one value.

08:28.190 --> 08:32.730
There are lots and lots and lots of values here behind hiding behind this war.

08:32.750 --> 08:37.880
This letter XTi and here as well and here as well and everywhere these are all

08:40.490 --> 08:41.100
layers.

08:41.210 --> 08:45.470
So just remember that and we're going to reference them as vectors because that's pretty much the same

08:45.470 --> 08:45.790
thing.

08:45.790 --> 08:49.370
Just lots of different values in one vector.

08:49.400 --> 08:56.390
Remove that and that will make it will allow you not to go down the wrong path in the intuitive understanding

08:56.390 --> 08:56.630
of this.

08:56.630 --> 08:58.330
Just remember that these are all vectors.

08:58.820 --> 09:00.560
And let's go through the legend.

09:00.560 --> 09:08.990
So we've got vector transfers so any line here is a vector being transferred or kind of moving around

09:08.990 --> 09:10.410
in this in this net.

09:10.430 --> 09:16.400
In this architecture and yes there's just going to reiterate that it is a vector.

09:16.400 --> 09:18.190
Then we got a concatenation here.

09:18.320 --> 09:22.940
So here you can see that there's two lines combining into one.

09:22.940 --> 09:31.010
And important to understand that this is done here just to save space and make it less convoluted than

09:31.010 --> 09:32.330
it possibly could be.

09:32.540 --> 09:38.030
But the way the best way to imagine it is you that these two lines are running in parallel legend not

09:38.030 --> 09:42.680
actually combining concatenation means that you combine these two actions on top of each other but I

09:42.680 --> 09:47.510
think it's even easier to understand if you just think of it as in these there are two pipes running

09:47.510 --> 09:49.040
here but they're running parallel to each other.

09:49.040 --> 09:51.940
You've got one pipe and then it goes here and the second pipe goes here.

09:52.010 --> 09:54.370
Then these same pipes go here and there they touch that.

09:54.370 --> 10:03.130
So basically you have two pipes running in parallel feeding into these neural network layer layer operations.

10:03.260 --> 10:04.310
Then you've got copy.

10:04.430 --> 10:10.220
So where do we have copy you have copy here the memories copies go ahead and just copy it here then

10:10.340 --> 10:12.590
this output is copied over here.

10:12.800 --> 10:14.180
Then you've got pointwise operations.

10:14.180 --> 10:15.580
Now we get to the interesting stuff.

10:15.680 --> 10:18.720
So you've got a couple of pointwise operations here.

10:18.830 --> 10:26.300
That's five of them and the first thing we're going to talk about these ones the x's the Xs are valves

10:26.660 --> 10:30.540
and they all the names this is the forget valve the memory valve and the output vault.

10:30.590 --> 10:35.570
And in literature you will see like letters f v and O.

10:35.990 --> 10:45.380
In actual formulas representing these valves and so a valve looks like this in plumbing and valve looks

10:45.380 --> 10:48.050
like this and we're going to kind of think of it that way as well.

10:48.050 --> 10:54.590
So you've got water or basically something flowing through and then you can either close it or you can

10:54.680 --> 10:56.960
open it or you can close it to some extent.

10:56.960 --> 10:57.650
Same thing here.

10:57.650 --> 11:05.450
So you've got the forget valve is basically controlled by this layer operation and we'll talk about

11:05.450 --> 11:06.590
that in a second.

11:06.740 --> 11:11.710
And based on the output of that based on the decision made here this valve was either closed or open

11:11.720 --> 11:18.770
so if it's open memory flows through freely if it's closed then memory is cut off and therefore it's

11:18.770 --> 11:19.250
not.

11:19.290 --> 11:24.830
Doesn't this notion sort of further and then new memory just is will be added here probably based on

11:24.830 --> 11:30.140
the results here then you've got the memory valve which again is controlled by a Sigma C must and for

11:30.140 --> 11:37.730
the sigmoid activation function that means that that's the activation function used in these lÃ©a operations.

11:37.730 --> 11:46.340
And as the decision is made here this value which is another layer operation which we'll get to in a

11:46.340 --> 11:52.850
second is either added to the memory or is somewhat added to the memory or is not added to the memory

11:52.970 --> 11:56.120
depending on the value that is decided in this valve.

11:56.510 --> 11:59.910
And then again another valve controlled by sigmoid is remember sigmoid.

12:00.150 --> 12:05.390
Why are we using was because they're from 0 to 1 and therefore zero sense for no memory no.

12:05.390 --> 12:07.210
Nothing goes through one stands for something goes through.

12:07.220 --> 12:13.550
And then here you've got them valve which is the forget valve since they are not forgetful this is the

12:13.550 --> 12:15.550
output valve and we'll get to that in a second.

12:15.560 --> 12:19.060
We're pretty much are you going through the whole network already.

12:19.370 --> 12:26.950
So and then we've got the T-shaped kind of like pipe or T-shaped joint.

12:27.320 --> 12:28.880
Well I'll show you where that is.

12:28.880 --> 12:29.960
That is over here.

12:30.140 --> 12:34.250
So where you have memory going through and then you can add additional memories if you would go back

12:34.340 --> 12:39.110
you've got memory flowing through the joint and maybe some additional memory will be added into it maybe

12:39.110 --> 12:42.220
not depending on the ball.

12:42.450 --> 12:48.200
And then you've got the engine operation here that's kind of like mathematical behind it why you want

12:48.200 --> 12:49.990
to be in between minus 1 and 1.

12:50.150 --> 12:53.140
We won't get into details on that but that's another pointwise operation here.

12:53.150 --> 12:59.560
And you have and you've got the neural network layer operations over here.

12:59.640 --> 13:01.430
That's that's their representation.

13:01.430 --> 13:07.670
So basically just think of them as you've got and you've got instead of pointwise points like element

13:07.670 --> 13:09.120
by element of a vector.

13:09.170 --> 13:14.120
You know if you want to multiply a vector by 0 you multiply every element by 0 or by one or by a certain

13:14.120 --> 13:14.560
amount.

13:14.560 --> 13:21.350
Kind of think of a way that's a very simplistic way to think about these pointless operations whereas

13:21.620 --> 13:24.950
these ones are a bit more complex you've got layer coming in.

13:24.950 --> 13:30.020
And then you go on Alair coming out or like a lot because again everything here is a vector so you've

13:30.020 --> 13:37.670
got a lair of the sigmoid coming out controlling the valve for each one of these elements in the vector

13:37.760 --> 13:38.540
of memory.

13:38.540 --> 13:43.940
So there's not just one scene where there's many different and that's why you've got a whole layer coming

13:43.940 --> 13:44.180
in.

13:44.180 --> 13:46.470
And then you've got a lair coming on.

13:46.490 --> 13:48.440
These are layer operations.

13:48.440 --> 13:50.210
So just remember that.

13:50.390 --> 13:56.570
And so we're ready to look into this in in step by step and we're going to very simple because we've

13:56.570 --> 14:04.210
discussed everything already in terms of hard work so we've got a new value coming in.

14:04.250 --> 14:11.310
You've got memory you've got non-memory you've got value coming from previous node and whoops.

14:11.540 --> 14:20.290
And together they are combined to try to decide whether this valve should go ahead or should be closed

14:20.290 --> 14:22.940
or should be open though it was somewhat closed or open.

14:22.940 --> 14:28.400
Then you've got these two again combined together on a combined like they again flow in parallel and

14:28.400 --> 14:32.370
then they're combined in in here or in this operation.

14:32.560 --> 14:37.890
And basically it's just them combined it's like there's lots of layers here lots of layers here one

14:37.950 --> 14:45.470
like lots of neurons here not some neurons here and then all of that is basically in one layer operation

14:45.500 --> 14:49.870
is used to decide what value we're going to pass through.

14:49.940 --> 14:55.640
And then also if that value is going to pass or not and to what extent then here we've got the memory

14:55.640 --> 14:56.660
flowing through.

14:56.660 --> 14:59.630
We've got the forget valve if it's closed or open.

14:59.630 --> 15:05.570
We've got memory valves close door open and we're adding in some memory possibly if we want to update.

15:05.570 --> 15:09.650
So basically we can let this whole memory flow through then keep this from closed keep this and open

15:09.650 --> 15:15.800
the memory and change keep this one or we can keep this one close and keep this one open then we can

15:15.800 --> 15:18.370
update the memory completely.

15:18.440 --> 15:27.500
And here finally we've got these two values combined to decide what part of the memory memory pipeline

15:28.130 --> 15:37.430
is going to be output you become output of this module is it going to go as fully as the output or to

15:37.430 --> 15:38.510
some extent and so on.

15:38.510 --> 15:40.550
So again these decide that.

15:40.820 --> 15:42.230
So that's how it all works.

15:42.230 --> 15:44.880
Pretty straightforward architecture.

15:45.050 --> 15:49.790
Let's have a look at a specific example to understand this a bit better.

15:49.790 --> 15:55.070
So the example we talked about I'm a boy who likes to learn is that it you cheat in translation to check

15:55.460 --> 15:59.840
if this were girl then here would is Sam Holker.

15:59.920 --> 16:06.320
Tara Rodda OK it meaning that these two words not just this word would change but also it would affect

16:06.320 --> 16:10.570
these two words so different to in check rather than in English.

16:10.580 --> 16:16.160
So these words are affected by the gender of the subject and therefore in our list and we might want

16:16.160 --> 16:19.850
to store the subject boy in this case in the memory.

16:19.850 --> 16:27.100
So for instance let's say boys start here and then it's just flowing through freely and nothing is changing.

16:27.110 --> 16:28.060
Like if we get.

16:28.250 --> 16:31.600
If our new information doesn't tell us that there's a new subject.

16:31.610 --> 16:35.300
We just have boy flying through freely and keeps flowing again.

16:35.340 --> 16:42.170
If for instance we have a new subject we have girl or we have we have a name like Amanda or something

16:42.170 --> 16:49.940
else and we understand that we've got a new subject then rules or we'll let this valve will close this

16:49.970 --> 16:55.730
Whalsay you know destroy the memory that we had then open this wall put a new memory and then put the

16:55.730 --> 17:00.210
name here put the subject on just put in the gender one just put in like female.

17:00.280 --> 17:03.170
Well I should put the whole as much information as we can.

17:03.170 --> 17:07.400
For instance this could be the Archita it doesn't have to be like this but as an example we could put

17:07.400 --> 17:13.330
in for instance girl now into this into this pipeline.

17:13.820 --> 17:15.270
And why would we do that.

17:15.290 --> 17:19.640
Well because then from that we can extract different elements of information we can extract that it's

17:19.640 --> 17:23.120
female we can extract that singular right.

17:23.160 --> 17:27.620
So not just that it's there there's additional information the word girl that is singular we can extract

17:27.620 --> 17:29.040
more information we can extract that.

17:29.320 --> 17:35.150
The word girl for instance has four letters were that it was capitalized or wasn't capitalized just

17:35.150 --> 17:42.440
as these are all very very intuitive examples doesn't have to work this way but this is how it could

17:42.440 --> 17:43.070
work.

17:43.100 --> 17:45.550
And so then we have the word girl in here.

17:45.650 --> 17:48.850
And so that's how this world works and this world works.

17:48.860 --> 17:53.660
And so what this evolved does is it extracts certain information from what you have in the memory.

17:53.660 --> 17:59.810
So for instance if we have no girl in here and for the purposes of the current of the next word or next

17:59.810 --> 18:04.250
sentence that's coming up you might need to know like we saw you need to know the gender.

18:04.250 --> 18:13.760
So then this valve will facilitate the extraction of the gender and that will go as an input into your

18:13.760 --> 18:18.000
next module and it'll help the next module like it will be here.

18:18.050 --> 18:27.230
Decide and decide how to deal with the information that's coming its way how to put it into the correct

18:27.620 --> 18:30.300
form for the female gender for example.

18:30.380 --> 18:35.690
And so that's how the output valve works and what it does.

18:35.690 --> 18:40.130
So there we go that's how the long short term memory works.

18:40.130 --> 18:47.660
And I hope this was a quite an intuitive explanation and now you have a bit of a better understanding

18:47.930 --> 18:53.240
what it what elist hams are like and those of additional reading you can definitely reference the original

18:53.240 --> 18:57.950
paper by our two offers who created elist yams.

18:57.950 --> 19:02.450
Alternatively if you don't want to get that deep into mathematics and into the technical stuff there

19:02.460 --> 19:06.840
is a great great blog by Christopher Ola which we've already mentioned.

19:06.860 --> 19:10.890
Great explanation of LSD DMD's I highly recommend to check it out.

19:10.910 --> 19:12.960
There's a bit of mathematics not too heavy.

19:13.220 --> 19:15.230
And there's another blog by.

19:15.680 --> 19:21.990
She yawned and asked him SDM and it's diagrams diagrams are a bit more in-depth.

19:22.010 --> 19:24.470
So they're a bit more.

19:24.780 --> 19:29.600
There's a bit more less space saving but dagger's have been more deft might be easier to understand

19:29.600 --> 19:33.440
in some cases no mathematics whatsoever just plain intuition.

19:33.440 --> 19:37.310
So I also highly recommend this blog to check it out.

19:37.340 --> 19:43.620
If you would like to get additional information on elist hymns and on that note I hope you enjoy this

19:43.620 --> 19:45.640
Statoil and I look forward to your next hour.

19:45.680 --> 19:47.360
Until then enjoy deep learning.
