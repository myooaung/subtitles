WEBVTT

00:00.270 --> 00:08.670
Hello and welcome to this very very important tutorial because this is Hoyo I'm going to tell you which

00:08.760 --> 00:10.510
are the best sources.

00:10.620 --> 00:17.420
You must have every time with you when you are taking this course and there are actually three resources.

00:17.580 --> 00:25.260
The first one is this cheat sheet of the world Noro with all the different steps of the process starting

00:25.260 --> 00:32.460
from the input from the very beginning up to the selection of the action to play in the environment

00:33.150 --> 00:37.230
and the other two resources for you to have all the time.

00:37.230 --> 00:46.830
Are these two websites which are first the world models that get have that Io which is the original

00:46.830 --> 00:55.170
article which is nothing else than the article made by the original authors of the world models who

00:55.170 --> 00:56.360
are first.

00:56.370 --> 01:05.580
David ha from Google brain in Tokyo Japan and Jorgan Schmidt who were from the Swiss company and a sense

01:05.880 --> 01:14.910
which was I think founded by the inventor of the LACMA So two big talents of the AI community.

01:14.970 --> 01:19.640
DAVID And Julian Schmidt who were and I recommend to check them out on the Internet.

01:19.770 --> 01:25.840
They also have fascinating articles and research papers written for AI research.

01:26.160 --> 01:32.130
But the reason why you should have this always with you and which is the same reason why we should be

01:32.130 --> 01:33.350
so grateful for them.

01:33.390 --> 01:40.470
So thank you so much to both David and Joergen Schmidt who were the reasons are that not only they created

01:40.560 --> 01:48.990
an insanely powerful model which is a world model but also they wrote a super clear crystal clear article

01:49.200 --> 01:56.010
on this model which basically explains everything not only the intuition you know in the first paragraph

01:56.010 --> 02:02.250
with this introduction you will get the intuition of the world model with the fact that the reality

02:02.250 --> 02:04.860
we perceive is based on our brains.

02:04.860 --> 02:10.370
Prediction of the future and therefore we all have a different perception of the reality.

02:10.380 --> 02:12.650
So all the intuition is explained here.

02:12.660 --> 02:18.690
Also with this baseball example you know a baseball batter doesn't have time to see the ball coming

02:18.690 --> 02:21.730
to him and to see where he should hit the ball.

02:21.750 --> 02:28.530
But the reason why the better managed to hit the ball is only because his brain predicts exactly where

02:28.530 --> 02:30.370
the ball is going to make contact.

02:30.480 --> 02:36.750
And so that's the intrusion but of course then you'll have the full theory explained quite in great

02:36.750 --> 02:45.520
detail in my opinion because it covers the different parts of the world model including the variational

02:45.560 --> 02:53.210
are in color as we can see here the the model with the different components of the variational are in

02:53.210 --> 03:01.230
color including the encoder which produces the latent vectors Z and then the decoder which recreate

03:01.470 --> 03:08.040
the frame from the latent vectors that and not only they explained this with nice graphics but also

03:08.340 --> 03:15.030
they provide some sort of demo you know interactive demo where you have a random screenshot as input

03:15.330 --> 03:19.380
and the options to randomize it and you get the reconstruction image.

03:19.380 --> 03:26.060
So you know it's not all the time that we get all these well-explained informations for any know.

03:26.070 --> 03:32.160
You know usually only get the research paper but the research paper is usually very advanced you know

03:32.160 --> 03:35.490
addressed to the other experts in the community.

03:35.490 --> 03:42.240
But here when I see this really I have the feeling that it was addressed to everybody which is our value

03:42.240 --> 03:49.170
here as instructors who want to spread the knowledge in the world and democratize artificial intelligence.

03:49.170 --> 03:51.200
So a big thank you for that.

03:51.210 --> 03:59.820
And then of course they cover the end on Arnold morrow and they clearly explain that the role of the

03:59.900 --> 04:08.400
end and which is I remind the mixture density network that is here the role of the MGM is just too narrow

04:08.760 --> 04:16.890
the probability distribution of the latent vectors that at time T plus one with respect to the action

04:17.010 --> 04:20.950
the latent vector and the hidden state at the current time.

04:21.090 --> 04:24.560
So no worries if you don't understand what all this means.

04:24.600 --> 04:29.750
We will explain everything in different steps of course but you'll see with our course.

04:29.760 --> 04:37.020
Plus this article you won't be able to miss any detail so that's why I have to insist.

04:37.020 --> 04:40.290
Make sure to have it open with you when you take the course.

04:40.290 --> 04:46.830
This link world models that get have that IO You just have to type world Norell on Google and this will

04:46.830 --> 04:49.940
definitely be either the first or second link.

04:50.100 --> 04:50.650
All right.

04:50.670 --> 04:57.730
So endian RNA and then of course they explain the controller and they clearly say that the controller

04:57.730 --> 05:05.400
is nothing else than a simple are linear model that maps Zanti to late and vector and H.T. the hidden

05:05.400 --> 05:09.730
state from the Arnon directly to the action.

05:09.930 --> 05:18.150
So basically the action that is played and each time is just the result of this mapping from the controller

05:18.390 --> 05:26.430
which is a simple single layer linear model taking as input to concatenated input vectors that THG Plus

05:26.430 --> 05:27.560
this bias.

05:27.570 --> 05:28.290
All right.

05:28.290 --> 05:36.020
And therefore you have to understand that WC Here is a matrix you know a linear matrix withinside some

05:36.030 --> 05:43.380
weight weights and the goal the next step in the process is to leverage deep neuro evolution because

05:43.380 --> 05:51.720
what deep neuro evolution will do is it will find the best parameters of this weight matrix meaning

05:51.720 --> 05:59.220
the best weight of the weight matrix so that the best actions are played and so that the reward is maximized

05:59.320 --> 06:05.520
as they can see here you know the gold ultimate goal in order to make the car drive as fast as possible

06:05.710 --> 06:12.120
is to maximize the expected cumulative reward of the agent during a rollout of the environment meaning

06:12.360 --> 06:17.490
during a certain number of steps like what I showed you in the British Statoil 900 steps.

06:17.490 --> 06:24.480
All right so the optimizer that will use in the end is to turn the course structure will be exactly

06:24.480 --> 06:31.000
used to figure out the best way out of this weight matrix of the controller.

06:31.230 --> 06:39.040
And that's when the second thing comes into play here is Noriko by the blog Ohtori dot net.

06:39.180 --> 06:43.360
And they made an amazing guide on evolution strategies.

06:43.500 --> 06:46.970
So you absolutely need to read it fully.

06:46.980 --> 06:53.550
We will of course cover it and step 10 but you absolutely need to read this because again the explanations

06:53.580 --> 06:54.490
are amazing.

06:54.560 --> 07:01.050
There are nice graphics that illustrate everything with not only graphics but as you can see some diagrams

07:01.530 --> 07:09.290
and they cover all the evolution strategies techniques from the simple evolution strategy including

07:09.300 --> 07:16.470
also the simple genetic algorithm then the covariance matrix adaptation evolution strategy.

07:16.470 --> 07:25.380
Or C A S which is the one we'll use to optimize our full world model and then also they cover the natural

07:25.380 --> 07:33.030
evolution strategies and I think also the open AI evolution strategy which is also a great one and it

07:33.030 --> 07:36.070
will be provided in the master toolkit.

07:36.390 --> 07:40.590
All right so you must read this as many times as you can.

07:40.650 --> 07:48.300
And so what you need to understand is that this will come into play to optimize the weight of the matrix

07:48.390 --> 07:49.260
of the controller.

07:49.410 --> 07:55.560
And this technique to optimize the weight of MRO to you know play the best actions that will lead to

07:55.560 --> 07:59.740
the highest rewards is called exactly policy gradient.

07:59.910 --> 08:06.990
It's a branch of artificial intelligence as opposed to the other competitive branch which is deep reinforcement

08:06.990 --> 08:07.660
learning.

08:07.830 --> 08:10.890
Well the difference between the two is that there is a gradient.

08:10.980 --> 08:17.910
We'll use some techniques like evolution strategies to figure out the best parameters of a model.

08:17.910 --> 08:24.060
So here Paul is great and we'll figure out the best weight of the control model and then differently

08:24.060 --> 08:31.500
separately have the deep reinforcement learning branch which will use the learning algorithm to figure

08:31.500 --> 08:34.330
out the best actions to play in order to maximize the reward.

08:34.350 --> 08:37.410
But this is absolutely not what we do here.

08:37.470 --> 08:44.250
Here we do policy gradient meaning we're optimizing the weight or the parameters of MRO here to control

08:44.250 --> 08:44.940
them all.

08:45.060 --> 08:50.910
So we won't do any Q learning or we will work within a few values here for the full world model what

08:50.910 --> 08:56.980
we're doing here to optimize the weight of the controller is by leveraging quality gradient.

08:57.090 --> 09:03.720
And of course the state of the art Noro in the policy graden branch of AI which is evolution strategies

09:03.960 --> 09:08.790
and more specifically the covariance matrix application evolution strategy.

09:08.820 --> 09:16.680
So I hope you are now convinced that you should always have it opened with you all of course remember

09:16.740 --> 09:23.580
well knows that you have the IO and Blug later that night and then you can just visually volution strategies

09:23.910 --> 09:30.150
with Otar will you cannot miss it but will also give it to you in the course so that we make sure you

09:30.430 --> 09:31.610
don't miss it.

09:31.650 --> 09:32.240
All right.

09:32.310 --> 09:39.120
And just finished a tutorial I would like to point out the fact that indeed the authors explain that

09:39.390 --> 09:47.880
they optimized the parameters of the Control-C with the covariance matrix adaptation evolutions Reggie

09:48.100 --> 09:52.220
CMAG as which we will implement in the course.

09:52.530 --> 09:55.020
So I can't wait to start all these steps.

09:55.020 --> 09:58.400
This was the last tutorial of this introductory section.

09:58.410 --> 10:04.530
Now it's time to begin our journey with the first step covering artificial neural networks if you already

10:04.530 --> 10:08.040
know our visual neural networks then you can move on to the next step.

10:08.040 --> 10:09.510
Convolutional neural networks.

10:09.620 --> 10:13.800
And if you already know them you can move on to the step after that.

10:13.800 --> 10:15.260
Variational are encouraged.

10:15.270 --> 10:21.990
You have to understand that this course was made for everyone so that everyone can tackle the full world

10:21.990 --> 10:22.600
model.

10:22.710 --> 10:27.830
And that's why we provided everything from scratch with a step by step structure.

10:27.930 --> 10:32.710
So please make sure to navigate your own way depending on what you already know.

10:32.790 --> 10:38.220
And on that note I will also give you the cheat sheet with the full structure so that you can clearly

10:38.220 --> 10:39.880
have it in your mind.

10:40.260 --> 10:42.620
Until then enjoy artificial intelligence.
