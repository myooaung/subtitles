WEBVTT

00:00.690 --> 00:05.730
Hello and welcome back to the artificial intelligence and master class in today's tutorial we're going

00:05.730 --> 00:08.250
to talk about the variational out and coder's.

00:08.310 --> 00:13.690
Now if you've already done the tutorials on normal or in kowtows you will find that variational our

00:13.760 --> 00:16.490
callers are extremely simple.

00:16.590 --> 00:24.660
So let's have a look here we've got our standard audio encoder and as we discussed previously what we're

00:24.660 --> 00:31.500
doing here is we're taking the inputs or the input vector we're mapping it onto this vector is here

00:31.590 --> 00:34.080
which is also called the latent vector.

00:34.110 --> 00:40.320
And from here we are extracting our output so this is called encoding.

00:40.320 --> 00:46.240
And this is called Decoding and this is called the latent vector also called the bottleneck in our callers.

00:46.680 --> 00:54.820
And what this allows us to do is it allows us to compress our inputs and then decompress them more here.

00:54.820 --> 00:58.600
So Autard Coras one use case for an Kodos is compression.

00:58.740 --> 01:03.400
You can compress images audio video files and other files in such a way.

01:04.200 --> 01:11.460
And as we also discussed and our encoder is a self supervised type of deploring algorithm where the

01:11.460 --> 01:18.300
way we train it is we compare our output to the original input and we try to make sure that they match

01:18.300 --> 01:20.070
up as close as possible.

01:20.070 --> 01:23.430
Now that's how it started out in coater what we're going to do here is we're going to make some slight

01:23.430 --> 01:28.830
changes to how this is illustrated just to give us more room on this image so we can move things around

01:28.830 --> 01:30.870
and add some new components.

01:30.870 --> 01:32.250
So there we go.

01:32.280 --> 01:38.490
This is exactly the same Otto and coater is just presented illustrate a different way so we don't have

01:38.490 --> 01:45.630
lots of connections here and lots of nos we just have one arrow on each side just to give us some more

01:45.630 --> 01:45.840
room.

01:45.840 --> 01:49.880
But effectively this is exactly the same thing as we have here.

01:50.130 --> 01:52.010
So this on normal or quarter.

01:52.030 --> 01:58.050
Now what is the difference between a normal on Ann Coulter and a variational.

01:58.360 --> 02:01.820
Well in a variational in color we want to add some took a subsidiary here.

02:01.830 --> 02:05.970
So we want to add some randomness to this latent vector.

02:06.090 --> 02:11.550
And basically what that means is we're not going to hear here what we're doing right now is we're taking

02:11.550 --> 02:17.120
our inputs and we're mapping them onto our lakes and vector in a variational encoder.

02:17.130 --> 02:22.500
We're going to take all inputs and we're going to map them onto a distribution and that will allow us

02:22.500 --> 02:29.940
to have randomness or sagacity in a latent vector and reconstruct many different environments or have

02:29.940 --> 02:33.760
some randomness every time we decodes the spectrum.

02:33.780 --> 02:36.730
So let's have a look how we're going to do that.

02:37.170 --> 02:42.510
All we need to do is rather than mapping straight to the latent vector we're going to first map our

02:42.810 --> 02:47.790
inputs onto two vectors one of them is going to be the mean vector.

02:47.880 --> 02:51.990
So that's the mean of our stand of our distribution.

02:51.990 --> 02:54.150
And then we get to have the standard deviation vector.

02:54.150 --> 02:58.660
So this is going to be these are great to be the standard deviations of our distribution.

02:58.660 --> 03:03.330
The means that here standardization is that here and then from these two vectors we're going to sample

03:03.630 --> 03:07.110
our latent vector every single time.

03:07.110 --> 03:07.840
So this is gold.

03:07.850 --> 03:15.840
The sampled latent vector and as you can see after this is all trained we're going to be able to have

03:15.960 --> 03:24.900
a slightly varying latent back to every single time we generated and every single time we decode allowing

03:24.900 --> 03:28.400
us to achieve that 2:54 that we're looking for.

03:28.410 --> 03:32.980
So that's variational auto encoders in a nutshell very simple concept.

03:32.980 --> 03:40.530
After once you know what how auto encoders work this step towards variational is very simple.

03:40.530 --> 03:47.070
Effectivity instead of mapping onto a layout and back to straight away we're mapping onto a mean vector

03:47.070 --> 03:52.560
to stand division that we're sampling the latent vector and in essence turns out that we are mapping

03:52.560 --> 03:59.280
our inputs onto a distribution from which we sample in order to then proceed with the decoding.

03:59.280 --> 04:01.440
So that's a very shallow kowtows.

04:01.500 --> 04:07.250
In short if you'd like to read more about them you get a bit more of a feel for the intuition.

04:07.430 --> 04:13.560
It is a wonderful article on medium is called intuitive understanding variational out and courters by

04:13.560 --> 04:15.320
your home Shafqat.

04:15.450 --> 04:21.250
And we've got the link here you'll also find the link in the notes for this course.

04:21.380 --> 04:22.360
So go.

04:22.410 --> 04:26.750
Hope you enjoyed today the Tauriel and I look forward to seeing you back here next on.

04:26.760 --> 04:28.640
Until then enjoy AI.
