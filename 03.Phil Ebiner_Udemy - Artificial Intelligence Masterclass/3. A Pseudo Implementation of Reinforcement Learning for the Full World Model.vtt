WEBVTT

00:00.360 --> 00:02.340
Hello and welcome to this new tutorial.

00:02.370 --> 00:06.660
And welcome again to step nine on reinforcement learning.

00:06.660 --> 00:12.420
So now that you've just seen intuition lecture with Carol to understand what reinforcement learning

00:12.420 --> 00:13.230
is about.

00:13.350 --> 00:20.850
Well we will implement some what we call pseudo code so that we can understand where reinforcement learning

00:20.850 --> 00:27.990
comes into play in this full world model meaning we're basically going to implement a very simplified

00:27.990 --> 00:35.410
code that will follow all the steps of the process happening in the full world model from start to finish.

00:35.550 --> 00:40.640
You know from the first observations which are the very first inputs going through the CNN then the

00:40.640 --> 00:46.610
the E which returns the latent vector z which will become the input of the controller.

00:46.770 --> 00:53.220
Also taking the hidden state as a second input then this controller will return the action to go inside

00:53.220 --> 00:59.020
the environment get the reward but also to go back into the MDA and Arnon because remember that the

00:59.130 --> 01:05.280
Indian origin actually takes three inputs the action the previous in state and the latent vector z.

01:05.700 --> 01:09.080
So we will recap all that into a code.

01:09.090 --> 01:16.460
This will not be the real and final code of the full world model which we will execute in the end to

01:16.550 --> 01:18.740
you know race against our A.I..

01:19.020 --> 01:24.990
But it's important to still implement that pseudo code so that we can not only really understand the

01:24.990 --> 01:32.160
process of the full world model but also put what you've learned about reinforcement learning into practice.

01:32.160 --> 01:33.330
All right so let's do this.

01:33.360 --> 01:35.880
And just before we start this.

01:35.880 --> 01:41.700
So the implementation I would just like to remind that the controller you know right here in the full

01:41.700 --> 01:45.720
world model is actually a simple linear matrix.

01:45.720 --> 01:53.280
You know it's a simple single layer linear model which is exactly that matrix here WC which takes obviously

01:53.280 --> 02:00.990
as input that an H that we can see here clearly and which will return as output the action.

02:00.990 --> 02:07.680
And it's really really important that you understand this that the controller is actually a simple matrix

02:07.920 --> 02:15.360
therefore of several weights or several parameters because then in Step 10 we will start to learn about

02:15.600 --> 02:24.090
genetic algorithms and evolution strategies which will exactly optimize the parameters inside this control

02:24.090 --> 02:27.600
matrix in order to maximize the cumulative reward.

02:27.600 --> 02:32.880
And speaking of the cumulative reward well we will include that in the of the code.

02:32.880 --> 02:35.660
You know that's really part of the process here.

02:35.690 --> 02:42.150
It is actually what's happening here you know once the action is played by the controller Well we get

02:42.270 --> 02:48.060
a reward inside the environment you know depending of course on the score that you saw in the very first

02:48.060 --> 02:49.640
tutorial of this course.

02:49.710 --> 02:55.670
Basically the faster the car goes and the further it gets along the road the high will be this court

02:55.710 --> 03:01.500
and therefore the higher will be the cumulative reward along one episode and that reward is important

03:01.500 --> 03:08.790
because that's exactly what we need to optimize and maximize things to the deep neuro evolution strategies

03:09.090 --> 03:09.810
in stepped in.

03:10.020 --> 03:17.220
So we will basically implement in that so the code everything to be ready for stepped in and also of

03:17.220 --> 03:22.080
course to put what you've learned about reinforcement learning into practice.

03:22.080 --> 03:23.280
All right so let's do this.

03:23.280 --> 03:30.750
Let's go back to Python and let's implement the whole full world mode process of course very simplified.

03:31.470 --> 03:31.740
All right.

03:31.740 --> 03:34.170
So we're going to start from the very beginning.

03:34.170 --> 03:39.940
That's why I've put the whole process of the full world neural just next to my code.

03:40.050 --> 03:46.460
And so the very first step is of course when the input frames come into the CNN view.

03:46.530 --> 03:46.770
Right.

03:46.770 --> 03:53.580
Remember that we implemented the C and plus the v e in the same court and we call that a cone.

03:53.610 --> 03:56.660
The E and so that's of course the first step of the process.

03:56.670 --> 04:03.420
But before we start you know this loop that will basically loop over all the iterations happening in

04:03.420 --> 04:09.910
one game you know exactly this game that we and the A.I. played at the very first tutorial of this course.

04:09.960 --> 04:13.950
Well we have to reset everything we have to reset the environment.

04:14.040 --> 04:16.210
We have to get the initial state.

04:16.290 --> 04:18.090
We have to set down to false.

04:18.100 --> 04:21.420
There is always a done and a reinforcement learning environment.

04:21.420 --> 04:23.910
And it says whether the game is over or not.

04:23.940 --> 04:26.390
You know Don equals true means that the game is over.

04:26.520 --> 04:27.670
The race is over.

04:27.750 --> 04:32.750
And then of course false means that the game still continues like we're still racing.

04:32.850 --> 04:37.820
And of course before we start the loop we have to initialize to cumulative reward to zero.

04:37.830 --> 04:38.160
Right.

04:38.160 --> 04:40.020
I told you about the cumulative reward.

04:40.020 --> 04:46.260
It is at the very beginning you know just before the race starts initialized to zero and then at each

04:46.260 --> 04:52.800
iteration we're going to incremented by the reward got at a very specific step of the environment meaning

04:52.800 --> 04:55.700
when the controller plays an action.

04:55.710 --> 05:03.180
All right so let's do all this initialization starting with the environment so usually when you work

05:03.180 --> 05:08.460
with a reinforcement learning environment like the car racing game which is a pre-built an environment

05:08.490 --> 05:10.490
I think by opening a gym.

05:10.590 --> 05:17.520
Well the developers of such environments always implement a reset method which not only resets the environment

05:17.550 --> 05:22.650
but also returns the first image or observation of the game.

05:22.650 --> 05:25.790
So you know that's the very starting point of the race.

05:25.920 --> 05:27.710
And so that's our first app here.

05:27.720 --> 05:33.240
And therefore I'm going to introduce a new variable which we're gonna call ups and that's exactly the

05:33.240 --> 05:41.250
real frame that the agent will see in reality therefore before the dream and so ups will get that first

05:41.370 --> 05:49.110
observation or first input frame by calling the reset method from the environment object of the car

05:49.110 --> 05:49.790
racing game.

05:50.160 --> 05:53.590
And so to call a method from an environment object you know.

05:53.610 --> 05:55.380
And here is an objective.

05:55.380 --> 05:57.230
The car is in game environment.

05:57.270 --> 06:00.420
Well we added that and we call that method.

06:00.420 --> 06:03.150
Reset with some parenthesis.

06:03.150 --> 06:08.180
All right so you will see that all the time playing with opening a gym environments.

06:08.250 --> 06:09.690
Then why do we say.

06:09.720 --> 06:15.690
The second thing we need to initialize here is of course the initial state of the engine art and right.

06:15.690 --> 06:21.970
Remember that in the engine or an engine especially in the iron then we not only have the output set

06:22.230 --> 06:24.810
of the iron and but also the hidden state.

06:24.890 --> 06:27.930
I've prepared that for you actually right here.

06:28.140 --> 06:34.890
Just as a quick reminder right you can see that the iron in takes as input three elements the previous

06:34.910 --> 06:41.120
instate the previous action plate and the previous latent vector z and returns is output.

06:41.160 --> 06:46.520
Well first the deterministic output of the iron in and of course the hidden state.

06:46.530 --> 06:53.940
And what we have to initialize here is the first hidden state which will come into the iron in and remember

06:53.940 --> 06:59.700
that we actually had a parameter for that when we implemented the iron and it was I think called the

06:59.700 --> 07:00.530
initial state.

07:00.930 --> 07:03.900
So that's exactly what we have to initialize right now.

07:04.140 --> 07:06.280
And so we're going to call that H.

07:06.300 --> 07:10.260
And if we go to our on an implementation.

07:10.260 --> 07:17.250
Well that's exactly what we introduced here as an object variable of the iron in this by the way will

07:17.250 --> 07:20.200
be of course part of the real final implementation.

07:20.220 --> 07:21.470
This is just some pseudocode.

07:21.480 --> 07:26.970
But what you have here are the real variables of course of the whole implementation.

07:26.970 --> 07:27.330
All right.

07:27.330 --> 07:31.710
And so back in our reinforcement learning sort of implementation.

07:31.710 --> 07:39.000
Well since the initial state you know this object variable that we created here comes from the build

07:39.000 --> 07:42.030
model method of the engine aren't in class.

07:42.030 --> 07:49.160
Well we will simply call it this way so that everybody can understand we're going to take an M.D. and

07:49.410 --> 07:58.230
are an object of that same engine aren't in class from which we're going to get the initial state object

07:58.230 --> 08:00.450
variable exactly this one.

08:00.450 --> 08:01.190
Right.

08:01.200 --> 08:05.230
It doesn't really matter here at this stage which format take exactly.

08:05.310 --> 08:11.190
But at least this way everybody will understand that this is indeed an object variable the initial state

08:11.490 --> 08:16.100
of the engine on an object of the engine and aren't in class.

08:16.140 --> 08:16.770
All right.

08:16.770 --> 08:25.110
So that initialize is the initial state then very important when you're iterating on an episode or also

08:25.110 --> 08:28.680
called a rollout of a reinforcement learning environment.

08:28.680 --> 08:36.300
You always have to initialize first the done variable which is always one of the four variables in each

08:36.300 --> 08:42.510
and every step of the environment which are let's remind them the current state the action plate the

08:42.510 --> 08:47.330
reward received and this done variable telling whether or not the game is over.

08:47.760 --> 08:52.950
And of course when we starting the game you know because right now we're initializing everything before

08:53.130 --> 09:00.840
we start the big loop of the game or of the race well done we'll be cool of course too false because

09:00.900 --> 09:03.880
the game is about start it is not finished yet.

09:04.140 --> 09:05.410
So done equals false.

09:05.460 --> 09:13.380
And then finally very important as well especially for what we will do in Step Ten to maximize or optimize

09:13.380 --> 09:21.600
the cumulative reward by finding the optimal parameters of the controller you know in that matrix WC

09:21.600 --> 09:22.740
that I've showed you before.

09:23.520 --> 09:25.850
Well compulsory step as well.

09:25.860 --> 09:31.750
We have to initialize a cumulative reward to zero.

09:31.860 --> 09:35.570
And now we can start the loop we can start a whole loop of the game.

09:35.580 --> 09:43.020
So this whole loop will be a single episode of the A.I. racing its car in the environment.

09:43.020 --> 09:50.310
So since we have done very well well the simplest way to implement that loop is with a while you know

09:50.430 --> 09:52.800
well not done.

09:52.800 --> 09:58.030
Meaning while the game is not done well we are not finished racing our cars.

09:58.110 --> 10:00.110
Well what will happen.

10:00.170 --> 10:01.090
Well here.

10:01.100 --> 10:05.420
That's when things start to get interesting because that's from this point.

10:05.450 --> 10:10.700
We will follow executive path of the full world more so we're gonna start from the very beginning from

10:10.700 --> 10:18.290
here when we get the input frames from the game of our car racing in the car racing environment and

10:18.290 --> 10:25.460
then following all this path while not forgetting any of the loops here you know until the final step

10:25.460 --> 10:26.020
of the path.

10:26.030 --> 10:33.370
When we reached the next state in the environment and then here again the process starts again.

10:33.470 --> 10:33.740
All right.

10:33.740 --> 10:41.030
So the very first step is to get the input frame then the input frames goes into the VAB which returns

10:41.150 --> 10:42.680
the latent vectors.

10:42.710 --> 10:50.240
And so what we will simply do here is since we've already initialized the observation meaning we got

10:50.420 --> 10:52.040
the first observation.

10:52.040 --> 10:55.770
Well we can directly get the output of the V.

10:55.820 --> 10:58.050
You know that latent vectors.

10:58.130 --> 11:05.060
And then you know later on in loop we will update the observation once we reach the final step of the

11:05.060 --> 11:05.990
process.

11:06.110 --> 11:09.160
But first we have to get that Z that latent vectors that.

11:09.170 --> 11:16.280
So what I'm gonna get here the next step is to get that latent vector z which will be returned by the

11:16.280 --> 11:16.850
V.

11:17.240 --> 11:22.150
And remember in the V we called R class cone V.

11:22.520 --> 11:29.230
So let's say that the object of this class which we will create later on to use that part of the full

11:29.330 --> 11:31.370
world model with the country.

11:31.400 --> 11:38.570
Well let's say that this object is called C and the AP or county as you prefer.

11:38.570 --> 11:40.290
And so the C and V.

11:40.440 --> 11:44.770
Let's simply write that it takes as input as we can clearly see here.

11:44.780 --> 11:52.010
Well the observation the observation which was initialized in avoid all the ups variable and therefore

11:52.010 --> 11:54.520
here we simply input ups.

11:54.590 --> 11:54.850
All right.

11:54.860 --> 11:58.010
So that's the first step getting the observation.

11:58.010 --> 12:02.380
It goes in to the CNN V which returns the latent vector C..

12:02.450 --> 12:03.190
Great.

12:03.200 --> 12:04.830
And now next step.

12:04.940 --> 12:07.700
What is the next step after we get the latest vector z.

12:08.120 --> 12:15.140
Well very simply the latent vector zis propagated up to the controller which takes it as input but which

12:15.200 --> 12:22.400
also takes the hidden state the hidden state that we already have because we initialized it in the initialization

12:22.400 --> 12:27.980
step and therefore we're gonna get it now with our other input Z.

12:28.160 --> 12:34.580
Therefore we are ready to call the controller and we're ready to get the action returned by the controller

12:34.610 --> 12:36.990
which will go into the environment.

12:37.070 --> 12:46.010
So let's do this let's get our action which we're gonna call a And now let's call our controller which

12:46.100 --> 12:49.350
will of course be created later on in the implementation.

12:49.430 --> 12:58.610
And as we can clearly see on the Oracle Well it takes as input that horizontal input vector z t and

12:58.670 --> 13:04.520
H2 you know the current latent vector z the last one got and the current hidden state H.

13:05.120 --> 13:08.360
So make sure to not confuse that with a product.

13:08.390 --> 13:13.730
This is just a horizontal vector and the reason why I say horizontal vector is because we are doing

13:13.730 --> 13:16.610
here again a matrix multiplication.

13:16.610 --> 13:22.790
You know this is a simple two dimensional matrix which will be multiplied as in matrix multiplication

13:22.790 --> 13:27.150
by this vector composed of Z T and H2.

13:27.290 --> 13:30.570
And so that's exactly what we going to sort of implement right now.

13:30.650 --> 13:37.460
We'll put these two vectors into a concatenated one dimensional input vector as it is said here.

13:37.470 --> 13:43.640
You know you can catch needed input vectors at the HD and therefore to simply write this we're going

13:43.640 --> 13:50.360
to add in some parentheses here that concatenated vector into square brackets and we will just input

13:50.360 --> 13:52.380
here said an H.

13:52.430 --> 13:53.080
Right.

13:53.090 --> 14:00.830
So basically that means that the controller takes that concatenated input vector composed of as we can

14:00.830 --> 14:08.000
clearly see here the latent vectors then and the hidden state age which we both have because we initialized

14:08.120 --> 14:12.160
the hidden state age and we just got the latent vector Zi.

14:12.380 --> 14:17.470
So we ready call the control with those two elements to get the action.

14:17.570 --> 14:17.930
All right.

14:17.930 --> 14:19.670
And that's the next step.

14:19.670 --> 14:21.080
And now let's see.

14:21.080 --> 14:22.210
Next thing we have to do.

14:22.310 --> 14:23.910
Well very interesting.

14:23.930 --> 14:29.660
That's the missing part of the full well model which we haven't covered yet and which is really the

14:29.660 --> 14:36.140
step where reinforcement learning comes into play because that's now that we're going to call our environment.

14:36.140 --> 14:42.740
You know that environment object of decreasing Game class really built by opening a gym because our

14:42.740 --> 14:51.140
environment as for any environment of any games in opening a gym has this method that we call step and

14:51.290 --> 14:53.450
which is exactly the method taking as input.

14:53.450 --> 14:58.430
The action that was just played to return three very important elements.

14:58.430 --> 15:05.520
You know the elements of a step of a reinforcement learning environment which are first the next state.

15:05.560 --> 15:12.460
So the next observation happening in the game and therefore the next input frame to go again into the

15:12.460 --> 15:13.730
full process.

15:13.780 --> 15:19.990
Then the second element is of course the reward the single reward received at the specific iteration

15:19.990 --> 15:24.640
but which will be incremented into the total cumulative reward.

15:24.640 --> 15:29.920
And the third element of course done you know telling us if the game has stopped.

15:29.950 --> 15:36.160
So for the core racing game done will only be equal to true when it's the end of the game you know when

15:36.430 --> 15:40.110
we have reached the 30 seconds but sometimes for some games.

15:40.180 --> 15:43.030
Well the game can finish before all right.

15:43.030 --> 15:49.480
And so that's exactly what we're about as we were about to go this step method from the environment

15:49.540 --> 15:56.380
which I remind once again is a pre-built method by opening a gym environments but it is always called

15:56.620 --> 15:59.380
Step and therefore let us get it.

15:59.380 --> 16:06.250
The step method returns as we said the three essential elements of a reinforcement learning environment

16:06.640 --> 16:13.240
which are first the next date with which we're gonna curl ups because it is simply the next observation

16:13.570 --> 16:20.950
of the game and therefore we're updating our abs variable which will become the new observation basically.

16:20.950 --> 16:29.110
Then of course the reward obtained at that specific iteration and then done telling us whether or not

16:29.110 --> 16:29.740
the game is over.

16:29.740 --> 16:36.030
That is for our case to Congress in game whether or not we have reached the final 30 seconds.

16:36.250 --> 16:36.780
All right.

16:36.790 --> 16:39.910
And so these three essential elements.

16:39.910 --> 16:46.090
Well we will get them by calling our environment that same here but this time we're not going to call

16:46.090 --> 16:47.440
the reset method.

16:47.440 --> 16:52.610
We're going to call as we said the step method which takes as arguments.

16:52.630 --> 17:00.070
Well exactly that action played here by the controller and therefore since we have it here since we

17:00.070 --> 17:06.370
got it by calling the controller with our two elements the latent vector z and the hidden state h Well

17:06.370 --> 17:07.960
we can simply input here.

17:07.960 --> 17:09.910
This last action played a.

17:10.090 --> 17:10.600
All right.

17:10.630 --> 17:11.620
And perfect.

17:11.620 --> 17:18.970
And so now the next step is very natural since we got the reward at that specific iteration Well we

17:18.970 --> 17:24.690
can update the cumulative reward by incrementing it by that reward.

17:24.700 --> 17:27.500
We've just got in that specific iteration.

17:27.700 --> 17:35.970
And so here what we're going to do is updated by incrementing it by that reward just obtained.

17:36.130 --> 17:38.160
And now there is a final step.

17:38.230 --> 17:46.120
We must not forget you know we have initialized the initial state of the MGM Ireland but at no time

17:46.120 --> 17:47.950
have we updated it.

17:48.130 --> 17:53.770
And that while loop you know that loop iterating on the whole game you know the whole race.

17:53.770 --> 18:00.010
And so we have to indeed get the next hidden state which is exactly what's happening here.

18:00.010 --> 18:05.860
You know there is a loop here right after the MGM are and then returns the hidden state.

18:05.980 --> 18:11.890
Well that hidden state is taken back to the engine Arnon along with the last action plate.

18:11.890 --> 18:18.030
That's exactly what we see here in the engine on and part of the article.

18:18.160 --> 18:18.780
Right.

18:18.790 --> 18:24.460
We clearly see here the previous hidden state which comes from the retro loop of the awning which you

18:24.460 --> 18:26.780
saw in the intuition lecture.

18:26.800 --> 18:27.130
Right.

18:27.130 --> 18:34.630
And so what we have to do now the last step before we make this game loop complete is to update the

18:34.630 --> 18:39.610
hidden state age or you know to get the next hidden state and therefore to do it.

18:39.790 --> 18:46.000
Well we're going to call again our engine on an object from the engine on in class which were built

18:46.150 --> 18:46.830
in.

18:46.930 --> 18:48.760
Well step eight.

18:48.790 --> 18:55.510
So we're gonna call that object and very simply we're going to take as input the three inputs of the

18:55.510 --> 18:58.390
engine Arnon which are the previous hidden state.

18:58.420 --> 19:06.190
The last action plate and the previous late in fact Z to return that next hidden state and therefore

19:06.420 --> 19:15.250
to write that simply well we're going to update the hidden state h by calling again R M.D. and on an

19:15.370 --> 19:23.830
object which we already introduce here and then are an object of the MGM Arnon class which will take

19:24.010 --> 19:31.360
as input this time can captain a vector of three elements which are as we've just said the last action

19:31.360 --> 19:32.360
plate right.

19:32.380 --> 19:34.050
That's the last action plate.

19:34.120 --> 19:38.560
Then the last latent vector z which is exactly this one said.

19:38.560 --> 19:45.010
And finally the last hidden state which at the first iteration of this while loop is the initial state

19:45.280 --> 19:50.980
H but then at the second iteration and the next ones it will be of course the previous hidden state

19:51.070 --> 19:54.710
eight of the previous iteration and perfect.

19:54.760 --> 20:00.000
This is how works a reinforcement learning environment for the full well known.
