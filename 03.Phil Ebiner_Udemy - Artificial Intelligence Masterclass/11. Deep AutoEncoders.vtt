WEBVTT

00:00.580 --> 00:05.580
Hello and welcome back to the course on deep learning today we're talking about deep online coders.

00:05.620 --> 00:10.900
In fact this is a quick tutorial to introduce de-powered and coders.

00:10.930 --> 00:12.970
So why is this important.

00:12.970 --> 00:18.640
Because I wanted to point out one thing that you should be aware of stacked up and coders are not the

00:18.640 --> 00:25.260
same thing as deep out and coders and this will become extremely obvious when you see their full picture.

00:25.300 --> 00:26.770
Are you ready for it.

00:26.800 --> 00:28.160
And here we go.

00:28.240 --> 00:30.100
This is a deep orange color.

00:30.220 --> 00:35.140
And right away just because based on the colors and the way we structure discourse you can tell that

00:35.200 --> 00:37.960
this is a blast from the past.

00:37.960 --> 00:44.560
These are restricted Bolsa machines so stacked pre-trained layer by layer rpms.

00:44.590 --> 00:46.830
This is what we have here.

00:46.960 --> 00:49.750
These are this is our stacked art and quarter.

00:49.750 --> 00:55.780
So basically they are beams that are stacked then they're pre-trained layer by layer then they're unrolled

00:55.870 --> 00:57.820
then they're fine tuned with back propagation.

00:57.820 --> 01:03.070
So then you do get directionality and then you in your network and then you have back propagation but

01:03.430 --> 01:10.120
in essence a deep out and coater comes from RBM stacked out on cotters or just normal or Encounter's

01:10.150 --> 01:16.900
stacked a deep out encoder is RPMs stacked on top of each other and then certain things are done with

01:16.900 --> 01:20.980
them in order to achieve a encoding mechanism.

01:20.980 --> 01:21.460
There we go.

01:21.460 --> 01:27.010
Just wanted to make sure you're aware of that and that if you hear one of these terms or the other that

01:27.010 --> 01:32.690
you know exactly what is being referenced and if you'd like to learn more about deep art and cotters

01:33.050 --> 01:40.060
great paper by Jeffrey Henthorn and others called reducing the dimensionality of data with neural networks.

01:40.060 --> 01:45.040
So check it out if you'd like to learn more about deep oughtnt coders and remember that they're based

01:45.040 --> 01:46.230
on rpms.

01:46.540 --> 01:48.400
And I look forward to seeing you next time.

01:48.400 --> 01:50.140
Until then enjoy learning.
