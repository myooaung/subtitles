WEBVTT

00:00.650 --> 00:05.510
Hello and welcome back to the course on deep learning in today's tutorial we're talking about gradient

00:05.540 --> 00:06.620
descent.

00:06.830 --> 00:13.580
What we learned previously was that in order for a neural network to learn what needs to happen is back

00:13.580 --> 00:20.420
propagation and that is when the error the difference or the sum of squared differences between the

00:20.420 --> 00:28.460
y hat and Y is back propagated through the neural network and the weights are adjusted accordingly.

00:28.460 --> 00:34.300
So we saw that and today we're going to learn exactly how these weights are adjusted.

00:34.340 --> 00:35.870
So let's have a look.

00:36.020 --> 00:44.000
This is our very simple version of a neural network a perception or a single live feed forward a neural

00:44.000 --> 00:44.810
network.

00:44.810 --> 00:52.370
And what we can see here is this whole process in action where we've got some input value then we've

00:52.370 --> 00:57.020
got a wait then a activation function is applied.

00:57.020 --> 01:01.790
We have we get y hat and then we compare it to the actual value we calculate the cost function.

01:01.790 --> 01:05.360
So how can we minimize the cost function.

01:05.360 --> 01:07.310
What can we do about it.

01:07.310 --> 01:14.690
Well one approach to do it is a brute force approach where we just take all lots of different possible

01:14.690 --> 01:20.930
weights and look at them and see which one looks best and what we do is for instance we would try out

01:21.020 --> 01:26.990
for let's say for example a thousand weights and we'd try the out would get something like this for

01:26.990 --> 01:33.380
the cost function and this is a chart of the Y axis of cross-functional the vertical axis on the horizontal

01:33.380 --> 01:39.170
axis of y hat because you can see the formulas Y had minutes y a square.

01:39.170 --> 01:46.150
This is what the cost function would look something like that and basically you'd find the best one

01:46.150 --> 01:47.810
is over here.

01:47.890 --> 01:50.820
So very simple very intuitive approach.

01:50.920 --> 01:53.140
Why not do this brute force method.

01:53.140 --> 02:01.570
Why not just try out a thousand different cost Chungs thousand different parameters or inputs for weights

02:01.630 --> 02:02.980
and see which one works best.

02:02.980 --> 02:04.180
You'll find the best one that way.

02:04.360 --> 02:10.240
Well if you have just one way to optimize this might work but as you increase the number of weights

02:10.420 --> 02:16.600
increase the number of Cynapsus in your network you have to face the curse of dimensionality.

02:16.600 --> 02:19.280
And so what is the curse of dimensionality.

02:19.390 --> 02:24.570
The best way to describe this or explain it is to just look at a practical example.

02:24.580 --> 02:30.430
So remember this example we had when we we're talking about how neural networks actually work where

02:30.430 --> 02:37.060
we were building or running in your own network for a property evaluation.

02:37.060 --> 02:40.600
So this is what it looked like when it was trained up already.

02:40.720 --> 02:45.940
Well when it's not trained before it's trained before we know which one or the weights it's the actual

02:45.940 --> 02:47.560
neural network looks like this.

02:47.670 --> 02:47.880
Right.

02:47.890 --> 02:55.160
Because we have all these different possible scenarios and we still have to train up the weights.

02:55.210 --> 03:00.970
And here we have a total of 25 weights so 4:46 at the start plus five more from the Himalayas.

03:01.020 --> 03:03.360
But Buttler 25 weights total.

03:03.640 --> 03:09.160
And let's see how we could possibly brute force 25 ways.

03:09.180 --> 03:14.380
It's a very simple neural network right here very simple just one hidden lair.

03:14.620 --> 03:21.100
And how could we brute force our way through a neural network of this size.

03:21.280 --> 03:24.320
Well there's some simple mathematical calculations.

03:24.370 --> 03:25.860
We have 25 weights.

03:25.870 --> 03:30.340
So that means if we have a thousand combinations that are going to solve for every weight the total

03:30.340 --> 03:33.680
number of combinations is 1000 to the power of 25 or 1000.

03:33.760 --> 03:37.750
We tend to parse any five different combinations.

03:37.750 --> 03:48.220
Now let's see how Sun the way to tohu light the world's Fosse's supercomputer as of June 2016 what how

03:48.220 --> 03:56.700
would it approach this problem right so Sunway tie who light it looks like this is a whole huge building

03:56.710 --> 04:05.140
pretty much for this one supercomputer and it got the Guinness World Record for being the Fosses supercomputer.

04:05.140 --> 04:12.550
Right now it is the fastest supercomputer in the world and Sunway type hilite can operate at a speed

04:12.550 --> 04:15.360
of 93 Petrof flops.

04:15.460 --> 04:19.820
Flop stands for floating operation per second.

04:19.930 --> 04:28.030
So it can do 93 to the power or oil times 10 to the power of 15 floating operations per second.

04:28.030 --> 04:32.270
That's how quick it is in comparison.

04:32.410 --> 04:34.110
Average computers right now.

04:34.470 --> 04:38.170
They do like just over several gigaflops and so on.

04:38.170 --> 04:44.310
So it's like kind of those ranges way less than type 1 way type of light.

04:44.350 --> 04:52.930
So suddenly all light is in the forefront of technology and let's say hypothetically that it can do

04:52.930 --> 04:54.570
one.

04:55.060 --> 04:59.700
One test one combination for on your own network in all one flop.

04:59.740 --> 05:05.290
Basically want one floating operation that is not possible that is not practical because you need multiple

05:05.290 --> 05:09.410
floating operations to test out a single weight in you and you're only two.

05:09.430 --> 05:11.200
But even Let's let's give it a head start.

05:11.200 --> 05:17.920
Let's say that it can do it in a ideal world it can do that in one floating operation it can do one

05:18.250 --> 05:19.840
test per one Floria operation.

05:20.080 --> 05:28.390
That means it will still require tend to Parvis any five divided by ninety three times ten or 15 seconds

05:28.600 --> 05:33.940
to come to run all of those tests to brute force through that network.

05:34.090 --> 05:36.900
So that means one or approximate sense of power.

05:36.920 --> 05:38.000
58 seconds.

05:38.020 --> 05:42.040
And that is the same as tend to the power of 50 years.

05:42.100 --> 05:49.840
That is a huge number that is longer than the universe has existed and that is definitely not going

05:49.840 --> 05:59.070
to simply this number is so huge is just definitely not going to work for us at all in our optimization.

05:59.080 --> 06:00.070
So there we go.

06:00.070 --> 06:01.150
This is a no no.

06:01.150 --> 06:05.370
Even on the world's fastest supercomputer Sunway tail light.

06:05.380 --> 06:10.140
So we have to come up with a different approach how are we going to find the optimal weight.

06:10.270 --> 06:15.850
By the way this our neural network was very simple what about if the neural networks looks like something

06:15.850 --> 06:17.010
like this.

06:17.170 --> 06:22.350
Or even greater than that then yeah its just not going to happen at all ever.

06:22.690 --> 06:28.250
So the method were going to be looking at is called gradient descent and you may have heard of it already.

06:28.510 --> 06:30.670
If not we will find out what it is right now.

06:30.670 --> 06:41.710
So there is our cost function and now we go into see how we can foster for some of a faster way to find

06:41.790 --> 06:43.130
the best option.

06:43.140 --> 06:45.870
So let's say we start somewhere you've got to start somewhere.

06:45.870 --> 06:47.190
So we start over there.

06:47.340 --> 06:56.940
And from that point in the top left what we're going to do is we're going to look at the angle of our

06:56.940 --> 06:57.930
cost function.

06:58.020 --> 07:02.070
At that point so is going to basically that's what's called a gradient because you have to differentiate.

07:02.100 --> 07:04.140
We're not going to look at the mathematical equations.

07:04.200 --> 07:09.560
We will provide some tips on additional reading at the end of the next lecture.

07:09.690 --> 07:17.100
But basically you just need to differentiate find out what the slope is in that specific point and find

07:17.100 --> 07:19.330
out if the slope is positive or negative.

07:19.370 --> 07:26.010
If if the slope is negative like in this case means that you're going downhill so to the right is downhill

07:26.010 --> 07:27.300
to the left is uphill.

07:27.300 --> 07:29.730
And from there it means you need to go right.

07:29.730 --> 07:31.480
Basically you need to go downhill.

07:31.620 --> 07:33.020
And that's what we're going to do.

07:33.040 --> 07:35.370
Boom takes a step right.

07:35.430 --> 07:37.400
The ball rolls down again.

07:37.410 --> 07:38.220
Same thing.

07:38.310 --> 07:43.530
You calculate the slope and the slope is positive meaning writer's uphill left is downhill.

07:43.530 --> 07:51.270
And you need to go left and you're all the ball down and again you calculate the slope and you're all

07:51.270 --> 07:52.760
the ball right.

07:53.110 --> 07:53.490
There you go.

07:53.500 --> 08:03.000
So that's how you find in in simple terms that's how you find the best weights the best situation that

08:03.000 --> 08:04.500
minimizes your cost function.

08:04.530 --> 08:08.940
Of course it's not going to be like a ball rolling is going to be a very zigzag type of approach but

08:09.150 --> 08:14.910
it's easier to remember kind of it is more fun to look at it as a ball rolling.

08:14.910 --> 08:19.600
But in reality yes you just is going to be like a step by step approach is going to be as the exact

08:19.680 --> 08:21.870
type of method.

08:22.020 --> 08:24.970
Yeah and also there's there's lots of other elements to it.

08:24.990 --> 08:35.040
There's things like for instance why like why does it go down why does it not go our way over the line

08:35.040 --> 08:40.410
so it could have jumped out of this gone upwards instead of downwards and things like that.

08:40.410 --> 08:41.910
So there are parameters that you can tweak.

08:41.910 --> 08:45.510
And again we will mention where you can find out more on that.

08:45.510 --> 08:51.060
And plus we'll have this in a practical application but in the simplest intuitive approach this is what

08:51.060 --> 08:51.730
is happening.

08:51.750 --> 08:57.720
We are getting to the bottom by just understanding which way we need to go instead of brute forcing

08:57.720 --> 09:02.870
through thousands and thousands and millions and billions and quadrillions of combinations.

09:02.970 --> 09:09.810
We can just simply every time have a look at where is where which way is it sloping so right like you're

09:09.810 --> 09:11.630
or you imagine you're standing on a hill.

09:11.640 --> 09:15.810
Which way does it feel that it's going downwards and whichever way it is going down and you just keep

09:15.810 --> 09:20.700
walking that way you like take 50 steps away and then you assess again OK which way is it going downwards

09:21.030 --> 09:21.420
this way.

09:21.440 --> 09:24.470
OK and I'll take 50 cents or less take 40 steps that way.

09:24.630 --> 09:28.460
So it gets less and less and less as you get closer.

09:28.470 --> 09:32.670
So here's an example of gradient descent applied in a two dimensional space.

09:32.670 --> 09:36.270
So that was a one dimensional example.

09:36.510 --> 09:40.180
Here we have a two dimensional space for the gradient descent.

09:40.260 --> 09:44.850
As you can see it's getting closer to the minimum and it's also called gradient descent because you're

09:44.850 --> 09:49.540
descending into the minimum of the cost function.

09:49.650 --> 09:53.340
And finally he has a gradient descent applied in three dimensions.

09:53.370 --> 09:58.680
This is what it looks like if you projected onto two dimensions you can see zigzagging its way into

09:58.680 --> 09:59.540
the minimum.

09:59.640 --> 10:01.940
So there you go that was gradient descent.

10:01.940 --> 10:03.780
Next Tauriel We'll talk about stochastic.

10:03.780 --> 10:06.710
Gradient descent is really a continuation of this tutorial.

10:06.960 --> 10:08.580
And I look forward to seeing you there.

10:08.670 --> 10:09.680
Until next time.

10:09.690 --> 10:10.570
Enjoyed deep learning.
