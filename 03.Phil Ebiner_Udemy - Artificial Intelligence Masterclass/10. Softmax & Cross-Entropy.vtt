WEBVTT

00:00.300 --> 00:06.450
Hello and welcome back to the course on deep learning this is an additional tutorial to talk about the

00:06.450 --> 00:08.590
soft and cross entropy functions.

00:08.610 --> 00:15.270
It is not 100 percent necessary in order for you to go through all of the parts that we've been through

00:15.300 --> 00:21.450
in the main part of this section where we're talking about the convolutional neural networks but at

00:21.450 --> 00:26.520
the same time I thought it would be a good addition to your bag of knowledge and skill set.

00:26.520 --> 00:30.510
So let's go ahead and dig into these functions.

00:30.780 --> 00:37.470
So to start off with what we have here is the conclusion of a neural network that we built in the main

00:37.470 --> 00:44.220
part of the section and then at the end it pops out some probabilities for zero point ninety five for

00:44.220 --> 00:47.850
a dog and 0.05 5 or 5 percent for a cat.

00:48.000 --> 00:53.190
Given that photo on the left as an input This is after the train has been conducted this is actually

00:53.190 --> 00:57.150
it's running and it's classifying a certain image.

00:57.300 --> 01:00.810
And so the question here is how can these two values add up to one.

01:00.840 --> 01:06.690
Because as far as we know from everything I learned about artificial neural networks there is nothing

01:06.690 --> 01:11.560
to say that these two final neurons are connected between each other.

01:11.670 --> 01:16.560
So how would they know what the value of the hold each one of them know what the value of the other

01:16.560 --> 01:17.250
one is.

01:17.340 --> 01:20.080
And how would they know to add their values up to one.

01:20.280 --> 01:22.200
Well the answer is they wouldn't.

01:22.200 --> 01:28.440
In the classic version of our artificial neural network and the only way that they do is because we

01:28.650 --> 01:33.900
introduce a special function called the soft max function in order to help us out of the situation.

01:33.900 --> 01:40.830
So normally what would happen is the dog and the cat neurons would have any kind of real values.

01:41.430 --> 01:44.870
They don't have to be they don't have to add up to one.

01:45.120 --> 01:51.840
But then we would apply the soft max function which is written up over there at the top and that would

01:51.840 --> 01:54.340
bring these values to be between 0 and 1.

01:54.360 --> 01:56.160
And it would make them add up to 1.

01:56.340 --> 02:03.210
And to quote We can pedia the soft next function or the normalized exponential function is a generalization

02:03.210 --> 02:10.260
of the logistic function that quote unquote squashes aka dimensional vector of arbitrary real values

02:10.260 --> 02:15.290
to a k dimensional vector of real values in the range of zero to one that add up to 1.

02:15.290 --> 02:17.590
So basically it does exactly what we want.

02:17.640 --> 02:22.810
It brings these values to be between 0 and 1 and make sure that they add up to 1.

02:22.890 --> 02:27.990
And the way it works is that the way that this is possible is that because at the bottom here you can

02:27.990 --> 02:29.940
see that there is a summation.

02:29.940 --> 02:37.800
So it takes the exponent and puts it in the power of Zed and adds it up so that one's that two across

02:37.830 --> 02:39.740
all of your classes all of these values.

02:39.930 --> 02:43.820
And so there that's your normalization happening right there.

02:44.340 --> 02:51.240
So that's how the Saucebox function works and it makes sense to introduce the soft next function into

02:51.540 --> 02:59.430
convolutional neural networks because how strange would it be if you had a possible classes of a dog

02:59.430 --> 03:05.090
and a cat and for the dog class you had probability of 80 percent.

03:05.100 --> 03:08.500
And for the cat class you had a good 45 percent right.

03:08.610 --> 03:11.370
It just doesn't make sense like that.

03:11.370 --> 03:15.690
And therefore it's much better when you introduce the soft max function and that's what you will find

03:15.990 --> 03:19.700
happening most of the time in convolutional in your own networks.

03:19.710 --> 03:25.950
Now the other thing is that the soft max function comes hand-in-hand with something called the Cross

03:26.040 --> 03:28.980
entropy function and it's a very handy thing for us.

03:28.990 --> 03:30.590
So let's first look at the formula.

03:30.600 --> 03:33.020
This is what the cross entry function looks like.

03:33.060 --> 03:38.670
We're actually going to be using a different calculation we're going to be using this representation

03:38.670 --> 03:40.620
of the cross entry but the results are basically the same.

03:40.620 --> 03:42.440
This is just easier to calculate.

03:42.510 --> 03:49.200
And what I know this might sound very related to anything right now just formulas on your screen but

03:49.920 --> 03:54.270
there'll be some additional re recommended reading at the end of this section so don't worry if you're

03:54.540 --> 03:59.760
not picking up on the math like if even if we haven't explained the math right now but the point here

03:59.760 --> 04:06.120
is that what is across entropy well across entropy function remember how we previously in artificial

04:06.120 --> 04:14.580
neural networks we had a function called the mean squared arrow function which we used as the cost function

04:14.820 --> 04:17.700
for assessing our natural performance.

04:17.700 --> 04:23.690
And our goal was to minimize the MSCE in order to optimize our network performance.

04:23.880 --> 04:30.780
Well that was our cost function then there there and in convolutional neural networks you can still

04:30.780 --> 04:37.890
use MSE but a better option in convolutional neural networks after you apply the soft max function turns

04:37.890 --> 04:44.210
out to be the across entropy function and in convolutional neural networks when you apply across century

04:44.220 --> 04:48.040
functions not cost called the cost function anymore is called the last function.

04:48.270 --> 04:49.390
And they are very similar.

04:49.410 --> 04:51.780
They're just a little terminological differences.

04:52.190 --> 04:55.470
And like little bit different in what they mean.

04:55.470 --> 04:58.370
But for all purposes it's pretty much the same thing.

04:58.390 --> 05:07.500
And what happens is the last function is again something that we want to minimize in order to maximize

05:07.500 --> 05:09.620
the performance of our network.

05:09.630 --> 05:15.210
So let's have a look at a quick example on how of how this function can be applied.

05:15.210 --> 05:19.220
So let's say we've put an image of a dog into our network.

05:19.590 --> 05:26.100
The predicted value for dog is 0.9 and this is doing the training so we know that we know the label

05:26.100 --> 05:27.220
that is a dog.

05:27.270 --> 05:32.240
So the predictive value 0.9 predicts value for a cat is 0.1.

05:32.340 --> 05:37.770
Then here we have the label so we know it's a dog because this is training and 0 1 for dogs or for cat.

05:37.920 --> 05:47.560
And so in this case you need to use you need to plug these numbers into your formula for the cross entropy.

05:47.760 --> 05:53.250
So how you do it is the values on the left going to the verbal cue.

05:53.370 --> 05:58.860
The one that is under the logarithm in the on the right side and the values from the right would go

05:58.860 --> 06:04.290
into P and so it's important to remember which one goes there because if you get them wrong you don't

06:04.290 --> 06:09.560
want to be taking a logarithm from me from zero value and or going from one.

06:09.570 --> 06:11.620
So you just want to plug them in.

06:11.670 --> 06:14.470
Make sure you plug them into the correct places.

06:14.790 --> 06:16.980
And then you basically add that up.

06:16.980 --> 06:21.960
So that's how the cross entry works and we'll look at a actually right now we're just going to look

06:21.960 --> 06:27.870
at a specific step by step example of applying this function in real life and Ill kind of make make

06:27.870 --> 06:32.310
more sense what Cross entropy is and it'll be less like that.

06:32.310 --> 06:39.230
My goal in this story is to make you more comfortable of cross century because it can sound very convoluted

06:39.270 --> 06:43.780
and no pun intended it can.

06:43.800 --> 06:45.790
Like convolutional neural networks.

06:46.080 --> 06:50.770
It can sound very complex and scary but it's not.

06:50.820 --> 06:51.600
That's that's the point.

06:51.600 --> 06:54.050
So let's go ahead and apply it just so we know that it's not scary.

06:54.090 --> 07:00.880
So here's neural net and also this will explain why we're doing this why we looking into different cause

07:00.930 --> 07:01.630
functions.

07:01.740 --> 07:06.600
So neural network one neural network let's say we have two neural networks and then we pass an image

07:06.600 --> 07:11.960
of a dog and we know that this is a dog and not a cat.

07:12.150 --> 07:15.050
And then we have another image over cat.

07:15.180 --> 07:21.240
This time an animal and it's a cat not a dog and here we have a we are looking at a hole which is in

07:21.240 --> 07:22.430
fact a dog and not a cat.

07:22.430 --> 07:24.230
If you look very closely.

07:24.270 --> 07:29.730
So we want to see what our neural networks were will predict in the first case neural network one 90

07:29.730 --> 07:37.850
percent dog 10 percent cat correct no network number to 60 percent dog 40 percent cat still correct.

07:37.880 --> 07:40.230
Worse but correct.

07:40.230 --> 07:45.990
Second option first neural network 10 percent cat dog 90 percent cat.

07:45.990 --> 07:47.240
Correct.

07:47.250 --> 07:53.270
You know that number to 30 percent dog 70 percent cat worse but still correct.

07:53.520 --> 07:59.820
And then finally neural network one in an image three year old network won 40 percent dog 60 percent

07:59.820 --> 08:08.220
cat incorrect neural network number to 10 percent dog and 90 percent cat incorrect and worse.

08:08.220 --> 08:15.330
So the key here is that even though both networks got it wrong in the last one through all three images

08:15.570 --> 08:18.810
neural network one was outperforming neural network.

08:18.840 --> 08:26.970
So even in the last case it was very it had it it gave dog like a 40 percent chance as opposed to neural

08:26.970 --> 08:32.280
network to only give dog a 10 percent chance or neural network one is outperforming across the board

08:33.150 --> 08:35.100
when compared to neural network.

08:35.390 --> 08:41.730
And so now we're going to look at the functions that they can measure performance that we've kind of

08:41.730 --> 08:42.720
talked about.

08:42.990 --> 08:48.040
So let's put these into a table so there's neural network 1 you have the wrong number.

08:48.300 --> 08:49.390
So that's the image number.

08:49.500 --> 08:54.060
And then for image one you have what's it predicted 90 percent dog Timpson cat.

08:54.060 --> 09:00.510
So there's that hat variables and then you have the actual value so dog correct cat incorrect.

09:00.510 --> 09:07.410
Same thing for image number two and same thing for a minimum of three and same for neural network number

09:07.410 --> 09:07.650
two.

09:07.650 --> 09:11.000
So Dog 60 percent kept 40 percent in the first image.

09:11.000 --> 09:12.070
That's what it predicted.

09:12.090 --> 09:13.740
Correct answer was dog not a cat.

09:13.770 --> 09:18.000
And so on and so now let's see what errors we can actually get.

09:18.000 --> 09:24.890
So what errors we can calculate to estimate the performance and monitor the performance of our networks.

09:24.900 --> 09:33.000
So one type of error is called the classification error and that is basically just asking it did you

09:33.000 --> 09:33.940
get it right or not.

09:33.960 --> 09:36.890
Regardless of the probabilities is just DID YOU GET IT RIGHT.

09:36.900 --> 09:37.920
Or did you get it right.

09:37.920 --> 09:44.740
So in both cases for both neural networks each of them they got one.

09:44.760 --> 09:46.290
So this is how they got wrong.

09:46.290 --> 09:48.410
So they got one out of three wrong.

09:48.420 --> 09:54.900
So 33 percent error rate for neural network 1 and 30 percent error rate for neural network.

09:55.050 --> 09:59.670
As a baseline from this standpoint both neural networks perform at the same level but we know that's

09:59.670 --> 10:00.210
not true.

10:00.210 --> 10:04.520
We know that neural network 1 is outperforming neural network.

10:05.070 --> 10:10.800
That's why a classification error is not a good measure especially for the purposes of back propagation

10:11.760 --> 10:17.910
mean square error different and by the way I did these calculations in Excel I just didn't want to bore

10:17.910 --> 10:21.960
you with them but you can totally just sit down and do them on a paper or in Excel.

10:21.960 --> 10:28.710
These are very straightforward calculations is basically take the sum of squared errors and then just

10:28.710 --> 10:34.960
take the average across your observations and that's pretty much it.

10:35.010 --> 10:43.760
So for the neural network 1 you get 25 percent for neural network 2 you get 71 percent error rates so

10:43.850 --> 10:45.880
as you can see this one is more accurate.

10:45.890 --> 10:50.340
It's telling us that nearly one has a much lower error rate than your own network.

10:51.110 --> 10:52.920
And then cross entropy again.

10:52.940 --> 10:53.810
We've seen the formula.

10:53.810 --> 10:54.690
You can also calculate.

10:54.690 --> 10:59.780
This is actually even easier to calculate the mean squared error Cross area cross entropy gives you

11:00.050 --> 11:05.330
38 percent for neural network 1 and 1.0 6 for neural network 2.

11:05.450 --> 11:07.950
So you can see the results are a bit different.

11:08.270 --> 11:16.460
When you look at them like that when you look at you know the miniskirt area and cross entropy and the

11:16.460 --> 11:26.300
question of why would you use cross entropy over means squared error isn't just about the kind of like

11:26.300 --> 11:31.990
the numbers that they spit but this these calculations were just to show you that this is all it's all

11:32.000 --> 11:34.620
doable you can just do it on a paper it's it's not.

11:34.730 --> 11:41.090
It is not very intense mathematics these are pretty pretty simple straightforward things.

11:41.150 --> 11:47.210
But the question of why would you use means cause entropy over means squirt air is a very very good

11:47.210 --> 11:48.200
question to ask.

11:48.200 --> 11:49.840
I'm glad you asked it.

11:50.420 --> 12:00.440
The answer to that is like there's several advantages of cross entropy over mean squared error which

12:00.440 --> 12:01.400
are not obvious.

12:01.400 --> 12:07.100
And so I'll I'll mention a couple but then I'll I'll let you know where you can find out more.

12:07.100 --> 12:18.470
So one of them is that if you for instance your at the very start of your back propagation your output

12:18.470 --> 12:22.200
value is very very very very tiny very tiny.

12:22.310 --> 12:25.630
So it's much smaller than the actual value that you want.

12:25.700 --> 12:32.870
Then at the very start the gradient in your great and decent bill will be very very low and you won't

12:32.870 --> 12:33.800
be enough.

12:33.820 --> 12:40.490
It'll be very hard for the neural network to actually start doing something and start moving around

12:40.490 --> 12:44.950
and start adjusting those weights and start move start actually moving in the right direction.

12:45.080 --> 12:50.840
Whereas when you use something like the cross entropy because it's got that logarithm in it it actually

12:51.350 --> 12:57.260
helps the network assess even a small area like that and do something about it.

12:57.260 --> 12:58.450
Here's how to think about it.

12:58.450 --> 13:03.210
So let's say in again this is very in and in very intuitive approach.

13:03.360 --> 13:08.780
There's going to be a link to the mathematics and you can derive these things through the mathematics

13:08.780 --> 13:11.180
in more detail but a very intuitive approach.

13:11.180 --> 13:17.480
Let's say your like your outcome that you want is is one.

13:17.660 --> 13:22.760
And right now you are at one one millionth of one.

13:22.790 --> 13:27.740
There is zero point 0 0 0 0 0 1 and then you improve.

13:27.740 --> 13:32.600
Next time you improve your outcome from from one millionth to one thousandth.

13:32.780 --> 13:39.260
And in terms of if you calculate the squared error you just subtracting one from the other.

13:39.530 --> 13:44.900
Or basically in each case you're Kalka in a square and you'll see that the squared errors when you compare

13:44.900 --> 13:48.140
one case versus other it didn't change that much.

13:48.150 --> 13:50.300
You didn't improve your network that much.

13:50.300 --> 13:51.870
When you looked at the mean square there.

13:52.070 --> 13:58.710
But if you're looking at the cross entropy because you're taking a logarithm and then you're comparing

13:58.710 --> 14:01.060
that to dividing one to the other.

14:01.340 --> 14:09.360
You will see that you have actually improved your network significantly so that that jump from one million

14:09.410 --> 14:12.760
to 1000 in mean squared error terms will be very low.

14:12.770 --> 14:15.650
It will be insignificant and it won't.

14:15.740 --> 14:22.060
It won't guide your gradient boosting process or your back propagation in the right direction.

14:22.170 --> 14:27.680
It will ill will guided in the right direction but it'll be like a very slow guidance it won't have

14:27.680 --> 14:29.280
enough power.

14:29.570 --> 14:35.450
Whereas if you do recross entropy across entropy will understand that even though these are very small

14:35.450 --> 14:42.830
adjustments that are just you know making a tiny change in absolute terms in relative terms it's a huge

14:42.830 --> 14:46.040
improvement and we are definitely going in the right direction.

14:46.040 --> 14:54.770
Let's keep going that way so across entropy will help your neural network get to the right gets to the

14:54.770 --> 14:56.650
optimal state.

14:56.810 --> 15:01.040
It's a better way for the neural network to get to get into an optimal state.

15:01.040 --> 15:08.210
But bear in mind that this only works when it across entropy is the preferred method only for classification.

15:08.210 --> 15:14.150
So if you're talking about things like regression like which we had in artificial neural networks then

15:14.180 --> 15:20.710
you would rather go with me and squared error whereas cross entropy is better for classification.

15:20.720 --> 15:25.610
Again it has to do with the fact that we are using soft next function so that's a kind of intuitive

15:25.610 --> 15:26.860
explanation of that.

15:26.930 --> 15:32.120
A good place to learn a bit more about that if you're really interested in you know why are you using

15:32.550 --> 15:35.150
cross entry versus mean squared error.

15:35.150 --> 15:43.170
Google a video by Geoffrey Hinton called the soft max output function and he explains it well and you

15:43.170 --> 15:48.710
know being the godfather of deep learning who can explain it better anyway.

15:48.840 --> 15:51.630
And by the way any video by Geoffrey Hinton is golden.

15:51.630 --> 15:55.540
He's just got a huge talent for explaining things anyway.

15:55.560 --> 16:01.260
So that's that soft nice versus cross and I hope that gives you kind of like an intuitive understanding

16:01.260 --> 16:02.060
of what's going on here.

16:02.070 --> 16:07.980
But more importantly that you're not put off by the term cross entropy because headline will mention

16:07.980 --> 16:11.140
it in the practical tutorials and I wanted to make sure that you're prepared for that.

16:11.240 --> 16:15.690
And it's just another way of calculating your last function.

16:15.690 --> 16:21.780
And another way of optimizing your network which is specifically tailored to classification problems

16:21.810 --> 16:27.880
and therefore convolutional neural networks and comes in hand-in-hand with the soft max function.

16:28.230 --> 16:35.430
So additional reading if you'd like a light introduction into cross entropy if you're interested in

16:35.430 --> 16:37.090
the concentrate a bit more of course.

16:37.160 --> 16:45.270
A good article to check out is called a friendly introduction to cross entropy loss by d Pietro 2016

16:45.300 --> 16:47.090
Here's the link below.

16:47.100 --> 16:54.290
Very very nice very soft and nothing no super complex math.

16:54.360 --> 16:59.610
Good analogies good examples use analogies of cars and you look at cars and talks about information

16:59.610 --> 17:04.310
and bits and restrictions and you know how would you decode this how you go that it's.

17:04.340 --> 17:09.990
So it's a good article to have a look at and we'll give you a good overview of a cross entry like from

17:09.990 --> 17:15.750
an introductory standpoint if you want to dig into the heavy math like what you see here.

17:15.870 --> 17:22.980
Then check out an article by or a blog by how to implement a neural network Intermezzo too so in terms

17:22.980 --> 17:30.660
of like is like an intermediary thing like a intermittency in you know like when you go to a theater

17:30.690 --> 17:36.150
and you have like a break between the first part and the second part.

17:36.280 --> 17:40.770
So because he's like going through all these steps and then he's like and then he says I got to explain

17:40.770 --> 17:42.340
this first.

17:42.420 --> 17:44.020
And yes so that's why it's called intermezzo.

17:44.040 --> 17:51.570
No other reason as far as I understand the articles by Peter Rolands 2016 as well so both are quite

17:51.570 --> 17:52.410
recent.

17:52.540 --> 18:00.830
And check out this if you'd like to dig into the mathematics behind Kross entropy behind the soft and

18:00.830 --> 18:02.540
cross entropy in this article actually.

18:02.850 --> 18:03.740
So there we go.

18:03.810 --> 18:07.310
That's all there is to these two.

18:07.320 --> 18:12.770
Hopefully I was able to add some additional clarity and good luck with that.

18:12.780 --> 18:16.920
It's going to be fun and enjoy the practical tutorials.

18:16.920 --> 18:18.030
I'll see you next time.

18:18.030 --> 18:19.650
Until then enjoy deep learning.
