WEBVTT
1
00:00:05.250 --> 00:00:07.140
Welcome back everyone in this lecture.

2
00:00:07.140 --> 00:00:10.130
We're going to discuss classification metrics.

3
00:00:10.560 --> 00:00:15.330
We just learned that after a machine learning process is complete we all use performance metrics to

4
00:00:15.330 --> 00:00:17.960
evaluate how well our model actually did.

5
00:00:18.030 --> 00:00:20.650
Let's discuss classification metrics in more detail.

6
00:00:21.900 --> 00:00:27.390
The key classification metrics we need to understand our accuracy recall precision and if one score

7
00:00:27.750 --> 00:00:33.700
since we're going to be using Python and carers to report those metrics back the first we should understand

8
00:00:33.700 --> 00:00:37.830
the reasoning behind these metrics and how they actually perform and work in the real world.

9
00:00:39.010 --> 00:00:43.390
Typically in any classification task your model can really only achieve two results.

10
00:00:43.480 --> 00:00:50.510
It's either correct and it's prediction or it's incorrect it's prediction Fortunately incorrect first

11
00:00:50.550 --> 00:00:51.170
correct.

12
00:00:51.210 --> 00:00:56.930
Expensive situations where you have multiple classes such as many types of hand-written digits.

13
00:00:56.940 --> 00:01:01.930
But for the purposes of explaining these metrics we're going to imagine a binary classification situation

14
00:01:02.250 --> 00:01:07.080
so binary meaning just two classes and we're actually going to perform this binary classification task

15
00:01:07.620 --> 00:01:10.610
towards the end of the section of the course.

16
00:01:10.840 --> 00:01:15.670
So in our example we're going to be attempting to predict if an image is a dog or cat.

17
00:01:15.910 --> 00:01:22.000
Since this is a supervised learning process we're going to first fit or train a model on the training

18
00:01:22.000 --> 00:01:25.120
data then we'll test the model on the testing data.

19
00:01:25.390 --> 00:01:30.930
Once we have the model predictions from the X test data we compare it to the true y values.

20
00:01:30.940 --> 00:01:33.170
So again we take our original images.

21
00:01:33.180 --> 00:01:36.040
So those are all the label images of dogs and cats.

22
00:01:36.160 --> 00:01:41.390
We split it took about 30 percent into a test set and 70 percent into a training set.

23
00:01:41.440 --> 00:01:45.370
We're going to use the training set and the model is going to learn what features it should be looking

24
00:01:45.370 --> 00:01:49.050
at when viewing an image to detect whether it's a dog or a cat.

25
00:01:49.210 --> 00:01:55.180
Then we're going to feed it the images of the test data and then see how it predicts on the test data

26
00:01:55.330 --> 00:02:00.450
and compare that to the labels that we already know on the test data.

27
00:02:00.460 --> 00:02:05.140
So let's actually visualize this let's imagine that we just finished training our model on the training

28
00:02:05.140 --> 00:02:06.420
data.

29
00:02:06.610 --> 00:02:11.950
So we're going to do is we're going to take a test image from our X test essentially just a raw image.

30
00:02:11.950 --> 00:02:14.950
And right now we're not actually going to tell the model what the label should be.

31
00:02:14.950 --> 00:02:18.250
The model is going to try to predict that on its own.

32
00:02:18.540 --> 00:02:21.800
So we already have the correct label from the Y test.

33
00:02:21.900 --> 00:02:23.670
So the corresponding heart label is image.

34
00:02:23.690 --> 00:02:27.470
But again we're not actually going to tell us model what the correct label is.

35
00:02:27.520 --> 00:02:32.530
Instead we're just going to feed in the image and then it's going to spit out its prediction.

36
00:02:32.550 --> 00:02:33.730
In this case it was correct.

37
00:02:33.870 --> 00:02:38.790
It predicted that the image was a dog than what we do in order to evaluate our model.

38
00:02:38.940 --> 00:02:44.160
If we just compare the correct label from my test to the predicted label that the trade model output

39
00:02:44.320 --> 00:02:48.970
and this case we just check is dog equal to dog and compare that prediction correctly label.

40
00:02:49.200 --> 00:02:51.020
So the train model that correct.

41
00:02:51.060 --> 00:02:57.910
In this case however it is possible the train model may be misclassified this image as a cat.

42
00:02:57.970 --> 00:03:03.430
So if the model says oh I predict that this is a cat we again take that prediction compare it to the

43
00:03:03.430 --> 00:03:04.190
correct label.

44
00:03:04.390 --> 00:03:06.130
And then we can see here that it was incorrect.

45
00:03:06.220 --> 00:03:10.240
And essentially what we're doing is we're just summing up instances of when it was correct versus when

46
00:03:10.240 --> 00:03:17.180
it was incorrect so we repeat this process for all the images in our X test data and at the end we're

47
00:03:17.180 --> 00:03:21.050
going to have a count of correct matches and accounts of incorrect Massad matches.

48
00:03:21.380 --> 00:03:26.870
And the key realization here we need to make is that in the real world not all incorrect or correct

49
00:03:26.870 --> 00:03:31.910
matches hold equal value specifically in certain situations where you have really high stakes such as

50
00:03:32.240 --> 00:03:34.930
classifying an image to see if it's a tumor or not.

51
00:03:35.060 --> 00:03:38.950
And that's actually a common application of image classification.

52
00:03:39.110 --> 00:03:44.990
You feed it pictures of moles and then you label some as cancerous and not cancerous and you see if

53
00:03:44.990 --> 00:03:49.970
your machine learning model can accurate predict given a new image of someone a new mole on their skin

54
00:03:50.240 --> 00:03:51.890
whether or not it's cancerous.

55
00:03:51.890 --> 00:03:57.200
So we can see here that a certain values of an incorrect match for the correct match are going to hold

56
00:03:57.200 --> 00:04:02.000
higher stakes because we don't want to accidentally tell someone that their mole is OK when it's actually

57
00:04:02.000 --> 00:04:03.110
cancerous.

58
00:04:03.590 --> 00:04:08.150
So that's where we come up with different metrics to view the different relationships between correct

59
00:04:08.150 --> 00:04:10.170
versus incorrect labeling.

60
00:04:10.460 --> 00:04:16.190
But first let's view our actual metrics understand what they represent and then we'll see them and how

61
00:04:16.190 --> 00:04:17.930
they apply to a confusion matrix.

62
00:04:17.930 --> 00:04:23.280
So first I was going to review the four metrics that we mentioned the first metric we mentioned was

63
00:04:23.370 --> 00:04:29.730
accuracy and accuracy and classification problems is a number of correct predictions made by the model

64
00:04:30.120 --> 00:04:33.920
divided by the total number of predictions.

65
00:04:34.040 --> 00:04:40.300
So for example if the X test set was a hundred images in our model correctly predicted 80 images and

66
00:04:40.310 --> 00:04:44.830
we have 80 out of 100 correctly predicted images divided by all the images.

67
00:04:44.870 --> 00:04:47.930
So that's 0.8 or 80 percent accuracy.

68
00:04:49.410 --> 00:04:55.590
And accuracy is useful when the target classes are well balanced so well balance would mean in our example

69
00:04:55.830 --> 00:04:59.360
we have roughly the same amount of cat images as we have dog images.

70
00:05:00.780 --> 00:05:04.500
Now accuracy is not a good choice with unbalanced classes.

71
00:05:04.530 --> 00:05:09.990
So let's imagine a situation where we had ninety nine images of dogs and only one image of a cat and

72
00:05:09.990 --> 00:05:11.510
that was our test data set.

73
00:05:11.880 --> 00:05:15.430
Our model could simply be a very simple model that always outputs.

74
00:05:15.450 --> 00:05:19.290
I think this is a dog regardless of what picture or information it sees.

75
00:05:19.530 --> 00:05:24.430
If that was the case with this particular dataset if we were to perform the accuracy calculation.

76
00:05:24.660 --> 00:05:26.360
Well there's only one instance of a cat.

77
00:05:26.370 --> 00:05:29.700
So there's only one possible time that the model could be wrong.

78
00:05:29.760 --> 00:05:34.970
So even if it was a really dumb model that just said I think this is a dog that it would still get ninety

79
00:05:34.970 --> 00:05:39.530
nine percent accuracy on this particular unbalanced test dataset.

80
00:05:39.540 --> 00:05:43.310
So that's why it's important the accuracy itself is not the complete story.

81
00:05:43.380 --> 00:05:48.600
We have to look at other metrics like precision and recall in order to understand how it's performing.

82
00:05:48.600 --> 00:05:51.670
On the other unbalanced class.

83
00:05:51.780 --> 00:05:59.340
So again what we want to do is understand recall and precision not just accuracy recall is the ability

84
00:05:59.340 --> 00:06:02.810
of a model to find all the relevant cases within that data set.

85
00:06:03.090 --> 00:06:08.580
So the precise definition the best medical definition of recall is the number of true positives divided

86
00:06:08.580 --> 00:06:11.970
by the number of true positives plus the number of false negatives.

87
00:06:12.210 --> 00:06:16.140
And again the stability of a model to find all the relevant cases of Finot dataset

88
00:06:19.170 --> 00:06:25.080
precision is the ability of a classification model to identify only the relevant data points.

89
00:06:25.290 --> 00:06:30.150
So the way we calculate this is precision is defined as the number of true positives divided by the

90
00:06:30.150 --> 00:06:37.260
number of true positives plus the number of false positives so often you have a tradeoff between recall

91
00:06:37.380 --> 00:06:43.800
and precision while recall expresses the ability to find all relevant instances in a data set precision

92
00:06:43.860 --> 00:06:49.380
expresses the proportion of the data points our model says was relevant versus when they were actually

93
00:06:49.380 --> 00:06:50.540
were relevant.

94
00:06:52.270 --> 00:06:55.520
And EF 1 score is a way we can actually combine and report back.

95
00:06:55.540 --> 00:06:57.810
This relationship between precision and recall.

96
00:06:58.060 --> 00:07:03.220
So in cases where we want to find some sort of optimal blend of precision and recall we can combine

97
00:07:03.220 --> 00:07:10.440
the two metrics using what is called the EF 1 score the one score is the harmonic mean of precision

98
00:07:10.440 --> 00:07:14.350
and recall taking both metrics into account and the following equation.

99
00:07:14.430 --> 00:07:18.270
So see if one score is equal to two times position times recall.

100
00:07:18.290 --> 00:07:23.450
Divide by precision plus recall notice we're not just taking the average of the EF 1 score.

101
00:07:23.460 --> 00:07:29.540
And the reason we take this kind of more specific harmonic mean instead of just a normal mean is because

102
00:07:29.660 --> 00:07:34.280
a simple average wouldn't work because it wouldn't punish extreme values.

103
00:07:34.280 --> 00:07:38.670
So what I mean by that is let's imagine we create a classifier that has a perfect precision of one point

104
00:07:38.670 --> 00:07:39.220
zero.

105
00:07:39.410 --> 00:07:44.750
However the recall is horrible and it actually has 0.0 recall that would mean if you were just to take

106
00:07:44.750 --> 00:07:47.540
a simple average it would equal 0.5.

107
00:07:47.540 --> 00:07:53.320
But if you apply that the 1.0 precision and the recall of 0.0 to the harmonic mean then you're going

108
00:07:53.320 --> 00:07:57.210
to have one time zero the top essentially making your EF 1 score 0.

109
00:07:57.230 --> 00:08:01.760
So it's going to punish a very unbalanced recall or a very unbalanced precision.

110
00:08:02.360 --> 00:08:07.910
And I really think the confusion matrix helps people understand recall precision because accuracy on

111
00:08:07.910 --> 00:08:09.370
its own it's pretty straightforward.

112
00:08:09.380 --> 00:08:14.600
Understand but until you view a confusion matrix then you kind of don't really get precision or recall

113
00:08:14.660 --> 00:08:20.620
in my opinion so let's review a confusion matrix for a situation where we're performing classification.

114
00:08:20.670 --> 00:08:23.070
So this is what a confusion matrix looks like.

115
00:08:23.070 --> 00:08:28.140
We have some sort of condition or trying to check for maybe we're trying to check to see if a mole is

116
00:08:28.140 --> 00:08:30.830
cancers or not using the image that it's fed into the model.

117
00:08:30.900 --> 00:08:34.300
Or maybe it's a simple condition whether or not an image is a dog.

118
00:08:34.350 --> 00:08:39.360
So we have these true conditions on the over here on the left hand side of condition positive condition

119
00:08:39.360 --> 00:08:40.010
negative.

120
00:08:40.110 --> 00:08:44.340
And then we have the predicted condition and this will eventually lead you to have true positives true

121
00:08:44.340 --> 00:08:46.850
negatives and false positives and false negatives.

122
00:08:46.890 --> 00:08:50.830
Otherwise known as type 1 errors and type 2 errors specifically in the medical field.

123
00:08:50.850 --> 00:08:56.240
That's a really common way of specifying that particular error and using those for values of true positive

124
00:08:56.250 --> 00:08:58.840
false negative false positive and negative.

125
00:08:58.890 --> 00:09:01.980
You can calculate a wide variety of statistics.

126
00:09:01.980 --> 00:09:04.960
So if you expand this you can check out that we have accuracy.

127
00:09:05.100 --> 00:09:10.070
We have our phos discovery rate negative predictive value recall and so on.

128
00:09:10.080 --> 00:09:14.190
So there's tons of the metrics can get off this confusion make tricks but we're really only concerned

129
00:09:14.190 --> 00:09:16.650
with precision recall and accuracy.

130
00:09:16.680 --> 00:09:20.100
And then once we have precision recall we can then calculate the EF 1 score.

131
00:09:21.390 --> 00:09:25.620
So the main point to remember here is that with the confusion matrix and the various calculated metrics

132
00:09:25.940 --> 00:09:30.690
is that they're all fundamentally different ways of comparing the predicted values versus the true values

133
00:09:31.020 --> 00:09:35.280
and what constitutes good metrics such as What's a good precision or what's a good recall or what's

134
00:09:35.280 --> 00:09:38.510
a good accuracy really depends on the specific situation.

135
00:09:39.930 --> 00:09:42.420
So let's look at a much simpler confusion matrix.

136
00:09:42.420 --> 00:09:45.810
And I think this will also help us understand precision recalled better.

137
00:09:45.840 --> 00:09:52.920
So let's imagine that we're testing for a disease and here we have 165 total patients and out of 165

138
00:09:53.370 --> 00:09:57.470
maybe we're taking images of their moles and seeing whether or not they have cancer.

139
00:09:57.540 --> 00:10:02.830
And for these specific 165 patients we actually already know whether or not they have cancer.

140
00:10:02.850 --> 00:10:05.520
We're training our model and testing the results.

141
00:10:06.350 --> 00:10:13.700
So out of these 165 patients if we're testing for Zeese actually some of them don't have the disease

142
00:10:13.790 --> 00:10:18.560
and then some of them do have disease and then we're going to run those images through a model to have

143
00:10:18.560 --> 00:10:22.520
them predict or have the model predict whether or not these patients have the disease.

144
00:10:22.520 --> 00:10:24.100
So no.

145
00:10:24.140 --> 00:10:26.600
Or a negative test is going to be called a false or zero.

146
00:10:26.630 --> 00:10:29.710
And yes a positive test is going to be equal to true or one.

147
00:10:29.960 --> 00:10:36.860
So we have a little confusion matrix here and it is the result of it being run on some test data.

148
00:10:36.870 --> 00:10:42.630
So the basic terminology we have here are true positives true negatives false positives and false negatives.

149
00:10:42.630 --> 00:10:48.720
So again we have this test data of sixty five patients and then we ran their images of let's say their

150
00:10:48.720 --> 00:10:54.120
moles on our actual training model and then we reported back whether or not we think they have the cancer

151
00:10:54.120 --> 00:10:55.650
or some sort of condition.

152
00:10:55.650 --> 00:10:57.410
So here are two negatives.

153
00:10:57.420 --> 00:11:00.290
That means that the patient did not have cancer.

154
00:11:00.330 --> 00:11:04.120
And our model predicted that they do not have cancer otherwise a true negative.

155
00:11:04.320 --> 00:11:09.600
So true negatives here we have equal to 50 then we have our false positives.

156
00:11:09.660 --> 00:11:12.590
That means that the patient doesn't actually have cancer.

157
00:11:12.750 --> 00:11:17.280
However the model when it saw the image of the mole predicted that the patient did have cancer.

158
00:11:17.280 --> 00:11:18.710
So that's a false positive.

159
00:11:18.720 --> 00:11:22.320
It's falsely saying that I think it's positive for cancer.

160
00:11:22.650 --> 00:11:27.870
Then we have the patience that actually do have some sort of cancer and the cancerous mole on their

161
00:11:27.870 --> 00:11:28.430
skin.

162
00:11:28.680 --> 00:11:32.150
And the actual model predicted no that's a false negative.

163
00:11:32.250 --> 00:11:35.320
So it's falsely saying they're negative for cancer.

164
00:11:35.520 --> 00:11:38.410
Then we have the patients that again actually do have cancer.

165
00:11:38.430 --> 00:11:41.010
And the model correctly predicts that they do have cancer.

166
00:11:41.010 --> 00:11:42.690
That's a true positive.

167
00:11:42.720 --> 00:11:47.950
So we can see real true positives true negatives false positives and false negatives.

168
00:11:48.000 --> 00:11:49.260
So the accuracy.

169
00:11:49.260 --> 00:11:52.710
Remember that's just overall asking how often is the model correct.

170
00:11:52.710 --> 00:11:55.690
So you have two positives plus true negatives over the total.

171
00:11:55.710 --> 00:12:02.540
So our model here on this test set of 165 patients was 91 percent accurate.

172
00:12:02.540 --> 00:12:06.310
Then there's other metrics you can calculate such as the misclassification rate or the error rate.

173
00:12:06.340 --> 00:12:08.220
Overall how often is the model wrong.

174
00:12:08.420 --> 00:12:11.240
It's essentially the reverse of the accuracy.

175
00:12:11.240 --> 00:12:16.340
So it's false positives plus false negatives divided by the total or 9 percent and that error rate is

176
00:12:16.340 --> 00:12:19.210
something you see reported as you perform Karris.

177
00:12:19.220 --> 00:12:24.350
So keep in mind in certain situations it's going to be really important to understand your false negatives

178
00:12:24.470 --> 00:12:26.100
and your false positives.

179
00:12:26.240 --> 00:12:31.670
For example here we have the kind of high stakes problem where we're trying to tell patients whether

180
00:12:31.670 --> 00:12:35.290
or not they have a cancer or a small base off the image of their mole.

181
00:12:35.570 --> 00:12:43.040
That means we really want to try our best to reduce false negatives because a false negative is essentially

182
00:12:43.040 --> 00:12:45.510
meaning that this person does have cancer.

183
00:12:45.620 --> 00:12:47.770
But our model is telling them not to worry.

184
00:12:47.870 --> 00:12:49.370
They don't have cancer.

185
00:12:49.490 --> 00:12:52.360
So that's an extremely dangerous position to put someone in.

186
00:12:52.370 --> 00:12:57.740
So you may want to trade off your precision versus your recall in order to help minimize your false

187
00:12:57.770 --> 00:12:58.650
negatives.

188
00:12:58.970 --> 00:13:03.290
Now in that sort of same situation false positives are also not really great because you know you're

189
00:13:03.290 --> 00:13:06.500
telling someone that doesn't have cancer that they do have cancer.

190
00:13:06.590 --> 00:13:12.050
But usually in the real world there's further tests you can do in order to now test these people that

191
00:13:12.050 --> 00:13:13.050
have false positives.

192
00:13:13.070 --> 00:13:16.810
Maybe the next that is an actual biopsy of their mole.

193
00:13:16.970 --> 00:13:21.710
But again we really want to try to reduce false negatives in that sort of high stakes situation because

194
00:13:21.710 --> 00:13:25.880
we don't want someone walking away from our clinic thinking they're OK when in reality they are not

195
00:13:25.880 --> 00:13:26.700
OK.

196
00:13:26.720 --> 00:13:27.580
So keep that in mind.

197
00:13:27.590 --> 00:13:31.880
Different situations call for different importance on different metrics.

198
00:13:31.880 --> 00:13:36.140
In that case everything is pretty low stakes because we're just dealing with images of cats and dogs

199
00:13:36.410 --> 00:13:39.350
not something as important as detecting cancer.

200
00:13:39.350 --> 00:13:39.830
All right.

201
00:13:39.830 --> 00:13:43.370
If you're still a little confused on the confusion matrix no pun intended.

202
00:13:43.380 --> 00:13:44.150
It's no problem.

203
00:13:44.150 --> 00:13:46.190
You can go ahead and check out the Wikipedia page for it.

204
00:13:46.190 --> 00:13:51.110
It has that diagram that we saw earlier as well as all the formulas for all the metrics and you can

205
00:13:51.110 --> 00:13:56.000
check out the different resource links I provided for you in this lecture for more breakdowns on precision

206
00:13:56.030 --> 00:13:58.030
versus recall versus accuracy.

207
00:13:58.130 --> 00:14:02.030
And throughout this training usually we're going to be doing is just printing out the metrics so we'll

208
00:14:02.030 --> 00:14:06.420
just be able to print out a classification report that prints out metrics such as accuracy precision.

209
00:14:06.440 --> 00:14:07.910
Recall an EF 1 score.

210
00:14:08.200 --> 00:14:10.030
OK let's go ahead continue.

211
00:14:10.040 --> 00:14:11.930
But I understand the basics of deep learning.
