WEBVTT
1
00:00:06.090 --> 00:00:10.190
Hello everyone and welcome to part two of the convolutional neural networks.

2
00:00:10.500 --> 00:00:12.450
We're going to continue right where we left off.

3
00:00:12.450 --> 00:00:17.670
Remember we were discussing the original convolutional neural network diagram and we were trying to

4
00:00:17.670 --> 00:00:20.690
get a understanding of what the convolutions were.

5
00:00:20.700 --> 00:00:26.570
Now what we need to do is discuss the subsampling or pooling portion of a convolutional neural network

6
00:00:26.730 --> 00:00:34.880
and it's actually quite a simple idea so pooling layers will subsample the input image which reduces

7
00:00:34.880 --> 00:00:39.560
the memory use and computer load as well as reducing the number of parameters.

8
00:00:40.380 --> 00:00:43.570
Let's imagine we have a layer of pixels in our image.

9
00:00:43.590 --> 00:00:47.670
Here we have a relatively simple input image is just six by six.

10
00:00:48.030 --> 00:00:53.160
And recall that for this digital data set each pixel has some sort of value representing what we'll

11
00:00:53.160 --> 00:00:58.180
call a darkness so 0 1 or maybe negative 1 to 1 extra.

12
00:00:58.240 --> 00:00:59.950
So here we have numbers filled out.

13
00:00:59.950 --> 00:01:04.940
You can imagine that this grid is entirely filled with numbers and the way pooling works is we create

14
00:01:04.970 --> 00:01:08.310
a two by two pool of pixels and that's known as a kernel.

15
00:01:08.590 --> 00:01:12.370
And then we evaluate the maximum value and it doesn't have to be two by two.

16
00:01:12.370 --> 00:01:15.760
It can be in any size you want but two by two is a pretty common choice.

17
00:01:15.820 --> 00:01:22.100
Again kind of the pends on the size of your input data and what you do is inside of two by two kernel.

18
00:01:22.300 --> 00:01:27.740
You evaluate the maximum value and only the max value makes its the next layer.

19
00:01:27.950 --> 00:01:33.840
So you basically decide at the maximum value is going to be representative of the entire kernel.

20
00:01:34.130 --> 00:01:39.320
So here we can see we have a two beta kernel we check OK what is the maximum pixel value and that's

21
00:01:39.320 --> 00:01:41.810
the only value that makes it the next layer.

22
00:01:41.900 --> 00:01:43.550
Then we move over by a stride.

23
00:01:43.550 --> 00:01:49.730
In this case we can imagine our stride is to grab the max value there and continue along.

24
00:01:49.750 --> 00:01:54.450
Now this pooling layer is going to end up removing a lot of information and even a small pulling kernel

25
00:01:54.450 --> 00:02:00.150
or two by two with a stride of two actually ends up removing 75 percent of the input data.

26
00:02:00.160 --> 00:02:06.010
However you should know that this is a really common step of convolutional neural networks as that vast

27
00:02:06.010 --> 00:02:07.770
amount of input data from an image.

28
00:02:07.900 --> 00:02:15.930
It's just basically going to be too computationally expensive to not have pooling layers for now another

29
00:02:15.930 --> 00:02:19.830
common technique deployed of convolutional neural networks is called dropout.

30
00:02:19.870 --> 00:02:23.160
Now we've actually discussed dropout in the past but let's have a quick review.

31
00:02:23.430 --> 00:02:28.680
Dropout can essentially be thought as a form of regularization to help prevent overfitting and during

32
00:02:28.680 --> 00:02:29.270
training.

33
00:02:29.340 --> 00:02:36.100
Units are randomly dropped along with their connections and this helps prevent units from CO adapting

34
00:02:36.100 --> 00:02:39.470
too much to any particular training set.

35
00:02:39.490 --> 00:02:44.080
Now let's also quickly point out some famous convolutional neural network architecture's.

36
00:02:44.210 --> 00:02:50.950
So we already discussed Linette fi and Lakhan But there's also famous sets like Alex that Google or

37
00:02:50.990 --> 00:02:56.870
Google that read that and you can check out the resource links to papers discussing these architectures.

38
00:02:56.870 --> 00:03:04.320
But essentially each of these architects architectures is just a set of design of layers.

39
00:03:04.320 --> 00:03:07.180
So for example in Alex that will look like this.

40
00:03:07.240 --> 00:03:12.390
It has a particular stride for its convolution that has a particular pooling layer that another pulling

41
00:03:12.390 --> 00:03:14.600
layer than other convolution etcetera.

42
00:03:14.940 --> 00:03:20.420
And we can see here Alex that basically the first convolutions are just edges and blobs.

43
00:03:20.490 --> 00:03:26.250
Then the third convolution pseudo scribe texture convolutions as we go higher and higher we get higher

44
00:03:26.250 --> 00:03:32.520
abstractions so then we can see the object parts and then we can eventually see objects classes able

45
00:03:32.520 --> 00:03:36.600
to distinguish things between like a dining table or clock a ship etc..

46
00:03:36.720 --> 00:03:42.810
So you can see here it's able to do up to a thousand or a hundred classes.

47
00:03:42.820 --> 00:03:49.090
Now realistically Alex that is so computationally expensive that it's actually distributed apart multiple

48
00:03:49.090 --> 00:03:49.870
GPS use.

49
00:03:49.900 --> 00:03:55.390
So you end up seeing diagrams that look like this where essentially half the image is going into one

50
00:03:55.390 --> 00:03:59.700
GPU and the other half's going to another GP you.

51
00:03:59.840 --> 00:04:03.260
OK so now he should fully understand the convolutional neural network.

52
00:04:03.290 --> 00:04:08.330
Remember we have convolutions then we have subsampling or pooling and eventually towards the end we

53
00:04:08.330 --> 00:04:14.030
have some sort of densely connected neural network and we can even have a soft Max regression if you

54
00:04:14.030 --> 00:04:16.580
want to use that for our output.

55
00:04:16.580 --> 00:04:21.820
Now let's continue by exploring how to implement it convolutional neural network with Cara's and Python.

56
00:04:21.950 --> 00:04:23.090
We'll see you at the next lecture.
