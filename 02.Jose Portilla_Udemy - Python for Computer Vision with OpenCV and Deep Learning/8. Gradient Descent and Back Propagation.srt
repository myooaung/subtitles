1
00:00:06,060 --> 00:00:10,980
Welcome back everyone to this lecture on gradient descent and back propagation.

2
00:00:11,060 --> 00:00:16,070
If you've dabbled in machine learning before or if you even looked at some online articles you've probably

3
00:00:16,070 --> 00:00:18,410
already heard of the term grading the sense.

4
00:00:18,410 --> 00:00:25,400
Let's quickly go over a high level overview and then discuss back propagation gradient descent is an

5
00:00:25,460 --> 00:00:30,410
optimization algorithm for finding the minimum of a function which is useful to us because we want to

6
00:00:30,410 --> 00:00:32,260
minimize that cost function.

7
00:00:32,420 --> 00:00:37,420
Sort of find a local minimum we take steps proportional to the negative of the gradient.

8
00:00:37,430 --> 00:00:41,200
Let's go ahead and go through a very simple example in just one dimension.

9
00:00:41,210 --> 00:00:47,000
So basically what we have here is our cost on the y axis so the result of that cost function and then

10
00:00:47,000 --> 00:00:51,020
on the x axis we have a particular way that we're trying to choose.

11
00:00:51,020 --> 00:00:53,800
So remember we end up choosing a random white in the beginning.

12
00:00:53,840 --> 00:00:59,000
So let's say that Bluepoint initiate that ran the weight that we chose and we want to somehow choose

13
00:00:59,000 --> 00:01:03,530
a weight that's going to minimize that cost and we can see here just visually when they mention that's

14
00:01:03,530 --> 00:01:09,550
basically at the bottom of that parabola so the weight gradient descent works in just a high level overview

15
00:01:09,910 --> 00:01:14,800
is that you end up taking the gradient which we know is a derivative of the function at that point and

16
00:01:14,800 --> 00:01:21,340
then seeing which way it goes in the negative direction and then we end up taking a step by step a sense

17
00:01:21,430 --> 00:01:25,870
along that gradient until we finally see that we get that minimum and more cost.

18
00:01:25,870 --> 00:01:32,780
So we see here just visually what the parameter value to choose to minimize our cost is.

19
00:01:32,780 --> 00:01:37,040
So finally this minimum is really simple for one dimension but our cases are going to have a lot more

20
00:01:37,040 --> 00:01:37,760
parameters.

21
00:01:37,850 --> 00:01:40,840
So we're not going to do that visually or with such a simple diagram.

22
00:01:41,000 --> 00:01:45,470
Meaning we're going to need to use built in linear algebra libraries that are deep learning library

23
00:01:45,640 --> 00:01:51,190
will provide using greened the sense we can figure out the best parameters for minimizing our cost.

24
00:01:51,190 --> 00:01:55,750
So for example find the best values for the weights of the near on inputs and we're going to go a lot

25
00:01:55,750 --> 00:02:01,790
more into the mathematics hungry and the sense when we actually go through the math example.

26
00:02:01,970 --> 00:02:06,680
So now we just have one issue to solve and that is how can we quickly just the optimal parameters or

27
00:02:06,680 --> 00:02:11,360
weights across our entire network in that one example we just showed for a single weight.

28
00:02:11,570 --> 00:02:17,150
But what if we want to actually go and then fix all the weights or adjust all the weights for the entire

29
00:02:17,150 --> 00:02:17,670
network.

30
00:02:17,750 --> 00:02:24,240
And that's we're back propagation comes in basically back propagation is used to calculate the air contribution

31
00:02:24,510 --> 00:02:30,000
of each year on after a batch of data is processed and it relies heavily on the chain rule from calculus

32
00:02:30,300 --> 00:02:36,580
to go back through the network and calculate these errors and the way back propagation works it calculates

33
00:02:36,580 --> 00:02:41,680
the air at the output and then distributes back through the network layers and it requires a known desired

34
00:02:41,740 --> 00:02:43,460
output for each input value.

35
00:02:43,460 --> 00:02:49,330
Remember that supervised learning needs some sort of comparison to actually get that error.

36
00:02:49,460 --> 00:02:53,550
All right so we now know enough terminology in theory to begin working.

37
00:02:53,560 --> 00:02:58,660
Of course we're going to be using a data set of features of counterfit versus real bills.

38
00:02:58,680 --> 00:03:01,200
Keep in mind we're still working on just numerical features.

39
00:03:01,260 --> 00:03:03,760
We're not actually reading an image stated that rectally.

40
00:03:04,050 --> 00:03:08,910
Later on we're going to learn about convolutional neural networks which is going which is going to allow

41
00:03:08,910 --> 00:03:13,790
us to actually handle image data that rectally the actual jpeg or PFG files.

42
00:03:13,800 --> 00:03:15,820
Ok so that's it for theory.

43
00:03:15,900 --> 00:03:18,110
Coming up next will work of course in Python.
