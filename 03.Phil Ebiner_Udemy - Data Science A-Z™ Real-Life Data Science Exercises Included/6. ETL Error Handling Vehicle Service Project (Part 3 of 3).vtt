WEBVTT
1

00:00:01.320  -->  00:00:03.450
And today we're going to finish off this homework.

2

00:00:03.450  -->  00:00:10.110
Finally we're going to look at the additional quality assurance checks and see what errors they will

3

00:00:10.350  -->  00:00:11.770
help us find.

4

00:00:12.180  -->  00:00:13.270
Let's have a look.

5

00:00:13.500  -->  00:00:14.840
Let's have a look at our door.

6

00:00:14.850  -->  00:00:19.330
So we already have the working file we can we can start working with that.

7

00:00:20.180  -->  00:00:21.450
Let's let's call this

8

00:00:24.060  -->  00:00:28.120
missional checks.

9

00:00:30.000  -->  00:00:34.660
Select star for home and this file.

10

00:00:35.160  -->  00:00:37.080
Oh this table.

11

00:00:37.080  -->  00:00:39.050
So what are the some logical checks here.

12

00:00:39.270  -->  00:00:46.470
Well first of all what we can do and what we probably should do is check what we were told in the conditions

13

00:00:46.470  -->  00:00:51.760
for this exercise.

14

00:00:51.760  -->  00:00:54.000
Just going to find it.

15

00:00:54.660  -->  00:00:55.080
All right.

16

00:00:55.110  -->  00:00:57.240
I think I need to open it up again.

17

00:00:57.270  -->  00:01:02.530
So here's our homework and original Doughton.

18

00:01:02.550  -->  00:01:05.550
So in the challenge what did they say.

19

00:01:05.550  -->  00:01:08.720
Here it says the customer ID field does not contain duplicate records.

20

00:01:08.880  -->  00:01:10.870
Now let's go ahead and check that.

21

00:01:10.980  -->  00:01:12.330
There's two ways you could have done it.

22

00:01:12.330  -->  00:01:19.710
Number one is through Excel and this is the part where you had to kind of switch your imagination on

23

00:01:19.800  -->  00:01:25.320
and think of ways it's not too trivial through Excel because you did have to break up the file into

24

00:01:25.320  -->  00:01:30.010
two and I'll show you how you could have done it through Excel.

25

00:01:30.090  -->  00:01:37.260
If we take let's say Part One let's well let's look at the CSPI so we don't have to load the text in

26

00:01:37.260  -->  00:01:39.960
again.

27

00:01:39.960  -->  00:01:47.880
And actually will also load part 2 in as well as a separate file.

28

00:01:47.970  -->  00:01:51.870
So this is the bit where you had to use a bit of imagination

29

00:01:54.360  -->  00:01:57.720
control shift L to fit a switch on the filters.

30

00:01:57.720  -->  00:02:04.320
And let's because we want to check if there are duplicate records in this field but also we know it's

31

00:02:04.320  -->  00:02:06.140
split up into two.

32

00:02:06.330  -->  00:02:09.200
So I'll actually do this.

33

00:02:09.270  -->  00:02:15.630
So we have two files that doesn't count as part 1 and there's part 2.

34

00:02:15.630  -->  00:02:18.000
So how do we check if there are duplicate.

35

00:02:18.020  -->  00:02:22.970
But not only in this file by itself but also across the two files.

36

00:02:23.010  -->  00:02:31.020
So the best way to do it is just sort them and so you know that definitely they're in ascending order

37

00:02:31.030  -->  00:02:31.200
.

38

00:02:31.440  -->  00:02:34.830
And then check these ones as well so sort these ones.

39

00:02:34.830  -->  00:02:36.020
Same way.

40

00:02:36.350  -->  00:02:41.460
And now you know that they're also in ascending order and that means that if you go to the bottom here

41

00:02:45.030  -->  00:02:55.080
you go to the bottom and you see 3 5 3 4 8 4 4 and 3 5 3 4 8 4 5 meaning that definitely they cannot

42

00:02:55.080  -->  00:03:00.870
be duplicated across the two files because the biggest one here is this one and the smallest one here

43

00:03:00.870  -->  00:03:02.130
is this one.

44

00:03:02.250  -->  00:03:09.330
That means that anything in this file is definitely greater than this value and anything in this file

45

00:03:09.330  -->  00:03:10.820
is great less than this value.

46

00:03:10.980  -->  00:03:17.490
And these are consecutive values so therefore the files are definitely like this one.

47

00:03:17.520  -->  00:03:22.530
And then the other one and there is no duplicates across them so all we have to now check is other duplicates

48

00:03:22.530  -->  00:03:24.300
in each one of these files so.

49

00:03:24.480  -->  00:03:32.970
Best way to check that is you go dark and just so I say remove duplicate continue of current selection

50

00:03:34.200  -->  00:03:35.430
and click OK.

51

00:03:35.730  -->  00:03:44.110
No duplicate values found good and now theres this one remove duplicate values continue current selection

52

00:03:44.110  -->  00:03:46.750
so it doesn't expand to the whole table.

53

00:03:47.700  -->  00:03:48.880
Okay.

54

00:03:49.540  -->  00:03:52.890
And then it says on one duplicate values found and removed.

55

00:03:52.920  -->  00:03:53.570
And these are.

56

00:03:53.610  -->  00:03:56.400
And the rest remain.

57

00:03:56.860  -->  00:03:57.150
OK.

58

00:03:57.150  -->  00:04:04.200
So what we can do is arrange price controls and so that it's back in there and we have the duplicate

59

00:04:04.200  -->  00:04:07.620
value here so how to how do you find duplicate value in Excel.

60

00:04:07.620  -->  00:04:17.550
One way is you highlight it in conditional formatting highlight duplicate values and like obviously

61

00:04:17.670  -->  00:04:23.920
it would be it would be insane trying to find it like by scrolling down.

62

00:04:24.930  -->  00:04:30.780
But if it's highlighted you can filter according to the highlighting.

63

00:04:31.050  -->  00:04:31.610
Right.

64

00:04:31.620  -->  00:04:42.030
So when this filter responds to my request we should be able to filter by highlighting while it's not

65

00:04:42.450  -->  00:04:43.490
the other way you could do it.

66

00:04:43.500  -->  00:04:51.400
Is your pivot table and do account with that that might be a bit more complicated.

67

00:04:51.400  -->  00:04:55.260
So this is taking a while indeed.

68

00:04:55.540  -->  00:05:01.540
Well what else what else you could do is break it up into smaller files which would be even faster to

69

00:05:01.540  -->  00:05:02.470
process.

70

00:05:02.650  -->  00:05:04.090
And I wasn't.

71

00:05:04.450  -->  00:05:10.950
This is definitely not something that is quick cleaning Dadda is never easy and quick so.

72

00:05:11.170  -->  00:05:16.870
All right so this one is not responding to me but you get the point so we've highlighted the duplicate

73

00:05:16.870  -->  00:05:22.390
values somewhere in there and you can just call it once a filter response or if your machine is a bit

74

00:05:22.390  -->  00:05:28.900
faster you can select based on the highlighting and find that value.

75

00:05:29.170  -->  00:05:36.550
All right so I'm going to just I'm actually going to bring up a skill so we'll I'll show you how to

76

00:05:36.550  -->  00:05:42.520
do it in a skill as well so the skills but we didn't talk about this in the course to do this you would've

77

00:05:42.580  -->  00:05:44.970
had to do some research on line.

78

00:05:45.010  -->  00:05:51.690
So we want to What do we want to do.

79

00:05:51.700  -->  00:05:59.950
Not not that obviously what we want to do is we want to select

80

00:06:02.550  -->  00:06:06.170
what a Carl selects.

81

00:06:06.340  -->  00:06:06.700
All right.

82

00:06:06.700  -->  00:06:15.220
So customer ID there real ones count star.

83

00:06:15.820  -->  00:06:22.660
So this is a group by statement from not from there from the working file

84

00:06:26.950  -->  00:06:31.190
and we want to group by customer ID

85

00:06:35.530  -->  00:06:41.580
and then we want to order or buy or even better.

86

00:06:41.580  -->  00:06:46.180
Having having cons are

87

00:06:49.300  -->  00:06:53.020
greater than one.

88

00:06:53.410  -->  00:07:02.650
So what that does is basically it selects all customer IDs and it groups your is kind of like a pivot

89

00:07:02.880  -->  00:07:03.820
it groups your

90

00:07:06.350  -->  00:07:11.360
aggregates your table by customer ID and then also count the number of records.

91

00:07:11.360  -->  00:07:13.900
If I dont do this part thats a filter.

92

00:07:13.900  -->  00:07:19.000
I dont do this part do this it just tells you the number of times this particular customer ID comes

93

00:07:19.000  -->  00:07:23.800
up if I do the having part it will only show me the ones that have columns.

94

00:07:24.040  -->  00:07:26.770
This column is greater than one so there it is.

95

00:07:26.800  -->  00:07:29.110
Thats the customer ID that came up twice.

96

00:07:29.320  -->  00:07:35.680
And so now if I want to check it we just go select star

97

00:07:39.940  -->  00:07:52.130
from here where customer ID is like and then just copy this value in here so we can just check.

98

00:07:52.130  -->  00:07:56.160
Is it a duplicate row or is it just something went wrong.

99

00:07:56.200  -->  00:08:00.660
So here you can see that the customers edge is the same but everything else is different.

100

00:08:00.670  -->  00:08:06.490
So just basically all that means is that somehow the customer My dear got duplicated over across.

101

00:08:06.490  -->  00:08:06.780
All right.

102

00:08:06.780  -->  00:08:09.110
So how many have we fallen so far.

103

00:08:09.250  -->  00:08:11.450
We have found three.

104

00:08:11.680  -->  00:08:13.390
So we have two more left to go.

105

00:08:13.390  -->  00:08:14.990
Two more areas to find.

106

00:08:15.370  -->  00:08:17.420
All right let's do this.

107

00:08:17.560  -->  00:08:17.830
OK.

108

00:08:17.830  -->  00:08:24.070
So the other things that you would do and these additional checks obviously you would want to put them

109

00:08:24.070  -->  00:08:27.100
into quotation marks or comments.

110

00:08:27.130  -->  00:08:33.320
The other thing is you do is check the values right or let's check the you check the customer since

111

00:08:33.320  -->  00:08:37.710
so we check the dates and to check the dates.

112

00:08:37.720  -->  00:08:42.210
Weve already talked about how you do this you go.

113

00:08:42.760  -->  00:08:51.340
You just select star from here where customer

114

00:08:54.160  -->  00:09:02.850
since is less than a certain date and let's say 100 or 100 let's say 50 years for the company.

115

00:09:02.860  -->  00:09:04.350
Right.

116

00:09:04.450  -->  00:09:13.460
There's a very low chance that this company has been in operation for longer than that opes that's that

117

00:09:13.460  -->  00:09:15.630
should be 1965.

118

00:09:16.210  -->  00:09:16.660
All right.

119

00:09:16.810  -->  00:09:18.820
If we run that they go.

120

00:09:18.820  -->  00:09:25.360
So there is one record where obviously somebody made a mistake and it should be it probably should be

121

00:09:25.360  -->  00:09:26.710
1999.

122

00:09:26.950  -->  00:09:29.820
And this person had this record.

123

00:09:29.830  -->  00:09:31.420
It has got the incorrect customer.

124

00:09:31.430  -->  00:09:38.230
And so once again we found one more and probably the reason here is somebody made a typo when putting

125

00:09:38.230  -->  00:09:44.440
this information in and the final one is if you really want to be diligent about your daughter you got

126

00:09:44.440  -->  00:09:45.910
to check these values right.

127

00:09:45.910  -->  00:09:47.740
So is anything.

128

00:09:47.740  -->  00:09:49.720
Are there any outliers and so on.

129

00:09:49.720  -->  00:09:55.750
Because if there's one error and one somebody put in a value incorrectly into one of these fields and

130

00:09:55.750  -->  00:10:00.010
you're looking for soms and that outlier is substantial He might skew everything.

131

00:10:00.010  -->  00:10:13.630
So we are a good way to do it if you go select let's say average of 2014 from

132

00:10:16.180  -->  00:10:17.240
there.

133

00:10:17.530  -->  00:10:20.280
So your average here is 388.

134

00:10:20.460  -->  00:10:20.740
Right.

135

00:10:20.740  -->  00:10:24.750
So they're kind of like nice high values max.

136

00:10:24.760  -->  00:10:28.300
So like the max here and then you see 20000.

137

00:10:28.330  -->  00:10:29.180
Right.

138

00:10:29.230  -->  00:10:30.630
So how can that be.

139

00:10:30.630  -->  00:10:35.180
If you look at 2015 the max is 800.

140

00:10:35.480  -->  00:10:42.340
Right if you look at 20 16 the max is 16.

141

00:10:42.370  -->  00:10:52.540
The expected Maxxis 800 right so and then just by looking at 2014 we can see that somebody putting a

142

00:10:52.540  -->  00:10:53.320
value or two.

143

00:10:53.320  -->  00:10:56.190
I remember we talked about looking for negative values.

144

00:10:56.190  -->  00:11:02.150
You could also check for men and see what the minimum value was here which is zero which is fine as

145

00:11:02.240  -->  00:11:06.070
not negative but the max is way out of line.

146

00:11:06.070  -->  00:11:12.570
So once again you look for something else that's an anomaly.

147

00:11:12.670  -->  00:11:17.110
And they think it can be any types of anomalies.

148

00:11:17.110  -->  00:11:20.740
That's just one example when the value is too high.

149

00:11:20.740  -->  00:11:22.230
Can be too low.

150

00:11:22.350  -->  00:11:24.400
It can be negative and things like that.

151

00:11:24.400  -->  00:11:31.950
So where 2014 is let's just say greater than 10000.

152

00:11:33.130  -->  00:11:37.210
So you see there's only one value or even greater than 800 of them.

153

00:11:37.300  -->  00:11:41.590
The average refund or not the average the max we found for the other columns once again there's only

154

00:11:41.800  -->  00:11:43.670
one value which is this one.

155

00:11:44.040  -->  00:11:49.300
And that looks like somebody just put in an incorrect value so that was error number five and I'm starting

156

00:11:49.300  -->  00:11:55.760
to go faster because this is the story is getting way too big anyway.

157

00:11:55.780  -->  00:11:58.740
So that's number five we founded.

158

00:11:58.780  -->  00:12:00.870
Last thing we had to do is check the sum.

159

00:12:00.880  -->  00:12:09.400
So let's do a calculation for the some select some 2016

160

00:12:11.590  -->  00:12:14.740
right from our table

161

00:12:17.740  -->  00:12:20.310
the 2016 e.

162

00:12:21.030  -->  00:12:21.420
Okay.

163

00:12:21.430  -->  00:12:29.440
So there's some and these surroundings they happen sometimes it's just how the how the data is stored

164

00:12:29.440  -->  00:12:38.140
in float Ballis so that's all right that's we can run that let's get excel up and let's put this in

165

00:12:38.140  -->  00:12:39.240
.

166

00:12:39.340  -->  00:12:40.200
So that's how.

167

00:12:40.240  -->  00:12:49.330
Value for the sum that we know and maybe less Format Cells Number two and then put some separators.

168

00:12:49.900  -->  00:13:01.930
So that's in ASCII All right then excluded row which we happen to have saved here.

169

00:13:02.980  -->  00:13:12.340
We know that's a sum or that's a value that's one excluded in skill and then one more

170

00:13:15.990  -->  00:13:17.690
in SS.

171

00:13:18.840  -->  00:13:24.690
Let's go find that one uploaded not analysis.

172

00:13:24.700  -->  00:13:26.610
So Rose there it is.

173

00:13:26.620  -->  00:13:32.060
So as you remember this got shifted so this is the actual value that we're looking for.

174

00:13:32.470  -->  00:13:37.290
So we put that in here and and plus plus plus that's all total.

175

00:13:37.300  -->  00:13:41.530
Now let's go have a look at the total in our document in our document.

176

00:13:41.530  -->  00:13:42.510
There is a total.

177

00:13:42.520  -->  00:13:44.180
Right.

178

00:13:45.580  -->  00:13:48.190
Okay that's a bit too big.

179

00:13:48.310  -->  00:13:52.600
So maybe it would subtract one from the other.

180

00:13:53.080  -->  00:13:53.470
Perfect.

181

00:13:53.470  -->  00:14:01.690
So that's the total that we found it matches to the total in the challenge and that is how this homework

182

00:14:01.690  -->  00:14:02.530
was done.

183

00:14:02.530  -->  00:14:05.550
As you can see this has extended over.

184

00:14:05.680  -->  00:14:08.880
I think it's over 40 minutes by now.

185

00:14:09.040  -->  00:14:14.210
It is tedious daughter prep is tedious but you've got to be diligent about it.

186

00:14:14.230  -->  00:14:17.420
And is the most time consuming part of our sense.

187

00:14:17.440  -->  00:14:23.740
I hope you went well with that and you learned a lot from this part of the course.

188

00:14:23.830  -->  00:14:25.000
And good luck.

189

00:14:25.000  -->  00:14:26.980
Best of luck in your daughter prep.

190

00:14:27.000  -->  00:14:31.810
Through all your daughter's science career and I look forward to seeing you in the other parts of this

191

00:14:31.810  -->  00:14:32.230
course.

192

00:14:32.230  -->  00:14:34.320
Until next time happy analyzing.
