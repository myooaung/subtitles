1

00:00:00,670  -->  00:00:04,320
The Prius tutorial we learn how to find data errors through skill.

2

00:00:04,370  -->  00:00:10,260
Today I'll show you how you can automate the same process but in essence I guess by the end of the editorial

3

00:00:10,260  -->  00:00:15,220
you will learn how to use another tool in essays the conditionals plit.

4

00:00:15,600  -->  00:00:16,780
Let's get started.

5

00:00:16,980  -->  00:00:19,760
First we're going to open up Visual Studio.

6

00:00:20,460  -->  00:00:25,750
Now open your saved project and go into your daughter.

7

00:00:26,220  -->  00:00:28,090
We'll be rerunning the upload.

8

00:00:28,350  -->  00:00:32,850
Therefore before we do anything at all we need to clean up the table that this daughter will be going

9

00:00:32,850  -->  00:00:33,600
into.

10

00:00:33,600  -->  00:00:36,560
Otherwise we will end up with duplicate records.

11

00:00:36,720  -->  00:00:42,480
So go to a school and select the top 1000 records of your table.

12

00:00:42,810  -->  00:00:50,190
Now delete everything between select and from and type in truncate table.

13

00:00:50,190  -->  00:00:53,730
This may seem similar to the drop table command we used previously.

14

00:00:53,730  -->  00:00:59,270
However truncate doesn't delete the table completely it just removes the data from the table.

15

00:00:59,280  -->  00:01:01,700
The headers are left untouched.

16

00:01:01,710  -->  00:01:03,840
Run your query now.

17

00:01:03,840  -->  00:01:08,680
Select the top 1000 rows of your table to verify that is empty.

18

00:01:08,760  -->  00:01:10,650
You can see that the headers are still there.

19

00:01:10,650  -->  00:01:14,130
However there is no data in the table at all.

20

00:01:14,700  -->  00:01:15,630
Good.

21

00:01:15,630  -->  00:01:17,880
Let's go back to Assayas.

22

00:01:17,880  -->  00:01:22,450
We need to add an extra step between the Flash file source and the destination.

23

00:01:22,530  -->  00:01:26,180
Click on the blue arrow and presently Tonja keyboard.

24

00:01:26,700  -->  00:01:32,100
Now go to your tool box and in the common tab you should find conditional split.

25

00:01:32,100  -->  00:01:35,880
Personally I prefer to see it in the transform step so lets move it there.

26

00:01:35,970  -->  00:01:42,060
Right click and choose move to other transforms.

27

00:01:42,060  -->  00:01:46,410
Open up transforms and drag the conditional split into your workspace.

28

00:01:46,500  -->  00:01:50,650
Now collapse all of the tabs in your toolbox and let's pause here for a second.

29

00:01:51,090  -->  00:01:54,230
You already know that e-tail stands for extractors form load.

30

00:01:54,360  -->  00:02:00,540
And as we discussed previously in Sosias we extract data from sources then we use transform to transform

31

00:02:00,540  -->  00:02:00,680
it.

32

00:02:00,690  -->  00:02:06,780
And finally we loaded into destination's right now on your workspace for the very first time.

33

00:02:06,810  -->  00:02:10,220
You have elements of all three components present.

34

00:02:10,290  -->  00:02:12,860
We will extract data from the flat file.

35

00:02:13,020  -->  00:02:19,100
Use the conditional split to transform it and loaded into the ONLY DB destination.

36

00:02:19,380  -->  00:02:20,980
Let's get to it.

37

00:02:21,000  -->  00:02:25,620
Use the blue arrow to connect to a flat file source and the conditional split.

38

00:02:25,620  -->  00:02:28,530
Double click of the split to open it up.

39

00:02:28,680  -->  00:02:34,420
Here we're going to add to the same filter as we created in s kill in the previous tutorial.

40

00:02:34,500  -->  00:02:37,220
It's extremely simple it's all drag and drop.

41

00:02:37,380  -->  00:02:39,590
First add an output name.

42

00:02:39,660  -->  00:02:43,260
Let's call it bad records.

43

00:02:43,260  -->  00:02:46,460
Now let's work on the condition.

44

00:02:46,470  -->  00:02:52,230
Open up the string functions folder and find the lende function.

45

00:02:52,230  -->  00:02:54,860
Drag this function into condition.

46

00:02:54,960  -->  00:03:03,200
Now open the columns folder and go to the end drag column 46 into the line function.

47

00:03:03,450  -->  00:03:13,380
Now on the right go to the operator's folder here find the Greater than operator dragged into your condition

48

00:03:13,440  -->  00:03:19,780
after the len function AAVSO the greater then operator type in a 0.

49

00:03:20,430  -->  00:03:21,930
That's the first part done.

50

00:03:21,960  -->  00:03:24,860
We are checking if call 46 is not empty.

51

00:03:24,900  -->  00:03:29,380
As I recall this takes care of all the right shifts.

52

00:03:29,400  -->  00:03:35,460
Now we need to pick up any left shifts in Asgill we checked for rows that did not have a period in the

53

00:03:35,460  -->  00:03:36,950
longitude column.

54

00:03:36,960  -->  00:03:39,630
Here we are going to achieve the same result but a bit different.

55

00:03:39,920  -->  00:03:44,550
We will check for rows where the longitude column is empty.

56

00:03:44,550  -->  00:03:46,360
Find the logical or operator.

57

00:03:46,470  -->  00:03:52,530
Note We don't need the bitwise OPERATOR We need the logical one draggle logical or into your condition

58

00:03:52,530  -->  00:03:53,510
.

59

00:03:53,730  -->  00:04:02,010
Drag another lead operator into condition after the logical or draggle longitude into the operator find

60

00:04:02,010  -->  00:04:09,090
the equal operator and place it off to the len operator and type in a 0.

61

00:04:09,090  -->  00:04:12,930
This will pick up and your rows shifted to the left.

62

00:04:12,930  -->  00:04:17,790
Great we have created a filter for bad records and euros that don't match this condition will go into

63

00:04:17,790  -->  00:04:19,410
the default output.

64

00:04:19,500  -->  00:04:27,180
So let's give the default output a name we'll call it good records click OK the good records are going

65

00:04:27,180  -->  00:04:31,910
to go into the database but we also need a facility where we will store the bad records.

66

00:04:31,920  -->  00:04:36,040
You can choose to send the bad records into a separate table in your database if you like.

67

00:04:36,060  -->  00:04:41,460
However it might be handy to send bad records straight into a flat file which you can then send off

68

00:04:41,460  -->  00:04:45,050
to the right person or team for investigation.

69

00:04:45,120  -->  00:04:46,890
I will show you this method.

70

00:04:46,890  -->  00:04:52,380
Go to the destination stabbing a tool box find the flat file destination and dragged into your workspace

71

00:04:52,380  -->  00:04:52,770
.

72

00:04:52,770  -->  00:04:56,100
Now let's connect the conditional split to the destinations.

73

00:04:56,130  -->  00:05:01,710
First connect to the flat file destination in the window that pops up select a better Rickard's output

74

00:05:02,220  -->  00:05:03,870
and click ok.

75

00:05:04,110  -->  00:05:09,930
If you click on the conditional split there will be a second blue arrow connected to the database destination

76

00:05:09,930  -->  00:05:10,460
.

77

00:05:10,560  -->  00:05:15,450
You can see that good records have been selected automatically and now there are no more blue arrows

78

00:05:15,510  -->  00:05:17,760
available from the conditional split.

79

00:05:17,760  -->  00:05:23,310
This makes sense because records will either meet our criteria and follow the Baracus path or they will

80

00:05:23,310  -->  00:05:26,550
not meet the criteria and follow the grid records.

81

00:05:27,000  -->  00:05:29,660
There is simply no alternative options.

82

00:05:29,670  -->  00:05:32,610
Now we will set up the flat file destination.

83

00:05:32,610  -->  00:05:41,900
Open it up and as usual we need to set up a manager first click new Select delimited and click OK and

84

00:05:41,910  -->  00:05:43,550
now click Browse.

85

00:05:43,710  -->  00:05:48,780
We want to be in the data errors subfolder of the analysis folder of our project.

86

00:05:48,780  -->  00:05:51,940
Make sure you navigate to this location first.

87

00:05:52,230  -->  00:05:59,310
Create a new folder called automatically excluded results inside this folder.

88

00:05:59,310  -->  00:06:01,950
Create a new text document.

89

00:06:02,040  -->  00:06:03,980
Let's call it 2015.

90

00:06:04,140  -->  00:06:09,850
0 8 0 5 underscore fake names underscore bad records.

91

00:06:10,170  -->  00:06:15,220
The date should match the date of the folder from where you're uploading the data.

92

00:06:15,540  -->  00:06:20,780
This way you will be able to understand which upload these errors relate to.

93

00:06:22,110  -->  00:06:24,220
Select the file and click open.

94

00:06:24,570  -->  00:06:26,580
Add a text qualifier.

95

00:06:26,580  -->  00:06:32,340
Note that when we open the file later it might feel a bit excessive to have text qualifies on every

96

00:06:32,340  -->  00:06:33,210
single cell.

97

00:06:33,210  -->  00:06:39,060
However I recommend using these additional text qualifiers because they will help you spot errors.

98

00:06:39,090  -->  00:06:43,820
Make sure to check the box for column names in the first dot a row before you proceed.

99

00:06:44,160  -->  00:06:49,110
The rest of the process is similar to flat file source setup but it has its nuances.

100

00:06:49,110  -->  00:06:53,620
Go to columns here you will see that there are no rows.

101

00:06:53,700  -->  00:06:57,110
Makes sense since we haven't loaded anything here yet.

102

00:06:57,150  -->  00:07:01,420
Make sure that the column headers are in place and now go to advanced.

103

00:07:01,530  -->  00:07:08,000
She will see that all of the columns have been assigned the correct with even the feedback column.

104

00:07:08,010  -->  00:07:13,020
This is because as I ask and see where the data will be coming from and has mimicked your flat file

105

00:07:13,020  -->  00:07:17,750
source don't change anything and proceed to the previous step.

106

00:07:18,090  -->  00:07:25,960
Click OK click on the mappings page and click OK now we're done.

107

00:07:26,040  -->  00:07:27,200
Let's say this package.

108

00:07:27,210  -->  00:07:30,250
And now let's launch it.

109

00:07:30,660  -->  00:07:36,870
The bad records are being separated from the good ones and slowly both destinations are being filled

110

00:07:36,870  -->  00:07:37,600
up.

111

00:07:38,160  -->  00:07:38,960
What do you know.

112

00:07:39,000  -->  00:07:41,300
There are 14 bad records.

113

00:07:41,370  -->  00:07:45,650
That's exactly the same amount as we found in Eskdale in the previous tutorial.

114

00:07:45,930  -->  00:07:48,990
Let's go and have a look at the results.

115

00:07:48,990  -->  00:07:53,240
Open up your project folder and navigate to the bad records file.

116

00:07:53,580  -->  00:08:00,690
Use Notepad plus plus to view it as you can see all 14 arrows are here and this means that now our table

117

00:08:00,690  -->  00:08:03,840
in the database is free of these anomalies.

118

00:08:03,840  -->  00:08:06,920
That's how you automate error handling and SOSIAS.

119

00:08:07,290  -->  00:08:12,570
As you can see the conditional split is an extremely powerful tool that can save you lots of time in

120

00:08:12,570  -->  00:08:13,300
the next Tauriel.

121

00:08:13,300  -->  00:08:19,290
We will delve deeper into this feature and learn how to combine multiple conditional splits in SSX
