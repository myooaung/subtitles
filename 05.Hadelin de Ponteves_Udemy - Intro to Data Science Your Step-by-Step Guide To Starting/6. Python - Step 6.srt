1
00:00:05,010 --> 00:00:09,480
Hello and welcome to the most exciting tutorial of the section.

2
00:00:09,480 --> 00:00:16,140
Indeed that's And this is oil that we're going to build our progression of and it's going to be gradient

3
00:00:16,410 --> 00:00:24,330
boosting gradient boosting is one of the best options to smash a regression problem or a classification

4
00:00:24,330 --> 00:00:24,950
problem.

5
00:00:25,200 --> 00:00:30,780
And the reason is that it has one of the best predictive powers among all the machinery models you can

6
00:00:30,780 --> 00:00:31,350
find.

7
00:00:31,500 --> 00:00:40,140
Actually we say today that the two best machinery models are extra boost and deep 30 miles an extra

8
00:00:40,140 --> 00:00:45,840
boost is one example of green boosting it's actually the best version of Green boosting.

9
00:00:45,840 --> 00:00:52,410
But the model that will implement right now in this toile is very close to actually boost but it's not

10
00:00:52,410 --> 00:00:57,870
exactly actually boost it's just the gradient boosting moral integrated inside it learn and you'll be

11
00:00:57,870 --> 00:01:04,560
convinced at the end of its power green boosting is like a team of what we call weak learners that are

12
00:01:04,560 --> 00:01:10,820
like decision trees you can check out these trees on the Internet or in our machinery in course gradient

13
00:01:10,830 --> 00:01:17,500
boosting is a team of weak learners which together form a powerful team to predict an outcome.

14
00:01:17,550 --> 00:01:23,040
And so they're trained on random samples of the training set to make predictions.

15
00:01:23,220 --> 00:01:28,680
Then we choose what we call a last function which measures the error between the prediction and the

16
00:01:28,680 --> 00:01:35,820
real result because we have the real result in why train and then step by step we add one week learner

17
00:01:35,820 --> 00:01:42,260
or one decision tree and we update the parameters in the direction of the last reduction that is we're

18
00:01:42,270 --> 00:01:47,570
trying to reduce the loss we're trying to reduce the error between the prediction and the real result.

19
00:01:47,670 --> 00:01:53,880
And by doing this step by step by taking each time a random sample of the training set without replacement

20
00:01:54,330 --> 00:02:01,740
we form a more and more powerful team of these weak learners which progressively become machinery moral.

21
00:02:01,830 --> 00:02:04,350
With great power of prediction.

22
00:02:04,650 --> 00:02:10,470
And so you're seeing this we will train our great boosting moral on the training set and then later

23
00:02:10,470 --> 00:02:16,260
on actually in the next toil we will make them predict new observations which will be of course the

24
00:02:16,260 --> 00:02:17,960
observations of the test.

25
00:02:17,980 --> 00:02:20,700
So just a few words I want to say about graden boosting.

26
00:02:20,700 --> 00:02:26,460
I'm not going to say much more because this is the section on bison but just know that gradient boosting

27
00:02:26,460 --> 00:02:28,100
is one of the best options.

28
00:02:28,210 --> 00:02:34,470
If there was no prior analysis of your data because you know what happens in general is that you have

29
00:02:34,470 --> 00:02:36,390
your data set you have your challenge.

30
00:02:36,390 --> 00:02:44,280
You study it you analyze it to see which machinery model would fit best to your specific challenge because

31
00:02:44,280 --> 00:02:50,670
remember there is not one best model for every mission or challenged each machine or in challenge has

32
00:02:50,670 --> 00:02:52,060
its best model.

33
00:02:52,080 --> 00:02:57,630
And that's why in our machinery course we introduce a lot of machinery models resizing the strengths

34
00:02:57,720 --> 00:03:03,060
and weaknesses of each model and then depending on the problem to solve or a challenge to solve.

35
00:03:03,150 --> 00:03:09,000
Well you can find the best model and that's why there is I remind this model selection module to find

36
00:03:09,000 --> 00:03:15,300
this best model and actually it doesn't even have to be one more all you can use what we call and assemble

37
00:03:15,360 --> 00:03:22,260
learning which consist of combining several models together which again form a great team at predicting

38
00:03:22,260 --> 00:03:23,000
something.

39
00:03:23,160 --> 00:03:28,860
And this is the same idea on which is based grading boosting and you will see that we will import the

40
00:03:28,860 --> 00:03:33,730
gradient boosting moral from a module of psyche learned that is called and symbol.

41
00:03:33,810 --> 00:03:39,630
And that's because gradient boosting is this idea and then symbol of weak learners predicting the same

42
00:03:39,630 --> 00:03:44,460
thing but together in a team so that the prediction is powerful.

43
00:03:44,460 --> 00:03:44,760
All right.

44
00:03:44,760 --> 00:03:48,660
So let's do this let's build this grid and boosting morale.

45
00:03:48,660 --> 00:03:54,420
We're going to use of course sikat learn because thanks to sikat learn we can build this model in just

46
00:03:54,570 --> 00:03:55,860
three lines of code.

47
00:03:55,920 --> 00:03:59,960
The first line will be to import the model from this symbol module.

48
00:04:00,000 --> 00:04:05,370
The second line of cool will be to get our regressors which will be an object of the brain boosting

49
00:04:05,370 --> 00:04:06,570
regressive class.

50
00:04:06,570 --> 00:04:08,260
I'll explain more what this means.

51
00:04:08,370 --> 00:04:13,860
And then of course the third line of code will be to fit our gradient boosting repressor to our training

52
00:04:13,860 --> 00:04:14,170
set.

53
00:04:14,170 --> 00:04:20,220
And basically what this means is that we are training our gradient boosting repressor on the training

54
00:04:20,220 --> 00:04:21,240
set.

55
00:04:21,240 --> 00:04:22,290
All right let's do this.

56
00:04:22,350 --> 00:04:24,480
If you're ready let's start right now.

57
00:04:24,480 --> 00:04:29,810
So as we said the first line of code is to import our graduate tomorrow.

58
00:04:29,850 --> 00:04:36,690
And so again we're going to import this from somewhere which is again sikat learn but then a specific

59
00:04:36,690 --> 00:04:44,700
module by sikat learn which is as we just said and assemble because it's based on this machinery technique

60
00:04:44,880 --> 00:04:47,040
which is called and Simbel learning.

61
00:04:47,380 --> 00:04:50,690
So symbol and from this symbol module.

62
00:04:50,850 --> 00:04:57,840
Well we are going to import gradient boosting regressors.

63
00:04:58,080 --> 00:04:58,770
Perfect.

64
00:04:58,800 --> 00:05:00,230
That's a first step done.

65
00:05:00,300 --> 00:05:07,790
So I have to explain what is exactly this graduate boosting regressors So you guess that it will of

66
00:05:07,790 --> 00:05:11,240
course contain our moral the gradient boosting the aggressor.

67
00:05:11,450 --> 00:05:19,310
But technically this is actually a class because Python is actually an object oriented programming language

68
00:05:19,700 --> 00:05:25,590
which means that it works with classes and objects and gradient boosting aggressor is a class.

69
00:05:25,590 --> 00:05:27,870
So now the question is what is a class.

70
00:05:27,980 --> 00:05:35,240
A class is basically an ensemble of instructions that define something you want to create because with

71
00:05:35,450 --> 00:05:39,120
objects and classes you can't actually create anything.

72
00:05:39,260 --> 00:05:45,050
You can create a self-driving car we do this in our artificial intelligence course you can build a block

73
00:05:45,050 --> 00:05:45,410
chain.

74
00:05:45,410 --> 00:05:47,710
We do this in our Plug-In course.

75
00:05:47,900 --> 00:05:49,280
And how do you build all this.

76
00:05:49,280 --> 00:05:55,770
Well you have this class that defines some parameters which define which one to create.

77
00:05:55,940 --> 00:06:01,430
And then inside the same class you also have a bunch of functions or more precisely what we call method

78
00:06:01,700 --> 00:06:05,000
which are like tools from which you apply your object.

79
00:06:05,000 --> 00:06:11,320
So for example these tools can be the self-driving car move forward go left go right go behind or break.

80
00:06:11,330 --> 00:06:16,440
All right so now if I give you the example of this gradient boosting regressive class.

81
00:06:16,580 --> 00:06:22,160
Well of course this is a class that defines the gradient boosting regress immoral moral and inside this

82
00:06:22,160 --> 00:06:25,950
class you have all the parameters that characterized the model.

83
00:06:25,970 --> 00:06:27,740
The grading boosting regress model.

84
00:06:27,950 --> 00:06:32,840
Then of course you have the whole algorithm of gradient boosting the one shown in the slide before with

85
00:06:32,840 --> 00:06:34,130
the six steps.

86
00:06:34,130 --> 00:06:40,940
And besides you have some tools like some function some method which are for example to fit to train

87
00:06:41,240 --> 00:06:43,250
your moral on some data sets.

88
00:06:43,430 --> 00:06:48,190
So actually this is a good example because that's the one we're going to use in this section.

89
00:06:48,230 --> 00:06:56,690
There is the fit method and the fit method is exactly used to train the machinery model to any data

90
00:06:56,680 --> 00:06:56,990
set.

91
00:06:56,990 --> 00:07:04,010
So basically you take your machinery model you add a dot you add the fit method and inside the fit method

92
00:07:04,010 --> 00:07:05,690
you include the training set.

93
00:07:05,720 --> 00:07:06,720
And that will fit.

94
00:07:06,790 --> 00:07:13,040
That will train your model to the transit and I'm saying machinery model in general because you have

95
00:07:13,040 --> 00:07:18,480
this same method in any machinery model inside cycle.

96
00:07:18,860 --> 00:07:21,790
All right so I hope you now get to classes.

97
00:07:21,800 --> 00:07:27,920
It's really important to understand what this is because it's very powerful to create as I said anything

98
00:07:28,620 --> 00:07:30,410
so great in boosting regressive of class.

99
00:07:30,410 --> 00:07:31,130
That's great.

100
00:07:31,130 --> 00:07:35,670
We have it and now we are going to create an object of this class.

101
00:07:35,840 --> 00:07:38,960
So now the question is what is an object of a class.

102
00:07:38,960 --> 00:07:43,600
Well that's an instance of the class because a class defines something.

103
00:07:43,610 --> 00:07:49,250
But then if you want to have the object itself there is for example the self-driving car itself or the

104
00:07:49,250 --> 00:07:53,270
block chain itself or this gradient boosting regressive model itself.

105
00:07:53,450 --> 00:07:57,690
Well you need to create an instance of the class and that's exactly what we're about to do.

106
00:07:57,690 --> 00:07:59,070
Now we have the class.

107
00:07:59,150 --> 00:08:05,550
But our model itself will be the object of this class these instance of this test.

108
00:08:05,570 --> 00:08:05,930
All right.

109
00:08:05,930 --> 00:08:13,430
And so let's create this model that is this object which we're going to call re aggressor regressors.

110
00:08:13,520 --> 00:08:16,120
So that's an object that's also variable.

111
00:08:16,310 --> 00:08:25,810
And by adding this equal here I'm going to define it as an object of the gradient boosting regressive

112
00:08:25,810 --> 00:08:26,760
class.

113
00:08:26,810 --> 00:08:29,540
And then you add some parenthesis and there you go.

114
00:08:29,540 --> 00:08:35,490
That's all you need to create an object of this class by just basically calling the class.

115
00:08:35,510 --> 00:08:35,860
All right.

116
00:08:35,870 --> 00:08:36,330
Perfect.

117
00:08:36,320 --> 00:08:37,510
So we have progressed.

118
00:08:37,520 --> 00:08:39,540
That's a really good step done.

119
00:08:39,590 --> 00:08:46,130
And now of course the final step is to train this regressors on the training set.

120
00:08:46,130 --> 00:08:51,920
Remember we train it on the training set and not the whole dataset because of course we have this training

121
00:08:51,920 --> 00:08:54,990
set for the training and the test set for the testing.

122
00:08:55,100 --> 00:08:57,850
So let's do this let's do this final step.

123
00:08:58,010 --> 00:09:05,030
You can see how it is easy and you can see how sikat learn facilitated the life of many machinery developers

124
00:09:05,030 --> 00:09:13,070
and data scientists because indeed we are about to smash this training in just three simple lines of

125
00:09:13,070 --> 00:09:14,820
code and that's amazing.

126
00:09:15,140 --> 00:09:21,640
So now let's do this final step to train our aggressor on the training set and to do this well is simply

127
00:09:21,650 --> 00:09:28,550
to take your aggressor that is your object and then apply one of these you know tools of the class.

128
00:09:28,550 --> 00:09:35,060
And as I said that's the example I give you the tool of the class we're going to use is the fit method

129
00:09:35,400 --> 00:09:41,650
and to use the method you need Adhir dirt and then fit and then some parenthesis.

130
00:09:41,810 --> 00:09:47,150
And then of course since fit is like a function we're going to hear some arguments.

131
00:09:47,150 --> 00:09:51,140
And so these arguments are very simple you can guess what they are.

132
00:09:51,140 --> 00:09:57,620
Basically we simply want to train this regressive to the training set and so there simply what we have

133
00:09:57,620 --> 00:09:59,490
to put is the training set.

134
00:09:59,600 --> 00:10:05,840
Nothing more complicated but you have to understand that the training set actually composed of X strain

135
00:10:06,080 --> 00:10:12,290
and y train because the model needs to understand the correlations between the independent variables

136
00:10:12,290 --> 00:10:17,810
contained in extraneous and the dependent variable contained in y train.

137
00:10:17,810 --> 00:10:21,380
And that's why we have to precise extrem in why train but that's all.

138
00:10:21,530 --> 00:10:27,560
That's the only thing you must not forget in order to train to fit a machine or immoral to a training

139
00:10:27,560 --> 00:10:28,460
set.

140
00:10:28,490 --> 00:10:37,100
So you take first the independent variables contained in X train and the dependent variable contained

141
00:10:37,280 --> 00:10:38,420
in y train.

142
00:10:38,480 --> 00:10:39,240
And that's it.

143
00:10:39,290 --> 00:10:45,920
Congratulations you are now ready to fit to train your regress tomorrow on the training set.

144
00:10:45,920 --> 00:10:46,220
All right.

145
00:10:46,220 --> 00:10:49,290
So let's do this let's select these three lines.

146
00:10:49,580 --> 00:10:50,480
And here we go.

147
00:10:50,510 --> 00:10:55,890
You're about to get your very first but powerful machine learning model.

148
00:10:56,000 --> 00:10:57,020
And here it is.

149
00:10:57,050 --> 00:10:59,030
Welcome to graden boosting.

150
00:10:59,030 --> 00:11:04,470
So all you see here are just the parameters of this grading boosting class.

151
00:11:04,520 --> 00:11:09,560
I didn't walk you through all these parameters because these are slightly more advanced and actually

152
00:11:09,560 --> 00:11:13,610
we don't need much to tweak these parameters.

153
00:11:13,610 --> 00:11:15,680
You will see will get some amazing results.

154
00:11:15,680 --> 00:11:19,360
That's because the deeper values here are generally the values use.

155
00:11:19,400 --> 00:11:24,350
But just be aware that there are some parameters that you can tweak in order to make your model even

156
00:11:24,350 --> 00:11:29,840
more powerful but you'll see that will already have a very powerful model.

157
00:11:29,840 --> 00:11:31,460
All right so congratulations.

158
00:11:31,520 --> 00:11:37,440
That was your first machinery model and we really started with one of the most powerful ones.

159
00:11:37,550 --> 00:11:43,020
If you want to go further in all this well I recommend two steps or two path.

160
00:11:43,220 --> 00:11:49,130
First if you're drilling for your business with a variety of problems for which grain boosting would

161
00:11:49,130 --> 00:11:51,540
not necessarily be the best option.

162
00:11:51,740 --> 00:11:57,440
Well I recommend to check out the other machinery models including some that might be a little less

163
00:11:57,440 --> 00:12:03,110
powerful than grading boosting but would provide some better results for your specific problem because

164
00:12:03,110 --> 00:12:04,530
of the way it is designed.

165
00:12:04,730 --> 00:12:11,300
And the other path is of course to check out and study in depth deep learning.

166
00:12:11,300 --> 00:12:17,870
Because as I said at the beginning of this it's oil it is the other branch of machinery that contains

167
00:12:17,870 --> 00:12:22,330
the best and the most powerful models but that's only for complex problems.

168
00:12:22,490 --> 00:12:28,610
And that's definitely only for nonlinear problems if you're dealing with some classic problem not too

169
00:12:28,790 --> 00:12:30,530
complicated not too complex.

170
00:12:30,530 --> 00:12:36,290
Well deploying will be too much you won't actually need it and therefore the classic machinery models

171
00:12:36,290 --> 00:12:38,480
will be very relevant.

172
00:12:38,480 --> 00:12:40,450
All right so just keep that in mind.

173
00:12:40,580 --> 00:12:46,490
And now we're going to move on to the final step which is also a very exciting step because basically

174
00:12:46,760 --> 00:12:49,920
we're going to test our model that we just built.

175
00:12:50,090 --> 00:12:52,010
So we'll do that in the next it's oil.

176
00:12:52,070 --> 00:12:53,960
And until then enjoy the science.
