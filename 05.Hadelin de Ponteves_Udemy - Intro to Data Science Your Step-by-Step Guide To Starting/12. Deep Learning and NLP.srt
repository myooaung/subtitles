1
00:00:04,600 --> 00:00:06,580
Hello welcome to toil.

2
00:00:06,670 --> 00:00:13,900
And today we're going to talk about the second main application for the second application is deep natural

3
00:00:14,050 --> 00:00:15,880
language processing.

4
00:00:15,880 --> 00:00:21,280
So first let's start with natural language processing or also called an LP.

5
00:00:21,300 --> 00:00:28,090
So natural language processing is basically the data science branch that allows to do a lot of magic

6
00:00:28,150 --> 00:00:31,140
with text text or even voice.

7
00:00:31,150 --> 00:00:37,660
So if we talk about the classic natural language processing well that consists of taking some text as

8
00:00:37,660 --> 00:00:42,720
input and returning as output a class like a category.

9
00:00:42,730 --> 00:00:46,820
For example let's take the very classic example of sentiment analysis.

10
00:00:47,080 --> 00:00:55,450
Well as inputs you can have some text like some reviews or some articles or some Twitter tweets or even

11
00:00:55,450 --> 00:00:56,980
some messages whatever you want.

12
00:00:56,980 --> 00:01:05,140
But the same kind of text and as output will return if the text is positive negative or even neutral.

13
00:01:05,200 --> 00:01:11,380
So for example you can predict with an B if A review is positive or negative and this doesn't even have

14
00:01:11,380 --> 00:01:19,150
to be binary your inputs can be some books or some articles or some stories and the outputs can be the

15
00:01:19,150 --> 00:01:21,430
jars of the text of the books.

16
00:01:21,580 --> 00:01:28,490
So for example nobody can predict if a book is a romance or a drama or comedy or even horror.

17
00:01:28,690 --> 00:01:31,820
So that's the classic example of an LPN by the way.

18
00:01:31,960 --> 00:01:36,490
I highly recommend because this course is to always recommend the first step.

19
00:01:36,490 --> 00:01:42,490
If you're interested in a specific field to start with sentiment analysis and speaking of this in our

20
00:01:42,490 --> 00:01:44,980
mission or in course we have a full section.

21
00:01:44,980 --> 00:01:51,100
We have a full pars on an LP and we deal with the sentiment analysis problem because that's the best

22
00:01:51,100 --> 00:01:52,000
way to start.

23
00:01:52,390 --> 00:01:57,510
But then even inside sentiment analysis you can have some complex problems.

24
00:01:57,550 --> 00:02:04,090
For example it's much more difficult to predict the jar of a book than whether or not a review is positive

25
00:02:04,090 --> 00:02:05,150
or negative.

26
00:02:05,160 --> 00:02:10,450
You can also predict the difficulty level of a text if you want to apply that to education for example

27
00:02:10,960 --> 00:02:13,380
or you can try to tackle some even more complex problems.

28
00:02:13,390 --> 00:02:20,170
But sentiment analysis is the starting point of an LP and then once you get some notions about this

29
00:02:20,170 --> 00:02:28,990
then you can attack deep and P which consist of leveraging deep learning to make some fantastic applications

30
00:02:29,290 --> 00:02:31,300
of natural language processing.

31
00:02:31,300 --> 00:02:34,000
So let's talk about these applications.

32
00:02:34,000 --> 00:02:40,360
So first of all you have machine translation you can basically not basically because it's not easy but

33
00:02:40,690 --> 00:02:46,500
you can build a translator like the one you use on Google Google Translate or whatever you want.

34
00:02:46,720 --> 00:02:54,190
You can build a translator by implementing a deep and a model which is the combination of deep learning

35
00:02:54,490 --> 00:02:56,450
and natural language processing.

36
00:02:56,530 --> 00:03:02,680
You can build this with a model called to set to Seck Moe and which is the most popular and the most

37
00:03:02,680 --> 00:03:09,250
powerful model in deep and peak this is the exact same model that is used to build chapbooks and I'll

38
00:03:09,250 --> 00:03:11,320
be talking of chapbooks in a few seconds.

39
00:03:11,320 --> 00:03:17,620
But first be aware that you can use the SEC to seek more oil to make a translator from one language

40
00:03:17,620 --> 00:03:19,740
to another and actually get heard.

41
00:03:19,840 --> 00:03:27,030
You can find a lot of translator implementations using this model which is the main deep end of the

42
00:03:27,160 --> 00:03:27,910
model.

43
00:03:28,250 --> 00:03:35,980
And speaking of this I'd like to recommend an amazing platform to use to build not only DPR animals

44
00:03:36,010 --> 00:03:37,780
but also AI models.

45
00:03:37,780 --> 00:03:45,130
And of course deep in our model is by torch by torch is one of the best platforms with tons of flow

46
00:03:45,340 --> 00:03:51,870
to implement deep green models and Emile's lets press enter here and why am I showing you by torture.

47
00:03:51,870 --> 00:03:58,120
That's because if you go to pite watch tutorials you will see in the documentation and implementation

48
00:03:58,180 --> 00:04:04,110
of deep learning for a.p which is exactly what we are talking about deep natural language processing.

49
00:04:04,120 --> 00:04:10,570
So let's click on this and you'll have a great explanation on DeBerry for an MP with by torche one of

50
00:04:10,570 --> 00:04:17,020
the most powerful and moreover if you're interested in that sector sick moral for translation as I've

51
00:04:17,020 --> 00:04:21,110
just been talking about well you can just type here sick too sick.

52
00:04:21,190 --> 00:04:22,940
And what will you find.

53
00:04:23,110 --> 00:04:27,130
Translation with the sequence to sequent network and attention.

54
00:04:27,130 --> 00:04:29,560
So let's click on this and there we go.

55
00:04:29,560 --> 00:04:34,750
You can find exactly a to sec implementation in pine torch to translate.

56
00:04:34,780 --> 00:04:35,490
I think it's yes.

57
00:04:35,500 --> 00:04:41,220
From French to English and you have all the code and all the explanations well-explained.

58
00:04:41,230 --> 00:04:45,250
And of course that's only if you want to go deep into deep and Oppy.

59
00:04:45,280 --> 00:04:51,290
So it's good to start with this and now I'm going to move on to the second main application of DePinho

60
00:04:51,300 --> 00:04:59,380
P which of course the chat but you can build a pretty smart chat but with deep a.p and the good news

61
00:04:59,380 --> 00:05:03,670
is that it's going to be almost the same implementation.

62
00:05:03,850 --> 00:05:09,470
Stick to say model for translation because you know it works the same way you have the inputs and outputs

63
00:05:09,820 --> 00:05:15,500
and translation the inputs is a sentence written in French and the output is a sentence written in English.

64
00:05:15,710 --> 00:05:23,000
Well for a chat both the inputs and outputs are kind of the same the input will be a question or something

65
00:05:23,000 --> 00:05:28,970
said by yourself and the output will be another sentence which is the answer of the chatbot.

66
00:05:29,210 --> 00:05:34,790
So basically it's kind of the same technique and almost the only thing that changes are the inputs and

67
00:05:34,790 --> 00:05:35,650
outputs.

68
00:05:35,840 --> 00:05:39,830
And so now how exactly does the chat but learn how to speak.

69
00:05:40,100 --> 00:05:42,490
Well the way it does this is pretty amazing.

70
00:05:42,530 --> 00:05:46,850
We feed the chat but a huge amount of text to chat.

71
00:05:46,850 --> 00:05:53,270
But we'll look at all the questions or all the input sentences and all the answers or the output sentences

72
00:05:53,690 --> 00:05:59,200
of these text and we'll learn how language works how we talk to each other.

73
00:05:59,240 --> 00:06:05,570
And so these compositions can be anything you can train your child but on all the Twitter conversations

74
00:06:05,570 --> 00:06:12,080
for example of 20:17 you can train the jackboots on hundreds of thousands of conversations taken from

75
00:06:12,080 --> 00:06:12,810
movies.

76
00:06:12,890 --> 00:06:16,290
And that's exactly what we do in our deep now because.

77
00:06:16,610 --> 00:06:20,570
Or you can train it on any other text or any other compositions.

78
00:06:20,570 --> 00:06:27,620
Basically what you only need is input sentences and output sentences and then the chat but we'll understand

79
00:06:27,620 --> 00:06:32,120
how this works how you know you need to start with a subject yourself or anyone.

80
00:06:32,210 --> 00:06:37,550
Then you add a verb then you add something which makes sense based on how the conversations work.

81
00:06:37,550 --> 00:06:45,020
So basically it's kind of exactly the same as how a baby learns how to speak a baby or a very young

82
00:06:45,020 --> 00:06:51,500
child doesn't know how to speak well at first and it makes some kind of weird sentences in the very

83
00:06:51,500 --> 00:06:58,130
beginning but then learns by hearing his parents or her parents talking all the time how to say sentences

84
00:06:58,130 --> 00:06:59,010
that make sense.

85
00:06:59,210 --> 00:07:04,550
And that's exactly the same here to be able to chat with we train it for quite some time actually it

86
00:07:04,550 --> 00:07:10,700
can take several days or even weeks to train a chat but so basically exactly like a child to learn how

87
00:07:10,700 --> 00:07:11,520
to speak.

88
00:07:11,630 --> 00:07:16,070
And so that's pretty fascinating because you know the goal of all this is the goal of deep learning

89
00:07:16,100 --> 00:07:22,910
AI and all that stuff is to build a brain an artificial brain that is closer and closer to the human

90
00:07:22,910 --> 00:07:25,410
brain how the real human brain works.

91
00:07:25,490 --> 00:07:31,550
I can relieve you if you're worried about all this a conspiracy taking over the world.

92
00:07:31,640 --> 00:07:35,210
We are still very far from the real human brain.

93
00:07:35,210 --> 00:07:39,710
Jeffrey Hinton is making some great progress with the capsule networks.

94
00:07:39,710 --> 00:07:42,470
This is this might be a new revolution in deeper.

95
00:07:42,490 --> 00:07:48,380
If that works well well this will disrupt the whole deeper in theory but at the time I'm speaking which

96
00:07:48,380 --> 00:07:52,560
is February 2018 we are still very far.

97
00:07:52,640 --> 00:07:53,110
So there we go.

98
00:07:53,120 --> 00:07:59,650
I hope you liked this introduction too deep and I'll definitely check out the sector model.

99
00:07:59,750 --> 00:08:05,480
But before this if it's your real first step in natural language processing Don't forget to start with

100
00:08:05,660 --> 00:08:08,550
natural language processing first without any neural networks.

101
00:08:08,690 --> 00:08:14,180
And the model you need to check out for sure is the bag of words model which is a model that cleans

102
00:08:14,180 --> 00:08:20,630
your text make the inputs and outputs in the right format so that your Leszek natural language processing

103
00:08:20,630 --> 00:08:23,210
model can make amazing predictions.

104
00:08:23,600 --> 00:08:29,690
All right so I'll see you in the next to toile for the final introduction of all these amazing David

105
00:08:29,700 --> 00:08:33,500
signs branches which is of course artificial intelligence.
