1
00:00:00,740 --> 00:00:04,440
[Autogenerated] now that we got good understanding on performance metrics,

2
00:00:04,440 --> 00:00:08,420
let's turn our attention to sage maker notebook instance and

3
00:00:08,420 --> 00:00:13,040
continue from where we left off from the last model.

4
00:00:13,040 --> 00:00:15,130
We already downloaded our data,

5
00:00:15,130 --> 00:00:20,630
split them and uploaded them to are S3 bucket in this module.

6
00:00:20,630 --> 00:00:24,240
Let's start the actual training process.

7
00:00:24,240 --> 00:00:26,970
If you have been falling along the different algorithms

8
00:00:26,970 --> 00:00:29,540
that we shared in the first two modules,

9
00:00:29,540 --> 00:00:32,740
this court should not look new to you.

10
00:00:32,740 --> 00:00:36,940
Let's quickly go over and understand this court better.

11
00:00:36,940 --> 00:00:41,300
We start with creating a sage maker session and error passing

12
00:00:41,300 --> 00:00:45,240
the container object on the role object.

13
00:00:45,240 --> 00:00:49,200
We're limiting the number of instances to one and error going

14
00:00:49,200 --> 00:00:53,440
to train on M four extra large instance.

15
00:00:53,440 --> 00:00:58,540
If we have a larger dataset, you can consider increasing the number of instances.

16
00:00:58,540 --> 00:01:00,640
And if you remember from Model one,

17
00:01:00,640 --> 00:01:06,740
that X G boosts training runs was only on CPU based instances only.

18
00:01:06,740 --> 00:01:12,460
He created S3 Bucket to hold the output and assign it in the output path and

19
00:01:12,460 --> 00:01:17,400
include the sagemaker position that we created before.

20
00:01:17,400 --> 00:01:20,640
Let's look at the hyper parameters in detail.

21
00:01:20,640 --> 00:01:25,110
The first value maximum depth indicates the maximum depth

22
00:01:25,110 --> 00:01:29,640
off a tree higher the number complex, the model will be,

23
00:01:29,640 --> 00:01:31,710
and it increases the chances off.

24
00:01:31,710 --> 00:01:33,860
War fitting E.

25
00:01:33,860 --> 00:01:34,090
T.

26
00:01:34,090 --> 00:01:34,450
A.

27
00:01:34,450 --> 00:01:37,740
Is a step size shrinkage value.

28
00:01:37,740 --> 00:01:41,940
Instead of getting the weights directly at the end off each boosting,

29
00:01:41,940 --> 00:01:43,650
this value actually shrinks.

30
00:01:43,650 --> 00:01:45,740
The feature weights.

31
00:01:45,740 --> 00:01:49,540
This value is I'll to prevent or fitting.

32
00:01:49,540 --> 00:01:53,830
Next is gamma larger the value of gamma,

33
00:01:53,830 --> 00:01:58,300
the more conservative the algorithm will be men.

34
00:01:58,300 --> 00:02:01,450
Child weight is a minimum number of instances.

35
00:02:01,450 --> 00:02:07,320
Wait needed in each node sub sample tells the ratio off

36
00:02:07,320 --> 00:02:10,530
data that is used to grow the trees,

37
00:02:10,530 --> 00:02:16,910
which in turn helps in preventing or fitting a silent value

38
00:02:16,910 --> 00:02:21,940
of zero will print a running messages on one indicates the

39
00:02:21,940 --> 00:02:23,560
algorithm will run in silent.

40
00:02:23,560 --> 00:02:29,590
More Objective-C space face the learning task on the

41
00:02:29,590 --> 00:02:32,040
corresponding learning Objective-C.

42
00:02:32,040 --> 00:02:35,340
In our case, we indicated binary logistic,

43
00:02:35,340 --> 00:02:41,140
which refers to a largest dict progression for a binary classification.

44
00:02:41,140 --> 00:02:44,240
Enum round, which is a required hyper parameter,

45
00:02:44,240 --> 00:02:49,740
represents a number off rounds to run the training process.

46
00:02:49,740 --> 00:02:53,540
Now it is humanly impossible to get a complete understanding off all

47
00:02:53,540 --> 00:02:56,640
the hyper parameters for all the built in algorithms.

48
00:02:56,640 --> 00:02:59,960
And you don't need to know all these hyper parameter

49
00:02:59,960 --> 00:03:02,740
values for your certification.

50
00:03:02,740 --> 00:03:05,110
However, as you start the training process,

51
00:03:05,110 --> 00:03:07,850
the best strategy is to select the UN garden.

52
00:03:07,850 --> 00:03:10,780
First on, then you can dive deep and steady.

53
00:03:10,780 --> 00:03:15,090
The hyper parameter is later, as you can see,

54
00:03:15,090 --> 00:03:18,100
the court complaints off syntax error.

55
00:03:18,100 --> 00:03:26,040
When I try to run, I'm going to move the parameter list in line with the object.

56
00:03:26,040 --> 00:03:27,360
Looks like it is running now,

57
00:03:27,360 --> 00:03:32,850
and it printed both the statements now that we created the

58
00:03:32,850 --> 00:03:36,350
estimator object on set the hyper parameters.

59
00:03:36,350 --> 00:03:40,070
It's called the Fit Method on past the training dataset.

60
00:03:40,070 --> 00:03:43,640
To start the training process,

61
00:03:43,640 --> 00:03:49,090
let me switch back to sage maker console Choose training job.

62
00:03:49,090 --> 00:03:51,740
Under the training section,

63
00:03:51,740 --> 00:03:55,260
there is a job at the top that is currently in in progress.

64
00:03:55,260 --> 00:03:57,340
Status.

65
00:03:57,340 --> 00:04:03,450
Click that it shows a creation time Last modified time on

66
00:04:03,450 --> 00:04:08,080
the AM role under algorithm section,

67
00:04:08,080 --> 00:04:11,890
you can see the instance count on instant Stipe value that

68
00:04:11,890 --> 00:04:15,940
we passed in the estimator object being reflected here

69
00:04:15,940 --> 00:04:17,750
under input data configuration,

70
00:04:17,750 --> 00:04:23,000
it shows the data source under its you are IT under metrics.

71
00:04:23,000 --> 00:04:28,030
It lists all the metrics that are output by this algorithm.

72
00:04:28,030 --> 00:04:31,830
The hyper parameter section shows all the hyper parameters that we

73
00:04:31,830 --> 00:04:36,640
said to the Xcode boost object under monitor,

74
00:04:36,640 --> 00:04:43,240
there is an option to view all garden metrics and instantiate tricks.

75
00:04:43,240 --> 00:04:47,040
Click on view algorithm metrics.

76
00:04:47,040 --> 00:04:53,280
It takes you to CloudWatch since the training job started just now,

77
00:04:53,280 --> 00:04:56,840
that is nothing to be displayed here.

78
00:04:56,840 --> 00:04:59,840
I just switched back to the notebook.

79
00:04:59,840 --> 00:05:06,540
You can see the instances are still getting launched under status,

80
00:05:06,540 --> 00:05:14,340
Click on View History and I can see the same status being shown here as well.

81
00:05:14,340 --> 00:05:16,940
Let me switch back to notebook instance,

82
00:05:16,940 --> 00:05:23,240
and instances are currently getting prepared for the training process.

83
00:05:23,240 --> 00:05:30,320
Once the input data is downloaded, all the 28,831 draws are being loaded.

84
00:05:30,320 --> 00:05:34,210
Now the training process is started.

85
00:05:34,210 --> 00:05:37,440
Under is currently under progress.

86
00:05:37,440 --> 00:05:44,090
The training error during the first run is 0.100968 and as a

87
00:05:44,090 --> 00:05:47,340
number of training integration increases,

88
00:05:47,340 --> 00:05:50,310
you can see the error goes down on the run.

89
00:05:50,310 --> 00:05:54,640
Number 96 has the lowest training error,

90
00:05:54,640 --> 00:05:57,800
and after that the training error is starting to go

91
00:05:57,800 --> 00:06:01,540
up from the 97th run onwards.

92
00:06:01,540 --> 00:06:05,890
However, the training continues to run until it finishes 100 runs,

93
00:06:05,890 --> 00:06:10,920
which is the number on hyper parameter that we initially say It took

94
00:06:10,920 --> 00:06:16,040
totally 67 seconds to complete the training process.

95
00:06:16,040 --> 00:06:17,870
Let me switch back to Sage Maker.

96
00:06:17,870 --> 00:06:23,540
Console on the status is now completed.

97
00:06:23,540 --> 00:06:27,210
Let me click on history and I can see the different

98
00:06:27,210 --> 00:06:30,640
stages of the training process.

99
00:06:30,640 --> 00:06:36,470
Though it took only 67 seconds only to train the data, you can see the instance.

100
00:06:36,470 --> 00:06:40,640
Preparation took close to 13 minutes.

101
00:06:40,640 --> 00:06:44,340
Let me click on view instantiate tricks.

102
00:06:44,340 --> 00:06:51,740
Let me select CPU utilization memory utilization on disk utilization.

103
00:06:51,740 --> 00:06:57,240
You can see that dots in the graph showing their corresponding values.

104
00:06:57,240 --> 00:07:02,510
Let me go back and select the algorithm metrics.

105
00:07:02,510 --> 00:07:07,540
Select the training error metric and you can see another point in the

106
00:07:07,540 --> 00:07:12,740
graph that corresponds to the training at our value.

107
00:07:12,740 --> 00:07:19,940
Let me log in back to S3 console and check if the output is stored properly.

108
00:07:19,940 --> 00:07:23,840
Click on Globomantics bucket.

109
00:07:23,840 --> 00:07:30,140
Choose sage maker Demo Xcode webOS Diem.

110
00:07:30,140 --> 00:07:33,840
Now select the output folder.

111
00:07:33,840 --> 00:07:39,010
The output off every training run is stood under its corresponding training job.

112
00:07:39,010 --> 00:07:40,440
Me,

113
00:07:40,440 --> 00:07:45,640
I'm going to select the sage maker Xcode boost algorithm

114
00:07:45,640 --> 00:07:51,250
click on output again and can see the gzip output file has

115
00:07:51,250 --> 00:07:54,940
been uploaded to this S3 bucket.

116
00:07:54,940 --> 00:07:58,010
This is the same output path that we specified

117
00:07:58,010 --> 00:08:01,840
while configuring a similar object.

118
00:08:01,840 --> 00:08:06,240
Now that you have seen how to train a model in the next model,

119
00:08:06,240 --> 00:08:15,000
you will learn about hyper parameter tuning and see a demo on automated hyper parameter tuning offered by sage maker.

