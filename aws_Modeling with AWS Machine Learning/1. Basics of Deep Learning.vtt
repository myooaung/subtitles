WEBVTT
1
00:00:01.840 --> 00:00:03.680
[Autogenerated] welcome to the module on deep

2
00:00:03.680 --> 00:00:06.440
learning foundations on algorithms.

3
00:00:06.440 --> 00:00:07.050
In this model,

4
00:00:07.050 --> 00:00:11.640
we will learn the business drivers behind deep learning and understand how

5
00:00:11.640 --> 00:00:16.670
an artificial neuron also call is Perceptron is designed after a

6
00:00:16.670 --> 00:00:22.440
biological brain on how a neural network is built.

7
00:00:22.440 --> 00:00:25.640
Then we will dive deep into different types off deep

8
00:00:25.640 --> 00:00:32.440
learning and understand convolution, Neural Network and recommend neural network.

9
00:00:32.440 --> 00:00:36.800
We will wrap up this model by understanding different CNN on our

10
00:00:36.800 --> 00:00:41.640
own and algorithms that are provided by AWS.

11
00:00:41.640 --> 00:00:45.540
Let's look at the business drivers behind deep learning.

12
00:00:45.540 --> 00:00:49.480
Deep learning algorithms learn on understand the features

13
00:00:49.480 --> 00:00:52.420
from the data in an incremental manner.

14
00:00:52.420 --> 00:00:57.940
Unlike a machine learning algorithm where you need a domain expert for this,

15
00:00:57.940 --> 00:01:01.170
deep learning works effectively in complex business

16
00:01:01.170 --> 00:01:06.840
problems like image processing, speech processing on language processing,

17
00:01:06.840 --> 00:01:11.670
deep learning algorithms do require high computational power to complete

18
00:01:11.670 --> 00:01:15.640
the training process in a reasonable amount of time.

19
00:01:15.640 --> 00:01:18.670
Deep learning continues to learn and became better

20
00:01:18.670 --> 00:01:21.740
as you provide more on more data.

21
00:01:21.740 --> 00:01:26.640
So deep learning is a very good candidate when you have a lot of data

22
00:01:26.640 --> 00:01:31.930
and it is another subset off machine learning the goal of deep learning

23
00:01:31.930 --> 00:01:36.390
is to create machines that are similar to human brain that can take

24
00:01:36.390 --> 00:01:39.740
positions by processing patterns.

25
00:01:39.740 --> 00:01:44.040
It's also called is in URL network on In Simple Terms,

26
00:01:44.040 --> 00:01:47.610
Deep Learning tries to achieve machine intelligence by

27
00:01:47.610 --> 00:01:52.380
representing data as a layer hierarchy off concepts where each

28
00:01:52.380 --> 00:01:56.240
layer is built from previous layers.

29
00:01:56.240 --> 00:01:58.740
Before looking at an artificial neuron,

30
00:01:58.740 --> 00:02:04.820
let's get a brief overview off biological neuron that make up our brain.

31
00:02:04.820 --> 00:02:11.440
A neuron has a body, our cell nucleus, axon and injury.

32
00:02:11.440 --> 00:02:16.340
Every input that an individual neuron receives in the farm off a signal

33
00:02:16.340 --> 00:02:22.040
travels down the axon to the din rights to the next neuron.

34
00:02:22.040 --> 00:02:26.940
The connection between one neuron with the other is called sin apps.

35
00:02:26.940 --> 00:02:28.040
In short,

36
00:02:28.040 --> 00:02:30.550
dent rates are the branches that receives the

37
00:02:30.550 --> 00:02:33.540
information from the neighboring neuron,

38
00:02:33.540 --> 00:02:39.040
and accents are like a cable that transmits this information.

39
00:02:39.040 --> 00:02:43.440
An individual neuron by itself don't offer much value,

40
00:02:43.440 --> 00:02:45.540
but when they are connected together,

41
00:02:45.540 --> 00:02:49.040
they can create things that are supremely complex.

42
00:02:49.040 --> 00:02:53.330
On our mind is a natural neural network and is said to

43
00:02:53.330 --> 00:02:58.520
have at least 100 billion neurons with 100,000 connections

44
00:02:58.520 --> 00:03:01.740
with its neighboring neurons.

45
00:03:01.740 --> 00:03:07.840
Artificial neural networks are computational models inspired by human brain.

46
00:03:07.840 --> 00:03:10.740
Before we jump in to artificial neural network,

47
00:03:10.740 --> 00:03:14.340
let's take a look at an artificial neuron.

48
00:03:14.340 --> 00:03:18.670
It's a mathematical representation off a biological neuron where you can

49
00:03:18.670 --> 00:03:25.840
visualized Dent writes to the input signals and accents to the output cell

50
00:03:25.840 --> 00:03:31.340
nucleus to the note and synapses to the rates.

51
00:03:31.340 --> 00:03:34.440
An artificial neuron are a perceptron.

52
00:03:34.440 --> 00:03:38.640
It's an elemental building, block off artificial neural network,

53
00:03:38.640 --> 00:03:41.840
and it has four important components.

54
00:03:41.840 --> 00:03:46.040
Input, values, weights and bias.

55
00:03:46.040 --> 00:03:50.240
Net some on activation function.

56
00:03:50.240 --> 00:03:55.480
A Perceptron accepts multiple input values a place a weight on

57
00:03:55.480 --> 00:03:59.850
the inputs sums all the waiter inputs on,

58
00:03:59.850 --> 00:04:01.920
then applies the transformation function.

59
00:04:01.920 --> 00:04:07.640
Are activation function to create an output SignalR an output value?

60
00:04:07.640 --> 00:04:11.640
It's important to remember that bias is one per neuron,

61
00:04:11.640 --> 00:04:15.640
and it's not one Perl input value.

62
00:04:15.640 --> 00:04:20.640
Let's dive a little deeper and study about activation function.

63
00:04:20.640 --> 00:04:23.390
An activation function can be a simple step.

64
00:04:23.390 --> 00:04:28.740
Function are a sine function that outputs zero or one.

65
00:04:28.740 --> 00:04:33.410
Our falsy are true when the some of the waiter input falsy

66
00:04:33.410 --> 00:04:39.610
shot or exceeds a threshold Value on output off one are true

67
00:04:39.610 --> 00:04:42.140
indicates that a neuron is trigger.

68
00:04:42.140 --> 00:04:44.220
And if it's zero R falsy.

69
00:04:44.220 --> 00:04:48.140
It indicates that the neuron not being triggered,

70
00:04:48.140 --> 00:04:51.740
this predictor output is then compared to the known output.

71
00:04:51.740 --> 00:04:54.240
And if there is an error in the prediction,

72
00:04:54.240 --> 00:04:58.500
this error is propagated backward to adjust the weights

73
00:04:58.500 --> 00:05:02.140
on re compute the training process.

74
00:05:02.140 --> 00:05:07.020
The step function of the sine function can be used for a typical binary

75
00:05:07.020 --> 00:05:10.740
classification on to utters a nonlinear problems,

76
00:05:10.740 --> 00:05:13.340
we can use a sigmoid function.

77
00:05:13.340 --> 00:05:17.940
A sigmoid function is a simple largest dict function that represents

78
00:05:17.940 --> 00:05:21.740
a probability off the value between zero and one.

79
00:05:21.740 --> 00:05:24.990
This is especially useful when one is interested in the

80
00:05:24.990 --> 00:05:29.740
probability mapping instead of a simple zero or one.

81
00:05:29.740 --> 00:05:34.410
The equation for a simple sigmoid function is one divided by

82
00:05:34.410 --> 00:05:38.440
one plus E to the power off negative X.

83
00:05:38.440 --> 00:05:42.970
Other commonly used activation function is rectified.

84
00:05:42.970 --> 00:05:45.790
Linear unit also called us real.

85
00:05:45.790 --> 00:05:46.090
Ooh,

86
00:05:46.090 --> 00:05:51.210
activation function on this function allows one to eliminate

87
00:05:51.210 --> 00:05:55.540
negative units in an artificial neural network.

88
00:05:55.540 --> 00:06:01.440
The equation for really function is Max off X or zero.

89
00:06:01.440 --> 00:06:05.110
That means if the value off x excess less than zero,

90
00:06:05.110 --> 00:06:10.540
these output will be zero on for values effects greater than zero.

91
00:06:10.540 --> 00:06:14.140
The output will be Samos X.

92
00:06:14.140 --> 00:06:19.640
There is no upper limit, unlike the sigmoid function, are step function.

93
00:06:19.640 --> 00:06:24.170
Some of the key benefits off a real function is that it performs

94
00:06:24.170 --> 00:06:27.740
the training in a more faster and effective way,

95
00:06:27.740 --> 00:06:30.840
and it scales very well.

96
00:06:30.840 --> 00:06:35.040
Hyperbolic tangent function is similar to sigmoid function,

97
00:06:35.040 --> 00:06:43.000
except in this case, the output values Marange from negative one to positive one instead of 0 to 1.

