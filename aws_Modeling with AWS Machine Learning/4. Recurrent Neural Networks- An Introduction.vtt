WEBVTT
1
00:00:01.140 --> 00:00:02.560
[Autogenerated] Let's switch your attention to

2
00:00:02.560 --> 00:00:06.130
recommend neural networks Common use cases.

3
00:00:06.130 --> 00:00:09.840
Off arguments are sequence prediction.

4
00:00:09.840 --> 00:00:17.300
Auto Completion Market predictions on many more All these cases need information

5
00:00:17.300 --> 00:00:22.340
about historical behavior to predict the current behavior.

6
00:00:22.340 --> 00:00:25.540
One off the limitations of a feed forward network is that the

7
00:00:25.540 --> 00:00:29.130
output off the network does not directly depend on the

8
00:00:29.130 --> 00:00:32.340
previous output off the same network.

9
00:00:32.340 --> 00:00:37.140
So the sequence off outputs are independent off each other.

10
00:00:37.140 --> 00:00:40.740
This are in in came into existence with the help of a hidden

11
00:00:40.740 --> 00:00:44.610
state that remembers all the information that has been

12
00:00:44.610 --> 00:00:48.640
calculated until that point in time.

13
00:00:48.640 --> 00:00:52.140
This picture shows a simple architectural diagram,

14
00:00:52.140 --> 00:00:57.640
a typical or in in that is unrolled in time.

15
00:00:57.640 --> 00:01:02.140
Each recurrent neuron has two sets off weights,

16
00:01:02.140 --> 00:01:07.500
one for the input x a time t on the other from the

17
00:01:07.500 --> 00:01:11.440
previous time in travel T minus one.

18
00:01:11.440 --> 00:01:16.640
This is also called a memory cell because at any point in time,

19
00:01:16.640 --> 00:01:20.100
the output at a specific point in time is a function off

20
00:01:20.100 --> 00:01:24.440
all the inputs from the previous steps.

21
00:01:24.440 --> 00:01:31.590
The cells hidden State H off P is different from output y off D because the

22
00:01:31.590 --> 00:01:38.240
hidden state hft is a function off its input at that Step X,

23
00:01:38.240 --> 00:01:44.240
and it's hidden state from the previous step H of T minus one.

24
00:01:44.240 --> 00:01:46.980
Let's look at different conflagrations off.

25
00:01:46.980 --> 00:01:52.740
Error in in first one is sequence to sequence.

26
00:01:52.740 --> 00:01:56.640
Here you feel the sequence off inputs to the recurrent

27
00:01:56.640 --> 00:02:01.240
neural network and get a sequence of output.

28
00:02:01.240 --> 00:02:05.940
This is primarily used in stock price predictions.

29
00:02:05.940 --> 00:02:11.060
Next one is sequence to better where you take a sequence

30
00:02:11.060 --> 00:02:15.640
off input and produce a single output.

31
00:02:15.640 --> 00:02:19.910
You can see this being used in reviewing systems to

32
00:02:19.910 --> 00:02:22.740
give a thumbs up or thumbs down.

33
00:02:22.740 --> 00:02:28.140
Vector to sequence here are in and takes one input

34
00:02:28.140 --> 00:02:31.340
and produce a sequence of outputs.

35
00:02:31.340 --> 00:02:38.840
This is used in a typical image captioning systems in quarter decoder.

36
00:02:38.840 --> 00:02:42.310
This has to networks that are connected together.

37
00:02:42.310 --> 00:02:46.940
The first are in the network, which is an end quarter is a sequence to victor.

38
00:02:46.940 --> 00:02:59.000
The second one, which is a decoder, is a victor to sequence on. This can be used in language, translations

