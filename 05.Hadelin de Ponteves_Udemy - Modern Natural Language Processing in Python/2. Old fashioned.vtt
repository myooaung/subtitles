WEBVTT
1
00:00:00.240 --> 00:00:05.820
Hi guys and welcome back to this email because so right before talking about the transformer we'll see

2
00:00:05.940 --> 00:00:12.960
how we used to address the sequence to sequence tasks before so all the tasks where we need to predict

3
00:00:13.050 --> 00:00:17.920
the whole text or a whole sentence from a text or a sentence that we are given.

4
00:00:18.060 --> 00:00:22.590
For instance this could be text summarization or translation Chad.

5
00:00:22.620 --> 00:00:27.060
What's all this kind of stuff and we'll see how this led to the transformer.

6
00:00:27.420 --> 00:00:34.380
So what we have mostly been doing to address those task is to use a pair of what we call encoder and

7
00:00:34.380 --> 00:00:34.980
decoder.

8
00:00:35.280 --> 00:00:42.900
So this encoder here is what is supposed to summarize all the information that we wanted to use from

9
00:00:42.900 --> 00:00:50.700
the input sentence and the Dakota will use as inputs the output of the encoder in order to create the

10
00:00:50.790 --> 00:00:57.750
right outputs and a natural way to do that was to use the R and ends so recurrent neural networks and

11
00:00:57.750 --> 00:01:05.410
the way those are and ends work is the following so we have a cell that takes as inputs the output of

12
00:01:05.410 --> 00:01:13.340
the produced cell and the next word from the input sentence and it computes the next what we call hidden

13
00:01:13.360 --> 00:01:14.290
states.

14
00:01:14.290 --> 00:01:20.870
The important point is that all of those cells are identical and they do exactly the same job.

15
00:01:20.890 --> 00:01:26.890
It's just that we feed them with new words from the input sentence each time and so it learns more and

16
00:01:26.890 --> 00:01:29.200
more about the whole input sequence.

17
00:01:29.200 --> 00:01:34.960
So this is why we call that recommend neural networks because there is this cell that is repeated as

18
00:01:34.960 --> 00:01:41.050
many times as needed which is actually the length of the input sentence and we actually do exactly the

19
00:01:41.050 --> 00:01:42.850
same for the decoder.

20
00:01:42.850 --> 00:01:48.010
We have a new cell that would be repeats until we reach the end of the decoding phase and this cell

21
00:01:48.010 --> 00:01:49.710
is different from the encoding one.

22
00:01:49.930 --> 00:01:56.470
But of course it's the same to the whole decoding phase and we would have us inputs the hidden state

23
00:01:56.470 --> 00:01:59.640
from the previous cell the previous predicted words.

24
00:01:59.890 --> 00:02:05.770
And we have our eyes outputs the next word that we predicts and a new hidden states.

25
00:02:06.100 --> 00:02:13.060
So a hidden state is just a huge vector or matrix that conveys information and so we add information

26
00:02:13.060 --> 00:02:14.230
to those vector.

27
00:02:14.740 --> 00:02:21.040
Each time we for instance are the new word from the input sentence or each time we predict a new word

28
00:02:21.100 --> 00:02:27.640
for the output sentence and those hidden states those are the information that we are carrying along

29
00:02:27.940 --> 00:02:29.410
to the whole process.

30
00:02:29.410 --> 00:02:36.280
So just to go through all the steps for those are unknowns the first thing we do is to embed the words

31
00:02:36.490 --> 00:02:43.150
into vectors so they can be used more efficiently by all computers by all algorithms.

32
00:02:43.150 --> 00:02:46.830
And so each word now is represented by a vector.

33
00:02:46.840 --> 00:02:53.850
Then we applied what I just explained about the and so we compute a new haven states for all encoder

34
00:02:54.310 --> 00:03:00.330
with the previous hit and states and the new embedded words that comes from the input sequence.

35
00:03:00.490 --> 00:03:05.770
After this whole process has been done with the encoder we have a hidden state that is the output of

36
00:03:05.770 --> 00:03:12.100
the encoder and that is supposed to convey all the information that we wanted to have from the input

37
00:03:12.100 --> 00:03:15.010
sequence and now we can apply the decoder.

38
00:03:15.010 --> 00:03:20.800
Of course in the decoder each cell for the audience are identical but they are different from the cells

39
00:03:20.800 --> 00:03:22.110
from the encoder.

40
00:03:22.990 --> 00:03:27.760
And this is kind of like unrolling the information that we get from the encoder.

41
00:03:28.300 --> 00:03:34.990
It's like in the encoder we made a package adding the new words so new information needs at each step

42
00:03:35.710 --> 00:03:39.320
and at the end we just retrieve a new information at each tape.

43
00:03:39.380 --> 00:03:43.160
A new word that will be a part of the output sequence.

44
00:03:43.420 --> 00:03:49.240
So I won't go into the details of Ivan's words how they are computed how we implement them because you

45
00:03:49.240 --> 00:03:54.730
will see later that we want to need that's for the transformer but it was important to you to see that

46
00:03:54.730 --> 00:04:03.190
because this is the origins of how we do sequence to sequence models and thinking of evolutions of models.

47
00:04:03.190 --> 00:04:10.210
We must be aware of the weaknesses of this model and one of the most important one is that you can see

48
00:04:10.210 --> 00:04:14.920
that we add words one after the other in the encoder.

49
00:04:15.100 --> 00:04:20.500
And so we will certainly lose information from the beginning of the input sentence for each word we

50
00:04:20.500 --> 00:04:21.030
add.

51
00:04:21.040 --> 00:04:26.830
So for very long sequences very long sentences the information will not be complete at the end of the

52
00:04:26.830 --> 00:04:27.490
encoder.

53
00:04:27.880 --> 00:04:32.980
And so when we wants to predict the next words in the decoder we could miss information that has been

54
00:04:32.980 --> 00:04:33.420
lost.

55
00:04:33.430 --> 00:04:34.980
The encoding phase.

56
00:04:35.060 --> 00:04:41.410
So what has been added to the audience in order to address this issue is what we call the attention

57
00:04:41.410 --> 00:04:42.460
mechanism.

58
00:04:42.460 --> 00:04:49.090
So basically what it does is that during the decoding phase we add new inputs to our cells in the Arnon

59
00:04:50.080 --> 00:04:55.540
and we call that the context vector and that's actually a vector that is supposed to convey a global

60
00:04:55.540 --> 00:05:02.170
information about the whole input sequence and more precisely how each step of the input sequence is

61
00:05:02.170 --> 00:05:06.010
related to our current status in the decoding phase.

62
00:05:06.010 --> 00:05:09.120
So let's suppose that we are at this phase right here.

63
00:05:09.130 --> 00:05:11.430
So we just predicted that would she.

64
00:05:11.520 --> 00:05:18.790
We have a hidden state that is G1 and what we would have done before it tests computes a new G do G

65
00:05:18.780 --> 00:05:27.400
to hit and states using as inputs G1 and the previews predicted would she and gets the new word is but

66
00:05:27.400 --> 00:05:34.330
here we add a new input which is this vector C that is as you can see he actually a weighted sum of

67
00:05:34.420 --> 00:05:37.610
all the hidden states from the encoder.

68
00:05:37.750 --> 00:05:44.650
And so the important part is how do we send those Hayden states and we can see it's represented by the

69
00:05:44.800 --> 00:05:48.910
coefficients a 0 a 1 and and so on.

70
00:05:49.330 --> 00:05:53.820
So the intuition behind that is just to assess how G1.

71
00:05:53.860 --> 00:06:02.050
So the current hidden states the current state of the decoder is related to each of the H hidden states.

72
00:06:02.080 --> 00:06:10.610
So how our current decoding phase is related to each of the encoding phase so if we want to get an intuition

73
00:06:10.610 --> 00:06:12.850
of that we could say that's here.

74
00:06:12.860 --> 00:06:18.620
The hidden states G1 is the one that we get just after predicting the world's sheep.

75
00:06:18.800 --> 00:06:25.910
So this state would probably conveys an information like we just declared the subject of the sentence

76
00:06:25.970 --> 00:06:26.870
which is she.

77
00:06:26.870 --> 00:06:30.650
So now we want to we want to know how these hidden states.

78
00:06:30.710 --> 00:06:36.580
So just having declared the subject of the sentence is related to each of those hidden states.

79
00:06:36.770 --> 00:06:44.420
And naturally having declared the subjects would be related to the verb that's is related to the subjects

80
00:06:44.570 --> 00:06:48.410
in the input sentence which is a question in our example.

81
00:06:48.410 --> 00:06:55.700
It's the same process as when we speak just after saying she we know that's our current state of speaking

82
00:06:55.700 --> 00:07:00.710
is related to the verb that the other person just used to talk about she.

83
00:07:00.710 --> 00:07:07.730
So if the person in front of you said how is she just after saying she you know you have to react to

84
00:07:07.880 --> 00:07:09.330
the verb is.

85
00:07:09.470 --> 00:07:16.700
So that's globally how the attention mechanism works here in the R.A. and if you want to go into mathematical

86
00:07:16.700 --> 00:07:18.960
details we would just say that.

87
00:07:18.970 --> 00:07:19.470
See I.

88
00:07:19.490 --> 00:07:23.810
That's the context vector that we want to feed the cell number I with.

89
00:07:23.810 --> 00:07:28.460
So these being G I minus 1 and he G I.

90
00:07:28.850 --> 00:07:33.890
So this is just a way to sum all of all the hidden states from the encoder.

91
00:07:34.100 --> 00:07:41.900
So basically each number of for i j just say how much we wanted to emphasize on the hidden state H.J.

92
00:07:42.020 --> 00:07:45.360
when we computes g i so he G2.

93
00:07:45.470 --> 00:07:48.820
So the important part is to create those alpha.

94
00:07:48.880 --> 00:07:49.580
I j.

95
00:07:49.730 --> 00:07:53.900
But basically say how all occur in states that is actually j 1.

96
00:07:53.930 --> 00:08:02.270
So J I minus one is related to each of those hidden states and the way to do that is that we will apply

97
00:08:02.560 --> 00:08:03.900
a soft max function.

98
00:08:04.010 --> 00:08:09.110
So that's just a tool that we use in order to create weights for a weighted sum.

99
00:08:09.230 --> 00:08:16.010
So the coefficients e being the similarity for instance between the state of our decoder and the hidden

100
00:08:16.010 --> 00:08:23.330
states of our encoder e being those coefficients we just computes the alpha in order to keep the relations

101
00:08:23.330 --> 00:08:24.530
between the E.

102
00:08:24.560 --> 00:08:31.190
So if one of these coefficient e is higher of course the corresponding Alpha would be higher too.

103
00:08:31.370 --> 00:08:38.090
But thanks to this operation the s Max the sum will be equal to one and each number will be between

104
00:08:38.300 --> 00:08:39.320
0 and 1.

105
00:08:39.440 --> 00:08:46.430
That actually makes all the alphas value weights that we can use to sum all the hidden states each and

106
00:08:46.430 --> 00:08:55.880
the most important parts D E I J that conveys information about the relation between J A minus one and

107
00:08:56.060 --> 00:09:02.270
each of the h j will just be a function that we can pick actually are different options for that's one

108
00:09:02.270 --> 00:09:08.930
of the most common one is to use that products because the duck predicts that later between two vectors

109
00:09:08.960 --> 00:09:12.500
is a good way to get how similar those vectors are.

110
00:09:12.650 --> 00:09:18.230
But the important thing to keep in mind is that's a supposed to be similarity function.

111
00:09:18.230 --> 00:09:23.270
So if you just repeat this backwards we are at the stage.

112
00:09:23.540 --> 00:09:24.700
For all decoder.

113
00:09:24.710 --> 00:09:32.240
So we are about to predict G I and have already predicted the words I minus 1 and the hidden states

114
00:09:32.570 --> 00:09:36.210
J I minus 1 and so in order to predict this.

115
00:09:36.240 --> 00:09:42.670
This J I will just wants to know how the previous hit in the States which is where we are right now.

116
00:09:42.680 --> 00:09:47.540
J I minus one is related to each of the hidden states of the encoder.

117
00:09:47.600 --> 00:09:53.990
So it's almost like if you are looking for how these words is related to each of the words from the

118
00:09:53.990 --> 00:09:59.420
input sentence and the way to do that is to use a similarity function between the occurrence hidden

119
00:09:59.420 --> 00:10:02.450
states and all of the hidden states of the encoder.

120
00:10:02.780 --> 00:10:10.310
And now that have those similarity weights we just apply a soft Max in order to have real weights that

121
00:10:10.310 --> 00:10:16.360
we can use for a weighted sum and we apply those weights to all of the hidden states of the encoder

122
00:10:16.520 --> 00:10:22.280
to finally have a vector that is mostly made of the Hayden states from the encoder that are related

123
00:10:22.280 --> 00:10:25.580
to our current states in the decoding phase.

124
00:10:25.610 --> 00:10:29.930
So that's basically how the attention mechanism works in the ordinance.

125
00:10:29.930 --> 00:10:35.930
It's just during the decoding phase we simply add a new vector that conveys more global information

126
00:10:35.930 --> 00:10:41.990
about the whole input sequence and this vector changes at each step of the decoding phase so that each

127
00:10:41.990 --> 00:10:48.950
cell of the R and then gets information from the input sequence that is related to the current states.

128
00:10:49.700 --> 00:10:57.010
So that's it for the attention and marginality for how we used to address sequence to sequence task

129
00:10:57.020 --> 00:10:57.730
before.

130
00:10:57.740 --> 00:11:02.900
Now we have all the bases to talk about the transformer and that's exactly what we will do next.

131
00:11:02.900 --> 00:11:03.200
So.
