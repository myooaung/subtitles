WEBVTT
1
00:00:00.210 --> 00:00:02.250
Hi and welcome back to this tutorial.

2
00:00:02.250 --> 00:00:06.810
Now that we have clean our compasses we are ready to take a nice them.

3
00:00:06.930 --> 00:00:12.020
So tokenization is just this process that consists in transforming each word.

4
00:00:12.030 --> 00:00:19.090
So each set of characters into one corresponding number which is way easier for a computer to process.

5
00:00:19.110 --> 00:00:24.720
And for that we will use the very useful tool provided by the tensor flow datasets module that we imported

6
00:00:24.720 --> 00:00:25.170
before.

7
00:00:25.560 --> 00:00:36.060
So let's create this objects took a nice English and it will be TFG s which is how we imported the tensor

8
00:00:36.060 --> 00:00:37.000
flow datasets.

9
00:00:37.020 --> 00:00:54.030
Library features that texts that sub Woods texts and call the dots builds from Corpus and with the first

10
00:00:54.030 --> 00:01:02.730
half to tell him the corpus that he has to use in order to build the token either an important feature

11
00:01:03.000 --> 00:01:10.770
we can ask him to use an approximate number of words in this Kabul area and it can be quite useful to

12
00:01:10.830 --> 00:01:18.390
have a lower number than expected because we could say that if if you want to count the number of words

13
00:01:18.390 --> 00:01:24.630
in all copies right now it would probably be around two hundred thousand which is a lots and actually

14
00:01:24.630 --> 00:01:31.530
trying to reduce this number means that taken kinase a we'll have to make some clever computation for

15
00:01:31.530 --> 00:01:38.010
instance if he doesn't know a word like transformer let's say he might consider that it's a mix between

16
00:01:38.130 --> 00:01:44.250
the word trans that he might maybe have in his vocabulary and former which actually makes sense goes

17
00:01:44.400 --> 00:01:45.660
this is where it comes from.

18
00:01:46.140 --> 00:01:51.450
So having a lower number of words can actually improve the performance of the whole model.

19
00:01:51.480 --> 00:01:57.270
But here I decided to use something around eight thousands.

20
00:01:57.330 --> 00:01:59.670
This is probably not the optimal one.

21
00:01:59.730 --> 00:02:04.530
Once you have your whole translator working you will have the opportunity to play with this if you want

22
00:02:04.530 --> 00:02:05.270
to test.

23
00:02:05.280 --> 00:02:13.860
If it's better with a higher number or lower anyway this is the one I used right here of course as usual

24
00:02:14.550 --> 00:02:18.680
we just have to do the same thing for our second corpus.

25
00:02:19.830 --> 00:02:25.100
So he creates another to Kinshasa for the French language.

26
00:02:25.170 --> 00:02:30.450
I would not run this cell right now because this recognizing face takes a lot of time just because our

27
00:02:30.450 --> 00:02:32.160
Corpus is quite huge.

28
00:02:32.160 --> 00:02:39.000
But after this is done we can create two global viable is that we will use later.

29
00:02:39.490 --> 00:02:46.960
Which are the vocab size for the first language and the way we have access to this one is by using took

30
00:02:46.960 --> 00:02:58.010
an idea a vote com size and we will add to just because we will add two homemade words.

31
00:02:58.090 --> 00:02:59.190
Those are not really words.

32
00:02:59.200 --> 00:03:03.850
Those will be Dawkins because this would be a token that we will add at the beginning of a sentence

33
00:03:04.090 --> 00:03:06.190
and another one at the end of the sentence.

34
00:03:06.220 --> 00:03:11.560
So those aren't really words but they will be used in our model and it's important to consider them

35
00:03:11.650 --> 00:03:13.710
in the size of our dictionary.

36
00:03:13.750 --> 00:03:24.660
And same thing for the French language F R and he f oh here we are.

37
00:03:24.910 --> 00:03:31.990
So I can't read this one because the organizing phase hasn't be done of course but the final parts.

38
00:03:32.170 --> 00:03:38.020
Once we have created AutoCAD NISA is of course to talk a nice all campuses and now that we are getting

39
00:03:38.020 --> 00:03:46.090
closer to creating real inputs we will call our valuables inputs and outputs so inputs will be a list

40
00:03:46.120 --> 00:03:47.070
of sentence.

41
00:03:47.170 --> 00:03:53.740
Each sentence being a sequence of numbers corresponding to the encoding of words and as I said before

42
00:03:53.860 --> 00:04:01.840
we will have to use starting and ending tokens so each sentence will start with the starting token that

43
00:04:01.840 --> 00:04:11.010
will be vocab size English minus 2 which is exactly the the size of the vocabulary of the original NISA

44
00:04:11.530 --> 00:04:18.430
and we have to know that if vocab size is the original size of the organizer the number vocab size is

45
00:04:18.430 --> 00:04:22.660
not used in the encoding phase it stops at vocab size minus one.

46
00:04:22.660 --> 00:04:28.220
This is why we can use vocab size English minus two which equals the original vocab size as a whole

47
00:04:28.220 --> 00:04:29.090
made token.

48
00:04:29.170 --> 00:04:36.090
Now that we are starting to can we can jests are the encoded sentence like that's we call to organize

49
00:04:36.110 --> 00:04:43.900
a that's in code which is the method that encodes a string sentence into a list of numbers according

50
00:04:43.900 --> 00:04:47.680
to the organizing creation that we found before.

51
00:04:49.290 --> 00:05:01.720
So we encode a sentence and we add at the end our homemade ending token vocab size English minus one.

52
00:05:01.750 --> 00:05:09.200
And this is for every sentence in our original corpus English.

53
00:05:09.700 --> 00:05:20.260
So now inputs is just a list of encoded sentences we do the same thing here for our second language.

54
00:05:20.290 --> 00:05:30.970
So outputs and we just change each e n by F are.

55
00:05:31.150 --> 00:05:32.860
Same thing here.

56
00:05:33.400 --> 00:05:34.940
Same thing right.

57
00:05:35.500 --> 00:05:36.430
And finally

58
00:05:39.100 --> 00:05:39.420
here.

59
00:05:40.300 --> 00:05:40.640
Sorry.

60
00:05:40.990 --> 00:05:41.720
So here we are.

61
00:05:41.830 --> 00:05:46.060
We have something that starts to look like real inputs and outputs because I have a list of numbers

62
00:05:46.090 --> 00:05:48.650
which is actually what concrete just prefer.

63
00:05:48.820 --> 00:05:54.670
We will just need to do one or two last thing in order to have all finalized inputs and outputs but

64
00:05:54.670 --> 00:05:56.920
we are quite close to start building our model.

65
00:05:56.950 --> 00:05:59.620
But for now it took an easing phase is done so see you soon.
