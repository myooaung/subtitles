1
00:00:01,000 --> 00:00:01,480
Hi guys.

2
00:00:01,490 --> 00:00:03,540
And welcome back to this MLP course.

3
00:00:03,550 --> 00:00:10,930
So now that we have all our data processing done we are ready to start writing the code for our deep

4
00:00:10,960 --> 00:00:14,080
convolution of neural network for A.P..

5
00:00:14,320 --> 00:00:20,530
So it just before we do that let's have a quick look at the architecture that we want to implement.

6
00:00:20,530 --> 00:00:27,220
So what we do is that once we have our inputs we will of course starts by applying an embedding layer

7
00:00:27,550 --> 00:00:35,200
and then we will have three different kinds of one D convolution or layer that we want to apply being

8
00:00:35,200 --> 00:00:37,630
of size to three and four.

9
00:00:37,990 --> 00:00:41,540
And we will apply a certain number of each of them.

10
00:00:41,800 --> 00:00:48,850
So each filter after the activation function will give us a new vector and we will take the max of each

11
00:00:48,850 --> 00:00:55,580
of those vectors with Max pulling and concatenate all of this apply a linear function.

12
00:00:55,600 --> 00:00:56,680
So a dense layer.

13
00:00:56,890 --> 00:00:59,560
And finally we will get our classification task done.

14
00:00:59,590 --> 00:01:10,490
So let's start right now by defining a class deep sea and then that we will inherit from Kira's that

15
00:01:10,920 --> 00:01:11,330
model.

16
00:01:11,340 --> 00:01:19,320
So we are defining all own model by inheriting from the T air that carries that model plus.

17
00:01:19,330 --> 00:01:22,570
So we will build our own initialization function.

18
00:01:22,570 --> 00:01:30,140
The call function and we will define all the layers that will be used and right how they should be used.

19
00:01:30,160 --> 00:01:35,660
So when we do that we always have to define these function in it.

20
00:01:35,670 --> 00:01:44,160
That has to be implemented for any model or labor intensive flow and its methods in class will take

21
00:01:44,160 --> 00:01:51,480
self as first parameter and then we just have to tell all the parameters that we wanted to use in order

22
00:01:51,480 --> 00:01:58,380
to build our models so we will need first the size of the vocabulary so we don't know why it will be

23
00:01:58,380 --> 00:02:05,770
yes it will just be given by NISA then we need dimension for the embedding phase.

24
00:02:05,810 --> 00:02:10,880
Let us set some random while not fully random but intuitive.

25
00:02:10,890 --> 00:02:13,620
Default values for those parameters.

26
00:02:13,620 --> 00:02:16,230
Then the number of filters.

27
00:02:16,290 --> 00:02:19,880
So that means for each type of convolution I'll filter.

28
00:02:20,290 --> 00:02:23,480
It's just the number of time that we want to apply it.

29
00:02:23,670 --> 00:02:30,270
Let's say that by default it would be 50 because of course in the paper they said two filters for each

30
00:02:30,270 --> 00:02:31,140
type of filter.

31
00:02:31,560 --> 00:02:34,040
But that was just so that it can fit in one image.

32
00:02:34,050 --> 00:02:39,930
We wanted to use many more of them in order to have a cloud forming model that can learn a lot.

33
00:02:39,930 --> 00:02:51,780
So let's say 50 as a default value then we will need the number of units for the feed for network at

34
00:02:51,780 --> 00:03:01,520
the end and let's sets 512 as default values the number of classes for all classification task.

35
00:03:01,530 --> 00:03:06,050
Let's say that default would be to the drop out rates.

36
00:03:06,080 --> 00:03:06,930
This is during the training.

37
00:03:06,930 --> 00:03:14,220
This is a tool that we use to kind of turn off certain parameters and certain valuables in order to

38
00:03:14,220 --> 00:03:15,480
avoid all the fitting.

39
00:03:15,480 --> 00:03:16,620
That's really standard.

40
00:03:16,770 --> 00:03:24,070
Let's say by default it's 0.01 we would use a training boolean.

41
00:03:24,180 --> 00:03:32,080
So that's the network knows if it's in the training phase or if it's an A evaluation phase.

42
00:03:32,160 --> 00:03:37,180
And this is mainly used in order to know if we wanted to apply the dropouts or not because the drop

43
00:03:37,180 --> 00:03:47,650
outs is only applied to doing training and finally let's give a name to our model does CNN.

44
00:03:47,710 --> 00:03:47,950
Okay.

45
00:03:47,950 --> 00:03:54,900
And first thing we want to do here is to call the init function from the class we are inheriting from.

46
00:03:54,970 --> 00:04:04,000
So this is done by calling super and we have to give the name of the class we are writing right now

47
00:04:04,510 --> 00:04:06,670
self and form super.

48
00:04:07,570 --> 00:04:17,830
We call in its and we just give it the name and that's it's normally our model is initialized properly

49
00:04:18,130 --> 00:04:22,770
using the initialize function from the air that carries that model class.

50
00:04:22,780 --> 00:04:26,680
So now we can start defining the layers that we will use.

51
00:04:26,680 --> 00:04:31,510
So we saw that the first one is an embedding layer equals layers.

52
00:04:31,510 --> 00:04:41,300
That's embedding and for the embedding we need to have the size of the vocabulary and the dimension

53
00:04:41,300 --> 00:04:44,280
of the embedding that we want to achieve.

54
00:04:44,300 --> 00:04:47,630
Now let's define the three different types of features.

55
00:04:47,720 --> 00:04:52,190
So the first one that we call diagram will be layers.

56
00:04:52,190 --> 00:04:53,030
That's.

57
00:04:53,240 --> 00:05:00,950
So it's one dimension convolution layer because the width is the model and we just apply the filter

58
00:05:01,040 --> 00:05:02,240
along one dimension.

59
00:05:02,240 --> 00:05:12,320
So layers that cover on the the number of those features that we want is no filters.

60
00:05:12,500 --> 00:05:18,340
It's all by grams so kernel size which is the.

61
00:05:18,380 --> 00:05:28,820
So the size of our filter would be two for this one putting will be valid in our case it's not really

62
00:05:28,820 --> 00:05:35,480
important which potting method we use because I just tried to we'll be equal to 1 which means that we

63
00:05:35,480 --> 00:05:39,760
will apply our filter by sliding its word by word.

64
00:05:40,040 --> 00:05:48,710
But if we want let's say to just try a number of two or three the last iterations of our filter might

65
00:05:48,710 --> 00:05:51,930
go further than the length of our sequence.

66
00:05:52,010 --> 00:05:58,880
So if we want to apply the filter as many times as possible and thus having the maximum information

67
00:05:58,880 --> 00:06:04,880
we can validate is usually the project method that we wanted to use because it adds the zeros we need

68
00:06:04,970 --> 00:06:07,580
in order to perform from the last convolutions.

69
00:06:07,580 --> 00:06:14,600
But as I said right here we wouldn't change anything because we use astride of one and let's set the

70
00:06:14,720 --> 00:06:26,390
activation function to be really which is very standards and works perfectly well in this kind of situation.

71
00:06:26,700 --> 00:06:37,170
And let's define the pulling layer that we will apply to those filters layers that's global.

72
00:06:38,200 --> 00:06:46,840
Max pool 1 D as well working with one dimensional convolution all layers.

73
00:06:46,850 --> 00:06:47,930
Let's do the same thing.

74
00:06:48,530 --> 00:06:58,040
And by doing the same thing I mean copy pasts with the next type of filters which we call tri gram.

75
00:06:58,280 --> 00:07:03,570
So it's exactly the same thing but this time the kernel size will be three.

76
00:07:03,710 --> 00:07:09,010
And we we will just change the name of the pulling layer right here.

77
00:07:09,680 --> 00:07:21,080
And finally for the third time we will define the foreground again exactly the same principle.

78
00:07:21,090 --> 00:07:26,070
But the kernel size is for and we change this name.

79
00:07:26,070 --> 00:07:33,630
So we have the main parts of our model and all we have to do is to define after the concatenation the

80
00:07:33,690 --> 00:07:38,790
dense layers that are used in order to finally gets the classification task done.

81
00:07:38,800 --> 00:07:54,000
So we first apply a dense layer that will be layers that tense that will be built with a number of units

82
00:07:54,150 --> 00:07:59,370
equal to f f and units.

83
00:07:59,370 --> 00:08:02,910
And again let's use a standard activation function.

84
00:08:02,910 --> 00:08:12,590
So really let us college drop outs key because there is a lot of parameters he and lots of links between

85
00:08:12,650 --> 00:08:13,570
valuables.

86
00:08:13,590 --> 00:08:22,130
This is a good place to apply drop outs in order to avoid the fittings so let's define this layer about

87
00:08:22,160 --> 00:08:23,150
equals layers.

88
00:08:23,150 --> 00:08:34,840
That's drop out rates equals dropout rates and finally we just have to define the last linear function.

89
00:08:34,840 --> 00:08:40,070
The Last Dance layer and the way we will do that it depends on how many classes we have.

90
00:08:40,180 --> 00:08:48,220
If we have two classes then we want to have as outputs just one number which should be between 0 and

91
00:08:48,220 --> 00:08:54,630
1 and if it's below 0 point 5 it means that it belongs to the class 0.

92
00:08:54,640 --> 00:08:59,330
And if it's above all point 5 it means that it belongs to the class 1.

93
00:09:00,010 --> 00:09:00,620
So.

94
00:09:01,060 --> 00:09:02,800
So let's write this.

95
00:09:02,800 --> 00:09:17,560
If the number of classes is 2 then lasts dance layer will be defined but by layers that's dense units

96
00:09:18,310 --> 00:09:30,640
equals 1 and we will use the activation function sigmoid which basically takes any number between minus

97
00:09:30,640 --> 00:09:38,470
infinity and Infinity and we tend to number between 0 and 1 so this is what we always use when we are

98
00:09:38,470 --> 00:09:40,700
doing binary classification.

99
00:09:41,020 --> 00:09:51,070
And if we have more than two classes so let's just say s in this case the last linear function will

100
00:09:51,070 --> 00:10:01,930
be layers that dance and the number of units will be the number of classes and the activation function

101
00:10:01,930 --> 00:10:03,980
will be soft Max.

102
00:10:04,680 --> 00:10:10,900
We've solved Max we get real probabilities all the output numbers will be between 0 and 1 and the sum

103
00:10:10,990 --> 00:10:12,850
will be equal to 1.

104
00:10:12,850 --> 00:10:17,830
This is exactly what probabilities are so that's the best way to end a classification task with more

105
00:10:17,830 --> 00:10:20,300
than two classes.

106
00:10:20,320 --> 00:10:26,840
So now we have all the defining phase but it's done so we can just define the call function.

107
00:10:26,840 --> 00:10:30,950
So this is the function that is used when we call our model.

108
00:10:30,950 --> 00:10:37,010
So basically when we give inputs and we want to get outputs out of it this is the call function that

109
00:10:37,010 --> 00:10:38,310
is used the code methods.

110
00:10:38,600 --> 00:10:42,470
So we always need to use self at first.

111
00:10:43,190 --> 00:10:49,460
Then the call function will use the inputs and these boolean training that we will need in order to

112
00:10:49,790 --> 00:10:55,550
know whether we are in a training phase or not and thus if we wanted to apply the dropouts or not so

113
00:10:56,720 --> 00:11:04,630
first let's say that we apply the embedding to our inputs so X will be equal to serve.

114
00:11:04,640 --> 00:11:10,450
That's embedding inputs and let's define the three outputs of our different filters.

115
00:11:10,460 --> 00:11:15,020
So let's say that x 1 is self.

116
00:11:15,020 --> 00:11:24,650
That's by gram applied to X and Y then wants to apply the pulling to this result.

117
00:11:24,650 --> 00:11:41,180
So x1 equals surf that's pool 1 applied to x 1 then let's do the same thing with x 2 which is the results

118
00:11:41,240 --> 00:11:43,850
of the tri gram.

119
00:11:43,850 --> 00:11:53,170
This one applied to X so that's correct and we apply the pulling to X2 and sending for the foreground

120
00:11:54,460 --> 00:11:55,720
applied to X.

121
00:11:55,750 --> 00:12:01,700
And here this is x3 that we wanted to apply the pulling 2.

122
00:12:01,720 --> 00:12:17,550
So let's merge all that's so merged equals T F dot com cuts and we applied to X1 X2 and X3 that sits

123
00:12:18,210 --> 00:12:25,260
along the axis minus one so along the last axis at this point the first axis represents the batches

124
00:12:25,620 --> 00:12:31,690
the second axis represents the value that we get after applying the max pulling so we wanted to conquer

125
00:12:31,690 --> 00:12:33,750
at all those last values.

126
00:12:33,750 --> 00:12:44,900
And so what we get right after this is something of the shape batch size 3 times no filters because

127
00:12:44,900 --> 00:12:52,310
this number of features P three times once for the diagram then for the program and finally for the

128
00:12:52,310 --> 00:12:55,970
four grams so this is the shape of what we get now.

129
00:12:56,120 --> 00:13:06,220
Next step is to apply our firsts dense layer so that we start the feet forward process.

130
00:13:06,590 --> 00:13:10,470
So let's apply dense 1 to merged.

131
00:13:11,030 --> 00:13:22,360
Then we apply the drop outs right now to merged and we say if it's a training phase or not.

132
00:13:22,550 --> 00:13:29,020
And finally we can get all outputs by calling the lasts dense layer.

133
00:13:29,990 --> 00:13:35,300
And here we are now that we have outputs all we have to do is to return.

134
00:13:35,300 --> 00:13:36,470
It's perfect.

135
00:13:36,470 --> 00:13:44,120
So now we have a fully functional module that has been initialized and can be called to get outputs

136
00:13:44,120 --> 00:13:46,010
from any inputs.

137
00:13:46,010 --> 00:13:46,810
So here we are.

138
00:13:46,810 --> 00:13:48,210
Our model is built.

139
00:13:48,290 --> 00:13:53,600
Now all we have to do is to trainings and we will finally be ready to see if we get good results out

140
00:13:53,600 --> 00:13:56,290
of its popular excited to see how it goes.

141
00:13:56,450 --> 00:13:57,040
And Susan.
