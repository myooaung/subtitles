1
00:00:00,120 --> 00:00:02,330
Hi and welcome back to this and because.

2
00:00:02,400 --> 00:00:08,750
So let's start this part about CNN for an by talking about the origins of CNN and how they were first

3
00:00:08,760 --> 00:00:10,050
applied for images.

4
00:00:10,050 --> 00:00:16,470
So for comes division and then we'll see how we can change them to apply CNN's for either peace or for

5
00:00:16,470 --> 00:00:17,490
texts.

6
00:00:17,490 --> 00:00:19,410
So first what does a CNN do.

7
00:00:19,410 --> 00:00:23,940
Basically it's a neural network that we'll take as inputs in the image and output.

8
00:00:23,940 --> 00:00:25,650
It will give us a label.

9
00:00:25,710 --> 00:00:31,650
So as I said in the previous section it could be for instance a number if all input image is a written

10
00:00:31,650 --> 00:00:36,540
number or it could be a class saying whether there is a plane or not in the image.

11
00:00:36,540 --> 00:00:38,210
So many different things.

12
00:00:38,310 --> 00:00:43,230
And it's been a huge part in the A.I. evolution throughout the years with a lot of contests for the

13
00:00:43,230 --> 00:00:47,490
best performing CNN for this kind of classification tasks.

14
00:00:47,490 --> 00:00:50,150
So what are the standard steps for CNN.

15
00:00:50,280 --> 00:00:54,720
The first one is the most important one what we call the convolution.

16
00:00:54,780 --> 00:00:59,580
So this is the step where we create a lot of small features detectors that will go through the whole

17
00:00:59,580 --> 00:01:06,270
image and we will end up with a list of feature maps that will tell us whether or not and where a specific

18
00:01:06,270 --> 00:01:08,390
feature appears in the image.

19
00:01:08,400 --> 00:01:10,470
Then the second step is the max pulling.

20
00:01:10,470 --> 00:01:17,220
So basically we just apply maximum function to all feature maps so that we can make them smaller and

21
00:01:17,220 --> 00:01:19,050
improve the performance of the model.

22
00:01:19,110 --> 00:01:24,870
Then there is this very simple flattening phase where we simply make a huge vector out of the maps which

23
00:01:24,870 --> 00:01:29,070
are mattresses and we finally end the model with a full connection.

24
00:01:29,070 --> 00:01:35,700
So a fit for a network that is a very standard neural network and that we learn from the feature extraction

25
00:01:35,700 --> 00:01:36,330
phase.

26
00:01:36,330 --> 00:01:39,180
How to give a label to all inputs.

27
00:01:39,180 --> 00:01:40,400
So let's go with the first phase.

28
00:01:40,410 --> 00:01:41,110
The convolution.

29
00:01:41,110 --> 00:01:44,240
So first what is an image for us humans.

30
00:01:44,250 --> 00:01:46,600
And the majority of animals.

31
00:01:46,710 --> 00:01:48,570
This is light that comes into our eyes.

32
00:01:48,630 --> 00:01:51,180
But for our computer it must be a list of numbers.

33
00:01:51,450 --> 00:01:56,820
So what we do is that we split anything that we want to represent into very very small areas.

34
00:01:56,850 --> 00:01:59,130
So could be peak sales for instance.

35
00:01:59,130 --> 00:02:01,580
And we put a number in this to simplify it.

36
00:02:01,590 --> 00:02:04,650
We will say that all images will be in black and white.

37
00:02:04,680 --> 00:02:11,770
And so for each of those very small areas or pixel the higher the number the darker this area will be.

38
00:02:11,790 --> 00:02:19,040
And this is how we go from this smiley face to the simplified one with a lot of small areas.

39
00:02:19,140 --> 00:02:20,490
And then finally a matrix.

40
00:02:20,520 --> 00:02:23,780
So at least a numbers that will represent this image.

41
00:02:23,820 --> 00:02:29,460
Of course if the image is in colors we will need three matrices one for each other the primary colors.

42
00:02:29,460 --> 00:02:34,710
But for now we will stick with the black and white version and we don't even need to think about those

43
00:02:34,710 --> 00:02:38,290
three channels those three colors when we will go into the energy parts.

44
00:02:38,310 --> 00:02:40,070
So let's stick with it.

45
00:02:40,130 --> 00:02:45,720
And now that we have our image that is represented as a matrix of numbers and that is usable by our

46
00:02:45,720 --> 00:02:46,290
computer.

47
00:02:46,470 --> 00:02:47,900
Let's see what we do with it.

48
00:02:47,910 --> 00:02:54,510
So the convolution phase consists of a lot of feature detector that are actually small mattresses that

49
00:02:54,510 --> 00:02:56,910
we will apply to the whole image.

50
00:02:56,910 --> 00:02:59,400
So how does this matrix obligation work.

51
00:02:59,400 --> 00:03:05,030
We go through patches in the original image that have the same size as our feature detector.

52
00:03:05,040 --> 00:03:11,400
Our filter and what we do is that we don't do a matrix multiplication standard matrix multiplication.

53
00:03:11,400 --> 00:03:13,430
We do an element y's multiplication.

54
00:03:13,440 --> 00:03:19,170
So what it means is that we all take for instance the top left number of this patch and when we multiply

55
00:03:19,170 --> 00:03:25,670
it by the top left number of our patch of our feature detector and we do the same with all the numbers.

56
00:03:25,700 --> 00:03:31,230
So top middle number multiplied by the top middle number here and we sum all of these results.

57
00:03:31,230 --> 00:03:35,970
So if we have a look at the first batch right here where we wanted to apply our feature detector we

58
00:03:35,970 --> 00:03:42,870
can see that there is no coalition between the once in this patch and the ones in these feature detector.

59
00:03:43,140 --> 00:03:48,820
So here for instance the 0 will be multiplied by the one and this one with an image played by 0.

60
00:03:48,990 --> 00:03:55,500
So the end result which is a single number is 0 and we will put it on the most top left box of our final

61
00:03:55,500 --> 00:03:56,330
feature map.

62
00:03:56,490 --> 00:03:59,370
Then we will repeat the process with the next batch.

63
00:03:59,370 --> 00:04:03,750
We just wanted to shift our feature detector by 1 to the right.

64
00:04:03,750 --> 00:04:08,730
And here we see that there is a correlation between the patch and the feature detector which is this

65
00:04:08,730 --> 00:04:14,670
one way we have a one in our patch from the image and the one in this feature detector.

66
00:04:14,670 --> 00:04:20,380
So when we multiply the patch with the feature detector element y's and then we sum everything we get

67
00:04:20,380 --> 00:04:26,850
the one that we just put here in the second element of our final future map and we repeat the process

68
00:04:28,360 --> 00:04:32,640
shifting off the two detector by one to the right when we go at the end of the line.

69
00:04:32,830 --> 00:04:37,200
We repeat the same thing just shifted by one towards the bottom of the image.

70
00:04:37,360 --> 00:04:39,760
We repeat this as many times as we need.

71
00:04:39,760 --> 00:04:43,240
Finally we will get at this point right before the end.

72
00:04:43,240 --> 00:04:46,180
We can double check the computation if we want.

73
00:04:46,240 --> 00:04:49,670
We can see that we will get ones from this top right spot.

74
00:04:49,690 --> 00:04:56,410
She where we have once for the patch and the feature detector and sending for the middle left parts

75
00:04:56,410 --> 00:04:58,090
here where we have to once.

76
00:04:58,090 --> 00:04:59,900
So when we sum everything we get it too.

77
00:04:59,930 --> 00:05:01,530
And that's what we see appearing here.

78
00:05:02,110 --> 00:05:07,800
And finally the last one when we get one again from the middle left parts of the patch.

79
00:05:08,320 --> 00:05:12,550
So that's the result of one convolution applied with only one feature detector.

80
00:05:13,120 --> 00:05:17,080
So this feature that is detected might not make any sense for us humans.

81
00:05:17,080 --> 00:05:20,510
It could be some computation with numbers that we don't really get.

82
00:05:20,680 --> 00:05:27,040
But when they are all put together and fed to a standard fit for a network the conventional neural network

83
00:05:27,070 --> 00:05:32,230
can achieve great results with it but sometimes it is something that is related to what we can understand

84
00:05:32,230 --> 00:05:33,190
as humans.

85
00:05:33,250 --> 00:05:39,730
For instance if I take the example of the handwritten digits recognition one of the feature could be

86
00:05:40,000 --> 00:05:47,080
to detect if there is a little cross somewhere in the image and if it appears if so if the feature detector

87
00:05:47,110 --> 00:05:52,740
is activated somewhere then there is a high probability that our number will be eight.

88
00:05:52,750 --> 00:05:56,200
So that's the kind of thing that the convolution old parts could do.

89
00:05:56,200 --> 00:06:01,700
And of course we went through an example for only one feature detector but the I.D. that's all CNN will

90
00:06:01,720 --> 00:06:06,630
have a lot of those feature detector and each detector will provide a feature map.

91
00:06:06,760 --> 00:06:12,880
Last thing that is important to have in mind is that those numbers right here they are of course randomly

92
00:06:12,880 --> 00:06:17,800
initialized and those are the valuables that are learned throughout the process of the training of the

93
00:06:17,800 --> 00:06:18,510
model.

94
00:06:18,520 --> 00:06:25,600
So after each prediction after comparing the results with the actual shoe label the model will tune

95
00:06:25,720 --> 00:06:30,730
those numbers in order to learn to detect more useful and accurate features.

96
00:06:30,730 --> 00:06:35,980
So it's interesting to see what those feature detectors look like at the end of the training because

97
00:06:36,430 --> 00:06:43,000
those will be according to our model the most useful feature to detect in the imaging although to perform

98
00:06:43,030 --> 00:06:44,200
all classification tasks.

99
00:06:44,980 --> 00:06:52,000
So the results after this operation is this we have our original image and we get at the end lists of

100
00:06:52,000 --> 00:06:52,680
features maps.

101
00:06:52,690 --> 00:06:56,310
So each of this map corresponds to a different feature detector.

102
00:06:56,320 --> 00:06:59,920
So a different convolution or filter and we are ready to go to the next phase.

103
00:06:59,950 --> 00:07:03,460
So this next step is the max pulling.

104
00:07:03,550 --> 00:07:07,550
So it's just a maximum operation that is performed for each map.

105
00:07:07,720 --> 00:07:14,710
And the idea is to reduce the size of it and the cost of course of the computation by being more global.

106
00:07:14,710 --> 00:07:16,620
So it's not a global maximum.

107
00:07:16,690 --> 00:07:20,770
We don't take just the higher number for each feature map but we will still reduce the size.

108
00:07:20,770 --> 00:07:26,890
So for instance in this example we will apply a much pulling for two times two patches.

109
00:07:26,920 --> 00:07:33,040
So we will take three years of size to two and we'll just take the higher number and put it in a new

110
00:07:33,230 --> 00:07:34,090
pooled feature map.

111
00:07:34,120 --> 00:07:41,740
So that's for the first area that we are looking for we at one then it will shift its by the size of

112
00:07:41,800 --> 00:07:42,220
our patch.

113
00:07:42,220 --> 00:07:47,320
So we will go directly to the next spots without having anything in common.

114
00:07:47,320 --> 00:07:50,680
And we get a new two by two patch where we apply a maximum.

115
00:07:50,680 --> 00:07:52,930
Again we get one and it's not important.

116
00:07:52,930 --> 00:07:59,240
If we go a little bit out of bounds The idea is that we get all the numbers that can be in our fetch

117
00:07:59,250 --> 00:08:04,690
him up and we speed the process again shifting by 2 this time because we don't want to have any number

118
00:08:04,780 --> 00:08:08,710
or any rule that appears in two different Mexico operation.

119
00:08:08,710 --> 00:08:12,610
Here we get a four that will be here and so on.

120
00:08:12,610 --> 00:08:17,770
Finally we get to the last part which contains only one number but that's not important.

121
00:08:17,770 --> 00:08:22,780
The important thing is just that this number had the opportunity to appear somewhere and that nothing

122
00:08:22,870 --> 00:08:25,040
appears two times in our Mexico operation.

123
00:08:25,060 --> 00:08:28,120
So we get here all pooled feature maps.

124
00:08:28,120 --> 00:08:33,790
So the idea is that we don't need to know everything about whether the feature appeared in the image

125
00:08:33,850 --> 00:08:34,420
or not.

126
00:08:34,450 --> 00:08:36,100
We don't need to know everywhere.

127
00:08:36,100 --> 00:08:39,600
It doesn't appear the most important thing is to know whether it appears or not.

128
00:08:39,640 --> 00:08:42,700
But we still want to keep some kind of locality in this process.

129
00:08:42,700 --> 00:08:48,700
We want to know more or less where the feature appeared because in any major positions of the elements

130
00:08:48,730 --> 00:08:49,720
are very important.

131
00:08:49,750 --> 00:08:55,240
So we still want to keep the relations between the positions of the features that is kind of a tradeoff

132
00:08:55,270 --> 00:09:01,210
between getting rid of the information we don't need and also making the maps smaller so that we can

133
00:09:01,210 --> 00:09:03,220
improve the process and reduce the costs.

134
00:09:03,220 --> 00:09:08,150
And the fact that we still want to keep an idea of the positions of elements in the image.

135
00:09:08,320 --> 00:09:09,700
So that was for the max pulling.

136
00:09:09,700 --> 00:09:16,800
Of course we apply this operation to each of our features map and we get the same number of pooled maps.

137
00:09:17,080 --> 00:09:17,890
Then the next step.

138
00:09:17,890 --> 00:09:19,510
Very simple just the flattening.

139
00:09:19,510 --> 00:09:23,670
So the idea is that in order to apply a stand on your network.

140
00:09:23,700 --> 00:09:27,310
So a fully connected network which teal to have a simple vector.

141
00:09:27,310 --> 00:09:30,450
So we just transform each matrix.

142
00:09:30,460 --> 00:09:33,040
So it's called feature map into a vector.

143
00:09:33,270 --> 00:09:34,890
There's nothing complicated about that.

144
00:09:34,890 --> 00:09:35,700
There is just.

145
00:09:35,910 --> 00:09:39,260
We take the first row which is the first part of the vector.

146
00:09:39,270 --> 00:09:42,930
Then we just add the second row to this vector and the third row.

147
00:09:42,960 --> 00:09:47,190
Same thing we added at the end of the vector so we get a vector out of the matrix.

148
00:09:47,190 --> 00:09:53,580
The important thing is that this flattening operation will always proceed the same way so that the locality

149
00:09:53,580 --> 00:09:57,300
that we had there the relations between the positions is kept here.

150
00:09:57,360 --> 00:10:02,730
So it's not in the same form it's less visual but the information is still there because there is an

151
00:10:02,730 --> 00:10:07,710
order in our numbers that directly comes from the order that we had in the matrix.

152
00:10:07,710 --> 00:10:14,850
So we still keep the order and the relations between the positions so that's it's out of those maps

153
00:10:14,880 --> 00:10:20,070
those mattresses we get to vector of course the vector is much bigger than this but this is just for

154
00:10:20,070 --> 00:10:26,190
the sake of the representations and we get something that could be valid inputs for our feed for network.

155
00:10:26,370 --> 00:10:32,760
So a quick look at our architecture are at the steps so far we have all original image and we apply

156
00:10:33,060 --> 00:10:37,310
many different feature detectors to it in the convolution phase.

157
00:10:37,320 --> 00:10:39,750
So we get a lot of feature maps.

158
00:10:39,750 --> 00:10:44,190
We apply a pulling to reaches them but still keeping valuable information.

159
00:10:44,310 --> 00:10:51,230
And we flatten everything in order to have vectors instead of the list of matches as that's its last

160
00:10:51,230 --> 00:10:54,620
step of all convolution of four images is the full connection.

161
00:10:54,620 --> 00:11:00,380
So it's a standard fully connected network that will learn the classification tasks from all convolution

162
00:11:00,380 --> 00:11:02,120
or steps.

163
00:11:02,120 --> 00:11:03,680
It's just very standard.

164
00:11:03,680 --> 00:11:06,070
We have all input vector.

165
00:11:06,080 --> 00:11:09,370
We can add as many he denies here that we want.

166
00:11:09,380 --> 00:11:13,070
So just new ones that we learn useful stuff throughout the process.

167
00:11:13,070 --> 00:11:18,070
And finally we get our output value so hey we decided that for days representation.

168
00:11:18,080 --> 00:11:24,410
That's the classification is to label a classification so all image should be put into two different

169
00:11:24,410 --> 00:11:25,160
classes.

170
00:11:25,190 --> 00:11:28,110
Of course we can have many more Newman's eyes outputs.

171
00:11:28,190 --> 00:11:33,800
In our example for the handwritten digits of course we will have 10 different new ones one for each

172
00:11:33,800 --> 00:11:39,530
number and the higher the number here the higher the probability that's our input image corresponds

173
00:11:39,530 --> 00:11:40,610
to this number.

174
00:11:40,850 --> 00:11:46,730
So usually the relation between a new one and all the new ones from the previous layer is that we multiply

175
00:11:46,820 --> 00:11:48,260
each number by a coefficient.

176
00:11:48,260 --> 00:11:53,780
So by a weight we sum everything and then we apply and only in our function most of the time which will

177
00:11:53,780 --> 00:11:54,830
be what we call really.

178
00:11:55,100 --> 00:12:00,440
So it basically gets rid of all the negative values putting them to zero and we keep all the positive

179
00:12:00,440 --> 00:12:04,270
values which we consider to be the activated values.

180
00:12:04,340 --> 00:12:08,400
So in all implementation this will just corresponds to dense layers.

181
00:12:08,420 --> 00:12:11,180
Nothing really new about that's and that's it.

182
00:12:11,210 --> 00:12:15,450
So here is the final summary of all hope CNN for images.

183
00:12:15,500 --> 00:12:17,810
Input image which is made of numbers.

184
00:12:17,810 --> 00:12:23,750
Of course the matrix of numbers we apply different features detectors and we get a list of feature maps

185
00:12:23,870 --> 00:12:29,030
that our network learns it learns what each conventional filter will be made of.

186
00:12:29,060 --> 00:12:32,390
So what are the most useful feature to look for.

187
00:12:32,480 --> 00:12:37,450
Then we apply a local maximum to reduce the size of all feature maps.

188
00:12:37,610 --> 00:12:44,210
And finally we apply a fit for network to our flattens information so to a whole vector to get as many

189
00:12:44,210 --> 00:12:47,770
new wins as the number of classes that are possible.

190
00:12:47,840 --> 00:12:52,040
So that was it for the quick introduction to CNN for images.

191
00:12:52,070 --> 00:12:58,520
Probably was clear enough and nothing we're ready to see how we can apply it to tests because intuitively

192
00:12:58,580 --> 00:13:00,590
takes is quite different from images.

193
00:13:00,620 --> 00:13:06,290
So first we have to see how we can make the texts similar to Indian age in order to apply these kind

194
00:13:06,290 --> 00:13:12,680
of algorithms and also we have to see what all the small changes and then we have to apply to the architecture

195
00:13:12,860 --> 00:13:18,420
to the whole structure in order to get good performances with our CNN popularity for that and to use

196
00:13:18,420 --> 00:13:18,630
using.
