WEBVTT
1
00:00:05.400 --> 00:00:09.500
Hello everyone and welcome to this section on ARIMA models.

2
00:00:09.610 --> 00:00:14.410
So we're not going to discuss one of the most common time series models Arema which stands for our aggressive

3
00:00:14.440 --> 00:00:16.020
integrated moving averages.

4
00:00:16.210 --> 00:00:21.700
And please note that this is an optional section of the course so for various reasons that we're going

5
00:00:21.700 --> 00:00:26.280
to be discovering later on Arema models by themselves really often don't work well.

6
00:00:26.290 --> 00:00:32.290
Historical stock or securities data however they are so commonly used and so fundamental to understanding

7
00:00:32.290 --> 00:00:35.880
time series analysis that it's still worth the time to go over them.

8
00:00:36.010 --> 00:00:42.370
So for other time series analysis things such as trying to predict how many sales a store will do during

9
00:00:42.370 --> 00:00:43.590
the winter or in the summer.

10
00:00:43.720 --> 00:00:48.580
Things of that nature Arema models are fantastic when it comes to actual financial data.

11
00:00:48.580 --> 00:00:51.860
They don't tend to actually work that well and later on we'll discover why.

12
00:00:51.880 --> 00:00:57.430
But in order to do that we actually understand how Arema models work and as we continue with this topic

13
00:00:57.430 --> 00:01:01.020
please keep in mind that Arema models can actually be quite complex.

14
00:01:01.060 --> 00:01:05.560
So make sure to make full use of the various links and extra resources that are presented throughout

15
00:01:05.560 --> 00:01:06.430
this section.

16
00:01:06.430 --> 00:01:11.760
If you want to later use Arema models for other applications or other problems.

17
00:01:11.770 --> 00:01:14.140
All right so let's discuss Ario models in general.

18
00:01:14.140 --> 00:01:19.510
So there are a regressive integrated moving average models and it's a generalization of another model

19
00:01:19.510 --> 00:01:24.210
known as the Armah model or auto regressive moving average.

20
00:01:24.260 --> 00:01:29.660
Both of these models Arema Armah or fitted to time series data either to better understand the data

21
00:01:29.660 --> 00:01:32.660
itself or to predict future points in the series.

22
00:01:32.660 --> 00:01:36.910
This is known as forecasting Arema models have two types.

23
00:01:36.910 --> 00:01:39.980
There is non seasonal Arema and then there are seasonal arena.

24
00:01:39.990 --> 00:01:45.100
And as you can probably guess not seasonal or the models are for non seasonal data and seasonal Orio

25
00:01:45.110 --> 00:01:47.520
models are for seasonal data.

26
00:01:47.740 --> 00:01:51.940
So we're going to start off by discussing non-sequel Arema models and then we'll move on to seasonal

27
00:01:51.970 --> 00:01:55.820
Arema models in the Python examples that we'll be doing at the end of this section.

28
00:01:55.820 --> 00:01:59.410
The course will be using seasonal Arema because it's kind of the more complex one.

29
00:01:59.440 --> 00:02:05.970
And if you understand seasonal Arema you'll understand basic arena OK or even models are applied.

30
00:02:05.970 --> 00:02:09.570
In some cases where they to show evidence of non stationary.

31
00:02:09.600 --> 00:02:14.970
That is where an initial differencing step corresponding to the integrated part of the model can be

32
00:02:14.970 --> 00:02:21.930
applied one or more times to eliminate this non stationary so differencing is actually a very simple

33
00:02:21.930 --> 00:02:26.690
idea but will put it on hold for now and talk a little bit more about Arema in general.

34
00:02:26.790 --> 00:02:29.910
So latish back on what that term differencing actually means.

35
00:02:29.910 --> 00:02:31.260
It's a pretty simple concept.

36
00:02:31.410 --> 00:02:34.550
But for now I want to talk about the major component of AREMA.

37
00:02:34.710 --> 00:02:36.420
That is the auto regressive portion.

38
00:02:36.420 --> 00:02:42.760
The integrated portion and that moving average portion so non seasonal ARIMA models are generally the

39
00:02:42.760 --> 00:02:49.720
noted as Arema p d q where the parameters P D and Q are non-negative integers.

40
00:02:49.720 --> 00:02:51.630
So let's discuss each of these components.

41
00:02:52.740 --> 00:02:56.670
We'll start off with the auto regression component that is that p component.

42
00:02:56.670 --> 00:03:02.460
A regression model that utilizes the dependent relationship between the current observation and observations

43
00:03:02.460 --> 00:03:04.060
over a previous period.

44
00:03:04.090 --> 00:03:07.750
So you can just imagine this as kind of a basic regression task.

45
00:03:07.750 --> 00:03:10.410
So that's that portion of the model.

46
00:03:10.430 --> 00:03:13.430
Then there's the integrated portion of the model and what that is.

47
00:03:13.440 --> 00:03:19.250
It's differencing of observations that is subtracting an observation from an observation at that previous

48
00:03:19.250 --> 00:03:22.320
time step in order to make the time series stationary.

49
00:03:22.370 --> 00:03:26.380
And we'll discuss differencing in a lot more detail in just a little bit.

50
00:03:27.570 --> 00:03:31.080
And this last model that we have is the moving average portion of the model.

51
00:03:31.110 --> 00:03:32.700
Otherwise known as the q part.

52
00:03:32.910 --> 00:03:38.640
And that's a model that utilizes the dependency between an observation and the residual air from the

53
00:03:38.640 --> 00:03:42.300
moving average model apply to lags observations.

54
00:03:43.810 --> 00:03:48.660
OK so we talked about these terms stationery versus non stationary data.

55
00:03:48.700 --> 00:03:54.160
Let's actually discuss what those terms are and how we can visually see in data whether it's stationary

56
00:03:54.190 --> 00:03:55.940
or non stationary.

57
00:03:55.930 --> 00:03:59.830
So to effectively use Arema we need to understand stationary in our data.

58
00:03:59.830 --> 00:04:07.310
So what makes the data stationary stationary series has a constant mean and variance over time a stationary

59
00:04:07.310 --> 00:04:13.280
dataset will allow our model to predict that the mean and variance will be the same in future periods.

60
00:04:13.280 --> 00:04:19.100
Let's take a look at a few examples visually and a few key elements that will allow us to easily identify

61
00:04:19.100 --> 00:04:21.470
what a stationary dataset looks like.

62
00:04:22.280 --> 00:04:27.710
The first key part of a stationary versey non stationary dataset is that the meaning has to be constant.

63
00:04:27.890 --> 00:04:30.960
So the average value has to be constant throughout time.

64
00:04:31.190 --> 00:04:34.860
So we can see here on our left in green we have a stationary dataset.

65
00:04:35.000 --> 00:04:40.520
So even though there's some variance there we can see that on average that mean value is constant versus

66
00:04:40.520 --> 00:04:46.010
non stationary data has a non-constant mean and we can see here we have this upward trend which means

67
00:04:46.010 --> 00:04:52.780
that the average value is slowly increasing over time the other thing that we need to take note on is

68
00:04:52.780 --> 00:04:55.270
that variance should not be a function of time.

69
00:04:55.450 --> 00:05:00.190
So on the left we have stationary again where we can see that the actual variance tends to stay the

70
00:05:00.190 --> 00:05:05.590
same over time here on the right we have non stationary data where you can see here that the variance

71
00:05:05.590 --> 00:05:09.880
starts off small and then it gets bigger than it was small and it goes big again.

72
00:05:09.880 --> 00:05:13.600
And somehow we can see that the variance is changing over time.

73
00:05:13.600 --> 00:05:18.700
Starting small Going big small big versus on the left we can see that the variance whether it's big

74
00:05:18.700 --> 00:05:25.210
or small it's not changing as a function of time as you go along the x axis and then the last aspect

75
00:05:25.210 --> 00:05:29.390
that we should be aware of is that covariance should not be a function of time.

76
00:05:29.510 --> 00:05:32.880
And visually you can see this in that non stationary data set.

77
00:05:32.890 --> 00:05:38.500
So on the right we can see that non stationary even though that means tends to look average and even

78
00:05:38.500 --> 00:05:41.090
the variance tends to look average or covariance.

79
00:05:41.230 --> 00:05:46.470
That is visually how fast that variance is changing a long time is not stationary.

80
00:05:46.480 --> 00:05:50.590
So you can see that cycle of a slow change in variance and then a quick change in variance.

81
00:05:50.620 --> 00:05:53.620
A slow change in variance and a quick change in variance again.

82
00:05:53.650 --> 00:05:58.090
So this one's a little harder to detect visually but you should be aware and there are mathematical

83
00:05:58.090 --> 00:06:00.720
tests for this OK.

84
00:06:00.830 --> 00:06:04.730
So as I mentioned there's mathematical tests so you don't just have to kind of look at data you can

85
00:06:04.730 --> 00:06:10.280
actually mathematically test it to see if has stationary in it and a really common test is known as

86
00:06:10.280 --> 00:06:11.090
the augmented.

87
00:06:11.090 --> 00:06:16.450
TICKY FULLERTON test and we're going to see how we can use that test with pythoness that's models.

88
00:06:17.820 --> 00:06:22.980
So if you've determined your data is not stationary either visually or mathematically with that sort

89
00:06:22.980 --> 00:06:28.730
of decie full or test you will then need to transform it to be stationary in order to evaluate it.

90
00:06:28.800 --> 00:06:31.100
And what type of Arema terms you're going to be using.

91
00:06:32.290 --> 00:06:36.940
So one simple way to do this that we previously mentioned is through differencing and the actual idea

92
00:06:36.940 --> 00:06:38.670
behind differencing is really quite simple.

93
00:06:38.680 --> 00:06:40.390
So let's go ahead and see an example.

94
00:06:42.470 --> 00:06:44.810
On the left we have the original dataset.

95
00:06:44.810 --> 00:06:50.900
So imagine we have some data set and it goes at time point 1 2 3 4 and 5 and then we have various values

96
00:06:50.900 --> 00:06:53.340
for it 10 12 a 14 and 7.

97
00:06:53.360 --> 00:07:00.350
The very first difference what you end up doing is at time let's say to you subtract the previous time

98
00:07:00.350 --> 00:07:03.220
value and then set that as your new value.

99
00:07:03.230 --> 00:07:05.950
So we have 12 minus 10 is equal to 2.

100
00:07:06.020 --> 00:07:10.840
And then for time 3 we end up doing 8 minus 12 is equal to negative 4 etc..

101
00:07:10.850 --> 00:07:12.590
So that's known as the first difference.

102
00:07:12.590 --> 00:07:20.120
Basically you take the value at time equals C and subtract that at the value of T minus 1 and that's

103
00:07:20.120 --> 00:07:21.360
your first difference.

104
00:07:21.500 --> 00:07:26.870
If you take a first difference and your data is not yet stationary after you either check it visually

105
00:07:26.870 --> 00:07:31.870
or do have mathematical tests on it we can do is then take the second difference and second difference.

106
00:07:31.880 --> 00:07:36.530
Same idea except now we're starting at time point three and doing the same subtraction.

107
00:07:36.540 --> 00:07:39.610
So negative for minus to get negative 6.

108
00:07:39.680 --> 00:07:43.700
Then we'd say 6 minus 4 or minus negative 4.

109
00:07:43.730 --> 00:07:49.850
That gives you plus 10 then minus 7 minus 6 will and that gives you minus 13 etc. and you can keep taking

110
00:07:49.850 --> 00:07:51.680
on as many differences as you need.

111
00:07:51.710 --> 00:07:56.540
Just keep in mind that if you keep taking more and more differences you end up sacrificing a row of

112
00:07:56.540 --> 00:07:58.750
data for every difference that you take.

113
00:08:00.360 --> 00:08:05.190
So as I just mentioned you can continue differencing until you reach stat. which you can check visually

114
00:08:05.190 --> 00:08:10.380
and mathematically each differencing step comes at the cost of losing a row of data which is not that

115
00:08:10.380 --> 00:08:11.200
huge of a cost.

116
00:08:11.220 --> 00:08:18.130
If you have a very large dataset what's nice about this idea of differencing is that for seasonal data

117
00:08:18.220 --> 00:08:20.260
you can also differenced by a season.

118
00:08:20.470 --> 00:08:26.230
For example if you had a monthly data with yearly seasonality that is every single row of data you had

119
00:08:26.530 --> 00:08:32.320
represented one month of data you could difference by a time of 12 instead of just one.

120
00:08:32.560 --> 00:08:36.880
And with pand those that would be really easy you would just use the dot shift method and instead of

121
00:08:36.880 --> 00:08:40.700
applying one to it you would apply 12 to it.

122
00:08:40.850 --> 00:08:45.980
In other common technique with seasonal ARIMA models is to combine both methods that is taking the seasonal

123
00:08:45.980 --> 00:08:47.900
difference of the first difference.

124
00:08:47.930 --> 00:08:52.000
So you end up taking the first reference and then after that you end up taking the seasonal difference.

125
00:08:52.010 --> 00:08:56.930
If your season has to be yearly then you just do another shift by 12.

126
00:08:56.930 --> 00:09:03.260
So now if your data in a stationary time series we can go back and discuss the p and q terms and how

127
00:09:03.260 --> 00:09:07.130
we actually choose those values so we end up doing is you take your data.

128
00:09:07.220 --> 00:09:08.530
You check that it's stationary.

129
00:09:08.540 --> 00:09:12.690
You can use those differencing techniques we talked about to make sure it's stationary and then you

130
00:09:12.690 --> 00:09:18.920
need to choose the p and q values to actually apply the auto regressive integrated and moving average

131
00:09:18.920 --> 00:09:24.950
portions of the Arema model and a big part of how to actually choose those values has to do with autocorrelation

132
00:09:24.950 --> 00:09:27.510
plots and partial autocorrelation plots.

133
00:09:27.530 --> 00:09:31.370
So let's move on to discuss those type of plots in the very next lecture.

134
00:09:31.370 --> 00:09:32.510
Thanks and I'll see you there.
