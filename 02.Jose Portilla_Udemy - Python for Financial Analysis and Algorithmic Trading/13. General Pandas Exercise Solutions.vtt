WEBVTT
1
00:00:05.410 --> 00:00:10.570
Welcome back everyone in this lecture will be going through the solutions for the pandas exercises problems.

2
00:00:10.780 --> 00:00:13.110
Let's open up the notebook and get started.

3
00:00:13.120 --> 00:00:13.340
OK.

4
00:00:13.360 --> 00:00:15.930
Here I am at the Pandurs exercise notebook.

5
00:00:15.940 --> 00:00:21.430
We actually did the first two in the previous lecture importing pandas and reading in that CXXVI file.

6
00:00:21.520 --> 00:00:23.190
So that issue the head of the data frame.

7
00:00:23.200 --> 00:00:29.170
All you have to do is say thanks the head and you'll get back the first five rows so it looks like we

8
00:00:29.170 --> 00:00:35.020
have a bank name column a city column a state column some sort of certification number column the acquiring

9
00:00:35.020 --> 00:00:38.050
institution the closing day and then the updated date.

10
00:00:38.050 --> 00:00:40.570
So we'll continue along and answer the rest of the questions.

11
00:00:40.570 --> 00:00:41.740
What are the column names.

12
00:00:41.740 --> 00:00:48.340
Well you can do that easily just by asking for banks columns and then you'll receive back all the column

13
00:00:48.340 --> 00:00:49.090
names.

14
00:00:49.180 --> 00:00:54.640
So hopefully the output that you saw there kind of hinted that you just wanted to call the columns.

15
00:00:54.670 --> 00:00:58.470
You could also just grab this directly from viewing the data frame itself.

16
00:00:58.480 --> 00:01:03.820
But you'll notice that if you had a bunch of columns here eventually you'll get some of them cut off

17
00:01:03.820 --> 00:01:05.580
in the middle say dot dot dot.

18
00:01:05.590 --> 00:01:11.640
So a good way to get all the column names just Bascombe for the attribute columns next question was

19
00:01:11.640 --> 00:01:14.280
how many states are represented in this dataset.

20
00:01:14.350 --> 00:01:18.940
The way you can do that is just ask for the s t.

21
00:01:18.960 --> 00:01:24.180
That is the state's column and then call an unique on this.

22
00:01:24.210 --> 00:01:30.600
Which brings back the number of unique states in this dataset which is 44 states to get a list of all

23
00:01:30.600 --> 00:01:36.320
the states in the dataset where you end up doing is you copy the exact same code but instead of saying

24
00:01:36.330 --> 00:01:41.910
number of unique just ask for unique itself and just return back an array of all the unique states in

25
00:01:41.910 --> 00:01:45.070
this case the state codes themselves.

26
00:01:45.070 --> 00:01:48.760
Now the question is What are the top five states with the most failed of banks.

27
00:01:48.790 --> 00:01:53.980
There are lots and lots of ways you can answer this question are going to show you one method some one

28
00:01:53.980 --> 00:01:55.020
layer method.

29
00:01:55.030 --> 00:01:57.930
So first off we want to say what are the banks with top five states.

30
00:01:58.030 --> 00:02:01.980
Which means we should probably group by the states and then do some sort of count.

31
00:02:02.020 --> 00:02:10.800
So say Banks Group by as T and then do a count of this.

32
00:02:10.820 --> 00:02:15.890
So if you do that you end up getting the states and then account for each single column.

33
00:02:15.900 --> 00:02:19.740
So I notice that the numbers are all the same it kind of makes sense.

34
00:02:19.790 --> 00:02:21.800
All the rows should be read the same way.

35
00:02:21.800 --> 00:02:25.930
So count should return the same number regardless of what column you're calling here.

36
00:02:25.940 --> 00:02:29.860
So right now we have all the states and how many failed banks they have.

37
00:02:29.870 --> 00:02:31.400
The problem is it's not sorted.

38
00:02:31.460 --> 00:02:38.240
So what we can do is sort it soon we're going to say sort's underscore values and I'm going to sort

39
00:02:38.240 --> 00:02:44.780
by bank name and if you just do that you'll end up getting descending order so you could do it.

40
00:02:44.780 --> 00:02:51.350
You could call the tail of this to see the bottom five and you can see it's Georgia Florida Illinois

41
00:02:51.350 --> 00:02:53.130
California and Minnesota.

42
00:02:53.570 --> 00:02:55.290
And that would essentially be your answer.

43
00:02:55.310 --> 00:02:59.880
Now to get the exact answer I got here in this format you could continue along with this.

44
00:03:00.070 --> 00:03:08.050
Instead of saying tale where you could do is say something like sending equals true.

45
00:03:08.420 --> 00:03:11.530
And then there are excuse me sending calls false.

46
00:03:11.960 --> 00:03:14.430
So that we see the very top ones.

47
00:03:14.450 --> 00:03:15.910
So if something gives you false.

48
00:03:15.980 --> 00:03:18.060
Now you see the top five right here.

49
00:03:18.130 --> 00:03:24.290
And if you want to only get the top five then you could just say something like I see for integer location

50
00:03:24.320 --> 00:03:29.570
and then say give me everything from the beginning up to five and then you get back this data frame.

51
00:03:29.570 --> 00:03:33.610
If you still only want it not the entire data frame but just the single count.

52
00:03:33.620 --> 00:03:37.850
You could just ask for any of these columns back so you could say something like at the very end of

53
00:03:37.850 --> 00:03:39.200
this call.

54
00:03:39.200 --> 00:03:42.590
Bank name and that's how you get the exact answer I got.

55
00:03:42.590 --> 00:03:48.380
Now clearly you could get the answer just for a few of these steps sorting it asking for the tail.

56
00:03:48.410 --> 00:03:53.870
It's really up to you if you want to continue by adding in ascending IOC and then bank name.

57
00:03:53.870 --> 00:03:54.100
All right.

58
00:03:54.110 --> 00:03:56.730
So that's how you can get the top five states the most failed banks.

59
00:03:56.810 --> 00:03:59.390
Here they are as well as their numbers.

60
00:03:59.390 --> 00:04:02.450
Now we want to know what are the top 5 acquiring institutions.

61
00:04:02.510 --> 00:04:09.790
So that's kind of similar to that that you just asked Banks and then passen acquiring institution and

62
00:04:09.800 --> 00:04:13.670
you could do something like value counts.

63
00:04:14.860 --> 00:04:19.830
And if you get back value counts you'll end up getting in order sorted by the actual counts the acquiring

64
00:04:19.830 --> 00:04:26.940
institutions and if you want the top five you could just say I'll go see everything up to five if you're

65
00:04:26.940 --> 00:04:30.180
including no Choire as an institution that would be it.

66
00:04:30.180 --> 00:04:35.370
If you don't want include that you do a top six and then get back this where I hear community and southern

67
00:04:35.370 --> 00:04:35.970
bank.

68
00:04:36.000 --> 00:04:37.180
So it's one quick way to do that.

69
00:04:37.200 --> 00:04:41.670
And you could see that it could actually similarly do something like this for this previous problem.

70
00:04:41.670 --> 00:04:43.340
What are the top five states.

71
00:04:43.350 --> 00:04:48.140
So again many many multiple ways to do this and it kind of depends on what your style of working with

72
00:04:48.150 --> 00:04:49.500
panaceas.

73
00:04:49.530 --> 00:04:54.480
So next question is how many banks has the State Bank of Texas acquired How many of them were actually

74
00:04:54.480 --> 00:04:55.740
in Texas.

75
00:04:55.740 --> 00:04:57.390
So let's do this.

76
00:04:57.390 --> 00:05:07.770
To do that you would ask banks get the acquiring institution column said equal to state bank of Texas

77
00:05:08.610 --> 00:05:15.400
and then pass that into banks run that and this will get you back to the data frame where State Bank

78
00:05:15.430 --> 00:05:19.990
Texas happens to be the acquiring institution and we can see here that only one of them happens to be

79
00:05:19.990 --> 00:05:21.550
in Texas Dallas Texas.

80
00:05:21.550 --> 00:05:24.770
The other two are in Chicago which is kind of interesting.

81
00:05:24.820 --> 00:05:26.350
So we'll continue along.

82
00:05:26.380 --> 00:05:29.730
Now what is the most common City in California for a bank to fail in.

83
00:05:30.010 --> 00:05:31.630
Let's kind of break this down.

84
00:05:31.630 --> 00:05:37.060
First we need to do is make sure that the list the banks we get back are actually in California.

85
00:05:37.180 --> 00:05:47.470
So I'll say banks where banks s t is equal to the California Kotas just CA and then sent back all the

86
00:05:47.470 --> 00:05:49.210
banks that are in California.

87
00:05:49.220 --> 00:05:51.290
But you'll notice here that there's multiple cities.

88
00:05:51.370 --> 00:05:54.290
So I'm going to do is group them by city.

89
00:05:54.380 --> 00:06:03.160
So say group by city and then I'm going to count how many there are.

90
00:06:03.450 --> 00:06:09.030
So now we get back this data frame which is all the banks in California route by city and then count

91
00:06:09.180 --> 00:06:10.810
how many instances there are.

92
00:06:10.890 --> 00:06:14.260
So we can see here that there's 1 1 2 4 et cetera.

93
00:06:14.460 --> 00:06:18.240
So in order to not have to scroll down and do this manually what we could do is just sort the valleys

94
00:06:18.240 --> 00:06:32.740
off of this so I could say sort values by bank name and then we'll say sending is equal to false

95
00:06:35.740 --> 00:06:39.550
run that and then I can see the top on right here is Los Angeles.

96
00:06:39.550 --> 00:06:42.540
So that's the city in California with the most failed banks.

97
00:06:42.550 --> 00:06:47.620
And if you just wanted that one you could at the end of this say something like head one and that just

98
00:06:47.620 --> 00:06:51.510
returns Los Angeles which is how I did get this answer right here.

99
00:06:51.520 --> 00:06:53.560
Again many ways you could have done this.

100
00:06:53.560 --> 00:06:57.910
You could also work to value accounts to kind of simplify this but hopefully this show you can up all

101
00:06:57.910 --> 00:06:59.270
the steps that we're doing here.

102
00:07:00.590 --> 00:07:05.240
So our next question was how many banks don't have the word bank in their name.

103
00:07:05.450 --> 00:07:06.770
So let's show you how you can do that.

104
00:07:08.210 --> 00:07:15.460
One way you could do it is by saying banks and then we're going to say bank name.

105
00:07:15.860 --> 00:07:19.600
So this gives me a series of all the banks and I want to know which of these don't have the word bank

106
00:07:19.610 --> 00:07:20.130
their name.

107
00:07:20.130 --> 00:07:21.560
So I'm going to do the following.

108
00:07:21.710 --> 00:07:28.740
I'm going to apply lambda expression weren't going to pass in that name which is the bank name.

109
00:07:28.980 --> 00:07:36.690
And then I'm going to check is bank not in name.

110
00:07:36.770 --> 00:07:43.970
So then if I run this I'll get back a series of bullying values false false true etc. and then what

111
00:07:43.970 --> 00:07:50.760
I can do just off of this is say summit and it will give me back the number 14.

112
00:07:50.770 --> 00:07:51.040
All right.

113
00:07:51.040 --> 00:07:54.490
And the next question was how many banks name start with the letter S.

114
00:07:54.490 --> 00:07:56.790
So that's going to be a pretty similar problem.

115
00:07:56.860 --> 00:08:07.170
We're going to do is say banks and then grab the bank name column and then I'm going to apply the and

116
00:08:07.210 --> 00:08:17.930
I want to return where the very first value here uppercase is equal to uppercase S and you can do the

117
00:08:17.930 --> 00:08:20.350
same thing with lower and lower case s.

118
00:08:20.570 --> 00:08:23.470
So if you get that you end up getting something that looks like this.

119
00:08:23.510 --> 00:08:25.230
So false false false true.

120
00:08:25.430 --> 00:08:30.380
So then what you could do is you could either then just ask for the value counts and then count where

121
00:08:30.380 --> 00:08:31.210
this is true.

122
00:08:31.230 --> 00:08:37.020
See a fifty three cases of this being true which means where name zero that is equal to s.

123
00:08:37.100 --> 00:08:39.820
Or we could also do is like we did before.

124
00:08:40.100 --> 00:08:43.980
Take the sum of this and you get back 53.

125
00:08:44.450 --> 00:08:48.680
Next question was how many values are above the number 20000.

126
00:08:48.680 --> 00:08:58.800
So you could just say banks where Sir is greater than 20000 that returns back this and then after that

127
00:08:58.860 --> 00:09:00.320
you could just take the sum.

128
00:09:00.600 --> 00:09:04.230
And again you could add value counts there as well as 4:17.

129
00:09:04.320 --> 00:09:06.680
Next was how many banks consist of just two words.

130
00:09:06.690 --> 00:09:08.610
First Bank and Bank of Georgia.

131
00:09:08.910 --> 00:09:10.650
So you can do that is the following.

132
00:09:10.650 --> 00:09:13.440
This one gets a little creative with the lambda expressions.

133
00:09:13.440 --> 00:09:18.830
So say bank think name and we are going to apply the following.

134
00:09:18.830 --> 00:09:26.080
I'll say Lamda taken the name and give me back I'm going to split the name.

135
00:09:26.210 --> 00:09:30.930
So now I have a list and then off of that I'm going to check the link for this list.

136
00:09:34.410 --> 00:09:37.150
Once I've done that I can check a fit equals two.

137
00:09:37.530 --> 00:09:43.010
And that should return back trues and falses that's returning true or the length of that list.

138
00:09:43.020 --> 00:09:48.480
So sample first bank split is going to be first comma Bank and Bank of Georgia is going to bank George

139
00:09:48.480 --> 00:09:51.080
is going to be bank of Georgia etc..

140
00:09:51.140 --> 00:09:57.210
So and then as we've done before I can just take the sum of this and it turns back 114.

141
00:09:57.290 --> 00:10:01.370
Now the bonus question so the bonus question is how many banks closed in the year 2008.

142
00:10:01.370 --> 00:10:03.640
It's a lot better it's work with time series data.

143
00:10:03.830 --> 00:10:10.100
But if you take a look at our banks data frame by saying info you'll notice that the closing date and

144
00:10:10.130 --> 00:10:15.140
updated day date time objects are just objects so essentially as in the case that they're strings.

145
00:10:15.140 --> 00:10:19.160
So that means we can't do any fancy date time manipulations with them.

146
00:10:19.160 --> 00:10:22.750
So what we're going to do instead is work with them with strings.

147
00:10:22.760 --> 00:10:27.740
Let's take a look at the head of our bank's data frame one more time and then notice we have closing

148
00:10:27.740 --> 00:10:34.130
date an updated date and it looks like the last two digits here are the last two elements or characters

149
00:10:34.130 --> 00:10:36.660
of the string happened to be the year.

150
00:10:36.740 --> 00:10:43.490
So we're going to be looking for the string 0 8 and one by string I mean just 0 8 in quotes.

151
00:10:43.700 --> 00:10:45.800
So let's try that logic.

152
00:10:45.800 --> 00:10:54.100
I'm going to say banks closing dates and I'm going to apply the following lambda expression I'm going

153
00:10:54.140 --> 00:11:02.110
to taken the dates as a string and then using dates negative to Colan which means go back to space's

154
00:11:02.110 --> 00:11:05.650
from the end of the string and then get me everything to the end.

155
00:11:06.290 --> 00:11:17.840
Check if that is equal to the string 0 8 and then after that one I'm going to do is take the sum of

156
00:11:17.840 --> 00:11:22.260
it and that returns back 25 banks.

157
00:11:22.280 --> 00:11:28.280
That happens to be failed and the year 0 8 and you can kind of experiment this so many banks failed

158
00:11:28.340 --> 00:11:29.460
in 0 7.

159
00:11:29.520 --> 00:11:34.130
So many banks failed in 0 9 and there's a big jump after the 2008 financial crisis.

160
00:11:34.130 --> 00:11:35.450
Now there is a better way to do this.

161
00:11:35.450 --> 00:11:37.190
It has to do with date time objects.

162
00:11:37.190 --> 00:11:42.780
The weak do that is by using PD to date time which are going to learn a lot more about later on.

163
00:11:43.010 --> 00:11:50.940
And you could pass in banks closing date column and this actually returns back date time object for

164
00:11:50.940 --> 00:11:51.740
each of these.

165
00:11:51.870 --> 00:11:54.340
And that means you could do something like the following of this.

166
00:11:54.360 --> 00:12:00.400
You could say apply and you'll land the expression and make a lot more sense you just pass some object

167
00:12:00.910 --> 00:12:10.210
and then ask for the year out where the year happened to be equal to 2008 and then off of that you could

168
00:12:10.210 --> 00:12:14.120
take some of this and that would also return 25.

169
00:12:14.120 --> 00:12:18.830
So we're going to learn a lot more about how to deal to date time in a feature section of the course.

170
00:12:18.890 --> 00:12:19.220
OK.

171
00:12:19.250 --> 00:12:20.440
That's it for the exercises.

172
00:12:20.480 --> 00:12:24.260
I hope they were pretty challenging the ones that should have been the most challenging are these ones

173
00:12:24.260 --> 00:12:27.930
with the land the expressions that gets you to really think about this especially if you have to do

174
00:12:27.930 --> 00:12:29.030
it on one line.

175
00:12:29.210 --> 00:12:31.470
If you just pass on a function here becomes a lot easier.

176
00:12:31.580 --> 00:12:36.920
But thinking about how to break down the idea into a single line land expression really test your knowledge

177
00:12:36.950 --> 00:12:38.950
of land expressions in Pandas.

178
00:12:39.020 --> 00:12:43.310
Don't worry if you are able to get these right away these are definitely a lot more challenging and

179
00:12:43.310 --> 00:12:46.550
we won't have to do them that much during the financial portion of this course.

180
00:12:46.770 --> 00:12:47.180
OK.

181
00:12:47.210 --> 00:12:49.870
If you have any questions feel free to post them to any forums.

182
00:12:50.060 --> 00:12:52.490
Thanks everyone and I'll see you at the next section of the course.
