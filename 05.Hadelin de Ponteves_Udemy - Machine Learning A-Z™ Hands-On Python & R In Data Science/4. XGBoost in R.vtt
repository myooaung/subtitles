WEBVTT
1
00:00:00.360 --> 00:00:04.400
Hello and welcome to this art tutorial so you learn a lot of stuff.

2
00:00:04.530 --> 00:00:11.040
But it would be a shame to leave this course without having some introduction to one of the most popular

3
00:00:11.310 --> 00:00:17.850
algorithm in machinery that is quite recently popular but still it is definitely a very powerful model

4
00:00:18.030 --> 00:00:20.580
especially if you work on large data sets.

5
00:00:20.580 --> 00:00:24.490
It will offer you very high performance while being fast to execute.

6
00:00:24.720 --> 00:00:30.930
And speaking of performance and execution speed it is important to remind that ex-GI boost is the most

7
00:00:30.930 --> 00:00:37.490
powerful implementation of gradient boosting in terms of model performance and execution speed.

8
00:00:37.530 --> 00:00:41.320
Therefore it's very important for you to have it in your toolkit.

9
00:00:41.400 --> 00:00:43.230
So let's implement extra boost.

10
00:00:43.230 --> 00:00:48.970
This is only going to be an introduction so we will make a simple implementation of actually boost.

11
00:00:49.050 --> 00:00:54.810
But you will have to contemplate on your computer and you'll be able to try it on your problems on your

12
00:00:54.810 --> 00:01:00.090
data sets and you'll see that even with this simple implementation it will definitely give you some

13
00:01:00.150 --> 00:01:01.740
excellent performance.

14
00:01:01.990 --> 00:01:06.900
And now what we're going to do is take one of the business problem we dealt with in this course.

15
00:01:06.900 --> 00:01:11.850
This is going to be actually the problem that we solved in the deep learning section.

16
00:01:11.850 --> 00:01:17.400
Remember this is the churn modeling problem where we need to predict the customers of the bank that

17
00:01:17.400 --> 00:01:18.390
will leave the bank.

18
00:01:18.540 --> 00:01:24.240
So this was a classification problem where we classify the customers in two classes those who will leave

19
00:01:24.240 --> 00:01:26.860
the bank and those who will not leave the bank.

20
00:01:26.880 --> 00:01:31.140
And so remember for this problem we obtained an accuracy of 86 percent.

21
00:01:31.260 --> 00:01:38.070
But that took quite a while because we trained an artificial neural network with many bugs and therefore

22
00:01:38.070 --> 00:01:40.170
it took quite some time to execute.

23
00:01:40.200 --> 00:01:42.850
And so now in this section we're going to do the same.

24
00:01:42.850 --> 00:01:46.480
We're going to apply extreme views on this churn modeling problem.

25
00:01:46.500 --> 00:01:47.850
This data set contains.

26
00:01:47.850 --> 00:01:51.710
If I remember 13 features but that's not a large dataset.

27
00:01:51.780 --> 00:01:57.750
And what is important to highlight is that even if this was a large data set a very large data set well

28
00:01:57.810 --> 00:02:01.650
extra boost would be one of the best model in terms of performance.

29
00:02:01.710 --> 00:02:05.810
That is to get a good accuracy and execution speed.

30
00:02:05.820 --> 00:02:12.530
So for example if you are working with a large data set I strongly encourage you to test ex-GI boost.

31
00:02:12.530 --> 00:02:17.100
All right so we're going to do now is take the pre processing phase.

32
00:02:17.180 --> 00:02:21.910
So that is only part 1 because part 2 is to implement the artificial neural network.

33
00:02:22.040 --> 00:02:27.740
And so we just want to preprocess the data for this churn modeling problem associated with this churn

34
00:02:27.740 --> 00:02:29.540
modeling as we fall.

35
00:02:29.990 --> 00:02:34.850
But actually we're not going to take everything in this pre-processing phase.

36
00:02:34.850 --> 00:02:41.360
The reason is that for the artificial neural network will feature scaling was totally compulsory no

37
00:02:41.360 --> 00:02:45.610
questions asked features going must be applied for deep learning.

38
00:02:45.800 --> 00:02:51.440
But the good news is that for actually boost world since actually boost is a great and boosting model

39
00:02:51.440 --> 00:02:56.960
with decision trees well accordingly features killing is totally unnecessary and thats one of the very

40
00:02:56.960 --> 00:03:02.520
good thing about ex-GI boos Besides its high performance and its fast execution speed.

41
00:03:02.590 --> 00:03:08.210
Its that you can keep the interpretation of your problem of your data set and half the results youll

42
00:03:08.210 --> 00:03:15.170
get after building the model so we can understand now why ex-GI Bruce is so popular its because it has

43
00:03:15.170 --> 00:03:22.520
the three qualities first quality high performance second quality first execution speed and third quality.

44
00:03:22.520 --> 00:03:26.240
You can keep all the interpretation of your problem and your model.

45
00:03:26.300 --> 00:03:29.320
So definitely a model to have your tool kit.

46
00:03:29.330 --> 00:03:35.630
All right so features killing is necessary here and therefore we will take everything from here up to

47
00:03:35.840 --> 00:03:42.360
the top like this copy and we'll paste that in our extreme use File.

48
00:03:42.380 --> 00:03:42.870
Here we go.

49
00:03:42.890 --> 00:03:45.850
And now we can implement extremist.

50
00:03:45.860 --> 00:03:54.710
So first let's introduce a new section fitting ex-GI boost to the trainset.

51
00:03:54.840 --> 00:03:55.670
All right.

52
00:03:55.670 --> 00:03:58.540
And first let's start actually.

53
00:03:58.820 --> 00:04:01.180
So as usual there's a package.

54
00:04:01.180 --> 00:04:05.870
It's the extra boost package that will allow us to implement extra booze very efficiently.

55
00:04:05.870 --> 00:04:07.560
So let's stop here as usual.

56
00:04:07.600 --> 00:04:17.380
Install that packages and inside the name of the extra package which is simply X G boost like this.

57
00:04:17.390 --> 00:04:22.910
Then you select this line and press command control enter to execute.

58
00:04:22.910 --> 00:04:25.900
And that's installing the extra boost package.

59
00:04:26.190 --> 00:04:26.540
Right.

60
00:04:26.540 --> 00:04:28.720
We can see it's precessing.

61
00:04:28.970 --> 00:04:33.690
And here we get to downloaded binary packages are in this package folder.

62
00:04:33.770 --> 00:04:36.160
All good ex-GI boosters installed.

63
00:04:36.410 --> 00:04:41.150
So let's put that section in comment there.

64
00:04:41.270 --> 00:04:46.150
And now let's import the expertise package because indeed we installed it.

65
00:04:46.340 --> 00:04:51.580
But if we go down to the bottom extreme was installed but not important.

66
00:04:51.680 --> 00:04:58.540
And we want to make it romantic so as is your music library and inside X G boost.

67
00:04:58.990 --> 00:05:01.540
And that's we'll import the package.

68
00:05:01.580 --> 00:05:02.350
All right.

69
00:05:02.350 --> 00:05:04.890
And now let's implement actually boost.

70
00:05:05.000 --> 00:05:10.100
And actually this is going to take one line because we're just going to make the classifier that extra

71
00:05:10.090 --> 00:05:11.640
boost class for itself.

72
00:05:11.870 --> 00:05:18.530
And so basically we just need to create a new variable that we call as usual classifier and then equals

73
00:05:18.740 --> 00:05:22.570
and then we use the extra boost function from this extra package.

74
00:05:22.700 --> 00:05:32.440
So extra boost and parenthesis and let's click here press one and get some information about this actually

75
00:05:32.450 --> 00:05:33.680
use function.

76
00:05:33.680 --> 00:05:39.520
So the information is we're interested in the arguments and what arguments do we need here.

77
00:05:39.890 --> 00:05:46.460
OK so first we see that we have this harams parameter which is actually a list of parameters and these

78
00:05:46.460 --> 00:05:49.340
parameters are all the parameters that you can see here.

79
00:05:49.370 --> 00:05:55.250
For example the operand of that controls the learning rate the gamma parameter which is the minimum

80
00:05:55.250 --> 00:05:56.320
loss reduction.

81
00:05:56.450 --> 00:05:58.480
Well you have a lot of these parameters.

82
00:05:58.610 --> 00:06:02.810
But this tutorial is just an introduction of X to boost.

83
00:06:02.870 --> 00:06:08.870
So we will not do some chewing on our model and this course but I'm sure in some future courses I will

84
00:06:08.870 --> 00:06:14.720
make some more complex implementations of extra boost on some more complex problems.

85
00:06:14.720 --> 00:06:19.460
But in this course it's just to end with a simple introduction of actually boost so that you can at

86
00:06:19.460 --> 00:06:23.350
least have some knowledge about it and have it in you target.

87
00:06:23.360 --> 00:06:29.900
So let's not focus on this now and let's move on to the compulsory parameters that are of course the

88
00:06:29.900 --> 00:06:31.220
first one is data.

89
00:06:31.220 --> 00:06:36.870
So there is of course your trainset data set on which you want to train your extremely small role.

90
00:06:36.890 --> 00:06:38.600
And so let's put that right now.

91
00:06:38.630 --> 00:06:44.260
So first argument data equals then training set.

92
00:06:44.330 --> 00:06:45.240
Here we go.

93
00:06:45.410 --> 00:06:50.810
And actually here we only need the features in the transits So we will remove the dependent variable

94
00:06:50.810 --> 00:06:56.200
from this training set because the train set contains both the features and that have been invaluable.

95
00:06:56.390 --> 00:07:00.550
But what this data parameter expects is only the features.

96
00:07:00.560 --> 00:07:05.030
So here we add some brackets and we remove that had been invaluable.

97
00:07:05.170 --> 00:07:06.390
And what does this index.

98
00:07:06.500 --> 00:07:11.510
Well to do this we need to import the data set first before importing the data set.

99
00:07:11.510 --> 00:07:14.510
Let's quickly set the rifle there as working directory.

100
00:07:14.510 --> 00:07:18.380
So right now we're in parts and then Section 49 extra boost.

101
00:07:18.590 --> 00:07:19.600
That's the right there.

102
00:07:19.610 --> 00:07:24.940
Make sure that you have to turn Mullings file and then click on sads working directory here.

103
00:07:25.040 --> 00:07:25.850
And here we go.

104
00:07:25.880 --> 00:07:27.780
Now we can import the data set.

105
00:07:28.010 --> 00:07:29.170
Let's import it.

106
00:07:29.240 --> 00:07:30.820
And that's the data set.

107
00:07:30.920 --> 00:07:35.840
But remember in this dataset we don't take all the independent variables because we're not interested

108
00:07:35.840 --> 00:07:42.990
in row number customer ID and surname we know that these revivals have no impact on the dependent variable.

109
00:07:43.070 --> 00:07:49.790
So we remove them and that's what we do in this line dataset equals data set for 14.

110
00:07:49.790 --> 00:07:55.790
That means that we take all the variables from the fourth variable of data set that is credit score

111
00:07:56.030 --> 00:07:58.950
up to the last variable exited the depende revival.

112
00:07:58.950 --> 00:08:00.540
That's the dependent variable.

113
00:08:00.650 --> 00:08:04.430
And so let's take this line and execute.

114
00:08:04.520 --> 00:08:11.420
And now if you look at our dataset Well this contains all the relevant features and to deepen invertible

115
00:08:11.480 --> 00:08:17.990
exited and so the challenge is with all these independent variables here we want to predict if the customer

116
00:08:17.990 --> 00:08:20.210
will leave or stay in the bank.

117
00:08:20.210 --> 00:08:26.930
And so that's the data set we consider to train the model and test its performance and therefore the

118
00:08:26.930 --> 00:08:32.360
index of the dependent variable we have to remove Now in the extreme lose function for the data parameter

119
00:08:32.520 --> 00:08:35.460
is the last index here of the exited column.

120
00:08:35.570 --> 00:08:40.960
And since we have 11 variables well that index is 11.

121
00:08:40.970 --> 00:08:47.510
So let's go back to actually boost and let's go back to our function and therefore here we have 2 inputs

122
00:08:47.780 --> 00:08:49.910
minus 11.

123
00:08:49.910 --> 00:08:54.930
All right so we have our whole training set but without the division rivals That's think that's exactly

124
00:08:54.930 --> 00:08:55.920
what we want.

125
00:08:55.920 --> 00:09:00.590
So now let's go back to help to see if we need some more info about this first parameter.

126
00:09:00.840 --> 00:09:06.810
Well indeed there is some very important information that we need to consider here is that this data

127
00:09:06.810 --> 00:09:10.650
set needs to be an X to be the matrix.

128
00:09:10.650 --> 00:09:18.420
So that's basically a type of Matrix but we can also see that in addition data the data parameter also

129
00:09:18.420 --> 00:09:20.340
accepts Matrix.

130
00:09:20.340 --> 00:09:22.400
But this is not a matrix.

131
00:09:22.400 --> 00:09:28.830
This is a data frame so this won't work if we input the features this way so we can either convert this

132
00:09:29.070 --> 00:09:33.690
into an to beat up the matrix or a simple matrix.

133
00:09:33.690 --> 00:09:39.780
So let's take the simplest solution let's convert this data frame features into a matrix and you know

134
00:09:39.780 --> 00:09:40.750
how to do this.

135
00:09:40.800 --> 00:09:48.850
We just need to use that as dot matrix function and put inside some parenthesis because it's a function.

136
00:09:48.900 --> 00:09:51.180
This data frame of features.

137
00:09:51.180 --> 00:09:51.800
Here we go.

138
00:09:51.870 --> 00:09:55.650
And now this becomes a matrix and that's exactly what we need.

139
00:09:55.650 --> 00:09:58.380
All right perfect then next argument.

140
00:09:58.380 --> 00:10:03.810
So here again you have a lot of other arguments but these are not compulsories we won't focus on them

141
00:10:03.810 --> 00:10:10.940
now but the next compulsory argument is this label argument because indeed here we can put the matrix

142
00:10:10.940 --> 00:10:16.290
of features but of course to train a classification model we need not only the matrix of features but

143
00:10:16.290 --> 00:10:18.230
also that have been invaluable.

144
00:10:18.360 --> 00:10:21.870
And that's what we put in this label parameter.

145
00:10:21.870 --> 00:10:28.680
And so as you might expect since we put the features into a matrix Well we need to put this label parameter

146
00:10:29.190 --> 00:10:37.530
as a vector and to get our dependent variable as vector we need to input label equals our training sets

147
00:10:38.370 --> 00:10:39.250
than dollar.

148
00:10:39.360 --> 00:10:45.160
And then we take the name of our dependent variable which is exited and this will give us a vector.

149
00:10:45.180 --> 00:10:48.380
So training said Dollar exited is the dependent variable.

150
00:10:48.420 --> 00:10:55.810
But given as a vector that's exactly what we need because as you can see label is expected to be a vector.

151
00:10:55.830 --> 00:11:02.010
The vector of response values the response values are of course the values of the dependent variable.

152
00:11:02.460 --> 00:11:03.120
All right.

153
00:11:03.240 --> 00:11:06.700
Now next argument what is the next argument.

154
00:11:06.780 --> 00:11:13.510
Well there is a third compulsory argument that we need to input here and that is actually above.

155
00:11:13.530 --> 00:11:18.540
But I wanted to put the label after the matrix of features that made kind of sense.

156
00:11:18.870 --> 00:11:25.080
And now there is a third argument that we need to input which is the and round's argument and round's

157
00:11:25.110 --> 00:11:28.330
arguments is the maximum number of iterations.

158
00:11:28.350 --> 00:11:33.990
So since we're not working on the too complex problem well the maximum number of 10 iterations will

159
00:11:33.990 --> 00:11:34.880
be sufficient.

160
00:11:35.070 --> 00:11:43.620
So we will input here and around as equals 10 an extra boost will be trained in maximum ten iterations.

161
00:11:43.620 --> 00:11:44.340
Perfect.

162
00:11:44.490 --> 00:11:52.430
And now actually this line of code is ready to be executed to train the extra boost classifier.

163
00:11:52.470 --> 00:11:58.740
So even if extra boost is a very advanced machine learning problem well thanks to this extra package

164
00:11:59.070 --> 00:12:05.040
you just need this single simple line of code to implement it very efficiently.

165
00:12:05.040 --> 00:12:05.700
All right.

166
00:12:05.700 --> 00:12:10.740
We're not going to execute this line now because first we need to run the data preprocessing phase and

167
00:12:10.740 --> 00:12:15.830
then I would like to add some code sections to evaluate our extremely small performance.

168
00:12:15.870 --> 00:12:18.660
So we're going to execute the whole thing in the end.

169
00:12:18.810 --> 00:12:25.190
But for now let's add the last sections to evaluate to actually boost performance and of course we're

170
00:12:25.200 --> 00:12:29.620
going to take our capful cross-validation technique to evaluate it.

171
00:12:29.700 --> 00:12:36.190
And therefore here I'm going to take the careful cross-validation section which is right here and we're

172
00:12:36.210 --> 00:12:39.540
going to use it on are extremely small.

173
00:12:39.540 --> 00:12:45.460
So here I just need to copy this section go back to my execute small and paste it here.

174
00:12:45.810 --> 00:12:47.730
And be careful inside of it.

175
00:12:47.730 --> 00:12:52.250
We need to change the classifier because right here that's the kernel as the classifier.

176
00:12:52.380 --> 00:13:00.700
And so basically we just need to replace this kernel as VM classifier by our extra boost classifier.

177
00:13:00.870 --> 00:13:08.010
So I'm just copying that here and go back to my K4 cross-validation section and paste the code to try

178
00:13:08.010 --> 00:13:11.980
and actually was gasifier on the training set right here.

179
00:13:12.330 --> 00:13:16.000
All right and then we need to add another line of code inside this section.

180
00:13:16.020 --> 00:13:22.710
It's related to the fact that this extra small will return the predictions as probabilities.

181
00:13:22.710 --> 00:13:28.830
You know it will return the probability of class 1 and therefore you know this trick to convert the

182
00:13:28.830 --> 00:13:32.340
probabilities into the real predictions 0 0 1.

183
00:13:32.520 --> 00:13:34.940
Well we need to add this line of code.

184
00:13:35.080 --> 00:13:39.430
Pred equals and then parenthesis.

185
00:13:39.430 --> 00:13:41.100
Quite correct.

186
00:13:41.370 --> 00:13:44.780
Larger than 0.5.

187
00:13:44.880 --> 00:13:49.860
So that's if the probability is larger than 0.5 then we will be one.

188
00:13:50.010 --> 00:13:54.700
And if the probability is lower than 0.5 then why spread zero.

189
00:13:54.890 --> 00:13:57.850
So that's where you will get the binary outcome 0 or 1.

190
00:13:57.920 --> 00:14:01.960
And that's exactly what escapable addition S. expects.

191
00:14:02.020 --> 00:14:07.860
And eventually before we execute the whole thing there are two things that we still need to change first.

192
00:14:07.890 --> 00:14:12.830
It's the fact that since the training set is expected to be a matrix all that's going to be the same

193
00:14:12.830 --> 00:14:13.910
for the test set.

194
00:14:13.910 --> 00:14:21.840
So here we also need add as dot matrix and inside the parenthesis we put our test fulled.

195
00:14:22.220 --> 00:14:23.600
So that's the first change.

196
00:14:23.600 --> 00:14:29.750
And now the second change is of course related to the index of the deep end Roybal because three here

197
00:14:29.750 --> 00:14:35.860
was the index of the dependent variable in our previous problem where we implemented careful radiation.

198
00:14:35.870 --> 00:14:41.600
So we need to replace this three index by the index of the dependent variable in our new problem which

199
00:14:41.600 --> 00:14:45.830
is not 3 but 11 and same right here.

200
00:14:45.830 --> 00:14:48.940
In the confusion matrix it is 11.

201
00:14:49.340 --> 00:14:49.910
All right.

202
00:14:49.970 --> 00:14:53.910
And now everything is ready we can execute the whole code.

203
00:14:53.960 --> 00:14:57.530
So let's do it and let's see which accuracy we get.

204
00:14:57.680 --> 00:14:59.950
So let's go back to the top.

205
00:15:00.170 --> 00:15:02.120
We already imported the data set.

206
00:15:02.120 --> 00:15:07.030
So now let's encode the categorical variables as factors.

207
00:15:07.040 --> 00:15:07.910
Here we go.

208
00:15:07.910 --> 00:15:08.820
Done.

209
00:15:08.870 --> 00:15:12.800
Now let's split the datasets into the train set and the test set.

210
00:15:12.800 --> 00:15:13.540
Here we go.

211
00:15:13.550 --> 00:15:14.880
Done as well.

212
00:15:14.900 --> 00:15:18.260
And now let's fit the extra with the training set.

213
00:15:18.470 --> 00:15:20.650
So the actual package was already imported.

214
00:15:20.840 --> 00:15:26.540
So basically we just need to select this line and execute.

215
00:15:26.540 --> 00:15:27.330
Here we go.

216
00:15:27.380 --> 00:15:31.090
We get the information of the root mean square error at each round.

217
00:15:31.100 --> 00:15:34.880
So basically the rudiments square error is a relevant computation of the error.

218
00:15:35.000 --> 00:15:36.670
You can picture this as the error.

219
00:15:36.680 --> 00:15:40.590
And of course the lower is the error the better is your MO.

220
00:15:40.730 --> 00:15:45.250
And we can see that from the first iteration to the last one detents one.

221
00:15:45.270 --> 00:15:49.910
Well the error decreased from 0.14 one down to 0.20 nine.

222
00:15:50.000 --> 00:15:56.150
And besides we can see that the maximum number of 10 iteration was a good choice because we can see

223
00:15:56.240 --> 00:16:00.350
that it is more or less converging around 0.30.

224
00:16:00.560 --> 00:16:05.750
Well feel free to try with more iterations and try to see if it's converging to a number that is less

225
00:16:05.750 --> 00:16:06.510
than 30.

226
00:16:06.650 --> 00:16:11.290
If you get a number close to 0.30 then 10 iterations was a good choice.

227
00:16:11.360 --> 00:16:15.610
So perfect extra boost is implemented and trained on the train set.

228
00:16:15.650 --> 00:16:22.110
And now let's apply K4 crossable addition to evaluates its performance with the accuracy metric.

229
00:16:22.220 --> 00:16:27.260
And actually I'm noticing that there is still one thing to change it's the name of the dip viable here.

230
00:16:27.350 --> 00:16:32.330
Congratulations to those of you who noticed that we need to replace purchased here by the real name

231
00:16:32.330 --> 00:16:37.940
of the dependent variable in our problem which is not purchased but exited.

232
00:16:38.090 --> 00:16:42.110
So let's replace purchase here by accident.

233
00:16:42.500 --> 00:16:43.290
And here we go.

234
00:16:43.310 --> 00:16:45.130
Now everything should be fine.

235
00:16:45.320 --> 00:16:51.680
Let's do one last check as matrix for the trainset as matrix for the test set whitebread converted into

236
00:16:51.680 --> 00:16:53.890
a binary outcome 0 or 1.

237
00:16:53.900 --> 00:16:56.450
Indexes are correct for the dependent variable.

238
00:16:56.450 --> 00:16:57.580
Everything looks fine.

239
00:16:57.680 --> 00:17:05.260
Let's select this whole section here and get the ultimate accuracy of are actually with morrow.

240
00:17:05.390 --> 00:17:06.540
Here we go.

241
00:17:06.890 --> 00:17:13.700
All executed properly and very fast and we get a final accuracy of 88 percent.

242
00:17:13.700 --> 00:17:19.850
So not only that was very efficient but also we managed to beat the accuracy of 10 with and then.

243
00:17:19.880 --> 00:17:26.650
And besides these values the relevant accuracy of actually Brousseau we can trust this value of 88 percent.

244
00:17:26.800 --> 00:17:27.850
So that's very good.

245
00:17:27.920 --> 00:17:32.360
Not only actually was was very fast but also gave us an amazing accuracy.

246
00:17:32.360 --> 00:17:36.230
Probably the best of all the models we implemented in this course.

247
00:17:36.230 --> 00:17:37.870
So that was an amazing job.

248
00:17:38.630 --> 00:17:43.870
And now it is time to say goodbye because this was actually the last the of this course.

249
00:17:43.910 --> 00:17:48.410
So that's quite a feeling because this is the end of this machine learning journey that I introduced

250
00:17:48.410 --> 00:17:50.760
in my very first tutorial of this course.

251
00:17:50.780 --> 00:17:53.570
So yes that's right that's the end of the journey.

252
00:17:53.600 --> 00:17:57.290
However I am sure this is not the last machine learning journey.

253
00:17:57.440 --> 00:17:59.510
This is your first missionary journey.

254
00:17:59.510 --> 00:18:01.930
I was so happy to take this adventure with you.

255
00:18:01.940 --> 00:18:04.770
I really enjoyed that journey I hope that's the case for you too.

256
00:18:04.850 --> 00:18:09.930
And I'll be very happy to make some new machinery and cause courses to start some new machinery journeys.

257
00:18:10.100 --> 00:18:11.750
So I hope I'll see you very soon.

258
00:18:11.750 --> 00:18:13.610
And until then learning.
