WEBVTT
1

00:00:00.360  -->  00:00:06.200
Hello and welcome to this art Tauriel I'm super excited because today we're going to implement this

2

00:00:06.540  -->  00:00:08.470
classification model in our.

3

00:00:08.640  -->  00:00:14.250
We just inviting for those of you who are here and this excitement is due to the fact that we will discover

4

00:00:14.250  -->  00:00:20.250
a very new kind of classification you will see that the prediction regions and prediction boundaries

5

00:00:20.370  -->  00:00:22.190
are something you haven't seen yet.

6

00:00:22.380  -->  00:00:24.420
So let's find out about that right now.

7

00:00:24.750  -->  00:00:27.490
So we'll quickly set our folder as we can directory.

8

00:00:27.690  -->  00:00:31.090
Part 3 classification decision tree classification.

9

00:00:31.110  -->  00:00:35.470
And here is to make sure that you have the social network and see the file.

10

00:00:35.550  -->  00:00:39.520
Then you click on this more button here to set the folder as working directory.

11

00:00:39.540  -->  00:00:50.490
Then let's quickly take our templates select everything from here to the bottom copy and paste it here

12

00:00:50.500  -->  00:00:51.180
.

13

00:00:51.900  -->  00:00:52.620
All right.

14

00:00:52.990  -->  00:00:55.950
And now let's change a few things.

15

00:00:55.950  -->  00:00:59.420
So just not forget this changed the titles in the plot.

16

00:00:59.640  -->  00:01:09.820
So we will replace classifier by Decision Tree and here as well.

17

00:01:12.530  -->  00:01:13.350
OK.

18

00:01:13.480  -->  00:01:20.910
And now let's create our classifier So an art to create a decision tree classifier we will use again

19

00:01:21.090  -->  00:01:25.070
the most popular library for that which is the our part library.

20

00:01:25.440  -->  00:01:30.070
So now just check to see if you have the airport library in your packages.

21

00:01:30.090  -->  00:01:32.980
So for example mine is right here.

22

00:01:33.030  -->  00:01:36.030
It might not be the case for you if you're starting or for the first time.

23

00:01:36.150  -->  00:01:40.600
So I'll just write this line of code for those of you who need to install it.

24

00:01:40.740  -->  00:01:44.940
And so as usual it's install packages.

25

00:01:45.180  -->  00:01:51.910
And then in quotes in the parenthesis you input the name of the package which is then our parts.

26

00:01:51.960  -->  00:01:52.740
All right.

27

00:01:52.830  -->  00:01:56.350
And then to install the package in it to select this line and execute.

28

00:01:56.550  -->  00:02:02.130
I won't do it right now because my baggage is already installed so I'll just put that as comment and

29

00:02:02.490  -->  00:02:10.140
however we're going to include this line of code here library and in parenthesis our part to automatically

30

00:02:10.140  -->  00:02:15.780
select this library because once this is executed this will be selected.

31

00:02:15.960  -->  00:02:20.610
As you can see right now it's not selected but it will be once this is executed.

32

00:02:21.070  -->  00:02:21.710
OK.

33

00:02:21.960  -->  00:02:25.180
And now we are ready to create our classifier.

34

00:02:25.290  -->  00:02:32.760
So let's do this classifier as usual equals and then we are going to use actually a function which is

35

00:02:32.760  -->  00:02:34.920
the same as the library or part.

36

00:02:34.980  -->  00:02:42.420
So our part and this function we will put the right parameters so well as you can see right now we can

37

00:02:42.420  -->  00:02:44.320
see what those parameters are.

38

00:02:44.430  -->  00:02:50.010
But if you want more info we can click here and press one.

39

00:02:50.580  -->  00:02:57.450
And here we just need to click here to get some info about our part I guess as you can see the first

40

00:02:57.480  -->  00:02:59.490
argument is formula.

41

00:02:59.490  -->  00:03:05.250
So this is a parameter that we have to input and as usual we're going to write the formula equals dependent

42

00:03:05.250  -->  00:03:11.700
variable till the dot in the data is a shortcut to replace the sum of the independent variables which

43

00:03:11.700  -->  00:03:13.720
is the other way of writing the formula.

44

00:03:13.920  -->  00:03:21.240
So that's the same as usual and then we have the data argument here which is of course the data on which

45

00:03:21.240  -->  00:03:22.890
you want to train your classifier.

46

00:03:22.890  -->  00:03:25.260
So this data will be the training set.

47

00:03:25.260  -->  00:03:27.050
All right so let's end with the arguments.

48

00:03:27.210  -->  00:03:34.580
So I remember the first argument with formula equals purchased.

49

00:03:34.620  -->  00:03:37.340
That's the dependent variable till the.

50

00:03:37.400  -->  00:03:45.900
I would just press out and and then a dot to include all the variables then come up and then put the

51

00:03:45.900  -->  00:03:52.820
second argument which remember it was made up and we pick our trainset perfect.

52

00:03:52.900  -->  00:03:56.340
And now let's execute the whole code.

53

00:03:56.340  -->  00:04:01.590
So first we execute this pre-processing part here as usual done.

54

00:04:01.590  -->  00:04:02.810
Perfect.

55

00:04:02.910  -->  00:04:07.800
So we can have a look at a data set dataset of find with are two independent variables age an estimated

56

00:04:07.800  -->  00:04:11.810
salary and our dependent variable chased training sets.

57

00:04:11.910  -->  00:04:14.410
All good and test set all good.

58

00:04:14.610  -->  00:04:15.240
OK.

59

00:04:15.420  -->  00:04:21.990
So the training set and the test sets are scales because we will plot the prediction regions with a

60

00:04:21.990  -->  00:04:24.090
high resolution so we need to scale.

61

00:04:24.090  -->  00:04:29.160
Actually you can try to not scale the independent variables here you know because for decision tree

62

00:04:29.160  -->  00:04:34.650
you don't need to scale your independent variables because the decision tree model is not based on Euclidean

63

00:04:34.650  -->  00:04:39.990
distance but since we want to plot the prediction regions with a high resolution you will see that your

64

00:04:39.990  -->  00:04:44.170
code will execute a huge time faster than if you don't scale it.

65

00:04:44.190  -->  00:04:51.030
Actually I think that if you don't scale your code might break you can try that but be careful.

66

00:04:51.030  -->  00:04:57.150
So we will do it and then we will execute the code again without the scaling to plug the tree.

67

00:04:57.360  -->  00:04:58.670
So we will clear everything.

68

00:04:58.830  -->  00:05:03.020
And then the pre-processing parts select everything except the feature scaling.

69

00:05:03.120  -->  00:05:06.120
And then we will plot our tree in a very simple way.

70

00:05:06.360  -->  00:05:08.530
But right now we want to plot in prediction regions.

71

00:05:08.550  -->  00:05:10.300
So we scale our independent variables.

72

00:05:10.510  -->  00:05:20.070
Okay so perfect now that Castro is ready so let's executed all right all good now we can execute this

73

00:05:20.070  -->  00:05:26.800
line to predict the test results and actually what's funny is that why pred is not the same as what

74

00:05:26.800  -->  00:05:27.820
we were used to.

75

00:05:27.810  -->  00:05:29.930
First for example we can see why bread here.

76

00:05:29.940  -->  00:05:34.480
Remember before white bread wasn't in the data here we had to type it in the console to have a look

77

00:05:34.480  -->  00:05:34.980
at it.

78

00:05:35.250  -->  00:05:36.190
And here it's here.

79

00:05:36.190  -->  00:05:39.440
So let's click on it to find out what it is.

80

00:05:39.450  -->  00:05:45.560
All right this is why pred and this is actually a matrix of two columns and a hundred lines.

81

00:05:45.810  -->  00:05:47.670
So what is this new why pray.

82

00:05:47.880  -->  00:05:48.720
What is it exactly.

83

00:05:48.730  -->  00:05:56.350
Well as you can see the some of the two cells here in each line is equal to one.

84

00:05:56.350  -->  00:05:58.610
So can you guess what it is.

85

00:05:59.110  -->  00:06:00.740
Well these are probabilities.

86

00:06:00.850  -->  00:06:05.770
The first column gives the probability that the observation the user belongs to can't zero.

87

00:06:05.820  -->  00:06:08.080
That is doesn't buy the SUV.

88

00:06:08.280  -->  00:06:15.490
And this probability in the second column is the probability that the user buys the SUV belongs to class

89

00:06:15.490  -->  00:06:17.820
one that is by the SUV.

90

00:06:17.820  -->  00:06:22.120
So here if you look at the first observation you can see that there is a very high probability that

91

00:06:22.120  -->  00:06:27.390
the user buys the SUV 0.26 against a very low probability.

92

00:06:27.390  -->  00:06:32.430
Well of course because the sum is equal to one and so that means here that the prediction here is that

93

00:06:32.440  -->  00:06:34.810
the user doesn't buy the SUV.

94

00:06:34.810  -->  00:06:41.080
And if you look at the test here and look at the index zero we can see that indeed in reality the user

95

00:06:41.070  -->  00:06:48.300
didn't buy the SUV and therefore the prediction is correct but that's because we have white bread this

96

00:06:48.310  -->  00:06:54.360
way in our templates and if you don't want this format of white bread Well you just need to add a simple

97

00:06:54.370  -->  00:06:59.340
argument which is type hair that type equals.

98

00:06:59.500  -->  00:07:01.920
And you just need to input class.

99

00:07:02.380  -->  00:07:07.940
Ok let's try let's try to execute this line again and here's why.

100

00:07:07.950  -->  00:07:12.690
Let's have a look at why Prit now widespread.

101

00:07:12.690  -->  00:07:13.290
There you go.

102

00:07:13.300  -->  00:07:14.710
Now why Pretis vector.

103

00:07:14.750  -->  00:07:21.100
So you can see for each observation of the test that is for each user of the set we have like before

104

00:07:21.490  -->  00:07:25.430
the predictions zero or one for each user 0.

105

00:07:25.440  -->  00:07:32.470
If the user is predicted not to buy the SUV and one if the user is predicted to buy the SUV according

106

00:07:32.710  -->  00:07:35.070
to our decision tree classifier.

107

00:07:35.650  -->  00:07:35.910
OK.

108

00:07:35.930  -->  00:07:38.400
So that's a little thing to change here.

109

00:07:38.400  -->  00:07:44.830
Make sure that you know your wife bread is your dependent vector of results zero or one that we were

110

00:07:44.820  -->  00:07:50.820
used to because as you can see we used the same product function with two arguments classifier and new

111

00:07:50.820  -->  00:07:52.710
data equals grid set.

112

00:07:52.890  -->  00:07:59.270
So that means that this won't work because this is supposed to be a vector of prediction results.

113

00:07:59.440  -->  00:08:05.330
Only this time it's for all the pixel points you know the imaginary pixel point users in the grid within

114

00:08:05.390  -->  00:08:10.650
the Predict function is associated to the classifier which is the decision tree gasifier.

115

00:08:10.730  -->  00:08:15.400
And if we only keep those two arguments here then this will make any sense because this will return

116

00:08:15.660  -->  00:08:22.330
WEIGERT as a matrix of the two probabilities and therefore here will have some problem because it will

117

00:08:22.320  -->  00:08:26.640
be a matrix of a matrix whereas here it's supposed to be a vector.

118

00:08:26.640  -->  00:08:33.500
So what we want them to do and we will do it now so that we don't forget is to add this type parameter

119

00:08:33.900  -->  00:08:39.120
and we will set it equal to class and then it will work perfectly.

120

00:08:39.390  -->  00:08:45.870
So I will copy this and added here as well.

121

00:08:45.880  -->  00:08:46.290
Perfect.

122

00:08:46.310  -->  00:08:47.690
And now it's ready now.

123

00:08:47.790  -->  00:08:51.010
It will plug the graphs without any errors.

124

00:08:51.000  -->  00:08:57.210
So I know I gave you a template that is supposed to work without changing anything to plus the classifications

125

00:08:57.220  -->  00:08:57.610
.

126

00:08:57.610  -->  00:09:02.620
I'm sorry sometimes we need to change a little few stuff and that's why we need to you know execute

127

00:09:02.710  -->  00:09:06.810
each of the lines one by one to see if it's as it should be.

128

00:09:07.000  -->  00:09:12.180
And here since we found out that due to this little different structure of the decision tree classifier

129

00:09:12.180  -->  00:09:12.350
.

130

00:09:12.660  -->  00:09:18.790
Well we need to adjust to get the right vector if predictions were spread that we had for the previous

131

00:09:18.780  -->  00:09:19.490
classifiers.

132

00:09:19.530  -->  00:09:25.530
And so we made the adjustment for decision trees by adding this type argument here and set it equal

133

00:09:25.530  -->  00:09:26.900
to class.

134

00:09:26.920  -->  00:09:29.210
Now we have the white pride factor that we needed.

135

00:09:29.560  -->  00:09:34.890
And besides yes we would have encountered some issues if we you know computed the confusion matrix this

136

00:09:34.890  -->  00:09:38.280
way with this why predecease the matrix of probabilities.

137

00:09:38.330  -->  00:09:42.270
But now it will be fine because white bread is set the correct way.

138

00:09:42.610  -->  00:09:46.230
So we'll execute this and look at the number of incorrect Productions.

139

00:09:46.240  -->  00:09:47.000
All right.

140

00:09:47.130  -->  00:09:54.840
Now let's enter C.M. here and we have 6 plus 11 UKL 17 incorrect predictions.

141

00:09:54.850  -->  00:10:01.200
So now let's see if we were right to change our code this way so that we can plot the graph.

142

00:10:01.200  -->  00:10:02.680
Let's see if it will work.

143

00:10:02.700  -->  00:10:08.430
I hope it will work because I want to show you the decision tree prediction regions and prediction boundary

144

00:10:08.430  -->  00:10:08.460
.

145

00:10:08.460  -->  00:10:09.990
I really want to show you this.

146

00:10:10.060  -->  00:10:13.370
For those of you who didn't follow the bike and Twitter of course.

147

00:10:13.540  -->  00:10:19.470
So let's select this and let's see if we made a good job.

148

00:10:19.480  -->  00:10:20.420
All right looks good.

149

00:10:20.430  -->  00:10:22.830
So far looks good no errors.

150

00:10:22.840  -->  00:10:24.190
Let's see what happens.

151

00:10:26.140  -->  00:10:31.750
And we were right this works perfectly well that's a decision tree test fired.

152

00:10:31.930  -->  00:10:33.880
That's the prediction boundary.

153

00:10:34.070  -->  00:10:37.940
So as you can see there is only horizontal and vertical lines.

154

00:10:37.960  -->  00:10:44.470
That's because as Karylle explains the density algorithm is based on some conditions of your independent

155

00:10:44.470  -->  00:10:44.920
variables.

156

00:10:44.920  -->  00:10:51.000
By finding you know each time intervals that will make conditions that will classify in some rectangles

157

00:10:51.370  -->  00:10:57.610
your observations and actually what's funny is that we clearly have less overfitting than in Python

158

00:10:57.620  -->  00:10:57.750
.

159

00:10:57.880  -->  00:11:04.690
And actually that's why we have more incorrect predictions because in python we had you know red rectangles

160

00:11:04.690  -->  00:11:06.700
here red rectangles here.

161

00:11:06.700  -->  00:11:10.720
There was also red rectangle here and here.

162

00:11:10.720  -->  00:11:16.420
We didn't actually specified more parameters but these amazing Archerd library and that's why it's very

163

00:11:16.420  -->  00:11:22.870
popular chose the right parameters the radial parameters to you know pre-event overfitting because here

164

00:11:23.080  -->  00:11:24.750
we clearly don't have overfitting.

165

00:11:24.850  -->  00:11:29.500
We have a fitting with Python because of all the red rectangles here that we're desperately trying to

166

00:11:29.500  -->  00:11:31.910
catch every user in the right category.

167

00:11:32.030  -->  00:11:34.490
But here is not the case in here.

168

00:11:34.600  -->  00:11:39.430
It's doing a terrific job you know classifying correctly most of the right points here.

169

00:11:39.430  -->  00:11:44.870
Most of the green points here in the right region and as well as these green users here who couldn't

170

00:11:44.900  -->  00:11:52.680
be well to fight for linear classifiers such as they just take regression or linear kernel as VM.

171

00:11:52.870  -->  00:11:55.900
So here is a pretty good job but still we have some incorrect protection.

172

00:11:55.900  -->  00:11:58.240
That's because that's difficult to classify.

173

00:11:58.240  -->  00:12:01.710
Well if you want to prevent overfitting in the data.

174

00:12:01.990  -->  00:12:07.210
So even if we had 17 correct additions that's a very good classification.

175

00:12:07.300  -->  00:12:08.300
We have here.

176

00:12:08.820  -->  00:12:14.170
OK but now let's look at the test results and I'm actually not too worried about that because since

177

00:12:14.170  -->  00:12:19.420
we don't have overfitting here then that means that we're very likely to have some good results as well

178

00:12:19.480  -->  00:12:20.180
on the test.

179

00:12:20.230  -->  00:12:21.880
Let's check it out.

180

00:12:22.000  -->  00:12:24.710
Test set and execute.

181

00:12:24.740  -->  00:12:26.770
That's it.

182

00:12:28.480  -->  00:12:29.470
And here's the test set.

183

00:12:29.530  -->  00:12:29.990
OK.

184

00:12:30.100  -->  00:12:32.440
So as I told you this looks very good.

185

00:12:32.770  -->  00:12:37.660
This is the set on which we have those 17 regulations you can count them if you want.

186

00:12:37.660  -->  00:12:40.140
You will find 17 and it's classified.

187

00:12:40.150  -->  00:12:44.980
Most of the red users in the red region and most of the great users and greener regions.

188

00:12:44.980  -->  00:12:50.070
That's quite OK by the way we can see that most of the incorrect predictions are here.

189

00:12:50.080  -->  00:12:54.920
We can see that we have many red points in the green region so that's unlucky.

190

00:12:54.950  -->  00:13:01.870
But as I told you we would rather prevent overfitting than trying to minimize to zero the number of

191

00:13:01.870  -->  00:13:03.450
incorrect predictions.

192

00:13:03.460  -->  00:13:10.960
So there is another new kind of classifier and therefore another new kind of prediction region and prediction

193

00:13:10.960  -->  00:13:11.560
boundary.

194

00:13:11.590  -->  00:13:17.620
And now in the next section you will find out about random forests who are actually a team of decision

195

00:13:17.620  -->  00:13:23.260
trees of many decision trees you know around them for us can contain 500 trees and so each decision

196

00:13:23.260  -->  00:13:29.740
tree of the team will vote for each data point whether this data point belongs to class 0 0 class 1

197

00:13:29.740  -->  00:13:29.920
.

198

00:13:29.920  -->  00:13:36.940
That is it will vote for each user if this user bought the SUV or didn't buy the SUV and based on a

199

00:13:36.940  -->  00:13:42.880
majority vote it will decide if each one belongs to the red region or the green region.

200

00:13:42.910  -->  00:13:48.160
And so I'm telling you this because you know since here we have the job of one decision tree.

201

00:13:48.160  -->  00:13:53.050
Imagine the job of a Normy of decision trees like 500000 trees.

202

00:13:53.050  -->  00:13:55.860
Imagine what he could do to classify the result.

203

00:13:56.080  -->  00:13:58.720
But let's not get too enthusiast about that.

204

00:13:58.720  -->  00:14:01.300
Remember that we want to prevent overfishing.

205

00:14:01.450  -->  00:14:04.430
So let's find out anyway.

206

00:14:04.450  -->  00:14:04.980
All right.

207

00:14:05.030  -->  00:14:07.250
Now time for the little extra bonus.

208

00:14:07.360  -->  00:14:09.640
Let's plug the decision tree.

209

00:14:09.640  -->  00:14:14.970
So in order to have the best interpretation we're going to remove the feature scaling and to do this

210

00:14:14.980  -->  00:14:19.430
we're going to clear everything and re execute the code.

211

00:14:19.620  -->  00:14:21.230
But without fishes can.

212

00:14:21.400  -->  00:14:22.240
So let's do this.

213

00:14:22.240  -->  00:14:26.680
We're going to clear the data here by clicking on this button here.

214

00:14:26.690  -->  00:14:30.250
And yes then the plugs here.

215

00:14:30.520  -->  00:14:31.760
Yes.

216

00:14:31.780  -->  00:14:34.690
And then kill the console as well by typing control.

217

00:14:34.760  -->  00:14:36.570
Well here we go.

218

00:14:36.590  -->  00:14:38.140
All right now everything is clear.

219

00:14:38.230  -->  00:14:41.070
We can execute the whole script.

220

00:14:41.080  -->  00:14:42.790
So let's do it step by step.

221

00:14:42.790  -->  00:14:49.270
We're first going to take all the preprocessing step but without the features came in here as you can

222

00:14:49.270  -->  00:14:51.060
see I'm not selecting this.

223

00:14:51.130  -->  00:14:55.020
All right so now command control please percentage is cute.

224

00:14:55.180  -->  00:14:56.200
All good.

225

00:14:56.200  -->  00:15:03.700
Now you can see that if we go to our dataset training set and test set you can see that the features

226

00:15:03.790  -->  00:15:05.390
are no longer scaled.

227

00:15:05.710  -->  00:15:05.980
All right.

228

00:15:05.980  -->  00:15:08.820
So for example in the training set we have the real age.

229

00:15:08.830  -->  00:15:14.630
I mean with their real values and the real estimated salaries the real value is ok.

230

00:15:14.710  -->  00:15:15.550
Nothing scale.

231

00:15:15.580  -->  00:15:21.550
Now let's select this to fit the decision tree classify it as a training set.

232

00:15:21.580  -->  00:15:24.970
So command control controllers and to execute it.

233

00:15:25.180  -->  00:15:37.220
Now it's fit it by the way I'm just going to replace this by Decision Tree classification.

234

00:15:37.260  -->  00:15:38.720
All right.

235

00:15:38.740  -->  00:15:42.810
What we're really interested in right now is to plot the decision tree.

236

00:15:42.820  -->  00:15:50.710
So we're going to add a new section here below and I'm going to call this section bloodying the decision

237

00:15:50.710  -->  00:15:52.280
tree.

238

00:15:53.710  -->  00:15:58.960
And as I told you in the Python tutorial this is going to take two lines.

239

00:15:58.960  -->  00:16:00.100
This is very simple.

240

00:16:00.100  -->  00:16:01.260
So let's just do it.

241

00:16:01.270  -->  00:16:03.600
It's going to be very quick.

242

00:16:03.610  -->  00:16:08.920
Again besides the two lines that we need to write could not be more simple because indeed what we need

243

00:16:08.920  -->  00:16:11.310
to type here is plot.

244

00:16:11.500  -->  00:16:19.240
And then in parenthesis classifier because you know the classifier Here's the classifier that we created

245

00:16:19.780  -->  00:16:26.800
here in this part using the our part function of the Arbre library and that's all we just need to type

246

00:16:26.800  -->  00:16:29.400
Plup classifier and that will plot the tree.

247

00:16:29.530  -->  00:16:33.880
But without the labels without the conditions reason explicitly.

248

00:16:33.880  -->  00:16:42.810
So in order to add these conditions written explicitly we just need to add below text classifier.

249

00:16:42.820  -->  00:16:43.340
Here we go.

250

00:16:43.360  -->  00:16:44.560
And now it's ready.

251

00:16:44.650  -->  00:16:46.020
Isn't that simple.

252

00:16:46.040  -->  00:16:50.000
There's only two lines we'll plot an interpretable decision tree.

253

00:16:50.200  -->  00:16:51.350
Let's check it out.

254

00:16:51.670  -->  00:16:58.850
I'm going to select all this and press command control press enter to execute.

255

00:16:59.440  -->  00:17:00.820
And here is a tree.

256

00:17:00.940  -->  00:17:06.620
So as you can see we have at each split the condition that is generating the splits.

257

00:17:06.820  -->  00:17:14.770
So for example the first place is made based upon the condition age lower than forty four point five

258

00:17:14.920  -->  00:17:15.710
years old.

259

00:17:16.000  -->  00:17:23.890
So that means that if the user is below 40 4.5 years old he will get into this subcategory after the

260

00:17:23.890  -->  00:17:30.500
split and if the user is older than forty four point five years old he will end up in this category

261

00:17:30.510  -->  00:17:37.240
of this place and then we have some new conditions here and new condition on the other independent variable

262

00:17:37.240  -->  00:17:37.400
.

263

00:17:37.420  -->  00:17:44.170
The estimated salary in this condition is estimated salary below $90000.

264

00:17:44.380  -->  00:17:50.800
So what that means is that if the user is younger than forty four point five years old and has an estimated

265

00:17:50.800  -->  00:17:58.900
salary below $90000 then according to our decision tree classifier this user won't buy the SUV because

266

00:17:59.200  -->  00:18:00.830
the result here is zero.

267

00:18:01.180  -->  00:18:08.650
And if the user is younger than forty four and five years old and has an estimated salary over $90000

268

00:18:09.070  -->  00:18:14.770
then according to our decision tree classifier this user will buy the SUV because the result here is

269

00:18:14.770  -->  00:18:15.810
one.

270

00:18:15.850  -->  00:18:21.880
Again if you go to the other side of the tree well this other side of the tree first contains all the

271

00:18:21.880  -->  00:18:25.390
users that are older than forty four point five years old.

272

00:18:25.630  -->  00:18:29.210
And then we have some new conditions generating new splits.

273

00:18:29.230  -->  00:18:34.900
So another condition of the age here then another condition on the estimated sorry and then again another

274

00:18:34.900  -->  00:18:40.980
condition on the military and following yes or no these conditions we end up in some final categories

275

00:18:40.990  -->  00:18:48.130
the final nodes of that decision tree where the user is predicted not to buy the SUV for this node and

276

00:18:48.130  -->  00:18:52.670
predicted to buy the SUV this node and this node And this note.

277

00:18:52.940  -->  00:18:53.310
OK.

278

00:18:53.320  -->  00:18:55.650
So that was worth looking at this.

279

00:18:55.840  -->  00:19:00.280
We sort of explored this decision tree classifiable behind the scenes.

280

00:19:00.400  -->  00:19:05.320
And what's important to understand now when you're using decision trees is that's a big advantage of

281

00:19:05.320  -->  00:19:11.860
this classifiers that we can have these very interpretable results because here on this decision tree

282

00:19:11.860  -->  00:19:14.440
plot we can see everything that's happening.

283

00:19:14.530  -->  00:19:21.810
We can see how the decision tree decides whether user will be predicted to not buy the SUV or to buy

284

00:19:21.810  -->  00:19:22.380
an SUV.

285

00:19:22.450  -->  00:19:29.080
We clearly see the whole thinking process of the decision tree and in some way we saw how it learned

286

00:19:29.230  -->  00:19:35.950
from the data how to classify each of our user of the social network.

287

00:19:35.950  -->  00:19:39.930
All right so I look forward to seeing you in the next section about running for us.

288

00:19:39.970  -->  00:19:45.400
We will implement training for us on Python and our and I can't wait to show you the final graphic results

289

00:19:45.970  -->  00:19:46.860
until next time.

290

00:19:46.900  -->  00:19:47.900
Enjoy machine learning
