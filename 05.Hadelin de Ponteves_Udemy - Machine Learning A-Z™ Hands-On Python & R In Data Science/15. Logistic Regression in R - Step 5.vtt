WEBVTT
1

00:00:00.330  -->  00:00:04.730
Hello and welcome to the final round of logistic regression.

2

00:00:04.830  -->  00:00:08.400
So let's recap what we did in the previous tutorials.

3

00:00:08.400  -->  00:00:14.430
We started by pre-processing our data using our data processing template and that's made our data set

4

00:00:14.430  -->  00:00:19.370
ready to fit the logistic regression to our trainset which is what we did in this step.

5

00:00:19.530  -->  00:00:26.190
Then we use this logistic regression classifier to predict the test results so that we can evaluate

6

00:00:26.250  -->  00:00:32.760
the predictive power of our logistic regression model and then indeed we made this confusion matrix

7

00:00:32.970  -->  00:00:37.170
to actually count all the correct predictions and the incorrect predictions.

8

00:00:37.320  -->  00:00:42.960
And now we're at the final step which is to visualize the trainset results and the test results as well

9

00:00:42.970  -->  00:00:43.240
.

10

00:00:43.470  -->  00:00:49.350
And now we're eventually at the final round of our logistic regression model which is the fun part because

11

00:00:49.350  -->  00:00:53.500
we're going to visualise the training set results as well as the test that results.

12

00:00:53.670  -->  00:00:59.010
And basically we're going to make a plot that will represent everything that is happening with our logistic

13

00:00:59.010  -->  00:01:00.050
regression model.

14

00:01:00.360  -->  00:01:03.470
So let's make this chart I'm going to do like in Python.

15

00:01:03.570  -->  00:01:07.820
I'm going to take the code that I've prepared and paste it here.

16

00:01:07.980  -->  00:01:10.770
I will select and execute the code.

17

00:01:10.770  -->  00:01:15.480
And then for those of you who are interested in the idea behind the code that is how the code plucks

18

00:01:15.480  -->  00:01:19.550
the graph I will explain how the code works at the end of this tutorial.

19

00:01:19.870  -->  00:01:20.140
OK.

20

00:01:20.160  -->  00:01:21.140
So let's do it.

21

00:01:21.150  -->  00:01:23.850
Let's start getting the fun again.

22

00:01:23.860  -->  00:01:26.090
So right now I'm going to paste the code.

23

00:01:26.250  -->  00:01:27.140
Here we go.

24

00:01:27.490  -->  00:01:33.810
OK so the first thing to do here is to notice that there is this Elam's that learn package that you

25

00:01:33.810  -->  00:01:34.730
need to install.

26

00:01:34.740  -->  00:01:37.410
If it's not in your package here.

27

00:01:37.560  -->  00:01:41.590
So as you can see it's in my package just list because I installed it before.

28

00:01:41.730  -->  00:01:47.520
If it's not here you need to install it so to install it you will need to remove this comment and you

29

00:01:47.520  -->  00:01:54.390
type install dot packages and then in parenthesis ElemStatLearn that's the name of the package in quotes

30

00:01:55.050  -->  00:02:00.120
then you select this line and execute.

31

00:02:00.150  -->  00:02:04.370
And right now you can see it's installing the package and now the package is installed.

32

00:02:04.380  -->  00:02:08.020
I didn't have to do it because it was already installed but that was just to show you.

33

00:02:08.160  -->  00:02:11.510
And now we are ready to select the code and execute it.

34

00:02:11.550  -->  00:02:17.790
And that is even if it's not selected here because I put the library Elam's that learn which ones executed

35

00:02:18.060  -->  00:02:20.750
will activate this package when selected.

36

00:02:21.030  -->  00:02:23.900
OK then this code contains 15 lines of code.

37

00:02:24.000  -->  00:02:29.580
There was also the case in Python and that's because it's the exact same code based on the same idea

38

00:02:29.580  -->  00:02:29.800
.

39

00:02:29.970  -->  00:02:33.540
And I will explain this idea that means I will explain how this code works.

40

00:02:33.630  -->  00:02:35.000
At the end of this tutorial.

41

00:02:35.400  -->  00:02:39.600
But for now let's select this code and let's see what happens.

42

00:02:39.600  -->  00:02:44.350
So command and control plus enter to execute and let's wait.

43

00:02:44.380  -->  00:02:49.580
So taking a little time that's know but it's going to plot something.

44

00:02:50.280  -->  00:02:51.290
There it is.

45

00:02:51.480  -->  00:02:51.990
There it is.

46

00:02:51.990  -->  00:02:53.280
That's the plot.

47

00:02:53.640  -->  00:02:54.070
OK.

48

00:02:54.090  -->  00:02:59.240
So for those of you who follow the Python tutorial we obtain exactly the same plot.

49

00:02:59.540  -->  00:03:00.670
So that describes this plot.

50

00:03:00.680  -->  00:03:03.430
I'm going to enlarge this.

51

00:03:04.190  -->  00:03:04.520
OK.

52

00:03:04.530  -->  00:03:08.110
So let's analyze this graph step by step.

53

00:03:08.190  -->  00:03:13.980
First let's focus on all the points here we can see that we have some red points and some green points

54

00:03:13.990  -->  00:03:14.300
.

55

00:03:14.610  -->  00:03:20.470
So all these points that we see on this graph are the observation points of our training.

56

00:03:20.790  -->  00:03:26.800
That is these are all the users of the social network that were selected to go to the training site

57

00:03:26.820  -->  00:03:27.100
.

58

00:03:27.540  -->  00:03:35.520
And each of these users here is characterized by its age here on the x axis and it's estimated salary

59

00:03:35.520  -->  00:03:37.260
here on the y axis.

60

00:03:37.260  -->  00:03:45.380
Now we can see that there are some red points here and some green points here the red points are the

61

00:03:45.390  -->  00:03:52.710
trainset observations for which the dependent variable purchased is equal to zero and the green points

62

00:03:52.800  -->  00:03:59.050
are the training set observations for which the dependent variable pre-Chase is equal to 1.

63

00:03:59.070  -->  00:04:06.210
That means that the red points here are the users who didn't buy the SUV and the green points here are

64

00:04:06.210  -->  00:04:09.130
the users who bought who actually bought the SUV.

65

00:04:09.330  -->  00:04:16.230
So now as a first step of analysis let's give an interpretation of what we observe here with this users

66

00:04:16.240  -->  00:04:16.500
.

67

00:04:16.730  -->  00:04:16.960
OK.

68

00:04:16.980  -->  00:04:25.260
So first we can see that the users were young with a low estimated salary so these users here actually

69

00:04:25.260  -->  00:04:26.940
didn't buy the SUV.

70

00:04:27.150  -->  00:04:33.870
Then if we look at the users who are older and with a higher estimated salary what we can see that most

71

00:04:33.870  -->  00:04:39.000
of these users actually bought their SUV and that actually makes sense because their SUV is more like

72

00:04:39.150  -->  00:04:42.600
a family car and therefore more interesting for these older users.

73

00:04:42.600  -->  00:04:49.260
Here with a high estimated salary besides we can also see that some older people even with a low estimated

74

00:04:49.260  -->  00:04:52.090
salary actually bought the SUV.

75

00:04:52.140  -->  00:04:58.170
We can see that we have some green points here that correspond to an age above the average the average

76

00:04:58.170  -->  00:04:59.070
is here.

77

00:04:59.580  -->  00:05:03.230
But an estimated salary below the average because the average is here.

78

00:05:03.690  -->  00:05:04.360
OK.

79

00:05:04.620  -->  00:05:09.290
So these guys these older guys although they have a low estimate the salary actually bought the SUV

80

00:05:09.570  -->  00:05:13.680
probably because they've been setting up some money or maybe they finished paying off their mortgage

81

00:05:13.680  -->  00:05:13.950
.

82

00:05:13.950  -->  00:05:20.900
I don't know what's for sure is that they couldn't resist buying this very cool luxury SUV offered at

83

00:05:20.910  -->  00:05:22.580
a ridiculously low price.

84

00:05:23.680  -->  00:05:29.890
And on the other hand we can also see that there are some young people here with a high estimated salary

85

00:05:30.160  -->  00:05:32.460
who actually bought the SUV.

86

00:05:32.630  -->  00:05:36.880
You know maybe because it's a very cool SUV and they want to impress their friends and take them into

87

00:05:36.880  -->  00:05:39.520
road trips or maybe they already have a family.

88

00:05:39.520  -->  00:05:40.490
I don't know.

89

00:05:40.640  -->  00:05:42.800
Anyway they bought the SUV.

90

00:05:43.090  -->  00:05:47.790
Actually there are a lot of buyers so this must be a very cool and cheap SUV.

91

00:05:48.060  -->  00:05:48.450
OK.

92

00:05:48.480  -->  00:05:51.670
And now what is the goal of classification.

93

00:05:51.730  -->  00:05:58.540
Now we're talking machine learning why are we making some classifiers and what will classifiers do at

94

00:05:58.540  -->  00:06:02.510
least what are we trying to make them do for this particular business problem.

95

00:06:02.830  -->  00:06:08.250
Well the goal here is to classify the right users into the right categories.

96

00:06:08.440  -->  00:06:15.280
That is we are trying to make a classifier that will catch the right users into the right category which

97

00:06:15.280  -->  00:06:22.000
are yes they buy the SUV and no they don't buy the SUV and we represented the way our classifier catches

98

00:06:22.000  -->  00:06:26.290
this users by plotting what I called the prediction regions.

99

00:06:26.320  -->  00:06:30.910
And so the prediction regions are the two regions that we see on this graph.

100

00:06:30.910  -->  00:06:37.690
This red one here and this green one here and the red prediction region is the region where our classifier

101

00:06:37.720  -->  00:06:43.720
catches all the users that don't buy the SUV and the green prediction region is the region where it

102

00:06:43.720  -->  00:06:46.950
grassfire catches all the users that buy the SUV.

103

00:06:47.080  -->  00:06:48.190
But be careful.

104

00:06:48.250  -->  00:06:54.130
This is according to the classifier that is for each user of this red prediction region.

105

00:06:54.130  -->  00:07:01.210
Here are our logistic regression classifier predicts that the user doesn't buy the SUV and for each

106

00:07:01.210  -->  00:07:08.140
user of this green prediction region here our classifiable predicts that the use of buying the SUV even

107

00:07:08.140  -->  00:07:10.600
if that's not the case in real life that's just a prediction.

108

00:07:10.780  -->  00:07:14.740
But that's what our classifier believes will happen.

109

00:07:14.740  -->  00:07:20.760
It is the classifier prediction compared to the truth here which is the point.

110

00:07:20.920  -->  00:07:24.600
The point is the truth and the reason here is the prediction.

111

00:07:24.820  -->  00:07:31.930
So that makes an awesome tool because for each new user of the social network Well our classifier a

112

00:07:31.930  -->  00:07:38.920
logistic regression cassowary will tell based on its age and its estimated salary if this usage belongs

113

00:07:38.920  -->  00:07:41.080
to this red prediction region here.

114

00:07:41.260  -->  00:07:46.630
And therefore it doesn't buy the SUV or if this user belongs to this green prediction region here and

115

00:07:46.630  -->  00:07:53.620
therefore buys the SUV and that way this business can car company can substantially optimize their marketing

116

00:07:53.620  -->  00:08:00.130
campaign by targeting the social network ads to the users in the green region because these are the

117

00:08:00.130  -->  00:08:05.150
users that are predicted to buy the yesterday according to our classifier.

118

00:08:05.380  -->  00:08:11.740
Now the other very important thing to understand is that these are two prediction regions separated

119

00:08:11.980  -->  00:08:18.100
by a straight line which is the straight line here and the straight line is called the prediction boundary

120

00:08:18.610  -->  00:08:22.030
because its the boundary between the two prediction regions.

121

00:08:22.270  -->  00:08:26.020
And the fact that its a straight line is not random.

122

00:08:26.020  -->  00:08:31.860
It is for a particular reason and thats the thing very important to understand because thats the sense

123

00:08:31.870  -->  00:08:36.350
of logistic regression if the prediction boundary is a straight line here.

124

00:08:36.490  -->  00:08:41.710
Thats because our logistic regression classifier is a linear our classifier.

125

00:08:41.950  -->  00:08:46.090
That means that here since we are in two dimensions you know because we have two independent variable

126

00:08:46.090  -->  00:08:51.610
the age and the estimated sorry's that we are in two dimensions then since the logistic regression classifier

127

00:08:51.670  -->  00:08:58.510
is a linear classifier then the prediction boundary separator here can only be a straight line.

128

00:08:58.570  -->  00:09:04.810
If we were in three dimensions then it would be a straight planet separating two spaces but here in

129

00:09:04.810  -->  00:09:10.060
two dimensions its a straight line and it will always be a straight line if your classifier is a linear

130

00:09:10.060  -->  00:09:12.310
classifier but you will see later.

131

00:09:12.400  -->  00:09:19.150
Thats when we build non-linear classifiers then the prediction boundary separator wont be a straight

132

00:09:19.150  -->  00:09:19.990
line anymore.

133

00:09:20.170  -->  00:09:23.600
I wont tell you more right now and I will let you wait for the surprise.

134

00:09:23.920  -->  00:09:30.490
So here we can clearly see that our logistic regression classifier manages to catch most of the users

135

00:09:30.820  -->  00:09:33.460
who didn't buy the SUV in the red region here.

136

00:09:33.700  -->  00:09:37.990
And most of the users who bought the SUV in the green region here.

137

00:09:37.990  -->  00:09:40.220
So it actually did a pretty good job.

138

00:09:40.240  -->  00:09:47.230
However it seems to have trouble catching some green users here who in spite of their low salary bought

139

00:09:47.320  -->  00:09:54.220
the luxury SUV as well as those other green users here who also bought the luxury SUV because as you

140

00:09:54.220  -->  00:10:00.980
can see these green points here and those here are in the red region which is a region where our classifier

141

00:10:01.000  -->  00:10:08.580
predicts that the users don't buy the SUV those incorrect predictions are Jews specifically to the fact

142

00:10:08.590  -->  00:10:16.000
that I classify it as a linear classifier and because our users are not linearly distributed if they

143

00:10:16.000  -->  00:10:21.550
were linearly distribution then we will have all the green points here in the space and all the red

144

00:10:21.550  -->  00:10:26.650
points here in the space and then a linear classifier with a straight line could perfectly separate

145

00:10:26.760  -->  00:10:29.040
all the red points here and all the green points here.

146

00:10:29.250  -->  00:10:35.130
But here we have some rebellious points who are not in the want of linear regions and because our classifier

147

00:10:35.130  -->  00:10:37.650
has a linear straight line separator.

148

00:10:37.650  -->  00:10:43.420
That's why it has trouble catching those users here and those here you can clearly see that even if

149

00:10:43.410  -->  00:10:50.160
you try to rotate this straight line here well you will always have some green points in the wrong category

150

00:10:50.160  -->  00:10:50.500
.

151

00:10:50.560  -->  00:10:55.000
For example if we try to rotate here this way like putting it down.

152

00:10:55.240  -->  00:10:59.290
Well OK we will catch these green points here and the right green region here.

153

00:10:59.470  -->  00:11:07.390
But since we rotated down we will take more green users here because this will go up and more green

154

00:11:07.380  -->  00:11:10.030
users will be in the red region.

155

00:11:10.020  -->  00:11:12.460
So that's the best separator.

156

00:11:12.460  -->  00:11:17.860
The logistic regression test very good fine and it couldn't do better because it can only be a straight

157

00:11:17.860  -->  00:11:23.130
line separating these two regions because to catch those users the green users here and the greenies

158

00:11:23.120  -->  00:11:27.550
users here in the right category that is the green region are classified we need to make some kind of

159

00:11:27.550  -->  00:11:34.390
a curve here to you know classify correctly those green users here and here and place them in a green

160

00:11:34.380  -->  00:11:39.910
region and that would prevent our class very from making this incorrect predictions here because it

161

00:11:39.900  -->  00:11:45.870
is a straight line with a curve here with catch all the red uses probably in the red region and all

162

00:11:45.880  -->  00:11:47.960
the green users in the green region.

163

00:11:47.980  -->  00:11:53.610
So that would make an awesome classifier and you will see how our nonlinear classifiers will make a

164

00:11:53.620  -->  00:11:55.270
terrific job in doing this.

165

00:11:55.260  -->  00:11:56.950
I can't wait to show you this.

166

00:11:57.440  -->  00:11:57.870
OK.

167

00:11:57.880  -->  00:12:03.570
And now eventually the last thing very important to understand is that this is the trainset.

168

00:12:03.730  -->  00:12:09.660
This is a training center that means that our classifier learned how to classify based on these informations

169

00:12:09.660  -->  00:12:10.270
here.

170

00:12:10.260  -->  00:12:15.400
So I would hold my breath a few more seconds until I find out if our logistic regression classifier

171

00:12:15.420  -->  00:12:18.940
can manage to make good predictions of new observations.

172

00:12:18.970  -->  00:12:24.720
That is to classify new users into the right regions which by the way are fixed regions here because

173

00:12:25.060  -->  00:12:30.430
these are the regions generated by the learning experience of our logistic regression classifier and

174

00:12:30.420  -->  00:12:36.990
therefore won't change if we look at some new observations that is new social network users and that's

175

00:12:37.000  -->  00:12:39.550
what we are about to find out on the test set.

176

00:12:39.550  -->  00:12:40.550
So hold on.

177

00:12:40.890  -->  00:12:47.090
So it's very simple we're just going to copy all this code section here.

178

00:12:48.150  -->  00:12:54.830
Paste it here and I'm just going to change the training set here by test set.

179

00:12:54.850  -->  00:12:55.720
Same here.

180

00:12:55.720  -->  00:12:59.280
I change training set by test sets and that's all.

181

00:12:59.590  -->  00:13:04.230
That's all because I structure the codes in such a way that we only need to change the trendsetting

182

00:13:04.230  -->  00:13:08.740
to the test set here to plot this graph on a specific set.

183

00:13:08.740  -->  00:13:15.850
However let's change the title here because we want to specify that it's the test set and it's ready

184

00:13:15.860  -->  00:13:16.200
.

185

00:13:16.650  -->  00:13:21.070
So let's select this and execute.

186

00:13:22.170  -->  00:13:24.240
Let's see what happens.

187

00:13:25.360  -->  00:13:28.700
And here are the results of the test set.

188

00:13:28.750  -->  00:13:29.830
So that's not too bad.

189

00:13:29.860  -->  00:13:35.530
That's not too bad because as we can see the major the majority of red points are in the right region

190

00:13:35.530  -->  00:13:35.680
.

191

00:13:35.760  -->  00:13:37.800
That means the region predicted to be zero.

192

00:13:38.080  -->  00:13:41.970
And the majority of green points are in the right region.

193

00:13:42.580  -->  00:13:46.960
As for the training center there are some observations that were incorrectly predicted.

194

00:13:46.990  -->  00:13:52.260
That's normal that's because it's a linear classifier and it can not make a curve here catching all

195

00:13:52.250  -->  00:13:54.490
the right guys.

196

00:13:54.940  -->  00:14:01.820
And remember in the confusion matrix we counted ten plus seven incorrect predictions.

197

00:14:02.200  -->  00:14:08.300
And if you want to count the incorrect predictions here you will find 17 incorrect predictions.

198

00:14:08.320  -->  00:14:11.610
All right so that's it for the interpretation of the graph.

199

00:14:11.620  -->  00:14:15.050
I can't wait to show you how we can make more powerful classifiers.

200

00:14:15.190  -->  00:14:16.930
So it's going to be in the next sections.

201

00:14:16.960  -->  00:14:20.660
And of course these are going to be non-linear or classifiers.

202

00:14:21.220  -->  00:14:25.520
So for those of you who are interested in knowing how the code works.

203

00:14:25.570  -->  00:14:27.350
Stay with me or we'll explain that.

204

00:14:27.520  -->  00:14:35.120
So I will reduce this OK and let's explain this.

205

00:14:35.590  -->  00:14:36.120
OK.

206

00:14:36.490  -->  00:14:43.210
So the idea is that we consider each of this pixel observation point as a user and the social network

207

00:14:43.210  -->  00:14:45.250
like an imaginative user.

208

00:14:45.580  -->  00:14:51.700
And so you know for example this pixel point here is not a user in the data set but we imagine this

209

00:14:51.700  -->  00:14:59.640
pixel point here as a user who has this salary and this age and we apply our classifier on this pixel

210

00:14:59.640  -->  00:15:05.500
observation point on this user here so that the classifier predicts if the user is going to buy yes

211

00:15:05.500  -->  00:15:06.140
or no.

212

00:15:06.250  -->  00:15:10.940
The SUV then wants to classify your makes its prediction.

213

00:15:11.020  -->  00:15:15.420
It's called rises the pixel observation point according to the prediction.

214

00:15:15.580  -->  00:15:22.020
So if the prediction is no this pixel use one by the SUV then it will colorize in red.

215

00:15:22.170  -->  00:15:29.040
And if the prediction is yes this pixel user will buy the SUV then it will colorize the point in green

216

00:15:29.050  -->  00:15:29.690
.

217

00:15:29.880  -->  00:15:36.870
And so we apply that idea on all the pixels in this frame so that eventually our classifier will colorize

218

00:15:37.030  -->  00:15:43.870
all the points that he predicted at zero in red and all the points that he predicted as one in green

219

00:15:43.880  -->  00:15:44.260
.

220

00:15:44.620  -->  00:15:49.780
So now that you get the idea let's look at our code here and go through the steps of completing this

221

00:15:49.780  -->  00:15:50.730
idea.

222

00:15:51.310  -->  00:15:51.830
OK.

223

00:15:51.900  -->  00:15:55.730
So first I declare a set of cross training set.

224

00:15:55.890  -->  00:16:00.050
That's because you know I want to plot this graph for the Trenin set and set.

225

00:16:00.280  -->  00:16:05.340
And since we're using Trenin set many times in the code I just replaced it by set here.

226

00:16:05.430  -->  00:16:09.650
So that's when I copy paste the same code for the test set.

227

00:16:09.700  -->  00:16:14.030
I just need to replace the set by trainset only here and not in the whole code.

228

00:16:14.430  -->  00:16:16.750
So basically that's just the shortcut.

229

00:16:16.840  -->  00:16:20.550
And so first we build a grid with x 1 and x 2.

230

00:16:20.830  -->  00:16:25.540
So we take the minimum of the values of the trainset minus one because we don't want the points to be

231

00:16:25.540  -->  00:16:27.180
squeezed in the graph.

232

00:16:27.180  -->  00:16:29.390
And same for the maximum plus 1.

233

00:16:29.740  -->  00:16:36.040
So by doing this we're taking the range of our training set observation points minus 1 plus 1 so that

234

00:16:36.230  -->  00:16:38.440
so that our points are not squeezed in the graph.

235

00:16:38.740  -->  00:16:44.800
And so we're doing this for The Age column of the trainset and the salary column of the training set

236

00:16:46.120  -->  00:16:48.730
then by writing this like we are building the grid.

237

00:16:48.750  -->  00:16:53.260
So basically with these three lines we're making the grid here with all the pixel observation points

238

00:16:53.530  -->  00:16:56.790
that are our imaginary social network users.

239

00:16:56.800  -->  00:17:01.510
Then since this grid set is actually a matrix of the two columns age and salary.

240

00:17:01.600  -->  00:17:05.720
But for all the imaginary users that are the big SO observation points.

241

00:17:05.830  -->  00:17:07.000
This is actually a matrix.

242

00:17:06.990  -->  00:17:12.400
So with this line we just give a name to the columns of this matrix which are age and estimated salary

243

00:17:13.720  -->  00:17:16.500
and then that's where all the magic happens.

244

00:17:16.740  -->  00:17:25.120
Because here that's where we use our classifier to predict the result of each of the pixel observation

245

00:17:25.120  -->  00:17:28.080
points that are the imaginary pixel users.

246

00:17:28.090  -->  00:17:34.660
So we predict this and then you know since the printing function returns the probabilities we transform

247

00:17:34.650  -->  00:17:38.060
it into one or zero result.

248

00:17:38.080  -->  00:17:43.830
So that's why I call it WEIGERT because it's the predictions of all the points in the grid so that returns

249

00:17:43.850  -->  00:17:46.850
a vector of predictions of all the points in the grid.

250

00:17:47.110  -->  00:17:50.270
And then finally we plot the whole graph.

251

00:17:50.290  -->  00:17:57.330
So in this plot we include all our real users and their real actions read.

252

00:17:57.370  -->  00:18:03.170
If they didn't buy the car and green if they bought the car and we plug the predicted results of all

253

00:18:03.170  -->  00:18:08.480
the pixel observation points that we created when we created the grid.

254

00:18:08.990  -->  00:18:10.020
So that's here.

255

00:18:10.380  -->  00:18:16.000
And so you know I'm using the color of spring green so that is the spring green color and tomato that

256

00:18:15.990  -->  00:18:17.550
is the tomato color.

257

00:18:18.200  -->  00:18:22.650
And for the real points I just use a green for which is the color and are and what three.

258

00:18:22.780  -->  00:18:26.210
So this color is green for this color is red.

259

00:18:26.200  -->  00:18:27.990
3 and that's it.

260

00:18:27.990  -->  00:18:29.970
That's the idea behind this code.

261

00:18:29.970  -->  00:18:33.170
It's totally fine if you didn't understand some things in this code.

262

00:18:33.390  -->  00:18:38.030
Because anyway we're going to use this code as a template for our next classifiers.

263

00:18:38.160  -->  00:18:41.350
And so we'll just have to copy paste this code but that's all.

264

00:18:41.560  -->  00:18:46.500
So that was just for those of you who are interested in coding who are interested in how we can use

265

00:18:46.510  -->  00:18:48.180
code to make such a plot.

266

00:18:48.370  -->  00:18:54.730
But the important thing to understand here is that the logistic regression is a linear classifier which

267

00:18:54.780  -->  00:19:00.690
in two dimensions means that it's a linear r separator and therefore it can miss some predictions like

268

00:19:00.700  -->  00:19:03.060
the predictions here that are incorrect.

269

00:19:03.340  -->  00:19:07.540
All right so thank you for watching this Arta toils and congratulations.

270

00:19:07.530  -->  00:19:11.880
Now you know how to implement a logistic regression model using our.

271

00:19:11.880  -->  00:19:15.600
This is the beginning of our journey in the classifiers world.

272

00:19:15.610  -->  00:19:20.230
You're going to see that we will meet some more powerful classifiers later in our journey.

273

00:19:20.280  -->  00:19:22.160
So I look forward to continue this journey.

274

00:19:22.300  -->  00:19:24.220
And until then enjoy machine learning
