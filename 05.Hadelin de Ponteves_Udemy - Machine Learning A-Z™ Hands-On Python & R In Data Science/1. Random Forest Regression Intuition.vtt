WEBVTT
1

00:00:01.030  -->  00:00:03.270
Hello and welcome back to the course of machine learning.

2

00:00:03.270  -->  00:00:06.760
Today we're talking about that random force and the intuition behind it.

3

00:00:06.900  -->  00:00:11.640
And specifically we're going to be talking about the rainforest applied to regression trees rather than

4

00:00:11.640  -->  00:00:16.140
classification resistance but the concept is very simple and you'll find that this story is very similar

5

00:00:16.140  -->  00:00:19.710
to the one for the random forest on classification trees.

6

00:00:19.710  -->  00:00:19.970
All right.

7

00:00:19.980  -->  00:00:24.170
So a random forest and ensemble learning ensemble learning.

8

00:00:24.260  -->  00:00:29.550
So random force is a version of ensemble learning you've got other versions such as gradient boosting

9

00:00:29.970  -->  00:00:39.900
and ensemble learning is when you take multiple algorithms or the same algorithm multiple times and

10

00:00:39.900  -->  00:00:43.760
you put them together to make something much more powerful than the original.

11

00:00:44.130  -->  00:00:45.660
And let's see how this works.

12

00:00:45.660  -->  00:00:51.570
So when you pick a random Kadir points from your training said so now we're kind of going to leverage

13

00:00:51.570  -->  00:00:56.300
a lot what we talked about in the session on regression entries.

14

00:00:56.430  -->  00:01:03.750
So you remember there we had lots of data points and then we built a regression tree and we put the

15

00:01:03.750  -->  00:01:12.510
decision tree and use that to forecast the value that would be assigned or the y value for any new element

16

00:01:12.510  -->  00:01:15.960
that will be added to our data set as the average in the terminal leaves space.

17

00:01:16.230  -->  00:01:19.160
So here what we're doing is we're using the whole dataset we had.

18

00:01:19.170  -->  00:01:22.470
And we're only picking K data points from that training set.

19

00:01:22.470  -->  00:01:29.730
Then we're going to build a decision tree associated to these K data points rather than building a decision

20

00:01:29.730  -->  00:01:34.860
tree based on everything you're done I said you just building a decision tree based on those data points

21

00:01:34.860  -->  00:01:40.430
that just like a subset of your arse and then you choose the number of trees that you want to build

22

00:01:40.440  -->  00:01:41.640
and you repeat steps on it.

23

00:01:41.640  -->  00:01:47.280
So you just keep building and building and building these streets or building a lot of regression decision

24

00:01:47.280  -->  00:01:48.010
trees.

25

00:01:48.210  -->  00:01:54.540
And then finally you as you use all of them to predict so far and you Darb point make each one of your

26

00:01:54.540  -->  00:02:02.070
entries predict the value of y for the data point in question and assign the new data point that average

27

00:02:02.130  -->  00:02:04.650
across all of the predicted y values.

28

00:02:04.650  -->  00:02:10.050
So basically instead of just getting one prediction you getting lot of prediction by default.

29

00:02:10.080  -->  00:02:13.770
See these algorithms are set to about 500 trees at least.

30

00:02:13.770  -->  00:02:17.380
So you're getting 500 predictions for the value of y.

31

00:02:17.700  -->  00:02:19.490
And then you taking the average across those.

32

00:02:19.680  -->  00:02:25.600
And in that way you're not just predicting one tree be ridiculed based on a forest of trees.

33

00:02:25.710  -->  00:02:31.830
And that improves the accuracy of your prediction because it is you're taking the average of many predictions

34

00:02:31.830  -->  00:02:39.930
and therefore even if one is some perseverance and somehow one of the decision trees was built exactly

35

00:02:39.930  -->  00:02:45.120
perfectly because the way of those data points were selected it just didn't turn out as a perfect tree

36

00:02:45.690  -->  00:02:49.460
or a gray tree even if you're using it by itself.

37

00:02:49.470  -->  00:02:53.400
You'd get a bad prediction because using the average it is less likely.

38

00:02:53.400  -->  00:02:56.070
So you're going to get a more accurate prediction and more.

39

00:02:56.100  -->  00:03:01.110
And the second thing is that there are more stable algorithms like this ensemble algorithms are more

40

00:03:01.110  -->  00:03:05.870
stable because any changes in your data set could really impact one tree.

41

00:03:05.880  -->  00:03:13.440
But two for them to really impact a forest of trees is much harder so therefore ensemble is much more

42

00:03:13.440  -->  00:03:15.160
powerful in that way.

43

00:03:15.420  -->  00:03:23.430
And what this reminds me of is the game that is often played at fairs or parties and things like that

44

00:03:23.430  -->  00:03:27.720
where you have a jar and inside this jar is lots and lots of.

45

00:03:27.810  -->  00:03:34.380
For instance jelly beans or it could be marbles or they could be like huge nets with balloons inside

46

00:03:34.380  -->  00:03:34.550
it.

47

00:03:34.560  -->  00:03:41.850
And we have one in the mall sometimes where there's lots of balloons inside and that's in the ceiling

48

00:03:41.860  -->  00:03:42.170
.

49

00:03:42.360  -->  00:03:48.430
And you need to guess how many balloons are and whoever guesses will get like a car can win a car.

50

00:03:48.450  -->  00:03:52.170
It's a crazy prize for just guessing a number of balloons.

51

00:03:52.470  -->  00:04:01.470
And although this is not an example of specifically a random forest or aggression on forest method it's

52

00:04:01.470  -->  00:04:04.520
still an example of an ensemble type of method.

53

00:04:04.620  -->  00:04:12.750
So the best way or one of the ways to beat that game when you need to guess the number of marbles in

54

00:04:12.750  -->  00:04:15.800
a jar for instance is not to actually go in.

55

00:04:15.820  -->  00:04:22.000
Yes but it's actually to get a pen and a paper and stand next to the person that's holding this jar

56

00:04:22.000  -->  00:04:24.070
or that's conducting this event.

57

00:04:24.240  -->  00:04:28.680
And you just stand next to them and then you wait for other people to come and guess every time somebody

58

00:04:28.680  -->  00:04:33.960
guesses you just ask them as soon as they're like guests and then walking away you ask them Hey.

59

00:04:34.260  -->  00:04:39.810
Because usually they write down their number and they put it in inside a like an envelope or something

60

00:04:39.810  -->  00:04:42.410
and then there is the winner is announced later on.

61

00:04:42.570  -->  00:04:44.090
So they don't know whether that gets right or wrong.

62

00:04:44.100  -->  00:04:48.260
But regardless of that they're walking away and they're just asking hey what number did you guess.

63

00:04:48.270  -->  00:04:52.050
And you just write down their number and then the next person guesses and you're right there number

64

00:04:52.050  -->  00:04:56.100
down and you're right there number and you keep writing the numbers down and you just keep doing that

65

00:04:56.130  -->  00:05:02.070
until you have like a substantial number of entries maybe 100 or maybe if it's a very popular contest

66

00:05:02.100  -->  00:05:08.970
and people are guessing like crazy like trying to tempt or attempting the guessing then you might even

67

00:05:08.970  -->  00:05:14.100
get like a couple of hundred or even if you're very determined you might get thousands of entries over

68

00:05:14.100  -->  00:05:15.150
a couple of days.

69

00:05:15.300  -->  00:05:20.100
And then what you do is you just averaged them out or if you don't want maybe maybe take the median

70

00:05:20.400  -->  00:05:25.680
if you don't want to outliers like people just guessing random numbers like one or 5 million that you

71

00:05:25.680  -->  00:05:29.440
don't want them to affect or you just take the outliers out and then you average out and you're either

72

00:05:29.480  -->  00:05:36.300
average it out or you take the median and statistically speaking you have a much higher likelihood of

73

00:05:36.300  -->  00:05:42.230
being closer to the truth if you take the average of people because people are natural beings and they're

74

00:05:42.270  -->  00:05:49.830
kind of visual perception will be most likely normally distributed and therefore you once you hit the

75

00:05:49.830  -->  00:05:55.410
middle of that normal distribution you are more likely to be on the money than any one of them.

76

00:05:55.410  -->  00:06:00.790
And that's pretty cool concept that that's an example of an ensemble method where you're taking instead

77

00:06:00.800  -->  00:06:05.050
of just drawing that guess by yourself or taking the guess of one individual person.

78

00:06:05.050  -->  00:06:11.880
You're averaging out across multiple guesses and you're more likely to be the closest one to the truth

79

00:06:11.890  -->  00:06:12.080
.

80

00:06:12.300  -->  00:06:17.910
And if the prize is given not just to the person that gets a spot on but to the person I guess is closest

81

00:06:17.910  -->  00:06:23.440
to the truth then you've got yourself a very powerful advantage using data size.

82

00:06:23.450  -->  00:06:28.380
So if you if you have the patience and determination then try it out next time you see one of these

83

00:06:28.380  -->  00:06:30.000
games and see how you go.

84

00:06:30.120  -->  00:06:33.820
We'd love to hear back from you because I never have the patience to stand there and just count.

85

00:06:33.990  -->  00:06:38.510
But it is it is a statistical approach to a challenge like that.

86

00:06:38.790  -->  00:06:40.710
So hopefully you enjoyed this tutorial.

87

00:06:40.710  -->  00:06:42.070
I'll look for you next time.

88

00:06:42.090  -->  00:06:43.860
Until then in Germany learning
