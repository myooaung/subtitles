1
00:00:00,390 --> 00:00:06,040
Hello, my friends, and welcome to the final practical activity of this course.

2
00:00:06,460 --> 00:00:12,480
Yes, I must start by saying that I am at the same time excited but sad that this is the end of the

3
00:00:12,480 --> 00:00:12,930
journey.

4
00:00:13,170 --> 00:00:14,070
But no worries.

5
00:00:14,100 --> 00:00:17,310
We're going to end on a very, very good note.

6
00:00:17,850 --> 00:00:20,670
And that good note is about, of course, extra boost.

7
00:00:20,850 --> 00:00:27,950
It is a super powerful machinery model, which I absolutely want you to have in the toolkit, because

8
00:00:27,960 --> 00:00:32,670
you will see that it brings excellent results in most machinery problems.

9
00:00:33,060 --> 00:00:39,750
And actually, the things so cool about this is that it can be both used for regression and classification.

10
00:00:40,110 --> 00:00:41,010
So there we go.

11
00:00:41,130 --> 00:00:43,530
Let's cross the finish line together.

12
00:00:43,640 --> 00:00:50,790
And this final tutorial by implementing the extra boost morale so that morale is given to you in part

13
00:00:50,790 --> 00:00:51,230
10.

14
00:00:51,480 --> 00:00:55,410
So just before we enter this part, make sure to be on that same page.

15
00:00:55,500 --> 00:00:59,160
I give you the link to this for the right before this story or in the article.

16
00:00:59,370 --> 00:01:00,660
So make sure we connect to it.

17
00:01:00,960 --> 00:01:02,040
And now here we go.

18
00:01:02,160 --> 00:01:07,950
Let's finish this journey together by entering points and then the final section of this course, section

19
00:01:07,950 --> 00:01:10,330
forty nine on x g boost.

20
00:01:10,950 --> 00:01:11,340
All right.

21
00:01:11,370 --> 00:01:15,280
And as usual, we're gonna start with Python, which contains two files.

22
00:01:15,450 --> 00:01:16,620
First data.

23
00:01:17,010 --> 00:01:18,900
And second, the implementation.

24
00:01:19,500 --> 00:01:25,230
And now you probably also noticed that there are many files open now on my machine.

25
00:01:25,260 --> 00:01:25,560
Right.

26
00:01:25,620 --> 00:01:26,460
All these files.

27
00:01:26,880 --> 00:01:31,740
And these are, you know, the files that we implemented so quickly and efficiently.

28
00:01:32,040 --> 00:01:37,670
When doing that, Morales selection demo at the end of Boit three classification.

29
00:01:38,090 --> 00:01:38,380
Right.

30
00:01:38,490 --> 00:01:44,610
I gave you this model selection folder with all these classification models which were experimented

31
00:01:44,670 --> 00:01:52,530
on the same data set, which is that dataset data that C V and which I remind consists of predicting

32
00:01:52,680 --> 00:01:59,940
if a breast cancer tumor is benign or malignant, meaning that each row of this data set corresponds

33
00:01:59,940 --> 00:02:02,190
to a patient, you know, is a certain patient.

34
00:02:02,550 --> 00:02:08,880
And for each of these patients, we have several features from the clump thickness, the uniformity

35
00:02:08,880 --> 00:02:15,360
of cell sized uniformity of cell shape, you know, all these features that are characteristics of a

36
00:02:15,360 --> 00:02:15,870
tumor.

37
00:02:16,260 --> 00:02:21,900
And with all these features, we were trying to predict if the tumor is benign or malignant, it is

38
00:02:21,900 --> 00:02:22,410
benign.

39
00:02:22,500 --> 00:02:29,370
If we get the class two and malignant if we get the class for all right and we build and trained all

40
00:02:29,370 --> 00:02:35,610
of our classification model, which are all the ones right here to learn the correlations between all

41
00:02:35,730 --> 00:02:41,730
these features and that Dippin viable telling if the tumor is benign or malignant.

42
00:02:42,270 --> 00:02:48,690
And remember that we had different accuracy's for each of these models with the logistic regression

43
00:02:48,690 --> 00:02:48,960
model.

44
00:02:48,960 --> 00:02:54,020
We had an accuracy of ninety four point seven percent with the Kimura's neighbors.

45
00:02:54,060 --> 00:02:56,820
We had an accuracy of ninety four point seven percent.

46
00:02:56,820 --> 00:03:04,270
Again, with the SVM, we had an accuracy of ninety four point one percent with the kernel SVM.

47
00:03:04,290 --> 00:03:10,290
We had a better accuracy actually, with ninety five point three percent with native Bayes.

48
00:03:10,530 --> 00:03:14,070
We got a lower accuracy of ninety four point one percent again.

49
00:03:14,370 --> 00:03:18,060
And with decision tree classification, well, that was the winner.

50
00:03:18,270 --> 00:03:22,620
We got an amazing accuracy of ninety five point nine percent.

51
00:03:22,710 --> 00:03:28,590
That was the number one on the podium, followed by the kernel SVM with that accuracy.

52
00:03:28,620 --> 00:03:35,010
And then, unfortunately, Random Forest did not do any good or, you know, not better than the others

53
00:03:35,250 --> 00:03:39,120
because we got a ninety three point five percent accuracy with it.

54
00:03:39,750 --> 00:03:47,220
And so what I want to do now, as you probably have guessed, is to build the extra boost morale and

55
00:03:47,220 --> 00:03:55,500
train it on the same data set to see if it can take the throne, hold by the decision tree classification

56
00:03:55,500 --> 00:03:55,890
model.

57
00:03:56,250 --> 00:04:02,550
In other words, to see if it can beat that accuracy obtained with the decision tree classification

58
00:04:02,550 --> 00:04:03,500
model and.

59
00:04:03,510 --> 00:04:07,440
Well, maybe, maybe that's the good note on which we will.

60
00:04:07,500 --> 00:04:09,090
And the journey of discourse.

61
00:04:09,510 --> 00:04:11,310
Are you ready to let us do this?

62
00:04:11,460 --> 00:04:18,630
We're now going to build and train are extra boost morale on the exact same data set and see if we can

63
00:04:18,630 --> 00:04:22,650
beat basically an accuracy of ninety five point nine percent.

64
00:04:22,980 --> 00:04:28,320
And not only we will test that on a single test set, but also, of course, now that we learned Caple

65
00:04:28,320 --> 00:04:34,290
cross-validation in the previous section, we will test this on ten test false so that we can get a

66
00:04:34,290 --> 00:04:41,130
relevant measure of the accuracy and make sure that perhaps extra boost will now become the number one

67
00:04:41,130 --> 00:04:44,770
on the podium with the ultimate machine learning thrown.

68
00:04:45,000 --> 00:04:46,780
So let's check this out right now.

69
00:04:46,900 --> 00:04:52,230
Let's open this implementation with either Google Collaboratory or Dupere notebook.

70
00:04:52,680 --> 00:04:54,010
I'm gonna put it last.

71
00:04:54,120 --> 00:04:58,710
You know, just next to all our other classification models and.

72
00:04:59,750 --> 00:05:00,880
The notebook just opened.

73
00:05:00,920 --> 00:05:06,920
But it is still in read only mode, so we're going to create a copy right away by clicking Save a copy

74
00:05:06,920 --> 00:05:07,350
and drive.

75
00:05:07,370 --> 00:05:12,950
You can notice that all these are copies of the original implementations which are right here, you

76
00:05:12,950 --> 00:05:13,060
know.

77
00:05:13,090 --> 00:05:18,860
That's the moral selection folder and then classification subfolder, which I give you at the end of

78
00:05:18,860 --> 00:05:19,460
point three.

79
00:05:19,730 --> 00:05:21,980
So you can run this goods again if you want.

80
00:05:22,010 --> 00:05:23,630
But we already did that.

81
00:05:23,720 --> 00:05:28,700
Just remember that the number one was indeed decision tree classification with an accuracy of ninety

82
00:05:28,700 --> 00:05:30,140
five point nine percent.

83
00:05:30,440 --> 00:05:34,310
And now we're gonna see if we can beat this with extra boost.

84
00:05:35,270 --> 00:05:35,900
All right.

85
00:05:35,990 --> 00:05:38,360
So, of course, no worries.

86
00:05:38,390 --> 00:05:40,240
We want reimplement all this.

87
00:05:40,250 --> 00:05:47,120
We will quickly get to the core of the implementation and mostly the exciting part, which are the results

88
00:05:47,210 --> 00:05:53,930
in the same tutorial, because indeed, all the sales of this implementation are just cells taken from

89
00:05:53,930 --> 00:05:54,860
our diverse toolkit.

90
00:05:55,190 --> 00:05:55,520
Right.

91
00:05:55,640 --> 00:06:01,530
These three first sales are, as you recognized perfectly, the sales of our data pre pricing template.

92
00:06:01,700 --> 00:06:01,940
Right.

93
00:06:01,970 --> 00:06:05,840
We first import the libraries and we import the dataset with the exact same code.

94
00:06:06,080 --> 00:06:10,910
I just put the name of the dataset here and then we split the datasets into the training set and test

95
00:06:10,910 --> 00:06:11,240
set.

96
00:06:11,780 --> 00:06:13,840
So this is all the data pricing phase.

97
00:06:13,940 --> 00:06:16,520
Then we train extra boots on a training set.

98
00:06:16,610 --> 00:06:21,740
And of course, I'm going to delete the sale right away because that's the cell we will reimplement

99
00:06:21,740 --> 00:06:22,280
together.

100
00:06:22,790 --> 00:06:27,710
And then we have the other tools of our other two kids, like the classification tool kit, because

101
00:06:27,710 --> 00:06:33,290
indeed this sale makes the confusion, matrix and prints at the same time the accuracy.

102
00:06:33,590 --> 00:06:39,410
I actually already deleted the output to make sure we get the full surprise by the end of this tutorial.

103
00:06:39,800 --> 00:06:45,530
And then, of course, as I told you, we are going to apply K4 cross-validation right at the end to

104
00:06:45,530 --> 00:06:49,460
make sure that indeed we didn't get lucky on the test set.

105
00:06:49,490 --> 00:06:55,160
You know, if we indeed can beat all the other algorithms so we will not only get a first measure of

106
00:06:55,160 --> 00:07:01,280
the performance thanks to a single test set with this cell, and then we'll get the ultimate measure

107
00:07:01,280 --> 00:07:03,560
of the accuracy with that cell.

108
00:07:04,280 --> 00:07:04,940
I'm ready.

109
00:07:05,130 --> 00:07:11,480
Let's start by building and training extra boost on the training set, which resulted from the split

110
00:07:11,480 --> 00:07:16,790
of the data set between the training set and test set and first in order to get the assistance of Google

111
00:07:16,790 --> 00:07:17,230
Kulab.

112
00:07:17,270 --> 00:07:22,220
Well, let's apply this reflex of uploading the data into the notebook.

113
00:07:22,640 --> 00:07:27,860
So, you know, I just clicked this folder button here and in a second we'll see the upload button to

114
00:07:27,980 --> 00:07:29,600
upload indeed our data set.

115
00:07:29,690 --> 00:07:30,500
So let's click it.

116
00:07:30,920 --> 00:07:33,230
And now let's go to our machine learning.

117
00:07:33,330 --> 00:07:35,060
Is it codes and data sets folder?

118
00:07:35,090 --> 00:07:40,040
Because you will still find the data that says we found in this folder in part then, of course.

119
00:07:40,310 --> 00:07:42,140
So let's go into this folder then.

120
00:07:42,140 --> 00:07:46,610
Let's go into part ten, then section forty nine, extra boost by phone.

121
00:07:46,790 --> 00:07:52,700
And here is the data set data at we of many patients with tumors for which we have to predict if the

122
00:07:52,700 --> 00:07:54,710
tumor is benign or malignant.

123
00:07:55,040 --> 00:07:55,820
So open.

124
00:07:56,480 --> 00:07:57,200
Okay.

125
00:07:57,620 --> 00:08:00,590
And now the data set is indeed uploaded into the notebook.

126
00:08:00,770 --> 00:08:01,640
So there we go.

127
00:08:01,760 --> 00:08:02,980
We can implement that.

128
00:08:03,290 --> 00:08:05,120
And then run the whole code.

129
00:08:05,450 --> 00:08:05,660
All right.

130
00:08:05,690 --> 00:08:07,160
So let's create a new code cell.

131
00:08:07,550 --> 00:08:08,360
And there we go.

132
00:08:08,450 --> 00:08:11,600
Let's build and train extra boost on the training set.

133
00:08:12,050 --> 00:08:14,660
So you're gonna see that it's going to be super easy.

134
00:08:14,870 --> 00:08:20,660
And in fact, we won't even do it with psychic learn, but with a library called Extra Boost in which

135
00:08:20,660 --> 00:08:27,230
we don't even have to install thanks to Google Kulab, because it is one of the many packages already

136
00:08:27,230 --> 00:08:28,370
installed on Google.

137
00:08:28,370 --> 00:08:30,140
Kulab, you know, already preinstalled.

138
00:08:30,410 --> 00:08:35,870
So we have nothing to worry about and we can just start building and training this extra boost model.

139
00:08:36,260 --> 00:08:39,860
But first, we're gonna import the class with which we're gonna build this.

140
00:08:40,130 --> 00:08:43,130
And this class belongs, of course, to this extra boost library.

141
00:08:43,340 --> 00:08:44,040
So there we go.

142
00:08:44,060 --> 00:08:48,280
We're gonna start from this x g boost library, right?

143
00:08:48,320 --> 00:08:51,050
It's spelled this way, just like the name of the model.

144
00:08:51,320 --> 00:08:52,400
Extra boost, indeed.

145
00:08:52,730 --> 00:08:55,220
And from this library, we're gonna import.

146
00:08:55,460 --> 00:09:02,150
Well, the class that can build an extra boost classification model and which is called x gibi.

147
00:09:02,480 --> 00:09:03,020
There we go.

148
00:09:03,080 --> 00:09:06,020
Google Kulab founded x chibi classifier.

149
00:09:06,500 --> 00:09:06,980
All right.

150
00:09:07,040 --> 00:09:13,160
And now the next natural step, as usual, is to create an instance of this class, which will be exactly

151
00:09:13,160 --> 00:09:16,490
the object containing the extra boost morale.

152
00:09:16,670 --> 00:09:19,430
So once again, we're gonna call it classifier.

153
00:09:20,780 --> 00:09:21,290
All right.

154
00:09:21,440 --> 00:09:28,160
And we'll create this classifier as an instance, indeed, of the x gibi classifier class.

155
00:09:28,370 --> 00:09:28,940
Perfect.

156
00:09:29,330 --> 00:09:34,550
And now the good news is that we won't have too much to worry about with this class because there is

157
00:09:34,550 --> 00:09:36,080
not much parameter to tune.

158
00:09:36,080 --> 00:09:36,440
Right.

159
00:09:36,710 --> 00:09:42,110
Basically, the default version of the extra boost morale will most of the time perform super well.

160
00:09:42,290 --> 00:09:43,460
So all good here.

161
00:09:43,700 --> 00:09:49,700
And now, of course, we finished this by connecting this extra was classifier to our training set.

162
00:09:50,030 --> 00:09:55,760
And the way to do this is by, of course, calling the fit method from our classifier object, which

163
00:09:55,760 --> 00:10:01,370
will do nothing else than train this extra boost classifier on the training set.

164
00:10:01,670 --> 00:10:01,910
Right.

165
00:10:01,940 --> 00:10:04,490
So something we did many times.

166
00:10:04,610 --> 00:10:05,230
So there we go.

167
00:10:05,240 --> 00:10:08,990
Let's do it one last time in this whole machine learning journey.

168
00:10:08,990 --> 00:10:13,800
But then I'm sure you will do it many times once again in the future, in your future machinery and

169
00:10:13,820 --> 00:10:14,290
career.

170
00:10:14,580 --> 00:10:15,410
So let's do this.

171
00:10:15,530 --> 00:10:23,810
We call our classifier from which we're going to call this fit method, which will train the classifier

172
00:10:24,050 --> 00:10:29,600
on the training set, which is composed of, first, the features of the training set represented by

173
00:10:29,710 --> 00:10:36,140
X train and then the dependent variable of the training set represented by Y train.

174
00:10:36,260 --> 00:10:40,100
And these are exactly, of course, the inputs of the fit method.

175
00:10:41,120 --> 00:10:41,630
All right.

176
00:10:41,810 --> 00:10:48,380
So in the flashiest of flashes, we built and trained this extra boost morale on the training set.

177
00:10:48,590 --> 00:10:50,900
We only had to implement the three lines of code.

178
00:10:50,990 --> 00:10:53,750
And then all the rest is something we've already done, you know.

179
00:10:53,780 --> 00:10:55,190
This is the confusing matrix.

180
00:10:55,280 --> 00:10:58,610
You have this code and all of your classification templates.

181
00:10:58,880 --> 00:11:03,620
And finally, of course, we apply the exact same scale that we implemented just before in the previous

182
00:11:03,620 --> 00:11:06,600
section to apply K4 cross-validation.

183
00:11:06,980 --> 00:11:08,480
So we are ready to run this code.

184
00:11:08,690 --> 00:11:14,390
But just before we do it, I just want to show you the way to build an x gibi regressive model, you

185
00:11:14,390 --> 00:11:16,910
know, an extra boost model for regression.

186
00:11:17,030 --> 00:11:18,410
It's actually super simple.

187
00:11:18,680 --> 00:11:24,620
The only thing you need to change here is just, of course, name of the class, which wouldn't be extra

188
00:11:24,620 --> 00:11:27,190
B classifier, but X, g, b.

189
00:11:27,590 --> 00:11:30,220
And as you will see, x, g, b, regress.

190
00:11:30,950 --> 00:11:33,900
And then, you know, you would just replace classifier here by regress.

191
00:11:34,460 --> 00:11:35,210
And then that's it.

192
00:11:35,450 --> 00:11:39,170
This way you will build a regression model based on extra boost.

193
00:11:39,470 --> 00:11:39,710
All right.

194
00:11:39,740 --> 00:11:43,280
But let's go back to our X to be classifier class.

195
00:11:43,520 --> 00:11:44,300
And there we go.

196
00:11:44,570 --> 00:11:51,050
Now we can just save this implementation and do a run, all to find out if XY boost is going to steal

197
00:11:51,050 --> 00:11:55,930
the throne of the decision tree classification model for this particular dataset.

198
00:11:56,360 --> 00:11:57,380
So quick reminder.

199
00:11:57,470 --> 00:12:02,660
With the decision tree classification, well, we got the best accuracy and the highest one of ninety

200
00:12:02,660 --> 00:12:04,430
five point nine percent.

201
00:12:04,820 --> 00:12:11,720
And now let's find out if we can beat this with the extra boost moral trained on the exact same data

202
00:12:11,750 --> 00:12:12,200
set.

203
00:12:12,770 --> 00:12:13,130
All right.

204
00:12:13,130 --> 00:12:14,660
So basically, we're ready now.

205
00:12:14,780 --> 00:12:18,980
We're just gonna do a run all by clicking runtime here and now.

206
00:12:19,070 --> 00:12:19,790
Are you ready?

207
00:12:20,150 --> 00:12:21,020
I bet you are.

208
00:12:21,170 --> 00:12:21,980
So let's do this.

209
00:12:22,190 --> 00:12:24,920
Three, two, one, go.

210
00:12:25,400 --> 00:12:25,790
All rise.

211
00:12:25,850 --> 00:12:34,160
All the cells are running now and we get an impressive accuracy of ninety seven point eight percent.

212
00:12:34,550 --> 00:12:37,820
When I was telling you that we're gonna end this journey on a good note.

213
00:12:37,970 --> 00:12:42,450
Well, I was choosing my words very, very carefully indeed.

214
00:12:42,560 --> 00:12:44,310
That's just an amazing accuracy.

215
00:12:44,340 --> 00:12:49,450
You know, there are only three incorrect predictions on such a sensitive problem, you know, cancer

216
00:12:49,460 --> 00:12:50,090
prediction.

217
00:12:50,210 --> 00:12:52,190
Well, this result is just amazing here.

218
00:12:52,190 --> 00:12:57,350
Indeed, we almost get 98 percent accuracy with these only three incorrect predictions.

219
00:12:57,740 --> 00:12:58,760
That's just amazing.

220
00:12:59,150 --> 00:13:04,730
But now we have to check one less thing because, you know, maybe we got lucky on this single test

221
00:13:04,730 --> 00:13:05,090
set.

222
00:13:05,150 --> 00:13:11,090
Maybe that single test was more favorable to actually boost than the other classification models, which

223
00:13:11,090 --> 00:13:13,370
could explain why X equals was number one.

224
00:13:13,790 --> 00:13:19,160
And the only way to check this is by actually computing other accuracy's on other test set.

225
00:13:19,460 --> 00:13:24,800
And this is exactly what K4 cross-validation is about, and that is why this is the last cell of this

226
00:13:24,800 --> 00:13:25,640
implementation.

227
00:13:25,910 --> 00:13:33,560
And we also have the results for this, which is, as we can see, still an amazing accuracy of ninety

228
00:13:33,560 --> 00:13:35,210
six point fifty three percent.

229
00:13:35,660 --> 00:13:40,700
This is, of course, an average accuracy obtained as a result of the average of 10.

230
00:13:40,700 --> 00:13:43,430
Different accuracy is measured on 10 different test sets.

231
00:13:43,790 --> 00:13:49,370
And besides, we have a rather small standard deviation of only two percent, which is good.

232
00:13:49,550 --> 00:13:53,090
Once again, for this sensitive problem of cancer prediction.

233
00:13:53,600 --> 00:13:56,900
So, yes, extra boost is definitely number one here.

234
00:13:57,020 --> 00:14:02,300
And that's why, my friends, I'm just super happy that we end on this good note with this final powerful

235
00:14:02,300 --> 00:14:07,580
tool that you get in your machinery and took it because now you can start your post machine journey,

236
00:14:07,700 --> 00:14:10,340
you know, for your career in full confidence.

237
00:14:10,640 --> 00:14:11,690
And about that.

238
00:14:11,710 --> 00:14:14,360
That will be my final words to you in this course.

239
00:14:14,630 --> 00:14:18,850
I wish you sons of great success in your future machinery project.

240
00:14:19,370 --> 00:14:19,730
I wish.

241
00:14:19,890 --> 00:14:25,890
You are the talented data scientist who brings the strongest insights and the highest value analysis

242
00:14:26,100 --> 00:14:27,960
to your team and to your clients.

243
00:14:28,440 --> 00:14:30,180
Now you're totally able to do this.

244
00:14:30,420 --> 00:14:36,300
Thanks to your complete and powerful machinery, two kids with D you, you're totally able to smash

245
00:14:36,420 --> 00:14:38,340
your future measuring problems.

246
00:14:38,760 --> 00:14:40,620
So once again, I wish you the best.

247
00:14:40,800 --> 00:14:45,390
And I look forward to seeing you in another course for a new data science journey.

248
00:14:45,720 --> 00:14:46,890
And until then, of course.

249
00:14:47,070 --> 00:14:48,540
Enjoy machine learning.
