1
00:00:00,390 --> 00:00:03,060
Hello and welcome back to the course on deep learning.

2
00:00:03,120 --> 00:00:06,010
So we've learned quite a lot in this section of the course.

3
00:00:06,030 --> 00:00:08,390
Let's summarize what we've talked about.

4
00:00:08,580 --> 00:00:09,920
All right so here we go.

5
00:00:10,110 --> 00:00:16,230
We started with an input image to which we applied multiple different feature detectors or also called

6
00:00:16,230 --> 00:00:19,100
filters to create these feature maps.

7
00:00:19,140 --> 00:00:21,530
And this comprises our convolutional lair.

8
00:00:21,630 --> 00:00:28,910
Then on top of that crucial Lehre we applied the reglue or rectified linear unit to remove any clarity

9
00:00:28,980 --> 00:00:32,050
or increase non-linearity in our images.

10
00:00:32,060 --> 00:00:36,970
Then we applied a pooling lair to our convolutional lair.

11
00:00:36,990 --> 00:00:44,910
So from every single feature map we created a puled feature map and basically the pulling Lehre has

12
00:00:44,910 --> 00:00:45,840
lots of advantages.

13
00:00:45,840 --> 00:00:54,150
The main purpose of the pulling Lair is to make sure that we have a special spatial invariants in our

14
00:00:54,330 --> 00:00:54,690
images.

15
00:00:54,690 --> 00:01:01,890
So basically if something tilts or twists or is a bit different to the ideal scenario then we can still

16
00:01:01,890 --> 00:01:07,210
pick up that feature plus pulling significantly reduces the size of our images.

17
00:01:07,260 --> 00:01:15,360
Also pooling helps with avoiding any kind of overfitting of our data or overall model to the data because

18
00:01:15,360 --> 00:01:18,220
it just simply gets rid of a lot of that data.

19
00:01:18,450 --> 00:01:24,300
But at the same time pooling preserves the main features that we're after just because the way instruction

20
00:01:24,330 --> 00:01:26,720
and the pooling were used was Max pooling.

21
00:01:26,970 --> 00:01:35,760
Then we flattened all of the pooled images into one long vector or column of all of these values and

22
00:01:35,760 --> 00:01:40,140
we put that into an artificial neural network and that was step flattening.

23
00:01:40,140 --> 00:01:46,770
And Step four is a fully connected artificial neural network where all of these features are processed

24
00:01:46,920 --> 00:01:53,700
through a network and then we have this final Lehre final fully connected layer which performs the voting

25
00:01:53,910 --> 00:02:00,630
towards the classes that we're after and then all of this is trained through a forward propagation and

26
00:02:00,720 --> 00:02:02,550
back propagation process.

27
00:02:02,580 --> 00:02:09,730
Lots of lots of iterations and at parks and in the end we have a very well defined neural network.

28
00:02:09,920 --> 00:02:10,470
And.

29
00:02:10,730 --> 00:02:14,850
Another important thing is not only the weights are trained in artificial neurons work part but also

30
00:02:15,180 --> 00:02:22,590
the feature detectors are trained and adjusted in that same ingredient decent process and that allows

31
00:02:22,590 --> 00:02:23,930
us to come up with the best feature maps.

32
00:02:23,940 --> 00:02:31,110
And in the end we get a fully trained convolutional neural network which can recognize images and classify

33
00:02:31,110 --> 00:02:31,700
them.

34
00:02:31,770 --> 00:02:32,360
So there we go.

35
00:02:32,370 --> 00:02:35,480
That's how convolutional neural networks work.

36
00:02:35,730 --> 00:02:42,220
And now you should be totally comfortable with this concept and ready to proceed to the practical applications.

37
00:02:42,330 --> 00:02:51,370
If you'd like to do some additional reading then there's a great blog by L.D. to disband from 2016.

38
00:02:51,450 --> 00:02:53,400
You can see the link up there at the bottom.

39
00:02:53,400 --> 00:02:58,360
So the blog is called The Nine deep learning papers you need to know about understanding CNN's part

40
00:02:58,440 --> 00:02:59,180
three.

41
00:02:59,310 --> 00:03:04,860
And this blog actually gives you a short overview of nine different CNN's that have been created by

42
00:03:04,860 --> 00:03:10,590
people like you and licken and others which you can then go ahead and study further.

43
00:03:10,590 --> 00:03:18,000
So there will be a lot of new things that will be totally new to you and that you will have to get your

44
00:03:18,000 --> 00:03:23,880
head around but just keep this blog in mind are these nine papers in mind and even if you're not ready

45
00:03:23,880 --> 00:03:29,220
to go through them right now maybe after the practical tutorials maybe after you do some additional

46
00:03:29,490 --> 00:03:36,180
training in the space of deep learning slowly you can then reference these works and ideally I think

47
00:03:36,180 --> 00:03:41,360
you will get a lot of value through looking through other people's neural networks and how they structured.

48
00:03:41,550 --> 00:03:46,620
There can be illusional nets and that will help you understand what are the best practices and why people

49
00:03:46,620 --> 00:03:51,870
did certain things in a certain way and that will help you with your architecture of neural networks

50
00:03:51,870 --> 00:03:57,900
because neural networks and convolutional neural networks are not an exception.

51
00:03:58,020 --> 00:04:05,670
They are like an architecture challenge you have to come up with a idea and then structure and then

52
00:04:05,670 --> 00:04:11,780
adjust it and tweak it to get the best possible design and the best possible and optimal performance.

53
00:04:11,790 --> 00:04:12,490
So there we go.

54
00:04:12,510 --> 00:04:13,430
That's us for today.

55
00:04:13,420 --> 00:04:17,720
I hope you enjoyed today's tutorial and this whole section and I look forward to see you next time.

56
00:04:17,730 --> 00:04:19,440
Until then enjoy deep learning.
