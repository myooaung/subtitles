WEBVTT
1
00:00:00.100 --> 00:00:06.720
All right my friends here we are at the final step of this implementation and actually the most exciting

2
00:00:06.720 --> 00:00:13.350
one because this is the step where we're going to visualize on a nice 2D plot the prediction curve in

3
00:00:13.350 --> 00:00:17.520
the prediction regions of the logistic regression model.

4
00:00:17.580 --> 00:00:17.990
All right.

5
00:00:18.000 --> 00:00:25.920
So more specifically what we're about to plot is a two dimensional plot with therefore to axis X and

6
00:00:25.920 --> 00:00:33.330
Y on the x axis you will have the first feature corresponding to age and on the y axis you'll have the

7
00:00:33.330 --> 00:00:39.900
second feature corresponding to the estimated salary and therefore each of the points you know each

8
00:00:39.900 --> 00:00:46.590
of the observation points you will see on the to the plot will correspond to a specific customer it

9
00:00:46.590 --> 00:00:52.230
will either be a customer of the training set you know on the plot of the training set results or a

10
00:00:52.230 --> 00:00:59.820
customer of the test set on the plot of the test results and what is most interesting to see in this

11
00:00:59.820 --> 00:01:07.230
plot will be the prediction regions meaning the regions where our logistic regression model predicts

12
00:01:07.470 --> 00:01:14.760
the class 0 meaning the customers didn't buy the SUV and the other region where our logistic regression

13
00:01:14.760 --> 00:01:19.630
model predicts the Class 1 meaning the customer bought the SUV.

14
00:01:19.920 --> 00:01:27.030
And lastly what will be really interesting to see is the curve separating these two regions.

15
00:01:27.090 --> 00:01:33.300
You know the region of the prediction 0 and the region of the predictions 1 and this is exactly how

16
00:01:33.420 --> 00:01:39.480
we're going to see the difference between Linear classifiers and nonlinear classifier.

17
00:01:39.490 --> 00:01:43.740
So here we are only starting with one classification model logistic regression.

18
00:01:43.800 --> 00:01:50.370
So we want to compare that yet but you will see in the next sections of this part that the prediction

19
00:01:50.370 --> 00:01:55.740
boundary you know we call this the prediction boundary between these two prediction regions will be

20
00:01:55.740 --> 00:02:00.180
different depending on whether or not your classifier is linear.

21
00:02:00.440 --> 00:02:00.740
All right.

22
00:02:00.740 --> 00:02:07.620
So I can't wait to do this and let us start first by visualizing these training set and test results

23
00:02:07.860 --> 00:02:10.120
for the logistic regression model.

24
00:02:10.290 --> 00:02:10.790
All right.

25
00:02:10.860 --> 00:02:17.490
So the code to visualize this is actually pretty advanced and not only it is pretty advanced but also

26
00:02:17.760 --> 00:02:23.610
you will probably never use it again in your career or let's say you will never have to implement that

27
00:02:23.610 --> 00:02:24.300
again.

28
00:02:24.300 --> 00:02:25.050
Why is that.

29
00:02:25.050 --> 00:02:30.860
Because in your career you will mostly work with data sets having many features you know more than two.

30
00:02:31.320 --> 00:02:37.560
And here the only reason why we have a dataset of two features is so that we can be able to visualize

31
00:02:37.620 --> 00:02:43.050
indeed well these prediction regions and prediction boundary because indeed in order to visualize this

32
00:02:43.350 --> 00:02:50.030
we need maximum two features because one feature corresponds to one dimension in this plot.

33
00:02:50.100 --> 00:02:58.740
So there's a code to visualize this result is only useful for training purposes and it will not be useful

34
00:02:58.950 --> 00:03:00.960
for your future machinery and career.

35
00:03:00.960 --> 00:03:07.950
So what I suggest is that we don't waste too much time understanding the whole code and reemployment

36
00:03:07.950 --> 00:03:11.450
it ourselves because really I'm gonna show it to you right away.

37
00:03:11.460 --> 00:03:17.590
You know on the original logistic regression implementation you will see that the code is pretty advanced.

38
00:03:17.580 --> 00:03:22.190
You know it's not like plotting a regression curve like we did in part 2.

39
00:03:22.620 --> 00:03:26.500
So that's the test results and we show you the trends that result.

40
00:03:26.510 --> 00:03:26.880
All right.

41
00:03:26.880 --> 00:03:28.590
So that's the code you see.

42
00:03:28.740 --> 00:03:34.890
It uses a lot of tricks to plot all these observation points prediction regions and prediction boundary.

43
00:03:34.890 --> 00:03:40.290
So if you want to have a look at it and understand it fine but really for the others it's totally okay.

44
00:03:40.290 --> 00:03:46.230
If we don't cover this code in detail because this is only for training purposes just so that I can

45
00:03:46.230 --> 00:03:51.030
show you the differences between linear classifiers and nonlinear classifiers and you will probably

46
00:03:51.030 --> 00:03:54.380
never use that again in your future machine learning project.

47
00:03:54.390 --> 00:03:57.930
However what I will do just now is explain how it's done.

48
00:03:57.930 --> 00:04:03.750
Basically what we do is we create as you can see a grid which is basically this frame here containing

49
00:04:03.810 --> 00:04:09.840
all the ages of your future and all the estimated salaries you know the ranges and you create this grid

50
00:04:09.840 --> 00:04:16.680
with a high density meaning that the pixels of this grid are not separated one by one but every 0 point

51
00:04:16.680 --> 00:04:17.310
twenty five.

52
00:04:17.340 --> 00:04:22.770
So here for example for the age it goes this way it goes from ten to ten point twenty five to ten point

53
00:04:22.770 --> 00:04:29.280
five to ten point seventy five to eleven etc. up to sixty nine sixty nine point twenty five six nine

54
00:04:29.280 --> 00:04:32.280
point five sixty nine point seventy five seventy okay.

55
00:04:32.370 --> 00:04:38.160
And same forward estimate its salary it goes from twenty thousand and twenty thousand point twenty five

56
00:04:38.160 --> 00:04:44.280
twenty thousand point five etc up to somewhere around one hundred forty nine thousand one hundred forty

57
00:04:44.280 --> 00:04:51.000
nine thousand point twenty five you see so resulting in having super dense points inside this grid and

58
00:04:51.000 --> 00:04:57.520
then the trick you know what we did is not only we plotted all the real observation points in a grid

59
00:04:57.520 --> 00:05:02.800
so all the points that you see here are the tumors of either your training set and then later on your

60
00:05:02.800 --> 00:05:08.830
test set the green points are of course to customers who bought the SUV you know represented by one

61
00:05:08.830 --> 00:05:10.880
here and the rep points.

62
00:05:10.880 --> 00:05:15.920
Of course the customers who didn't buy the SUV represented by zero here.

63
00:05:15.920 --> 00:05:22.600
Okay so all the points are your observation points your customers and then so the trick in order to

64
00:05:22.600 --> 00:05:29.470
plot the prediction regions and therefore that prediction boundary here separating the two regions is

65
00:05:29.470 --> 00:05:37.210
to apply to predict method onto each of these dense points in the grid so that all the dense points

66
00:05:37.210 --> 00:05:44.500
here you know in this region were actually predicted to be zero meaning all the customers you know other

67
00:05:44.500 --> 00:05:51.850
customers inside this region are predicted not to buy an SUV and all the observation points in this

68
00:05:52.090 --> 00:05:59.320
green region are actually predicted to by the SUV because the region here is green representing the

69
00:05:59.470 --> 00:06:02.440
predictions of one and all the red points here.

70
00:06:02.440 --> 00:06:03.850
You know this red region.

71
00:06:03.850 --> 00:06:09.490
Our old observation points predicted a zero meaning all the customers who would predict and not to buy

72
00:06:09.700 --> 00:06:10.840
the SUV.

73
00:06:10.840 --> 00:06:11.800
So you see how this works.

74
00:06:11.800 --> 00:06:12.480
That's the trick.

75
00:06:12.490 --> 00:06:18.280
And then really you don't have to understand all the techniques used to implement this because once

76
00:06:18.280 --> 00:06:22.750
again you will probably never have to implement that kind of code in your career.

77
00:06:22.750 --> 00:06:23.470
All right.

78
00:06:23.470 --> 00:06:29.460
So what we're gonna do now is we're going to get that whole code you know from the original file.

79
00:06:29.570 --> 00:06:34.840
We're gonna paste it inside our new implementation.

80
00:06:34.840 --> 00:06:35.680
There we go.

81
00:06:35.740 --> 00:06:43.310
And we're going to do the same for the test set and then I will actually try not to show you the test

82
00:06:43.340 --> 00:06:52.220
now but let's get the code let's base that below in a new code sale and now let's enjoy the results.

83
00:06:52.400 --> 00:06:56.420
So what we're gonna do is first execute this cell.

84
00:06:56.420 --> 00:07:02.090
It's gonna take a little well actually because you know the step is open 25 meaning we will get a very

85
00:07:02.090 --> 00:07:03.570
very dense crit.

86
00:07:03.650 --> 00:07:09.410
As I just explained and therefore you know that predicts method is applied on each of these dance points

87
00:07:09.820 --> 00:07:10.820
of the grid.

88
00:07:10.820 --> 00:07:15.160
And that's why there are actually a lot lot lot of predictions to compute.

89
00:07:15.170 --> 00:07:17.020
And that's why it's taking a little time.

90
00:07:17.090 --> 00:07:18.200
But there you go it's coming.

91
00:07:18.200 --> 00:07:18.800
Here we go.

92
00:07:18.800 --> 00:07:20.090
So not that long.

93
00:07:20.090 --> 00:07:20.910
That's good.

94
00:07:20.930 --> 00:07:23.510
We just got the results of the training set.

95
00:07:23.510 --> 00:07:29.570
Let's also plot now the results of the test set and then I will make my comments on the results.

96
00:07:29.900 --> 00:07:30.070
All right.

97
00:07:30.080 --> 00:07:38.050
So we can just let it run and start by observing and interpreting the results of the training set.

98
00:07:38.060 --> 00:07:38.270
All right.

99
00:07:38.300 --> 00:07:43.520
So just to recap you have to understand four things in this plot is that all the points that you see

100
00:07:43.520 --> 00:07:50.300
here whether they're red or green are the real customers and they're real results in the training set

101
00:07:50.720 --> 00:07:55.690
the green points correspond of course to the customers who put the produce as you is.

102
00:07:56.030 --> 00:08:02.690
And the red points correspond of course to the customers who didn't buy any produce SUV and then the

103
00:08:02.690 --> 00:08:08.180
other two things to understand is that those colored regions here in this red region and the green region

104
00:08:08.510 --> 00:08:10.360
are the prediction regions.

105
00:08:10.370 --> 00:08:16.520
So this region is the region where our model predicts that the customer didn't buy the SUV and this

106
00:08:16.520 --> 00:08:22.490
region is the region where our logistic regression model predicts that the customers bought the previous

107
00:08:22.490 --> 00:08:22.990
SUV.

108
00:08:23.480 --> 00:08:29.630
And so in order to figure out where the correct predictions are and the incorrect directions are while

109
00:08:29.630 --> 00:08:36.680
the correct predictions are where we have some observation points with the same color as the prediction

110
00:08:36.680 --> 00:08:42.800
region and the incorrect predictions are where we have some observation points with a color that is

111
00:08:42.800 --> 00:08:49.640
different than the prediction regions for example this customer here who in reality what the SUV you

112
00:08:49.640 --> 00:08:55.870
know corresponding to one here is actually an incorrect prediction because it falls into the wrong region

113
00:08:55.910 --> 00:08:57.770
the red region and vice versa.

114
00:08:57.800 --> 00:09:04.310
This customer here who in reality didn't buy the SUV because it corresponds to zero was a wrong prediction

115
00:09:04.340 --> 00:09:10.760
because it falls into the green region where customers are predicted to buy the SUV.

116
00:09:10.760 --> 00:09:17.150
And then finally what's the most interesting in all this is the prediction boundary as you understood

117
00:09:17.390 --> 00:09:23.000
the prediction boundary is the boundary between those two prediction regions.

118
00:09:23.000 --> 00:09:29.600
You know the green prediction region and the red prediction region it is where your classifier separate

119
00:09:29.900 --> 00:09:33.970
basically the two classes the class 1 and the class 0.

120
00:09:34.130 --> 00:09:37.650
And now you have to understand something very very important.

121
00:09:37.730 --> 00:09:43.610
It is the fact you know it is the observation that the prediction curve of the logistic regression model

122
00:09:43.730 --> 00:09:47.430
is actually a straight line for one specific reason.

123
00:09:47.600 --> 00:09:54.640
It is because the logistic regression model is a linear classifier for any linear classifier.

124
00:09:54.770 --> 00:10:01.190
The prediction boundary or the prediction curve will always be a straight line in two dimensions in

125
00:10:01.190 --> 00:10:01.910
three dimensions.

126
00:10:01.910 --> 00:10:03.520
It will be a straight plan.

127
00:10:03.560 --> 00:10:04.130
Okay.

128
00:10:04.220 --> 00:10:07.090
So that's what we get for Linear classifiers.

129
00:10:07.220 --> 00:10:13.070
And what will be really interesting to see in the next sections you know when building other classification

130
00:10:13.070 --> 00:10:20.960
models is that indeed for the non linear morals like for example named baz OR kernel as VM with a nonlinear

131
00:10:20.990 --> 00:10:28.010
kernel well you will see that the prediction curve or the prediction boundary will actually not be a

132
00:10:28.010 --> 00:10:34.240
straight line and I won't reveal now where it will be but we will see it will be fantastic to observe.

133
00:10:34.280 --> 00:10:34.640
All right.

134
00:10:34.640 --> 00:10:42.080
So now you understand it better but remember that these are the training set results therefore all these

135
00:10:42.080 --> 00:10:45.960
customers that we see here are actually in the training set.

136
00:10:45.980 --> 00:10:51.910
Therefore these are customers with which are logistic regression or was trained and therefore that's

137
00:10:51.920 --> 00:10:56.890
kind of easy to provide search results because these are exactly the observations of the training.

138
00:10:56.930 --> 00:11:04.760
But now what we would like to see is how our logistic regression model was able to perform on new observations

139
00:11:04.910 --> 00:11:10.190
meaning on the observations of the test that the customers of the test set because indeed the customers

140
00:11:10.190 --> 00:11:16.420
of the test set are new customers with which our logistic regression model wasn't trained.

141
00:11:16.760 --> 00:11:24.380
And so we have to see if our logistic regression model was still able to separate Well the two classes

142
00:11:24.410 --> 00:11:30.110
meaning the customers who bought the SUV and the customers who didn't buy the SUV even despite the fact

143
00:11:30.110 --> 00:11:33.710
that these are new customers on which the model wasn't trained.

144
00:11:33.800 --> 00:11:40.180
And that's exactly what we're about to see now when visualizing the test results we already executed

145
00:11:40.180 --> 00:11:41.250
the sale here.

146
00:11:41.380 --> 00:11:43.570
And so these are the test results.

147
00:11:43.630 --> 00:11:50.440
And still our logistic regression war was perfectly able to separate Well the two classes zero.

148
00:11:50.450 --> 00:11:53.890
You know all those red points here and one all those green points.

149
00:11:54.120 --> 00:11:59.680
There are still some incorrect predictions of course like this customer who in reality didn't buy the

150
00:11:59.680 --> 00:12:06.040
new the brand new beautiful SUV but was created to buy it and a few incorrect predictions here of the

151
00:12:06.100 --> 00:12:11.980
other class meaning these customers who in reality but the SUV but were predicted not to because they

152
00:12:11.980 --> 00:12:13.940
fall in the red region.

153
00:12:14.470 --> 00:12:15.140
All right.

154
00:12:15.160 --> 00:12:18.220
And so how can we conclude here what should we conclude.

155
00:12:18.220 --> 00:12:23.620
And what are the takeaways we should get for our future classification models.

156
00:12:23.620 --> 00:12:30.130
Well first the logistic regression model does a very good job at separating our two classes and therefore

157
00:12:30.220 --> 00:12:39.250
at predicting whether the customers but SUV but we actually would hope to build a model that has less

158
00:12:39.250 --> 00:12:40.610
prediction errors.

159
00:12:40.780 --> 00:12:46.330
And how can we build one what would we need to get you know as the prediction curve in order not to

160
00:12:46.360 --> 00:12:49.850
predict incorrectly all these wrong predictions here.

161
00:12:49.900 --> 00:12:51.940
All these customers here.

162
00:12:52.090 --> 00:12:57.190
Well we actually would need a prediction boundary that is something else than a straight line because

163
00:12:57.250 --> 00:13:02.110
even if you try to rotate your prediction line for example to be like that.

164
00:13:02.290 --> 00:13:05.770
Well it will still catch many incorrect predictions.

165
00:13:05.770 --> 00:13:12.430
So what we will need to get you know optimally is some kind of curve some kind of prediction curve that

166
00:13:12.430 --> 00:13:13.340
goes this way.

167
00:13:13.360 --> 00:13:19.720
Catches all the red points and all the red customers here and then go around like this in order to catch

168
00:13:19.810 --> 00:13:25.810
all the red points the red customers and leave all the green points to green customers inside the green

169
00:13:25.810 --> 00:13:32.990
region and well as you might guess this is what we might be able to get with nonlinear classifiers.

170
00:13:33.010 --> 00:13:39.730
I won't tell you more now but be ready for some even more performance classification models that manage

171
00:13:39.730 --> 00:13:41.320
to separate even better.

172
00:13:41.320 --> 00:13:42.680
These two classes.

173
00:13:42.770 --> 00:13:43.570
All right.

174
00:13:43.570 --> 00:13:45.600
So that was the teaser for the next sections.

175
00:13:45.640 --> 00:13:50.160
And now I hope you enjoyed this implementation of the logistic regression model.

176
00:13:50.170 --> 00:13:55.840
Congratulations because there was a long one however after this big amount of effort will actually collect

177
00:13:55.840 --> 00:13:58.220
some nice fruit because guess what.

178
00:13:58.270 --> 00:14:05.650
The implementation we did here for the logistic regression model will work for all our other classification

179
00:14:05.650 --> 00:14:06.400
models.

180
00:14:06.400 --> 00:14:08.220
And by that what do I mean.

181
00:14:08.230 --> 00:14:14.790
Well I mean that the implementation we did here is actually a very good code template.

182
00:14:14.800 --> 00:14:21.220
It is a code template for classification because indeed in order to implement the other classification

183
00:14:21.220 --> 00:14:28.960
models we will only have to change one single cell of this implementation which is of course the cell

184
00:14:29.110 --> 00:14:33.670
where we build and train the classification model.

185
00:14:33.670 --> 00:14:40.480
We will only have to change this cell where we build a classifier and train it on the training set and

186
00:14:40.600 --> 00:14:42.160
all the rest will be the same.

187
00:14:42.160 --> 00:14:48.130
We won't have to change anything here because we will just call our classifier to predict this single

188
00:14:48.130 --> 00:14:50.880
result and then all the results in a test set.

189
00:14:51.070 --> 00:14:56.790
And same for the visualization we won't have to change anything except the name of the model here.

190
00:14:56.890 --> 00:14:59.640
Because indeed all the variable names will be the same.

191
00:14:59.670 --> 00:15:04.600
You know the classifier which here is the logistic regression mode but in the next sections will be

192
00:15:04.600 --> 00:15:06.520
the other classification models.

193
00:15:06.610 --> 00:15:10.870
And then of course we have the same variable names for the training set and test set.

194
00:15:10.930 --> 00:15:16.660
And so that's why we won't have anything to change in our future implementations of the classification

195
00:15:16.660 --> 00:15:22.950
models except the part where indeed we build and train our classification model.

196
00:15:23.080 --> 00:15:28.720
So we will be extremely efficient in our future implementations of the classification models because

197
00:15:28.720 --> 00:15:35.380
we will only have to recode that cell here and that's it we'll just re implement this you know to implement

198
00:15:35.380 --> 00:15:40.880
the right classification model and then we will do a run all to enjoy.

199
00:15:41.170 --> 00:15:42.960
Adrienne the final results.

200
00:15:42.970 --> 00:15:49.120
You know the new prediction regions and especially the ones for the non-linear model where we're hoping

201
00:15:49.120 --> 00:15:54.800
to get a regression curve that goes like this in order to catch the maximum right predictions.

202
00:15:54.820 --> 00:15:55.550
All right.

203
00:15:55.660 --> 00:15:56.680
So there we go.

204
00:15:56.680 --> 00:16:01.930
That was the big part of the job you did it and now follow me in the next sections to implement the

205
00:16:01.930 --> 00:16:03.710
other classification models.

206
00:16:03.730 --> 00:16:05.680
And until then enjoy machine learning.
