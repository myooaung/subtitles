WEBVTT
1
00:00:00.120 --> 00:00:07.140
HELLO MY FRIENDS AND WELCOME TO THE FINAL practical activity of this Part 3 on a classification.

2
00:00:07.260 --> 00:00:14.490
We're about to find out if we can beat the record accuracy we've got so far which is 93 percent in which

3
00:00:14.490 --> 00:00:21.300
was obtained with both the K nearest neighbor and the colonel SVM and we're about to find out if we

4
00:00:21.300 --> 00:00:24.880
can beat that with random forest regression.

5
00:00:25.050 --> 00:00:26.670
So I can't wait to see.

6
00:00:26.670 --> 00:00:29.580
I actually am not sure whether we're going to beat it.

7
00:00:29.610 --> 00:00:30.540
So there we go.

8
00:00:30.540 --> 00:00:32.900
Let us discover this together.

9
00:00:32.940 --> 00:00:35.610
So first let us make sure everyone here is on the same page.

10
00:00:35.610 --> 00:00:40.140
This is the folder containing all the codes and data sets and of which I give you the link in the previous

11
00:00:40.140 --> 00:00:41.640
tutorial in the article.

12
00:00:41.670 --> 00:00:48.210
So make sure to connect to that link and now we're all gonna go into Part 3 classification to implement

13
00:00:48.240 --> 00:00:54.330
the final classification model of this Part 3 The Random Forest classification model.

14
00:00:54.330 --> 00:00:54.810
All right.

15
00:00:54.930 --> 00:00:56.580
And we're gonna start with Python Of course.

16
00:00:56.790 --> 00:01:01.950
And inside this fall they will get the same two files for as the implementation of this classification

17
00:01:01.950 --> 00:01:10.160
moral and the social network at dataset which contains 400 observations corresponding to 400 customers.

18
00:01:10.230 --> 00:01:16.470
Each row is a customer and for each of them we get the two features age an estimated salary with which

19
00:01:16.680 --> 00:01:22.320
we're going to predict this dependent variable per chase which tells if yes or no.

20
00:01:22.320 --> 00:01:28.650
Each customer bought an SUV from this car dealership and then once we train this model to understand

21
00:01:28.650 --> 00:01:33.420
the correlations between these two features and the dependent variable vector we will be able to predict

22
00:01:33.450 --> 00:01:39.930
which new customers will buy that brand new SUV just released by this car company and therefore will

23
00:01:39.930 --> 00:01:46.100
be able to target the best way our customers through beautiful ads on social networks.

24
00:01:46.110 --> 00:01:48.080
All right so let's do this.

25
00:01:48.090 --> 00:01:55.020
Let's start the implementation random forest classification and let's open this with either Google collaboratively

26
00:01:55.200 --> 00:01:56.960
or duper notebook.

27
00:01:57.030 --> 00:01:59.750
Wherever is your favorite.

28
00:01:59.760 --> 00:02:03.500
All right so right now it is opening the notebook loading it laying it out.

29
00:02:03.510 --> 00:02:10.710
And here is the random forest classification implementation which results once again from that classification

30
00:02:10.710 --> 00:02:14.160
template we made in the first section on logistic regression.

31
00:02:14.520 --> 00:02:20.100
So all these sales here are exactly the same as in logistic regression you know with the same variable

32
00:02:20.100 --> 00:02:28.080
names and everything except this cell where we build and train the XXX classification model here the

33
00:02:28.080 --> 00:02:30.840
random forest classification model.

34
00:02:30.840 --> 00:02:36.810
So we're going to re implement that cell and since this is an read only mode so that you can all access

35
00:02:36.810 --> 00:02:42.660
it well we're going to create a copy of this file by clicking here and save a copy in drive.

36
00:02:42.660 --> 00:02:45.420
This creates a copy and there we go.

37
00:02:45.420 --> 00:02:51.540
We will be able to re implement that cell to train our random forest classification model on the training

38
00:02:51.540 --> 00:02:52.220
set.

39
00:02:52.230 --> 00:02:52.620
All right.

40
00:02:52.980 --> 00:02:59.820
So first let's remove that cell and now now is the time where you're gonna press pause on the video

41
00:03:00.110 --> 00:03:05.670
to of course implement this yourself and also to learn how to be independent in machine learning and

42
00:03:05.670 --> 00:03:12.870
learn how to get familiar with that psychic learn API which is the way you're going to find the information

43
00:03:12.870 --> 00:03:17.040
you need right now to build this random forest classification model.

44
00:03:17.070 --> 00:03:17.510
All right.

45
00:03:17.610 --> 00:03:19.070
So let's do this together.

46
00:03:19.110 --> 00:03:27.640
Let's go to the API and let's find that class that we need to build a random forest classification model.

47
00:03:27.670 --> 00:03:27.990
All right.

48
00:03:27.990 --> 00:03:34.560
So here as opposed to before we won't find the module we need easily you know by scrolling down for

49
00:03:34.560 --> 00:03:39.750
example down to random forest because know the name of the module is not random forest as it was the

50
00:03:39.750 --> 00:03:42.150
case with the previous classification models.

51
00:03:42.210 --> 00:03:47.470
This time it's actually right here it is and symbol method in the name of the module is exactly.

52
00:03:47.610 --> 00:03:52.920
And symbol so that's where you had to find but you know if you looked for it by scrolling down that's

53
00:03:52.920 --> 00:03:57.620
fine because really I want you to get familiar with the cyclone API.

54
00:03:57.620 --> 00:04:02.760
And so now the question is among all these and symbol method where is the one we want.

55
00:04:02.790 --> 00:04:06.250
Well that's of course this one random forest classifier.

56
00:04:06.300 --> 00:04:07.220
Hard to miss right.

57
00:04:07.380 --> 00:04:10.770
So we're going to click this link and there we go.

58
00:04:10.800 --> 00:04:15.270
This is the random forest class bio class with all the parameters so check them out.

59
00:04:15.300 --> 00:04:17.060
We won't enter all of them.

60
00:04:17.190 --> 00:04:22.920
But let me tell you right now the ones who will enter the first and most important one is the first

61
00:04:22.920 --> 00:04:28.470
one actually an estimated which is of course the number of trees you want to have in your random forest

62
00:04:28.470 --> 00:04:29.250
classifier.

63
00:04:29.310 --> 00:04:29.640
Right.

64
00:04:29.650 --> 00:04:31.770
Number of trees in the forest.

65
00:04:31.770 --> 00:04:38.640
Then once again will choose another value of the criterion that in order to be aligned with what you

66
00:04:38.640 --> 00:04:44.490
learned in the theory you know with rules intuition lectures he taught you about the random forest classification

67
00:04:44.490 --> 00:04:47.070
model with the entropy criterion.

68
00:04:47.070 --> 00:04:50.230
So we're going to select this and that's it.

69
00:04:50.280 --> 00:04:54.530
No more parameters you know for the other parameters here we'll just keep the default values.

70
00:04:54.630 --> 00:05:00.280
However we will just add a random state parameter and set its value to zero just so that we can have

71
00:05:00.370 --> 00:05:02.860
the same results displayed on our notebook.

72
00:05:02.860 --> 00:05:03.610
All right.

73
00:05:03.610 --> 00:05:05.290
So first let's copy this.

74
00:05:05.290 --> 00:05:13.000
You know the name of the class in the module right on copying is going back to our implementation creating

75
00:05:13.060 --> 00:05:14.690
a new code cell here.

76
00:05:14.920 --> 00:05:18.660
Pasting that and then remember we have to start from.

77
00:05:18.880 --> 00:05:25.300
So from the cycle learn library then from the assemble module the secondary library and then remember

78
00:05:25.660 --> 00:05:27.930
we need to adhere import.

79
00:05:28.240 --> 00:05:34.750
Well that random forest classifier which will allow us to build this random forest classification model.

80
00:05:35.020 --> 00:05:38.740
And speaking of building it Well that's exactly our next step here.

81
00:05:38.740 --> 00:05:45.160
We're going to build the classifier through this classifier variable which will be nothing else than

82
00:05:45.160 --> 00:05:47.680
the instance of the random forest classifier.

83
00:05:47.680 --> 00:05:52.200
Class therefore nothing else than the random forest classifier model itself.

84
00:05:52.210 --> 00:05:56.510
So here I am copying this and pasting it right here adding some parentheses.

85
00:05:56.710 --> 00:05:57.370
And there we go.

86
00:05:57.370 --> 00:05:59.560
Now let's add are two parameters.

87
00:05:59.560 --> 00:06:02.460
You know the ones of which we're changing the default values.

88
00:06:02.650 --> 00:06:06.500
The first one is an S T maters.

89
00:06:06.580 --> 00:06:08.630
So that's number of trees in the forest.

90
00:06:08.710 --> 00:06:15.430
The default value is actually one hundred but it will be totally fine with 10 estimates you know 10

91
00:06:15.430 --> 00:06:16.660
trees in the forest.

92
00:06:16.660 --> 00:06:17.380
Why is that.

93
00:06:17.380 --> 00:06:19.960
That's because our dataset is actually quite simple.

94
00:06:19.990 --> 00:06:25.070
It only contains two features and only 400 customers you know 400 observations.

95
00:06:25.120 --> 00:06:30.070
So we will definitely be fine with only 10 trees in the forest.

96
00:06:30.070 --> 00:06:30.520
All right.

97
00:06:30.520 --> 00:06:33.330
And feel free to try other numbers if you wish.

98
00:06:33.330 --> 00:06:38.840
All right then as we said we want to get the same criterion as India into which lecturers meaning and

99
00:06:38.840 --> 00:06:40.690
trippy with the information gain.

100
00:06:40.690 --> 00:06:41.590
So there we go.

101
00:06:41.590 --> 00:06:46.720
Let's add criterion equals in quote entropy.

102
00:06:46.840 --> 00:06:47.370
Great.

103
00:06:47.380 --> 00:06:54.800
And then finally that final parameter random on this course date to which we set the value zero.

104
00:06:54.850 --> 00:06:55.760
Perfect.

105
00:06:55.780 --> 00:07:03.100
And now final step you know it by heart classifier then from this classifier we call the fit method

106
00:07:03.370 --> 00:07:10.960
which will train the classifier only built so far onto the training set composed of the two arguments

107
00:07:10.960 --> 00:07:18.010
we have to input here which are X train for the matrix of features of the training set and then why

108
00:07:18.430 --> 00:07:22.900
train for the dependent variable vector of the same training set.

109
00:07:22.930 --> 00:07:24.070
And that's in my friends.

110
00:07:24.070 --> 00:07:30.140
Now we're about to find out if we can beat that record accuracy of ninety three percent.

111
00:07:30.280 --> 00:07:31.990
I actually have a good feeling about this.

112
00:07:31.990 --> 00:07:34.500
We might beat it but let's not talk too fast.

113
00:07:34.500 --> 00:07:36.190
We never know what's going to happen.

114
00:07:36.220 --> 00:07:39.940
So first let's upload the data said by clicking this fully here.

115
00:07:40.060 --> 00:07:42.010
Let's upload it in the notebook.

116
00:07:42.010 --> 00:07:48.100
So right now as usual you know same story the color notebook is connecting to a runtime to enable file

117
00:07:48.100 --> 00:07:52.600
browsing on your machine and we'll get the upload button in a second.

118
00:07:52.600 --> 00:07:53.300
There we go.

119
00:07:53.680 --> 00:07:58.480
So let's click it and let's go where we have our machinery.

120
00:07:58.480 --> 00:07:59.770
It is that folder.

121
00:07:59.770 --> 00:08:00.280
There it is.

122
00:08:00.280 --> 00:08:01.810
Mine is on my machine.

123
00:08:01.810 --> 00:08:07.470
So we're going to go inside and part three classification then Section 20 random forest classification

124
00:08:07.480 --> 00:08:09.840
the last classification model of this part.

125
00:08:09.850 --> 00:08:13.820
Graduations again for making such huge progress with this course.

126
00:08:13.840 --> 00:08:20.650
There we go inside and python and then social network at that CSC let's open it.

127
00:08:20.650 --> 00:08:26.110
And now we're very close to the final result you know to the final discovery of whether we're going

128
00:08:26.110 --> 00:08:27.560
to beat yes or no.

129
00:08:27.610 --> 00:08:30.330
The record accuracy of 93 percent.

130
00:08:30.370 --> 00:08:37.930
There we go let's click runtime here and then let's click Run 0 to build and train again the random

131
00:08:37.930 --> 00:08:38.880
forest classification.

132
00:08:38.880 --> 00:08:42.210
Here we go we have it now and our future prediction.

133
00:08:42.220 --> 00:08:44.850
So let's see let's see let's see what we get first.

134
00:08:45.010 --> 00:08:50.320
That prediction of the protease decision that single customer of age 30 an eighty seven thousand dollars

135
00:08:50.450 --> 00:08:52.390
estimate his salary is correct.

136
00:08:52.390 --> 00:08:52.660
Right.

137
00:08:52.660 --> 00:08:56.260
Because in reality this customer didn't buy the SUV.

138
00:08:56.260 --> 00:09:01.440
And now with the test results let's call back up here and let's see but what we have.

139
00:09:01.590 --> 00:09:05.650
So all this is correct here correct one incorrect prediction here.

140
00:09:05.740 --> 00:09:07.560
Two other incorrect reasons here.

141
00:09:07.590 --> 00:09:09.560
Oh maybe we won't beat it.

142
00:09:09.580 --> 00:09:13.870
You know let's see directly if we build it and well actually no.

143
00:09:13.870 --> 00:09:14.250
Wow.

144
00:09:14.270 --> 00:09:14.560
OK.

145
00:09:14.560 --> 00:09:15.810
I'm very surprised.

146
00:09:15.850 --> 00:09:18.010
I thought we had a chance to beat it.

147
00:09:18.010 --> 00:09:19.590
I hope you're not too disappointed.

148
00:09:19.660 --> 00:09:25.210
But indeed we didn't beat that record accuracy of ninety three percent because indeed with the random

149
00:09:25.210 --> 00:09:27.710
forest we get 91 percent.

150
00:09:27.770 --> 00:09:30.700
Let's try to tune you know this is not our final word.

151
00:09:30.700 --> 00:09:32.290
Let's try to tune a bit.

152
00:09:32.290 --> 00:09:35.230
The number of estimates is maybe we can get a better one.

153
00:09:35.230 --> 00:09:38.130
Let's try for example the default value of one hundred.

154
00:09:38.140 --> 00:09:43.330
But you know I don't think we will even improve that because we might yet anyway overfishing and this

155
00:09:43.330 --> 00:09:47.370
will not help of course for the predictions of new observations in the test set.

156
00:09:47.400 --> 00:09:48.620
But anyway let's try.

157
00:09:48.700 --> 00:09:50.800
Let's run all again.

158
00:09:50.800 --> 00:09:56.080
So this will rebuild and retrain your random forest classification with 100 trees.

159
00:09:56.080 --> 00:09:58.700
This time it takes actually more time to train it.

160
00:09:58.840 --> 00:10:01.510
But we should get it very soon.

161
00:10:01.520 --> 00:10:02.780
That's interesting actually.

162
00:10:05.210 --> 00:10:05.720
All right.

163
00:10:05.730 --> 00:10:06.590
We're about to get.

164
00:10:06.600 --> 00:10:07.740
We're about to get a new one.

165
00:10:07.740 --> 00:10:08.540
There we go.

166
00:10:08.580 --> 00:10:12.540
Now we have indeed 100 trees in the random forest.

167
00:10:12.570 --> 00:10:12.970
All right.

168
00:10:12.960 --> 00:10:15.650
And you is operation is still correct as a result.

169
00:10:15.660 --> 00:10:16.120
OK.

170
00:10:16.140 --> 00:10:18.060
And now let's see the computer matrix.

171
00:10:18.090 --> 00:10:20.490
That's where I was telling you still 91 percent.

172
00:10:20.510 --> 00:10:23.370
So it was perhaps better trained on the training set.

173
00:10:23.370 --> 00:10:26.250
But what we get on a test is just the same.

174
00:10:26.250 --> 00:10:32.130
So anyway you know clearly the best model for our days that here you know for classification is Colonel

175
00:10:32.180 --> 00:10:34.970
as GM and Kay nearest neighbours.

176
00:10:34.980 --> 00:10:37.040
So I'm going to put that back to 10.

177
00:10:37.210 --> 00:10:37.890
Right.

178
00:10:38.010 --> 00:10:41.530
Press save run everything again and I'm going to show you.

179
00:10:41.720 --> 00:10:46.250
You know the final visualization results for random for us because it's always good to see it.

180
00:10:46.400 --> 00:10:50.070
You know even if we didn't see the accuracy let's observe them.

181
00:10:50.070 --> 00:10:54.360
Let's actually observe them in the original film because it is right now running.

182
00:10:54.730 --> 00:10:55.080
All right.

183
00:10:55.110 --> 00:10:56.730
So we'll find them at the bottom.

184
00:10:56.730 --> 00:11:01.290
But you know it's the same as the decision tree classification moral right.

185
00:11:01.290 --> 00:11:06.960
Since we get all these plots resulting from each of the decision trees algorithms Well we get kind of

186
00:11:06.960 --> 00:11:11.200
the same prediction regions and prediction boundary as the decision tree.

187
00:11:11.220 --> 00:11:11.940
And so there you go.

188
00:11:11.940 --> 00:11:17.010
That's the results of the training set and below you have the results in the test set and indeed we

189
00:11:17.010 --> 00:11:23.280
see that even if it could be very well trained on the training set well we still have some wrong predictions

190
00:11:23.280 --> 00:11:27.050
here of green customers who fall in the wrong red region.

191
00:11:27.060 --> 00:11:32.550
And there is not much we can do you know when choosing the running for its classification to catch correctly

192
00:11:32.700 --> 00:11:34.380
these customers here.

193
00:11:34.440 --> 00:11:39.970
But I want to say something now you know maybe you want to play with the diverse classification models

194
00:11:39.990 --> 00:11:45.090
we implemented and by playing with them I mean playing with the parameters you know trying different

195
00:11:45.150 --> 00:11:46.780
values of the parameters.

196
00:11:46.890 --> 00:11:48.030
And please let me know.

197
00:11:48.050 --> 00:11:54.270
You know in either private message or in the Q and A if you managed to beat 93 percent you know as a

198
00:11:54.270 --> 00:11:57.840
final accuracy on the test set of course that could be a good challenge.

199
00:11:57.900 --> 00:11:58.850
I think it's possible.

200
00:11:58.860 --> 00:12:02.820
I haven't tried it myself because anyway 93 percent is really good.

201
00:12:02.880 --> 00:12:06.920
But let me know if you succeed I'll be very interested to see how you did it.

202
00:12:06.930 --> 00:12:07.440
All right.

203
00:12:07.710 --> 00:12:10.170
So here we are at the end of this section.

204
00:12:10.170 --> 00:12:12.240
Congratulations for completing it.

205
00:12:12.240 --> 00:12:15.530
Now you have some great tools in your classification tool kit.

206
00:12:15.660 --> 00:12:21.420
Please understand that the best models we got here are just for this data set before your future data

207
00:12:21.420 --> 00:12:21.840
set.

208
00:12:21.900 --> 00:12:27.240
The best more might be another one it might be random forest or it might be native based.

209
00:12:27.270 --> 00:12:28.920
So you have to try all of them.

210
00:12:28.920 --> 00:12:33.220
And speaking of which this is exactly what we'll do next in this part 3.

211
00:12:33.230 --> 00:12:34.610
No we're not done yet.

212
00:12:34.620 --> 00:12:37.560
I'm gonna take all these code templates here that we made.

213
00:12:37.560 --> 00:12:42.450
I'm gonna simplify them you know I'm going to remove all the prints and everything so that it can be

214
00:12:42.450 --> 00:12:48.330
very clear and well-structured and mostly so that you can get very efficient code templates that you

215
00:12:48.330 --> 00:12:53.730
can try and deploy very quickly and efficiently on your data set so that you can quickly figure out

216
00:12:53.820 --> 00:12:55.380
what is the best model.

217
00:12:55.530 --> 00:12:58.230
And that's what we'll do in this last section of part 3.

218
00:12:58.260 --> 00:12:59.670
I can't wait to show you the demo.

219
00:12:59.670 --> 00:13:02.810
You know that same demo we made in part a regression.

220
00:13:02.850 --> 00:13:07.660
We're gonna have this demo of deploying all our classification models and in flashlight we're going

221
00:13:07.660 --> 00:13:09.470
to select the best one.

222
00:13:09.540 --> 00:13:12.060
So take a good break now you really deserve it.

223
00:13:12.300 --> 00:13:17.550
And once you're ready with some good energy well please join me in this final section to deploy all

224
00:13:17.550 --> 00:13:24.860
our classification models in maximum efficiency for any dataset regarding the number of features.

225
00:13:24.870 --> 00:13:26.060
Can't wait to meet you there.

226
00:13:26.070 --> 00:13:27.890
And until then enjoy machine learning.
