1
00:00:00,220 --> 00:00:00,910
I other friends.

2
00:00:01,180 --> 00:00:03,840
I ready to build that convolutional neural network.

3
00:00:03,930 --> 00:00:06,960
We have a long but yet very exciting journey ahead of us.

4
00:00:07,110 --> 00:00:07,920
Let's do this.

5
00:00:08,070 --> 00:00:09,060
Let's kick this off.

6
00:00:09,660 --> 00:00:09,900
All right.

7
00:00:09,930 --> 00:00:14,340
So we're going to start with the very essential step, empowering the libraries, which will only consist

8
00:00:14,430 --> 00:00:19,320
of importing sense of flow and actually the preprocessing module by the Keris library.

9
00:00:19,530 --> 00:00:23,160
So let's do this for us quickly and efficiently in a new Kotel.

10
00:00:23,490 --> 00:00:26,150
So actually, you know how to import Tenzer flow.

11
00:00:26,240 --> 00:00:32,010
We start with the import command and we specify the name of the library Tensor Flow.

12
00:00:32,520 --> 00:00:34,380
And we add this shortcut.

13
00:00:34,740 --> 00:00:37,200
T.F. Just like before with the Eynon.

14
00:00:37,770 --> 00:00:43,200
Then I would like to import something else which will allow us to do the image preprocessing in part

15
00:00:43,200 --> 00:00:45,190
one in which is the image.

16
00:00:45,410 --> 00:00:48,600
Module of the preprocessing module of the Keris library.

17
00:00:49,050 --> 00:00:51,460
And therefore here we're going to start from.

18
00:00:51,720 --> 00:00:59,520
Well, the Keris library from which we're gonna get access to the pre prah siccing module from which

19
00:00:59,580 --> 00:01:02,540
we're going to import the image sub submodular.

20
00:01:03,030 --> 00:01:08,610
And the reason why we want to import this is because we want to import a specific class, which is the

21
00:01:08,730 --> 00:01:10,830
image data generator.

22
00:01:11,250 --> 00:01:13,330
And I will explain very quickly what this is about.

23
00:01:13,390 --> 00:01:15,720
But this is absolutely compulsory.

24
00:01:15,990 --> 00:01:20,190
In part one, data processing, you know, when preprocessing your images.

25
00:01:20,600 --> 00:01:21,400
So that's important.

26
00:01:21,600 --> 00:01:27,510
Here we just need to add import and then image data generator.

27
00:01:28,590 --> 00:01:29,560
OK, good.

28
00:01:30,090 --> 00:01:33,360
I will explain very soon what this is about and how we will use it.

29
00:01:33,660 --> 00:01:34,050
All right.

30
00:01:34,410 --> 00:01:39,120
And then, you know, this other thing that I like doing just to show you that indeed we are working

31
00:01:39,120 --> 00:01:40,540
with Tenderfoot 2.0.

32
00:01:40,890 --> 00:01:44,790
I just want to print the version of tensor flow we're using right now.

33
00:01:45,000 --> 00:01:51,180
And remember, to do this, we need to call center flow first and then after a dot, a double underscore

34
00:01:51,240 --> 00:01:53,590
and then version and double on the core again.

35
00:01:53,910 --> 00:01:58,060
And this will, you know, print in the output, the version of tend to flow.

36
00:01:58,140 --> 00:02:01,740
We're using this is just to make sure we're working with 10th floor 2.0.

37
00:02:01,980 --> 00:02:06,150
However, you know, depending on when you're on this code, you know, after a record is tutorial,

38
00:02:06,360 --> 00:02:10,860
you might have a different version, but you will definitely get a sense of flow to version.

39
00:02:11,040 --> 00:02:11,340
OK.

40
00:02:11,900 --> 00:02:12,180
All right.

41
00:02:12,210 --> 00:02:18,090
So here I just executed the first cell in boring tend to flow end image repressing Modulate Kiraz.

42
00:02:18,330 --> 00:02:26,820
And now let's run this to indeed reassure ourselves that we are working with of Flow 2.0, which is

43
00:02:26,820 --> 00:02:28,970
so much better than tons of low one.

44
00:02:29,700 --> 00:02:30,720
All right, good.

45
00:02:30,870 --> 00:02:35,250
So now we can move on to part one data repressing, which will be done in two steps.

46
00:02:35,460 --> 00:02:40,020
First, repricing the training set and second, preprocessing the test set.

47
00:02:40,410 --> 00:02:43,630
So let's start with the training set and let's create a new coattail.

48
00:02:43,950 --> 00:02:47,020
And now let me explain how we're gonna do this.

49
00:02:47,690 --> 00:02:48,030
All right.

50
00:02:48,060 --> 00:02:51,510
So how are we going to pre-process our images?

51
00:02:51,930 --> 00:02:54,600
Well, we're actually going to do multiple things.

52
00:02:54,750 --> 00:03:01,680
The first thing we'll do is we will apply some transformations on all the images of the training set.

53
00:03:01,890 --> 00:03:06,280
The images of the training set only we want apply these same transformations on the test.

54
00:03:06,840 --> 00:03:13,080
The reason why we want to apply some transformations on the images of the training set is for only one

55
00:03:13,080 --> 00:03:13,590
purpose.

56
00:03:13,860 --> 00:03:15,730
It is to avoid overfilling.

57
00:03:16,260 --> 00:03:22,950
Indeed, if we don't apply these transformations well when training our CNN on the training set, we

58
00:03:22,950 --> 00:03:28,790
will get a huge difference between the accuracy on the training set and the one on the test set.

59
00:03:29,010 --> 00:03:34,380
On the evaluation set, actually, we will get very high accuracy on the training set, you know, close

60
00:03:34,380 --> 00:03:38,330
to 98 percent and much lower accuracy on the test.

61
00:03:38,700 --> 00:03:40,250
And that is Scott Overfitting.

62
00:03:40,320 --> 00:03:43,110
And that's something we absolutely need to avoid.

63
00:03:43,460 --> 00:03:48,680
Anyway, you know, whether you're working on a classic data set or working for a computer vision and

64
00:03:48,690 --> 00:03:49,680
for computer vision.

65
00:03:49,770 --> 00:03:55,290
Well, the way to avoid overfilling is, as I said, to apply transformations.

66
00:03:55,800 --> 00:03:56,580
So that was the way.

67
00:03:56,640 --> 00:04:01,020
And now let me explain the what you know, what are these transformations?

68
00:04:01,110 --> 00:04:04,110
And then I will proceed in the end to the how how we're gonna implement that.

69
00:04:04,590 --> 00:04:07,310
So the what what are these transformations is.

70
00:04:07,770 --> 00:04:14,790
Well, some simple geometrical transformations or some zooms or some rotations on your images.

71
00:04:15,060 --> 00:04:21,090
So basically, we're gonna apply some geometrical transformations like transaction's to shift some of

72
00:04:21,090 --> 00:04:21,840
the pixels.

73
00:04:22,290 --> 00:04:23,620
Then we're gonna rotate a bit.

74
00:04:23,640 --> 00:04:26,190
The images, we're gonna do some horizontal flips.

75
00:04:26,460 --> 00:04:28,770
We're gonna do some zoom in and zoom out.

76
00:04:28,980 --> 00:04:34,380
Well, you know, we're going to play a series of transformation so as to modify the images and get

77
00:04:34,380 --> 00:04:36,270
them, as we say, augmented.

78
00:04:36,540 --> 00:04:41,070
In fact, the technical term of what we're gonna do now, you know, with all these transformations

79
00:04:41,400 --> 00:04:47,670
is called image augmentation, which consists basically of transforming your images of the training

80
00:04:47,670 --> 00:04:54,960
set so that your CNN model doesn't overlearned, you know, is not overtrained on the existing images,

81
00:04:55,050 --> 00:04:59,790
because by applying these transformations, we will get new images, which is the reason.

82
00:04:59,870 --> 00:05:02,090
Why we call this image augmentation.

83
00:05:02,120 --> 00:05:04,340
We basically augment the variety.

84
00:05:04,360 --> 00:05:07,040
You know, the diversity of the training set images.

85
00:05:07,640 --> 00:05:08,060
All right.

86
00:05:08,180 --> 00:05:09,180
So that is the what?

87
00:05:09,380 --> 00:05:12,570
And now we're going to proceed to the how to proceed to the how.

88
00:05:12,590 --> 00:05:16,400
I'm gonna take you to the Keris API because you have to see it.

89
00:05:16,700 --> 00:05:18,560
You know, just like what we did with Cycad Learn.

90
00:05:18,590 --> 00:05:24,020
I'm going to show you and guide you through the Curse API to find the exact tool we're gonna use for

91
00:05:24,020 --> 00:05:24,350
this.

92
00:05:24,830 --> 00:05:26,750
So let's open a new tab here.

93
00:05:27,530 --> 00:05:28,100
There we go.

94
00:05:28,160 --> 00:05:34,670
And in the search bar, let's enter just keris crassly that lets press enter and let's just get the

95
00:05:34,670 --> 00:05:35,240
first link.

96
00:05:35,250 --> 00:05:36,330
There is only one cares.

97
00:05:36,680 --> 00:05:41,540
And that's, of course, a deep learning library in Python developed by France specially.

98
00:05:41,570 --> 00:05:44,930
By the way, a very talented French data scientist.

99
00:05:45,410 --> 00:05:45,640
All right.

100
00:05:45,680 --> 00:05:49,840
So let's go now to API docs and now, my friends.

101
00:05:49,970 --> 00:05:52,220
Welcome to the Keris API.

102
00:05:52,280 --> 00:05:55,260
This is probably my favorite deep learning library.

103
00:05:55,280 --> 00:05:56,780
It's absolutely fantastic.

104
00:05:57,200 --> 00:06:02,000
And now where we want to go is, of course, to data preprocessing, which includes, of course, three

105
00:06:02,000 --> 00:06:02,330
things.

106
00:06:02,330 --> 00:06:06,770
Actually, you have to know it image the repricing, which is what we're about to use right now.

107
00:06:07,100 --> 00:06:11,450
But then also time series data pricing and also text data pricing.

108
00:06:11,570 --> 00:06:16,610
You can also do some deep and help, you know, an LP with deploying with carers.

109
00:06:17,150 --> 00:06:20,730
But now, of course, we're looking for something in image data repressing.

110
00:06:21,050 --> 00:06:23,990
And let me show you exactly what that something is.

111
00:06:24,320 --> 00:06:25,310
We just need to scroll down.

112
00:06:25,340 --> 00:06:28,760
Well, actually, you already know what this is, because we already import the class.

113
00:06:29,000 --> 00:06:29,840
But there it is.

114
00:06:30,110 --> 00:06:36,920
I'm talking, of course, about the image data generator class, which will indeed generate batches

115
00:06:36,920 --> 00:06:42,520
of tensor image data with real time data augmentation, which is exactly what I've just explained.

116
00:06:42,860 --> 00:06:45,110
And I haven't mentioned the batches yet.

117
00:06:45,320 --> 00:06:50,180
Well, that's because, you know, we will create different batches of actually 32 images.

118
00:06:50,600 --> 00:06:55,880
And these images will either be the original ones or, you know, the augmented ones, the transformed

119
00:06:55,910 --> 00:06:57,970
ones after we apply the transformations.

120
00:06:58,640 --> 00:07:03,800
And speaking of applying these transformations, well, we're going to do that exactly with this image

121
00:07:03,800 --> 00:07:07,700
data generator, Class four, which will find all the arguments here.

122
00:07:07,730 --> 00:07:11,240
And, you know, most of them correspond to different transformations.

123
00:07:11,570 --> 00:07:16,850
I can already tell you that we will use the zoom range, which consists of zooming in or zooming out

124
00:07:16,910 --> 00:07:17,690
on the images.

125
00:07:18,080 --> 00:07:23,090
But also we'll use the horizontal flip, which consists of flipping the images horizontally.

126
00:07:23,420 --> 00:07:28,150
And then we will also use this one to shear range, which is some kind of transaction.

127
00:07:28,290 --> 00:07:29,130
You can check it online.

128
00:07:29,210 --> 00:07:30,860
But no need to understand this.

129
00:07:30,860 --> 00:07:34,580
And all the details just know that it's a geometrical transformation.

130
00:07:34,700 --> 00:07:38,270
And if you want to go further than it's actually some kind of transaction.

131
00:07:38,570 --> 00:07:39,350
But there we go.

132
00:07:39,560 --> 00:07:45,830
These are the three transformations we'll use the sheer range, the zoom range and the horizontal flip.

133
00:07:46,190 --> 00:07:50,180
And now I'm sure some of you are asking, why do we use these transformations?

134
00:07:50,490 --> 00:07:51,920
Well, I'll be honest with you.

135
00:07:52,160 --> 00:07:58,730
The reason I'm using them is because I simply took, you know, the code snippet example from Keris,

136
00:07:59,060 --> 00:08:01,490
which is right below exactly here.

137
00:08:01,790 --> 00:08:06,560
This is the code snippet example using image data generator class.

138
00:08:06,830 --> 00:08:13,010
And as you can see, we use sharing transformation, a zoom transformation in a horizontal flip transformation.

139
00:08:13,040 --> 00:08:14,390
And we're just gonna do the same.

140
00:08:14,660 --> 00:08:17,330
But, of course, feel free to try some other transformations.

141
00:08:17,570 --> 00:08:20,510
Who knows, maybe you'll get better accuracy Indians.

142
00:08:20,730 --> 00:08:22,370
Okay, but let's just trust this.

143
00:08:22,400 --> 00:08:26,780
And actually, I chose this because, of course, I tried it on our future CNN, which we're about to

144
00:08:26,780 --> 00:08:27,140
build.

145
00:08:27,320 --> 00:08:30,800
And you're going to see that the results in the end will be absolutely amazing.

146
00:08:31,130 --> 00:08:37,520
Okay, so let's just take this let's just take this code snippet to, you know, actually get a tool

147
00:08:37,610 --> 00:08:39,560
that will apply these transformations.

148
00:08:39,920 --> 00:08:42,740
Then, of course, we'll have to connect a tool to our training set.

149
00:08:43,250 --> 00:08:45,320
So back into our implementation.

150
00:08:45,320 --> 00:08:47,600
Well, let's base that right here.

151
00:08:48,110 --> 00:08:54,800
And this, as you can see, it creates an object which we call trend data gen up the image data generator

152
00:08:54,800 --> 00:08:55,190
class.

153
00:08:55,190 --> 00:09:00,950
A trained data gen is an instance of that image data generator class in which represents, of course,

154
00:09:01,280 --> 00:09:06,590
the tool that will apply all the transformations on the images of the training set.

155
00:09:06,950 --> 00:09:08,570
And there is one I haven't mentioned.

156
00:09:08,600 --> 00:09:12,150
You know, I mentioned and explained these three ones, which are the transformations.

157
00:09:12,650 --> 00:09:17,840
But we also notice this one rescale equals one divided by 255.

158
00:09:18,230 --> 00:09:19,910
Can you guess what this is about?

159
00:09:20,250 --> 00:09:23,960
You know, we already saw this many times on our classic data set.

160
00:09:24,230 --> 00:09:26,690
Well, this is, of course, about feature scaling.

161
00:09:27,020 --> 00:09:33,590
This will apply feature scaling to each and every single one of your pixels by dividing their value

162
00:09:33,800 --> 00:09:35,320
by 255.

163
00:09:35,390 --> 00:09:40,600
Because remember that each pixel takes a value between zero and 255.

164
00:09:40,820 --> 00:09:47,480
So by dividing all of them by 255, we indeed get all the pixel values between zero and one, which

165
00:09:47,480 --> 00:09:49,400
is just like a normalization.

166
00:09:49,730 --> 00:09:54,590
And once again, features going is absolutely compulsory for neural networks.

167
00:09:54,620 --> 00:09:56,420
You know, when training neural networks.

168
00:09:56,760 --> 00:09:57,020
All right.

169
00:09:57,020 --> 00:09:59,330
So basically, this is feature scaling and.

170
00:09:59,520 --> 00:10:05,880
These are the transformations that will perform image augmentation on the images of the training set.

171
00:10:05,970 --> 00:10:09,460
And this I remain is in order to prevent overfilling.

172
00:10:09,560 --> 00:10:14,070
In the end, you can try actually, you know, the future training will have without these and you will

173
00:10:14,070 --> 00:10:15,810
see what I mean by overfitting.

174
00:10:16,350 --> 00:10:17,430
All right, good.

175
00:10:17,610 --> 00:10:18,420
So that's not it.

176
00:10:18,630 --> 00:10:19,110
That's not it.

177
00:10:19,150 --> 00:10:20,910
You know, the training set repricing.

178
00:10:21,150 --> 00:10:27,330
We need to, of course, now connect that train data gen object to our training set, you know, to

179
00:10:27,330 --> 00:10:29,130
our training set images so far.

180
00:10:29,220 --> 00:10:30,300
This is just the object.

181
00:10:30,750 --> 00:10:38,070
And so the way we're going to do this is we will, of course, go back to our Keris API, because indeed,

182
00:10:38,070 --> 00:10:46,080
the way to do this is just to take this next code here that will actually import the training set by

183
00:10:46,080 --> 00:10:52,560
accessing it from, you know, our directory and at the same time creating these batches and resizing

184
00:10:52,770 --> 00:10:53,490
the images.

185
00:10:53,520 --> 00:10:59,020
Now, in case we need to resize them in order to reduce the computations of the machine, you know,

186
00:10:59,040 --> 00:11:03,810
to make it less compute intensive, which is what we all do, because we will see that with a lower

187
00:11:03,810 --> 00:11:06,270
size will still get amazing results in the end.

188
00:11:06,750 --> 00:11:07,600
So let's get this.

189
00:11:07,740 --> 00:11:10,620
And once again, I will explain all this code.

190
00:11:10,680 --> 00:11:16,650
And mostly we will have to change it the right way so that we can adapt it indeed to our situation.

191
00:11:17,220 --> 00:11:18,990
So let's take it step by step.

192
00:11:19,200 --> 00:11:25,440
This is actually the name you want to give to your training set, which you are importing in the notebook.

193
00:11:25,770 --> 00:11:27,570
And let's just give the usual names.

194
00:11:27,660 --> 00:11:32,130
We're going to call that training underscore set just like before.

195
00:11:32,610 --> 00:11:35,820
Then we take indeed our trained data agent object.

196
00:11:35,970 --> 00:11:38,610
That instance of the image data generated class.

197
00:11:38,940 --> 00:11:43,710
And from this object, we're going to call a method of this class.

198
00:11:43,860 --> 00:11:44,100
Right.

199
00:11:44,130 --> 00:11:46,500
Because this class is every class contains methods.

200
00:11:46,740 --> 00:11:52,260
And one of them is this flow from directory, which will just simply, you know, connect this image

201
00:11:52,320 --> 00:11:55,770
augmentation tool to the images of your training set.

202
00:11:56,430 --> 00:11:59,430
All right, then let's have a look at the different parameters.

203
00:11:59,580 --> 00:12:05,310
So the first one here is actually the path leading to your training set.

204
00:12:05,730 --> 00:12:10,410
And so, of course, we have to change this because we have a different path to our data set.

205
00:12:10,710 --> 00:12:14,970
So this is a whole folder which I've shared with you at the beginning of this section.

206
00:12:15,270 --> 00:12:17,040
And this is also the root folder.

207
00:12:17,070 --> 00:12:18,380
You know, this is the base of the folder.

208
00:12:18,450 --> 00:12:19,980
You know, the beginning of the path.

209
00:12:20,400 --> 00:12:25,260
And so now, in order to access the training set, well, we first need to specify that we want to go

210
00:12:25,260 --> 00:12:29,250
into this data set folder and then into this training set folder.

211
00:12:29,340 --> 00:12:32,550
And that's exactly the past leading to the training set.

212
00:12:32,940 --> 00:12:38,100
And therefore, here, you know, in this parameter of the flow from directory function, well, we

213
00:12:38,100 --> 00:12:44,310
simply need to replace data here by data set and then train here by training set.

214
00:12:44,790 --> 00:12:44,980
All right.

215
00:12:45,000 --> 00:12:47,850
This is a simple path leading to the train folder.

216
00:12:48,210 --> 00:12:50,850
Starting from the root of our directory folder.

217
00:12:51,540 --> 00:12:51,830
OK.

218
00:12:52,110 --> 00:12:52,470
Good.

219
00:12:52,740 --> 00:12:54,930
Now, next argument, target size.

220
00:12:54,970 --> 00:12:58,170
That's indeed the final size of your images.

221
00:12:58,250 --> 00:13:02,250
One day, you know, will be fed into the convolutional neural network.

222
00:13:02,730 --> 00:13:06,330
And actually, I tried with 150 by 150.

223
00:13:06,570 --> 00:13:09,210
And that actually made the training very, very long.

224
00:13:09,270 --> 00:13:14,520
So I actually wanted to reduce that to, you know, 64 by.

225
00:13:15,540 --> 00:13:16,190
64.

226
00:13:16,320 --> 00:13:17,370
And that Stolley fine.

227
00:13:17,760 --> 00:13:19,560
This will make the training much faster.

228
00:13:19,710 --> 00:13:21,780
And still we will have amazing results.

229
00:13:21,900 --> 00:13:22,760
You'll see that again.

230
00:13:23,460 --> 00:13:28,200
Then the batch sizes, you know, the size of the batches, meaning how many images we want to have

231
00:13:28,200 --> 00:13:28,980
in each batch.

232
00:13:29,270 --> 00:13:31,490
And the 32 is a classic default value.

233
00:13:31,500 --> 00:13:32,390
And we're going to keep that.

234
00:13:32,430 --> 00:13:33,570
It will be totally fine.

235
00:13:33,930 --> 00:13:38,930
And finally, we have to specify the class mode, which is either binary or categorical.

236
00:13:39,360 --> 00:13:43,500
And of course, since now we have a binary outcome, you know, cat or dog.

237
00:13:43,680 --> 00:13:46,770
Well, we have to choose, of course, class mode equals binary.

238
00:13:47,350 --> 00:13:48,500
Okay, perfect.

239
00:13:48,690 --> 00:13:52,110
And that closes the preprocessing of the training set.

240
00:13:52,260 --> 00:13:54,930
We're done with this first step of decompressing.

241
00:13:55,410 --> 00:13:58,110
And so now we're gonna move on to the next step.

242
00:13:58,380 --> 00:13:59,990
Preprocessing detests it.

243
00:14:00,480 --> 00:14:04,140
And of course, in the spirit of always, be as much efficient as we can.

244
00:14:04,380 --> 00:14:09,090
Well, we're going to go back to our Keris API and we're just gonna take this time.

245
00:14:09,450 --> 00:14:17,040
This line of code to get that same image data generator object to, you know, apply to transformation's

246
00:14:17,040 --> 00:14:18,170
to the test images.

247
00:14:18,450 --> 00:14:23,550
But be careful, we're not going to apply the same transformation's here, such as a sheering the zoom

248
00:14:23,550 --> 00:14:28,120
in the horizontal flip, because, of course, we don't want to touch the test images because they're

249
00:14:28,120 --> 00:14:29,250
are like new images.

250
00:14:29,490 --> 00:14:31,500
Like when deploying our model in production.

251
00:14:31,770 --> 00:14:35,460
And therefore, of course, we have to keep them intact like the original ones.

252
00:14:35,820 --> 00:14:41,160
However, what we have to do to them is indeed to rescale their pixels.

253
00:14:41,310 --> 00:14:42,990
And that's the same as before, you know.

254
00:14:43,260 --> 00:14:47,400
Remember when we were applying feature scaling to our training set and test set?

255
00:14:47,700 --> 00:14:53,560
Well, we used the fit transfer method on the training set, but only the trend for method on the test

256
00:14:53,560 --> 00:14:53,820
set.

257
00:14:54,060 --> 00:14:57,690
And that was, of course, to avoid information leakage from the test set.

258
00:14:58,080 --> 00:15:03,480
And while here, that's exactly the same, we have to keep the images of the test intact by not applying

259
00:15:03,480 --> 00:15:04,470
any transformation.

260
00:15:04,740 --> 00:15:10,530
However, we have to feature scale them because once again, the future predict method of CNN will have

261
00:15:10,530 --> 00:15:15,360
to be applied to the same scaling as the one that was applied on the training set.

262
00:15:15,570 --> 00:15:19,920
So you see, this is exactly the same as before, is just that we are using some different classes,

263
00:15:19,950 --> 00:15:22,230
but which are, after all, the same tools.

264
00:15:22,770 --> 00:15:23,010
All right.

265
00:15:23,020 --> 00:15:27,720
So let's get this and let's put that back into our implementation in a new coattail.

266
00:15:28,290 --> 00:15:29,520
So we're going to paste that.

267
00:15:29,730 --> 00:15:31,920
We're going to keep the same name for the object.

268
00:15:31,920 --> 00:15:32,700
That's solely fun.

269
00:15:33,180 --> 00:15:33,720
And then.

270
00:15:33,840 --> 00:15:34,710
Well, same.

271
00:15:34,800 --> 00:15:41,220
We're gonna go back to this and we're going to get exactly this, which will actually import the test,

272
00:15:41,220 --> 00:15:44,010
set images into our notebook.

273
00:15:44,340 --> 00:15:44,630
All right.

274
00:15:44,640 --> 00:15:46,170
So let's face that.

275
00:15:46,470 --> 00:15:48,390
And now let's do the required change.

276
00:15:48,420 --> 00:15:51,990
Actually, please press pause in the video and do the changes yourself.

277
00:15:52,020 --> 00:15:56,160
I'm sure you're going to do this successfully because this is exactly the same as before.

278
00:15:56,580 --> 00:15:57,030
All right.

279
00:15:57,450 --> 00:15:58,860
So now let's do it together.

280
00:15:59,250 --> 00:16:03,990
The first thing I would like to do is just change its name, which is exactly the name of the variable

281
00:16:04,010 --> 00:16:05,340
that will contain the test set.

282
00:16:05,640 --> 00:16:08,250
And just to be consistent with before.

283
00:16:08,400 --> 00:16:09,870
Well, let's just go the test set.

284
00:16:11,100 --> 00:16:11,580
All right.

285
00:16:11,940 --> 00:16:12,650
So test set.

286
00:16:13,050 --> 00:16:13,230
Then.

287
00:16:13,230 --> 00:16:13,810
This is correct.

288
00:16:13,830 --> 00:16:15,060
We call our test data.

289
00:16:15,330 --> 00:16:19,810
Here, which will only apply if it is going to the pixels of the test images.

290
00:16:20,250 --> 00:16:25,560
Then we call that same function flow from directory to access the test set from our directory.

291
00:16:25,920 --> 00:16:32,760
And here once again, we need to replace data here by data set and then validation by, you know, remember.

292
00:16:33,750 --> 00:16:39,420
Now we want to get the path that leads to the test set and therefore that first dataset and then test

293
00:16:39,460 --> 00:16:39,710
set.

294
00:16:39,960 --> 00:16:40,350
All right.

295
00:16:40,620 --> 00:16:45,750
So here we just need to replace validation by test set.

296
00:16:46,230 --> 00:16:46,620
Good.

297
00:16:47,070 --> 00:16:52,350
Then, of course, we need to have the same target size because basically the brick method has to be

298
00:16:52,350 --> 00:16:56,760
called on the exact same format as the one that was used for the images of the training.

299
00:16:56,790 --> 00:17:00,540
So here we need to get the same size as in the training set.

300
00:17:00,630 --> 00:17:04,480
Therefore, 64 by 64.

301
00:17:04,950 --> 00:17:06,570
And the same batch size.

302
00:17:06,690 --> 00:17:10,440
Basically, our will be evaluated on batches of 32 images.

303
00:17:10,800 --> 00:17:12,210
And of course, the same class mode.

304
00:17:12,390 --> 00:17:12,900
Binary.

305
00:17:13,620 --> 00:17:14,010
Good.

306
00:17:14,310 --> 00:17:15,120
Well, there you go.

307
00:17:15,210 --> 00:17:17,400
We're done with data processing.

308
00:17:17,550 --> 00:17:18,540
It was very different.

309
00:17:18,540 --> 00:17:19,690
It was actually brand new.

310
00:17:19,890 --> 00:17:23,940
But we recognize some of the same process steps as what we did before.

311
00:17:24,780 --> 00:17:30,350
And so now I'm very excited because we can move onto the exciting part, which is about building the

312
00:17:30,350 --> 00:17:30,930
CNN.

313
00:17:31,320 --> 00:17:35,040
Yes, we are ready for part two now, which we're going to tackle on several steps.

314
00:17:35,400 --> 00:17:37,620
And so make sure to get good energy for this.

315
00:17:37,770 --> 00:17:41,790
And as soon as there is a case, join me in a next story or to smash design.

316
00:17:41,940 --> 00:17:43,860
Part two building to CNN.

317
00:17:44,370 --> 00:17:46,290
And until then, enjoy machine learning.
