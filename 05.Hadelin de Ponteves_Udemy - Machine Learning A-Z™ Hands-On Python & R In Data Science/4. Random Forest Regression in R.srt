1

00:00:00,420  -->  00:00:06,720
Hello and welcome to this art tutorial here we are at the final round of regression with our final regression

2

00:00:06,720  -->  00:00:08,970
model random Forest's regression.

3

00:00:09,300  -->  00:00:12,660
In a previous section we saw the decision tree regression model.

4

00:00:12,700  -->  00:00:17,460
So now the decision tree regression doesn't have any secret for you then you will perfectly understand

5

00:00:17,520  -->  00:00:23,610
ran M-4s regression because the random forest is just a team of decision trees each one making some

6

00:00:23,610  -->  00:00:29,760
prediction of your dependent variable and the ultimate prediction of the random first itself is simply

7

00:00:29,760  -->  00:00:33,730
the average of the different predictions of all the different trees in the forest.

8

00:00:33,840  -->  00:00:36,870
And actually at the end of the previous section about decision trees.

9

00:00:36,870  -->  00:00:38,220
I asked you an enigma.

10

00:00:38,250  -->  00:00:40,800
The Enigma was knowing the result we got.

11

00:00:40,800  -->  00:00:48,240
With one tree what would be the result with ten trees or 100 trees or 500 trees in terms of visualization

12

00:00:48,420  -->  00:00:49,960
and interim of prediction.

13

00:00:50,190  -->  00:00:56,460
So I hope that after watching the intuition to toys made by Karylle you actually ask yourself this question

14

00:00:56,760  -->  00:01:01,060
and try to predict what's going to happen here with random first regression.

15

00:01:01,230  -->  00:01:02,750
So let's find out about that.

16

00:01:02,760  -->  00:01:06,740
We are going to build a random first regression model and see what happens.

17

00:01:06,870  -->  00:01:08,040
So let's do it.

18

00:01:08,040  -->  00:01:11,520
We are going to start by selecting the right father as a working directory.

19

00:01:11,550  -->  00:01:14,100
So it's in part two regression.

20

00:01:14,100  -->  00:01:16,340
And here is the final regression model we are building.

21

00:01:16,400  -->  00:01:17,590
Them for a regression.

22

00:01:17,730  -->  00:01:22,810
So let's go inside and that's the rifle that we want to set as working directory with the position services

23

00:01:22,920  -->  00:01:23,550
file.

24

00:01:23,730  -->  00:01:28,270
So let's click on this more button and set as a working directory.

25

00:01:28,290  -->  00:01:29,060
All good.

26

00:01:29,200  -->  00:01:33,250
And now let's take our regression template to build this model efficiently.

27

00:01:33,330  -->  00:01:39,420
So we are actually going to take everything from here to the bottom but we will only include this code

28

00:01:39,420  -->  00:01:44,580
section to visualize the regression model results because you understood that the decision tree regression

29

00:01:44,580  -->  00:01:50,310
model is a non-continuous regression model and since random forest is a combination of decision trees

30

00:01:50,640  -->  00:01:55,320
then it's a combination of non-continuous regression model and intuitively we understand.

31

00:01:55,320  -->  00:01:59,990
We can guess that the Ranum for us regression will is not going to be continuous either.

32

00:02:00,240  -->  00:02:06,060
So since this code doesn't work for non continuous regression model we will actually use this one that

33

00:02:06,060  -->  00:02:07,840
works perfectly for it.

34

00:02:07,840  -->  00:02:15,360
So I'm going to copy this paste that here and remove this section that is non appropriate for non-continuous

35

00:02:15,420  -->  00:02:16,650
regression models.

36

00:02:16,650  -->  00:02:17,400
Here we go.

37

00:02:17,510  -->  00:02:19,830
And now the template is ready.

38

00:02:19,890  -->  00:02:21,420
Let's change the basics.

39

00:02:21,420  -->  00:02:28,380
Let's replace here regression all by random forest regression.

40

00:02:28,380  -->  00:02:35,500
Visualizing the run for US regression results and fitting random forest regression to our data set.

41

00:02:35,610  -->  00:02:36,390
OK great.

42

00:02:36,510  -->  00:02:40,080
So now let's build the model which is in this section here.

43

00:02:40,080  -->  00:02:42,990
So let's remove this.

44

00:02:42,990  -->  00:02:50,340
And as usual we're going to import the right library for the job and then use a function to build our

45

00:02:50,430  -->  00:02:51,980
Random forest regressors.

46

00:02:52,320  -->  00:02:55,940
So the package you are going to import is called Ranum forest.

47

00:02:56,190  -->  00:03:00,810
So for those of you who don't have the package installed your packages here.

48

00:03:00,990  -->  00:03:02,520
Well you can check it out.

49

00:03:02,520  -->  00:03:08,040
Mine is already installed because I used it before but I'm going to write this line here.

50

00:03:08,040  -->  00:03:18,150
For those of you who need to install it so install dot packages parenthesis and in quotes random so

51

00:03:18,180  -->  00:03:21,840
no capital R but then capital F o r s t.

52

00:03:21,870  -->  00:03:22,380
All right.

53

00:03:22,390  -->  00:03:23,330
Ranum forest.

54

00:03:23,500  -->  00:03:27,460
And so I'm not going to install it because my needs are in style so I'm going to put down comments.

55

00:03:27,570  -->  00:03:32,130
But if you want to install it you just need to select this line as I just did and press command control

56

00:03:32,130  -->  00:03:33,650
press enter to execute it.

57

00:03:33,810  -->  00:03:35,980
And this will install the package properly.

58

00:03:36,150  -->  00:03:39,750
But here I'm going to put in comment by pressing command plus shift Blassie.

59

00:03:39,750  -->  00:03:40,650
Here we go.

60

00:03:40,790  -->  00:03:48,330
And now when we have to do is to add this you know Library random forest to actually automatically select

61

00:03:48,330  -->  00:03:53,460
the box here to import automatically to run him for his package when we execute the whole code or the

62

00:03:53,460  -->  00:03:54,190
section.

63

00:03:54,510  -->  00:03:59,530
So that's important and now time to build the aggressor so let's do it.

64

00:03:59,580  -->  00:04:04,810
We're going to call the aggressor regressors as usual to keep things simple and equals.

65

00:04:04,960  -->  00:04:11,290
And now the function that we're going to use is also random forest written the same.

66

00:04:11,470  -->  00:04:17,450
So now let's add some parenthesis and now the press one to have a look at the arguments.

67

00:04:17,820  -->  00:04:23,790
The arguments are here and the first argument is data but as you can see it specifies that it's an optional

68

00:04:23,790  -->  00:04:30,360
data frame and we could use this argument to build our regressors but we are going to use the main arguments

69

00:04:30,360  -->  00:04:35,420
to specify the independent variables on one side and the dependent variable and another side.

70

00:04:35,580  -->  00:04:39,600
And to do this we are going to use these two arguments x and y.

71

00:04:39,720  -->  00:04:46,260
So X will contain the matrix of features that is the independent variables and y will contain the dependent

72

00:04:46,260  -->  00:04:47,280
variable vector.

73

00:04:47,400  -->  00:04:49,530
That is the sorry column.

74

00:04:49,650  -->  00:04:55,770
So let's first input these two arguments to the first argument is X equals and so we have several ways

75

00:04:55,770  -->  00:04:58,050
to take our independent variables.

76

00:04:58,140  -->  00:05:05,790
So one of the way is to take our dataset here and then choose to write columns of the independent variables

77

00:05:06,150  -->  00:05:11,590
and you know our dataset is composed of two columns the first column indexed by one which is the independent

78

00:05:11,620  -->  00:05:17,380
variable column and the second column indexed by 2 which is our dependent variable column.

79

00:05:17,400  -->  00:05:23,790
So here we need index 1 because we want to take the independent variable right now next argument the

80

00:05:23,790  -->  00:05:26,730
next argument is why the dependent variable vector.

81

00:05:26,820  -->  00:05:31,260
And now as you can see why is expected to be a response vector.

82

00:05:31,260  -->  00:05:34,920
It's actually a vector and here it expected to have a data frame.

83

00:05:35,130  -->  00:05:41,670
So by using this one index and two brackets here I actually import a data from that here to get a vector

84

00:05:41,680  -->  00:05:41,730
.

85

00:05:41,760  -->  00:05:46,830
I actually need to use another trick another technique which is know to use the dollar sign and then

86

00:05:46,830  -->  00:05:50,160
the name of the column which is of course salary.

87

00:05:50,700  -->  00:05:51,960
And that will give me a vector.

88

00:05:51,960  -->  00:05:59,640
So just to recap this syntax here will give me a data frame because we're taking some sub data frame

89

00:05:59,670  -->  00:06:06,110
of our original data from data set and here by using the dollar sign syntax here taking data said Doris

90

00:06:06,120  -->  00:06:06,930
unsorry.

91

00:06:06,960  -->  00:06:11,640
I'm actually taking the salary column of our data dataset but that will make it a vector and that's

92

00:06:11,640  -->  00:06:15,750
exactly what we run because the Y argument here is expecting a vector.

93

00:06:16,020  -->  00:06:16,990
So we're all good.

94

00:06:16,990  -->  00:06:20,530
And now we actually need to input a third argument.

95

00:06:20,580  -->  00:06:21,920
Can you guess what that is.

96

00:06:21,930  -->  00:06:26,670
For those of you all the Python tutorial Well you will guess what it's going to be.

97

00:06:26,850  -->  00:06:30,510
It's actually going to be entry the number of trees in the forest.

98

00:06:30,510  -->  00:06:35,370
Well of course we're building around them forests so it's actually a lot better if we can choose the

99

00:06:35,370  -->  00:06:40,560
number of trees that we build in our forest and it's even better considering the fact that we're going

100

00:06:40,560  -->  00:06:45,720
to play around with different number of trees that is we're going to start with 10 trees with a forest

101

00:06:45,720  -->  00:06:52,320
of 10 trees and then you know we'll try with a lot more than 10 trees like 100 trees or 300 trees or

102

00:06:52,350  -->  00:06:53,440
500 trees.

103

00:06:53,670  -->  00:06:59,580
So that's what we're going to input the third argument entry and we're going to start with 10 trees

104

00:06:59,580  -->  00:06:59,910
.

105

00:06:59,910  -->  00:07:05,220
All right so let's start with this and that's all the arguments we need to build around forest.

106

00:07:05,250  -->  00:07:07,000
We only need independent variables.

107

00:07:07,080  -->  00:07:13,410
The dependent variable and the number of trees and that will already make a robust Ranum for us regression

108

00:07:13,410  -->  00:07:19,010
model and then we will make it even more robust by adding more trees in the forest.

109

00:07:19,020  -->  00:07:24,660
But before we continue let's set the Ranum factors to something fixed so that we all get the same results

110

00:07:24,660  -->  00:07:24,910
.

111

00:07:24,960  -->  00:07:28,250
So you know in Python We used a random set parameter equal to zero.

112

00:07:28,350  -->  00:07:34,100
Here we can do the same on or by using the set dot seed function.

113

00:07:34,320  -->  00:07:39,750
And then in this function we actually put a seed and you know we can use whatever we want in Bison we

114

00:07:39,750  -->  00:07:47,550
usually take zero 42 and we like to do is you know take either 1 2 3 or 1 2 3 4.

115

00:07:47,550  -->  00:07:49,850
So let's use the seed to get the same result.

116

00:07:50,070  -->  00:07:54,660
And that's what made this tutorial easier to follow if you're coding at the same time.

117

00:07:54,690  -->  00:07:59,090
So now we're all good we're actually all good with the whole code.

118

00:07:59,100  -->  00:08:01,020
We don't have anything to replace.

119

00:08:01,110  -->  00:08:07,260
The only thing that will do now is to you know try several Rudham for us with several number of trees

120

00:08:07,650  -->  00:08:13,660
and look at the visualization results and look at the prediction to see if we're getting close to the

121

00:08:13,680  -->  00:08:19,150
supposed 160 K per year salary of our new employee that is about to be hired.

122

00:08:19,620  -->  00:08:20,750
So let's do it.

123

00:08:20,760  -->  00:08:23,280
Let's execute the sections one by one.

124

00:08:23,280  -->  00:08:26,390
So let's import the data set first.

125

00:08:26,400  -->  00:08:27,090
Here we go.

126

00:08:27,090  -->  00:08:32,850
David said Well important we make sure we have our two columns the independent variable level and the

127

00:08:32,850  -->  00:08:34,440
dependent variable salary.

128

00:08:34,470  -->  00:08:35,610
Perfect.

129

00:08:35,760  -->  00:08:39,090
Now no need to split the dataset into 20 sets in the test set.

130

00:08:39,120  -->  00:08:45,560
No need to apply feature scaling and now time to create our first random forest.

131

00:08:45,570  -->  00:08:52,330
So let's do this let's execute this code section here and here it is random for us.

132

00:08:52,350  -->  00:08:54,260
Well created perfect.

133

00:08:54,300  -->  00:08:55,990
So now it's time to have fun.

134

00:08:56,010  -->  00:08:59,920
Would you like to visualize the result first or getting the prediction.

135

00:09:00,150  -->  00:09:05,220
Well first let's maybe visualize the results because we want to make sure we have the right model and

136

00:09:05,220  -->  00:09:08,610
we want to validate it because we will try several number of trees.

137

00:09:08,700  -->  00:09:10,200
Here we are starting with centuries.

138

00:09:10,200  -->  00:09:12,600
So we want to see if it looks like a correct model.

139

00:09:12,690  -->  00:09:15,110
So I'm going to execute this section.

140

00:09:15,180  -->  00:09:18,430
Here we go and let's see what we'll get.

141

00:09:19,290  -->  00:09:21,080
OK so first of all this looks fine.

142

00:09:21,090  -->  00:09:23,810
We don't seem to have any problem here.

143

00:09:23,820  -->  00:09:28,340
The only thing that we can improve very quickly is actually you know those straight lines here.

144

00:09:28,500  -->  00:09:32,330
There are supposed to be vertical and to get a better representation of this.

145

00:09:32,430  -->  00:09:37,430
We just need to increase the resolution as we did for Decision Tree regression.

146

00:09:37,440  -->  00:09:45,760
So let's add on one that will be sufficient and let's re execute this and now much better it almost

147

00:09:45,760  -->  00:09:50,520
looks like it's some vertical straight lines representing better than non-continuous.

148

00:09:50,770  -->  00:09:51,780
And so now what can we say.

149

00:09:51,790  -->  00:09:55,980
Let's zoom on this plot to have a better look at.

150

00:09:55,990  -->  00:09:57,240
Now that's Interbrand.

151

00:09:57,570  -->  00:09:58,030
OK.

152

00:09:58,090  -->  00:10:02,830
So the answer to the enigma that I asked you in the previous section and that I was asking you again

153

00:10:02,840  -->  00:10:10,330
in this tutorial is that we simply get more steps in the stairs by having several decision trees instead

154

00:10:10,330  -->  00:10:12,150
of one decision tree.

155

00:10:12,340  -->  00:10:18,370
We have a lot more steps in the stairs than what we had with one decision tree and therefore we have

156

00:10:18,580  -->  00:10:23,890
a lot more of splits of the whole range of levels and therefore a lot more intervals of the different

157

00:10:23,890  -->  00:10:31,000
levels so each straight horizontal line here separate by these vertical lines or one interval that is

158

00:10:31,090  -->  00:10:36,340
one split and the fact that we get more steps in the stairs is actually quite intuitive because you

159

00:10:36,340  -->  00:10:42,370
know if we get for example this prediction here for the 6.5 level Well what happened for this prediction

160

00:10:42,370  -->  00:10:49,520
is that we had 10 trees voting on which step the salary of the 6.5 level position would be.

161

00:10:49,750  -->  00:10:54,880
And then the Ranum for us took the average of all the different predictions of the salary of the 6.5

162

00:10:54,880  -->  00:10:57,520
level made by all the different trees in the forest.

163

00:10:57,610  -->  00:11:03,430
And for example if we take the fourth position level 10 votes were made each of these 10 votes correspond

164

00:11:03,430  -->  00:11:09,280
to one prediction of the level for salary made by each one of those ten trees and then to run them for

165

00:11:09,280  -->  00:11:14,710
us took the average of these 10 predictions and this average is nothing else than the prediction of

166

00:11:14,710  -->  00:11:18,540
the level for salary made by the random forest itself.

167

00:11:18,700  -->  00:11:24,580
And so we get more steps because simply the whole range of levels is split into more intervals and that

168

00:11:24,820  -->  00:11:29,770
is because the random forest is calculating many different averages of its decision trees predictions

169

00:11:29,800  -->  00:11:31,480
in each of these intervals.

170

00:11:31,720  -->  00:11:34,050
So that's what happened it's quite intuitive.

171

00:11:34,060  -->  00:11:39,580
However there is something important to point out here is that if we add a lot more trees in our random

172

00:11:39,580  -->  00:11:46,210
forest Well it doesn't mean we'll get a lot more steps on the stairs because the more you add some trees

173

00:11:46,570  -->  00:11:51,850
the more the average of the different predictions made by the trees is converging to the same average

174

00:11:51,850  -->  00:11:52,030
.

175

00:11:52,090  -->  00:11:56,060
You know this is based on the same technique entropy and information gain.

176

00:11:56,110  -->  00:12:01,660
So the more you add trees the more the average of these votes will converge to the same ultimate average

177

00:12:02,260  -->  00:12:06,280
and therefore it will converge to some certain shape of stairs here.

178

00:12:06,280  -->  00:12:08,630
So that's important to visualize this as well.

179

00:12:08,650  -->  00:12:13,290
And now since we have our intuition of the visualization of the run for US regression one day.

180

00:12:13,540  -->  00:12:15,510
Let's see what happens with the prediction.

181

00:12:15,640  -->  00:12:18,160
So let's see what prediction we get.

182

00:12:18,160  -->  00:12:21,940
Remember that disemployed said that it's pretty sorry was a 160 k.

183

00:12:22,180  -->  00:12:25,930
And now let's see what's around for us composed of 10 trees.

184

00:12:25,930  -->  00:12:31,720
So let's look at that and it says that a previous Saori was a hundred and forty one thousand dollars

185

00:12:31,730  -->  00:12:31,820
.

186

00:12:31,900  -->  00:12:38,360
That's actually a very dangerous prediction because we are way below the 160 K Sellery that this new

187

00:12:38,360  -->  00:12:40,400
entry is set to have in its previous company.

188

00:12:40,510  -->  00:12:45,850
So if we trust this prediction we will actually think this employee's bluffing but no worries we will

189

00:12:45,850  -->  00:12:46,770
not stop here.

190

00:12:46,780  -->  00:12:50,860
Right now we're going to try run first with a lot more than 10 trees.

191

00:12:50,920  -->  00:12:54,130
So let's pick for example 100 trees and let's see what we'll get.

192

00:12:54,130  -->  00:12:57,680
So I'm going to rebuild the model.

193

00:12:57,730  -->  00:12:58,500
Here we go.

194

00:12:58,630  -->  00:13:01,720
And now let's look at the graphic results.

195

00:13:02,080  -->  00:13:08,740
And as I was telling you we don't get much more steps in this plot or new random for us regression.

196

00:13:08,980  -->  00:13:14,140
You know we multiplied our number of trees by 10 but the number of steps was definitely not multiply

197

00:13:14,140  -->  00:13:14,980
by 10.

198

00:13:15,020  -->  00:13:17,480
We compare we can compare that very quickly.

199

00:13:17,500  -->  00:13:20,360
This is the previous plot and this is a new one.

200

00:13:20,540  -->  00:13:26,710
Ten one hundred trees we can see that we have maybe a little more steps but definitely not ten times

201

00:13:26,790  -->  00:13:27,790
the previous steps.

202

00:13:27,790  -->  00:13:33,340
So the reason for this the explanation is related to this convergence idea that I talked to you about

203

00:13:33,340  -->  00:13:33,590
.

204

00:13:33,820  -->  00:13:39,910
And so what changes here with 100 trees in terms of the plot is not the number of steps there was increased

205

00:13:40,210  -->  00:13:47,110
but a better choice a better location of the steps and the stairs with respect to our salary access

206

00:13:47,110  -->  00:13:47,210
.

207

00:13:47,230  -->  00:13:53,620
That means that maybe the steps are better located to make our ultimate predictions of the salaries

208

00:13:53,980  -->  00:13:57,840
of each of our level from 1 to 10 incremented by on one.

209

00:13:58,060  -->  00:14:05,220
So to check that out we simply need to make our final prediction to predict the salary of the 6.5 level

210

00:14:05,230  -->  00:14:05,380
.

211

00:14:05,380  -->  00:14:12,200
So let's recap the employer is saying 160 k around and random forest was dently said a hundred 41 k

212

00:14:12,210  -->  00:14:12,510
.

213

00:14:12,670  -->  00:14:19,740
And now let's see what say around M-4s with 100 trees exited and now it says 166 K..

214

00:14:19,780  -->  00:14:20,690
So much better.

215

00:14:20,710  -->  00:14:26,980
We're getting close to the supposed real salary of 160 K and besides we're never actually on the good

216

00:14:26,980  -->  00:14:31,630
side of negotiation because we will no longer think that this employee is bluffing.

217

00:14:31,630  -->  00:14:37,010
So since the prediction seems to be improving as we increase the number of trees let's actually try

218

00:14:37,030  -->  00:14:39,970
with 500 trees that's a huge forest.

219

00:14:39,970  -->  00:14:40,650
We have now.

220

00:14:40,660  -->  00:14:46,010
So let's execute this to build our new huge forest of 500 trees.

221

00:14:46,030  -->  00:14:46,770
Here we go.

222

00:14:46,770  -->  00:14:48,430
Ufer has created.

223

00:14:48,430  -->  00:14:51,370
Let's have a quick look at the visualization plot results.

224

00:14:51,520  -->  00:14:56,090
But it's going to be the same thing we will not get a lot of more stares maybe a little more.

225

00:14:56,200  -->  00:14:58,270
Well actually let's check it out.

226

00:14:59,140  -->  00:15:00,370
Well definitely not.

227

00:15:00,370  -->  00:15:03,040
We seem to have the same number of steps on the stairs.

228

00:15:03,190  -->  00:15:09,460
But as I was telling you each of the steps in this series might actually be better located to make each

229

00:15:09,460  -->  00:15:13,440
ultimate prediction of the salaries for each of the 10 levels here.

230

00:15:13,450  -->  00:15:20,350
So the best way to check that out is actually to get our ultimate prediction of the sorry of this 6.5

231

00:15:20,350  -->  00:15:21,340
level.

232

00:15:21,340  -->  00:15:22,790
And let's check it out.

233

00:15:22,840  -->  00:15:26,150
Let's see if we get a better prediction than the 166 OK.

234

00:15:26,550  -->  00:15:27,540
Executing.

235

00:15:27,700  -->  00:15:34,980
And right on the spot we hit the bull's eye with 160 458 predicted sorry.

236

00:15:35,080  -->  00:15:40,360
So awesome job that just running for US with 500 trees just dead here because it predicted almost the

237

00:15:40,360  -->  00:15:43,890
same sorry as to suppose to 160 K salary.

238

00:15:43,890  -->  00:15:49,470
The disputer simply said to have and it's Breese company and actually so far before we made this run

239

00:15:49,480  -->  00:15:56,530
for us with 500 trees the best model that made the closest prediction to this 160 get salary was a polynomial

240

00:15:56,530  -->  00:16:02,110
regression model and now the Ranum for us regression is beating the polynomial regression model because

241

00:16:02,110  -->  00:16:05,890
now we get a prediction that is almost the same as a real value.

242

00:16:06,160  -->  00:16:07,480
So right in the spot.

243

00:16:07,570  -->  00:16:08,800
Congratulations.

244

00:16:08,980  -->  00:16:15,220
We actually made our final model and now I just want to conclude this tutorial by making this transition

245

00:16:15,220  -->  00:16:21,750
to one of our future point which is actually part 10 in part and we will build some essential machinery

246

00:16:21,790  -->  00:16:22,160
model.

247

00:16:22,180  -->  00:16:28,540
There is some models that are a combination of several machinery models and you know in machinery these

248

00:16:28,540  -->  00:16:30,080
are actually the best models.

249

00:16:30,130  -->  00:16:35,020
You know when you have a team of several machine models they can actually make an awesome prediction

250

00:16:35,020  -->  00:16:39,820
because unless we have a nine time machine running model in argumentation of machinery models that is

251

00:16:39,820  -->  00:16:41,240
the only model to be right.

252

00:16:41,440  -->  00:16:45,880
Well you are more likely to get the correct prediction with ten machine learning models predicting the

253

00:16:45,880  -->  00:16:48,370
same thing than was just one model.

254

00:16:48,550  -->  00:16:50,120
So that's actually what we did here.

255

00:16:50,230  -->  00:16:55,490
Well we had a team of same machine learning models which were decision tree regression models.

256

00:16:55,570  -->  00:16:59,710
But in the future we'll make a team of different machine only models.

257

00:16:59,770  -->  00:17:01,010
So that's going to be very fun.

258

00:17:01,030  -->  00:17:03,140
That's going to be very powerful as well.

259

00:17:03,190  -->  00:17:05,490
And I look forward to getting there with you.

260

00:17:05,710  -->  00:17:10,980
So now I'm telling you congratulations for two things first for building this very powerful regression

261

00:17:10,990  -->  00:17:17,640
model to run for US regression model and second for having build all our regression models.

262

00:17:17,710  -->  00:17:23,580
We built some linear regression models some non linear regression models some non-linear are non-continuous

263

00:17:23,590  -->  00:17:28,930
regression models and some non-linear or non-continuous and simple regression models.

264

00:17:29,080  -->  00:17:33,950
So congratulations you're definitely on your way to becoming some expert in machine learning.

265

00:17:34,060  -->  00:17:36,180
But wait for what's coming next.

266

00:17:36,190  -->  00:17:40,840
So speaking of what's coming next I look forward to seeing you in the next sections or next parts.

267

00:17:41,020  -->  00:17:42,930
And until then enjoy mission early.
