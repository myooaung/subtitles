WEBVTT
1
00:00:00.240 --> 00:00:06.150
Hello, my friends, and welcome to this new practical activity of part nine dimensionality reduction.

2
00:00:06.570 --> 00:00:12.750
So in the previous section, we experimented with PCI, a principal component analysis, and we indeed

3
00:00:12.750 --> 00:00:19.260
got great results with our wine dataset, which will be the same data set for this new section, because,

4
00:00:19.260 --> 00:00:22.740
you know, we want to compare several dimensionality reduction techniques.

5
00:00:23.040 --> 00:00:23.690
So there we go.

6
00:00:23.700 --> 00:00:29.580
We're going to see if we can even beat BCA, which only had one incorrect prediction.

7
00:00:30.000 --> 00:00:31.730
So we're going to work with the same set.

8
00:00:31.980 --> 00:00:37.380
And therefore, the implementation will be exactly the same, except one cell, which will be the cell,

9
00:00:37.380 --> 00:00:40.740
of course, where we implement LDA instead of Pichette.

10
00:00:41.280 --> 00:00:42.620
All right, I'm ready.

11
00:00:42.760 --> 00:00:47.760
Let's do this before we get into this for the partner and let's make sure everyone here is on the same

12
00:00:47.760 --> 00:00:48.150
page.

13
00:00:48.240 --> 00:00:52.950
I give you the link to this for there containing all the codes and data sets right before this to Tokyo.

14
00:00:53.280 --> 00:00:54.750
So make sure to connect to it.

15
00:00:54.870 --> 00:00:55.770
And now here we go.

16
00:00:56.040 --> 00:00:59.370
Let's end two part nine dimensionality reduction.

17
00:00:59.850 --> 00:01:06.540
And now we're gonna go into Section 44 Linear Discriminant Analysis LDA, which will be a new technique

18
00:01:06.570 --> 00:01:08.100
of dimensionality reduction.

19
00:01:08.370 --> 00:01:10.170
Very powerful, as we will see.

20
00:01:10.260 --> 00:01:12.030
So let's start with Python, as usual.

21
00:01:12.150 --> 00:01:13.320
And there we go.

22
00:01:13.630 --> 00:01:16.650
This folder, just as a previous one, has two fouls.

23
00:01:16.920 --> 00:01:18.270
This is the implementation.

24
00:01:18.330 --> 00:01:25.230
And this is the same wine dataset which, you know, belongs to a wine shop business owner who first

25
00:01:25.290 --> 00:01:31.170
asked you, you know, the most talented data scientist to do some clustering to identify different

26
00:01:31.170 --> 00:01:35.610
customer segments for each of, you know, the wines of this dataset.

27
00:01:35.670 --> 00:01:39.270
You know, each row of this data set corresponds to a certain wine.

28
00:01:39.510 --> 00:01:44.910
And for each one, we have several wine features or, you know, characteristics all these up to here.

29
00:01:45.180 --> 00:01:51.870
And you used all these features to identify those three customer segments or, you know, customer clusters.

30
00:01:52.440 --> 00:01:57.900
And after which, you know, since this wine shop owner was so happy and impressed by your job, well,

31
00:01:57.900 --> 00:02:02.460
then, of course, the owner asked you to do another mission, which is the one we're about to do with

32
00:02:02.550 --> 00:02:09.480
LDA, which consist of building a predictive moral combine, two dimensionality reduction applied to

33
00:02:09.480 --> 00:02:15.330
this dataset so that for each new wine that this owner has in its wine shop.

34
00:02:15.600 --> 00:02:21.810
Well, by deploying this new predictive model, this owner will be able to predict which customer segment

35
00:02:22.110 --> 00:02:28.050
this new wine belongs to so that it can recommend this new wine to the right customers and therefore

36
00:02:28.410 --> 00:02:30.420
optimize eventually the sales.

37
00:02:31.140 --> 00:02:31.460
All right.

38
00:02:31.500 --> 00:02:33.210
That's exactly the same data set.

39
00:02:33.360 --> 00:02:35.820
And now let's move on to our implementation.

40
00:02:35.910 --> 00:02:42.840
Linear discriminant analysis, which we can either open with Google Collaboratory as I'm doing it now,

41
00:02:43.200 --> 00:02:44.660
or Dupere in notebook.

42
00:02:45.120 --> 00:02:51.630
And as you notice, I kept this previous implementation we did on PCH so that we can, you know, compare

43
00:02:51.870 --> 00:02:52.440
the results.

44
00:02:52.580 --> 00:02:53.220
Indian, right.

45
00:02:53.300 --> 00:02:54.220
This is BCA.

46
00:02:54.630 --> 00:02:55.740
This is LDA.

47
00:02:55.770 --> 00:03:01.170
But, you know, since this isn't read only mode, we're going to create now a copy so that we can reimplement

48
00:03:01.170 --> 00:03:03.990
that cell that builds the LDA model.

49
00:03:04.230 --> 00:03:06.320
So there we go, save a copy in drive.

50
00:03:06.690 --> 00:03:12.060
This will create a copy inside of which will be able to reimplement the LDA model.

51
00:03:12.600 --> 00:03:13.110
All right.

52
00:03:13.170 --> 00:03:17.970
And now we can, you know, close this so that we can have the two implementations next to each other,

53
00:03:17.980 --> 00:03:18.990
you know, the two copies.

54
00:03:19.480 --> 00:03:20.400
And now let's do this.

55
00:03:20.490 --> 00:03:25.350
Let's quickly remove, you know, the cell that implements the LDA.

56
00:03:25.500 --> 00:03:26.170
This one.

57
00:03:26.520 --> 00:03:30.840
And let's reimplement this because, you know, all the rest is the same.

58
00:03:31.080 --> 00:03:36.570
I will actually remove all these outputs here so that you don't see the final results and we can keep

59
00:03:36.570 --> 00:03:37.590
them as a surprise.

60
00:03:37.950 --> 00:03:40.700
So let me just remove the outputs, too.

61
00:03:41.280 --> 00:03:42.390
Don't look too close.

62
00:03:42.510 --> 00:03:42.990
And there we go.

63
00:03:43.230 --> 00:03:43.470
All right.

64
00:03:43.500 --> 00:03:51.240
So basically, all the cells of this implementation are exactly the same as the previous one BCA, except,

65
00:03:51.240 --> 00:03:55.350
of course, the cell that implements LDA right here.

66
00:03:55.470 --> 00:03:57.090
So no need to re-explain on this.

67
00:03:57.090 --> 00:04:00.670
Plus, all these other cells result from our diverse toolkit.

68
00:04:00.780 --> 00:04:04.200
So you're definitely 100 percent familiar with them.

69
00:04:04.890 --> 00:04:05.210
All right.

70
00:04:05.220 --> 00:04:06.090
So let's do this.

71
00:04:06.150 --> 00:04:09.030
Let's, you know, apply lda.

72
00:04:09.270 --> 00:04:11.050
So we're going to create a new code cell.

73
00:04:11.160 --> 00:04:12.120
And there we go.

74
00:04:12.450 --> 00:04:15.540
Let's implement linear discriminant analysis.

75
00:04:15.780 --> 00:04:17.550
So now you have two options.

76
00:04:17.730 --> 00:04:24.390
The first and the best option is to press bus on the video and try to implement this yourself by, of

77
00:04:24.390 --> 00:04:32.010
course, browsing the Seigler an API and find that LDA class that can implement that LDA dimensionality

78
00:04:32.010 --> 00:04:36.120
reduction technique and you will definitely end up with the same solution.

79
00:04:36.210 --> 00:04:37.890
I will implement in a few seconds.

80
00:04:38.340 --> 00:04:44.340
And the second option is, of course, to well, not press pause on the video and implement with me

81
00:04:44.370 --> 00:04:46.890
the solution in, let's say, three seconds.

82
00:04:47.400 --> 00:04:50.520
Three, two, one, and go.

83
00:04:50.580 --> 00:04:51.480
All right, let's do this.

84
00:04:51.510 --> 00:04:53.580
Let's implement together lda.

85
00:04:54.270 --> 00:04:58.980
So as I've just said, we're gonna implement LDA things to do cycler in library.

86
00:04:59.010 --> 00:04:59.610
Therefore, we're gonna.

87
00:05:00.170 --> 00:05:01.860
From České.

88
00:05:02.070 --> 00:05:08.850
Learn from which we're gonna get access to this time, not, you know, the.

89
00:05:09.180 --> 00:05:10.670
Well, let me go to Peter here.

90
00:05:11.070 --> 00:05:18.210
Not the decomposition module of psych learned, but a new one, which is very easy to remember because

91
00:05:18.210 --> 00:05:20.430
this is actually discriminant.

92
00:05:22.210 --> 00:05:22.910
Underscore.

93
00:05:23.140 --> 00:05:23.890
Analysis.

94
00:05:24.490 --> 00:05:24.850
OK.

95
00:05:24.970 --> 00:05:31.480
That's another module of cycler, and it contains, of course, the class that can implement LTA and

96
00:05:31.480 --> 00:05:32.030
that class.

97
00:05:32.050 --> 00:05:37.180
Well, you know, after this import here, we have to add the name of this class and the name of this

98
00:05:37.180 --> 00:05:39.120
class is Capital L..

99
00:05:39.310 --> 00:05:44.980
And then very simply, linear discriminant analysis.

100
00:05:46.100 --> 00:05:46.520
All right.

101
00:05:46.610 --> 00:05:47.090
Very good.

102
00:05:47.120 --> 00:05:51.890
The reason why Google killer, by the way, is not helping me with the suggestions is because the notebook

103
00:05:51.920 --> 00:05:52.700
is not running.

104
00:05:52.730 --> 00:05:56.190
And remember to run the notebook or, you know, to connect it.

105
00:05:56.510 --> 00:06:01.230
Well, you need to either run any of the first cells or upload the data set.

106
00:06:01.260 --> 00:06:04.760
So let's do it right now so that, you know, Google Kulab can assist me.

107
00:06:04.790 --> 00:06:06.680
I really love it when it does it.

108
00:06:07.040 --> 00:06:12.920
So right now, I just clicked on this for the button and then let's click the upload button and we will

109
00:06:12.920 --> 00:06:16.020
end up in the know previous folder for principal component analysis.

110
00:06:16.070 --> 00:06:17.570
But let me show you the path again.

111
00:06:17.900 --> 00:06:19.310
I put my machine learning.

112
00:06:19.370 --> 00:06:20.840
It is that folder in my desktop.

113
00:06:21.170 --> 00:06:28.910
So inside, we're gonna go now to Part nine and then Section 44, linear discriminant analysis and Python.

114
00:06:29.000 --> 00:06:29.800
And then one.

115
00:06:30.020 --> 00:06:30.250
All right.

116
00:06:30.260 --> 00:06:34.550
So this is exactly the same dataset as before, but I just wanted to show you the path.

117
00:06:34.960 --> 00:06:35.540
All right.

118
00:06:35.780 --> 00:06:37.190
And there we go.

119
00:06:37.190 --> 00:06:37.940
We have the one.

120
00:06:38.300 --> 00:06:43.370
And so now I'm going to show you if I retype this linear discriminant, see?

121
00:06:43.520 --> 00:06:44.450
Now it is helping me.

122
00:06:44.480 --> 00:06:49.730
So that's maybe better to have, you know, this reflex to upload data set right at the beginning.

123
00:06:50.030 --> 00:06:50.310
OK.

124
00:06:50.690 --> 00:06:52.190
So linear discriminant, analysis.

125
00:06:52.220 --> 00:06:56.720
But since this class name is actually pretty long and pretty, not practical.

126
00:06:56.720 --> 00:07:01.460
Well, it's just, you know, add a simple shortcut like lda.

127
00:07:01.640 --> 00:07:02.270
We can do this.

128
00:07:02.300 --> 00:07:02.770
That's solid.

129
00:07:02.780 --> 00:07:03.200
Fine.

130
00:07:03.560 --> 00:07:10.970
And now let's press enter to move on to the next step, which is, of course, naturally to create an

131
00:07:11.090 --> 00:07:14.720
object of this linear discriminant analysis class.

132
00:07:14.990 --> 00:07:15.250
All right.

133
00:07:15.260 --> 00:07:17.530
So, of course, we're going to call it LDA.

134
00:07:17.810 --> 00:07:18.260
Right.

135
00:07:18.530 --> 00:07:20.120
And now we're going to call this class.

136
00:07:20.150 --> 00:07:25.970
And since we gave it the shortcut LDA, well, we can simply call lda this way.

137
00:07:26.440 --> 00:07:26.930
And now.

138
00:07:26.990 --> 00:07:29.240
Well, exactly the same as before.

139
00:07:29.390 --> 00:07:35.700
This LDA class needs to take as input only one argument, which is exactly the same as before.

140
00:07:35.930 --> 00:07:38.030
And also it has the exact same name.

141
00:07:38.120 --> 00:07:44.780
It is an component's which corresponds, of course, to the final number of extracted features.

142
00:07:45.050 --> 00:07:49.280
You want to end up with after applying this dimensionality reduction technique.

143
00:07:49.670 --> 00:07:54.980
And of course, as I recommended in the previous section, we're going to start with two so that we

144
00:07:54.980 --> 00:07:57.800
can see it even with only two extracted features.

145
00:07:58.070 --> 00:07:59.760
Well, we can get great results.

146
00:07:59.930 --> 00:08:04.130
And if that's the case, well, not only will get great results, but also cherry on the cake.

147
00:08:04.160 --> 00:08:09.920
We'll be able to visualize the results on a nice to plot Indian, you know, thanks to these two code

148
00:08:09.920 --> 00:08:10.400
section.

149
00:08:11.010 --> 00:08:11.530
All right.

150
00:08:11.810 --> 00:08:13.720
But right now, we need to finish this.

151
00:08:13.820 --> 00:08:14.360
There we go.

152
00:08:14.840 --> 00:08:17.100
We're gonna extract only two features.

153
00:08:17.120 --> 00:08:17.720
In the end.

154
00:08:18.080 --> 00:08:23.660
And to do this, we need now, of course, to connect our LDA object to our data set.

155
00:08:23.960 --> 00:08:28.580
But once again, separately, the training set and the test set and two connected.

156
00:08:28.670 --> 00:08:35.450
Well, of course, we need to apply the fit transform method to the training set and then only to transform

157
00:08:35.450 --> 00:08:36.770
method on the test.

158
00:08:36.920 --> 00:08:39.050
That's for the exact same reason as before.

159
00:08:39.350 --> 00:08:42.500
It is to avoid information leakage from the test set.

160
00:08:42.740 --> 00:08:43.010
All right.

161
00:08:43.010 --> 00:08:43.640
So let's do this.

162
00:08:43.670 --> 00:08:45.140
That's our next step here.

163
00:08:45.440 --> 00:08:51.200
So we're gonna take first X train, right, which we're going to update to become the new X train.

164
00:08:51.320 --> 00:08:54.620
After we reapply this LDA feature extraction technique.

165
00:08:54.980 --> 00:09:01.730
And to do this well, we need to take, of course, our LDA object from which we're gonna call the fit

166
00:09:02.300 --> 00:09:06.530
transform method, which will take as input.

167
00:09:06.770 --> 00:09:07.590
Well, here.

168
00:09:07.610 --> 00:09:08.330
Be careful.

169
00:09:08.420 --> 00:09:14.390
It's not gonna be exactly the same input as before because, you know, with PCI a the fit transfer

170
00:09:14.390 --> 00:09:22.190
method took only extranet as input because it only need the features to apply this PCI a dimensionality

171
00:09:22.190 --> 00:09:23.030
reduction technique.

172
00:09:23.270 --> 00:09:27.110
But LDA is actually different in order to apply the technique.

173
00:09:27.410 --> 00:09:33.800
It needs not only the features, but also the dependent variable rate dependent variable is a required

174
00:09:33.860 --> 00:09:36.510
element inside the equation of LTA.

175
00:09:36.740 --> 00:09:43.640
And therefore, here in the fit transfer method, we need to input not only X train the old version

176
00:09:43.640 --> 00:09:48.560
of extreme before we apply LDA and Y train.

177
00:09:49.030 --> 00:09:49.290
All right.

178
00:09:49.340 --> 00:09:50.960
So be very careful with this.

179
00:09:50.960 --> 00:09:54.040
Whether you choose to apply LDA or BCA for PCI.

180
00:09:54.080 --> 00:09:56.450
You only have to input extra in and for LDA.

181
00:09:56.480 --> 00:10:00.320
You have to input both the features X strain and the depending foible Y train.

182
00:10:01.130 --> 00:10:01.580
All right.

183
00:10:01.760 --> 00:10:02.570
Final step.

184
00:10:02.750 --> 00:10:10.640
Well, now that we have an LDA feature extractor object fitted to the training set, well, we can apply

185
00:10:10.640 --> 00:10:14.750
it to the test set by only calling the transform method.

186
00:10:14.930 --> 00:10:15.200
Right.

187
00:10:15.290 --> 00:10:21.170
It doesn't make sense to fit it again to the test set because the test set is supposed to be new data

188
00:10:21.410 --> 00:10:23.990
on which we deploy our moral like in production.

189
00:10:24.260 --> 00:10:27.470
Therefore, we must only apply the transform method here.

190
00:10:27.770 --> 00:10:35.060
And therefore I'm updating our X test variable the following way by first calling our LDA object from

191
00:10:35.060 --> 00:10:40.040
which we're only gonna call the trend form method.

192
00:10:40.490 --> 00:10:45.260
And now, according to you, does it need to take only X test as input or.

193
00:10:45.720 --> 00:10:46.830
Test and white test.

194
00:10:47.070 --> 00:10:52.620
Well, obviously, it only need to take X test because we're not supposed to have Y tests.

195
00:10:52.620 --> 00:10:56.710
You know, X test is like new data on which we're going to deploy our model.

196
00:10:56.910 --> 00:11:01.700
Then we'll get our predictions and y bread, you know, and we'll compare Wipro to whitest.

197
00:11:02.070 --> 00:11:06.990
But we're not supposed to have white tests because whites are the real result that contained the hidden

198
00:11:06.990 --> 00:11:07.380
truth.

199
00:11:07.400 --> 00:11:08.550
You know, the ground truth.

200
00:11:08.850 --> 00:11:12.150
So, of course, here we only need to apply X test.

201
00:11:12.480 --> 00:11:18.030
And the reason why we could enter waitering here is because indeed, we're supposed to get the ground

202
00:11:18.030 --> 00:11:19.290
truth of the training set.

203
00:11:19.500 --> 00:11:22.920
Otherwise, we wouldn't be able to train our machine learning model.

204
00:11:23.520 --> 00:11:23.820
All right.

205
00:11:23.850 --> 00:11:25.610
So access and there we go.

206
00:11:26.160 --> 00:11:32.340
Not only the implementation of LDA is over, but also the whole implementation is over as well.

207
00:11:33.120 --> 00:11:35.170
So now we're going to do this run.

208
00:11:35.310 --> 00:11:40.740
All now that we, you know, uploaded the data, sent into the notebooks, we are 100 percent ready.

209
00:11:41.100 --> 00:11:45.810
And let's just remind what we want to improve compared to previously.

210
00:11:46.140 --> 00:11:52.530
Well, you know, in the principal component analysis implementation we had when obtaining the computer

211
00:11:52.530 --> 00:11:58.170
matrix on the one incorrect prediction resulting in having an accuracy of 97 percent.

212
00:11:58.530 --> 00:12:02.610
And in the test that results, which are the most interesting ones.

213
00:12:02.790 --> 00:12:07.500
Well, we had indeed an almost perfect separation of the three classes.

214
00:12:07.770 --> 00:12:12.780
And now we're gonna see if with our new feature extracted from LDA.

215
00:12:13.050 --> 00:12:19.320
Well, we can get a perfect separation of the classes and therefore 100 percent accuracy.

216
00:12:19.950 --> 00:12:20.670
Are you ready?

217
00:12:20.880 --> 00:12:21.660
Let's do this.

218
00:12:22.020 --> 00:12:25.560
Three, two, one, go, run.

219
00:12:25.740 --> 00:12:27.720
Also now all the cells are running and there we go.

220
00:12:27.780 --> 00:12:29.280
Oh, there we go.

221
00:12:29.400 --> 00:12:32.820
We just had a 100 percent accuracy.

222
00:12:33.060 --> 00:12:39.980
So, in other words, are logistic regression model Westerley able to classify perfectly our three classes

223
00:12:40.110 --> 00:12:42.180
by separating them separately?

224
00:12:42.180 --> 00:12:44.320
And that's exactly what we're gonna see.

225
00:12:44.480 --> 00:12:48.270
And, you know, the test set results because indeed.

226
00:12:48.390 --> 00:12:50.580
Well, we almost had an incorrect one here.

227
00:12:50.610 --> 00:12:58.290
But as we see the real Wein's, you know, which are all the points here, red, green and blue fall

228
00:12:58.350 --> 00:13:03.630
into the right prediction regions, that red prediction region where our models predict that the wine

229
00:13:03.630 --> 00:13:05.490
belongs to customer segment number one.

230
00:13:06.060 --> 00:13:10.590
Then this one where our model predicts that the wines belong to customer segment number two.

231
00:13:10.890 --> 00:13:16.890
And finally, this Brigden region where our model predicts that the wine should be recommended to customer

232
00:13:16.890 --> 00:13:18.360
segment number three.

233
00:13:19.050 --> 00:13:25.830
And thanks to these new extracted features, no LG one and LG two, well, this time we have a perfect

234
00:13:26.070 --> 00:13:27.180
class separator.

235
00:13:27.300 --> 00:13:29.610
In other words, we have a perfect classifier.

236
00:13:30.150 --> 00:13:34.480
And if you're wondering how did LDA manage to separate perfectly the classes?

237
00:13:34.710 --> 00:13:40.530
Whereas, you know, in PPA, we could see that it was very difficult to separate, you know, the wines

238
00:13:40.530 --> 00:13:41.210
of the test set.

239
00:13:41.370 --> 00:13:42.570
You know, especially this one.

240
00:13:42.570 --> 00:13:45.350
This one falls in the middle of the red wines.

241
00:13:45.630 --> 00:13:48.390
Well, that's because to extract these features are different.

242
00:13:48.420 --> 00:13:51.150
You know, they're not the same as P.c one and pursue two.

243
00:13:51.660 --> 00:13:58.050
There are, you know, in some other dimensions in which, well, this time it is possible to separate

244
00:13:58.050 --> 00:13:59.190
perfectly the classes.

245
00:13:59.460 --> 00:14:00.870
And that's why this time it works.

246
00:14:00.990 --> 00:14:03.570
We are, in other words, in another dimension.

247
00:14:04.120 --> 00:14:04.400
OK.

248
00:14:04.590 --> 00:14:09.500
So I guess now we don't have much of a challenge because it's impossible to beat this.

249
00:14:09.530 --> 00:14:10.500
This is just perfect.

250
00:14:10.770 --> 00:14:13.420
I remind that I did not make this data set.

251
00:14:13.470 --> 00:14:17.010
You know, it's a dataset taken from the UCI email registry.

252
00:14:17.250 --> 00:14:19.490
So, you know, it's very close to a real world data set.

253
00:14:19.800 --> 00:14:20.300
But there you go.

254
00:14:20.310 --> 00:14:23.880
That shows the power of this dimensionality reduction technique.

255
00:14:24.150 --> 00:14:25.710
Linear discriminant analysis.

256
00:14:26.280 --> 00:14:31.230
So now we're going to move on to the next practical activity, the next section on this time.

257
00:14:31.360 --> 00:14:32.870
Colonel, T.S.A..

258
00:14:33.150 --> 00:14:39.540
And we can just hope that we'll get, you know, at least as good results, SPCA or some as good results

259
00:14:39.660 --> 00:14:40.650
as LDA.

260
00:14:41.100 --> 00:14:45.450
In other words, let's hope that we get maximum one incorrect prediction.

261
00:14:46.200 --> 00:14:51.560
So I look forward to seeing you in this next section to implement Colonel BCA and until then, enjoy

262
00:14:51.570 --> 00:14:52.210
machine learning.
