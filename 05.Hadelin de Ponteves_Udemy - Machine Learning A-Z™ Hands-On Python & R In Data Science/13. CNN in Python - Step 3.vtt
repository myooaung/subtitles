WEBVTT
1
00:00:00.180 --> 00:00:05.490
Hello, my friends, and welcome to part two of this implementation, where we're going to build together

2
00:00:05.580 --> 00:00:11.490
the convolutional neural network and more specifically, the whole architecture of this new artificial

3
00:00:11.490 --> 00:00:12.180
neural network.

4
00:00:12.810 --> 00:00:13.200
All right.

5
00:00:13.260 --> 00:00:19.260
So it's actually going to start the same as with our artificial Newville network, because convolutional

6
00:00:19.260 --> 00:00:22.020
neural network is still a sequence of layers.

7
00:00:22.350 --> 00:00:28.320
Therefore, we're going to initialize here our CNN with the same class, which remember is sequential

8
00:00:28.320 --> 00:00:28.690
class.

9
00:00:29.070 --> 00:00:35.280
And that's a first step where we're going to not only call that sequential class, but mostly create

10
00:00:35.280 --> 00:00:41.790
that see an and variable which will represent exactly this convolutional neural network.

11
00:00:42.270 --> 00:00:49.410
And the CNN voidable will be created once again as an instance of that sequential class, which allows

12
00:00:49.440 --> 00:00:53.640
to create an artificial neural network as a sequence of layers.

13
00:00:54.210 --> 00:00:57.160
So now remember how we can get access to this class.

14
00:00:57.260 --> 00:01:02.700
Well, first, we need to call center flow, which has a sugar T.F. from which we're going to call the

15
00:01:02.700 --> 00:01:08.550
Keris Library, from which we're going to get access to the models module.

16
00:01:08.760 --> 00:01:12.540
And from which we're going to call that sequential class.

17
00:01:12.570 --> 00:01:14.700
So exactly the same as before.

18
00:01:14.710 --> 00:01:21.960
And this initializes indeed are CNN has a sequence of layers as opposed to a computational graph.

19
00:01:22.740 --> 00:01:23.190
Great.

20
00:01:23.460 --> 00:01:30.780
And now, step by step, we're going to use this, you know, add method to add different layers, whether

21
00:01:30.780 --> 00:01:36.060
they are convolutional layers or fully connected layers and India and the outer layer.

22
00:01:36.480 --> 00:01:41.220
So we're going to use successively now the add method, starting with tip one convolution.

23
00:01:41.580 --> 00:01:43.380
And so now in a new code cell.

24
00:01:43.380 --> 00:01:49.530
Well, first we'll take our CNN object or convolutional neural network from which we're gonna call,

25
00:01:49.530 --> 00:01:55.260
of course, the ADD method to add our very first convolutional layer.

26
00:01:55.890 --> 00:01:56.130
All right.

27
00:01:56.160 --> 00:01:59.340
So what I want right now is, of course, a convolutional layer.

28
00:01:59.400 --> 00:02:05.690
And once again, this convolutional layer will be an object of a certain class, which is the conv to

29
00:02:05.690 --> 00:02:06.720
D class.

30
00:02:07.080 --> 00:02:11.820
And this class, just as the dance class, which allows to build a fully connected layer, belongs to

31
00:02:11.820 --> 00:02:15.390
the same module, which is a layer's module from the Keris library.

32
00:02:15.570 --> 00:02:17.050
From this time tensor flow.

33
00:02:17.190 --> 00:02:18.360
Since Denser Flow 2.0.

34
00:02:18.810 --> 00:02:24.090
So for us, we're going to get Tenzer flow from which we're gonna get access to the Keris library,

35
00:02:24.420 --> 00:02:27.060
from which we're gonna get access to the Layers module.

36
00:02:27.570 --> 00:02:28.290
And there we go.

37
00:02:28.290 --> 00:02:32.050
From which we're gonna call that conv to D..

38
00:02:32.220 --> 00:02:32.880
There we go.

39
00:02:33.090 --> 00:02:35.280
Class Konsta, the class.

40
00:02:35.310 --> 00:02:37.980
Then we add some parenthesis, of course, because that's a class.

41
00:02:38.210 --> 00:02:44.400
And inside we have to input three important parameters, which are the filters.

42
00:02:44.430 --> 00:02:49.530
Which basically is the number of feature detectors you want to apply to your images, you know, to

43
00:02:49.530 --> 00:02:50.460
detect features.

44
00:02:50.760 --> 00:02:53.380
And these are indeed also called filters or Col's.

45
00:02:53.760 --> 00:02:54.150
Right.

46
00:02:54.270 --> 00:02:58.260
If we scroll down here, actually have all the info of these parameters.

47
00:02:58.560 --> 00:02:58.830
Right.

48
00:02:58.890 --> 00:03:02.970
Arguments, filters, the number of output filters in the convolution.

49
00:03:03.180 --> 00:03:05.430
Well, basically that the feature detectors.

50
00:03:05.820 --> 00:03:07.440
So that's our first argument.

51
00:03:07.770 --> 00:03:10.490
Then we will also specify a kernel size.

52
00:03:10.500 --> 00:03:13.950
And actually I've prepared some slides that I made to show use.

53
00:03:14.040 --> 00:03:20.130
Basically, this is the filter and this filters parameter will tell us how many feature detectors we

54
00:03:20.130 --> 00:03:21.660
want, how many filters we want.

55
00:03:22.080 --> 00:03:27.600
And the kernel size is exactly, you know, the size of that feature detector, meaning the number of

56
00:03:27.870 --> 00:03:29.970
rows, which is also the number of columns.

57
00:03:30.000 --> 00:03:32.810
Because this is usually a squared array, you know.

58
00:03:33.000 --> 00:03:37.050
So, for example, if we choose a size three, which is actually the size we're gonna choose.

59
00:03:37.500 --> 00:03:41.040
Well, that means that the size of our feature detector will be three by three.

60
00:03:41.240 --> 00:03:41.480
OK.

61
00:03:41.610 --> 00:03:42.360
As simple as that.

62
00:03:42.960 --> 00:03:46.400
And then we have some other arguments, but no worries.

63
00:03:46.410 --> 00:03:50.130
We'll just keep the default value for the rest of them because that will be just fine.

64
00:03:50.430 --> 00:03:55.770
However, of course, we're not going to keep the default value of that activation parameter, which

65
00:03:55.770 --> 00:03:57.900
corresponds, of course, to the activation function.

66
00:03:58.230 --> 00:04:03.090
Because indeed, you know, on a general rule, as long as we haven't reached the output layer, we

67
00:04:03.180 --> 00:04:06.330
rather want to get a rectifier activation function.

68
00:04:06.450 --> 00:04:11.730
And therefore, for this one activation, we're gonna choose once again the RELU parameters name, which

69
00:04:11.730 --> 00:04:13.920
corresponds to the rectify activation function.

70
00:04:14.640 --> 00:04:19.680
And finally, we have one less argument we have to input in, which is not displayed here, because

71
00:04:19.710 --> 00:04:23.010
that's hidden by this one, but which you can actually see here.

72
00:04:23.230 --> 00:04:24.510
That's the input shape.

73
00:04:24.840 --> 00:04:30.310
When you add your very first layer, whether it is a convolutional layer or a dense layer.

74
00:04:30.450 --> 00:04:34.470
Well, you have to specify the input shape of your inputs.

75
00:04:34.740 --> 00:04:39.990
And here, since we were working with colored images, therefore in three dimensions, you know, corresponding

76
00:04:39.990 --> 00:04:42.390
to the RTP code of colors.

77
00:04:42.600 --> 00:04:48.220
And since we actually resized in part one, are images down to 64 by 64.

78
00:04:48.480 --> 00:04:53.820
Well, the input shape of our images will be 64, 64 and three.

79
00:04:54.180 --> 00:04:58.170
And if we were working with black and white images here, we would have one instead of three.

80
00:04:58.410 --> 00:04:59.610
But we're working with.

81
00:04:59.970 --> 00:05:04.680
Images, and therefore, that's what will be our input shape, 64, 64 and three.

82
00:05:05.220 --> 00:05:05.520
All right.

83
00:05:05.550 --> 00:05:08.610
And that's the essential parameters that we have to enter.

84
00:05:08.700 --> 00:05:09.510
So there we go.

85
00:05:09.540 --> 00:05:13.560
Let's enter them, starting with the filters parameter.

86
00:05:13.920 --> 00:05:14.550
There we go.

87
00:05:14.910 --> 00:05:17.970
And now the question is, well, how many feature detectors we want?

88
00:05:18.270 --> 00:05:21.150
Well, we're just gonna go for a classic architecture.

89
00:05:21.150 --> 00:05:24.890
You can find many architectures of convolutional on the one that works online.

90
00:05:25.200 --> 00:05:30.210
We're gonna choose a classic one, which I, of course, tried on are images and which turns out to

91
00:05:30.210 --> 00:05:30.990
work very well.

92
00:05:31.350 --> 00:05:38.460
And that architecture consists of having 32 filters in that first convolutional layer and then another

93
00:05:38.460 --> 00:05:41.370
32 filters in that second convolutional layer.

94
00:05:41.670 --> 00:05:41.940
All right.

95
00:05:41.970 --> 00:05:43.170
So let's just choose here.

96
00:05:43.230 --> 00:05:49.500
Filters equals 32 and feel free, of course, to choose another parameter value.

97
00:05:49.740 --> 00:05:51.810
I remind that this is an artist job.

98
00:05:51.840 --> 00:05:57.750
You're free to experiment any architecture you like, and you might probably get some better results

99
00:05:57.960 --> 00:05:59.680
than the ones we're about to get India.

100
00:06:00.480 --> 00:06:00.650
All right.

101
00:06:00.660 --> 00:06:03.390
So filters equal 32 that for the first parameter.

102
00:06:03.660 --> 00:06:10.320
And now for the second parameter, as we said, it is going to be the kernel underscore size, this

103
00:06:10.320 --> 00:06:10.620
one.

104
00:06:10.650 --> 00:06:13.710
And as we said, we want a three by three dimensions.

105
00:06:13.770 --> 00:06:15.800
And we only have to specify three.

106
00:06:16.590 --> 00:06:21.570
And then, as we said, we want to make sure that we have a rectifier activation function.

107
00:06:21.600 --> 00:06:26.160
So here for this new activation parameter, we're going to choose in quotes.

108
00:06:26.310 --> 00:06:29.970
Well, the RELU activation function exactly the same as before.

109
00:06:30.300 --> 00:06:38.760
And finally, for that last parameter, we have to specify the input shape of our images, which we

110
00:06:38.760 --> 00:06:41.280
have to enter in a pair of square brackets.

111
00:06:41.610 --> 00:06:42.450
And there we go.

112
00:06:42.650 --> 00:06:48.570
We resized our images to have the 64 by 64 dimensions.

113
00:06:48.930 --> 00:06:53.640
And since we're working with Godet images, we have to add here three instead of one.

114
00:06:53.880 --> 00:06:54.240
All right.

115
00:06:54.630 --> 00:06:55.860
That's our input shape.

116
00:06:56.070 --> 00:06:56.820
And that's it.

117
00:06:57.120 --> 00:07:04.080
This add the convolutional layer to your CNN so far initialized as a sequence of layers.

118
00:07:04.770 --> 00:07:05.160
Good.

119
00:07:05.430 --> 00:07:08.860
So now we can move on to step two puling.

120
00:07:09.120 --> 00:07:10.320
So let's create a new coattail.

121
00:07:10.530 --> 00:07:13.770
And this consists, of course, of applying, puling.

122
00:07:13.770 --> 00:07:18.780
And more specifically, we're going to apply max puling, just as you saw in the intuition lectures.

123
00:07:19.530 --> 00:07:19.800
All right.

124
00:07:19.830 --> 00:07:26.820
So once again, we need to take our seat and object from which we're gonna call now a new method.

125
00:07:27.000 --> 00:07:28.210
But is that really new?

126
00:07:28.410 --> 00:07:30.960
According to you, do we have to call the add method again?

127
00:07:31.350 --> 00:07:32.610
Well, yes, we do.

128
00:07:32.730 --> 00:07:37.050
Actually, we're adding the pooling layers to our convolutional layer.

129
00:07:37.230 --> 00:07:39.780
You know, as the next step in the sequence of layers.

130
00:07:40.140 --> 00:07:40.850
So there we go.

131
00:07:40.860 --> 00:07:42.690
We call that add method once again.

132
00:07:42.900 --> 00:07:43.800
And then inside.

133
00:07:43.860 --> 00:07:50.010
Well, that max pulling layer will once again be created as an object, you know, as an instance of

134
00:07:50.010 --> 00:07:53.730
a certain class, which is called the max pool to the class.

135
00:07:54.060 --> 00:07:58.170
And this class belongs to the same module as before the layers module.

136
00:07:58.440 --> 00:07:59.000
So there we go.

137
00:07:59.010 --> 00:08:00.900
We're actually going to take this.

138
00:08:01.350 --> 00:08:05.700
You know, I'm going to copy this, paste that right here.

139
00:08:06.030 --> 00:08:12.150
And then instead of taking the comes to the class, we're going to take this time the max pool to the

140
00:08:12.150 --> 00:08:12.630
class.

141
00:08:12.870 --> 00:08:13.380
Perfect.

142
00:08:13.900 --> 00:08:18.420
And inside, we have to specify two essential arguments, which are the pool size.

143
00:08:18.450 --> 00:08:20.730
So let me show you what this corresponds to.

144
00:08:21.040 --> 00:08:25.680
So this is a whole convolutional process, you know, with the feature detector applied to the input

145
00:08:25.740 --> 00:08:29.490
image resulting in having that created feature map.

146
00:08:29.970 --> 00:08:30.450
All right.

147
00:08:30.630 --> 00:08:31.620
So that's all this.

148
00:08:31.650 --> 00:08:32.400
And there we go.

149
00:08:32.490 --> 00:08:33.750
Now we move on to step two.

150
00:08:34.020 --> 00:08:39.330
So we have our feature map, which is the result of our previous convolution, and we apply, Max,

151
00:08:39.330 --> 00:08:41.790
pulling to obtain this pooled feature map.

152
00:08:41.820 --> 00:08:46.620
You know, that same feature map of the convolutional layer after we apply Max pulling.

153
00:08:47.070 --> 00:08:53.430
And when we do this, you know, we have this little frame here that will get the maximum pixel of the

154
00:08:53.430 --> 00:08:56.310
four cells inside, you know, the four pixels inside.

155
00:08:56.760 --> 00:09:03.210
And that pool size parameter, which we're about to enter here, is exactly the size of that frame,

156
00:09:03.270 --> 00:09:05.040
you know, which is once again a square.

157
00:09:05.040 --> 00:09:08.700
So we only have to specify the width or, you know, the height.

158
00:09:08.760 --> 00:09:11.250
So basically here in this example, too.

159
00:09:11.660 --> 00:09:11.940
Okay.

160
00:09:12.430 --> 00:09:16.800
And, well, you know, speaking of two, that's exactly the pool size which we're going to choose and

161
00:09:16.800 --> 00:09:21.040
which is the one I recommend to work with when applying Max Pulley.

162
00:09:21.680 --> 00:09:23.160
Okay, then let's see.

163
00:09:23.310 --> 00:09:26.450
The other parameter that is important is the strides.

164
00:09:26.780 --> 00:09:26.950
OK.

165
00:09:27.030 --> 00:09:27.810
So interesting.

166
00:09:27.810 --> 00:09:30.150
Let's have a look at our slides once again.

167
00:09:30.480 --> 00:09:38.040
So I'm going to move to the next slide here and let's see by which number of pixels is this frame shifted

168
00:09:38.040 --> 00:09:38.610
to the right.

169
00:09:38.970 --> 00:09:39.600
Let's see.

170
00:09:39.840 --> 00:09:40.140
All right.

171
00:09:40.200 --> 00:09:42.840
So it is actually shifted by two pixels.

172
00:09:42.870 --> 00:09:49.590
You know, instead of going from this frame to that frame, which would be a slight of one.

173
00:09:49.890 --> 00:09:53.040
Well, we went directly from this frame to that one.

174
00:09:53.300 --> 00:09:53.670
OK.

175
00:09:53.910 --> 00:09:58.590
And this makes sense when applying, Max pulling because we only want to get the maximum of each square

176
00:09:58.590 --> 00:09:59.490
here of pixels.

177
00:09:59.690 --> 00:09:59.930
Right.

178
00:10:00.290 --> 00:10:07.390
So the recommended slide once again, is to they indeed were sliding two by two pixels.

179
00:10:07.670 --> 00:10:12.500
And when we reach, you know, that border of the future map here, you know, with extra empty cells

180
00:10:12.500 --> 00:10:15.170
here, well, you can actually choose two different ways.

181
00:10:15.230 --> 00:10:18.440
And that corresponds to that parameter, which is the bedding.

182
00:10:18.770 --> 00:10:20.030
The default value is valid.

183
00:10:20.090 --> 00:10:21.740
But the other value is same.

184
00:10:22.110 --> 00:10:26.570
And, well, the differences is that, you know, with a valid bedding, you will just ignore the other

185
00:10:26.570 --> 00:10:27.560
two cells here.

186
00:10:27.770 --> 00:10:33.170
And with the same bedding, you will just add an extra column was just fake pixels that are equal to

187
00:10:33.170 --> 00:10:33.530
zero.

188
00:10:34.010 --> 00:10:36.010
But don't worry too much about this bedding.

189
00:10:36.170 --> 00:10:37.790
I tried actually with the two values.

190
00:10:37.820 --> 00:10:40.310
This doesn't change anything in the final result.

191
00:10:40.640 --> 00:10:43.760
And so I just recommend to keep the default value.

192
00:10:44.000 --> 00:10:45.500
But the slide is important.

193
00:10:45.530 --> 00:10:47.420
And now we're sliding two by two.

194
00:10:47.780 --> 00:10:48.090
OK.

195
00:10:48.530 --> 00:10:49.610
So let's do this.

196
00:10:49.670 --> 00:10:53.420
Let's enter our parameters for that max pool to the class.

197
00:10:53.690 --> 00:10:56.810
The first one is, as we said, the pool size.

198
00:10:56.840 --> 00:10:57.530
There we go.

199
00:10:57.590 --> 00:11:03.650
And we want to have exactly as in this slide, a two by two frame, which we only have to specify with

200
00:11:03.800 --> 00:11:05.450
the parameter input, too.

201
00:11:06.080 --> 00:11:09.770
And then the second argument is the strides, you know, the essential one.

202
00:11:10.130 --> 00:11:15.320
And we want to shift that frame every two pixels and therefore we're going to choose traits of two.

203
00:11:15.950 --> 00:11:16.310
All right.

204
00:11:16.400 --> 00:11:18.140
And that apply successfully.

205
00:11:18.290 --> 00:11:20.330
Max, pulling as simple as that.

206
00:11:20.360 --> 00:11:23.240
And you can just copy paste this line wherever you want to apply.

207
00:11:23.600 --> 00:11:27.020
Max, pulling to your CNN and actually speaking of which.

208
00:11:27.260 --> 00:11:32.360
Well, you know, now we want to add a second convolutional layer and admire how I'm going to do this

209
00:11:32.420 --> 00:11:33.560
so efficiently.

210
00:11:33.740 --> 00:11:35.850
So first, I'm going to create a new coat cell.

211
00:11:36.020 --> 00:11:38.600
Then I'm going to take that cell.

212
00:11:38.810 --> 00:11:42.200
Copy it, then paste it right here.

213
00:11:42.530 --> 00:11:43.480
Then for the pooling.

214
00:11:43.520 --> 00:11:44.510
Take that cell.

215
00:11:44.840 --> 00:11:45.500
Copy it.

216
00:11:45.920 --> 00:11:49.010
Then pasted right below.

217
00:11:49.280 --> 00:11:52.100
And now, according to you, do we have to change something here?

218
00:11:52.130 --> 00:11:53.480
Or can we leave it this way?

219
00:11:53.900 --> 00:11:55.790
Well, we can't leave it this way, actually.

220
00:11:55.790 --> 00:12:02.390
We just need to remove that input shape parameter because this one is entered only when you add your

221
00:12:02.390 --> 00:12:08.600
very first layer, you know, to automatically connect that first layer to the input layer, which automatically

222
00:12:08.630 --> 00:12:09.620
adds the input layer.

223
00:12:09.950 --> 00:12:13.280
But here we are already with our second convolutional layer.

224
00:12:13.550 --> 00:12:14.480
So that's all good.

225
00:12:14.660 --> 00:12:16.220
We can just remove it.

226
00:12:17.650 --> 00:12:18.040
Good.

227
00:12:18.230 --> 00:12:18.800
And now.

228
00:12:18.860 --> 00:12:19.430
Perfect.

229
00:12:19.640 --> 00:12:26.630
All this add a second convolutional layer with Max pulling a plate and we can now move on to step three

230
00:12:26.990 --> 00:12:32.990
flattening, which will consist of, of course, flattening the result of all these convolutions and

231
00:12:33.010 --> 00:12:40.280
pullings into a one dimensional vector, which will become the input of a future fully connected new

232
00:12:40.280 --> 00:12:40.730
network.

233
00:12:40.820 --> 00:12:42.770
Just as we built in the previous section.

234
00:12:43.460 --> 00:12:43.700
All right.

235
00:12:43.700 --> 00:12:44.300
So let's do this.

236
00:12:44.330 --> 00:12:50.560
Let's implement that step three flattening and well, as usual, you know, we need to take our seat

237
00:12:50.660 --> 00:12:57.350
and an object from which we're gonna call the ADD method once again, because the way we're gonna create

238
00:12:57.410 --> 00:13:03.590
that flattening layer is once again by creating an instance of a certain class and that certain class

239
00:13:03.680 --> 00:13:05.270
is the flattened class.

240
00:13:05.540 --> 00:13:10.790
And, you know, carers will automatically understand that it is the result of all these convolutions

241
00:13:10.790 --> 00:13:14.840
and pullings that will be flattened into this one dimensional vector.

242
00:13:15.110 --> 00:13:19.950
So the only thing we have to do is just to specify that we want to apply flattening and to do this.

243
00:13:19.970 --> 00:13:26.890
We need to call once again the layers module by the Keris library, by sense of flow from which we're

244
00:13:26.900 --> 00:13:31.150
going to call this time that flatten class.

245
00:13:31.370 --> 00:13:36.050
And good news, this class actually doesn't need to take anything as parameters.

246
00:13:36.230 --> 00:13:36.490
All right.

247
00:13:36.500 --> 00:13:43.820
So this simply implements step three flattening and we can directly move on to step four for connection.

248
00:13:44.570 --> 00:13:44.900
All right.

249
00:13:44.930 --> 00:13:46.340
So now it's your turn.

250
00:13:46.370 --> 00:13:47.990
You can actually do it yourself.

251
00:13:48.050 --> 00:13:53.750
So I would like you to please press buzz on the video, because we are in the exact same situation as

252
00:13:53.750 --> 00:13:56.210
before, you know, building a fully connected neural network.

253
00:13:56.510 --> 00:13:58.280
So you know exactly how to do this.

254
00:13:58.520 --> 00:14:05.690
I would like you to add a new fully connected layer to that flatten layer, which is now nothing else

255
00:14:05.780 --> 00:14:11.030
than a one dimensional vector that will become the input of a fully connected neural network.

256
00:14:11.390 --> 00:14:15.020
So do this first and we'll implement the solution in a few seconds.

257
00:14:16.610 --> 00:14:16.970
Good.

258
00:14:17.180 --> 00:14:17.420
All right.

259
00:14:17.420 --> 00:14:18.110
Let's do this.

260
00:14:18.260 --> 00:14:20.120
So you know exactly how to do this for us.

261
00:14:20.150 --> 00:14:21.770
We create a new code cell.

262
00:14:22.070 --> 00:14:28.280
Then we take once again our C and N neural network from which we're gonna call the ADD method.

263
00:14:28.640 --> 00:14:34.280
Because now we're about add a new layer, which is a fully connected layer and which belongs still to

264
00:14:34.280 --> 00:14:37.550
that T.F. Keris layers, which I've already copied.

265
00:14:37.820 --> 00:14:42.890
So I can just base that here and this time take that dance class.

266
00:14:42.920 --> 00:14:43.580
Perfect.

267
00:14:43.910 --> 00:14:46.130
And now in to some new parentheses.

268
00:14:46.160 --> 00:14:47.870
I'm sure you figured out what.

269
00:14:47.990 --> 00:14:49.190
Enter as parameters.

270
00:14:49.430 --> 00:14:52.880
First, remember, we have the unit, which is number of hidden neurons.

271
00:14:52.910 --> 00:14:55.790
You want to have into this new fully connected layer.

272
00:14:56.210 --> 00:14:59.550
And since now we're actually dealing with a more complex problem.

273
00:14:59.620 --> 00:15:03.330
You know, computer vision is way more complex than data mining.

274
00:15:03.350 --> 00:15:04.880
Classic data set as before.

275
00:15:05.270 --> 00:15:08.420
Well, we're going to choose a larger number of hidden neurons.

276
00:15:08.510 --> 00:15:11.150
We're gonna choose one hundred twenty eight hidden neurons.

277
00:15:11.480 --> 00:15:16.430
However, if you picked the same number as before, you know, in the previous section with our CNN,

278
00:15:16.850 --> 00:15:20.450
well, I'm sure that story fun and I'm sure you'll get great results afterwards.

279
00:15:20.630 --> 00:15:24.530
But we might get a better accuracy in the end with a larger number of neurons.

280
00:15:24.770 --> 00:15:26.150
And therefore, let's choose here.

281
00:15:26.450 --> 00:15:30.030
Units equals one hundred and twenty eight.

282
00:15:30.530 --> 00:15:31.060
Great.

283
00:15:31.280 --> 00:15:34.880
And now second argument, that, of course, the activation function.

284
00:15:35.090 --> 00:15:40.160
And once again, my recommendation is that as long as you haven't reached the final output layer, I

285
00:15:40.160 --> 00:15:43.190
recommend to use a rectifier activation function.

286
00:15:43.580 --> 00:15:48.200
And that's exactly what we're going to specify here with this activation parameter.

287
00:15:48.650 --> 00:15:53.110
And remember, the code name for the rectifier activation function is real.

288
00:15:53.140 --> 00:15:55.580
You are right and perfect.

289
00:15:55.760 --> 00:15:58.580
This add are fully connected layer.

290
00:15:59.150 --> 00:16:01.700
And finally, the step five.

291
00:16:01.880 --> 00:16:05.000
You see how we're being so efficient here in that step five.

292
00:16:05.020 --> 00:16:10.760
We need to add the final output layer, which will still be fully connected to that previous hidden

293
00:16:10.760 --> 00:16:11.030
layer.

294
00:16:11.060 --> 00:16:13.850
Therefore, we're gonna call once again the dance class.

295
00:16:14.180 --> 00:16:21.320
And therefore, in a new code cell, I'm going to actually, you know, take all this, then base that

296
00:16:21.320 --> 00:16:27.230
here and inside will only have to replace two things, which are the values of these two parameters,

297
00:16:27.500 --> 00:16:33.410
because indeed, the number of units, you know, the number of neurons in the final output layer is

298
00:16:33.410 --> 00:16:35.220
definitely not 128.

299
00:16:35.570 --> 00:16:37.130
But you tell me.

300
00:16:37.580 --> 00:16:39.440
Well, it's actually, of course, one.

301
00:16:39.560 --> 00:16:40.970
It's exactly the same as before.

302
00:16:40.970 --> 00:16:43.100
We're doing binary classification.

303
00:16:43.400 --> 00:16:50.060
Therefore, we only need one neuron to encode that binary class zero or one or, you know, cat or dog.

304
00:16:50.390 --> 00:16:52.230
And therefore, we only need one neuron.

305
00:16:52.730 --> 00:16:57.860
And for the activation function, remember that for the output layer, it is not recommended to have

306
00:16:57.860 --> 00:17:03.830
a rectifier activation function, but rather a sigmoid activation function.

307
00:17:04.280 --> 00:17:06.830
And that's because, of course, we're doing binary classification.

308
00:17:06.980 --> 00:17:12.830
Otherwise, if we were doing multiclass classification, we would have, remember, a softmax activation

309
00:17:12.830 --> 00:17:13.340
function.

310
00:17:13.670 --> 00:17:14.390
But there we go.

311
00:17:14.450 --> 00:17:19.340
This will add a great output layer, which will optimize the result in the end.

312
00:17:20.060 --> 00:17:25.820
And now I have to say once again, a huge congratulations to you, because there you go.

313
00:17:25.910 --> 00:17:28.640
You just build a convolutional neural network.

314
00:17:28.880 --> 00:17:30.230
We're done with part two.

315
00:17:30.530 --> 00:17:33.770
We can already move on to part three, training the CNN.

316
00:17:34.130 --> 00:17:39.740
And this will consist, of course, making this brain, you know, this artificial brain with some eyes.

317
00:17:40.040 --> 00:17:43.940
Pretty smart to recognize cats or dogs in images.

318
00:17:44.450 --> 00:17:46.610
So now we deserve a good break.

319
00:17:46.760 --> 00:17:50.600
So make sure to recharge in good energy for the next Turrill.

320
00:17:50.900 --> 00:17:55.880
And as soon as you're ready, we'll let us mash together Part three, training the CNN.
