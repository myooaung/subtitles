WEBVTT
1
00:00:00.330 --> 00:00:01.190
Okay my friends.

2
00:00:01.200 --> 00:00:02.840
I'm ready to finish this.

3
00:00:02.850 --> 00:00:08.100
It's actually very simple now and I'm sure that you're confident that the solution I'm about to give

4
00:00:08.100 --> 00:00:14.250
you will be the same as your solution because indeed now the only thing that we have to do is juggle

5
00:00:14.280 --> 00:00:20.280
with our different machinery learning toolkits including the data processing tool kit and the classification

6
00:00:20.280 --> 00:00:23.310
tool kit to complete this implementation.

7
00:00:23.430 --> 00:00:24.430
So let's do this.

8
00:00:24.540 --> 00:00:28.480
Starting with splitting the data set into the training set in a desert.

9
00:00:28.500 --> 00:00:29.830
Well that's super easy.

10
00:00:29.830 --> 00:00:35.580
We're ready to do this and only one copy paste because we have indeed the matrix of Features X and the

11
00:00:35.580 --> 00:00:37.170
dependent variable vector Y.

12
00:00:37.260 --> 00:00:43.500
Therefore the only thing that we have to do here is just to go to our data repricing template and then

13
00:00:43.710 --> 00:00:51.780
take exactly these two lines of code to indeed split our data set composed of the matrix of Features

14
00:00:51.810 --> 00:00:58.410
X and the independent viable vector Y into well a new training set and test it.

15
00:00:58.470 --> 00:01:03.910
And that's our first copy paste and of course here we have nothing to change now.

16
00:01:03.930 --> 00:01:07.170
Next step training the name based model on the training set.

17
00:01:07.470 --> 00:01:13.410
So here we'll have to juggle with another of our machinery and tool kit which is of course the classification

18
00:01:13.410 --> 00:01:14.150
tool kit.

19
00:01:14.190 --> 00:01:20.250
So we're gonna go back into our whole machine learning is it further then we're going to you know use

20
00:01:20.250 --> 00:01:25.580
this little shortcut here to go back to the base of the folder which is this one machine learning a

21
00:01:25.600 --> 00:01:32.490
Z coding data set then we're going to go into point three classification and then we'll see all our

22
00:01:32.490 --> 00:01:38.820
different models including the native base but I would just like to remind that you know the choice

23
00:01:38.910 --> 00:01:45.540
of native base was just based on my experience I observed that the knife base does very well with natural

24
00:01:45.540 --> 00:01:51.390
language processing problems but I'll give you another exercise at the end of this tutorial which will

25
00:01:51.390 --> 00:01:57.240
be to beat me or you know to beat the score that we're gonna get at the end of this implementation.

26
00:01:57.270 --> 00:02:03.450
And so your goal will be to get an even better accuracy and if you get it you'll posted in the comments

27
00:02:03.510 --> 00:02:08.790
or you can send me a private message to say that indeed you managed to beat the accuracy score that

28
00:02:08.790 --> 00:02:14.200
we're about to get together and that you probably got by yourself when doing this exercise.

29
00:02:14.240 --> 00:02:17.790
All right so there we go let's just choose for now the knife based model.

30
00:02:17.820 --> 00:02:23.820
So we're going to go into the section section 18 knife bays then we're going to go into python and then

31
00:02:23.820 --> 00:02:31.200
we're going to open this knife base implementation open with Google collaboratively and you know I can

32
00:02:31.200 --> 00:02:36.990
just put it here it is now opening lowering it laying out a notebook and there we go.

33
00:02:36.990 --> 00:02:44.550
And now you have everything you can just find the cell that you know trains native based model on the

34
00:02:44.610 --> 00:02:50.130
training set which is right here by the way you could also take you know your moral selection folder

35
00:02:50.160 --> 00:02:55.650
containing all the classification models as you want but there we go what we need right now is this

36
00:02:55.650 --> 00:02:57.290
cell and nothing else.

37
00:02:57.300 --> 00:03:00.430
And we actually don't have anything to change inside.

38
00:03:00.520 --> 00:03:01.740
That's all good.

39
00:03:01.740 --> 00:03:07.230
Now let's go back to our copy of our natural language pricing implementation.

40
00:03:07.230 --> 00:03:12.270
Let's create a new coat sell here and let's just days that sell to train.

41
00:03:12.270 --> 00:03:19.140
Well the GAO Naif based model on the training set composed of X train and Y train that was just created

42
00:03:19.410 --> 00:03:20.670
just before.

43
00:03:20.670 --> 00:03:21.860
All right good.

44
00:03:21.870 --> 00:03:24.510
Now next step creating the test results.

45
00:03:24.510 --> 00:03:31.040
Well once again here we won't have anything to do except a simple copy paste still from our name based

46
00:03:31.050 --> 00:03:37.860
implementation because you know what we just want to do here is display next to each other the vector

47
00:03:37.860 --> 00:03:42.470
of predictions and the vector of real results containing the real reviews.

48
00:03:42.460 --> 00:03:47.670
You know the real outcomes of the reviews whether they're positive which gives a 1 or negative which

49
00:03:47.670 --> 00:03:49.010
gives a zero.

50
00:03:49.020 --> 00:03:55.700
So here I just copied and I'm about to paste that here in a new coat sale.

51
00:03:55.710 --> 00:03:56.100
All right.

52
00:03:56.100 --> 00:03:59.150
And this will indeed print next to each other.

53
00:03:59.190 --> 00:04:05.430
First the vector of predictions which we've got here in this first line of code and the vector of real

54
00:04:05.430 --> 00:04:08.640
results containing the real outcomes of the reviews.

55
00:04:08.640 --> 00:04:09.320
Perfect.

56
00:04:09.330 --> 00:04:13.210
And finally making the confusion matrix that's our last up here.

57
00:04:13.260 --> 00:04:17.730
And once again we're going to go back to our two kids you know the classification toolkit for native

58
00:04:17.730 --> 00:04:18.310
base.

59
00:04:18.450 --> 00:04:24.870
We're going to scroll down a bit more and we're going to find indeed the confusion matrix computing

60
00:04:24.990 --> 00:04:31.980
as well the accuracy score you know the accuracy being simply the number of correct predictions divided

61
00:04:31.980 --> 00:04:35.430
by the total number of observations in the test set of course.

62
00:04:35.970 --> 00:04:40.270
All right so let's copy and paste that in this new coat sale.

63
00:04:40.380 --> 00:04:41.840
And there you go my friends.

64
00:04:41.850 --> 00:04:43.870
Now this implementation is over.

65
00:04:43.920 --> 00:04:48.170
We finished it in just a few seconds or you know in a few minutes with the explanation.

66
00:04:48.330 --> 00:04:49.170
But there we go.

67
00:04:49.350 --> 00:04:52.560
That's what I mean by juggling with your different toolkits.

68
00:04:52.590 --> 00:04:59.040
You can be super efficient at implementing the classification or regression model by using your code

69
00:04:59.040 --> 00:05:01.060
templates all right.

70
00:05:01.060 --> 00:05:02.350
Now it's showtime.

71
00:05:02.350 --> 00:05:06.150
We will execute the cells that we haven't executed so far.

72
00:05:06.160 --> 00:05:08.380
So the last one we executed was this one.

73
00:05:08.380 --> 00:05:11.910
Basically everything that is related to the bag of words model.

74
00:05:11.920 --> 00:05:18.760
So now let's play the rest of his cell starting this one which will split the data set into the training

75
00:05:18.760 --> 00:05:20.800
set and to set all good.

76
00:05:20.920 --> 00:05:22.210
Now that we had a training set.

77
00:05:22.210 --> 00:05:26.910
We're going to train the native baseball on the training set and all good again.

78
00:05:26.980 --> 00:05:32.950
And now we're going to predict the test results by displaying next to each other the vector of predictions

79
00:05:33.100 --> 00:05:36.900
and the vector of real outcomes of the reviews.

80
00:05:37.120 --> 00:05:38.880
And well well look at this.

81
00:05:38.980 --> 00:05:45.430
We don't start well because we start with three incorrect predictions right for the first review which

82
00:05:45.430 --> 00:05:51.520
be careful is not well love this place because these are the reviews of the test not the whole dataset.

83
00:05:51.550 --> 00:05:53.260
So this is not the first review.

84
00:05:53.290 --> 00:05:54.010
I love this place.

85
00:05:54.010 --> 00:05:59.060
This is just a random review taken from the original data set and put into the test.

86
00:05:59.140 --> 00:06:04.780
But anyway for this first review of the test set our moral predicted this review to be positive whereas

87
00:06:04.840 --> 00:06:07.010
in reality it is negative.

88
00:06:07.060 --> 00:06:09.460
Same for the second review predicted positive.

89
00:06:09.460 --> 00:06:13.180
But in reality negative same for the third review predicted positive.

90
00:06:13.210 --> 00:06:18.730
But in reality negative and then this is correct that's a negative review which wasn't it predicted

91
00:06:18.820 --> 00:06:19.750
as negative.

92
00:06:19.750 --> 00:06:22.960
Same for this one negative review predicted as negative here.

93
00:06:22.960 --> 00:06:27.040
Another mistake negative review predicted as positive here.

94
00:06:27.040 --> 00:06:28.110
Correct prediction.

95
00:06:28.150 --> 00:06:30.640
Positive review predicted as positive then.

96
00:06:30.640 --> 00:06:31.540
Incorrect prediction.

97
00:06:31.540 --> 00:06:35.470
Negative review predicted as positive then an incorrect prediction again.

98
00:06:35.470 --> 00:06:38.380
Negative review predicted as positive and correct.

99
00:06:38.380 --> 00:06:38.940
Correct.

100
00:06:38.950 --> 00:06:39.610
Correct.

101
00:06:39.610 --> 00:06:41.420
Incorrect anyway so.

102
00:06:41.420 --> 00:06:49.020
Yeah but by scrolling down we can see you know that we actually have many correct predictions.

103
00:06:49.100 --> 00:06:54.050
And anyway we're going to check that right away with our computing matrix below.

104
00:06:54.100 --> 00:06:56.700
I'm actually going to scroll down from here.

105
00:06:56.710 --> 00:06:57.980
There we go.

106
00:06:57.970 --> 00:06:58.980
And perfect.

107
00:06:59.020 --> 00:07:05.020
So this is our last cell which will display the confusion matrix and compute the accuracy score which

108
00:07:05.080 --> 00:07:09.620
I will want you to beat right after this tutorial as a final exercise of an LP.

109
00:07:09.940 --> 00:07:15.940
And so let's play this cell to see what the confusion matrix looks like and mostly to see the final

110
00:07:15.940 --> 00:07:19.240
accuracy which is 73 percent.

111
00:07:19.240 --> 00:07:19.690
All right.

112
00:07:19.720 --> 00:07:20.790
So that's pretty good.

113
00:07:20.980 --> 00:07:22.860
But I'm sure we can do better.

114
00:07:22.870 --> 00:07:24.450
You know there are many ways to better.

115
00:07:24.760 --> 00:07:30.280
And so I really look forward to seeing your results you know after your experiment with more classification

116
00:07:30.280 --> 00:07:36.670
models or even by doing a better cleaning of the text you know the reviews maybe you can add some more

117
00:07:36.670 --> 00:07:42.640
exclusions in the list of stuff words you know we excluded not from the list of stop words but maybe

118
00:07:42.640 --> 00:07:44.590
you can exclude as well isn't.

119
00:07:44.660 --> 00:07:47.800
You know I know that isn't is actually part of the stopwatch list.

120
00:07:47.890 --> 00:07:53.500
So you know you can do some extra work in order to improve this because I'm sure that we can get a better

121
00:07:53.500 --> 00:07:56.740
accuracy than 73 percent but still this is pretty good.

122
00:07:56.740 --> 00:08:01.660
You know remember that we actually trained a machine to understand English and you know a couple of

123
00:08:01.660 --> 00:08:05.510
years ago maybe decades ago this would definitely seem very challenging.

124
00:08:05.620 --> 00:08:09.580
But here we did it in just a few minutes and that's absolutely incredible.

125
00:08:09.580 --> 00:08:14.390
And the morale predicts whether these English words and reviews are positive or negative.

126
00:08:14.450 --> 00:08:16.970
Correctly 73 percent of the time.

127
00:08:16.990 --> 00:08:18.190
So that's really really good.

128
00:08:18.190 --> 00:08:25.600
And that's the confusion matrix with 55 correct predictions of negative reviews 91 correct predictions

129
00:08:25.600 --> 00:08:33.370
of positive reviews 42 incorrect predictions of positive reviews and 12 incorrect predictions of negative

130
00:08:33.370 --> 00:08:33.940
reviews.

131
00:08:33.940 --> 00:08:39.880
All right so try to reduce these two numbers here and let me know or let everybody know in the comments

132
00:08:40.060 --> 00:08:43.600
what you managed to get which solution you managed to improve.

133
00:08:43.600 --> 00:08:48.600
And I look forward to seeing if you manage to you know for example go over 80 percent.

134
00:08:48.600 --> 00:08:50.050
That would be fantastic.

135
00:08:50.050 --> 00:08:53.690
You know you would definitely beat me by pretty far.

136
00:08:53.770 --> 00:08:54.220
All right.

137
00:08:54.220 --> 00:08:57.150
So now we're done with natural language processing.

138
00:08:57.160 --> 00:09:03.470
I hope you like this introduction to sentiment analysis if you liked an LP and if you want to you know

139
00:09:03.730 --> 00:09:06.870
study more in depth this branch of machine learning.

140
00:09:06.910 --> 00:09:10.670
Well know that we have other courses about Chad birds about the birds model.

141
00:09:10.750 --> 00:09:12.540
So I really recommend to check it out.

142
00:09:12.580 --> 00:09:18.580
But first I recommend that you know you complete this journey of machine learning and speaking of this.

143
00:09:18.610 --> 00:09:24.850
Well the next step of our journey here is to enter the fascinating world of deep learning where we will

144
00:09:24.850 --> 00:09:32.080
you know mimic the processes of the human brain to actually give for the first time to our A.I. an artificial

145
00:09:32.080 --> 00:09:35.790
brain which will itself perform some predictions.

146
00:09:35.800 --> 00:09:40.840
It's super fascinating it's actually you know the most fascinating branch of machinery because this

147
00:09:40.840 --> 00:09:44.140
is the closest one to human intelligence.

148
00:09:44.140 --> 00:09:50.370
So now I just can't wait to see you in this next part to enter the world of deep learning.

149
00:09:50.410 --> 00:09:52.390
And until then enjoy machine learning.
