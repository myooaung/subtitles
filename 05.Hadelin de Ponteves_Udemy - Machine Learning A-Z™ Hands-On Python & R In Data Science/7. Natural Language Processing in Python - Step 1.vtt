WEBVTT
1
00:00:00.360 --> 00:00:05.830
Hello my friends and welcome to this new part on natural language processing.

2
00:00:05.850 --> 00:00:10.200
I'm super excited to start this part because this is the branch of machine learning with which you can

3
00:00:10.200 --> 00:00:13.180
build checkbooks and Machine Translations.

4
00:00:13.290 --> 00:00:18.030
So of course this is not what we're going to do in this part because this is really advanced and LP

5
00:00:18.300 --> 00:00:25.320
so we'll just cover the basics with sentiment analysis which consists of training a machine to understand

6
00:00:25.320 --> 00:00:29.450
some text and predict a certain outcome for these texts.

7
00:00:29.460 --> 00:00:35.280
So in our case study here these text will be reviews of a restaurant and we'll have to train a machine

8
00:00:35.280 --> 00:00:39.150
to understand if each review is positive or negative.

9
00:00:39.450 --> 00:00:44.730
So very simple very classic but the best way to be introduced to an LP.

10
00:00:44.790 --> 00:00:48.390
All right before we start let's make sure everyone here is on the same page.

11
00:00:48.390 --> 00:00:54.120
This is the folder containing all the codes and data sets and of which I give you the link right before

12
00:00:54.120 --> 00:00:55.740
this tutorial in the article.

13
00:00:55.740 --> 00:00:57.560
So make sure to connect to that link.

14
00:00:57.680 --> 00:00:58.770
And now there we go.

15
00:00:58.770 --> 00:01:02.950
We can and support seven natural language processing.

16
00:01:03.000 --> 00:01:08.880
So in this part you will only find one section that's because we only do one case study of an LP about

17
00:01:08.880 --> 00:01:10.410
sentiment analysis.

18
00:01:10.410 --> 00:01:16.150
However you will see that you can try diverse machinery models to tackle the problem.

19
00:01:16.170 --> 00:01:22.650
Indeed the essential part of our implementation will be to build the bag of words model but then once

20
00:01:22.650 --> 00:01:26.370
it is built we can try several classification models.

21
00:01:26.370 --> 00:01:27.690
Why classification models.

22
00:01:27.690 --> 00:01:33.870
That's because we'll have to predict you know a binary outcome 1 or 0 1 meaning the review is positive

23
00:01:34.140 --> 00:01:36.600
and 0 meaning the review is negative.

24
00:01:36.690 --> 00:01:42.570
So you have actually the flexibility to try several machinery models and this will actually be your

25
00:01:42.570 --> 00:01:45.630
final exercise of this section.

26
00:01:45.630 --> 00:01:46.230
So there we go.

27
00:01:46.230 --> 00:01:49.340
Let's do this let's enter section 36 now.

28
00:01:49.440 --> 00:01:55.320
Natural Language Processing and as usual we're going to start with python in which you will find two

29
00:01:55.320 --> 00:02:01.200
fouls the implementation natural language processing that IP Wendy which you can open with either Google

30
00:02:01.220 --> 00:02:05.190
collaboratively or to put a notebook and or data set.

31
00:02:05.310 --> 00:02:10.080
Restaurant reviews dot this time not see as V but t as V.

32
00:02:10.080 --> 00:02:17.340
And this will be a good opportunity for me to train you on how to import a t as V dataset t as we mean

33
00:02:17.460 --> 00:02:22.230
tab separate value instead of comma separated value like in this year's V.

34
00:02:22.230 --> 00:02:26.330
So basically the only difference is that in the previous data sets we worked with.

35
00:02:26.580 --> 00:02:31.560
Well you know the features and the dependent variable were separated by commerce and in this one.

36
00:02:31.740 --> 00:02:36.840
Well instead of being separated by commerce the reviews and the dependent variable will be separated

37
00:02:36.840 --> 00:02:38.040
by a tab.

38
00:02:38.040 --> 00:02:39.020
And this makes sense right.

39
00:02:39.030 --> 00:02:43.860
Because in the reviews we already have commerce and therefore that would create nonsense features.

40
00:02:43.860 --> 00:02:46.620
But let me show you what this dataset looks like.

41
00:02:46.620 --> 00:02:48.510
So as you can see there are only two columns.

42
00:02:48.510 --> 00:02:51.170
The first one containing all the reviews.

43
00:02:51.180 --> 00:02:57.130
So for example this is a first one well let this place a second one crust is not good.

44
00:02:57.270 --> 00:02:59.670
Another one great touch etc..

45
00:02:59.670 --> 00:03:03.450
So you have in total let's see 1000 reviews.

46
00:03:03.450 --> 00:03:03.730
Right.

47
00:03:03.740 --> 00:03:08.190
So we're going to trend on machine learning to actually understand text and predict if the texts are

48
00:03:08.190 --> 00:03:11.540
positive or negative with 1000 texts.

49
00:03:11.760 --> 00:03:12.360
All right.

50
00:03:12.360 --> 00:03:17.040
And then the second column is of course if the review is positive or negative.

51
00:03:17.040 --> 00:03:23.450
So one means that it is positive meaning the customer liked it and zero means that the review is negative.

52
00:03:23.550 --> 00:03:28.830
And of course we have the real outcomes in order to train our machinery model to understand if each

53
00:03:28.920 --> 00:03:32.160
of these text is positive or negative.

54
00:03:32.160 --> 00:03:35.820
So that's purely in the end you know a classification problem.

55
00:03:35.820 --> 00:03:41.850
But the essential part of it is that will train the machine to understand these texts first and then

56
00:03:41.970 --> 00:03:45.570
to predict if they are positive or negative.

57
00:03:45.570 --> 00:03:46.020
All right.

58
00:03:46.050 --> 00:03:48.700
So very simple case that very simple data set.

59
00:03:48.720 --> 00:03:54.020
That means we are ready to start the implementation of natural language processing.

60
00:03:54.060 --> 00:03:55.360
So as you prefer.

61
00:03:55.380 --> 00:03:59.330
Feel free to open it with either Google collaboratively or Julia notebook.

62
00:03:59.490 --> 00:04:02.150
I'm opening it with Google collab as usual.

63
00:04:02.250 --> 00:04:04.110
So feel free to do the same if you'd like.

64
00:04:04.140 --> 00:04:09.870
And now the notebook is loading in a second it will be laying out all right.

65
00:04:09.870 --> 00:04:10.760
Loading laying out.

66
00:04:10.770 --> 00:04:11.360
Perfect.

67
00:04:11.370 --> 00:04:13.910
And this is the implementation.

68
00:04:13.960 --> 00:04:19.940
And as usual this is and read only mode and we want to re implement this from scratch therefore we're

69
00:04:19.950 --> 00:04:24.490
going to create a copy right away so that we can modify the code inside.

70
00:04:24.570 --> 00:04:27.270
So we're going to click Save a copy and drive here.

71
00:04:27.300 --> 00:04:34.310
This will create a copy after which we will be able to modify the code and re implement it from scratch.

72
00:04:34.350 --> 00:04:40.380
And speaking of re implementing it from scratch well let's delete all the code sales because we will

73
00:04:40.770 --> 00:04:41.770
implement them.

74
00:04:41.790 --> 00:04:47.820
So let's kick this trash button here in each of the code sales but not the text so that we can keep

75
00:04:48.130 --> 00:04:54.510
that well highlighted structure and see where we are going at each time of the implementation.

76
00:04:54.510 --> 00:04:55.420
All right.

77
00:04:55.500 --> 00:04:57.200
So almost done.

78
00:04:57.480 --> 00:05:03.680
It's actually an implementation in about ten steps but you will recognize some of the steps as steps

79
00:05:03.760 --> 00:05:04.970
we did before.

80
00:05:05.050 --> 00:05:07.320
You'll see I'm going to show you in a second.

81
00:05:07.330 --> 00:05:10.580
All right so let's have a look at the structure of this implementation.

82
00:05:10.600 --> 00:05:15.730
We will start first by importing the libraries as usual because indeed we will need several libraries

83
00:05:15.730 --> 00:05:20.410
to pre process our texts and train our future machine learning model.

84
00:05:20.410 --> 00:05:23.970
Then we will imported data set so that actually that data repricing phase.

85
00:05:23.980 --> 00:05:30.450
But not only that the data depressing phase will also contain the next two sales cleaning the tax.

86
00:05:30.460 --> 00:05:37.090
Indeed we will have to simplify the text as much as we can in order to ease the learning process of

87
00:05:37.090 --> 00:05:38.250
the machinery model.

88
00:05:38.260 --> 00:05:40.240
You know we'll have to remove all the punctuation.

89
00:05:40.240 --> 00:05:44.110
We'll have to put all the letters in lowercase then we'll have to apply stemming.

90
00:05:44.140 --> 00:05:49.850
You know we'll have to make very clean text to alleviate the learning process of the future classification

91
00:05:49.850 --> 00:05:50.920
model will build.

92
00:05:50.920 --> 00:05:56.830
So that's a compulsory process when doing an LP you have to pre process the text basically.

93
00:05:56.830 --> 00:06:02.650
Then we'll create the back of words moral which is at the heart of sentiment analysis.

94
00:06:02.650 --> 00:06:03.970
And then there you go.

95
00:06:04.000 --> 00:06:06.160
That's where you will recognize everything.

96
00:06:06.280 --> 00:06:10.990
Once we have the bag of words model we basically have a data set ready to be trained.

97
00:06:11.140 --> 00:06:11.440
Right.

98
00:06:11.440 --> 00:06:16.810
We have a date that ready to be trained by a machine learning model and that's why then we will just

99
00:06:16.810 --> 00:06:19.700
apply the classic process of training a model.

100
00:06:19.720 --> 00:06:25.360
First we will split the data set into the training set and test set so that we can indeed have a set

101
00:06:25.360 --> 00:06:30.460
where we train the model to understand texts and predict if the texts are positive or negative.

102
00:06:30.460 --> 00:06:36.580
And the test set so that we can evaluate the performance on new texts on which the model wasn't trained.

103
00:06:36.670 --> 00:06:38.380
And then there we go we train.

104
00:06:38.590 --> 00:06:44.140
So I chose a knife based model on the training set but you will see that your final exercise at the

105
00:06:44.140 --> 00:06:50.200
end will be to try the other classification models and see if you can beat the accuracy I will get in

106
00:06:50.200 --> 00:06:51.460
this implementation.

107
00:06:51.640 --> 00:06:53.730
Then we will predict the test results.

108
00:06:53.740 --> 00:06:59.860
And finally we will make the confused matrix and get the final accuracy so that our structure that's

109
00:06:59.860 --> 00:07:01.500
our MLP journey.

110
00:07:01.540 --> 00:07:08.590
So now as soon as you're ready let's start in the next tutorial with the simple therapy pricing phase.

111
00:07:08.590 --> 00:07:11.040
I can't wait to start see you in the next toil.

112
00:07:11.050 --> 00:07:12.850
And until then enjoy machine learning.
