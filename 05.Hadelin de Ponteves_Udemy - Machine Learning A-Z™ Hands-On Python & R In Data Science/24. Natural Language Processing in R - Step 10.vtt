WEBVTT
1
00:00:00.240 --> 00:00:02.580
Helen welcome to this arts of Tauriel.

2
00:00:02.580 --> 00:00:08.760
So we did the main steps we cleaned all the text all the reviews we created our bag of words moral and

3
00:00:08.760 --> 00:00:14.400
now we have to do one more thing which is of course to build our machine learning to model and we can

4
00:00:14.400 --> 00:00:20.310
do that because we have all our independent variables in this sparse matrix DDM here builds thanks to

5
00:00:20.310 --> 00:00:22.190
this function documentary matrix.

6
00:00:22.440 --> 00:00:26.750
And besides we applied a filter here to remove the non frequent words.

7
00:00:26.790 --> 00:00:32.220
Well a few of them but still this considerably reduced the number of words in the matrix.

8
00:00:32.220 --> 00:00:35.520
So that's always good for a model to run faster.

9
00:00:35.520 --> 00:00:37.390
All right so now let's build a model.

10
00:00:37.410 --> 00:00:44.430
So what we'll do is go to our files to get back to the part 3 classification because what we'll do of

11
00:00:44.430 --> 00:00:48.400
course is take a classification model that we already built.

12
00:00:48.570 --> 00:00:54.420
And we will apply on our tests here because out of this decs we managed to create a matrix of features

13
00:00:54.420 --> 00:00:56.310
containing independent variables.

14
00:00:56.310 --> 00:01:01.270
And of course we have one dependent variable which is the second column of our data set.

15
00:01:01.410 --> 00:01:04.020
So I like to call them which tells of yes or no.

16
00:01:04.050 --> 00:01:05.330
The review is positive.

17
00:01:06.460 --> 00:01:12.600
So we have everything and therefore we only need to take our model now and therefore we go to Partha

18
00:01:12.600 --> 00:01:13.850
reclassification.

19
00:01:13.900 --> 00:01:17.360
And here we can find all our classification morals.

20
00:01:17.380 --> 00:01:21.910
All right so which one to pick which one to choose for natural language processing.

21
00:01:22.120 --> 00:01:28.630
Well in general based on experience the most common classification models used for natural language

22
00:01:28.630 --> 00:01:33.560
processing are now base decision tree or random forest.

23
00:01:33.580 --> 00:01:37.710
You also have the card model which is another type of decision tree model.

24
00:01:37.810 --> 00:01:43.940
And you also have the maximum entropy model which is based on entropy as well like for decision trees.

25
00:01:43.960 --> 00:01:49.210
So these models work very well for natural language processing and therefore he will pick one that is

26
00:01:49.210 --> 00:01:55.660
related to entropy and that's the case for Decision Tree classification model as well as our Random

27
00:01:55.660 --> 00:02:01.030
forest classification model because of course a run for us is a combination of trees making the same

28
00:02:01.030 --> 00:02:02.190
predictions together.

29
00:02:02.530 --> 00:02:07.510
And keep in mind that you can also use knowledge base which is commonly used as well for natural language

30
00:02:07.510 --> 00:02:08.390
processing.

31
00:02:08.800 --> 00:02:12.990
But here in this tutorial will choose random for classification.

32
00:02:13.000 --> 00:02:18.550
So let's go into this section and here are all the files you know with the data set the classification

33
00:02:18.550 --> 00:02:21.760
templates and our model in Python and our.

34
00:02:21.760 --> 00:02:23.170
So let's one are.

35
00:02:23.190 --> 00:02:25.060
So I'm just clicking on the file.

36
00:02:25.060 --> 00:02:25.960
Here we go.

37
00:02:26.050 --> 00:02:27.860
Model open here.

38
00:02:27.910 --> 00:02:29.190
So what do we need here.

39
00:02:29.320 --> 00:02:34.630
Well first of all let's notice that you know when we are using to run them for this classification model

40
00:02:34.950 --> 00:02:41.560
we starting with a data set which is a data frame and that contains both the independent variables and

41
00:02:41.560 --> 00:02:42.780
the dependent variable.

42
00:02:42.940 --> 00:02:49.690
So what we have to do right now is go back to our natural language processing file and create exactly

43
00:02:49.690 --> 00:02:50.110
the same.

44
00:02:50.110 --> 00:02:56.110
That is create a data set containing independent variables and one dependent variable and that will

45
00:02:56.110 --> 00:03:01.840
be the inputs of this model here because you know we have our data set here and then we use our dataset

46
00:03:01.920 --> 00:03:03.700
in each of the code sections here.

47
00:03:03.880 --> 00:03:09.370
And then you know we split the data sets into training set in a test set and we train our machinery

48
00:03:09.370 --> 00:03:12.500
and classification model on the trend set here.

49
00:03:12.550 --> 00:03:17.710
So it's we only have to do is just to create this dataset containing the independent variables and the

50
00:03:17.710 --> 00:03:18.880
dependent variable.

51
00:03:18.910 --> 00:03:22.890
So that's very simple we already have are independent variables.

52
00:03:22.990 --> 00:03:28.120
But the problem is that our independent variables right now are in the Matrix because you know this

53
00:03:28.240 --> 00:03:33.630
documentary matrix function returns a matrix of DGM is a matrix right now.

54
00:03:33.880 --> 00:03:40.570
And as you remember in our classification models on our world this data set is a data frame.

55
00:03:40.630 --> 00:03:41.890
It's not a matrix.

56
00:03:41.980 --> 00:03:47.860
So we need to make sure that here for the inputs of the model that we are going to apply on this bag

57
00:03:47.860 --> 00:03:50.820
of words small that we just created them to produce the totals.

58
00:03:51.040 --> 00:03:55.330
Well we need to make sure that we have a data frame but that's actually very simple.

59
00:03:55.360 --> 00:04:03.250
We just need to take our matrix and use the function as Dot data dot frame and we will put our sparse

60
00:04:03.250 --> 00:04:09.980
matrix that GM inside and that will transform our DTN sparse matrix into a data frame.

61
00:04:10.000 --> 00:04:11.030
So let's do this.

62
00:04:11.140 --> 00:04:16.590
And since you know we are just going to copy paste or run them for us classification here.

63
00:04:16.730 --> 00:04:20.270
Well since the input of this model is basically the data sets.

64
00:04:20.470 --> 00:04:24.260
Well we will use the same name to create this data frame.

65
00:04:24.310 --> 00:04:33.290
And so we will call that dataset data set equals and then that's when we use the as that data don't

66
00:04:33.410 --> 00:04:33.790
frame.

67
00:04:33.790 --> 00:04:34.240
Here it is.

68
00:04:34.240 --> 00:04:35.700
That's the first one.

69
00:04:35.740 --> 00:04:36.570
Here we go.

70
00:04:36.700 --> 00:04:42.650
And so now we need to put the Matrix we want to transform into a data frame and that's of course d t

71
00:04:42.880 --> 00:04:43.650
am.

72
00:04:43.750 --> 00:04:50.320
And just to make sure we have the Matrix type expected by this as that data frame function well we need

73
00:04:50.320 --> 00:04:59.230
to use here the function as dot matrix and put the DMAs input of this as matrix function because you

74
00:04:59.230 --> 00:05:05.860
know this sparse matrix DTM here is definitely a matrix but it doesn't have the type expected by this

75
00:05:05.940 --> 00:05:07.590
as that data frame function.

76
00:05:07.780 --> 00:05:12.910
And to make sure we have the right Matrix type Well we need to use this dot matrix function.

77
00:05:13.120 --> 00:05:13.480
Right.

78
00:05:13.520 --> 00:05:14.440
Now let's be careful.

79
00:05:14.440 --> 00:05:16.610
We lost one parenthesis.

80
00:05:16.690 --> 00:05:18.490
So I'm just adding it all right.

81
00:05:18.490 --> 00:05:19.390
Now we're good.

82
00:05:19.570 --> 00:05:24.910
We are ready to transform our sparse matrix of features into a data frame.

83
00:05:25.090 --> 00:05:26.030
So let's do it.

84
00:05:26.170 --> 00:05:29.500
I'm going to select this line and executes all right.

85
00:05:29.770 --> 00:05:34.570
And now it's interesting to see is that we have the real data set you know with all the reviews and

86
00:05:34.570 --> 00:05:40.980
the rows and all the words that we took from the corpus and then were filtered thanks to this remove

87
00:05:40.980 --> 00:05:42.410
sparse terms function.

88
00:05:42.580 --> 00:05:50.170
Well we didn't see the full dataset here with this one thousand rows and all these 691 columns each

89
00:05:50.170 --> 00:05:53.920
one corresponding to a word that comes from the reviews in the corpus.

90
00:05:53.920 --> 00:05:57.850
And that was not filtered by the remove sparse terms function.

91
00:05:57.850 --> 00:05:58.370
All right.

92
00:05:58.570 --> 00:06:04.420
So here you can have a look at this huge table and we can clearly see here that this is a sparse matrix

93
00:06:04.810 --> 00:06:07.440
because basically we can only see zeroes.

94
00:06:07.440 --> 00:06:08.880
Well we have very few ones.

95
00:06:08.880 --> 00:06:12.250
We have one here one here but all the rest is zeroes.

96
00:06:12.360 --> 00:06:14.680
And so for example if I take this one here.

97
00:06:14.850 --> 00:06:22.240
Well this one belongs to me also called him and to the 23rd row that is the 23rd review.

98
00:06:22.500 --> 00:06:28.330
And so this one here means that the word also appears in the review 23.

99
00:06:28.650 --> 00:06:30.410
All right so that's the sparse matrix.

100
00:06:30.450 --> 00:06:33.210
And now you can really see what it is with your own eyes.

101
00:06:33.210 --> 00:06:33.570
All right.

102
00:06:33.570 --> 00:06:36.780
So let's go back to our natural language processing file.

103
00:06:36.780 --> 00:06:44.020
So we have our data dataset which is now a data frame as we wanted but still incomplete.

104
00:06:44.070 --> 00:06:50.100
You know why it's because the data set we start with in this random first classification model and in

105
00:06:50.100 --> 00:06:53.220
general with classification models is a data frame.

106
00:06:53.250 --> 00:06:59.930
So we're good on that but a data from content in both the independent variables and the dependent variable.

107
00:07:00.090 --> 00:07:06.870
So what we need to do right now is and that has been invaluable to this data from data set because right

108
00:07:06.870 --> 00:07:09.750
now it only contains the independent variables.

109
00:07:09.750 --> 00:07:14.810
All right so you might remember how to add the dependent variable column to a data set that is the data

110
00:07:14.820 --> 00:07:15.380
frame.

111
00:07:15.510 --> 00:07:21.330
Remember we need to take our data set then add a dollar sign here and then after this data sign we can

112
00:07:21.360 --> 00:07:29.280
either take one of the existing column here if we want to update the column or create a new column to

113
00:07:29.310 --> 00:07:30.810
add to this dataset.

114
00:07:31.020 --> 00:07:32.690
And that's exactly what we want to do.

115
00:07:32.700 --> 00:07:35.090
We want to create a new column to this data set.

116
00:07:35.130 --> 00:07:40.230
While that's an existing column that's the light column but we created for this data set because it

117
00:07:40.230 --> 00:07:46.140
is new column and so will give to this column the same name as the real depend Roybal call him that

118
00:07:46.190 --> 00:07:54.340
is light's all right so by doing this we are adding this new column that we call light and then equals

119
00:07:54.580 --> 00:08:00.220
and then after this equal we need to specify what we want to add in this new column and what we want

120
00:08:00.220 --> 00:08:05.690
to add is nothing else than the existing liked column of our dataset.

121
00:08:05.860 --> 00:08:11.440
But be careful because our dataset was just a data to this new data frame and therefore we no longer

122
00:08:11.440 --> 00:08:14.530
have the dataset that we imported originally.

123
00:08:14.530 --> 00:08:21.480
So what we'll do is very simple We'll just rename the data set by adding a nun's core and then original.

124
00:08:21.550 --> 00:08:25.890
We go and we will select this line again and executes.

125
00:08:25.930 --> 00:08:31.120
All right so now we have our original data set and therefore we can have access to the liked column

126
00:08:31.240 --> 00:08:35.230
of this original dataset which is going to be our dependent variable.

127
00:08:35.500 --> 00:08:39.060
So let's add this dependent variable right now to our dataset.

128
00:08:39.280 --> 00:08:46.000
And so to take this dependent variable we need to take our data sets original here this because that's

129
00:08:46.150 --> 00:08:49.790
the original data set containing the dependent variable light.

130
00:08:49.900 --> 00:08:56.860
And so to take this dependent variable vector we need to add a dollar sign here same and then take the

131
00:08:56.860 --> 00:08:59.680
column we want which is the liked column.

132
00:08:59.680 --> 00:09:00.940
All right so that's good.

133
00:09:01.030 --> 00:09:09.820
By selecting this line and executing it we add the like dependent variable vector column to our dataset

134
00:09:09.940 --> 00:09:15.040
already containing the independent variables that are all the filtered words of our clean revues in

135
00:09:15.040 --> 00:09:16.330
the corpus.

136
00:09:16.330 --> 00:09:21.390
All right so now we have everything we need and we are ready to take our machine or an investigation

137
00:09:21.390 --> 00:09:27.430
all because we have our data set that not only is a data frame but also contains both the independent

138
00:09:27.430 --> 00:09:29.250
variables and the dependent variable.

139
00:09:29.260 --> 00:09:33.470
So we have everything what is expecting a them for us classification all here.

140
00:09:33.640 --> 00:09:38.890
So where we only need to do here is say everything from here and not from here you know because this

141
00:09:38.890 --> 00:09:44.380
section is to import the data set but we already have our data set that is ready for our classification

142
00:09:44.380 --> 00:09:45.050
model.

143
00:09:45.070 --> 00:09:50.840
So we just need to take everything from here because this is where the data set starts to be processed.

144
00:09:51.040 --> 00:09:58.540
And so we take everything from here to here and we cannot take this because this is to plot the results

145
00:09:58.540 --> 00:10:00.940
in 2D that is two independent variables.

146
00:10:00.940 --> 00:10:04.360
And here since of course we have a lot more than two independent variables.

147
00:10:04.450 --> 00:10:09.520
Well we cannot use this to plot the results but we will definitely have a look at the confusion matrix

148
00:10:09.520 --> 00:10:14.410
to see the number of correct predictions as well as the number of incorrect predictions so that we can

149
00:10:14.410 --> 00:10:16.570
evaluate the model performance.

150
00:10:16.570 --> 00:10:22.300
All right so let's get back to our natural language processing file and we will face our random for

151
00:10:22.300 --> 00:10:25.250
us classification model right here.

152
00:10:25.270 --> 00:10:30.810
All right so now we just need to modify a very few things because everything is basically ready.

153
00:10:31.060 --> 00:10:33.440
But let's see what we can modify.

154
00:10:33.530 --> 00:10:38.800
Well first here in the section that encodes the target feature as vector Well of course we need to replace

155
00:10:38.800 --> 00:10:43.520
this purchased dependent variable which was the dependent variable in part 3.

156
00:10:43.660 --> 00:10:48.320
Well we need to replace it with our new depend Roybal which is liked.

157
00:10:48.400 --> 00:10:54.090
All right and same here we replaced purchased by likes.

158
00:10:54.100 --> 00:10:54.660
All right.

159
00:10:54.730 --> 00:10:59.980
Good for the section then in the next section we split the datasets into the training set and a test

160
00:11:00.050 --> 00:11:00.580
set.

161
00:11:00.750 --> 00:11:05.800
Well that's very important to do this unless you want to create a new review but you know we will train

162
00:11:05.840 --> 00:11:12.790
or run and for us classification models on say for example 800 reviews and we will test the predictive

163
00:11:12.790 --> 00:11:18.960
power of run them for us on 200 new reviews on which are run them for us classification or wasn't a

164
00:11:18.970 --> 00:11:24.610
trained and therefore these 200 reviews and the test set will be new reviews for a run for US classification

165
00:11:24.610 --> 00:11:25.280
model.

166
00:11:25.450 --> 00:11:31.990
And so we will see how it manages to predict whether each of these 200 reviews is positive or negative

167
00:11:32.380 --> 00:11:37.330
and then that's in the confusing matrix that will see the number of correct predictions and the number

168
00:11:37.330 --> 00:11:40.980
of incorrect predictions in this 200 new reviews.

169
00:11:40.990 --> 00:11:42.960
All right so that's what is done in this section.

170
00:11:43.030 --> 00:11:49.450
And since I just gave as an example 800 reviews to train the moral and 200 reviews to test it well let's

171
00:11:49.450 --> 00:11:56.470
go with this choice of numbers and so we need to change the split ratio here to point 8 because that's

172
00:11:56.560 --> 00:11:57.320
80 percent.

173
00:11:57.340 --> 00:12:04.300
And we have 1000 reviews so 80 percent of 1000 reviews is 800 reviews to go to the trainset and therefore

174
00:12:04.300 --> 00:12:06.640
200 reviews to go to the test set.

175
00:12:06.640 --> 00:12:06.990
All right.

176
00:12:07.000 --> 00:12:12.670
That's good and of course let's not forget to replace the per chaced variable here by our new depan

177
00:12:12.670 --> 00:12:14.920
variable that is light.

178
00:12:15.340 --> 00:12:17.730
All right so I think we're good with this section.

179
00:12:17.740 --> 00:12:19.570
So now let's move onto the next one.

180
00:12:19.630 --> 00:12:24.150
The next one is about feature scaling and so here do we need to apply future scaling.

181
00:12:24.310 --> 00:12:30.460
Well not really because we only have zeros and ones in the sparse matrix of features and therefore we

182
00:12:30.460 --> 00:12:34.510
don't have one independent variable dominating another independent variable.

183
00:12:34.510 --> 00:12:39.230
So we don't need to play future skating so we will remove this section.

184
00:12:39.400 --> 00:12:41.320
All right and so what about this one.

185
00:12:41.320 --> 00:12:47.050
Yes of course we keep this one because this is the section where we build around them for us classification

186
00:12:47.050 --> 00:12:52.570
model that will classify the reviews and that's where we train to run them for us classification model

187
00:12:52.780 --> 00:12:56.400
on the training sets and therefore he will need to change two things.

188
00:12:56.530 --> 00:13:02.980
First the index here that you know is the index of the dependent variable that we need to remove from

189
00:13:03.070 --> 00:13:08.770
X because X is supposed to be the training set without the dependent variable.

190
00:13:08.830 --> 00:13:15.430
So we need to remove it but the index of our newly been invaluable like it is not 3 but is six hundred

191
00:13:15.480 --> 00:13:18.200
and ninety two we can see that here very easily.

192
00:13:18.310 --> 00:13:21.990
So let's replace three by six hundred and ninety two.

193
00:13:22.300 --> 00:13:23.850
All right good.

194
00:13:23.860 --> 00:13:29.080
And now the second thing that we need to change is of course this per chaced here that we still need

195
00:13:29.080 --> 00:13:33.430
to replace by liked this way.

196
00:13:33.760 --> 00:13:39.130
And then if we want we can train our running for US classification with more trees.

197
00:13:39.280 --> 00:13:45.220
Right now we have 10 trees so we will keep 10 trees that might be enough for our 1000 reviews which

198
00:13:45.220 --> 00:13:51.610
is quite a small number of reviews and especially our six hundred ninety two words columns that we have

199
00:13:51.850 --> 00:13:55.060
in our sparse matrix of features trees might be enough.

200
00:13:55.180 --> 00:14:00.280
But of course you're welcome to try more around them for us gasification models with more trees.

201
00:14:00.640 --> 00:14:02.080
So we're going with the section.

202
00:14:02.170 --> 00:14:04.160
And now let's move on to the next one.

203
00:14:04.240 --> 00:14:10.860
The next one is about predicting the test results so making the predictions on 200 new reviews.

204
00:14:10.930 --> 00:14:16.180
That's our model won't know anything about and therefore for these new reviews are always going to try

205
00:14:16.180 --> 00:14:19.390
to predict if those reviews are positive or negative.

206
00:14:19.690 --> 00:14:24.940
And therefore it will be very interesting to see if it's making some correct predictions on your reviews.

207
00:14:24.940 --> 00:14:29.920
So right now it's the same we have to replace this index here that corresponds to the index of the dependent

208
00:14:29.920 --> 00:14:30.750
variable.

209
00:14:30.850 --> 00:14:32.610
And so we need to replace three by.

210
00:14:32.620 --> 00:14:33.330
Of course.

211
00:14:33.330 --> 00:14:34.840
Six hundred ninety two.

212
00:14:34.840 --> 00:14:38.190
That's exactly the same as we did for the trading set here.

213
00:14:38.440 --> 00:14:40.590
And so now we're good for this section.

214
00:14:40.690 --> 00:14:44.290
We're finally getting to the last section that is making the confusion matrix.

215
00:14:44.440 --> 00:14:48.790
That's the interesting section that will tell us the number of correct predictions and the number of

216
00:14:48.850 --> 00:14:51.690
incorrect predictions for these two hundred new reviews.

217
00:14:51.790 --> 00:14:53.100
So we will see that right now.

218
00:14:53.140 --> 00:14:58.730
But of course we need to replace this 3 index that corresponds to the index of the dependent variable

219
00:14:58.750 --> 00:15:03.230
still the same and replace it by 692.

220
00:15:03.410 --> 00:15:05.150
All right so now everything is good.

221
00:15:05.210 --> 00:15:11.540
We are ready to train around for us classification model on our 800 reviews of the training set and

222
00:15:11.540 --> 00:15:17.030
then evaluate the predictive power of our model on our 200 new reviews in the test set.

223
00:15:17.030 --> 00:15:21.610
So let's do it since we already executed everything up to here.

224
00:15:21.680 --> 00:15:26.990
What we need to do now is just select everything from here to the bottom.

225
00:15:26.990 --> 00:15:27.940
And now we're good.

226
00:15:28.010 --> 00:15:33.290
We just need to press commando control controllers and to execute to train them all and test it on the

227
00:15:33.290 --> 00:15:37.820
test set and eventually have a look at the number of correct predictions and the number of incorrect

228
00:15:37.820 --> 00:15:40.530
predictions on 200 new reviews.

229
00:15:40.550 --> 00:15:41.600
So let's do it.

230
00:15:41.630 --> 00:15:45.120
I'm going to press him and press enter to execute.

231
00:15:45.340 --> 00:15:48.300
And here we go everything worked properly great.

232
00:15:48.300 --> 00:15:49.780
So let's have a look.

233
00:15:49.810 --> 00:15:54.730
We will have a look at the confusion matrix of course by typing here.

234
00:15:54.730 --> 00:15:56.580
See em in the console.

235
00:15:56.590 --> 00:15:57.780
Here we go.

236
00:15:58.180 --> 00:15:59.230
So let's see what we have.

237
00:15:59.230 --> 00:16:03.610
We have 17:9 correct predictions of negative reviews.

238
00:16:03.610 --> 00:16:06.670
70 correct predictions of positive reviews.

239
00:16:06.670 --> 00:16:14.350
Twenty one incorrect predictions of negative reviews and 30 incorrect predictions of positive reviews.

240
00:16:14.350 --> 00:16:16.250
All right so that's actually not too bad.

241
00:16:16.250 --> 00:16:19.680
No because we only had eight hundred reviews to train the model.

242
00:16:19.690 --> 00:16:24.760
That's not much when you're working with text and therefore 30 plus 21 equals 51.

243
00:16:24.790 --> 00:16:26.890
Incorrect prediction is not bad.

244
00:16:26.890 --> 00:16:33.730
Out of 200 new reviews when you know that you're trend your classification model on only 800 reviews

245
00:16:34.060 --> 00:16:39.280
and actually let's have a look at the accuracy the accuracy is the number of correct predictions.

246
00:16:39.280 --> 00:16:49.320
That is 79 plus 70 divided by the total number of observations in the test set and that is 200.

247
00:16:49.330 --> 00:16:51.330
So let's have a look at the accuracy.

248
00:16:51.370 --> 00:16:56.460
Pressing enter here and the accuracy is seventy four point five percent.

249
00:16:56.470 --> 00:17:02.160
So again that's not bad considering the fact that we trained our model on only 800 reviews and you'll

250
00:17:02.170 --> 00:17:07.930
clearly see that if you had a lot more reviews to train your classification model you will get a much

251
00:17:07.930 --> 00:17:09.670
better accuracy.

252
00:17:09.670 --> 00:17:14.980
All right so that's dead of natural language processing and our congratulations for having completed

253
00:17:14.980 --> 00:17:20.150
all this creating the backwards model training classification model and this dataset.

254
00:17:20.260 --> 00:17:25.330
But that's not the end of your natural language processing journey because right after this video you'll

255
00:17:25.330 --> 00:17:27.200
get a little challenge.

256
00:17:27.370 --> 00:17:29.280
So I will let you find out about that.

257
00:17:29.350 --> 00:17:31.330
And until then enjoy machine learning
