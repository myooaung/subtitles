1
00:00:00,210 --> 00:00:06,900
Hello my friends and welcome to this new practical activity for Decision Tree classification.

2
00:00:06,900 --> 00:00:13,020
Wow we're almost at the end of this part 3 classification and the very thing I suggest is to recap on

3
00:00:13,020 --> 00:00:16,220
what we obtained so far with our previous classifiers.

4
00:00:16,620 --> 00:00:24,150
So with logistic regression we got an accuracy of 89 percent with K and we got the best accuracy of

5
00:00:24,210 --> 00:00:32,550
93 percent with SVM with a linear kernel we got an accuracy of 90 percent with kernel SVM with a nonlinear

6
00:00:32,580 --> 00:00:40,740
kernel we got again inaccuracy of 93 percent and with knife base we got an accuracy of 90 percent therefore

7
00:00:40,830 --> 00:00:45,940
the two best models we had so far are key and and kernel SVM.

8
00:00:46,020 --> 00:00:51,660
And so now the big question is can we still beat that with one of our two classification models that

9
00:00:51,660 --> 00:00:52,490
we have left.

10
00:00:52,590 --> 00:00:59,130
Decision Tree classification or random forest classification and this new section we are about to implement

11
00:00:59,220 --> 00:01:01,110
decision tree classification.

12
00:01:01,230 --> 00:01:02,190
So let's do this.

13
00:01:02,190 --> 00:01:05,980
But first let us make sure everyone here is on the same page.

14
00:01:06,000 --> 00:01:09,140
I gave you the link to this whole folder right before this tutorial.

15
00:01:09,150 --> 00:01:10,030
Any Oracle.

16
00:01:10,030 --> 00:01:11,680
So make sure to connect to it.

17
00:01:11,790 --> 00:01:16,800
And now please follow me into Part 3 classification to implement.

18
00:01:16,800 --> 00:01:20,120
This time the decision tree classification model.

19
00:01:20,340 --> 00:01:22,560
And as usual we're gonna start with python.

20
00:01:22,710 --> 00:01:28,470
This Python folder contains two files the implementation of the decision tree classification model in

21
00:01:28,560 --> 00:01:36,930
IP one format and the same data set social network at that CSB which contains informations of 400 customers

22
00:01:36,930 --> 00:01:43,260
of a car dealership which just released a brand new luxury SUV and the strategy team gathered this data

23
00:01:43,260 --> 00:01:49,980
to understand which customers buy the most the SUV in order to target them with ads that will be posted

24
00:01:50,070 --> 00:01:51,360
on social networks.

25
00:01:51,390 --> 00:01:56,580
So each of these rows correspond to different customers and for each of these customers we have two

26
00:01:56,580 --> 00:02:02,550
features The Age and the estimated salary with which we are going to predict that have been viable purchased

27
00:02:02,850 --> 00:02:09,630
which takes binary values zero meaning that the customer didn't buy the SUV and won meaning that the

28
00:02:09,630 --> 00:02:12,200
customer but the SUV.

29
00:02:12,300 --> 00:02:13,710
All right let's do this.

30
00:02:13,710 --> 00:02:19,530
Let's see if Decision Tree classification can beat the previous accuracies we've got so far meaning

31
00:02:19,530 --> 00:02:20,860
93 percent.

32
00:02:20,880 --> 00:02:24,640
So without further ado let's open that with Google collab.

33
00:02:24,690 --> 00:02:25,850
Choose your favorite.

34
00:02:25,960 --> 00:02:30,590
And I'm gonna put it here because this is our next classification model.

35
00:02:30,630 --> 00:02:31,050
All right.

36
00:02:31,050 --> 00:02:31,560
Perfect.

37
00:02:31,560 --> 00:02:37,230
So once again this implementation results from that classification template we made in the first section

38
00:02:37,230 --> 00:02:43,530
of this part 3 when we implemented logistic regression moral and therefore the only cell that changes

39
00:02:43,650 --> 00:02:52,120
here in its implementation is as usual that cell where we build and train the classification model right.

40
00:02:52,170 --> 00:02:58,380
This cell here and all the rest you know all the other cells are exactly the same as in the logistic

41
00:02:58,710 --> 00:03:02,220
regression code template or you know the classification code template.

42
00:03:02,370 --> 00:03:06,730
Because indeed we use the same variable names classifier and then extraneous test.

43
00:03:06,840 --> 00:03:09,050
Everything is the same except this cell.

44
00:03:09,060 --> 00:03:14,550
So once again we're going to re implement this cell but to do this we need to create a copy because

45
00:03:14,700 --> 00:03:16,980
this is an read only mode.

46
00:03:16,980 --> 00:03:23,290
So right now we're going to create that copy on which we will be able to re implement that cell.

47
00:03:23,310 --> 00:03:23,860
All right.

48
00:03:23,970 --> 00:03:30,180
So let's crawl down again to find that cell where we build and train the dysentery classification model.

49
00:03:30,180 --> 00:03:35,850
Now let's put it in the trash immediately because I don't want you to see you know the name of the class

50
00:03:36,360 --> 00:03:38,730
because indeed I want you to find it on your own.

51
00:03:38,730 --> 00:03:40,170
So there you go.

52
00:03:40,170 --> 00:03:47,460
Now is the time you press pause on this video to find the right class name that allows us to build the

53
00:03:47,460 --> 00:03:49,530
decision tree classification model.

54
00:03:49,620 --> 00:03:55,410
You will have to either look for it directly through an online search or the other option and the one

55
00:03:55,410 --> 00:04:00,640
that I recommend to you because I want you to get familiar with the cyclotron API.

56
00:04:00,660 --> 00:04:04,580
That other option is to look for it inside the cyclone API.

57
00:04:04,600 --> 00:04:10,370
And this is exactly what we're going to do together in two seconds after you press buzz and.

58
00:04:10,650 --> 00:04:11,950
All right let's do this.

59
00:04:12,090 --> 00:04:19,350
So let's go to the cyclone API to find the class that allows us to build a decision tree classification

60
00:04:19,350 --> 00:04:20,070
model.

61
00:04:20,070 --> 00:04:20,390
All right.

62
00:04:20,400 --> 00:04:28,140
So API and by memory you know let's suppose actually that I have no idea of the module or even the class

63
00:04:28,140 --> 00:04:31,190
that builds this decision tree classification model.

64
00:04:31,200 --> 00:04:35,750
So here I'm scrolling down you know to observe the different modules maybe it is.

65
00:04:35,820 --> 00:04:40,650
And symbol methods because you know I know that random forest we built you know the random forest regret

66
00:04:40,650 --> 00:04:47,310
her in part to random forest is an int symbol method but no here we don't see the decision tree so let's

67
00:04:47,580 --> 00:04:50,270
scroll back down again which makes sense right.

68
00:04:50,280 --> 00:04:52,250
Because the decision tree is just a single model.

69
00:04:52,260 --> 00:04:54,800
It is not an ensemble of models.

70
00:04:54,810 --> 00:04:55,230
All right.

71
00:04:55,230 --> 00:04:56,850
So many fall learning metrics.

72
00:04:56,850 --> 00:04:57,970
No no.

73
00:04:57,970 --> 00:05:00,270
You know it's really important that you get familiar with this why.

74
00:05:00,290 --> 00:05:05,120
Because the more you get familiar the more expert you'll become and the better you will juggle with

75
00:05:05,180 --> 00:05:06,340
all the machinery and tools.

76
00:05:06,350 --> 00:05:09,590
You know besides the one I I'm giving you in this course.

77
00:05:09,590 --> 00:05:11,570
All right so here I am scrolling down more.

78
00:05:11,570 --> 00:05:18,200
All right repricing random projection semi supervised learning support vector machines decision trees.

79
00:05:18,200 --> 00:05:19,420
There we go.

80
00:05:19,450 --> 00:05:24,860
You know usually the module you're looking for in the Sigler API will have the same name here as the

81
00:05:24,860 --> 00:05:26,450
model you want to build right.

82
00:05:26,450 --> 00:05:29,250
And you know this is organized in alphabetical order.

83
00:05:29,300 --> 00:05:31,940
So that's why I wanted you to do this exercise.

84
00:05:31,940 --> 00:05:34,900
It's actually very easy to find what you want.

85
00:05:34,940 --> 00:05:36,440
All right decision trees and now.

86
00:05:36,440 --> 00:05:38,630
Which one do we want to get among these.

87
00:05:38,630 --> 00:05:43,220
Well that's of course the first one you know that was the one we used in part two regression but now

88
00:05:43,220 --> 00:05:47,700
we're doing classifications so we want a decision tree classifier class.

89
00:05:47,720 --> 00:05:48,170
All right.

90
00:05:48,230 --> 00:05:50,420
So now why do we have to do first.

91
00:05:50,420 --> 00:05:55,430
Well let's do you know what we have to do anyway which is to import that class.

92
00:05:55,430 --> 00:06:01,220
And so I'm coupling this and then back in my copy we're going to create a new code cell we're going

93
00:06:01,220 --> 00:06:07,790
to paste that and once again we're going to start from the cycle learn and then the tree module by cycle

94
00:06:07,790 --> 00:06:14,110
learn and from that tree module we're going to import that decision tree classifier.

95
00:06:14,210 --> 00:06:15,420
So that's the first step.

96
00:06:15,590 --> 00:06:16,010
Right.

97
00:06:16,010 --> 00:06:21,290
You know it by heart now and now the second step I don't even have to tell you it is of course to create

98
00:06:21,360 --> 00:06:29,140
or a classifier object which will be created as an instance of this decision tree classifier class and

99
00:06:29,150 --> 00:06:33,920
which will represent nothing else than the decision tree classifier model.

100
00:06:33,920 --> 00:06:40,190
All right so here we add some parentheses and now the second question is what do we have to input here

101
00:06:40,460 --> 00:06:46,340
as parameters you know if this decision tree classifier class let's see let's go back to the documentation

102
00:06:46,340 --> 00:06:48,350
the singular an API and.

103
00:06:48,410 --> 00:06:51,660
Well this time we actually have a lot of parameters.

104
00:06:51,680 --> 00:06:53,660
So if you want you can read all of them.

105
00:06:53,670 --> 00:06:58,670
Their description in case you know you want to tune later on a decision tree classification for one

106
00:06:58,670 --> 00:07:00,920
of your future machinery models.

107
00:07:01,010 --> 00:07:06,860
But in our case and this is just in order to be aligned with what you learned in the intuition lecture

108
00:07:06,860 --> 00:07:10,110
by Kiril you know in order for you to be aligned with the theory.

109
00:07:10,220 --> 00:07:17,240
Well we will just change one you know default value of the parameters which is this one criterion.

110
00:07:17,240 --> 00:07:20,740
So the default value of criterion is Ginny.

111
00:07:21,050 --> 00:07:26,180
But with respect to what you learned in the theory with Gabriel's intuition lecture we're going to choose

112
00:07:26,270 --> 00:07:28,240
an entropy criterion.

113
00:07:28,340 --> 00:07:28,610
Right.

114
00:07:28,610 --> 00:07:31,140
This is what you learned in the intuition lecture.

115
00:07:31,160 --> 00:07:36,800
So the criterion is of course the function to measure the quality of a split and that quality is measured

116
00:07:36,830 --> 00:07:39,570
by entropy at least what we're going to choose here.

117
00:07:39,650 --> 00:07:40,040
Okay.

118
00:07:40,040 --> 00:07:43,190
You know remember entropy for the information game.

119
00:07:43,220 --> 00:07:43,520
All right.

120
00:07:43,520 --> 00:07:46,910
So that's the only thing we want to input here all the rest.

121
00:07:46,910 --> 00:07:48,380
We're going to keep busy for values.

122
00:07:48,380 --> 00:07:50,740
Feel free to read them if you need to learn more.

123
00:07:50,780 --> 00:07:52,580
But that's the main parameter.

124
00:07:52,580 --> 00:07:55,130
That's mostly what we have to select here.

125
00:07:55,160 --> 00:08:01,190
And then remember that we will also add a random state parameter to make sure that we have the same

126
00:08:01,190 --> 00:08:03,590
results displayed in our notebook.

127
00:08:03,590 --> 00:08:09,770
All right let's do this criterion equals in quotes and trippy.

128
00:08:09,830 --> 00:08:10,670
Perfect.

129
00:08:10,670 --> 00:08:17,250
And then the second one random state parameter that we set equal to zero.

130
00:08:17,260 --> 00:08:17,930
Great.

131
00:08:17,960 --> 00:08:20,840
And now final step you know exactly what to do.

132
00:08:20,930 --> 00:08:29,570
We take our classifier and from this classifier we call the fit method to train our decision tree classifier

133
00:08:29,870 --> 00:08:39,080
on the training set that is composed as is expected by the fifth method of X train and Y train exactly

134
00:08:39,080 --> 00:08:40,930
the same as before.

135
00:08:40,970 --> 00:08:46,070
And now once again we're done very efficiently with this implementation.

136
00:08:46,070 --> 00:08:47,690
So I can't wait to see results.

137
00:08:47,690 --> 00:08:52,260
I don't think we will beat the accuracy record but let's see we never know.

138
00:08:52,280 --> 00:08:57,800
So let's click this for the button here and then you know right now it is connecting to a runtime to

139
00:08:57,800 --> 00:09:04,130
enable file browsing so that you know we can access your files on your machine and in a second we should

140
00:09:04,130 --> 00:09:05,660
be able to get the upload button.

141
00:09:05,660 --> 00:09:08,690
There we go as resource upload.

142
00:09:08,810 --> 00:09:10,410
And so that's the right data set.

143
00:09:10,430 --> 00:09:12,170
Let me show you the path again.

144
00:09:12,210 --> 00:09:13,790
That's the whole machinery it is at folder.

145
00:09:13,790 --> 00:09:15,370
Please find it on your machine.

146
00:09:15,470 --> 00:09:20,420
And then we're going to go to page three classification then Decision Tree classification in Python

147
00:09:20,540 --> 00:09:22,590
and then social network ads.

148
00:09:22,700 --> 00:09:24,160
That's yes.

149
00:09:24,170 --> 00:09:24,560
All right.

150
00:09:24,580 --> 00:09:25,730
Let's press OK.

151
00:09:25,730 --> 00:09:26,780
And now here we go.

152
00:09:26,780 --> 00:09:32,990
We are ready to run all the sales by clicking this runtime button and then run.

153
00:09:33,030 --> 00:09:34,490
Oh all right.

154
00:09:34,490 --> 00:09:36,720
And nowadays training decisions reclassification moral.

155
00:09:36,720 --> 00:09:39,010
Here we go we have it now.

156
00:09:39,080 --> 00:09:44,540
You know with all the default values of the parameters except criterion which we set equal to entropy

157
00:09:44,990 --> 00:09:46,480
then what about that new result.

158
00:09:46,490 --> 00:09:47,050
Great.

159
00:09:47,060 --> 00:09:52,880
We got the right prediction remember that customer of age 30 an estimated salary eighty seven thousand

160
00:09:52,880 --> 00:09:54,310
dollars didn't buy.

161
00:09:54,320 --> 00:09:58,020
In reality the SUV was predicted not to buy it either.

162
00:09:58,070 --> 00:09:58,960
Perfect.

163
00:09:58,970 --> 00:10:05,600
Then when the test results we indeed get a lot of good predictions except some incorrect ones here for

164
00:10:05,600 --> 00:10:07,330
example and then.

165
00:10:07,340 --> 00:10:08,930
Well it looks actually pretty good.

166
00:10:08,930 --> 00:10:10,820
Maybe you know we will be the accuracy.

167
00:10:10,830 --> 00:10:12,250
That's another one.

168
00:10:12,260 --> 00:10:12,920
All right.

169
00:10:12,950 --> 00:10:14,140
Another one.

170
00:10:14,570 --> 00:10:16,110
And OK let's see.

171
00:10:16,130 --> 00:10:16,390
OK.

172
00:10:16,400 --> 00:10:20,890
Because you know there is actually also when you scroll up some more prediction.

173
00:10:20,900 --> 00:10:21,500
But let's see.

174
00:10:21,500 --> 00:10:23,830
I'm very curious actually maybe I spoke too fast.

175
00:10:24,260 --> 00:10:27,410
We're about to find out right now with the confusion matrix.

176
00:10:27,410 --> 00:10:28,210
I ready.

177
00:10:28,340 --> 00:10:34,640
The accuracy of the decision tree classification model is 91 percent.

178
00:10:34,760 --> 00:10:35,300
Wow.

179
00:10:35,330 --> 00:10:35,660
OK.

180
00:10:35,660 --> 00:10:43,010
So it's actually in the podium you know right after K. and in a kernel SVM which got the best accuracy

181
00:10:43,010 --> 00:10:44,630
of 93 percent.

182
00:10:44,630 --> 00:10:44,960
Wow.

183
00:10:45,000 --> 00:10:45,910
That's really good actually.

184
00:10:45,950 --> 00:10:51,860
This is really a good sign for running forest because random forest is basically a team of decision

185
00:10:51,860 --> 00:10:56,990
trees making the predictions and you know how Team Spirit always improves the results.

186
00:10:56,990 --> 00:11:02,090
So we might have a chance to beat the record accuracy with random forest.

187
00:11:02,090 --> 00:11:03,430
So that's pretty exciting.

188
00:11:03,440 --> 00:11:07,970
And now when visualizing the trends that results which we already got know the execution was not too

189
00:11:07,970 --> 00:11:08,210
long.

190
00:11:08,210 --> 00:11:09,770
Let's see what it looks like.

191
00:11:09,790 --> 00:11:10,910
Wow OK.

192
00:11:10,910 --> 00:11:13,020
So that's pretty different as before.

193
00:11:13,020 --> 00:11:18,980
And no wonder why it got pretty good accuracy because indeed it looks like it was able to catch you

194
00:11:18,980 --> 00:11:24,140
know the little observation points that were really hard to get by either a straight line you know with

195
00:11:24,140 --> 00:11:29,770
linear classifiers or a nice curve like with Colonel SVM or knowledge base.

196
00:11:29,780 --> 00:11:35,420
Here we actually play with this whole grid into smaller sub grids and that's because you know we have

197
00:11:35,450 --> 00:11:39,520
all these splits in the decision tree classification algorithm.

198
00:11:39,560 --> 00:11:44,450
So no wonder why we get all these sub grids and therefore we get separate friction regions.

199
00:11:44,450 --> 00:11:49,200
It's really interesting that captures very well the observation points.

200
00:11:49,280 --> 00:11:53,900
So it catches all the red customers here who didn't buy in reality the SUV.

201
00:11:53,900 --> 00:11:58,570
It catches also all these green customers who but in reality the SUV.

202
00:11:58,700 --> 00:12:05,960
And it catches you know these very hard to catch customers here by creating these sub grids of the grid

203
00:12:06,350 --> 00:12:08,030
with the right predicted regions.

204
00:12:08,030 --> 00:12:10,430
So you see how it got that good accuracy.

205
00:12:10,430 --> 00:12:15,470
It really tried to catch everything even for example these green points that were cut among all these

206
00:12:15,470 --> 00:12:16,090
red points.

207
00:12:16,180 --> 00:12:16,510
OK.

208
00:12:16,520 --> 00:12:18,350
These red customers OK.

209
00:12:18,380 --> 00:12:19,100
But that's.

210
00:12:19,100 --> 00:12:20,000
Be careful.

211
00:12:20,000 --> 00:12:23,780
The training set you know on which the model was trained.

212
00:12:23,870 --> 00:12:25,600
Let's see what happens with the test.

213
00:12:25,610 --> 00:12:29,740
And we already know that we will get good results because we already know that accuracy on the test

214
00:12:29,780 --> 00:12:36,580
is 90 percent but still let's see what we get with new observations on which the model wasn't trained.

215
00:12:36,590 --> 00:12:37,070
All right.

216
00:12:37,070 --> 00:12:40,620
This is what we get and actually here we see things more clearly.

217
00:12:40,760 --> 00:12:45,680
This is the brain region you know which funnily was a good fit for the training said we're here is not

218
00:12:45,680 --> 00:12:46,640
catching anything.

219
00:12:46,660 --> 00:12:52,340
You know neither red customers or green customers here seem to be too incorrect predictions you know

220
00:12:52,340 --> 00:12:55,550
because they fall in the green region then here.

221
00:12:55,550 --> 00:12:56,270
That's all good.

222
00:12:56,270 --> 00:13:01,820
You know that's all the customers of small age and small estimate salary which therefore won't be likely

223
00:13:01,820 --> 00:13:04,510
to buy the SUV as it is the case here.

224
00:13:04,610 --> 00:13:07,600
And then all these green points are correctly predicted.

225
00:13:07,700 --> 00:13:09,470
This one is incorrectly predicted.

226
00:13:09,500 --> 00:13:13,260
So indeed we have our 10 incorrect predictions in all this.

227
00:13:13,430 --> 00:13:14,720
But there you go.

228
00:13:14,740 --> 00:13:19,390
You know if I didn't see the accuracy First I would be afraid that we have some overfishing here.

229
00:13:19,460 --> 00:13:24,080
But no it doesn't seem to be the case even with new observations of the test set.

230
00:13:24,110 --> 00:13:26,590
You know we get great predictions.

231
00:13:27,230 --> 00:13:34,160
But now what I really want to see you know what I really want to find out is the final accuracy of our

232
00:13:34,280 --> 00:13:40,390
final classification model because if we got these great results with a single tree let's see what we're

233
00:13:40,400 --> 00:13:46,070
going to get with a team of trees which is exactly the principle of random forest.

234
00:13:46,070 --> 00:13:50,600
So really I can't wait to see you in the final section of this course.

235
00:13:50,600 --> 00:13:58,080
Random Forest classification because we might have a chance to beat that record accuracy of 93 percent.

236
00:13:58,080 --> 00:14:01,340
Let's find out about this next practical activity.

237
00:14:01,370 --> 00:14:03,200
And until then enjoy machine learning.
