1
00:00:00,180 --> 00:00:02,450
Hello and welcome to this Arta Tauriel.

2
00:00:02,640 --> 00:00:03,420
So that's it.

3
00:00:03,420 --> 00:00:07,710
We did the big first step of natural language processing which consisted of cleaning the text we're

4
00:00:07,710 --> 00:00:08,460
working with.

5
00:00:08,700 --> 00:00:14,040
And now it's time to create the sparse matrix of features containing all the different reviews and the

6
00:00:14,040 --> 00:00:17,590
rows and all the different words of the reviews in the columns.

7
00:00:17,790 --> 00:00:24,600
So as a reminder what we're about to build is a huge table in which the rows of the 1000 reviews so

8
00:00:24,600 --> 00:00:26,920
we're going to have one row for each review.

9
00:00:26,940 --> 00:00:31,860
So we're going to have 1000 rows and then the columns are going to contain all the words we can find

10
00:00:31,860 --> 00:00:36,580
in the 1000 reviews in this corpus that is the 1000 cleaned reviews.

11
00:00:36,840 --> 00:00:42,440
And so basically what this means is that we are going to take all the different words in the 1000 interviews

12
00:00:42,480 --> 00:00:46,850
in the corpus and we're going to create one column for each word.

13
00:00:46,860 --> 00:00:51,960
So suppose we count in total 1500 words in this corpus of reviews.

14
00:00:52,110 --> 00:00:57,040
Well that means there are huge tables going to contain 1500 columns.

15
00:00:57,240 --> 00:01:02,880
And then for each cell in this huge table where then each cell will correspond to one review corresponding

16
00:01:02,880 --> 00:01:05,940
to the row and one word corresponding to the column.

17
00:01:06,090 --> 00:01:11,560
And so the value that will contain the cell is the number of times the word appears in the review.

18
00:01:11,820 --> 00:01:15,820
So as we explained earlier since most of the words don't appear in the reviews.

19
00:01:16,020 --> 00:01:19,010
Well most of the cells will contain a zero.

20
00:01:19,320 --> 00:01:24,680
And then of course we'll get a few ones because each review is composed between five to ten words so

21
00:01:24,810 --> 00:01:30,830
in each row we're going to have five or ten cells having a one and all the other cells will have zero

22
00:01:30,990 --> 00:01:35,590
and the cells that have a one will be in the columns corresponding to the worst on the review.

23
00:01:35,880 --> 00:01:39,880
And maybe sometimes but very rarely will get a two or three.

24
00:01:39,960 --> 00:01:43,930
That happens when the word appears twice or three times in a review.

25
00:01:44,130 --> 00:01:49,620
I can give you a simple example let's imagine that we have a super positive review saying I love this

26
00:01:49,620 --> 00:01:51,170
restaurant very very much.

27
00:01:51,320 --> 00:01:54,580
Well in this review the word very appears twice.

28
00:01:54,750 --> 00:02:00,480
So for this particular review in the table that is let's say it's the row 100 well and the cell that

29
00:02:00,480 --> 00:02:04,600
belongs to this row and that belongs to the column corresponding to the word.

30
00:02:04,600 --> 00:02:09,040
Very well we'll get it to because very appears twice in this review.

31
00:02:09,180 --> 00:02:11,820
So that could happen but it's very rare.

32
00:02:11,910 --> 00:02:18,690
And what's most important to understand is that in this huge table we'll get mostly zeroes a few ones

33
00:02:18,910 --> 00:02:25,860
and a very few twos or threes and we'll get so many zeroes that we call this table a sparse matrix a

34
00:02:25,860 --> 00:02:29,060
sparse matrix is a table that contains a lot of zeroes.

35
00:02:29,220 --> 00:02:34,610
It contains very few non-zero values and that's exactly what we're about to obtain because of what we've

36
00:02:34,620 --> 00:02:35,670
just explained.

37
00:02:36,150 --> 00:02:40,980
And you'll see in the information of the table we build that we have the word sparsity and sparsity

38
00:02:40,980 --> 00:02:44,110
refers to that situation where we have a lot of zeros.

39
00:02:44,400 --> 00:02:49,710
And speaking of sparsity let's also keep in mind that if we cleans all the reviews here in this first

40
00:02:49,710 --> 00:02:54,360
step of natural language processing it's in order to reduce as much as possible.

41
00:02:54,390 --> 00:02:59,420
This future sparsity that will occur in this huge table that we're about to build.

42
00:02:59,460 --> 00:03:05,280
So that's the whole point behind this first step here cleaning the text is to avoid having too much

43
00:03:05,280 --> 00:03:11,700
sparsity that is it's to avoid having a table too big with too many columns because remember one column

44
00:03:11,700 --> 00:03:14,090
is created for each word in the corpus.

45
00:03:14,280 --> 00:03:20,580
So by doing all these steps here we removed a lot of words and a lot of characters punctuations numbers

46
00:03:20,670 --> 00:03:26,550
etc. so that in this final huge table we get the minimum number of words and therefore the minimum number

47
00:03:26,550 --> 00:03:27,370
of columns.

48
00:03:27,630 --> 00:03:33,540
And one last quick reminder we are creating this table in order to have the framework of classification

49
00:03:33,540 --> 00:03:34,220
morals.

50
00:03:34,380 --> 00:03:39,960
That is you know having several independent variables and one dependent variable we haven't created

51
00:03:39,960 --> 00:03:45,570
the dependent variable yet it's actually in this dataset here we will just take the second column of

52
00:03:45,570 --> 00:03:49,970
this dataset because this contains the outcome whether the review is positive or negative.

53
00:03:50,010 --> 00:03:52,770
We can see that here it's the second column light.

54
00:03:52,920 --> 00:03:56,370
One of the reviewers positive and zero reviews negative.

55
00:03:56,370 --> 00:04:03,090
So that's the invariable column and the independent variables are going to be nothing else than these

56
00:04:03,210 --> 00:04:08,610
columns corresponding to each one of the words in the clean reviews of the Corbis because for each review

57
00:04:08,610 --> 00:04:13,950
that is each observation we can link their review to each of the columns because for each of the review

58
00:04:13,950 --> 00:04:20,160
we can associate a value for each of the columns and this value is the number of times the word corresponding

59
00:04:20,160 --> 00:04:22,340
to the column appears in the review.

60
00:04:22,350 --> 00:04:28,080
So that's how we create our independent variables and then we'll create our dependent variable and therefore

61
00:04:28,080 --> 00:04:33,570
we'll get the classification model as we use to work with and eventually we win because we will have

62
00:04:33,570 --> 00:04:36,780
everything we will have our independent variables.

63
00:04:36,780 --> 00:04:42,290
We will have our dependent variable and we already have all our classification models that's the models

64
00:04:42,300 --> 00:04:43,660
we made in part 3.

65
00:04:43,800 --> 00:04:49,550
So we will just need to apply these morals on our new data set that we are about to create that contains

66
00:04:49,550 --> 00:04:54,210
the independent variables after words and that have been invaluable as the like to call them in our

67
00:04:54,210 --> 00:04:55,840
original data sets.

68
00:04:55,920 --> 00:05:01,980
All right so let's do it let's create this table and in our we can do efficiently using a function or

69
00:05:02,080 --> 00:05:07,570
function that is called document or a matrix and it's super easy because this function will only take

70
00:05:07,750 --> 00:05:12,610
one argument and as you might have guessed it's going to be the Corvis and that's it.

71
00:05:12,610 --> 00:05:18,670
This will create this huge sparse matrix with all the 1000 revues in the rows and with all the words

72
00:05:18,670 --> 00:05:20,650
of the reviews and the columns.

73
00:05:20,650 --> 00:05:21,620
So let's do it.

74
00:05:21,670 --> 00:05:25,380
Let's call this sparse matrix of features D.T..

75
00:05:25,460 --> 00:05:29,470
And because the function we're about to use is documentor matrix.

76
00:05:29,500 --> 00:05:37,450
So so far we'll call it DTN so equals and then we use the superfunction document term matrix.

77
00:05:37,450 --> 00:05:38,010
Here it is.

78
00:05:38,020 --> 00:05:39,590
I just need to press enter.

79
00:05:39,790 --> 00:05:45,550
And as I just said we just need to input one argument which is our corpus.

80
00:05:45,550 --> 00:05:46,050
All right.

81
00:05:46,060 --> 00:05:46,680
And that's done.

82
00:05:46,680 --> 00:05:48,100
Here it is Corgies.

83
00:05:48,130 --> 00:05:50,970
This will create our sparse matrix of features.

84
00:05:51,190 --> 00:05:57,880
So I'm going to select this line and execute and done the sparse matrix of features is created it appears

85
00:05:57,880 --> 00:06:02,260
right here Deci and we can click on this button here to have some info.

86
00:06:02,410 --> 00:06:07,540
And actually what's interesting to see now is the total number of words counted in the corpus to create

87
00:06:07,600 --> 00:06:12,480
all the columns and we can see the stall count here 1577.

88
00:06:12,580 --> 00:06:19,540
So that means that the number of columns indicated by and call here in our documentary matrix are sparse

89
00:06:19,550 --> 00:06:23,320
matrix is 1577.

90
00:06:23,320 --> 00:06:26,740
So that means that this huge table has 1000 rows.

91
00:06:26,770 --> 00:06:32,260
So we expected this because of course we have 1000 reviews but we didn't expect the number of columns

92
00:06:32,260 --> 00:06:35,750
in total because simply that was the total number of words in the reviews.

93
00:06:35,800 --> 00:06:40,440
So we can count them but we can see this number here 1577.

94
00:06:40,660 --> 00:06:42,120
So that's a really big table.

95
00:06:42,130 --> 00:06:48,290
But be prepared if you're working with more complicated text or longer text like articles or books.

96
00:06:48,430 --> 00:06:53,130
Well you might get a lot more columns here because you will get a lot more worse.

97
00:06:53,230 --> 00:06:58,900
So what you'll have to do and you can ask me any questions about that in the Q&amp;A is reduced even more

98
00:06:58,900 --> 00:07:02,990
to sparsity by filtering the words in your text.

99
00:07:03,190 --> 00:07:06,560
And speaking of filtering that's what we're going to do right now.

100
00:07:06,760 --> 00:07:13,690
We are going to apply a filter to clean even more the reviews by only considering the most frequent

101
00:07:13,690 --> 00:07:20,620
words that means that it's like we're going to add a step in this text line process which will consist

102
00:07:20,710 --> 00:07:24,340
of only taking the words that are the most frequent.

103
00:07:24,370 --> 00:07:27,420
For example the words that appear in only one review.

104
00:07:27,490 --> 00:07:30,270
Well they might be removed because they're not frequent.

105
00:07:30,340 --> 00:07:36,070
They only appear once only one cell in the matrix contains one because these words only appear in one

106
00:07:36,070 --> 00:07:41,450
review and these words of course are not very relevant because since they only appear in one review.

107
00:07:41,590 --> 00:07:46,450
Well our machine learning classification model will not be able to establish any correlation between

108
00:07:46,450 --> 00:07:51,550
this word and the outcome whether the review is positive or negative because indeed to understand such

109
00:07:51,550 --> 00:07:55,740
correlations the word would need to appear in at least two reviews.

110
00:07:55,750 --> 00:07:58,230
So that's the kind of words we're going to remove.

111
00:07:58,330 --> 00:08:01,200
And again this is in order to reduce sparsity.

112
00:08:01,240 --> 00:08:06,190
And speaking of sparsity I will show you something very interesting right now if we go to the console

113
00:08:06,280 --> 00:08:14,320
and type here ditty and then we'll get other information about this sparse matrix of features and information

114
00:08:14,320 --> 00:08:18,720
that I want to highlight here is of course this sparsity information.

115
00:08:18,910 --> 00:08:24,610
And as you can see the sparsity is 100 percent right now and that's because there are a lot of zeros

116
00:08:24,610 --> 00:08:25,370
in the Matrix.

117
00:08:25,510 --> 00:08:29,300
And also because we haven't filtered and non frequent word yet.

118
00:08:29,410 --> 00:08:30,720
So that's what we'll do right now.

119
00:08:30,730 --> 00:08:36,310
We will filter all the words that appear only once we will filter other words that are not frequent

120
00:08:36,370 --> 00:08:37,510
in the reviews.

121
00:08:37,510 --> 00:08:38,730
All right so let's do it.

122
00:08:38,830 --> 00:08:43,240
To do this we are going to update our documentary matrix.

123
00:08:43,240 --> 00:08:46,020
So we're taking again DTM here.

124
00:08:46,040 --> 00:08:46,430
All right.

125
00:08:46,450 --> 00:08:53,170
Because we're updating our sparse metrics and equals and now we're going to use a function a very practical

126
00:08:53,170 --> 00:09:00,280
function that will filter the non frequent words of our sparse matrix which so far is nothing else than

127
00:09:00,280 --> 00:09:01,110
DTN.

128
00:09:01,180 --> 00:09:07,270
So DTM will be one of the inputs and we will filter all the non frequent words by specifying a proportion

129
00:09:07,270 --> 00:09:12,640
of non frequent words that we want to remove from the sparse matrix and this proportion of non frequent

130
00:09:12,640 --> 00:09:17,800
words will be obtained thanks to the second input of this function because the second input is the percentage

131
00:09:17,800 --> 00:09:21,000
of the most frequent words we want to keep in the reviews.

132
00:09:21,040 --> 00:09:26,930
So let's say you want to keep 99 percent of the words in the review that are the most Rikan words.

133
00:09:27,070 --> 00:09:30,750
Well this second input will take the value of 99 percent.

134
00:09:30,790 --> 00:09:32,140
So let's use this function.

135
00:09:32,140 --> 00:09:35,920
This function is remove sparse terms.

136
00:09:35,920 --> 00:09:36,700
Here it is.

137
00:09:36,700 --> 00:09:37,990
Remove sparse terms.

138
00:09:38,020 --> 00:09:41,790
So pressing enter and ready to input the two arguments.

139
00:09:41,830 --> 00:09:46,390
So the first argument is of course the sparse matrix on which one to apply.

140
00:09:46,390 --> 00:09:53,200
This filtering so of course it did them all right and the second input is the proportion of words that

141
00:09:53,200 --> 00:09:57,170
are the most frequent words and that will be kept in the sparse matrix.

142
00:09:57,220 --> 00:10:03,410
So let's say you want to keep 99 percent the most frequent words well we would need to put here point

143
00:10:03,510 --> 00:10:09,950
ninety nine and therefore we'll build the same sparse matrix but this unconsenting 99 percent of the

144
00:10:09,950 --> 00:10:15,170
words that are the most frequent in this sparse matrix of features and therefore you know we're not

145
00:10:15,170 --> 00:10:19,910
looking at the Corpus containing all the words and counting the most frequent words of this Corvis.

146
00:10:19,910 --> 00:10:25,130
What this function remove sparse terms will do is to look at all the columns of the sparse matrix here

147
00:10:25,370 --> 00:10:31,880
and then keep 99 percent of the columns that have the most ones in the columns because each column corresponds

148
00:10:31,880 --> 00:10:37,130
to a word and therefore when there are very few ones in the columns that means that this word appears

149
00:10:37,130 --> 00:10:42,710
in very few reviews and therefore these other words that are not frequent in the reviews and accordingly

150
00:10:42,740 --> 00:10:43,780
not relevant.

151
00:10:43,940 --> 00:10:45,920
And that's why we can remove them.

152
00:10:46,310 --> 00:10:49,650
All right so let's do it let's apply the filter to be cautious.

153
00:10:49,650 --> 00:10:55,850
Let's maybe take a higher proportion of freek'n words we keep because actually with 99 percent we might

154
00:10:55,850 --> 00:10:56,810
remove a lot of words.

155
00:10:56,810 --> 00:10:59,290
You can try it on your our studio to see.

156
00:10:59,480 --> 00:11:04,940
But here since we don't have many reviews you know we have 1000 reviews that is not much compared to

157
00:11:04,940 --> 00:11:08,110
other text we can work with in natural language processing.

158
00:11:08,150 --> 00:11:09,670
Let's be careful here and apply.

159
00:11:09,680 --> 00:11:16,100
And ninety nine point nine percent proportion of freek'n words are him going to add a 9 and you'll see

160
00:11:16,100 --> 00:11:18,970
that it will already remove quite a lot of words.

161
00:11:18,980 --> 00:11:20,510
So let's try it.

162
00:11:20,510 --> 00:11:28,670
I'm going to select this and execute and indeed as you can see we now have 691 columns in the sparse

163
00:11:28,670 --> 00:11:32,750
matrix that is we only kept 691 words.

164
00:11:32,870 --> 00:11:37,460
So clearly we can see that by keeping ninety nine point nine percent of the words that are the most

165
00:11:37,460 --> 00:11:38,330
frequent.

166
00:11:38,330 --> 00:11:45,560
Well that already removes almost 1000 words because originally we had remember more than 1500 words.

167
00:11:45,620 --> 00:11:46,950
So be careful with this.

168
00:11:46,970 --> 00:11:53,480
Be careful not to apply a too low proportion of frequent words you want to keep and to choose that remember

169
00:11:53,480 --> 00:11:58,340
to look at the total number of words that is counted when you build this first parse matrix.

170
00:11:58,340 --> 00:12:02,780
And of course you can also choose this number by considering the total number of reviews you have in

171
00:12:02,780 --> 00:12:04,360
your original data set.

172
00:12:04,460 --> 00:12:06,990
And you know since we only had 1000 reviews.

173
00:12:07,190 --> 00:12:10,110
Well that's why we take such a high proportion here.

174
00:12:10,370 --> 00:12:12,660
And let's see by how much reduce the sparsity.

175
00:12:12,680 --> 00:12:19,190
So we have two tight deci again because our documents force matrix that is our sparse matrix was just

176
00:12:19,190 --> 00:12:21,400
a data with all these words removed.

177
00:12:21,530 --> 00:12:27,260
So pressing enter here and the sparsity now became 99 percent so better.

178
00:12:27,260 --> 00:12:30,650
But anyway that was fine because we didn't have too much columns.

179
00:12:30,650 --> 00:12:35,420
You will see that if you work with larger text you will get a lot more words and therefore a lot more

180
00:12:35,420 --> 00:12:36,500
columns.

181
00:12:36,500 --> 00:12:38,780
All right so that will be all for this to Tauriel.

182
00:12:38,780 --> 00:12:40,870
We built our bag over words.

183
00:12:40,880 --> 00:12:42,290
Congratulations for that.

184
00:12:42,500 --> 00:12:45,330
And now it's time to make the classification model.

185
00:12:45,410 --> 00:12:48,840
So that's what we'll do in the next and final tutorial section.

186
00:12:48,860 --> 00:12:50,540
And until then and let me see learning
