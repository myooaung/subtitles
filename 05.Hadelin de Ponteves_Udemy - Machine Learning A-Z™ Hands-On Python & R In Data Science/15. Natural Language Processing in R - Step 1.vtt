WEBVTT
1
00:00:00.330 --> 00:00:06.320
Hello and welcome to this Arta Tauriel and mostly welcome to part Seven Natural Language Processing.

2
00:00:06.540 --> 00:00:08.820
So what does natural language processing all about.

3
00:00:09.000 --> 00:00:11.430
Well it's about analyzing text.

4
00:00:11.430 --> 00:00:18.840
These texts can be books reviews some H-G male web pages that you extract from crapping all sorts of

5
00:00:18.840 --> 00:00:24.900
text and therefore natural language processing is a branch of machine learning where we do some predictive

6
00:00:24.900 --> 00:00:27.700
analysis on text mostly.

7
00:00:27.720 --> 00:00:33.460
So in this part these texts are going to be reviews you know written reviews of restaurants.

8
00:00:33.660 --> 00:00:36.550
And so we will make some machinery models that will predict.

9
00:00:36.600 --> 00:00:39.340
If the review is positive or negative.

10
00:00:39.510 --> 00:00:44.520
So this is a simple example of the application of natural language processing.

11
00:00:44.700 --> 00:00:50.070
But the algorithm will make in this part will be very well applicable to other kinds of text you know

12
00:00:50.070 --> 00:00:55.320
you'll be able to apply this on books for example to predict the job of a book you know whether a book

13
00:00:55.320 --> 00:00:58.680
is a thriller a comedy or a romance.

14
00:00:58.680 --> 00:01:04.500
You'll be also able to use it on HD male web pages to do whatever kind of analysis you want to do on

15
00:01:04.500 --> 00:01:05.490
this pages.

16
00:01:05.550 --> 00:01:11.030
You can also play it on newspapers you know to predict in which category an article belongs to.

17
00:01:11.200 --> 00:01:16.960
Well you'll see that will make a general model that you'll be able to apply on most of the attacks.

18
00:01:17.100 --> 00:01:21.480
And of course if you need to apply this on a more complicated text you can ask me some questions in

19
00:01:21.480 --> 00:01:27.160
the Q&amp;A and I'll tell you what to add to make this code work properly for your problem your text.

20
00:01:27.360 --> 00:01:27.980
OK.

21
00:01:28.170 --> 00:01:33.840
So let's start this let's start our algorithm and if we're starting with the first code section let's

22
00:01:33.840 --> 00:01:34.970
do the basic step.

23
00:01:35.130 --> 00:01:40.510
Let's set the rifle that was working directories so we'll get to our machine or in a folder then part

24
00:01:40.510 --> 00:01:46.380
7 natural language processing so congratulations for reaching the parts you are now entering into a

25
00:01:46.380 --> 00:01:49.270
very useful and exciting branch of machine learning.

26
00:01:49.290 --> 00:01:53.410
So let's go in there and now we have to be D.S. natural language processing.

27
00:01:53.430 --> 00:01:54.450
So here we go.

28
00:01:54.450 --> 00:01:56.960
That's the father we want to set as a working directory.

29
00:01:57.090 --> 00:02:02.030
So now we'll click on this more button here and then set as a working directory.

30
00:02:02.310 --> 00:02:03.110
All good.

31
00:02:03.300 --> 00:02:08.340
And now as you can notice in this full of the widget set it is working directory.

32
00:02:08.460 --> 00:02:10.590
We have two data files.

33
00:02:10.620 --> 00:02:16.910
We have a restaurant reviews that says the file and restaurant reviews that t as we follow.

34
00:02:17.040 --> 00:02:21.810
So I left these two data files on purpose because there is an important thing to understand when we

35
00:02:21.810 --> 00:02:26.250
prepare text datasets that I would like to highlight and show you right now.

36
00:02:26.250 --> 00:02:31.590
So I'm going to go to my folder on my computer and we'll have a look at these two files from there.

37
00:02:31.590 --> 00:02:34.890
So right now I'm going to my folder here it is.

38
00:02:35.040 --> 00:02:38.230
And let's open the two restoral of UCSC.

39
00:02:38.340 --> 00:02:41.160
So open with text in it.

40
00:02:41.190 --> 00:02:41.890
Here we go.

41
00:02:41.940 --> 00:02:45.010
And restaurant reviews dot TSB.

42
00:02:45.290 --> 00:02:49.610
So same opened with text at it.

43
00:02:49.650 --> 00:02:56.100
So that's the CSB and that's the TSB that's for us have a look at the CSC as you can see the first line

44
00:02:56.100 --> 00:03:00.900
here is the title of the future columns we're going to have in our studio.

45
00:03:01.110 --> 00:03:07.890
The first column is Review and the second column is light and I can see that because these two terms

46
00:03:07.890 --> 00:03:14.250
here are separated by a comma and this is a C as we found meaning that all the columns are separated

47
00:03:14.340 --> 00:03:15.990
by a comma.

48
00:03:16.050 --> 00:03:19.060
So that's the first line containing the titles of the columns.

49
00:03:19.320 --> 00:03:22.740
And then we have all our observations.

50
00:03:22.740 --> 00:03:29.850
So one line corresponds to one observation and as you can see in each of these lines we first have the

51
00:03:29.850 --> 00:03:30.380
review.

52
00:03:30.420 --> 00:03:33.150
So this is of course a review of a restaurant.

53
00:03:33.210 --> 00:03:34.220
So wow.

54
00:03:34.230 --> 00:03:39.810
Love this place meaning of course that the reviews are positive and therefore in the second column The

55
00:03:39.810 --> 00:03:45.450
liked column we have a one here meaning that the review is indeed positive.

56
00:03:45.750 --> 00:03:47.300
So the variable for this.

57
00:03:47.300 --> 00:03:54.360
Like column can take two values 1 or 0 and 1 means that it's a positive review and 0 means that it's

58
00:03:54.360 --> 00:03:55.880
a negative review.

59
00:03:55.890 --> 00:03:59.840
So indeed as you can see in the second review crust is not good.

60
00:04:00.000 --> 00:04:04.110
Well of course that's a negative review and therefore there is a zero here.

61
00:04:04.110 --> 00:04:10.050
All right so that's actually the kind of file We're used to because since the beginning of this course

62
00:04:10.050 --> 00:04:15.810
we've only been using some see as we fell where the columns are separated by a comma.

63
00:04:16.200 --> 00:04:19.160
But here we have something different.

64
00:04:19.290 --> 00:04:21.600
We can see that we have the same columns.

65
00:04:21.610 --> 00:04:27.060
First column is review second coming to light and we have the same reviews so these are exactly the

66
00:04:27.060 --> 00:04:29.420
same data sets with the same data.

67
00:04:29.610 --> 00:04:31.500
But there is one major difference.

68
00:04:31.530 --> 00:04:35.120
And as you might have guessed this difference is the delimiter.

69
00:04:35.400 --> 00:04:37.760
And this fall here the delimiter is a comma.

70
00:04:37.830 --> 00:04:40.290
So that's the delimiter separating the two columns.

71
00:04:40.290 --> 00:04:48.240
And in this file the delimiter is a tab and that's why we call it T as V tap separated values vs says

72
00:04:48.240 --> 00:04:50.730
Vee comma separated values.

73
00:04:51.150 --> 00:04:58.320
And so now according to you which one should we choose for our future algorithm you know we'll have

74
00:04:58.520 --> 00:05:03.660
a machine learning algorithm analyzing all the reviews here and then the goal of this algorithm will

75
00:05:03.660 --> 00:05:07.340
be to predict whether the review is positive or negative.

76
00:05:07.350 --> 00:05:07.860
All right.

77
00:05:07.860 --> 00:05:13.870
But now the question is do we need a data set where the columns are separated by a comma or by a tab.

78
00:05:14.070 --> 00:05:17.850
Well the answer is as you might have guessed by a tab.

79
00:05:17.940 --> 00:05:19.010
And why is that.

80
00:05:19.070 --> 00:05:22.910
It's because we already have some callers in the reviews itself.

81
00:05:23.190 --> 00:05:27.860
Well for example this one this review is the food corner.

82
00:05:28.020 --> 00:05:29.200
Amazing.

83
00:05:29.250 --> 00:05:33.360
So if we use our A C as we fell where the delimiter is a comma.

84
00:05:33.510 --> 00:05:39.540
Well we'll have a problem for this review here because for this particular observation the first column

85
00:05:39.540 --> 00:05:41.440
will contain the food here.

86
00:05:41.550 --> 00:05:48.810
So our will think it's a review the food and the second column will not be one here but amazing because

87
00:05:48.810 --> 00:05:53.910
there is this coming here that is taken for the delimiter and therefore it will separate the food and

88
00:05:54.000 --> 00:06:00.480
amazing and therefore what will happen to one it will go to the next observation and therefore one will

89
00:06:00.480 --> 00:06:02.180
be taken for a new review.

90
00:06:02.490 --> 00:06:08.580
So that will not make any sense and this will mess up with the whole algorithm and that's why it's way

91
00:06:08.580 --> 00:06:15.480
better to take tabs here because you know when people write reviews they do not put tabs in the review.

92
00:06:15.510 --> 00:06:16.770
That would be very rare.

93
00:06:16.900 --> 00:06:23.370
They would put come up very easily as we can see for this particular review and we can find other reviews

94
00:06:23.370 --> 00:06:24.760
with Gummer's I'm sure of it.

95
00:06:24.930 --> 00:06:26.790
Yes indeed we have another one here.

96
00:06:26.790 --> 00:06:28.570
This place is not worth your time.

97
00:06:28.590 --> 00:06:31.050
Come up let alone Vegas.

98
00:06:31.170 --> 00:06:37.080
So it's very natural to put some comers in the reviews but not much natural to put some tabs in the

99
00:06:37.080 --> 00:06:43.080
reviews and besides if you press on time when you're writing a review Well this will get to the next

100
00:06:43.530 --> 00:06:49.670
you know button to like Submit your review or something else but by pressing the top button when you're

101
00:06:49.680 --> 00:06:54.480
writing your review you would get out of the review and you would not be able to continue to write it.

102
00:06:54.570 --> 00:07:01.350
So we will never find a tab in the review and that's why we will never have this problem of getting

103
00:07:01.350 --> 00:07:07.060
these anomalies due to duplicate delimiters in one specific review.

104
00:07:07.440 --> 00:07:13.980
So I really recommend to prepare your text data sets this way with a tap separator because you will

105
00:07:13.980 --> 00:07:15.840
never have that kind of problem.

106
00:07:15.840 --> 00:07:21.870
One other solution if you really want to use this yes we will be to include some double quotes here.

107
00:07:21.870 --> 00:07:26.400
One of the left of the review and one at the right but you would still get some problems in case you

108
00:07:26.400 --> 00:07:29.740
have some double quotes and the reviews itself.

109
00:07:29.880 --> 00:07:31.260
I'm sure we can find one.

110
00:07:31.530 --> 00:07:35.410
Let's have a look at the press command to find a double quote.

111
00:07:35.750 --> 00:07:36.650
Here we are.

112
00:07:36.990 --> 00:07:39.120
And that's exactly what I'm talking about.

113
00:07:39.120 --> 00:07:41.750
For example let's have a look at this review here.

114
00:07:41.940 --> 00:07:44.680
The description said yum yum sauce.

115
00:07:44.710 --> 00:07:49.300
Well that's because this person here is quoting a description found somewhere.

116
00:07:49.410 --> 00:07:52.650
And so since it's occurring it's using some double quotes here.

117
00:07:52.650 --> 00:07:53.720
Yum yum sauce.

118
00:07:53.880 --> 00:07:55.110
And another one here.

119
00:07:55.110 --> 00:08:01.580
So even if you put some double quotes to separate your reviews from the result that is one or zero.

120
00:08:01.710 --> 00:08:07.410
Well you would still have this kind of problem whereas if you separate your review in the light Voivode

121
00:08:07.410 --> 00:08:13.300
by a tab you will never get this kind of problem because no one will press tab by writing a review.

122
00:08:13.500 --> 00:08:20.990
So definitely that's the one we'll go for restaurant underscore reviews Dot T S V tab separated values

123
00:08:21.330 --> 00:08:26.700
and by the way this is a data set taken from a paper from group to individual labels using deep features

124
00:08:26.880 --> 00:08:28.500
by Coachy us.

125
00:08:28.860 --> 00:08:35.790
So we will use that he said and this contains 1000 reviews for each of the review we have the real results

126
00:08:35.970 --> 00:08:37.740
0 or 1.

127
00:08:37.770 --> 00:08:43.560
So let's do it let's start implementing our algorithm and we will start with the first step there is

128
00:08:43.560 --> 00:08:50.550
to import this data set restaurant reviews tears V into our studio so let's do it.

129
00:08:50.580 --> 00:08:55.180
Let's close this and let's go back to our studio.

130
00:08:55.650 --> 00:08:58.180
All right so now let's import the data set.

131
00:08:58.260 --> 00:09:06.060
So as usual we're going to call our data set data set this way and then equals and then that's where

132
00:09:06.210 --> 00:09:09.220
we use a function to import the data set.

133
00:09:09.390 --> 00:09:16.680
However so far we've been using the reduction function to import our data sets because simply our data

134
00:09:16.680 --> 00:09:18.380
sets where C has the files.

135
00:09:18.510 --> 00:09:24.140
But as we just understood this time we're not dealing with a C as we file we are dealing with a TSB

136
00:09:24.170 --> 00:09:24.810
file.

137
00:09:24.840 --> 00:09:33.210
So of course things might be different now but we will still type read and see as we hear and then some

138
00:09:33.210 --> 00:09:39.840
parenthesis and then let's press on here to get some info about this readout function.

139
00:09:39.840 --> 00:09:41.590
So what do we see first.

140
00:09:41.700 --> 00:09:45.050
We see that we don't only have one import function.

141
00:09:45.450 --> 00:09:50.890
Indeed we can see here that we have to read that table function which we haven't used yet.

142
00:09:51.330 --> 00:09:56.630
The reduction function which is the function we've been using since the beginning of this course.

143
00:09:56.730 --> 00:10:03.160
Then we also have the readouts with two function which is the same as this one with the only difference

144
00:10:03.160 --> 00:10:05.110
that the default separator.

145
00:10:05.170 --> 00:10:11.950
You know the delimiter that is separating your columns is a semicolon instead of a comma as the default

146
00:10:11.950 --> 00:10:14.350
parameter for the reader is the function.

147
00:10:14.350 --> 00:10:16.810
So that's the main difference between the two.

148
00:10:17.020 --> 00:10:21.910
But that's not what we are interested in right now because we would like to use a function where the

149
00:10:21.910 --> 00:10:26.890
default parameter for the separator is a tab and not a semicolon.

150
00:10:26.890 --> 00:10:33.850
We could still use this readout function and change the set parameter but you know let's use another

151
00:10:33.850 --> 00:10:38.400
function for wants to import the dataset with the right default parameter.

152
00:10:38.620 --> 00:10:42.950
And speaking of the default parameter which should be the tab separator.

153
00:10:43.180 --> 00:10:49.820
Well that's actually the next import function which is the read that the lym function.

154
00:10:49.960 --> 00:10:57.160
Indeed you can see here that the default parameter for the separator is a tab slushed here means tab

155
00:10:57.160 --> 00:10:57.520
actually.

156
00:10:57.520 --> 00:11:00.630
So that's exactly the function we want.

157
00:11:00.640 --> 00:11:07.090
That's the best function to use for data set right now because our dataset contains columns separated

158
00:11:07.090 --> 00:11:08.140
by type.

159
00:11:08.200 --> 00:11:10.340
So that's the functional use.

160
00:11:10.510 --> 00:11:17.890
So I'm going to remove Here read that says as and replace it by read out the LEM.

161
00:11:17.890 --> 00:11:18.630
Here we go.

162
00:11:18.640 --> 00:11:21.000
And now we input the parameters.

163
00:11:21.130 --> 00:11:24.240
So it's the same principle as for retouches V.

164
00:11:24.310 --> 00:11:31.540
We of course need to input first the dataset in quotes and the data set is called restaurant

165
00:11:33.880 --> 00:11:37.010
reviews don't t as read.

166
00:11:37.120 --> 00:11:41.930
So we need to specify this because indeed in our working directory folder we have the two files she

167
00:11:42.040 --> 00:11:45.000
has me in tears so we need to specify here TSB.

168
00:11:45.100 --> 00:11:50.830
That's the first parameter and then we have some other parameters like this had a parameter here which

169
00:11:50.980 --> 00:11:56.800
is by default equals to true meaning that it considers the first line of our data set as the titles

170
00:11:56.800 --> 00:12:04.030
of the columns which is the case for our data set because remember the first line is Review tab liked

171
00:12:04.360 --> 00:12:08.670
and review is the title of the first column and liked is the title of the second column.

172
00:12:08.680 --> 00:12:10.510
So we are good with this head of parameter.

173
00:12:10.570 --> 00:12:17.470
So we don't need to put this and same for this next parameter set because by default the parameter for

174
00:12:17.470 --> 00:12:20.720
the separator is a tab and that's exactly what we need right now.

175
00:12:20.860 --> 00:12:27.070
And then we have this parameter quote and that's a very useful parameter to input for natural language

176
00:12:27.070 --> 00:12:31.300
processing because most of the time you'll find some quotes.

177
00:12:31.300 --> 00:12:37.540
Most of the time the above quotes in your text we checked that we had some in our reviews and so we

178
00:12:37.540 --> 00:12:42.160
need to ignore these quotes because we don't want to have some kind of misinterpretation.

179
00:12:42.160 --> 00:12:45.120
When I read that Dilling function reads all the reviews.

180
00:12:45.190 --> 00:12:49.530
So in general and natural language processing it's better to ignore any kind of quote.

181
00:12:49.540 --> 00:12:52.440
We did exactly the same in Python and everything went well.

182
00:12:52.480 --> 00:12:53.800
So we'll do the same here.

183
00:12:53.800 --> 00:13:03.120
And to do this we add this quote parameter and we said it equals to actually nothing in quotes.

184
00:13:03.210 --> 00:13:08.890
You know by putting nothing in this quote here that means that it's ignoring any kind of quotes in the

185
00:13:08.890 --> 00:13:09.550
text.

186
00:13:09.580 --> 00:13:10.410
So that's good.

187
00:13:10.570 --> 00:13:19.250
And now we'll add a last parameter that is not specified here and which is the strings as factors parameter.

188
00:13:19.360 --> 00:13:21.210
And what is this parameter used for.

189
00:13:21.370 --> 00:13:27.610
Well you know the first column of our dataset contains the written reviews and you know when or when

190
00:13:27.610 --> 00:13:33.130
we're making some classification models which will be what we'll be doing here in natural language processing

191
00:13:33.130 --> 00:13:38.180
because basically we'll be classifying your reviews and tell whether they are positive or negative.

192
00:13:38.440 --> 00:13:40.030
So that's classification.

193
00:13:40.210 --> 00:13:44.740
And you know when we're doing some classification models and working with some categorical variables

194
00:13:45.100 --> 00:13:50.920
Well remember we use this factor function to specify the categorical variables as factors.

195
00:13:51.190 --> 00:13:53.550
And you know right now we have some reviews.

196
00:13:53.650 --> 00:13:56.420
And since in some way it's not a numeric variable.

197
00:13:56.590 --> 00:13:59.380
You know taking some continuous real values.

198
00:13:59.380 --> 00:14:04.450
Well in some way it can be considered as a categorical variable having some different factors but in

199
00:14:04.450 --> 00:14:08.990
natural language processing we must not identify the reviews as factors in our.

200
00:14:09.160 --> 00:14:14.140
And that's because we will analyze the inside of the reviews because we'll be analyzing the different

201
00:14:14.140 --> 00:14:18.970
words of the review to understand the correlations between the presence of the words and the result

202
00:14:18.970 --> 00:14:21.190
whether to reduce positive or negative.

203
00:14:21.190 --> 00:14:27.630
So since we'll drill into the review and analyze its content Well we must not specify the review as

204
00:14:27.640 --> 00:14:33.940
factors as if it was a single entity because that's what a factor would be a single entity having a

205
00:14:33.940 --> 00:14:38.350
single meaning regardless of the different meanings of the different words of the review.

206
00:14:38.680 --> 00:14:42.440
And so to prevent from identifying those reviews as factors.

207
00:14:42.550 --> 00:14:47.980
Well what we need to do is add this other parameter which is the strain as factors parameter.

208
00:14:47.980 --> 00:14:48.730
Here it is.

209
00:14:48.730 --> 00:14:50.540
I just need to press enter.

210
00:14:50.830 --> 00:14:58.480
And now we just need to input false like this in quotes and that will not identify the reviews as factors

211
00:14:58.960 --> 00:15:05.830
and that's all that's how we should import this text file you know using the redeliver import function

212
00:15:05.830 --> 00:15:12.160
to import a T as we file by default and then add this quote parameter to ignore the quotes and then

213
00:15:12.160 --> 00:15:17.560
add the string as factors parameter to prevent us from identifying the reviews as factors.

214
00:15:17.560 --> 00:15:18.900
All right so let's do it.

215
00:15:19.060 --> 00:15:22.550
Let's select this line of code and execute.

216
00:15:22.600 --> 00:15:23.050
All right.

217
00:15:23.060 --> 00:15:25.240
All good our data set is important.

218
00:15:25.450 --> 00:15:31.930
As you can see it has 1000 observations that means that the cut between the review column and the liked

219
00:15:31.930 --> 00:15:34.900
column was done very properly without any issue.

220
00:15:35.020 --> 00:15:38.650
So now let's open our data sets and let's have a look.

221
00:15:38.770 --> 00:15:45.670
As you can see all the reviews are very well separated to their verdict whether it's a positive or negative

222
00:15:45.670 --> 00:15:46.320
review.

223
00:15:46.540 --> 00:15:52.690
And so everything here looks great and we need to make sure that we have our 1000 reviews where we can

224
00:15:52.810 --> 00:15:53.950
see that here very easily.

225
00:15:53.950 --> 00:15:59.740
But you know we have our 1000 reviews and when I scroll up we can see that all the reviews are well

226
00:15:59.920 --> 00:16:07.310
in the review column and all the light results 0 or 1 are well and light colored.

227
00:16:07.310 --> 00:16:15.080
Here you see if I scroll up we don't have any review in the like column or a 1 or 0 in the review column.

228
00:16:15.160 --> 00:16:20.770
So everything looks great we are ready to move on to the next step which will be to clean the different

229
00:16:20.770 --> 00:16:21.650
reviews.

230
00:16:21.670 --> 00:16:27.520
There is a compulsory step in natural language processing which consists of cleaning the text to make

231
00:16:27.520 --> 00:16:31.500
it ready for our future machine learning algorithms.

232
00:16:31.540 --> 00:16:32.920
So that's what we'll do in the next story.

233
00:16:33.080 --> 00:16:35.180
Well and until then enjoy machine learning
