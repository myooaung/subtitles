WEBVTT
1
00:00:00.480 --> 00:00:05.910
Hello and welcome to our tutorial soon to produce tutorials we take care of the preprocessing phase

2
00:00:05.970 --> 00:00:13.020
and then we apply PCA on our data set to reduce its dimensionality down to two new extracted features

3
00:00:13.380 --> 00:00:16.250
and now we are ready to build a classification model.

4
00:00:16.530 --> 00:00:22.320
So speaking of classification model we started with a logistic regression model but actually from this

5
00:00:22.320 --> 00:00:28.290
point we can build any classification model among all the classification models we made in part 3.

6
00:00:28.320 --> 00:00:34.410
If I go back to part three classification this folder here you have here all the models we made in this

7
00:00:34.410 --> 00:00:35.250
part 3.

8
00:00:35.310 --> 00:00:41.550
And basically from this point you can build any model you want by just selecting your classification

9
00:00:41.550 --> 00:00:41.850
model.

10
00:00:41.850 --> 00:00:46.240
For example let's take support vaccination classification model.

11
00:00:46.440 --> 00:00:53.400
Then you can just open the SVM file and then basically all you need to do is take everything after the

12
00:00:53.400 --> 00:01:00.120
data processing phase that is from where you started to build your model and select everything down

13
00:01:00.120 --> 00:01:07.890
to the bottom and copy and then your PC a file you can include your classification model right after

14
00:01:07.890 --> 00:01:10.340
applying PCA on your dataset.

15
00:01:10.350 --> 00:01:17.040
And so here I just replaced the logistic regression model by the SVM model and you can do this for any

16
00:01:17.040 --> 00:01:20.580
classification model you want among the classification models.

17
00:01:20.580 --> 00:01:21.890
We made it by three.

18
00:01:22.080 --> 00:01:27.000
So that's very easy to replace your different models by this simple copy paste so that you can try different

19
00:01:27.000 --> 00:01:29.090
classification models very efficiently.

20
00:01:29.430 --> 00:01:31.410
So let's see what we get with this as a model.

21
00:01:31.410 --> 00:01:37.620
For example we just need to change here the name of the dependent Roybal does not purchased but customer

22
00:01:37.620 --> 00:01:38.310
segment.

23
00:01:40.170 --> 00:01:41.040
Here we go.

24
00:01:41.400 --> 00:01:46.650
And that's the only thing we need to change because the data here has embittering sets and this is the

25
00:01:46.650 --> 00:01:51.540
trend is that we just transformed the new training set composed of the new extracted features.

26
00:01:51.540 --> 00:01:59.930
And so basically we are ready to select the section and execute it to build our classification model.

27
00:02:00.210 --> 00:02:05.430
And now that the model is built we are ready to predict the new observations of the test set.

28
00:02:05.490 --> 00:02:06.990
And so this line is ready.

29
00:02:07.140 --> 00:02:12.510
And actually we don't need to change anything because this index 3 here is the index of the dependent

30
00:02:12.510 --> 00:02:13.130
variable.

31
00:02:13.230 --> 00:02:18.180
And since we reduced the dimensionality of our data set down to 2 that means that we have two features

32
00:02:18.210 --> 00:02:19.380
and one happened.

33
00:02:19.620 --> 00:02:22.700
And therefore the index of the dependent variable is still 3.

34
00:02:22.860 --> 00:02:25.910
And so we are ready to select this line and execute.

35
00:02:26.130 --> 00:02:30.790
And now we have the predictions the test that results so we can have a look.

36
00:02:30.870 --> 00:02:33.900
Why pred in the console press enter.

37
00:02:34.230 --> 00:02:39.840
And for each observation of the test set we have it's prediction by the model the SVM model.

38
00:02:39.840 --> 00:02:45.930
So for example the fourth line of the dataset that belongs to the test set is predicted to belong to

39
00:02:45.930 --> 00:02:53.090
customer number one and the one number 132 is predicted to belong to customer segment number 3.

40
00:02:53.130 --> 00:02:53.970
So very easy.

41
00:02:53.970 --> 00:02:59.820
And then we can make the confusion matrix and say we don't need to change anything here because this

42
00:02:59.820 --> 00:03:02.330
corresponds to the index of the dependent variable.

43
00:03:02.400 --> 00:03:04.930
So we are ready to execute this as well.

44
00:03:05.050 --> 00:03:12.190
Executes the confusion matrix is ready let's have a look and well perfect results.

45
00:03:12.210 --> 00:03:18.090
We only get correct predictions as you can see 12 ones were correctly predicted to belong to customer

46
00:03:18.090 --> 00:03:23.010
segment number one 14 ones were correctly predicted to be on to customer segment the 2.

47
00:03:23.130 --> 00:03:26.860
And then once were correctly predicted to belong to customer segment number three.

48
00:03:27.180 --> 00:03:30.110
And then we have zero incorrect predictions.

49
00:03:30.120 --> 00:03:31.700
So these are excellent results.

50
00:03:31.710 --> 00:03:34.810
And of course we get 100 percent accuracy.

51
00:03:34.950 --> 00:03:37.140
So now we're moving onto the next part.

52
00:03:37.140 --> 00:03:43.320
To visualize the 2010 results we should get amazingly well separated prediction regions and a very clear

53
00:03:43.320 --> 00:03:44.490
prediction boundary.

54
00:03:44.490 --> 00:03:45.570
So let's check it out.

55
00:03:45.780 --> 00:03:52.410
But now we have something to change and this is not a tiny change as we used to do because now we have

56
00:03:52.410 --> 00:03:53.410
three classes.

57
00:03:53.550 --> 00:03:59.310
And as you can notice in this code when we plot the prediction regions thanks to this line well this

58
00:03:59.310 --> 00:04:05.010
code template allows us to do it when we only have two classes because as you can see we have this if

59
00:04:05.010 --> 00:04:12.390
else condition if Weigelt equals one then the color is green and else if we really call 0 then the call

60
00:04:12.390 --> 00:04:13.360
is tomato.

61
00:04:13.500 --> 00:04:15.310
And so when we plot observations.

62
00:04:15.390 --> 00:04:21.580
If the observations of the set that is a trend set belongs to class 1 then it's green.

63
00:04:21.720 --> 00:04:24.960
And if it belongs to zero that is an else condition.

64
00:04:25.050 --> 00:04:30.530
The point will be red but now the problem is that we have three classes so we need to improve this code

65
00:04:30.540 --> 00:04:38.790
here two distinct the three conditions if one equals zero if Waco's one and if y to.

66
00:04:39.150 --> 00:04:39.990
So let's do it.

67
00:04:39.990 --> 00:04:41.700
That will be good coding practice.

68
00:04:41.820 --> 00:04:47.160
And speaking of coding practice what would be very good is that you try to do it before I do it in this

69
00:04:47.160 --> 00:04:50.370
tutorial so you can press poznaÅ„ try.

70
00:04:50.430 --> 00:04:52.130
And now I'm going to do it.

71
00:04:52.140 --> 00:04:57.650
So basically we need to add one more condition to condition where y equals two.

72
00:04:57.660 --> 00:05:01.350
So let's do it let's add this new condition here.

73
00:05:01.480 --> 00:05:07.020
If WEIGERT equals equals 2 then come up.

74
00:05:07.130 --> 00:05:10.410
And then after this condition What grade is equal to 2.

75
00:05:10.550 --> 00:05:16.940
We will put what we want and what we want is a new color because there is one color associated to each

76
00:05:17.060 --> 00:05:18.590
value of y grade.

77
00:05:18.770 --> 00:05:23.250
So we will keep spring green 3 for the case where WEIGERT equals 1.

78
00:05:23.420 --> 00:05:26.950
And we'll keep tomato for the case where white grid equals zero.

79
00:05:27.290 --> 00:05:30.640
But for what we call Sumathi to introduce a new color.

80
00:05:30.830 --> 00:05:34.300
And since we have here green and red Let's put blue.

81
00:05:34.520 --> 00:05:39.080
So good color is actually deep sky blue.

82
00:05:40.360 --> 00:05:43.600
Then come up to get the next conditions.

83
00:05:43.600 --> 00:05:50.140
So so far what we see is that if Wagh really equals equals two then the call will be Skyblue than if

84
00:05:50.140 --> 00:05:55.620
y really calls one then the color will be green and if waggery equals zero then the color will be red.

85
00:05:55.750 --> 00:05:57.580
But this is not how it works.

86
00:05:57.580 --> 00:06:03.970
It's not as simple as that because this is actually not a correct syntax because this if else function

87
00:06:04.210 --> 00:06:05.960
expects three arguments.

88
00:06:06.070 --> 00:06:12.840
The first argument is the condition which really calls one then the second argument is the result.

89
00:06:12.940 --> 00:06:16.960
When this condition is true and the third argument is the result.

90
00:06:16.960 --> 00:06:18.680
When this condition is not true.

91
00:06:18.910 --> 00:06:21.960
So here we have a lot more than three arguments.

92
00:06:21.970 --> 00:06:23.230
That's not right.

93
00:06:23.230 --> 00:06:27.860
And so the trick to solve this is to put all this.

94
00:06:27.880 --> 00:06:31.880
That is the waggery equals one condition and then the results bring in three.

95
00:06:32.080 --> 00:06:38.740
And then the result if y critical 0 into the third argument of this if else function.

96
00:06:38.920 --> 00:06:43.380
So that means that we'll get the first argument why really calls to that's the condition.

97
00:06:43.480 --> 00:06:49.390
Then the second argument deep sky blue that is the result when we get to and the third argument.

98
00:06:49.510 --> 00:06:51.650
All this in one same argument.

99
00:06:51.680 --> 00:06:55.170
And so how can we include all this in one same arguments.

100
00:06:55.390 --> 00:07:02.480
Well we need to use another if else here which will contain the other two conditions where y really

101
00:07:02.480 --> 00:07:04.860
equals 1 and Y really equals zero.

102
00:07:05.170 --> 00:07:10.120
And so we need to be careful with the parenthesis because we had in your function this new function

103
00:07:10.150 --> 00:07:10.990
if else.

104
00:07:11.200 --> 00:07:12.510
And here it is.

105
00:07:12.880 --> 00:07:16.290
Then you Berit this is added and now it should be fine.

106
00:07:16.300 --> 00:07:17.530
So let's recap.

107
00:07:17.710 --> 00:07:18.790
We start with this first.

108
00:07:18.790 --> 00:07:19.710
If else here.

109
00:07:19.810 --> 00:07:24.060
So if we're really close to then the call will be this guy blue.

110
00:07:24.190 --> 00:07:30.640
And then if we agree is not equal to two then we go into this new else and this new IF ELSE contains

111
00:07:30.640 --> 00:07:32.890
the two last remaining conditions.

112
00:07:32.980 --> 00:07:37.420
That is if we agree because 1 then the call will be Springboro 3.

113
00:07:37.630 --> 00:07:44.940
And if we call 0 then the call will be tomato like red and therefore we get our three conditions and

114
00:07:44.950 --> 00:07:46.180
the correct syntax.

115
00:07:46.420 --> 00:07:47.350
So that's a trick.

116
00:07:47.350 --> 00:07:49.550
It's actually quite common to do it in coding.

117
00:07:49.600 --> 00:07:51.310
So it's good to know how to do it.

118
00:07:52.560 --> 00:07:55.770
And that's the same two plus the colors of our observation points.

119
00:07:55.770 --> 00:08:04.110
So we need to take this and paste it here again and then replace this one here by two.

120
00:08:04.140 --> 00:08:06.280
So that is the new first condition.

121
00:08:06.420 --> 00:08:12.810
If our observation point belongs to class 2 then we want to give it a new color which will be a blue

122
00:08:12.810 --> 00:08:16.140
color but a different blue than this deep sky blue.

123
00:08:16.170 --> 00:08:18.460
And so you know we need to get a good contrast to that.

124
00:08:18.600 --> 00:08:22.200
We don't confuse the color of the points and the color of the region.

125
00:08:22.200 --> 00:08:29.090
So actually a good color to use here is lutherie blue realty that it will give us a good contrast.

126
00:08:29.220 --> 00:08:35.550
And so that's the first result of the first condition and then say we need to include the two remaining

127
00:08:35.550 --> 00:08:39.990
conditions here into one argument that is in sight and if else.

128
00:08:40.110 --> 00:08:49.250
So if elsea in parenthesis and we don't forget at the closing parenthesis here and here we go.

129
00:08:49.290 --> 00:08:50.340
This is ready.

130
00:08:50.370 --> 00:08:57.070
So recap again if our observation point belongs to class 2 then it will have the color blue 3.

131
00:08:57.210 --> 00:09:00.430
Then if it doesn't belong to class 2 then we go here.

132
00:09:00.570 --> 00:09:03.360
And here we have two new separate conditions.

133
00:09:03.360 --> 00:09:09.240
If our observation points belongs to class 1 then it will have the color green for and if it doesn't

134
00:09:09.240 --> 00:09:12.810
belong to class 1 then it will have the color red 3.

135
00:09:13.350 --> 00:09:18.000
So that should be ready and then we have to tiny changes to add.

136
00:09:18.000 --> 00:09:25.050
So remember in this line here line 49 with the column names we need to put the real column names of

137
00:09:25.050 --> 00:09:29.700
the columns of the training set and these column names are not age and estimated salary.

138
00:09:29.700 --> 00:09:32.490
That was for the previous classification problem.

139
00:09:32.490 --> 00:09:37.070
Now the column names are of course P.S. 1 and 2.

140
00:09:37.310 --> 00:09:45.890
So here we just need to replace age by PS1 and estimated salary by the seat.

141
00:09:46.040 --> 00:09:47.020
So that's compulsory.

142
00:09:47.030 --> 00:09:48.740
That's actually what you need input.

143
00:09:48.920 --> 00:09:52.030
Otherwise you will get an error when you execute your code.

144
00:09:52.280 --> 00:09:56.440
And then here it's not compulsory but it's better for the visualization.

145
00:09:56.450 --> 00:10:03.730
You can replace age by piece 1 and estimated salary by PC too.

146
00:10:03.890 --> 00:10:08.730
But if you don't do it you will not get an error because this is just for the visualization.

147
00:10:08.730 --> 00:10:13.340
This is just for the labels that you will see on the graph.

148
00:10:13.370 --> 00:10:13.730
All right.

149
00:10:13.730 --> 00:10:15.130
And then I think we're good.

150
00:10:15.170 --> 00:10:17.000
I think this is ready to be executed.

151
00:10:17.000 --> 00:10:20.110
Let's hope that I didn't make any mistake.

152
00:10:20.150 --> 00:10:22.460
So we're going to try to execute this.

153
00:10:22.580 --> 00:10:25.150
And let's see what we get.

154
00:10:25.190 --> 00:10:28.000
So I'm going to select everything in this section.

155
00:10:28.130 --> 00:10:34.550
So from here up to the top here and let's exiguous.

156
00:10:34.570 --> 00:10:35.420
All right.

157
00:10:35.420 --> 00:10:36.740
Good start.

158
00:10:36.740 --> 00:10:38.510
It's running.

159
00:10:38.860 --> 00:10:39.620
Let's see what we get.

160
00:10:39.620 --> 00:10:45.530
Let's go into this plot tab it is still running.

161
00:10:45.620 --> 00:10:48.470
And here we go we get our beautiful results.

162
00:10:48.560 --> 00:10:50.960
So I hope you like my choice of the colors.

163
00:10:50.990 --> 00:10:52.530
This is the deep sky blue.

164
00:10:52.730 --> 00:10:54.950
And this is the blue 3:6 that we get.

165
00:10:54.950 --> 00:11:01.280
The contrast between the observation points and the prediction regions so we can actually notch this

166
00:11:01.340 --> 00:11:04.210
if you want.

167
00:11:04.220 --> 00:11:09.620
So as a quick reminder the points are the real observation points that is these are the winds that we

168
00:11:09.620 --> 00:11:15.710
have in our trainset and the regions are where our model predicts that once belonged to the customer

169
00:11:15.710 --> 00:11:16.330
segments.

170
00:11:16.430 --> 00:11:21.470
So for example the Green points are the ones of the training set belonging to customer segment number

171
00:11:21.490 --> 00:11:23.240
two and this green region.

172
00:11:23.240 --> 00:11:28.180
Here is where the model predicts that the ones belong to customers saying we're number two.

173
00:11:28.460 --> 00:11:31.250
And same for the blue and red parts here.

174
00:11:31.250 --> 00:11:31.630
All right.

175
00:11:31.630 --> 00:11:33.900
Now we can quickly do the same for the test set.

176
00:11:33.920 --> 00:11:37.600
So we actually need to do the same change just as we did for the training set.

177
00:11:37.790 --> 00:11:40.340
That is let's start with the simplest one we need to replace here.

178
00:11:40.340 --> 00:11:46.210
Age by PC One estimated salary by P C 2.

179
00:11:46.220 --> 00:11:52.190
So these are compulsory changes and we can also change the labels even if that's not compulsory change

180
00:11:52.190 --> 00:11:59.060
just replace age by PC 1 replace estimated salary by pc 2.

181
00:11:59.570 --> 00:12:04.840
And here we go we are almost ready we need to make this big change here to add the third condition to

182
00:12:04.920 --> 00:12:16.770
the third color and we can actually take these two lines here copy them and select this and paste.

183
00:12:16.780 --> 00:12:17.160
All right.

184
00:12:17.200 --> 00:12:23.290
We can do this because these are the same variable names as for the training set because we use this

185
00:12:23.530 --> 00:12:29.310
set a variable name here for both the training set and the test set.

186
00:12:29.830 --> 00:12:37.180
And so basically that's ready we can now select this whole section here and executes to visualize the

187
00:12:37.180 --> 00:12:38.300
test results.

188
00:12:38.530 --> 00:12:39.970
So let's do it.

189
00:12:39.970 --> 00:12:40.690
Here we go.

190
00:12:40.720 --> 00:12:48.220
Precessing the test results are come in and we should get a perfect plot's with no incorrect predictions

191
00:12:48.240 --> 00:12:53.170
that means that we should get all the green points in the green region all the red points where here

192
00:12:53.170 --> 00:12:54.670
we go all the red points.

193
00:12:54.670 --> 00:12:59.560
As you can see are in the region and all the blue points in the blue region.

194
00:12:59.790 --> 00:13:04.400
So that's perfect that's a perfect representation of 100 percent accuracy.

195
00:13:04.540 --> 00:13:11.680
And so in conclusion we were able to transform a dataset composed of 13 independent variables into this

196
00:13:11.680 --> 00:13:17.920
new data set of reduced dimension we were able to reduce the diamond down to two things to which we

197
00:13:17.920 --> 00:13:21.220
could visualize the results in two dimensions.

198
00:13:21.260 --> 00:13:22.190
Okay perfect.

199
00:13:22.210 --> 00:13:27.580
We are done with this first section about PCa and now the interesting thing that we want to see is how

200
00:13:27.700 --> 00:13:33.670
our next dimensionality reduction technique that we're going to implement is going to do on this dataset.

201
00:13:33.670 --> 00:13:38.500
This next dimensional reduction technique is LDA linear discriminant analysis.

202
00:13:38.650 --> 00:13:40.790
So we'll find out about that in the next section.

203
00:13:40.900 --> 00:13:42.630
And until then nothing.
