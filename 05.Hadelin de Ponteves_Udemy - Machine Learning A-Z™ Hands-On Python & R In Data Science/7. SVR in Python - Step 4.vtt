WEBVTT
1
00:00:00.330 --> 00:00:00.800
All right.

2
00:00:00.800 --> 00:00:01.410
Here we go.

3
00:00:01.410 --> 00:00:06.110
Let us predict that salary with the SVR machinery model.

4
00:00:06.120 --> 00:00:06.390
All right.

5
00:00:06.420 --> 00:00:12.600
So this time it's not going to be as easy as previously because we'll have to do some inverse transformation.

6
00:00:12.600 --> 00:00:14.630
I asked you to try to do it yourself.

7
00:00:14.880 --> 00:00:18.440
And now we're about to implement the solution together.

8
00:00:18.450 --> 00:00:19.350
All right so let's do this.

9
00:00:19.350 --> 00:00:22.220
Let's start by creating a new coat cell.

10
00:00:22.290 --> 00:00:26.020
And according to you now what is the first step we have to do.

11
00:00:26.040 --> 00:00:31.350
You know it's very natural indeed we want to predict new results so the natural idea that comes to mind

12
00:00:31.530 --> 00:00:38.460
is to call the predict method to call the predict method from are as we are regress or object here.

13
00:00:38.490 --> 00:00:44.550
So let's do this let's for girl you know the regress for itself because that's from this object that

14
00:00:44.550 --> 00:00:50.820
you have to call the predict method and inside the parenthesis you know what we have to input we have

15
00:00:50.820 --> 00:00:54.960
to input of course that position level of six point five.

16
00:00:54.990 --> 00:00:55.260
Right.

17
00:00:55.260 --> 00:01:01.680
Because this person we want higher had a region manager position but after a couple of years so we consider

18
00:01:01.680 --> 00:01:03.300
it to be six point five.

19
00:01:03.300 --> 00:01:07.490
And remember that this position level has to be in a 2D array.

20
00:01:07.560 --> 00:01:13.290
So we have to adhere double pair of square brackets and inside input six point five.

21
00:01:13.290 --> 00:01:20.400
So that was the original format you know expected by the brick method of our previous machinery models

22
00:01:20.640 --> 00:01:22.890
including polynomial regression.

23
00:01:22.890 --> 00:01:23.470
OK.

24
00:01:23.640 --> 00:01:29.210
But as I told you it's not that easy here because we did a lot of a feature scaling and transformations

25
00:01:29.220 --> 00:01:33.920
on everything the matrix of Features X and the dependent variable back to Y.

26
00:01:33.930 --> 00:01:36.800
So here we have to do some extra things.

27
00:01:36.810 --> 00:01:37.080
All right.

28
00:01:37.080 --> 00:01:45.420
So first thing well since we actually trained that as we are moral on X and Y which were both scales

29
00:01:45.540 --> 00:01:50.340
with two different scales you know X was killed by a C X..

30
00:01:50.340 --> 00:01:56.270
I can show this to you again X were scaled by a C X and Y was killed by as you Y.

31
00:01:56.670 --> 00:02:04.650
And so the consequence of this is that when you use the predict method on an observation well that observation

32
00:02:04.650 --> 00:02:11.030
has to be on the same scale as the values of your matrix of features on which your model was straight.

33
00:02:11.040 --> 00:02:17.220
So here the first thing we have to do is apply the transformation you know apply this killing to that

34
00:02:17.280 --> 00:02:18.470
particular value.

35
00:02:18.540 --> 00:02:19.740
And how do we do this.

36
00:02:19.740 --> 00:02:20.910
Well that's very easy.

37
00:02:20.910 --> 00:02:27.570
We have to call our scalar object for X. You know S.E. X the matrix of features x and then from this

38
00:02:27.630 --> 00:02:32.920
object apply the transform method which will exactly feature scale.

39
00:02:32.940 --> 00:02:38.160
This particular value you know put this particular value in the same scale that was applied to the whole

40
00:02:38.160 --> 00:02:39.650
matrix of features X..

41
00:02:39.660 --> 00:02:40.620
All right so let's do this.

42
00:02:40.620 --> 00:02:42.240
That's the first thing we had to do here.

43
00:02:42.270 --> 00:02:50.190
We have to call R S C underscore X scalar objection for the matrix of features X from which we're gonna

44
00:02:50.240 --> 00:02:53.100
call the trend form method.

45
00:02:53.130 --> 00:02:56.460
Thank you Google app which will take as input.

46
00:02:56.460 --> 00:03:01.620
Exactly this 2D array containing the single value six point five.

47
00:03:01.620 --> 00:03:01.920
All right.

48
00:03:01.920 --> 00:03:05.310
I have to add a new one here and there you go.

49
00:03:05.370 --> 00:03:11.140
So that's the first thing you had to do to make sure that your value here inside the brick method is

50
00:03:11.140 --> 00:03:16.580
scaled on the same scale that was used to scale the whole matrix of features x.

51
00:03:16.650 --> 00:03:17.030
All right.

52
00:03:17.130 --> 00:03:25.830
That's a first thing and therefore this will return your predicted salary but in the scale that was

53
00:03:25.830 --> 00:03:33.660
applied to y you know this killing that resulted from applying that second scalar object on the dependent

54
00:03:33.660 --> 00:03:41.430
variable vector y and therefore instead of you know getting here salary around let's say 160 K we will

55
00:03:41.430 --> 00:03:44.510
get a salary around this range of values.

56
00:03:44.550 --> 00:03:44.850
All right.

57
00:03:44.850 --> 00:03:46.500
And that's absolutely not what we want.

58
00:03:46.500 --> 00:03:47.610
It is still correct.

59
00:03:47.610 --> 00:03:52.880
But of course what we have to do now is reverse the scaling of Y.

60
00:03:53.070 --> 00:03:58.920
And more specifically of that prediction of that predicted salary in order to get the original scale

61
00:03:59.190 --> 00:04:00.280
of that prediction.

62
00:04:00.510 --> 00:04:07.170
And that's where you have to use this new method that can reverse this killing and which is called I

63
00:04:07.170 --> 00:04:10.340
hope you found it on the Internet or any documentation.

64
00:04:10.440 --> 00:04:17.190
This method is called the inverse transform method and that's exactly what we have to apply here to

65
00:04:17.190 --> 00:04:22.920
indeed reverse this killing to go back to the original skill of y and therefore to go back to the original

66
00:04:22.920 --> 00:04:25.050
scale of that predicted salary.

67
00:04:25.050 --> 00:04:29.520
All right so let's do this in order to apply that inverse transfer method.

68
00:04:29.520 --> 00:04:35.790
Well remember that any method has to be called from an object and make sure to understand which object

69
00:04:35.820 --> 00:04:36.950
we have to go here.

70
00:04:36.960 --> 00:04:39.180
That's of course SC Why.

71
00:04:39.180 --> 00:04:44.070
Because we want to reverse this killing that was used to transform the dependent variable vector Y.

72
00:04:44.280 --> 00:04:53.390
So here we have to go SC underscore why from which we're gonna call that inverse transform method.

73
00:04:53.400 --> 00:05:01.060
Thank you again Google collab which will take as it put this whole prediction of the salary of the position

74
00:05:01.060 --> 00:05:02.420
level six point five.

75
00:05:02.420 --> 00:05:03.940
And there you go my friends.

76
00:05:03.940 --> 00:05:06.740
Now you have your prediction in the right scale.

77
00:05:06.790 --> 00:05:07.720
You want to check it out.

78
00:05:07.900 --> 00:05:09.070
Let's do this.

79
00:05:09.100 --> 00:05:12.030
We don't have to do a print here because this will return directly.

80
00:05:12.160 --> 00:05:13.210
The prediction.

81
00:05:13.210 --> 00:05:18.220
I hope I didn't make any mistake with all these trends form and interest trends for method and now all

82
00:05:18.220 --> 00:05:20.010
these parenthesis as you can see.

83
00:05:20.080 --> 00:05:20.810
But let's see.

84
00:05:20.830 --> 00:05:22.980
I just followed some logic here.

85
00:05:23.200 --> 00:05:24.730
And so now if you're ready.

86
00:05:24.730 --> 00:05:27.400
Well let's execute our two cells here.

87
00:05:27.400 --> 00:05:31.400
First this cell to indeed build and trained as your model.

88
00:05:31.420 --> 00:05:32.530
So let's do this.

89
00:05:32.530 --> 00:05:38.110
And there we go we get in the output there as your model with all the parameters that were chosen and

90
00:05:38.380 --> 00:05:44.630
we actually kept the default value for most of them except that Colonel parameter for which we chose

91
00:05:44.630 --> 00:05:46.960
indeed the radial basis function Colonel.

92
00:05:46.990 --> 00:05:53.110
And now time to get the final prediction the predicted salary for that position level six point five

93
00:05:53.380 --> 00:05:55.880
of course from our SVR morale.

94
00:05:55.960 --> 00:06:03.340
And to do this we simply to execute this cell to figure out that the predicted salary is in fact one

95
00:06:03.340 --> 00:06:07.080
hundred seventy thousand dollars per year.

96
00:06:07.210 --> 00:06:11.370
So that's once again a prediction that seems to make a lot of sense.

97
00:06:11.410 --> 00:06:17.620
You know based on what this person mentioned as a previous salary I remind that in our little story

98
00:06:17.830 --> 00:06:24.760
this person mentioned to have a 160 K salary in the previous company characterized by this data.

99
00:06:24.760 --> 00:06:30.570
So not only it is very close but also our predicted salary is higher than the requested salary.

100
00:06:30.580 --> 00:06:33.300
So that's all good for our negotiation.

101
00:06:33.400 --> 00:06:37.160
And to compare with our previous morale you know the pulling them your regression model.

102
00:06:37.240 --> 00:06:42.750
Well remember that we had a one hundred and fifty eight K prorated salary and now we have a one hundred

103
00:06:42.740 --> 00:06:43.660
and seventy K.

104
00:06:43.780 --> 00:06:50.620
So anyway both these models return a prediction that is indeed close to what would be the real salary

105
00:06:50.620 --> 00:06:53.410
of that position level number of six point five.

106
00:06:53.410 --> 00:06:59.620
And so once again that shows that that as your model is really well adapted to this data set because

107
00:06:59.620 --> 00:07:05.660
indeed it can handle these nonlinear correlations between the position levels and the salaries and we're

108
00:07:05.680 --> 00:07:10.450
going to see that much better in the visualization you know because that's our next step visualizing

109
00:07:10.450 --> 00:07:13.980
that as we are results in low resolution and high resolution.

110
00:07:13.990 --> 00:07:16.170
So that's what we'll do in this last tutorial.

111
00:07:16.180 --> 00:07:22.450
But now I really want to say congratulations because you now know how to handle the not only transformations

112
00:07:22.480 --> 00:07:24.570
but also inverse transformations.

113
00:07:24.730 --> 00:07:31.420
And you can now return a prediction when some feature scaling was applied on both the matrix of features

114
00:07:31.450 --> 00:07:34.240
x and the dependent variable vector y.

115
00:07:34.270 --> 00:07:38.800
It's really good that you know how to do this because indeed in your future dataset and your future

116
00:07:38.830 --> 00:07:42.940
MRO projects you will definitely encounter this situation.

117
00:07:42.940 --> 00:07:48.210
So I'm really happy for you that you now have this extra tool in your machinery tool kit.

118
00:07:48.280 --> 00:07:53.240
And now let's finish with a less exciting tutorial where we're going to visualize the results.

119
00:07:53.290 --> 00:07:58.300
So as soon as you're ready let's observe the regression curve of the SVR model.

120
00:07:58.690 --> 00:08:00.580
And until then enjoy machine learning.
