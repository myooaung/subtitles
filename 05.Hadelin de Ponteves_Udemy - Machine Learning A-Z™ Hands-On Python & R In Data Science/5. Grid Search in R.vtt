WEBVTT
1
00:00:00.490 --> 00:00:02.680
Hello and welcome to our tutorial.

2
00:00:02.680 --> 00:00:07.480
So in the previous Statoil we learned a very efficient way to evaluate our mall performance.

3
00:00:07.540 --> 00:00:10.710
So that was about evaluating our model performance.

4
00:00:10.780 --> 00:00:15.940
And in today's Statoil we are going to learn a technique that is going to be about improving models

5
00:00:15.940 --> 00:00:17.140
performance.

6
00:00:17.140 --> 00:00:23.320
So we already built very powerful models but we can still improve them and how can we do that well by

7
00:00:23.320 --> 00:00:28.930
finding the optimal values of the hyper parameters because indeed any machine early model is composed

8
00:00:28.930 --> 00:00:30.660
of two types of parameters.

9
00:00:30.700 --> 00:00:35.620
The first type of parameters are the parameters that are learned through the machine learning algorithm.

10
00:00:35.710 --> 00:00:39.090
And the second type of parameters are the parameters that we choose.

11
00:00:39.130 --> 00:00:45.670
Like for example the kernel in the model or the penalty parameter or even some regularization parameter

12
00:00:45.670 --> 00:00:47.460
enrich regression or less so.

13
00:00:47.650 --> 00:00:52.350
And basically in any machine any model we have a lot of parameters that are not learned.

14
00:00:52.450 --> 00:00:57.500
And so far what we've done in this course was just to choose one single value for these parameters.

15
00:00:57.550 --> 00:01:02.620
We never actually experienced with these hyper parameters we never tried several values of them.

16
00:01:02.830 --> 00:01:07.510
And so that gives us still a lot of room for improvement because maybe there were better choices for

17
00:01:07.510 --> 00:01:09.550
the values of these hyper parameters.

18
00:01:09.550 --> 00:01:12.420
You know a better choice than the value we chose.

19
00:01:12.730 --> 00:01:17.920
So this technique called grid search will answer to one of your questions that were asked many times

20
00:01:17.920 --> 00:01:23.340
in this course which is how do I know which parameter to select when I make a machine learning model

21
00:01:23.350 --> 00:01:28.690
what is the optimal value of these hyper parameters and a grid search will give an answer to that because

22
00:01:28.960 --> 00:01:31.950
it will find the optimal values for these parameters.

23
00:01:31.990 --> 00:01:33.460
So let's see how it will do that.

24
00:01:33.460 --> 00:01:35.500
And let's implement grid search.

25
00:01:35.740 --> 00:01:39.430
So we're going to work on the same problem as in the previous two toile.

26
00:01:39.490 --> 00:01:44.340
That is you know this classification problem where we need to classify the users in the social network

27
00:01:44.340 --> 00:01:49.240
and predict if they're going to click yes or no on the ad to buy the SUV and therefore we're working

28
00:01:49.240 --> 00:01:51.530
with this data set social network ads.

29
00:01:51.700 --> 00:01:57.310
And besides that's a good example because the kernel as VM model has many parameters you know it has

30
00:01:57.310 --> 00:01:58.810
this penalty parameter.

31
00:01:58.810 --> 00:02:04.420
This gamma parameter and some of you asked me which values we should choose for these parameters.

32
00:02:04.420 --> 00:02:07.110
And so a grid search will tell us exactly that.

33
00:02:07.120 --> 00:02:08.720
So let's do it.

34
00:02:08.770 --> 00:02:14.020
We are working in the same folder as in the previous tutorial that is the model selection for make sure

35
00:02:14.020 --> 00:02:16.390
that you have this social network as you file.

36
00:02:16.480 --> 00:02:18.410
And if that's the case you're ready to go.

37
00:02:18.430 --> 00:02:25.090
So we are going to like grid search anywhere after the data preprocessing phase because we are not actually

38
00:02:25.090 --> 00:02:30.910
going to use this kernel as we are moral and then doing some parameter tuning on it to find the optimal

39
00:02:30.910 --> 00:02:31.710
values.

40
00:02:31.720 --> 00:02:37.990
We were actually going to use a new package that you will see is a very practical package to build the

41
00:02:37.990 --> 00:02:40.820
same kernel as VM model as this one here.

42
00:02:40.870 --> 00:02:47.440
But by applying grid search on it at the same time and this packet you already know it it's the carrot

43
00:02:47.440 --> 00:02:48.400
package.

44
00:02:48.400 --> 00:02:53.710
And really when we talk about machine learning and our the carrot package is one of the most practical

45
00:02:53.710 --> 00:02:59.710
packages because basically with this package you can build any machine learning model and not only you

46
00:02:59.710 --> 00:03:05.380
can build any machine any model but when you build a machine any model with a care package well it will

47
00:03:05.380 --> 00:03:08.890
give you the model you want with all the optimal parameters.

48
00:03:08.890 --> 00:03:14.210
So that's pretty powerful and therefore that's one of the most powerful packages in our.

49
00:03:14.260 --> 00:03:21.430
So as I told you we can build this new Choon model anywhere after the data processing phase so let's

50
00:03:21.430 --> 00:03:27.610
actually make it right after the careful cross-validation section to you know keep the other interesting

51
00:03:27.610 --> 00:03:32.700
code sections here so that for example you can compare the two models you know the one we made here

52
00:03:32.980 --> 00:03:36.070
and the numeral we were about to make with the carrot package.

53
00:03:36.220 --> 00:03:38.850
Feel free to compare their performance results.

54
00:03:38.920 --> 00:03:44.050
So we're going to build this new kernel as a model right here.

55
00:03:44.170 --> 00:03:48.880
So we're playing great search to find the best parameters I prepared this code section here.

56
00:03:49.090 --> 00:03:53.680
But actually it's not only applying grid search it's also building a new model.

57
00:03:53.740 --> 00:03:59.500
So first the first thing that we have to do is to import the carrot package we've already installed

58
00:03:59.500 --> 00:04:01.400
it in some previous tutorials.

59
00:04:01.510 --> 00:04:03.800
But let's make sure you install it.

60
00:04:03.940 --> 00:04:06.350
If you didn't follow those procedures the toils.

61
00:04:06.370 --> 00:04:11.580
So I'm just putting back this command and inside of course carrots in quotes.

62
00:04:11.590 --> 00:04:11.950
All right.

63
00:04:11.950 --> 00:04:14.110
And then I'll put that in comment.

64
00:04:14.110 --> 00:04:14.940
Here we go.

65
00:04:15.160 --> 00:04:19.120
And now let's import the package with the library command.

66
00:04:19.120 --> 00:04:22.460
Here we go library and carrots inside.

67
00:04:22.510 --> 00:04:23.090
Perfect.

68
00:04:23.170 --> 00:04:26.860
So now let's build the same kernel as your model.

69
00:04:26.860 --> 00:04:30.800
But with the carrot package you're going to see that it's going to be something you.

70
00:04:31.250 --> 00:04:35.540
OK so since we're building this new classifier the kernel as a pacifier.

71
00:04:35.740 --> 00:04:39.180
Well let's go our model with the usual name that is classified.

72
00:04:40.350 --> 00:04:45.480
All right so that's the variable then equals and then that's where we use to carry the baggage.

73
00:04:45.720 --> 00:04:47.540
And what is the function we're going to use now.

74
00:04:47.610 --> 00:04:50.250
It's one of the most used function of the carrot package.

75
00:04:50.430 --> 00:04:52.370
It is the train function.

76
00:04:52.560 --> 00:04:53.820
So parenthesis.

77
00:04:53.820 --> 00:04:58.200
And now what's really interesting to do is to take a browser.

78
00:04:58.200 --> 00:04:58.950
Here it is.

79
00:04:59.070 --> 00:05:09.360
And let's type carrots enter and let's take the get hub link let's click on it and inside of it you

80
00:05:09.380 --> 00:05:17.740
need to click on this link right here and then let's click on six available models because in this section

81
00:05:18.100 --> 00:05:23.280
you'll get all the models you can build with the carrot package and there are a lot of models.

82
00:05:23.290 --> 00:05:25.870
There are actually older models we built in this course.

83
00:05:25.900 --> 00:05:31.220
So right now some of you might be thinking then why didn't we build all these model with the carit package

84
00:05:31.420 --> 00:05:36.170
since indeed the carrot package will give us this model with the best optimal parameters.

85
00:05:36.370 --> 00:05:41.470
Well that's because the packages were used to build all the models in the course have great options

86
00:05:41.560 --> 00:05:44.560
some of which you can not use with the carrot package.

87
00:05:44.560 --> 00:05:49.630
So it's good to know how to use both the packages we used before in the course and the carrot package

88
00:05:49.780 --> 00:05:54.400
but definitely for parameter tuning you should use the carrot package.

89
00:05:54.430 --> 00:06:00.000
All right so this list you see here is the list of all the models you can build with the carrot package.

90
00:06:00.160 --> 00:06:04.030
And since right now we're building the kernel as a whole.

91
00:06:04.070 --> 00:06:12.100
Well you will see that it is available I mean in this part of this list and we can find it right here

92
00:06:12.280 --> 00:06:18.340
at the bottom of this list because indeed we can see as we have many support vector machines model and

93
00:06:18.340 --> 00:06:25.360
the one we are interested in is the support vector machines with radial basis function called Remember

94
00:06:25.600 --> 00:06:28.460
the radio bases function girl is the gashing girl.

95
00:06:28.570 --> 00:06:34.390
The most commonly used kernel to build the kernel as a VM model and so this is the model we are going

96
00:06:34.390 --> 00:06:39.700
to build right now with a carrot package and the information that we need to take right now that is

97
00:06:39.700 --> 00:06:46.570
going to be one input of the train function we were about to use is this information here thats the

98
00:06:46.570 --> 00:06:49.570
input of one of the parameters of the train function.

99
00:06:49.570 --> 00:06:54.850
The method parameter and that is with this parameter that the trend function will know which model to

100
00:06:54.850 --> 00:06:56.850
build and which model to choose.

101
00:06:57.070 --> 00:07:04.510
So right now what we have to do is to take this name copy it and we will paste it as input as the method

102
00:07:04.510 --> 00:07:07.080
parameter inside the train function.

103
00:07:07.090 --> 00:07:08.810
So let's go back to our studio.

104
00:07:08.920 --> 00:07:09.560
Here we go.

105
00:07:09.610 --> 00:07:17.930
And now let's build the model so we can actually press F on here to get the informations of arguments.

106
00:07:18.010 --> 00:07:24.720
So let's see what we have to input the first compulsory argument we need to put in this form argument.

107
00:07:24.820 --> 00:07:26.950
And of course that's the formula.

108
00:07:26.950 --> 00:07:27.960
So let's do it.

109
00:07:28.180 --> 00:07:34.450
Let's put this first argument form equals and then we need to put the formula exactly as we used to

110
00:07:34.450 --> 00:07:36.660
do when building the previous models.

111
00:07:36.670 --> 00:07:41.340
So that's the dependent variable which is as a reminder purchased.

112
00:07:41.600 --> 00:07:44.680
You know it's the social network ads business problem.

113
00:07:44.680 --> 00:07:53.860
So here we need two inputs purchased here we go then sell the and then all the independent variables.

114
00:07:53.990 --> 00:07:59.230
And remember we don't have to put the names of all the variables we can use this shortcut here.

115
00:07:59.390 --> 00:08:00.840
That is just dumb.

116
00:08:01.070 --> 00:08:02.280
All right then come on.

117
00:08:02.300 --> 00:08:04.270
And then next argument.

118
00:08:04.280 --> 00:08:09.850
So the next argument is data and that's of course your training set you're building you classify you

119
00:08:09.880 --> 00:08:10.990
on a train set.

120
00:08:11.150 --> 00:08:13.270
And so you need input here.

121
00:08:13.400 --> 00:08:17.080
Data equals training set.

122
00:08:17.240 --> 00:08:17.950
Here we go.

123
00:08:18.080 --> 00:08:23.780
So now is the form the formula and the data the training sets you have all the information you need

124
00:08:24.080 --> 00:08:25.250
to train your model.

125
00:08:25.610 --> 00:08:30.590
But then of course we need to specify which model we have to build that is we need to specify that we

126
00:08:30.590 --> 00:08:36.830
want to make the kernel as being moral and that's what happens with the third argument which is not

127
00:08:36.830 --> 00:08:38.570
one of these arguments here.

128
00:08:38.570 --> 00:08:44.930
These are not compulsory but it's the method arguments actually have the link here the link we just

129
00:08:45.140 --> 00:08:50.660
browsed on Google this link will give you the list of all the models available and character.

130
00:08:50.900 --> 00:08:55.670
And so that's with this method parameter that we need to input what we copied in this link.

131
00:08:55.670 --> 00:08:57.760
That is as V.M. radio.

132
00:08:58.010 --> 00:09:06.680
So let's put that right now comma and then method equals then quotes and inside of these quotes we need

133
00:09:06.680 --> 00:09:08.680
to taste SBM radio.

134
00:09:08.900 --> 00:09:15.380
And that's actually all with these three arguments the formula and the data and the method you will

135
00:09:15.380 --> 00:09:20.630
build or kernel as VMO and then for the parameter that you know you're going to see what happens.

136
00:09:20.630 --> 00:09:27.230
So first we're going to do is execute the data processing phase because we need to import the data set

137
00:09:27.560 --> 00:09:30.200
and applying the data processing phase.

138
00:09:30.230 --> 00:09:31.480
So let's do it right now.

139
00:09:31.520 --> 00:09:36.140
We're not going to execute the sections because this is to build the kernel as VM all the other ways

140
00:09:36.170 --> 00:09:38.230
we just need to take everything from here.

141
00:09:39.140 --> 00:09:42.230
Up to the top here we go let's do it.

142
00:09:42.230 --> 00:09:42.880
All right.

143
00:09:42.890 --> 00:09:44.240
They just said we'll import it.

144
00:09:44.240 --> 00:09:50.590
We have the data set the trends that editors set and the data processing phase was applied all correctly.

145
00:09:50.930 --> 00:09:56.820
All right so now let's build the kernel as a model with the carrot package.

146
00:09:56.930 --> 00:10:01.040
So let's make sure we import the carrot package we go.

147
00:10:01.160 --> 00:10:06.450
And now let's select this line and execute it.

148
00:10:06.480 --> 00:10:06.920
All right.

149
00:10:06.920 --> 00:10:08.350
It's taking a little time.

150
00:10:08.510 --> 00:10:10.160
Well around one second.

151
00:10:10.190 --> 00:10:10.910
That's fine.

152
00:10:11.060 --> 00:10:12.980
And here we get our classifier.

153
00:10:13.100 --> 00:10:15.410
So that's the kernel as we all classify it.

154
00:10:15.650 --> 00:10:19.940
It is not yet today but you're going to see what's going to happen because now what we're going to do

155
00:10:20.420 --> 00:10:29.060
is pressing enter here and we're simply going to type classifier and selecting this and pressing enter.

156
00:10:29.060 --> 00:10:35.450
And there you get a lot of very interesting informations about your kernel as we classify you you just

157
00:10:35.450 --> 00:10:42.440
built with the carrot package because indeed what you see here is the conclusion of some parameter tuning

158
00:10:42.440 --> 00:10:45.490
that was done on this kernel SVM classify.

159
00:10:45.500 --> 00:10:46.390
You just built.

160
00:10:46.580 --> 00:10:52.250
And what is this conclusion is that accuracy was used to select the optimal model using the largest

161
00:10:52.250 --> 00:10:52.800
value.

162
00:10:52.880 --> 00:10:57.910
So that means that accuracy was the performance metric to evaluate your model performance.

163
00:10:58.100 --> 00:11:05.480
And just below you see that the final values used for the model were Sigma equals one point twelve one

164
00:11:05.540 --> 00:11:08.830
point thirteen and C equals 0.5.

165
00:11:09.110 --> 00:11:15.170
And that's the results of your parameter Chihuly that's the optimal values of the hyper parameters of

166
00:11:15.170 --> 00:11:15.710
the call.

167
00:11:15.720 --> 00:11:21.950
As we model the ones that will make your kernel as a model even more performance than the one you built

168
00:11:21.950 --> 00:11:23.070
with the previous method.

169
00:11:23.210 --> 00:11:27.180
Because of course maybe the default value for the C parameter was 0.5.

170
00:11:27.180 --> 00:11:32.450
This looks like the default value but definitely the default value for the stigma around group which

171
00:11:32.450 --> 00:11:37.900
is another hyper parameter of kernel as VM was definitely not 1.30.

172
00:11:38.060 --> 00:11:45.380
And with this one point thirteen value for the Sigma parameter you get the optimal accuracy and accuracy

173
00:11:45.530 --> 00:11:51.230
that is higher than the accuracy obtained with the kernel as we model built previously and you can even

174
00:11:51.230 --> 00:11:53.040
see what this accuracy is.

175
00:11:53.090 --> 00:11:54.300
It is shown right here.

176
00:11:54.500 --> 00:11:56.950
Accuracy is 92 percent.

177
00:11:57.080 --> 00:12:02.120
And besides this is irrelevant accuracy does not the accuracy measured on one 20 split.

178
00:12:02.150 --> 00:12:08.150
This was measured on several Trente splits which you can actually see here with this information re-assembling

179
00:12:08.480 --> 00:12:14.240
bootstrap with 25 repetitions which is exactly the same principle as careful Croswell EDITION.

180
00:12:14.330 --> 00:12:18.800
That means that you take different samples of train sets and test sets and you build your model in the

181
00:12:18.920 --> 00:12:24.170
trainset and test its performance on the test sets for each of these samples and eventually you take

182
00:12:24.170 --> 00:12:28.290
the mean of the accuracy's and you get a 92 percent accuracy.

183
00:12:28.550 --> 00:12:35.780
So thats definitely a good accuracy and besides thats the best you can obtain with these optimal values

184
00:12:35.780 --> 00:12:40.790
of the hyper parameters Sigma equals one point thirteen and C equals 0.5.

185
00:12:41.090 --> 00:12:45.620
And now I'll give you an even faster way to get these optimal values of the hyper parameters.

186
00:12:45.800 --> 00:12:47.120
You just need to.

187
00:12:47.510 --> 00:12:51.070
Well let's keep this classifier here but let's copy that.

188
00:12:51.080 --> 00:12:58.610
You just need to take your classifier and then add a dollar sign and then you can add here best choon.

189
00:12:58.640 --> 00:13:06.410
Here you go you've selected you executed and you get directly the optimal values of the hyper parameters.

190
00:13:06.410 --> 00:13:10.550
Sigma equals one point thirteen and C equals 0.5.

191
00:13:10.640 --> 00:13:17.150
And now congratulations you got your best kernel as your model with the best Rinku efficient and the

192
00:13:17.150 --> 00:13:18.610
best hyper parameters.

193
00:13:18.770 --> 00:13:20.590
And now the choice is yours.

194
00:13:20.660 --> 00:13:26.810
You can either take this kernel as VM transfer you built here with the carrier package or you can take

195
00:13:27.590 --> 00:13:29.840
the class or you build with the usual way.

196
00:13:30.020 --> 00:13:36.220
But inside of this SVM function you can input the optimal values you found here for the hyper parameterless

197
00:13:36.230 --> 00:13:36.800
that is.

198
00:13:36.800 --> 00:13:40.380
Sigma equals one point thirteen and C equals 0.5.

199
00:13:40.610 --> 00:13:45.920
I will let you that for practice and judge yourself which method you want to choose and I'll see you

200
00:13:45.920 --> 00:13:50.690
in the next and last section of discourse in which we're going to implement one of the most powerful

201
00:13:50.690 --> 00:13:53.560
models in machine learning X to boost.

202
00:13:53.570 --> 00:13:55.890
So this is going to be a very exciting section.

203
00:13:56.030 --> 00:13:57.420
I can't wait to see you there.

204
00:13:57.500 --> 00:13:59.270
And until then in Germany learning.
