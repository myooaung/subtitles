1
00:00:00,120 --> 00:00:05,550
A Welcome Back in this video we're finally going to learn how to scrape it easy.

2
00:00:05,550 --> 00:00:14,040
You just you take something out of a plate and a fork you just kind of rub it back and say oh my god

3
00:00:14,040 --> 00:00:16,900
that is so awful.

4
00:00:17,220 --> 00:00:18,750
What have I done.

5
00:00:19,190 --> 00:00:19,870
Oh yeah.

6
00:00:19,890 --> 00:00:21,950
That was my joke about scraping.

7
00:00:22,110 --> 00:00:22,350
OK.

8
00:00:22,350 --> 00:00:22,840
For real.

9
00:00:22,840 --> 00:00:24,800
I mean get this played out of here.

10
00:00:24,840 --> 00:00:28,500
We're actually going to start our web scraping in this video and we're going to use something called

11
00:00:28,530 --> 00:00:29,630
beautiful soup.

12
00:00:29,640 --> 00:00:32,230
It's a beautiful soup is a library package.

13
00:00:32,250 --> 00:00:33,450
We need to download.

14
00:00:33,570 --> 00:00:35,910
So that's the very first thing we're going to do.

15
00:00:36,030 --> 00:00:37,410
And it kind of drives me insane.

16
00:00:37,430 --> 00:00:42,470
But the name that we actually download is called B-S for now beautiful soup.

17
00:00:42,540 --> 00:00:46,270
But for in our command line over here in Python.

18
00:00:46,300 --> 00:00:48,430
Dash m Pip.

19
00:00:48,660 --> 00:00:56,090
Install the S for and ired have it installed requirement already satisfied.

20
00:00:56,160 --> 00:00:57,620
It should take a moment for you.

21
00:00:57,730 --> 00:01:00,630
You mean you have your internet running and then you get the package.

22
00:01:00,750 --> 00:01:05,480
Ok now that we have beautiful soob installed Let's talk about what it is and what it isn't.

23
00:01:05,490 --> 00:01:10,430
It allows us to navigate through and extract data from him using Python.

24
00:01:10,430 --> 00:01:15,550
But and this is a big big but it does not download H-2 him for us.

25
00:01:15,630 --> 00:01:18,940
We have to manually make the request to get the data.

26
00:01:19,080 --> 00:01:22,350
Let's say we're scraping I am D-B not saying it's a good idea to do that.

27
00:01:22,370 --> 00:01:27,850
But if we were and I was I don't know scraping every single web page I could find spacing out the request

28
00:01:27,850 --> 00:01:29,170
by three seconds.

29
00:01:29,190 --> 00:01:34,680
I would use the request module to send a get request to one page get the data back and then I take that

30
00:01:34,710 --> 00:01:40,340
each time it comes back I send it over to beautiful soup and I extract whatever information I want.

31
00:01:40,350 --> 00:01:46,110
Maybe I'm trying to find links that I can crawl across and then send a further request to every link

32
00:01:46,110 --> 00:01:52,280
we get back from whoever Danny DeVito web page and then every link we get back from Danny to Vito's

33
00:01:52,290 --> 00:01:52,880
page.

34
00:01:52,890 --> 00:01:55,140
We're then going to further scrape and keep going.

35
00:01:55,140 --> 00:01:58,840
Anyway my point is that beautiful soup does not download the Timo.

36
00:01:58,980 --> 00:02:04,410
So in this video we're just going to begin with Mocket HMO in a variable.

37
00:02:04,410 --> 00:02:09,630
I've created a variable called edged him out equals and then I have a multi-line string where I've just

38
00:02:09,630 --> 00:02:12,320
put h tim out inside of there their very basic.

39
00:02:12,330 --> 00:02:15,210
It just says you know title first Tim L page.

40
00:02:15,210 --> 00:02:23,770
We have a div with an ID and H-3 what with a data attribute a paragraph ordered list or some basic H.T.

41
00:02:23,780 --> 00:02:24,580
mail stuff.

42
00:02:24,630 --> 00:02:28,900
If you'd like to follow along you can download the file and type as I type.

43
00:02:28,950 --> 00:02:34,050
So we're going to suppose that we got this back from making a request rather than it being in a variable

44
00:02:34,050 --> 00:02:35,080
from the very beginning.

45
00:02:35,250 --> 00:02:39,390
In the next video though I'll actually talk about making the request and then giving that data to beautiful

46
00:02:39,390 --> 00:02:40,130
soup.

47
00:02:40,140 --> 00:02:41,110
So where do we begin.

48
00:02:41,310 --> 00:02:43,370
Well we have to start by initialising.

49
00:02:43,380 --> 00:02:44,280
Beautiful soup.

50
00:02:44,370 --> 00:02:49,560
After we import it we called this line here to actually parse the page Timo.

51
00:02:49,590 --> 00:02:54,050
Remember HTL comes back as a giant string just like what we have here.

52
00:02:54,150 --> 00:02:57,540
This is how it comes back from wherever we make a request.

53
00:02:57,690 --> 00:02:58,890
It comes back as a string.

54
00:02:58,890 --> 00:03:02,840
It's not an object it's not a class or anything not a dictionary.

55
00:03:03,030 --> 00:03:08,000
So we pass that string in two beautiful soup as well as this string.

56
00:03:08,160 --> 00:03:09,700
H tim Elda parser.

57
00:03:09,810 --> 00:03:15,520
You have to do this because beautiful soup also supports parsing Zemo which is another markup language.

58
00:03:15,570 --> 00:03:18,920
We want the aged him L-1 so we pass that in as well.

59
00:03:18,930 --> 00:03:20,200
But that's always the same for us.

60
00:03:20,250 --> 00:03:23,150
And then that we'll go and parse all of the HTML.

61
00:03:23,190 --> 00:03:28,470
Assuming that it is correct each time there's no errors in it there's no missing brackets or tags or

62
00:03:28,470 --> 00:03:29,220
something.

63
00:03:29,280 --> 00:03:31,830
It goes through and it's actually quite complex.

64
00:03:31,860 --> 00:03:37,100
The logic that has to happen and if we save the result to a variable we can then use that parsed h tim

65
00:03:37,110 --> 00:03:39,320
L and navigate through it.

66
00:03:39,420 --> 00:03:41,180
So there are several ways to navigate.

67
00:03:41,190 --> 00:03:45,990
We can try and use a tag name so I could parse all of it and then say hey beautiful soup.

68
00:03:46,020 --> 00:03:51,840
Give me all of the paragraph tags give me all of the H-1 tags give me all the anchor tags that sort

69
00:03:51,840 --> 00:03:52,520
of thing.

70
00:03:52,890 --> 00:03:59,310
They can use a method called Find which will return one matching tag or find all that will return a

71
00:03:59,310 --> 00:04:04,770
list of matching tags so find one paragraph versus find all paragraphs.

72
00:04:05,130 --> 00:04:08,030
And we can also navigate using CSSA selectors.

73
00:04:08,120 --> 00:04:10,660
If you haven't seen or worked with C S S Don't worry.

74
00:04:10,920 --> 00:04:16,350
But if you have we can use just as style selectors you can use a method called select but let's just

75
00:04:16,350 --> 00:04:18,550
start with the basic tag name selectors.

76
00:04:18,600 --> 00:04:24,170
So first things first I need to import B-S for in all we want from it.

77
00:04:24,180 --> 00:04:28,370
I'm actually going to switch this up from B-S for imports.

78
00:04:28,440 --> 00:04:34,230
Beautiful can spelt be a beautiful soup like that.

79
00:04:34,380 --> 00:04:35,000
OK.

80
00:04:35,310 --> 00:04:42,450
And then we'll just make a variable that's just called Soup equals beautiful soup and we passed in this

81
00:04:42,450 --> 00:04:45,990
string of H Tim L that is called H Timo that.

82
00:04:46,140 --> 00:04:52,760
And then we need to pass an h l dot parser and what will happen now is that soup if we just do pre-print

83
00:04:52,770 --> 00:04:57,140
soup and then we also do a print type of soup.

84
00:04:57,300 --> 00:04:58,290
I run it.

85
00:04:58,590 --> 00:05:02,830
The representation that is printed to us looks just like the aged string.

86
00:05:02,850 --> 00:05:05,370
However that class is not string.

87
00:05:05,520 --> 00:05:06,960
It is a beautiful soup.

88
00:05:06,960 --> 00:05:12,540
It's an instance of beautiful soup and now has a bunch of methods that we can use to parse it even though

89
00:05:12,540 --> 00:05:15,300
it doesn't look like it has when it printed out.

90
00:05:15,360 --> 00:05:22,500
I promise you it has parsed it so we can do things now like let's do print soop dot and then we can

91
00:05:22,500 --> 00:05:23,660
do dot body.

92
00:05:24,060 --> 00:05:26,070
So that would be this right here.

93
00:05:26,220 --> 00:05:29,370
And if I run it like what we get.

94
00:05:29,450 --> 00:05:31,160
This is the body.

95
00:05:31,160 --> 00:05:38,180
OK so that's one thing we can also do things like print souped up body dot div.

96
00:05:38,270 --> 00:05:40,070
There are two divs in the body.

97
00:05:40,070 --> 00:05:41,860
Notice what happens.

98
00:05:42,060 --> 00:05:49,320
I run it and we only get the first div So just be aware that this was the first div it had the three

99
00:05:49,330 --> 00:05:50,540
in the paragraph in it.

100
00:05:50,570 --> 00:05:55,180
We didn't get this bottom div So we'll talk about find all in just a moment.

101
00:05:55,190 --> 00:05:56,870
We can also do this.

102
00:05:57,050 --> 00:06:06,970
Print soop find and then passing a string div and this gives us the same thing.

103
00:06:06,980 --> 00:06:13,010
Also notice when it's printed out it looks like it's a string but none of this is actually strings.

104
00:06:13,040 --> 00:06:17,610
If we look at what this actually is souped up find div and what this returns.

105
00:06:17,630 --> 00:06:24,330
Call it D and then the print type of d i run it.

106
00:06:24,560 --> 00:06:28,260
It's a beautiful suit for element tag instance.

107
00:06:28,310 --> 00:06:33,140
It's not a string it is its own instance of its own object.

108
00:06:33,140 --> 00:06:38,000
So when we parse h him it's taking beautiful soup takes that giant string and it goes through and it

109
00:06:38,000 --> 00:06:38,380
turns.

110
00:06:38,390 --> 00:06:44,230
Each individual tag into its own object and then it puts them all together into the main.

111
00:06:44,270 --> 00:06:50,510
What we have here soup this main variable whatever is returned from here contains all of the instances

112
00:06:50,510 --> 00:06:51,660
of elements.

113
00:06:52,040 --> 00:06:59,300
OK so we can do fine div but we can also do find all div and if I do that I'm only printing that's just

114
00:06:59,300 --> 00:07:03,010
print D now and run it.

115
00:07:03,470 --> 00:07:09,890
We now get a list and the first item in there is the first object that contains the div and then our

116
00:07:09,890 --> 00:07:12,500
second object div.

117
00:07:12,510 --> 00:07:17,610
OK so that's finding based off of a tag name we can do find or find all we can change this to be an

118
00:07:17,610 --> 00:07:18,240
ally.

119
00:07:18,260 --> 00:07:22,120
We do find all we get all allies returned in the list.

120
00:07:22,170 --> 00:07:26,830
If I do find instead a find all we only get the first one as you can see here.

121
00:07:27,320 --> 00:07:29,100
Let's make some space.

122
00:07:29,220 --> 00:07:35,700
The next thing I'll talk about is selecting using attributes for example an ID or a class.

123
00:07:35,700 --> 00:07:39,010
So here we have an idea on this div called first.

124
00:07:39,040 --> 00:07:46,120
Now if I wanted to select using that I can do souped up find and then I'd just say ID equals first.

125
00:07:46,680 --> 00:07:53,820
And this is going to find me whatever the element that has Id set to first and retrieve it and never

126
00:07:53,820 --> 00:08:03,110
get that div and if I move this to the second lie and save now I get that second lie

127
00:08:05,830 --> 00:08:10,770
Likewise I can select based off of a class an idea supposed to only be used once.

128
00:08:10,780 --> 00:08:16,750
So you wouldn't use find all you shouldn't have to but we would use find all on a class.

129
00:08:16,750 --> 00:08:19,380
So we have class equals special.

130
00:08:20,370 --> 00:08:20,900
OK.

131
00:08:20,960 --> 00:08:25,680
Oh I need two essays and if I run that you'll see that there's a problem.

132
00:08:25,700 --> 00:08:31,090
It's like saying hey wait a minute that's a special word in Python so this won't work if I try it.

133
00:08:31,090 --> 00:08:34,360
It doesn't thinks I'm declaring a class instead.

134
00:08:34,470 --> 00:08:36,230
If to do class underscore.

135
00:08:36,400 --> 00:08:42,860
So that's just something you have to remember about when you're working with beautiful soup class underscore.

136
00:08:43,000 --> 00:08:48,730
And now we get those two items class equal special class equal special.

137
00:08:48,730 --> 00:08:55,330
We can also select based off of attributes like this thing has a data attribute or data examples set

138
00:08:55,330 --> 00:08:56,540
to Yes.

139
00:08:56,590 --> 00:09:02,500
And if I wanted to do that instead let just duplicate this comment that one out of the argument name

140
00:09:02,740 --> 00:09:11,080
is adder's like this and then pass in a dictionary and I'm looking for where data for example is and

141
00:09:11,080 --> 00:09:11,640
I can tell.

142
00:09:11,710 --> 00:09:15,110
Colon Yes.

143
00:09:15,220 --> 00:09:16,730
So give me all.

144
00:09:16,900 --> 00:09:21,520
Every item every element that has the data example attribute set to Yes.

145
00:09:21,520 --> 00:09:25,530
So if I give that two things let's do this.

146
00:09:25,840 --> 00:09:34,050
They div at the bottom it says by this and I right we get the age 3 and the div..

147
00:09:34,370 --> 00:09:40,830
So this is how we can select based off of attributes whether it's you know a source for an image and

148
00:09:40,860 --> 00:09:42,710
a track for an anchor tag.

149
00:09:42,990 --> 00:09:46,080
We can also select based off of class and ideas we saw and tag them.
