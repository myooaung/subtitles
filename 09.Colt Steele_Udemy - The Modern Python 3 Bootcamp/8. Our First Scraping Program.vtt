WEBVTT
1
00:00:00.210 --> 00:00:02.460
OK this is it.

2
00:00:02.460 --> 00:00:04.830
Or finally going to disgrace a real web site.

3
00:00:04.920 --> 00:00:06.820
We're not working with dummy data anymore.

4
00:00:06.960 --> 00:00:09.590
We've learned enough about beautiful soup on its own.

5
00:00:09.600 --> 00:00:13.990
Now we're going to connect it with requests and we're going to write to see history file.

6
00:00:14.010 --> 00:00:15.280
So what's our task.

7
00:00:15.630 --> 00:00:20.750
Well as I mentioned before in the intro video to the section Scraping is a kind of delicate issue.

8
00:00:20.850 --> 00:00:26.460
I'm not I don't feel comfortable sending however many thousands of students to go scrape you know some

9
00:00:26.460 --> 00:00:31.690
well-known website where I could be liable for something that they did and I want to be the person who

10
00:00:31.860 --> 00:00:35.620
encourages people to do something that those sites may disapprove of.

11
00:00:36.090 --> 00:00:41.700
So we are going to scrape a site that I 100 percent know we have permission to scrape because it's a

12
00:00:41.700 --> 00:00:43.960
site that I've helped make.

13
00:00:44.220 --> 00:00:47.460
So this is the blog for rhythm's school.

14
00:00:47.710 --> 00:00:54.840
I won't go into full advertising mode but it's a boot camp that I help out at and they have a blog that

15
00:00:54.840 --> 00:00:58.360
has a bunch of posts on it and we're just going to scrape these posts.

16
00:00:58.390 --> 00:01:04.080
They're in a very easy to extract format and all that we'll do is we'll scrape through and grab the

17
00:01:04.080 --> 00:01:10.700
title and they'll link as well as the date and then we'll store all of them in a CACP file so we'll

18
00:01:10.710 --> 00:01:12.480
scrape all of them on this page.

19
00:01:12.490 --> 00:01:13.990
There you are al by the way.

20
00:01:14.020 --> 00:01:17.920
All repeat this in the video it's rhythm's school dot com slash blog.

21
00:01:18.390 --> 00:01:24.270
So whatever we do here what we do here will apply to other Web sites that may or may not approve of

22
00:01:24.270 --> 00:01:26.550
scraping but this one absolutely does.

23
00:01:26.550 --> 00:01:27.960
So we don't have to worry.

24
00:01:27.960 --> 00:01:34.410
So our goal scrape data into a CSP file We're going to grab all links from rhythm schools blog and then

25
00:01:34.410 --> 00:01:39.670
we're going to store that you know the text inside the anchor tag and a date.

26
00:01:39.690 --> 00:01:40.700
So let's get started.

27
00:01:40.700 --> 00:01:43.700
I have a brand new file called blague scraping up why.

28
00:01:44.070 --> 00:01:47.890
And let's just get our dependencies in here so we're going to need to import.

29
00:01:48.180 --> 00:01:53.390
Well first thing Lamport is requests because we're going to need to make a request to this you are well.

30
00:01:53.750 --> 00:02:02.300
So that's first then we're going to get beautiful soup from B-S for import Tafel soup.

31
00:02:02.610 --> 00:02:05.340
And while we're up here we'll import CSB.

32
00:02:05.340 --> 00:02:08.840
So the order of things is that we're going to make the request get the response back.

33
00:02:08.880 --> 00:02:14.610
Take the team Shall we get back send it to beautiful soup then navigate through that extracts the info

34
00:02:14.610 --> 00:02:18.400
we want and write it to a file using CXVII.

35
00:02:18.450 --> 00:02:21.560
It's a lot but it actually won't take that much code.

36
00:02:21.810 --> 00:02:26.910
So let's start with a very simple stewe response equals requests don't get.

37
00:02:27.330 --> 00:02:31.500
And then this you know is a string.

38
00:02:32.250 --> 00:02:37.680
And let's just start by printing response and if we just print response for this get a response object

39
00:02:37.890 --> 00:02:43.220
if we want to see the actual body the stuff that comes back we need to do dot text.

40
00:02:43.470 --> 00:02:47.940
Let's just make sure we get him back when we run it and I might take a moment.

41
00:02:48.120 --> 00:02:48.890
There we go.

42
00:02:48.900 --> 00:02:49.770
This looks good to me.

43
00:02:49.770 --> 00:02:51.440
This is all Timo.

44
00:02:51.990 --> 00:02:53.100
So you can see it here.

45
00:02:53.160 --> 00:02:59.040
And this is not ideal to try and figure out what we want we could find the articles in here.

46
00:02:59.100 --> 00:03:05.650
I'm sure if we looked hard enough looks like here's one of the article here.

47
00:03:05.670 --> 00:03:06.000
Yeah.

48
00:03:06.060 --> 00:03:06.560
There we go.

49
00:03:06.570 --> 00:03:09.990
There's a section heading Here's a link the title.

50
00:03:09.990 --> 00:03:14.880
And here's a date that is just not how we want to go about it because this is very difficult to actually

51
00:03:15.120 --> 00:03:17.420
read and kind of understand in this terminal.

52
00:03:17.430 --> 00:03:23.950
So what we do instead is go to the you are self and visited and open up the developer tools.

53
00:03:24.240 --> 00:03:27.540
What we're trying to extract to start is the title.

54
00:03:27.660 --> 00:03:30.390
So each title here we can just find one.

55
00:03:30.420 --> 00:03:31.890
They all follow the same pattern.

56
00:03:31.900 --> 00:03:40.000
So right click and then go to inspect and then this opens up the developer tools and you can see see

57
00:03:40.010 --> 00:03:42.660
if we can make this bigger for you.

58
00:03:42.710 --> 00:03:44.340
It is an anchor tag.

59
00:03:44.810 --> 00:03:46.420
And even before that you can see it.

60
00:03:46.430 --> 00:03:51.880
Each article here each blog post is enclosed in an article element.

61
00:03:51.890 --> 00:03:54.900
So there's that type of NHT e-mail tag called an Article.

62
00:03:54.950 --> 00:04:00.800
So each one of those what we're going to want to do is just select all of these articles on the page

63
00:04:01.340 --> 00:04:07.400
and then for each article we're going to want to select the age for and the anchor tag inside of that

64
00:04:07.910 --> 00:04:12.990
we'll save the URL and we'll also save that text instead of the title.

65
00:04:13.100 --> 00:04:19.370
And then finally to get the date you could look in here let's see you'd probably right click here.

66
00:04:19.430 --> 00:04:24.500
This is one place where the date is stored and this is pretty common that you might find more than one

67
00:04:24.500 --> 00:04:25.600
place to find the date.

68
00:04:25.700 --> 00:04:28.100
For example here it is on the page.

69
00:04:28.100 --> 00:04:35.110
But then here is this time element that has a date time set to 2018 January 2nd 2018.

70
00:04:35.450 --> 00:04:39.050
This doesn't show up on the page if you actually click on it.

71
00:04:39.050 --> 00:04:40.240
Is it showing up anywhere.

72
00:04:40.250 --> 00:04:41.380
I don't think so.

73
00:04:41.400 --> 00:04:45.880
No it's just metadata that is basically encoded with this article.

74
00:04:45.980 --> 00:04:46.940
So we can grab it from here.

75
00:04:46.940 --> 00:04:51.010
This is easy or we could grab it from here but this isn't a nice date time format.

76
00:04:51.140 --> 00:04:53.370
This is more English formatted.

77
00:04:53.630 --> 00:04:54.020
OK.

78
00:04:54.170 --> 00:04:55.450
So that's our game plan.

79
00:04:55.460 --> 00:04:58.460
We're going to select all articles and we'll start there.

80
00:04:58.550 --> 00:05:04.160
Then for each article we're going to select the anchor tag inside the page for us.

81
00:05:04.160 --> 00:05:08.060
There might be more than one anchor tag that might be the only one we'll play around and see if that's

82
00:05:08.060 --> 00:05:12.910
the only one will be nice and easy and then we'll select the left to get that you are.

83
00:05:13.100 --> 00:05:13.970
So each link.

84
00:05:14.210 --> 00:05:18.910
And then also select the title as well as a date which is in div class card.

85
00:05:18.920 --> 00:05:19.370
All right.

86
00:05:19.490 --> 00:05:21.400
So this process is really common.

87
00:05:21.410 --> 00:05:22.710
You have a site you want to scrape.

88
00:05:22.850 --> 00:05:28.220
You just have to understand the structure first because the scraping is just you telling it where to

89
00:05:28.220 --> 00:05:28.580
look.

90
00:05:28.580 --> 00:05:30.850
Where should your Python code look in the HMO.

91
00:05:31.040 --> 00:05:33.200
Well you have to know first in order to direct it.

92
00:05:33.200 --> 00:05:38.660
So back here we're now going to send this response that text and send it over to beautiful soup and

93
00:05:38.780 --> 00:05:39.160
send it.

94
00:05:39.170 --> 00:05:41.890
I mean we're going to instantiate a new instance.

95
00:05:41.960 --> 00:05:43.610
So let's just call this soup.

96
00:05:43.710 --> 00:05:48.210
It goes to beautiful soup and then will pass in response to that text.

97
00:05:48.230 --> 00:05:53.590
And then remember this sort of annoying thing is Tim Al-Kut parser as a string and we can get rid of

98
00:05:53.590 --> 00:05:54.360
this.

99
00:05:54.470 --> 00:05:58.190
And then with the soup we should be able to do something like soup.

100
00:05:58.220 --> 00:06:03.680
And if we want to find all article tags it just find all article.

101
00:06:03.800 --> 00:06:09.060
So let's save that to a variable called articles and then just print articles.

102
00:06:09.530 --> 00:06:09.910
OK.

103
00:06:09.920 --> 00:06:11.760
So we're requesting this euro.

104
00:06:11.780 --> 00:06:13.100
We get him back.

105
00:06:13.100 --> 00:06:14.490
We give it to beautiful soup.

106
00:06:14.490 --> 00:06:15.980
We tell us please parse this.

107
00:06:15.980 --> 00:06:16.760
Take it from a string.

108
00:06:16.760 --> 00:06:20.230
Turn it into Tim object or turn it into objects in Python.

109
00:06:20.390 --> 00:06:25.180
And then we can call find all one of the built in methods on that result of soup.

110
00:06:25.400 --> 00:06:29.900
Find articles and ideally we'll have them all in a collection.

111
00:06:29.900 --> 00:06:34.180
Let's see béclère this takes a moment.

112
00:06:34.640 --> 00:06:36.530
And if we look all right we have a list.

113
00:06:36.530 --> 00:06:42.170
You can see the bracket down here and if we scroll way up probably here there's a lot of text here.

114
00:06:42.320 --> 00:06:43.130
Here's the beginning.

115
00:06:43.220 --> 00:06:48.560
But you can see we're getting every article as well as all the inclosing information in each article.

116
00:06:48.560 --> 00:06:51.080
So here's this is one article right there.

117
00:06:51.560 --> 00:06:52.230
Awesome.

118
00:06:52.520 --> 00:06:58.550
Now what we want to do is for each article let's just start with the title which as you can see is located

119
00:06:58.550 --> 00:06:59.870
in the anchor tag.

120
00:07:00.140 --> 00:07:02.830
And then it's the text inside that anchor tag.

121
00:07:03.380 --> 00:07:07.610
So because it's the first anchor tag as well as the is it the only one.

122
00:07:07.650 --> 00:07:13.940
No there's another one here but it's the first one we can just called Dot find rather than find all.

123
00:07:13.990 --> 00:07:17.100
So a loop through articles however you'd like.

124
00:07:17.130 --> 00:07:26.980
But for article in articles let's just print and we're going to do something like article for each individual

125
00:07:26.980 --> 00:07:30.050
one does find an anchor tag.

126
00:07:30.650 --> 00:07:34.400
And that's going to give us the whole anchor tag if we print that.

127
00:07:34.610 --> 00:07:35.300
See what happens.

128
00:07:35.310 --> 00:07:42.110
Now all right so we're getting all the anchor tags what we want is the text inside.

129
00:07:42.140 --> 00:07:49.620
And remember we saw a method to do that dot get text as a method and no moment of truth.

130
00:07:50.710 --> 00:07:52.450
Awesome look we're getting all the titles.

131
00:07:52.450 --> 00:07:53.760
How exciting.

132
00:07:54.100 --> 00:07:59.380
I actually haven't gone through it deliberately didn't go do this ahead of time and know the solution

133
00:07:59.380 --> 00:08:04.150
because I wanted to show the process of identifying the information you want to scrape.

134
00:08:04.150 --> 00:08:07.180
So I am very relieved that it worked OK.

135
00:08:07.450 --> 00:08:13.300
So that gets us the text that gives us the title so we could I don't know make a variable to start and

136
00:08:13.300 --> 00:08:18.850
just call it title equals article about find a get text.

137
00:08:18.850 --> 00:08:20.850
Remember we're trying to find title.

138
00:08:21.010 --> 00:08:27.760
We also want to find the date and we want to find the ATF the URL for the article.

139
00:08:28.210 --> 00:08:28.600
OK.

140
00:08:28.690 --> 00:08:32.860
And then we're going to take all of them and add them to a file that we haven't done yet.

141
00:08:32.860 --> 00:08:35.730
Next let's actually go with the ref.

142
00:08:36.010 --> 00:08:38.860
The attribute on the same anchor tag.

143
00:08:38.860 --> 00:08:46.930
So we're going to do article about find out get text and then we could do article but find again anchor

144
00:08:46.930 --> 00:08:57.170
tag and use the this syntax to find a Tref or there's adder's that we saw if you just print that that

145
00:08:57.180 --> 00:09:00.110
suit we get there we go.

146
00:09:00.120 --> 00:09:01.690
It's just full of ATF.

147
00:09:01.920 --> 00:09:11.400
So you could do this Todd adder's a Tref but there's a shorter syntax which is just to directly use

148
00:09:11.490 --> 00:09:13.310
a ref like this.

149
00:09:15.150 --> 00:09:16.440
And we run it now.

150
00:09:17.800 --> 00:09:18.570
Perfect.

151
00:09:18.610 --> 00:09:25.090
So we're now getting each you RL But instead of searching or doing article to find twice it would be

152
00:09:25.090 --> 00:09:32.990
better to just do it in a variable like let's go with a a tag equals that article about find a.

153
00:09:33.130 --> 00:09:37.290
And then here we can do a tag get text and then same thing here.

154
00:09:37.360 --> 00:09:43.430
Replace this a tag a Tref and now it should just get a list of eight drafts.

155
00:09:44.500 --> 00:09:45.050
Good.

156
00:09:45.110 --> 00:09:46.010
Nothing changed.

157
00:09:46.060 --> 00:09:50.820
And let's save that to variable and just call it a ref equals a tag.

158
00:09:51.130 --> 00:09:52.490
Or how about you throw.

159
00:09:52.690 --> 00:09:57.640
And then we have the last one which is the date and remember the date.

160
00:09:57.640 --> 00:10:03.730
If we go back here is inside the article if this is one article here.

161
00:10:03.970 --> 00:10:11.670
It's not inside the H for it's not inside that anchor tag but it is inside this time element.

162
00:10:11.800 --> 00:10:16.820
So we can just to find time always trying to find the time.

163
00:10:16.900 --> 00:10:17.860
Very funny.

164
00:10:17.860 --> 00:10:25.180
So we can do it instead of a tag what we want to reference is article but find the time element.

165
00:10:25.180 --> 00:10:27.410
The first one I think there's only one in there.

166
00:10:27.430 --> 00:10:28.480
In each article.

167
00:10:28.770 --> 00:10:30.220
And let's just print what we get

168
00:10:33.020 --> 00:10:35.600
what happens awesome.

169
00:10:36.130 --> 00:10:37.420
So that gets us this far.

170
00:10:37.420 --> 00:10:43.270
And now what we want is the value of the date time attribute which is exactly the same.

171
00:10:43.270 --> 00:10:45.490
Or we can use the same syntax we could do.

172
00:10:45.650 --> 00:10:53.180
TR s or just directs the access date time as a string.

173
00:10:53.200 --> 00:10:57.770
And if I do that now I assume are getting the date.

174
00:10:58.300 --> 00:11:06.380
And now let's just save that variable Let's call that date like that perfect.

175
00:11:06.470 --> 00:11:12.140
And before we actually go ahead and deal with cxxviii let's just do a simple print of title and then

176
00:11:12.140 --> 00:11:15.420
you URL and then date like this for each one.

177
00:11:15.500 --> 00:11:20.400
And let's just make sure it works it looks good.

178
00:11:20.400 --> 00:11:25.560
So we're getting the title we're getting the URL and then we're getting the date.

179
00:11:26.070 --> 00:11:30.930
And you can see you know these things are that you are Ellas correct at least New Year's resolutions

180
00:11:31.120 --> 00:11:34.670
is the title and the URL is blogsite New Year's resolutions.

181
00:11:34.920 --> 00:11:39.250
And we can just trust that the data is correct because we're working inside the same article.

182
00:11:39.420 --> 00:11:42.540
So we're only burrowing deeper into the article that makes sense.

183
00:11:42.540 --> 00:11:45.260
Everything is collected inside each article tag.

184
00:11:45.360 --> 00:11:47.550
So we've actually finished the scraping portion.

185
00:11:47.550 --> 00:11:50.880
We've requested the page we've found the data we want.

186
00:11:50.880 --> 00:11:54.710
Now we're just going to write it to a file and this will actually be review.

187
00:11:54.840 --> 00:11:58.120
So we have CXXVI and I'm going to do it here.

188
00:11:58.140 --> 00:12:03.130
We'll start with opening a file that it doesn't exist yet let's just call it blog data.

189
00:12:03.150 --> 00:12:09.140
Dot cxxviii and we need to open that as a right with the right mode as.

190
00:12:09.420 --> 00:12:12.350
And let's call it CSB file.

191
00:12:12.890 --> 00:12:24.100
And then we need to writer equals the writer or I actually prefer to do from cxxviii import writer so

192
00:12:24.100 --> 00:12:29.380
we could use a dict writer but we have our data here would be easiest just to put it in a list in my

193
00:12:29.380 --> 00:12:30.030
opinion.

194
00:12:30.160 --> 00:12:39.370
So we'll just do this from CSP import writer and then we'll say story writer equals writer and then

195
00:12:39.370 --> 00:12:40.900
we need to give it the file.

196
00:12:40.900 --> 00:12:45.880
If this is confusing make sure you don't have to of course but it would help to go back and rewashed

197
00:12:45.880 --> 00:12:46.680
that section.

198
00:12:46.840 --> 00:12:52.900
But this is the syntax you give it a file call writer which is the function here that gives us the CSP

199
00:12:52.900 --> 00:12:55.720
writer with that file bug data.

200
00:12:55.720 --> 00:13:03.580
And then I just call writer dot right row and let's start by writing the headers so we give it a list

201
00:13:04.000 --> 00:13:04.920
and the headers.

202
00:13:04.930 --> 00:13:10.840
Let's go with title and link and then date.

203
00:13:11.290 --> 00:13:12.730
And let's just verify.

204
00:13:12.730 --> 00:13:14.140
We'll even comment out the requests.

205
00:13:14.140 --> 00:13:18.350
Let's just make sure that we end up with the file and the headers are in there.

206
00:13:18.910 --> 00:13:21.950
And it says object has no attribute right row.

207
00:13:22.060 --> 00:13:24.590
And that would be because there is nothing called writer.

208
00:13:24.610 --> 00:13:26.080
I mean that's the name of the function.

209
00:13:26.170 --> 00:13:29.330
But we need to call it on CSB writer.

210
00:13:29.500 --> 00:13:31.020
Hopefully you saw that before me.

211
00:13:32.390 --> 00:13:40.620
And now if we look let's open up the log data see history file.

212
00:13:40.630 --> 00:13:41.910
All right so we have our headers in there.

213
00:13:41.920 --> 00:13:46.430
Now all we have to do is actually add in all the data that we've requested.

214
00:13:46.640 --> 00:13:49.530
Let's begin by just commenting this.

215
00:13:49.630 --> 00:13:53.080
And then what we need to do is indent all this code.

216
00:13:53.110 --> 00:13:56.320
First of all so that we're doing it while the file is open.

217
00:13:56.590 --> 00:14:01.840
If we're going to use a with statement so we want to do it while it's open we write the headers and

218
00:14:01.840 --> 00:14:06.490
then we're going to loop through articles we extract the data we want.

219
00:14:06.490 --> 00:14:15.700
And then instead of printing we're just going to call CSP writer dot right row and then we have a list

220
00:14:16.180 --> 00:14:19.610
because that's how I see every writer works for not using dict writer.

221
00:14:19.720 --> 00:14:25.780
And I just pass in our variables so we have title we need to match the order we defined our peers or

222
00:14:25.780 --> 00:14:27.770
title comma you are l.

223
00:14:27.790 --> 00:14:28.960
That's the name I went with.

224
00:14:29.200 --> 00:14:35.550
And then date and the moment of truth is so exciting.

225
00:14:36.090 --> 00:14:41.430
What happens didn't mess anything up and sweet.

226
00:14:41.640 --> 00:14:45.740
So we have a cxxviii file now we've successfully scraped data.

227
00:14:45.870 --> 00:14:51.330
I know it's not the most exciting data ever but just use your imagination replace this with some juicy

228
00:14:51.330 --> 00:14:57.410
hard to obtain data that doesn't have an API and you can see how this can quickly become useful.

229
00:14:57.420 --> 00:14:59.060
So we've got title linking date.

230
00:14:59.200 --> 00:15:00.170
Just take a look.

231
00:15:00.300 --> 00:15:01.790
Here's our first title.

232
00:15:02.100 --> 00:15:08.040
Here's the comma and then the date that you are l the link and then the date and we grabbed all this

233
00:15:08.040 --> 00:15:12.210
from a web page making a real request and getting data back.

234
00:15:12.240 --> 00:15:13.050
Fun stuff.

235
00:15:13.200 --> 00:15:15.480
And if you want an extra challenge.

236
00:15:15.480 --> 00:15:22.410
One thing you could try right now we're only scraping whatever's on the front page but there are at

237
00:15:22.410 --> 00:15:24.700
least what five pages right now.

238
00:15:24.700 --> 00:15:26.120
Maybe more I'm not sure.

239
00:15:26.220 --> 00:15:27.770
They might truncate how many show up.

240
00:15:27.780 --> 00:15:33.430
Now there's five pages of blog posts and you might want to automate it to that.

241
00:15:33.450 --> 00:15:38.850
It starts here on this first page and it grabs all of these links because each one of these is a link

242
00:15:39.030 --> 00:15:40.890
and this is where we get into web crawling.

243
00:15:41.160 --> 00:15:41.980
So you would.

244
00:15:42.300 --> 00:15:46.930
What are these listed as Looks like they're called the page.

245
00:15:46.950 --> 00:15:48.520
I think I accidentally clicked on it.

246
00:15:48.570 --> 00:15:49.730
Try one more time.

247
00:15:49.920 --> 00:15:50.780
Can see.

248
00:15:51.060 --> 00:15:51.350
Yes.

249
00:15:51.360 --> 00:15:54.590
These are spans of the class he called a page currents.

250
00:15:54.600 --> 00:15:59.890
These are span's and inside there there's an a tag that says previous.

251
00:15:59.940 --> 00:16:06.420
So we do something like search for all Spanta to have classic little page then grab that you are ill.

252
00:16:06.450 --> 00:16:12.450
So this is the home page slash blog but the next one should be slash blog question mark page equals

253
00:16:12.450 --> 00:16:15.100
three and four.

254
00:16:15.120 --> 00:16:20.400
And also if you knew that it was just a number incrementing we could do it programmatically and just

255
00:16:20.610 --> 00:16:27.900
have a variable called counter and keep adding one until we don't get a response until we get a negative.

256
00:16:27.960 --> 00:16:35.100
Good you know for a 4 or something if I tried to request page 70 then we would stop and then for each

257
00:16:35.160 --> 00:16:39.800
result we get back we would then scrape the articles if that makes sense.

258
00:16:39.810 --> 00:16:44.910
So you kind of loop you get all of these you or else you figure out which you or else you want to scrape

259
00:16:44.940 --> 00:16:47.490
and then you just loop through and call each one.

260
00:16:47.640 --> 00:16:50.470
But to be polite it's best to put in a delay.

261
00:16:50.490 --> 00:16:54.570
So you're not going one after another with you know a millisecond in between.

262
00:16:54.570 --> 00:16:57.370
Anyway that is a fun challenge if you want to try it out.
