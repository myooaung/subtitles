1
00:00:00,210 --> 00:00:01,370
Hey welcome back.

2
00:00:01,380 --> 00:00:03,910
This is a video I am really excited about.

3
00:00:03,930 --> 00:00:08,390
I'm going to show you a pretty powerful tool called scrappy slash scrappy.

4
00:00:08,390 --> 00:00:10,320
I've never known how it's actually pronounced.

5
00:00:10,320 --> 00:00:13,290
I just say both internally in my head.

6
00:00:13,290 --> 00:00:20,430
So one of those whatever it's pronounced it is a really really really powerful three release framework

7
00:00:20,430 --> 00:00:22,780
for scraping and write a framework.

8
00:00:22,930 --> 00:00:25,580
It's a it's different than things we've seen so far.

9
00:00:25,580 --> 00:00:32,760
Like the requests module or beautiful soup our single purpose separate entities that we can connect

10
00:00:32,760 --> 00:00:37,860
together and we have a lot of flexibility in how we actually structure our code if we want to write

11
00:00:37,860 --> 00:00:40,580
it class if we want to write just a bunch of functions.

12
00:00:40,590 --> 00:00:44,030
If we don't want any functions at all we can name things whatever we want.

13
00:00:44,280 --> 00:00:46,890
Scrappy skyscraper is very different.

14
00:00:46,920 --> 00:00:53,190
It's a framework for extracting data bundles together the making of the HDP requests along with the

15
00:00:53,190 --> 00:00:59,100
extraction of the data and writing the data to a file and it can very easily change from writing to

16
00:00:59,100 --> 00:01:01,200
a CXXVI file to adjacent file.

17
00:01:01,260 --> 00:01:05,350
It has all of this functionality built in with just a couple of lines.

18
00:01:05,460 --> 00:01:09,570
We can get some really really impressive results if we had to do it on our own.

19
00:01:09,570 --> 00:01:11,750
It would it would take significantly longer.

20
00:01:11,880 --> 00:01:16,590
But the downside is that we have to follow the rules that the framework sets up.

21
00:01:16,590 --> 00:01:22,050
We have to name things a particular way we have to have a special variable called Start you or else

22
00:01:22,050 --> 00:01:26,040
for example because it's going to look for that variable name.

23
00:01:26,040 --> 00:01:30,060
So if we want to use it any framework if you want to use a framework whether it's for scraping or web

24
00:01:30,060 --> 00:01:37,210
development or anything else you generally end up sacrificing flexibility for speed and ease of use.

25
00:01:37,590 --> 00:01:42,240
Like any external module we need to start by running the install.

26
00:01:42,240 --> 00:01:46,160
So Pipp install rapee scrappy.

27
00:01:46,260 --> 00:01:47,840
I already have it on here.

28
00:01:47,880 --> 00:01:53,640
It is a bit of a long install compared to some other things because it's actually installing a lot of

29
00:01:53,700 --> 00:01:56,570
other modules and packages that it relies on.

30
00:01:57,060 --> 00:01:58,920
And then I'm going to make a file.

31
00:01:59,190 --> 00:02:06,170
And let's just call this book scraper why we're going to be working with this same web site.

32
00:02:06,180 --> 00:02:12,050
What's it called books to scrape or to scraped outcome except we're going to work with books.

33
00:02:12,060 --> 00:02:13,820
There are a whole bunch of them.

34
00:02:13,820 --> 00:02:18,960
It's 50 pages of books and we are going to automate the scraping of all of them and get data from all

35
00:02:18,960 --> 00:02:19,850
of them.

36
00:02:20,400 --> 00:02:23,400
I mean it's going to put that you are out here the way that scraper works.

37
00:02:23,400 --> 00:02:26,410
If you want to look at the documentation they have some very good docs.

38
00:02:26,700 --> 00:02:33,960
Unlike a lot of other things in Python where it talks about dependencies but also lots of examples how

39
00:02:33,960 --> 00:02:37,870
you can create what are known as spiders and that's what we're going to create now.

40
00:02:38,160 --> 00:02:43,530
And it's a class that defines how a certain site or group of sites will be scraped including how to

41
00:02:43,530 --> 00:02:44,920
perform the crawl.

42
00:02:44,970 --> 00:02:51,480
So crawl refers to scraping a page and then and then moving onto the next page finding another link

43
00:02:51,480 --> 00:02:56,460
on that site and following that and scraping it and then following a link from that site which is a

44
00:02:56,460 --> 00:02:58,000
really powerful concept.

45
00:02:58,080 --> 00:03:03,060
We're just going to scrape and crawl across the book's data but if you wanted to you know you could

46
00:03:03,060 --> 00:03:04,840
start on one page on Amazon.

47
00:03:04,960 --> 00:03:07,250
Now I recommend you do this and I want to get in trouble.

48
00:03:07,260 --> 00:03:11,870
But if you did scrape a single page on Amazon the front page how many links are on there.

49
00:03:11,940 --> 00:03:14,270
A thousand links something like that.

50
00:03:14,310 --> 00:03:15,530
Maybe a little less.

51
00:03:15,750 --> 00:03:22,950
But if you followed each one of those whether it was a book's link or a shopping cart link or sporting

52
00:03:22,950 --> 00:03:29,370
goods or recently viewed whatever best selling you click or you follow all of them dynamically using

53
00:03:29,370 --> 00:03:33,760
your code and then you scrape all the links on that page and you follow all of those.

54
00:03:33,780 --> 00:03:40,080
Pretty soon you'll most likely have a complete picture of every page on Amazon that you can access without

55
00:03:40,170 --> 00:03:41,910
logging in without credentials.

56
00:03:41,910 --> 00:03:46,020
It's the same idea that powers Google so Google knows about sites that come up.

57
00:03:46,020 --> 00:03:52,020
It's all about links which is why a lot of SEO has to do with links or at least it used to anyway.

58
00:03:52,270 --> 00:03:56,550
We're going to start on a very small scale but you could take what I'm going to show you and apply it

59
00:03:56,550 --> 00:04:00,870
to scraping something much more intense and that's why I'm showing it to you if you want to do some

60
00:04:00,870 --> 00:04:04,890
real intensive scraping scrape you can really be helpful.

61
00:04:04,920 --> 00:04:07,890
First and foremost we need to import scrape.

62
00:04:09,030 --> 00:04:11,010
And then we need to define a class.

63
00:04:11,550 --> 00:04:19,270
And I'm going to call it book spider and it has to inherit from scrape the spider.

64
00:04:19,320 --> 00:04:21,620
So this is some of the framework coming into play.

65
00:04:21,660 --> 00:04:27,800
It works by having us to find a class that inherits from scraping dust spider and then we have to define

66
00:04:27,810 --> 00:04:36,120
a couple of things like well defined a name in here just call it book spider and then we have to define

67
00:04:36,150 --> 00:04:38,200
a list called Start you or else.

68
00:04:38,220 --> 00:04:42,900
So these are the do or else that we're going to start off on like that the first thing to request and

69
00:04:42,900 --> 00:04:45,910
scrape and for us we're just going to start with this one.

70
00:04:45,930 --> 00:04:49,470
You are out right here and that's it.

71
00:04:49,470 --> 00:04:56,220
Next and most importantly we have to define the Parse method and scrape he is going to look for every

72
00:04:56,220 --> 00:04:59,370
spider that we define to have this parse method.

73
00:04:59,470 --> 00:05:06,630
So we'll do death parts and it works like this self Khama response.

74
00:05:06,640 --> 00:05:11,720
So this is how we want to parse the requests are the result of the request.

75
00:05:11,800 --> 00:05:16,560
So it's going to make this request automatically I don't have to say you know requests don't get I don't

76
00:05:16,570 --> 00:05:17,890
have to do any of that.

77
00:05:17,950 --> 00:05:23,860
It does it for me and then it's going to call parse and it's going to parse over and over depending

78
00:05:23,860 --> 00:05:29,160
on how we define parse and I'll show you that in a moment it's going to call it on every single response

79
00:05:29,160 --> 00:05:31,670
it gets back that it's that we want to scrape.

80
00:05:32,080 --> 00:05:36,460
So the response needs to be in there we can call it whatever we want just like we can call self whatever

81
00:05:36,460 --> 00:05:37,050
we want.

82
00:05:37,120 --> 00:05:42,890
That this is standard and response is going to refer to what we get back from the HTP request.

83
00:05:42,910 --> 00:05:46,130
So now we have to go over here and figure out the data we want.

84
00:05:46,240 --> 00:05:50,930
So if we inspect and we look at one of these books what are we going to save.

85
00:05:51,100 --> 00:05:54,080
Looks like each book is in an article.

86
00:05:54,310 --> 00:05:56,430
And I think that's true for all of them.

87
00:05:56,440 --> 00:05:58,780
Just move over here.

88
00:05:58,780 --> 00:06:02,930
That one is also in an article that has a class of product pade.

89
00:06:03,220 --> 00:06:05,860
So we are going to select all of those.

90
00:06:06,040 --> 00:06:11,410
Everything that has article or an article with a class product pod and then will loop through all of

91
00:06:11,410 --> 00:06:16,590
them and to find the price on this one for example.

92
00:06:16,690 --> 00:06:20,510
It's inside of a div with the class of product price.

93
00:06:20,770 --> 00:06:25,720
And actually inside of that we have a classical price color and that's what we're going to want.

94
00:06:25,720 --> 00:06:28,170
Here is the inner text of that.

95
00:06:28,630 --> 00:06:30,010
And then what else.

96
00:06:30,010 --> 00:06:33,850
The title title black dust.

97
00:06:33,850 --> 00:06:40,690
We find that here instead of an anchor tag with we just want to attribute here title title equals black

98
00:06:40,690 --> 00:06:42,040
dust or the inner text.

99
00:06:42,040 --> 00:06:46,600
But I'm going to show you how to get an attribute because the syntax we're no longer using beautiful

100
00:06:46,600 --> 00:06:47,040
soup.

101
00:06:47,100 --> 00:06:51,400
So everything we've seen in this section aside from the concept of how it works everything's going to

102
00:06:51,400 --> 00:06:53,410
be different syntactically.

103
00:06:53,740 --> 00:06:55,870
OK so we saw those classes.

104
00:06:55,900 --> 00:07:01,100
Now what we need to do is first start by calling response CSSA.

105
00:07:01,120 --> 00:07:02,820
And this is the method that we use.

106
00:07:02,830 --> 00:07:08,350
It's sort of like select when we use beautiful soup we had soup out select and then we could tell it

107
00:07:08,710 --> 00:07:11,640
find based off of a class name like Dot.

108
00:07:11,800 --> 00:07:12,850
What was it called.

109
00:07:12,970 --> 00:07:15,400
I don't know what special that would give.

110
00:07:15,400 --> 00:07:17,400
All items are a special class.

111
00:07:17,410 --> 00:07:24,520
We're going to do respond step C S S and we want to look for all articles with the class product pod.

112
00:07:24,700 --> 00:07:26,050
This is CSSA syntax.

113
00:07:26,050 --> 00:07:28,420
If you don't know C S S I apologize.

114
00:07:28,420 --> 00:07:34,360
This is how we specify all articles that have the class product pod and then we're going to do a four

115
00:07:34,780 --> 00:07:40,420
article in that the way this framework works when we create a spider.

116
00:07:40,420 --> 00:07:45,480
We need to use yield instead of return because it's going to continue to go over and over and over.

117
00:07:45,490 --> 00:07:48,340
So whatever we yield here let me just type it out.

118
00:07:48,340 --> 00:07:54,210
Yield and or just can yield a little dictionary that has a price set to something.

119
00:07:54,340 --> 00:07:56,220
So I'm going to just add what we want.

120
00:07:56,230 --> 00:07:57,630
It's not going to work just yet.

121
00:07:57,670 --> 00:08:00,420
So we're going to have a price and then we'll have tidal.

122
00:08:00,850 --> 00:08:03,640
Let's just start with that.

123
00:08:03,940 --> 00:08:04,430
OK.

124
00:08:04,490 --> 00:08:09,970
And so by yielding this it will at the end of the day be taken care of and added to a file this is how

125
00:08:09,970 --> 00:08:15,040
it works on the dock so you can see the Parse method is in charge of processing their response and returning

126
00:08:15,040 --> 00:08:18,970
scrape data and or more year up or else to follow.

127
00:08:18,970 --> 00:08:24,000
So to get the price we have the article ready so we can call article does see assess.

128
00:08:24,020 --> 00:08:29,300
Again this is the special method name C Ss So it's different than beautiful soup.

129
00:08:29,410 --> 00:08:35,200
So it's new and it may be confusing but you can always just look at the documentation and we want the

130
00:08:35,220 --> 00:08:35,420
key.

131
00:08:35,470 --> 00:08:37,230
What was it price color.

132
00:08:37,840 --> 00:08:46,630
And then to get the text we do this here and if we just leave it at this actually you know I just do

133
00:08:46,630 --> 00:08:47,450
this.

134
00:08:47,740 --> 00:08:50,910
This is only going to run right now on the first page here.

135
00:08:50,920 --> 00:08:54,720
We're not moving onto the next pages and continuing to crawl.

136
00:08:55,010 --> 00:08:59,100
And we're just going to have a single dictionary with price and I have nothing else.

137
00:08:59,100 --> 00:08:59,800
This is it.

138
00:08:59,830 --> 00:09:04,630
We're just extracting this information directly executed is a bit more complicated.

139
00:09:04,630 --> 00:09:05,500
We run.

140
00:09:05,560 --> 00:09:07,710
Well what I named this file book scraper.

141
00:09:07,870 --> 00:09:16,540
So we have to run great b space runs spider space dash o space and then the name of the file we want

142
00:09:16,540 --> 00:09:17,940
to save the data to.

143
00:09:18,190 --> 00:09:26,410
If we want bookstore CXVII books that Jason and we do see s v space then the name of the file book scraper

144
00:09:26,500 --> 00:09:27,620
duppy.

145
00:09:28,600 --> 00:09:31,430
And look at all that that happened there.

146
00:09:31,660 --> 00:09:34,010
Let's open up the books file.

147
00:09:34,510 --> 00:09:34,880
OK.

148
00:09:34,930 --> 00:09:40,170
So what we're getting here is you can see we're getting something for each price.

149
00:09:40,180 --> 00:09:41,710
This is what we see.

150
00:09:41,980 --> 00:09:43,720
And this is not quite correct.

151
00:09:43,780 --> 00:09:50,250
What we're getting is the object that is trying to be it's being printed out because when we use Scrappy

152
00:09:50,560 --> 00:09:55,550
just like with beautiful soup each element itself is represented with an object.

153
00:09:55,630 --> 00:09:58,470
So we need to extract the information from that object.

154
00:09:58,510 --> 00:10:00,760
We want to do the equivalent of soup.

155
00:10:00,820 --> 00:10:06,520
Find blah blah blog and then we do get text to get the middle or the inner text out.

156
00:10:06,670 --> 00:10:08,810
The equivalent is a method called extract.

157
00:10:08,830 --> 00:10:11,350
First the docs explain this pretty well.

158
00:10:11,350 --> 00:10:17,560
When we called C S S it's going to return to us Albacete even if it is only one item in and so you could

159
00:10:17,560 --> 00:10:25,810
do this give us the first item dot extract or there there's a built in method called extract first and

160
00:10:25,810 --> 00:10:31,120
that's just going to give us the text or the inner H came out of the first item which in our case there

161
00:10:31,120 --> 00:10:32,140
only is one item.

162
00:10:32,140 --> 00:10:33,480
So we want extract first.

163
00:10:33,670 --> 00:10:35,900
We could do this but extract first is nicer.

164
00:10:36,040 --> 00:10:47,550
So now we called Dot extract first as a method and if we rerun this takes a moment let's go back over

165
00:10:47,550 --> 00:10:49,240
to our CXXVI file.

166
00:10:49,620 --> 00:10:53,370
And I believe that it uses append mode by default.

167
00:10:53,610 --> 00:10:59,840
So it is not overwriting things we have the old stuff but down here you've got all the new prices.

168
00:11:00,060 --> 00:11:01,760
OK so we're getting price just fine.

169
00:11:01,890 --> 00:11:10,780
Let's quickly add the title and so the way we do title is going to set title to the article that says

170
00:11:10,980 --> 00:11:15,430
for each article it's the anchor tag instead of an age 3.

171
00:11:15,450 --> 00:11:21,690
There's probably other anchor tags potentially so we don't want to just say a tag you want to say H-3

172
00:11:21,870 --> 00:11:24,290
within a tag directly inside of it.

173
00:11:24,510 --> 00:11:32,910
And that's not too hard if you know C Ss can just type this anchor tag and you only want anchor tags

174
00:11:32,910 --> 00:11:42,540
with the attribute title and then we'll just call extract first on that.

175
00:11:42,540 --> 00:11:44,910
So as you can see the syntax is very different.

176
00:11:45,210 --> 00:11:47,520
It's not that different but it's different words.

177
00:11:47,640 --> 00:11:53,370
It's the same idea using CSA as selectors might seem foreign but at the end of the day it's just another

178
00:11:53,370 --> 00:11:58,890
implementation of scraping functionality just different method names instead of beautiful soup.

179
00:11:58,890 --> 00:12:03,480
We do it this way which is admitted the foreign to us at this point.

180
00:12:03,660 --> 00:12:05,110
But I think you'll be impressed.

181
00:12:05,130 --> 00:12:10,230
Hopefully already you've been impressed by how easy it is once we add this in here we tell it what to

182
00:12:10,230 --> 00:12:13,200
look for and it takes care of everything else.

183
00:12:13,200 --> 00:12:16,560
We never have to export it to a file that does it for us.

184
00:12:16,560 --> 00:12:22,740
And if we leave this as it is and rerun it with other a different file name sort of boxed up so yes

185
00:12:22,750 --> 00:12:27,120
we just do books to see history and what do we get.

186
00:12:27,120 --> 00:12:37,540
I don't have it open let's do books to awesome and automatically experts to CSB price and title like

187
00:12:37,630 --> 00:12:40,330
that our data is here from one page.

188
00:12:40,330 --> 00:12:44,350
So now all we want to do is update this so that it keeps going.

189
00:12:44,580 --> 00:12:49,400
And it's actually really easy what we want to do is just check if there is a next.

190
00:12:49,510 --> 00:12:57,180
So we're going to do response see SS and we have to identify where that next button is right here.

191
00:12:57,800 --> 00:12:58,580
OK.

192
00:12:58,750 --> 00:13:01,750
So there is an in line with the class of next.

193
00:13:01,750 --> 00:13:05,610
And then there's an anchor tag inside and we need the ref from there.

194
00:13:06,710 --> 00:13:08,410
So that's not too bad.

195
00:13:08,450 --> 00:13:15,240
Assuming I can type response correctly we're going to do see SS and then we want this thing with a classroom

196
00:13:15,260 --> 00:13:22,560
next instead of that in a tag with the attribute ref.

197
00:13:23,260 --> 00:13:26,850
And we just call that extract first.

198
00:13:27,380 --> 00:13:34,490
So calling that to just give us the value of that Tref which is some you know and then we just add this

199
00:13:34,490 --> 00:13:40,280
little bit of logic if there is a next because on the last page we get to there won't be one that will

200
00:13:40,280 --> 00:13:42,750
be the starting point if there is one.

201
00:13:42,920 --> 00:13:47,290
We do another yield it looks like this response does follow.

202
00:13:47,300 --> 00:13:50,900
And then we tell it where to follow which is this link.

203
00:13:50,910 --> 00:13:52,580
This you Arel next.

204
00:13:52,580 --> 00:13:55,040
And then we pass in self up parts.

205
00:13:55,070 --> 00:13:58,240
So inside this function parse we are actually calling parse again.

206
00:13:58,250 --> 00:14:04,040
This is something called recursion which we haven't talked much about but this is how spiders are set

207
00:14:04,040 --> 00:14:06,030
up using scrapin.

208
00:14:06,410 --> 00:14:13,960
So we select you are l next if we can find it if there is one then we yield response stuff follow.

209
00:14:14,000 --> 00:14:15,790
This is all just boilerplate.

210
00:14:15,890 --> 00:14:21,990
We pass on to you or else what to do next where to go and call the same parse.

211
00:14:21,990 --> 00:14:23,620
And so it's going to keep doing this.

212
00:14:23,660 --> 00:14:27,470
So it's going to go through everything on this page scrape all these.

213
00:14:27,470 --> 00:14:31,760
See if there's an X button if there is then it's going to start the whole process over on this page

214
00:14:32,350 --> 00:14:37,370
scrape all these check for next button and it will keep going until there isn't one left and then it's

215
00:14:37,370 --> 00:14:42,060
done and that is the magic of playing by these rules.

216
00:14:42,260 --> 00:14:47,610
If you make the sacrifice of flexibility and you just follow the syntax fill in the blank.

217
00:14:47,610 --> 00:14:52,210
Basically we hit save and run it again.

218
00:14:54,710 --> 00:14:58,880
And they did just append all of this data to the existing file.

219
00:14:59,130 --> 00:15:04,970
Take a bit longer because it's now doing this for 50 some 50 pages wasn't it.

220
00:15:05,810 --> 00:15:08,640
See it's on page 30 32 34.

221
00:15:08,660 --> 00:15:10,200
Keep talking through this.

222
00:15:10,670 --> 00:15:12,230
Oh man this is hard to fill.

223
00:15:12,320 --> 00:15:14,090
All right we finished.

224
00:15:14,210 --> 00:15:20,360
Now if you look at that file this was the beginning stuff and it overwrote it didn't overwrite it appended

225
00:15:20,360 --> 00:15:26,040
to the end and we get a whole bunch more check it out pretty cool.

226
00:15:26,250 --> 00:15:33,090
How easy that was and even cooler is that we can easily change this to be Jason with a single.

227
00:15:33,120 --> 00:15:35,750
I mean what four characters would change there.

228
00:15:36,330 --> 00:15:41,100
Now it's going to write all of our results as Jaison instead of CXXVI and we don't have to do anything

229
00:15:41,100 --> 00:15:41,370
else.

230
00:15:41,370 --> 00:15:45,780
It has a little adapter that it switches over to used Jaison instead.

231
00:15:45,780 --> 00:15:47,800
I personally am a big fan escapee.

232
00:15:48,180 --> 00:15:52,560
I didn't teach it and didn't focus on it because you have to understand how scraping works you have

233
00:15:52,560 --> 00:15:57,660
to understand the fact that you're making requests and parsing them back because it hides all of that

234
00:15:57,660 --> 00:15:58,650
from you.

235
00:15:58,650 --> 00:16:00,570
Now if I open up books today.

236
00:16:00,620 --> 00:16:03,410
Jason here we go.

237
00:16:03,430 --> 00:16:05,950
It's all stored on now.

238
00:16:06,240 --> 00:16:07,160
Is really cool.

239
00:16:07,180 --> 00:16:11,650
This is just the code for the pound the euro symbol of the pound.

240
00:16:11,650 --> 00:16:15,380
It wasn't looking anyway so it's just a little bit of work.

241
00:16:15,490 --> 00:16:18,220
We were able to write this crawler it's a spider.

242
00:16:18,340 --> 00:16:21,880
We have to play by the rules inherit from scraping that spider.

243
00:16:21,910 --> 00:16:26,290
We have to define start you or else we have to define a parse method and the first thing that happens

244
00:16:26,320 --> 00:16:27,410
is that it calls parse.

245
00:16:27,460 --> 00:16:30,330
On this first you IRL or however many you are Elser in here.

246
00:16:30,400 --> 00:16:35,520
And then for each response or grabbing the data we want as well as checking if there is a next page.

247
00:16:35,560 --> 00:16:40,150
And the third is recalling follow which will then called parts and the whole process starts over and

248
00:16:40,150 --> 00:16:42,020
we keep going until we hit a dead end.

249
00:16:42,040 --> 00:16:42,670
Cool.

250
00:16:42,670 --> 00:16:43,290
All right.

251
00:16:43,300 --> 00:16:44,150
That was a lot.

252
00:16:44,230 --> 00:16:46,540
So if this was confusing Don't worry.

253
00:16:46,540 --> 00:16:51,130
This is just a more advanced tool that you can use if you're really interested in scraping and you need

254
00:16:51,130 --> 00:16:53,680
to do it at a high volume or you need to crawl.

255
00:16:53,800 --> 00:16:56,220
Then I highly recommend that you look in discreet be.
