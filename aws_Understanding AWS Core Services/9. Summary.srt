1
00:00:01,040 --> 00:00:04,740
So we have looked at several different services over the course of this module

2
00:00:04,740 --> 00:00:08,280
that have to do with both file storage and data transfer.

3
00:00:08,280 --> 00:00:11,230
So, let's quickly review what we've covered before we dive

4
00:00:11,230 --> 00:00:13,470
in and take a look at our scenarios.

5
00:00:13,470 --> 00:00:14,690
So first of all,

6
00:00:14,690 --> 00:00:18,990
we were able to review the different storage services that are available on AWS.

7
00:00:18,990 --> 00:00:23,460
As a part of that, we examined Amazon S3 and its capabilities.

8
00:00:23,460 --> 00:00:28,210
We also implemented a static website on Amazon S3. We then explored the

9
00:00:28,210 --> 00:00:32,240
archive capabilities within Glacier and Glacier Deep Archive.

10
00:00:32,240 --> 00:00:36,900
We reviewed EC2 storage, leveraging both EBS and EFS.

11
00:00:36,900 --> 00:00:39,830
And then, we also examined the large‑scale data transfer

12
00:00:39,830 --> 00:00:42,470
services that are available on AWS.

13
00:00:42,470 --> 00:00:45,140
So now, let's review our scenarios.

14
00:00:45,140 --> 00:00:45,910
So first of all,

15
00:00:45,910 --> 00:00:48,860
we had Elaine, and she was trying to determine how she could

16
00:00:48,860 --> 00:00:52,360
reduce her S3 costs while maintaining durability.

17
00:00:52,360 --> 00:00:53,870
Well, what's the answer for her?

18
00:00:53,870 --> 00:00:54,590
Well in this case,

19
00:00:54,590 --> 00:01:00,270
it would be to use S3 lifecycle rules with S3 Standard‑Infrequent Access.

20
00:01:00,270 --> 00:01:01,840
So let's break that down.

21
00:01:01,840 --> 00:01:06,100
First of all, she could use S3 lifecycle rules because in this case,

22
00:01:06,100 --> 00:01:10,380
the assets are only popular within a week of when they are released, so she

23
00:01:10,380 --> 00:01:14,680
could define a policy that would change storage classes for the data after seven

24
00:01:14,680 --> 00:01:17,740
days of when they are placed inside of the S3 bucket.

25
00:01:17,740 --> 00:01:20,740
And then we could choose to leverage the S3 Standard‑Infrequent

26
00:01:20,740 --> 00:01:24,650
Access storage class because that data is not frequently accessed,

27
00:01:24,650 --> 00:01:28,360
but we would want to stick to this storage class and not one zone

28
00:01:28,360 --> 00:01:31,560
Infrequent Access because we do want to maintain the durability of

29
00:01:31,560 --> 00:01:35,240
the data. Now next, let's look at Esteban.

30
00:01:35,240 --> 00:01:38,400
And so what approach would you recommend for him to get 2 PB

31
00:01:38,400 --> 00:01:41,440
of user‑generated content into the cloud?

32
00:01:41,440 --> 00:01:44,470
Well in this case, it's going to be AWS Snowball.

33
00:01:44,470 --> 00:01:47,750
Now if we look here, we're looking at 2 PB of data.

34
00:01:47,750 --> 00:01:52,170
If we were looking at exabytes of data, AWS Snowball wouldn't work. We

35
00:01:52,170 --> 00:01:55,640
would need to look at AWS Snowmobile. But because we're still dealing

36
00:01:55,640 --> 00:01:58,800
in petabytes, this would work just fine.

37
00:01:58,800 --> 00:02:02,350
So next we have Emily, and Emily was looking for a shared file system

38
00:02:02,350 --> 00:02:05,820
that would work between eight different Linux EC2 instances and would

39
00:02:05,820 --> 00:02:08,240
need to support up to 1 PB of data.

40
00:02:08,240 --> 00:02:10,010
So what approach would you recommend?

41
00:02:10,010 --> 00:02:13,850
Well in this case, it would be Amazon Elastic File System, or EFS.

42
00:02:13,850 --> 00:02:16,070
But note that there are a few conditions that need to

43
00:02:16,070 --> 00:02:18,740
be true before we can choose EFS.

44
00:02:18,740 --> 00:02:21,250
First of all, it needs to be for Linux instances,

45
00:02:21,250 --> 00:02:25,650
and in this case, it is. Also, it needs to be in petabytes of data or less,

46
00:02:25,650 --> 00:02:27,870
and not in exabytes of data.

47
00:02:27,870 --> 00:02:34,000
So it seems like both of these criteria are met, and EFS would be a great candidate for Emily.

