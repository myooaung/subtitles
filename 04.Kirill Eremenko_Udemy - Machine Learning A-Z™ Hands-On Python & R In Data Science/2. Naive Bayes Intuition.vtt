WEBVTT
1

00:00:00.870  -->  00:00:03.000
Hello and welcome back to the course of machine learning.

2

00:00:03.180  -->  00:00:04.740
This is Kirill Eremenko.

3

00:00:04.770  -->  00:00:08.570
And in today's Turrell we're talking about the naÃ¯ve base classifier.

4

00:00:08.640  -->  00:00:13.740
This is a very interesting machine learning algorithm and today we're going to get to know it on a very

5

00:00:13.830  -->  00:00:20.200
intuitive level and in line with the super data science mission which is making the complex simple.

6

00:00:20.220  -->  00:00:26.670
We're going to break down this complex topic into simple steps and bite size pieces of information.

7

00:00:26.840  -->  00:00:29.000
I've got some very exciting slides prepared ahead.

8

00:00:29.100  -->  00:00:31.500
So let's dive straight into it.

9

00:00:31.500  -->  00:00:35.570
All right so here we've got the Bayes Theorem and this is something we talked about in the press tutorial

10

00:00:35.580  -->  00:00:41.400
so by now it should be quite comfortable with the concept how we're going to apply it to create a machine

11

00:00:41.400  -->  00:00:42.450
learning algorithm.

12

00:00:42.690  -->  00:00:45.520
Well let's have a look here we've got a data set.

13

00:00:45.510  -->  00:00:47.310
So it has two features.

14

00:00:47.310  -->  00:00:48.980
It has x1 and x2.

15

00:00:49.020  -->  00:00:53.520
And there are two categories Category 1 which is rated Category 2 which is green but instead of working

16

00:00:53.520  -->  00:00:58.480
with these abstract terms we're going to convert them into something that we can understand a bit better

17

00:00:58.480  -->  00:01:01.780
or something that's a bit easier to operate with or to talk about.

18

00:01:01.800  -->  00:01:07.530
So we're going to call the Y variable extra variable salary and the x 1 variable is going to be age

19

00:01:07.530  -->  00:01:07.560
.

20

00:01:07.560  -->  00:01:13.230
So basically we are presenting observations or people that are part of a data set in terms of their

21

00:01:13.290  -->  00:01:17.610
age and salary as you can see we have 30 people here on this chart.

22

00:01:17.910  -->  00:01:20.520
And the candidate is we're going to replace them with walks.

23

00:01:20.550  -->  00:01:23.800
Meaning that person walks to work and Green will be dry.

24

00:01:23.820  -->  00:01:25.710
That means that person drives to work.

25

00:01:26.010  -->  00:01:29.750
And so now we get our problem to the machine learning challenge that we're going to be solving.

26

00:01:29.930  -->  00:01:34.370
What happens if we add a new observation a new data point into the set.

27

00:01:34.470  -->  00:01:37.500
How do we classify this new data point.

28

00:01:37.500  -->  00:01:42.870
So as you can tell this is a supervised machine learning algorithm because we're classifying something

29

00:01:42.870  -->  00:01:45.560
based on previously known Klaas's.

30

00:01:45.570  -->  00:01:50.070
And so the question is is this person going to be classified as a person who walks to work or is this

31

00:01:50.070  -->  00:01:55.620
person going to be classified as a person who drives to work and then they leave base algorithm is going

32

00:01:55.620  -->  00:01:57.630
to help us solve this challenge.

33

00:01:57.630  -->  00:01:59.580
All right so how are we going to approach this.

34

00:01:59.580  -->  00:02:02.720
We need a plan of attack it is going to be quite a complex approach.

35

00:02:02.730  -->  00:02:06.960
But at the same time we're going to break it down into steps and they'll all make sense will be very

36

00:02:06.960  -->  00:02:08.280
easy to understand.

37

00:02:08.280  -->  00:02:12.880
So our plan of attack we're going to take the Bayes Theorem and we can apply it twice.

38

00:02:12.990  -->  00:02:19.740
First time we're going to apply it to find out what is the probability that this person walks given

39

00:02:19.740  -->  00:02:26.970
his features and X over here is the features or presents the features of that data point so let's go

40

00:02:26.970  -->  00:02:28.760
back to the visualization here.

41

00:02:28.770  -->  00:02:34.830
So here you can see that this is our new datapoint that person has a certain age so let's say the age

42

00:02:34.830  -->  00:02:37.650
of that person maybe is like 25 years old.

43

00:02:37.920  -->  00:02:41.870
And then they have a salary so let's say their salary is $3000 per year.

44

00:02:42.120  -->  00:02:48.240
So those are features of this observation right now we're only working with two variables just for simplicity's

45

00:02:48.240  -->  00:02:53.400
sake so we can visualize things age and salary but in reality there could be many many many more features

46

00:02:53.400  -->  00:02:59.400
that could be features on how many what what industry they work in or how many years of education they

47

00:02:59.400  -->  00:03:02.470
have or how long they've had a driver's license for.

48

00:03:02.550  -->  00:03:06.890
And I think they got off how far away they live from work so there could be lots of variables.

49

00:03:06.900  -->  00:03:11.250
But at the same time right down are you going to be dealing with two age and salary and regardless of

50

00:03:11.250  -->  00:03:15.790
how many variables you have they will be called in we're going to call them features.

51

00:03:15.810  -->  00:03:23.970
So given the features of x so given the age of 25 and the salary of $30000 and we'll talk in more detail

52

00:03:23.970  -->  00:03:27.220
about exactly what we mean by features just in the moment.

53

00:03:27.240  -->  00:03:33.480
And so therefore this part represents that person that we're trying to classify what is the likelihood

54

00:03:33.540  -->  00:03:36.000
of a person with those features.

55

00:03:36.000  -->  00:03:41.390
So we know that we are taking somebody for those features that we have in our new data point.

56

00:03:41.390  -->  00:03:44.450
What is the likelihood of them walking and then you've got the right side.

57

00:03:44.450  -->  00:03:48.180
So we're going to talk through each one of these as we calculate them.

58

00:03:48.180  -->  00:03:51.080
But for now let's just give them their names going from right to left.

59

00:03:51.080  -->  00:03:57.600
So this one on over here is called the prior probability and we're going to calculate that first because

60

00:03:57.600  -->  00:03:59.040
it's the easiest to calculate.

61

00:03:59.070  -->  00:04:05.790
Next one is the marginal likelihood and we're going to calculate that second the third one is a likelihood

62

00:04:05.880  -->  00:04:06.140
.

63

00:04:06.270  -->  00:04:07.560
That's just the names that they have.

64

00:04:07.590  -->  00:04:09.670
And we're going to Califate that third.

65

00:04:09.780  -->  00:04:13.740
And finally what we're looking for is called the post theory or probability we're going to calculate

66

00:04:13.740  -->  00:04:14.790
that force.

67

00:04:14.790  -->  00:04:17.330
All right so that's our plan of attack for step 1.

68

00:04:17.370  -->  00:04:21.090
This is all still step 1 to calculate the probability that somebody walks.

69

00:04:21.090  -->  00:04:25.090
Given those features X that we see in our new data point.

70

00:04:25.500  -->  00:04:29.430
Next we're going to have Step two where we're going to calculate the probability that somebody drives

71

00:04:29.520  -->  00:04:33.220
given those features X that we see in our new data point.

72

00:04:33.300  -->  00:04:37.770
And again here will have the probability which will calculate first then the marginal likelihood then

73

00:04:37.770  -->  00:04:40.440
the likelihood and then you'll get to pester probability.

74

00:04:40.530  -->  00:04:44.700
And finally we're going to compare the possibility that somebody walks given features X and versus the

75

00:04:44.700  -->  00:04:49.650
probability that somebody drives human features X and then from there we'll decide which Clauss to put

76

00:04:49.740  -->  00:04:51.750
that new data point in.

77

00:04:51.750  -->  00:04:57.000
So as you can see that the base class for is a probabilistic type of classify because we're first calculating

78

00:04:57.000  -->  00:05:00.820
the probabilities and then based on probabilities we're assigning it close.

79

00:05:00.840  -->  00:05:01.110
All right.

80

00:05:01.110  -->  00:05:08.020
So are you ready to form these steps it's going to be lots of fun we're going to take it nice and easy

81

00:05:08.020  -->  00:05:10.580
nice and slowly so that we understand everything.

82

00:05:10.600  -->  00:05:14.060
And after this event to be very comfortable with the Navy base catfight.

83

00:05:14.230  -->  00:05:14.830
Step one.

84

00:05:14.830  -->  00:05:16.610
All right so here we have our installation.

85

00:05:16.660  -->  00:05:20.070
Let's move it to the left a little bit so we can make some space.

86

00:05:20.080  -->  00:05:24.030
Now we're going to calculate the first probability in our Bayes Theorem.

87

00:05:24.040  -->  00:05:28.900
We're going to calculate the probability that somebody walks right just the overall probability.

88

00:05:28.900  -->  00:05:29.580
What does that mean.

89

00:05:29.590  -->  00:05:35.250
That is the probability that somebody to fight knowing anything about them so we're just saying we're

90

00:05:35.250  -->  00:05:38.630
going to add a new observation to our data set into here.

91

00:05:38.770  -->  00:05:43.330
But we don't know their age and we don't know their salary is going to put it somewhere into our data

92

00:05:43.330  -->  00:05:44.050
set.

93

00:05:44.050  -->  00:05:48.440
What is the probability that this person that we're adding to our database walks to work bullets.

94

00:05:48.460  -->  00:05:51.940
Very easy answer for from here we don't have much choice.

95

00:05:51.940  -->  00:05:55.710
The only thing we can do is calculate the number of read observations.

96

00:05:55.720  -->  00:06:00.820
Number of people that actually walk and divide by the overall number so probably that person walks to

97

00:06:00.820  -->  00:06:06.040
work with farden any other knowledge is the number of walkers and number of people or walk which is

98

00:06:06.430  -->  00:06:10.660
these are adults divided by the total number absorption the green dots are the gray dot isn't participating

99

00:06:10.660  -->  00:06:12.340
in these calculations.

100

00:06:12.340  -->  00:06:17.940
So here we have probably if somebody walks is 10 10 red dots divide by 50 dots overall.

101

00:06:17.950  -->  00:06:18.250
All right.

102

00:06:18.250  -->  00:06:19.060
So that was easy.

103

00:06:19.060  -->  00:06:23.570
We've calculated the prior probability next with calculating the marginal likelihood.

104

00:06:23.680  -->  00:06:26.130
And this is where things get interesting.

105

00:06:26.140  -->  00:06:28.850
So how do we calculate the margin of likelihood.

106

00:06:29.350  -->  00:06:30.160
Let's have a look.

107

00:06:30.400  -->  00:06:32.180
Here's our data set again.

108

00:06:32.320  -->  00:06:37.990
And the first thing you can do is we're going to select a radius and we're going to draw a circle around

109

00:06:38.080  -->  00:06:40.410
our observation like that.

110

00:06:40.420  -->  00:06:44.960
Now this radius you need to select on your own and you need to decide for you.

111

00:06:44.980  -->  00:06:48.490
Algor and this is going to be like an input parameter or an algorithm you could select less because

112

00:06:48.550  -->  00:06:50.670
like that more it's up to you.

113

00:06:50.710  -->  00:06:52.020
Now what does this radius do.

114

00:06:52.060  -->  00:06:57.050
Well what we're going to do is we're going to first of all let's just to make things easier.

115

00:06:57.050  -->  00:07:01.980
We're going to remove our DOT for now just so that it's not confusing us.

116

00:07:02.230  -->  00:07:07.930
And then we're going to look at all the points that are inside this series and what we're saying here

117

00:07:07.930  -->  00:07:15.640
is that all of the points inside the circle are we going to deem them to be similar in terms of features

118

00:07:15.640  -->  00:07:17.150
to the point that we had.

119

00:07:17.290  -->  00:07:18.160
The point that we had.

120

00:07:18.160  -->  00:07:22.580
Remember it had an age of for example 25 and a salary of $30000 per year.

121

00:07:22.600  -->  00:07:28.720
So now we're going to draw a radius around it and let's say anybody between the age of 20 and 30 and

122

00:07:28.720  -->  00:07:33.380
in the salaries of $25000 to $35000.

123

00:07:33.640  -->  00:07:39.090
Anybody that falls in that circle again is it's not a square it's a square is a circle.

124

00:07:39.340  -->  00:07:47.410
Anybody who falls somewhere is somewhere in that vicinity is going to be deemed similar to the new data

125

00:07:47.440  -->  00:07:49.190
point that we're adding to our data set.

126

00:07:49.330  -->  00:07:54.550
So as you can imagine this radius is actually going to have a big say in the way your algorithm works

127

00:07:54.570  -->  00:07:54.740
.

128

00:07:54.880  -->  00:07:58.570
Well let's say we have this radius and this is how it all played out.

129

00:07:58.570  -->  00:08:01.110
We have three red dots one green dot in them.

130

00:08:01.360  -->  00:08:01.650
All right.

131

00:08:01.660  -->  00:08:03.100
So now what do we do.

132

00:08:03.100  -->  00:08:07.270
How do we calculate the probability of X and what is the probability of X.

133

00:08:07.390  -->  00:08:15.190
Well the probability of X is the probability of a new point that we add to our data set being similar

134

00:08:15.190  -->  00:08:19.640
in features to the point that we actually are adding to it.

135

00:08:19.660  -->  00:08:24.520
So basically it's a probability of that new point that we're adding or like any random point that we

136

00:08:24.520  -->  00:08:31.930
add is the probability that any random point to fall into this circle and P of X is calculated as the

137

00:08:31.930  -->  00:08:36.190
number of similar observations so the number of observations that already we can see in the circle so

138

00:08:36.250  -->  00:08:40.500
1 2 3 4 divided by the total number of durations which is 30.

139

00:08:40.660  -->  00:08:42.680
So p of X is foredoomed Bethy.

140

00:08:43.030  -->  00:08:49.120
Once again just to reiterate P of x it tells us what is the likelihood of any new random variable that

141

00:08:49.120  -->  00:08:53.010
we add to this data set fulling inside the circle.

142

00:08:53.140  -->  00:08:56.070
And it is 430 because we only have four.

143

00:08:56.290  -->  00:09:00.960
Based on prior knowledge we can solve this for here and this 2d it also is four with 30.

144

00:09:01.120  -->  00:09:03.660
All right so that wasn't hard at all as well.

145

00:09:03.670  -->  00:09:05.380
We called the marginal likelihood.

146

00:09:05.380  -->  00:09:07.790
So so far we got this one and we got this one.

147

00:09:07.860  -->  00:09:12.120
Next we're moving onto the likelihood and this is probably the most complex one.

148

00:09:12.130  -->  00:09:19.930
What is the likelihood that somebody who walks exhibits features X will actually after we've spoken

149

00:09:19.930  -->  00:09:24.060
about the marginal likelihood calculating the likelihood won't be as complex.

150

00:09:24.070  -->  00:09:25.410
So let's have a look.

151

00:09:25.420  -->  00:09:27.100
So there is our chart.

152

00:09:27.250  -->  00:09:33.570
And now what we're going to do is we're going to draw the same circle again and once again we're going

153

00:09:33.570  -->  00:09:37.120
to remove the gray point for now and we're going to color a circle.

154

00:09:37.330  -->  00:09:43.770
And so anything that falls inside the circle is deemed to be similar to the point that we're adding

155

00:09:43.790  -->  00:09:44.030
.

156

00:09:44.260  -->  00:09:51.070
So the question is what is the probability that a randomly selected data point from our data set will

157

00:09:51.070  -->  00:09:54.070
be similar to the data point that we're adding.

158

00:09:54.070  -->  00:10:00.670
So basically what is the likelihood that a randomly selected data point will be from this circle given

159

00:10:00.820  -->  00:10:07.390
this vertical pipe means given that that person walks that we know that that person walks to work the

160

00:10:07.390  -->  00:10:11.990
other way to think about this is we're only working with people who walk to work.

161

00:10:11.990  -->  00:10:15.690
So we're only working with the red dots which represent people who walk to work.

162

00:10:15.690  -->  00:10:17.390
So let's forget about the green dots.

163

00:10:17.440  -->  00:10:21.230
They're like they're now they're faint and we're not even talking about them at all.

164

00:10:21.250  -->  00:10:22.830
We're only talking about the red dots.

165

00:10:22.840  -->  00:10:29.590
So the question is given that we're only working with the red dots What is the likelihood that a randomly

166

00:10:29.590  -->  00:10:38.020
selected datapoint from our daughter said from the red dots is somebody who exhibits features similar

167

00:10:38.260  -->  00:10:41.450
to the point that we are adding to our daughters.

168

00:10:41.450  -->  00:10:48.160
And so basically what is the likelihood that a randomly selected red dot falls into this gray area into

169

00:10:48.160  -->  00:10:49.250
the circle.

170

00:10:49.270  -->  00:10:50.840
That's what the question we're asking.

171

00:10:50.860  -->  00:10:54.390
And there's also very simple now that we know how all this works.

172

00:10:54.490  -->  00:10:59.440
It's basically the number of civil or observations among those who work so the number of red dots that

173

00:10:59.440  -->  00:11:05.110
actually fall inside this red circle in this great circle that's three divided by the total number of

174

00:11:05.170  -->  00:11:10.600
walkers so people and total number of people who walk to work and that is three over 10.

175

00:11:10.600  -->  00:11:11.060
There we go.

176

00:11:11.070  -->  00:11:17.200
So that's our P of the likelihood of somebody exhibiting the features similar to that broader point

177

00:11:17.200  -->  00:11:21.760
that we're about to add given that we're only selecting among the red dots.

178

00:11:21.850  -->  00:11:24.280
So that's three over 10 and that was our likelihood.

179

00:11:24.280  -->  00:11:28.350
So now if we plug all that in so there we go that likelihood is done.

180

00:11:28.360  -->  00:11:29.550
So if you plug all of that in.

181

00:11:29.560  -->  00:11:31.350
We'll get our posterior probability.

182

00:11:31.390  -->  00:11:36.320
So three over 10 times 10 or 30 and divided by four or three.

183

00:11:36.400  -->  00:11:39.730
So if we calculate that Ill give us zero point seventy five.

184

00:11:39.730  -->  00:11:46.030
Seventy five percent is the probability that somebody that we put into the place where we're putting

185

00:11:46.060  -->  00:11:50.680
x is should be classified as a person who walks to work.

186

00:11:50.680  -->  00:11:57.020
That was step one was pretty intense ride pretty exciting to calculate this value.

187

00:11:57.030  -->  00:11:59.840
Now the next step is step two step one done.

188

00:11:59.890  -->  00:12:06.460
Next step a step to do the same thing for the likelihood that somebody with features X will be classified

189

00:12:06.610  -->  00:12:09.820
or should be classified as a person who drives to work.

190

00:12:10.180  -->  00:12:17.710
And here to you a challenge I'm going to challenge you to post this video or rewind back to find out

191

00:12:17.710  -->  00:12:17.830
.

192

00:12:17.890  -->  00:12:23.770
To have the image in front of you and do these calculations yourself to actually go through the same

193

00:12:23.770  -->  00:12:25.870
steps and perform those calculations.

194

00:12:25.870  -->  00:12:32.920
If you'd like to see and compare to my calculations then I'm going to put in another video after this

195

00:12:32.920  -->  00:12:37.420
one so another tutorial after this one in the course you can just go to the next tutorial and compare

196

00:12:37.440  -->  00:12:37.960
.

197

00:12:38.090  -->  00:12:40.200
Otherwise I'm going to show you the result now.

198

00:12:40.420  -->  00:12:47.630
So the result is one of a 24 likelihood or this are from the right probability is 20 or 30 margin electrical

199

00:12:47.770  -->  00:12:49.490
remains unchanged for with.

200

00:12:49.540  -->  00:12:51.980
Likelihood changes to one over 20.

201

00:12:52.000  -->  00:12:58.330
So the probability of somebody who exhibits features X being a person who drives to work is 25 percent

202

00:12:58.330  -->  00:12:58.810
.

203

00:12:58.870  -->  00:13:00.290
So that was step two.

204

00:13:00.400  -->  00:13:01.630
Now we're going to do step three.

205

00:13:01.660  -->  00:13:08.470
We're going to compare the probability of somebody with features X Brazilia of them being a person who

206

00:13:08.470  -->  00:13:13.010
walks to work versus the probability of somebody features X being present drives to it.

207

00:13:13.240  -->  00:13:19.420
So it's 75 percent versus 25 percent and therefore the first is great and the second and therefore it

208

00:13:19.420  -->  00:13:26.620
is more likely that that person with features X is going to be a person who walks to work than the person

209

00:13:26.620  -->  00:13:27.280
who drives to work.

210

00:13:27.280  -->  00:13:31.500
So still a 25 percent chance that that is a person who drives to work but.

211

00:13:31.540  -->  00:13:37.540
Percent chance that it is a person who walks or is gray 75 percent and therefore we're going to classify

212

00:13:37.810  -->  00:13:41.420
this point as a person who walks to work.

213

00:13:41.500  -->  00:13:42.100
There we go.

214

00:13:42.100  -->  00:13:47.760
That is how they leave base algorithm in machine learning works.

215

00:13:47.770  -->  00:13:49.980
I hope you find this Tauriel useful.

216

00:13:49.990  -->  00:13:56.860
I was pretty excited and pretty proud of these Sly's and hopefully this is a step by step and a simple

217

00:13:56.860  -->  00:13:59.230
explanation of a complex concept.

218

00:13:59.290  -->  00:14:00.970
And I look forward to seeing you next time.

219

00:14:00.970  -->  00:14:02.720
Until then enjoy your machine learning
