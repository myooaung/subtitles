WEBVTT
1
00:00:00.120 --> 00:00:01.080
Hello, my friends.

2
00:00:01.200 --> 00:00:03.280
I'm super excited to start now.

3
00:00:03.550 --> 00:00:10.080
A part of this chorus that we've been excitedly waiting for, I'm talking, of course, about deep learning.

4
00:00:10.390 --> 00:00:13.250
You just got the amazing intuition lectures by Kiril.

5
00:00:13.260 --> 00:00:19.260
And so now you understand that what deep learning consists of is building an artificial brain.

6
00:00:19.500 --> 00:00:20.670
So there you go, my friends.

7
00:00:20.760 --> 00:00:26.640
Now we're about to build an artificial brain using the brand new tensor Flow 2.0.

8
00:00:27.030 --> 00:00:28.410
This will be super exciting.

9
00:00:28.530 --> 00:00:34.230
We will really build a deep neural network with neurons and fully connected layers connecting these

10
00:00:34.230 --> 00:00:34.740
neurons.

11
00:00:35.070 --> 00:00:38.310
And we will apply all this to a business problem as usual.

12
00:00:38.340 --> 00:00:42.930
And now that we are, you know, quite advanced and of course, the dataset we'll be working on looks

13
00:00:42.930 --> 00:00:47.430
more like a real world dataset with many observations and many features.

14
00:00:47.430 --> 00:00:52.830
And you'll see that we will actually have to not only use our data processing template, but also use

15
00:00:52.830 --> 00:00:55.530
some of the tools of our data pricing toolkit.

16
00:00:56.100 --> 00:00:57.570
So are you ready?

17
00:00:57.750 --> 00:00:59.370
Are you ready to go next level?

18
00:00:59.460 --> 00:00:59.840
Indeed.

19
00:00:59.850 --> 00:01:02.760
Now we are about to do some more advanced machine learning.

20
00:01:03.030 --> 00:01:07.860
And so I'm super excited that you're even pushing further your expertise in machine learning.

21
00:01:08.370 --> 00:01:08.640
All right.

22
00:01:08.640 --> 00:01:09.330
So let's do this.

23
00:01:09.360 --> 00:01:12.660
But first, let's just make sure everyone here is on the same page.

24
00:01:12.810 --> 00:01:15.420
So this is the folder of the whole machinery.

25
00:01:15.450 --> 00:01:16.590
It is with all the codes.

26
00:01:16.590 --> 00:01:21.450
And they said, I gave you the link to this for the right before this is oil, so make sure you connect

27
00:01:21.450 --> 00:01:21.780
to it.

28
00:01:22.050 --> 00:01:23.040
And now there we go.

29
00:01:23.170 --> 00:01:30.750
Let's enter part eight, Deep Learning with first the classic artificial neural network, meaning the

30
00:01:30.750 --> 00:01:36.930
fully connected neural network with only fully connected layers, you know, with no convolutional layers

31
00:01:36.960 --> 00:01:38.550
or other types of layers.

32
00:01:38.670 --> 00:01:44.190
Here we will just have an input vector containing different features and we will predict an outcome

33
00:01:44.190 --> 00:01:48.510
which will be a binary variable, because you have to know that actually artificial neural networks

34
00:01:48.510 --> 00:01:51.750
can be both used for regression or classification.

35
00:01:52.080 --> 00:01:54.600
And here we're going to do it for classification.

36
00:01:54.870 --> 00:02:00.300
However, note that we have a free course on artificial Newill network in which this time we build an

37
00:02:00.330 --> 00:02:02.640
artificial Nool network for regression.

38
00:02:02.700 --> 00:02:03.810
So make sure to check it out.

39
00:02:03.870 --> 00:02:07.260
I will think of including the links somewhere so that you can get it as well.

40
00:02:07.570 --> 00:02:08.150
It's really good.

41
00:02:08.150 --> 00:02:13.320
You will have both cases, you know, the classification case with discourse and the regression case

42
00:02:13.590 --> 00:02:14.580
with the other free course.

43
00:02:14.820 --> 00:02:15.150
All right.

44
00:02:15.600 --> 00:02:18.630
So now, as usual, we're gonna start with Python.

45
00:02:18.900 --> 00:02:19.710
And there we go.

46
00:02:19.800 --> 00:02:25.740
This folder contains the implementation first artificial new network that IPY NDB, which you can either

47
00:02:25.770 --> 00:02:28.710
open with Jubran Notebook or Google Collaboratory.

48
00:02:29.100 --> 00:02:33.510
And we have, of course, our dataset, which I'm going to explain right now.

49
00:02:34.200 --> 00:02:34.510
All right.

50
00:02:34.530 --> 00:02:39.500
So as you can see, as I told you, it looks indeed more like a real world data set.

51
00:02:39.840 --> 00:02:40.140
Right.

52
00:02:40.160 --> 00:02:45.060
For the first time, our dataset takes the full screen here because indeed, this time we have many

53
00:02:45.060 --> 00:02:50.550
features, you know, starting from here up to this one and the dependent variable right here.

54
00:02:51.150 --> 00:02:51.450
All right.

55
00:02:51.480 --> 00:02:53.310
So let me explain what this is about.

56
00:02:53.700 --> 00:03:00.570
This is the data set of a bank which collected some informations about their customers.

57
00:03:00.930 --> 00:03:02.850
These informations are, well, row.

58
00:03:02.850 --> 00:03:06.030
No, they're just non relevant feature will get rid of it.

59
00:03:06.360 --> 00:03:10.620
Then the customer I.D., that's just the ID key of each customer.

60
00:03:10.940 --> 00:03:16.080
The surname, the credit score, the geography, meaning the country the customer lives in.

61
00:03:16.440 --> 00:03:22.860
The gender, the age, the tenure, meaning the number of years they have been in the bank, the balance,

62
00:03:22.890 --> 00:03:27.840
meaning the amount of money they have on their account, the number of products they use from the bank.

63
00:03:27.870 --> 00:03:34.440
You know, like a credit card or a checkbook or a MasterCard or even a loan or home loan, you know,

64
00:03:34.470 --> 00:03:35.930
any banking product.

65
00:03:36.120 --> 00:03:36.360
OK.

66
00:03:36.420 --> 00:03:40.740
So that's the number of perks each customer has then has credit card.

67
00:03:40.770 --> 00:03:41.250
Yes, I know.

68
00:03:41.250 --> 00:03:43.350
Meaning that this very was equal to one.

69
00:03:43.500 --> 00:03:50.400
If the customer has a credit card and zero otherwise is active member meaning, is the customer active?

70
00:03:50.520 --> 00:03:56.940
Is he or she using the bank, you know, connecting to its account or using its credit card or any other

71
00:03:56.940 --> 00:03:57.290
card?

72
00:03:57.300 --> 00:04:01.890
You know, let's say they have some measurement system to measure if a customer is active or not.

73
00:04:01.980 --> 00:04:07.130
And one means, of course, that the customer is active and zero means that it is an active okay.

74
00:04:07.590 --> 00:04:09.060
Then estimated salary.

75
00:04:09.420 --> 00:04:13.380
Well, you know, the salary of the customer estimated by the bank and that's it.

76
00:04:13.530 --> 00:04:14.520
That's the last feature.

77
00:04:14.910 --> 00:04:19.290
And then the last column here exhibit is the dependent variable.

78
00:04:19.650 --> 00:04:21.660
And it tells if yes or no.

79
00:04:22.020 --> 00:04:26.160
Well, the customer stayed in the bank or left the bank.

80
00:04:26.490 --> 00:04:30.480
One means that it left the bank as an exited equals.

81
00:04:30.510 --> 00:04:30.900
Yes.

82
00:04:31.170 --> 00:04:34.710
And zero means that the customer stayed in the bank as an exit.

83
00:04:34.710 --> 00:04:35.370
It equals no.

84
00:04:35.880 --> 00:04:42.240
So what happened in reality is that this bank actually observed their customers for a certain period

85
00:04:42.240 --> 00:04:43.940
of time, let's say, six months.

86
00:04:44.370 --> 00:04:48.600
They observed it during the six months they left the bank or stayed in the bank.

87
00:04:48.840 --> 00:04:52.590
And they gathered these outcomes in this last dependent variable.

88
00:04:52.890 --> 00:04:55.440
And at the same time, you know, they got all these features.

89
00:04:55.830 --> 00:04:57.360
Well, to guess what.

90
00:04:57.750 --> 00:04:59.770
Understand the correlations with.

91
00:05:00.210 --> 00:05:07.200
These features and the fact whether or not the customer stays in the bank or leave the bank, and that

92
00:05:07.200 --> 00:05:07.950
makes sense, right?

93
00:05:07.980 --> 00:05:10.980
Because a bank wants to have the maximum customers.

94
00:05:11.080 --> 00:05:11.290
Right.

95
00:05:11.420 --> 00:05:12.330
That's how they make money.

96
00:05:12.330 --> 00:05:16.890
The more customers they have, the more they have money in the bank and the more they make money from

97
00:05:16.890 --> 00:05:19.590
the diverse products they offer to their customers.

98
00:05:19.830 --> 00:05:23.550
So, of course, their interest is to keep the maximum customers.

99
00:05:23.850 --> 00:05:31.050
And therefore, they made this data set to understand the reasons somehow why customers leave the bank.

100
00:05:31.410 --> 00:05:37.650
And mostly once they managed to build a predictive model that can predict if any new customer leaves

101
00:05:37.650 --> 00:05:38.100
the bank.

102
00:05:38.220 --> 00:05:40.810
You know, a model that was trained, of course, on this dataset.

103
00:05:41.280 --> 00:05:44.850
Well, they will deploy this model on new customers.

104
00:05:45.000 --> 00:05:50.370
And for all the customers where the model predicts that the customer leaves the bank, well, they will

105
00:05:50.370 --> 00:05:56.190
be prepared and they might do some special offer to the customers so that it will stay in the bank.

106
00:05:56.280 --> 00:05:56.730
You see.

107
00:05:57.090 --> 00:06:02.640
So all this is to prevent the maximum customers from leaving the bank.

108
00:06:02.730 --> 00:06:04.630
And why is this girl churn modelling?

109
00:06:04.800 --> 00:06:10.920
Because customer churn means exactly the situation where some customers exit, you know, become no

110
00:06:10.920 --> 00:06:11.850
longer a customer.

111
00:06:12.420 --> 00:06:12.840
All right.

112
00:06:13.020 --> 00:06:19.320
And of course, the bank has asked you the most down to data scientist to make this predictive model,

113
00:06:19.380 --> 00:06:25.290
to first train it on this dataset, to understand the correlations of all these features and to depend

114
00:06:25.300 --> 00:06:29.310
InVivo and then to deploy this model on future customers.

115
00:06:29.640 --> 00:06:34.410
And you will see that in the implementation we will actually deploy the future machinery model will

116
00:06:34.410 --> 00:06:38.000
get on a different customer, you know, not part of this data set.

117
00:06:38.280 --> 00:06:43.330
So asked to predict if this new customer will stay in the bank or leave the bank.

118
00:06:43.380 --> 00:06:49.530
And even better than this, we will actually predict the probability that this customer leaves the bank.

119
00:06:49.950 --> 00:06:51.660
So we have a lot to do ahead of us.

120
00:06:51.690 --> 00:06:56.340
But I'm super excited because deep learning is a fascinating branch of machine learning.

121
00:06:56.610 --> 00:07:03.540
So let's start right away and let's open our implementation artificial new network that IP won't be

122
00:07:03.930 --> 00:07:09.770
which you can feel free to open with either Google collaboratory, as I'm about to do, or Dupere in

123
00:07:09.770 --> 00:07:10.980
notebook as you want.

124
00:07:11.250 --> 00:07:15.600
Just make sure you feel comfortable coding on your favorite iji.

125
00:07:16.470 --> 00:07:16.830
All right.

126
00:07:16.860 --> 00:07:19.230
So now the implementation is opening.

127
00:07:19.560 --> 00:07:20.140
Laying it out.

128
00:07:20.160 --> 00:07:20.730
The notebook.

129
00:07:20.820 --> 00:07:21.420
Perfect.

130
00:07:21.780 --> 00:07:25.170
That's the implementation once again in read only mode right now.

131
00:07:25.170 --> 00:07:29.640
We will click foul here to create a copy of this implementation.

132
00:07:30.000 --> 00:07:37.380
And as usual, we will reimplement all this from scratch so that we can really learn by doing.

133
00:07:37.980 --> 00:07:38.460
All right.

134
00:07:38.490 --> 00:07:39.340
So that's the copy.

135
00:07:39.430 --> 00:07:42.030
Now, let's remove all the cells.

136
00:07:42.420 --> 00:07:42.840
Right.

137
00:07:43.110 --> 00:07:48.390
This one, not the text cells, of course, because we want to keep the well highlighted structure of

138
00:07:48.390 --> 00:07:49.500
this implementation.

139
00:07:49.920 --> 00:07:52.440
But let's definitely remove the code cells.

140
00:07:52.650 --> 00:07:53.550
So there are many of them.

141
00:07:53.560 --> 00:07:56.040
So you will actually have a bit of time doing it.

142
00:07:56.250 --> 00:07:57.720
But, you know, it's not that long.

143
00:07:57.740 --> 00:07:58.290
It's fine.

144
00:07:58.740 --> 00:08:04.110
There is a long data repricing phase and then a long phase of building the neural network.

145
00:08:04.470 --> 00:08:07.650
And I really gave the details of the implementation.

146
00:08:07.850 --> 00:08:10.110
You will actually see many steps.

147
00:08:10.350 --> 00:08:10.890
All right.

148
00:08:11.190 --> 00:08:17.220
And there are so many steps that I actually added a level of structure, because as you can see, the

149
00:08:17.280 --> 00:08:19.620
full implementation is divided into three parts.

150
00:08:20.040 --> 00:08:20.430
Right.

151
00:08:20.520 --> 00:08:22.530
There is a homework at the end, the solution.

152
00:08:22.620 --> 00:08:24.690
So let me not show it to you.

153
00:08:25.080 --> 00:08:26.910
And that's almost the end.

154
00:08:27.630 --> 00:08:29.490
Confusion, matrix and perfect.

155
00:08:29.700 --> 00:08:30.060
All right.

156
00:08:30.920 --> 00:08:31.300
Okay.

157
00:08:31.340 --> 00:08:33.940
So, as you saw, this is a long implementation.

158
00:08:33.960 --> 00:08:37.680
This will be a long part, but it is absolutely worth it.

159
00:08:37.830 --> 00:08:41.520
Deep learning is one of the most powerful branches of machinery.

160
00:08:42.240 --> 00:08:43.140
Okay, so let's see.

161
00:08:43.530 --> 00:08:46.440
We start first by empowering the libraries as usual.

162
00:08:46.710 --> 00:08:49.410
And then we enter part one data preprocessing.

163
00:08:49.480 --> 00:08:52.070
So that's the first part of the whole implementation.

164
00:08:52.320 --> 00:08:54.570
Structured in four parts, which are the following.

165
00:08:54.690 --> 00:08:55.260
Part one.

166
00:08:55.260 --> 00:08:56.250
Data processing.

167
00:08:56.520 --> 00:08:57.070
Part two.

168
00:08:57.270 --> 00:08:59.280
Building the N.N. Part three.

169
00:08:59.290 --> 00:09:03.990
Training the N.N. and part for making the predictions and evaluating the moral.

170
00:09:04.410 --> 00:09:05.280
And then each part.

171
00:09:05.330 --> 00:09:07.520
Well, we have the different steps in parts.

172
00:09:07.530 --> 00:09:10.530
One day you'll be pressing Will first important data set, of course.

173
00:09:10.770 --> 00:09:12.900
Then we'll have some data pricing to do.

174
00:09:12.990 --> 00:09:17.250
You know, not only the classic steps of the template, but also some extra tools.

175
00:09:17.280 --> 00:09:18.450
And we will see that together.

176
00:09:18.930 --> 00:09:23.850
Then in part two, we will first initialized and then we'll add an input layer.

177
00:09:23.910 --> 00:09:26.460
And the first hidden layer to our artificial brain.

178
00:09:26.670 --> 00:09:28.500
Then we'll add the second hidden layer.

179
00:09:28.650 --> 00:09:29.790
Then the output layer.

180
00:09:30.220 --> 00:09:32.370
Then in the next part three training.

181
00:09:32.510 --> 00:09:37.890
And then we will first start by compiling the AON into, you know, an optimizer and a loss function.

182
00:09:38.250 --> 00:09:40.680
Then we will train the Eynon on the training set.

183
00:09:41.010 --> 00:09:42.510
And finally, in part four.

184
00:09:42.690 --> 00:09:48.120
Well, we will deploy our moral in production to predict the result of a single observation, meaning

185
00:09:48.330 --> 00:09:51.960
to predict if a new customer will stay or leave the bank.

186
00:09:52.410 --> 00:09:58.670
Then we will predict the test results to, you know, get that widespread vector to eventually make

187
00:09:58.680 --> 00:09:59.150
the confusion.

188
00:09:59.460 --> 00:10:01.830
Tricks with the final accuracy.

189
00:10:02.700 --> 00:10:03.150
All right.

190
00:10:03.240 --> 00:10:06.390
So, once again, as you can see, this implementation is very long.

191
00:10:06.420 --> 00:10:12.330
So make sure to have full energy and full motivation for this, because we have a long but yet exciting

192
00:10:12.420 --> 00:10:13.730
journey in front of us.

193
00:10:14.040 --> 00:10:15.220
So as soon as you're ready.

194
00:10:15.330 --> 00:10:18.900
Well, meet me in the next tutorial to smash this implementation.

195
00:10:19.170 --> 00:10:21.000
And until then, enjoy machine learning.
