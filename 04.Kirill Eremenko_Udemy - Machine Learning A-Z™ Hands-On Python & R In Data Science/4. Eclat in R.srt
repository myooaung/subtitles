1

00:00:00,240  -->  00:00:02,330
Hello and welcome to this art tutorial.

2

00:00:02,520  -->  00:00:07,740
So in the previous section we implemented the a priori model to find some relevant association rules

3

00:00:07,890  -->  00:00:13,110
that could help us find a more strategic placement of some products in a grocery store in the south

4

00:00:13,110  -->  00:00:13,860
of France.

5

00:00:14,040  -->  00:00:18,780
And therefore thanks to these association rules the manager of this store is likely to optimize the

6

00:00:18,780  -->  00:00:24,540
sales and therefore increase the revenues and accordingly we manage to create some added values in this

7

00:00:24,540  -->  00:00:27,710
business thanks to association rule learning.

8

00:00:27,780  -->  00:00:34,770
And today we are going to implement a new model of associational learning which is called the Ekla model

9

00:00:34,790  -->  00:00:35,070
.

10

00:00:35,310  -->  00:00:39,710
So the first thing I want to say is to prevent any disappointment you might have.

11

00:00:39,900  -->  00:00:45,450
The A-Class model is actually very simple compared to what we did previously because the example is

12

00:00:45,450  -->  00:00:50,520
basically the apriori model simplified because previously we had two parameters.

13

00:00:50,520  -->  00:00:53,670
We had the support and the confidence and we also had the left.

14

00:00:53,730  -->  00:00:57,830
By the way when we ranked our rules by their decreasing list.

15

00:00:58,080  -->  00:01:03,720
But here in the cloud we will only have one parameter which will be the support parameter and therefore

16

00:01:03,720  -->  00:01:09,270
when we obtain our rules that will not be the rules that we obtained earlier or like people who bought

17

00:01:09,270  -->  00:01:14,910
this also bought that but we will just get some different sets of products frequently but together we

18

00:01:14,910  -->  00:01:19,720
will get the different sets of products there are the most frequently pre-Chase together.

19

00:01:19,740  -->  00:01:20,960
So that's what we'll get.

20

00:01:20,970  -->  00:01:22,360
We have to expect this.

21

00:01:22,500  -->  00:01:28,980
But you know the can be very useful if you don't have much time if you want to get some simple results

22

00:01:28,980  -->  00:01:29,110
.

23

00:01:29,190  -->  00:01:33,690
If you don't want to play with too many parameters like to support the confidence here we don't even

24

00:01:33,690  -->  00:01:35,600
have to compute the value for the support.

25

00:01:35,610  -->  00:01:40,350
And moreover we don't even have to choose a value for the confidence because there is no confidence

26

00:01:40,350  -->  00:01:41,150
parameter.

27

00:01:41,340  -->  00:01:46,090
So you'll see this is the way of using Association rule learning in the simplest way.

28

00:01:46,230  -->  00:01:47,430
So let's do it right now.

29

00:01:47,460  -->  00:01:49,990
You'll see what I mean at the end of this tutorial.

30

00:01:50,100  -->  00:01:52,120
So let's start with the very first step.

31

00:01:52,290  -->  00:01:57,990
Setting the right for there is working directories will go to our machinery the fuller part 5 Association

32

00:01:57,990  -->  00:01:58,870
rule learning.

33

00:01:58,980  -->  00:02:01,040
And right now we are in a club.

34

00:02:01,200  -->  00:02:06,260
So here it is we have the market basket optimization will work on the same business problem.

35

00:02:06,420  -->  00:02:10,760
And so we can click on this more button here and now set us working directory.

36

00:02:11,010  -->  00:02:17,400
And you know since I told you the example is a simplified version of the premium or or what we'll do

37

00:02:17,400  -->  00:02:24,060
now is take our a priori model and you'll see that it will almost be the same and we will just need

38

00:02:24,060  -->  00:02:25,280
to change one thing.

39

00:02:25,440  -->  00:02:30,070
So I'm going to select everything from here to here.

40

00:02:30,360  -->  00:02:34,260
Copied and pasted in the clamorously.

41

00:02:34,380  -->  00:02:35,260
All right.

42

00:02:35,430  -->  00:02:39,920
So here in this section data pre-processing we don't have anything to change.

43

00:02:39,970  -->  00:02:47,290
We are just going to import the data set with the regions actual functions be careful not with the results

44

00:02:47,390  -->  00:02:54,180
we found because we still need to have our sparse matrix that will also be an input of the Ekla function

45

00:02:54,390  -->  00:02:56,300
like here for the a priori function.

46

00:02:56,550  -->  00:03:00,260
So we will select this and executes.

47

00:03:00,420  -->  00:03:05,700
And of course we get the same number of duplicates the CSP file hasn't changed.

48

00:03:05,700  -->  00:03:09,870
We have five duplicates and no triplicated just five duplicates.

49

00:03:10,020  -->  00:03:10,830
OK.

50

00:03:10,830  -->  00:03:15,870
Then of course we can use the summary function to get some info about this dataset but of course these

51

00:03:15,870  -->  00:03:17,750
are going to be the same for us before.

52

00:03:17,820  -->  00:03:26,880
So we have 7500 transactions a hundred and nineteen products and the density is 0.03 that means that

53

00:03:26,880  -->  00:03:32,260
the proportion of non-zero values in the matrix is 0.03 3 percent.

54

00:03:32,480  -->  00:03:34,520
And then of course we have the most frequent items.

55

00:03:34,590  -->  00:03:40,950
Mineral water comes first and eggs and we can actually have a better look at these most frequent items

56

00:03:41,250  -->  00:03:43,400
by selecting this line and executes.

57

00:03:43,560  -->  00:03:46,580
And of course we get the same frequency plot.

58

00:03:46,590  -->  00:03:48,230
So that's exactly like before.

59

00:03:48,240  -->  00:03:50,640
We don't have anything to change here however.

60

00:03:50,760  -->  00:03:56,660
Now we enter the second code section which is to train the eclat model on the data set so for us we

61

00:03:56,670  -->  00:04:01,060
will just replace a priori here by a clock.

62

00:04:01,080  -->  00:04:01,880
Here we go.

63

00:04:02,190  -->  00:04:04,680
And guess what it is so simple.

64

00:04:04,680  -->  00:04:08,500
Here we have the a priori function to train the pre or model.

65

00:04:08,610  -->  00:04:11,660
And guess what the function is going to be to train the Clamato.

66

00:04:11,730  -->  00:04:13,860
It's going to be Ekla.

67

00:04:14,100  -->  00:04:15,120
So very simple.

68

00:04:15,180  -->  00:04:21,750
We are almost ready to train Ekla on our dataset but of course the cameral is much more simple.

69

00:04:21,780  -->  00:04:24,510
It doesn't include the confidence and the parameters.

70

00:04:24,510  -->  00:04:28,980
If right now we get to the slide of the eclat algorithm from the intuition tutorial.

71

00:04:29,160  -->  00:04:35,250
Well we can see that the Ekla algorithm has three steps and the first step is to set a minimum support

72

00:04:35,250  -->  00:04:35,460
.

73

00:04:35,550  -->  00:04:40,710
Remember before for the apriori algorithm the first step was to set a minimum support and a minimum

74

00:04:40,710  -->  00:04:41,670
confidence.

75

00:04:41,670  -->  00:04:45,470
Now we only need to set a minimum support so we don't need this.

76

00:04:45,510  -->  00:04:51,450
Actually this parameter is only for the a priori model so if we keep that we'll get an error so I'll

77

00:04:51,450  -->  00:04:56,750
just remove this and we can leave the minimum support to 0.04.

78

00:04:56,760  -->  00:04:59,580
But you'll see that it's even not necessary here.

79

00:04:59,580  -->  00:05:06,330
You'll see why at the end however we might need to add another parameter because since you know I mentioned

80

00:05:06,330  -->  00:05:13,240
that the algorithm is just going to return a set of items most frequently bought together.

81

00:05:13,410  -->  00:05:17,410
Well it wouldn't be interesting to have a set of only one item.

82

00:05:17,430  -->  00:05:23,580
So in order to get some sets of at least two items we will add another parameter here which we actually

83

00:05:23,580  -->  00:05:31,110
encountered earlier and which is the main line parameter and we will set it to 2 because we want to

84

00:05:31,110  -->  00:05:35,800
have the different sets of at least two items most frequently pre-Chase together.

85

00:05:36,130  -->  00:05:37,900
OK and now we're ready.

86

00:05:38,010  -->  00:05:39,440
See how it was simple.

87

00:05:39,510  -->  00:05:46,050
So let's select this and train the Clamato on our dataset done.

88

00:05:46,400  -->  00:05:47,020
OK.

89

00:05:47,430  -->  00:05:49,840
So now things change a little bit.

90

00:05:49,860  -->  00:05:56,190
First of course we see that we just train the camel obviously then we have the parameter specification

91

00:05:56,190  -->  00:05:59,160
like before but this time we don't have the confidence parameter.

92

00:05:59,160  -->  00:06:03,050
We have the support that we set to 0.04.

93

00:06:03,050  -->  00:06:04,520
That is the minimum support.

94

00:06:04,740  -->  00:06:09,840
And of course this time we have the minimum equals to remember before the millin parameter was set to

95

00:06:09,840  -->  00:06:15,970
1 but we didn't have any problem with that because all our rules contained at least two products but

96

00:06:15,990  -->  00:06:22,320
when using Ekla we need to set this minimum parameter to 2 because otherwise we'll get some sets of

97

00:06:22,320  -->  00:06:23,890
only one item.

98

00:06:24,210  -->  00:06:27,870
OK then we have these other more advanced informations here.

99

00:06:27,960  -->  00:06:31,210
And then there is something that I would like to highlight here.

100

00:06:31,350  -->  00:06:34,950
It's the number of sets and not rules.

101

00:06:34,950  -->  00:06:38,990
Remember before we had you know let's say we had eight hundred forty five.

102

00:06:39,060  -->  00:06:44,120
It was written eight hundred forty five rules because we had the rules of the forum.

103

00:06:44,220  -->  00:06:49,300
If people lie like cream then they're likely to buy chicken with the confidence of 40 percent that is

104

00:06:49,500  -->  00:06:51,010
with 40 percent chance.

105

00:06:51,180  -->  00:06:57,420
And here since you know we don't get these kind of rules of this form and we only get sets of items

106

00:06:57,430  -->  00:06:57,560
.

107

00:06:57,720  -->  00:06:58,990
Well this time we have indeed.

108

00:06:59,170  -->  00:07:01,110
Eight hundred forty five sets.

109

00:07:01,110  -->  00:07:07,170
So even if A-class considered as the Association of rule early model Well this doesn't return rules

110

00:07:07,200  -->  00:07:09,370
does actually return some sets.

111

00:07:09,630  -->  00:07:12,560
OK so we're going to have a look at the sets right now.

112

00:07:12,630  -->  00:07:18,690
We are ready to move to the last step of our role where we can actually jump back to the slide and as

113

00:07:18,690  -->  00:07:23,970
we can see the step two is to take all the subsets and transactions having higher support than minimum

114

00:07:23,970  -->  00:07:24,420
support.

115

00:07:24,420  -->  00:07:26,850
So that's where the function éclat is itself.

116

00:07:26,970  -->  00:07:31,910
And then the last step step three is to sort the subset by decreasing support.

117

00:07:31,980  -->  00:07:37,310
So this time it's not by decreasing left like for a priori there is no lift in a cloud.

118

00:07:37,440  -->  00:07:44,190
So this time we're going to sort the rules by decreasing support.

119

00:07:44,200  -->  00:07:44,760
All right.

120

00:07:44,760  -->  00:07:49,860
And we're going to take the 10 first worlds with the 10 highest supports So we are actually ready.

121

00:07:49,950  -->  00:07:52,550
Well the whole code is actually ready.

122

00:07:52,620  -->  00:07:55,500
We did it very efficiently Well it was very simple.

123

00:07:55,530  -->  00:08:01,320
So eventually let's have a look at the rules and you'll see that it's much more simple and honestly

124

00:08:01,320  -->  00:08:03,570
less interesting than a priori.

125

00:08:03,750  -->  00:08:05,180
But that's what we'll get.

126

00:08:05,190  -->  00:08:08,970
So I'm going to select this line and execute.

127

00:08:09,180  -->  00:08:17,660
And as I just told you we simply get different sets of items most frequently purchased together.

128

00:08:17,670  -->  00:08:24,150
So for example the set of items most frequently purchased together is mineral water and spaghetti with

129

00:08:24,150  -->  00:08:27,210
the support of point 0 5 9.

130

00:08:27,210  -->  00:08:32,560
And then it's chocolate and then it's chocolate and mineral water then eggs and mineral water.

131

00:08:32,640  -->  00:08:35,090
And that's of course strongly related to this.

132

00:08:35,310  -->  00:08:38,090
Most frequently purchased products in the store.

133

00:08:38,280  -->  00:08:41,870
So that's the results much less interesting than the a priori.

134

00:08:42,000  -->  00:08:46,180
But that can be very useful if you are looking for some very simple information.

135

00:08:46,230  -->  00:08:52,800
And by the way since I was telling you if we you know changed the support and set it to 0.03 like we

136

00:08:52,800  -->  00:08:55,520
did for the a priori model in the first place.

137

00:08:55,680  -->  00:09:00,930
Well you'll see that we'll get the same ranking here because as you can see the supports of these 10

138

00:09:00,930  -->  00:09:09,320
first sets of items most frequently pre-Chase together all have supports higher than 0.04 or 0.03.

139

00:09:09,330  -->  00:09:16,780
So indeed if we retrain the Clamato with the minimum support of 0.03 And if we select that again and

140

00:09:16,800  -->  00:09:25,040
executes we get the same ranking with mineral water and spaghetti coming first and then frozen vegetables

141

00:09:25,050  -->  00:09:30,470
mineral water as the tenth set of products most frequently pre-Chase together.

142

00:09:30,480  -->  00:09:33,110
All right so that's what the Clemenceau.

143

00:09:33,330  -->  00:09:38,430
Again if you want to have a serious analysis about how to create some added value for your business

144

00:09:38,730  -->  00:09:42,570
optimize the sales and the revenue you should go for a priori.

145

00:09:42,750  -->  00:09:48,120
But if you are looking for some very simple informations like the sets of products most frequently pre-Chase

146

00:09:48,120  -->  00:09:51,120
together then you can go for a cloud.

147

00:09:51,390  -->  00:09:53,110
So congratulations anyway.

148

00:09:53,220  -->  00:09:58,900
You now know how to implement to association role models the apriori and Ekla.

149

00:09:59,070  -->  00:10:01,170
And you know what to use them for.

150

00:10:01,170  -->  00:10:05,670
So thank you very much for watching these tutorials and I look forward to seeing you in the next part

151

00:10:05,670  -->  00:10:05,770
.

152

00:10:05,850  -->  00:10:07,560
Reinforcement learning.

153

00:10:07,560  -->  00:10:09,490
Until then enjoy machine learning.
