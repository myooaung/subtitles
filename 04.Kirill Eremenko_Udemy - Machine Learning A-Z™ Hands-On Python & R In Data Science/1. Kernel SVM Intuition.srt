1
00:00:00,670 --> 00:00:02,890
Hello and welcome back to the course of machine learning.

2
00:00:03,040 --> 00:00:08,130
Previously we talked about the support vector machine algorithm and we are going to talk about the kernel

3
00:00:08,130 --> 00:00:11,090
support vector machine algorithm and the intuition behind it.

4
00:00:11,370 --> 00:00:20,160
So as you recall in the support vector machine situation we had a set of observations which belong to

5
00:00:20,160 --> 00:00:27,960
different classes and the point was to find this decision boundary between them so that any future observations

6
00:00:28,380 --> 00:00:31,570
could be identified which Clauss they fall into.

7
00:00:31,890 --> 00:00:36,810
And in this case we can see that there a decision boundary and the support vector machine algorithm

8
00:00:36,840 --> 00:00:39,740
tells us exactly how to find that boundary.

9
00:00:39,990 --> 00:00:44,820
But what happens when we cannot find a boundary.

10
00:00:44,820 --> 00:00:51,150
What happens in a situation like this for example what do we do here we can't just draw a line through

11
00:00:51,150 --> 00:00:55,710
these points right we can just separate them with a straight line.

12
00:00:55,800 --> 00:01:01,440
We can separate them of a horizontal line concept broom with a vertical line whatever we try to do.

13
00:01:01,530 --> 00:01:09,960
We cannot separate these points in the same way that the support vector machine algorithm told us to.

14
00:01:10,230 --> 00:01:12,030
And so what happens in this situation.

15
00:01:12,180 --> 00:01:13,310
And why does it happen.

16
00:01:13,410 --> 00:01:20,480
Well this happens because in this case the data is not linearly set separable.

17
00:01:20,730 --> 00:01:25,380
So here we've got the two example side by side on the left the linearity separable data and on the right

18
00:01:25,380 --> 00:01:27,090
the nonlinear separable data.

19
00:01:27,330 --> 00:01:36,720
And what's the support vector machine algorithm does is it helps us find that decision boundary or correctly

20
00:01:36,720 --> 00:01:38,940
place that decision boundary.

21
00:01:39,090 --> 00:01:40,780
But it does have an assumption.

22
00:01:40,800 --> 00:01:47,640
The assumption is that the data is the new is separable So basically saying that it is possible to place

23
00:01:47,760 --> 00:01:53,790
the decision boundary so the port vector machine algorithm helps us choose the optimal decision boundary

24
00:01:53,820 --> 00:01:59,820
or the multiple decision boundaries or we can draw whereas in that non linear separable case we can't

25
00:01:59,820 --> 00:02:06,510
even draw one single decision boundary or linear decision boundary so therefore the support vector machine

26
00:02:06,540 --> 00:02:09,250
algorithm just won't work by definition.

27
00:02:09,330 --> 00:02:11,420
And so what are we going to do.

28
00:02:11,450 --> 00:02:13,920
Well this is what this section is about.

29
00:02:13,920 --> 00:02:19,530
This section is about coming up with an algorithm that will help us deal with situations like that where

30
00:02:19,920 --> 00:02:27,120
we have to extract this class when it is surrounded or in other situations where we can just simply

31
00:02:27,120 --> 00:02:28,520
just draw a line.

32
00:02:28,860 --> 00:02:33,750
And the way this section is structured or the intuition tutorials of the section is first of all we're

33
00:02:33,750 --> 00:02:40,740
going to explore a method we just called Going to a higher dimensional space where we will learn how

34
00:02:40,740 --> 00:02:52,110
we can take this dataset and add an extra dimension into our space that we're dealing with and make

35
00:02:52,140 --> 00:02:58,260
our data a linearly separable So we'll have to look at a couple of those examples and then we will discuss

36
00:02:58,260 --> 00:03:04,320
the kernel trick which allows us to do these things in a more can be patiently efficient way without

37
00:03:04,350 --> 00:03:07,470
having to deal with multiple or higher dimensions.

38
00:03:07,620 --> 00:03:10,770
And finally we will talk about the different types of kernels that exist.

39
00:03:10,890 --> 00:03:14,950
So quite an interesting section ahead and I can't wait to see the next tutorial.

40
00:03:14,970 --> 00:03:16,500
Until then happy analyzing
