WEBVTT
1

00:00:00.870  -->  00:00:04.770
Hello and welcome back to the course on machine learning in today's tutorial we're going to talk about

2

00:00:04.770  -->  00:00:07.610
the K nearest neighbors algorithm.

3

00:00:07.620  -->  00:00:09.430
All right so let's get started.

4

00:00:09.960  -->  00:00:16.390
So let's imagine that we have a scenario where we have two categories already present in our data set

5

00:00:16.390  -->  00:00:22.950
so we've identified two categories and one is carrier one on the left which is red card two is green

6

00:00:22.950  -->  00:00:23.700
on the right.

7

00:00:23.700  -->  00:00:29.580
And for simplicity's sake we're just going to take into consideration two variables or two columns in

8

00:00:29.580  -->  00:00:34.830
our data set so all of this grouping is happening based on these two columns x1 and x2.

9

00:00:34.870  -->  00:00:38.130
And now let's say we add a new data point into our data set.

10

00:00:38.130  -->  00:00:42.930
The question is should it fall into the red category or should fall to the green category.

11

00:00:42.930  -->  00:00:44.340
How do we decide that.

12

00:00:44.340  -->  00:00:48.850
So how do we classify this new data point to a cluster of fires.

13

00:00:48.900  -->  00:00:54.300
As a Red data point or a green data point and that's where the nearest neighbors algorithm will come

14

00:00:54.300  -->  00:00:55.300
to assist us.

15

00:00:55.530  -->  00:01:01.400
At the end of performing this algorithm we'll be able to identify whether it's a red or green point

16

00:01:01.410  -->  00:01:04.050
and in this case the point turned out to be red.

17

00:01:04.260  -->  00:01:07.870
So how does the K nearest neighbor algorithm work.

18

00:01:07.890  -->  00:01:08.840
How did it do that.

19

00:01:09.000  -->  00:01:16.290
Well we're going to build a step by step rule guides to the K and then and after we've built it we're

20

00:01:16.290  -->  00:01:19.250
going to actually perform it manually to see how it works.

21

00:01:19.290  -->  00:01:21.840
And as you'll see is a very very simple algorithm.

22

00:01:21.840  -->  00:01:27.260
All right so the first step is to choose a number k of neighbors that you're going to have in your algorithm

23

00:01:27.260  -->  00:01:33.500
so you go to you have to identify whether K is equal to 1 2 2 3 5 or some other number.

24

00:01:33.660  -->  00:01:40.980
And one of the most common default values for k is 5 and Eckstine to take the K nearest neighbors of

25

00:01:40.980  -->  00:01:43.980
the new data point according to their Euclidean distance.

26

00:01:43.980  -->  00:01:49.500
Now here you can if you don't have to use Euclidean distance you can use other distances such as a manhattan

27

00:01:49.500  -->  00:01:53.550
distance or any other distances that you might be considering.

28

00:01:53.730  -->  00:01:57.750
But in most cases Euclidean distance is so we're to stick to those.

29

00:01:57.750  -->  00:02:03.480
So once you've taken the nearest neighbors among these K neighbors you need to count the number of data

30

00:02:03.510  -->  00:02:04.650
points in each category.

31

00:02:04.650  -->  00:02:10.080
So how many data points fell into one category to the other carrier and so on if you might even have

32

00:02:10.080  -->  00:02:12.160
more than two categories in your data set.

33

00:02:12.180  -->  00:02:17.130
So you just need to calculate how many fall into each category and then you need to assign the new data

34

00:02:17.130  -->  00:02:20.080
point to the category where you counted the most neighbors.

35

00:02:20.280  -->  00:02:21.210
As simple as that.

36

00:02:21.210  -->  00:02:23.340
That's why it's called Kinnear's neighbors.

37

00:02:23.490  -->  00:02:28.200
And then your model is ready as it is it's a very simple algorithm and moral which is going to do a

38

00:02:28.200  -->  00:02:31.320
manual exercise right now to really solidify this knowledge.

39

00:02:31.320  -->  00:02:33.210
So let's move onto that.

40

00:02:33.600  -->  00:02:38.890
So here we've got the new data point has been added to our scatterplot as we saw previously.

41

00:02:38.970  -->  00:02:43.710
How do we find the nearest neighbors of this new datapoint.

42

00:02:44.010  -->  00:02:49.560
Well let's have a look at the Euclidean distance that we're going to use so quickly and distance is

43

00:02:49.560  -->  00:02:55.500
a very basic type of distance that we define in geometry it's the one we use in geometry.

44

00:02:55.500  -->  00:03:01.260
And basically if you have two points over here one and two then the distance between the two points

45

00:03:01.500  -->  00:03:03.630
is measured according to this formula.

46

00:03:03.630  -->  00:03:10.650
So x 2 minus X the difference between the x coordinates and then squared plus the difference between

47

00:03:10.650  -->  00:03:15.790
the y coordinates squared and then you take a square root out of all that.

48

00:03:16.080  -->  00:03:22.650
And that is basically if you look at it this way it's a right angled triangle as suiting you to in.

49

00:03:22.670  -->  00:03:27.810
Cathedral's and you squaring it you take in other theaters and it's growing it's taking you adding them

50

00:03:27.810  -->  00:03:32.610
up you're taking a square root and that gives you the length of the high poisoner's.

51

00:03:32.610  -->  00:03:33.610
So there we go.

52

00:03:33.630  -->  00:03:35.880
That's how Euclidean distance work.

53

00:03:35.910  -->  00:03:40.550
Again you could use any type of distance but this is the geometrical distance and this is what we're

54

00:03:40.550  -->  00:03:41.410
going to stick to.

55

00:03:41.460  -->  00:03:47.010
So basically on a scatterplot a two dimensional kind of polygon just draw the lines and see what is

56

00:03:47.010  -->  00:03:48.280
closer.

57

00:03:48.300  -->  00:03:53.040
So here on your data point how are going to identify which other closest five neighbors.

58

00:03:53.160  -->  00:03:58.740
So basically we just look at them and we see the distances here so we can see that's the closest one

59

00:03:59.120  -->  00:04:04.920
that's probably the second closest ONE-THIRD closest fourth closest fifth closest So let's outline those

60

00:04:04.920  -->  00:04:05.520
.

61

00:04:05.520  -->  00:04:05.840
There we go.

62

00:04:05.840  -->  00:04:11.790
So now all we have to do is step three among these Cockayne neighbors count the number of data points

63

00:04:11.790  -->  00:04:17.240
in each category is in category 1 in the red when we have 3 neighbors in category 2 we have two neighbors

64

00:04:17.250  -->  00:04:17.460
.

65

00:04:17.640  -->  00:04:23.190
So therefore step 4 are assigned the new data point to the category where you counted the most neighbors

66

00:04:23.190  -->  00:04:23.670
.

67

00:04:23.670  -->  00:04:27.960
That means we need to assign it to the read category as simple as that.

68

00:04:27.960  -->  00:04:28.590
There we go.

69

00:04:28.590  -->  00:04:33.150
Now we have classified this new point and your model is ready.

70

00:04:33.180  -->  00:04:33.690
There we go.

71

00:04:33.680  -->  00:04:36.190
It's a it's a very straightforward algorithm.

72

00:04:36.210  -->  00:04:40.980
One of the simplest you can think about it so it's very good to get this section started with a simple

73

00:04:40.980  -->  00:04:41.920
example like that.

74

00:04:42.000  -->  00:04:48.340
And also we're going to have a look at some practical exercises and Hudlin We'll show you around aren't

75

00:04:48.390  -->  00:04:48.890
Python.

76

00:04:49.050  -->  00:04:50.240
Eloquent singer in XM.

77

00:04:50.250  -->  00:04:52.350
And until then in Germany machine learning
