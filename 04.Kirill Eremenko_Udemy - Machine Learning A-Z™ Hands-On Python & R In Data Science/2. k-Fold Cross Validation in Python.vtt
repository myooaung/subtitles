WEBVTT
1
00:00:00.150 --> 00:00:07.140
Hello, my friends, and welcome to the final part of this course 410 on natural selection and boosting.

2
00:00:07.590 --> 00:00:10.230
So in this part, we're going to see three important things.

3
00:00:10.230 --> 00:00:13.620
You know, the final tools that I would like you to have in your tool kit.

4
00:00:14.040 --> 00:00:19.740
And these are, first, the careful cross-validation technique, which will allow you to measure in

5
00:00:19.740 --> 00:00:24.540
the most relevant way the performance of each of your future machinery models.

6
00:00:24.990 --> 00:00:31.020
Then I will teach you grid search, which is a technique that allows you to find the best version of

7
00:00:31.020 --> 00:00:31.500
a moral.

8
00:00:31.620 --> 00:00:36.660
By finding the optimal parameters leading to the best score of performance.

9
00:00:36.870 --> 00:00:42.140
So if you're doing regression well, you will find the optimal hyper parameters to find the best squared.

10
00:00:42.420 --> 00:00:47.250
And if you're doing classification, you will find the best hyper parameters for the highest accuracy.

11
00:00:47.790 --> 00:00:50.680
And then that's the final bonus section of discourse.

12
00:00:50.730 --> 00:00:56.640
Let's say I will teach you one of the most powerful models in machine learning, which is x g boost.

13
00:00:56.730 --> 00:00:59.580
I really want you to have extra boost in your toolkit.

14
00:00:59.910 --> 00:01:02.670
And then you will just be close to invincible.

15
00:01:02.970 --> 00:01:08.880
So let's finish with this at the same time, very excited and a bit moved that, you know, we're approaching

16
00:01:08.880 --> 00:01:09.840
the end of the journey.

17
00:01:10.050 --> 00:01:15.090
But let's finish on a good note and let's end two parts and moral selection and boosting.

18
00:01:15.390 --> 00:01:18.480
But just before, let's make sure that everyone here is on the same page.

19
00:01:18.590 --> 00:01:22.260
I give you the link to this for the right before this is oriels and make sure we connect to it.

20
00:01:22.560 --> 00:01:23.610
And now there we go.

21
00:01:23.730 --> 00:01:26.970
Let's end the points and model selection and boosting.

22
00:01:27.720 --> 00:01:28.170
All right.

23
00:01:28.230 --> 00:01:33.120
And so, as we said, well, we have two sections, mole selection, where I will teach you careful

24
00:01:33.120 --> 00:01:37.560
cross-validation and grid search and then the final section, Extubate.

25
00:01:37.590 --> 00:01:41.760
So let's start with mole selection and let's start with Python, as usual.

26
00:01:42.240 --> 00:01:48.150
And in this folder, you will find two implementations, assign one for K4 cross-validation, where

27
00:01:48.150 --> 00:01:53.700
we will implement this key for cross-validation technique to find the best accuracy because we will

28
00:01:54.030 --> 00:01:56.340
implement this for a classification problem.

29
00:01:56.400 --> 00:02:01.230
Actually, the same one as in the classification point so that we can stay familiar with the dataset

30
00:02:01.260 --> 00:02:03.330
and focus on the technique itself.

31
00:02:03.690 --> 00:02:09.360
And then in the next store, I will teach you grid search, which will apply on, you know, the kernel

32
00:02:09.450 --> 00:02:13.130
as model, which we implemented in part three with the same data set.

33
00:02:13.440 --> 00:02:16.760
And for this model, we will find the best hyper parameters.

34
00:02:16.800 --> 00:02:21.750
You know, the best kernel, the best parameter is C and maybe other hyper parameters.

35
00:02:21.750 --> 00:02:22.170
We'll see.

36
00:02:22.290 --> 00:02:26.520
So let me just quickly remind what this dataset is about very quickly.

37
00:02:26.700 --> 00:02:34.020
So remember, this is a data set of a car company which has just released a brand new luxury SUV.

38
00:02:34.320 --> 00:02:40.710
And it is trying to understand through this data set, well, which customers buy SUV so that we can

39
00:02:40.980 --> 00:02:46.200
then train a machinery Morrall, you know, a classification model to learn the correlations between

40
00:02:46.200 --> 00:02:51.270
these two features, age, an estimated salary and the dependent variable purchased.

41
00:02:51.510 --> 00:02:51.790
All right.

42
00:02:51.810 --> 00:02:58.260
So each row of this data set corresponds to different customers and each of them, we have the age,

43
00:02:58.500 --> 00:03:04.380
the estimated salary and whether or not they purchased the SUV in the past couple of years.

44
00:03:04.590 --> 00:03:04.890
OK.

45
00:03:05.250 --> 00:03:06.840
So that was a quick reminder.

46
00:03:06.900 --> 00:03:10.050
And now let's focus on our first technique.

47
00:03:10.380 --> 00:03:16.380
The K4 cross-validation and therefore let us open this implementation on either Google Collaboratory

48
00:03:16.680 --> 00:03:18.040
or Jupiter notebook.

49
00:03:18.210 --> 00:03:19.490
Feel free to choose your favorite.

50
00:03:19.920 --> 00:03:22.890
I'm opening it with Google Collaboratory, as usual.

51
00:03:23.340 --> 00:03:25.200
And now that notebook is loading.

52
00:03:25.490 --> 00:03:26.550
It is laying it out.

53
00:03:26.700 --> 00:03:28.380
And there we go.

54
00:03:28.470 --> 00:03:31.470
Welcome to the K4 cross-validation implementation.

55
00:03:32.070 --> 00:03:32.280
All right.

56
00:03:32.310 --> 00:03:34.840
So, as usual, this file is in read-only mode.

57
00:03:34.950 --> 00:03:40.800
We're going to create right away a copy by clicking foul here and then save a copy in drive.

58
00:03:40.860 --> 00:03:46.030
This will create a copy and insight, which we will be able to re.

59
00:03:47.220 --> 00:03:52.560
Not the whole implementation, because you're going to see that all the cells of this implementation

60
00:03:52.650 --> 00:03:56.400
are the same as in the kernel as VM implementation.

61
00:03:56.460 --> 00:04:04.260
We made in part three except one cell, which is a cell of course, where we implement this K4 trust

62
00:04:04.260 --> 00:04:05.280
validation technique.

63
00:04:05.610 --> 00:04:10.470
So I'm going to show you all these cells here are, you know, for the data processing phase to which

64
00:04:10.470 --> 00:04:12.570
we added that features killing tool.

65
00:04:12.870 --> 00:04:18.330
Then we train indeed the kernel as VM or on the train sets or just as in our previous kernel, as an

66
00:04:18.390 --> 00:04:19.260
implementation.

67
00:04:19.740 --> 00:04:26.130
Then we make the computer matrix to do, you know, first evaluation of the morale with the accuracy

68
00:04:26.220 --> 00:04:27.540
on the test only.

69
00:04:27.900 --> 00:04:29.490
And here there we go.

70
00:04:29.550 --> 00:04:31.830
That's where we apply Caple cross-validation.

71
00:04:31.860 --> 00:04:39.060
And this cell, which I'm going to remove right away, is the only difference with our previous implementation

72
00:04:39.060 --> 00:04:40.300
of the kernel as we're Morrall.

73
00:04:40.590 --> 00:04:44.640
We implemented in part three, you know, because the last two cells here are the same.

74
00:04:44.880 --> 00:04:49.590
We visualise here the transit resource on this 2D plot and same for below.

75
00:04:49.680 --> 00:04:52.540
We visualize the test results on the same 2D plot.

76
00:04:52.780 --> 00:04:53.090
Okay.

77
00:04:53.430 --> 00:04:59.460
So we're just going to reimplement K four cross-validation here so that we can add in our.

78
00:05:00.060 --> 00:05:00.780
Tool kit.

79
00:05:01.040 --> 00:05:06.290
An extra tool to measure the performance of any classification model.

80
00:05:06.560 --> 00:05:08.050
All right, I'm ready.

81
00:05:08.180 --> 00:05:09.080
Let's do this.

82
00:05:09.170 --> 00:05:11.140
Let's create a new coat here.

83
00:05:11.450 --> 00:05:17.810
And first, remember that in order to get the Google Kulab assistance, well, we need to run the notebook.

84
00:05:17.830 --> 00:05:23.510
And to do this well, we can either run the first cell or upload the dataset to let us do it right now.

85
00:05:23.930 --> 00:05:28.040
So I just clicked on this little folder here to access to files of our machine.

86
00:05:28.250 --> 00:05:33.020
And right now, The Notebook is connecting to a runtime to enable file browsing, which we should be

87
00:05:33.020 --> 00:05:34.850
able to do in a second.

88
00:05:34.890 --> 00:05:36.890
You know, we should be able to see the upload button.

89
00:05:36.950 --> 00:05:37.430
There we go.

90
00:05:37.940 --> 00:05:41.240
So let's click upload then please find on your machine.

91
00:05:41.300 --> 00:05:46.760
You know, this machinery is the Kosen datasets folder, wherever you put it on your machine.

92
00:05:46.880 --> 00:05:48.110
I put it on my desktop.

93
00:05:48.320 --> 00:05:50.030
And then let's go inside.

94
00:05:50.210 --> 00:05:56.120
Let's end to part 10, most election busting and then section forty eight, most election python.

95
00:05:56.360 --> 00:06:00.170
And that social network adds that CSB dataset.

96
00:06:00.650 --> 00:06:02.150
Click open and click.

97
00:06:02.150 --> 00:06:07.880
OK, now we have the dataset into our notebook and therefore we'll be able to get the assistance of

98
00:06:07.880 --> 00:06:11.330
Google Kulab when applying K4 cross-validation.

99
00:06:12.080 --> 00:06:12.710
Are you ready?

100
00:06:12.980 --> 00:06:14.540
Now once again, you have two options.

101
00:06:14.540 --> 00:06:18.330
You can try to do it yourself first by checking out, of course, the cycle.

102
00:06:18.420 --> 00:06:22.640
You're an API, you know the documentation or you know, we can just do it together.

103
00:06:22.930 --> 00:06:25.040
Anyway, we're on to the finish line here.

104
00:06:25.130 --> 00:06:28.970
I'm already super proud of all the work you've provided within this course.

105
00:06:28.970 --> 00:06:30.140
You know, this is a huge cause.

106
00:06:30.440 --> 00:06:33.740
So if you want, let's just finish this together on the best note.

107
00:06:34.850 --> 00:06:35.810
All right, let's do this.

108
00:06:35.840 --> 00:06:42.320
So as you understood, we're gonna start from cycler and because indeed, we're gonna grab a tool from

109
00:06:42.320 --> 00:06:46.490
the cyclone library to implement this K4 cross-validation technique.

110
00:06:46.880 --> 00:06:47.660
So psyched.

111
00:06:47.660 --> 00:06:50.510
Learn from which we're gonna get access to.

112
00:06:50.720 --> 00:06:57.920
Well, that's not all selection module, which is also the name of our section here, moral selection

113
00:06:57.920 --> 00:07:03.560
module, because this is the module that contains, of course, the cross valse core function.

114
00:07:03.860 --> 00:07:10.550
Another class this time just a function which will perform the K4 cross-validation on our training set,

115
00:07:10.670 --> 00:07:11.630
not dataset.

116
00:07:11.810 --> 00:07:16.400
We have to apply it on a training set because, you know, we always want to have this separate test

117
00:07:16.400 --> 00:07:22.340
set to test the deployment of our model on brandnew observations on which Moe wasn't trained.

118
00:07:22.680 --> 00:07:26.840
So this gave for cross-validation technique is supposed to be called on the train set.

119
00:07:27.350 --> 00:07:28.100
So let's do this.

120
00:07:28.190 --> 00:07:32.570
Let's import that cross vow.

121
00:07:33.020 --> 00:07:33.460
There we go.

122
00:07:33.470 --> 00:07:39.860
I was waiting for curl up to help me crossbars called function to implement the K4 cross-validation

123
00:07:39.860 --> 00:07:40.310
technique.

124
00:07:41.100 --> 00:07:41.440
All right.

125
00:07:41.720 --> 00:07:49.330
So as you understood, K4 transmutation consist of creating a certain number of trained test fold.

126
00:07:49.640 --> 00:07:53.810
We are going to say 10 trendies false because 10 is too commonly chosen value.

127
00:07:54.080 --> 00:08:00.560
So it consists of creating 10 trained test folds on which you're gonna train your machine learning model

128
00:08:00.590 --> 00:08:01.950
on each of the train forward.

129
00:08:02.420 --> 00:08:06.050
And at the same time tested separately on the test.

130
00:08:06.800 --> 00:08:13.190
And therefore, since you have 10 trained test falls, well, you will end up with ten different accuracy's

131
00:08:13.400 --> 00:08:15.050
on different test sets.

132
00:08:15.080 --> 00:08:16.820
You know, like 10 different test sets.

133
00:08:17.180 --> 00:08:21.590
And therefore, this is a much relevant way to measure the performance of your model, because, you

134
00:08:21.590 --> 00:08:25.670
know, this makes sure that we don't get lucky on one test set.

135
00:08:25.730 --> 00:08:30.890
You know, when measuring the performance of the model right here on this test set, when making the

136
00:08:30.890 --> 00:08:33.480
computer matrix while we get here, 93 percent.

137
00:08:33.560 --> 00:08:38.890
But we could have randomly chosen a test set on which the got lucky one making its predictions.

138
00:08:39.050 --> 00:08:40.100
It's truly possible.

139
00:08:40.220 --> 00:08:42.980
So here we are creating 10 different test sets.

140
00:08:43.010 --> 00:08:44.760
But the correct term is test false.

141
00:08:45.050 --> 00:08:50.780
So as to evaluate the model on different test sets and therefore reducing any risk of getting lucky

142
00:08:50.900 --> 00:08:52.060
on a separate test.

143
00:08:52.280 --> 00:08:52.580
Right.

144
00:08:52.610 --> 00:08:59.390
And of course, the final accuracy will get will be the average of the 10 accuracies obtained with the

145
00:08:59.390 --> 00:09:00.080
ten test.

146
00:09:00.080 --> 00:09:00.300
False.

147
00:09:00.560 --> 00:09:00.770
Right.

148
00:09:00.890 --> 00:09:02.300
This is exactly what this is about.

149
00:09:02.390 --> 00:09:08.330
And therefore, now what we're about to do is create a new variable, which we're gonna call accuracy's

150
00:09:09.010 --> 00:09:16.130
and which will simply be the list of the different accuracy's we get by testing the model on the 10

151
00:09:16.280 --> 00:09:17.000
test folds.

152
00:09:17.300 --> 00:09:22.550
And in which, of course, each accuracy is the accuracy obtained on each of the 10 test faults.

153
00:09:22.670 --> 00:09:29.000
So we're gonna get first this list and then will compute the mean of all the accuracies in that list.

154
00:09:29.550 --> 00:09:29.830
All right.

155
00:09:29.870 --> 00:09:31.760
And how can we get that list?

156
00:09:31.850 --> 00:09:33.350
Well, of course we can get it.

157
00:09:33.560 --> 00:09:35.900
Thanks to this Cross Vänskä function.

158
00:09:36.230 --> 00:09:38.090
So let's call this function right away.

159
00:09:38.330 --> 00:09:40.910
Cross VAO Score.

160
00:09:40.940 --> 00:09:41.490
Thank you.

161
00:09:41.600 --> 00:09:42.080
CoLab.

162
00:09:42.390 --> 00:09:48.350
And inside which we're going to enter four essential parameters which are first the estimate here,

163
00:09:48.440 --> 00:09:49.520
which is your classifier.

164
00:09:49.550 --> 00:09:53.930
You know the same one we created in that cell, that classifier here.

165
00:09:54.470 --> 00:09:57.350
Then we'll have to input the features of the training set.

166
00:09:57.470 --> 00:09:57.800
Right.

167
00:09:57.830 --> 00:09:58.450
Which is X.

168
00:09:58.940 --> 00:09:59.720
Then the dependent.

169
00:09:59.810 --> 00:10:01.830
Foible of the training set waitering.

170
00:10:02.150 --> 00:10:06.740
And finally, the fourth and last essential argument, which is, of course, number of fault.

171
00:10:07.040 --> 00:10:09.680
We want to have when performing K4 cross-validation.

172
00:10:10.010 --> 00:10:11.720
So I told you 10 as an example.

173
00:10:11.770 --> 00:10:13.760
But it is also the most common value.

174
00:10:13.970 --> 00:10:15.110
And that's a valuable truth.

175
00:10:15.710 --> 00:10:16.100
All right.

176
00:10:16.190 --> 00:10:17.810
So let's input these parameters.

177
00:10:18.050 --> 00:10:20.470
As we said, the first one is estimate her.

178
00:10:21.080 --> 00:10:26.030
And that is, of course, our classifier, which is our kernel as VM model.

179
00:10:26.570 --> 00:10:30.890
Then the next one is the features of the training set.

180
00:10:30.890 --> 00:10:36.150
Therefore, we have to enter here X train and the name of the parameter is X.

181
00:10:36.680 --> 00:10:37.730
Okay then.

182
00:10:37.730 --> 00:10:42.110
Same for the dependent variable values of the training set.

183
00:10:42.410 --> 00:10:44.590
And for this, the name of the parameter is Y.

184
00:10:44.990 --> 00:10:48.050
And this is equal of course to Y train.

185
00:10:48.320 --> 00:10:48.710
Right.

186
00:10:48.860 --> 00:10:51.320
The ground truth of the training set basically.

187
00:10:51.740 --> 00:10:57.190
And finally the number of trained test falls we want to have, which is given by the parameters named

188
00:10:57.290 --> 00:10:57.880
CV.

189
00:10:58.460 --> 00:11:00.630
And that we choose to be ten.

190
00:11:01.470 --> 00:11:01.870
All right.

191
00:11:01.940 --> 00:11:02.600
And that's it.

192
00:11:02.810 --> 00:11:08.450
This will not only perform the K for cross-validation technique, but at the same time return these

193
00:11:08.600 --> 00:11:13.670
10 accuracy's resulting from deploying our model on the ten test.

194
00:11:13.820 --> 00:11:14.300
False.

195
00:11:14.810 --> 00:11:15.410
Okay.

196
00:11:15.740 --> 00:11:19.250
And now we're gonna end this with two beautiful prints.

197
00:11:19.610 --> 00:11:19.850
Right.

198
00:11:19.940 --> 00:11:25.610
Just I'm gonna make a much more beautiful print than this one, which just prints the accuracy in this

199
00:11:25.610 --> 00:11:26.060
format.

200
00:11:26.450 --> 00:11:27.650
But check this out.

201
00:11:27.680 --> 00:11:33.230
We're gonna make a much more beautiful one by using the format function in Python three.

202
00:11:33.410 --> 00:11:37.700
This is also something I would like to get and that I would like to know how to implement.

203
00:11:37.970 --> 00:11:39.140
So let's do this here.

204
00:11:39.290 --> 00:11:40.970
We're gonna start directly with the print.

205
00:11:41.510 --> 00:11:41.810
Right.

206
00:11:41.840 --> 00:11:42.340
Print.

207
00:11:42.750 --> 00:11:44.840
Then we're going to enter in quotes.

208
00:11:45.140 --> 00:11:46.830
Well, first, accuracy.

209
00:11:47.090 --> 00:11:51.380
This way to, you know, clearly specify that we're printing the accuracy.

210
00:11:51.470 --> 00:11:52.610
So that's just a string.

211
00:11:52.970 --> 00:11:55.460
And then inside a pair of square brackets.

212
00:11:55.550 --> 00:12:00.650
Just like that, we will enter inside the format of the accuracy.

213
00:12:01.100 --> 00:12:04.670
And I would actually like to print this with two decimals.

214
00:12:04.790 --> 00:12:06.950
After the comma and as a float, of course.

215
00:12:07.250 --> 00:12:15.820
And a way to do this with this format technique in Python is to specify here, Colin point to F meaning

216
00:12:15.830 --> 00:12:16.370
exactly.

217
00:12:16.370 --> 00:12:19.010
That it's gonna be a float with two decimals.

218
00:12:19.190 --> 00:12:19.880
After the comma.

219
00:12:20.270 --> 00:12:20.570
Okay.

220
00:12:21.320 --> 00:12:26.430
And then since, you know, we're not gonna displayed as a form of here, for example, open ninety

221
00:12:26.430 --> 00:12:28.610
three but ninety three percent.

222
00:12:29.030 --> 00:12:32.240
Well I'm gonna add a space here and then percent.

223
00:12:32.480 --> 00:12:36.240
Okay, so that's what you have to answer within the double quote.

224
00:12:36.710 --> 00:12:40.520
But then outside of the double quotes, let's add a dot here.

225
00:12:40.850 --> 00:12:48.320
And then this format function in which you're going to input exactly the variable that corresponds to

226
00:12:48.320 --> 00:12:50.990
this value here with this specific format.

227
00:12:51.230 --> 00:12:57.860
And therefore, what you have to enter here is exactly the mean of the accuracies in that list.

228
00:12:58.280 --> 00:13:04.040
And the trick to do this efficiently, you know, to compute this efficiently is to get that variable,

229
00:13:04.070 --> 00:13:05.560
you know, that list of accuracies.

230
00:13:05.960 --> 00:13:12.980
And then we're gonna use a pre-built method of a python list, which is very simply the main function.

231
00:13:13.340 --> 00:13:19.380
And to use this, you can just add here a dot just after your list and then I mean, with some parenthesis.

232
00:13:19.730 --> 00:13:20.450
And that's it.

233
00:13:20.600 --> 00:13:24.470
This will compute the mean of the ten accuracies within that list.

234
00:13:24.950 --> 00:13:31.490
And of course, since we want to have it in the present format here when it's at times 100.

235
00:13:31.880 --> 00:13:32.140
Right.

236
00:13:32.150 --> 00:13:35.990
In order to transform this from open ninety three to ninety three percent.

237
00:13:36.830 --> 00:13:37.150
Okay.

238
00:13:37.220 --> 00:13:37.610
Good.

239
00:13:37.760 --> 00:13:40.310
So this line of code prints the accuracy.

240
00:13:40.550 --> 00:13:41.420
We could stop here.

241
00:13:41.450 --> 00:13:47.060
But you know, it would actually be very interesting to also print the standard deviation because you

242
00:13:47.060 --> 00:13:49.600
know, since we getting 10 different accuracy's.

243
00:13:50.000 --> 00:13:55.100
Well, it would be very interesting to see what variance we had among these accuracies.

244
00:13:55.460 --> 00:14:00.320
For example, let's say that the mean of the accuracy's in that list is 93 percent.

245
00:14:00.740 --> 00:14:06.340
Well, you know, we would have two very different situations if on the one hand, we have some accuracy,

246
00:14:06.380 --> 00:14:11.080
close to 98 percent and others close to, let's say, 84, 85 percent.

247
00:14:11.180 --> 00:14:13.670
But all of them averaging at 93 percent.

248
00:14:14.060 --> 00:14:15.200
That's the first situation.

249
00:14:15.260 --> 00:14:21.140
And that situation would be very different than having some accuracy is very close to 93 percent, like

250
00:14:21.140 --> 00:14:26.570
92, 93, 94, 92, again, 94 again, etc. close to the average.

251
00:14:26.990 --> 00:14:29.810
And that's something we can get with the standard deviation.

252
00:14:30.110 --> 00:14:36.350
And that's exactly what I will add here as a final line of code inside this K4 for cross-validation

253
00:14:36.350 --> 00:14:36.720
cell.

254
00:14:36.860 --> 00:14:43.370
So I'm actually going to copy that line of code and then just below, I'm going to do another print,

255
00:14:43.580 --> 00:14:45.620
which is the print of this time.

256
00:14:46.040 --> 00:14:46.880
The standard.

257
00:14:47.900 --> 00:14:52.540
Deviation, which will be exactly, you know, the root of the variance of these accuracies.

258
00:14:53.090 --> 00:14:53.700
And same.

259
00:14:53.780 --> 00:14:57.770
I'm going to choose to have a float with two decimals after the comma and here.

260
00:14:57.860 --> 00:15:01.850
Well, to compute the standard deviation of the accuracies within that list.

261
00:15:02.030 --> 00:15:09.080
Well, we need to replace that mean function by SDD, the SDD function, which computes exactly the

262
00:15:09.080 --> 00:15:11.660
standard deviation of values within a list.

263
00:15:11.780 --> 00:15:14.750
Within this accuracy's list and all good.

264
00:15:15.080 --> 00:15:16.760
Now we are ready for the final run.

265
00:15:16.990 --> 00:15:23.720
And I'm just gonna run everything again to see what will be this most relevant measure of the accuracy.

266
00:15:23.930 --> 00:15:27.710
And at the same time, the standard deviation of the accuracy is within the test.

267
00:15:27.710 --> 00:15:28.070
False.

268
00:15:28.630 --> 00:15:29.060
All right.

269
00:15:29.270 --> 00:15:29.870
Are you ready?

270
00:15:30.020 --> 00:15:30.890
Let's do this.

271
00:15:31.100 --> 00:15:36.920
Let's click runtime here and then we can just do a restart and run.

272
00:15:37.010 --> 00:15:38.110
Oh, here.

273
00:15:38.210 --> 00:15:40.190
If you're sure you're ready, let's click.

274
00:15:40.270 --> 00:15:40.790
Yes.

275
00:15:41.060 --> 00:15:42.470
And let's do this now.

276
00:15:42.530 --> 00:15:44.750
All the cells will be running, as we can see.

277
00:15:45.200 --> 00:15:48.800
And first we're gonna get again the ninety three percent.

278
00:15:48.830 --> 00:15:50.780
But here, let's see what we get.

279
00:15:50.870 --> 00:15:55.190
Well, of course we get a different accuracy than the one we had here.

280
00:15:55.400 --> 00:16:00.740
And that's because I told you, you know, we can be a bit lucky when evaluating the accuracy of our

281
00:16:00.740 --> 00:16:03.770
classification model on this specific test set.

282
00:16:03.800 --> 00:16:09.770
You know, this is a single test on which the accuracy can totally be indeed a bit higher than the average

283
00:16:09.830 --> 00:16:11.990
accuracy over 10 different test sets.

284
00:16:12.320 --> 00:16:17.330
And this is exactly the case here, because when we get 10 different test sets, well, we get that

285
00:16:17.480 --> 00:16:19.340
average accuracy of the 10.

286
00:16:19.400 --> 00:16:21.080
Accuracy is resulting from this test.

287
00:16:21.080 --> 00:16:24.680
Falls is indeed ninety point thirty three percent.

288
00:16:24.890 --> 00:16:26.270
And that makes much more sense.

289
00:16:26.390 --> 00:16:33.230
And this is actually the real or, you know, most relevant value of the accuracy for this specific

290
00:16:33.230 --> 00:16:36.930
model, the kernel SVM on this specific dataset.

291
00:16:37.670 --> 00:16:39.830
And let's see what the standard deviation is.

292
00:16:39.860 --> 00:16:42.470
So it is six point fifty seven percent.

293
00:16:42.860 --> 00:16:43.920
And that means only one thing.

294
00:16:43.940 --> 00:16:50.350
That means that more or less, while the 10 accuracy is resulting from our test falls fall more or less

295
00:16:50.360 --> 00:16:57.140
between ninety point thirty three minus six point fifty seven and ninety point thirty three plus six

296
00:16:57.140 --> 00:16:57.980
point fifty seven.

297
00:16:58.010 --> 00:17:04.400
In other words, they fall around between eighty three point five percent and ninety six point five

298
00:17:04.400 --> 00:17:04.910
percent.

299
00:17:05.240 --> 00:17:08.300
And so we have actually a pretty high standard deviation.

300
00:17:08.660 --> 00:17:12.920
And that's good to know because, you know, that's a very different situation than if we had, you

301
00:17:12.950 --> 00:17:16.190
know, accuracy's falling between 89 and 91.

302
00:17:16.280 --> 00:17:18.860
For example, you know, that would be a low standard deviation.

303
00:17:19.550 --> 00:17:20.490
All right, good.

304
00:17:20.580 --> 00:17:23.730
Now you have this Caple cross-validation technique in your tool kit.

305
00:17:23.840 --> 00:17:29.930
So you're now packed with the best and most relevant way to measure the accuracy of your classification

306
00:17:29.930 --> 00:17:30.320
model.

307
00:17:30.740 --> 00:17:35.940
And now we're gonna move on to the next section, because now that you have this most performance measure.

308
00:17:36.080 --> 00:17:42.470
Well, I would like you to also have the technique that can find optimal values of your hyper parameters

309
00:17:42.680 --> 00:17:43.310
in your model.

310
00:17:43.370 --> 00:17:45.860
You know, these parameters that are not treant.

311
00:17:46.070 --> 00:17:48.350
And I will teach you how to find the best values.

312
00:17:48.860 --> 00:17:51.770
So as soon as you're ready for this next section will join me.

313
00:17:51.860 --> 00:17:53.060
Let's do this together.

314
00:17:53.180 --> 00:17:55.000
And until then, enjoy machine learning.
