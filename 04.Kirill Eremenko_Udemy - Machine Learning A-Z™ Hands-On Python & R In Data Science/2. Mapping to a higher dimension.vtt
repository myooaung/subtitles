WEBVTT
1
00:00:00.490 --> 00:00:04.930
Hello and welcome back to the course of machine learning in today's tutorial we will find out how we

2
00:00:04.930 --> 00:00:11.590
can take our nonlinearly separable data set map it to our higher dimension and get a linearly separable

3
00:00:11.590 --> 00:00:12.360
data set.

4
00:00:12.430 --> 00:00:18.550
Invoke the support vector machine algorithm build a decision boundary for a dataset and then project

5
00:00:18.580 --> 00:00:21.440
all of that back into our original dimensions.

6
00:00:21.460 --> 00:00:22.600
So quite a lot to cover.

7
00:00:22.600 --> 00:00:24.790
Let's get started.

8
00:00:24.790 --> 00:00:29.740
First off we're going to look at a simplified example we're going to look at a one dimensional dataset.

9
00:00:29.740 --> 00:00:36.910
So normally we visualize everything in the PowerPoint with two dimensions to make it look pretty and

10
00:00:36.910 --> 00:00:40.710
so that we can kind of understand how it would work in multiple dimensions.

11
00:00:40.810 --> 00:00:44.470
But right now that will be a bit too complex for us to start with.

12
00:00:44.470 --> 00:00:52.240
So we are going to start with a single dimension So here we've got the X-1 dimension we've got some

13
00:00:52.240 --> 00:00:54.110
points here so we get nine data points.

14
00:00:54.310 --> 00:01:02.620
And as we can see our nonlinearity separable So in a in this dimensional in a single dimension dimensional

15
00:01:02.620 --> 00:01:06.520
space a linear separator would not be a line or would be a dot.

16
00:01:06.520 --> 00:01:06.790
Right.

17
00:01:06.790 --> 00:01:14.960
So in a two dimensional space or a linear linear separator is a line in a three dimensional space as

18
00:01:14.970 --> 00:01:20.290
a hyper plane but in a one dimensional space it's a single dot so can we simply separate the green from

19
00:01:20.290 --> 00:01:21.900
the red with a single dot here.

20
00:01:21.910 --> 00:01:27.400
No we cannot we if we put it here then these will be separate from that of who put it here will be separate

21
00:01:27.400 --> 00:01:27.790
from that.

22
00:01:27.790 --> 00:01:31.440
So this is an nonlinearity separable dataset.

23
00:01:31.480 --> 00:01:40.450
Now how can we apply the method of increasing the dimensionality of this space to make it a leaner separable

24
00:01:40.780 --> 00:01:43.600
data set in a higher dimension and that's what we're going to look at.

25
00:01:43.780 --> 00:01:46.170
And it might seem impossible right.

26
00:01:46.190 --> 00:01:52.630
So the first some I learn about this for me it was like wow how can you take a nonlinearity several

27
00:01:52.630 --> 00:01:59.060
dissidents somehow magically increase the dimensionality and you get a linearly separable dataset that

28
00:01:59.190 --> 00:02:01.520
you know sounded like absurd.

29
00:02:01.720 --> 00:02:05.470
But it is actually possible and that's what we're going to see right now.

30
00:02:05.470 --> 00:02:08.020
So we are going to create this mapping function on the fly.

31
00:02:08.020 --> 00:02:13.180
So let's say that at this point or here is around 5.

32
00:02:13.180 --> 00:02:17.050
So this is zero here and then somewhere here we've got five.

33
00:02:17.050 --> 00:02:17.710
It doesn't really matter.

34
00:02:17.710 --> 00:02:22.080
It can be any number but just for argument's sake let's say that this point over here is five.

35
00:02:22.240 --> 00:02:22.640
Right.

36
00:02:22.640 --> 00:02:23.380
And then it keeps going.

37
00:02:23.380 --> 00:02:28.690
So our first step to build the mapping function it can be multiple mapping functions that you can build.

38
00:02:28.690 --> 00:02:32.280
I'm just going to show you one that came to my mind.

39
00:02:32.380 --> 00:02:39.120
So the first step would be to go F equals X minus 5 so as to subtract 5 from our dataset.

40
00:02:39.160 --> 00:02:47.290
And that is going to what does that are going to do that is going to move everything to the left so

41
00:02:47.290 --> 00:02:52.660
basically now this is what the result looks like.

42
00:02:52.660 --> 00:02:58.270
So if you take 5 you subtract 5 from X you would get you know like these ones will go into negative.

43
00:02:58.270 --> 00:03:06.040
These ones will stay positive and then the next step would be to square all of that so f is now equals

44
00:03:06.040 --> 00:03:07.900
2 x minus 5 square.

45
00:03:07.900 --> 00:03:09.220
So how will that all look like.

46
00:03:09.220 --> 00:03:18.580
Well basically you'll have this squared function going through your chart and then all of these will

47
00:03:18.580 --> 00:03:20.760
be projected onto the function.

48
00:03:20.770 --> 00:03:21.280
There we go.

49
00:03:21.280 --> 00:03:22.470
So that's what it looks like.

50
00:03:22.470 --> 00:03:24.650
F equals X minus 5 squared.

51
00:03:24.850 --> 00:03:30.100
And now what we want to do is we just want to see that it is indeed linearly separable.

52
00:03:30.100 --> 00:03:30.940
So there we go.

53
00:03:31.000 --> 00:03:33.030
There's our linear separator.

54
00:03:33.160 --> 00:03:37.860
So in a two dimensional space I remember a linear separator is a straight line.

55
00:03:37.960 --> 00:03:45.760
And as you can see this dataset became linearly separable in this dimension I know it's surprising and

56
00:03:45.760 --> 00:03:50.980
even a bit shocking but indeed it is the case so you can see that we were able to take this line and

57
00:03:50.980 --> 00:03:58.180
separate all of the red elements although data set from the green elements and that's it.

58
00:03:58.360 --> 00:04:04.450
And then what we do next from here is we would project everything back onto our original space and we

59
00:04:04.450 --> 00:04:10.090
would know how to functionally separate the green from the red.

60
00:04:10.600 --> 00:04:15.320
And that is what happens when you map something to a higher dimension.

61
00:04:15.490 --> 00:04:23.880
So now knowing this example and seeing that it works in reality we can proceed to a higher dimensional

62
00:04:23.890 --> 00:04:26.680
you know to start with a two dimensional space so let's have a look.

63
00:04:26.890 --> 00:04:33.350
So there's our two dimensional space and basically you apply the same principle So here you can apply.

64
00:04:33.400 --> 00:04:40.510
You cannot invoke the support of a machine algorithm because it is not a non linearly Super Bowl.

65
00:04:40.830 --> 00:04:42.920
Data set in this space.

66
00:04:42.930 --> 00:04:49.090
But then you would apply some sort of mapping function like right now we won't go into detail what exactly

67
00:04:49.090 --> 00:04:51.410
mapping function it would be.

68
00:04:51.520 --> 00:04:54.230
And again there could be multiple different options and so on.

69
00:04:54.340 --> 00:05:00.460
But basically based on the previous example we now know that it's possible like we've seen empirical

70
00:05:00.460 --> 00:05:05.260
evidence that it is possible to do the same thing applies to two dimensional space moving into three

71
00:05:05.260 --> 00:05:11.440
dimensional space you'd map it into three dimensional space and then somehow it would become a linearly

72
00:05:11.440 --> 00:05:17.770
separable data set in this space and here we've got a new dimension which is Zed and in a three dimensional

73
00:05:17.770 --> 00:05:21.300
space the linear separator is no longer a line.

74
00:05:21.310 --> 00:05:22.450
It's a hyperplane.

75
00:05:22.600 --> 00:05:29.360
And so this hyperplane separates the two parts of our dataset in the way we want.

76
00:05:29.380 --> 00:05:35.110
So the support vector machine algorithm has helped us build this hyperplane and then basically So we've

77
00:05:35.110 --> 00:05:41.680
got this result then we just projected back into our two dimensional space and we've got this circle

78
00:05:41.710 --> 00:05:51.440
that encompasses our classes or surprise or classes and there we go we've got the non linear separator.

79
00:05:51.460 --> 00:06:00.220
So as you can see we can still even though we get to a bit of a more complex problem where we cannot

80
00:06:00.670 --> 00:06:04.820
directly apply the support vector machine algorithm as we used to.

81
00:06:04.900 --> 00:06:14.110
We can still go into a higher dimension and then apply the support X machine algorithm and like we won't

82
00:06:14.110 --> 00:06:19.100
go into detail if it's possible all the time and if this cases when it's not possible and so on when

83
00:06:19.110 --> 00:06:19.740
you do there.

84
00:06:19.810 --> 00:06:27.340
But the point is that there is a solution that you can explore the higher dimensions that this is not

85
00:06:27.340 --> 00:06:29.160
a dead end you can just do that.

86
00:06:29.170 --> 00:06:34.480
But there's a problem the problem with this algorithm and the problem is that mapping to a higher dimensional

87
00:06:34.480 --> 00:06:41.380
space can be highly compute intensive so it might require a lot of computation a lot of processing power

88
00:06:41.380 --> 00:06:50.440
and you know the larger your dataset the more of a problem this can cause and therefore this approach

89
00:06:50.500 --> 00:06:57.130
isn't the best because you can imagine like you have a dataset and then mapping it to a higher dimension

90
00:06:57.130 --> 00:07:03.850
performing all the calculations there and then coming back to your lower dimension that can even for

91
00:07:03.850 --> 00:07:10.330
a computer and on just like in our minds ask as humans but just like for a computer that can cause a

92
00:07:10.330 --> 00:07:18.800
lot of delays it can cause a lot of like processing backlog and issues.

93
00:07:18.820 --> 00:07:23.710
In that sense and we don't want that to happen and therefore we're going to explore something else we're

94
00:07:23.710 --> 00:07:32.320
going explore a different approach which is called In mathematics the kernel trick and that approach

95
00:07:32.320 --> 00:07:38.910
is going to help us perform very similar gets very similar results.

96
00:07:38.980 --> 00:07:45.340
But without having to go to a higher dimensional space and we're going to talk about that in the next

97
00:07:45.340 --> 00:07:48.080
tutorial is going to be exciting and I can't wait to see there.

98
00:07:48.100 --> 00:07:49.900
Until then happy analyzing
