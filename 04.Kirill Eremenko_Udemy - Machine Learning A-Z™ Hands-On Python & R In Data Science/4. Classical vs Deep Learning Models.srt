1
00:00:00,720 --> 00:00:03,900
Hello, welcome back to the course on deep natural language processing.

2
00:00:04,050 --> 00:00:08,010
And today we're talking about classical versus deep learning models.

3
00:00:08,520 --> 00:00:09,240
So let's have a look.

4
00:00:10,230 --> 00:00:11,840
As we discuss, we've got this diagram.

5
00:00:12,030 --> 00:00:16,050
And on the left, you've got the natural language processing models overall.

6
00:00:16,680 --> 00:00:23,370
And on the right, we have the deep learning models or also there overlap is the deep natural language

7
00:00:23,370 --> 00:00:23,960
processing.

8
00:00:23,970 --> 00:00:28,710
Those are the very advanced types of natural language processing models which use deep learning.

9
00:00:29,130 --> 00:00:32,850
And then finally at the bottom here, we've got the sequence to sequence models, which will be interested

10
00:00:32,850 --> 00:00:33,450
in that.

11
00:00:34,030 --> 00:00:40,860
And so I thought that this tutorial would be fun if we had a look at a comparison between a couple.

12
00:00:40,890 --> 00:00:43,260
So we'll look at a couple of examples from here.

13
00:00:44,130 --> 00:00:45,600
We'll look at an example from here.

14
00:00:45,600 --> 00:00:52,260
And then finally, we'll mention the sequence to sequence just to kind of lay out the playing field

15
00:00:52,290 --> 00:00:57,720
so we know what we're dealing with and what types exist, because we could definitely just jump into

16
00:00:57,750 --> 00:01:00,630
sequence to sequence and look at the most powerful things.

17
00:01:00,930 --> 00:01:05,930
But then it would still be a mystery what our national language processing models, how they're different

18
00:01:05,930 --> 00:01:09,640
to deep natural language processing models and how they're different to sequence, to sequence.

19
00:01:09,720 --> 00:01:11,640
What their applications and so on.

20
00:01:12,120 --> 00:01:14,790
So let's have a look at a couple of examples.

21
00:01:15,660 --> 00:01:16,170
All right.

22
00:01:16,800 --> 00:01:17,820
Some examples.

23
00:01:17,820 --> 00:01:19,770
We're going to have our diagram on the right.

24
00:01:19,800 --> 00:01:22,080
And we're gonna start going through these examples one by one.

25
00:01:22,560 --> 00:01:25,680
So number one is the if else rules.

26
00:01:26,220 --> 00:01:32,670
And this is a way that we used to create Chad bots back in the day.

27
00:01:32,700 --> 00:01:38,780
So if El's roles are looking over here on our diagram in the just the MLP part.

28
00:01:39,920 --> 00:01:48,020
And what they entail is a huge list of possible questions and answers to those questions.

29
00:01:48,050 --> 00:01:55,250
And so once somebody in the chat asks a question or we can identify that part of the sentence is the

30
00:01:55,250 --> 00:02:01,610
question that we have recorded, then we will give them the correct answer, the answer that is associated

31
00:02:01,610 --> 00:02:02,330
with that question.

32
00:02:02,810 --> 00:02:11,600
But as you can imagine, such mechanics, such a mechanical approach to answering questions or chatting

33
00:02:11,600 --> 00:02:17,750
with people does not result in anything human like anything realistic.

34
00:02:18,110 --> 00:02:23,690
And it results in something like this, rather, where you have questions and then you have the predetermined

35
00:02:23,720 --> 00:02:24,240
answers.

36
00:02:24,290 --> 00:02:28,010
But people wants something tailored to them, something specific.

37
00:02:28,010 --> 00:02:32,870
They asking about something else but doesn't fit into that list of questions and answers.

38
00:02:32,870 --> 00:02:35,720
And it just very quickly becomes a mess.

39
00:02:36,230 --> 00:02:45,860
But that's how we used to create Chad, the next type or the next natural language processing model.

40
00:02:45,890 --> 00:02:53,990
We will look at is called an audio frequency components analysis, and it's used for speech recognition

41
00:02:54,890 --> 00:02:55,730
and sits over here.

42
00:02:55,760 --> 00:03:02,010
So we're specifically looking like we're purposefully looking at different examples, not just childbirths.

43
00:03:02,240 --> 00:03:04,030
We're looking at different applications.

44
00:03:04,070 --> 00:03:07,460
So you can see that an LP can be applied to speech recognition.

45
00:03:07,850 --> 00:03:15,320
And there used to be or still are algorithms that use non deep learning methods for speech recognition.

46
00:03:15,470 --> 00:03:18,740
And one of them is audio frequency components analysis.

47
00:03:19,070 --> 00:03:26,300
So in essence, what happens is we look at the sound wave of somebody talking some prerecorded or Real-Time

48
00:03:26,690 --> 00:03:29,210
audio over human speech.

49
00:03:29,780 --> 00:03:34,310
And then we try to identify what wave forms exist in there.

50
00:03:34,610 --> 00:03:43,070
So in a very kind of simple way of explaining it, let's have a look at, for example, here.

51
00:03:43,070 --> 00:03:48,830
We've got a four year transformation and we've got there's our frequency over there says like one of

52
00:03:48,830 --> 00:03:50,630
these is going over there.

53
00:03:51,680 --> 00:03:55,740
Of course, this is this doesn't even look like human speech, to be frank.

54
00:03:55,790 --> 00:03:57,660
But the concept stay.

55
00:03:57,700 --> 00:04:02,360
So, like, we look at what frequencies it can, it consists of.

56
00:04:02,690 --> 00:04:09,010
And then we compare them to like the pre recorded frequencies.

57
00:04:09,050 --> 00:04:14,900
So we know that, for instance, certain certain combinations of frequencies means this type of word

58
00:04:14,930 --> 00:04:15,860
or this type of word.

59
00:04:16,130 --> 00:04:17,300
And it's just about the frequencies.

60
00:04:17,300 --> 00:04:19,610
Of course, it's it's much more complex than that.

61
00:04:19,640 --> 00:04:23,340
But this is a very general overreach of what happens.

62
00:04:23,360 --> 00:04:26,540
We look at the frequencies, certain mathematical operations.

63
00:04:26,570 --> 00:04:32,070
This is the key point that we're not doing any neural computations.

64
00:04:32,090 --> 00:04:33,790
We're not creating new neural networks.

65
00:04:33,800 --> 00:04:39,470
We're just doing mathematical calculations around the frequencies that we can observe.

66
00:04:39,800 --> 00:04:46,760
Comparing them to the mathematical calculations we have in a library of pre analyzed frequencies.

67
00:04:47,090 --> 00:04:48,320
And then we're matching it up.

68
00:04:48,350 --> 00:04:54,050
We're finding what word the person is saying, what question they're asking or what the sentence is

69
00:04:54,050 --> 00:04:54,530
meaning.

70
00:04:54,980 --> 00:04:57,770
And then that is how we recognize speech.

71
00:04:58,640 --> 00:05:03,650
So there's another version of natural language processing, which is in the green area.

72
00:05:04,730 --> 00:05:12,590
Next one is the back of words model, which is used for classification, a very, very popular approach

73
00:05:12,650 --> 00:05:16,850
for text analysis or natural language processing.

74
00:05:17,300 --> 00:05:26,440
And what it does is basically it also sits in the MLP part and just the green area, although as we

75
00:05:26,440 --> 00:05:31,310
will see for the on, it can sit both in the green area and in the purple area over here.

76
00:05:31,700 --> 00:05:34,580
But that's something for discussion.

77
00:05:34,730 --> 00:05:37,900
What's the bag of words model does is there's a bag of words.

78
00:05:38,750 --> 00:05:42,440
And the way it works is a you might have like lots of text.

79
00:05:42,440 --> 00:05:53,520
For instance, here we've got tutors or lecturers or examiners who marked grades.

80
00:05:53,630 --> 00:06:01,640
So who gave a pass or fail grade to different papers or different assignments that were submitted.

81
00:06:01,760 --> 00:06:03,430
And then they also left the comments.

82
00:06:03,440 --> 00:06:08,000
So somebody said great job and left a comment on one meaning a pass.

83
00:06:08,020 --> 00:06:10,280
Somebody said amazing work left to come out of one.

84
00:06:10,430 --> 00:06:11,240
Well done, one.

85
00:06:11,250 --> 00:06:12,410
Very well written one.

86
00:06:12,440 --> 00:06:14,360
And then poor effort zero could have done better.

87
00:06:14,370 --> 00:06:15,590
Zero, try harder next time.

88
00:06:15,590 --> 00:06:16,280
Zero and so on.

89
00:06:16,790 --> 00:06:24,210
And so from this, what the bag awards model will do is it will put all these words into a bag and it

90
00:06:24,210 --> 00:06:24,800
will remember.

91
00:06:24,830 --> 00:06:28,430
So how often does the word grades come up with a one?

92
00:06:28,760 --> 00:06:30,770
And how often does the word grade come up with a zero?

93
00:06:30,860 --> 00:06:35,780
This is, again, a very general explanation, a very high level explanation of what happens.

94
00:06:36,140 --> 00:06:39,140
But in essence, this is the concept that.

95
00:06:39,520 --> 00:06:41,560
We'll look at the words and it'll look.

96
00:06:41,810 --> 00:06:50,630
Try to classify these words ill timed, Ill try to associate these words with either positive results

97
00:06:50,630 --> 00:06:52,160
or negative result in our case.

98
00:06:52,490 --> 00:06:56,120
And so in this case, like amazing would be most likely associated of positive.

99
00:06:56,120 --> 00:07:02,060
You don't often see amazing when you're trying to say when when the sentiment is negative, when you

100
00:07:02,060 --> 00:07:08,030
try it, when the person was trying to say something like the work wasn't well done or well would be

101
00:07:08,030 --> 00:07:11,480
associated with one, like the word work could be associated with both.

102
00:07:11,900 --> 00:07:18,020
But then these other words, they would be mostly associated with past some like words like poor or

103
00:07:18,320 --> 00:07:23,110
harder or could have maybe you would be associate zeros.

104
00:07:23,480 --> 00:07:27,260
And so then it will remember that we'll keep these words in a bag and next time something comes up.

105
00:07:28,040 --> 00:07:34,070
For instance, somebody says, good, a good job.

106
00:07:34,760 --> 00:07:41,450
Keep keep it up or something, that it will analyze the words that are in that new sentence by pulling

107
00:07:41,450 --> 00:07:46,220
them out of the bag and looking at them and understanding are they mostly associated ones or zeros.

108
00:07:46,610 --> 00:07:53,660
And then we'll be able to predict it will be able to classify the new new comment even without knowing

109
00:07:53,690 --> 00:07:54,710
what the mark was.

110
00:07:54,710 --> 00:07:59,060
Pass or fail will be able to say based on the communal tables, say, is it a pass or fail?

111
00:07:59,960 --> 00:08:03,680
So that's like a very simplistic application of the bag of words model.

112
00:08:03,890 --> 00:08:06,110
But there's another one and OPIS model.

113
00:08:08,090 --> 00:08:08,430
OK.

114
00:08:08,480 --> 00:08:11,240
So let's look at a deep national language processing model.

115
00:08:12,110 --> 00:08:14,680
And this one is going to be over here.

116
00:08:15,110 --> 00:08:18,980
It's called convolutional neural networks for text recognition.

117
00:08:19,340 --> 00:08:25,730
So they take the model we're going to be looking at further down is indeed a deep national language

118
00:08:25,730 --> 00:08:26,300
processing model.

119
00:08:26,480 --> 00:08:29,900
But I was I wanted to give an example of a different model.

120
00:08:29,930 --> 00:08:34,730
So not not just the one that we're looking at, but like also a deep natural language processing model,

121
00:08:34,760 --> 00:08:35,480
but a different one.

122
00:08:35,840 --> 00:08:42,380
And when I came across is when we use convolutional neural networks for text recognition.

123
00:08:42,440 --> 00:08:44,000
And then for the classification.

124
00:08:44,900 --> 00:08:51,200
And so if if you're familiar with conventional networks or even if you're not, what they're used for

125
00:08:51,200 --> 00:08:56,200
is it's a neural network that is used for mostly image recognition for like videos.

126
00:08:57,680 --> 00:09:01,440
Self-driving cars using to detect obstacles and roads, people and so on.

127
00:09:01,460 --> 00:09:04,850
So mostly it's used for image processing or video processing.

128
00:09:05,490 --> 00:09:11,060
And so it is very interesting to see how convolutional neural networks can actually be useful text processing.

129
00:09:11,320 --> 00:09:12,350
And the way it works is.

130
00:09:14,540 --> 00:09:23,780
It's these words are transformed into a matrix, and that's done through an operation called embedding

131
00:09:23,780 --> 00:09:24,290
of words.

132
00:09:24,740 --> 00:09:31,910
And then once they're in a matrix, the same principles as kind of evolution as we would apply for images

133
00:09:31,910 --> 00:09:33,770
and convolutional neural networks are applied.

134
00:09:33,800 --> 00:09:36,500
So there's a convolution operation going through these images.

135
00:09:37,190 --> 00:09:41,210
Then they're they're pooled.

136
00:09:41,270 --> 00:09:43,260
Max, pull them in, pull their some pull.

137
00:09:45,020 --> 00:09:46,590
And then they're flattened.

138
00:09:47,450 --> 00:09:49,460
And then, you know, then we have the prediction.

139
00:09:50,280 --> 00:09:56,300
So we won't go into detail right now of how convolution convolutional operations work.

140
00:09:56,480 --> 00:09:59,950
There's lots of tutorials out there and so on.

141
00:09:59,960 --> 00:10:01,760
And it's just not in the scope of this.

142
00:10:01,960 --> 00:10:06,470
Because in this case, we'll be looking at mostly recurrent neural networks, which is Ahran in a different

143
00:10:06,470 --> 00:10:06,970
type of know.

144
00:10:07,670 --> 00:10:13,550
But I just wanted to put it out there that you could technically use convolutional neural networks for

145
00:10:13,550 --> 00:10:18,200
text recognition, just as you would do with images and very interesting concept.

146
00:10:18,210 --> 00:10:21,310
That was one of the earlier ones that was explored.

147
00:10:21,320 --> 00:10:26,510
And then we moved on to recurrent neural networks, as we'll see down the track.

148
00:10:27,350 --> 00:10:27,880
So there we go.

149
00:10:27,890 --> 00:10:28,730
That's another one.

150
00:10:29,060 --> 00:10:34,640
And then finally, the main model that we'll be looking for working with sequence to sequence has many,

151
00:10:34,640 --> 00:10:37,310
many, many different applications, as we'll see down.

152
00:10:37,850 --> 00:10:39,260
And that's the one that's over there.

153
00:10:39,410 --> 00:10:42,380
So I'll give you a short preview of what's coming up.

154
00:10:42,500 --> 00:10:43,630
Looks something like this.

155
00:10:43,640 --> 00:10:44,770
And don't worry.

156
00:10:44,780 --> 00:10:53,030
It might look very complex right now, but in a couple of tutorials from here, we'll be very, very

157
00:10:53,030 --> 00:10:58,460
comfortable with what's going on here and how this whole construct is working in what's called sequence

158
00:10:58,460 --> 00:10:59,090
to sequence.

159
00:10:59,120 --> 00:11:01,790
And what exactly it allows us to do.

160
00:11:02,510 --> 00:11:03,050
So there we go.

161
00:11:03,080 --> 00:11:12,290
I hope you enjoy this quick overview and comparison of the different types of models and different applications

162
00:11:12,290 --> 00:11:12,920
that they have.

163
00:11:13,400 --> 00:11:14,460
And we'll continue next.

164
00:11:14,460 --> 00:11:16,070
Then I'll look forward to seeing you there.

165
00:11:16,090 --> 00:11:19,270
And until then, enjoy Deep and OPIS.
