WEBVTT
1
00:00:00.270 --> 00:00:00.840
OK, good.

2
00:00:00.870 --> 00:00:02.890
So now we're going to test that out, of course.

3
00:00:03.030 --> 00:00:07.360
This whole implementation is over because all the rest comes from our different templates.

4
00:00:07.360 --> 00:00:12.510
That data we Bressington play, the data preprocessing took it and the logistic regression implementation,

5
00:00:12.510 --> 00:00:13.290
you know, for the rest.

6
00:00:13.710 --> 00:00:17.970
So now we can just run the whole code and see what happens.

7
00:00:18.120 --> 00:00:18.520
India.

8
00:00:18.750 --> 00:00:21.360
So basically we are ready to run this code.

9
00:00:21.500 --> 00:00:25.890
But before let's not forget to upload the data set inside this notebook.

10
00:00:26.250 --> 00:00:30.240
So let's click this upload button, then please find your machine learning.

11
00:00:30.340 --> 00:00:33.190
Is it for there on your machine or wherever you downloaded it?

12
00:00:33.570 --> 00:00:34.740
And then let's go inside.

13
00:00:34.890 --> 00:00:40.820
Part nine dimensionality reduction, then principal component analysis in Python.

14
00:00:41.180 --> 00:00:41.620
There we go.

15
00:00:41.680 --> 00:00:43.680
Let's take our wine dataset.

16
00:00:44.130 --> 00:00:45.660
Click open and click.

17
00:00:45.660 --> 00:00:51.960
OK, now, now you're ready to run this implementation with a simple run.

18
00:00:52.010 --> 00:00:54.360
Oh, so let's click runtime here.

19
00:00:54.540 --> 00:00:55.410
And are you ready?

20
00:00:55.860 --> 00:00:56.430
Let's do this.

21
00:00:56.430 --> 00:00:58.290
And three, two, one.

22
00:00:58.710 --> 00:00:59.400
Go run.

23
00:00:59.460 --> 00:00:59.670
Oh.

24
00:01:00.120 --> 00:01:00.430
All right.

25
00:01:00.510 --> 00:01:05.370
Pouring the libraries and importing the data set or playing the data repricing phase.

26
00:01:05.610 --> 00:01:06.550
Then applying pressure.

27
00:01:06.600 --> 00:01:08.160
Well, it goes way faster than me.

28
00:01:08.580 --> 00:01:10.260
And why do we end up with.

29
00:01:10.350 --> 00:01:15.870
Well, we actually end up with an amazing accuracy of 97 percent.

30
00:01:16.320 --> 00:01:22.500
So this business owner, you know, of this wine shop definitely had a great intuition with this idea

31
00:01:22.500 --> 00:01:27.780
of applying dimensionality reduction because, you know, dimensionality reduction doesn't only consist

32
00:01:27.840 --> 00:01:29.780
of reducing the complexity of your dataset.

33
00:01:30.030 --> 00:01:31.860
It can also improve the final results.

34
00:01:31.890 --> 00:01:36.090
You know, by combining dimensionality reduction and your predictive model.

35
00:01:36.390 --> 00:01:38.160
And here this is exactly what happened.

36
00:01:38.190 --> 00:01:44.950
We get an amazing accuracy of 97 percent with actually only one incorrect prediction right in here.

37
00:01:44.970 --> 00:01:46.320
By the way, this is interesting.

38
00:01:46.350 --> 00:01:50.280
This is the first time you see a confusion matrix of three rows and three columns.

39
00:01:50.460 --> 00:01:51.250
And that's the first sign.

40
00:01:51.300 --> 00:01:53.030
We have three classes, right?

41
00:01:53.100 --> 00:01:55.650
We have three customer segments, one, two and three.

42
00:01:55.990 --> 00:01:57.630
Therefore, we have three classes to predict.

43
00:01:57.960 --> 00:02:01.830
And so this is the number of correct predictions of customer segment one.

44
00:02:02.100 --> 00:02:04.980
This is number of correct predictions of customer segment two.

45
00:02:05.220 --> 00:02:07.860
This is the number of correct positions of customer segment three.

46
00:02:08.040 --> 00:02:12.150
And this is the number of incorrect predictions of customer segment one.

47
00:02:12.450 --> 00:02:12.690
Right.

48
00:02:12.720 --> 00:02:18.510
So anyway, on the one incorrect prediction interval and that's why we end up with such a great accuracy.

49
00:02:18.930 --> 00:02:24.450
And so now we should see amazing results on the graphs, you know, for us with visualizing the training

50
00:02:24.450 --> 00:02:25.200
set result.

51
00:02:25.560 --> 00:02:26.460
And indeed.

52
00:02:26.670 --> 00:02:29.520
Well, you know, these are two principal components.

53
00:02:29.790 --> 00:02:33.780
P.S. one on the X axis and P.S. two on the Y axis.

54
00:02:34.290 --> 00:02:37.080
And then these are, you know, the different prediction regions.

55
00:02:37.350 --> 00:02:43.920
This is the prediction region of class number three, where each one of which to extract it, coordinates

56
00:02:43.920 --> 00:02:49.680
of P.c one, NPC two are in this blue region will be predicted to belong to customer segment number

57
00:02:49.680 --> 00:02:49.980
three.

58
00:02:50.430 --> 00:02:56.220
Then this is the prediction region of customer segment number two, where all the wines for which the

59
00:02:56.430 --> 00:03:02.460
piece you want and p.c to extracted features that fall in this green region will be predicted to belong

60
00:03:02.460 --> 00:03:03.810
to customer single number two.

61
00:03:04.170 --> 00:03:10.740
And finally, this prediction region Red corresponds to customer segment number one inside which all

62
00:03:10.740 --> 00:03:16.950
the wines of which the extracted feature is you one and pieces to fall in this region will be predicted

63
00:03:16.950 --> 00:03:19.290
to belong to customer segment number one.

64
00:03:19.770 --> 00:03:20.040
All right.

65
00:03:20.070 --> 00:03:24.240
And then, of course, all the dots here, you know, the green dots here, the blue dots here and the

66
00:03:24.360 --> 00:03:30.960
red dots here are the real observations, you know, the real wines themselves of the training set.

67
00:03:31.170 --> 00:03:34.110
And so here we can indeed see that we have a few incorrect predictions.

68
00:03:34.140 --> 00:03:35.910
But that's only on the training set.

69
00:03:36.210 --> 00:03:40.380
For example, this is a green wine, meaning a wine that belongs to a customer.

70
00:03:40.390 --> 00:03:45.840
Segment number two, which was predicted by the model to belong to customer segment number three.

71
00:03:46.230 --> 00:03:51.930
And these are two other incorrect predictions of wines that belong in reality to customer segment number

72
00:03:51.930 --> 00:03:56.040
two, but were predicted by the model to belong to customer segment number one.

73
00:03:56.550 --> 00:03:56.860
All right.

74
00:03:56.910 --> 00:03:58.500
And then another incorrect predictions here.

75
00:03:58.550 --> 00:04:03.300
But to look at the incorrect predictions, it's more interesting to check them out on the test set.

76
00:04:03.690 --> 00:04:07.620
And here is a test set and indeed even on new observations.

77
00:04:07.770 --> 00:04:13.460
Well, our logistic regression model combined to dimensionality reduction was perfectly able to separate.

78
00:04:13.470 --> 00:04:19.110
Well, the three classes and we can see very well that, you know, incorrect prediction that we saw

79
00:04:19.110 --> 00:04:25.690
here in the computed matrix, which is, you know, right here, that's the incorrect prediction and

80
00:04:25.710 --> 00:04:31.170
corresponds to a green wine, meaning a wine that in reality belongs to customer segment number two,

81
00:04:31.470 --> 00:04:35.190
but was predicted by the model to belong to customer segment number one.

82
00:04:35.580 --> 00:04:36.300
But it's OK.

83
00:04:36.360 --> 00:04:41.580
You know, any business order or data scientist that ends up with such results with only one incorrect

84
00:04:41.580 --> 00:04:43.590
prediction can just be super happy.

85
00:04:44.130 --> 00:04:50.460
But let's see if we can beat that with our other dimensionality, reduction techniques like linear discriminant,

86
00:04:50.490 --> 00:04:51.180
analysis.

87
00:04:51.600 --> 00:04:52.860
And, you know, to beat that.

88
00:04:52.890 --> 00:04:56.340
Well, we'll have to simply get a 100 percent accuracy.

89
00:04:56.730 --> 00:04:59.820
So we'll see if you know the linear discriminant.

90
00:04:59.930 --> 00:05:02.750
Which are the extracted features of the LDA technique?

91
00:05:03.080 --> 00:05:06.420
Can build, you know, a prediction boundary that can separate.

92
00:05:06.460 --> 00:05:08.390
Well, all these three classes.

93
00:05:08.690 --> 00:05:10.070
So this will be quite challenging.

94
00:05:10.130 --> 00:05:11.570
But this is doable.

95
00:05:11.690 --> 00:05:17.510
And then we'll check if we can also do the same with Colonel PTA, which is our final dimensionality

96
00:05:17.510 --> 00:05:18.410
reduction technique.

97
00:05:19.100 --> 00:05:19.520
All right.

98
00:05:19.610 --> 00:05:22.040
So as soon as you're ready for the next technique.

99
00:05:22.190 --> 00:05:27.350
Well, I'll be super happy to join you in the next practical activity to implement LDA.

100
00:05:27.740 --> 00:05:29.750
And until then, enjoy machine learning.
