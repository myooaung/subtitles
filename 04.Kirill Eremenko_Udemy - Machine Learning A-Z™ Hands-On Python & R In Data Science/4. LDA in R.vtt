WEBVTT
1
00:00:00.300 --> 00:00:02.670
Hello and welcome to this art tutorial.

2
00:00:02.670 --> 00:00:09.480
So in the previous section this PCA feature extraction technique reduced the dimensionality of our problem

3
00:00:09.480 --> 00:00:13.500
by extracting the variables that explained the most the variance.

4
00:00:13.640 --> 00:00:15.720
And now in LDH this is quite different.

5
00:00:15.750 --> 00:00:22.020
We are extracting some new independent variables that will separate the most the classes of the dependent

6
00:00:22.020 --> 00:00:22.700
variable.

7
00:00:22.980 --> 00:00:27.810
And therefore since this time it considers the classes of the dependent variable.

8
00:00:27.960 --> 00:00:33.090
Well that means that it considers the dependent variable to proceed to this feature extraction technique

9
00:00:33.300 --> 00:00:38.760
and therefore that makes LDA a supervised dimensionality reduction model.

10
00:00:38.760 --> 00:00:41.780
All right so now let's apply LDA on our.

11
00:00:42.050 --> 00:00:45.190
So first Very quickly let's add the rifle there as we can directory's.

12
00:00:45.210 --> 00:00:50.700
We'll get to our mission in it is that pointer point nine dimensional reduction and we are now in the

13
00:00:50.700 --> 00:00:54.130
section 44 linear discriminant analysis.

14
00:00:54.150 --> 00:00:56.130
So let's go inside and that's to follow.

15
00:00:56.130 --> 00:00:58.030
You want to set as working directory.

16
00:00:58.060 --> 00:01:00.340
We're still working on the one that says the file.

17
00:01:00.340 --> 00:01:05.840
So that's exactly the same business problem as the one we worked with when implementing PCA.

18
00:01:05.880 --> 00:01:11.610
And so that will be a good opportunity to compare this dimensionality reduction technique LDA to the

19
00:01:11.610 --> 00:01:13.260
previous one PCA.

20
00:01:13.620 --> 00:01:18.390
Now let's not forget to click on this more button here and set as a working directory.

21
00:01:18.480 --> 00:01:19.100
Perfect.

22
00:01:19.200 --> 00:01:22.770
And so since we're working on the same business problem as before.

23
00:01:22.770 --> 00:01:27.210
As for PCa well implementing LDA here is going to be very easy.

24
00:01:27.330 --> 00:01:35.750
We will just take the PCA code take everything from here down to the bottom copy and we'll go back to

25
00:01:35.760 --> 00:01:43.200
LGA and paste the whole thing here and inside of this code we will just need basically to replace this

26
00:01:43.380 --> 00:01:48.490
applying PCA section by a new section dedicated to applying LDA.

27
00:01:48.510 --> 00:01:51.650
So I'm going to remove all this section.

28
00:01:51.780 --> 00:02:00.490
So let's remove this and let's replace here PCA by LDA and now it's time to implement LDA.

29
00:02:00.690 --> 00:02:07.230
So the package that we're going to use to apply LDA is the mass package and a s s and it's actually

30
00:02:07.230 --> 00:02:10.950
a package that is by default in your list of packages.

31
00:02:11.040 --> 00:02:17.550
It's the package right here mass support functions and data sets for enables and replacements.

32
00:02:17.550 --> 00:02:17.820
All right.

33
00:02:17.820 --> 00:02:20.080
So as you can see this is not important.

34
00:02:20.130 --> 00:02:23.700
So let's imported by using the library command.

35
00:02:23.700 --> 00:02:28.050
Here we go and mass this way in parenthesis.

36
00:02:28.050 --> 00:02:28.710
All right.

37
00:02:28.710 --> 00:02:33.180
We can already select this to import the package.

38
00:02:33.180 --> 00:02:35.420
And now let's implement LDA.

39
00:02:35.550 --> 00:02:42.420
So the first thing that we have to do is ask for BCA create an LDA variable that will use to transform

40
00:02:42.480 --> 00:02:47.700
our original dataset into this new data set composed of the Lynnie or discriminants.

41
00:02:47.700 --> 00:02:51.240
And so we're going to call this variable LDA equals.

42
00:02:51.510 --> 00:02:55.200
And now we're going to use the LDA function as simple as that.

43
00:02:55.320 --> 00:02:57.690
And let's add some parenthesis.

44
00:02:57.690 --> 00:03:01.680
And now let's see the arguments by pressing f one.

45
00:03:01.680 --> 00:03:03.480
All right so the arguments are here.

46
00:03:03.480 --> 00:03:05.750
The first argument is formula.

47
00:03:05.940 --> 00:03:12.050
So that's exactly the formula of your dependent variable with respect to the independent variables.

48
00:03:12.060 --> 00:03:13.950
So far the original ones.

49
00:03:14.010 --> 00:03:16.850
So here as a first argument we will input.

50
00:03:16.920 --> 00:03:20.370
Formula equals customer segment.

51
00:03:21.690 --> 00:03:26.190
Remember that's the name of the dependent variable and filled up.

52
00:03:26.310 --> 00:03:31.890
And we can add a dot dot where we we don't have to write all the names of the independent variables.

53
00:03:31.890 --> 00:03:33.740
The dot is here for us.

54
00:03:33.780 --> 00:03:38.770
So come up and then next argument and then the next argument is data.

55
00:03:38.970 --> 00:03:43.730
And as for Pca the data here is going to be the training set.

56
00:03:43.740 --> 00:03:44.760
So let's add here.

57
00:03:44.820 --> 00:03:46.720
Training training set.

58
00:03:46.740 --> 00:03:47.570
Here we go.

59
00:03:47.940 --> 00:03:48.530
All right.

60
00:03:48.690 --> 00:03:54.630
And actually that's all and that's for a specific reason a very important reason that is directly related

61
00:03:54.840 --> 00:03:55.770
to LDA.

62
00:03:56.040 --> 00:04:00.990
It's due to the fact that LDA is a supervised dimensionality reduction technique.

63
00:04:01.140 --> 00:04:06.900
Remember supervised means that the LDA model takes into account the dependent variable and since it

64
00:04:06.900 --> 00:04:08.870
takes into account the dependent variable.

65
00:04:09.000 --> 00:04:15.540
Well it's quite intuitive to understand that the number of Lynnie or discriminants will be related to

66
00:04:15.540 --> 00:04:20.610
the information of the depende viable and that information is actually the number of classes in the

67
00:04:20.610 --> 00:04:21.700
dependent variable.

68
00:04:21.810 --> 00:04:27.510
And there is this explicit correlation between the number of linear or discriminants and the information

69
00:04:27.510 --> 00:04:28.870
of the dependent variable.

70
00:04:28.880 --> 00:04:35.200
It's that there will be k minus one linear discriminants where k is the number of classes.

71
00:04:35.220 --> 00:04:40.960
So here since we have three classes that means that we'll get at most three minus one equals two linear

72
00:04:40.960 --> 00:04:42.000
or discriminants.

73
00:04:42.150 --> 00:04:48.690
So here without specifying the number of linear discriminants equal to 2 we will get automatically to

74
00:04:48.740 --> 00:04:51.710
linear or discriminants and therefore that's all.

75
00:04:51.710 --> 00:04:54.160
We don't need to add any other arguments.

76
00:04:54.420 --> 00:05:02.640
So the LDA object is ready to be used to transform our original dataset into this new one composed of

77
00:05:02.690 --> 00:05:03.910
the discriminants.

78
00:05:04.110 --> 00:05:11.160
We will get to our discriminants which is exactly what we want asked for PCa we want to do extracted

79
00:05:11.160 --> 00:05:16.530
features and this time these two new extracted features will separate the most two classes.

80
00:05:16.620 --> 00:05:19.560
So we should get very good results as well.

81
00:05:19.560 --> 00:05:22.640
And then that's the same as for PCa.

82
00:05:22.800 --> 00:05:28.830
We need to transform the training set and the test set so that we can then use them in the next sections

83
00:05:28.950 --> 00:05:35.430
so we'll use the training set to fit SVM Well actually to the training set to build the classifier and

84
00:05:35.430 --> 00:05:40.920
then we will make our predictions predicting the test results with this test set that will be transformed

85
00:05:40.920 --> 00:05:46.740
as well and make the conclusion matrix and most importantly we will visualize the training set and the

86
00:05:46.740 --> 00:05:52.200
test results something that we'll be able to do because we now have two features.

87
00:05:52.200 --> 00:05:52.530
All right.

88
00:05:52.530 --> 00:05:53.360
So let's do it.

89
00:05:53.400 --> 00:05:57.690
Let's do the same to apply LDA on the training set and the test.

90
00:05:57.690 --> 00:06:00.930
So first let's start with a training set training set.

91
00:06:01.200 --> 00:06:05.730
So remember we're keeping them as a training set so that we don't have to change it in the rest of the

92
00:06:05.730 --> 00:06:06.490
sections.

93
00:06:06.540 --> 00:06:08.540
So trainset equals.

94
00:06:08.880 --> 00:06:14.340
And then remember we need to use to predict function that's actually exactly the same as for PCa.

95
00:06:14.340 --> 00:06:18.210
We will however need to add something to make it work.

96
00:06:18.270 --> 00:06:23.820
And I will explain what it is but definitely we're doing this transformation with the pretty function.

97
00:06:23.970 --> 00:06:29.160
So parenthesis now inside this pretty function we need to specify.

98
00:06:29.160 --> 00:06:31.510
You know the first argument here which is the object.

99
00:06:31.650 --> 00:06:38.070
So the object is LPA and then come up and then the second argument and the second argument is the dataset

100
00:06:38.160 --> 00:06:42.200
on which we want to make the transformation that is extract the new features.

101
00:06:42.300 --> 00:06:43.750
And that's the trainset.

102
00:06:43.860 --> 00:06:44.700
Here it is.

103
00:06:44.700 --> 00:06:50.430
So as a reminder this is the original dataset composed of 13 independent variables and this will be

104
00:06:50.430 --> 00:06:57.550
the new training sets composed of the two extracted features that are the two Lindhardt discriminants.

105
00:06:57.570 --> 00:06:58.150
All right.

106
00:06:58.200 --> 00:07:03.660
So we can already do that to see if we get the right training set as we expect.

107
00:07:03.690 --> 00:07:09.900
So before executing this line of course we need to execute the previous sections because we need to

108
00:07:09.900 --> 00:07:12.690
import the data set and apply data preprocessing.

109
00:07:12.780 --> 00:07:13.890
So let's do it.

110
00:07:13.890 --> 00:07:15.480
We don't have anything to change here.

111
00:07:15.480 --> 00:07:17.260
Everything is already well-prepared.

112
00:07:17.340 --> 00:07:20.560
Thanks to what we did in the preview section with PCa.

113
00:07:20.670 --> 00:07:22.170
So let's execute this.

114
00:07:22.170 --> 00:07:22.860
Here we go.

115
00:07:22.920 --> 00:07:23.960
Well executed.

116
00:07:24.210 --> 00:07:32.530
And now let's apply LDH create the LDA object and then use this object to transform our original training

117
00:07:32.540 --> 00:07:36.760
set into this new training set composed of the two Linnea our discriminants.

118
00:07:36.810 --> 00:07:39.380
So we already imported the mass package.

119
00:07:39.390 --> 00:07:42.190
So we just need to execute this line of code.

120
00:07:42.240 --> 00:07:43.280
Let's do it.

121
00:07:43.350 --> 00:07:44.060
Here we go.

122
00:07:44.070 --> 00:07:45.800
LDA object well created.

123
00:07:45.990 --> 00:07:48.740
Now we are ready to transform the training set.

124
00:07:49.080 --> 00:07:54.750
But before we select this line and executed we need to add this one more thing that I just mentioned

125
00:07:55.170 --> 00:08:02.850
which is a function that will apply to this whole product LDA training set here and that will set this

126
00:08:02.850 --> 00:08:11.250
training set that will be transformed into a data frame because for PCa when we did this when we transform

127
00:08:11.250 --> 00:08:15.520
here the train set to this new train set composed of the principal components.

128
00:08:15.540 --> 00:08:20.320
Well we got a data frame but for LDA that's not the same.

129
00:08:20.430 --> 00:08:25.890
We will get a matrix and we need a data frame because then you know we have the next code sections.

130
00:08:25.990 --> 00:08:31.420
And inside these code sections the functions that we use expect a data frame for the trainset set here

131
00:08:31.420 --> 00:08:33.510
for example it expects a data frame.

132
00:08:33.790 --> 00:08:39.580
And so we absolutely need to convert this transform training set that will not be a data frame if we

133
00:08:39.580 --> 00:08:41.930
execute it this way but a matrix.

134
00:08:42.160 --> 00:08:49.060
And so to simply convert this into a data frame I think we already do that before we need to use the

135
00:08:49.060 --> 00:08:59.830
function as Dot data dot frame and parenthesis and we close the parenthesis here and that will set this

136
00:08:59.830 --> 00:09:02.520
transform training set as a data frame.

137
00:09:02.620 --> 00:09:08.020
And now we are ready to execute this line of code to obtain our new training set composed of the extracted

138
00:09:08.020 --> 00:09:10.390
features that are discriminants.

139
00:09:10.420 --> 00:09:14.140
So let's do it let's like this line and execute.

140
00:09:14.200 --> 00:09:17.910
And as you can see the training set is still a training set.

141
00:09:18.040 --> 00:09:19.740
Otherwise it would be in values.

142
00:09:19.960 --> 00:09:24.210
And when I click on it let's have a look at what we just created.

143
00:09:24.460 --> 00:09:28.540
So first of all the first thing that we see is this first come here.

144
00:09:28.630 --> 00:09:30.420
That is the dependent variable itself.

145
00:09:30.550 --> 00:09:35.640
I know this is no longer called customer segments but it's actually the customer segment called them

146
00:09:35.650 --> 00:09:41.300
that's exactly the same one for the same observations and the same labels 1 2 and 3 but the magically

147
00:09:41.380 --> 00:09:43.810
it was called class by function.

148
00:09:43.900 --> 00:09:45.100
So don't worry about that.

149
00:09:45.160 --> 00:09:46.570
That's the dependent variable.

150
00:09:46.690 --> 00:09:52.980
And then the next interesting thing we see are the two linear discriminants LG 1 and LG 2.

151
00:09:53.200 --> 00:09:58.420
And as I told you that's the number of linier discriminants we get due to the fact that we have three

152
00:09:58.420 --> 00:10:00.540
classes four that have been invaluable.

153
00:10:00.670 --> 00:10:02.400
So that's what matters for us now.

154
00:10:02.500 --> 00:10:07.180
And that's the variables that we'll use That's the new extracted features that we'll use to train the

155
00:10:07.180 --> 00:10:12.840
SVM model to make predictions to make the computer matrix and eventually to visualize the results.

156
00:10:13.210 --> 00:10:17.600
And then we have these other three variables in one procedure to improve to 3.

157
00:10:17.680 --> 00:10:21.100
That is just variables derived from the LDA model equations.

158
00:10:21.100 --> 00:10:22.630
So that's not very important here.

159
00:10:22.630 --> 00:10:28.180
What matters is that we have our dependent variable class and our two new extracted features.

160
00:10:28.180 --> 00:10:30.430
The Linnie are discriminants one and two.

161
00:10:30.690 --> 00:10:35.110
And so now what we have to do is set our training set into the right format.

162
00:10:35.170 --> 00:10:40.810
That is we want to train and set that is composed of first to two extracted features to two new independent

163
00:10:40.810 --> 00:10:44.800
variables and then less position the dependent variable class.

164
00:10:44.830 --> 00:10:46.820
There is nothing else than the customer segment.

165
00:10:46.930 --> 00:10:52.920
So basically what we need to do here is the same as what we did for PCa that is play with the indexes

166
00:10:53.190 --> 00:10:58.580
to not only set the right order for columns but also to not include the three columns here.

167
00:10:58.580 --> 00:11:01.140
So you want to pursue three.

168
00:11:01.180 --> 00:11:10.630
So what we'll do here to be efficient is take our PCA model and we will take this line here copy and

169
00:11:11.350 --> 00:11:14.530
go back to LGA and page that here.

170
00:11:14.710 --> 00:11:15.150
All right.

171
00:11:15.170 --> 00:11:22.160
And now as for PCa we need to include three indexes this first and next year will be the index of L.D.

172
00:11:22.160 --> 00:11:22.650
1.

173
00:11:22.660 --> 00:11:29.110
So that is the Index of this column that is 1 2 3 4 and 5 that's index 5.

174
00:11:29.320 --> 00:11:30.720
So let's add it here.

175
00:11:30.790 --> 00:11:32.960
Replace 2 by 5.

176
00:11:33.010 --> 00:11:39.010
Then the second index here should be the index of the second you extract that feature that is Elda 2.

177
00:11:39.010 --> 00:11:40.610
So that is indexed 6.

178
00:11:40.630 --> 00:11:42.250
This column has index 6.

179
00:11:42.430 --> 00:11:45.670
So let's replace your 3 by 6.

180
00:11:45.670 --> 00:11:46.550
All right.

181
00:11:46.600 --> 00:11:53.380
And eventually this should be the index of the dependent variable and this index is of course one because

182
00:11:53.380 --> 00:11:56.040
this is the first column here that hasn't x1.

183
00:11:56.380 --> 00:12:01.930
So now when I execute this line here we go look at our new trading sets.

184
00:12:02.200 --> 00:12:04.770
Well that is this time exactly what we want.

185
00:12:04.930 --> 00:12:09.640
The first two columns are the new extracted features and the last column is have been invaluable.

186
00:12:09.640 --> 00:12:14.040
Vector exactly as what is expected in the rest of the code sections.

187
00:12:14.410 --> 00:12:15.290
So perfect.

188
00:12:15.410 --> 00:12:21.430
Our trainset is well transformed and ready to be used to train the SBA model.

189
00:12:21.490 --> 00:12:21.880
All right.

190
00:12:21.880 --> 00:12:23.780
Now we need to do the same for the test set.

191
00:12:23.800 --> 00:12:25.630
So that will be very quick and easy.

192
00:12:25.690 --> 00:12:28.390
We will select these two hands here.

193
00:12:28.390 --> 00:12:36.510
Copy paste and now we just need to replace training set by test set here here as well.

194
00:12:37.660 --> 00:12:45.450
Here and eventually here also and now we can just ask you these two lines but let's take them one by

195
00:12:45.450 --> 00:12:46.240
one.

196
00:12:46.260 --> 00:12:50.510
This is the test set so far composed of 13 and they've been invaluable.

197
00:12:50.520 --> 00:12:51.820
The original ones.

198
00:12:52.080 --> 00:13:00.150
Then when we select this line and execute well we only get two new extracted features that we want to

199
00:13:00.150 --> 00:13:03.780
know the two and three variables here of the equation.

200
00:13:03.870 --> 00:13:05.850
And of course that happened in Bible class.

201
00:13:06.090 --> 00:13:12.660
And then when we do this again to take the right index is in the correct order we execute this.

202
00:13:12.660 --> 00:13:15.410
And now we get the test that composed of the two.

203
00:13:15.450 --> 00:13:20.610
You extract the teachers in first positions Elodie one and the other two and the dependent variable

204
00:13:20.660 --> 00:13:21.670
in last position.

205
00:13:21.930 --> 00:13:23.070
So that's perfect.

206
00:13:23.070 --> 00:13:28.820
Now we are ready to execute the rest of the sections to build our SVM model.

207
00:13:28.830 --> 00:13:29.990
All right let's do it.

208
00:13:30.030 --> 00:13:35.080
And actually we don't have much thing to change in this section do you think we need to change something.

209
00:13:35.220 --> 00:13:42.150
Well the answer is yes because remember the dependent variable is no longer called customer segment.

210
00:13:42.150 --> 00:13:47.790
Even if it is the customer saying one variable this time it has a different name which is class and

211
00:13:47.790 --> 00:13:53.010
that's actually the only thing that we need to change because the training set still has the same name.

212
00:13:53.070 --> 00:13:55.110
It's trainset that we're just friends from here.

213
00:13:55.230 --> 00:13:56.080
So that's fine.

214
00:13:56.100 --> 00:14:01.700
And then it's the same type and the same kernel because we're building a linear or as a VM model.

215
00:14:01.710 --> 00:14:06.030
All right so perfect let's execute this section let's do it.

216
00:14:06.120 --> 00:14:11.660
Done model created and now we are ready to predict the test results.

217
00:14:11.670 --> 00:14:14.960
So the test results do we need to change something here.

218
00:14:15.180 --> 00:14:20.480
Well this time the answer is no because we have our test transforms.

219
00:14:20.490 --> 00:14:23.230
It has the same name and the classifier as this one.

220
00:14:23.370 --> 00:14:24.630
So everything is perfect.

221
00:14:24.690 --> 00:14:29.430
We are ready to execute this line of code perfect.

222
00:14:29.610 --> 00:14:30.860
And same for here.

223
00:14:30.960 --> 00:14:32.360
We don't need to change anything.

224
00:14:32.370 --> 00:14:36.960
We can just make the confusion matrix by executing this line.

225
00:14:36.960 --> 00:14:37.620
Here we go.

226
00:14:37.610 --> 00:14:39.170
Confusion matrix created.

227
00:14:39.300 --> 00:14:43.200
Let's see if we also get 100 percent accuracy.

228
00:14:43.200 --> 00:14:50.310
We will be able to see that in a flashlight because if there is one incorrect prediction Well that means

229
00:14:50.310 --> 00:14:54.590
that we will not get 100 percent accuracy as we obtained with PCa.

230
00:14:54.600 --> 00:14:57.430
So that means that it will not be as perfect as PCA.

231
00:14:57.600 --> 00:15:01.460
So let's do it let's type C.M. here and press enter.

232
00:15:01.770 --> 00:15:05.880
And unfortunately we get one incorrect prediction here.

233
00:15:06.180 --> 00:15:12.570
But that's not such a big deal because not only one incorrect prediction is still excellent but also

234
00:15:12.750 --> 00:15:15.830
remember when we visualize the trends that result with PCa.

235
00:15:16.020 --> 00:15:17.760
Well we had incorrect predictions.

236
00:15:17.760 --> 00:15:22.620
So we were just a little lucky to get zero incorrect predictions with this year.

237
00:15:22.740 --> 00:15:23.390
So right.

238
00:15:23.410 --> 00:15:25.030
So that still excellent results.

239
00:15:25.050 --> 00:15:27.640
And now let's visualize the trends that results.

240
00:15:27.690 --> 00:15:30.080
Now do we need to change something in this section.

241
00:15:30.240 --> 00:15:36.600
Well try to figure out if yes we do because it's important to understand this in case you use this section

242
00:15:36.600 --> 00:15:40.290
here to visualize the results on your problem on your dataset.

243
00:15:40.560 --> 00:15:48.570
Well the answer is this time yes because this line of code here call names expects to have the real

244
00:15:48.570 --> 00:15:50.610
name of your independent variables.

245
00:15:50.600 --> 00:15:53.540
The new extracted features L.D. you want an alt..

246
00:15:53.700 --> 00:16:01.530
So here we need to replace PC one and be sure to buy respectively X that L.D. one because that's the

247
00:16:01.530 --> 00:16:08.910
name of the first extracted feature the first new independent variable and X that L.D. to the second

248
00:16:09.180 --> 00:16:10.300
you extract it.

249
00:16:10.330 --> 00:16:12.200
The second independent variable.

250
00:16:12.270 --> 00:16:22.090
So let's replace them one by X and and one and decide to buy X that L.D. 2.

251
00:16:22.350 --> 00:16:27.000
So that's very important and that's the only thing that you want to change the call names you must have

252
00:16:27.300 --> 00:16:29.500
the real names of your independent variables.

253
00:16:29.550 --> 00:16:33.450
When you visualize the results and then you will need to change something else.

254
00:16:33.450 --> 00:16:39.750
Well this we can also change but it's not compulsory that's just for the labels of the x axis and the

255
00:16:39.750 --> 00:16:40.660
y axis.

256
00:16:40.680 --> 00:16:41.870
So let's do it anyway.

257
00:16:41.910 --> 00:16:45.570
And this some we don't need to specify the real name of the independent variable.

258
00:16:45.570 --> 00:16:47.400
We can just replace one by.

259
00:16:47.390 --> 00:16:52.900
Else you want to specify that it's the linear discriminant and not the principal component.

260
00:16:52.980 --> 00:16:53.950
And same here.

261
00:16:54.000 --> 00:16:56.710
We can replace two by L.D. two.

262
00:16:56.730 --> 00:16:57.100
All right.

263
00:16:57.120 --> 00:16:58.110
And now it's done.

264
00:16:58.140 --> 00:17:00.260
Now this code is ready to be executed.

265
00:17:00.300 --> 00:17:05.350
So let's make the quick same changes for the visualization of the test results.

266
00:17:05.520 --> 00:17:17.790
So let's replace one here by x that will do one then pc 2 by X LG 2 and replace one by one and PC two

267
00:17:18.270 --> 00:17:19.700
by the two.

268
00:17:19.920 --> 00:17:23.340
And now everything is ready we don't need to change anything more.

269
00:17:23.340 --> 00:17:28.720
We can grab a cup of coffee and visualize the training that results and the test results.

270
00:17:28.770 --> 00:17:29.840
So let's do it.

271
00:17:30.060 --> 00:17:31.930
Let's hope that everything is OK.

272
00:17:32.010 --> 00:17:35.510
So I'm going to select all the section up to here.

273
00:17:35.720 --> 00:17:38.650
So that's to visualize trining results let's do it.

274
00:17:38.680 --> 00:17:40.180
And here we go.

275
00:17:40.540 --> 00:17:40.890
All right.

276
00:17:40.900 --> 00:17:47.650
So it's executing it's always taking a little time but we will get there we can already click on plot's

277
00:17:47.990 --> 00:17:56.190
horizon it can be stations are being made almost done and here are the results beautiful results.

278
00:17:56.350 --> 00:18:00.250
The three classes were almost well separated perfectly well separated.

279
00:18:00.250 --> 00:18:05.420
We can see the incorrect prediction but be careful that's not the same incorrect prediction we saw with

280
00:18:05.420 --> 00:18:08.710
the confusing Matrix because that concerns the test set.

281
00:18:08.740 --> 00:18:13.800
Here it's the training said so we also have one incorrect prediction on the trend set.

282
00:18:13.800 --> 00:18:16.180
That's this one but that's almost perfect.

283
00:18:16.180 --> 00:18:22.570
Which is quite intuitive to understand because as you remember LDH tries to separate the most classes

284
00:18:22.660 --> 00:18:28.720
of your dependent variable so that's why here we can see that the prediction boundary is kind of equidistant

285
00:18:28.720 --> 00:18:33.200
to the majority of the green points here and the majority of the points here.

286
00:18:33.370 --> 00:18:34.840
So that's perfect.

287
00:18:34.840 --> 00:18:40.720
Each wine is in its correct segment of customers and therefore this wine business owner can feel pretty

288
00:18:40.720 --> 00:18:47.710
confident at predicting for each new wine to which customer segments he should recommend it and not

289
00:18:47.710 --> 00:18:52.930
only he can be pretty confident that recommending the new wines to the right customers but also thanks

290
00:18:52.930 --> 00:18:57.960
to the feature extraction technique that allows him to visualize its results in two dimensions.

291
00:18:58.020 --> 00:18:58.540
Thanks.

292
00:18:58.570 --> 00:19:02.560
Number two new independent variables in the discriminants.

293
00:19:02.710 --> 00:19:08.890
Well now this wine business owner can make a clear plot of its different segment of customers and putting

294
00:19:08.890 --> 00:19:14.680
in each segment of customers the different ones so eventually that can be pretty convenient.

295
00:19:15.040 --> 00:19:16.330
Okay so perfect.

296
00:19:16.390 --> 00:19:21.890
We managed to build a great LDA model so we'll finish on this good note and therefore we will move on

297
00:19:21.890 --> 00:19:26.660
to the next section of this course which is going to be another feature extraction technique.

298
00:19:26.740 --> 00:19:29.790
But this time adapted for non-linear problems.

299
00:19:29.810 --> 00:19:37.120
So since this problem is obviously a linear problem because we manage to apply very successfully Lynnie

300
00:19:37.120 --> 00:19:44.050
our models SPCA and LDK Well it won't be relevant to apply and in the future extraction model on this

301
00:19:44.050 --> 00:19:44.820
dataset.

302
00:19:45.010 --> 00:19:46.900
So we will work on another dataset.

303
00:19:46.960 --> 00:19:52.300
There is of course going to be nonlinear and this next new feature extraction technique that we're going

304
00:19:52.300 --> 00:19:55.170
to see is going to be kernel PCA.

305
00:19:55.360 --> 00:19:57.700
So I look forward to starting this in the next section.

306
00:19:57.850 --> 00:19:59.620
And until then enjoyer machine learning.
