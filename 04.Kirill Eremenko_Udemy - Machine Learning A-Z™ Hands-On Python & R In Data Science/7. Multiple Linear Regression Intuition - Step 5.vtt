WEBVTT
1

00:00:00.900  -->  00:00:05.040
Welcome back to the ultimate Dodson's course and I am super excited about today.

2

00:00:05.040  -->  00:00:10.470
I've got an incredible tutorial prepared for you we're going to cover off a very important topic and

3

00:00:10.470  -->  00:00:13.710
that is how to build models step by step.

4

00:00:13.710  -->  00:00:15.210
I couldn't hold myself.

5

00:00:15.210  -->  00:00:21.600
I had to add that extra line step by step to the name of the tutorial because that is exactly what we're

6

00:00:21.600  -->  00:00:23.020
going to be looking at.

7

00:00:23.040  -->  00:00:28.170
I'm going to give you a framework for several different methods and it's all going to be step by step

8

00:00:28.190  -->  00:00:28.320
.

9

00:00:28.440  -->  00:00:31.160
So let's jump straight into it.

10

00:00:31.200  -->  00:00:36.440
Do you remember the good old days when we had one dependent variable and one independent variable.

11

00:00:36.480  -->  00:00:41.660
Everything was easy and we just had a simple linear regression that we had to build.

12

00:00:41.670  -->  00:00:43.620
And everything worked great.

13

00:00:43.770  -->  00:00:48.090
But now in our data we have all these columns.

14

00:00:48.240  -->  00:00:54.390
Those easy days are gone now all of these columns are potential predictors for a dependent variable

15

00:00:55.200  -->  00:01:01.770
and there's just so many of them and we need to decide which ones we want to keep and which ones we

16

00:01:01.770  -->  00:01:03.330
want to throw out.

17

00:01:03.510  -->  00:01:08.880
And you'll ask why do we need to throw out columns or do we need to get rid of data why can't we just

18

00:01:08.880  -->  00:01:11.170
use everything in our model.

19

00:01:11.250  -->  00:01:14.130
Well I can think of two reasons of the top of my head.

20

00:01:14.130  -->  00:01:17.230
Number one is garbage in garbage out.

21

00:01:17.310  -->  00:01:24.030
If you throw in a lot of stuff into your model then your model will not be a good model it won't be

22

00:01:24.390  -->  00:01:28.510
reliable it won't be doing what it's supposed to be doing is going to be a.

23

00:01:28.590  -->  00:01:30.640
So to speak garbage model.

24

00:01:30.750  -->  00:01:36.600
And number two at the end of the day you're going to have to explain these variables and understand

25

00:01:36.930  -->  00:01:43.860
the not just the math behind them but actually what it means that certain variables predict the behavior

26

00:01:43.860  -->  00:01:50.580
of your dependent variable and you will have to explain that to your executives to your boss to people

27

00:01:50.580  -->  00:01:51.390
you're presenting to.

28

00:01:51.390  -->  00:01:56.100
So if you have a thousand variables is not going to be practical to try and explain that.

29

00:01:56.100  -->  00:02:02.040
So you want to keep only the very important ones the ones that actually predict something.

30

00:02:02.040  -->  00:02:04.500
So how do we construct a model.

31

00:02:04.500  -->  00:02:09.250
This is the process of building the model selecting the right variables.

32

00:02:09.270  -->  00:02:10.660
So how do we construct a model.

33

00:02:10.740  -->  00:02:15.900
Well there are five methods that we're going to discuss of building models.

34

00:02:15.900  -->  00:02:20.190
Number one is all in the Mattew is backward elimination.

35

00:02:20.190  -->  00:02:25.610
Number three is forward selection and before is bi directional of an elimination.

36

00:02:25.620  -->  00:02:28.380
And number five is score comparison.

37

00:02:28.380  -->  00:02:33.450
We're going to talk about each one of them just now before we do I wanted to say that sometimes you'll

38

00:02:33.450  -->  00:02:42.930
hear stepwise regression so stepwise regression actually refers to number 2 3 and 4 because as it likes

39

00:02:43.070  -->  00:02:46.190
really the the true step by step methods.

40

00:02:46.260  -->  00:02:52.020
But sometimes you will hear people say stepwise stepwise regression in reference to just not before

41

00:02:52.020  -->  00:02:58.980
so there will replays be bidirectional elimination of stepwise regression and that's fine that's that's

42

00:02:58.980  -->  00:03:03.680
normal that's just because that's the more as you'll see from what we discuss.

43

00:03:03.690  -->  00:03:09.330
Bi directional ammunition is kind of the more general approach and when people say stepwise regression

44

00:03:09.330  -->  00:03:17.070
they kind of by default infer imply imply that is bi directional elimination and you have to inferred

45

00:03:17.090  -->  00:03:18.580
from there.

46

00:03:18.660  -->  00:03:21.960
OK so let's move on to our methods.

47

00:03:22.080  -->  00:03:23.730
Method number one all in.

48

00:03:23.730  -->  00:03:29.190
It's not a technical term I just call it all in basically what it means is just throw in all your variables

49

00:03:29.190  -->  00:03:29.310
.

50

00:03:29.310  -->  00:03:31.240
Something we just discussed we shouldn't do.

51

00:03:31.290  -->  00:03:32.350
When would you do that.

52

00:03:32.370  -->  00:03:38.310
One is if you have prior knowledge if you know that these exact variables are the ones are your true

53

00:03:38.310  -->  00:03:43.580
predictors you don't have to build anything you already know that this is the case.

54

00:03:43.590  -->  00:03:49.710
You might know it from domain knowledge or you might know it because you've done this model before somebody

55

00:03:49.710  -->  00:03:52.660
just gave you these variables and said please build a model.

56

00:03:52.770  -->  00:03:54.400
Well then you don't really have a choice.

57

00:03:54.450  -->  00:03:56.130
You just build the model.

58

00:03:56.340  -->  00:04:02.040
The other one is you have to perhaps like I can't really think of good examples here but maybe there's

59

00:04:02.340  -->  00:04:08.570
some framework in your company that says that you have to use these variables right.

60

00:04:08.580  -->  00:04:15.360
So it's kind of similar to prior knowledge but it's not it's not a it's not your decision it's you might

61

00:04:15.360  -->  00:04:19.100
want to do it differently but there is a framework.

62

00:04:19.110  -->  00:04:20.180
You know like maybe a bank.

63

00:04:20.190  -->  00:04:26.280
And to predict credit like with the likelihood of somebody defaulting on something they have to use

64

00:04:26.280  -->  00:04:28.530
the superballs once again.

65

00:04:28.560  -->  00:04:33.420
I'm not sure in which industries that would be the case but perhaps that could be the case.

66

00:04:33.450  -->  00:04:40.590
And number three you would use this method if you're preparing for a backward elimination type of construction

67

00:04:40.590  -->  00:04:43.500
of regression which is our next type.

68

00:04:43.500  -->  00:04:46.640
So let's move on to backward elimination.

69

00:04:47.070  -->  00:04:52.470
OK so here comes a step by step stuff you might you might want to get your pens out and write these

70

00:04:52.470  -->  00:04:56.660
things down because we're going to have a truly step by step method now.

71

00:04:56.850  -->  00:04:57.510
All right.

72

00:04:57.510  -->  00:05:00.120
Backward elimination first thing.

73

00:05:00.120  -->  00:05:04.900
Step one you have to select a significance level to stay in the model.

74

00:05:05.010  -->  00:05:11.960
So by default we're going to go with 5 percent so 0.05 and we're going to use it in the next step so

75

00:05:12.200  -->  00:05:15.810
it's at the beginning you decide on this significant level.

76

00:05:15.810  -->  00:05:20.880
Step two you fit the full model with all possible predicter So you kind of do that all in approach which

77

00:05:20.880  -->  00:05:27.500
we just talked about and you put all of your variables into your model and now we're going to start

78

00:05:27.500  -->  00:05:28.720
getting rid of them.

79

00:05:28.760  -->  00:05:32.520
Step three you considered the predicter with the highest P-value.

80

00:05:32.550  -->  00:05:37.700
So remember those P-value that we talked about in for instance in Grettel or in any software or you

81

00:05:37.700  -->  00:05:38.670
can see them.

82

00:05:38.660  -->  00:05:42.870
So after you fitted the model you'll see the one with the highest P value.

83

00:05:43.110  -->  00:05:52.340
So if p is greater than your significance level then you go to Step 4 and Step 4 is you have to remove

84

00:05:52.340  -->  00:05:56.400
that predict to remove basically the variable that has the highest P value.

85

00:05:56.610  -->  00:06:02.720
And from Step 4 you fit the model before this variable so there's a star here because I just wanted

86

00:06:02.730  -->  00:06:08.650
to remind myself to tell you that if you just remove the variable.

87

00:06:08.840  -->  00:06:11.080
Obviously you can just say that.

88

00:06:11.090  -->  00:06:16.220
OK now now I've got the new model you have to actually refit them all you have to re recreate them all

89

00:06:16.230  -->  00:06:21.960
rebuild it with the fewer number of variables so if you had maybe I don't know a hundred variables and

90

00:06:21.950  -->  00:06:24.570
you removed them one of them you have 99 now.

91

00:06:24.590  -->  00:06:27.830
Well you have to rebuild it so the coefficients are going to be different.

92

00:06:27.870  -->  00:06:29.710
The constant is going to be different.

93

00:06:29.840  -->  00:06:36.110
And you you need to perform that step because once you remove a variable it affects all the other variables

94

00:06:36.140  -->  00:06:38.580
in your whole regression.

95

00:06:38.780  -->  00:06:41.930
And so after Step 5 you go back to Step 3.

96

00:06:41.960  -->  00:06:46.880
Once again you look for the variable with the highest value in your new model.

97

00:06:47.030  -->  00:06:49.340
You take it out you remove.

98

00:06:49.340  -->  00:06:51.290
So basically step 4 you remove it.

99

00:06:51.290  -->  00:06:54.140
You fit the model again with one less variable.

100

00:06:54.140  -->  00:07:00.410
And so on and you keep doing that until you come to a point where the even the variable of the highest

101

00:07:00.410  -->  00:07:05.660
P value that p value is still less than your significance level.

102

00:07:05.660  -->  00:07:11.990
So if that condition P is greater than s.l is not correct then you don't go just have 4 anymore.

103

00:07:12.000  -->  00:07:16.080
You go to fin and this case of Feehan is the finish.

104

00:07:16.090  -->  00:07:17.800
And your model is ready.

105

00:07:17.810  -->  00:07:25.170
So as soon as all of the variables that you have left in your model are there p values are less than

106

00:07:25.400  -->  00:07:26.570
the significance level.

107

00:07:26.630  -->  00:07:28.340
Your models prepared.

108

00:07:29.030  -->  00:07:31.250
So that's how the backward elimination method works.

109

00:07:31.380  -->  00:07:32.780
Let's move on to the next one.

110

00:07:32.880  -->  00:07:35.110
Next method is the forward selection.

111

00:07:35.270  -->  00:07:40.140
This is it sounds like the opposite and the picture in the top right corner does illustrate the opposite

112

00:07:40.150  -->  00:07:40.340
.

113

00:07:40.580  -->  00:07:46.410
But it's a much more complex than just simply reversing the procedure.

114

00:07:46.440  -->  00:07:50.390
You will see that it's a much larger procedure.

115

00:07:50.450  -->  00:07:52.220
We started with step 1.

116

00:07:52.380  -->  00:07:55.570
Select the significance level to enter the model.

117

00:07:55.910  -->  00:07:59.600
And in this case once again we're going to select 5 percent.

118

00:07:59.880  -->  00:08:00.920
Then we go to Step 2.

119

00:08:00.920  -->  00:08:04.880
We fit all possible simple regression models.

120

00:08:04.880  -->  00:08:12.080
So we take the dependent variable and we create a regression model with every single independent variable

121

00:08:12.080  -->  00:08:13.040
that we have.

122

00:08:13.470  -->  00:08:20.060
And then we select out of all those models we select the one which has the lowest p value for the independent

123

00:08:20.060  -->  00:08:21.490
variable.

124

00:08:21.720  -->  00:08:25.220
So you can already see that that is in itself a lot of work.

125

00:08:25.280  -->  00:08:27.550
Then what we do is we move to step three.

126

00:08:27.620  -->  00:08:36.380
We keep this variable that we've just chosen and we fit all other possible models with one extra predicter

127

00:08:36.990  -->  00:08:39.440
added to the one we usually have.

128

00:08:39.440  -->  00:08:41.250
So what does that mean.

129

00:08:41.250  -->  00:08:47.630
That means we've selected a simple linear regression with one variable.

130

00:08:47.630  -->  00:08:55.190
Now we need to construct all possible linear regressions with two variables where one of those two variables

131

00:08:55.190  -->  00:08:56.390
is the one of various.

132

00:08:56.470  -->  00:09:03.240
So basically we add on all possible all of the other variables one by one so we choose OK let's add

133

00:09:03.240  -->  00:09:06.580
on this variable and then let's add on the next one like.

134

00:09:06.650  -->  00:09:14.280
But separately so we construct all possible 2 variable linear regressions and just keeping definitely

135

00:09:14.270  -->  00:09:16.300
keeping the variable that we're very selected.

136

00:09:16.590  -->  00:09:18.930
So what do we do after that.

137

00:09:18.920  -->  00:09:26.150
Out of all of these possible two variable regressions we consider the one where the new variable that

138

00:09:26.150  -->  00:09:29.710
we added had the lowest p value.

139

00:09:29.820  -->  00:09:37.020
So if that p value is less than our significance level meaning that you know that variables a good one

140

00:09:37.260  -->  00:09:41.070
it's a significant variable then we moved back to Step 3.

141

00:09:41.150  -->  00:09:41.830
What does that mean.

142

00:09:41.850  -->  00:09:47.760
Means that now we have a regression with two variables and now we will add a third variable We'll try

143

00:09:47.750  -->  00:09:54.990
all possible variables that we have left as our third variable and then out of all of those models with

144

00:09:54.990  -->  00:10:00.800
three variables we will go to Step 4 and we'll select again the one of the lowest p value for that third

145

00:10:00.810  -->  00:10:01.430
variable.

146

00:10:01.430  -->  00:10:02.160
We added.

147

00:10:02.310  -->  00:10:03.230
And I'll keep doing that.

148

00:10:03.240  -->  00:10:08.340
So basically we'll be keep growing the regression model but not just randomly will be actually selecting

149

00:10:08.340  -->  00:10:15.340
out of the all all of the possible combinations every single time and growing at one variable at a time

150

00:10:15.350  -->  00:10:15.530
.

151

00:10:16.360  -->  00:10:24.430
And then we will only stop when the variable that we've added It has a p value that is greater than

152

00:10:24.700  -->  00:10:25.860
our significance level.

153

00:10:25.900  -->  00:10:32.590
So when this condition is less than SL is not true then we don't go to Step 3 we finished the regression

154

00:10:32.590  -->  00:10:38.120
y well because that variable though we just added is no longer is not significant anymore.

155

00:10:38.200  -->  00:10:43.090
And we also know that we selected the one with the lowest P-value So there is no other variable that

156

00:10:43.090  -->  00:10:51.910
we can add that its p value will be greater will be less than s.l any any regression which is from then

157

00:10:51.910  -->  00:10:56.390
onwards it will the variable the new variable will always be insignificant.

158

00:10:56.410  -->  00:10:58.680
And so here we finish the regression.

159

00:10:58.900  -->  00:11:03.480
And the trick here is that you keep not the current model but the previous one.

160

00:11:03.700  -->  00:11:08.830
And that makes sense because you've just added a variable which is insignificant So what's the point

161

00:11:08.830  -->  00:11:10.760
of that variable just move a step back.

162

00:11:11.050  -->  00:11:12.760
So that's how forward selection works.

163

00:11:12.760  -->  00:11:17.200
I know it's a bit confusing but just try to wrap your head around and maybe read through these instructions

164

00:11:17.200  -->  00:11:18.130
one more time.

165

00:11:18.120  -->  00:11:24.720
It does make a lot of sense when you and it's like kind of picture what exactly is going on.

166

00:11:25.600  -->  00:11:29.290
And next we're moving on to the bidirectional imit elimination.

167

00:11:29.290  -->  00:11:35.440
So this one as you can assume or perhaps yes it combines the two step one.

168

00:11:35.500  -->  00:11:40.780
Select a significant level to stay or to enter and a significant level to stay.

169

00:11:40.990  -->  00:11:47.530
So we're going to select in both cases Phipson but it's up to you what you select set to perform the

170

00:11:47.530  -->  00:11:54.540
next step of the forward selection meaning that the one that we just discussed.

171

00:11:54.550  -->  00:12:01.420
So where new variables when they enter in order to enter they have to be less than the significance

172

00:12:01.420  -->  00:12:03.220
level to enter.

173

00:12:03.220  -->  00:12:07.610
So basically add on a new variable based on the forward selection method.

174

00:12:07.840  -->  00:12:11.450
Step 3 perform all of the steps of the backward elimination process.

175

00:12:11.470  -->  00:12:13.720
So now you should have two variables.

176

00:12:13.780  -->  00:12:19.980
Start getting rid of them and see if you can get rid of any of them and then move back to step 2 so

177

00:12:19.990  -->  00:12:21.550
then grow by another variable.

178

00:12:21.580  -->  00:12:27.100
And every time you go by verbal say let's say you were at five verbalising went to six since you went

179

00:12:27.100  -->  00:12:31.180
to six you have to form all the steps of backwardly mesh so you don't just eliminate one variable if

180

00:12:31.180  -->  00:12:34.480
you can eliminate one to three however many you can.

181

00:12:34.780  -->  00:12:37.420
And then from there you go back to Step 2.

182

00:12:37.450  -->  00:12:39.220
And so this is a very iterative process.

183

00:12:39.220  -->  00:12:46.000
You keep doing that until at some point you cannot add new variables no variables can enter or no old

184

00:12:46.000  -->  00:12:48.870
variables can exit as soon as you get there.

185

00:12:48.880  -->  00:12:52.740
Then you proceed to the finish because your model is ready to call.

186

00:12:52.810  -->  00:12:57.220
You can add anything you can take anything out that means you've you've created the model.

187

00:12:57.220  -->  00:13:00.520
So this is one of the more tedious methods.

188

00:13:00.550  -->  00:13:06.880
Of course you would have to get a computer to do this for you because otherwise you'd go go insane putting

189

00:13:06.880  -->  00:13:08.430
variables in and taking them out.

190

00:13:08.440  -->  00:13:11.440
But that's how bidirectional elimination works.

191

00:13:11.880  -->  00:13:17.600
And once again some people call it stepwise regression.

192

00:13:17.740  -->  00:13:20.940
And finally all possible models.

193

00:13:20.950  -->  00:13:29.500
So this is the most probably Saro approach but also the most resource consuming approach because you

194

00:13:29.500  -->  00:13:34.960
select a criterion of goodness of fit for instance that caky criterion can be the R-squared lots of

195

00:13:34.960  -->  00:13:40.470
different criterions then you construct all possible regression model so if you had and variables and

196

00:13:40.690  -->  00:13:46.420
there'll be a two to the power of and minus one total combinations of these variables and that that's

197

00:13:46.420  -->  00:13:53.350
exactly how many models there can possibly be and then step three you select the one of these models

198

00:13:53.350  -->  00:13:56.850
with the best criterion that you're looking at.

199

00:13:56.860  -->  00:13:58.410
There you go your model is ready.

200

00:13:58.420  -->  00:14:03.700
So sounds easy but let's have a look an example even if you have 10 columns in your daughter you'll

201

00:14:03.700  -->  00:14:05.630
have 1023 models.

202

00:14:05.650  -->  00:14:06.760
That's insane.

203

00:14:06.760  -->  00:14:11.050
That's an insane amount of models and we're not talking about columns that you've already filter out

204

00:14:11.050  -->  00:14:18.110
so call them that you know that like in our example you might have five or six columns.

205

00:14:18.100  -->  00:14:24.070
Now we're talking about when you get a data set that you need just is pretty much a rule and it has

206

00:14:24.070  -->  00:14:26.390
like maybe 100 columns.

207

00:14:26.460  -->  00:14:33.940
I've worked with data sets were around that maybe 50 to 100 maybe more columns and instead of going

208

00:14:33.940  -->  00:14:37.990
through them this is what this method is suggesting sort of going through them and picking out the ones

209

00:14:37.990  -->  00:14:39.650
that you think should be in the model.

210

00:14:39.670  -->  00:14:40.810
You just throw everything in.

211

00:14:40.810  -->  00:14:48.250
Well it's not a good approach because basically it's growing exponentially the number of models is growing

212

00:14:48.250  -->  00:14:49.160
exponentially.

213

00:14:49.360  -->  00:14:56.310
And it's it's very resource consuming to get a result from this approach.

214

00:14:56.680  -->  00:15:02.010
And finally so where how we come to we've come to our conclusion.

215

00:15:02.130  -->  00:15:07.750
We have five methods of building model models all in backwords elimination for selection by directional

216

00:15:07.780  -->  00:15:09.810
immigration and the score comparison.

217

00:15:10.030  -->  00:15:16.810
So the one way are going to be looking at in these tutorials in order to get our head around how to

218

00:15:16.810  -->  00:15:22.030
build models step by step and get some practice is the backward elimination process because it is the

219

00:15:22.030  -->  00:15:28.010
fastest one or all of them and you will still get to see exactly how the step by step method works.

220

00:15:28.180  -->  00:15:35.590
And plus we'll throw in a few extra tricks along the way to make sure our models are very robust Cant' wait

221

00:15:35.650  -->  00:15:36.920
to get started.

222

00:15:36.970  -->  00:15:37.930
Lots of theory.

223

00:15:37.930  -->  00:15:40.520
Let's get to the practice of forcing in next time
