1
00:00:00,270 --> 00:00:01,110
Hello my friends.

2
00:00:01,110 --> 00:00:04,530
Welcome to this new practical activity on this time.

3
00:00:04,530 --> 00:00:07,070
The Naive Bayes classification model.

4
00:00:07,140 --> 00:00:11,570
This is our fifth classification model we're implementing in this part 3 classification.

5
00:00:11,640 --> 00:00:17,880
We already implemented logistic regression key nearest neighbor is as VM and kernel as VM and so for

6
00:00:17,880 --> 00:00:22,530
the best accuracy we've got with these for model is all point ninety three.

7
00:00:22,530 --> 00:00:28,230
Ninety three percent which we got with both K nearest neighbors and kernel SVM.

8
00:00:28,410 --> 00:00:32,710
And now we're about to implement a new classification model than a base one.

9
00:00:32,730 --> 00:00:34,950
A very well known and widely used.

10
00:00:34,950 --> 00:00:37,150
So you definitely want to have it in a toolkit.

11
00:00:37,260 --> 00:00:43,080
And so the big question is will we beat that best accuracy of open 93.

12
00:00:43,130 --> 00:00:47,640
All right so let's stop immediately because I can't wait to show this to you but just before we start

13
00:00:47,670 --> 00:00:49,790
let's make sure everyone here is on the same page.

14
00:00:49,800 --> 00:00:53,550
I gave you the link to this folder right before this tutorial in the Oracle.

15
00:00:53,700 --> 00:00:55,200
So make sure you connect to it.

16
00:00:55,200 --> 00:00:56,330
And now let's do this.

17
00:00:56,370 --> 00:01:00,420
Let's start the implementation of native base.

18
00:01:00,420 --> 00:01:05,400
So we're gonna go into this folder and we're gonna start with Python as usual and insiders follow you

19
00:01:05,400 --> 00:01:11,640
will find two files the Navy base implementation in the IP y and B format and the same social network

20
00:01:11,750 --> 00:01:15,390
adds data set which I will quickly re-explain.

21
00:01:15,390 --> 00:01:22,020
So this is a dataset containing 400 customers each row here represents the customer and for each customer

22
00:01:22,020 --> 00:01:28,890
we get the two features age and estimated salary with which we are going to predict that dependent variable

23
00:01:29,000 --> 00:01:36,570
per taste which tells if yes or no the customer but the SUV so zero means that indeed the customer didn't

24
00:01:36,570 --> 00:01:40,310
buy the SUV and one means that the customer bought the SUV.

25
00:01:40,470 --> 00:01:40,760
All right.

26
00:01:40,770 --> 00:01:45,210
So we're going to train a new classification model to understand the correlations between these two

27
00:01:45,210 --> 00:01:46,830
features age and salary.

28
00:01:46,830 --> 00:01:52,880
And that depend on verbal protests in order to predict the customers that will buy the SUV.

29
00:01:52,900 --> 00:01:57,780
And once we get these predictions we will target these customers on social networks with some beautiful

30
00:01:57,780 --> 00:01:59,730
ad of this beautiful car.

31
00:01:59,790 --> 00:02:00,210
All right.

32
00:02:00,210 --> 00:02:06,960
So that's the same story and now we're going to start our implementation either with Google collaboratively

33
00:02:07,260 --> 00:02:08,700
or Uber and notebook.

34
00:02:08,700 --> 00:02:14,640
Feel free to choose your favorite and I can't wait to see if we're going to beat the accuracy and I

35
00:02:14,640 --> 00:02:15,000
can't wait.

36
00:02:15,030 --> 00:02:18,520
Also to show you the visualization results at end.

37
00:02:18,540 --> 00:02:21,630
All right so right now it is loading and laying out the notebook.

38
00:02:21,810 --> 00:02:22,530
And here it is.

39
00:02:22,530 --> 00:02:28,260
Here is a Navy base implementation still resulting from the same classification template which we made

40
00:02:28,320 --> 00:02:32,310
in the first section of this by 3 when implementing logistic regression.

41
00:02:32,330 --> 00:02:38,820
So basically all the cells here are the same as in this logistic regression implementation or classification

42
00:02:38,820 --> 00:02:39,540
template.

43
00:02:39,540 --> 00:02:45,510
The only cell that will change will be you know the one where we build and train the classification

44
00:02:45,510 --> 00:02:51,840
model meaning this one that's the only cell that we will re implement ourselves because all the rest

45
00:02:51,990 --> 00:02:52,620
is the same.

46
00:02:52,830 --> 00:02:58,410
But this notebook is an read on the mode and therefore in order to re implement that cell we need to

47
00:02:58,650 --> 00:03:03,920
create a copy of this notebook by clicking save a copy in drive.

48
00:03:03,930 --> 00:03:10,450
And as you can see this will create a copy in which we will be able to implement that cell.

49
00:03:10,470 --> 00:03:13,280
All right so once again laying out the notebook and there we go.

50
00:03:13,290 --> 00:03:19,920
Here is our copy on which we are authorized to modify anything and especially that cell.

51
00:03:19,920 --> 00:03:21,600
We want to implement.

52
00:03:21,630 --> 00:03:27,720
So scrolling down here to find it here it is training the Navy base model on the training set.

53
00:03:27,750 --> 00:03:34,260
So let's immediately remove that cell because I want you to re implement it from scratch as if we had

54
00:03:34,260 --> 00:03:36,320
no idea on how to implement this.

55
00:03:36,360 --> 00:03:40,160
All right let's create a new code cell and now your turn.

56
00:03:40,230 --> 00:03:47,280
I want you to train your machinery independence by figuring out by yourself how to indeed build and

57
00:03:47,280 --> 00:03:50,810
train that knife based more on the training set.

58
00:03:51,060 --> 00:03:56,160
So of course to do this you have several options you can directly type in the search bar of Google or

59
00:03:56,340 --> 00:04:00,170
being well naive based psychic learned class okay.

60
00:04:00,240 --> 00:04:07,590
Or you can navigate the cycle or an API which is right here in order to find that class which we need

61
00:04:07,590 --> 00:04:09,460
to build or Navy based model.

62
00:04:09,600 --> 00:04:14,730
And my personal recommendation is to try it with the second option because indeed this will get you

63
00:04:14,730 --> 00:04:19,230
familiar with this API and the more you get familiar with it the better.

64
00:04:19,230 --> 00:04:19,530
Okay.

65
00:04:19,920 --> 00:04:25,200
So please press buzz now try to find this class and then try to build this knife based model and training

66
00:04:25,260 --> 00:04:31,170
on the training set and now in two seconds we will implement the solution together.

67
00:04:31,170 --> 00:04:32,620
All right let's do this.

68
00:04:32,700 --> 00:04:35,430
Let's build this Navy base model with psychiatry.

69
00:04:36,060 --> 00:04:40,560
So the API is huge and this is organized in alphabetical order.

70
00:04:40,560 --> 00:04:45,920
And so the first thing we'll try is to find you know a module called maybe naive base.

71
00:04:45,930 --> 00:04:46,460
Right.

72
00:04:46,470 --> 00:04:47,730
Just the name of the model.

73
00:04:47,790 --> 00:04:55,380
So we will scroll down down to an you know the letter N and we'll see if we get now base somewhere or

74
00:04:55,410 --> 00:04:56,690
something close to it.

75
00:04:56,760 --> 00:05:00,570
Let's see linear model many for learning tricks.

76
00:05:00,720 --> 00:05:06,000
We're getting close and Gaussian Mixture morals model selection multi class.

77
00:05:06,000 --> 00:05:07,090
Well there is a lot of em.

78
00:05:07,260 --> 00:05:08,750
Okay perfect.

79
00:05:08,820 --> 00:05:10,270
Naive Bayes.

80
00:05:10,290 --> 00:05:12,320
So let's kick this nice base.

81
00:05:12,420 --> 00:05:12,990
All right.

82
00:05:12,990 --> 00:05:17,910
And so the module indeed is named on the score base and among these models.

83
00:05:17,910 --> 00:05:20,730
Well according to you which one are we going to take here.

84
00:05:20,730 --> 00:05:22,230
That's a good question actually.

85
00:05:22,230 --> 00:05:27,990
Well in fact the classic one and the one that you learned in the intuition lecture is of course this

86
00:05:27,990 --> 00:05:35,010
one the Gaussian naÃ¯ve base Gaussian and B and that's exactly what we'll use to implement are naive

87
00:05:35,070 --> 00:05:36,730
base model.

88
00:05:36,750 --> 00:05:39,110
All right so the name of the class is Gaussian and B.

89
00:05:39,300 --> 00:05:44,220
And this time the good news is that we won't have to worry too much about the parameters because there

90
00:05:44,220 --> 00:05:50,960
are actually only two parameters priors which are the prior probabilities of the classes if specified

91
00:05:50,980 --> 00:05:53,450
priors are not adjusted according to the data.

92
00:05:53,460 --> 00:05:57,130
So of course for this we will keep the default value of None.

93
00:05:57,270 --> 00:05:57,540
Right.

94
00:05:57,540 --> 00:06:03,420
We won't specify some price here and for the second one voice smoothing it is the portion of the largest

95
00:06:03,420 --> 00:06:08,220
variance of all features that is added to variances for calculation stability.

96
00:06:08,220 --> 00:06:12,140
The default value is 1 times 10 and the power of minus nine.

97
00:06:12,210 --> 00:06:13,670
We will just keep the default value.

98
00:06:13,680 --> 00:06:14,320
That's fine.

99
00:06:14,320 --> 00:06:19,620
You know we will worry later about parameter tuning which indeed allows you to tune the values of your

100
00:06:19,620 --> 00:06:22,520
parameters but that will be covered in pattern.

101
00:06:22,590 --> 00:06:28,980
And so here very simply we will just call this class without inputting any parameters.

102
00:06:29,070 --> 00:06:30,480
So that's super easy.

103
00:06:30,490 --> 00:06:35,210
Let's copy this and let us go back to our implementation.

104
00:06:35,220 --> 00:06:35,940
Copy 1.

105
00:06:36,420 --> 00:06:38,130
Let's reset here.

106
00:06:38,190 --> 00:06:42,460
Let's remove this little thing here and now you know how to adapt this.

107
00:06:42,480 --> 00:06:48,640
We need to start with from you know the naive bayes module of the cycle and library.

108
00:06:48,660 --> 00:06:51,990
And then here you add import.

109
00:06:52,080 --> 00:06:55,830
There we go that Gaussian in B class.

110
00:06:55,830 --> 00:06:56,540
Perfect.

111
00:06:56,550 --> 00:07:02,490
The next step is to of course create an instance of this class which will be an object representing

112
00:07:02,490 --> 00:07:05,220
exactly that native base model.

113
00:07:05,220 --> 00:07:11,040
And so there we go are going to call this as usual classifier in order to be coherent with the next

114
00:07:11,340 --> 00:07:15,830
sections of this implementation and mostly so that we don't have to change anything right.

115
00:07:15,840 --> 00:07:21,550
Because then we call this classifier variable to predict the results and visualize the results.

116
00:07:21,550 --> 00:07:22,170
So there we go.

117
00:07:22,170 --> 00:07:28,560
Classify you here and then equals and then we're going to call this Gaussian and B class in order to

118
00:07:28,560 --> 00:07:28,940
create.

119
00:07:28,950 --> 00:07:31,530
Indeed this knife based model.

120
00:07:32,100 --> 00:07:33,260
Okay perfect.

121
00:07:33,270 --> 00:07:35,400
And now you know how to finish this.

122
00:07:35,520 --> 00:07:41,820
We need to take our classifier again from which we're going to call the fit method which will train

123
00:07:41,880 --> 00:07:50,420
this classifier on the training set composed of indeed X train and Y train.

124
00:07:50,610 --> 00:07:50,930
Right.

125
00:07:50,940 --> 00:07:56,280
I hope you did it even faster than me because indeed this is exactly the same as before and now you

126
00:07:56,580 --> 00:08:01,620
are also independent and know how to find the information you need in the API.

127
00:08:02,520 --> 00:08:03,300
Okay great.

128
00:08:03,330 --> 00:08:09,540
So once again there we go that implementation is over and we're ready to get the final results.

129
00:08:09,540 --> 00:08:15,780
And mostly we're ready to find out if we can beat the record accuracy you know 93 percent.

130
00:08:15,780 --> 00:08:18,170
So I can't wait to see so let's do it right now.

131
00:08:18,180 --> 00:08:22,860
Let's click this folder button to upload data set right.

132
00:08:22,890 --> 00:08:27,090
We have to do it in order to train indeed that is based more on the training sets.

133
00:08:27,090 --> 00:08:32,060
Right now your notebook is connecting to a runtime to enable file browsing.

134
00:08:32,250 --> 00:08:35,310
And once again in a second we should get the upload button.

135
00:08:35,310 --> 00:08:35,850
There we go.

136
00:08:36,240 --> 00:08:41,450
So we're going to click it and here we are in the kernel as VM folder.

137
00:08:41,460 --> 00:08:45,160
So let me show you the path once again please find your whole machinery.

138
00:08:45,160 --> 00:08:50,550
It is at fault which you could download in the previous tutorial if not already and then inside.

139
00:08:50,550 --> 00:08:56,100
We're going to go to part three classification then Section 18 we're making good progress here.

140
00:08:56,190 --> 00:09:04,010
Native Baz then python and then social network at that CSC okay we press OK.

141
00:09:04,040 --> 00:09:05,450
Here we have the data set.

142
00:09:05,470 --> 00:09:06,290
Oh good.

143
00:09:06,330 --> 00:09:14,410
And now now we can run everything in order to get indeed our new results so let's do this run all.

144
00:09:14,520 --> 00:09:20,730
And now all the cells are running and especially this one there we go we now have our Gaussian native

145
00:09:20,730 --> 00:09:26,880
based model and well let's see the results one by one starting with this ones that that's the prediction

146
00:09:26,880 --> 00:09:33,030
of a single result you know that first customer of the test set of age 30 an estimated salary eighty

147
00:09:33,030 --> 00:09:34,440
seven thousand dollars.

148
00:09:34,440 --> 00:09:40,440
And remember in the White says the real outcome was zero meaning that this customer didn't buy the SUV

149
00:09:40,710 --> 00:09:43,920
and that's the prediction which is indeed the correct prediction.

150
00:09:44,430 --> 00:09:46,500
And then when predicting the test results.

151
00:09:46,500 --> 00:09:50,730
Well once again we see that we have a lot of correct predictions.

152
00:09:50,730 --> 00:09:51,700
All this is correct.

153
00:09:51,720 --> 00:09:52,700
All this is correct.

154
00:09:52,770 --> 00:09:54,770
This is our first incorrect prediction.

155
00:09:54,900 --> 00:09:58,360
Another one here and then another one here.

156
00:09:58,380 --> 00:10:01,440
All correct I'll correct another one here.

157
00:10:01,760 --> 00:10:04,980
Oh I'm not sure we're going to beat actually that accuracy.

158
00:10:04,990 --> 00:10:09,090
We seem to have more than seven incorrect predictions at first.

159
00:10:09,090 --> 00:10:09,770
I'm not sure.

160
00:10:09,770 --> 00:10:11,030
But let's see let's see.

161
00:10:11,030 --> 00:10:13,660
Well that's exactly what we're about to find out right now.

162
00:10:13,660 --> 00:10:14,840
So are you ready.

163
00:10:14,840 --> 00:10:21,500
The question is will we beat the accuracy of 93 percent which was the best accuracy resulting from both

164
00:10:21,620 --> 00:10:23,680
Keenan and Colonel SVM.

165
00:10:23,720 --> 00:10:29,600
And so let's see what we get with Navy base and no unfortunately we don't beat the record.

166
00:10:29,600 --> 00:10:36,230
Indeed the accuracy we get with that Navy based model is 90 percent which beats indeed logistic regression

167
00:10:36,260 --> 00:10:42,160
but does equally the same as the classic SVM model with a linear kernel.

168
00:10:42,170 --> 00:10:46,010
All right but still I think we will get nice visualization results.

169
00:10:46,040 --> 00:10:50,930
That's the code cell where we visualize the trends that result in well this time we got the results

170
00:10:50,950 --> 00:10:51,740
pretty fast.

171
00:10:51,770 --> 00:10:54,650
You can see that the cell is already executed.

172
00:10:54,650 --> 00:10:55,520
So let's see.

173
00:10:55,850 --> 00:11:00,350
I can show you that indeed the Navy base curve is pretty nice right.

174
00:11:00,350 --> 00:11:03,240
It is a nice smooth curve right.

175
00:11:03,290 --> 00:11:05,260
That catches well indeed.

176
00:11:05,330 --> 00:11:11,420
These green customers here you know the ones who in reality but the SUV in the right green region.

177
00:11:11,510 --> 00:11:16,370
But unfortunately you know it's separated two classes you know with these two operation regions a bit

178
00:11:16,370 --> 00:11:21,950
large you know not very precisely and that's why we don't get an accuracy that is higher than 93 percent

179
00:11:22,220 --> 00:11:26,810
but still you know we made a progress with respect to logistic regression because indeed remember that

180
00:11:27,140 --> 00:11:32,900
for logistic regression these green customers here could not be well classified right because of the

181
00:11:32,900 --> 00:11:35,740
straight line they fall in the red region.

182
00:11:35,840 --> 00:11:40,970
And here in our knife base implementation well these green customers fall in the right region so at

183
00:11:40,970 --> 00:11:45,420
least it corrected that but since here there is going to be a large margin.

184
00:11:45,420 --> 00:11:52,620
Well these green customers which were correctly classified with the kernel SVM if you remember right.

185
00:11:52,640 --> 00:11:58,010
These are the training set results right so I'm talking about these ones here they are correctly classified

186
00:11:58,010 --> 00:11:59,900
except these two and the third one.

187
00:11:59,990 --> 00:12:04,060
But clearly with Knight base well they fall into the wrong region.

188
00:12:04,100 --> 00:12:04,390
OK.

189
00:12:04,400 --> 00:12:10,800
But anyway at least you see the prediction curve of the night based model and mostly you see that nail

190
00:12:10,820 --> 00:12:13,910
based model is clearly a nonlinear classifier.

191
00:12:13,910 --> 00:12:19,880
And you know in some other situations because the knife base is causing less or refitting well in some

192
00:12:19,880 --> 00:12:22,760
situations it will do better than your other models.

193
00:12:22,760 --> 00:12:25,580
That's why it's always very important to try all of them.

194
00:12:25,580 --> 00:12:31,460
And remember at the end of this point I will actually deploy all our models with new simplified code

195
00:12:31,460 --> 00:12:35,750
templates for each model you know without all the prints and everything in order to deploy them in a

196
00:12:35,750 --> 00:12:41,630
flashlight so that we can quickly figure out what is the best classification model for any data set.

197
00:12:41,650 --> 00:12:44,450
You know regardless of the number of features.

198
00:12:44,480 --> 00:12:44,750
All right.

199
00:12:44,750 --> 00:12:45,860
So that's for the train set.

200
00:12:45,860 --> 00:12:48,590
And now let's come in quickly on the test set.

201
00:12:48,590 --> 00:12:48,830
All right.

202
00:12:48,860 --> 00:12:50,120
This is already executed.

203
00:12:50,120 --> 00:12:51,060
Perfect.

204
00:12:51,080 --> 00:12:53,610
So here are the results of Navy based on the test set.

205
00:12:53,750 --> 00:12:57,610
And actually we realized that here we were pretty unlucky.

206
00:12:57,620 --> 00:13:04,100
You know nice base was pretty unlucky because the three predictions that it missed you know this red

207
00:13:04,100 --> 00:13:10,040
customer that falls incorrectly in the green region and these two green customers that fall incorrectly

208
00:13:10,040 --> 00:13:10,970
in the red region.

209
00:13:11,120 --> 00:13:16,460
Well Navy Base was pretty unlucky on this one because you know they're almost on the breaks and boundary

210
00:13:16,490 --> 00:13:20,120
and it is what it missed by very closely to beat the record.

211
00:13:20,120 --> 00:13:21,280
And same for this one actually.

212
00:13:21,290 --> 00:13:23,570
You know this one was very shortly missed.

213
00:13:23,630 --> 00:13:23,890
Right.

214
00:13:23,900 --> 00:13:29,270
Because it is an incorrect relation of a customer who in reality didn't buy the SUV because it's red

215
00:13:29,300 --> 00:13:33,030
but was predicted to buy the SUV because it falls in the green region.

216
00:13:33,050 --> 00:13:36,850
So pretty unlucky which explains the accuracy of 90 percent.

217
00:13:36,920 --> 00:13:40,810
I think that if we tune a bit nice base we could beat the record.

218
00:13:40,910 --> 00:13:43,090
But once again that will be important.

219
00:13:43,090 --> 00:13:48,830
And now the next step will be to buy the last two classification models which belong to a branch of

220
00:13:48,830 --> 00:13:51,230
machinery called and symbol models.

221
00:13:51,230 --> 00:13:55,940
I'm talking of course about decision tree classification and random forest classification.

222
00:13:56,420 --> 00:14:02,630
And actually with these two and especially random forest we might have a chance to beat that record

223
00:14:02,660 --> 00:14:03,360
accuracy.

224
00:14:03,380 --> 00:14:05,660
You know we might beat 93 percent.

225
00:14:05,660 --> 00:14:08,580
So that's what on the menu for this end of Part 3.

226
00:14:08,720 --> 00:14:12,280
So I can't wait to see you in the next part to build first.

227
00:14:12,380 --> 00:14:14,250
The decision tree classification model.

228
00:14:14,330 --> 00:14:17,620
And then finally the random forest classification.

229
00:14:17,720 --> 00:14:19,460
And until then enjoy machine learning.
