1

00:00:00,270  -->  00:00:06,750
Hello and welcome to this or to Tauriel today we're going to implement the naive base algorithm on our

2

00:00:06,770  -->  00:00:06,960
.

3

00:00:07,140  -->  00:00:07,930
So let's start right now.

4

00:00:07,950  -->  00:00:10,970
Let's first set the folder as working directory.

5

00:00:11,100  -->  00:00:16,090
So I'm going to Part 3 classification and that's the folder.

6

00:00:16,320  -->  00:00:20,760
So make sure you have the social network and see the file and if that's the case click on this more

7

00:00:20,760  -->  00:00:23,890
about I'm here to set this folder as working directory.

8

00:00:24,200  -->  00:00:28,240
And now let's start implementing names based on our.

9

00:00:28,440  -->  00:00:30,880
So we're going to take our classification templates.

10

00:00:30,930  -->  00:00:41,820
We're going to take everything from here to here and then paste right here and now we just need to change

11

00:00:41,910  -->  00:00:43,220
a few things.

12

00:00:43,230  -->  00:00:47,480
So first let's change the titles or usual so that we don't forget.

13

00:00:47,490  -->  00:00:51,440
So here we are with the Navy base classifier.

14

00:00:51,660  -->  00:00:54,370
Same for the training set.

15

00:00:54,510  -->  00:00:57,250
So that is plotting the title in on graphs.

16

00:00:57,360  -->  00:01:04,150
And now what we all need to do is to create our Navy base classified.

17

00:01:04,530  -->  00:01:06,180
So let's do this.

18

00:01:06,180  -->  00:01:09,850
Going to remove this and now what are we going to do.

19

00:01:10,020  -->  00:01:16,440
Well it's actually funny because we're going to use a library that we already used before but for different

20

00:01:16,460  -->  00:01:20,370
classify it it's to be 10 71 library.

21

00:01:20,760  -->  00:01:26,100
So there is no surprise why this library is very popular one of the most popular ones because it contains

22

00:01:26,190  -->  00:01:27,390
a lot of tools.

23

00:01:27,390  -->  00:01:31,320
Tools for SVM and tools for Navy base.

24

00:01:31,560  -->  00:01:33,840
So here again we'll use this library.

25

00:01:33,840  -->  00:01:37,460
So for those of you who didn't follow the SVM tutorials on our.

26

00:01:37,530  -->  00:01:44,530
And for those of you who are starting or for the first time maybe you don't have the 1871 packages in

27

00:01:44,540  -->  00:01:45,990
your packages list.

28

00:01:46,140  -->  00:01:49,820
So if that's the case you just need to type the command here.

29

00:01:49,820  -->  00:01:57,750
Install that packages and then in quotes here in the parenthesis you just add the name of the library

30

00:01:57,750  -->  00:02:04,710
which is 10 71 and then you just need to select this and execute to install the package.

31

00:02:04,710  -->  00:02:10,180
I won't do it here because I already have mine installed but I promise you it will work properly.

32

00:02:10,530  -->  00:02:13,700
So I'm just going to put that in comment.

33

00:02:13,710  -->  00:02:14,460
Here we go.

34

00:02:14,610  -->  00:02:21,150
And now let's just also import the library that is to selected in the package is this because it's selected

35

00:02:21,180  -->  00:02:22,790
but it might not always be the case.

36

00:02:22,800  -->  00:02:27,930
And if you want to automate your machine learning scripts it's better to always have this script line

37

00:02:27,930  -->  00:02:33,540
here that selects to your package just through a library it tenso anyone.

38

00:02:33,540  -->  00:02:37,420
And now we are ready to start creating our naiv by as class 5.

39

00:02:37,860  -->  00:02:39,260
OK so let's do this.

40

00:02:39,330  -->  00:02:42,080
As usual we will create a variable classifier

41

00:02:44,670  -->  00:02:52,590
and then we will use the name base function of the intense 71 library the Eatons only one library contains

42

00:02:52,590  -->  00:02:57,550
a lot of functions and one of them is to create a nice Bies classifier.

43

00:02:58,000  -->  00:03:00,980
OK so careful with the capitals.

44

00:03:01,020  -->  00:03:04,740
So it's not capital and that capital B.

45

00:03:04,850  -->  00:03:05,090
Right.

46

00:03:05,100  -->  00:03:10,920
But then you know are helping with the terms so you will have this naive box popping up here so you

47

00:03:10,920  -->  00:03:12,430
just have to press enter.

48

00:03:12,870  -->  00:03:16,430
And now let's input the parameters so the parameters what are they.

49

00:03:16,440  -->  00:03:20,100
Let's look at them we press one here to have a look.

50

00:03:20,760  -->  00:03:22,280
And here they are.

51

00:03:22,470  -->  00:03:27,120
So we will actually need to input only the two first arguments.

52

00:03:27,120  -->  00:03:32,770
We want you to put the formula like for the other one it will work perfectly well with only the first

53

00:03:32,770  -->  00:03:33,630
two ones.

54

00:03:33,780  -->  00:03:39,460
So you'll see that input X here which is by the way the training set.

55

00:03:39,480  -->  00:03:44,550
So the training set but we need to remove the last column of our and said because X is actually the

56

00:03:44,550  -->  00:03:49,770
matrix features you see it's written here numeric matrix or data frame.

57

00:03:49,800  -->  00:03:56,260
And by that they mean the matrix of features that is the matrix of the independent variables are here

58

00:03:56,270  -->  00:03:56,440
.

59

00:03:56,550  -->  00:04:01,860
Since the training set contains both the independent variables and the dependent variable we need to

60

00:04:01,860  -->  00:04:03,490
exclude the dependent variable.

61

00:04:03,540  -->  00:04:10,560
So to do this we're going to add brackets here and remove the last column by inputting here a minus

62

00:04:10,680  -->  00:04:17,430
and then the index of the last column which is where we can see it's minus 3 so it's minus 3.

63

00:04:17,430  -->  00:04:19,360
All right so perfect.

64

00:04:19,530  -->  00:04:23,310
Then come up and then the next arguments.

65

00:04:23,340  -->  00:04:25,890
So you guess what the next argument is.

66

00:04:26,250  -->  00:04:31,620
Well of course to try to classify you we need to be in the Ben enviro's and the response and the responses

67

00:04:31,620  -->  00:04:32,890
to been invaluable.

68

00:04:32,910  -->  00:04:36,180
So of course Y is going to be the dependent variable.

69

00:04:36,180  -->  00:04:39,210
So why the cause and to take it.

70

00:04:39,210  -->  00:04:47,480
We are going to take it this way we're going to put trainset dollar per chase.

71

00:04:47,520  -->  00:04:53,030
So I'm choosing to write it this way because we can clearly see that we are taking the dependent variable

72

00:04:53,030  -->  00:04:53,880
purchased.

73

00:04:54,030  -->  00:04:54,700
All right.

74

00:04:54,720  -->  00:04:57,810
And that's all you will see that it will work perfectly well.

75

00:04:57,810  -->  00:05:00,040
So I'll show you right now.

76

00:05:00,180  -->  00:05:01,940
Let's select the.

77

00:05:02,010  -->  00:05:02,490
Sorry.

78

00:05:02,520  -->  00:05:07,370
So before we have to select what's above which is all the preprocessing steps.

79

00:05:07,500  -->  00:05:14,170
So command and control plus and to execute all right pre-processing well done we are good to go.

80

00:05:14,400  -->  00:05:16,930
And now we can create our classifier.

81

00:05:17,190  -->  00:05:19,100
So yes let's do this.

82

00:05:19,170  -->  00:05:23,870
If you're tens of any one package is not selected you need to select that as well.

83

00:05:23,910  -->  00:05:26,320
So commander control us enter to execute.

84

00:05:26,510  -->  00:05:35,940
And here is our classifier arson So as you can see it works perfectly well the classifier as well created

85

00:05:36,180  -->  00:05:43,260
using only those two informations and when you think about it well this is all we need to train a classifier

86

00:05:43,260  -->  00:05:49,350
because we need the information of the independent variables and the information of the dependent variable

87

00:05:49,530  -->  00:05:55,260
so that it can learn the correlations between the independent variables and the dependent variable.

88

00:05:55,260  -->  00:05:57,020
All right so that makes sense.

89

00:05:57,270  -->  00:06:03,000
And now we can create our vector of prediction y pred using the Predict function on our classifier and

90

00:06:03,000  -->  00:06:05,490
on our new data which is the test set.

91

00:06:05,490  -->  00:06:07,150
So here we go.

92

00:06:07,170  -->  00:06:08,290
White bread is great.

93

00:06:08,310  -->  00:06:09,910
Let's have a quick look.

94

00:06:10,080  -->  00:06:11,430
Why bread.

95

00:06:11,910  -->  00:06:14,790
So for the first time we're obtaining something here.

96

00:06:14,790  -->  00:06:21,150
Remember before when we entered Why present the console we have all the predictions listed in the console

97

00:06:21,150  -->  00:06:21,450
.

98

00:06:21,450  -->  00:06:24,430
But here we have this factor zero here.

99

00:06:24,960  -->  00:06:27,960
So that means that white bread is a vector of factors.

100

00:06:28,110  -->  00:06:29,560
But with no factor.

101

00:06:29,670  -->  00:06:31,640
So it's basically an empty vector.

102

00:06:31,920  -->  00:06:37,860
And that is because the Navy base function of the of any one library didn't recognize our dependent

103

00:06:37,860  -->  00:06:43,560
variable vector purchased as a categorical variable with 0 and 1 factors.

104

00:06:43,560  -->  00:06:49,200
So for the libraries and functions we have been using recognized the dependent variable as factors.

105

00:06:49,290  -->  00:06:54,450
So we didn't have any issues with our predictions and we didn't have to include the dependent variable

106

00:06:54,450  -->  00:06:56,290
purchased as factors.

107

00:06:56,430  -->  00:06:58,830
But here with the base it's not the case.

108

00:06:58,950  -->  00:07:02,580
It doesn't recognize the dependent variable purchased as factors.

109

00:07:02,580  -->  00:07:08,790
So we need to encode it as factors because as you can see if we try to compute the confusion matrix

110

00:07:08,790  -->  00:07:12,180
below we will get the following error message.

111

00:07:12,240  -->  00:07:14,460
All arguments must have the same length.

112

00:07:14,730  -->  00:07:20,010
Well yes indeed we have a problem because the two arguments here are test set three.

113

00:07:20,010  -->  00:07:22,730
That is the third call and purchase of the test set.

114

00:07:23,040  -->  00:07:24,970
And then we spread it.

115

00:07:25,020  -->  00:07:31,050
So since this set 3 has links 100 and white bread has length zero because it's an empty vector with

116

00:07:31,050  -->  00:07:35,660
no vectors then obviously we can not compute any confusion matrix.

117

00:07:35,970  -->  00:07:42,150
So what we need to do is to jump back up to the pre-processing first step to encode our dependent variable

118

00:07:42,150  -->  00:07:49,230
purchased as factors then the based function will recognize the dependent variable factors and will

119

00:07:49,230  -->  00:07:55,890
perfectly be able to create a classifier that will allow to predict function to return the expected

120

00:07:55,890  -->  00:07:57,540
vector of predictions y.

121

00:07:57,780  -->  00:08:03,530
So it's great that you see this error because this is a classic error in machinery in our and that way

122

00:08:03,540  -->  00:08:09,100
from now on you will make this error in the future or if you make it you'll know how to fix it.

123

00:08:10,160  -->  00:08:15,300
So let's do it right now let's add a new section here and that's called it and coding

124

00:08:18,420  -->  00:08:19,260
the targets

125

00:08:21,750  -->  00:08:25,250
feature X Factor.

126

00:08:26,100  -->  00:08:30,940
All right and now let's vectorize or call them purchased.

127

00:08:31,180  -->  00:08:32,190
OK let's do this.

128

00:08:32,370  -->  00:08:35,010
So we will take it from start.

129

00:08:35,010  -->  00:08:40,590
That means that we will take the purchased dependent variable column of the data set and then we will

130

00:08:40,590  -->  00:08:45,720
recompute all of these to set this purchase column everywhere for all our sets.

131

00:08:45,720  -->  00:08:47,960
There is a training set and the test.

132

00:08:48,210  -->  00:08:48,450
OK.

133

00:08:48,450  -->  00:08:55,440
So we're going to take our last call in from our data set so we're going to type data set dollar purchased

134

00:08:56,760  -->  00:09:02,640
rights equals factor in parenthesis dataset.

135

00:09:02,700  -->  00:09:07,870
And then again we're taking the purchased call and that is a dependent variable column of our dataset

136

00:09:07,920  -->  00:09:07,950
.

137

00:09:07,950  -->  00:09:15,180
So again we're taking the dollar sign here purchased to take this column and then here we will just

138

00:09:15,180  -->  00:09:21,490
specify the levels or levels are see 0 and 1.

139

00:09:21,500  -->  00:09:22,330
All right.

140

00:09:22,590  -->  00:09:23,230
And that's it.

141

00:09:23,280  -->  00:09:29,490
That's how you encode your dependent variable convert chased into factors and that's actually what we

142

00:09:29,490  -->  00:09:35,760
saw in Botwin data pre-processing when I explained how to encode a categorical variable and set it as

143

00:09:35,760  -->  00:09:36,490
factors.

144

00:09:36,660  -->  00:09:41,100
Well here we have to do it and actually we are going to leave that in the template we're going to leave

145

00:09:41,400  -->  00:09:48,340
this encoding the target featurettes factor because even if we encode the debate envoi will come for

146

00:09:48,360  -->  00:09:55,110
other classifiers like logistic regression as VM and the other classifiers then it will even be better

147

00:09:55,110  -->  00:10:01,680
to have this purchase Culham and codices factors so that we can know for sure that our recognizes it

148

00:10:01,890  -->  00:10:03,280
as a factor.

149

00:10:03,360  -->  00:10:09,300
OK for some classifiers we don't need to specify it but let's do this anyway so that we never get any

150

00:10:09,300  -->  00:10:10,350
error.

151

00:10:10,350  -->  00:10:12,610
All right so we're going to start from scratch now.

152

00:10:12,630  -->  00:10:16,820
So to do this we can keep everything so that's where we're going to do everything here.

153

00:10:16,980  -->  00:10:24,510
So we're doing the data clearing the script here some pressing control Al to clear and that's it.

154

00:10:24,540  -->  00:10:29,330
And now let's do the whole thing and you're going to see that we won't get the error.

155

00:10:29,520  -->  00:10:32,820
So let's select this case.

156

00:10:32,830  -->  00:10:35,240
That's all the data pre-processing steps with.

157

00:10:35,250  -->  00:10:39,330
Now the encoding included so Exiguus.

158

00:10:39,480  -->  00:10:46,020
All right now we're going to fit the base model to the trainset executes.

159

00:10:46,200  -->  00:10:47,290
All good.

160

00:10:47,370  -->  00:10:50,830
Now we're going to create our vector of predictions why Pred.

161

00:10:51,210  -->  00:10:58,530
OK you're going to see that if I type Y pred here we will not have the predictions so we can compare

162

00:10:58,530  -->  00:11:04,860
it to Y said which is this third column of the test set K so we can compare them but let's not do this

163

00:11:04,860  -->  00:11:11,100
let's just get to the point where I want to show you that now a confusion matrix is going to be created

164

00:11:11,100  -->  00:11:12,160
without any error.

165

00:11:12,210  -->  00:11:15,010
So let's select this execute.

166

00:11:15,030  -->  00:11:20,140
And now as you can see that confusion matrix is created without any problem.

167

00:11:20,160  -->  00:11:27,180
So now we can have a look to see the number of incorrect predictions which is 6:53 equals 14 incorrect

168

00:11:27,180  -->  00:11:31,500
predictions not bad out of the 100 observations of the test set.

169

00:11:31,590  -->  00:11:32,560
Okay great.

170

00:11:32,850  -->  00:11:37,070
And now we're finally getting to the fun part which is to visualize the results.

171

00:11:37,080  -->  00:11:42,540
So if you didn't follow the Python tutorials a good exercise for you is to try to predict what's going

172

00:11:42,540  -->  00:11:48,570
to happen predict what you're about to see based on the intuitions that really explain to you.

173

00:11:48,570  -->  00:11:51,660
Can you guess what prediction regions are going to be.

174

00:11:51,750  -->  00:11:54,200
And especially the prediction boundary.

175

00:11:54,240  -->  00:11:55,780
Can you guess what it's going to be.

176

00:11:55,860  -->  00:12:01,620
Is the Navy Bies classify a linear classifier and then it will be a straight line or is the nave by

177

00:12:01,620  -->  00:12:09,060
specifier a narrowcast variable then is the prediction boundary going to be a smooth curve or a prediction

178

00:12:09,060  -->  00:12:12,730
boundary with a lot of irregularities like for Canon.

179

00:12:12,990  -->  00:12:19,110
That's a good exercise you know to try to practice your intuition that you have just built you it to

180

00:12:19,110  -->  00:12:20,210
practice it now.

181

00:12:20,450  -->  00:12:20,760
OK.

182

00:12:20,760  -->  00:12:27,210
So I'll just you can pause on the video and now I'll just select this to show you if your prediction

183

00:12:27,210  -->  00:12:28,500
is correct.

184

00:12:28,560  -->  00:12:29,280
So let's see.

185

00:12:29,280  -->  00:12:36,510
Command and control and to to execute.

186

00:12:40,320  -->  00:12:44,190
And there is our Navy base graphic resolve.

187

00:12:44,340  -->  00:12:48,910
Isn't it beautiful how this prediction boundary is a smooth curve.

188

00:12:48,990  -->  00:12:54,370
Classifying quite well the data sets that is a data set of observations.

189

00:12:54,420  -->  00:12:56,010
None of them are separable.

190

00:12:56,010  -->  00:12:58,220
It's kind of like the kernel as curve.

191

00:12:58,220  -->  00:13:03,600
You know it's a beautiful smooth curve that manages to catch those green users that couldn't be caught

192

00:13:04,080  -->  00:13:09,960
by linear classifiers because we had the straight line and therefore it couldn't catch the green users

193

00:13:09,960  -->  00:13:11,890
here and put them in a green category.

194

00:13:11,950  -->  00:13:17,730
Were in the red category that thinks this curve can see that it's making less incorrect predictions

195

00:13:17,760  -->  00:13:23,360
but still some like those one two three here regarding these users here.

196

00:13:23,360  -->  00:13:29,670
These older users with no estimated salary that's where incorrectly predicted by the linear classifiers

197

00:13:30,270  -->  00:13:36,000
and then however still making some few mistakes here we would have liked to have a lower curve here

198

00:13:36,000  -->  00:13:37,630
like the curve starting from here.

199

00:13:37,830  -->  00:13:39,660
Well that's what they can do here.

200

00:13:39,660  -->  00:13:41,750
And that's already quite a good job.

201

00:13:42,060  -->  00:13:45,790
So now let's see what it does on the test results.

202

00:13:45,900  -->  00:13:48,390
Here we have the test result code.

203

00:13:48,530  -->  00:13:52,080
Let's execute it.

204

00:13:54,990  -->  00:13:56,450
And here is a test.

205

00:13:56,720  -->  00:14:01,920
If the execution of this code is taking too much time you can try to take a lower resolution because

206

00:14:01,920  -->  00:14:07,100
right now you can see that we have a very high resolution with this by 0.01 step here.

207

00:14:07,170  -->  00:14:10,420
We cannot see the pixels here thanks to this resolution.

208

00:14:10,530  -->  00:14:16,260
If you take no point one resolution this crate will execute much faster but then you will see the pixel

209

00:14:16,260  -->  00:14:17,730
point so it's as you want.

210

00:14:17,730  -->  00:14:21,380
So the test results are actually not bad as well.

211

00:14:21,450  -->  00:14:27,660
It did a pretty good job classifying this green users here to the right grid category but still some

212

00:14:27,720  -->  00:14:33,250
indirect predictions resist since those green points here stay in the red region.

213

00:14:33,690  -->  00:14:36,690
All right so that's the name based graphic results.

214

00:14:36,690  -->  00:14:38,280
I hope you enjoyed what you saw.

215

00:14:38,280  -->  00:14:41,430
We're going to have other different surprises of other categories.

216

00:14:41,430  -->  00:14:46,470
You'll see that we will get very different kind of prediction of boundaries when we look at the decision

217

00:14:46,470  -->  00:14:49,460
trees classifiers and the random forest fires.

218

00:14:49,680  -->  00:14:51,620
So I look forward to showing that to you.

219

00:14:51,660  -->  00:14:53,490
And until that enjoy machine learning
