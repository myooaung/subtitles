1
00:00:00,330 --> 00:00:01,200
Hello my friends.

2
00:00:01,200 --> 00:00:02,810
Welcome to this new tutorial.

3
00:00:02,820 --> 00:00:07,140
And now we're about to train that as we are moral on the whole data set.

4
00:00:07,290 --> 00:00:15,030
After a successful data repricing phase with your new experience in feature Skilling because now you

5
00:00:15,030 --> 00:00:18,090
can handle very well all the different situations.

6
00:00:18,090 --> 00:00:23,880
All right so let's train that as your model on the whole data set of course because we did not do a

7
00:00:23,880 --> 00:00:27,260
split of the data set between training and test sets.

8
00:00:27,270 --> 00:00:28,040
So there you go.

9
00:00:28,050 --> 00:00:30,170
Let's create a new coat cell.

10
00:00:30,180 --> 00:00:35,390
And now let's build of course in the most efficient way that as we are moral.

11
00:00:35,430 --> 00:00:37,890
So we're gonna build it with cycle learn of course.

12
00:00:37,920 --> 00:00:43,350
I remind that cycle learn is the best data science library excluding deep learning because of course

13
00:00:43,350 --> 00:00:44,980
we have tensor flown by torch.

14
00:00:45,000 --> 00:00:49,070
But for any machinery model that is not based on neural networks.

15
00:00:49,230 --> 00:00:54,870
Well psychic learns is definitely by far the best data science library and for what we are interested

16
00:00:54,870 --> 00:00:56,770
in right now you know that as your model.

17
00:00:56,850 --> 00:01:04,050
Well we will build it with a class called as we are as simple as that which belongs to a module of psychic

18
00:01:04,050 --> 00:01:05,890
learn called SVM.

19
00:01:05,940 --> 00:01:13,420
And so we have to start here from first psychic learn to assess the psychiatric library from which we

20
00:01:13,430 --> 00:01:17,400
are going to get access to that as the end modules.

21
00:01:17,400 --> 00:01:23,610
That's the module from which we're going to import that is the are class.

22
00:01:23,640 --> 00:01:24,540
Perfect.

23
00:01:24,540 --> 00:01:30,600
Now we have the class and you of course know the next natural step any time you import a class next

24
00:01:30,600 --> 00:01:36,260
natural step is of course to create an object or an instance of this class.

25
00:01:36,360 --> 00:01:40,260
And we're going to call that object regress or just like before.

26
00:01:40,350 --> 00:01:48,140
Because this instance or object of the is your class is nothing else than that as we are regressing

27
00:01:48,180 --> 00:01:48,660
itself.

28
00:01:48,660 --> 00:01:53,210
You know the support vector for regression regressive so regressive.

29
00:01:53,240 --> 00:01:59,760
And then of course we are going to call this class as we are at some parenthesis and this time we have

30
00:01:59,760 --> 00:02:06,420
to input a parameter because indeed remember in the intuition lecture of Support Vector regression and

31
00:02:06,450 --> 00:02:12,880
you will also see in the intuition lecture of the support vector machine in the next part but reclassification.

32
00:02:13,050 --> 00:02:19,050
Well you actually have what we call kernels which can either learn some linear relationships and that's

33
00:02:19,050 --> 00:02:26,100
the linear kernel or nonlinear relationships in your dataset which are the nonlinear kernel such as

34
00:02:26,280 --> 00:02:28,020
the RPF radial basis.

35
00:02:28,110 --> 00:02:30,470
I will actually show it to you right away.

36
00:02:30,480 --> 00:02:37,230
You know that's the Gaussian RPF kernel which is given by this formula and I can actually show you the

37
00:02:37,230 --> 00:02:39,270
plot of that kernel here it is.

38
00:02:39,270 --> 00:02:44,790
So that's the Gaussian RPF kernel and then you also have some other kernels which I've prepared here

39
00:02:44,790 --> 00:02:50,790
on this great Web site showing clearly the different kernels of the support vector machine but also

40
00:02:50,790 --> 00:02:56,760
that as we are because as we are is nothing else than a support vector machine model for regression.

41
00:02:56,790 --> 00:02:58,050
So let's see here they are.

42
00:02:58,050 --> 00:03:01,610
There is the pull in the mill kernel adapted for non-linear dataset.

43
00:03:01,680 --> 00:03:06,960
You have the Gaussian kernel which has a classic Gaussian function than the Gaussian radial basis function.

44
00:03:06,960 --> 00:03:11,070
The most widely used one and that's actually the one we will use.

45
00:03:11,070 --> 00:03:14,310
Then you have the plus one the hyperbolic tangent kernel as well.

46
00:03:14,320 --> 00:03:18,360
That's a popular one Sigma in well you have all of them here.

47
00:03:18,420 --> 00:03:21,000
And so if you're curious yes you can have a look at them.

48
00:03:21,090 --> 00:03:26,920
But the one we will use for our implementation will be and that's the one I recommend.

49
00:03:26,970 --> 00:03:33,700
Each time you experiment with an as your model and that the radial basis function kernel the RPF kernel.

50
00:03:33,870 --> 00:03:37,290
And that's exactly what we have to input here in our parameters.

51
00:03:37,290 --> 00:03:39,730
And so the name of that parameter is kernel.

52
00:03:39,790 --> 00:03:44,940
So that's the name of the parameter and then the value we want for that parameter which corresponds

53
00:03:44,940 --> 00:03:51,050
to the radial basis function has the code name in quotes are the F and that's it.

54
00:03:51,160 --> 00:03:53,880
That's only what we have to input here.

55
00:03:53,880 --> 00:03:59,130
So that basically creates the energy our model with the radial basis function kernel.

56
00:03:59,220 --> 00:04:04,440
And so now that means only one thing that means that we already have built the model we already have

57
00:04:04,440 --> 00:04:06,070
the SVR model itself.

58
00:04:06,120 --> 00:04:13,090
And so now the last final natural step is to of course train that regressive on the whole dataset.

59
00:04:13,110 --> 00:04:13,410
Right.

60
00:04:13,410 --> 00:04:14,670
Which is also the training set.

61
00:04:14,850 --> 00:04:15,100
Okay.

62
00:04:15,120 --> 00:04:16,470
So let's do this.

63
00:04:16,470 --> 00:04:21,030
You know exactly how to do this you know from this point it's exactly the same as before you know the

64
00:04:21,030 --> 00:04:28,350
same function which is of course the fit function and which will train this as we are model on your

65
00:04:28,350 --> 00:04:29,720
whole data set.

66
00:04:29,730 --> 00:04:30,090
Right.

67
00:04:30,090 --> 00:04:35,700
To learn the correlations between the position levels and the salaries and all this done with the radial

68
00:04:35,700 --> 00:04:37,080
basis function Colonel.

69
00:04:37,080 --> 00:04:38,040
All right so let's do this.

70
00:04:38,040 --> 00:04:40,030
Let's train are regressive.

71
00:04:40,410 --> 00:04:47,700
So we take a regressive object first and as usual we add here a dot and then the fit method which takes

72
00:04:47,700 --> 00:04:53,670
as input this time not extraneous why train because we have not created a separate training set but

73
00:04:53,670 --> 00:04:59,550
of course this time the whole dataset which remember was feature scaled you know both on the matrix

74
00:04:59,550 --> 00:05:03,570
of features parte and the dependent variable y part.

75
00:05:03,590 --> 00:05:08,580
And so that's exactly what we have to input here both X and Y.

76
00:05:08,720 --> 00:05:09,620
And that's it.

77
00:05:09,620 --> 00:05:10,850
Congratulations.

78
00:05:10,850 --> 00:05:17,690
Now you know how to build and train and as your model after a successful date of repricing phase including

79
00:05:17,810 --> 00:05:20,050
special feature scaling.

80
00:05:20,120 --> 00:05:20,630
All right.

81
00:05:20,630 --> 00:05:24,650
And so now we have a very interesting next step.

82
00:05:24,650 --> 00:05:30,230
It is the step where we predict the new result and I'm not saying this because I'm looking forward to

83
00:05:30,230 --> 00:05:32,840
seeing the result of that as your prediction.

84
00:05:32,840 --> 00:05:35,640
You know compared to the polynomial regression prediction.

85
00:05:35,720 --> 00:05:41,660
Know what I mean is that this next step is very interesting because it will teach you how to reverse

86
00:05:41,930 --> 00:05:48,320
the scaling of your prediction because you will see that when we apply the predict method to predict

87
00:05:48,530 --> 00:05:53,470
this new result well it will be returned in the scale that was used for Y.

88
00:05:53,510 --> 00:05:53,960
Right.

89
00:05:53,960 --> 00:05:59,450
The new skill of y after it was transformed and so we'll have to reverse this transformation will have

90
00:05:59,450 --> 00:06:05,540
to reverse the scaling in order to get the original scale of y and I will teach you exactly how to do

91
00:06:05,540 --> 00:06:05,940
it.

92
00:06:05,990 --> 00:06:11,270
Of course you can try to do it on your own you know by looking at the documentation which method can

93
00:06:11,390 --> 00:06:16,910
do this you reverse the scaling and you know if you figure it out well you should be able to predict

94
00:06:17,000 --> 00:06:18,070
that salary.

95
00:06:18,200 --> 00:06:19,930
But I'm warning you it's not that direct.

96
00:06:20,120 --> 00:06:24,990
So there you go as soon as you're ready let us implement that solution in the next sartorial.

97
00:06:25,110 --> 00:06:26,920
And until then enjoy machine learning.
