WEBVTT
1
00:00:00.240 --> 00:00:03.810
Hello my friends and welcome to this new practical activity.

2
00:00:03.840 --> 00:00:06.720
On this time the Colonel as a model.

3
00:00:06.720 --> 00:00:12.000
So in the previous section we implemented the classic as we moral meaning the support vector machine

4
00:00:12.150 --> 00:00:13.860
with a linear kernel.

5
00:00:14.130 --> 00:00:20.160
And now we're about to see what we get with a nonlinear kernel like the radial basis function or another

6
00:00:20.160 --> 00:00:20.430
one.

7
00:00:20.460 --> 00:00:22.290
So that will be very exciting.

8
00:00:22.290 --> 00:00:28.230
And mostly what will be very exciting to see is if we manage to beat the previous accuracies we got

9
00:00:28.230 --> 00:00:29.000
previously.

10
00:00:29.100 --> 00:00:34.870
I remind that the best one we've got so far was 93 percent from the key and moral.

11
00:00:34.890 --> 00:00:41.520
And now let's see if by adding a nonlinear kernel to the SVM Well we get a better accuracy than that.

12
00:00:41.790 --> 00:00:42.360
Grit.

13
00:00:42.360 --> 00:00:46.080
So as usual I want to make sure that everyone here is on the same page.

14
00:00:46.110 --> 00:00:49.470
I give you the link to this further in the previous tutorial in the articles.

15
00:00:49.470 --> 00:00:51.010
Make sure to connect to it.

16
00:00:51.150 --> 00:00:58.070
And if that's the case well please follow me into Part 3 classification to implement the kernel as in

17
00:00:58.200 --> 00:01:01.170
moral and as usual we're going to start with Python.

18
00:01:01.170 --> 00:01:04.640
And here we are with our two files.

19
00:01:04.710 --> 00:01:10.400
The first one that contains the whole implementation of the kernel as immoral in the API any format.

20
00:01:10.500 --> 00:01:16.440
And the second one the data set that same dataset on which we trained and will train all the classification

21
00:01:16.440 --> 00:01:17.830
models of this part.

22
00:01:17.880 --> 00:01:19.840
So let's have a quick look at it again.

23
00:01:19.950 --> 00:01:25.840
This dataset contains some information of 400 customers represented by each row here.

24
00:01:25.840 --> 00:01:31.410
So each row corresponds to a customer and for each of these customers well we have the age which is

25
00:01:31.410 --> 00:01:37.220
a first feature The estimated salary which is a second feature and the dependent variable purchased

26
00:01:37.380 --> 00:01:42.960
which tells whether or not they bought no or yes previous SUV.

27
00:01:43.020 --> 00:01:43.470
All right.

28
00:01:43.470 --> 00:01:48.720
And so in our practical activities of this part three classification we build several classification

29
00:01:48.720 --> 00:01:54.060
models to understand the correlations between these two features here and this dependent variable per

30
00:01:54.060 --> 00:02:01.320
chased in order to predict which future customers will buy brand new luxury SUV that is advertised on

31
00:02:01.320 --> 00:02:02.370
social networks.

32
00:02:02.370 --> 00:02:02.920
All right.

33
00:02:03.000 --> 00:02:05.120
So still the same story and now.

34
00:02:05.250 --> 00:02:06.410
Well let's begin.

35
00:02:06.410 --> 00:02:11.340
Ah Colonel as VM implementation I'm going to open it with Google collaborator.

36
00:02:11.370 --> 00:02:14.490
But you had the choice to open it with Gebran notebook as well.

37
00:02:14.490 --> 00:02:19.620
Choose your favorite and now it is opening the notebook loading it laying it out.

38
00:02:19.660 --> 00:02:21.180
And there we go.

39
00:02:21.180 --> 00:02:22.810
Welcome to the kernel.

40
00:02:22.830 --> 00:02:24.350
As VM implementation.

41
00:02:25.050 --> 00:02:25.500
All right.

42
00:02:25.500 --> 00:02:30.480
And of course it results from the exact same classification template which we made in the first section

43
00:02:30.480 --> 00:02:31.150
of this part.

44
00:02:31.230 --> 00:02:37.560
Logistic Regression indeed when we built the logistic regression model we made at the same time a great

45
00:02:37.620 --> 00:02:44.490
classification template which allows us to only change one cell when building another classification

46
00:02:44.490 --> 00:02:50.790
model which is exactly you know the cell where we build and train this classification model on the training

47
00:02:50.790 --> 00:02:51.380
set.

48
00:02:51.390 --> 00:02:54.480
So this cell is the only one we have to modify here.

49
00:02:54.720 --> 00:02:58.120
However this notebook is in read only mode.

50
00:02:58.230 --> 00:03:04.020
So in order to modify the cell to build that kernel as we model we have to indeed click fell here in

51
00:03:04.020 --> 00:03:10.070
order to create a copy of this notebook and to do this interesting to click Save a copy and drive.

52
00:03:10.080 --> 00:03:17.680
This will create a copy of this notebook and in a second you'll have it opened on your machine.

53
00:03:17.700 --> 00:03:20.170
All right I'm going to put everything in that order.

54
00:03:20.310 --> 00:03:21.090
All right.

55
00:03:21.100 --> 00:03:22.370
And this is our copy.

56
00:03:22.410 --> 00:03:27.960
And so inside the only thing we have to re implement is indeed that cell where we build and train the

57
00:03:27.960 --> 00:03:30.550
kernel as a model on the training set.

58
00:03:30.570 --> 00:03:32.660
So let's not look at it.

59
00:03:32.760 --> 00:03:34.370
Let's put it in the trash.

60
00:03:34.410 --> 00:03:38.370
And now let's re implemented from scratch.

61
00:03:38.370 --> 00:03:38.850
All right.

62
00:03:38.880 --> 00:03:39.240
Perfect.

63
00:03:39.240 --> 00:03:42.760
So it's actually super super easy.

64
00:03:42.780 --> 00:03:49.980
We can even do it even more efficiently than you know the usual way when we went to the cyclone API

65
00:03:50.070 --> 00:03:55.340
and got the name of the function because as I explained in the previous section when building the original

66
00:03:55.430 --> 00:04:01.650
model the classic one well the way we built it was actually to you know build an object of this class

67
00:04:01.710 --> 00:04:04.410
as we see and then choose a linear kernel.

68
00:04:04.800 --> 00:04:07.060
And now actually you know we're about to do the same.

69
00:04:07.080 --> 00:04:10.710
We're about to use the same class to build this kernel as VM objects.

70
00:04:10.920 --> 00:04:15.270
But this time instead of choosing a linear kernel we will choose a non-linear one.

71
00:04:15.390 --> 00:04:16.890
And that's the best example of it.

72
00:04:16.920 --> 00:04:23.760
You know the most commonly used it is the RPF radial basis function kernel and you also have some other

73
00:04:23.760 --> 00:04:26.910
ones like the polynomial kernel or the sigmoid kernel.

74
00:04:26.910 --> 00:04:29.730
But really this one is the best option to start with.

75
00:04:29.730 --> 00:04:35.880
And that's the one will choose for kernel as VM implementation as you might have guessed and therefore

76
00:04:35.970 --> 00:04:36.630
remember how.

77
00:04:36.630 --> 00:04:41.640
I also want to train you on how to juggle with the different tools of your machinery to get you know

78
00:04:41.640 --> 00:04:46.410
the data processing toolkit or the regression toolkit or even now the classifications will get containing

79
00:04:46.440 --> 00:04:47.990
all your classification models.

80
00:04:48.120 --> 00:04:55.980
The most efficient way now to build this kernel SVM model is just to go back to our SVM implementation

81
00:04:56.330 --> 00:05:03.720
then find that cell where we build that as VM indeed since we built that SVM with the same class as

82
00:05:03.720 --> 00:05:06.310
we see but choosing a linear kernel.

83
00:05:06.420 --> 00:05:08.300
Well now as simple as that.

84
00:05:08.310 --> 00:05:15.030
In order to build our kernel as VM We just need to take this code sell and paste it right here in a

85
00:05:15.030 --> 00:05:17.470
new code sale and then guess what.

86
00:05:17.490 --> 00:05:20.610
Just change that kernel parameter.

87
00:05:20.640 --> 00:05:21.310
All right.

88
00:05:21.390 --> 00:05:24.500
And as we said this time we're not using a linear kernel.

89
00:05:24.510 --> 00:05:30.960
We're gonna use an R B F kernel the radial basic function which was explained in the intuition lecturers.

90
00:05:31.090 --> 00:05:32.480
And there you go my friend.

91
00:05:32.580 --> 00:05:39.210
That implementation is over you see how I'm doing all these templates to make you as most efficient

92
00:05:39.300 --> 00:05:40.220
as possible.

93
00:05:40.230 --> 00:05:41.210
Well that's what I mean.

94
00:05:41.220 --> 00:05:46.550
Now we can juggle between our different codes different tools of machine learning and we can build new

95
00:05:46.560 --> 00:05:48.240
models in a flashlight.

96
00:05:48.240 --> 00:05:51.420
And that's what we just did with Colonel Azaria.

97
00:05:51.430 --> 00:05:51.750
All right.

98
00:05:51.750 --> 00:05:53.750
So basically we're done we're ready.

99
00:05:53.760 --> 00:05:59.520
So the next natural step is to upload the data set inside the notebook and to do this you need to click

100
00:05:59.520 --> 00:06:01.170
this for the button here.

101
00:06:01.290 --> 00:06:07.170
Then your notebook will be connecting to a runtime to enable file browsing still the same story in a

102
00:06:07.170 --> 00:06:07.800
few seconds.

103
00:06:07.800 --> 00:06:10.710
We should be able to see the upload button.

104
00:06:10.710 --> 00:06:11.480
Perfect.

105
00:06:11.490 --> 00:06:12.130
There we go.

106
00:06:12.400 --> 00:06:18.180
And so now we're going to click it and we will directly get access to social network as if you know

107
00:06:18.390 --> 00:06:20.310
you were here in the SVM section.

108
00:06:20.310 --> 00:06:22.390
But let me show you the path once again.

109
00:06:22.530 --> 00:06:23.700
So please find for us.

110
00:06:23.730 --> 00:06:30.930
Your whole machinery is it further than please go to part 3 then Section 17 current as VM then python

111
00:06:31.170 --> 00:06:39.060
and then social network at that see as we perfect it will upload upload the set inside the notebook

112
00:06:39.450 --> 00:06:40.190
and now.

113
00:06:40.230 --> 00:06:46.700
Now we can run everything by clicking runtime here and then run 0 run.

114
00:06:46.710 --> 00:06:52.270
All this will run all the cells including this one which will get us a new model with a new non-linear

115
00:06:52.290 --> 00:06:54.210
kernel Where is it right here.

116
00:06:54.270 --> 00:06:55.870
Kernel equals RPF.

117
00:06:56.100 --> 00:07:02.220
And so now now it's very interesting because this time we have a nonlinear classifier and therefore

118
00:07:02.280 --> 00:07:06.870
I look forward to showing you the visualization result at the end you know both for the train set and

119
00:07:06.900 --> 00:07:13.290
it tested because you will see that we will get a super beautiful curve that will catch the right observation

120
00:07:13.290 --> 00:07:18.920
points you the right red customers in the right red region and the right green customers in right.

121
00:07:18.930 --> 00:07:25.830
Green prediction region not all of them of course but the ones which couldn't get cut properly by the

122
00:07:25.830 --> 00:07:31.290
previous you know linear models such as the Support Vector Machine or the logistic regression model

123
00:07:31.470 --> 00:07:31.890
right.

124
00:07:31.890 --> 00:07:37.290
Remember this trait line the straight line could not catch these customer properly in the right green

125
00:07:37.290 --> 00:07:39.380
region and these ones as well.

126
00:07:39.450 --> 00:07:42.750
And so you will see that now we will get much better results.

127
00:07:42.750 --> 00:07:47.710
But first because you know right now I think it should be you know loading running the code yes.

128
00:07:47.730 --> 00:07:50.280
So we don't have yet the final result but that's okay.

129
00:07:50.290 --> 00:07:56.310
Let's observe you know the different predictions of first are kernel as VM was perfectly able to predict

130
00:07:56.310 --> 00:07:57.420
the right result here.

131
00:07:57.420 --> 00:08:03.300
Remember with that customer of age 30 an estimated salary eighty seven thousand dollars.

132
00:08:03.360 --> 00:08:08.730
Who in reality didn't buy indeed an SUV and he would get the same predictions.

133
00:08:08.730 --> 00:08:09.790
That's all good.

134
00:08:09.810 --> 00:08:12.210
Then let's look at the test results.

135
00:08:12.570 --> 00:08:13.130
All right.

136
00:08:13.230 --> 00:08:15.630
So let's crawl back up a bit.

137
00:08:15.840 --> 00:08:17.210
So that's the test results.

138
00:08:17.220 --> 00:08:22.190
And well once again we have many correct predictions an incorrect one here.

139
00:08:22.230 --> 00:08:27.870
The real result is zero meaning that in reality this particular customer didn't buy the SUV but our

140
00:08:27.870 --> 00:08:32.800
model predicted that discussing with the SUV then we have the same incorrect prediction here.

141
00:08:32.820 --> 00:08:35.880
Then all correct I'll correct it looks very good.

142
00:08:35.880 --> 00:08:38.100
Here we have an incorrect prediction the inverse one.

143
00:08:38.190 --> 00:08:44.530
In reality the customer but the SUV but our model predicted that the customer didn't buy the SUV at

144
00:08:44.620 --> 00:08:44.820
all.

145
00:08:44.820 --> 00:08:45.500
Correct.

146
00:08:45.510 --> 00:08:46.140
Correct.

147
00:08:46.170 --> 00:08:46.970
Correct.

148
00:08:46.980 --> 00:08:49.920
It looks very very very good right.

149
00:08:49.920 --> 00:08:56.110
I really wonder if we can beat this accuracy and that's what we're about to find out right now with

150
00:08:56.310 --> 00:08:57.750
that confusion matrix.

151
00:08:57.750 --> 00:08:58.410
Are you ready.

152
00:08:58.410 --> 00:08:59.590
Are we gonna beat that.

153
00:08:59.590 --> 00:09:08.100
So for best accuracy we got which was I remind 93 percent from the K nearest neighbor model.

154
00:09:08.100 --> 00:09:10.560
Here it is 93 percent.

155
00:09:10.560 --> 00:09:13.170
And now let's see if we managed to beat it.

156
00:09:13.170 --> 00:09:17.640
All right let's scroll down and the accuracy is 93 percent again.

157
00:09:17.690 --> 00:09:23.970
OK so we didn't beat it but we reached the same level as the K and then and now let's see if we get

158
00:09:23.970 --> 00:09:24.750
the curve.

159
00:09:24.750 --> 00:09:26.060
No still not.

160
00:09:26.060 --> 00:09:27.480
All right you know it's still running.

161
00:09:27.490 --> 00:09:30.410
That's because we have a small step here open 25.

162
00:09:30.420 --> 00:09:35.620
If you want this to run faster you can actually increase the step to open five or even 1 but you'll

163
00:09:35.640 --> 00:09:39.420
get a less smooth curve no less nice curve but that's OK.

164
00:09:39.960 --> 00:09:45.170
So you know just in case it will get us the results in a few minutes it took a long time.

165
00:09:45.180 --> 00:09:50.520
Well let us maybe have a look at them in the original implementation which is right here.

166
00:09:50.550 --> 00:09:52.250
OK just let's do that.

167
00:09:52.680 --> 00:09:54.430
Let's observe the final results.

168
00:09:54.450 --> 00:09:55.890
And here they are.

169
00:09:55.890 --> 00:10:03.150
As you can see we get a super nice prediction curve that catches this time perfectly well.

170
00:10:03.200 --> 00:10:08.540
Those green customers who couldn't get cut properly by the Linear classifiers you know with the straight

171
00:10:08.540 --> 00:10:09.200
line.

172
00:10:09.200 --> 00:10:13.400
This time it is catching them perfectly well except these ones of course because they're trapped in

173
00:10:13.400 --> 00:10:15.260
the middle of many red customers.

174
00:10:15.410 --> 00:10:20.810
But you know these ones which couldn't be cut properly by the logistic regression model or you know

175
00:10:20.810 --> 00:10:22.520
the SVM classifier.

176
00:10:22.520 --> 00:10:23.020
Right.

177
00:10:23.040 --> 00:10:30.350
If we look at it again remember that the customers here couldn't get cut properly because of that straight

178
00:10:30.350 --> 00:10:30.580
line.

179
00:10:30.590 --> 00:10:33.050
But now that we have a curve Oh there we go.

180
00:10:33.050 --> 00:10:35.050
We have the results now that we had this curve.

181
00:10:35.060 --> 00:10:41.540
Well these same customers you know these are exactly the same one these same customers are now cut properly

182
00:10:41.660 --> 00:10:42.850
in the green region.

183
00:10:43.100 --> 00:10:46.250
And same for these ones actually you know these ones are cut and right.

184
00:10:46.250 --> 00:10:49.420
Green region by very short and simple these ones.

185
00:10:49.430 --> 00:10:54.410
And here you know these ones are incorrect operations but once again that's all you find we want to

186
00:10:54.410 --> 00:11:01.880
avoid overfishing anyway because indeed what's mostly important are the results of the test which we

187
00:11:01.880 --> 00:11:07.850
still don't have it is still running but we should get them in a second or you know let's just have

188
00:11:07.850 --> 00:11:09.200
a look at them right here.

189
00:11:09.230 --> 00:11:15.230
So now the challenge is to see whether we have the same results on new observations on observations

190
00:11:15.440 --> 00:11:17.540
with which are more wasn't trained.

191
00:11:17.540 --> 00:11:23.180
And that's exactly the observations that said you know that 100 customers of the tested and well well

192
00:11:23.210 --> 00:11:26.140
here now it looks even better which makes sense right.

193
00:11:26.140 --> 00:11:31.610
Because we had ninety three percent accuracy and since they are 100 customers in a test we actually

194
00:11:31.610 --> 00:11:35.350
had 93 correct predictions and seven incorrect predictions.

195
00:11:35.360 --> 00:11:39.410
And we can actually count them here you know the seven incorrect positions you have the two first ones

196
00:11:39.410 --> 00:11:45.470
here the red customers falling in the wrong green prediction region then these two here two other red

197
00:11:45.470 --> 00:11:50.840
customers pulling in the wrong green prediction regions and then these three green customers falling

198
00:11:51.080 --> 00:11:54.840
in the wrong red prediction region but that's totally fine.

199
00:11:54.890 --> 00:12:00.650
Once again thanks to this beautiful curve here we managed to catch these customers in the right region

200
00:12:00.890 --> 00:12:06.050
which couldn't be the case previously because of the straight line resulting from the linear classifier

201
00:12:06.380 --> 00:12:06.830
this time.

202
00:12:06.830 --> 00:12:11.810
There we go we have a nonlinear classifier which is the reason why we had this curve and that's why

203
00:12:11.930 --> 00:12:14.240
we get a better accuracy.

204
00:12:14.240 --> 00:12:14.840
All right.

205
00:12:14.870 --> 00:12:20.840
So I'm at the same time very happy and excited because this curve is beautiful and we got great results

206
00:12:21.110 --> 00:12:28.130
but I'm still hoping that with a future classification model like the three ones we have left we can

207
00:12:28.220 --> 00:12:36.590
beat that 93 percent accuracy which we got so far as the best ones by two models that came nearest neighbors

208
00:12:36.830 --> 00:12:39.120
and the colonel SVM.

209
00:12:39.290 --> 00:12:42.810
So I can't wait to see you in the next section.

210
00:12:42.810 --> 00:12:50.330
You know the next practical activity to build this time the Navy base classification model to impatiently

211
00:12:50.450 --> 00:12:57.440
find out if it's going to beat 93 percent the accuracy of both the cane and and the Colonel as yet.

212
00:12:57.950 --> 00:13:01.020
So I look forward to seeing you in this next practical activity.

213
00:13:01.070 --> 00:13:03.190
And until then enjoy machine learning.
