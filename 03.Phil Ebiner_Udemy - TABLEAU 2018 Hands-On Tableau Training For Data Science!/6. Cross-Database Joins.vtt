WEBVTT
ï»¿1
00:00:04.010 --> 00:00:09.260
Hello and welcome back to the Course table and the press tutorial will learn how to create these clusters.

2
00:00:09.460 --> 00:00:14.920
And at the same time we also identified that even though they are telling us some sort of story and

3
00:00:14.920 --> 00:00:18.850
they are identifying some trends that we're also seeing with our naked eye.

4
00:00:19.000 --> 00:00:23.350
At the same time they might not be the final step in our analysis.

5
00:00:23.350 --> 00:00:27.420
Maybe we can somehow refine these clusters to have an even better story.

6
00:00:27.550 --> 00:00:29.500
And that's exactly what we're going to do today.

7
00:00:29.500 --> 00:00:36.370
So to start off with let's just step away from this analytics in Tablo for a second and let's just do

8
00:00:36.370 --> 00:00:39.010
some analytical and critical thinking for a bit.

9
00:00:39.040 --> 00:00:41.080
So what company are we working for.

10
00:00:41.080 --> 00:00:45.730
It's important to always remember the domain knowledge behind what you're doing.

11
00:00:45.730 --> 00:00:52.280
It's a very powerful skill in a sense that capacity and ability to understand the domain knowledge.

12
00:00:52.290 --> 00:00:58.540
So this company this company isn't just a company that produces parts for a factory.

13
00:00:58.540 --> 00:01:06.190
This company isn't just a financial services organization that gives out loans to countries around the

14
00:01:06.190 --> 00:01:06.940
world.

15
00:01:06.940 --> 00:01:09.050
This company deals with people.

16
00:01:09.070 --> 00:01:12.630
This company heavily actually relies on people.

17
00:01:12.640 --> 00:01:19.060
In fact this company what they do is they come to your house they collect your washing and then they

18
00:01:19.060 --> 00:01:21.190
take it to their store.

19
00:01:21.190 --> 00:01:23.430
They wash it for you and then they bring it back.

20
00:01:23.440 --> 00:01:27.180
So it's kind of like an outsourcing of your laundry.

21
00:01:27.220 --> 00:01:31.210
That's a service that they provide and that's been their entire business.

22
00:01:31.210 --> 00:01:38.650
So in this case as you can imagine their amount of work that they get is directly proportionate to the

23
00:01:38.650 --> 00:01:41.520
amount of people that they can potentially service.

24
00:01:41.560 --> 00:01:49.570
So what exactly I'm hinting at is population to put it very bluntly the amount of dirty clothes in the

25
00:01:49.570 --> 00:01:53.730
city is directly correlated to the population of that city.

26
00:01:54.010 --> 00:01:59.680
Other companies that service a population they might not have such a direct correlation of the amount

27
00:01:59.680 --> 00:02:01.410
of work to the amount of people.

28
00:02:01.570 --> 00:02:08.830
But for a laundry company the more people you have exactly proportionately the more you will have dirty

29
00:02:08.830 --> 00:02:10.050
laundry in that city.

30
00:02:10.240 --> 00:02:17.390
So population of a city might play actually a huge role in the analysis that we are about to perform

31
00:02:17.650 --> 00:02:23.830
and that is important to always remember and understand so what if these cities so this company it works

32
00:02:24.100 --> 00:02:26.790
all across different parts of the U.S..

33
00:02:27.010 --> 00:02:29.250
And as we discussed at the beginning.

34
00:02:29.260 --> 00:02:34.450
They can't afford to compete with the major players so they're not present in cities like New York like

35
00:02:34.450 --> 00:02:38.890
San Francisco Los Angeles like that were not present in the very big big cities.

36
00:02:38.890 --> 00:02:44.100
They're going for after the smaller cities still large cities of hundreds of thousands of population.

37
00:02:44.290 --> 00:02:52.510
But at the same time a bit smaller so what if what is going on here is somehow related to the size of

38
00:02:52.510 --> 00:02:57.130
the population maybe what if this trend that we're seeing here that even though they're investing more

39
00:02:57.130 --> 00:03:01.810
and more into marketing they're not getting more and more people or they're not getting more and more

40
00:03:01.810 --> 00:03:08.170
revenue maybe that is related to the fact that they've maxed out their capacity or their share of that

41
00:03:08.500 --> 00:03:11.260
population and associated city.

42
00:03:11.290 --> 00:03:13.590
Maybe there's some sort of correlation there.

43
00:03:13.780 --> 00:03:19.240
Maybe those marketing efforts that they're investing into whether they're online or off line.

44
00:03:19.300 --> 00:03:25.450
Maybe those marketing efforts have a limit of how much what purchase and what portion of the population

45
00:03:25.450 --> 00:03:27.050
they can bring.

46
00:03:27.190 --> 00:03:33.100
And in these specific cities maybe they're just smaller cities and maybe we'll see something going on

47
00:03:33.100 --> 00:03:40.870
like that there so is a very powerful skill to have to be able to bring in external data sets into your

48
00:03:40.870 --> 00:03:42.920
analysis to refine your analysis.

49
00:03:42.940 --> 00:03:44.790
And that's exactly what we're going to do now.

50
00:03:45.070 --> 00:03:52.060
So as you remember in our data said we had another file which was the population data set.

51
00:03:52.060 --> 00:03:57.750
So let's go ahead and find that from what I can do is I can connect to that file.

52
00:03:58.000 --> 00:04:03.970
I can create a new data source connection and as I remember it wasn't an Excel file was a CSP file so

53
00:04:03.970 --> 00:04:09.850
if I go to text you will see here among these files U.S. cities population.

54
00:04:09.880 --> 00:04:17.380
So this file contains information on the population of the cities of the top cities in the U.S. and

55
00:04:17.410 --> 00:04:20.640
among them all of our cities are among those cities.

56
00:04:20.650 --> 00:04:25.480
So what we could do is we could create a new connection and then we could create new tab.

57
00:04:25.480 --> 00:04:27.670
And as you can see we have two separate connections.

58
00:04:27.820 --> 00:04:34.330
So in order to bring these cities into the analysis what we would need to do is perform a blend and

59
00:04:34.510 --> 00:04:37.210
that is one of the options that we have.

60
00:04:37.210 --> 00:04:42.610
At the same time we can now do something that was not possible before and tabel previously and previous

61
00:04:42.670 --> 00:04:47.380
older versions of Tablo what she could do is you couldn't create a cross started.

62
00:04:47.380 --> 00:04:53.500
And so right now I want to create a joint and I want to join an Excel data source so that what we have

63
00:04:53.500 --> 00:05:00.190
now to s.c.s the data file the one that we are going to connect to now the file with the populations

64
00:05:00.890 --> 00:05:07.720
you couldn't do that in tablet but the tablet have created a facility that now you can create cross-out

65
00:05:07.760 --> 00:05:12.140
adjoins and you can create a job for lots of different data sources.

66
00:05:12.140 --> 00:05:15.620
Not everything but for a very a huge variety you can.

67
00:05:15.620 --> 00:05:17.770
And one of them is Excel to see a.

68
00:05:17.780 --> 00:05:19.330
So that's exactly what we're going to do.

69
00:05:19.340 --> 00:05:24.080
We're going to click the Add button over here and from the dropdown we're going to select text file

70
00:05:24.080 --> 00:05:27.570
because we're connecting just as we file you as cities population.

71
00:05:27.920 --> 00:05:31.730
And once we've selected that as you can see it has been added.

72
00:05:31.820 --> 00:05:36.440
Now before we talk about this joint first thing and we want to do is make sure it's being performed

73
00:05:36.440 --> 00:05:37.220
correctly.

74
00:05:37.220 --> 00:05:40.270
If we click on this button here you'll see that it's been matched by city.

75
00:05:40.520 --> 00:05:46.760
But as we discussed there are duplicate cities in the US and more so they can be duplicate cities in

76
00:05:46.760 --> 00:05:47.650
our data set.

77
00:05:47.660 --> 00:05:52.600
So therefore this could be a do adjoint duplicate that's happening right now.

78
00:05:52.610 --> 00:05:57.680
So just city by itself is not a good enough is not specific enough.

79
00:05:57.680 --> 00:05:59.500
And as you can see that is exactly the case.

80
00:05:59.540 --> 00:06:03.000
Right now we actually have 159 rows instead of 150.

81
00:06:03.140 --> 00:06:07.410
So some rows indeed got duplicated in part as part of this join.

82
00:06:07.490 --> 00:06:10.860
There's something very important you need to look out for.

83
00:06:10.880 --> 00:06:14.320
So you always make sure you're performing the joint correctly.

84
00:06:14.360 --> 00:06:20.930
In our case we want to not only join on city but we want to join on state from this data source and

85
00:06:21.020 --> 00:06:23.810
state from this data source.

86
00:06:23.840 --> 00:06:29.060
So we have to make sure that our Join has been performed correctly and now if I close this you'll see

87
00:06:29.060 --> 00:06:30.510
that there is 150 rows.

88
00:06:30.530 --> 00:06:32.900
So no duplication has occurred.

89
00:06:32.900 --> 00:06:38.600
So if you scroll down you'll see that there's 150 rows and it's always a good thing to check to just

90
00:06:38.630 --> 00:06:45.080
double check that you haven't performed any duplications of rows and that the Joines performed correctly.

91
00:06:45.290 --> 00:06:50.700
And so now moving back to this great feature of cross data join would we actually create.

92
00:06:50.690 --> 00:06:52.120
Here is a one of them.

93
00:06:52.120 --> 00:06:53.720
One of those cross status joins.

94
00:06:53.720 --> 00:06:57.830
We've got an Excel file on the left to see yes we found the right Preuss it was not possible.

95
00:06:57.830 --> 00:06:59.350
Now we can do that.

96
00:06:59.360 --> 00:06:59.690
We can.

97
00:06:59.690 --> 00:07:06.910
In fact we can create connect a Asgill database with a Excel file and a CSP file and also add a minuscule

98
00:07:06.910 --> 00:07:12.200
scale database so there's lots of different combinations that you can create a great shout out to the

99
00:07:12.200 --> 00:07:15.050
guys from Tablo for creating this functionality.

100
00:07:15.050 --> 00:07:21.920
It is going to make analytics much more fun and interesting and it will allow us to perform at least

101
00:07:21.920 --> 00:07:23.280
that prusik we couldn't perform.

102
00:07:23.300 --> 00:07:29.870
So that's a great great feature and now we're just going to proceed with this combined data set so we

103
00:07:29.870 --> 00:07:38.210
can actually just rename and say combined data set and hear what we have is the original data set with

104
00:07:38.210 --> 00:07:43.630
the blue lines at the top then we get Calford feels at the end and then Orange is the new data set.

105
00:07:43.640 --> 00:07:47.000
So we all all we need from here is actually the population.

106
00:07:47.000 --> 00:07:49.000
So the 2015 estimate.

107
00:07:49.030 --> 00:07:54.590
So we're going to just select all the other columns hold down control and we're going to right click

108
00:07:54.650 --> 00:07:57.990
and we're going to hide them because we don't need them in our analysis.

109
00:07:58.050 --> 00:08:03.900
There is the estimate of the population which is going to call it population for all purposes.

110
00:08:04.160 --> 00:08:12.350
And now we can proceed on to our advanced clustering or refined clustering and we'll start on that in

111
00:08:12.350 --> 00:08:14.570
the next tutorial can't wait to see there.

112
00:08:14.630 --> 00:08:16.130
Until then happy analyzing.

