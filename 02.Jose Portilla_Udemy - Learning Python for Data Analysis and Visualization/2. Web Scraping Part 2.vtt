WEBVTT
1
1

00:00:02.220  -->  00:00:06.720
Hey everyone, welcome to part two of
the Web Scraping tutorial with Python.
2

2

00:00:06.720  -->  00:00:10.710
So, in this second part,
what we're gonna do is
3

3

00:00:10.710  -->  00:00:15.270
basically parse through the tables
object we made in the first video and
4

4

00:00:15.270  -->  00:00:19.780
use the find all method in BeautifulSoup
to Grab the actual table contents.
5

5

00:00:20.780  -->  00:00:24.630
To start off,
I'm going to set up an empty data list.
6

6

00:00:25.650  -->  00:00:29.610
I'll make a list called data,
have it be empty, and then
7

7

00:00:29.610  -->  00:00:34.780
I'm going to set rows as the first
indexed object in tables.
8

8

00:00:34.780  -->  00:00:38.950
So, what I mean by that is I'm
gonna make an object called rows.
9

9

00:00:40.590  -->  00:00:46.680
And then that's gonna be the first
index object in my tables object.
10

10

00:00:48.350  -->  00:00:52.530
And then I'm gonna call findAll method.
11

11

00:00:52.530  -->  00:00:57.580
And I'm gonna look for
every row in that tables.
12

12

00:00:59.690  -->  00:01:02.380
And that index of zero is just
the way that BeautifulSoup
13

13

00:01:02.380  -->  00:01:04.550
stores the information so
that's why you have to do that.
14

14

00:01:06.310  -->  00:01:13.833
And now what I'm going to
do is I'm going to say for
15

15

00:01:13.833  -->  00:01:18.669
every table row, or tr in rows,
16

16

00:01:18.669  -->  00:01:24.044
I'm going to have the cols object or
17

17

00:01:24.044  -->  00:01:28.011
columns = tr.findAll.
18

18

00:01:28.011  -->  00:01:33.592
[SOUND] And find everything of an HTML
19

19

00:01:33.592  -->  00:01:38.248
td tag, so that entry there.
20

20

00:01:38.248  -->  00:01:39.753
So I'm gonna find every cell and
21

21

00:01:39.753  -->  00:01:43.000
then what I'm gonna do is I'm gonna
check to see if text is in the row.
22

22

00:01:43.000  -->  00:01:47.398
So I'm gonna say for
23

23

00:01:47.398  -->  00:01:53.608
basically every cell in that
24

24

00:01:53.608  -->  00:01:58.782
new cols object I made,
25

25

00:01:58.782  -->  00:02:03.240
text will be equal to
26

26

00:02:03.240  -->  00:02:07.291
>> That particular cell,
27

27

00:02:07.291  -->  00:02:15.609
I'm gonna pass to method find(text=True).
28

28

00:02:15.609  -->  00:02:19.490
And that's text.
29

29

00:02:19.490  -->  00:02:21.790
Then, I'm going to print that text object.
30

30

00:02:24.290  -->  00:02:27.270
And, finally,
I'll append it to my data list.
31

31

00:02:27.270  -->  00:02:35.000
So, let's go ahead and break down,
one more time, what we're doing here.
32

32

00:02:35.000  -->  00:02:37.020
We're gonna set up an empty data list.
33

33

00:02:38.130  -->  00:02:44.620
Set rows as that first index object,
for every find all table row.
34

34

00:02:45.720  -->  00:02:50.430
And then for every item in that
rows object, I'm going to set
35

35

00:02:50.430  -->  00:02:54.900
cols as finding everywhere where
there's a td, or a table cell.
36

36

00:02:54.900  -->  00:02:59.630
And then for every table cell in that cols
object, I'll check if there's text there.
37

37

00:02:59.630  -->  00:03:04.630
And if there is, I'll print the text and
append that text to my data list.
38

38

00:03:05.922  -->  00:03:07.220
All right, so go ahead and
39

39

00:03:07.220  -->  00:03:12.650
check out the BeautifulSoup link
I posted in the notebook here.
40

40

00:03:12.650  -->  00:03:15.530
If the findAll's a little confusing for
you.
41

41

00:03:15.530  -->  00:03:20.630
But if you're familiar with HTML, then
this probably makes more or less sense.
42

42

00:03:20.630  -->  00:03:22.070
And let's go ahead and
run it, see what happens.
43

43

00:03:23.770  -->  00:03:24.730
Great.
44

44

00:03:24.730  -->  00:03:28.400
So now you can see all the text
information that was in
45

45

00:03:30.760  -->  00:03:32.840
that calls object that I made.
46

46

00:03:32.840  -->  00:03:37.292
So let's go ahead see what
the data list we made looks like.
47

47

00:03:37.292  -->  00:03:39.490
Okay, great.
48

48

00:03:39.490  -->  00:03:45.340
So, now you can see that my data
list consists of that entry,
49

49

00:03:45.340  -->  00:03:48.490
the date, and then that pdf file.
50

50

00:03:51.060  -->  00:03:51.560
Awesome.
51

51

00:03:53.030  -->  00:03:58.350
So continuing on, you'll notice
there's going to be some other so
52

52

00:03:58.350  -->  00:04:02.360
for instance here i'm
high lighting \xa0 so
53

53

00:04:02.360  -->  00:04:05.140
we're gonna have to deal with
that in the future a little bit.
54

54

00:04:05.140  -->  00:04:07.800
We're gonna have to deal
with these new lines.
55

55

00:04:07.800  -->  00:04:10.640
And also I want links to
have the pdf on them.
56

56

00:04:10.640  -->  00:04:16.820
So I'm gonna have to find a way to get rid
of these other links that aren't even,
57

57

00:04:16.820  -->  00:04:18.399
don't exist yet, and don't have the pdfs.
58

58

00:04:19.880  -->  00:04:23.330
So continuing now, in order to do that,
59

59

00:04:23.330  -->  00:04:28.260
to set up my panel's data frame, I'm gonna
use a for loop to go through the list and
60

60

00:04:28.260  -->  00:04:31.100
grab only the cells
with a pf file in them.
61

61

00:04:31.100  -->  00:04:34.840
And we'll also need to keep track of the
index to set up the date of the report.
62

62

00:04:36.500  -->  00:04:38.199
So, I'm gonna go ahead and
set up empty lists.
63

63

00:04:40.370  -->  00:04:42.760
So I'll set reports as an empty list.
64

64

00:04:42.760  -->  00:04:46.964
I'll set up date as another empty list.
65

65

00:04:51.386  -->  00:04:53.770
I'll set an object called index = 0.
66

66

00:04:53.770  -->  00:04:57.120
And that's gonna be basically
almost like a counting object.
67

67

00:04:57.120  -->  00:04:59.520
And now I'm gonna go through every item.
68

68

00:05:00.889  -->  00:05:04.303
So for every item in
69

69

00:05:10.643  -->  00:05:11.711
My data.
70

70

00:05:16.562  -->  00:05:23.668
If pdf, if that string is in the item,
then I know that item has a,
71

71

00:05:23.668  -->  00:05:29.250
basically I know that's
a report in my data.
72

72

00:05:29.250  -->  00:05:32.145
So I'm gonna say date.append.
73

73

00:05:36.038  -->  00:05:39.230
Index-1, and I'll break that down
while I'm doing that in a second.
74

74

00:05:39.230  -->  00:05:43.452
And then I'm also gonna say
75

75

00:05:43.452  -->  00:05:49.334
reports.append(item.replace.
76

76

00:05:53.884  -->  00:05:57.754
And then I'm gonna do this to take
care of that non unicode stuff, and
77

77

00:05:57.754  -->  00:05:59.510
I'll explain why in a second.
78

78

00:06:01.490  -->  00:06:09.490
U space and then add one to the index.
79

79

00:06:09.490  -->  00:06:16.010
So this is a bit of a complicated for
loop, so let's go ahead and break it down.
80

80

00:06:16.010  -->  00:06:16.949
What am I actually doing here?
81

81

00:06:18.130  -->  00:06:23.030
So I made my reports and date as
empty lists and I set my index = 0.
82

82

00:06:23.030  -->  00:06:25.060
I know what my data list looks like.
83

83

00:06:26.330  -->  00:06:31.210
It's got the entry number,
the date of the entry, and
84

84

00:06:31.210  -->  00:06:34.320
then the pdf report that's attached to it.
85

85

00:06:34.320  -->  00:06:35.210
It has that pattern.
86

86

00:06:35.210  -->  00:06:41.740
It goes entry, date, pdf report, entry,
date, pdf report, and that continues on.
87

87

00:06:41.740  -->  00:06:46.680
So I know that if I found
an entry that has a pdf,
88

88

00:06:46.680  -->  00:06:51.070
one before it in the data list is
gonna be the date of that report.
89

89

00:06:53.140  -->  00:06:55.720
So we'll keep that in mind, and
another thing to keep in mind
90

90

00:06:55.720  -->  00:07:00.190
are these non unicode objects here and
that's what I was mentioning before.
91

91

00:07:00.190  -->  00:07:02.320
We're probably gonna have
to clean up your data.
92

92

00:07:02.320  -->  00:07:03.260
This is one way to do that.
93

93

00:07:04.570  -->  00:07:06.020
So we're gonna have to do that.
94

94

00:07:06.020  -->  00:07:08.530
So I'm saying for
every item in that data list.
95

95

00:07:08.530  -->  00:07:10.400
If pdf is in that item, so for
96

96

00:07:10.400  -->  00:07:15.810
instance, if I came across this item here,
Cogneration Energy Conservation.
97

97

00:07:15.810  -->  00:07:18.440
Major Capital Projects pdf.
98

98

00:07:18.440  -->  00:07:24.110
So if pdf is in that item,
append the item before
99

99

00:07:24.110  -->  00:07:27.620
it using that index counter to the date.
100

100

00:07:27.620  -->  00:07:29.650
So if I came across this Cogeneration and
101

101

00:07:29.650  -->  00:07:33.350
Energy Conservation pdf file,
go ahead go back one.
102

102

00:07:34.350  -->  00:07:37.930
Find that date, and
append it to my date list.
103

103

00:07:39.290  -->  00:07:42.780
And then as far as the reports go,
append the item itself.
104

104

00:07:43.890  -->  00:07:47.350
Now what's going on here
with the item.replace?
105

105

00:07:47.350  -->  00:07:51.425
So back when I was running this on my own,
I was getting a lot of unicode errors.
106

106

00:07:51.425  -->  00:07:56.575
Because of this \xa0.
107

107

00:07:56.575  -->  00:08:00.920
So what I have to do was I'm gonna hop
over right to the IPython Notebook here.
108

108

00:08:03.460  -->  00:08:07.295
So you'll notice the line I
take to take care of that \xa0.
109

109

00:08:07.295  -->  00:08:11.930
So this is due to a unicode error
that occurs if you don't do that.
110

110

00:08:11.930  -->  00:08:15.670
And like I was mentioning before,
webpages can be messy and inconsistent.
111

111

00:08:15.670  -->  00:08:18.250
And it's pretty likely you're gonna have
to do some research how to take care of
112

112

00:08:18.250  -->  00:08:19.610
problems like these.
113

113

00:08:19.610  -->  00:08:23.320
So, I just looked it up on StackOverflow.
114

114

00:08:23.320  -->  00:08:27.180
And usually someone's already had
similar problem to what you've had.
115

115

00:08:27.180  -->  00:08:29.448
And I was about to look up the solution.
116

116

00:08:29.448  -->  00:08:32.810
For removing \xa0 from a string.
117

117

00:08:34.070  -->  00:08:37.650
And someone here posted the solution,
so I was able to solve that problem.
118

118

00:08:37.650  -->  00:08:40.410
And you're probably gonna have to
do a similar thing when you're
119

119

00:08:40.410  -->  00:08:42.210
doing your own Web Scraping.
120

120

00:08:42.210  -->  00:08:44.280
Let's go ahead and see how this cell runs.
121

121

00:08:44.280  -->  00:08:44.780
Great.
122

122

00:08:46.790  -->  00:08:51.440
And now all that's left is to organize
our data into a pandas data frame.
123

123

00:08:53.691  -->  00:08:57.560
So I'll set some series objects.
124

124

00:08:57.560  -->  00:09:01.962
I'll set date = Series(date).
125

125

00:09:01.962  -->  00:09:05.913
reports = Series(reports).
126

126

00:09:05.913  -->  00:09:11.076
Now I"m going to concatenate both
127

127

00:09:11.076  -->  00:09:16.245
of those series into a data frame.
128

128

00:09:17.665  -->  00:09:21.741
And I'll call this legislative_df for
my data frame name.
129

129

00:09:21.741  -->  00:09:25.010
Use the pd.concat method.
130

130

00:09:27.819  -->  00:09:35.860
Put in the name of both those series And
then I'm gonna do it on the first axis.
131

131

00:09:35.860  -->  00:09:43.520
So it's joined together just like
in the table on the website.
132

132

00:09:43.520  -->  00:09:50.590
And now I just need to set up the columns
since it doesn't have any column names.
133

133

00:09:50.590  -->  00:09:59.141
I'll say legislative_dfcolumns
134

134

00:09:59.141  -->  00:10:07.155
= [ 'Date','Report'].
135

135

00:10:07.155  -->  00:10:14.425
Now what I'm going to do,
is check the data frame we've made.
136

136

00:10:14.425  -->  00:10:21.810
And there you have it now I have a data
frame that basically emulates this
137

137

00:10:21.810  -->  00:10:27.470
table online from this web page except
it only has the entries of a pdf file.
138

138

00:10:29.230  -->  00:10:29.920
Okay.
139

139

00:10:29.920  -->  00:10:31.120
Great.
140

140

00:10:31.120  -->  00:10:32.180
So what did we do here?
141

141

00:10:32.180  -->  00:10:34.840
Let's just go over everything we just did.
142

142

00:10:34.840  -->  00:10:37.310
I'll go hop back over to
the IPhyton notebook.
143

143

00:10:38.330  -->  00:10:43.390
So we make sure we learned
stuff about Web Scraping.
144

144

00:10:43.390  -->  00:10:46.170
Gave you some links to
learn a little more HTML.
145

145

00:10:46.170  -->  00:10:47.480
Downloaded the modules we needed.
146

146

00:10:48.772  -->  00:10:51.560
We used request to get
a connection to that url.
147

147

00:10:53.148  -->  00:10:55.030
We used BeautifulSoup and
148

148

00:10:55.030  -->  00:10:58.570
the find all method to find
exactly where that content was.
149

149

00:10:58.570  -->  00:11:01.949
Find all the tables, then find the table
rows, then then the table cells.
150

150

00:11:03.050  -->  00:11:05.810
We store that as a lists called data.
151

151

00:11:05.810  -->  00:11:07.300
We looked at our lists.
152

152

00:11:07.300  -->  00:11:10.930
We had to clean it up a bit to set
it up as the pandas DataFrame.
153

153

00:11:10.930  -->  00:11:13.130
So we use this index counter method.
154

154

00:11:13.130  -->  00:11:16.240
Grabbed only where the pdfs
were in the items and
155

155

00:11:16.240  -->  00:11:17.800
then set up our date lists as well.
156

156

00:11:19.070  -->  00:11:23.150
Then set up everything we needed
to create a pandas DataFrame.
157

157

00:11:23.150  -->  00:11:24.650
And there you have it.
158

158

00:11:24.650  -->  00:11:28.000
So another thing that I want
you guys to be aware of is,
159

159

00:11:28.000  -->  00:11:30.250
there's some less intense options for
Web Scraping.
160

160

00:11:30.250  -->  00:11:36.300
You can go ahead and check out these two
companies, import.io and kiminolabs.
161

161

00:11:36.300  -->  00:11:40.090
These are basically
automatic Web Scraping,
162

162

00:11:40.090  -->  00:11:44.250
or automatic API for any website.
163

163

00:11:44.250  -->  00:11:45.750
So I encourage you to go ahead and
164

164

00:11:45.750  -->  00:11:49.130
check out these links if you're
more interested in Web Scraping.
165

165

00:11:49.130  -->  00:11:51.630
But basically what they do is they
166

166

00:11:51.630  -->  00:11:56.410
instantly turn a web page into usable
data that you can access through an API.
167

167

00:11:58.050  -->  00:12:01.370
Depending on how much use you want
from them, you may have to pay for it.
168

168

00:12:01.370  -->  00:12:05.290
But, it's definitely a good starting
point if it's very tedious for
169

169

00:12:05.290  -->  00:12:08.262
you to do the Web Scraping
on the particular website.
170

170

00:12:08.262  -->  00:12:09.330
All right guys, thank you so
171

171

00:12:09.330  -->  00:12:12.620
much for watching this appendix on
Web Scraping, and I hope you enjoyed it.
172

172

00:12:12.620  -->  00:12:13.120
Thanks.
