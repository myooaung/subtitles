WEBVTT
1
00:00:02.050 --> 00:00:08.040
Hai semuanya dan selamat datang di kuliah kedua di bagian klasifikasi multi-kelas.

2
00:00:08.040 --> 00:00:14.520
Jadi sekarang mari kita mulai dan mulai menggunakan sikat belajar untuk melakukan klasifikasi multiclass menggunakan teknik

3
00:00:14.520 --> 00:00:18.550
regresi logistik yang satu lawan satu dibandingkan yang lain.

4
00:00:19.050 --> 00:00:22.670
Jadi kita sudah memiliki X dan Y mendefinisikan fitur data dan Target tersebut.

5
00:00:22.860 --> 00:00:26.080
Jadi kita akan pergi ke depan dan melanjutkan dengan array itu.

6
00:00:26.080 --> 00:00:32.490
Kami kemudian harus membagi data menjadi set pengujian dan pelatihan dan argumen ukuran tes untuk memiliki data

7
00:00:32.490 --> 00:00:38.280
pengujian menjadi 40 persen dari total set data dan juga melewati nomor benih acak.

8
00:00:38.710 --> 00:00:40.900
Jadi hasilnya bisa berubah setiap saat.

9
00:00:41.100 --> 00:00:42.820
Ayo maju dan kembali ke kode kehidupan.

10
00:00:45.790 --> 00:00:46.980
OK bagus.

11
00:00:46.980 --> 00:00:52.980
Jadi ingat kita baru saja selesai memformat data kita dan memvisualisasikannya semoga

12
00:00:52.980 --> 00:00:55.220
juga beberapa visualisasi milikmu.

13
00:00:55.220 --> 00:01:03.230
Jadi mari kita pergi ke dan dari S K belajar linear menggarisbawahi model.

14
00:01:03.230 --> 00:01:15.060
Kita akan mengimpor kelas regresi logistik kita dan kemudian terlihat seperti linear.

15
00:01:15.060 --> 00:01:16.430
BAIK.

16
00:01:16.430 --> 00:01:21.730
Dan juga dari skala belajar validasi silang.

17
00:01:22.160 --> 00:01:31.230
Kami akan mengimpor pemisahan uji kereta sehingga mudah-mudahan ini juga sudah cukup familiar bagi Anda untuk

18
00:01:31.750 --> 00:01:38.730
Anda dari seri kuliah regresi logistik dan salah mengeja yang salah.

19
00:01:38.730 --> 00:01:40.170
Oke, sempurna.

20
00:01:41.170 --> 00:01:43.960
Jadi mari kita keluar dan membuat objek kelas regresi logistik.

21
00:01:43.960 --> 00:01:47.220
Saya akan menyebutnya log.

22
00:01:47.220 --> 00:01:49.190
Jadi, Anda akan melihat ini.

23
00:01:49.190 --> 00:01:54.080
Ini sangat mirip dengan apa yang baru saja kami lakukan dalam kuliah regresi logistik.

24
00:01:57.720 --> 00:02:03.940
Dan kemudian apa yang akan saya lakukan adalah membagi data menjadi

25
00:02:04.760 --> 00:02:21.500
set data pelatihan dan pengujian sehingga saya akan meminta X saya melatih X saya menguji y saya dan uji Y saya dan kemudian saya akan menerapkan uji split kereta dan kemudian

26
00:02:21.500 --> 00:02:23.340
logistik kuliah regresi.

27
00:02:23.340 --> 00:02:26.630
Kami tidak memberikan argumen tambahan, tetapi kali ini.

28
00:02:27.110 --> 00:02:29.800
Saya akan menambahkan beberapa argumen tambahan.

29
00:02:29.800 --> 00:02:34.150
Kita harus melewati x dan y harus tahu apa yang harus dibagi.

30
00:02:34.150 --> 00:02:41.200
Tetapi sejauh argumen tambahan saya akan lulus tes sistem default biasanya 30 persen untuk ukuran

31
00:02:41.200 --> 00:02:42.600
pengujian Anda.

32
00:02:42.600 --> 00:02:49.040
Dalam hal ini kita akan menjadi 40 persen sehingga melewati nol koma empat maka saya

33
00:02:49.040 --> 00:02:52.040
juga akan memulai keadaan acak 3.

34
00:02:52.040 --> 00:03:00.930
Ini hanya nomor benih untuk acak apa yang disebut twister belas kasih yang hanya algoritma yang digunakan untuk membuat

35
00:03:00.930 --> 00:03:06.630
angka acak sehingga mereka akan memilih secara acak selama pelatihan hingga x

36
00:03:07.060 --> 00:03:11.820
dan set data y untuk membuat pelatihan dan menguji dataset.

37
00:03:11.820 --> 00:03:14.730
Dan mari kita masuk dan mencoba memastikan bahwa semuanya bekerja dengan baik.

38
00:03:14.730 --> 00:03:15.560
OK sepertinya.

39
00:03:16.820 --> 00:03:24.810
Jadi seperti dalam kuliah regresi logistik yang akan kita lakukan adalah melatih model kereta yang ada di sana.

40
00:03:24.810 --> 00:03:32.400
Dan kami melakukannya dengan mencocokkan model kami sehingga kami akan menyesuaikan model kami dengan set data pelatihan X train

41
00:03:32.400 --> 00:03:33.820
dan y train.

42
00:03:35.270 --> 00:03:36.070
Dan kita mulai.

43
00:03:36.070 --> 00:03:42.050
Jadi sekarang kita melatih model kita dengan set pelatihan yang akan kita lakukan adalah melihat seberapa akurat

44
00:03:42.050 --> 00:03:43.660
kita dengan set pengujian.

45
00:03:43.660 --> 00:03:48.000
Jadi kami akan membuat prediksi menggunakan model kami dan kemudian memeriksa akurasinya.

46
00:03:48.000 --> 00:03:54.200
Jadi hanya untuk membahas apa yang kami lakukan seperti yang kami lakukan dengan cepat tetapi hampir sama dengan apa yang

47
00:03:54.200 --> 00:03:56.110
kami lakukan dalam kuliah regresi logistik.

48
00:03:56.110 --> 00:04:02.180
Kami mengimpor regresi logistik dan melatih untuk memisahkan dari Eskay belajar dari perpustakaan khusus

49
00:04:02.180 --> 00:04:04.890
model linier dan validasi silang ini.

50
00:04:04.890 --> 00:04:11.700
Kami membuat objek regresi regresi logistik menyebutnya log dan kemudian kami membagi fitur dan target

51
00:04:11.700 --> 00:04:17.060
kami menjadi set pelatihan dan set pengujian menggunakan kereta untuk membagi.

52
00:04:17.060 --> 00:04:21.940
Dan dalam hal ini saya memberikan argumen tambahan untuk keacakan.

53
00:04:21.940 --> 00:04:27.140
Dan kemudian setelah itu kami mengambil model kami dan menyesuaikan data pelatihan dengan itu.

54
00:04:27.140 --> 00:04:32.380
Dan Anda akan melihat ini hanya rutinitas yang sangat umum karena Anda melakukan lebih

55
00:04:32.380 --> 00:04:39.990
banyak pembelajaran mesin dan lainnya kami melatih model kami, kami ingin menguji akurasinya dan kami akan mengatakan dari Eskay untuk belajar

56
00:04:41.030 --> 00:04:41.790
metrik impor.

57
00:04:45.010 --> 00:04:46.220
Besar.

58
00:04:46.220 --> 00:04:53.330
Jadi sekarang saya ingin tahu prediksi kami, jadi saya akan membuat objek baru

59
00:04:53.360 --> 00:04:56.570
yang disebut Pra-underscore untuk prediksi.

60
00:04:56.570 --> 00:05:06.630
Dan saya akan mengambil model saya, Reg saya, objek kelas regresi logistik yang sekarang menjadi model dan saya benar-benar

61
00:05:06.630 --> 00:05:10.680
akan menekan melewati metode Predict di atasnya.

62
00:05:11.240 --> 00:05:19.550
Dan saya akan lulus tes X saya jadi apa yang kita lakukan di sini adalah saya mengambil fitur saya yang

63
00:05:19.550 --> 00:05:28.210
ada di set pengujian saya dan saya akan mencoba untuk memprediksi kelas atau target menggunakan log read up predict pada dataset

64
00:05:28.210 --> 00:05:29.070
pengujian .

65
00:05:29.970 --> 00:05:34.810
OK jadi sekarang luas adalah prediksi saya diberikan set pengujian X

66
00:05:34.810 --> 00:05:39.360
Sekarang ingat kita memiliki tes Y kami sehingga kami tahu jawaban yang benar dan saya ingin memeriksa atau keakuratan

67
00:05:39.360 --> 00:05:40.060
model kami.

68
00:05:40.450 --> 00:05:46.480
Jadi yang akan saya tahu adalah mencetak keakuratan dan memeriksa keakuratannya.

69
00:05:46.480 --> 00:05:54.520
Yang harus Anda lakukan adalah memanggil pustaka metrik yang kami impor dan melewati akurasi metode

70
00:05:54.520 --> 00:05:59.590
di bawah skor skor dan kemudian kami ingin membandingkan.

71
00:06:00.860 --> 00:06:04.080
Pertama, koreksi terhadap prediksi kami.

72
00:06:07.350 --> 00:06:09.030
Dan mari kita lihat bagaimana kami melakukannya.

73
00:06:09.030 --> 00:06:10.180
BAIK.

74
00:06:10.180 --> 00:06:14.220
Jadi sepertinya model kami memiliki akurasi hampir 93 persen.

75
00:06:14.220 --> 00:06:17.230
Ini mungkin berubah untuk Anda tergantung pada pemisahan acak.

76
00:06:17.230 --> 00:06:19.000
Jadi ingatlah itu.

77
00:06:19.930 --> 00:06:23.700
Haruskah kita mempercayai tingkat akurasi ini.

78
00:06:23.700 --> 00:06:28.750
Jadi saya mendorong Anda untuk mencari cara untuk mencoba memahami secara intuitif apakah akurasi ini masuk

79
00:06:28.750 --> 00:06:29.720
akal atau tidak.

80
00:06:29.720 --> 00:06:34.080
Apakah Anda mengharapkan akurasi 50 persen seperti akurasi 93 persen.

81
00:06:34.080 --> 00:06:38.970
Jadi jika kita melihat plot pasangan itu lagi dan saya akan gulir ke sini.

82
00:06:38.970 --> 00:06:44.740
Ingat kelas dan bahkan kolom pertama kelas virginica sebenarnya cukup baik dipisahkan

83
00:06:44.740 --> 00:06:46.880
di sebagian besar fitur.

84
00:06:46.880 --> 00:06:48.390
Paling pasti.

85
00:06:48.390 --> 00:06:53.460
Tapi benar-benar hanya ada beberapa fitur yang mungkin seperti orang-orang C di sini di mana agak sulit

86
00:06:53.460 --> 00:06:55.190
untuk memisahkan semburan warna virginica.

87
00:06:55.190 --> 00:07:01.330
Tetapi untuk sebagian besar fitur lain seperti pedal dengan panjang orang Anda benar-benar dapat melihat bahwa ada pemisahan

88
00:07:01.470 --> 00:07:03.440
yang cukup jelas di sana.

89
00:07:05.560 --> 00:07:12.130
Dan kita lihat di sini bahwa melawan Tosa sejauh salah satu fitur didefinisikan dengan cukup baik itu sendiri

90
00:07:12.400 --> 00:07:17.660
dan tidak ada banyak crossover antara panjang pedal untuk vs warna virginica virginica.

91
00:07:17.660 --> 00:07:27.150
Ya, jadi kita harus benar-benar mengharapkan model yang cukup kuat sehingga cara untuk secara intuitif memahami apa yang seharusnya dilakukan model

92
00:07:27.150 --> 00:07:32.460
Anda adalah dengan memvisualisasikan data dan itu hanya datang dari pengalaman dan

93
00:07:32.460 --> 00:07:36.040
mengetahui data Anda dan dari mana asalnya.

94
00:07:36.040 --> 00:07:43.020
Jadi saya mendorong Anda untuk mencoba mengubah parameter ukuran tes di sini di kereta untuk membagi dan

95
00:07:43.020 --> 00:07:45.280
melihat bagaimana itu mempengaruhi hasil.

96
00:07:45.280 --> 00:07:53.280
Sejauh ini apa yang kami lakukan adalah membersihkan data dan memisahkan beberapa fitur sehingga kami harus mengharapkan akurasi

97
00:07:53.280 --> 00:08:00.690
yang cukup tinggi dan yang akan kami lakukan sekarang adalah menggunakan tetangga terdekat untuk mengimplementasikan klasifikasi

98
00:08:00.690 --> 00:08:07.270
multikelas dengan mudah hanya dengan perkembangan dan yang diterapkan satu vs semua metode.

99
00:08:07.270 --> 00:08:10.620
Dan sekarang kita akan belajar tentang tetangga terdekat K jadi mari kita pergi dan melompat ke sana.

100
00:08:12.060 --> 00:08:13.510
Tidak ada buku

101
00:08:13.510 --> 00:08:21.620
Sekarang jika kita gulir ke bawah di sini kita memiliki gambaran umum dasar dari algoritma tetangga terdekat sehingga Anda dapat masuk dan

102
00:08:21.620 --> 00:08:28.690
klik tautan ini di sini dan saya akan membawa Anda ke halaman Wikipedia di mana itu menggambarkan algoritma.

103
00:08:28.690 --> 00:08:35.030
Ini dikenal sebagai algoritma tipe pembelajaran malas tetapi saya akan membiarkan Anda melanjutkan dan membacanya sendiri.

104
00:08:36.140 --> 00:08:38.620
Tujuan jam sebenarnya cukup sederhana.

105
00:08:38.620 --> 00:08:45.160
Anda diberikan objek untuk ditugaskan ke kelas dalam ruang fitur sehingga menguji x objek dan

106
00:08:45.160 --> 00:08:50.790
kemudian Anda memilih kelas yang terdekat dengan tetangganya dalam set pelatihan dan bahwa

107
00:08:50.790 --> 00:08:53.290
nyaris terjadi adalah metrik jarak.

108
00:08:53.290 --> 00:08:59.050
Dan algoritma tetangga terdekat raja sebenarnya dijelaskan dengan sangat baik dalam dua video berikut di

109
00:08:59.050 --> 00:08:59.800
sini.

110
00:08:59.800 --> 00:09:02.140
Jadi yang pertama hanya penjelasan keseluruhan yang cepat.

111
00:09:02.140 --> 00:09:06.810
Hanya beberapa menit dan kemudian jika Anda ingin menyelam lebih dalam

112
00:09:06.810 --> 00:09:11.890
ke matematika sebenarnya ada kuliah penuh MIT yang panjangnya hampir satu jam.

113
00:09:11.890 --> 00:09:14.260
Mereka dapat memeriksa di sini.

114
00:09:14.260 --> 00:09:19.420
Saya akan melakukan penjelasan singkat tentang kuliah snowpack karena kita belum membahas tentang tetangga

115
00:09:19.420 --> 00:09:23.800
operator seperti regresi logistik membahasnya di kuliah sebelumnya, tetapi karena K tetangga

116
00:09:23.800 --> 00:09:27.760
terdekat sebenarnya cukup baru, saya akan melanjutkan dan membahasnya sendiri.

117
00:09:28.940 --> 00:09:34.060
Anda dapat pergi dan melihat video-video itu tetapi lakukan penjelasan singkat tentang bagaimana video Nabors Anda bekerja.

118
00:09:34.060 --> 00:09:36.700
Ini sebenarnya cukup sederhana.

119
00:09:36.700 --> 00:09:38.390
Jadi pergilah dan lihat gambar ini.

120
00:09:38.390 --> 00:09:43.060
Diagram ini dan kami memiliki ruang fitur kami.

121
00:09:43.060 --> 00:09:45.950
X1 x2 ini.

122
00:09:45.950 --> 00:09:51.880
Dan jika Anda perhatikan di sini di kuning Kami memiliki kelas A dan ungu

123
00:09:51.880 --> 00:09:59.950
kami memiliki kelas B sehingga Anda dapat membayangkan kami menjalankan set pelatihan kami dan kami memetakan set pelatihan di ruang fitur.

124
00:09:59.950 --> 00:10:03.840
Jadi kita memiliki dua kelas di set kereta A dan B.

125
00:10:03.840 --> 00:10:07.510
Dan kemudian kita harus mengklasifikasikan titik data baru dalam data pengujian kami.

126
00:10:07.510 --> 00:10:10.200
Jadi kami mewakili ini sebagai bintang merah.

127
00:10:10.200 --> 00:10:17.760
Jadi Anda menjalankan pelatihan Anda sehingga Anda memiliki model Anda dan sekarang menjalankan data pengujian Anda terlebih dahulu yang

128
00:10:17.760 --> 00:10:18.310
muncul.

129
00:10:18.310 --> 00:10:22.440
Anda taruh di sini di ruang fitur itu dan sekarang itu bintang merah Anda.

130
00:10:22.720 --> 00:10:29.270
Jadi, mengapa Nabors Anda berfungsi adalah kami hanya perlu memperluas jarak tertentu dari ruang fitur kami

131
00:10:29.270 --> 00:10:35.590
hingga kami mencapai sejumlah titik data lainnya dan Anda menentukan K pada gambar di atas.

132
00:10:37.370 --> 00:10:40.660
Apa yang Anda lakukan adalah katakanlah Kate Anda sama dengan tiga.

133
00:10:41.040 --> 00:10:46.610
Maka pada dasarnya Anda terus berjalan dan berkembang jauh dari titik data pengujian hingga Anda

134
00:10:47.750 --> 00:10:55.160
menekan tiga objek berbeda atau titik data berbeda di ruang fitur pelatihan Anda dan kemudian Anda memutuskan kelas mana

135
00:10:55.160 --> 00:10:59.530
yang Anda pilih berdasarkan kelas mana yang paling Anda tekan.

136
00:10:59.530 --> 00:11:08.070
Jadi dalam hal ini Cable 3 menggunakan metrik jarak itu Anda dapat menekan 2 dari kelas B dan 2

137
00:11:08.070 --> 00:11:09.650
dari Kelas A.

138
00:11:09.650 --> 00:11:15.040
Jadi pada algoritma tetangga terdekat K akan dilakukan dalam hal ini Cable 3 dalam contoh khusus ini.

139
00:11:15.040 --> 00:11:19.230
Itu akan mengatakan bahwa titik baru bahwa bintang merah milik kelas B.

140
00:11:19.230 --> 00:11:25.530
Karena menggunakan K memanggil tiga, Anda masuk ke kelas B alih-alih satu kelas A.

141
00:11:25.530 --> 00:11:30.690
Sekarang jika Anda mengubah titik K Anda menjadi katakanlah 6.

142
00:11:30.690 --> 00:11:33.100
Sekarang kami berkembang lebih jauh hingga mencapai 6 poin.

143
00:11:33.100 --> 00:11:34.970
Jadi kita tekan enter 3 di sini.

144
00:11:34.970 --> 00:11:40.940
Tapi kemudian tiga berikutnya yang paling dekat dengan data pengujian baru kami menunjukkan bahwa bintang merah atau tiga

145
00:11:40.940 --> 00:11:41.990
kuning sekali kelas-A.

146
00:11:42.320 --> 00:11:49.240
Jadi Anda bisa pergi dan melihatnya di sana dan sekarang Anda akan melihat perubahan keputusan kami.

147
00:11:49.240 --> 00:11:50.200
Jadi sekarang kita punya 4.

148
00:11:50.460 --> 00:11:53.920
Di Kelas A versus dua kelas B.

149
00:11:53.920 --> 00:12:01.620
Jadi jika KCl 6 algoritma tetangga terdekat K benar-benar akan mengatakan bintang merah ini milik kelas A.

150
00:12:02.040 --> 00:12:09.080
Jadi Anda bisa melihat perbedaan antara berbagai nilai k dan hal penting yang perlu diketahui untuk klasifikasi biner

151
00:12:09.080 --> 00:12:10.370
menggunakan metode ini.

152
00:12:10.370 --> 00:12:16.970
Anda selalu perlu memilih nomor ganjil untuk K untuk klasifikasi biner dan alasannya.

153
00:12:17.910 --> 00:12:23.310
Bayangkan mereka hanya memiliki dua kelas untuk dipilih dan Anda memilih K keren juga.

154
00:12:23.310 --> 00:12:30.710
Kalau begitu dalam situasi yang sangat jarang atau mungkin situasi umum tergantung pada data Anda dapat

155
00:12:31.170 --> 00:12:34.350
menemukan hubungan antara poin data Anda.

156
00:12:34.350 --> 00:12:41.530
Jadi, jika Anda memiliki kabel ke dua, Anda mungkin menekan tepat di tengah antara dua titik dan kemudian algoritma Anda

157
00:12:41.530 --> 00:12:44.100
tidak akan tahu yang harus dipilih.

158
00:12:44.100 --> 00:12:49.400
Jadi, Anda selalu harus memilih nomor ganjil untuk klasifikasi biner menggunakan tetangga K terdekat.

159
00:12:49.400 --> 00:12:52.520
Jadi itu untuk menghindari kasus jarak ikatan antara dua kelas.

160
00:12:54.510 --> 00:12:56.070
Jadi bagi saya itu masuk akal.

161
00:12:56.070 --> 00:12:58.390
Cukup sederhana, Anda baru saja lagi.

162
00:12:58.610 --> 00:13:02.040
Anda memiliki data pelatihan dan ruang fitur mereka.

163
00:13:02.040 --> 00:13:08.580
Data pengujian baru Anda masuk ke ruang fitur itu dan Anda berpihak pada nilai gua Anda dan berapa banyak poin

164
00:13:08.580 --> 00:13:11.800
pelatihan lainnya yang Anda tekan pada nilai k itu.

165
00:13:11.800 --> 00:13:14.620
Anda masuk ke kelas mana.

166
00:13:14.620 --> 00:13:19.030
Jadi sekarang yang akan kita lakukan adalah maju dan menerapkan ini dengan sikat belajar dan Anda

167
00:13:19.690 --> 00:13:23.660
akan melihat bahwa itu sebenarnya sangat mudah dan sederhana sehingga kembali ke kode langsung.

168
00:13:23.660 --> 00:13:24.990
Ayo maju dan mulai itu.

169
00:13:24.990 --> 00:13:33.690
Jadi perpustakaan yang perlu kita impor sekarang disebut Dari mempelajari pikiran tetangga dan hanya

170
00:13:37.060 --> 00:13:43.930
catatan cepat setiap orang Inggris berbahasa Inggris belajar tetangga tidak memiliki

171
00:13:43.930 --> 00:13:47.300
penggunaan tambahan jadi hanya karena.

172
00:13:47.300 --> 00:13:49.780
Dan kemudian kita akan mengimpor k.

173
00:13:50.100 --> 00:13:56.360
Klasifikasi tetangga E G H U R K.

174
00:13:59.500 --> 00:14:01.300
Dan karena kebohongan.

175
00:14:01.300 --> 00:14:07.810
OK jadi dari Eskay saya mengetahui bahwa tetangga mengimpor tetangga klasifikasi di silakan dan lihat dokumentasi

176
00:14:07.810 --> 00:14:11.560
jika Anda ingin belajar lebih banyak tentang API itu.

177
00:14:11.560 --> 00:14:16.240
Jadi yang akan kita lakukan sekarang adalah menerapkan metode ini ke status Ira yang kita miliki.

178
00:14:16.240 --> 00:14:20.370
Jadi mari kita lanjutkan dan impor yang datang tetangga klasifikasi.

179
00:14:20.370 --> 00:14:28.600
Jadi kita akan membuat objek yang disebut k dan n sama dengan K tetangga.

180
00:14:28.600 --> 00:14:33.900
Klasifikasi Jadi sangat mirip dengan apa yang kami lakukan untuk regresi logistik.

181
00:14:33.900 --> 00:14:38.730
Tetapi dalam kasus ini ketika kami mengimpornya, Anda akan ingin mengatakan berapa nilai gua Anda, berapa

182
00:14:38.730 --> 00:14:41.830
jumlah tetangga Anda yang akan Anda tunggu untuk kena.

183
00:14:41.830 --> 00:14:44.810
Jadi yang kami lakukan adalah meloloskan argumen yang

184
00:14:47.500 --> 00:14:50.760
disebut tetangga garis bawah dan kasus ini akan menjadi enam.

185
00:14:54.390 --> 00:14:57.570
Dan Whoops sepertinya tidak sengaja memanfaatkan huruf L.

186
00:14:57.570 --> 00:15:01.610
OK jadi sekarang kita memiliki model tetangga terdekat K kami.

187
00:15:01.610 --> 00:15:04.450
Dan lagi-lagi seperti untuk regresi logistik.

188
00:15:04.760 --> 00:15:06.670
Kami akan mencocokkan data kami.

189
00:15:06.670 --> 00:15:08.840
Jadi mari kita pergi dan menyesuaikan data pelatihan kami.

190
00:15:08.840 --> 00:15:14.490
X train y train.

191
00:15:14.490 --> 00:15:15.460
Besar.

192
00:15:15.460 --> 00:15:19.410
Dan sekarang kita akan menjalankan prediksi seperti yang kami lakukan untuk regresi logistik.

193
00:15:19.780 --> 00:15:23.650
Jadi Anda akan melihat cara-cara seperti itu mengetahui Anda pada dasarnya mengulangi hal-hal yang sama persis hanya

194
00:15:23.650 --> 00:15:25.080
menggunakan model yang sama sekali berbeda.

195
00:15:25.080 --> 00:15:35.410
Jadi itu cukup bagus dan kami akan memanggil prediksi dan kami ingin memprediksi lagi menggunakan kereta yang

196
00:15:35.730 --> 00:15:38.080
menguji data fitur.

197
00:15:38.080 --> 00:15:40.900
OK jadi kita jalankan itu.

198
00:15:41.330 --> 00:15:52.780
Dan sekarang seperti bagian terakhir tentang regresi logistik kita akan mencetak akurasi dan akurasi metrik

199
00:15:53.910 --> 00:15:57.830
di bawah skor skor.

200
00:15:59.700 --> 00:16:05.640
Sekarang kita akan membandingkan tes putih dengan prediksi kami menggunakan K tetangga

201
00:16:05.640 --> 00:16:13.230
terdekat dengan kabel ke 6 dan perhatikan sikat belajar memberitahu Anda semua parameter lain yang diasumsikan.

202
00:16:13.230 --> 00:16:19.960
OK dan Anda selalu dapat melihat dokumentasi untuk informasi lebih lanjut tentang itu.

203
00:16:20.690 --> 00:16:25.850
Jadi sepertinya menggunakan kabel 6 membuat kami memiliki akurasi sekitar 95 persen.

204
00:16:25.850 --> 00:16:29.660
Mari kita masuk dan melihat apa yang terjadi jika kita mengurangi nilai itu menjadi K sama dengan 1.

205
00:16:29.660 --> 00:16:34.520
Jadi itu berarti apa pun titik terdekat di ruang mendatang ke titik data

206
00:16:34.520 --> 00:16:42.380
pengujian kami akan menjadi kelas yang bergabung dengan titik pengujian. Jadi jika kita gulir kembali ke gambar ini dengan take adalah 1.

207
00:16:42.380 --> 00:16:45.960
Titik pelatihan apa pun yang paling dekat dengan titik pengujian kami berikutnya.

208
00:16:45.960 --> 00:16:48.380
Itu kelas yang akhirnya masuk ke dalamnya.

209
00:16:48.380 --> 00:16:54.520
OK jadi mari kita lanjutkan dan lakukan itu.

210
00:16:54.520 --> 00:16:56.970
Ini akan menjadi proses yang sangat mirip lagi.

211
00:16:56.970 --> 00:17:04.600
Tetapi dalam kasus ini kita hanya akan mengganti nama k nm ke klasifikasi

212
00:17:07.210 --> 00:17:14.330
tetangga K dengan tetangga garis bawah sama dengan 1 dalam kasus kita.

213
00:17:15.060 --> 00:17:22.020
Hebat dan lagi, lakukan saja hal yang sama persis seperti kita akan melatih model

214
00:17:22.020 --> 00:17:25.560
kita atau cocok dengan data pelatihan.

215
00:17:27.910 --> 00:17:29.360
Kami akan menjalankan prediksi kami saat ini.

216
00:17:35.430 --> 00:17:43.720
Dan Will memprediksi siapa nilai target yang digunakan adalah dataset pengujian

217
00:17:43.720 --> 00:17:59.090
x dan mari kita lihat bagaimana kami melakukannya akan mencetak akurasi metrik atau skor skor membandingkan y menguji dataset tersebut dengan prediksi Y.

218
00:17:59.090 --> 00:18:02.980
OK jadi sepertinya menggunakan K sama dengan 1.

219
00:18:02.980 --> 00:18:06.140
Kami mendapat akurasi 96 persen.

220
00:18:06.140 --> 00:18:12.390
Jadi pertanyaannya sekarang bagaimana kita tahu apa yang terbaik digunakan.

221
00:18:12.390 --> 00:18:18.620
Anda dapat melanjutkan dan melihat beberapa sumber daya lain yang tersisa di bagian bawah ini.

222
00:18:18.620 --> 00:18:25.950
Saya Python tidak ada buku untuk mendapatkan klarifikasi lebih sedikit tentang bagaimana menggunakan dan belajar untuk menemukan apa nilai gua

223
00:18:25.950 --> 00:18:27.370
terbaik untuk digunakan

224
00:18:27.370 --> 00:18:32.640
Tetapi dalam kasus kami ini adalah data kami sehingga sangat kecil. Yang akan kami lakukan sebenarnya adalah siklus

225
00:18:32.640 --> 00:18:35.970
melalui berbagai nilai k untuk menemukan nilai k yang optimal.

226
00:18:35.970 --> 00:18:38.010
Sekarang jika set data Anda sangat besar.

227
00:18:38.030 --> 00:18:40.190
Anda tahu dia bukan Anda tahu saya selalu bisa melakukan ini.

228
00:18:40.190 --> 00:18:43.230
Namun dalam kasus kami, menarik untuk mengetahui apa yang terjadi.

229
00:18:43.790 --> 00:18:52.320
Jadi yang akan saya lakukan adalah objek Range sama dengan rentang 1 hingga 21.

230
00:18:52.320 --> 00:18:55.970
Jadi nilai test case dari 1 hingga 20 ingat.

231
00:18:55.970 --> 00:19:07.560
Dan itu dimulai dari nol jadi 21 sebenarnya hanya Anda yang tahu hingga 20 akan membuat daftar kosong yang disebut akurasi.

232
00:19:08.010 --> 00:19:15.890
Dan kemudian saya akan mengulangi proses di atas kami hanya melakukannya untuk semua nilai k dan menambahkan hasilnya

233
00:19:15.890 --> 00:19:17.360
ke daftar akurasi.

234
00:19:17.360 --> 00:19:29.420
Jadi apa yang saya maksud dengan mengatakan untuk k dalam rentang K kita akan membuat model kita classifier tetangga K dan kemudian

235
00:19:31.980 --> 00:19:42.400
saya akan mengatur argumen tetangga sama dengan nilai k saat ini dalam for loop dan kemudian saya akan cocok

236
00:19:45.850 --> 00:19:52.180
dengan model itu dengan Cate X latih dia y train.

237
00:19:56.770 --> 00:19:58.240
Lalu saya

238
00:20:02.480 --> 00:20:07.560
akan memprediksi menggunakan set data uji X saya.

239
00:20:07.560 --> 00:20:14.030
Dan apakah itu cukup oke sebenarnya saya hanya akan menambahkannya ke daftar kosong yang kami buat.

240
00:20:14.030 --> 00:20:22.780
Akurasi metrik atau skor skor.

241
00:20:22.780 --> 00:20:24.240
Dan kami memiliki

242
00:20:29.150 --> 00:20:32.360
data tes putih kami dalam prediksi Y kami.

243
00:20:32.360 --> 00:20:33.930
Ayo maju dan jalankan itu.

244
00:20:33.930 --> 00:20:38.360
OK sepertinya kita punya model-model itu dan saya akan

245
00:20:38.360 --> 00:20:39.970
memplot hasilnya dan

246
00:20:42.930 --> 00:20:50.540
saya akan melakukannya menggunakan Matlab yang kami impor jadi kami memiliki akurasi vs jadi saya ingin memplot

247
00:20:50.540 --> 00:20:56.770
untuk setiap nilai case berapa sebenarnya saya dan kemudian saya akan cukup masukkan

248
00:20:59.640 --> 00:21:07.360
label beberapa label X akan menjadi nilai ok dan label y akan menjadi akurasi atau akurasi pengujian.

249
00:21:15.510 --> 00:21:16.470
BAIK.

250
00:21:16.470 --> 00:21:22.280
Jadi sepertinya Anda mendapatkan perilaku aneh ini, Anda mendapatkan kabel speaker hanya

251
00:21:22.280 --> 00:21:28.340
karena cara datanya dan kemudian mulai turun lagi muncul sebentar antara lima dan

252
00:21:28.340 --> 00:21:33.450
10 turun dan kemudian terlihat saat Anda memperluas nilai gua.

253
00:21:33.450 --> 00:21:36.830
Anda mencapai puncak itu lagi.

254
00:21:36.830 --> 00:21:39.180
Kemudian kembali mereka naik kembali.

255
00:21:39.180 --> 00:21:42.660
Dan itu hanya akan bervariasi tergantung pada seperti apa data Anda.

256
00:21:42.660 --> 00:21:48.530
Dan kami berada dalam posisi yang cukup nyaman di sini karena kami hanya memiliki 150 titik data yang dapat kami

257
00:21:48.530 --> 00:21:52.420
jalankan ini yang berkali-kali Anda tidak selalu harus melakukannya dengan data Anda.

258
00:21:52.420 --> 00:22:00.120
Jadi saya sarankan Anda maju dan periksa tautan tambahan untuk mempelajari lebih lanjut tentang klasifikasi multi kelas

259
00:22:00.120 --> 00:22:05.330
Tapi untuk saat ini semestinya cukup mudah semoga.

260
00:22:05.330 --> 00:22:10.710
Hanya untuk meninjau semua yang telah kami lakukan di bagian ini, kami memiliki masalah tindakan

261
00:22:10.710 --> 00:22:12.080
set data iris.

262
00:22:12.080 --> 00:22:13.890
Pelajari sedikit tentang itu.

263
00:22:13.890 --> 00:22:19.510
Kemudian kami memiliki pengantar cepat untuk klasifikasi multi-kelas menggunakan regresi logistik yang

264
00:22:19.510 --> 00:22:22.130
cukup sederhana hanya klasifikasi biner.

265
00:22:22.130 --> 00:22:23.660
Salah satu dari semua metode.

266
00:22:23.660 --> 00:22:29.290
Jadi Anda hanya melakukan klasifikasi biner untuk satu kelas versus semua kelas lainnya.

267
00:22:29.290 --> 00:22:31.300
Lalu kami memformat data kami.

268
00:22:31.300 --> 00:22:32.250
Kami memeriksanya.

269
00:22:32.250 --> 00:22:34.020
Kami melihatnya di Panda.

270
00:22:34.020 --> 00:22:35.930
Kami melakukan beberapa visualisasi cepat.

271
00:22:35.930 --> 00:22:44.010
Mudah-mudahan bahwa lebih dari itu kami melakukan teknik regresi logistik ini dan belajar kami melakukan K tetangga terdekat yang kami pelajari

272
00:22:44.010 --> 00:22:45.580
tentang hal itu.

273
00:22:46.410 --> 00:22:52.710
Dan kemudian kami melakukan tetangga paranormal belajar melakukan analisis cepat tentang apa itu bus K untuk digunakan dan kemudian kami memiliki

274
00:22:52.710 --> 00:22:55.220
kesimpulan kami dengan beberapa tautan di sini.

275
00:22:55.220 --> 00:22:55.890
BAIK.

276
00:22:56.240 --> 00:22:57.320
Pekerjaan yang sangat bagus.

277
00:22:57.320 --> 00:23:02.400
Semoga Anda menikmati klasifikasi kelas saya dan sampai jumpa di kuliah berikutnya.

278
00:23:02.400 --> 00:23:02.890
Terima kasih.
