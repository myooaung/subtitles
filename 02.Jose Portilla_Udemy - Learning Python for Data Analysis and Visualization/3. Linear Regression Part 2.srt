1
1

00:00:02,430  -->  00:00:05,270
Hello everyone and
welcome to the second lecture
2

2

00:00:05,270  -->  00:00:07,870
in the Supervised Learning Linear
Regression lecture series.
3

3

00:00:08,970  -->  00:00:11,980
So in this lecture we're gonna go ahead
look a little deeper into the mathematics
4

4

00:00:11,980  -->  00:00:17,455
behind the Least Squares Method, which
is the method we're gonna be using for
5

5

00:00:17,455  -->  00:00:21,980
our linear regression on our
Boston housing data sets.
6

6

00:00:21,980  -->  00:00:26,270
So if we take a quick look at the plot
we made in the last lecture with C1.
7

7

00:00:28,660  -->  00:00:33,433
You see that there's our best fit line and
your data points,
8

8

00:00:33,433  -->  00:00:37,177
and what we kinda wanna
answer the question is,
9

9

00:00:37,177  -->  00:00:40,934
how do we know when the line
is the best fit line?
10

10

00:00:40,934  -->  00:00:43,866
How do we know it's not slanted or
has a different slope or
11

11

00:00:43,866  -->  00:00:45,219
a different y intercept?
12

12

00:00:45,219  -->  00:00:49,339
How do we actually find this
line with the best fit?
13

13

00:00:49,339  -->  00:00:50,866
So if you look at each data point,
14

14

00:00:50,866  -->  00:00:53,879
you know they're each gonna have
a coordinate in the form x and y.
15

15

00:00:53,879  -->  00:01:01,010
In this case, price as your y and
room as your x.
16

16

00:01:01,010  -->  00:01:05,700
So imagine drawing a line between
each point to your current choice
17

17

00:01:06,890  -->  00:01:08,490
of your best fit line.
18

18

00:01:08,490  -->  00:01:11,980
So to visualize this,
I've got an image from Wikipedia here.
19

19

00:01:11,980  -->  00:01:16,750
So you have your current choice of your
curve fit line, your best fit line.
20

20

00:01:16,750  -->  00:01:21,760
So you might be iterating through these
and now you have all your data points.
21

21

00:01:21,760  -->  00:01:26,751
And you've drawn a line from
each data point to your
22

22

00:01:26,751  -->  00:01:30,475
current choice of your best fit line.
23

23

00:01:30,475  -->  00:01:35,283
So if you label each of these green
lines as having a distance d,
24

24

00:01:35,283  -->  00:01:39,656
we can now define a best fit
line as having this property.
25

25

00:01:39,656  -->  00:01:44,850
So the sum of each of
those distances squared.
26

26

00:01:45,910  -->  00:01:48,660
If it's minimized for
27

27

00:01:48,660  -->  00:01:52,556
each of the data points,
then we know we have a best fit line.
28

28

00:01:52,556  -->  00:01:56,760
So basically you're looking for
29

29

00:01:56,760  -->  00:02:01,110
the line or the sum of all these squares,
of these green lines,
30

30

00:02:01,110  -->  00:02:04,930
the distance between your data points
to your current curve fit is minimized.
31

31

00:02:06,330  -->  00:02:08,050
So how do you find this actual line?
32

32

00:02:09,340  -->  00:02:14,570
So in the least square line
method of approximating.
33

33

00:02:14,570  -->  00:02:18,720
You have your set of points, you have
X,Y1, you have all these coordinate
34

34

00:02:18,720  -->  00:02:22,760
points, 2, 3, 4, 5, fo,r etcetera,
etcetera, for all your data points.
35

35

00:02:22,760  -->  00:02:27,770
And you know the line has
the equation Y = mx + b.
36

36

00:02:27,770  -->  00:02:31,820
And in this case, we can write this as
Y equals A naught plus a1, times X.
37

37

00:02:34,740  -->  00:02:38,169
And you can actually solve for
these constants, a naught, and a1,
38

38

00:02:38,169  -->  00:02:40,670
by simultaneously solving
these two equations.
39

39

00:02:40,670  -->  00:02:45,134
The sum of all your Y values
equal to a naught times N
40

40

00:02:45,134  -->  00:02:48,970
plus a1 times the sum
of all your X values.
41

41

00:02:51,960  -->  00:02:54,748
And the sum of X times
Y equals to a naught,
42

42

00:02:54,748  -->  00:02:59,150
sum of all your x plus a1 times
the sum of all your x square values.
43

43

00:03:00,410  -->  00:03:04,120
So these are actually called the normal
equations for the least square lines.
44

44

00:03:05,370  -->  00:03:09,980
There's actually quite a bit more as far
as the further steps you have to take and
45

45

00:03:09,980  -->  00:03:15,030
rearranging these equations
in order to solve for Y.
46

46

00:03:15,030  -->  00:03:17,838
But since we don't want to
go too much into the math,
47

47

00:03:17,838  -->  00:03:21,730
I'll let scikit-learn do the rest
of the heavy lifting here.
48

48

00:03:22,970  -->  00:03:26,750
If you're interested in how you can
continue learning more about this
49

49

00:03:26,750  -->  00:03:30,470
particular equation and
what your N stands for And
50

50

00:03:30,470  -->  00:03:34,420
how a naught a1 come to play,
I've linked a video here.
51

51

00:03:37,480  -->  00:03:44,440
And in this video it's a great
video by a man named [INAUDIBLE].
52

52

00:03:44,440  -->  00:03:47,645
And it just goes over
some basic statistics and
53

53

00:03:47,645  -->  00:03:53,314
he'll go over the simple linear
regression as far common in statistics So
54

54

00:03:53,314  -->  00:03:55,550
you can just go back [CROSSTALK].
55

55

00:03:55,550  -->  00:04:02,182
So go ahead and check out that
video if your further interested
56

56

00:04:02,182  -->  00:04:08,180
in the actual mathematics
behind this method.
57

57

00:04:08,180  -->  00:04:12,488
But As far as what we're doing,
as far as an overview is we're basically
58

58

00:04:12,488  -->  00:04:16,870
having NumPy scikit-learn do this for
59

59

00:04:16,870  -->  00:04:21,850
us, finding where on your
best fit line is this D
60

60

00:04:21,850  -->  00:04:26,820
squared value minimized, as far as
the sum of all these green lines.
61

61

00:04:26,820  -->  00:04:29,220
So that's just a very basic overview.
62

62

00:04:29,220  -->  00:04:32,690
If you're interested in more of
the mathematics, go ahead and check those
63

63

00:04:32,690  -->  00:04:37,840
video links and
they'll fill you in on that information.
64

64

00:04:37,840  -->  00:04:43,475
Moving along, now we're gonna to use NumPy
to create a univariate linear regression.
65

65

00:04:43,475  -->  00:04:47,139
Now remember, univariate is just
a fancy way of saying single variable.
66

66

00:04:47,139  -->  00:04:51,427
So were just gonna wanna
take one single variable or
67

67

00:04:51,427  -->  00:04:56,212
one data type,
in this case average number of rooms, and
68

68

00:04:56,212  -->  00:05:02,310
try to fit it to the price data, or
label that we have, that target data.
69

69

00:05:02,310  -->  00:05:05,485
So the way we're gonna do this is we'll
start out by setting x and y arrays for
70

70

00:05:05,485  -->  00:05:07,090
NumPy to take in.
71

71

00:05:07,090  -->  00:05:09,180
And an important note for the x array.
72

72

00:05:09,180  -->  00:05:14,190
NumPy expects a two-dimensional array,
the first dimension is
73

73

00:05:14,190  -->  00:05:17,970
different example values, and the second
dimension is the attribute number, and
74

74

00:05:17,970  -->  00:05:20,590
I'll show you that in a second.
75

75

00:05:20,590  -->  00:05:23,484
So in this case we have our value as
the mean number of rooms per house, and
76

76

00:05:23,484  -->  00:05:26,820
since this is a single attribute, so the
second dimension of the array is just one.
77

77

00:05:26,820  -->  00:05:32,137
So we'll need to create a shape
array 506 by 1 since if you remember
78

78

00:05:32,137  -->  00:05:39,070
the description of the Boston data set
it had 506 attributes for each feature.
79

79

00:05:39,070  -->  00:05:42,030
So there's a few ways to do this,
but an easy way to do that is by
80

80

00:05:42,030  -->  00:05:47,026
using Numpy's built in vertical stack
tool, which is just vstack method.
81

81

00:05:47,026  -->  00:05:50,548
So let's go ahead and
actually go to the live coding so
82

82

00:05:50,548  -->  00:05:53,930
I can show you what I mean
by everything I just said.
83

83

00:05:53,930  -->  00:05:58,188
Remember, if I'm going a little to fast,
you can always pause the video,
84

84

00:05:58,188  -->  00:06:02,580
slow it down, or just go ahead and
open up the iPython Notebook yourself, and
85

85

00:06:02,580  -->  00:06:05,573
read along through all
the detailed explanations.
86

86

00:06:05,573  -->  00:06:09,552
But if you want to move along
to the algorithmic stuff,
87

87

00:06:09,552  -->  00:06:12,410
we can go ahead and start that.
88

88

00:06:12,410  -->  00:06:17,360
So I'm gonna set up my X,
and I'm gonna set that up
89

89

00:06:17,360  -->  00:06:19,360
as the median of the room value.
90

90

00:06:19,360  -->  00:06:22,560
So if you remember from the dataframe
we made in the last video,
91

91

00:06:22,560  -->  00:06:25,447
we have our boston_df.
92

92

00:06:25,447  -->  00:06:28,610
All right,
remember we named all the columns.
93

93

00:06:28,610  -->  00:06:31,410
So I'm just gonna call that room column,
set that up as my X.
94

94

00:06:31,410  -->  00:06:34,550
And if we look at what X is right now,
95

95

00:06:36,530  -->  00:06:41,848
it's just an array, or a series, in this
case, of all those pricing data points.
96

96

00:06:41,848  -->  00:06:44,589
But NumPy,
97

97

00:06:44,589  -->  00:06:49,480
for its linear algebra library,
it needs to have it in the form,
98

98

00:06:52,440  -->  00:06:56,670
all your features and then how many
attributes there actually are.
99

99

00:06:56,670  -->  00:07:00,366
So in order to do that,
I'm gonna set x equal to.
100

100

00:07:04,718  -->  00:07:09,686
NumPy, or np dot the vstack method
101

101

00:07:14,019  -->  00:07:16,370
And then I'm going to call
that on that room calls.
102

102

00:07:16,370  -->  00:07:20,030
So this is what we're
actually gonna to need to do.
103

103

00:07:20,030  -->  00:07:21,980
And if we see what x looks like now,
104

104

00:07:21,980  -->  00:07:27,490
as far as the shape,
now we have a 506 by one.
105

105

00:07:29,190  -->  00:07:35,060
And if we look at the shape of
the last one, it was just 506.
106

106

00:07:35,060  -->  00:07:39,908
So, what NumPy needs is it needs
to know how many values you have,
107

107

00:07:39,908  -->  00:07:42,650
and how many attributes there were.
108

108

00:07:42,650  -->  00:07:45,482
So we have one attribute which
is the average room size and
109

109

00:07:45,482  -->  00:07:48,732
we have 506 different values for
that particular attribute.
110

110

00:07:48,732  -->  00:07:53,330
Let's go ahead and set up the Y.
111

111

00:07:53,330  -->  00:07:57,620
The Y is actually straightforward,
it's just the target price of the houses.
112

112

00:08:00,540  -->  00:08:03,110
I can that by calling price
column we made earlier.
113

113

00:08:07,613  -->  00:08:11,125
And so now that we know and
have our x and y, we can go ahead and
114

114

00:08:11,125  -->  00:08:15,580
use NumPy to create the single
variable linear regression.
115

115

00:08:15,580  -->  00:08:17,240
So I'm gonna hop back over
to the iPython Notebook.
116

116

00:08:18,380  -->  00:08:19,990
That's completed for us.
117

117

00:08:19,990  -->  00:08:24,060
And if you remember,
the line has an equation of y = mx + b.
118

118

00:08:24,060  -->  00:08:26,300
But we're gonna have to rewrite this,
using matrices.
119

119

00:08:26,300  -->  00:08:34,320
And the way we can do that is have y equal
to A times p, where A is just x by 1.
120

120

00:08:34,320  -->  00:08:39,770
And as you can imagine, we kind of already
have that here, in our X here 506 by 1.
121

121

00:08:39,770  -->  00:08:44,849
And where P is equal to m times b,
122

122

00:08:44,849  -->  00:08:50,690
or m, a matrix of m over b, a2 by 1.
123

123

00:08:50,690  -->  00:08:55,630
So this is the same as the first equation
if you carry out the linear algebra.
124

124

00:08:55,630  -->  00:08:58,770
So just in different forms
with these matrices.
125

125

00:08:58,770  -->  00:09:02,980
And we'll start by creating
that A matrix here using NumPy.
126

126

00:09:02,980  -->  00:09:05,040
And we'll do this by creating
a matrix of the form X1.
127

127

00:09:05,040  -->  00:09:09,960
So we'll call every value in our
original X that we just made, and
128

128

00:09:09,960  -->  00:09:15,880
use list comprehension to basically make
this x1 array, this is a one by two.
129

129

00:09:15,880  -->  00:09:19,768
And we'll set up an array in the form X1.
130

130

00:09:19,768  -->  00:09:21,690
So let me show you what I mean by that.
131

131

00:09:24,420  -->  00:09:26,760
So I need to create an array,
right now we have our x,
132

132

00:09:26,760  -->  00:09:31,040
let me show you what it looks like here.
133

133

00:09:33,390  -->  00:09:34,940
So that was actually the original x,
134

134

00:09:34,940  -->  00:09:38,590
let me show you what it looks
like after we run that vstack.
135

135

00:09:38,590  -->  00:09:42,070
There we go, so
that's what our x currently looks like.
136

136

00:09:42,070  -->  00:09:46,513
The previous x I showed was actually
just that boston_df, room column, where
137

137

00:09:46,513  -->  00:09:51,378
remember, we had to use that np.vstack to
get it get into this form, the 506 by 1.
138

138

00:09:51,378  -->  00:09:55,820
And what I wanna do
139

139

00:09:55,820  -->  00:10:00,854
now is create a array
140

140

00:10:00,854  -->  00:10:05,850
in the form of x1.
141

141

00:10:05,850  -->  00:10:09,182
Right now I just have
a bunch of the X values and
142

142

00:10:09,182  -->  00:10:13,580
I need to have it look like that
due to the matrix equations.
143

143

00:10:13,580  -->  00:10:14,751
So the way to do that.
144

144

00:10:17,863  -->  00:10:21,508
Is by,
there's different ways you can do this.
145

145

00:10:21,508  -->  00:10:23,273
You can do this doing a for
loop, but in this case,
146

146

00:10:23,273  -->  00:10:24,886
I'm gonna do it using
a list comprehension.
147

147

00:10:24,886  -->  00:10:26,905
So I'll write it out and
then break it down.
148

148

00:10:42,659  -->  00:10:43,490
Okay.
149

149

00:10:43,490  -->  00:10:44,890
So what are we actually doing here?
150

150

00:10:46,410  -->  00:10:48,760
So if we look at what x
looks like right now.
151

151

00:10:52,102  -->  00:10:52,729
Just gonna go ahead and
print it out for you.
152

152

00:10:52,729  -->  00:10:57,770
X basically just looks like
a bunch of x's, all those values.
153

153

00:10:57,770  -->  00:11:01,605
And I want it to look like this,
to match our equation here,
154

154

00:11:01,605  -->  00:11:03,770
this matrix of 1 by 2, x and 1.
155

155

00:11:05,120  -->  00:11:08,900
So the way to do that is I'm gonna
use the list comprehension, and
156

156

00:11:08,900  -->  00:11:13,830
I'm gonna say, basically, this is just
kind of a different way of writing a for
157

157

00:11:13,830  -->  00:11:15,520
loop almost that just builds a list.
158

158

00:11:16,560  -->  00:11:20,120
So I'm going to say, for
every value in my x, so
159

159

00:11:20,120  -->  00:11:22,520
go through this array,
so every value here.
160

160

00:11:25,230  -->  00:11:30,380
Set it up as the value comma 1, so
it matches this form here, that A matrix.
161

161

00:11:32,000  -->  00:11:35,220
And then make an array
of all those values.
162

162

00:11:36,780  -->  00:11:39,820
So if I run that now,
let's go ahead see what x looks like.
163

163

00:11:43,350  -->  00:11:46,110
And now it looks exactly the same
except now there's that 1 attached.
164

164

00:11:46,110  -->  00:11:47,150
And remember,
165

165

00:11:47,150  -->  00:11:51,390
we need that one because NumPy needs
to know how many attributes you have.
166

166

00:11:51,390  -->  00:11:55,760
And in this case, it's just a simple
single variable linear regression so
167

167

00:11:55,760  -->  00:11:58,250
we have one attribute,
the average room number.
168

168

00:11:59,720  -->  00:12:04,500
Okay, and now that we have all that,
that's actually all we need and
169

169

00:12:04,500  -->  00:12:06,690
we can get our best fit values.
170

170

00:12:06,690  -->  00:12:10,770
So again, you can always review everything
we just did in the IPython Notebook.
171

171

00:12:10,770  -->  00:12:15,670
But continuing on for now,
we can go ahead and get the m and
172

172

00:12:15,670  -->  00:12:17,810
b values for our best fit line.
173

173

00:12:17,810  -->  00:12:22,285
And the way you do this is like this.
174

174

00:12:22,285  -->  00:12:25,610
You set up two new objects, m and
175

175

00:12:25,610  -->  00:12:30,150
b, and then the NumPy method
we're going to use is
176

176

00:12:32,870  -->  00:12:35,970
in the linear algebra library, which
is this, linalg, and then we're using
177

177

00:12:37,530  -->  00:12:42,546
the least square methods and
that's just lstq.
178

178

00:12:42,546  -->  00:12:46,030
Then you call your X and Y.
179

179

00:12:46,030  -->  00:12:50,860
And we
180

180

00:12:50,860  -->  00:12:55,220
actually only want
the first index value at 0.
181

181

00:12:55,220  -->  00:12:58,550
The reason for
that is just the format that
182

182

00:12:58,550  -->  00:13:00,340
linear algebra least squares
methods spits it out.
183

183

00:13:00,340  -->  00:13:02,460
If you're interested in that,
just go ahead and
184

184

00:13:02,460  -->  00:13:07,810
Google np.linalgs.leastsquares and
check out the documentation there.
185

185

00:13:09,310  -->  00:13:12,420
If I Google it right now,
I'll go ahead and Google it.
186

186

00:13:12,420  -->  00:13:14,801
Go to the documentation, and
187

187

00:13:14,801  -->  00:13:19,683
there's actually almost like
a little mini tutorial here.
188

188

00:13:19,683  -->  00:13:25,120
And you can see, just like I mentioned,
you can rewrite the line equation, y = ap,
189

189

00:13:25,120  -->  00:13:30,940
as these two different matrix equations,
and they kind of also build it out here.
190

190

00:13:30,940  -->  00:13:33,691
So go ahead and check out
the documentation in case you want
191

191

00:13:33,691  -->  00:13:36,445
some more resources on what
NumPy's actually doing, but
192

192

00:13:36,445  -->  00:13:38,310
in this case we just wanna crank it out.
193

193

00:13:38,310  -->  00:13:40,810
Go ahead and run that.
194

194

00:13:40,810  -->  00:13:45,883
And, whoops, I messed,
oh, it's actually lstsq,
195

195

00:13:45,883  -->  00:13:50,990
I'm sorry, least squares,
so lstsq, I apologize.
196

196

00:13:53,780  -->  00:13:56,460
So now that we have that,
we can plot it all together.
197

197

00:13:57,560  -->  00:14:02,010
So note that we're gonna use the original
format of the Boston information.
198

198

00:14:02,010  -->  00:14:07,500
So we only did our matrix transformations
to utilize the NumPy least square method.
199

199

00:14:09,600  -->  00:14:12,640
So let's go ahead and
plot the original points.
200

200

00:14:12,640  -->  00:14:19,110
So I am going to say plt.
201

201

00:14:19,110  -->  00:14:22,917
Oops, let me get back here, okay.
202

202

00:14:22,917  -->  00:14:23,973
So looking back I am going to say plt.
203

203

00:14:26,098  -->  00:14:29,197
Let me make, okay, .plot.
204

204

00:14:29,197  -->  00:14:37,981
And I'm gonna say boston_df.RM,
205

205

00:14:37,981  -->  00:14:43,371
boston_df.Price.
206

206

00:14:45,915  -->  00:14:52,040
So, that's gonna create our scatterplot.
207

207

00:14:52,040  -->  00:14:56,150
It's just gonna make a marker for
every average number of room and
208

208

00:14:56,150  -->  00:14:57,300
the price associated with it.
209

209

00:14:57,300  -->  00:15:00,390
And that's what I was actually
meaning before when we were
210

210

00:15:00,390  -->  00:15:03,110
using the original format
of the Boston information.
211

211

00:15:03,110  -->  00:15:05,806
Okay, and now let's go ahead and
212

212

00:15:05,806  -->  00:15:10,052
plot our best fit line that
we just made using NumPy.
213

213

00:15:10,052  -->  00:15:14,314
And what I'm gonna do for
that is I'm gonna set lowercase x.
214

214

00:15:18,143  -->  00:15:19,165
Equal to that room column.
215

215

00:15:25,584  -->  00:15:31,194
And then I'm gonna say plt.plot.
216

216

00:15:31,194  -->  00:15:31,973
I'm gonna have all my x
values be those x values.
217

217

00:15:31,973  -->  00:15:36,364
And then I'm just gonna put in the
equation of a line here using the m and
218

218

00:15:36,364  -->  00:15:41,054
b we just obtained from our NumPy least
squares method in the linear algebra
219

219

00:15:41,054  -->  00:15:41,740
library.
220

220

00:15:43,500  -->  00:15:47,610
I'll go ahead and make this a red
line by passing the r as a string.
221

221

00:15:47,610  -->  00:15:52,460
And then I'll label it, best fit line.
222

222

00:15:55,660  -->  00:15:59,000
So let's just break down what we've
done so far before I plot this out.
223

223

00:16:01,920  -->  00:16:05,370
We are trying to use the NumPy
224

224

00:16:05,370  -->  00:16:08,950
linear algebra library to
perform a least square method.
225

225

00:16:10,720  -->  00:16:15,640
So we needed our data in the form
of a 506 by 1 shape array,
226

226

00:16:15,640  -->  00:16:20,440
since NumPy needs to know both,
every single value, but
227

227

00:16:20,440  -->  00:16:23,560
it also needs to know what
the attribute number for that is.
228

228

00:16:23,560  -->  00:16:25,320
And since we're only using
a single variable or
229

229

00:16:25,320  -->  00:16:30,520
a single attribute,
it's just 1 there, so 506 by 1.
230

230

00:16:30,520  -->  00:16:32,660
We used vstack to make it two dimensional.
231

231

00:16:34,210  -->  00:16:38,665
We needed to rewrite our equation of
a line in this matrix form in order for
232

232

00:16:38,665  -->  00:16:40,530
NumPy to accept it using linear algebra.
233

233

00:16:41,980  -->  00:16:45,100
So we use that list creation, or
list comprehension, excuse me,
234

234

00:16:45,100  -->  00:16:50,740
to create that list and then transform
it into an array using np.array.
235

235

00:16:50,740  -->  00:16:58,640
Then we can finally pass it using
np.linalg.lstsq and our x y values.
236

236

00:16:58,640  -->  00:17:00,630
And we only needed that
first index value of that.
237

237

00:17:00,630  -->  00:17:01,853
And you can go ahead and
238

238

00:17:01,853  -->  00:17:06,120
check out that documentation as far as
how exactly your values are outputted.
239

239

00:17:06,120  -->  00:17:08,196
All we really care about is that m and
b to make our equation for
240

240

00:17:08,196  -->  00:17:08,905
the line y = mx +b.
241

241

00:17:08,905  -->  00:17:15,810
And then after that,
we made a scatter plot.
242

242

00:17:15,810  -->  00:17:19,570
And we're about to recreate it
here on our live coding session.
243

243

00:17:19,570  -->  00:17:22,740
So scrolling down here,
we have our scatter plot and
244

244

00:17:22,740  -->  00:17:25,670
we also plotted an x and
that linear fit line.
245

245

00:17:27,590  -->  00:17:28,910
And there you have it.
246

246

00:17:28,910  -->  00:17:30,760
So we did it.
247

247

00:17:30,760  -->  00:17:35,760
If we scroll back up here, we basically
manually recreated this seaborn plot.
248

248

00:17:36,920  -->  00:17:39,270
So this is just a simple
linear regression.
249

249

00:17:39,270  -->  00:17:41,370
Seaborn did this automatically for us.
250

250

00:17:41,370  -->  00:17:45,260
But we were also able to do it on our own,
through some matrix tricks and
251

251

00:17:45,260  -->  00:17:50,500
NumPy, by using the least squares method.
252

252

00:17:50,500  -->  00:17:56,500
Okay, so in the next steps,
what we're gonna do is get the error and
253

253

00:17:56,500  -->  00:18:01,120
then move on to using scikit-learn to
implement a multivariate regression.
254

254

00:18:01,120  -->  00:18:06,010
So, remember our data set didn't just have
the median number of rooms as a feature,
255

255

00:18:06,010  -->  00:18:09,400
it had a bunch of
different features to it.
256

256

00:18:09,400  -->  00:18:12,750
And now we wanna take all of those
into account when trying to predict
257

257

00:18:12,750  -->  00:18:13,580
a house price.
258

258

00:18:14,970  -->  00:18:18,780
Okay, so I'll stop this video here and
I'll see you at the next video.
259

259

00:18:18,780  -->  00:18:19,280
Thanks guys.
