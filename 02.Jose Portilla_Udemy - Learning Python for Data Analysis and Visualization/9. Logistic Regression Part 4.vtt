WEBVTT
1
00:00:01.780 --> 00:00:07.480
Halo semuanya dan selamat datang di kuliah keempat dalam seri regresi logistik.

2
00:00:07.480 --> 00:00:14.200
Jadi dalam kuliah terakhir kami membahas bagaimana mempersiapkan atau menyiapkan data.

3
00:00:14.200 --> 00:00:21.250
Jadi kami membuat variabel dummy yang kami butuhkan karena variabel kategorinya memberi Anda sumber daya

4
00:00:21.250 --> 00:00:28.290
untuk memeriksa variabel dummy di Wikipedia dan kemudian kami membuat kerangka data asli kami.

5
00:00:28.290 --> 00:00:35.830
Pada dasarnya jatuhkan pekerjaan dan atur sekelompok kolom pekerjaan baru dengan variabel dummy.

6
00:00:35.830 --> 00:00:37.960
Oke bagus.

7
00:00:37.960 --> 00:00:45.230
Jadi sekarang kita telah datang pada sesuatu yang disebut multi-collinearity dan kita perlu menyingkirkan

8
00:00:45.230 --> 00:00:51.740
beberapa kolom kita akan menjatuhkan satu pekerjaan dan suami menempati satu kolom

9
00:00:51.740 --> 00:00:55.610
untuk menghindari sesuatu yang disebut multi-collinearity.

10
00:00:55.610 --> 00:01:01.110
Jadi multi-collinearity terjadi karena variabel dummy dibuat.

11
00:01:01.110 --> 00:01:04.290
Saya mendorong Anda untuk terus maju dan melihat tautan ini di Wikipedia.

12
00:01:05.280 --> 00:01:11.480
Ini memberi Anda banyak solusi untuk multi-collinearity dan juga memberi tahu Anda mengapa hal itu muncul

13
00:01:11.480 --> 00:01:18.180
tetapi pada dasarnya yang terjadi adalah gambaran singkat tentang apa yang terjadi adalah variabel dummy yang kami

14
00:01:18.570 --> 00:01:24.540
buat sangat berkorelasi dan model kami mulai terdistorsi karena salah satu variabel dummy dapat diprediksi

15
00:01:24.540 --> 00:01:26.620
secara linear dari yang lain.

16
00:01:26.620 --> 00:01:29.530
Jadi apa artinya itu jika kita melihat kembali ke sini.

17
00:01:30.240 --> 00:01:41.000
Jika Anda melihat misalnya semua variabel dummy di kolom pekerjaan wanita jika Anda memiliki kelima variabel tersebut maka semuanya terkait satu

18
00:01:41.000 --> 00:01:49.450
sama lain dan Anda tahu jika salah satu kolom sama dengan satu maka Anda akan tahu

19
00:01:49.480 --> 00:01:53.810
untuk fakta bahwa sisanya sama dengan nol.

20
00:01:53.810 --> 00:01:59.430
Atau misalnya jika Anda tahu semua kecuali satu sama dengan nol maka Anda tahu yang lain sama dengan satu.

21
00:01:59.430 --> 00:02:02.430
Jadi mereka saling berkorelasi satu sama lain.

22
00:02:04.050 --> 00:02:08.890
Dan itu adalah distorsi konstan karena itu akan membuat Anda berpikir Anda memiliki prediksi

23
00:02:08.890 --> 00:02:12.330
akurasi yang sangat tinggi ketika Anda menjalankannya melalui regresi.

24
00:02:12.850 --> 00:02:17.980
Untuk mengatasi masalah ini, kita akan menjatuhkan salah satu variabel dummy dari setiap

25
00:02:17.980 --> 00:02:24.890
set dan Anda pada dasarnya akan kita lakukan adalah menjatuhkan pekerjaan kolom 1 pertama dan pekerjaan suami 1.

26
00:02:25.500 --> 00:02:32.920
Dengan begitu kami mengorbankan titik data untuk menghindari multi-collinearity.

27
00:02:32.920 --> 00:02:40.310
Jadi sekali lagi saya mendorong Anda untuk memeriksa halaman Wikipedia yang menjelaskan lagi sedikit lebih detail tentang

28
00:02:40.660 --> 00:02:46.650
bagaimana multi-collinearity adalah fenomena di mana dua atau lebih variabel prediktif dalam model regresi

29
00:02:46.650 --> 00:02:48.470
berganda sangat berkorelasi.

30
00:02:48.470 --> 00:02:50.940
Sama seperti variabel dummy kami.

31
00:02:50.940 --> 00:02:55.210
Dan itu juga memberi Anda definisi yang lebih matematis.

32
00:02:55.790 --> 00:02:59.660
Ini memberi Anda memberi Anda cara matematis untuk mendeteksi multi-collinearity.

33
00:03:00.900 --> 00:03:06.370
Ini memberi tahu Anda konsekuensi dan juga memberi Anda beberapa solusi dan kemudian memberi Anda beberapa contoh.

34
00:03:06.370 --> 00:03:13.300
Jadi dalam hal ini Seperti yang saya katakan sebelumnya saya harus memiliki multi-collinearity karena variabel dummy yang

35
00:03:13.300 --> 00:03:18.810
Anda buat, tetapi kita bisa mengatasinya dengan menjatuhkan kolom pertama dari kedua set.

36
00:03:18.810 --> 00:03:21.260
Apa yang akan terjadi jika kami tidak menjatuhkan kolom itu.

37
00:03:21.590 --> 00:03:28.740
Nah, jika Anda menjalankan melalui regresi logistik pada dasarnya Anda memiliki prediktor yang hampir mendekati 100 persen dan itu

38
00:03:28.740 --> 00:03:34.630
akan menjadi bendera merah besar karena tidak mungkin Anda memiliki prediktor semacam itu dalam kebanyakan

39
00:03:36.240 --> 00:03:39.830
kasus dalam beberapa kasus Anda bisa mendapatkan yang akurat.

40
00:03:39.830 --> 00:03:43.540
Tetapi mengingat set data yang kita miliki, itu akan sangat tidak mungkin.

41
00:03:43.970 --> 00:03:46.740
Pokoknya mari kita pergi ke

42
00:03:48.950 --> 00:03:53.210
depan dan mengurus multi-collinearity ini jadi turun kembali ke sini.

43
00:03:53.210 --> 00:04:00.400
Kita dapat menangani multi-collinearity dengan menjatuhkan satu kolom dari setiap variabel dummy sehingga jumlah yang kita miliki

44
00:04:00.400 --> 00:04:01.310
untuk x.

45
00:04:01.310 --> 00:04:07.230
Mari kita masuk dan melihatnya dan kita akan menjatuhkan kolom pekerjaan pertama dan kolom

46
00:04:07.230 --> 00:04:08.780
pekerjaan suami pertama.

47
00:04:09.750 --> 00:04:14.770
Jadi di set X sama dengan x drop.

48
00:04:17.940 --> 00:04:22.100
Pasien Satu anggota mengatakan akses sama dengan 1.

49
00:04:22.100 --> 00:04:24.640
Karena kita lakukan bersama dengan kolom.

50
00:04:25.440 --> 00:04:29.290
Anda bisa saja mengatakan X di tempat.

51
00:04:29.290 --> 00:04:34.350
Jadi saya juga ingin menjatuhkan kolom pekerjaan suami.

52
00:04:34.350 --> 00:04:37.390
Yang pertama jadi saya akan melakukannya juga.

53
00:04:37.390 --> 00:04:38.850
Saya hanya berlari itu.

54
00:04:39.860 --> 00:04:47.460
Dan mari kita masuk dan melihat seperti apa frame data X kita sekarang.

55
00:04:47.460 --> 00:04:48.130
Oke bagus.

56
00:04:48.130 --> 00:04:54.150
Jadi kami telah menjatuhkan kedua loop kolom pekerjaan pertama dan kolom suami pertama.

57
00:04:55.090 --> 00:04:58.190
Tetapi lihat di sini kita juga memiliki kolom urusan.

58
00:04:58.190 --> 00:05:01.660
Jadi kita perlu menjatuhkan kolom perselingkuhan ini juga.

59
00:05:01.660 --> 00:05:07.710
Dan alasan untuk itu adalah itu pada dasarnya hanyalah pengulangan dari target Y kami.

60
00:05:07.710 --> 00:05:12.250
Hanya saja bukan 1 0 0 0 dan nomor lainnya.

61
00:05:12.250 --> 00:05:17.270
Jadi agar target y masuk akal, kita perlu mengeluarkan itu dari data kita karena pada dasarnya

62
00:05:17.300 --> 00:05:19.730
itu hanya pengulangan dari target di sana.

63
00:05:20.410 --> 00:05:24.360
Jadi mari kita lanjutkan dan lakukan itu jadi saya akan mengatakan X sama dengan x drop.

64
00:05:30.430 --> 00:05:40.010
Urusan sumbu sama dengan 1.

65
00:05:40.010 --> 00:05:45.000
Dan sekarang akhirnya mari kita tinjau seperti apa rasanya sejauh ini.

66
00:05:45.000 --> 00:05:45.800
Oke, sempurna.

67
00:05:45.800 --> 00:05:51.060
Jadi kami telah menjatuhkan kolom urusan dan kami juga menjatuhkan kolom pertama untuk masing-masing

68
00:05:51.060 --> 00:05:56.900
Baik untuk pekerjaan suami dan pekerjaan istri.

69
00:05:57.340 --> 00:06:03.820
Sekarang lanjutkan dan lihat lagi seperti apa wajahmu.

70
00:06:04.310 --> 00:06:05.950
Jadi ingat ini adalah target kami.

71
00:06:06.730 --> 00:06:09.320
Itu hanya indeks dan kemudian satu atau nol.

72
00:06:09.320 --> 00:06:13.210
Jadi kami memiliki kedua kelas kami di sana baik berselingkuh atau tidak berselingkuh.

73
00:06:13.980 --> 00:06:16.350
Untuk menggunakan ini mengapa saya ingin belajar.

74
00:06:16.350 --> 00:06:18.710
Kita perlu mengaturnya sebagai satu D atau A.

75
00:06:19.090 --> 00:06:22.590
Jadi ini pada dasarnya berarti kita perlu meratakan array.

76
00:06:22.590 --> 00:06:26.980
Untungnya tidak ada pi yang memiliki metode bawaan untuk ini.

77
00:06:26.980 --> 00:06:30.340
Dan saya memberi Anda tautan di sini.

78
00:06:30.340 --> 00:06:32.600
Ravel atau Ravel tergantung bagaimana Anda ingin melakukannya.

79
00:06:32.600 --> 00:06:36.590
Tetapi pada dasarnya itu hanya mendatar dan mengembalikan 1 untuk memerintah.

80
00:06:36.590 --> 00:06:44.450
Jadi saya bisa mendapatkan y dan mengaturnya sama dengan NPD.

81
00:06:47.310 --> 00:06:49.530
Velma memikirkan itu sebabnya.

82
00:06:49.530 --> 00:06:52.390
Dan mari kita lanjutkan dan periksa hasilnya.

83
00:06:52.390 --> 00:06:57.230
Dan sekarang saya memiliki array semua yang ada dan semua nol secara berurutan.

84
00:06:57.230 --> 00:06:58.050
Sempurna.

85
00:06:58.050 --> 00:06:59.870
Jadi itulah yang kita butuhkan.

86
00:06:59.870 --> 00:07:03.550
Dan kami siap untuk terus maju dan menjalankan regresi logistik menggunakan sikat belajar.

87
00:07:03.550 --> 00:07:11.370
Jadi semua data kami menyebar dan pada dasarnya kami akan melakukan proses yang sangat mirip dengan regresi linier dari bagian sebelumnya, tetapi

88
00:07:11.780 --> 00:07:18.340
kami hanya akan membuat model yang cocok dengan data ke dalam pemeriksaan model atau skor akurasi dan kemudian

89
00:07:18.340 --> 00:07:23.310
kami akan membagi data ke dalam set pengujian dan pelatihan dan melihat apakah

90
00:07:23.310 --> 00:07:24.600
hasil kami membaik.

91
00:07:24.600 --> 00:07:28.210
Jadi itu akan dimulai dengan memulai model.

92
00:07:28.210 --> 00:07:28.750
Jadi

93
00:07:31.560 --> 00:07:36.460
Anda telah melihat video ceramah regresi linier dan Anda akan melihat ini

94
00:07:36.460 --> 00:07:38.540
akan sangat mirip dengan itu.

95
00:07:38.540 --> 00:07:40.330
Tetapi dalam hal ini saya hanya akan menelepon.

96
00:07:42.590 --> 00:07:51.450
Log menggarisbawahi model objek regresi logistik saya dan kemudian saya akan mencocokkan data saya dengan memanggil

97
00:07:51.450 --> 00:07:53.030
metode fit.

98
00:07:53.030 --> 00:07:58.260
Jadi ini adalah persis apa yang kami lakukan dalam seri kuliah regresi linier.

99
00:07:58.260 --> 00:08:02.760
Tapi sekarang saya hanya memanggil model regresi logistik.

100
00:08:03.480 --> 00:08:09.990
Dan saya juga dapat memeriksa catatan melihat melihat model log panggilan garis bawah atau log garis bawah.

101
00:08:09.990 --> 00:08:17.070
Ya lihat ini untuk model skor X kolom.

102
00:08:18.100 --> 00:08:19.230
Dan begitulah.

103
00:08:19.230 --> 00:08:21.260
Jadi, apa sebenarnya angka ini berarti.

104
00:08:21.260 --> 00:08:24.710
Intinya 7 sampai dasarnya 3 kali seratus.

105
00:08:24.710 --> 00:08:32.420
Itu peringkat X C Anda sejauh ini sepertinya kami memiliki peringkat akurasi 73 persen 3 persen.

106
00:08:33.160 --> 00:08:38.730
Jadi mari kita lanjutkan dan membandingkan ini dengan data Why yang asli dan kita

107
00:08:38.730 --> 00:08:45.180
bisa melakukan ini dengan hanya mengambil rata-rata dari data putih karena itu dalam format 1 atau 0.

108
00:08:45.180 --> 00:08:48.260
Jadi ingat ini adalah seperti apa Y kita sekarang.

109
00:08:48.260 --> 00:08:50.250
Ini banyak dan nol.

110
00:08:50.720 --> 00:08:52.200
Jadi, jika Anda mengambil mean.

111
00:08:56.370 --> 00:09:02.740
Dari data itu yang pada dasarnya sesuai dengan persentase wanita yang berselingkuh.

112
00:09:03.960 --> 00:09:12.050
Jadi kita bisa menghitung persentase wanita yang dilaporkan berselingkuh dan yang akan dilakukan adalah membandingkan

113
00:09:12.050 --> 00:09:15.140
ini sebagai tingkat kesalahan nol.

114
00:09:15.140 --> 00:09:23.570
Dan saya menaruh tautan di sini di buku catatan Python jika Anda ingin melihat lebih jauh ke

115
00:09:23.570 --> 00:09:26.100
dalam tingkat kesalahan nol.

116
00:09:26.100 --> 00:09:37.420
Tetapi untuk sekarang apa artinya ini adalah Anda pada dasarnya memiliki sekitar 32 persen wanita berselingkuh.

117
00:09:37.420 --> 00:09:41.870
Jadi jika kita gulir ke bawah sini kembali ke buku AI Python.

118
00:09:42.660 --> 00:09:46.910
Jadi, Anda memiliki sekitar 32 persen wanita yang tidak berselingkuh.

119
00:09:46.910 --> 00:09:50.610
Jadi itu berarti jika Anda berselingkuh permisi.

120
00:09:50.610 --> 00:09:58.620
Jadi tiga koma dua persen wanita yang berselingkuh dan model mendapat peringkat akurasi

121
00:09:58.660 --> 00:10:03.650
73 persen sejauh menebak siapa yang tidak selingkuh.

122
00:10:03.650 --> 00:10:06.460
Tapi seberapa bagus ini sebenarnya.

123
00:10:06.460 --> 00:10:11.310
Jadi kita akan membandingkannya dengan tingkat kesalahan pabrik yang artinya ini.

124
00:10:11.310 --> 00:10:19.640
Jadi apa artinya ini adalah jika model kita hanya menebak tidak ada perselingkuhan untuk setiap wanita lajang yang kita miliki.

125
00:10:19.660 --> 00:10:26.070
Baru saja mendapat satu minus 0. 3 hingga Henri peringkat akurasi 68 persen.

126
00:10:26.070 --> 00:10:32.640
Jadi, sementara kami melakukan lebih baik daripada tingkat kesalahan pabrik di 73 persen dengan memasukkan semua variabel

127
00:10:32.670 --> 00:10:35.430
data dalam model regresi logistik kami.

128
00:10:35.670 --> 00:10:41.380
Kami tidak melakukan jauh lebih baik daripada tingkat kesalahan pabrik yang akurasi 60 persen.

129
00:10:41.800 --> 00:10:45.400
Jadi kita akan melakukan mirip dengan kuliah regresi linier.

130
00:10:45.400 --> 00:10:49.770
Jika kita hanya akan melanjutkan dan memeriksa koefisien model

131
00:10:49.770 --> 00:10:55.260
kita untuk memeriksa apa yang tampaknya menjadi prediktor kuat kemungkinan adil untuk melakukannya.

132
00:10:55.550 --> 00:11:02.650
Apa yang akan saya lakukan adalah membuat kerangka data koefisien.

133
00:11:02.650 --> 00:11:06.840
Ingat kami melakukan sesuatu yang sangat mirip dalam kuliah regresi linear

134
00:11:09.220 --> 00:11:10.600
untuk melakukan ini.

135
00:11:10.600 --> 00:11:11.590
Saya

136
00:11:14.220 --> 00:11:20.800
akan zip kolom saya di x dan kemudian saya akan menggunakan transpos numpad.

137
00:11:26.820 --> 00:11:37.700
Koefisien model log dan Anda hanya bisa mendapatkannya dengan memanggil kode C garis bawah hebat.

138
00:11:37.700 --> 00:11:41.650
Tidak ada tanda kurung terakhir di sana untuk bingkai data.

139
00:11:41.650 --> 00:11:51.480
OK jadi untuk memecah ini kita akan pergi ke depan dan menjalankan pemeriksaan ini dengan bingkai data seperti.

140
00:11:51.480 --> 00:11:55.310
Dan sekarang kita lihat di sini ambil koefisien log itu.

141
00:11:55.310 --> 00:11:57.200
Model regresi logistik.

142
00:11:57.200 --> 00:12:02.730
Transpose sehingga kita bisa mengaturnya sebagai bingkai data dengan kolom X.

143
00:12:03.580 --> 00:12:06.040
Dan sekarang di sini kita memiliki semua koefisien kita.

144
00:12:06.040 --> 00:12:06.710
Begitu.

145
00:12:08.350 --> 00:12:12.520
Apa yang kita lihat di sini melihat koefisien di sini.

146
00:12:12.520 --> 00:12:18.440
Kita dapat melihat bahwa koefisien positif sesuai dengan peningkatan kemungkinan berselingkuh,

147
00:12:18.680 --> 00:12:23.820
sedangkan koefisien negatif berarti berhubungan dengan penurunan kemungkinan berselingkuh.

148
00:12:23.820 --> 00:12:26.340
Sebagai titik nilai data meningkat.

149
00:12:26.770 --> 00:12:32.980
Jadi, Anda mungkin mengharapkan peningkatan peringkat pernikahan terkait dengan penurunan kemungkinan

150
00:12:32.980 --> 00:12:34.070
berselingkuh.

151
00:12:34.070 --> 00:12:39.050
Jadi jika wanita itu melaporkan memiliki pernikahan yang baik, kemungkinan berselingkuh berkurang.

152
00:12:39.050 --> 00:12:47.820
Itu masuk akal dan sebagian besar terlihat seperti peningkatan religiusitas tampaknya sesuai dengan

153
00:12:47.820 --> 00:12:50.230
penurunan kemungkinan berselingkuh.

154
00:12:50.980 --> 00:12:56.010
Karena semua variabel dummy pekerjaan istri dan suami adalah positif.

155
00:12:56.010 --> 00:13:02.190
Itu berarti bahwa kemungkinan terendah untuk berselingkuh sama dengan garis dasar yang merupakan pekerjaan

156
00:13:02.190 --> 00:13:03.970
yang kami jatuhkan.

157
00:13:03.970 --> 00:13:07.940
Dan dalam hal itu adalah salah satu yang merupakan pekerjaan siswa.

158
00:13:07.940 --> 00:13:14.430
Dan kami juga mengonfirmasi hal itu dengan visualisasi data kami di kuliah sebelumnya.

159
00:13:14.430 --> 00:13:17.650
Jadi mari kita lanjutkan.

160
00:13:18.070 --> 00:13:22.850
Sekarang kita telah melihat koefisien kita akan beralih ke bagian delapan yang menguji dan

161
00:13:22.850 --> 00:13:24.270
melatih set data kami.

162
00:13:24.270 --> 00:13:29.820
Jadi sekali lagi benar-benar mirip dengan kuliah regresi linier kita harus membagi data kita menjadi data

163
00:13:29.820 --> 00:13:30.990
pelatihan dan pengujian.

164
00:13:30.990 --> 00:13:37.300
Jadi kita akan mengikuti prosedur serupa dengan menggunakan pembelajaran psikis yang dibangun dengan metode split tes kereta.

165
00:13:38.400 --> 00:13:47.870
Jadi melompat kembali ke kode langsung, mari kita pergi dan membagi data jadi saya membuat empat objek di sini.

166
00:13:47.870 --> 00:14:04.820
X train x test Y train Y test dan saya katakan sama dengan train underscore test underscore split.

167
00:14:04.820 --> 00:14:11.760
Saya akan meneruskan data x dan y saya adalah titik-titik yang dibuat.

168
00:14:11.760 --> 00:14:18.600
Jadi sekarang psyched to learning telah secara otomatis membagi target Y dan x data menjadi data pelatihan

169
00:14:18.600 --> 00:14:19.800
dan pengujian.

170
00:14:19.800 --> 00:14:26.650
Mudah-mudahan saya Rimmer dari kuliah regresi linier yang dapat Anda sampaikan lebih banyak argumen.

171
00:14:26.650 --> 00:14:32.780
Tapi sekarang ini pada 75 25 dan itu secara acak.

172
00:14:32.780 --> 00:14:34.280
Karena itu adalah default.

173
00:14:34.280 --> 00:14:38.330
Jadi mari kita lanjutkan dan membuat model log baru.

174
00:14:38.330 --> 00:14:47.690
Jadi kami memanggil model regresi logistik jika Anda ingin mencatat model yang akan

175
00:14:48.750 --> 00:15:02.940
membuat regresi logistik objek regresi itu dan saya akan cocok dengan model baru itu untuk mencetak model dan menyesuaikannya dengan set pelatihan.

176
00:15:06.010 --> 00:15:06.630
BAIK.

177
00:15:06.630 --> 00:15:07.970
Besar.

178
00:15:07.970 --> 00:15:14.800
Dan sekarang kami memiliki objek regresi logistik kami dan karena kami melakukan pelatihan dan pengujian atau akan lakukan sekarang adalah

179
00:15:14.800 --> 00:15:17.840
seperti yang kami lakukan dalam kuliah regresi linier.

180
00:15:17.840 --> 00:15:25.810
Kita dapat menggunakan prediksi untuk memprediksi label klasifikasi untuk set tes berikutnya dan kemudian kita akan

181
00:15:25.810 --> 00:15:28.310
mengevaluasi kembali skor akurasi kita.

182
00:15:28.310 --> 00:15:43.350
Jadi saya akan membuat objek yang disebut Class predicts dan saya akan membuatnya sama dengan model log untuk memprediksi.

183
00:15:43.350 --> 00:15:50.240
Dan kami pada dasarnya mengambil model kami dan mencoba memprediksi set pengujian X kami.

184
00:15:50.240 --> 00:15:51.150
Baik.

185
00:15:51.150 --> 00:15:55.600
Dan sekarang kita akan membandingkan kelas prediksi dengan kelas tes yang sebenarnya.

186
00:15:55.600 --> 00:16:07.400
Jadi untuk melakukan ini saya akan Pangeran dan kita akan menggunakan perpustakaan metrik kit mata yang disebut metode

187
00:16:07.430 --> 00:16:10.430
skor akurasi garis bawah.

188
00:16:10.430 --> 00:16:11.340
Dan apa yang akan dilakukan.

189
00:16:11.680 --> 00:16:13.830
Saya akan menempatkan

190
00:16:16.400 --> 00:16:24.210
set pengujian saya terlebih dahulu dan saya akan membandingkannya dengan prediksi saya.

191
00:16:24.210 --> 00:16:25.690
Prediksi kelas saya.

192
00:16:25.690 --> 00:16:27.860
Jadi yang sedang dilakukan adalah.

193
00:16:30.120 --> 00:16:31.120
Baiklah, saya uraikan.

194
00:16:31.120 --> 00:16:39.190
Ingat kami dulu melatih tes blit untuk membuat set pelatihan dan set pengujian untuk Xs

195
00:16:39.190 --> 00:16:40.710
dan Ys.

196
00:16:40.710 --> 00:16:43.650
Jadi semua data kami dan kemudian kelas target kami.

197
00:16:44.230 --> 00:16:51.380
Dalam hal ini ini adalah klasifikasi biner Cesar atau atau yang diciptakan kembali model regresi logistik baru.

198
00:16:51.380 --> 00:17:00.740
Lalu kami cocok dengan model itu hanya menggunakan set data pelatihan kami dan kemudian model yang menjalankan prediksi

199
00:17:00.740 --> 00:17:04.450
menggunakan set data uji X sekarang.

200
00:17:04.450 --> 00:17:11.440
Dan kami menyebutnya kelas objek memprediksi prediktor klasifikasi, Anda dapat menyebutnya apa pun yang Anda inginkan.

201
00:17:11.440 --> 00:17:18.530
Dan saya akan menggunakan seperti itu belajar metrik bukan skor akurasi untuk membandingkan tes putih dengan

202
00:17:18.530 --> 00:17:20.180
prediksi kelas terasa.

203
00:17:20.210 --> 00:17:20.950
Jika itu sempurna.

204
00:17:21.290 --> 00:17:27.570
Ketika Anda mendapatkan satu kembali yang berarti semua prediksi Anda cocok dengan data pengujian Anda.

205
00:17:27.570 --> 00:17:36.750
Jadi mari kita pergi untuk melihat apa yang kita jalankan yang sepertinya kita dapatkan poin tujuh dua dua tiga.

206
00:17:36.750 --> 00:17:46.050
Jadi jika kita gulir kembali ke sini yang terakhir kita lakukan adalah poin 7 hingga 5 8.

207
00:17:46.050 --> 00:17:50.650
Jadi sangat mirip dengan aslinya.

208
00:17:50.650 --> 00:17:54.180
Ini pada dasarnya skor akurasi yang sama.

209
00:17:54.180 --> 00:18:01.050
Jadi apa yang bisa kita lakukan untuk lebih meningkatkan model regresi logistik kita akan melompat ke sini kembali

210
00:18:01.490 --> 00:18:02.980
ke notebook Python.

211
00:18:02.980 --> 00:18:10.390
Jadi kami mencoba pelatihan dan pengujian kami dan pada dasarnya ketika saya menjalankannya pertama kali di sini saya mendapat tujuh puluh

212
00:18:10.390 --> 00:18:11.920
tiga koma tiga lima.

213
00:18:12.670 --> 00:18:16.130
Yang pada dasarnya sama dengan yang asli saya tujuh dua koma lima delapan.

214
00:18:16.710 --> 00:18:24.330
Jadi apa yang dapat kita lakukan untuk meningkatkan model kita sehingga kita dapat mencoba beberapa teknik regularisasi.

215
00:18:24.330 --> 00:18:30.880
Dan saya memiliki tautan di sana untuk menunjukkan kepada Anda tentang regularisasi dalam pembelajaran statistik mesin sehingga Anda dapat keluar

216
00:18:30.880 --> 00:18:31.820
dan memeriksanya.

217
00:18:33.360 --> 00:18:37.840
Dan kita juga bisa menggunakan model non-linear yang akan dibahas dalam kuliah mendatang.

218
00:18:37.840 --> 00:18:44.720
Tetapi pada dasarnya saya akan meninggalkan topik regresi logistik di sini agar Anda dapat

219
00:18:44.770 --> 00:18:46.900
mengeksplorasi lebih banyak sendiri.

220
00:18:46.900 --> 00:18:54.730
Dan saya telah memberi Anda beberapa contoh sumber regresi logistik yang hebat yang beberapa di antaranya menggunakan

221
00:18:54.730 --> 00:18:56.310
pustaka pembelajaran psikis.

222
00:18:56.660 --> 00:19:03.010
Jadi saya akan menuju dan menunjukkan kepada Anda ini cepat pertama nyata adalah posting yang sangat bagus

223
00:19:03.010 --> 00:19:07.560
tentang menggunakan analisis regresi logistik dengan modul statistik model dari y hat

224
00:19:07.560 --> 00:19:13.880
Jadi, jika Anda mengklik tautan itu, saya akan membawa Anda ke sini dan hampir seperti kuliah lain yang baru saja saya lalui

225
00:19:13.880 --> 00:19:15.970
dalam kasus ini mereka menggunakan model statistik.

226
00:19:15.970 --> 00:19:17.750
Sekali lagi mereka memvisualisasikan data mereka.

227
00:19:17.750 --> 00:19:24.570
Mereka membahas variabel dummy dan Pan yang menggunakan get dummies dan kemudian mereka juga melakukan

228
00:19:26.210 --> 00:19:32.850
regresi dan mereka lagi mereka menyebutkan multi-collinearity kadang-kadang disebut perangkap variabel dummy yang saya

229
00:19:32.850 --> 00:19:34.360
sebutkan sebelumnya.

230
00:19:35.780 --> 00:19:40.350
Dan mereka menginterpretasikan hasil dalam kasus ini yang tidak mereka gunakan. Saya tidak dapat mempelajari model statistik ini

231
00:19:40.740 --> 00:19:45.350
tetapi Anda dapat melanjutkan dan mempelajari lebih lanjut tentang itu dan kemudian mereka masuk lebih dalam ke dalam rasio

232
00:19:46.070 --> 00:19:48.930
odds dan kemudian mereka menggali lebih jauh lagi menggunakan nomor pi.

233
00:19:49.400 --> 00:19:52.660
Hebat jadi pasti mendengar Anda memeriksa ini.

234
00:19:52.660 --> 00:19:58.160
Sumber yang sangat bagus untuk dilihat terutama setelah melihat ceramah saya.

235
00:19:59.510 --> 00:20:04.480
Sekarang dokumentasi yang dipelajari secara psikis mencakup beberapa contoh jadi jika Anda mengklik tautan itu akan membawa Anda

236
00:20:04.480 --> 00:20:05.140
ke psikis.

237
00:20:05.140 --> 00:20:07.740
Pelajari regresi logistik.

238
00:20:07.740 --> 00:20:12.060
Ini akan membahas API itu sendiri.

239
00:20:12.740 --> 00:20:17.750
Cara menyesuaikan cara mendapatkan parameter cara menggunakan prediksi, tetapi jika Anda gulir ke

240
00:20:17.750 --> 00:20:21.910
bawah sini ada banyak contoh di sini sejauh contoh klasifikasi.

241
00:20:21.910 --> 00:20:26.270
Jadi pasti periksa juga.

242
00:20:26.270 --> 00:20:33.450
Sumber ketiga adalah dari data robot yang memiliki tinjauan umum regresi logistik jadi jika Anda

243
00:20:33.450 --> 00:20:39.420
mengklik tautan ini akan membawa Anda ke sana-sini menggunakan klasifikasi psikis belajar.

244
00:20:39.420 --> 00:20:48.570
Jadi mereka melakukan gambaran yang sangat mirip dari pemahaman klasifikasi dan sekali lagi di sini mereka menyebutkan regresi

245
00:20:48.570 --> 00:20:51.920
dan klasifikasi sebenarnya masalah yang sama.

246
00:20:51.920 --> 00:20:59.380
Sama seperti kita pergi pada dasarnya memasukkan regresi linier ke dalam fungsi logistik dan mereka membahas bagaimana

247
00:20:59.380 --> 00:21:00.770
Anda mempelajarinya.

248
00:21:01.530 --> 00:21:02.730
Metrik lagi.

249
00:21:03.960 --> 00:21:09.830
Mereka juga menunjukkan matriks kebingungan yang dapat Anda periksa dan Google sendiri

250
00:21:09.830 --> 00:21:18.450
dan mereka membahas beberapa teknik klasifikasi regresi logistik lebih lanjut dan kami akan membahas beberapa di antaranya dalam kuliah mendatang.

251
00:21:18.450 --> 00:21:25.050
Dan akhirnya ada sumber daya yang fantastis di sini mulai dari emosi AI sampai titik blog pada regresi logistik,

252
00:21:25.050 --> 00:21:29.020
matematika tentang bagaimana hal itu berkaitan dengan fungsi biaya dan gradien.

253
00:21:29.020 --> 00:21:39.400
Jadi jika Anda mengikuti dengan kursus internet blogspot ini di sini akan jenis memecah menggunakan Python bagaimana hal itu berkaitan dengan fungsi biaya

254
00:21:40.570 --> 00:21:46.950
dan gradien sehingga Andrew tentu saja berjalan lebih dari itu juga dan di sini

255
00:21:46.950 --> 00:21:50.340
Anda dapat melihatnya disajikan melalui python.

256
00:21:50.340 --> 00:21:54.210
OK jadi masuk dan periksa semua sumber daya itu.

257
00:21:54.210 --> 00:22:00.630
Anda akan memerhatikan saat kami bergerak lebih jauh dalam kuliah pembelajaran mesin, kemungkinan aktual

258
00:22:00.630 --> 00:22:02.340
cenderung semakin berkembang.

259
00:22:02.340 --> 00:22:09.440
Jadi ada banyak cara untuk mendekati masalah pembelajaran mesin dan saya selalu berusaha memberi Anda sebanyak mungkin

260
00:22:09.440 --> 00:22:11.580
sumber daya yang saya bisa.

261
00:22:12.610 --> 00:22:18.820
Semoga Anda menikmati ikhtisar regresi logistik ini dan saya harap Anda belajar banyak.

262
00:22:18.820 --> 00:22:20.100
Baiklah, terima kasih kawan.
