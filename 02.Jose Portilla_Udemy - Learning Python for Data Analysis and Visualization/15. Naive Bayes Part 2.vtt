WEBVTT
1
00:00:02.400 --> 00:00:07.800
Hai semuanya dan selamat datang di kuliah kedua dan seri fase naif pembelajaran yang diawasi ini.

2
00:00:07.800 --> 00:00:12.840
Jadi dalam kuliah terakhir kami membahas hal-hal berikut.

3
00:00:12.840 --> 00:00:18.960
Kami melihat hanya beberapa catatan tentang notasi dalam istilah matematika produk urutan

4
00:00:18.960 --> 00:00:21.750
ini dan argumen notasi maksimum.

5
00:00:21.990 --> 00:00:27.270
Dan kami pergi ke tempat untuk melihat teorema Bayes secara umum bahwa dosen Bayes

6
00:00:27.720 --> 00:00:32.060
Theorem dalam lampiran mendapat pengantar cepat untuk karya Bayes secara teori.

7
00:00:32.640 --> 00:00:38.550
Dan bagian penting di sana adalah bahwa Beys yang naif mengasumsikan semua fitur harus berada di dalam liontin

8
00:00:38.550 --> 00:00:43.350
satu sama lain dan kita tahu bahwa biasanya tidak demikian dalam data dunia

9
00:00:43.350 --> 00:00:49.510
nyata tetapi pengembalian rendah akurasi yang cukup baik dengan asumsi itu maka kita punya cepat ikhtisar classifier Angkatan Laut.

10
00:00:49.590 --> 00:00:58.170
Jadi kami telah memberikan variabel kelas y vektor fitur independen x X satu melalui Xon oven.

11
00:00:58.170 --> 00:01:03.190
Kemudian kita tancapkan ke Bayes Theorem dan gunakan itu bahkan bukan asumsi pennance.

12
00:01:03.320 --> 00:01:11.310
Kita dapat menyatakan kembali probabilitas ini untuk memiliki semua fitur ini diberikan dalam kelas Y sebagai urutan produk

13
00:01:11.310 --> 00:01:18.100
memiliki fitur tunggal kelas Y tertentu karena kita memiliki itu dalam asumsi pendens di sana.

14
00:01:19.380 --> 00:01:25.650
Jadi karena dalam kursus ini kita lebih fokus pada aplikasi praktis ilmu data menggunakan Python

15
00:01:25.650 --> 00:01:27.440
dan sikat belajar.

16
00:01:27.630 --> 00:01:30.610
Saya ingin menunjukkan beberapa hal.

17
00:01:30.660 --> 00:01:36.230
Jadi jika Anda gulir ke bawah ke bagian bawah kuliah ini Anda akan melihat di sini kami memiliki lebih banyak sumber daya untuk Anda.

18
00:01:36.510 --> 00:01:39.580
Jadi saya sangat merekomendasikan tiga yang terakhir ini.

19
00:01:39.660 --> 00:01:44.990
Jika Anda ingin belajar lebih banyak tentang matematika dasar pisau.

20
00:01:45.060 --> 00:01:50.370
Jadi kita telah memasuki catatan kelas berikutnya sebuah ceramah video yang dia lakukan serta U. C. Kuliah -Berkeley.

21
00:01:50.370 --> 00:01:59.250
Yang ini panjangnya sekitar satu jam tapi sangat bagus dan akan kembali ke sini jadi kita tahu bagaimana kelas malam

22
00:01:59.520 --> 00:02:02.010
untuk tinjauan umum matematika bekerja.

23
00:02:02.010 --> 00:02:08.220
Memasukkannya di fitur liontin atau meminta variabel kelas dan vektor fitur pendalaman.

24
00:02:08.550 --> 00:02:14.700
Dan kemudian kita mengasumsikan bahwa fitur independen yang kita dapat membangun classifier dari

25
00:02:14.700 --> 00:02:16.490
model probabilitas itu.

26
00:02:16.680 --> 00:02:21.940
Jadi kita memperoleh bahwa model fitur independen menggunakan model probabilitas berbasis pisau dan api

27
00:02:21.940 --> 00:02:27.390
kelas dasar menggabungkan model untuk keputusan dan kemudian bahwa aturan keputusan akan memungkinkan kita untuk memutuskan

28
00:02:27.390 --> 00:02:29.300
hipotesis mana yang paling mungkin.

29
00:02:29.340 --> 00:02:33.840
Dan dalam contoh kita akan menjadi kelas bunga mana yang paling mungkin.

30
00:02:33.870 --> 00:02:40.170
Jadi untuk memilih hipotesis yang paling memungkinkan, kami menggunakan apa yang disebut aturan keputusan peta dan saya akan membiarkan

31
00:02:40.170 --> 00:02:44.940
Anda maju dan memeriksa sumber daya di bagian bawah jika Anda ingin mempelajari matematika lebih

32
00:02:44.940 --> 00:02:46.200
lanjut di sini.

33
00:02:46.560 --> 00:02:52.860
Tetapi pada dasarnya apa yang terjadi adalah diberikan bahwa probabilitas bahwa vektor fitur

34
00:02:52.860 --> 00:03:00.210
adalah konstan diberikan input apa pun yang kita miliki, kita menggunakan aturan klasifikasi yang memetakan aturan keputusan

35
00:03:00.210 --> 00:03:06.580
dan kemudian kita dapat memperkirakan berapa probabilitas berada di kelas tertentu mengingat vektor fitur tersebut.

36
00:03:06.810 --> 00:03:11.880
Dan kemudian perbedaan utama antara pengklasifikasi basis naif adalah asumsi yang mereka

37
00:03:11.880 --> 00:03:16.890
buat mengenai distribusi istilah ini di sini kemungkinan memiliki fitur tunggal.

38
00:03:17.040 --> 00:03:19.450
Mengingat Anda berada di kelas mengapa.

39
00:03:20.850 --> 00:03:26.250
Dan seperti saya katakan maju dan menyelam lebih dalam ke sumber daya lain jika Anda

40
00:03:26.280 --> 00:03:28.530
tertarik dengan matematika yang lebih berat.

41
00:03:28.530 --> 00:03:32.810
Tapi kita akan fokus pada kuliah khusus ini Gaussian my base.

42
00:03:32.870 --> 00:03:38.850
Jadi ketika Anda berurusan dengan data kontinu, asumsi umum yang ingin Anda buat adalah bahwa kontinu adalah

43
00:03:39.240 --> 00:03:43.740
bahwa nilai kontinu dari setiap kelas didistribusikan sesuai dengan distribusi Gaussian dan Anda

44
00:03:44.270 --> 00:03:48.540
dapat pergi dan memeriksa kuliah distribusi normal untuk mendapatkan review formula

45
00:03:48.540 --> 00:03:50.600
untuk Gaussian atau distribusi normal.

46
00:03:51.090 --> 00:03:57.540
Tapi mudah-mudahan jika Anda sudah terbiasa dengan distribusi normal atau distribusi Gaussian yang bisa kita

47
00:03:57.540 --> 00:04:03.830
lakukan adalah memasukkan data pelatihan itu ke distribusi Gaussian dan apa yang akan Anda lakukan

48
00:04:06.550 --> 00:04:12.520
untuk itu adalah Anda akan mengelompokkan data berdasarkan kelas pertama adalah data latihan

49
00:04:12.520 --> 00:04:17.710
Anda maka Anda akan menghitung varians rata-rata x di setiap kelas.

50
00:04:17.710 --> 00:04:24.760
Dan contoh khusus ini saya minta Anda menggunakan subsea menjadi mean dari nilai dan X

51
00:04:24.760 --> 00:04:33.470
associate dari kelas C dan Anda membiarkan Sigma kuadrat lihat menjadi varians dari nilai dan X associate dari kelas C.

52
00:04:33.520 --> 00:04:36.740
Jadi, Anda memiliki distribusi probabilitas dari beberapa nilai.

53
00:04:37.120 --> 00:04:44.740
Jadi karena x sama dengan C yang diberikan, Anda dapat menghitungnya dengan memasukkan V

54
00:04:44.740 --> 00:04:53.540
ke dalam persamaan untuk distribusi normal dengan parameter myu itu dan varian yang baru saja kami hitung.

55
00:04:54.400 --> 00:04:59.140
Dan kemudian jika Anda terbiasa jika pengadilan itu dan jika Anda kembali ke kemewahan itu pada

56
00:04:59.140 --> 00:05:03.130
dasarnya Anda bisa melihat bagaimana itu adalah fungsi distribusi Gaussian di sana.

57
00:05:03.130 --> 00:05:09.780
Mengingat bahwa Anda melakukan segmentasi data untuk semua fitur tersebut oleh setiap kelas.

58
00:05:10.030 --> 00:05:10.930
BAIK.

59
00:05:11.860 --> 00:05:16.400
Jadi, periksa sumber daya lain jika Anda ingin menyelam lebih dalam ke matematika.

60
00:05:16.510 --> 00:05:22.830
Tapi karena kita sedang melihat aplikasi yang lebih praktis dari pembelajaran mesin kita akan menyelam

61
00:05:22.840 --> 00:05:29.770
dan itu sikat belajar perpustakaan sekarang tapi ingat kunci dasar malam membuat Anda tahu asumsi yang cukup

62
00:05:29.770 --> 00:05:35.590
besar bahwa ada atau tidak adanya masing-masing fitur data independen satu sama lain dan

63
00:05:35.590 --> 00:05:38.830
tergantung pada data yang memiliki label tertentu.

64
00:05:40.210 --> 00:05:48.880
Jadi pindah ke bagian 7 Gaussian Beys dari sikat learn sebenarnya tidak akan melakukan coding langsung untuk kuliah khusus ini karena Anda akan melihat

65
00:05:48.880 --> 00:05:54.490
itu sangat sangat mirip dengan apa yang telah kami lakukan di masa lalu dan

66
00:05:54.490 --> 00:06:00.370
sebenarnya benar-benar tidak perlu bagiku ketik melalui itu dan Anda akan melihat ketika kita pergi bersama

67
00:06:00.370 --> 00:06:06.120
mengapa itu terjadi jadi mengapa mereka di sini di bagian 7 ini diimpor seperti biasa.

68
00:06:06.460 --> 00:06:11.620
Dan dalam hal ini kita sebenarnya hanya akan menggunakan perpustakaan yang dipelajari sikat ini.

69
00:06:11.620 --> 00:06:18.220
Anda tidak perlu mengimpor panda atau peta literasi yang lahir sejak dalam beberapa kuliah terakhir kita lanjutkan.

70
00:06:18.250 --> 00:06:21.520
Kami pergi ke depan dan melihat kumpulan data dan kami menganalisisnya sedikit.

71
00:06:21.520 --> 00:06:22.780
Jadi kita tidak perlu merencanakan apa pun.

72
00:06:22.780 --> 00:06:25.370
Kami sudah terbiasa dengan set data iris.

73
00:06:26.140 --> 00:06:30.430
Jadi, lanjutkan apa yang kita lakukan di sini.

74
00:06:30.640 --> 00:06:36.440
Sama seperti di kuliah klasifikasi multikelas terakhir, Anda memuat kumpulan data sebagai set data yang

75
00:06:36.450 --> 00:06:39.660
sama dengan yang memuat tanda kurung Irus.

76
00:06:39.970 --> 00:06:41.510
Kami mengambil fitur dan target.

77
00:06:41.530 --> 00:06:46.590
Ingat kami melakukan ini sebelum dan kemudian kuliah klasifikasi multikelas di sini kami

78
00:06:46.590 --> 00:06:50.160
mencetak deskripsi data bawaan yang opsional untuk Anda.

79
00:06:51.550 --> 00:06:57.140
Dan di sini kita akan masuk ke metode berbasis pisau.

80
00:06:57.220 --> 00:07:02.680
Jadi kita sudah melakukan analisis umum pada kuliah sebelumnya dan yang ingin kita lakukan untuk metode

81
00:07:02.680 --> 00:07:08.060
berbasis malam akan sangat sangat mirip dengan metode klasifikasi multikelas yang kita gunakan sebelumnya.

82
00:07:08.350 --> 00:07:11.710
Jadi, hal pertama yang Anda lakukan adalah membuat model.

83
00:07:11.710 --> 00:07:18.060
Jadi dalam hal ini kita sebut Gaussian dan B yang menyebabkan basis tidak ada argumen tambahan di sana.

84
00:07:18.100 --> 00:07:19.930
Kami menetapkan itu sama dengan model.

85
00:07:20.590 --> 00:07:27.070
Dan sekarang setelah Anda memiliki model Anda, Anda dapat melanjutkan dengan memisahkannya ke dalam set pelatihan dan pengujian

86
00:07:27.070 --> 00:07:33.580
seperti yang Anda lakukan dalam kuliah terakhir dan di sini saya membagi data ke dalam set pelatihan dan

87
00:07:33.580 --> 00:07:40.750
pengujian X train x test Y train y test dan kami melakukannya dengan menggunakan perpecahan tes kereta dari perpustakaan validasi

88
00:07:40.760 --> 00:07:42.390
silang dan sikat dipelajari.

89
00:07:43.600 --> 00:07:48.210
Dan sekarang setelah Anda memilikinya, Anda dapat menyesuaikan model Anda menggunakan set data pelatihan.

90
00:07:48.250 --> 00:07:55.690
Jadi kami telah memodelkan kecocokan itu dan saya memberikan data pelatihan saya baik fitur maupun kelasnya dan kemudian

91
00:07:55.720 --> 00:07:58.360
untuk memprediksi hasil dari set pengujian.

92
00:07:58.450 --> 00:08:04.650
Saya hanya bisa mengatakan memodelkan metode prediksi dan kemudian memiliki data pengujian saya di sana.

93
00:08:04.720 --> 00:08:11.920
Jadi saya menetapkan bahwa sebagai objek yang disebut diprediksi sama dengan model masa lalu dan metode prediksi menggunakan uji

94
00:08:11.920 --> 00:08:17.800
X dan mudah-mudahan semuanya sangat mirip atau akrab dari klasifikasi kuliah multikelas lainnya terutama karena dia

95
00:08:17.860 --> 00:08:20.860
berada di kumpulan data yang sama persis.

96
00:08:21.820 --> 00:08:25.490
Dan kemudian Anda benar-benar hasil yang diharapkan sedangkan yang paling putih.

97
00:08:25.690 --> 00:08:32.220
Jadi ingat kami membagi pelatihan dan pengujian untuk fitur dan target.

98
00:08:32.260 --> 00:08:37.590
Kami melatih model kami menggunakan fit dengan data pelatihan.

99
00:08:37.900 --> 00:08:45.430
Dan kemudian kami ingin memprediksi hasil dari fitur x pengujian kami dan kami ingin

100
00:08:45.430 --> 00:08:48.810
membandingkannya dengan kelas tes White.

101
00:08:48.820 --> 00:08:53.440
Jadi dalam hal metrik untuk kinerja seperti yang kami lakukan terakhir

102
00:08:54.190 --> 00:09:03.400
kali yang akan kami lakukan adalah mencetak metrik yang akurasi menggarisbawahi skor dan lulus yang diharapkan dibandingkan yang diprediksi dan sepertinya kami memiliki

103
00:09:03.400 --> 00:09:09.660
akurasi sembilan puluh empat koma tujuh persen yang mungkin berubah untuk Anda tergantung pada bagaimana

104
00:09:09.750 --> 00:09:12.350
keacakan perpecahan tes kereta ini terjadi.

105
00:09:12.840 --> 00:09:19.350
Tapi seperti yang Anda lihat di sini, ini bukan soal akurasi mengingat asumsi naif bahwa semua fitur

106
00:09:19.410 --> 00:09:22.530
itu ada di liontin satu sama lain.

107
00:09:22.620 --> 00:09:31.200
Jadi hanya untuk melihat apa yang kami lakukan dalam kuliah ini, kami membuat beberapa catatan tentang notasi dalam istilah matematika yang mungkin berguna

108
00:09:31.320 --> 00:09:34.650
untuk memahami white base seperti produk dari urutan.

109
00:09:34.950 --> 00:09:41.860
Dan argumen maksimum yang kami bahas berdasarkan teorema melakukan pengantar cepat ke dasar pisau.

110
00:09:41.920 --> 00:09:48.720
Ingat poin utama di sini adalah Anda mengasumsikan semua fitur harus berada di titik satu sama lain.

111
00:09:48.720 --> 00:09:50.190
Itu takeaway utama.

112
00:09:50.820 --> 00:09:57.080
Dan kemudian kita melihat dan kelas dasar untuk tinjauan matematika bagaimana kita pasang itu ke teorema dasar.

113
00:09:57.110 --> 00:10:03.660
Gunakan knave dalam asumsi Penda untuk membaginya menjadi produk urutan ini di sini dan

114
00:10:03.660 --> 00:10:10.710
kemudian dalam kasus khusus kami, kami menggunakan aturan keputusan peta yang dapat Anda baca lebih lanjut tentang

115
00:10:12.400 --> 00:10:19.080
sumber daya lain dan dalam kasus khusus kami, kami akan menggunakan Gaussian, basis yang berarti bahwa

116
00:10:19.110 --> 00:10:26.070
kita mengasumsikan nilai kontinu yang terkait untuk setiap kelas didistribusikan sesuai dengan distribusi Gaussian atau normal.

117
00:10:26.870 --> 00:10:27.550
BAIK.

118
00:10:27.900 --> 00:10:34.320
Dan kemudian kami baru saja melakukan tinjauan singkat tentang bagaimana menerapkannya dalam sikat belajar dan mudah-mudahan itu

119
00:10:34.320 --> 00:10:39.940
benar-benar mirip dengan kuliah klasifikasi multikelas lain yang pernah Anda lihat di masa lalu.

120
00:10:39.960 --> 00:10:44.220
Sekali lagi saya sangat menyarankan Anda memeriksa sumber daya lebih lanjut tentang ini.

121
00:10:44.250 --> 00:10:48.780
Jadi ada dokumentasi belajar sikat yang sangat bagus.

122
00:10:48.900 --> 00:10:56.160
Saya memiliki tautan lain ini di sini menggunakan Beys saya dengan LDK yang memproses bahasa alami

123
00:10:56.160 --> 00:11:06.090
di sini di Solander berarti itu atau contoh yang lebih baik dari presentasi Beys pisau yang lebih klasik yang merupakan pengklasifikasi

124
00:11:06.890 --> 00:11:07.840
spam.

125
00:11:07.860 --> 00:11:11.850
Itulah contoh Beys naif klasik.

126
00:11:11.850 --> 00:11:17.350
Dan kemudian dokumentasi itu sendiri di sini adalah semua persamaan yang sedang kita bahas.

127
00:11:17.610 --> 00:11:23.940
Dan pangkalan naif spesifik pertama adalah Gaussian I Anda mengklik tautan itu di sini dalam dokumentasi untuk membahas

128
00:11:24.480 --> 00:11:27.180
lebih banyak argumen yang dapat Anda sampaikan.

129
00:11:27.180 --> 00:11:32.210
Dan contoh lain cara kerja Ghasia markas saya.

130
00:11:32.280 --> 00:11:36.720
Lalu ada artikel Wikipedia tentang klasifikasi Bayes saya.

131
00:11:36.720 --> 00:11:39.430
Ini benar-benar didokumentasikan dengan sangat baik.

132
00:11:39.510 --> 00:11:46.080
Sekali lagi ia membahas model probabilitas probabilistik yang kami bahas lebih detail jika Anda ingin

133
00:11:46.080 --> 00:11:47.590
lebih banyak matematika.

134
00:11:47.820 --> 00:11:56.160
Dan kemudian ia membahas Gaussian fase bersama dengan contoh yang saya tunjukkan kepada Anda dan kemudian ada masukkan catatan kelas berikutnya dalam video

135
00:11:56.160 --> 00:11:59.980
ceramah serta U. C. Kuliah -Berkeley.

136
00:12:00.120 --> 00:12:03.540
Jadi saya mendorong Anda untuk memeriksa lebih banyak sumber daya di pangkalan saya.

137
00:12:03.540 --> 00:12:05.660
Pasti sesuatu yang Anda harus baca lebih lanjut.

138
00:12:05.970 --> 00:12:10.530
Tetapi jika Anda hanya ingin praktis bagaimana menerapkannya di sikat belajar.

139
00:12:10.530 --> 00:12:15.080
Anda akan mendapati bahwa Anda berdasarkan pada kuliah-kuliah sebelumnya sudah cukup tahu cara melakukannya.

140
00:12:15.160 --> 00:12:17.130
Anda hanya perlu memasukkan model baru.

141
00:12:17.400 --> 00:12:17.890
BAIK.

142
00:12:18.030 --> 00:12:23.280
Jadi saya harap itu bermanfaat bagi kalian dan Anda menikmati ceramah dan sampai jumpa di ceramah berikutnya.

143
00:12:23.280 --> 00:12:25.010
Baiklah, terima kasih kawan.
