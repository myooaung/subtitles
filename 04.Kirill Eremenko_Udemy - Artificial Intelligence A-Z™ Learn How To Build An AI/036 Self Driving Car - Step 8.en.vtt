WEBVTT
1
00:00:00.240 --> 00:00:02.790
Hello and welcome to this Python tutorial.

2
00:00:02.790 --> 00:00:07.230
All right we have one last function to implement in our replay memory class.

3
00:00:07.230 --> 00:00:12.060
That's the simple function and that's of course to get some random samples from our memory.

4
00:00:12.210 --> 00:00:16.080
And therefore this function will return these random samples.

5
00:00:16.080 --> 00:00:17.850
All right so let's into it.

6
00:00:17.870 --> 00:00:20.630
We are going to call it simple.

7
00:00:20.640 --> 00:00:21.330
Here we go.

8
00:00:21.420 --> 00:00:25.030
And this function takes two arguments as input.

9
00:00:25.260 --> 00:00:32.490
The first one as usual self our future object to replay memory class and the second argument is can

10
00:00:32.490 --> 00:00:33.430
you try to guess.

11
00:00:33.510 --> 00:00:40.290
Well we're taking some samples of fixed size and therefore we need to choose a size for samples and

12
00:00:40.290 --> 00:00:42.610
more precisely we call it a batch size.

13
00:00:42.820 --> 00:00:47.910
So that's the name we're going to give to our second argument batch size.

14
00:00:48.180 --> 00:00:49.530
And there we go.

15
00:00:49.560 --> 00:00:54.490
We have our two arguments and now we can implement the simple function.

16
00:00:54.500 --> 00:01:00.780
So now I just want to warn you this is going to get a little technical but I'll try my best to explain.

17
00:01:01.250 --> 00:01:05.140
So we're going to start by creating the samples variable.

18
00:01:05.150 --> 00:01:09.620
This is just the variable that will contain the samples of the memory.

19
00:01:09.620 --> 00:01:11.340
All right so simple is equal.

20
00:01:11.360 --> 00:01:14.090
And so now how are we going to get these samples.

21
00:01:14.450 --> 00:01:20.960
Well first of all we have to take our memory because we are getting the samples from our memory.

22
00:01:20.960 --> 00:01:26.870
Then we will probably need the batch size because the samples are going to get contained batch size

23
00:01:26.930 --> 00:01:27.720
Elon's.

24
00:01:27.830 --> 00:01:33.920
So we need memory we need batch size and then we need some by torche or bison tricks to get a good format

25
00:01:34.040 --> 00:01:35.600
of these samples.

26
00:01:35.600 --> 00:01:40.790
So what I'm going to do I'm going to write the line of code and then I'm going to explain that element

27
00:01:40.790 --> 00:01:41.900
by element.

28
00:01:41.990 --> 00:01:42.860
So let's do it.

29
00:01:42.860 --> 00:01:48.580
I'm starting by taking a zip function I'm going to explain very soon what it does.

30
00:01:48.620 --> 00:01:51.980
And inside this function I'm going to add a star.

31
00:01:52.100 --> 00:01:53.800
I'm going to expand that as well.

32
00:01:53.900 --> 00:02:02.600
The Star and random thought Central so random as you might have guessed is the random library that we

33
00:02:02.600 --> 00:02:03.360
imported here.

34
00:02:03.410 --> 00:02:08.900
So that's the main reason why we had to import this random libraries because we're taking some random

35
00:02:08.900 --> 00:02:09.730
samples.

36
00:02:10.040 --> 00:02:15.570
So from this random library we're going to use the simple function.

37
00:02:15.600 --> 00:02:20.060
So this is our variables and this is a function so I'm going to add some parenthesis.

38
00:02:20.210 --> 00:02:26.110
And now as you can see sample is a function and we have to put some arguments so that you can see the

39
00:02:26.110 --> 00:02:27.850
first argument itself.

40
00:02:27.850 --> 00:02:34.660
And actually speaking of self this corresponds to self-talk memory the memory of our future instance

41
00:02:34.870 --> 00:02:36.970
object of our replay memory for us.

42
00:02:37.180 --> 00:02:44.290
So I'm going to add here self that memory and then the second argument is as you might have guessed

43
00:02:44.530 --> 00:02:51.360
the size of the Bechuana take randomly from our memory and that we gave it a name that is batch size.

44
00:02:51.550 --> 00:02:55.690
So the second argument is going to be Bachche sucks.

45
00:02:55.870 --> 00:02:56.410
All right.

46
00:02:56.470 --> 00:03:01.080
So the line of code is typed and I'm going to explain what it does.

47
00:03:01.510 --> 00:03:08.260
So first of all with this random dot simple function we are taking some random samples from the memory

48
00:03:08.800 --> 00:03:11.820
that had a fixed size of that size.

49
00:03:12.550 --> 00:03:13.940
So that's understandable.

50
00:03:14.150 --> 00:03:18.230
But then what does this Zipp star function does.

51
00:03:18.530 --> 00:03:20.500
Well there is no mystery about it.

52
00:03:20.540 --> 00:03:22.870
It is just like reshape function.

53
00:03:22.880 --> 00:03:28.460
So for example I'm going to add a little come here just to explain that I'm going to remove it.

54
00:03:28.460 --> 00:03:34.680
So let's say that for example we have a list of the following elements for example.

55
00:03:34.820 --> 00:03:37.900
First one two three.

56
00:03:38.180 --> 00:03:39.680
And then the second element.

57
00:03:39.680 --> 00:03:43.180
Four five six.

58
00:03:43.190 --> 00:03:48.020
So we have a list of two doubles of three elements one two three four five six.

59
00:03:48.380 --> 00:03:52.690
Well then if I apply the zip function with the star on it.

60
00:03:52.880 --> 00:04:02.840
Well what would become so Zip star list is going to be equal to a new list but of a different shape

61
00:04:03.380 --> 00:04:12.440
and different shape is going to be one for then two three and then five six.

62
00:04:12.460 --> 00:04:12.770
All right.

63
00:04:12.800 --> 00:04:13.950
So that's just what it does.

64
00:04:13.970 --> 00:04:16.640
It just reshapes your list.

65
00:04:16.990 --> 00:04:21.610
OK so now that you understand what this Zipp star list does.

66
00:04:21.680 --> 00:04:24.560
Well now let's explain why we had to do it.

67
00:04:24.590 --> 00:04:30.360
So as you understood we are going to add the events to the memory and the events have the form.

68
00:04:30.440 --> 00:04:34.530
First the state then the action and then the reward.

69
00:04:34.820 --> 00:04:40.330
But for our algorithm we don't want this format we actually want our samples to have the following format.

70
00:04:40.330 --> 00:04:47.540
A format is composed of three samples one sample for the states one sample for the actions and one sample

71
00:04:47.540 --> 00:04:48.660
for the reward.

72
00:04:48.800 --> 00:04:53.690
So for example let's say that this one to three is state one action one.

73
00:04:53.800 --> 00:05:01.100
We want one and then state to action two and we were to well what we want is one batch for each one

74
00:05:01.100 --> 00:05:03.630
batch for state one and state two.

75
00:05:03.680 --> 00:05:10.090
One is a match for action one in action two and a third that for we were one and we were two.

76
00:05:10.190 --> 00:05:15.680
That's just the format that is going to be expected next because then we'll wrap these batches into

77
00:05:15.770 --> 00:05:22.610
a pie towards horrible impact which far we will remember is a variable that contains both a tensor and

78
00:05:22.610 --> 00:05:23.280
a gradient.

79
00:05:23.510 --> 00:05:29.750
And that in order to be able to differentiate with respect to a tensor to be able to differentiate with

80
00:05:29.750 --> 00:05:35.480
respect to intenser within the structure of a horrible containing tensor and a gradient.

81
00:05:35.540 --> 00:05:37.820
Again that's how Pi torch works.

82
00:05:37.820 --> 00:05:44.480
So to summarize we creating one batch for each of the state's actions and rewards and then we're going

83
00:05:44.480 --> 00:05:50.600
to put each of these vetches separately into some bytes which Horrible's which each one will get a gradient

84
00:05:50.840 --> 00:05:54.360
so that eventually we'll be able to differentiate each of them.

85
00:05:54.620 --> 00:05:57.430
All right so that's the purpose of the Zipp function.

86
00:05:57.480 --> 00:06:00.350
So let me just remove this comment.

87
00:06:00.530 --> 00:06:05.980
And now the only thing that we have to do that is to return the samples.

88
00:06:06.230 --> 00:06:12.650
So as I just explained we cannot return the samples directly for the simple reason that we want to put

89
00:06:12.830 --> 00:06:15.670
the samples into a by torche viable.

90
00:06:15.680 --> 00:06:22.580
So to do this for each of the samples we're going to use the map function and this map function will

91
00:06:22.580 --> 00:06:30.180
do the mapping from the samples to torture variables that will contain a tensor and a gradient.

92
00:06:30.200 --> 00:06:33.550
So as you can see this map function takes several arguments.

93
00:06:33.620 --> 00:06:38.600
The first argument is a function and this function is going to be the function that will convert the

94
00:06:38.600 --> 00:06:40.590
samples into some torche variables.

95
00:06:40.760 --> 00:06:45.340
And the second argument is what we want to apply this function onto.

96
00:06:45.590 --> 00:06:50.580
So that will be the arguments of this function and therefore what is it going to be.

97
00:06:50.600 --> 00:06:52.780
That's of course going to be the samples.

98
00:06:52.790 --> 00:06:55.510
So the second argument here is going to be the symbols.

99
00:06:55.870 --> 00:06:59.440
But then let's define the function on which we want to apply.

100
00:06:59.480 --> 00:07:00.620
Each of the symbols.

101
00:07:01.040 --> 00:07:06.890
So to define a function here we need to first give a name to the function which will call lambda.

102
00:07:07.070 --> 00:07:14.330
That's just a name and giving Lenda then X which is going to be the variable of this function.

103
00:07:14.420 --> 00:07:18.390
So that is just a name and giving for the variable and then.

104
00:07:18.500 --> 00:07:24.140
And here we give the expression of the function that is what we want this lambda function to return

105
00:07:25.400 --> 00:07:32.010
and see what it is going to be well it's supposed to be something that will convert our samples into

106
00:07:32.250 --> 00:07:34.800
a torch variable and to do this.

107
00:07:34.800 --> 00:07:37.370
We already mentioned it in some previous tutorials.

108
00:07:37.560 --> 00:07:39.880
Well we have the Voivode function for them.

109
00:07:40.110 --> 00:07:46.680
The Voice will function we'll make that conversion from a torch dancer to a variable that will contain

110
00:07:46.690 --> 00:07:48.590
the sensor and the greatest.

111
00:07:48.810 --> 00:07:55.830
So the first thing I'm going to add here is variable variable inside of which I'm going to convert X

112
00:07:56.010 --> 00:08:01.540
because X is going to be the simples ones that will be applied onto the samples.

113
00:08:02.420 --> 00:08:09.080
But then that's all there is one last technical thing that we need to implement is the fact that for

114
00:08:09.140 --> 00:08:15.680
each batch which is contained in the sample for example the batch of actions a 1 8 2 3 and the other

115
00:08:15.680 --> 00:08:23.080
actions we have to concatenate it with respect to the first time engine which corresponds to the States.

116
00:08:23.120 --> 00:08:25.380
And why do we have to make this concatenation.

117
00:08:25.550 --> 00:08:27.610
It's just for everything to be well aligned.

118
00:08:27.710 --> 00:08:34.740
That is that in each row to state the action and the reward corresponds to the same time t..

119
00:08:35.180 --> 00:08:42.240
So that eventually we get a list of batches all well-aligned and each batch is a pie towards Voivod.

120
00:08:42.470 --> 00:08:44.700
So how can we make this concatenation.

121
00:08:44.840 --> 00:08:48.030
Well we need to use the cat function from the torch library.

122
00:08:48.170 --> 00:08:55.400
So we're going to add here torch to which we add that gets applied to X but then in this function we

123
00:08:55.400 --> 00:09:00.820
need to specify the dimension with respect to which we want to make that concatenation.

124
00:09:00.860 --> 00:09:05.460
And as I just mentioned this is the first time engine that has index era.

125
00:09:05.900 --> 00:09:12.320
And here we go we have our function ready this Lunda that function will take the samples concatenate

126
00:09:12.320 --> 00:09:18.110
them with respect to the first time engine and then eventually we convert the sensors into some torche

127
00:09:18.110 --> 00:09:24.230
variables that contains both a tensor and a gradient so that later when we apply to castigate in the

128
00:09:24.230 --> 00:09:28.610
sense we will be able to differentiate to have data weights.

129
00:09:28.610 --> 00:09:30.170
All right so this function is ready.

130
00:09:30.410 --> 00:09:35.270
And then here is the second argument of the map function.

131
00:09:35.270 --> 00:09:42.960
We need to specify onto what we want to apply this function and that is on all our samples.

132
00:09:43.040 --> 00:09:43.850
There we go.

133
00:09:43.850 --> 00:09:50.750
We will apply this lambda function on all the samples so that eventually we obtain a list of matches

134
00:09:50.990 --> 00:09:53.840
where each match is a PI torch viable.

135
00:09:53.840 --> 00:09:58.810
All right so that was quite technical but now at least everything will work well.

136
00:09:58.850 --> 00:10:00.060
We want to use this technique.

137
00:10:00.080 --> 00:10:05.150
Afterwards we only use it here so if you don't want to have a deep understanding of the technical details

138
00:10:05.150 --> 00:10:11.060
here well that's fine you can just copy these three lines of code to simple your memory if you want

139
00:10:11.060 --> 00:10:14.460
to make an artificial intelligence with by torch it as you want.

140
00:10:14.600 --> 00:10:21.110
But now the good news is that we're done with this replaying memory class experience replay is now implemented

141
00:10:21.470 --> 00:10:26.780
and we can move on to the next and final class which will be the whole security mode.

142
00:10:26.960 --> 00:10:34.520
So in this model we will have of course our network who will experience replay and then all the rest

143
00:10:34.640 --> 00:10:36.600
of the security algorithm.

144
00:10:36.620 --> 00:10:39.150
So it's going to be a much bigger class.

145
00:10:39.200 --> 00:10:44.240
We're going to make about 10 functions but that's only because we're doing this step by step so that

146
00:10:44.240 --> 00:10:46.130
you can understand better what's going on.

147
00:10:46.520 --> 00:10:49.260
So I can't wait to implement our security model.

148
00:10:49.280 --> 00:10:50.900
And until then I.
