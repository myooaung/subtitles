1
00:00:00,420 --> 00:00:06,440
Hello and welcome to the last step of our process to make the brain we have one function to make left.

2
00:00:06,450 --> 00:00:12,180
This is the forward function that will propagate the signals in all the layers of the neural network

3
00:00:12,510 --> 00:00:16,180
including the three convolutional layers and the fully connected layer.

4
00:00:16,380 --> 00:00:21,660
And so this function is the forward function exactly like for the self-driving car except this time

5
00:00:21,780 --> 00:00:27,440
we have to propagate the signals in the convolutional layers before the fully connected layer.

6
00:00:27,660 --> 00:00:32,610
And the good news is we already did that in the previous step with the count Newnes function.

7
00:00:32,640 --> 00:00:37,160
So we already have the code to propagate the signal in the country.

8
00:00:37,340 --> 00:00:38,940
And so this will be very quick.

9
00:00:38,950 --> 00:00:43,950
We'll just combine what we did here and what we did for the submarine car and we'll get our forward

10
00:00:43,950 --> 00:00:45,720
function for our brain.

11
00:00:45,780 --> 00:00:46,860
So let's do this.

12
00:00:46,860 --> 00:00:54,660
We introduce a new function here the last one for the brain and this function is the forward function

13
00:00:55,170 --> 00:00:56,940
which takes an argument.

14
00:00:57,160 --> 00:01:05,400
Well exactly like before self to refer to the objects and X which will be first the input images and

15
00:01:05,400 --> 00:01:10,280
then you know X will be updated as the signal is propagated into the new network.

16
00:01:10,650 --> 00:01:11,460
All right.

17
00:01:11,460 --> 00:01:14,350
So Cullen and then let's go inside the function.

18
00:01:14,370 --> 00:01:20,500
So as I just said we already made the code to propagate the signals in the three convolutional layers.

19
00:01:20,580 --> 00:01:23,440
That's exactly these three lines of code.

20
00:01:23,640 --> 00:01:29,040
So I'm copying them and pasting them here and there we go.

21
00:01:29,040 --> 00:01:34,430
We already have our propagation of the signal in the three convolutional layers.

22
00:01:34,520 --> 00:01:40,830
And so now we just need to propagate the signal from the convolutional layers to the hidden layer and

23
00:01:40,830 --> 00:01:46,680
then eventually to the output layer that is at the very end of the neural network and to do this we

24
00:01:46,680 --> 00:01:53,520
first need to flatten the third convolutional layer that we obtained here remember X at first is the

25
00:01:53,520 --> 00:01:54,610
input image.

26
00:01:54,720 --> 00:02:00,780
Then here X becomes the first convolutional layer then X becomes the second convolutional there.

27
00:02:00,930 --> 00:02:03,840
And then here X becomes the third convolutional there.

28
00:02:03,960 --> 00:02:07,730
So right now at this stage X is the third convolutional layer.

29
00:02:07,860 --> 00:02:15,000
And now to take the flattening there we need to flatten this third convolutional layer X and to do this

30
00:02:15,070 --> 00:02:20,220
we are going to do something quite similar as we did here only this time we don't need the number of

31
00:02:20,220 --> 00:02:25,090
neurons we simply need to flatten the channels in the third convolutional there.

32
00:02:25,140 --> 00:02:28,130
So this will be quite more simple but very similar.

33
00:02:28,410 --> 00:02:35,570
And to do this well we're going to take X again because X is going to become the flattening layer.

34
00:02:35,890 --> 00:02:43,990
We're just updating X so x equal then we take X again but this X is the old X that is the third convolutional

35
00:02:43,990 --> 00:02:44,480
layer.

36
00:02:44,550 --> 00:02:51,510
So we take the third convolutional there then but then we take the view function to which we apply to

37
00:02:51,510 --> 00:02:52,460
arguments.

38
00:02:52,530 --> 00:02:55,730
The first one is X that size zero.

39
00:02:56,100 --> 00:03:01,800
So again we take the size function to take all the pixels of all the channels and the third convolutional

40
00:03:01,800 --> 00:03:07,320
layer and we put them one after the other in this huge vector that is going to become this X here and

41
00:03:07,320 --> 00:03:11,280
this X then will become the input of the fully connected network.

42
00:03:11,460 --> 00:03:12,860
But that's not all we need to.

43
00:03:13,080 --> 00:03:14,530
And here come up.

44
00:03:14,650 --> 00:03:15,990
And minus one.

45
00:03:16,200 --> 00:03:19,170
So that trick you can find it in the pocket watch tutorials.

46
00:03:19,350 --> 00:03:25,130
That's how you can flatten a convolutional they are composed of several channels by using the size function.

47
00:03:25,200 --> 00:03:29,890
And of course if you want more details on how this works you can go to the pocket watch tutorials.

48
00:03:30,030 --> 00:03:31,410
I will provide the link.

49
00:03:31,800 --> 00:03:38,250
So now that we got our flattening there well you know this flattening is going to become the input of

50
00:03:38,580 --> 00:03:43,800
a classic fully connected network with a simple linear transmission of the signal.

51
00:03:43,830 --> 00:03:49,440
And so now we're not going to use a convolution function to pass on the signal we're going to use a

52
00:03:49,440 --> 00:03:54,720
linear transmission with a linear class and then to break the linearity because when we're working with

53
00:03:54,780 --> 00:03:57,670
images and images have non-linear relationships.

54
00:03:57,900 --> 00:04:03,360
Well we're going to use a rectifiable function to be able to learn these non-linear relationships.

55
00:04:03,510 --> 00:04:04,560
So let's do this.

56
00:04:04,560 --> 00:04:06,160
This is actually the next step.

57
00:04:06,210 --> 00:04:09,320
And so now that's exactly like what we did for the self-driving car.

58
00:04:09,360 --> 00:04:15,030
We take X because we want to update it again we want to get the hidden layer now.

59
00:04:15,100 --> 00:04:20,970
And so first what we do is we take our full connection SE1 because the full connection.

60
00:04:20,970 --> 00:04:27,030
FC one is the one that connects the flattening layer to the hidden there and therefore we need to take

61
00:04:27,300 --> 00:04:32,850
SE1 and apply it to the X that we have right now which is the flooding there.

62
00:04:33,000 --> 00:04:38,780
And we don't forget the self of course because everyone is a variable of our init function.

63
00:04:38,890 --> 00:04:45,510
So self-taught SE1 X and so that passes on linearly the signal from the flatteringly to the hidden there.

64
00:04:45,690 --> 00:04:49,990
But now we need to activate these neurons while at the same time breaking the law in errancy.

65
00:04:50,130 --> 00:04:53,480
And that's exactly what we do with the rectifier activation function.

66
00:04:53,490 --> 00:05:00,990
So now what we have to do is take our functional module and from this functional module we take of course

67
00:05:01,140 --> 00:05:03,870
or rectifiable function that is the real you.

68
00:05:04,230 --> 00:05:08,130
And we put self-taught SE1 inside the parenthesis.

69
00:05:08,220 --> 00:05:14,280
All right so what happens in this line of code is that first we propagate the signals from the flattening

70
00:05:14,280 --> 00:05:17,210
layer to the hidden layer of the fully connected network.

71
00:05:17,580 --> 00:05:23,970
And then we activate the neurons of this hidden layer by breaking in the air with this rectifier activation

72
00:05:23,970 --> 00:05:29,100
function and we get our head in there that is here perfect.

73
00:05:29,130 --> 00:05:35,370
And now we have one last step to do it is of course to propagate the signal from the hidden layer to

74
00:05:35,670 --> 00:05:39,680
the output layer with the final output neurons and to do this.

75
00:05:39,720 --> 00:05:41,160
Well that's very simple.

76
00:05:41,160 --> 00:05:43,680
That's exactly like what we did with the in car.

77
00:05:43,680 --> 00:05:51,720
We take our second full collection of C2 and we apply it to of course the neurons of the hidden layer

78
00:05:51,990 --> 00:05:53,700
that is right now x.

79
00:05:53,740 --> 00:05:55,900
So here is the neurons here and there.

80
00:05:56,040 --> 00:06:02,840
This all x and x here becomes of course the output neurons of the output layer containing the cube values.

81
00:06:03,210 --> 00:06:10,070
And finally we simply return the output neurons that is x with the cube values.

82
00:06:10,320 --> 00:06:11,240
So perfect.

83
00:06:11,250 --> 00:06:16,950
Congratulations we just made a brain we just made the brain of our AI AI with the eyes and the rest

84
00:06:16,950 --> 00:06:17,740
of the cells.

85
00:06:17,790 --> 00:06:19,130
So congratulations.

86
00:06:19,140 --> 00:06:23,550
Now it's time to make the body that is defining how we're going to play the action.

87
00:06:23,550 --> 00:06:26,480
After all the signals are processed in the brain.

88
00:06:26,690 --> 00:06:28,370
So that's our second big step.

89
00:06:28,380 --> 00:06:30,280
Let's do this in the next tutorials.

90
00:06:30,300 --> 00:06:31,780
And until then AI.
