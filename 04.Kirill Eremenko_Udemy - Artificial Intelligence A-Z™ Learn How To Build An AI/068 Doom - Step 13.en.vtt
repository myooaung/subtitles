WEBVTT
1
00:00:00.450 --> 00:00:02.520
Hello and welcome to this tutorial.

2
00:00:02.520 --> 00:00:06.140
All right now we have our AI it is ready to be trained.

3
00:00:06.330 --> 00:00:09.660
And the first step of the training is to set our experience replay.

4
00:00:09.900 --> 00:00:11.330
So we're slowly getting there.

5
00:00:11.340 --> 00:00:16.690
The training and the good news is that we have an implemented version of experience replay.

6
00:00:16.830 --> 00:00:23.490
Besides that is adapted to eligibility trace which I remind is a technique that instead of learning

7
00:00:23.490 --> 00:00:27.160
the q values every transition learns it every 10 transitions.

8
00:00:27.210 --> 00:00:29.460
So basically that's exactly the same as before.

9
00:00:29.490 --> 00:00:35.190
But instead of having a single target a single word for each step we're going to have a cumulative target

10
00:00:35.190 --> 00:00:40.770
of ten steps and a cumulative reward of 10 steps and we will learn on the 10 steps each time.

11
00:00:40.980 --> 00:00:44.960
So we are learning on transitions 10 steps instead of one like before.

12
00:00:45.180 --> 00:00:50.440
And with this I will work wonders and that will make some wonders for the training process.

13
00:00:50.440 --> 00:00:52.570
You know the training will take much less time.

14
00:00:52.620 --> 00:00:58.850
Thanks to this technique but we have to specify inexperience replay that we are learning every 10 steps.

15
00:00:58.980 --> 00:01:04.230
So that's why this experience replay is not a classic implementation of experience replay.

16
00:01:04.320 --> 00:01:06.160
Like do one for the self-driving car.

17
00:01:06.210 --> 00:01:12.540
It is an experience replay implementation taking into account this 10 steps learning and therefore you

18
00:01:12.540 --> 00:01:19.110
will find in this experience replay file two classes one class that makes your AI progress doing ten

19
00:01:19.110 --> 00:01:23.460
steps so that it can sum the rewards are observed on these 10 steps.

20
00:01:23.460 --> 00:01:28.430
That's the first class and we need this class because we need to include these 10 steps in the replay

21
00:01:28.430 --> 00:01:33.570
memory class which is the classroom implement for experience we play and that's how we make sure that

22
00:01:33.630 --> 00:01:38.020
the memory also takes into account the fact that we're learning on 10 steps.

23
00:01:38.040 --> 00:01:42.660
So that's why you will find two classes in this implementation of experience replay but that's only

24
00:01:42.660 --> 00:01:49.070
to take into account that we're learning in 10 steps and that must be taken into account also into memory.

25
00:01:49.410 --> 00:01:51.940
So speaking of our memory let's create it.

26
00:01:51.970 --> 00:02:00.150
We're going to call our memory memory and memory is going to be an object of the replay memory cast

27
00:02:00.570 --> 00:02:04.900
and the replay memory class is a class of this experience replay right now.

28
00:02:05.070 --> 00:02:14.430
And so I'm taking first this felt experience replay conduct and that's where I take the replay memory

29
00:02:15.180 --> 00:02:15.880
class.

30
00:02:15.960 --> 00:02:16.840
Perfect.

31
00:02:16.890 --> 00:02:23.460
And now you can see we have to put two arguments the first argument is and steps which corresponds exactly

32
00:02:23.640 --> 00:02:27.540
to the number of steps on which we're going to learn the key values.

33
00:02:27.540 --> 00:02:32.090
So you know the number of steps on which we accumulate the target and we want.

34
00:02:32.180 --> 00:02:37.290
We're going to have a cumulative target and the cumulative reward and then the second argument is the

35
00:02:37.290 --> 00:02:39.840
capacity that is the size of the memory.

36
00:02:39.840 --> 00:02:42.650
So for example here we can see ten thousands.

37
00:02:42.810 --> 00:02:48.270
So if the capacity is equal to 10000 that means that then we will have a size of 10000 and therefore

38
00:02:48.270 --> 00:02:54.210
that means that we will get a memory of the 10000 steps performed by the eye.

39
00:02:54.390 --> 00:02:57.390
But again we're not going to learn every transition.

40
00:02:57.390 --> 00:03:02.690
We're going to learn every ten steps along these last 10000 steps of the memory and that's exactly this

41
00:03:02.700 --> 00:03:06.100
new feature that we introduce here compared to before.

42
00:03:06.180 --> 00:03:12.480
Before we only had this replay memory trick and here we have this replay memory trick plus this trick

43
00:03:12.570 --> 00:03:16.920
I've learned every ten steps and we're going to learn every ten steps and we're going to do it in the

44
00:03:16.920 --> 00:03:20.000
memory composed of the last 10000 steps.

45
00:03:20.190 --> 00:03:27.810
And this is experienced replay combined to ineligibility traits with 10 steps will considerably improve

46
00:03:27.810 --> 00:03:29.240
the training performance.

47
00:03:29.490 --> 00:03:31.130
So let's end with these two arguments.

48
00:03:31.200 --> 00:03:35.900
The first one is and steps and that will be equal to.

49
00:03:36.030 --> 00:03:43.180
But for now let's say and steps will specify what step is right after that it will actually be an abject

50
00:03:43.360 --> 00:03:49.130
of the other class of this experience replay file which is the end step progress class and that allows

51
00:03:49.140 --> 00:03:52.170
to make progress during ten steps.

52
00:03:52.170 --> 00:03:57.990
And remember during the 10 steps we will sound the words on the ten steps to get the cumulative rewards

53
00:03:58.050 --> 00:03:59.550
over 10 steps.

54
00:03:59.550 --> 00:04:02.250
And that is exactly eligibility test.

55
00:04:02.250 --> 00:04:09.350
So now what we have to do is create this steps here and we create it with the second class that we have

56
00:04:09.360 --> 00:04:13.040
in this experience replay file which is step progress.

57
00:04:13.160 --> 00:04:24.430
So now we're going to create steps this and this will be an object of the step progress class that we

58
00:04:24.430 --> 00:04:30.650
take again from our experience we play.

59
00:04:30.940 --> 00:04:31.720
There we go.

60
00:04:31.840 --> 00:04:37.160
So that's the anti-progress class and now we have to put three arguments as you can see we have to put

61
00:04:37.180 --> 00:04:41.010
the environment which is the environment here that we imported.

62
00:04:41.230 --> 00:04:47.680
Then the second argument is our AI and this will be of course the AI that we built right here in the

63
00:04:47.830 --> 00:04:55.550
U.S. and the last argument is step in this that's where we will specify that we want 10 steps you know

64
00:04:55.570 --> 00:04:59.120
to learn every 10 steps that is every 10 transitions.

65
00:04:59.200 --> 00:05:01.140
So let's help with these arguments.

66
00:05:01.180 --> 00:05:06.920
The first one is the environment and that's doom and all right.

67
00:05:06.940 --> 00:05:11.700
Then the second one is our AI AI and that we counted ai ai.

68
00:05:11.830 --> 00:05:12.720
That's the one here.

69
00:05:12.760 --> 00:05:17.590
So this is just the name of the argument of the step progress class and this ai ai.

70
00:05:17.590 --> 00:05:19.730
Here is our ai ai.

71
00:05:19.750 --> 00:05:27.150
The one that we built and then the last argument is and stack and that is equal to 10.

72
00:05:27.160 --> 00:05:27.910
All right.

73
00:05:27.910 --> 00:05:33.670
So right now we are just taking into account in the memory that there is a learning on 10 steps and

74
00:05:33.670 --> 00:05:37.210
this learning on 10 steps is called eligibility trace.

75
00:05:37.240 --> 00:05:39.520
So we're really working on the advanced stuff here.

76
00:05:39.640 --> 00:05:44.160
But remember that's because we're trying to be Dume that's nothing like making a piece of cake.

77
00:05:44.290 --> 00:05:47.650
So we need these advanced techniques to make it work.

78
00:05:47.660 --> 00:05:48.880
So now we're almost ready.

79
00:05:48.880 --> 00:05:55.450
Before moving on to the next step which will be actually about implementing LGBT trays the only thing

80
00:05:55.450 --> 00:06:03.730
that we have to include is the capacity of course and that is let's say 10000 men we will have a size

81
00:06:03.730 --> 00:06:11.470
of 10000 meaning that the memory will contain the last 10000 steps performed by the AI and that will

82
00:06:11.500 --> 00:06:13.810
allow us to generate some many.

83
00:06:13.930 --> 00:06:16.020
As I remember was a simple function.

84
00:06:16.180 --> 00:06:22.840
You know the memory contains 10000 transitions but to train the eye we're going to sample so many batches

85
00:06:22.840 --> 00:06:28.360
of ten transitions not one compared to before 10 transitions this time and we will sample these mini

86
00:06:28.360 --> 00:06:33.490
batches of 10 transitions into memory composed of the 10000 steps.

87
00:06:33.490 --> 00:06:38.770
All right so now I guess we're ready to move on to the next step which is about implementing eligibility

88
00:06:38.770 --> 00:06:39.490
trace.

89
00:06:39.520 --> 00:06:41.680
So we're going to have some adventure here.

90
00:06:41.680 --> 00:06:43.690
This will not be a simple implementation.

91
00:06:43.720 --> 00:06:47.830
So have a good break and when you're ready we can attack this.

92
00:06:47.920 --> 00:06:49.120
And so then enjoy.
