1
00:00:00,480 --> 00:00:06,370
Hello and welcome to the super exciting part of our creation the part where we make it smart.

2
00:00:06,460 --> 00:00:12,160
So that's exactly what happens when training the day I will train it's intelligence to reach the goal

3
00:00:12,160 --> 00:00:16,010
we wanted to accomplish that is reaching the vest without getting killed.

4
00:00:16,260 --> 00:00:22,170
And to do this we're going to basically train the neural network to output the right predictions and

5
00:00:22,170 --> 00:00:27,120
then everything is already ready because these output signals from the brain already have the right

6
00:00:27,120 --> 00:00:30,480
transmission to the body to play the final actions.

7
00:00:30,480 --> 00:00:35,080
So basically now what we're about to do is something we already did before.

8
00:00:35,160 --> 00:00:40,590
We're just going to take some random batches from the memory get our inputs from the samples get the

9
00:00:40,590 --> 00:00:46,410
outputs get the target get the predictions compute the last area between the predictions and the target

10
00:00:46,680 --> 00:00:52,110
and then perform backward propagation to categorize in the sense that the weights according to how much

11
00:00:52,110 --> 00:00:54,740
they contributed to this last error.

12
00:00:54,750 --> 00:00:55,890
So let's do all this.

13
00:00:55,890 --> 00:01:00,450
You're going to see how it's going to be so easy because we already have all the tools to implement

14
00:01:00,450 --> 00:01:07,500
this not only we have the torch tools like the optimizer and the last functions but also we have all

15
00:01:07,500 --> 00:01:13,360
the classes that we made before like our brain of course which we're going to use to get the predictions.

16
00:01:13,500 --> 00:01:20,710
Then our experience replay implementation eligibility trace and all these tools combined to the parts

17
00:01:20,730 --> 00:01:26,550
which tools will make the training super performance and therefore eventually we will get a super powerful

18
00:01:26,580 --> 00:01:27,250
AI.

19
00:01:27,510 --> 00:01:29,130
So let's make this happen.

20
00:01:29,220 --> 00:01:35,310
Let's make our smart and the first thing we're going to do now is get the last function that we're used

21
00:01:35,320 --> 00:01:39,130
to in the training when computing the error and an optimizer.

22
00:01:39,360 --> 00:01:40,740
That's the first thing we'll do.

23
00:01:40,830 --> 00:01:43,440
So let's create a variable for the last function.

24
00:01:43,470 --> 00:01:53,440
We're going to call it loss and this will be equal to the MSEE loss function from the end module.

25
00:01:53,820 --> 00:01:59,720
And that MSEE us that's the last function we'll use because basically our predictions are key values

26
00:01:59,730 --> 00:02:02,730
you know we're predicting the core values of the different actions.

27
00:02:02,850 --> 00:02:08,630
And therefore since these are real numbers well we're kind of doing some neural net work for regression

28
00:02:08,880 --> 00:02:11,880
and therefore the last function is the means CT error.

29
00:02:12,000 --> 00:02:13,440
That's the last function we use.

30
00:02:13,440 --> 00:02:15,320
In general for regression.

31
00:02:15,330 --> 00:02:19,380
All right so now that we have our last function Let's get our optimizer.

32
00:02:19,380 --> 00:02:24,060
So optimized here that's the Voivode we create Freda's optimizer.

33
00:02:24,150 --> 00:02:26,740
And we're going to take as usual.

34
00:02:26,760 --> 00:02:29,800
As for the self-driving car the atom atomizer.

35
00:02:29,930 --> 00:02:34,700
That's a very powerful optimizer that will work wonders for the training.

36
00:02:34,950 --> 00:02:39,420
So let's get this one up in that Adam.

37
00:02:39,760 --> 00:02:45,870
And remember that's exactly what the self-driving car we have to input two essential arguments.

38
00:02:45,930 --> 00:02:51,960
The first one is the one that will make the connection between the optimizer and the parameters of our

39
00:02:51,960 --> 00:02:52,850
neural network.

40
00:02:53,130 --> 00:02:55,910
That is the weights of the neurons of our brain.

41
00:02:55,910 --> 00:02:59,760
And to do this we take our brain which we called CNN.

42
00:02:59,890 --> 00:03:02,120
That's the object we created for our brain.

43
00:03:02,160 --> 00:03:10,740
And so CNN that remember parameters that we go and some parenthesis so that makes the connection between

44
00:03:10,950 --> 00:03:15,400
the optimizer and the weights of the neurons in the brain of our AI.

45
00:03:15,690 --> 00:03:21,810
And then the second argument is a learning rate and that's given by our.

46
00:03:21,870 --> 00:03:27,540
And so here we have to take a smaller raise because we don't want to converge too fast and we want to

47
00:03:27,540 --> 00:03:30,850
have some acceleration and therefore good learning ways that we can take.

48
00:03:30,850 --> 00:03:36,200
Here is a small one that is on point or one that is open 1 percent.

49
00:03:36,270 --> 00:03:39,230
I think that's the same we use for the self-driving car.

50
00:03:40,200 --> 00:03:46,250
All right so now we have a last function atomizer so we are almost ready to start the fall.

51
00:03:46,330 --> 00:03:52,210
Well actually we will start to it right now but just before we do it we're going to decide the size

52
00:03:52,540 --> 00:03:55,570
of the number of epochs we will be training the.

53
00:03:55,780 --> 00:04:03,300
And therefore in creating a new variable here which will respond to this number of abox and let's say

54
00:04:03,300 --> 00:04:05,110
it equal to 100.

55
00:04:05,350 --> 00:04:07,650
That will be way enough to train the AI.

56
00:04:07,750 --> 00:04:12,560
And even that that's the way I will manage to reach the best way before 100.

57
00:04:12,700 --> 00:04:14,080
Like 20 or 30.

58
00:04:14,320 --> 00:04:14,770
Let's see.

59
00:04:14,770 --> 00:04:16,740
But for now let's take 100.

60
00:04:16,870 --> 00:04:18,840
And if we need it we will increase it.

61
00:04:18,970 --> 00:04:20,850
But I don't think that will be necessary.

62
00:04:21,220 --> 00:04:24,910
OK so now that we have our number of airbags we can start to make the follow up.

63
00:04:24,940 --> 00:04:29,680
You know this main for loop of the training when we train over the Xbox.

64
00:04:29,860 --> 00:04:34,310
So for then it's a really viable is going to be book.

65
00:04:34,360 --> 00:04:37,840
That's what we choose for it back in.

66
00:04:37,840 --> 00:04:45,100
Now of course we're going to use the range function to say that we want to go from the first back one

67
00:04:45,490 --> 00:04:47,320
to number of books

68
00:04:49,970 --> 00:04:54,930
plus one because remember the upper bound of a range is not to.

69
00:04:54,960 --> 00:04:57,300
Therefore we want to go up to 100.

70
00:04:57,560 --> 00:05:02,270
Well we have to specify and be the plus one to go up to 100.

71
00:05:02,270 --> 00:05:06,500
All right so CULLEN And now let's get into the loop.

72
00:05:06,500 --> 00:05:11,450
All right so the first thing we're going to do is to do 200 runs of 10 steps.

73
00:05:11,510 --> 00:05:19,040
So each epoch will be 200 runs one after the other of 10 steps and to do this we have this one step

74
00:05:19,040 --> 00:05:24,020
function from our experience replay class and therefore to use this function which is actually a method

75
00:05:24,050 --> 00:05:30,620
because we will get it from our memory object which is an object from the replay memory class to generate

76
00:05:30,770 --> 00:05:33,050
these two hundred runs of 10 steps.

77
00:05:33,170 --> 00:05:41,190
Well we have to say our memory object that I remind we created right here memory is an object to replay

78
00:05:41,190 --> 00:05:46,720
memory class with steps 10 steps and a capacity of 10000.

79
00:05:46,760 --> 00:05:50,690
We created this object and from this object we take.

80
00:05:50,950 --> 00:05:59,750
Well this run step function run steps and we specify two hundred successive runs of 10 steps.

81
00:05:59,820 --> 00:06:01,590
So that will just add each epoch.

82
00:06:01,700 --> 00:06:03,790
Basically one hundred steps.

83
00:06:03,880 --> 00:06:08,000
Now now that we have these 200 steps running at each epoch.

84
00:06:08,220 --> 00:06:13,480
Well it's time to sample some batches from these runs and to sample these batches.

85
00:06:13,520 --> 00:06:20,060
We have another function from our memory which is sample batch and that will exactly generate some batches

86
00:06:20,330 --> 00:06:21,970
from these two hundred runs.

87
00:06:22,250 --> 00:06:27,800
But remember these batches are this time batches of series of transitions.

88
00:06:27,850 --> 00:06:33,200
There is a series of 10 steps as opposed to before where the batches were just some batches of single

89
00:06:33,200 --> 00:06:35,140
transitions here this time.

90
00:06:35,160 --> 00:06:41,390
There are going to be batches of ten steps 10 transitions and therefore Now it's time to get from our

91
00:06:41,390 --> 00:06:42,530
memory.

92
00:06:42,530 --> 00:06:43,740
These random batches.

93
00:06:44,030 --> 00:06:50,570
And to get them we used the simple batch function to which we have to apply the batch size for the batch

94
00:06:50,570 --> 00:06:56,810
size where we can take 32 or even 64 or even 128.

95
00:06:56,930 --> 00:07:00,920
Remember for that says this It's a common practice to use 32.

96
00:07:00,920 --> 00:07:06,450
That's what you will see in general in the neural networks architectures when doing some Bachche learning.

97
00:07:06,590 --> 00:07:08,520
But this time it's quite different.

98
00:07:08,540 --> 00:07:12,170
We were just sending some batches of ten steps.

99
00:07:12,170 --> 00:07:14,440
So it's better to take benches with larger sizes.

100
00:07:14,540 --> 00:07:17,990
So that's why we can take 64 or 128.

101
00:07:17,990 --> 00:07:26,120
So we're going to take 128 and actually this is going to be inside a Fulop because we want to take several

102
00:07:26,120 --> 00:07:31,950
batches and we're taking them in what is returned by the sample batch function.

103
00:07:32,270 --> 00:07:41,870
So this Fulop for that in memory sample batch 128 means that every 128 steps Well our memory will give

104
00:07:41,870 --> 00:07:50,810
us a batch size 128 which will contain actually the last 128 steps that were just run.

105
00:07:50,810 --> 00:07:57,130
We're just getting some batches of size 128 and the learning is going to happen on these batches.

106
00:07:57,170 --> 00:08:01,880
And besides inside these batches we will have eligibility trays running you know to learn every ten

107
00:08:01,880 --> 00:08:03,110
steps.

108
00:08:03,110 --> 00:08:07,520
All right so now inside this loop which is still happening in one epoch.

109
00:08:07,730 --> 00:08:13,910
But now this time we are in a specific batch and so now the first thing we're going to do is we're going

110
00:08:13,910 --> 00:08:17,050
to get our inputs and our targets separately.

111
00:08:17,450 --> 00:08:24,080
And that as I told you is you can do it with one of the tools we implemented which is eligibility tricks

112
00:08:24,500 --> 00:08:25,470
as you can see here.

113
00:08:25,640 --> 00:08:29,700
This illegibly trace function takes a batch as input.

114
00:08:29,760 --> 00:08:35,270
Now we have to batch and returns as output the inputs and the targets.

115
00:08:35,270 --> 00:08:40,430
So right now what we can simply do is create two new variables which are going to be the inputs and

116
00:08:40,430 --> 00:08:50,180
the targets and do these inputs cannot tell us equals exactly what returns this LGBTI trace function

117
00:08:50,490 --> 00:08:54,940
applied to batch to apply this function to the batch of our loop.

118
00:08:55,150 --> 00:09:03,560
And so what we'll do is just eligibility trace applied to the batch of our up.

119
00:09:03,560 --> 00:09:04,170
All right.

120
00:09:04,310 --> 00:09:10,430
So that gets us the input and the toilets but in PI torch there is always something more we have to

121
00:09:10,430 --> 00:09:10,900
do.

122
00:09:10,970 --> 00:09:16,990
And of course this is to convert the inputs of the neural network and also the target into some of variables.

123
00:09:17,180 --> 00:09:19,030
But nor is there is nothing new.

124
00:09:19,040 --> 00:09:20,300
We know how to do it.

125
00:09:20,390 --> 00:09:24,860
We can do it this way we take our inputs then our targets.

126
00:09:25,250 --> 00:09:35,610
And while they will be equal to variable inputs that's for the inputs and variable targets and that's

127
00:09:35,610 --> 00:09:36,970
where the targets.

128
00:09:36,970 --> 00:09:45,010
All right so the inputs of the brain are convert into some torche variables and the targets also are

129
00:09:45,010 --> 00:09:47,160
converted into some variables.

130
00:09:47,170 --> 00:09:51,300
So now we can get the inputs into the neural network.

131
00:09:51,520 --> 00:09:52,940
And why do we need to do this.

132
00:09:53,080 --> 00:09:58,260
That's because the next step is to get the predictions we have in which we have to target.

133
00:09:58,420 --> 00:10:03,460
Now of course we need our predictions because then what happens is that we will compute the loss between

134
00:10:03,460 --> 00:10:05,790
the predictions and the targets.

135
00:10:05,800 --> 00:10:09,340
Let's get these predictions to get then.

136
00:10:09,520 --> 00:10:15,560
Well again this is so simple now we just need to take our brain which is CNN our constitutional news

137
00:10:15,560 --> 00:10:19,870
network and apply it to our inputs.

138
00:10:20,170 --> 00:10:20,980
There we go.

139
00:10:21,160 --> 00:10:26,250
The inputs go into the neural network and the neural network will output the predictions.

140
00:10:27,020 --> 00:10:27,700
Perfect.

141
00:10:27,740 --> 00:10:32,960
So now we have the protections we have to target so we can get the laws and that's the next step.

142
00:10:33,200 --> 00:10:37,970
We're going to introduce a new variable because right now we're going to get the last error which is

143
00:10:37,970 --> 00:10:42,690
different than the last function because we use the laws function to get the last error.

144
00:10:42,940 --> 00:10:53,330
So last error here and that we will get it with the last function applied to our predictions and the

145
00:10:53,330 --> 00:10:57,290
targets that we go see how everything is smooth.

146
00:10:57,290 --> 00:10:58,940
Now everything is logical.

147
00:10:58,940 --> 00:11:04,250
We get input first the targets then thanks to the inputs we get the predictions and then thanks to the

148
00:11:04,250 --> 00:11:06,840
predictions and the toilets we get the last error.

149
00:11:07,920 --> 00:11:10,000
So very logical and smooth.

150
00:11:10,180 --> 00:11:11,840
And now what is the next step.

151
00:11:12,030 --> 00:11:17,790
Well same logical path now that we have the last we can back propagate this last error back into the

152
00:11:17,790 --> 00:11:23,710
neural network to do weights and we do that with the gas grill in the center and to perform to get degrees

153
00:11:23,840 --> 00:11:25,800
in the sense we need our optimizer.

154
00:11:25,920 --> 00:11:27,470
But we already got it here.

155
00:11:27,570 --> 00:11:29,040
Our Adam atomizer.

156
00:11:29,280 --> 00:11:31,980
But now at this point remember what we had to do.

157
00:11:32,220 --> 00:11:35,160
We had to initialize it and to initialize it.

158
00:11:35,160 --> 00:11:37,990
Remember we take our optimizer object.

159
00:11:38,220 --> 00:11:43,600
And then we apply the zero grette method.

160
00:11:44,290 --> 00:11:48,050
There we go we don't forget the parenthesis that's initialized it.

161
00:11:48,360 --> 00:11:54,620
And now next step is to back propagate the last error back into the new network and to do this well

162
00:11:54,620 --> 00:12:01,770
we take our last error and we apply on it the backward method.

163
00:12:01,830 --> 00:12:04,820
So that's exactly to ply backward propagation.

164
00:12:05,000 --> 00:12:10,300
And then finally now that the last error is back propagated into the new one that work well we can update

165
00:12:10,310 --> 00:12:10,920
the weights.

166
00:12:10,920 --> 00:12:18,170
We're still getting great in the sense and to do this remember we take our optimizer and then we apply

167
00:12:18,500 --> 00:12:20,310
the step method.

168
00:12:20,330 --> 00:12:21,240
There we go.

169
00:12:21,260 --> 00:12:22,950
The weights are now dated.

170
00:12:23,090 --> 00:12:28,970
As I told you not only we already did it but now it seems so simple and so natural.

171
00:12:29,030 --> 00:12:31,450
So now we're going to do something fun.

172
00:12:31,470 --> 00:12:34,530
We are going to print the average we work every epoch.

173
00:12:34,700 --> 00:12:38,980
So you know we can keep track of how the AI is going how the training is going.

174
00:12:39,050 --> 00:12:45,180
We want to see the average reward increasing over the steps of the epochs and at first of course there

175
00:12:45,180 --> 00:12:51,710
is this exploitation phase so the average would not increase at the beginning but then once it reaches

176
00:12:51,710 --> 00:12:57,600
the exploitation phase then we'll see the average reward definitely increase and it will increase up

177
00:12:57,740 --> 00:13:02,230
to a certain level which is when it reaches the vest as fast as possible.

178
00:13:02,240 --> 00:13:04,770
So let's start with the premise.

179
00:13:05,050 --> 00:13:10,400
You know we are doing this in one way but we have to go back to the loop here print and then we're going

180
00:13:10,400 --> 00:13:11,170
to print.

181
00:13:11,300 --> 00:13:19,280
Well first but then percent s because we're going to convert everything to a string that's better.

182
00:13:19,670 --> 00:13:28,950
And then we're going to add the average we were and then we add percent S as well then we're going to

183
00:13:28,950 --> 00:13:33,580
close the quote and then we add a percent.

184
00:13:33,690 --> 00:13:39,600
And on the other side you know we can put the variables that are going to be this first person as that

185
00:13:39,600 --> 00:13:41,090
is the epoch here.

186
00:13:41,310 --> 00:13:45,700
And the second variable correspond to the average word which will compute right now.

187
00:13:45,870 --> 00:13:48,660
So the average word viable doesn't exist yet.

188
00:13:48,660 --> 00:13:50,690
We're going to create it right now.

189
00:13:50,970 --> 00:13:58,830
So we are going to use Esti our epoch even if is the number to convert that into his train gets better

190
00:13:59,430 --> 00:14:04,870
and we are going to add s t r that's going to be the average reward.

191
00:14:05,040 --> 00:14:09,690
And so we're going to create a variable that we're going to call a Viji reward.

192
00:14:10,230 --> 00:14:13,000
And now we're going to create this variable and compute it.

193
00:14:13,440 --> 00:14:15,210
OK so let's do this.

194
00:14:15,210 --> 00:14:19,040
That's the only thing we had to do left so EPOC we already have.

195
00:14:19,050 --> 00:14:22,950
Now let's compute average word and we need to compute that right here.

196
00:14:22,950 --> 00:14:29,850
Still in the loop but our the bachelor because now we have our patch samples and we have a training

197
00:14:29,880 --> 00:14:31,060
happening in the batch.

198
00:14:31,130 --> 00:14:34,900
But now that for propagation Plus the backward propagation is done in the batch.

199
00:14:35,040 --> 00:14:41,670
So we are getting back into the epoche loop and we can now compute the cumulative words which we can

200
00:14:41,670 --> 00:14:49,330
do with our steps object because our insteps object contains this function we what steps that allows

201
00:14:49,330 --> 00:14:54,490
us to get the cumulative rewards happening in the steps you know and steps run.

202
00:14:54,520 --> 00:15:02,830
So we are going to use it right now to update the new rewards of the steps and then we will update the

203
00:15:03,310 --> 00:15:08,970
moving average object by adding the cumulative words to the moving every object.

204
00:15:09,160 --> 00:15:14,260
And then we computing the average and that's how we're going to get the average of what we think is

205
00:15:14,350 --> 00:15:17,700
the first thing we need is the word Stribild.

206
00:15:17,830 --> 00:15:29,170
So let's call them the words steps and then as we said we take our end steps object which was I remind

207
00:15:29,270 --> 00:15:36,600
created here and object of the step progress class from our experience replay for instance objects.

208
00:15:36,860 --> 00:15:42,570
Then we add words steps and then some parenthesis.

209
00:15:42,610 --> 00:15:42,920
All right.

210
00:15:42,940 --> 00:15:46,880
That will get us the new cumulative rewards of the steps.

211
00:15:46,880 --> 00:15:54,320
All right but then we need to add these new cumulative rewards in our moving average object and to do

212
00:15:54,320 --> 00:16:00,970
this we have a method this time in the moving average class which is this method that's very simple

213
00:16:00,980 --> 00:16:08,480
we take our moving average object which we created here with 100 steps then we're going to use our add

214
00:16:08,480 --> 00:16:09,280
method.

215
00:16:09,470 --> 00:16:17,510
And then in the Admetus we put our words steps and this will add the rewards of the steps into the moving

216
00:16:17,510 --> 00:16:18,830
average.

217
00:16:18,860 --> 00:16:24,440
All right and finally we can compute the average word and that is well you know that's the variable

218
00:16:24,440 --> 00:16:25,150
here.

219
00:16:25,550 --> 00:16:33,290
So that's what is going to be equal to the average word and to get it we just need to use the average

220
00:16:33,380 --> 00:16:42,050
method this time from our moving average object and that is we do and say that average to say that because

221
00:16:42,320 --> 00:16:47,900
our moving average was already updated with the new word steps that we just added things to the Add

222
00:16:47,900 --> 00:16:49,810
method great.

223
00:16:49,820 --> 00:16:52,940
Now we have our average word so that will populate here.

224
00:16:53,120 --> 00:16:55,710
And this is going to be printed every hour.

225
00:16:55,730 --> 00:17:00,420
So this is going to be very exciting because each episode we're going to keep track of how the air is

226
00:17:00,420 --> 00:17:01,070
going.

227
00:17:01,190 --> 00:17:04,220
You know we're going to see if the average word is increasing.

228
00:17:04,220 --> 00:17:05,930
And speaking of something exciting.

229
00:17:06,080 --> 00:17:13,390
What's very exciting is that I know that it reaches the vest for sure when it reaches 1500.

230
00:17:13,550 --> 00:17:24,580
So what we're going to do now is that if the average reward reaches 1500.

231
00:17:24,730 --> 00:17:30,320
Now if it's getting higher than 1500 Well we're going to print something new and that's going to be

232
00:17:30,320 --> 00:17:31,390
the final print.

233
00:17:31,490 --> 00:17:37,110
The very successful print we're going to print that we can say.

234
00:17:37,130 --> 00:17:44,350
Congratulations to your AI because your AI wins.

235
00:17:44,510 --> 00:17:48,690
So that would be awesome if that happens if we reached non-average word.

236
00:17:48,770 --> 00:17:50,730
Larger than 1500.

237
00:17:50,780 --> 00:17:54,390
We can be 100 percent sure that the AI which is the best.

238
00:17:54,470 --> 00:18:00,900
Maybe it's a little lower than that but I experimented with 1000 and sometimes didn't reach advanced

239
00:18:01,160 --> 00:18:06,010
but with 1500 if we get there well the AI wins for sure.

240
00:18:06,230 --> 00:18:08,820
So we're going to watch that afterwards.

241
00:18:09,260 --> 00:18:11,380
All right so that would be very exciting.

242
00:18:11,390 --> 00:18:14,060
And then that's actually only the end of the curve.

243
00:18:14,060 --> 00:18:17,750
We need to specify that we want to start the training.

244
00:18:17,810 --> 00:18:22,200
If that happens you know if it reaches the vast wealth it no longer needs to train.

245
00:18:22,250 --> 00:18:29,550
So we're going to end all the process with a break if we reach an average world of 1500.

246
00:18:29,720 --> 00:18:33,840
We can finish the whole process and to do this we can use this break here.

247
00:18:33,860 --> 00:18:34,840
All right.

248
00:18:34,850 --> 00:18:39,350
And then one last thing to do when the game finishes that is when the AI wins.

249
00:18:39,440 --> 00:18:45,080
Well we can close environment and so on adding just one last action here which I'm going to call closing

250
00:18:45,830 --> 00:18:48,640
the doom environment.

251
00:18:50,200 --> 00:18:56,060
So to do this we take our Dume environment which we called Doom and remember.

252
00:18:56,290 --> 00:19:02,050
And then we can just add that close and that's all that we'll just close environment which is better

253
00:19:02,050 --> 00:19:03,980
to do once you've finished the game.

254
00:19:04,000 --> 00:19:05,460
All right so we're done.

255
00:19:05,530 --> 00:19:10,900
So I'm so excited to see the results and actually I'm going to have a surprise for you in the next tutorial

256
00:19:11,020 --> 00:19:12,540
while watching the results.

257
00:19:12,660 --> 00:19:14,060
It's going to be pretty exciting.

258
00:19:14,230 --> 00:19:17,380
And so now I guess it's time to play with it and have fun.

259
00:19:17,530 --> 00:19:20,710
So let's hope we can reach this vest as fast as possible.

260
00:19:20,710 --> 00:19:23,650
That's the goal of this new environment.

261
00:19:23,650 --> 00:19:25,990
All right so let's have fun.

262
00:19:25,990 --> 00:19:28,850
Prepare yourself a good coffee or a good tea.

263
00:19:28,900 --> 00:19:34,340
Now stand to sit comfortably in our chair and watch some very cool videos of our planes.

264
00:19:34,510 --> 00:19:36,180
Let's do that in the next tutorial.

265
00:19:36,220 --> 00:19:37,810
And until then enjoy AI.
