1
00:00:01,140 --> 00:00:01,480
Hi,

2
00:00:01,480 --> 00:00:08,160
this is Craig Golightly, and welcome to Migrating and Deploying Databases.

3
00:00:08,160 --> 00:00:13,520
If you're looking to migrate a database to the cloud,

4
00:00:13,520 --> 00:00:17,440
there are several tools available to make your job easier.

5
00:00:17,440 --> 00:00:20,560
We're going to cover some AWS‑provided options,

6
00:00:20,560 --> 00:00:25,440
as well as how you can make use of native database migration tools.

7
00:00:25,440 --> 00:00:28,260
Moving large amounts of data to the cloud can also

8
00:00:28,260 --> 00:00:30,540
present logistical challenges,

9
00:00:30,540 --> 00:00:34,240
especially when you're dealing with petabytes of data.

10
00:00:34,240 --> 00:00:38,820
We'll discuss some solutions AWS has created to help you quickly and

11
00:00:38,820 --> 00:00:43,840
securely transfer your data to the cloud. We'll also cover some planning

12
00:00:43,840 --> 00:00:49,000
aspects to consider when performing a database migration. Building and

13
00:00:49,000 --> 00:00:53,330
maintaining test environments for your databases is another task that can

14
00:00:53,330 --> 00:00:55,740
present some logistical challenges.

15
00:00:55,740 --> 00:00:58,960
We'll cover some strategies to automate your deployments,

16
00:00:58,960 --> 00:01:03,450
as well as requirements that can put constraints on your test environment

17
00:01:03,450 --> 00:01:08,600
design. When migrating from a source to a target database,

18
00:01:08,600 --> 00:01:13,310
one basic approach could be make a backup of your source database,

19
00:01:13,310 --> 00:01:16,740
restore that backup to your target database,

20
00:01:16,740 --> 00:01:21,980
use replication to catch up on what happened on the source in between

21
00:01:21,980 --> 00:01:26,040
taking the backup and loading the backup to the target,

22
00:01:26,040 --> 00:01:30,840
then switch traffic over from the source to the target database.

23
00:01:30,840 --> 00:01:32,770
While that sounds easy enough,

24
00:01:32,770 --> 00:01:38,340
there can be significant challenges when trying to execute this in real life.

25
00:01:38,340 --> 00:01:43,620
For example, taking a full backup can be taxing on your source database,

26
00:01:43,620 --> 00:01:46,590
so you'll need to make sure that normal operations can

27
00:01:46,590 --> 00:01:50,020
proceed at an acceptable level of performance.

28
00:01:50,020 --> 00:01:53,480
Do you have enough disk space to store a full backup?

29
00:01:53,480 --> 00:01:56,130
How long will it take to transfer the backup from

30
00:01:56,130 --> 00:01:58,740
the source to the target database?

31
00:01:58,740 --> 00:02:01,810
Will your migration require any downtime?

32
00:02:01,810 --> 00:02:07,540
If so, can you fit that downtime into a planned maintenance window?

33
00:02:07,540 --> 00:02:12,840
AWS provide some tools to help make migrating to the cloud easier.

34
00:02:12,840 --> 00:02:18,020
Over 300,000 databases have been migrated using AWS Database

35
00:02:18,020 --> 00:02:22,900
Migration Service. With AWS DMS, migration starts while the

36
00:02:22,900 --> 00:02:25,740
original database stays live.

37
00:02:25,740 --> 00:02:28,870
It takes care of replicating any changes that occur in the

38
00:02:28,870 --> 00:02:32,600
source database during the migration process, so you don't

39
00:02:32,600 --> 00:02:34,840
have to worry about missing anything,

40
00:02:34,840 --> 00:02:38,140
and you can migrate with virtually no downtime.

41
00:02:38,140 --> 00:02:41,110
It's also highly resilient and self‑healing,

42
00:02:41,110 --> 00:02:42,980
So if there is an interruption,

43
00:02:42,980 --> 00:02:47,690
it automatically restarts the process and continues migration from where it

44
00:02:47,690 --> 00:02:52,650
stopped. You can view metrics to track the progress of your migration, and

45
00:02:52,650 --> 00:02:58,940
you can also use AWS DMS for replication tasks across regions or to help

46
00:02:58,940 --> 00:03:02,940
build databases in test environments.

47
00:03:02,940 --> 00:03:08,740
AWS DMS supports most commercial and open‑source databases.

48
00:03:08,740 --> 00:03:13,850
You can perform homogeneous migrations such as Oracle to Oracle,

49
00:03:13,850 --> 00:03:19,200
as well as heterogeneous migrations between different database platforms

50
00:03:19,200 --> 00:03:24,890
such as Oracle to Amazon Aurora. For heterogeneous migrations, you can

51
00:03:24,890 --> 00:03:28,900
first make use of the AWS Schema Conversion Tool,

52
00:03:28,900 --> 00:03:32,440
which will determine the complexity of the conversion and

53
00:03:32,440 --> 00:03:35,220
perform much of the conversion automatically,

54
00:03:35,220 --> 00:03:38,740
including psSQL and T‑SQL code.

55
00:03:38,740 --> 00:03:41,420
If there are any code fragments that cannot be

56
00:03:41,420 --> 00:03:44,240
automatically converted to the target language,

57
00:03:44,240 --> 00:03:48,110
it'll flag those for review. For homogeneous migrations,

58
00:03:48,110 --> 00:03:52,380
you can also make use of native schema export tools

59
00:03:52,380 --> 00:03:55,540
available for your database engine.

60
00:03:55,540 --> 00:04:00,660
You can use a VPN or set up a Direct Connect connection between your

61
00:04:00,660 --> 00:04:05,140
data center and AWS to securely transfer your data.

62
00:04:05,140 --> 00:04:09,840
However, if your database has tens or hundreds of petabytes of data,

63
00:04:09,840 --> 00:04:14,710
transferring over traditional data connections just isn't practical.

64
00:04:14,710 --> 00:04:15,750
For example,

65
00:04:15,750 --> 00:04:20,630
it could take more than 20 years to transfer 100 petabytes of data over a

66
00:04:20,630 --> 00:04:25,240
direct connect line with a 1 gigabit‑per‑second connection.

67
00:04:25,240 --> 00:04:26,390
For these cases,

68
00:04:26,390 --> 00:04:30,530
you may want to consider the Amazon Snow Family to securely

69
00:04:30,530 --> 00:04:35,740
migrate data into or out of the AWS cloud.

70
00:04:35,740 --> 00:04:41,100
AWS Snowmobile is a 45‑foot long ruggedized shipping container

71
00:04:41,100 --> 00:04:44,690
that can transfer up to 100 petabytes of data.

72
00:04:44,690 --> 00:04:49,000
It's delivered to your data center where you can then load your data, then

73
00:04:49,000 --> 00:04:54,410
it's driven back to AWS where your data is imported into S3.

74
00:04:54,410 --> 00:04:59,090
Snowmobile uses multiple layers of security and encryption for secure

75
00:04:59,090 --> 00:05:02,740
transfer and full chain of custody of your data.

76
00:05:02,740 --> 00:05:05,510
If you don't have quite that much data to transfer,

77
00:05:05,510 --> 00:05:07,690
but still need a physical device,

78
00:05:07,690 --> 00:05:12,760
you can use AWS Snowball. Snowball is about the size of a desktop

79
00:05:12,760 --> 00:05:17,040
computer case and can transfer up to 80 terabytes of data.

80
00:05:17,040 --> 00:05:21,890
You can also chain multiple snowball devices together for larger transfers.

81
00:05:21,890 --> 00:05:26,210
Simply load it with data and contact the designated shipping carrier in

82
00:05:26,210 --> 00:05:31,040
your country who will pick it up and take it to AWS.

83
00:05:31,040 --> 00:05:36,330
Once your data is transferred to AWS, if you need to perform any extract,

84
00:05:36,330 --> 00:05:42,400
transform, and load or ETL activities, you can use AWS Glue.

85
00:05:42,400 --> 00:05:45,840
Glue is a serverless data preparation service.

86
00:05:45,840 --> 00:05:51,450
You can use it to catalog multiple AWS datasets without moving the data.

87
00:05:51,450 --> 00:05:54,450
This allows you to search and query the data using

88
00:05:54,450 --> 00:05:57,640
multiple AWS database services.

89
00:05:57,640 --> 00:06:08,000
It can also run ETL jobs as new data arrives and provides both visual and code‑based interfaces to make data preparation easy.

