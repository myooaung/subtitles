WEBVTT
1
00:00:00.360 --> 00:00:04.170
Hello and welcome to the section on convolutional neural networks.

2
00:00:04.170 --> 00:00:07.600
Super excited that you're joining us for this section as well.

3
00:00:07.740 --> 00:00:13.650
And today we're going to color off the plan of attack how are we going to learn everything in this section.

4
00:00:13.650 --> 00:00:15.630
There's so much to learn.

5
00:00:15.660 --> 00:00:17.490
Let's see how we're going to approach this.

6
00:00:17.490 --> 00:00:19.590
All right what we learn in the section.

7
00:00:19.590 --> 00:00:25.140
First of all we'll talk about what convolutional networks actually are very important to understand

8
00:00:25.140 --> 00:00:28.770
the end goal that you're working towards before you actually start working towards it.

9
00:00:28.770 --> 00:00:32.090
So we'll hear what features will have a look at a few little examples.

10
00:00:32.090 --> 00:00:36.750
Will compare the human brain to artificial neural networks in terms of image recognition.

11
00:00:36.840 --> 00:00:42.630
So it'll be a fun light tutorial to get us started for this whole section.

12
00:00:42.630 --> 00:00:44.810
Then we'll talk about Step 1.

13
00:00:44.850 --> 00:00:47.410
Diving straight into it can volution operation.

14
00:00:47.420 --> 00:00:56.280
So this part of the Course contains several steps that we need to go through in order to build a convolutional

15
00:00:56.310 --> 00:00:59.040
neural network and that's how these internals are going to be broken up.

16
00:00:59.040 --> 00:01:06.510
So this was going to be step one the evolution operation we'll learn everything about feature detectors.

17
00:01:06.510 --> 00:01:08.790
We'll talk about which are also filters.

18
00:01:08.790 --> 00:01:11.030
We'll talk about future maps.

19
00:01:11.070 --> 00:01:16.110
And you know how what are the different parameters they're what they mean and have a look at some visual

20
00:01:16.110 --> 00:01:17.360
examples as well.

21
00:01:17.520 --> 00:01:26.010
Then we'll talk about Step 1 Part B the realm you Lehre or really layer which is then rectified leaner

22
00:01:26.130 --> 00:01:34.830
unit and we'll talk about why linearity is not good and how we want more nonlinearity in our network

23
00:01:34.830 --> 00:01:36.700
for image recognition.

24
00:01:36.870 --> 00:01:42.420
Then we'll talk about Step 2 pooling and will understand how pooling works we'll talk specifically about

25
00:01:42.420 --> 00:01:48.960
Max pooling and we also mention a couple of things about I mean pooling or some pooling and other approaches

26
00:01:48.960 --> 00:01:51.960
that you can take to the process of pooling.

27
00:01:51.960 --> 00:01:58.650
Also in this lecture we'll have a really cool example so there will be a very visual interactive tool

28
00:01:58.650 --> 00:01:59.730
that we're going to look at.

29
00:01:59.730 --> 00:02:05.190
So make sure to stick around to the end of that lecture because that's going to add a lot of value to

30
00:02:05.190 --> 00:02:06.810
your learning process.

31
00:02:06.810 --> 00:02:09.110
What we're going to discuss at the end there.

32
00:02:09.120 --> 00:02:10.650
Step three flattening.

33
00:02:10.680 --> 00:02:16.290
So here we will it's going to be a quick tutorial on how to proceed from your pooled layers to your

34
00:02:16.380 --> 00:02:19.560
flatten lair and then we're going to talk about full connections.

35
00:02:19.560 --> 00:02:26.130
This is a very meaty tutorial that puts everything together and puts everything into perspective and

36
00:02:26.130 --> 00:02:29.680
actually shows you how everything works.

37
00:02:29.700 --> 00:02:35.100
At the end of the day and how those final neurons understand how to classify Umich very very important.

38
00:02:35.100 --> 00:02:42.340
Tauriel and hopefully that will summarize or kind of put everything together for you.

39
00:02:42.540 --> 00:02:46.260
And finally we'll have a summary which will summarize everything we've talked about.

40
00:02:46.560 --> 00:02:52.260
And as an extra little feature I've included a tutorial on soft Max and cross entropy.

41
00:02:52.260 --> 00:02:57.780
So you don't have to take this tutorial but I thought it would be a great addition of knowledge because

42
00:02:57.780 --> 00:03:02.010
these are terms that you will come across when dealing with convolutional neural networks.

43
00:03:02.070 --> 00:03:08.130
So maybe maybe take it right away maybe when you come across these terms you can you will always know

44
00:03:08.130 --> 00:03:13.800
you can come back to this course and take this tutorial to understand better what soft Max and cross

45
00:03:13.800 --> 00:03:14.530
entropy are.

46
00:03:14.760 --> 00:03:20.610
And also as always throughout these editorials there will be lots of recommended reading for you to

47
00:03:20.670 --> 00:03:23.100
further upscale and get more knowledge.

48
00:03:23.370 --> 00:03:28.250
And on that note I can't wait to see on the first tutorial this is going to be very fun and exciting.

49
00:03:28.310 --> 00:03:31.260
S. And until next time enjoy deep learning.
