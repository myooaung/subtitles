WEBVTT
1
00:00:05.290 --> 00:00:10.530
Welcome everyone to this logistic regression code along lecture we're going to start off by working

2
00:00:10.530 --> 00:00:15.720
through a classic classification example and that is the Titanic dataset which is a really common exercise

3
00:00:15.990 --> 00:00:18.770
when you're starting off learning about machine learning classification.

4
00:00:18.920 --> 00:00:24.330
So you'll find lots of examples of it online so you can compare the approach we take here to other approaches

5
00:00:25.880 --> 00:00:30.890
and we're going to be using this dataset in an attempt to predict what passengers survived the Titanic

6
00:00:30.890 --> 00:00:31.520
crash.

7
00:00:31.660 --> 00:00:34.490
Bates solely on those passengers features.

8
00:00:34.520 --> 00:00:38.810
So for example have a bunch of features such as a passenger's age their cabin.

9
00:00:38.810 --> 00:00:42.240
How many children they had whether they were male or female etcetera.

10
00:00:42.320 --> 00:00:46.520
And just using those features we're going to be able to predict whether or not they survived the Titanic

11
00:00:46.520 --> 00:00:52.310
crash and eventually you'll see that people who were male or were in the lower class such as the third

12
00:00:52.310 --> 00:00:54.170
class tended to not survive.

13
00:00:54.320 --> 00:00:59.870
And people that were in higher classes like first class and there were female to have a higher likelihood

14
00:00:59.870 --> 00:01:00.730
of survival.

15
00:01:00.950 --> 00:01:06.050
So we're also going to explore a few more topics we'll see some better ways to deal with categorical

16
00:01:06.050 --> 00:01:10.790
data through a two step process and we're also going to show you how to use pipelines to set stages

17
00:01:11.030 --> 00:01:12.930
and build models that can be easily used.

18
00:01:12.940 --> 00:01:16.690
Again our data is also going to have lots of missing information.

19
00:01:16.690 --> 00:01:20.980
So we're going to need to deal with that as well and learn how to deal with missing information.

20
00:01:20.980 --> 00:01:22.030
Let's get started.

21
00:01:22.060 --> 00:01:27.700
I'm actually going to be using the data Brick's notebook setup but of course you can use your installation

22
00:01:27.790 --> 00:01:29.410
of whatever method you prefer.

23
00:01:29.590 --> 00:01:35.740
The data file for this is called Titanic thought CXXVI and it's located in the same folder as the Titanic

24
00:01:35.740 --> 00:01:37.590
logger regression code along notebook.

25
00:01:37.750 --> 00:01:39.770
Going to open up my browser and get started.

26
00:01:40.130 --> 00:01:43.290
OK here I am I'm actually working with the data Brick's format.

27
00:01:43.300 --> 00:01:48.560
Again you can work with this in your virtual box environment on Amazon AWOS whatever you prefer.

28
00:01:48.640 --> 00:01:54.490
Since I'm working on data bricks I upload it as a table and then I've selected it from the Tenex underscore

29
00:01:54.490 --> 00:01:55.130
CXVII.

30
00:01:55.180 --> 00:01:57.630
Of course you could do a read that see every command.

31
00:01:57.760 --> 00:02:01.600
Just make sure you say infer scheme is true as well as being true.

32
00:02:01.600 --> 00:02:09.460
And if I check up my data frame or my data ongoing going to print the schema for this run that cell

33
00:02:09.880 --> 00:02:15.760
and then eventually see my schema here I have the passenger ID survives the passenger class name of

34
00:02:15.760 --> 00:02:21.940
the passenger sex that is gender age siblings or spouses they had onboard parents or children they had

35
00:02:21.940 --> 00:02:23.840
on board tickets.

36
00:02:23.900 --> 00:02:27.720
So how much they actually paid for their ticket what cabin they were in and embarked.

37
00:02:27.730 --> 00:02:33.700
Which is a string indicating whether they embarked in a certain city and that actual string is just

38
00:02:33.700 --> 00:02:35.170
a single letter.

39
00:02:35.390 --> 00:02:38.590
We're going to do for now is select only the data that we need.

40
00:02:38.590 --> 00:02:42.260
So passenger ID that's just basically an index position.

41
00:02:42.460 --> 00:02:43.840
So it's not really useful to us.

42
00:02:43.930 --> 00:02:46.760
So we're going to only select columns that are useful to us.

43
00:02:46.840 --> 00:02:55.060
So we'll say my calls for my columns is DMF and I'm going to select only select a list of columns here

44
00:02:55.290 --> 00:03:02.460
but in order to kind of speed up this process let me actually say DMF and call the columns themselves.

45
00:03:03.530 --> 00:03:08.540
And then over here Will's kind of copy the same come in and this will save me some typing.

46
00:03:08.570 --> 00:03:14.720
So the ones I want are passenger or survive excuse me which is going to be our label 0 or 1 whether

47
00:03:14.720 --> 00:03:16.980
or not they survive the class.

48
00:03:17.090 --> 00:03:19.760
So we're going to use those as well.

49
00:03:19.880 --> 00:03:25.130
I'm not going to use their actual name because whether or not someone's named Harry or Sally is going

50
00:03:25.130 --> 00:03:29.510
to be hard to indicate whether or not they survived later on you could kind of use feature engineering

51
00:03:29.510 --> 00:03:34.520
to grab things like whether or not they were a doctor or Mr. or Mrs. Right now will just kind of ignore

52
00:03:34.520 --> 00:03:35.040
that state.

53
00:03:35.050 --> 00:03:38.660
The basics they were going to grab the sex and age call them as well.

54
00:03:38.870 --> 00:03:41.560
Along with the sibling and spouse parent children.

55
00:03:41.920 --> 00:03:49.190
So let's grab those adding the men and then we're also going to grab the Fair column which is how much

56
00:03:49.190 --> 00:03:51.260
they paid for their ticket and then embarked.

57
00:03:51.260 --> 00:03:58.140
Which is a string indicating what city embarked on or embarked on to the ship.

58
00:03:58.160 --> 00:04:03.860
So now you have this list of columns and what I'm going to do now is first dealing with the missing

59
00:04:03.860 --> 00:04:04.460
data.

60
00:04:04.730 --> 00:04:09.540
So the way we're going to deal of missing data right now is just kind of get rid of it and drop it.

61
00:04:09.560 --> 00:04:12.330
That's the simplest way to deal with missing data.

62
00:04:12.350 --> 00:04:14.840
It's a kind of extreme way of dealing with missing data.

63
00:04:15.020 --> 00:04:20.030
But the other alternative is to fill in the data to keep things simple for now.

64
00:04:20.060 --> 00:04:28.570
We'll just say my final data is equal to elysÃ©e my calls.

65
00:04:28.590 --> 00:04:34.530
So those are the columns I wanted because I said DFI select those columns and then I'll say a drop and

66
00:04:34.530 --> 00:04:37.410
that's going to drop the missing data from it.

67
00:04:37.410 --> 00:04:42.290
So now I have my final data and we're going to do now is work with categorical columns.

68
00:04:42.300 --> 00:04:46.040
So we're going to break this down into multiple steps to make it all clear.

69
00:04:46.320 --> 00:04:50.040
And we're going to be copying and pasting code from the notebook to make sure that we don't have any

70
00:04:50.040 --> 00:04:51.090
typos here.

71
00:04:51.090 --> 00:04:55.620
So you can go ahead and copy and paste this code or just kind of type along with it but would recommend

72
00:04:55.620 --> 00:04:57.750
you copy and paste so you don't have any typos.

73
00:04:57.750 --> 00:04:59.010
So let's explain what we're doing here.

74
00:04:59.010 --> 00:05:05.360
The first thing to do is just do a couple of imports from the Paice bark machine learning feature section.

75
00:05:05.580 --> 00:05:11.040
And we're going to import a vector assembler a vector indexer a one hot encoder and a string that that's

76
00:05:11.070 --> 00:05:13.780
her son going to run that and import those.

77
00:05:13.860 --> 00:05:18.960
And then the way we're going to operate working categorical columns here is we're first going to create

78
00:05:19.020 --> 00:05:25.620
a string indexer and then we're going to one hot encode them can basically what that means is the string

79
00:05:25.620 --> 00:05:30.240
indexer allows us to convert every string into a number.

80
00:05:30.300 --> 00:05:35.360
So a perfect example of this is going to be kind of the male female.

81
00:05:35.400 --> 00:05:39.600
So let's go ahead and create a gender index or we'll type along now and then we'll copy of the next

82
00:05:39.600 --> 00:05:40.230
one.

83
00:05:40.350 --> 00:05:47.460
So I'm going to say gender indexer is equal to a string indexer.

84
00:05:47.460 --> 00:05:53.720
So we can just kind of tap autocomplete that and stree indexer is going to take in an input column parameters

85
00:05:53.820 --> 00:05:59.860
input call and the input column for this is going to be that sex column which indicates their gender.

86
00:06:00.150 --> 00:06:02.230
And then we need to say the output column.

87
00:06:02.400 --> 00:06:07.440
And then this is where you choose a name and kind of by default we usually choose that column name and

88
00:06:07.440 --> 00:06:10.450
then add an index kind of right next to it.

89
00:06:10.470 --> 00:06:16.860
So this gender indexer it's going to accept the sex column and then it's going to output an indexed

90
00:06:16.920 --> 00:06:17.670
version of it.

91
00:06:17.670 --> 00:06:24.330
And basically what that means is it's going to assign a number for every category of that column.

92
00:06:24.360 --> 00:06:30.540
So you can imagine let's say we had three different letters in a column A B and C and they each said

93
00:06:30.540 --> 00:06:35.850
stood for different categories what the indexer would do is it would assign a number to each of those

94
00:06:35.850 --> 00:06:39.620
so a would be zero would be one soon to be two and so on.

95
00:06:39.660 --> 00:06:42.520
So every particular category then gets a number.

96
00:06:42.750 --> 00:06:50.790
Then after that we're going to one hot encode them and what that means is it transforms the actual numbers

97
00:06:50.790 --> 00:06:56.490
for the categories into a one hot encoding where you have an array indicating kind of zeros and ones

98
00:06:56.880 --> 00:07:00.390
of what the actual category was.

99
00:07:00.390 --> 00:07:03.270
So I wanted to kind of show you in comments what that would actually look like.

100
00:07:03.450 --> 00:07:04.290
So the indexer.

101
00:07:04.320 --> 00:07:09.780
We start off with kind of something like ABC for in the case of sex it's really as male female but I

102
00:07:09.780 --> 00:07:12.970
know you use a couple of more letters here so it's clear kind of what we're doing.

103
00:07:13.110 --> 00:07:19.530
And then we have ABC indexers going to say 0 1 to kind of.

104
00:07:19.530 --> 00:07:20.500
And so on.

105
00:07:21.000 --> 00:07:28.530
And then we're going to do is one hot encoding and one high end coating does is for each of these kind

106
00:07:28.530 --> 00:07:33.710
of particular categories it's going to create a one hot encoding.

107
00:07:34.050 --> 00:07:41.670
And what that means is if you have a key that is basically the columns ABC to one hot encode an example

108
00:07:41.670 --> 00:07:51.290
column let's say the example was a damn what we can do for example they say that one zero zero and that's

109
00:07:51.290 --> 00:07:53.330
how you would one encode a.

110
00:07:53.390 --> 00:07:56.060
And let me zoom in here a little bit.

111
00:07:56.060 --> 00:08:02.060
What this does is in the case a vector format of only ones and zeros what the category is.

112
00:08:02.060 --> 00:08:04.740
So if you have a one there then you match up with a.

113
00:08:04.760 --> 00:08:08.520
And it basically indicates that has no B and no C category.

114
00:08:08.540 --> 00:08:14.600
So that's how you encode an example row the value of a in an order to do this we need two steps.

115
00:08:14.600 --> 00:08:20.150
One is to convert the strings into numbers and then the numbers and the being encoded through a 100

116
00:08:20.200 --> 00:08:20.600
coater.

117
00:08:20.630 --> 00:08:22.020
So the next thing we're going to do.

118
00:08:22.220 --> 00:08:28.590
We're going to say a gender encoder is equal to my one hot encoder

119
00:08:31.640 --> 00:08:37.010
and then the input column for this guy is going to be the sex index column that we just created with

120
00:08:37.040 --> 00:08:42.400
our indexer and then we're going to say output column is equal to.

121
00:08:42.570 --> 00:08:47.880
And in this case kind of the convention is to have the name of the original column and then say that

122
00:08:48.150 --> 00:08:52.040
because they're kind of vectors instead of single strings.

123
00:08:52.230 --> 00:08:58.290
So basically what happens is in every row you had a single string category such as male or female A

124
00:08:58.290 --> 00:09:00.720
or B or C etc. or whatever that string was.

125
00:09:00.780 --> 00:09:04.740
And eventually each entry just gets converted to a vector.

126
00:09:04.770 --> 00:09:12.060
So if you had an example of male female a row could have the input of male and eventually it would come

127
00:09:12.060 --> 00:09:17.200
out with something like a vector of 1 common 0 indicating male was one female with zero.

128
00:09:17.290 --> 00:09:21.490
And if someone were female at the end of the day they would have something like 0 1 is the vector.

129
00:09:21.970 --> 00:09:25.090
OK so that's her indexer and that's our encoder.

130
00:09:25.110 --> 00:09:30.420
We're going to do the exact same thing for the Embarq columns and it's kind of nice about this is you

131
00:09:30.420 --> 00:09:33.380
actually don't need to know how many categories there are beforehand.

132
00:09:33.570 --> 00:09:38.240
The indexer and encoder combination essentially takes care of this for you.

133
00:09:38.450 --> 00:09:43.540
So here we that the Embarq indexer and Embarq and coater so embark in there.

134
00:09:43.570 --> 00:09:46.730
Again that's kind of a category that's actually really similar to ABC.

135
00:09:46.730 --> 00:09:48.190
I think the values are C-s.

136
00:09:48.190 --> 00:09:55.360
Q In the kitting kind of first letters of different cities and then the next step is to create an assembler.

137
00:09:55.620 --> 00:10:02.830
So we're going to assemble all this using a vector assembler and then we're going to have input columns

138
00:10:03.760 --> 00:10:10.700
and the input columns are going to be P.E. class and then we're going to choose the vector columns we

139
00:10:10.700 --> 00:10:11.120
created.

140
00:10:11.120 --> 00:10:16.910
So we create a sex vector column and then we're also going to end up creating a Embarq vector column

141
00:10:18.000 --> 00:10:25.290
and then whoops then after that we have basically everything else we did which was age and these kind

142
00:10:25.290 --> 00:10:26.230
of already.

143
00:10:26.280 --> 00:10:30.210
And if you take a look at the data they're already integer so you really don't mess with them.

144
00:10:31.580 --> 00:10:36.700
Then we'll do parent child and I believe Fair which is a floating point value.

145
00:10:36.890 --> 00:10:37.660
So we have that.

146
00:10:37.790 --> 00:10:44.870
And then those are the input columns the output columns that we're going to be doing here put call that's

147
00:10:44.870 --> 00:10:49.340
just going to be equal to let's say features which is I think the default as well.

148
00:10:49.370 --> 00:10:51.890
So let's run that make sure we didn't do a typo there.

149
00:10:51.890 --> 00:10:54.320
OK so there's our assembler it's a vector assembler.

150
00:10:54.320 --> 00:10:58.890
Notice how we're no longer using the embarked column or the sex column.

151
00:10:58.970 --> 00:11:03.950
We're only really going to be using the sex vector and Embarq vector and that's going to allow our logistic

152
00:11:03.950 --> 00:11:07.110
regression model to work with categorical data.

153
00:11:07.520 --> 00:11:12.470
So now it's time to actually use that classification model and not only are we're going to use the classification

154
00:11:12.470 --> 00:11:13.330
all directly.

155
00:11:13.370 --> 00:11:17.960
We're also going to show you how to use a pipeline because we really haven't actually called the index

156
00:11:17.970 --> 00:11:19.130
or encoder yet.

157
00:11:19.130 --> 00:11:20.090
We've only created them.

158
00:11:20.090 --> 00:11:21.920
We're still need to call them.

159
00:11:22.340 --> 00:11:30.170
We're going to do now is safe from Paice sparked the M-L dot classification import our logistic regression

160
00:11:31.190 --> 00:11:39.950
and then we'll also say from Paice spark the M-L import our pipeline and now it's time to create our

161
00:11:39.950 --> 00:11:40.910
pipeline.

162
00:11:40.910 --> 00:11:45.080
So the pipeline does is it sets stages for different steps.

163
00:11:45.140 --> 00:11:51.470
If you have a very complex machine learning task you will often have to set up a bunch of stages and

164
00:11:51.470 --> 00:11:55.670
in our case this was a little more complex than what we've dealt with before because we're dealing with

165
00:11:55.730 --> 00:11:59.510
indexing and coding so we want to set the stages of how to do this.

166
00:11:59.510 --> 00:12:05.910
So before we actually assemble this vector we need to do the indexing and encoding for the actual dataset.

167
00:12:06.110 --> 00:12:09.220
So that's what the pipeline serves as.

168
00:12:09.470 --> 00:12:15.530
So we're first going to do the following will say log score read Titanic and you can always copy the

169
00:12:15.530 --> 00:12:17.240
notes here.

170
00:12:17.240 --> 00:12:19.410
Titanic is equal to.

171
00:12:19.490 --> 00:12:25.550
It will create a logistic regression object and the features column it expects is going to be just called

172
00:12:25.580 --> 00:12:33.100
features and then the label column is going to be survived.

173
00:12:33.200 --> 00:12:36.250
That's already a zero on one column for us.

174
00:12:36.680 --> 00:12:36.940
OK.

175
00:12:36.950 --> 00:12:40.610
So that's our logistic regression object that expects features and survives.

176
00:12:40.700 --> 00:12:42.350
And now we're going to create a pipeline.

177
00:12:42.380 --> 00:12:47.480
So I'm going to copy and paste this from the notes so you guys can do the same to make sure to have

178
00:12:47.480 --> 00:12:50.360
any typos here but this is essentially what we're going to do.

179
00:12:50.360 --> 00:12:55.730
The pipeline has a status parameter which is a list of everything you want to do.

180
00:12:56.400 --> 00:13:02.490
So the first thing to do is index the gender column and remember the indexer right here for the gender

181
00:13:02.550 --> 00:13:06.810
takes in the input column Sex and then outputs a column sex index.

182
00:13:06.810 --> 00:13:11.130
Keep in mind that once you apply that you actually still have that sex column in your data.

183
00:13:11.220 --> 00:13:14.780
But now in addition to that you also have a sex index column in your data.

184
00:13:14.780 --> 00:13:20.380
Then we're going to run our 100 coater which grabs that sex index column just created and then outputs

185
00:13:20.410 --> 00:13:24.690
sex vector column and the vector column is the only one I really need at the end of the day which is

186
00:13:24.690 --> 00:13:30.090
why in our vector Sembler we only grab the X Factor column and the Embarq vector column as well as the

187
00:13:30.090 --> 00:13:33.830
ones that were already either an integer or float format for us.

188
00:13:33.960 --> 00:13:35.490
And then we're going to the same thing.

189
00:13:35.550 --> 00:13:40.500
Transforming the Embarq category column into one hot uncoated column.

190
00:13:40.500 --> 00:13:41.940
So that's all this is doing.

191
00:13:41.940 --> 00:13:45.270
And we passed this in as a list of steps for the objects or assembler.

192
00:13:45.420 --> 00:13:48.600
And then finally at the end you passen your actual model.

193
00:13:48.780 --> 00:13:52.180
So kind of interesting here you said stages of the transformations.

194
00:13:52.320 --> 00:13:54.910
Assemble them and then have the model.

195
00:13:54.930 --> 00:13:56.180
So that's our pipeline.

196
00:13:57.380 --> 00:14:02.120
And then what's really cool about this pipeline object is you can treat it just like you would a normal

197
00:14:02.120 --> 00:14:03.020
model.

198
00:14:03.020 --> 00:14:07.110
And the first thing I want to do is get some training data and some test data.

199
00:14:07.370 --> 00:14:15.080
So I'm going to say my training data Khama my test data do a little bit of tuple unpacking here is equal

200
00:14:15.080 --> 00:14:22.710
to my final data that I created and I'm going to call a random split off of this will say random split

201
00:14:23.340 --> 00:14:30.440
and we'll say 17:30 So 70 percent of it is training data 30 percent of it is testate and you can see

202
00:14:30.440 --> 00:14:32.040
here I've created two data frames.

203
00:14:32.240 --> 00:14:38.780
And then the next thing you're always to do is say my fitted model is equal to and you call a pipeline

204
00:14:38.780 --> 00:14:43.330
object just like you would a normal machine learning object or machine learning algorithm.

205
00:14:43.430 --> 00:14:49.160
You call it right off of it and then passing your training data so pipelined fit in I'm going to get

206
00:14:49.160 --> 00:14:52.340
to that training data that's running that job right now.

207
00:14:52.540 --> 00:14:54.380
Can a hop forward in time to finish training.

208
00:14:54.380 --> 00:14:54.620
OK.

209
00:14:54.640 --> 00:15:00.780
Well there you go only took five seconds and then the next step is to transform our test data.

210
00:15:00.800 --> 00:15:03.660
So we actually want to test our results on a test data set.

211
00:15:04.130 --> 00:15:15.290
So I'll say the results is equal to that model and then I'm going to transform my test data so transforming

212
00:15:15.300 --> 00:15:16.010
the test data.

213
00:15:16.110 --> 00:15:24.030
And then after I've done that it's time to evaluate we'll say from Paice bark the M-L the evaluation.

214
00:15:25.250 --> 00:15:32.200
I'm going to import a binary classification evaluator and that allows me to get some quick classification

215
00:15:32.200 --> 00:15:34.900
metrics for a binary classification.

216
00:15:34.930 --> 00:15:41.860
So from MLT evaluation import binary classification and then we'll say my evaluator object or my evil

217
00:15:42.640 --> 00:15:46.900
is equal to a binary classification evaluator object.

218
00:15:46.900 --> 00:15:52.930
And then depending on what version you're using of SPARC you need to pass in either prediction column

219
00:15:52.960 --> 00:15:54.390
or raw prediction column.

220
00:15:54.550 --> 00:16:01.470
If you're using 2.2 which I am here it should be raw prediction column and then we'll passen prediction.

221
00:16:01.480 --> 00:16:03.990
So how do I know to pass this string prediction.

222
00:16:04.000 --> 00:16:10.180
Well by default whenever you fit or excuse me whatever transforms and test data it's going to automatically

223
00:16:10.180 --> 00:16:14.710
call its predicted column prediction in lowercase so we can have that right there.

224
00:16:14.830 --> 00:16:21.420
And then the label call them is the label column of the original data which in our case is survived

225
00:16:23.400 --> 00:16:25.170
so we've created our evaluator object.

226
00:16:25.170 --> 00:16:28.540
Now it's time to actually use it.

227
00:16:28.590 --> 00:16:35.190
Now what I'm going to do is say my evil and then I'm going to call the evaluate method off of this and

228
00:16:35.190 --> 00:16:39.920
then I'm going to pasan my results before I do this I just want to show you what the results look like.

229
00:16:39.940 --> 00:16:41.310
What say results.

230
00:16:41.340 --> 00:16:49.220
And let's go ahead and just say results select and let's select the columns that we're passing in.

231
00:16:49.240 --> 00:16:56.440
So it's going to be a survived column and the prediction column and sales show.

232
00:16:56.730 --> 00:17:02.010
So what the results actually look like if we select just these two columns It's a survived column and

233
00:17:02.010 --> 00:17:03.270
a prediction column.

234
00:17:03.270 --> 00:17:06.520
So here at the very top you can see that none of these people survived.

235
00:17:06.540 --> 00:17:09.060
But some of them were actually predicting that they did survive.

236
00:17:09.060 --> 00:17:13.070
Some of them were matching up with correct prediction of 0 0.

237
00:17:13.110 --> 00:17:17.730
So in order to actually evaluate this we can use the binary classification evaluator and the binary

238
00:17:17.730 --> 00:17:21.180
classification evaluator is going to return area under the curve.

239
00:17:21.510 --> 00:17:30.170
So I'll say a U.S. is equal to my evil and we can evaluate the results.

240
00:17:31.200 --> 00:17:36.120
And then we can check if we scroll down a little more with the area under the curve was there.

241
00:17:36.570 --> 00:17:39.230
So in this case it's 0.7 6.

242
00:17:39.660 --> 00:17:45.230
OK and that's the basics of working with a logistic regression on a more realistic example.

243
00:17:45.270 --> 00:17:50.580
Keep in line we're going to see lots more examples of doing some sort of binary classification evaluation

244
00:17:50.940 --> 00:17:56.400
and later on we'll see some other metrics how to get things like accuracy or recall or precision those

245
00:17:56.400 --> 00:18:00.120
sort of things right now we're just dealing with area under the curve which you can check up the resource

246
00:18:00.120 --> 00:18:04.700
link if you're unfamiliar with it has to do if ROIC curves if you need help in interpreting it.

247
00:18:04.830 --> 00:18:10.290
And also in the resource links there's a really nice example for an other dataset where you have to

248
00:18:10.290 --> 00:18:13.770
do kind of a similar task of indexing and encoding main things.

249
00:18:13.770 --> 00:18:20.670
I just want to go over here is that we learned now how to do indexing and in coding for a categorical

250
00:18:20.670 --> 00:18:24.370
data and then we also learn how to set up our pipelines for them.

251
00:18:24.630 --> 00:18:25.090
OK.

252
00:18:25.200 --> 00:18:30.420
Definitely recommend you check out the two links in this lecture for resources on more information as

253
00:18:30.420 --> 00:18:32.390
well as more examples how to do this.

254
00:18:32.400 --> 00:18:35.580
Coming up next is going to be your example consulting project.

255
00:18:35.610 --> 00:18:36.630
I'll see you there.
