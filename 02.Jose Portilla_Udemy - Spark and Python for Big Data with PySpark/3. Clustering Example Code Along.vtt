WEBVTT
1
00:00:06.110 --> 00:00:11.210
Hello everyone and welcome to the K means clustering custom code along lecture.

2
00:00:11.290 --> 00:00:16.870
So we're going to be working through a real dataset containing some data on three distinct seed types

3
00:00:17.080 --> 00:00:20.720
and the corresponding notebook for this lecture is called clustering code along.

4
00:00:20.800 --> 00:00:27.020
And you can find it under the clustering folder under Sparke for machine learning of the spark notes.

5
00:00:27.050 --> 00:00:31.830
So for a certain machine learning algorithms sometimes it's a good idea to actually scale your data.

6
00:00:31.940 --> 00:00:36.440
And this is due to something called curse of dimensionality which you can read about in an introduction

7
00:00:36.440 --> 00:00:37.790
to Cisco learning.

8
00:00:37.940 --> 00:00:42.880
But basically what happens is you have a drop in model performance with highly dimensional data.

9
00:00:42.890 --> 00:00:46.280
So we're going to get some practice scaling features using Paice park.

10
00:00:46.370 --> 00:00:50.750
While it may not be necessary for this particular dataset that we work with later on if you're dealing

11
00:00:50.750 --> 00:00:56.960
with a dataset that has many many features to it or many columns of data which is highly dimensional

12
00:00:57.180 --> 00:01:01.170
work what you need to do is actually scale those features so we'll show you how to do that.

13
00:01:01.190 --> 00:01:02.940
Paice park and get some practice.

14
00:01:03.880 --> 00:01:10.390
Now remember there won't be any confusion matrix or classification test results when dealing with unsupervised

15
00:01:10.390 --> 00:01:11.150
learning.

16
00:01:11.140 --> 00:01:15.550
Meaning we don't have the original labels to actually perform some sort of test again.

17
00:01:15.580 --> 00:01:21.310
And again I keep mentioning this issue that it's a little hard to evaluate an unsupervised learning

18
00:01:21.340 --> 00:01:25.740
algorithm for truth because you never have those true labels to begin with.

19
00:01:26.580 --> 00:01:29.180
So this is a common point of confusion for beginners.

20
00:01:29.220 --> 00:01:32.660
You can't easily check to see how well you're clustering algorithm perform.

21
00:01:32.670 --> 00:01:35.720
And this is a difficulty for all unsupervised tasks.

22
00:01:35.730 --> 00:01:40.380
And if this is something that is a confusing point for you then I suggest you do the reading and an

23
00:01:40.380 --> 00:01:45.630
introduction to statistical learning that explains the unsupervised learning process and clustering

24
00:01:45.660 --> 00:01:47.020
algorithms in general.

25
00:01:47.190 --> 00:01:51.920
So let's get started to open up a new Jupiter notebook and walk through the data as well as the process

26
00:01:54.170 --> 00:01:55.560
or start a new notebook.

27
00:01:55.570 --> 00:02:00.820
And the first thing I'm going to do is actually start a spark session so I'll do this quickly since

28
00:02:00.820 --> 00:02:02.670
we've done it so many times before.

29
00:02:02.890 --> 00:02:08.890
Sparke session builder app name and I'll just call this cluster and then I'm going to get or create

30
00:02:08.980 --> 00:02:11.670
this Sparke session.

31
00:02:11.710 --> 00:02:15.440
Now the next step is to explore the data and what it actually looks like.

32
00:02:15.520 --> 00:02:21.430
It's just a simple see as the data set so we can say spark that read see a city and this dataset is

33
00:02:21.430 --> 00:02:27.140
called seed's underscore the data set that CXXVI and then will say Hetter is true.

34
00:02:28.180 --> 00:02:35.270
And we'll also say infer schema is true and else take a look at the data set schema.

35
00:02:36.030 --> 00:02:37.630
So I'll say print schema.

36
00:02:37.810 --> 00:02:42.730
Now we can see here we have various columns area parameter compactness etc..

37
00:02:42.730 --> 00:02:45.520
So let me give you a little bit of background behind the data set.

38
00:02:45.520 --> 00:02:51.190
So this is a seed data set and it's obtained from the University of California Irvine machine learning

39
00:02:51.190 --> 00:02:55.480
repository and the link is in the notebook that corresponds to this lecture.

40
00:02:55.570 --> 00:03:01.900
And basically what this is is an experiment in visualizing kernels and their actual features.

41
00:03:01.900 --> 00:03:10.360
So a group got some kernels for three different varieties of wheat that is Khama Rosa and Canadien and

42
00:03:10.360 --> 00:03:12.220
there were seven the elements each.

43
00:03:12.340 --> 00:03:14.590
And we randomly select them for this experiment.

44
00:03:14.650 --> 00:03:20.500
And what they ended up doing was they created a really high quality visualization of the internal kernel

45
00:03:20.500 --> 00:03:24.620
that is the seed structure that was detected using an X-ray technique.

46
00:03:24.880 --> 00:03:31.700
So what they ended up doing is taking a bunch of geometric parameters using those X-ray pictures.

47
00:03:31.780 --> 00:03:36.930
Things like the area of the kernel the perimeter of the kernel the length with cetera.

48
00:03:37.150 --> 00:03:43.120
So you have all these characteristics and all of these values are just real valued continuous numbers.

49
00:03:43.150 --> 00:03:46.710
If we take a look at what an example looks like.

50
00:03:46.760 --> 00:03:52.490
So if I say to the head of one notice here I have the area parameter x that are there all essentially

51
00:03:52.490 --> 00:03:56.580
just numbers so these are all just features of those actual seed and kernels.

52
00:03:56.580 --> 00:04:00.210
Now we don't have that many features that dimensions here.

53
00:04:00.260 --> 00:04:07.110
And actually either all around the same magnitude from 0 to maybe around 25 ish.

54
00:04:07.160 --> 00:04:12.710
So as far as scaling goes it will probably not be necessary but we're going to work through that anyways.

55
00:04:12.910 --> 00:04:13.440
OK.

56
00:04:13.700 --> 00:04:20.030
So again one more thing to notice here is that there's no actual label indicating what seed belongs

57
00:04:20.030 --> 00:04:21.230
to what group.

58
00:04:21.440 --> 00:04:25.190
What I do know however is there was three different varieties of wheat.

59
00:04:25.220 --> 00:04:30.860
So what we're going to try to do is Cluster this with K equals 3 and that's essentially our domain knowledge

60
00:04:30.860 --> 00:04:32.170
comes into play.

61
00:04:32.180 --> 00:04:37.050
So maybe you're out with these experimenter's you're out in the field collecting samples and through

62
00:04:37.070 --> 00:04:42.180
your domain knowledge you know that there's three possible seed types to be found in this field.

63
00:04:42.350 --> 00:04:47.050
However you don't know what seeds you collect there or something and you want to close them.

64
00:04:47.180 --> 00:04:52.280
Knowing your theory there may knowledge into three separate groups so that's kind of an example of using

65
00:04:52.280 --> 00:04:54.140
your domain knowledge.

66
00:04:54.140 --> 00:05:03.660
So we'll continue along here and we'll say from PI sparked the M-L clustering import Kamins.

67
00:05:03.710 --> 00:05:12.470
All right now we're going to do is actually format that data so we can say from Paice bark M-L feature

68
00:05:13.600 --> 00:05:16.370
import and import a vector assembler.

69
00:05:16.700 --> 00:05:22.130
And then if we take a look at the columns remember that this is an unsupervised learning algorithm.

70
00:05:22.130 --> 00:05:27.470
And luckily I only have feature columns here and this is a python list which is exactly what I would

71
00:05:27.470 --> 00:05:29.620
usually pass to my assembler object.

72
00:05:29.780 --> 00:05:36.110
So instead of the input column argument going to be a list I will just pass in this dataset that columns

73
00:05:36.110 --> 00:05:38.010
objects Let me show what I mean by that.

74
00:05:38.300 --> 00:05:45.260
And going to create my assembler set equal to vector assembler and specify that the input calls argument

75
00:05:45.440 --> 00:05:50.270
is going to be data set columns instead of having to copy and paste all that.

76
00:05:50.270 --> 00:05:58.280
And then finally the next thing I want is my output column to be called features so now will run that

77
00:05:58.730 --> 00:06:04.010
and then the next thing I'm going to do is just transform this file have final data is equal to my assembler

78
00:06:04.010 --> 00:06:11.930
object and then I'm going to transform my dataset and if I take a look at my final data and print the

79
00:06:11.930 --> 00:06:19.140
schema I can see here it has these various columns but more importantly it has the features column.

80
00:06:19.260 --> 00:06:24.060
Now in the past what we've done is just select the features column that actually a lot of these machine

81
00:06:24.060 --> 00:06:25.380
learning algorithm objects.

82
00:06:25.440 --> 00:06:27.500
They don't mind having a bunch of extra columns.

83
00:06:27.630 --> 00:06:32.160
They won't read them and they won't do anything of them but they're really looking for is the features

84
00:06:32.160 --> 00:06:35.190
column or the label column in this case we don't need a label.

85
00:06:35.190 --> 00:06:37.560
We're really just needing that particular features column.

86
00:06:37.560 --> 00:06:39.660
So we'll just keep final data as is.

87
00:06:39.930 --> 00:06:43.540
And now what I want to do is work through the ceiling of the data.

88
00:06:43.590 --> 00:06:46.600
So in order to scale the data it's actually quite simple.

89
00:06:46.800 --> 00:06:56.060
I will say from Paice park the M-L in the feature family will say import standard scaler and then this

90
00:06:56.060 --> 00:07:02.450
is almost like creating an assembler object will say scalar is equal to a standard scalar object where

91
00:07:02.450 --> 00:07:11.900
we expect the input column to be equal to features and we expect the output column to be equal to and

92
00:07:11.900 --> 00:07:20.320
we'll call this wups pasand a string here we'll call the scaled features and then there's other parameters

93
00:07:20.320 --> 00:07:24.090
here and if you do shift tab over at standard scaler you can explore them.

94
00:07:24.310 --> 00:07:28.750
But essentially what you can do is with mean or with standard deviation.

95
00:07:28.750 --> 00:07:34.920
So essentially just describes how you actually want to do the scaling itself.

96
00:07:34.930 --> 00:07:38.460
So you want to do with the mean or do you want to do it with the standard deviation.

97
00:07:38.470 --> 00:07:43.180
Then you can actually check here the Wikipedia page in case you want more information about the background

98
00:07:43.450 --> 00:07:45.970
behind the mathematics behind that we'll just keep the default.

99
00:07:46.000 --> 00:07:47.830
That should be fine for us.

100
00:07:47.830 --> 00:07:52.160
So we'll run that scalar object and then we're going to fit that to the final data.

101
00:07:52.240 --> 00:08:02.100
So I'm going to make a scalar model variable and that's going to be scalar fits on the final data.

102
00:08:02.100 --> 00:08:04.980
It's almost behaving just like a machine learning object.

103
00:08:05.070 --> 00:08:09.460
So you fit it to the final data and then you transform the final data.

104
00:08:09.520 --> 00:08:17.970
So I'm going to overwrite my final data and then say scaler model and then transform my final data and

105
00:08:17.970 --> 00:08:20.260
some people like to do this all in one step.

106
00:08:20.370 --> 00:08:23.180
So they put this transform right over here.

107
00:08:23.220 --> 00:08:27.210
After that that fit and just doing in two steps so you can get an idea of what's happening.

108
00:08:27.210 --> 00:08:30.900
So basically we're happening at what is happening here is this scalar object.

109
00:08:31.110 --> 00:08:36.300
When you fit it to this final data it collects that standard deviation information and then it sets

110
00:08:36.440 --> 00:08:37.890
into the scalar model.

111
00:08:37.950 --> 00:08:43.290
So the scarer model is then ready to collect anything in the form of final data and now can transform

112
00:08:43.290 --> 00:08:46.400
it to be scaled which is what happens when called transform.

113
00:08:46.410 --> 00:08:48.750
So that's kind of what these two steps are doing here.

114
00:08:48.750 --> 00:08:52.520
All right so now let's actually take a look at what final data looks like.

115
00:08:52.910 --> 00:08:59.370
If we take a look at the head notice we have our features column and then we have our scaled features

116
00:08:59.370 --> 00:09:00.160
column.

117
00:09:00.180 --> 00:09:04.610
Now in this case as I mentioned everything seems to be around the same order of magnitude.

118
00:09:04.620 --> 00:09:08.020
So the scaling isn't going to be such a big deal here.

119
00:09:08.100 --> 00:09:13.260
So you can see that there is a change between the features and the scaled features but it's not a huge

120
00:09:13.260 --> 00:09:14.060
change.

121
00:09:14.070 --> 00:09:20.460
This becomes more important when you have not just high mentions of data meaning many features but you

122
00:09:20.460 --> 00:09:25.310
also have orders of magnitude varying a lot between your actual data.

123
00:09:25.320 --> 00:09:31.380
So maybe you have one column that's in units of thousands of miles or something and then you have another

124
00:09:31.380 --> 00:09:33.770
column that's in units of millimeters.

125
00:09:33.810 --> 00:09:38.490
Those kind of things can also be improved upon using some sort of scaling technique.

126
00:09:38.490 --> 00:09:42.990
So hopefully by now you understand how to use scaler even though he probably didn't really need it for

127
00:09:42.990 --> 00:09:44.700
this particular data set.

128
00:09:44.700 --> 00:09:49.700
So now it just comes time to train the model and try our best to evaluate it.

129
00:09:49.770 --> 00:09:54.880
So we'll see Kamins is equal to an instance of Kamins loops.

130
00:09:54.910 --> 00:09:55.670
There we go.

131
00:09:55.900 --> 00:10:01.780
And earlier we saw this method the set K but you could also provide it as a parameter so if you do shift

132
00:10:01.780 --> 00:10:05.670
tab here you can see here that it actually takes in a k value.

133
00:10:05.710 --> 00:10:12.040
So we're going to do is the following will specify that the features column is going to be the scaled

134
00:10:12.040 --> 00:10:18.910
features so will say skill features and then we'll specify K is equal to 3 since we know we expect 3

135
00:10:18.910 --> 00:10:23.550
seed groups and then we're going to do is actually create the model.

136
00:10:23.590 --> 00:10:30.900
So model is that case means object and we'll fit it to all the final data and now once this is done

137
00:10:30.900 --> 00:10:32.360
we're basically done the model.

138
00:10:32.580 --> 00:10:39.260
So what we can do is the following can say WSOC C for the within set some of squirters.

139
00:10:39.360 --> 00:10:42.070
And that's just equal to what we can actually print this.

140
00:10:42.090 --> 00:10:50.950
So I'll say prints within set sum of squared errors and then we will say the following will say print

141
00:10:51.760 --> 00:10:57.440
the model and we can compute the cost on that final dataset.

142
00:10:57.530 --> 00:10:59.970
And so there is are within set some of squared errors.

143
00:11:00.140 --> 00:11:05.180
Now this isn't super helpful because we scale the data but just keep that in mind that you can access

144
00:11:05.180 --> 00:11:07.000
that information if you ever need to.

145
00:11:07.100 --> 00:11:12.980
If you're looking for the cluster centers we can imagine doing is just saying centers is equal to model

146
00:11:13.910 --> 00:11:22.840
cluster centers and then we can print the centers and there you have it some Morais and we have it's

147
00:11:22.980 --> 00:11:28.140
a little sloppy because I'm zoomed in here but you can see here that and this one two three four five

148
00:11:28.140 --> 00:11:32.610
six seven dimensional space or more you can see the centers themselves.

149
00:11:32.610 --> 00:11:36.810
And there's three groups of centers which makes sense because we set K equal to three.

150
00:11:36.810 --> 00:11:39.710
Now what we really want to know probably is the actual predictions.

151
00:11:39.870 --> 00:11:41.860
And that's usually what you're most interested in.

152
00:11:42.360 --> 00:11:49.950
So we can grab those by saying model transform the final data and then we're going to show the results

153
00:11:49.950 --> 00:11:50.400
here.

154
00:11:51.720 --> 00:11:53.370
So we have a bunch of columns here.

155
00:11:53.370 --> 00:11:57.060
So let's actually select the ones we're interested in.

156
00:11:57.450 --> 00:12:05.280
So we're going to say select and we'll say prediction and I can see the prediction for each of the groups

157
00:12:05.310 --> 00:12:07.120
and it's going to start indexing at zero.

158
00:12:07.140 --> 00:12:11.210
So the group possibilities for my prediction are either 0 1 or 2.

159
00:12:11.580 --> 00:12:12.900
And that's basically it.

160
00:12:13.230 --> 00:12:19.200
As far as checking this against some labels you can if you want to because this particular data set

161
00:12:19.260 --> 00:12:22.340
from UCI it did actually come with labels.

162
00:12:22.350 --> 00:12:24.330
I strip those before we started.

163
00:12:24.330 --> 00:12:28.830
But in a realistic setting you wouldn't have those labels otherwise you wouldn't need to do clustering.

164
00:12:28.830 --> 00:12:29.720
Since the beginning.

165
00:12:29.990 --> 00:12:35.600
But hopefully this shows you how you can use k means to discover clusters within your unlabeled data

166
00:12:35.600 --> 00:12:36.330
sets.

167
00:12:36.540 --> 00:12:37.290
OK.

168
00:12:37.290 --> 00:12:39.350
Coming up next is your consulting project.

169
00:12:39.360 --> 00:12:41.030
I think it should be a really fun one.

170
00:12:41.040 --> 00:12:44.550
There's a really cool premise behind it so I'll see in the next lecture.
