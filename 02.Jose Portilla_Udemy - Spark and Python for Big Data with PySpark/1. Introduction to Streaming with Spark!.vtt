WEBVTT
1
00:00:05.310 --> 00:00:10.850
Hello everyone and welcome to the sparks streaming section of the course sparks streaming is an extension

2
00:00:10.850 --> 00:00:17.240
of the Course part API that enables scalable high throughput fault tolerant stream processing of live

3
00:00:17.240 --> 00:00:18.520
data streams.

4
00:00:18.530 --> 00:00:22.700
Previously we've been working with data that's already been collected in this case we're going to move

5
00:00:22.700 --> 00:00:29.980
on to actually accepting live data data can be ingested from many sources like Kafka flume Kinesis or

6
00:00:29.980 --> 00:00:36.160
TCAP sockets and it can be processed using complex algorithms expressive high level functions like map

7
00:00:36.160 --> 00:00:38.830
reduce join and window.

8
00:00:38.870 --> 00:00:40.130
So what does it actually look like.

9
00:00:40.130 --> 00:00:45.920
Well basically you have here in green on the left hand side whatever your data sources and that's streaming

10
00:00:45.920 --> 00:00:47.060
live data.

11
00:00:47.060 --> 00:00:49.830
So Kafka flume can ysis the effects.

12
00:00:49.880 --> 00:00:55.300
Those are more of open source libraries for actually setting up an infrastructure for streaming.

13
00:00:55.310 --> 00:00:59.540
Some of you may be more familiar with as just that consumer is something like Twitter where we have

14
00:00:59.540 --> 00:01:02.440
streaming information from different users something different tweets.

15
00:01:02.480 --> 00:01:05.300
And we're actually going to explore that in the section of the course.

16
00:01:05.330 --> 00:01:12.140
So whatever your data sources Sparke streaming can then ingest it work with it and output it either

17
00:01:12.320 --> 00:01:15.620
as HD effects into a database or into dashboards.

18
00:01:15.620 --> 00:01:21.110
Showing you live feeds and later on in the course will actually show you how we can kind of make a pseudo

19
00:01:21.110 --> 00:01:28.120
dashboard showing some live stats on Twitter and internally sparks streaming receives live input data

20
00:01:28.120 --> 00:01:33.250
streams and it divides the data into batches which are then processed by the spark engine to generate

21
00:01:33.250 --> 00:01:35.650
the final stream of results in batches.

22
00:01:35.740 --> 00:01:37.290
So it basically looks like this.

23
00:01:37.320 --> 00:01:42.880
The input data stream that's just coming live and then you have the batches of the input data coming

24
00:01:42.880 --> 00:01:47.440
out after it goes through specked streaming and then it goes through the basic spark engine which actually

25
00:01:47.440 --> 00:01:50.130
processes the batches of those data points.

26
00:01:50.980 --> 00:01:55.420
Now for this course we're going to first work through just a very simple stream example and you're going

27
00:01:55.420 --> 00:02:00.120
to need to be able to simultaneously use a Jupiter notebook and a terminal for this particular example

28
00:02:00.130 --> 00:02:01.630
we're going to do it in the next lecture.

29
00:02:01.690 --> 00:02:07.000
Essentially we're going to be doing is a pretty simple example where we treat our terminal as the input

30
00:02:07.000 --> 00:02:12.420
of a live stream in a Jupiter notebook is going to mean to use for streaming to constantly take in the

31
00:02:12.430 --> 00:02:13.850
batches of the data.

32
00:02:13.990 --> 00:02:18.450
So we'll type something in our terminal and we'll be able to record it live as if it was a direct stream.

33
00:02:18.460 --> 00:02:23.380
And as a quick to that particular lecture it's really easy to follow along through a local installation

34
00:02:23.380 --> 00:02:27.300
using Virtual Box if you've been using something like E.S. to or EMR.

35
00:02:27.340 --> 00:02:31.420
I would suggest you take a look at the virtual box installation lectures and follow along with those.

36
00:02:31.420 --> 00:02:32.810
That's probably the easiest way to do it.

37
00:02:34.120 --> 00:02:38.770
Now after working through that simple example will finish off the course with a Twitter analysis project

38
00:02:38.800 --> 00:02:44.660
where we're kind of going to be making a bit of a pseudo dashboard just a little visualization spark

39
00:02:44.660 --> 00:02:48.100
and Jupiter notebook and that visualization is going to update.

40
00:02:48.130 --> 00:02:51.970
Let's say every 15 seconds with different counts on Twitter.

41
00:02:51.970 --> 00:02:56.140
Now to follow along with that particular Twitter project you'll need to install a few visualization

42
00:02:56.140 --> 00:02:58.740
libraries and set up a Twitter developer account.

43
00:02:58.960 --> 00:03:00.940
And we're actually going to walk you through all those steps.

44
00:03:01.030 --> 00:03:06.690
When the time comes during those lectures now for the various possible other data sources that we mentioned

45
00:03:06.720 --> 00:03:12.030
earlier things like Kafka flume can ysis realistically we can't really show that in a single computer

46
00:03:12.030 --> 00:03:15.990
setting and it's actually quite tedious to set up that entire task.

47
00:03:15.990 --> 00:03:21.890
However if in your place of work you actually necessitate the use of one of those particular data sources.

48
00:03:21.900 --> 00:03:26.760
Realistically it would be a work environment you wouldn't really run these data sources yourself for

49
00:03:26.760 --> 00:03:27.760
personal use.

50
00:03:27.840 --> 00:03:31.710
Sparke actually provides full integration guides that are really quite thorough.

51
00:03:32.160 --> 00:03:37.440
So keep in mind that not every source version for instance that every version of Kafka is available

52
00:03:37.440 --> 00:03:38.950
with the python API.

53
00:03:39.150 --> 00:03:42.690
So let's jump to the documentation because I want to show you where you can find additional information

54
00:03:42.930 --> 00:03:46.060
on Sparke streaming of actually has its own full documentation page.

55
00:03:46.200 --> 00:03:51.470
And even then on the documentation page for each particular data source it has its own integration guide

56
00:03:51.480 --> 00:03:52.590
so it's really quite thorough.

57
00:03:52.710 --> 00:03:58.170
If this is something that needed to use at work you have the whole guide there and really thorough instructions.

58
00:03:58.200 --> 00:04:03.650
So it's simpler to Spark's made documentation page and then we'll work through the actual Spahr extremely

59
00:04:03.660 --> 00:04:06.360
documentation and show you where you can find everything.

60
00:04:06.420 --> 00:04:11.980
After that we'll head on over to the next lecture where we begin actually coding out with Sparke streaming.

61
00:04:12.110 --> 00:04:14.350
All right here I am at SPARC that Apache the org.

62
00:04:14.400 --> 00:04:19.560
And I'm going to walk you through the documentation and I'm also going to talk about a more modern version

63
00:04:19.560 --> 00:04:23.770
of Sparke streaming that is still in alpha at the point of this filming so we won't be using it.

64
00:04:23.880 --> 00:04:25.620
But I do want you to be aware of it.

65
00:04:25.740 --> 00:04:30.660
So if you come over as documentation and go to latest release sparked 2.1 then you can come to programming

66
00:04:30.660 --> 00:04:31.220
guides.

67
00:04:31.410 --> 00:04:34.020
And what we're going to be using is called SPARC streaming.

68
00:04:34.170 --> 00:04:38.310
But there's also structured streaming and I want to quickly talk about structures streaming before we

69
00:04:38.310 --> 00:04:40.200
jump straight into Sparke streaming.

70
00:04:40.200 --> 00:04:42.260
So he comes to structure streaming.

71
00:04:42.340 --> 00:04:47.010
There is a very large overview page but keep in mind that it's still an alpha it's not even in beta

72
00:04:47.010 --> 00:04:48.440
yet so it is quite early.

73
00:04:48.600 --> 00:04:53.370
But basically what this is it's it's using Spark's sessions what we've been using throughout the course

74
00:04:53.640 --> 00:04:58.580
for streaming and what it does it's it's basically Sparke streaming that's running on top of the sparks

75
00:04:58.580 --> 00:05:02.340
sequel engine and then you can use that data set and data from API.

76
00:05:02.410 --> 00:05:07.830
Either Skala a Java or Python and eventually at some point in the future this is going to be your main

77
00:05:07.830 --> 00:05:10.540
go to whenever approaching a streaming problem.

78
00:05:10.560 --> 00:05:13.310
And if you come over here they actually already have quick examples.

79
00:05:13.350 --> 00:05:18.840
And what we're going to do is work through the sparks streaming examples not these structures me examples

80
00:05:18.870 --> 00:05:22.560
because this is still too early to basically use it responsibly.

81
00:05:22.590 --> 00:05:24.680
You shouldn't really be using this in a production setting.

82
00:05:24.690 --> 00:05:29.520
We should be using at this point in time is the regular SPARC streaming but once you've done Sparke

83
00:05:29.520 --> 00:05:33.240
streaming you should come over to structure streaming and check out the examples.

84
00:05:33.240 --> 00:05:34.720
It'll be really easy to follow along.

85
00:05:34.830 --> 00:05:38.720
It's just basically a few different imports and the syntax has changed.

86
00:05:38.820 --> 00:05:41.410
But what's actually going on is quite similar.

87
00:05:41.410 --> 00:05:46.020
Or along the same lines and to basically kind of show you a simple example if you keep scrolling down

88
00:05:46.020 --> 00:05:52.140
here and as a basic overview of what you can think structures streaming is is it similar to taking that

89
00:05:52.140 --> 00:05:55.120
data stream input and then separating it into batches.

90
00:05:55.200 --> 00:06:01.050
But every piece of data in the data stream is a new row appended to an unbound the table.

91
00:06:01.060 --> 00:06:06.390
Basically what the unbound the table means is that it's unbounded in terms of limits of how large it

92
00:06:06.390 --> 00:06:06.940
was.

93
00:06:06.960 --> 00:06:11.760
We've been working around the data where we actually already know how large the data file is this case

94
00:06:12.060 --> 00:06:13.120
it's about the table.

95
00:06:13.140 --> 00:06:19.200
So you can just keep adding new streams to it and then you have actual triggers they can do so a query

96
00:06:19.200 --> 00:06:25.800
on that actual input generates a result table and every trigger interval say every second new rows get

97
00:06:25.800 --> 00:06:30.210
appended to the input table which eventually updates the result table and whenever the result table

98
00:06:30.210 --> 00:06:35.220
gets updated where you want to do is write the change result rows to the what's known as an external

99
00:06:35.220 --> 00:06:35.940
sync.

100
00:06:35.940 --> 00:06:37.600
So that's basically what it looks like here.

101
00:06:37.710 --> 00:06:39.900
This is for structures streaming not sparks streaming.

102
00:06:39.900 --> 00:06:41.340
Keep that in mind.

103
00:06:41.340 --> 00:06:45.410
So you have a trigger that's the query and the result and then some sort of output.

104
00:06:45.660 --> 00:06:46.210
OK.

105
00:06:46.350 --> 00:06:49.910
So you can keep looking at this page if you're interested in more details of this.

106
00:06:49.920 --> 00:06:55.080
This is still in alpha I want to stress that it will be the future but it's still a little too early

107
00:06:55.080 --> 00:06:58.680
for us to really use this responsibly in a production environment.

108
00:06:58.680 --> 00:07:00.110
So we'll come back up here.

109
00:07:00.120 --> 00:07:02.560
I do want you to check this out at some point in your free time.

110
00:07:02.730 --> 00:07:07.860
But if you come back to programming guides will focus on sparc streaming because this is the most stable

111
00:07:07.860 --> 00:07:12.700
version that's been used a lot already connects with all those external sources et cetera.

112
00:07:13.140 --> 00:07:17.310
So what we're going to work through is this quick example we'll do that in the next lecture and we'll

113
00:07:17.310 --> 00:07:23.970
be using a streaming context and we'll also be importing a spark context now context and spark context

114
00:07:23.970 --> 00:07:24.920
in general.

115
00:07:24.960 --> 00:07:31.050
That was the main way of using SPARC before SPARC 2.0 switched to that SPARC sequel and SPARC data frames

116
00:07:31.090 --> 00:07:32.340
SPARC sessions.

117
00:07:32.340 --> 00:07:36.240
So this is kind of a bit of a throwback but it's the most stable version especially when dealing with

118
00:07:36.240 --> 00:07:40.620
streaming data you want stuff to be stable and used and tested heavily.

119
00:07:40.800 --> 00:07:45.000
So we're this what we're going to be focusing on where we'll work through this simple example of reading

120
00:07:45.000 --> 00:07:50.190
stuff that's going into the terminal using Python and then later on we'll also work with actually using

121
00:07:50.460 --> 00:07:51.640
this with Twitter.

122
00:07:52.080 --> 00:07:56.040
So coming back up here you can see those diagrams that we were explaining before.

123
00:07:56.040 --> 00:07:58.920
You can check out the MLM operations that are available.

124
00:07:59.010 --> 00:08:03.210
But what I wanted to tell you about was inputting from different sources.

125
00:08:03.360 --> 00:08:06.780
Cephalic here it says input streams and receivers.

126
00:08:06.900 --> 00:08:12.120
It will take you to the general documentation part where it talks about some advanced sources like Kafka.

127
00:08:12.110 --> 00:08:15.480
Flume can ysis and these are things you may be using at a place of work.

128
00:08:15.480 --> 00:08:20.020
So if you keep scrolling down here beyond basic sources it'll come to advanced sources.

129
00:08:20.040 --> 00:08:23.390
And then it also has some notes about the python API.

130
00:08:23.550 --> 00:08:29.460
So as a spark to point one point one which is the latest version Kafka needs some flumes are now available

131
00:08:29.460 --> 00:08:31.850
in the Python API these to not be.

132
00:08:31.980 --> 00:08:37.500
But if you continue going here this is where I really want to point out some details under Kafka Flemington

133
00:08:37.560 --> 00:08:38.380
ysis.

134
00:08:38.490 --> 00:08:43.060
There are certain compatability issues and there's full integration guides for more details.

135
00:08:43.080 --> 00:08:47.670
So for example let's say at your place of work you want to implement some Spark's streaming tools with

136
00:08:47.670 --> 00:08:48.450
Kafka.

137
00:08:48.630 --> 00:08:53.550
So once you're on this guide or integration guide page you'll notice that there's actually some details

138
00:08:53.550 --> 00:09:00.510
regarding certain versions of the consumer API for Kafka especially between versions 0.8 0.9 and 0.10.

139
00:09:00.690 --> 00:09:04.040
So there's actually two separate corresponding Sparke streaming packages.

140
00:09:04.040 --> 00:09:08.800
And so for these advanced sources you may find yourself using There's corresponding Spurr extreme and

141
00:09:08.790 --> 00:09:11.840
packages not all of them are available for Python.

142
00:09:12.160 --> 00:09:16.870
So for Python you're going to have to use this SPARC streaming Kafka 0.8.

143
00:09:16.870 --> 00:09:22.630
Now it does work and it is compatible with 0.9 0.10 brokers but there are certain capabilities that

144
00:09:22.630 --> 00:09:23.560
it doesn't have.

145
00:09:23.620 --> 00:09:27.720
So it can't do things like SSL support or offset API.

146
00:09:27.850 --> 00:09:33.940
Now the higher versions of Kafka streaming 0.10 this is still an experimental API and it's actually

147
00:09:33.940 --> 00:09:35.830
not available for Python yet.

148
00:09:35.830 --> 00:09:38.480
That's probably something that's going to come with SPARC 2.2.

149
00:09:38.800 --> 00:09:41.710
So I just want you to be aware when dealing with some of these advance sources.

150
00:09:41.710 --> 00:09:44.800
Not everything is going to be available for you in Python.

151
00:09:44.890 --> 00:09:46.720
However maybe you just wait a year.

152
00:09:46.720 --> 00:09:49.290
Heck the way SPARC moves at such speed.

153
00:09:49.340 --> 00:09:52.720
You maybe even wait for months and stuff will be available more and more.

154
00:09:52.720 --> 00:09:56.180
Keep in mind things are always experimental and buggy when they first come out.

155
00:09:56.320 --> 00:10:00.100
So if you want true stability always go something that's labeled stable.

156
00:10:00.300 --> 00:10:00.810
OK.

157
00:10:00.970 --> 00:10:02.920
So definitely check out the rest of that programming guide.

158
00:10:02.920 --> 00:10:05.820
It's really quite thorough and it has a ton of information on it.

159
00:10:05.890 --> 00:10:11.560
But for now let's hop over and willing to open up my virtual box in the next lecture and set up a simple

160
00:10:11.560 --> 00:10:16.610
example of using Sparke streaming to reach streaming data from our own terminal.

161
00:10:16.630 --> 00:10:18.700
Thanks and I'll see you at the next lecture.
