WEBVTT
1
00:00:05.460 --> 00:00:11.120
Hello everyone and welcome to the natural language processing section of the course we're going to now

2
00:00:11.120 --> 00:00:13.430
learn the basics of natural language processing.

3
00:00:13.460 --> 00:00:18.920
And this is the field of machine learning that focuses on creating predictive models or even unsupervised

4
00:00:18.920 --> 00:00:21.740
learning algorithms from a text data source.

5
00:00:21.740 --> 00:00:25.960
That is straight from articles consisting of words.

6
00:00:26.030 --> 00:00:31.190
Now this natural language processing section otherwise known as an LP will just contain a single custom

7
00:00:31.190 --> 00:00:32.500
code long example.

8
00:00:32.630 --> 00:00:35.670
For one thing the documentation doesn't really have a full example instead.

9
00:00:35.810 --> 00:00:39.980
As little miniature examples on different features that we're going to be using and the custom code

10
00:00:39.980 --> 00:00:44.390
along is actually a larger multi-step process which is why it's going to be the main and only focus

11
00:00:44.570 --> 00:00:46.050
of this section of the course.

12
00:00:47.140 --> 00:00:52.240
Now as another note this is actually a very large field of machine learning with its own unique challenges

13
00:00:52.240 --> 00:00:54.220
and sets of algorithms and features.

14
00:00:54.220 --> 00:00:58.210
So really what we're going to cover here is just scratching the surface of this really interesting field

15
00:00:58.210 --> 00:01:00.180
of natural language processing.

16
00:01:00.250 --> 00:01:05.200
Now as far as optional reading suggestions if this is something that interests you I would suggest that

17
00:01:05.200 --> 00:01:09.270
you first start off just reading the Wikipedia article on natural language processing.

18
00:01:09.330 --> 00:01:13.120
Actually pretty good it gives you a nice overview of different techniques that are available and a little

19
00:01:13.120 --> 00:01:15.280
bit of the history of natural language processing.

20
00:01:15.430 --> 00:01:20.110
Then if you're really interested in focusing on natural language processing with Python I would suggest

21
00:01:20.140 --> 00:01:25.840
you do a google search for the T.K. book now an LDK is a python library that's actually separate from

22
00:01:25.840 --> 00:01:31.000
Sparc and it's kind of outside the scope of this course and it's not really for distributed datasets

23
00:01:31.030 --> 00:01:34.990
but if you find yourself really interested in natural language processing that's a really great book

24
00:01:35.020 --> 00:01:36.040
that's online.

25
00:01:36.050 --> 00:01:37.910
It uses it's own and LDK library.

26
00:01:37.930 --> 00:01:42.550
But the reason I suggest that is that it's totally free and it's online and it's a great resource for

27
00:01:42.550 --> 00:01:47.350
you if you really are interested in natural language processing specifically of Python.

28
00:01:47.380 --> 00:01:52.240
Now if you're interested in just statistical natural language processing the mathematics behind it I

29
00:01:52.240 --> 00:01:56.710
recommend the book Foundations of statistical natural language processing by Manning.

30
00:01:56.710 --> 00:01:57.000
OK.

31
00:01:57.010 --> 00:02:00.880
So again those are just optional reading suggestions in case this is a topic that you're really interested

32
00:02:00.880 --> 00:02:01.440
in.

33
00:02:02.350 --> 00:02:06.290
So moving along let's discuss what natural language processing could be used for.

34
00:02:06.430 --> 00:02:08.880
You could use it to cluster news articles.

35
00:02:09.010 --> 00:02:13.100
You could also use it to suggest similar books based on the actual content of the book.

36
00:02:13.150 --> 00:02:16.280
You could group legal documents together based on similarities.

37
00:02:16.420 --> 00:02:20.950
You could analyze consumer feedback or you could even do spam e-mail detection which is what we're going

38
00:02:20.950 --> 00:02:23.750
to be doing in this particular section of the course.

39
00:02:24.750 --> 00:02:30.880
Our basic process for any and all the task is to compile all the documents into what's known as a corpus

40
00:02:31.270 --> 00:02:35.650
then we're going to feature the words into some sort of numerical content like a matrix.

41
00:02:35.650 --> 00:02:37.750
Now not every machine learning algorithm needs this.

42
00:02:37.750 --> 00:02:43.120
There are some more advanced machine learning algorithms such as Word to vector or word to VEC that

43
00:02:43.120 --> 00:02:48.220
it can actually take in words themselves and then analyze them and create vectors from that.

44
00:02:48.220 --> 00:02:50.350
But that's outside the scope of this course.

45
00:02:50.370 --> 00:02:55.480
Said organo into focus is on classical natural language processing which can be extremely effective

46
00:02:55.780 --> 00:03:01.330
and that focuses on taking in words converting them to some sort of numerical data some sort of numerical

47
00:03:01.330 --> 00:03:02.130
matrix.

48
00:03:02.140 --> 00:03:06.800
And then we can compare features of documents using that numerical data.

49
00:03:06.830 --> 00:03:13.100
So let's discuss the standard way of converting a document or a corpus full of actual words or a text

50
00:03:13.340 --> 00:03:16.640
into something that a machine learning algorithm can understand.

51
00:03:16.640 --> 00:03:21.770
Most of these algorithms can only take in pure numbers so how do we actually transform words into a

52
00:03:21.770 --> 00:03:23.140
matrix full of numbers.

53
00:03:23.300 --> 00:03:28.760
Well the standard way of doing this is through the use of what is known as T.F. IDF A.F. of stands for

54
00:03:28.760 --> 00:03:31.360
term frequency inverse documented frequency.

55
00:03:31.360 --> 00:03:34.020
So let's explain how this particular topic works.

56
00:03:34.980 --> 00:03:40.200
So is a simple example let's imagine that we have two documents in our corpus of tax and one document

57
00:03:40.530 --> 00:03:45.480
is just a single line says Blue House and the next document is another line it says rent house.

58
00:03:45.480 --> 00:03:50.930
So what you could do as an idea to featuritis these words into numbers is just totally based off word

59
00:03:50.930 --> 00:03:51.500
count.

60
00:03:51.630 --> 00:03:57.420
And this is coming in as a bag of words approach so we say okay what are all the words that are available

61
00:03:57.420 --> 00:04:00.450
across this entire corpus or across all the documents.

62
00:04:00.450 --> 00:04:02.710
Well that would be blue red and house.

63
00:04:02.760 --> 00:04:05.480
Those are all the words that appear off the entire corpus.

64
00:04:05.490 --> 00:04:07.640
So what we end up doing is a simple word count.

65
00:04:07.710 --> 00:04:13.710
So blue house gets translated as we check against the actual count of all the words available and we

66
00:04:13.710 --> 00:04:16.430
get something that looks like this is a vector's 0 1 1.

67
00:04:16.500 --> 00:04:22.000
So the word read shows up 0 times the word blue shows up one time and the word house shows up one time.

68
00:04:22.140 --> 00:04:27.620
Then for Red House well we know red shows up one time blue shows up 0 times and house shows up one time.

69
00:04:27.720 --> 00:04:32.670
So we can already get an idea of how we could use word counts to convert words to an actual vector of

70
00:04:32.670 --> 00:04:33.460
numbers.

71
00:04:33.750 --> 00:04:36.840
And now we have a document represented as a vector of word counts.

72
00:04:36.840 --> 00:04:39.500
And the general idea again is called bag of words.

73
00:04:39.630 --> 00:04:43.070
Essentially you just have an entire bag full of words and you just counting it.

74
00:04:43.260 --> 00:04:46.300
So again now blue house and Red House are these vectors.

75
00:04:46.320 --> 00:04:49.140
So now they're vectors in an end dimensional space.

76
00:04:49.140 --> 00:04:51.520
In this case just three dimensional space.

77
00:04:51.600 --> 00:04:56.780
And what we can do is you can compare vectors of cosigned similarity and there is a formula right there.

78
00:04:56.790 --> 00:05:02.280
And essentially you're just comparing how close are these two vectors in this three them in space.

79
00:05:02.280 --> 00:05:05.830
And if you have a lot of words then it's an end dimensional space right.

80
00:05:05.850 --> 00:05:10.980
So now what's really cool as you can use mathematics to actually compare well how similar are these

81
00:05:10.980 --> 00:05:13.900
documents given the actual word counts.

82
00:05:14.250 --> 00:05:19.560
And we can actually improve on bag of words by justing word counts based on their frequency in the corpus

83
00:05:19.620 --> 00:05:21.100
or the group of all documents.

84
00:05:21.180 --> 00:05:24.560
And this is known as term frequency inverse document frequency.

85
00:05:24.600 --> 00:05:28.100
There are some issues if you just use a pure bag of word approach.

86
00:05:28.230 --> 00:05:31.720
While it is a great basic approach you can actually improve on this a lot.

87
00:05:32.830 --> 00:05:35.440
So we end up doing is we take these two ideas.

88
00:05:35.440 --> 00:05:37.930
Term frequency and inverse docking of frequency.

89
00:05:38.170 --> 00:05:42.780
So the term frequency applies the importance of the term within that document.

90
00:05:42.790 --> 00:05:49.580
So we have this T.F. of X and Y and that's the number of occurrences of the term X in document y.

91
00:05:49.600 --> 00:05:54.830
So for example if you have a document where the word house appears a ton of times.

92
00:05:54.940 --> 00:05:59.290
What's really interesting is that you have this term frequency so you know that this particular document

93
00:05:59.560 --> 00:06:03.400
has the word house a lot in relation to the actual document.

94
00:06:03.550 --> 00:06:05.020
Total number of words.

95
00:06:05.050 --> 00:06:10.210
Now the issue of just doing term frequency by itself is sometimes you'll have a really long document

96
00:06:10.410 --> 00:06:12.430
and other times you have a really short document.

97
00:06:12.580 --> 00:06:16.520
So you want to somehow take the actual document length into account.

98
00:06:16.540 --> 00:06:19.000
And that's where inversed document frequency comes in.

99
00:06:19.030 --> 00:06:22.180
So that's the importance of the term throughout the entire corpus.

100
00:06:22.180 --> 00:06:29.530
That is all the documents so we'll say Inv. and frequency of T is equal to log of and over DFA of x

101
00:06:29.530 --> 00:06:35.670
where n is that total number of documents and if x is the number of documents with the term.

102
00:06:35.680 --> 00:06:38.300
So what does this actually look like when we see this mathematically.

103
00:06:38.470 --> 00:06:42.170
Well term frequency inverse document frequencies that expressed like this.

104
00:06:42.370 --> 00:06:50.350
So we have the frequency of x in y multiplied by the log of the total number of documents divided by

105
00:06:50.650 --> 00:06:53.330
the number of documents containing that specific term.

106
00:06:53.470 --> 00:06:59.010
And this is the term frequency inverse document frequency of the term X within document Y.

107
00:06:59.020 --> 00:06:59.540
All right.

108
00:06:59.740 --> 00:07:02.490
So definitely check out the reading of the Wikipedia article.

109
00:07:02.500 --> 00:07:04.840
It has a much more in-depth explanation.

110
00:07:04.960 --> 00:07:11.050
Well you should be really getting out of this as far as bird's eye view is the fact that we're essentially

111
00:07:11.140 --> 00:07:17.350
using some sort of workout device and then taking into account how often that particular term shows

112
00:07:17.350 --> 00:07:23.110
up in the document and then using inversed documented frequency to take into account the actual differences

113
00:07:23.110 --> 00:07:25.650
between document links.

114
00:07:25.660 --> 00:07:31.400
OK so luckily Sparke has a lot of paice sparked the M-L thought feature tools to help out of this entire

115
00:07:31.400 --> 00:07:33.230
process and make it easy for you.

116
00:07:33.230 --> 00:07:37.400
You never actually have to do any of that mathematics at all happens behind the scenes and you can easily

117
00:07:37.400 --> 00:07:40.900
call some feature vectorize or take care of all that for you.

118
00:07:40.940 --> 00:07:48.550
So we're going to do is jump into a custom code example that detects spam versus ham or good text notifications

119
00:07:48.560 --> 00:07:54.100
so we're going to jump into all of that and we'll walk through its entire process one step at a time.

120
00:07:54.110 --> 00:07:58.220
So let's jump to the next lecture where we go and open up a new chapter notebook and create our own

121
00:07:58.220 --> 00:07:59.890
spam detection model.

122
00:07:59.900 --> 00:08:01.460
Thanks and I'll see you at the next lecture.
