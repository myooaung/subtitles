WEBVTT
1
00:00:05.480 --> 00:00:06.080
Hello everyone.

2
00:00:06.080 --> 00:00:11.060
Welcome back to part two of tools for natural language processing in this lecture we're going to finally

3
00:00:11.060 --> 00:00:15.580
finish off with discussing term frequency inverse document frequency of spark.

4
00:00:15.740 --> 00:00:18.520
And then we'll also discuss count vectorization spark.

5
00:00:18.590 --> 00:00:20.840
So let's go to the Jupiter notebook and get started.

6
00:00:21.950 --> 00:00:25.260
OK so here I am at the Jupiter notebook I've already started the sparks session.

7
00:00:25.280 --> 00:00:28.830
And let's begin with term frequency inverse document frequency.

8
00:00:28.830 --> 00:00:35.150
And remember that's basically a feature of vectorization used with text to reflect the importance of

9
00:00:35.150 --> 00:00:38.360
a term to a document in the corpus itself.

10
00:00:38.360 --> 00:00:44.680
So what I'm going to do is start off by saying from Paice sparked the M-L that feature import.

11
00:00:45.030 --> 00:00:48.370
And I'm going to import hashing T.F. for term frequency.

12
00:00:48.510 --> 00:00:52.940
An idea for inversed document frequency as well as tokenizer.

13
00:00:53.160 --> 00:00:57.750
And then I'm going to copy and paste the sentence data frame from the notes which just looks like this

14
00:00:57.900 --> 00:01:00.690
is actually a data frame that we've worked with before.

15
00:01:00.690 --> 00:01:05.920
Now let's just run this and let's show what it looks like since it's data show.

16
00:01:06.360 --> 00:01:10.880
And I can see here you have the label of the document it belongs to as well as the sentence.

17
00:01:10.890 --> 00:01:16.120
So what we're to do next is actually tokenize this which you've seen already before.

18
00:01:16.350 --> 00:01:24.250
So we create our tokenizer object and that's going to be equal to tokenizer where my input column is

19
00:01:24.250 --> 00:01:33.090
the sentence and my output column loops my outs put column is words.

20
00:01:33.130 --> 00:01:34.300
So that's my tokenizer.

21
00:01:34.310 --> 00:01:41.980
And now let's say words data is equal to that tokenizer just created and then I'm going to transform

22
00:01:42.040 --> 00:01:46.240
the actual data from itself which in this case was sentence data.

23
00:01:47.140 --> 00:01:47.610
OK.

24
00:01:47.810 --> 00:01:53.790
Now let's actually show it words data looks like my transformed data frame and I see here I have the

25
00:01:53.790 --> 00:01:55.550
sentence and the words themselves.

26
00:01:55.550 --> 00:02:00.650
And if I wanted to see everything I could always just say truncate is equal to false and that will kind

27
00:02:00.650 --> 00:02:07.080
of expand this to see everything but we'll just show the actual frame itself with the dot dot dot or

28
00:02:07.100 --> 00:02:08.620
ellipses notation.

29
00:02:08.630 --> 00:02:11.760
Now the next thing you want to do is actually grab the term frequency.

30
00:02:11.930 --> 00:02:18.440
So what I will do is say hashing T.F. is equal to an instance of that hashing T.F. and I need to specify

31
00:02:18.440 --> 00:02:27.010
that my input column is those tokens the words and also say my output column is output column.

32
00:02:27.320 --> 00:02:31.630
And I'm going to call this raw features that kind of matches up the actual documentation.

33
00:02:33.440 --> 00:02:39.670
So once we have that hashing T.F. object I'm going to create something called feature sized data that

34
00:02:39.670 --> 00:02:41.110
don't exactly spell that right.

35
00:02:42.290 --> 00:02:50.150
So a feature is data and then we'll say equals two hashing T.F. and then I'm going to transform the

36
00:02:50.150 --> 00:02:53.450
words data.

37
00:02:53.470 --> 00:03:00.090
Ok so now that I have that featuritis data what I can do is apply the inverse document frequency.

38
00:03:00.310 --> 00:03:07.180
So I'll create an object IDF that's equal to IDF with the input column is now going to be those raw

39
00:03:07.180 --> 00:03:07.990
features.

40
00:03:11.030 --> 00:03:18.800
Features and then the output column is just going to be features because usually by this stage you know

41
00:03:18.800 --> 00:03:26.660
you kind of towards the end so will say up column is equal to features then I call by an IDF model and

42
00:03:26.660 --> 00:03:30.590
say IDF fit onto that featuritis data.

43
00:03:30.590 --> 00:03:36.170
So T.F. IDF kind of has these two phases the term frequency part and then the inverse document frequency

44
00:03:36.170 --> 00:03:37.000
part.

45
00:03:37.430 --> 00:03:45.370
And then once we have our IDF model I can transform the features data so we'll call this rescaled data

46
00:03:46.090 --> 00:03:54.550
and then we set this equal to the IDF model and we're going to transform the featuritis data and this

47
00:03:54.550 --> 00:03:59.170
kind of workflow almost feels like similar to when we were doing Kamins clustering where we fit and

48
00:03:59.170 --> 00:04:00.780
then transform that everything.

49
00:04:01.150 --> 00:04:09.220
So now shakha our rescaled data and I'm just going to select the label column and then the features

50
00:04:09.220 --> 00:04:15.320
column and then say that show and you can see here I have my features.

51
00:04:15.320 --> 00:04:18.140
So if we say truncate false

52
00:04:20.890 --> 00:04:27.490
and run this you can see here I've been able to transform those words into a term frequency inverse

53
00:04:27.490 --> 00:04:29.170
document frequency.

54
00:04:29.170 --> 00:04:31.430
Essentially what is a large array.

55
00:04:31.720 --> 00:04:35.990
And now they're ready to go for any machine learning algorithm to deal with.

56
00:04:36.010 --> 00:04:37.600
We have label and features.

57
00:04:37.600 --> 00:04:40.540
So this is ready for any supervised learning algorithm.

58
00:04:40.540 --> 00:04:42.570
Now let's show you count vectorizing.

59
00:04:42.580 --> 00:04:47.800
Moving on from TFN IDF count vectorize are as well as the Count vectorize or model.

60
00:04:47.950 --> 00:04:54.750
What they do is the aim to help convert a collection of documents into vectors of word counts.

61
00:04:54.850 --> 00:05:03.280
So something like a tokenization so well one that doing say from Paice spark thought M-L up feature

62
00:05:04.140 --> 00:05:04.820
import.

63
00:05:04.930 --> 00:05:10.300
And then I'm going to import count vector Isar and I'm going to copy and paste the actual data frame.

64
00:05:10.330 --> 00:05:15.520
And again this is actually from the documentation Billett show this data frame that I just copied and

65
00:05:15.520 --> 00:05:20.830
pasted from the notes that you can grab as well is I have an ID and then some words.

66
00:05:20.980 --> 00:05:27.960
And then I have ABC and in a BBCA I want to go into do is create a count vectorize or objects just call

67
00:05:27.960 --> 00:05:38.760
it C-v count victimiser I'll specify that my input column is going to be words my output column is going

68
00:05:38.760 --> 00:05:40.730
to be called features.

69
00:05:41.190 --> 00:05:43.980
And then I can actually indicate a vocabulary size.

70
00:05:43.980 --> 00:05:49.620
So what's the maximum number of available vocabulary words I want in this case.

71
00:05:49.620 --> 00:05:58.560
I could just count them so I have a B and C so I could just say as an argument vocab size is equal to

72
00:05:58.560 --> 00:05:59.230
three.

73
00:05:59.370 --> 00:06:07.310
If you do shift tab here on the count vector Isar and come down the vocab size by default is 206 2144.

74
00:06:07.470 --> 00:06:12.100
And that should be around I think 2 to the power of 20 or 10.

75
00:06:12.120 --> 00:06:14.210
And then we can also specify a minimum.

76
00:06:14.210 --> 00:06:21.160
The Fiesole say the F is equal to 2.0 So this minimum the F that's an optional parameter.

77
00:06:21.210 --> 00:06:26.760
And what this does it affects that fitting process by specifying the minimum number of documents.

78
00:06:26.760 --> 00:06:30.040
A term must appear in to be included in the vocabulary.

79
00:06:30.120 --> 00:06:36.540
So if one term only appears in one document and the minimum DMF had that cut off then it wouldn't appear

80
00:06:36.690 --> 00:06:38.070
in the count victimiser.

81
00:06:38.370 --> 00:06:42.480
So you can do that if you have a bunch of esoteric words all around and you think that's going to mess

82
00:06:42.480 --> 00:06:45.690
up your actual natural language processing.

83
00:06:45.700 --> 00:06:50.550
But what I'm going to do now is say model is equal to CV fit and in order to fit it to the data frame

84
00:06:50.550 --> 00:06:55.250
it was created and then I'm going to say results as equal to model.

85
00:06:55.440 --> 00:07:02.350
Transform and then I'm going to transform my data frame and then I'm going to show the result let's

86
00:07:02.360 --> 00:07:04.370
actually say truncate is equal to false here.

87
00:07:04.370 --> 00:07:07.700
So when I see the whole thing and there we go we have our words.

88
00:07:07.730 --> 00:07:11.410
And now we have our count vectorization models ready to go.

89
00:07:11.450 --> 00:07:18.550
So you can see here how they kind of relate to a B and C so a appears once in the first document.

90
00:07:18.550 --> 00:07:21.540
So it's right there 1.0 a appears twice.

91
00:07:21.550 --> 00:07:27.460
So at the end than at the beginning of the second document so we say two counts for a B only appears

92
00:07:27.460 --> 00:07:28.140
once.

93
00:07:28.240 --> 00:07:31.220
And then C only appears once in the first document.

94
00:07:31.360 --> 00:07:35.980
And we can see B appears twice here 2.0 and c appears once one point.

95
00:07:36.190 --> 00:07:40.180
And this is essentially that bag of words method that we were discussing earlier.

96
00:07:40.180 --> 00:07:45.400
And we can actually use count vectorization along with T.F. and IDF.

97
00:07:45.640 --> 00:07:46.220
OK.

98
00:07:46.330 --> 00:07:50.980
So hopefully this gave you an idea of how to use count vectorize for as well as how to use T.F. and

99
00:07:51.040 --> 00:07:56.500
IDF along with tokenization what we're gonna do next is heads straight to the custom code along project

100
00:07:56.790 --> 00:08:01.330
where we're going to combine all these tools that we've been learning to actually build a spam detection

101
00:08:01.330 --> 00:08:02.100
filter.

102
00:08:02.380 --> 00:08:04.250
Thanks and I'll see you at the next lecture.
