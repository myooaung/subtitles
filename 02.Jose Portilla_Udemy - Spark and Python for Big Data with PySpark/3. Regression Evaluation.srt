1
00:00:05,310 --> 00:00:09,850
Hello everyone and welcome to this lecture on evaluating regression.

2
00:00:09,920 --> 00:00:14,560
Let's take a quick break from our coding lecturers to discuss evaluating regression models.

3
00:00:14,600 --> 00:00:19,310
Since we saw some evaluation metrics in the previous lecture but didn't really touch upon what they

4
00:00:19,310 --> 00:00:24,620
actually stand for or what they mean and what we discuss in this lecture is not just for linear regression

5
00:00:24,860 --> 00:00:30,050
but any model that attempts to predict continuous values and categorical values which is a classification

6
00:00:30,050 --> 00:00:32,030
task.

7
00:00:32,040 --> 00:00:36,810
So as you been studying machine learning you may have heard of some evaluation metrics like accuracy

8
00:00:36,840 --> 00:00:42,690
or recall these sort of metrics aren't useful for regression problems because we need metrics designed

9
00:00:42,690 --> 00:00:46,760
specifically for continuous values.

10
00:00:46,820 --> 00:00:50,750
So let's go ahead and discuss some of the most common evaluation metrics for regression.

11
00:00:50,930 --> 00:00:54,340
And this includes mean absolute error mean square error.

12
00:00:54,470 --> 00:01:00,320
The root mean square error and R-squared values in R-squared values are exactly the evaluation metric.

13
00:01:00,320 --> 00:01:02,790
It's more of a statistical property of your model.

14
00:01:02,900 --> 00:01:08,490
And we'll talk about that towards the end of this lecture so we'll begin with the simplest one which

15
00:01:08,490 --> 00:01:11,730
has mean absolute error otherwise known as an A.

16
00:01:12,000 --> 00:01:15,470
This is just the mean of the absolute value of the errors.

17
00:01:15,540 --> 00:01:19,090
So it's the easiest to understand because it's basically just the average error.

18
00:01:19,140 --> 00:01:20,520
And this is what the equation looks like.

19
00:01:20,520 --> 00:01:21,820
So let's break it down.

20
00:01:21,960 --> 00:01:27,750
Starting from the left hand side we have one over N and then we have sigma from equals 1 all the way

21
00:01:27,750 --> 00:01:32,260
to end which is just another way of saying take the sum and then divide it by N.

22
00:01:32,340 --> 00:01:38,400
Now taking the sum of a array of numbers and then dividing it by the count of all those numbers is just

23
00:01:38,400 --> 00:01:39,570
taking the average.

24
00:01:39,570 --> 00:01:43,800
So essentially all the left hand side is saying is just take the average.

25
00:01:43,890 --> 00:01:45,430
Now take the average of what.

26
00:01:45,480 --> 00:01:50,370
So we go to the right hand side and we see we have absolute value bars and we're essentially just taking

27
00:01:50,370 --> 00:01:54,990
the average of the true value minus the predicted value.

28
00:01:54,990 --> 00:01:56,430
So what does this actually mean.

29
00:01:56,610 --> 00:01:59,340
Well we're just trying to see what was or average error.

30
00:01:59,400 --> 00:02:02,960
And the reason we have those absolute value bars is because of favoring aggression.

31
00:02:02,970 --> 00:02:07,070
Fit line your prediction may be below or above the true value.

32
00:02:07,080 --> 00:02:13,230
So you may be negative or you may be positive since we don't want the negatives to cancel out the positive

33
00:02:13,260 --> 00:02:17,020
errors when we actually take the average we take the absolute value first.

34
00:02:17,100 --> 00:02:19,260
So we basically have the average error.

35
00:02:19,290 --> 00:02:22,770
So what would this actually look like in a more realistic scenario.

36
00:02:22,950 --> 00:02:28,350
Well let's imagine again to our common continuous problem where we're trying to predict the possible

37
00:02:28,380 --> 00:02:29,910
price of a house.

38
00:02:29,940 --> 00:02:35,610
So your average error may be something like you're off by a hundred thousand dollars on average.

39
00:02:35,610 --> 00:02:37,000
So it does mean absolute error.

40
00:02:37,020 --> 00:02:39,660
Actually look like a realistic example.

41
00:02:39,750 --> 00:02:45,960
Well let's imagine back to that classic problem of trying to predict housing prices based off the features

42
00:02:45,960 --> 00:02:46,940
of the house.

43
00:02:47,190 --> 00:02:54,210
So your mean absolute error may sound something like on average you're off by $100000 on the price of

44
00:02:54,210 --> 00:02:55,140
a house.

45
00:02:55,170 --> 00:03:00,060
So that's all the mean absolute area does it just tells you that on average how far off are you on the

46
00:03:00,060 --> 00:03:05,450
House now in trying to improve on that we have the mean squared error.

47
00:03:05,480 --> 00:03:08,010
So this is the mean of the squared errors.

48
00:03:08,150 --> 00:03:11,600
So we still want to try to retain some sort of positive error.

49
00:03:11,600 --> 00:03:15,740
So instead of taking the absolute value when we end up doing is we square it.

50
00:03:15,950 --> 00:03:23,090
And the larger errors are noted more than with the mean absolute error making MSE more popular.

51
00:03:23,090 --> 00:03:24,300
So what does that actually mean.

52
00:03:24,320 --> 00:03:29,870
Well you can imagine that if you have a very large difference between your predictive value and your

53
00:03:29,870 --> 00:03:35,720
true value then when you square it that difference is going to be essentially punished more you're going

54
00:03:35,720 --> 00:03:41,300
to see the error even more when you take that average which is why MSEE is more popular because it takes

55
00:03:41,300 --> 00:03:45,140
into account when you were off by a very large margin.

56
00:03:45,140 --> 00:03:51,080
Now the issue with this are the units you're no longer in the same units as the why you were trying

57
00:03:51,080 --> 00:03:51,730
to predict.

58
00:03:51,830 --> 00:03:55,870
You are now in the units of squared or Y which isn't super helpful.

59
00:03:56,090 --> 00:03:59,570
So we end up doing is we take the root mean absolute error.

60
00:03:59,570 --> 00:04:05,680
So this is essentially the most popular because it takes into effect the actual squared error values.

61
00:04:05,690 --> 00:04:08,560
So it still has that punishing effect on really large errors.

62
00:04:08,660 --> 00:04:13,470
But it also has the nice value of being in the same units as wide.

63
00:04:13,490 --> 00:04:17,360
Which is why we take a square root at the very end of all that.

64
00:04:17,360 --> 00:04:19,060
So root mean absolute error.

65
00:04:19,070 --> 00:04:23,560
Our MSEE is the most popular error metric for regression models.

66
00:04:23,630 --> 00:04:29,030
However hopefully you should have an understanding now of all three of those error metrics and in different

67
00:04:29,030 --> 00:04:34,370
cases some are really more useful than others are MSE is the most popular for the reasons I previously

68
00:04:34,370 --> 00:04:35,210
stated.

69
00:04:36,060 --> 00:04:38,020
Now let's discuss R-squared values.

70
00:04:38,070 --> 00:04:41,020
This is also known as the coefficient of determination.

71
00:04:41,040 --> 00:04:46,680
It's not quite in error metric because we saw from the previous era metrics it's more of just a statistical

72
00:04:46,680 --> 00:04:48,530
measure of your regression model.

73
00:04:48,990 --> 00:04:55,440
As a quick know by itself just knowing only the R-squared value won't really tell you the whole story

74
00:04:55,440 --> 00:04:58,970
of how well your model is performing in a very basic sense.

75
00:04:58,980 --> 00:05:04,290
It's just a measure of how much variance your model accounts for and it's always going to be between

76
00:05:04,290 --> 00:05:07,410
0 and 1 or 0 percent to 100 percent.

77
00:05:07,410 --> 00:05:13,920
So an example would be your model has a point nine value for our square meaning explains 90 percent

78
00:05:14,010 --> 00:05:15,930
of the variance of the data.

79
00:05:16,140 --> 00:05:20,970
There's also different ways of obtaining R-squared such as adjusted r squared and some are squared methods

80
00:05:20,970 --> 00:05:23,750
can actually yield a negative R-squared value.

81
00:05:23,880 --> 00:05:28,370
So keep that in mind as you kind of explore machine learning on your own and you see a negative R-squared

82
00:05:28,380 --> 00:05:33,540
value they may have calculated it using a different metric and the Wikipedia page for R-squared value

83
00:05:34,020 --> 00:05:39,140
has really nice details on the different methods that you can calculate R-squared.

84
00:05:39,260 --> 00:05:43,720
Now a full analysis and explanation of interpreting R-Squared is outside the scope of this course.

85
00:05:43,820 --> 00:05:47,390
But check out the resource links for more information on this topic.

86
00:05:47,420 --> 00:05:48,940
What I really want you to know about R-squared.

87
00:05:48,950 --> 00:05:52,650
It's basically just a measurement of how much variance your model explains.

88
00:05:52,650 --> 00:05:55,500
There's different types of R-squared such as that just that are squared.

89
00:05:55,520 --> 00:05:59,930
It's really useful when you want to compare models to each other but you take in mind that you should

90
00:05:59,930 --> 00:06:03,160
compare models and adjust that are squared not a normal or square.

91
00:06:03,200 --> 00:06:04,810
And there's a lot more to this.

92
00:06:04,820 --> 00:06:10,610
It's not as straightforward as just saying oh I have a point nine R-squared value which means my model

93
00:06:10,610 --> 00:06:11,730
is great.

94
00:06:11,730 --> 00:06:16,760
There's more subtleties to it than that but that whole discussion is outside the scope of this course.

95
00:06:16,760 --> 00:06:19,050
It would take a really long lecture to discuss all that.

96
00:06:19,100 --> 00:06:22,080
So I put in resources here just some links for you to read.

97
00:06:22,160 --> 00:06:27,030
In case you're interested in finding out more about those R-squared features.

98
00:06:27,050 --> 00:06:27,710
All right.

99
00:06:27,710 --> 00:06:32,240
So again keep in mind the R-squared can enhance your understanding of a model or help you compare models

100
00:06:32,420 --> 00:06:35,420
but it shouldn't be your sole source for evaluating a model.

101
00:06:35,660 --> 00:06:37,750
Now let's continue on to our code example.

102
00:06:37,760 --> 00:06:42,380
We're going to walk you through a more realistic code example for use in linear regression with Python

103
00:06:42,440 --> 00:06:43,550
and Sparke.

104
00:06:43,550 --> 00:06:45,300
Thanks I'll see at the next lecture.
