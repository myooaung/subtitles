WEBVTT
1
00:00:06.730 --> 00:00:12.210
Welcome to our first section on the machine learning half of this course linear regression the linear

2
00:00:12.210 --> 00:00:14.700
regression section has the following lectures.

3
00:00:14.700 --> 00:00:18.140
We have a theory overview lecture which is this lecture continued on.

4
00:00:18.150 --> 00:00:22.980
Then after that we'll have a documentation example lecture where we'll actually walk through the documentation's

5
00:00:23.040 --> 00:00:28.410
own example of using linear regression of python and Sparke then we have our own custom code example

6
00:00:28.590 --> 00:00:33.210
which is a little bit more of a realistic take on some more real estate data and actually applying linear

7
00:00:33.210 --> 00:00:36.240
regression with Python and Sparke on that data.

8
00:00:36.240 --> 00:00:40.710
At the end we'll have a consulting project exercise which we'll discuss in more detail when we get to

9
00:00:40.710 --> 00:00:43.740
that lecture as a quick note.

10
00:00:43.790 --> 00:00:48.350
If you've already gone through another course of mine that has machine learning topics or maybe you've

11
00:00:48.350 --> 00:00:52.790
already read about machine learning and linear regression theory in general you may have already seen

12
00:00:52.790 --> 00:00:53.870
this material already.

13
00:00:53.870 --> 00:00:55.240
Be comfortable with the theory.

14
00:00:55.430 --> 00:01:00.880
If you are feel free to skip the rest of this the theory of linear regression hasn't changed in a while.

15
00:01:01.120 --> 00:01:06.650
All right as I suggested reading assignment I would recommend reading chapters 2 and 3 of an introduction

16
00:01:06.650 --> 00:01:09.950
to mystical learning by Gareth James.

17
00:01:10.010 --> 00:01:14.340
Let's continue on with the very basics by starting the history of linear regression.

18
00:01:14.390 --> 00:01:19.460
So this entire idea all started in the 1800s with a man named Francis Galton.

19
00:01:19.540 --> 00:01:22.580
Galton was studying the relationship between parents and their children.

20
00:01:22.790 --> 00:01:27.200
In particular he investigated the relationship between the heights of fathers and the heights of their

21
00:01:27.200 --> 00:01:33.640
sons when he discovered was that a man's son tended to be roughly as tall as his father.

22
00:01:33.640 --> 00:01:35.650
However this was Galton's breakthrough.

23
00:01:35.730 --> 00:01:40.880
It was at a son's height tended to be closer to the overall average height of all people.

24
00:01:41.080 --> 00:01:42.750
Let's take a look at an example.

25
00:01:43.060 --> 00:01:47.280
Shaquille O'Neal is a basketball player otherwise known as Shaq and Shaq is really tall.

26
00:01:47.290 --> 00:01:50.110
He's 7 feet 1 inch or 2.2 metres.

27
00:01:50.260 --> 00:01:55.690
If Shaquille O'Neal has a son chances are that sun's going to be pretty tall too over Shaq is such an

28
00:01:55.690 --> 00:02:00.580
anomaly in his height that there's also a very good chance that his son will not be as tall as Shaq

29
00:02:01.450 --> 00:02:06.190
and it turns out this is the case so Shaq does have a son and the son is pretty tall six feet seven

30
00:02:06.190 --> 00:02:06.790
inches.

31
00:02:06.790 --> 00:02:12.400
But that's not nearly as tall as that at 7 foot 1 and Galton calls this sort of phenomenon regression

32
00:02:12.720 --> 00:02:14.170
as in a father son.

33
00:02:14.260 --> 00:02:19.400
Height tends to regress or drift towards the mean or average height.

34
00:02:19.530 --> 00:02:22.970
Let's take a really simple example in fact the simplest example possible.

35
00:02:23.190 --> 00:02:28.880
Calculating a regression with only two data points so we're trying to do when we calculate a regression

36
00:02:28.880 --> 00:02:34.370
line is draw a line that's as close to everything as possible for classically any regression or the

37
00:02:34.370 --> 00:02:35.990
least squares method.

38
00:02:36.020 --> 00:02:40.070
What you're trying to do is only measure the closeness in this up and down the section or along the

39
00:02:40.070 --> 00:02:41.060
y axis.

40
00:02:42.450 --> 00:02:47.580
So in this case wouldn't it be great if we could apply the same concept to a graph with more than just

41
00:02:47.580 --> 00:02:48.710
two data points.

42
00:02:48.710 --> 00:02:53.880
If we go back a slide we can see that this particular regression fits the data perfectly.

43
00:02:53.880 --> 00:02:57.000
However if you have more data points how can we actually do this.

44
00:02:57.030 --> 00:03:02.550
Well to take the same concept we could take multiple men and their sons heights and do things like tell

45
00:03:02.550 --> 00:03:06.060
a man how tall we expect his son to be before he even has a son.

46
00:03:06.060 --> 00:03:11.220
Now if we look at this plot we can see for this particular example we have a perfectly fitted line.

47
00:03:11.340 --> 00:03:15.430
We've created a regression line that fits those two points perfectly.

48
00:03:15.560 --> 00:03:20.420
However wouldn't it be great if we could apply this same concept to a graph with more than just two

49
00:03:20.420 --> 00:03:21.410
data points.

50
00:03:21.410 --> 00:03:26.390
By doing this we could take multiple men and their sons heights and do things like tell a man how tall

51
00:03:26.390 --> 00:03:29.700
we expect his son to be before he even has a son.

52
00:03:29.780 --> 00:03:34.790
And if you look at this plot on the x axis we have a father's height and on the y axis we have a son's

53
00:03:34.790 --> 00:03:36.470
height and then various data points.

54
00:03:36.560 --> 00:03:41.300
And you can probably already tell just by looking at this plot how a regression line would look on top

55
00:03:41.300 --> 00:03:41.710
of it.

56
00:03:42.820 --> 00:03:47.560
So our goal with linear regression is to minimize that vertical distance between all the data points

57
00:03:47.650 --> 00:03:48.740
and our line.

58
00:03:49.000 --> 00:03:53.860
So in determining the baseline we are attempting to minimize that distance between all the points and

59
00:03:53.860 --> 00:03:55.410
the distance to our line.

60
00:03:56.420 --> 00:03:58.310
Now there's lots of ways to minimize this.

61
00:03:58.310 --> 00:04:00.220
And this is basically where that mask comes in.

62
00:04:00.260 --> 00:04:04.310
Some of these different methods that you may have heard of before called some of squared errors some

63
00:04:04.310 --> 00:04:06.120
of absolute errors cetera.

64
00:04:06.290 --> 00:04:11.220
But all of these methods have the same general goal of minimizing that distance.

65
00:04:11.240 --> 00:04:15.520
So for example one of the most popular methods is the least squares method.

66
00:04:15.560 --> 00:04:19.110
So here we have some blue data points along an x and y axis.

67
00:04:19.900 --> 00:04:23.000
Now we try to fit a linear regression to the line.

68
00:04:23.020 --> 00:04:28.030
However the question remains how do we decide which line is the best fitting one in order to decide

69
00:04:28.030 --> 00:04:34.090
which line fits this data the best we can use the least squares method which is fitted by minimizing

70
00:04:34.090 --> 00:04:39.880
the sum of squares of the residuals where the residuals for an observation is the difference between

71
00:04:39.880 --> 00:04:43.990
the observation itself that y value versus the fitted line.

72
00:04:43.990 --> 00:04:48.500
So essentially the residual there are those red lines in this plot.

73
00:04:48.500 --> 00:04:53.480
Now let's get an idea of how to implement this idea with Paice spark will ease into all of this by checking

74
00:04:53.480 --> 00:04:56.380
out the simpler documentation example first.

75
00:04:56.390 --> 00:05:00.560
Thanks and I'll see in the next lecture where we actually dive straight into this documentation example

76
00:05:00.740 --> 00:05:01.940
using the Jupiter notebook.
