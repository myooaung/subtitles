WEBVTT
1
00:00:05.970 --> 00:00:11.060
Welcome to the documentation coding example for the linear regression section of the course.

2
00:00:11.250 --> 00:00:15.840
While it may not be super interesting understanding how to read and re-applied documentation examples

3
00:00:16.080 --> 00:00:18.900
will rapidly speed up your own learning process.

4
00:00:18.900 --> 00:00:23.850
Let's walk through the linear regression documentation example together in the browser I'm about to

5
00:00:23.850 --> 00:00:24.320
jump to.

6
00:00:24.330 --> 00:00:26.050
I'm going to have several tabs open.

7
00:00:26.100 --> 00:00:28.490
Those are going to include the documentation page.

8
00:00:28.650 --> 00:00:34.320
The data page from the documentation linear regression example the IPY and B that's the notebook file

9
00:00:34.470 --> 00:00:36.370
for what we're actually going to be going through.

10
00:00:36.480 --> 00:00:40.880
And then they knew untitled notebook which is where I'm going to be coding in for this lecture.

11
00:00:40.890 --> 00:00:43.970
Let's hop over to the browser and get started.

12
00:00:44.170 --> 00:00:44.490
All right.

13
00:00:44.500 --> 00:00:46.260
So here I am at M-L guy.

14
00:00:46.270 --> 00:00:49.720
This is the machine learning library documentation Guide page.

15
00:00:49.750 --> 00:00:54.970
If you come over here to classification or regression and click on that you'll be taken to this page

16
00:00:55.060 --> 00:00:57.950
and then scroll down until you find the linear regression.

17
00:00:58.150 --> 00:01:01.870
Click on that and this is the documentation example we're going to be working through.

18
00:01:01.870 --> 00:01:03.280
Click over here on Python.

19
00:01:03.520 --> 00:01:05.660
There it is the documentation example.

20
00:01:05.680 --> 00:01:11.170
It's pretty simple so the main idea here is to try to get an idea of the workflow for dealing with machine

21
00:01:11.170 --> 00:01:12.980
learning algorithms with SPARC.

22
00:01:13.090 --> 00:01:20.160
And we're also going to expand on this documentation example just a little bit if you come to in your

23
00:01:20.160 --> 00:01:25.200
notes for the course Python SPARC for big data SPARC for machine learning and then the linear regression

24
00:01:25.200 --> 00:01:29.730
folder the notebook that has everything that we're going to be working through is linear regression

25
00:01:29.780 --> 00:01:31.680
exampled IPY and B.

26
00:01:31.740 --> 00:01:33.100
So that's over here.

27
00:01:33.390 --> 00:01:37.470
If I click it open you can see that there are some text notes and then all the code in case you ever

28
00:01:37.470 --> 00:01:41.250
want to reference anything that we're doing you can just copy and paste from this notebook.

29
00:01:41.520 --> 00:01:45.850
And then if we come back here we also have the sample in the regression data text.

30
00:01:45.930 --> 00:01:48.290
That's the actual documentation text file.

31
00:01:48.390 --> 00:01:49.590
And I've already downloaded for you.

32
00:01:49.590 --> 00:01:50.410
Looks like this.

33
00:01:50.430 --> 00:01:53.000
So this is the actual text file We're going to be working with.

34
00:01:53.010 --> 00:01:57.580
So let's hop over to this new notebook that's untitled and get started.

35
00:01:57.700 --> 00:02:03.390
So the very first thing we need to do is actually start a session that will say from Paice spark that

36
00:02:03.390 --> 00:02:11.610
sequel import the spark session and let's create it will say Sparke session builder app and I'm just

37
00:02:11.610 --> 00:02:17.900
doing tab there to quickly autocomplete that I'll create one called L-R X and then I'm going to get

38
00:02:17.970 --> 00:02:22.260
or create that spark session hit run.

39
00:02:22.470 --> 00:02:26.370
And then whenever you're working with machine learning with my Sparke what you end up doing is you say

40
00:02:26.400 --> 00:02:33.870
from Paice park M-L dot and then if you hit tab here you essentially see the families of machine learning

41
00:02:33.930 --> 00:02:39.450
that is available to you so classification clustering evaluation pipelines et cetera we're going to

42
00:02:39.450 --> 00:02:44.770
be dealing with the regression family and then I'm going to import an actual regression algorithm if

43
00:02:44.870 --> 00:02:51.320
you hit tab you'll see various options here but we'll eventually just choose linear regression so run

44
00:02:51.320 --> 00:02:55.680
that the next step is to actually load in our training data.

45
00:02:55.760 --> 00:03:01.670
So I'm going to make training a new variable and they'll say Sparke read it.

46
00:03:01.940 --> 00:03:07.400
Now we've read in CXXVI files and Jason files we're going to say that format which is the generalized

47
00:03:07.400 --> 00:03:10.240
version instead of passing in V.

48
00:03:10.340 --> 00:03:14.480
The actual data is in this form called lib SVM.

49
00:03:14.480 --> 00:03:20.720
So if you look at the data the actual page I have it open on you can see here we have this lib SVM format.

50
00:03:20.720 --> 00:03:22.350
So this is the format itself.

51
00:03:22.360 --> 00:03:31.430
So that's where you have to read it this way and I'm going to say lib SVM and then load the actual text

52
00:03:31.430 --> 00:03:31.850
file.

53
00:03:31.910 --> 00:03:34.100
So sample any regression data.

54
00:03:34.100 --> 00:03:34.940
So we run that.

55
00:03:34.970 --> 00:03:38.170
And let's actually see what the data looks like.

56
00:03:40.230 --> 00:03:46.350
So we have a label column and a features column and this is the actual format Sparke needs in order

57
00:03:46.350 --> 00:03:48.810
to run a machine learning algorithm on it.

58
00:03:48.810 --> 00:03:52.630
Now in real life most of your data isn't already going to be conveniently formatted this way.

59
00:03:52.800 --> 00:03:54.450
But we'll discuss that in the next lecture.

60
00:03:54.450 --> 00:03:57.200
Will we do a code along through a more realistic dataset.

61
00:03:57.270 --> 00:04:02.130
But all the documentation examples come with data that's already formatted this way for you.

62
00:04:02.230 --> 00:04:07.800
But keep in mind that Slagle column needs to have some sort of numerical label and that will be either

63
00:04:07.800 --> 00:04:12.930
a regression numerical value or a numerical value that matches a classification grouping.

64
00:04:12.930 --> 00:04:18.180
The feature column has inside of a vector and that's a vector of all the features that belong to that

65
00:04:18.180 --> 00:04:18.780
row.

66
00:04:19.020 --> 00:04:23.760
Usually what we end up doing is combining the various feature columns in a realistic dataset into that

67
00:04:23.760 --> 00:04:28.150
single features column using the data transformations that we're going to be learning about later on.

68
00:04:29.110 --> 00:04:32.400
And let's continue working through this simple example.

69
00:04:32.400 --> 00:04:36.650
The next thing you want to do is actually create an instance of our model.

70
00:04:36.860 --> 00:04:42.890
So we say L-R is equal to linear regression and then close parentheses for the very basics.

71
00:04:42.890 --> 00:04:45.170
But I want to talk about three parameters.

72
00:04:45.170 --> 00:04:51.280
And those are the lips features call parameter which tells this model.

73
00:04:51.290 --> 00:04:54.170
What is the features column called now by default.

74
00:04:54.170 --> 00:04:59.450
This is just features that in case you forget that and you actually call this something else maybe call

75
00:04:59.450 --> 00:05:05.100
it my features or house features etc. you can just pass that in as a name to be expected.

76
00:05:05.120 --> 00:05:12.300
So default is features so I'll leave that in the label column name that we expect is label again that's

77
00:05:12.350 --> 00:05:14.220
the label column here.

78
00:05:14.890 --> 00:05:24.070
And then the last argument we can provide is the prediction column and what's the name of the actual

79
00:05:24.070 --> 00:05:24.720
predictions.

80
00:05:24.720 --> 00:05:27.270
So default is prediction.

81
00:05:27.460 --> 00:05:32.410
But again if you're dealing in something like housing prices you could say oh house price prediction

82
00:05:32.410 --> 00:05:33.090
here.

83
00:05:33.100 --> 00:05:37.900
So once we actually evaluate some data and it has some predictive values it will label that under a

84
00:05:37.900 --> 00:05:41.240
column called prediction so we'll run that.

85
00:05:41.560 --> 00:05:44.740
And then the next step is to actually just fit or train the model.

86
00:05:44.740 --> 00:05:47.240
So we will say our model.

87
00:05:47.260 --> 00:05:55.690
So that's just some variable and we'll say L-R fit and we're going to fit onto the training data and

88
00:05:55.690 --> 00:05:59.080
this should actually probably be snake casing but for now we'll just leave it that way.

89
00:06:00.550 --> 00:06:01.990
And that may take a little bit of time.

90
00:06:01.990 --> 00:06:06.910
The pinning on how Fasher computer is especially how fast the or how large the data is that we're working

91
00:06:06.910 --> 00:06:11.100
with is that here we have L.R. model and it's been fit to the training data.

92
00:06:11.110 --> 00:06:13.960
So now there's a lot of information we can grab off this model.

93
00:06:14.730 --> 00:06:19.920
So I can say L.R. model in fact actually grab the coefficients off the model does that the coefficient

94
00:06:19.920 --> 00:06:27.080
for that linear regression and I can also grab the intercept and say intercept and grab that intercept.

95
00:06:27.090 --> 00:06:30.960
So if in case you're interested in how these coefficients actually relate to the model sensor these

96
00:06:30.960 --> 00:06:35.940
coefficients correspond to a particular feature and you can do the reading an introduction to the learning

97
00:06:36.190 --> 00:06:40.470
case you want to learn more about the mathematics behind relating coefficient to something like feature

98
00:06:40.470 --> 00:06:41.760
importance.

99
00:06:41.760 --> 00:06:42.400
All right.

100
00:06:42.570 --> 00:06:47.450
Now let's talk about the summary attribute which contains even more information.

101
00:06:47.700 --> 00:06:55.620
For this I'll create a variable called training summary and I will set it equal to the summary attribute

102
00:06:55.780 --> 00:06:57.530
on this machine learning model.

103
00:06:57.900 --> 00:07:04.880
And from this training summary I can call lots of attributes and methods that are really useful.

104
00:07:05.070 --> 00:07:11.070
So for example I can call the R-squared value where R-squared just explains or tells you how much variance

105
00:07:11.070 --> 00:07:12.750
is explained by your model.

106
00:07:12.750 --> 00:07:13.920
This data is kind of wacky.

107
00:07:13.920 --> 00:07:18.690
So we actually don't get every great art squared value but keep that in mind that that's available to

108
00:07:18.690 --> 00:07:19.170
you.

109
00:07:19.320 --> 00:07:24.960
And then more conventional things like root mean squared error that's also available to basically any

110
00:07:24.960 --> 00:07:28.920
general any regression error metric is here available for you.

111
00:07:29.700 --> 00:07:32.990
Now we've actually committed kind of a big mistake.

112
00:07:33.000 --> 00:07:37.020
We never really separated our data into a training set and a test set.

113
00:07:37.020 --> 00:07:41.460
And the reason for that is because the documentation kind of glosses over that and never actually does

114
00:07:41.460 --> 00:07:41.690
it.

115
00:07:41.760 --> 00:07:46.500
So what we ended up doing right now is we trained them on all our available data.

116
00:07:46.650 --> 00:07:49.310
That's something we certainly actually want to avoid doing.

117
00:07:49.380 --> 00:07:54.130
Instead we wanted to train to split that we discussed during the introduction to machine learning.

118
00:07:54.210 --> 00:07:59.020
We want to train on some training data and then evaluate our model on that actual test data.

119
00:07:59.070 --> 00:08:06.440
So let's show you how you can actually do a train test split I'm going to create a variable called all

120
00:08:06.440 --> 00:08:09.800
data and say spark read.

121
00:08:10.210 --> 00:08:12.730
And I'm actually going to read the exact same data in.

122
00:08:12.730 --> 00:08:14.330
I'm just calling all data now.

123
00:08:14.350 --> 00:08:17.660
So this is kind of a more realistic situation.

124
00:08:18.690 --> 00:08:19.090
All right.

125
00:08:19.260 --> 00:08:21.930
So let's imagine that we just pulled in all that data.

126
00:08:21.960 --> 00:08:26.350
Now how do we actually split that data into some training data and some test data.

127
00:08:26.360 --> 00:08:32.380
We do that is by calling random split and that's actually a method available on any data frame.

128
00:08:32.490 --> 00:08:33.630
So we say all data

129
00:08:36.340 --> 00:08:43.590
random split should be lower case and then inside of this you pasan a list and the list just dictates

130
00:08:43.830 --> 00:08:45.390
how the split is done.

131
00:08:45.390 --> 00:08:50.770
So the first item or first data frame out will have 70 percent of the data.

132
00:08:50.970 --> 00:08:53.580
The second data frame out we'll have 30 percent.

133
00:08:53.580 --> 00:08:57.190
So let me kind of do a little example here.

134
00:08:57.240 --> 00:08:58.970
I'll call this split object.

135
00:08:58.980 --> 00:09:02.220
The results of this and now check out the split object.

136
00:09:02.220 --> 00:09:03.660
What does it actually look like.

137
00:09:03.660 --> 00:09:09.450
Well it's actually a list of two data frames where the first data frame in this list has 70 percent

138
00:09:09.540 --> 00:09:11.650
of the data of all that data.

139
00:09:12.000 --> 00:09:15.980
And the second data frame in the list has 30 percent of all that data.

140
00:09:15.990 --> 00:09:19.080
So usually However you won't put this in a split object.

141
00:09:19.100 --> 00:09:25.940
Instead you'll do a little tuple and packing and you'll say train data test data.

142
00:09:26.150 --> 00:09:27.800
So now I dictate that.

143
00:09:28.160 --> 00:09:29.050
Spell that wrong.

144
00:09:30.080 --> 00:09:37.250
Now dictate that train data is 70 percent of all this data and that data is 30 percent and this is randomly

145
00:09:37.250 --> 00:09:39.500
split for you so you don't need to worry about that.

146
00:09:39.650 --> 00:09:44.000
Well you don't want to do is just grab the top 70 percent in the bottom 30 percent in case by accident

147
00:09:44.000 --> 00:09:45.140
your data has been sorted.

148
00:09:45.350 --> 00:09:50.840
But instead you actually just want to randomly grab 70 percent of values stick to training at 30 percent

149
00:09:50.840 --> 00:09:52.790
of values and that's taken to test data.

150
00:09:52.790 --> 00:09:54.390
So let's run this.

151
00:09:54.710 --> 00:09:58.460
And now we're going to be doing is working off that train data and that test data.

152
00:09:58.460 --> 00:10:04.670
So again this is just if we kind of show this again this is wups a spell that again.

153
00:10:04.820 --> 00:10:05.920
There we go.

154
00:10:05.960 --> 00:10:08.240
This is just the actual label and feature.

155
00:10:08.240 --> 00:10:10.680
So this looks exactly like the data we had before.

156
00:10:10.820 --> 00:10:14.200
But if we take a look at describe show.

157
00:10:14.600 --> 00:10:17.590
Note that the training data has 349 count.

158
00:10:17.690 --> 00:10:19.760
If I look at the test data.

159
00:10:19.760 --> 00:10:25.010
So I say this scribe show off of that this has a 152.

160
00:10:25.020 --> 00:10:32.160
So 30 percent is the test say the 70 percent of it goes to training OK let's continue on and she had

161
00:10:32.160 --> 00:10:33.640
actually all of these.

162
00:10:33.870 --> 00:10:41.650
So we can do is let's create a new model we'll call this correct model and I'm going to fit it to that

163
00:10:41.650 --> 00:10:44.230
training data.

164
00:10:44.430 --> 00:10:46.070
And then once that is done running.

165
00:10:46.200 --> 00:10:52.170
Now I actually want to evaluate how well my model did not on training data but instead on that test

166
00:10:52.170 --> 00:10:52.740
data.

167
00:10:52.980 --> 00:10:58.700
So what I end up doing is say test results is equal to that correct model.

168
00:10:58.800 --> 00:11:01.480
And I call the evaluate method for this.

169
00:11:01.530 --> 00:11:09.450
And inside of that evaluate method I pass in test data and if these test results I can grab anything

170
00:11:09.450 --> 00:11:17.640
I want so I can grab those residuals and show the actual residuals between what the model expected versus

171
00:11:17.640 --> 00:11:20.030
the labels that actually knew for the test data.

172
00:11:20.130 --> 00:11:25.480
Or it could do things like the root mean squared error and then it gives me that back as well.

173
00:11:25.650 --> 00:11:29.240
So what's the difference between what we just did versus what we did before.

174
00:11:29.430 --> 00:11:35.460
Well in our case using evaluate on the test data it means that we're actually comparing our predictions

175
00:11:35.730 --> 00:11:38.880
to the labels that were already assigned in the test data.

176
00:11:38.880 --> 00:11:43.810
So you did this a couple of times maybe adding more parameters into the linear regression model.

177
00:11:43.830 --> 00:11:47.190
So if you come back up here when you actually created that model.

178
00:11:47.190 --> 00:11:51.230
So if we come up to L.R. model Ellar here.

179
00:11:51.420 --> 00:11:58.110
So linear regression to kick Khama there's actually other things like elastic net parameters that intercept

180
00:11:58.170 --> 00:12:03.100
Max iteration regularisation parameters a lot more stuff that we can discuss later on.

181
00:12:03.150 --> 00:12:08.430
But basically the next step after you do that is to try to mess around those parameters see what works

182
00:12:08.430 --> 00:12:13.020
see what doesn't and then reiterate on this process until you have a model that you feel comfortable

183
00:12:13.020 --> 00:12:13.650
with.

184
00:12:13.710 --> 00:12:16.030
Once you do that the models are ready for deployment.

185
00:12:16.080 --> 00:12:21.320
Now you usually deploy your model on data that has no label Otherwise what would be the point in the

186
00:12:21.320 --> 00:12:22.050
plane the model.

187
00:12:22.140 --> 00:12:30.080
If you already had the label so it's kind of mimic that by creating a object called unlabeled data.

188
00:12:31.790 --> 00:12:42.430
And I'm going to set it equal to the test data and I'm going to select features.

189
00:12:42.570 --> 00:12:45.800
So if I look at this unlabeled data if I just show it here.

190
00:12:45.810 --> 00:12:47.480
Note that it's only the features column.

191
00:12:47.490 --> 00:12:51.520
So this doesn't have any actual labels so this is essentially unlabeled data.

192
00:12:51.570 --> 00:12:54.750
And our model is ready to be deployed on this unlabeled data.

193
00:12:54.750 --> 00:12:59.940
So now we're ready for example to predict on housing prices for houses that we don't have a price for

194
00:12:59.940 --> 00:13:00.540
yet.

195
00:13:01.020 --> 00:13:08.460
So we come down here and then what we're going to end up doing is we use the transform method so I'll

196
00:13:08.460 --> 00:13:11.090
create a data frame called predictions.

197
00:13:11.460 --> 00:13:17.460
I call my correct in model I say transform and I transform on that unlabeled data.

198
00:13:17.460 --> 00:13:25.280
So instead of evaluating now I can transform and then if I check up predictions I have features and

199
00:13:25.280 --> 00:13:26.270
prediction.

200
00:13:26.270 --> 00:13:32.360
And now this is the predicted value based off of these features and there's no way I can really evaluate

201
00:13:32.360 --> 00:13:36.710
this as far as r squared or stuff like that or the root mean square error because the state was unable

202
00:13:36.710 --> 00:13:37.210
to begin with.

203
00:13:37.210 --> 00:13:40.130
So I have nothing to really have no correct values to compare it to.

204
00:13:40.250 --> 00:13:43.570
That's the kind of thing I had to take care of back when I had the test data.

205
00:13:43.730 --> 00:13:46.300
So this kind of on the model deployment phase.

206
00:13:46.310 --> 00:13:46.880
All right.

207
00:13:47.090 --> 00:13:51.770
So hopefully that gave you an idea of how the actual workflow for the machine learning algorithm was

208
00:13:51.770 --> 00:13:54.140
Sparke is intended to be used.

209
00:13:54.230 --> 00:13:58.070
Don't worry if you didn't catch every single detail we're going to be working on a much more realistic

210
00:13:58.070 --> 00:13:58.520
example.

211
00:13:58.520 --> 00:14:04.490
Up next and that realistic example help your mind kind of connect features to actual things in real

212
00:14:04.490 --> 00:14:04.870
life.

213
00:14:04.880 --> 00:14:09.740
So I hope you understood the general workflow just the general idea of how things are done here in the

214
00:14:09.740 --> 00:14:14.010
next lecture we're going to clarify a lot of this and also show you some preprocessing steps.

215
00:14:14.060 --> 00:14:16.570
They probably have to run into in the real world.

216
00:14:16.760 --> 00:14:18.390
Thanks and I'll see at the next lecture.
