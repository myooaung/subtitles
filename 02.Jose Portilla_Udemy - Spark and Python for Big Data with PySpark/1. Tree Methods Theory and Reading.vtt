WEBVTT
1
00:00:05.550 --> 00:00:11.910
Welcome to the section on tree methods a very powerful group of algorithms falls under the tree method's

2
00:00:11.920 --> 00:00:18.750
title and we'll be discussing decision trees random forests and gradient boosted trees.

3
00:00:18.750 --> 00:00:22.100
Now I can't really do justice to such a wide variety of topics.

4
00:00:22.140 --> 00:00:28.530
So I really recommend you read Chapter 8 of an introduction to mystical learning OK.

5
00:00:28.590 --> 00:00:33.900
Let's start off of thought experiments give some motivation behind using a decision tree method.

6
00:00:34.040 --> 00:00:38.280
Imagine that I play tennis every Saturday and I always invite a friend to come with me.

7
00:00:38.360 --> 00:00:41.330
Sometimes my friend shows up and sometimes he doesn't.

8
00:00:41.330 --> 00:00:47.360
For him it really depends on a variety of factors or features such as the weather outlook the temperature

9
00:00:47.420 --> 00:00:49.370
the humidity the wind etc..

10
00:00:49.610 --> 00:00:54.400
And I start keeping track of these features and whether or not he showed up to play with me.

11
00:00:54.590 --> 00:00:58.640
So in that getting some data that looks like this and hopefully this sort of structure looks familiar

12
00:00:58.640 --> 00:00:59.440
to you by now.

13
00:00:59.630 --> 00:01:05.810
I have temperature with some categorical column the outlook humidity Wednesday and then my label whether

14
00:01:05.810 --> 00:01:11.270
or not my friend came out to play with me I want to use this data to predict whether or not he will

15
00:01:11.270 --> 00:01:16.640
show up to play in an intuitive way to do this is through decision tree which we've drawn out here as

16
00:01:16.640 --> 00:01:23.750
a little diagram and industry we have nodes and nodes are these split for the value of a certain attribute

17
00:01:23.780 --> 00:01:24.580
or feature.

18
00:01:24.640 --> 00:01:29.960
You can see here splitting on the outlook feature than the humidity feature as well as the windy feature

19
00:01:30.380 --> 00:01:32.530
the edges are the outcome of a split.

20
00:01:32.540 --> 00:01:33.540
The next note.

21
00:01:33.620 --> 00:01:39.520
So you can see here if I split outlook on Sydney then my next note becomes checking for the humidity.

22
00:01:39.620 --> 00:01:44.150
And after that we have two more features which are the root and the leaves.

23
00:01:44.180 --> 00:01:47.190
The root is the No that performs the first split.

24
00:01:47.270 --> 00:01:49.190
In this case that's the outlook node.

25
00:01:49.250 --> 00:01:53.870
That's the very first feature that I'm splitting on that I have the leaves which are the terminal nodes

26
00:01:53.930 --> 00:01:55.030
that predict the outcome.

27
00:01:55.040 --> 00:01:59.600
So those green and red yes or no nodes.

28
00:01:59.720 --> 00:02:05.200
Now question you may have is how do we actually decide what features to split on and in what order.

29
00:02:05.210 --> 00:02:07.380
So few Colback one slide.

30
00:02:07.430 --> 00:02:11.200
How do we decide to split an outlook first instead of humidity first.

31
00:02:11.540 --> 00:02:15.140
So what we want to do is try to get some intuition behind these splits.

32
00:02:15.170 --> 00:02:20.810
Let's look at this imaginary data set with three features X Y and Z with two possible classes.

33
00:02:20.810 --> 00:02:22.520
Class A or class B.

34
00:02:23.770 --> 00:02:28.490
Now if I split on y that gives us a clear separation between classes.

35
00:02:28.540 --> 00:02:29.990
All I have to do is ask why.

36
00:02:30.010 --> 00:02:33.680
Well one of them is split perfectly between class a class B.

37
00:02:33.940 --> 00:02:39.790
So if we go back one slide I can see here that according to that middle y column that lines up exactly

38
00:02:39.880 --> 00:02:42.680
the class that I'm trying to predict as far as the split value.

39
00:02:42.890 --> 00:02:48.710
So there may be a good idea to try to split on that first we could have also tried splitting on other

40
00:02:48.710 --> 00:02:55.460
features and we can see here the results of first splitting on X Y or Z where y gives me that perfect

41
00:02:55.460 --> 00:02:57.680
classification if I split on that first.

42
00:02:58.070 --> 00:03:03.710
So what we really want to do is figure out some sort of methodology for deciding what features to split

43
00:03:03.710 --> 00:03:05.150
on and how to split on them.

44
00:03:06.050 --> 00:03:10.970
So entropy and information gain are the mathematical methods of choosing the best split.

45
00:03:11.120 --> 00:03:15.860
And I would refer you to the reading assignment if you're interested in finding out more about the mathematics

46
00:03:15.860 --> 00:03:17.910
behind entropy and information gain.

47
00:03:18.020 --> 00:03:21.230
All of this stuff is done by Sparke for you behind the scenes.

48
00:03:21.350 --> 00:03:25.430
But if you're interested in the mathematics behind it you can always refer to that chapter 8 reading

49
00:03:25.430 --> 00:03:29.900
assignment and that will definitely touch upon entropy and information gain which is basically how we

50
00:03:29.900 --> 00:03:32.090
decide the best split.

51
00:03:32.090 --> 00:03:35.700
Now the message we just discussed refer to decision trees.

52
00:03:35.850 --> 00:03:40.380
But we can try to improve performance by incorporating a random force.

53
00:03:40.640 --> 00:03:46.690
What we can do is use many trees with the random sample of features chosen as the split where he knew

54
00:03:46.700 --> 00:03:51.470
random sample of features is chosen for every single tree every single split.

55
00:03:51.530 --> 00:03:57.590
And keep in mind this method works for both classification and regression tasks classification we'll

56
00:03:57.590 --> 00:04:02.990
just end up taking votes for what class this particular sample belongs to regression.

57
00:04:02.990 --> 00:04:10.720
We'll just take the average of the predicted values and then set that as it's predicted continuous label.

58
00:04:10.720 --> 00:04:14.710
So you maybe wondering what's the point of choosing a random set of samples.

59
00:04:14.710 --> 00:04:16.460
That's kind of a weird idea.

60
00:04:17.350 --> 00:04:23.590
Well suppose there's one really strong feature in the dataset then most of the trees will end up using

61
00:04:23.590 --> 00:04:31.990
that feature as the top split resulting in an ensemble of similar trees that are highly correlated and

62
00:04:31.990 --> 00:04:37.560
then averaging highly correlated quantities does not significantly reduce variance.

63
00:04:37.570 --> 00:04:43.240
So what we end up doing is by randomly leaving out candidate features from each split ran the forests

64
00:04:43.330 --> 00:04:49.600
the Coralee the trees such that the averaging process can reduce the variance of the resulting model.

65
00:04:51.030 --> 00:04:58.410
So finally let's discuss gradient to trees gradient boosting involves three main elements.

66
00:04:58.410 --> 00:05:00.620
A last function to be optimized.

67
00:05:00.750 --> 00:05:06.610
A weak learner to make predictions and an additive model to add weak learners to minimize the lost function

68
00:05:07.960 --> 00:05:08.610
solaces.

69
00:05:08.640 --> 00:05:14.190
Each of those the last function in basic terms is just a function or equation you will use to determine

70
00:05:14.400 --> 00:05:16.950
how far off your predictions are.

71
00:05:16.950 --> 00:05:23.040
So for example we've actually seen some loss functions the regression may use a squared error last function

72
00:05:23.130 --> 00:05:27.080
and something like classification may use a logarithmic loss.

73
00:05:27.120 --> 00:05:31.560
We won't have to deal this directly using Sparke as far as choosing the last function.

74
00:05:31.560 --> 00:05:36.020
In fact basically all the theory that we discuss here happens under the hood.

75
00:05:36.120 --> 00:05:40.620
In fact that's the whole purpose of using Sparke is that someone already coded this out for you.

76
00:05:40.650 --> 00:05:47.750
And so all the hard work so onto that second point of the week learner decision trees are used as the

77
00:05:47.750 --> 00:05:53.600
weak learner in gradient boosting its comments constrain the week learners such as constraining the

78
00:05:53.600 --> 00:05:58.800
maximum number of layers or nodes or splits or leaf nodes.

79
00:05:58.910 --> 00:06:04.520
Then we have an additive model where trees can be added in one at a time and existing trees in the model

80
00:06:04.520 --> 00:06:11.690
are not changed a gradient the set procedure is used to minimize the loss when adding the trees.

81
00:06:11.710 --> 00:06:16.000
So what is the most intuitive way to think about all of this if Spark's basically doing all of this

82
00:06:16.000 --> 00:06:17.360
for us under the hood.

83
00:06:17.590 --> 00:06:21.280
What do you want to keep in the back of your mind when using Ingredion boosted trees.

84
00:06:21.280 --> 00:06:22.920
So here's the nice way to think about it.

85
00:06:22.930 --> 00:06:25.630
And three quote easy steps.

86
00:06:25.630 --> 00:06:31.570
So the first step would be to train a week model M using data samples drawn according to some sort of

87
00:06:31.570 --> 00:06:32.760
weight distribution.

88
00:06:32.980 --> 00:06:38.950
So each sample has a weight to it and then we want to do is increase the weight of samples that are

89
00:06:38.950 --> 00:06:45.050
misclassified by model M and decrease the weight of samples that are classified correctly by model M

90
00:06:46.360 --> 00:06:52.400
then we train the next week model using samples drawn according to the updated weight distribution.

91
00:06:52.620 --> 00:06:57.660
And in this way basically what we're doing is the algorithm always trains models using data samples

92
00:06:57.660 --> 00:06:59.640
that are quote difficult to learn.

93
00:06:59.640 --> 00:07:04.830
In previous rounds which results in an ensemble of models that are really good at learning different

94
00:07:04.830 --> 00:07:07.420
parts of that training data.

95
00:07:07.520 --> 00:07:12.510
Basically this is boosting the weights of samples that were difficult to get correct.

96
00:07:12.560 --> 00:07:19.870
Which is where this boosted name comes from the real details of greedy and boosting lies in the mathematics

97
00:07:19.870 --> 00:07:23.470
which you may or may not be ready for the pending on your background.

98
00:07:23.590 --> 00:07:29.260
So I encourage you to read the full details there towards the end of Chapter 8 and I SLR in case you're

99
00:07:29.260 --> 00:07:32.850
interested in the mathematics behind everything we discussed here.

100
00:07:33.070 --> 00:07:36.360
So for the most part Sparke is taking care of all of this for you.

101
00:07:36.370 --> 00:07:40.330
Someone already did the hard work of coding all this out and then before that someone that all the hard

102
00:07:40.330 --> 00:07:43.860
work of figuring out the mathematics behind all of this.

103
00:07:43.870 --> 00:07:46.450
So again Sparke handles all of this for you.

104
00:07:46.450 --> 00:07:51.190
You can just use the defaults if you want or dive into Eissler and begin to play around with some of

105
00:07:51.190 --> 00:07:52.200
the parameters.

106
00:07:52.470 --> 00:07:58.300
OK let's continue on by walking through the documentation example for Python and Sparc and these various

107
00:07:58.300 --> 00:07:58.870
methods.
