1
00:00:05,790 --> 00:00:09,830
Welcome to the solutions lecture for the K means clustering consulting project.

2
00:00:09,870 --> 00:00:12,360
Let's open a new Jupiter notebook and get started.

3
00:00:13,630 --> 00:00:13,900
All right.

4
00:00:13,900 --> 00:00:19,680
I have the consulting project notebook open here and I'm going to scroll down and create a new cell

5
00:00:19,750 --> 00:00:21,040
and get started.

6
00:00:21,040 --> 00:00:28,000
First we want to do is say from Paice park import sparked session and then we're going to create a spark

7
00:00:28,000 --> 00:00:31,060
session just as we've done before in the past.

8
00:00:31,060 --> 00:00:36,490
So our main idea is to try to figure out whether it was two hackers or three hackers based off of this

9
00:00:36,490 --> 00:00:42,130
key piece of evidence where they alternate hack Scofield's 100 total hacks and there's two hackers should

10
00:00:42,130 --> 00:00:47,410
be 50 hacks 100 total hacks there's three hackers there should be thirty three hacks and hopefully Kamins

11
00:00:47,410 --> 00:00:49,340
clustering can help us with that.

12
00:00:49,390 --> 00:00:57,490
So let's create our cluster and then envoy's they get or create.

13
00:00:57,640 --> 00:01:02,340
OK so what I'm going to do is load up our data set in our data set.

14
00:01:02,350 --> 00:01:08,310
Isn't there a spark read C S V and it's the hack underscored data set.

15
00:01:08,350 --> 00:01:12,280
So hack underscore data that CXXVIII.

16
00:01:12,550 --> 00:01:19,940
And we're going to say header's true and that in first schema is also true.

17
00:01:20,880 --> 00:01:22,260
It can put that on any line.

18
00:01:24,300 --> 00:01:28,840
OK so now that we have our data set we can see an example row from it.

19
00:01:29,200 --> 00:01:31,250
So we'll say data set our head.

20
00:01:31,360 --> 00:01:33,760
See the following.

21
00:01:33,910 --> 00:01:38,930
So we have the session connection time that bytes transferred whether or not Kelly trace was used.

22
00:01:38,980 --> 00:01:42,590
Servers corrupted and these are all Americal except for the location.

23
00:01:42,650 --> 00:01:43,890
But remember the location.

24
00:01:43,900 --> 00:01:48,460
The forensic engineer said that it was probably useless because we're using a VPN.

25
00:01:48,670 --> 00:01:51,390
So will actually be able to drop that and ignore it.

26
00:01:51,400 --> 00:01:56,110
So let's begin by importing what we need from Paice Parker m-L.

27
00:01:56,220 --> 00:01:59,380
So I'll say from Paice park M-L thought clustering

28
00:02:01,890 --> 00:02:14,220
import Kamins and then we're also going to do is import vector assembler from PI SPARC M-L feature import

29
00:02:14,370 --> 00:02:16,600
vector assembler.

30
00:02:16,740 --> 00:02:22,240
And now let's actually grab the data set columns for our similar object within the past and we want

31
00:02:22,240 --> 00:02:25,770
all of these except location because unsupervised learning.

32
00:02:25,770 --> 00:02:33,480
There's no label here and I'm going to pass this in I'll say feature columns is equal to and then just

33
00:02:33,480 --> 00:02:38,790
grab these paste them here and then I'm going to remove location.

34
00:02:39,170 --> 00:02:45,380
So I'll run that and then I'm going to create an assembler object called assembler.

35
00:02:45,650 --> 00:02:48,930
And it's a vector assembler and it takes the input column.

36
00:02:49,520 --> 00:02:55,970
And then I'm just going to type in feature calls and then the output column is going to just be called

37
00:02:56,450 --> 00:02:57,590
features.

38
00:02:57,800 --> 00:03:01,320
So I'll run that and then I'm going to transform my data.

39
00:03:01,400 --> 00:03:03,180
So I get a final dataset.

40
00:03:03,200 --> 00:03:13,160
So my final data is equal to this assembler object transforming my actual dataset so we're run that.

41
00:03:13,160 --> 00:03:17,890
And if we take a look at my final data it can print it schema and I should be able to see the final

42
00:03:17,890 --> 00:03:19,020
features column.

43
00:03:19,030 --> 00:03:21,300
OK so there's my features column and it's a vector.

44
00:03:21,400 --> 00:03:22,550
So it's ready to go.

45
00:03:22,600 --> 00:03:25,630
And just to be safe let's practice scaling everything.

46
00:03:25,630 --> 00:03:28,230
Although we probably don't need to do it for this particular dataset.

47
00:03:28,360 --> 00:03:30,750
So it is a good idea to get some practice in.

48
00:03:30,820 --> 00:03:38,410
So I'll say from page sparked the MLA feature import standard scaler and then I'll create a scalar object

49
00:03:38,740 --> 00:03:40,170
called scaler.

50
00:03:40,570 --> 00:03:48,160
So that my standard scaler there and the input column expected is features and then the output column

51
00:03:48,260 --> 00:03:52,750
expected is going to be called skilled features.

52
00:03:52,780 --> 00:03:56,980
So we're run that and now we want to do is scale the actual data.

53
00:03:57,070 --> 00:04:06,210
So I'll say my scaler model is equal to that scalar object and I'm going to fit it to my final data

54
00:04:08,430 --> 00:04:17,390
and then what I'm also going to then do is take that and say cluster final data is equal to that scalar

55
00:04:17,390 --> 00:04:18,360
model.

56
00:04:19,260 --> 00:04:21,220
Transform on that final data.

57
00:04:24,180 --> 00:04:25,420
So let's run this.

58
00:04:25,510 --> 00:04:26,850
Make sure it works.

59
00:04:26,920 --> 00:04:27,930
And there we go.

60
00:04:27,940 --> 00:04:33,490
So now if I see my cluster final data and print the schema I have all the way down at the bottom.

61
00:04:33,490 --> 00:04:37,870
My scaled features which is what I'm going to be using for the actual training and if I wanted to I

62
00:04:37,870 --> 00:04:38,950
could just select those.

63
00:04:39,070 --> 00:04:44,310
Or I could just passen everything and my actual algorithms will only be looking for skilled features.

64
00:04:44,640 --> 00:04:45,230
OK.

65
00:04:45,320 --> 00:04:48,970
So our main task is to figure out was it two hackers or three hackers.

66
00:04:48,970 --> 00:04:53,950
And hopefully that gives you a hint that you should be creating two Kamins models one for cables two

67
00:04:54,040 --> 00:04:55,520
and one for Keiko's three.

68
00:04:55,540 --> 00:05:05,790
So let's do that say k means two is equal to k means and make sure we've imported this came in India

69
00:05:06,700 --> 00:05:14,690
and we're going to pass in the feature columns expected to be scaled features and they will specify

70
00:05:15,200 --> 00:05:24,730
that cosmical the two here are going to copy this and paste this but change the K value to be 3 so Kamins

71
00:05:24,730 --> 00:05:26,120
to Kamins 3.

72
00:05:26,530 --> 00:05:34,720
And then we're going to do is create the model so model K-2 is equal to k means 2.

73
00:05:35,020 --> 00:05:40,480
And then we will fit it to that final cluster data base or cluster final data.

74
00:05:40,480 --> 00:05:43,130
And I'm going to copy this and paste this.

75
00:05:43,150 --> 00:05:50,470
Now we're going to do the exact same thing here except this will now be for Keiko's 3 so will run that.

76
00:05:50,470 --> 00:05:50,880
All right.

77
00:05:50,920 --> 00:05:56,150
Now comes for the trickiest part trying to figure out if it was two hackers or three hackers.

78
00:05:56,170 --> 00:05:59,320
And the key to this entire thing is a specific hint.

79
00:05:59,350 --> 00:06:04,030
If you come back up here it was that they should have roughly the same amount of tax.

80
00:06:04,060 --> 00:06:09,010
Which means if we were clustering these into three groups and two groups we could check against the

81
00:06:09,010 --> 00:06:14,770
predictions grouped them by their actual prediction values and then just do a count to see if there

82
00:06:14,770 --> 00:06:17,340
was three even groups or two even groups.

83
00:06:17,400 --> 00:06:19,710
So will scroll all the way back down and check this out.

84
00:06:20,840 --> 00:06:29,080
So we'll do is say model K3 will transform that cluster final data.

85
00:06:29,510 --> 00:06:35,900
So if I take a look at this I just do that then I have a data frame and then I can say for example selects

86
00:06:36,560 --> 00:06:38,210
the actual prediction column

87
00:06:40,910 --> 00:06:45,240
and then show this and then we have predictions.

88
00:06:45,240 --> 00:06:49,020
So the first ones are all zeros but you get the idea.

89
00:06:49,020 --> 00:06:58,200
So what I'm going to end up doing is saying cluster final data group by that prediction column and I'm

90
00:06:58,200 --> 00:07:02,160
going to zoom out just a little bit so we can see this whole command.

91
00:07:02,160 --> 00:07:06,160
So model K3 transform cluster final data group by prediction.

92
00:07:06,360 --> 00:07:09,400
And then what I'm going to end up doing is a count on this.

93
00:07:09,870 --> 00:07:11,380
And then let's show the result.

94
00:07:13,210 --> 00:07:15,520
So this may take a little bit of time but there you have it.

95
00:07:15,730 --> 00:07:18,880
OK so we have three groups here.

96
00:07:18,880 --> 00:07:21,710
Hacker one hacker two and then hackers zero.

97
00:07:21,910 --> 00:07:28,690
And you can see the counts 88 79 167 counts may have differed a little bit but overall you can see that

98
00:07:28,690 --> 00:07:31,060
these are not even looks like hackers 0.

99
00:07:31,060 --> 00:07:37,060
If it's grouped this way has an uneven amount of hacking meaning as far as the behavior goes it's not

100
00:07:37,060 --> 00:07:38,410
an even split.

101
00:07:38,470 --> 00:07:44,660
So let's try this exact same thing except we're going to do is call it with K-2.

102
00:07:44,830 --> 00:07:46,330
So we will run this.

103
00:07:46,600 --> 00:07:53,860
And now we get the actual truth which is here we have an exact even split 167 between 1 and 0.

104
00:07:53,920 --> 00:07:58,480
And you can kind of tell what happened when you went from cake was to teacake was three and you can

105
00:07:58,480 --> 00:08:00,140
basically see what happened here.

106
00:08:00,160 --> 00:08:07,060
It grabbed one of the hackers at two and then if you push the cake three it split it into another two

107
00:08:07,060 --> 00:08:12,490
hacking sessions which essentially kind of dictates that it should have been just two hackers because

108
00:08:12,490 --> 00:08:17,890
we have the even counts here of 0 and 1 versus three hackers where there are uneven counts.

109
00:08:18,280 --> 00:08:18,790
OK.

110
00:08:18,970 --> 00:08:21,460
Hopefully that makes sense if you have any questions on this.

111
00:08:21,490 --> 00:08:23,290
Feel free to post the Q&amp;A forums.

112
00:08:23,290 --> 00:08:28,330
I know this was a bit of a stretch from what we'd done before even with consulting projects in the past

113
00:08:28,360 --> 00:08:33,370
but I hope you enjoy this specific challenge because you have to think about how you could actually

114
00:08:33,400 --> 00:08:38,380
grab the predictions and then do the group buying counts in order to clarify whether the groups were

115
00:08:38,410 --> 00:08:39,670
even or not.

116
00:08:39,670 --> 00:08:41,990
Thanks and I'll see you at the next section.
