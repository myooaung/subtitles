WEBVTT
1
00:00:05.430 --> 00:00:10.210
Hello everyone and welcome to the data bricks set up lecture data bricks.

2
00:00:10.230 --> 00:00:16.140
A company started by the creator of spark that provides clusters that run on top of Amazon Web Services

3
00:00:16.140 --> 00:00:21.450
and as a convenience feature of having a notebook system of their own already set up and the ability

4
00:00:21.450 --> 00:00:27.770
to quickly add files either from storage like Amazon S3 or from your local computer.

5
00:00:27.840 --> 00:00:33.480
It has a free community version that supports a six gigabyte cluster obviously six gigabytes is small

6
00:00:33.480 --> 00:00:36.410
enough that you wouldn't really need big data but that's the free version.

7
00:00:36.420 --> 00:00:37.320
So it makes sense.

8
00:00:37.380 --> 00:00:42.810
They have their own paid versions as well and also has own storage format known as data bricks file

9
00:00:42.810 --> 00:00:43.230
storage.

10
00:00:43.230 --> 00:00:48.030
So we'll be showing you how to do that as well and this sort of table format that's what they call it

11
00:00:48.420 --> 00:00:50.340
needs to be accessed in a particular way.

12
00:00:50.370 --> 00:00:55.080
So after we set up the data Brick's notebook and upload some data I'm going to show you quickly how

13
00:00:55.080 --> 00:00:59.960
you can create data frame off of a table in a data format.

14
00:00:59.990 --> 00:01:00.590
All right.

15
00:01:00.630 --> 00:01:04.240
So we're going to walk you through how to set up data bricks count how you can upload data and then

16
00:01:04.240 --> 00:01:09.090
said as the data frame so that you can continue on with the rest of the Course as far as recommending

17
00:01:09.090 --> 00:01:13.230
data breaks I would recommend that for people who want the quickest online setup.

18
00:01:13.230 --> 00:01:17.370
It really doesn't take that much time but make sure you actually have a good internet connection to

19
00:01:17.370 --> 00:01:18.540
use data.

20
00:01:18.750 --> 00:01:19.410
All right.

21
00:01:19.590 --> 00:01:23.710
Now to get started we're going to go to David Brigstock dot com slash try.

22
00:01:23.850 --> 00:01:29.730
Data bricks and you're going to need an e-mail that you can have access to as well as your name and

23
00:01:29.730 --> 00:01:30.870
then some company name.

24
00:01:30.870 --> 00:01:34.410
So let's head over to this Web site to see what that's all about.

25
00:01:35.850 --> 00:01:36.110
All right.

26
00:01:36.120 --> 00:01:41.820
Here I am at that link and you see two versions the full platform trial and then the community edition.

27
00:01:41.850 --> 00:01:46.890
The full platform trial that has everything on limited cluster's notebooks dashboards and track the

28
00:01:46.890 --> 00:01:52.530
guy sparking data bricks business intelligence tools and you get a 14 day free trial excluding the Amazon

29
00:01:52.530 --> 00:01:56.060
web service charges that this platform runs on top of.

30
00:01:56.100 --> 00:02:00.720
Now we will be using that instead we'll be using the community edition which has that many six gigabyte

31
00:02:00.720 --> 00:02:06.420
cluster interactive notebooks and dashboards and everything you do I believe is public so anyone can

32
00:02:06.420 --> 00:02:07.830
just view it.

33
00:02:07.850 --> 00:02:09.020
All right let's click on this.

34
00:02:09.030 --> 00:02:15.650
Start today and this will take you to a log in page well a sign a page should say and then what we're

35
00:02:15.650 --> 00:02:18.680
going to do is you're going to fill out your first name last name.

36
00:02:18.680 --> 00:02:19.640
Select the company name.

37
00:02:19.640 --> 00:02:24.590
We're just kind of make one up you don't have one then your work email and this is going to be an e-mail

38
00:02:24.590 --> 00:02:28.460
that you can actually log into because they're going to send you a confirmation link.

39
00:02:28.460 --> 00:02:31.700
You click on the confirmation link and that will take you to the log in page.

40
00:02:31.850 --> 00:02:32.900
Then make up a password.

41
00:02:32.900 --> 00:02:34.150
Confirm that password.

42
00:02:34.280 --> 00:02:39.650
You don't need to provide a phone number and then intended use case you can say taking an online course.

43
00:02:39.770 --> 00:02:45.330
And you know the scriber will roll some student I'm not a robot and then Sign-Up right.

44
00:02:45.430 --> 00:02:45.670
OK.

45
00:02:45.680 --> 00:02:50.460
So fill that out and then go check your email for the confirmation email and then click on that log

46
00:02:50.460 --> 00:02:51.260
in link.

47
00:02:51.290 --> 00:02:55.700
I'm going to hop forward in time to the actual log in a page and then walk through what that actually

48
00:02:55.700 --> 00:02:56.500
looks like.

49
00:02:57.880 --> 00:03:02.160
Once you fill that in in your e-mail you should see something that looks like this was the data brick

50
00:03:02.170 --> 00:03:06.600
community edition and then you want to verify your email address and then you click on that link.

51
00:03:07.120 --> 00:03:11.510
And then what you're going to do is just sign in.

52
00:03:11.670 --> 00:03:17.280
You can see here I'm using a temporary e-mail address so it won't work forever.

53
00:03:18.290 --> 00:03:18.510
All right.

54
00:03:18.560 --> 00:03:21.320
And then once you're logged in it's going to look something like this is going to take a little bit

55
00:03:21.320 --> 00:03:22.180
of time to connect.

56
00:03:22.190 --> 00:03:28.250
But this is what the data Brick's community looks like and there's a lot of featured notebooks which

57
00:03:28.250 --> 00:03:29.670
are kind of nice guides.

58
00:03:29.720 --> 00:03:34.490
For instance if you're interested in data breaks for data scientists or maybe Python or our data Rick's

59
00:03:34.490 --> 00:03:39.350
guide you can just click on any of these and there are already no books that are set up with a marked

60
00:03:39.370 --> 00:03:42.920
down text explanatory text plus some extra code in there.

61
00:03:42.950 --> 00:03:44.780
For instance a scholar version.

62
00:03:44.990 --> 00:03:46.790
So once that downloading can check it out.

63
00:03:47.000 --> 00:03:50.490
So this one in particular is done in Scala and you can see here it says walk.

64
00:03:50.490 --> 00:03:51.170
The data breaks.

65
00:03:51.170 --> 00:03:56.630
It has some introduction series and the other ones for Python for data engineering etc. and it's just

66
00:03:56.630 --> 00:03:59.720
a bunch of explanatory text plus different examples.

67
00:03:59.750 --> 00:04:04.940
So there are some abilities for visualization here that we're not going to really cover in this course

68
00:04:04.940 --> 00:04:10.220
because are more specific to data bricks formatting but they are available as options.

69
00:04:10.220 --> 00:04:10.480
OK.

70
00:04:10.480 --> 00:04:14.060
And you can see here the kind of really focusing on the spark sequel aspect of it.

71
00:04:14.060 --> 00:04:17.720
Now realistically what are you going to use when you come to this home page.

72
00:04:17.720 --> 00:04:22.070
You will click on a new notebook but before you actually create a notebook need to create a cluster

73
00:04:22.070 --> 00:04:23.060
to connect that to.

74
00:04:23.360 --> 00:04:31.510
So that can cluster under new and give this a cluster name call it my first cluster and then we're going

75
00:04:31.510 --> 00:04:37.420
to be using the spark 2.1 auto updating with Skala 2.1 0.

76
00:04:37.450 --> 00:04:41.910
You can also do Skala two point one one but I don't want to go to the absolute latest in case there's

77
00:04:41.920 --> 00:04:43.120
any bugs there.

78
00:04:43.120 --> 00:04:44.490
So keep with this one.

79
00:04:44.590 --> 00:04:46.660
An instance that's fine.

80
00:04:46.660 --> 00:04:48.380
AWOS that's also fine.

81
00:04:48.490 --> 00:04:50.230
And let's create a cluster.

82
00:04:50.290 --> 00:04:55.630
So this is going to create that six gigabyte cluster and it's actually creating it on Amazon Web Services.

83
00:04:55.660 --> 00:05:00.820
So keep in mind that data breaks basically running on top of Amazon Web services and providing this

84
00:05:00.820 --> 00:05:06.100
sort of convenience factor which is really what you're paying for plus some of those visualization tools

85
00:05:06.100 --> 00:05:08.470
and business intelligence tools.

86
00:05:08.470 --> 00:05:09.010
OK.

87
00:05:09.280 --> 00:05:14.770
So while this cluster is created We're going to see something like pending and eventually it will say

88
00:05:14.770 --> 00:05:19.890
something like running or created and you can see our current active clusters as well as your terminated

89
00:05:19.890 --> 00:05:22.680
clusters and there it is it's currently running.

90
00:05:22.690 --> 00:05:27.860
So once you clusters running you can come back to home or data Brick's and then we're going to click

91
00:05:27.860 --> 00:05:29.190
on new notebook.

92
00:05:29.210 --> 00:05:34.310
You can also do this by going to workspace and then seeing your other notebooks there but we'll click

93
00:05:34.310 --> 00:05:42.330
on notebook we'll give it a name like my first notebook Salek Python agile language you can see all

94
00:05:42.330 --> 00:05:44.940
the various API languages for Sparc.

95
00:05:45.000 --> 00:05:48.120
We'll stick with Python and then we want to connect it to a cluster.

96
00:05:48.120 --> 00:05:52.020
In this case we only get one option of cluster because a run in community is created

97
00:05:55.580 --> 00:05:55.950
great.

98
00:05:56.060 --> 00:06:00.410
And so this is what the notebook format looks like on data breaks as the data breaks.

99
00:06:00.410 --> 00:06:06.050
Notebook it's really really similar to a Jupiter notebook runs a lot of the same things and to test

100
00:06:06.050 --> 00:06:09.580
that this is actually working in a say import Paice bark.

101
00:06:09.650 --> 00:06:11.480
And then you do shift enter to run

102
00:06:14.390 --> 00:06:15.350
and there it is.

103
00:06:15.350 --> 00:06:17.760
So tell me how long that command took cetera.

104
00:06:17.890 --> 00:06:20.320
Time you ran it out on your first cluster.

105
00:06:20.390 --> 00:06:25.520
So now I want to walk through how you would actually upload data to data bricks and then how you can

106
00:06:25.520 --> 00:06:29.810
call it into a data frame which is going to be working with throughout the rest of the course.

107
00:06:30.590 --> 00:06:34.970
So click on tables here and tables is basically where you store the data from data.

108
00:06:35.210 --> 00:06:39.760
So click over here and create table and it's going to ask you what is the data source.

109
00:06:39.770 --> 00:06:44.940
So we'll basically be working with files that I provided you from that zip file download.

110
00:06:45.170 --> 00:06:50.660
But you can also do things like connected to Amazon S3 service or even connect It's like a sequel engine.

111
00:06:50.680 --> 00:06:55.900
You can see that your all the using the password and then the actual query you're doing but S-3 you

112
00:06:55.890 --> 00:06:59.840
to provide your key your secret access key the bucket name and you could browse the bucket for some

113
00:06:59.840 --> 00:07:05.750
data but we'll just do a local file and I'm going to click here and upload.

114
00:07:05.760 --> 00:07:07.430
I already have my data.

115
00:07:07.440 --> 00:07:10.140
This is a CSFB file even though has a little Excel icon.

116
00:07:10.160 --> 00:07:16.050
It is a very simple CSFB file and that's basically going to be working with most of the course.

117
00:07:16.260 --> 00:07:20.700
And once you've done that it's really small file it's been uploaded to D-B fast so that they Debrett's

118
00:07:20.700 --> 00:07:23.830
format and you can see here data file system.

119
00:07:24.150 --> 00:07:30.360
And what we're gonna do is say whip's preview table and this and I ask you to fill in some table details

120
00:07:30.510 --> 00:07:38.920
so we'll call this my table for a table name is a CSFB file and then it's going to try to detect what

121
00:07:38.920 --> 00:07:43.180
is the limiter that is what is actually separating the columns themselves.

122
00:07:43.180 --> 00:07:46.070
For all comma separated variables it's almost always a comma.

123
00:07:46.090 --> 00:07:51.940
At least all the ones I'm providing you but sometimes you also do things like a pipe operator or maybe

124
00:07:51.940 --> 00:07:52.620
even a tab.

125
00:07:52.630 --> 00:07:59.470
So keep that in mind and you'll notice here a lot of time Sparke will read the first line not as a header

126
00:07:59.590 --> 00:08:00.560
by default.

127
00:08:00.730 --> 00:08:05.620
So if we look at what my table actually looks like I have a column of names and a column of sales figures.

128
00:08:05.630 --> 00:08:10.860
But these first two instances in the first row is the actual column names.

129
00:08:10.870 --> 00:08:15.530
So I want to detect the first row is the header and there it is.

130
00:08:15.530 --> 00:08:20.390
We have named sales and string is name but sales is not a string.

131
00:08:20.600 --> 00:08:25.550
So I also want to set up the data type that says we can either set this as like a double or floating

132
00:08:25.550 --> 00:08:30.940
point but I'm going to be sending this up as an integer because I can see here that there are all integers

133
00:08:32.050 --> 00:08:33.490
and then I create the table.

134
00:08:33.490 --> 00:08:37.630
Obviously if you had something like 100 point five you would set it up as a floating point number.

135
00:08:37.930 --> 00:08:43.400
So let's create that table.

136
00:08:43.510 --> 00:08:45.500
Now I can see my table has been created.

137
00:08:45.510 --> 00:08:49.150
So let me show you how you can actually access this data in the notebook system.

138
00:08:49.150 --> 00:08:54.920
So you come back to workspace you come back to my first book Double click on it.

139
00:08:54.940 --> 00:08:55.890
Here we are.

140
00:08:55.930 --> 00:09:00.120
So we're going to be working with something called Data frames and we'll talk a lot more about this.

141
00:09:00.130 --> 00:09:04.990
But I just want to show you how you can grab that table data as a data frame.

142
00:09:05.050 --> 00:09:09.750
So in order to actually access all the data and save it to a data frame where we're going to end up

143
00:09:09.760 --> 00:09:15.640
doing is using what's known as a sequel context and a little bit of sequel code or a sequel syntax.

144
00:09:15.850 --> 00:09:18.240
So we'll have whatever you want to call the data frame.

145
00:09:18.250 --> 00:09:25.330
It's really common to just call it the f or data or my data frame etc. I call it Dhia and then I'm going

146
00:09:25.330 --> 00:09:27.060
to set it equal to.

147
00:09:27.690 --> 00:09:30.880
And I will call a sequel context.

148
00:09:31.030 --> 00:09:35.830
Usually if you're running Sparke on another system you would have to import sequel context and set it

149
00:09:35.830 --> 00:09:36.260
up.

150
00:09:36.310 --> 00:09:40.950
But Daybreak's says this automatically for you it's kind of part of the convenience and then you get

151
00:09:40.950 --> 00:09:50.860
to say that sequel and then what you're going to write here is selects select base Asterix from and

152
00:09:50.860 --> 00:09:53.000
then whatever you happen to call the table.

153
00:09:53.200 --> 00:09:55.390
So check back on table you can always click on this.

154
00:09:55.480 --> 00:10:00.600
They'll say what are we called the table we'll call my table.

155
00:10:00.970 --> 00:10:04.270
And then you're going to run shift enter to actually run this.

156
00:10:04.450 --> 00:10:08.890
And this is essentially a sequel query in the sequel contact so it's running on top of spark sequel.

157
00:10:08.990 --> 00:10:13.180
Scrubbing everything in that table and setting it to the data frame and the data frame is what we're

158
00:10:13.180 --> 00:10:15.420
going to be working with throughout the rest of the course.

159
00:10:15.430 --> 00:10:20.440
So this particular line this is just raw data Brick's users we won't really be seeing this sort of thing

160
00:10:20.800 --> 00:10:21.690
throughout the rest of the course.

161
00:10:21.700 --> 00:10:24.780
But if you decide to bricks you know it's for you you really like it.

162
00:10:24.870 --> 00:10:31.330
That's really tough to do for the rest of the course so that we can say DMF and if you just say the

163
00:10:31.350 --> 00:10:34.800
F and run the shift enter it's going to say hey I have a data frame here.

164
00:10:34.820 --> 00:10:39.230
Remember we discussed transformations and actions we actually want to show this.

165
00:10:39.350 --> 00:10:41.640
I need to call an action to show it.

166
00:10:41.660 --> 00:10:48.200
So you say that show open and close print sees shift enter and you can see here we have the name the

167
00:10:48.200 --> 00:10:50.300
sales and then the actual values.

168
00:10:50.480 --> 00:10:54.260
And that's the very basics of using the data bricks platform.

169
00:10:54.260 --> 00:10:58.700
Now also want to point out I do not work for bricks and not affiliated with them in any way.

170
00:10:58.730 --> 00:11:02.760
I don't get a kickback if you guys decide to sign up for their paid program.

171
00:11:02.780 --> 00:11:08.080
The only reason I really show it as a private company which is offering a solution is because they Brick's

172
00:11:08.180 --> 00:11:10.250
was founded by the creator of Sparke.

173
00:11:10.250 --> 00:11:16.190
So not many companies are founded by the actual creator of the software framework that they're built

174
00:11:16.190 --> 00:11:16.640
upon.

175
00:11:16.640 --> 00:11:21.050
So that's really why I wanted to show you data bricks and why it's kind of important that you know the

176
00:11:21.050 --> 00:11:23.190
name and the sort of ecosystem.

177
00:11:23.540 --> 00:11:23.920
OK.

178
00:11:24.080 --> 00:11:25.710
So that's it for bricks.

179
00:11:25.730 --> 00:11:28.770
Feel free to the site to use it if it felt like a good connection for you.

180
00:11:28.790 --> 00:11:33.410
But we're going to move on to a couple of more insulation options but if you're done with this feel

181
00:11:33.410 --> 00:11:34.380
free to move forward.

182
00:11:34.400 --> 00:11:38.440
The actual Python crash course or the spark data section.

183
00:11:38.460 --> 00:11:40.280
Thanks and I'll see you at the next lecture.
