WEBVTT
1
00:00:06.250 --> 00:00:11.970
Hello everyone and welcome to the tools for natural language processing lecture part 1.

2
00:00:12.000 --> 00:00:17.160
Before we jump straight into the code along project I want to take a brief moment to explore a few of

3
00:00:17.160 --> 00:00:21.840
the tools that SPARC has for dealing with text data understanding these tools.

4
00:00:21.870 --> 00:00:26.510
We're going to be able to use them easily in our custom coatl project.

5
00:00:26.520 --> 00:00:31.340
So we're going to learn a lot of these basic features that you'll find yourself using all the time.

6
00:00:31.410 --> 00:00:34.390
If you end up dealing with texts they show a spark and Python.

7
00:00:34.500 --> 00:00:36.930
So let's open up a new Jupiter notebook and get started.

8
00:00:39.380 --> 00:00:45.170
OK here I am at a new Jupiter notebook and a quick note the actual descriptions and code that I'm going

9
00:00:45.170 --> 00:00:47.230
to go through can be found underneath.

10
00:00:47.270 --> 00:00:48.970
These are from machine learning folder.

11
00:00:48.980 --> 00:00:51.900
Natural Language Processing the tools for an LP.

12
00:00:52.070 --> 00:00:53.210
IPY and B.

13
00:00:53.420 --> 00:00:56.990
So the imported Sparke session and I've created a spark session.

14
00:00:57.320 --> 00:01:03.500
So let's get started the first thing we're going to do is discuss tokenizer and tokenization tokenization

15
00:01:03.500 --> 00:01:09.260
is the process of taking text such as a sentence and then breaking into individual terms.

16
00:01:09.260 --> 00:01:14.310
And that's usually words in the simple tokenizer class provides this functionality.

17
00:01:14.330 --> 00:01:21.220
So SPARC comes with a tokenizer class and we're also going to talk about is a regular expression tokenizer.

18
00:01:21.380 --> 00:01:25.720
So that allows more advanced tokenization based on a regular expression.

19
00:01:25.910 --> 00:01:30.110
We don't actually cover price on regular expressions in this course but you can go ahead and check out

20
00:01:30.110 --> 00:01:33.160
the resources in case that's something that interests you.

21
00:01:33.470 --> 00:01:40.580
By default the parameter for the regular expression pattern is used as delimiters or spaces to actually

22
00:01:40.580 --> 00:01:41.950
split the input text.

23
00:01:42.020 --> 00:01:46.820
And we're going to do is show you how you can pass in your own parameters for a pattern you want to

24
00:01:46.820 --> 00:01:47.930
split on.

25
00:01:47.930 --> 00:01:48.190
OK.

26
00:01:48.200 --> 00:01:51.690
Now let's start by actually importing everything we were just discussing.

27
00:01:51.770 --> 00:01:59.360
So I'll say from Paice sparked the M-L that feature import tokenizer and rejects are regular expression

28
00:01:59.360 --> 00:02:00.590
tokenizer.

29
00:02:00.590 --> 00:02:02.830
And then I'm also going to import a few more things.

30
00:02:03.020 --> 00:02:10.100
We'll say from Weissberg sequel that functions import call and that allows us to kind of call a column

31
00:02:10.550 --> 00:02:13.520
and then UGF which stands for user defined function.

32
00:02:13.670 --> 00:02:18.560
So we'll also show you how you can create your own functions using lambda expressions and then we'll

33
00:02:18.560 --> 00:02:21.530
save from Paice sparkplugs sequel types.

34
00:02:21.770 --> 00:02:24.990
Import the integer type.

35
00:02:25.020 --> 00:02:32.450
OK so next I want to create a data frame full of sentences so I will say send the death sentence state

36
00:02:32.460 --> 00:02:37.100
of frame is equal to spark create a frame.

37
00:02:37.340 --> 00:02:43.640
And this allows me to pass in a list of tuples where the actual entries are essentially the rows.

38
00:02:43.640 --> 00:02:45.530
So I'll say something like Hi.

39
00:02:45.770 --> 00:02:49.660
I heard about spark and you can just copy and paste this from the note book itself.

40
00:02:49.670 --> 00:02:53.840
You don't want to type all this stuff and then create two more sentences.

41
00:02:53.900 --> 00:02:55.930
This is all from the documentation.

42
00:02:56.030 --> 00:03:03.780
I will say I wish Java could use case classes.

43
00:03:03.960 --> 00:03:07.220
And then last one is going to be a little different.

44
00:03:07.220 --> 00:03:15.770
We're going to say logistic comma regression comma models comma our needs.

45
00:03:15.780 --> 00:03:17.620
Notice the actual commas there.

46
00:03:17.660 --> 00:03:21.430
These other ones are split on the actual whitespace.

47
00:03:21.440 --> 00:03:27.700
This has commas and then finally for the second argument inside this spark create data frame.

48
00:03:27.720 --> 00:03:30.960
We're going to do is pasan a list of the column names.

49
00:03:31.100 --> 00:03:33.360
So the first column name is the ID.

50
00:03:33.440 --> 00:03:36.130
The second column name is the sentence.

51
00:03:36.710 --> 00:03:39.930
So I'm going to run that and then it may take a little bit of time.

52
00:03:39.930 --> 00:03:45.660
If it's your very first time running any data frame but will say send the F that show.

53
00:03:45.950 --> 00:03:47.560
And there we have our simple data frame.

54
00:03:47.570 --> 00:03:52.700
So it's an ID column with a sentence column and you can see that I thought if it was too much to actually

55
00:03:52.700 --> 00:03:57.290
show and what's important to notice here is that our first two entries are split on whitespace as per

56
00:03:57.290 --> 00:03:58.180
the sentence.

57
00:03:58.250 --> 00:04:03.400
Our second or third entry index number two actually has commas in between the words.

58
00:04:03.740 --> 00:04:08.840
Now we're going to do is create the actual tokenizer objects and this is really this should feel familiar

59
00:04:08.840 --> 00:04:09.260
to you.

60
00:04:09.290 --> 00:04:11.090
Given that we've used features before.

61
00:04:11.330 --> 00:04:14.100
All you do is create a feature variable.

62
00:04:14.240 --> 00:04:19.640
In this case we'll say tokenizer and then create an instance of that tokenizer class and you can say

63
00:04:19.640 --> 00:04:21.790
what input column you expect.

64
00:04:22.040 --> 00:04:23.600
In this case I expect sentence.

65
00:04:23.610 --> 00:04:31.050
I don't know have the output column be called words so let's run that and then I'm also going to do

66
00:04:31.110 --> 00:04:37.680
a regular expression tokenizer so will say regex tokenizer

67
00:04:40.560 --> 00:04:47.340
give that a little split there so my regex tokenizer is going to be an instance of that Regex tokenizer

68
00:04:47.820 --> 00:04:48.670
and then same thing.

69
00:04:48.710 --> 00:04:50.980
Input column is that sentence column.

70
00:04:51.240 --> 00:04:56.820
The output column is that words column Salt's given equal sign there.

71
00:04:56.840 --> 00:04:58.350
Run this.

72
00:04:58.370 --> 00:04:59.840
OK now.

73
00:04:59.970 --> 00:05:04.290
Well we're also going to add to the regular expression tokenizer is a third parameter.

74
00:05:04.520 --> 00:05:07.130
So what I want to do is click shift type here to show it to you.

75
00:05:07.400 --> 00:05:09.600
So we have this pattern parameter.

76
00:05:09.710 --> 00:05:14.690
And what this does is it extracts the tokens based off whatever pattern you provide.

77
00:05:14.860 --> 00:05:19.310
So regular expressions in general are kind of a deeper dive topic.

78
00:05:19.310 --> 00:05:24.170
That's outside the scope of this course that would allow you to do is define what you want to split

79
00:05:24.170 --> 00:05:24.560
on.

80
00:05:24.700 --> 00:05:29.000
So if you don't want to split on whitespace you can split a hash tags commas etc..

81
00:05:29.210 --> 00:05:36.440
So we're going to do is specify the pattern and will specify the pattern as in regular expression terms

82
00:05:36.920 --> 00:05:40.270
backglass backslash capital W.

83
00:05:40.310 --> 00:05:47.360
So let's run that and then we're going to create a user defined function called Count tokens and this

84
00:05:47.360 --> 00:05:54.560
user defined function is going to be a tough call and will pass and lambda expression so takes in words

85
00:05:54.800 --> 00:05:58.240
and then it's going to return the length of the words.

86
00:05:58.310 --> 00:06:03.790
And we also have to specify as a second argument here and use the Find function what the type is.

87
00:06:03.790 --> 00:06:10.100
So I will specify that it's an integer type so it doesn't make those lengths of words into strings.

88
00:06:10.370 --> 00:06:11.630
So there's my count tokens.

89
00:06:11.630 --> 00:06:13.400
My use of the find function.

90
00:06:13.490 --> 00:06:15.590
So let's actually play off this now.

91
00:06:15.590 --> 00:06:23.060
Or create a data frame called tokenized which is equal to tokenizer and then I'm going to transform

92
00:06:23.480 --> 00:06:24.810
my data in this case.

93
00:06:24.810 --> 00:06:30.080
That was a sentence state of frame or send underscored the F so I will run that and I'll take a look

94
00:06:30.080 --> 00:06:35.340
at tokenized and show the results here and I can see what happened.

95
00:06:35.350 --> 00:06:41.680
I have my ID column my sentence column and then my words column is now a list of the tokens.

96
00:06:41.680 --> 00:06:46.660
Now something to note here is how many words are actually in each list.

97
00:06:46.720 --> 00:06:54.610
So what's a little unclear given this output is if for the entries themselves is how many words are

98
00:06:54.610 --> 00:06:55.880
in the third list.

99
00:06:56.020 --> 00:07:00.820
Because remember this third list had commas already in it so I don't know if this is just one long string

100
00:07:01.150 --> 00:07:05.610
including the commas or if it's actually all the words separated.

101
00:07:05.680 --> 00:07:10.890
So what we can do is use our count tokens to count how many items are actually in this list.

102
00:07:11.920 --> 00:07:22.950
So from tokenized you're going to do is say with column ongoings create a new column called tokens and

103
00:07:22.950 --> 00:07:28.740
then I'm going to call my use the Find function called Count tokens and call it on this words column

104
00:07:28.770 --> 00:07:32.250
so I can say well words.

105
00:07:32.350 --> 00:07:36.260
And then finally at the end of all this I'm just going to show this.

106
00:07:36.430 --> 00:07:39.100
So showing that now I can see the actual tokens.

107
00:07:39.160 --> 00:07:43.640
So there were five tokens in the first sentence seven tokens in the second sentence.

108
00:07:43.750 --> 00:07:45.510
But notice what happens at the third sentence.

109
00:07:45.640 --> 00:07:50.830
It treats it all as one giant string because it was expecting to split on actual white space.

110
00:07:50.830 --> 00:07:52.630
This is for the tokenizer.

111
00:07:52.690 --> 00:07:57.970
So if you come back up here to our original sentences it's treating this whole thing as one giant string

112
00:07:58.180 --> 00:08:01.090
because there was no white spaces split on.

113
00:08:01.160 --> 00:08:06.690
So coming back down here let's use the regex tokenizer to actually try to get out the tokens so will

114
00:08:06.690 --> 00:08:10.350
specify that we're going to split on not just whitespace.

115
00:08:11.380 --> 00:08:14.060
So we'll say a regex tokenizer here.

116
00:08:14.500 --> 00:08:26.380
And let's actually call transform on this and then say sent sentence to F and then we'll say say r.g

117
00:08:27.720 --> 00:08:36.720
tokenized is equal to reach X tokenizer that transform into Cramm that R.G. tokenized books are not

118
00:08:36.720 --> 00:08:38.940
T.G. R.G. tokenized.

119
00:08:38.970 --> 00:08:41.900
And then let's let's apply this exact same thing to it.

120
00:08:41.970 --> 00:08:44.800
So I'll say with column and then count columns and then show.

121
00:08:45.030 --> 00:08:49.930
So copy and paste this over here and let's run it.

122
00:08:50.010 --> 00:08:55.230
And now I can see the token count is correct is actually treating each of these based off they're split

123
00:08:55.290 --> 00:08:55.910
on commas.

124
00:08:55.920 --> 00:08:57.080
Not just whitespace.

125
00:08:57.110 --> 00:08:58.810
You can actually see an indication of this.

126
00:08:58.950 --> 00:09:04.160
The fact that there's a whitespace between logistic Khama regression versus here there was no whitespace.

127
00:09:04.330 --> 00:09:07.540
And obviously the tokened count makes that pretty clear.

128
00:09:07.560 --> 00:09:08.060
All right.

129
00:09:08.250 --> 00:09:14.520
So if you're interested in learning more about regular expressions check out the resource links provided.

130
00:09:14.760 --> 00:09:21.600
The next thing you want to cover is stop words removal so a stop word is a word that should be excluded

131
00:09:21.600 --> 00:09:26.650
from the input because typically these words appear frequently and don't really care that much meaning.

132
00:09:26.670 --> 00:09:28.030
So these are words like.

133
00:09:28.200 --> 00:09:29.960
Or the etc..

134
00:09:29.970 --> 00:09:36.000
So just common words that you see all the time and we can use a stop words remover to actually do that.

135
00:09:36.000 --> 00:09:40.900
So luckily Stoppard's remover actually has lots of language options available.

136
00:09:41.040 --> 00:09:45.960
So if you're working with data that's not in English maybe it's in Danish Dutch French German Norwegian

137
00:09:45.990 --> 00:09:50.250
Spanish etc. Sparke actually has a lot of these things available for you.

138
00:09:50.250 --> 00:09:54.100
So you have to check the documentation to make sure your language is supported.

139
00:09:54.180 --> 00:09:55.290
So keep that in mind.

140
00:09:55.290 --> 00:09:58.380
Right now we're dealing with English which is definitely supported.

141
00:09:58.680 --> 00:10:02.330
We'll say from Paice park M-L feature import.

142
00:10:02.520 --> 00:10:05.910
And we're going to do import stop Ora's remover.

143
00:10:06.010 --> 00:10:11.560
Now let's create in other data frame we'll call this one sentence data frame and I'm going to say equal

144
00:10:11.560 --> 00:10:16.580
to spark create data frame and again like I mentioned you could just copy and paste this from the notes

145
00:10:17.050 --> 00:10:23.520
but it's going to be two sentences and we'll say we'll actually make these tokenized already.

146
00:10:23.740 --> 00:10:26.830
So I'll say I saw

147
00:10:31.020 --> 00:10:34.190
I don't know green horse.

148
00:10:34.350 --> 00:10:40.080
So some of these words are really common like I and the sum of these words green and horse are not so

149
00:10:40.080 --> 00:10:43.850
common and I'll create another entry.

150
00:10:44.170 --> 00:10:45.220
One more entry.

151
00:10:45.330 --> 00:10:50.440
This will be another tokenize sentence will say Mary had

152
00:10:53.350 --> 00:10:57.220
little and then lamb OK.

153
00:10:57.550 --> 00:11:02.080
And then finally as a second argument let's get these columns titles we'll say Id.

154
00:11:02.650 --> 00:11:08.820
And then we can say tokens so we'll run that make sure everything's balanced.

155
00:11:08.810 --> 00:11:09.650
Perfect.

156
00:11:09.690 --> 00:11:15.670
So if I take a look at my sentence data frame and show the results notice here how my column.

157
00:11:15.690 --> 00:11:24.780
And then my tokens so we can do is create an object called remover and say Stoppard's remover.

158
00:11:24.910 --> 00:11:31.560
So we import it and specify the input and output columns the input column is called tokens and the output

159
00:11:31.560 --> 00:11:35.700
column is called say filtered.

160
00:11:35.790 --> 00:11:37.230
So there's a remover object.

161
00:11:37.230 --> 00:11:44.500
Now we're going to grab that remover object and transform our sentence data frame and then we're going

162
00:11:44.500 --> 00:11:45.020
to show it.

163
00:11:45.020 --> 00:11:48.360
So I'm not saying it's anything I'm just going to show it off.

164
00:11:48.480 --> 00:11:50.390
And now we can see the filter results.

165
00:11:50.430 --> 00:11:55.560
So notice that saw a green horse and a merry little lamb are different than the original tokens so what

166
00:11:55.560 --> 00:12:00.900
it did is it just filtered out the common words based off a dictionary that it has inside of this object

167
00:12:01.170 --> 00:12:06.300
and you can do shift tab here and see other things so you can ask OK is it going to be case sensitive.

168
00:12:06.330 --> 00:12:09.530
What are the stop words so you can add additional stop words.

169
00:12:09.750 --> 00:12:15.030
If that is something that you know for your particular domain is needed maybe certain legal terms are

170
00:12:15.030 --> 00:12:18.840
really common so you want to add those to the stoppered count.

171
00:12:18.880 --> 00:12:20.480
So that's the basic idea.

172
00:12:20.530 --> 00:12:25.240
This is a really common effect for natural language processing and hopefully you can begin to see the

173
00:12:25.240 --> 00:12:26.100
steps.

174
00:12:26.110 --> 00:12:31.510
So a lot of this natural language processing is kind of building a pipeline of step after step after

175
00:12:31.510 --> 00:12:32.140
step.

176
00:12:32.260 --> 00:12:38.890
So you would take the raw sentence split it into tokens take those tokens remove the Stoppard's etc..

177
00:12:38.890 --> 00:12:44.710
Now the final feature I want to go over for part one is called engrams and an engram and just spell

178
00:12:44.710 --> 00:12:46.180
it out so you know what I'm saying.

179
00:12:46.200 --> 00:12:53.260
And Graham that is a sequence of tokens typically words for some integer.

180
00:12:53.260 --> 00:12:59.680
So it's a sequence of end tokens for some integer N and the anagram class can be used to transform input

181
00:12:59.680 --> 00:13:01.310
features into engrams.

182
00:13:01.360 --> 00:13:04.710
So what engram does is it takes an input sequence of strings.

183
00:13:04.720 --> 00:13:10.000
So basically the output of a tokenizer and then the parameter end is used to determine the number of

184
00:13:10.000 --> 00:13:15.850
terms in each anagram and the output will consist of a sequence of these engrams where each engram is

185
00:13:15.850 --> 00:13:21.370
represented by space delimited string that is the space is actually what's the limiting that actual

186
00:13:21.370 --> 00:13:23.620
string of n and consecutive words.

187
00:13:23.620 --> 00:13:27.700
So if the input sequence contains fewer than in strings then output is produced.

188
00:13:27.700 --> 00:13:29.830
Let me show you what all of that means.

189
00:13:30.710 --> 00:13:33.020
I'll say from Paice Burke.

190
00:13:33.060 --> 00:13:40.140
But M-L that feature import the anagram and then what I wanted to do is just copy and paste from the

191
00:13:40.140 --> 00:13:41.170
notes here.

192
00:13:41.250 --> 00:13:44.500
The actual data frame because you don't want to see me type through all this.

193
00:13:44.550 --> 00:13:47.090
So we have our word data frame just like we did before.

194
00:13:47.100 --> 00:13:50.700
Create a spark data frame and we have an ID column and then award's column.

195
00:13:50.700 --> 00:13:56.810
So clearly this is the tokenization of the words column we used before.

196
00:13:56.820 --> 00:14:01.890
So I have this word data frame and now I'm going to create an anagram and going to be an instance of

197
00:14:01.890 --> 00:14:10.680
the engram class and always specify the end is equal to two in my input column is the words column and

198
00:14:10.680 --> 00:14:16.450
then I'm going to say my output column is Gram's So around that.

199
00:14:16.460 --> 00:14:25.340
Now what I want to do is take Engram and transform my actual data which is the word data frame and then

200
00:14:25.340 --> 00:14:27.050
we're going to show off the results.

201
00:14:28.080 --> 00:14:35.190
So let's show that off and we can see here that I have the ID column the words and then Grims the engrams

202
00:14:35.250 --> 00:14:39.070
are representing strings of consecutive words.

203
00:14:39.090 --> 00:14:41.040
So they give you a clear picture of that.

204
00:14:41.130 --> 00:14:48.490
I'm just going to select the grams column for this and then show the results here and let me say I don't

205
00:14:48.490 --> 00:14:50.870
want this to be truncated with three dots.

206
00:14:51.100 --> 00:15:00.580
So before I say you or inside of show I'll call it truncates equal to false analysis from that and I

207
00:15:00.580 --> 00:15:02.030
can see the truncated data.

208
00:15:02.050 --> 00:15:07.420
So the full data itself and again all engrams are doing is they're showing you kind of these pairs of

209
00:15:07.420 --> 00:15:13.000
consecutive words because I labeled and as equal to to each of these grams is going to be two tokens

210
00:15:13.360 --> 00:15:16.070
where we show the consecutive words themselves so high.

211
00:15:16.080 --> 00:15:24.130
I I heard her I heard heard about her about about spark and this kind of is useful when you want the

212
00:15:24.130 --> 00:15:29.350
relationships between two words which words always appear next to each other and the engram kind of

213
00:15:29.350 --> 00:15:31.060
helps show that off.

214
00:15:31.060 --> 00:15:33.490
Now if you start going to large of an engram.

215
00:15:33.490 --> 00:15:37.990
So let's say three then it's going to take longer and longer because you're going to have more and more

216
00:15:38.800 --> 00:15:41.110
data in your tokens so keep that in mind.

217
00:15:41.140 --> 00:15:46.000
Now we won't actually be using anagrams too often but later on if more advanced natural language processing

218
00:15:46.000 --> 00:15:51.370
technique techniques you may find the need for them so keep that in mind that spark does have them available.

219
00:15:51.370 --> 00:15:54.910
So there are two engrams and those are the results of just two.

220
00:15:54.990 --> 00:15:55.630
OK.

221
00:15:55.780 --> 00:15:58.500
If you have any questions feel free to post the Q&amp;A forums.

222
00:15:58.510 --> 00:16:04.390
Coming up next we're going to do is discuss part two of the tools for an LP with T.F. idea.

223
00:16:04.420 --> 00:16:10.570
That's that term frequency inversed document frequency as well as count of vectorize or thanks.

224
00:16:10.690 --> 00:16:11.980
I'll see you at the next lecture.
