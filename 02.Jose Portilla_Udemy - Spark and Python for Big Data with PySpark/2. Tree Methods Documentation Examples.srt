1
00:00:05,660 --> 00:00:06,810
Welcome back everybody.

2
00:00:06,980 --> 00:00:11,500
In this lecture we're going to go through the documentation example for the various tree methods.

3
00:00:11,840 --> 00:00:16,040
So now that we have the intuitive understanding of how these algorithms work we're going to do is dive

4
00:00:16,040 --> 00:00:21,480
into the documentation examples we'll work through decision tree random forest and gradient boosted

5
00:00:21,510 --> 00:00:26,490
trees and you'll see that it's actually very easy to swap almost any machine learning algorithm out

6
00:00:26,490 --> 00:00:27,300
for another.

7
00:00:27,300 --> 00:00:30,550
The way pi Sparc and M-L lib have been constructed.

8
00:00:30,630 --> 00:00:35,730
We will also expand a little more from the documentation example and show some useful evaluation features

9
00:00:36,030 --> 00:00:40,590
as well as how you can use multiclass evaluators on a binary classification.

10
00:00:40,590 --> 00:00:45,930
So let's open up a new Jupiter notebook and get started or I started in new Jupiter notebook and as

11
00:00:45,930 --> 00:00:50,820
a quick note all the data that you need is available under the spark for machine learning under the

12
00:00:50,820 --> 00:00:52,220
tree methods folder.

13
00:00:52,220 --> 00:00:56,470
There's all the data and all the notebooks for this lecture as well as all the other lecturers in the

14
00:00:56,470 --> 00:00:57,480
section.

15
00:00:57,510 --> 00:01:01,350
I started a spark session I've called it my treats are ready to get started.

16
00:01:01,350 --> 00:01:03,360
Most are going to do a couple of more imports.

17
00:01:03,360 --> 00:01:07,220
I'm going to import from Paice sparked the M-L.

18
00:01:07,470 --> 00:01:14,510
I'm going to import pipeline and then I will also import classification models from Paice sparked thought

19
00:01:14,510 --> 00:01:15,220
m-L.

20
00:01:15,430 --> 00:01:19,860
So I will say from Paice park the M-L classification import.

21
00:01:20,020 --> 00:01:23,710
And then the ones we want I want a random forest classifier.

22
00:01:23,710 --> 00:01:30,550
I also want the gradient boosted GBC classifier So they kind of shorten it there and then I also want

23
00:01:30,550 --> 00:01:37,330
the decision tree classifier and let me put this more on a couple of lines here so we can see them all

24
00:01:38,300 --> 00:01:42,680
so ran the first classifier PBT classifier and Decision Tree classifier.

25
00:01:42,710 --> 00:01:48,590
Now as a quick note all these various tree methods depending on how you actually apply them can be used

26
00:01:48,590 --> 00:01:50,190
for regression as well.

27
00:01:50,240 --> 00:01:54,680
So if you ever regression problem that you're interested in applying ran the forced on you could say

28
00:01:54,680 --> 00:02:00,980
from Paice spark the M-L regression and then import and they can click tab here and see what models

29
00:02:00,980 --> 00:02:01,900
are available for you.

30
00:02:01,950 --> 00:02:07,550
You can see you're here you have a decision tree repressor a general or a gradient to tree or aggressor.

31
00:02:07,580 --> 00:02:09,070
And if you start writing.

32
00:02:09,080 --> 00:02:13,640
Are you also heavy rain the forest regressors you can use all these models for regression tasks as well.

33
00:02:13,820 --> 00:02:18,440
And remember for a classification if you have an ensemble of trees basically what we're doing is we're

34
00:02:18,440 --> 00:02:23,000
taking votes to see what class the actual road should belong to.

35
00:02:23,090 --> 00:02:27,880
For regression what they end up doing is the average all the guesses for that continuous variable.

36
00:02:28,160 --> 00:02:33,080
So we're going to the classification for the tree method just because it's a little easier to evaluate

37
00:02:33,560 --> 00:02:38,660
things like root mean squared error or sometimes a little hard to tell or understand intuitively things

38
00:02:38,660 --> 00:02:42,910
like accuracy or area under the curve are a little easier.

39
00:02:42,980 --> 00:02:49,080
So let's continue on a classification and what we're going to do is read in the actual data.

40
00:02:49,900 --> 00:02:53,240
So I will say data is equal to spark read.

41
00:02:53,380 --> 00:02:55,800
And this is the documentation data.

42
00:02:55,810 --> 00:02:59,230
So it's in that kind of esoteric live SVM format.

43
00:02:59,770 --> 00:03:00,990
And then it will say load.

44
00:03:01,210 --> 00:03:07,150
And it's the sample live SVM data that text that's already available for you in the tree methods folder.

45
00:03:07,150 --> 00:03:09,150
So run that we have the data.

46
00:03:09,190 --> 00:03:13,960
And let's just show off that data what it looks like and because it's documentation data it's already

47
00:03:13,960 --> 00:03:19,870
conveniently shown as the label and features column so we don't actually need to do much with this.

48
00:03:19,870 --> 00:03:21,220
So we'll keep scrolling down.

49
00:03:21,430 --> 00:03:24,940
We're going to do just split this into a training set and a test set.

50
00:03:25,300 --> 00:03:30,520
So we'll say something like train data and test data is equal to data.

51
00:03:30,530 --> 00:03:32,090
The random split.

52
00:03:32,350 --> 00:03:34,400
And then let's do a 70 30 split here.

53
00:03:36,110 --> 00:03:36,640
OK.

54
00:03:36,910 --> 00:03:39,760
So now what we can do is create various models.

55
00:03:39,820 --> 00:03:41,520
Let's start off with the most basic one.

56
00:03:41,560 --> 00:03:48,850
A decision tree classifier and then set that equal to this entry classifier we want to specify the label

57
00:03:48,850 --> 00:03:50,940
column as label in the features column is features.

58
00:03:50,950 --> 00:03:53,580
But right now if you look those are already the default.

59
00:03:53,590 --> 00:03:58,740
So we don't really need to specify these the parameter that may be interesting to specify feel like

60
00:03:58,740 --> 00:04:05,710
that shift plus tab are things like the maximum depth for the tree the max on bins and things like minimum

61
00:04:05,710 --> 00:04:06,760
information gain.

62
00:04:06,970 --> 00:04:11,380
I encourage you to play around with those values and if you're confused on how they may affect your

63
00:04:11,380 --> 00:04:16,090
actual decision tree what you should do is do the reading an introduction to statistical learning to

64
00:04:16,090 --> 00:04:19,480
learn a little bit more about the mathematical background behind those parameters.

65
00:04:19,480 --> 00:04:24,310
If you want to mess around beyond the defaults leave everything default right now.

66
00:04:24,580 --> 00:04:32,270
And besides a decision tree classifier I'm also going to make a RAFC or random forest classifier and

67
00:04:32,280 --> 00:04:39,080
have also make a great and boosted tree classifier So it's called PBT so GBG classifier so have these

68
00:04:39,080 --> 00:04:43,630
three classifier objects are all use and the default parameters something like random forest.

69
00:04:43,660 --> 00:04:48,710
And if you do shift tab here not only do you have things like Max def and Max bin's but you also have

70
00:04:48,710 --> 00:04:54,800
a really important one which if you expand on this is the number of trees to use now 20 by default maybe

71
00:04:54,830 --> 00:04:57,360
pretty low especially if a really large dataset.

72
00:04:57,440 --> 00:05:01,540
So we can do here is say something like number of trees equal to 100.

73
00:05:01,610 --> 00:05:06,470
And there's no right or wrong answer for how many trees you should use at a certain point if you start

74
00:05:06,470 --> 00:05:07,790
using too many trees.

75
00:05:07,910 --> 00:05:13,100
You're not going to gain any more accuracy for the additional trees an additional computation.

76
00:05:13,100 --> 00:05:18,280
So sometimes what people do is they run multiple instances of different tree amounts.

77
00:05:18,290 --> 00:05:23,090
Testor accuracy and kind of see where that cutoff point is where they're not gaining any more accuracy

78
00:05:23,450 --> 00:05:27,670
or whatever they're trying to actually evaluate on whatever metric for adding more treason.

79
00:05:27,680 --> 00:05:32,070
So the more trees you add the more computation time and at a certain point you don't get anything back.

80
00:05:32,060 --> 00:05:33,610
Freezing more trees.

81
00:05:33,640 --> 00:05:34,070
OK.

82
00:05:34,250 --> 00:05:36,110
So it will trees to 100.

83
00:05:36,230 --> 00:05:37,700
It's also fine if you just use the default.

84
00:05:37,700 --> 00:05:42,170
It shouldn't really make a difference given that we're kind of working with meaningless data here because

85
00:05:42,170 --> 00:05:44,540
it's just that random live SVM file.

86
00:05:44,540 --> 00:05:47,980
Now that we have these various classifiers what you can do is actually fit them.

87
00:05:47,990 --> 00:05:55,760
So we will see the TC model is equal to the PC and I will fit this to my training data and will do this

88
00:05:55,760 --> 00:05:56,600
for all three of them.

89
00:05:56,600 --> 00:06:03,150
So let's create those three models and then say RAFC for this first one or the second one I should say

90
00:06:03,200 --> 00:06:05,870
and then G-B t for the third one.

91
00:06:09,900 --> 00:06:13,980
So once you run that this may take a little bit of time because you're just fitting three models in

92
00:06:13,980 --> 00:06:14,570
a row.

93
00:06:14,730 --> 00:06:19,540
Once those models have been fitted we can use them and transform the test data to get predictions.

94
00:06:19,650 --> 00:06:30,650
So I can say DTC spreads is equal to my DTC model thought transform and then grabbed my test data and

95
00:06:30,650 --> 00:06:32,620
I'm going to do that for all three of them again.

96
00:06:33,050 --> 00:06:42,290
So let's copy and paste this and we will say R.F. spreads is equal to the RAFC model on that evaluation

97
00:06:42,290 --> 00:06:44,630
or prediction or transformation I should say.

98
00:06:44,780 --> 00:06:48,300
And then she Beati and G-B here.

99
00:06:48,320 --> 00:06:51,950
So basically doing the exact same thing for all three models and then we kind of compare them at the

100
00:06:51,950 --> 00:06:52,740
end.

101
00:06:52,760 --> 00:06:59,280
So now if we take a look at just one of these I'll say DTIC breads and then show them.

102
00:06:59,280 --> 00:07:03,490
So I'm going to let's just say show.

103
00:07:04,290 --> 00:07:05,470
I have the label.

104
00:07:05,490 --> 00:07:09,930
The original features and then RAW prediction the probability and then the prediction itself for the

105
00:07:09,930 --> 00:07:10,920
label.

106
00:07:10,920 --> 00:07:16,470
Now if you look at all of these for example RAFC this also looks really similar has that raw prediction

107
00:07:16,470 --> 00:07:20,200
column but CBT will not have that raw prediction column.

108
00:07:20,220 --> 00:07:23,360
So if you're Renji t it only has prediction itself.

109
00:07:23,370 --> 00:07:25,200
It doesn't have that raw prediction value.

110
00:07:25,230 --> 00:07:30,330
So when you're using something like a multiclass classification evaluator or a binary class classification

111
00:07:30,330 --> 00:07:35,400
evaluator sometimes it has raw prediction as its default input column.

112
00:07:35,490 --> 00:07:40,740
You'll have to change that to prediction for CBT and we show that in the very next lecture so keep that

113
00:07:40,740 --> 00:07:42,090
in mind.

114
00:07:42,090 --> 00:07:46,490
But now that we've done this let's show you how you can use an evaluator on this.

115
00:07:46,500 --> 00:07:53,930
So what I'm going to do is say from Paice park dot evaluation import.

116
00:07:54,180 --> 00:08:00,330
Now even though this is a binary classification dataset if I only do a binary classification evaluator

117
00:08:00,330 --> 00:08:06,180
here I'm only going to be able to do stuff like the precision recall curve and the area under the Orosi

118
00:08:06,180 --> 00:08:12,780
curve but luckily with the multiclass classification evaluator this will still work even though it's

119
00:08:12,840 --> 00:08:14,160
a binary class.

120
00:08:14,250 --> 00:08:19,920
And with this multiclass classification evaluator I can do direct metrics such as precision recall or

121
00:08:19,950 --> 00:08:20,670
accuracy.

122
00:08:20,670 --> 00:08:25,210
And you can check the documentation as we've shown in the past for those actual metric names.

123
00:08:25,210 --> 00:08:30,200
But let's create a multiclass classification evaluator that works for accuracy.

124
00:08:30,300 --> 00:08:39,120
So I will say A.S.C. evil is equal to a multiclass classification evaluator where the if you do shift

125
00:08:39,120 --> 00:08:43,850
tab here expects a prediction column called prediction and a label column called label.

126
00:08:43,860 --> 00:08:45,530
Well we just saw all these threads.

127
00:08:45,690 --> 00:08:47,280
These data frames those are all have it.

128
00:08:47,340 --> 00:08:49,460
And then the metric name the default is EF 1.

129
00:08:49,470 --> 00:08:53,660
But you can do is say metric name to be accuracy.

130
00:08:53,730 --> 00:08:57,640
And again check the documentation for all the various metric names available to you.

131
00:08:57,780 --> 00:09:00,090
But once you have that well we can do is just evaluate.

132
00:09:00,120 --> 00:09:08,600
So I can say A.S.C. evil and I'm going to evaluate the predictions so the predictions I have right now

133
00:09:08,600 --> 00:09:13,200
let's just do the decision tree breads and I'm going to print something.

134
00:09:13,200 --> 00:09:19,760
So this is labeled so DTC we'll say accuracy.

135
00:09:19,780 --> 00:09:23,550
So if I run this we should see the actual accuracy.

136
00:09:23,590 --> 00:09:26,970
So accuracy is 100 percent for decision tree classifier.

137
00:09:27,160 --> 00:09:32,350
That should definitely raise some eyebrows but you should also note that this data over here is really

138
00:09:32,350 --> 00:09:33,960
easily fit.

139
00:09:34,000 --> 00:09:39,820
So it's highly separable meaning even if a decision tree a single decision tree has 100 percent accuracy

140
00:09:40,720 --> 00:09:41,920
it's not a good sign.

141
00:09:41,920 --> 00:09:46,050
As far as how hard the data was to separate.

142
00:09:46,600 --> 00:09:50,710
So keep that in mind in the next lecture we're going to do a much more realistic data set.

143
00:09:50,740 --> 00:09:54,900
And these numbers will be more meaningful but the methods themselves won't change.

144
00:09:54,910 --> 00:10:03,410
So let's check out random forest classifier accuracy RAFC spreads will get 100 percent on accuracy for

145
00:10:03,410 --> 00:10:04,830
around a first class fire.

146
00:10:04,880 --> 00:10:07,300
And finally check reading boosted tree.

147
00:10:07,310 --> 00:10:10,580
Now remember Green was a tree we were just using the default parameters.

148
00:10:10,580 --> 00:10:12,520
So we also get a hundred percent accuracy.

149
00:10:12,560 --> 00:10:13,600
Not very helpful I know.

150
00:10:13,610 --> 00:10:15,830
But the actual methods are the same.

151
00:10:15,830 --> 00:10:21,170
So again in the next lecture on our own custom code example we're going to see something much more realistic.

152
00:10:21,170 --> 00:10:23,710
And these numbers won't be so perfect.

153
00:10:24,420 --> 00:10:24,970
OK.

154
00:10:25,210 --> 00:10:29,740
One final note that I want to show you is the ability to grab feature importance.

155
00:10:29,740 --> 00:10:32,680
So let's grab our model that's fitted.

156
00:10:32,800 --> 00:10:36,940
For example our Random forest model going to copy this.

157
00:10:36,970 --> 00:10:38,100
Paste this here.

158
00:10:38,320 --> 00:10:45,170
And if you hit dot tab you'll notice that one of the things you can grab off of this is feature importances.

159
00:10:45,390 --> 00:10:49,780
So this is not a very good example to show this but it hit shift enter on this.

160
00:10:49,800 --> 00:10:56,670
Essentially what you get is the feature itself and then how important it was so the higher the number

161
00:10:57,090 --> 00:10:59,160
the more important it was.

162
00:10:59,160 --> 00:11:04,980
Now this is kind of a weird example to show because the vector was sparse and it had a lot of features

163
00:11:04,980 --> 00:11:05,460
in it.

164
00:11:05,610 --> 00:11:09,680
So I'm going to leave this just for now kind of hanging in the air.

165
00:11:09,840 --> 00:11:14,410
The reason for that is this is a really big part of your consulting project in this section.

166
00:11:14,460 --> 00:11:16,830
So I kind of want to just leave it out there.

167
00:11:16,890 --> 00:11:22,440
And I want you to explore the documentation on how this feature importance actually works as an attribute

168
00:11:22,770 --> 00:11:25,750
off the model because you're going to be using it for your consulting project.

169
00:11:25,890 --> 00:11:31,950
So consider this just a really heavy hints for the consulting project and I'm going to leave it at that.

170
00:11:31,950 --> 00:11:33,980
This is something you can explore on your own.

171
00:11:34,260 --> 00:11:38,760
If after viewing the consulting project solutions you saw questions on this feel free to post them to

172
00:11:38,760 --> 00:11:40,000
the Kewney forums.

173
00:11:40,110 --> 00:11:46,260
But again this particular dataset is kind of strange to call feature importances on it wasn't very clear

174
00:11:46,260 --> 00:11:52,080
what was going on for the next coatl long project and the consulting project it will be a lot more clear

175
00:11:52,080 --> 00:11:55,760
to you so I won't mention this again until we get to the consulting project.

176
00:11:55,770 --> 00:11:58,880
But keep in mind it's going to be really useful for that particular project.

177
00:11:59,070 --> 00:11:59,590
OK.

178
00:11:59,880 --> 00:12:04,500
Overall I hope you understand now how you can actually call Decision Tree classifier ran the forest

179
00:12:04,500 --> 00:12:06,750
classifier ingredient boost tree classifier.

180
00:12:06,750 --> 00:12:10,110
And again if you wanted to this could all be used as regression models.

181
00:12:10,110 --> 00:12:15,180
Main things to get out of this is basically you should have a full understanding of how to use almost

182
00:12:15,240 --> 00:12:16,770
any model with Sparke.

183
00:12:16,870 --> 00:12:18,360
Essentially you just import it.

184
00:12:18,390 --> 00:12:23,100
You could use a defaults or set up your own parameters then you fit the training data.

185
00:12:23,100 --> 00:12:24,580
Transform any test data.

186
00:12:24,630 --> 00:12:29,610
Or if you have test data or some data that doesn't have the labels already you would just evaluate it

187
00:12:30,420 --> 00:12:33,730
then you get your predictions out and then you can call your evaluation.

188
00:12:33,730 --> 00:12:38,010
Now you can only call these evaluation metrics if you have test data.

189
00:12:38,010 --> 00:12:43,590
If you have data where you only had for example the features you don't have the actual label so you're

190
00:12:43,590 --> 00:12:47,170
kind of about that deployment stage you're not going to be able to call the evaluator.

191
00:12:47,190 --> 00:12:49,790
Instead you're just going to do the evaluation and show your predictions.

192
00:12:49,830 --> 00:12:54,110
As we've shown in the past before and the second thing I want to put out is that you can cut.

193
00:12:54,150 --> 00:13:00,380
You can use this multiclass classification evaluator even on a binary metric or binary data set.

194
00:13:00,480 --> 00:13:03,270
And you can use metric names such as accuracy.

195
00:13:03,270 --> 00:13:08,820
This particular data set not very fun because it's essentially perfect but the next data set that we'll

196
00:13:08,820 --> 00:13:11,330
see in the next lecture they'll become a lot more clear.

197
00:13:11,330 --> 00:13:11,900
All right.

198
00:13:11,910 --> 00:13:14,290
Do you have any questions feel free to the Kewney forums.

199
00:13:14,310 --> 00:13:16,380
But hopefully by now most of this is clear to you.

200
00:13:16,530 --> 00:13:17,490
I'll see in the next lecture.
