1
00:00:05,490 --> 00:00:09,900
Hello everyone and welcome to the tree methods Kodo long lecture where we're going to be coding through

2
00:00:09,900 --> 00:00:10,980
a custom example.

3
00:00:12,160 --> 00:00:16,620
We're going to be doing is working through all three tree methods discussed and compare the results

4
00:00:16,620 --> 00:00:17,940
on a college dataset.

5
00:00:18,120 --> 00:00:23,160
The data has features of universities and that the universities are labeled either a private university

6
00:00:23,160 --> 00:00:24,560
or a public university.

7
00:00:24,690 --> 00:00:29,130
And we're going to use a different tree methods to attempt to label or build a model that can predict

8
00:00:29,370 --> 00:00:32,600
whether a university based Office features is private or public.

9
00:00:32,610 --> 00:00:34,700
Let's jump to a new notebook and get started.

10
00:00:37,140 --> 00:00:42,180
OK here I am at a new notebook and the college data set that we're going to be working with is available

11
00:00:42,180 --> 00:00:47,370
for you underneath the tree methods folder underneath SPARC for machine learning and it's called college.

12
00:00:47,390 --> 00:00:49,750
SEE AS WE But before we get started with that.

13
00:00:49,980 --> 00:00:56,100
Let's start a PI sparks session or a spark session I should say and then we'll import Spark's session

14
00:00:56,790 --> 00:01:03,750
from Paice supercup sequel and we'll create Sparke is equal to spark session builder and give the app

15
00:01:03,840 --> 00:01:07,630
name you can call it whatever you want I'll call it tree.

16
00:01:07,800 --> 00:01:10,350
And then we will get or create this spark session.

17
00:01:11,120 --> 00:01:13,890
Now the next thing you want to do is actually load in that training data.

18
00:01:14,540 --> 00:01:17,880
So we them before since it's just a see every file.

19
00:01:17,880 --> 00:01:25,470
I'll say read CXXVI and it's called college cxxviii and you can always use tab to autocomplete and affirm

20
00:01:25,470 --> 00:01:26,010
that.

21
00:01:26,430 --> 00:01:30,350
And we'll say in first game as true as well as Hetter to be true.

22
00:01:31,420 --> 00:01:36,850
So once you've done that and once the data is loaded We're going to print schema just so we can get

23
00:01:36,850 --> 00:01:38,400
an idea of what the data looks like.

24
00:01:38,440 --> 00:01:43,930
Once you print to this schema you can see here the various columns and if you want definitions or descriptions

25
00:01:43,930 --> 00:01:48,730
for each of these columns you can just check up the code along lecture and it has descriptions for each

26
00:01:48,730 --> 00:01:49,220
of these.

27
00:01:49,450 --> 00:01:52,340
But essentially what these are they're just features of the schools.

28
00:01:52,420 --> 00:01:56,770
So you have the school name then you have whether or not it's private which is what we're going to be

29
00:01:56,770 --> 00:01:57,700
trying to predict.

30
00:01:57,850 --> 00:01:59,760
And there are a whole bunch of features about it.

31
00:01:59,800 --> 00:02:03,860
Things such as the number of applications received the number of students enrolled.

32
00:02:04,030 --> 00:02:08,470
What percentage of new students come from the top 10 percent of the high school class or the top 25

33
00:02:08,470 --> 00:02:10,420
percent they see hear etc..

34
00:02:10,590 --> 00:02:12,080
They are state tuition.

35
00:02:12,520 --> 00:02:17,650
So if you want an example of what this actually looks like you can just say head one and then check

36
00:02:17,650 --> 00:02:19,030
out the actual data itself.

37
00:02:19,030 --> 00:02:21,630
So you can see here we have this first school.

38
00:02:21,760 --> 00:02:23,120
Yes it's a private university.

39
00:02:23,120 --> 00:02:28,990
The number of applications the percent of alumni who donate back to the school etc..

40
00:02:28,990 --> 00:02:32,280
So what we're going to be doing is testing out the various tree methods on this.

41
00:02:32,320 --> 00:02:34,540
So let's actually start formatting this data.

42
00:02:34,570 --> 00:02:43,390
There's a lot of columns to format so we'll say from PI spark thought M-L thought feature import vector

43
00:02:43,420 --> 00:02:44,450
assembler.

44
00:02:44,800 --> 00:02:49,360
So I'm going to import that and then I'm also going to call for the columns themselves to save myself

45
00:02:49,360 --> 00:02:53,800
some typing when I actually pass the feature columns into vector assembler.

46
00:02:53,800 --> 00:02:55,390
So let's build this out now.

47
00:02:56,580 --> 00:03:02,660
I will say assembler is equal to this vector assembler is within the past.

48
00:03:02,780 --> 00:03:07,100
And then I need to grab the input columns so that is an empty list right now.

49
00:03:07,130 --> 00:03:12,670
And I also want to grab the output column or just define the upper column as features.

50
00:03:13,010 --> 00:03:18,350
So what we need is the actual features that we're going to be using and we're going to be using basically

51
00:03:18,350 --> 00:03:20,870
everything below the school's name.

52
00:03:21,020 --> 00:03:22,890
And whether or not it's private.

53
00:03:23,090 --> 00:03:27,670
So everything from apps all the way to the graduation rate works for us.

54
00:03:27,670 --> 00:03:30,900
I'm going to copy this and then pasted in here.

55
00:03:32,070 --> 00:03:35,030
And let me make sure everything's balanced that should work for us.

56
00:03:35,040 --> 00:03:36,480
Let's try this.

57
00:03:36,480 --> 00:03:36,960
Run that.

58
00:03:36,960 --> 00:03:37,520
Make sure it OK.

59
00:03:37,530 --> 00:03:38,230
Any errors.

60
00:03:38,250 --> 00:03:38,790
OK.

61
00:03:38,910 --> 00:03:40,230
So far it's all looking good.

62
00:03:40,440 --> 00:03:45,510
We'll continue on and actually create the output that we'll see output is equal to that assembler.

63
00:03:46,470 --> 00:03:53,580
And then I will transform the data run that and you shouldn't get any errors there.

64
00:03:53,810 --> 00:03:54,340
OK.

65
00:03:54,480 --> 00:03:56,680
Now we want to deal with the private column.

66
00:03:56,700 --> 00:03:59,540
So if you take a look at the actual output.

67
00:03:59,550 --> 00:04:03,900
So if we come back up here remember that this private column is what we're trying to predict.

68
00:04:03,900 --> 00:04:05,690
But right now we have the issue that it's a string.

69
00:04:05,690 --> 00:04:07,130
It says either yes or no.

70
00:04:07,320 --> 00:04:13,230
So we need to actually change that to a 0 or 1 because Spark's lib library can't deal for yes or no

71
00:04:13,230 --> 00:04:14,350
directly.

72
00:04:14,360 --> 00:04:22,920
So we'll come back down here and we're going to say from Paice park M-L that feature import a string

73
00:04:22,920 --> 00:04:32,490
indexer and then run that and then we'll say indexer is indexer the input column I'm expecting just

74
00:04:32,490 --> 00:04:40,830
says private and the output column I'm going to create will just be something very similar maybe private

75
00:04:40,840 --> 00:04:43,000
index.

76
00:04:43,200 --> 00:04:53,400
And then I want the output underscore fixed to be equal to that index or objects that fit and then I'm

77
00:04:53,430 --> 00:04:59,220
going to fit the output and then transformed the outputs and do this all in one step.

78
00:04:59,350 --> 00:05:02,890
If you want to you can actually split this up into two steps.

79
00:05:02,890 --> 00:05:07,810
We've seen it split before and this is just what it looks like on the one step and you could also use

80
00:05:07,810 --> 00:05:12,940
a pipeline if you wanted to if you had to do this for a lot more features for a very strong indexers

81
00:05:13,130 --> 00:05:14,920
I'd definitely suggest a pipeline.

82
00:05:14,920 --> 00:05:20,350
But if there's just one column just a single column you have to apply string index to it's OK to not

83
00:05:20,350 --> 00:05:22,270
use a pipeline truly up to you.

84
00:05:22,450 --> 00:05:26,170
And what pipelines are really for is if you're going to repeat your code a lot that's when you have

85
00:05:26,170 --> 00:05:28,420
to really consider the pipeline right now.

86
00:05:28,450 --> 00:05:31,380
This is kind of the code along so we're fine just doing it this way.

87
00:05:31,720 --> 00:05:34,970
And then finally when I'm going to do is I have my output fixed.

88
00:05:35,140 --> 00:05:36,510
Let's check what that looks like.

89
00:05:37,510 --> 00:05:45,040
I'm going to say print the schema of this run that so I have school private schools a string if I scroll

90
00:05:45,050 --> 00:05:47,180
way down I see features.

91
00:05:47,260 --> 00:05:47,530
Great.

92
00:05:47,560 --> 00:05:50,000
That's a vector that I created and then private index.

93
00:05:50,020 --> 00:05:51,300
It's the double that it created.

94
00:05:51,340 --> 00:05:52,630
So everything's looking good.

95
00:05:52,660 --> 00:05:56,130
I really just need these two columns but you can pass them all.

96
00:05:56,140 --> 00:05:57,330
We really wanted to.

97
00:05:57,640 --> 00:06:07,400
And we're going to do is say my final data is equal to output fixed and then just those two columns

98
00:06:07,400 --> 00:06:15,560
that I just mentioned features and then private index as a string private index.

99
00:06:15,560 --> 00:06:16,960
So let's run that.

100
00:06:16,960 --> 00:06:22,330
And now I want to split this into some training data and some test data.

101
00:06:22,670 --> 00:06:25,920
So I will say final data call random split on it.

102
00:06:26,060 --> 00:06:30,300
And as we've done before Elisha's to 0.7 0.3 run that.

103
00:06:30,300 --> 00:06:35,600
So now we have our trained data on our test data and it's time to actually check out the different classification

104
00:06:35,600 --> 00:06:42,950
methods I could use that tree classifier the Kriti boosted tree CROSSFIRE or the random forest classifier.

105
00:06:42,960 --> 00:06:50,160
So since we're using the tree methods for classification I need to say Paice park M-L and then call

106
00:06:50,160 --> 00:06:51,460
classification here.

107
00:06:51,690 --> 00:06:55,950
Then I can call decision tree classifier.

108
00:06:56,370 --> 00:07:03,990
I can also then call you start typing G-B there is a job classifier and then there's also a if you start

109
00:07:03,990 --> 00:07:04,840
typing random.

110
00:07:04,840 --> 00:07:06,340
I put this on a new line.

111
00:07:08,050 --> 00:07:08,530
Whoops.

112
00:07:08,610 --> 00:07:12,160
Let's put this in print sees that we can do multiple lines here.

113
00:07:12,420 --> 00:07:19,050
And then the final one if we had this over is going to be called random forest classifier.

114
00:07:19,050 --> 00:07:24,860
So let's run them forests classifier So let's run those.

115
00:07:24,890 --> 00:07:28,950
Now you can use tree methods as I mentioned earlier for regression tasks.

116
00:07:28,970 --> 00:07:35,750
So you could say from Paice park the M-L regression and then begin exploring some of these regression.

117
00:07:35,750 --> 00:07:41,420
So if you start typing in random There is a fan the random forest repressor you could use.

118
00:07:41,420 --> 00:07:46,310
And again the difference is as we note in the theory lecture these random forest classifiers or tree

119
00:07:46,310 --> 00:07:52,010
method classifiers in general essentially are just voting for what class they think the actual sample

120
00:07:52,010 --> 00:07:54,150
belongs to for aggressor.

121
00:07:54,320 --> 00:08:00,590
If you have a bunch of trees versus just a single tree put the aggressor will do is all the trees will

122
00:08:00,590 --> 00:08:03,640
give their estimate of what they think the continuous value should be.

123
00:08:03,800 --> 00:08:07,820
And then you just average all of those and that's the random forest repressor right now we're just doing

124
00:08:07,830 --> 00:08:13,760
classification classifications kind of easier to explore as far as evaluation just because it becomes

125
00:08:13,760 --> 00:08:17,480
really clear what accuracy precision or recall mean how many got right.

126
00:08:17,480 --> 00:08:22,370
How many you got wrong versus checking something out like the root mean squared error but either way

127
00:08:22,370 --> 00:08:27,120
just keep in mind all of these can also be used for regression as long as they're available for you.

128
00:08:27,130 --> 00:08:29,190
PI sparked the M-L that regression.

129
00:08:29,390 --> 00:08:34,180
And let's actually show off how to use the pipeline a little more.

130
00:08:34,310 --> 00:08:37,480
So we'll say import pipeline from Paice park that M-L.

131
00:08:37,760 --> 00:08:38,310
OK.

132
00:08:38,480 --> 00:08:42,850
So I'm going to create three different models here DTC.

133
00:08:42,890 --> 00:08:48,260
That's going to stand for my decision tree classifier and it is going to have the label column of private

134
00:08:48,260 --> 00:08:50,780
index.

135
00:08:50,980 --> 00:08:56,350
And then my features column is going to be just called features and essentially these are all going

136
00:08:56,350 --> 00:08:57,650
to be the same.

137
00:08:57,730 --> 00:09:04,510
So I will say our FC is equal to my random forest classifier and I'm going to pass in the exact same

138
00:09:04,510 --> 00:09:07,690
arguments so label column features column.

139
00:09:07,750 --> 00:09:14,770
Copy those and then paste them in and then I'm also going to say PBT for gradient boosting trees.

140
00:09:15,120 --> 00:09:20,460
That's going to be my whoops greedy and boosted classifier and then copy and paste that in and hopefully

141
00:09:20,460 --> 00:09:26,700
by now it feels super familiar to you and you realize just how easy it is to swap classifiers are any

142
00:09:26,700 --> 00:09:29,640
different models in and out using Python and Sparke.

143
00:09:29,640 --> 00:09:31,870
So we want to train all of these models.

144
00:09:32,160 --> 00:09:40,440
So I'll say DTC model is equal to DTC and I will fit this to my training data and I will do the exact

145
00:09:40,440 --> 00:09:41,790
same thing for the rest of them.

146
00:09:41,850 --> 00:09:47,150
So create the model object that's going to be fitted again to my training data.

147
00:09:47,270 --> 00:09:49,700
One more will say G-B T-model

148
00:09:52,510 --> 00:10:04,820
and this is our see fit to G-B Tina R-S.C. Scuse me TVT fit to the train data perfect school run all

149
00:10:04,840 --> 00:10:09,040
those that may take a little bit of time because it's fitting three different models just in a single

150
00:10:09,040 --> 00:10:09,630
cell.

151
00:10:09,780 --> 00:10:14,620
You're going to let it run for just a little bit and we're going to do in the meantime is check out

152
00:10:14,620 --> 00:10:18,820
the model comparison to the model comparison is going to look really similar to this.

153
00:10:18,820 --> 00:10:25,420
We're essentially going to transform for all of the soul say DTC threads for predictions is equal to

154
00:10:25,840 --> 00:10:32,660
DTC underscore a model and I'm going to then transform my test data and I'm going to do a similar thing

155
00:10:32,680 --> 00:10:33,920
to the rest of them.

156
00:10:34,180 --> 00:10:43,060
So let's just copy and paste this so we can add it in later on we say RAFC Freda's is equal to RAFC

157
00:10:43,750 --> 00:10:48,190
and then paste the rest of that and then we'll do the same thing for a gradient boosted trees.

158
00:10:48,190 --> 00:10:54,240
So make green tree predictions physical to Djibouti and then copy and paste the model.

159
00:10:54,430 --> 00:10:55,810
So run that as well.

160
00:10:55,840 --> 00:10:57,440
Again that may take a little bit of time.

161
00:10:57,460 --> 00:11:00,650
The transform process however should be pretty quick.

162
00:11:00,670 --> 00:11:03,890
The fitting is what takes a long time with models.

163
00:11:03,910 --> 00:11:08,260
So now let's get some evaluation metrics and actually see how all of these did.

164
00:11:08,680 --> 00:11:18,060
So I'll say from PI Sparke da M-L top evaluation we're going to import the binary classification evaluator

165
00:11:19,520 --> 00:11:24,350
So once you've imported that binary classification evaluator I can create my evaluation object we'll

166
00:11:24,350 --> 00:11:29,100
call this my and I'm actually going to call this my binary Eva.

167
00:11:29,150 --> 00:11:35,960
All and then I will set this equal to an instance of that binary classification evaluator.

168
00:11:36,050 --> 00:11:40,570
And if you shift tab remember the arguments are a raw prediction column and label column.

169
00:11:40,580 --> 00:11:43,120
Right now my label column is not called label.

170
00:11:43,130 --> 00:11:44,660
It's called private index.

171
00:11:44,660 --> 00:11:45,690
So I want to clarify that.

172
00:11:45,740 --> 00:11:51,800
I'll say a label column is the private index and we'll just go ahead and feed it that raw prediction

173
00:11:51,800 --> 00:11:52,150
column.

174
00:11:52,160 --> 00:11:53,300
That's fine.

175
00:11:53,450 --> 00:11:59,000
And now it's time to evaluate each of these models so let's print out a little line for us all say DTC

176
00:11:59,570 --> 00:12:03,170
for the decision tree classifier.

177
00:12:03,170 --> 00:12:11,300
And then what I'm going to do is print out the result of my binary evil and then I will evaluate the

178
00:12:11,300 --> 00:12:17,100
results of the actual predictions which was DTIC underscore Freda's.

179
00:12:17,150 --> 00:12:19,480
So let's run this and see the results.

180
00:12:19,690 --> 00:12:23,690
OK so this is the area under the curve zero point 0 9.

181
00:12:23,930 --> 00:12:26,510
That's pretty good especially for a decision tree.

182
00:12:26,510 --> 00:12:31,240
So that should already indicate that this particular dataset was highly separable especially if we're

183
00:12:31,250 --> 00:12:33,840
getting such good performance off a single decision tree.

184
00:12:34,040 --> 00:12:38,450
So you may see a little extra from the RAFC or the PBT.

185
00:12:38,510 --> 00:12:41,010
We're also using the default so that's important to remember.

186
00:12:41,090 --> 00:12:45,440
You may be able to juice a little more out of the random force and the gradient boost of trees let's

187
00:12:45,450 --> 00:12:48,740
explore just so the default arguments or default parameters.

188
00:12:48,740 --> 00:12:49,910
How will they do.

189
00:12:50,180 --> 00:13:00,310
So I'll say no RAFC and I'm going to print again my binary evaluates but I'm going to call it.

190
00:13:00,560 --> 00:13:07,710
On the other one so that's our See predictions run this.

191
00:13:07,930 --> 00:13:08,260
OK.

192
00:13:08,290 --> 00:13:10,980
So RAFC that's really quite good.

193
00:13:11,120 --> 00:13:12,160
0.27.

194
00:13:12,310 --> 00:13:17,910
So you can see here that using just the decision tree is not nearly as good as a random forest.

195
00:13:17,920 --> 00:13:22,720
And honestly it should never really be any other way.

196
00:13:22,720 --> 00:13:28,750
It makes sense that having a lot more trees to do the voting or choose the classification would work

197
00:13:28,750 --> 00:13:33,370
better than just a single decision tree especially when we're doing the random splits so you should

198
00:13:33,370 --> 00:13:38,100
always fecht re-inforce to outperform decision tree in almost every situation.

199
00:13:38,140 --> 00:13:42,760
Now something to note before we hit the gradient boosted trees is that greeting boosted trees doesn't

200
00:13:42,760 --> 00:13:46,540
actually have the default prediction or prediction column.

201
00:13:46,540 --> 00:13:52,570
So let's take a look at what JPT Fred's actually looks like by saying.

202
00:13:52,600 --> 00:13:57,280
Print schema offer this and you can see here only has the prediction column.

203
00:13:57,280 --> 00:13:58,110
It doesn't have.

204
00:13:58,120 --> 00:14:05,640
Unlike the other guys I call our spreads and then print this schema these days I actually have those

205
00:14:05,640 --> 00:14:07,970
raw predictions and those probabilities.

206
00:14:08,070 --> 00:14:13,980
So that means if I try to pass the predictions into my binary evil I'm going to get some errors.

207
00:14:14,040 --> 00:14:20,100
So let's actually create another evaluator object that expects just a prediction column.

208
00:14:20,100 --> 00:14:25,650
Remember we discussed earlier that binary classifier evaluator has no problem using that prediction

209
00:14:25,650 --> 00:14:27,190
column instead of the raw prediction.

210
00:14:27,390 --> 00:14:29,470
So I will grab this guy.

211
00:14:29,730 --> 00:14:30,440
Copy it.

212
00:14:31,390 --> 00:14:36,760
Paste it here and we'll say my Ameri evil and let's leave it to.

213
00:14:37,260 --> 00:14:43,740
I am also going to add here is that the prediction column is where I should say this.

214
00:14:44,010 --> 00:14:49,740
So remember we need to specify raw prediction column and instead of prediction we're going to specify

215
00:14:49,740 --> 00:14:50,670
that it's just prediction.

216
00:14:50,670 --> 00:14:55,420
And if you look at the documentation it specifies that that is ok to do OK.

217
00:14:55,450 --> 00:14:57,790
So let's make sure those principles are balanced.

218
00:14:57,790 --> 00:14:58,830
I'm a little zoomed in here.

219
00:14:58,850 --> 00:15:00,180
So hopefully that runs well.

220
00:15:00,320 --> 00:15:00,950
Okay perfect.

221
00:15:00,960 --> 00:15:02,610
So there's my Barner Eve too.

222
00:15:02,680 --> 00:15:10,010
So you can take in this prediction column and we're going to delete that and then prints Green a tree

223
00:15:11,420 --> 00:15:19,400
and print out my binary eveil to evaluate these PBT predictions or Pretz.

224
00:15:19,470 --> 00:15:25,890
Let's run that and we can see here this gradient boosting tree did not perform as well as RAFC and also

225
00:15:25,890 --> 00:15:28,140
did not perform as well as the decision tree.

226
00:15:28,320 --> 00:15:29,830
So that may be surprising to you.

227
00:15:29,880 --> 00:15:34,020
And the reason for that is because we're the green and blue tree we actually didn't really edit any

228
00:15:34,080 --> 00:15:38,590
of the default parameters and the default parameters that spark provides.

229
00:15:38,610 --> 00:15:41,650
You may not be really good for your particular dataset.

230
00:15:41,700 --> 00:15:46,350
So what you should do is when you actually create the model come back appear to GBG classifier.

231
00:15:46,350 --> 00:15:54,120
And if you shift tab you'll notice that you have max depth Max bins and Max information gain a lot of

232
00:15:54,120 --> 00:15:56,080
parameters here to mess around with.

233
00:15:56,100 --> 00:15:58,400
So you should definitely check these out play around with them.

234
00:15:58,410 --> 00:16:00,590
That's kind of an assignment to be left for you.

235
00:16:00,690 --> 00:16:06,810
Can you deal with random force classifier and green Boosey tree classifiers to try to juice a little

236
00:16:06,810 --> 00:16:11,190
more performance out of them especially if Ranum Court for us classifier if you look at the defaults

237
00:16:11,190 --> 00:16:16,190
here the actual number of trees used isn't that high.

238
00:16:16,190 --> 00:16:20,100
If you expand on this they will tell you that it's only 20 trees.

239
00:16:20,100 --> 00:16:22,200
That's a pretty low number of trees.

240
00:16:22,290 --> 00:16:27,690
It's for this particular case it's no problem but a lot of times people use up to 100 or 200 trees on

241
00:16:27,690 --> 00:16:29,200
a tree in the forest again.

242
00:16:29,760 --> 00:16:34,650
It really depends on your particular dataset and whether that number of trees makes sense but number

243
00:16:34,650 --> 00:16:35,640
of trees 20.

244
00:16:35,640 --> 00:16:40,680
Depending on how many entries you have that could be a pretty low number so might be fun to try to increase

245
00:16:40,680 --> 00:16:43,950
the number of trees here and see if that helps out with a prediction.

246
00:16:43,950 --> 00:16:48,270
So let's just actually do that right now since it's easy enough we'll see a number of trees and I'm

247
00:16:48,270 --> 00:16:50,760
going to set the sequel to 150.

248
00:16:50,790 --> 00:16:52,050
So let's run these again.

249
00:16:52,680 --> 00:16:54,210
And we're not going to fit all the data.

250
00:16:54,210 --> 00:16:59,250
Instead we're going to just fit around the force model run that.

251
00:16:59,250 --> 00:17:04,290
So now that we have 150 trees toppling and it take quite a bit longer than it used to but that's OK.

252
00:17:04,580 --> 00:17:10,890
Now I'm just going to do our F.C. predictions run that and then we'll say from Paice park that again

253
00:17:11,430 --> 00:17:12,600
that should still be good.

254
00:17:12,660 --> 00:17:14,120
And let's run these again.

255
00:17:14,190 --> 00:17:18,270
So that's still 0.9 0 9.

256
00:17:18,480 --> 00:17:20,080
And this is we're going to redo.

257
00:17:20,080 --> 00:17:21,820
So this is with the 20 trees.

258
00:17:21,840 --> 00:17:26,180
Let's see if Brina it's 150 trees increased accuracy and there we go.

259
00:17:26,370 --> 00:17:30,560
So you can see here that already juiced up to 1 percent as the area under the curve.

260
00:17:30,600 --> 00:17:35,400
Not bad especially since we just had to change the number of trees and the actual work we had to do

261
00:17:35,460 --> 00:17:38,260
was actually just changing one line of code.

262
00:17:38,280 --> 00:17:38,710
All right.

263
00:17:38,740 --> 00:17:44,020
So we saw how affecting the parameters of the models can help increase performance but what else we

264
00:17:44,020 --> 00:17:47,000
want to discuss is about our classification evaluators.

265
00:17:47,050 --> 00:17:52,690
Right now we're doing binary classification so we imported the binary classification evaluator but fortunately

266
00:17:52,690 --> 00:17:58,300
if you want to test things like accuracy precision or recall you can't grab those directly from a binary

267
00:17:58,300 --> 00:18:03,670
classification evaluator but you can grab them from the multiclass classification evaluate it directly

268
00:18:03,940 --> 00:18:05,670
and actually spark plays nicely.

269
00:18:05,770 --> 00:18:08,730
Even if you're just doing binary class and you can use that as well.

270
00:18:08,740 --> 00:18:13,510
So let me walk through how to do this and there's a more detailed example and the spark notes or the

271
00:18:13,510 --> 00:18:18,890
notebook that goes with this lecture will say from Paice park the M-L that evaluation.

272
00:18:18,940 --> 00:18:23,260
And we will import the multiclass classification evaluator.

273
00:18:23,260 --> 00:18:31,620
So we'll run that and then let's create our will say this is an accuracy evaluator acq evil will take

274
00:18:31,620 --> 00:18:37,170
multiclass classification evaluator specify that the label column is that private index column from

275
00:18:37,170 --> 00:18:41,320
earlier and then that should actually be in the prediction column.

276
00:18:41,340 --> 00:18:44,210
The shift tab here is by default prediction.

277
00:18:44,250 --> 00:18:47,100
It's included in all of them so we'll just run that.

278
00:18:47,400 --> 00:18:51,100
And then the next thing you're going to do is actually grab the accuracy.

279
00:18:51,120 --> 00:18:57,090
So let's make sure that we're doing accuracy Fidus shift tab so the metric name right now is EF 1.

280
00:18:57,390 --> 00:19:05,010
And I'm actually going to copy this and set it to be accuracy so it can specifically be set to accuracy

281
00:19:05,010 --> 00:19:05,920
as a string.

282
00:19:06,000 --> 00:19:09,850
And as you mentioned before we can check the documentation for all the metrics available.

283
00:19:10,050 --> 00:19:13,980
So we will run that this time and then we're going to do is evaluate this again.

284
00:19:14,070 --> 00:19:19,380
So let's evaluate something like the random forest will say random for us.

285
00:19:19,380 --> 00:19:29,140
Accuracy is equal to this accuracy evaluator and evaluate on RAFC predictions.

286
00:19:29,140 --> 00:19:35,330
So once that's been running we'll say our the accuracy and it's 94 percent accuracy.

287
00:19:35,360 --> 00:19:39,770
And if you want to check other metrics you can always show us replace this metric name with the other

288
00:19:39,770 --> 00:19:42,250
available metrics shown in the documentation.

289
00:19:42,560 --> 00:19:47,090
As a quick side note you can check out the notebook for this particular lecture and I've gone through

290
00:19:47,120 --> 00:19:52,970
and shown the accuracy of just using pure defaults on all three models and all super similar.

291
00:19:52,970 --> 00:19:55,840
They all get about ninety one point five percent accuracy.

292
00:19:55,850 --> 00:20:00,270
The one that has the highest accuracy just with the defaults is the radio industry.

293
00:20:00,410 --> 00:20:04,100
But again that may come at the cost of good precision or good recall.

294
00:20:04,100 --> 00:20:05,860
So definitely have a lot of fun with this.

295
00:20:06,080 --> 00:20:10,130
I want to leave it as an optional assignment for you but I highly encourage you to just start playing

296
00:20:10,130 --> 00:20:12,320
around the parameters in these kind of models.

297
00:20:12,320 --> 00:20:18,590
It's really useful to see how changing certain parameters can affect the accuracy or performance of

298
00:20:18,590 --> 00:20:19,550
the model itself.

299
00:20:19,580 --> 00:20:23,840
And if you have questions on some of those parameters you can do the reading in an introduction to school

300
00:20:23,840 --> 00:20:28,620
learning to find out more about each of those parameters actually does ok.

301
00:20:28,790 --> 00:20:30,080
Hope you enjoyed this project.

302
00:20:30,080 --> 00:20:35,570
Coming up next we're going to discuss actually your consulting project for this section of the course.

303
00:20:35,570 --> 00:20:36,260
I'll see you there.
