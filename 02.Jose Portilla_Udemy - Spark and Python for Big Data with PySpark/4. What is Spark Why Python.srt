1
00:00:05,350 --> 00:00:12,310
Welcome to the overview of Sparke lecture before we begin the set up and coding of python and spark.

2
00:00:12,550 --> 00:00:18,250
Discuss what spark is in the context of big data will begin of a general explanation of what big data

3
00:00:18,250 --> 00:00:20,920
is and then the related technologies.

4
00:00:20,920 --> 00:00:24,580
So again the general things are going to be covering and this lecture is the answer to the question

5
00:00:24,580 --> 00:00:30,730
What does Big Data give an explanation of Hadoop map reduce spark talk about local versus distributed

6
00:00:30,730 --> 00:00:36,730
systems an overview of the Hadoop ecosystem and get an overview of Sparke now on this course when we're

7
00:00:36,730 --> 00:00:42,810
actually coding we'll be working with the spark Python API which comes in the form of a library called

8
00:00:42,820 --> 00:00:43,810
Paice park.

9
00:00:44,080 --> 00:00:48,280
But in order to really understand why you would need to use that and what the purpose of SPARC is and

10
00:00:48,280 --> 00:00:53,080
what problem is trying to solve you need to understand Hadoop and map reduce and the entire big data

11
00:00:53,110 --> 00:00:55,930
ecosystem which is why we're going to talk about that here.

12
00:00:56,880 --> 00:00:59,210
So first off what is Big Data.

13
00:00:59,520 --> 00:01:04,770
Well let's say you have some data that can fit on a local computer somewhere in the scale of 0 to 32

14
00:01:04,770 --> 00:01:10,860
gigabytes maybe even 64 gigabytes depending on how much RAM you have that kind of scale of data is not

15
00:01:10,860 --> 00:01:14,710
big data even if you have somewhere between like 30 and 60 gigabytes.

16
00:01:14,760 --> 00:01:20,100
It's still large enough that conceivably you could buy enough RAM to just fit it all in your random

17
00:01:20,100 --> 00:01:24,490
access memory that's memory that is used when you open up something like an Excel file.

18
00:01:24,630 --> 00:01:27,580
You know everything is stored on RAM and you have quick access to it.

19
00:01:27,810 --> 00:01:33,180
But what do you do if you end up having a larger set of data than what you can hold on to ram.

20
00:01:33,180 --> 00:01:34,350
Well they have a couple of options.

21
00:01:34,350 --> 00:01:38,960
You can try using a sequel database to move storage onto a hard drive instead of RAM.

22
00:01:39,180 --> 00:01:44,300
Or you can use a distributed system something that distributes the data to multiple machines and computers.

23
00:01:44,490 --> 00:01:49,290
And this is where SPARC is going to come into play if you're using SPARC you're at a point where it

24
00:01:49,320 --> 00:01:54,600
no longer makes sense to fit all your data on RAM and it no longer makes sense to fit all your data

25
00:01:54,960 --> 00:01:56,950
into a single machine.

26
00:01:57,650 --> 00:02:01,350
So what is a local system versus a distributed system.

27
00:02:01,640 --> 00:02:03,830
Well a local system is probably what you're used to.

28
00:02:03,860 --> 00:02:05,910
It's just a single machine a single computer.

29
00:02:05,930 --> 00:02:10,580
It all shares the same ram the same hard drive a the Strip it system looks something like what we see

30
00:02:10,580 --> 00:02:11,430
on the right.

31
00:02:11,480 --> 00:02:18,110
You have one main computer some sort of master node and you also have data and calculations distributed

32
00:02:18,410 --> 00:02:20,090
onto the other computers.

33
00:02:21,350 --> 00:02:26,750
And what you can do in a local system versus a distributed system is your local system you'll be limited

34
00:02:26,750 --> 00:02:29,720
to however many cores are on your local system.

35
00:02:29,720 --> 00:02:34,050
That is how much processing power or how much storage you have locally.

36
00:02:34,190 --> 00:02:40,580
But in a distributed system what you can do is leverage the power of a bunch of weaker machines to eventually

37
00:02:40,580 --> 00:02:45,270
have more corps and more capabilities than even a strong local single machine.

38
00:02:45,350 --> 00:02:49,790
So we can see here on the local machine we have four cores but what we can do if distributed machines

39
00:02:49,820 --> 00:02:55,400
is get a bunch of weaker servers or weaker machines with less power but distribute either the data and

40
00:02:55,400 --> 00:03:00,590
the calculations across all them as machines to take advantage of a distributed system which is kind

41
00:03:00,590 --> 00:03:02,670
of at the crux of what SPARC is trying to do.

42
00:03:03,840 --> 00:03:09,360
So a local process will use the computation resources of a single machine widely distributed process

43
00:03:09,360 --> 00:03:14,280
has access to the computational resources across a number of machines connected through some sort of

44
00:03:14,280 --> 00:03:15,080
a network.

45
00:03:16,090 --> 00:03:22,090
And after a certain point it's much easier to scale out to many lower CPQ machines in the distributed

46
00:03:22,090 --> 00:03:28,030
system than try to scale up to a single machine with a high CPQ and distributed machines also have the

47
00:03:28,030 --> 00:03:29,560
advantage of easily scaling.

48
00:03:29,560 --> 00:03:34,270
All you have to do is just add more machines versus a single computer no matter how nice it is there

49
00:03:34,340 --> 00:03:38,320
there's going to be a limit on how much RAM or how much storage you can add to a single machine.

50
00:03:38,320 --> 00:03:42,790
So if distributed machines you can just keep adding systems to the network and just get more power that

51
00:03:42,790 --> 00:03:43,740
way.

52
00:03:43,990 --> 00:03:48,250
And they also include fault tolerance which is really important when you're talking about large data

53
00:03:48,250 --> 00:03:48,820
sets.

54
00:03:48,970 --> 00:03:54,010
If one machine fails the whole network can still go on which you can't do on a local machine if your

55
00:03:54,010 --> 00:03:58,720
local machine crashes due to some error in the calculation that well that's it you just lost all your

56
00:03:58,720 --> 00:04:04,450
calculations all your data and fault tolerance is a fundamental idea where you're going to be replicating

57
00:04:04,450 --> 00:04:09,520
your data across multiple machines so even if one goes down your calculations and your data still persists

58
00:04:09,580 --> 00:04:11,290
and goes on.

59
00:04:11,290 --> 00:04:18,030
Now let's discuss the typical form of a distributed architecture that uses something like Hadoop so

60
00:04:18,030 --> 00:04:22,850
Hadoop is a way to distribute very large files across multiple machines.

61
00:04:22,860 --> 00:04:28,740
The issue of taking one large file and storing it onto one machine isn't really that complicated of

62
00:04:28,740 --> 00:04:30,310
a task was quite trivial.

63
00:04:30,450 --> 00:04:36,750
But the idea of grabbing a really really large file that no longer sits on one machine and distributing

64
00:04:36,750 --> 00:04:42,300
it in a way that makes sense across multiple Sheens was quite a complicated issue back in the day and

65
00:04:42,300 --> 00:04:48,300
that's what led to the creation of Hadoop which uses the Hadoop distributed file system or HFS and you

66
00:04:48,300 --> 00:04:53,880
may have seen that term HD effect when you've read stuff on big data and HD FS allows a user to work

67
00:04:53,880 --> 00:04:57,030
with really large data sets across a distributed system.

68
00:04:57,030 --> 00:05:02,610
What's also super important about HFS it duplicate blocks of data for fault tolerance which is the idea

69
00:05:02,850 --> 00:05:07,760
if one machine goes down in your system your data is duplicated on some other machine.

70
00:05:07,830 --> 00:05:09,420
So it still continues.

71
00:05:10,030 --> 00:05:16,370
It also uses what's known as map reduce and map reduce allows computations on that distributed data.

72
00:05:16,420 --> 00:05:21,280
So it can have two fundamental ideas here Hadoop sheeted file system which is the way that we get a

73
00:05:21,280 --> 00:05:26,740
really large dataset distributed across multiple machines and then we have the idea of map reduce which

74
00:05:26,740 --> 00:05:30,220
allows computations across the distributed data set.

75
00:05:31,740 --> 00:05:34,260
So for distributed storage you have something that looks like this.

76
00:05:34,260 --> 00:05:39,630
Again you have some sort of master node or a named node has its own C-p its own RAM and that basically

77
00:05:39,630 --> 00:05:45,330
controls the process of either distributing the storage or the calculations to these other something

78
00:05:45,330 --> 00:05:46,440
known as slave nodes.

79
00:05:46,440 --> 00:05:50,260
These other data nodes are their own super and own RAM.

80
00:05:50,430 --> 00:05:56,650
So what the effect does is it uses blocks of data with a size of 128 megabytes by default.

81
00:05:56,820 --> 00:06:01,680
And each of these blocks is replicated three times and the blocks are distributed in a way support fault

82
00:06:01,680 --> 00:06:02,610
tolerance.

83
00:06:02,610 --> 00:06:09,470
So a machine can go down but your data will go on so smaller blocks provide more paralyzation during

84
00:06:09,470 --> 00:06:16,350
processing and multiple copies of a block prevent loss of data due to the failure of a node and a map

85
00:06:16,350 --> 00:06:21,780
reduce is a way of splitting computational tasks to a distributed set of files such as Hadoop distributed

86
00:06:21,780 --> 00:06:26,070
file system and it consists of a jog tracker and multiple task trackers.

87
00:06:26,070 --> 00:06:29,960
And this idea of trackers is also going to go on with Sparke.

88
00:06:30,090 --> 00:06:36,300
So in the context of map reduce the job tracker sends code to run on the task trackers and then the

89
00:06:36,300 --> 00:06:44,220
task trackers allocate the CPI in memory for the tasks and monitor the tasks on these worker nodes.

90
00:06:44,360 --> 00:06:50,510
So what we cover can be thought of in two distinct parts using the first distribute large datasets and

91
00:06:50,510 --> 00:06:54,970
then using map reduce to distribute a computational task to a distributed dataset.

92
00:06:54,980 --> 00:06:58,000
And next we're going to do is learn about the latest technology in this space.

93
00:06:58,010 --> 00:07:05,570
That second half of it known as SPARK and spark improves on the concepts of using distribution so we're

94
00:07:05,570 --> 00:07:11,750
going to continue talking this lecture just an abstract overview of spark spark versus map reduce what

95
00:07:11,750 --> 00:07:17,360
sparked already these are and then spark data frames spark is one of the latest technologies being used

96
00:07:17,360 --> 00:07:19,600
to quickly and easily handle big data.

97
00:07:19,880 --> 00:07:24,800
It is an open source project on Apache So you have access to all the source code and it was first released

98
00:07:24,800 --> 00:07:30,430
in February 2013 and has exploded in popularity due to its ease of use and its speed.

99
00:07:30,530 --> 00:07:33,610
And it was created at the app lab at UC Berkeley.

100
00:07:34,160 --> 00:07:38,030
And you can think of Sparke as a flexible alternative to map reduce.

101
00:07:38,150 --> 00:07:44,330
So don't think of it in the concepts of Hadoop versus SPARC but instead of map reduce versus spark and

102
00:07:44,330 --> 00:07:50,150
spark and use data stored in a variety of formats including things that are stored on Sandra AWOS S3

103
00:07:50,150 --> 00:07:54,220
storage Hadoop distributed file system and more.

104
00:07:54,230 --> 00:08:00,320
Now let's take a little bit of time to discuss the idea of SPARC versus map reduce this does dive in

105
00:08:00,320 --> 00:08:05,560
a little bit into semantics because sometimes people phrase things in a way that's a little bit confusing.

106
00:08:05,570 --> 00:08:10,510
But I want you to get the idea that it's SPARC versus map reduce not SPARC versus do.

107
00:08:10,700 --> 00:08:16,160
So a lot of times you'll hear that discussion of Hadoop map reduce versus Sparc and then people kind

108
00:08:16,160 --> 00:08:20,820
of shorten that to Hadoop versus Spart but that's not really a good analogy or comparison.

109
00:08:20,840 --> 00:08:26,270
But let me explain why sometimes people say Hadoop map reduce and then just shorten it to Hadoop.

110
00:08:26,270 --> 00:08:30,790
The reason is map reduce remember that's the half that actually distributes the calculation.

111
00:08:30,830 --> 00:08:35,420
It requires files to be stored in Hadoop distributed file system.

112
00:08:35,510 --> 00:08:41,410
So because of that people tend to put Hadoop in map reduce together as kind of a single phrase.

113
00:08:41,480 --> 00:08:46,130
And then when they discuss SPARC they'll say Hadoop versus SPARC but really it's map reduce that you're

114
00:08:46,130 --> 00:08:47,710
comparing with SPARK.

115
00:08:47,760 --> 00:08:53,120
Spark can use Hadoop striated file system which is why it doesn't really make sense to say SPARC versus

116
00:08:53,120 --> 00:08:58,490
Hadoop by itself because SPARC can actually work with the Hadoop disputer file system but SPARC can

117
00:08:58,490 --> 00:09:01,460
also work with a wide variety of data formats.

118
00:09:01,460 --> 00:09:04,510
Which is why we kind of always hook up Hadoop with map reduce.

119
00:09:04,520 --> 00:09:10,690
But SPARC can work with not just Hadoop distributed file system but quite a wide variety of file formats.

120
00:09:10,940 --> 00:09:16,400
But more importantly spark can also perform operations up to 100 times faster than map reduce.

121
00:09:16,400 --> 00:09:20,450
And that's one of the big selling points of spark and that's what you hear a lot of times when you're

122
00:09:20,450 --> 00:09:22,150
reading about Sparke online.

123
00:09:22,160 --> 00:09:25,760
So the question is how does it actually achieve the speed 100 times faster.

124
00:09:25,760 --> 00:09:27,920
Is a huge increase in performance.

125
00:09:28,590 --> 00:09:35,430
Well map reduce writes Most data to disk after each map and reduce operation Sparke will keep most of

126
00:09:35,430 --> 00:09:36,590
that data in memory.

127
00:09:36,600 --> 00:09:42,210
That is something like Ramm after each transformation and spark can spill over to disk if the memory

128
00:09:42,210 --> 00:09:46,230
is filled so if you don't have enough RAM on that single local machine it can spill over to the hard

129
00:09:46,230 --> 00:09:46,690
drive.

130
00:09:46,860 --> 00:09:53,010
But the main idea here is that as computers and ram have gotten cheaper it makes sense to just put everything

131
00:09:53,010 --> 00:09:56,600
on to ram instead of writing and reading to the hard drive.

132
00:09:56,610 --> 00:10:01,140
And that's one of the main differences between spark and map reduce as kind of an high level overview

133
00:10:02,700 --> 00:10:08,820
now stuck about just SPARC in particular at the core of SPARC is the idea of an already very resilient

134
00:10:08,820 --> 00:10:15,570
distributed data set and really at the stream that data set or our DD has four main features a distributed

135
00:10:15,570 --> 00:10:21,930
collection of data fault tolerance parallel operation the ability to be partitioned and the ability

136
00:10:21,930 --> 00:10:27,730
to use many data sources not just be limited to something like Hadoop subunit file system.

137
00:10:27,930 --> 00:10:29,260
So what does it actually look like.

138
00:10:29,280 --> 00:10:34,200
Well usually you have something that looks actually really similar to the map reduce framework that

139
00:10:34,200 --> 00:10:35,370
we talked about earlier.

140
00:10:35,400 --> 00:10:38,980
You have some sort of driver program in a spark context or even a spark session.

141
00:10:39,120 --> 00:10:43,260
When we talk about SPARC 2.0 and you have some sort of cluster manager and then you're working nodes

142
00:10:43,320 --> 00:10:45,680
that distributed system.

143
00:10:45,680 --> 00:10:46,810
Now what does this actually look like.

144
00:10:46,810 --> 00:10:51,560
Physically we'll have something that looks like this a bunch of servers somewhere or maybe AWOS is your

145
00:10:51,560 --> 00:10:52,410
provider of them.

146
00:10:52,440 --> 00:10:54,200
You just link to them online.

147
00:10:54,210 --> 00:10:59,200
You have this master node and sending back tasks and the slaves sent back results.

148
00:10:59,690 --> 00:11:02,800
And on those worker nodes are sometimes called Slave nodes.

149
00:11:02,990 --> 00:11:11,960
They have the data cache to ram slash disk so these are immutable lazily evaluated and cashable and

150
00:11:11,960 --> 00:11:14,670
there's basically two types of SPARC operations.

151
00:11:14,690 --> 00:11:20,360
There's transformations and actions transformations are basically a recipe to follow and then actions

152
00:11:20,390 --> 00:11:26,690
actually perform what the recipe says to do and then return something back and this behavior of transformations

153
00:11:26,690 --> 00:11:32,660
versus actions also carries over to the syntax when coding a lot of times when you write a method a

154
00:11:32,660 --> 00:11:37,310
call off of a data frame which are going to be working with Paice SPARC you won't see anything as a

155
00:11:37,310 --> 00:11:40,590
result until you call in action something like show.

156
00:11:40,970 --> 00:11:45,530
And this makes sense because if you're working with a really large data set you don't want to constantly

157
00:11:45,530 --> 00:11:50,750
be calculating all the transformations maybe transformation can be something like take the average or

158
00:11:50,750 --> 00:11:56,630
take the count of particular data or show me where column x is greater than the number 2 x that are

159
00:11:56,700 --> 00:11:57,330
like that.

160
00:11:57,650 --> 00:12:02,120
But you don't want to actually calculate that every time until you're sure you want to perform it because

161
00:12:02,120 --> 00:12:03,450
it's such a huge dataset.

162
00:12:03,590 --> 00:12:07,670
It's quite a task to calculate everything every time you type something.

163
00:12:07,670 --> 00:12:12,450
So that's why everything is separated between transformations and then those calls to action.

164
00:12:12,590 --> 00:12:18,120
And we'll see this more when we actually begin coding of Pi Spahr and Sparke data frame's Now there's

165
00:12:18,130 --> 00:12:23,500
one last point of confusion that arises that I want to clear up right now when discussing the spark

166
00:12:23,500 --> 00:12:29,090
syntax that is what you're actually going to be coding the syntax of the code you type into a keyboard.

167
00:12:29,240 --> 00:12:33,940
You will often see our DD syntax versus data frame Syntec show up.

168
00:12:33,940 --> 00:12:39,800
Now with the release of SPARC 2.0 SPARC is moving towards what's known as a data frame based syntax.

169
00:12:39,940 --> 00:12:45,790
So if you work with something like pandas or R or maybe even like an Excel spreadsheet those are more

170
00:12:45,790 --> 00:12:48,250
like data frame you have columns and rows.

171
00:12:48,370 --> 00:12:53,350
But keep in mind that the way the files are being distributed physically can still be thought of as

172
00:12:53,350 --> 00:12:54,220
an RFID.

173
00:12:54,220 --> 00:12:59,870
The resilient distributed data set your data is still being stored in a resilient distributed manner.

174
00:12:59,890 --> 00:13:02,310
So art is still the way it's happening physically.

175
00:13:02,410 --> 00:13:05,710
Just the syntax of what you're working with is now called data.

176
00:13:05,920 --> 00:13:08,510
So it's just the type that syntax that is changing.

177
00:13:08,520 --> 00:13:12,880
All right so we covered a lot but I don't want you to worry if you didn't memorize all these details

178
00:13:12,910 --> 00:13:16,570
or if a lot of this went over your head and left you maybe a little confused.

179
00:13:16,570 --> 00:13:21,100
We're going to cover a lot of this again and again as we learn more about how to actually code of Python

180
00:13:21,100 --> 00:13:23,770
how to code a spark and utilize all these ideas.

181
00:13:23,800 --> 00:13:28,210
They should just give you kind of a very high level understanding of what a distributed system looks

182
00:13:28,210 --> 00:13:28,760
like.

183
00:13:28,930 --> 00:13:35,140
Why we actually phrase things as SPARC versus map reduce and maybe how SPARC works of art these really

184
00:13:35,170 --> 00:13:36,410
sort of very high level.

185
00:13:36,550 --> 00:13:40,810
But again don't worry if you don't memorize all this will going to cover this a lot more as we continue

186
00:13:40,810 --> 00:13:42,760
coding throughout the course.

187
00:13:42,760 --> 00:13:43,340
Okay.

188
00:13:43,480 --> 00:13:48,420
So a quick now on Sparke data frames which is what we're going to really focus here in the course Sparke

189
00:13:48,430 --> 00:13:53,290
data frames are now the standard way of using Spark's machine learning capabilities and Sparke data

190
00:13:53,280 --> 00:13:56,600
from documentation is still pretty new and can be a little sparse.

191
00:13:56,860 --> 00:14:01,420
I think it's actually getting better so we'll take a brief tour of the documentation just to show you

192
00:14:01,420 --> 00:14:06,490
what you can expect and how we're going to cover this course and it kind of blend in the documentation.

193
00:14:06,490 --> 00:14:11,950
So you have the skills to reference that documentation because it could be a little mysterious at first.

194
00:14:12,130 --> 00:14:17,050
So I'm going to jump over to spark that especially the org in my browser and you can just Google Sparke

195
00:14:17,050 --> 00:14:21,490
or Apache and it'll probably take you there and we're going explore the documentation together for just

196
00:14:21,490 --> 00:14:22,080
a little bit.

197
00:14:23,320 --> 00:14:27,140
All right here I am at Sparks that a patch that you can already see here.

198
00:14:27,140 --> 00:14:29,990
They tell you that it can run programs up to 100 times faster.

199
00:14:29,990 --> 00:14:36,080
And note here it says Hadoop map reduce not just Hadoop and it runs even 10 times faster on disk.

200
00:14:36,230 --> 00:14:42,140
That's when it goes to the harddrive and it's also ease of use so we can write applications in not just

201
00:14:42,200 --> 00:14:45,100
Java or Skala but we can also do it in Python and our.

202
00:14:45,350 --> 00:14:49,300
And as I mentioned we're going to be working with Python in this course and then also a generality so

203
00:14:49,300 --> 00:14:54,500
it can combine sequel streaming in complex analytics etc. but it's going to scroll up here and talk

204
00:14:54,500 --> 00:14:55,780
about where we're going to be working with.

205
00:14:55,970 --> 00:14:58,930
This course is a work with SPARC 2.1.

206
00:14:59,150 --> 00:15:04,300
And if you click here on the latest release documentation it'll take you to link a spark overview.

207
00:15:04,300 --> 00:15:09,010
And this work kind of describes downloading many examples in the show launching on a cluster where to

208
00:15:09,010 --> 00:15:10,740
go from here QuickStart etc..

209
00:15:10,990 --> 00:15:16,120
And as we explore Sparke more and more are eventually going to run into what's known as data frame's

210
00:15:16,120 --> 00:15:17,590
data sets and sequel.

211
00:15:17,800 --> 00:15:21,250
This is going to be one of the first things we cover and this is essentially the way you can work with

212
00:15:21,250 --> 00:15:25,930
Sparke with dataset so basic data analysis cleaning up your data manging data were going to be working

213
00:15:25,930 --> 00:15:29,430
with Sparke sequel Ensberg data frames and essentially spurts sequel.

214
00:15:29,470 --> 00:15:33,640
Is this platform where we build on top of ticket data sets and data frames.

215
00:15:33,640 --> 00:15:38,320
And the reason it's called Sparke sequel is because it has the ability to execute sequel queries so

216
00:15:38,320 --> 00:15:43,360
if you already know sequel you're going be able to use that sequel knowledge to filter out your data

217
00:15:43,390 --> 00:15:46,980
base off a sequel query and we'll cover that later on in this course.

218
00:15:47,160 --> 00:15:51,820
And as you continue on through the documentation you know there's these nice little tabs that show you

219
00:15:51,880 --> 00:15:55,570
the Skala Java Python and equivalent our code.

220
00:15:55,720 --> 00:15:59,460
Now not every operation is available in every language.

221
00:15:59,530 --> 00:16:04,150
Now something that we haven't really discussed yet is that SPARC itself is not a programming language

222
00:16:04,210 --> 00:16:09,220
it's just a framework for dealing with large data and distributing it and doing those calculations across

223
00:16:09,280 --> 00:16:14,800
a distributor network SPARC itself is written in a programming language known as Skala.

224
00:16:15,010 --> 00:16:21,490
So the Skala API for Sparc is the one that gets the latest features which makes sense because SPARC

225
00:16:21,490 --> 00:16:22,930
has literally written in Scala.

226
00:16:22,930 --> 00:16:24,140
It's going to get the latest features.

227
00:16:24,160 --> 00:16:29,190
Now Skala is actually written in Java so Java a lot of times also gets a lot of the latest features.

228
00:16:29,440 --> 00:16:34,110
The API has for Python and are are usually a little bit behind all the latest stuff.

229
00:16:34,120 --> 00:16:35,250
So keep that in mind.

230
00:16:35,350 --> 00:16:39,260
A lot of the more advanced things are the latest technologies with SPARC.

231
00:16:39,280 --> 00:16:44,320
Sometimes it takes maybe one generation or one more release cycle to get to Python and then maybe inexorably

232
00:16:44,320 --> 00:16:50,190
cycle to get to our creating data frames all this stuff and kind of see here it looks like a table we're

233
00:16:50,200 --> 00:16:53,280
going to be covering this an entire section talking about data frames.

234
00:16:53,350 --> 00:16:59,790
The last programming I do want to show you is this Madlib guide and if you click on that you basically

235
00:16:59,790 --> 00:17:01,500
see the machine learning library right.

236
00:17:01,520 --> 00:17:05,070
And this is something we're going to be referencing a lot throughout the course especially in that second

237
00:17:05,070 --> 00:17:06,330
half on machine learning.

238
00:17:06,540 --> 00:17:11,430
But you will also notice it has this RTT based API guide and this is kind of what I was referencing

239
00:17:11,430 --> 00:17:15,150
when I talked about ardy the syntax versus data frame syntax.

240
00:17:15,180 --> 00:17:19,520
And there's this big announcement with SPARC 2.0 are already on 2.1.

241
00:17:19,590 --> 00:17:25,740
The data frame based API is the primary API which is why we're going to be focusing exclusively on the

242
00:17:25,740 --> 00:17:27,300
data base the API.

243
00:17:27,420 --> 00:17:32,160
And honestly it's a lot easier to work with that makes a lot more sense than the ARDE based API so it's

244
00:17:32,160 --> 00:17:37,170
a welcome change and you can see here you can click on the main guide or check out particular topics

245
00:17:37,200 --> 00:17:42,100
maybe you're interested in clustering So you click on clustering or it's in and k means click here.

246
00:17:42,240 --> 00:17:47,620
It gives you some information about Kamins and Shoji examples for Escala Java Python and are.

247
00:17:48,120 --> 00:17:51,900
So that's definitely stuff we're all going to be talking about and focusing on a lot more in the second

248
00:17:51,900 --> 00:17:53,790
half of the course when we talk about machine learning.

249
00:17:53,790 --> 00:17:57,750
But I do want you to know that the documentation is all here for you.

250
00:17:57,780 --> 00:17:59,290
It's pretty good for Emelin.

251
00:17:59,460 --> 00:18:01,790
Maybe you're a student collaborative filtering say come over here.

252
00:18:01,790 --> 00:18:02,660
Scroll down.

253
00:18:02,680 --> 00:18:06,330
They show you a full example but they also have the API documentation.

254
00:18:06,360 --> 00:18:08,300
So click here on Python.

255
00:18:08,340 --> 00:18:09,800
This will take you to the API docs.

256
00:18:09,810 --> 00:18:12,450
So come over here Sparke sequel functions.

257
00:18:12,450 --> 00:18:17,130
That's something we're going to be using a lot and then it tells you basically the pure API as close

258
00:18:17,130 --> 00:18:20,910
as it can to the actual source code without clicking here where it says source if you click on that

259
00:18:21,150 --> 00:18:25,410
then we'll actually take you to the source code itself for further explanation.

260
00:18:25,410 --> 00:18:30,270
Now usually we'll just re ref referencing the documentation not the actual API docs.

261
00:18:30,270 --> 00:18:35,370
This is more of a heavy duty especially if you're hitting the source code button but this is about as

262
00:18:35,370 --> 00:18:39,690
far as we're going to get if you ever have questions or want to dive deeper into what certain functions

263
00:18:39,690 --> 00:18:39,970
do.

264
00:18:39,990 --> 00:18:42,080
You can check with the API docs.

265
00:18:42,090 --> 00:18:42,580
All right.

266
00:18:42,750 --> 00:18:47,340
So I know there was a lot but hopefully that kind of clears up some misunderstandings of spark and where

267
00:18:47,340 --> 00:18:48,180
we're going to be using it.

268
00:18:48,180 --> 00:18:49,110
What's its purpose.

269
00:18:49,110 --> 00:18:49,960
Cetera.

270
00:18:49,960 --> 00:18:55,020
I will see you in the next lecture will probably discuss how to actually install setup and get everything

271
00:18:55,020 --> 00:18:56,400
running on your computer.

272
00:18:56,400 --> 00:18:57,120
I'll see you there.
