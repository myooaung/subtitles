WEBVTT
1
00:00:05.460 --> 00:00:08.510
Hello everyone and welcome to the section on logistic regression.

2
00:00:08.640 --> 00:00:13.140
In this lecture we're going to describe the theory behind logistic regression as well as the theory

3
00:00:13.140 --> 00:00:18.500
behind classification metrics in general not all labels are continuous.

4
00:00:18.500 --> 00:00:21.010
Sometimes we need to do is predict categories.

5
00:00:21.140 --> 00:00:23.000
And this is known as classification.

6
00:00:23.240 --> 00:00:26.660
Logistic Regression is one of the basic ways to perform classification.

7
00:00:26.810 --> 00:00:30.620
And don't be confused by the word regression even though it's just that your Russian has that word in

8
00:00:30.620 --> 00:00:30.900
it.

9
00:00:30.980 --> 00:00:37.360
It is a classification algorithm if you want some theory behind the mathematics of logistic regression.

10
00:00:37.360 --> 00:00:40.800
Go ahead and read sections for 4.3 of an introduction.

11
00:00:40.920 --> 00:00:42.260
Mystical learning by Garreth.

12
00:00:42.260 --> 00:00:48.160
James if you want to fully understand some of the concepts behind the evaluation methods and metrics

13
00:00:48.160 --> 00:00:52.430
behind classification I really recommend that reading assignment.

14
00:00:52.460 --> 00:00:53.020
All right.

15
00:00:53.090 --> 00:00:58.190
So we want to learn about discretion as a method for classification and some examples of classification

16
00:00:58.190 --> 00:01:04.460
problems are things like separating spam emails versus ham or good emails trying to decide whether a

17
00:01:04.460 --> 00:01:09.950
customer is going to default on their loan or even diagnosing a disease for instance if someone has

18
00:01:09.950 --> 00:01:10.880
cancer or not.

19
00:01:11.060 --> 00:01:16.370
And all of those above examples are examples of what's known as binary classification where the classification

20
00:01:16.400 --> 00:01:18.130
is just between two classes.

21
00:01:19.630 --> 00:01:21.770
And so far we've only seen regression problems.

22
00:01:21.770 --> 00:01:26.260
We're trying to predict the continuous value although the name may be confusing at first.

23
00:01:26.260 --> 00:01:30.910
Logistic Regression allows us to solve classification problems where we are trying to predict discrete

24
00:01:30.940 --> 00:01:32.010
categories.

25
00:01:33.120 --> 00:01:37.890
The convention for binary classification is to have two classes at 0 and 1.

26
00:01:37.920 --> 00:01:42.480
And we're going to walk through the basic idea a logistic regression will also explain why it has a

27
00:01:42.480 --> 00:01:49.020
term regression in it even though it's used for classification imagine we plotted out some categorical

28
00:01:49.020 --> 00:01:51.770
data against one of its features.

29
00:01:51.810 --> 00:01:57.600
So on the x axis we have some representative feature that goes from negative 10 all the way to 10 and

30
00:01:57.600 --> 00:02:01.630
the y axis represents the probability of belonging to class 1.

31
00:02:01.860 --> 00:02:07.920
So you can see here the blue points belong to class 0 which means they have zero probability of belonging

32
00:02:07.920 --> 00:02:12.670
to class 1 and the orange points on top belong to class 1 to the have one.

33
00:02:12.670 --> 00:02:18.930
There are 100 percent probability now we can't use a normal linear regression model on binary groups.

34
00:02:18.950 --> 00:02:20.540
It won't lead to a good fit.

35
00:02:20.630 --> 00:02:26.780
You can see here that we just don't fit so many of the points we need a function that will fit binary

36
00:02:26.780 --> 00:02:32.320
categorical data and it would be great if we can find the function with this sort of behavior something

37
00:02:32.320 --> 00:02:38.660
like this tries its best to match up to the different sides of the Y-axis.

38
00:02:38.700 --> 00:02:45.120
Luckily for us there is such a function the sigmoid also known as the logistic function takes in any

39
00:02:45.120 --> 00:02:48.720
value and that outputs it to be between 0 and 1.

40
00:02:48.810 --> 00:02:51.100
And you can actually see the formula there on top of this graph.

41
00:02:51.120 --> 00:02:53.670
And when you plot it out this is what it looks like.

42
00:02:53.670 --> 00:02:59.310
And that's basically means that no matter what value of Z you put into this function the output will

43
00:02:59.370 --> 00:03:02.910
always be between the numbers 0 and 1.

44
00:03:02.910 --> 00:03:08.340
And this means we can take a linear regression solution and place it into the sigmoid function or the

45
00:03:08.370 --> 00:03:09.610
logistic function.

46
00:03:10.490 --> 00:03:12.230
So what does this actually look like.

47
00:03:12.230 --> 00:03:17.730
Well basically we have our old linear model something that looks like y plus beta plus Beta 1 times

48
00:03:17.730 --> 00:03:24.230
X. And we insert it into the sigmoid function that one over one plus eats in the minus and then that's

49
00:03:24.230 --> 00:03:29.930
where our linear model goes which means no matter what we put into that linear model as an output once

50
00:03:29.930 --> 00:03:37.050
it goes into the sigmoid function the value will always be between 0 and 1 and the result in a probability

51
00:03:37.050 --> 00:03:44.800
from 0 to 1 belonging to the one class and then we can set a cutoff point at 0.5 to the anything below

52
00:03:44.800 --> 00:03:49.560
it results in class 0 and anything above is Class 1.

53
00:03:49.690 --> 00:03:54.730
We use the logistic function to output a value ranging from 0 to 1 and then based off this probability

54
00:03:54.910 --> 00:04:01.380
we can assign a class after training logistic regression model and some training data you'll evaluate

55
00:04:01.380 --> 00:04:06.180
your models performance on some test data and you can use a confusion matrix to evaluate classification

56
00:04:06.180 --> 00:04:06.860
models.

57
00:04:06.870 --> 00:04:09.020
So let's discuss that and a little more detail.

58
00:04:09.420 --> 00:04:11.810
So this is what a confusion matrix looks like.

59
00:04:11.820 --> 00:04:18.120
It's basically a cross diagram between the predicted condition or what you predicted the label to be

60
00:04:18.510 --> 00:04:22.370
versus the true condition what the true label actually was.

61
00:04:22.380 --> 00:04:25.440
So you can see this cross diagram we get various outputs.

62
00:04:25.460 --> 00:04:27.860
And let's go through the four main ones here.

63
00:04:27.960 --> 00:04:34.560
If our true condition was that the condition was positive for example we were testing for a disease

64
00:04:34.770 --> 00:04:39.930
and we true condition is that the person really did have the disease and it predicted condition what

65
00:04:39.930 --> 00:04:44.520
our model ended up outputting was that this person does have the disease as our prediction.

66
00:04:44.520 --> 00:04:49.890
Now we have what is known as a true positive which means the person does have the disease or the conditional

67
00:04:49.890 --> 00:04:50.780
is positive.

68
00:04:50.910 --> 00:04:54.090
And our prediction was that the person was positive for this disease.

69
00:04:54.090 --> 00:04:56.130
So that's a true positive value.

70
00:04:56.130 --> 00:04:59.310
Now let's imagine that the true condition was actually false.

71
00:04:59.310 --> 00:05:01.740
So this person doesn't actually have the disease.

72
00:05:01.770 --> 00:05:04.420
However we predicted for it to be positive.

73
00:05:04.740 --> 00:05:06.900
This means we have a false positive.

74
00:05:06.930 --> 00:05:09.460
Also known as a type 1 error.

75
00:05:09.900 --> 00:05:14.230
If we look on the other column the prediction negative we have the opposite set of values.

76
00:05:14.250 --> 00:05:20.970
For example if a person truly has a disease but we predict that they do not have the disease we have

77
00:05:20.970 --> 00:05:24.390
what's known as a false negative or a type 2 error.

78
00:05:24.390 --> 00:05:30.750
Likewise if the predicted condition is negative and the person is truly negative so the condition is

79
00:05:30.750 --> 00:05:34.170
negative then we have what's known as a true negative.

80
00:05:34.420 --> 00:05:40.680
And we can expand this matrix with ratios and these ratios are essentially all the valuation metrics

81
00:05:40.690 --> 00:05:43.480
we talk about when discussing classification.

82
00:05:43.480 --> 00:05:46.840
So some of the more common ones are things like accuracy.

83
00:05:46.840 --> 00:05:53.410
So you can see all the way in the left hand side at the bottom of that column we have accuracy and accuracy

84
00:05:53.500 --> 00:05:59.860
is the sum of the number of true positives plus the number of true negatives divided by the total population

85
00:06:00.310 --> 00:06:03.990
which basically means how many did you get right out of the total population.

86
00:06:04.240 --> 00:06:06.620
Now in certain cases that's not enough information.

87
00:06:06.640 --> 00:06:12.250
You also want to know things like the number of true positives divided by the number of conditional

88
00:06:12.250 --> 00:06:18.030
positives so that can be something like sensitivity or recall otherwise known as a true positive rate.

89
00:06:18.040 --> 00:06:23.110
Then there's other things like precision which is going to be the sum of the number of true positives

90
00:06:23.380 --> 00:06:25.960
divided by the prediction positive.

91
00:06:25.960 --> 00:06:28.980
So all of these metrics can be available to you.

92
00:06:29.110 --> 00:06:34.450
The main idea is that there are all ratios based off of counts divided by some sort of total population

93
00:06:34.480 --> 00:06:39.650
or some sort of total condition negative or total prediction positive prediction negative etc..

94
00:06:39.760 --> 00:06:41.320
So you don't need to memorize any of these.

95
00:06:41.320 --> 00:06:43.360
In fact this is just from the Wikipedia page.

96
00:06:43.360 --> 00:06:44.710
You can always reference it.

97
00:06:44.860 --> 00:06:51.590
All you have to be aware of is that the confusion matrix can lead to all these various metrics.

98
00:06:51.830 --> 00:06:56.030
Again the main point to remember of the confusion matrix and the various calculated metrics is that

99
00:06:56.030 --> 00:07:00.950
they are all fundamentally weights comparing the predicted values versus the true values.

100
00:07:00.950 --> 00:07:06.170
The question of what constitutes a good metric will really depend on the specific situation.

101
00:07:06.170 --> 00:07:08.610
In some cases accuracy is really important.

102
00:07:08.630 --> 00:07:11.900
In other cases things like precision and recall are really important.

103
00:07:11.900 --> 00:07:14.420
It really depends on the specific situation.

104
00:07:14.450 --> 00:07:18.990
So asking the question hey is this a good recall value or is this a good accuracy value.

105
00:07:19.010 --> 00:07:23.270
We're really the pen on the cost associated with each of those metrics.

106
00:07:24.360 --> 00:07:29.400
So let's walk through an example of how we could use a confusion matrix to evaluate our model and we're

107
00:07:29.400 --> 00:07:32.910
going to touch back again on the example of testing for a disease.

108
00:07:32.910 --> 00:07:37.130
And so that's one of the most common examples whenever we're discussing confusion matrix.

109
00:07:37.410 --> 00:07:44.580
So here we have our confusion matrix and we tested a total of 165 people or an equal to 65 and all the

110
00:07:44.580 --> 00:07:49.770
way on the left hand side we can see the actual truth whether or not this person has disease yes or

111
00:07:49.770 --> 00:07:50.250
no.

112
00:07:50.400 --> 00:07:52.980
And on the columns we can see what we predicted.

113
00:07:53.130 --> 00:07:54.630
No or Yes.

114
00:07:54.630 --> 00:07:59.460
So for example the test for the presence of the zees know is equal to a negative test which is equal

115
00:07:59.460 --> 00:08:00.930
to false or zero.

116
00:08:01.290 --> 00:08:06.810
And yes is equal to a positive test or true as equal to 1.

117
00:08:06.810 --> 00:08:10.040
So again the basic terminology can be things like true positives.

118
00:08:10.080 --> 00:08:12.390
In this case that's equal to 100.

119
00:08:12.580 --> 00:08:15.000
Yes some predictive Yes true negatives.

120
00:08:15.000 --> 00:08:18.350
In this case equal to 50 actual and predicted no.

121
00:08:18.450 --> 00:08:23.960
And then we have false positives and false negatives so false negatives is five where the actual is

122
00:08:23.970 --> 00:08:24.230
yes.

123
00:08:24.240 --> 00:08:25.260
But we predicted no.

124
00:08:25.260 --> 00:08:30.560
So a false negative and then false positives is equal to 10 where the actual is no.

125
00:08:30.630 --> 00:08:31.490
But we predicted.

126
00:08:31.500 --> 00:08:34.600
Yes.

127
00:08:34.640 --> 00:08:36.560
So the accuracy answer is the question.

128
00:08:36.590 --> 00:08:38.890
Overall how often is it correct.

129
00:08:38.990 --> 00:08:44.450
And we get that by summing the number of true positives versus the number true negatives over the total.

130
00:08:44.510 --> 00:08:48.380
And in this case we get 91 percent accuracy.

131
00:08:48.530 --> 00:08:51.810
The misclassification rate or error rate answers the question.

132
00:08:51.840 --> 00:08:53.720
Overall how often is it wrong.

133
00:08:53.880 --> 00:08:57.960
And you can do something like take the number of false positives plus the number of false negatives

134
00:08:57.990 --> 00:08:58.880
over that total.

135
00:08:59.100 --> 00:09:02.250
And that gives you a rate of around 9 percent.

136
00:09:02.270 --> 00:09:07.940
So if you ever get confused on the discussion between type 1 errors or type 2 errors aka false positives

137
00:09:07.940 --> 00:09:11.870
vs. false negatives This is a little handy funny picture for you to remember.

138
00:09:11.870 --> 00:09:13.780
So telling a man You're pregnant.

139
00:09:13.790 --> 00:09:18.200
Well that's definitely a false positive versus telling a woman who's obviously pregnant you're not pregnant

140
00:09:18.290 --> 00:09:22.050
that's a false negative aka type 1 error versus type 2 errors.

141
00:09:23.390 --> 00:09:26.290
If you're still confused on the confusion matrix no pun intended.

142
00:09:26.330 --> 00:09:27.680
That's really no problem.

143
00:09:27.680 --> 00:09:30.820
What I would suggest you do is check out the Wikipedia page for it.

144
00:09:30.830 --> 00:09:34.760
That's where all the diagrams are from and it actually has a really good diagram of all the formulas

145
00:09:34.760 --> 00:09:35.810
for all the metrics.

146
00:09:35.960 --> 00:09:39.860
And essentially what we're going to do is just refer to those in case you ever want clarification on

147
00:09:39.860 --> 00:09:40.340
them.

148
00:09:40.340 --> 00:09:44.000
And throughout the course what we're usually going to do is just print out one of the metrics such as

149
00:09:44.270 --> 00:09:50.720
accuracy or recall now binary classification which is what we've been discussing has some of its own

150
00:09:50.720 --> 00:09:56.660
special classification metrics and these usually include visualizations of metrics from the confusion

151
00:09:56.660 --> 00:10:03.560
matrix the receiver operator curve or our seeker was developed during World War II to help analyze radar

152
00:10:03.560 --> 00:10:03.980
data.

153
00:10:03.980 --> 00:10:07.570
So let's take a look at the Aros seeker of the Orosi curve.

154
00:10:07.580 --> 00:10:14.840
It's just the plot of the sensitivity versus one minus the specificity otherwise known as the true positive

155
00:10:14.840 --> 00:10:20.420
rate on that y axis plotted against the false positive rate.

156
00:10:20.520 --> 00:10:26.040
And if you want a general outline of what the Orosi curve looks like and how it can be evaluated.

157
00:10:26.190 --> 00:10:32.400
This diagonal line across the center in a red dashed line is essentially what a random guess is basically

158
00:10:32.400 --> 00:10:34.840
a 50/50 chance of getting it right.

159
00:10:34.950 --> 00:10:39.380
If you're above that line that means your model is performing better than a random guess.

160
00:10:39.510 --> 00:10:43.530
If you're below that line that means your models are performing worse than a random guess.

161
00:10:43.560 --> 00:10:48.630
And if you have absolute perfect classification then your model is eventually going to reach one over

162
00:10:48.630 --> 00:10:50.290
there and that's what your curve will look like.

163
00:10:50.370 --> 00:10:53.750
Basically right up against that y axis.

164
00:10:53.880 --> 00:10:58.590
So a full discussion of the Orosi curve is beyond the scope of this course but if you're really interested

165
00:10:58.590 --> 00:11:01.310
in it the reading assignment goes into much more detail.

166
00:11:01.500 --> 00:11:05.940
For now you just need to know that the area under the curve is a metric for how well a model fit the

167
00:11:05.940 --> 00:11:10.490
data with one being absolutely perfect fit.

168
00:11:10.510 --> 00:11:11.010
All right.

169
00:11:11.110 --> 00:11:15.730
Let's continue on exploring these concepts for a walk through of the documentation example and then

170
00:11:15.730 --> 00:11:19.420
we'll also add some more stuff on evaluation to the example.

171
00:11:19.420 --> 00:11:21.190
Thanks and I'll see at the next lecture.
