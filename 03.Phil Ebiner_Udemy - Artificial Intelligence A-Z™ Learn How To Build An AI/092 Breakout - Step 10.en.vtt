WEBVTT
1
00:00:00.330 --> 00:00:02.170
Hello and welcome to the Statoil.

2
00:00:02.400 --> 00:00:06.270
OK so we just computed the entropy and added it to the entropies list.

3
00:00:06.270 --> 00:00:11.640
And now what we're going to do is take a random drop of an action according to the distribution of probabilities

4
00:00:11.700 --> 00:00:13.190
of the next.

5
00:00:13.200 --> 00:00:14.540
So let's do this.

6
00:00:14.540 --> 00:00:15.780
That's the next step.

7
00:00:15.870 --> 00:00:19.910
We are still in the loop because we're still running on the steps here.

8
00:00:20.160 --> 00:00:22.550
And so you now know how to play the action.

9
00:00:22.590 --> 00:00:28.740
We will first introduce a variable for the action called action and then we take our distribution of

10
00:00:28.740 --> 00:00:37.350
probabilities and we're going to use the multi no neural function to take a random draw from this distribution

11
00:00:37.350 --> 00:00:41.390
of probabilities and then we add that data.

12
00:00:41.500 --> 00:00:48.550
So it's important to note that the action will actually be a tensor with only one value but you should

13
00:00:48.550 --> 00:00:51.010
not see this as a simple value.

14
00:00:51.010 --> 00:00:57.050
You should see this as a tensor damnations one by one that contains this value for the action.

15
00:00:57.190 --> 00:01:02.970
And that's because it isn't squeezed out still in the same for loop.

16
00:01:02.970 --> 00:01:09.880
We're going to get the log probability associated to the action that was just played.

17
00:01:10.170 --> 00:01:16.750
And so when I'm dating my luck probability here by taking the previous one the previous luck from that

18
00:01:16.810 --> 00:01:25.480
we computed here and then I'm going to use the other method to which I'm going to input 1 and the action

19
00:01:25.480 --> 00:01:31.510
that was just playing because we want to get the luck probability that is associated to this action.

20
00:01:31.510 --> 00:01:38.230
And so the second argument here I'm going to put my action but there has to be as a torture horrible

21
00:01:38.860 --> 00:01:44.530
as required by the gathered function and the gathered function just indexes with a tensor integer.

22
00:01:44.530 --> 00:01:48.910
All right so now we just got the look of associated to the action that was displayed.

23
00:01:49.030 --> 00:01:53.790
And now the next step is to append what we got to the list here.

24
00:01:53.800 --> 00:01:55.570
So we got the value.

25
00:01:55.750 --> 00:01:58.820
That's what we got here as the output of the model.

26
00:01:58.840 --> 00:02:00.880
Then we also got the lock problem.

27
00:02:00.910 --> 00:02:04.030
So we are going to add the lock to the lock props list.

28
00:02:04.180 --> 00:02:09.610
We already append the entropy to the entropy is less good and the rewards will get it afterwards.

29
00:02:09.700 --> 00:02:15.250
So we will now open to value and the look up to the values list and the law process.

30
00:02:15.520 --> 00:02:16.180
Let's do this.

31
00:02:16.180 --> 00:02:23.800
We take our values list we add that we use the spend function and we add the value that was returned

32
00:02:23.920 --> 00:02:32.700
by the model perfect then Same for the lock probs We just got our new props and we are going to append

33
00:02:32.710 --> 00:02:36.080
it to the lock props list.

34
00:02:36.180 --> 00:02:43.960
And so in this append function we can put a log from our luck probably it was just computed here.

35
00:02:43.960 --> 00:02:47.320
All right so our lists are now well updated.

36
00:02:47.350 --> 00:02:53.060
Now what we're going to do is play the action because actually right here we selected the action by

37
00:02:53.060 --> 00:02:56.570
taking a random draw from the distribution of probabilities here.

38
00:02:56.650 --> 00:03:03.040
But we actually haven't played it yet and we're going to play it now so that we can reach the new state

39
00:03:03.220 --> 00:03:06.150
and therefore get the new transition and to play it.

40
00:03:06.170 --> 00:03:10.960
We're going to take our environment because we play the action in our environment then we're going to

41
00:03:10.960 --> 00:03:12.990
use the step method.

42
00:03:13.210 --> 00:03:20.650
And inside we specify the action that was selected to play it and to do this we take our action and

43
00:03:20.650 --> 00:03:25.280
we add that none by because that's what is expected that is the function.

44
00:03:25.750 --> 00:03:35.820
Ok but this returns actually the new state and also the new reward because by reaching the new state

45
00:03:36.000 --> 00:03:43.500
we get a new reward and also we get a new value for Dunn to know if the game is done or not.

46
00:03:43.500 --> 00:03:49.180
All right so with this we play the action we reach a new state and we get a reward and we know if we're

47
00:03:49.200 --> 00:03:50.510
done with the game.

48
00:03:50.520 --> 00:03:52.740
And speaking of being done with the game.

49
00:03:52.990 --> 00:03:58.590
Well we're just going to add something here that will make sure that an agent is not stacked in some

50
00:03:58.590 --> 00:03:59.180
state.

51
00:03:59.280 --> 00:04:04.240
And to do that we're going to update that done very well the following way.

52
00:04:04.860 --> 00:04:11.910
Well it's going to be equal to done or we're going to add a condition saying that the episode of the

53
00:04:11.910 --> 00:04:19.200
game should not last too much time and we will see in the main function that there will be a max length

54
00:04:19.200 --> 00:04:21.960
parameter which will be equal to 10000.

55
00:04:22.170 --> 00:04:25.750
And we don't want an episode to last more than 10000 units.

56
00:04:25.860 --> 00:04:34.200
So we're going to hear episode length which is the length of an episode and we're going to write a condition

57
00:04:34.830 --> 00:04:43.250
larger than max episode Lex that we haven't actually said this at length.

58
00:04:43.250 --> 00:04:49.210
We are getting it from our parameters for an ending here Paramjit but Ramstad.

59
00:04:49.210 --> 00:04:50.600
Max is at length.

60
00:04:50.600 --> 00:04:59.330
So this means that if the game is done or the length of the episode is larger than the maximum length

61
00:04:59.330 --> 00:05:02.110
of episode set which will be equal to 10000.

62
00:05:02.270 --> 00:05:05.410
Well the game will be done and we will start a new game.

63
00:05:05.960 --> 00:05:08.040
OK so that's just a precaution.

64
00:05:08.180 --> 00:05:14.360
And speaking of precaution we're going to add another precaution to clamp the reward between minus 1

65
00:05:14.360 --> 00:05:15.400
and plus 1.

66
00:05:15.470 --> 00:05:20.450
We already got the we were here but we want to make sure that the reward is between minus 1 and plus

67
00:05:20.450 --> 00:05:20.960
1.

68
00:05:21.140 --> 00:05:27.740
And to do this we simply need to update the reward by doing this taking the max then taking the men

69
00:05:28.240 --> 00:05:31.060
of reward and 1.

70
00:05:31.190 --> 00:05:37.790
And here we take the max of the minimum of reward and 1 and minus 1 and that will make sure the reward

71
00:05:37.790 --> 00:05:40.040
is between minus one plus one.

72
00:05:40.160 --> 00:05:40.910
All right.

73
00:05:40.910 --> 00:05:42.180
So another percussion.

74
00:05:42.380 --> 00:05:49.070
And now we just want to check if the game is done in which case we will restart the environment.

75
00:05:49.220 --> 00:05:53.010
And why do we need to check that now it's because we just reached a new state.

76
00:05:53.090 --> 00:05:54.880
We just passed a new transition.

77
00:05:54.890 --> 00:05:58.010
So we need to check that after passing this new transition.

78
00:05:58.130 --> 00:06:06.860
Well the game is not done so if done again if done then in that case we will restart the environments

79
00:06:07.250 --> 00:06:14.180
by setting the episode length to zero.

80
00:06:14.330 --> 00:06:21.800
And also the state will be re-initialize to re-initialize as we take our environment and we use the

81
00:06:21.800 --> 00:06:25.270
reset function OK.

82
00:06:25.310 --> 00:06:29.040
Now we get out of this condition that was just checking.

83
00:06:29.230 --> 00:06:34.640
And now what we're going to do is since we reached a new state while this new state is right now and

84
00:06:34.640 --> 00:06:40.410
then by Ray because remember the states are the input images which originally are named by arrays.

85
00:06:40.570 --> 00:06:44.430
And so now what we have to do is to convert the new state into a tortured answer.

86
00:06:44.600 --> 00:06:50.410
So we're going to update our state and we're going to use the torch library.

87
00:06:50.630 --> 00:07:00.880
And of course the from non-Thai function to convert this non-payers state the input images into a torch

88
00:07:00.890 --> 00:07:01.800
sensor.

89
00:07:02.150 --> 00:07:03.150
Perfect.

90
00:07:03.260 --> 00:07:08.620
And now the last thing we need to do before getting out of this for loop that is the loop on our steps

91
00:07:08.810 --> 00:07:13.030
Well it's to of course spend the reward to the Watchlist.

92
00:07:13.040 --> 00:07:18.310
That's the last thing that needs to be updated we updated all the list here except for the reward.

93
00:07:18.320 --> 00:07:24.830
So we're going to do that right now we take our rewards and we use your brain function to append the

94
00:07:24.830 --> 00:07:28.110
last word that was just received perfect.

95
00:07:28.220 --> 00:07:36.110
And just before we get out of the for loop we just need to do one last check to check that if it's done

96
00:07:37.340 --> 00:07:39.490
then we want to stop the expiration.

97
00:07:39.530 --> 00:07:42.550
And so we're simply going to add here a break.

98
00:07:42.560 --> 00:07:48.590
Meaning that if it's done we stop the exploration and we directly move on to the next step which will

99
00:07:48.590 --> 00:07:56.930
be the update of the shared model and now we are done with this for now that the agent has done its

100
00:07:56.930 --> 00:07:57.980
exploration.

101
00:07:58.190 --> 00:08:04.910
It will update the shared model and we will take care of that in the next tutorial and so then I.
