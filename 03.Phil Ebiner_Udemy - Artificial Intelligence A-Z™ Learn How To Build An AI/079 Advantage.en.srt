1
00:00:00,730 --> 00:00:03,690
Hello and welcome back to the course on artificial intelligence.

2
00:00:03,730 --> 00:00:07,440
In today's story we're talking about the final a in a 3C.

3
00:00:07,450 --> 00:00:09,400
We're talking about advantage.

4
00:00:09,550 --> 00:00:10,340
So there it is.

5
00:00:10,360 --> 00:00:14,040
We've already spoken about actor critic and a synchronous previously.

6
00:00:14,260 --> 00:00:20,500
And so he built a way to what we're going to be looking at today and with advantage we're going to put

7
00:00:20,500 --> 00:00:21,560
everything together.

8
00:00:21,730 --> 00:00:28,750
So this is what we have so far we've got a neural network which is shared between the agents the asynchronous

9
00:00:28,840 --> 00:00:33,550
agents and then we've got the critic which is also shared between age and so.

10
00:00:33,580 --> 00:00:36,490
How does this all play out and why is this critic shared between the agents.

11
00:00:36,500 --> 00:00:37,390
Let's have a look at that.

12
00:00:37,600 --> 00:00:42,250
Well understands better we're going to look at an example we're going to look at this agent for instance

13
00:00:42,280 --> 00:00:46,730
and see what happens when he's in a certain state and he needs to make a decision what action to play.

14
00:00:46,840 --> 00:00:53,890
So this agent is in a state he sees this image and then what happens is this information goes into the

15
00:00:53,980 --> 00:00:59,200
neural network it goes to the convolutional lair then goes into the pooling lair then it goes into the

16
00:00:59,200 --> 00:01:04,330
flattening lair and then from there it goes into the hidden layers of the neural network and then as

17
00:01:04,330 --> 00:01:09,870
an output he gets all of these policy values that you values are the policy.

18
00:01:09,970 --> 00:01:14,160
And also he gets the value the critic.

19
00:01:14,230 --> 00:01:21,640
And so as we know neural networks in order to operate they need to propagate certain errors or losses

20
00:01:21,640 --> 00:01:22,720
back through the network.

21
00:01:22,720 --> 00:01:29,020
So this way in order to update the weights so what waits or so what losses are we going to be dealing

22
00:01:29,080 --> 00:01:29,520
with here.

23
00:01:29,590 --> 00:01:30,540
Well we're two losses.

24
00:01:30,540 --> 00:01:32,970
We've got the value loss and the postals.

25
00:01:32,980 --> 00:01:38,360
So the value loss is linked to the value partial loss is linked to pools and so valuable.

26
00:01:38,400 --> 00:01:41,080
We've already dealt with it before.

27
00:01:41,260 --> 00:01:48,850
We know that we have rewards and we know that we have a discount factor so basically this is very similar

28
00:01:48,850 --> 00:01:54,690
to what we were talking about in the conversion in deep learning tutorials.

29
00:01:54,730 --> 00:02:03,190
Basically the network predicts a certain value V and at the same time we can estimate what should be

30
00:02:03,190 --> 00:02:09,160
based on what we know about the environment so far we can estimate what should the value be in the state

31
00:02:09,190 --> 00:02:13,350
and by comparing the two we can calculate the value loss and then back propagator network update the

32
00:02:13,360 --> 00:02:13,730
weights.

33
00:02:13,870 --> 00:02:17,670
So that's that's bracing for the new thing here is the policy loss.

34
00:02:17,770 --> 00:02:21,700
And so what is this policy loss and how does it work.

35
00:02:21,700 --> 00:02:28,960
Well this is the part where this whole situation where the critic is shared between the actors or between

36
00:02:28,960 --> 00:02:32,520
the agents is going to finally emerge.

37
00:02:32,590 --> 00:02:38,920
So to understand palsu loss we need to introduce a value called Advantage hence the name of this part

38
00:02:39,340 --> 00:02:45,460
of the story on this whole part of the Salyut the advantage and the advantage is calculated as Q of

39
00:02:45,480 --> 00:02:47,900
as an A minus V of s.

40
00:02:47,920 --> 00:02:54,550
So basically the Q value or that you chose to play all of the action that you chose to play in the state

41
00:02:54,550 --> 00:02:57,420
that you are in state S minus the value of that state.

42
00:02:57,640 --> 00:03:00,670
So this is the difference between the two and that is called that one.

43
00:03:00,670 --> 00:03:04,950
And the advantage is used in the calculation of the pulseless.

44
00:03:04,990 --> 00:03:10,510
Now we won't go into the formula of the pulseless calculation because it's quite complex it uses entropy

45
00:03:10,710 --> 00:03:12,860
or you can use entropy doesn't have to.

46
00:03:12,990 --> 00:03:17,020
We're not going to dissect that formula but we're going to understand this on an intuitive level.

47
00:03:17,020 --> 00:03:21,470
Why are we doing this why are we calculating this advantage and how is it going to help us.

48
00:03:21,640 --> 00:03:24,190
Well let's look at this premise for a second.

49
00:03:24,190 --> 00:03:33,040
The Q value here comes from what the neural network predicted for this agent and so predictive of in

50
00:03:33,040 --> 00:03:37,840
this specific action in this specific state for the action that it can play so it's got these actions

51
00:03:38,110 --> 00:03:45,790
and it can slide one of them and it can play it well whereas the Wii value is the value that is dictated

52
00:03:45,790 --> 00:03:46,480
by the critic.

53
00:03:46,480 --> 00:03:52,320
It is the value that we have here in this shared part and that's the key here that this part is shared

54
00:03:52,330 --> 00:03:56,310
so the critic break because this is how the credit comes into play.

55
00:03:56,470 --> 00:04:01,300
Because we've got a value that we choose or the action that we choose to play for this agent in that

56
00:04:01,300 --> 00:04:01,930
state.

57
00:04:01,990 --> 00:04:09,340
But then the critic can tell us what is the known value of that state what is overall the known value

58
00:04:09,340 --> 00:04:15,790
for this whole group of of agents that are performing together because their sharing doesn't answer

59
00:04:15,820 --> 00:04:21,280
because the initial B because they're sharing the critic they're all contributing to this to these v

60
00:04:21,280 --> 00:04:25,690
values that are being calculated for a different set so the whole a.z algorithm says OK.

61
00:04:25,690 --> 00:04:34,060
So the critic knows a v value how much better is your q value that you're selecting compared to the

62
00:04:34,060 --> 00:04:35,290
known v value.

63
00:04:35,350 --> 00:04:36,480
That's what it's saying.

64
00:04:36,640 --> 00:04:37,930
So that's basically it.

65
00:04:37,930 --> 00:04:45,430
So that I'm going to select a q value here based on my my policy based on whether whatever we use like

66
00:04:45,700 --> 00:04:50,150
a soft max function or on or a Epsilon Grealy policy or something like that.

67
00:04:50,170 --> 00:04:55,660
And of course we'll be out exploration plus exploitation combined in there but we selected Q value and

68
00:04:55,660 --> 00:04:59,260
now the question is what is the extra.

69
00:04:59,290 --> 00:05:05,820
What does that scolded ones what is the advantage that your selected action brings compared to the known

70
00:05:05,820 --> 00:05:12,480
value of that state and that is the essence of the advantage and basically then that is used to calculate

71
00:05:12,480 --> 00:05:17,940
the policy loss and then the policy loss is then back propagated through back to the network.

72
00:05:17,940 --> 00:05:23,910
So they're both back propagate through a network and the weights are adjusted in order for the network

73
00:05:23,910 --> 00:05:28,280
to better represent the value of the critic and also so that's the top part.

74
00:05:28,410 --> 00:05:33,030
But then this also part of the key here is that the value the weights are bakra.

75
00:05:33,120 --> 00:05:40,530
When is this post-offices back forget that rates are adjusted in such a way so that this advantage is

76
00:05:40,590 --> 00:05:46,200
maximized so like that's that's intuitive side of the intuitive understanding of it that we are back

77
00:05:46,350 --> 00:05:52,010
again this policy last through the network in order to help maximize this advantage.

78
00:05:52,050 --> 00:05:57,660
And what that means is basically that when an agent comes across bad actions like actions where the

79
00:05:57,660 --> 00:06:00,930
q values less than the known value for the state.

80
00:06:00,930 --> 00:06:07,110
So basically the whole ATC algorithm knows that the value for the state is something X and then all

81
00:06:07,110 --> 00:06:11,690
of a sudden you came across a very bad action and the and you did it you chose about action.

82
00:06:11,700 --> 00:06:17,130
And what that means for the enthusiasm is that well why would we do something like that when its worse

83
00:06:17,130 --> 00:06:21,960
than we already what we already know about this whole environment and what could could have done.

84
00:06:22,080 --> 00:06:23,500
So we shouldn't do more of that.

85
00:06:23,610 --> 00:06:27,670
And therefore the weights are just in a way so that happens is rarer.

86
00:06:27,690 --> 00:06:29,900
So that happens less rare.

87
00:06:30,300 --> 00:06:33,430
So thats a less frequent occurrence that we choose that bad action.

88
00:06:33,510 --> 00:06:38,880
On the other hand if you choose a very good action where q value is greater than V or much greater then

89
00:06:39,050 --> 00:06:43,220
where during this backwardation of the Polish loss through the network the weights are going to be update

90
00:06:43,230 --> 00:06:50,780
and in such a way to really reinforce that to encourage reassure that to happen again so that the weights

91
00:06:50,790 --> 00:06:55,140
will be adjusted in such a way that so the atresia algorithm will think oh well that does really cool

92
00:06:55,140 --> 00:06:56,790
that Wantage was very high there.

93
00:06:56,880 --> 00:07:03,750
I should do more of that and therefore you will update the weights in such a way that will be more likely

94
00:07:03,750 --> 00:07:05,550
to occur in the future that action.

95
00:07:05,560 --> 00:07:13,080
So and therefore that is you know thats how the network is slowly slowly going to adapt and slowly going

96
00:07:13,080 --> 00:07:19,770
to construct itself into something that on one hand calculates the value correctly and then on the other

97
00:07:19,770 --> 00:07:21,990
hand or as correct as possible.

98
00:07:21,990 --> 00:07:28,330
And on the other hand it encourages or it has actions which have a high advantage.

99
00:07:28,380 --> 00:07:30,570
So there we go that's that's this part.

100
00:07:30,570 --> 00:07:35,930
And now let's have a look at another one just to reinforce what we just discussed at the top 1.

101
00:07:36,000 --> 00:07:37,210
So same thing here.

102
00:07:37,350 --> 00:07:45,330
The top agencies a situation a state is in a state and then needs to decide what to do so since this

103
00:07:45,330 --> 00:07:50,100
information to the networks of this image use internet regrows to convolutional they're pulling their

104
00:07:50,100 --> 00:07:56,290
flattening Lehre goes into the hidden layers and then from here we get an output we get the acute values

105
00:07:56,310 --> 00:08:00,960
of the policy we get the V values again the same thing we've got two losses.

106
00:08:00,960 --> 00:08:05,240
We've got the value loss which is here Polish loss which is here value loss.

107
00:08:05,250 --> 00:08:06,830
We already know how is calculated.

108
00:08:06,960 --> 00:08:10,950
When we discussed this in the deep Q learning and just discussing just now as well.

109
00:08:10,950 --> 00:08:17,340
So that's how the value was calculated and then the policy loss again in order to calculate that which

110
00:08:17,340 --> 00:08:18,370
we're not going to go into for him.

111
00:08:18,390 --> 00:08:25,530
But on an intuitive level we're calculating that advantage which is okay so we took a certain action

112
00:08:25,530 --> 00:08:31,140
we chose a certain action based on our selection policy whether it's soft Max or upselling greedy or

113
00:08:31,170 --> 00:08:34,470
whatever other social policy that we're using.

114
00:08:34,620 --> 00:08:42,630
And then what's the action we took Now let's compare it to the known and value of the state which comes

115
00:08:42,630 --> 00:08:47,670
from the shared critics so this critic is kind of like if you think about it's kind of observing all

116
00:08:47,670 --> 00:08:52,950
of these agents at the same time is looking at this one look at this one this one they're all contributing

117
00:08:52,950 --> 00:08:58,140
towards a critic to get the critic more up to speed with the environment to make sure that the critic

118
00:08:58,380 --> 00:09:03,600
is representative of what's going on in the actual environment so that the weights.

119
00:09:03,660 --> 00:09:10,170
This is that's where the value loss comes in so that the weights of the actual neural network that they

120
00:09:10,260 --> 00:09:17,700
reflect very well the actual situation of things in the environment so that they can then rely on this

121
00:09:17,700 --> 00:09:20,090
value and then use it here.

122
00:09:20,090 --> 00:09:21,550
And so basically.

123
00:09:21,720 --> 00:09:26,880
So all of these agents all of these agents are contributing to this critic.

124
00:09:26,880 --> 00:09:32,850
But then at the same time through through this valueless but at same time the critic is observing the

125
00:09:32,850 --> 00:09:35,490
decisions or the policies of these agencies.

126
00:09:35,490 --> 00:09:39,960
It's like it's like going looking back at the like I'm trying to draw like an arrow to the poles an

127
00:09:39,960 --> 00:09:40,740
arrow an arrow.

128
00:09:40,830 --> 00:09:45,780
So looking back at them at the decision that they're making is criticizing these decisions through that

129
00:09:45,780 --> 00:09:50,210
vantage and saying OK you made a decision you chose this you chose this action.

130
00:09:50,220 --> 00:09:51,240
That's great.

131
00:09:51,240 --> 00:09:56,570
Now let's calculate the advantage or disadvantage ranch's is a equals you know the Q value of might

132
00:09:56,590 --> 00:10:04,040
have made the decision I made or the choice I made to the I made chose to take minus the known value

133
00:10:04,100 --> 00:10:05,100
to the critic.

134
00:10:05,110 --> 00:10:06,470
Not about the critic.

135
00:10:06,470 --> 00:10:12,010
So Kalika the difference if it's a low difference you're Polish then when you polish your losses back

136
00:10:12,020 --> 00:10:16,580
propagated through the network the way it's going to be adjusted is going to encourage the weights to

137
00:10:16,580 --> 00:10:22,010
be adjusted in such a way that that doesn't happen again that that Q value or that Q value is going

138
00:10:22,010 --> 00:10:28,910
to be lower so that because our policy selects the actions based on the q values the higher the Q value

139
00:10:28,910 --> 00:10:30,560
the more likely it's going to be selected.

140
00:10:30,560 --> 00:10:35,210
So if we were using like an arc Max policy then we just always select the one of the highest as you

141
00:10:35,360 --> 00:10:38,870
remember we discussed this then we'd always select the one with the highest value.

142
00:10:38,870 --> 00:10:43,790
But we actually were using a probabilistic approach where I was using like soft Max or upselling greedy

143
00:10:43,790 --> 00:10:44,420
policy.

144
00:10:44,540 --> 00:10:49,010
And then we were basically selecting where we can select any one of them but the higher the cube the

145
00:10:49,010 --> 00:10:49,220
better.

146
00:10:49,220 --> 00:10:55,310
So if we selected something and then the advantage was very low then bomb the network is going to be

147
00:10:55,310 --> 00:11:01,970
added in such a way that next time the value of that certain action is going to be less and maybe something

148
00:11:01,970 --> 00:11:02,910
else will be more.

149
00:11:02,960 --> 00:11:09,770
So that's how that is split out and on the other hand if we select something where that advantage is

150
00:11:09,770 --> 00:11:15,050
going to be high then this is going to go into the policy laws and then the networks and we update it

151
00:11:15,060 --> 00:11:19,590
so that is more commonly observed event like scenario.

152
00:11:19,640 --> 00:11:26,300
And so basically this whole Polish loss helps the network adapt or morph in such a way that we do.

153
00:11:26,300 --> 00:11:31,620
Moral of the good stuff good good actions and good things and do less of the bad things.

154
00:11:31,700 --> 00:11:35,040
And that's how these two losses come into play and that's how they're back appropriate.

155
00:11:35,120 --> 00:11:41,800
So hopefully that clears up in a very intuitive way of course we didn't go into the formulas into into

156
00:11:41,810 --> 00:11:46,930
the complex mathematics behind all of this and like into the very intricate details.

157
00:11:47,060 --> 00:11:50,940
But at the same time hopefully on the intuitive way in an intuitive way.

158
00:11:50,990 --> 00:11:58,850
All of this clears up why we have the actor and the critic and how they interact together that you know

159
00:11:58,850 --> 00:12:03,780
you have these agents asynchronous or synchronous side of things.

160
00:12:03,790 --> 00:12:08,480
Then this is this is your actor and critic and this is the advantage and how that all comes into play.

161
00:12:08,480 --> 00:12:10,840
So these are synchronous agents.

162
00:12:10,850 --> 00:12:11,920
They're going.

163
00:12:12,040 --> 00:12:18,740
They're playing this or exploring the environment and working through environment and they're all altogether

164
00:12:18,740 --> 00:12:26,720
contributing to a critic which is then observing their policies observing the actors which is what this

165
00:12:26,720 --> 00:12:27,400
is called.

166
00:12:27,560 --> 00:12:34,610
And through the through that vantage and therefore coming up this poses a loss and then policy and value

167
00:12:34,610 --> 00:12:38,210
loss or back propagate to just the network in order to.

168
00:12:38,240 --> 00:12:43,540
On one hand represent the true way of things in the environment.

169
00:12:43,580 --> 00:12:47,630
Another hand to improve the actors performances.

170
00:12:47,930 --> 00:12:48,410
So there we go.

171
00:12:48,410 --> 00:12:52,800
That's that's a quick recap of the intuition we discussed.

172
00:12:52,790 --> 00:12:58,550
Once again hopefully this is all coming together on an intuitive level and of course in the practical

173
00:12:58,550 --> 00:13:04,880
tutorials We'll talk more about how all of this works in Atlanta we'll walk you through this process

174
00:13:04,880 --> 00:13:05,560
of building owners.

175
00:13:05,570 --> 00:13:11,240
But having this this image in your mind and this like kind of like a roadmap of everything how it comes

176
00:13:11,240 --> 00:13:17,240
together will be well should be I hope will be very helpful for you to better navigate the practical

177
00:13:17,240 --> 00:13:18,160
side of things.

178
00:13:18,290 --> 00:13:25,710
And in terms of additional reading for today we've got two elements so first one is on the advantage.

179
00:13:25,760 --> 00:13:32,480
So here we've got high demential continuous controlling using generalized advantage estimation by John

180
00:13:32,480 --> 00:13:38,930
Shulman and this is an image of a stick figure getting up like standing up.

181
00:13:38,930 --> 00:13:44,030
And here you can find even more about advantaged and advantage and you'll find all the different types

182
00:13:44,030 --> 00:13:44,960
of advantages.

183
00:13:44,990 --> 00:13:50,870
You've got the general advantage in estimation and you've got advantages that you use actually in the

184
00:13:51,110 --> 00:13:56,360
in the forms in the calculations so if you want to find out more about advantage and exactly how it

185
00:13:56,360 --> 00:14:05,330
works the formulas behind it and some of the the top top elements or formulas and no holes in the space

186
00:14:05,330 --> 00:14:06,810
of this advantage.

187
00:14:06,830 --> 00:14:08,700
We discussed them.

188
00:14:08,720 --> 00:14:10,140
This is the article to go to.

189
00:14:10,490 --> 00:14:19,880
And one more one other element that or piece of work that we wanted to remind you about is the blog

190
00:14:19,990 --> 00:14:24,350
a series of blog post by Arthur Giuliani which we've mentioned a couple times already.

191
00:14:24,350 --> 00:14:28,010
This is part eight which is specifically about A-3 sea.

192
00:14:28,280 --> 00:14:33,280
So here you can get a nother explanation.

193
00:14:33,650 --> 00:14:38,900
So with a bit more mathematics about what is going on and you maybe you can pick up some additional

194
00:14:38,900 --> 00:14:39,420
things from here.

195
00:14:39,440 --> 00:14:45,470
Just two things to keep in mind first of all as always this blog is intenser followers we're using pi

196
00:14:45,470 --> 00:14:46,130
torch.

197
00:14:46,130 --> 00:14:46,840
So keep that in mind.

198
00:14:46,840 --> 00:14:53,330
And the second thing is that the way we structured our approach is we talked about active critic first

199
00:14:53,960 --> 00:14:55,730
then we talked about a synchronous.

200
00:14:55,730 --> 00:15:03,040
And then we talked about advantage whereas in log Arthur's first talks about a Synchronoss an actor

201
00:15:03,050 --> 00:15:07,260
critic an advantage so keep that in mind so hopefully that doesn't throw you off.

202
00:15:07,340 --> 00:15:10,480
But other than that of course is a great piece of content.

203
00:15:10,490 --> 00:15:14,510
And we do highly recommend checking it out for some additional information.

204
00:15:14,780 --> 00:15:16,840
So there we go hopefully enjoy today's tutorial.

205
00:15:16,880 --> 00:15:18,710
And I look forward to seeing you next time.

206
00:15:18,710 --> 00:15:20,240
Until then enjoy.

207
00:15:20,260 --> 00:15:20,520
I.
