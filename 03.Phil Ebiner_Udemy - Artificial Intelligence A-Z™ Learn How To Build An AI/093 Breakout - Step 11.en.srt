1
00:00:00,760 --> 00:00:02,520
Hello and welcome to the Statoil.

2
00:00:02,830 --> 00:00:10,050
So now the agent has done its exploration and then when he's about to do is to update the shared network.

3
00:00:10,240 --> 00:00:17,050
So the first thing we're going to do is initialize the cumulative what we're going to call it our capital

4
00:00:17,070 --> 00:00:25,180
R and we will initialize it as a torch tensor but that will have dimensions one by one because it's

5
00:00:25,180 --> 00:00:27,710
just a value but we wanted to be a tensor.

6
00:00:27,940 --> 00:00:33,460
And so I'm using here but zeros and then 1 1.

7
00:00:33,790 --> 00:00:37,230
So basically the cumulative reward is initialized to 0.

8
00:00:37,610 --> 00:00:43,870
OK then saying if we're not done that is if the game is not over.

9
00:00:43,870 --> 00:00:50,200
What we want right now is the cumulative reward to be equal to the value of the last trade reached by

10
00:00:50,200 --> 00:00:51,750
the shared network.

11
00:00:51,820 --> 00:00:54,400
So we're going to get the value output.

12
00:00:54,460 --> 00:01:00,870
You know the value of the function outputs of our model and this is the value we will give to the community

13
00:01:00,870 --> 00:01:01,490
we work.

14
00:01:01,780 --> 00:01:05,500
So let's first get this value we can get it this way.

15
00:01:05,560 --> 00:01:12,820
Value Then you know since we only want the value we can add here underscore and then underscore again

16
00:01:13,240 --> 00:01:19,980
and then we get our model because it will output this value but only the first output of the moral thing

17
00:01:19,980 --> 00:01:25,310
to do is double on its course here and here we can just copy paste what we have here.

18
00:01:25,430 --> 00:01:33,050
That is the input of the model with the input images and the pull of the states and the South states.

19
00:01:33,220 --> 00:01:35,820
So I'm just pasting that and there we go.

20
00:01:35,860 --> 00:01:37,490
We will get the value.

21
00:01:37,810 --> 00:01:47,080
And so now what we're going to do is give to our value so all will be equal to value and to access to

22
00:01:47,080 --> 00:01:49,350
the value we at this start there.

23
00:01:49,750 --> 00:01:50,430
All right.

24
00:01:50,440 --> 00:01:57,610
Now the if condition is done and now what we're going to do since we just got a new value by you know

25
00:01:57,670 --> 00:02:04,090
getting the output of the model the first output of the model well that's already append this new value

26
00:02:04,090 --> 00:02:05,330
to the values list.

27
00:02:05,500 --> 00:02:16,340
Therefore we can take directly our values list then towards a tent and we put variable are because our.

28
00:02:16,530 --> 00:02:20,560
This last value so great that is done now.

29
00:02:20,850 --> 00:02:25,180
We are going to initialize the losses and remembering intuition lectures.

30
00:02:25,260 --> 00:02:26,320
You have two losses.

31
00:02:26,340 --> 00:02:31,780
You have the last of the policy that is the last related to the predictions of the agent.

32
00:02:31,860 --> 00:02:36,070
And then you have the last of the value which is less related to the predictions of the critic.

33
00:02:36,180 --> 00:02:41,130
So we are going to introduce these two variables initialized into zero and they're going to take here

34
00:02:41,250 --> 00:02:44,600
policy for us horrible policy loss.

35
00:02:44,690 --> 00:02:52,680
Initialize it to zero and then value lost a lot of the value and say initialized it to zero then let's

36
00:02:52,680 --> 00:02:58,680
not forget to set the cumulative reward as a torch variable because we will need it to be a torch Roybal

37
00:02:58,680 --> 00:03:03,990
because we will be computing a gradient with respect to it because the cumulative reward is going to

38
00:03:03,990 --> 00:03:05,850
be a term of the value loss.

39
00:03:05,850 --> 00:03:10,050
So is this viable it is now attached to the dynamic graphs with a gradient.

40
00:03:10,530 --> 00:03:16,150
And now finally the last thing we need to do before starting the big trend loop you know when we applied

41
00:03:16,170 --> 00:03:20,580
to gas degrade in the sun to reduce this last between the predictions and targets.

42
00:03:20,850 --> 00:03:28,160
Well we need to initialize the GAAP to generalized advantage estimation and not get it or uncoated.

43
00:03:28,260 --> 00:03:34,530
Be careful with that GAAP the variable that we're about to initialize right now is generalized advantage

44
00:03:34,590 --> 00:03:35,510
estimation.

45
00:03:35,520 --> 00:03:42,480
So as a reminder generalized advantage estimation is by definition the advantage of playing the action

46
00:03:42,540 --> 00:03:45,170
a by observing the state s.

47
00:03:45,210 --> 00:03:51,330
So it's a function of the action and the state s and it is equal to the difference between the q values

48
00:03:51,450 --> 00:03:54,780
Q A S and the value of the V function.

49
00:03:54,780 --> 00:03:57,120
So actually I can write it here.

50
00:03:57,540 --> 00:04:05,130
The generalized advantage estimation is a function of the action and the state s and that is equal to

51
00:04:05,580 --> 00:04:12,570
the q values of the action A and the state S minus the value of the V function applied to the state

52
00:04:12,650 --> 00:04:13,440
s.

53
00:04:13,530 --> 00:04:19,000
That's the generalized advantages to mention and that's what we want to initialize right now.

54
00:04:19,200 --> 00:04:20,770
And we will initialize it to zero.

55
00:04:21,470 --> 00:04:27,320
But it has to be towards dancers who were going to use the same trick as what we just did right here

56
00:04:27,730 --> 00:04:35,330
we are going to take the torch library and apply to zebra's function to set it as a tensor of only one

57
00:04:35,330 --> 00:04:36,730
value which is zero.

58
00:04:37,160 --> 00:04:45,730
And we are going to use this new variable g and that will be equal to that torch that zeros 1 one as

59
00:04:45,740 --> 00:04:46,540
initialises us.

60
00:04:46,550 --> 00:04:52,730
So this will be initialized to zero and therefore the q values of the action the state s will be equal

61
00:04:52,730 --> 00:04:55,600
to the value of the V function of the state s.

62
00:04:55,780 --> 00:04:56,290
All right.

63
00:04:56,320 --> 00:04:58,810
And now we are ready to start the for loop.

64
00:04:58,850 --> 00:05:00,470
So we're going to have an adventure here.

65
00:05:00,500 --> 00:05:04,700
So take a good break and I'll see you in the next tutorial to attack that.

66
00:05:04,820 --> 00:05:06,170
And so then I.
