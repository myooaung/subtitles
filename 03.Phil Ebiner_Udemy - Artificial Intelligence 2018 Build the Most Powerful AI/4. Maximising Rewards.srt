1
00:00:01,760 --> 00:00:06,260
Hello and welcome back to the course on augmented random search in today's tutorial we're going to talk

2
00:00:06,260 --> 00:00:08,040
about maximizing rewards.

3
00:00:08,210 --> 00:00:09,790
So what is this all about.

4
00:00:09,980 --> 00:00:15,940
Well to start off with let's look at our perception and how it interacts with thing varment.

5
00:00:16,190 --> 00:00:23,690
So for simplicity's sake we're going to take a perception with three inputs and outputs as we discussed

6
00:00:23,690 --> 00:00:24,920
previously.

7
00:00:24,920 --> 00:00:29,990
The outputs are simply the weighted sums of the inputs.

8
00:00:29,990 --> 00:00:37,340
And so basically it's a matrix multiplication of the inputs times the matrix or the vector of inputs

9
00:00:37,340 --> 00:00:42,160
times a matrix of weights of this perception in order to get outputs.

10
00:00:42,470 --> 00:00:45,010
So let's have a look at an example.

11
00:00:45,050 --> 00:00:50,570
And so here we got Mujer code environment with a humanoid that's learning how to walk.

12
00:00:50,600 --> 00:00:51,290
Let's have a look.

13
00:00:51,290 --> 00:00:52,880
This is going to be quite funny.

14
00:00:52,880 --> 00:01:01,220
There is that's one of the most efficient ways of walking apparently according to this AI.

15
00:01:01,430 --> 00:01:07,890
And so as you can see what happens is the model moves most moves or at some point it falls falls over

16
00:01:08,630 --> 00:01:11,980
and that's where this episode would end.

17
00:01:12,020 --> 00:01:14,730
There are certain times when the episode can end.

18
00:01:14,780 --> 00:01:20,750
So this whole this whole period from Start from where the model starts to when it finishes it's called

19
00:01:20,750 --> 00:01:21,530
an episode.

20
00:01:21,810 --> 00:01:29,150
They're the moral perform certain actions during episode and the episode can end when the model falls

21
00:01:29,150 --> 00:01:32,070
over and there's no recovery from that.

22
00:01:32,070 --> 00:01:38,420
The episode can and for instance when a tie in time runs out or when the model reaches a certain goal

23
00:01:39,090 --> 00:01:40,660
it depends on the environment.

24
00:01:40,670 --> 00:01:46,750
For instance in this case we were had Michiko and this episode is ending when the is falling or so because

25
00:01:46,750 --> 00:01:51,980
you can see the model falls over here and from there what happens next.

26
00:01:51,980 --> 00:02:00,350
Well in order for the AI to learn how to walk certain feedback has to be provided to the AI by the environment

27
00:02:00,350 --> 00:02:03,690
because if there's no feedback remember the AI is just a black box.

28
00:02:03,740 --> 00:02:11,360
It's getting inputs right and it's getting like from how the environment is what a situation in the

29
00:02:11,360 --> 00:02:15,220
right environment is right now what forces are in play and it provides outputs.

30
00:02:15,290 --> 00:02:19,490
But apart from that it needs some kind of mechanism for it.

31
00:02:19,640 --> 00:02:23,650
In order for it to be able to develop learn.

32
00:02:23,780 --> 00:02:30,800
And so what happens is the environment provides a reward reward to that artificial intelligence based

33
00:02:30,800 --> 00:02:32,040
on how well it performs.

34
00:02:32,060 --> 00:02:36,520
So the longer it stayed upright the further it went.

35
00:02:36,520 --> 00:02:40,010
The higher reward the reward will be the sooner it fell the

36
00:02:42,500 --> 00:02:43,120
less.

37
00:02:43,130 --> 00:02:50,460
Far more like the closer it is to the start of the episode when actually fell the lesser the reward

38
00:02:50,450 --> 00:02:51,040
will be.

39
00:02:51,080 --> 00:02:55,870
And so this reward is then used by the AI to adjust the weights.

40
00:02:55,910 --> 00:02:57,680
That's how the process works.

41
00:02:57,680 --> 00:03:01,780
Now how exactly or just the way we'll talk about that in the next tutorial.

42
00:03:02,000 --> 00:03:07,320
But for for now what is important for us to understand is that this is the feedback loop.

43
00:03:07,340 --> 00:03:16,340
This is the feedback mechanism that works in order to help the AI in this case the Erris to learn and

44
00:03:16,340 --> 00:03:21,380
it learns by adjusting the ways because ultimately there's nothing else that it can do is nothing else

45
00:03:21,680 --> 00:03:23,820
specific to this AI.

46
00:03:23,840 --> 00:03:26,140
These inputs are provided by the environment.

47
00:03:26,150 --> 00:03:32,690
These outputs go to the environment and only thing the only component of the AI that the AI itself is

48
00:03:32,690 --> 00:03:38,410
in control of are these weights because adjusting the weights it will change the output so you can you

49
00:03:38,410 --> 00:03:43,350
can increase or decrease the weights and then output will be different and same similar here.

50
00:03:43,700 --> 00:03:47,470
And again we'll talk about this in the coming Tauriel.

51
00:03:47,480 --> 00:03:56,360
But what I wanted to highlight here is the understanding of how can we better get an intuition for this

52
00:03:56,870 --> 00:03:58,060
from the real world.

53
00:03:58,100 --> 00:04:05,240
And my favorite example is if you take a child if you take a baby that's learning to walk then how is

54
00:04:05,240 --> 00:04:08,320
it getting feedback how is it getting reward from the environment.

55
00:04:08,540 --> 00:04:13,310
Well for instance if it gets up and then it tries to put it's like left foot for the right foot forward

56
00:04:13,580 --> 00:04:15,190
then left foot forward again.

57
00:04:15,350 --> 00:04:20,410
And then for instance it tries to put his left or right forward over its shoulder.

58
00:04:20,510 --> 00:04:21,420
It's going to fall over.

59
00:04:21,590 --> 00:04:26,570
And what it's going to get after falling over is pain is going to get a bruise.

60
00:04:26,960 --> 00:04:30,460
But that's more for like the parents to see that you know the baby fell over.

61
00:04:30,590 --> 00:04:38,390
But in terms of the baby itself it's going to get pain and that is the central nervous system signaling

62
00:04:38,390 --> 00:04:41,330
to the brain that that was a bad decision.

63
00:04:41,330 --> 00:04:42,640
Don't do that again.

64
00:04:42,900 --> 00:04:50,150
That that is not what we want because the central nervous system through evolution through DNA is is

65
00:04:50,150 --> 00:04:55,730
coded to know exactly what are in what is good for our survival what's on.

66
00:04:55,730 --> 00:04:57,080
So for instance walking is good.

67
00:04:57,080 --> 00:05:03,390
Falling over isn't and that's why when you fall over you get a signal a central nervous system of pain

68
00:05:03,690 --> 00:05:07,750
in order to train you or the baby to not do that again.

69
00:05:07,830 --> 00:05:15,540
And so in this case or when the baby does get legs start walking you know that the parents are clapping.

70
00:05:15,540 --> 00:05:21,330
It gets to its target destination for instance it was chasing the catechize the cat or gets to the couch

71
00:05:21,330 --> 00:05:22,120
or something like that.

72
00:05:22,260 --> 00:05:26,890
And the central nervous system rewards the baby by saying hey you did a good job.

73
00:05:27,060 --> 00:05:28,810
You know like you know you're happy.

74
00:05:28,980 --> 00:05:32,070
And so basically from there the baby learns how to walk.

75
00:05:32,070 --> 00:05:34,650
Of course it's much more complex for humans.

76
00:05:34,650 --> 00:05:39,450
So it's just a set of weights a matrix of weights that we're updating in our brains.

77
00:05:39,450 --> 00:05:41,020
But the concept stays the same.

78
00:05:41,040 --> 00:05:45,600
That's why through these rewards the AI learns.

79
00:05:45,620 --> 00:05:51,840
OK so the higher if I do certain things then I get very rewards and therefore I'm I'm adjusting or it's

80
00:05:51,840 --> 00:05:52,660
in the right direction.

81
00:05:53,100 --> 00:05:54,660
But that's running a little bit ahead.

82
00:05:54,660 --> 00:05:56,480
We'll talk about that in the next tutorial.

83
00:05:56,640 --> 00:05:58,260
And I look forward to seeing you there.

84
00:05:58,260 --> 00:05:59,130
Hope you enjoy it today.

85
00:05:59,190 --> 00:06:01,440
And until next time enjoy AI.

