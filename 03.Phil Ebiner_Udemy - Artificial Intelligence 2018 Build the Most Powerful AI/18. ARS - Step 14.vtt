WEBVTT
ï»¿1
00:00:00.390 --> 00:00:05.670
Hello and welcome to this new step of the training and this one we're going to get the positive rewards

2
00:00:06.000 --> 00:00:11.370
in the positive direction that is the reward obtained when we apply the perturbations in the positive

3
00:00:11.370 --> 00:00:15.690
directions which are in total 16 positive directions.

4
00:00:15.690 --> 00:00:20.690
But then remember We'll also have the perturbations in the opposite directions.

5
00:00:20.700 --> 00:00:27.030
Therefore we'll have 16 more opposite directions to get the negative rewards.

6
00:00:27.300 --> 00:00:34.410
And that's in order to run the 1 step of grading descent because indeed in order to do weight we're

7
00:00:34.410 --> 00:00:40.020
going to apply the method of fitting differences to approximate the gradient of the war with respect

8
00:00:40.020 --> 00:00:45.450
to the weight in order to obtain this weight in the direction that increases the words.

9
00:00:45.450 --> 00:00:48.290
So for us we need to get the positive reward.

10
00:00:48.330 --> 00:00:55.010
And then in the next code section will get the negative rewards in the negative or opposite directions.

11
00:00:55.320 --> 00:00:56.980
So let's start with the positive rewards.

12
00:00:57.030 --> 00:01:03.600
Basically that just consists of updating this list of positive rewards which were initialized in previous

13
00:01:03.600 --> 00:01:12.900
tutorials and which I remind will contain the 16 accumulated rewards for each of the 16 positive directions

14
00:01:13.290 --> 00:01:19.770
that were sampled in this previous code section actually right here when we used a simple delta method

15
00:01:19.770 --> 00:01:21.420
from the policy class.

16
00:01:21.420 --> 00:01:30.420
So since we're about to populate this positive words list with the 16 argumentative words obtain for

17
00:01:30.510 --> 00:01:32.430
each of the 16 directions.

18
00:01:32.430 --> 00:01:38.050
Well we have to do a for loop to iterate on each of these 16 directions.

19
00:01:38.190 --> 00:01:41.360
Therefore I'm starting with a 4 here.

20
00:01:41.490 --> 00:01:49.140
Then we can choose any loop variable name like K and for k in all the range of all directions which

21
00:01:49.140 --> 00:01:56.370
we can get by taking our hyper parameter object and then it's and the directions variable.

22
00:01:56.610 --> 00:02:03.880
Basically that means for each of the 16 different directions we will update our list of positive rewards.

23
00:02:04.170 --> 00:02:06.000
And how are we going to update this list.

24
00:02:06.060 --> 00:02:11.580
Well that's the purpose of doing a follow up here by obtaining each of its elements.

25
00:02:11.700 --> 00:02:19.500
So that's why here I'm going to add some brackets and then put K here so that positive rewards of index

26
00:02:19.560 --> 00:02:28.050
k is the reward that we'll get by playing the perturbation on the case positive direction and that's

27
00:02:28.050 --> 00:02:36.030
where I can use the Explore function that we made in the previous code section actually right here which

28
00:02:36.030 --> 00:02:44.640
will compute the accumulated reward on one full episode of 1000 actions or until the eye falls and for

29
00:02:44.790 --> 00:02:49.660
each specific direction which will be the case direction here.

30
00:02:49.710 --> 00:02:58.110
And so here in the export function we simply to put all the arguments expected by the export function

31
00:02:58.140 --> 00:03:01.190
but Norreys I will update them the right way.

32
00:03:01.320 --> 00:03:04.210
So let's see if it's the same environment here.

33
00:03:04.210 --> 00:03:05.200
So that's all right.

34
00:03:05.340 --> 00:03:10.110
Normalizer is the say normalizer here so no need to change anything.

35
00:03:10.110 --> 00:03:16.470
Same for the policy we were exploring our policy the same policy that will input here as the arguments

36
00:03:16.470 --> 00:03:21.960
of the train function and then Direction in Delta which will be according to you what.

37
00:03:22.200 --> 00:03:27.120
Well that's when things will start to get all connected and make sense in your head.

38
00:03:27.120 --> 00:03:30.510
Direction is going to be the positive direction.

39
00:03:30.570 --> 00:03:39.450
But remember in the policy class when we evaluate our policy Well we said that if the direction is positive

40
00:03:39.720 --> 00:03:47.270
then we will apply the perturbation in the positive direction by adding this plus here to add the perturbation.

41
00:03:47.490 --> 00:03:55.530
So what we have to put here for direction is this positive string so that later on when we used Evaluate

42
00:03:55.530 --> 00:04:03.390
function we can recognize that we are in the positive direction to add this positive perturbation.

43
00:04:03.390 --> 00:04:11.370
All right so here instead of none we need to input positive because of course we are getting the rewards

44
00:04:11.400 --> 00:04:14.210
after applying the perturbations in the positive direction.

45
00:04:14.430 --> 00:04:17.020
And then for Delta What is it going to be.

46
00:04:17.220 --> 00:04:24.810
Well of course remember that the delta here is a list of 16 elements each of the element being a matrix

47
00:04:25.140 --> 00:04:28.400
of the perturbations in one specific direction.

48
00:04:28.530 --> 00:04:34.400
And so here since we're looping over each of the 16 different directions and K basically represents

49
00:04:34.400 --> 00:04:40.830
here the direction Well we're going to get the case elements of this delta here because this corresponds

50
00:04:40.830 --> 00:04:44.410
to the perturbation applied in the case direction.

51
00:04:44.580 --> 00:04:54.310
So I'm going to take this list deltas here of the perturbations and Basan here and of course the case

52
00:04:54.820 --> 00:05:01.240
element of this list to get the case perturbation that is the perturbation of things in the case direction

53
00:05:01.660 --> 00:05:05.240
and things to this string positive here for the direction.

54
00:05:05.520 --> 00:05:12.730
Well the Explorer function will know at this specific step here when playing the action by calling the

55
00:05:12.730 --> 00:05:15.200
Evaluate function that direction will be positive.

56
00:05:15.220 --> 00:05:19.870
And therefore this Evaluate function will be in this specific condition.

57
00:05:19.870 --> 00:05:26.380
Elif direction calls positive and therefore it knows that it will have to return this output which is

58
00:05:26.380 --> 00:05:30.650
the output that we get by applying perturbation in positive direction.

59
00:05:30.880 --> 00:05:36.670
That is by adding this plus H beat up noise times the perturbation Delta.

60
00:05:36.670 --> 00:05:42.010
So everything makes sense everything is well connected so that we get Indeed the positive rewards that

61
00:05:42.010 --> 00:05:47.920
is rewards obtained by playing the perturbations in the case positive direction.

62
00:05:48.190 --> 00:05:48.670
Perfect.

63
00:05:48.670 --> 00:05:49.600
And that's it.

64
00:05:49.600 --> 00:05:53.580
That's all we have to do here for the positive reward.

65
00:05:53.620 --> 00:06:00.010
Don't forget to get this case perturbation here or is put into the case direction among the 16 directions

66
00:06:00.010 --> 00:06:01.040
we're testing.

67
00:06:01.380 --> 00:06:05.560
And now we're going into exactly the same for the negative words.

68
00:06:05.620 --> 00:06:11.690
We'll do that in the next tutorial to update this list of negative words and then we'll get all the

69
00:06:11.690 --> 00:06:14.350
words to compute the standard deviation of the reward.

70
00:06:14.500 --> 00:06:18.120
Then we'll sort them by the highest ones to get the best directions.

71
00:06:18.190 --> 00:06:22.780
And finally we'll update our policy which is one step of great dissent.

72
00:06:23.020 --> 00:06:24.510
Let's do this in the next tutorial.

73
00:06:24.520 --> 00:06:26.050
And until then enjoy AI.

