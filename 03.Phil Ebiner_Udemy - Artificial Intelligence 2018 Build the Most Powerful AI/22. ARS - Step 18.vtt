WEBVTT
ï»¿1
00:00:00.900 --> 00:00:03.990
Hello and here we go for that update step.

2
00:00:03.990 --> 00:00:10.470
Now that we've got our rollout we have everything to make this one step of great in the sense to date

3
00:00:10.530 --> 00:00:16.410
the weight of the presumption of our policy in the best directions that increase the most we reward.

4
00:00:16.530 --> 00:00:25.260
So I remind quickly that we going to do this test here and that here these words we get in a positive

5
00:00:25.260 --> 00:00:31.440
direction once we get in negative directions and the perturbations are composed in what we got some

6
00:00:31.440 --> 00:00:37.110
rolled out but they also talked about these rollout in the paper that we're going to put as the argument

7
00:00:37.200 --> 00:00:44.710
of the update method of our policy class and therefore making that data step will be a piece of cake.

8
00:00:44.820 --> 00:00:47.850
It will not only take one line but a couple of words.

9
00:00:47.850 --> 00:00:49.180
Let's do this.

10
00:00:49.180 --> 00:00:50.310
I actually tried to do it.

11
00:00:50.330 --> 00:00:55.460
For me it's a good practice to know Heno the method and class in Python.

12
00:00:55.500 --> 00:00:59.970
Well of course let's have a look at our policy class first.

13
00:01:00.000 --> 00:01:01.250
That's where we built the AI.

14
00:01:01.270 --> 00:01:08.310
Remember we made that policy class that is composed of not only the matrix of weight saeter has the

15
00:01:08.310 --> 00:01:13.830
main variable of the future object policies of this class and then a couple of two.

16
00:01:13.830 --> 00:01:17.260
Here we have the evaluate method that applies to perturbations.

17
00:01:17.430 --> 00:01:22.700
Here we have the simple deltas method that sampled the Deltas following normal distribution.

18
00:01:22.830 --> 00:01:29.860
And mostly we have to update method that makes that one step have approximated great in the sense.

19
00:01:29.880 --> 00:01:38.130
And as we see the update method takes it and put the roll out and the variance standard deviation of

20
00:01:38.550 --> 00:01:42.900
all the words including the positive ones and the negative ones.

21
00:01:42.900 --> 00:01:50.460
So in order to make that a data step what we simply need to do is take our policy object which will

22
00:01:50.460 --> 00:01:58.320
be created later on but we can still use it here because we are doing that inside a function brain function.

23
00:01:58.380 --> 00:02:04.350
So we take our policy object then from this object we're going to call the update method which will

24
00:02:04.350 --> 00:02:10.590
take as arguments are rolled out that we've just gathered in the previous tutorial and of course the

25
00:02:10.670 --> 00:02:18.780
standardization of the word which we also gathered or computed to tutorials before and then the previous

26
00:02:18.780 --> 00:02:20.500
one but the one before.

27
00:02:20.540 --> 00:02:24.830
So we have everything to make that a data step in a flashlight.

28
00:02:25.050 --> 00:02:32.950
Let's do this let's take first as we said our policy object which will create later.

29
00:02:33.210 --> 00:02:40.270
Then from the subject we take the update method which takes as input only two arguments.

30
00:02:40.290 --> 00:02:48.040
First one is our rollout that we've gathered here and the second one is the standard deviation of the

31
00:02:48.060 --> 00:02:48.650
word.

32
00:02:48.690 --> 00:02:54.970
Sigma R which we computed here all good and that's done nothing else to do.

33
00:02:55.110 --> 00:02:59.940
You have made your data step one step of great in this sense.

34
00:03:00.150 --> 00:03:06.990
To update your policy in the best directions that increase the most wars and therefore that gives your

35
00:03:06.990 --> 00:03:09.830
policy a better ability to walk.

36
00:03:10.110 --> 00:03:11.120
Congratulations.

37
00:03:11.250 --> 00:03:12.720
That's it for this Statoil.

38
00:03:12.720 --> 00:03:17.670
Now we move onto the next step which will be to you know when we launch the training we would like to

39
00:03:17.670 --> 00:03:24.780
see appearing in the canso all the accumulated words we get at the end of each training loop that is

40
00:03:24.780 --> 00:03:29.460
at the end of each of these steps here in this fall loop and how we're going to do that.

41
00:03:29.460 --> 00:03:37.710
Well remember that we made this explore function that is not only used to explore the policy on one

42
00:03:37.710 --> 00:03:41.070
specific direction whether it is positive or negative.

43
00:03:41.100 --> 00:03:46.350
And over one episode remember the direction it can take the value none and the perturbation that I can

44
00:03:46.350 --> 00:03:53.580
take the value none as well therefore to get this final word at the end of each of the train steps we're

45
00:03:53.580 --> 00:03:59.250
simply going to call this explore function without any directions or with direction it goes none and

46
00:03:59.250 --> 00:04:03.630
without any perturbation but with the new policy that was just updated.

47
00:04:03.680 --> 00:04:08.910
And with our normalizer so that we get the accumulated B word with the new weights.

48
00:04:09.150 --> 00:04:14.880
You know after they were dead in the best directions so that at the end of each training loop we can

49
00:04:14.880 --> 00:04:21.090
get the reward and keep track of how the word is evolving and hopefully improving over the training

50
00:04:21.090 --> 00:04:21.680
loops.

51
00:04:22.020 --> 00:04:26.650
And then you'll see in the end we'll do a print to print these final words.

52
00:04:26.820 --> 00:04:29.640
All right so let's do this and do that in the next toils.

53
00:04:29.720 --> 00:04:31.290
And until then enjoy.

