WEBVTT
1
00:00:01.040 --> 00:00:01.810
So next,

2
00:00:01.810 --> 00:00:07.100
we're going to discuss how we can have fault‑tolerant SageMaker endpoints.

3
00:00:07.100 --> 00:00:10.240
And to understand this, let's look at an example.

4
00:00:10.240 --> 00:00:12.800
So let's assume that we have our client application

5
00:00:12.800 --> 00:00:14.840
that we've discussed previously.

6
00:00:14.840 --> 00:00:20.440
Now it's going to need to access a SageMaker endpoint to perform inference.

7
00:00:20.440 --> 00:00:21.580
Now in this case,

8
00:00:21.580 --> 00:00:24.480
this endpoint has been configured within a

9
00:00:24.480 --> 00:00:28.540
customer‑managed virtual private cloud, or VPC.

10
00:00:28.540 --> 00:00:32.510
Now we know that an endpoint works with a load balancer and that the load

11
00:00:32.510 --> 00:00:36.540
balancer itself can communicate with multiple instances.

12
00:00:36.540 --> 00:00:38.720
Now let's say that this has been configured within

13
00:00:38.720 --> 00:00:41.650
this VPC to support one subnet,

14
00:00:41.650 --> 00:00:45.490
and that subnet exists within availability zone because subnets

15
00:00:45.490 --> 00:00:48.440
are limited to a single availability zone.

16
00:00:48.440 --> 00:00:50.650
Now let's say within that availability zone,

17
00:00:50.650 --> 00:00:53.560
we have multiple instances that have been configured.

18
00:00:53.560 --> 00:00:54.740
And matter of fact,

19
00:00:54.740 --> 00:00:59.550
let's say that we've already configured autoscaling so that we can shrink or

20
00:00:59.550 --> 00:01:03.250
grow the number of instances that we have based on user demand.

21
00:01:03.250 --> 00:01:04.340
And in this case,

22
00:01:04.340 --> 00:01:08.740
that load balancer knows how to communicate with any of those three instances.

23
00:01:08.740 --> 00:01:11.940
So right off the bat, we have fixed one major problem,

24
00:01:11.940 --> 00:01:15.370
and that is that we are able to now absorb the failure of any

25
00:01:15.370 --> 00:01:18.620
one instance within our availability zone.

26
00:01:18.620 --> 00:01:21.240
So if we have a server that goes down, that's okay.

27
00:01:21.240 --> 00:01:24.640
There's still two more that the load balancer can do communicate with,

28
00:01:24.640 --> 00:01:29.390
and it can route users to a working instance. So that's great. However,

29
00:01:29.390 --> 00:01:30.840
we have a problem.

30
00:01:30.840 --> 00:01:32.180
And that problem is,

31
00:01:32.180 --> 00:01:35.260
even though we've been able to autoscaling and we're able to

32
00:01:35.260 --> 00:01:37.620
deal with the failure of a single instance,

33
00:01:37.620 --> 00:01:41.540
we're not able to deal with the failure of an availability zone.

34
00:01:41.540 --> 00:01:43.200
Now while this is uncommon,

35
00:01:43.200 --> 00:01:48.300
we do need to build in support for multiple availability zones to make sure that

36
00:01:48.300 --> 00:01:52.640
we can absorb the failure of a complete availability zone.

37
00:01:52.640 --> 00:01:55.720
So in this case, if our availability zone were to go away,

38
00:01:55.720 --> 00:01:57.650
if it were no longer to be functional,

39
00:01:57.650 --> 00:02:01.050
our users would have no access to those instances,

40
00:02:01.050 --> 00:02:05.140
and they wouldn't be able to actually leverage the endpoint for inference.

41
00:02:05.140 --> 00:02:06.840
So how do we get around this?

42
00:02:06.840 --> 00:02:09.560
Well, ideally, when we configure our endpoint,

43
00:02:09.560 --> 00:02:12.560
we're going to give it support for multiple subnets that

44
00:02:12.560 --> 00:02:15.150
exist within multiple availability zones.

45
00:02:15.150 --> 00:02:19.010
And while I'm showing two here, in many cases, we actually see this pushed out

46
00:02:19.010 --> 00:02:22.670
to three different availability zones. And once we do that,

47
00:02:22.670 --> 00:02:25.210
we're then able to configure autoscaling to not be

48
00:02:25.210 --> 00:02:29.740
limited to a single availability zone, but to support multiple.

49
00:02:29.740 --> 00:02:31.530
And so now, in this example,

50
00:02:31.530 --> 00:02:37.100
we have multiple availability zones with instances in each so that the

51
00:02:37.100 --> 00:02:40.970
load balancer can now route users to working instances so that even if

52
00:02:40.970 --> 00:02:45.340
we lost an entire availability zone, we would still be able to perform inference.

53
00:02:45.340 --> 00:02:48.580
And one of the great things about the autoscaling for SageMaker

54
00:02:48.580 --> 00:02:52.270
endpoints is that if it has the ability to support multiple

55
00:02:52.270 --> 00:02:54.490
subnets and multiple availability zones,

56
00:02:54.490 --> 00:02:57.590
it can automatically put instances in the right

57
00:02:57.590 --> 00:02:59.860
availability zone so you don't end up with,

58
00:02:59.860 --> 00:03:04.040
for example, 95% of your instances in a single availability zone,

59
00:03:04.040 --> 00:03:05.230
which could certainly cause issues.

60
00:03:05.230 --> 00:03:07.540
It will try to spread them out evenly.

61
00:03:07.540 --> 00:03:09.900
So with this example in place,

62
00:03:09.900 --> 00:03:14.540
let's talk about some general concepts for a fault‑tolerant endpoint.

63
00:03:14.540 --> 00:03:19.190
So first, SageMaker is designed with high availability in mind.

64
00:03:19.190 --> 00:03:22.120
Many aspects of the service are already highly available.

65
00:03:22.120 --> 00:03:25.140
We really just need to customize how we're using it.

66
00:03:25.140 --> 00:03:27.970
And as we mentioned before, for security purposes,

67
00:03:27.970 --> 00:03:33.040
we want to be sure that we're launching SageMaker within a VPC that we manage.

68
00:03:33.040 --> 00:03:38.440
And if we want to support the outage of an instance, here, we can utilize

69
00:03:38.440 --> 00:03:41.660
multiple instances. We need to have more than one instance.

70
00:03:41.660 --> 00:03:42.870
But in addition to that,

71
00:03:42.870 --> 00:03:46.540
we also want to be able to support an availability zone outage.

72
00:03:46.540 --> 00:03:47.580
And so to do that,

73
00:03:47.580 --> 00:03:54.000
we need to be sure that we're launching our endpoints across multiple availability zones.

