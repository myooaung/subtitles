1
00:00:01,040 --> 00:00:03,880
Now the primary way that we implement our machine

2
00:00:03,880 --> 00:00:07,840
learning workflow on AWS is with SageMaker.

3
00:00:07,840 --> 00:00:10,380
However, there are other ways you can go about it,

4
00:00:10,380 --> 00:00:13,100
and we'll cover those later within this module.

5
00:00:13,100 --> 00:00:13,580
However,

6
00:00:13,580 --> 00:00:16,350
here I want to review all of the different services

7
00:00:16,350 --> 00:00:18,540
that are provided with SageMaker.

8
00:00:18,540 --> 00:00:23,240
So first at a high level, let's talk about the general SageMaker process.

9
00:00:23,240 --> 00:00:24,720
We're going to start off with data,

10
00:00:24,720 --> 00:00:28,640
and we're going to need to label our data so we can then use it for training.

11
00:00:28,640 --> 00:00:30,210
So once we have our data labeled,

12
00:00:30,210 --> 00:00:33,840
we'll then be able to go through and build our initial model.

13
00:00:33,840 --> 00:00:34,870
Once we've built it,

14
00:00:34,870 --> 00:00:39,040
we'll then need to go through the process of training and tuning our model.

15
00:00:39,040 --> 00:00:41,730
Once we feel comfortable with the model that we have,

16
00:00:41,730 --> 00:00:45,640
we'll then be able to actually deploy it and then manage it once it's deployed.

17
00:00:45,640 --> 00:00:46,560
And then over time,

18
00:00:46,560 --> 00:00:49,170
we'll go back through this process again to continually

19
00:00:49,170 --> 00:00:52,040
improve our model with the data that we have.

20
00:00:52,040 --> 00:00:54,220
So one of the first key services that we have in this

21
00:00:54,220 --> 00:00:57,540
process is SageMaker Ground Truth,

22
00:00:57,540 --> 00:01:00,000
and this is a service that allows us to label our

23
00:01:00,000 --> 00:01:02,430
data for our training datasets.

24
00:01:02,430 --> 00:01:05,590
And one of the great things about this service is that it can

25
00:01:05,590 --> 00:01:08,940
leverage both human and automated classification.

26
00:01:08,940 --> 00:01:11,280
So we can do a couple of different approaches here.

27
00:01:11,280 --> 00:01:14,140
First of all, we can utilize a provided workforce.

28
00:01:14,140 --> 00:01:16,960
So we can take employees from our organization that

29
00:01:16,960 --> 00:01:21,330
know how to actually label our data, and we can set them on that task.

30
00:01:21,330 --> 00:01:23,580
But we also can do look at leveraging a third‑party

31
00:01:23,580 --> 00:01:26,840
workforce through Amazon Mechanical Turk.

32
00:01:26,840 --> 00:01:31,270
We also have the ability here to use an automated labeling workflow.

33
00:01:31,270 --> 00:01:34,310
So it can actually label data that it has high levels of

34
00:01:34,310 --> 00:01:38,030
confidence in and then simply pass the remainder of the data onto

35
00:01:38,030 --> 00:01:40,440
the humans that would be working on labeling.

36
00:01:40,440 --> 00:01:44,160
Now you can see SageMaker in action here within the console.

37
00:01:44,160 --> 00:01:46,610
Now this is using the image task category.

38
00:01:46,610 --> 00:01:49,530
And you can see here that it has configurations for

39
00:01:49,530 --> 00:01:51,970
things like single label classifications,

40
00:01:51,970 --> 00:01:54,180
multi‑label classification, bounding box,

41
00:01:54,180 --> 00:01:57,270
semantic segmentation, and this is just one of the categories.

42
00:01:57,270 --> 00:01:59,840
There are other categories included as well.

43
00:01:59,840 --> 00:02:03,130
Now the next thing that we have, after we actually have our data labeled,

44
00:02:03,130 --> 00:02:05,590
is to go through the process of building our model.

45
00:02:05,590 --> 00:02:09,220
And here we can look to leverage SageMaker Notebooks.

46
00:02:09,220 --> 00:02:14,230
And this is a managed compute environment for running our Jupyter notebooks.

47
00:02:14,230 --> 00:02:17,050
And the great thing here is that it launches compute environments that

48
00:02:17,050 --> 00:02:20,340
are pre‑configured for our machine learning tasks.

49
00:02:20,340 --> 00:02:23,900
Another great thing about using SageMaker Notebooks is that it integrates

50
00:02:23,900 --> 00:02:27,630
storage volumes that can persist between notebook instances.

51
00:02:27,630 --> 00:02:30,760
So if you want to have storage that you keep the results,

52
00:02:30,760 --> 00:02:33,870
maybe your training datasets and your resulting models in,

53
00:02:33,870 --> 00:02:36,370
you can utilize SageMaker Notebooks for that.

54
00:02:36,370 --> 00:02:39,460
And here you can see an example of a SageMaker Notebook

55
00:02:39,460 --> 00:02:42,240
instance up and running in the cloud.

56
00:02:42,240 --> 00:02:46,230
Now if we want to go beyond that, we have another option that's a bit newer,

57
00:02:46,230 --> 00:02:48,440
and that is SageMaker Studio.

58
00:02:48,440 --> 00:02:52,400
And what SageMaker Studio is is a complete integrated development

59
00:02:52,400 --> 00:02:56,010
environment for machine learning directly within your browser.

60
00:02:56,010 --> 00:02:58,940
And what it does is it provides support for building,

61
00:02:58,940 --> 00:03:02,770
tuning, and deploying and managing your models on AWS.

62
00:03:02,770 --> 00:03:07,390
Now it's built on JupyterLab, but it's also deeply integrated in with SageMaker.

63
00:03:07,390 --> 00:03:10,060
So many of the services you'll see here you can interact

64
00:03:10,060 --> 00:03:13,440
with directly from within SageMaker Studio.

65
00:03:13,440 --> 00:03:18,940
And here's an example of a notebook up and running within SageMaker Studio.

66
00:03:18,940 --> 00:03:22,800
Now the next thing that we have is SageMaker Autopilot.

67
00:03:22,800 --> 00:03:25,160
Now it automates the building, training,

68
00:03:25,160 --> 00:03:28,080
and tuning of your machine learning models.

69
00:03:28,080 --> 00:03:30,580
Now one of the unique things about this over most

70
00:03:30,580 --> 00:03:33,910
automatic machine learning solutions is that it supports

71
00:03:33,910 --> 00:03:36,050
the automation of the model creation.

72
00:03:36,050 --> 00:03:37,080
And again, that's common.

73
00:03:37,080 --> 00:03:40,040
There are several different solutions that exist today that do that.

74
00:03:40,040 --> 00:03:43,940
However, it also allows manual customization.

75
00:03:43,940 --> 00:03:47,310
Now the way this works is that it receives a tabular dataset,

76
00:03:47,310 --> 00:03:50,540
and there is a column that is targeted for prediction.

77
00:03:50,540 --> 00:03:53,260
Now this works really for two different algorithm types,

78
00:03:53,260 --> 00:03:57,940
the first being classification and the second being regression.

79
00:03:57,940 --> 00:04:00,340
Now next, let's talk about something that's similar,

80
00:04:00,340 --> 00:04:04,240
yet different, and that is SageMaker Automatic Model Tuning.

81
00:04:04,240 --> 00:04:07,310
So unlike Autopilot where it creates our model for us,

82
00:04:07,310 --> 00:04:09,990
here, this takes a model that we have created,

83
00:04:09,990 --> 00:04:13,020
and it can utilize machine learning to optimize the

84
00:04:13,020 --> 00:04:16,350
hyperparameters in your model training process.

85
00:04:16,350 --> 00:04:19,530
And one of the benefits of this is that you can set these runs

86
00:04:19,530 --> 00:04:22,910
to be running in parallel so that you can optimize the speed at

87
00:04:22,910 --> 00:04:24,640
which your models are trained.

88
00:04:24,640 --> 00:04:26,780
Now this supports the following type of parameters.

89
00:04:26,780 --> 00:04:29,970
You can use categorical so that you can have different

90
00:04:29,970 --> 00:04:32,010
values that can be substituted in.

91
00:04:32,010 --> 00:04:34,800
It could be continuous where we can pick any number that

92
00:04:34,800 --> 00:04:36,870
exists between a minimum and a maximum.

93
00:04:36,870 --> 00:04:40,260
Or we can use integer values where we need to pick integers

94
00:04:40,260 --> 00:04:43,040
between a minimum and a maximum value.

95
00:04:43,040 --> 00:04:47,800
Now if you're using SageMaker Autopilot or SageMaker Automatic Model Tuning,

96
00:04:47,800 --> 00:04:51,770
you're going to have many different runs of your model training,

97
00:04:51,770 --> 00:04:54,620
and it's going to be hard to keep all of that organized.

98
00:04:54,620 --> 00:04:57,840
And that's where we get to SageMaker experiments.

99
00:04:57,840 --> 00:05:01,740
And what this does is it enables organization of your training jobs and

100
00:05:01,740 --> 00:05:05,240
the analytics related to those training jobs at scale.

101
00:05:05,240 --> 00:05:07,990
And this integrates in with solutions like SageMaker

102
00:05:07,990 --> 00:05:11,390
Autopilot and SageMaker Automatic Model Tuning.

103
00:05:11,390 --> 00:05:14,240
It also directly integrates in with SageMaker Studio.

104
00:05:14,240 --> 00:05:17,670
This was one of those services I mentioned earlier that is integrated with

105
00:05:17,670 --> 00:05:22,210
SageMaker Studio. Now once you have your model in a great place,

106
00:05:22,210 --> 00:05:25,920
you feel confident that it is tuned for your data, you then can do

107
00:05:25,920 --> 00:05:29,240
look to leverage a service called SageMaker Neo.

108
00:05:29,240 --> 00:05:33,450
And what this does is it provides both model optimization and

109
00:05:33,450 --> 00:05:36,540
cross‑platform compilation of your models.

110
00:05:36,540 --> 00:05:39,670
Now one of the great things here is that this enables you to execute

111
00:05:39,670 --> 00:05:44,270
your model both in the cloud, as well is on the edge. And it supports

112
00:05:44,270 --> 00:05:46,120
several different hardware architectures,

113
00:05:46,120 --> 00:05:49,540
including Intel, NVIDIA, and ARM.

114
00:05:49,540 --> 00:05:51,880
Now the next service we're going to talk about has to do with

115
00:05:51,880 --> 00:05:55,890
how we continually improve the models that we're using, and

116
00:05:55,890 --> 00:05:59,640
that is Amazon Augmented AI or A2I.

117
00:05:59,640 --> 00:06:04,040
Now this service integrates human review into the inference process.

118
00:06:04,040 --> 00:06:07,830
As we see machine learning permeate more and more into the business world,

119
00:06:07,830 --> 00:06:11,360
we're now seeing it be applied into situations where we need to

120
00:06:11,360 --> 00:06:13,960
make sure we are as accurate as possible.

121
00:06:13,960 --> 00:06:17,450
And what this does is that it provides a configurable workflow for

122
00:06:17,450 --> 00:06:21,280
these situations when accuracy is critical. Let's say,

123
00:06:21,280 --> 00:06:25,070
for example, you're wanting to read forms that have been filled out by humans,

124
00:06:25,070 --> 00:06:27,660
and we're trying to pull data off of those forms, and maybe these

125
00:06:27,660 --> 00:06:31,430
are critical forms that have to do with financial transactions. And

126
00:06:31,430 --> 00:06:33,900
in those cases, we want it to be right.

127
00:06:33,900 --> 00:06:37,760
And so if our model detects a low level of confidence in

128
00:06:37,760 --> 00:06:40,200
something it's trying to pull out of that, we can then

129
00:06:40,200 --> 00:06:43,040
trigger a human review of that data.

130
00:06:43,040 --> 00:06:52,000
Now one of the great things about A2I is that it is directly integrated into both Amazon Textract and Amazon Recognition.

