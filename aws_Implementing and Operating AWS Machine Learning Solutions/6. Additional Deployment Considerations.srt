1
00:00:01,040 --> 00:00:06,040
So next, we're going to discuss some additional deployment considerations.

2
00:00:06,040 --> 00:00:06,990
So first of all,

3
00:00:06,990 --> 00:00:11,650
we're going to be looking at the concept of Amazon Elastic Inference.

4
00:00:11,650 --> 00:00:14,620
Then, we'll be looking at multi‑model endpoints.

5
00:00:14,620 --> 00:00:15,690
And finally.

6
00:00:15,690 --> 00:00:18,940
we'll be looking at inference pipelines.

7
00:00:18,940 --> 00:00:22,640
So first, let's look at Amazon Elastic Inference.

8
00:00:22,640 --> 00:00:27,590
So Elastic Inference gives you access to a fractional GPU as opposed to

9
00:00:27,590 --> 00:00:31,170
fully dedicating a GPU to your inference instances.

10
00:00:31,170 --> 00:00:33,330
In this case, you get that fractional GPU,

11
00:00:33,330 --> 00:00:39,240
which is also a fraction of the overall cost of a fully dedicated GPU.

12
00:00:39,240 --> 00:00:42,940
Now it's important to note here, this isn't supported in all use cases,

13
00:00:42,940 --> 00:00:46,690
and it only works with frameworks that have been pre‑configured to work with it.

14
00:00:46,690 --> 00:00:49,180
So the support here will vary,

15
00:00:49,180 --> 00:00:52,780
and you need to look at the documentation if you're considering this to see

16
00:00:52,780 --> 00:00:55,920
if the framework you're planning to use is supported.

17
00:00:55,920 --> 00:01:00,710
Now next, let's talk about the concept of multi‑model endpoints.

18
00:01:00,710 --> 00:01:03,840
Now this is a bit different from something that we discussed earlier,

19
00:01:03,840 --> 00:01:06,550
and that's using multiple production variants.

20
00:01:06,550 --> 00:01:10,510
But in this case, endpoints can serve multiple distinct models.

21
00:01:10,510 --> 00:01:13,620
So let's say you maybe have five or six different models that

22
00:01:13,620 --> 00:01:17,070
you use within your organization. Instead of having to spin up

23
00:01:17,070 --> 00:01:18,700
infrastructure for each of those,

24
00:01:18,700 --> 00:01:22,570
you could choose to deploy all of them to a single endpoint and be

25
00:01:22,570 --> 00:01:25,140
able to serve each of those models distinctly.

26
00:01:25,140 --> 00:01:30,140
It also supports multiple production variants on multiple models. So you

27
00:01:30,140 --> 00:01:34,040
theoretically can have four or five distinct different models and then have

28
00:01:34,040 --> 00:01:37,840
each of those with multiple production variants on each.

29
00:01:37,840 --> 00:01:41,200
But note here, it doesn't support serial inference pipelines,

30
00:01:41,200 --> 00:01:44,340
and we'll talk more about inference pipelines next.

31
00:01:44,340 --> 00:01:48,540
It also does not support GPU instance types or Elastic Inference,

32
00:01:48,540 --> 00:01:50,640
so there are some limitations there.

33
00:01:50,640 --> 00:01:55,080
But this can greatly reduce your overall cost by leveraging existing

34
00:01:55,080 --> 00:01:59,140
instances instead of having to spin up new ones for each model.

35
00:01:59,140 --> 00:01:59,920
Now next,

36
00:01:59,920 --> 00:02:03,530
an inference pipeline is an Amazon SageMaker model that is

37
00:02:03,530 --> 00:02:07,240
composed of a linear sequence of two to five containers that

38
00:02:07,240 --> 00:02:11,510
process requests for inferences on data. And in certain

39
00:02:11,510 --> 00:02:14,220
situations, this can be a very powerful feature.

40
00:02:14,220 --> 00:02:16,850
Now as mentioned, it supports two to five containers.

41
00:02:16,850 --> 00:02:21,090
So we do have some limits here on how much we can do. And the communication

42
00:02:21,090 --> 00:02:25,010
actually happens as HTTP requests between the containers.

43
00:02:25,010 --> 00:02:28,640
So just like we kick off the process with an HTTP request,

44
00:02:28,640 --> 00:02:31,480
it just continues chaining those requests together through each of

45
00:02:31,480 --> 00:02:34,340
the containers that are a part of the pipeline.

46
00:02:34,340 --> 00:02:37,640
Now the containers are located on the same EC2 instances,

47
00:02:37,640 --> 00:02:41,250
so this really minimizes latency overall. There's able to be fairly

48
00:02:41,250 --> 00:02:44,380
seamless communication between those containers.

49
00:02:44,380 --> 00:02:47,720
And what it can do is it can enable a complete workflow

50
00:02:47,720 --> 00:02:50,340
without you having to worry about doing any external data

51
00:02:50,340 --> 00:02:57,000
pre‑processing or post‑processing. You can just integrate all of that into the pipeline.

