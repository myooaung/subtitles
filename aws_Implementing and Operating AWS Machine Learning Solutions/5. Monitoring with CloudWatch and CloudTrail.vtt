WEBVTT
1
00:00:01.040 --> 00:00:04.230
So an important element of creating both highly available

2
00:00:04.230 --> 00:00:08.280
and fault‑tolerant solutions on AWS is how we monitor the

3
00:00:08.280 --> 00:00:10.340
solutions that we deploy.

4
00:00:10.340 --> 00:00:14.440
And here, we're really talking about two different solutions.

5
00:00:14.440 --> 00:00:17.560
The first is Amazon CloudWatch,

6
00:00:17.560 --> 00:00:20.940
and this is what allows us to capture metrics and logs,

7
00:00:20.940 --> 00:00:23.110
in this case from our SageMaker instances.

8
00:00:23.110 --> 00:00:27.040
But you'll see that CloudWatch works with many different services on AWS.

9
00:00:27.040 --> 00:00:31.040
And then on the other side, we have AWS CloudTrail,

10
00:00:31.040 --> 00:00:35.290
which creates an audit trail for actions that are taken against SageMaker.

11
00:00:35.290 --> 00:00:36.650
But just like with CloudWatch,

12
00:00:36.650 --> 00:00:40.840
this also integrates with other services on the AWS platform.

13
00:00:40.840 --> 00:00:46.040
So let's first look at the metrics that we capture with Amazon CloudWatch.

14
00:00:46.040 --> 00:00:46.700
So first,

15
00:00:46.700 --> 00:00:51.770
it's important to note that it collects metrics from our SageMaker instances,

16
00:00:51.770 --> 00:00:53.910
and this includes things like our training jobs,

17
00:00:53.910 --> 00:00:57.940
for example, as well as our hosted endpoints.

18
00:00:57.940 --> 00:01:01.700
Now in this case, metrics are going to be captured in a 1‑minute frequency.

19
00:01:01.700 --> 00:01:06.040
So we get a lot of updates about our instances and how they're performing.

20
00:01:06.040 --> 00:01:10.360
Now this metric data is going to be stored within the service for 15 months.

21
00:01:10.360 --> 00:01:13.910
However, you can only look at 2 weeks within the console.

22
00:01:13.910 --> 00:01:18.890
But you can utilize the CLI and the SDK to get access to the full 15 months

23
00:01:18.890 --> 00:01:23.140
of data Now another important thing to note about CloudWatch metrics is

24
00:01:23.140 --> 00:01:27.340
that you can configure alarms based on metrics.

25
00:01:27.340 --> 00:01:29.600
So if something isn't performing as it should be,

26
00:01:29.600 --> 00:01:32.840
you don't have to wait until you go in and check and find that out.

27
00:01:32.840 --> 00:01:35.170
The service can be proactive in letting you know that

28
00:01:35.170 --> 00:01:37.340
something isn't performing as it should be.

29
00:01:37.340 --> 00:01:39.790
Now let's use an endpoint example.

30
00:01:39.790 --> 00:01:44.940
So here, we're going to be looking at metrics for a SageMaker endpoint.

31
00:01:44.940 --> 00:01:48.740
So we could, for example, have Invocation4XXErrors.

32
00:01:48.740 --> 00:01:51.350
We could also have Invocation5XXErrors.

33
00:01:51.350 --> 00:01:54.220
We can have the total number of invocations,

34
00:01:54.220 --> 00:01:57.760
invocations per instance, and we've actually already talked about this metric.

35
00:01:57.760 --> 00:01:59.340
This is what we use with scaling.

36
00:01:59.340 --> 00:02:03.460
We also could have ModelLatency and OverheadLatency.

37
00:02:03.460 --> 00:02:05.520
Now I've included a note here that there are

38
00:02:05.520 --> 00:02:10.440
dimensions provided on these metrics, and those are EndpointName and VariantName.

39
00:02:10.440 --> 00:02:13.440
So if you're trying to determine the number of 500 errors that

40
00:02:13.440 --> 00:02:16.160
happen for a specific production variant,

41
00:02:16.160 --> 00:02:19.060
you can go into CloudWatch and get that data.

42
00:02:19.060 --> 00:02:22.220
If you're trying to find that out for time within the last 2 weeks,

43
00:02:22.220 --> 00:02:24.260
you can go into the console and get that.

44
00:02:24.260 --> 00:02:27.110
You can even save that information in a dashboard that

45
00:02:27.110 --> 00:02:28.770
you can go back and visit later.

46
00:02:28.770 --> 00:02:33.610
Or you can configure an alarm based on that metric so that it can send you a

47
00:02:33.610 --> 00:02:38.430
notification when you have a threshold that is met Now next,

48
00:02:38.430 --> 00:02:40.720
we have Amazon CloudWatch Logs,

49
00:02:40.720 --> 00:02:44.140
which is another aspect to the CloudWatch services.

50
00:02:44.140 --> 00:02:46.820
And what this does is it's going to collect the standard output and

51
00:02:46.820 --> 00:02:50.100
standard error output for your SageMaker instances.

52
00:02:50.100 --> 00:02:51.720
And this includes your training jobs,

53
00:02:51.720 --> 00:02:55.970
as well as your endpoints and your notebook instances and any

54
00:02:55.970 --> 00:02:59.240
batch transform jobs that you've executed.

55
00:02:59.240 --> 00:03:02.270
So log events are grouped into log streams,

56
00:03:02.270 --> 00:03:05.830
and those streams are grouped in to overall log groups.

57
00:03:05.830 --> 00:03:08.100
And within a log group, you can set settings like,

58
00:03:08.100 --> 00:03:10.020
for example, retention settings.

59
00:03:10.020 --> 00:03:13.950
Now we mentioned that CloudWatch metrics are available for up to 15 months.

60
00:03:13.950 --> 00:03:16.040
However, with CloudWatch Logs,

61
00:03:16.040 --> 00:03:19.540
you get to configure how long you want those logs to stay around.

62
00:03:19.540 --> 00:03:21.910
And you will pay some for the amount of storage

63
00:03:21.910 --> 00:03:24.540
that those logs actually take up.

64
00:03:24.540 --> 00:03:27.540
Let's look at an example log configuration.

65
00:03:27.540 --> 00:03:31.260
So here, we have a log group for a SageMaker endpoint,

66
00:03:31.260 --> 00:03:36.790
and so it's going to follow this standard structure of /aws/sagemaker/Endpoints,

67
00:03:36.790 --> 00:03:39.140
and then whatever name you give that endpoint.

68
00:03:39.140 --> 00:03:40.410
Now within that,

69
00:03:40.410 --> 00:03:43.900
there will be log streams for each production variant and then for

70
00:03:43.900 --> 00:03:47.150
each instance that uses a production variant.

71
00:03:47.150 --> 00:03:49.350
So you'll see that'll follow the naming structure of the

72
00:03:49.350 --> 00:03:53.440
production variant name slash and then the instance‑id.

73
00:03:53.440 --> 00:03:58.100
So if we look here, here is a screenshot from my CloudWatch logs,

74
00:03:58.100 --> 00:04:01.170
specifically for the demo that we set up in a previous module.

75
00:04:01.170 --> 00:04:05.440
And here, you can see that we have an overall log group,

76
00:04:05.440 --> 00:04:08.910
and this has the DEMO‑XGBoostEndpoint and then the date string

77
00:04:08.910 --> 00:04:11.700
that I've included after thatf as the log group, and then the

78
00:04:11.700 --> 00:04:13.840
specific streams are included.

79
00:04:13.840 --> 00:04:17.320
And first, we can see here that we have Model2, which is a production

80
00:04:17.320 --> 00:04:20.870
variant, and then we can see an instance ID for a specific instance

81
00:04:20.870 --> 00:04:23.080
that was running that particular model.

82
00:04:23.080 --> 00:04:26.350
We can see that we also have Model1, and then we have the AllTraffic,

83
00:04:26.350 --> 00:04:31.040
and that was before we had actually added in the different production variants.

84
00:04:31.040 --> 00:04:35.350
Now let's talk next about AWS CloudTrail. And this is different from

85
00:04:35.350 --> 00:04:39.140
CloudWatch because, in this case, it's more about providing an audit trail

86
00:04:39.140 --> 00:04:42.870
because it captures actions from the SageMaker API,

87
00:04:42.870 --> 00:04:44.390
and this could be from users.

88
00:04:44.390 --> 00:04:48.660
So if you want to know what user went in and edited an endpoint,

89
00:04:48.660 --> 00:04:49.940
you could find that out.

90
00:04:49.940 --> 00:04:53.670
Also, roles and other AWS Services that have taken

91
00:04:53.670 --> 00:04:56.740
actions on your behalf against SageMaker.

92
00:04:56.740 --> 00:05:00.050
Now once it is configured, and it's not configured by default,

93
00:05:00.050 --> 00:05:04.040
but it will collect all of this data into Amazon S3.

94
00:05:04.040 --> 00:05:06.570
And since this data is stored in Amazon S3,

95
00:05:06.570 --> 00:05:09.740
things like retention can be configured by you.

96
00:05:09.740 --> 00:05:11.440
You can make it work how you want.

97
00:05:11.440 --> 00:05:14.820
You can even use S3 lifecycle rules to potentially send this

98
00:05:14.820 --> 00:05:17.540
out to Glacier or Glacier Deep Archive.

99
00:05:17.540 --> 00:05:20.040
And this does provide that audit trail that really

100
00:05:20.040 --> 00:05:22.740
should be in place for any AWS service.

101
00:05:22.740 --> 00:05:31.000
And if you're interested in viewing this information, you can actually go in and query this data using AWS Athena.

