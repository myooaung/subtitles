WEBVTT
1
00:00:00.640 --> 00:00:02.260
[Autogenerated] Okay, let's dive into Ken.

2
00:00:02.260 --> 00:00:03.860
Is is data firehose.

3
00:00:03.860 --> 00:00:06.420
So I'm going to get started in this time,

4
00:00:06.420 --> 00:00:09.700
we're gonna use a kinesis firehose delivery stream,

5
00:00:09.700 --> 00:00:12.140
so I'm gonna click on create delivery stream.

6
00:00:12.140 --> 00:00:14.040
This brings us to a similar wizard.

7
00:00:14.040 --> 00:00:17.140
When we created our data stream, so I had to give it a name.

8
00:00:17.140 --> 00:00:19.440
We'll call it Pluralsight firehose.

9
00:00:19.440 --> 00:00:21.780
And from here, we choose a source.

10
00:00:21.780 --> 00:00:26.440
So what error we ingesting records from into our delivery stream.

11
00:00:26.440 --> 00:00:28.870
Now you can do direct puts or other sources,

12
00:00:28.870 --> 00:00:31.720
So if you have your own custom message producers,

13
00:00:31.720 --> 00:00:35.730
you can use those or you can send them from services like I O.

14
00:00:35.730 --> 00:00:36.610
T.

15
00:00:36.610 --> 00:00:38.240
And then CloudWatch.

16
00:00:38.240 --> 00:00:38.620
Now,

17
00:00:38.620 --> 00:00:42.290
another fancy thing is you can go ahead and put in

18
00:00:42.290 --> 00:00:44.550
Ken is is data stream records.

19
00:00:44.550 --> 00:00:48.720
And what this does is it allows us to ingest massive

20
00:00:48.720 --> 00:00:53.270
amounts of data from several different nested streams and

21
00:00:53.270 --> 00:00:55.740
then output it to a destination.

22
00:00:55.740 --> 00:00:57.970
So it allows for a lot of flexibility.

23
00:00:57.970 --> 00:01:01.670
I'll put direct put for this, and then we have this similar option.

24
00:01:01.670 --> 00:01:04.820
We can enable server site encryption for source

25
00:01:04.820 --> 00:01:07.190
records and we can specify the type.

26
00:01:07.190 --> 00:01:08.510
So what kind of key?

27
00:01:08.510 --> 00:01:11.240
So with that I'm going to click on next,

28
00:01:11.240 --> 00:01:13.740
and this brings us to our transformation.

29
00:01:13.740 --> 00:01:16.160
So if we wanted to, if you remember,

30
00:01:16.160 --> 00:01:20.080
we can transform our records with AWS Lambda.

31
00:01:20.080 --> 00:01:22.640
So what happens is the record comes in.

32
00:01:22.640 --> 00:01:26.710
It invokes a lambda, which transforms that source record,

33
00:01:26.710 --> 00:01:30.470
and then it puts it back into the delivery stream for

34
00:01:30.470 --> 00:01:32.620
whatever output processing we want.

35
00:01:32.620 --> 00:01:35.560
So if I wanted to, I would click enabled,

36
00:01:35.560 --> 00:01:37.920
and I would choose my lamb to function here that

37
00:01:37.920 --> 00:01:40.440
performs this data transformation.

38
00:01:40.440 --> 00:01:43.650
This is really meet that it ties in essentially,

39
00:01:43.650 --> 00:01:47.200
seamlessly and handles transformation without having

40
00:01:47.200 --> 00:01:49.280
to put the record back in manually.

41
00:01:49.280 --> 00:01:51.540
It's a really cool integration.

42
00:01:51.540 --> 00:01:55.140
I'm gonna choose disabled because I'm not transforming anything,

43
00:01:55.140 --> 00:01:58.040
and then we can convert our record format.

44
00:01:58.040 --> 00:02:03.240
So what this allows us to do is convert our JSON formatted source records

45
00:02:03.240 --> 00:02:07.290
using some type of schema that we can define and you can see.

46
00:02:07.290 --> 00:02:11.070
They even recommend that the Apache formats listed here

47
00:02:11.070 --> 00:02:13.430
are usually more efficient to query.

48
00:02:13.430 --> 00:02:17.270
And that's because with data firehose, we're not ingesting.

49
00:02:17.270 --> 00:02:21.060
A few 100 records usually were usually ingesting a massive

50
00:02:21.060 --> 00:02:23.640
amount up to thousands and thousands.

51
00:02:23.640 --> 00:02:27.650
So that's why you might have a conversion done because you are working with

52
00:02:27.650 --> 00:02:31.170
a much larger dataset than you would with a data stream.

53
00:02:31.170 --> 00:02:32.430
I'll choose disabled.

54
00:02:32.430 --> 00:02:35.840
I'll go to next, and now we look at our destination.

55
00:02:35.840 --> 00:02:40.690
So where do we want to output this data and you can see we have four options.

56
00:02:40.690 --> 00:02:42.030
There's S3.

57
00:02:42.030 --> 00:02:46.160
There's red shift, elastic search and then Splunk-ES.

58
00:02:46.160 --> 00:02:48.850
Now we're gonna look at red shift and elastic search a

59
00:02:48.850 --> 00:02:51.340
little bit later in this course,

60
00:02:51.340 --> 00:02:57.040
and then Splunk-ES is is a very well known tool for operational intelligence,

61
00:02:57.040 --> 00:03:00.740
and it allows for extremely good monitoring and logging.

62
00:03:00.740 --> 00:03:03.840
Now I'll choose S3 for this example,

63
00:03:03.840 --> 00:03:06.220
and down at the bottom is where I can figure my bucket.

64
00:03:06.220 --> 00:03:07.870
So let me choose my bucket.

65
00:03:07.870 --> 00:03:10.080
I'll choose my Pluralsight bucket that I used

66
00:03:10.080 --> 00:03:12.670
previously and you can see by default.

67
00:03:12.670 --> 00:03:17.240
Ken is is appends a prefix, which is this format listed here.

68
00:03:17.240 --> 00:03:21.690
If I wanted to I can override that in supply a custom one.

69
00:03:21.690 --> 00:03:25.480
Now this prefix that they automatically apply by default

70
00:03:25.480 --> 00:03:28.090
allows for better data partitioning,

71
00:03:28.090 --> 00:03:33.120
which allows for much more optimize reading and getting of objects.

72
00:03:33.120 --> 00:03:37.540
This is a best practice, and you should really follow it whenever you can,

73
00:03:37.540 --> 00:03:43.440
so I'll go ahead and leave the default and then for errors if we wanted to.

74
00:03:43.440 --> 00:03:46.500
We can specify a different prefix for those records.

75
00:03:46.500 --> 00:03:47.520
So if we have an error,

76
00:03:47.520 --> 00:03:53.240
maybe I wanna put error here and then that's the prefix for any error records.

77
00:03:53.240 --> 00:03:56.810
But the default successful ones will follow this default here,

78
00:03:56.810 --> 00:04:00.730
so I'll go to next, and this brings us to our buffer size.

79
00:04:00.730 --> 00:04:02.930
So how many records do we want?

80
00:04:02.930 --> 00:04:06.040
A buffer before delivering him to our bucket.

81
00:04:06.040 --> 00:04:10.160
So we can buffer between one and 128 megabytes.

82
00:04:10.160 --> 00:04:14.840
Let me just make this one, and then you can see there's also an interval.

83
00:04:14.840 --> 00:04:17.850
Now you can see this is 300 seconds and we can enter a

84
00:04:17.850 --> 00:04:20.550
minute all the way up to 15 minutes.

85
00:04:20.550 --> 00:04:24.960
But what these error doing is essentially giving us to thresholds

86
00:04:24.960 --> 00:04:28.260
for us to conditionally forward records with.

87
00:04:28.260 --> 00:04:31.300
If one of these gets breached, then it's going to go ahead,

88
00:04:31.300 --> 00:04:33.030
cut off right there,

89
00:04:33.030 --> 00:04:37.400
send those records to our output and then continue ingesting later on.

90
00:04:37.400 --> 00:04:42.220
This is a either or clause, so if our buffer size gets broke,

91
00:04:42.220 --> 00:04:43.850
it doesn't matter how long it's been.

92
00:04:43.850 --> 00:04:47.360
It'll ignore the interval period and then vice versa.

93
00:04:47.360 --> 00:04:51.760
If we only have a half a Meg in here, but it's been five minutes,

94
00:04:51.760 --> 00:04:54.240
then it's going to go ahead and send that data out.

95
00:04:54.240 --> 00:04:57.130
Now let's move on to S3 compression and encryption.

96
00:04:57.130 --> 00:05:02.700
So if we wanted to we can gzip snappy or zip these files now,

97
00:05:02.700 --> 00:05:05.590
I'll do disabled for now because I'm gonna kill this right away.

98
00:05:05.590 --> 00:05:08.460
But it's usually a really good practice to gzip it.

99
00:05:08.460 --> 00:05:11.020
So then your data is compressed and it's easier to

100
00:05:11.020 --> 00:05:13.440
parse and pulled down for analytics.

101
00:05:13.440 --> 00:05:16.000
And you can also see we can encrypt our bucket so I can

102
00:05:16.000 --> 00:05:18.850
use my master key to encrypt our files.

103
00:05:18.850 --> 00:05:23.340
I'll choose disabled, but that options there for compliance purposes.

104
00:05:23.340 --> 00:05:25.950
Now we can also do error logging to CloudWatch,

105
00:05:25.950 --> 00:05:29.590
which I'll go ahead and leave enabled we can give our tags,

106
00:05:29.590 --> 00:05:33.060
and then we have to specify and I am role,

107
00:05:33.060 --> 00:05:36.610
and what this does is allows us to create a delivery role for

108
00:05:36.610 --> 00:05:39.970
fire hose that has the basic permissions.

109
00:05:39.970 --> 00:05:43.760
So this is more than enough because it's gonna go ahead and allow us

110
00:05:43.760 --> 00:05:48.900
to put data into our bucket that we talked about and then it could

111
00:05:48.900 --> 00:05:51.120
invoke a function if we wanted it to.

112
00:05:51.120 --> 00:05:53.940
We don't have that configured, but that's all right.

113
00:05:53.940 --> 00:05:55.470
We're putting log events.

114
00:05:55.470 --> 00:06:00.340
We have some Ken is is actions and then some kms.

115
00:06:00.340 --> 00:06:03.790
So I'm gonna go ahead and click on allow it creates the role,

116
00:06:03.790 --> 00:06:06.020
brings us back, and there you go.

117
00:06:06.020 --> 00:06:11.020
So I'll click on next we can review everything, I'll create it.

118
00:06:11.020 --> 00:06:12.470
And then while this is creating,

119
00:06:12.470 --> 00:06:16.920
I'll pause the clip and once it's done in available will pick back up.

120
00:06:16.920 --> 00:06:20.540
Okay, That only took about a minute for it to become active.

121
00:06:20.540 --> 00:06:23.100
And you can see our settings here were direct put

122
00:06:23.100 --> 00:06:25.300
in other sources for our source.

123
00:06:25.300 --> 00:06:28.940
We're not doing transformations, and we're out putting to S3.

124
00:06:28.940 --> 00:06:32.390
So let me go and open this up just so we can have it ready.

125
00:06:32.390 --> 00:06:35.100
And then I'm gonna go down to Pluralsight firehose.

126
00:06:35.100 --> 00:06:37.910
Now, this has all of our details, which we looked at,

127
00:06:37.910 --> 00:06:40.240
so I'm not gonna go through this page.

128
00:06:40.240 --> 00:06:43.790
But you can see here they allow us to test with demo data,

129
00:06:43.790 --> 00:06:45.840
and this is a ws provided.

130
00:06:45.840 --> 00:06:50.930
So if I click start sending demo data, you can see the format here at the top.

131
00:06:50.930 --> 00:06:56.140
This is what we should see and it starts sending data to our destination.

132
00:06:56.140 --> 00:06:58.010
So when I'm done, I'll click on stop.

133
00:06:58.010 --> 00:07:02.040
But for now, let me go to S3 are fresh.

134
00:07:02.040 --> 00:07:05.440
And then what I'll do is I'm gonna give this a little bit of time.

135
00:07:05.440 --> 00:07:07.890
I'll see how long it takes and I'll report back.

136
00:07:07.890 --> 00:07:11.000
And then we'll pick back up after I pause here so we

137
00:07:11.000 --> 00:07:12.740
can start looking at these records.

138
00:07:12.740 --> 00:07:16.890
All right, So you can see now that it started populating data.

139
00:07:16.890 --> 00:07:18.740
We have the year.

140
00:07:18.740 --> 00:07:25.440
We have the month, the day and then the hour in UTC time.

141
00:07:25.440 --> 00:07:31.340
And then we have our record here, so I'll see if I can open this.

142
00:07:31.340 --> 00:07:34.480
I'll go ahead and open it with the default text editor and then let

143
00:07:34.480 --> 00:07:37.900
me go over to it and you can see all of the data.

144
00:07:37.900 --> 00:07:40.920
So these error all of those records that were ingested

145
00:07:40.920 --> 00:07:43.370
and put into our delivery stream.

146
00:07:43.370 --> 00:07:45.610
You can see there's a massive amount.

147
00:07:45.610 --> 00:07:49.840
We're not gonna count him, but you can see we have our ticker symbol,

148
00:07:49.840 --> 00:07:51.640
the sector,

149
00:07:51.640 --> 00:07:55.610
and then all of the other data that's in there and there is hundreds and

150
00:07:55.610 --> 00:07:59.640
hundreds of records that are populated from that demo data.

151
00:07:59.640 --> 00:08:01.640
So I may go and go back.

152
00:08:01.640 --> 00:08:04.700
I'll go back to Fire Hose, I'll stop sending.

153
00:08:04.700 --> 00:08:07.050
And now if I look at monitoring we can, See,

154
00:08:07.050 --> 00:08:09.430
we have a lot of incoming bites and records,

155
00:08:09.430 --> 00:08:14.440
and eventually we would see our delivery to S3 records populate here.

156
00:08:14.440 --> 00:08:14.720
Now,

157
00:08:14.720 --> 00:08:17.270
another cool feature from the console is you can see

158
00:08:17.270 --> 00:08:19.820
what law group you're out putting too.

159
00:08:19.820 --> 00:08:28.030
So in CloudWatch, if I go to that log group eventually,

160
00:08:28.030 --> 00:08:32.400
this is going to start being populated with log files if we enable it.

161
00:08:32.400 --> 00:08:34.340
So it's a pretty cool feature.

162
00:08:34.340 --> 00:08:34.590
Now,

163
00:08:34.590 --> 00:08:37.820
one last thing I want to talk about before we wrap up is

164
00:08:37.820 --> 00:08:40.940
this source record as three backup.

165
00:08:40.940 --> 00:08:43.100
So what that does is if you remember,

166
00:08:43.100 --> 00:08:46.990
in our diagram we had it allows us to back up our

167
00:08:46.990 --> 00:08:50.740
source records before we convert them.

168
00:08:50.740 --> 00:08:54.740
So if we had a failed transformation from our conversion up here,

169
00:08:54.740 --> 00:08:58.610
what we can do is we can go ahead and specify a source record back

170
00:08:58.610 --> 00:09:01.350
up in a different bucket or a different prefix.

171
00:09:01.350 --> 00:09:04.730
So then we have the original record for diagnosis and we can.

172
00:09:04.730 --> 00:09:06.740
See what went wrong.

173
00:09:06.740 --> 00:09:08.420
Now that's gonna do it for this data.

174
00:09:08.420 --> 00:09:09.970
Firehose demo.

175
00:09:09.970 --> 00:09:11.640
I hope you've learned quite a bit.

176
00:09:11.640 --> 00:09:18.000
In the next clips, we're going to start looking at video streams and Data Analytics.

