1
00:00:00,940 --> 00:00:01,440
[Autogenerated] Alrighty.

2
00:00:01,440 --> 00:00:03,550
Let's dive into Amazon Kinesis.

3
00:00:03,550 --> 00:00:07,580
Now, the first clip we're going to look at is for data streams.

4
00:00:07,580 --> 00:00:10,550
Now, we just had an overview on what data streams are,

5
00:00:10,550 --> 00:00:15,350
but let's go ahead and start looking at how we can create a kinesis data stream.

6
00:00:15,350 --> 00:00:19,820
And then how we can create a producer and a consumer so

7
00:00:19,820 --> 00:00:22,410
we can visualize the flow of data.

8
00:00:22,410 --> 00:00:23,100
Something to click on.

9
00:00:23,100 --> 00:00:23,780
Get started.

10
00:00:23,780 --> 00:00:26,140
Here on the cane is is dashboard.

11
00:00:26,140 --> 00:00:29,270
And you see, we get our four options here on the left.

12
00:00:29,270 --> 00:00:30,540
Now, for this clip,

13
00:00:30,540 --> 00:00:33,520
we're just gonna focus on the first option ingest and

14
00:00:33,520 --> 00:00:36,850
process streaming data with Chinese streams.

15
00:00:36,850 --> 00:00:41,240
These other three over here, we'll talk about in some upcoming clips.

16
00:00:41,240 --> 00:00:41,380
Now,

17
00:00:41,380 --> 00:00:44,200
I'm gonna go ahead and click on Create Data Stream and

18
00:00:44,200 --> 00:00:46,140
it brings me to my create scream.

19
00:00:46,140 --> 00:00:47,710
I had to give it a name.

20
00:00:47,710 --> 00:00:51,880
We'll call it Pluralsight Stream, and then we go down to our shards.

21
00:00:51,880 --> 00:00:56,810
So if you recall a shard is how we scale Arkin is is stream and

22
00:00:56,810 --> 00:01:00,890
a very important factor here is that in each shard can ingest

23
00:01:00,890 --> 00:01:05,660
up to one megabyte per second or up to 1000 records per second

24
00:01:05,660 --> 00:01:09,180
there's a buffer limit, and as soon as it hits that buffer limit,

25
00:01:09,180 --> 00:01:11,800
it's going to cut off that shard and then start

26
00:01:11,800 --> 00:01:14,940
generating new records into the shard itself.

27
00:01:14,940 --> 00:01:18,880
And you can see here that they actually read at two megabytes per

28
00:01:18,880 --> 00:01:22,540
second so we can pull data at two Meg per second.

29
00:01:22,540 --> 00:01:27,340
Now, an important thing to keep in mind is that if you want to scale up,

30
00:01:27,340 --> 00:01:30,140
you have to increase the number of shards.

31
00:01:30,140 --> 00:01:33,730
And we specify that here in this box right below this

32
00:01:33,730 --> 00:01:36,950
diagram and what's nice is in the console.

33
00:01:36,950 --> 00:01:42,010
A W s provides you a shard calculator so you can estimate what you need.

34
00:01:42,010 --> 00:01:45,000
So we just guess our average record size,

35
00:01:45,000 --> 00:01:48,250
Let's just say 2 56 were going to Max, right?

36
00:01:48,250 --> 00:01:53,240
25 records per second number of consumer applications will have one.

37
00:01:53,240 --> 00:01:56,220
So with this, they're estimating we need seven shards,

38
00:01:56,220 --> 00:02:00,040
and that's based off this ingest limit at the top.

39
00:02:00,040 --> 00:02:01,900
Now, I'm not going to come close to this,

40
00:02:01,900 --> 00:02:04,680
so I'm gonna go and exit this and I'm just gonna put one

41
00:02:04,680 --> 00:02:07,040
because that could get pretty expensive,

42
00:02:07,040 --> 00:02:11,050
and that brings us to our total stream capacity so using

43
00:02:11,050 --> 00:02:13,030
our shards that we just specified.

44
00:02:13,030 --> 00:02:13,310
You see,

45
00:02:13,310 --> 00:02:17,190
we have a one megabyte per second in the 1000 records

46
00:02:17,190 --> 00:02:20,140
per second as our right limits.

47
00:02:20,140 --> 00:02:23,510
So if I change this to two, you can see it doubles,

48
00:02:23,510 --> 00:02:25,380
which is expected, so I'll keep it back.

49
00:02:25,380 --> 00:02:28,940
At one I'll click on Create and that's it.

50
00:02:28,940 --> 00:02:30,490
You have now created a can.

51
00:02:30,490 --> 00:02:35,000
Is is data stream within a W s now or done with the

52
00:02:35,000 --> 00:02:37,840
can is is part is far as creating.

53
00:02:37,840 --> 00:02:40,880
But what I want to do is going to here and I want

54
00:02:40,880 --> 00:02:43,140
to run through this little menu.

55
00:02:43,140 --> 00:02:46,860
So in the details page we see our S3 Marne our status.

56
00:02:46,860 --> 00:02:51,040
And then when we look in the middle, we see our number of open shards.

57
00:02:51,040 --> 00:02:54,040
But there's also a close shards count.

58
00:02:54,040 --> 00:02:57,800
So what's cool about Ken is is if you tie this back to the

59
00:02:57,800 --> 00:03:01,810
retention period that they talk about, what happens is for us.

60
00:03:01,810 --> 00:03:07,240
Since we're specifying a data retention period of 24 hours,

61
00:03:07,240 --> 00:03:11,270
what you can see here anytime we close a shard,

62
00:03:11,270 --> 00:03:15,400
the data in that shard will be available for consumers

63
00:03:15,400 --> 00:03:17,700
for that entire retention period.

64
00:03:17,700 --> 00:03:21,070
So that shard does not go away completely until the data

65
00:03:21,070 --> 00:03:24,440
retention period you specified is over.

66
00:03:24,440 --> 00:03:28,840
Now, the next thing we can do is we can enable services IT encryption.

67
00:03:28,840 --> 00:03:30,030
So if we wanted to,

68
00:03:30,030 --> 00:03:36,790
for compliance purposes we can encrypt all data coming into Ken is is using kms.

69
00:03:36,790 --> 00:03:38,490
So I can go ahead and use the default key.

70
00:03:38,490 --> 00:03:42,240
If I wanted to, in all of the traffic would be encrypted.

71
00:03:42,240 --> 00:03:45,640
I'm not going to enable that because that encourage some extra cost.

72
00:03:45,640 --> 00:03:47,630
And then we just covered data retention.

73
00:03:47,630 --> 00:03:50,840
So let's look at shard level metrics.

74
00:03:50,840 --> 00:03:58,640
So here we can actually look at shard level metrics that get measured by a W s.

75
00:03:58,640 --> 00:04:02,910
So we can go ahead and identify all of these and then

76
00:04:02,910 --> 00:04:05,740
go ahead and collect that data.

77
00:04:05,740 --> 00:04:09,500
Now you'll notice it's an additional cost because it's picking up

78
00:04:09,500 --> 00:04:12,640
massive amounts of data in a one minute period.

79
00:04:12,640 --> 00:04:13,980
So I don't want to enable that.

80
00:04:13,980 --> 00:04:16,180
But if you wanted to view your metrics,

81
00:04:16,180 --> 00:04:18,740
that's how you could do it at a shard level.

82
00:04:18,740 --> 00:04:20,140
So I'll cancel out.

83
00:04:20,140 --> 00:04:23,660
And what I want to do here is I want to create a producer

84
00:04:23,660 --> 00:04:26,340
and I did that already here in Lambda.

85
00:04:26,340 --> 00:04:29,640
You'll see that I have my producer function and I

86
00:04:29,640 --> 00:04:31,200
wrote some code in Python here,

87
00:04:31,200 --> 00:04:36,090
let me go and close this and all it's doing is it's creating

88
00:04:36,090 --> 00:04:38,610
a kinesis client with the bow to three.

89
00:04:38,610 --> 00:04:41,950
It's pulling in a stream name from our environment variables,

90
00:04:41,950 --> 00:04:44,470
which is right here at the bottom of the screen.

91
00:04:44,470 --> 00:04:47,290
I'm creating a random string with a function here

92
00:04:47,290 --> 00:04:49,440
with the default length of tin.

93
00:04:49,440 --> 00:04:51,770
And then it's just returning that random string.

94
00:04:51,770 --> 00:04:56,040
Now the lamb to handler itself, which is what actually executes the code.

95
00:04:56,040 --> 00:04:58,460
This is just doing a few different things.

96
00:04:58,460 --> 00:04:59,950
I set an IT aerator.

97
00:04:59,950 --> 00:05:00,300
And then,

98
00:05:00,300 --> 00:05:05,940
while that IT aerator is less than 21 I'm basically generating a random string,

99
00:05:05,940 --> 00:05:09,700
printing out that string into the logs and then putting that data

100
00:05:09,700 --> 00:05:14,670
that string as a record into arcane is is stream and you can see

101
00:05:14,670 --> 00:05:18,340
we're passing in the stream name, which is what we read in our string,

102
00:05:18,340 --> 00:05:21,240
which is set right at the top of this wild loop.

103
00:05:21,240 --> 00:05:23,440
And then we have to set a partition key.

104
00:05:23,440 --> 00:05:27,440
So this is how you can partition data for optimizing reads.

105
00:05:27,440 --> 00:05:29,890
Now, for our use case, I'm gonna put Pluralsight.

106
00:05:29,890 --> 00:05:34,230
So we should see Pluralsight in our partition keys when we start looking at

107
00:05:34,230 --> 00:05:37,930
records and then I just finish it off with plus equals one.

108
00:05:37,930 --> 00:05:41,740
So I'm just adding one as I iterate through this wild loop.

109
00:05:41,740 --> 00:05:43,450
Now that's one side of the equation.

110
00:05:43,450 --> 00:05:44,920
So we have our producer.

111
00:05:44,920 --> 00:05:48,290
We have our stream, but we also need a consumer.

112
00:05:48,290 --> 00:05:53,480
So if I look at my consumer here, you can see I created another function,

113
00:05:53,480 --> 00:05:57,430
and this is just doing a similar thing except for its consuming.

114
00:05:57,430 --> 00:05:59,360
So I have my lamb to handler.

115
00:05:59,360 --> 00:06:02,000
It's pulling in the event and we'll see what that is here

116
00:06:02,000 --> 00:06:05,140
in a minute and then a context object.

117
00:06:05,140 --> 00:06:08,630
Now I'm reading that event, and I'm setting a record variable here,

118
00:06:08,630 --> 00:06:11,530
and I'm pulling in the data from those records.

119
00:06:11,530 --> 00:06:13,010
I'm printing the event.

120
00:06:13,010 --> 00:06:16,600
I'm printing the record, and then I'm printing the partition key.

121
00:06:16,600 --> 00:06:20,620
Just so we can verify that it's actually working in all of these

122
00:06:20,620 --> 00:06:23,670
data values should be output to CloudWatch.

123
00:06:23,670 --> 00:06:26,850
So before we actually do this, I need to configure this,

124
00:06:26,850 --> 00:06:31,100
as a consumer for Ken is is so I'll do this from the Lambda console.

125
00:06:31,100 --> 00:06:36,920
I'll add a trigger I'll select can is is we choose our stream.

126
00:06:36,920 --> 00:06:43,100
An optional consumer are batch size which all set to 20 our batch window.

127
00:06:43,100 --> 00:06:46,180
So how long it's gonna wait to collect records and

128
00:06:46,180 --> 00:06:47,860
then we set our starting position.

129
00:06:47,860 --> 00:06:51,040
So latest is the latest record in the stream.

130
00:06:51,040 --> 00:06:53,840
You can specify at a certain time stamp,

131
00:06:53,840 --> 00:06:55,220
or you can trim horizon,

132
00:06:55,220 --> 00:06:59,040
which is the last record that was trimmed from the last read.

133
00:06:59,040 --> 00:07:02,120
So I'll do trim horizon and you can see we have some

134
00:07:02,120 --> 00:07:05,720
additional setting so on any failures we can alert via a

135
00:07:05,720 --> 00:07:08,550
topic we can set our retry attempts.

136
00:07:08,550 --> 00:07:11,140
Let me just limit this to 100.

137
00:07:11,140 --> 00:07:14,660
We set a max age we can split on error and we can

138
00:07:14,660 --> 00:07:17,340
set concurrent batches per shard.

139
00:07:17,340 --> 00:07:20,230
So I'll enable this trigger and there we go.

140
00:07:20,230 --> 00:07:21,610
We're now pulling in.

141
00:07:21,610 --> 00:07:26,420
Data from that can is is stream once it gets put into the stream itself.

142
00:07:26,420 --> 00:07:30,200
So if I go up to my producer now and I go ahead and test,

143
00:07:30,200 --> 00:07:32,820
this is just invoking it with a hello world function,

144
00:07:32,820 --> 00:07:36,760
we get a success and you see that I'm putting all of

145
00:07:36,760 --> 00:07:39,700
these data strings into the records.

146
00:07:39,700 --> 00:07:42,860
So there's 20 of them in there and they're all random strings.

147
00:07:42,860 --> 00:07:45,340
And if I go to consumer,

148
00:07:45,340 --> 00:07:47,860
I go to monitoring and I'm just gonna shortcut to

149
00:07:47,860 --> 00:07:50,540
CloudWatch here and after I refreshed you.

150
00:07:50,540 --> 00:07:54,800
See, we have our latest stream here, So if I go in here we can see Hey,

151
00:07:54,800 --> 00:07:56,830
I'm printing that record that I received.

152
00:07:56,830 --> 00:08:00,850
So there's 20 records within this batch file within

153
00:08:00,850 --> 00:08:03,170
this batch event that was received.

154
00:08:03,170 --> 00:08:07,070
So these error the random strings that were produced by our function,

155
00:08:07,070 --> 00:08:11,030
our producer function and put into the stream itself.

156
00:08:11,030 --> 00:08:15,440
I'm printing the data itself that was pulled in for the first record.

157
00:08:15,440 --> 00:08:18,380
And then I print Pluralsight, which is our partition key.

158
00:08:18,380 --> 00:08:22,240
And you can see the partition key here in the JSON record.

159
00:08:22,240 --> 00:08:24,870
And then for each record, it gets invoked again.

160
00:08:24,870 --> 00:08:27,540
So there was another record placed on there,

161
00:08:27,540 --> 00:08:32,240
and we can see the data right here and then it's got the same partition key.

162
00:08:32,240 --> 00:08:34,080
So that's how you can use Ken.

163
00:08:34,080 --> 00:08:39,140
Is is to stream data from both producers to consumers.

164
00:08:39,140 --> 00:08:42,680
Now, one thing to keep in mind here is with your shards.

165
00:08:42,680 --> 00:08:44,860
If you're having lambda on the back end,

166
00:08:44,860 --> 00:08:48,810
you're essentially configuring it for Lambda to scale

167
00:08:48,810 --> 00:08:51,540
automatically to the number of shards.

168
00:08:51,540 --> 00:08:55,110
So you'll basically have one concurrent execution per

169
00:08:55,110 --> 00:08:57,440
open shard that you're consuming from.

170
00:08:57,440 --> 00:08:59,210
So be sure to keep that in mind.

171
00:08:59,210 --> 00:09:00,280
That's how that works.

172
00:09:00,280 --> 00:09:03,050
Alright, so that's going to do it for this clip.

173
00:09:03,050 --> 00:09:06,260
Hopefully, you're a little bit more comfortable with poking around.

174
00:09:06,260 --> 00:09:09,010
Can is, is data streams and how they work.

175
00:09:09,010 --> 00:09:13,650
We can figured a producer, the stream itself that ingested records.

176
00:09:13,650 --> 00:09:17,030
And then we configured a Lambda Consumer on the back end that

177
00:09:17,030 --> 00:09:19,610
printed out those records that were received.

178
00:09:19,610 --> 00:09:21,740
And we visualized that here.

179
00:09:21,740 --> 00:09:22,040
Now,

180
00:09:22,040 --> 00:09:24,950
I'll be sure to include this code if you want to play around with

181
00:09:24,950 --> 00:09:28,740
it in the downloadable files for this course.

182
00:09:28,740 --> 00:09:35,000
But with that, let's go ahead and wrap up and then we're gonna move on To Ken. Is is data firehose

