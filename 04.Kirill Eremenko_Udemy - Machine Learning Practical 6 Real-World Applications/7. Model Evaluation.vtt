WEBVTT

00:03.530 --> 00:05.960
So let's take a look at the model evaluation.

00:06.080 --> 00:06.530
OK.

00:06.800 --> 00:08.050
So now within the model.

00:08.090 --> 00:11.410
And we wanted to test that modeling reality.

00:11.450 --> 00:17.000
So we're going to explore the models to data that model hasn't seen before doing the training.

00:17.000 --> 00:22.850
And that's the key essence in testing any machine learning or deep learning model.

00:22.910 --> 00:25.900
So let's take a look at our model here.

00:26.090 --> 00:29.450
So let's assume again let's go back to our two classes we didn't have the classes.

00:29.450 --> 00:35.330
Here we have the red classes and we have kind of two features on the x axis and y axis and we separated

00:35.450 --> 00:40.380
that these two classes using what we call the maximum margin hyperplane.

00:40.700 --> 00:46.460
OK which is the line which is this appalling Dr. machine learning technique has been able to separate

00:46.460 --> 00:48.600
these two classes.

00:49.790 --> 00:50.650
So the point is.

00:50.690 --> 00:52.190
OK so now we changed our model.

00:52.190 --> 00:54.320
Now the model is good looks perfect.

00:54.320 --> 00:55.390
Now we want to test it.

00:55.490 --> 00:56.430
OK.

00:56.450 --> 01:02.170
A key element that we do not use data that we have used during training to test our model.

01:02.240 --> 01:02.590
OK.

01:02.630 --> 01:04.430
We don't simply do this.

01:04.460 --> 01:04.760
Why.

01:04.760 --> 01:07.720
Because in general these models are kind of trained.

01:07.790 --> 01:11.140
They have been trained to to use specific datasets.

01:11.210 --> 01:13.930
We generally don't use the training data to test our model.

01:14.090 --> 01:19.370
We actually use another data sets we'll call it testing data to test our model and see if it can classify

01:19.370 --> 01:22.940
as our data set into again malignant ORBIN I guess.

01:22.940 --> 01:28.640
So that's where we use actual testing data which is kind of data that the model hasn't seen before during

01:28.640 --> 01:29.390
training.

01:29.390 --> 01:34.120
And let's test our model to see if it can actually generalize or classify as or model or not.

01:34.130 --> 01:34.450
OK.

01:34.550 --> 01:36.560
So that's how we evaluate our model.

01:37.520 --> 01:43.760
So one of the key features or out of the or one of the key objectives are our machine learning techniques

01:43.880 --> 01:50.680
that we want these models to generalize the data which mean what we mean by modern generalization is

01:50.690 --> 01:59.240
that we want the machine learning strategy to not train or to not train our model specifically for this

01:59.240 --> 02:00.380
training data set.

02:00.440 --> 02:06.410
We want the model to be general to basically look at most of the images moving forward of let's say

02:06.410 --> 02:12.860
cancer data or images and tell us if it's malignant or benign even if the images hasn't been seen before

02:12.900 --> 02:13.770
during training.

02:13.980 --> 02:14.260
OK.

02:14.270 --> 02:18.710
And that's basically what how can we effectively use machine anti-theists techniques or strategies in

02:18.710 --> 02:19.240
real life.

02:19.310 --> 02:20.140
All right.

02:20.720 --> 02:27.590
So this model actually if if our support machine was able to classify or maybe put the hyperplane in

02:27.590 --> 02:30.980
here that can tell us OK this model is kind of generalized model.

02:31.130 --> 02:31.660
OK.

02:31.910 --> 02:40.850
However we can as well draw a boundary OK in a way that simply or is specifically designed to fit our

02:40.850 --> 02:41.870
training data set.

02:42.040 --> 02:42.660
OK.

02:42.710 --> 02:48.170
And in this case what we call the model is a overfitting model which means that the model has learned

02:48.230 --> 02:51.070
all the characteristics out of only the training data.

02:51.260 --> 02:55.730
OK so if we expose the model again to some testing data.

02:55.880 --> 02:58.180
Actually this model might be why were better.

02:58.190 --> 03:00.370
The generalized model will be working better.

03:00.410 --> 03:00.890
Okay.

03:01.040 --> 03:06.620
And that's one of the key elements when we choose our model that we want to model to be as general as

03:06.620 --> 03:07.430
possible.

03:07.440 --> 03:07.980
OK.

03:08.180 --> 03:15.880
So it could classify as again future images that hasn't been seen during training moving forward.

03:16.010 --> 03:16.790
OK.

03:17.230 --> 03:17.810
All right.

03:18.110 --> 03:19.630
So how can we evaluate our models.

03:19.670 --> 03:22.270
So we simply use what we call the confusion matrix.

03:22.280 --> 03:26.240
We're going to use to see how can we implemented using Python.

03:26.570 --> 03:32.420
So what we're going to do is we simply wanted to kind of look at one matrix that tell us OK this is

03:32.420 --> 03:35.970
the results of our testing or or training.

03:36.040 --> 03:36.680
OK.

03:36.980 --> 03:43.220
So what we do is that we draw that matrix where here the rows show the predictions and all the columns

03:43.430 --> 03:44.730
all the true classes.

03:44.840 --> 03:45.370
OK.

03:45.590 --> 03:50.720
So this is kind of the output of the model output of the or the guess of the model.

03:50.950 --> 03:57.680
The columns can tell us a true class which is kind of the true prediction or are the true value of our

03:57.860 --> 03:58.670
target class.

03:58.730 --> 03:59.500
Okay.

04:00.370 --> 04:00.760
OK.

04:00.860 --> 04:07.180
So if the prediction for example our model or our model tellt told us okay this cancer is malignant

04:07.190 --> 04:08.260
for example.

04:08.330 --> 04:08.810
Right.

04:09.020 --> 04:12.050
And the two class told us as well it's malignant that we we're good.

04:12.050 --> 04:13.180
That means it's true.

04:13.280 --> 04:18.450
We're going to put kind of a summary here of all the number of samples that the predictions had matched

04:18.570 --> 04:20.000
the toward the top of the class.

04:20.100 --> 04:20.610
OK.

04:20.870 --> 04:21.530
All right.

04:21.680 --> 04:28.570
What if for instance the predictions for example said the the that the patient as is negative for instance

04:29.120 --> 04:32.150
and the true class was negative which means that the patient is good.

04:32.180 --> 04:33.760
You know he doesn't have cancer.

04:33.860 --> 04:39.260
That means if the prediction said that he doesn't have cancer and the true class that he actually the

04:39.260 --> 04:44.030
patient didn't have cancer then what good are they going to put all the total numbers of classes here

04:44.270 --> 04:45.920
that has been classified correctly.

04:45.920 --> 04:46.270
OK.

04:46.520 --> 04:51.740
So simply with some the number of elements in here now with the elements in here that will tell us the

04:51.740 --> 04:56.630
overall number of truly classified samples in a very simple form.

04:57.050 --> 05:05.120
However if for instance our prediction told us that the patient has cancer for example which is kind

05:05.120 --> 05:06.340
of positive.

05:06.560 --> 05:10.100
However our true class was negative.

05:10.250 --> 05:10.890
OK.

05:10.910 --> 05:13.900
That means that this false prediction that mean something went wrong.

05:13.910 --> 05:15.260
However it's not that severe.

05:15.260 --> 05:16.460
I'm going to show you that later.

05:16.570 --> 05:20.720
I can show you after I started covering this this element but again this this kind of misclassified

05:20.720 --> 05:24.920
class which means the prediction was off compared to our class.

05:25.780 --> 05:26.180
All right.

05:26.180 --> 05:32.270
So the next element is OK what if for instance our machine learning model predicts that our predictions

05:32.270 --> 05:34.490
were negative and the class was positive.

05:34.590 --> 05:35.130
OK.

05:35.630 --> 05:38.060
That means again misclassified or false classified.

05:38.150 --> 05:43.130
And that means that the number of elements in here and the number of elements in here that will tell

05:43.130 --> 05:46.000
us the overall number of misclassified samples.

05:46.350 --> 05:47.200
All right.

05:47.710 --> 05:47.990
OK.

05:47.990 --> 05:49.390
Now to a very important point.

05:49.400 --> 05:56.740
So we have what we call a type 1 error and type 2 error type 1 error indicates that the prediction was

05:56.740 --> 05:59.370
tell us that the patient has a disease.

05:59.630 --> 06:06.100
However for the class he actually didn't which is again of course it's it's still an error.

06:06.110 --> 06:10.150
But again we call a type 1 error why because the patient is still fine he's still OK.

06:10.220 --> 06:12.530
We said he might have something but he didn't.

06:12.710 --> 06:18.800
However for type 2 error that's a big problem and we wanted to avoid this at all costs specially if

06:18.800 --> 06:21.430
it's life threatening disease like cancer.

06:21.740 --> 06:27.380
So here if we have our true class said it's positive that means the patient had cancer but we said no

06:27.380 --> 06:27.780
no no.

06:27.800 --> 06:31.410
That the machine learning model said the patient is fine the patient is OK.

06:31.430 --> 06:32.260
This is a big problem.

06:32.260 --> 06:34.250
That's what I call a type 2 error.

06:34.340 --> 06:34.870
OK.

06:34.910 --> 06:38.090
And we want again to avoid this classification at all costs.

06:38.120 --> 06:39.270
Right.

06:39.290 --> 06:41.530
And that's all we have with the modern evaluation.

06:41.540 --> 06:45.070
Let's look at the Jupiter notebook and let's see how can we evaluate our model.

06:45.290 --> 06:47.750
So let's take a look at how can we evaluate our model.

06:47.890 --> 06:48.480
OK.

06:48.740 --> 06:54.620
So let's recap so that we did so far that we just train our models to use the fit method when our object

06:54.650 --> 06:59.410
as we see and there's a good model and we use taining data Exene and watching.

06:59.470 --> 07:02.120
OK so now it's the time to actually evaluate them on.

07:02.240 --> 07:08.740
OK so let's improve let's evaluate the first step is we're going to use our object to as we see the

07:08.740 --> 07:17.270
school model and then getting used again with the Predict method to predict our or to run kind of our

07:17.540 --> 07:19.560
evaluation on our team.

07:19.970 --> 07:23.540
But in this time which is very important that we're not going to pass the training data we actually

07:23.550 --> 07:27.340
Inupiat the testing data which is data that the model has never seen before.

07:27.520 --> 07:33.260
So we got X underscores tests that's simply our what's called this should be uppercase X and this contest

07:33.710 --> 07:34.750
and we're going to do that.

07:34.760 --> 07:37.930
Basically this method should return back our way.

07:37.930 --> 07:39.760
We're going to call it what I predict.

07:40.020 --> 07:40.370
OK.

07:40.370 --> 07:41.780
That would give me our predictions.

07:41.780 --> 07:42.290
OK.

07:42.650 --> 07:43.430
That's the first step.

07:43.430 --> 07:48.130
So let's run this concern successfully and why predict supposed to be our prediction.

07:48.140 --> 07:50.120
Actually let's take a look at it again right.

07:50.310 --> 07:51.550
We're just going to copy it here.

07:51.590 --> 07:52.770
And let's take a look at it.

07:52.880 --> 07:56.630
It's actually going to show it show me actually it's really pretty interesting and showing me all of

07:56.630 --> 08:02.240
them are ones which is you know which kind of makes sense I'm going to show you why later which is simply

08:02.300 --> 08:04.140
telling me that the models you know it's not true.

08:04.160 --> 08:06.940
That means that the model is literally like useless.

08:06.980 --> 08:07.440
OK.

08:07.550 --> 08:09.820
And we'll see how can we actually improve them on them moving forward.

08:10.070 --> 08:11.130
So just keep going.

08:11.180 --> 08:12.070
So I'm going to do.

08:12.130 --> 08:14.110
We're going to plot what we call confusion matrix.

08:14.150 --> 08:14.740
OK.

08:15.290 --> 08:23.030
So if you got the call confusion matrix just give you one one stop shop of all the matrix that shows

08:23.030 --> 08:27.190
all the correctly classified samples and the misclassified samples as well.

08:27.340 --> 08:28.020
All right.

08:28.130 --> 08:33.650
So that's sort of confusion and underscore matrix.

08:33.800 --> 08:41.090
And then what we need to do is that we need to specify or compare our true value versus the predicted

08:41.180 --> 08:41.810
that.

08:42.120 --> 08:48.020
So our true value is simply why underscore test which is simply this is the true target value right.

08:48.140 --> 08:53.790
And we have our values or our predictions we'll call it underscored predict.

08:54.650 --> 08:56.650
And simply this we're going to return.

08:56.690 --> 08:59.320
Let's say we call it S.M. and let's run it.

08:59.390 --> 08:59.620
OK.

08:59.630 --> 09:01.280
So then we have our confusion matrix.

09:01.280 --> 09:07.070
Now one of you the confusion matrix Gerkin used the seaboard again to view our confusion matrix going

09:07.100 --> 09:15.940
to use a heat map in our Seaborn library with S and S the heat map and we're going to do going to write

09:15.950 --> 09:21.050
S.M. which is simply our values supposed to be the matrix.

09:21.050 --> 09:23.780
And actually let's run it first without trying anything.

09:23.780 --> 09:24.870
So we're going to do.

09:24.880 --> 09:30.140
Second to show me this is our matrix which doesn't show simply any values or any counts here which is

09:30.140 --> 09:31.280
basically useless.

09:31.280 --> 09:36.500
So we're going to do with going and we need to show the notation notation equals true which is show

09:36.500 --> 09:37.980
me the actual value.

09:38.000 --> 09:39.500
Now it makes sense.

09:39.500 --> 09:46.370
So what we can see from the confusion matrix that simply we have zero correctly classified samples in

09:46.370 --> 09:47.220
the first class.

09:47.450 --> 09:54.650
So we have four forty eight samples are mis classified which is really terrible evaluation.

09:54.650 --> 09:55.970
All right.

09:56.240 --> 09:58.380
So that's pretty much how can we evaluate our models.

09:58.520 --> 10:02.520
Obviously we have done a little bit a lot of stuff maybe a little bit wrong.

10:02.810 --> 10:03.880
You could improve the model.

10:04.030 --> 10:06.560
I'm going to show you in the next step how can we improve the model.

10:06.600 --> 10:08.610
I hope you guys enjoyed it and see in the next section.
