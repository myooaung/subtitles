WEBVTT

00:01.260 --> 00:07.440
Hello everybody and welcome back in this video we're going to execute the one hurting coding part of

00:07.440 --> 00:10.600
this project as well as a feature Skelly empire.

00:10.980 --> 00:16.920
And this is going to bowl a couple of different steps in the way so let's jump into it right now.

00:16.920 --> 00:22.670
So first let's begin with a one hot and cold night to do so.

00:22.710 --> 00:28.740
We like to leverage our pandas function that get dummys what they're going to do is is going to find

00:28.770 --> 00:33.930
all the categorical variable and variables and then the metric and it's going to encode them into their

00:33.990 --> 00:38.010
own dummy variables and it's going to do it all for us very quickly.

00:38.010 --> 00:40.220
So that's where we get if we use them.

00:40.230 --> 00:49.190
So first we run data set and we selected the result open those that get dummys on the dataset.

00:49.740 --> 00:56.220
So we run these this works and if we look at the columns using data columns we'll see the changes applied

00:56.220 --> 00:58.070
to our Does it.

00:58.110 --> 01:02.790
So here in the console we see that all the columns seem pretty straightforward they're the same that

01:02.790 --> 01:09.510
we had initially said for the pay scale columns and the pre-scheduled columns are the biweekly Lebel

01:09.630 --> 01:12.900
the monthly Lebel semimonthly level and the weekly level.

01:12.960 --> 01:16.400
So it is split that column into four different ones.

01:16.410 --> 01:22.440
Now if you remember from the machine learning course or in your experience we need to remove one of

01:22.440 --> 01:28.620
these columns from the data set to avoid a dummy variable type meaning that if we keep all of these

01:28.620 --> 01:34.650
columns here they're going to be now dependent column not linearly independent columns which is what

01:34.650 --> 01:35.490
we want.

01:35.490 --> 01:39.340
So by removing one of them we make them linearly independent again.

01:39.840 --> 01:40.970
So which one are we going to read.

01:41.010 --> 01:42.580
Well this is a choice that you can do.

01:42.580 --> 01:44.550
There's no right answer for this.

01:44.550 --> 01:50.880
I personally going to remove semimonthly just because semimonthly is the weirdness of the pay schedules

01:51.150 --> 01:54.360
so an odds are going to be the less frequent.

01:54.510 --> 01:56.130
So I'm just going to remove that one.

01:56.520 --> 02:04.150
So that's true that we're going to begin with simply just removing that column with data set icles set

02:04.230 --> 02:05.710
that drop.

02:05.880 --> 02:09.750
You guys are probably familiar with this line of dropping columns already because we've done so many

02:09.750 --> 02:18.810
times and we're going to select this column rather quickly from the console and I mean one second and

02:18.870 --> 02:19.470
that's done.

02:19.590 --> 02:21.510
So we're going to drop this column and that's done.

02:21.660 --> 02:26.050
So now there's a couple things to do once we find like the one card encoding.

02:26.130 --> 02:32.790
This was very quickly as you can see the next theme is to get rid of all the columns that are useful

02:33.030 --> 02:36.000
but are not going to be part of the training set.

02:36.000 --> 02:42.340
So removing some extra columns let's call it as it's called D.S. that.

02:42.390 --> 02:43.360
And what are we removing.

02:43.380 --> 02:49.200
Well first we're removing the response bearable and that's going to be make sense in the next line.

02:49.320 --> 02:55.170
The next section sorry where we do the Trenta split and we're rateable we want the response bearable

02:55.170 --> 02:57.980
separate from the trainings.

02:58.080 --> 03:06.670
So we get rid of esign we save it in it's own response variable and we do the same for the user identifier.

03:06.930 --> 03:11.250
So we're going to save the user identify its own variable.

03:11.250 --> 03:14.530
And this is called entry Id just so that you remember.

03:14.580 --> 03:23.820
And finally the data set is going to be have you guys can guess that DRob columns because the name of

03:23.820 --> 03:28.360
these two variables that were removed.

03:28.830 --> 03:34.850
So let's get rid of that one Furr's let's get rid of and 380 as well.

03:38.070 --> 03:40.580
And that should be it.

03:40.600 --> 03:44.180
So we run this one and we run the last two lines and there you go.

03:44.380 --> 03:51.030
Now we have a data set where the independent variables are localize and then we have the response parable

03:51.070 --> 03:55.020
and the user identifier columns taken out.

03:55.030 --> 03:57.260
Now the next part of this is to do that.

03:57.290 --> 04:03.250
Trento split so is bleeding into train and Teske.

04:04.120 --> 04:05.110
OK.

04:05.110 --> 04:06.410
So let's jump into that.

04:06.430 --> 04:14.710
So if you guys remember from last time the train split function is going to return for items the x axis

04:14.780 --> 04:20.930
are the independent variables for the trend to test as well as the dependent variables for that training

04:20.930 --> 04:22.730
to test them.

04:23.290 --> 04:26.430
So these are going to be four variables here that we're getting.

04:26.740 --> 04:30.900
And that is going to be the result of a 20 split function.

04:31.330 --> 04:36.700
But now one thing before we run this line is that we actually have to import that function and that

04:36.700 --> 04:45.520
is from a scale learn that model selection important trying to split.

04:46.060 --> 04:53.380
And here we're going to play a train to split as well to our dataset which is just the x Balios the

04:53.380 --> 05:03.190
response is that y values at test size which of 0.2 we need at 20 percent is going to be the size of

05:03.200 --> 05:04.230
that response variable.

05:04.280 --> 05:07.750
That's what I'd chosen to add random statical zero.

05:07.750 --> 05:08.460
Why.

05:08.470 --> 05:11.070
Because this is how I like.

05:11.440 --> 05:13.540
So let's run these two lines.

05:14.140 --> 05:15.200
And there you go.

05:15.250 --> 05:17.540
We have a train split immediately done.

05:17.860 --> 05:20.720
OK we're just peering through these steps.

05:20.770 --> 05:23.220
We've done the steps a couple times before already.

05:23.260 --> 05:26.690
So there's no need to go too deep into them so let's just speed through them.

05:26.740 --> 05:34.250
The final step in this video that we're going to do is going to be the feature skeleton feature Schelling.

05:34.330 --> 05:42.970
Now again we're going to import a function from a scalar and that's going to be as we learn pre-processing

05:47.180 --> 05:48.040
and scaler.

05:48.040 --> 05:48.990
There you go.

05:49.210 --> 05:50.390
Pre-processing.

05:51.070 --> 05:55.200
And we're going to import standard skill.

05:55.390 --> 05:56.250
Great.

05:56.830 --> 06:02.560
And what do we do with it and descaler well with first graders and their Scheller by calling the class.

06:02.680 --> 06:04.960
So we have our newest and their skills are created.

06:04.960 --> 06:14.260
We're going to actually set up a plan for X. And they go we run these two lines then we're actually

06:14.260 --> 06:21.130
going to fit descaler to the X train set and then we're going to transform the extreme based on that

06:21.130 --> 06:21.930
scale.

06:22.330 --> 06:32.120
So the way we do it is you use the Scheller we use the com transform and we play on the extremes.

06:33.010 --> 06:39.080
Now these two resulting data set is not going to be a Pandurs for them any more.

06:39.240 --> 06:44.100
I mean that it is going to lose the columns names and it's going to lose the indexes.

06:44.380 --> 06:48.100
So we want to convert this to an actual panacea frame too.

06:48.370 --> 06:55.450
So we're going to apply Pandurs that data frame we're going to apply to the result of this and that's

06:55.450 --> 07:01.490
going to be our actual skill set.

07:01.570 --> 07:02.070
Right.

07:02.380 --> 07:07.530
And we're going to call this the extreme two for now in the game.

07:07.750 --> 07:12.050
This is the result of the X-raying been scale great.

07:12.310 --> 07:17.890
So we run this and we're good then we're going to actually do the same thing almost almost exactly to

07:17.890 --> 07:27.930
the test that we create the X test it goes to we actually only transform because we already have it

07:28.780 --> 07:34.950
the Scheller and we do it on testing Exham.

07:35.110 --> 07:37.300
So we scale the test as well.

07:37.300 --> 07:44.940
Now what is next to do is to copy the columns in the indexes to this extra in an extra two datasets

07:45.130 --> 07:47.580
because those have been lost if you can remember.

07:48.100 --> 07:57.160
So we do extreme to make sure this is an underscore instead and the columns which is actually into that

07:57.160 --> 07:57.810
columns.

07:57.850 --> 07:58.390
Great.

07:58.690 --> 08:06.160
And they're going to be the original extra columns that Ballis.

08:06.800 --> 08:07.330
OK.

08:07.510 --> 08:13.300
And they should be they should work to copy the original columns to the extreme.

08:13.300 --> 08:19.840
Now we do the exact same thing with access by copy and pasting and just plain and on the X test.

08:19.840 --> 08:20.740
Good.

08:20.740 --> 08:22.530
The next step is in Access.

08:22.540 --> 08:28.680
So again in a similar manner we do extra into that index.

08:28.720 --> 08:29.490
Great.

08:29.650 --> 08:38.740
And we're going to set it to extreme and one the original one that indicates that balas great and we're

08:38.740 --> 08:41.650
going to copy this for the next test as well.

08:41.710 --> 08:49.710
So this is going to be the values the index values from Access and this is going to be the next test.

08:50.650 --> 08:51.180
Excellent.

08:51.310 --> 08:58.060
So now are extreme and everything has been scaled properly and the indexes in the columns have been

08:58.060 --> 08:59.080
recover.

08:59.170 --> 09:04.990
So the only thing left to do is to actually set the extreme to these new extreme too.

09:05.170 --> 09:09.740
So this is just making sure that we keep the name of the extreme consistent in the future lines.

09:09.850 --> 09:14.890
So we just set it to this all right here and let's run it one in two.

09:14.890 --> 09:15.620
And there you go.

09:15.760 --> 09:17.810
Feature Skillern is completely done.

09:17.980 --> 09:22.210
And now in the next video we can start doing the model building process.

09:22.210 --> 09:26.860
This is the process that all these guys have been waiting for from the second the first we started but

09:26.860 --> 09:29.280
we've been slowly building our way to it.

09:29.290 --> 09:36.880
We've gone through a Barry in-depth analysis on all the data fields that we have all the columns the

09:36.880 --> 09:39.890
correlations that things that don't make sense or that make sense.

09:40.060 --> 09:44.700
And we have cleaned up the data getting ready for modeling and that's it.

09:44.860 --> 09:47.920
Thank you very much for watching and see you in the next video.
