WEBVTT

00:06.560 --> 00:12.670
The deal with that the output of each person could be a zero or a one.

00:12.720 --> 00:20.010
What we're saying is that we're using a step function that function returns US-EU for any negative.

00:20.020 --> 00:27.650
But you know one for every positive body the function that determines the perceptions output.

00:27.650 --> 00:34.060
Our goal activation functions and they are feeling part on concept in deep learning.

00:34.230 --> 00:39.390
We're going to go over a few of them that we will be using in our problem.

00:41.520 --> 00:48.400
Another really important activation function is that sigmoid activation function.

00:48.430 --> 00:55.450
It is very similar to the step function but with the only difference that is softer on the change from

00:55.450 --> 00:57.170
2 to 1.

00:57.370 --> 01:04.810
That means that we inject a probability instead of always being certain for a given feature is present

01:04.840 --> 01:06.060
or not.

01:06.160 --> 01:12.560
We can now say for example I'm pointing two percent sure of this output.

01:12.790 --> 01:16.190
Don't worry too much about the math behind this function.

01:16.270 --> 01:23.870
We will just be using it as a parameter in our network and other people activation function.

01:23.870 --> 01:28.840
Is there value activation function value stunts for the fight.

01:28.880 --> 01:36.410
The Linnea unit This function returns US-EU for any negative value and returns the same value for any

01:36.410 --> 01:38.070
positive by the.

01:38.210 --> 01:47.040
It is really frequent to use value with images such as you have seen in the fashion business case before.

01:47.470 --> 01:55.330
There are also other activation functions such as the parabolic tangent but we won't be using those.

01:55.460 --> 02:02.030
We just stick to that sigmoid aktif and Velu activation functions.

02:02.160 --> 02:08.570
The process of calculating the output for a given input for our neural network is called feedforward

02:09.290 --> 02:10.740
fit for work.

02:10.800 --> 02:17.120
It's the first step of our training process and it will give us the prediction for our input.

02:17.890 --> 02:21.570
We need to compare that with the expected output.

02:21.670 --> 02:25.560
Remember that we are using supervised learning techniques.

02:25.750 --> 02:27.460
That means that we are very happy.

02:27.490 --> 02:35.510
The expected output for any given input that we can use to train our network once we have the expected

02:35.570 --> 02:41.550
output and the production together we can see for prediction was right.

02:41.600 --> 02:48.590
If we find any difference between them we come back propagate Deever through the entire network.

02:48.650 --> 02:54.680
That means that we're going to see whose fault was it that we didn't get our predictions right.

02:54.680 --> 02:59.960
Once we identified the weights that are causing our mistake we can update them.

02:59.960 --> 03:06.920
So our next fit for what process can get a whether result done this one we will repeat this process

03:06.950 --> 03:11.400
many times until we are satisfied with our results.

03:13.140 --> 03:22.100
But when we leave the we can determine that we will continue training until we get a given accuracy.

03:22.250 --> 03:23.850
But that's hard to do.

03:23.870 --> 03:24.850
We don't know how many.

03:24.950 --> 03:27.430
How much time will it take us to get there.

03:27.600 --> 03:29.570
Or even if it's possible.

03:29.870 --> 03:34.250
So what we usually do is to train for a given amount of a box.

03:34.580 --> 03:38.710
For example we will iterate over 500 times.

03:39.080 --> 03:43.580
This can be this can get to be a really slow process.

03:43.580 --> 03:46.010
Suppose we have 1 million infections.

03:46.250 --> 03:52.180
We've got to go over the 1 million records 500 times and that takes a lot of time.

03:52.990 --> 03:59.560
So what we do is define a batch size that is the amount of records that we will fit for forward at once

03:59.890 --> 04:02.630
before doing the right preparation process.

04:02.690 --> 04:12.040
That's the slowest part in all the training but you will never know with your entire data set you will

04:12.040 --> 04:19.510
split it into a training validation and testing data said that the training that the set will be the

04:19.510 --> 04:23.670
one used to feet forward and but propagate through the network.

04:24.130 --> 04:30.380
The validation data set will be used to measure objectively results while training.

04:30.850 --> 04:37.190
You can use your network's performance on your validation that decide to take decisions about updating

04:37.280 --> 04:44.020
it on not and the testing that the set is used to evaluate your model after you've finished training

04:44.890 --> 04:49.490
your testing and validation data sets should be a percentage of your training.

04:49.480 --> 04:57.010
That said That means that if we only point 1 percent of your transactions are fraudulent you should

04:57.010 --> 05:01.090
cut that same proportion in all three subsets.

05:01.090 --> 05:09.960
In most cases you can just achieve that by using of random sample from your original data set turning.

05:10.050 --> 05:17.320
That said should be the biggest one is the one that will help you improve your model the most publication

05:17.440 --> 05:25.210
and test data sets usually go from 10 percent to 40 percent of the entire data set.

05:25.390 --> 05:31.180
The goal of leading your data set is to know how will your model behave with data.

05:31.180 --> 05:35.040
It was never seen during the training process in Greece.

05:35.050 --> 05:39.350
Your model is too simple for the reality that you're trying to represent.

05:39.460 --> 05:44.090
That's gold under fitting that image on the screen.

05:44.530 --> 05:50.110
On the other hand your mole could be too complex for the reality that you are trying to represent that

05:50.170 --> 05:50.650
image.

05:50.670 --> 05:53.510
See on the right with the.

05:53.590 --> 05:59.440
That was your model will cover better performance in your training data set than in your body Dacian

05:59.470 --> 06:02.320
or testing data sets it yearly.

06:02.350 --> 06:06.480
Your performance should be the same on all the data sets.

06:06.640 --> 06:09.090
If you take that you are under 15.

06:09.190 --> 06:11.070
That's not a big problem.

06:11.170 --> 06:17.860
You can always add more layers to your model or continue trying but if you are Malalas overfitting that

06:17.860 --> 06:25.200
means that you learn too much Holyoake training that upby shapes it and it is difficult to teach to

06:25.210 --> 06:32.510
the computer to forget that one of the most popular techniques to try to solve this is to use chomp

06:32.510 --> 06:42.880
out layers about layers with tone of some of the weights during the process in a minor this with tone

06:42.900 --> 06:49.290
of some predominant features during training and will allow other ways to be able to change themselves.

06:49.760 --> 06:55.370
When you find out about later you will need to indicate the probability of each weight being turned

06:55.370 --> 06:56.250
off.

06:56.390 --> 07:01.930
For example up 50 percent probability it does a number between 0 and 1.

07:02.030 --> 07:09.230
When we use one of these players it is highly likely that we will overfit at that since we have a highly

07:09.230 --> 07:15.950
unbalanced dataset that does it for now but we will continue in the next speediness.

07:15.970 --> 07:16.570
Thank you.
