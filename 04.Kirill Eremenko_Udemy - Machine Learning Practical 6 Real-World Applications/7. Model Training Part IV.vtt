WEBVTT

00:00.090 --> 00:00.710
All right.

00:00.810 --> 00:02.680
So let's train our model.

00:02.780 --> 00:09.420
So the first step is we're going to prepare our X train and X tests and y train and weightiest kind

00:09.420 --> 00:10.310
of data set.

00:10.530 --> 00:11.510
So let's get started.

00:11.670 --> 00:19.680
So first that is another case specify our X train that will be equal to our training but we're not going

00:19.680 --> 00:24.950
to just simply specify all our rows.

00:24.960 --> 00:31.930
However we can specify the columns only accepting except the first column which set the target column.

00:32.100 --> 00:34.460
So we're going to do is going to select one and all.

00:34.560 --> 00:35.400
OK.

00:35.460 --> 00:40.940
And in addition to that we're just going to divide by 255 just to do kind of you know normalization

00:40.960 --> 00:41.780
the beginning.

00:41.910 --> 00:44.650
Second step they're going to specify OK why train.

00:44.850 --> 00:46.380
Will be equal to.

00:46.380 --> 00:51.380
Again my training but I actually needs the column number zero.

00:51.420 --> 00:52.340
That's all I need.

00:52.560 --> 00:53.270
OK great.

00:53.520 --> 00:56.020
Let's do the exact same thing.

00:56.040 --> 00:58.470
All right let's do the same for the testing data sets.

00:58.470 --> 01:00.510
So we're going to do we're just going to copy this.

01:00.540 --> 01:01.640
OK put it here.

01:01.890 --> 01:04.470
And it's of chain which is going to call it tests.

01:04.710 --> 01:10.870
And here we're going to call it test and instead of training we're going to call it testing instead.

01:11.190 --> 01:14.530
And we're just going to sort of test it is going to do this thing as well.

01:14.790 --> 01:16.440
And that's pretty much all what I need.

01:16.440 --> 01:18.120
All right let's run this.

01:18.200 --> 01:18.510
All right.

01:18.540 --> 01:19.820
So now we're going to go.

01:20.130 --> 01:22.610
The second step is we wanted to.

01:22.680 --> 01:24.040
So now we have a training dataset.

01:24.060 --> 01:30.840
We have a testing datasets but what we need to do that we need to develop or use We'll call it the validation

01:30.840 --> 01:31.870
dataset.

01:31.890 --> 01:33.710
All right validation data set.

01:33.780 --> 01:39.420
It's kind of a data set that we used during training but it can be used to help the model generalize

01:39.670 --> 01:40.370
simply.

01:40.410 --> 01:42.720
We want the model to avoid overfitting.

01:43.020 --> 01:49.070
Mainly specifically fitting the data to the training data sets and avoiding generalization.

01:49.140 --> 01:53.880
OK so what if instead of doing this what we need to do that we need to do the validation data set every

01:53.880 --> 02:00.180
once in a while to actually validate that we are heading in and the model is actually generalizing as

02:00.180 --> 02:01.020
we go.

02:01.020 --> 02:01.740
All right.

02:01.860 --> 02:08.220
We're going to do in this we're going to use the frame as killer and we can use a test train to split.

02:08.220 --> 02:09.330
So let's do this.

02:09.330 --> 02:16.440
We're going to do from Skillern that model underscores selection.

02:16.440 --> 02:16.890
All right.

02:16.950 --> 02:22.470
We're going to call imports and train underscored test underscore splits.

02:22.640 --> 02:23.620
All right.

02:23.620 --> 02:29.900
It looks fine and run it looks good next step is we're going to use our trained test pilots.

02:30.090 --> 02:34.430
And you have done that before so just let's simply copy the commands.

02:34.470 --> 02:36.410
It's basically train splits.

02:36.540 --> 02:39.450
They're going to use our extreme or why train.

02:39.510 --> 02:44.890
You can specify the ratio centers specify 20 percent going to be for the testing and then random state

02:44.890 --> 02:49.080
is just going to specify if you guys want to match my results we're just going to put put one two three

02:49.080 --> 02:51.970
four five or whatever five or 1 or whatever.

02:52.200 --> 02:57.330
And here going to specify or exchange X validate why train and why validate.

02:57.330 --> 02:59.680
All right let's run it.

03:00.690 --> 03:01.140
All right.

03:01.170 --> 03:02.240
We're good to go.

03:02.550 --> 03:03.570
Okay perfect.

03:03.570 --> 03:11.640
The next step is we're going to take our training testing and validation data and put them in a form

03:11.730 --> 03:14.520
that we can actually feed to our deep network.

03:14.670 --> 03:18.550
OK so now we have all our data in an array format.

03:18.560 --> 03:18.880
OK.

03:18.880 --> 03:20.570
And a kind of a matrix format.

03:20.580 --> 03:26.190
What we need to do that we need to reshape our data to be in a form of twenty eight by twenty eight

03:26.250 --> 03:26.850
by one.

03:26.970 --> 03:27.480
OK.

03:27.660 --> 03:31.620
This is actually a very critical point because that's the form that our converged convolutional neural

03:31.620 --> 03:33.610
network was going to be accepting the data.

03:33.640 --> 03:34.620
All right.

03:34.620 --> 03:40.940
So in order to do this are just going to simply copy the codes that perform this.

03:40.950 --> 03:44.730
So if you guys can see here we have our take our exchange.

03:45.290 --> 03:53.070
We can do V-shape just to reshape our data and then we're going to simply take our Eckstine cheap zero

03:53.760 --> 03:57.580
and we're going to reshape it to 28 by 28 by 1 or 8.

03:57.990 --> 04:05.010
We simply we're going to go and reshape all out arrays to make it and this kind of an image format that

04:05.010 --> 04:12.540
would help us to simply feed all our data is going to all of feed all our images and the conclusion

04:12.540 --> 04:13.100
of our network.

04:13.190 --> 04:15.090
OK let's run it.

04:15.370 --> 04:15.920
All right.

04:15.990 --> 04:17.000
Looks great.

04:17.280 --> 04:21.960
And what we need to do there are actually let's take a look at extreme and let's take a look at the

04:21.960 --> 04:25.410
shape list of let's see OK or interesting that looks great.

04:25.410 --> 04:32.820
So now we have 48000 samples and each one of them is twenty eight by twenty eight by one or eight which

04:32.820 --> 04:36.510
means that basically it's a greyscale image that's 28 by 28.

04:36.510 --> 04:37.210
All right.

04:37.380 --> 04:45.890
Let's make sure there's whether the X state X tests x that the same X that shape looks good ten thousand

04:45.890 --> 04:51.170
samples and we have X validate as well that shape.

04:51.170 --> 04:52.180
All right perfect.

04:52.190 --> 04:54.070
So now we have our training.

04:54.080 --> 05:01.220
If you guys recall we divided them to 40000 samples of train and 12000 samples to validate and we still

05:01.220 --> 05:05.710
have 10000 samples that we had before which is kind of a separate file that we have for testing.

05:06.110 --> 05:13.150
All right so now we divided all our all our data and I think we are pretty much ready for training.

05:13.320 --> 05:13.910
OK.

05:14.300 --> 05:15.960
So how can we train our model.

05:16.040 --> 05:16.320
Right.

05:16.340 --> 05:18.260
This is pretty interesting.

05:18.470 --> 05:23.450
And let's create a couple of newer yourselves in here.

05:23.450 --> 05:23.860
All right.

05:23.900 --> 05:32.100
So the first step is we're going to import an open source neural network library that we call it.

05:32.090 --> 05:34.320
Chris Chris is really powerful.

05:34.340 --> 05:40.220
We're going to see how can we simply build in like you know like an advanced convolutional neural network

05:40.250 --> 05:46.390
all the stuff that we learned in literally like 10 10 12 lines of code is really simple.

05:46.400 --> 05:47.120
All right.

05:47.120 --> 05:52.240
So first of all let's import tariffs and import Karris first up.

05:53.420 --> 05:57.460
And then here we're just using tensor flowback back end which is great.

05:57.800 --> 06:08.180
And then the next step is that we're going to import simply our sequential convolutional layers Max

06:08.190 --> 06:09.530
spooling layers.

06:09.540 --> 06:15.470
Dancing dancing as well operations flattening drop out along with the optimizers.

06:15.620 --> 06:16.770
Okay okay.

06:16.790 --> 06:22.700
Let's see what are these so let's copy them and let's take a look.

06:22.700 --> 06:23.990
So we imported carrots.

06:24.080 --> 06:25.000
OK that's great.

06:25.010 --> 06:29.770
And then the next step is we're going to import carrots models.

06:29.780 --> 06:31.330
I'm going to use sequential.

06:31.390 --> 06:32.840
We're going to show you how can we use.

06:32.840 --> 06:38.870
Why do we need to use sequential which is in essence we're going to build our our network in a kind

06:38.870 --> 06:40.100
of a sequential form.

06:40.310 --> 06:46.160
Well again I start to build our convolutional lair first followed by the max pooling followed by let's

06:46.160 --> 06:51.880
say drop out followed by flattening and followed by let's say the dense would our complete of fissioning

06:51.920 --> 06:52.880
an hour afterwards.

06:53.020 --> 06:59.210
Well a fully connected network afterwards we're going to say from Carousel players that we'll import

06:59.240 --> 07:00.810
our con to a D.

07:00.830 --> 07:04.060
As you guys guessed it it's simply used for the convolution.

07:04.310 --> 07:05.320
Max pooling.

07:05.520 --> 07:13.610
You get to use the dense to create our fully connected network flatten to flatten our features and the

07:13.610 --> 07:21.070
drop outs actually perform drop out and then we're going to import the optimizers which is Adam optimizer.

07:21.380 --> 07:25.050
And we're going to use simply the tensor board just for callbacks.

07:25.050 --> 07:27.250
OK you guys can ignore ignore this.

07:27.260 --> 07:32.280
I'm not going to go into all the details of of of that.

07:32.540 --> 07:34.870
And let's start by building our model.

07:34.880 --> 07:39.330
So the first step is that we're going to build our model in a kind of a sequential form.

07:39.530 --> 07:45.670
So we're going to do with going around and see and model and we get a call sequential here.

07:46.480 --> 07:47.630
OK.

07:48.770 --> 07:50.160
All right that's a first step.

07:50.320 --> 07:52.740
And then we're going to start building on top of it.

07:52.750 --> 07:53.560
All right.

07:53.560 --> 07:59.910
So what we're going to do is we're going to call OK CNN on the school tomorrow and the first step if

07:59.920 --> 08:03.320
they're going to add our convolutional layer first.

08:03.410 --> 08:03.860
OK.

08:04.090 --> 08:07.790
So simply we're going to use our corn to the that we imported before.

08:07.860 --> 08:15.070
So we're going to use corn to the right and then we're going to specify and that's very actually interesting

08:15.070 --> 08:16.080
and important too.

08:16.360 --> 08:20.800
First we're going to specify how many kernels we're going to be applying how many feature detectors

08:20.800 --> 08:21.690
we may be using.

08:21.910 --> 08:26.250
So 32 each feature detector get the dimensions of three and three.

08:26.260 --> 08:26.610
All right.

08:26.620 --> 08:27.670
Perfect.

08:27.670 --> 08:35.290
And then afterwards we're gonna specify our input shape so that our input shape we need to specify what's

08:35.380 --> 08:36.730
what's our input look like.

08:36.910 --> 08:39.650
So what inputs action image that's 28.

08:39.690 --> 08:41.320
Why 28.

08:41.540 --> 08:42.420
Oh I'm sorry.

08:42.580 --> 08:45.130
So twenty eight by twenty eight by one.

08:45.130 --> 08:46.920
That's our image right.

08:47.260 --> 08:52.780
And next we wanted to specify our activation function or activation function.

08:52.980 --> 08:57.260
We're going to be equal to really two activation function.

08:57.970 --> 08:58.410
All right.

08:58.480 --> 09:02.450
Let's make sure that we have all the rights elements in here.

09:02.500 --> 09:10.270
So he specified our kernels we have 32 kernels size three by three or input shape is 28 28 by 1 which

09:10.270 --> 09:11.690
is the size of our image.

09:11.890 --> 09:14.260
And then we specify the really the activation function.

09:14.260 --> 09:15.820
All right let's run this.

09:16.190 --> 09:19.650
OK it says can to the is not defined my apologies.

09:19.810 --> 09:21.320
So this should be uppercase.

09:21.340 --> 09:22.470
OK now we're good.

09:22.600 --> 09:23.560
All right.

09:23.560 --> 09:27.230
The next step is we're going to add the max pooling layer.

09:27.400 --> 09:29.950
All right let's specify our pooling layer.

09:29.950 --> 09:38.140
So here we're going to call CNM underscore underscore model and we're going to add simply our pooling

09:38.140 --> 09:42.820
layer we can actually imported this by getting Max pool to the game.

09:42.820 --> 09:44.740
So Max pulling to the.

09:44.980 --> 09:47.280
And here we need to specify our pool size.

09:47.290 --> 09:51.460
We're going to call pool size equals to.

09:51.790 --> 09:56.860
And here we're going to call it 2 and 2 which basically the size of our max pulling function.

09:56.860 --> 09:57.230
All right.

09:57.250 --> 10:00.930
Let's make sure that everything is good CNN models and Max pooling.

10:00.940 --> 10:01.440
Looks great.

10:01.440 --> 10:02.170
Let's run it.

10:02.250 --> 10:03.970
OK Lance perfect.

10:03.970 --> 10:10.780
The next step is we want to flatten our our model actually flatten is really simple.

10:10.900 --> 10:19.800
So CNN underscored the model and what we're going to do with him in ads is flattening layers of Flaten

10:21.040 --> 10:28.170
flattened as you can see here that's just too flat out features into one single array so it can actually

10:28.170 --> 10:33.820
feel it or fully dense fully connected neural network in neural network.

10:34.260 --> 10:36.070
All right we're good.

10:36.080 --> 10:38.980
The next step is we're going to add our function.

10:39.090 --> 10:42.240
So see an underscore tomorrow.

10:42.660 --> 10:46.250
We're going to and again let's add our dance.

10:46.270 --> 10:47.250
OK.

10:47.760 --> 10:51.210
And we need to specify our dimensions simply.

10:51.240 --> 10:51.720
OK.

10:51.900 --> 11:04.920
So here we're going to call our output dimension equal to 32 and we are going to specify our activation

11:06.870 --> 11:10.440
equals to really activation.

11:10.440 --> 11:15.330
This is simply the layer it's kind of the hidden layer that we're going to include in between between

11:15.330 --> 11:20.620
the outputs because the final output has to be obviously our 10 classes that we took to specify.

11:20.780 --> 11:21.310
OK.

11:21.570 --> 11:24.210
So we're just going to copy this.

11:24.210 --> 11:27.070
We're going to add another layer that will be the output layer.

11:27.240 --> 11:29.580
So the open dimension here has to be 10.

11:29.980 --> 11:35.560
OK and then when we activation because we need to specify kind of a 1 out of 10 classes.

11:35.620 --> 11:40.980
So instead of using the elu would guess you can use the sigmoid function sigmoid function instead.

11:41.490 --> 11:43.700
All right let's run this.

11:43.880 --> 11:45.740
OK it looks good looks good.

11:45.900 --> 11:46.700
All right.

11:46.860 --> 11:52.530
That's pretty much all what we need to develop or build our layers one by one.

11:52.530 --> 11:59.910
The next step is that we want to simply specify our our training how can we how we're going to be turning

12:00.210 --> 12:01.660
this basically network.

12:01.800 --> 12:03.460
OK let's create some cells in here.

12:03.630 --> 12:07.710
The next step is we're going to use simply our Adam optimizer.

12:07.910 --> 12:13.320
OK for emetics who are going to be our accuracy and we're going to use what we call it the last function

12:13.320 --> 12:19.200
going to be loss categorial cross entropy we use cross categorial because we are simply categorizing

12:19.230 --> 12:23.610
our data dataset out of out of out of 10 samples simply.

12:23.760 --> 12:26.510
It's not like a binary binary across entropy.

12:26.580 --> 12:31.950
You can use binary if you only have 0 or 1 but he would have to get the girl across entropy and again

12:32.130 --> 12:38.970
now just just going to compile simply our model and the last step that we wanted to fit our model.

12:39.270 --> 12:45.120
So in order to do this we're going to do specify first how many epochs we want to run our model.

12:45.120 --> 12:48.150
So epochs here let's say we specify 50.

12:48.150 --> 12:52.950
For example when you say epix that means how many times are going to be the presenting or presenting

12:53.250 --> 12:56.510
our data set and updating the weights as we go.

12:56.630 --> 12:57.320
OK.

12:57.600 --> 12:58.120
All right.

12:58.200 --> 13:05.750
The next step is we're going to create or fits basically apply the fit method to our CNN models are

13:05.760 --> 13:08.990
going to call CNN model that fits.

13:09.360 --> 13:13.880
And then we're going to use our training data exchange.

13:13.950 --> 13:15.000
Why train.

13:15.000 --> 13:20.070
And then we're going to specify all the number of epochs and the validation data as well.

13:20.070 --> 13:28.850
All right let's take a look at so here if you guys go into the documentation you can simply copy the

13:28.900 --> 13:30.280
information here

13:33.700 --> 13:34.540
get.

13:34.960 --> 13:35.300
All right.

13:35.300 --> 13:40.820
So here we have our exchange we have white train her batch size specified this from 112 can specify

13:40.820 --> 13:42.290
whatever number you want.

13:42.320 --> 13:44.490
Number of epoxied was post-fight a 50.

13:44.660 --> 13:49.220
We can't boast how many like you know like our kind of information you need while you're training we're

13:49.220 --> 13:51.880
running through the training data and validation data.

13:51.890 --> 13:56.470
We use x and y validate just to do a kind of a cross-validation as we will.

13:56.550 --> 13:57.190
Right.

13:57.500 --> 13:59.040
OK let's run this model.

13:59.090 --> 13:59.630
Looks great.

13:59.630 --> 14:01.350
Let's run this a looks fine.

14:01.610 --> 14:03.470
Here we fit the model.

14:03.650 --> 14:06.670
And actually that's how we are performing the training.

14:06.670 --> 14:07.170
All right.

14:07.430 --> 14:13.460
As you can see here we're going through with one out of 50 and we that's how we pretty much treat our

14:13.460 --> 14:13.910
model.

14:13.940 --> 14:14.570
OK.

14:14.780 --> 14:20.180
So every time we have to look and check the accuracy how the accuracy is going it is going around 70

14:20.180 --> 14:25.050
percent or now we move forward it's actually going to point to and we'll see.

14:25.070 --> 14:29.390
No it keeps getting better and better and we actually see how can we even evaluate our model moving

14:29.390 --> 14:30.040
forward.

14:30.110 --> 14:31.940
And how do we test our model.

14:32.060 --> 14:32.830
Right.

14:32.900 --> 14:35.560
Or just it will take a while to train you 350 epochs.

14:35.600 --> 14:40.730
So that's going to skip through it and and see you in a bit.

14:40.730 --> 14:41.440
All right.

14:41.480 --> 14:42.950
So we're back.

14:42.950 --> 14:50.840
So after 50 epochs you can see that the accuracy at each point 9 5 percent which is pretty good assuming

14:50.840 --> 14:59.150
that we only use 3 to 32 kernels and we use again Max putting two by two and we didn't even use any

14:59.160 --> 15:03.730
dropouts or any additional feature enhancements.

15:03.920 --> 15:04.550
Right.

15:04.670 --> 15:07.450
That's pretty much all what we have for the section.

15:07.460 --> 15:11.840
I hope you guys enjoyed it and see you in the evaluation section.
