WEBVTT
1
00:00:00.990 --> 00:00:03.590
Hello and welcome to this new tutorial.

2
00:00:03.580 --> 00:00:09.180
All right so to start our daily pre-processing journey we have to start by importing the data set of

3
00:00:09.180 --> 00:00:14.250
course because we got the data set from the website but it is still not in Python right now.

4
00:00:14.250 --> 00:00:15.090
We need to load it.

5
00:00:15.090 --> 00:00:20.910
We need to import it and to do this we're going to use the open function we could use pen.

6
00:00:20.960 --> 00:00:27.140
But the open function will be OK and we will load both the conversations and the lines.

7
00:00:27.180 --> 00:00:31.600
So we're going to give two variable names for two data sets.

8
00:00:31.650 --> 00:00:36.900
We're going to curl the data set of the lines lines and the data set of the conversations conversations

9
00:00:37.020 --> 00:00:39.430
so that everything is crystal clear.

10
00:00:39.450 --> 00:00:39.690
All right.

11
00:00:39.690 --> 00:00:47.150
So let's start with the lines as we said we give a name to this dataset which is lines here lines equal.

12
00:00:47.190 --> 00:00:52.970
And as we said we use the open function to open this dataset to load these days.

13
00:00:53.340 --> 00:00:54.710
And so to load it.

14
00:00:54.720 --> 00:00:55.860
Well that's very simple.

15
00:00:55.860 --> 00:01:05.940
We start by entering the name in quotes of the dataset which is movie underscore lines that takes t.

16
00:01:06.290 --> 00:01:09.180
And then we need to add several other arguments.

17
00:01:09.230 --> 00:01:14.490
I started to import this way directly but it didn't work due to an encoding issue.

18
00:01:14.510 --> 00:01:25.270
Therefore to avoid this encoding issue you just need to improve your encoding equals in quote UTF it.

19
00:01:25.670 --> 00:01:28.250
Then we need to import another argument.

20
00:01:28.340 --> 00:01:34.040
We can ignore the little errors and to do this we can input here errors Igor's.

21
00:01:34.100 --> 00:01:35.740
Quote ignore.

22
00:01:35.930 --> 00:01:38.080
And that's it for the arguments then.

23
00:01:38.270 --> 00:01:43.820
We simply need to add here that read that actually to read the dataset when we loaded.

24
00:01:44.180 --> 00:01:54.110
And eventually we are going to add that split parenthesis backslash and to split the observations by

25
00:01:54.110 --> 00:01:58.660
the lines by the lines of the dataset and the solution has to be in quotes.

26
00:01:58.760 --> 00:02:02.150
All right and this line imports the data set correctly.

27
00:02:02.150 --> 00:02:03.880
We will check that right afterwards.

28
00:02:03.890 --> 00:02:08.340
But I'm just going to do the same for our conversations.

29
00:02:08.550 --> 00:02:15.640
So I'm copying the first line pasting in here then replacing the lines here like conversations.

30
00:02:16.010 --> 00:02:20.020
And here we need to replace movie lines by of course.

31
00:02:20.150 --> 00:02:27.580
Movie conversations and there we go we are ready also to import the conversations.

32
00:02:27.590 --> 00:02:28.790
All right let's give it a go.

33
00:02:28.910 --> 00:02:36.650
So I'm going to select these two lines here and press command or control plus enter to execute.

34
00:02:36.830 --> 00:02:37.620
And here we go.

35
00:02:37.820 --> 00:02:42.850
Well executed as you can see our lines and conversations were well import.

36
00:02:42.950 --> 00:02:45.940
Let's have a look in will explore to see what they look like.

37
00:02:46.010 --> 00:02:48.240
So let's open lines first.

38
00:02:48.330 --> 00:02:51.780
So we've got a warning here because it's a huge data set.

39
00:02:51.920 --> 00:02:54.530
So you can take the risk to open it.

40
00:02:54.650 --> 00:02:58.130
I'm going to take the risk click like yes let's see how it goes.

41
00:02:58.130 --> 00:02:58.940
Perfect.

42
00:02:58.940 --> 00:03:00.130
Nothing.

43
00:03:00.140 --> 00:03:01.690
Not too much trouble.

44
00:03:01.710 --> 00:03:03.290
And so here are the lines.

45
00:03:03.290 --> 00:03:03.650
All right.

46
00:03:03.650 --> 00:03:07.790
So they are separated by the rows that things split.

47
00:03:07.970 --> 00:03:08.920
Slash and.

48
00:03:09.170 --> 00:03:09.680
And.

49
00:03:09.800 --> 00:03:15.080
Well it's like what we saw in the original data set before it was imported here.

50
00:03:15.140 --> 00:03:18.970
You know we have each row corresponding to one line.

51
00:03:19.040 --> 00:03:25.940
Now a conversation with one line said by a character for example here Cameron which is user number two

52
00:03:26.390 --> 00:03:27.930
in a specific movie.

53
00:03:28.010 --> 00:03:31.230
Movie zero and the line is she OK.

54
00:03:31.640 --> 00:03:37.720
An important thing to understand this line has a key identifier which is El 984.

55
00:03:37.760 --> 00:03:39.960
That's a unique key identifier.

56
00:03:39.980 --> 00:03:43.250
We even got the time here which is a string of course.

57
00:03:43.250 --> 00:03:44.150
All right.

58
00:03:44.150 --> 00:03:47.500
Perfect so I'm going to close this by just pressing OK.

59
00:03:47.720 --> 00:03:54.500
And now we're going to open the conversations we might get another warning or just some time to wait.

60
00:03:55.870 --> 00:03:57.130
All right conversations.

61
00:03:57.170 --> 00:03:58.200
Perfect.

62
00:03:58.270 --> 00:04:00.100
So let's have a look.

63
00:04:00.100 --> 00:04:02.020
Just to make sure you got this right.

64
00:04:02.020 --> 00:04:09.430
So each row corresponds to one conversation and a specific movie movie Zero here between two characters

65
00:04:09.540 --> 00:04:11.640
use a zero and use or two here.

66
00:04:11.650 --> 00:04:18.100
And as you can see it doesn't display the conversation in text but it gives the composition by just

67
00:04:18.100 --> 00:04:20.080
giving the key identifiers of the line.

68
00:04:20.170 --> 00:04:28.030
So for example this conversation here is composed of four lines of conversation of four sentences exchanged

69
00:04:28.030 --> 00:04:29.830
between two characters.

70
00:04:29.920 --> 00:04:32.280
The first one is El 194.

71
00:04:32.560 --> 00:04:40.390
Then the second one which is the reply by user to to user 0 I suppose is El 195 then uses 0 replies

72
00:04:40.530 --> 00:04:41.720
El 186.

73
00:04:41.980 --> 00:04:46.730
And eventually user 2 replies l 1 One is that right.

74
00:04:46.760 --> 00:04:53.840
And these are all the conversations we have in the whole corpus Cornell movie corporate dialogue days.

75
00:04:54.440 --> 00:04:54.880
Perfect.

76
00:04:54.880 --> 00:05:02.170
So we got our data set and now we are ready to move on to the next step which is to make a dictionary

77
00:05:02.650 --> 00:05:06.860
that will map each lines ID with its text.

78
00:05:06.880 --> 00:05:10.350
We already have a mapping between the key and interfaces and the lines.

79
00:05:10.360 --> 00:05:16.600
But we want to make a python dictionary because we will use it afterward to get our data ready for the

80
00:05:16.600 --> 00:05:18.980
future neural network that we're going to build.

81
00:05:19.090 --> 00:05:22.690
So let's do this in the next tutorial and until then enjoy has been LP.
