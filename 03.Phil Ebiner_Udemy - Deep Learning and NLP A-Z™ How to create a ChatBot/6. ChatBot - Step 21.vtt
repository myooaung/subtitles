WEBVTT
1
00:00:00.630 --> 00:00:05.880
Hello and welcome to this new tutorial and the previous to toile we took care of the encoder and layer

2
00:00:06.180 --> 00:00:10.950
and then this one we're going to take care of the decoder arland layer and we're going to have to do

3
00:00:10.950 --> 00:00:12.230
it in three steps.

4
00:00:12.240 --> 00:00:17.330
First step will be to decode the training set and that's what we'll do in the Statoil.

5
00:00:17.340 --> 00:00:20.370
Second step will be to decode the validation set.

6
00:00:20.460 --> 00:00:22.120
And that's what we'll do in the next tutorial.

7
00:00:22.320 --> 00:00:27.230
And then eventually we'll be ready to create the decoder Arnon there.

8
00:00:27.230 --> 00:00:32.040
So let's take the first step in today's tutorial decoding the training set.

9
00:00:32.040 --> 00:00:34.680
So again we're going to make another function.

10
00:00:34.770 --> 00:00:39.960
This architecture is all about making functions and this function we're going to make is going to be

11
00:00:39.960 --> 00:00:42.490
called the decode.

12
00:00:42.630 --> 00:00:48.440
And those core training on this core set that way we clearly understand what's going on.

13
00:00:48.500 --> 00:00:53.180
The coach training set and it is going to take a lot of arguments.

14
00:00:53.250 --> 00:00:54.810
So let's add them one by one.

15
00:00:54.900 --> 00:01:03.900
The first argument is going to be the encoder underscore state because our decoder is getting the encoded

16
00:01:03.900 --> 00:01:08.400
state as input as part of the inputs to proceed to the decoding.

17
00:01:08.400 --> 00:01:13.500
And therefore we will need the anchor to state that was returned by the previous function and current

18
00:01:13.500 --> 00:01:16.540
RNA layer to proceed to the decoding.

19
00:01:16.540 --> 00:01:23.440
All right so go to state then the next argument is going to be the decoder cell.

20
00:01:23.490 --> 00:01:27.760
And that's basically the cell in the record neural network of the decoder.

21
00:01:27.870 --> 00:01:36.210
Then the next argument is going to be the decoder embedded on this core input and that's basically the

22
00:01:36.390 --> 00:01:41.980
inputs on which were applied embedding and I'm going to explain to you right now what this means.

23
00:01:42.120 --> 00:01:48.720
But to do this the best way is that I go to the tenths of a Web site in the embedding section in the

24
00:01:48.720 --> 00:01:49.820
programmer's guide.

25
00:01:50.010 --> 00:01:55.440
So basically we have a clear explanation here and embedding is a mapping from discrete objects such

26
00:01:55.440 --> 00:02:01.360
as words and especially words here we're dealing with words two vectors of real numbers.

27
00:02:01.560 --> 00:02:08.550
So for example a 300 dimensional embedding for English words like we're dealing with could include blue

28
00:02:08.670 --> 00:02:09.910
with the following code.

29
00:02:10.020 --> 00:02:15.450
The following embedding code than Bleus the following embedding code basically and embedding is just

30
00:02:15.450 --> 00:02:19.780
a mapping from our words to vectors of real numbers.

31
00:02:19.800 --> 00:02:23.840
Each one encoding uniquely the word associated to it.

32
00:02:23.910 --> 00:02:27.710
And so that's very important to work with embeddings.

33
00:02:27.810 --> 00:02:33.360
When building a set like model because that's basically the kind of format that the decoder is accepting

34
00:02:33.510 --> 00:02:37.430
for its inputs and therefore I'm going to go back to Python.

35
00:02:37.590 --> 00:02:44.360
Well we need to have our embedded input and I'm precise saying that these are the inputs of the decoder.

36
00:02:44.570 --> 00:02:46.310
So decoder embedded inputs.

37
00:02:46.530 --> 00:02:51.790
Then we're going to take again our sequence length.

38
00:02:52.080 --> 00:02:55.940
Then the next one is going to be the decoding.

39
00:02:56.120 --> 00:02:57.830
And this core scope.

40
00:02:57.870 --> 00:02:59.390
So this is pretty technical.

41
00:02:59.390 --> 00:03:06.470
And again to explain that to you I'm going to go back to tenso flow on this time the T.F. that variable

42
00:03:06.470 --> 00:03:14.610
scope class by tensor flow which I get not in the programmer's guide but in the API are 1.4 at the time

43
00:03:14.610 --> 00:03:15.690
Im speaking.

44
00:03:15.690 --> 00:03:17.090
So variable scope.

45
00:03:17.100 --> 00:03:17.810
What is it.

46
00:03:18.010 --> 00:03:24.600
That's basically an advanced data structure that will wrap your tenths of a variables.

47
00:03:24.600 --> 00:03:30.390
And so the next argument here that we're inputting is the decoding scope but that will be like an object

48
00:03:30.390 --> 00:03:32.280
of this variable scope.

49
00:03:32.280 --> 00:03:34.070
All right so let's go back by then.

50
00:03:34.200 --> 00:03:40.650
And here we are back in by Indigo decoding scope then we have three more arguments to go the next one

51
00:03:40.650 --> 00:03:44.370
is output and this core function.

52
00:03:44.370 --> 00:03:50.820
So basically that's a function we will use to return the decoder outputs and then we're going to have

53
00:03:50.910 --> 00:03:58.470
our keep prob argument because again we're going to apply some dropout regularization and eventually

54
00:03:58.890 --> 00:04:03.640
the batch size because we'll work again with batches.

55
00:04:03.790 --> 00:04:04.680
Right and that's it.

56
00:04:04.680 --> 00:04:08.160
So that's a lot of arguments but no worries.

57
00:04:08.160 --> 00:04:10.040
We will try not to get lost.

58
00:04:10.050 --> 00:04:10.380
All right.

59
00:04:10.380 --> 00:04:15.960
So the first thing we're going to have to do is get the attention States so I'm introducing a new variable

60
00:04:15.960 --> 00:04:22.990
here a tension underscore States and how are we going to get them.

61
00:04:23.070 --> 00:04:28.920
Well before getting them we're going to have to initialize them and we're going to initialize them as

62
00:04:29.100 --> 00:04:32.800
three dimensional matrices containing only zeroes.

63
00:04:33.000 --> 00:04:40.360
And to do that we have to use the T.F. that zeroes function like the non-pilot zero's function.

64
00:04:40.530 --> 00:04:46.770
And inside this function will like the number of zeros function we have to specify in square brackets

65
00:04:47.190 --> 00:04:52.710
the number of lines of this three dimensional matrix you want to create the number of columns and the

66
00:04:52.710 --> 00:04:55.170
number of elements in the third dimension.

67
00:04:55.170 --> 00:04:57.340
That is the third axis.

68
00:04:57.360 --> 00:05:03.850
So let's take care of the lines first the lines corresponds to the observations and therefore since

69
00:05:03.850 --> 00:05:05.520
we're working with batches here.

70
00:05:05.620 --> 00:05:10.960
Well we have to create a batch and therefore the number of lines is going to be the batch size.

71
00:05:11.020 --> 00:05:17.850
And so since batch size is one of our arguments Well the number of lines here is going to be Bache underscore

72
00:05:17.970 --> 00:05:19.220
size.

73
00:05:19.240 --> 00:05:22.380
All right then the number of columns is going to be 1.

74
00:05:22.390 --> 00:05:27.240
We only need one column and for the third dimension the third axis.

75
00:05:27.250 --> 00:05:32.010
Well the number of elements in the third axis is going to be the decoder cell.

76
00:05:32.850 --> 00:05:38.560
That output size the decoder cell output size.

77
00:05:38.590 --> 00:05:39.260
All right.

78
00:05:39.330 --> 00:05:42.670
So we just initialized our attention States.

79
00:05:42.810 --> 00:05:49.080
And now what we have to do is prepare the keys the values the functions for the attention.

80
00:05:49.230 --> 00:05:55.770
So we're going to get first the attention keys then the attention values then the attention score function

81
00:05:55.980 --> 00:06:01.780
then the attention construct function and all these we will get it in one shot.

82
00:06:01.830 --> 00:06:07.680
Thanks to a very useful Tenterfield function that belongs to the SEC to sec submodule from the contrib

83
00:06:07.680 --> 00:06:10.820
module which is prepare attention.

84
00:06:10.930 --> 00:06:12.360
That's exactly what you want right now.

85
00:06:12.360 --> 00:06:18.020
We want to preprocess the training data in order to prepare it to the attention process.

86
00:06:18.150 --> 00:06:19.870
So let's get them first.

87
00:06:19.890 --> 00:06:36.190
We said the attention kids the attention values the attention score function the attention construct

88
00:06:36.610 --> 00:06:44.790
function and all these We're going to get them from sense of the sense of the library then from the

89
00:06:44.790 --> 00:06:54.350
contrib module and then from this control module the SEC to sec submodule I'm really glad that of flow

90
00:06:54.450 --> 00:06:55.440
offers this.

91
00:06:55.530 --> 00:07:01.400
This is very practical for all the pre-processing of attention so to speak.

92
00:07:01.530 --> 00:07:08.730
And so from the SEC to sex submodule we're going to have to get the prepare underscore a tension function

93
00:07:08.730 --> 00:07:12.470
because right now we want to do some pre-processing for the attention process.

94
00:07:12.660 --> 00:07:14.310
So the preparedness and function.

95
00:07:14.340 --> 00:07:17.790
And here we go for another round of many arguments.

96
00:07:17.790 --> 00:07:25.260
The first one is going to be the attention states that we just initialized in the previous line right

97
00:07:25.260 --> 00:07:25.880
here.

98
00:07:26.040 --> 00:07:29.830
So attention States right now they're just initialized as zeros.

99
00:07:30.060 --> 00:07:36.210
Attention States then we're going to have to choose an attention option.

100
00:07:36.540 --> 00:07:40.790
So basically you have several options to proceed to the attention process.

101
00:07:40.920 --> 00:07:47.230
And one of them which is the one we're going to choose is be a H D A and A you.

102
00:07:47.490 --> 00:07:49.200
So attention option equals better.

103
00:07:49.200 --> 00:07:49.760
No.

104
00:07:49.920 --> 00:07:58.780
And the next argument is going to be the number of units which we get thanks to the decoder cell.

105
00:07:58.800 --> 00:08:05.280
So we'll take the stickers cell argument that is the cell of the decoder and we'll get its output size

106
00:08:05.580 --> 00:08:10.630
which corresponds to the number of units in this prepare attention function.

107
00:08:10.650 --> 00:08:19.580
So we take our decoder cell and we add the output on those core size.

108
00:08:19.580 --> 00:08:25.400
All right so now that we added everything I'm just going to say a few words on what exactly these guys

109
00:08:25.490 --> 00:08:26.040
are.

110
00:08:26.180 --> 00:08:33.140
So the attention keys that's the key is to be compared with the target states then the attention values

111
00:08:33.320 --> 00:08:37.370
that's the values that we'll use to construct the context vectors.

112
00:08:37.390 --> 00:08:44.840
I remind them the context is returned by the encoder and that should be used by the decoder as the first

113
00:08:44.900 --> 00:08:46.580
element of the decoding.

114
00:08:46.580 --> 00:08:52.340
Then the attention's Gore function is used to compute the similarity between the keys and the target

115
00:08:52.340 --> 00:09:00.560
states and eventually the attention constructs function is a function used to build the tension state.

116
00:09:00.560 --> 00:09:04.000
So we have all these attention features.

117
00:09:04.250 --> 00:09:11.990
And now the next step is to get the training to code a function that will do the decoding of the training

118
00:09:11.990 --> 00:09:12.700
set.

119
00:09:12.740 --> 00:09:15.480
And so this is a function that we're going to get right now.

120
00:09:15.520 --> 00:09:22.520
I'm going to introduce a new variable that will be this function that I'm going to call training underscore

121
00:09:22.910 --> 00:09:26.240
decoder underscore function.

122
00:09:26.270 --> 00:09:31.760
Sorry again for the long variable names but that's just to clearly show you what's going on here that

123
00:09:31.760 --> 00:09:34.630
you have a clear visual at what's going on.

124
00:09:34.640 --> 00:09:41.410
So training the caller function and thank goodness we're going to get it from another function I just

125
00:09:41.410 --> 00:09:49.520
sick to sick module of the tens of library which is the attention the coater and train function.

126
00:09:49.730 --> 00:09:57.510
So you see I'm not the only one to choose big variable names for all these tens of law does that too.

127
00:09:57.560 --> 00:10:06.320
So let's get this function from first the center of library then from the contrib module then from the

128
00:10:06.320 --> 00:10:14.180
control module we get the SEC to sec submodule and from the SEC to submodule we get the function we

129
00:10:14.180 --> 00:10:23.700
want which is the tension underscore decoder underscore an underscore train.

130
00:10:23.840 --> 00:10:31.580
And that's the function that will decode the training set but it can only do it once we prepare correctly

131
00:10:31.820 --> 00:10:36.590
the attention which is what we've just done here in the two previous lines.

132
00:10:36.590 --> 00:10:38.280
All right but now we can get it.

133
00:10:38.300 --> 00:10:44.360
And as you can imagine the attention features that we've prepared just before thanks to the prepared

134
00:10:44.370 --> 00:10:50.840
attention function is going to be the arguments of this attention decoder and train function.

135
00:10:51.000 --> 00:10:54.310
So let's do this let's put them one by one.

136
00:10:54.330 --> 00:11:00.170
However we're going to input all of them but we will put one more in that will be our first argument

137
00:11:00.170 --> 00:11:08.810
here and that's the encoder state and code underscore state which I remind we got from the previous

138
00:11:08.810 --> 00:11:11.070
function and could a Arnon layer.

139
00:11:11.150 --> 00:11:17.060
We got it here thanks to the bidirectional dynamic Arnon function again from the tensor for library.

140
00:11:17.300 --> 00:11:21.130
So that does encourage state here but to get exactly what we want.

141
00:11:21.170 --> 00:11:22.560
That is the value of the state.

142
00:11:22.730 --> 00:11:28.000
Well we need to get the index zero of this and go to state variable.

143
00:11:28.010 --> 00:11:30.540
All right so and code of state zero.

144
00:11:30.650 --> 00:11:37.370
That's the first argument and then we're going to have to input all the attention features as the other

145
00:11:37.370 --> 00:11:38.140
argument.

146
00:11:38.360 --> 00:11:40.220
So let's take care of them one by one.

147
00:11:40.280 --> 00:11:47.900
The first one is are the tension keys that I remind will be compared to the target states.

148
00:11:47.900 --> 00:11:55.970
Then the next argument is our attention and those core values that I remind again are used to construct

149
00:11:56.180 --> 00:11:57.730
the context vectors.

150
00:11:57.850 --> 00:11:58.340
Right.

151
00:11:58.370 --> 00:12:06.590
The next the next argument is are a tension score a function that compute the similarity between the

152
00:12:06.590 --> 00:12:08.550
keys and the target state.

153
00:12:08.690 --> 00:12:14.310
So tension score function.

154
00:12:14.440 --> 00:12:14.950
All right.

155
00:12:14.980 --> 00:12:15.900
Perfect.

156
00:12:15.970 --> 00:12:24.620
The next argument is our attention construct function to build the attention state so that we go attention

157
00:12:25.190 --> 00:12:34.130
on this core construct function and eventually we have this last argument here which is the name argument

158
00:12:34.540 --> 00:12:41.150
and this corresponds to a name scope for the decoding function and the name scope we want here is a

159
00:12:41.150 --> 00:12:47.940
sense of flow name scope attention but spelled this way 80 n underscore deck for the coder.

160
00:12:48.100 --> 00:12:52.930
And this core train that you have to put in quotes.

161
00:12:52.930 --> 00:12:56.360
All right so there we go we have all our arguments here.

162
00:12:56.560 --> 00:13:01.600
And now I think it's really important that we take a step back and explain what's going on because this

163
00:13:01.600 --> 00:13:02.900
is pretty overwhelming.

164
00:13:03.040 --> 00:13:07.900
But in order to survive overwhelming as well we need to take a step back.

165
00:13:08.020 --> 00:13:09.190
That's very important.

166
00:13:09.430 --> 00:13:16.990
So basically what we just did here is that we got an attentional Dakota function for the training of

167
00:13:17.050 --> 00:13:22.660
our dynamic Arnon an indicator that we're going to make in two tutorials because in the next one will

168
00:13:22.660 --> 00:13:24.100
take care of the validation set.

169
00:13:24.340 --> 00:13:27.290
But we're doing this for this future dynamic.

170
00:13:27.320 --> 00:13:28.310
Arnon Dakota.

171
00:13:28.630 --> 00:13:36.810
So now that we did that we're going to move on to the next step which is to get the decoder output so

172
00:13:36.820 --> 00:13:46.650
I'm introducing a new variable which is decoder output and this I'm going to get it from Again another

173
00:13:46.650 --> 00:13:54.000
function by the SEC the SEC module of tenso flow which is a dynamic Arnon decoder so dynamic Island

174
00:13:54.000 --> 00:14:02.130
decoder is a function that is used to get the output to final state and the final context state of the

175
00:14:02.130 --> 00:14:02.870
decoder.

176
00:14:03.090 --> 00:14:09.110
And therefore since it returns first the decoder output then the Daker final state then the decoded

177
00:14:09.110 --> 00:14:10.210
final text.

178
00:14:10.220 --> 00:14:13.290
But we are only interested in the decoder output.

179
00:14:13.290 --> 00:14:20.190
Well what we have to do before applying this function is as you know these underscore an underscore

180
00:14:20.190 --> 00:14:26.220
here because this function that we're going to use returns the three elements but we only need the first

181
00:14:26.220 --> 00:14:26.880
one.

182
00:14:26.880 --> 00:14:32.520
And if you want I can even write to you what this returns if you want to have a more clear understanding

183
00:14:32.640 --> 00:14:33.950
of what is going on.

184
00:14:34.140 --> 00:14:47.160
So the second argument is the decoder final state and the third argument is that decoder underscore

185
00:14:47.310 --> 00:14:55.040
final underscore context underscore state that words will be returned by that function.

186
00:14:55.050 --> 00:15:01.480
But we only need the decoder output therefore feel free to replace all these by underscores.

187
00:15:01.680 --> 00:15:08.520
But for those of you who liked the idea of clearly showing what's going on here I can even replace this

188
00:15:08.760 --> 00:15:15.810
underscore here by the first elements returned by this bidirectional dynamic Arnon function which is

189
00:15:15.930 --> 00:15:20.140
again the encoder output which we don't need for encoding.

190
00:15:20.340 --> 00:15:25.350
But for those of you who are interested to see what's going on with the functions we're using I can

191
00:15:25.350 --> 00:15:32.310
still add it here encoder and the score outputs and Kuru output and for the other group of people who

192
00:15:32.310 --> 00:15:35.140
doesn't care of seeing everything.

193
00:15:35.160 --> 00:15:40.200
If I may say Well feel free to replace that by an underscore right.

194
00:15:40.310 --> 00:15:47.670
So but now not only we have a clear visual at what is going on there also we will get what we want which

195
00:15:47.670 --> 00:15:50.400
right now is the decoder output.

196
00:15:50.400 --> 00:15:56.460
Keep in mind in short you can actually write that down for the encoder you will only need the encoder

197
00:15:56.460 --> 00:16:02.730
state because that will then go into the decoder and for the decoder you will only need the decoder

198
00:16:02.730 --> 00:16:08.600
output the decoder output and not the final state or the final context state.

199
00:16:08.650 --> 00:16:14.480
Right so now let's get this function that will return the decoder output.

200
00:16:14.700 --> 00:16:17.700
So this function would get it from the tens of library of course.

201
00:16:17.740 --> 00:16:28.380
Then the contrib module then the SEC 2 sec submodule and from the 6 to 6 module we get that dynamic

202
00:16:28.530 --> 00:16:33.660
underscore Arnon underscore decoder function.

203
00:16:33.720 --> 00:16:41.160
That's the one dynamic Arnon decoder function and this dynamic Arnon decode function takes several arguments.

204
00:16:41.160 --> 00:16:49.770
The first one is the decoder cell which is one of our arguments of the decode training set function

205
00:16:49.770 --> 00:16:50.990
that we're making right now.

206
00:16:51.000 --> 00:16:59.550
C decode a cell here and that's what we need first for this dynamic Arnon decode function to decode

207
00:16:59.550 --> 00:17:00.190
a cell.

208
00:17:00.240 --> 00:17:06.420
Then the next argument is going to be our training underscore decoder.

209
00:17:06.630 --> 00:17:10.950
On this core function that we just got at the previous line.

210
00:17:11.070 --> 00:17:12.190
This line here.

211
00:17:12.260 --> 00:17:14.540
All right training the code a function.

212
00:17:14.580 --> 00:17:21.870
Then the next argument is going to be the embedded input of our decoder which is one of the arguments

213
00:17:21.870 --> 00:17:24.060
of our de-code training that function.

214
00:17:24.060 --> 00:17:26.300
This one the code for embedded input.

215
00:17:26.430 --> 00:17:32.340
So the argument here the code for embedded input.

216
00:17:32.580 --> 00:17:37.830
Perfect then next argument is the sequence lengths.

217
00:17:37.830 --> 00:17:45.480
Then the next one after that is the scope which is another of our arguments and that's the coding scope

218
00:17:45.480 --> 00:17:46.230
here.

219
00:17:46.230 --> 00:17:50.840
So the coding underscores cope all right and we're done.

220
00:17:50.910 --> 00:17:52.680
Then one last step.

221
00:17:52.680 --> 00:17:55.250
So let's go back here.

222
00:17:55.260 --> 00:17:56.220
Perfect.

223
00:17:56.220 --> 00:18:02.970
The final step that we should do here is to apply a final dropout to our decoder output and therefore

224
00:18:03.180 --> 00:18:10.290
I'm going to introduce a final variable here that I'm going to call the Kirner underscore output underscore

225
00:18:10.530 --> 00:18:12.190
drop out.

226
00:18:12.570 --> 00:18:17.930
And now it's going to be simple We'll just used to drop out function from the end and Mudgal by tens

227
00:18:17.930 --> 00:18:26.210
of flow to basically apply dropout to our decoder output that we just got here with.

228
00:18:26.220 --> 00:18:30.450
Still the key prob controlling parameter of the dropout rate.

229
00:18:30.450 --> 00:18:37.710
All right so let's get the tens of library first then and then module and from this end and module we

230
00:18:37.710 --> 00:18:41.530
get the drop out function.

231
00:18:41.830 --> 00:18:43.560
That's going to take two arguments.

232
00:18:43.570 --> 00:18:50.710
First the output of the decoder on which we want to apply dropout decoder output so decoder underscore

233
00:18:50.780 --> 00:18:53.900
output and the second one.

234
00:18:54.010 --> 00:19:01.120
Then keep prob parameter controlling the dropout rate so keep trub and perfect.

235
00:19:01.160 --> 00:19:05.300
We just get our decoder output with drop out applied.

236
00:19:05.360 --> 00:19:07.440
And for the training data.

237
00:19:07.700 --> 00:19:11.710
So let's not forget just to return it return.

238
00:19:11.920 --> 00:19:18.230
And now to return it the right way we're going to use our output function which is one of our arguments

239
00:19:18.230 --> 00:19:25.610
here output function and this output function is going to take what we want to return which is our decoder

240
00:19:26.360 --> 00:19:29.060
output dropout.

241
00:19:29.210 --> 00:19:30.730
And congratulations.

242
00:19:30.840 --> 00:19:33.340
We're just done with this function.

243
00:19:33.350 --> 00:19:34.970
I hope that wasn't too overwhelming.

244
00:19:34.970 --> 00:19:38.420
I really tried to take a step back several times.

245
00:19:38.510 --> 00:19:44.160
I will give you the resource of this tutorial the links of the tensor flow pages that I showed you.

246
00:19:44.180 --> 00:19:47.780
You have a lot more explanations that I could tell you in one tutorial.

247
00:19:47.840 --> 00:19:54.320
So have a look at the resources in this tutorial and hopefully have a better understanding.

248
00:19:54.320 --> 00:19:54.580
All right.

249
00:19:54.590 --> 00:19:55.610
So we're going to execute.

250
00:19:55.610 --> 00:19:57.950
I'm just seeing a little typo here.

251
00:19:58.090 --> 00:20:00.680
Output and this should be all fine.

252
00:20:00.680 --> 00:20:07.990
So we're going to select all these and execute perfect decoding of the training set done.

253
00:20:08.110 --> 00:20:11.770
And now we're going to tackle the decoding of the validation set.

254
00:20:11.800 --> 00:20:15.490
The good news is that it's going to be quite the same as what we just did.

255
00:20:15.490 --> 00:20:21.160
We're just going to make a copy paste and then we'll have a few things to change but we'll get quickly

256
00:20:21.340 --> 00:20:25.760
to creating the decoder Arnon layer into tutorials.

257
00:20:25.760 --> 00:20:27.190
Until then I'll be.
