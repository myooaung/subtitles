1
00:00:00,910 --> 00:00:04,470
Hello and welcome back to the course on the deep natural language processing.

2
00:00:04,480 --> 00:00:10,990
And today we're finally talking about this sequence to sequence model and we're going to be discussing

3
00:00:11,020 --> 00:00:12,880
its architecture.

4
00:00:12,880 --> 00:00:15,720
All right so where did we stop last time.

5
00:00:15,910 --> 00:00:21,220
Last time we stopped here where we were talking about the bag of words moral.

6
00:00:21,440 --> 00:00:27,430
And as you can see here on the right we've got our bag and we discussed that there's two at least two

7
00:00:27,430 --> 00:00:34,900
ways that you can go about it once you've got your 20000 elements long representations all of your sentences

8
00:00:34,900 --> 00:00:42,650
or emails and you have all this training data you can fit it into a non deep learning model such as

9
00:00:42,650 --> 00:00:49,270
the logistic regression and that will give you an algorithm over here or you can fit into a neural network

10
00:00:49,980 --> 00:00:57,210
meaning a deep learning algorithm deeper and model and that will give you a deep natural language processing

11
00:00:57,210 --> 00:01:04,350
model over here so both a bag of words but different types of bags of words.

12
00:01:04,870 --> 00:01:11,680
And we said that there was at least one drawback one limitation and that is that this approach just

13
00:01:11,710 --> 00:01:15,750
gives you a yes no response yes no type of response it's a good start.

14
00:01:15,760 --> 00:01:20,050
But at the same time it's not going to get very far if you're trying to build the child but you want

15
00:01:20,050 --> 00:01:25,560
to chat to people not just say yes or no to any kind of question or sentence.

16
00:01:25,590 --> 00:01:32,150
They're saying you want to have a meaningful conversation so let's have a look at some of the other

17
00:01:32,360 --> 00:01:33,310
limitations.

18
00:01:34,030 --> 00:01:42,500
Issues with that bag of words model that are that make it undesirable for a chat board for a chat but

19
00:01:42,520 --> 00:01:46,880
Project Number one is a fixed sized input.

20
00:01:46,900 --> 00:01:56,210
So if we go back here you'll see that every single time we have to have the same type of inputs same

21
00:01:56,210 --> 00:02:01,450
same level of input for this training to go well because on one hand if we get into a logistic regression

22
00:02:01,450 --> 00:02:07,750
we want to be sure that the features are in the right columns in the right places and then among the

23
00:02:07,750 --> 00:02:17,440
training data and the test or the verification data or with the actual data that we want to get a result

24
00:02:17,440 --> 00:02:18,100
for.

25
00:02:18,100 --> 00:02:26,170
So that's a limitation as that's why we have the 20000 that long or longer has that little limit that

26
00:02:26,170 --> 00:02:32,670
size and also the input layer for the neural networks same thing you get like a certain number of neurons

27
00:02:32,680 --> 00:02:39,250
you got to fit in the right words in the right spots into this neural network.

28
00:02:39,250 --> 00:02:43,210
You can mix them up you can have a different size in order for it to work.

29
00:02:43,270 --> 00:02:51,110
So that's a limitation fixed size impose a second limitation it doesn't take word order into account.

30
00:02:51,220 --> 00:02:51,950
Interesting.

31
00:02:52,150 --> 00:02:52,860
So what does that mean.

32
00:02:52,870 --> 00:02:59,680
Well if we look back jumping back to this slide we here if we look back over here you'll notice that

33
00:03:00,370 --> 00:03:10,900
we're just counting each occurrence of each word rather than looking at not just occurrences but also

34
00:03:10,900 --> 00:03:12,100
where they are placed.

35
00:03:12,100 --> 00:03:21,340
So for instance if you jumble up this this jumble up this email all the words in this email you will

36
00:03:21,340 --> 00:03:23,300
still have the same representation.

37
00:03:23,440 --> 00:03:35,890
And like we all know we all know that if we change our own words in a sentence they can change the meaning.

38
00:03:35,890 --> 00:03:37,030
For instance let's say

39
00:03:40,210 --> 00:03:44,830
for example is the cat shorter than the dog.

40
00:03:44,830 --> 00:03:46,480
Right that's one sentence.

41
00:03:46,480 --> 00:03:48,120
And the answer might be yes.

42
00:03:48,130 --> 00:03:53,200
Usually cats are short and dogs and let's say we have an example that a cat is is cat shorter than the

43
00:03:53,200 --> 00:03:53,690
dog.

44
00:03:53,860 --> 00:03:58,600
If you just change the cat and the dog has two words around and becomes is the dog shorter than the

45
00:03:58,600 --> 00:03:59,160
cat.

46
00:03:59,380 --> 00:04:04,150
Exactly same words completely opposite meaning of the answer in that case would be no.

47
00:04:04,150 --> 00:04:09,400
And so you can come up with examples where if you jumble up text in the emails words and emails even

48
00:04:09,400 --> 00:04:13,950
though the representation will be the same your answer would have changed from yes to no.

49
00:04:14,110 --> 00:04:16,630
But the model is going to get you the same answer.

50
00:04:16,780 --> 00:04:22,270
So that's a significant limitation that basically what's happening is loss of information we're losing

51
00:04:22,270 --> 00:04:27,310
information through the bag of words moral just the way it's constructed because we're not taking into

52
00:04:27,310 --> 00:04:31,680
account the position of the words to fix as input.

53
00:04:31,700 --> 00:04:33,590
It doesn't take word ordering takeout.

54
00:04:33,610 --> 00:04:39,790
And the third one is what we talked about already fixed size output in this case is just one word yes

55
00:04:39,790 --> 00:04:40,280
or no.

56
00:04:40,420 --> 00:04:47,490
But basically that's not only yes and all we could come up with for more like a longer outputs may have

57
00:04:47,500 --> 00:04:49,720
multiple words but it won't be variable.

58
00:04:49,720 --> 00:04:56,170
That's that's kind of like even an even stronger statement that we don't just want we don't we just

59
00:04:56,170 --> 00:04:57,260
want yes no.

60
00:04:57,610 --> 00:05:02,680
Like even if we have more options and yes now we might have like five options or something like that

61
00:05:02,710 --> 00:05:05,580
or they might be might be longer.

62
00:05:05,650 --> 00:05:10,390
But what we want is we want variable sized outputs so depending on what we were asked we want to be

63
00:05:10,390 --> 00:05:16,190
able to say a sentence or maybe five words or 50 words should be totally up to us.

64
00:05:16,240 --> 00:05:22,780
The model should be flexible to give out a response not just limited to whatever number of words it

65
00:05:22,780 --> 00:05:27,910
can be like a short response can be a long response just like as humans when we talk we don't limit

66
00:05:27,910 --> 00:05:32,720
ourselves to we have to answer with five words so we have to answer with 10 words.

67
00:05:32,770 --> 00:05:34,180
It has to be variable.

68
00:05:34,180 --> 00:05:37,440
So those are the main limitations of size.

69
00:05:37,630 --> 00:05:45,710
Input doesn't take world order into account and fixed sized outputs stations of that bag of words model.

70
00:05:46,090 --> 00:05:48,750
So the question is What are we going to do.

71
00:05:48,760 --> 00:05:51,040
How are we going to solve this puzzle.

72
00:05:52,360 --> 00:05:56,020
And overcome all these limitations.

73
00:05:56,230 --> 00:06:02,560
And the answer is our and thence recurrent neural networks.

74
00:06:02,590 --> 00:06:09,520
So if you haven't had a look at that annex for this course on artificial neural networks and recurrent

75
00:06:09,520 --> 00:06:18,080
neural networks now would be a great time to look at it and get into this space understand what recurrent

76
00:06:18,090 --> 00:06:24,570
neural networks are and how they work what they're constructed of different types and so on.

77
00:06:24,570 --> 00:06:27,630
What back propagation is to cast a gradient descent.

78
00:06:27,640 --> 00:06:33,700
And so there's lots and lots of concepts that are going to be assumed going forward but that you are

79
00:06:33,700 --> 00:06:34,770
up to date with them.

80
00:06:34,780 --> 00:06:38,400
You know what what they are in the future.

81
00:06:38,410 --> 00:06:39,860
Tauriel starting from now.

82
00:06:40,000 --> 00:06:43,030
And further down in this course.

83
00:06:43,090 --> 00:06:49,540
So if you haven't looked at the Alecs higher command to stop this video now and look at that Alex but

84
00:06:49,540 --> 00:06:51,770
otherwise let's proceed.

85
00:06:52,620 --> 00:06:52,970
All right.

86
00:06:52,980 --> 00:07:05,190
So this is like if you've seen that X for the recurrent neural networks this is what Andrey Karpovsky

87
00:07:05,640 --> 00:07:13,620
has come up with in his log on to get help of the different types of recurrent old network architectures

88
00:07:13,620 --> 00:07:19,190
that you can and just a reminder recurrent neural networks here.

89
00:07:19,230 --> 00:07:28,670
Every single box is not actually one cell it's actually a whole layer of neurons a whole layer of neurons

90
00:07:28,700 --> 00:07:37,750
are presented by every single box just that here what we're looking at is unraveled neural network and

91
00:07:37,820 --> 00:07:41,120
unravel time a loop of the recurrent neural network.

92
00:07:41,120 --> 00:07:48,800
So this is the neural network feeding into itself through time like each one is different of course

93
00:07:49,160 --> 00:07:54,860
but it's feeding itself into itself through time but if you look at it like from the top actually here

94
00:07:55,290 --> 00:08:00,670
like it's going into the image you've got a whole layer of you know it's like a three dimensional image

95
00:08:00,680 --> 00:08:02,030
that's why you can see it.

96
00:08:02,030 --> 00:08:03,500
That's the way I imagine it.

97
00:08:03,500 --> 00:08:07,520
We've got a whole layer whole layer everywhere is a layer of neurons.

98
00:08:07,760 --> 00:08:10,940
So one way is like layer layer layer.

99
00:08:11,090 --> 00:08:17,870
Another one is OK so layer one layer and then one too many so that they actually have titles and top

100
00:08:18,260 --> 00:08:19,220
many to one.

101
00:08:19,220 --> 00:08:20,180
So you have many.

102
00:08:20,180 --> 00:08:26,530
And then you come one you have many many many too many this way.

103
00:08:26,540 --> 00:08:34,740
So right away just based on what we're trying to build here we want we want a variable sized input and

104
00:08:34,740 --> 00:08:35,780
a variable sized output.

105
00:08:35,780 --> 00:08:39,770
So one to one would it work for us one too many would work for us.

106
00:08:39,770 --> 00:08:44,610
Because we have we don't have just one word as an input.

107
00:08:44,650 --> 00:08:51,080
So we're going to be actually going to put words into each cell and it will make sense as we go along.

108
00:08:51,230 --> 00:08:54,410
It will make sense how this works.

109
00:08:54,580 --> 00:09:00,270
But for now the way we're going to look at it so this one won't work this whole will work is always

110
00:09:00,310 --> 00:09:03,410
work because it's just one on one.

111
00:09:03,500 --> 00:09:08,380
We want more like this is kind of like if we have a yes no response this could work for us but we want

112
00:09:08,380 --> 00:09:09,200
more than one.

113
00:09:09,220 --> 00:09:13,170
So we're left with one of these two options.

114
00:09:13,200 --> 00:09:17,430
Your own network with many many like this are made to you.

115
00:09:17,830 --> 00:09:20,310
And the one we're going to go with is this one.

116
00:09:20,400 --> 00:09:29,230
And the reason for that is because we not necessarily will have always the same size inputs and the

117
00:09:29,230 --> 00:09:30,930
same size outputs.

118
00:09:30,940 --> 00:09:36,220
So if somebody asks us a question or five words the we will have five words and that's so that's why

119
00:09:36,220 --> 00:09:42,870
it was a terrible terrible terrible input and output but they might be different.

120
00:09:43,240 --> 00:09:49,750
So that's like a short ex-lawyers into what's what type of neural network architecture we're going to

121
00:09:49,750 --> 00:09:50,750
be aiming for.

122
00:09:50,980 --> 00:09:51,850
It's going to be that way.

123
00:09:52,120 --> 00:09:57,210
But now let's jump back to our text and we'll make much more sense from there.

124
00:09:58,060 --> 00:10:05,050
So there's our 20000 element long vector which we use for the bag of words and we're going to just throw

125
00:10:05,050 --> 00:10:06,970
it away because we don't need it.

126
00:10:07,030 --> 00:10:08,960
In fact we're actually going to shorten the sentence.

127
00:10:08,980 --> 00:10:14,950
It's not it's not something that's part of the algorithm of course we can work with any slam sentences

128
00:10:14,950 --> 00:10:15,870
and emails and so on.

129
00:10:16,000 --> 00:10:25,370
But just for simplicity's sake for us to look at look at the whole picture and be able to grasp it.

130
00:10:25,480 --> 00:10:26,440
We're going to shorten it.

131
00:10:26,470 --> 00:10:28,800
And that's just for educational purposes.

132
00:10:28,810 --> 00:10:36,560
Again this whole this algorithm works for any live question or email or chat whatever we're creating.

133
00:10:37,000 --> 00:10:39,460
And so yeah.

134
00:10:39,580 --> 00:10:44,020
So what are we going to do is we're going to take these words each single word including the comma and

135
00:10:44,050 --> 00:10:46,060
full stop and we're going to code them.

136
00:10:46,060 --> 00:10:47,830
So we're going to create a different picture.

137
00:10:47,830 --> 00:10:50,170
This time it's not 20000 symbols.

138
00:10:50,350 --> 00:10:52,550
It's just going to have Word.

139
00:10:52,550 --> 00:10:58,720
So like one way to create it is we take those you know from that big vector of 20000.

140
00:10:58,780 --> 00:11:05,200
We just take the position of every word so say hello or was in position 5 so we're going to put a 5

141
00:11:05,200 --> 00:11:05,910
here.

142
00:11:05,920 --> 00:11:12,070
Carol doesn't have a position that 20000 vectors so going to put a zero because one of those non-English

143
00:11:12,070 --> 00:11:14,340
words comma position 9.

144
00:11:14,340 --> 00:11:18,880
So we're going to put a nine checking into instead of counting the recurrence of every word.

145
00:11:18,910 --> 00:11:27,860
We're creating a vector which is as long as our vector as long as our sentence plus and a start of sentence

146
00:11:27,990 --> 00:11:34,340
here is our senses end of sentence at the beginning and they as we remember they have reserve numbers.

147
00:11:34,340 --> 00:11:37,410
So this was always going to be one this was always going to be two.

148
00:11:37,900 --> 00:11:44,740
And then for every element we just replacing the word with the actual numerical version of the word

149
00:11:44,750 --> 00:11:47,620
the number that's the position that's associated with that.

150
00:11:47,620 --> 00:11:52,600
So like for instance if you had two of the same words like if we had if and if you'd have a seven and

151
00:11:52,600 --> 00:11:57,220
another seven or you had like a comma and another column we would have a nine and another night.

152
00:11:57,230 --> 00:11:59,900
So that's totally fine.

153
00:11:59,920 --> 00:12:01,540
And that this is how we construct intersect.

154
00:12:01,540 --> 00:12:09,220
And so as you can see there's the size the length of this vector will depend on how many words we have

155
00:12:09,220 --> 00:12:11,700
in our email how long our email is.
