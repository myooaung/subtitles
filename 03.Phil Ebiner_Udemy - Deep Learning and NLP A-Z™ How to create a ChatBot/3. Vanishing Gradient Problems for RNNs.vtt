WEBVTT
1
00:00:00.550 --> 00:00:05.560
Hello and welcome back to the course on deep learning I hope you enjoyed the introduction into our Anand's

2
00:00:05.650 --> 00:00:07.980
and today right away off the bat.

3
00:00:07.990 --> 00:00:12.610
We're going to jump into a huge problem that exists with our own ends.

4
00:00:12.610 --> 00:00:17.350
All right so what is this problem called the vanishing gradient.

5
00:00:17.370 --> 00:00:25.040
It is first discovered by Sep hoc writer and I hope I'm pronouncing that right.

6
00:00:25.070 --> 00:00:31.210
I know we have students from Germany so you can comment if the pronunciation is incorrect but e he said

7
00:00:31.510 --> 00:00:44.310
Sep or Joeseph is his full name is a genius scientist who back in the 90s while he was still not a professor

8
00:00:44.340 --> 00:00:48.830
is a recent photo so he was much younger when he actually came up with this concept.

9
00:00:48.850 --> 00:00:51.250
He found that there was a big big problem.

10
00:00:51.280 --> 00:00:57.460
And we'll talk about the the problem solved just now I just wanted to point out who are the people that

11
00:00:57.460 --> 00:01:04.240
spotted this and basically as you'll learn from the further Charles SAP is one of the founding people

12
00:01:04.330 --> 00:01:10.420
in the modern way that we use our own ends and LACMA and we'll talk about that further down.

13
00:01:10.420 --> 00:01:11.950
But this is hip hop right.

14
00:01:11.960 --> 00:01:16.160
So I just wanted to make sure you know who is behind all this.

15
00:01:16.300 --> 00:01:19.150
And the second person is Yoshio banjo.

16
00:01:19.150 --> 00:01:24.280
He's a professor at the University of Montreal.

17
00:01:24.310 --> 00:01:27.020
He also discovered that there's this problem.

18
00:01:27.460 --> 00:01:32.480
I think a bit later in 1994 so CEP was discovered in 1991.

19
00:01:32.500 --> 00:01:35.170
Yoshio wrote about this in 1994.

20
00:01:35.530 --> 00:01:41.920
But again this is another person driving this whole trip pushing the envelope in the space of our own

21
00:01:41.920 --> 00:01:42.240
ends.

22
00:01:42.250 --> 00:01:49.840
And if you go to the University of Monteriano you look up Yoshio your show's profile you will find that

23
00:01:50.350 --> 00:01:54.880
Russia has over five hundred research papers.

24
00:01:54.880 --> 00:01:59.920
And by the way there is Yoshio banjo just hanging out with young Licken as you can see they all know

25
00:01:59.920 --> 00:02:04.360
each other it's a very tight knit community and it does sound like a conspiracy.

26
00:02:04.360 --> 00:02:08.710
These people that are driving are pushing the envelope in terms of deep learning.

27
00:02:08.780 --> 00:02:14.350
These are just like a select group of people who are all all in it together they all know what's going

28
00:02:14.350 --> 00:02:14.920
on there.

29
00:02:14.950 --> 00:02:19.090
Been doing it since the 80s or 90s and now it's all popping out into the world.

30
00:02:19.360 --> 00:02:24.330
So there you go that's just to give you a quick idea of who's behind all this and today.

31
00:02:24.340 --> 00:02:28.100
And now we're proceeding to the vanishing problem itself.

32
00:02:28.120 --> 00:02:34.030
So as you remember this is a gradient descent algorithm we're trying to find the log of the global minimum

33
00:02:34.300 --> 00:02:40.650
of your cost function and that's going to be the optimal solution optimal set up for your neural network.

34
00:02:40.960 --> 00:02:49.720
As you also recall your gradient or your information travels through your neural network to your answer

35
00:02:50.020 --> 00:02:55.450
to get your outputs and then the error is calculated and is propagated back to the network to update

36
00:02:55.480 --> 00:02:56.880
the weights.

37
00:02:56.890 --> 00:02:58.640
And here we've got some way to find out.

38
00:02:58.660 --> 00:03:03.330
So what happens in a recurrent neural network is a similar thing.

39
00:03:03.340 --> 00:03:07.030
But here you've got a bit more going on right.

40
00:03:07.030 --> 00:03:10.790
So when your information trials through the network it travels like that.

41
00:03:10.810 --> 00:03:17.470
You've got lots of travels through time and information from previous timepoint goes keeps coming through

42
00:03:17.470 --> 00:03:21.910
the network and remember that every note here it's always very important to remember for neural networks

43
00:03:21.910 --> 00:03:30.080
that every single node here is not just a node it's a representation of a whole layer of knowledge.

44
00:03:30.090 --> 00:03:33.370
Remember we're looking at from like from the top or from the bottom.

45
00:03:33.370 --> 00:03:41.110
They are they actually extend through this chart down there into into this presentation you can see

46
00:03:41.170 --> 00:03:46.450
that there's lots more neurons behind the ones that we can't actually see because each one represents

47
00:03:46.470 --> 00:03:48.390
Alair very important to remember that.

48
00:03:48.490 --> 00:03:54.140
And so at each point in time you can calculate your cost function or your error.

49
00:03:54.160 --> 00:04:00.040
So basically your cost function compares your output which is in the right circle to your desired output

50
00:04:00.040 --> 00:04:02.500
to what you should be getting.

51
00:04:02.500 --> 00:04:04.250
And this happens during the training.

52
00:04:04.420 --> 00:04:08.810
And so and you have these values for throughout the time series.

53
00:04:08.830 --> 00:04:16.000
So for every single one of these red circles you can calculate the cost function and let's focus on

54
00:04:16.000 --> 00:04:16.360
one.

55
00:04:16.360 --> 00:04:20.790
Just to understand what's going on let's focus on this one specifically at time t.

56
00:04:21.010 --> 00:04:27.070
So you've calculated the cost function Epsilon team and now you want to propagate your cost function

57
00:04:27.070 --> 00:04:28.170
back through the network.

58
00:04:28.180 --> 00:04:30.480
How is this going to look because you need to update the weight.

59
00:04:30.490 --> 00:04:35.410
So every single neuron which participated in the calculation of the output associated with the cost

60
00:04:35.440 --> 00:04:42.550
function should be should have their weight updated to in order to help it better calculate the output

61
00:04:42.580 --> 00:04:43.920
to minimize that error.

62
00:04:44.080 --> 00:04:50.290
And the thing here is that it's not just the neurons in directly below excellente directly below this

63
00:04:50.290 --> 00:04:52.470
red circle it's all the neurons that contributed.

64
00:04:52.630 --> 00:04:53.880
And there's many more of them.

65
00:04:53.910 --> 00:04:59.200
There's all of these neurons as far back as you go depending on how many times steps you take you might

66
00:04:59.200 --> 00:05:04.890
take one and take two I take 50 ton steps depending on how you set up your network.

67
00:05:05.030 --> 00:05:11.060
And this is where the problem lies that you have to update or you have to propagate all the way back

68
00:05:11.780 --> 00:05:13.800
through time to these neurons.

69
00:05:13.880 --> 00:05:17.750
And we talk when we talk about time it's not that the problem is that we can travel through time not

70
00:05:17.750 --> 00:05:20.300
at all that we've unraveled this network.

71
00:05:20.300 --> 00:05:22.860
So this whole propagation can be facilitated.

72
00:05:22.880 --> 00:05:28.320
The problem lies in something else and as much I don't like putting math on the slide on intuition slides

73
00:05:28.340 --> 00:05:31.450
we're not going to go through the math but I'll point out one thing here.

74
00:05:31.450 --> 00:05:37.790
So this is the math behind our hands and we'll definitely direct you to more additional reading which

75
00:05:37.790 --> 00:05:40.690
you can do to upskilling on all of these math.

76
00:05:40.790 --> 00:05:47.780
So here we've got w rec and W stands for weight and recurring and that is the weight that is used to

77
00:05:48.200 --> 00:05:57.740
connect the hidden Lares to themselves in the unrolled temporal loop and as you can see here on the

78
00:05:57.740 --> 00:06:04.970
left you're in order to get from XTi my history from this layer remember this is a letter to you sister

79
00:06:05.010 --> 00:06:07.810
to you need to apply double check.

80
00:06:07.820 --> 00:06:09.320
Then from here to here appliable.

81
00:06:09.340 --> 00:06:09.660
Right.

82
00:06:09.770 --> 00:06:16.790
And in simple terms into intuitive terms what you're doing is you're simply multiplying the value here

83
00:06:16.790 --> 00:06:20.940
as you remember to get from one lairds index we multiply the output by the way.

84
00:06:20.960 --> 00:06:24.710
And then we get to the next line and then multiply the output by the way and get to the X and then we

85
00:06:24.710 --> 00:06:25.990
modify the output from here.

86
00:06:26.000 --> 00:06:26.970
By the way get here.

87
00:06:27.230 --> 00:06:32.420
And the thing here is that we're multiplying with the same exact ways multiple times many times as many

88
00:06:32.420 --> 00:06:34.880
times as we need to go through this temporal loop.

89
00:06:34.880 --> 00:06:39.500
And this can be set this you can set this in your network do you want to do it once you want to look

90
00:06:39.500 --> 00:06:43.220
back one step you'll look back three steps you'll look back 50 steps.

91
00:06:43.220 --> 00:06:46.980
But as many times as we do it we have to multiply by the weight.

92
00:06:47.150 --> 00:06:55.400
And this is where the problem arises because when you multiply by something small your value decreases

93
00:06:55.400 --> 00:07:02.690
very quickly and this multiplication comes from this p here you can see that pea's says for multiplication

94
00:07:02.700 --> 00:07:03.790
so we have to multiply.

95
00:07:03.860 --> 00:07:06.930
And that's what is representing that as far as you go back you multiply.

96
00:07:07.130 --> 00:07:14.090
And as you remember weights are assigned at the start of your neural network as you see in the practical

97
00:07:14.090 --> 00:07:14.470
tutorials.

98
00:07:14.480 --> 00:07:19.970
Your weights are assigned as a random value but run of those clusters or and from there.

99
00:07:19.970 --> 00:07:23.460
The network trains them up and identifies what they should be.

100
00:07:23.660 --> 00:07:30.170
But if you start with random double random double close to zero then because you're multiplying but

101
00:07:30.170 --> 00:07:36.980
many times the more you multiply the lower the value gets so if you start off you might have a certain

102
00:07:36.980 --> 00:07:41.220
gradient going through your network being back propagate through your network.

103
00:07:41.540 --> 00:07:47.090
Then you move backwards your gradient becomes less then your gradient becomes less and then your brain

104
00:07:47.090 --> 00:07:48.250
becomes even less.

105
00:07:48.320 --> 00:07:49.860
And what does that mean for the network.

106
00:07:49.860 --> 00:07:51.830
This is the important part too.

107
00:07:51.830 --> 00:07:53.200
Kind of like get your head around that.

108
00:07:53.210 --> 00:07:57.470
What does a vanishing gradient like that why is it bad for the network.

109
00:07:57.490 --> 00:08:02.760
Well because the gradient as it goes back through the network it is used to update the way.

110
00:08:02.780 --> 00:08:09.170
And we know that already well the lower the gradient is the harder it is for the network to update the

111
00:08:09.170 --> 00:08:09.430
way.

112
00:08:09.420 --> 00:08:11.160
It cannot.

113
00:08:11.270 --> 00:08:19.550
It can sylke understand what kind of contribution each of these outputs has towards the error and therefore

114
00:08:19.550 --> 00:08:20.430
we can update the weights.

115
00:08:20.450 --> 00:08:25.700
But the lower the gradient the slower it's going to update the weights and the higher the gradient the

116
00:08:25.700 --> 00:08:29.060
fosters going to update the weights and get to the final result.

117
00:08:29.300 --> 00:08:36.380
And so therefore if you have like for instance a thousand pocks these weights for for this layer might

118
00:08:36.380 --> 00:08:42.470
be updated towards the end of the thousand and they'll have some final results but these weights because

119
00:08:42.470 --> 00:08:49.220
the greens are so much smaller they're going to be up there it's slower and therefore by the end of

120
00:08:49.220 --> 00:08:54.230
the thousand epochs you might not have the final results there and therefore this part of the network

121
00:08:54.310 --> 00:08:58.650
trained this part of network is not trained based on this cause function.

122
00:08:58.730 --> 00:09:05.030
But the problem here is not just that half your network is not trained properly but also that these

123
00:09:05.480 --> 00:09:12.140
way these weights are this layer it's outputs are being used as inputs for further Lares.

124
00:09:12.170 --> 00:09:16.130
So the training here has been happening all along.

125
00:09:16.250 --> 00:09:22.670
Based on vore based off of inputs that are coming from untrained neurons untrained Lerer So it's kind

126
00:09:22.670 --> 00:09:24.200
of like a vicious cycle.

127
00:09:24.200 --> 00:09:28.400
You're you're training here and you think you're getting good results but because you're gradient so

128
00:09:28.400 --> 00:09:36.140
small here this is training so slow and outputs is giving is so are are incorrect are not final outputs

129
00:09:36.140 --> 00:09:40.880
and therefore your training on the non-final outputs so because of this disbalance because of vanishing

130
00:09:40.880 --> 00:09:48.650
grading you have a a problem in your network which is not just that these weights are not being trained

131
00:09:48.650 --> 00:09:52.760
properly the whole network has not been trained properly because these weights are not being trained

132
00:09:52.760 --> 00:09:53.410
properly.

133
00:09:53.520 --> 00:09:59.870
And and that's there's like a domino effect like this wherever you look in the network always the ones

134
00:09:59.870 --> 00:10:01.830
at the very back are going to have that problem.

135
00:10:01.970 --> 00:10:08.360
And this is in a nutshell what the vanishing gradient problem for recurrent neural networks is.

136
00:10:08.560 --> 00:10:15.990
And that's kind of the main roadblock to using recurrent neural networks.

137
00:10:16.040 --> 00:10:24.670
And let's summarize this in a in a short slide so double Jurek is small then your you have a vanishing

138
00:10:24.680 --> 00:10:29.200
grade problem if W is large you have an exploding Green problem because same thing.

139
00:10:29.210 --> 00:10:31.040
But then it's just going to explode.

140
00:10:31.250 --> 00:10:36.900
And here's an important thing to point out here is that of course there's more to it right.

141
00:10:36.920 --> 00:10:39.950
There's as you can see there's this formula.

142
00:10:39.960 --> 00:10:43.070
There's other things like the activation functions and so on.

143
00:10:43.070 --> 00:10:46.380
So it's not as simple as small or large or less than one greater than one.

144
00:10:46.430 --> 00:10:50.450
But as a rule of thumb if you have very small weights you've got to have a vaccine ready and if you

145
00:10:50.450 --> 00:10:55.970
have very large weights we put 100 in there the value of 100 and you're going to buy by you're going

146
00:10:55.970 --> 00:11:00.190
to have 10000 by step three you're going to have a million.

147
00:11:00.200 --> 00:11:03.660
So then you have an exploding graded problem.

148
00:11:03.920 --> 00:11:09.410
So hopefully this summarizes xponent grain problem on an intuitive level because.

149
00:11:09.530 --> 00:11:16.850
So in short because you're unraveling the temporal loop the further you go through your network the

150
00:11:16.880 --> 00:11:21.290
lower your gradient is and the harder it is to train the ways which has a domino effect on all of the

151
00:11:21.290 --> 00:11:24.470
further weights throughout the network as well.

152
00:11:24.470 --> 00:11:27.170
And so how do you combat vanishing brain problem.

153
00:11:27.170 --> 00:11:28.860
There's a couple of solutions.

154
00:11:29.060 --> 00:11:34.370
For instance for the exploding gradient you can have truncated Bracq back propagation so you stop back

155
00:11:34.370 --> 00:11:38.660
propagating after a certain point but as you can imagine that's that's probably not optimal because

156
00:11:38.660 --> 00:11:44.780
then you're not updating all the weights but if you don't stop then you're just going to have a completely

157
00:11:44.840 --> 00:11:48.860
irrelevant network so it's better than the original approach.

158
00:11:49.110 --> 00:11:55.580
Then you can have penalties so you could have the gradient being penalize and Beanery is artificially

159
00:11:55.580 --> 00:11:56.640
reduced.

160
00:11:56.720 --> 00:12:02.330
You can have gradient clipping so you could have like a maximum limit for the gradient you could say

161
00:12:02.360 --> 00:12:05.080
never never have the gradient go over this value.

162
00:12:05.120 --> 00:12:09.540
And then if it's over that value just stays at that value as it propagates further down through the

163
00:12:09.540 --> 00:12:10.300
network.

164
00:12:10.730 --> 00:12:17.630
You can have the Vanish Ingredion problem you have a CRN other solutions you have weight initialization

165
00:12:18.500 --> 00:12:25.500
where you are smart about how you initialize your weights to minimize the potential for vanishing gradient.

166
00:12:25.580 --> 00:12:33.080
You can have there's a type of network called the Echo state networks and we're not going to talk about

167
00:12:33.080 --> 00:12:39.480
that but they do somehow solve that or they are designed to solve the vanishing problem.

168
00:12:39.530 --> 00:12:44.610
But there's also a different type of network called The Long short term memory networks or the LACMA

169
00:12:44.630 --> 00:12:53.630
for short which are extremely popular which are considered to be the go to network for implementing

170
00:12:53.690 --> 00:12:58.820
recurrent neural networks and that's exactly what we're going to be talking about in the rest of this

171
00:12:58.910 --> 00:12:59.480
section.

172
00:12:59.480 --> 00:13:01.640
So that's going to be very exciting.

173
00:13:01.670 --> 00:13:07.320
We're going to look at a brand new architecture for recurrent neural networks really Conway to get started

174
00:13:07.320 --> 00:13:11.830
on that it's a very interesting topic to get into.

175
00:13:12.530 --> 00:13:19.100
But for now this is this brings us to the end of the vanishing gray problem Tauriel and some additional

176
00:13:19.100 --> 00:13:19.630
reading.

177
00:13:19.700 --> 00:13:26.330
So additional reading you can definitely reference the original works by self harm writer and social

178
00:13:26.330 --> 00:13:26.620
change.

179
00:13:26.630 --> 00:13:31.020
So this is Cephus paper in 1991 from 1991.

180
00:13:31.190 --> 00:13:36.440
It's completely in German so I'm not even going to read the name but if you understand and can read

181
00:13:36.440 --> 00:13:39.980
German then definitely this could be a good read for you.

182
00:13:40.040 --> 00:13:45.260
Then there's Yoshua banjo's paper which is called Learn learning long term dependency of green dissent

183
00:13:45.290 --> 00:13:48.140
is difficult 1994.

184
00:13:48.140 --> 00:13:53.930
Also you can reference that but what I would recommend looking into is a paper called on the difficulty

185
00:13:53.930 --> 00:13:57.440
of training a recurrent neural networks by Razvan Pascal.

186
00:13:57.440 --> 00:14:03.030
No it's just newer it's 2013 and it's also got a banjo as the co-author.

187
00:14:03.050 --> 00:14:05.210
So probably you're supervising some of this research.

188
00:14:05.390 --> 00:14:12.530
And here it summarizes a lot of things that Yoshio banjo talks about in his paper from 1994.

189
00:14:12.530 --> 00:14:18.220
So why not look at something you or I would recommend checking this paper out.

190
00:14:18.680 --> 00:14:19.580
So there we go.

191
00:14:19.640 --> 00:14:22.170
That's the vanishing Green problem.

192
00:14:22.220 --> 00:14:23.120
Hope this was fun.

193
00:14:23.150 --> 00:14:26.300
Can wait to see on the next tutorial and until then enjoy deep learning.
