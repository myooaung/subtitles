WEBVTT
1
00:00:00.390 --> 00:00:06.930
Hello and welcome back to the course on deep learning to do it just quickly cover off the variations

2
00:00:06.930 --> 00:00:09.820
of long short term memory architectures.

3
00:00:09.840 --> 00:00:16.770
I think it's important so that you are at least aware that there are other alternatives to the elist

4
00:00:16.770 --> 00:00:20.930
him version that we looked at and that you might encounter them sometime in your work.

5
00:00:20.940 --> 00:00:27.590
So here is the standard LACMA which we discussed it should be it should be a pretty comfortable fit

6
00:00:27.600 --> 00:00:28.320
by now.

7
00:00:28.540 --> 00:00:32.660
And now let's have a look at a couple of variations some variations.

8
00:00:32.670 --> 00:00:34.960
One is when you add peepholes.

9
00:00:34.980 --> 00:00:44.110
So I'm going to go back again so you see these lines are added connecting these sigmoid activation functions

10
00:00:44.110 --> 00:00:53.670
or these layer neural network layer operations additional input is being provided it's information about

11
00:00:53.670 --> 00:00:59.790
the current state of the memory cell and that basically allows kind of like a is that's what it's called

12
00:00:59.790 --> 00:01:07.290
peephole allowance allows these decisions about the valves to be made with taking into account what

13
00:01:07.290 --> 00:01:13.470
is actually sitting there in the memory modification Number two is when you take these two valves.

14
00:01:13.470 --> 00:01:16.890
Forget valve and the memory valve and you connect them.

15
00:01:16.890 --> 00:01:24.030
So let's go back instead of having a separate decision for the memory valve.

16
00:01:24.060 --> 00:01:29.250
Now you have a combined decision for the forgetful and the memory valve and basically when whenever

17
00:01:29.250 --> 00:01:30.770
you add something into memory.

18
00:01:30.780 --> 00:01:36.000
So whenever you close the memory off whenever this is zero this becomes the opposite one month zero

19
00:01:36.000 --> 00:01:38.060
becomes a one so you have to put something in.

20
00:01:38.190 --> 00:01:43.410
If you put this if you make this a one way close to one it becomes a zero and therefore you don't put

21
00:01:43.410 --> 00:01:44.670
anything in here.

22
00:01:44.670 --> 00:01:45.780
You just keep the old memory.

23
00:01:45.840 --> 00:01:49.410
So it just makes sense to combine them sometimes.

24
00:01:49.890 --> 00:01:58.080
And then another modification a very popular modification called the good recurring units G.R. use for

25
00:01:58.080 --> 00:01:59.470
short.

26
00:01:59.880 --> 00:02:05.610
They what they do is they completely get rid of the C pipeline as you can see the memory problem and

27
00:02:05.610 --> 00:02:11.450
they replace it with the H which is the hidden pipeline which we had before at the bottom.

28
00:02:11.550 --> 00:02:12.930
They just combined basically the two.

29
00:02:12.930 --> 00:02:19.540
So now instead of having two separate values one for the memory and one for the hidden state.

30
00:02:19.560 --> 00:02:26.790
Now you have one and it make it simplifies things so it's probably a bit less flexible but in terms

31
00:02:26.790 --> 00:02:31.970
of how many things are being controlled and monitored there there's actually less.

32
00:02:32.120 --> 00:02:34.150
And and that's it.

33
00:02:34.170 --> 00:02:35.630
And that's this is harder.

34
00:02:35.730 --> 00:02:40.410
Get a recurring unit works as you can see it might look a bit more quickly but in reality it's a bit

35
00:02:40.410 --> 00:02:45.250
simpler you only have you know this this valve.

36
00:02:45.750 --> 00:02:50.090
Then you have another valve here another valve here and these two are currently connected as well.

37
00:02:50.250 --> 00:02:58.380
So this is in essence to get current unit costs behind it is to get rid of the memory cell and just

38
00:02:58.440 --> 00:03:04.080
have this one pipeline going through there that takes care of everything.

39
00:03:04.110 --> 00:03:04.640
So there we go.

40
00:03:04.640 --> 00:03:08.580
That's the three Alycia variations.

41
00:03:08.580 --> 00:03:10.980
There's there's lots more other ones.

42
00:03:10.980 --> 00:03:20.010
A good paper to check out is called episteme a search space odyssey by Klaus Graf and others 2015.

43
00:03:20.010 --> 00:03:26.770
So there they compared quite a few different LACMA you might like this research that they did.

44
00:03:27.120 --> 00:03:27.600
So there you go.

45
00:03:27.600 --> 00:03:34.260
That's a short intro into the alternative architectures of the LS team and I look forward to seeing

46
00:03:34.260 --> 00:03:35.030
you next time.

47
00:03:35.040 --> 00:03:36.460
Until then enjoy deep learning.
