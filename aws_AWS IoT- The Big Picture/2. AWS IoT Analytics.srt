1
00:00:01,282 --> 00:00:04,600
Let's get started with AWS IoT Analytics.

2
00:00:04,600 --> 00:00:07,797
AWS IoT Analytics is as fully-managed service that makes it

3
00:00:07,797 --> 00:00:11,477
easy to run and operationalize sophisticated analytics on

4
00:00:11,477 --> 00:00:14,096
massive volumes of IoT data.

5
00:00:14,096 --> 00:00:15,667
So what does this actually mean?

6
00:00:15,667 --> 00:00:18,354
Well it's going to make analyzing all of our IoT data

7
00:00:18,354 --> 00:00:20,525
a lot simpler and a lot easier.

8
00:00:20,525 --> 00:00:24,780
Let's start by breaking this out into the two sections of IoT Analytics,

9
00:00:24,780 --> 00:00:27,710
preparing and analyzing information.

10
00:00:27,710 --> 00:00:31,703
We'll start with preparing information by looking at how we can use channels,

11
00:00:31,703 --> 00:00:32,047
pipelines,

12
00:00:32,047 --> 00:00:35,824
and data stores to get us all set up to collect and

13
00:00:35,824 --> 00:00:37,423
prepare that data to be analyzed.

14
00:00:37,423 --> 00:00:37,682
Then,

15
00:00:37,682 --> 00:00:41,114
we'll look at how we can actually analyze this data

16
00:00:41,114 --> 00:00:43,052
using data sets and notebooks.

17
00:00:43,052 --> 00:00:44,921
So let's get started.

18
00:00:44,921 --> 00:00:49,732
Let's imagine we had the earlier IoT rule that we made in a previous module,

19
00:00:49,732 --> 00:00:52,289
and let's just say we sent the data to AWS IoT Analytics rather

20
00:00:52,289 --> 00:00:55,507
than some of the other rules we could configure.

21
00:00:55,507 --> 00:00:58,487
And let's go ahead and say we've done this now and IoT

22
00:00:58,487 --> 00:01:01,453
Analytics is now prepared to take this data in.

23
00:01:01,453 --> 00:01:06,136
Well that's going to get dumped into what's called an IoT Analytics channel.

24
00:01:06,136 --> 00:01:09,846
Now this IoT Analytics channel has a few pieces of the

25
00:01:09,846 --> 00:01:12,470
configuration that we should know about.

26
00:01:12,470 --> 00:01:15,408
The first for the channels are the target for the IoT rule.

27
00:01:15,408 --> 00:01:19,226
Now because channels are targets for an IoT rule,

28
00:01:19,226 --> 00:01:21,719
we can configure the channels as a target or we can

29
00:01:21,719 --> 00:01:25,949
configure the rules as a source, basically it doesn't matter which way we go,

30
00:01:25,949 --> 00:01:27,703
it just depends on which part of the user interface

31
00:01:27,703 --> 00:01:30,057
we're using inside of the AWS console.

32
00:01:30,057 --> 00:01:30,822
But either way,

33
00:01:30,822 --> 00:01:34,439
we will need to start with an IoT rule that outputs

34
00:01:34,439 --> 00:01:37,717
into an AWS IoT Analytics channel.

35
00:01:37,717 --> 00:01:40,991
After that we'll be able to configure our channel to set data retention

36
00:01:40,991 --> 00:01:44,546
settings on how long it wants to keep our information around for.

37
00:01:44,546 --> 00:01:46,399
Now in addition to the channels,

38
00:01:46,399 --> 00:01:49,568
we're also going to have the ability to send the information

39
00:01:49,568 --> 00:01:53,507
from a channel to an IoT Analytics pipeline.

40
00:01:53,507 --> 00:01:55,617
Now this analytics pipeline could do absolutely nothing,

41
00:01:55,617 --> 00:01:59,656
it could just move the data from the channel to an eventual destination.

42
00:01:59,656 --> 00:02:03,567
But it also has the option to take a few configuration settings.

43
00:02:03,567 --> 00:02:06,412
Specifically, we can put an input channel here.

44
00:02:06,412 --> 00:02:08,404
Now this will determine which channel is actually

45
00:02:08,404 --> 00:02:10,016
getting the information to the pipeline,

46
00:02:10,016 --> 00:02:14,049
and then we can say what the output data store will be.

47
00:02:14,049 --> 00:02:16,986
So we can say where are we sending this data.

48
00:02:16,986 --> 00:02:19,139
Now we'd have to have a data store created and configured in

49
00:02:19,139 --> 00:02:21,253
our account in order to set a data store,

50
00:02:21,253 --> 00:02:23,490
and we'll talk about that in just as moment.

51
00:02:23,490 --> 00:02:26,961
But finally, we can also do transformation activities inside of our pipelines,

52
00:02:26,961 --> 00:02:31,064
and these can shift and transform the data that's coming in through IoT

53
00:02:31,064 --> 00:02:34,071
rules and IoT channels to feed into our pipeline.

54
00:02:34,071 --> 00:02:36,903
So what do these transformation activities look like?

55
00:02:36,903 --> 00:02:38,838
Let's take a look at a few of them.

56
00:02:38,838 --> 00:02:41,720
Transformation activities can take the shape of lambda activities where a

57
00:02:41,720 --> 00:02:46,515
lambda function processes potentially more complex data in a customized

58
00:02:46,515 --> 00:02:50,148
manner using code that you write in a lambda function in one of many

59
00:02:50,148 --> 00:02:52,859
different programming language possibilities.

60
00:02:52,859 --> 00:02:55,273
We could also use device registry activities,

61
00:02:55,273 --> 00:02:58,658
which are configured for us in the AWS console or using

62
00:02:58,658 --> 00:03:01,012
configuration through the CLIs or SDKs,

63
00:03:01,012 --> 00:03:04,423
and these are essentially what allow us to go to the Thing Registry,

64
00:03:04,423 --> 00:03:08,373
find information about different devices and enrich the data doing that.

65
00:03:08,373 --> 00:03:12,675
Another option is to use a math activity where we do some math on the

66
00:03:12,675 --> 00:03:16,359
input that's coming in and then output based on that math.

67
00:03:16,359 --> 00:03:20,293
In addition to math, we could do something like a select or filter.

68
00:03:20,293 --> 00:03:21,173
In this case,

69
00:03:21,173 --> 00:03:23,134
we could select information from the input and then

70
00:03:23,134 --> 00:03:25,225
output it to the outgoing message.

71
00:03:25,225 --> 00:03:25,956
Alternatively,

72
00:03:25,956 --> 00:03:29,484
we could filter based on the attributes inside of the message to

73
00:03:29,484 --> 00:03:32,629
determine whether or not we allow it to pass through.

74
00:03:32,629 --> 00:03:34,211
We can do things like look at device shadow activity,

75
00:03:34,211 --> 00:03:38,811
so that would be going to the device shadow itself and pulling

76
00:03:38,811 --> 00:03:41,076
information from that to enrich the data coming in.

77
00:03:41,076 --> 00:03:41,586
And finally,

78
00:03:41,586 --> 00:03:45,772
we could do something like adding and removing attributes themselves.

79
00:03:45,772 --> 00:03:48,415
So we could add our own custom attributes or simply say we

80
00:03:48,415 --> 00:03:50,459
didn't need some attributes that were coming in anymore

81
00:03:50,459 --> 00:03:52,884
and discard them completely.

82
00:03:52,884 --> 00:03:55,606
So this is what we can do in this pipeline stage here.

83
00:03:55,606 --> 00:03:59,023
But after the data moves through the channel towards a pipeline,

84
00:03:59,023 --> 00:04:02,893
it's going to need to eventually end up in a data store.

85
00:04:02,893 --> 00:04:05,777
Now there's a little bit more configuration for data stores as well.

86
00:04:05,777 --> 00:04:06,002
Specifically,

87
00:04:06,002 --> 00:04:09,079
we need to pick how long we want the data to be

88
00:04:09,079 --> 00:04:10,806
retained inside of this data store.

89
00:04:10,806 --> 00:04:15,189
Is this an infinite amount of time or potentially only 90 days?

90
00:04:15,189 --> 00:04:18,538
These configurations do allow us to make sure we're hitting potential legal

91
00:04:18,538 --> 00:04:22,236
requirements or just company policies around our data retention.

92
00:04:22,236 --> 00:04:26,004
So after we've taken a look at all of these different configuration options,

93
00:04:26,004 --> 00:04:28,222
what happens at the point where we're actually done

94
00:04:28,222 --> 00:04:32,672
taking the data in through an IoT rule, an IoT Analytics channel,

95
00:04:32,672 --> 00:04:35,671
sending it over an IoT Analytics pipeline into a data store?

96
00:04:35,671 --> 00:04:39,426
How do we actually take some action on that data and get some insight from it?

97
00:04:39,426 --> 00:04:43,836
Well this is where the next part of the IoT Analytics chain comes into play.

98
00:04:43,836 --> 00:04:46,344
The analyze section will allow us to take a data

99
00:04:46,344 --> 00:04:48,434
store and turn it into a data set.

100
00:04:48,434 --> 00:04:51,253
Now what could we do with these data sets?

101
00:04:51,253 --> 00:04:52,911
Well when we have data sets,

102
00:04:52,911 --> 00:04:56,231
we can configure them with Amazon SageMaker Notebooks.

103
00:04:56,231 --> 00:04:58,090
Now this is going to allow us to run a variety of

104
00:04:58,090 --> 00:04:59,929
different code on the data we have,

105
00:04:59,929 --> 00:05:04,867
including Python code inside of Jupiter Notebooks and other statistical and

106
00:05:04,867 --> 00:05:09,220
programming languages that we could run on these data sets.

107
00:05:09,220 --> 00:05:12,879
Now what does the configuration look like for data sets in Notebooks?

108
00:05:12,879 --> 00:05:14,580
Let's take a look at that now.

109
00:05:14,580 --> 00:05:18,285
When we're doing this, the data sets are going to allow us to set up a SQL query,

110
00:05:18,285 --> 00:05:21,906
so essentially this is going to pick the data store and

111
00:05:21,906 --> 00:05:24,956
potential transformations that we could run on the data store

112
00:05:24,956 --> 00:05:27,095
information and turn it into a data set.

113
00:05:27,095 --> 00:05:31,364
It's also going to determine a schedule with which to run the SQL

114
00:05:31,364 --> 00:05:34,855
query and output a data set from the data store.

115
00:05:34,855 --> 00:05:37,393
So this essentially is going to refresh the data periodically

116
00:05:37,393 --> 00:05:41,000
as it continues to stream in to that data store and provide

117
00:05:41,000 --> 00:05:43,533
us with additional information.

118
00:05:43,533 --> 00:05:43,830
After that,

119
00:05:43,830 --> 00:05:46,362
we can set a retention period for how long we want to keep these

120
00:05:46,362 --> 00:05:49,285
data sets around after we've created them.

121
00:05:49,285 --> 00:05:51,119
We might not want them to get stale.

122
00:05:51,119 --> 00:05:53,279
And in addition to the data set configuration,

123
00:05:53,279 --> 00:05:56,234
we have configuration we can set for notebooks,

124
00:05:56,234 --> 00:05:59,341
and the notebooks are going to allow us to take a look at

125
00:05:59,341 --> 00:06:01,651
a variety of different templates that will allow us to

126
00:06:01,651 --> 00:06:05,428
accomplish different things, depending on what we need the notebook to do.

127
00:06:05,428 --> 00:06:07,202
We'll take a look at that in just a moment.

128
00:06:07,202 --> 00:06:10,476
But the other piece of configuration is, of course, the input data set.

129
00:06:10,476 --> 00:06:12,908
We can take whatever data sets we create and pick which

130
00:06:12,908 --> 00:06:16,374
ones to feed into our notebooks for additional analysis and

131
00:06:16,374 --> 00:06:18,766
visualizations down the line.

132
00:06:18,766 --> 00:06:21,226
So now let's go ahead and take a look at what we can actually do with

133
00:06:21,226 --> 00:06:25,043
some of the prebuilt templates for SageMaker Notebooks.

134
00:06:25,043 --> 00:06:27,349
One of the first and most common possibilities is to

135
00:06:27,349 --> 00:06:29,458
do some sort of anomaly detection.

136
00:06:29,458 --> 00:06:32,713
You'll want to know if there's something weird happening with all of your IoT

137
00:06:32,713 --> 00:06:35,243
devices and this is a perfect way to start checking for that.

138
00:06:35,243 --> 00:06:37,659
You'd also do some output forecasting.

139
00:06:37,659 --> 00:06:38,282
So for example,

140
00:06:38,282 --> 00:06:42,330
if you see some sort of trend inside of the data that you'd like to forecast,

141
00:06:42,330 --> 00:06:45,315
you can do that with SageMaker Notebooks.

142
00:06:45,315 --> 00:06:46,813
There are also templates for predictive maintenance,

143
00:06:46,813 --> 00:06:51,614
if you have maybe a fleet of machines that periodically breaks down,

144
00:06:51,614 --> 00:06:53,767
you want to be able to see which of those machines might break down

145
00:06:53,767 --> 00:06:57,788
soon so that you can call it in early and try and replace faulty

146
00:06:57,788 --> 00:07:00,762
parts or give it preventative maintenance.

147
00:07:00,762 --> 00:07:03,646
Another notebook template would be to use customer segmentation.

148
00:07:03,646 --> 00:07:07,257
For example, if you have a bunch of different varieties of customers,

149
00:07:07,257 --> 00:07:09,993
maybe grouping them together would allow you to target them

150
00:07:09,993 --> 00:07:12,398
more effectively or provide them with products and services

151
00:07:12,398 --> 00:07:14,918
that they'd rather use and see.

152
00:07:14,918 --> 00:07:15,155
After that,

153
00:07:15,155 --> 00:07:17,359
maybe you could do some congestion forecasting and see

154
00:07:17,359 --> 00:07:20,689
whether or not your product was likely to hit its peak

155
00:07:20,689 --> 00:07:23,431
capacity or something along those lines.

156
00:07:23,431 --> 00:07:25,823
Now these are just a few specific examples of templates,

157
00:07:25,823 --> 00:07:29,393
but the power of Amazon SageMaker Notebooks is that you can write

158
00:07:29,393 --> 00:07:33,348
whatever custom code you'd like and you can also use a variety of

159
00:07:33,348 --> 00:07:36,999
common machine learning and data science libraries to power all the

160
00:07:36,999 --> 00:07:39,917
insights that you're trying to generate.

161
00:07:39,917 --> 00:07:41,794
So hopefully now you can see some of the major

162
00:07:41,794 --> 00:07:43,807
benefits that we get from IoT Analytics,

163
00:07:43,807 --> 00:07:46,896
allowing us to not only ingest data from IoT rules,

164
00:07:46,896 --> 00:07:51,126
but to also configure retention periods to determine whether or not we'd like

165
00:07:51,126 --> 00:07:56,237
to transform or enrich our data with the IoT Analytics pipelines and also to

166
00:07:56,237 --> 00:08:04,000
eventually run machine learning algorithms on top of the data sets we create to get more sophisticated insights.

