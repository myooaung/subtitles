1
00:00:00,390 --> 00:00:02,940
Hi guys welcome to this editorial.

2
00:00:02,940 --> 00:00:08,780
We've just built our convolution all new network from start to finish with tons flow 2.0.

3
00:00:08,850 --> 00:00:10,930
So now it is time to train it.

4
00:00:10,950 --> 00:00:14,580
We are already loaded and pre processed the data set.

5
00:00:14,670 --> 00:00:18,110
So everything is ready to be connected together.

6
00:00:18,120 --> 00:00:23,160
And speaking of connections well the first thing we have to do is of course compile the moral meaning

7
00:00:23,160 --> 00:00:26,980
to connect it to an optimizer and a loss.

8
00:00:27,060 --> 00:00:33,450
The optimizer is the tool that will performs to Cassie gradient descent to date the weights in order

9
00:00:33,450 --> 00:00:40,380
to reduce the loss and the loss is indeed the error between the predictions of the model you know which

10
00:00:40,470 --> 00:00:47,160
objects it predicts the images to be and the real values that are of course in the training set.

11
00:00:47,160 --> 00:00:48,790
You know why train.

12
00:00:48,790 --> 00:00:49,040
Right.

13
00:00:49,050 --> 00:00:54,150
Because when we're training we have the real values which we are comparing to the predictions to incur

14
00:00:54,150 --> 00:00:59,400
the loss and back propagated into the new network in order to finally update the weights with the Cassie

15
00:00:59,400 --> 00:01:00,600
gradient descent.

16
00:01:00,600 --> 00:01:06,810
And once again we're gonna use an Adam optimizer it is one of the best stochastic greenness and optimizer.

17
00:01:06,870 --> 00:01:09,310
So this is the one I recommend by default.

18
00:01:09,450 --> 00:01:14,250
And as for the less well we're going to use the same one as before the sports category called Cross

19
00:01:14,250 --> 00:01:15,110
entropy.

20
00:01:15,150 --> 00:01:15,630
Right.

21
00:01:15,630 --> 00:01:22,500
In fact this is the exact same way to compile then with our artificial new network.

22
00:01:22,510 --> 00:01:28,950
Remember we used an Adam optimizer as poor scanner equal crossings would be less and poor as categorical

23
00:01:29,010 --> 00:01:34,430
accuracy metrics which is the same metrics we use here for a convolution or new network.

24
00:01:34,440 --> 00:01:37,730
And that's because the application is more or less the same.

25
00:01:37,740 --> 00:01:43,430
You know we're doing image classification on more than two glasses so it's not binary classification.

26
00:01:43,560 --> 00:01:45,280
It's multi class classification.

27
00:01:45,420 --> 00:01:49,070
And that's why we had to use here sports categorical accuracy.

28
00:01:49,140 --> 00:01:54,400
And for those you who want more details on this sparse categorical accuracy.

29
00:01:54,450 --> 00:01:59,860
Well I provided some extra source here where you can find some extra explanations.

30
00:01:59,970 --> 00:02:06,930
But what's most important to remember is that this is the metrics you should use when doing multi class

31
00:02:06,930 --> 00:02:07,700
classification.

32
00:02:07,700 --> 00:02:11,550
You know when predicting more than two classes and so there you go.

33
00:02:11,550 --> 00:02:17,760
This is exactly the same as before you call the compile method from your model and you will put the

34
00:02:17,760 --> 00:02:20,100
less the optimizer and the metrics.

35
00:02:20,100 --> 00:02:25,810
And this is the exact same set of inputs as are artificial neural network.

36
00:02:25,830 --> 00:02:26,450
All right.

37
00:02:26,490 --> 00:02:32,320
So now that we connected our moral to an optimizer loss in a matrix.

38
00:02:32,460 --> 00:02:35,550
Well the next step is naturally to train it.

39
00:02:35,610 --> 00:02:42,930
Training the model so to train the model it's once again exactly the same as the artificial neural network

40
00:02:43,260 --> 00:02:45,330
we're gonna use the fit method.

41
00:02:45,330 --> 00:02:50,630
Remember where you input your input images contained in exchange.

42
00:02:50,730 --> 00:02:57,480
The real answer is you know the real classes of the images and extreme in white train and the number

43
00:02:57,480 --> 00:03:02,970
of bugs meaning the number of times you want to feed the whole training set into the neural network

44
00:03:02,970 --> 00:03:04,920
and update each time the weights.

45
00:03:04,920 --> 00:03:11,330
And we chose five epochs for a and then and now for a CNN it's exactly the same five books.

46
00:03:11,520 --> 00:03:17,310
And once again feel free to choose a batch size if you want to do mini batch learning it's usually recommended

47
00:03:17,640 --> 00:03:24,270
but you know remember that you had the exercise then which will have you explore some more techniques

48
00:03:24,330 --> 00:03:25,840
in order to improve the results.

49
00:03:25,860 --> 00:03:28,670
And one of them is by including a batch size.

50
00:03:28,770 --> 00:03:30,390
So let's do this.

51
00:03:30,450 --> 00:03:38,880
Let's train the model by executing this well what do we get we get are five epics with for each the

52
00:03:38,880 --> 00:03:40,890
loss and the accuracy.

53
00:03:40,920 --> 00:03:47,010
So here it's interesting because remember before we got some pretty good results at the very first epic

54
00:03:47,310 --> 00:03:50,350
but now the problem is much more complex.

55
00:03:50,350 --> 00:03:57,090
Now the relationships between the different convolution or layers inside our CNN are much more complex

56
00:03:57,360 --> 00:04:00,480
than inside just some fully connected layers in the end.

57
00:04:00,600 --> 00:04:07,260
So that's why in the first book we get a low accuracy but what's interesting to see is that it quickly

58
00:04:07,350 --> 00:04:12,000
improves you know the accuracy almost doubles at the second book.

59
00:04:12,030 --> 00:04:19,500
Right we go from 28 percent to 44 percent and then it improves once again pretty well 51 percent 56

60
00:04:19,500 --> 00:04:21,610
percent and 60 percent.

61
00:04:21,660 --> 00:04:26,260
So it's pretty obvious of how you can improve the results.

62
00:04:26,340 --> 00:04:26,760
Right.

63
00:04:26,850 --> 00:04:32,810
I'm sure you're thinking right now that one solution of improvement is to try with more epochs.

64
00:04:32,830 --> 00:04:36,330
Of course if you try with more epics you should get a higher accuracy.

65
00:04:36,330 --> 00:04:42,030
So this will be part of the exercises and then you know you have many more options to improve the results.

66
00:04:42,120 --> 00:04:47,520
You can change the architecture you can at drop out you can add more tools you'll actually get to experience

67
00:04:47,520 --> 00:04:55,260
on this in the practical activities and remember that okay we improved the accuracy over these five

68
00:04:55,410 --> 00:05:00,750
books and even if we got a final 60 percent accuracy which is quite decent.

69
00:05:00,800 --> 00:05:01,760
Well don't forget that.

70
00:05:01,910 --> 00:05:03,740
This is not what we're looking for.

71
00:05:03,830 --> 00:05:10,850
Ultimately what we're looking for ultimately is to get a high accuracy on the evaluation set you know

72
00:05:10,880 --> 00:05:17,570
because the evaluation set contains the new observations meaning new images on which our model wasn't

73
00:05:17,570 --> 00:05:18,250
trained.

74
00:05:18,380 --> 00:05:21,990
And so that's exactly the last part of this notebook.

75
00:05:22,100 --> 00:05:30,080
Evaluating the morale and you evaluate it exactly the same way as before by calling the Evaluate method

76
00:05:30,080 --> 00:05:32,890
from your moral object which takes as input.

77
00:05:32,990 --> 00:05:40,360
The test set composed of all the input images of the test in excess and they're real classes in white

78
00:05:40,360 --> 00:05:40,960
test.

79
00:05:41,110 --> 00:05:47,360
And by executing this this will get you the accuracy and also the loss on the test set.

80
00:05:47,360 --> 00:05:52,550
That's why we introduce these two variables here because this evaluate method actually returns the loss

81
00:05:52,580 --> 00:05:56,570
and the accuracy but it also outputs them this way.

82
00:05:56,560 --> 00:06:04,730
You know by running the model on these 10000 observations and we get the final accuracy of 59 percent

83
00:06:05,210 --> 00:06:10,790
we could have feared that we would get lower accuracy than here because actually our model didn't have

84
00:06:10,790 --> 00:06:16,880
much time to train first of all and second of all no drop out was applied but still we get a good accuracy

85
00:06:16,880 --> 00:06:24,140
here but there is still much much room for improvement so I want you to try improving this will guide

86
00:06:24,140 --> 00:06:26,390
you on how to do this through the exercises.

87
00:06:26,780 --> 00:06:32,810
But of course if you think of other extra solutions which largely improves the accuracy.

88
00:06:32,810 --> 00:06:38,230
Well feel free to share your solutions and your results we'll be happy to see them once again.

89
00:06:38,240 --> 00:06:42,530
This will help the other students in this course and that will make the course even greater.

90
00:06:42,560 --> 00:06:43,400
So there you go.

91
00:06:43,430 --> 00:06:45,560
And then we have the final print.

92
00:06:45,590 --> 00:06:51,200
You know when you only want the accuracy and not the loss you know loss is not really the way to evaluate

93
00:06:51,200 --> 00:06:54,140
the performance of your morals so you just want the accuracy.

94
00:06:54,140 --> 00:06:58,780
Well here is the final prints with indeed the same test accuracy.

95
00:06:58,780 --> 00:06:59,420
All right.

96
00:06:59,420 --> 00:07:01,550
So congratulations.

97
00:07:01,550 --> 00:07:06,860
Now you know how to build a convolution or new one that work intensive low 2.0.

98
00:07:06,860 --> 00:07:08,690
Now it's time for you to practice.

99
00:07:08,690 --> 00:07:15,350
It's time for you to really improve this test accuracy even if it is quite decent but I left some room

100
00:07:15,350 --> 00:07:19,510
for improvement on purpose for you to succeed on improving this.

101
00:07:19,640 --> 00:07:26,090
And once again share your results share your solutions communicate and then you will generate ideas

102
00:07:26,090 --> 00:07:30,390
between each other and will probably improve your skills even greater.

103
00:07:30,470 --> 00:07:32,810
So good luck on the exercises.

104
00:07:32,810 --> 00:07:39,980
I'll see you in the next section for third kind of new network which will be a recurrent neural network

105
00:07:40,340 --> 00:07:43,010
for which the application will be much different.

106
00:07:43,010 --> 00:07:45,740
It will be an MLP application.

107
00:07:45,740 --> 00:07:48,320
I'll let you find out about this in the next section.
