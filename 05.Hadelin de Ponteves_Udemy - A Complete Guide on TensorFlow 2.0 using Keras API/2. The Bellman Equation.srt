1
00:00:01,010 --> 00:00:04,400
Hello and welcome back to the course on artificial intelligence.

2
00:00:04,400 --> 00:00:07,550
Today we're going to talk about the Belmont equation.

3
00:00:07,550 --> 00:00:11,890
It's quite a complex topic and we're going to introduce it in a step by step manner.

4
00:00:11,900 --> 00:00:16,730
Throughout this whole section of the course so we're not going to just jump straight into the most complex

5
00:00:16,730 --> 00:00:20,990
version of the Belmont equation right away but instead we're going to introduce it slowly in order to

6
00:00:20,990 --> 00:00:23,210
gradually understand how it works.

7
00:00:23,360 --> 00:00:26,630
And I hope you call off that approach if you're if you are.

8
00:00:26,630 --> 00:00:28,190
Let's get straight into it.

9
00:00:28,640 --> 00:00:33,770
So we're going to have a couple of key concepts that we're going to be operating with and these concepts

10
00:00:33,770 --> 00:00:36,100
are S stands for states.

11
00:00:36,110 --> 00:00:42,980
So the state in which our agent is or any other possible state in which it can be a represents an action

12
00:00:42,980 --> 00:00:45,440
that a an agent can take.

13
00:00:45,440 --> 00:00:50,630
So an agent can have access to a certain list of actions and actions are very important when they're

14
00:00:50,630 --> 00:00:53,570
looked at in a state combination.

15
00:00:53,570 --> 00:00:57,800
So when you're in a certain state and then you look at actions and it starts to make sense what's going

16
00:00:57,800 --> 00:01:01,820
to be the result of those actions because if you look at an action by itself of a states does it really

17
00:01:01,820 --> 00:01:05,440
make sense because you don't know where you are and where you can possibly end up.

18
00:01:06,410 --> 00:01:08,920
Then we have we'll have AAV which stands for reward.

19
00:01:08,930 --> 00:01:16,940
And that's the reward the agent gets for entering into a certain state and Gamma is the discount factor.

20
00:01:16,940 --> 00:01:19,310
And we'll talk about the discount factor in a second all make sense.

21
00:01:19,310 --> 00:01:21,760
Just now but just take notes.

22
00:01:21,770 --> 00:01:25,820
Make a mental note that we are going to have this letter a gamble that we'll be operating with later

23
00:01:25,820 --> 00:01:25,940
on.

24
00:01:26,540 --> 00:01:31,280
So the person behind the Belmont equation is Richard Ernest Bellman.

25
00:01:31,310 --> 00:01:39,050
He was a applied mathematician and came up with the concept of dynamic programming which we are now

26
00:01:39,110 --> 00:01:42,950
which we now call reinforcement learning or which we call the Belmont equation.

27
00:01:42,950 --> 00:01:43,730
Now in.

28
00:01:44,060 --> 00:01:46,820
Well that's what we call now in 1953.

29
00:01:46,820 --> 00:01:52,560
He came up with that concept and that's when the Belmont Belmont equation came to me.

30
00:01:52,580 --> 00:01:56,390
So let's have a look at how this all works.

31
00:01:56,470 --> 00:02:00,850
There's our lovely agent in the bottom left corner and he is in a maze.

32
00:02:00,980 --> 00:02:06,320
And this is quite a classical maze where you've got some blocks the white blocks are blocks in which

33
00:02:06,380 --> 00:02:12,950
the agent can step into the gray block is the one one that is just not accessible says like a wall in

34
00:02:12,950 --> 00:02:19,370
this maze the green is where the agent is should be aiming to end up in and that's where we want the

35
00:02:19,370 --> 00:02:20,950
agent to go that's the finish.

36
00:02:21,140 --> 00:02:26,860
And the red is a firepit so if the agent falls into the carpet he will lose the game.

37
00:02:26,870 --> 00:02:31,280
So in the firepit the reward which is are is minus one.

38
00:02:31,280 --> 00:02:36,280
So that's our way of telling the agent that's not something we want you to do.

39
00:02:36,380 --> 00:02:41,090
Like remembering the example of when we're training dogs we want to tell them like Bad Dog if if it's

40
00:02:41,090 --> 00:02:45,470
not doing the right thing that we wanted to do something here we want to tell the agent that this is

41
00:02:45,470 --> 00:02:49,160
not something that you should be doing it shouldn't be ending up in the square so every time it doesn't

42
00:02:49,160 --> 00:02:53,290
up in the square you'll get a minus one reward so we'll be punished with a minus one reward.

43
00:02:53,480 --> 00:02:57,560
On the other hand if it ends up in the Green Square it will get a plus one reward meaning that that

44
00:02:57,560 --> 00:02:59,500
is what we wanted to do.

45
00:02:59,510 --> 00:03:02,360
So those are the two rewards that the agent can possibly get.

46
00:03:02,420 --> 00:03:08,600
And how does it learn how to operate in this maze just like in that example of the robot dogs that learn

47
00:03:08,600 --> 00:03:09,260
to walk.

48
00:03:09,260 --> 00:03:12,430
We're just going to let it not we'll just tell it that here are the actions you can do.

49
00:03:12,440 --> 00:03:14,570
You can go up right left or down.

50
00:03:14,570 --> 00:03:18,260
Those are the four possible actions that you can take and that's it.

51
00:03:18,260 --> 00:03:21,280
How have a play around with that see what you can come up with.

52
00:03:21,350 --> 00:03:26,240
So the agent might go to the right then they might go to more to the right they might go back to the

53
00:03:26,240 --> 00:03:30,680
left they're just randomly pressing his button and they're trying to see what happens and they go back

54
00:03:30,680 --> 00:03:34,580
here they go up go up go down go up go right.

55
00:03:34,580 --> 00:03:38,390
So for now they haven't learned anything they just so far nothing's happened.

56
00:03:38,390 --> 00:03:41,650
They go right and then bam they end up in the Green Square.

57
00:03:41,780 --> 00:03:45,470
So they realize wow I just got a plus one reward.

58
00:03:45,470 --> 00:03:51,350
So as soon as I stepped into the Green Square they got a plus one reward and that triggers the algorithm

59
00:03:51,350 --> 00:03:53,290
to say OK that's really cool.

60
00:03:53,780 --> 00:03:56,410
I am rewarded for ending up in the square.

61
00:03:56,410 --> 00:03:58,860
So I want to end up in the square.

62
00:03:58,880 --> 00:04:00,720
So what does that mean for the agent.

63
00:04:00,860 --> 00:04:04,230
That means it starts ask the question how did I get to the square.

64
00:04:04,220 --> 00:04:10,340
What was the preceding state I was in and what action that I take to get at the square and then it looks

65
00:04:10,340 --> 00:04:11,570
back and says OK.

66
00:04:11,600 --> 00:04:14,900
So the preceding stage was this one.

67
00:04:14,900 --> 00:04:17,320
It turns out to be valuable in that state.

68
00:04:17,330 --> 00:04:21,170
The one that spoke with the Red Arrow because from that state you're I'm.

69
00:04:21,240 --> 00:04:29,240
I'm just one step away from getting the maximum reward I can possibly dream of of Plus One like a biscuit

70
00:04:29,240 --> 00:04:35,320
for a dog from as soon as I know if I ever am in that state that square marked with a red arrow all

71
00:04:35,330 --> 00:04:36,960
we'll have to do is press right.

72
00:04:36,980 --> 00:04:43,400
So how do I tell myself how to remember that that state is valuable Well to me there's no difference

73
00:04:43,400 --> 00:04:45,090
actually as the agent.

74
00:04:45,090 --> 00:04:50,300
There's no difference in whether I am in the Green Square or in the white square right in the Green

75
00:04:50,300 --> 00:04:51,560
Square I get the reward of one.

76
00:04:51,590 --> 00:04:57,920
So I'm going to mark for myself that the white square is got for me it has the value of one because

77
00:04:58,040 --> 00:05:03,290
it leads exactly to reward 1 so as soon as I'm in the white square I know I'll just take one more action.

78
00:05:03,320 --> 00:05:05,360
I'll be in the Green Square and I'll get a reward of one.

79
00:05:05,360 --> 00:05:11,320
So that's why I'm going to say that the value of this square is equal to one because it leads directly

80
00:05:11,320 --> 00:05:14,250
off on any sort of subtractions.

81
00:05:14,300 --> 00:05:18,510
As soon as I mean here I know my reward will be once I'm going to mark this square as equal to one.

82
00:05:18,530 --> 00:05:22,280
That's the value that's the perceived value of being in this state.

83
00:05:22,370 --> 00:05:26,840
Next the agent is going to be like OK so how did I get into this square.

84
00:05:26,990 --> 00:05:32,030
And you know he might walk around again and so on and up in the square again and be like Okay how did

85
00:05:32,030 --> 00:05:33,570
I get into this square before that.

86
00:05:33,740 --> 00:05:36,820
And the way I got into this square was from this square.

87
00:05:36,830 --> 00:05:37,460
Interesting.

88
00:05:37,510 --> 00:05:37,750
Okay.

89
00:05:37,780 --> 00:05:42,930
So as soon as I get into this square I know that all I have to do is go right.

90
00:05:42,950 --> 00:05:45,600
And then from here I already know that I'm going to win.

91
00:05:45,620 --> 00:05:49,910
I know exactly how everything is gonna unravel from here and I know the value of being in this state

92
00:05:49,910 --> 00:05:50,960
is equal to 1.

93
00:05:50,960 --> 00:05:56,370
And since there is no nothing is stopping me from running it from here to here.

94
00:05:56,420 --> 00:05:59,450
The value in this is going to the perceived value.

95
00:05:59,450 --> 00:06:04,970
I'm going to value being in here as equal to one as well because this is I mean here I know be here

96
00:06:05,000 --> 00:06:06,680
and I'll be here pretty quickly.

97
00:06:06,680 --> 00:06:07,970
So I'm going to win.

98
00:06:08,150 --> 00:06:10,460
And then how do I get into this square before that.

99
00:06:10,460 --> 00:06:16,850
Well I got into the square from this square so the value is similar approach the value of being here

100
00:06:16,880 --> 00:06:22,160
is also equal to one and so on so the value of being here is equal to one value of being here is equal

101
00:06:22,160 --> 00:06:26,130
time because each one of them leads to the next one and leads to the finish line.

102
00:06:26,180 --> 00:06:29,890
So that's all like pretty logical at this stage.

103
00:06:29,930 --> 00:06:33,350
This is us pretty much designing the Belmont equation right now.

104
00:06:33,350 --> 00:06:40,370
So this is we could possibly think about designing an equation that helps an agent go through the maze.

105
00:06:40,430 --> 00:06:43,370
So look at the reward than the preceding state.

106
00:06:43,370 --> 00:06:47,360
Give it a value of equal to reward the preceding sentence also that kind of like creates this pathway

107
00:06:48,140 --> 00:06:55,280
is all great and well but the problem here is OK what happens if our agent for some reason starts in

108
00:06:55,280 --> 00:07:00,500
this state instead of starting here and taking these actions and it actually starts in the state.

109
00:07:00,560 --> 00:07:04,250
How does it know how does it remember which action to take.

110
00:07:04,250 --> 00:07:08,470
Should it go right or should it go down or should maybe go left or should go up.

111
00:07:08,470 --> 00:07:09,670
How does it remember.

112
00:07:09,740 --> 00:07:13,090
Which is the next continuation from here.

113
00:07:13,160 --> 00:07:18,590
If the only values it has is these values of equal to answer you cannot see what's further away.

114
00:07:18,590 --> 00:07:19,590
It can only see.

115
00:07:19,640 --> 00:07:19,940
All right.

116
00:07:19,940 --> 00:07:21,860
What I have here and what I have here.

117
00:07:21,920 --> 00:07:23,600
How does it know which way to go.

118
00:07:23,600 --> 00:07:27,820
Well at this stage it doesn't it's as pre identical for the agent which way to go.

119
00:07:27,950 --> 00:07:30,650
And so that's why this approach doesn't really work.

120
00:07:30,730 --> 00:07:32,840
It's a very very simplistic explanation.

121
00:07:32,840 --> 00:07:34,450
Of course there's much more to it.

122
00:07:34,460 --> 00:07:36,060
But in an intuitive way.

123
00:07:36,140 --> 00:07:41,690
That's why we cannot just assign just carry on this value backwards like that because one of the reasons

124
00:07:41,690 --> 00:07:47,120
is once the agent is in between these two values which whereas you're going to go it doesn't it can

125
00:07:47,120 --> 00:07:48,050
get confused like that.

126
00:07:48,560 --> 00:07:51,020
And so how do we solve this problem.

127
00:07:51,020 --> 00:07:52,280
What are we going to do here.

128
00:07:52,340 --> 00:07:57,800
And this is where we're going to start introducing the Belmont equation in its actual form slowly step

129
00:07:57,800 --> 00:07:58,130
by step.

130
00:07:58,610 --> 00:08:01,580
So the Belmont equation looks something like this.

131
00:08:01,580 --> 00:08:07,040
So we've already talked about the value of being in a certain state as is your current state or any

132
00:08:07,040 --> 00:08:08,030
given state.

133
00:08:08,240 --> 00:08:10,310
And there is s as well.

134
00:08:10,310 --> 00:08:16,770
And s prime is the state the following state the state that you will end up in after this state.

135
00:08:16,940 --> 00:08:18,670
And by taking concerted action.

136
00:08:18,890 --> 00:08:24,190
But we know that there's many actions that a agent can take and that's why we've got this Max over here.

137
00:08:24,200 --> 00:08:27,170
So by taking an action what will happen to an agent.

138
00:08:27,200 --> 00:08:33,410
So let's say we're in State s by taking an action in state s and we take action a what will happen is

139
00:08:33,410 --> 00:08:36,600
will instantly get a reward by getting into a new state.

140
00:08:36,710 --> 00:08:41,930
And remember that reward can be won or plus 1 or minus 1 if it's at the end of the game where it can

141
00:08:41,930 --> 00:08:43,580
be a zero if it's throughout the game.

142
00:08:43,580 --> 00:08:46,160
In this case our reward throughout the game is zero.

143
00:08:46,220 --> 00:08:47,840
So that that's the reward.

144
00:08:47,840 --> 00:08:55,130
Plus we will get into a new state which has value of as prime.

145
00:08:55,130 --> 00:08:57,770
So that's the value of the new state and gamma.

146
00:08:57,770 --> 00:09:02,150
We'll talk about that in a second but the point I'm trying to raise here or the point I'm raising here

147
00:09:02,150 --> 00:09:05,750
is that we've got many different actions that we can take and that's why we've got the maximum.

148
00:09:05,750 --> 00:09:09,680
So by taking action we get a reward plus we end up in a new state.

149
00:09:09,680 --> 00:09:14,780
And so for every out of though in our case for our possible actions for every one of the possible four

150
00:09:14,780 --> 00:09:17,750
actions we're going to have a equation like this.

151
00:09:17,750 --> 00:09:23,420
So this is going to have a value for their will have different value for everyone and for actions.

152
00:09:23,450 --> 00:09:28,730
And we're going to look at only the maximum because of course the agent wants to take the optimal state.

153
00:09:28,760 --> 00:09:33,820
So if he's in state s he's going to look at these values he's going to find the maximum based on the

154
00:09:33,820 --> 00:09:37,410
action and going to take that action that leads to the maximum of these values.

155
00:09:37,580 --> 00:09:41,410
So hopefully that makes sense why we're taking a maximum here.

156
00:09:41,570 --> 00:09:45,580
Then once we got the reward and the value the state why do we have this gamma parameter here.

157
00:09:45,590 --> 00:09:52,160
Well it's there exactly to solve that problem of where the agent doesn't know which way to go because

158
00:09:52,160 --> 00:09:56,370
it cannot it's comparing the values of two states on the on both sides and they're the same.

159
00:09:56,750 --> 00:10:00,830
That's why the game is the discounting factor so we're going to have a look at that in now to better

160
00:10:00,830 --> 00:10:01,620
understand.

161
00:10:02,000 --> 00:10:04,680
So let's take our formula we'll put it here on the top right.

162
00:10:04,730 --> 00:10:11,390
And now we will analyze what the values of the different states are and every state here is a square

163
00:10:11,390 --> 00:10:11,750
and army.

164
00:10:11,780 --> 00:10:16,640
So one of these any one of these white squares is a state and we were going to calculate the value of

165
00:10:16,640 --> 00:10:18,230
being in that state.

166
00:10:18,230 --> 00:10:19,430
So let's start with the square.

167
00:10:19,730 --> 00:10:21,800
What is the value of being in this state.

168
00:10:21,800 --> 00:10:28,430
Well we need to take the maximum of this value across elections and we know that this value represents

169
00:10:28,700 --> 00:10:31,100
is maximized as we get closer to the finish line.

170
00:10:31,100 --> 00:10:35,690
That's how it's constructed and by just by looking at you can see because here has got the reward.

171
00:10:35,690 --> 00:10:40,910
And here has got a just counting factor multiplied by the value of the next state.

172
00:10:41,000 --> 00:10:44,770
And it just makes sense that that's how we would construct that equation.

173
00:10:44,770 --> 00:10:50,150
So it makes sense that from here the maximum of this value will be if we move to the right.

174
00:10:50,300 --> 00:10:56,060
So that's how we calculate the value of a state this value of this state is equals the maximum or equals

175
00:10:56,210 --> 00:10:57,430
to this value.

176
00:10:57,440 --> 00:10:59,090
If we move to the right.

177
00:10:59,090 --> 00:11:00,960
If we take that action of moving to the right.

178
00:11:00,980 --> 00:11:02,270
So what will this value be.

179
00:11:02,270 --> 00:11:04,960
Well the reward of moving to the right is equal to one.

180
00:11:05,030 --> 00:11:07,670
And regardless what comment gamma is.

181
00:11:07,670 --> 00:11:11,390
We don't have a value in the state because we are already in the best state possible.

182
00:11:11,660 --> 00:11:12,830
So this is the final state.

183
00:11:12,830 --> 00:11:16,210
It won't have a value we just get a reward here and that's the end of the game.

184
00:11:16,220 --> 00:11:20,430
So the value will be of this maximum will be equal to one.

185
00:11:20,480 --> 00:11:23,810
And that's why value of state as here is equal to 1.

186
00:11:23,810 --> 00:11:27,590
Now things get interesting when we move to the left when we move backwards a bit.

187
00:11:27,950 --> 00:11:34,000
So now let's calculate the value of this of being in this state and for that we're going to need karma.

188
00:11:34,010 --> 00:11:36,730
So let's say our discounting factor is zero point nine.

189
00:11:37,400 --> 00:11:38,810
And it all makes sense.

190
00:11:38,870 --> 00:11:40,910
What a discounting factor is once we calculate that.

191
00:11:40,910 --> 00:11:46,880
So from here just based on our intuition and based because we know how this maze is working how this

192
00:11:46,880 --> 00:11:51,460
maze works we know that the best possible action is go to the right because from here we go here.

193
00:11:51,470 --> 00:11:56,100
So that means a maximum will be achieved when in this state you go to the right.

194
00:11:56,210 --> 00:11:58,900
And so let's see what happens if we plug it in here.

195
00:11:58,910 --> 00:12:03,800
So if you go from here to here you don't getting your rewards will be a zero but then you'll get gamblers

196
00:12:03,830 --> 00:12:07,560
who get zero point nine times the value of the new state which has won.

197
00:12:07,580 --> 00:12:13,610
So in this case the value the hold result of this is 1 times or zero point nine times one equals zero

198
00:12:13,610 --> 00:12:14,000
point nine.

199
00:12:14,000 --> 00:12:15,520
So that's our values.

200
00:12:16,190 --> 00:12:20,990
So if we calculate this now you'll see that from here we know just by looking at the maze we know because

201
00:12:21,080 --> 00:12:27,230
we as humans because we're understanding how this equation works of course an A.I. the agent would have

202
00:12:27,230 --> 00:12:28,390
to experiment with these things.

203
00:12:28,400 --> 00:12:32,090
But because we have like a crystal ball we can see this whole maze.

204
00:12:32,090 --> 00:12:33,780
We have the bird's eye view right now.

205
00:12:33,800 --> 00:12:36,250
We know that the best action is going to go to the right.

206
00:12:36,260 --> 00:12:42,170
So if we plug it all in here it'll be zero no reward plus the report 9 times the value in the state

207
00:12:42,170 --> 00:12:44,360
zero point nine is zero point eighty one.

208
00:12:44,870 --> 00:12:45,470
And so on.

209
00:12:45,470 --> 00:12:47,690
So here it will be zero point seventy three.

210
00:12:47,870 --> 00:12:49,730
And he will be zero point sixty six.

211
00:12:50,360 --> 00:12:57,560
So you can see that the way the discounted factor works is it discounts the value of the state as you

212
00:12:57,560 --> 00:12:58,550
are further away.

213
00:12:58,550 --> 00:13:04,660
So if you're familiar with finance theory then it's a simple thing similar to time value of money.

214
00:13:05,000 --> 00:13:10,760
Like what would you think about it this way what would you prefer to have five dollars today or five

215
00:13:10,760 --> 00:13:13,100
dollars in ten days from now.

216
00:13:13,210 --> 00:13:16,170
Just if somebody was to give you a choice I will give you five dollars today.

217
00:13:16,190 --> 00:13:18,240
All you five dollars 10 days from now.

218
00:13:18,320 --> 00:13:20,240
Well of course you would choose five dollars today.

219
00:13:20,240 --> 00:13:20,840
Why is that.

220
00:13:20,840 --> 00:13:25,790
Well because you can take those five dollars and you can invest them at a certain interest rate which

221
00:13:25,790 --> 00:13:31,820
is very similar to Gamma and your five dollars in 10 days will actually grow into maybe five dollars

222
00:13:31,820 --> 00:13:33,800
and 73 cents or something like that.

223
00:13:34,010 --> 00:13:35,300
And that's how.

224
00:13:35,300 --> 00:13:38,210
Time value of money works and very similar concept here.

225
00:13:38,220 --> 00:13:43,170
And the important thing to understand here this is just a theory a way that reinforcement learning works

226
00:13:43,170 --> 00:13:43,370
so.

227
00:13:43,430 --> 00:13:48,680
Richard Belman came up with this equation and from then now that's how we use it.

228
00:13:48,700 --> 00:13:48,930
So.

229
00:13:48,970 --> 00:13:51,370
So you could go ahead and come up with a different equation.

230
00:13:51,370 --> 00:13:54,830
It doesn't have to have gone might have some other factor it might not even have a factor.

231
00:13:54,890 --> 00:13:57,420
But this approach works and that's why we're using it.

232
00:13:57,800 --> 00:14:00,740
And this this is what it visually looks like.

233
00:14:00,740 --> 00:14:04,810
So the further away you are the less value of it being in this state.

234
00:14:04,820 --> 00:14:07,910
And in terms of time value for money if I could say to you.

235
00:14:07,910 --> 00:14:11,140
Where would you rather be would you rather be here would you rather be here.

236
00:14:11,240 --> 00:14:12,860
You would say I would rather be here.

237
00:14:12,860 --> 00:14:18,410
So we're creating that hope that same phenomenon as in time value of money we're artificially creating

238
00:14:18,410 --> 00:14:19,090
it through gamma.

239
00:14:19,100 --> 00:14:24,620
So that in order to incentivize agents or inspire agents to be closer to the finish line.

240
00:14:24,620 --> 00:14:30,170
So if an agent were to be asked which rugby hero here because of the way this equation works it would

241
00:14:30,170 --> 00:14:31,520
choose to be here.

242
00:14:31,560 --> 00:14:32,680
There's nothing more to that.

243
00:14:32,680 --> 00:14:33,320
Nothing less.

244
00:14:33,320 --> 00:14:35,780
It's not something that the world works this way.

245
00:14:35,780 --> 00:14:42,590
No it's just something that we're artificially creating in order for our agents to understand that this

246
00:14:42,710 --> 00:14:44,030
is this is good.

247
00:14:44,060 --> 00:14:44,560
This is good.

248
00:14:44,570 --> 00:14:44,990
This is good.

249
00:14:44,990 --> 00:14:45,670
They're all good.

250
00:14:45,680 --> 00:14:49,640
But this one is better than this one and this one is better than this one and this one has been in this

251
00:14:49,640 --> 00:14:54,720
one and that way you can see the old agent can see in which direction needs to go.

252
00:14:54,740 --> 00:15:00,020
So it can see that if I'm standing here remember that problem that we had or was he standing here Yeah

253
00:15:00,070 --> 00:15:01,320
so if he's standing here.

254
00:15:01,430 --> 00:15:04,820
Do I go down or if I'm standing here to go up or do I go down.

255
00:15:05,140 --> 00:15:09,670
Well now there's no it's not a problem anymore because he can see that it's actually better to go up

256
00:15:09,700 --> 00:15:11,500
because the value is bigger here.

257
00:15:11,500 --> 00:15:12,750
And then from here it is better to go right.

258
00:15:12,760 --> 00:15:14,500
Because the value is bigger here than here.

259
00:15:14,500 --> 00:15:15,700
And then from here is better to go right.

260
00:15:15,700 --> 00:15:18,480
Because the value here is bigger than here here and from here.

261
00:15:18,550 --> 00:15:22,570
He already knows that he needs to go right because he'll get a reward here of one.

262
00:15:22,600 --> 00:15:24,910
So that's how this whole approach works.

263
00:15:24,910 --> 00:15:27,470
Now let's have a quick look at the rest of the square.

264
00:15:27,550 --> 00:15:29,980
So how do we calculate the value in this square.

265
00:15:29,980 --> 00:15:32,410
Well here here is where things get tricky.

266
00:15:32,410 --> 00:15:37,300
So from here you might not actually go left right you might actually go right.

267
00:15:37,330 --> 00:15:41,470
So we can't just keep going like that because it might actually be shorter to go this way.

268
00:15:41,470 --> 00:15:46,120
So what we're going to do is we're going to calculate the value in the square first and because obviously

269
00:15:46,120 --> 00:15:48,430
from here are the best ways to go is up again.

270
00:15:48,430 --> 00:15:52,870
That's because we see the crew we have the crystal ball we can see things and you'll see further down

271
00:15:52,870 --> 00:15:57,160
in the section you'll see how the agent actually explores this understands this on their lives through

272
00:15:57,160 --> 00:15:57,950
experimentation.

273
00:15:58,030 --> 00:16:02,530
But for us we know that it's better to go this way so we're going to calculate the value here and that's

274
00:16:02,530 --> 00:16:06,330
why we're going to calculate the value in this square first.

275
00:16:06,340 --> 00:16:09,180
So here we have three possible actions.

276
00:16:09,190 --> 00:16:11,560
In reality we actually have four we can also go left.

277
00:16:11,560 --> 00:16:16,270
The agent could hypothetically press left and bump into the wall and stay here but for simplicity's

278
00:16:16,270 --> 00:16:21,580
sake we're just going to show the actions that we knowing what we know and having the crystal ball we

279
00:16:21,580 --> 00:16:26,800
know which actions are the ones actually to lead lead to something other than the same state again.

280
00:16:26,800 --> 00:16:31,960
And so here from here we know that again just because we have a crystal ball we know that the best way

281
00:16:31,960 --> 00:16:33,310
to go is this way.

282
00:16:33,310 --> 00:16:37,330
And agent of course would have to experiment and find the best way and you will see how that happens

283
00:16:37,330 --> 00:16:42,190
to further down in this action you'll see actually how an agent walks around and how you would experiment

284
00:16:42,280 --> 00:16:43,530
trying to find these values.

285
00:16:43,540 --> 00:16:45,300
But for us we know it's that way.

286
00:16:45,310 --> 00:16:51,390
So here if we plug everything in once to the maximum the best output is when you go up here is a ones

287
00:16:51,430 --> 00:16:53,730
point nine zero so you plug that in.

288
00:16:53,770 --> 00:16:56,000
You get zero point nine.

289
00:16:56,160 --> 00:16:56,410
OK.

290
00:16:56,440 --> 00:16:59,760
So we calculate that one this calculate this one same approach.

291
00:16:59,770 --> 00:17:05,670
This is you have three ways you can go actually four for the agent but for us we can see it's only three.

292
00:17:05,830 --> 00:17:08,620
So zero point eighty one from here.

293
00:17:08,860 --> 00:17:14,380
You have zero point seventy three and it actually ties in nicely with this value because then you just

294
00:17:14,380 --> 00:17:18,550
kind of getting a three point sixty six and here you have zero point zero three because this is the

295
00:17:18,550 --> 00:17:19,910
optimal right.

296
00:17:20,050 --> 00:17:21,160
So there you go.

297
00:17:21,160 --> 00:17:23,700
That is the values all of these states.

298
00:17:23,710 --> 00:17:29,650
And now you can see that because we've created this equation we've created synthetically this whole

299
00:17:29,650 --> 00:17:37,810
concept of the closer you are to the finish line the more valuable that state is not because we've created

300
00:17:37,810 --> 00:17:41,770
that now it's pretty obvious for the agent which way it should go.

301
00:17:41,890 --> 00:17:44,560
And we'll talk more about that in the coming Charles.

302
00:17:44,860 --> 00:17:52,230
I hope you enjoyed today's session and I know it's a bit it might sound a bit very basic at this stage

303
00:17:52,270 --> 00:17:57,260
but as we go through this section we will add a bit more complexity to it at the same time.

304
00:17:57,280 --> 00:18:02,470
If you cannot wait if you want to jump into it then there is a paper which you can look at and it is

305
00:18:02,470 --> 00:18:04,290
the original paper by Richard Bellman.

306
00:18:04,290 --> 00:18:08,320
It's called The Theory of dynamic programming from 1954.

307
00:18:08,320 --> 00:18:15,190
And you can find it at this link and they go so you can jump straight into it and read from the author

308
00:18:15,250 --> 00:18:20,810
of the Belmont equation but just bear in mind that this is quite a mathematically heavy paper.

309
00:18:20,920 --> 00:18:22,750
And on that note I look forward to your next.

310
00:18:22,800 --> 00:18:24,500
And until then enjoy a.
