WEBVTT
1
00:00:00.240 --> 00:00:06.500
Hello everyone and welcome to this amazing section of the course where we are going to talk about transfer

2
00:00:06.510 --> 00:00:11.500
learning and fine tuning before we dig into these techniques.

3
00:00:11.520 --> 00:00:13.450
Let's see what we had so far.

4
00:00:13.860 --> 00:00:20.490
So far we built a decent number of neural networks and you might think those networks were too big or

5
00:00:20.490 --> 00:00:28.620
too complex but in reality all networks that were built so far were amazing but not even close at the

6
00:00:28.620 --> 00:00:34.680
complexity of some neural networks that we are going to work with throughout this section.

7
00:00:34.730 --> 00:00:39.130
There is an image classification global competition called Image net.

8
00:00:39.200 --> 00:00:45.830
And companies all around the world are making massive complex CNN architectures to win this competition

9
00:00:47.030 --> 00:00:49.790
if you taught our neural networks were complex.

10
00:00:49.790 --> 00:00:55.550
Let's take a look at this one the network on this slide is called Inception net.

11
00:00:55.650 --> 00:01:00.100
It is made by Google specifically for the image net competition.

12
00:01:00.360 --> 00:01:04.200
All of our networks were trained in a few minutes or even a few seconds.

13
00:01:04.950 --> 00:01:12.120
But these kind of detectors companies are training on hundreds of deep use for many weeks.

14
00:01:12.260 --> 00:01:18.260
You would think at this point that these networks are just useless for everyday tasks and those resources

15
00:01:18.260 --> 00:01:20.540
are just thrown away for a competition.

16
00:01:21.020 --> 00:01:26.550
But in reality the real power of these networks come with transfer learning.

17
00:01:26.870 --> 00:01:33.230
Before we dig into transfer learning let's take a moment to talk about the Internet as we already discussed

18
00:01:33.530 --> 00:01:39.910
the image net is the annual computer vision competition but it is the data set as well.

19
00:01:40.020 --> 00:01:46.020
It has about 40 million images that are separated into one thousand classes.

20
00:01:46.020 --> 00:01:52.740
The real question is this what if we could apply that knowledge gained from the image net to a totally

21
00:01:52.740 --> 00:01:58.260
different custom task and still perform with the State of the art the results.

22
00:01:58.500 --> 00:02:04.800
And that is exactly what the transfer learning is doing the transfer learning is a technique where we

23
00:02:04.800 --> 00:02:13.740
use a pre train model from Image net to perform great on a custom task custom task can be anything.

24
00:02:13.800 --> 00:02:17.370
In our case it will be dogs versus cats classification.

25
00:02:17.580 --> 00:02:22.760
These base model is the pre train model on the image and the data set.

26
00:02:22.950 --> 00:02:30.870
It could be in the inception and there is a mobile Net V-J 16 Vejjajiva 19 and many more.

27
00:02:30.870 --> 00:02:35.640
There is a lot of pre train models that we could use in this case.

28
00:02:35.640 --> 00:02:38.200
We decided for mobile Net.

29
00:02:38.380 --> 00:02:39.480
This is all great.

30
00:02:39.520 --> 00:02:46.060
But how to change the pre train model for the custom tasks such as binary classification between cats

31
00:02:46.060 --> 00:02:47.350
and dogs.

32
00:02:47.350 --> 00:02:54.560
When we defined a base model we deleted a few lost layers from it that are designed specifically for

33
00:02:54.560 --> 00:02:56.750
the image and classification.

34
00:02:56.750 --> 00:03:00.750
The name of that top part of the CNN is called Head.

35
00:03:01.010 --> 00:03:07.940
We use the whole base model until it's head and change the head with a custom network or the custom

36
00:03:07.940 --> 00:03:14.990
head for the custom task and now we have the base model with a custom head.

37
00:03:14.990 --> 00:03:21.490
And the last thing that we have to do is to freeze the whole base model when performing transfer learning

38
00:03:21.850 --> 00:03:28.810
our base model was trained on a data set such as image net and its weights are learned we don't touch

39
00:03:28.810 --> 00:03:29.910
them anymore.

40
00:03:29.980 --> 00:03:36.610
The only part of the network that is trained is the custom head part but there is one more technique

41
00:03:36.610 --> 00:03:44.450
that we need to discuss and that is fine tuning fine tuning is more suited for bigger data sets.

42
00:03:44.470 --> 00:03:49.490
Sometimes when the custom data set is pretty different from the image in that data set.

43
00:03:49.570 --> 00:03:53.290
We need information about it in the base model as well.

44
00:03:53.500 --> 00:03:59.900
In the fine tuning we still define a custom head but we don't freeze the whole base model.

45
00:04:00.010 --> 00:04:06.370
We freeze only the beginning of the model and the rest of the layers are unfrozen and ready to be changed

46
00:04:06.490 --> 00:04:08.520
according to the custom data set.

47
00:04:09.360 --> 00:04:16.520
And for the end of this video let us these cars when to use each technique if we have a large and different

48
00:04:16.520 --> 00:04:19.960
data set train the whole model.

49
00:04:20.110 --> 00:04:26.950
And when we have a large and similar data set to the one that our base model was trained on use fine

50
00:04:26.950 --> 00:04:32.820
tuning then in the case you have a small and different data set.

51
00:04:32.840 --> 00:04:38.810
Use the fine tuning again and finally if you have small and similar data set.

52
00:04:38.890 --> 00:04:42.540
Use transfer learning and that's it for now.

53
00:04:42.800 --> 00:04:46.760
In the next video we will start with our transfer learning project.

54
00:04:46.760 --> 00:04:51.540
If you have any questions or comments so far please post them in the comments section.

55
00:04:51.530 --> 00:04:53.480
Otherwise assume the next tutorial.
