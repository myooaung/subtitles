1
00:00:00,180 --> 00:00:03,420
Hello everyone and welcome to this python tutorial.

2
00:00:03,420 --> 00:00:09,330
In the previous video we have completed the setup process of our environment and install all dependencies

3
00:00:09,420 --> 00:00:10,980
for this project.

4
00:00:11,430 --> 00:00:18,300
And in this video we are going to input all of them input our data set and prepared for tens flow transform

5
00:00:18,300 --> 00:00:19,480
library.

6
00:00:19,560 --> 00:00:25,410
But before we jump into data set pre processing stage of this video let's talk about the few libraries

7
00:00:25,620 --> 00:00:29,040
that we are going to use for this project.

8
00:00:29,060 --> 00:00:35,780
The first one is temp file which is a native title library that we don't have to install and its job

9
00:00:35,780 --> 00:00:43,070
is very simple creating temporary files for logging purposes then we have pandas.

10
00:00:43,240 --> 00:00:50,030
And as always we are going to use it for loading and working with CSC files the first library extends

11
00:00:50,030 --> 00:00:51,050
the flow.

12
00:00:51,390 --> 00:00:59,110
We will use it to create some variables then we have our tensor flow transform and this tends to flow

13
00:00:59,110 --> 00:00:59,900
transformed.

14
00:00:59,950 --> 00:01:04,890
Beam is here to create context for Apache beam.

15
00:01:05,050 --> 00:01:08,500
That is a great thing that is happening in the background for us.

16
00:01:08,560 --> 00:01:16,340
The Apache Beam is a whole infrastructure of tools that are used for faster computation and pre processing

17
00:01:17,020 --> 00:01:18,900
since it is very complex.

18
00:01:18,910 --> 00:01:21,770
We are not going to talk about it in this course.

19
00:01:21,820 --> 00:01:25,080
The tensor floaters forum is using that in the background.

20
00:01:25,100 --> 00:01:30,180
That's all what you have to know so far but I'm going to provide a few links for you.

21
00:01:30,250 --> 00:01:36,860
So if you're interested in learning more about Apache beam to use it in your day to day project besides

22
00:01:36,910 --> 00:01:42,570
the flow transform you're welcome to use those links then we are going to input from the future.

23
00:01:42,640 --> 00:01:49,670
Our brain function if we are using Python do it is more suited for us to empower these so we can use

24
00:01:49,670 --> 00:01:53,970
a simple brain function from the Python free as well.

25
00:01:54,000 --> 00:02:01,080
And lastly we are going to import a few things from div dot metadata the data set metadata to create

26
00:02:01,140 --> 00:02:07,860
meta information about data set and the data set schema which will help us in that process.

27
00:02:07,860 --> 00:02:15,460
We will discuss that in further detail down the road for now execute the cell and now we have all dependencies

28
00:02:15,520 --> 00:02:21,930
at our disposal the data set that we used in the previous section is the one that we are going to use

29
00:02:22,020 --> 00:02:23,790
in this section as well.

30
00:02:23,940 --> 00:02:31,560
The pollution data set of the city of Belgrade to access it in this notebook as well go to this arrow

31
00:02:31,890 --> 00:02:33,620
and then files.

32
00:02:33,630 --> 00:02:40,550
If you don't have it already in your system upload it in the same way as you did in the previous section.

33
00:02:40,650 --> 00:02:46,410
In my case it is already there but again if you don't have it there click on upload.

34
00:02:46,410 --> 00:02:51,650
Select the file that you downloaded from the resources of this lesson and upload it simply there.

35
00:02:52,950 --> 00:03:00,590
The first part of our dataset pre processing is to load a data set with pandas so let's write data set

36
00:03:00,930 --> 00:03:04,670
equal to BD that read CSB.

37
00:03:04,770 --> 00:03:08,960
And as always we have to specify the name of a file that we want to load.

38
00:03:09,210 --> 00:03:16,450
In our case that is pollution small dot we execute a cell to remind ourselves.

39
00:03:16,520 --> 00:03:19,990
Let's see what columns we have in our data set.

40
00:03:20,090 --> 00:03:23,260
So let's bring the first five rows of the data set.

41
00:03:23,600 --> 00:03:30,080
And as you can see we have a data column and we have four features that will describe the whole dataset

42
00:03:30,940 --> 00:03:34,230
since we are as a stamp of data pre processing.

43
00:03:34,270 --> 00:03:36,370
The data is not that important.

44
00:03:36,550 --> 00:03:41,200
It will be important down the road but at this stage we can drop it.

45
00:03:41,200 --> 00:03:47,590
So they drop it and create a variable called feature that will hold all columns except a date column

46
00:03:48,720 --> 00:03:49,280
right.

47
00:03:49,280 --> 00:03:54,980
Features equal to data said the drop in the drop function.

48
00:03:54,980 --> 00:03:57,090
We have to specify a few things.

49
00:03:57,090 --> 00:03:59,220
The first one is the column name.

50
00:03:59,550 --> 00:04:03,190
And in this case it is simply a date then.

51
00:04:03,260 --> 00:04:10,820
The second argument is to specify access because in our data set columns are separated by commas and

52
00:04:10,850 --> 00:04:12,440
organize in this way.

53
00:04:12,440 --> 00:04:13,930
The axis is equal to 1.

54
00:04:14,750 --> 00:04:17,250
So let's execute the cell and we are ready.

55
00:04:17,270 --> 00:04:19,110
We have our features.

56
00:04:19,280 --> 00:04:24,440
Let's visualize the first five rows as we did previously to see if all features are there.

57
00:04:25,040 --> 00:04:31,910
And as you can see we have everything here except the date column which is what we intended to do and

58
00:04:31,910 --> 00:04:38,180
the last thing that we have to do in this tutorial is to convert the data set from this fracture to

59
00:04:38,180 --> 00:04:40,580
the list of byte and dictionaries.

60
00:04:40,610 --> 00:04:45,890
The reason for that is the tensor flow transform library the way that we are going to use it.

61
00:04:45,890 --> 00:04:50,410
In this section requires Python dictionaries as inputs.

62
00:04:50,550 --> 00:04:56,270
We are going to convert this data frame into the list of Python dictionaries where each row is going

63
00:04:56,270 --> 00:04:58,820
to be a separated python dictionary.

64
00:04:58,820 --> 00:05:06,280
The keys are going to be names of columns and the values are going to be values the conversion is easy

65
00:05:06,280 --> 00:05:08,780
to achieve and it's only one line of code.

66
00:05:09,830 --> 00:05:17,390
Let's define dict features variable that is going to hold our Annual structure and set it to list.

67
00:05:17,470 --> 00:05:24,750
And in this list let's provide our features and call a built in function to dict in the brackets.

68
00:05:24,790 --> 00:05:31,610
We need to specify a first argument called Orient since we want each row to be separate dictionary.

69
00:05:31,720 --> 00:05:36,070
We need to specify that the reference is to look for is that index.

70
00:05:36,070 --> 00:05:41,710
The Orient argument determines the type of values and the type of the dictionary.

71
00:05:41,710 --> 00:05:48,940
In our case an index is used to determine how to convert this data frame to dictionaries.

72
00:05:49,060 --> 00:05:51,560
We will end up with a structure like this.

73
00:05:51,700 --> 00:05:55,010
Column value and that is what we want.

74
00:05:55,060 --> 00:05:58,210
So in the brackets specify an index.

75
00:05:58,210 --> 00:06:01,740
We need to specify these dot values and execute the set.

76
00:06:02,650 --> 00:06:07,290
Let's visualize the first two rows to see how it's structured.

77
00:06:07,360 --> 00:06:14,440
And as you can see each row is a separate dictionary where keys are column names and values are still

78
00:06:14,440 --> 00:06:19,480
values and that's it for our video and data says pre processing.

79
00:06:19,660 --> 00:06:23,010
We are ready to use tens of floaters for for now.

80
00:06:23,020 --> 00:06:27,170
If you have any questions or comments please post them in the comments section.

81
00:06:27,340 --> 00:06:29,200
Otherwise I see in the next tutorial.
