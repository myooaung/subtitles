WEBVTT
1
00:00:00.650 --> 00:00:05.480
Hello and welcome back to the course on deep learning in today's vitriol we're talking about gradient

2
00:00:05.540 --> 00:00:12.980
descent what we learned previously was that in order for a neural network to learn what needs to happen

3
00:00:13.010 --> 00:00:14.310
is back propagation.

4
00:00:14.330 --> 00:00:21.680
And that is when the error the difference or the sum of squared differences between the y hat and Y

5
00:00:22.550 --> 00:00:28.450
is back propagated through the neural network and the weights are adjusted accordingly.

6
00:00:28.460 --> 00:00:34.250
So we saw that and today we're going to learn exactly how these weights are adjusted.

7
00:00:34.340 --> 00:00:35.980
So let's have a look.

8
00:00:36.020 --> 00:00:43.970
This is our very simple version of a neural network a perception or a single their feet forward a neural

9
00:00:43.970 --> 00:00:44.810
network.

10
00:00:44.810 --> 00:00:52.370
And what we can see here is this whole process in action where we've got some input value then we've

11
00:00:52.370 --> 00:00:53.780
got to wait.

12
00:00:53.870 --> 00:00:56.930
Then a activation function is applied.

13
00:00:56.930 --> 00:01:01.790
We have we get white hats and then we compare it to the actual value recalculate the cost function.

14
00:01:01.790 --> 00:01:05.340
So how can we minimize the cost function.

15
00:01:05.360 --> 00:01:07.310
What can we do about it.

16
00:01:07.310 --> 00:01:14.690
Well one approach to do it is a brute force approach where we just take all lots of different possible

17
00:01:14.690 --> 00:01:18.160
weights and look at them and see which one works best.

18
00:01:18.170 --> 00:01:24.500
And what we do is for instance would try out for let's say for example a thousand weights and we'd try

19
00:01:24.500 --> 00:01:31.160
them out we'd get something like this for the cost function and this is a chart of on the y axis you

20
00:01:31.160 --> 00:01:36.230
have cost function or the vertical axis and the horizontal axis you have y had and because you can see

21
00:01:36.230 --> 00:01:39.140
the formulas y had minus Y squared.

22
00:01:39.140 --> 00:01:46.120
This is what the cost function would look something like that and basically you'd find the best one

23
00:01:46.120 --> 00:01:47.680
is over here.

24
00:01:47.890 --> 00:01:50.870
So very simple very intuitive approach.

25
00:01:50.950 --> 00:01:58.120
Why not do this brute force method why not just try out a thousand different costs for those thousand

26
00:01:58.120 --> 00:02:03.640
different parameters or inputs for weights and see which one works the best you'll find the best one

27
00:02:03.640 --> 00:02:04.350
that way.

28
00:02:04.360 --> 00:02:10.240
Well if you have just one way to optimize this might work but as you increase the number of weights

29
00:02:10.420 --> 00:02:16.570
increase the number of synopses in your network you have to face the curse of dimensionality.

30
00:02:16.570 --> 00:02:19.390
And so what is the course of dimensionality.

31
00:02:19.390 --> 00:02:24.310
The best way to describe this or explain it is to just look at a practical example.

32
00:02:24.580 --> 00:02:30.550
So remember this example we had when we were talking about how neural networks actually work where we

33
00:02:30.550 --> 00:02:37.060
were building or running a neural network for a full property valuation.

34
00:02:37.060 --> 00:02:40.610
So this is what it looked like when it was trained up already.

35
00:02:40.720 --> 00:02:44.190
Well when it's not trained before it's train before we know which.

36
00:02:44.320 --> 00:02:45.300
What are the weights.

37
00:02:45.490 --> 00:02:47.680
The actual neural network looks like this.

38
00:02:47.680 --> 00:02:47.860
Right.

39
00:02:47.860 --> 00:02:55.110
Because we have all these different possible synopses and we still have to train up the weights.

40
00:02:55.210 --> 00:03:00.430
And here we have a total of twenty five weight so four times five at the start plus five more from the

41
00:03:00.430 --> 00:03:03.610
hinterland it's about Butler twenty five weights total.

42
00:03:03.610 --> 00:03:12.160
And let's see how we could possibly brute force twenty five ways this is a very simple neural network

43
00:03:12.160 --> 00:03:14.480
right here very simple just one hidden layer.

44
00:03:14.620 --> 00:03:21.290
And how could we brute force our way through a neural network of this size.

45
00:03:21.310 --> 00:03:24.330
Well let's do some simple mathematical calculations.

46
00:03:24.370 --> 00:03:25.850
We have 25 weights.

47
00:03:25.870 --> 00:03:30.340
So that means if we have a thousand combinations that are going to test out for every weight the total

48
00:03:30.340 --> 00:03:34.690
number of combinations is a thousand to the power of twenty five or a thousand or tend to oppose any

49
00:03:34.690 --> 00:03:37.750
five different combinations.

50
00:03:37.750 --> 00:03:47.250
Now let's see how sun the way to table light the world's fastest supercomputer as of June 2016.

51
00:03:47.360 --> 00:03:49.810
What how would it approach is probably right.

52
00:03:49.810 --> 00:03:58.630
So some way tie to light it looks like this is a whole huge building pretty much for this one supercomputer

53
00:03:58.990 --> 00:04:04.760
and it got the Guinness World Records for being the fastest supercomputer.

54
00:04:05.170 --> 00:04:12.550
Right now it is the fastest supercomputer in the world and Sunday title light can operate at a speed

55
00:04:12.580 --> 00:04:22.150
of 93 Petter flops a flop stands for floating operation per second so it can do ninety three to the

56
00:04:22.150 --> 00:04:27.890
power times tend to the power of 15 floating operations per second.

57
00:04:28.030 --> 00:04:32.290
That's how quick it is in comparison.

58
00:04:32.410 --> 00:04:34.520
Average computers right now.

59
00:04:34.570 --> 00:04:43.300
Did you like just over several giga flops and so on so like kind of those ranges way less than tie Sunday.

60
00:04:43.320 --> 00:04:51.160
Tail light so suddenly tail light is in the forefront of technology and let's say hypothetically that

61
00:04:51.490 --> 00:05:00.640
it can do one on one test one combination for our neural network in one floppy disk and one floating

62
00:05:00.640 --> 00:05:06.490
operation that is not possible that is not practical because you need multiple floating operations to

63
00:05:06.490 --> 00:05:11.680
test out a single weight in your neural network but even let's let's give it a head start let's say

64
00:05:11.860 --> 00:05:18.910
that it can do it in a ideal world it can do that in one floating operation it can do one test per one

65
00:05:18.910 --> 00:05:26.020
floating operation that means it will still require 10 to the power of five divide by ninety three times

66
00:05:26.040 --> 00:05:34.060
tens of our 15 seconds to come to run all of those tests to brute force through that network.

67
00:05:34.060 --> 00:05:39.790
So that means one or approximate center power of 58 seconds and that is the same as 10 to the power

68
00:05:39.790 --> 00:05:42.040
of 50 years.

69
00:05:42.070 --> 00:05:48.120
That is a huge number that is longer than the universe has existed.

70
00:05:48.130 --> 00:05:54.580
And that is definitely not going to simply this number is so huge is just definitely not going to work

71
00:05:54.580 --> 00:05:59.070
for us at all in our optimization.

72
00:05:59.080 --> 00:06:00.020
So there we go.

73
00:06:00.100 --> 00:06:04.190
This is a no no even on the world's fastest supercomputer.

74
00:06:04.230 --> 00:06:05.370
Some way tail light.

75
00:06:05.410 --> 00:06:10.780
So we have to come up with a different approach how are we going to find the optimal weight by the way

76
00:06:11.230 --> 00:06:16.060
this our neural network was very simple what about if the neural networks looks like something like

77
00:06:16.060 --> 00:06:22.610
this or even greater than that then yeah it's just not going to happen at all ever.

78
00:06:22.690 --> 00:06:28.440
So the method we're going to be looking at is called gradient descent and you may have heard of it already.

79
00:06:28.510 --> 00:06:30.670
If not we will find out what it is right now.

80
00:06:30.670 --> 00:06:40.480
So there is our cost function and now we're going to see how we can foster for come of a faster way

81
00:06:40.480 --> 00:06:45.840
to find the best option so let's say we start somewhere you gotta start somewhere.

82
00:06:45.870 --> 00:06:47.340
So we start over there.

83
00:06:47.340 --> 00:06:56.910
And from that point in the top left what we're going to do is we're going to look at the angle of our

84
00:06:56.910 --> 00:06:58.020
cost function.

85
00:06:58.020 --> 00:07:01.960
At that point so we're just going to basically that's what's called gradient because you have to differentiate.

86
00:07:02.070 --> 00:07:07.230
We're not going to look at the mathematical equations are we will provide some tips on additional reading

87
00:07:07.500 --> 00:07:09.570
at the end of the next lecture.

88
00:07:09.690 --> 00:07:12.960
But basically you just need to differentiate.

89
00:07:12.960 --> 00:07:19.200
Find out what the slope is in that specific point and find out if the slope is positive or negative.

90
00:07:19.290 --> 00:07:23.900
If the if the slope is negative like in this case means that you're going downhill.

91
00:07:23.910 --> 00:07:29.610
So to the right is downhill to the left is uphill and from there it means you need to go right.

92
00:07:29.700 --> 00:07:32.990
Basically you need to go downhill and that's what we're going to do.

93
00:07:33.030 --> 00:07:35.340
Room takes a step right.

94
00:07:35.400 --> 00:07:37.410
The ball rolls down again.

95
00:07:37.410 --> 00:07:43.170
Same thing you calculate the slope the standard slope is positive meaning rate as uphill left as downhill

96
00:07:43.530 --> 00:07:46.740
and you need to go left and you roll the ball down.

97
00:07:46.740 --> 00:07:53.040
And again you calculate the slope and you're all the ball right.

98
00:07:53.100 --> 00:07:53.480
There you go.

99
00:07:53.480 --> 00:07:55.010
So that's how you find.

100
00:07:55.010 --> 00:08:04.530
In simple terms that's how you find the best weights the best situation that minimizes your cost function.

101
00:08:04.530 --> 00:08:08.940
Of course it's not going to be like a ball rolling is going to be a very zigzag type of approach but

102
00:08:09.150 --> 00:08:15.720
it's easier to remember error kind of it's is more fun to look at it as a ball rolling but in reality

103
00:08:15.750 --> 00:08:21.640
yes you just is going to be like a step by step approach is going to be a zigzag type of method.

104
00:08:22.020 --> 00:08:22.220
Yeah.

105
00:08:22.230 --> 00:08:29.880
And also there's there's lots of other elements to it so there's things like for instance why.

106
00:08:30.060 --> 00:08:36.750
Like why does it go down why does it not to go way over the line so it could have jumped out of this

107
00:08:37.650 --> 00:08:40.410
gone upwards instead of downwards and things like that.

108
00:08:40.410 --> 00:08:41.900
So there are parameters that you can tweak.

109
00:08:41.910 --> 00:08:46.950
And again we will mention where you can find out more on that and plus we'll have this in the practical

110
00:08:46.950 --> 00:08:51.690
application but in the simplest intuitive approach this is what is happening.

111
00:08:51.720 --> 00:08:57.720
We are getting to the bottom by just understanding which way we need to go instead of brute forcing

112
00:08:57.720 --> 00:09:02.840
through thousands and thousands and millions and billions and quite trillions of combinations.

113
00:09:02.970 --> 00:09:07.530
We can just simply every time have a look at where is where.

114
00:09:07.590 --> 00:09:10.030
Which way is it sloping so like Europe.

115
00:09:10.100 --> 00:09:11.640
Imagine you're standing on a hill.

116
00:09:11.640 --> 00:09:13.740
Which way does it feel that it's going downwards.

117
00:09:13.740 --> 00:09:18.030
And whichever way it is going downwards you just keep walking that way you like take 50 steps that way

118
00:09:18.030 --> 00:09:19.350
and then you assess again.

119
00:09:19.350 --> 00:09:19.550
Okay.

120
00:09:19.560 --> 00:09:21.420
Which way is it going downwards this way.

121
00:09:21.420 --> 00:09:21.690
Okay.

122
00:09:21.730 --> 00:09:24.550
Now take 50 steps or less take 40 steps that way.

123
00:09:24.630 --> 00:09:27.770
So it gets less and less and less as you get closer.

124
00:09:28.470 --> 00:09:32.670
So here's an example of gradient descent applied in a two dimensional space.

125
00:09:32.670 --> 00:09:36.160
So that was a one dimensional example.

126
00:09:36.510 --> 00:09:40.190
Here we have a two dimensional space for the gradient descent.

127
00:09:40.260 --> 00:09:42.980
As you can see it's getting closer to the minimum.

128
00:09:42.990 --> 00:09:49.640
And it's also called gradient descent because you're descending into the minimum of the cost function.

129
00:09:49.650 --> 00:09:53.360
And finally here's the gradient descent applied in three dimensions.

130
00:09:53.370 --> 00:09:54.350
This is what it looks like.

131
00:09:54.360 --> 00:09:59.610
If you projected onto two dimensions you can see it's zigzagging its way into the minimum.

132
00:09:59.640 --> 00:10:00.090
So there we go.

133
00:10:00.090 --> 00:10:05.910
That was gradient descent in next drill we'll talk about stochastic gradient descent is a continuation

134
00:10:05.910 --> 00:10:09.690
of this material and I look forward to seeing you there until next time.

135
00:10:09.690 --> 00:10:10.560
Enjoy deep learning.
