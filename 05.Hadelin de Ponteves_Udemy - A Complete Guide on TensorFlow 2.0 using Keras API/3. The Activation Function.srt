1
00:00:00,210 --> 00:00:02,700
Hello and welcome back to of course on deep learning.

2
00:00:02,700 --> 00:00:03,060
All right.

3
00:00:03,060 --> 00:00:05,100
Today we're talking about activation function.

4
00:00:05,100 --> 00:00:06,940
Let's get straight into it.

5
00:00:06,960 --> 00:00:11,770
So this is where we left off previously we talked about the structure of a one neuron.

6
00:00:11,940 --> 00:00:16,710
So there it is in the middle we know that it has some inputs values coming in it's got some weights

7
00:00:17,070 --> 00:00:23,310
then it adds up the weighted it calculates a weight at some of those inputs and then applies activation

8
00:00:23,310 --> 00:00:28,450
function and step three it passes on the signal to the next year.

9
00:00:28,470 --> 00:00:29,730
And that's what we're talking about today.

10
00:00:29,730 --> 00:00:34,170
We're talking about the value that is going to be passed over so we're talking about the activation

11
00:00:34,170 --> 00:00:36,330
function that's being applied.

12
00:00:36,330 --> 00:00:39,210
So what options do we have for activation function.

13
00:00:39,210 --> 00:00:43,350
Well we're going to look at four different types of activation functions that you could choose from.

14
00:00:43,350 --> 00:00:47,190
Of course they're are more different types of activation functions but these are the predominate ones

15
00:00:47,190 --> 00:00:49,890
that you'll be hearing about and that we'll be using in this course.

16
00:00:50,340 --> 00:00:53,010
So here is the threshold function.

17
00:00:53,010 --> 00:00:53,940
This is what it looks like.

18
00:00:54,240 --> 00:01:03,240
So on the x axis you have the weighted sum of inputs on the y axis you have just the values from 0 to

19
00:01:03,240 --> 00:01:03,780
1.

20
00:01:03,930 --> 00:01:12,120
And basically the threshold function is a very simple type of function where if the value is less than

21
00:01:12,780 --> 00:01:16,450
zero then the threshold function passes on zero.

22
00:01:16,830 --> 00:01:22,890
If the value is more than zero or equal to zero then threshold per function passes on a 1.

23
00:01:22,890 --> 00:01:26,760
So it's basically kind of like yes no type of function.

24
00:01:26,880 --> 00:01:33,420
Very very straightforward very kind of like rigid type of function either yes or no.

25
00:01:33,450 --> 00:01:34,990
No other options.

26
00:01:35,010 --> 00:01:35,460
So there you go.

27
00:01:35,460 --> 00:01:36,150
That's how it works.

28
00:01:36,150 --> 00:01:37,380
Very simple function.

29
00:01:37,380 --> 00:01:40,380
Let's move on to something a bit more complex now.

30
00:01:40,550 --> 00:01:48,660
The sigmoid function very interesting formula that we have here you'll see just now there is one divide

31
00:01:48,660 --> 00:01:51,350
by one plus E to the power of minus X..

32
00:01:51,450 --> 00:01:59,030
Whereas in this case of course X is the value of the sum of the way that sums and.

33
00:02:00,090 --> 00:02:00,490
So yes.

34
00:02:00,510 --> 00:02:02,560
So this is what the sigmoid looks like.

35
00:02:02,570 --> 00:02:09,470
It's a function which is used in the logistic regression if you recall from the machine learning course.

36
00:02:09,480 --> 00:02:14,890
So what is good about this function is that it is smooth unlike the threshold function.

37
00:02:14,910 --> 00:02:21,660
This one doesn't have those kinks in its curve and therefore is just nice and smooth gradual progression.

38
00:02:21,660 --> 00:02:26,280
So anything below zero it just drops off above zero.

39
00:02:26,280 --> 00:02:34,830
It looks approximates towards one and this sigmoid function is very useful in the final layer in the

40
00:02:34,830 --> 00:02:38,850
output layer especially when you're trying to predict probabilities.

41
00:02:38,860 --> 00:02:40,770
And we'll see that throughout this course.

42
00:02:41,130 --> 00:02:47,310
And then we've got the rectified function rectified a function even though it has a kink is one of the

43
00:02:47,310 --> 00:02:50,850
most popular functions for artificial neural network.

44
00:02:50,860 --> 00:02:59,580
So it goes all the way to zero it is zero and then from there it gradually progresses as the input value

45
00:03:00,600 --> 00:03:01,430
increases as well.

46
00:03:01,710 --> 00:03:05,490
And we'll see that throughout the course we'll see that in other intuition tutorials and we'll also

47
00:03:05,490 --> 00:03:11,340
see that how we use this function in the practical side of the course and I will comment on this a bit

48
00:03:11,340 --> 00:03:13,530
more in a few slides from now.

49
00:03:13,530 --> 00:03:18,970
So just remember the direct fire function is one of the most used functions in artificial neural networks.

50
00:03:18,990 --> 00:03:22,710
And finally we've got one more function that you will probably hear about.

51
00:03:22,770 --> 00:03:25,230
It's the hyperbolic tangent function.

52
00:03:25,230 --> 00:03:32,360
It's very similar to the sigmoid function but here the hyperbolic tangent function goes below zero.

53
00:03:32,370 --> 00:03:39,590
So the values go from 0 to 1 or approximate to 1 and go from 0 to minus 1 on the other side.

54
00:03:39,690 --> 00:03:42,330
And that can be useful in some applications.

55
00:03:42,330 --> 00:03:45,740
So we're not going to go into too much depth on each one of these functions.

56
00:03:45,740 --> 00:03:51,720
I just wanted to acquaint you with them so that you know what they look like and what they're called.

57
00:03:51,720 --> 00:03:57,410
If you'd like to get some additional reading then check out this paper by.

58
00:03:57,460 --> 00:04:05,630
So you have year Laura what have you got called Deep sparse rectifying neural networks 2011 paper.

59
00:04:05,730 --> 00:04:15,210
And there you will find out exactly why the rectify function is such a valuable function why it's so

60
00:04:15,210 --> 00:04:21,150
popular used but nevertheless for now you don't really need to know all of those things for now we just

61
00:04:21,150 --> 00:04:24,130
need to start applying them when you start using them more and more and more.

62
00:04:24,210 --> 00:04:31,230
And so when you feel comfortable with the practical side of things then you can go and refer to this

63
00:04:31,230 --> 00:04:37,140
paper and then you will be able to soak in that knowledge much quicker and it'll make much more sense.

64
00:04:37,290 --> 00:04:41,910
But just keep this in mind that when you're ready when you feel that you're ready then you can go and

65
00:04:41,970 --> 00:04:44,670
refer to this paper and get some valuable knowledge from them.

66
00:04:45,510 --> 00:04:53,070
So I just to quickly recap we have the threshold activation function which looks like this the sigmoid

67
00:04:53,070 --> 00:04:55,450
activation function which looks like this.

68
00:04:55,640 --> 00:05:02,210
We have the rectifying function and we have the hyperbolic tangent from and now to finish off this tutorial.

69
00:05:02,230 --> 00:05:09,070
Let's quickly do a few exercise soldiers do quick two quick exercises to help that knowledge sink in.

70
00:05:09,100 --> 00:05:15,130
So first one is we've got an example here of a neural network of just one neuron and then right away

71
00:05:15,130 --> 00:05:15,790
the output layer.

72
00:05:16,090 --> 00:05:22,570
And the question is assuming that your dependent variable is binary so it's either 0 1 which threshold

73
00:05:22,570 --> 00:05:25,690
function would you use some out of the ones that we've discussed.

74
00:05:25,990 --> 00:05:34,030
We have the threshold function the sigmoid function there rectify our function and we've got the hyperbolic

75
00:05:34,030 --> 00:05:37,900
tangent function in it's in there all forms.

76
00:05:37,930 --> 00:05:43,890
Which ones would you be able to use for a binary variable.

77
00:05:43,930 --> 00:05:44,360
Okay.

78
00:05:44,410 --> 00:05:49,310
So the answers here are there's two options that we can approaches with.

79
00:05:49,330 --> 00:05:55,120
So number one is the threshold activation function because we know that it's between 0 and 1 and it

80
00:05:55,270 --> 00:06:00,040
gives you a 0 under certain rules in an otherwise it gives you once and only can give you two values

81
00:06:00,040 --> 00:06:09,940
it fits perfectly fits this requirement perfectly and therefore you could say Y equals the threshold

82
00:06:09,940 --> 00:06:13,810
function of your suite at some and that's it.

83
00:06:13,930 --> 00:06:19,900
And in the second case which you could use is the sigmoid activation function it is actually also between

84
00:06:19,900 --> 00:06:21,640
0 and 1 just what we need.

85
00:06:21,700 --> 00:06:25,570
But at the same time you want it's just 0 1 right.

86
00:06:25,570 --> 00:06:33,220
So you it's not exactly true that what we need but in this case what you could use it as is the probability

87
00:06:33,700 --> 00:06:37,480
of Y being yes or no.

88
00:06:37,480 --> 00:06:45,760
So we want y to be 0 0 1 but instead will say that the sigmoid function similar activation function

89
00:06:45,760 --> 00:06:51,790
tells us whether it tells us the probability of Y being equal to 1.

90
00:06:51,820 --> 00:06:59,050
So basically the closer you get to the top the more likely it is that this is indeed a one where a yes

91
00:06:59,080 --> 00:07:01,020
rather than a no hand.

92
00:07:01,050 --> 00:07:01,270
Yeah.

93
00:07:01,300 --> 00:07:04,760
So that's very similar to the logistic regression approach.

94
00:07:04,940 --> 00:07:07,500
And those are just two examples.

95
00:07:07,510 --> 00:07:13,090
And if you have a binary variable Now let's have a look at another practical application let's have

96
00:07:13,090 --> 00:07:17,330
a look at how all this would play out if we had a neural network like this.

97
00:07:17,380 --> 00:07:24,430
So in the first input layer we have some inputs they are sent off to our first hidden layer and then

98
00:07:24,850 --> 00:07:26,020
an exhibition function is applied.

99
00:07:26,040 --> 00:07:30,590
And usually what you would apply here and what you'll see throughout this course is we would apply a

100
00:07:30,610 --> 00:07:32,830
rectify activation function.

101
00:07:32,830 --> 00:07:38,260
So it would look something like that we apply the rectify activation function and then from there the

102
00:07:38,260 --> 00:07:45,040
signals would be passed on to the output layer where the sigmoid activation function would be applied.

103
00:07:45,040 --> 00:07:49,000
And that would be our final output and that could predict a probability for instance.

104
00:07:49,000 --> 00:07:53,950
So this combination is gonna be quite common where in the hidden layers we apply the rectifying function

105
00:07:54,520 --> 00:07:55,780
and then an output layer.

106
00:07:55,810 --> 00:07:58,710
We apply the sigmoid function.

107
00:07:58,840 --> 00:07:59,800
So there we go.

108
00:07:59,800 --> 00:08:01,090
Hope you enjoyed this.

109
00:08:01,090 --> 00:08:05,740
DOYLE Now you are quite well versed in the four different types of activation functions and you will

110
00:08:05,740 --> 00:08:11,890
get some hands on practical experience with them throughout this course we'll be using them all over

111
00:08:11,890 --> 00:08:16,330
the place so you'll get to know them quite intimately and you should be quite comfortable with them.

112
00:08:16,480 --> 00:08:22,150
But for now this is the knowledge that you need to progress and understand what is going to be happening

113
00:08:22,180 --> 00:08:23,590
further down in this course.

114
00:08:23,890 --> 00:08:26,890
And on that note I will look forward to seeing you next time.

115
00:08:26,890 --> 00:08:28,510
Until then enjoy deep learning.
