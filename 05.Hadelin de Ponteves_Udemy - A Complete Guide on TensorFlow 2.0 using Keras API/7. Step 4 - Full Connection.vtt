WEBVTT
1
00:00:00.480 --> 00:00:05.070
Hello and welcome back to the course and deep learning today we're finally at step number four full

2
00:00:05.070 --> 00:00:06.270
connection.

3
00:00:06.270 --> 00:00:08.330
So what is this step all about.

4
00:00:08.370 --> 00:00:16.960
Well in this step we're adding a whole artificial neural network to our convolution all neural network.

5
00:00:16.970 --> 00:00:22.400
So to all of the things that we've done so far which are convolution pooling and flattening.

6
00:00:22.440 --> 00:00:27.150
Now we're adding a whole new A and then on the back of that.

7
00:00:27.300 --> 00:00:28.920
How intense is that.

8
00:00:28.920 --> 00:00:32.500
That is just that is something that is definitely something.

9
00:00:32.550 --> 00:00:35.550
And so here we've got the input layer.

10
00:00:35.550 --> 00:00:41.490
We've got a fully connected land output there and by the way the fully connected layer in the artificial

11
00:00:41.490 --> 00:00:46.230
neural net which we used to call them hidden layers and here we calling a fully connected there because

12
00:00:46.260 --> 00:00:51.690
they are hidden layers but at the same time they're a more specific type of hidden layers they are fully

13
00:00:51.690 --> 00:00:55.030
connected layer in artificial neural networks.

14
00:00:55.470 --> 00:00:59.940
Hidden layers don't have to be fully connected whereas in can evolutionary neural networks we're going

15
00:00:59.940 --> 00:01:05.700
to be using fully connected layers and that's why they're generally called fully connected layers.

16
00:01:05.730 --> 00:01:11.580
And so basically that whole column or vector of outputs that we have after the flattening we're passing

17
00:01:11.580 --> 00:01:17.980
it into the input learned here and we've got a very simplified example just for illustration purposes.

18
00:01:18.090 --> 00:01:25.950
And what's the main purpose of the artificial neural network is is to combine our features into more

19
00:01:25.980 --> 00:01:28.920
attributes that predict the classes even better.

20
00:01:28.920 --> 00:01:37.620
So we already in our vector of outputs and the flattening of the flattened result from what we've really

21
00:01:37.620 --> 00:01:44.010
done we have some features encoded in the numbers in that vector and they can really do probably a pretty

22
00:01:44.010 --> 00:01:51.810
good job at predicting what's what class we're looking at whether it's a dog or a cat or whether it's

23
00:01:51.810 --> 00:01:53.850
a tumor or not a tumor and so on.

24
00:01:53.850 --> 00:02:00.540
But at the same time we know that we have this structure called artificial neural network which is designed

25
00:02:00.540 --> 00:02:07.770
which which has a purpose of dealing with attributes and coming out or dealing with features and coming

26
00:02:07.770 --> 00:02:16.050
up with new attributes and combining attributes together to even better predict things that we're trying

27
00:02:16.050 --> 00:02:18.650
to predict and we know that from the previous part.

28
00:02:18.650 --> 00:02:20.290
So why not leverage that.

29
00:02:20.370 --> 00:02:22.710
And that's exactly what the planning here is.

30
00:02:22.710 --> 00:02:29.070
So how about we pass on those values into an artificial neural network and let it even further optimize

31
00:02:29.100 --> 00:02:30.520
everything that we're doing.

32
00:02:30.600 --> 00:02:31.830
And so that's what we're going to be doing.

33
00:02:31.830 --> 00:02:36.520
But let's look at a more realistic example because this one is a bit too simple.

34
00:02:36.540 --> 00:02:43.920
So here we've got a better looking artificial neural network where we have five attributes on the inputs

35
00:02:43.950 --> 00:02:50.550
than we have in the first hearing layer we have six neurons in the second year or in the second fully

36
00:02:50.550 --> 00:02:55.480
connected layer we have eight neurons and then we have two outputs one for dog and one for Cat.

37
00:02:55.560 --> 00:03:02.170
And so an important thing to talk of for us to talk about here is that why do we have two outputs.

38
00:03:02.220 --> 00:03:06.840
We're kind of used to having only one output in our artificial neural networks.

39
00:03:06.840 --> 00:03:13.410
Well one output is for kind of when you're predicting a numerical value when you predict when you're

40
00:03:13.500 --> 00:03:20.340
running a regression type of problem but when you're doing classification you need an output per class

41
00:03:20.380 --> 00:03:25.320
except for the exception is when you have just two classes like we have two classes here a dog and cat

42
00:03:25.620 --> 00:03:31.710
and we could have just done one output and made it a binary output and said one is a dog and zeros a

43
00:03:31.710 --> 00:03:32.440
cat.

44
00:03:32.520 --> 00:03:37.260
And that would have worked totally fine and actually in fact you'll see had lunch do that in the Practical

45
00:03:37.260 --> 00:03:39.170
Tutorials and that's how they'll be structured.

46
00:03:39.180 --> 00:03:46.020
But at the same time if you have more than two categories for instance dogs cats and birds then you

47
00:03:46.020 --> 00:03:52.380
have to have a neuron per every category and that's why we're going to practice with two categories

48
00:03:52.380 --> 00:03:58.470
in this example so that we know what to expect if we ever have more than two categories.

49
00:03:58.470 --> 00:03:59.960
And so it was going to be happening here.

50
00:03:59.970 --> 00:04:05.190
So we've already done all the groundwork we've done the convolution we've done the pulling and the flattening

51
00:04:05.550 --> 00:04:09.470
and now the information is gonna go through the artificial neural network.

52
00:04:09.480 --> 00:04:15.120
So let's have a look at how that all happens there is information going through from the very start

53
00:04:15.120 --> 00:04:22.290
from the moment when the image is processed and kind of a luke involved then pulled flattened and then

54
00:04:22.290 --> 00:04:28.620
through the artificial neural network all four steps and then a prediction is made and we'll see how

55
00:04:28.620 --> 00:04:29.160
this happens.

56
00:04:29.160 --> 00:04:30.650
In a moment we'll be very very interesting.

57
00:04:30.680 --> 00:04:32.850
But for now let's just say a prediction is made.

58
00:04:32.850 --> 00:04:35.730
And for instance 80 percent that it's a dog.

59
00:04:36.030 --> 00:04:40.850
But it turns out to be a cat and then an error is calculate a.

60
00:04:41.120 --> 00:04:47.670
Well what we used to call a cost function in a artificial neural network and we use the means squared

61
00:04:47.700 --> 00:04:51.390
error there or in common delusional neural networks.

62
00:04:51.390 --> 00:04:58.500
It's called a lost function and we use a cross entropy function for that and we'll talk about cross

63
00:04:58.500 --> 00:05:02.730
entropy and means squared errors in a separate tutorial and how all that happens.

64
00:05:02.760 --> 00:05:08.670
But for now let's just say we have a lost type of function which tells us how well our network is performing

65
00:05:08.670 --> 00:05:13.400
and we're trying to optimize optimize it or minimize that function to optimize our network.

66
00:05:13.680 --> 00:05:14.520
So we.

67
00:05:14.520 --> 00:05:19.410
The error is calculated and then it's back propagated through the network just like we had in the artificial

68
00:05:19.410 --> 00:05:26.640
neural networks is back propagated and the some things are adjusted in the network to help optimize

69
00:05:26.940 --> 00:05:31.380
the performance and the things that are just that are as usual the weights in the artificial neural

70
00:05:31.380 --> 00:05:35.110
network parts of the the blue lines that you see here the synopses.

71
00:05:35.280 --> 00:05:41.600
Then also another thing that is adjusted is the feature detectors.

72
00:05:41.610 --> 00:05:46.620
So we know that we're looking for features but what if we're looking for the wrong features what if

73
00:05:47.070 --> 00:05:51.510
this didn't work out because the features are incorrect and so the feature detectors those remember

74
00:05:51.510 --> 00:06:00.720
those little matrices that we had that the three by three matrices they are adjusted so that maybe next

75
00:06:00.720 --> 00:06:03.810
time it'll be better and let's see what happens type of thing.

76
00:06:03.820 --> 00:06:11.010
And but of course it's all done with a lot of science in the background of a lot of math and it's all

77
00:06:11.010 --> 00:06:14.520
done through a gradient gradient descent of back propagation.

78
00:06:14.520 --> 00:06:20.750
So it's all it's all not just random perturbations it's actually very thought through how it's done.

79
00:06:21.120 --> 00:06:27.570
But nevertheless the feature detectors are adjusted the weights are adjusted and this whole process

80
00:06:27.570 --> 00:06:33.360
happens again and then again the errors back propagate and this keeps going on and on and on and that's

81
00:06:33.360 --> 00:06:37.890
how our network is optimized that's how our network trains on the data.

82
00:06:37.890 --> 00:06:42.180
So the important important thing here is that the data goes through the whole network from the very

83
00:06:42.180 --> 00:06:46.700
start to the very end then the errors compared.

84
00:06:47.240 --> 00:06:49.910
So the error is calculated and then is back propagate.

85
00:06:49.920 --> 00:06:56.460
So same story as with artificial neural networks just a bit longer because of that hole for the first

86
00:06:56.460 --> 00:06:58.790
three steps that we really had.

87
00:06:58.980 --> 00:07:04.380
And now let's have a look at the interesting by the really interesting part how do these two classes

88
00:07:04.380 --> 00:07:07.080
work because or how do these two output neurons work.

89
00:07:07.080 --> 00:07:13.250
Because before we've always kind of had one output neuron what happens when we have two how does.

90
00:07:13.470 --> 00:07:17.610
How does a situation of classification of images play out.

91
00:07:17.610 --> 00:07:22.020
Well let's start with the top neuron first going to start with the dog.

92
00:07:22.020 --> 00:07:22.750
How do we.

93
00:07:22.830 --> 00:07:29.730
The main purpose what we need to do first is we need to understand what weights to assign to all of

94
00:07:29.730 --> 00:07:37.350
these synopses that connect the dog so that we know which of the previous neurons are actually important

95
00:07:37.350 --> 00:07:38.850
for the dog and let's see how that is done.

96
00:07:38.880 --> 00:07:46.290
So let's say hypothetically we've got these numbers in our previous layer of previous fully connected

97
00:07:46.410 --> 00:07:47.970
in the final fully connected layer.

98
00:07:48.060 --> 00:07:53.460
And again these numbers can be absolutely anything they don't have to be that they there can be any

99
00:07:53.460 --> 00:08:00.450
numbers but just for argument's sake we're going to agree that we are looking specifically at numbers

100
00:08:00.450 --> 00:08:01.530
between 0 and 1.

101
00:08:02.250 --> 00:08:09.750
So it's easier for us to argue these things and understand and one means that that neuron was very confident

102
00:08:09.780 --> 00:08:15.900
that it found a certain feature and 0 is going to mean that that neuron didn't find a feature is looking

103
00:08:15.900 --> 00:08:23.280
for so because at the end of the day these neurons are like anything else on this from on this left

104
00:08:23.280 --> 00:08:25.370
side is just looking at features at an image.

105
00:08:25.410 --> 00:08:30.990
This is already very very processed but still it's detecting a certain feature or a combination of features

106
00:08:31.410 --> 00:08:34.530
on the image right before we in the convulse step.

107
00:08:34.530 --> 00:08:39.000
We had kind of recognizable features in the pools that they're less recognizable then they become even

108
00:08:39.000 --> 00:08:42.470
less recognizable visible in the flattened image and then they get combined and so on.

109
00:08:42.510 --> 00:08:48.650
But nevertheless this we're talking about here certain features that our present image or their combination

110
00:08:48.690 --> 00:08:54.450
so and one which has been passed and this is important has been passed to both the dog and the cat at

111
00:08:54.450 --> 00:08:57.050
the same time to both the output neurons.

112
00:08:57.060 --> 00:09:06.120
So one means that for us for our argument it means that this this neuron has is firing up its really

113
00:09:06.120 --> 00:09:11.780
rapidly detecting that feature that you know might be an eyebrow it might be detecting this eyebrow

114
00:09:11.790 --> 00:09:17.840
for again for simplicity's sake is it this eyebrow and communicate that to the dog neuron to the cartoon

115
00:09:18.120 --> 00:09:22.110
neuron saying I can see my eyebrow I can see my eyebrow and then it's up to the dog and the cat neuron

116
00:09:22.380 --> 00:09:25.770
to understand what that means for them right.

117
00:09:25.830 --> 00:09:30.790
And so in this case which neurons are firing up these three neurons are firing up the eyebrow and that

118
00:09:30.790 --> 00:09:37.080
say the nose is saying I can see I can see a big nose and I can see floppy years so it and it's saying

119
00:09:37.080 --> 00:09:42.900
that to the dog and to the cat and then what the dog and then what happens is we know that this is a

120
00:09:42.900 --> 00:09:43.380
dog.

121
00:09:43.410 --> 00:09:49.740
So the dog neuron knows that the answer is it is actually a dog because at the end we are comparing

122
00:09:49.740 --> 00:09:53.580
to the picture or to the label on the picture and when it knows the dog.

123
00:09:53.610 --> 00:09:56.270
So basically the dog neuron is going to say Aha.

124
00:09:56.280 --> 00:09:58.740
So I should be triggered in this case.

125
00:09:58.770 --> 00:10:04.560
So these are my neurons they are telling this signal that they're sending to both to me to the dog and

126
00:10:04.560 --> 00:10:08.930
to the cat is actually a indication for me that it is a dog.

127
00:10:08.940 --> 00:10:13.890
And throughout these lots and lots and lots of iterations of this happens many times the dog will learn

128
00:10:13.890 --> 00:10:19.610
that these neurons do indeed fire up when the feature belongs to a dog.

129
00:10:19.610 --> 00:10:24.210
On the other hand the cat neuron will know that its not a cat and it will know that this feature is

130
00:10:24.210 --> 00:10:28.180
firing up and this neuron is telling me it can see floppy years floppy ears sloppy ears.

131
00:10:28.290 --> 00:10:30.990
But at the same time it's not a cat.

132
00:10:31.020 --> 00:10:35.650
So basically to me that's a signal that I should ignore this neuron.

133
00:10:35.760 --> 00:10:41.280
And the more that happens the more the cat neuron is going to ignore this neuron about the floppy ears

134
00:10:42.390 --> 00:10:47.950
and so basically that that's how through lots and lots of iterations.

135
00:10:48.030 --> 00:10:50.060
If this happens often so this is just one example.

136
00:10:50.070 --> 00:10:54.120
But if this happens often maybe a one maybe a zero point eight zero point nine maybe sometimes it won't

137
00:10:54.120 --> 00:10:54.370
fire.

138
00:10:54.390 --> 00:11:01.110
But overall on average this neuron is lighting up very often when it is indeed a dog.

139
00:11:01.110 --> 00:11:05.870
The dog neuron will start attributing higher importance to this neuron.

140
00:11:05.880 --> 00:11:09.930
And so there we go that's that's how we're going to signify that we're going to say that these three

141
00:11:09.930 --> 00:11:15.990
neurons through this it's a relative process with me and with many many many many samples and many many

142
00:11:15.990 --> 00:11:21.600
epochs remember so sample is growing your data set an epoch is when you go through your whole dataset

143
00:11:21.600 --> 00:11:28.950
again and again and again through lots and lots of iterations this dog neuron learned that this eyebrow

144
00:11:28.950 --> 00:11:39.120
neuron and this big nose neuron and this floppy ear neuron they all seem to really contribute very well

145
00:11:39.120 --> 00:11:44.380
to the classification of what it's looking for and which is a dog.

146
00:11:44.430 --> 00:11:45.690
So that's how it works.

147
00:11:45.690 --> 00:11:55.080
And again these years and nose and eyebrows those are very very approximate or like very far fetched

148
00:11:55.080 --> 00:12:01.560
examples because by this stage in this whole convolution conventional neural network it is completely

149
00:12:01.560 --> 00:12:07.320
unrecognizable what they're looking for but at the same time it is something in the features of dogs

150
00:12:07.350 --> 00:12:08.900
or cats or whatever you classify.

151
00:12:09.360 --> 00:12:13.650
And then so let's move on to next and now we're going to look at the cat neuron but these begin to remember

152
00:12:13.650 --> 00:12:19.350
that these weights are you know they how we've sorted out the dogs so the dog is kind of like pretty

153
00:12:19.350 --> 00:12:22.690
much ignoring all these other neurons One two three four or five.

154
00:12:22.710 --> 00:12:26.290
But it's really paying attention to what these three neurons are saying.

155
00:12:26.520 --> 00:12:28.400
Now what is the cat's listening to.

156
00:12:28.410 --> 00:12:35.190
Well whenever it is actually a cat right this is this is an example of a situation when it's actually

157
00:12:35.190 --> 00:12:35.520
a cat.

158
00:12:35.550 --> 00:12:42.390
So you'll see that this these three neurons zero point nine zero point nine and one they're saying something

159
00:12:42.420 --> 00:12:44.540
they're saying something to both the dog and the cat.

160
00:12:44.550 --> 00:12:49.570
And this is again important remember so this output signal goes both ways it's the same right.

161
00:12:49.860 --> 00:12:54.810
It's saying one to the dog's saying one to the cat but then it's up to the dog into the cat to decide

162
00:12:54.810 --> 00:13:00.350
to whether to take into account that signal and learn from it or not.

163
00:13:00.450 --> 00:13:05.480
And both the dog and the cat can see that this is a photo I should should've put a photo of a cat here.

164
00:13:05.550 --> 00:13:09.810
But basically imagine a photo of a cat both a dog and a cat can see that this is actually a cat.

165
00:13:10.140 --> 00:13:12.950
So basically the dog is like OK.

166
00:13:12.990 --> 00:13:23.400
So these whiskers and these pointy triangle ears and this small size I guess or oh maybe these this

167
00:13:23.400 --> 00:13:29.340
type you know how cats have these things in their eyes their eyes are like little they're not circles

168
00:13:29.340 --> 00:13:33.300
their lines or something like cat eyes.

169
00:13:33.300 --> 00:13:37.380
Basically these cat eyes they're definitely not working for me.

170
00:13:37.410 --> 00:13:43.110
They're not helping me out predict because every time these neurons light up the prediction is not what

171
00:13:43.110 --> 00:13:44.040
I'm looking for.

172
00:13:44.190 --> 00:13:49.860
On the other hand the cat is like hmm that's interesting every time these this one lights up it's or

173
00:13:49.860 --> 00:13:51.440
most of the time it lights up.

174
00:13:51.570 --> 00:13:55.260
It matches my expectation it matches what I'm looking for.

175
00:13:55.260 --> 00:14:00.180
Okay I'm gonna listen to this guy more than this one this one same thing every time it lights up or

176
00:14:00.390 --> 00:14:02.760
most of the times it lights up.

177
00:14:02.760 --> 00:14:09.090
I happen to get a good I happened to be rewarded for my prediction because I get it right.

178
00:14:09.090 --> 00:14:09.930
It's a cat Okay.

179
00:14:09.990 --> 00:14:11.360
So I'm going to listen to him more.

180
00:14:11.400 --> 00:14:12.090
You know this one.

181
00:14:12.090 --> 00:14:17.090
Useless to me because he's not actually you know like his.

182
00:14:17.090 --> 00:14:19.880
He's not even lighting up it's a cat but it's He's not lighting up.

183
00:14:19.890 --> 00:14:20.990
So the opposite is happening.

184
00:14:21.000 --> 00:14:22.040
And this one as well.

185
00:14:22.050 --> 00:14:25.940
It's a cat but he's not lighting up so I'm not gonna listen to him but this one when he.

186
00:14:25.970 --> 00:14:26.890
When.

187
00:14:26.890 --> 00:14:29.790
What is this the eyes the cat eyes light up.

188
00:14:29.790 --> 00:14:30.570
We can see.

189
00:14:30.570 --> 00:14:31.800
I can see that it's a cat.

190
00:14:31.800 --> 00:14:36.420
It matches most of the time so I'm gonna learn from that and I'm going to listen to these three guys

191
00:14:36.930 --> 00:14:38.700
more often than not.

192
00:14:38.700 --> 00:14:44.760
And so basically the cat is listening to these three and it's ignoring the other five and that is how

193
00:14:45.270 --> 00:14:53.640
these final neurons learn which neurons in the final fully connected lair to listen to.

194
00:14:53.640 --> 00:14:59.640
So the output neurons learn which of the fully which of the final fully connected neurons to listen

195
00:14:59.640 --> 00:15:02.770
to and that's how they understand.

196
00:15:02.780 --> 00:15:08.900
Basically that's how the features are propagated through the network and conveyed to the output.

197
00:15:08.910 --> 00:15:14.010
And so even though these features of course don't have that much meaning to them like floppy ears or

198
00:15:14.010 --> 00:15:18.490
whiskers at the same time they do have some distinctive.

199
00:15:18.660 --> 00:15:23.820
They are a distinctive feature of that specific class and that's how the network is trained because

200
00:15:23.820 --> 00:15:29.670
we also during remember during the back propagation process we also adjust the feature detectors.

201
00:15:29.670 --> 00:15:36.750
So if a feature is useless to the output it's going to is going to probably be disregarded because this

202
00:15:36.750 --> 00:15:40.950
doesn't happen until one or two to is this happens through thousands and thousands of iterations.

203
00:15:40.980 --> 00:15:46.560
So with time a future that is useless to the network is going to be disregarded and replaced with future

204
00:15:46.560 --> 00:15:47.150
is useful.

205
00:15:47.150 --> 00:15:53.520
And so at the end of the day in this final layer of neurons you are likely to have lots of features

206
00:15:53.520 --> 00:16:00.090
or combinations of features from the image that are indeed representative or descriptive of dogs and

207
00:16:00.090 --> 00:16:06.600
cats and so then once your network is trained up then this is how it's applied.

208
00:16:06.600 --> 00:16:09.270
So this is the next step like we've already trained up our network will this happen.

209
00:16:09.270 --> 00:16:12.960
This is what happens when the this network is applied.

210
00:16:12.960 --> 00:16:15.960
So let's say we pass on an image of a dog.

211
00:16:16.350 --> 00:16:18.970
The values are propagated through a network.

212
00:16:18.990 --> 00:16:20.380
We get certain values.

213
00:16:20.550 --> 00:16:26.660
And so this time the dog and the cat neurons don't know they don't have the image of the dog here.

214
00:16:26.670 --> 00:16:28.410
They don't know that it's a dog or a cat.

215
00:16:28.410 --> 00:16:35.640
They have no idea what it is but they have learned to listen to what is being shown here right.

216
00:16:35.640 --> 00:16:40.110
They have learned to listen to dog ear and listens to these three neurons the cat neuron listens to

217
00:16:40.110 --> 00:16:40.780
these three.

218
00:16:40.890 --> 00:16:43.730
And so the dog neuron looks at one two three and says aha.

219
00:16:43.740 --> 00:16:44.810
These are pretty high.

220
00:16:44.880 --> 00:16:50.390
So my probability is gonna be high that it's a dog the cat neuron looks at these three and says okay

221
00:16:50.420 --> 00:16:50.840
these.

222
00:16:50.920 --> 00:16:53.510
This one is pretty high but these are pretty low.

223
00:16:53.610 --> 00:16:54.270
Interesting.

224
00:16:54.270 --> 00:16:56.890
So my probability is gonna be zero point zero five.

225
00:16:56.990 --> 00:17:00.060
And and then and that's and that's where you get your prediction.

226
00:17:00.060 --> 00:17:04.470
So then your first choice for this neural network is dog.

227
00:17:04.470 --> 00:17:05.670
Second choice is Cat.

228
00:17:05.700 --> 00:17:06.870
And that's pretty much it.

229
00:17:06.870 --> 00:17:11.530
So the answer is dog and same thing happens when you pass an image of a cat.

230
00:17:11.880 --> 00:17:16.540
You get new values and you can see that even though this one's high these ones are low.

231
00:17:16.710 --> 00:17:18.900
And for the cat this one's high.

232
00:17:18.900 --> 00:17:23.900
This one's high and this one's a bit low so the probability here might not be as great as previously.

233
00:17:23.910 --> 00:17:26.540
But still you can see that it's a cat of seventy nine percent.

234
00:17:26.880 --> 00:17:30.030
And so therefore the neural network is going to vote that it's a cat.

235
00:17:30.220 --> 00:17:33.270
And so basically all the neural network is going to conclude that it's a cat.

236
00:17:33.270 --> 00:17:36.240
Voting is a term that is used for these guys.

237
00:17:36.240 --> 00:17:42.800
So these neurons in the final fully connected layer they get to vote and these are their votes.

238
00:17:42.810 --> 00:17:47.120
And again we are just for argument's sake putting values between 0 and 1 here.

239
00:17:47.120 --> 00:17:54.450
These could be any values but they get to vote and then these weights are the importance of their votes.

240
00:17:54.480 --> 00:18:00.480
So this is these these purple weights are how the dog neuron views their votes.

241
00:18:00.480 --> 00:18:04.760
How much importance is it assigns to these neurons and to those votes.

242
00:18:04.770 --> 00:18:12.690
And this is how much importance the cat's neuron assigns to these votes are the votes of these neurons.

243
00:18:12.690 --> 00:18:16.580
And so these neurons vote the dog and the cats based on their learned weights.

244
00:18:16.590 --> 00:18:22.050
They decide who to listen to and then they make their predictions and then hold neural network concludes

245
00:18:22.440 --> 00:18:26.930
that this is in this case a cat and then that's and then that's your conclusion.

246
00:18:26.940 --> 00:18:35.000
And that's how you get images like this where you have a cheetah and then you have a cheetah class you

247
00:18:35.220 --> 00:18:36.770
like a high high probability.

248
00:18:36.770 --> 00:18:41.310
So this is you know the probability that the network has predicted and these are lost but these still

249
00:18:41.310 --> 00:18:46.650
exist because there's still kind of like a small chance the other neurons are also listening to their

250
00:18:46.860 --> 00:18:50.520
voters and they're saying oh maybe it's actually a leopard and a bullet train.

251
00:18:50.520 --> 00:18:52.400
Very very probable I hear a scissors.

252
00:18:52.410 --> 00:18:57.540
You know this one one but hand glass was very close second and in frame past stethoscope because you

253
00:18:57.540 --> 00:19:02.940
could see like these guys this this neuron the scissors neuron the output surfaces as neuron listened

254
00:19:03.030 --> 00:19:09.390
to its voters and it had the predominant mode overall but then the hand Glass had a good outcome as

255
00:19:09.390 --> 00:19:10.110
well.

256
00:19:10.140 --> 00:19:16.540
So there we go that's how the full connection works and how this is all how this all plays out together.

257
00:19:16.620 --> 00:19:18.720
I hope you enjoyed today's tutorial.

258
00:19:18.750 --> 00:19:22.790
We're going to summarize all of this in the summary as well and I'll see you next time.

259
00:19:22.800 --> 00:19:24.660
Until then enjoy deep learning.
