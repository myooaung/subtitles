WEBVTT
1
00:00:00.660 --> 00:00:02.890
Hi guys welcome to this new tutorial.

2
00:00:02.910 --> 00:00:03.840
And here we go.

3
00:00:03.840 --> 00:00:10.680
We are ready to start building our record a new network with tons of flow 2.0 and as usual we start

4
00:00:10.710 --> 00:00:12.590
by defining the moral.

5
00:00:12.600 --> 00:00:18.870
Still with the sequential class because a record Renewal Network is still a sequence of layers and still

6
00:00:18.870 --> 00:00:20.870
not a computational graph.

7
00:00:21.000 --> 00:00:27.330
So again we initialize our aunt and model with the sequential class taken from the caris library through

8
00:00:27.330 --> 00:00:34.530
tensor flow and once again we're going to use the add method to add the different layers one by one

9
00:00:35.100 --> 00:00:40.290
in the first layer we add is the embedding layer so before adding it.

10
00:00:40.500 --> 00:00:43.010
Let me explain what is the embedding layer.

11
00:00:43.110 --> 00:00:50.190
The embedding layer is actually a layer used to create a word vector representation of the words you

12
00:00:50.190 --> 00:00:57.360
know the words in the reviews so that instead of using pre trained word vectors as if you had the reviews

13
00:00:57.420 --> 00:01:04.260
in vectors of words with the padding included Well we're gonna use what we call this embedding layer

14
00:01:04.620 --> 00:01:13.380
to train the word vectors in a large matrix and this large matrix will be a matrix where each row corresponds

15
00:01:13.380 --> 00:01:14.100
to a word.

16
00:01:14.160 --> 00:01:22.080
You know all the 20000 words in r reviews and the columns are actually encoding the word with what we

17
00:01:22.080 --> 00:01:25.770
call a representation of the word in the dataset vocabulary.

18
00:01:26.190 --> 00:01:33.570
So by using this embedding layer we're going to learn those word representations jointly with the weights

19
00:01:33.720 --> 00:01:35.280
in the network itself.

20
00:01:35.280 --> 00:01:35.550
Right.

21
00:01:35.550 --> 00:01:42.720
So this matrix will contain some encoded relationships between the words so that the record Renewal

22
00:01:42.750 --> 00:01:49.320
Network can learn these relationships and predict in the end if a NASA station of word leads to a positive

23
00:01:49.440 --> 00:01:50.700
or negative review.

24
00:01:50.760 --> 00:01:51.000
Right.

25
00:01:51.000 --> 00:01:58.050
So this was just a little recap on how embedding works but make sure to re study this in more details

26
00:01:58.050 --> 00:02:03.750
of this is the first time you hear about embedding because this is quite fundamental when doing natural

27
00:02:03.750 --> 00:02:05.330
language processing.

28
00:02:05.340 --> 00:02:11.370
All right so we start with this emerging layer and so in the add method the layer we're going to add

29
00:02:11.580 --> 00:02:17.100
is called embedding it's going to be an object of the embedding class which represents this embedding

30
00:02:17.100 --> 00:02:22.130
layer itself and to create this well we have to input a couple of arguments.

31
00:02:22.230 --> 00:02:24.510
The first one is the input dimension.

32
00:02:24.510 --> 00:02:30.600
You know before we create this embedding matrix and the input dimension is simply the number of words

33
00:02:30.630 --> 00:02:38.100
because remember in this matrix each row corresponds to each of the 20000 words among all our reviews.

34
00:02:38.130 --> 00:02:42.440
Then the second argument is output dim and you might guess what it is.

35
00:02:42.440 --> 00:02:48.960
It is of course the number of columns that are going to embed each word into this large representation

36
00:02:48.960 --> 00:02:51.510
of words in our embedded matrix.

37
00:02:51.510 --> 00:02:56.490
And so we're going to choose one hundred and twenty eight columns to represent the words you know to

38
00:02:56.490 --> 00:02:57.780
encode the words.

39
00:02:57.990 --> 00:03:06.390
And so well you will get an embedding matrix composed of 128 columns and then to this embedding class

40
00:03:06.420 --> 00:03:13.140
we also have to input the input shape which is of course the shape of extreme right extra and contains

41
00:03:13.260 --> 00:03:14.620
all the input reviews.

42
00:03:14.850 --> 00:03:21.210
After of course the batting was applied and to get that shape of extreme which is expected by the embedding

43
00:03:21.210 --> 00:03:21.640
class.

44
00:03:21.780 --> 00:03:27.890
Well you get it through the shape attribute which is actually a tensor of several informations.

45
00:03:27.960 --> 00:03:32.870
And the shape itself is actually the second element of this tensor meaning of index 1.

46
00:03:32.940 --> 00:03:36.230
All right and that's how you create your end burning layer.

47
00:03:36.420 --> 00:03:42.540
Then after the embedding layer comes the Elysium layer which is what will be used to understand the

48
00:03:42.540 --> 00:03:46.280
relationships between the different elements of a sequence.

49
00:03:46.380 --> 00:03:52.140
And in our case the reviews while the Elysium layer will be used to understand the different relationships

50
00:03:52.170 --> 00:03:56.340
between the words and the reviews and so well to build an Elysium layer.

51
00:03:56.460 --> 00:03:59.400
You have to specify two compulsory elements.

52
00:03:59.400 --> 00:04:05.250
First one is the number of units which are the Elysium themselves inside your Elysium layer.

53
00:04:05.370 --> 00:04:08.940
And we're going to choose 1 128 LACMA cells.

54
00:04:09.090 --> 00:04:15.750
And the second one is the activation function which as we saw in an Elsom layer is most of the time

55
00:04:15.870 --> 00:04:19.450
a hyperbolic tangent which has a parameter is named an H.

56
00:04:19.530 --> 00:04:26.190
And so there you go the way you add your new Elysium there is by using of course add method called from

57
00:04:26.190 --> 00:04:32.390
the moral and then inside you're going to take the Elysium class from the layers from carers from tensor

58
00:04:32.410 --> 00:04:32.930
flow.

59
00:04:33.150 --> 00:04:37.710
And this Elysium class will exactly take the two parameters given here.

60
00:04:37.900 --> 00:04:46.620
One hundred and twenty eight units meaning 128 Elysium cells and and hyperbolic tangent activation function.

61
00:04:46.800 --> 00:04:47.070
Right.

62
00:04:47.070 --> 00:04:54.930
So very easy to add an Elysium layer and then finally well since we have the embedding layer that creates

63
00:04:55.050 --> 00:05:02.020
this representation of words and the Elysium layer that learns the Asian ships between the words Well

64
00:05:02.090 --> 00:05:06.790
we are ready to add the output layer which will contain the final prediction.

65
00:05:06.790 --> 00:05:12.900
And since the final prediction is just a single number you know zero or one where zero means to reduce

66
00:05:12.940 --> 00:05:15.910
negative and one means there is false positive.

67
00:05:15.910 --> 00:05:20.660
Well we'll only have one output neuron which will contain that prediction.

68
00:05:20.770 --> 00:05:27.970
And once again we use the sigmoid activation function in order to get the probabilities of 0 and 1.

69
00:05:27.970 --> 00:05:33.790
You know we'll get the probability that the output is 1 meaning the probability that the review is positive

70
00:05:34.030 --> 00:05:36.990
and that we can get with the sigmoid activation function.

71
00:05:37.600 --> 00:05:44.810
And well you know very well how to add an output layer since it is fully connected to the previous layer.

72
00:05:44.830 --> 00:05:50.260
Well we used to dance class to establish these full connections and the dance classes still taken from

73
00:05:50.260 --> 00:05:52.810
the latest module by carers by tend to flow.

74
00:05:52.810 --> 00:05:59.860
And once again we input the number of neurons in this output layer which as we saw is one and the sigmoid

75
00:05:59.920 --> 00:06:03.720
activation function to get a probabilistic output.

76
00:06:03.940 --> 00:06:04.860
And there you go.

77
00:06:04.900 --> 00:06:05.520
That's it.

78
00:06:05.740 --> 00:06:08.530
That's how you build a record a new network.

79
00:06:08.560 --> 00:06:15.040
And now we're ready for the next step which is to train it by first compiling it with an optimizer and

80
00:06:15.040 --> 00:06:21.760
a loss and a metric and then training it on a certain number of epochs and eventually evaluating it

81
00:06:21.940 --> 00:06:23.220
on the test set.

82
00:06:23.230 --> 00:06:25.840
So I look forward to showing you the final results.

83
00:06:25.850 --> 00:06:26.770
They're really good.

84
00:06:26.770 --> 00:06:28.750
It will be actually hard for you to improve that.

85
00:06:28.960 --> 00:06:30.160
So that's quite a challenge.

86
00:06:30.190 --> 00:06:35.660
But now you start getting a couple of tools to improve the performance of a model.

87
00:06:35.660 --> 00:06:41.010
So let's see what you figure out and I'll see you in the next trial to train and evaluate the model.
