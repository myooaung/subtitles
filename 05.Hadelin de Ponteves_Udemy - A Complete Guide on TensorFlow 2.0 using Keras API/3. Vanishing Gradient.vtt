WEBVTT
1
00:00:00.500 --> 00:00:03.290
Hello and welcome back to the course on deep learning.

2
00:00:03.290 --> 00:00:07.940
I hope you enjoyed the introduction into our ends and today right away off the bat.

3
00:00:07.940 --> 00:00:12.590
We're going to jump into a huge problem that exists with our own ends.

4
00:00:12.590 --> 00:00:13.000
All right.

5
00:00:13.010 --> 00:00:17.280
So what is this problem called The Vanishing gradient.

6
00:00:17.300 --> 00:00:22.070
It was first discovered by Sepp hockey writer.

7
00:00:22.070 --> 00:00:24.980
And I hope I'm pronouncing it right.

8
00:00:25.010 --> 00:00:31.170
I know we have students from Germany so you can comment if if the pronunciation is incorrect but e said

9
00:00:31.460 --> 00:00:43.760
Sepp or Joseph is his full name is a genius scientist who back in the 90s while he was still not not

10
00:00:43.850 --> 00:00:45.190
a professor this is a recent photo.

11
00:00:45.200 --> 00:00:48.780
So he was much younger when he actually came up with this concept.

12
00:00:48.800 --> 00:00:51.190
He found that there is a big big problem.

13
00:00:51.230 --> 00:00:57.410
And we'll talk about the problem solved just now I just wanted to point out who are the people that

14
00:00:57.410 --> 00:01:04.160
spotted this and basically as you'll learn from the further Charles Sepp is one of the founding people

15
00:01:04.280 --> 00:01:10.520
in the modern way that we use our own ends and Alistair MS and we'll talk about that for the dumb but

16
00:01:10.760 --> 00:01:16.150
this is hip hop writer so I just want to make sure you know who is behind all this.

17
00:01:16.260 --> 00:01:19.110
And the second person is your show banjo.

18
00:01:19.160 --> 00:01:24.250
He's a professor at the University of Montreal.

19
00:01:24.260 --> 00:01:29.390
He also discovered that this this problem I think is a bit later in 1994.

20
00:01:29.390 --> 00:01:32.420
So Sepp was discovered this in 1991.

21
00:01:32.420 --> 00:01:35.480
Joshua wrote about this in 1994.

22
00:01:35.480 --> 00:01:42.160
But again this is another person driving this whole pushing the envelope in the space of our own ends.

23
00:01:42.170 --> 00:01:48.980
And if you go to the University of Maryland you look up your shoe your show's profile you will find

24
00:01:49.070 --> 00:01:57.710
that your show has over 500 research papers and by the way there is your show banjo just hanging out

25
00:01:57.710 --> 00:02:00.320
with young look on as you can see they all know each other.

26
00:02:00.320 --> 00:02:06.110
It's a very tight knit community and it does sound like a conspiracy these people that are driving or

27
00:02:06.110 --> 00:02:08.650
pushing the envelope in terms of deep learning.

28
00:02:08.750 --> 00:02:14.300
This is just like a select group of people who are all all in it together they all know what's going

29
00:02:14.300 --> 00:02:14.570
on there.

30
00:02:14.590 --> 00:02:18.750
All Been doing it since the 80s or 90s and now it's all popping out into the world.

31
00:02:19.310 --> 00:02:25.040
So that's just to give you a quick idea of who's behind all this and today and now we're proceeding

32
00:02:25.040 --> 00:02:28.060
to the vanishing period problem itself.

33
00:02:28.070 --> 00:02:33.470
So as you remember this is the gradient descent algorithm we're trying to find the look at the global

34
00:02:33.470 --> 00:02:39.740
minimum of your cost function and that's gonna be the optimal solution optimal setup for your neural

35
00:02:39.740 --> 00:02:48.680
network as you also recall your gradient or your information travels through your neural network to

36
00:02:48.680 --> 00:02:54.650
your answer to get your output and then the error is calculated and is propagated back through a network

37
00:02:54.650 --> 00:02:56.810
to update the weights.

38
00:02:56.810 --> 00:02:58.510
And here we've got some way to announce.

39
00:02:58.610 --> 00:03:06.590
So what happens in a recurrent neural network is a similar thing but here you've got a bit more going

40
00:03:06.590 --> 00:03:06.770
on.

41
00:03:06.770 --> 00:03:06.980
Right.

42
00:03:06.980 --> 00:03:13.340
So when your information travels through the network it travels like that you've got lots of it travels

43
00:03:13.340 --> 00:03:18.530
through time and information from previous time points goes keeps coming through the network and remember

44
00:03:18.530 --> 00:03:23.360
that every node here it's always very important to remember for neural networks that every single node

45
00:03:23.360 --> 00:03:31.190
here is not just a node it's a representation of a whole layer of nodes remember we're looking at from

46
00:03:31.190 --> 00:03:38.330
like from the top or from the bottom they are they actually extend through this child down there into

47
00:03:38.720 --> 00:03:45.140
into this presentation you can see there is lots more neurons behind the ones that we can actually see

48
00:03:45.170 --> 00:03:46.970
because each one represents a layer.

49
00:03:47.020 --> 00:03:48.440
They're important to remember that.

50
00:03:48.440 --> 00:03:54.110
And so at each point in time you can calculate the your cost function or your error.

51
00:03:54.110 --> 00:03:59.960
So basically your cost function compares your output which is in the red circle to your desired output

52
00:03:59.960 --> 00:04:02.450
to what you should be getting.

53
00:04:02.450 --> 00:04:03.950
And this happens during the training.

54
00:04:04.340 --> 00:04:08.750
And so you have these values for throughout the time series.

55
00:04:08.750 --> 00:04:15.950
So for every single one of these red circles you can calculate the cost function and let's focus on

56
00:04:15.950 --> 00:04:17.690
one just to understand what's going on.

57
00:04:17.690 --> 00:04:20.840
Let's focus on this one specifically at time T.

58
00:04:20.960 --> 00:04:27.410
So you've calculated the cost function Epsilon T and now you want to propagate your cost function back

59
00:04:27.410 --> 00:04:28.120
through the network.

60
00:04:28.130 --> 00:04:29.110
How is this going to look.

61
00:04:29.120 --> 00:04:30.430
Because you need to update the weight.

62
00:04:30.440 --> 00:04:35.390
So every single neuron which participated in the calculation of the output associated with this cost

63
00:04:35.390 --> 00:04:42.470
function should be should have their weight updated too in order to help it better calculate the output

64
00:04:42.500 --> 00:04:44.030
to minimize that error.

65
00:04:44.030 --> 00:04:50.240
And the thing here is that it's not just the neurons in directly below Epsilon see directly below this

66
00:04:50.240 --> 00:04:52.580
red circle is all of the neurons that contributed.

67
00:04:52.580 --> 00:04:53.810
And there's many more of them.

68
00:04:53.840 --> 00:04:59.150
There's all these neurons as far back as you go depending on how many times steps you take you might

69
00:04:59.150 --> 00:05:04.870
take one somewhere take two you to take 50 ten steps depending on how you set up your network.

70
00:05:04.980 --> 00:05:10.980
And this is where the problem lies that you have to update or you have to propagate all the way back

71
00:05:11.640 --> 00:05:16.410
through time to these Eurozone and we talk when we talk about time it's not that the problem is that

72
00:05:16.410 --> 00:05:17.490
we can't travel through time.

73
00:05:17.490 --> 00:05:20.240
Not at all that we've unraveled this network.

74
00:05:20.250 --> 00:05:22.680
So this whole propagation can be facilitated.

75
00:05:22.830 --> 00:05:27.990
The problem lies in something else and as much as I don't like putting math on the slide on intuition

76
00:05:27.990 --> 00:05:31.380
slides we're not going to go through this math but I'll point out one thing here.

77
00:05:31.380 --> 00:05:37.740
So this is the math behind answers and we'll definitely direct you to more additional reading which

78
00:05:37.740 --> 00:05:40.600
you can do to upskilling on all of these maths.

79
00:05:40.740 --> 00:05:47.730
So here we've got W rec and W rec stands for weight recurring and that is the weight that is used to

80
00:05:48.120 --> 00:05:57.690
connect the hidden layers to themselves in the unrolled temporal loop and as you can see here on the

81
00:05:57.690 --> 00:06:03.120
left you're in order to get from X T minus three from this layer.

82
00:06:03.120 --> 00:06:08.810
Remember this is a layer to X minus two you need to apply w rank then from here to here.

83
00:06:08.820 --> 00:06:09.480
Apply W.

84
00:06:09.690 --> 00:06:16.530
And in simple terms in into intuitive terms what you're doing is you're simply multiplying the values

85
00:06:16.530 --> 00:06:18.630
here as you remember to get from one layer.

86
00:06:18.630 --> 00:06:23.010
Next we multiply the output by the weight and then we get to the next layer and then we multiply the

87
00:06:23.010 --> 00:06:24.400
output by the weight and get to next.

88
00:06:24.420 --> 00:06:25.890
Then we multiply output from here.

89
00:06:25.920 --> 00:06:31.320
By the way to get here and the thing here is that we're multiplying we're the same exact weight multiple

90
00:06:31.320 --> 00:06:35.980
time many times as many times as we need to go through this temporal loop and this can be set.

91
00:06:36.120 --> 00:06:40.740
You can set this in your network do you want to do it once you want to look back one step you look back

92
00:06:40.740 --> 00:06:46.650
three steps you'll look back 50 steps but as many times as we do it we have to multiply by the weight

93
00:06:47.100 --> 00:06:55.350
and this is where the problem arises because when you multiply by something small your value decreases

94
00:06:55.350 --> 00:07:01.920
very quickly and this is the multiplication comes from this P here and you can see that Piers stands

95
00:07:01.920 --> 00:07:06.000
for multiplication so we have to multiply and that's where it's representing that as far as you go back

96
00:07:06.030 --> 00:07:12.960
you're multiplying and as you remember weights are assigned at the start of your neural network you

97
00:07:13.020 --> 00:07:18.810
as you see in the practical materials your weight are assigned at a random value but random values clusters

98
00:07:18.810 --> 00:07:24.540
are and from there the network trains them up and identifies what they should be but if you start with

99
00:07:24.750 --> 00:07:32.370
random double or random w red close to zero then because you're multiplying by many times the more you

100
00:07:32.370 --> 00:07:34.110
multiply the lower the value gets.

101
00:07:34.110 --> 00:07:39.900
So if you start off you might have a certain gradient going through your network being back propagated

102
00:07:39.900 --> 00:07:46.350
through your network then you move backwards your gradient becomes less then your gradient becomes less

103
00:07:46.440 --> 00:07:50.490
and then your grain becomes even less and what does that mean for the network and this is the important

104
00:07:50.490 --> 00:07:57.030
part to kind of like get your head around what does a vanishing gradient like that why is it bad for

105
00:07:57.030 --> 00:07:57.420
the network.

106
00:07:57.420 --> 00:08:02.940
Well because the gradient as it goes back through the network it is used to update the weights and we

107
00:08:02.940 --> 00:08:09.390
know that already well the lower the gradient is the harder it is for the network to update the weights

108
00:08:09.420 --> 00:08:18.330
it cannot like it Deacon silk understand what kind of contribution each of these outputs has towards

109
00:08:18.330 --> 00:08:23.280
the error and therefore you can update the weights but the lower the gradient the slower it's going

110
00:08:23.280 --> 00:08:27.780
to update the weights and the higher the gradient the faster is going to update the weights and get

111
00:08:27.780 --> 00:08:34.230
to the final result and so therefore if you have like for instance a thousand epochs these weights for

112
00:08:34.410 --> 00:08:40.320
for this layer might be updated towards the end of the thousand there you'll have some final results

113
00:08:40.350 --> 00:08:48.510
but these weights because the grain so so much smaller they're going to be update it's slower and therefore

114
00:08:48.660 --> 00:08:53.880
by then of the thousand epochs you might not have the final results there and therefore this part of

115
00:08:53.880 --> 00:08:59.400
the network is train this part of network is not trained based on this cost function but the problem

116
00:08:59.400 --> 00:09:06.660
here is not just that you have your network is not trained properly but also that these way these weights

117
00:09:06.660 --> 00:09:14.580
are this layer it's outputs are being used as inputs for further layers so the training here has been

118
00:09:14.580 --> 00:09:21.630
happening all along based on involved based off of inputs that are coming from untrained neurons untrained

119
00:09:21.630 --> 00:09:26.580
layers so it's kind of like a vicious cycle you're you're training here and you think you're getting

120
00:09:26.580 --> 00:09:31.380
good results but because you're grading and so small here this is training so slow and outputs is giving

121
00:09:31.380 --> 00:09:39.150
is so are incorrect are not final outputs and therefore you're training on the non final output so because

122
00:09:39.150 --> 00:09:47.040
of this this balance because of varnish and grading you have a a problem in your network which is not

123
00:09:47.040 --> 00:09:50.760
just that these weights and I've been trained probably the whole network has not been trained properly

124
00:09:51.060 --> 00:09:56.400
because these weights are not being trained properly and and that's there's like a domino effect like

125
00:09:56.400 --> 00:10:01.620
this wherever you look in the network always the ones at the very back are going to have that problem

126
00:10:01.920 --> 00:10:07.800
and this is in a nutshell what the vanishing gradient problem for recurrent net root neural networks

127
00:10:07.830 --> 00:10:17.100
is and that's that was kind of the main roadblock to using recurrent neural networks and let's summarize

128
00:10:17.100 --> 00:10:26.040
this in a in a short slide so if w Iraq is small then your you have a vanishing great problem if w wreck

129
00:10:26.040 --> 00:10:31.500
is large you have an exploding grain problem because same thing but then it's just gonna explode and

130
00:10:31.930 --> 00:10:37.740
a hero important thing to point out here is that of course there's more to it right there's as you can

131
00:10:37.740 --> 00:10:43.020
see there is in this formula there's other things like the activation function and and so on.

132
00:10:43.020 --> 00:10:47.700
So it's not as simple as small or large or less than one greater than one but as a rule of thumb if

133
00:10:47.700 --> 00:10:51.360
you are very small weights you're going to have a vanishing gradient if you have very large weights

134
00:10:51.610 --> 00:10:55.880
you put one hundred in there the value of one hundred and you're going to buy it by step you're going

135
00:10:55.870 --> 00:11:00.070
to have 10000 by Sept 3 you're going to have a million.

136
00:11:00.120 --> 00:11:03.350
So then you have an exploding great problem.

137
00:11:03.870 --> 00:11:09.250
So hopefully this summarizes exponent grading problem on an intuitive level because.

138
00:11:09.480 --> 00:11:16.800
So in short because you're unraveling the temporal loop the further you go through your network the

139
00:11:16.890 --> 00:11:21.210
lower your gradient is and the harder is to train the waste which has a domino effect on all of the

140
00:11:21.210 --> 00:11:24.420
further weights throughout the network as well.

141
00:11:24.420 --> 00:11:27.120
And so how do you combat the vanishing green problem.

142
00:11:27.120 --> 00:11:30.910
There's a couple of solutions for instance for exploring gradient.

143
00:11:30.930 --> 00:11:36.270
You can have truncated BRAC back for irrigation so you stop back propagating after a certain point but

144
00:11:36.270 --> 00:11:40.190
as you can imagine that's that's probably not optimal because then you're not updating all the weights

145
00:11:41.130 --> 00:11:45.630
but if you don't stop then you're just going to have a completely irrelevant network.

146
00:11:45.630 --> 00:11:50.070
So is better than the original approach then you can have penalties.

147
00:11:50.070 --> 00:11:56.610
So you could have the gradient being penalized and being artificially reduced.

148
00:11:56.610 --> 00:12:01.730
You can have gradient clipping so you could have like a maximum limit for the gradient.

149
00:12:01.740 --> 00:12:05.000
You could say never never have the gradient go over this value.

150
00:12:05.040 --> 00:12:10.660
And then if it's over that value just stays at that value as it propagates further down through a network.

151
00:12:10.680 --> 00:12:17.580
You can have that for the vanishing gradient problem you have soon other solutions you have weight initialization

152
00:12:18.420 --> 00:12:25.500
where you are smart about how you initialize your weights to minimize the potential for vanishing gradient.

153
00:12:25.500 --> 00:12:33.030
You can have there is a type of network called The Echo state networks and we're not going to talk about

154
00:12:33.030 --> 00:12:39.450
that but they do somehow solve that or they are designed to solve the vanishing game problem.

155
00:12:39.450 --> 00:12:44.730
But there's also a different type of network called the long short term memory networks or Ellis for

156
00:12:44.730 --> 00:12:54.030
short which are extremely popular which are considered to be the go to network for implementing recurrent

157
00:12:54.030 --> 00:12:54.990
neural networks.

158
00:12:55.050 --> 00:12:59.430
And that's exactly what we're going to be talking about in the rest of this section.

159
00:12:59.430 --> 00:13:01.520
So that's going to be very exciting.

160
00:13:01.610 --> 00:13:06.090
We're going to look at a brand new architecture for recurrent neural networks.

161
00:13:06.210 --> 00:13:07.900
Really can't wait to get started on that.

162
00:13:07.940 --> 00:13:12.090
It's it's it's a very interesting topic to get into.

163
00:13:12.480 --> 00:13:19.050
But for now this is this brings us to the end of the vanishing green problem tutorial and some additional

164
00:13:19.050 --> 00:13:25.920
reading so additional reading you can definitely reference the original works by self harm writer and

165
00:13:25.920 --> 00:13:26.580
Yoshio Bender.

166
00:13:26.580 --> 00:13:31.130
So this is Seth's paper in 1991 from 1991.

167
00:13:31.140 --> 00:13:36.360
It's completely in German so I'm not even going to read the name but if you understand and can read

168
00:13:36.360 --> 00:13:39.710
German then definitely this could be a good read for you.

169
00:13:39.990 --> 00:13:44.850
Then there's your show Benji's paper which is called learn learning long term dependencies of gradient

170
00:13:44.850 --> 00:13:47.360
descent is difficult 1994.

171
00:13:48.120 --> 00:13:53.880
Also you can reference that but what I would recommend looking into is a paper called on the difficulty

172
00:13:53.880 --> 00:13:58.040
of training recurrent neural networks by Erasmus and scanner.

173
00:13:58.050 --> 00:14:02.930
It's just newer it's 2013 and it's also got a sharp banjo as the co-author.

174
00:14:03.000 --> 00:14:05.240
So probably who's supervising some of this research.

175
00:14:05.340 --> 00:14:08.130
And here it summarizes a lot of the things that your show.

176
00:14:08.130 --> 00:14:12.440
Banjo talks about in his paper from 1994.

177
00:14:12.480 --> 00:14:14.770
So why not look at something newer.

178
00:14:14.940 --> 00:14:17.700
I would recommend checking this paper out.

179
00:14:18.630 --> 00:14:19.520
So there we go.

180
00:14:19.590 --> 00:14:22.170
That's the vanishing grading problem.

181
00:14:22.170 --> 00:14:23.090
Hope this was fun.

182
00:14:23.100 --> 00:14:26.270
Can't wait to see you on the next material and until then enjoy deep learning.
