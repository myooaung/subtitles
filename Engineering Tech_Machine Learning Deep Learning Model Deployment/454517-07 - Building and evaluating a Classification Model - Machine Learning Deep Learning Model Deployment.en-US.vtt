WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.880
We pulled, we build a text classifier.

00:00:02.880 --> 00:00:07.150
Let's understand how classification is done on numeric data.

00:00:07.430 --> 00:00:10.605
We have the store purchase data.

00:00:10.605 --> 00:00:14.025
We have data for different customers.

00:00:14.025 --> 00:00:18.060
There is in their salary and whether their purchase or not.

00:00:18.060 --> 00:00:20.490
Based on these data,

00:00:20.490 --> 00:00:23.850
we'll build a machine learning classification model,

00:00:23.850 --> 00:00:30.580
which will predict whether a new customer with a certain agent salary would buy or not.

00:00:32.150 --> 00:00:37.375
So in this is in salary or independent variables.

00:00:37.375 --> 00:00:41.090
Machine learning classification model using kNN,

00:00:41.090 --> 00:00:44.630
which will get trade with distort purchase data.

00:00:44.630 --> 00:00:49.610
Let's understand k nearest neighbor or k-NN machine-learning algorithm

00:00:49.610 --> 00:00:51.740
through a very simple example.

00:00:51.740 --> 00:00:57.545
Imagine we have cats and dogs shown in this diagram.

00:00:57.545 --> 00:01:02.150
On the x-axis we have weight and on the y-axis we have height.

00:01:02.150 --> 00:01:06.320
All the green ones are cats because obviously they

00:01:06.320 --> 00:01:11.285
would have less weight and laicite and all the blue ones are dogs.

00:01:11.285 --> 00:01:15.440
And if we know the height and weight of a new animal,

00:01:15.440 --> 00:01:17.585
let's say this new one in the center.

00:01:17.585 --> 00:01:20.670
Can we predict whether it's a cat or dog?

00:01:21.270 --> 00:01:24.880
Knn algorithm? Besides on that,

00:01:24.880 --> 00:01:28.690
based on the characteristics of the nearest neighbors.

00:01:28.690 --> 00:01:31.135
Typically the k value is five.

00:01:31.135 --> 00:01:34.990
We look at the five nearest neighbors and based on that,

00:01:34.990 --> 00:01:38.980
we decide which class the animal could be lumped two.

00:01:38.980 --> 00:01:41.335
For example, in this case,

00:01:41.335 --> 00:01:43.540
there are three greens and blues.

00:01:43.540 --> 00:01:46.540
That means there are three cats and

00:01:46.540 --> 00:01:50.725
dogs who have similar characteristics because the new animal.

00:01:50.725 --> 00:01:54.460
So this AnyVal is more likely to be a cat because

00:01:54.460 --> 00:01:59.995
the majority of the animals belong to the cat class in the nearest neighbor rule.

00:01:59.995 --> 00:02:03.680
So this is k nearest neighbor technique where

00:02:03.680 --> 00:02:08.285
outcome is predicted based on characteristics shown by the nearest neighbors.

00:02:08.285 --> 00:02:10.985
And that gave a Louis typically five.

00:02:10.985 --> 00:02:15.410
Let's apply this technique on the store purchase data.

00:02:15.410 --> 00:02:18.875
We have the data in the project folder.

00:02:18.875 --> 00:02:24.245
We can spider, you have to select your project folder here.

00:02:24.245 --> 00:02:29.030
And then you can go to files and see all the source code and files.

00:02:29.030 --> 00:02:31.895
So this is the store purchase data we have

00:02:31.895 --> 00:02:35.780
using which will build a machine learning classification model.

00:02:35.780 --> 00:02:38.970
Let's create a new Python file.

00:02:41.200 --> 00:02:45.900
Will imitate ML Pipeline.

00:02:53.590 --> 00:02:57.690
We'll import numpy and pandas.

00:02:58.720 --> 00:03:01.820
In spider. As soon as you type,

00:03:01.820 --> 00:03:04.295
you get all the errors or warnings.

00:03:04.295 --> 00:03:07.835
It saying we are not using Numpy pandas, that's fine.

00:03:07.835 --> 00:03:11.430
We'll be writing the code for the same shortly.

00:03:12.310 --> 00:03:18.540
Now, let's load the store purchase data to a Pandas DataFrame.

00:03:18.670 --> 00:03:21.065
We live it training data,

00:03:21.065 --> 00:03:24.410
dataframe, which will store the store purchase data.

00:03:24.410 --> 00:03:27.380
Note that will not be training with the entire data.

00:03:27.380 --> 00:03:31.115
We'll have some requests for training and some for testing, which we'll see next.

00:03:31.115 --> 00:03:37.920
What training data pandas DataFrame would store the entire CSV file data.

00:03:39.160 --> 00:03:43.310
You can run the entire file by selecting the cycle,

00:03:43.310 --> 00:03:45.890
or you can run the selection.

00:03:45.890 --> 00:03:48.480
Let's run the selection.

00:03:49.060 --> 00:03:52.040
We can go to variable explorer,

00:03:52.040 --> 00:03:53.645
click on cleaning data,

00:03:53.645 --> 00:04:00.125
and we can see that is salary participant loaded to the training data, dataframe.

00:04:00.125 --> 00:04:05.010
Let's get some statistical in boat dot printing data.

00:04:08.500 --> 00:04:13.280
We can see various statistical information about the data.

00:04:13.280 --> 00:04:16.985
How many records? We have, 40 records.

00:04:16.985 --> 00:04:18.830
We can see the mean,

00:04:18.830 --> 00:04:22.670
standard deviation and some other statistics about

00:04:22.670 --> 00:04:27.650
the data will store the independent variables in an IRA.

00:04:27.650 --> 00:04:33.215
Will take rose up to the last column and stored them in a dependent variable X,

00:04:33.215 --> 00:04:35.045
which is a NumPy array.

00:04:35.045 --> 00:04:40.640
Let's do that. So this should populate agents salary.

00:04:40.640 --> 00:04:49.100
And next, lets go

00:04:49.100 --> 00:04:52.040
to variable explorer and checkout.

00:04:52.040 --> 00:04:58.370
We can see that agent salary have now populated in NumPy array,

00:04:58.370 --> 00:05:01.535
will populate the purchase column,

00:05:01.535 --> 00:05:06.425
which is the prediction to Canada NumPy array away.

00:05:06.425 --> 00:05:15.350
So this should populate the last column and stored it in way too.

00:05:15.350 --> 00:05:19.190
This is our y, which is the dependent variable or

00:05:19.190 --> 00:05:23.615
the one where trying to predict we have Asian salary and x NumPy array.

00:05:23.615 --> 00:05:27.365
And we have y, which is the purchase data.

00:05:27.365 --> 00:05:30.395
For not purchased, one is for purchase.

00:05:30.395 --> 00:05:33.905
So that is stored in a Numpy array.

00:05:33.905 --> 00:05:37.340
Now we have the independent variables and dependent

00:05:37.340 --> 00:05:40.520
variables in two separate Numpy arrays.

00:05:40.520 --> 00:05:47.825
Next, using scikit-learn will separate the data into training set and test set.

00:05:47.825 --> 00:05:50.765
And we'll huge 80-20 ratio,

00:05:50.765 --> 00:05:54.980
80% of the data for training and 20% for testing.

00:05:54.980 --> 00:05:59.240
Scikit-learn is a very popular library for machine learning using Python.

00:05:59.240 --> 00:06:02.645
Scikit-learn comes pre-installed with Anaconda spider.

00:06:02.645 --> 00:06:05.330
If I'm using a different Python and Marmot,

00:06:05.330 --> 00:06:09.455
you might have to install scikit-learn using pip install is Kalen.

00:06:09.455 --> 00:06:13.535
Pip install is the command to install any Python libraries.

00:06:13.535 --> 00:06:18.500
Anaconda spider comes with scikit-learn, numpy, pandas,

00:06:18.500 --> 00:06:20.900
and many other libraries that are required

00:06:20.900 --> 00:06:23.915
for scientific competition and machine learning.

00:06:23.915 --> 00:06:26.450
We're using scikit-learn, train,

00:06:26.450 --> 00:06:31.340
test split class to split the dataset into two parts.

00:06:31.340 --> 00:06:33.320
Now once we do this,

00:06:33.320 --> 00:06:35.675
we'll have the training set and the test set.

00:06:35.675 --> 00:06:38.720
The training set will have 32 records.

00:06:38.720 --> 00:06:42.035
We said 80% data will be used for training.

00:06:42.035 --> 00:06:47.130
So we've totaled 40 records of which 32 will be used for cleaning.

00:06:47.680 --> 00:06:50.820
So this is extreme.

00:06:51.070 --> 00:06:57.120
And white tr1. 32 records for trading.

00:06:59.200 --> 00:07:03.920
And x-test has heat records.

00:07:03.920 --> 00:07:06.860
Similarly, waitlist will have eight records.

00:07:06.860 --> 00:07:09.500
This is the data for testing the border.

00:07:09.500 --> 00:07:12.005
Next we'll feature scale the data.

00:07:12.005 --> 00:07:15.710
So that is in salary are in the same bridge

00:07:15.710 --> 00:07:19.790
and the machine learning model could not get influenced by salary,

00:07:19.790 --> 00:07:21.725
which is in a higher range.

00:07:21.725 --> 00:07:28.835
Let's run this. Now we can see the scale data.

00:07:28.835 --> 00:07:32.120
Standard scaler distributes the data in

00:07:32.120 --> 00:07:35.870
a way so that the mean is 0 and standard deviation is one.

00:07:35.870 --> 00:07:39.395
Now both the A's in salary and in the same page.

00:07:39.395 --> 00:07:46.460
Next, we'll build a classification model using the K nearest neighbors technique.

00:07:47.790 --> 00:07:50.440
Will have five neighbors.

00:07:50.440 --> 00:07:53.110
We lose the Minkowski metrics.

00:07:53.110 --> 00:07:55.180
To build this classifier.

00:07:55.180 --> 00:08:00.880
Minkowski metrics works built on the Euclidian distance between two points.

00:08:00.880 --> 00:08:05.125
Euclidean distance is nothing but the shortest distance between two points.

00:08:05.125 --> 00:08:08.905
That's how it decides which neighbors are the nearest.

00:08:08.905 --> 00:08:13.975
Next will fit the training data to the classifier to clean it.

00:08:13.975 --> 00:08:18.050
So this is where the model is getting trained.

00:08:25.380 --> 00:08:31.645
This is the classifier object which is been trained with certain cleaning data,

00:08:31.645 --> 00:08:37.255
which is, is it salary is the input variable and purchases the output variable.

00:08:37.255 --> 00:08:39.835
The classifier is our model.

00:08:39.835 --> 00:08:42.520
Will quickly check the accuracy of

00:08:42.520 --> 00:08:46.990
the classifier by trying to predict. For the test data.

00:08:46.990 --> 00:08:50.020
Classifier has a predict method which takes

00:08:50.020 --> 00:08:55.520
a Numpy arrays input and returns an output in another number.

00:09:00.510 --> 00:09:03.955
So this is our X test,

00:09:03.955 --> 00:09:06.585
and this is the weight.

00:09:06.585 --> 00:09:09.875
And let's see what is the prediction.

00:09:09.875 --> 00:09:14.780
Waited? Six sit for one record.

00:09:14.780 --> 00:09:17.225
The model predicted it correctly.

00:09:17.225 --> 00:09:19.130
For all of the records.

00:09:19.130 --> 00:09:24.780
We can also check the probability of prediction for all these data.

00:09:34.390 --> 00:09:39.725
Here we can see that wherever we have more than 0.5 probability,

00:09:39.725 --> 00:09:43.100
the model is predicting that the customer owed by.

00:09:43.100 --> 00:09:45.930
Otherwise the customer would not buy.

00:09:47.080 --> 00:09:52.925
Probability is helpful when you look to SAR data from the prediction and

00:09:52.925 --> 00:09:59.390
the customers were more likely to purchase from this tree.

00:09:59.390 --> 00:10:05.270
The third one is more likely to purchase because the probability is 0.8 or 80 plus

00:10:05.270 --> 00:10:12.630
it will check the accuracy of the model using Confusion Matrix.

00:10:16.300 --> 00:10:19.880
Confusion Matrix is a statistical technique to

00:10:19.880 --> 00:10:22.970
predict it courtesy of a classification model.

00:10:22.970 --> 00:10:25.055
The way it works is pretty simple.

00:10:25.055 --> 00:10:30.555
If the actual value is one and the model predicted one, then in-situ project.

00:10:30.555 --> 00:10:35.005
Actual value is one and model predicted 0, it's false-negative.

00:10:35.005 --> 00:10:39.805
Similarly, 00 is true negative and 01 is false positive.

00:10:39.805 --> 00:10:42.970
It can also be represented in this format.

00:10:42.970 --> 00:10:45.175
So once we know all four types,

00:10:45.175 --> 00:10:47.990
we can easily determine the accuracy.

00:10:48.120 --> 00:10:51.310
So they couldn't see is true positive plus true

00:10:51.310 --> 00:10:54.984
negative divided by all four types of predictions.

00:10:54.984 --> 00:10:58.330
No matter which classification technique you are using,

00:10:58.330 --> 00:11:04.400
kNN or any other Confusion Matrix can be used to calculate the accuracy of the model.

00:11:04.530 --> 00:11:08.575
Scikit-learn and other machine learning libraries,

00:11:08.575 --> 00:11:14.240
the built-in classes to generate confusion matrix, comic Julian predictor.

00:11:15.300 --> 00:11:18.265
Let's create the confusion matrix.

00:11:18.265 --> 00:11:23.620
Will pass the childValue app.js sit that is whitest and the predicted values,

00:11:23.620 --> 00:11:24.940
that is white bread.

00:11:24.940 --> 00:11:30.250
And get the confusion metrics from the scikit-learn confusion underscore metrics class.

00:11:30.250 --> 00:11:37.820
Go to spider variable explorer and we can see the confusion matrix over here.

00:11:38.370 --> 00:11:43.824
We have three true negatives for true positives.

00:11:43.824 --> 00:11:48.800
Only one false negative and false positive.

00:11:49.080 --> 00:11:52.090
So this model is very good it,

00:11:52.090 --> 00:11:57.080
because we have only one false positive or negative from eight record.

00:11:58.780 --> 00:12:02.640
Let's calculate the accuracy of the model.

00:12:04.600 --> 00:12:10.205
And we'll print the quiz 0.875.

00:12:10.205 --> 00:12:13.835
So our model is 87.5% a great.

00:12:13.835 --> 00:12:17.120
So this model can predict whether a customer with

00:12:17.120 --> 00:12:24.510
a particular agent salary would buy or not with 87% accuracy.

00:12:26.620 --> 00:12:31.130
You can also get the entire classification report to

00:12:31.130 --> 00:12:35.975
understand more about precision recall and F1 score.

00:12:35.975 --> 00:12:39.470
So we've taken this toward purchase and created

00:12:39.470 --> 00:12:43.025
a classifier which can predict whether somebody would by R naught.

00:12:43.025 --> 00:12:46.100
That model or classifier can be used to

00:12:46.100 --> 00:12:51.095
predict whether a customer with a particular agents salary would BYOD naught.

00:12:51.095 --> 00:12:58.805
So let's try to predict whether a customer with age 40 salaried today 1000 goodbye.

00:12:58.805 --> 00:13:03.860
Note that this model takes a NumPy array and returns a number.

00:13:03.860 --> 00:13:08.615
You have to create a Numpy array from agents salary,

00:13:08.615 --> 00:13:10.550
feature skill that data,

00:13:10.550 --> 00:13:12.769
and then feed it to the classifier.

00:13:12.769 --> 00:13:15.950
Because the classifier is trained on feature skill data,

00:13:15.950 --> 00:13:21.980
suitably ensured the data you are fitting is also feature scaled with the same technique,

00:13:21.980 --> 00:13:25.320
which is standard scaler In our case.

00:13:27.310 --> 00:13:29.660
And the prediction is 0,

00:13:29.660 --> 00:13:33.305
the customer or not by somebody with

00:13:33.305 --> 00:13:39.090
age 40 and salary to detergent would not buy is but this model,

00:13:39.340 --> 00:13:44.570
we can check the probability of the prediction for the same data.

00:13:44.570 --> 00:13:51.480
Classify Reza predict parameter using which you can get the probability.

00:13:54.730 --> 00:13:58.040
So the probability is 0.2 or 20%.

00:13:58.040 --> 00:14:01.350
That's why the model set to customer would not buy.

00:14:03.280 --> 00:14:05.660
Let's try to predict.

00:14:05.660 --> 00:14:10.080
For a customer with a age 42 and salary 50 thousand.

00:14:17.110 --> 00:14:20.855
This time the model set the customer or buyer.

00:14:20.855 --> 00:14:29.000
Let's check out the probability. It's 0.880%.

00:14:29.000 --> 00:14:32.465
So there are 80% chances of the customer buying.

00:14:32.465 --> 00:14:35.120
Now we will machine learning model, brady.

00:14:35.120 --> 00:14:37.145
It's a classification model.

00:14:37.145 --> 00:14:42.030
It can predict whether a customer with a certain days in cell D would by R naught.
