WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:04.260
Let's solve the customer behavior prediction use case.

00:00:04.260 --> 00:00:06.570
With Python.

00:00:06.570 --> 00:00:10.065
We'll head to Google collab and create a new notebook.

00:00:10.065 --> 00:00:14.070
It's really easy to get started with the pipe Tarjan, Google collab,

00:00:14.070 --> 00:00:16.364
because all the liabilities at pre-installed,

00:00:16.364 --> 00:00:19.920
you can just create a notebook and start coding.

00:00:19.920 --> 00:00:23.970
Let's give a name to this notebook by Dutch,

00:00:23.970 --> 00:00:31.680
create and save will import the standard libraries for numpy and pandas.

00:00:31.680 --> 00:00:37.640
And also we need to import liquid liabilities by dots is based on the torch library.

00:00:37.640 --> 00:00:39.860
So you have to import to

00:00:39.860 --> 00:00:46.140
import dot-dot-dot neural network can import functionality we have.

00:00:47.890 --> 00:00:50.510
If we are using some other environment,

00:00:50.510 --> 00:00:53.795
make sure Python installed in that environment.

00:00:53.795 --> 00:00:58.129
Next, we load the customer purchase data from our GitHub repository.

00:00:58.129 --> 00:01:01.910
At this time we have slightly more number of records.

00:01:01.910 --> 00:01:08.300
Let's check it out. So we have 1550 records because you are building neural network.

00:01:08.300 --> 00:01:12.330
It's better to try with slightly higher volume of data.

00:01:13.240 --> 00:01:19.945
And we can do dataset or to see some sample records.

00:01:19.945 --> 00:01:24.820
It's same as before, a salary and whether the customer is partition or not.

00:01:24.820 --> 00:01:30.730
Separating the dataset to cleaning ab.js dataset and features scaling.

00:01:30.730 --> 00:01:34.375
Stapes are similar to what we have done earlier.

00:01:34.375 --> 00:01:37.825
80% for training, 20% is artistic.

00:01:37.825 --> 00:01:40.630
Next, we'll do feature scaling.

00:01:40.630 --> 00:01:44.080
We just killing is a must for D planning labor disease.

00:01:44.080 --> 00:01:50.890
Next, we'll convert that IRAs for training and test data to bite arch tensor format.

00:01:50.890 --> 00:01:53.680
Tensor is The made data type in Python.

00:01:53.680 --> 00:01:56.695
Tensor are similar to arrays.

00:01:56.695 --> 00:01:59.215
Let's check out sample values.

00:01:59.215 --> 00:02:01.795
You can see it's similar to array,

00:02:01.795 --> 00:02:05.740
two dimensional array, but it's in a cancer hallmark.

00:02:05.740 --> 00:02:09.025
That's the main data type for Python.

00:02:09.025 --> 00:02:13.435
Will also convert the dependent variable depends upon matter.

00:02:13.435 --> 00:02:17.575
We can check the shape of the tensors.

00:02:17.575 --> 00:02:25.105
So we have about 1243 records for training and 311 that cuts for testing.

00:02:25.105 --> 00:02:29.095
Next we'll construct the neural network using Python.

00:02:29.095 --> 00:02:31.300
Let's declare three variables.

00:02:31.300 --> 00:02:32.725
What is the input size is two.

00:02:32.725 --> 00:02:36.805
That is, the two variables we have in the input layers is in salary,

00:02:36.805 --> 00:02:38.275
then output size two,

00:02:38.275 --> 00:02:40.180
because we have predicting two classes,

00:02:40.180 --> 00:02:42.545
whether the customer is god not yes or no.

00:02:42.545 --> 00:02:43.760
So that'll be two.

00:02:43.760 --> 00:02:45.785
Then we'll have hidden sizes.

00:02:45.785 --> 00:02:48.425
In each of these layers will have ten neutrons.

00:02:48.425 --> 00:02:52.550
You can experiment and try with different number of neutrons in each of the layers.

00:02:52.550 --> 00:02:56.750
We'll build a neural network with ten neurons in two layers,

00:02:56.750 --> 00:03:00.935
mix to create a class for the neural network.

00:03:00.935 --> 00:03:04.970
And the syntax looks something like this.

00:03:04.970 --> 00:03:09.530
Here you have to specify how many layers you need.

00:03:09.530 --> 00:03:13.380
Sfc stands for fully-connected layer.

00:03:13.750 --> 00:03:16.625
In a fully-connected layer,

00:03:16.625 --> 00:03:21.900
a neuron in each layer is connected to all the neurons in the subsequently.

00:03:22.590 --> 00:03:27.115
And we would need fully-connected layers.

00:03:27.115 --> 00:03:29.230
One for input to hidden,

00:03:29.230 --> 00:03:30.895
the other four hidden to hidden.

00:03:30.895 --> 00:03:36.710
And the third one for hidden to the output layer will have just two hidden layers.

00:03:39.600 --> 00:03:44.260
And then we are using renew activation function in

00:03:44.260 --> 00:03:50.540
the hidden layers and softmax and the output layer.

00:03:50.970 --> 00:03:55.810
So this is how you create a neural network class in Python.

00:03:55.810 --> 00:03:58.210
And once your classes defined,

00:03:58.210 --> 00:04:00.385
then you instantiate though modally.

00:04:00.385 --> 00:04:06.025
After that, you define what is the learning rate and what is the loss function.

00:04:06.025 --> 00:04:10.255
Using this learning rate loss function and Adam optimizer,

00:04:10.255 --> 00:04:13.810
that deep learning neural network will learn from data,

00:04:13.810 --> 00:04:17.810
adjust the weights, and give the prediction.

00:04:18.390 --> 00:04:21.640
And we'll define a 100 epochs.

00:04:21.640 --> 00:04:24.790
That means the neural network will learn a 100 times from

00:04:24.790 --> 00:04:28.850
the same data and keep on adjusting the weights to get the final output.

00:04:29.400 --> 00:04:34.540
Next, let's start training the neural network with a 100 epochs.

00:04:34.540 --> 00:04:38.180
And we'll print the loss after each epoch.

00:04:40.740 --> 00:04:46.210
So you can see that the loss is getting minimized after each tray.

00:04:46.210 --> 00:04:50.135
And after a 100 epochs from 0 to 99,

00:04:50.135 --> 00:04:53.700
we let the model ready for prediction.

00:04:56.170 --> 00:05:01.490
With model.predict amateurs, you can see various parameters of the model.

00:05:01.490 --> 00:05:05.300
Now this model can predict for new setup data,

00:05:05.300 --> 00:05:08.630
but you have to make sure that the data is fading denser format.

00:05:08.630 --> 00:05:14.540
Let's try to predict for each party and suddenly 20 thousand the way we've been doing.

00:05:14.540 --> 00:05:18.920
But this time we'll be converting 48 to 2002 tensor format.

00:05:18.920 --> 00:05:20.840
And we fought that will have produced feature

00:05:20.840 --> 00:05:24.275
scaling because the model has been trained on scale data.

00:05:24.275 --> 00:05:27.140
So using dot-dot-dot from NumPy,

00:05:27.140 --> 00:05:32.660
we can convert a NumPy array to a tensor RA.

00:05:32.660 --> 00:05:37.400
And you feed that data to the model and you get the output.

00:05:37.400 --> 00:05:40.250
So the output is two columns.

00:05:40.250 --> 00:05:43.670
First twenties for 0 and the second one is for what?

00:05:43.670 --> 00:05:48.200
This means if this value is higher than the customer is not going to buy.

00:05:48.200 --> 00:05:52.055
And if this value is higher than the customer is going to buy,

00:05:52.055 --> 00:05:58.860
you can also get the maximum loop of the two and predict which class the output is.

00:05:58.960 --> 00:06:02.555
So it says 0 customer is not going to buy.

00:06:02.555 --> 00:06:06.920
Similarly, let's predict for age 42 and salary 50 thousand.

00:06:06.920 --> 00:06:13.025
We can see that this value is minus 0.61 is higher than minus 0.707.

00:06:13.025 --> 00:06:15.905
So the customer is going to, by.

00:06:15.905 --> 00:06:18.065
Now that the model is ready,

00:06:18.065 --> 00:06:21.080
there are many ways you can save and export the model.

00:06:21.080 --> 00:06:25.850
One of the ways is to save the model Eugene dot-dot-dot save.

00:06:25.850 --> 00:06:29.705
The model will get saved in a file.

00:06:29.705 --> 00:06:35.465
In Calabria can do a list to see the file which has been created in the same directory.

00:06:35.465 --> 00:06:38.645
Now, you can export this Morton and

00:06:38.645 --> 00:06:42.815
simply do dot-dot-dot load and then restored the model.

00:06:42.815 --> 00:06:44.825
We're trying it in the same notebook,

00:06:44.825 --> 00:06:48.950
but you could take it to another notebook or another environment and this would work.

00:06:48.950 --> 00:06:55.260
Let's predict from the restored model class and we are getting the same prediction.

00:06:55.870 --> 00:07:00.575
But this is not the most preferred way of saving biters model.

00:07:00.575 --> 00:07:05.210
Pi-thirds recommends saving the model dictionary instead of the entire model.

00:07:05.210 --> 00:07:09.260
And then you can take the dictionary and use it in another environment.

00:07:09.260 --> 00:07:11.450
Using state underscore zq method,

00:07:11.450 --> 00:07:14.120
you can get that dictionary of the model.

00:07:14.120 --> 00:07:16.010
This has various details,

00:07:16.010 --> 00:07:18.630
lake weights of the neural network.

00:07:19.150 --> 00:07:22.220
To use the neural network in another environment,

00:07:22.220 --> 00:07:26.675
all you need is the weights and that is available in the pipe dodge model dictionary.

00:07:26.675 --> 00:07:30.990
And you can save the dictionary using dot-dot-dot save model,

00:07:30.990 --> 00:07:34.400
state dictionary, and specifying a file name.

00:07:34.400 --> 00:07:38.340
Let's export this dictionary to another enviroment.

00:07:38.980 --> 00:07:44.990
You can also predict the same notebook. Using the dictionary.

00:07:44.990 --> 00:07:48.980
You define a new predictor with the class,

00:07:48.980 --> 00:07:52.470
and you can predict the way you predicted earlier.

00:07:52.990 --> 00:07:56.900
Now this customer buys state dictionary can be exported

00:07:56.900 --> 00:07:59.960
to another pirates environment and they used their ship,

00:07:59.960 --> 00:08:03.575
zipped it using the Linux zip command.

00:08:03.575 --> 00:08:06.695
Let's download it from here.

00:08:06.695 --> 00:08:12.020
You can download a file from Google collab using the files class.

00:08:12.020 --> 00:08:14.615
Make sure you are using Chrome Gaussian.

00:08:14.615 --> 00:08:17.195
Otherwise the Download might not work.

00:08:17.195 --> 00:08:20.760
So this is the downloaded model file.

00:08:21.100 --> 00:08:27.245
Let's check it out. And this is in binary format.

00:08:27.245 --> 00:08:28.985
So this is all the details around

00:08:28.985 --> 00:08:32.060
the weights and various other parameters for the biters.

00:08:32.060 --> 00:08:37.130
Martin will upload this to our GitHub repository.

00:08:37.130 --> 00:08:42.330
Let me give it a new name because there must be a file with the same name.

00:08:42.790 --> 00:08:45.110
Let's create a new notebook.

00:08:45.110 --> 00:08:50.374
We'll call it use by dictionary.

00:08:50.374 --> 00:08:54.290
So currently there is nothing in this directory.

00:08:54.290 --> 00:08:58.710
Will get the model from the GitHub repository.

00:09:01.900 --> 00:09:04.460
Let's unzip it.

00:09:04.460 --> 00:09:09.290
Now this file is the python dictionary that we created earlier.

00:09:09.290 --> 00:09:13.415
We also need the pickle file to scale the data.

00:09:13.415 --> 00:09:20.780
Let's import pickle created new scalar, import the libraries.

00:09:20.780 --> 00:09:24.185
Now we need to create a class for the neural network.

00:09:24.185 --> 00:09:26.360
The input size, output size,

00:09:26.360 --> 00:09:29.240
and hidden size of that class should match with

00:09:29.240 --> 00:09:33.740
the original class using which the model was created.

00:09:33.740 --> 00:09:37.550
Number of fully-connected layers should match with dot is

00:09:37.550 --> 00:09:44.795
middle class will define a new variable, nu predictor.

00:09:44.795 --> 00:09:49.380
Now we'll load the dictionary to the new predictor.

00:09:49.630 --> 00:09:55.440
And after that, we can predict the way we are predicting earlier.

00:09:59.140 --> 00:10:02.600
We need to import numpy also.

00:10:02.600 --> 00:10:06.210
We can see the predicted output here.
