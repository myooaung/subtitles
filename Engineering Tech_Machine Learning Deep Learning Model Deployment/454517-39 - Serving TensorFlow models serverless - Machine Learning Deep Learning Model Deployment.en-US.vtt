WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.400
Let's now understand how to create

00:00:02.400 --> 00:00:08.140
a serverless rest API for the TensorFlow text classifier that we've just created.

00:00:08.810 --> 00:00:15.015
You can create the model and are downloaded to a local environment,

00:00:15.015 --> 00:00:17.380
though we have shown earlier.

00:00:19.670 --> 00:00:22.110
So when we download the model,

00:00:22.110 --> 00:00:24.990
we get a protocol file and we get dow,

00:00:24.990 --> 00:00:27.760
which is within the variable folder.

00:00:27.920 --> 00:00:31.455
Within the Google Cloud Function environment.

00:00:31.455 --> 00:00:35.430
We won't be able to create a directory to Lord this protocol file.

00:00:35.430 --> 00:00:38.085
So what we will do is we would load

00:00:38.085 --> 00:00:42.460
these weeds which are present under variables subdirectory,

00:00:42.460 --> 00:00:46.385
and then use them to reconstruct the modelling.

00:00:46.385 --> 00:00:51.230
So let's go to Google Cloud environment and search for Buckets.

00:00:51.230 --> 00:00:53.660
We'll create a new bucket.

00:00:53.660 --> 00:00:56.285
Let's name it as effects.

00:00:56.285 --> 00:00:58.950
Bf demo.

00:01:02.230 --> 00:01:06.540
You can keep everything as default and just create.

00:01:09.190 --> 00:01:15.420
Now let's applaud the dart TF-IDF model in TensorFlow weights.

00:01:21.460 --> 00:01:27.180
So we'll go to the variable static tree and applaud the widths.

00:01:31.120 --> 00:01:36.110
And let's applaud the DFA D of more than load.

00:01:36.110 --> 00:01:46.500
So we'll upload the pickle file that contains the TF-IDF model.

00:01:48.220 --> 00:01:54.245
So waits DFID model have been uploaded to this bucket.

00:01:54.245 --> 00:01:57.660
Now let's go to Google Cloud Function.

00:01:58.600 --> 00:02:02.159
Search for Cloud Functions.

00:02:02.320 --> 00:02:06.315
And lets clear the new function.

00:02:06.315 --> 00:02:11.120
And we'll set Dart triggered type is HTTP.

00:02:12.210 --> 00:02:17.800
And let's select slightly higher memory which is required for the TensorFlow.

00:02:17.800 --> 00:02:21.260
Martin will select one GB memory.

00:02:23.820 --> 00:02:30.080
And we'll say allow unauthenticated invocation. Save it.

00:02:38.160 --> 00:02:43.010
Here we'll select Python 3.7.

00:02:44.670 --> 00:02:49.590
Now let's look at the code that we need to first import,

00:02:49.590 --> 00:02:54.455
request, pickle, Google Cloud Storage and tensor flow.

00:02:54.455 --> 00:02:59.645
And then we need to create an instance of the bucket is shown earlier.

00:02:59.645 --> 00:03:03.245
And our bucket this time is a fixed PF demo.

00:03:03.245 --> 00:03:08.239
And let's Lord though weights to different blog variables,

00:03:08.239 --> 00:03:11.255
variable startIndex and data file.

00:03:11.255 --> 00:03:14.480
And then we'll Lord the pickle file to

00:03:14.480 --> 00:03:17.555
deter similar steps which you applied for the second LAN model.

00:03:17.555 --> 00:03:20.970
Just that we are loading the weights this time.

00:03:21.250 --> 00:03:28.370
And then we can download the wage two doors last TMP directory to each axis.

00:03:28.370 --> 00:03:31.745
And now to reconstruct the TensorFlow model.

00:03:31.745 --> 00:03:35.030
We'll have to define the model class.

00:03:35.030 --> 00:03:38.435
This should match with the model that you created earlier.

00:03:38.435 --> 00:03:40.025
And once that is done,

00:03:40.025 --> 00:03:41.870
you can load the weights.

00:03:41.870 --> 00:03:44.390
Using this load weights.

00:03:44.390 --> 00:03:47.120
You have to give variables which is

00:03:47.120 --> 00:03:51.275
the filename without the extension for both the files.

00:03:51.275 --> 00:03:54.830
Then you can predict the way you predicted earlier.

00:03:54.830 --> 00:04:00.035
You have to read the sentence from the incoming request.

00:04:00.035 --> 00:04:03.020
And using the TF-IDF vectors,

00:04:03.020 --> 00:04:05.900
you can convert it to numeric format.

00:04:05.900 --> 00:04:10.080
Then using TensorFlow model predict method you can predict.

00:04:10.180 --> 00:04:14.135
So these tapes that similar to what we have done earlier,

00:04:14.135 --> 00:04:17.135
only difference is how do we load the weights

00:04:17.135 --> 00:04:20.915
and then how do we reconstruct the TensorFlow model?

00:04:20.915 --> 00:04:26.105
And in the requirements.txt will have to add the required packages.

00:04:26.105 --> 00:04:30.815
We learn, TensorFlow, Google Cloud Storage, scikit-learn and request,

00:04:30.815 --> 00:04:36.270
make sure you use the same worsens are the ways you might face challenges.

00:04:36.310 --> 00:04:39.360
Let's deployed now.

00:04:43.780 --> 00:04:47.405
Here you see different options to view logs.

00:04:47.405 --> 00:04:49.370
If we're deployment fails for some reason,

00:04:49.370 --> 00:04:53.225
you can go to this view logs and see what went wrong.

00:04:53.225 --> 00:04:59.310
While it is loading. Let me just pull up another function which I've deployed.

00:05:01.660 --> 00:05:02.900
Here.

00:05:02.900 --> 00:05:05.765
You see logs for various runs that we've done earlier.

00:05:05.765 --> 00:05:08.720
You can pick on this log file and

00:05:08.720 --> 00:05:12.035
get more details about the Iranian case you face any error.

00:05:12.035 --> 00:05:17.075
You can also copy this to clipboard and take it to a text pad for analysis.

00:05:17.075 --> 00:05:21.454
Here you'll find logs for deployment and also logs for function invocation.

00:05:21.454 --> 00:05:24.500
Let's go back and see if our function got deployed.

00:05:24.500 --> 00:05:28.279
It's still loading narratives successful.

00:05:28.279 --> 00:05:33.320
Let's click on it. We'll go to the testing tab.

00:05:33.320 --> 00:05:37.740
Let's pass it JSON string to test this function.

00:05:40.390 --> 00:05:46.280
We got the sentiment is positive and if it changes good to bad,

00:05:46.280 --> 00:05:48.990
let's see the output.

00:05:50.890 --> 00:05:53.854
It's a negative sentence.

00:05:53.854 --> 00:06:00.305
We can also invoke this cloud function from outside using the HTTP trigger endpoint.

00:06:00.305 --> 00:06:02.075
So let's find that.

00:06:02.075 --> 00:06:04.490
Click on that triggered tab.

00:06:04.490 --> 00:06:07.340
And you'll see the triggering point.

00:06:07.340 --> 00:06:15.215
Let's copy this. Will go to Postman and paste the entire URL.

00:06:15.215 --> 00:06:18.455
And we have this sentence in the main body.

00:06:18.455 --> 00:06:20.250
Let's send it.

00:06:20.290 --> 00:06:24.980
So this should hit the goal cloud function and give us the prediction.

00:06:24.980 --> 00:06:27.540
The sentiment is negative.

00:06:29.380 --> 00:06:33.410
And if we change the sentence to something positive,

00:06:33.410 --> 00:06:35.855
will get a positive sentiment.

00:06:35.855 --> 00:06:41.850
So this is how you can create a serverless Cloud Function for your TensorFlow models.
