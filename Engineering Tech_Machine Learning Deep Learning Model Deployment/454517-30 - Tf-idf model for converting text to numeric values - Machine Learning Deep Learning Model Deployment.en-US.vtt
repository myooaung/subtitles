WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:01.160 --> 00:00:07.710
Tf-idf is a popular technique to convert takes to numeric format.

00:00:07.710 --> 00:00:13.510
Tf-idf stands for term frequency and inverse document frequency.

00:00:15.160 --> 00:00:17.465
It's put this model,

00:00:17.465 --> 00:00:22.265
if your word occurs more number of times in a document or a sentence,

00:00:22.265 --> 00:00:24.140
it is given more importance.

00:00:24.140 --> 00:00:28.954
However, if the same order occurs in many sentences or many documents,

00:00:28.954 --> 00:00:32.120
then the word is given less importance.

00:00:32.120 --> 00:00:34.880
Let's look at an example.

00:00:34.880 --> 00:00:37.220
Tf is Term frequency,

00:00:37.220 --> 00:00:40.160
that is number of occurrences of a word in

00:00:40.160 --> 00:00:45.539
a document divided by number of words in that document or sentence.

00:00:46.900 --> 00:00:51.650
For example, if today food is good and salaries Nice.

00:00:51.650 --> 00:00:53.135
That's a sentence.

00:00:53.135 --> 00:00:56.810
Then the Term Frequency of what the good is one by

00:00:56.810 --> 00:01:01.025
eight because the word good occurs once and there are total eight words.

00:01:01.025 --> 00:01:04.370
Similarly, the target frequency of word 0s

00:01:04.370 --> 00:01:08.240
is two by eight because the word iz occurs twice.

00:01:08.240 --> 00:01:10.980
And there are properly towards.

00:01:12.070 --> 00:01:17.450
So going by this model would easily have higher importance than

00:01:17.450 --> 00:01:22.130
we're good because it is occurring more number of times in this particular sentence.

00:01:22.130 --> 00:01:24.890
However, if toward easy common watercress,

00:01:24.890 --> 00:01:26.870
multiple sentences are documents,

00:01:26.870 --> 00:01:28.340
it's importance would be lower.

00:01:28.340 --> 00:01:31.100
So that is driven by inverse document frequency,

00:01:31.100 --> 00:01:32.570
which will look next.

00:01:32.570 --> 00:01:37.745
Idf Inverse Document frequencies calculated based on this formula.

00:01:37.745 --> 00:01:43.910
Log base C, number of sentences divided by number of sentences containing the word.

00:01:43.910 --> 00:01:46.610
Again, you don't have to remember this formula.

00:01:46.610 --> 00:01:51.545
You love libraries available to calculate TF and IDF values.

00:01:51.545 --> 00:01:54.600
For now, understand the concepts.

00:01:56.140 --> 00:02:01.175
Let's look at a simple example to understand IDF.

00:02:01.175 --> 00:02:04.040
Imagine we have three sentences. Services good.

00:02:04.040 --> 00:02:05.645
Today ambience is really nice,

00:02:05.645 --> 00:02:08.645
and today food is good and solid is nice.

00:02:08.645 --> 00:02:11.045
We already know how to calculate

00:02:11.045 --> 00:02:14.840
frequency of different words appearing in these sentences.

00:02:14.840 --> 00:02:17.990
Now to calculate inverse document frequency will

00:02:17.990 --> 00:02:20.885
have to do log base C, number of sentences.

00:02:20.885 --> 00:02:25.910
That is three for all the words divided by number of sentences containing the word.

00:02:25.910 --> 00:02:28.865
For example, eases a peering in all three sentences.

00:02:28.865 --> 00:02:33.935
So in the denominator we have three for each than log base e, three by threes 0.

00:02:33.935 --> 00:02:39.755
Now, the word Israel have lower importance because it is a commonly occurring words.

00:02:39.755 --> 00:02:42.710
Similarly for word good, it is occurring.

00:02:42.710 --> 00:02:46.925
And to document, If we apply log base e three by two,

00:02:46.925 --> 00:02:49.085
we will get a very low point for one.

00:02:49.085 --> 00:02:52.505
And then we can calculate for all the words.

00:02:52.505 --> 00:02:56.345
Service occurs only in one sentence or one document,

00:02:56.345 --> 00:02:58.310
so its value is 1.09.

00:02:58.310 --> 00:03:00.950
To calculate numeric value of each word,

00:03:00.950 --> 00:03:04.115
we take into account both TF and IDF.

00:03:04.115 --> 00:03:07.640
Simply multiply TF, IDF, for example,

00:03:07.640 --> 00:03:11.405
for what is TAP is 0.25 and IDF is 0.

00:03:11.405 --> 00:03:16.290
Similarly, you can calculate TF-IDF value for all the words.

00:03:19.450 --> 00:03:24.500
Now you can see that words are given importance based on how many times they're

00:03:24.500 --> 00:03:29.675
occurring in a sentence and how many times they're occurring in all the sentences.

00:03:29.675 --> 00:03:33.110
Unlike bag-of-words model, we give more importance

00:03:33.110 --> 00:03:36.965
towards which occur more number of times in a particular sentence,

00:03:36.965 --> 00:03:39.470
but they are lists spread out.

00:03:39.470 --> 00:03:46.400
This is TF-IDF model using which you can convert takes to numeric format.

00:03:46.400 --> 00:03:49.640
Now once you have this text in numeric format,

00:03:49.640 --> 00:03:52.520
this can we fit to a machine learning model?

00:03:52.520 --> 00:03:56.135
Each of these words in a text based classification system

00:03:56.135 --> 00:03:59.060
would be a feature or independent variables.

00:03:59.060 --> 00:04:03.680
And your dependent variable would be whether the sentiment is positive or not.

00:04:03.680 --> 00:04:10.410
That can be represented in numeric format is one or geo instead of positive or negative.
