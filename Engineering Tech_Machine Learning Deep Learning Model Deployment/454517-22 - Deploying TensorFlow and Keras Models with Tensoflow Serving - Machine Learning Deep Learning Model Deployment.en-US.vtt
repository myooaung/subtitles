WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:01.160 --> 00:00:06.765
Tensorflow and kala-azar popular libraries for machine learning and deep learning.

00:00:06.765 --> 00:00:09.930
Tensorflow provides something called TensorFlow Serving,

00:00:09.930 --> 00:00:13.275
using which you can save and export your models.

00:00:13.275 --> 00:00:17.955
Tensorflow Serving is a high-performance modern serving system.

00:00:17.955 --> 00:00:21.345
Let's look at an example to understand this.

00:00:21.345 --> 00:00:25.275
We have logged into their Google collab environment.

00:00:25.275 --> 00:00:29.020
Let's create a new notebook.

00:00:30.320 --> 00:00:34.990
Tensorflow is pre-installed in the Google collab environment.

00:00:35.900 --> 00:00:38.340
If you're using any other environment,

00:00:38.340 --> 00:00:42.005
you have to make sure tens of kilo and TensorFlow servings are installed.

00:00:42.005 --> 00:00:45.710
Let's give this file a name.

00:00:47.280 --> 00:00:50.695
We'll first import the standard liberties.

00:00:50.695 --> 00:00:54.489
Let's import numpy pandas.

00:00:54.489 --> 00:00:59.485
And we also need TensorFlow library will important supplies.

00:00:59.485 --> 00:01:04.870
Pf, TensorFlow 2.2x has

00:01:04.870 --> 00:01:07.510
a tight integration with Kara celebrity and

00:01:07.510 --> 00:01:11.300
you do not need to import Cara's library separately.

00:01:12.570 --> 00:01:16.015
Get us Libraries the preferred way to build

00:01:16.015 --> 00:01:20.600
TensorFlow models with TensorFlow 2.0. and higher worsens.

00:01:21.540 --> 00:01:25.610
Next we'll import the store purchase data.

00:01:26.700 --> 00:01:32.930
And this time we have a large file with around 1500 records.

00:01:34.170 --> 00:01:37.825
Because we'll be using neural networks to build our model.

00:01:37.825 --> 00:01:42.980
We need slightly high volume of data to get higher accuracy.

00:01:43.590 --> 00:01:48.505
So this data set has similar data as before,

00:01:48.505 --> 00:01:51.770
and this is about 1500 records.

00:01:52.080 --> 00:01:54.850
Let's see sample the cuts we have AID,

00:01:54.850 --> 00:01:57.740
salary and purchase as before,

00:01:58.560 --> 00:02:03.350
will separate out the independent and dependent variables.

00:02:03.870 --> 00:02:08.750
We'll do train test split using scikit-learn library.

00:02:09.640 --> 00:02:13.745
Next, we'll feature scale using the standard scaler.

00:02:13.745 --> 00:02:15.960
The way we've done earlier.

00:02:18.520 --> 00:02:25.590
Now will build a neural network using TensorFlow cameras, models sequence helpless.

00:02:25.900 --> 00:02:31.175
And we'll have to dense layer of ten neurons each.

00:02:31.175 --> 00:02:34.355
And the output layer will have two values.

00:02:34.355 --> 00:02:37.445
That is whether a customer is purchased or not.

00:02:37.445 --> 00:02:39.440
And in terms of activation,

00:02:39.440 --> 00:02:43.700
methods will use railway activation for the hidden layers.

00:02:43.700 --> 00:02:47.250
And we'll use softmax for the output layer.

00:02:50.050 --> 00:02:54.270
Now our model is ready to be trained.

00:02:55.840 --> 00:03:02.610
We lose the Adam optimizer and cross entropy technique to measure the loss.

00:03:04.720 --> 00:03:08.705
Next will train the model using the cleaning data.

00:03:08.705 --> 00:03:11.880
And we'll specify 50 books.

00:03:12.550 --> 00:03:15.365
This is how with a very few lines of code,

00:03:15.365 --> 00:03:18.410
you can build a neural network with density locators.

00:03:18.410 --> 00:03:27.000
Lovely. Model training is over and we can see that we got ninety-five percent accuracy.

00:03:28.930 --> 00:03:32.945
We can pass the test data and extract the Law Center.

00:03:32.945 --> 00:03:36.240
From the model dot-dot-dot evaluate method.

00:03:38.560 --> 00:03:41.645
So a courtesy for this model is quite high.

00:03:41.645 --> 00:03:43.970
About ninety-five percent.

00:03:43.970 --> 00:03:49.250
Model.predict gives us different starts around the model.

00:03:49.250 --> 00:03:52.470
How many layers and parameters?

00:03:53.710 --> 00:03:57.440
Ten supplicate as model is a predict method using which we

00:03:57.440 --> 00:04:01.290
can predict data for a new set of values.

00:04:02.140 --> 00:04:06.140
For another customer with ties 4,250 thousand,

00:04:06.140 --> 00:04:07.730
the probability of buying his,

00:04:07.730 --> 00:04:10.500
but this model is 0.6.

00:04:11.350 --> 00:04:15.420
And for a customer retains to D8 salary 40 thousand,

00:04:15.420 --> 00:04:18.730
probability is very low should this customer will not buy.

00:04:18.730 --> 00:04:22.610
And the other one with is what to encrypt Italian. Goodbye.

00:04:24.030 --> 00:04:28.510
Let's now see how to save this model using tensorflow serving.

00:04:28.510 --> 00:04:31.930
You simply invoke the modal dot-dot-dot save method,

00:04:31.930 --> 00:04:37.850
giver modelling name, and avoid some number and save the model.

00:04:40.440 --> 00:04:44.870
It will get saved to the project folder.

00:04:45.570 --> 00:04:48.190
In this case, in the Columbian moment,

00:04:48.190 --> 00:04:51.560
we can do a list and see the directory.

00:04:52.950 --> 00:04:56.450
Let's go inside the directory.

00:04:58.050 --> 00:05:02.650
We can see a directory with worsen m1 will go inside

00:05:02.650 --> 00:05:10.690
that saved model.predict B,

00:05:10.690 --> 00:05:14.530
that is the model that is saved in portable of file palmer.

00:05:14.530 --> 00:05:16.525
We didn't variables.

00:05:16.525 --> 00:05:21.820
Tensorflow stores all the check points and we didn't assets all the graphs.

00:05:21.820 --> 00:05:25.060
Now to use this model in another environment,

00:05:25.060 --> 00:05:29.230
we have to export this model files and Lord them,

00:05:29.230 --> 00:05:31.970
and then use the model to predict.

00:05:33.060 --> 00:05:37.105
Let's see how that can be done in this notebook.

00:05:37.105 --> 00:05:41.330
At the same technique would work in any other environment.

00:05:42.630 --> 00:05:47.180
From tens uploaded Cara's Import Load model.

00:05:48.330 --> 00:05:52.120
Now we'll declare a new model class,

00:05:52.120 --> 00:05:56.065
and we'll call the Lord Martel and pass the directory name.

00:05:56.065 --> 00:06:00.440
And the model will get loaded to the cost model variable.

00:06:01.080 --> 00:06:07.130
Once that is done, we can predict the way we predicted earlier using the new variable.

00:06:07.770 --> 00:06:11.125
To export the model from the Columbia environment,

00:06:11.125 --> 00:06:14.000
you can zip it and download it.

00:06:14.700 --> 00:06:17.170
Calabresi Linux environment.

00:06:17.170 --> 00:06:19.315
So use zip minus r,

00:06:19.315 --> 00:06:25.450
that is recursive zipping and create a zip file port,

00:06:25.450 --> 00:06:28.375
the files library to download the file.

00:06:28.375 --> 00:06:33.590
Make sure you are using Chrome browser or growing the Download might not work.

00:06:35.880 --> 00:06:38.050
The file got downloaded.

00:06:38.050 --> 00:06:41.180
Let's open it and see what is inside it.

00:06:44.490 --> 00:06:48.590
We can see the saved model protocol file.

00:06:49.410 --> 00:06:53.990
This is the main model file which has all the details.

00:06:56.460 --> 00:07:01.270
You can take the entire model directory to another environmental music.

00:07:01.270 --> 00:07:08.180
They're loaded the way we load it in the Colombian moment and use it.
