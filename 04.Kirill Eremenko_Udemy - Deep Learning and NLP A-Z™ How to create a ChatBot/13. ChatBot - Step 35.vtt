WEBVTT
1
00:00:00.570 --> 00:00:04.750
Three two one go let's start the training.

2
00:00:04.820 --> 00:00:13.860
So we're going to start by introducing the Bache index check training class that we're going to say

3
00:00:13.860 --> 00:00:15.120
equal to 100.

4
00:00:15.180 --> 00:00:21.360
And that's because we will check the training class every 100 badges so you'll see how we'll use it

5
00:00:21.360 --> 00:00:22.030
later.

6
00:00:22.290 --> 00:00:28.940
Then I'm going to keep it as viable because the second variable we will introduce is going to read a

7
00:00:28.950 --> 00:00:37.140
batch index check validation lots and that the validation nurse who will check it halfway and at the

8
00:00:37.140 --> 00:00:37.980
end of an epoch.

9
00:00:38.160 --> 00:00:42.740
And therefore since the middle of an epoch corresponds to half of the number of batches.

10
00:00:43.020 --> 00:00:48.660
I'm going to get this half of the number of batches by taking all our questions.

11
00:00:48.660 --> 00:00:50.620
That is the total number of our questions.

12
00:00:50.790 --> 00:01:00.560
So Len training questions that I'm going to divide by the batch size to get the total number of batches

13
00:01:01.080 --> 00:01:08.730
and then I'm going to divide by 2 to get half of this number of batches and then I just need to use

14
00:01:08.910 --> 00:01:18.840
a minus one just around the down then I'm going to introduce the tural training last error and I'm going

15
00:01:18.840 --> 00:01:25.530
to initialize it to zero and that will be used to compute the sum of the training losses on 100 batches

16
00:01:25.770 --> 00:01:30.350
because we chose to check the last every 100 batches.

17
00:01:30.360 --> 00:01:30.740
All right.

18
00:01:30.750 --> 00:01:38.460
And then I'm going to copy this because the next variable that I'm going to introduce is first not a

19
00:01:38.460 --> 00:01:47.580
total less error but a list of less errors and not at the training but the validation that's going to

20
00:01:47.580 --> 00:01:50.090
be the least of the validation plus errors.

21
00:01:50.160 --> 00:01:54.900
And why do we have to make a list that's because we're going to use the early stopping technique which

22
00:01:54.900 --> 00:02:01.800
consists of checking if we managed to reach a loss that is below the minimum of all the losses we got

23
00:02:02.040 --> 00:02:05.190
and all the losses we get we're going to put them in that list.

24
00:02:05.190 --> 00:02:05.510
All right.

25
00:02:05.520 --> 00:02:09.060
And I'm going to initialize this as an analyst then.

26
00:02:09.120 --> 00:02:16.470
Speaking of early stopping I'm introducing early being and I'm going to introduce to early stopping

27
00:02:16.470 --> 00:02:22.080
Voivode the first one is going to be early stopping to check that it's going to be zero and that is

28
00:02:22.080 --> 00:02:28.380
going to correspond to the number of checks each time there is no improvement of the validation.

29
00:02:28.410 --> 00:02:34.140
So each time we don't reduce the validation less early stopping check is going to be incremented by

30
00:02:34.140 --> 00:02:39.250
1 and once it reaches a certain number which is going to be our next variable here.

31
00:02:39.420 --> 00:02:41.030
Well we will stop everything.

32
00:02:41.030 --> 00:02:42.630
We will stop the training.

33
00:02:42.900 --> 00:02:47.670
And that number from which we're going to stop we're going to call it early.

34
00:02:47.670 --> 00:02:53.810
STOP BEING stop and that is going to be equal to 1000.

35
00:02:54.000 --> 00:02:59.640
So 1000 is a bit high but that's just because I want to make the training last until the last book that

36
00:02:59.640 --> 00:03:00.660
is 100.

37
00:03:00.690 --> 00:03:05.640
But if you want to really apply early stopping I would recommend to choose rather 100.

38
00:03:06.060 --> 00:03:09.450
All right and then we're going to get our check point.

39
00:03:09.450 --> 00:03:15.630
That's just to save the weights which we will be able to load whenever we want to chat with our trained

40
00:03:15.780 --> 00:03:16.050
chat.

41
00:03:16.070 --> 00:03:24.220
But I'm going to put them in quotes and I'm going to call them shots but underscore weights that S.K.

42
00:03:24.250 --> 00:03:25.640
Peachi.

43
00:03:25.680 --> 00:03:26.280
All right.

44
00:03:26.400 --> 00:03:28.990
That's going to be the file containing the weights.

45
00:03:29.340 --> 00:03:36.120
Then we're going to run our session by taking our session object and then using the run method.

46
00:03:36.270 --> 00:03:41.940
And inside this run method we have to initialize all the global variables by taking first the tenths

47
00:03:41.940 --> 00:03:50.610
of the libraries and then taking the global variables initialiser function like that.

48
00:03:50.840 --> 00:03:51.980
OK so far.

49
00:03:51.980 --> 00:03:52.580
Perfect.

50
00:03:52.580 --> 00:03:58.520
Next step the next step is to start the big fall up this huge fall that it's going to do all the training

51
00:03:58.810 --> 00:03:59.610
so for.

52
00:03:59.630 --> 00:04:03.090
It's going to be of course a full loop over the book.

53
00:04:03.350 --> 00:04:12.230
So for Epoque in range and we're going to start from one up to the total number of epochs we chose and

54
00:04:12.230 --> 00:04:13.520
the hyper parameters.

55
00:04:13.750 --> 00:04:19.160
And that was given by the variables ebox only because the upper bound in a range is excluded.

56
00:04:19.340 --> 00:04:21.940
We want to add a plus one here.

57
00:04:22.190 --> 00:04:27.350
Right and now let's go inside this for loop and there goes already a second for you because of course

58
00:04:27.620 --> 00:04:33.890
in each epoch we have to loop over all the batches because I remind that an epoch is completed when

59
00:04:33.950 --> 00:04:40.120
all the batches go back and forth to double record a new one that works in our second segment.

60
00:04:40.370 --> 00:04:44.910
So four and now we're going to use the split in two batches.

61
00:04:44.990 --> 00:04:49.920
Function that we made in the previous code section to get the batch indexes.

62
00:04:50.060 --> 00:04:51.380
That is the index of the batch.

63
00:04:51.470 --> 00:04:56.270
And then for each of these vetches Well we're going to get the Pettitt questions in the batch and the

64
00:04:56.260 --> 00:04:57.980
parrot answers in the batch.

65
00:04:58.130 --> 00:05:00.720
And therefore my variables are going to be here.

66
00:05:00.860 --> 00:05:03.890
First batch index.

67
00:05:04.010 --> 00:05:19.100
And then in a couple my battered questions in the batch and my bad answers in the batch and all of them

68
00:05:19.280 --> 00:05:20.280
we're going to get it.

69
00:05:20.330 --> 00:05:27.700
Thanks to first the enumerate function because they split into batches function returns several elements.

70
00:05:28.010 --> 00:05:37.120
And now that's when we get our split into batches function that remember it takes three arguments.

71
00:05:37.190 --> 00:05:48.430
First are training questions the inputs then our training answers the targets and then the batch size

72
00:05:49.650 --> 00:05:49.890
right.

73
00:05:49.890 --> 00:05:51.330
And that will give us what we want.

74
00:05:51.420 --> 00:06:00.030
That is deliverables perfect than color and next step the next step is to get the starting sign.

75
00:06:00.090 --> 00:06:05.010
That is we're going to measure the time of the training of each batch and therefore we're going to get

76
00:06:05.010 --> 00:06:06.140
the starting time here.

77
00:06:06.180 --> 00:06:10.410
Thanks to first time librarian then the time function.

78
00:06:10.420 --> 00:06:10.850
All right.

79
00:06:10.850 --> 00:06:14.580
And then we'll get unending time and it will do the difference between the end time and starting time

80
00:06:14.580 --> 00:06:22.980
to get the time at the bench then next step is to get the training last air of this specific batch we're

81
00:06:22.980 --> 00:06:27.710
dealing with right now and to get this batch trending less error.

82
00:06:27.810 --> 00:06:33.780
We're going to run our session with our optimizer and the last area that we'll put into a list and the

83
00:06:33.780 --> 00:06:38.880
inputs to targets to learn to read the sequence length and to keep probability Well basically everything

84
00:06:38.880 --> 00:06:45.200
that we need to get the training class error as we defined in the previous code section to get the training

85
00:06:45.210 --> 00:06:46.730
that are on this batch.

86
00:06:46.740 --> 00:06:52.040
And since this run method by the session object is going to return two elements.

87
00:06:52.200 --> 00:06:56.820
And since actually the best training last Erricker response to the second element we have to start with

88
00:06:56.820 --> 00:07:02.400
an underscore here to say that we don't want the first elements returns but only the second one which

89
00:07:02.400 --> 00:07:08.590
is the one we want and that is batch training last hour.

90
00:07:08.620 --> 00:07:09.050
All right.

91
00:07:09.060 --> 00:07:16.530
And that's where our session comes into play from this session we're going to take the run method and

92
00:07:16.530 --> 00:07:25.520
first we need to get in a list our optimizer with grades and clipping applied.

93
00:07:25.650 --> 00:07:27.500
And of course the loss.

94
00:07:28.260 --> 00:07:40.230
And then in the dictionary you will get first the inputs which are of course are bad questions in batch

95
00:07:40.560 --> 00:07:51.290
then we're going to get our targets which are of course or padded answers and batch then the learning

96
00:07:51.290 --> 00:07:52.780
rates are.

97
00:07:53.030 --> 00:08:03.020
And remember the hyper parameters learning rates then the next one is the sequence length and then we

98
00:08:03.020 --> 00:08:11.090
can get it by taking the answers and batch the best we're dealing with right now and then taking the

99
00:08:11.090 --> 00:08:16.880
shape of index one of these balances and that that gives us the sequence length.

100
00:08:16.880 --> 00:08:26.090
We already covered that in one of the quotations before then last one the keep trub parameter that remember

101
00:08:26.090 --> 00:08:34.160
when the hyper parameters we took keep the score up but Billy T which is equal to 50 percent.

102
00:08:34.310 --> 00:08:34.930
Great.

103
00:08:34.970 --> 00:08:40.020
So then now that we've just got our batched training plus error.

104
00:08:40.160 --> 00:08:46.000
Well we're going to add it to the total training less error because then we want to get the total training

105
00:08:46.000 --> 00:08:48.710
less error on 100 batches.

106
00:08:48.710 --> 00:08:54.890
And then we'll do an average and therefore I'm going to do now I'm going to take this total training

107
00:08:55.120 --> 00:09:03.210
error that I'm copying pasting in here and adding to it the batched training last error.

108
00:09:03.300 --> 00:09:08.390
So I'm copying this and pasting that here.

109
00:09:08.390 --> 00:09:08.720
All right.

110
00:09:08.730 --> 00:09:09.560
Perfect.

111
00:09:09.660 --> 00:09:11.050
Then next.

112
00:09:11.190 --> 00:09:15.350
So the next step is simple it's just to get the ending time.

113
00:09:15.540 --> 00:09:21.530
Which we can get just by measuring the time again with the time function by the time library.

114
00:09:21.840 --> 00:09:30.690
The next step is to get the training time of this batch which is well let's call it batch time which

115
00:09:30.690 --> 00:09:38.350
is equal to the end in time minus the starting time.

116
00:09:38.540 --> 00:09:42.590
All right so that will give us the time of the training of this batched this specific batch we're dealing

117
00:09:42.590 --> 00:09:43.740
with right now in the loop.

118
00:09:43.760 --> 00:09:50.390
The second for loop and now we are going to compute the average of the training less errors on 100 batches

119
00:09:50.690 --> 00:09:55.550
and we will print that error to keep track of the training errors.

120
00:09:55.940 --> 00:10:02.030
All right so to get these 100 batches we're going to do that with an IF condition that will check if

121
00:10:02.030 --> 00:10:06.440
the Bache index is divisible by.

122
00:10:06.500 --> 00:10:11.660
Well the Bache index check stringless that is 100.

123
00:10:11.720 --> 00:10:19.940
So if it is divisible by 100 which we can check by seeing if the rest of the division of batch indexed

124
00:10:19.940 --> 00:10:28.280
by batch index check training is equal to zero then that means that we would reach 100 batches or 200

125
00:10:28.280 --> 00:10:29.550
batches or 300.

126
00:10:29.630 --> 00:10:32.130
That's basically every 100 batches.

127
00:10:32.330 --> 00:10:38.090
And so if that's the case we're going to print the book the batch the average training last error on

128
00:10:38.100 --> 00:10:43.250
100 batches and the training time on these same 100 batches.

129
00:10:43.490 --> 00:10:46.780
So let's do this let's make this print quickly.

130
00:10:46.880 --> 00:10:50.800
So we have to put double quotes first.

131
00:10:50.880 --> 00:10:59.740
First we're going to print the e-book and that's going to be with three figures over the total number

132
00:10:59.740 --> 00:11:05.510
of Epoque that we have to put in brackets I'm going to give you some recommendation on the syntax I'm

133
00:11:05.510 --> 00:11:06.190
using.

134
00:11:06.250 --> 00:11:13.690
Don't worry about this then we're going to print the batch we reached the batched index and since we

135
00:11:13.690 --> 00:11:20.800
have a lot of batches we're going to use Well this time four figures over again.

136
00:11:20.800 --> 00:11:23.650
The total number of batches.

137
00:11:23.980 --> 00:11:32.090
Then the training last error of course and this time we're going to have some decimals in the lower

138
00:11:32.120 --> 00:11:39.160
figures so let's put six figures with three decimal and that's a float.

139
00:11:39.260 --> 00:11:47.780
And also we can add the training time on one hundred batches to be specific.

140
00:11:48.020 --> 00:11:51.110
And that will be an integer.

141
00:11:51.260 --> 00:11:57.840
So Carl in the end we can add then seconds to specify that it's measured in seconds.

142
00:11:57.860 --> 00:12:02.400
All right and then we need to add all our variables in format.

143
00:12:02.630 --> 00:12:10.670
And so first we wanted the puck reached then we need to print the total number of books which we get

144
00:12:10.700 --> 00:12:12.870
thanks to a viable box with an s.

145
00:12:12.920 --> 00:12:15.830
Then we need the Bachche index.

146
00:12:15.830 --> 00:12:23.180
Then the total number of batches which we can get by dividing the total number of training questions

147
00:12:23.660 --> 00:12:28.870
by the batch size.

148
00:12:28.950 --> 00:12:36.990
Then finally what's most interesting the average of the training errors on these 100 batches and that

149
00:12:37.020 --> 00:12:48.730
we can get by dividing the total training last error by the Bache index check.

150
00:12:49.260 --> 00:12:52.610
Training last which is equal to 100.

151
00:12:52.830 --> 00:13:00.750
And finally the training time of these 100 batches which we can get by taking first in function to make

152
00:13:00.750 --> 00:13:03.570
sure we get an integer number of seconds.

153
00:13:03.600 --> 00:13:08.310
So int batch time times.

154
00:13:08.520 --> 00:13:12.960
Well this same batch index check training class which is equal to 100.

155
00:13:12.960 --> 00:13:18.290
And that gives us the training time of 100 vetches perfect.

156
00:13:18.340 --> 00:13:20.110
We're done with the printing.

157
00:13:20.190 --> 00:13:25.020
Now next step the next step is well first to make sure we're still in the if that's the case.

158
00:13:25.020 --> 00:13:25.680
Perfect.

159
00:13:25.890 --> 00:13:35.550
And to re-initialize the tool training last error to zero because that was just to compute the total

160
00:13:35.550 --> 00:13:40.170
trailing loss error of 100 batches and we were done dealing with these 100 batches.

161
00:13:40.230 --> 00:13:43.680
So we're going to have to do the same thing for the next 100 batches.

162
00:13:43.680 --> 00:13:51.250
All right then the next step is first get out of this if condition and do a new IF condition to this

163
00:13:51.260 --> 00:13:53.720
time take care of the validation set.

164
00:13:53.720 --> 00:13:54.990
So we're going to do the same.

165
00:13:54.990 --> 00:13:59.620
We're going to compute an average of some validation plus errors.

166
00:13:59.610 --> 00:14:04.770
But on the validation set and so the first thing we're going to do isn't if condition because remember

167
00:14:04.770 --> 00:14:08.570
we want to check the validation error halfway.

168
00:14:08.610 --> 00:14:16.260
And at the end of an epoch and therefore to do this we're going to check if our batch index that we

169
00:14:16.260 --> 00:14:25.720
have reached so far is divisible by this time the Bache index check validation less which is equal to

170
00:14:25.860 --> 00:14:32.790
how much of the total number of batches and therefore and pasting them here and I'm checking if this

171
00:14:32.790 --> 00:14:34.610
is equal to zero.

172
00:14:34.860 --> 00:14:38.520
But then we don't want to include the first batch.

173
00:14:38.580 --> 00:14:45.420
So I'm excluding it by checking if the batched index is larger than zero.

174
00:14:45.420 --> 00:14:48.140
All right so if that's the case where are we going to do.

175
00:14:48.330 --> 00:14:58.680
Well first we're going to initialize the total validation last error to zero exactly as we did before

176
00:14:58.680 --> 00:15:00.460
with the total trending last error.

177
00:15:00.720 --> 00:15:06.470
Then we're going to get a starting time because we're going to measure the time of each batch again.

178
00:15:06.510 --> 00:15:13.400
So talling time that we get things to our time library and then the time function then what we're going

179
00:15:13.400 --> 00:15:15.840
to do is a new for loop.

180
00:15:15.840 --> 00:15:20.520
Still in this condition there is actually going to be the same for loop as what we did here.

181
00:15:20.640 --> 00:15:24.290
But of course replacing all the training by validation.

182
00:15:24.630 --> 00:15:28.100
So let's copy this second for loop.

183
00:15:28.350 --> 00:15:31.200
That's quite dangerous to do it but we'll manage.

184
00:15:31.500 --> 00:15:35.670
Let's copy it and let's paste it here.

185
00:15:35.670 --> 00:15:37.760
Just below starting time.

186
00:15:37.760 --> 00:15:38.180
All right.

187
00:15:38.210 --> 00:15:41.050
And now let's make the required changes.

188
00:15:41.280 --> 00:15:47.550
So first of all we're going to give a different name to that Blueboy will batched index to specify that

189
00:15:47.550 --> 00:15:52.410
this time it's going to be the index of a batch in the validation set.

190
00:15:52.410 --> 00:15:55.420
So I'm specifying here badge index validation.

191
00:15:55.680 --> 00:16:00.690
Then still we're going to get our Pettitt questions in the batch pat answers in the batch and we're

192
00:16:00.690 --> 00:16:03.210
going to get things to split into batches function.

193
00:16:03.300 --> 00:16:09.630
But that this time is going to take as arguments not the training questions and the training answers

194
00:16:09.660 --> 00:16:12.420
but the validation questions.

195
00:16:12.480 --> 00:16:19.760
And of course the validation answers to validation questions validation answers and the same batch size

196
00:16:19.760 --> 00:16:20.990
of course.

197
00:16:21.200 --> 00:16:25.780
And let's go back then since we already introduce here the training time.

198
00:16:25.910 --> 00:16:27.420
We don't need that.

199
00:16:27.430 --> 00:16:29.250
I'm removing this line.

200
00:16:29.540 --> 00:16:32.000
And then a very important thing to change here.

201
00:16:32.240 --> 00:16:33.840
When we run our session here.

202
00:16:33.980 --> 00:16:40.100
Well since we're not doing some training here with the validation because the validation consists of

203
00:16:40.100 --> 00:16:46.880
just testing your model on new observations which won't be used for the training well for that reason

204
00:16:46.880 --> 00:16:49.010
we don't need the optimizer.

205
00:16:49.010 --> 00:16:54.480
The optimizer is only used to compute the gradient and therefore only used for the training.

206
00:16:54.560 --> 00:17:00.190
So I'm removing this and also these square brackets here we don't need them.

207
00:17:00.260 --> 00:17:01.680
We just need the last error.

208
00:17:01.850 --> 00:17:07.940
And then since we removed the optimizer Well this run method is going to return just one argument which

209
00:17:07.940 --> 00:17:17.390
will be the Bachche not training but validation less error and therefore I'm removing this underscore

210
00:17:17.630 --> 00:17:20.010
because it only returns one argument.

211
00:17:20.180 --> 00:17:29.180
Great then we need to align things here for the target then the learning rate then the sequence length

212
00:17:29.360 --> 00:17:33.000
and keep probability but there does that ring the alarm.

213
00:17:33.140 --> 00:17:39.410
Well the keep probability is only used for the training because that's used to activate the neuron with

214
00:17:39.410 --> 00:17:43.150
a certain probability which we chose to be 50 percent.

215
00:17:43.190 --> 00:17:49.190
But here we're dealing with the validation set and therefore the probability is always 1 because the

216
00:17:49.190 --> 00:17:51.090
neurons always have to be present.

217
00:17:51.260 --> 00:17:55.900
Deactivating them with a probability of 50 percent is just to improve the training.

218
00:17:56.060 --> 00:18:00.420
And therefore instead of keep probability here I'm going to put one right.

219
00:18:00.560 --> 00:18:03.920
And that said that was the only replacement we had to make here.

220
00:18:03.920 --> 00:18:13.480
All right then we add the batch not training but validation last error to our total not training less

221
00:18:13.490 --> 00:18:16.710
error but validation less error.

222
00:18:16.970 --> 00:18:22.700
And then we get the ending time but it has to go outside of the loop because we started the time before

223
00:18:22.700 --> 00:18:29.830
the loop and therefore same for the batch time which is equal to the anti-man is the starting time perfect.

224
00:18:29.960 --> 00:18:35.900
We are done with the second loop and therefore we are ready to compute the average of these validation

225
00:18:35.900 --> 00:18:40.470
losses and that's exactly what we're going to do now and to compute it we're going to introduce first

226
00:18:40.520 --> 00:18:49.750
a new variable that I'm going to call average validation last error that is going to be equal of course

227
00:18:49.750 --> 00:19:00.590
to the total validation less error divided by the number of matches in the validation set of our questions.

228
00:19:00.880 --> 00:19:11.350
And therefore I'm taking here the length of the validation questions divided by the batch size and that

229
00:19:11.350 --> 00:19:15.000
gives me exactly the number of batches in the validation set.

230
00:19:15.020 --> 00:19:15.770
All right.

231
00:19:15.850 --> 00:19:21.820
And therefore I get the average validation error which is exactly what I want to print right now at

232
00:19:21.820 --> 00:19:27.400
this stage of the loop and therefore we're going to do a quick print again we're going to print in quotes

233
00:19:27.490 --> 00:19:38.350
the validation last error going to scroll down a bit validation last error there will be a float that

234
00:19:38.350 --> 00:19:46.220
we have to put in brackets with let's say six figures and three decimals and I have to specify a float.

235
00:19:46.220 --> 00:19:56.580
Then we're going to print also the batch validation time and that would be an integer Caliente.

236
00:19:57.000 --> 00:20:05.240
And then we can specify it seconds again and then and the format function we need to enter these variables.

237
00:20:05.280 --> 00:20:11.020
So the first one is what we just computed here that is the average validation last error.

238
00:20:11.400 --> 00:20:13.960
That's what we'll get in these brackets here.

239
00:20:14.160 --> 00:20:15.720
So that's the first one.

240
00:20:16.200 --> 00:20:24.810
And the second one is going to be simply the batch time that I make sure to be an int an integer are

241
00:20:24.870 --> 00:20:25.600
perfect.

242
00:20:25.620 --> 00:20:31.050
Next step the next step is to apply early stopping but we'll do a small break here and we'll do that

243
00:20:31.110 --> 00:20:34.140
in the next to toil until then Id on be.
