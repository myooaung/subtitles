1
00:00:00,520 --> 00:00:08,260
Hello and welcome to the final round of this part to building the sector model we now have all the tools

2
00:00:08,500 --> 00:00:13,250
that we just need to assemble to build this final set to set moral.

3
00:00:13,270 --> 00:00:15,080
And that's exactly what we're going to do.

4
00:00:15,130 --> 00:00:22,770
And this is oil we will just use the previous tools we made like first of all of course the encoder

5
00:00:22,870 --> 00:00:30,760
Arnon which is the first main part of the SEC to sec then the decoder Arnon which is the second main

6
00:00:30,760 --> 00:00:32,070
part of the SEC to Sec.

7
00:00:32,200 --> 00:00:38,810
And as we made things to these two functions here dico turning set and Dakotah set which we want to

8
00:00:38,830 --> 00:00:45,360
have to use again because these were just used to make the decoder Arnon the decoder record new in that

9
00:00:45,360 --> 00:00:46,030
work.

10
00:00:46,030 --> 00:00:53,260
So basically we will just assemble these two and could Arnon and decoder Arnon and that will be our

11
00:00:53,260 --> 00:00:57,290
final round for this part to building the same model.

12
00:00:57,340 --> 00:01:07,080
So let's do it let's make this final function that we're going to call sec 2 sec underscore morrow.

13
00:01:07,170 --> 00:01:07,590
All right.

14
00:01:07,600 --> 00:01:14,970
It's the final ultimatum model that we will chat with during our testing phase in part 4.

15
00:01:15,100 --> 00:01:16,470
Basically you got it.

16
00:01:16,510 --> 00:01:19,110
That's the brain of our checkbook.

17
00:01:19,570 --> 00:01:22,750
All right so this brain is going to take several arguments.

18
00:01:22,750 --> 00:01:28,810
The first one is going to be the inputs which are of course the questions of the Cornell movie corpus

19
00:01:28,810 --> 00:01:34,440
Dyalog data set and after the training these inputs will be of course the questions we'll ask to the

20
00:01:34,440 --> 00:01:34,640
chair.

21
00:01:34,660 --> 00:01:39,570
But when we check with it so input then target.

22
00:01:39,700 --> 00:01:44,680
So that's of course the answers the real answer is that we have.

23
00:01:44,680 --> 00:01:51,280
Because in our Cornell movie corpus Dyalog data set we have the real answers of the questions and from

24
00:01:51,280 --> 00:01:53,440
which we will train the child to speak.

25
00:01:53,500 --> 00:01:54,830
So that target.

26
00:01:54,940 --> 00:01:59,620
And then we have all our other arguments that we've used before.

27
00:01:59,620 --> 00:02:02,860
So the third one is keep prob.

28
00:02:03,310 --> 00:02:07,190
Then we have the batch size of course.

29
00:02:07,190 --> 00:02:15,610
We're working with patches as always with deep learning then sequence underscore length sequence length

30
00:02:16,150 --> 00:02:25,050
then we're going to get our answers none words which is the total number of words in all the answers.

31
00:02:25,090 --> 00:02:31,810
That's like our previous argument that we used before words here but this time since we're going to

32
00:02:31,810 --> 00:02:38,620
use both the total number of words of the answers and the questions I'm specifying answers here and

33
00:02:38,880 --> 00:02:46,530
in the next argument actually I'm going to specify the questions underscore no underscore words.

34
00:02:46,760 --> 00:02:50,770
That's another argument answers words and questions.

35
00:02:50,770 --> 00:02:51,540
Non-words.

36
00:02:51,640 --> 00:02:53,610
All right then what do we have.

37
00:02:53,770 --> 00:02:58,970
Then we have the and Khoder and bearing size.

38
00:02:59,110 --> 00:03:04,520
And that's the number of dimensions of the embedding matrix for the anchor.

39
00:03:04,780 --> 00:03:12,550
And then of course we'll have also the decoder and this core embedding and this core size which is this

40
00:03:12,550 --> 00:03:17,340
time the number of dimensions of the embedding matrix of the decoder.

41
00:03:17,380 --> 00:03:23,280
All right then we have of course the Arnon size which we used before.

42
00:03:23,470 --> 00:03:32,620
Then the number of layers in our decoder cell containing the stacked LSD with dropout applied non-players

43
00:03:32,770 --> 00:03:38,620
and then almost done we have a final argument to input and that's our questions.

44
00:03:38,800 --> 00:03:47,560
Word 2 and dictionary are questions words too in dictionary which we'll use to preprocess the targets.

45
00:03:47,590 --> 00:03:54,610
Remember if we have a look again at our preprocess target function right here.

46
00:03:55,470 --> 00:03:56,460
Pre-process targets.

47
00:03:56,470 --> 00:03:58,500
Well it takes where to end.

48
00:03:58,540 --> 00:04:01,060
As one of the argument which is the dictionary.

49
00:04:01,120 --> 00:04:05,370
And so the dictionary will use here will be questions words to it.

50
00:04:05,470 --> 00:04:05,950
All right.

51
00:04:05,950 --> 00:04:13,620
So we are done with the arguments that was the last one here questions words to end so let's add a colon

52
00:04:13,660 --> 00:04:16,570
here and then let's go inside the function.

53
00:04:16,600 --> 00:04:24,130
So this function is supposed to return in the end the training predictions and the test predictions.

54
00:04:24,130 --> 00:04:26,430
That's basically what we've returned before.

55
00:04:26,560 --> 00:04:33,610
But we want all in one package which is the sectors like model that will return at the same time the

56
00:04:33,610 --> 00:04:35,660
training predictions and the test predictions.

57
00:04:35,830 --> 00:04:41,590
And therefore that's why the best word to describe what we're doing right now is some blush like the

58
00:04:41,590 --> 00:04:42,490
French word.

59
00:04:42,550 --> 00:04:49,150
We're going to assemble the inquiry that returns the inkers date and the dickered returns the training

60
00:04:49,150 --> 00:04:51,250
predictions and the test predictions.

61
00:04:51,250 --> 00:04:56,350
But we also need to encourage Of course because in order for the decoder to return the training predictions

62
00:04:56,350 --> 00:05:01,830
and test predictions it needs to take as input the encoder States returned by the encoder.

63
00:05:01,900 --> 00:05:03,920
And that's why we need those two networks here.

64
00:05:04,030 --> 00:05:07,860
And we actually need to start Of course with the encoder.

65
00:05:08,140 --> 00:05:14,290
But before we get in good state we need the encoder embedded input and that's why the first thing we

66
00:05:14,290 --> 00:05:19,960
have to do here is to introduce the encoder and embedded input.

67
00:05:19,990 --> 00:05:25,360
We've already seen this very well and this are embedded in pursuit and again I will put the resource

68
00:05:25,600 --> 00:05:27,260
for you to see how it works.

69
00:05:27,290 --> 00:05:33,440
We're going to get it from the MBRD sequence by the layers module of the terms of the library.

70
00:05:33,460 --> 00:05:40,210
I'll put that in the resource and therefore I'm going to do now is take for us tens of flow T.F. then

71
00:05:40,300 --> 00:05:42,500
country of course as usual.

72
00:05:42,670 --> 00:05:49,810
And then from country I'm getting this layer's module and from the sloes module I'm getting the MBRD

73
00:05:50,110 --> 00:05:58,780
underscore sequence which will return the embedded input of the anchor and this and that sequence function

74
00:05:58,780 --> 00:06:00,310
takes several arguments.

75
00:06:00,430 --> 00:06:03,100
The first one is our inputs.

76
00:06:03,100 --> 00:06:06,750
Of course that's the inputs we actually want to embed.

77
00:06:06,760 --> 00:06:13,330
So that's of course one of the argument then the next argument is going to be the total number of words

78
00:06:13,420 --> 00:06:18,040
of the answers so that answers none words.

79
00:06:18,100 --> 00:06:22,920
And then we need to add a plus one because the upper bound in the sequence is excluded.

80
00:06:23,110 --> 00:06:25,120
So answers words plus one.

81
00:06:25,390 --> 00:06:34,120
Then the next argument is going to be the encoder and the ring size which I remind is number of dimensions

82
00:06:34,120 --> 00:06:36,860
in the embedding matrix at the Inquirer.

83
00:06:36,940 --> 00:06:46,600
So in Currumbin in size then next one that's going to be the initialiser which is going to be a random

84
00:06:46,600 --> 00:06:47,990
uniform initialiser.

85
00:06:48,220 --> 00:06:54,520
And to get it we are actually going to use tens of flow from tens of so we're going to get the random

86
00:06:55,090 --> 00:07:03,340
uniform initialiser function and this function takes two arguments argument the bounds of these random

87
00:07:03,340 --> 00:07:09,790
numbers we want to get based on a uniform distribution and these bands are going to be from zero to

88
00:07:09,790 --> 00:07:10,230
1.

89
00:07:10,390 --> 00:07:10,980
All right.

90
00:07:11,170 --> 00:07:13,750
So we've got our initialiser and that's it.

91
00:07:13,780 --> 00:07:17,710
That will give us the and color embedded in it.

92
00:07:17,770 --> 00:07:23,200
So again if you want more details on this and that sequence in its arguments check out the resources

93
00:07:23,290 --> 00:07:24,430
of this lecture.

94
00:07:24,430 --> 00:07:25,060
Perfect.

95
00:07:25,060 --> 00:07:29,740
Now let's move on to the next step which is to get our anchors to act.

96
00:07:29,770 --> 00:07:33,530
I remind that the anchoring state is what is returned by the Inquirer.

97
00:07:33,610 --> 00:07:38,380
That is the output of the anchor and what will become the input of the decoder.

98
00:07:38,630 --> 00:07:44,760
So that's the natural next step to get the output of the encoder and we can actually get this output

99
00:07:44,770 --> 00:07:48,740
the anchor state thanks to our embedded input.

100
00:07:49,000 --> 00:07:58,390
So let's get it and coner state which we will get from of course the record and neural network of our

101
00:07:58,390 --> 00:07:59,120
anchor.

102
00:07:59,290 --> 00:08:04,660
We're going to feed the record neural network of our encoder with the encoder embedded input and it

103
00:08:04,660 --> 00:08:09,020
will return the anchoring state which will then become the input of the Ticker.

104
00:08:09,270 --> 00:08:15,880
So when girls date then we get our encoder underscore Arnon the record renewal network of the encoder

105
00:08:15,880 --> 00:08:21,700
which is a function we defined previously and that function takes several arguments.

106
00:08:21,700 --> 00:08:26,190
The first one is of course well our anchor embedded input.

107
00:08:26,260 --> 00:08:33,030
That's basically the input could embedded in the right format to be accepted by the neural network and

108
00:08:33,260 --> 00:08:37,020
embed an input and then a couple of other arguments.

109
00:08:37,060 --> 00:08:47,110
The Arnon size the number of layers then they keep drub because of course in the uncurl we have the

110
00:08:47,110 --> 00:08:51,390
inkers cell which is stacked LACMA with out applied.

111
00:08:51,400 --> 00:08:57,130
So we have to include this key prob parameter that can the dropout rate keep prob..

112
00:08:57,140 --> 00:09:03,610
And now one final argument in this could go on and function can have a look at the function made again.

113
00:09:03,800 --> 00:09:11,100
But if you remember well the final argument is sequence underscore length.

114
00:09:11,120 --> 00:09:11,590
All right.

115
00:09:11,600 --> 00:09:18,880
Now we got our encoder state which is the output of our encoder record renewal network.

116
00:09:19,100 --> 00:09:25,700
And now that we have the increased data we are ready to get to our final predictions the training predictions

117
00:09:25,700 --> 00:09:26,910
and the test predictions.

118
00:09:27,140 --> 00:09:34,230
But before that we need to get first the pre-processed target because we will need them for the training.

119
00:09:34,250 --> 00:09:40,200
We also need to get the decoder embeddings matrix which is the embeddings matrix of the decoder.

120
00:09:40,400 --> 00:09:48,710
And that will be used to get Deaker embedded input and then that's where we'll be fully ready to apply

121
00:09:48,930 --> 00:09:55,580
to decode the Arnon function which represents the regular neural network of the decoder to get the final

122
00:09:55,670 --> 00:09:58,190
training predictions and test predictions.

123
00:09:58,520 --> 00:10:03,080
All right so let's get all these before we take our Dr. Arnon.

124
00:10:03,150 --> 00:10:07,010
Everything we need together here is our free throw.

125
00:10:07,010 --> 00:10:09,130
C'est targets.

126
00:10:09,170 --> 00:10:10,260
That is our target.

127
00:10:10,310 --> 00:10:13,180
Pre-process the right way to later on in the training.

128
00:10:13,180 --> 00:10:17,560
Use it to back propagate the last between the politicians and the target.

129
00:10:17,600 --> 00:10:20,780
So preprocess targets and these.

130
00:10:20,990 --> 00:10:29,690
Well remember we made a function for that which we called prae pro sess targets and this function takes

131
00:10:29,690 --> 00:10:31,130
several arguments.

132
00:10:31,130 --> 00:10:39,650
The first one is of course our target because that's what we want to preprocess so targets then to preprocess

133
00:10:39,650 --> 00:10:40,250
these targets.

134
00:10:40,250 --> 00:10:46,350
Remember I showed that to you at the beginning of the tutorial we need two questions words in dictionary

135
00:10:46,530 --> 00:10:58,970
so that our second argument word two in dictionary questions words to end and eventually the batch size.

136
00:10:58,970 --> 00:11:03,150
All right so now we have our preprocess targets.

137
00:11:03,260 --> 00:11:04,080
Perfect.

138
00:11:04,100 --> 00:11:10,120
Now what we need is the decoder embeddings matrix the embeddings matrix of the decoder.

139
00:11:10,220 --> 00:11:20,040
And so I'm introducing here this new variable decoder and beddings matrix which we will get by creating

140
00:11:20,310 --> 00:11:23,250
a sense of flow variable.

141
00:11:23,340 --> 00:11:28,200
So voidable is a class and therefore the decoder embeddings matrix will be an object of this variable

142
00:11:28,200 --> 00:11:34,710
class and this variable class takes several arguments which are going to be mostly the dimensions of

143
00:11:34,800 --> 00:11:36,750
this embedding matrix.

144
00:11:36,750 --> 00:11:43,110
So remember how the embeddings matrix works well you have basically all the words in lines and then

145
00:11:43,110 --> 00:11:45,760
in comes you have the embedding values.

146
00:11:45,780 --> 00:11:52,200
So since the number of lines corresponds to the total number of words in the whole corpus.

147
00:11:52,420 --> 00:11:56,060
Well this number of lines is going to be the questions no words.

148
00:11:56,070 --> 00:12:00,520
That is total number of words of the questions and again we need to add a plus one.

149
00:12:00,780 --> 00:12:03,670
And then for the number of columns the second dimension.

150
00:12:03,780 --> 00:12:09,430
Well it's going to be one of the arguments of this sector segmental function which is the decoder amening

151
00:12:09,480 --> 00:12:11,810
size the amening size of the decoder.

152
00:12:12,000 --> 00:12:18,420
So basically you can have several sizes for the embeddings that is several number of columns and that

153
00:12:18,420 --> 00:12:20,310
is determined by the embedding size.

154
00:12:20,610 --> 00:12:27,300
And so right now what we have to do in this variable is initialized this matrix not only setting the

155
00:12:27,300 --> 00:12:34,320
dimensions but also fill it with random numbers taken from a uniform distribution just to initialize

156
00:12:34,320 --> 00:12:40,890
it and then it will be of course populated with learning numbers that will participate into the training

157
00:12:41,130 --> 00:12:41,910
of the chat.

158
00:12:41,920 --> 00:12:47,550
But to speak like a human but at the beginning we have to initialize it with random numbers taken from

159
00:12:47,550 --> 00:12:53,240
a uniform distribution and to get these random numbers taken from that uniform distribution.

160
00:12:53,230 --> 00:13:01,890
We're going to use of flow of course and then the random uniform function random uniform function which

161
00:13:01,890 --> 00:13:07,800
will take as arguments the dimension of this matrix and the dimension of this matrix will have to enter

162
00:13:07,800 --> 00:13:09,710
them in square brackets.

163
00:13:09,750 --> 00:13:15,300
So as we said the number of lines is going to be the total number of words in the questions because

164
00:13:15,510 --> 00:13:21,700
each line correspond to a token a questions word and therefore the number of lines is going to be here.

165
00:13:21,810 --> 00:13:25,170
Questions not words.

166
00:13:25,470 --> 00:13:30,290
And as we said plus one questions words plus one.

167
00:13:30,290 --> 00:13:30,730
All right.

168
00:13:30,750 --> 00:13:32,400
That's the number of lines.

169
00:13:32,580 --> 00:13:33,770
The first time in Sion.

170
00:13:33,930 --> 00:13:39,360
And then for the second dimension that's the embedding size which is one of our arguments and that's

171
00:13:39,570 --> 00:13:40,620
of course for the record.

172
00:13:40,620 --> 00:13:46,060
So we'll get the decoder embedding size.

173
00:13:46,290 --> 00:13:46,850
Great.

174
00:13:46,920 --> 00:13:50,210
And so now we get out of the square brackets.

175
00:13:50,220 --> 00:13:55,470
We're done with the diamond JONES But we need to add two more arguments which are going to be the lower

176
00:13:55,470 --> 00:14:00,820
bound in the upper bound of those random numbers that we're filling this matrix with.

177
00:14:00,900 --> 00:14:07,980
And so we're going to fill this matrix with random numbers from 0 to 1.

178
00:14:08,100 --> 00:14:14,870
And all these random numbers will be taken from a uniform distribution and that's it that initializes

179
00:14:15,080 --> 00:14:22,220
the embeddings matrix of the decoder with random numbers between 0 and 1 taken from a uniform distribution.

180
00:14:22,230 --> 00:14:22,850
Great.

181
00:14:22,860 --> 00:14:30,810
Now we're ready to move on to the next step and the next step is naturally to use this decoder embeddings

182
00:14:30,810 --> 00:14:35,230
matrix to get that decoder embedded input.

183
00:14:35,250 --> 00:14:37,290
We're actually going to get these inputs.

184
00:14:37,410 --> 00:14:39,710
Thanks to the embeddings matrix.

185
00:14:39,810 --> 00:14:45,990
And so let's introduce this new variable decoder embedded input.

186
00:14:46,110 --> 00:14:53,050
And we're going to get these inputs from a function by sense of flow which is taken from the end module

187
00:14:53,250 --> 00:15:01,170
and the name of this function taken from the in a module is and then look up that's the function imbedding

188
00:15:01,500 --> 00:15:05,870
look up and this and then in Look-Up is of course going to take as input.

189
00:15:06,000 --> 00:15:09,010
The decoder embeddings matrix.

190
00:15:09,090 --> 00:15:13,710
So I'm going to copy that to avoid any typo.

191
00:15:13,830 --> 00:15:14,410
Copy.

192
00:15:14,460 --> 00:15:19,240
And that's going to be our first argument in this embedding lookup function.

193
00:15:19,240 --> 00:15:19,780
All right.

194
00:15:19,830 --> 00:15:26,810
And then the second argument that this embedding Look-Up is going to need is of course the pre-processed

195
00:15:26,810 --> 00:15:34,040
target because in order to get our decoder embedded input we need to pre-processed targets as they are

196
00:15:34,050 --> 00:15:36,210
the answers to the questions.

197
00:15:36,240 --> 00:15:44,100
So let's do it let's add the second argument pre pro SEST underscore targets and there we go.

198
00:15:44,100 --> 00:15:51,780
We now get our decoder embedded input which means we are ready to get the training predictions and the

199
00:15:51,780 --> 00:15:52,970
test predictions.

200
00:15:53,010 --> 00:15:59,230
And the reason for this is that we have everything we need to apply the decoder arland function.

201
00:15:59,340 --> 00:16:04,910
That is we have everything we need to feed the Recker a neural network of the decoder.

202
00:16:05,100 --> 00:16:11,730
And therefore now we're going to get our two ultimate variables that will be returned which are our

203
00:16:11,730 --> 00:16:21,210
training predictions and art test predictions are training predictions and our desperations which are

204
00:16:21,210 --> 00:16:28,320
respectively the predictions of the observations used for the training and the predictions of new observations

205
00:16:28,320 --> 00:16:30,030
that won't be used for the training.

206
00:16:30,120 --> 00:16:37,350
And that will either be used for cross-validation to test the predictive power of the model on new observations

207
00:16:37,350 --> 00:16:44,050
that won't be used to train the model more or some new predictions just to chat with the chad.

208
00:16:44,050 --> 00:16:49,370
But all right so make sure you understand the difference between these two predictions.

209
00:16:49,650 --> 00:16:56,260
And so now we're going to use our decoder because we have all the inputs we need to feed the doctors

210
00:16:56,280 --> 00:17:03,760
who were ready to get the output of the decoder and therefore we were ready to get the decoder and then

211
00:17:03,930 --> 00:17:05,510
the decoder record.

212
00:17:05,580 --> 00:17:07,130
Neural Network.

213
00:17:07,410 --> 00:17:14,280
And so now very easily We're going to just input all the arguments of this Daker Island function that

214
00:17:14,280 --> 00:17:14,870
we made.

215
00:17:14,940 --> 00:17:23,520
Starting with the decoder and bedded input that we just prepared here thanks to the record and beddings

216
00:17:23,520 --> 00:17:24,460
matrix.

217
00:17:24,450 --> 00:17:33,660
So decoder embedded input then next argument that's going can be actually the decoder embeddings matrix

218
00:17:34,290 --> 00:17:39,950
then we're going to need of course the encoder state because this is actually the output of the anchor

219
00:17:39,960 --> 00:17:42,540
that becomes the input of the decoder.

220
00:17:42,690 --> 00:17:46,540
And so let's get it and color state.

221
00:17:46,640 --> 00:17:53,190
All right the next argument is going to be the total number of words of the questions which we get with

222
00:17:53,280 --> 00:17:58,870
the questions number words argument.

223
00:17:58,870 --> 00:18:08,320
Then we're going to need the sequence lengths and after that the Arnon size then the number of layers

224
00:18:08,320 --> 00:18:09,010
in the.

225
00:18:09,020 --> 00:18:17,240
LS then are questions words 2 and dictionary then are.

226
00:18:17,250 --> 00:18:21,490
Keep drub parameter controlling dropout rate.

227
00:18:21,610 --> 00:18:31,930
And eventually last but not least the batch size all right and congratulations for your very last step

228
00:18:32,170 --> 00:18:35,480
of this part to building the sectors like moral.

229
00:18:35,530 --> 00:18:46,030
It's of course to return the training predictions and the test predictions are right.

230
00:18:46,060 --> 00:18:51,220
Congratulations you completed part to building the sectors like morrow.

231
00:18:51,220 --> 00:18:53,550
So now your child has a brain.

232
00:18:53,560 --> 00:18:54,960
It's time to train it.

233
00:18:55,000 --> 00:18:56,640
We'll do that in part three.

234
00:18:56,680 --> 00:18:58,420
Training the SEC to say moral.

235
00:18:58,480 --> 00:18:59,610
I can't wait to start.

236
00:18:59,620 --> 00:19:02,990
I can't wait to teach our children how to speak.

237
00:19:03,070 --> 00:19:08,410
Now that it has a brain and will do that starting from the next tutorial and now I'm going to close

238
00:19:08,410 --> 00:19:10,090
this part two on a good note.

239
00:19:10,090 --> 00:19:13,980
I am going to get our set to sec moral.

240
00:19:14,080 --> 00:19:15,160
Here is your brain.

241
00:19:15,160 --> 00:19:16,130
Time to train it.

242
00:19:16,240 --> 00:19:17,790
And until then I depend on.
