WEBVTT
1
00:00:00.500 --> 00:00:05.870
Hello and welcome to this new tutorial and the previous one we made to decode training said function

2
00:00:06.230 --> 00:00:09.820
that decoded the observations of the training set.

3
00:00:09.920 --> 00:00:15.980
And at the same time we turned the output of the decoder but that was for the observations of the training

4
00:00:15.980 --> 00:00:21.560
said that is some observations there are going back into the neural network to update the weights and

5
00:00:21.560 --> 00:00:25.000
improve the ability of the jackboot to talk like a human.

6
00:00:25.190 --> 00:00:28.960
But now in Mr. toile we're going to make kind of the same function.

7
00:00:29.150 --> 00:00:36.470
But for a new kind of observations these observations are actually the observations of the test set

8
00:00:36.710 --> 00:00:38.270
and the validation set.

9
00:00:38.270 --> 00:00:45.260
And basically these are new observations that won't be used for the training and that's why as you can

10
00:00:45.260 --> 00:00:52.280
see here I called this code section decoding the test validation set because the function we're about

11
00:00:52.280 --> 00:00:59.240
to make will be used not only to predict the observations of the test in the end that is the answers

12
00:00:59.300 --> 00:01:05.150
of the questions we'll be asking to the chat but in the test phase part 4 remember.

13
00:01:05.330 --> 00:01:07.860
But then also the validation set.

14
00:01:07.910 --> 00:01:13.450
And where does the validation set that the set we will make during the training also.

15
00:01:13.460 --> 00:01:22.580
But we will do some cross-validation technique to keep separately 10 percent of the trainset for cross-validation

16
00:01:22.580 --> 00:01:28.730
which is a technique that keeps a small part of the training data to test the predictive power on new

17
00:01:28.790 --> 00:01:29.720
observations.

18
00:01:29.900 --> 00:01:36.980
That's a very useful technique to not only reduce overfilling but also to improve the accuracy on your

19
00:01:36.980 --> 00:01:38.070
observations.

20
00:01:38.150 --> 00:01:44.150
And so the function we were about to make right now will be very similar to the one we just made only

21
00:01:44.150 --> 00:01:47.380
there is going to be one fundamental change.

22
00:01:47.380 --> 00:01:54.110
It is the function by tens of flow we are going to use right here in the previous Statoil to make the

23
00:01:54.110 --> 00:01:55.960
de-code trainings that function.

24
00:01:55.970 --> 00:02:02.980
We used the attention to Kirner and train function to make this training decoded function.

25
00:02:03.200 --> 00:02:10.010
But now since we'll have to make a test decoding function to you know decode the observations of the

26
00:02:10.200 --> 00:02:12.530
set or the validation set.

27
00:02:12.680 --> 00:02:17.900
Well this time we want to use the attention decode F and train function by tons of flow we will use

28
00:02:17.900 --> 00:02:25.600
another one called the attention decoder for an inference function y inference.

29
00:02:25.610 --> 00:02:28.780
What does that inference term mean exactly.

30
00:02:28.940 --> 00:02:36.920
Well if you look at the definition on a dictionary to infer means to deduce logically and that perfectly

31
00:02:36.920 --> 00:02:43.160
makes sense because once the Chadwell is trained it has a logic inside its brain and therefore it is

32
00:02:43.160 --> 00:02:51.050
able to deduce logically the answers to the questions that is being asked based on the logic it learned

33
00:02:51.140 --> 00:02:52.190
during the training.

34
00:02:52.400 --> 00:02:59.060
So it understood its own logic during the training and therefore after the training on new observations

35
00:02:59.060 --> 00:03:02.890
it will infer the answers to the questions that is being asked.

36
00:03:02.900 --> 00:03:09.140
So that's why the name of this new function where we belt to use will perfectly make sense here to decode

37
00:03:09.350 --> 00:03:15.380
the test validation set that is to decode some new observations that won't be used for the training

38
00:03:15.770 --> 00:03:20.540
we will use the attention that code and inference function.

39
00:03:20.780 --> 00:03:25.500
And that's the main change we'll have to make here we will have to make some little other changes.

40
00:03:25.520 --> 00:03:28.550
But that's the most fundamental one.

41
00:03:28.550 --> 00:03:29.720
All right so let's do this.

42
00:03:29.720 --> 00:03:30.690
Are you ready.

43
00:03:30.750 --> 00:03:38.820
We're going to do this efficiently I'm going to copy this function and paste it here.

44
00:03:39.020 --> 00:03:44.450
So first what we'll have to do is of course change the name of the function so you can either call it

45
00:03:44.720 --> 00:03:51.440
the code test set or to code validation set or code test validation set as you want.

46
00:03:51.620 --> 00:03:58.460
But since the ultimate goal is to predict the new observations during the test set that is in part for

47
00:03:58.820 --> 00:04:06.080
testing the sector model testing the chad but well I would like to call it the code test set but keep

48
00:04:06.080 --> 00:04:12.470
in mind that we will also use this function for the validation set when applying cross-validation during

49
00:04:12.470 --> 00:04:13.550
the training.

50
00:04:13.550 --> 00:04:22.010
All right so Dakotah set and now this function is going to take almost the same arguments here but plus

51
00:04:22.160 --> 00:04:29.320
four new ones that we'll have to input between decoder embedded input and sequence length.

52
00:04:29.330 --> 00:04:30.890
So I'm going to add them here.

53
00:04:30.980 --> 00:04:38.810
The first one we need to add is the S.O.S token ID the store of string token ID which I'm going to name

54
00:04:38.930 --> 00:04:41.530
as 0 as ID.

55
00:04:41.540 --> 00:04:46.760
Then the second one will need is going to be the end of string token ID this time so I'm going to call

56
00:04:46.760 --> 00:04:57.080
this one O S underscore ID then the maximum underscore length which corresponds to the length of the

57
00:04:57.080 --> 00:04:59.470
longest entry you can find in the batch.

58
00:04:59.920 --> 00:05:06.560
And length and then a final one that we have to add here which is the total number of words of all the

59
00:05:06.560 --> 00:05:07.310
answers.

60
00:05:07.370 --> 00:05:14.240
So basically you have to take the answers words to dictionary take the length of the dictionary because

61
00:05:14.240 --> 00:05:21.470
in this dictionary we have all the different words as the key identifiers of all the answers text and

62
00:05:21.470 --> 00:05:28.200
therefore later on this argument that I'm about at here will be this length of the answers were in the

63
00:05:28.200 --> 00:05:33.620
dictionary but right now I just need to give a name to that and I'm going to call that number like number

64
00:05:33.890 --> 00:05:34.800
words.

65
00:05:34.810 --> 00:05:35.320
All right.

66
00:05:35.330 --> 00:05:39.030
I think that makes sense that it's a total number of words.

67
00:05:39.230 --> 00:05:45.170
You also use in this function but at some point we'll also use the whole number of words of the questions.

68
00:05:45.350 --> 00:05:48.120
But we will make that difference clear.

69
00:05:48.140 --> 00:05:52.630
All right so that was the four new arguments we needed to add.

70
00:05:52.790 --> 00:05:59.390
So now the question is why do we need to add them that just because of this new function we will use

71
00:05:59.390 --> 00:06:04.550
here you know we're not going to use the attention A.F. and train function as we said we're going to

72
00:06:04.550 --> 00:06:11.960
replace it by tension that could infer function and this attention seeker if and infer function will

73
00:06:11.960 --> 00:06:14.380
need these four new arguments here.

74
00:06:14.430 --> 00:06:17.360
That's the only reason why we have to add them.

75
00:06:17.630 --> 00:06:18.120
OK.

76
00:06:18.260 --> 00:06:23.140
So as soon as Id as ID maximum length number of words perfect.

77
00:06:23.150 --> 00:06:27.480
Now let's move on to the other lines to see if we need to change anything.

78
00:06:27.710 --> 00:06:31.570
So here the attention States will of course again we will need them.

79
00:06:31.590 --> 00:06:37.850
So nothing to change here then the attention keeps the attention values the tension score function and

80
00:06:37.850 --> 00:06:44.350
the tension construct function will also be needed of course in sight side the attention they could

81
00:06:44.510 --> 00:06:51.590
find in for a function because of course we still need to consider the attention to predict our new

82
00:06:51.590 --> 00:06:52.280
observations.

83
00:06:52.280 --> 00:06:57.470
The only difference is that these new observations won't be used in the train that is they won't be

84
00:06:57.740 --> 00:07:03.980
back propagated inside the neural network but we still need attention because attention is part of the

85
00:07:03.980 --> 00:07:05.330
power of the prediction.

86
00:07:05.540 --> 00:07:11.860
That's why we need to keep the attention features here that will be inside this new function.

87
00:07:11.900 --> 00:07:13.480
And speaking of this new function.

88
00:07:13.490 --> 00:07:17.170
Well that's the fundamental change we need to include here.

89
00:07:17.300 --> 00:07:21.590
So first of all let's not forget to change this variable name.

90
00:07:21.590 --> 00:07:27.470
Now we're not going to make a training decoding function but a test decode function.

91
00:07:27.670 --> 00:07:34.620
And so now let's apply the fundamental change we're going to make this testing function.

92
00:07:34.820 --> 00:07:37.590
Thanks to the attention to coater.

93
00:07:37.700 --> 00:07:46.030
If an inference function and attention trigger an inference function is still taken from set to stick

94
00:07:46.040 --> 00:07:48.910
of course from country and from tends to flow.

95
00:07:49.270 --> 00:07:54.630
All right and now this function is going to take almost the same arguments.

96
00:07:54.650 --> 00:07:57.460
But of course new ones as you understood.

97
00:07:57.680 --> 00:08:02.030
So first of all the first argument it's going to take is not increase data.

98
00:08:02.150 --> 00:08:03.470
Be careful with that one.

99
00:08:03.560 --> 00:08:06.410
It's going to be the output function.

100
00:08:06.650 --> 00:08:13.730
The output function is needed because we will not return our final test predictions using the output

101
00:08:13.730 --> 00:08:14.870
function here.

102
00:08:14.930 --> 00:08:22.130
The attention to go to an inference function allows to input this output function and therefore instead

103
00:08:22.130 --> 00:08:26.060
of using it in the end we will use it right here.

104
00:08:26.060 --> 00:08:29.900
Therefore we have to do it here and that's why I am adding right now.

105
00:08:29.900 --> 00:08:38.530
Output function which is still our argument of our new dico tests that function.

106
00:08:38.660 --> 00:08:40.040
All right so let's move on.

107
00:08:40.040 --> 00:08:42.680
Let's not forget coming here and now.

108
00:08:42.710 --> 00:08:48.140
Let's see which arguments we need to keep here which arguments we need to remove and which arguments

109
00:08:48.140 --> 00:08:48.990
we need to add.

110
00:08:49.250 --> 00:08:52.190
So output function than encoded state 0.

111
00:08:52.190 --> 00:08:53.450
Yes we have to keep it.

112
00:08:53.550 --> 00:08:54.420
Attention keys.

113
00:08:54.470 --> 00:08:57.190
Yes we have to keep it as well intentioned values.

114
00:08:57.290 --> 00:08:57.990
Yes.

115
00:08:58.120 --> 00:08:59.250
Tensions function.

116
00:08:59.360 --> 00:09:02.840
Yes an attention construct function yes as well.

117
00:09:02.840 --> 00:09:08.480
And that's because these are the intention features that we need to keep because we need to keep the

118
00:09:08.480 --> 00:09:12.380
attention part of the neural network of the decoder.

119
00:09:12.380 --> 00:09:17.600
Because of course that's a part that plays a powerful role into predicting the final outcome.

120
00:09:17.600 --> 00:09:23.940
So we have of course to keep the attention to propagate the signal and get even more relevant predictions.

121
00:09:24.140 --> 00:09:30.020
So we have to get all the attention features and then that's where we need to add some arguments and

122
00:09:30.020 --> 00:09:32.660
I'm sure you know which ones we need to add.

123
00:09:32.660 --> 00:09:39.090
That's of course the ones we added here as new arguments have our de-code tested function.

124
00:09:39.230 --> 00:09:46.040
But that's not all we also need to include this one which by the way is not exactly the decoder embedded

125
00:09:46.040 --> 00:09:46.720
in.

126
00:09:46.880 --> 00:09:53.630
This is rather the decoder and beddings matrix that gives us these inputs.

127
00:09:53.830 --> 00:09:58.700
So they go to embedding matrix not to be confused with the decoder embedded input.

128
00:09:58.730 --> 00:10:06.570
You'll see when we make the good Arnon and then the SEC the SEC moral and actually tutorials Well we

129
00:10:06.570 --> 00:10:12.360
will make distinction between these two to decode and that input and the decoder embeddings matrix.

130
00:10:12.510 --> 00:10:19.160
And the argument here that wanted to add and this attention to to an inference function is the Deaker

131
00:10:19.180 --> 00:10:21.360
embeddings matrix by the way.

132
00:10:21.360 --> 00:10:29.190
Again I recommend to check out the resources of each lecture because in each lecture I provide the essential

133
00:10:29.370 --> 00:10:32.300
sense of documentation that you have to check out to.

134
00:10:32.340 --> 00:10:36.310
If you won't get more details on the essential function we are using.

135
00:10:36.320 --> 00:10:42.870
So for example for this lecture I will provide the documentation for this attention decoded if an inference

136
00:10:42.870 --> 00:10:49.290
function so that you can see more details on each of these arguments for example and what it returns

137
00:10:49.530 --> 00:10:51.230
and what it does exactly.

138
00:10:51.290 --> 00:10:57.030
That's because I cannot provide all the details in the lectures but you'll have all the details in the

139
00:10:57.030 --> 00:10:59.010
tens of documentation.

140
00:10:59.010 --> 00:11:04.470
All right so we are going to add these new arguments for this attention to as an inference function

141
00:11:04.920 --> 00:11:13.370
and that starts from this one decoder embeddings Matrix up to the number of words that the decoder embeddings

142
00:11:13.380 --> 00:11:19.900
matrix to store of string token ID the end of strings or the maximum length and number of words.

143
00:11:20.100 --> 00:11:21.710
So let's get all of them.

144
00:11:21.780 --> 00:11:29.210
I'm going to copy them and I'm going to add them efficiently by just pasting them right here.

145
00:11:29.280 --> 00:11:33.910
So let's add them one by one first ticker embeddings matrix.

146
00:11:34.110 --> 00:11:35.170
Perfect then.

147
00:11:35.200 --> 00:11:36.610
So I say ID.

148
00:11:36.810 --> 00:11:37.890
There we go.

149
00:11:38.130 --> 00:11:39.000
Id.

150
00:11:39.570 --> 00:11:44.010
And maximum length and total number of words non-words.

151
00:11:44.310 --> 00:11:49.670
All right so now we have our new arguments that's perfect but that's not all for this tension in the

152
00:11:49.670 --> 00:11:49.940
code.

153
00:11:49.950 --> 00:11:57.630
If an inference function we need to change the scope name here which for the attention f and Trend function

154
00:11:57.630 --> 00:11:59.670
was the attention decoder.

155
00:11:59.730 --> 00:12:02.100
Train code for this coupling.

156
00:12:02.370 --> 00:12:07.890
But now we're no longer doing some training we're doing some inference meaning that we are predicting

157
00:12:08.190 --> 00:12:15.570
the outcomes of new observations that won't be used for the training based on the logic that was learned

158
00:12:15.630 --> 00:12:22.530
and understood by the church during its training and therefore to specify that we're not in training

159
00:12:22.530 --> 00:12:28.720
mode but instead in inference mode well we need to replace this train here by then.

160
00:12:28.920 --> 00:12:35.870
So basically this name scope is the name of the node of the decoder whether it is in training mode to

161
00:12:35.880 --> 00:12:42.510
trend the jabat on the observations or the inference mode to predict the outcomes of new observations

162
00:12:42.510 --> 00:12:44.910
that won't be used for the training.

163
00:12:44.910 --> 00:12:50.260
All right so we got our new mode attention tech and attention seeker inference.

164
00:12:50.580 --> 00:12:51.320
That's great.

165
00:12:51.330 --> 00:12:53.320
And now we're almost done.

166
00:12:53.400 --> 00:12:56.730
We need to change a few things left right here.

167
00:12:56.790 --> 00:13:04.350
So we're still going to use the dynamic RNA and decode function to get the final output.

168
00:13:04.500 --> 00:13:10.040
But since this final output won't be used for the training that it won't be back propagate into the

169
00:13:10.040 --> 00:13:18.120
neural network we can directly call this output not decode the output but test predictions because these

170
00:13:18.120 --> 00:13:21.540
are our ultimate predictions desperate actions.

171
00:13:21.810 --> 00:13:26.330
And then if you want you can keep the DeCola found state and the decoded final context date.

172
00:13:26.490 --> 00:13:32.700
Even though we won't be using them for the rest of the implementation that's only if you want to have

173
00:13:32.700 --> 00:13:38.470
a clear visual look at the elements returned by the function you're using.

174
00:13:38.610 --> 00:13:41.750
It's OK if you keep them that's just for clarity.

175
00:13:41.760 --> 00:13:42.320
All right.

176
00:13:42.360 --> 00:13:44.190
Perfect so test predictions.

177
00:13:44.190 --> 00:13:47.810
That's what this code tested function will return.

178
00:13:47.830 --> 00:13:50.740
And now let's change a few things left here.

179
00:13:50.880 --> 00:13:53.280
And this dynamic ion the current function.

180
00:13:53.280 --> 00:14:00.550
So first of all let a line everything like that sequence length and scope.

181
00:14:00.580 --> 00:14:01.510
All right.

182
00:14:01.510 --> 00:14:04.090
And now let's have a look at the arguments one by one.

183
00:14:04.090 --> 00:14:10.120
All right so to get our final test predictions we're going to use the dynamic and decode function and

184
00:14:10.120 --> 00:14:11.580
we'll need to decode a cell.

185
00:14:11.590 --> 00:14:13.180
So we have to keep it here.

186
00:14:13.300 --> 00:14:14.210
The next argument.

187
00:14:14.230 --> 00:14:15.680
Training the code function.

188
00:14:15.790 --> 00:14:23.080
Well of course we won't be using the training decode function this time but instead the test decoding

189
00:14:23.110 --> 00:14:23.970
function.

190
00:14:24.160 --> 00:14:26.170
That's the function we just got here.

191
00:14:26.230 --> 00:14:30.700
Thanks to the attention Nicola Ephron inference function my sense of flow.

192
00:14:30.790 --> 00:14:38.020
So make sure to replace this by as a function and then decode or embedded input and sequence length

193
00:14:38.410 --> 00:14:44.350
are only used for the training they're not used to get the final predictions or the predictions on the

194
00:14:44.350 --> 00:14:45.640
cross-validation set.

195
00:14:45.820 --> 00:14:49.810
So we can remove them and removing that line here.

196
00:14:49.840 --> 00:14:50.590
All right.

197
00:14:50.590 --> 00:14:51.340
Perfect.

198
00:14:51.550 --> 00:14:55.090
And now let's see the last one scope equals decoding scope.

199
00:14:55.210 --> 00:15:00.310
Yes we still need that for the test predictions and the validation predictions.

200
00:15:00.310 --> 00:15:02.060
So we will keep that.

201
00:15:02.080 --> 00:15:06.570
And therefore we're done with these test predictions that we just got.

202
00:15:06.790 --> 00:15:07.710
Perfect.

203
00:15:07.780 --> 00:15:10.830
Then next line to go to output drop out.

204
00:15:10.840 --> 00:15:13.180
Well do think we have to keep it.

205
00:15:13.180 --> 00:15:18.970
The answer is of course no because the drop out is only used for the training to reduce overfishing

206
00:15:19.000 --> 00:15:20.740
and improve the accuracy.

207
00:15:20.920 --> 00:15:24.620
That's only for the training and therefore not for the test part.

208
00:15:24.700 --> 00:15:27.860
And so we can remove this as well.

209
00:15:27.860 --> 00:15:28.520
All right.

210
00:15:28.540 --> 00:15:33.020
And finally return output function decode Alpha dropout.

211
00:15:33.160 --> 00:15:36.610
Well here you'll probably guess what we have to do it's actually very simple.

212
00:15:36.700 --> 00:15:41.530
Since I told you we're not going to use the output function here in the end to get our test predictions

213
00:15:41.800 --> 00:15:48.520
because we use this output function arguments of this attention to an inference function well instead

214
00:15:48.520 --> 00:15:57.250
of using it we will just replace that by what we want to return and which is directly our test score

215
00:15:57.520 --> 00:15:58.910
predictions.

216
00:15:59.020 --> 00:16:01.300
Perfect return test predictions.

217
00:16:01.300 --> 00:16:07.890
And now we're done with this new de-code test that functions that the code the observations of the test

218
00:16:07.900 --> 00:16:13.630
set and the validation set and that returns the test prediction or the validation predictions.

219
00:16:13.630 --> 00:16:14.210
Great.

220
00:16:14.240 --> 00:16:20.670
So now we ready to select this function and execute perfect.

221
00:16:20.680 --> 00:16:24.030
We now have our de-code tested function.

222
00:16:24.070 --> 00:16:25.370
So great news.

223
00:16:25.370 --> 00:16:33.730
Now we are ready to make the decoder RNA layer or more simply the decoder Arnon by the way we called

224
00:16:33.850 --> 00:16:36.510
this function encoder and there.

225
00:16:36.520 --> 00:16:41.800
I don't want you to have any confusion between the main layers of the sector Seck moral that is the

226
00:16:42.040 --> 00:16:46.800
good are there and the Dakotah there and the sub layers inside each of these models.

227
00:16:46.990 --> 00:16:53.080
So in order to avoid any kind of confusion like that I will just replace this in good RNA and layer

228
00:16:53.380 --> 00:16:59.770
name function by simply and code Arnon so that you understand that we are just building the architecture

229
00:16:59.770 --> 00:17:05.620
of the organ and of the encoder which we sometimes call the main layer of the anchor Arnon.

230
00:17:05.620 --> 00:17:10.120
But in order not to confuse it with the layers inside the Arnon.

231
00:17:10.300 --> 00:17:12.350
Well I prefer to call it in good or Arnon.

232
00:17:12.490 --> 00:17:15.200
I think this will avoid any confusion.

233
00:17:15.520 --> 00:17:22.340
And therefore in the next story we will make the function that will build the decoder Arnon and Willey's

234
00:17:22.360 --> 00:17:28.690
of course these two functions dico trainset and Dakota set because we will not only get the training

235
00:17:28.690 --> 00:17:35.800
predictions but also the test predictions for respectively the training set and both the validation

236
00:17:35.800 --> 00:17:37.080
set and the test.

237
00:17:37.330 --> 00:17:41.080
So we'll do that in the next tutorial and until then and Jaideep been LP.
