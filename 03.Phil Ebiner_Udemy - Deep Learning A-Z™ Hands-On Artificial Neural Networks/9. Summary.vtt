WEBVTT
1
1

00:00:00.330  -->  00:00:01.820
<v Instructor>Hello and welcome back to the course on</v>
2

2

00:00:01.820  -->  00:00:03.030
deep learning.
3

3

00:00:03.030  -->  00:00:06.000
So we've learnt quite a lot in this section of the course.
4

4

00:00:06.000  -->  00:00:08.480
Let's summarize what we've talked about.
5

5

00:00:08.480  -->  00:00:09.980
Alright so here we go;
6

6

00:00:09.980  -->  00:00:11.700
We started with an input image
7

7

00:00:11.700  -->  00:00:14.713
to which we applied multiple different feature detectors
8

8

00:00:14.713  -->  00:00:18.990
what are also called filters to create these feature maps
9

9

00:00:18.990  -->  00:00:21.500
and this comprises our Convolutional Layer.
10

10

00:00:21.500  -->  00:00:24.690
Then on top of that Convolutional Layer we applied the RLU
11

11

00:00:24.690  -->  00:00:26.920
or Rectified Linear Unit
12

12

00:00:26.920  -->  00:00:30.250
to remove any linearity or increase nonlinearity
13

13

00:00:30.250  -->  00:00:31.920
in our images.
14

14

00:00:31.920  -->  00:00:36.900
Then we applied a Pooling Layer to our Convolutional Layer
15

15

00:00:36.900  -->  00:00:41.720
so from every single Feature Map we created a
16

16

00:00:41.720  -->  00:00:43.670
Pooled Feature Map and basically
17

17

00:00:43.670  -->  00:00:45.770
the Pooling Layer has lots of advantages.
18

18

00:00:45.770  -->  00:00:47.690
The main purpose of the Pooling Layer
19

19

00:00:47.690  -->  00:00:49.110
is to make sure
20

20

00:00:49.110  -->  00:00:49.943
that
21

21

00:00:49.943  -->  00:00:50.776
we have
22

22

00:00:52.090  -->  00:00:54.630
spatial invariance in our images
23

23

00:00:54.630  -->  00:00:57.980
so basically if something tilts or twists or um
24

24

00:00:57.980  -->  00:01:01.130
is a bit different to the ideal scenario
25

25

00:01:01.130  -->  00:01:03.300
then we can still pick up that feature plus
26

26

00:01:03.300  -->  00:01:07.120
Pooling significantly reduces the size of our images.
27

27

00:01:07.120  -->  00:01:10.800
And also Pooling helps with avoiding
28

28

00:01:10.800  -->  00:01:13.470
any kind of over fitting of our data
29

29

00:01:13.470  -->  00:01:15.920
or of our model to the data because it just
30

30

00:01:15.920  -->  00:01:18.320
simply gets rid of a lot of that data.
31

31

00:01:18.320  -->  00:01:20.969
But at the same time Pooling preserves
32

32

00:01:20.969  -->  00:01:23.000
the main features that we're after
33

33

00:01:23.000  -->  00:01:24.250
just because the way it's structured
34

34

00:01:24.250  -->  00:01:26.850
and the Pooling we used was max Pooling.
35

35

00:01:26.850  -->  00:01:29.840
Then we flattened all of the Pooled images into
36

36

00:01:29.840  -->  00:01:32.084
one long vector or
37

37

00:01:32.084  -->  00:01:34.860
column of all of these
38

38

00:01:34.860  -->  00:01:38.220
values and we input that into an artificial neural network
39

39

00:01:38.220  -->  00:01:40.810
and that was Step 3: Flattening and Step 4
40

40

00:01:40.810  -->  00:01:43.940
is a fully connected artificial neural network
41

41

00:01:43.940  -->  00:01:46.820
where all of these features are processed
42

42

00:01:46.820  -->  00:01:50.120
through a network and then we have this final layer
43

43

00:01:50.120  -->  00:01:51.920
final fully connected layer
44

44

00:01:51.920  -->  00:01:55.300
which performs the voting towards the classes
45

45

00:01:55.300  -->  00:01:56.420
that we're altering.
46

46

00:01:56.420  -->  00:02:00.390
Then all of this is trudged though a forward propagation
47

47

00:02:00.390  -->  00:02:01.740
and back propagation.
48

48

00:02:01.740  -->  00:02:04.780
The process and lots and lots of iterations an epox
49

49

00:02:04.780  -->  00:02:09.258
and in the end we have a very well defined neural network.
50

50

00:02:09.258  -->  00:02:12.090
And another important thing is not only
51

51

00:02:12.090  -->  00:02:14.360
the weights are trained in the artificial neural network but
52

52

00:02:14.360  -->  00:02:18.060
also the feature detectors are trained and
53

53

00:02:18.060  -->  00:02:21.860
adjusted in that same gradient decent process
54

54

00:02:21.860  -->  00:02:23.870
and that allows us to come up with the best Feature Maps.
55

55

00:02:23.870  -->  00:02:26.460
And in the end we get a fully trained
56

56

00:02:26.460  -->  00:02:27.600
Convolutional Neural Network
57

57

00:02:27.600  -->  00:02:31.640
which can recognize images and classify them.
58

58

00:02:31.640  -->  00:02:35.610
So there we go that's how Convolutional Neural Networks work
59

59

00:02:35.610  -->  00:02:38.900
and now you should be totally comfortable with this concept
60

60

00:02:38.900  -->  00:02:42.210
and ready to proceed to the practical applications.
61

61

00:02:42.210  -->  00:02:44.510
If you'd like to do some additional reading
62

62

00:02:44.510  -->  00:02:49.510
then there's a great blog by Adit Deshpande from 2016.
63

63

00:02:50.917  -->  00:02:53.290 line:15% 
You can see the link there at the bottom.
64

64

00:02:53.290  -->  00:02:55.870 line:15% 
So the blog is called The Nine Deep Learning Papers
65

65

00:02:55.870  -->  00:02:57.547 line:15% 
You Need to Know About (Understanding CNNs Part 3).
66

66

00:02:59.200  -->  00:03:01.620
And this blog actually gives you a short overview
67

67

00:03:01.620  -->  00:03:03.530
of nine different CNNs
68

68

00:03:03.530  -->  00:03:06.470
that have been created by people like Yann LeCun and
69

69

00:03:06.470  -->  00:03:10.520
others which you can then go ahead and study further.
70

70

00:03:10.520  -->  00:03:14.600
So there will be a lot of new things that will be
71

71

00:03:14.600  -->  00:03:17.530
totally new to you and that you will have to
72

72

00:03:17.530  -->  00:03:18.930
get your head around but
73

73

00:03:18.930  -->  00:03:20.520
just keep this blog in mind
74

74

00:03:20.520  -->  00:03:22.770
or these nine papers in mind and
75

75

00:03:22.770  -->  00:03:25.070
even if you're not ready to go through them right now
76

76

00:03:25.070  -->  00:03:26.750
maybe after the practical tutorials
77

77

00:03:26.750  -->  00:03:29.980
maybe after you do some additional training
78

78

00:03:29.980  -->  00:03:31.430
in the space of Deep Learning
79

79

00:03:31.430  -->  00:03:34.880
slowly you can then reference these works and
80

80

00:03:34.880  -->  00:03:37.880
ideally I think you will get a lot of value through
81

81

00:03:37.880  -->  00:03:39.850
looking through other peoples neural networks
82

82

00:03:39.850  -->  00:03:43.130
and how they structured their Convolutional Nets
83

83

00:03:43.130  -->  00:03:44.670
and that'll help you understand
84

84

00:03:44.670  -->  00:03:45.940
what are the best practices
85

85

00:03:45.940  -->  00:03:48.330
and why people did certain things in a certain way.
86

86

00:03:48.330  -->  00:03:50.930
And that will help you with your architecture
87

87

00:03:50.930  -->  00:03:51.800
of neural networks.
88

88

00:03:51.800  -->  00:03:52.920
Because neural networks
89

89

00:03:52.920  -->  00:03:57.900
and Convolutional Neural Networks are not an exception
90

90

00:03:57.900  -->  00:04:01.520
they are like an architecture challenge.
91

91

00:04:01.520  -->  00:04:04.170
You have to come up with a idea
92

92

00:04:04.170  -->  00:04:06.670
and then a structure and then adjust it and tweak it
93

93

00:04:06.670  -->  00:04:08.930
to get the best possible design and
94

94

00:04:08.930  -->  00:04:11.690
the best possible and optimal performance.
95

95

00:04:11.690  -->  00:04:12.523
So there we go
96

96

00:04:12.523  -->  00:04:13.356
that's us for today.
97

97

00:04:13.356  -->  00:04:14.730
I hope you enjoyed today's tutorial
98

98

00:04:14.730  -->  00:04:16.040
and this whole section
99

99

00:04:16.040  -->  00:04:17.600
and I look forward to seeing you next time.
100

100

00:04:17.600  -->  00:04:19.513
Until then enjoy Deep Learning.
