WEBVTT
1
1

00:00:00.000  -->  00:00:02.800
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.800  -->  00:00:05.450
So now, it's time to create the architecture
3

3

00:00:05.450  -->  00:00:06.420
of the neural network.
4

4

00:00:06.420  -->  00:00:09.250
The architecture of our auto-encoders.
5

5

00:00:09.250  -->  00:00:11.760
So, how are we going to do that?
6

6

00:00:11.760  -->  00:00:13.910
Well, we're going to make a class.
7

7

00:00:13.910  -->  00:00:16.080
So first of all, what is a class?
8

8

00:00:16.080  -->  00:00:18.550
A class is a model of something we want to build.
9

9

00:00:18.550  -->  00:00:21.150
So right now we want to build a neural encoder
10

10

00:00:21.150  -->  00:00:23.067
so the class that we're going to make
11

11

00:00:23.067  -->  00:00:25.940
will be the model that will contain the instructions
12

12

00:00:25.940  -->  00:00:28.380
on how to build the auto-encoder.
13

13

00:00:28.380  -->  00:00:30.226
So, it's like an ensemble of instructions
14

14

00:00:30.226  -->  00:00:32.960
and all these instructions will allow to create
15

15

00:00:32.960  -->  00:00:35.070
what we call an object, because then,
16

16

00:00:35.070  -->  00:00:36.683
once we're done making the class,
17

17

00:00:36.683  -->  00:00:39.540
what we can do is create an object of the class,
18

18

00:00:39.540  -->  00:00:42.430
like the many objects we created in this course.
19

19

00:00:42.430  -->  00:00:44.870
And of course, for the class we're about to make,
20

20

00:00:44.870  -->  00:00:47.900
well, when we create an object of this class
21

21

00:00:47.900  -->  00:00:51.650
this object will be the auto-encoder encoder itself.
22

22

00:00:51.650  -->  00:00:54.360
So why do we need a class to create an auto-encoder?
23

23

00:00:54.360  -->  00:00:56.260
Well, that's because a class can contain
24

24

00:00:56.260  -->  00:00:58.550
some variables and also some functions
25

25

00:00:58.550  -->  00:01:00.170
that we call methods.
26

26

00:01:00.170  -->  00:01:02.540
And with all these variables and these methods,
27

27

00:01:02.540  -->  00:01:05.710
these functions, well, we can make an auto-encoder.
28

28

00:01:05.710  -->  00:01:07.490
Because, indeed, to make an auto-encoder,
29

29

00:01:07.490  -->  00:01:09.400
we need to define multiple things.
30

30

00:01:09.400  -->  00:01:11.860
We need to define the layers: how many layers
31

31

00:01:11.860  -->  00:01:14.450
we want to have, how many nodes in the layers,
32

32

00:01:14.450  -->  00:01:16.385
we also need an activation function,
33

33

00:01:16.385  -->  00:01:19.710
a criterion, and an optimizer function.
34

34

00:01:19.710  -->  00:01:21.770
So basically, to make an auto-encoder,
35

35

00:01:21.770  -->  00:01:23.880
we not only need some variables,
36

36

00:01:23.880  -->  00:01:26.370
that's will get, for example, the info of the layers,
37

37

00:01:26.370  -->  00:01:30.060
and some functions, for the activation and the optimizer.
38

38

00:01:30.060  -->  00:01:32.370
And to get all this in one same recipe,
39

39

00:01:32.370  -->  00:01:34.230
well, we can only use a class,
40

40

00:01:34.230  -->  00:01:36.560
or, at least, that's the simple solution.
41

41

00:01:36.560  -->  00:01:38.430
We could also make some modules,
42

42

00:01:38.430  -->  00:01:40.430
a module contains several classes,
43

43

00:01:40.430  -->  00:01:43.550
or even some libraries, which contain several modules.
44

44

00:01:43.550  -->  00:01:46.527
But the simplest solution is to make a class.
45

45

00:01:46.527  -->  00:01:49.060
And a simple function wouldn't be enough.
46

46

00:01:49.060  -->  00:01:50.460
So that's the first reason.
47

47

00:01:50.460  -->  00:01:52.160
The class can gather all these features,
48

48

00:01:52.160  -->  00:01:55.140
variables and functions to make the auto-encoder.
49

49

00:01:55.140  -->  00:01:56.740
But then there is a second reason,
50

50

00:01:56.740  -->  00:01:58.030
and this is very important, because
51

51

00:01:58.030  -->  00:01:59.358
this is related to PyTorch.
52

52

00:01:59.358  -->  00:02:01.436
We're gonna use inheritance.
53

53

00:02:01.436  -->  00:02:02.930
What is inheritance?
54

54

00:02:02.930  -->  00:02:05.070
That is that we're going to create a class
55

55

00:02:05.070  -->  00:02:08.630
that's we're gonna call S-A-E, for stacked auto-encoders.
56

56

00:02:08.630  -->  00:02:10.727
That is actually going to be the child class
57

57

00:02:10.727  -->  00:02:14.080
of an existing parent class in PyTorch.
58

58

00:02:14.080  -->  00:02:15.520
And this existing parent class
59

59

00:02:15.520  -->  00:02:18.170
is called Module, with capital M.
60

60

00:02:18.170  -->  00:02:22.030
And this taken from the nn module that we imported here.
61

61

00:02:22.030  -->  00:02:24.430
torch.nn as nn.
62

62

00:02:24.430  -->  00:02:26.740
So, from this nn module, we're gonna take
63

63

00:02:26.740  -->  00:02:29.510
the module class, which is the parent class.
64

64

00:02:29.510  -->  00:02:32.310
And from this class, we will create a child class
65

65

00:02:32.310  -->  00:02:34.830
through the process of inheritance.
66

66

00:02:34.830  -->  00:02:37.070
And now the question is, why are we creating
67

67

00:02:37.070  -->  00:02:39.790
this child class from the parent class Module?
68

68

00:02:39.790  -->  00:02:42.510
Well, we are doing this so that we can use all the variables
69

69

00:02:42.510  -->  00:02:45.490
and functions from the parent class Module,
70

70

00:02:45.490  -->  00:02:47.820
and that's what inheritance is all about.
71

71

00:02:47.820  -->  00:02:51.150
The use of inheritance is for us to be able to use
72

72

00:02:51.150  -->  00:02:54.170
the variables and functions from a parent class.
73

73

00:02:54.170  -->  00:02:56.140
And we need to use the variables and functions
74

74

00:02:56.140  -->  00:02:58.420
from the parent class Module, because
75

75

00:02:58.420  -->  00:03:00.170
this parent class Module contains
76

76

00:03:00.170  -->  00:03:02.415
all the tools to make an auto-encoder.
77

77

00:03:02.415  -->  00:03:04.694
Indeed, it contains an optimizer function,
78

78

00:03:04.694  -->  00:03:08.410
it contains a criterion, it contains tools to make
79

79

00:03:08.410  -->  00:03:10.250
full connections between the layers.
80

80

00:03:10.250  -->  00:03:12.589
So basically, it contains everything we need
81

81

00:03:12.589  -->  00:03:14.770
to make an auto-encoder.
82

82

00:03:14.770  -->  00:03:15.970
Alright, so let's do this.
83

83

00:03:15.970  -->  00:03:19.320
Let's start with this process of inheritance.
84

84

00:03:19.320  -->  00:03:21.020
So what's we're gonna do now is
85

85

00:03:21.020  -->  00:03:24.101
first create this child class that's were gonna call
86

86

00:03:24.101  -->  00:03:26.298
SAE, capital SAE, because, you know,
87

87

00:03:26.298  -->  00:03:28.730
we use capitals for classes.
88

88

00:03:28.730  -->  00:03:30.530
So, to define a class in Python,
89

89

00:03:30.530  -->  00:03:33.089
we start with class, and then now we put
90

90

00:03:33.089  -->  00:03:36.550
the name of the class, which we call SAE,
91

91

00:03:36.550  -->  00:03:38.380
for stacked auto-encoders.
92

92

00:03:38.380  -->  00:03:39.930
By the way, our auto-encoder is a
93

93

00:03:39.930  -->  00:03:42.030
stacked auto-encoder, because we will have
94

94

00:03:42.030  -->  00:03:43.815
several hidden layers, so we will have several
95

95

00:03:43.815  -->  00:03:46.346
encodings of the input vector features.
96

96

00:03:46.346  -->  00:03:50.400
And so stacked auto-encoders, and then parentheses.
97

97

00:03:50.400  -->  00:03:53.090
And now that's where the process of inheritance
98

98

00:03:53.090  -->  00:03:56.060
comes into play, because, in these parentheses,
99

99

00:03:56.060  -->  00:03:58.310
we're going to input the parent class
100

100

00:03:58.310  -->  00:04:02.045
which is Module, with capital M, so Module,
101

101

00:04:02.045  -->  00:04:07.045
and this Module class is actually taken from the nn module.
102

102

00:04:07.500  -->  00:04:11.413
Well, nn is a module, and the name of the class from this
103

103

00:04:11.413  -->  00:04:14.488
nn module is called module also.
104

104

00:04:14.488  -->  00:04:18.060
Alright, so, now the inheritance is done.
105

105

00:04:18.060  -->  00:04:20.760
We will then optimize it with the super function
106

106

00:04:20.760  -->  00:04:22.570
that we're going to use very quickly, but still,
107

107

00:04:22.570  -->  00:04:25.050
the main action about inheritance is done.
108

108

00:04:25.050  -->  00:04:27.410
So now, what we have to do is add a colon,
109

109

00:04:27.410  -->  00:04:30.210
and then we press Enter to go inside the class
110

110

00:04:30.210  -->  00:04:33.950
and give all the instructions to build our auto-encoders.
111

111

00:04:33.950  -->  00:04:35.880
Alright, so the first thing we have to do
112

112

00:04:35.880  -->  00:04:38.390
when we define a class, and that's compulsory,
113

113

00:04:38.390  -->  00:04:40.120
you will always see this, even if it's
114

114

00:04:40.120  -->  00:04:43.250
a default function, but still, we need to explicit it,
115

115

00:04:43.250  -->  00:04:44.567
because we will add some parameters,
116

116

00:04:44.567  -->  00:04:48.380
is to define the innit function that is used
117

117

00:04:48.380  -->  00:04:50.619
to initialize the object of the class.
118

118

00:04:50.619  -->  00:04:53.160
That is that, when we create an object
119

119

00:04:53.160  -->  00:04:56.170
of this SAE class, well, this innit function
120

120

00:04:56.170  -->  00:04:58.793
that we're about to define, will initialize the object
121

121

00:04:58.793  -->  00:05:02.482
with all the parameters defined by this innit function.
122

122

00:05:02.482  -->  00:05:05.680
And so, to define this innit function, we do it this way:
123

123

00:05:05.680  -->  00:05:08.335
we start with def, because we're defining a function,
124

124

00:05:08.335  -->  00:05:10.890
and then we add a double-underscore,
125

125

00:05:10.890  -->  00:05:14.450
and then, innit, and then, another double-underscore again.
126

126

00:05:14.450  -->  00:05:16.740
Then, parentheses, and, inside the parentheses,
127

127

00:05:16.740  -->  00:05:20.149
we input self, that refers to the object of the class.
128

128

00:05:20.149  -->  00:05:23.380
You can picture self as the auto-encoders
129

129

00:05:23.380  -->  00:05:25.795
because this is the object of the SAE class.
130

130

00:05:25.795  -->  00:05:28.609
And then, we add a comma, and then just nothing
131

131

00:05:28.609  -->  00:05:31.270
because this will just consider the variables
132

132

00:05:31.270  -->  00:05:33.915
of the Module class, because we are doing inheritance.
133

133

00:05:33.915  -->  00:05:36.760
And here then, another colon, that's
134

134

00:05:36.760  -->  00:05:39.180
the syntax in Python to define a function.
135

135

00:05:39.180  -->  00:05:42.040
And then, enter to go inside the function
136

136

00:05:42.040  -->  00:05:45.060
and give, this time the instructions of the function.
137

137

00:05:45.060  -->  00:05:47.850
So, as I just mentioned, the process of inheritance
138

138

00:05:47.850  -->  00:05:50.083
is mainly done, but we can optimize it
139

139

00:05:50.083  -->  00:05:52.600
by using the super function.
140

140

00:05:52.600  -->  00:05:54.975
So, the function is actually called super.
141

141

00:05:54.975  -->  00:05:58.301
And the use of this super function that we're about to use
142

142

00:05:58.301  -->  00:06:02.020
is to get the inherited methods from the Module class.
143

143

00:06:02.020  -->  00:06:04.482
So we use this super function to be able to use
144

144

00:06:04.482  -->  00:06:07.470
the methods and classes from the nn module,
145

145

00:06:07.470  -->  00:06:09.163
and the one we need and we're gonna use is
146

146

00:06:09.163  -->  00:06:11.300
the Linear class, which will make
147

147

00:06:11.300  -->  00:06:13.360
the different full connections between the layers.
148

148

00:06:13.360  -->  00:06:16.070
You'll see, we're about to use it in a minute.
149

149

00:06:16.070  -->  00:06:19.263
So we add here super, then parentheses,
150

150

00:06:19.263  -->  00:06:21.504
and inside the parentheses we first input
151

151

00:06:21.504  -->  00:06:26.064
our SAE class, then comma, and then we input
152

152

00:06:26.064  -->  00:06:29.710
the object of the SAE class, which is self.
153

153

00:06:29.710  -->  00:06:32.800
Because remember, self refers to the object.
154

154

00:06:32.800  -->  00:06:35.850
Okay, and then, we add a dot, and then we take
155

155

00:06:35.850  -->  00:06:40.088
our innit function, like this, so I'm copying this,
156

156

00:06:40.088  -->  00:06:44.360
and pasting that here, and I'm adding the parentheses.
157

157

00:06:44.360  -->  00:06:46.500
Here we go, that's will make sure we get
158

158

00:06:46.500  -->  00:06:48.335
all the inherited classes and methods
159

159

00:06:48.335  -->  00:06:50.760
of the parent class and then module.
160

160

00:06:50.760  -->  00:06:53.430
Okay, and now, that's when we start creating
161

161

00:06:53.430  -->  00:06:55.230
the architecture of the neural network,
162

162

00:06:55.230  -->  00:06:58.155
I mean, explicitly, because, what we're about to do now
163

163

00:06:58.155  -->  00:07:01.848
is define this architecture by choosing the number of layers
164

164

00:07:01.848  -->  00:07:05.120
and the hidden neurons in each of these hidden layers.
165

165

00:07:05.120  -->  00:07:06.510
So let's do this.
166

166

00:07:06.510  -->  00:07:09.800
Let's start with the first part of the neural network
167

167

00:07:09.800  -->  00:07:11.421
that is the full connection between
168

168

00:07:11.421  -->  00:07:14.900
the input vector features that, as a reminder, is
169

169

00:07:14.900  -->  00:07:18.333
the ratings of all the movies for one specific user.
170

170

00:07:18.333  -->  00:07:21.430
And the first hidden layer, which, remember,
171

171

00:07:21.430  -->  00:07:24.460
is a shorter vector than the input vector,
172

172

00:07:24.460  -->  00:07:26.040
that's the technique of auto-encoders.
173

173

00:07:26.040  -->  00:07:29.790
We're encoding the input vector into a shorter vector.
174

174

00:07:29.790  -->  00:07:32.270
That's will take place in the first hidden layer.
175

175

00:07:32.270  -->  00:07:36.420
Okay, so now we're going to create an object of the class
176

176

00:07:36.420  -->  00:07:38.900
that is inherited from the nn module.
177

177

00:07:38.900  -->  00:07:40.530
And this object will represent
178

178

00:07:40.530  -->  00:07:43.715
the full connection between this first input vector features
179

179

00:07:43.715  -->  00:07:46.360
and the first encoded vector.
180

180

00:07:46.360  -->  00:07:48.710
We're gonna call this first full connection
181

181

00:07:48.710  -->  00:07:52.520
fc1, and, actually, we need to specify
182

182

00:07:52.520  -->  00:07:53.695
that this first full connection
183

183

00:07:53.695  -->  00:07:55.687
is associated to our object,
184

184

00:07:55.687  -->  00:07:57.343
and, to specify this, we need to use
185

185

00:07:57.343  -->  00:08:01.576
self.fc1, so that we understand
186

186

00:08:01.576  -->  00:08:04.610
that fc1 is actually the first full connection
187

187

00:08:04.610  -->  00:08:07.550
related to our auto-encoders object,
188

188

00:08:07.550  -->  00:08:09.510
which is represented by self.
189

189

00:08:09.510  -->  00:08:13.065
Alright, so self.fc1, and then equal
190

190

00:08:13.065  -->  00:08:15.720
and now, that's when we use the inherited class
191

191

00:08:15.720  -->  00:08:20.210
from the nn module, which is Linear, with parentheses.
192

192

00:08:20.210  -->  00:08:24.570
And this class, we're taking it from the nn module.
193

193

00:08:24.570  -->  00:08:29.190
Alright, so nn.Linear(), and now, we have to input
194

194

00:08:29.190  -->  00:08:30.740
something in Linear, of course.
195

195

00:08:30.740  -->  00:08:33.210
The first input is the number of features
196

196

00:08:33.210  -->  00:08:35.910
in the input vector, and, as you understood,
197

197

00:08:35.910  -->  00:08:38.282
since the features are actually movies,
198

198

00:08:38.282  -->  00:08:40.060
because one observation contains
199

199

00:08:40.060  -->  00:08:41.344
all the ratings of all the movies,
200

200

00:08:41.344  -->  00:08:45.937
well, this number of input features is of course nb_movies.
201

201

00:08:48.420  -->  00:08:50.426
Alright, the number of movies.
202

202

00:08:50.426  -->  00:08:52.670
And then, as you might have guessed,
203

203

00:08:52.670  -->  00:08:54.550
the second input is going to be
204

204

00:08:54.550  -->  00:08:56.720
the number of nodes, the number of neurons
205

205

00:08:56.720  -->  00:08:58.870
in the first hidden layer.
206

206

00:08:58.870  -->  00:09:02.159
That is the number of elements in the first encoded vector.
207

207

00:09:02.159  -->  00:09:04.490
So here, I'm not gonna hide that I tried
208

208

00:09:04.490  -->  00:09:06.400
several values for this vector.
209

209

00:09:06.400  -->  00:09:08.080
And so the value that I'm about to give you
210

210

00:09:08.080  -->  00:09:09.730
is based on experiments.
211

211

00:09:09.730  -->  00:09:11.500
But this is still not tuned.
212

212

00:09:11.500  -->  00:09:12.950
And of course then, the challenge for you
213

213

00:09:12.950  -->  00:09:14.339
will be to tune it and to find a better value
214

214

00:09:14.339  -->  00:09:17.420
to get eventually to a better score.
215

215

00:09:17.420  -->  00:09:19.730
But this value I found led to a pretty good score.
216

216

00:09:19.730  -->  00:09:21.420
So I eventually accepted it.
217

217

00:09:21.420  -->  00:09:22.760
This value is 20.
218

218

00:09:22.760  -->  00:09:25.700
So, 20 nodes in the first hidden layer,
219

219

00:09:25.700  -->  00:09:27.836
which means that our first encoded vector
220

220

00:09:27.836  -->  00:09:30.230
is a vector of 20 elements.
221

221

00:09:30.230  -->  00:09:33.090
And now, if we want to make the parallel with the problem,
222

222

00:09:33.090  -->  00:09:36.080
well, what would represent this first hidden layer
223

223

00:09:36.080  -->  00:09:37.458
composed of 20 neurons?
224

224

00:09:37.458  -->  00:09:41.438
Well, remember that the neurons in the auto-encoder
225

225

00:09:41.438  -->  00:09:44.350
represent actually some features that,
226

226

00:09:44.350  -->  00:09:46.050
through unsupervised learning,
227

227

00:09:46.050  -->  00:09:48.490
the auto-encoder detects, and so,
228

228

00:09:48.490  -->  00:09:51.356
these 20 features that are in the first hidden layer
229

229

00:09:51.356  -->  00:09:54.440
can represent some features of movies
230

230

00:09:54.440  -->  00:09:56.206
that are liked similar people.
231

231

00:09:56.206  -->  00:09:58.740
So, for example, one of these 20 nodes
232

232

00:09:58.740  -->  00:10:01.170
could be a specific genre of a movie.
233

233

00:10:01.170  -->  00:10:04.230
One of the detected feature could be the horror movie genre.
234

234

00:10:04.230  -->  00:10:06.936
And, in that case, when a new user comes in the system
235

235

00:10:06.936  -->  00:10:10.130
then, if this new user gave some good ratings
236

236

00:10:10.130  -->  00:10:12.120
to all the horror movies of the database,
237

237

00:10:12.120  -->  00:10:15.366
then this will activate this horror genre neuron
238

238

00:10:15.366  -->  00:10:17.930
and therefore a big weight will be attributed
239

239

00:10:17.930  -->  00:10:19.900
to this neuron in the final prediction
240

240

00:10:19.900  -->  00:10:22.283
to predict the rating of a horror movie.
241

241

00:10:22.283  -->  00:10:25.350
So this is a very simple example and of course,
242

242

00:10:25.350  -->  00:10:28.250
the features can be some more complex features
243

243

00:10:28.250  -->  00:10:29.943
that even would be difficult to define,
244

244

00:10:29.943  -->  00:10:31.890
but that's how it works.
245

245

00:10:31.890  -->  00:10:34.380
With this number 20 here, we're trying
246

246

00:10:34.380  -->  00:10:36.017
to detect 20 features.
247

247

00:10:36.017  -->  00:10:38.880
Alright, so that's for the first hidden layer.
248

248

00:10:38.880  -->  00:10:41.303
But now, since we're making some stacked auto-encoders,
249

249

00:10:41.303  -->  00:10:43.089
we're gonna make another layer.
250

250

00:10:43.089  -->  00:10:45.074
And it's actually going to be very simple.
251

251

00:10:45.074  -->  00:10:47.808
We have to make a second full connection.
252

252

00:10:47.808  -->  00:10:50.220
And so, same, we take our object,
253

253

00:10:50.220  -->  00:10:51.733
because that's the second full connection
254

254

00:10:51.733  -->  00:10:54.934
of our auto-encoders, so then, dot, and then,
255

255

00:10:54.934  -->  00:10:57.370
let's define a new variable that will represent
256

256

00:10:57.370  -->  00:10:59.320
this second full connection, and of course,
257

257

00:10:59.320  -->  00:11:01.464
we will call it fc2.
258

258

00:11:01.464  -->  00:11:04.113
So then, we take again our nn module,
259

259

00:11:04.113  -->  00:11:07.120
then dot, and then again we're gonna use
260

260

00:11:07.120  -->  00:11:09.994
the Linear class that will make this full connection.
261

261

00:11:09.994  -->  00:11:11.293
So Linear.
262

262

00:11:12.210  -->  00:11:14.800
And now, we have to make the full connection between
263

263

00:11:14.800  -->  00:11:18.120
the first hidden layer, composed of the 20 neurons
264

264

00:11:18.120  -->  00:11:21.290
and a second hidden layer, so again, we need to choose
265

265

00:11:21.290  -->  00:11:23.388
the number of neurons in the second hidden layer.
266

266

00:11:23.388  -->  00:11:25.730
And so, after all my experiments,
267

267

00:11:25.730  -->  00:11:28.220
I eventually chose 10 neurons.
268

268

00:11:28.220  -->  00:11:30.150
So, what we add here is first,
269

269

00:11:30.150  -->  00:11:32.240
the number of neurons between the first
270

270

00:11:32.240  -->  00:11:33.660
hidden layer of the connection
271

271

00:11:33.660  -->  00:11:37.069
that contains 20 neurons, so here, we input 20.
272

272

00:11:37.069  -->  00:11:39.500
You see, it's going to be like a domino.
273

273

00:11:39.500  -->  00:11:42.491
So 20, and then comma, and now 10,
274

274

00:11:42.491  -->  00:11:44.974
because we want 10 neurons in the second hidden layer.
275

275

00:11:44.974  -->  00:11:48.570
And this will detect even more features,
276

276

00:11:48.570  -->  00:11:51.400
but based on the previous features that were detected.
277

277

00:11:51.400  -->  00:11:54.513
So this is getting even more complex, but we get the idea.
278

278

00:11:55.760  -->  00:11:57.760
And, since we're doing deep learning,
279

279

00:11:57.760  -->  00:11:59.670
let's add a third layer.
280

280

00:11:59.670  -->  00:12:01.520
So I'm going to copy this line.
281

281

00:12:01.520  -->  00:12:02.920
You start to get the idea.
282

282

00:12:02.920  -->  00:12:05.947
Paste it, and we're gonna call
283

283

00:12:05.947  -->  00:12:07.768
the third full connection between
284

284

00:12:07.768  -->  00:12:12.247
the second hidden layer and the third hidden layer fc3.
285

285

00:12:12.247  -->  00:12:16.070
And now, first, we need to input the number of neurons
286

286

00:12:16.070  -->  00:12:17.770
in the first layer of the connection,
287

287

00:12:17.770  -->  00:12:19.340
which is the second layer here,
288

288

00:12:19.340  -->  00:12:21.350
so that's 10 neurons, and so here
289

289

00:12:21.350  -->  00:12:24.290
we input 10, so exactly like a domino.
290

290

00:12:24.290  -->  00:12:27.200
And then here, we need to input the number of neurons
291

291

00:12:27.200  -->  00:12:28.925
in this third hidden layer, but now,
292

292

00:12:28.925  -->  00:12:30.820
the important thing to understand is
293

293

00:12:30.820  -->  00:12:33.152
that we're not doing some more encoding,
294

294

00:12:33.152  -->  00:12:34.888
we're starting to decode.
295

295

00:12:34.888  -->  00:12:36.707
You know we're starting to reconstruct
296

296

00:12:36.707  -->  00:12:38.760
the original input vector.
297

297

00:12:38.760  -->  00:12:40.600
So, let's make things symmetrical.
298

298

00:12:40.600  -->  00:12:44.270
We're going to input 20 here to have 20 neurons
299

299

00:12:44.270  -->  00:12:46.050
in this third hidden layer.
300

300

00:12:46.050  -->  00:12:48.200
So that's the first part of the decoding.
301

301

00:12:48.200  -->  00:12:50.066
And now, the last part of the decoding
302

302

00:12:50.066  -->  00:12:52.178
is the last full connection we have to make
303

303

00:12:52.178  -->  00:12:54.730
and this is, so I'm pressing enter,
304

304

00:12:54.730  -->  00:12:57.419
I'm pasting the line again, I'm defining
305

305

00:12:57.419  -->  00:13:00.393
the fourth full connection as fc4.
306

306

00:13:01.400  -->  00:13:03.710
And now that's the same, still the domino.
307

307

00:13:03.710  -->  00:13:05.608
Here we actually input 20, because we had
308

308

00:13:05.608  -->  00:13:07.810
20 nodes in the third layer.
309

309

00:13:07.810  -->  00:13:09.780
So 20 here, and then, eventually,
310

310

00:13:09.780  -->  00:13:11.490
the second input here is, of course,
311

311

00:13:11.490  -->  00:13:13.502
the number of neurons in the output layer,
312

312

00:13:13.502  -->  00:13:16.027
and remember, in auto-encoders, we are re-constructing
313

313

00:13:16.027  -->  00:13:19.007
the input vector, so, the output vector
314

314

00:13:19.007  -->  00:13:21.816
should have the same dimension as the input vector.
315

315

00:13:21.816  -->  00:13:24.610
Therefore, the number of neurons in the output layer
316

316

00:13:24.610  -->  00:13:26.890
is also going to be nb_movies.
317

317

00:13:26.890  -->  00:13:29.300
So here, we're just replacing 10
318

318

00:13:29.300  -->  00:13:32.960
by nb, underscore, movies.
319

319

00:13:32.960  -->  00:13:34.975
Alright, and that's the architecture
320

320

00:13:34.975  -->  00:13:37.042
of our stacked auto-encoder.
321

321

00:13:37.042  -->  00:13:39.870
You're totally welcome to try to change it
322

322

00:13:39.870  -->  00:13:41.758
and actually, I strongly encourage you to do that,
323

323

00:13:41.758  -->  00:13:43.790
Because you will probably get a better score
324

324

00:13:43.790  -->  00:13:47.060
in the end, while, so far, that's the best I could obtain.
325

325

00:13:47.060  -->  00:13:49.259
But we're gonna take part in the competition together
326

326

00:13:49.259  -->  00:13:51.491
and we will try to get the best score.
327

327

00:13:51.491  -->  00:13:54.810
Alright, so that's it for the architecture.
328

328

00:13:54.810  -->  00:13:55.920
Now it is defined.
329

329

00:13:55.920  -->  00:13:59.565
And now what we have to do is specify an activation function
330

330

00:13:59.565  -->  00:14:02.210
that will, you know, activate the neurons
331

331

00:14:02.210  -->  00:14:04.380
when the observation goes into the network.
332

332

00:14:04.380  -->  00:14:06.426
So, it's exactly like the example I've just given to you.
333

333

00:14:06.426  -->  00:14:08.760
For example, if someone gives some good ratings
334

334

00:14:08.760  -->  00:14:11.188
for horror movies, well, this will activate
335

335

00:14:11.188  -->  00:14:13.674
the specific feature for the horror movie genre.
336

336

00:14:13.674  -->  00:14:16.246
And this activation will be done with a certain function,
337

337

00:14:16.246  -->  00:14:19.070
which is the activation we're about to define right now.
338

338

00:14:19.070  -->  00:14:20.334
So, pressing Enter.
339

339

00:14:20.334  -->  00:14:22.857
And now, let's define this activation function,
340

340

00:14:22.857  -->  00:14:26.550
which is another instruction, and all these instructions
341

341

00:14:26.550  -->  00:14:29.232
given by the class to build our auto-encoders.
342

342

00:14:29.232  -->  00:14:33.541
So again, we take our object, represented by self.
343

343

00:14:33.541  -->  00:14:36.000
And then dot, and now we're gonna give a name
344

344

00:14:36.000  -->  00:14:38.260
to the variable for the activation function.
345

345

00:14:38.260  -->  00:14:40.390
So we're just defining a variable here.
346

346

00:14:40.390  -->  00:14:42.315
And we're gonna call it, (mumbles) activation.
347

347

00:14:42.315  -->  00:14:47.315
And then equal, and so again, I tried some experimenting
348

348

00:14:47.530  -->  00:14:49.809
and I tried the rectifier activation function
349

349

00:14:49.809  -->  00:14:52.624
and also, the sigmoid activation function,
350

350

00:14:52.624  -->  00:14:54.488
and it turns out that I got better results
351

351

00:14:54.488  -->  00:14:57.050
with a sigmoid activation function
352

352

00:14:57.050  -->  00:14:59.410
between the four full connections.
353

353

00:14:59.410  -->  00:15:01.610
Of course, then there will be a section to improve
354

354

00:15:01.610  -->  00:15:03.679
our model and tune it, but so far,
355

355

00:15:03.679  -->  00:15:06.415
let's just use this sigmoid activation function
356

356

00:15:06.415  -->  00:15:09.807
to activate the neurons in these four full connections.
357

357

00:15:09.807  -->  00:15:12.100
And then, you know, we'll try some combinations
358

358

00:15:12.100  -->  00:15:13.578
of the rectifier activation function
359

359

00:15:13.578  -->  00:15:15.857
and the sigmoid activation function.
360

360

00:15:15.857  -->  00:15:18.417
So, to get the sigmoid activation function,
361

361

00:15:18.417  -->  00:15:20.710
We're gonna use the Sigmoid class,
362

362

00:15:20.710  -->  00:15:22.886
with capital S, and again, we're taking
363

363

00:15:22.886  -->  00:15:26.080
the Sigmoid class from the nn module,
364

364

00:15:26.080  -->  00:15:28.630
so we add here, dot, and then we take our class
365

365

00:15:28.630  -->  00:15:32.670
Sigmoid, with some parentheses, and no input.
366

366

00:15:32.670  -->  00:15:34.840
We don't need any arguments, that's enough to get
367

367

00:15:34.840  -->  00:15:36.470
the sigmoid activation function.
368

368

00:15:36.470  -->  00:15:38.050
Perfect!
369

369

00:15:38.050  -->  00:15:39.900
So that's all for the innit function,
370

370

00:15:39.900  -->  00:15:41.960
and so now, you can visualize this better.
371

371

00:15:41.960  -->  00:15:44.940
With the innit function, we defined the features
372

372

00:15:44.940  -->  00:15:46.690
of our auto-encoders, that is,
373

373

00:15:46.690  -->  00:15:49.910
the different full connections and the activation function.
374

374

00:15:49.910  -->  00:15:52.330
And, technically speaking, we just created
375

375

00:15:52.330  -->  00:15:55.190
five objects of two different classes,
376

376

00:15:55.190  -->  00:15:57.500
four objects of the Linear class,
377

377

00:15:57.500  -->  00:16:00.841
and one object of this Sigmoid class.
378

378

00:16:00.841  -->  00:16:03.880
Okay, so perfect, the innit function is done.
379

379

00:16:03.880  -->  00:16:06.430
And now, we're gonna make another function
380

380

00:16:06.430  -->  00:16:09.100
that is required to build our auto-encoders.
381

381

00:16:09.100  -->  00:16:11.090
And so again, now you see that we have to define
382

382

00:16:11.090  -->  00:16:13.630
two functions to build our auto-encoders.
383

383

00:16:13.630  -->  00:16:16.340
So that's why we definitely had to use a class.
384

384

00:16:16.340  -->  00:16:19.720
Okay, so what's this second function going to do?
385

385

00:16:19.720  -->  00:16:21.620
Well, it's basically going to do
386

386

00:16:21.620  -->  00:16:24.730
the action that takes place in auto-encoder.
387

387

00:16:24.730  -->  00:16:26.100
And what is this action?
388

388

00:16:26.100  -->  00:16:28.910
Well, that's basically the encoding, and the decoding.
389

389

00:16:28.910  -->  00:16:30.550
So what's were about to do right now,
390

390

00:16:30.550  -->  00:16:32.998
is make this function, that we're going to call forward.
391

391

00:16:32.998  -->  00:16:35.620
And that will proceed to the different encodings
392

392

00:16:35.620  -->  00:16:37.370
and decodings when the observation
393

393

00:16:37.370  -->  00:16:39.590
is forwarded into the network.
394

394

00:16:39.590  -->  00:16:41.250
But we're gonna take a small break now
395

395

00:16:41.250  -->  00:16:43.545
and we're gonna do this in the next tutorial.
396

396

00:16:43.545  -->  00:16:45.553
Until then, enjoy deep learning!
