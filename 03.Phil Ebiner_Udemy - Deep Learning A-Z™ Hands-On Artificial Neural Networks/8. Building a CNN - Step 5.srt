1
1

00:00:00,330  -->  00:00:02,930
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,930  -->  00:00:05,340
In the previous tutorial we took care of the first step,
3

3

00:00:05,340  -->  00:00:06,670
the convolution step.
4

4

00:00:06,670  -->  00:00:09,570
So that's one step done and now three more to go.
5

5

00:00:09,570  -->  00:00:12,200
And in today's tutorial we'll take care of the second step,
6

6

00:00:12,200  -->  00:00:13,570
the pooling step.
7

7

00:00:13,570  -->  00:00:16,540
So, quick reminder, this pooling step is very easy,
8

8

00:00:16,540  -->  00:00:20,370
it just consists of reducing the size of your feature maps.
9

9

00:00:20,370  -->  00:00:21,600
And how do we do that?
10

10

00:00:21,600  -->  00:00:24,390
Well again we take this two-by-two sub table,
11

11

00:00:24,390  -->  00:00:26,930
this blue square table that you can see here
12

12

00:00:26,930  -->  00:00:29,100
and that we slide all over the feature map
13

13

00:00:29,100  -->  00:00:31,950
and each time we take the maximum of the four cells
14

14

00:00:31,950  -->  00:00:33,390
inside this blue square.
15

15

00:00:33,390  -->  00:00:34,327
So, taking the max is called Max Pooling,
16

16

00:00:34,327  -->  00:00:38,560
and we slide the squared table with a stride of two
17

17

00:00:38,560  -->  00:00:40,950
not with a stride of one as we did previously
18

18

00:00:40,950  -->  00:00:44,610
with the feature detector that slided over the input image
19

19

00:00:44,610  -->  00:00:46,170
with a stride of one.
20

20

00:00:46,170  -->  00:00:48,790
Here, it's with a stride of two and therefore
21

21

00:00:48,790  -->  00:00:52,290
since each time we take the max of a two-by-two table,
22

22

00:00:52,290  -->  00:00:53,410
well what happens in the end
23

23

00:00:53,410  -->  00:00:56,820
is that we get a new feature map, with a reduced size.
24

24

00:00:56,820  -->  00:00:59,990
And more precisely, the size of the original feature map
25

25

00:00:59,990  -->  00:01:03,290
is divided by two when we apply max pooling.
26

26

00:01:03,290  -->  00:01:06,210
Well here we can see that the dimensions are five-by-five
27

27

00:01:06,210  -->  00:01:08,370
because we have five rows and five columns
28

28

00:01:08,370  -->  00:01:11,640
and when we apply max pooling, we get a reduced feature map
29

29

00:01:11,640  -->  00:01:14,050
of three-by-three so that's actually half
30

30

00:01:14,050  -->  00:01:15,950
of the original size plus one
31

31

00:01:15,950  -->  00:01:18,780
but then if the size of the feature map was an even number,
32

32

00:01:18,780  -->  00:01:21,290
then it's size would be divided by two.
33

33

00:01:21,290  -->  00:01:24,670
So then we apply max pooling on each of our feature maps.
34

34

00:01:24,670  -->  00:01:26,400
And then we obtain our next layer
35

35

00:01:26,400  -->  00:01:28,650
composed of all these reduced feature maps
36

36

00:01:28,650  -->  00:01:31,230
and that is called the pooling layer.
37

37

00:01:31,230  -->  00:01:32,850
Alright, and why do we do this?
38

38

00:01:32,850  -->  00:01:35,120
Why do we apply this max pooling step?
39

39

00:01:35,120  -->  00:01:37,810
It's because we want to reduce the number of nodes
40

40

00:01:37,810  -->  00:01:40,930
we'll get in the next step, that is the Flattening step
41

41

00:01:40,930  -->  00:01:43,400
and then the Full Connection step
42

42

00:01:43,400  -->  00:01:45,980
because in these next steps basically what we'll get
43

43

00:01:45,980  -->  00:01:48,590
is all the cells of our feature maps,
44

44

00:01:48,590  -->  00:01:51,940
flattened in one huge one-dimensional vector.
45

45

00:01:51,940  -->  00:01:55,310
So, if we don't reduce the size of these feature maps,
46

46

00:01:55,310  -->  00:01:57,390
well we'll get to too large vector and then we'll get
47

47

00:01:57,390  -->  00:01:59,950
too many nodes in the fully connected layers
48

48

00:01:59,950  -->  00:02:02,980
and therefore our model will be highly compute-intensive
49

49

00:02:02,980  -->  00:02:04,680
and we want to avoid this so we're applying
50

50

00:02:04,680  -->  00:02:08,330
this max pooling step to reduce the size of our feature maps
51

51

00:02:08,330  -->  00:02:10,410
and therefore reduce the number of nodes
52

52

00:02:10,410  -->  00:02:12,370
in the feature fully connected layers.
53

53

00:02:12,370  -->  00:02:14,550
And that way this will reduce the complexity
54

54

00:02:14,550  -->  00:02:17,370
and the time execution but without the losing
55

55

00:02:17,370  -->  00:02:20,330
the performance because, by ticking the maximum
56

56

00:02:20,330  -->  00:02:23,560
of these two-by-two sub tables of the feature map,
57

57

00:02:23,560  -->  00:02:25,960
we are in some way keeping the information
58

58

00:02:25,960  -->  00:02:28,920
because we are keeping track of the part of the image,
59

59

00:02:28,920  -->  00:02:30,700
that contained the high numbers
60

60

00:02:30,700  -->  00:02:33,070
corresponding to where the feature detectors,
61

61

00:02:33,070  -->  00:02:36,260
detected some specific features in the input image.
62

62

00:02:36,260  -->  00:02:38,720
So we don't lose the spacial structure information,
63

63

00:02:38,720  -->  00:02:41,010
and therefore we don't lose the performance of the model
64

64

00:02:41,010  -->  00:02:43,560
but at the same time, we managed to reduce
65

65

00:02:43,560  -->  00:02:47,030
the time complexity and we make it less compute-intensive.
66

66

00:02:47,030  -->  00:02:48,560
So that's a very important step
67

67

00:02:48,560  -->  00:02:51,220
and lets implement it right now on Spider.
68

68

00:02:51,220  -->  00:02:55,370
So here it is, all prepared, we're gonna start by taking
69

69

00:02:55,370  -->  00:02:58,330
our classifier as usual because again,
70

70

00:02:58,330  -->  00:03:01,520
what we're gonna do now is, use a method
71

71

00:03:01,520  -->  00:03:03,320
which is still going to be the same
72

72

00:03:03,320  -->  00:03:06,920
add method so dot here and then add.
73

73

00:03:06,920  -->  00:03:09,850
Alright, and now parenthesis, and then we're gonna use
74

74

00:03:09,850  -->  00:03:13,550
a function that will complete this max pooling step.
75

75

00:03:13,550  -->  00:03:15,240
So according to you what is going to be
76

76

00:03:15,240  -->  00:03:16,860
the name of this function?
77

77

00:03:16,860  -->  00:03:21,250
Well very easy, you have it right here it's MaxPooling2D.
78

78

00:03:21,250  -->  00:03:24,890
So let's input that into our add method, MaxPooling2D.
79

79

00:03:27,580  -->  00:03:31,290
Alright and then new parenthesis to input the size
80

80

00:03:31,290  -->  00:03:34,350
of you know this sub table that we slide all over
81

81

00:03:34,350  -->  00:03:37,560
the feature map to take the maximum in each sub table.
82

82

00:03:37,560  -->  00:03:40,633
So, the size parameter is called pool size.
83

83

00:03:42,140  -->  00:03:46,620
Here we go, and then equals and then parenthesis,
84

84

00:03:46,620  -->  00:03:49,020
and that's where we input the size.
85

85

00:03:49,020  -->  00:03:49,853
So in the example given in the slide, as you can see
86

86

00:03:49,853  -->  00:03:54,710
we take a size of two-by-two and this is actually
87

87

00:03:54,710  -->  00:03:57,310
what we take in general when we apply max pooling
88

88

00:03:57,310  -->  00:04:00,070
on our feature maps created after the first step.
89

89

00:04:00,070  -->  00:04:02,190
Well most of the time we take a two-by two
90

90

00:04:02,190  -->  00:04:04,400
because we don't wanna lose the information,
91

91

00:04:04,400  -->  00:04:06,610
by taking two-by-two, we keep the information
92

92

00:04:06,610  -->  00:04:09,490
and we're still being precise on where we have
93

93

00:04:09,490  -->  00:04:11,490
the high numbers in the feature maps,
94

94

00:04:11,490  -->  00:04:15,090
that the text and specific features of the input image.
95

95

00:04:15,090  -->  00:04:17,540
So two-by-two is fine and I recommend
96

96

00:04:17,540  -->  00:04:19,100
using that most of the time.
97

97

00:04:19,100  -->  00:04:21,090
So to input this two-by-two dimensions,
98

98

00:04:21,090  -->  00:04:24,653
it's very simple we just add two comma and two.
99

99

00:04:25,580  -->  00:04:28,820
Alright and that's done, the max pooling step is done.
100

100

00:04:28,820  -->  00:04:31,090
We don't have anymore parameters to input,
101

101

00:04:31,090  -->  00:04:34,160
basically adding this line will reduce the size
102

102

00:04:34,160  -->  00:04:37,560
of your feature maps, and it will divide it by two.
103

103

00:04:37,560  -->  00:04:42,560
So I'm going to select this line and execute, here we go,
104

104

00:04:42,880  -->  00:04:46,190
now the size of our feature maps is divided by two,
105

105

00:04:46,190  -->  00:04:48,570
so we just reduced the complexity of our model
106

106

00:04:48,570  -->  00:04:51,040
without reducing its performance.
107

107

00:04:51,040  -->  00:04:51,873
Perfect.
108

108

00:04:51,873  -->  00:04:53,970
So second step done, two more to go
109

109

00:04:53,970  -->  00:04:56,600
and we'll do the third step in the next tutorial.
110

110

00:04:56,600  -->  00:04:58,200
Until then, enjoy deep learning!
