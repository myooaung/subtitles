WEBVTT
0
1
00:00:00.370 --> 00:00:02.850
<v ->Alright, exciting tutorial ahead.</v>
1

2
00:00:02.850 --> 00:00:04.770
Welcome back to the course on deep learning.
2

3
00:00:04.770 --> 00:00:07.990
Today we're talking about how do neural networks work.
3

4
00:00:07.990 --> 00:00:09.700
Now we've laid a lot of groundwork
4

5
00:00:09.700 --> 00:00:12.930
we've talked about how neural networks are structured,
5

6
00:00:12.930 --> 00:00:14.850
what elements they consist of,
6

7
00:00:14.850 --> 00:00:16.573
and even their functionality.
7

8
00:00:16.573 --> 00:00:19.500
Today we're going to look at a real example
8

9
00:00:19.500 --> 00:00:21.760
of how a neural network can be applied
9

10
00:00:21.760 --> 00:00:24.390
and we're actually gonna work step-by-step
10

11
00:00:24.390 --> 00:00:26.410
through the process of its application
11

12
00:00:26.410 --> 00:00:28.250
so we know what's going on.
12

13
00:00:28.250 --> 00:00:29.160
So let's have a look.
13

14
00:00:29.160 --> 00:00:30.680
What example are we going to be talking about?
14

15
00:00:30.680 --> 00:00:33.650
We're going to be looking at property valuation, so
15

16
00:00:33.650 --> 00:00:36.840
we're going to look at a neural network that
16

17
00:00:36.840 --> 00:00:39.500
takes in some parameters about a property and values it.
17

18
00:00:39.500 --> 00:00:41.830
And the thing here, there's a small caveat
18

19
00:00:41.830 --> 00:00:43.240
for today's tutorial,
19

20
00:00:43.240 --> 00:00:45.730
and that is, we're not actually going to train the network.
20

21
00:00:45.730 --> 00:00:48.200
So a very important part in neural networks
21

22
00:00:48.200 --> 00:00:49.280
is training them up,
22

23
00:00:49.280 --> 00:00:52.210
and we're going to look at that in the next tutorials
23

24
00:00:52.210 --> 00:00:53.340
in this section.
24

25
00:00:53.340 --> 00:00:55.520
For now we're going to focus on the actual application
25

26
00:00:55.520 --> 00:00:58.370
so we're going to work with a neural network that
26

27
00:00:58.370 --> 00:00:59.400
we're going to
27

28
00:00:59.400 --> 00:01:01.810
pretend is already trained up.
28

29
00:01:01.810 --> 00:01:03.600
And that will allow us to focus
29

30
00:01:03.600 --> 00:01:05.220
on the application side of things
30

31
00:01:05.220 --> 00:01:07.690
and not get bogged down in the training aspect.
31

32
00:01:07.690 --> 00:01:09.090
And then we'll cover off the training
32

33
00:01:09.090 --> 00:01:11.690
when we already know the end goal we're working towards.
33

34
00:01:11.690 --> 00:01:12.523
Sound good?
34

35
00:01:12.523 --> 00:01:15.120
Alright, let's jump straight into it.
35

36
00:01:15.120 --> 00:01:19.340
So let's say we have some input parameters.
36

37
00:01:19.340 --> 00:01:21.360
So let's say we have four parameters about the property.
37

38
00:01:21.360 --> 00:01:24.140
We have area in squared feet,
38

39
00:01:24.140 --> 00:01:25.520
we have the number of bedrooms,
39

40
00:01:25.520 --> 00:01:27.950
the distance to the city in miles,
40

41
00:01:27.950 --> 00:01:28.783
the nearest city
41

42
00:01:28.783 --> 00:01:29.720
and the age of the property.
42

43
00:01:29.720 --> 00:01:31.740
And all of those four are going to comprise
43

44
00:01:31.740 --> 00:01:33.560
our input layer.
44

45
00:01:33.560 --> 00:01:37.130
Now of course there're probably way more parameters
45

46
00:01:37.130 --> 00:01:39.860
that define the price of a property
46

47
00:01:39.860 --> 00:01:40.960
but for simplicity's sake
47

48
00:01:40.960 --> 00:01:42.520
we're going to look at just these four.
48

49
00:01:42.520 --> 00:01:46.717
Now in its very basic form, a neural network only has
49

50
00:01:46.717 --> 00:01:49.960
an input layer and an output layer, so no hidden layers
50

51
00:01:49.960 --> 00:01:52.200
and our output layer is the price that we're predicting.
51

52
00:01:52.200 --> 00:01:57.030
So in this form, what these input variables would do
52

53
00:01:57.030 --> 00:02:00.930
is they would just be weighted up by the synapses
53

54
00:02:00.930 --> 00:02:03.960
and then the output layer would be calculated
54

55
00:02:03.960 --> 00:02:05.440
or basically the price would be calculated
55

56
00:02:05.440 --> 00:02:06.570
and we'd get a price.
56

57
00:02:06.570 --> 00:02:09.230
For instance a price could be calculated as
57

58
00:02:09.230 --> 00:02:14.160
as simple as, the weighted sum of all of the inputs.
58

59
00:02:14.160 --> 00:02:16.510
And again here you could use pretty much any function.
59

60
00:02:16.510 --> 00:02:19.030
You could use what we're using now,
60

61
00:02:19.030 --> 00:02:21.920
we could use any of the activation functions
61

62
00:02:21.920 --> 00:02:23.210
we had previously,
62

63
00:02:23.210 --> 00:02:25.960
you could use a logistic regression,
63

64
00:02:25.960 --> 00:02:29.120
you could use a squared function,
64

65
00:02:29.120 --> 00:02:31.130
you could use pretty much anything here.
65

66
00:02:31.130 --> 00:02:33.390
But the point is that you get a certain output.
66

67
00:02:33.390 --> 00:02:37.220
And moreover, most of the machine learning algorithms
67

68
00:02:37.220 --> 00:02:40.940
that exist can be represented in this format.
68

69
00:02:40.940 --> 00:02:42.970
This is basically a diagrammatic representation
69

70
00:02:42.970 --> 00:02:45.570
of how you deal with the variables, right.
70

71
00:02:45.570 --> 00:02:47.250
By changing the weights, the formulas,
71

72
00:02:47.250 --> 00:02:49.270
you can accomplish quite a lot
72

73
00:02:49.270 --> 00:02:50.940
of the machine learning algorithms
73

74
00:02:50.940 --> 00:02:53.820
that we've talked about before
74

75
00:02:53.820 --> 00:02:55.330
and put them into this form.
75

76
00:02:55.330 --> 00:02:56.700
And that just stands to show
76

77
00:02:56.700 --> 00:02:58.600
how powerful neural networks are,
77

78
00:02:58.600 --> 00:03:00.840
that even without the hidden layers
78

79
00:03:00.840 --> 00:03:03.030
we already have a representation
79

80
00:03:03.030 --> 00:03:05.400
that works for most other machine learning algorithms.
80

81
00:03:05.400 --> 00:03:08.840
But in neural networks, what we do have
81

82
00:03:08.840 --> 00:03:10.250
is an extra advantage that gives us
82

83
00:03:10.250 --> 00:03:12.710
lots of flexibility and power
83

84
00:03:12.710 --> 00:03:16.950
which is where that increase in accuracy comes from,
84

85
00:03:16.950 --> 00:03:20.520
and that power is the hidden layers.
85

86
00:03:20.520 --> 00:03:22.810
And there we go, that's our hidden layer
86

87
00:03:22.810 --> 00:03:23.850
we've added it in
87

88
00:03:23.850 --> 00:03:26.150
and now we're going to understand
88

89
00:03:26.150 --> 00:03:30.170
how that hidden layer gives us that extra power.
89

90
00:03:30.170 --> 00:03:33.500
And in fact to do that we're going to walk through
90

91
00:03:33.500 --> 00:03:34.333
an example.
91

92
00:03:34.333 --> 00:03:35.610
So as we agreed this neural network
92

93
00:03:35.610 --> 00:03:37.030
has already been trained up,
93

94
00:03:37.030 --> 00:03:38.500
and now we just going to plug in
94

95
00:03:38.500 --> 00:03:41.430
we're going to imagine that we're plugging in a property,
95

96
00:03:41.430 --> 00:03:43.740
and we're going to walk step-by-step
96

97
00:03:43.740 --> 00:03:46.270
through how the neural network
97

98
00:03:46.270 --> 00:03:49.090
will deal with the input variables
98

99
00:03:49.090 --> 00:03:50.400
and calculate the hidden layer
99

100
00:03:50.400 --> 00:03:51.820
and then calculate the output layer.
100

101
00:03:51.820 --> 00:03:54.960
So let's go through this, this is going to be exciting.
101

102
00:03:54.960 --> 00:03:57.010
We've got all four variables on the left,
102

103
00:03:58.030 --> 00:03:59.910
and we're going to first start with the top neuron
103

104
00:03:59.910 --> 00:04:01.330
on the hidden layer.
104

105
00:04:01.330 --> 00:04:04.240
Now as we previously saw in the previous tutorials,
105

106
00:04:04.240 --> 00:04:06.860
all of the neurons from the input layer
106

107
00:04:06.860 --> 00:04:10.380
they have synapses connecting it each one of them
107

108
00:04:10.380 --> 00:04:13.390
to the top neuron in the hidden layer.
108

109
00:04:13.390 --> 00:04:15.140
And those synapses have weight.
109

110
00:04:15.140 --> 00:04:17.340
Now let's agree that some weights
110

111
00:04:17.340 --> 00:04:18.750
will have a non-zero value,
111

112
00:04:18.750 --> 00:04:20.040
some weights will have a zero value.
112

113
00:04:20.040 --> 00:04:24.790
Because basically not all inputs will be valid
113

114
00:04:24.790 --> 00:04:26.560
or not all inputs will be important
114

115
00:04:26.560 --> 00:04:27.770
for every single neuron.
115

116
00:04:27.770 --> 00:04:29.260
Sometimes inputs will not be important.
116

117
00:04:29.260 --> 00:04:30.530
Now here we can see two examples.
117

118
00:04:30.530 --> 00:04:31.730
That X1 and X3,
118

119
00:04:31.730 --> 00:04:33.870
the area and the distance to the city in miles
119

120
00:04:33.870 --> 00:04:35.580
are important for that neuron
120

121
00:04:35.580 --> 00:04:37.178
whereas bedrooms and age are not.
121

122
00:04:37.178 --> 00:04:38.660
And let's think about this for a second.
122

123
00:04:38.660 --> 00:04:40.610
Why, how would that be the case.
123

124
00:04:40.610 --> 00:04:44.160
Why would a certain neuron be
124

125
00:04:44.160 --> 00:04:46.170
linked to the area and the distance.
125

126
00:04:46.170 --> 00:04:47.260
What could that mean?
126

127
00:04:47.260 --> 00:04:48.715
Well that could mean that
127

128
00:04:48.715 --> 00:04:51.570
normally the further away you get from the city,
128

129
00:04:51.570 --> 00:04:53.340
the cheaper real estate becomes
129

130
00:04:53.340 --> 00:04:57.260
and therefore the space in square feet of properties
130

131
00:04:57.260 --> 00:04:58.440
becomes larger.
131

132
00:04:58.440 --> 00:05:00.390
So for the same price you can get a larger property
132

133
00:05:00.390 --> 00:05:01.780
the further away you go from the city.
133

134
00:05:01.780 --> 00:05:03.610
That's normal, right, that makes sense.
134

135
00:05:03.610 --> 00:05:05.910
And probably what this neuron is doing
135

136
00:05:05.910 --> 00:05:08.900
is it is looking specifically
136

137
00:05:08.900 --> 00:05:12.430
like a sniper, it's looking for area of properties
137

138
00:05:12.430 --> 00:05:16.480
which have, which are not so far from the city
138

139
00:05:16.480 --> 00:05:17.530
but have a large area.
139

140
00:05:17.530 --> 00:05:21.940
So for their distance from the city, they have an unfair
140

141
00:05:23.240 --> 00:05:25.010
square foot area, right.
141

142
00:05:25.010 --> 00:05:27.560
So something that's abnormal, it's higher than average.
142

143
00:05:27.560 --> 00:05:29.900
So they're quite close to the city but they're still large
143

144
00:05:29.900 --> 00:05:33.110
as opposed to the other ones at the same distance.
144

145
00:05:33.110 --> 00:05:35.870
So that neuron, again we're speculating here
145

146
00:05:35.870 --> 00:05:37.590
but that neuron might be picking out,
146

147
00:05:37.590 --> 00:05:40.060
laser picking out, those specific properties
147

148
00:05:40.060 --> 00:05:43.080
and it will activate, and hence the activation function,
148

149
00:05:43.080 --> 00:05:45.190
it will activate, it'll fire up
149

150
00:05:45.190 --> 00:05:46.960
only when the certain criteria is met
150

151
00:05:46.960 --> 00:05:49.540
that you know, the distance and the area of the prop,
151

152
00:05:49.540 --> 00:05:51.940
distance to the city and the area of the property
152

153
00:05:51.940 --> 00:05:55.390
and it performs its own calculations inside itself
153

154
00:05:55.390 --> 00:05:57.280
and it combines those two
154

155
00:05:57.280 --> 00:05:59.330
and as soon as certain criteria is met it fires up
155

156
00:05:59.330 --> 00:06:02.040
and that contributes to the price in the output layer.
156

157
00:06:02.040 --> 00:06:04.200
And therefore this neuron doesn't really care
157

158
00:06:04.200 --> 00:06:05.830
about bedrooms and age of the property
158

159
00:06:05.830 --> 00:06:07.610
because it's focused on that specific thing.
159

160
00:06:07.610 --> 00:06:10.760
That's where the power of the neural network comes from
160

161
00:06:10.760 --> 00:06:12.300
because you have many of these neurons
161

162
00:06:12.300 --> 00:06:14.195
and we'll see just now how the other ones work.
162

163
00:06:14.195 --> 00:06:16.530
But what I wanted to agree here is that
163

164
00:06:16.530 --> 00:06:20.260
let's not even draw these lines for the synapses
164

165
00:06:20.260 --> 00:06:22.530
that are not in play
165

166
00:06:22.530 --> 00:06:24.110
so that we don't clutter up our image
166

167
00:06:24.110 --> 00:06:25.920
that's the only reason we're not gonna draw them.
167

168
00:06:25.920 --> 00:06:27.340
Let's just get rid of those two
168

169
00:06:27.340 --> 00:06:29.430
and that way we will know exactly, okay,
169

170
00:06:29.430 --> 00:06:33.683
so this neuron is focused on area and distance to the city.
170

171
00:06:35.020 --> 00:06:36.790
As long as we agree on that let's move on to the next one.
171

172
00:06:36.790 --> 00:06:38.760
Let's take the one in the middle.
172

173
00:06:38.760 --> 00:06:42.010
Here we've got three parameters feeding into this neuron
173

174
00:06:42.010 --> 00:06:43.950
so we've got the area, the bedrooms,
174

175
00:06:43.950 --> 00:06:45.920
and the age of the property.
175

176
00:06:45.920 --> 00:06:48.690
So what could be the reason here?
176

177
00:06:48.690 --> 00:06:50.930
Again let's try to understand
177

178
00:06:50.930 --> 00:06:54.590
the intuition, the thinking of this neuron.
178

179
00:06:54.590 --> 00:06:56.020
How is this neuron thinking,
179

180
00:06:56.020 --> 00:06:57.700
why is it picking these three parameters,
180

181
00:06:57.700 --> 00:07:01.880
what could it be, what could have it like found in the data
181

182
00:07:01.880 --> 00:07:04.510
so we've already established this is a trained up data set.
182

183
00:07:04.510 --> 00:07:06.500
The training has happened a long time ago
183

184
00:07:06.500 --> 00:07:07.640
maybe like a day ago
184

185
00:07:07.640 --> 00:07:09.460
or somebody's already trained up this data set
185

186
00:07:09.460 --> 00:07:10.410
now we're just applying
186

187
00:07:10.410 --> 00:07:11.960
and we know that this neuron
187

188
00:07:11.960 --> 00:07:14.270
through all the thousands of examples of properties
188

189
00:07:14.270 --> 00:07:18.410
has found out that the area plus the bedrooms plus the age
189

190
00:07:18.410 --> 00:07:20.500
combination of those parameters is important.
190

191
00:07:20.500 --> 00:07:22.330
Why could that be the case?
191

192
00:07:22.330 --> 00:07:27.170
For instance, maybe in that specific city, in those suburbs
192

193
00:07:27.170 --> 00:07:30.660
that this neural network has been trained up in
193

194
00:07:30.660 --> 00:07:34.960
perhaps there's a lot of families
194

195
00:07:34.960 --> 00:07:37.270
with kids, with two or more children
195

196
00:07:37.270 --> 00:07:40.820
who are looking for large properties
196

197
00:07:40.820 --> 00:07:43.530
with lots of bedrooms but which are new.
197

198
00:07:43.530 --> 00:07:45.310
Which are
198

199
00:07:45.310 --> 00:07:46.640
not old properties
199

200
00:07:46.640 --> 00:07:49.210
because maybe in that area
200

201
00:07:50.260 --> 00:07:51.470
most of the properties are kind of like
201

202
00:07:51.470 --> 00:07:52.850
big properties are usually old
202

203
00:07:52.850 --> 00:07:55.030
but there's lots of modern families
203

204
00:07:55.030 --> 00:07:58.240
and maybe there has been a socio-demographic shift
204

205
00:07:58.240 --> 00:08:02.810
and, or maybe there's been a lot of some growth
205

206
00:08:02.810 --> 00:08:06.150
in terms of employment and jobs for
206

207
00:08:06.150 --> 00:08:07.480
the younger side of population
207

208
00:08:07.480 --> 00:08:11.700
maybe just, the population demographics have changed
208

209
00:08:11.700 --> 00:08:16.470
and now younger couples or younger families
209

210
00:08:16.470 --> 00:08:18.290
are looking for properties
210

211
00:08:18.290 --> 00:08:20.870
but they prefer newer properties.
211

212
00:08:20.870 --> 00:08:23.670
So they want the age of the property to be lower.
212

213
00:08:23.670 --> 00:08:25.530
And hence, from the training
213

214
00:08:25.530 --> 00:08:27.280
that this neural network has undergone
214

215
00:08:27.280 --> 00:08:31.070
it knows that when there's a property with a large area
215

216
00:08:31.070 --> 00:08:32.258
and with lots of bedrooms
216

217
00:08:32.258 --> 00:08:34.140
at least three bedrooms for the parents
217

218
00:08:34.140 --> 00:08:35.600
for the first child, for the second child
218

219
00:08:35.600 --> 00:08:38.410
for at least three bedrooms, maybe a guest room
219

220
00:08:38.410 --> 00:08:40.520
when a new property with
220

221
00:08:40.520 --> 00:08:42.800
higher area and lots of bedrooms
221

222
00:08:42.800 --> 00:08:43.903
that is valued.
222

223
00:08:43.903 --> 00:08:45.890
In that market, that is valuable.
223

224
00:08:45.890 --> 00:08:48.170
So that neuron has picked that up
224

225
00:08:48.170 --> 00:08:51.380
it knows that okay, so this is what I'm gonna be looking for
225

226
00:08:51.380 --> 00:08:53.180
I don't care about the distance to the city in miles
226

227
00:08:53.180 --> 00:08:56.220
wherever it is, as long as it has high area,
227

228
00:08:56.220 --> 00:08:58.950
lots of bedrooms, as soon as that criteria is met
228

229
00:08:58.950 --> 00:08:59.783
that neuron fires up
229

230
00:08:59.783 --> 00:09:01.600
and the combination of these three parameters
230

231
00:09:01.600 --> 00:09:03.280
and this is again, this is where the power
231

232
00:09:03.280 --> 00:09:04.790
of the neural network is coming from
232

233
00:09:04.790 --> 00:09:07.070
because it combines these three parameters
233

234
00:09:07.070 --> 00:09:11.350
into a brand new parameter, into a brand attribute
234

235
00:09:11.350 --> 00:09:15.700
that helps with the evaluation
235

236
00:09:15.700 --> 00:09:17.870
that helps with the evaluation of the property
236

237
00:09:17.870 --> 00:09:19.670
combines them into a new attribute
237

238
00:09:19.670 --> 00:09:21.770
and therefore it's more precise.
238

239
00:09:21.770 --> 00:09:24.140
So there we go that's how that neuron works.
239

240
00:09:24.140 --> 00:09:25.480
And let's look at another one
240

241
00:09:25.480 --> 00:09:27.050
let's look at the very bottom one.
241

242
00:09:27.050 --> 00:09:28.810
For instance this neuron could be
242

243
00:09:29.810 --> 00:09:31.780
could even have picked up just one parameter
243

244
00:09:31.780 --> 00:09:33.610
it could have just picked up age
244

245
00:09:33.610 --> 00:09:35.440
and not any of the other ones.
245

246
00:09:35.440 --> 00:09:37.800
And how could that be the case?
246

247
00:09:37.800 --> 00:09:42.800
Well this is a classic example of when age could mean
247

248
00:09:43.040 --> 00:09:45.210
as we all know, the older property
248

249
00:09:45.210 --> 00:09:46.540
is usually, it's less valuable
249

250
00:09:46.540 --> 00:09:48.950
because it's worn out, probably the building is old
250

251
00:09:48.950 --> 00:09:50.660
probably things are falling apart,
251

252
00:09:50.660 --> 00:09:51.850
more maintenance is required.
252

253
00:09:51.850 --> 00:09:55.430
So the price drops in terms of the price of the real estate
253

254
00:09:55.430 --> 00:09:57.660
whereas a brand new building, it would be more expensive
254

255
00:09:57.660 --> 00:09:58.920
because it's brand new.
255

256
00:09:58.920 --> 00:10:01.880
But perhaps if a property's over a certain age
256

257
00:10:01.880 --> 00:10:03.920
that could indicate that it's a historic property.
257

258
00:10:03.920 --> 00:10:06.670
For instance, if a property's under 100 years old
258

259
00:10:06.670 --> 00:10:11.300
then the older it is, the less valuable it is.
259

260
00:10:11.300 --> 00:10:13.960
But as soon as it jumps over 100 years old
260

261
00:10:13.960 --> 00:10:15.790
all of a sudden it becomes a historic property
261

262
00:10:15.790 --> 00:10:18.110
because this is a property
262

263
00:10:18.110 --> 00:10:20.730
where people used to live hundreds of years ago,
263

264
00:10:20.730 --> 00:10:24.010
it tells a story, it's got all this history behind it
264

265
00:10:24.010 --> 00:10:25.100
and some people like that.
265

266
00:10:25.100 --> 00:10:26.160
Some people value that.
266

267
00:10:26.160 --> 00:10:28.180
In fact quite a lot of people would like that
267

268
00:10:28.180 --> 00:10:30.946
and would be proud to live in a property and
268

269
00:10:30.946 --> 00:10:34.570
especially in the higher socioeconomic classes
269

270
00:10:34.570 --> 00:10:37.010
they would show off to their friends or things like that.
270

271
00:10:37.010 --> 00:10:40.320
And therefore, properties that are over 100 years old
271

272
00:10:40.320 --> 00:10:43.610
could be deemed as historic, and therefore this neuron
272

273
00:10:43.610 --> 00:10:46.280
as soon as it sees a property over 100 years old
273

274
00:10:46.280 --> 00:10:49.280
it will fire up and contribute to the overall price.
274

275
00:10:49.280 --> 00:10:51.590
And otherwise, if it's under 100 years old
275

276
00:10:51.590 --> 00:10:52.707
then it won't even work.
276

277
00:10:52.707 --> 00:10:54.508
And this is a good example of
277

278
00:10:54.508 --> 00:10:57.500
the rectifier function being applied.
278

279
00:10:57.500 --> 00:11:01.110
So here you've got a very
279

280
00:11:01.110 --> 00:11:03.050
a zero, until a certain point,
280

281
00:11:03.050 --> 00:11:04.450
and then let's say 100 years old
281

282
00:11:04.450 --> 00:11:05.550
and then after 100 years old,
282

283
00:11:05.550 --> 00:11:08.810
the older it gets, the higher the contribution
283

284
00:11:08.810 --> 00:11:11.360
of this neuron to the overall price.
284

285
00:11:11.360 --> 00:11:13.842
And this just a wonderful example of
285

286
00:11:13.842 --> 00:11:17.943
very simple example of this rectifier function in action.
286

287
00:11:18.790 --> 00:11:20.940
So there we go, that could be this neuron.
287

288
00:11:20.940 --> 00:11:24.370
And moreover, the neural network could have even
288

289
00:11:24.370 --> 00:11:27.887
picked up things that we wouldn't have thought of ourselves
289

290
00:11:27.887 --> 00:11:30.810
for instance bedrooms plus distance to the city.
290

291
00:11:30.810 --> 00:11:33.650
Maybe that's in combination somehow
291

292
00:11:33.650 --> 00:11:34.520
contributes to the price.
292

293
00:11:34.520 --> 00:11:36.720
Maybe it's not as strong as the other neurons
293

294
00:11:36.720 --> 00:11:38.160
and it contributes, but it still contributes.
294

295
00:11:38.160 --> 00:11:39.753
Or maybe it detracts from the price,
295

296
00:11:39.753 --> 00:11:41.340
that could also be the case.
296

297
00:11:41.340 --> 00:11:42.570
Or other things like that.
297

298
00:11:42.570 --> 00:11:45.150
And maybe a neuron picked up all four,
298

299
00:11:45.150 --> 00:11:47.240
a combination of all four of these parameters
299

300
00:11:47.240 --> 00:11:48.340
and as you can see the
300

301
00:11:49.580 --> 00:11:52.840
that these neurons, this whole hidden layer situation
301

302
00:11:52.840 --> 00:11:56.380
allows you to increase the flexibility
302

303
00:11:56.380 --> 00:11:58.470
of your neural network
303

304
00:11:58.470 --> 00:12:00.680
and allows you to really look,
304

305
00:12:00.680 --> 00:12:04.140
allows the neural network to look for very specific things
305

306
00:12:04.140 --> 00:12:08.140
and then in combination, that's where the power comes from.
306

307
00:12:08.140 --> 00:12:09.660
It's like that example with the ants
307

308
00:12:09.660 --> 00:12:12.500
like an ant by itself cannot build an anthill,
308

309
00:12:12.500 --> 00:12:15.360
but when you have 1,000 or 100,000 ants
309

310
00:12:15.360 --> 00:12:17.070
they can build an anthill together.
310

311
00:12:17.070 --> 00:12:18.530
And that's the situation here.
311

312
00:12:18.530 --> 00:12:21.030
Each one of these neurons by itself cannot predict the price
312

313
00:12:21.030 --> 00:12:23.990
but together they have superpowers
313

314
00:12:23.990 --> 00:12:26.010
and they predict the price
314

315
00:12:26.010 --> 00:12:27.830
and they can do quite an accurate job
315

316
00:12:27.830 --> 00:12:30.620
if trained properly, if set up properly.
316

317
00:12:30.620 --> 00:12:32.850
And that's what this whole course is about
317

318
00:12:32.850 --> 00:12:35.270
in understanding how to utilize them.
318

319
00:12:35.270 --> 00:12:36.180
There we go so
319

320
00:12:36.180 --> 00:12:39.780
that is a step-by-step example and walkthrough
320

321
00:12:39.780 --> 00:12:42.320
of how neural networks actually work.
321

322
00:12:42.320 --> 00:12:43.670
I hope you enjoyed today's tutorial,
322

323
00:12:43.670 --> 00:12:45.490
and I can't wait to see you next time.
323

324
00:12:45.490 --> 00:12:47.433
Until then, enjoy deep learning.
