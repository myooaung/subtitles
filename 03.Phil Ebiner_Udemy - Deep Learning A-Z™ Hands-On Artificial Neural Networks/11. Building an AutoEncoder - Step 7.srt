1
1

00:00:00,080  -->  00:00:02,071
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,071  -->  00:00:05,151
So in the previous tutorial, we defined the init function,
3

3

00:00:05,151  -->  00:00:07,500
that basically defines the architecture
4

4

00:00:07,500  -->  00:00:08,780
of our neral network.
5

5

00:00:08,780  -->  00:00:09,670
And now in this tutorial,
6

6

00:00:09,670  -->  00:00:12,030
we're gonna make the second function required,
7

7

00:00:12,030  -->  00:00:14,800
which will not only do the action of encoding
8

8

00:00:14,800  -->  00:00:17,380
and decoding, but also will apply to different
9

9

00:00:17,380  -->  00:00:20,740
activation functions inside the full connections.
10

10

00:00:20,740  -->  00:00:23,410
But also, the main purpose of making this function
11

11

00:00:23,410  -->  00:00:25,930
is that it will return in the end the vector
12

12

00:00:25,930  -->  00:00:28,050
of predicted ratings that we will compare
13

13

00:00:28,050  -->  00:00:29,700
to the vector of real ratings,
14

14

00:00:29,700  -->  00:00:31,700
that is, the input vector.
15

15

00:00:31,700  -->  00:00:33,840
So let's do this, it's going to make sense.
16

16

00:00:33,840  -->  00:00:37,070
So again, we're defining functions starting with def,
17

17

00:00:37,070  -->  00:00:39,350
then we give the name of the function
18

18

00:00:39,350  -->  00:00:43,700
which we call forward, like forward propagation,
19

19

00:00:43,700  -->  00:00:45,570
because that's during the forward propagation
20

20

00:00:45,570  -->  00:00:48,060
that the encoding and decoding take place.
21

21

00:00:48,060  -->  00:00:49,720
Then some parenthesis.
22

22

00:00:49,720  -->  00:00:51,468
And in this function, we have to input two arguments,
23

23

00:00:51,468  -->  00:00:53,770
one first argument that we have to input
24

24

00:00:53,770  -->  00:00:55,990
all the time that is self because we're gonna
25

25

00:00:55,990  -->  00:00:58,373
use our object and so we need self to specify
26

26

00:00:58,373  -->  00:01:00,880
that the functions we'll use will be applied
27

27

00:01:00,880  -->  00:01:02,060
from our object.
28

28

00:01:02,060  -->  00:01:07,060
So self, then comma, and then we a second argument x
29

29

00:01:07,580  -->  00:01:09,570
which is our input vector,
30

30

00:01:09,570  -->  00:01:12,298
which you'll that we will transform this input vector x,
31

31

00:01:12,298  -->  00:01:15,157
that is our input vector features with all the ratings
32

32

00:01:15,157  -->  00:01:17,470
for the movies in one specific user,
33

33

00:01:17,470  -->  00:01:21,630
by encoding it twice and then decoding it twice again
34

34

00:01:21,630  -->  00:01:24,890
to get the final output vector that is the decoded vector
35

35

00:01:24,890  -->  00:01:26,630
that was reconstructed, okay.
36

36

00:01:26,630  -->  00:01:28,645
So that's call the second argument x
37

37

00:01:28,645  -->  00:01:30,469
and now we add a colon
38

38

00:01:30,469  -->  00:01:32,996
and then we go inside the function.
39

39

00:01:32,996  -->  00:01:35,260
Okay, so what do we do first?
40

40

00:01:35,260  -->  00:01:38,090
Well what we have to do first is very simply
41

41

00:01:38,090  -->  00:01:40,270
the first encoding that takes place,
42

42

00:01:40,270  -->  00:01:42,090
that is that we are about to encode
43

43

00:01:42,090  -->  00:01:45,110
our input vector features that is x
44

44

00:01:45,110  -->  00:01:49,070
into a first shorter vector composed of 20 elements
45

45

00:01:49,070  -->  00:01:50,710
in our first heading layer.
46

46

00:01:50,710  -->  00:01:51,543
So here we go.
47

47

00:01:51,543  -->  00:01:52,850
How are we going to do this?
48

48

00:01:52,850  -->  00:01:55,060
Well we're going to take our outer encoders
49

49

00:01:55,060  -->  00:01:58,850
that is self then dot and then we'll use
50

50

00:01:58,850  -->  00:02:01,830
this activation object that we created here
51

51

00:02:01,830  -->  00:02:03,071
from the sigmoid class,
52

52

00:02:03,071  -->  00:02:05,940
because this sigmoid activation function
53

53

00:02:05,940  -->  00:02:07,390
will activate the neurons
54

54

00:02:07,390  -->  00:02:09,987
of this first encoded vector of 20 elements.
55

55

00:02:09,987  -->  00:02:12,630
So let's use the activation function here.
56

56

00:02:12,630  -->  00:02:15,879
So we add activation and inside activation
57

57

00:02:15,879  -->  00:02:18,560
we have to input our input vector
58

58

00:02:18,560  -->  00:02:21,380
that so far is represented by x,
59

59

00:02:21,380  -->  00:02:24,110
but remember what's we're doing now is the encoding
60

60

00:02:24,110  -->  00:02:25,780
and to do the encoding we have to apply
61

61

00:02:25,780  -->  00:02:29,053
the activation function on the first full connection
62

62

00:02:29,053  -->  00:02:31,330
and the first full connection is represented
63

63

00:02:31,330  -->  00:02:34,860
by this object here, fc1, so in fact to include
64

64

00:02:34,860  -->  00:02:37,360
that information of the first full connection
65

65

00:02:37,360  -->  00:02:39,130
we don't input x directly.
66

66

00:02:39,130  -->  00:02:42,130
We are gonna input x in this object fc1,
67

67

00:02:42,130  -->  00:02:44,633
because fc1 is an object of the linear class
68

68

00:02:44,633  -->  00:02:47,740
so fc1 takes an argument and this argument
69

69

00:02:47,740  -->  00:02:50,998
is execute a vector at the left of this full connection
70

70

00:02:50,998  -->  00:02:53,578
and right now since we're in the first full connection,
71

71

00:02:53,578  -->  00:02:56,274
this vector at the left is the input vector x
72

72

00:02:56,274  -->  00:03:00,030
and so what we take here is our first full connection
73

73

00:03:00,030  -->  00:03:04,070
fc1 applied to the vector at the left of the full connection
74

74

00:03:04,070  -->  00:03:07,500
that is x and since this first full connection
75

75

00:03:07,500  -->  00:03:10,019
is associated to our object, our encoder,
76

76

00:03:10,019  -->  00:03:12,720
we have to specify here that we're taking
77

77

00:03:12,720  -->  00:03:14,833
this first full connection from self
78

78

00:03:14,833  -->  00:03:18,080
because self represents our object.
79

79

00:03:18,080  -->  00:03:21,470
All right, and this returns the encoded vector
80

80

00:03:21,470  -->  00:03:24,092
and actually since this forward function
81

81

00:03:24,092  -->  00:03:27,000
will return in the end the output vector
82

82

00:03:27,000  -->  00:03:29,220
that is actually the vector predicted ratings
83

83

00:03:29,220  -->  00:03:31,100
that will compare to the real rating,
84

84

00:03:31,100  -->  00:03:33,470
well what we're going to do is modify x
85

85

00:03:33,470  -->  00:03:36,140
after each encoding or decoding
86

86

00:03:36,140  -->  00:03:39,930
so since this returns the first encoded vector
87

87

00:03:39,930  -->  00:03:42,767
in the first hidden layer while this will be the new x
88

88

00:03:42,767  -->  00:03:44,190
and so I'm gonna add here,
89

89

00:03:44,190  -->  00:03:47,606
x equals self activation self fc1x.
90

90

00:03:47,606  -->  00:03:52,050
So to recap, this x here is the input vector features
91

91

00:03:52,050  -->  00:03:54,460
at the left of the first full connection
92

92

00:03:54,460  -->  00:03:56,090
and this x here will be
93

93

00:03:56,090  -->  00:03:59,020
the new first encoded vector resulting
94

94

00:03:59,020  -->  00:04:01,290
from this first encoding that happens here
95

95

00:04:01,290  -->  00:04:04,370
with the activation function in the first full connection.
96

96

00:04:04,370  -->  00:04:06,770
Okay, so that's what's maybe the tough part
97

97

00:04:06,770  -->  00:04:08,780
but now it's going to be very easy
98

98

00:04:08,780  -->  00:04:10,560
because basically what we need to do
99

99

00:04:10,560  -->  00:04:12,891
is do the same for the other full connections
100

100

00:04:12,891  -->  00:04:15,560
and so this is going to be exactly the same.
101

101

00:04:15,560  -->  00:04:17,543
We need to copy this line,
102

102

00:04:18,380  -->  00:04:21,430
paste it below for the second full connection
103

103

00:04:21,430  -->  00:04:22,858
and now what we only have to do
104

104

00:04:22,858  -->  00:04:26,630
is to replace fc1 by fc2
105

105

00:04:26,630  -->  00:04:28,424
because this is the exact same principle,
106

106

00:04:28,424  -->  00:04:32,002
this x here is the vector of the first hidden layer,
107

107

00:04:32,002  -->  00:04:34,700
then on this vector in the first hidden layer,
108

108

00:04:34,700  -->  00:04:36,266
we're making the second full connection
109

109

00:04:36,266  -->  00:04:38,660
which will encode this vector of 20 elements
110

110

00:04:38,660  -->  00:04:41,286
into a shorter vector of 10 elements
111

111

00:04:41,286  -->  00:04:42,540
and at the same time,
112

112

00:04:42,540  -->  00:04:44,110
we apply the sigmoid activation function
113

113

00:04:44,110  -->  00:04:45,785
to activate the neurons
114

114

00:04:45,785  -->  00:04:49,898
and then eventually x becomes this new encoded vector
115

115

00:04:49,898  -->  00:04:53,850
of 10 elements in this second hidden layer, all right.
116

116

00:04:53,850  -->  00:04:56,997
And now let's do the same for the third full connection
117

117

00:04:56,997  -->  00:04:59,580
represented by fc3.
118

118

00:04:59,580  -->  00:05:01,188
So I'm copying this line,
119

119

00:05:01,188  -->  00:05:06,180
pasting it below and replacing fc2 by fc3
120

120

00:05:06,180  -->  00:05:08,150
for the third full connection
121

121

00:05:08,150  -->  00:05:09,481
and that's the exact same principle
122

122

00:05:09,481  -->  00:05:11,610
or almost because this time
123

123

00:05:11,610  -->  00:05:14,310
we're not encoding this input vector x
124

124

00:05:14,310  -->  00:05:16,644
at the left of the third full connection fc3
125

125

00:05:16,644  -->  00:05:20,080
into this new vector x at the right of the full connection.
126

126

00:05:20,080  -->  00:05:21,900
We are now decoding it,
127

127

00:05:21,900  -->  00:05:24,021
because remember the third full connection fc3
128

128

00:05:24,021  -->  00:05:26,924
corresponds to the decoding
129

129

00:05:26,924  -->  00:05:29,703
from an input vector of 10 elements
130

130

00:05:29,703  -->  00:05:32,630
that is the input vector in the second hidden layer
131

131

00:05:32,630  -->  00:05:34,365
to a larger output vector
132

132

00:05:34,365  -->  00:05:36,680
at the right of the full connection
133

133

00:05:36,680  -->  00:05:38,550
that is composed of 20 elements.
134

134

00:05:38,550  -->  00:05:39,830
So that's decoding.
135

135

00:05:39,830  -->  00:05:41,250
The first part of decoding
136

136

00:05:41,250  -->  00:05:45,270
before getting back our reconstructive input vector
137

137

00:05:45,270  -->  00:05:46,930
with the original number of features
138

138

00:05:46,930  -->  00:05:47,970
and the movies.
139

139

00:05:47,970  -->  00:05:50,860
So that's decoding but that's the same principle.
140

140

00:05:50,860  -->  00:05:52,090
And then eventually we're getting
141

141

00:05:52,090  -->  00:05:54,480
to our final output vector
142

142

00:05:54,480  -->  00:05:57,250
that is at the very right of our auto encoders
143

143

00:05:57,250  -->  00:06:00,040
and that's of course our reconstructed output vector.
144

144

00:06:00,040  -->  00:06:02,428
That is the copy of our input vector.
145

145

00:06:02,428  -->  00:06:05,437
And so this is going to be x then equals,
146

146

00:06:05,437  -->  00:06:09,740
then we take our object self again then dot
147

147

00:06:09,740  -->  00:06:12,090
and then try to guess what we have to code next.
148

148

00:06:12,090  -->  00:06:14,140
While we don't apply the activation function
149

149

00:06:14,140  -->  00:06:16,390
because this is the final part of the decoding
150

150

00:06:16,390  -->  00:06:17,490
and therefore we only have
151

151

00:06:17,490  -->  00:06:20,120
to use our fourth full connection, fc4
152

152

00:06:20,120  -->  00:06:21,880
without the activation function.
153

153

00:06:21,880  -->  00:06:24,220
That's the specificity of the auto encoders
154

154

00:06:24,220  -->  00:06:26,960
when reconstructing our decoded output vector,
155

155

00:06:26,960  -->  00:06:29,186
we directly use the fourth full connection fc4.
156

156

00:06:29,186  -->  00:06:31,973
So here we just need to add fc4
157

157

00:06:31,973  -->  00:06:35,490
and then inside we just need to input the vector
158

158

00:06:35,490  -->  00:06:38,315
at the right of this fourth full connection fc4
159

159

00:06:38,315  -->  00:06:40,850
which is of course x.
160

160

00:06:40,850  -->  00:06:41,847
All right, and actually that's done.
161

161

00:06:41,847  -->  00:06:44,410
We did first two encodings
162

162

00:06:44,410  -->  00:06:47,130
of our input vector features and then two decodings
163

163

00:06:47,130  -->  00:06:51,090
to get our reconstructed output vector in the output layer
164

164

00:06:51,090  -->  00:06:52,830
and that's exactly what we would like to return
165

165

00:06:52,830  -->  00:06:55,762
because this is our vector of predicted ratings
166

166

00:06:55,762  -->  00:06:57,340
and this will be the vector
167

167

00:06:57,340  -->  00:06:59,580
that will compare to the real ratings
168

168

00:06:59,580  -->  00:07:01,231
that is actually the input vector of features
169

169

00:07:01,231  -->  00:07:03,410
and after which we will measure the loss
170

170

00:07:03,410  -->  00:07:05,270
so that we can then obtain the weight
171

171

00:07:05,270  -->  00:07:06,903
to eventually reduce this loss.
172

172

00:07:07,760  -->  00:07:10,660
All right so let's add what we need to add now
173

173

00:07:10,660  -->  00:07:13,017
which is this final step of the forward function
174

174

00:07:13,017  -->  00:07:17,150
that is return and x.
175

175

00:07:17,150  -->  00:07:20,970
So x is our vector of predicted ratings.
176

176

00:07:20,970  -->  00:07:23,930
So here we go, the class is actually over now.
177

177

00:07:23,930  -->  00:07:25,760
It was composed of two functions,
178

178

00:07:25,760  -->  00:07:28,410
the Init function that defines the architecture
179

179

00:07:28,410  -->  00:07:30,880
of our auto encoders and the forward function
180

180

00:07:30,880  -->  00:07:33,530
that does the action of establishing
181

181

00:07:33,530  -->  00:07:35,150
these different full connections
182

182

00:07:35,150  -->  00:07:37,870
by applying at the same time the activation functions
183

183

00:07:37,870  -->  00:07:40,334
to activate the right neurons in the network.
184

184

00:07:40,334  -->  00:07:42,170
All right, so now what we're gonna do
185

185

00:07:42,170  -->  00:07:44,560
is create an object of this class
186

186

00:07:44,560  -->  00:07:46,470
which will be our auto encoders.
187

187

00:07:46,470  -->  00:07:47,303
That's how it works.
188

188

00:07:47,303  -->  00:07:49,610
You are probably more comfortable with this
189

189

00:07:49,610  -->  00:07:51,550
because we already created many objects
190

190

00:07:51,550  -->  00:07:53,080
of different classes before
191

191

00:07:53,080  -->  00:07:55,549
so remember we have to use non capital letters
192

192

00:07:55,549  -->  00:07:58,756
for the object and so we're gonna call it SAE as well,
193

193

00:07:58,756  -->  00:08:01,197
first act auto encoders but that's the object
194

194

00:08:01,197  -->  00:08:05,280
and now that's when we use the SAE class
195

195

00:08:05,280  -->  00:08:08,200
with capital letters to create an object
196

196

00:08:08,200  -->  00:08:10,805
from this SAE class that we chose to define here.
197

197

00:08:10,805  -->  00:08:14,570
And since we didn't specify any arguments
198

198

00:08:14,570  -->  00:08:16,770
in the init function of this SAE class
199

199

00:08:16,770  -->  00:08:18,867
while we have the self argument but it is always here,
200

200

00:08:18,867  -->  00:08:21,745
you know it's a default argument just to specify the object.
201

201

00:08:21,745  -->  00:08:25,030
Well, we don't have to input any arguments here
202

202

00:08:25,030  -->  00:08:27,664
so basically our auto encoders is ready
203

203

00:08:27,664  -->  00:08:30,550
because it is already defined by this architecture
204

204

00:08:30,550  -->  00:08:31,840
and we don't have any parameters
205

205

00:08:31,840  -->  00:08:33,770
in this forward function.
206

206

00:08:33,770  -->  00:08:35,480
Okay so that creates the object
207

207

00:08:35,480  -->  00:08:37,210
and now what we're gonna do
208

208

00:08:37,210  -->  00:08:39,592
is define a criterion which we'll need
209

209

00:08:39,592  -->  00:08:41,730
for the training afterwards
210

210

00:08:41,730  -->  00:08:43,750
which will happen in the next tutorial,
211

211

00:08:43,750  -->  00:08:45,447
so it's gonna be another exciting step.
212

212

00:08:45,447  -->  00:08:46,780
I can't wait to get there
213

213

00:08:46,780  -->  00:08:48,700
because that's where the action takes place.
214

214

00:08:48,700  -->  00:08:50,100
So let's create a new variable
215

215

00:08:50,100  -->  00:08:52,189
which we're gonna call criterion
216

216

00:08:52,189  -->  00:08:55,051
and that's basically the criterion for the loss function,
217

217

00:08:55,051  -->  00:08:58,010
and the loss function is going to be the mean squared error.
218

218

00:08:58,010  -->  00:09:02,841
And so here we're gonna use the MSE Loss with capital L
219

219

00:09:02,841  -->  00:09:04,700
and some parenthesis.
220

220

00:09:04,700  -->  00:09:07,230
And so this MSE Loss is a class,
221

221

00:09:07,230  -->  00:09:10,420
so criterion will be an object of this class.
222

222

00:09:10,420  -->  00:09:12,294
Okay, and this will make much more sense to you
223

223

00:09:12,294  -->  00:09:15,920
in the next tutorial when we use this criterion object.
224

224

00:09:15,920  -->  00:09:17,360
You will see that we will use it
225

225

00:09:17,360  -->  00:09:20,120
to measure the mean squared error, all right.
226

226

00:09:20,120  -->  00:09:22,220
And now one last thing we need to do.
227

227

00:09:22,220  -->  00:09:25,650
Actually, one last thing that we need is an optimizer.
228

228

00:09:25,650  -->  00:09:27,560
Of course, exactly like in Keras,
229

229

00:09:27,560  -->  00:09:29,240
we have to end up with an optimizer,
230

230

00:09:29,240  -->  00:09:31,740
that will applies to category in descent
231

231

00:09:31,740  -->  00:09:33,060
to update the different weights
232

232

00:09:33,060  -->  00:09:35,930
in order to reduce the error at each epoch.
233

233

00:09:35,930  -->  00:09:38,483
So that's the last variable we need to define here,
234

234

00:09:38,483  -->  00:09:40,770
so we're going to create a new variable
235

235

00:09:40,770  -->  00:09:42,457
that we're gonna call optimizer,
236

236

00:09:43,977  -->  00:09:48,110
and this will be equal to an object of another class.
237

237

00:09:48,110  -->  00:09:51,087
And actually we have one class for each optimizer,
238

238

00:09:51,087  -->  00:09:54,540
that is we have a class for the Atom optimizer.
239

239

00:09:54,540  -->  00:09:57,750
We also have a class for the RMS prop optimizer,
240

240

00:09:57,750  -->  00:10:00,580
and as I told you I did some experimenting
241

241

00:10:00,580  -->  00:10:01,970
and I actually tried the two,
242

242

00:10:01,970  -->  00:10:03,760
I actually tried the Atom class
243

243

00:10:03,760  -->  00:10:05,380
that is the Atom optimizer
244

244

00:10:05,380  -->  00:10:09,610
and the RMS prop class for the RMS prop optimizer,
245

245

00:10:09,610  -->  00:10:12,250
and I got the best results with RMS prop.
246

246

00:10:12,250  -->  00:10:13,740
So what we're gonna do now,
247

247

00:10:13,740  -->  00:10:17,530
is create an object optimizer from this RMS prop class.
248

248

00:10:17,530  -->  00:10:20,230
But don't worry there is still much room for improvement
249

249

00:10:20,230  -->  00:10:22,730
for you to practice and try to get better results
250

250

00:10:22,730  -->  00:10:24,790
than the results we will get in the end.
251

251

00:10:24,790  -->  00:10:29,173
So, now we have to take the RMS prop class.
252

252

00:10:30,100  -->  00:10:33,660
So parenthesis and actually we're taking these two classes
253

253

00:10:33,660  -->  00:10:35,020
from different modules.
254

254

00:10:35,020  -->  00:10:39,520
MSE Loss class is taken from the NN module,
255

255

00:10:39,520  -->  00:10:43,160
but this RMS prop class is taken from another module
256

256

00:10:43,160  -->  00:10:46,060
that we imported in the first code section here
257

257

00:10:46,060  -->  00:10:47,580
and which is as you might have guessed,
258

258

00:10:47,580  -->  00:10:50,210
the optin module from the Torch library.
259

259

00:10:50,210  -->  00:10:55,210
So actually, RMS prop is taken from the optin module.
260

260

00:10:57,210  -->  00:10:59,780
All right, and that's not all unfortunately.
261

261

00:10:59,780  -->  00:11:03,175
We have to input some arguments in this RMS prop class
262

262

00:11:03,175  -->  00:11:05,650
and we will actually input three things.
263

263

00:11:05,650  -->  00:11:08,920
So first, we have to input all the parameters
264

264

00:11:08,920  -->  00:11:11,970
of our AutoEncoders, that is these parameters here
265

265

00:11:11,970  -->  00:11:14,640
and the Sigmoid activation function.
266

266

00:11:14,640  -->  00:11:18,030
Well, all the parameters that define the architecture
267

267

00:11:18,030  -->  00:11:20,000
of our neural network, and don't worry
268

268

00:11:20,000  -->  00:11:22,630
we don't have to rewrite all that again.
269

269

00:11:22,630  -->  00:11:25,321
We have an attribute from our SAE object
270

270

00:11:25,321  -->  00:11:27,810
which will get us all these parameters.
271

271

00:11:27,810  -->  00:11:30,150
So to get the attribute we need to get first the object
272

272

00:11:30,150  -->  00:11:34,657
which is SAE then dot, and this attribute is parameters.
273

273

00:11:34,657  -->  00:11:37,070
And we add some parenthesis.
274

274

00:11:37,070  -->  00:11:40,230
So good, we have all the parameters of our AutoEncoders.
275

275

00:11:40,230  -->  00:11:42,070
And now second input.
276

276

00:11:42,070  -->  00:11:43,450
The second input is going to be
277

277

00:11:43,450  -->  00:11:45,522
a learning rate that it's better to specify,
278

278

00:11:45,522  -->  00:11:50,044
and so the name of the parameter for that is LR equals,
279

279

00:11:50,044  -->  00:11:52,730
and again I did some experimenting
280

280

00:11:52,730  -->  00:11:56,790
and a good value I found was 0.01, all right,
281

281

00:11:56,790  -->  00:11:59,325
but again you're welcome to try some other values.
282

282

00:11:59,325  -->  00:12:02,240
And then final input which is the decay.
283

283

00:12:02,240  -->  00:12:04,810
So as a reminder, the decay is used
284

284

00:12:04,810  -->  00:12:07,900
to reduce the learning rate after every few epochs
285

285

00:12:07,900  -->  00:12:10,490
and that's in order to regulate the convergence.
286

286

00:12:10,490  -->  00:12:13,293
So that's a parameter that can improve your model even more
287

287

00:12:13,293  -->  00:12:15,650
and so we'll add a value for this parameter,
288

288

00:12:15,650  -->  00:12:17,587
and we'll call it weight_decay,
289

289

00:12:19,430  -->  00:12:21,110
and based on my experimenting
290

290

00:12:21,110  -->  00:12:25,046
a good value I found was 0.5.
291

291

00:12:25,046  -->  00:12:27,330
And here we go, that's actually
292

292

00:12:27,330  -->  00:12:30,257
the end of the architecture of our neural networks.
293

293

00:12:30,257  -->  00:12:32,032
Congratulations for following.
294

294

00:12:32,032  -->  00:12:34,788
We actually took some kind of a more advanced level here,
295

295

00:12:34,788  -->  00:12:37,758
but that's part of the job of a de-per-ing scientist,
296

296

00:12:37,758  -->  00:12:41,690
to experiment, do research, and make very robust model,
297

297

00:12:41,690  -->  00:12:43,120
and you will see that with this one.
298

298

00:12:43,120  -->  00:12:44,585
We will get great results.
299

299

00:12:44,585  -->  00:12:46,750
Okay, so now that we're done,
300

300

00:12:46,750  -->  00:12:49,550
we're going to select all this section
301

301

00:12:49,550  -->  00:12:52,170
to define our class SAE
302

302

00:12:52,170  -->  00:12:54,590
and create this object of this class
303

303

00:12:54,590  -->  00:12:58,400
and besides we're defining a criterion and an optimizer.
304

304

00:12:58,400  -->  00:13:00,810
So let's do this, let's press command and control
305

305

00:13:00,810  -->  00:13:02,950
plus enter to execute.
306

306

00:13:02,950  -->  00:13:05,560
And here we go, the class is now created.
307

307

00:13:05,560  -->  00:13:08,240
So basically that created our AutoEncoder
308

308

00:13:08,240  -->  00:13:10,070
and remember that the class,
309

309

00:13:10,070  -->  00:13:12,670
or the instructions to build the AutoEncoder,
310

310

00:13:12,670  -->  00:13:16,540
and this line of code creates the AutoEncoder
311

311

00:13:16,540  -->  00:13:19,410
because it creates an object of this class.
312

312

00:13:19,410  -->  00:13:20,882
So, that's it for this tutorial
313

313

00:13:20,882  -->  00:13:23,141
and in the next of tutorial we're gonna make the code
314

314

00:13:23,141  -->  00:13:24,600
to do the training,
315

315

00:13:24,600  -->  00:13:26,820
and that will get us closer to the exciting phase
316

316

00:13:26,820  -->  00:13:28,597
where we see our model learning,
317

317

00:13:28,597  -->  00:13:31,410
and of course eventually the final results
318

318

00:13:31,410  -->  00:13:33,560
with the test set performance.
319

319

00:13:33,560  -->  00:13:35,460
So we'll see all this in the next tutorials
320

320

00:13:35,460  -->  00:13:37,543
and until then enjoy the learning.
