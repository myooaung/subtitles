WEBVTT
1
1

00:00:01.720  -->  00:00:02.710
<v Narrator>Hello and welcome back</v>
2

2

00:00:02.710  -->  00:00:04.100
to the course on deep learning.
3

3

00:00:04.100  -->  00:00:07.250
Today we're going to wrap up with backpropagation.
4

4

00:00:07.250  -->  00:00:09.660
Alright, so we already know pretty much everything
5

5

00:00:09.660  -->  00:00:12.000
we need to know about what happens in the neural network.
6

6

00:00:12.000  -->  00:00:15.110
We know that there's a process called forward propagation,
7

7

00:00:15.110  -->  00:00:18.570
where information is entered into the input layer
8

8

00:00:18.570  -->  00:00:22.119
and then is propagated forward to get our y-hats,
9

9

00:00:22.119  -->  00:00:23.560
our output values.
10

10

00:00:23.560  -->  00:00:26.990
And then we compare those to the actual values
11

11

00:00:26.990  -->  00:00:29.100
that we have in our training set
12

12

00:00:29.100  -->  00:00:31.910
and then we calculate the errors.
13

13

00:00:31.910  -->  00:00:35.460
Then the errors are back-propagated through the network
14

14

00:00:35.460  -->  00:00:38.970
in the opposite direction and that allows us
15

15

00:00:38.970  -->  00:00:41.550
to train the network by adjusting the weights.
16

16

00:00:41.550  -->  00:00:45.010
So, the one key important thing to remember here
17

17

00:00:45.010  -->  00:00:49.270
is that backpropagation is an advanced algorithm
18

18

00:00:49.270  -->  00:00:54.270
driven by very interesting and sophisticated mathematics
19

19

00:00:55.470  -->  00:00:58.930
which allows us to adjust the weights,
20

20

00:00:58.930  -->  00:01:00.200
all of them at the same time,
21

21

00:01:00.200  -->  00:01:02.410
all of the weights are adjusted simultaneously.
22

22

00:01:02.410  -->  00:01:05.610
So, if we were doing this manually,
23

23

00:01:05.610  -->  00:01:08.420
or if we were coming up with a different type of algorithm,
24

24

00:01:08.420  -->  00:01:10.320
then even if we calculated the error
25

25

00:01:10.320  -->  00:01:12.230
and then we were trying to understand
26

26

00:01:12.230  -->  00:01:14.800
what effect each of the weights has on the error,
27

27

00:01:14.800  -->  00:01:18.560
we'd have to somehow adjust each of the weights
28

28

00:01:18.560  -->  00:01:20.853
independently or individually.
29

29

00:01:21.900  -->  00:01:24.330
The huge advantage of backpropagation,
30

30

00:01:24.330  -->  00:01:26.480
and this is a key thing to remember,
31

31

00:01:26.480  -->  00:01:30.190
is that during the process of backpropagation,
32

32

00:01:30.190  -->  00:01:35.190
simply because of the way the algorithm is structured,
33

33

00:01:36.750  -->  00:01:40.500
you are able to adjust all of the weights at the same time.
34

34

00:01:40.500  -->  00:01:43.600
So you basically know which part of the error
35

35

00:01:43.600  -->  00:01:45.810
each of your weights in the neural network
36

36

00:01:45.810  -->  00:01:47.320
is responsible for.
37

37

00:01:47.320  -->  00:01:52.320
Now, that is the key fundamental, underlying principle
38

38

00:01:52.830  -->  00:01:57.830
of backpropagation and this was why it picked up
39

39

00:01:58.140  -->  00:02:02.680
so rapidly in the 1980's, and this was a major breakthrough.
40

40

00:02:02.680  -->  00:02:04.640
And if you'd like to learn more about that
41

41

00:02:04.640  -->  00:02:08.490
and how exactly the mathematics works in the background,
42

42

00:02:08.490  -->  00:02:11.360
then a good article which we've already mentioned
43

43

00:02:11.360  -->  00:02:14.380
is the Neural Networks and Deep Learning.
44

44

00:02:14.380  -->  00:02:16.440
It's actually a book by Michael Nielsen.
45

45

00:02:16.440  -->  00:02:19.320
There you'll find the mathematics written out
46

46

00:02:19.320  -->  00:02:23.590
and it'll help you understand how exactly this is possible.
47

47

00:02:23.590  -->  00:02:26.124
But for now, for our purposes,
48

48

00:02:26.124  -->  00:02:28.150
from an intuition point of view,
49

49

00:02:28.150  -->  00:02:30.730
the important part is to remember that
50

50

00:02:30.730  -->  00:02:33.220
that's what backpropagation does,
51

51

00:02:33.220  -->  00:02:36.840
it adjusts all of the weights at the same time.
52

52

00:02:36.840  -->  00:02:39.160
And now we're going to just wrap everything up
53

53

00:02:39.160  -->  00:02:40.940
with a step by step walkthrough
54

54

00:02:40.940  -->  00:02:45.280
of what happens in the training of a neural network.
55

55

00:02:45.280  -->  00:02:48.260
Alright, so step one, we randomly initialize the weights
56

56

00:02:48.260  -->  00:02:50.870
to small numbers close to zero but not zero.
57

57

00:02:50.870  -->  00:02:53.680
We didn't really focus on the initialization of weights
58

58

00:02:53.680  -->  00:02:55.140
during the intuition tutorials,
59

59

00:02:55.140  -->  00:02:58.210
but the weights have to start somewhere
60

60

00:02:58.210  -->  00:03:02.540
and they are initialized with random values near zero
61

61

00:03:02.540  -->  00:03:05.630
and from there, through the process of forward propagation
62

62

00:03:05.630  -->  00:03:07.940
backpropagation these weights are adjusted
63

63

00:03:09.020  -->  00:03:11.850
until the error is minimized.
64

64

00:03:11.850  -->  00:03:13.640
Til the cost function is minimized.
65

65

00:03:13.640  -->  00:03:17.610
Then, step two, input the first observation of your dataset
66

66

00:03:17.610  -->  00:03:19.370
for the first row into the input layer,
67

67

00:03:19.370  -->  00:03:21.410
each feature is one input node.
68

68

00:03:21.410  -->  00:03:22.620
So basically take the columns
69

69

00:03:22.620  -->  00:03:25.670
and put them into the input nodes.
70

70

00:03:25.670  -->  00:03:27.810
Step three, forward propagation, from left to right,
71

71

00:03:27.810  -->  00:03:29.606
the neurons are activated in a way that
72

72

00:03:29.606  -->  00:03:31.830
the impact of each neuron's activation
73

73

00:03:31.830  -->  00:03:32.810
is limited by the weights.
74

74

00:03:32.810  -->  00:03:35.190
So the weights basically determine
75

75

00:03:35.190  -->  00:03:38.080
how important each neuron's activation is.
76

76

00:03:38.080  -->  00:03:39.770
Then propagate the activations
77

77

00:03:39.770  -->  00:03:43.870
until getting the predicted result, y-hat in this case.
78

78

00:03:43.870  -->  00:03:46.600
So, basically you propagate from left to right.
79

79

00:03:46.600  -->  00:03:48.850
You go all the way until you get to the end
80

80

00:03:48.850  -->  00:03:50.210
and you get your y-hat.
81

81

00:03:50.210  -->  00:03:52.650
Then compare the predicted result to the actual result.
82

82

00:03:52.650  -->  00:03:55.340
Measure the generated error.
83

83

00:03:55.340  -->  00:03:57.410
Then you do the backpropagation from right to left,
84

84

00:03:57.410  -->  00:03:58.540
the error is back-propagated.
85

85

00:03:58.540  -->  00:04:00.290
Update the weights according to how much
86

86

00:04:00.290  -->  00:04:02.140
they are responsible for the error.
87

87

00:04:02.140  -->  00:04:05.030
Again you are able to calculate that because of
88

88

00:04:05.030  -->  00:04:09.390
the way that backpropagation algorithm is structured.
89

89

00:04:09.390  -->  00:04:12.550
The learning rate decides by how much we update the weights.
90

90

00:04:12.550  -->  00:04:14.680
Learning rate is parameter you can control
91

91

00:04:14.680  -->  00:04:17.650
in your neural network.
92

92

00:04:17.650  -->  00:04:19.940
Step six, repeat steps one to five
93

93

00:04:19.940  -->  00:04:22.874
and update the weights after each observation.
94

94

00:04:22.874  -->  00:04:25.950
That is called Reinforcement Learning, and in our case,
95

95

00:04:25.950  -->  00:04:29.460
that was Stochastic Gradient Descent.
96

96

00:04:29.460  -->  00:04:32.130
Or repeat steps one to five but update weights
97

97

00:04:32.130  -->  00:04:35.250
only after a batch of observations, so Batch Learning.
98

98

00:04:35.250  -->  00:04:38.720
It's either Full Gradient Descent or Batch Gradient Descent
99

99

00:04:38.720  -->  00:04:40.870
or Mini Batch Gradient Descent.
100

100

00:04:40.870  -->  00:04:42.760
And step seven, when the whole training set
101

101

00:04:42.760  -->  00:04:45.313
passed through the Artificial Neural Network,
102

102

00:04:45.313  -->  00:04:48.870
that makes an epoch, redo more epochs.
103

103

00:04:48.870  -->  00:04:50.950
So basically, you just keep doing that and doing that
104

104

00:04:50.950  -->  00:04:55.830
and doing that, allow your neural network to train better
105

105

00:04:55.830  -->  00:04:58.440
and better and better and constantly adjust itself
106

106

00:04:59.517  -->  00:05:02.630
as you minimize the cost function.
107

107

00:05:02.630  -->  00:05:05.798
So, there we go, those are the steps you need to take
108

108

00:05:05.798  -->  00:05:09.920
to build your artificial neural networks and train it.
109

109

00:05:09.920  -->  00:05:13.540
And these are the steps that you will be taking together
110

110

00:05:13.540  -->  00:05:16.000
with Hadelin in the practical tutorials.
111

111

00:05:16.000  -->  00:05:17.080
Wish you the best of luck,
112

112

00:05:17.080  -->  00:05:19.410
and I look forward to seeing you next time.
113

113

00:05:19.410  -->  00:05:21.363
Until then, enjoy Deep Learning.
