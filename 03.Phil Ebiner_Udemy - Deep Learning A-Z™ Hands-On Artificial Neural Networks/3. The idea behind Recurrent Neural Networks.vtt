WEBVTT
1
1

00:00:00.260  -->  00:00:01.200
<v Instructor>Hello, and welcome back</v>
2

2

00:00:01.200  -->  00:00:02.820
to the course on deep learning.
3

3

00:00:02.820  -->  00:00:03.760
Today, we are kicking off
4

4

00:00:03.760  -->  00:00:06.700
this section for recurrent neural networks
5

5

00:00:06.700  -->  00:00:09.640
and I'm very excited about this section.
6

6

00:00:09.640  -->  00:00:12.650
There's going to be quite some interesting tutorials.
7

7

00:00:12.650  -->  00:00:16.580
This is one of the most advanced algorithms
8

8

00:00:16.580  -->  00:00:19.310
that exists in the world of supervised deep learning,
9

9

00:00:19.310  -->  00:00:20.483
so let's get started.
10

10

00:00:21.640  -->  00:00:24.610
We have our little break down of supervised
11

11

00:00:24.610  -->  00:00:27.860
versus unsupervised deep learning branches,
12

12

00:00:27.860  -->  00:00:30.890
and here we've got artificial neural networks
13

13

00:00:30.890  -->  00:00:32.130
which we've already talked about,
14

14

00:00:32.130  -->  00:00:34.890
we've already talked about convolutional neural networks
15

15

00:00:34.890  -->  00:00:37.380
as well, and today we're talking
16

16

00:00:37.380  -->  00:00:38.720
about recurrent neural networks.
17

17

00:00:38.720  -->  00:00:41.170
So, this is just so that we see where we are
18

18

00:00:41.170  -->  00:00:43.580
in the big picture of things slowly getting
19

19

00:00:43.580  -->  00:00:46.300
to the unsupervised part of the course.
20

20

00:00:46.300  -->  00:00:49.710
But nevertheless, let's focus on RNNs today.
21

21

00:00:49.710  -->  00:00:51.540
Alright, so now that we know where we are
22

22

00:00:51.540  -->  00:00:54.130
on the map in terms of a list, let's have a look
23

23

00:00:54.130  -->  00:00:57.220
where we are on the map in terms of the human brain.
24

24

00:00:57.220  -->  00:00:58.610
And so, why are we doing this?
25

25

00:00:58.610  -->  00:01:00.247
Why are we looking at the human brain
26

26

00:01:00.247  -->  00:01:02.280
as if it's a map of the world?
27

27

00:01:02.280  -->  00:01:05.320
Well, the reason for that is the whole concept
28

28

00:01:05.320  -->  00:01:09.200
behind deep learning is to try and mimic the human brain
29

29

00:01:09.200  -->  00:01:14.200
and get kind of similar functions as the human brain has
30

30

00:01:15.380  -->  00:01:17.160
and leverage the things that evolution
31

31

00:01:17.160  -->  00:01:18.810
has already developed for us.
32

32

00:01:18.810  -->  00:01:20.160
And I thought it would be pretty cool
33

33

00:01:20.160  -->  00:01:24.782
if we could somehow link the different branches
34

34

00:01:24.782  -->  00:01:27.050
of deep learning that we've discussed,
35

35

00:01:27.050  -->  00:01:29.570
or the algorithms that we discussed.
36

36

00:01:29.570  -->  00:01:32.120
We talked about ANN, CNN, and now we're talking about RNN.
37

37

00:01:32.120  -->  00:01:34.440
If we could link those to some parts
38

38

00:01:34.440  -->  00:01:37.820
of the human brain and if it all makes sense.
39

39

00:01:37.820  -->  00:01:39.490
So, let's have a look.
40

40

00:01:39.490  -->  00:01:41.640
Here we've got the brain, the human brain,
41

41

00:01:41.640  -->  00:01:42.473
it's got three parts.
42

42

00:01:42.473  -->  00:01:44.420
So, we've got the cerebrum
43

43

00:01:44.420  -->  00:01:46.020
which is all of this colored part.
44

44

00:01:46.020  -->  00:01:47.280
And then we've got the cerebellum
45

45

00:01:47.280  -->  00:01:49.790
which is underneath there and that's the little brain.
46

46

00:01:49.790  -->  00:01:51.010
I actually looked it up in Latin,
47

47

00:01:51.010  -->  00:01:54.000
that does mean little brain, how funny is that?
48

48

00:01:54.000  -->  00:01:58.500
And we've already looked at a dissection of the cerebellum
49

49

00:01:58.500  -->  00:02:00.210
in the part where we're talking about ANNs,
50

50

00:02:00.210  -->  00:02:03.881
that big orange picture where we saw all
51

51

00:02:03.881  -->  00:02:06.820
of those little neurons everywhere trying
52

52

00:02:06.820  -->  00:02:09.380
to kind of gauge how many there are there.
53

53

00:02:09.380  -->  00:02:12.530
There's millions of neurons in the brain.
54

54

00:02:12.530  -->  00:02:15.060
And then we've got the brain stem over here
55

55

00:02:15.060  -->  00:02:18.020
which connects the brain to the organs
56

56

00:02:18.020  -->  00:02:21.293
and our arms and legs and so on.
57

57

00:02:22.270  -->  00:02:26.830
And so those are the main three parts of the brain.
58

58

00:02:26.830  -->  00:02:30.110
Now, the cerebrum has four lobes
59

59

00:02:30.110  -->  00:02:31.863
and they're colored in here.
60

60

00:02:31.863  -->  00:02:35.040
So, it's got the frontal lobe, it's got the temporal lobe,
61

61

00:02:35.040  -->  00:02:38.582
it's got the parietal lobe, and it's got the occipital lobe.
62

62

00:02:38.582  -->  00:02:41.690
Now, how do we link these, right?
63

63

00:02:41.690  -->  00:02:46.120
So, we've got ANN, we've already discussed CNN, and RNN.
64

64

00:02:46.120  -->  00:02:50.642
And the hardest one was probably to start off with ANN
65

65

00:02:50.642  -->  00:02:54.210
because what is the main advantage of ANN?
66

66

00:02:54.210  -->  00:02:57.600
Well, the main advantage, the main breakthrough in ANNs is,
67

67

00:02:57.600  -->  00:02:59.700
apart from the back propagation algorithm
68

68

00:02:59.700  -->  00:03:01.440
which kind of applies to all of them
69

69

00:03:01.440  -->  00:03:04.370
and in fact whatever applies to an ANN
70

70

00:03:04.370  -->  00:03:05.700
applies to everything here.
71

71

00:03:05.700  -->  00:03:10.000
But for me, I think the main thing about ANNs
72

72

00:03:10.000  -->  00:03:15.000
and it wouldn't even exist without this whole concept
73

73

00:03:15.350  -->  00:03:18.180
of deep learning wouldn't exist, are the weights.
74

74

00:03:18.180  -->  00:03:23.180
The fact that ANNs can learn through prior experience,
75

75

00:03:24.070  -->  00:03:25.760
or through prior impulse,
76

76

00:03:25.760  -->  00:03:29.370
and through prior observations that's extremely valuable.
77

77

00:03:29.370  -->  00:03:32.700
And so, what do those weights represent?
78

78

00:03:32.700  -->  00:03:34.590
And moreover, the weights of course are present
79

79

00:03:34.590  -->  00:03:35.930
across all neurons in the brain,
80

80

00:03:35.930  -->  00:03:38.620
but we're going to try to take away
81

81

00:03:38.620  -->  00:03:41.340
the main philosophical underlying notion there
82

82

00:03:41.340  -->  00:03:44.160
and that is that weights represent long term memory.
83

83

00:03:44.160  -->  00:03:47.210
That once you've run your ANN and you've trained it,
84

84

00:03:47.210  -->  00:03:49.300
you can switch it off, you can come back later.
85

85

00:03:49.300  -->  00:03:50.970
It's trained up, you know the weights,
86

86

00:03:50.970  -->  00:03:53.760
and so whatever input you put into it
87

87

00:03:53.760  -->  00:03:56.890
it will process it the same way as it would yesterday,
88

88

00:03:56.890  -->  00:03:59.240
as it will tomorrow, as it will the day after.
89

89

00:03:59.240  -->  00:04:02.470
So, the weights are long term memory of a neural network.
90

90

00:04:02.470  -->  00:04:06.450
And that's why weights, or the ANN, go into temporal lobe.
91

91

00:04:06.450  -->  00:04:08.610
Again, the weights exist across the whole brain
92

92

00:04:08.610  -->  00:04:13.160
but philosophically, ANNs are a start to deep learning
93

93

00:04:13.160  -->  00:04:15.030
and they represent long term memory.
94

94

00:04:15.030  -->  00:04:16.440
So, we've to put them in the temporal lobe
95

95

00:04:16.440  -->  00:04:19.430
because the temporal lobe is responsible
96

96

00:04:19.430  -->  00:04:22.110
for long term memory.
97

97

00:04:22.110  -->  00:04:23.500
Hence, it's called a temporal lobe
98

98

00:04:23.500  -->  00:04:26.473
meaning things last through time in there.
99

99

00:04:27.319  -->  00:04:29.130
The brain is very complex and of course other parts
100

100

00:04:29.130  -->  00:04:31.190
are also responsible for memory as well
101

101

00:04:31.190  -->  00:04:33.080
but we're going to simplify things
102

102

00:04:33.080  -->  00:04:35.690
and say ANN is like the temporal lobe.
103

103

00:04:35.690  -->  00:04:38.210
Then, CNN is much easier, it's vision,
104

104

00:04:38.210  -->  00:04:41.480
recognition of images and objects and so on,
105

105

00:04:41.480  -->  00:04:43.330
so that's the occipital lobe.
106

106

00:04:43.330  -->  00:04:45.490
And today, we're talking about RNNs
107

107

00:04:45.490  -->  00:04:49.560
and as you'll find out RNNs are like short term memory.
108

108

00:04:49.560  -->  00:04:52.600
They can remember things that just happened
109

109

00:04:52.600  -->  00:04:54.450
in the previous couple of observations
110

110

00:04:54.450  -->  00:04:56.570
and apply that knowledge going forward.
111

111

00:04:56.570  -->  00:04:58.466
I'm giving away so much already,
112

112

00:04:58.466  -->  00:05:02.040
you pretty much know the rest of this tutorial,
113

113

00:05:02.040  -->  00:05:03.690
but nevertheless.
114

114

00:05:03.690  -->  00:05:05.090
So, that's the frontal lobe.
115

115

00:05:06.008  -->  00:05:07.010
That's where have a lot of the short term memory
116

116

00:05:07.010  -->  00:05:11.310
and of course the frontal, like a quick break down.
117

117

00:05:11.310  -->  00:05:14.740
The frontal lobe also is responsible for personality,
118

118

00:05:14.740  -->  00:05:17.470
behavior, motor cortex, working memory,
119

119

00:05:17.470  -->  00:05:19.140
and lots of other things.
120

120

00:05:19.140  -->  00:05:20.680
But with our purposes the main thing
121

121

00:05:20.680  -->  00:05:24.440
that we're looking out for is the short memory.
122

122

00:05:24.440  -->  00:05:26.108
By the way, if you're interested,
123

123

00:05:26.108  -->  00:05:31.108
temporal lobe is concerned with recognition memory
124

124

00:05:31.310  -->  00:05:33.130
that's our long term memory.
125

125

00:05:33.130  -->  00:05:36.880
Parietal lobe, and these are from Wikipedia,
126

126

00:05:36.880  -->  00:05:39.230
the parietal lobe is responsible for sensation
127

127

00:05:39.230  -->  00:05:42.520
and perception, and constructing a spacial
128

128

00:05:42.520  -->  00:05:46.130
coordination system to represent the world around us.
129

129

00:05:46.130  -->  00:05:49.240
And we are yet to create a neural network
130

130

00:05:49.240  -->  00:05:50.910
which would fit into that category.
131

131

00:05:50.910  -->  00:05:52.940
And occipital is vision.
132

132

00:05:52.940  -->  00:05:55.150
Alright, so there we got a bit of neuroscience
133

133

00:05:55.150  -->  00:06:00.000
so let's move on to our favorite neural network.
134

134

00:06:00.000  -->  00:06:02.580
So here, we've got a simple artificial neural network.
135

135

00:06:02.580  -->  00:06:05.050
Three inputs, two outputs, one hidden layer.
136

136

00:06:05.050  -->  00:06:08.013
What does an RNN do?
137

137

00:06:08.013  -->  00:06:11.660
How do we represent or turn this into an RNN?
138

138

00:06:11.660  -->  00:06:13.330
Well, we squash it.
139

139

00:06:13.330  -->  00:06:14.840
We squash it all together.
140

140

00:06:14.840  -->  00:06:17.480
So, they're still there but think of it
141

141

00:06:17.480  -->  00:06:21.727
as if we're looking from underneath this neural network
142

142

00:06:24.180  -->  00:06:26.070
so we're looking in an new dimension.
143

143

00:06:26.070  -->  00:06:29.640
So, it's still there it's just flattened out.
144

144

00:06:29.640  -->  00:06:32.470
We're adding a new dimension to all of this,
145

145

00:06:32.470  -->  00:06:34.410
but remember that those neurons,
146

146

00:06:34.410  -->  00:06:35.610
the whole network, is still there.
147

147

00:06:35.610  -->  00:06:37.990
Nothing changed, we just squashed it for our purposes.
148

148

00:06:37.990  -->  00:06:40.010
Then to simplify things we're just going
149

149

00:06:40.010  -->  00:06:42.330
to change these multipliers into two,
150

150

00:06:42.330  -->  00:06:44.000
then we're gonna twist thing whole thing,
151

151

00:06:44.000  -->  00:06:46.900
make it vertical because that's the standard representation.
152

152

00:06:46.900  -->  00:06:49.150
And then in terms of neural metrics we're gonna color them,
153

153

00:06:49.150  -->  00:06:50.530
instead of green we're gonna color
154

154

00:06:50.530  -->  00:06:53.300
the hidden layers in blue, and there you go.
155

155

00:06:53.300  -->  00:06:56.460
And we're gonna add this line, and what does that line do?
156

156

00:06:56.460  -->  00:07:00.880
Well, that line is the temporal loop.
157

157

00:07:00.880  -->  00:07:03.960
And this is an old school representation of RNNs
158

158

00:07:03.960  -->  00:07:07.760
and basically means that this hidden layer
159

159

00:07:07.760  -->  00:07:10.990
not only gives an output but also feeds back into itself.
160

160

00:07:10.990  -->  00:07:12.860
So again, this is an old school representation
161

161

00:07:12.860  -->  00:07:17.860
so the common kind of approach is now to unwind,
162

162

00:07:18.830  -->  00:07:22.480
or unroll, this temporal loop and represent ANNs
163

163

00:07:22.480  -->  00:07:24.970
in the following manner, like that.
164

164

00:07:24.970  -->  00:07:27.880
So, you can see that, again don't forget
165

165

00:07:27.880  -->  00:07:31.360
that we've lots of these things happening
166

166

00:07:31.360  -->  00:07:33.160
so you're looking in a new dimension
167

167

00:07:33.160  -->  00:07:35.600
that the layers are actually still there.
168

168

00:07:35.600  -->  00:07:38.820
They're still there, but we're just not focusing on them.
169

169

00:07:38.820  -->  00:07:41.320
We just remember that each one of these circles
170

170

00:07:41.320  -->  00:07:44.510
is not one neuron, it's a whole layer of neurons.
171

171

00:07:44.510  -->  00:07:47.870
And so, what is happening is you've inputs coming
172

172

00:07:47.870  -->  00:07:49.530
into the neurons, then you got outputs,
173

173

00:07:49.530  -->  00:07:51.860
but also the neurons are connecting
174

174

00:07:51.860  -->  00:07:54.020
to themselves through time.
175

175

00:07:54.020  -->  00:07:57.740
So, that's the whole concept that they some sort of memory,
176

176

00:07:57.740  -->  00:08:00.160
short term memory, that they remember what was
177

177

00:08:00.160  -->  00:08:01.843
in that neuron just previously.
178

178

00:08:02.890  -->  00:08:05.860
And then before that it just remembers
179

179

00:08:05.860  -->  00:08:07.680
what it was previously, and that allows them
180

180

00:08:07.680  -->  00:08:10.190
to pass information on to themselves
181

181

00:08:10.190  -->  00:08:13.564
in the future and analyze things.
182

182

00:08:13.564  -->  00:08:17.800
Kind of like when you're watching this course, right?
183

183

00:08:17.800  -->  00:08:21.810
It would be very sad if you could not remember
184

184

00:08:21.810  -->  00:08:24.310
what was happening in the previous tutorial, right?
185

185

00:08:24.310  -->  00:08:27.440
Even if we break time down discreetly through,
186

186

00:08:27.440  -->  00:08:30.210
not by seconds but continuously by seconds
187

187

00:08:30.210  -->  00:08:34.630
by discreetly through tutorials and we say like every moment
188

188

00:08:34.630  -->  00:08:36.670
in time is a new tutorial, it would be very sad
189

189

00:08:36.670  -->  00:08:38.330
if you did not remember what we had
190

190

00:08:38.330  -->  00:08:40.760
in the previous tutorial, or in the previous section,
191

191

00:08:40.760  -->  00:08:42.850
or in the previous part of the course.
192

192

00:08:42.850  -->  00:08:44.810
Because then, this whole neural networks
193

193

00:08:44.810  -->  00:08:46.570
part wouldn't make any sense.
194

194

00:08:46.570  -->  00:08:48.540
You wouldn't have memory of what a neuron is,
195

195

00:08:48.540  -->  00:08:50.640
what an activation function is,
196

196

00:08:50.640  -->  00:08:51.770
but because you do remember
197

197

00:08:51.770  -->  00:08:53.500
those things you can understand this.
198

198

00:08:53.500  -->  00:08:57.017
And same thing here, so why would we deprive
199

199

00:08:57.017  -->  00:09:00.240
an artificial construct which is supposed
200

200

00:09:00.240  -->  00:09:04.640
to be a synthetic human brain, or mimicking the human brain,
201

201

00:09:04.640  -->  00:09:06.760
why would we deprive it of something
202

202

00:09:06.760  -->  00:09:08.960
so powerful as short term memory.
203

203

00:09:08.960  -->  00:09:11.410
Long term memory's great, but short term memory is
204

204

00:09:11.410  -->  00:09:14.150
so powerful why would we not give it that opportunity?
205

205

00:09:14.150  -->  00:09:16.390
And that's where recurrent neural networks come in,
206

206

00:09:16.390  -->  00:09:18.113
that's the gap that they fill in.
207

207

00:09:19.040  -->  00:09:21.453
And so, let's have a look at a couple of examples.
208

208

00:09:22.460  -->  00:09:26.930
A huge shout to the Karpathy blog, karpathy.github.io,
209

209

00:09:26.930  -->  00:09:28.540
some of these examples are from here.
210

210

00:09:28.540  -->  00:09:31.080
So, one to many relationships,
211

211

00:09:31.080  -->  00:09:33.380
this is when you have one input and have multiple outputs.
212

212

00:09:33.380  -->  00:09:36.076
An example of this is an image
213

213

00:09:36.076  -->  00:09:39.580
where a computer describes the image.
214

214

00:09:39.580  -->  00:09:40.960
So, you have one input, the image,
215

215

00:09:40.960  -->  00:09:42.200
and that would go through a CNN
216

216

00:09:42.200  -->  00:09:44.880
and then it would be fed into an RNN,
217

217

00:09:44.880  -->  00:09:46.870
and then the computer would come up
218

218

00:09:46.870  -->  00:09:48.260
with words to describe the image.
219

219

00:09:48.260  -->  00:09:51.770
And this is an actual computer describing the image,
220

220

00:09:51.770  -->  00:09:52.760
how accurate is that?
221

221

00:09:52.760  -->  00:09:54.740
Black and white dog jumps over bar.
222

222

00:09:54.740  -->  00:09:56.270
This is a computer that looked at this image
223

223

00:09:56.270  -->  00:09:58.270
and it was like, oh, it's a black and white dog.
224

224

00:09:58.270  -->  00:10:00.620
Based on what it's previously learned,
225

225

00:10:00.620  -->  00:10:04.490
the long term memory, it allowed it to come up with weights,
226

226

00:10:04.490  -->  00:10:09.490
and come up with certain feature recognition system,
227

227

00:10:11.149  -->  00:10:13.110
and come up with the filters,
228

228

00:10:13.110  -->  00:10:15.570
come up with everything that is required in a CNN.
229

229

00:10:15.570  -->  00:10:17.130
And then the RNN allows it
230

230

00:10:17.130  -->  00:10:19.110
to make sense out of the sentence.
231

231

00:10:19.110  -->  00:10:21.687
So, you can see that the sentence actually flows.
232

232

00:10:21.687  -->  00:10:26.687
There's an and, there's an over the bar,
233

233

00:10:27.120  -->  00:10:30.120
and then there's like a verb, there's a noun, and so on.
234

234

00:10:30.120  -->  00:10:34.170
So, basically the RNN is what allows it
235

235

00:10:34.170  -->  00:10:36.450
to put a sentence together in this case.
236

236

00:10:36.450  -->  00:10:40.580
Then a many to one, an example would be sentiment analysis.
237

237

00:10:40.580  -->  00:10:43.470
So, when you have a lot of text
238

238

00:10:43.470  -->  00:10:46.569
and from that text you kind of need to gauge is
239

239

00:10:46.569  -->  00:10:51.200
this a positive comment or is it a negative comment?
240

240

00:10:51.200  -->  00:10:52.650
What's the chance that it's a positive,
241

241

00:10:52.650  -->  00:10:55.200
or how positive or how negative is that comment?
242

242

00:10:55.200  -->  00:10:57.970
And you've got an example here as well.
243

243

00:10:57.970  -->  00:10:59.660
Again, there's lots of other different examples,
244

244

00:10:59.660  -->  00:11:00.660
these are just some.
245

245

00:11:01.500  -->  00:11:03.740
Then we're many to many, for instance,
246

246

00:11:03.740  -->  00:11:06.420
oh yeah I wanted to show you this one.
247

247

00:11:06.420  -->  00:11:09.370
So here, we've got an example of Google Translator.
248

248

00:11:09.370  -->  00:11:13.010
And I don't know if Google Translator uses RNNs or not,
249

249

00:11:13.010  -->  00:11:16.200
I know that they have very sophisticated deep learning
250

250

00:11:16.200  -->  00:11:19.020
Google mind and I've heard that they've used that
251

251

00:11:19.020  -->  00:11:21.490
in their Android systems and so on.
252

252

00:11:21.490  -->  00:11:23.200
I'm not sure if they use RNNs here or not,
253

253

00:11:23.200  -->  00:11:25.000
but the concept remains.
254

254

00:11:25.000  -->  00:11:28.370
So, if I say here from English to Czech.
255

255

00:11:28.370  -->  00:11:31.298
I say, "I am a boy who likes to learn."
256

256

00:11:31.298  -->  00:11:33.540
(speaking foreign language)
257

257

00:11:33.540  -->  00:11:36.510
And basically, in other languages,
258

258

00:11:36.510  -->  00:11:39.320
in some other languages it is important
259

259

00:11:39.320  -->  00:11:42.480
what gender your person is, right?
260

260

00:11:42.480  -->  00:11:43.870
So, here boy is male
261

261

00:11:43.870  -->  00:11:45.627
so that's why it's got (speaking in foreign language).
262

262

00:11:46.630  -->  00:11:50.650
And if you see, if I change this to girl in English,
263

263

00:11:50.650  -->  00:11:52.340
the other words don't change.
264

264

00:11:52.340  -->  00:11:56.044
But in Czech, the other words change.
265

265

00:11:56.044  -->  00:11:59.030
(speaking in foreign language)
266

266

00:11:59.030  -->  00:12:01.112
So, you can see right away, now it's not
267

267

00:12:01.112  -->  00:12:04.200
(speaking in foreign language)
268

268

00:12:04.200  -->  00:12:06.830
meaning that these words they depend
269

269

00:12:06.830  -->  00:12:09.350
on the gender of this word, holka.
270

270

00:12:09.350  -->  00:12:11.740
And holka is a girl and therefore
271

271

00:12:11.740  -->  00:12:13.147
this becomes (speaking in foreign language).
272

272

00:12:14.818  -->  00:12:17.390
And again, I don't know if Google Translate uses an RNN,
273

273

00:12:17.390  -->  00:12:19.540
I'm not going to comment on that,
274

274

00:12:19.540  -->  00:12:22.760
but basically the concept is the same
275

275

00:12:22.760  -->  00:12:25.340
that you need short term information
276

276

00:12:25.340  -->  00:12:27.640
about the previous word in order
277

277

00:12:27.640  -->  00:12:29.210
to translate the next word, right?
278

278

00:12:29.210  -->  00:12:31.620
You can't just translate word by word.
279

279

00:12:31.620  -->  00:12:33.720
And it's just a simple example, of course,
280

280

00:12:34.710  -->  00:12:37.733
to make a sentence make sense you do need information
281

281

00:12:37.733  -->  00:12:39.297
about the previous words.
282

282

00:12:39.297  -->  00:12:41.880
But a very visual example we have here is
283

283

00:12:41.880  -->  00:12:46.880
that at least you need to know the gender of this word
284

284

00:12:47.470  -->  00:12:49.950
in order to translate the following words
285

285

00:12:49.950  -->  00:12:51.400
for the sentence to make sense.
286

286

00:12:51.400  -->  00:12:53.930
And that's where RNNs have power
287

287

00:12:53.930  -->  00:12:56.250
because they have short term memory
288

288

00:12:56.250  -->  00:12:57.860
and they can do these things.
289

289

00:12:57.860  -->  00:12:59.590
And so, that's a many to many.
290

290

00:12:59.590  -->  00:13:00.640
You put in lots of words
291

291

00:13:00.640  -->  00:13:02.510
and then you get lots of words out, that's translation.
292

292

00:13:02.510  -->  00:13:04.970
And of course, not every example has to be related
293

293

00:13:04.970  -->  00:13:07.670
to texts or images, they can be lots
294

294

00:13:07.670  -->  00:13:09.340
and lots of different applications of RNNs.
295

295

00:13:09.340  -->  00:13:11.290
For instance, many to many you can use
296

296

00:13:11.290  -->  00:13:14.100
RNNs to subtitle movies.
297

297

00:13:14.100  -->  00:13:18.253
Meaning that you can have running subtitles,
298

298

00:13:18.253  -->  00:13:21.510
or describe every single frame in a movie
299

299

00:13:21.510  -->  00:13:23.350
and that is something you can't simply do
300

300

00:13:23.350  -->  00:13:26.180
with a CNN or other neural networks
301

301

00:13:26.180  -->  00:13:28.260
because if you're watching a movie you need context
302

302

00:13:28.260  -->  00:13:29.660
about what happened previously
303

303

00:13:29.660  -->  00:13:31.630
to describe what's happening now.
304

304

00:13:31.630  -->  00:13:33.770
And so, you need that short term memory.
305

305

00:13:33.770  -->  00:13:35.650
You can't just dry run through a million movies
306

306

00:13:35.650  -->  00:13:38.320
and kind of understand the whole plot
307

307

00:13:38.320  -->  00:13:39.270
that is going to happen.
308

308

00:13:39.270  -->  00:13:41.690
You need short term memory of the plot
309

309

00:13:41.690  -->  00:13:44.410
to be able to comment on every single frame.
310

310

00:13:44.410  -->  00:13:47.340
And speaking of movies, today we don't have
311

311

00:13:47.340  -->  00:13:50.310
additional reading, today we have additional watching.
312

312

00:13:50.310  -->  00:13:54.280
So, a movie called Sunspring in 2016
313

313

00:13:54.280  -->  00:13:56.000
directed by Oscar Sharp.
314

314

00:13:56.000  -->  00:13:57.970
And it's got, you might know this actor,
315

315

00:13:57.970  -->  00:14:00.970
Thomas Middleditch from TV show Silicon Valley.
316

316

00:14:00.970  -->  00:14:03.240
And this movie was entirely written
317

317

00:14:03.240  -->  00:14:08.240
by Benjamin who is an RNN, an LSTM to be specific.
318

318

00:14:08.560  -->  00:14:12.006
So, I'm gonna show you this movie now.
319

319

00:14:12.006  -->  00:14:13.700
Well, I'm not gonna play it.
320

320

00:14:13.700  -->  00:14:14.740
I'm just gonna show you where to find it.
321

321

00:14:14.740  -->  00:14:16.350
So, you need to go to Ars Technica.
322

322

00:14:16.350  -->  00:14:18.640
It's only nine minutes long, I highly recommend it.
323

323

00:14:18.640  -->  00:14:20.221
I've seen it twice.
324

324

00:14:20.221  -->  00:14:21.719
It's so fun.
325

325

00:14:21.719  -->  00:14:26.630
So, a couple of things there's a great description here
326

326

00:14:26.630  -->  00:14:28.160
as well so it's worth reading through.
327

327

00:14:28.160  -->  00:14:29.420
There's actually an interview of Benjamin
328

328

00:14:29.420  -->  00:14:31.570
and he actually gave himself the name of Benjamin,
329

329

00:14:31.570  -->  00:14:33.250
that's why they call him Benjamin.
330

330

00:14:33.250  -->  00:14:36.240
It's really cool to see these things
331

331

00:14:36.240  -->  00:14:37.960
and what you'll find about the movie
332

332

00:14:37.960  -->  00:14:40.100
is the acting is amazing.
333

333

00:14:40.100  -->  00:14:41.790
Just amazing, like I had shivers
334

334

00:14:41.790  -->  00:14:43.658
down my spine towards the end.
335

335

00:14:43.658  -->  00:14:48.580
It got in the top ten in the SCI-FI-LONDON Festival.
336

336

00:14:51.590  -->  00:14:55.240
And then what you'll find is that Benjamin is able
337

337

00:14:55.240  -->  00:14:57.880
to construct sentences which kind of make sense
338

338

00:14:57.880  -->  00:15:00.680
for the most part, which is good,
339

339

00:15:00.680  -->  00:15:03.420
but what he lacks is kind of the bigger picture.
340

340

00:15:03.420  -->  00:15:08.420
He cannot come up with a plot that consistently makes sense.
341

341

00:15:08.520  -->  00:15:10.380
Even though the actors do a great job
342

342

00:15:10.380  -->  00:15:11.820
about putting it together,
343

343

00:15:11.820  -->  00:15:14.400
and it does look amazing in the end,
344

344

00:15:14.400  -->  00:15:16.734
but you will notice and kind of look out for this.
345

345

00:15:16.734  -->  00:15:19.580
When you're watching, separate the sentences
346

346

00:15:19.580  -->  00:15:20.910
and you'll see that each sentence
347

347

00:15:20.910  -->  00:15:24.270
on its own more or less, 90% of the time, makes sense.
348

348

00:15:24.270  -->  00:15:27.410
But overall, he can't properly link sentences together.
349

349

00:15:27.410  -->  00:15:29.848
And that's the next step for RNNs,
350

350

00:15:29.848  -->  00:15:31.960
this is still quite a new technology,
351

351

00:15:31.960  -->  00:15:33.860
or it's only picking up recently,
352

352

00:15:33.860  -->  00:15:35.690
so it'll be developed very soon.
353

353

00:15:35.690  -->  00:15:37.130
And maybe when you're watching this tutorial
354

354

00:15:37.130  -->  00:15:40.670
you're laughing in the future, five years from now,
355

355

00:15:40.670  -->  00:15:42.340
you're laughing to yourself and saying, oh yeah,
356

356

00:15:42.340  -->  00:15:43.910
we've already passed that milestone.
357

357

00:15:43.910  -->  00:15:45.180
And probably we will very soon,
358

358

00:15:45.180  -->  00:15:47.140
but this is where things are now
359

359

00:15:47.140  -->  00:15:49.130
and highly recommend checking this out,
360

360

00:15:49.130  -->  00:15:50.891
only nine minutes long.
361

361

00:15:50.891  -->  00:15:54.170
So there you go, that's what RNNs are in a nutshell
362

362

00:15:54.170  -->  00:15:58.550
and in the next tutorial we will continue digging deeper.
363

363

00:15:58.550  -->  00:15:59.650
I look forward to seeing you next time.
364

364

00:15:59.650  -->  00:16:01.363
Until then, enjoy deep learning.
