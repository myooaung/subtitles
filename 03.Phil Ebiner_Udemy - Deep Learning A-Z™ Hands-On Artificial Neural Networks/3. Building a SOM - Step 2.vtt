WEBVTT
1
1

00:00:00.200  -->  00:00:02.560
<v ->Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.560  -->  00:00:05.120
So, we just took care of the data pre-processing phase
3

3

00:00:05.120  -->  00:00:07.540
and now we're ready to train the SOM.
4

4

00:00:07.540  -->  00:00:09.480
So how are we going to do that?
5

5

00:00:09.480  -->  00:00:10.820
Well, we have two options.
6

6

00:00:10.820  -->  00:00:14.360
The first option is to implement the SOM from scratch
7

7

00:00:14.360  -->  00:00:16.940
and the second option is to use a code,
8

8

00:00:16.940  -->  00:00:19.380
a class made by another developer,
9

9

00:00:19.380  -->  00:00:21.210
exactly like when we use cyclic learning
10

10

00:00:21.210  -->  00:00:23.200
that is made by other developers.
11

11

00:00:23.200  -->  00:00:25.360
Except that here, the self-organizing map
12

12

00:00:25.360  -->  00:00:27.730
doesn't have an implementation in cyclic learning.
13

13

00:00:27.730  -->  00:00:30.410
So we'll need to take it from another developer.
14

14

00:00:30.410  -->  00:00:32.430
So which option are we going to take?
15

15

00:00:32.430  -->  00:00:35.060
Well that depends on what is available online
16

16

00:00:35.060  -->  00:00:39.060
and if some good implementations of self-organizing maps
17

17

00:00:39.060  -->  00:00:40.540
were made by developers.
18

18

00:00:40.540  -->  00:00:44.230
And fortunately there is one excellent implementation
19

19

00:00:44.230  -->  00:00:48.230
of SOM, which is this one, MiniSom 1.0.
20

20

00:00:48.230  -->  00:00:51.630
So, the developer is use up vertically
21

21

00:00:51.630  -->  00:00:53.770
and it is a numPy based implementation
22

22

00:00:53.770  -->  00:00:55.370
of self-organizing maps.
23

23

00:00:55.370  -->  00:00:58.510
The license is Creative Commons by 3.0
24

24

00:00:58.510  -->  00:01:01.810
which means that we can share the codes,
25

25

00:01:01.810  -->  00:01:04.740
adapt the codes, well, do whatever we want with the code
26

26

00:01:04.740  -->  00:01:08.710
and that's why we can totally use it to build our SOM.
27

27

00:01:08.710  -->  00:01:10.400
Alright, so that's an important reason
28

28

00:01:10.400  -->  00:01:12.910
why I'm choosing this implementation here.
29

29

00:01:12.910  -->  00:01:14.030
And the second reason is that
30

30

00:01:14.030  -->  00:01:16.000
I would like to start this volume two
31

31

00:01:16.000  -->  00:01:18.230
on supervised deep learning, rather slowly
32

32

00:01:18.230  -->  00:01:20.060
because then we will have a huge job
33

33

00:01:20.060  -->  00:01:21.850
implementing the other models,
34

34

00:01:21.850  -->  00:01:23.640
the other deep learning models.
35

35

00:01:23.640  -->  00:01:25.748
So we will build the next deep learning models
36

36

00:01:25.748  -->  00:01:28.840
that is Boltzmann machines and stacked to our encoders
37

37

00:01:28.840  -->  00:01:30.020
almost from scratch.
38

38

00:01:30.020  -->  00:01:32.250
That is that we will make our own class
39

39

00:01:32.250  -->  00:01:33.730
and this will be excellent for you
40

40

00:01:33.730  -->  00:01:36.440
because you will learn how to manipulate classes,
41

41

00:01:36.440  -->  00:01:38.090
objects, methods.
42

42

00:01:38.090  -->  00:01:41.080
Well basically all the advanced features of Python
43

43

00:01:41.080  -->  00:01:43.040
that are quite essential in deep learning
44

44

00:01:43.040  -->  00:01:45.040
because you will not always use libraries.
45

45

00:01:45.040  -->  00:01:47.560
Sometimes you will need to implement your own models
46

46

00:01:47.560  -->  00:01:48.393
from scratch.
47

47

00:01:48.393  -->  00:01:50.250
So that's why right now we take it easy
48

48

00:01:50.250  -->  00:01:52.590
by using this MiniSom implementation
49

49

00:01:52.590  -->  00:01:54.810
and we save up some energy for the last parts,
50

50

00:01:54.810  -->  00:01:58.230
where we'll make our own robust deep learning models.
51

51

00:01:58.230  -->  00:02:00.140
Alright, so now that we agree on using
52

52

00:02:00.140  -->  00:02:03.750
this MiniSom implementation of self-organizing maps.
53

53

00:02:03.750  -->  00:02:05.750
Well let's go back to Python
54

54

00:02:05.750  -->  00:02:08.600
and let's start training the SOM.
55

55

00:02:08.600  -->  00:02:11.150
Okay, so the first thing we need to do is
56

56

00:02:11.150  -->  00:02:13.220
to import MiniSom of course.
57

57

00:02:13.220  -->  00:02:15.550
And by the way you have to make sure
58

58

00:02:15.550  -->  00:02:19.040
that in your working directory folder in file explorer
59

59

00:02:19.040  -->  00:02:22.140
you get this MiniSom.py which is
60

60

00:02:22.140  -->  00:02:24.890
the implementation of self-organizing map itself
61

61

00:02:24.890  -->  00:02:26.530
made by this developer.
62

62

00:02:26.530  -->  00:02:29.850
So you just need to have this MiniSom.py Python file
63

63

00:02:29.850  -->  00:02:32.770
in your working directory folder that contains the dataset.
64

64

00:02:32.770  -->  00:02:33.840
And if that's the case,
65

65

00:02:33.840  -->  00:02:36.770
you are ready to import this MiniSom
66

66

00:02:36.770  -->  00:02:38.950
in the code that we were about to make right now.
67

67

00:02:38.950  -->  00:02:40.610
Alright, so let's import the class
68

68

00:02:40.610  -->  00:02:43.380
and the class is actually called MiniSom,
69

69

00:02:43.380  -->  00:02:45.310
with capital M.
70

70

00:02:45.310  -->  00:02:47.960
Mini, capital, S om.
71

71

00:02:47.960  -->  00:02:49.720
Alright, so that's the class.
72

72

00:02:49.720  -->  00:02:52.590
And we import this MiniSom class
73

73

00:02:52.590  -->  00:02:56.790
from the MiniSom Python file.
74

74

00:02:56.790  -->  00:02:58.100
That is exactly this file.
75

75

00:02:58.100  -->  00:02:59.575
MinSom.py.
76

76

00:02:59.575  -->  00:03:04.370
So we can already try to see if that works.
77

77

00:03:04.370  -->  00:03:06.590
Here we go, MiniSom is imported.
78

78

00:03:06.590  -->  00:03:07.480
Perfect.
79

79

00:03:07.480  -->  00:03:10.350
Now let's create an object of this class
80

80

00:03:10.350  -->  00:03:12.220
and this object is going to be
81

81

00:03:12.220  -->  00:03:14.030
the self-organizing map itself,
82

82

00:03:14.030  -->  00:03:17.920
that is going to be trained on x and not x and y
83

83

00:03:17.920  -->  00:03:20.280
because we're doing some unsupervised learning.
84

84

00:03:20.280  -->  00:03:22.650
That is, we are trained to identify some patterns
85

85

00:03:22.650  -->  00:03:25.050
inside the independent variables
86

86

00:03:25.050  -->  00:03:26.500
that are contained in x
87

87

00:03:26.500  -->  00:03:29.260
and we don't use the information of the dependent variable.
88

88

00:03:29.260  -->  00:03:32.210
We don't consider that information in y here.
89

89

00:03:32.210  -->  00:03:34.080
So let's create this object
90

90

00:03:34.080  -->  00:03:37.290
and since this object is the self-organizing map itself,
91

91

00:03:37.290  -->  00:03:40.680
we're gonna call it Som and equals,
92

92

00:03:40.680  -->  00:03:45.680
and then we call the class MiniSom and parenthesis.
93

93

00:03:46.300  -->  00:03:48.840
And now let's see what arguments we need to input.
94

94

00:03:48.840  -->  00:03:52.010
So as we can see here, the first arguments are x and y
95

95

00:03:52.010  -->  00:03:54.480
which are of course the dimensions of the grid
96

96

00:03:54.480  -->  00:03:56.250
to self-organizing map.
97

97

00:03:56.250  -->  00:03:58.270
So here the choice is pretty arbitrary.
98

98

00:03:58.270  -->  00:04:00.670
We can choose whatever dimensions we want
99

99

00:04:00.670  -->  00:04:02.830
for our self-organizing map.
100

100

00:04:02.830  -->  00:04:04.470
It must not be too small because
101

101

00:04:04.470  -->  00:04:08.500
we want to get the outliers pretty in evidence
102

102

00:04:08.500  -->  00:04:10.860
and only if you want to be very accurate
103

103

00:04:10.860  -->  00:04:13.290
with your search of outliers well,
104

104

00:04:13.290  -->  00:04:15.020
you can make a bigger map.
105

105

00:04:15.020  -->  00:04:17.480
But here, we don't have that much observations.
106

106

00:04:17.480  -->  00:04:19.160
We don't have that much customers
107

107

00:04:19.160  -->  00:04:21.480
in our dataset Credit card applications,
108

108

00:04:21.480  -->  00:04:24.480
so we're just gonna make a 10 by 10 grid
109

109

00:04:24.480  -->  00:04:27.370
and therefore, here we're going to input x equals 10
110

110

00:04:27.370  -->  00:04:28.940
and y equals 10.
111

111

00:04:28.940  -->  00:04:31.360
Then the next parameter is input link,
112

112

00:04:31.360  -->  00:04:35.140
which of course corresponds to the number of features
113

113

00:04:35.140  -->  00:04:36.670
we have in our dataset.
114

114

00:04:36.670  -->  00:04:38.540
Not in the original dataset here,
115

115

00:04:38.540  -->  00:04:43.190
but in x because we are training the SOM object on x
116

116

00:04:43.190  -->  00:04:47.720
and x contains, remember 14 attributes plus the customer ID.
117

117

00:04:47.720  -->  00:04:49.790
We don't really need to consider the customer ID
118

118

00:04:49.790  -->  00:04:53.940
because it has of course no significance on the patterns,
119

119

00:04:53.940  -->  00:04:55.880
but we're gonna keep it because then
120

120

00:04:55.880  -->  00:04:58.370
we want to identify the potential cheaters
121

121

00:04:58.370  -->  00:05:00.506
and for this we will need the customer ID.
122

122

00:05:00.506  -->  00:05:05.506
So input length will be equal to 14 plus one equals 15.
123

123

00:05:06.040  -->  00:05:09.140
Then sigma, so remember from the intuition lecture
124

124

00:05:09.140  -->  00:05:10.900
that sigma is the radius
125

125

00:05:10.900  -->  00:05:12.910
of the different neighborhoods in the grid.
126

126

00:05:12.910  -->  00:05:15.720
So we will keep its default value 1.0
127

127

00:05:15.720  -->  00:05:17.370
and then we have the learning rate,
128

128

00:05:17.370  -->  00:05:19.380
which remember is this hyper-parameter
129

129

00:05:19.380  -->  00:05:22.080
that decides by how much the weights are updated
130

130

00:05:22.080  -->  00:05:23.670
during each iteration.
131

131

00:05:23.670  -->  00:05:26.010
So the higher is the learning rate,
132

132

00:05:26.010  -->  00:05:28.270
the faster there will be convergence
133

133

00:05:28.270  -->  00:05:29.890
and the lower is the learning rate,
134

134

00:05:29.890  -->  00:05:31.660
the longer the self-organizing map
135

135

00:05:31.660  -->  00:05:33.230
will take time to be built.
136

136

00:05:33.230  -->  00:05:36.740
And so same here, we're gonna keep it to default value 0.5
137

137

00:05:36.740  -->  00:05:39.520
and then finally we have a decay function parameter
138

138

00:05:39.520  -->  00:05:42.430
that can be used to improve the convergence.
139

139

00:05:42.430  -->  00:05:43.830
But here we're going to leave it to none
140

140

00:05:43.830  -->  00:05:45.260
and not use a decay.
141

141

00:05:45.260  -->  00:05:47.390
This will be perfectly fine with these values
142

142

00:05:47.390  -->  00:05:50.230
of the hyperparameters, sigma and learning rate.
143

143

00:05:50.230  -->  00:05:51.710
Okay, so we have everything we need.
144

144

00:05:51.710  -->  00:05:53.140
We don't need a random seed.
145

145

00:05:53.140  -->  00:05:54.410
That will be fine as well.
146

146

00:05:54.410  -->  00:05:56.130
And therefore, let's input the parameters.
147

147

00:05:56.130  -->  00:06:01.130
So we said x equals 10, y equals 10
148

148

00:06:01.940  -->  00:06:04.230
so we have a 10 by 10 grid.
149

149

00:06:04.230  -->  00:06:08.730
Then input len is the number of our features in x.
150

150

00:06:08.730  -->  00:06:10.358
So that's 15.
151

151

00:06:10.358  -->  00:06:15.358
Then sigma, we keep the default value to 1.0
152

152

00:06:16.060  -->  00:06:18.500
and finally we also keep the default value
153

153

00:06:18.500  -->  00:06:19.820
for the learning rate,
154

154

00:06:19.820  -->  00:06:20.703
which is 0.5.
155

155

00:06:22.530  -->  00:06:26.400
And perfect, our SOM objects is ready to be created.
156

156

00:06:26.400  -->  00:06:28.120
So what about creating it now?
157

157

00:06:28.120  -->  00:06:31.650
I'm going to select this line and execute
158

158

00:06:31.650  -->  00:06:34.600
and perfect, the objective is well created.
159

159

00:06:34.600  -->  00:06:35.810
And now, what we're gonna do
160

160

00:06:35.810  -->  00:06:40.810
is of course train this self-organizing map object on x.
161

161

00:06:40.920  -->  00:06:42.290
But before we do that,
162

162

00:06:42.290  -->  00:06:44.230
we have to do something very important.
163

163

00:06:44.230  -->  00:06:46.320
It's to initialize the weights.
164

164

00:06:46.320  -->  00:06:48.930
Because remember, if we go back to the algorithm
165

165

00:06:48.930  -->  00:06:50.560
of the self-organizing map,
166

166

00:06:50.560  -->  00:06:52.570
well we can see here that step three
167

167

00:06:52.570  -->  00:06:56.200
is to randomly initialize the values of the weight vectors
168

168

00:06:56.200  -->  00:06:59.070
to small numbers close to zero, but not zero.
169

169

00:06:59.070  -->  00:07:01.730
And then we can train the whole algorithm.
170

170

00:07:01.730  -->  00:07:04.260
But we need to do that not only in theory
171

171

00:07:04.260  -->  00:07:06.020
but also on Python.
172

172

00:07:06.020  -->  00:07:07.410
So to do this,
173

173

00:07:07.410  -->  00:07:10.500
we're gonna take our object, SOM,
174

174

00:07:10.500  -->  00:07:11.720
and then dot.
175

175

00:07:11.720  -->  00:07:14.650
And here we're gonna use a method made by this developer,
176

176

00:07:14.650  -->  00:07:17.500
that is the MiniSom.py Python file.
177

177

00:07:17.500  -->  00:07:19.090
You can check it out by the way.
178

178

00:07:19.090  -->  00:07:24.090
And this method is random weights underscore in it.
179

179

00:07:25.800  -->  00:07:26.720
Here it is.
180

180

00:07:26.720  -->  00:07:29.290
So that's the method that will initialize the weights.
181

181

00:07:29.290  -->  00:07:32.930
And inside this method, we just need to input the data.
182

182

00:07:32.930  -->  00:07:36.233
That is x, our data on which the model will be trained.
183

183

00:07:37.380  -->  00:07:39.080
And then one last line.
184

184

00:07:39.080  -->  00:07:41.120
The last line is about using another method,
185

185

00:07:41.120  -->  00:07:43.720
which is of course the method to train
186

186

00:07:43.720  -->  00:07:46.210
the self-organizing map on x.
187

187

00:07:46.210  -->  00:07:49.500
And this method is called Train Random.
188

188

00:07:49.500  -->  00:07:52.730
And of course, this is the method that will apply
189

189

00:07:52.730  -->  00:07:56.410
all the different steps here from step four to step nine
190

190

00:07:56.410  -->  00:07:58.900
and of course by repeating steps four to nine
191

191

00:07:58.900  -->  00:08:00.430
for many iterations.
192

192

00:08:00.430  -->  00:08:03.350
And we will specify in this Train Random method,
193

193

00:08:03.350  -->  00:08:06.050
the number of iterations to train our SOM.
194

194

00:08:06.050  -->  00:08:07.060
So let's do it.
195

195

00:08:07.060  -->  00:08:09.537
We take our SOM object again som.
196

196

00:08:10.571  -->  00:08:15.571
And now we use the train underscore random method,
197

197

00:08:15.800  -->  00:08:18.550
parenthesis and inside the parenthesis,
198

198

00:08:18.550  -->  00:08:21.440
as you can see, we need to input two arguments.
199

199

00:08:21.440  -->  00:08:22.700
The first one is data.
200

200

00:08:22.700  -->  00:08:26.680
So again, that's x because we are training the SOM on x.
201

201

00:08:26.680  -->  00:08:29.290
And the second argument is number of iterations.
202

202

00:08:29.290  -->  00:08:31.120
Number of iteration, and of course
203

203

00:08:31.120  -->  00:08:33.090
that's the number of iterations.
204

204

00:08:33.090  -->  00:08:35.730
We want to repeat steps four to nine.
205

205

00:08:35.730  -->  00:08:38.360
And so here we're gonna try with a hundred iterations
206

206

00:08:38.360  -->  00:08:40.730
that will be way enough for our dataset.
207

207

00:08:40.730  -->  00:08:42.510
So let's input the parameter,
208

208

00:08:42.510  -->  00:08:45.940
first is data equals x.
209

209

00:08:45.940  -->  00:08:50.797
And then num underscore iteration equals 100
210

210

00:08:52.444  -->  00:08:54.010
and that's ready.
211

211

00:08:54.010  -->  00:08:56.300
So first we're going to select this line
212

212

00:08:56.300  -->  00:09:00.220
and execute to randomly initialize the weights.
213

213

00:09:00.220  -->  00:09:03.772
And now let's select this line and execute
214

214

00:09:03.772  -->  00:09:07.350
to train the self-organizing map on x.
215

215

00:09:07.350  -->  00:09:08.410
And here we go.
216

216

00:09:08.410  -->  00:09:11.360
That didn't take a while because we have a small dataset
217

217

00:09:11.360  -->  00:09:14.410
with this also works very well on larger datasets.
218

218

00:09:14.410  -->  00:09:17.740
And so now basically ourself-organizing map is trained
219

219

00:09:17.740  -->  00:09:19.410
on our matrix of features x
220

220

00:09:19.410  -->  00:09:22.050
and the patterns are already identified.
221

221

00:09:22.050  -->  00:09:24.390
But of course, now we need to make something
222

222

00:09:24.390  -->  00:09:25.600
even more important.
223

223

00:09:25.600  -->  00:09:27.300
It's to visualize the results,
224

224

00:09:27.300  -->  00:09:31.140
to identify the outline neurons inside the map.
225

225

00:09:31.140  -->  00:09:31.973
And so to do this,
226

226

00:09:31.973  -->  00:09:34.890
we will simply plot the self-organizing map,
227

227

00:09:34.890  -->  00:09:37.940
and that's exactly what we're gonna do in the next tutorial.
228

228

00:09:37.940  -->  00:09:39.553
Until then, enjoy deep learning.
