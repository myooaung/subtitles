WEBVTT
1
1

00:00:00.200  -->  00:00:02.740
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.740  -->  00:00:05.830
All right, so let's do our k-steps of the Random Walk
3

3

00:00:05.830  -->  00:00:09.410
in Gibbs sampling, which I remind is an MCMC technique.
4

4

00:00:09.410  -->  00:00:11.650
That is Markov Chain Monte Carlo technique.
5

5

00:00:11.650  -->  00:00:13.430
And very soon we'll stake a step back
6

6

00:00:13.430  -->  00:00:16.270
to explain what's going on with this random walk.
7

7

00:00:16.270  -->  00:00:17.900
It's actually pretty fascinating,
8

8

00:00:17.900  -->  00:00:19.870
and we will try to give a good representation
9

9

00:00:19.870  -->  00:00:21.590
of what is going on.
10

10

00:00:21.590  -->  00:00:25.350
All right, so we left off at what we have to add
11

11

00:00:25.350  -->  00:00:26.990
inside this new for loop,
12

12

00:00:26.990  -->  00:00:29.310
the third one of this training process
13

13

00:00:29.310  -->  00:00:32.063
which is about the k-step contrasted divergence.
14

14

00:00:33.100  -->  00:00:35.880
Okay, so basically, Gibbs sampling consists
15

15

00:00:35.880  -->  00:00:37.400
of making this Gibbs chain.
16

16

00:00:37.400  -->  00:00:40.210
There are simply several round trips from the visible nodes
17

17

00:00:40.210  -->  00:00:41.200
to the hidden nodes,
18

18

00:00:41.200  -->  00:00:44.070
and then from the hidden nodes to the visible nodes.
19

19

00:00:44.070  -->  00:00:47.890
And in each round trip of this Gibbs chain of Gibbs sampling
20

20

00:00:47.890  -->  00:00:49.910
well, the visible nodes are dated.
21

21

00:00:49.910  -->  00:00:51.980
And step after step, we get closer
22

22

00:00:51.980  -->  00:00:54.220
to our good predicted ratings.
23

23

00:00:54.220  -->  00:00:57.140
So first we need to wonder what happens at the beginning.
24

24

00:00:57.140  -->  00:00:58.340
What happens at the beginning is
25

25

00:00:58.340  -->  00:01:01.870
that we start with our input batch of observations.
26

26

00:01:01.870  -->  00:01:05.600
That is our input ratings that are all invisible,
27

27

00:01:05.600  -->  00:01:08.200
that is, our batch of 100 users.
28

28

00:01:08.200  -->  00:01:10.660
We get all the ratings from all these users
29

29

00:01:10.660  -->  00:01:12.240
for all the movies.
30

30

00:01:12.240  -->  00:01:15.170
Then in the first step of Gibbs sampling,
31

31

00:01:15.170  -->  00:01:17.840
from this batch input vector of original ratings,
32

32

00:01:17.840  -->  00:01:21.030
we are going to sample the first hidden nodes.
33

33

00:01:21.030  -->  00:01:23.070
And we are going to sample these hidden nodes
34

34

00:01:23.070  -->  00:01:25.060
with a Bernoulli sampling
35

35

00:01:25.060  -->  00:01:29.140
following a PH given v0 distribution.
36

36

00:01:29.140  -->  00:01:32.870
And that's exactly what happens in this sample_h function.
37

37

00:01:32.870  -->  00:01:34.700
So basically, the first step
38

38

00:01:34.700  -->  00:01:38.247
of k-steps contrasted divergence is to call sample_h
39

39

00:01:38.247  -->  00:01:40.320
on the visible nodes,
40

40

00:01:40.320  -->  00:01:42.890
because sample_h on the visible nodes will get
41

41

00:01:42.890  -->  00:01:45.970
the first sampling of the first hidden nodes.
42

42

00:01:45.970  -->  00:01:46.930
So that's what we have to do.
43

43

00:01:46.930  -->  00:01:49.100
We have to call the sample_h function
44

44

00:01:49.100  -->  00:01:52.540
to get the first sampled hidden nodes.
45

45

00:01:52.540  -->  00:01:56.311
And since we actually want to get this second element
46

46

00:01:56.311  -->  00:01:58.630
that sample_h function returns,
47

47

00:01:58.630  -->  00:02:01.120
well, we're going to use the inverse trick as
48

48

00:02:01.120  -->  00:02:02.350
what we did here.
49

49

00:02:02.350  -->  00:02:05.570
We are going to start with an underscore
50

50

00:02:05.570  -->  00:02:09.040
and then comma and then get hK
51

51

00:02:09.040  -->  00:02:11.600
which are going to be the hidden nodes obtained
52

52

00:02:11.600  -->  00:02:13.270
at the k-th step
53

53

00:02:13.270  -->  00:02:14.810
of contrasted divergence.
54

54

00:02:14.810  -->  00:02:16.090
Right now, we are at the first step.
55

55

00:02:16.090  -->  00:02:17.420
So k equals 0.
56

56

00:02:17.420  -->  00:02:21.260
But h0 is going to be the second element returned
57

57

00:02:21.260  -->  00:02:23.340
by the sample_h method
58

58

00:02:23.340  -->  00:02:27.440
and since the sample_h method is method in the rbm class,
59

59

00:02:27.440  -->  00:02:29.710
well we need to call the sample_h function
60

60

00:02:29.710  -->  00:02:32.810
from our rbm object.
61

61

00:02:32.810  -->  00:02:35.840
And then, here we go, sample_h.
62

62

00:02:35.840  -->  00:02:37.890
And since we are doing the sampling
63

63

00:02:37.890  -->  00:02:39.160
of the first hidden nodes,
64

64

00:02:39.160  -->  00:02:41.770
given the values of the first visible nodes,
65

65

00:02:41.770  -->  00:02:43.760
that is our original ratings,
66

66

00:02:43.760  -->  00:02:46.060
well the first input for our sample_h function
67

67

00:02:46.060  -->  00:02:50.250
in this first step of Gibbs sampling is going to be visual.
68

68

00:02:50.250  -->  00:02:52.950
But be careful, visual is the target,
69

69

00:02:52.950  -->  00:02:54.660
which we don't wanna change.
70

70

00:02:54.660  -->  00:02:56.560
So we have to take this vk here
71

71

00:02:56.560  -->  00:02:59.580
because vk so far is our input batch
72

72

00:02:59.580  -->  00:03:02.540
of observations, and then vk will be updated.
73

73

00:03:02.540  -->  00:03:04.910
And you'll see that we'll update vk right after
74

74

00:03:04.910  -->  00:03:07.850
we update hk with the other sampling function
75

75

00:03:07.850  -->  00:03:09.720
which is sample_v.
76

76

00:03:09.720  -->  00:03:10.790
So right now let's be careful.
77

77

00:03:10.790  -->  00:03:14.410
We have to input vk and not v0.
78

78

00:03:14.410  -->  00:03:16.240
So right now vk equals v0,
79

79

00:03:16.240  -->  00:03:19.740
but then the next step is to update vk
80

80

00:03:19.740  -->  00:03:22.250
so that vk is no longer v0,
81

81

00:03:22.250  -->  00:03:25.330
but now vk is going to be,
82

82

00:03:25.330  -->  00:03:27.630
according to you what is it going to be?
83

83

00:03:27.630  -->  00:03:30.530
Well, it's going to be the sampled visible nodes
84

84

00:03:30.530  -->  00:03:33.010
after the first step of Gibbs sampling.
85

85

00:03:33.010  -->  00:03:34.700
And how do we get this sample?
86

86

00:03:34.700  -->  00:03:38.010
Well of course is by calling the sample_v function
87

87

00:03:38.010  -->  00:03:42.740
on the first sample of our hidden nodes that is hk,
88

88

00:03:42.740  -->  00:03:45.130
the result of this first sampling
89

89

00:03:45.130  -->  00:03:46.900
based on the first visible nodes,
90

90

00:03:46.900  -->  00:03:49.020
the original visible nodes.
91

91

00:03:49.020  -->  00:03:54.020
And so now vk is going to be rbm.sample
92

92

00:03:54.640  -->  00:03:59.640
and now sample_v that we call on hk.
93

93

00:04:00.110  -->  00:04:01.720
The hk that is here.
94

94

00:04:01.720  -->  00:04:04.110
That is the first sampled hidden nodes.
95

95

00:04:04.110  -->  00:04:04.943
All right.
96

96

00:04:04.943  -->  00:04:08.060
So we get our first update of the visible nodes
97

97

00:04:08.060  -->  00:04:10.710
that is the visible nodes after the first sampling.
98

98

00:04:10.710  -->  00:04:12.870
That's the first step of our random walk,
99

99

00:04:12.870  -->  00:04:15.230
that is the first step of Gibbs sampling.
100

100

00:04:15.230  -->  00:04:18.520
And now what's wonderful is that we make this for loop.
101

101

00:04:18.520  -->  00:04:21.750
So basically this will continue by itself,
102

102

00:04:21.750  -->  00:04:24.370
because indeed then k will be equal to one
103

103

00:04:24.370  -->  00:04:27.400
so we will enter the second step of Gibbs sampling
104

104

00:04:27.400  -->  00:04:29.880
and so hk is going to be updated.
105

105

00:04:29.880  -->  00:04:33.170
But since vk is now equal to the first sample
106

106

00:04:33.170  -->  00:04:36.580
of visible nodes, well the sample_h(vk) will return
107

107

00:04:36.580  -->  00:04:38.890
the second sample of hidden nodes.
108

108

00:04:38.890  -->  00:04:42.150
And then this second sample of hidden nodes is going
109

109

00:04:42.150  -->  00:04:45.550
to become the input of our sample_v function.
110

110

00:04:45.550  -->  00:04:48.770
And that will return the new sample of the visible nodes,
111

111

00:04:48.770  -->  00:04:51.580
that is the second sample of the visible nodes.
112

112

00:04:51.580  -->  00:04:54.070
And again the ratings will be updated.
113

113

00:04:54.070  -->  00:04:55.030
And then et cetera.
114

114

00:04:55.030  -->  00:04:57.990
At the next step we get the third sample of hidden nodes
115

115

00:04:57.990  -->  00:05:00.250
then the third sample of visible nodes
116

116

00:05:00.250  -->  00:05:03.810
until the end of the loop we get the 10th sample
117

117

00:05:03.810  -->  00:05:07.720
of hidden nodes and the 10th sample of visible nodes.
118

118

00:05:07.720  -->  00:05:12.510
And now since we have our hk and vk after the last step
119

119

00:05:12.510  -->  00:05:13.640
of the random walk,
120

120

00:05:13.640  -->  00:05:17.630
well, we can now approximate the gradients
121

121

00:05:17.630  -->  00:05:20.390
thanks to these lines: eight, nine, and 10.
122

122

00:05:20.390  -->  00:05:23.240
That's all the purpose of doing what we just did.
123

123

00:05:23.240  -->  00:05:24.073
You know?
124

124

00:05:24.073  -->  00:05:27.710
What we just did here is the lines five and six.
125

125

00:05:27.710  -->  00:05:29.770
We want to get the last sample
126

126

00:05:29.770  -->  00:05:31.570
after the last step of the random walk,
127

127

00:05:31.570  -->  00:05:33.060
the last sample of hidden nodes
128

128

00:05:33.060  -->  00:05:35.010
and the last sample of visible nodes.
129

129

00:05:35.010  -->  00:05:37.450
And that's because to update the weight
130

130

00:05:37.450  -->  00:05:40.010
and the bias to approximate the gradient,
131

131

00:05:40.010  -->  00:05:41.930
well you can see that here we need
132

132

00:05:41.930  -->  00:05:43.630
the last sample of the visible nodes.
133

133

00:05:43.630  -->  00:05:46.120
We don't directly use the last sample of the hidden nodes,
134

134

00:05:46.120  -->  00:05:47.930
but that was just to get the last sample
135

135

00:05:47.930  -->  00:05:49.380
of the visible nodes.
136

136

00:05:49.380  -->  00:05:50.240
So that's great!
137

137

00:05:50.240  -->  00:05:52.340
We just got our vk
138

138

00:05:52.340  -->  00:05:53.410
and thanks to these vk,
139

139

00:05:53.410  -->  00:05:56.450
we can now approximate the gradient
140

140

00:05:56.450  -->  00:05:59.070
to update the weights and the bias.
141

141

00:05:59.070  -->  00:06:01.080
And we have to right function for that.
142

142

00:06:01.080  -->  00:06:03.510
It's the train function of our rbm class
143

143

00:06:03.510  -->  00:06:06.530
and so it will be kids' stuff to update the weights.
144

144

00:06:06.530  -->  00:06:08.400
All right, so let's go back to Python.
145

145

00:06:08.400  -->  00:06:11.160
And back in Python, before we apply this train function
146

146

00:06:11.160  -->  00:06:12.560
to update the weight,
147

147

00:06:12.560  -->  00:06:14.720
well we need to do something very important.
148

148

00:06:14.720  -->  00:06:17.110
It's related to the fact that we don't wanna learn
149

149

00:06:17.110  -->  00:06:19.100
where there is no rating,
150

150

00:06:19.100  -->  00:06:21.750
that is for the sales that have a minus one.
151

151

00:06:21.750  -->  00:06:23.800
And to not include these sales
152

152

00:06:23.800  -->  00:06:25.730
that contain the minus one ratings
153

153

00:06:25.730  -->  00:06:27.100
in the training process,
154

154

00:06:27.100  -->  00:06:29.200
well, we're going to freeze these visible nodes
155

155

00:06:29.200  -->  00:06:31.250
that contain the minus one ratings.
156

156

00:06:31.250  -->  00:06:33.092
That means that won't be possible to update them
157

157

00:06:33.092  -->  00:06:34.820
during Gibbs sampling.
158

158

00:06:34.820  -->  00:06:35.900
And so how can we do that?
159

159

00:06:35.900  -->  00:06:38.190
How can we freeze these visible nodes
160

160

00:06:38.190  -->  00:06:40.180
containing the minus one ratings?
161

161

00:06:40.180  -->  00:06:41.460
Well, that's pretty easy.
162

162

00:06:41.460  -->  00:06:43.760
We need to take our vk
163

163

00:06:43.760  -->  00:06:46.260
because our vk is our visible nodes
164

164

00:06:46.260  -->  00:06:48.170
that are being updated during the k steps
165

165

00:06:48.170  -->  00:06:49.400
of the random walk.
166

166

00:06:49.400  -->  00:06:53.329
So vk and then in brackets we're gonna get the nodes
167

167

00:06:53.329  -->  00:06:55.000
that have a minus one rating,
168

168

00:06:55.000  -->  00:06:59.070
that is that were not originally rated by the users.
169

169

00:06:59.070  -->  00:07:02.410
And to do this, we need to get our target v0
170

170

00:07:02.410  -->  00:07:03.950
because this wasn't changed.
171

171

00:07:03.950  -->  00:07:06.070
This keeps the original ratings.
172

172

00:07:06.070  -->  00:07:09.560
And we add lower than zero, to get the minus one ratings
173

173

00:07:09.560  -->  00:07:13.940
because our ratings are either minus one, zero, or one.
174

174

00:07:13.940  -->  00:07:15.660
And then for these visible nodes,
175

175

00:07:15.660  -->  00:07:19.210
we're gonna say that they're equal to minus one.
176

176

00:07:19.210  -->  00:07:20.180
Actually we're gonna say
177

177

00:07:20.180  -->  00:07:22.570
that they keep the minus one ratings.
178

178

00:07:22.570  -->  00:07:24.570
And so how can we make sure they keep them?
179

179

00:07:24.570  -->  00:07:28.370
Well, it's by taking the original minus one ratings
180

180

00:07:28.370  -->  00:07:31.180
from the target because it is not changed.
181

181

00:07:31.180  -->  00:07:32.680
So, here we do the same.
182

182

00:07:32.680  -->  00:07:35.210
We take some brackets and inside the brackets,
183

183

00:07:35.210  -->  00:07:39.580
we take our v0 lower than zero.
184

184

00:07:39.580  -->  00:07:42.430
Because that get all the minus one ratings.
185

185

00:07:42.430  -->  00:07:43.263
All right!
186

186

00:07:43.263  -->  00:07:44.110
So now by doing this,
187

187

00:07:44.110  -->  00:07:48.570
we make sure that the training is not done on these ratings
188

188

00:07:48.570  -->  00:07:50.740
that were not actually existent.
189

189

00:07:50.740  -->  00:07:53.250
We only want to do the training on the ratings
190

190

00:07:53.250  -->  00:07:54.083
that happened.
191

191

00:07:54.083  -->  00:07:56.020
That actually makes sense.
192

192

00:07:56.020  -->  00:07:56.853
All right!
193

193

00:07:56.853  -->  00:08:01.060
So now we can get out of this third for loop
194

194

00:08:01.060  -->  00:08:04.140
and soon enough we will start the training.
195

195

00:08:04.140  -->  00:08:06.710
Okay, so this is now that we want to apply
196

196

00:08:06.710  -->  00:08:10.600
the train function to update the weight and the bias.
197

197

00:08:10.600  -->  00:08:13.270
But, you notice that in this train function,
198

198

00:08:13.270  -->  00:08:16.714
well, we not only need our target, v0,
199

199

00:08:16.714  -->  00:08:18.840
our sampled visible nodes
200

200

00:08:18.840  -->  00:08:22.020
at the last step of the random walk, vk,
201

201

00:08:22.020  -->  00:08:24.840
we also need ph0 that we have right here
202

202

00:08:24.840  -->  00:08:27.880
so that's fine, but we also need phk.
203

203

00:08:27.880  -->  00:08:30.380
And right now, we don't have any phk.
204

204

00:08:30.380  -->  00:08:31.720
So what we have to do first
205

205

00:08:31.720  -->  00:08:36.430
before applying the train function is to compute phk.
206

206

00:08:36.430  -->  00:08:37.263
So let's do this!
207

207

00:08:37.263  -->  00:08:38.670
It's going to be quite simple.
208

208

00:08:38.670  -->  00:08:41.380
Actually try to compute it yourself before I do it.
209

209

00:08:41.380  -->  00:08:45.370
So phk, then comma and underscore
210

210

00:08:45.370  -->  00:08:47.710
because, well, you have guessed it,
211

211

00:08:47.710  -->  00:08:50.640
we want to get the first element returned
212

212

00:08:50.640  -->  00:08:52.470
by the sample_h function.
213

213

00:08:52.470  -->  00:08:54.070
And so, you guessed it,
214

214

00:08:54.070  -->  00:08:58.350
now we need to get the sample_h function applied.
215

215

00:08:58.350  -->  00:08:59.183
Applied on what?
216

216

00:08:59.183  -->  00:09:00.610
That is the question.
217

217

00:09:00.610  -->  00:09:05.080
Applied on the last sample of the visible nodes.
218

218

00:09:05.080  -->  00:09:07.160
You know at the end of the for loop.
219

219

00:09:07.160  -->  00:09:11.110
So the last sample_visible nodes, at the 10th step
220

220

00:09:11.110  -->  00:09:14.100
of the random walk, Gibbs sampling.
221

221

00:09:14.100  -->  00:09:15.340
All right, so phk.
222

222

00:09:15.340  -->  00:09:18.860
And then we take our rbm object, as usual,
223

223

00:09:18.860  -->  00:09:23.023
and then from this rbm object we take our sample_h function.
224

224

00:09:25.050  -->  00:09:28.340
And we apply the sample_h function to the last sample
225

225

00:09:28.340  -->  00:09:31.823
of the visible nodes after the 10 steps and that is vk.
226

226

00:09:33.300  -->  00:09:35.490
All right and now we get our phk
227

227

00:09:35.490  -->  00:09:36.980
and therefore we get everything
228

228

00:09:36.980  -->  00:09:39.093
we need to apply the train function.
229

229

00:09:40.060  -->  00:09:41.080
So let's do this!
230

230

00:09:41.080  -->  00:09:42.750
Let's apply the train function.
231

231

00:09:42.750  -->  00:09:45.270
It's going to be kid stuff as I told you.
232

232

00:09:45.270  -->  00:09:48.270
First, the train function doesn't return anything.
233

233

00:09:48.270  -->  00:09:51.140
It just updates the weights according
234

234

00:09:51.140  -->  00:09:53.060
to the lines eight, nine and ten
235

235

00:09:53.060  -->  00:09:56.910
of this algorithm one k-step contrastive divergence.
236

236

00:09:56.910  -->  00:09:59.470
So very simply, we don't create a new variable
237

237

00:09:59.470  -->  00:10:01.380
because it doesn't return anything.
238

238

00:10:01.380  -->  00:10:03.790
And we just take our rbm object
239

239

00:10:03.790  -->  00:10:06.300
because this is a function from the rbm class.
240

240

00:10:06.300  -->  00:10:11.300
And so from our rbm object, we call our train function,
241

241

00:10:11.400  -->  00:10:14.570
train and now we are going to input
242

242

00:10:14.570  -->  00:10:18.410
our four arguments that Python well recognized.
243

243

00:10:18.410  -->  00:10:20.450
And so now we don't have to think or anything.
244

244

00:10:20.450  -->  00:10:22.330
We can just input our four arguments
245

245

00:10:22.330  -->  00:10:23.760
because everything is ready.
246

246

00:10:23.760  -->  00:10:24.750
So here we go.
247

247

00:10:24.750  -->  00:10:27.570
V0 then vk and ph0
248

248

00:10:30.410  -->  00:10:34.710
and eventually phk that we just computed.
249

249

00:10:34.710  -->  00:10:35.720
Perfect!
250

250

00:10:35.720  -->  00:10:37.490
Now the training is going to happen.
251

251

00:10:37.490  -->  00:10:40.400
The weight and the bias are going to be updated
252

252

00:10:40.400  -->  00:10:43.500
towards the direction of maximum likelihood.
253

253

00:10:43.500  -->  00:10:46.130
And therefore, all our probabilities will pv
254

254

00:10:46.130  -->  00:10:47.660
given the states of the hidden nodes
255

255

00:10:47.660  -->  00:10:49.260
will be more and more relevant,
256

256

00:10:49.260  -->  00:10:52.590
will get the largest weights for the probabilities
257

257

00:10:52.590  -->  00:10:54.320
that are the most significant
258

258

00:10:54.320  -->  00:10:58.010
and eventually that will lead us to some predicted ratings
259

259

00:10:58.010  -->  00:11:00.910
that are going to be close to the real ratings.
260

260

00:11:00.910  -->  00:11:03.270
And speaking of being close to the real ratings,
261

261

00:11:03.270  -->  00:11:05.160
that's exactly what we will measure
262

262

00:11:05.160  -->  00:11:09.570
in the next line of code with the train loss.
263

263

00:11:09.570  -->  00:11:13.340
So right now what we have to do is update the train loss
264

264

00:11:13.340  -->  00:11:16.470
because we actually have our predictions
265

265

00:11:16.470  -->  00:11:19.900
thanks to train function here that updates the weight.
266

266

00:11:19.900  -->  00:11:22.710
Well, after all the observations of the batch go
267

267

00:11:22.710  -->  00:11:23.950
into the network,
268

268

00:11:23.950  -->  00:11:27.660
well, we're gonna compute all our hks and vks
269

269

00:11:27.660  -->  00:11:30.450
and eventually get the last sample of the visible nodes
270

270

00:11:30.450  -->  00:11:33.190
of the Gibbs chain but with the new weight
271

271

00:11:33.190  -->  00:11:36.010
that were updated at the previous iteration.
272

272

00:11:36.010  -->  00:11:40.060
And so, step-by-step we get our final vk
273

273

00:11:40.060  -->  00:11:41.400
with better weights.
274

274

00:11:41.400  -->  00:11:44.190
And at the last steps, the weights are going close
275

275

00:11:44.190  -->  00:11:45.470
to the optimal weights
276

276

00:11:45.470  -->  00:11:48.870
thanks to which we get the optimal sample_visible nodes
277

277

00:11:48.870  -->  00:11:51.100
after the 10 steps of Gibbs sampling
278

278

00:11:51.100  -->  00:11:54.060
which contain our best predicted ratings.
279

279

00:11:54.060  -->  00:11:56.350
So now let's measure the loss
280

280

00:11:56.350  -->  00:11:59.750
by updating the train loss variable.
281

281

00:11:59.750  -->  00:12:02.500
And how are going to update this train loss?
282

282

00:12:02.500  -->  00:12:04.870
Well, that's simple.
283

283

00:12:04.870  -->  00:12:07.010
First, we are adding something to this train loss.
284

284

00:12:07.010  -->  00:12:10.180
That's why I am using a plus equal.
285

285

00:12:10.180  -->  00:12:12.490
And so now the question is what are we going to add
286

286

00:12:12.490  -->  00:12:13.610
to this train loss.
287

287

00:12:13.610  -->  00:12:15.870
Well, that's the error, that's the difference
288

288

00:12:15.870  -->  00:12:19.800
between the predictive ratings and the real original ratings
289

289

00:12:19.800  -->  00:12:22.360
of our target, v0.
290

290

00:12:22.360  -->  00:12:25.740
So now what we have to do is compare vk
291

291

00:12:25.740  -->  00:12:28.480
which is the last of the last visible nodes
292

292

00:12:28.480  -->  00:12:31.890
after the last batch of users that went through the network.
293

293

00:12:31.890  -->  00:12:34.400
So vk that we compared to v0,
294

294

00:12:34.400  -->  00:12:37.730
the target that hasn't changed since the beginning.
295

295

00:12:37.730  -->  00:12:40.220
And so as I told you in the previous tutorial,
296

296

00:12:40.220  -->  00:12:42.410
well, we have two ways of measuring the error.
297

297

00:12:42.410  -->  00:12:44.618
It's either with the RMAC tools
298

298

00:12:44.618  -->  00:12:46.710
or the autoencoders.
299

299

00:12:46.710  -->  00:12:48.900
Right now we'll gonna take the distance,
300

300

00:12:48.900  -->  00:12:51.650
the simple distance in absolute values
301

301

00:12:51.650  -->  00:12:54.740
between the prediction and the real rating.
302

302

00:12:54.740  -->  00:12:58.940
And so what I am going to do now is user a torch function
303

303

00:12:58.940  -->  00:13:01.830
that is actually the mean, very simply.
304

304

00:13:01.830  -->  00:13:04.420
And inside this mean function I'm going to use
305

305

00:13:04.420  -->  00:13:08.530
another torch function which is the abs function
306

306

00:13:08.530  -->  00:13:10.730
that returns the absolute value of a number.
307

307

00:13:10.730  -->  00:13:13.193
So abs of minus two returns two.
308

308

00:13:14.130  -->  00:13:16.130
And so we're taking the absolute value
309

309

00:13:16.130  -->  00:13:20.980
of the difference between the target and our prediction.
310

310

00:13:20.980  -->  00:13:25.750
So the target is v0, that hasn't changed since the beginning
311

311

00:13:25.750  -->  00:13:28.690
and the prediction is vk.
312

312

00:13:28.690  -->  00:13:31.400
Our last of the last sample of visible nodes
313

313

00:13:31.400  -->  00:13:34.220
after the last step of contrasted divergence
314

314

00:13:34.220  -->  00:13:38.070
after the last batch of users went into the network.
315

315

00:13:38.070  -->  00:13:41.190
All right so the absolute value of v0 minus vk,
316

316

00:13:41.190  -->  00:13:43.020
but we can still improve this.
317

317

00:13:43.020  -->  00:13:45.990
Remember in the training we didn't include the ratings
318

318

00:13:45.990  -->  00:13:48.250
that were actually non-existent,
319

319

00:13:48.250  -->  00:13:50.630
and therefore when we compute this difference
320

320

00:13:50.630  -->  00:13:53.640
between the real original ratings and the predictions,
321

321

00:13:53.640  -->  00:13:58.640
well, we have to take them for the ones that actually exist.
322

322

00:13:59.730  -->  00:14:04.240
There is all the ratings that are larger than zero
323

323

00:14:04.240  -->  00:14:05.650
and same for vk.
324

324

00:14:05.650  -->  00:14:09.769
So I am just taking these because these guys
325

325

00:14:09.769  -->  00:14:11.700
in the brackets correspond
326

326

00:14:11.700  -->  00:14:14.940
to the indexes of the ratings that are existent.
327

327

00:14:14.940  -->  00:14:19.080
So basically we need to just copy this to get in vk
328

328

00:14:19.080  -->  00:14:22.470
the same indexes corresponding to the ratings
329

329

00:14:22.470  -->  00:14:24.060
that are existent.
330

330

00:14:24.060  -->  00:14:25.550
So that's just to be consistent
331

331

00:14:25.550  -->  00:14:29.070
with what we did actually here,
332

332

00:14:29.070  -->  00:14:32.720
to include only the existent ratings in the training.
333

333

00:14:32.720  -->  00:14:33.553
All right!
334

334

00:14:33.553  -->  00:14:35.050
So we're getting close to the end!
335

335

00:14:35.050  -->  00:14:37.320
We have two more lines of code to go.
336

336

00:14:37.320  -->  00:14:40.320
The first one is to update the counter
337

337

00:14:40.320  -->  00:14:43.780
because the counter is just to normalize the train loss.
338

338

00:14:43.780  -->  00:14:48.780
And so here we just need to increment it by one in float.
339

339

00:14:50.140  -->  00:14:53.530
And then, the last line of code is
340

340

00:14:53.530  -->  00:14:56.510
to print what is going to happen in the training.
341

341

00:14:56.510  -->  00:14:59.480
You know, when we execute the code we would like
342

342

00:14:59.480  -->  00:15:00.630
to see what happens.
343

343

00:15:00.630  -->  00:15:04.080
And what we would like to see is the number of the epoch,
344

344

00:15:04.080  -->  00:15:07.080
you know, to see in which epoch we are during the training.
345

345

00:15:07.080  -->  00:15:10.110
And then for these epoch we want to see the loss,
346

346

00:15:10.110  -->  00:15:11.380
how it's decreasing.
347

347

00:15:11.380  -->  00:15:14.930
This will get us a great idea of how the training is doing.
348

348

00:15:14.930  -->  00:15:18.240
All right so we're gonna use the print function
349

349

00:15:18.240  -->  00:15:21.970
and this print function is included in the for loop,
350

350

00:15:21.970  -->  00:15:23.570
looping through all the epochs
351

351

00:15:23.570  -->  00:15:26.460
because we are going to print this at each epoch.
352

352

00:15:26.460  -->  00:15:27.680
All right so in this print function,
353

353

00:15:27.680  -->  00:15:31.980
we're gonna start with a string which is going to be epoch,
354

354

00:15:31.980  -->  00:15:35.860
all right, then colon, and then a space
355

355

00:15:35.860  -->  00:15:37.070
and I close the string.
356

356

00:15:37.070  -->  00:15:39.820
Because after the string, I'm going to put the epoch
357

357

00:15:39.820  -->  00:15:41.830
where we are at in the training.
358

358

00:15:41.830  -->  00:15:43.410
And so I am adding a plus
359

359

00:15:43.410  -->  00:15:45.750
because that's how we concatenate two strings.
360

360

00:15:45.750  -->  00:15:48.420
You know we use a plus to separate two strings,
361

361

00:15:48.420  -->  00:15:50.370
that is this first string epoch
362

362

00:15:50.370  -->  00:15:52.330
and a second string
363

363

00:15:52.330  -->  00:15:54.680
that I'm getting with the str function
364

364

00:15:54.680  -->  00:15:58.330
because in this str function I'm inputting the epoch
365

365

00:15:58.330  -->  00:15:59.740
we are at in the training.
366

366

00:15:59.740  -->  00:16:01.300
So the epoch is an integer,
367

367

00:16:01.300  -->  00:16:05.370
but inside this str function, it will become a string.
368

368

00:16:05.370  -->  00:16:08.000
And that is just because we want to concatenate
369

369

00:16:08.000  -->  00:16:09.670
two strings with each other.
370

370

00:16:09.670  -->  00:16:14.100
So str epoch and then plus
371

371

00:16:14.100  -->  00:16:16.870
and now I'm opening a new string.
372

372

00:16:16.870  -->  00:16:17.980
And inside this new string,
373

373

00:16:17.980  -->  00:16:21.610
well first I'm going to add a space to separate
374

374

00:16:21.610  -->  00:16:26.610
the epoch value and what I'm about to add which is loss
375

375

00:16:27.070  -->  00:16:30.830
then colon, then space again to separate the last string
376

376

00:16:30.830  -->  00:16:32.460
to the last result.
377

377

00:16:32.460  -->  00:16:35.720
And now I have to get out of the string that is just
378

378

00:16:35.720  -->  00:16:38.380
for the last string that I just added.
379

379

00:16:38.380  -->  00:16:41.890
And here again, plus because we're concatenating the strings
380

380

00:16:41.890  -->  00:16:45.750
plus again str to only have strings.
381

381

00:16:45.750  -->  00:16:49.143
And then str I'm putting the train loss,
382

382

00:16:50.260  -->  00:16:52.860
but that I want to normalize.
383

383

00:16:52.860  -->  00:16:57.690
And to normalize it, I have to divide it by s to counter.
384

384

00:16:57.690  -->  00:17:02.170
All right, and that will print the epoch where we are at
385

385

00:17:02.170  -->  00:17:05.070
in the training and the associated loss,
386

386

00:17:05.070  -->  00:17:08.220
actually the normalized train loss.
387

387

00:17:08.220  -->  00:17:09.053
Perfect!
388

388

00:17:09.053  -->  00:17:12.730
So now I think the training is ready to be executed,
389

389

00:17:12.730  -->  00:17:14.710
I hope we didn't make any mistake
390

390

00:17:14.710  -->  00:17:16.460
in all these lines of code.
391

391

00:17:16.460  -->  00:17:18.370
Well, let's check it out.
392

392

00:17:18.370  -->  00:17:19.920
Well first we don't have any warnings,
393

393

00:17:19.920  -->  00:17:21.050
that's a good sign.
394

394

00:17:21.050  -->  00:17:22.220
So let's try it.
395

395

00:17:22.220  -->  00:17:25.460
I'm going to select all these lines.
396

396

00:17:25.460  -->  00:17:29.240
And let's execute.
397

397

00:17:29.240  -->  00:17:32.560
All right, here we go, it seems to be okay.
398

398

00:17:32.560  -->  00:17:34.470
The training is happening.
399

399

00:17:34.470  -->  00:17:39.470
And we end up with a train loss of 0.25 which is pretty good
400

400

00:17:40.580  -->  00:17:43.400
because that means that in the training set,
401

401

00:17:43.400  -->  00:17:45.860
well we get the correct predictive rating,
402

402

00:17:45.860  -->  00:17:48.090
three times out of four.
403

403

00:17:48.090  -->  00:17:50.170
One time out of four we make a mistake
404

404

00:17:50.170  -->  00:17:53.510
when predicting the ratings of the movies by all our users.
405

405

00:17:53.510  -->  00:17:54.700
So that's pretty good,
406

406

00:17:54.700  -->  00:17:57.130
but let's not scream victory yet.
407

407

00:17:57.130  -->  00:18:00.550
We need to evaluate our model on new observations
408

408

00:18:00.550  -->  00:18:04.050
and that is what the test set is for.
409

409

00:18:04.050  -->  00:18:07.400
So in the next tutorial we will do the same on the test set,
410

410

00:18:07.400  -->  00:18:09.460
we will make our predictions on the test set
411

411

00:18:09.460  -->  00:18:12.080
without doing any training, of course.
412

412

00:18:12.080  -->  00:18:14.570
And same, we will compute a test loss,
413

413

00:18:14.570  -->  00:18:17.430
which will be the same loss, that absolute distance,
414

414

00:18:17.430  -->  00:18:19.840
well actually the mean of the absolute distance is
415

415

00:18:19.840  -->  00:18:22.590
between the predictions and the ratings.
416

416

00:18:22.590  -->  00:18:26.560
And so what we're hoping for is to get a test loss
417

417

00:18:26.560  -->  00:18:29.680
that is around this 0.25 value.
418

418

00:18:29.680  -->  00:18:33.100
If it is around it, that means that even on new observations
419

419

00:18:33.100  -->  00:18:36.400
well we predict correctly, three ratings out of four.
420

420

00:18:36.400  -->  00:18:38.080
So that would be amazing.
421

421

00:18:38.080  -->  00:18:40.700
So we will find out about that in the next tutorial
422

422

00:18:40.700  -->  00:18:43.120
and until then enjoy deep learning!
