1
1

00:00:00,760  -->  00:00:01,620
<v Instructor>Hello, and welcome back</v>
2

2

00:00:01,620  -->  00:00:03,160
to the course on deep learning.
3

3

00:00:03,160  -->  00:00:05,340
Now that we've seen neural networks in action,
4

4

00:00:05,340  -->  00:00:08,360
it's time for us to find out how they learn.
5

5

00:00:08,360  -->  00:00:10,390
So, let's dive straight into it.
6

6

00:00:10,390  -->  00:00:13,730
There are two fundamentally different approaches
7

7

00:00:13,730  -->  00:00:15,010
to getting a program to do
8

8

00:00:15,010  -->  00:00:16,140
what you want it to do.
9

9

00:00:16,140  -->  00:00:19,840
One is hard-coded coding, where you actually
10

10

00:00:19,840  -->  00:00:23,670
tell the program specific rules and
11

11

00:00:23,670  -->  00:00:25,980
what outcomes you want, and you just
12

12

00:00:25,980  -->  00:00:27,620
guide it throughout the whole way,
13

13

00:00:27,620  -->  00:00:30,450
and you account for all the possible options
14

14

00:00:30,450  -->  00:00:33,200
that the program has to deal with.
15

15

00:00:33,200  -->  00:00:34,830
On the other hand, you have neural networks
16

16

00:00:34,830  -->  00:00:37,170
where you create a
17

17

00:00:38,650  -->  00:00:41,650
facility for the program to be able to
18

18

00:00:41,650  -->  00:00:43,430
understand what it is doing on it's own,
19

19

00:00:43,430  -->  00:00:45,980
so you basically create this neural network
20

20

00:00:45,980  -->  00:00:48,440
where you provide it inputs,
21

21

00:00:48,440  -->  00:00:50,030
you tell it which one has outputs,
22

22

00:00:50,030  -->  00:00:53,290
and then you let it figure everything out on its own.
23

23

00:00:53,290  -->  00:00:55,890
Two fundamentally different approaches,
24

24

00:00:55,890  -->  00:00:59,080
and that is something to keep in mind
25

25

00:00:59,080  -->  00:01:00,760
as we go through these tutorials.
26

26

00:01:00,760  -->  00:01:04,120
Our goal is to create this network
27

27

00:01:04,120  -->  00:01:06,210
which then learns on its own.
28

28

00:01:06,210  -->  00:01:08,240
We're going to avoid
29

29

00:01:08,240  -->  00:01:11,350
trying to put in the rules.
30

30

00:01:11,350  -->  00:01:14,350
A good example that I can give you right now is
31

31

00:01:14,350  -->  00:01:16,400
this will come further in the course, but
32

32

00:01:16,400  -->  00:01:18,020
it's just a very visual example.
33

33

00:01:18,020  -->  00:01:20,240
For instance, how do you distinguish
34

34

00:01:20,240  -->  00:01:21,680
between a dog and a cat?
35

35

00:01:21,680  -->  00:01:24,310
For on the left side, on the project
36

36

00:01:24,310  -->  00:01:25,590
that's depicted on the left,
37

37

00:01:25,590  -->  00:01:28,470
you would program things in like,
38

38

00:01:28,470  -->  00:01:30,470
the cat's ears have to be like this,
39

39

00:01:30,470  -->  00:01:34,070
look out for whiskers, look out for this type of nose,
40

40

00:01:34,070  -->  00:01:37,710
look out for this type of shape of face,
41

41

00:01:37,710  -->  00:01:40,580
look out for these colors, you describe all these things
42

42

00:01:40,580  -->  00:01:41,573
and you'd have conditions, like
43

43

00:01:41,573  -->  00:01:44,560
if the ears are pointy, then cat.
44

44

00:01:44,560  -->  00:01:48,290
If the ears are slopping down, then possibly dog,
45

45

00:01:48,290  -->  00:01:49,480
and so on.
46

46

00:01:49,480  -->  00:01:50,840
On the other hand, for a neural network,
47

47

00:01:50,840  -->  00:01:53,020
you'd just code the neural network,
48

48

00:01:53,020  -->  00:01:54,820
so you'd code the architecture,
49

49

00:01:54,820  -->  00:01:56,630
and then you point the neural network
50

50

00:01:56,630  -->  00:01:59,670
at a folder of all these cats and dogs
51

51

00:01:59,670  -->  00:02:00,950
with images of cats and dogs,
52

52

00:02:00,950  -->  00:02:02,590
which are already categorized,
53

53

00:02:02,590  -->  00:02:03,949
and you tell it, "Okay.
54

54

00:02:03,949  -->  00:02:06,707
"I've got some images of cats and dogs.
55

55

00:02:06,707  -->  00:02:08,707
"Go and learn what a cat is.
56

56

00:02:08,707  -->  00:02:10,470
"Go and learn what a dog is."
57

57

00:02:10,470  -->  00:02:12,980
And the neural network will, on its own,
58

58

00:02:12,980  -->  00:02:15,160
understand everything it needs to understand,
59

59

00:02:15,160  -->  00:02:17,430
and then, further down, once it's trained up,
60

60

00:02:17,430  -->  00:02:19,770
when you give it a new image of a cat or dog,
61

61

00:02:19,770  -->  00:02:21,500
it'll be able to understand what it was.
62

62

00:02:21,500  -->  00:02:23,080
So, there they are.
63

63

00:02:23,080  -->  00:02:25,570
Those are the two fundamentally different approaches.
64

64

00:02:25,570  -->  00:02:27,640
And today, we're going to slowly start
65

65

00:02:27,640  -->  00:02:30,990
getting into how that second approach works.
66

66

00:02:30,990  -->  00:02:33,270
Alright, so let's get straight to it.
67

67

00:02:33,270  -->  00:02:35,650
Here we have a very basic neural network
68

68

00:02:35,650  -->  00:02:37,290
with one layer.
69

69

00:02:37,290  -->  00:02:40,560
This is called a single layer feedforward neural network,
70

70

00:02:40,560  -->  00:02:42,670
and it is also called a perceptron.
71

71

00:02:42,670  -->  00:02:44,250
Now, before we proceed, one thing that
72

72

00:02:44,250  -->  00:02:47,270
we do need to adjust is that output value.
73

73

00:02:47,270  -->  00:02:49,260
Right now, we can see that it's just a y.
74

74

00:02:49,260  -->  00:02:51,020
We need to put a y hat in there,
75

75

00:02:51,020  -->  00:02:52,360
and the reason for that is
76

76

00:02:52,360  -->  00:02:55,441
usually y stands for the actual value,
77

77

00:02:55,441  -->  00:02:56,380
and that's what we're going to be using.
78

78

00:02:56,380  -->  00:02:58,330
So, y is going to be the actual value,
79

79

00:02:58,330  -->  00:03:01,130
which we see in reality.
80

80

00:03:01,130  -->  00:03:03,610
Output value is the predicted value
81

81

00:03:03,610  -->  00:03:05,770
by the algorithm, by the neural network.
82

82

00:03:05,770  -->  00:03:09,540
Y hat is the output value, basically.
83

83

00:03:09,540  -->  00:03:11,630
That's the denomination for the output value.
84

84

00:03:11,630  -->  00:03:15,260
And the perceptron was first invented in 1957
85

85

00:03:15,260  -->  00:03:17,340
by Frank Rosenblatt,
86

86

00:03:17,340  -->  00:03:20,060
and his whole idea was to create
87

87

00:03:20,060  -->  00:03:25,060
something that can actually learn, and adjust itself.
88

88

00:03:25,100  -->  00:03:27,900
And this is what we're going to be looking at now.
89

89

00:03:27,900  -->  00:03:30,170
So, we've got our perceptron.
90

90

00:03:30,170  -->  00:03:31,950
Let's see how a perceptron learns.
91

91

00:03:31,950  -->  00:03:34,890
So, let's say we have some input values
92

92

00:03:34,890  -->  00:03:37,480
that have been supplied to the perceptron,
93

93

00:03:37,480  -->  00:03:40,220
and, basically to our neural network,
94

94

00:03:40,220  -->  00:03:44,070
then the excavation functions apply,
95

95

00:03:44,070  -->  00:03:46,190
we have an output, and now we're going to
96

96

00:03:46,190  -->  00:03:49,100
plot the output on a chart.
97

97

00:03:49,100  -->  00:03:51,700
So, there it is, our output y hat.
98

98

00:03:51,700  -->  00:03:53,780
Now, what we need to do is
99

99

00:03:53,780  -->  00:03:54,870
in order to be able to learn,
100

100

00:03:54,870  -->  00:03:56,790
we need to compare the output value
101

101

00:03:56,790  -->  00:03:59,460
to the actual value that we want
102

102

00:03:59,460  -->  00:04:01,480
the neural network to get, right?
103

103

00:04:01,480  -->  00:04:04,720
And that is the value y.
104

104

00:04:04,720  -->  00:04:05,840
And so if we plot it here,
105

105

00:04:05,840  -->  00:04:08,220
you'll see that there's a bit of a difference.
106

106

00:04:08,220  -->  00:04:10,100
Now, we're going to calculate a function
107

107

00:04:10,100  -->  00:04:11,820
called the cost function, it's calculated
108

108

00:04:11,820  -->  00:04:14,260
as one half of the squared difference
109

109

00:04:14,260  -->  00:04:17,090
between the actual value and the output value.
110

110

00:04:17,090  -->  00:04:19,460
Now, there are many ways you can
111

111

00:04:19,460  -->  00:04:20,420
come up for cost function.
112

112

00:04:20,420  -->  00:04:21,990
There are many different cost functions
113

113

00:04:21,990  -->  00:04:23,140
that you can use.
114

114

00:04:23,140  -->  00:04:24,900
This is probably the most commonly
115

115

00:04:24,900  -->  00:04:26,050
used cost function, and
116

116

00:04:27,060  -->  00:04:30,237
why it is specifically this function,
117

117

00:04:30,237  -->  00:04:31,650
you will find out further down,
118

118

00:04:31,650  -->  00:04:34,210
when we are talking about gradient descent,
119

119

00:04:34,210  -->  00:04:36,233
but for now, we're just going to agree
120

120

00:04:36,233  -->  00:04:38,230
that this is the cost function, and basically
121

121

00:04:38,230  -->  00:04:40,170
what the cost function is telling us is
122

122

00:04:40,170  -->  00:04:44,190
what is the error that you have in your prediction,
123

123

00:04:44,190  -->  00:04:47,020
and our goal is to minimize the cost function,
124

124

00:04:47,020  -->  00:04:49,360
because the lower the cost function,
125

125

00:04:49,360  -->  00:04:52,040
the closer the y hat is to y.
126

126

00:04:52,040  -->  00:04:54,360
Okay, so as long as we agree on that, let's proceed.
127

127

00:04:54,360  -->  00:04:57,170
So, basically, from here, what happens is
128

128

00:04:57,170  -->  00:04:58,370
there is our cost function,
129

129

00:04:58,370  -->  00:04:59,970
and from here, what happens is
130

130

00:04:59,970  -->  00:05:03,020
now we're going to, once we've compared,
131

131

00:05:03,020  -->  00:05:05,610
now we're going to feed this information
132

132

00:05:05,610  -->  00:05:08,880
back into the neural network.
133

133

00:05:08,880  -->  00:05:11,130
So, there we go, there's the information
134

134

00:05:11,130  -->  00:05:12,880
going back into the neural network,
135

135

00:05:12,880  -->  00:05:14,100
and it goes to the weights,
136

136

00:05:14,100  -->  00:05:15,600
and the weights get updated.
137

137

00:05:15,600  -->  00:05:17,950
Basically, the only thing that we have control of
138

138

00:05:17,950  -->  00:05:20,850
in this very simple neural network are the weights.
139

139

00:05:20,850  -->  00:05:22,593
W1, W2, all the way to Wm.
140

140

00:05:23,961  -->  00:05:26,690
And our goal is to minimize the cost function,
141

141

00:05:26,690  -->  00:05:29,380
so all we can do is update the weight.
142

142

00:05:29,380  -->  00:05:30,713
So, we update the weights,
143

143

00:05:32,130  -->  00:05:33,810
tweak them a little bit,
144

144

00:05:33,810  -->  00:05:36,120
and how exactly, we'll find out further down,
145

145

00:05:36,120  -->  00:05:38,840
but for now, we agree that we update the weights,
146

146

00:05:38,840  -->  00:05:40,350
and then we continue.
147

147

00:05:40,350  -->  00:05:43,370
But here, I've put up this screenshot
148

148

00:05:43,370  -->  00:05:47,290
of the data, just to make one point very clear,
149

149

00:05:47,290  -->  00:05:50,350
that right now, throughout this whole experiment,
150

150

00:05:50,350  -->  00:05:51,890
everything we're doing right now,
151

151

00:05:51,890  -->  00:05:53,930
we're dealing with just the one row.
152

152

00:05:53,930  -->  00:05:57,070
So, we have a denizen of one row,
153

153

00:05:57,070  -->  00:05:58,860
where we have, for instance,
154

154

00:05:58,860  -->  00:06:01,740
we're dealing with how long you study,
155

155

00:06:01,740  -->  00:06:04,965
and the variable that we're predicting is
156

156

00:06:04,965  -->  00:06:08,290
what results you're gonna get on an exam,
157

157

00:06:08,290  -->  00:06:10,550
and the independent variables that we have
158

158

00:06:10,550  -->  00:06:12,610
is how many hours did you study for,
159

159

00:06:12,610  -->  00:06:13,900
how many hours did you sleep,
160

160

00:06:13,900  -->  00:06:15,350
and what did you get on the quiz
161

161

00:06:15,350  -->  00:06:16,710
in the mid-semester?
162

162

00:06:16,710  -->  00:06:18,450
So, in the middle of the semester,
163

163

00:06:18,450  -->  00:06:20,300
there's a quiz, what percentage did you get there?
164

164

00:06:20,300  -->  00:06:22,550
Based on those variables, we're trying to predict
165

165

00:06:22,550  -->  00:06:24,590
what score you'll get for the exam.
166

166

00:06:24,590  -->  00:06:26,710
And in an exam, the 93 percent,
167

167

00:06:26,710  -->  00:06:29,480
that's the actual value, so that's y.
168

168

00:06:29,480  -->  00:06:32,490
So, we feed these three values into
169

169

00:06:32,490  -->  00:06:35,680
our neural network again, for the second time now,
170

170

00:06:35,680  -->  00:06:37,560
and then we're going to be comparing
171

171

00:06:37,560  -->  00:06:39,030
the result to y.
172

172

00:06:39,030  -->  00:06:40,690
So, let's see how this works.
173

173

00:06:40,690  -->  00:06:43,730
We feed these values into the neural network,
174

174

00:06:43,730  -->  00:06:46,640
everything gets adjusted, and weights get adjusted.
175

175

00:06:46,640  -->  00:06:48,920
So, as you can see, this is, again.
176

176

00:06:48,920  -->  00:06:50,090
We're going to feed the values,
177

177

00:06:50,090  -->  00:06:51,550
again, the point here is that we're
178

178

00:06:51,550  -->  00:06:53,110
feeding in the same values,
179

179

00:06:53,110  -->  00:06:54,454
so we only have one row.
180

180

00:06:54,454  -->  00:06:56,300
We're training on one row.
181

181

00:06:56,300  -->  00:06:59,510
This is because this is just a very simple, basic example,
182

182

00:06:59,510  -->  00:07:01,670
then we'll see what happens when there's more rows.
183

183

00:07:01,670  -->  00:07:03,920
So, again, we feed these rows in,
184

184

00:07:03,920  -->  00:07:06,090
our cost functions get adjusted,
185

185

00:07:06,090  -->  00:07:10,870
as you can see everything happens along those lines.
186

186

00:07:10,870  -->  00:07:13,610
As you can see every time our y hat is changing,
187

187

00:07:13,610  -->  00:07:14,930
because we've tweaked the weights,
188

188

00:07:14,930  -->  00:07:17,490
our y hat is changing, our cost function is changing,
189

189

00:07:17,490  -->  00:07:19,010
let's have a look again.
190

190

00:07:19,010  -->  00:07:21,380
We feed those in, y hat is changing,
191

191

00:07:21,380  -->  00:07:23,970
cost function is changing, we get information back,
192

192

00:07:23,970  -->  00:07:25,740
feed back to the weights, so that
193

193

00:07:25,740  -->  00:07:26,940
the weights get adjusted again,
194

194

00:07:26,940  -->  00:07:29,680
we feed in the same values every time,
195

195

00:07:29,680  -->  00:07:31,760
everything gets adjusted, goes back to the weights.
196

196

00:07:31,760  -->  00:07:33,110
And one more time.
197

197

00:07:33,110  -->  00:07:34,513
Feed in, okay.
198

198

00:07:35,610  -->  00:07:38,430
And another time, so we've adjusted the weights,
199

199

00:07:38,430  -->  00:07:39,653
we feed in information,
200

200

00:07:40,700  -->  00:07:42,353
and there we go.
201

201

00:07:42,353  -->  00:07:44,680
And now, this time, the y hat is equal to y,
202

202

00:07:44,680  -->  00:07:45,880
cost function is zero.
203

203

00:07:45,880  -->  00:07:48,340
Usually, you won't get cost function equal to zero,
204

204

00:07:48,340  -->  00:07:50,660
but this is a very simple example.
205

205

00:07:50,660  -->  00:07:52,730
So, hopefully, all that made sense.
206

206

00:07:52,730  -->  00:07:56,180
Every time we feed in exactly that same row,
207

207

00:07:56,180  -->  00:07:57,790
because, just in this case, we're just
208

208

00:07:57,790  -->  00:07:59,700
dealing with that one row,
209

209

00:07:59,700  -->  00:08:02,363
into our neural network, where then,
210

210

00:08:03,817  -->  00:08:05,600
the values get multiplied by the weights,
211

211

00:08:05,600  -->  00:08:06,900
the activation function is applied,
212

212

00:08:06,900  -->  00:08:10,200
we get y hat, y hat is compared to y,
213

213

00:08:10,200  -->  00:08:12,270
then we see how the cost function is changed.
214

214

00:08:12,270  -->  00:08:14,904
Feed that information back to the neural network,
215

215

00:08:14,904  -->  00:08:17,330
and adjust the weights again.
216

216

00:08:17,330  -->  00:08:19,470
And then we repeat the same process again
217

217

00:08:19,470  -->  00:08:21,510
with the same exact row.
218

218

00:08:21,510  -->  00:08:23,390
We're trying to minimize that cost function.
219

219

00:08:23,390  -->  00:08:25,660
So, up until now, we've been dealing with
220

220

00:08:25,660  -->  00:08:26,920
just that one row.
221

221

00:08:26,920  -->  00:08:29,370
Let's see what happens when you have multiple rows.
222

222

00:08:29,370  -->  00:08:31,250
So, here's the full data set.
223

223

00:08:31,250  -->  00:08:35,270
We have eight rows of how many hours you slept,
224

224

00:08:35,270  -->  00:08:38,170
or maybe these are different students
225

225

00:08:38,170  -->  00:08:41,210
taking the same exam, how many hours they studied,
226

226

00:08:41,210  -->  00:08:43,170
how many hours they slept before the exam,
227

227

00:08:43,170  -->  00:08:44,400
what did they get on the quiz,
228

228

00:08:44,400  -->  00:08:47,370
and their final result on the test.
229

229

00:08:47,370  -->  00:08:49,240
And as you can see here, on the left,
230

230

00:08:49,240  -->  00:08:51,810
I've got eight of these perceptrons.
231

231

00:08:51,810  -->  00:08:54,670
Actually, they are all the same perceptron,
232

232

00:08:54,670  -->  00:08:55,930
so this is also important to say,
233

233

00:08:55,930  -->  00:08:58,630
I just multiplied it, or
234

234

00:08:58,630  -->  00:09:01,250
duplicated eight times, just so that we can
235

235

00:09:03,260  -->  00:09:05,340
conceptually understand, but the important thing here,
236

236

00:09:05,340  -->  00:09:06,680
it's the same neural network.
237

237

00:09:06,680  -->  00:09:08,980
We're going to be feeding these into the
238

238

00:09:08,980  -->  00:09:10,900
one same neural network, so let's go.
239

239

00:09:10,900  -->  00:09:12,170
Let's get started.
240

240

00:09:12,170  -->  00:09:17,160
One epoch, as you'll hear Hadelin mentioning,
241

241

00:09:17,160  -->  00:09:20,500
one epoch is when we go through a whole data set,
242

242

00:09:20,500  -->  00:09:23,840
and we train our neural network
243

243

00:09:23,840  -->  00:09:26,240
on all of these rows.
244

244

00:09:26,240  -->  00:09:27,690
So, let's go, let's get started.
245

245

00:09:27,690  -->  00:09:29,690
There's our first row, and there's
246

246

00:09:29,690  -->  00:09:31,333
y hat for the first row.
247

247

00:09:32,460  -->  00:09:33,860
There's a second row, there's
248

248

00:09:33,860  -->  00:09:35,190
y hat for the second row.
249

249

00:09:35,190  -->  00:09:37,500
So, again, it's being fed into
250

250

00:09:37,500  -->  00:09:39,500
the same neural network every time.
251

251

00:09:39,500  -->  00:09:41,110
I just copied them several times,
252

252

00:09:41,110  -->  00:09:43,943
so we can visually see how this is happening.
253

253

00:09:44,960  -->  00:09:47,800
Then, again, it's happening again,
254

254

00:09:47,800  -->  00:09:50,570
that's third row, fourth row,
255

255

00:09:50,570  -->  00:09:53,680
there's our y hat for the fourth row, and so on, basically.
256

256

00:09:53,680  -->  00:09:55,170
Then we get the same values for
257

257

00:09:55,170  -->  00:09:56,880
the remaining four rows, as well.
258

258

00:09:56,880  -->  00:09:59,150
Every time, we just feed in a row into
259

259

00:10:00,350  -->  00:10:02,573
our neural network, we get a value.
260

260

00:10:03,610  -->  00:10:06,900
Then, we compare to the actual value.
261

261

00:10:06,900  -->  00:10:08,640
So, there are the actual values.
262

262

00:10:08,640  -->  00:10:11,530
So, for every single row, we have an actual value.
263

263

00:10:11,530  -->  00:10:13,920
And now, based on all of these
264

264

00:10:13,920  -->  00:10:16,120
differences between y hat and y,
265

265

00:10:16,120  -->  00:10:18,220
we can calculate the cost function,
266

266

00:10:18,220  -->  00:10:20,750
which is the sum of all of those
267

267

00:10:22,130  -->  00:10:26,180
squared differences between y hat and y and
268

268

00:10:26,180  -->  00:10:27,233
all that is halved.
269

269

00:10:28,110  -->  00:10:30,240
And there's a cost function.
270

270

00:10:30,240  -->  00:10:31,610
And basically now, what we do
271

271

00:10:31,610  -->  00:10:34,230
after we have the full cost function,
272

272

00:10:34,230  -->  00:10:37,060
we go back and we update the weights.
273

273

00:10:37,060  -->  00:10:39,410
We update W1, W2, W3.
274

274

00:10:39,410  -->  00:10:41,290
And the important thing to remember here is that
275

275

00:10:41,290  -->  00:10:42,320
all of these
276

276

00:10:44,030  -->  00:10:45,730
perceptrons, or all of these neural networks
277

277

00:10:45,730  -->  00:10:47,270
is actually one neural network,
278

278

00:10:47,270  -->  00:10:49,570
so there's not eight of them, there's just one.
279

279

00:10:49,570  -->  00:10:51,470
And when we update the weights,
280

280

00:10:51,470  -->  00:10:53,130
we're going to update the weights
281

281

00:10:53,130  -->  00:10:55,000
in that one neural network, so basically,
282

282

00:10:55,000  -->  00:10:56,280
the weights are going to be the same
283

283

00:10:56,280  -->  00:10:57,633
for all of the rows.
284

284

00:10:58,560  -->  00:10:59,700
It's not the case that every row
285

285

00:10:59,700  -->  00:11:00,660
has its own weights, no.
286

286

00:11:00,660  -->  00:11:02,790
All the rows share the weights.
287

287

00:11:02,790  -->  00:11:06,280
And so, that's why we looked at the cost function,
288

288

00:11:06,280  -->  00:11:10,230
which is the sum of the squared differences,
289

289

00:11:10,230  -->  00:11:11,860
and then we updated the weights.
290

290

00:11:11,860  -->  00:11:15,190
And now, from here, that was just one iteration.
291

291

00:11:15,190  -->  00:11:18,920
Next, we're going to run this whole thing again.
292

292

00:11:18,920  -->  00:11:22,240
We're going to feed every single row
293

293

00:11:22,240  -->  00:11:24,960
into the neural network, find out our cost function,
294

294

00:11:24,960  -->  00:11:26,700
and do this whole process again.
295

295

00:11:26,700  -->  00:11:28,460
Just as we showed previously,
296

296

00:11:28,460  -->  00:11:31,400
where we had just one row, and we were doing everything
297

297

00:11:31,400  -->  00:11:32,780
again and again, again, again,
298

298

00:11:32,780  -->  00:11:34,210
same thing here, but now we're going to be
299

299

00:11:34,210  -->  00:11:35,080
doing it for eight rows,
300

300

00:11:35,080  -->  00:11:37,510
or 800 rows, or 8000 rows.
301

301

00:11:37,510  -->  00:11:40,710
How many rows you have in your data set,
302

302

00:11:40,710  -->  00:11:42,280
you do this process, and then you
303

303

00:11:42,280  -->  00:11:44,090
calculate the cost function.
304

304

00:11:44,090  -->  00:11:48,340
And the goal here is to minimize the cost function
305

305

00:11:48,340  -->  00:11:50,800
and to get, as soon as you've found
306

306

00:11:50,800  -->  00:11:51,930
the minimum of the cost function,
307

307

00:11:51,930  -->  00:11:54,280
that is your final neural network,
308

308

00:11:54,280  -->  00:11:56,630
that means your weights have been adjusted
309

309

00:11:56,630  -->  00:12:00,640
and you have found
310

310

00:12:00,640  -->  00:12:03,250
the optimal weights
311

311

00:12:03,250  -->  00:12:07,090
for this data set that you're training on,
312

312

00:12:07,090  -->  00:12:09,260
and you're ready to proceed to the testing phase,
313

313

00:12:09,260  -->  00:12:11,450
or to the application phase.
314

314

00:12:11,450  -->  00:12:14,880
And this whole process is called backpropogation.
315

315

00:12:14,880  -->  00:12:17,030
So, some additional reading that
316

316

00:12:17,030  -->  00:12:20,410
you might want to do for the cost function,
317

317

00:12:20,410  -->  00:12:22,580
and, I know we just talked about one,
318

318

00:12:22,580  -->  00:12:24,760
and there are many different ones,
319

319

00:12:24,760  -->  00:12:28,540
a good article is located on Cross Validated.
320

320

00:12:28,540  -->  00:12:30,537
It's called "a list of cost functions used
321

321

00:12:30,537  -->  00:12:32,970
"in neural networks alongside applications."
322

322

00:12:32,970  -->  00:12:35,150
So, the URL is there, but you can just
323

323

00:12:35,150  -->  00:12:37,990
google for that exact search term,
324

324

00:12:37,990  -->  00:12:40,810
or search phrase, and this one will be
325

325

00:12:40,810  -->  00:12:42,030
the first one that pops up.
326

326

00:12:42,030  -->  00:12:45,840
It's actually got some good examples and application,
327

327

00:12:45,840  -->  00:12:48,350
or use cases for different cost functions,
328

328

00:12:48,350  -->  00:12:49,300
so if you're interested to learn
329

329

00:12:49,300  -->  00:12:51,870
more about cost functions, check out this article,
330

330

00:12:51,870  -->  00:12:54,310
and on that note, I hope you enjoyed today's tutorial.
331

331

00:12:54,310  -->  00:12:55,960
I look forward to seeing you next time.
332

332

00:12:55,960  -->  00:12:58,113
Until then, enjoy deep learning.
