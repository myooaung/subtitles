WEBVTT
1
1

00:00:00.300  -->  00:00:02.410
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.410  -->  00:00:04.020
So, in the last section of the course,
3

3

00:00:04.020  -->  00:00:07.090
we trained our artificial neural network twice
4

4

00:00:07.090  -->  00:00:08.409
and we noticed that the second time,
5

5

00:00:08.409  -->  00:00:10.810
we obtained a lower accuracy,
6

6

00:00:10.810  -->  00:00:13.210
both on the training set and the test set
7

7

00:00:13.210  -->  00:00:14.570
than the first time.
8

8

00:00:14.570  -->  00:00:18.400
And so, that introduces us to the Bias-Variance Tradeoff.
9

9

00:00:18.400  -->  00:00:20.450
The Bias-Variance Tradeoff is the fact that
10

10

00:00:20.450  -->  00:00:22.710
we're trying to train a model
11

11

00:00:22.710  -->  00:00:24.700
that will not only be accurate,
12

12

00:00:24.700  -->  00:00:27.420
but also that should not have too much variance of accuracy,
13

13

00:00:27.420  -->  00:00:29.290
when we train it several times.
14

14

00:00:29.290  -->  00:00:31.740
And what happened for our artificial neural network?
15

15

00:00:31.740  -->  00:00:33.036
Well, we trained it twice
16

16

00:00:33.036  -->  00:00:35.880
and we obtained two different accuracies,
17

17

00:00:35.880  -->  00:00:38.910
86% first and then 84%,
18

18

00:00:38.910  -->  00:00:40.500
and so now we are not very sure
19

19

00:00:40.500  -->  00:00:42.710
which one of these two accuracies we should take
20

20

00:00:42.710  -->  00:00:45.050
to evaluate our models performance.
21

21

00:00:45.050  -->  00:00:49.010
And so, we need to optimize our way to evaluate our models.
22

22

00:00:49.010  -->  00:00:50.700
Because so far what we did
23

23

00:00:50.700  -->  00:00:53.200
is split our data set between a training set
24

24

00:00:53.200  -->  00:00:54.320
and a test set
25

25

00:00:54.320  -->  00:00:56.830
and you know we trained our model on the training set
26

26

00:00:56.830  -->  00:00:59.670
and we tested it's performance on the test set.
27

27

00:00:59.670  -->  00:01:02.613
That's a correct way of evaluating the model performance,
28

28

00:01:02.613  -->  00:01:04.080
but that's not the best one.
29

29

00:01:04.080  -->  00:01:06.810
Because, we actual have the Variance Problem.
30

30

00:01:06.810  -->  00:01:09.730
The Variance problem can be explained by the fact that
31

31

00:01:09.730  -->  00:01:12.000
when we get the accuracy on the test set,
32

32

00:01:12.000  -->  00:01:13.850
well, if we run the model again
33

33

00:01:13.850  -->  00:01:16.700
and test again it's performance on another test set,
34

34

00:01:16.700  -->  00:01:19.470
well, we can get a very different accuracy.
35

35

00:01:19.470  -->  00:01:23.550
So, judging our model performance only on one accuracy
36

36

00:01:23.550  -->  00:01:24.720
on one test set,
37

37

00:01:24.720  -->  00:01:26.390
is actually not super relevant.
38

38

00:01:26.390  -->  00:01:28.220
That's not the most relevant way,
39

39

00:01:28.220  -->  00:01:30.110
to evaluate the model performance.
40

40

00:01:30.110  -->  00:01:31.812
And so there is this technique called
41

41

00:01:31.812  -->  00:01:35.470
K-Fold Cross Validation that improves this a lot.
42

42

00:01:35.470  -->  00:01:38.198
Because, that will fix this variance problem.
43

43

00:01:38.198  -->  00:01:40.000
And how will it fix it?
44

44

00:01:40.000  -->  00:01:43.480
It will fix it by splitting the training set into 10 folds
45

45

00:01:43.480  -->  00:01:46.640
when K equals 10, and most of the time, K equals 10.
46

46

00:01:46.640  -->  00:01:49.050
And we train our model on 9-folds
47

47

00:01:49.050  -->  00:01:51.570
and we test it on the last remaining fold.
48

48

00:01:51.570  -->  00:01:52.950
And since with 10-folds
49

49

00:01:52.950  -->  00:01:56.020
we can make ten different combinations of 9-folds
50

50

00:01:56.020  -->  00:01:58.460
to train a model and 1-fold to test it.
51

51

00:01:58.460  -->  00:02:00.230
That means that we can train the model
52

52

00:02:00.230  -->  00:02:02.640
and test the model on ten combinations
53

53

00:02:02.640  -->  00:02:04.450
of training and test sets.
54

54

00:02:04.450  -->  00:02:06.630
And that will already give us a much better idea
55

55

00:02:06.630  -->  00:02:08.061
of the model performance because,
56

56

00:02:08.061  -->  00:02:10.865
what we can do afterwards, is take an average
57

57

00:02:10.865  -->  00:02:13.599
of the different accuracies of the ten evaluations
58

58

00:02:13.599  -->  00:02:16.000
and also compute the standard deviation
59

59

00:02:16.000  -->  00:02:17.590
to have a look at the variance.
60

60

00:02:17.590  -->  00:02:21.120
So eventually, our analysis will be much more relevant.
61

61

00:02:21.120  -->  00:02:21.953
And besides,
62

62

00:02:21.953  -->  00:02:24.094
we'll know which in these four categories we'll be
63

63

00:02:24.094  -->  00:02:27.620
because, if we get a good accuracy on a small variance,
64

64

00:02:27.620  -->  00:02:29.500
we'll be on the lower left one.
65

65

00:02:29.500  -->  00:02:31.620
If we get a large accuracy and a high variance,
66

66

00:02:31.620  -->  00:02:33.980
we will be on the lower right one.
67

67

00:02:33.980  -->  00:02:36.470
If we get a small accuracy and a low variance,
68

68

00:02:36.470  -->  00:02:38.200
we will be on the upper left one
69

69

00:02:38.200  -->  00:02:40.950
and eventually if we get a low accuracy and a high variance
70

70

00:02:40.950  -->  00:02:42.810
we will be on the upper right one.
71

71

00:02:42.810  -->  00:02:46.020
So, this K-fold cross validation is very useful
72

72

00:02:46.020  -->  00:02:50.100
and besides, our performance analysis is much more relevant.
73

73

00:02:50.100  -->  00:02:53.400
Alright, so let's implement here K-fold cross validation.
74

74

00:02:53.400  -->  00:02:55.540
So, as you saw I just restarted my kernel
75

75

00:02:55.540  -->  00:02:59.080
because actually when we run K-fold cross validation
76

76

00:02:59.080  -->  00:03:01.900
well we don't need to re-execute this part
77

77

00:03:01.900  -->  00:03:04.384
because actually we will include this part
78

78

00:03:04.384  -->  00:03:06.730
in K-fold cross validation.
79

79

00:03:06.730  -->  00:03:09.178
However we do need to run the data pre-processing phase
80

80

00:03:09.178  -->  00:03:11.700
because of course we need to import the data set
81

81

00:03:11.700  -->  00:03:14.860
and pre-process it since it will be part of the multiple
82

82

00:03:14.860  -->  00:03:17.110
training in K-fold cross validation.
83

83

00:03:17.110  -->  00:03:20.680
So, we just need to re-execute the code
84

84

00:03:20.680  -->  00:03:22.630
for part 1 data pre-processing.
85

85

00:03:22.630  -->  00:03:23.860
So here we go.
86

86

00:03:23.860  -->  00:03:25.781
Now the data pre-processing is done
87

87

00:03:25.781  -->  00:03:29.150
and we're ready to implement K-fold cross validation
88

88

00:03:29.150  -->  00:03:33.450
and get a much more relevant accuracy on the test set.
89

89

00:03:33.450  -->  00:03:34.610
Alright, so here we go.
90

90

00:03:34.610  -->  00:03:37.660
Let's implement K-fold cross validation.
91

91

00:03:37.660  -->  00:03:39.016
So, what do we start with ?
92

92

00:03:39.016  -->  00:03:41.810
Well the tricky thing is that here,
93

93

00:03:41.810  -->  00:03:44.070
we implemented a model with Keras
94

94

00:03:44.070  -->  00:03:46.630
and the K-fold cross validation function that we'll use,
95

95

00:03:46.630  -->  00:03:48.410
belongs to Scikit-learn,
96

96

00:03:48.410  -->  00:03:50.450
and therefore we need to, in some way,
97

97

00:03:50.450  -->  00:03:53.410
combine Keras and Scikit-learn together.
98

98

00:03:53.410  -->  00:03:55.660
And there is the perfect module for it
99

99

00:03:55.660  -->  00:03:57.250
and that belongs to Keras.
100

100

00:03:57.250  -->  00:03:59.130
Its a Keras wrapper
101

101

00:03:59.130  -->  00:04:02.420
that will wrap K-fold cross validation by Scikit-learn,
102

102

00:04:02.420  -->  00:04:04.390
into the Keras model.
103

103

00:04:04.390  -->  00:04:05.370
So in other words,
104

104

00:04:05.370  -->  00:04:08.090
we will be able to include K-fold cross validation
105

105

00:04:08.090  -->  00:04:09.970
in our Keras classifier.
106

106

00:04:09.970  -->  00:04:11.840
So, the first thing that we need to do
107

107

00:04:11.840  -->  00:04:15.100
is to import all these tools, that is the wrapper
108

108

00:04:15.100  -->  00:04:17.900
and the cross validation function by Scikit-learn,
109

109

00:04:17.900  -->  00:04:19.380
that we will then combine.
110

110

00:04:19.380  -->  00:04:22.570
So here we go, let's start with the Keras wrapper.
111

111

00:04:22.570  -->  00:04:25.560
So this Keras wrapper is called Keras Classifier.
112

112

00:04:25.560  -->  00:04:30.557
So we're gonna start here with import Keras Classifier
113

113

00:04:32.470  -->  00:04:37.266
and this Keras Classifier wrapper is taken from
114

114

00:04:37.266  -->  00:04:42.266
keras.wrappers.scikit-learn.
115

115

00:04:42.410  -->  00:04:43.400
Here we go.
116

116

00:04:43.400  -->  00:04:45.720
That's the Keras Classifier wrapper.
117

117

00:04:45.720  -->  00:04:48.200
So let's try to import it and see if that works.
118

118

00:04:48.200  -->  00:04:49.760
Here we go, that works
119

119

00:04:49.760  -->  00:04:52.260
and we're already using ten (inaudible).
120

120

00:04:52.260  -->  00:04:53.160
Great.
121

121

00:04:53.160  -->  00:04:56.160
Now the second thing we need is of course
122

122

00:04:56.160  -->  00:05:00.260
the K-fold cross validation function from Scikit-learn.
123

123

00:05:00.260  -->  00:05:03.060
And so from Scikit-learn
124

124

00:05:03.060  -->  00:05:04.300
and now we need to go to the module
125

125

00:05:04.300  -->  00:05:08.770
that contains this function which is model selection.
126

126

00:05:08.770  -->  00:05:12.890
We import this cross validation function that is called
127

127

00:05:12.890  -->  00:05:14.680
Cross Val Score.
128

128

00:05:14.680  -->  00:05:15.513
So
129

129

00:05:15.513  -->  00:05:16.346
Cross
130

130

00:05:16.346  -->  00:05:17.210
Val
131

131

00:05:17.210  -->  00:05:18.150
Score.
132

132

00:05:18.150  -->  00:05:20.680
Alright, let's try to import that as well.
133

133

00:05:20.680  -->  00:05:22.740
Here we go, well imported.
134

134

00:05:22.740  -->  00:05:23.590
Perfect.
135

135

00:05:23.590  -->  00:05:25.370
So now we have all the tools we need
136

136

00:05:25.370  -->  00:05:28.060
and so basically we're ready to start implementing
137

137

00:05:28.060  -->  00:05:30.680
K-fold cross validation inside Keras.
138

138

00:05:30.680  -->  00:05:32.780
And to do this were gonna use a function
139

139

00:05:32.780  -->  00:05:36.290
because the Keras classifier that we imported here
140

140

00:05:36.290  -->  00:05:39.380
expects for one of its arguments a function.
141

141

00:05:39.380  -->  00:05:41.860
It's actually its first argument we will see
142

142

00:05:41.860  -->  00:05:44.680
called Build FN, Build Function.
143

143

00:05:44.680  -->  00:05:46.260
And this function is simply a function
144

144

00:05:46.260  -->  00:05:48.650
that returns the classifier that we made here
145

145

00:05:48.650  -->  00:05:50.380
with all this architecture.
146

146

00:05:50.380  -->  00:05:53.560
So basically this function just builds the architecture
147

147

00:05:53.560  -->  00:05:56.170
of our artificial neural network.
148

148

00:05:56.170  -->  00:05:57.003
So here we go.
149

149

00:05:57.003  -->  00:05:59.100
Let's start making this function.
150

150

00:05:59.100  -->  00:06:01.160
And to define a function in Python
151

151

00:06:01.160  -->  00:06:04.520
we start with def, for define
152

152

00:06:04.520  -->  00:06:06.760
and then we give a name to this function
153

153

00:06:06.760  -->  00:06:11.600
and so we're gonna call it build underscore classifier
154

154

00:06:11.600  -->  00:06:13.420
and parenthesis
155

155

00:06:13.420  -->  00:06:15.110
and then column
156

156

00:06:15.110  -->  00:06:18.290
and now by pressing enter we go inside the function
157

157

00:06:18.290  -->  00:06:21.170
and that's where we define what the function will do.
158

158

00:06:21.170  -->  00:06:24.480
Okay, so now basically as I told you this function
159

159

00:06:24.480  -->  00:06:27.530
builds the artificial neural network classifier.
160

160

00:06:27.530  -->  00:06:30.120
And it will build it exactly as we built it
161

161

00:06:30.120  -->  00:06:32.540
here in this section when making the ANN.
162

162

00:06:32.540  -->  00:06:34.670
So basically what we only need to do
163

163

00:06:34.670  -->  00:06:37.400
is take all the lines of code here
164

164

00:06:37.400  -->  00:06:40.120
that builds the artificial neural network
165

165

00:06:40.120  -->  00:06:42.840
but only the part of code that builds the architecture
166

166

00:06:42.840  -->  00:06:44.980
not this fitting part here
167

167

00:06:44.980  -->  00:06:47.060
that trains it with the train set.
168

168

00:06:47.060  -->  00:06:49.770
So what we only need to take is
169

169

00:06:49.770  -->  00:06:52.490
everything from this last line of code
170

170

00:06:52.490  -->  00:06:54.910
to the beginning.
171

171

00:06:54.910  -->  00:06:55.743
Here we go.
172

172

00:06:55.743  -->  00:06:58.250
I'm copying this and I'm gonna put that
173

173

00:06:58.250  -->  00:07:01.100
in my build classifier function.
174

174

00:07:01.100  -->  00:07:04.150
So paste and now we need to do a little bit of cleaning.
175

175

00:07:04.150  -->  00:07:06.720
So first let's remove the comment,
176

176

00:07:06.720  -->  00:07:09.410
we don't need them, it was just to explain this.
177

177

00:07:09.410  -->  00:07:11.360
Then let's take care of the indentation.
178

178

00:07:11.360  -->  00:07:15.080
Like this, then remove this comment again.
179

179

00:07:15.080  -->  00:07:15.913
Alright.
180

180

00:07:16.800  -->  00:07:18.317
Indentation.
181

181

00:07:18.317  -->  00:07:19.150
(typing)
182

182

00:07:19.150  -->  00:07:20.732
Comment, indentation.
183

183

00:07:20.732  -->  00:07:22.815
(typing)
184

184

00:07:23.820  -->  00:07:25.170
Here we go.
185

185

00:07:25.170  -->  00:07:26.070
Almost ready.
186

186

00:07:26.070  -->  00:07:28.930
Compiling the ANN, removing the comment.
187

187

00:07:28.930  -->  00:07:31.260
And indentation, Perfect!
188

188

00:07:31.260  -->  00:07:33.709
Now we've built our classifier but inside
189

189

00:07:33.709  -->  00:07:36.134
this build classifier function.
190

190

00:07:36.134  -->  00:07:39.040
Okay, so the function is almost ready, but that's not over.
191

191

00:07:39.040  -->  00:07:40.980
This function returns something
192

192

00:07:40.980  -->  00:07:43.150
and this something is the classifier.
193

193

00:07:43.150  -->  00:07:46.100
So here we just need to add in the end
194

194

00:07:46.100  -->  00:07:48.714
return classifier.
195

195

00:07:48.714  -->  00:07:49.710
Alright.
196

196

00:07:49.710  -->  00:07:52.790
So now not only this function builds the classifier
197

197

00:07:52.790  -->  00:07:55.490
with the same architecture as we used before,
198

198

00:07:55.490  -->  00:07:57.730
but also it returns it.
199

199

00:07:57.730  -->  00:07:59.240
And now the function is ready.
200

200

00:07:59.240  -->  00:08:01.100
We can move on to the next step
201

201

00:08:01.100  -->  00:08:03.830
which is to wrap the whole thing.
202

202

00:08:03.830  -->  00:08:04.740
Okay.
203

203

00:08:04.740  -->  00:08:08.420
So now what we're gonna do is create a new classifier
204

204

00:08:08.420  -->  00:08:11.130
that will be the global classifier variable
205

205

00:08:11.130  -->  00:08:13.690
because the classifier that we define here
206

206

00:08:13.690  -->  00:08:16.320
is a local variable because it is a variable
207

207

00:08:16.320  -->  00:08:18.040
inside the function.
208

208

00:08:18.040  -->  00:08:21.400
So this variable will only be created inside the function
209

209

00:08:21.400  -->  00:08:24.830
when we execute this code here that defines the function.
210

210

00:08:24.830  -->  00:08:26.780
Then, as soon as the function is defined
211

211

00:08:26.780  -->  00:08:29.170
and we execute the next line of code,
212

212

00:08:29.170  -->  00:08:32.750
well, the local classifier variable will no longer exist.
213

213

00:08:32.750  -->  00:08:35.760
And so now what we can do is create a new classifier
214

214

00:08:35.760  -->  00:08:38.738
that we're gonna also call classifier.
215

215

00:08:38.738  -->  00:08:41.830
And this classifier will be the same classifier
216

216

00:08:41.830  -->  00:08:45.780
as we built here but this will not be trained on
217

217

00:08:45.780  -->  00:08:48.500
the whole training set composed of x train and y train.
218

218

00:08:48.500  -->  00:08:51.350
It will be built through careful cross validation
219

219

00:08:51.350  -->  00:08:53.820
on 10 different training folds
220

220

00:08:53.820  -->  00:08:55.760
and by each time measuring the model performance
221

221

00:08:55.760  -->  00:08:57.710
on one test fold.
222

222

00:08:57.710  -->  00:08:59.976
So the only thing that is different between this classifier
223

223

00:08:59.976  -->  00:09:03.290
and the one above that we built in the previous section,
224

224

00:09:03.290  -->  00:09:06.080
is only in the training part.
225

225

00:09:06.080  -->  00:09:07.670
Alright, so let's do it.
226

226

00:09:07.670  -->  00:09:10.220
Let's build this new classifier train
227

227

00:09:10.220  -->  00:09:11.980
with K-fold cross validation.
228

228

00:09:11.980  -->  00:09:14.120
So classifier equals.
229

229

00:09:14.120  -->  00:09:16.770
And now that's where we're gonna use the Keras classifier,
230

230

00:09:16.770  -->  00:09:20.120
the wrapper of K-fold cross validation.
231

231

00:09:20.120  -->  00:09:22.120
So we take our Keras classifier
232

232

00:09:23.800  -->  00:09:25.040
which by the way is a class,
233

233

00:09:25.040  -->  00:09:27.110
so right now we're creating an object
234

234

00:09:27.110  -->  00:09:29.040
of the Keras classifier class
235

235

00:09:29.040  -->  00:09:33.270
and inside this class we're gonna input the arguments.
236

236

00:09:33.270  -->  00:09:36.250
Okay, so the first argument is, as I mentioned,
237

237

00:09:36.250  -->  00:09:38.970
this function, the build classifier function
238

238

00:09:38.970  -->  00:09:40.440
that builds the architecture
239

239

00:09:40.440  -->  00:09:42.440
of our artificial neural network.
240

240

00:09:42.440  -->  00:09:47.220
And so the name of this argument is build underscore fn,
241

241

00:09:47.220  -->  00:09:51.350
and so this will be equal to our build classifier function.
242

242

00:09:51.350  -->  00:09:53.024
So I'm copying this.
243

243

00:09:53.024  -->  00:09:54.330
Here we go.
244

244

00:09:54.330  -->  00:09:59.240
And I'm putting that as input for our build fn argument.
245

245

00:09:59.240  -->  00:10:02.410
And this should be input this way, not inside quotes.
246

246

00:10:02.410  -->  00:10:04.720
Alright then, next argument.
247

247

00:10:04.720  -->  00:10:07.290
The next argument is the batch size.
248

248

00:10:07.290  -->  00:10:08.260
Because indeed,
249

249

00:10:08.260  -->  00:10:11.610
so far we've just built our artificial neural network
250

250

00:10:11.610  -->  00:10:14.780
and we included this code in the build classifier function
251

251

00:10:14.780  -->  00:10:15.670
but remember,
252

252

00:10:15.670  -->  00:10:19.700
we didn't include this part of code that does the training.
253

253

00:10:19.700  -->  00:10:21.170
And in this part of code, as you noticed,
254

254

00:10:21.170  -->  00:10:24.200
there is the batch size and the number of epoch.
255

255

00:10:24.200  -->  00:10:26.676
So that's all we need to add to specify
256

256

00:10:26.676  -->  00:10:28.920
on how many epochs we wanna train
257

257

00:10:28.920  -->  00:10:32.870
our artificial neural network and with which batch size.
258

258

00:10:32.870  -->  00:10:34.170
So here we go.
259

259

00:10:34.170  -->  00:10:36.000
Let's add these information here.
260

260

00:10:36.000  -->  00:10:37.030
And by the way,
261

261

00:10:37.030  -->  00:10:39.720
the name of the two arguments we need to input here
262

262

00:10:39.720  -->  00:10:42.040
are the exact same names as the two arguments here
263

263

00:10:42.040  -->  00:10:44.310
so we just need to copy them.
264

264

00:10:44.310  -->  00:10:47.380
Copy and add them here.
265

265

00:10:47.380  -->  00:10:50.100
Alright, and now we have everything we need.
266

266

00:10:50.100  -->  00:10:53.390
We have the function that builds the architecture of the ANN
267

267

00:10:53.390  -->  00:10:55.808
and we have the two important information for the training,
268

268

00:10:55.808  -->  00:10:58.917
that is the number of epoch and the batch size.
269

269

00:10:58.917  -->  00:11:03.760
Okay, but now, K-fold cross validation is still not applied.
270

270

00:11:03.760  -->  00:11:05.480
And that's where we use Scikit-learn
271

271

00:11:05.480  -->  00:11:07.494
and more precisely the cross val score function
272

272

00:11:07.494  -->  00:11:10.695
by the Scikit-learn model selection module.
273

273

00:11:10.695  -->  00:11:12.760
So, next line of code.
274

274

00:11:12.760  -->  00:11:14.686
And let's input that.
275

275

00:11:14.686  -->  00:11:18.150
So as you understood, the use of K-fold cross validation
276

276

00:11:18.150  -->  00:11:20.970
is to return a relevant measure of the accuracy
277

277

00:11:20.970  -->  00:11:24.000
of our artificial neural network on the test set
278

278

00:11:24.000  -->  00:11:26.800
and therefore the cross val score function will return
279

279

00:11:26.800  -->  00:11:30.064
the 10 accuracies of the 10 test folds that occur
280

280

00:11:30.064  -->  00:11:32.120
in K-fold cross validation.
281

281

00:11:32.120  -->  00:11:33.640
And therefore, what we need to start with
282

282

00:11:33.640  -->  00:11:35.462
when using the cross val score function,
283

283

00:11:35.462  -->  00:11:39.940
is to create a new variable that we're gonna call accuracies
284

284

00:11:39.940  -->  00:11:42.190
and that will contain the 10 accuracies
285

285

00:11:42.190  -->  00:11:45.753
returned by K-fold cross validation with K equals 10.
286

286

00:11:45.753  -->  00:11:48.200
And so here we will add equals.
287

287

00:11:48.200  -->  00:11:51.300
We're gonna use the cross val score function.
288

288

00:11:51.300  -->  00:11:52.930
Here we go.
289

289

00:11:52.930  -->  00:11:55.230
Cross val score and in parenthesis,
290

290

00:11:55.230  -->  00:11:57.700
we need to input the several arguments.
291

291

00:11:57.700  -->  00:11:59.900
So let's inspect this function
292

292

00:11:59.900  -->  00:12:02.060
to see the arguments that we need to input.
293

293

00:12:02.060  -->  00:12:04.870
Here they are, parameters, and here we go.
294

294

00:12:04.870  -->  00:12:06.980
The first argument is
295

295

00:12:06.980  -->  00:12:07.823
estimator.
296

296

00:12:09.550  -->  00:12:10.840
And so what is it?
297

297

00:12:10.840  -->  00:12:13.660
That's the object to use to fit the data.
298

298

00:12:13.660  -->  00:12:15.270
So can you guess what it is ?
299

299

00:12:15.270  -->  00:12:17.553
Well, that's of course, our classifier
300

300

00:12:17.553  -->  00:12:21.040
that we just built here with Keras classifier.
301

301

00:12:21.040  -->  00:12:22.840
So that's exactly our estimator
302

302

00:12:22.840  -->  00:12:24.300
and therefore we need to add here
303

303

00:12:24.300  -->  00:12:26.683
estimator equals classifier.
304

304

00:12:28.050  -->  00:12:29.090
Perfect.
305

305

00:12:29.090  -->  00:12:30.720
Then, next argument.
306

306

00:12:30.720  -->  00:12:34.390
The next parameter is x and that is the data to fit.
307

307

00:12:34.390  -->  00:12:35.927
So basically, that's the training set.
308

308

00:12:35.927  -->  00:12:38.200
The training set from which we're gonna create
309

309

00:12:38.200  -->  00:12:40.060
10 train test folds
310

310

00:12:40.060  -->  00:12:43.070
on which is going to be applied K-fold cross validation.
311

311

00:12:43.070  -->  00:12:46.680
And x is the matrix of features part of the training set
312

312

00:12:46.680  -->  00:12:49.766
and therefore x is of course x train.
313

313

00:12:49.766  -->  00:12:51.720
(typing)
314

314

00:12:51.720  -->  00:12:52.640
Great.
315

315

00:12:52.640  -->  00:12:54.976
Now the next argument is y
316

316

00:12:54.976  -->  00:12:58.170
and y is the target variable of this same train set
317

317

00:12:58.170  -->  00:13:01.400
and that's of course y equals y train
318

318

00:13:02.570  -->  00:13:05.010
because of course to train a classification model
319

319

00:13:05.010  -->  00:13:06.660
like artificial neural network
320

320

00:13:06.660  -->  00:13:09.120
we need both x train and y train
321

321

00:13:09.120  -->  00:13:11.430
to understand the correlations.
322

322

00:13:11.430  -->  00:13:14.420
Alright, and now another very important parameter
323

323

00:13:14.420  -->  00:13:18.070
that we will see below, here it is, that's cv.
324

324

00:13:18.070  -->  00:13:20.500
And cv is basically the number of folds,
325

325

00:13:20.500  -->  00:13:22.320
of train test folds you wanna create
326

326

00:13:22.320  -->  00:13:24.410
when applying K-fold cross validation.
327

327

00:13:24.410  -->  00:13:26.390
And so the choice is pretty arbitrary
328

328

00:13:26.390  -->  00:13:29.080
but most of the time 10 folds are used
329

329

00:13:29.080  -->  00:13:32.410
for the simple reason that we will get a relevant idea
330

330

00:13:32.410  -->  00:13:35.720
of the accuracy if we have 10 accuracies.
331

331

00:13:35.720  -->  00:13:37.890
With 10 accuracies we will clearly see
332

332

00:13:37.890  -->  00:13:39.200
if we have good accuracy,
333

333

00:13:39.200  -->  00:13:41.180
that is, If we have low bias
334

334

00:13:41.180  -->  00:13:42.880
and if we don't have too much variance,
335

335

00:13:42.880  -->  00:13:44.160
which is what we want.
336

336

00:13:44.160  -->  00:13:47.750
So let's pick cv equals 10
337

337

00:13:47.750  -->  00:13:50.530
and I recommend this number most of the time.
338

338

00:13:50.530  -->  00:13:52.100
Alright so we could stop here
339

339

00:13:52.100  -->  00:13:54.360
but actually there is one last argument
340

340

00:13:54.360  -->  00:13:56.279
that we need to input and that is very important
341

341

00:13:56.279  -->  00:13:59.860
when applying K-fold cross validation for deep learning.
342

342

00:13:59.860  -->  00:14:04.080
This argument, this last parameter, is n jobs.
343

343

00:14:04.080  -->  00:14:06.210
Okay, so n jobs is the number of cv used
344

344

00:14:06.210  -->  00:14:08.430
to use to do the computation.
345

345

00:14:08.430  -->  00:14:10.910
And minus one means all CPU's.
346

346

00:14:10.910  -->  00:14:12.950
So why is this an important parameter ?
347

347

00:14:12.950  -->  00:14:15.810
Well, that's because basically
348

348

00:14:15.810  -->  00:14:17.550
the artificial neural network
349

349

00:14:17.550  -->  00:14:19.550
that we trained in the previous section
350

350

00:14:19.550  -->  00:14:23.070
is this time actually going to be trained 10 times
351

351

00:14:23.070  -->  00:14:26.450
because it's going to be trained on 10 train folds.
352

352

00:14:26.450  -->  00:14:28.280
And, you remember how it took time
353

353

00:14:28.280  -->  00:14:31.080
to train the model on one training fold,
354

354

00:14:31.080  -->  00:14:33.040
that was by the way the whole training set,
355

355

00:14:33.040  -->  00:14:35.230
so imagine how mch time it will take
356

356

00:14:35.230  -->  00:14:37.030
if we train it on 10 train folds.
357

357

00:14:37.030  -->  00:14:39.960
And so of course we want our computations to go faster,
358

358

00:14:39.960  -->  00:14:42.570
and that's by using the n jobs parameter
359

359

00:14:42.570  -->  00:14:46.730
and setting it to minus one to use all the CPU's
360

360

00:14:46.730  -->  00:14:48.070
and therefore what it will do
361

361

00:14:48.070  -->  00:14:51.770
is run parallel computations to get this done faster.
362

362

00:14:51.770  -->  00:14:54.720
So we will not see one training fold one at a time,
363

363

00:14:54.720  -->  00:14:56.580
we will see the different trainings
364

364

00:14:56.580  -->  00:14:59.890
on the different train folds run at the same time.
365

365

00:14:59.890  -->  00:15:02.410
So that's a very important argument
366

366

00:15:02.410  -->  00:15:07.350
and here we'll add n jobs equals minus one.
367

367

00:15:07.350  -->  00:15:08.183
Perfect.
368

368

00:15:08.183  -->  00:15:10.660
And that was the last argument we needed to input
369

369

00:15:10.660  -->  00:15:12.890
in this cross val score function.
370

370

00:15:12.890  -->  00:15:15.912
So basically we are now ready to run K-fold cross validation
371

371

00:15:15.912  -->  00:15:18.170
so that's what we're gonna do right now.
372

372

00:15:18.170  -->  00:15:21.040
Remember, we are doing this to check two things;
373

373

00:15:21.040  -->  00:15:24.080
first, if the real relevance accuracy
374

374

00:15:24.080  -->  00:15:26.730
is closer to the first one we obtained that is
375

375

00:15:26.730  -->  00:15:29.760
rather 86% or the second one we obtained
376

376

00:15:29.760  -->  00:15:32.050
that is rather 84%.
377

377

00:15:32.050  -->  00:15:34.260
And then the second thing we wanna check is
378

378

00:15:34.260  -->  00:15:36.820
where we are in the bias variance trade-off
379

379

00:15:36.820  -->  00:15:39.250
and so we need to check that we have a low bias
380

380

00:15:39.250  -->  00:15:42.010
and a low variance that is, high accuracies
381

381

00:15:42.010  -->  00:15:44.510
and small differences, small variance
382

382

00:15:44.510  -->  00:15:47.092
between these accuracies that we are about to get.
383

383

00:15:47.092  -->  00:15:48.030
Alright.
384

384

00:15:48.030  -->  00:15:48.863
So,
385

385

00:15:48.863  -->  00:15:49.830
There we go.
386

386

00:15:49.830  -->  00:15:51.420
We are ready to execute this,
387

387

00:15:51.420  -->  00:15:53.950
however remember since I reset everything
388

388

00:15:53.950  -->  00:15:55.360
by restoring my kernel
389

389

00:15:55.360  -->  00:15:59.840
we need to also import the Keras libraries
390

390

00:15:59.840  -->  00:16:00.673
because as you can see,
391

391

00:16:00.673  -->  00:16:04.830
we used them in this K-fold cross validation codes.
392

392

00:16:04.830  -->  00:16:07.351
And actually, I think it might be easier for you
393

393

00:16:07.351  -->  00:16:12.030
to have them already in the code here
394

394

00:16:12.030  -->  00:16:12.863
in case,
395

395

00:16:12.863  -->  00:16:16.080
you know you are using K-fold cross validation directly.
396

396

00:16:16.080  -->  00:16:17.680
Okay, so here we go.
397

397

00:16:17.680  -->  00:16:19.540
We're going to execute this.
398

398

00:16:19.540  -->  00:16:20.900
So it's going to take a while
399

399

00:16:20.900  -->  00:16:23.170
because our artificial neural network
400

400

00:16:23.170  -->  00:16:26.530
is going to be trained 10 times on the 10 train fold
401

401

00:16:26.530  -->  00:16:29.190
and each time we're gonna measure the accuracy
402

402

00:16:29.190  -->  00:16:30.550
on the test fold,
403

403

00:16:30.550  -->  00:16:33.590
and that's how we're gonna get 10 accuracies.
404

404

00:16:33.590  -->  00:16:34.670
So let's do it.
405

405

00:16:34.670  -->  00:16:38.760
We just need to actually select all these lines
406

406

00:16:38.760  -->  00:16:40.820
because we already executed this.
407

407

00:16:40.820  -->  00:16:43.570
Now let's press command and control plus enter to execute.
408

408

00:16:43.570  -->  00:16:44.550
Here we go.
409

409

00:16:44.550  -->  00:16:45.383
It's on.
410

410

00:16:45.383  -->  00:16:46.216
It's live.
411

411

00:16:46.216  -->  00:16:47.070
So as you can see,
412

412

00:16:47.070  -->  00:16:49.030
there are parallel computations right now.
413

413

00:16:49.030  -->  00:16:52.730
We can see the same epoch running separately
414

414

00:16:52.730  -->  00:16:54.060
on different train folds,
415

415

00:16:54.060  -->  00:16:55.530
so that's exactly what I was telling you,
416

416

00:16:55.530  -->  00:16:57.540
that's parallel computation.
417

417

00:16:57.540  -->  00:16:59.140
So this is going to take a while
418

418

00:16:59.140  -->  00:17:01.890
so I'm going to move forward to the future.
419

419

00:17:01.890  -->  00:17:03.080
So I'll see you very soon
420

420

00:17:03.080  -->  00:17:05.573
at the end of this K-fold cross validation.
421

421

00:17:07.090  -->  00:17:08.910
Alright, its about to end
422

422

00:17:08.910  -->  00:17:10.170
98,
423

423

00:17:10.170  -->  00:17:11.143
99,
424

424

00:17:12.080  -->  00:17:14.700
and here we go, that's over.
425

425

00:17:14.700  -->  00:17:16.430
K-fold cross validation is over
426

426

00:17:16.430  -->  00:17:21.370
and it seems that the accuracies are around 83%.
427

427

00:17:21.370  -->  00:17:22.350
Let's check it out right now.
428

428

00:17:22.350  -->  00:17:24.760
We're gonna go to variable explorer
429

429

00:17:24.760  -->  00:17:27.680
to have a look at our vector of accuracies.
430

430

00:17:27.680  -->  00:17:28.513
Here it is.
431

431

00:17:28.513  -->  00:17:33.374
And indeed, the accuracies are around 83%, 84%.
432

432

00:17:33.374  -->  00:17:36.440
So that's exactly the accuracy we obtained the second time
433

433

00:17:36.440  -->  00:17:37.390
on the test set.
434

434

00:17:37.390  -->  00:17:40.440
So the 86% accuracy we obtained the first time
435

435

00:17:40.440  -->  00:17:41.860
was not relevant.
436

436

00:17:41.860  -->  00:17:43.940
So that's why I want you to be motivated
437

437

00:17:43.940  -->  00:17:45.950
to use K-fold cross validation.
438

438

00:17:45.950  -->  00:17:47.910
It's to avoid that kind of trap.
439

439

00:17:47.910  -->  00:17:52.910
This is the relevance result of performance evaluation
440

440

00:17:52.980  -->  00:17:56.060
and if you want to have one final result,
441

441

00:17:56.060  -->  00:17:59.080
well, the relevance accuracy would be to take the mean
442

442

00:17:59.080  -->  00:18:01.890
of all these accuracies and of course we will then compute
443

443

00:18:01.890  -->  00:18:03.910
the variance to see if we have high variance
444

444

00:18:03.910  -->  00:18:05.920
or low variance.
445

445

00:18:05.920  -->  00:18:07.060
So let's do it.
446

446

00:18:07.060  -->  00:18:09.550
We're gonna compute the mean of these accuracies.
447

447

00:18:09.550  -->  00:18:12.870
So let's create a new variable mean equals,
448

448

00:18:12.870  -->  00:18:15.240
and to take the mean of this vector of accuracies,
449

449

00:18:15.240  -->  00:18:16.790
there is nothing more simple,
450

450

00:18:16.790  -->  00:18:20.100
we just need to take our vector accuracies
451

451

00:18:20.100  -->  00:18:23.780
and then add a dot and then add the function mean
452

452

00:18:23.780  -->  00:18:25.130
with parenthesis.
453

453

00:18:25.130  -->  00:18:27.040
And this will get you the mean.
454

454

00:18:27.040  -->  00:18:29.150
And let's also compute the variance.
455

455

00:18:29.150  -->  00:18:32.060
So we're gonna call this variance as well
456

456

00:18:32.060  -->  00:18:36.090
equals then again we take our accuracies vector
457

457

00:18:36.090  -->  00:18:38.050
then dot and then to get the variance
458

458

00:18:38.050  -->  00:18:42.180
we use the function STD with parenthesis as well.
459

459

00:18:42.180  -->  00:18:46.620
So I'm going to select these two lines and execute.
460

460

00:18:46.620  -->  00:18:47.453
Here we go.
461

461

00:18:47.453  -->  00:18:49.080
We are about to get the mean,
462

462

00:18:49.080  -->  00:18:51.630
which is of course 83.4%,
463

463

00:18:51.630  -->  00:18:52.770
so that's what we said,
464

464

00:18:52.770  -->  00:18:55.060
that's definitely the relevance accuracy
465

465

00:18:55.060  -->  00:18:57.210
to evaluate our models performance
466

466

00:18:57.210  -->  00:18:59.683
and the variance is actually quite small,
467

467

00:18:59.683  -->  00:19:02.880
0.9%, so that's less than 1%
468

468

00:19:02.880  -->  00:19:05.420
and therefore we have rather low variance.
469

469

00:19:05.420  -->  00:19:06.280
So here we go.
470

470

00:19:06.280  -->  00:19:08.850
That's where we are in the bias variance trade-off
471

471

00:19:08.850  -->  00:19:10.190
and now we have a challenge.
472

472

00:19:10.190  -->  00:19:13.340
Now that we know how to get the relevance accuracy
473

473

00:19:13.340  -->  00:19:15.230
for any deep learning model,
474

474

00:19:15.230  -->  00:19:16.790
well we're gonna take the challenge
475

475

00:19:16.790  -->  00:19:19.720
to beat this relevance accuracy.
476

476

00:19:19.720  -->  00:19:21.900
That means that we will try to make a better
477

477

00:19:21.900  -->  00:19:25.290
artificial neural network to get a higher accuracy
478

478

00:19:25.290  -->  00:19:26.810
than 83%.
479

479

00:19:26.810  -->  00:19:30.420
We will try to go up to 86%, that will be our challenge
480

480

00:19:30.420  -->  00:19:32.340
and we will try to make it successful
481

481

00:19:32.340  -->  00:19:34.200
with parameter tuning.
482

482

00:19:34.200  -->  00:19:36.263
Until then, enjoy deep learning.
