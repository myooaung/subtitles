WEBVTT
1
1

00:00:00.270  -->  00:00:02.670
<v ->Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.670  -->  00:00:04.780
Alright, so this is the exciting tutorial,
3

3

00:00:04.780  -->  00:00:07.920
and this one you can comfortably sit in your chair,
4

4

00:00:07.920  -->  00:00:11.020
and get yourself some coffee or tea or your favorite snack,
5

5

00:00:11.020  -->  00:00:13.750
because, basically, we won't type any line of code
6

6

00:00:13.750  -->  00:00:15.970
and we will just execute this action
7

7

00:00:15.970  -->  00:00:18.160
to see the action take place.
8

8

00:00:18.160  -->  00:00:20.520
And this action is, of course, the training
9

9

00:00:20.520  -->  00:00:22.640
of our stacked outer encoders,
10

10

00:00:22.640  -->  00:00:25.100
that's going to happen thanks to all this code here.
11

11

00:00:25.100  -->  00:00:26.890
But before we execute this action,
12

12

00:00:26.890  -->  00:00:30.330
let's set some expectations of what we would like to get.
13

13

00:00:30.330  -->  00:00:33.400
Remember, at each epoch it will print the epoch
14

14

00:00:33.400  -->  00:00:34.870
and the loss.
15

15

00:00:34.870  -->  00:00:38.170
And the loss represents the average of the differences
16

16

00:00:38.170  -->  00:00:40.930
between the real rating and the predicted rating,
17

17

00:00:40.930  -->  00:00:42.630
but on the training set.
18

18

00:00:42.630  -->  00:00:44.680
Which means that, for example, if we get
19

19

00:00:44.680  -->  00:00:47.820
a loss of one at the last epoch that is a de-ant
20

20

00:00:47.820  -->  00:00:48.800
of the training.
21

21

00:00:48.800  -->  00:00:50.780
Well that will mean that the average difference
22

22

00:00:50.780  -->  00:00:53.940
between the real ratings of the movies by the users
23

23

00:00:53.940  -->  00:00:56.690
and the predicted ratings will be one.
24

24

00:00:56.690  -->  00:00:59.040
And therefore that's not too bad because that means
25

25

00:00:59.040  -->  00:01:02.320
that when we predict if a user is going to like a movie,
26

26

00:01:02.320  -->  00:01:05.120
well on average we will make an error of one star.
27

27

00:01:05.120  -->  00:01:06.730
One star out of five.
28

28

00:01:06.730  -->  00:01:09.280
And so that's okay, but, of course, we were hoping
29

29

00:01:09.280  -->  00:01:11.521
to get a loss that would be less than one star
30

30

00:01:11.521  -->  00:01:13.530
so that indeed the average difference
31

31

00:01:13.530  -->  00:01:15.760
between the real rating and the predicted rating
32

32

00:01:15.760  -->  00:01:18.110
is less than one, and therefore, on average,
33

33

00:01:18.110  -->  00:01:20.620
we will make better predictions of whether a user
34

34

00:01:20.620  -->  00:01:22.240
is going to like a movie.
35

35

00:01:22.240  -->  00:01:23.720
Okay? So that's the first thing.
36

36

00:01:23.720  -->  00:01:26.340
We are at least expecting to get a loss of less than one,
37

37

00:01:26.340  -->  00:01:28.830
so that our recommending system can still predict
38

38

00:01:28.830  -->  00:01:30.910
if users are gonna like the movies,
39

39

00:01:30.910  -->  00:01:34.200
and then the second thing to keep in mind is that this loss
40

40

00:01:34.200  -->  00:01:37.580
that we're about to get is the loss on the training sets,
41

41

00:01:37.580  -->  00:01:40.270
and of course, as usual, what we're most interested in
42

42

00:01:40.270  -->  00:01:41.840
is the loss on the test set.
43

43

00:01:41.840  -->  00:01:45.010
But still, if we get a good loss on the training set
44

44

00:01:45.010  -->  00:01:46.720
then we might have a good chance to get
45

45

00:01:46.720  -->  00:01:48.380
a good loss on the test set,
46

46

00:01:48.380  -->  00:01:50.130
unless there is high over-filling.
47

47

00:01:50.130  -->  00:01:53.500
But remember this model is based on some experimenting
48

48

00:01:53.500  -->  00:01:56.209
that I did so I made sure it's not the case,
49

49

00:01:56.209  -->  00:01:58.320
and therefore we should be also getting
50

50

00:01:58.320  -->  00:02:00.410
a good loss error the test set.
51

51

00:02:00.410  -->  00:02:01.350
So let's check it out.
52

52

00:02:01.350  -->  00:02:04.050
We will first do it with the training set,
53

53

00:02:04.050  -->  00:02:05.770
then you'll have a little coding homework
54

54

00:02:05.770  -->  00:02:09.070
to do this on the test set so that eventually
55

55

00:02:09.070  -->  00:02:11.750
we can all get the loss error on the test sets.
56

56

00:02:11.750  -->  00:02:15.240
So here we go, I'm going to select this section
57

57

00:02:15.240  -->  00:02:17.410
to do the training and...
58

58

00:02:17.410  -->  00:02:18.253
showtime!
59

59

00:02:19.140  -->  00:02:21.050
Alright, so it's training, here we go.
60

60

00:02:21.050  -->  00:02:22.950
We didn't make any error in the code.
61

61

00:02:22.950  -->  00:02:25.670
That's great, and it's going at a good pace.
62

62

00:02:25.670  -->  00:02:28.620
And the loss so far is over than one.
63

63

00:02:28.620  -->  00:02:30.950
Let's hope that we get below than one.
64

64

00:02:30.950  -->  00:02:34.210
It seems to be quite a slow convergence,
65

65

00:02:34.210  -->  00:02:36.183
but let's keep fingers crossed.
66

66

00:02:37.050  -->  00:02:39.170
Alright, and now we see that we're about to get
67

67

00:02:39.170  -->  00:02:40.330
below than one.
68

68

00:02:40.330  -->  00:02:43.920
And here we go. Below than one. 0.99, perfect.
69

69

00:02:43.920  -->  00:02:46.100
So that's good, I'm gonna move forward now
70

70

00:02:46.100  -->  00:02:48.700
and we'll see each other at the end of the training.
71

71

00:02:50.170  -->  00:02:52.170
And here we go, we are at the last epochs.
72

72

00:02:52.170  -->  00:02:54.280
We might expect a final
73

73

00:02:54.280  -->  00:02:56.410
loss of 0.91,
74

74

00:02:56.410  -->  00:02:58.410
because it seems to be converging right now.
75

75

00:02:58.410  -->  00:03:02.400
Remember that this result is based on experimenting,
76

76

00:03:02.400  -->  00:03:05.490
and, of course, by having more epochs you can get
77

77

00:03:05.490  -->  00:03:07.430
a lower loss on the training set,
78

78

00:03:07.430  -->  00:03:10.120
but then you might end up with a larger difference
79

79

00:03:10.120  -->  00:03:12.760
of the loss between the training set and the test set,
80

80

00:03:12.760  -->  00:03:14.430
which we want to avoid.
81

81

00:03:14.430  -->  00:03:17.190
And here with this configuration, we will see that
82

82

00:03:17.190  -->  00:03:19.470
we will get a pretty good loss error on the test set
83

83

00:03:19.470  -->  00:03:21.640
that is not too far from this loss error
84

84

00:03:21.640  -->  00:03:22.950
on the training set.
85

85

00:03:22.950  -->  00:03:26.880
Alright, we just passed 190 epochs, now five more to go,
86

86

00:03:26.880  -->  00:03:29.470
that's almost done, and...
87

87

00:03:30.690  -->  00:03:32.030
Here we go! That's over!
88

88

00:03:32.030  -->  00:03:36.060
We ended up with a loss error of 0.91.
89

89

00:03:36.060  -->  00:03:37.120
So that's not too bad.
90

90

00:03:37.120  -->  00:03:40.150
Besides, we were training on 100,000 ratings
91

91

00:03:40.150  -->  00:03:42.430
and you will definitely get a better loss error
92

92

00:03:42.430  -->  00:03:44.300
if you train this on more ratings.
93

93

00:03:44.300  -->  00:03:46.750
Remember that you have the one million ratings data set
94

94

00:03:46.750  -->  00:03:48.830
in the zip folder, so feel free to try it,
95

95

00:03:48.830  -->  00:03:51.950
and also remember that our code is optimized
96

96

00:03:51.950  -->  00:03:54.200
so that the training doesn't take too much time.
97

97

00:03:54.200  -->  00:03:57.190
So that's pretty good and now let's hope
98

98

00:03:57.190  -->  00:03:59.750
that we will get around the same error on the test set.
99

99

00:03:59.750  -->  00:04:02.290
But before that you will have this coding exercise
100

100

00:04:02.290  -->  00:04:03.940
to get the test set error,
101

101

00:04:03.940  -->  00:04:06.420
so that will involve a full loop as well.
102

102

00:04:06.420  -->  00:04:08.830
So give it a try. That's very good if you try.
103

103

00:04:08.830  -->  00:04:10.497
That's the best part of practice.
104

104

00:04:10.497  -->  00:04:13.120
Even if you don't succeed in the end, you have to try.
105

105

00:04:13.120  -->  00:04:14.840
And of course I'll provide the solution
106

106

00:04:14.840  -->  00:04:16.880
in the next tutorial, and we will see
107

107

00:04:16.880  -->  00:04:18.620
our final test set error.
108

108

00:04:18.620  -->  00:04:21.020
So I'll see you in this next tutorial, and until then,
109

109

00:04:21.020  -->  00:04:22.020
enjoy deep learning!
