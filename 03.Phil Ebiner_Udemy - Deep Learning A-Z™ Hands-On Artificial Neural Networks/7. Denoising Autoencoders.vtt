WEBVTT
1
1

00:00:00.990  -->  00:00:03.460
<v ->Hello and welcome back to the course on deep learning.</v>
2

2

00:00:03.460  -->  00:00:05.290
Today we're talking about Denoising Autoencoders.
3

3

00:00:05.290  -->  00:00:07.040
Denoising Autoencoders is another
4

4

00:00:08.407  -->  00:00:12.110
regularization technique, which is here
5

5

00:00:12.110  -->  00:00:15.511
to combat the problem of when we have
6

6

00:00:15.511  -->  00:00:17.690
more nodes in the hidden layer
7

7

00:00:17.690  -->  00:00:19.520
than in the input layer and therefore
8

8

00:00:19.520  -->  00:00:21.540
the Autoencoder can simply just
9

9

00:00:21.540  -->  00:00:24.470
copy these values across without finding
10

10

00:00:24.470  -->  00:00:26.290
any meaningful features and undergoing
11

11

00:00:26.290  -->  00:00:28.390
the training that we want it to undergo,
12

12

00:00:28.390  -->  00:00:29.940
and so, what we are going to do here
13

13

00:00:29.940  -->  00:00:31.860
is we are going to take these input values
14

14

00:00:31.860  -->  00:00:33.540
and we're going to move them to the left
15

15

00:00:33.540  -->  00:00:35.215
and replace them with something else
16

16

00:00:35.215  -->  00:00:37.780
and this something else is a modified
17

17

00:00:37.780  -->  00:00:40.330
version of our input values. So, let's
18

18

00:00:40.330  -->  00:00:43.430
say we have input values X1, X2, X3 and X4.
19

19

00:00:43.430  -->  00:00:45.230
Well, what we are going to do is we're
20

20

00:00:45.230  -->  00:00:47.550
going to take these inputs and
21

21

00:00:47.550  -->  00:00:49.670
randomly out of them, we're going
22

22

00:00:49.670  -->  00:00:52.180
to turn some of them into zeros,
23

23

00:00:52.180  -->  00:00:54.350
just like that, and it's a parameter
24

24

00:00:54.350  -->  00:00:58.570
you can specify in the setup of your Autoencoder,
25

25

00:00:58.570  -->  00:01:00.160
it can be, for instance, half of
26

26

00:01:00.160  -->  00:01:03.230
your inputs that you have are turned into zeros
27

27

00:01:03.230  -->  00:01:05.170
every single time and it happens randomly,
28

28

00:01:05.170  -->  00:01:08.262
so at every single pass it can be
29

29

00:01:08.262  -->  00:01:11.630
different variables and then, once you
30

30

00:01:11.630  -->  00:01:14.500
put this data through your Autoencoder
31

31

00:01:14.500  -->  00:01:16.350
what you do in the end is you can
32

32

00:01:16.350  -->  00:01:19.320
pair the output, not with the modified values,
33

33

00:01:19.320  -->  00:01:21.000
but with their original values,
34

34

00:01:21.000  -->  00:01:23.130
and that prevents the Autoencoder
35

35

00:01:23.130  -->  00:01:26.730
from simply just copying that data
36

36

00:01:26.730  -->  00:01:28.570
or those inputs all the way through
37

37

00:01:28.570  -->  00:01:30.060
to the outputs because it's actually
38

38

00:01:30.060  -->  00:01:32.100
comparing the output, not with the
39

39

00:01:32.100  -->  00:01:34.730
noisy but with the original inputs
40

40

00:01:34.730  -->  00:01:36.270
and that helps combat the problem
41

41

00:01:36.270  -->  00:01:38.763
that we are facing and also it's
42

42

00:01:38.763  -->  00:01:41.290
important to note here that because
43

43

00:01:41.290  -->  00:01:43.470
this happens randomly, this type
44

44

00:01:43.470  -->  00:01:45.926
of Autoencoder is a stochastic Autoencoder
45

45

00:01:45.926  -->  00:01:48.380
so basically it depends on this
46

46

00:01:48.380  -->  00:01:50.300
random generation, or random selection
47

47

00:01:50.300  -->  00:01:53.560
of which values are going to be zeroed out
48

48

00:01:53.560  -->  00:01:55.950
and, yeah, so it just becomes a
49

49

00:01:55.950  -->  00:01:57.523
stochastic type of Autoencoder.
50

50

00:01:58.420  -->  00:02:01.070
So there we go, that's how this,
51

51

00:02:01.070  -->  00:02:02.185
how the Denoising Autoencoders work,
52

52

00:02:02.185  -->  00:02:03.850
also quite a popular technique
53

53

00:02:03.850  -->  00:02:06.060
you will hear about it and you
54

54

00:02:06.060  -->  00:02:07.890
will come across it if you're going
55

55

00:02:07.890  -->  00:02:09.927
to delve into the world of Autoencoders,
56

56

00:02:09.927  -->  00:02:12.705
and in terms of additional reading, here is a
57

57

00:02:12.705  -->  00:02:17.380
great paper by Pascal Vincent and others, 2008,
58

58

00:02:17.380  -->  00:02:18.850
it's called Extracting and Composing
59

59

00:02:18.850  -->  00:02:21.420
Robust Features with Denoising Autoencoders.
60

60

00:02:21.420  -->  00:02:23.840
Exactly as you can see from the image,
61

61

00:02:23.840  -->  00:02:27.050
exactly what we spoke about. So, there we go.
62

62

00:02:27.050  -->  00:02:28.930
That's Denoising Autoencoders and
63

63

00:02:28.930  -->  00:02:30.220
I look forward to seeing you next time,
64

64

00:02:30.220  -->  00:02:32.083
until then, enjoy deep learning.
