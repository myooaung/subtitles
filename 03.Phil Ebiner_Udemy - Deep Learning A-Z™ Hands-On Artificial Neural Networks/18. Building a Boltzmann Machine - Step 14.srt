1
1

00:00:00,210  -->  00:00:02,660
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,660  -->  00:00:05,350
Alright I'm super excited because we are about to get
3

3

00:00:05,350  -->  00:00:08,330
the final results on new observations with
4

4

00:00:08,330  -->  00:00:09,750
the tested results.
5

5

00:00:09,750  -->  00:00:12,700
And so we will see if the results are close
6

6

00:00:12,700  -->  00:00:13,700
to the training set results.
7

7

00:00:13,700  -->  00:00:17,320
That is if even on new observations, we can predict
8

8

00:00:17,320  -->  00:00:20,060
three correct ratings out of four.
9

9

00:00:20,060  -->  00:00:22,270
These are binary ratings, but still,
10

10

00:00:22,270  -->  00:00:23,960
this would be excellent results
11

11

00:00:23,960  -->  00:00:26,560
and we would definitely succeed at making
12

12

00:00:26,560  -->  00:00:28,120
a recommended system.
13

13

00:00:28,120  -->  00:00:31,730
So let's do it, let's find out about these results.
14

14

00:00:31,730  -->  00:00:34,540
So, good news, it's actually going to be pretty easy
15

15

00:00:34,540  -->  00:00:36,890
because we are gonna do it the efficient way,
16

16

00:00:36,890  -->  00:00:39,060
we are going to take this code,
17

17

00:00:39,060  -->  00:00:41,550
because getting the tested results is going to be quite
18

18

00:00:41,550  -->  00:00:44,120
similar as getting the trend set results.
19

19

00:00:44,120  -->  00:00:45,980
The only difference is that there is
20

20

00:00:45,980  -->  00:00:47,290
not gonna be a training.
21

21

00:00:47,290  -->  00:00:49,510
So we will remove at least one full loop.
22

22

00:00:49,510  -->  00:00:51,840
It will be the full loop over the epochs.
23

23

00:00:51,840  -->  00:00:53,810
But, that might not be the only full loop
24

24

00:00:53,810  -->  00:00:56,180
that we remove and that will lead us
25

25

00:00:56,180  -->  00:00:59,590
to the essential, the crucial point to understand.
26

26

00:00:59,590  -->  00:01:02,540
And this crucial point is linked to MCMC,
27

27

00:01:02,540  -->  00:01:04,740
Markov chain Monte Carlo techniques.
28

28

00:01:04,740  -->  00:01:06,410
So we will get there very quickly
29

29

00:01:06,410  -->  00:01:08,940
but first let's prepare the code.
30

30

00:01:08,940  -->  00:01:11,130
Alright, so the first thing we're gonna do is,
31

31

00:01:11,130  -->  00:01:16,040
just copy this code and we will make the required change.
32

32

00:01:16,040  -->  00:01:19,490
So, pasting that here and now let's see what we have
33

33

00:01:19,490  -->  00:01:22,500
to change to get the test set results.
34

34

00:01:22,500  -->  00:01:25,050
Alright, so first there is no training right now,
35

35

00:01:25,050  -->  00:01:28,480
so we don't need to loop over the epoch,
36

36

00:01:28,480  -->  00:01:31,910
and therefore I'm going go remove nb epoch equals 10
37

37

00:01:31,910  -->  00:01:35,670
and I'm going to remove this first full loop.
38

38

00:01:35,670  -->  00:01:38,580
Alright, then we need to align everything.
39

39

00:01:38,580  -->  00:01:41,690
So, let's do it, and there we go.
40

40

00:01:41,690  -->  00:01:44,840
Alright, so now what change do we need to make.
41

41

00:01:44,840  -->  00:01:47,780
Well first, we will change the name of this variable
42

42

00:01:47,780  -->  00:01:49,000
to compute the loss.
43

43

00:01:49,000  -->  00:01:52,810
It is now not the train loss but the test loss,
44

44

00:01:52,810  -->  00:01:54,700
that we are trying to measure.
45

45

00:01:54,700  -->  00:01:58,000
Then we will keep the counter that we initialize at zero,
46

46

00:01:58,000  -->  00:02:01,160
and we will increment it by one at each step.
47

47

00:02:01,160  -->  00:02:03,920
Alright, so all good for the first two lines.
48

48

00:02:03,920  -->  00:02:06,240
Then we have this loop over all the users
49

49

00:02:06,240  -->  00:02:08,310
of our, this time, test set.
50

50

00:02:08,310  -->  00:02:10,780
In here, do we need a batch size?
51

51

00:02:10,780  -->  00:02:12,370
Well, no, of course.
52

52

00:02:12,370  -->  00:02:14,560
Because the batch size is just a technique
53

53

00:02:14,560  -->  00:02:16,040
specific to the training.
54

54

00:02:16,040  -->  00:02:17,520
You know the batch size is a parameter
55

55

00:02:17,520  -->  00:02:21,130
that you tune to get more or less better performance results
56

56

00:02:21,130  -->  00:02:23,610
on the training set, and therefore, on the test set.
57

57

00:02:23,610  -->  00:02:26,150
But gathering the observations in the batch size
58

58

00:02:26,150  -->  00:02:27,820
is only for the training phase.
59

59

00:02:27,820  -->  00:02:29,980
And therefore, we will remove everything
60

60

00:02:29,980  -->  00:02:31,350
that is related to the batch size.
61

61

00:02:31,350  -->  00:02:35,730
So we remove the step and we take our users,
62

62

00:02:35,730  -->  00:02:38,280
up to the last user, because basically we will
63

63

00:02:38,280  -->  00:02:41,960
make some predictions for each of the users one by one.
64

64

00:02:41,960  -->  00:02:44,160
And therefore, we can also remove the zero here
65

65

00:02:44,160  -->  00:02:46,330
because that's the default start.
66

66

00:02:46,330  -->  00:02:50,100
Here we go, for id user in range nb users,
67

67

00:02:50,100  -->  00:02:52,770
so basically now we are looping over all the users,
68

68

00:02:52,770  -->  00:02:53,740
one by one.
69

69

00:02:53,740  -->  00:02:54,573
Okay.
70

70

00:02:54,573  -->  00:02:57,780
So for each user and now we go into the loop.
71

71

00:02:57,780  -->  00:03:01,060
Now, same, we have to remove the batch sizes here,
72

72

00:03:01,060  -->  00:03:02,220
we don't need them.
73

73

00:03:02,220  -->  00:03:04,440
And there is more, since we are gonna make some predictions
74

74

00:03:04,440  -->  00:03:06,970
for each of the users one by one,
75

75

00:03:06,970  -->  00:03:11,850
we will replace this batch size by actually one.
76

76

00:03:11,850  -->  00:03:13,683
And batch size one.
77

77

00:03:14,560  -->  00:03:16,780
Okay, now just to make things very clear
78

78

00:03:16,780  -->  00:03:21,780
we are going to replace vk by actually v and v0,
79

79

00:03:21,940  -->  00:03:25,220
which remember was the target by vt.
80

80

00:03:25,220  -->  00:03:28,090
So that now it's very clear that vt is the target.
81

81

00:03:28,090  -->  00:03:33,090
But now v is the input on which we will make the prediction.
82

82

00:03:33,180  -->  00:03:36,450
So now the question is, we are dealing with the test set
83

83

00:03:36,450  -->  00:03:40,040
so we are trying to predict the ratings in the test set.
84

84

00:03:40,040  -->  00:03:42,460
So it would make sense to replace both
85

85

00:03:42,460  -->  00:03:45,010
the training sets here by test sets,
86

86

00:03:45,010  -->  00:03:48,850
but do we actually need to replace both training sets here?
87

87

00:03:48,850  -->  00:03:52,390
Well let's see, let's start with the target vt.
88

88

00:03:52,390  -->  00:03:56,410
So vt contains the original ratings of the test set,
89

89

00:03:56,410  -->  00:03:59,380
so that is what we will compare to our predictions
90

90

00:03:59,380  -->  00:04:00,340
in the end.
91

91

00:04:00,340  -->  00:04:02,880
And, of course, we need to replace training set here
92

92

00:04:02,880  -->  00:04:06,760
by test set because we want to compare the real ratings
93

93

00:04:06,760  -->  00:04:09,560
of the test set to our predictions.
94

94

00:04:09,560  -->  00:04:12,890
But here for v, the input, do you think we need
95

95

00:04:12,890  -->  00:04:16,070
to replace the training set by test set?
96

96

00:04:16,070  -->  00:04:18,200
If your answer to that question is correct,
97

97

00:04:18,200  -->  00:04:21,680
then that means you really understand what's going on.
98

98

00:04:21,680  -->  00:04:23,750
So the answer to that question is,
99

99

00:04:23,750  -->  00:04:28,400
no, we must not replace training set here by test set.
100

100

00:04:28,400  -->  00:04:31,440
We actually need to keep the training set.
101

101

00:04:31,440  -->  00:04:32,670
And why is that?
102

102

00:04:32,670  -->  00:04:36,130
It's because actually the training set is the input
103

103

00:04:36,130  -->  00:04:39,300
that will be used to activate the hidden neurons
104

104

00:04:39,300  -->  00:04:41,220
to get the output.
105

105

00:04:41,220  -->  00:04:43,700
Right now the training set contains the ratings
106

106

00:04:43,700  -->  00:04:46,780
of the training set and it doesn't contain the answers
107

107

00:04:46,780  -->  00:04:48,090
of the test set.
108

108

00:04:48,090  -->  00:04:50,830
But, by using the inputs of the training set
109

109

00:04:50,830  -->  00:04:54,040
we will activate the neurons of our RBM
110

110

00:04:54,040  -->  00:04:56,680
to predict the ratings of the movies
111

111

00:04:56,680  -->  00:04:59,820
that were not rated yet, and that is the ratings
112

112

00:04:59,820  -->  00:05:00,950
of the test set.
113

113

00:05:00,950  -->  00:05:04,320
So we need this as input to get the predicted ratings
114

114

00:05:04,320  -->  00:05:05,360
of the test set.
115

115

00:05:05,360  -->  00:05:07,480
Because we are getting these predicted ratings
116

116

00:05:07,480  -->  00:05:09,660
from the inputs of the training set
117

117

00:05:09,660  -->  00:05:12,640
that are used to activate the neurons of our RBM.
118

118

00:05:12,640  -->  00:05:15,690
And that's the first very important point to understand,
119

119

00:05:15,690  -->  00:05:18,520
that we are using the inputs of the training set
120

120

00:05:18,520  -->  00:05:20,920
to activate the neurons of the RBM
121

121

00:05:20,920  -->  00:05:24,120
to get the predicted ratings of the test set.
122

122

00:05:24,120  -->  00:05:26,300
Alright so now that you understand this first
123

123

00:05:26,300  -->  00:05:28,930
crucial point we can move on to what's next.
124

124

00:05:28,930  -->  00:05:31,350
But then you will see that there is another very crucial
125

125

00:05:31,350  -->  00:05:34,470
point to understand, that is more related to mathematics
126

126

00:05:34,470  -->  00:05:36,480
but that is fascinating because it is related
127

127

00:05:36,480  -->  00:05:40,560
to MCMC techniques, Markov chains Monte Carlo techniques
128

128

00:05:40,560  -->  00:05:43,400
and that is of course is related to the random walk.
129

129

00:05:43,400  -->  00:05:46,540
Or more precisely you'll see the blind walk.
130

130

00:05:46,540  -->  00:05:48,730
Alright so let's move onto what's next.
131

131

00:05:48,730  -->  00:05:52,240
So what's next is this line of code that computes
132

132

00:05:52,240  -->  00:05:55,400
the probabilities that the hidden nodes equal one
133

133

00:05:55,400  -->  00:05:58,630
given the values of divisible nodes in the original
134

134

00:05:58,630  -->  00:06:01,560
input vector that is given the original ratings.
135

135

00:06:01,560  -->  00:06:04,930
And this ph0 was just something we needed to train the model
136

136

00:06:04,930  -->  00:06:09,010
with cdk, remember that that's actually what is needed
137

137

00:06:09,010  -->  00:06:13,390
to start this loop to make this Gibbs chain give something.
138

138

00:06:13,390  -->  00:06:15,820
So that is just for the training and therefore,
139

139

00:06:15,820  -->  00:06:18,380
we don't need it for the test set.
140

140

00:06:18,380  -->  00:06:22,030
So let's remove it and let's move on to the next step.
141

141

00:06:22,030  -->  00:06:23,960
So the next step is, again,
142

142

00:06:23,960  -->  00:06:26,470
about this contrastive divergence
143

143

00:06:26,470  -->  00:06:28,750
and here we are getting very close
144

144

00:06:28,750  -->  00:06:31,310
to the second crucial point to understand.
145

145

00:06:31,310  -->  00:06:33,090
Actually we're getting to it right now,
146

146

00:06:33,090  -->  00:06:34,760
so let me explain.
147

147

00:06:34,760  -->  00:06:36,920
The question that I'm asking you now
148

148

00:06:36,920  -->  00:06:39,560
to get to this second crucial point to understand
149

149

00:06:39,560  -->  00:06:44,520
is that to get our predictions of the test set ratings
150

150

00:06:44,520  -->  00:06:48,090
then according to you, do we need to apply again
151

151

00:06:48,090  -->  00:06:50,420
the CASE step contrastive divergence?
152

152

00:06:50,420  -->  00:06:52,420
Or more precisely the question is,
153

153

00:06:52,420  -->  00:06:55,470
do we need to make CASE steps of the random walk,
154

154

00:06:55,470  -->  00:06:57,650
that is 10 steps of the random walk,
155

155

00:06:57,650  -->  00:07:01,440
or do we need to make one step of the random walk?
156

156

00:07:01,440  -->  00:07:03,340
That's the one million dollar question.
157

157

00:07:03,340  -->  00:07:06,010
Do we need to make 10 steps or one step
158

158

00:07:06,010  -->  00:07:08,830
to get our final prediction?
159

159

00:07:08,830  -->  00:07:11,260
And the answer to that question is,
160

160

00:07:11,260  -->  00:07:14,190
we need to make one step.
161

161

00:07:14,190  -->  00:07:17,290
We need to make one step because the principal
162

162

00:07:17,290  -->  00:07:20,030
of the random walk, and actually this is not exactly
163

163

00:07:20,030  -->  00:07:22,210
the random walk because in the random walk
164

164

00:07:22,210  -->  00:07:23,840
the probabilities are the same.
165

165

00:07:23,840  -->  00:07:25,770
Here, even if it's a Markov chain
166

166

00:07:25,770  -->  00:07:27,710
the probabilities are not the same
167

167

00:07:27,710  -->  00:07:31,560
so it's not a random walk, so it's rather a blind walk
168

168

00:07:31,560  -->  00:07:34,190
and the principle of the blind walk is that,
169

169

00:07:34,190  -->  00:07:36,480
imagine you were blindfolded and you had
170

170

00:07:36,480  -->  00:07:39,110
to make 100 steps on a straight line
171

171

00:07:39,110  -->  00:07:41,060
without getting out of the straight line.
172

172

00:07:41,060  -->  00:07:43,740
Well, you will be trained with Gibbs sampling
173

173

00:07:43,740  -->  00:07:47,610
to make 100 steps by staying on the straight line
174

174

00:07:47,610  -->  00:07:49,500
but you're blindfolded so you know it's not
175

175

00:07:49,500  -->  00:07:51,970
easy to make some steps and always stay
176

176

00:07:51,970  -->  00:07:54,450
on the straight line, so, you know sometimes you go a little
177

177

00:07:54,450  -->  00:07:57,450
bit on the left, on the right and 100 steps after it's
178

178

00:07:57,450  -->  00:07:59,520
hard for you to be on the straight line.
179

179

00:07:59,520  -->  00:08:02,840
But you were trained to make these 100 steps,
180

180

00:08:02,840  -->  00:08:05,490
staying on the straight line being blindfolded.
181

181

00:08:05,490  -->  00:08:08,070
And that is by doing some random steps.
182

182

00:08:08,070  -->  00:08:10,160
So that's why it is close to the random walk technique
183

183

00:08:10,160  -->  00:08:13,160
and the MCMC but the difference is that in the random walk
184

184

00:08:13,160  -->  00:08:16,010
the probabilities are the same and here that's not the case.
185

185

00:08:16,010  -->  00:08:18,730
Because, of course, all of the probabilities are different.
186

186

00:08:18,730  -->  00:08:22,330
But that's the thing you are trained to make 100 steps
187

187

00:08:22,330  -->  00:08:24,030
by staying on a straight line,
188

188

00:08:24,030  -->  00:08:25,510
and so the principle of all this
189

189

00:08:25,510  -->  00:08:28,890
is that you are trained to do this for 100 steps
190

190

00:08:28,890  -->  00:08:31,790
so that when you make one step, when you have to take
191

191

00:08:31,790  -->  00:08:35,440
the challenge to make only one step and still be on
192

192

00:08:35,440  -->  00:08:37,690
that straight line, well you will have high chance
193

193

00:08:37,690  -->  00:08:42,070
of doing it because you were trained to do it for 100 steps.
194

194

00:08:42,070  -->  00:08:43,750
You were trained to stay on the straight line
195

195

00:08:43,750  -->  00:08:47,360
for 100 steps being blindfolded so when you have to do the
196

196

00:08:47,360  -->  00:08:50,190
same exercise it's much easier because you have to make it
197

197

00:08:50,190  -->  00:08:53,680
for one step, well you will have high chance to succeed.
198

198

00:08:53,680  -->  00:08:56,470
And so in that example, the correct prediction
199

199

00:08:56,470  -->  00:08:58,550
would be to manage to stay on the straight line
200

200

00:08:58,550  -->  00:09:00,990
after one step and the incorrect prediction
201

201

00:09:00,990  -->  00:09:04,400
would be to get out of the straight line after one step.
202

202

00:09:04,400  -->  00:09:07,470
So that's the whole principle of the blind walk technique
203

203

00:09:07,470  -->  00:09:10,200
from MCMC, Markov chain Monte Carlo.
204

204

00:09:10,200  -->  00:09:11,600
That's close to the random walk
205

205

00:09:11,600  -->  00:09:14,150
but keep in mind that the probabilities are not the same.
206

206

00:09:14,150  -->  00:09:16,290
And so now, you might have the correct answer
207

207

00:09:16,290  -->  00:09:18,090
to the question I just asked you.
208

208

00:09:18,090  -->  00:09:22,840
Here we were trained to make 10 steps blindfolded
209

209

00:09:22,840  -->  00:09:25,120
so that at the end of the 10 steps we stay
210

210

00:09:25,120  -->  00:09:26,190
on the straight line.
211

211

00:09:26,190  -->  00:09:29,350
So for one step we will be much better at it.
212

212

00:09:29,350  -->  00:09:31,780
We will be much better at staying on the straight line
213

213

00:09:31,780  -->  00:09:32,800
on one step.
214

214

00:09:32,800  -->  00:09:34,720
That's the whole principle behind it,
215

215

00:09:34,720  -->  00:09:37,200
and therefore the correct answer to the question,
216

216

00:09:37,200  -->  00:09:39,570
do we need to make one step or 10 steps?
217

217

00:09:39,570  -->  00:09:42,040
Is of course, we need to make one step.
218

218

00:09:42,040  -->  00:09:44,840
So our prediction will be directly the result
219

219

00:09:44,840  -->  00:09:48,050
of one round trip of Gibbs sampling.
220

220

00:09:48,050  -->  00:09:51,950
One iteration, one step of the blind walk.
221

221

00:09:51,950  -->  00:09:53,400
Alright, so let's make the step
222

222

00:09:53,400  -->  00:09:55,900
and then we'll get all our predictions of the test set
223

223

00:09:55,900  -->  00:09:57,070
in one shot.
224

224

00:09:57,070  -->  00:09:58,850
So things are gonna be very easy.
225

225

00:09:58,850  -->  00:10:01,000
And so now, the first thing we have to do, of course,
226

226

00:10:01,000  -->  00:10:03,690
is to remove this full loop because we don't have
227

227

00:10:03,690  -->  00:10:04,820
to make 10 steps.
228

228

00:10:04,820  -->  00:10:07,140
We only have to make one step.
229

229

00:10:07,140  -->  00:10:11,390
So, to make this step we're gonna start with an if condition
230

230

00:10:11,390  -->  00:10:14,570
because you know that's always the same idea we want to test
231

231

00:10:14,570  -->  00:10:17,290
the predictions to the ratings of the test set
232

232

00:10:17,290  -->  00:10:18,530
that actually exist.
233

233

00:10:18,530  -->  00:10:21,010
You know, in the test set we still have some ratings
234

234

00:10:21,010  -->  00:10:22,980
that are existent in the test set
235

235

00:10:22,980  -->  00:10:25,570
but we also still have some minus one ratings.
236

236

00:10:25,570  -->  00:10:28,560
So the minus one are ratings that just never happened.
237

237

00:10:28,560  -->  00:10:31,330
Whether it was in the training set or in the test set.
238

238

00:10:31,330  -->  00:10:34,010
And we don't want to consider these ratings in the test set,
239

239

00:10:34,010  -->  00:10:36,120
of course, that's always the same id.
240

240

00:10:36,120  -->  00:10:37,820
And so we are making this if condition
241

241

00:10:37,820  -->  00:10:42,060
to filter these non-existent ratings of the test set.
242

242

00:10:42,060  -->  00:10:45,440
And so to do this, we're gonna take the len function
243

243

00:10:45,440  -->  00:10:46,710
and parenthesis.
244

244

00:10:46,710  -->  00:10:49,900
And so now we're gonna get the ratings that are existent
245

245

00:10:49,900  -->  00:10:52,300
and we are gonna get them from the target,
246

246

00:10:52,300  -->  00:10:54,330
because we don't want to get them from the predictions,
247

247

00:10:54,330  -->  00:10:55,360
that wouldn't make sense.
248

248

00:10:55,360  -->  00:10:56,590
We wanna get them from the target
249

249

00:10:56,590  -->  00:10:58,470
because that's the target that contains
250

250

00:10:58,470  -->  00:11:00,600
the original ratings of the test set.
251

251

00:11:00,600  -->  00:11:04,660
And so here we take vt, and then again, some brackets
252

252

00:11:04,660  -->  00:11:07,970
and inside the brackets we take all the ratings
253

253

00:11:07,970  -->  00:11:09,830
that are existent.
254

254

00:11:09,830  -->  00:11:11,990
And the ratings that are existent are the ratings
255

255

00:11:11,990  -->  00:11:15,160
zero one, that is the ratings larger than zero.
256

256

00:11:15,160  -->  00:11:20,160
Alright, and so, if len, the length that is the number
257

257

00:11:20,260  -->  00:11:22,750
of divisible nodes containing set ratings
258

258

00:11:22,750  -->  00:11:27,750
must be larger than zero, and in that condition we can make
259

259

00:11:27,840  -->  00:11:28,673
some predictions.
260

260

00:11:28,673  -->  00:11:31,540
You know that's just a condition to allow ourselves
261

261

00:11:31,540  -->  00:11:33,060
to make some predictions.
262

262

00:11:33,060  -->  00:11:36,840
So no worries this will most of the time be verified.
263

263

00:11:36,840  -->  00:11:38,860
Alright, now time to make the predictions.
264

264

00:11:38,860  -->  00:11:40,650
And according to what I've just explained
265

265

00:11:40,650  -->  00:11:43,840
with the blind walk, well, we only have to make one step
266

266

00:11:43,840  -->  00:11:45,890
of the blind walk that is Gibbs sampling.
267

267

00:11:46,730  -->  00:11:48,930
Okay, so we don't have a k anymore
268

268

00:11:48,930  -->  00:11:51,610
because we don't have a loop over 10 steps,
269

269

00:11:51,610  -->  00:11:53,610
we only have one step so we're gonna remove
270

270

00:11:53,610  -->  00:11:56,350
the k here and remove the k here as well
271

271

00:11:56,350  -->  00:11:57,810
for the visible nodes.
272

272

00:11:57,810  -->  00:12:00,350
And here that's the same, remove the k
273

273

00:12:00,350  -->  00:12:03,390
for the input visible nodes that we'll use to get
274

274

00:12:03,390  -->  00:12:06,200
the first hidden node, well actually the only hidden node
275

275

00:12:06,200  -->  00:12:08,160
because there is one step only.
276

276

00:12:08,160  -->  00:12:12,290
And here we remove the k because we only have one vector
277

277

00:12:12,290  -->  00:12:15,570
of hidden nodes to get our final predictions,
278

278

00:12:15,570  -->  00:12:17,480
the vector of predicted ratings.
279

279

00:12:17,480  -->  00:12:21,380
And that's all, that's the only step of the blind walk.
280

280

00:12:21,380  -->  00:12:24,440
Then here we will get rid of this line
281

281

00:12:24,440  -->  00:12:26,230
because this was just for the training,
282

282

00:12:26,230  -->  00:12:29,100
you know, we used this line to only consider
283

283

00:12:29,100  -->  00:12:31,400
the existent ratings for the training
284

284

00:12:31,400  -->  00:12:33,410
but then we are not doing any training now
285

285

00:12:33,410  -->  00:12:35,660
so let's remove it.
286

286

00:12:35,660  -->  00:12:38,610
And then here that's the same, that was just to get one
287

287

00:12:38,610  -->  00:12:42,080
of the argument of the train function, the phk argument.
288

288

00:12:42,080  -->  00:12:45,570
And again, since we're not doing any training anymore
289

289

00:12:45,570  -->  00:12:49,600
well, we will get rid of these two lines that were just
290

290

00:12:49,600  -->  00:12:52,560
to update the weights in the training phase.
291

291

00:12:52,560  -->  00:12:54,940
So goodbye, and here we go.
292

292

00:12:54,940  -->  00:12:57,500
We are getting close to the final result.
293

293

00:12:57,500  -->  00:13:00,970
Now we have to update, not the train loss
294

294

00:13:00,970  -->  00:13:03,080
but the test loss.
295

295

00:13:03,080  -->  00:13:06,590
And again, we take the mean function from the torch library
296

296

00:13:06,590  -->  00:13:09,240
and here be careful, we are still taking the absolute
297

297

00:13:09,240  -->  00:13:12,870
distance between the prediction and the target.
298

298

00:13:12,870  -->  00:13:15,967
So, the target is, this time not v0
299

299

00:13:15,967  -->  00:13:20,220
but we called it vt, then here that's the same.
300

300

00:13:20,220  -->  00:13:22,770
We take all the ratings that are existent
301

301

00:13:22,770  -->  00:13:23,653
in the test set.
302

302

00:13:24,760  -->  00:13:27,000
And the prediction is not vk,
303

303

00:13:27,000  -->  00:13:30,760
because there is one step this time so we called it v.
304

304

00:13:30,760  -->  00:13:33,460
And here that's the same, we take the existent ratings
305

305

00:13:33,460  -->  00:13:36,090
so we take vt larger than zero.
306

306

00:13:36,090  -->  00:13:38,160
Remember with this vt larger than zero
307

307

00:13:38,160  -->  00:13:40,680
we are getting the indexes of the cells
308

308

00:13:40,680  -->  00:13:42,890
that have the existent ratings.
309

309

00:13:42,890  -->  00:13:46,550
Alright, so perfect and then we need to update the counter.
310

310

00:13:46,550  -->  00:13:48,490
That's to normalize the test loss.
311

311

00:13:48,490  -->  00:13:51,770
And finally, we are getting to the last line of code
312

312

00:13:51,770  -->  00:13:54,010
of this whole RBM model, which is
313

313

00:13:54,010  -->  00:13:56,090
to print the final test loss.
314

314

00:13:56,090  -->  00:13:59,730
So again, there is no epoch here so we will get rid
315

315

00:13:59,730  -->  00:14:01,420
of all this.
316

316

00:14:01,420  -->  00:14:04,960
Here we go, then in the first string here we will replace
317

317

00:14:04,960  -->  00:14:08,280
loss by test loss, you know to specify
318

318

00:14:08,280  -->  00:14:09,930
that is a test loss.
319

319

00:14:09,930  -->  00:14:12,790
And, of course, it would be a shame to make a mistake
320

320

00:14:12,790  -->  00:14:13,710
in the end.
321

321

00:14:13,710  -->  00:14:16,510
In the very end we need to replace train loss here
322

322

00:14:16,510  -->  00:14:20,670
by test loss, that we divide by s
323

323

00:14:20,670  -->  00:14:22,090
to normalize.
324

324

00:14:22,090  -->  00:14:24,930
Alright, so now, as usual, one last check
325

325

00:14:24,930  -->  00:14:28,120
before we execute because even if we get no warning
326

326

00:14:28,120  -->  00:14:29,880
it's quite easy to make a mistake
327

327

00:14:29,880  -->  00:14:31,960
besides when we copy paste something.
328

328

00:14:31,960  -->  00:14:34,920
So let's see, we start with a test loss of zero
329

329

00:14:34,920  -->  00:14:36,540
then the counter to zero.
330

330

00:14:36,540  -->  00:14:38,800
We loop over all our users.
331

331

00:14:38,800  -->  00:14:42,240
Then for all the ratings that are existent
332

332

00:14:42,240  -->  00:14:45,570
in the test set, we sample the hidden nodes first
333

333

00:14:45,570  -->  00:14:48,070
then we use these hidden nodes as input
334

334

00:14:48,070  -->  00:14:50,260
to sample divisible nodes.
335

335

00:14:50,260  -->  00:14:52,220
And we do this for only one round trip
336

336

00:14:52,220  -->  00:14:54,400
according to the MCMC theory.
337

337

00:14:54,400  -->  00:14:57,630
Then, actually, this needs to be in the if
338

338

00:14:57,630  -->  00:14:59,630
because we are still computing the test loss
339

339

00:14:59,630  -->  00:15:01,430
only for the existent ratings.
340

340

00:15:01,430  -->  00:15:04,060
And here that's the same, we increment s,
341

341

00:15:04,060  -->  00:15:06,050
still when that is verified.
342

342

00:15:06,050  -->  00:15:07,400
So there we go, now it's better.
343

343

00:15:07,400  -->  00:15:09,540
So with this test loss line of code
344

344

00:15:09,540  -->  00:15:12,550
we are computing the mean of the absolute distances
345

345

00:15:12,550  -->  00:15:15,400
between the predicted ratings and the real ratings.
346

346

00:15:15,400  -->  00:15:20,150
Then we increment s by one, and finally we print the final
347

347

00:15:20,150  -->  00:15:22,070
normalized test loss.
348

348

00:15:22,070  -->  00:15:26,020
Okay, perfect, so I think everything is all good now
349

349

00:15:26,020  -->  00:15:29,930
let's get the final result and let's hope we get
350

350

00:15:29,930  -->  00:15:34,200
a normalized test loss around the 0.25
351

351

00:15:34,200  -->  00:15:37,100
training loss that we obtained for the training set.
352

352

00:15:37,100  -->  00:15:39,310
So let's do this, fingers crossed.
353

353

00:15:39,310  -->  00:15:43,080
And let's press Command or Control plus, Enter to execute
354

354

00:15:43,080  -->  00:15:47,150
and we get a test loss of 0.24.
355

355

00:15:47,150  -->  00:15:49,300
Which is definitely excellent,
356

356

00:15:49,300  -->  00:15:52,370
because that's for new observations, so for new observations
357

357

00:15:52,370  -->  00:15:55,750
for new movies we managed to predict some correct ratings
358

358

00:15:55,750  -->  00:15:58,730
three times out of four and even better than that.
359

359

00:15:58,730  -->  00:16:00,970
Because we are slightly below 25%.
360

360

00:16:00,970  -->  00:16:03,900
So that's excellent results and we definitely managed
361

361

00:16:03,900  -->  00:16:06,670
to make a robust recommended system,
362

362

00:16:06,670  -->  00:16:08,670
but, remember this was the easy one.
363

363

00:16:08,670  -->  00:16:12,720
Predicting binary ratings zero one and in the next part
364

364

00:16:12,720  -->  00:16:14,900
we'll take it to the next level because we will be
365

365

00:16:14,900  -->  00:16:18,060
predicting some ratings from one to five.
366

366

00:16:18,060  -->  00:16:20,700
So that will increase the complexity of the problem,
367

367

00:16:20,700  -->  00:16:23,420
of course, working with binary values is easier
368

368

00:16:23,420  -->  00:16:27,080
and less complex than working with some continuous values
369

369

00:16:27,080  -->  00:16:28,880
but at least you'll know how to make
370

370

00:16:28,880  -->  00:16:31,980
two different recommended systems by applying
371

371

00:16:31,980  -->  00:16:34,120
two different deep learning models.
372

372

00:16:34,120  -->  00:16:36,260
But, the good news is that, even if the problem
373

373

00:16:36,260  -->  00:16:38,950
is more complex, while the model that we're gonna make
374

374

00:16:38,950  -->  00:16:41,840
in the next part, that is the outer encoders,
375

375

00:16:41,840  -->  00:16:44,380
is actually a much more simple model than this
376

376

00:16:44,380  -->  00:16:45,720
Boltzmann machine.
377

377

00:16:45,720  -->  00:16:48,250
You'll see that very quickly with the intuition lectures
378

378

00:16:48,250  -->  00:16:50,380
that it is actually more simple.
379

379

00:16:50,380  -->  00:16:52,850
But, never the less, despite the simplicity
380

380

00:16:52,850  -->  00:16:55,730
there is this amazing contrast with what it is able
381

381

00:16:55,730  -->  00:16:56,563
to make.
382

382

00:16:56,563  -->  00:16:59,070
You're gonna see that we will get some amazing predictions
383

383

00:16:59,070  -->  00:17:01,290
even for ratings between one to five.
384

384

00:17:01,290  -->  00:17:03,510
So I can't wait to make these powerful outer encoders
385

385

00:17:03,510  -->  00:17:05,940
with you, have fun with the intuition lectures
386

386

00:17:05,940  -->  00:17:08,180
and I'll see you after that for the practical tutorials
387

387

00:17:08,180  -->  00:17:09,953
and until then enjoy deep learning.
