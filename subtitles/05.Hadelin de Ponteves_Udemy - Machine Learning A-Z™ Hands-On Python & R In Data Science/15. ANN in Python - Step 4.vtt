WEBVTT
1
00:00:00.270 --> 00:00:01.710
Hello, my friends, welcome back.

2
00:00:01.830 --> 00:00:06.540
I'm sure you feel amazing after having built your very first artificial neural network.

3
00:00:06.780 --> 00:00:09.090
But remember, that's only half part of the job.

4
00:00:09.180 --> 00:00:13.690
The second half will be, of course, to train it on the whole training set.

5
00:00:13.830 --> 00:00:15.960
And we're going to do this in two steps.

6
00:00:16.200 --> 00:00:22.770
The first one is to compile the hénin with an optimizer lost function and a metric which will be, of

7
00:00:22.770 --> 00:00:26.280
course, the accuracy because we're doing some classification.

8
00:00:26.550 --> 00:00:31.920
And then the second step will be, of course, to train the N.N. on the training set over a certain

9
00:00:31.920 --> 00:00:32.870
number of ebooks.

10
00:00:33.390 --> 00:00:34.080
Are you ready?

11
00:00:34.350 --> 00:00:36.450
Let's do this, starting with this first step.

12
00:00:36.720 --> 00:00:38.350
Compiling the A.N..

13
00:00:39.180 --> 00:00:39.510
All right.

14
00:00:39.540 --> 00:00:46.500
So once again, doing this will be super simple thanks to the Tensor Flow Library that integrated keris,

15
00:00:46.770 --> 00:00:53.430
because indeed to compile are a and we first need to start from our end n object, which I remind was

16
00:00:53.430 --> 00:00:56.220
created as an instance of the sequential class.

17
00:00:56.700 --> 00:01:01.830
And then from this object, we're going to call a new method, which this time of course, won't be

18
00:01:01.830 --> 00:01:02.580
the add method.

19
00:01:02.880 --> 00:01:05.680
But can you actually guess what this method is going to be?

20
00:01:05.700 --> 00:01:09.450
You know, there is no trap, intensive flow, nor any confusion.

21
00:01:09.780 --> 00:01:18.090
Well, the method to compile an artificial neural network is simply the compile method as simple as

22
00:01:18.090 --> 00:01:18.390
that.

23
00:01:18.540 --> 00:01:18.870
Right.

24
00:01:18.900 --> 00:01:24.180
We didn't even have to look at the tensor flow documentation, which, by the way, I still recommend

25
00:01:24.180 --> 00:01:29.290
to have look at, because you will get a lot of information on the diverse tools you have in the tens

26
00:01:29.290 --> 00:01:30.000
of library.

27
00:01:30.400 --> 00:01:31.830
But here, it's super intuitive.

28
00:01:31.830 --> 00:01:32.700
It's super easy.

29
00:01:32.970 --> 00:01:35.040
So now I have a next question for you.

30
00:01:35.160 --> 00:01:39.630
According to you, what do we have to enter as parameters inside this compound method?

31
00:01:39.930 --> 00:01:41.310
Well, I actually said it.

32
00:01:41.460 --> 00:01:43.080
We have to enter three parameters.

33
00:01:43.320 --> 00:01:46.140
The first one is the optimizer to choose an optimizer.

34
00:01:46.530 --> 00:01:49.470
Then the second one is to luss to choose the lost function.

35
00:01:49.770 --> 00:01:53.700
And the third one is the matrix with an S parameter.

36
00:01:53.880 --> 00:01:54.330
Because no.

37
00:01:54.420 --> 00:01:59.430
That you can actually choose several metrics to evaluate your Eynon at the same time.

38
00:01:59.610 --> 00:02:02.310
But we will only choose one and we will choose the accuracy.

39
00:02:02.670 --> 00:02:03.210
But there you go.

40
00:02:03.240 --> 00:02:04.860
These are the three parameters.

41
00:02:04.890 --> 00:02:09.630
So I suggest that we start by entering them and then we'll enter their values.

42
00:02:10.240 --> 00:02:10.500
Okay.

43
00:02:10.560 --> 00:02:14.850
So let's start with the first one up to Mizer equals.

44
00:02:15.060 --> 00:02:15.390
All right.

45
00:02:15.450 --> 00:02:20.250
And come to the next one is the loss for the last function.

46
00:02:20.610 --> 00:02:25.530
And finally, the third one is the metrics parameter.

47
00:02:26.040 --> 00:02:26.510
All right.

48
00:02:26.640 --> 00:02:29.640
So for the optimizer, which one would you like to get?

49
00:02:30.030 --> 00:02:31.680
Well, into intuition lecturers.

50
00:02:31.710 --> 00:02:37.380
Curole mentioned that the best one are the optimizers that can perform to gas the gradient descent.

51
00:02:37.530 --> 00:02:42.990
And the best of them, you know, the one that I recommend by default is the atom optimizer, which

52
00:02:42.990 --> 00:02:47.430
is very performance optimizer that can perform stochastic gradient descent.

53
00:02:47.580 --> 00:02:51.450
And by that, let me just remind what's the Cassey gradient descent allows to do?

54
00:02:51.720 --> 00:02:57.990
Well, you know, it is what will update the weights in order to reduce the loss error between your

55
00:02:57.990 --> 00:03:00.330
predictions and the real results.

56
00:03:00.360 --> 00:03:04.800
You know, when we trained in and on the training set, we will at each iteration.

57
00:03:04.920 --> 00:03:09.930
Compare the predictions in a batch to the real results in the same batch.

58
00:03:10.200 --> 00:03:15.630
And that optimizer here will update the weights through stochastic gradient descent because we're going

59
00:03:15.630 --> 00:03:20.580
to choose Adam Optimizer to add the next iteration, hopefully reduce the loss.

60
00:03:21.120 --> 00:03:21.380
All right.

61
00:03:21.390 --> 00:03:26.220
So that's why right here we have to choose an optimizer, but also lost function, which is the way

62
00:03:26.220 --> 00:03:29.220
to compute the difference between the predictions and the real result.

63
00:03:29.370 --> 00:03:33.450
And then the accuracy, of course, because that's our final evaluation metric.

64
00:03:33.990 --> 00:03:34.240
All right.

65
00:03:34.260 --> 00:03:37.050
So as we said, we're going to choose the Adam optimizer.

66
00:03:37.080 --> 00:03:40.650
And the code name for that is simply but with no capital letter.

67
00:03:40.980 --> 00:03:41.400
Adam.

68
00:03:42.100 --> 00:03:42.450
OK.

69
00:03:42.900 --> 00:03:43.710
Congratulations.

70
00:03:43.740 --> 00:03:47.700
Now you know how to compile an artificial new network with an optimizer.

71
00:03:48.150 --> 00:03:50.910
But then we also have to combine it with a lost function.

72
00:03:51.240 --> 00:03:53.520
And now you have to know something very important.

73
00:03:53.730 --> 00:03:58.740
When you are doing binary classification, you know, classification, when you have to predict a binary

74
00:03:58.770 --> 00:03:59.310
outcome.

75
00:03:59.580 --> 00:04:03.120
Well, the lost function must always be the following.

76
00:04:03.120 --> 00:04:12.030
One entered in quotes, of course, which is binary underscore cross entropy just like that.

77
00:04:12.510 --> 00:04:17.520
And now let me tell you what you would have to enter if you were doing non binary classification.

78
00:04:17.580 --> 00:04:20.100
You know, like, for example, predicting three different categories.

79
00:04:20.400 --> 00:04:25.480
Well, here you would have to enter a category call cross entropy loss.

80
00:04:25.600 --> 00:04:25.920
OK.

81
00:04:26.100 --> 00:04:31.500
For binary classification, the laws must be binary cross entropy and for non binary classification,

82
00:04:31.500 --> 00:04:33.190
the laws must be categorical.

83
00:04:33.270 --> 00:04:37.770
Cross entropy and then also, you know, entering non binary classification.

84
00:04:37.860 --> 00:04:39.840
When predicting more than two categories.

85
00:04:40.140 --> 00:04:42.930
Well, the activation should not be sigmoid.

86
00:04:43.080 --> 00:04:44.510
But softmax.

87
00:04:44.730 --> 00:04:45.000
Right.

88
00:04:45.090 --> 00:04:50.160
I take this opportunity to also give you the other cases of classification which you could encounter.

89
00:04:50.490 --> 00:04:50.700
Okay.

90
00:04:50.850 --> 00:04:51.870
Now you know everything.

91
00:04:51.870 --> 00:04:57.720
And then remember that for regression, because we can also do artificial neural networks for regression.

92
00:04:57.990 --> 00:04:59.910
Well, we have this free course, which I.

93
00:04:59.970 --> 00:05:04.960
Give me the link, you can just take this course for free and you will get the full implementation of

94
00:05:04.960 --> 00:05:08.320
an artificial new network for a regression case study.

95
00:05:08.470 --> 00:05:13.150
So you have really everything that you can do with an artificial neural network.

96
00:05:13.750 --> 00:05:14.830
All right, great.

97
00:05:15.040 --> 00:05:18.220
And now let's enter the final parameter here, metrics.

98
00:05:18.760 --> 00:05:22.090
As I said, we can actually choose several metrics at the same time.

99
00:05:22.390 --> 00:05:27.130
Therefore, in order to enter the values of this parameter, well, we have to enter them in a pair

100
00:05:27.130 --> 00:05:31.960
of square brackets, which is supposed to be, you know, the list of the different metrics with which

101
00:05:31.960 --> 00:05:33.960
you want to evaluate your hénin.

102
00:05:34.120 --> 00:05:35.110
During the training.

103
00:05:35.440 --> 00:05:37.300
But we will only choose the main one.

104
00:05:37.330 --> 00:05:42.880
You know, the most essential one, which is the accuracy in which you have to enter in quotes.

105
00:05:43.090 --> 00:05:43.420
All right.

106
00:05:43.540 --> 00:05:45.970
Accuracy, just like the classic spelling.

107
00:05:46.290 --> 00:05:47.050
And now.

108
00:05:47.350 --> 00:05:47.590
Now.

109
00:05:47.590 --> 00:05:48.550
Congratulations.

110
00:05:48.760 --> 00:05:55.660
You know how to do a fool compile of your hénin with an optimizer, a loss and some metrics.

111
00:05:56.230 --> 00:05:56.770
Perfect.

112
00:05:56.860 --> 00:06:04.420
So now let's move on to the ultimate step, meaning the step where we will train the Eynon onto the

113
00:06:04.420 --> 00:06:05.800
whole training set.

114
00:06:06.040 --> 00:06:07.990
So let's create a new code cell.

115
00:06:08.260 --> 00:06:12.010
And now, according to you, how do we need to start this training?

116
00:06:12.430 --> 00:06:14.830
Well, once again, you know, it's always the same thing.

117
00:06:15.070 --> 00:06:20.590
We need to take our A and an object, then call a new method, which will perform the training and then

118
00:06:20.590 --> 00:06:22.120
enter a couple of parameters.

119
00:06:22.450 --> 00:06:23.410
So let's do this.

120
00:06:23.440 --> 00:06:26.140
Let's start with AION and first or object.

121
00:06:26.530 --> 00:06:30.650
And then according to you, what will be the method that can train you?

122
00:06:30.820 --> 00:06:31.140
Afternoon.

123
00:06:31.220 --> 00:06:32.400
That network on the training set.

124
00:06:32.830 --> 00:06:34.660
Well, nothing has changed here.

125
00:06:34.660 --> 00:06:41.200
And actually, I think I said it earlier in the course, the method to train whatever machinery model

126
00:06:41.430 --> 00:06:42.700
is always the same one.

127
00:06:42.820 --> 00:06:49.180
It is the fit method, the fit method, and which will take always the same parameters.

128
00:06:49.510 --> 00:06:57.490
The first one is X train for, you know, the matrix of features of the training set, then Y train

129
00:06:57.730 --> 00:07:00.340
for the deben voidable vector of the training set.

130
00:07:00.610 --> 00:07:06.460
And then when training an artificial new network, we actually to enter two more parameters which are

131
00:07:06.460 --> 00:07:15.130
first batch size, because indeed batch learning is always more efficient and more performant when training

132
00:07:15.130 --> 00:07:16.460
in artificial new network.

133
00:07:16.720 --> 00:07:21.850
Meaning that instead of comparing a prediction to the real result one by one, you know, to compute

134
00:07:21.850 --> 00:07:22.750
and reduce the loss.

135
00:07:22.960 --> 00:07:29.020
Well, you're going to do that with several predictions compared to several real results into a batch

136
00:07:29.260 --> 00:07:30.640
and the batch size here.

137
00:07:30.670 --> 00:07:35.470
You know, the batch size parameter gives exactly the number of predictions you want to have in the

138
00:07:35.470 --> 00:07:38.710
batch to be compared to that same number of real results.

139
00:07:39.100 --> 00:07:44.130
And the classic value of the batch size that is usually chosen is 32.

140
00:07:44.360 --> 00:07:44.620
Right.

141
00:07:44.680 --> 00:07:50.380
If you don't want to spend too much time tuning this hyper parameter, well, I recommend to choose

142
00:07:50.380 --> 00:07:51.340
the default value.

143
00:07:51.530 --> 00:07:52.080
Thirty two.

144
00:07:52.120 --> 00:07:56.920
But anyway, I wanted to highlight that hyper parameter here because indeed it is very important to

145
00:07:56.920 --> 00:07:59.710
remember that we doing batch learning.

146
00:07:59.900 --> 00:08:02.340
Okay, so batch size equal 32.

147
00:08:02.500 --> 00:08:07.180
And finally, I'm sure you know which final parameter we have to end to here.

148
00:08:07.450 --> 00:08:13.840
That's of course the number of books, you know, a neural network has to be trained over a certain

149
00:08:13.840 --> 00:08:14.830
amount of Epic's.

150
00:08:15.070 --> 00:08:17.710
So as to improve the accuracy over time.

151
00:08:17.840 --> 00:08:20.620
We will clearly see that once we execute this cell.

152
00:08:21.190 --> 00:08:25.880
So the name of the parameter for the number of Epic's is simply Epic's.

153
00:08:26.320 --> 00:08:30.910
And while you will see that it will go very fast so we can just take one hundred bucks.

154
00:08:30.970 --> 00:08:35.560
But once again, feel free to choose another number as long as it is not too small because you know,

155
00:08:35.560 --> 00:08:40.540
your neural network needs a certain amount of epochs in order to learn properly, you know, learn the

156
00:08:40.690 --> 00:08:43.210
correlations to get the ultimate best predictions.

157
00:08:44.020 --> 00:08:45.250
All right, great.

158
00:08:45.280 --> 00:08:47.500
So we're actually done with point three now.

159
00:08:47.590 --> 00:08:51.650
So I suggest we no longer wait and that we execute all the sales.

160
00:08:51.670 --> 00:08:55.960
We haven't executed so far, which I think, you know, start from part two, right?

161
00:08:56.080 --> 00:08:56.290
Yes.

162
00:08:56.320 --> 00:08:59.200
This was the last cell of the data pricing phase.

163
00:08:59.320 --> 00:09:00.400
It was run properly.

164
00:09:00.730 --> 00:09:05.590
So let's actually run each cell one by one and see what we're gonna get in the end.

165
00:09:05.920 --> 00:09:06.730
During the training.

166
00:09:06.880 --> 00:09:07.960
So let's go with this one.

167
00:09:08.050 --> 00:09:09.370
Initializing the A and.

168
00:09:10.000 --> 00:09:10.360
Good.

169
00:09:10.720 --> 00:09:14.200
Now this one adding the input layer and the first hidden layer.

170
00:09:14.890 --> 00:09:15.340
Good.

171
00:09:15.670 --> 00:09:18.010
Now this one adding the second hidden layer.

172
00:09:18.310 --> 00:09:18.910
All good.

173
00:09:19.270 --> 00:09:21.310
And now this one adding the output layer.

174
00:09:21.730 --> 00:09:22.690
All good still.

175
00:09:22.990 --> 00:09:24.490
Now we enter part three.

176
00:09:25.090 --> 00:09:26.080
Executing first.

177
00:09:26.290 --> 00:09:28.410
This cell compiling the N.N..

178
00:09:28.740 --> 00:09:29.350
All good.

179
00:09:29.670 --> 00:09:31.690
And now are you ready, my friends?

180
00:09:31.880 --> 00:09:36.310
We're about to train the artificial neural network on the training set.

181
00:09:36.610 --> 00:09:39.370
Over one hundred epochs.

182
00:09:39.430 --> 00:09:40.150
And here we go.

183
00:09:40.450 --> 00:09:41.800
The training is starting.

184
00:09:41.830 --> 00:09:43.870
And as I told you, it's going pretty fast.

185
00:09:43.900 --> 00:09:44.770
But look at this.

186
00:09:44.830 --> 00:09:48.480
Look at the accuracy and how it is evolving over the box.

187
00:09:48.520 --> 00:09:52.030
And we can see that it is actually increasing pretty fast.

188
00:09:52.120 --> 00:09:55.090
And mostly we see that it is actually converging.

189
00:09:55.410 --> 00:09:56.830
No converging pretty quickly.

190
00:09:56.920 --> 00:09:59.470
You know, we converged at all.

191
00:09:59.700 --> 00:10:06.910
In 86, you know, at about the 20th airpark, we actually didn't need that 100 bucks, but 20 was fine.

192
00:10:06.940 --> 00:10:11.890
But anyway, you know, it's going really fast and I'm sure it's very over soon now because indeed.

193
00:10:11.970 --> 00:10:12.370
Yes.

194
00:10:12.580 --> 00:10:13.210
There we go.

195
00:10:13.290 --> 00:10:16.420
The training was done in more or less 20 seconds.

196
00:10:16.690 --> 00:10:22.120
And the final accuracy we get on the training set, we'll have to check the same on the test set is

197
00:10:22.270 --> 00:10:24.280
averaging around four point eighty six.

198
00:10:24.310 --> 00:10:25.000
That's really good.

199
00:10:25.030 --> 00:10:29.500
That means that out of 100 observations, you have 86 correct predictions.

200
00:10:29.890 --> 00:10:31.030
So congratulations.

201
00:10:31.030 --> 00:10:34.120
You made a very good first deep learning model.

202
00:10:34.210 --> 00:10:38.590
You can be proud of yourself and mostly now you can take a little break because we're going to enter

203
00:10:38.590 --> 00:10:42.660
part four and not only we're going to end support for, but also you're going to see that you're going

204
00:10:42.660 --> 00:10:48.670
to have a little homework, which will consist of predicting the result of a single observation, winning

205
00:10:48.790 --> 00:10:49.900
a single customer.

206
00:10:50.010 --> 00:10:53.680
You will have to predict of this customer will stay in or leave the bank.

207
00:10:53.950 --> 00:10:59.890
You will enter your solution here and we will implement a solution together in the next tutorial.

208
00:11:00.250 --> 00:11:01.540
So make sure to do it.

209
00:11:01.570 --> 00:11:03.720
Please try at least to do it.

210
00:11:03.730 --> 00:11:08.830
You actually know how to do it because we already learned how to do a single prediction before, you

211
00:11:08.830 --> 00:11:10.600
know, a prediction of a single observation.

212
00:11:10.930 --> 00:11:12.010
So you have everything.

213
00:11:12.010 --> 00:11:15.640
Maybe check out again by three classification if you have a doubt.

214
00:11:15.910 --> 00:11:16.820
But there you go.

215
00:11:16.840 --> 00:11:17.710
That's your homework.

216
00:11:17.980 --> 00:11:23.740
You have to use our Ayen and Model to predict if the customer with the following information will leave

217
00:11:23.830 --> 00:11:24.310
the bank.

218
00:11:24.370 --> 00:11:25.000
Yes or no.

219
00:11:25.390 --> 00:11:31.210
And he's following inflammations or that it is a French customer with a credit score of 600, a male

220
00:11:31.210 --> 00:11:31.570
one.

221
00:11:31.720 --> 00:11:33.040
He is four years old.

222
00:11:33.130 --> 00:11:34.810
He has been in the bank for three years.

223
00:11:35.080 --> 00:11:38.290
He has sixty thousand dollars in his account.

224
00:11:38.560 --> 00:11:40.060
He has to pirogues in the bank.

225
00:11:40.210 --> 00:11:41.540
He has a great car indeed.

226
00:11:41.710 --> 00:11:47.020
He is also an active member and he has an estimated salary of fifty thousand dollars.

227
00:11:47.350 --> 00:11:51.040
And the question is, so shall we say good bye to that customer?

228
00:11:51.400 --> 00:11:54.150
Well, please figure it out and we will see if you're right.

229
00:11:54.280 --> 00:11:55.330
In the next tutorial.

230
00:11:55.780 --> 00:11:57.550
Until then, enjoy machine learning.
