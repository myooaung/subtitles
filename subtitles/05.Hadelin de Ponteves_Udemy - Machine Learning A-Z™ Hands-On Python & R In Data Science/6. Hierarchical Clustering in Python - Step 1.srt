1
00:00:00,180 --> 00:00:05,490
Hello my friends and welcome to this new practical activity where we're going to build together this

2
00:00:05,490 --> 00:00:12,330
time the hierarchical clustering algorithm and the case study for this model will be the same as for

3
00:00:12,390 --> 00:00:19,380
k means because you know the purpose of hierarchical clustering is to give you an extra clustering model

4
00:00:19,380 --> 00:00:25,470
to compare to k means so that you can at least have two clustering models in your toolkit for clustering

5
00:00:25,800 --> 00:00:30,000
and therefore we're going to work on the exact same data set same problem.

6
00:00:30,000 --> 00:00:36,360
You know that model trying to understand better its customers by finding some patterns which are clusters

7
00:00:36,420 --> 00:00:37,420
in the data.

8
00:00:37,470 --> 00:00:39,040
All right so let's do this.

9
00:00:39,090 --> 00:00:42,190
First let's make sure everyone here is on the same page.

10
00:00:42,210 --> 00:00:47,520
I give you the link to this folder containing the whole structure of the course with all the codes and

11
00:00:47,520 --> 00:00:49,590
data set in the previous articles.

12
00:00:49,590 --> 00:00:51,090
Make sure to connect to it.

13
00:00:51,240 --> 00:00:58,620
And now we're all gonna go into port for clustering to this time tackle the hierarchical clustering

14
00:00:58,680 --> 00:00:59,290
moral.

15
00:00:59,400 --> 00:01:02,850
And we're going to start with Python as usual.

16
00:01:02,850 --> 00:01:03,310
All right.

17
00:01:03,330 --> 00:01:09,450
So as I've just told you we're gonna work on the same data set more customers right where each row corresponds

18
00:01:09,450 --> 00:01:12,270
to a customer and for each of these customers.

19
00:01:12,270 --> 00:01:19,050
Well the mob gathered some info like the customer I.D. to jar male female to age the annual income and

20
00:01:19,050 --> 00:01:24,580
the spending score and we're actually going to work only with these two features the annual income and

21
00:01:24,580 --> 00:01:27,840
the spending score to identify these clusters.

22
00:01:27,840 --> 00:01:30,900
But this time with hierarchical clustering.

23
00:01:30,930 --> 00:01:31,230
All right.

24
00:01:31,230 --> 00:01:36,860
So executives same and therefore let us proceed directly to the implementation.

25
00:01:36,900 --> 00:01:38,250
You have it right here.

26
00:01:38,250 --> 00:01:44,790
Hierarchical clustering that IP when B which you can open with either Google collaboratively as we're

27
00:01:44,820 --> 00:01:50,300
about to do or if you don't like Google collaboratively you can open it with Jupiter notebook.

28
00:01:50,310 --> 00:01:50,810
All right.

29
00:01:50,880 --> 00:01:52,630
So choose your favorite idea.

30
00:01:52,670 --> 00:01:54,600
And for Google collaboratively lovers.

31
00:01:54,600 --> 00:02:00,100
Well follow me here to open this implementation in Google collab.

32
00:02:00,670 --> 00:02:06,120
All right so right now it is loading the notebook you know laying out a notebook and Dario.

33
00:02:06,300 --> 00:02:13,110
So that's the horrible Google clustering implementation as you can see it follows the exact same structure

34
00:02:13,230 --> 00:02:20,670
as k means we first import the libraries we then imported data set exactly the same way as how we did

35
00:02:20,670 --> 00:02:26,280
for k means you know we select these two columns of index three and four which corresponds of course

36
00:02:26,280 --> 00:02:29,430
to the annual salary and the spending score.

37
00:02:29,430 --> 00:02:35,880
So executives saying we want re-employment is together and then this time instead of using the Elbow

38
00:02:35,880 --> 00:02:42,090
Method you know like with K Means Well we're gonna use the DNA program to find the optimal number of

39
00:02:42,090 --> 00:02:42,870
customers.

40
00:02:42,960 --> 00:02:48,540
And I will explain not only the implementation you know we will implement this together and also I will

41
00:02:48,540 --> 00:02:53,730
explain how to find that optimal number of clusters in this graph.

42
00:02:53,730 --> 00:02:59,490
And finally we will train the hierarchical clustering model on the data set using the agile narrative

43
00:02:59,500 --> 00:03:00,690
clustering class.

44
00:03:00,750 --> 00:03:07,590
And finally we will visualize the clusters exactly the same way as what we did with K Means and actually

45
00:03:07,590 --> 00:03:09,700
hear the code is exactly the same.

46
00:03:09,720 --> 00:03:14,070
The only thing that changes is the name of that dependent variable which we create.

47
00:03:14,070 --> 00:03:19,830
Right because still with hierarchical clustering we are going to create that dependent variable.

48
00:03:19,830 --> 00:03:26,030
But this time instead of calling it y k means as we did for k means we're calling it simply y HFC.

49
00:03:26,190 --> 00:03:32,070
And therefore here it's exactly the same code with only that different name for that created dependent

50
00:03:32,070 --> 00:03:32,670
variable.

51
00:03:32,670 --> 00:03:38,460
So we want to implement this either we'll just keep the code and therefore there we go we're only going

52
00:03:38,460 --> 00:03:44,520
to re implement two sales which is this one to build a Daniel gram and figure out that optimal number

53
00:03:44,520 --> 00:03:51,130
of clusters and to build the hierarchical grocery model and train it on the whole dataset.

54
00:03:51,180 --> 00:03:52,050
Are you ready.

55
00:03:52,050 --> 00:03:53,040
Let's do this.

56
00:03:53,040 --> 00:03:58,590
And in order to do this we have to create a copy of this notebook because this is in read only mode

57
00:03:58,860 --> 00:04:05,070
and therefore we're going to go to file here and then click Save a copy enjoy drive to indeed create

58
00:04:05,340 --> 00:04:08,380
a copy of this notebook.

59
00:04:08,460 --> 00:04:09,300
Perfect.

60
00:04:09,330 --> 00:04:09,690
All right.

61
00:04:09,720 --> 00:04:11,210
So there we go.

62
00:04:11,220 --> 00:04:11,980
That's our copy.

63
00:04:11,990 --> 00:04:13,130
Now we can modify it.

64
00:04:13,140 --> 00:04:18,220
Now we can re implement it but as we've just said we won't re implement everything.

65
00:04:18,230 --> 00:04:25,260
We will just re-employment these two cells here first the DNA program how to build it and how to read

66
00:04:25,260 --> 00:04:25,620
it.

67
00:04:25,740 --> 00:04:31,680
And then of course well how to build the hierarchical clustering model on the dataset and then we keep

68
00:04:31,800 --> 00:04:37,560
the other cells because they're exactly the same as in k means and if you want it's just you know remove

69
00:04:37,560 --> 00:04:40,060
this that we don't see the final result.

70
00:04:40,230 --> 00:04:41,430
And perfect now.

71
00:04:41,430 --> 00:04:42,180
All right.

72
00:04:42,180 --> 00:04:48,330
All you see here is executives same as with k means the only thing that will change are these two sales

73
00:04:48,450 --> 00:04:51,510
which we will re implement together.

74
00:04:51,510 --> 00:04:51,830
Okay.

75
00:04:51,830 --> 00:04:52,260
Perfect.

76
00:04:52,260 --> 00:04:58,980
So first step let's just execute these two for sales here and to do this we need of course to upload

77
00:04:59,100 --> 00:05:02,470
the data set so let's click this further here.

78
00:05:02,580 --> 00:05:08,670
Now it is connecting to a runtime to enable fibrosis you know a new computer in your machine and in

79
00:05:08,670 --> 00:05:12,140
a second we should see the upload button.

80
00:05:12,150 --> 00:05:13,110
There we go.

81
00:05:13,110 --> 00:05:15,070
Upload and end.

82
00:05:15,120 --> 00:05:17,340
Well I'm already in the Caymans folder.

83
00:05:17,340 --> 00:05:20,290
But let me show you again the whole path.

84
00:05:20,370 --> 00:05:26,370
So that's the folder you were given at the beginning of each section including this one higher clustering

85
00:05:26,610 --> 00:05:28,110
which you could download on your machine.

86
00:05:28,110 --> 00:05:29,730
So I hope you have it right now.

87
00:05:29,730 --> 00:05:34,380
Otherwise you would just need to go back to the previous article and now we're all gonna go inside.

88
00:05:34,380 --> 00:05:39,940
Then we're going to go to part 4 of course then Section 25 hierarchical clustering.

89
00:05:40,020 --> 00:05:41,070
Then python.

90
00:05:41,070 --> 00:05:42,080
And then there we go.

91
00:05:42,090 --> 00:05:44,600
More customers start CSB.

92
00:05:44,790 --> 00:05:48,140
This will upload the dataset into the notebook.

93
00:05:48,270 --> 00:05:52,850
And so now we can run these two for sales first importing the libraries.

94
00:05:52,860 --> 00:06:01,320
And now that we have vendors we can import the data set which at the same time creates this matrix of

95
00:06:01,320 --> 00:06:08,550
two features containing only let's see in the dataset containing only the annual income and the spending

96
00:06:08,550 --> 00:06:08,910
score.

97
00:06:08,910 --> 00:06:13,350
In other words x is just these two columns here with all the rows.

98
00:06:13,350 --> 00:06:13,800
Okay.

99
00:06:14,310 --> 00:06:14,630
All right.

100
00:06:14,660 --> 00:06:17,310
So they should be pressing face done.

101
00:06:17,310 --> 00:06:24,240
Now we can focus on the heart of the higher Google Classroom model which is first to build the Daniel

102
00:06:24,250 --> 00:06:27,240
gram to indeed find the optimal number of clusters.

103
00:06:27,270 --> 00:06:32,730
And of course the optimal number of clusters that will result from this general gram will be the same

104
00:06:32,730 --> 00:06:36,680
number as the one we found with k means meaning five clusters.

105
00:06:36,720 --> 00:06:42,450
But I will explain how to read Daniel gram in order to indeed end up with an optimal number of five

106
00:06:42,450 --> 00:06:43,700
clusters.

107
00:06:43,710 --> 00:06:44,100
All right.

108
00:06:44,130 --> 00:06:48,110
So that was introduction and as you know I like to take it step by step.

109
00:06:48,120 --> 00:06:53,390
So we will implement that next step of building the Daniel gram in the next material.

110
00:06:53,430 --> 00:06:54,510
So get ready.

111
00:06:54,510 --> 00:06:56,430
And until then enjoy machine learning.
