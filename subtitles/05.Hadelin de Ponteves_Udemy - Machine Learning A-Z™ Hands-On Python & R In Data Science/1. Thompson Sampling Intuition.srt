1
00:00:00,830 --> 00:00:03,470
Hello and welcome back to the course on machine learning.

2
00:00:03,470 --> 00:00:08,740
Today we've got a very interesting topic today we're talking about Thompson sampling and the algorithms

3
00:00:08,860 --> 00:00:09,690
intuition.

4
00:00:09,740 --> 00:00:15,940
And again we're going to be using this algorithm to solve the multi armed bandit problem.

5
00:00:16,160 --> 00:00:16,480
All right.

6
00:00:16,490 --> 00:00:18,070
So let's get started.

7
00:00:18,080 --> 00:00:20,820
A quick refresher on that multi armed bandit problem.

8
00:00:20,840 --> 00:00:25,630
We have several slot machines each one of them has a distribution behind it.

9
00:00:25,630 --> 00:00:27,130
Do you want to G-5.

10
00:00:27,140 --> 00:00:32,810
We don't know what these distributions are and we need to start playing these machines and at the same

11
00:00:32,810 --> 00:00:39,700
time figure out which one has the best distribution so that we then exploit that best distribution.

12
00:00:39,860 --> 00:00:45,770
And what it'll take us some time to figure it out so we need to maximize our return during the process

13
00:00:45,770 --> 00:00:46,290
of figuring.

14
00:00:46,290 --> 00:00:53,860
So we have to find that ideal balance or tradeoff between exploration and exploitation.

15
00:00:53,870 --> 00:00:59,510
We had a few tutorials previously on these things so we talked about the multi armed bandit problem

16
00:00:59,530 --> 00:01:00,570
you know a lot of detail.

17
00:01:00,580 --> 00:01:04,200
If you haven't watched the tutorial highly recommend jumping into the previous section and watching

18
00:01:04,200 --> 00:01:12,020
it there they're also understanding of the upper confidence bound algorithm will really help you grasp

19
00:01:12,320 --> 00:01:20,570
the concepts of Thompson sampling so if you haven't seen the upper confidence bound tutorial then I

20
00:01:20,570 --> 00:01:24,260
highly recommend checking that out before proceeding with today's lecture.

21
00:01:24,260 --> 00:01:24,700
All right.

22
00:01:24,740 --> 00:01:26,760
So let's get started.

23
00:01:26,780 --> 00:01:34,840
A quick summary of the mulcher armband problem is as follows We have d arms for example arms are ads

24
00:01:34,850 --> 00:01:35,990
that we just play to users.

25
00:01:35,990 --> 00:01:38,730
Each time they connect to a web page so by the way.

26
00:01:38,780 --> 00:01:40,850
Yes indeed.

27
00:01:41,330 --> 00:01:47,470
A modern application of the more or modern representation of a multi armed bandit problem is advertising

28
00:01:47,480 --> 00:01:54,800
so when you display ads that is very similar or the algorithms are going to be applying can learning

29
00:01:55,490 --> 00:01:59,210
can be applied to solving a problem where you are displaying ads.

30
00:01:59,210 --> 00:02:04,400
If you go back here instead of having these slot machines each one of these is a different add and you

31
00:02:04,400 --> 00:02:09,260
want to figure out which one of these ads is the best performing ad but you don't have time to do an

32
00:02:09,260 --> 00:02:09,740
AB test.

33
00:02:09,740 --> 00:02:13,200
You don't have the funds or the resources to do an AB test.

34
00:02:13,220 --> 00:02:15,080
You want to figure it out on the fly.

35
00:02:15,080 --> 00:02:21,170
That's when you apply one of these algorithms are we talking about in this part of the course and of

36
00:02:21,250 --> 00:02:26,110
courses of or other lots of other modern problems that are very similar.

37
00:02:26,110 --> 00:02:31,210
There must have been a problem there for these algorithms are valid for them too.

38
00:02:31,220 --> 00:02:34,090
All right so moving back here so we've got DRM.

39
00:02:34,100 --> 00:02:39,020
For example arms are ads that we just place uses a shell that connects to a web page each time a user

40
00:02:39,020 --> 00:02:45,500
connects to this page that is considered as a round at each round and we choose which ad is displayed

41
00:02:45,500 --> 00:02:53,180
to the user at restrung and the ad gives us a reward either 0 or 1 1 if the ad is clogs or if the ad

42
00:02:53,170 --> 00:02:54,000
is not clicked.

43
00:02:54,110 --> 00:03:00,230
In the case of the actual bandits it will be a monetary war or will be just to one it will be of some

44
00:03:00,230 --> 00:03:01,640
value.

45
00:03:01,640 --> 00:03:06,280
Our goal is to Maxxis maximize the total reward we get over many runs.

46
00:03:06,290 --> 00:03:11,050
All right so that's a quick overview of the problem though is solving.

47
00:03:11,280 --> 00:03:20,060
Then here we've got some very complex looking mathematics and Bayesian inference and all these distributions

48
00:03:21,160 --> 00:03:28,810
and Syria operability and other prior distributions and bebetter dispositions and so on.

49
00:03:28,960 --> 00:03:34,420
We're not going to delve deeper into this right now so headline will take some time to walk through

50
00:03:34,420 --> 00:03:43,210
this slide with you in the practical tutorials because we're going to be coding this from scratch.

51
00:03:43,450 --> 00:03:44,440
This in our.

52
00:03:44,740 --> 00:03:49,360
And therefore he will actually walk you through this slide.

53
00:03:49,360 --> 00:03:55,900
Our goal for today is to understand the intuition behind all of these things so we're going to skip

54
00:03:55,960 --> 00:03:58,710
skip the slide and get to the intuition part.

55
00:03:58,720 --> 00:04:04,570
And this is the actual steps that you're going to be using in the practical tutorial.

56
00:04:04,570 --> 00:04:07,900
So again had had will walk you through this slide as well.

57
00:04:08,440 --> 00:04:10,300
And today we're talking about intuition.

58
00:04:10,480 --> 00:04:14,230
So this is going to be fun this is some interesting slides coming up.

59
00:04:14,260 --> 00:04:16,960
And get ready for a fun ride.

60
00:04:16,960 --> 00:04:22,930
So grab a cup of coffee or a cup of tea and some popcorn and let's get started.

61
00:04:22,930 --> 00:04:23,220
All right.

62
00:04:23,230 --> 00:04:26,390
So here we've got a scale.

63
00:04:26,570 --> 00:04:27,250
Why.

64
00:04:27,280 --> 00:04:33,120
Or the horizontal axis is the return the return that we expected to get from a band.

65
00:04:33,130 --> 00:04:37,540
So we're going to look at a simplified problem we're going to look at another five or even 10 we're

66
00:04:37,540 --> 00:04:42,000
going to get three bandits because they're going to a lot of stuff going on on this chart.

67
00:04:42,010 --> 00:04:46,960
And I don't want to clutter it up and we want to keep it as simple as possible just so that we can understand

68
00:04:46,960 --> 00:04:53,970
the concept and then something applies for five or 10 or however many machines you'd be looking at.

69
00:04:54,400 --> 00:05:01,270
So here we've got the return and these vertical lines just like in the case of the upper confidence

70
00:05:01,270 --> 00:05:01,990
bounds.

71
00:05:02,080 --> 00:05:08,740
We had the horizontal lines these vertical lines represent the expected return for each of the machines

72
00:05:08,740 --> 00:05:15,370
so each of the machines out of the three machines that we have each of the machines has a distribution

73
00:05:15,370 --> 00:05:23,020
behind it so that the result the amount of money that you win per game is picked as a random value from

74
00:05:23,020 --> 00:05:24,200
that distribution.

75
00:05:24,650 --> 00:05:28,780
And when you get a job description but basically just imagine distribution behind each one of these

76
00:05:30,870 --> 00:05:31,780
expected values.

77
00:05:31,780 --> 00:05:38,700
So this is this is just the center of central distribution or the actual expected return from that machine.

78
00:05:38,710 --> 00:05:47,170
And we're just visualizing it here but this is as kind of like looking into it into the actual machine

79
00:05:47,170 --> 00:05:49,950
itself like pulling it apart and knowing how it works.

80
00:05:49,960 --> 00:05:53,710
Being the designer of the machine in reality when you're playing these machines of course you don't

81
00:05:53,710 --> 00:05:54,030
know.

82
00:05:54,040 --> 00:05:58,620
So this is some additional information that will guide us.

83
00:05:58,620 --> 00:06:03,270
That'll help us understand how the algorithm actually works the algorithm itself doesn't know this information

84
00:06:03,270 --> 00:06:03,440
right.

85
00:06:03,460 --> 00:06:07,800
So this is hidden but it's just there for us so that we can better understand what's going on.

86
00:06:08,020 --> 00:06:11,640
So these are the expected returns the actual expected returns of each of the machines.

87
00:06:11,640 --> 00:06:15,850
And obviously if you knew this right away you would say that machine of the three or the yellow machine

88
00:06:15,880 --> 00:06:19,990
is the best one because it has the highest return right.

89
00:06:19,990 --> 00:06:23,070
It has the highest return you'd always just bet on this one.

90
00:06:23,080 --> 00:06:25,990
But again you don't know this yet.

91
00:06:26,110 --> 00:06:28,800
All right so what happens with this algorithm.

92
00:06:28,810 --> 00:06:34,450
Well at the start just like with upper confidence band algorithm you don't know anything right.

93
00:06:34,480 --> 00:06:42,010
You have no prior knowledge of the current situation or status of things and therefore all the machines

94
00:06:42,010 --> 00:06:42,670
are identical.

95
00:06:42,730 --> 00:06:49,520
And you have to have at least one or even a couple of trial rounds just to get some data to analyze.

96
00:06:49,550 --> 00:06:50,600
So let's see.

97
00:06:50,620 --> 00:06:56,100
Let's say that has happened and there's some trial runs for some machine.

98
00:06:56,140 --> 00:06:57,580
One or the blue machine.

99
00:06:57,730 --> 00:07:05,440
And based on those trial runs the algorithm the Tompson something get them this is where it starts getting

100
00:07:05,470 --> 00:07:10,530
different to the upper conditions but we'll construct a distribution right.

101
00:07:10,540 --> 00:07:12,060
So we'll get to the distribution.

102
00:07:12,080 --> 00:07:13,280
Second what means.

103
00:07:13,300 --> 00:07:17,500
But for now let's do the same thing for machine the green machine.

104
00:07:17,500 --> 00:07:24,310
And so we're just pulling the arm several times like four times for instance and we're getting some

105
00:07:24,310 --> 00:07:28,430
values and that are going to be somewhere around.

106
00:07:28,450 --> 00:07:31,590
Obviously the actual expected return.

107
00:07:31,660 --> 00:07:36,230
And based on this and those values of course is probably going to be a bit more than four.

108
00:07:36,340 --> 00:07:42,360
We're constructing some sort of distribution or some sort of perception of the current state of things.

109
00:07:42,400 --> 00:07:44,730
And this is the part where it gets interesting.

110
00:07:44,740 --> 00:07:54,610
So the actual meaning of this these distributions is not what you might think at first so these distributions

111
00:07:55,000 --> 00:08:02,410
are actually showing us or they're representing not to the degree we're not trying to guess the distributions

112
00:08:02,410 --> 00:08:03,430
behind the machine.

113
00:08:03,430 --> 00:08:05,980
So the first thing that might come to mind is that.

114
00:08:06,130 --> 00:08:12,550
All right so we've constructed distribution and so the blue distribution is our attempt at guessing

115
00:08:12,550 --> 00:08:15,300
the actual distribution behind the blue machine right.

116
00:08:15,310 --> 00:08:20,460
The distribution that this is expected to turn for and the green is for the green machine the elephant.

117
00:08:21,160 --> 00:08:23,250
Well actually not that's not the case.

118
00:08:23,290 --> 00:08:28,260
We are constructing something completely different something completely out of this world.

119
00:08:28,340 --> 00:08:36,380
We're constructing distributions of where we think the actual expected value might lie.

120
00:08:37,230 --> 00:08:44,880
It's very important to understand that so we are creating kind of a if you want think of it that way

121
00:08:44,880 --> 00:08:48,800
we're creating an auxiliary mechanism for us to solve the problem.

122
00:08:48,810 --> 00:08:56,550
So we're not we're not trying to recreate these machines we're recreating the possible way these machines

123
00:08:56,550 --> 00:09:03,960
could have been created kind of in that sense so let's let's just solidify that this is where we think

124
00:09:04,170 --> 00:09:09,310
that expected the actual expected values will be so let's look at the blue one for instance.

125
00:09:09,330 --> 00:09:15,810
We've got four values and based on those four values we've constructed this distribution which will

126
00:09:15,810 --> 00:09:21,300
show up which is showing us where is that value star.

127
00:09:21,300 --> 00:09:24,000
So this is mystar the actual mystar but we don't know it.

128
00:09:24,000 --> 00:09:28,830
So the algorithm doesn't know it so it's constructed the distribution trying to guess where this value

129
00:09:28,830 --> 00:09:29,150
is.

130
00:09:29,160 --> 00:09:32,070
And of course Kansas say it's here or is there it's over there.

131
00:09:32,240 --> 00:09:36,630
It's saying OK there's a is a very high likelihood here but it also could be here and could be here

132
00:09:36,630 --> 00:09:37,180
could be here.

133
00:09:37,320 --> 00:09:42,210
And as you move away the likelihood drops but it still could be anywhere in this space.

134
00:09:42,210 --> 00:09:45,060
Same thing for the green distribution.

135
00:09:45,060 --> 00:09:50,490
So based on the values that we've seen there and valleys that have been selected in the four four rounds

136
00:09:52,620 --> 00:10:02,260
that algorithm has created this distribution which is saying that this actual actual expected return

137
00:10:02,500 --> 00:10:08,220
from machine from the green machine is somewhere in this area and it's most likely to be here.

138
00:10:08,230 --> 00:10:11,400
But it could be here it could be here could be here it could be anywhere it could be.

139
00:10:11,410 --> 00:10:17,950
So is it most more likely to be here than could be here could be here and as you move away the likelihood

140
00:10:17,950 --> 00:10:21,110
of it actually been there drops off something for the oil.

141
00:10:21,110 --> 00:10:27,020
So it's very important on the stand so just to reiterate we are not trying to guess the distributions

142
00:10:27,040 --> 00:10:28,290
behind machines right.

143
00:10:28,450 --> 00:10:34,230
We are doing like kind of a little magic trick or a little hat trick.

144
00:10:34,250 --> 00:10:42,040
I don't know Patrick but we're trying to do this create this perception of the world we're trying to

145
00:10:44,380 --> 00:10:51,580
mathematically explain what we think is actually going on or what could be going on and that is that

146
00:10:51,720 --> 00:11:00,100
is important also important thing because this demonstrates that the Tomsen sampling is a probabilistic

147
00:11:00,220 --> 00:11:01,120
algorithm.

148
00:11:01,120 --> 00:11:07,150
The upper confidence Bond was a deterministic algorithm where everything was strict you know everything

149
00:11:07,150 --> 00:11:12,640
was OK so whichever one has a pie some pre-conference bound us on we're going to choose and so on.

150
00:11:12,700 --> 00:11:18,020
But here we are creating a probabilistic perception of the world we're saying.

151
00:11:18,130 --> 00:11:22,630
So it's likely to be here but it could be anywhere in the blue area and this one could be new in history

152
00:11:22,690 --> 00:11:27,810
and so on and you'll see exactly why we've done this in the next slide will understand how this works.

153
00:11:28,000 --> 00:11:29,610
And let's jump straight into that.

154
00:11:29,620 --> 00:11:35,770
Let's understand now that this is probably the hardest part to kind of get your head around what we've

155
00:11:35,770 --> 00:11:36,190
created.

156
00:11:36,220 --> 00:11:43,690
And now that we've created it let's see how the algorithm is going to utilize this auxilary mechanism

157
00:11:43,690 --> 00:11:44,300
that we have.

158
00:11:44,380 --> 00:11:46,000
Chris Lawrence.

159
00:11:46,510 --> 00:11:49,380
So there are our distributions that's where we think.

160
00:11:49,390 --> 00:11:53,540
So these are the actual expected returns for each of the machines.

161
00:11:53,740 --> 00:11:57,410
But the algorithm doesn't know them the old algorithm has created is.

162
00:11:57,430 --> 00:12:05,700
Are these distributions where we allow it to kind of guess where the actual distribution might lie for

163
00:12:05,690 --> 00:12:07,200
each for each of these machines.

164
00:12:07,210 --> 00:12:11,050
So what is going to do is it's actually going to trigger each of these distributions.

165
00:12:11,050 --> 00:12:16,450
So like we're in the we're in in year round we have to choose a machine to use.

166
00:12:16,480 --> 00:12:23,230
So what the algorithm will do is it will go and call this distribution and it will pull out of value

167
00:12:23,230 --> 00:12:27,780
out of the suspicion and let's say it pulled that value then will pull a value out of the grid distribution.

168
00:12:27,880 --> 00:12:33,220
So pulled that values out of the grinch vision and let's say and then pulls a value of Yeldon.

169
00:12:33,250 --> 00:12:35,540
And this is that decision or that value.

170
00:12:35,710 --> 00:12:39,280
And again it's pulling them so according to the distribution right.

171
00:12:39,280 --> 00:12:44,980
So this is a distribution of values so basically it's most likely to pull a value somewhere in this

172
00:12:44,980 --> 00:12:48,110
area then less likely in the way you go.

173
00:12:48,130 --> 00:12:54,520
It's more or less less and less likely but still it can happen that you can see this yellow values actually

174
00:12:54,520 --> 00:12:59,260
quite far from the center but it still can happen that it pulled this Valier distribution and it can

175
00:12:59,260 --> 00:13:03,610
happen and pick this Greenan value out of the green distribution totally can happen in the long term

176
00:13:03,610 --> 00:13:07,730
of course going to be picking somewhere close to the center like over the long run.

177
00:13:07,900 --> 00:13:11,370
But on a one off basis this can totally happen.

178
00:13:11,500 --> 00:13:17,950
And so now let's pick these values and guess what that means will what this actually means is that we

179
00:13:17,950 --> 00:13:19,030
have.

180
00:13:19,030 --> 00:13:23,570
By doing that we have generated our own branded configuration.

181
00:13:23,830 --> 00:13:30,430
So we have created a this hypothetical or imaginary

182
00:13:33,330 --> 00:13:41,100
batch of mush or not batch imaginary set of machines in our own virtual world where we are saying Okay

183
00:13:41,140 --> 00:13:45,840
so the expected the actual expected return for the blue machine.

184
00:13:45,850 --> 00:13:50,740
Is this value the actual expected return for the green machine is this value and the actual expected

185
00:13:50,740 --> 00:13:52,450
return for the yellow machine is this value.

186
00:13:52,450 --> 00:14:01,600
So we've created this Sub-Zero sayto world or hypothetical virtual reality in which we have all our

187
00:14:01,600 --> 00:14:04,950
own three bandits and how we're going to solve this problem.

188
00:14:05,680 --> 00:14:07,670
And is this a problem is very easy to solve.

189
00:14:07,690 --> 00:14:09,760
It's obvious how to solve this problem.

190
00:14:09,790 --> 00:14:15,620
You just pick this machine right because this machine has the highest expected return out of the street.

191
00:14:15,940 --> 00:14:20,720
And I was just going to go with this machine in the virtual world in the search.

192
00:14:20,780 --> 00:14:29,290
So the new reality and what that means is that now we translate this result into the actual world in

193
00:14:29,290 --> 00:14:32,150
the virtual hypothetical world.

194
00:14:32,140 --> 00:14:37,420
We've selected the green machine so in the actual world that the algorithm will also select the green

195
00:14:37,420 --> 00:14:41,560
machine and what that will do so basically pull the lever for this machine.

196
00:14:41,590 --> 00:14:42,080
Right.

197
00:14:42,160 --> 00:14:48,160
And what that will do is actually it will give us a values to the machine spit it out or spit out a

198
00:14:48,160 --> 00:14:53,680
value but that value is going to be based on the distribution behind this machine where this is the

199
00:14:53,680 --> 00:14:55,860
actual expected value of that distribution.

200
00:14:55,870 --> 00:15:01,510
So the value is going to be somewhere here probably close to the actual expected value doesn't have

201
00:15:01,510 --> 00:15:01,810
to be.

202
00:15:01,810 --> 00:15:03,480
Again there's a distribution behind all this.

203
00:15:03,620 --> 00:15:06,260
Far you could because but let's say in this case is this.

204
00:15:06,460 --> 00:15:06,750
All right.

205
00:15:06,760 --> 00:15:10,630
So now this information this is new information to the algorithm.

206
00:15:10,780 --> 00:15:16,450
What is it going to do is it's going to say Aha OK so the I pull the green lever.

207
00:15:16,470 --> 00:15:18,860
The lever for the green machine I got this value.

208
00:15:19,030 --> 00:15:22,030
So now I have to adjust my perception of the world.

209
00:15:22,030 --> 00:15:28,880
Right so I have a prior probability So these are all for the Green Machine This is my prior distribution.

210
00:15:28,880 --> 00:15:33,190
This is where the vision in France comes into play or is it's already been in play and this is where

211
00:15:33,190 --> 00:15:34,900
adding to the Bajun inference.

212
00:15:34,900 --> 00:15:37,370
So that's our prior product.

213
00:15:37,390 --> 00:15:39,390
Now we've got some new information.

214
00:15:39,490 --> 00:15:44,230
This is our new information we're going to added in and see where see how that changes our perception

215
00:15:44,260 --> 00:15:46,850
of the world our perception of the role has changed.

216
00:15:46,930 --> 00:15:53,680
The tradition has shifted a bit and it's become narrower because we have more information we are sample

217
00:15:53,680 --> 00:15:55,040
sizes and creeds of course.

218
00:15:56,420 --> 00:16:00,680
Excuse me of course it's not going to increase that that much.

219
00:16:00,680 --> 00:16:06,120
This is just an example to demonstrate what we're talking about to get the point across.

220
00:16:06,380 --> 00:16:11,930
But that's that's the point that every time we add new information our distribution becomes more and

221
00:16:11,930 --> 00:16:12,910
more refined.

222
00:16:13,160 --> 00:16:18,230
All right so now we have a new perception in the world now what happens next is a new round.

223
00:16:18,230 --> 00:16:18,500
Right.

224
00:16:18,500 --> 00:16:21,000
Same thing we're going to do the same thing again for a neuron.

225
00:16:21,230 --> 00:16:28,460
Again we generate or we pick some values out of our distributions that they are now we've constructed

226
00:16:28,520 --> 00:16:36,440
a or we generate our own branded configuration in our virtual virtual reality or in our a hypothetical

227
00:16:36,440 --> 00:16:41,300
world out of these three we have to pick the best bandit which is of course the one here the yellow

228
00:16:41,320 --> 00:16:48,710
bandit and we're going to now puled actually pool in the real world pull that lever of the yellow bandit

229
00:16:49,130 --> 00:16:50,320
all algorithms can do that.

230
00:16:50,330 --> 00:16:54,020
That's going to trigger the distribution behind the yellow bandit.

231
00:16:54,080 --> 00:16:56,600
And that will give us some sort of value.

232
00:16:56,600 --> 00:17:00,320
So this is the actual value that we received in the real world.

233
00:17:00,320 --> 00:17:05,840
Now we're going to incorporate that value into our perception of the world and our perception of the

234
00:17:05,840 --> 00:17:07,360
world is going to change.

235
00:17:07,390 --> 00:17:10,910
It's just that we go now we're into debt again.

236
00:17:10,910 --> 00:17:11,290
All right.

237
00:17:11,300 --> 00:17:16,700
So let's do this one more time just so we practice the logic behind all of this.

238
00:17:16,730 --> 00:17:23,670
So neuron's generates the generous banded configuration right.

239
00:17:23,690 --> 00:17:31,460
So this is what we think that our expected actual expected returns are going to be Alban's confession.

240
00:17:31,460 --> 00:17:37,880
This is obviously the best one we're going to use pool the yellow machines are a lever that's going

241
00:17:37,880 --> 00:17:43,530
to trigger the distribution behind the old machine is going to spit out a value in the real world.

242
00:17:43,620 --> 00:17:50,120
There is a value and then we're going to have to adjust our perception of the world again to match or

243
00:17:50,120 --> 00:17:57,050
to incorporate the new information and so on we're going to keep doing that until we get to a point

244
00:17:57,170 --> 00:18:02,960
where we've refined the distributions substantially and picture looks like that so they might be to

245
00:18:02,960 --> 00:18:04,010
provide even more.

246
00:18:04,040 --> 00:18:08,870
This one might be more fun this one might be more refined but as you could see from there we slowly

247
00:18:08,870 --> 00:18:14,820
will start generating more and more rounds based on the yellow machine will be treating the machine

248
00:18:14,870 --> 00:18:20,450
So these ones might not even get that refined in the long run which is totally fine because our point

249
00:18:20,450 --> 00:18:25,280
is to get to the best machine to find it and exploit it as much as we can.

250
00:18:25,370 --> 00:18:26,450
So there we go.

251
00:18:26,450 --> 00:18:30,590
That is pretty much how the Tomsen sampling algorithm works.

252
00:18:30,590 --> 00:18:39,800
And as you can see it is a stick algorithm and every time we're generating these values they are kind

253
00:18:39,800 --> 00:18:49,310
of creating this hypothetical set up of the bandits and then we're solving that and then we're applying

254
00:18:49,310 --> 00:18:50,420
the results to the real world.

255
00:18:50,540 --> 00:18:54,560
We're adjusting our perception of reality based on the new information that generates and then we're

256
00:18:54,560 --> 00:18:55,380
doing that again.

257
00:18:55,580 --> 00:18:59,710
So hopefully this was a interesting tutorial.

258
00:18:59,720 --> 00:19:02,050
I find this algorithm very very cool.

259
00:19:02,150 --> 00:19:08,360
And in the next tutorial we're going to compare a little bit of a confidence bond and the Thompson sampling

260
00:19:08,360 --> 00:19:12,190
algorithm and I can't wait to see there until next time happy analyzing.
