1

00:00:00,510  -->  00:00:06,360
Hello and welcome to this art Tauriel today we're going to implement support vector machine or more

2

00:00:06,360  -->  00:00:08,220
commonly called SVM.

3

00:00:08,220  -->  00:00:13,080
And after having implemented it on python we were going to do it on our.

4

00:00:13,140  -->  00:00:20,580
So let's quickly set the folder as working directory machinary easy part 3 classification SBM.

5

00:00:20,640  -->  00:00:25,800
And here we are make sure that you have this social network and starts your file and then you can click

6

00:00:25,800  -->  00:00:29,250
on this more button here to set the folder as a working directory.

7

00:00:29,250  -->  00:00:30,120
All good.

8

00:00:30,180  -->  00:00:35,010
Now we're going to go to our classification template that we made in the logistic regression section

9

00:00:35,020  -->  00:00:35,250
.

10

00:00:35,490  -->  00:00:46,360
So let's take everything from here to the end copy and let's face it in our SVM file.

11

00:00:46,410  -->  00:00:46,930
Here we go.

12

00:00:46,960  -->  00:00:50,160
Now we need to change very few things.

13

00:00:50,190  -->  00:00:56,250
So first we need to create or classify here which is going to be the SVM transfer of course and then

14

00:00:56,250  -->  00:01:01,380
we just need to change the title here for the craft the training search results.

15

00:01:01,380  -->  00:01:08,680
So we're going to specify that it's the SVM classifier same for the test set here as of yet.

16

00:01:08,910  -->  00:01:13,210
And now let's move back up to create our classifier.

17

00:01:13,540  -->  00:01:13,860
OK.

18

00:01:13,860  -->  00:01:20,070
So as usual we are going to import a library and this time the library is going to be the most popular

19

00:01:20,070  -->  00:01:23,750
library that is used for SVM which is the library.

20

00:01:23,750  -->  00:01:25,330
It turned 71.

21

00:01:25,440  -->  00:01:31,140
So there are actually two very popular library ads intensive anyone and current lab.

22

00:01:31,230  -->  00:01:32,990
And actually I tried them both.

23

00:01:33,000  -->  00:01:34,570
We had 10 kind of the same results.

24

00:01:34,680  -->  00:01:36,880
So you can try the other one to her lab.

25

00:01:36,930  -->  00:01:38,810
It's actually almost the same parameters.

26

00:01:38,820  -->  00:01:45,370
You just have to press F1 to look at the parameters on to K SVM because the function for current that

27

00:01:45,420  -->  00:01:48,080
is k SVM and you could check it out.

28

00:01:48,090  -->  00:01:51,390
But right now we're going to do it with each 71.

29

00:01:51,420  -->  00:01:54,580
So let's do this first let's have a look at the packages here.

30

00:01:54,690  -->  00:02:01,050
Check out to see if you have the Eton 71 library it is installed on my arse because I used to do a lot

31

00:02:01,050  -->  00:02:05,010
of times but if you installed our For the first time then you might not have it.

32

00:02:05,160  -->  00:02:08,280
So I'm going to just write this command here.

33

00:02:08,280  -->  00:02:13,720
For those of you who need to install it so install packages.

34

00:02:13,770  -->  00:02:14,550
Here it is.

35

00:02:14,550  -->  00:02:20,360
And then in parenthesis you put in quotes the name of the library which is named this way.

36

00:02:20,580  -->  00:02:23,100
Ten seventy 71.

37

00:02:23,130  -->  00:02:23,490
All right.

38

00:02:23,490  -->  00:02:28,440
So if you select an exit to this line it will install the package.

39

00:02:28,440  -->  00:02:31,740
I won't do it because it's already installed but it will do it.

40

00:02:31,770  -->  00:02:32,460
I promise you.

41

00:02:32,710  -->  00:02:35,920
So I'm going to put that comment there.

42

00:02:36,120  -->  00:02:39,330
And now let's start creating or classifier.

43

00:02:39,330  -->  00:02:48,660
Well first we need to know that this line library turned 71 and that in case you want to make some automated

44

00:02:48,660  -->  00:02:54,110
script that select automatically your library here because this won't be selected all the time.

45

00:02:54,120  -->  00:02:56,050
So you want to make sure that you select it.

46

00:02:56,280  -->  00:02:59,320
And now we are going to create our SVM class.

47

00:02:59,430  -->  00:03:04,230
So as usual we are going to call our classifier classifier equals.

48

00:03:04,680  -->  00:03:08,760
And then here we're going to use very simply the function as V.

49

00:03:08,770  -->  00:03:10,820
And so let's do this as V.

50

00:03:10,830  -->  00:03:17,870
And then let's press F1 to look at the parameters F1 and here are the parameters the arguments.

51

00:03:17,880  -->  00:03:19,880
So the first argument is formula.

52

00:03:20,160  -->  00:03:21,230
So let's add it.

53

00:03:21,230  -->  00:03:28,830
So as usual it's the formula that is the dependent variable expressed using Attila with respect to all

54

00:03:28,830  -->  00:03:31,920
your independent variables which you represent by a dot.

55

00:03:31,920  -->  00:03:34,750
So here we're just going to add formula equals.

56

00:03:34,800  -->  00:03:42,630
So first we take our dependent variable which is purchased all right and now we used to still the here

57

00:03:42,720  -->  00:03:49,160
and then a dot dot meaning that we're taking all the independent variables of our data set.

58

00:03:49,430  -->  00:03:50,030
OK.

59

00:03:50,130  -->  00:03:55,850
Now come up to go to the next argument then the next argument is data.

60

00:03:55,860  -->  00:04:01,230
So of course that's the data on which you want to train your classifier on which you want your class

61

00:04:01,230  -->  00:04:05,230
for you to learn the data to make the future classification.

62

00:04:05,360  -->  00:04:08,960
And of course this data is our training set.

63

00:04:09,420  -->  00:04:11,890
Okay perfect come to.

64

00:04:12,030  -->  00:04:18,100
Let's go to the next argument the next argument is X and Y but we don't really care about that.

65

00:04:18,270  -->  00:04:26,370
And however we do care about the time here and the kernel because the type as you notice there is two

66

00:04:26,370  -->  00:04:33,090
types of as V.M. there is that as V and for classification and the as we are for regression which is

67

00:04:33,090  -->  00:04:38,790
kind of the same support vector machines algorithm but there is one for classification and one for regression

68

00:04:38,790  -->  00:04:38,820
.

69

00:04:38,820  -->  00:04:45,830
So here with this argument type you choose the classification then the default type for classification

70

00:04:45,830  -->  00:04:47,490
is see classification.

71

00:04:47,490  -->  00:04:50,220
So that's the one we're going to choose for type.

72

00:04:50,270  -->  00:05:02,050
So let's add here type equals C classification are right and now the last very important argument is

73

00:05:02,050  -->  00:05:03,200
the kernel.

74

00:05:03,250  -->  00:05:09,580
So we're starting with the basics we're starting with the most simple as V M which is the Linnie are

75

00:05:09,620  -->  00:05:10,430
as V.M..

76

00:05:10,450  -->  00:05:15,610
So here we're going to choose to a linear our kernel the linear kernel and then we're going to try some

77

00:05:15,610  -->  00:05:19,400
more sophisticated SVM with some gaussian kernel.

78

00:05:19,450  -->  00:05:24,070
So I will let you find out about the surprise you can actually try to practice by yourself and play

79

00:05:24,070  -->  00:05:28,450
around with the different kernels if you want to you know do the course in advance.

80

00:05:28,450  -->  00:05:33,280
That's actually a very good thing to do a course in advance and then watched a course to compare your

81

00:05:33,370  -->  00:05:35,270
work with the content of the course.

82

00:05:35,380  -->  00:05:42,650
But for now we are going to choose a linear kernel's so he will improve the kernel equals Linna.

83

00:05:42,850  -->  00:05:44,440
All right and that's all.

84

00:05:44,440  -->  00:05:49,290
With these four arguments our CROSSFIRE is ready as VM classifier is ready.

85

00:05:49,570  -->  00:05:54,490
So we're going to select the different steps over our sections of our code.

86

00:05:54,520  -->  00:05:58,600
So let's first select this one to preprocess the data.

87

00:05:58,960  -->  00:05:59,950
Here it is.

88

00:06:00,230  -->  00:06:02,710
OK so perfect we have a data set.

89

00:06:02,710  -->  00:06:05,940
Here are trainset and our test set.

90

00:06:06,070  -->  00:06:12,520
So dataset contains 400 observations which are informations about users of a social network including

91

00:06:12,520  -->  00:06:16,240
the age and the estimated salary in this column purchased here tells.

92

00:06:16,270  -->  00:06:24,250
If yes or no the user bought a car when the user got the ad that the car company put on the social network

93

00:06:24,550  -->  00:06:26,730
for marketing campaign purposes.

94

00:06:26,880  -->  00:06:32,560
And so our transfer as usual is going to try to classify the users into the two categories.

95

00:06:32,650  -->  00:06:35,230
Yes they bought the car and no they didn't buy the car.

96

00:06:35,530  -->  00:06:38,030
OK so let's go back to our classifier.

97

00:06:38,290  -->  00:06:43,010
So the pre-processing is done and now we're going to create a classifier.

98

00:06:43,090  -->  00:06:48,120
Everything should go well and I'll have to select this because mine was already selected.

99

00:06:48,430  -->  00:06:52,140
So let's press command control plus enter to execute.

100

00:06:52,150  -->  00:06:55,340
And here it is cast failure created.

101

00:06:55,380  -->  00:06:56,240
All good.

102

00:06:56,590  -->  00:06:57,010
OK.

103

00:06:57,000  -->  00:07:02,470
Now we can make some predictions of new observations which are here to test that observation so let's

104

00:07:02,470  -->  00:07:03,490
do it.

105

00:07:03,500  -->  00:07:05,320
White bread white bread.

106

00:07:05,320  -->  00:07:07,810
All right so let's have a look at why bread.

107

00:07:07,900  -->  00:07:10,420
Why KRAD.

108

00:07:11,230  -->  00:07:16,470
So these are all the predictions of the tested observations that is for each user of the test sets or

109

00:07:16,470  -->  00:07:19,700
classify or predicts if the user buys the SUV or not.

110

00:07:19,990  -->  00:07:24,910
So let's compare the predictions with the truth which are contained in the set.

111

00:07:25,180  -->  00:07:29,960
So this column tells the truth about whether users but yes or no.

112

00:07:29,970  -->  00:07:35,890
The SUV and this are the predictions there is are as VM transfer of predicted whether each user bought

113

00:07:36,050  -->  00:07:37,270
one or the SUV.

114

00:07:37,260  -->  00:07:39,660
So let's look at for example the Web.

115

00:07:39,820  -->  00:07:40,030
OK.

116

00:07:40,030  -->  00:07:43,560
So all these first guys here were predicted not to buy the SUV.

117

00:07:43,570  -->  00:07:46,070
We have all zeroes until the 103.

118

00:07:46,420  -->  00:07:53,830
But here as you can see we have some guys who actually bought the SUV in reality is the 18 19 20 and

119

00:07:53,830  -->  00:07:56,970
22 there are classified predicted that they didn't buy it.

120

00:07:57,160  -->  00:07:59,540
And the truth is that they actually bought it.

121

00:07:59,560  -->  00:08:01,510
So that's incorrect predictions.

122

00:08:01,500  -->  00:08:07,700
But let's look at the incorrect predictions in a more efficient way by looking at the confusion matrix

123

00:08:07,700  -->  00:08:08,000
.

124

00:08:08,050  -->  00:08:11,240
So let's select this line here and execute.

125

00:08:11,350  -->  00:08:15,530
And now let's find out about the real number of incorrect predictions.

126

00:08:15,790  -->  00:08:18,660
So let's stop C.M. here in the council and press enter.

127

00:08:19,120  -->  00:08:23,630
And wow that's actually a quite large number of incorrect predictions.

128

00:08:23,830  -->  00:08:28,600
And by the way we don't intend the same results as in Python because we have some random factors in

129

00:08:28,600  -->  00:08:32,690
the model and here we didn't specify seat so you might actually have some different results.

130

00:08:32,710  -->  00:08:34,930
But the ideas here.

131

00:08:34,960  -->  00:08:38,740
So OK so let's look at now the graph to see how it's doing.

132

00:08:38,760  -->  00:08:44,980
So for those of you who actually didn't watch the Python tutorial about as V.M. a good exercise is to

133

00:08:45,040  -->  00:08:47,110
try to guess what's going to happen.

134

00:08:47,110  -->  00:08:51,930
That is what we're going to be the prediction regions where what is going to be the prediction boundary

135

00:08:51,940  -->  00:08:52,240
.

136

00:08:52,240  -->  00:08:54,730
What do you guess you will see.

137

00:08:54,750  -->  00:08:56,230
So I will let you think for a second.

138

00:08:56,230  -->  00:08:59,370
You can pass on the video and right now I'm going to tell you.

139

00:08:59,380  -->  00:09:07,180
So as you notice we chose a linear kernel which means that are as being classified as a linear classifier

140

00:09:07,190  -->  00:09:07,370
.

141

00:09:07,540  -->  00:09:14,690
So as I explained in the logistic regression tutorials on Python and are well a linear classifier in

142

00:09:14,680  -->  00:09:17,620
2D dimensional space is a straight line.

143

00:09:17,620  -->  00:09:23,010
So here I'm telling you this right now we're going to get a straight line not to get any disappointment

144

00:09:23,290  -->  00:09:28,890
because I know we proved our model with a cane and before we obtained good prediction boundary that

145

00:09:29,020  -->  00:09:35,760
cut you know those users on the bottom right corner that's actually but the SUV but who were incorrect

146

00:09:35,800  -->  00:09:38,160
predations for logistic regression.

147

00:09:38,160  -->  00:09:43,830
Well here it's also going to be some indirect protection for the linearize VM because it's actually

148

00:09:43,960  -->  00:09:45,520
a linear classifier.

149

00:09:45,660  -->  00:09:48,000
So let's look at the results right now.

150

00:09:48,010  -->  00:09:51,940
Select this and press command control press enter to execute

151

00:09:54,040  -->  00:09:56,180
.

152

00:09:56,590  -->  00:09:58,670
And here are the results.

153

00:09:58,990  -->  00:09:59,290
OK.

154

00:09:59,290  -->  00:10:01,510
So as you can see that's exactly what I just told you.

155

00:10:01,510  -->  00:10:08,830
You know those users here with a lower salary and higher age well they actually bought the SUV in reality

156

00:10:09,250  -->  00:10:15,490
because the points are green the points are the real observations but they fell into the red region

157

00:10:15,520  -->  00:10:21,390
here because since the Lynbrook testifiers a straight line it couldn't you know made some kind of curve

158

00:10:21,390  -->  00:10:28,840
here to catch all the red points into the right place and therefore it caught some green points putting

159

00:10:28,840  -->  00:10:30,410
them in the red region.

160

00:10:30,420  -->  00:10:33,520
So yeah that's exactly the same as for logistic regression.

161

00:10:33,660  -->  00:10:39,860
Sorry about the disappointment but don't worry in the next section we will introduce a new kind of Caspari

162

00:10:39,880  -->  00:10:44,510
which will be well the kernel as VM with a different color than Lindauer kernel.

163

00:10:44,560  -->  00:10:49,510
It's going to be a gaussian kernel or even we can try some more kernels where you can practice that

164

00:10:49,510  -->  00:10:50,190
yourself.

165

00:10:50,200  -->  00:10:52,040
It can be very good practice for you.

166

00:10:52,260  -->  00:10:57,810
Let's hear yes it's a linear testifiers So basically it's the same as logistic regression.

167

00:10:58,270  -->  00:11:04,390
And if we look at the test that result is going to be the same let's have a look.

168

00:11:07,030  -->  00:11:08,400
And here is the test set.

169

00:11:08,530  -->  00:11:08,850
OK.

170

00:11:08,860  -->  00:11:17,040
So yeah same thing here we have some users of higher age and lower estimated salary but yes U-V but

171

00:11:17,200  -->  00:11:22,990
fell into the red region again because our class very is a straight line and that's the best you could

172

00:11:22,990  -->  00:11:28,360
do to classify those two points and place them into the corresponding category.

173

00:11:28,620  -->  00:11:32,360
So a few incorrect predictions here a few incorrect additions here.

174

00:11:32,500  -->  00:11:36,550
And if you want you can count the number of incorrectly actions that is the number of green points in

175

00:11:36,550  -->  00:11:42,330
the red region plus the number of red points in the green region and you will count the number of incorrect

176

00:11:42,340  -->  00:11:48,260
predictions we found in the confusion matrix which was 20 incorrect predictions.

177

00:11:48,310  -->  00:11:54,170
That's great that we have this result because that gives us the motivation to improve our classifier

178

00:11:54,260  -->  00:11:55,600
improve our model.

179

00:11:55,720  -->  00:11:57,690
And that's what we're going to do in the next section.

180

00:11:57,700  -->  00:12:03,610
So I look forward to seeing you in the next section and show you how we can substantially improve our

181

00:12:03,610  -->  00:12:05,140
classification model.

182

00:12:05,160  -->  00:12:07,260
So I look forward to showing you the next level.

183

00:12:07,270  -->  00:12:09,170
And until then enjoy machine learning
