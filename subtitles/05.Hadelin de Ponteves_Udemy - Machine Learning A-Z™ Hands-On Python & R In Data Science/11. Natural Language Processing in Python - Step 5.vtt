WEBVTT
1
00:00:00.270 --> 00:00:01.220
Hello my friends.

2
00:00:01.230 --> 00:00:06.840
Are you ready for the most essential step of this implementation which is at the heart of sentiment

3
00:00:06.840 --> 00:00:13.530
analysis that is creating the bag of words model which we are ready to do now because all our reviews

4
00:00:13.650 --> 00:00:15.360
are properly cleaned.

5
00:00:15.480 --> 00:00:21.750
So we're going to get them into the back of words model to create you know this porous matrix which

6
00:00:21.750 --> 00:00:24.320
will contain in the Rose Well the different reviews.

7
00:00:24.320 --> 00:00:30.870
You know the same reviews as the ones in our corpus and in the columns all the different words taken

8
00:00:30.870 --> 00:00:37.230
from all the different reviews you know all of them and each cell will either get a zero or a one it

9
00:00:37.230 --> 00:00:44.700
will get a zero if the word of the column is not in the review of the row and it will get a one if the

10
00:00:44.700 --> 00:00:49.330
word of the column is indeed part of the words in the review of the row.

11
00:00:49.380 --> 00:00:49.740
All right.

12
00:00:49.770 --> 00:00:52.230
So that's where the sparse matrix is about.

13
00:00:52.260 --> 00:00:58.230
And the process of creating all these columns corresponding to each of the words taken from all the

14
00:00:58.230 --> 00:01:00.810
reviews is called tokenization.

15
00:01:00.810 --> 00:01:03.190
So that's exactly what we'll do in this new cell.

16
00:01:03.210 --> 00:01:05.970
But first let me actually show you what we created.

17
00:01:05.970 --> 00:01:11.640
You know I just want to show you the corpus so actually right here we're going to create a new code

18
00:01:11.640 --> 00:01:20.100
cell and I'm just gonna do a print of the corpus so that I can show you indeed what we created.

19
00:01:20.100 --> 00:01:23.890
So let's press play here and this will show the corpus.

20
00:01:24.330 --> 00:01:25.000
All right.

21
00:01:25.050 --> 00:01:30.420
So this is the first review after the cleaning you know after all this cleaning process in different

22
00:01:30.420 --> 00:01:31.110
steps.

23
00:01:31.110 --> 00:01:34.290
Remember I can actually show you the original review here.

24
00:01:34.290 --> 00:01:40.260
The original review was wow with capital letters some punctuation is here with the three little dots

25
00:01:40.350 --> 00:01:42.420
and then left this place.

26
00:01:42.420 --> 00:01:46.770
And after the cleaning process it became while love place.

27
00:01:46.890 --> 00:01:51.450
Indeed we removed all the top words such as you know this.

28
00:01:51.450 --> 00:01:56.780
You know that's an oracle that doesn't give any hint on whether the review is positive or negative.

29
00:01:56.790 --> 00:02:02.030
However of course we kept loved because loved means of course that the review is positive.

30
00:02:02.070 --> 00:02:05.490
However we transformed loved into love.

31
00:02:05.490 --> 00:02:07.190
That's the process of stemming.

32
00:02:07.200 --> 00:02:13.050
So we can simplify all the words by their root and then of course we kept plays because that's of course

33
00:02:13.260 --> 00:02:14.400
not a step forward.

34
00:02:14.400 --> 00:02:19.530
All right then let's have a look at the second one crust is not good.

35
00:02:19.920 --> 00:02:25.680
All right let's try to guess actually how it was transformed so crust was just transform into crust

36
00:02:25.740 --> 00:02:29.970
with a lowercase c then is was probably removed.

37
00:02:29.970 --> 00:02:30.240
Right.

38
00:02:30.240 --> 00:02:35.730
Because it doesn't give a hint on whether the review is positive or negative not was definitely kept

39
00:02:35.760 --> 00:02:40.800
because that's a negative statement and good was of course kept.

40
00:02:40.800 --> 00:02:41.340
OK.

41
00:02:41.370 --> 00:02:47.700
So after the transformation you know after all the cleaning this review must become crust with a lower

42
00:02:47.700 --> 00:02:50.180
case c not good.

43
00:02:50.250 --> 00:02:53.560
Let's check that it is the case and crust.

44
00:02:53.640 --> 00:02:54.600
Oh OK.

45
00:02:54.600 --> 00:03:01.140
So actually they removed the knot which is a bit strange actually because you know not clearly indicates

46
00:03:01.410 --> 00:03:02.220
a negative thing.

47
00:03:02.220 --> 00:03:03.410
You know a negative review.

48
00:03:03.480 --> 00:03:07.800
We clearly have a difference between crust is good and crust is not good.

49
00:03:08.250 --> 00:03:17.030
So I think we need to do some extra work here in order to not include the not word from the top word.

50
00:03:17.100 --> 00:03:19.220
And I'm going to show you how you can do this.

51
00:03:19.230 --> 00:03:20.670
It's very easy.

52
00:03:20.730 --> 00:03:22.870
So we're going to work again on this code.

53
00:03:22.890 --> 00:03:30.450
I'm actually going to take this here you know stop words or the English top words I'm going to cut that

54
00:03:30.870 --> 00:03:33.960
then right here in a new line of code.

55
00:03:33.990 --> 00:03:40.740
I'm going to paste that then I'm going to create actually a new variable which I'm going to call all

56
00:03:41.100 --> 00:03:44.050
underscore stop words right.

57
00:03:44.230 --> 00:03:47.250
And which will be equal to exactly this.

58
00:03:47.250 --> 00:03:54.690
But then what I'm gonna do just below is to take this again which was now created as you know this whole

59
00:03:54.690 --> 00:03:56.420
and symbol of all the stop words.

60
00:03:56.430 --> 00:04:02.250
But we don't want to include not in this top word because that's clearly a negative term indicating

61
00:04:02.250 --> 00:04:04.360
therefore a negative review.

62
00:04:04.380 --> 00:04:11.130
So I'm going to paisa here and I'm just going to add here at that remove and in the parentheses I'm

63
00:04:11.130 --> 00:04:15.110
simply going to include in quotes not all right.

64
00:04:15.130 --> 00:04:19.530
That will not include the not word from the Stop words.

65
00:04:19.530 --> 00:04:26.290
And therefore here instead of taking the set of the original and symbol of stop words.

66
00:04:26.490 --> 00:04:33.220
Well we're now going to take the original and symbol of stop words excluding this time the not word.

67
00:04:33.270 --> 00:04:34.420
Let's see if it works.

68
00:04:34.440 --> 00:04:38.640
I'm kind of improvising things here but it might work.

69
00:04:38.640 --> 00:04:44.130
So we actually have to restart the runtime so let's do this restore runtime.

70
00:04:44.160 --> 00:04:46.290
Yes we still have our data set.

71
00:04:46.290 --> 00:04:47.380
Oh good.

72
00:04:47.430 --> 00:04:50.660
And now let's see if this works so we're going to re execute the cells.

73
00:04:50.660 --> 00:04:54.240
I can not do a run out here because the implementation is not over.

74
00:04:54.510 --> 00:04:56.420
But let us import the libraries first.

75
00:04:56.430 --> 00:04:58.060
Now the data set.

76
00:04:58.080 --> 00:05:03.040
And now let's clean text I hope this will work.

77
00:05:03.150 --> 00:05:04.550
Let's play.

78
00:05:04.620 --> 00:05:05.400
All right.

79
00:05:05.430 --> 00:05:06.540
There seems to be good.

80
00:05:06.630 --> 00:05:07.410
Good.

81
00:05:07.410 --> 00:05:11.520
Now let's remove this output this was the previous output.

82
00:05:11.520 --> 00:05:12.000
Right.

83
00:05:12.030 --> 00:05:13.370
And now let's bring the corpus.

84
00:05:13.380 --> 00:05:19.820
Let's hope that the second review is now no longer you know crust good but indeed crust.

85
00:05:19.890 --> 00:05:20.760
Not good.

86
00:05:20.960 --> 00:05:24.650
Okay so let's press play and perfect.

87
00:05:24.660 --> 00:05:25.530
Okay good.

88
00:05:25.620 --> 00:05:32.070
I'm relieved you know this was really bad to remove the nut because it's clearly a negative term indicating

89
00:05:32.220 --> 00:05:33.660
a negative review.

90
00:05:33.660 --> 00:05:33.900
All right.

91
00:05:33.900 --> 00:05:34.760
So much better now.

92
00:05:34.770 --> 00:05:36.850
And actually you know same for the next one.

93
00:05:36.890 --> 00:05:39.270
Not tasty texture nasty.

94
00:05:39.270 --> 00:05:41.460
That definitely means a negative review.

95
00:05:41.460 --> 00:05:44.240
Let's actually check that right.

96
00:05:44.280 --> 00:05:44.830
Yes.

97
00:05:44.880 --> 00:05:46.860
Not tasty and whatever.

98
00:05:46.860 --> 00:05:48.290
Zero negative review.

99
00:05:48.300 --> 00:05:49.740
And same for this one.

100
00:05:49.770 --> 00:05:50.010
All right.

101
00:05:50.040 --> 00:05:50.430
So good.

102
00:05:50.430 --> 00:05:52.530
We have actually a much better model now.

103
00:05:52.560 --> 00:05:57.780
So we can continue and we can mostly create the back of its model.

104
00:05:57.780 --> 00:05:58.050
All right.

105
00:05:58.050 --> 00:05:58.830
So let's do this.

106
00:05:58.830 --> 00:06:02.490
Let's actually scroll down a bit and there we go.

107
00:06:02.490 --> 00:06:09.090
New coat sale and now let's proceed with this tokenization to create this pass matrix containing all

108
00:06:09.090 --> 00:06:13.720
the reviews in different rows and all the words from all the reviews in the different columns where

109
00:06:13.730 --> 00:06:18.630
the sales will get a one if the word is in the review and a zero otherwise.

110
00:06:18.630 --> 00:06:19.080
All right.

111
00:06:19.080 --> 00:06:24.860
So we're gonna do this with actually psychic learn you know the tokenization process will be done.

112
00:06:24.930 --> 00:06:30.720
Thanks to a class from psychic learn more specifically from a module of psychic learning called feature

113
00:06:30.720 --> 00:06:35.120
extraction and that class is called Count vector riser.

114
00:06:35.130 --> 00:06:35.970
All right so let's do this.

115
00:06:35.970 --> 00:06:43.980
Let's start from psychic learn you know this library very well as K learn from which we're going to

116
00:06:43.980 --> 00:06:45.540
call that feature.

117
00:06:45.540 --> 00:06:46.290
There we go.

118
00:06:46.290 --> 00:06:50.070
Extraction module from which actually you know it's not over.

119
00:06:50.100 --> 00:06:58.380
We're gonna get access to the sub module called text text from which we're going to import that count

120
00:06:59.040 --> 00:07:00.180
vector as a class.

121
00:07:00.180 --> 00:07:00.720
Perfect.

122
00:07:00.720 --> 00:07:04.530
I really love Google collab when it assist me this well.

123
00:07:04.540 --> 00:07:08.550
Okay so we have the class now you know what is the next natural step.

124
00:07:08.550 --> 00:07:11.390
It is to create an instance of this class.

125
00:07:11.430 --> 00:07:18.480
And we're gonna call that CV as count vector riser which will be created as you know an instance of

126
00:07:18.480 --> 00:07:21.430
this count vector race of class.

127
00:07:21.540 --> 00:07:29.370
Perfect which has to take as input only one important parameter can you actually guess what it is.

128
00:07:29.370 --> 00:07:33.170
Well it is actually the maximum size of sparse matrix.

129
00:07:33.180 --> 00:07:38.280
You know the maximum number of columns therefore the maximum number of words you want to include in

130
00:07:38.280 --> 00:07:40.500
the columns of the sparse matrix.

131
00:07:40.710 --> 00:07:46.340
And why is this important that because you know in our corpus of reviews now with all the simplifications.

132
00:07:46.470 --> 00:07:52.650
Well we actually have still some words that are not relevant or you know not helpful to predictive for

133
00:07:52.650 --> 00:07:57.330
reviews positive or negative even if they were not part of the stop words and these include for example

134
00:07:57.510 --> 00:08:03.720
you know texture texture it doesn't really help to predict it for reviews positive or negative or you

135
00:08:03.720 --> 00:08:09.720
know bank or holiday or Rick and even Steve you know Steve doesn't help at all.

136
00:08:09.720 --> 00:08:15.000
So we still have these words which even if they're not part of the stop word don't help at all to predict

137
00:08:15.030 --> 00:08:21.120
if a review is positive or negative and a way to get rid of them is by you know entering this parameter

138
00:08:21.210 --> 00:08:28.080
that we're about to enter the way to get rid of them is just to take actually the most frequent words

139
00:08:28.200 --> 00:08:33.700
you know the words that appear most frequently in the reviews because probably hear Steve on your peers

140
00:08:33.700 --> 00:08:34.280
wants.

141
00:08:34.320 --> 00:08:40.380
So if we only take the most frequent words we won't include Steve in this sparse matrix you know in

142
00:08:40.380 --> 00:08:41.760
the tokenization process.

143
00:08:42.180 --> 00:08:44.000
So so that's the trick.

144
00:08:44.010 --> 00:08:48.540
And so now we need to just choose a maximum size of the sparse matrix.

145
00:08:48.540 --> 00:08:53.880
However we can't really know now how many words there are in total you know before we take the most

146
00:08:53.880 --> 00:08:54.540
frequent ones.

147
00:08:54.840 --> 00:09:00.210
So what we'll do in fact is we will leave this for now you know we won't enter this parameter now we

148
00:09:00.210 --> 00:09:06.210
will run this cell once we create the sparse matrix which is actually going to be the matrix of features

149
00:09:06.270 --> 00:09:11.460
when training are named based model on the training set it's going to be the matrix of features and

150
00:09:11.460 --> 00:09:16.740
therefore we will do a print in order to know the total number of columns and we will get therefore

151
00:09:16.740 --> 00:09:22.770
the total number of words and then we can reduce that total number of words to a lower number of the

152
00:09:22.770 --> 00:09:28.370
most frequent words in the sparse matrix so that we can simplify even more the bag of words model.

153
00:09:28.380 --> 00:09:28.880
Okay.

154
00:09:29.040 --> 00:09:29.970
So that's what we'll do.

155
00:09:29.970 --> 00:09:36.420
Therefore so far let's not enter anything let's just continue to create that bag of words model.

156
00:09:36.570 --> 00:09:37.260
All right.

157
00:09:37.260 --> 00:09:42.240
And actually speaking of the matrix of features that's exactly our next step here.

158
00:09:42.300 --> 00:09:43.070
We are ready.

159
00:09:43.110 --> 00:09:49.820
Thanks to discount vector as a class to create the matrix of features which is indeed that sparse matrix.

160
00:09:49.890 --> 00:09:55.350
So we're gonna call it X as usual as every of our previous matrices of features.

161
00:09:55.350 --> 00:09:56.700
So x equals.

162
00:09:56.700 --> 00:09:59.220
And now according to you what is the next step here.

163
00:09:59.370 --> 00:10:05.260
Well you guess that we're going to create this pass matrix thanks to our CV object.

164
00:10:05.320 --> 00:10:11.280
So there we go I'm calling CV first from which I'm gonna call now a method which you know very well

165
00:10:11.400 --> 00:10:19.410
which we already called many times and that method is the fit transform method.

166
00:10:19.410 --> 00:10:25.680
All right fit transform method which will indeed fit well you know the input of this fit transfer method

167
00:10:25.710 --> 00:10:31.600
which will be you know I'll tell you now the corpus it will fit the corpus to X what does it mean.

168
00:10:31.620 --> 00:10:37.920
It means exactly that it will take all the words from all the reviews in the corpus and then using this

169
00:10:37.920 --> 00:10:42.160
transform part of the method it will put all these words in different columns.

170
00:10:42.180 --> 00:10:43.590
So you see that is very simple.

171
00:10:43.590 --> 00:10:48.420
The fifth method will just take all the words and the transform method will put all these words into

172
00:10:48.420 --> 00:10:49.140
the columns.

173
00:10:49.140 --> 00:10:49.890
That's it.

174
00:10:49.890 --> 00:10:50.550
Nothing more.

175
00:10:50.750 --> 00:10:51.290
Okay.

176
00:10:51.330 --> 00:10:58.110
So of course inside this fit transfer method we have to input our corpus of reviews of very clean reviews

177
00:10:58.770 --> 00:11:04.980
and then we just need to add here to array because actually you know remember that the matrix of features

178
00:11:04.980 --> 00:11:06.630
must be to the array.

179
00:11:06.630 --> 00:11:11.940
It has to be a 2D array because then you know we will train the name based model on the training set

180
00:11:12.390 --> 00:11:17.500
and this expect of course an array as the format of its inputs you know the matrix of features.

181
00:11:17.580 --> 00:11:19.980
So you know X will be an array here.

182
00:11:19.980 --> 00:11:24.720
Then it will be split it into the training center set you know with extra chain y trend access and why

183
00:11:24.730 --> 00:11:25.300
test.

184
00:11:25.440 --> 00:11:26.610
And then there we go.

185
00:11:26.610 --> 00:11:31.620
We'll have the right array reformat to train the knife based model on the training set composed of X

186
00:11:31.620 --> 00:11:33.980
train and Y train so to array.

187
00:11:33.990 --> 00:11:38.870
Let's not forget the parenthesis and now there we go we're almost done.

188
00:11:38.910 --> 00:11:43.050
Our final step here is to create the dependent viable vector Y.

189
00:11:43.140 --> 00:11:48.080
And actually I will let you do this now because you know exactly how to do it right.

190
00:11:48.090 --> 00:11:53.790
We simply need to take that second column here because that's exactly the dependent variable vector

191
00:11:53.790 --> 00:11:59.160
and we don't have anything to do here because it's already ready with the binary outcome 0 or 1 and

192
00:11:59.160 --> 00:11:59.550
so.

193
00:11:59.640 --> 00:12:05.640
Well the way to get this is actually very simple and I'm actually thinking right now of an even simpler

194
00:12:05.640 --> 00:12:11.910
way which is to go to our data processing template than take this line of code because you know I'm

195
00:12:11.910 --> 00:12:20.400
very lazy and so I'm copying this and pasting it right you know doing this and right here and that's

196
00:12:20.400 --> 00:12:22.160
exactly all dependent variable.

197
00:12:22.170 --> 00:12:22.680
Right.

198
00:12:22.680 --> 00:12:27.890
It is just taking the last column of our dataset which is the same as the second column.

199
00:12:27.930 --> 00:12:28.190
Right.

200
00:12:28.190 --> 00:12:33.210
You can either put a minus one year or the index one but we want to make this a code template if we

201
00:12:33.210 --> 00:12:33.690
can.

202
00:12:33.690 --> 00:12:36.030
So let's just keep that okay.

203
00:12:36.450 --> 00:12:37.280
Wow so good.

204
00:12:37.290 --> 00:12:39.620
We're done with actually a bag of words model.

205
00:12:39.930 --> 00:12:45.210
So now as we said we're going to run this to figure out the number of columns in the matrix X meaning

206
00:12:45.210 --> 00:12:48.490
the total number of words in that sparse matrix.

207
00:12:48.540 --> 00:12:55.860
So let's play this cell in order to first create X and Y and then we'll do the necessary to indeed get

208
00:12:55.920 --> 00:13:01.380
that total number of columns in X and that's exactly what we're ready to do now you saw that this cell

209
00:13:01.470 --> 00:13:08.820
executed properly and now the trick to get that number of columns and X or you know that number of words

210
00:13:09.060 --> 00:13:16.200
resulting from the tokenization is just to call the len function here which is going to take as input

211
00:13:16.470 --> 00:13:21.660
this matrix of features x and then only the first row.

212
00:13:21.660 --> 00:13:21.930
Right.

213
00:13:21.930 --> 00:13:27.670
Remember that the first index here and the pair of square brackets corresponds to the index of the row.

214
00:13:27.690 --> 00:13:28.010
All right.

215
00:13:28.020 --> 00:13:33.090
So this will give us exactly the number of elements basically in the first row.

216
00:13:33.120 --> 00:13:35.700
Therefore the number of columns of x.

217
00:13:35.790 --> 00:13:36.960
So let's see.

218
00:13:36.960 --> 00:13:40.230
Let's press play and we're going to get now that indeed.

219
00:13:40.230 --> 00:13:41.480
Well OK.

220
00:13:41.580 --> 00:13:47.540
There are 1000 566 words resulting from the tokenization.

221
00:13:47.580 --> 00:13:54.030
Basically we have 1000 566 words that were taken from all the reviews and for each of the reviews we

222
00:13:54.030 --> 00:13:59.910
have either one in the columns corresponding to the words that are in the review and zero to all the

223
00:13:59.910 --> 00:14:03.870
other columns corresponding to the words that are not in the review.

224
00:14:03.870 --> 00:14:04.070
All right.

225
00:14:04.080 --> 00:14:09.660
So basically we have one thousand five hundred and sixty six words and we can simplify this even more

226
00:14:09.660 --> 00:14:16.740
by for example taking the 1500 most frequent words so that we can you know get rid of words such as

227
00:14:16.830 --> 00:14:26.200
Rick Steve and maybe you know holiday or or let's say you know for I don't know what that means rubber.

228
00:14:26.340 --> 00:14:33.600
You know this probably appears only once and you know words like that words that don't help at all predict

229
00:14:33.690 --> 00:14:36.070
if the review is positive or negative.

230
00:14:36.090 --> 00:14:36.560
OK.

231
00:14:36.690 --> 00:14:38.040
So that's the idea.

232
00:14:38.250 --> 00:14:44.550
So let's just take you know the one thousand five hundred most frequent words and therefore to do this

233
00:14:44.550 --> 00:14:49.920
we just need to enter Max on this core features parameters.

234
00:14:49.920 --> 00:14:50.940
There we go.

235
00:14:50.940 --> 00:14:56.190
And in order to get only the 1500 most frequent words we just need to enter here.

236
00:14:56.220 --> 00:15:02.360
One thousand five hundred and feel free to try with other values like for example the 1000 most frequent

237
00:15:02.360 --> 00:15:02.650
words.

238
00:15:02.660 --> 00:15:05.410
But be careful not to remove too many words.

239
00:15:05.420 --> 00:15:06.280
Okay.

240
00:15:06.440 --> 00:15:07.100
All right.

241
00:15:07.100 --> 00:15:08.170
So good.

242
00:15:08.270 --> 00:15:11.920
Therefore now we're gonna you know rerun that sale.

243
00:15:12.230 --> 00:15:13.180
So let's do this.

244
00:15:13.280 --> 00:15:15.520
Let's press play.

245
00:15:15.560 --> 00:15:15.880
Okay.

246
00:15:15.890 --> 00:15:16.220
Good.

247
00:15:16.280 --> 00:15:19.940
And now if we rerun that sale we should get 1500 here.

248
00:15:19.940 --> 00:15:21.140
Perfect.

249
00:15:21.140 --> 00:15:25.000
So now we have a nice bag of words model with only relevant words.

250
00:15:25.010 --> 00:15:27.700
You know that appear at least a certain amount of times.

251
00:15:27.710 --> 00:15:34.730
And without all the non relevant words that appear once like Rick Steve or that weird four word we saw

252
00:15:34.790 --> 00:15:35.900
in one of the reviews.

253
00:15:35.900 --> 00:15:37.640
Okay good.

254
00:15:37.640 --> 00:15:38.330
And so now.

255
00:15:38.390 --> 00:15:41.220
Well we basically did the most difficult part.

256
00:15:41.270 --> 00:15:43.280
We created the bag of words model.

257
00:15:43.370 --> 00:15:48.530
And so now I actually have an exercise for you which you're going to do by yourself first before we

258
00:15:48.530 --> 00:15:49.540
do it together.

259
00:15:49.550 --> 00:15:53.300
It is of course to do all the rest of the different steps here.

260
00:15:53.300 --> 00:15:59.140
And you know how to do them because you basically have everything you have a matrix of features an independent

261
00:15:59.150 --> 00:16:05.630
viable vector Y which you can therefore split into a training set and to set you know composed respectively

262
00:16:05.660 --> 00:16:12.660
of X train in y train and X and Y test then you're going to use the training set composed of extraneous

263
00:16:12.750 --> 00:16:18.680
y train to train the Navy base model on the train set then you're going to predict the test results

264
00:16:18.770 --> 00:16:25.050
using the test set containing therefore reviews and their outcomes on which the model wasn't trained.

265
00:16:25.070 --> 00:16:30.800
And finally you're gonna make the confusion matrix and compute the accuracy of course you're gonna do

266
00:16:30.800 --> 00:16:36.310
this using your machine learning tool kit containing all the code templates we've built so far.

267
00:16:36.410 --> 00:16:40.850
So you totally have the right to do that and I actually hope that you're going to do this because I

268
00:16:40.850 --> 00:16:47.750
want you to be as most efficient as possible and therefore that's exactly what we will do in the next

269
00:16:47.780 --> 00:16:50.140
and final tutorial of this section.

270
00:16:50.270 --> 00:16:56.690
I will show you how to juggle with our diverse toolkit and especially the classification tool kit to

271
00:16:56.780 --> 00:17:02.180
in a flashlight split the data set into the training set and test it then trained the native based model

272
00:17:02.180 --> 00:17:06.580
on the train set than predictive test results and making the confusion matrix.

273
00:17:06.620 --> 00:17:12.650
I will show you that I will do this with only copy paste nothing else we won't type any code.

274
00:17:12.650 --> 00:17:15.530
Now we have everything in our diverse toolkit.

275
00:17:15.650 --> 00:17:18.610
But please make it first please do it on your own first.

276
00:17:18.680 --> 00:17:22.300
And we will implement the solution together in the next tutorial.

277
00:17:22.310 --> 00:17:24.200
Until then enjoy machine learning.
