WEBVTT
1
00:00:00.090 --> 00:00:06.180
Hello my friends and welcome to the final section of part 3 classification where we're going to answer

2
00:00:06.180 --> 00:00:08.620
together a very important question.

3
00:00:08.630 --> 00:00:14.970
You know one of the most frequently asked question in the data science community which is which classification

4
00:00:14.970 --> 00:00:18.890
model should I select you know should I choose for my data set.

5
00:00:19.020 --> 00:00:25.230
And the goal of this tutorial because we'll try to make it and want to trial is to show you how with

6
00:00:25.380 --> 00:00:29.670
any dataset you know regardless of the number of features you have any data set.

7
00:00:29.740 --> 00:00:35.620
Well I will show you how to select quickly and efficiently the best classification model.

8
00:00:35.620 --> 00:00:35.930
All right.

9
00:00:35.940 --> 00:00:41.520
So that's why here we are back into our mission earning it is that moral selection for the arena which

10
00:00:41.520 --> 00:00:46.730
is a separate folder compared to the whole machinery is at fault with all the codes and data set.

11
00:00:46.860 --> 00:00:51.740
And I gave you the link to this folder right before this tutorial in the article.

12
00:00:51.780 --> 00:00:58.590
So make sure not to miss it and mostly make sure to click that link and also download that folder by

13
00:00:58.590 --> 00:01:02.550
clicking the zip folder given at the bottom of the same oracle.

14
00:01:02.700 --> 00:01:03.420
All right.

15
00:01:03.420 --> 00:01:09.900
And so now after downloading the zip folder and clicking the link to this Google Drive folder we should

16
00:01:09.900 --> 00:01:16.600
all be on the same page to figure out how we're going to select the best classification model.

17
00:01:16.620 --> 00:01:16.880
All right.

18
00:01:16.890 --> 00:01:20.310
So here we are in the classification folder of our moral selection.

19
00:01:20.310 --> 00:01:21.250
Big folder.

20
00:01:21.360 --> 00:01:26.520
And as you can recognize in this form that we have all the classification models that we implemented

21
00:01:26.520 --> 00:01:29.980
together all along this part 3 you have all of them.

22
00:01:30.000 --> 00:01:35.820
However I slightly modified them but the only thing I did you know with respect to what we did before

23
00:01:36.180 --> 00:01:42.450
is that I removed all the prints you know to alleviate or lighten the implementation so that we can

24
00:01:42.450 --> 00:01:43.590
see more clearly.

25
00:01:43.590 --> 00:01:49.500
And also of course you know at the end I removed the two cells where we visualize the train set and

26
00:01:49.500 --> 00:01:50.410
test results.

27
00:01:50.430 --> 00:01:50.660
Right.

28
00:01:50.660 --> 00:01:55.020
Because remember this visualizations only work when you have two features.

29
00:01:55.080 --> 00:02:01.200
And here as you can see I took a classic dataset with many features you can see all of them here.

30
00:02:01.200 --> 00:02:02.520
So these are all the features.

31
00:02:02.580 --> 00:02:08.790
And this is the deep and variable but you can see this data set as a generic dataset containing many

32
00:02:08.870 --> 00:02:11.180
features always numerical values right.

33
00:02:11.190 --> 00:02:18.210
We won't do any kind of specific data repressing and indeed a binary dependent variable taking values

34
00:02:18.210 --> 00:02:19.440
2 or 4.

35
00:02:19.440 --> 00:02:19.740
All right.

36
00:02:19.740 --> 00:02:24.260
So since we have the data set in front of us well let me explain what this is about.

37
00:02:24.270 --> 00:02:29.070
Even if you know it doesn't really matter because the goal of this tutorial is just to explain how to

38
00:02:29.070 --> 00:02:30.470
deploy it efficiently.

39
00:02:30.480 --> 00:02:35.430
All your classification models and quickly figure out what is the best one on any dataset regardless

40
00:02:35.430 --> 00:02:36.960
of the number of features.

41
00:02:37.110 --> 00:02:39.050
But let me still explain what this is about.

42
00:02:39.050 --> 00:02:45.870
So this is a classic dataset which belongs to the UCI machinery repository and which is about breast

43
00:02:45.960 --> 00:02:46.910
cancer.

44
00:02:46.920 --> 00:02:53.730
So in this dataset each row corresponds to a patient you know different patients here and for each of

45
00:02:53.730 --> 00:02:55.620
these patients we gathered.

46
00:02:55.620 --> 00:02:57.390
Well first a sample code number.

47
00:02:57.570 --> 00:03:06.480
The clump thickness the uniformity of cell size the uniformity of cell shape the marginal addition the

48
00:03:06.480 --> 00:03:14.350
single epithelial cell the Bernoulli the bland cremating the normal nuclear free and the Meters us.

49
00:03:14.390 --> 00:03:15.060
OK.

50
00:03:15.180 --> 00:03:19.800
And all these variables are the feature as you know from sample code number even if that's not really

51
00:03:19.800 --> 00:03:27.210
a feature up to mitosis and with all these features we are predicting the class which deals for each

52
00:03:27.210 --> 00:03:34.770
patient if the tumour is benign in which case class takes the value of two or malignant in which case

53
00:03:34.770 --> 00:03:36.770
class takes a value of four.

54
00:03:36.780 --> 00:03:36.990
All right.

55
00:03:36.990 --> 00:03:38.760
So that's what the data set is about.

56
00:03:38.760 --> 00:03:44.280
You can find this on the UCI email repertory by the name breast cancer and you can take the original

57
00:03:44.280 --> 00:03:49.950
version but really don't worry about all these features because you know most of us don't understand

58
00:03:49.950 --> 00:03:56.280
what they mean you know we're not doctors here but we are data scientists and even if we don't understand

59
00:03:56.280 --> 00:04:01.830
the domain knowledge here of oncology you know cancer medicine well that's still fine because we can

60
00:04:01.830 --> 00:04:08.400
still build classification models to understand the correlations between all these features here and

61
00:04:08.610 --> 00:04:14.370
the dependent variable class which want to predict telling if the tumor of each of these patients is

62
00:04:14.370 --> 00:04:16.020
benign or malignant.

63
00:04:16.020 --> 00:04:16.770
All right.

64
00:04:16.770 --> 00:04:21.790
And so we're going to use this data set to deploy all our classification models in a flashlight you

65
00:04:21.790 --> 00:04:23.310
know in a matter of seconds.

66
00:04:23.460 --> 00:04:29.370
And after just a few clicks we will be able to figure out what is the best classification model for

67
00:04:29.490 --> 00:04:30.510
this dataset.

68
00:04:30.510 --> 00:04:31.690
All right great.

69
00:04:31.690 --> 00:04:32.550
So let's do this.

70
00:04:32.550 --> 00:04:33.750
Let's close this.

71
00:04:33.750 --> 00:04:39.870
And now now what we're going to do in order to start a demo is because you know this is a Google Drive

72
00:04:39.870 --> 00:04:44.810
folder to which all of you have access and therefore you can't modify it obviously.

73
00:04:44.820 --> 00:04:49.770
And so what you have to do in order to modify these cells you know because we will have to enter the

74
00:04:49.830 --> 00:04:54.450
name of the dataset because these are all code templates in order to modify these cells you need to

75
00:04:54.510 --> 00:04:55.300
create a copy.

76
00:04:55.310 --> 00:04:57.380
So that's the first thing we'll do here.

77
00:04:57.390 --> 00:04:58.500
Let's do this quickly.

78
00:04:58.500 --> 00:05:04.050
You know you just need to right click and then make a copy for each of them.

79
00:05:04.050 --> 00:05:09.520
All right then Colonel SVM make a copy logistic regression.

80
00:05:09.580 --> 00:05:10.470
So this is pretty far.

81
00:05:10.470 --> 00:05:17.440
Sorry about that but at least it only takes a few seconds and then you get all your copies in case you

82
00:05:17.440 --> 00:05:18.570
know you want to modify them.

83
00:05:18.580 --> 00:05:25.660
But I recommend to all right then your copies would go naturally to your main drive or you know in the

84
00:05:26.020 --> 00:05:29.690
Q lab notebooks folder here they just went into my drive.

85
00:05:29.690 --> 00:05:30.710
So all good.

86
00:05:30.760 --> 00:05:35.410
Now we're going to open them also starting with the last one random forest.

87
00:05:35.410 --> 00:05:41.860
All right then we're going to open decision tree classification open.

88
00:05:42.120 --> 00:05:45.130
All right you can open it with Jupiter Nobuko so if you want.

89
00:05:45.520 --> 00:05:48.990
Right then we're going to open knife bays.

90
00:05:49.180 --> 00:05:55.450
All right then we're going to open kernel as VM perfect.

91
00:05:55.570 --> 00:05:58.020
Then we're going to open as V.

92
00:05:58.120 --> 00:05:59.040
And where is it.

93
00:05:59.080 --> 00:06:07.430
Right here support vector machine open then we're gonna open the Kenya es neighbors.

94
00:06:07.530 --> 00:06:08.430
All right.

95
00:06:08.520 --> 00:06:12.300
And finally we're going to open logistic regression.

96
00:06:12.330 --> 00:06:15.830
I went from the last to the first because as you can see this is the way.

97
00:06:15.870 --> 00:06:19.080
Now we have all the files in the correct order.

98
00:06:19.200 --> 00:06:19.910
All right great.

99
00:06:19.980 --> 00:06:21.210
So now they're all copies.

100
00:06:21.210 --> 00:06:22.650
Therefore we can modify them.

101
00:06:23.010 --> 00:06:28.980
Let me just show you once again how I modified the original classification codes we made before to these

102
00:06:28.980 --> 00:06:35.190
new ones in a more simplified ones which will get you the accuracy quickly and efficiently so the data

103
00:06:35.310 --> 00:06:39.990
processing phase I kept exactly the same including you know the feature scaling applied to extra in

104
00:06:39.990 --> 00:06:41.010
the next test.

105
00:06:41.010 --> 00:06:48.890
But here I put a template name and I highlighted that you have to enter the name of your data set here.

106
00:06:48.900 --> 00:06:49.680
So that's what we'll do.

107
00:06:49.680 --> 00:06:50.520
I'll show you.

108
00:06:50.520 --> 00:06:52.590
But that's the only thing that was changed.

109
00:06:52.590 --> 00:06:57.690
Indeed we don't have to do much else because this will automatically select all the features and not

110
00:06:57.690 --> 00:06:58.810
your dependent variable.

111
00:06:58.920 --> 00:07:01.910
And this will automatically select your dependent variable.

112
00:07:01.920 --> 00:07:05.410
And that provided of course you have in your dataset first.

113
00:07:05.480 --> 00:07:08.410
The feature is you know in the first columns and last.

114
00:07:08.430 --> 00:07:10.480
The dependent variable in the last column.

115
00:07:10.500 --> 00:07:10.960
Right.

116
00:07:10.980 --> 00:07:12.160
Make sure of this.

117
00:07:12.210 --> 00:07:16.840
What we're gonna do now works for any data sets regardless of the number of features.

118
00:07:16.920 --> 00:07:21.790
As long as they have the features in the first columns and that have been viable in the last column.

119
00:07:21.810 --> 00:07:23.450
Make sure to remember this.

120
00:07:24.080 --> 00:07:24.440
All right.

121
00:07:24.450 --> 00:07:28.710
And then all good here if you want you can change that from open 25 to open too.

122
00:07:28.710 --> 00:07:29.280
That's fine.

123
00:07:29.280 --> 00:07:33.170
Both values work well and feature scaling.

124
00:07:33.180 --> 00:07:39.030
All right then for each of the classification model I kept you know the code to implement and train

125
00:07:39.030 --> 00:07:39.530
it.

126
00:07:39.570 --> 00:07:45.930
And finally what I did in the last sales is simply I removed you know the prints that displayed the

127
00:07:46.020 --> 00:07:50.220
vector of predictions and the vector of real results next to each other because you know we don't really

128
00:07:50.220 --> 00:07:52.240
need it for our model selection process.

129
00:07:52.260 --> 00:07:58.320
However what I did is that I kept this of course but in order to compute the commission matrix and the

130
00:07:58.320 --> 00:08:04.590
accuracy I had to create that y print vector containing all the predictions like calling to predict

131
00:08:04.590 --> 00:08:09.000
method apply to X test from our classifier and that's all I did.

132
00:08:09.000 --> 00:08:11.880
And I did the same in all the different files right.

133
00:08:11.930 --> 00:08:16.710
Kenya is neighbors data repricing phase training and confusion matrix.

134
00:08:16.710 --> 00:08:23.760
Same support vector machine data repressing training and confusion matrix then kernel as VM same data

135
00:08:23.830 --> 00:08:30.870
repricing phase training and confusion matrix namespace data repricing phase training and confusion

136
00:08:30.870 --> 00:08:38.010
matrix and decision tree classification data repricing phase training confusion matrix and finally random

137
00:08:38.010 --> 00:08:42.140
forest data repricing phase training and confusion matrix C.

138
00:08:42.240 --> 00:08:47.460
So you have the exact same code templates for each of the classification models we've built together.

139
00:08:47.490 --> 00:08:53.220
The only thing that changed is actually this cell because this cell actually builds and train the classification

140
00:08:53.220 --> 00:08:56.410
Well you want to try through this model selection process.

141
00:08:56.460 --> 00:08:57.050
All right.

142
00:08:57.150 --> 00:08:57.540
Perfect.

143
00:08:57.570 --> 00:09:00.660
So now we're getting very close to the demo.

144
00:09:00.870 --> 00:09:06.420
And so just to recap this demo works for any data set regardless of the number of features.

145
00:09:06.450 --> 00:09:11.100
And as long as you have your features in the first columns and your dependent variable in the last column

146
00:09:11.430 --> 00:09:16.580
and also as long as you don't have some special data repricing tools to use on your dataset.

147
00:09:16.710 --> 00:09:21.540
If you have any category call variables you know in strings or categorical variables where you have

148
00:09:21.540 --> 00:09:23.220
to perform one HUD encoding.

149
00:09:23.250 --> 00:09:29.340
Well don't forget to use your DRP pricing tool kit to process the right way your data set and then you

150
00:09:29.340 --> 00:09:35.280
can just deploy all your classification code templates here and that my friend is exactly what I'm about

151
00:09:35.280 --> 00:09:36.570
to show you right now.

152
00:09:36.570 --> 00:09:39.720
So now the demo is gonna start I ready.

153
00:09:39.720 --> 00:09:42.540
Three two one go.

154
00:09:42.540 --> 00:09:47.250
All right I'm gonna do this as efficiently as I can in order to show the power of code templates.

155
00:09:47.640 --> 00:09:52.470
So first step the first step is to upload the data set inside the notebook.

156
00:09:52.470 --> 00:09:55.360
Right now it is connecting to a runtime to enable file browsing.

157
00:09:55.380 --> 00:10:00.820
Actually I'm gonna do this for each of the models here because you know it always takes a few seconds.

158
00:10:00.960 --> 00:10:03.120
So let's do it this way to be efficient.

159
00:10:03.120 --> 00:10:07.930
All right so I'm just you know loading all the files here.

160
00:10:07.980 --> 00:10:08.400
All right.

161
00:10:08.400 --> 00:10:09.370
Perfect.

162
00:10:09.390 --> 00:10:14.040
And everything you know every file is now connecting to a runtime.

163
00:10:14.040 --> 00:10:17.590
Now be careful if you don't see the sample data here you have to refresh.

164
00:10:17.670 --> 00:10:20.470
Otherwise you will have issues uploading your dataset.

165
00:10:20.490 --> 00:10:20.850
Good.

166
00:10:20.850 --> 00:10:21.830
Now it's good.

167
00:10:21.840 --> 00:10:24.390
So the next step we upload the data set.

168
00:10:24.390 --> 00:10:24.600
All right.

169
00:10:24.600 --> 00:10:28.290
So this is the model selection further and more precisely the classification folder.

170
00:10:28.500 --> 00:10:30.210
But let me show you the path again.

171
00:10:30.330 --> 00:10:34.180
I put this machine learning moral selection folder into my desktop.

172
00:10:34.200 --> 00:10:36.980
But make sure to find it on your machine wherever it is.

173
00:10:37.080 --> 00:10:42.840
If you have not downloaded that already make sure to download it right before this tutorial in the article.

174
00:10:42.840 --> 00:10:48.130
You will find a link at the bottom of the Oracle then together we're going to go inside then inside

175
00:10:48.150 --> 00:10:50.010
classification and there we go.

176
00:10:50.010 --> 00:10:51.750
We select data that's easy.

177
00:10:52.260 --> 00:10:54.590
Then we click open and we press okay.

178
00:10:54.690 --> 00:11:01.200
And then what we simply need to do inside is could template is just to put here the name of the dataset.

179
00:11:01.200 --> 00:11:08.460
So you just double click this then enter data that says we all know the name of your future dataset.

180
00:11:08.600 --> 00:11:08.930
All right.

181
00:11:08.990 --> 00:11:09.720
And that's it.

182
00:11:09.740 --> 00:11:12.830
That's all we have to do in each code templates.

183
00:11:12.860 --> 00:11:17.110
Only one thing to change so that we can really call it a curve template.

184
00:11:17.210 --> 00:11:17.570
Great.

185
00:11:17.570 --> 00:11:20.640
Now we're going to do the same in each other implementation.

186
00:11:20.990 --> 00:11:24.410
So now K nearest neighbours let's refresh this because we need to see this.

187
00:11:24.410 --> 00:11:25.200
There we go.

188
00:11:25.340 --> 00:11:29.590
Then upload then data that's easy then open.

189
00:11:29.650 --> 00:11:29.970
All right.

190
00:11:29.970 --> 00:11:30.580
Perfect.

191
00:11:30.590 --> 00:11:31.410
Okay.

192
00:11:31.430 --> 00:11:34.560
And then we replace him the name by data.

193
00:11:34.680 --> 00:11:36.510
That's just perfect.

194
00:11:36.530 --> 00:11:45.390
The next one support vector machine refresh upload then data that that's we then open and perfect.

195
00:11:45.500 --> 00:11:46.590
We have the data set.

196
00:11:46.610 --> 00:11:51.470
Now we replace this by data that CSC and all good SVM is ready.

197
00:11:51.590 --> 00:11:59.680
Now Colonel as VM refresh upload data that says we open OK.

198
00:12:00.490 --> 00:12:04.000
Replacing this by data that's easy or the name of your future dataset.

199
00:12:04.000 --> 00:12:04.720
And there we go.

200
00:12:04.730 --> 00:12:06.240
Colonel SVM is ready.

201
00:12:06.250 --> 00:12:15.200
All right then Navy Base refresh upload data that says we open OK.

202
00:12:15.260 --> 00:12:18.470
And then replacing this by data that's easy.

203
00:12:18.470 --> 00:12:21.620
I hope you appreciate how this is a very efficient process.

204
00:12:21.770 --> 00:12:22.760
Knife base is ready.

205
00:12:22.790 --> 00:12:24.620
So now let's move on to decision tree classification.

206
00:12:24.630 --> 00:12:25.740
And there you go.

207
00:12:25.750 --> 00:12:29.420
So that happens when you have too many sessions opened on Google collab.

208
00:12:29.420 --> 00:12:34.370
I left it on purpose because I'm sure you will also encounter this situation and what to do in this

209
00:12:34.370 --> 00:12:36.050
situation well no worries at all.

210
00:12:36.050 --> 00:12:37.140
That's very simple.

211
00:12:37.250 --> 00:12:41.980
Since Google killer app actually allows only maximum five sessions to run at the same time.

212
00:12:41.990 --> 00:12:46.940
Well what we'll just do here is that for decision for gratification in a random forest because here

213
00:12:46.940 --> 00:12:47.900
that's the same.

214
00:12:47.900 --> 00:12:49.760
Well we'll just close them for now.

215
00:12:50.030 --> 00:12:50.750
All right.

216
00:12:50.750 --> 00:12:55.880
And we'll reopen them after we get the best accuracy from these five models.

217
00:12:55.910 --> 00:12:56.370
OK.

218
00:12:56.480 --> 00:12:57.640
We'll get the best from these five.

219
00:12:57.650 --> 00:12:59.650
Then we'll run the last two ones.

220
00:12:59.660 --> 00:13:02.290
This isn't reclassification and run for its classification.

221
00:13:02.450 --> 00:13:05.210
And we'll see which one wins which one is the big winner.

222
00:13:05.210 --> 00:13:05.570
All right.

223
00:13:06.020 --> 00:13:09.410
So now all are implementations already.

224
00:13:09.530 --> 00:13:16.430
So of course the next natural step now is to run all these sales to get all the accuracies of these

225
00:13:16.430 --> 00:13:17.300
five miles.

226
00:13:17.300 --> 00:13:17.590
All right.

227
00:13:17.600 --> 00:13:19.980
So let's start with logistic regression.

228
00:13:20.000 --> 00:13:20.710
Are you ready.

229
00:13:20.720 --> 00:13:23.570
Let's click run time and then run.

230
00:13:23.640 --> 00:13:27.370
Oh and all the sales will be running and we shouldn't get any error.

231
00:13:27.410 --> 00:13:28.100
Indeed.

232
00:13:28.100 --> 00:13:28.750
And we get.

233
00:13:28.780 --> 00:13:29.550
Wow.

234
00:13:29.630 --> 00:13:36.910
We start with a very good accuracy because we get an accuracy close to 95 percent for logistic regression.

235
00:13:36.920 --> 00:13:37.200
All right.

236
00:13:37.210 --> 00:13:42.440
And indeed we only have four plus five of course nine errors nine incorrect predictions.

237
00:13:42.440 --> 00:13:43.190
Well pretty good.

238
00:13:43.190 --> 00:13:47.930
So let's see what we get with the next ones you know and it's really reassuring that we get these high

239
00:13:47.930 --> 00:13:50.750
accuracy because we are doing predictions for breast cancer.

240
00:13:50.780 --> 00:13:58.100
So we really want to you know be accurate on predicting if patients have a benign or malignant tumor.

241
00:13:58.300 --> 00:13:58.780
OK.

242
00:13:58.850 --> 00:14:01.670
So let's hope that we can do even better than this.

243
00:14:01.670 --> 00:14:01.900
All right.

244
00:14:01.910 --> 00:14:07.230
I'm going to scroll back up or no actually I'm gonna leave that here in case we forget to appoint 94

245
00:14:07.280 --> 00:14:08.020
7.

246
00:14:08.030 --> 00:14:10.440
Now let's move on to K nearest neighbors.

247
00:14:10.520 --> 00:14:12.890
Let's click runtime here then run all.

248
00:14:13.080 --> 00:14:13.670
And there we go.

249
00:14:13.670 --> 00:14:19.670
My friends we're about to get the next accuracy which is exactly the same one I just checked that you

250
00:14:19.670 --> 00:14:21.520
know I put the right model here.

251
00:14:21.530 --> 00:14:27.110
But we have exactly the same one actually which you know can totally happen because you just have to

252
00:14:27.110 --> 00:14:29.300
make nine incorrect predictions.

253
00:14:29.300 --> 00:14:33.440
You know two classification models can make the same number of incorrect predictions and therefore you

254
00:14:33.440 --> 00:14:36.150
will end up with the exact same accuracy.

255
00:14:36.170 --> 00:14:37.400
So that's very interesting actually.

256
00:14:37.400 --> 00:14:39.590
This is the first time I observed this.

257
00:14:39.590 --> 00:14:39.890
All right.

258
00:14:39.890 --> 00:14:44.410
So well let's still hope we can beat this with our next classification models.

259
00:14:44.420 --> 00:14:50.030
So now with support vector machine we're going to click runtime and we're going to click Run all to

260
00:14:50.030 --> 00:14:52.880
see that next accuracy we're getting and.

261
00:14:53.150 --> 00:14:59.270
All right interesting this time we get a lower accuracy but still a very very good one and you know

262
00:14:59.270 --> 00:15:04.940
that makes me very excited to see what Colonel SVM is going to do you know with a nonlinear kernel because

263
00:15:04.940 --> 00:15:10.760
indeed here we get 10 incorrect predictions as opposed to nine incorrect positions before with logistic

264
00:15:10.760 --> 00:15:12.570
regression and Kenya's neighbours.

265
00:15:12.650 --> 00:15:15.280
But here with SVM it's still very very good.

266
00:15:15.290 --> 00:15:18.090
We get 94 percent accuracy.

267
00:15:18.260 --> 00:15:20.320
All right and now let's try Colonel SVM.

268
00:15:20.330 --> 00:15:22.400
I look forward to seeing what we're going to get.

269
00:15:22.400 --> 00:15:31.640
So let's click runtime and let's click Run all and the accuracy is yes we beat it 95 percent ninety

270
00:15:31.640 --> 00:15:33.130
five point three percent.

271
00:15:33.140 --> 00:15:35.540
That's excellent and that was actually expected.

272
00:15:35.540 --> 00:15:38.070
Colonel SVM you know is really really good.

273
00:15:38.090 --> 00:15:43.010
You will get good results with this because you know we get flexibility on the curve to catch the correct

274
00:15:43.010 --> 00:15:44.090
predictions.

275
00:15:44.090 --> 00:15:45.070
All right so very very good.

276
00:15:45.080 --> 00:15:48.170
But we still have three other classification models.

277
00:15:48.170 --> 00:15:52.120
Let's see what we're going to get with them starting with native base.

278
00:15:52.130 --> 00:15:52.510
All right.

279
00:15:52.520 --> 00:15:58.800
So let's click runtime and then run out and the next accuracy is OK.

280
00:15:58.820 --> 00:16:04.890
So like SVM 10 incorrect predictions resulting in an accuracy of 94 percent.

281
00:16:05.120 --> 00:16:05.830
All right.

282
00:16:05.900 --> 00:16:06.530
That's OK.

283
00:16:06.890 --> 00:16:09.570
And now well we still have two more chances.

284
00:16:09.710 --> 00:16:13.910
One with decision tree classification and the other one with random forest classification.

285
00:16:13.910 --> 00:16:21.260
So now what we're gonna do is we're gonna click runtime here then manage sessions then we're gonna terminate

286
00:16:21.380 --> 00:16:27.980
all these sessions here because you know we're only allowed to run maximum five sessions at the same

287
00:16:27.980 --> 00:16:28.730
time.

288
00:16:28.820 --> 00:16:30.680
So I terminated all of them.

289
00:16:30.680 --> 00:16:34.520
You can close it now and we still keep the accuracy so that's totally fine right.

290
00:16:34.520 --> 00:16:39.410
We keep the accuracy everywhere here so we can truly compare with our last two.

291
00:16:39.410 --> 00:16:40.310
So let's do this.

292
00:16:40.350 --> 00:16:47.420
Let's open first you know random forest classification because you know it gives them in that order.

293
00:16:47.450 --> 00:16:48.940
Well actually that doesn't really matter here.

294
00:16:48.950 --> 00:16:53.180
But anyway let's open decision tree classification now.

295
00:16:53.210 --> 00:16:53.950
All right.

296
00:16:54.050 --> 00:16:55.850
And here we go.

297
00:16:55.850 --> 00:16:57.410
We have our last two models.

298
00:16:57.410 --> 00:17:01.910
I can't wait to try them because I can't wait to see who is going to be the big winner and if we can

299
00:17:01.910 --> 00:17:05.960
beat even more that best accuracy of ninety five point three percent.

300
00:17:06.440 --> 00:17:06.770
All right.

301
00:17:06.770 --> 00:17:12.470
So next step is not to click runtime here because remember we haven't uploaded yet the data set into

302
00:17:12.470 --> 00:17:14.790
the notebook so no refresh here.

303
00:17:14.820 --> 00:17:19.370
All good upload then data that's easy open.

304
00:17:19.470 --> 00:17:22.490
Then let's do quickly the same for random forest classification.

305
00:17:22.500 --> 00:17:27.450
But first let's not forget to replace this by data that says we are good.

306
00:17:27.450 --> 00:17:33.180
Now random forest classification little folder here connecting to runtime to enable file browsing.

307
00:17:33.190 --> 00:17:36.420
Okay so we'll just have to wait a few seconds here.

308
00:17:36.420 --> 00:17:37.110
There we go.

309
00:17:37.110 --> 00:17:42.710
No need to reflect then upload then data it's easy open then.

310
00:17:42.750 --> 00:17:43.690
Okay.

311
00:17:43.740 --> 00:17:47.640
And then let's replace this by data that CSC.

312
00:17:47.640 --> 00:17:48.120
All right.

313
00:17:48.120 --> 00:17:51.420
Now our friends we're about to reveal the final podium.

314
00:17:51.420 --> 00:17:55.310
You know the three best models with the three highest accuracy is.

315
00:17:55.350 --> 00:17:56.200
So let's do this.

316
00:17:56.220 --> 00:17:58.890
Starting with decision tree classification.

317
00:17:58.890 --> 00:18:00.600
So let's click runtime here.

318
00:18:00.600 --> 00:18:00.920
Run.

319
00:18:00.950 --> 00:18:01.590
All.

320
00:18:01.650 --> 00:18:03.600
And now there we go.

321
00:18:03.630 --> 00:18:04.810
Wow.

322
00:18:04.920 --> 00:18:08.490
That's incredible we actually beat the accuracy.

323
00:18:08.520 --> 00:18:13.080
I didn't I really didn't expect this usually decision tree classification is not the winner.

324
00:18:13.140 --> 00:18:15.620
But here we have a beautiful exception to the rule.

325
00:18:15.630 --> 00:18:21.530
Indeed we get a beautiful accuracy of almost 96 percent ninety five point nine.

326
00:18:21.540 --> 00:18:23.640
It beats by very little.

327
00:18:23.640 --> 00:18:30.000
You know the accuracy of the kernel SVM the kernel as GM did eight incorrect predictions five plus three

328
00:18:30.330 --> 00:18:36.840
and the decision tree classification did four plus three seven incorrect predictions resulting in having

329
00:18:36.840 --> 00:18:39.660
an accuracy almost equal to 96 percent.

330
00:18:40.050 --> 00:18:41.160
Well that's really really good.

331
00:18:41.160 --> 00:18:45.990
You know this is the first time actually that I'm doing this you know trying this breast cancer dataset

332
00:18:46.020 --> 00:18:47.640
with you with all these models.

333
00:18:47.640 --> 00:18:52.650
I tried it once was a logistic regression in another course but this is the first time that I implement

334
00:18:52.710 --> 00:18:56.820
all these models and deploy them to do a model selection process on this dataset.

335
00:18:57.090 --> 00:19:00.290
So you know that shows you how I'm confident about these code templates.

336
00:19:00.300 --> 00:19:01.440
I never tried them before.

337
00:19:01.440 --> 00:19:06.420
On this day you said and I'm making this demo with you for the first time on this dataset.

338
00:19:06.420 --> 00:19:06.730
All right.

339
00:19:06.750 --> 00:19:12.420
Therefore the excitement and now final one I'm very curious to see if I can still beat it with a random

340
00:19:12.420 --> 00:19:18.580
forest classification so let us do this run all and final accuracy is war.

341
00:19:18.810 --> 00:19:19.410
Oh all right.

342
00:19:19.440 --> 00:19:19.770
Okay.

343
00:19:19.770 --> 00:19:24.820
So very disappointing the random forest classification indeed really messed up with the teamwork.

344
00:19:24.870 --> 00:19:29.310
And that's another exception to the rule because you know usually teamwork is better than individual

345
00:19:29.310 --> 00:19:29.660
work.

346
00:19:29.820 --> 00:19:35.820
But no we had a very powerful decision tree classification well before and it didn't need anybody else

347
00:19:35.880 --> 00:19:36.960
to be performance.

348
00:19:36.960 --> 00:19:38.540
All right so that's very interesting actually.

349
00:19:38.550 --> 00:19:44.040
This is very surprising results but that's why it's really really important to try all your models and

350
00:19:44.040 --> 00:19:49.530
have these very efficient code templates that allow you to do a quick and efficient moral selection

351
00:19:49.530 --> 00:19:52.520
process to quickly figure out the best model.

352
00:19:52.530 --> 00:19:52.990
All right.

353
00:19:53.040 --> 00:19:55.160
You have to understand that there is no rule of thumbs.

354
00:19:55.290 --> 00:20:00.990
And for some other data sets with other machinery problems the best model will be another one of these

355
00:20:01.050 --> 00:20:02.070
five models.

356
00:20:02.070 --> 00:20:05.460
All right so it was really important for me to show you this and to give you this.

357
00:20:06.220 --> 00:20:06.650
OK.

358
00:20:06.680 --> 00:20:12.930
So now there we go my friends we are at the end of Part Three classification big congratulations to

359
00:20:12.930 --> 00:20:17.720
you for well completing this part three and making such a good progress on this course.

360
00:20:17.730 --> 00:20:23.250
Now in the next part we will start clustering which will be our first unsupervised model.

361
00:20:23.250 --> 00:20:27.810
I remind that the difference between supervised and unsupervised is that with supervised learning you

362
00:20:27.810 --> 00:20:28.650
know what to predict.

363
00:20:28.650 --> 00:20:33.600
You know which deep and to predict and with unsupervised learning you don't know what to predict and

364
00:20:33.600 --> 00:20:38.640
you will actually have to find some patterns in the data to figure out something you could predict as

365
00:20:38.640 --> 00:20:43.500
independent viable but that you don't have a primary and that you create epistolary.

366
00:20:43.500 --> 00:20:43.770
All right.

367
00:20:43.770 --> 00:20:46.860
So Nora is we will see all this in the next part.

368
00:20:47.110 --> 00:20:49.850
And until then I think you deserve a good break now.

369
00:20:49.860 --> 00:20:55.530
So get some good rest relax a bit and as soon as you're back to back with some great energy well join

370
00:20:55.530 --> 00:20:58.380
us in the next part to tackle clustering.

371
00:20:58.380 --> 00:21:00.000
And until then enjoy machine learning.
