1

00:00:00,810  -->  00:00:03,440
Hello and welcome back to the course of machine learning.

2

00:00:03,540  -->  00:00:07,570
Today we're talking about support vector machines or SVM for short.

3

00:00:07,670  -->  00:00:16,800
So SVM is were initially developed in the 1960s then they were refined again in the 1990s and only now

4

00:00:16,800  -->  00:00:22,170
they're becoming very popular in machine learning because they are demonstrating that they can be very

5

00:00:22,170  -->  00:00:26,830
very powerful because they are somewhat different to other machine learning algorithms.

6

00:00:26,910  -->  00:00:28,860
And we'll find out how they're special.

7

00:00:28,860  -->  00:00:29,970
Towards the end of the story.

8

00:00:30,090  -->  00:00:34,560
But for now let's understand how support vector machines actually work.

9

00:00:34,560  -->  00:00:39,620
All right so here we've got as usual points on a two dimensional space for simplicity's sake.

10

00:00:39,660  -->  00:00:41,850
We've got just two columns x1 and x2.

11

00:00:42,060  -->  00:00:48,060
And we've got some observations some already have agreed so we've already classified them but now how

12

00:00:48,060  -->  00:00:51,530
do we derive a line that's going to separate them.

13

00:00:51,530  -->  00:00:53,820
So how do we actually separate these points.

14

00:00:53,820  -->  00:00:55,710
Because that's a separation.

15

00:00:55,710  -->  00:01:00,960
Or in other words that decision boundary is going to be very important for us going forward when we

16

00:01:00,960  -->  00:01:04,410
start adding new points so that's that's the point of our classification.

17

00:01:04,410  -->  00:01:08,880
That's the purpose of openness because we want to create a boundary between these two so that when we

18

00:01:08,880  -->  00:01:13,740
in the future add new points that we want to classify that haven't been classified yet we will know

19

00:01:13,740  -->  00:01:17,240
where they will fall either in the greater green area or in the red area.

20

00:01:17,370  -->  00:01:20,390
So how can we separate these points we see here.

21

00:01:20,610  -->  00:01:26,840
Well one way is to draw a line like that in our two dimensional space and then say anything to the right

22

00:01:26,840  -->  00:01:32,640
will be green anything to the left will be red and if a new point falls somewhere on this space we will

23

00:01:32,640  -->  00:01:35,550
know right away if it's red or green because we'll know where it falls.

24

00:01:35,550  -->  00:01:40,860
However there's another way we can draw a horizontal line like that or we can draw a diagonal line like

25

00:01:40,860  -->  00:01:41,640
that.

26

00:01:41,640  -->  00:01:44,960
We can actually draw another diagonal line or we can draw another diagonal.

27

00:01:44,970  -->  00:01:49,920
So there's lots of different lines that we can create that will achieve the same result they'll separate

28

00:01:49,950  -->  00:01:51,790
our points to two classes.

29

00:01:51,810  -->  00:01:58,560
But at the same time they all in the future will have different consequences so when we add new points

30

00:01:58,830  -->  00:02:03,090
depending on where that point will fall it'll either be classed as part of the Green Zone and part the

31

00:02:03,090  -->  00:02:07,290
Reds or we want to find the optimal line and that's what fields are all about.

32

00:02:07,320  -->  00:02:14,490
They're about finding the best line or the best decision boundary which will help us separate our space

33

00:02:14,490  -->  00:02:15,420
into classes.

34

00:02:15,720  -->  00:02:20,920
So let's find out how the SVM actually searches for this light.

35

00:02:21,090  -->  00:02:28,620
Well the line is searched through the maximum margin so here you can see a line and this is the line

36

00:02:28,620  -->  00:02:28,640
.

37

00:02:28,650  -->  00:02:30,200
And SVM would draw.

38

00:02:30,330  -->  00:02:35,630
And so basically it's the line that separates these two Klaas's of points.

39

00:02:35,730  -->  00:02:42,570
And at the same time it has the maximum margin which means this distance so this line is drawn equidistant

40

00:02:42,570  -->  00:02:47,430
from this point and this point and we'll find out exactly why these points in a second.

41

00:02:47,430  -->  00:02:52,620
And then the distance between the line and each one of these points that's equidistant.

42

00:02:52,860  -->  00:02:59,940
And that's margin's So the sum of these two distances has to be maximized in order for this line to

43

00:02:59,940  -->  00:03:01,510
be the result of the SVM.

44

00:03:01,740  -->  00:03:06,850
And these two points are actually called the support vectors why they're called vectors was about an

45

00:03:06,860  -->  00:03:07,540
hour and a second.

46

00:03:07,540  -->  00:03:13,140
But so basically they these two points are supporting this whole algorithm.

47

00:03:13,140  -->  00:03:18,480
So even if you get rid of all the rest of the points that thing will change the algorithm will be exactly

48

00:03:18,480  -->  00:03:18,810
the same.

49

00:03:18,810  -->  00:03:25,620
So these other points they don't contribute to the result of the algorithm only these two points are

50

00:03:25,890  -->  00:03:31,200
contributing and therefore they called the supporting vectors you can call them supporting points but

51

00:03:31,200  -->  00:03:32,490
in reality they are vectors.

52

00:03:32,490  -->  00:03:38,590
And this is why because in a multi dimensional space when you have more than just two variables you

53

00:03:38,590  -->  00:03:41,400
can have three five 10 or 100 variables.

54

00:03:41,430  -->  00:03:46,620
Each point is actually no longer a point because you can't visualize it on a two dimensional plane or

55

00:03:46,620  -->  00:03:53,400
even a three dimensional space and therefore each of those points that we see here is considered is

56

00:03:53,400  -->  00:03:59,730
actually a vector in a multi dimensional space so the more general term for points that we see here

57

00:03:59,820  -->  00:04:06,090
are vectors and this is something that is studied in mathematics in university or high school mathematics

58

00:04:06,120  -->  00:04:07,020
and basically.

59

00:04:07,020  -->  00:04:12,810
So generally speaking they are all vectors just in this particular example and we have two dimensions

60

00:04:12,820  -->  00:04:16,230
then we can call them points but in reality there are pictures and that's why they're called support

61

00:04:16,230  -->  00:04:16,950
vectors.

62

00:04:17,040  -->  00:04:23,010
So hence these two specific vectors are the ones supporting kind of supporting this decision boundary

63

00:04:23,010  -->  00:04:26,940
or this way we're building this algorithm that's why they're important and that's why this whole algorithm

64

00:04:27,180  -->  00:04:29,350
is called the support vector machines.

65

00:04:29,550  -->  00:04:31,080
So now what else do we have here.

66

00:04:31,080  -->  00:04:37,140
Well we've got the line in the middle which is called the maximum margin hyperplane or the maximum margin

67

00:04:37,140  -->  00:04:37,910
classifier.

68

00:04:37,920  -->  00:04:43,190
So in a two dimensional space it's just like a classifier is just the line.

69

00:04:43,290  -->  00:04:47,000
But actually in a multidimensional space it's a hyperplane.

70

00:04:47,250  -->  00:04:52,800
And I know it's a very confusing term but that's what is called a maximum margin hyperbola.

71

00:04:52,800  -->  00:04:57,440
So those all of the ones that we saw were also hyperplane but there weren't the maximum margin hybrid

72

00:04:57,450  -->  00:05:02,400
blades and you can check that yourself so you can draw a different hyperplane here and just check out

73

00:05:02,400  -->  00:05:03,060
the marginal.

74

00:05:03,060  -->  00:05:06,530
It'll always be less because this is the one with the maximum margin.

75

00:05:06,660  -->  00:05:09,320
And then you've got the green and the red dotted lines.

76

00:05:09,330  -->  00:05:14,130
So the green one is called the positive hyperplane and the red was called the negative hyperplane.

77

00:05:14,130  -->  00:05:18,660
It doesn't really matter in which order you name them just the point is that one of them is positive

78

00:05:18,900  -->  00:05:26,160
and negative or basically anything to the right of the positive is classified as the green category

79

00:05:26,160  -->  00:05:31,940
or the positive category anything to the left to classify as a negative category or the red category

80

00:05:31,950  -->  00:05:32,570
in our case.

81

00:05:32,670  -->  00:05:39,540
So that's how the supervision machine algorithm works of course there's some complicated mathematics

82

00:05:39,540  -->  00:05:46,240
behind it but the essence of the intuitive part of it is exactly this that we're working with a linearly

83

00:05:46,290  -->  00:05:52,410
separable data set where we can actually it's given to us by default that we can put a line through

84

00:05:52,410  -->  00:05:58,470
a chart which will separate the two categories and then we're just searching for the one with the maximum

85

00:05:58,680  -->  00:05:59,820
margin.

86

00:05:59,820  -->  00:06:05,010
So conceptually when you think about it it's actually a pretty simple algorithm when you think about

87

00:06:05,010  -->  00:06:05,770
it this way.

88

00:06:05,840  -->  00:06:11,460
If I were going into the mathematics and the question is what's so special about SVM is why are they

89

00:06:11,460  -->  00:06:17,520
so popular and why are they different to other machine learning algorithms and that's exactly what we're

90

00:06:17,520  -->  00:06:18,740
going to talk about right now.

91

00:06:18,930  -->  00:06:26,820
So imagine you're trying to teach a machine how to distinguish between apples and oranges how to classify

92

00:06:27,500  -->  00:06:29,390
a fruit into either an apple an orange.

93

00:06:29,390  -->  00:06:31,400
So you're telling a machine that.

94

00:06:31,440  -->  00:06:36,620
All right I'm going to give you some test data so have a look at all of these apples.

95

00:06:36,630  -->  00:06:38,220
These are apples oranges.

96

00:06:38,510  -->  00:06:39,180
Analyze them.

97

00:06:39,180  -->  00:06:43,830
Look at them see what parameters they have and then next time they're going to give you.

98

00:06:43,830  -->  00:06:48,270
I'm going to give you a fruit which will be either an apple an orange and you're going to need to classify

99

00:06:48,270  -->  00:06:50,520
it and tell me whether it's an apple or an orange.

100

00:06:50,520  -->  00:06:50,820
Right.

101

00:06:50,820  -->  00:06:55,370
So that's kind of a standard machine learning problem.

102

00:06:55,470  -->  00:07:01,590
Now in our case here you can see let's say on the right we have oranges on the left we have apples.

103

00:07:01,620  -->  00:07:08,130
So what predominately machine Algren's would do is they would look at the most Apple the apples and

104

00:07:08,130  -->  00:07:13,320
the most orange the orange so they would look at the most stock standard common type of apples and the

105

00:07:13,320  -->  00:07:19,410
most stock standard common type of oranges and now case would be Apple some more there in that in the

106

00:07:19,440  -->  00:07:24,700
very heart of the apple Clauss far away from the oranges.

107

00:07:24,810  -->  00:07:26,670
And for the oranges would be somewhere over there.

108

00:07:26,670  -->  00:07:31,070
So also in the very heart of the orange Clauss far away from the Apple so they were tried.

109

00:07:31,110  -->  00:07:37,050
A machine would try to learn from the apples that are very like apples so it would know what an apple

110

00:07:37,050  -->  00:07:37,480
is.

111

00:07:37,530  -->  00:07:43,110
And it also tried to learn from oranges so it would know what an orange actually is and that's how most

112

00:07:43,110  -->  00:07:47,910
of the machine learning algorithms work and then based on that it would be able to come up with some

113

00:07:47,910  -->  00:07:55,050
predictions and classifying four new data elements and variables that you would get it in the case of

114

00:07:55,050  -->  00:07:56,730
support vector machine.

115

00:07:56,730  -->  00:07:57,850
It's a bit different.

116

00:07:57,870  -->  00:08:03,910
Instead of looking at the most stocks standard apples and stocks and oranges what this support victualling

117

00:08:03,910  -->  00:08:10,050
machines do is they actually look at the apples that are very much like an orange so here you can see

118

00:08:10,050  -->  00:08:13,590
an apple which is not your standard Apple is orange and color right.

119

00:08:13,590  -->  00:08:19,110
So it's very easy to infuse this apple of an orange and they would look at oranges which are not stock

120

00:08:19,110  -->  00:08:23,400
standard oranges which are more like apples than anything else so you can order the Lemon here.

121

00:08:23,400  -->  00:08:29,670
So those of us in the image just out of the oranges the SVM would pick the one that is that looks the

122

00:08:29,670  -->  00:08:31,200
most like an apple in this case.

123

00:08:31,210  -->  00:08:32,550
We have a green orange.

124

00:08:32,610  -->  00:08:37,490
It's not normal to have a green orange when you think of orange you think of orange orange.

125

00:08:37,560  -->  00:08:39,420
And so what that is is.

126

00:08:39,600  -->  00:08:44,070
Those are the support the support vectors so the support vectors you can see that they're actually very

127

00:08:44,070  -->  00:08:49,260
close to the boundary so they're very close to the apple or the red one would be very close to the green

128

00:08:49,260  -->  00:08:54,300
ones and the orange or the green mark here would be very close to the red ones and therefore the support

129

00:08:54,300  -->  00:09:00,240
vector machine in that sense you can think of it is like a more extreme type of algorithm a very rebellious

130

00:09:00,240  -->  00:09:06,390
type of algorithm a very risky type of algorithm because it looks at a very extreme case which is very

131

00:09:06,390  -->  00:09:13,030
close to the boundary and it uses that to construct its analysis.

132

00:09:13,140  -->  00:09:21,090
And that in itself makes the support vector machine algorithms very special very different to most of

133

00:09:21,090  -->  00:09:22,860
the other machine learning algorithms.

134

00:09:22,860  -->  00:09:29,880
And that's why at times they perform much better than non-supported of vector machine algorithms.

135

00:09:30,090  -->  00:09:35,760
So there you go I hope this explanation and intuition of support vector machines was useful and now

136

00:09:35,790  -->  00:09:41,700
not only you know how they work but also why they are different to other algorithms out there that are

137

00:09:41,700  -->  00:09:43,050
used in machine learning.

138

00:09:43,050  -->  00:09:45,180
And on that note we're going to enter this material.

139

00:09:45,260  -->  00:09:46,940
I look forward to seeing you next time.

140

00:09:46,940  -->  00:09:48,630
Until then in Germany learning
