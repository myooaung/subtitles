1
00:00:00,210 --> 00:00:01,280
All right my friends.

2
00:00:01,320 --> 00:00:03,840
Are you ready for the demo.

3
00:00:03,840 --> 00:00:09,170
I remind that this demo works for any data set you know regardless of the number of features.

4
00:00:09,240 --> 00:00:13,740
And as long as they have you know the features in the first columns and then the dependent variable

5
00:00:13,860 --> 00:00:20,310
in the last column and also assuming that any missing data or categorical data was already taken care

6
00:00:20,310 --> 00:00:23,420
of thanks to your data pricing tool kit.

7
00:00:23,490 --> 00:00:23,850
All right.

8
00:00:23,880 --> 00:00:29,910
So this is gonna be very exciting because it's really now that I'm going to show you the power of code

9
00:00:29,910 --> 00:00:36,660
templates and how you can quickly and efficiently select the best regression model.

10
00:00:36,660 --> 00:00:37,000
All right.

11
00:00:37,050 --> 00:00:37,930
So let's do this.

12
00:00:37,980 --> 00:00:38,910
Enough talking.

13
00:00:38,910 --> 00:00:40,730
I'm going to proceed to the demo now.

14
00:00:40,920 --> 00:00:44,940
Just resetting everything because we're gonna do something fun.

15
00:00:44,940 --> 00:00:52,800
We will actually use this run all option of the runtime which will run all our sales at once so that

16
00:00:52,830 --> 00:00:59,580
you know we can really optimize the efficiency but let's not forget to upload the data set in each of

17
00:00:59,580 --> 00:01:00,960
the implementations.

18
00:01:00,960 --> 00:01:04,470
Otherwise this sale won't be able to execute.

19
00:01:04,650 --> 00:01:06,340
So we're going to upload it now.

20
00:01:06,360 --> 00:01:11,400
It is connecting to runtime in a second we should be able to see the button there we go.

21
00:01:11,880 --> 00:01:18,300
So let's click this upload button and now on your machine you're gonna find the folder you know the

22
00:01:18,300 --> 00:01:19,450
model selection of all the.

23
00:01:19,450 --> 00:01:20,980
That's the whole machinery.

24
00:01:20,990 --> 00:01:21,660
It is it for them.

25
00:01:21,660 --> 00:01:24,510
And that this new model selection for the container.

26
00:01:24,510 --> 00:01:29,580
You know that regression folder with all the good templates for regression and the classification with

27
00:01:29,610 --> 00:01:31,570
all the good templates for classification.

28
00:01:31,770 --> 00:01:37,830
If you missed that folder somehow don't worry it was given right before this tutorial you know in the

29
00:01:37,830 --> 00:01:44,160
article at the bottom you had a zip file attached which you could download on your machine and which

30
00:01:44,160 --> 00:01:46,580
contains exactly the same as what I have here.

31
00:01:46,590 --> 00:01:46,890
All right.

32
00:01:46,890 --> 00:01:52,620
So now we're going to go to the regression folder which contains all the implementations meaning all

33
00:01:52,620 --> 00:01:58,590
the code templates for each of your regression models both in IP y and B format which you can open with

34
00:01:58,770 --> 00:02:04,440
either Google cool app or Jupiter notebook and in P Y format which you can open with a classic Python

35
00:02:04,440 --> 00:02:07,170
terminal or spider in Anaconda.

36
00:02:07,170 --> 00:02:13,050
So you have everything and you also have the data set you know containing these four features the energy

37
00:02:13,050 --> 00:02:19,520
temperature the vacuum the engine pressure and the humidity and we predict the energy output.

38
00:02:19,530 --> 00:02:21,390
All right so that's a very classic data set.

39
00:02:21,420 --> 00:02:27,450
Once again very generic trying to represent the other future data set you'll be working on and well

40
00:02:27,450 --> 00:02:30,510
speaking of this data set that's exactly what we have to select here.

41
00:02:30,510 --> 00:02:36,850
So we're going to click open to upload the data set in our notebook and there it is.

42
00:02:36,870 --> 00:02:42,570
And now as I told you here in the implementation you only have to answer the name of your dataset.

43
00:02:42,570 --> 00:02:47,370
And as far as we're concerned here for a demo Well this they say is called data data.

44
00:02:47,370 --> 00:02:47,760
Yes.

45
00:02:48,080 --> 00:02:48,780
All right.

46
00:02:48,780 --> 00:02:55,700
So now we're going to quickly do exactly the same for other implementations upload then data that says

47
00:02:55,700 --> 00:03:03,620
we then open and loading it's uploading it and we'll have it in a second and then we'll just replace

48
00:03:03,620 --> 00:03:06,230
the name data set here by data.

49
00:03:06,240 --> 00:03:12,980
That's easy that's for polynomial regression now for support vector regression will same upload data

50
00:03:12,980 --> 00:03:19,190
that says we open and loading it uploading it in a second we should have it.

51
00:03:19,220 --> 00:03:20,190
There we go.

52
00:03:20,210 --> 00:03:27,140
Now we only replace this by data that's years V then for decision tree regression.

53
00:03:27,140 --> 00:03:27,920
There we go.

54
00:03:27,920 --> 00:03:36,710
Upload then data that says we open and we will have it in a second uploaded in the notebook and now

55
00:03:36,800 --> 00:03:39,400
replacing this by data that's easy.

56
00:03:39,530 --> 00:03:49,010
And finally for random forest regression upload data see as we open and now replacing this byte data

57
00:03:49,210 --> 00:03:49,890
that is.

58
00:03:49,980 --> 00:03:57,890
And now my friends we are finally ready to test each of our regression models and figure out in a flashlight

59
00:03:58,100 --> 00:03:59,650
which one is the best.

60
00:03:59,660 --> 00:04:05,600
Remember the closer the r squared coefficient is to 1 the better is your regression more.

61
00:04:05,600 --> 00:04:10,910
So in order to figure out which is going to be the best model we'll just take the one with the highest

62
00:04:11,090 --> 00:04:14,890
r squared you know with our square that is the closest to one.

63
00:04:14,930 --> 00:04:15,860
All right.

64
00:04:15,860 --> 00:04:16,520
Are you ready.

65
00:04:16,610 --> 00:04:17,480
Let's do this.

66
00:04:17,510 --> 00:04:19,700
Starting with multiple in our regression.

67
00:04:19,700 --> 00:04:26,310
So now we're simply going to go to runtime and then run 0 and all the sales are now executing.

68
00:04:26,600 --> 00:04:27,940
And there you go.

69
00:04:27,950 --> 00:04:33,000
We end up with an art squared coefficient of 0 point ninety three.

70
00:04:33,020 --> 00:04:33,800
Very good.

71
00:04:33,800 --> 00:04:38,300
Very good one as we can clearly see you know the predictions are amazing they're very close to the real

72
00:04:38,300 --> 00:04:38,810
results.

73
00:04:38,840 --> 00:04:43,820
So remember this first column is the vector of predictions and this second one is the vector of real

74
00:04:43,820 --> 00:04:47,840
results and that's why here we have an amazing are squared coefficients.

75
00:04:48,050 --> 00:04:50,840
But according to you is it going to be the best model.

76
00:04:50,840 --> 00:04:57,560
Well that's what we're going to figure out very quickly because then we're going to run all the cells

77
00:04:57,560 --> 00:04:59,930
here for the pulling them a regression.

78
00:04:59,930 --> 00:05:00,450
There we go.

79
00:05:00,470 --> 00:05:06,830
As you can see it's growing very fast and well for the polynomial regression model with remember a degree

80
00:05:06,830 --> 00:05:07,630
of four.

81
00:05:07,760 --> 00:05:12,910
We get a final r squared coefficient of 0 point ninety four 258.

82
00:05:13,010 --> 00:05:19,760
So we've already beat our multiple in our regression model and therefore so far the best one is polynomial

83
00:05:19,760 --> 00:05:20,300
regression.

84
00:05:20,300 --> 00:05:24,150
Feel free to actually test some other degrees if you want.

85
00:05:24,380 --> 00:05:29,990
But now we're going to move on to support vector regression and see if it's going to beat that pulling

86
00:05:29,990 --> 00:05:30,920
them a regression.

87
00:05:30,920 --> 00:05:35,730
I actually really liked this model and usually I get the best results with this but let's see.

88
00:05:35,780 --> 00:05:41,550
Let's run all the cells and in flashlight we'll get the final performance.

89
00:05:41,780 --> 00:05:43,190
No not yet not yet actually.

90
00:05:43,190 --> 00:05:50,120
And by very very short it beats indeed as a polynomial regression mode you know 0 point ninety four

91
00:05:50,150 --> 00:05:50,780
eighty.

92
00:05:50,810 --> 00:05:58,000
And here we had actually 0 point ninety 458 so so far the best model is the support vector regression.

93
00:05:58,040 --> 00:06:00,470
Can you see how we're going super fast here.

94
00:06:00,500 --> 00:06:00,800
Right.

95
00:06:00,800 --> 00:06:06,620
We only had to change the name of the dataset here and all the rest is automatic and that's the beauty

96
00:06:06,890 --> 00:06:08,150
of code templates.

97
00:06:08,180 --> 00:06:13,700
Now let's move onto the next one decision tree regression and let's see if it can beat the support vector

98
00:06:13,700 --> 00:06:17,750
regression which remember has opened 94 8.

99
00:06:17,780 --> 00:06:18,680
So let's see.

100
00:06:18,680 --> 00:06:27,300
Let's run you know all the cells and the final result is 0 point ninety two two.

101
00:06:27,550 --> 00:06:27,820
Okay.

102
00:06:27,830 --> 00:06:29,240
So that's actually the worst.

103
00:06:29,420 --> 00:06:32,810
I think it is indeed worse than the multiple in our regression.

104
00:06:32,870 --> 00:06:33,530
Yes.

105
00:06:33,530 --> 00:06:38,540
So the decision tree regression rule did not perform well here but maybe that's because it didn't have

106
00:06:38,540 --> 00:06:43,940
enough team spirit to tackle these predictions and that's what we're going to figure out with random

107
00:06:43,940 --> 00:06:49,820
forest regression because indeed a random forest regression model is a bunch of trees teaming up to

108
00:06:49,880 --> 00:06:52,000
return an ultimate prediction.

109
00:06:52,010 --> 00:06:59,720
So now the ultimate question is do you think that the final winner of this competition is going to be

110
00:06:59,810 --> 00:07:04,060
the support vector regression mall or the random forest regression model.

111
00:07:04,100 --> 00:07:07,090
Take your bet and let's see if you're right.

112
00:07:07,160 --> 00:07:07,640
There we go.

113
00:07:07,640 --> 00:07:12,710
We're about to find out in a second because we're going to click Run or now and we're going to get the

114
00:07:12,710 --> 00:07:19,670
final final result with a final r squared coefficient of actually 0 point ninety six which therefore

115
00:07:19,670 --> 00:07:25,190
makes the random forest regression the big winner of this data competition.

116
00:07:25,220 --> 00:07:31,550
So congratulations in a very few seconds you were able to quickly identify and select the best regression

117
00:07:31,550 --> 00:07:37,280
model and mostly you know the most important thing in all this is to finally know how to answer this

118
00:07:37,430 --> 00:07:41,760
very often asked question how do I select the best model.

119
00:07:41,860 --> 00:07:48,470
And well the answer is you simply try all of them and using the r squared coefficient you compare them

120
00:07:48,620 --> 00:07:51,290
and conclude on which one is the best.

121
00:07:51,290 --> 00:07:54,370
And in our situation here you know for this data set.

122
00:07:54,450 --> 00:07:57,710
Well the best is the random for its regression.

123
00:07:57,750 --> 00:07:58,500
All right.

124
00:07:58,500 --> 00:08:00,060
So congratulations.

125
00:08:00,060 --> 00:08:01,640
Now you know a lot.

126
00:08:01,650 --> 00:08:05,050
You know you have an expertise in regression models.

127
00:08:05,100 --> 00:08:09,450
You not only know how to build them but also you have all these code templates which you can use very

128
00:08:09,450 --> 00:08:14,330
efficiently to select the best machinery model for your regression problem.

129
00:08:14,400 --> 00:08:16,480
And that's for any dataset.

130
00:08:16,620 --> 00:08:22,860
And remember if your dataset has missing data or categorical data well you only have to grab your tools

131
00:08:22,920 --> 00:08:26,320
in the data pricing toolkit to handle these situations.

132
00:08:26,400 --> 00:08:29,440
And then you can deploy your code templates.

133
00:08:29,520 --> 00:08:30,080
All right.

134
00:08:30,090 --> 00:08:34,660
So I'm super excited and super happy now that we completed 100 percent.

135
00:08:34,740 --> 00:08:41,010
This part 2 on regression and now my friends we're going to move on to a brand new branch of machine

136
00:08:41,010 --> 00:08:46,980
learning which is classification and which we're gonna do exactly the same but this time to predict

137
00:08:47,190 --> 00:08:51,390
a category and same as what we did for this regression branch.

138
00:08:51,390 --> 00:08:56,880
Well we will build several classification models and at the end I'll show you again how to select the

139
00:08:56,880 --> 00:08:57,740
best one.

140
00:08:57,780 --> 00:09:00,670
So I can't wait to see you in this part 3.

141
00:09:00,690 --> 00:09:02,580
And until then enjoy machine learning.
