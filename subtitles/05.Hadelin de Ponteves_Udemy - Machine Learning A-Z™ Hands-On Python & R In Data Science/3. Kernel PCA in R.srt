1
00:00:00,300 --> 00:00:02,580
Hello and welcome to the art of Tauriel.

2
00:00:02,640 --> 00:00:06,240
So now we know how to implement to feature extraction techniques.

3
00:00:06,240 --> 00:00:11,580
These are PCa and LDA But these feature extraction techniques work on Lynnie are problems.

4
00:00:11,580 --> 00:00:14,200
That is when the data is linearly separable.

5
00:00:14,550 --> 00:00:19,260
And in this section we are going to see one new feature extraction technique but this time adapted for

6
00:00:19,260 --> 00:00:23,150
non-linear problems where the data is nonlinearly separable.

7
00:00:23,160 --> 00:00:25,790
So this technique is called Colonel PCA.

8
00:00:26,160 --> 00:00:32,130
Colonel PCa is a current life version of PCa where we map the data to a higher dimension using the kernel

9
00:00:32,130 --> 00:00:32,570
trick.

10
00:00:32,670 --> 00:00:37,320
And then from there we extract some new principal components and we're going to see how it manages to

11
00:00:37,320 --> 00:00:39,320
deal with nonlinear problems.

12
00:00:39,570 --> 00:00:44,970
So we're not going to work on the same problem as we did in the previous sections with the one dataset

13
00:00:45,210 --> 00:00:50,250
but we're going to work on the same dataset as the one used in part 3 classification because now we

14
00:00:50,250 --> 00:00:50,960
need visuals.

15
00:00:51,000 --> 00:00:58,170
We need to clearly see what happens we need to see how current PCA manages to extract some new independent

16
00:00:58,170 --> 00:01:03,360
variables the principal components even when the problem is non-linear or that is when the data is not

17
00:01:03,360 --> 00:01:04,510
linearly separable.

18
00:01:04,710 --> 00:01:10,770
And this dataset that we used in part 3 the social network adds data sets will remember it was clearly

19
00:01:10,770 --> 00:01:15,600
a nonlinear problem because nonlinear classifiers showed much better performance.

20
00:01:15,600 --> 00:01:21,280
So let's take this dataset and let's have like PCA to see how it will handle the nominee already.

21
00:01:21,300 --> 00:01:24,780
So let's find this data set into our working directory folder.

22
00:01:24,810 --> 00:01:26,380
So we'll get to our mission.

23
00:01:26,380 --> 00:01:29,410
It is a folder and part of nine dimensionality reduction.

24
00:01:29,640 --> 00:01:33,900
And here we are at the last section of this part 9 kernel PCA.

25
00:01:33,950 --> 00:01:37,830
So that's the folder you want to set as a working directory make sure that you have the social network

26
00:01:37,830 --> 00:01:38,980
at as well.

27
00:01:39,120 --> 00:01:44,220
And if that's the case you really click on this more about near to set the folder as a working directory.

28
00:01:44,500 --> 00:01:50,700
And now what we're going to do is take this logistic regression model because you know this logistic

29
00:01:50,700 --> 00:01:56,340
regression model is a linear classifier therefore it will not be appropriate for our problem because

30
00:01:56,550 --> 00:01:58,610
our data is not linearly separable.

31
00:01:58,620 --> 00:02:05,220
So what we're going to do is take this linear classify here but we are going to apply kernel Pitsea

32
00:02:05,220 --> 00:02:09,990
inside of it to see how kernel PCA will save the situation.

33
00:02:10,020 --> 00:02:12,930
And so you will see that even if we apply a linear model.

34
00:02:13,050 --> 00:02:19,380
Well thanks to Colonel PCa that we managed to extract new principal components adapted for this nonlinearly

35
00:02:19,380 --> 00:02:20,440
separable data.

36
00:02:20,550 --> 00:02:22,840
Well you will see that will get amazing results.

37
00:02:23,010 --> 00:02:29,180
So right now let's copy the whole model here from the top down to the bottom copy.

38
00:02:29,460 --> 00:02:32,990
And let's face it in our code PCA file.

39
00:02:33,140 --> 00:02:33,960
All right.

40
00:02:34,200 --> 00:02:40,620
And now basically the only thing that we have to do is to apply code PCa at the right place.

41
00:02:40,620 --> 00:02:45,840
But before we do that I would just like to visualize again why this linear model is not appropriate

42
00:02:45,990 --> 00:02:47,720
for this dataset.

43
00:02:47,730 --> 00:02:53,090
So what we're going to do is take everything from here because you know this will visualize the trainings

44
00:02:53,090 --> 00:02:56,820
that results by plotting the prediction regions and the prediction boundary.

45
00:02:56,940 --> 00:03:02,940
So we're going to take everything from here up to the top to you know important data set apply the preprocessing

46
00:03:02,940 --> 00:03:08,500
phase filter logistic regression to the training set and eventually plot the training that resolves.

47
00:03:08,520 --> 00:03:14,040
So let's do it let's visualize this again very quickly and that will give us motivation to apply.

48
00:03:14,040 --> 00:03:17,410
Colonel PICHETTE and here we go.

49
00:03:17,470 --> 00:03:22,180
All executed properly so as a reminder the points are the real observations.

50
00:03:22,180 --> 00:03:28,420
That is our real customers in the social network represented by their age and their estimated salary.

51
00:03:28,420 --> 00:03:33,920
So that's our real observation points and our predictions are represented by these regions.

52
00:03:33,940 --> 00:03:36,240
The red region here and the green region here.

53
00:03:36,310 --> 00:03:42,580
And basically this red region is where our model predicts that the customer will not click on the ad

54
00:03:42,940 --> 00:03:47,680
and this green region here is the region where a model predicts that the customers will click on the

55
00:03:47,680 --> 00:03:49,870
ad and buy the SUV.

56
00:03:49,900 --> 00:03:56,500
And so remember the problem was that this straight line here is actually the prediction boundary generated

57
00:03:56,500 --> 00:03:58,200
by the logistic regression model.

58
00:03:58,360 --> 00:04:03,490
But since the logistic regression model is a linear classifier then it has to be a straight line here

59
00:04:03,490 --> 00:04:08,320
separating the data and therefore remember the problem is that it can not make some kind of a curve

60
00:04:08,320 --> 00:04:14,360
here to catch these green users that should be in the green region right now they're in the red region.

61
00:04:14,460 --> 00:04:19,420
And so this clearly represents the fact that our data is not literally separable because we can clearly

62
00:04:19,420 --> 00:04:25,270
see that this prediction boundary here that plays the role of separator and that is supposed to separate

63
00:04:25,360 --> 00:04:26,360
the two classes.

64
00:04:26,590 --> 00:04:31,540
Well it cannot separate the two classes properly because as you can see these users are not in the right

65
00:04:31,540 --> 00:04:37,690
region and so now what we are going to do is not make a nonlinear classifier like we did in part three

66
00:04:37,840 --> 00:04:41,800
you know when we made Colonel SBM base decision trees around him for us.

67
00:04:41,950 --> 00:04:48,400
Well what we're going to do now instead is a black Colonel PCA so that we keep a straight line as a

68
00:04:48,400 --> 00:04:53,470
separator as the prediction boundary of a linear classifier that is still going to be the prediction

69
00:04:53,470 --> 00:04:55,590
boundary of the logistic regression model.

70
00:04:55,840 --> 00:05:02,320
But since we're going to apply kernel PCA Well this will manage to apply some trick where the trick

71
00:05:02,320 --> 00:05:08,800
is actually the kernel trick to map the data into a higher dimension and then apply PCA to extract new

72
00:05:08,800 --> 00:05:13,050
components that will be new dimensions that explain the most variants.

73
00:05:13,090 --> 00:05:19,240
But thanks to this Colonel Treyc Well you'll see that we'll manage to get some new dimensions in which

74
00:05:19,240 --> 00:05:24,420
the data will be linearly separable even by a linear classifier like logistic regression.

75
00:05:24,580 --> 00:05:25,580
So let's see that right now.

76
00:05:25,600 --> 00:05:27,250
I can't wait to show you this.

77
00:05:27,280 --> 00:05:28,540
I'm going to close this.

78
00:05:28,600 --> 00:05:32,920
And now let's apply kernel PCa at the right location.

79
00:05:32,950 --> 00:05:34,810
So you already know what this location is.

80
00:05:34,810 --> 00:05:37,600
It's actually not different than before.

81
00:05:37,600 --> 00:05:43,630
We need to apply kernel PCA right up to the data preprocessing phase and just before fitting our classifier

82
00:05:43,630 --> 00:05:46,080
like logistic regression to our training centers.

83
00:05:46,150 --> 00:05:56,420
So basically we need to apply current PCA right here so U.S. here applying Kerno PCa and here we go

84
00:05:56,610 --> 00:05:57,560
let's do it.

85
00:05:57,900 --> 00:05:58,190
All right.

86
00:05:58,200 --> 00:06:03,210
So first we need to install a new package that is called current lab which I don't think we'll install

87
00:06:03,210 --> 00:06:04,800
before so let's do it right now.

88
00:06:05,070 --> 00:06:08,850
So we use the command install packages.

89
00:06:08,880 --> 00:06:09,550
Here we go.

90
00:06:09,690 --> 00:06:12,450
And in quotes kerflop.

91
00:06:12,510 --> 00:06:13,080
All right.

92
00:06:13,320 --> 00:06:15,650
So I think I already have it installed.

93
00:06:15,720 --> 00:06:17,130
Let's check it out.

94
00:06:17,130 --> 00:06:20,410
So here it is current lab kernel based machinery lab.

95
00:06:20,520 --> 00:06:21,980
So I will not install it.

96
00:06:21,990 --> 00:06:25,190
But if you want to do it you just like this line and execute.

97
00:06:25,260 --> 00:06:27,810
So I will just put this line as a comment.

98
00:06:27,840 --> 00:06:28,540
Here we go.

99
00:06:28,680 --> 00:06:35,290
But then since it is not imported I will import using the library command Kurla.

100
00:06:35,640 --> 00:06:36,290
All right.

101
00:06:36,390 --> 00:06:38,900
And that will import it.

102
00:06:39,030 --> 00:06:39,670
All right.

103
00:06:39,680 --> 00:06:41,170
Kurla will import it.

104
00:06:41,280 --> 00:06:44,590
And now let's start applying kernel PCA.

105
00:06:44,940 --> 00:06:51,540
So ask for PCa and LDA we're going to start by creating an object which will be the kernel object that

106
00:06:51,540 --> 00:06:55,990
we will use to transform our original data set into this dataset.

107
00:06:56,010 --> 00:07:02,700
After using the kernel trick so we'll call this object K PCa and then equals and then that's where we

108
00:07:02,700 --> 00:07:06,300
use the function that will create this kernel object.

109
00:07:06,540 --> 00:07:13,230
So this function is also K PCa then parenthesis and then lets input the different arguments.

110
00:07:13,230 --> 00:07:18,390
So let's take it out let's press on here to have a look at the arguments.

111
00:07:18,810 --> 00:07:24,920
So the first argument is X and this is actually the data matrix or the formula describing the model.

112
00:07:24,930 --> 00:07:30,060
And here I'll give you a little trick to describe the model very simply and very efficiently we can

113
00:07:30,060 --> 00:07:32,150
simply input here until the.

114
00:07:32,230 --> 00:07:38,700
And that and that will be enough for the KPC a function to understand what the formula is because then

115
00:07:38,730 --> 00:07:45,720
we will add the second argument which is data and that is actually the training set but without the

116
00:07:45,720 --> 00:07:51,540
dependent variable because remember Colonel PCA is just a PCA technique where we use the kernel trick

117
00:07:51,750 --> 00:07:57,570
to map the data into a higher dimension and then apply PCA because indeed in this high dimension the

118
00:07:57,570 --> 00:08:04,380
data is linearly separable and therefore since we apply PCa in this higher dimension and PCA is an unsupervised

119
00:08:04,380 --> 00:08:05,090
technique.

120
00:08:05,160 --> 00:08:10,890
Well here for the data argument we just need to input the training set but without the dependent variable

121
00:08:11,190 --> 00:08:19,830
and therefore asked for PCa we input here data equals training set in brackets to remove the dependent

122
00:08:19,830 --> 00:08:25,170
variable which is indexed by 3 because we only have two independent variables.

123
00:08:25,200 --> 00:08:25,710
All right.

124
00:08:25,710 --> 00:08:27,760
And then the next argument is kernel.

125
00:08:27,840 --> 00:08:32,130
So kernel is a kernel you want to use to apply your kernel trick.

126
00:08:32,160 --> 00:08:38,100
Remember when we studied kernel SVM we saw that there were several kernels to use the kernel trick and

127
00:08:38,100 --> 00:08:43,520
here we are going to use the most common one which is the gashing Colonel and that is called here RBA.

128
00:08:43,740 --> 00:08:45,880
So that's our third argument.

129
00:08:46,050 --> 00:08:51,960
And so here we can put kernel equals RDF dot.

130
00:08:52,190 --> 00:08:52,850
All right.

131
00:08:52,920 --> 00:08:58,600
And then what is the next argument the next argument is keep or we will actually not use this one.

132
00:08:58,740 --> 00:09:04,950
But then we have a very important argument that is at the heart of dimensionality reduction that is

133
00:09:04,950 --> 00:09:10,440
features which is the number of features the number of principal components you want to end up with.

134
00:09:10,590 --> 00:09:15,960
So here of course who would like to visualize the trainings that results in the test results in two

135
00:09:15,960 --> 00:09:22,230
dimensions and to have this in two dimensions we need to keep a number of two new extracted independent

136
00:09:22,230 --> 00:09:23,030
variables.

137
00:09:23,040 --> 00:09:28,000
So here the number of features will be to ask for PCa.

138
00:09:28,260 --> 00:09:32,790
So we will input here features equals to.

139
00:09:33,200 --> 00:09:33,590
All right.

140
00:09:33,590 --> 00:09:35,770
And that's it for our PCA.

141
00:09:35,800 --> 00:09:41,960
Object is ready to be created and to be used to transform our original data set into this new data set

142
00:09:41,960 --> 00:09:45,700
with the new extracted features derived from kernel PCA.

143
00:09:45,710 --> 00:09:49,330
So let's select this line and create the object.

144
00:09:49,340 --> 00:09:50,330
Here it is.

145
00:09:50,330 --> 00:09:56,210
You say well create it and now let's move onto the next step which is to transform our original dataset

146
00:09:56,330 --> 00:09:59,660
into this new extracted dataset.

147
00:09:59,660 --> 00:10:03,090
So now things are going to look like what we did with PCa.

148
00:10:03,260 --> 00:10:08,030
But some things are going to change so we will do it step by step and we'll see where we need to make

149
00:10:08,180 --> 00:10:09,240
some changes.

150
00:10:09,560 --> 00:10:15,370
All right so first as we're busy we're going to use the predict function to transform our original trainset

151
00:10:15,620 --> 00:10:17,800
into this new extracted training set.

152
00:10:17,840 --> 00:10:24,530
So this new trainset with a new extracted features derived from Colonel PCA we call it training set

153
00:10:25,190 --> 00:10:26,910
underscore PCA.

154
00:10:27,200 --> 00:10:32,650
All right and then equals and then we use to predict function to do the transformation.

155
00:10:32,690 --> 00:10:38,830
And inside this pretty function we first input our KPC object as we did for PCa.

156
00:10:39,050 --> 00:10:42,720
And then the training set the original training set.

157
00:10:42,730 --> 00:10:44,590
So let's do it training set.

158
00:10:44,690 --> 00:10:46,150
The second one.

159
00:10:46,190 --> 00:10:46,940
All right.

160
00:10:47,180 --> 00:10:48,980
And as opposed to PCA.

161
00:10:48,980 --> 00:10:57,230
And as with LDA this will return a matrix and we need it as a date frame to ask for LDK will use that

162
00:10:57,320 --> 00:11:02,340
as that data thought brain function.

163
00:11:02,360 --> 00:11:09,640
So parenthesis here and we close the parenthesis here to set this transform training said trining said

164
00:11:09,650 --> 00:11:16,320
PCA as different and as a reminder we're doing this to give what the next function will use in the next

165
00:11:16,320 --> 00:11:18,190
sections expect.

166
00:11:18,200 --> 00:11:19,960
All right so far so good.

167
00:11:20,020 --> 00:11:25,460
And so now let's select this plan and execute this and you're going to see what's going to happen and

168
00:11:25,460 --> 00:11:31,010
you're going to understand why we called this new training set training said PCa with a different name

169
00:11:31,100 --> 00:11:33,840
than the original training said trainset.

170
00:11:33,860 --> 00:11:35,060
All right so let's execute.

171
00:11:35,060 --> 00:11:36,100
Here we go.

172
00:11:36,140 --> 00:11:37,490
Executed properly.

173
00:11:37,670 --> 00:11:42,550
And now let's have a look at our training said PCa that we just created.

174
00:11:42,560 --> 00:11:47,990
So I'm going to enlarge this so that we can see which one is strains that PCA that's the one.

175
00:11:48,050 --> 00:11:49,540
So let's have a look at this.

176
00:11:49,550 --> 00:11:53,360
I'm going to click on it and here is our trainset PCA.

177
00:11:53,360 --> 00:11:57,930
So as we can see it is composed of only two columns we want and V-2.

178
00:11:58,370 --> 00:12:00,840
So try to guess what these two columns are.

179
00:12:00,860 --> 00:12:06,470
I'm going to tell you right now these two columns are the principal components that we obtained through

180
00:12:06,470 --> 00:12:07,570
current PCA.

181
00:12:07,700 --> 00:12:11,000
That is these are two new extracted features.

182
00:12:11,030 --> 00:12:16,580
After all this mapping into this high dimension using the Caltech and then applying PCA to the data

183
00:12:16,580 --> 00:12:18,780
set mapped into this high dimension.

184
00:12:19,220 --> 00:12:26,390
But now the problem is that in this training said PCA we don't have to depend Roybal and we need it

185
00:12:26,390 --> 00:12:32,270
for the next sections because in our code templates we need to have the independent variables and the

186
00:12:32,270 --> 00:12:33,480
dependent variable.

187
00:12:33,530 --> 00:12:40,450
So what is the next step now the next step is to add the dependent variable into this training set PCa

188
00:12:40,790 --> 00:12:47,180
and sort of thing to understand here is that we lost the deep and Roybal but we kept the observations

189
00:12:47,180 --> 00:12:53,830
that is this one here corresponds to the first observation we had in the original training set.

190
00:12:53,960 --> 00:12:54,650
This one.

191
00:12:54,650 --> 00:13:01,590
So this first observation here has the zero label that is that this first customer didn't buy the SUV.

192
00:13:01,700 --> 00:13:04,190
This was the original independent variables.

193
00:13:04,190 --> 00:13:10,460
And then if you go to our training said PCA Well this first customer is the same first customer as this

194
00:13:10,460 --> 00:13:11,290
training sets.

195
00:13:11,300 --> 00:13:14,840
So it will have the zero label in the purchase column.

196
00:13:14,840 --> 00:13:20,690
But then these are new extracted features of course we don't get the same values as for the independent

197
00:13:20,690 --> 00:13:22,940
variables of our original trainset.

198
00:13:22,940 --> 00:13:29,510
So what we can do now is simply take the dependent variable call him purchased of this original training

199
00:13:29,510 --> 00:13:36,890
set and add it to our training set PCA because these observations here are the same observations of

200
00:13:36,890 --> 00:13:38,500
our original trainset.

201
00:13:38,570 --> 00:13:46,070
And so what we need to do now is very simple we just need to take our training set PCa then we're going

202
00:13:46,070 --> 00:13:49,450
to add a new column that will call purchased.

203
00:13:49,670 --> 00:13:55,280
So by doing this you know I'm just creating a new column that I also called purchased because this new

204
00:13:55,280 --> 00:14:01,430
column is going to be purchased dependent invaluable and then equals and then what I have to do now

205
00:14:01,520 --> 00:14:07,730
is to take the real purchase that Ben Roybal call him from the original trainset and we can do that

206
00:14:07,730 --> 00:14:12,920
because the training said PCA contains the same observations as the observations of the original training

207
00:14:12,920 --> 00:14:13,570
set.

208
00:14:13,670 --> 00:14:19,430
So here to take the purchase column of the original trainset we just need to take our original trainset

209
00:14:19,550 --> 00:14:22,590
which is called training sets and then dollar.

210
00:14:22,820 --> 00:14:26,100
And then that's where we take the chaste column.

211
00:14:26,180 --> 00:14:32,890
So by doing this I will add this new column pre-Chase and this you can pre-Chase I will include the

212
00:14:32,890 --> 00:14:36,190
values of the pre-Chase column of the original training set.

213
00:14:36,420 --> 00:14:37,600
Let's check it out.

214
00:14:37,600 --> 00:14:40,480
I'm going to select this line and execute.

215
00:14:40,660 --> 00:14:47,590
And now as you can see if I go back to training that PCA this contains the pre-Chase column of the original

216
00:14:47,590 --> 00:14:48,480
trainset.

217
00:14:48,670 --> 00:14:49,200
So that's good.

218
00:14:49,200 --> 00:14:50,490
That's the next step done.

219
00:14:50,530 --> 00:14:55,960
And now we need to take care of the desert and so to take care of that said we're going to do exactly

220
00:14:55,960 --> 00:14:59,090
the same as we did for this training said PCA.

221
00:14:59,110 --> 00:15:03,670
So let's copy this copy and let's paste it here.

222
00:15:03,700 --> 00:15:10,760
And of course what we're going to do now is replace this training said PCA by test set PCA.

223
00:15:11,050 --> 00:15:17,110
And same here we take the original test set to make the transformation and then we're going to add the

224
00:15:17,110 --> 00:15:22,600
purchase column of the original test set to this new test set.

225
00:15:22,690 --> 00:15:25,550
That is a tested extracted from code PCA.

226
00:15:25,650 --> 00:15:28,400
So test it here and that should be OK.

227
00:15:28,540 --> 00:15:32,890
So I'm going to select these two lines here and execute.

228
00:15:33,130 --> 00:15:34,040
Perfect.

229
00:15:34,190 --> 00:15:36,410
Our tests tested PCA is created.

230
00:15:36,580 --> 00:15:37,860
Let's have a quick check.

231
00:15:37,870 --> 00:15:39,130
So that's the test set.

232
00:15:39,460 --> 00:15:45,480
And that's our test said PCa with the two new extracted features and the purchase column.

233
00:15:45,530 --> 00:15:49,180
And now that means that we correctly applied Colonel PCA.

234
00:15:49,180 --> 00:15:51,960
So great we are ready to move on to the next section.

235
00:15:52,140 --> 00:15:59,590
So let's go back to our current PCA our file and let's now fix the logistic regression to the trainset.

236
00:15:59,590 --> 00:16:02,200
Now do we need to change anything in this code section.

237
00:16:02,320 --> 00:16:04,540
Well yes of course we do because.

238
00:16:04,570 --> 00:16:05,460
Be careful.

239
00:16:05,470 --> 00:16:09,670
We called our new extracted trainset Chernin said PCA.

240
00:16:09,790 --> 00:16:14,820
So here for the data arguments we need to specify training said PCA.

241
00:16:14,830 --> 00:16:16,510
So that's the only thing we need to change here.

242
00:16:16,510 --> 00:16:20,830
So we are ready to select the section and execute.

243
00:16:20,830 --> 00:16:22,290
All right CAS very ready.

244
00:16:22,450 --> 00:16:26,150
And now let's move on to the next section relating to test results.

245
00:16:26,320 --> 00:16:33,640
And of course here that's the same we had to replace test set by test set PCA mean to an artist a little

246
00:16:33,640 --> 00:16:34,500
bit.

247
00:16:34,690 --> 00:16:35,400
Right.

248
00:16:35,950 --> 00:16:36,620
And that's it.

249
00:16:36,660 --> 00:16:40,810
We are ready to execute the section to predict the results.

250
00:16:41,080 --> 00:16:41,920
And here we go.

251
00:16:41,920 --> 00:16:46,730
We get our vector of predictions why pred for this new test set this year.

252
00:16:47,020 --> 00:16:49,330
Right now let's make the confusion matrix.

253
00:16:49,330 --> 00:16:53,570
We of course need to change test set by test said PCA.

254
00:16:53,900 --> 00:16:54,620
Here we go.

255
00:16:54,640 --> 00:16:55,630
And now it's ready.

256
00:16:55,720 --> 00:17:01,180
Now we can execute this line of code to get the confusion matrix.

257
00:17:01,180 --> 00:17:08,620
And here it is we can have a quick look and the council by person see him enter and we get 57 plus 26

258
00:17:08,710 --> 00:17:10,120
equals 83.

259
00:17:10,210 --> 00:17:16,030
And since we have 100 observations in a test set that gives us an 83 percent accuracy.

260
00:17:16,070 --> 00:17:17,170
So that's pretty good.

261
00:17:17,350 --> 00:17:19,210
And now let's get to the exciting part.

262
00:17:19,240 --> 00:17:23,500
Visualizing the trends that results so very quickly what do we need to change.

263
00:17:23,500 --> 00:17:27,640
Remember we need to change the names of the independent variables and call names here.

264
00:17:27,640 --> 00:17:28,820
That's compulsory.

265
00:17:28,960 --> 00:17:34,590
So as a reminder the names are the one and the two that's the name of the independent variables.

266
00:17:34,600 --> 00:17:41,890
So here we need to replace age by one and estimated salary by V-2.

267
00:17:42,280 --> 00:17:44,540
And that's not compulsory.

268
00:17:44,540 --> 00:17:46,650
And anyway we already have two good names.

269
00:17:46,650 --> 00:17:48,000
P.s. one NPC too.

270
00:17:48,160 --> 00:17:49,580
So don't forget about that.

271
00:17:49,660 --> 00:17:54,700
And of course we need to change the name of the training set because we got our training set training

272
00:17:54,700 --> 00:17:55,900
said PCA.

273
00:17:55,930 --> 00:17:58,440
So here I'm adding trainset PCA.

274
00:17:58,690 --> 00:17:59,770
And that's perfect.

275
00:17:59,770 --> 00:18:01,540
That's ready to be executed.

276
00:18:01,540 --> 00:18:03,470
To visualize the trainset results.

277
00:18:03,730 --> 00:18:05,970
So we will only visualize the trends that result.

278
00:18:05,980 --> 00:18:11,310
But let's make the same changes for the test sets so that you can have a look at it yourself.

279
00:18:11,320 --> 00:18:18,160
So same we're replacing age by the one estimated salary by V-2.

280
00:18:18,160 --> 00:18:23,040
And here we replace tested by tests that underscore PCA.

281
00:18:23,410 --> 00:18:23,860
All right.

282
00:18:23,870 --> 00:18:25,060
Now let's have a look.

283
00:18:25,060 --> 00:18:27,100
I look forward to show you what's going to happen.

284
00:18:27,130 --> 00:18:32,890
So I'm going to select everything from here up to the top here that is the whole section to visualize

285
00:18:32,890 --> 00:18:34,150
the trends that result.

286
00:18:34,180 --> 00:18:37,810
And let's press command control us enter to execute.

287
00:18:37,810 --> 00:18:38,560
Here we go.

288
00:18:38,590 --> 00:18:42,870
The computer patients are being run.

289
00:18:42,960 --> 00:18:43,320
All right.

290
00:18:43,320 --> 00:18:50,000
So these are the results of Colonel PCA combined to a logistic regression model that we apply on a non-linear

291
00:18:50,010 --> 00:18:51,800
separable data set.

292
00:18:52,020 --> 00:18:57,540
And so we can appreciate the contrast between the simplicity of the obtained results and the complexity

293
00:18:57,540 --> 00:19:02,820
of what happened behind the scenes because indeed we have this very simple result here with these two

294
00:19:02,820 --> 00:19:08,340
classes separated by a straight line with what happened behind the scenes is that our original data

295
00:19:08,340 --> 00:19:15,030
set in our original feature space was mapped to a higher dimension using the kernel trick to avoid to

296
00:19:15,030 --> 00:19:17,120
highly compute intensive computation.

297
00:19:17,310 --> 00:19:22,950
And then by mapping our data set in the original feature space to this high dimension well for us that

298
00:19:22,950 --> 00:19:24,550
created some new dimensions.

299
00:19:24,630 --> 00:19:30,520
And mostly that created a new feature space where our data was then linearly separable.

300
00:19:30,780 --> 00:19:34,950
But by doing that we had more dimensions than the original number of dimensions.

301
00:19:34,950 --> 00:19:40,470
So we still needed to apply the PCA dimensionality reduction technique to end up with a lower number

302
00:19:40,470 --> 00:19:41,620
of dimensions.

303
00:19:41,640 --> 00:19:46,950
So then PCa was applied to this new feature space where the data was linearly separable and through

304
00:19:47,430 --> 00:19:52,950
some you extracted in the been available were created there are nothing else than the principal components

305
00:19:53,040 --> 00:19:53,860
of PCa.

306
00:19:54,000 --> 00:20:00,450
And eventually we obtain this new feature space formed by these two new extracted principal components

307
00:20:00,660 --> 00:20:07,110
resulting from PCa in which now our data is linearly separable and much better separated by a linear

308
00:20:07,110 --> 00:20:08,140
classifier.

309
00:20:08,550 --> 00:20:10,690
All right so that's it for kernel PCA.

310
00:20:10,770 --> 00:20:16,420
And that's also the end of this part dimensionality reduction and I'll see you in the next part part

311
00:20:16,520 --> 00:20:22,650
and moral selection and boosting the last part of this courseware will cover a very exciting algorithm

312
00:20:22,650 --> 00:20:25,820
in machine learning that is called Extreme boost.

313
00:20:25,830 --> 00:20:28,260
So I look forward to seeing you in this next part.

314
00:20:28,260 --> 00:20:30,170
And until then enjoyer machine learning.
