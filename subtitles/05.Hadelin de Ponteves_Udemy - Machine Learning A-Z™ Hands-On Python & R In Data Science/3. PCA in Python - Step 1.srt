1
00:00:00,210 --> 00:00:06,480
Hello, my friends, and welcome to this new practical activity on this time dimensionality reduction,

2
00:00:06,780 --> 00:00:13,110
which is not a branch of machinery, per say, but a very important technique to know how to handle

3
00:00:13,200 --> 00:00:18,030
when you work with big datasets, you know, huge datasets with many features.

4
00:00:18,270 --> 00:00:23,440
And you know, for which you would like to reduce to complexity by reducing the dimensionality.

5
00:00:23,460 --> 00:00:26,760
And this is exactly what dimensionality reduction is about.

6
00:00:27,360 --> 00:00:33,240
So in this new part, part nine dimensionality reduction, we will build three different models that

7
00:00:33,240 --> 00:00:34,830
can perform such a task.

8
00:00:35,100 --> 00:00:41,760
These are first principal component analysis, the most famous one, then linear discriminant analysis.

9
00:00:42,090 --> 00:00:43,320
And finally, Colonel.

10
00:00:44,190 --> 00:00:47,010
So we will build these three models, one for each section.

11
00:00:47,340 --> 00:00:52,140
And now we're about to start with the first one principal component analysis.

12
00:00:52,800 --> 00:00:56,190
But before we start, let's just make sure everyone here is on the same page.

13
00:00:56,520 --> 00:00:59,930
I gave you the link to this for the right before this material in the articles.

14
00:00:59,970 --> 00:01:01,290
And make sure to connect to it.

15
00:01:01,590 --> 00:01:07,230
And now we should be on the same page or we're gonna go into part nine dimensionality reduction.

16
00:01:08,070 --> 00:01:08,490
All right.

17
00:01:08,550 --> 00:01:12,060
And as I told you, you have the three sections corresponding to each of the models.

18
00:01:12,120 --> 00:01:15,970
And we're going to start with principal component analysis SPCA.

19
00:01:16,860 --> 00:01:18,840
And as usual, we're gonna start with Python.

20
00:01:19,110 --> 00:01:22,320
And then despite and for there, you will find two fouls as usual.

21
00:01:22,560 --> 00:01:29,210
First, the implementation in IPY in B format, which now once again, we will be able to run on Google

22
00:01:29,220 --> 00:01:32,460
collaboratory because we will work with a classic dataset.

23
00:01:32,760 --> 00:01:35,540
And speaking of which, here is a dataset wine.

24
00:01:35,650 --> 00:01:36,500
That's GSV.

25
00:01:36,900 --> 00:01:40,740
So let's open it and let me explain what this is about.

26
00:01:41,330 --> 00:01:41,480
All right.

27
00:01:41,520 --> 00:01:45,360
So actually, first you notice indeed that we have many features.

28
00:01:45,660 --> 00:01:50,360
I did not take a dataset with hundreds of features because then we would, you know, get lost in dataset.

29
00:01:50,790 --> 00:01:53,550
So I just took a dataset with more than 10 features.

30
00:01:53,700 --> 00:01:58,530
And of course, these are all the features from here, alcohol to this one pro line.

31
00:01:58,920 --> 00:02:05,100
And as you can guess, each feature gives a certain information of a certain wine, right.

32
00:02:05,340 --> 00:02:08,040
Each row corresponds to a wine.

33
00:02:08,310 --> 00:02:14,490
And for each wine, we have diverse information, diverse features, characteristics of the wine, the

34
00:02:14,520 --> 00:02:16,820
alcohol level, the malic acid.

35
00:02:16,890 --> 00:02:21,210
I'm not an expert of wines, but these are some wines characteristics.

36
00:02:21,700 --> 00:02:22,150
Ash.

37
00:02:22,300 --> 00:02:22,560
Ash.

38
00:02:22,610 --> 00:02:23,370
L. Kennedy.

39
00:02:23,370 --> 00:02:26,790
Magnesium 2s or Fennell's flavonoids anyway.

40
00:02:27,000 --> 00:02:30,990
So you see, you have many wine features and for each of these wines.

41
00:02:31,260 --> 00:02:32,190
Well, there we go.

42
00:02:32,220 --> 00:02:35,850
I'm about to explain the dependent variable for each of these wines.

43
00:02:36,090 --> 00:02:38,730
We have the customer segment.

44
00:02:38,950 --> 00:02:42,750
You know, that's the last column to which the wines belong.

45
00:02:43,020 --> 00:02:46,660
Okay, so let me explain what happened in terms of business.

46
00:02:46,680 --> 00:02:50,750
First of all, this is a data set I took from the UCI and male representatives.

47
00:02:50,750 --> 00:02:54,780
So all the credits go, of course, to this amazing platform of data set.

48
00:02:55,110 --> 00:03:01,200
However, in this dataset, I just changed the last column customer segment to make it more business

49
00:03:01,200 --> 00:03:05,850
wise, you know, to make this case any more a business case study, because a scenario is the following.

50
00:03:06,300 --> 00:03:12,930
Let's imagine that this dataset belongs to a wine merchant with many different bottles of wine to sell

51
00:03:12,930 --> 00:03:15,090
and therefore a large base of customers.

52
00:03:15,480 --> 00:03:23,400
And this wine shop owner actually hired you, as do scientists, to first do a preliminary work of clustering,

53
00:03:23,490 --> 00:03:29,790
meaning that at first we had all these features without this last column customer segment.

54
00:03:30,120 --> 00:03:32,970
We have all these features from alcohol to pro line.

55
00:03:33,330 --> 00:03:39,690
And this wine shop owner actually asked you to perform some clustering to identify diverse segments

56
00:03:39,780 --> 00:03:45,450
of customers grouped by similarities which correspond to the wines they prefer.

57
00:03:45,570 --> 00:03:45,870
All right.

58
00:03:45,900 --> 00:03:47,940
So each customer segment here.

59
00:03:48,240 --> 00:03:50,100
And by the way, there are three of them, right?

60
00:03:50,100 --> 00:03:55,620
If we scroll down, we can see that we have three different categories or, you know, clusters.

61
00:03:55,980 --> 00:04:02,670
And each of these segments, well, correspond to a certain group of customers that have similar preferences

62
00:04:02,730 --> 00:04:04,140
for similar wines.

63
00:04:04,230 --> 00:04:06,150
And that's exactly what these segments are about.

64
00:04:06,480 --> 00:04:07,610
But that was the first work.

65
00:04:07,620 --> 00:04:11,220
And if you want, you can have fun and do this first work yourself.

66
00:04:11,550 --> 00:04:14,280
But here we want to work on dimensionality reduction.

67
00:04:14,280 --> 00:04:18,930
So there goes the second mission that this wine shop owner asked you to do.

68
00:04:19,410 --> 00:04:23,640
This wine shop owner was actually satisfied with your first work, you know, identifying these three

69
00:04:23,640 --> 00:04:24,120
segments.

70
00:04:24,420 --> 00:04:30,120
But now the owner would like to, you know, reduce the complexity of this dataset by ending up with

71
00:04:30,210 --> 00:04:31,650
a smaller amount of features.

72
00:04:31,920 --> 00:04:37,740
And at the same time, this owner would like you to build a predictive model that will be trained on

73
00:04:37,740 --> 00:04:44,040
this data, you know, including the feature is up to here and the dependent variable so that for each

74
00:04:44,040 --> 00:04:51,260
new wine that this owner has in its Schupp, well, we can deploy this predictive model applied to a

75
00:04:51,260 --> 00:04:58,620
reduced dimensionality data set to predict which customer segment this new wine belongs to.

76
00:04:58,860 --> 00:04:59,160
Right.

77
00:04:59,430 --> 00:04:59,810
And def.

78
00:04:59,930 --> 00:05:04,010
For once, we managed to predict which customer segment this one belongs to.

79
00:05:04,340 --> 00:05:08,690
Then we can recommend this wine to the right customers.

80
00:05:08,990 --> 00:05:14,570
And that's exactly why what we're about to do is like a recommender system, because for each new wine

81
00:05:14,720 --> 00:05:15,860
that will be in the shop.

82
00:05:16,100 --> 00:05:22,700
Well, our predictive model will tell us to which customer segment it will be the most appropriate.

83
00:05:22,760 --> 00:05:24,940
You know, it will be the most appreciated.

84
00:05:25,550 --> 00:05:25,820
All right.

85
00:05:25,850 --> 00:05:27,290
So that's the business case theory.

86
00:05:27,290 --> 00:05:32,450
And therefore, you know, our predictive model will add tons of value to this owner because therefore,

87
00:05:32,750 --> 00:05:38,150
if this owner manages to build a good recommender system, of course, it will optimize the sales and

88
00:05:38,150 --> 00:05:40,640
therefore the profit of the business.

89
00:05:41,150 --> 00:05:41,600
Okay.

90
00:05:41,660 --> 00:05:43,550
So that's what the case study is about.

91
00:05:43,640 --> 00:05:46,940
Now we're going to move on to the implementation, of course.

92
00:05:47,360 --> 00:05:53,330
Therefore, I'm opening this file principal component analysis, which you had a choice open with either

93
00:05:53,330 --> 00:05:58,340
Google Collaboratory or chapter in the book, as we did in the previous section on CNN.

94
00:05:58,670 --> 00:05:59,390
But there we go.

95
00:05:59,640 --> 00:06:05,360
Let's open it with Google collaboratory and enjoy a brand new implementation on it.

96
00:06:06,470 --> 00:06:06,780
All right.

97
00:06:06,830 --> 00:06:10,100
So here is the implementation principal component analysis.

98
00:06:10,400 --> 00:06:11,710
This is an read only mode.

99
00:06:11,720 --> 00:06:17,330
So as usual, we will create a copy by clicking file here and then save a copy in drive.

100
00:06:17,690 --> 00:06:24,320
This will create a copy inside which we will be able to re implement, not the whole implementation

101
00:06:24,320 --> 00:06:29,570
this time, because I will explain that most of the sales are sales we already did before.

102
00:06:29,750 --> 00:06:34,220
You know, many times in the Classification Board and also in the first section of Part eight.

103
00:06:34,490 --> 00:06:36,870
So we won't have to reimplement everything.

104
00:06:36,890 --> 00:06:42,290
This would be a waste of time and mostly we rather want to focus on dimensionality reduction.

105
00:06:42,680 --> 00:06:45,080
And therefore, here is what we're gonna do.

106
00:06:45,200 --> 00:06:47,090
I'm going to show you the implementation, of course.

107
00:06:47,120 --> 00:06:53,730
But the only cell that we will reimplement will be this one applying PCI.

108
00:06:53,840 --> 00:06:55,790
So let's remove it right away.

109
00:06:55,970 --> 00:06:56,710
Not the Turkcell.

110
00:06:56,750 --> 00:06:57,350
Only this one.

111
00:06:57,770 --> 00:07:03,530
And now I'm going to show you that indeed, you know, all the cells are super familiar to us.

112
00:07:03,680 --> 00:07:04,010
Right.

113
00:07:04,310 --> 00:07:08,930
Because indeed, we start by empowering the libraries that we did 100 times.

114
00:07:09,040 --> 00:07:09,370
Right.

115
00:07:09,410 --> 00:07:11,420
We had the three essential libraries here.

116
00:07:11,720 --> 00:07:18,440
Then we import the dataset with the exact same code as the one you have in your data pre pricing template.

117
00:07:18,770 --> 00:07:24,430
So, of course, here I just put the right name of the dataset, which is wine that CSP.

118
00:07:25,080 --> 00:07:25,480
Okay.

119
00:07:25,850 --> 00:07:32,180
Then you will recognize the next steps of the data Bressington plate, which is to split the dataset

120
00:07:32,210 --> 00:07:35,450
into the training set and test set exactly the same code.

121
00:07:35,780 --> 00:07:40,070
Then we apply fichus scaling as it is, you know, most of the time recommended.

122
00:07:40,100 --> 00:07:47,520
So we applied, of course, on separately the training set and a test set and that closes the data pricing

123
00:07:47,540 --> 00:07:47,990
phase.

124
00:07:48,350 --> 00:07:51,220
Then we apply this year and that's of course, the sale.

125
00:07:51,310 --> 00:07:53,210
We will reimplement together.

126
00:07:53,630 --> 00:07:57,240
Then let me just remove all the outputs here so that you don't see them.

127
00:07:57,260 --> 00:07:59,750
I hope you close your eyes when I just remove them.

128
00:07:59,900 --> 00:08:01,640
But there you now close your eyes a little bit.

129
00:08:01,700 --> 00:08:07,100
I'm going to remove that output as well, because actually the dimensionality reduction technique that

130
00:08:07,100 --> 00:08:11,810
we'll use will manage to get us great results with only two extracted features.

131
00:08:11,990 --> 00:08:12,320
Right.

132
00:08:12,410 --> 00:08:15,440
We're not reducing the number of existing features.

133
00:08:15,770 --> 00:08:20,720
We are creating new extracted features based on these existing features.

134
00:08:20,750 --> 00:08:26,360
So we will get totally different new features at the end, which we call no principal components.

135
00:08:26,450 --> 00:08:30,380
So we'll have principal component one and principal component to add.

136
00:08:31,130 --> 00:08:32,270
But there we go.

137
00:08:32,360 --> 00:08:35,090
So back to our implementation.

138
00:08:35,540 --> 00:08:41,090
After applying BCA, which we will redo together, well, we train the logistic regression model on

139
00:08:41,090 --> 00:08:41,750
the training set.

140
00:08:42,140 --> 00:08:46,940
I chose the logistic regression model as the first model of our classification toolkit.

141
00:08:47,210 --> 00:08:49,160
But I could have chosen any other ones.

142
00:08:49,430 --> 00:08:51,890
But you will see that we'll get great results with this one.

143
00:08:51,950 --> 00:08:54,680
But feel free to choose another classification model.

144
00:08:54,980 --> 00:08:56,000
Any will work.

145
00:08:56,270 --> 00:09:02,810
But notice that it is important to apply PRCA before training your classification model on the training

146
00:09:02,810 --> 00:09:03,050
set.

147
00:09:03,390 --> 00:09:03,700
Right.

148
00:09:03,770 --> 00:09:10,460
You want to reduce to dimensionality of your data set before, of course, training it on your training

149
00:09:10,460 --> 00:09:14,810
set right to training said basically is the final version of your data.

150
00:09:14,930 --> 00:09:19,820
After you performed all the data repricing phase and dimensionality reduction if you want.

151
00:09:20,000 --> 00:09:20,310
Okay.

152
00:09:20,720 --> 00:09:24,710
So the training happens after applying your dimensionality reduction technique.

153
00:09:25,160 --> 00:09:25,880
And then of course.

154
00:09:25,910 --> 00:09:28,250
Well, we will make the comparison matrix.

155
00:09:28,400 --> 00:09:29,390
You know how to do that.

156
00:09:29,450 --> 00:09:30,530
We did it many times.

157
00:09:30,770 --> 00:09:36,680
And then since our dimensionality reduction technique will get us great results with only two extracted

158
00:09:36,680 --> 00:09:40,030
features, principal component one and principal component two.

159
00:09:40,370 --> 00:09:44,360
Well, that will allow us to visualize a transit result in two dimensions.

160
00:09:44,510 --> 00:09:44,730
Right.

161
00:09:44,750 --> 00:09:48,560
Because remember that each dimension corresponds to one feature.

162
00:09:49,040 --> 00:09:51,620
And we do this for the training set right here.

163
00:09:51,740 --> 00:09:52,850
And the test.

164
00:09:53,620 --> 00:09:53,940
Okay.

165
00:09:54,320 --> 00:09:59,750
So as you can see, what I did with this implementation is something you can do in less than five.

166
00:09:59,890 --> 00:10:06,030
But right now, thanks to your toolkits, right, because you just need to take the be pressing to get

167
00:10:06,150 --> 00:10:11,640
to make these four cells, then you just need to grab the features going to in your data prepossessing

168
00:10:11,640 --> 00:10:12,150
toolkit.

169
00:10:12,540 --> 00:10:17,670
Then you just need to grab your logistic regression implementation to implement this cell.

170
00:10:17,880 --> 00:10:22,340
And same for the other ones, you know, the computer matrix and same for these last two, visualizing

171
00:10:22,350 --> 00:10:24,750
the transit result and visualizing the test results.

172
00:10:25,080 --> 00:10:29,660
These are all cells that you have in your logistic regression implementation.

173
00:10:29,940 --> 00:10:32,250
So absolutely no need to do it together again.

174
00:10:32,430 --> 00:10:38,180
And therefore, we can now focus directly on this cell applying BCA.

175
00:10:38,730 --> 00:10:39,440
So there we go.

176
00:10:39,450 --> 00:10:41,250
We're going to create a new code cell.

177
00:10:41,370 --> 00:10:46,020
And now let's implement PTA principal component analysis.

178
00:10:46,800 --> 00:10:47,130
All right.

179
00:10:47,160 --> 00:10:54,300
So you could almost press pause on the video now and get the right tool from the Cycler API to see how

180
00:10:54,300 --> 00:10:55,170
to implement this.

181
00:10:55,530 --> 00:10:56,910
That would be a good exercise.

182
00:10:57,060 --> 00:10:58,830
But if you don't want to do it, it's fine.

183
00:10:58,960 --> 00:11:00,480
Let us implement this right now.

184
00:11:00,690 --> 00:11:07,140
And as you guess, by what I've just said, well, we're going to implement PTA using the Cycler Library.

185
00:11:07,500 --> 00:11:14,130
So the first thing we'll do is start from Saikat, learn from which we're gonna get access to a certain

186
00:11:14,130 --> 00:11:20,670
module, which you will find in the Sackler and API and which is called decomposed Xitian, just either

187
00:11:20,700 --> 00:11:26,850
decomposition from which we're going to import, of course, a class that will allow us to build this

188
00:11:27,060 --> 00:11:33,840
object, which will be nothing else than this T.S.A. tool that will apply dimensionality reduction on

189
00:11:33,840 --> 00:11:34,540
our dataset.

190
00:11:34,920 --> 00:11:37,730
And that class is called very simply, P.

191
00:11:38,600 --> 00:11:38,930
OK.

192
00:11:39,240 --> 00:11:42,030
So you can't miss it in the API BCA.

193
00:11:42,420 --> 00:11:48,090
And now next natural step is, of course, to create an object or, you know, an instance of this class.

194
00:11:48,390 --> 00:11:50,460
And guess how we're gonna call that object?

195
00:11:50,640 --> 00:11:53,280
Well, very simply, we're going to call it object pizza.

196
00:11:53,880 --> 00:11:54,120
Right.

197
00:11:54,150 --> 00:11:55,920
So this is super intuitive.

198
00:11:56,610 --> 00:12:04,020
And now, you know, the next step, next step is to call the pizza a class which needs to take one

199
00:12:04,050 --> 00:12:05,190
essential argument.

200
00:12:05,370 --> 00:12:10,050
You know, we only have to input one argument here, and you can totally guess what this argument will

201
00:12:10,050 --> 00:12:10,900
be, right?

202
00:12:11,400 --> 00:12:14,850
It is the final number of extracted features.

203
00:12:15,180 --> 00:12:17,840
You want to end up with in your new dataset.

204
00:12:18,090 --> 00:12:22,440
And that argument to choose that number is called an underscore.

205
00:12:22,620 --> 00:12:25,320
Components and components.

206
00:12:26,100 --> 00:12:26,430
All right.

207
00:12:26,520 --> 00:12:29,880
So now the question is, of course, which number should we choose?

208
00:12:30,000 --> 00:12:30,330
Right.

209
00:12:30,420 --> 00:12:33,360
How do we know down to which number of features?

210
00:12:33,540 --> 00:12:33,810
Right.

211
00:12:33,840 --> 00:12:34,670
Extract features.

212
00:12:34,680 --> 00:12:37,380
We want to reduce dimensionality of our dataset.

213
00:12:37,950 --> 00:12:40,410
Well, I have a very simple answer to that question.

214
00:12:40,590 --> 00:12:44,490
What I usually do is start with two, you know, two principal components.

215
00:12:44,520 --> 00:12:48,120
Therefore, to extract it features and see the results, I get Indian.

216
00:12:48,360 --> 00:12:50,320
And thanks to our code, you know, our code template.

217
00:12:50,400 --> 00:12:52,800
We can check that very quickly and easily.

218
00:12:53,190 --> 00:12:58,170
And besides, we do want to try with two, because then if we get good results with two, well, we

219
00:12:58,170 --> 00:13:03,510
will be able to visualize a transit result and test it result in two dimensions, you know, in this

220
00:13:03,510 --> 00:13:06,840
nice plot that we had in part three classification.

221
00:13:07,290 --> 00:13:09,030
So we definitely want to start with two.

222
00:13:09,150 --> 00:13:15,030
And if, you know, we get really poor results and we see on the graphics here that we can't separate

223
00:13:15,120 --> 00:13:16,500
the three classes properly.

224
00:13:16,620 --> 00:13:20,280
You know, remember with those different prediction regions and the prediction boundary.

225
00:13:20,700 --> 00:13:25,680
Well, if we see that we have poor results on the visualizations, then we can try with higher numbers

226
00:13:25,740 --> 00:13:28,350
of principal components, meaning three than four.

227
00:13:28,590 --> 00:13:33,750
And at some point we'll get, you know, some extracted features that explain well enough the variance,

228
00:13:34,110 --> 00:13:36,270
which is exactly what PCI is about.

229
00:13:36,360 --> 00:13:36,620
Right.

230
00:13:36,630 --> 00:13:40,950
It is about extracting some features that explain well enough the variance.

231
00:13:41,040 --> 00:13:46,140
And once you find them, well, you will get good results even with a lower dimensionality.

232
00:13:46,470 --> 00:13:49,260
Okay, so let's try was two and let's see what we'll get.

233
00:13:49,290 --> 00:13:52,440
But I already told you that we'll get amazing results.

234
00:13:52,980 --> 00:13:53,910
Therefore, there you go.

235
00:13:54,030 --> 00:13:57,990
And components equals two two principal components.

236
00:13:58,110 --> 00:14:00,540
Or in other words, two extracted features.

237
00:14:01,280 --> 00:14:01,580
Okay.

238
00:14:01,640 --> 00:14:02,770
So that's four object.

239
00:14:02,910 --> 00:14:10,260
And now the next step, of course, is to apply this object to our training set to reduce the dimensionality

240
00:14:10,260 --> 00:14:15,360
of our training set in order to ease the learning process of the logistic regression model.

241
00:14:15,810 --> 00:14:22,320
But also, we will have to apply it on the test set, because remember that the predict method that

242
00:14:22,320 --> 00:14:29,460
we will call here has to be called on the exact same format of data as the one that was used for the

243
00:14:29,460 --> 00:14:30,090
training set.

244
00:14:30,450 --> 00:14:36,120
So as long as you apply some transformations like data preprocessing or dimensionality reduction on

245
00:14:36,120 --> 00:14:39,570
your training set, well, you have to do the same on your test set.

246
00:14:39,960 --> 00:14:43,500
However, be careful exactly as feature scaling.

247
00:14:43,680 --> 00:14:50,730
We will have to apply the fit transform method on the training set, but only the transform method on

248
00:14:50,730 --> 00:14:51,330
the test set.

249
00:14:51,660 --> 00:14:56,970
And that's always for the same reason that because we want to avoid information like it on the test

250
00:14:56,970 --> 00:14:57,270
set.

251
00:14:57,540 --> 00:14:57,810
Right.

252
00:14:57,840 --> 00:14:59,370
The test set is supposed to be new.

253
00:14:59,650 --> 00:15:05,860
Observations like data on which we deploy our model in production and therefore we're not supposed to

254
00:15:05,860 --> 00:15:10,960
fit our scalar or, you know, feature extractor objects on the test set.

255
00:15:11,110 --> 00:15:12,940
We can apply them to transform them.

256
00:15:13,060 --> 00:15:14,950
Right, because they were fitted on a train set.

257
00:15:15,160 --> 00:15:20,200
But we can't fit them again to the test head because that would be like trying to get some hints of

258
00:15:20,200 --> 00:15:23,380
information from the test set that we're not supposed to have.

259
00:15:23,470 --> 00:15:25,720
That's exactly what information leakage is about.

260
00:15:26,160 --> 00:15:26,910
So there you go.

261
00:15:26,950 --> 00:15:27,750
I said everything.

262
00:15:27,760 --> 00:15:33,130
Now you can press pause on this video to finish this implementation of BCA.

263
00:15:33,550 --> 00:15:37,210
And in two seconds, I'm going to implement with you the solution.

264
00:15:40,190 --> 00:15:40,640
All right.

265
00:15:40,970 --> 00:15:41,900
I hope you did well.

266
00:15:42,050 --> 00:15:43,310
Now let's do it together.

267
00:15:43,400 --> 00:15:48,150
So, as we said, we want to play this piece of objects separately on the strings in two sets of first.

268
00:15:48,260 --> 00:15:50,660
I'm going to take X train.

269
00:15:51,180 --> 00:15:51,680
All right.

270
00:15:51,710 --> 00:16:00,560
Which I'm going to update by applying this piece, a object from which I'm going to call it fit transform

271
00:16:01,400 --> 00:16:08,050
method on this all the version of X train, meaning before the transformation of PTA.

272
00:16:08,390 --> 00:16:13,910
And so here what happens technically is that the fit part of this fit transfer method will get all the

273
00:16:13,910 --> 00:16:18,380
information it needs from X train to apply principal component analysis.

274
00:16:18,710 --> 00:16:24,230
And then, of course, the transformed part of this fit transfer method will apply the transformation

275
00:16:24,230 --> 00:16:27,380
itself to extract the principal component features.

276
00:16:27,560 --> 00:16:30,620
OK, so that what it means technically and now.

277
00:16:30,980 --> 00:16:38,390
Well, that's the same actually for excess on copying this pasting in here and replacing here X train

278
00:16:38,390 --> 00:16:47,750
by excessed, then extra in here again by X test and only applying the transform method.

279
00:16:48,300 --> 00:16:49,580
And there we go my friends.

280
00:16:49,820 --> 00:16:52,610
This implementation is already over.
