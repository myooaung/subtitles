WEBVTT
1
00:00:00.270 --> 00:00:00.940
All right.

2
00:00:00.960 --> 00:00:02.530
Are you ready for the final step.

3
00:00:02.730 --> 00:00:05.550
Visualizing the results of the SVR model.

4
00:00:05.670 --> 00:00:10.920
So we're gonna basically do the same thing as what we did before you know with the simple in our regression

5
00:00:10.920 --> 00:00:13.500
model and the polynomial regression model.

6
00:00:13.560 --> 00:00:17.700
And since we actually did it before we're not going to do it all over again.

7
00:00:17.730 --> 00:00:23.460
You know the whole implementation of the code that plots the graph instead I have a much better idea

8
00:00:23.520 --> 00:00:31.020
which will actually be a great exercise for you to practice what was just explained before with this

9
00:00:31.080 --> 00:00:32.310
inverse transform method.

10
00:00:32.700 --> 00:00:40.500
So the idea I have in mind is to take the visualization code of the polynomial regression model which

11
00:00:40.650 --> 00:00:42.300
you know is right the bottom.

12
00:00:42.300 --> 00:00:43.140
Here it is.

13
00:00:43.140 --> 00:00:44.840
No that's the linear regression model.

14
00:00:44.850 --> 00:00:45.840
And here it is.

15
00:00:45.840 --> 00:00:47.850
The polynomial regression result.

16
00:00:47.940 --> 00:00:55.410
So we're going to take this whole code here and we're gonna paste that inside a new code sell here for

17
00:00:55.620 --> 00:00:57.020
the SVR results.

18
00:00:57.150 --> 00:00:58.370
And now here we go.

19
00:00:58.440 --> 00:01:04.740
The exercise I would like you to do is to figure out what needs to be replaced in that code in order

20
00:01:04.740 --> 00:01:07.440
to make it work for the SVR.

21
00:01:07.440 --> 00:01:09.930
And really I have explains everything.

22
00:01:09.930 --> 00:01:11.860
Let me actually zoom out a bit.

23
00:01:11.910 --> 00:01:12.480
Here it is.

24
00:01:12.480 --> 00:01:14.160
I had explains everything.

25
00:01:14.220 --> 00:01:19.800
You know with the transform methods and the inverse transform methods for you to be able to solve this

26
00:01:19.830 --> 00:01:20.560
problem.

27
00:01:20.580 --> 00:01:20.880
All right.

28
00:01:20.880 --> 00:01:29.350
So now please press pause on the video and in a few seconds we will implement a solution together.

29
00:01:29.900 --> 00:01:31.330
Okay let's do this.

30
00:01:31.340 --> 00:01:37.220
I hope you at least tried you know what matters is that you try to practice and if you did not succeed

31
00:01:37.220 --> 00:01:41.360
that's fine because right now we're about to succeed together.

32
00:01:41.420 --> 00:01:41.870
All right.

33
00:01:41.960 --> 00:01:46.160
So let's do this first let's just replace the obvious change.

34
00:01:46.160 --> 00:01:55.130
We're going to replace polynomial here by Support Vector regression or as your as you want.

35
00:01:55.130 --> 00:01:57.910
All right and then let's take it line by line.

36
00:01:57.950 --> 00:02:01.370
This first line here is the line where we scatter blood.

37
00:02:01.370 --> 00:02:07.520
You know the different points of coordinates given by the position levels and the real salaries.

38
00:02:07.610 --> 00:02:14.780
So these red points here will be the real observations containing the real salaries for all the 10 different

39
00:02:14.780 --> 00:02:16.170
position levels.

40
00:02:16.190 --> 00:02:20.120
So here my question is do we need to change anything.

41
00:02:20.150 --> 00:02:23.240
And the answer is Well absolutely yes.

42
00:02:23.270 --> 00:02:24.050
Why is that.

43
00:02:24.050 --> 00:02:30.530
That's because we would like to have a nice chart with the original scales of both the values of our

44
00:02:30.530 --> 00:02:33.740
feature and the values of our dependent variable.

45
00:02:33.740 --> 00:02:38.900
And remember that both X and Y were skilled with two different objects.

46
00:02:38.960 --> 00:02:45.560
X was killed with the SC X scalar and Y was killed with X Y scalar.

47
00:02:45.590 --> 00:02:51.860
So they are both on a certain scale different than the original scale which is this range of values

48
00:02:52.250 --> 00:02:58.160
and therefore now what we would like to do is of course to reverse the scale in order to scatter plot

49
00:02:58.520 --> 00:03:00.360
points in their original scale.

50
00:03:00.360 --> 00:03:06.740
So very simply we simply have to apply the inverse transfer method here on both the matrix of Features

51
00:03:06.770 --> 00:03:09.020
X and the dependent variable vector Y.

52
00:03:09.020 --> 00:03:10.050
So let's do this.

53
00:03:10.070 --> 00:03:11.970
That's our first step here.

54
00:03:11.990 --> 00:03:19.160
We're first gonna call the SC underscore X scalar object from which we're going to call the inverse

55
00:03:19.730 --> 00:03:26.510
transform method which will take hasn't put the whole matrix of features X..

56
00:03:26.510 --> 00:03:27.020
Perfect.

57
00:03:27.080 --> 00:03:32.390
And now let's do the same for Y which is still skilled with a scalar object SC y.

58
00:03:32.390 --> 00:03:33.020
So there you go.

59
00:03:33.020 --> 00:03:39.470
That's what we need to start with a SC on the score y or scalar object for the dependent variable vector.

60
00:03:39.470 --> 00:03:48.450
And from which we have to call again the inverse transform method applied to that skilled vector Y.

61
00:03:48.590 --> 00:03:48.980
Perfect.

62
00:03:48.980 --> 00:03:54.910
So now this will give us the points you know the real points in their original skills.

63
00:03:54.960 --> 00:03:55.480
That's good.

64
00:03:55.490 --> 00:03:58.070
And then we can keep the red color.

65
00:03:58.070 --> 00:03:59.590
That's all good.

66
00:03:59.600 --> 00:04:00.710
Perfect for the first line.

67
00:04:00.710 --> 00:04:01.580
Congratulations.

68
00:04:01.580 --> 00:04:05.330
Now let's move on to the regression curve.

69
00:04:05.330 --> 00:04:05.890
All right.

70
00:04:05.930 --> 00:04:10.190
So here are the first obvious change is we will replace the name of the regressive.

71
00:04:10.190 --> 00:04:14.080
This was for the polynomial regressive which we called Lynn rect too.

72
00:04:14.150 --> 00:04:17.260
And now we called R as we are regressive regressive.

73
00:04:17.420 --> 00:04:21.590
So we're quickly going to replace him Lin rec 2 by regret sir.

74
00:04:21.830 --> 00:04:24.000
First obvious change then a second.

75
00:04:24.020 --> 00:04:27.970
Obvious changes once again to remove all the poorly rag.

76
00:04:27.980 --> 00:04:33.800
You know that was the actually transformation of the matrix of single feature into this matrix of bound

77
00:04:33.800 --> 00:04:34.550
features.

78
00:04:34.550 --> 00:04:41.180
So we have to remove all this actually we're just going to keep X so far and there he goes.

79
00:04:41.180 --> 00:04:47.210
Now we have a clear line with just the predict method called from our aggressor applied to the matrix

80
00:04:47.210 --> 00:04:53.690
of features x and so now the question is what do we need to do in order to get the regression curve

81
00:04:53.990 --> 00:04:55.800
back into the original scale.

82
00:04:55.850 --> 00:05:00.020
So try to figure it out and now I'm going to use solution.

83
00:05:00.470 --> 00:05:06.780
Well let's start with that coordinate you know which is the x coordinates containing the position levels.

84
00:05:06.830 --> 00:05:15.050
Well once again X right now at this stage is still scaled with that scalar object SC X and therefore

85
00:05:15.050 --> 00:05:22.310
we have to reverse the scaling here by applying once again the inverse transform method from r sc x

86
00:05:22.370 --> 00:05:27.380
object and therefore I'm going to copy this because we're gonna do exactly the same here.

87
00:05:27.480 --> 00:05:33.360
We are going to reverse the scale of X to find the original scale.

88
00:05:33.500 --> 00:05:40.250
All right so all good for the X coordinates and now a bit more difficult the y coordinates meaning the

89
00:05:40.250 --> 00:05:42.080
predicted salaries.

90
00:05:42.080 --> 00:05:42.490
Right.

91
00:05:42.500 --> 00:05:47.690
This gave us the real salaries and now we would like to have the predicted salaries on the regression

92
00:05:47.690 --> 00:05:49.880
curve of the SVR model.

93
00:05:49.880 --> 00:05:52.250
So according to you what do we need to do here.

94
00:05:52.370 --> 00:05:57.370
And then this will actually be our final change but yet not the easiest one.

95
00:05:57.410 --> 00:06:02.550
So please take a few seconds to think about this and try to make the right change.

96
00:06:03.020 --> 00:06:05.000
Right so I'm gonna give you a solution now.

97
00:06:05.120 --> 00:06:08.330
Well let's take it step by step logically.

98
00:06:08.420 --> 00:06:11.240
First of all predict X what does it return.

99
00:06:11.390 --> 00:06:14.990
Well it returns indeed as a predicted salaries.

100
00:06:15.110 --> 00:06:20.960
But in the new scale of y you know the new scale of the dependent variable vector and here.

101
00:06:20.960 --> 00:06:24.990
Be careful not to apply the SC X transfer method.

102
00:06:25.040 --> 00:06:29.500
Here we had to apply it because this was the original scale of the feature.

103
00:06:29.720 --> 00:06:33.990
But here X is already skilled with that as X Geller object.

104
00:06:34.100 --> 00:06:37.410
So we don't have to apply this transfer method again.

105
00:06:37.430 --> 00:06:38.090
It's all good.

106
00:06:38.120 --> 00:06:42.440
So X is correct here you know as the input of the predicate method.

107
00:06:42.710 --> 00:06:50.020
But what is not correct is the scale of that predicted salary because indeed as we just said the predict

108
00:06:50.020 --> 00:06:56.530
math we'll return the prediction in the same scale as the one that was used in the training.

109
00:06:56.710 --> 00:07:03.070
And therefore here we need to reverse the scale of that predicted salary in order to find the original

110
00:07:03.070 --> 00:07:06.760
skill of the dependent variable having these values.

111
00:07:06.760 --> 00:07:07.330
All right.

112
00:07:07.480 --> 00:07:15.760
So the only thing we need to do here is to call R S C underscore why object from which we're gonna call

113
00:07:15.880 --> 00:07:24.920
the inverse transform method which will take as input all this right the inverse transfer method will

114
00:07:24.920 --> 00:07:32.810
take as input the predicted salary but in the same scale that was used for the training and therefore

115
00:07:32.810 --> 00:07:37.290
we have to reverse that killing in order to find the original skill of Y.

116
00:07:37.770 --> 00:07:44.300
All right you see the idea we're going to check now that all this is correct you know it's easy to get

117
00:07:44.300 --> 00:07:48.140
confused sometimes but I think I follow the right logic here.

118
00:07:48.140 --> 00:07:49.000
So let's see.

119
00:07:49.040 --> 00:07:55.270
Let's execute this cell here and here are the easy are results.

120
00:07:55.280 --> 00:08:02.090
Congratulations if you managed to do the exercise and also if you tried to do the exercise and now congratulations

121
00:08:02.090 --> 00:08:09.110
for getting this graph showing indeed the regression curve of the as we are morals now we're going to

122
00:08:09.110 --> 00:08:13.820
do the same to get the curve with the higher resolution because I would like to make my comments on

123
00:08:13.820 --> 00:08:15.370
this curve it is much nicer.

124
00:08:15.470 --> 00:08:17.520
So we're going to do the same here.

125
00:08:17.600 --> 00:08:23.510
You're going to do the same exercise with that code which plots that high resolution curve.

126
00:08:23.510 --> 00:08:25.280
So let's get it here.

127
00:08:25.280 --> 00:08:26.840
Here it is.

128
00:08:26.840 --> 00:08:36.740
So copying this and pasting it right here below in a new code cell and now let's quickly replace everything.

129
00:08:36.740 --> 00:08:43.620
So here we're going to replace polynomial by support vector regression.

130
00:08:43.730 --> 00:08:48.320
Then let's replace that obvious regress or by regress sir.

131
00:08:49.100 --> 00:08:49.570
Good.

132
00:08:49.790 --> 00:08:53.960
And then let's get rid of poorly read here with the Fit transfer method.

133
00:08:54.300 --> 00:08:56.090
And the parenthesis end good.

134
00:08:56.240 --> 00:08:57.890
Now let's see what we have to replace.

135
00:08:58.460 --> 00:09:04.520
So first of all we get the grid you know the high density points in the x axis meaning that instead

136
00:09:04.520 --> 00:09:10.700
of having one two three four five six seven eight nine 10 will have 1 1 1 1 one point to one point three

137
00:09:10.990 --> 00:09:15.660
up to nine nine point one nine point two nine point thirty nine point eight nine point nine ten.

138
00:09:15.680 --> 00:09:15.920
Right.

139
00:09:15.920 --> 00:09:17.110
That's what we are creating here.

140
00:09:17.120 --> 00:09:19.010
But when doing this.

141
00:09:19.010 --> 00:09:24.080
The problem is that X here is once again in the scale after the transformation.

142
00:09:24.080 --> 00:09:29.510
Therefore here naturally we have to reverse that scale you know directly inside this main function in

143
00:09:29.510 --> 00:09:30.140
max function.

144
00:09:30.140 --> 00:09:30.740
So here.

145
00:09:30.860 --> 00:09:37.700
Let's quickly reverse that scale of X by calling or see x object from which we're going to call the

146
00:09:37.820 --> 00:09:43.790
inverse transform method which takes that input indeed x.

147
00:09:43.810 --> 00:09:44.320
All right.

148
00:09:44.330 --> 00:09:53.490
And then same here as C on the score X dot inverse transform method applied to X.

149
00:09:53.690 --> 00:09:56.990
And now we're good with the grid then next step.

150
00:09:57.020 --> 00:09:59.300
Well since the grid is now fine.

151
00:09:59.360 --> 00:10:04.660
Well the reshape that we do here in order to get indeed all these high density points is fine.

152
00:10:04.790 --> 00:10:09.890
So we can move on directly to the scatter plot of the real points you know with the real observations

153
00:10:10.190 --> 00:10:12.950
meaning the position levels in the real salaries.

154
00:10:13.100 --> 00:10:13.760
And here.

155
00:10:13.760 --> 00:10:17.510
Well that's pretty obvious based on what we've just done here.

156
00:10:17.510 --> 00:10:23.300
Of course X and Y are in the new scale you know resulting from standardization.

157
00:10:23.390 --> 00:10:31.070
And so here we have to of course reverse that scale first by calling the SC x object from which we call

158
00:10:31.070 --> 00:10:40.310
the inverse transform method to reverse the scale of X and then same we call the SC y object from which

159
00:10:40.310 --> 00:10:45.520
we call the inverse transfer method to reverse the scale of Y.

160
00:10:45.620 --> 00:10:46.160
All right.

161
00:10:46.250 --> 00:10:48.360
So that's for real observations.

162
00:10:48.410 --> 00:10:49.280
All good.

163
00:10:49.310 --> 00:10:55.550
And now let's quickly finish it with that little change we have to make here for the regression curve

164
00:10:55.610 --> 00:11:00.590
in high resolution thanks to this grid here that we made on the x axis.

165
00:11:00.590 --> 00:11:01.250
And so here.

166
00:11:01.250 --> 00:11:07.580
Well as we said X grid is fine it is already back into the right original scale but our predictions

167
00:11:07.580 --> 00:11:14.450
here you know the method is applied to X grid which as we've just said is in the right original scale

168
00:11:14.630 --> 00:11:19.730
and therefore we have to put it back into the scale that was used for the training.

169
00:11:19.730 --> 00:11:27.740
And that's why the little trick here was to not to forget to apply this SC underscore X scalar object

170
00:11:27.920 --> 00:11:37.520
from which we call the transform method in order to put X grid back into the scale resulting from standardization.

171
00:11:37.520 --> 00:11:41.750
Otherwise the predict method would return nonsense predictions.

172
00:11:41.750 --> 00:11:49.040
All right so all this regressive that predict FCX transform makes grid returns indeed the predicted

173
00:11:49.040 --> 00:11:55.580
salaries for all the dance points in the x axis given by X grid but it returns these predictions in

174
00:11:55.790 --> 00:12:00.490
the new scale of Y meaning the one resulting from standardization as well.

175
00:12:00.770 --> 00:12:05.900
So we have to do one final thing here which is of course to reverse that scale and to do this we have

176
00:12:05.900 --> 00:12:13.550
to call R S C underscore why objects from which we're going to call the inverse transfer method applied

177
00:12:13.790 --> 00:12:22.320
to this whole prediction returned by the predict method applied to the transformed excrete All right.

178
00:12:22.370 --> 00:12:23.430
Now it should be fine.

179
00:12:23.440 --> 00:12:24.870
Let's check it out.

180
00:12:24.880 --> 00:12:29.040
We're going to have a much nicer curve now thanks to the high density points.

181
00:12:29.080 --> 00:12:34.390
So let's well first hope I didn't make a mistake but I think it's really fun to play with all these

182
00:12:34.560 --> 00:12:36.780
transform and trends from method.

183
00:12:36.880 --> 00:12:42.310
If you're still a bit confused or overwhelmed by all this I really recommend to try to do it again on

184
00:12:42.310 --> 00:12:47.830
your own because you know we just have to follow a logical process by figuring out in which scale we

185
00:12:47.830 --> 00:12:53.830
are and which transformation we have to do whether it is the direct transformation to scale or the inverse

186
00:12:53.830 --> 00:12:56.430
transformation to go back to the original scale.

187
00:12:56.560 --> 00:12:58.780
So you can totally do this the same on your own.

188
00:12:58.780 --> 00:13:00.220
I think it's actually pretty fun.

189
00:13:00.340 --> 00:13:09.670
And now let's play that cell in order to get well that beautiful regression curve of the support vector

190
00:13:09.670 --> 00:13:17.320
regression model in high resolution and as we can see once again we see that that as we are it's perfectly

191
00:13:17.320 --> 00:13:24.160
well adapted to this kind of data set you know with non-linear relationships because indeed the predictions

192
00:13:24.250 --> 00:13:31.060
on this regression curve come very close to the real results meaning the real salaries except of course

193
00:13:31.060 --> 00:13:31.960
for that one.

194
00:13:31.960 --> 00:13:34.010
And that's good to know about the SVR.

195
00:13:34.150 --> 00:13:40.780
If you have some kind of outlier or you know a point that is far from the majority of your other points.

196
00:13:40.960 --> 00:13:45.040
Well the Support Vector regression model won't catch that properly.

197
00:13:45.040 --> 00:13:45.340
All right.

198
00:13:45.340 --> 00:13:49.240
So it's very interesting to see the pulling on the regression model did amazing here.

199
00:13:49.240 --> 00:13:55.570
You know we had a nice prediction for this last salary but remember that there was some overfishing

200
00:13:55.870 --> 00:14:01.480
because we chose a high degree and so maybe here we have a better learning actually because there is

201
00:14:01.480 --> 00:14:07.120
less overfishing but still it's quite borderline but that's OK because what we wanted after all was

202
00:14:07.120 --> 00:14:12.280
to get a good prediction for the salary of position level number six point five and that was very good

203
00:14:12.280 --> 00:14:13.690
for negotiation.

204
00:14:13.690 --> 00:14:19.180
So we can be very satisfied of the results provided by the SVR moral and mostly.

205
00:14:19.180 --> 00:14:25.410
The important thing to remember is that indeed the SVR morale is great for actually most of the data

206
00:14:25.410 --> 00:14:28.070
sets you know whether they are linear or non-linear.

207
00:14:28.090 --> 00:14:34.870
Because remember when you build you as your model you can actually choose a linear kernel or a nonlinear

208
00:14:34.900 --> 00:14:36.010
kernel you know.

209
00:14:36.040 --> 00:14:42.160
And if you choose a linear kernel well you will get amazing results on a data set with linear relationships.

210
00:14:42.280 --> 00:14:47.380
And if you get a nonlinear kernel like the regional basis function or the polynomial kernel you will

211
00:14:47.380 --> 00:14:52.240
get again amazing results on data sets with non-linear relationships.

212
00:14:52.240 --> 00:14:57.820
Here we have some polynomial relationships between the salary and the position level.

213
00:14:57.820 --> 00:15:00.140
All right so the SVR is an amazing model.

214
00:15:00.160 --> 00:15:06.190
But just be careful of the outliers which are not cut well by the SVR model in some data sets it's fine

215
00:15:06.250 --> 00:15:12.460
but in some other problems you know outliers can be important because they can reveal important insight.

216
00:15:12.460 --> 00:15:13.190
All right.

217
00:15:13.210 --> 00:15:15.150
So congratulations again.

218
00:15:15.160 --> 00:15:17.770
Now we're going to move on to the next section.

219
00:15:17.770 --> 00:15:23.950
So for our lovers who also want to learn Ah well join me in the art tutorials to learn how to implement

220
00:15:23.950 --> 00:15:25.860
the SVR model in R.

221
00:15:25.990 --> 00:15:28.480
And for those who only work on python.

222
00:15:28.480 --> 00:15:34.450
Well join me in the next practical activity where we're going to build together a decision tree regression

223
00:15:34.450 --> 00:15:35.210
model.

224
00:15:35.230 --> 00:15:38.060
I can't wait to move on to this new model.

225
00:15:38.080 --> 00:15:39.910
And until then enjoy machine learning.
