WEBVTT
1
00:00:00.210 --> 00:00:06.300
Hello, my friends, and welcome to the final practical activity of this part, nine dimensionality

2
00:00:06.390 --> 00:00:07.050
reduction.

3
00:00:07.440 --> 00:00:14.130
We already build two dimensionality reduction models, first principal component analysis, and then

4
00:00:14.130 --> 00:00:16.830
second, linear discriminant analysis.

5
00:00:17.280 --> 00:00:23.280
We got amazing results with both but slightly better results and actually perfect results with linear

6
00:00:23.280 --> 00:00:24.420
discriminant analysis.

7
00:00:24.840 --> 00:00:30.750
So now we're hoping that with our third tool of the dimensionality reduction target that we get at least

8
00:00:30.750 --> 00:00:35.010
the same thing as BCA or that same perfect result as with LDA.

9
00:00:35.490 --> 00:00:38.900
And you might guess that since now we're about to add a kernel.

10
00:00:39.120 --> 00:00:44.010
And as we saw with as VM and kernel as VM, adding a kernel always improves the result.

11
00:00:44.310 --> 00:00:47.430
Well, you might guess that we are about to get amazing results as well.

12
00:00:48.120 --> 00:00:48.570
All right.

13
00:00:48.660 --> 00:00:50.880
So let's start let's build that final model.

14
00:00:50.940 --> 00:00:54.060
But before this, let's make sure everyone here is on the same page.

15
00:00:54.120 --> 00:00:57.250
I give you the link to this whole folder right before these tutorials.

16
00:00:57.330 --> 00:00:58.500
Make sure you connect to it.

17
00:00:58.770 --> 00:00:59.790
And now here we go.

18
00:00:59.880 --> 00:01:04.530
Let's enter part nine and then section forty five, kernel BCA.

19
00:01:05.250 --> 00:01:07.230
And as usual, we're gonna start with Python.

20
00:01:07.410 --> 00:01:09.870
And this python for there contains two files.

21
00:01:10.110 --> 00:01:13.920
First, the kernel implementation in the IPY and B format.

22
00:01:14.280 --> 00:01:21.630
And of course, the same data set wine CSP, which is a data set of many wines in many different wines.

23
00:01:21.690 --> 00:01:24.030
Each rogue responds to a wine.

24
00:01:24.330 --> 00:01:29.460
And for each of these wines, we have these features from the alcohol level to the pro line.

25
00:01:29.790 --> 00:01:36.180
And then for each of these wines, we also have the customer segment, which is the segment of customers

26
00:01:36.180 --> 00:01:42.450
to which the wine belongs to in the sense that all the customers of each segment and we have three segments

27
00:01:42.450 --> 00:01:46.110
in total, have the same preference for such wines.

28
00:01:46.170 --> 00:01:46.520
Okay.

29
00:01:46.990 --> 00:01:52.590
And now the challenge is to build a logistic regression model combined to some dimensionality reduction

30
00:01:52.590 --> 00:01:58.740
techniques applied to this dataset so that we can end up with a less complex data set, which at the

31
00:01:58.740 --> 00:02:05.010
same time will provide an excellent way for the logistic regression model to learn the correlations

32
00:02:05.010 --> 00:02:08.530
between all these features and the deep end.

33
00:02:08.530 --> 00:02:09.120
InVivo.

34
00:02:09.450 --> 00:02:09.850
All right.

35
00:02:09.870 --> 00:02:15.810
And then for each new wine, we will deploy this predictive model to predict the customer segment to

36
00:02:15.810 --> 00:02:17.430
which this wine belongs.

37
00:02:17.760 --> 00:02:23.010
So that the owner of the wine shop can recommend each new wine to the right customer.

38
00:02:23.580 --> 00:02:23.910
All right.

39
00:02:23.970 --> 00:02:26.220
That's the exact same case study as before.

40
00:02:26.310 --> 00:02:32.400
And now let's open this implementation with either Google Collaboratory or Dupere notebook.

41
00:02:32.880 --> 00:02:39.270
As you can see, I kept this PCI implementation and this LDA implementation so that we can compare.

42
00:02:39.720 --> 00:02:44.700
And now, well, as usual, this implementation is in read only mode because you are have access to

43
00:02:44.700 --> 00:02:44.850
it.

44
00:02:45.150 --> 00:02:51.150
So let's create a copy by clicking for here, then save a copy and drive, because indeed in this copy

45
00:02:51.150 --> 00:02:55.570
we will reimplement the sale that implements Colonel Pichette.

46
00:02:56.190 --> 00:03:00.720
Let's get rid of this so that we can have clearly the three dimensionality reduction techniques.

47
00:03:01.050 --> 00:03:02.130
And now there we go.

48
00:03:02.250 --> 00:03:04.110
Let's implement Colonel Pichette.

49
00:03:04.230 --> 00:03:09.810
But first, this time, let's upload the data set first so that we can, you know, get the assistance

50
00:03:09.870 --> 00:03:13.410
of Google CoLab right now or notebook is connecting to runtime.

51
00:03:13.440 --> 00:03:13.980
There we go.

52
00:03:14.250 --> 00:03:14.790
Then let's click.

53
00:03:14.910 --> 00:03:19.020
Upload will end up in the linear discriminant analysis folder.

54
00:03:19.260 --> 00:03:21.780
So let's just do the whole path again.

55
00:03:21.930 --> 00:03:24.560
So this is the whole machine learning is at folder.

56
00:03:24.780 --> 00:03:31.440
Let's go inside and let's go to part nine dimensionality reduction and Section 45 current BCA by phone

57
00:03:31.560 --> 00:03:33.930
and why not ASV open.

58
00:03:34.360 --> 00:03:35.970
Okay, there we go.

59
00:03:36.360 --> 00:03:38.410
Now our notebook is connected.

60
00:03:39.200 --> 00:03:39.440
Okay.

61
00:03:39.550 --> 00:03:40.770
And now we're going to do two things.

62
00:03:40.770 --> 00:03:41.070
First.

63
00:03:41.160 --> 00:03:43.660
We're gonna remove that cell.

64
00:03:44.040 --> 00:03:46.720
Put it in the bin so that we can reimplemented.

65
00:03:46.770 --> 00:03:51.090
But also let's, you know, remove all the outputs by trying not to look at them.

66
00:03:51.250 --> 00:03:54.170
You know, I hope I hope you didn't look at the results.

67
00:03:54.210 --> 00:03:57.990
But anyway, I'm sure you expect an amazing result as well.

68
00:03:58.350 --> 00:04:00.930
So let's just remove the output here.

69
00:04:01.020 --> 00:04:03.170
That's the visualization of the change that results.

70
00:04:03.570 --> 00:04:06.390
And this one visualization of the test set result.

71
00:04:06.960 --> 00:04:08.010
All right, then.

72
00:04:08.040 --> 00:04:11.870
Let's take the table of contents applying kernel BCA.

73
00:04:12.150 --> 00:04:12.840
There we go.

74
00:04:12.960 --> 00:04:14.700
We are ready to implement this.

75
00:04:15.180 --> 00:04:16.910
So let's create a new Kotel.

76
00:04:17.040 --> 00:04:20.980
And now do we want to reimplement this?

77
00:04:21.000 --> 00:04:25.080
You know, from the very scratch or do we want to be efficient and.

78
00:04:25.140 --> 00:04:29.520
Well, of course, that's really my spirit as a coder, as a machine learning programmer.

79
00:04:29.670 --> 00:04:31.200
I always want to be efficient.

80
00:04:31.500 --> 00:04:39.240
And by that I mean that, you know, the kernel PCI implementation is super close to the PCI implementation

81
00:04:39.540 --> 00:04:45.450
because basically it will be almost the same, except that will have to add a kernel in one of the inputs.

82
00:04:45.810 --> 00:04:52.380
So what we're gonna do, you know, in that spirit of efficiency is we will go to our PCI implementation.

83
00:04:52.630 --> 00:04:58.080
We will say that cell, because you're going to see that it's going to be almost the same.

84
00:04:58.470 --> 00:04:59.520
So let's.

85
00:04:59.800 --> 00:05:04.360
Paiser here and now the only thing that we have to change its first.

86
00:05:04.450 --> 00:05:10.530
The name of the class, but not the module because the class were about import to implement COL BCA.

87
00:05:10.960 --> 00:05:14.770
Still belongs to this decomposition module by the cyclone library.

88
00:05:15.070 --> 00:05:20.470
And that class is, of course, Colonel PVA just like that.

89
00:05:20.750 --> 00:05:22.810
So that's the class then.

90
00:05:22.990 --> 00:05:25.120
Well, let's give a different name to the object.

91
00:05:25.240 --> 00:05:29.720
We're not gonna call it T.S.A., but we can call it, you know, K P.D.A as you want.

92
00:05:29.740 --> 00:05:34.900
You know, then of course here when we've called a class to create an instance of this object, which

93
00:05:34.900 --> 00:05:36.520
will be this KPC, a variable.

94
00:05:36.790 --> 00:05:40.950
Well, of course, we need to call the right class, which is COL PCI.

95
00:05:41.800 --> 00:05:44.150
And now inside this class will same.

96
00:05:44.160 --> 00:05:49.720
We have to choose a number of extracted features, which is still given by this argument and components.

97
00:05:50.080 --> 00:05:54.330
But since now we're working with a kernel, you know, we're doing col BCA.

98
00:05:54.670 --> 00:05:59.320
Well, exactly the same as when we transition from as VM to kernel as VM.

99
00:05:59.620 --> 00:06:06.160
Well, we simply need to add a kernel argument here and we'll actually choose the same kernel as with

100
00:06:06.160 --> 00:06:11.040
kernel as VM, meaning the RBA kernel, which is the radial basis function kernel.

101
00:06:11.120 --> 00:06:11.860
So there we go.

102
00:06:11.950 --> 00:06:13.720
That's our second argument here.

103
00:06:14.110 --> 00:06:16.850
Col equals in quotes.

104
00:06:17.230 --> 00:06:20.740
Well, r b f radial basis function.

105
00:06:21.470 --> 00:06:24.490
And now let's see, let's see what there is left to change.

106
00:06:24.730 --> 00:06:25.690
So this line is good.

107
00:06:25.900 --> 00:06:26.980
The next line of code.

108
00:06:27.130 --> 00:06:27.950
Well, same.

109
00:06:28.000 --> 00:06:32.170
In order to perform the kernel BCA dimensionality reduction technique.

110
00:06:32.320 --> 00:06:37.330
Well we only need the features of X train and not that's been viable Y train.

111
00:06:37.600 --> 00:06:38.230
So all good.

112
00:06:38.260 --> 00:06:43.870
You know, that's the same as BCA, but not the same as LDA, which required the Dippin enjoyable y

113
00:06:43.870 --> 00:06:44.290
train.

114
00:06:44.590 --> 00:06:45.490
So all good here.

115
00:06:45.580 --> 00:06:46.690
However, be careful.

116
00:06:46.750 --> 00:06:50.550
We renamed our object, not BCA, but KPC.

117
00:06:51.210 --> 00:06:54.690
So same here, KPC and NRMA Friends.

118
00:06:54.880 --> 00:06:56.860
This implementation is over.

119
00:06:57.160 --> 00:06:59.410
That's what happens, you know, when we're being efficient.

120
00:06:59.710 --> 00:07:03.100
The implementation is sometimes completed faster than expected.

121
00:07:03.500 --> 00:07:08.650
And that's because, as you can see, Colonel Pithier is very similar to PCM, you know, in terms of

122
00:07:08.650 --> 00:07:09.730
its implementation.

123
00:07:10.580 --> 00:07:13.600
Okay, so now we're just ready to do a run.

124
00:07:13.650 --> 00:07:15.630
Oh, again, we have our data set.

125
00:07:15.940 --> 00:07:17.800
Our implementation is all good.

126
00:07:18.070 --> 00:07:19.010
So let's do this.

127
00:07:19.210 --> 00:07:24.220
Let's click runtime here and then three to one run.

128
00:07:24.290 --> 00:07:25.150
I'll go.

129
00:07:25.710 --> 00:07:25.960
All right.

130
00:07:25.960 --> 00:07:27.400
So now all the cells are running.

131
00:07:27.510 --> 00:07:31.840
Are logistic regression models built and as expected?

132
00:07:31.990 --> 00:07:35.220
Well, we get, of course, an accuracy of 100 percent.

133
00:07:35.650 --> 00:07:42.130
I've rarely seen some cases where, you know, the non kernel version of the model beats the kernel

134
00:07:42.130 --> 00:07:43.060
version of the model.

135
00:07:43.420 --> 00:07:45.250
It can happen, but it's very rare.

136
00:07:45.430 --> 00:07:45.930
There you go.

137
00:07:45.940 --> 00:07:50.170
Here, of course, Col BCA manages to beat the PCI.

138
00:07:50.590 --> 00:07:52.360
Moral thinks that Col.

139
00:07:52.360 --> 00:07:55.840
Well, we fixed that incorrect prediction, which we had.

140
00:07:56.110 --> 00:07:58.810
Remember in BCA right here.

141
00:07:59.080 --> 00:08:00.430
So all good here.

142
00:08:00.430 --> 00:08:02.410
We get a 100 percent accuracy.

143
00:08:02.800 --> 00:08:04.290
And now let's have a look at the results.

144
00:08:04.330 --> 00:08:09.490
No how Colonel BCA was able to separate our classes in the test set.

145
00:08:09.790 --> 00:08:10.000
Right.

146
00:08:10.090 --> 00:08:13.240
Which are new observations on which the model wasn't trained.

147
00:08:13.570 --> 00:08:14.260
Well, there you go.

148
00:08:14.290 --> 00:08:16.360
That's our two principal components.

149
00:08:16.390 --> 00:08:17.680
BCO one and P.c two.

150
00:08:18.070 --> 00:08:24.280
And now in a new dimension, once again, you know, because the wine's observation points here are

151
00:08:24.610 --> 00:08:28.620
arranged in a different way than with P.c A right.

152
00:08:28.660 --> 00:08:31.100
We have very different arrangement of the points here.

153
00:08:31.120 --> 00:08:34.930
We can see them more dispersed than here, right with our piece.

154
00:08:34.950 --> 00:08:35.290
Yay!

155
00:08:35.590 --> 00:08:38.020
Well, that's because we are in a new dimension.

156
00:08:38.200 --> 00:08:39.610
We are with different dimensions.

157
00:08:39.740 --> 00:08:43.090
P.S. one piece, you too many different extracted features.

158
00:08:43.300 --> 00:08:44.530
So that's still in normal.

159
00:08:44.590 --> 00:08:46.450
That are observation points.

160
00:08:46.450 --> 00:08:49.510
You know, the ones here are arranged in a very different way.

161
00:08:49.690 --> 00:08:52.300
That's because we are in different dimension in which.

162
00:08:52.450 --> 00:08:59.200
Well, the logistic regression model was perfectly able to classify are observation points with these

163
00:08:59.470 --> 00:09:01.540
three prediction regions.

164
00:09:01.900 --> 00:09:03.090
And same for the LDA.

165
00:09:03.100 --> 00:09:10.270
The observation points are arranged differently because once again, these are some different dimensions.

166
00:09:10.300 --> 00:09:12.640
We are in another dimension here.

167
00:09:12.730 --> 00:09:14.740
Thanks to these extracted features.

168
00:09:14.950 --> 00:09:19.700
So you see this dimensionality reduction technique is pretty fascinating because it basically allows

169
00:09:19.720 --> 00:09:24.340
to create a new space of dimensions and in some new dimension.

170
00:09:24.340 --> 00:09:30.580
Well, indeed, we can perfectly classify some observations like it is the case for linear discriminant

171
00:09:30.580 --> 00:09:31.210
analysis.

172
00:09:31.330 --> 00:09:32.690
And Colonel PICU.

173
00:09:33.490 --> 00:09:38.080
Now, what I recommend for you to do is to practice this on other data sets.

174
00:09:38.130 --> 00:09:45.010
So I recommend, for example, to check out the UCI email repository platform and go to the classification

175
00:09:45.010 --> 00:09:48.670
section and try to call BCA on other datasets.

176
00:09:48.790 --> 00:09:54.640
And you'll see you will end up with similar results, with some prediction boundaries like that separating.

177
00:09:54.640 --> 00:09:55.780
Well, the classes.

178
00:09:56.260 --> 00:09:59.620
Please share your results in a Q&amp;A, especially the ones where we clearly.

179
00:09:59.730 --> 00:10:02.140
See an improvement with Colonel PRCA.

180
00:10:02.170 --> 00:10:07.750
With respect to PRCA, you know, maybe you'll find some data sets where PCI performs poorly.

181
00:10:07.780 --> 00:10:11.920
But then by adding a colonel with Colonel BCA, you will get much better results.

182
00:10:12.190 --> 00:10:13.180
So please share this.

183
00:10:13.300 --> 00:10:15.820
I'm actually very interested to see what you get.

184
00:10:16.600 --> 00:10:16.960
All right.

185
00:10:17.170 --> 00:10:18.070
Thanks in advance.

186
00:10:18.220 --> 00:10:19.750
And now congratulations.

187
00:10:19.780 --> 00:10:22.960
This new chapter on dimensionality reduction is done.

188
00:10:23.380 --> 00:10:25.990
And now we're gonna move on to the final chapter of the course.

189
00:10:26.170 --> 00:10:33.550
Barton Morel selection and boosting where you will learn your three last and very important tools which

190
00:10:33.550 --> 00:10:37.480
are first K4 cross-validation to evaluate your machinery.

191
00:10:37.480 --> 00:10:43.930
Morel's the best way, then parameter tuning to find the best values of your hyper parameters.

192
00:10:44.290 --> 00:10:51.520
And finally, the cherry on the cake of this course I will teach you and give you the x g boost model,

193
00:10:51.610 --> 00:10:57.370
which is one of the best and most powerful machinery models for regression or classification that will

194
00:10:57.370 --> 00:10:59.770
complete your master machine learning tool kit.

195
00:10:59.890 --> 00:11:00.610
The best way.

196
00:11:00.910 --> 00:11:02.890
And until then, enjoy machine learning.
