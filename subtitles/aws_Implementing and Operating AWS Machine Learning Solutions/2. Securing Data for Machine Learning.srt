1
00:00:01,040 --> 00:00:05,050
Now one of the key concerns for anyone working in machine learning is

2
00:00:05,050 --> 00:00:08,840
knowing how to best secure the data that they've collected.

3
00:00:08,840 --> 00:00:11,970
And we can go here to a quote from the CTO of Amazon,

4
00:00:11,970 --> 00:00:14,300
which he said "Dance like nobody is watching.

5
00:00:14,300 --> 00:00:18,220
Encrypt like everyone is." And there's so much truth in that.

6
00:00:18,220 --> 00:00:21,310
We need to make sure that we are protecting data,

7
00:00:21,310 --> 00:00:24,590
especially when we think about the kinds of data that we're dealing with

8
00:00:24,590 --> 00:00:26,670
when we're dealing with machine learning at a large scale.

9
00:00:26,670 --> 00:00:27,570
In many cases,

10
00:00:27,570 --> 00:00:30,410
we'll have a lot of information about customer

11
00:00:30,410 --> 00:00:32,800
behavior and actions that they've taken,

12
00:00:32,800 --> 00:00:35,870
products that they've purchased, and we're trying to analyze that.

13
00:00:35,870 --> 00:00:38,180
So we need to take time and focus in on how we're

14
00:00:38,180 --> 00:00:40,740
actually working with our customer's data.

15
00:00:40,740 --> 00:00:43,250
So there's two things we need to consider when we're looking at

16
00:00:43,250 --> 00:00:48,240
aspects of data encryption with data encryption being just one area

17
00:00:48,240 --> 00:00:50,380
of data security that we need to consider.

18
00:00:50,380 --> 00:00:53,600
And that is first data at rest, meaning again,

19
00:00:53,600 --> 00:00:56,380
data that's stored on a volume, in a database,

20
00:00:56,380 --> 00:00:57,880
or other AWS service.

21
00:00:57,880 --> 00:01:00,150
So it's not moving from one place to another.

22
00:01:00,150 --> 00:01:02,140
It's actually stored in a location.

23
00:01:02,140 --> 00:01:03,870
That's data at rest.

24
00:01:03,870 --> 00:01:07,040
However, we also have data in transit,

25
00:01:07,040 --> 00:01:11,000
and that could be data that's being sent from the internet to a specific AWS

26
00:01:11,000 --> 00:01:15,740
service or server or even between different AWS services.

27
00:01:15,740 --> 00:01:20,440
So we need to understand how data encryption applies in both of these scenarios.

28
00:01:20,440 --> 00:01:23,240
So let's look first at data at rest.

29
00:01:23,240 --> 00:01:24,240
So first of all,

30
00:01:24,240 --> 00:01:30,320
data stored in Amazon S3 should follow best practices and enforce encryption.

31
00:01:30,320 --> 00:01:34,580
So SageMaker itself works with S3 directly.

32
00:01:34,580 --> 00:01:38,520
However, if you're implementing a machine learning solution on AWS,

33
00:01:38,520 --> 00:01:41,600
one of your first steps is going to be to upload the data that you're

34
00:01:41,600 --> 00:01:45,080
going to use for your model training into Amazon S3.

35
00:01:45,080 --> 00:01:49,840
So you're interacting with S3 in many cases before you even touch SageMaker.

36
00:01:49,840 --> 00:01:51,240
And because of that,

37
00:01:51,240 --> 00:01:55,640
you need to be sure that you know how to work with S3 to ensure encryption.

38
00:01:55,640 --> 00:01:59,610
So first, you can leverage bucket policies to enforce encryption,

39
00:01:59,610 --> 00:02:04,000
and you can utilize a customer‑managed KMS key for encryption.

40
00:02:04,000 --> 00:02:08,320
Now you could use the S3 default key, but you'll see in working with SageMaker,

41
00:02:08,320 --> 00:02:12,540
there's an advantage to choosing to use customer‑managed keys throughout.

42
00:02:12,540 --> 00:02:17,650
Now SageMaker verifies that its data outputs are encrypted at rest.

43
00:02:17,650 --> 00:02:20,430
So if you're thinking about the outputs of SageMaker,

44
00:02:20,430 --> 00:02:23,740
for example, things like your compiled model,

45
00:02:23,740 --> 00:02:26,250
it makes sure that those things are encrypted.

46
00:02:26,250 --> 00:02:27,040
But again,

47
00:02:27,040 --> 00:02:29,490
if you're storing things in an S3 bucket that you've

48
00:02:29,490 --> 00:02:31,870
properly configured for encryption,

49
00:02:31,870 --> 00:02:35,220
you're making sure that even the data you're working with before you jump

50
00:02:35,220 --> 00:02:39,040
into SageMaker follows those best practices as well.

51
00:02:39,040 --> 00:02:41,620
Now it's important to note here that there are some specific

52
00:02:41,620 --> 00:02:44,310
Nitro‑based instances that are encrypted,

53
00:02:44,310 --> 00:02:46,430
but not with KMS.

54
00:02:46,430 --> 00:02:49,700
So if you're using a Nitro‑based instance with SageMaker,

55
00:02:49,700 --> 00:02:53,140
encryption will be handled in a bit of a different way.

56
00:02:53,140 --> 00:02:56,180
Now let's talk about encrypted volumes at a high level.

57
00:02:56,180 --> 00:03:01,240
So if you choose not to specify that customer‑managed KMS key with SageMaker,

58
00:03:01,240 --> 00:03:05,070
it will be encrypted, but it's going to leverage what we call a transient key,

59
00:03:05,070 --> 00:03:07,570
and that means that key is going to be discarded after

60
00:03:07,570 --> 00:03:09,370
it's used to encrypt that volume.

61
00:03:09,370 --> 00:03:09,970
So again,

62
00:03:09,970 --> 00:03:14,640
in most cases you're going to want to leverage a customer‑managed KMS key.

63
00:03:14,640 --> 00:03:17,850
Now let's look at different areas where SageMaker is using

64
00:03:17,850 --> 00:03:20,640
these volumes that need to be encrypted.

65
00:03:20,640 --> 00:03:22,720
We see it with notebook instances.

66
00:03:22,720 --> 00:03:26,380
So when you spin up your notebook instances and you're working on your

67
00:03:26,380 --> 00:03:29,080
different hypotheses that you're implementing for your machine learning

68
00:03:29,080 --> 00:03:33,570
solution, that has an encrypted volume that is attached to that instance

69
00:03:33,570 --> 00:03:37,920
where it's storing the output of what you're doing. Now also processing

70
00:03:37,920 --> 00:03:42,730
jobs, as well as training jobs. Those instances also have volumes that are

71
00:03:42,730 --> 00:03:44,140
attached to them.

72
00:03:44,140 --> 00:03:46,660
We also can look at batch transform jobs that we

73
00:03:46,660 --> 00:03:52,140
talked about in the last module, as well as hosting services endpoints.

74
00:03:52,140 --> 00:03:55,700
In addition, if you're doing any hyperparameter tuning jobs,

75
00:03:55,700 --> 00:03:58,800
those instances that are doing that work also have

76
00:03:58,800 --> 00:04:00,600
volumes that are attached to them.

77
00:04:00,600 --> 00:04:02,840
So all of those are different areas where we need to be

78
00:04:02,840 --> 00:04:06,840
concerned with our data being encrypted at rest.

79
00:04:06,840 --> 00:04:11,480
But in addition to data at rest, we also have data in transit.

80
00:04:11,480 --> 00:04:14,880
So one of the great things is within SageMaker, the inner network, most

81
00:04:14,880 --> 00:04:20,040
of the data is encrypted in transit with TLS 1.2.

82
00:04:20,040 --> 00:04:23,500
Now there are some areas that are not encrypted by default,

83
00:04:23,500 --> 00:04:25,130
and you need to understand these.

84
00:04:25,130 --> 00:04:28,160
The first is communication that's between the actual

85
00:04:28,160 --> 00:04:30,850
control plane and the training job instances.

86
00:04:30,850 --> 00:04:33,740
However, that doesn't really involve any of your data,

87
00:04:33,740 --> 00:04:36,150
so that's still entirely within the service.

88
00:04:36,150 --> 00:04:40,140
Nothing is leaving the service, and it's not containing any of your data.

89
00:04:40,140 --> 00:04:40,850
However,

90
00:04:40,850 --> 00:04:45,560
we also have node communication in distributed processing jobs and

91
00:04:45,560 --> 00:04:48,850
node communication in distributed training jobs.

92
00:04:48,850 --> 00:04:51,850
Now there is a way you can get your node communication and

93
00:04:51,850 --> 00:04:54,290
distributed training jobs encrypted,

94
00:04:54,290 --> 00:04:56,340
and there's different steps you can walk through in

95
00:04:56,340 --> 00:04:58,040
the console to make that happen.

96
00:04:58,040 --> 00:04:59,180
However, in doing so,

97
00:04:59,180 --> 00:05:02,630
you're also going to take a significant performance hit depending on

98
00:05:02,630 --> 00:05:06,010
how you're implementing your model training, so just note that. But

99
00:05:06,010 --> 00:05:09,060
it is important to note that with those two areas where you're

100
00:05:09,060 --> 00:05:13,770
potentially dealing with data, that data is still never leaving the service.

101
00:05:13,770 --> 00:05:18,540
Anytime data leaves the service, it's going to be encrypted with TLS 1.2.

102
00:05:18,540 --> 00:05:22,210
So let's take a look back at our high‑level diagram showing how

103
00:05:22,210 --> 00:05:25,140
we interact with SageMaker hosting services.

104
00:05:25,140 --> 00:05:26,760
So here, in this case,

105
00:05:26,760 --> 00:05:30,150
the client application is going to interact with API Gateway, and

106
00:05:30,150 --> 00:05:34,370
API Gateway can be configured to use just TLS 1.2.

107
00:05:34,370 --> 00:05:37,250
So here we can see encryption between those two points.

108
00:05:37,250 --> 00:05:38,010
In addition,

109
00:05:38,010 --> 00:05:40,910
we're going to see encryption in terms of how API Gateway

110
00:05:40,910 --> 00:05:43,140
interacts with the lambda function.

111
00:05:43,140 --> 00:05:45,350
And then since we mentioned earlier that all of the

112
00:05:45,350 --> 00:05:49,050
communication within SageMaker is going to be encrypted,

113
00:05:49,050 --> 00:05:51,420
we're going to see that encryption between lambda and the

114
00:05:51,420 --> 00:05:54,700
endpoint between the endpoint and the load balancer and between

115
00:05:54,700 --> 00:05:57,540
the load balancer and those instances.

116
00:05:57,540 --> 00:05:59,950
Now we're also going to see encryption between those

117
00:05:59,950 --> 00:06:01,820
instances and the trained model.

118
00:06:01,820 --> 00:06:03,610
But later on within this module,

119
00:06:03,610 --> 00:06:08,940
we're going to look at a better way for interacting with S3 from within our VPC.

120
00:06:08,940 --> 00:06:12,630
Now one of the great things is at a high level, SageMaker has data

121
00:06:12,630 --> 00:06:15,540
security integrated into the service as a whole.

122
00:06:15,540 --> 00:06:19,900
So if you're following best practices for using S3 when you're using SageMaker,

123
00:06:19,900 --> 00:06:24,040
most of the most serious risks have already been covered.

124
00:06:24,040 --> 00:06:24,550
However,

125
00:06:24,550 --> 00:06:26,860
there's another step that you can take when you're

126
00:06:26,860 --> 00:06:29,940
considering data security on SageMaker,

127
00:06:29,940 --> 00:06:33,330
and that's Amazon Macie. And it is a fully managed

128
00:06:33,330 --> 00:06:36,340
service that's focused on data security,

129
00:06:36,340 --> 00:06:40,790
and it actually utilizes machine learning itself to both discover and

130
00:06:40,790 --> 00:06:44,840
report on the data that you have stored within Amazon S3.

131
00:06:44,840 --> 00:06:48,220
And one of the things that it does is that it can alert you if

132
00:06:48,220 --> 00:06:52,510
several conditions are met, things like unusual data access patterns

133
00:06:52,510 --> 00:06:55,720
or configuration errors for sensitive data.

134
00:06:55,720 --> 00:06:58,300
So if it's scanned this data and it knows that it's actual

135
00:06:58,300 --> 00:07:01,450
sensitive data and all of a sudden somebody goes through and

136
00:07:01,450 --> 00:07:04,440
chooses to make this bucket publicly available,

137
00:07:04,440 --> 00:07:07,040
it can alert you on those kind of conditions.

138
00:07:07,040 --> 00:07:11,320
This service can prevent some configuration errors, as well as bad

139
00:07:11,320 --> 00:07:14,440
actors potentially trying to get access to your data.

140
00:07:14,440 --> 00:07:23,000
So one of the great things is that it can enable some automated compliance checking for your data storage that you're storing in Amazon S3.

