WEBVTT
1
00:00:01.040 --> 00:00:04.210
So next, we're going to discuss deep learning containers.

2
00:00:04.210 --> 00:00:07.380
Now I introduced these earlier within the course,

3
00:00:07.380 --> 00:00:10.540
but now I want to look at them in a little bit more detail.

4
00:00:10.540 --> 00:00:11.990
So just as a refresher,

5
00:00:11.990 --> 00:00:15.380
deep learning containers are pre‑configured Docker containers

6
00:00:15.380 --> 00:00:17.660
for common machine learning frameworks.

7
00:00:17.660 --> 00:00:18.880
Now what that means is,

8
00:00:18.880 --> 00:00:22.220
not only do you have the framework built in on these containers,

9
00:00:22.220 --> 00:00:26.190
but also much of the configuration that's required for things like GPU

10
00:00:26.190 --> 00:00:29.840
support has also been built into the container already.

11
00:00:29.840 --> 00:00:34.780
And you can choose to leverage these containers with AWS services like EC2,

12
00:00:34.780 --> 00:00:37.490
ECS, and EKS.

13
00:00:37.490 --> 00:00:41.300
Now right now, the supported frameworks are TensorFlow,

14
00:00:41.300 --> 00:00:43.350
MXNet, and Pytorch,

15
00:00:43.350 --> 00:00:46.430
and there are specific versions of each framework that are

16
00:00:46.430 --> 00:00:48.840
supported with deep learning containers.

17
00:00:48.840 --> 00:00:50.300
Now a few general notes.

18
00:00:50.300 --> 00:00:51.310
First of all,

19
00:00:51.310 --> 00:00:56.440
there are versions of each framework for Elastic Inference that a also included.

20
00:00:56.440 --> 00:01:00.900
So if you want to leverage Elastic Inference, again that partial GPU support,

21
00:01:00.900 --> 00:01:03.840
you need to be sure that you're picking a container that already

22
00:01:03.840 --> 00:01:08.470
has that support built in. Next, deep learning containers can

23
00:01:08.470 --> 00:01:11.770
support both training and inference.

24
00:01:11.770 --> 00:01:12.390
However,

25
00:01:12.390 --> 00:01:15.250
you need to be sure that you're picking the correct one because there

26
00:01:15.250 --> 00:01:18.740
will be pre‑built versions for training or inference.

27
00:01:18.740 --> 00:01:22.010
Now next, fault‑tolerant architectures are possible,

28
00:01:22.010 --> 00:01:24.580
but you need to follow the service best practices

29
00:01:24.580 --> 00:01:26.640
depending on which service you're using.

30
00:01:26.640 --> 00:01:27.520
So, for example,

31
00:01:27.520 --> 00:01:31.420
if you're using EC2 and you want to spin up your own infrastructure

32
00:01:31.420 --> 00:01:34.310
using a deep learning container for inference,

33
00:01:34.310 --> 00:01:37.790
you'll be responsible for setting up an autoscaling group and

34
00:01:37.790 --> 00:01:41.020
putting an application load balancer on the front end of it to

35
00:01:41.020 --> 00:01:42.670
make sure that it's configured properly.

36
00:01:42.670 --> 00:01:44.140
That's just one example.

37
00:01:44.140 --> 00:01:48.140
So instead of relying on SageMaker and the best practices for that service,

38
00:01:48.140 --> 00:01:56.000
you need to be sure that you know the service you're deploying on well enough to implement a fault‑tolerant architecture there.

