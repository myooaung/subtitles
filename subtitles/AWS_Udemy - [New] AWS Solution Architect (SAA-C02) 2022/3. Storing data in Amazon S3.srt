1
00:00:00,090 --> 00:00:07,320
Another feature that is supported with AC three is called Cross Origin Resource Sharing, and this is

2
00:00:07,320 --> 00:00:09,180
also very important for the exam.

3
00:00:09,180 --> 00:00:15,420
So course defines a way for a client with applications that are loaded in one domain, let's say your

4
00:00:15,420 --> 00:00:24,420
domain and let's call this for example, a W i.e. once the user inside your page a click on an external

5
00:00:24,420 --> 00:00:31,770
domain or a different domain, let's say Facebook dot com if cause not enable that kind of action will

6
00:00:31,770 --> 00:00:32,400
be blocked.

7
00:00:32,490 --> 00:00:39,180
But in SC three, you can allow this kind of behavior, which is the cause and once you allow it at

8
00:00:39,180 --> 00:00:41,430
the client side to the application of your asset.

9
00:00:41,430 --> 00:00:46,860
Three, This will allow across origin access to your Amazon asset resources.

10
00:00:47,040 --> 00:00:53,400
Now how you would do that in a specific bucket normally course configure as an external document with

11
00:00:53,560 --> 00:01:00,720
specific rules that basically identify the following the origins that you will allow to access your

12
00:01:00,720 --> 00:01:06,390
bucket, the operation with a history type method that will support for each origin.

13
00:01:06,660 --> 00:01:15,030
In this example we have put pause and delete are allowed for going to a domain called WWE example dot

14
00:01:15,030 --> 00:01:22,170
com, which could be configured using Amazon Route 53 to be another SC three bucket, for example.

15
00:01:22,170 --> 00:01:25,740
And the last object is operation is specific information.

16
00:01:25,770 --> 00:01:33,480
Now why we sometimes need to refer from an AC three a bucket to another S3 bucket in order to cover

17
00:01:33,480 --> 00:01:34,260
two domains.

18
00:01:34,260 --> 00:01:42,480
For example example dot com and your domain name system should be a record and w w w dot example dot

19
00:01:42,480 --> 00:01:44,400
com is another protocol.

20
00:01:44,640 --> 00:01:47,880
So most likely you will end up creating two buckets to cover both.

21
00:01:47,880 --> 00:01:55,770
One contains the actual website and the other you only use it with causative referring and configuration

22
00:01:55,770 --> 00:02:03,360
just to point to the WW dot example dot com and let us now understand how we can host a static website

23
00:02:03,390 --> 00:02:06,030
using the guided lab of module three.

24
00:02:06,030 --> 00:02:12,750
In this lab we are going to create a bucket in Amazon S3 We will upload a sample content to that bucket

25
00:02:12,750 --> 00:02:17,730
and we will enable access to the object and we update the website.

26
00:02:17,760 --> 00:02:25,050
We have multiple use cases besides hosting a static live site, and the third use case is to use a three

27
00:02:25,050 --> 00:02:29,760
bucket for stair storing data for computation and analysis.

28
00:02:29,760 --> 00:02:34,440
And most likely this is will come with machine learning and big data.

29
00:02:34,440 --> 00:02:41,160
So those large scale analytics projects such as financial transaction, clickstream analysis, media

30
00:02:41,160 --> 00:02:48,870
transcoding, Amazon necessary can support this workload because of it is ability to horizontally scale

31
00:02:48,900 --> 00:02:52,410
and enabling multiple concurrent transaction.

32
00:02:52,560 --> 00:02:59,370
So for example, the diagram here is showing us an Amazon elastic compute cloud, easy to spot deflate

33
00:02:59,370 --> 00:03:08,390
and those are basically created when the bid price for the spot instances is low or when an Amazon or

34
00:03:08,490 --> 00:03:12,180
a cluster elastic MapReduce or cluster is spun.

35
00:03:12,330 --> 00:03:20,490
After the compute capacity is available, the role and a process data is extracted from Amazon S3 and

36
00:03:20,490 --> 00:03:22,180
also from another data source.

37
00:03:22,200 --> 00:03:28,410
So the data is run through compute algorithms that integrate and transform it.

38
00:03:28,500 --> 00:03:34,740
The result will be a processed data that will be loaded into different Amazon S3 bucket.

39
00:03:34,860 --> 00:03:41,700
In this case, the data has been processed and the compute capacity is terminated to save one cost.

40
00:03:41,790 --> 00:03:48,990
Finally, analytics tools such as Amazon, a quick site might be used to harvest meaningful insights

41
00:03:48,990 --> 00:03:50,200
from the process data.

42
00:03:50,340 --> 00:03:56,730
So this is just only one simple scenario how you can employ Amazon, etc. in a big data and machine

43
00:03:56,730 --> 00:03:57,750
learning project.

44
00:03:57,810 --> 00:04:01,800
You could also use it for backup and archiving of the critical data.

45
00:04:01,830 --> 00:04:08,520
And this is very important also to understand because S3 is highly durable and scalable, it could work

46
00:04:08,520 --> 00:04:12,660
very good for data, backup and archiving tool in this scenario.

47
00:04:12,660 --> 00:04:19,770
Here, data is backed up from one on premise data center and also from a large number of Amazon, easy

48
00:04:19,770 --> 00:04:27,270
to in instances running in a year, you can move long term data from Amazon S3 standard to storage to

49
00:04:27,270 --> 00:04:30,120
assist re English in the process.

50
00:04:30,120 --> 00:04:37,200
We will discuss now when we study Iglesia, but in fact this is will give you a reduced cost for those

51
00:04:37,200 --> 00:04:43,920
objects and S3 will be able to move your data based on the usage pattern so you can specify to move

52
00:04:43,920 --> 00:04:51,150
your data to Glazier, which is an archiving solution after 30 days or 60 days or 90 days if those data

53
00:04:51,390 --> 00:04:52,830
are not accessed.

54
00:04:52,830 --> 00:04:58,190
I know that Amazon, as the three objects that you can configure guys is to achieve higher level off

55
00:04:58,200 --> 00:04:59,280
in your ability.

56
00:04:59,280 --> 00:04:59,910
Is it cost?

57
00:05:00,090 --> 00:05:06,630
Regional replication and the cross region application will allow you to basically copy the content of

58
00:05:06,630 --> 00:05:08,190
all unmodified objects.

59
00:05:08,220 --> 00:05:11,010
Pay attention to this guides all modified objects.

60
00:05:11,010 --> 00:05:19,590
You copy those modified updated deleted objects to a replica in another ancestry bucket in another object

61
00:05:19,600 --> 00:05:24,750
that are uploaded to the bucket in one region will be automatically copied to another asset, three

62
00:05:24,750 --> 00:05:27,210
buckets in that other region.

63
00:05:27,270 --> 00:05:31,470
And lastly, what is the data consistency model in Amazonas?

64
00:05:31,620 --> 00:05:38,700
Three And seriously, this is very important for you to keep in your mind before Dixon Amazon S3 provide

65
00:05:38,700 --> 00:05:44,160
us with read after right consistency for pots and all in you objects.

66
00:05:44,280 --> 00:05:49,140
So any new object upload to AC three, you will read it after you write it.

67
00:05:49,140 --> 00:05:50,580
So imagine the same objects.

68
00:05:50,580 --> 00:05:55,440
There is two person one writing to that object and one reading from that object.

69
00:05:55,440 --> 00:05:59,790
In this case, the previous version of the object will be retrieved.

70
00:05:59,880 --> 00:06:08,190
The second option is eventual consistency for all the right pot and delete in all regions.

71
00:06:08,190 --> 00:06:13,770
So that applies for all existing objects, not a new object.

72
00:06:13,770 --> 00:06:21,390
In this case, if somebody trying to read the file and somebody is trying to write to the file, in

73
00:06:21,390 --> 00:06:25,890
this case the right operation go first and then the read.

74
00:06:26,400 --> 00:06:34,380
So if you put to an existing key a subsequent read my system, the old or stolen data or back dated

75
00:06:34,380 --> 00:06:42,600
data, Amazon S3 achieves high availability by replicating data across multiple servers within the data

76
00:06:42,600 --> 00:06:43,050
center.

77
00:06:43,050 --> 00:06:49,710
So if there is a post request and it is a successful request, your data will be safely stored.

78
00:06:49,800 --> 00:06:56,430
The information about this changes must replicate across multiple Amazon facility, which might take

79
00:06:56,430 --> 00:06:56,850
time.

80
00:06:57,150 --> 00:07:03,510
So the process to write and you object to Amazon etc. and immediately, unless the key is in that object,

81
00:07:03,510 --> 00:07:09,510
until the change is fully propagated, the object might not appear in the list.

82
00:07:09,780 --> 00:07:17,070
But if you have a process replaces an existing object and immediately tonight to read it until the change

83
00:07:17,070 --> 00:07:24,480
is fully propagated and isn't necessary, we give you the previous data if the process deletes an existing

84
00:07:24,480 --> 00:07:32,250
object and immediately try to read it until that process of deletion is completely and fully propagated,

85
00:07:32,520 --> 00:07:36,540
Amazon etc. might retain the deleted data.

86
00:07:36,690 --> 00:07:37,770
So you see the difference.

87
00:07:38,040 --> 00:07:39,510
You need to pay attention to that.

88
00:07:39,510 --> 00:07:48,150
So in summary, a city has to consistency with read after right for a new object and plots, as will

89
00:07:48,180 --> 00:07:52,380
also give you eventual consistency for overrides.

90
00:07:52,560 --> 00:07:56,070
So if you delete the object, most likely somebody's trying to read it.

91
00:07:56,070 --> 00:07:58,260
He might get that deleted content.

92
00:07:58,320 --> 00:08:05,070
So a process deletes an existing object and immediately lists keys in its buckets until the deletion

93
00:08:05,070 --> 00:08:06,270
is fully propagated.

94
00:08:06,450 --> 00:08:11,070
Amazon etc. list also those deleted objects.

95
00:08:11,130 --> 00:08:13,850
Now how about storing data in Amazon?

96
00:08:14,450 --> 00:08:23,130
As we said, Amazon S3 has multiple classes and the first class is the S3 standard, which is for frequently

97
00:08:23,130 --> 00:08:24,090
access data.

98
00:08:24,090 --> 00:08:31,890
You could also move your data from the standard one to Amazon S3 standard, any frequently accessed

99
00:08:31,890 --> 00:08:34,980
class with the standard, any frequent access guys.

100
00:08:34,980 --> 00:08:42,850
It's offer you all the benefits of Amazon S3 standard, but it runs on a different course model to store

101
00:08:43,050 --> 00:08:49,710
any frequently access data such as an old digital images or older look files.

102
00:08:49,770 --> 00:08:57,210
This will give you a three day minimum storage fee apply to any data placed in the SC three standard

103
00:08:57,210 --> 00:09:04,770
i a another I can answer from this is called the Amazon S3 one zone infrequently access data.

104
00:09:04,770 --> 00:09:12,390
In this particular class, it is suited the customer who want a lower cost option and who do not need

105
00:09:12,600 --> 00:09:19,290
the availability and resilience of the AC three standard or AC three standard infrequent access.

106
00:09:19,320 --> 00:09:21,150
So you need to pay attention to that.

107
00:09:21,150 --> 00:09:27,870
If you use this with your disaster recovery plan, you need to pay attention if you are moving your

108
00:09:27,870 --> 00:09:32,430
data backup to a31 zone and frequent access.

109
00:09:32,610 --> 00:09:38,790
Remember, this is going to give you a cheaper cost model, but at the same time is not guarantee.

110
00:09:38,790 --> 00:09:45,450
If the whole zone is not available and you need to recover and restore your data, maybe your data is

111
00:09:45,450 --> 00:09:46,410
not available.

112
00:09:46,440 --> 00:09:52,500
So you have to be careful whether you need to use one zone or standard infrequent access multi zone

113
00:09:52,740 --> 00:09:53,400
for the exam.

114
00:09:53,400 --> 00:09:59,850
Also they refer to ac3 standard I a on infrequent access sometimes with the multi zone just to the.

115
00:09:59,880 --> 00:10:02,630
Distinguish this, that this is not one zone.

116
00:10:02,640 --> 00:10:04,400
Now about the intelligent theory.

117
00:10:04,470 --> 00:10:10,860
This is designed to optimize the course by automatically looking to your usage pattern from the SD three

118
00:10:10,860 --> 00:10:16,680
standard and moving your data to a lower cost model, which is the intelligent theory.

119
00:10:16,860 --> 00:10:21,990
This is very good for small monthly monitoring and automation fee per object.

120
00:10:22,050 --> 00:10:28,770
This is basically will give you a good access pattern and at the same time you will be able to retrieve

121
00:10:28,770 --> 00:10:31,950
your object and they also have a better durability.

122
00:10:31,980 --> 00:10:36,870
Also, we have something called Amazon S3 Glacier or deep archiving.

123
00:10:36,900 --> 00:10:42,520
Now remember, this is a C3 class which is designed mainly for data archiving.

124
00:10:42,540 --> 00:10:49,470
You can store any amount of data at cost that are competitive and to keep your costs low.

125
00:10:49,530 --> 00:10:59,040
You can also move those data from a central glacier to S3 Glacier Deep Archive, which is another lower

126
00:10:59,490 --> 00:11:00,960
class of storage.

127
00:11:01,110 --> 00:11:07,890
You can look at it as a cold storage at the same time when you put your archiving solution in a glacier.

128
00:11:08,100 --> 00:11:10,590
You have a three different retrieval model.

129
00:11:10,860 --> 00:11:17,220
To reduce the cost, you have to expedite retrieval, which make the objects available to you in 1 to

130
00:11:17,220 --> 00:11:17,850
5 minutes.

131
00:11:17,880 --> 00:11:26,160
You also have the standard retrieval, which an object retrieve may be between 3 to 5 hours, and you

132
00:11:26,160 --> 00:11:31,650
have finally the bulk retrieve, which can complete within 5 to 12 hours.

133
00:11:31,890 --> 00:11:36,760
In general, Amazon Glacier Deep Archive is the lowest cost.

134
00:11:36,760 --> 00:11:44,610
This storage class in Amazon S3 is more suited for long term retention and digital preservation.

135
00:11:44,640 --> 00:11:53,370
However, the data is stored across at least a three geographically distant availability zone and they

136
00:11:53,370 --> 00:12:00,640
are protected by 11 nines of durability and the object can be restored within 12 hours.

137
00:12:00,660 --> 00:12:02,910
So remember guys in Acetylene Glacier?

138
00:12:03,090 --> 00:12:06,390
We have expanded the standard and bulk in deep archive.

139
00:12:06,570 --> 00:12:09,050
We can get the object in 12 hour.

140
00:12:09,090 --> 00:12:10,950
It is the lowest and achieve it.

141
00:12:11,280 --> 00:12:17,470
Let us look now to something called the Amazon S3 Lifecycle Policy, a lifecycle policy.

142
00:12:17,640 --> 00:12:21,810
You can control how your objects will be moved from a class to another.

143
00:12:22,260 --> 00:12:26,160
You define a rule, and that rule will take action over the data.

144
00:12:26,170 --> 00:12:32,940
So, for example, you want to move all the objects that you don't access in the last 30 days from ac3

145
00:12:32,940 --> 00:12:36,740
standard to Amazon S3 standard, any frequent access.

146
00:12:36,750 --> 00:12:43,650
And then you want to move the object from Amazon S3 standard, infrequent access after 60 days, you

147
00:12:43,650 --> 00:12:51,780
want to move them to Amazon S3 Glacier and then you want to delete them completely after 365 days.

148
00:12:51,810 --> 00:12:54,900
Now what is the course model in Amazon SS3?

149
00:12:54,930 --> 00:12:59,490
Now with Amazon S3, you pay only for what you use and there is no minimum fee.

150
00:12:59,490 --> 00:13:04,740
There is four course components that you need to consider when you want to use Amazon.

151
00:13:04,740 --> 00:13:13,200
S3 is storage class, so you pay per gigabyte for the all objects that you store panel and each class

152
00:13:13,200 --> 00:13:17,730
of object or storage class has its own pricing model per region.

153
00:13:17,730 --> 00:13:20,900
So from one region to another, the cost could be different.

154
00:13:20,970 --> 00:13:28,110
Now, when you transfer the data from AC three to the Internet, you need to pay for all parts copy,

155
00:13:28,110 --> 00:13:30,750
post, list, get and select.

156
00:13:30,930 --> 00:13:38,760
And the lifecycle transaction is including data that when you move the data from one class to another

157
00:13:39,000 --> 00:13:44,310
and also data retrieval request, when you request to retrieve your data from a glacier, you also need

158
00:13:44,310 --> 00:13:44,730
to pay.

159
00:13:45,050 --> 00:13:50,490
However, there is no charge when you approach your data and when you want to transfer the content of

160
00:13:50,490 --> 00:13:57,030
the bucket from Amazon S3 to cloud to front or to another service in the same region.

161
00:13:57,030 --> 00:14:01,380
And there is no charge as well for any delete and consider liquids.

162
00:14:01,380 --> 00:14:05,850
So when you delete an object or you cancel the upload of an object, there is no charge.

163
00:14:06,090 --> 00:14:07,920
So keep this in your mind.

164
00:14:08,340 --> 00:14:11,340
Now, before I forget, guys, also you need to pay.

165
00:14:11,730 --> 00:14:18,390
If you transfer the content from AC three to another AC three in another region, you have to pay.

166
00:14:18,420 --> 00:14:23,910
So to move data from Amazon, AC three to another parts, you can use the management console and we

167
00:14:23,910 --> 00:14:25,350
will see this now in the demo.

168
00:14:25,350 --> 00:14:29,730
Or you could use the command line interface similar to what we see here in the screen.

169
00:14:29,910 --> 00:14:37,410
You can use the command NWC, AC three copy just dot text and this is will be copied to AC three, the

170
00:14:37,410 --> 00:14:44,670
name of the bucket slash, also the name of the object because you have to specify this name because

171
00:14:44,670 --> 00:14:51,810
this is basically the key for the object and this is how it will be stored physically in etc. You could

172
00:14:51,810 --> 00:14:59,250
also use third party tool or W as tools and as the key to uploading large object as we would see like

173
00:14:59,250 --> 00:14:59,640
multiple.

174
00:14:59,690 --> 00:15:02,300
Part of loads accelerated the transfer.

175
00:15:02,660 --> 00:15:07,490
You can also upload multiple art of the objects, especially when you have a large object.

176
00:15:07,500 --> 00:15:11,180
We say the necessary a single object could be four terabyte.

177
00:15:11,210 --> 00:15:13,880
In this case you can upload it all at once.

178
00:15:13,880 --> 00:15:15,260
But this will never happen.

179
00:15:15,440 --> 00:15:20,980
Using a drag and drop to the interface though, you have to use a tool or a third party tool or Amazon

180
00:15:21,140 --> 00:15:28,280
and you can have the ability to pause a resume also similar to the any cloud storage tool that we use

181
00:15:28,280 --> 00:15:28,820
normally.

182
00:15:29,030 --> 00:15:34,370
You can pose at a zoom at any time and it give you an improved throughput.

183
00:15:34,820 --> 00:15:42,200
However, there is something called Amazon S3 transfer acceleration, which is a better tool to speed

184
00:15:42,200 --> 00:15:50,600
up the upload and this is will deal with your bucket via enabling fast and easy data transfer if you

185
00:15:50,600 --> 00:15:58,580
are in which make it available in the locations of a cloud different and this is will be globally distributed.

186
00:15:58,700 --> 00:16:06,500
So the data once it's uploaded to the AC three transfer acceleration, it will be routed to Amazon S3

187
00:16:06,500 --> 00:16:08,870
over an optimised network path.

188
00:16:09,140 --> 00:16:14,960
Now when you can use Amazon S3 transfer acceleration, when you have customers all around the globe

189
00:16:14,960 --> 00:16:22,570
and you want to upload locally or centrally from multiple customers distributed in multiple countries,

190
00:16:22,580 --> 00:16:29,630
when you want to transfer a gigabytes of data or terabytes of data across continents in a regular basis,

191
00:16:29,630 --> 00:16:36,590
and when you have, for example, a very bad bandwidth and you want to upload files to Amazon S3 over

192
00:16:36,590 --> 00:16:42,870
the Internet, then it is much better for you to use them as an asset re transfer acceleration, which

193
00:16:42,870 --> 00:16:46,100
will I will show you now in the next demo now.

194
00:16:46,110 --> 00:16:52,010
Lastly, sometimes you want to migrate to the cloud and you have a large object in beta white scaled

195
00:16:52,010 --> 00:16:52,190
it.

196
00:16:52,280 --> 00:16:54,530
In this case you can use a service.

197
00:16:54,530 --> 00:16:57,050
It's called a W as is snow mode.

198
00:16:57,110 --> 00:17:05,300
Snowball is a data transport option that does not require you to write any code or to purchase any hardware

199
00:17:05,300 --> 00:17:07,580
to transfer your data to Amazon.

200
00:17:07,670 --> 00:17:14,360
S3 All you need to do is to create a job in the management console and a snowball will be shipped to

201
00:17:14,360 --> 00:17:14,510
you.

202
00:17:14,510 --> 00:17:19,850
So there is no physical hardware that will be shipped to your data center, to your office, and you

203
00:17:19,850 --> 00:17:24,080
will post the data in it, and then you put it back and you ship it back to Amazon.

204
00:17:24,110 --> 00:17:29,990
So all you need to do is to attach it to your local network and to transfer the file directly into it.

205
00:17:30,200 --> 00:17:39,260
And then you ship it back to your as closest region or to the nearest Amazon facility and the data will

206
00:17:39,260 --> 00:17:41,900
be transferred into your account.

207
00:17:41,960 --> 00:17:50,720
If you have large amount of data, what you can get is this no mobile even larger than the previous

208
00:17:50,720 --> 00:17:52,100
one, which is the snowball.

209
00:17:52,400 --> 00:17:53,780
It is snow mobile.

210
00:17:54,050 --> 00:17:58,550
It is a data transfer option that operate in exabyte scale.

211
00:17:58,580 --> 00:18:04,070
And exabyte is a 1 million terabytes or 1 billion gigabytes.

212
00:18:04,100 --> 00:18:09,200
It should only be used to move extremely large amount of data into a w.

213
00:18:09,740 --> 00:18:19,490
A snow mobile is a 45 foot long shipped container that will come to you in a semi trailer truck and

214
00:18:19,490 --> 00:18:23,960
you can transfer 100 petabyte of data here.

215
00:18:23,960 --> 00:18:24,680
It's normal.

216
00:18:25,010 --> 00:18:31,880
So if you ordered this feature or this service to your on premise data center, it will use as multi

217
00:18:31,910 --> 00:18:39,290
layer of security designed to protect your data include as well dedicated security personnel, GPS tracking,

218
00:18:39,290 --> 00:18:47,720
alarm monitoring and 24 over seven video and also an optional security escort, ADT encryption with

219
00:18:47,720 --> 00:18:50,510
256 bit encryption key.

220
00:18:50,750 --> 00:18:55,130
And you could also use it as CMS keys to encrypt the data.

221
00:18:55,400 --> 00:19:02,180
And it is designed to ensure both security and full chain of custody over your data.

222
00:19:02,270 --> 00:19:05,000
So the last thing for today, guys, is choosing a region.

223
00:19:05,000 --> 00:19:06,350
And we discussed this before.

224
00:19:06,350 --> 00:19:12,290
So just to make sure we understand how we can select the best region for our application.

225
00:19:12,710 --> 00:19:20,540
So first, you should consider data privacy law and your regulation and compliance requirement data

226
00:19:20,540 --> 00:19:26,930
you storing in the US is subject to the law of the country and the locality where it is is stored.

227
00:19:27,140 --> 00:19:34,580
Second, the proximity to your customer base and the latency, which is a very good and crucial factor

228
00:19:34,820 --> 00:19:40,670
for your business, then it is the difference between using closest region and the forest.

229
00:19:40,670 --> 00:19:44,840
Region is very small, but even a small difference in latency.

230
00:19:45,140 --> 00:19:51,950
You can impact in your customer experience also the availability of service in that region and the cost

231
00:19:51,950 --> 00:19:55,490
effectiveness because costs vary between regions.

232
00:19:55,490 --> 00:19:59,120
So for example, at2 medium will cost in the.

233
00:19:59,460 --> 00:20:06,600
Region to cent per hour, but in the Frankfort region it will cost 1.5 cents per hour.

234
00:20:06,870 --> 00:20:13,470
So the last thing we want to do to understand a city is the challenge lab for creating a static website

235
00:20:13,470 --> 00:20:14,820
for our cafe.

236
00:20:14,830 --> 00:20:21,570
So they mentioned that they want to create a website, should also provide customers with business details,

237
00:20:21,570 --> 00:20:26,070
the location of the store, the business hours and their telephone number, and they want to create

238
00:20:26,070 --> 00:20:28,260
this first website for the cafe.

239
00:20:28,290 --> 00:20:34,920
You will take the role of Nicky in this activity and work to produce the result that everyone back as

240
00:20:34,920 --> 00:20:37,140
the cafe hopes you can deliver.

241
00:20:37,410 --> 00:20:42,750
So the task are very similar to the guided lab, but here you will be given a code that you need to

242
00:20:42,750 --> 00:20:43,560
extract it.

243
00:20:43,560 --> 00:20:49,080
You create a three bucket to host your static groups like you upload the content to it and you create

244
00:20:49,080 --> 00:20:52,170
a bucket policy to grant public re access.

245
00:20:52,170 --> 00:20:58,380
And then you need to enable versioning on that AC three bucket and you need to set up a lifecycle policy

246
00:20:58,380 --> 00:21:02,760
and you will enable as well across region replication.

247
00:21:02,760 --> 00:21:05,850
So the find that a product of the lab will look like this.

248
00:21:05,850 --> 00:21:13,170
You will have the cafe website served from one Amazon S3 bucket and the data will be replicated to another

249
00:21:13,170 --> 00:21:14,400
S3 bucket.

250
00:21:14,550 --> 00:21:20,760
Thank you for seeing this class, which is adding a storage layer for that, Kathy.
