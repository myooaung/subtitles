WEBVTT
1
00:00:00.030 --> 00:00:06.480
We said one of the downside of relational database systems they are vertical is that you can't make

2
00:00:06.480 --> 00:00:13.410
them horizontally scalable because in fact for any relational database system you need at all time,

3
00:00:13.410 --> 00:00:17.880
one must know which contains that ground truth about the deed.

4
00:00:18.030 --> 00:00:18.460
Why?

5
00:00:18.540 --> 00:00:25.410
Because this is the only way we can guarantee high level of data consistency between multiple system,

6
00:00:25.410 --> 00:00:30.300
multiple branches in the banking system to access your relational database.

7
00:00:30.720 --> 00:00:37.980
So the problem with relational database and being in a situation where we cannot scale then horizontally,

8
00:00:37.980 --> 00:00:40.950
then we are left off with this scaling vertical.

9
00:00:41.040 --> 00:00:43.530
Now remember, guys, there is an upper limit.

10
00:00:43.560 --> 00:00:51.600
It is how much you can add with a specific capacity of at certain point you reach the physical limit

11
00:00:51.600 --> 00:00:55.050
of the computer hardware and you cannot sustain those database.

12
00:00:55.080 --> 00:00:55.620
Fair enough.

13
00:00:55.650 --> 00:01:03.150
Now you can scale your DB in a relational database system from a micro instances all the way to two

14
00:01:03.150 --> 00:01:05.910
for a 24 x large instance.

15
00:01:05.910 --> 00:01:10.380
And you can scale vertically with a very minimum down time.

16
00:01:10.410 --> 00:01:17.430
Now, with horizontal scaling, you can horizontally scale a relational database or read by creating

17
00:01:17.430 --> 00:01:20.190
any replica object for writing nodes.

18
00:01:20.220 --> 00:01:27.750
You are not having that option at all again because you only can have one master node which contains

19
00:01:27.840 --> 00:01:34.240
that schema of your relational database and the ground truth about you or data system in either.

20
00:01:34.260 --> 00:01:41.880
Yes, you can create up to five read replicas and up to 15 or replication is asynchronous, which means

21
00:01:41.880 --> 00:01:47.340
the data will be moved between the master node, the primary node and the reader replica.

22
00:01:47.340 --> 00:01:54.150
Without any time constraint, there will be asynchronously replicate in Aurora, for example, when

23
00:01:54.150 --> 00:02:00.200
you create an Aurora cluster, you can have up to 15 or a replica and this is an exam question.

24
00:02:00.210 --> 00:02:03.540
So in Aurora it's 15 replicas read replicas.

25
00:02:03.540 --> 00:02:10.380
But in a normal database you can have up to five read replicas like my sequel or Postgres.

26
00:02:10.900 --> 00:02:17.520
Now there is a new version of Amazon Aurora, which is Amazon Aurora Customer List Compute this can

27
00:02:17.520 --> 00:02:24.000
respond to your application automatically and is very effectively intelligence chronicle kind of scaled

28
00:02:24.000 --> 00:02:27.390
the capacity and the start up and shut down.

29
00:02:27.870 --> 00:02:32.490
Those are all the cluster based on the already met in Aurora server list.

30
00:02:32.490 --> 00:02:39.090
You pay for the number of Aurora capacity unit and they are very good for any kind of unpredictable

31
00:02:39.240 --> 00:02:39.750
work.

32
00:02:39.930 --> 00:02:47.400
You could also horizontally scale a database using what a technique of data is the filled with sharding?

33
00:02:47.400 --> 00:02:50.100
No, you can have or without sharding.

34
00:02:50.100 --> 00:02:53.790
Normally the same primary node take all the right load.

35
00:02:53.800 --> 00:03:01.320
And this means if you have a heavy like application, then in this case you will start to see a slow

36
00:03:01.320 --> 00:03:08.040
and the bad before the result is you get to create charm and in this case the database will drive into

37
00:03:08.040 --> 00:03:13.830
multiple shards or multiple nodes and those DB shards are basically work as follows.

38
00:03:13.860 --> 00:03:20.550
For example, if you have a table of employees, all the employee I.D. with even number, they go to

39
00:03:20.550 --> 00:03:26.610
the first shop with an even employee number goes to the first shard and the second shard will go all

40
00:03:26.610 --> 00:03:28.590
employees with odd number.

41
00:03:28.620 --> 00:03:35.310
Now remember, this option is only available when you want to build your database in an easy to you

42
00:03:35.310 --> 00:03:38.130
can't use sharding with Amazon on there.

43
00:03:38.160 --> 00:03:45.000
You could also use a scalability with dynamo the B table and as we know that we Dynamo DB is an equal

44
00:03:45.000 --> 00:03:52.560
database that you can scale up to meet thousands and millions of requests a second in on demand scaling

45
00:03:52.560 --> 00:03:59.220
of dynamo will be you are going to pay per request depending whether you will have a provisioned re

46
00:03:59.250 --> 00:04:05.700
capacity unit or write capacity unit, or you run out of those speed capacity unit of the unit.

47
00:04:05.730 --> 00:04:11.280
Normally use an on demand the scalability for dynamo would you be when you have a spike and a predictable

48
00:04:11.280 --> 00:04:18.480
workload in order to rapidly accommodate that me or however you could use all the scaling and this is

49
00:04:18.480 --> 00:04:25.440
will be available for all in your dynamo be table and they are very good for most application.

50
00:04:25.460 --> 00:04:28.230
So how you can implement Dynamo DB or the scaling.

51
00:04:28.230 --> 00:04:34.980
When you create an ultra scaling policy in Dynamo DB, you can basically specify the consumed capacity

52
00:04:34.980 --> 00:04:39.690
metrics and you populate those to another on a cloud, in a cloud which you can generate.

53
00:04:39.690 --> 00:04:46.500
And as soon as once those capacity units are consumed and there is no capacity unit available in Dynamo

54
00:04:46.890 --> 00:04:53.760
and this is will trigger the order scaling where the ultra scaling will add more capacity units to your

55
00:04:53.760 --> 00:04:54.260
dynamo.

56
00:04:55.140 --> 00:04:57.630
You could also scale through capacity.

57
00:04:57.630 --> 00:04:59.970
You can enable a reading and write.

58
00:05:00.180 --> 00:05:02.920
To hold politicians without throttling.

59
00:05:02.940 --> 00:05:08.760
Now, before we explain what to read and write without the throttle, throttling means because understand

60
00:05:08.760 --> 00:05:10.410
what we mean by politicians.

61
00:05:10.420 --> 00:05:17.220
We know from the cloud of practitioners that a dynamic debate table is going to be divided into politicians.

62
00:05:17.220 --> 00:05:24.280
So if this is our table and we say, for example, the employee I.D. win first, then this is our dynamo

63
00:05:24.280 --> 00:05:29.840
on seven and just accidentally we make the first name to be our partition.

64
00:05:30.480 --> 00:05:34.370
The employee you are most likely to get people with the same name.

65
00:05:34.640 --> 00:05:38.940
Let us take the and you will find there is some dependence on line.

66
00:05:38.970 --> 00:05:39.450
Okay.

67
00:05:39.450 --> 00:05:45.790
Or but in this particular case, if you have more than one person with the first name Sam or above,

68
00:05:45.810 --> 00:05:48.000
you are going to create a hot partition.

69
00:05:48.000 --> 00:05:49.500
And that's what partition means.

70
00:05:49.530 --> 00:05:51.780
The partition itself is very large.

71
00:05:51.780 --> 00:05:57.360
So every time you want to read or write to that partition, you will experience a bad performance.

72
00:05:57.370 --> 00:06:04.030
So when you have one of these hot partition and normally is caused by deciding to use a very bad partition

73
00:06:04.050 --> 00:06:10.500
key, like for example, you can use the partition key like that time of the day to capture, for example,

74
00:06:10.510 --> 00:06:11.400
sensory data.

75
00:06:11.400 --> 00:06:17.730
In this particular scenario, you find each partition for each hour is very large and very hot at the

76
00:06:17.730 --> 00:06:21.210
same time because you keep reading and writing to the same partition.

77
00:06:21.330 --> 00:06:26.430
So you could enable with dynamic DB three than writing to partition without it throttling.

78
00:06:26.460 --> 00:06:33.180
Throttling means you add more capacity unit to read and try to add the partition, and that can automatically

79
00:06:33.180 --> 00:06:37.470
increase the throughput capacity for the partition that receives more.

80
00:06:37.470 --> 00:06:46.650
It's now the scaling through capacity is enabled for every dynamo DB you could use as well an adaptive

81
00:06:46.650 --> 00:06:51.660
capacity for mastering or managing the scalability of your dynamo.

82
00:06:52.530 --> 00:06:53.910
Let us look to this example.

83
00:06:53.910 --> 00:06:55.200
We are for partition.

84
00:06:55.360 --> 00:06:59.910
Let's say the partition is for all people, starting with their family.

85
00:06:59.910 --> 00:07:03.640
Last name eight, then big, then C and then D.

86
00:07:03.690 --> 00:07:11.070
Now the total number of provision capacity for your dynamo meeting table is 400 write capacity unit

87
00:07:11.070 --> 00:07:12.180
up to this level.

88
00:07:12.180 --> 00:07:15.620
If you look here, each partition is consuming 50.

89
00:07:15.630 --> 00:07:25.320
So this is 50 plus 50 plus 50 here and plus 50, which means we are getting now total of 200 write capacity

90
00:07:25.320 --> 00:07:25.660
unit.

91
00:07:26.220 --> 00:07:33.260
If one of those partition is partition for any reason due to a high volume, for example, or let's

92
00:07:33.300 --> 00:07:39.270
say we got multiple employees with the last name is starting with B, then we want to keep rising to

93
00:07:39.270 --> 00:07:41.120
partition in this example.

94
00:07:41.120 --> 00:07:48.270
Then with the aid of adaptive capacity in item would be it will allocate the remaining of the provision

95
00:07:48.330 --> 00:07:51.750
capacity unit to meet the demand of partition for.

96
00:07:51.870 --> 00:07:58.790
So instead of having only 50 up to here, we can just add another 50 adaptive capacity to meet this

97
00:07:58.790 --> 00:08:06.660
side of the demand to this particular partition, even if that happened to be a very hot partition and

98
00:08:06.660 --> 00:08:14.700
we need more capacity in this case, the adaptive capacity can is throttle and increase your allocated

99
00:08:14.700 --> 00:08:22.130
capacity unit from 100 by adding another 50, which makes the total consumed write capacity unit for

100
00:08:22.140 --> 00:08:24.660
partition for 150.

101
00:08:24.660 --> 00:08:29.700
As you can see here, the total consumed capacity in this case is 300.

102
00:08:29.790 --> 00:08:37.320
Still, we are getting and we still have room to add more adaptive capacity because we have available

103
00:08:37.320 --> 00:08:39.000
400 vide capacity.

104
00:08:39.240 --> 00:08:41.370
This is a very important sort of exam.

105
00:08:41.370 --> 00:08:48.600
Normally they ask about this scenario in the solution architect exam, so pay attention to it now adaptive

106
00:08:48.600 --> 00:08:53.220
capacity and this is also a very important this adaptive capacity.

107
00:08:53.220 --> 00:09:00.360
They are not fixing hot partition because you have to fix the whole partition by using a proper partition

108
00:09:00.360 --> 00:09:00.720
key.

109
00:09:00.720 --> 00:09:07.470
For example, if you choose the user I.D. and the application has many user I.D., this make it, for

110
00:09:07.470 --> 00:09:09.570
example, it's an online store.

111
00:09:09.570 --> 00:09:16.500
The same user could do between 5 to 10 orders or 0 to 10 orders within the partition will be relatively

112
00:09:16.500 --> 00:09:16.950
small.

113
00:09:16.980 --> 00:09:23.160
But if you choose, for example, this data stored your captioning session data between users accessing

114
00:09:23.160 --> 00:09:26.760
your website every time they get to or to you store them.

115
00:09:26.760 --> 00:09:30.120
Every time they get five or five, you store them.

116
00:09:30.180 --> 00:09:32.720
This is will be very bad partition key.

117
00:09:32.760 --> 00:09:33.050
Why?

118
00:09:33.180 --> 00:09:38.570
Because this is will make a big partition a big table only for two or two.

119
00:09:38.610 --> 00:09:45.300
And let's say we kept our as well the user I.D. because you only have two options or three options for

120
00:09:45.300 --> 00:09:46.290
a status code.

121
00:09:46.290 --> 00:09:50.580
The item creation date is also a very bad partition.

122
00:09:50.790 --> 00:09:51.180
Why?

123
00:09:51.180 --> 00:09:58.470
Because the creation date of the idea we could create there our ten or 20 or thousands of item in your

124
00:09:58.470 --> 00:09:59.910
table in this case, this.

125
00:10:00.000 --> 00:10:07.950
Make the item table for that particular partition per hour is very large and will be considered as a

126
00:10:08.190 --> 00:10:08.790
partition.

127
00:10:09.740 --> 00:10:18.830
The idea is a good idea if and only if each device access data similar and determine if they are accessing

128
00:10:18.830 --> 00:10:25.010
your data at the same interval or very close to each other, then it is a good choice, but it will

129
00:10:25.010 --> 00:10:28.360
not be a good choice if many devices are tracked.

130
00:10:28.370 --> 00:10:35.020
One is much more popular than all the others because this makes that device which is very popular.

131
00:10:35.030 --> 00:10:41.000
The partition for that device will be very hot because it will get a lot of query, a lot of requests

132
00:10:41.000 --> 00:10:42.290
and a lot of fear.

133
00:10:42.680 --> 00:10:45.950
So this is such a mystery, guys, about this delivery.

134
00:10:46.100 --> 00:10:52.850
So you can use push button scaling to vertically scale any compute capacity of your RDF.

135
00:10:52.880 --> 00:11:00.140
You can read a replica or sharp horizontal scale, your Autodesk DV in instances, and you can use Amazon

136
00:11:00.140 --> 00:11:04.040
Aurora where you can choose up to 15 read replicas.

137
00:11:04.040 --> 00:11:11.570
Aurora serverless data scale resources automatically based on the minimum and the maximum capacity specification.

138
00:11:11.570 --> 00:11:19.580
The RCU and the W the you write Amazon data will be on demand offers a pay per request pricing model

139
00:11:19.610 --> 00:11:26.240
and dynamo DB auto scaling uses Amazon application or scaling to dynamically adjust to provision just

140
00:11:26.240 --> 00:11:32.540
throughput capacity dynamo DB Adaptive capacity works by automatically increasing its robot capacity

141
00:11:32.540 --> 00:11:36.340
for partitions that receive more however.
