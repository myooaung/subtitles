WEBVTT
1
00:00:04.700 --> 00:00:11.000
Hello and welcome to this new bite into toile So we're going to start our implementation now to solve

2
00:00:11.150 --> 00:00:12.900
our classification problem.

3
00:00:13.190 --> 00:00:19.880
This classification problem is to predict if a tumor is cancerous or benign and we're going to predict

4
00:00:19.880 --> 00:00:26.640
that thanks to nine independent variables which are for example the clum thickness or the mitosis.

5
00:00:26.780 --> 00:00:32.300
We are going to use again gradient boosting because there will not be preliminary analysis of the data.

6
00:00:32.300 --> 00:00:38.810
And the problem you know whether or not it is linear or with outliers or with a certain distribution

7
00:00:38.810 --> 00:00:43.900
of the features we want do such Bryer analysis but we actually don't need to because the model we're

8
00:00:43.910 --> 00:00:46.470
going to use is one of the best models.

9
00:00:46.550 --> 00:00:53.240
One of the most powerful models of machine learning and besides using this gradient boosting model for

10
00:00:53.240 --> 00:00:59.450
classification will give me the opportunity to show you how you can switch very easily from regression

11
00:00:59.660 --> 00:01:00.700
to classification.

12
00:01:00.890 --> 00:01:02.810
And that's our starting point right now.

13
00:01:02.840 --> 00:01:08.960
You know as you can see I'm having the regression implementation and I'm going to show you how you can

14
00:01:09.230 --> 00:01:15.100
very efficiently switch to classification by only changing a few parameters.

15
00:01:15.110 --> 00:01:16.520
All right so let's do this.

16
00:01:16.520 --> 00:01:21.980
First of all the first thing we need to do is restart the kernel because we want to reset everything

17
00:01:22.160 --> 00:01:23.390
and restart to zero.

18
00:01:23.390 --> 00:01:27.880
So here you click on this tool button and then you click on restart kernel.

19
00:01:27.950 --> 00:01:31.440
That's the first thing you should do if it's not already done.

20
00:01:32.000 --> 00:01:36.160
And then the second thing we're going to do here we go the kernel is restarted.

21
00:01:36.170 --> 00:01:41.650
Now the second thing we're going to do is create a new file of course because this will be the python

22
00:01:41.680 --> 00:01:45.220
file for declassification So let's do this.

23
00:01:45.260 --> 00:01:49.130
You click on file here you file and then here we go.

24
00:01:49.130 --> 00:01:50.300
Perfect.

25
00:01:50.310 --> 00:01:52.560
We're going to select all this to lead it.

26
00:01:52.800 --> 00:01:56.900
And now since we want to switch easily from aggression to classification.

27
00:01:56.960 --> 00:02:04.340
Well what I'm going to do is take my regression implementation select everything copy go back to this

28
00:02:04.460 --> 00:02:06.620
future classification file.

29
00:02:06.740 --> 00:02:14.750
And here let's just replace regression by classification perfect no worries that's not the only thing

30
00:02:14.750 --> 00:02:21.560
we have to do but you'll be surprised that we have actually very few things left to do to tackle our

31
00:02:21.650 --> 00:02:23.350
classification problem.

32
00:02:23.420 --> 00:02:30.200
So but before we do any more changes in this implementation remember what we have to do that very very

33
00:02:30.200 --> 00:02:34.560
important we must be in the right working directory for that.

34
00:02:34.730 --> 00:02:37.370
So that's why right now I'm going to fall explore.

35
00:02:37.580 --> 00:02:40.240
And then here is the regression folder.

36
00:02:40.250 --> 00:02:47.540
So now we want to go back to our main introduction to data science folder and now we want to go to the

37
00:02:47.540 --> 00:02:49.010
classification folder.

38
00:02:49.250 --> 00:02:56.690
And as you saw in the Bruce article I told you to download this breast cancer as well and then put it

39
00:02:56.870 --> 00:03:00.310
in your classification folder and that's exactly what I did here.

40
00:03:00.440 --> 00:03:01.740
I'm going to show you right now.

41
00:03:01.760 --> 00:03:06.080
Remember you can directly click on the dataset here and there we go.

42
00:03:06.080 --> 00:03:11.530
This time it doesn't even open some text or excel like you know it did for regression.

43
00:03:11.540 --> 00:03:12.610
It had to open excel.

44
00:03:12.620 --> 00:03:16.520
But here it actually opened this as we felt directly in spider.

45
00:03:16.620 --> 00:03:18.310
So that's even more amazing.

46
00:03:18.440 --> 00:03:18.920
And there we go.

47
00:03:18.920 --> 00:03:20.140
You can see what I did.

48
00:03:20.240 --> 00:03:27.050
I put simply the names of the attributes as the first line line one here and I just separated them by

49
00:03:27.500 --> 00:03:34.330
commas because this is a C as we fell which means that the values are separated by commas.

50
00:03:34.340 --> 00:03:37.150
So yes the means comma separated values.

51
00:03:37.370 --> 00:03:41.230
And so you just need to separate the names by come as again as the values here.

52
00:03:41.420 --> 00:03:47.360
And then as I told you I removed all the lines all the observations that contained question marks that

53
00:03:47.360 --> 00:03:48.990
is missing values.

54
00:03:49.040 --> 00:03:49.870
Perfect.

55
00:03:49.870 --> 00:03:53.020
And now let's get our implementation ready.

56
00:03:53.330 --> 00:03:59.570
So first we're going to save this classification in this folder a working directory folder.

57
00:03:59.690 --> 00:04:04.860
And so here we need to go back to Introduction to data science and now to classification.

58
00:04:04.940 --> 00:04:10.510
There we go and we can just call it the spice file class c v.

59
00:04:11.130 --> 00:04:13.040
Cl. save.

60
00:04:13.220 --> 00:04:14.340
And here we go.

61
00:04:14.360 --> 00:04:15.600
We are ready to start.

62
00:04:16.010 --> 00:04:21.650
All right so the best way to switch from regression to classification is just to check every line.

63
00:04:21.650 --> 00:04:26.490
You know we check every line we need to see if we have to change something and if we change it.

64
00:04:26.510 --> 00:04:28.800
All right so let's do it line by line.

65
00:04:29.030 --> 00:04:30.710
The first line is importing the libraries.

66
00:04:30.770 --> 00:04:32.450
Yes we will need depends.

67
00:04:32.450 --> 00:04:33.500
Library again.

68
00:04:33.590 --> 00:04:37.240
So we don't touch this then second line.

69
00:04:37.280 --> 00:04:38.800
We import the dataset here.

70
00:04:39.020 --> 00:04:42.320
Well here we have actually two things to change.

71
00:04:42.350 --> 00:04:46.520
The first thing is related to the fact that this time we don't have an excel file.

72
00:04:46.520 --> 00:04:53.540
We have a C as well and therefore we're not going to use the read underscore Excel function by Penders

73
00:04:53.870 --> 00:04:58.280
but just name the read underscore C as V.

74
00:04:58.490 --> 00:05:03.770
Basically with that you have two main functions to import datasets the read underscores the function

75
00:05:03.770 --> 00:05:08.950
to import cc files and the read underscore as a function to import Exel files.

76
00:05:09.110 --> 00:05:15.680
Ok so read on this course and then of course the second thing we need to change is the name of the data

77
00:05:15.680 --> 00:05:22.590
set which is no longer FULDE fives times to underscore that Excel as X.

78
00:05:22.730 --> 00:05:31.120
But this time it's more meaningful maybe breast the score cancer but C as we.

79
00:05:31.250 --> 00:05:32.950
Let's not forget this.

80
00:05:33.410 --> 00:05:36.560
OK so now it's better and we can actually try.

81
00:05:36.560 --> 00:05:37.040
If you want.

82
00:05:37.040 --> 00:05:39.760
Let's import pends first.

83
00:05:39.770 --> 00:05:40.510
There we go.

84
00:05:40.520 --> 00:05:41.750
Ben is important.

85
00:05:41.810 --> 00:05:44.060
And now let's import the dataset.

86
00:05:44.090 --> 00:05:45.320
Let's do this.

87
00:05:45.320 --> 00:05:46.370
Execute.

88
00:05:46.370 --> 00:05:47.070
Here we go.

89
00:05:47.150 --> 00:05:47.980
Let's check.

90
00:05:47.990 --> 00:05:49.360
Variable explore.

91
00:05:49.380 --> 00:05:51.030
And here is our dataset.

92
00:05:51.110 --> 00:06:02.180
So it now has 683 observations and then we have our 11 attributes which so far contain all the original

93
00:06:02.180 --> 00:06:05.360
attributes we had in the original dataset.

94
00:06:05.390 --> 00:06:09.030
So let's check them out quickly we have the symbol code number.

95
00:06:09.110 --> 00:06:13.230
The Klump thickness uniformity of cell size.

96
00:06:13.390 --> 00:06:15.950
The uniformity of cell shape.

97
00:06:16.130 --> 00:06:17.880
The marginal addition.

98
00:06:18.140 --> 00:06:26.390
The single epithelial cell size Bernoulli Blen chromatin normal nuclear we and our class and I remind

99
00:06:26.390 --> 00:06:30.720
that to means benign and for means malignant.

100
00:06:30.740 --> 00:06:37.580
All right so again these are our independent variables optimises here and from all these independent

101
00:06:37.580 --> 00:06:43.640
variables we're going to try to understand some correlations between them and this class here.

102
00:06:43.640 --> 00:06:49.460
Whether or not the tumor is cancerous to eventually predict if this class here which is the dependent

103
00:06:49.460 --> 00:06:51.830
variable has value to for.

104
00:06:51.980 --> 00:06:54.400
All right so that's the problem.

105
00:06:54.530 --> 00:06:56.450
And now the solution.

106
00:06:56.480 --> 00:06:58.630
So next line x.

107
00:06:58.790 --> 00:07:02.140
So here there is something interesting which I already mentioned.

108
00:07:02.240 --> 00:07:09.650
It's the fact that you know remember X takes a subset of the data set that contains all the lines and

109
00:07:09.650 --> 00:07:12.340
then all the columns except the last one.

110
00:07:12.340 --> 00:07:12.630
Right.

111
00:07:12.640 --> 00:07:17.690
Because the last one is a dependent variable and X only takes the independent variables.

112
00:07:17.710 --> 00:07:24.850
However among these attributes here there is definitely one that has literally no impact on the dependent

113
00:07:24.850 --> 00:07:25.570
variable.

114
00:07:25.600 --> 00:07:26.680
You can guess what this is.

115
00:07:26.680 --> 00:07:28.820
This is the simple code number.

116
00:07:28.930 --> 00:07:31.660
Of course this is just a key identifier.

117
00:07:31.660 --> 00:07:38.560
That of course has no impact or no effect or not any relationship with the fact that a tumor is cancerous

118
00:07:38.590 --> 00:07:39.370
or benign.

119
00:07:39.550 --> 00:07:44.890
So we can remove this even if the gradient boosting algorithm will detect that it has nothing to do

120
00:07:44.890 --> 00:07:45.530
with it.

121
00:07:45.580 --> 00:07:47.470
We can already remove it.

122
00:07:47.470 --> 00:07:52.420
That will not only help our gradient boosting mo but also this makes much more sense.

123
00:07:52.660 --> 00:07:59.500
And that's exactly what we have to do here in this line to get the array x in the pin enviro's.

124
00:07:59.620 --> 00:08:05.180
We just want to get everything except the last one of course because that's deepening.

125
00:08:05.220 --> 00:08:10.880
I will also accept the first one because the simple code number was the first column.

126
00:08:11.050 --> 00:08:13.450
And remember in Python the indexes started 0.

127
00:08:13.480 --> 00:08:18.370
So this first column has an exit row and therefore we don't want to include the index 0 and therefore

128
00:08:18.580 --> 00:08:24.880
instead of starting here at zero because remember that by not putting anything here that means that

129
00:08:24.880 --> 00:08:26.390
we're starting at zero.

130
00:08:26.470 --> 00:08:28.990
And so instead of starting at zero here.

131
00:08:28.990 --> 00:08:33.790
Well we will start at 1 in order to not include this first column.

132
00:08:34.060 --> 00:08:34.870
And there we go.

133
00:08:34.870 --> 00:08:36.670
Now we will have a better X..

134
00:08:36.820 --> 00:08:37.970
We can create it.

135
00:08:38.110 --> 00:08:39.090
And here we go.

136
00:08:39.160 --> 00:08:46.720
We obtained this time indeed a matrix of nine independent variables and you can check that these are

137
00:08:46.750 --> 00:08:51.150
the right values of the independent variables starting from the clum thickness.

138
00:08:51.160 --> 00:08:51.770
OK.

139
00:08:52.180 --> 00:08:53.630
So that's good for X.

140
00:08:53.660 --> 00:08:56.620
And now why do you think we need to change something here.

141
00:08:56.620 --> 00:09:02.860
Well of course absolutely not because the dependent variable is still in the last column.

142
00:09:02.950 --> 00:09:05.290
It is this class here.

143
00:09:05.310 --> 00:09:12.660
That's the last column and here remember by taking the index minus one here for the column.

144
00:09:12.790 --> 00:09:18.010
Well that means that we're taking the last column because minus one is exactly the index of the last

145
00:09:18.010 --> 00:09:18.560
column.

146
00:09:18.910 --> 00:09:19.750
And there we go.

147
00:09:19.750 --> 00:09:21.690
We don't have to change anything here.

148
00:09:21.720 --> 00:09:25.810
We are ready to execute this perfect.

149
00:09:25.810 --> 00:09:26.650
We can check it out.

150
00:09:26.680 --> 00:09:34.360
Yes that's definitely the dependent variable with the two letters two for benign and for for cancers.

151
00:09:34.390 --> 00:09:35.230
OK.

152
00:09:35.640 --> 00:09:36.300
OK.

153
00:09:36.530 --> 00:09:42.370
And now let's move onto the next line or even the next code section because of course here we don't

154
00:09:42.370 --> 00:09:43.420
have anything to change.

155
00:09:43.420 --> 00:09:50.110
We will still import to try and test it function to you know have this training set to train the model

156
00:09:50.350 --> 00:09:53.320
and then this test set to test the model.

157
00:09:53.470 --> 00:09:56.100
So of course we need this nothing to change.

158
00:09:56.140 --> 00:10:04.510
Here let's see we are creating x strain X test Y train Y test which we get from the trenches function

159
00:10:04.870 --> 00:10:12.070
which takes as arguments the full matrix of independent variables X the fool dependent variable y and

160
00:10:12.330 --> 00:10:18.190
test size of 20 percent meaning that we will get 80 percent of the data in the training set to train

161
00:10:18.190 --> 00:10:22.130
the model and 20 percent of the data set to test them all.

162
00:10:22.150 --> 00:10:23.580
And that's fair enough.

163
00:10:23.620 --> 00:10:27.630
That's actually the value that I recommend to take most of the time.

164
00:10:27.820 --> 00:10:33.010
And then a random state equals zero if you want if you want to get exactly the same result as I do that

165
00:10:33.010 --> 00:10:36.630
is exactly the same training set and tested.

166
00:10:36.640 --> 00:10:38.640
All right so perfect nothing to change here.

167
00:10:38.650 --> 00:10:41.060
I'm selecting these two lines.

168
00:10:41.200 --> 00:10:49.930
And here we go we have our training sets composed of X train and y train and our test set composed of

169
00:10:49.930 --> 00:10:51.570
X test and Y test.

170
00:10:51.640 --> 00:10:52.930
We don't need to check this.

171
00:10:52.930 --> 00:10:53.980
Everything's all good.

172
00:10:54.130 --> 00:10:55.760
If you really have a look if you are.

173
00:10:56.220 --> 00:11:03.910
All right so now next line that's when things get really interesting because that's really at this point

174
00:11:03.910 --> 00:11:11.320
that the switch is so easy you know gradient boosting works as well for regression as for classification.

175
00:11:11.440 --> 00:11:12.970
And now I'm going to make the switch.

176
00:11:12.970 --> 00:11:14.170
It's so easy.

177
00:11:14.170 --> 00:11:21.160
The only thing that you need to do and you might guess what it is is to replace regressors by classifier

178
00:11:22.150 --> 00:11:27.220
and that's it your gradient boosting class for class is really done here of course need to change the

179
00:11:27.220 --> 00:11:28.510
name of the class again.

180
00:11:28.640 --> 00:11:32.770
Gradient boosting class fire didn't make everything perfect.

181
00:11:32.770 --> 00:11:36.850
We will of course replace the name of our object here which I remind is our model.

182
00:11:36.850 --> 00:11:43.540
You know the model itself as opposed to this class which is the symbol of instructions that allow to

183
00:11:43.600 --> 00:11:49.000
define the model with all the great and boosting algorithm plus the tools the methods we can use like

184
00:11:49.240 --> 00:11:56.310
training it on the trainset or predicting some new observations either on the set or any new observations.

185
00:11:56.350 --> 00:12:03.700
Well here since that's exactly our model we're going to call it of course classifier classifier.

186
00:12:03.820 --> 00:12:10.360
And then I'm going to copy this because that's the same here I'm replacing the aggressor by classifying

187
00:12:11.180 --> 00:12:11.930
and that's it.

188
00:12:12.120 --> 00:12:15.630
Here's how you do this which basically very simple.

189
00:12:15.630 --> 00:12:21.640
We just had to replace regressors by classifiers with capital C capital CS.

190
00:12:21.810 --> 00:12:23.130
So that's pretty amazing.

191
00:12:23.130 --> 00:12:27.930
Not only it takes again three lines of code but we didn't have much effort to do.

192
00:12:27.930 --> 00:12:32.160
And you'll see that the results are going to be really amazing.

193
00:12:32.160 --> 00:12:39.130
OK so let's create our moral that is the green boost the morale and let's train it on the training set.

194
00:12:39.150 --> 00:12:40.300
We are ready for that.

195
00:12:40.350 --> 00:12:41.520
Let's do this.

196
00:12:41.520 --> 00:12:47.670
Here we go all the different parameters again taken are most ready not only it is ready but it is also

197
00:12:47.670 --> 00:12:48.380
trained.

198
00:12:48.420 --> 00:12:52.150
So now we are ready to predict the test results.

199
00:12:52.320 --> 00:12:57.470
So I get a little warning here telling me that there is an undefined aggressor.

200
00:12:57.540 --> 00:13:07.040
Of course that's the final change the very last change we have to do replacing regressors by classifying.

201
00:13:07.380 --> 00:13:11.410
And there we go when they're ready to get the final results.

202
00:13:11.580 --> 00:13:12.510
So let's do this.

203
00:13:12.510 --> 00:13:17.130
I'm going to select this line and execute.

204
00:13:17.490 --> 00:13:18.360
Perfect.

205
00:13:18.420 --> 00:13:18.880
I have.

206
00:13:18.900 --> 00:13:19.510
Why.

207
00:13:19.590 --> 00:13:26.130
I have one test which will allow me to compare the predictions of whether or not a tumor is cancerous

208
00:13:26.790 --> 00:13:33.450
with the real the real ground truth containing the truth whether or not this tumor is cancerous.

209
00:13:33.510 --> 00:13:35.180
That's values and why tests.

210
00:13:35.400 --> 00:13:38.770
Let's do this let's open Whiting's first.

211
00:13:38.790 --> 00:13:40.270
That's the real values.

212
00:13:40.560 --> 00:13:43.240
Then why spread the predictions.

213
00:13:43.440 --> 00:13:46.150
Let's put them just next to it.

214
00:13:46.350 --> 00:13:47.810
And here we go.

215
00:13:47.820 --> 00:13:49.200
We can compare.

216
00:13:49.200 --> 00:13:55.090
So I can already see like you know with a very quick look that the predictions are amazing.

217
00:13:55.320 --> 00:13:57.100
Let's have a look at them one by one.

218
00:13:57.120 --> 00:14:03.800
So the first observation which contains a certain uniformity of cell size or number of mitosis.

219
00:14:03.930 --> 00:14:09.990
Well for this observation for this particular tumor the truth is that it is benign.

220
00:14:10.050 --> 00:14:13.360
And our model predicted that it was indeed benign.

221
00:14:13.520 --> 00:14:15.750
So perfect for aspiration correct.

222
00:14:15.870 --> 00:14:18.440
Second prediction the tumor is benign.

223
00:14:18.570 --> 00:14:22.070
The tumor was predicted to be benign third observation.

224
00:14:22.110 --> 00:14:24.150
The tumor is cancerous.

225
00:14:24.150 --> 00:14:25.320
That's the truth.

226
00:14:25.320 --> 00:14:29.520
And our moral predicted that it was indeed cancerous.

227
00:14:29.520 --> 00:14:31.460
Same for the fourth observation.

228
00:14:31.490 --> 00:14:35.660
Cancer is the truth and cancer is the prediction.

229
00:14:35.820 --> 00:14:42.620
And again we have three benign tumors here that were also predicted to be benign cancers and we're here

230
00:14:42.630 --> 00:14:45.060
for real which are most predicted.

231
00:14:45.210 --> 00:14:47.120
And actually I'm not seeing any error here.

232
00:14:47.130 --> 00:14:55.050
We can see that the predictions are really impressive but I have actually a better way to check the

233
00:14:55.050 --> 00:14:56.800
number of incorrect predictions actually.

234
00:14:56.870 --> 00:15:03.310
No we're not going to look at all the 690 lines and look for the incorrect predictions.

235
00:15:03.300 --> 00:15:07.390
Instead we're going to make what we call a confusion matrix.

236
00:15:07.530 --> 00:15:13.620
And in this matrix basically we'll see the number of correct predictions and also the number of incorrect

237
00:15:13.620 --> 00:15:14.440
predictions.

238
00:15:14.550 --> 00:15:19.260
And that will be very interesting to see because not only will see the number of incorrect predictions

239
00:15:19.320 --> 00:15:26.190
out of the 683 observations but also that will allow us to compute the accuracy of the model which is

240
00:15:26.190 --> 00:15:32.580
simply one minus the error rate and the error is of course the number of incorrect predictions divided

241
00:15:32.580 --> 00:15:36.800
by the total number of observations that will be very interesting to see.

242
00:15:36.810 --> 00:15:42.420
We'll check that in the next its oil and at the same time I will introduce you to this very important

243
00:15:42.420 --> 00:15:44.380
concept to understand and data science.

244
00:15:44.460 --> 00:15:46.990
The false positives and the false negatives.

245
00:15:47.460 --> 00:15:49.740
That's going to be the final tutorial of our section.

246
00:15:49.860 --> 00:15:51.540
Until then there is science.
