1
00:00:00,520 --> 00:00:05,830
Hello and welcome to the final tutorial of the section we're going to make the last function of this

2
00:00:06,180 --> 00:00:12,510
D2 in class which is going to be of course the load function that naturally comes after the same function.

3
00:00:12,580 --> 00:00:17,450
You save your model and then you want to be able to load it whenever you go back to the application.

4
00:00:17,500 --> 00:00:18,940
So let's do this.

5
00:00:18,940 --> 00:00:22,420
We're going to make def then load.

6
00:00:22,480 --> 00:00:29,230
We call the class function load again this load function will take as arguments self and you probably

7
00:00:29,230 --> 00:00:35,210
guess where this self will be for it will be exactly to load what was saved in the same function.

8
00:00:35,230 --> 00:00:37,170
So we will take self-direct morrow.

9
00:00:37,210 --> 00:00:39,310
And of course solve that optimizer.

10
00:00:39,550 --> 00:00:42,620
So the self-heal will be for the model and the optimizer.

11
00:00:43,030 --> 00:00:46,530
So then Cullin And now let's slow the model.

12
00:00:46,900 --> 00:00:53,350
So since Dymo is in the last brindled ph file we want to make sure this file exist and therefore that's

13
00:00:53,350 --> 00:00:54,620
what we're going to start with.

14
00:00:54,690 --> 00:01:00,520
We're going to make an IF condition to make sure that this file exists and if it exists we will be loading

15
00:01:00,790 --> 00:01:05,160
what we have in the dictionary which is in this last print PDA.

16
00:01:05,500 --> 00:01:13,430
So we start with an if then we're going to take our operating system and the path leading to this last

17
00:01:13,450 --> 00:01:14,980
brain that PVH found.

18
00:01:15,160 --> 00:01:19,730
So that path is exactly the path that leads to the working directory folder.

19
00:01:19,810 --> 00:01:26,530
So as far as I'm concerned that's this fast desktop than my artificial intelligence it is a folder then

20
00:01:26,530 --> 00:01:32,080
model one self-driving car and then the module one self-driving car folder there is this folder here

21
00:01:32,080 --> 00:01:33,970
with the last print that you found.

22
00:01:34,210 --> 00:01:39,570
And then we're going to add to what is a file file this one.

23
00:01:39,850 --> 00:01:40,990
So that's a function.

24
00:01:41,080 --> 00:01:42,700
So I'm going to add some parenthesis.

25
00:01:42,850 --> 00:01:48,040
And inside the parenthesis I'm going to input the name of the file name of the file that contains the

26
00:01:48,040 --> 00:01:50,810
model that is last brain de-th.

27
00:01:50,890 --> 00:01:53,660
So we have to put it in quotes.

28
00:01:53,860 --> 00:02:04,000
And so I'm entering last brain that de-th and so is file last brain that page will return true if the

29
00:02:04,000 --> 00:02:10,750
file that's friend that age exists and falls if it doesn't exist and therefore this if condition means

30
00:02:10,990 --> 00:02:13,410
if we have the last render.

31
00:02:13,630 --> 00:02:18,110
Well now working directory folder then let's go.

32
00:02:18,110 --> 00:02:19,900
What's going to happen in that case.

33
00:02:19,900 --> 00:02:22,470
In that case if this file exists.

34
00:02:22,660 --> 00:02:28,150
Well first we're going to print something to say that you know we're loading the morals of for example

35
00:02:28,150 --> 00:02:35,900
you can say a little arrow and then loading checkpoints with three little dots.

36
00:02:35,980 --> 00:02:36,320
All right.

37
00:02:36,340 --> 00:02:38,350
That's just to say we're loading tomorrow.

38
00:02:38,620 --> 00:02:41,050
And then of course we're going to load the model.

39
00:02:41,080 --> 00:02:46,690
So the moral and the optimizer and we're going to put what we load in a will that I'm going to call

40
00:02:46,840 --> 00:02:54,040
checkpoint equals and that's where we're going to use the load function to load what was saved in the

41
00:02:54,040 --> 00:02:54,920
same function.

42
00:02:55,150 --> 00:03:02,530
So of course this is a function from the torch library to torch dot and the name of the Sloat function

43
00:03:02,530 --> 00:03:08,990
is simply that parenthesis and inside the parenthesis according to you what do we need to input.

44
00:03:09,190 --> 00:03:15,340
Well very simply we need to input the file that contains our saved tomorrow and are saved optimizing

45
00:03:15,370 --> 00:03:24,150
optimizer so we simply need to put the name of the file which is last bring dot de-th.

46
00:03:24,390 --> 00:03:30,300
Let's bring that BGH and we load this file only in the condition that this file exists.

47
00:03:30,300 --> 00:03:33,040
So that's why we had to call this condition here.

48
00:03:33,860 --> 00:03:36,590
OK so now that we loaded the model and the optimizer.

49
00:03:36,690 --> 00:03:43,520
Well what we're going to do is update separately our model and the optimizer because actually we loaded

50
00:03:43,530 --> 00:03:48,080
the parameters we loaded the weights and the parameters of the optimizer.

51
00:03:48,160 --> 00:03:56,490
So now what we need to do is update our existing model which is this one cell block model and our existing

52
00:03:56,550 --> 00:04:04,890
optimizer solved that optimizer with the parameters with the weights that are in this last Brender.

53
00:04:05,150 --> 00:04:10,680
So we simply need to make these two update separately and to do this we're going to use a method from

54
00:04:10,680 --> 00:04:11,980
the torture modules.

55
00:04:12,150 --> 00:04:17,970
So there's going to be inheritance which will allow us to use this method that is called Load state

56
00:04:17,970 --> 00:04:25,500
dict and the slowed static method will allow us to date all the parameters of our model and our optimizer.

57
00:04:25,500 --> 00:04:28,710
So let's do this and let's start by updating our models.

58
00:04:28,710 --> 00:04:35,820
So we take our model which is self taught model since the self-taught model inherits from the methods

59
00:04:35,820 --> 00:04:40,530
of the torch module to use the load state dict method.

60
00:04:40,650 --> 00:04:43,190
So that's the method we're taking from the inheritance.

61
00:04:43,380 --> 00:04:48,490
And thanks to this method we are going to update all the parameters of the model that is all the way.

62
00:04:48,810 --> 00:04:54,120
And so what we need to put in this state Dick method is our checkpoint very well.

63
00:04:54,180 --> 00:04:56,210
That is the result of the load function.

64
00:04:56,290 --> 00:05:04,080
So checkpoints then brackets and now we need to enter the name of the key that corresponds to our model

65
00:05:04,470 --> 00:05:07,270
that is that corresponds to subduct model state date.

66
00:05:07,560 --> 00:05:09,490
And that is state.

67
00:05:09,800 --> 00:05:17,970
So in checkpoints and the brackets we enter in quote States underscore tickets and this line of code

68
00:05:18,210 --> 00:05:20,170
will update your model.

69
00:05:20,220 --> 00:05:23,440
That is a tool that awaits the parameters of your model.

70
00:05:23,580 --> 00:05:26,330
And now we need to do the same for the optimizer.

71
00:05:26,520 --> 00:05:28,380
And that's going to be almost the same.

72
00:05:28,470 --> 00:05:33,290
So I'm going to copy this line pasted below.

73
00:05:33,420 --> 00:05:40,620
And so this time we're going to date not tomorrow but the optimizer that optimizer.

74
00:05:40,650 --> 00:05:47,730
Then again we use to load state Dick method that inherits from the torch module methods and we apply

75
00:05:47,730 --> 00:05:52,020
this function to checkpoint of state dict.

76
00:05:52,170 --> 00:05:56,970
But the key that corresponds to the optimizer and that is optimizer.

77
00:05:57,150 --> 00:06:02,260
So here we just replace a date by up to Mizer.

78
00:06:02,280 --> 00:06:03,180
There we go.

79
00:06:03,180 --> 00:06:04,880
Here we update the weights of the.

80
00:06:04,920 --> 00:06:07,600
And here we update the parameters of the optimizer.

81
00:06:08,340 --> 00:06:15,870
Perfect and then just to finish we can print a little done like that.

82
00:06:15,870 --> 00:06:21,510
And finally we just need to specify what happens if this condition is not respected.

83
00:06:21,570 --> 00:06:29,670
That is if there is not a lot of Pythia trial and so we just need to add an else than Colin and Cindy

84
00:06:29,700 --> 00:06:32,630
we're just going to say that there is no such file.

85
00:06:32,640 --> 00:06:34,110
Let's bring the PTA age.

86
00:06:34,200 --> 00:06:44,350
So we're just going to print something like No check point out and three little that if you want.

87
00:06:44,360 --> 00:06:45,180
All right.

88
00:06:45,230 --> 00:06:50,500
And that gives us a functional load function and mostly functional.

89
00:06:50,540 --> 00:06:57,890
Did you in class and now huge congratulations because our artificial intelligence is ready you can probably

90
00:06:57,890 --> 00:07:02,230
hear by the sound of my voice and I'm getting very excited because now it's time for the demo.

91
00:07:02,330 --> 00:07:08,510
We just made a brain and we're going to put this brain in the car and we will see how it is clever enough

92
00:07:08,840 --> 00:07:13,440
to do these round trips between the airport and downtown wherever the road is.

93
00:07:13,520 --> 00:07:15,310
So I can't wait to show you the demo.

94
00:07:15,410 --> 00:07:17,430
This is going to be in the next section.

95
00:07:17,510 --> 00:07:19,100
And until then I.
