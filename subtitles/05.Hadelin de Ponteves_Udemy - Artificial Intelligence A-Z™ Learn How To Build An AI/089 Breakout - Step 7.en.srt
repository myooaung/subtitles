1
00:00:00,390 --> 00:00:02,550
Hello and welcome to this tutorial.

2
00:00:02,550 --> 00:00:02,870
All right.

3
00:00:02,880 --> 00:00:08,370
So in the previous tutorials we made the brain or if you want the brains for the A-3 see now we need

4
00:00:08,370 --> 00:00:09,560
to train this brain.

5
00:00:09,660 --> 00:00:15,240
But in order to trained his brains we need an optimizer that's this tool use and stick a cigarette in

6
00:00:15,240 --> 00:00:20,820
the center to take the weight according to how much they contribute to the error between the predictions

7
00:00:20,940 --> 00:00:21,900
and the targets.

8
00:00:22,200 --> 00:00:29,790
And what we did up until now in the first and second module we used the atom optimizer by torche in

9
00:00:29,910 --> 00:00:30,660
the training.

10
00:00:30,840 --> 00:00:38,260
But as I told you we are dealing with a very challenging problem that is break out and the A-380 algorithm

11
00:00:38,260 --> 00:00:41,560
by itself is not enough to solve this problem.

12
00:00:41,580 --> 00:00:48,480
We need some customized optimizers and a lot of different tricks to solve this problem without waiting

13
00:00:48,480 --> 00:00:49,340
for ages.

14
00:00:49,530 --> 00:00:56,790
So that's the purpose of doing this and that is why we have a separate custom optimizer based on the

15
00:00:56,790 --> 00:00:57,990
atom optimizer.

16
00:00:58,200 --> 00:01:02,680
And that is contained in this shared enemy class and why shared atom.

17
00:01:02,880 --> 00:01:08,190
It's because it is actually the atom optimizer but that will work on shared States.

18
00:01:08,190 --> 00:01:12,840
So we're going to explain how it works and it's to toile So we're going to go through the different

19
00:01:12,840 --> 00:01:18,170
functions here without killing them because you know we want to keep some energy for the next and in

20
00:01:18,180 --> 00:01:23,190
addition that is the train that I fell which will take more than one hundred lines of code.

21
00:01:23,190 --> 00:01:24,550
So be ready for that.

22
00:01:24,600 --> 00:01:30,440
And therefore we will try to explain what's going on here in one Statoil Statoil.

23
00:01:30,480 --> 00:01:31,970
And let's start right now.

24
00:01:32,810 --> 00:01:38,480
All right so first we introduce this class sharing atom that will contain three functions the init function

25
00:01:38,480 --> 00:01:40,900
the shared memory function and the step function.

26
00:01:41,180 --> 00:01:48,410
So what we do first is that we inherit from optin that atom which is of course the atom optimizer and

27
00:01:48,410 --> 00:01:52,100
that we get from the Upton module from the torch library.

28
00:01:52,280 --> 00:01:57,980
So here we are plain heritance to get the tools all related to the atom optimizer and then we start

29
00:01:57,980 --> 00:01:59,320
with the init function.

30
00:01:59,330 --> 00:02:00,990
So what happens here.

31
00:02:01,150 --> 00:02:08,050
First we use the superfunction to inherit from all the tools and all the basic parameters from the atom

32
00:02:08,050 --> 00:02:11,310
the Atom class and these basic parameters are here.

33
00:02:11,380 --> 00:02:16,090
Harams learning race baiters Epsilon and weight decay.

34
00:02:16,240 --> 00:02:17,920
And then we start a follow up.

35
00:02:17,980 --> 00:02:21,840
This first Foluke for group itself that Paramo groups.

36
00:02:21,850 --> 00:02:28,310
So for us what is parum groups set up around groups contains all the attributes of the optimizer.

37
00:02:28,510 --> 00:02:34,030
And among these attributes we have the parameters that we have to optimize these parameters that we

38
00:02:34,030 --> 00:02:40,790
want atomize other ways of the network that are contained in self parum groups perhaps.

39
00:02:40,930 --> 00:02:44,860
So there we go group belongs to self-help groups.

40
00:02:44,980 --> 00:02:50,920
And here we have the second Faltu which will get these parameters that we want to optimize and that

41
00:02:50,920 --> 00:02:54,910
are exactly contained in self-doubt parum groups perhaps.

42
00:02:54,910 --> 00:03:01,870
So basically we go through self-help groups that contains all the parameters and for each group of parameters

43
00:03:01,930 --> 00:03:07,480
and self-talk from groups we are going to go through the parameters that we want to optimize.

44
00:03:07,540 --> 00:03:14,300
Therefore for the group Paramo here means for each Tancer of weights that we want to optimize.

45
00:03:14,410 --> 00:03:20,200
So for each sensor weights that we want to optimize and then what happens inside this group with these

46
00:03:20,200 --> 00:03:21,550
four lines of code.

47
00:03:21,820 --> 00:03:29,650
Basically what happens is that the update made by the optimizer is based on an exponential moving average

48
00:03:29,890 --> 00:03:31,170
of the gradient.

49
00:03:31,250 --> 00:03:32,880
That's this line of code here.

50
00:03:33,010 --> 00:03:38,270
That's the exponential moving average of the gradient of moment one that is of order one.

51
00:03:38,500 --> 00:03:44,860
But the objects made by atom is not only based on that it is also based on an exponential moving average

52
00:03:45,190 --> 00:03:47,140
of the square of the gradient.

53
00:03:47,260 --> 00:03:51,770
That is an exponential moving average of the gradient of momentum to or two.

54
00:03:52,030 --> 00:03:55,320
So here is the exponential moving average of all of one.

55
00:03:55,480 --> 00:04:00,560
And here is the exponential moving average of two for each of them that EMJ have degraded.

56
00:04:00,790 --> 00:04:01,890
So that what happens here.

57
00:04:02,080 --> 00:04:07,930
And now if you want to get more in depth of how the exponential moving average works well I highly encourage

58
00:04:07,930 --> 00:04:14,560
you to have a look at this research paper Adam method for stochastic optimization because basically

59
00:04:14,860 --> 00:04:20,610
the atom optimizer that we're implementing right now is based on the algorithm one here.

60
00:04:20,890 --> 00:04:27,700
So if you want to have more details on how the algorithm works well this paper will be definitely helpful.

61
00:04:27,700 --> 00:04:32,720
And then you have some further explanations on the algorithm with the atoms and the rules.

62
00:04:32,860 --> 00:04:37,840
And so you know that's only if you want to attack this before attacking the big train function that

63
00:04:37,840 --> 00:04:39,120
will make afterwards.

64
00:04:39,400 --> 00:04:41,990
OK so let's go back to bison.

65
00:04:42,220 --> 00:04:46,140
And now let's move on to the second function shared memory.

66
00:04:46,190 --> 00:04:47,890
So now I'm just going to say a few words.

67
00:04:48,010 --> 00:04:54,580
The idea of this shared memory function is kind of like tensor that kuda you know is an accelerator

68
00:04:54,580 --> 00:04:55,830
based on a view.

69
00:04:55,870 --> 00:05:03,160
And so basically what happens here is that we have these tensors of the states that share memory here

70
00:05:03,160 --> 00:05:10,210
here and here that behave a little bit like tenso that could to accelerate accelerated computation.

71
00:05:10,420 --> 00:05:17,140
But the difference is that here the sensors that share memory send the computation to a part of the

72
00:05:17,140 --> 00:05:22,150
GP you or you that is accessible to all the paralyzed threat.

73
00:05:22,160 --> 00:05:23,580
So that's basically what is done here.

74
00:05:23,590 --> 00:05:30,220
That's a little bit like 10 to that kuda but it's only sent to a part of the GP to be accessible to

75
00:05:30,220 --> 00:05:32,090
the parallelize threats.

76
00:05:32,090 --> 00:05:32,460
All right.

77
00:05:32,470 --> 00:05:35,100
And then we have the last function step.

78
00:05:35,110 --> 00:05:41,830
So you know this function it's like the step method of the atom optimizer that we are use in this course.

79
00:05:41,830 --> 00:05:47,170
And so again this is based on the algorithm one of the same paper that we saw before.

80
00:05:47,170 --> 00:05:48,610
So this algorithm.

81
00:05:48,850 --> 00:05:52,250
So again you want to understand in details the following lines of code.

82
00:05:52,420 --> 00:05:57,240
Well again I Ingrid's you to have a look at this algorithm one by this paper.

83
00:05:57,580 --> 00:06:04,330
And besides what is done here is not totally compulsory because this is actually a copy paste of the

84
00:06:04,330 --> 00:06:07,180
step method acting that atom class.

85
00:06:07,180 --> 00:06:14,050
So basically what is done here we could have done it by using our inheritance because here we inherit

86
00:06:14,050 --> 00:06:20,620
from Acton that Adam and so to use our inheritance Well what we can do instead of doing all this is

87
00:06:20,620 --> 00:06:29,260
just going to write here comment is just use the superfunction that we apply to our shared Adam class

88
00:06:29,710 --> 00:06:38,020
then our object self and here we just add step with parenthesis step is the method of the act in that

89
00:06:38,320 --> 00:06:40,550
class and that's exactly the same.

90
00:06:40,750 --> 00:06:45,820
That's why I was just saying that here is just a copy paste of the step method of the act in the Atom

91
00:06:45,850 --> 00:06:46,860
class.

92
00:06:46,930 --> 00:06:53,440
So I think that if you replace all this by this super function applied to share Adam and the step method

93
00:06:53,830 --> 00:06:55,960
well we might get exactly the same thing.

94
00:06:57,220 --> 00:06:59,900
All right so that was interesting to have a quick look at it.

95
00:06:59,920 --> 00:07:02,750
Basically you can see this as the Adam optimizer.

96
00:07:02,850 --> 00:07:04,530
It's like we had a deeper look at it.

97
00:07:04,640 --> 00:07:10,000
But again if you want to go in more details of all this and if you want to understand what happens behind

98
00:07:10,000 --> 00:07:14,120
the scene Well I encourage you to have a look at this research paper.

99
00:07:14,170 --> 00:07:16,120
I'll put the link in the comments here.

100
00:07:16,120 --> 00:07:19,940
You know remember you will have all the code connected in great details.

101
00:07:19,990 --> 00:07:22,120
So it's really good if you can have a look at it.

102
00:07:22,580 --> 00:07:30,310
And now I hope you have some great energy because we are going to move on to the train file which will

103
00:07:30,310 --> 00:07:35,860
contain this huge train function and that will basically train our brains which now we can do because

104
00:07:35,860 --> 00:07:37,510
we have our optimizer.

105
00:07:37,690 --> 00:07:39,230
So have a good break now.

106
00:07:39,250 --> 00:07:41,840
Have a good sleep and whenever you feel in great shape.

107
00:07:41,980 --> 00:07:44,440
Let's move on to the next step.

108
00:07:44,440 --> 00:07:45,910
Until then enjoy AI.
