WEBVTT
1

00:00:00.510  -->  00:00:03.530
All right so this is the fun part adjusted R-squared.

2

00:00:03.810  -->  00:00:07.250
So here we've got a simple inner aggression.

3

00:00:07.260  -->  00:00:12.630
We've talked about how the R-Squared is derived so what the formula one R-Squared is it's equal to one

4

00:00:12.630  -->  00:00:19.140
minus sum of squares of residuals divided by total sum of squares where some goalscorers of residuals

5

00:00:19.680  -->  00:00:28.440
is the sum of squares of the distances from the observations to your fitted line so we don't see it

6

00:00:28.440  -->  00:00:35.990
here that's your linear a simple linear regression line that we fit onto our daughter and the sum total

7

00:00:36.000  -->  00:00:37.260
sum of squares we can see it here.

8

00:00:37.260  -->  00:00:44.190
So that's just total sum of those squares of those red lines we can see here so the difference between

9

00:00:44.790  -->  00:00:46.320
your observations and the average.

10

00:00:46.320  -->  00:00:51.450
So basically R-Squared is telling us how well is your model fitted to the daughter in fact how much

11

00:00:51.450  -->  00:00:56.840
better is it fitted than the average line that she can just draw through her daughter very easily.

12

00:00:57.120  -->  00:01:02.550
So let's get rid of all of that and put all R-squared for me on the left for a second.

13

00:01:02.580  -->  00:01:07.190
Now we talked about R-squared for a simple linear regression.

14

00:01:07.290  -->  00:01:10.880
Well the same concepts apply for a multiple in your regression.

15

00:01:10.880  -->  00:01:16.530
So for instance with two variables R-squared would be calculated in the same way.

16

00:01:16.530  -->  00:01:22.890
We're just not going to go through all these derivations right now because it's just easier to visualize

17

00:01:22.890  -->  00:01:23.710
the way we've done it.

18

00:01:23.730  -->  00:01:31.590
And that's exactly the same concepts apply to a multiple linear regression and still.

19

00:01:31.950  -->  00:01:36.040
So what that means is that all ordinary least squares method is used.

20

00:01:36.060  -->  00:01:44.120
And the best fitting is a multiple regression is the one that has the least sum of squares of residual

21

00:01:44.130  -->  00:01:49.170
So you're trying to minimize the sum of squares of residual and I bring that up well we'll need that

22

00:01:49.230  -->  00:01:50.030
in a second.

23

00:01:50.250  -->  00:01:56.070
So we want to use R-squared as we discussed as a goodness of fit parameters so the bigger it is the

24

00:01:56.070  -->  00:01:59.160
better the closer it is to one the better your model.

25

00:01:59.200  -->  00:01:59.900
That's awesome.

26

00:01:59.910  -->  00:02:06.930
We can totally try doing that but there is one problem and the problem is or starts occur when you add

27

00:02:06.930  -->  00:02:09.720
more variables to your model.

28

00:02:09.720  -->  00:02:14.700
So here you can see we've got two variables in our multiple integration what will happen if we add a

29

00:02:14.700  -->  00:02:17.620
third variable to our model.

30

00:02:17.940  -->  00:02:22.440
Think of it as an example let's look at that example with a salary.

31

00:02:22.440  -->  00:02:30.960
So your salary equals to a constant 30000 plus 10000 times the years of experience plus for instance

32

00:02:30.990  -->  00:02:38.430
BE2 could be a constant hour coefficient times the number of qualifications you have.

33

00:02:38.520  -->  00:02:45.920
And I don't know and B-3 could be how much how much money you brought the company in the previous year

34

00:02:45.930  -->  00:02:51.180
so if you're a salesman for instance so it coefficient times the amount of dollars that you brought

35

00:02:51.450  -->  00:02:53.890
the company in the previous year something like that.

36

00:02:53.940  -->  00:03:01.380
So you can keep adding on variables that you think are actually impacting the outcome so are impacting

37

00:03:01.380  -->  00:03:08.100
the dependent variable and you want to fit and model and see if it's better or not so in this case we're

38

00:03:08.100  -->  00:03:09.240
adding a third variable.

39

00:03:09.240  -->  00:03:11.860
We already have a model with two variables and it works okay.

40

00:03:11.960  -->  00:03:17.550
And now we want to add a third variable and see if maybe using a third variable we can fit our model

41

00:03:17.610  -->  00:03:21.220
even better so we can create an even better model with three variables.

42

00:03:21.270  -->  00:03:25.470
So how can we judge that after we've added all Berbel and we've run the model.

43

00:03:25.470  -->  00:03:28.370
We can look at the R-squared did the R-squared get better or worse.

44

00:03:28.380  -->  00:03:34.730
So in fact did the R-squared increase or did it decrease or stay the same.

45

00:03:34.740  -->  00:03:41.010
So the problem here is that R-squared because of these two things are separate will never decrease.

46

00:03:41.030  -->  00:03:43.310
And let's go through that in a bit more detail.

47

00:03:43.350  -->  00:03:47.310
I'm going to use my mouse here because this is quite an important part.

48

00:03:47.430  -->  00:03:54.630
So R-squared over here is equal to 1 minus some squared of residual divide by some square of the total

49

00:03:54.630  -->  00:03:55.740
.

50

00:03:55.770  -->  00:04:00.600
So once you add a new variable to your model.

51

00:04:00.600  -->  00:04:02.290
Right.

52

00:04:02.700  -->  00:04:10.620
Is going to somehow affect what the model looks like and the fact that we're trying to minimize the

53

00:04:10.620  -->  00:04:12.170
sum of squares of residual.

54

00:04:12.300  -->  00:04:18.690
What that means is that either this new variable will help minimize the somewhat square of residual

55

00:04:18.690  -->  00:04:26.040
somehow the regression process will find a way to give it a coefficient that will help minimize the

56

00:04:26.040  -->  00:04:27.990
sum of square residuals.

57

00:04:28.310  -->  00:04:33.110
And then in that case R-squared will happen to us squared.

58

00:04:33.170  -->  00:04:37.260
Will be one minus something that is less than it used to be.

59

00:04:37.380  -->  00:04:38.060
Right.

60

00:04:38.250  -->  00:04:44.520
Divided by the same value because we by adding a new variable were not affecting the observations were

61

00:04:44.520  -->  00:04:47.120
not affecting the averages of their observations right.

62

00:04:47.130  -->  00:04:48.510
This does not change.

63

00:04:48.660  -->  00:04:55.360
So by adding new variable the regression processes through this condition we'll definitely try to minimize

64

00:04:55.420  -->  00:04:58.860
this value make it even smaller than it is currently.

65

00:04:59.140  -->  00:05:05.350
And that way this whole part will decrease and this whole part will increase.

66

00:05:05.350  -->  00:05:11.140
So you are going to increase if it so happens that the new variable that's added whatever coefficient

67

00:05:11.170  -->  00:05:13.700
you give it a new coefficient you give it.

68

00:05:13.960  -->  00:05:18.950
If you cannot decrease SS residual right what will happen.

69

00:05:18.970  -->  00:05:20.560
Well this coefficient will just become zero.

70

00:05:20.560  -->  00:05:26.450
It very rarely happens I don't think I've ever seen a coefficient like B exactly zero.

71

00:05:26.530  -->  00:05:28.330
And I'll tell you just now why.

72

00:05:28.450  -->  00:05:33.370
But in the worst case scenario the regression process will say now this variable is you know completely

73

00:05:33.820  -->  00:05:39.640
just making the world model definitely worse so I'll just put a 0 instead of this coefficient and forget

74

00:05:39.640  -->  00:05:40.680
about it and that way.

75

00:05:40.700  -->  00:05:43.720
SS residual won't change and R-squared won't change.

76

00:05:43.780  -->  00:05:45.000
So you only have two options.

77

00:05:45.190  -->  00:05:49.110
Either R-squared will increase or it won't change at all.

78

00:05:49.120  -->  00:05:51.000
So R-squared will never decrease.

79

00:05:51.100  -->  00:05:58.730
If you add variables and why say that B-3 is never equal to exactly zero because they can always be

80

00:05:58.770  -->  00:06:06.790
or there will always be at least a slight random correlation between the independent on the dependent

81

00:06:06.790  -->  00:06:07.300
variable.

82

00:06:07.400  -->  00:06:08.810
Doesn't matter what you put in here.

83

00:06:08.820  -->  00:06:16.240
So even in the example when we're looking at salary so salary equals years of experience plus how many

84

00:06:16.570  -->  00:06:22.810
how much I mean qualifications a person has and then we can add in what is the last digit of the person

85

00:06:23.010  -->  00:06:24.480
of the person's mobile number.

86

00:06:24.550  -->  00:06:24.860
Right.

87

00:06:25.000  -->  00:06:27.740
Of course that is not going to.

88

00:06:27.900  -->  00:06:33.210
It doesn't have any correlation with the independent variable.

89

00:06:33.220  -->  00:06:37.990
What sort of that will there is no causative factor there's no association between the lozenge of your

90

00:06:38.000  -->  00:06:41.510
mobile number has no effect on your salary.

91

00:06:41.530  -->  00:06:46.840
But if we add that in and they'll will randomly be a slight correlation you know.

92

00:06:46.840  -->  00:06:51.950
It'll just be a random correlation and the regression process will pick it up and I'll give it a.

93

00:06:52.210  -->  00:06:58.290
And R-squared will probably decrease by that tiny little or increase by that tiny little bit.

94

00:06:58.370  -->  00:07:01.350
And so that is why there's a problem with R-squared.

95

00:07:01.490  -->  00:07:07.120
You can add variables and you will not know if those variables are helping your model or if they're

96

00:07:07.120  -->  00:07:12.490
not helping because you're R-Squared is going to is biased is basically always increasing regardless

97

00:07:12.580  -->  00:07:15.800
of the actual improvement or not improvement in fit.

98

00:07:15.790  -->  00:07:23.890
So we've got to come up with different parameter to measure goodness of fit and that is where adjusted

99

00:07:23.890  -->  00:07:25.170
R-squared comes in.

100

00:07:25.190  -->  00:07:27.260
This is the form of our adjusted R-squared.

101

00:07:27.340  -->  00:07:29.590
Here is the number of regressors.

102

00:07:29.800  -->  00:07:34.830
So the number of independent variables and is the sample size so it's over here.

103

00:07:35.380  -->  00:07:39.180
Now they say that adjusted R-squared has a penalization factor.

104

00:07:39.280  -->  00:07:44.930
It penalizes you for adding independent variables that don't help your model.

105

00:07:44.950  -->  00:07:46.240
And let's talk about that.

106

00:07:46.250  -->  00:07:50.760
That's the important bit about just Herskowitz so p is a number of independent malls.

107

00:07:50.770  -->  00:07:51.680
Let's look at P..

108

00:07:51.910  -->  00:07:55.540
PS here is in the bottom in the denominator.

109

00:07:55.540  -->  00:07:59.110
That means that when PPI increases when you increase the number of independent variables.

110

00:07:59.120  -->  00:08:01.640
This whole part decreases.

111

00:08:01.960  -->  00:08:08.050
And when this whole part decreases the denominator decreases the ratio increases and as the ratio increases

112

00:08:08.170  -->  00:08:10.390
this whole bid increases as well.

113

00:08:10.430  -->  00:08:14.870
And as this Holbert increases one minus minuses hold decreases.

114

00:08:14.990  -->  00:08:22.100
So as you can see as you're adding more regressors the adjusted R-squared is decreasing and going further

115

00:08:22.090  -->  00:08:23.770
away from one.

116

00:08:23.950  -->  00:08:25.550
And that is the important part.

117

00:08:25.570  -->  00:08:33.210
Also you can see here what happens when R-squared increases when just normal R-squared increases R-squared

118

00:08:33.220  -->  00:08:34.160
increases.

119

00:08:34.250  -->  00:08:37.060
So this part decreases right.

120

00:08:37.060  -->  00:08:42.290
One might as R-squared decreases and that means that this whole part increases.

121

00:08:42.310  -->  00:08:48.430
So you've got a battle here on one hand R-squared by adding a new variable you're increasing R-squared

122

00:08:49.510  -->  00:08:52.200
So you're increasing adjusted R-squared.

123

00:08:52.210  -->  00:08:57.630
But on the other hand by adding numerable you increase PPI So you're decreasing adjusted R-squared.

124

00:08:57.860  -->  00:09:03.060
And in that sense is a fair like it's a fair battle here.

125

00:09:03.250  -->  00:09:10.540
If your variable is not good is not helping the model then adjusted R-squared this will be an insignificant

126

00:09:10.540  -->  00:09:17.490
increase and this penalization factor will actually drive R-squared down adjusted R-squared down.

127

00:09:17.770  -->  00:09:23.560
If on the other hand your new variable that you added is helping to model a lot then the increase in

128

00:09:23.570  -->  00:09:29.940
R-squared will be substantial and it will overwhelm this penalization factor so even though you'll still

129

00:09:29.950  -->  00:09:36.640
get penalized for adding a variable that increase the benefit to the model will be so great that even

130

00:09:36.640  -->  00:09:38.830
that justed R-squared will go up.

131

00:09:39.110  -->  00:09:41.800
And so that's how the adjusted R-squared basically works.

132

00:09:41.860  -->  00:09:47.860
It's it's a very good metric it helps you understand whether you're adding good variables to model or

133

00:09:47.870  -->  00:09:54.650
not and we'll be using adjusted R-squared throughout this course to make sure our models are robust

134

00:09:54.640  -->  00:09:55.490
when we're building them
