WEBVTT
1
00:00:01.510 --> 00:00:04.990
Another problem we might face in our dataset is that

2
00:00:04.990 --> 00:00:08.600
our data format is not organized.

3
00:00:08.600 --> 00:00:12.570
Usually the data doesn't follow an organized format due to a lack of

4
00:00:12.570 --> 00:00:16.430
validations in the upstream input systems, for example,

5
00:00:16.430 --> 00:00:19.440
a lack of validation in the database or the web UI

6
00:00:19.440 --> 00:00:22.230
from which the data is entered.

7
00:00:22.230 --> 00:00:23.000
For example,

8
00:00:23.000 --> 00:00:30.590
a location column might be a city and country such as Madrid and Spain, only a

9
00:00:30.590 --> 00:00:36.900
country such as Sweden, or only a city such as California.

10
00:00:36.900 --> 00:00:39.900
As you can see, the data format is not consistent,

11
00:00:39.900 --> 00:00:44.490
and that would be problematic for the machine learning algorithms.

12
00:00:44.490 --> 00:00:49.690
So let's see what would be the possible solutions for inconsistent formats.

13
00:00:49.690 --> 00:00:53.350
The optimal solution would be definitely ensuring that this does not

14
00:00:53.350 --> 00:00:57.430
happen in the first place by making sure that the source systems

15
00:00:57.430 --> 00:01:01.920
implement proper validation measures and provide us with as cleanly

16
00:01:01.920 --> 00:01:05.110
formatted data as possible.

17
00:01:05.110 --> 00:01:10.850
If in medicine they say, "an apple a day makes a doctor away," in data analysis,

18
00:01:10.850 --> 00:01:16.900
I would say, "a validation a day makes inconsistent formats away." This is

19
00:01:16.900 --> 00:01:21.270
usually easy to enforce if all the data you are relying on lies within the

20
00:01:21.270 --> 00:01:25.240
boundaries of your organization. However,

21
00:01:25.240 --> 00:01:28.740
it will be more challenging to enforce if the data you are

22
00:01:28.740 --> 00:01:33.640
relying on is coming from external providers.

23
00:01:33.640 --> 00:01:36.870
A very painful solution for this challenge would be to fix

24
00:01:36.870 --> 00:01:39.890
the data manually, that is, to go through the dataset

25
00:01:39.890 --> 00:01:46.010
instance one by one and fix the rows, which is often an impractical solution.

26
00:01:46.010 --> 00:01:50.860
However, another solution would be to try to deduce patterns in the data.

27
00:01:50.860 --> 00:01:54.810
For example, if you note that the city is always entered first,

28
00:01:54.810 --> 00:01:57.610
then a space, you can write your custom logic to

29
00:01:57.610 --> 00:02:01.440
parse that using regular expressions.

30
00:02:01.440 --> 00:02:04.610
One modern solution would be to use some fuzzy matching

31
00:02:04.610 --> 00:02:07.860
libraries that can match close enough string entries

32
00:02:07.860 --> 00:02:11.240
against the correct string entries.

33
00:02:11.240 --> 00:02:12.130
For example,

34
00:02:12.130 --> 00:02:17.490
it would match "hotle" wrongly spelled to "hotel." A common

35
00:02:17.490 --> 00:02:21.150
fuzzy matching tool is an algorithm developed by a Russian

36
00:02:21.150 --> 00:02:25.540
scientist called Levenshtien distance.

37
00:02:25.540 --> 00:02:30.640
You can read about it in the internet if you are interested.

38
00:02:30.640 --> 00:02:38.000
A Python library that can help with fuzzy string matching is FuzzyWuzzy. I would recommend reading about it.

