1
00:00:01,080 --> 00:00:04,270
After we have discussed different techniques for analyzing and

2
00:00:04,270 --> 00:00:08,370
understanding our data, you might think that Globomantics data

3
00:00:08,370 --> 00:00:12,240
analysis team tasks are done and it is time to hand over to the

4
00:00:12,240 --> 00:00:14,220
machine learning team.

5
00:00:14,220 --> 00:00:16,520
Mm, really?

6
00:00:16,520 --> 00:00:22,130
The bad news is that no, that's not what we expect.

7
00:00:22,130 --> 00:00:27,960
Our data will most likely have many issues that prevent using it directly.

8
00:00:27,960 --> 00:00:31,140
Let's examine some of these issues.

9
00:00:31,140 --> 00:00:34,970
Firstly, our dataset might be imbalanced,

10
00:00:34,970 --> 00:00:38,220
which means that we might not have representative samples

11
00:00:38,220 --> 00:00:41,540
from all real cases of our problem domain.

12
00:00:41,540 --> 00:00:47,330
This is particularly important for classification problems.

13
00:00:47,330 --> 00:00:50,640
Secondly, our data might use different scales,

14
00:00:50,640 --> 00:00:53,790
which definitely means that we will have to make sure that we are

15
00:00:53,790 --> 00:00:57,850
using the same scales everywhere so that we compare apples to

16
00:00:57,850 --> 00:01:02,360
apples. Or even corruption, in some sensors,

17
00:01:02,360 --> 00:01:07,620
we read the data from. Our data might not be straightforward

18
00:01:07,620 --> 00:01:10,970
numerical data that the machine learning models can directly

19
00:01:10,970 --> 00:01:15,360
consume, it can be audio files or even categorical data that

20
00:01:15,360 --> 00:01:18,600
requires a special processing.

21
00:01:18,600 --> 00:01:24,430
We might have missing data due to optional fields or even system failures.

22
00:01:24,430 --> 00:01:25,450
Or even worse,

23
00:01:25,450 --> 00:01:28,770
our data might contain some outliers that are not

24
00:01:28,770 --> 00:01:33,420
representative of the real problem domain. Or even it could be

25
00:01:33,420 --> 00:01:36,470
that our data is highly dimensional, that is,

26
00:01:36,470 --> 00:01:39,570
it has too many features which make it difficult for us to

27
00:01:39,570 --> 00:01:44,580
visualize and train. Our data might also expect what

28
00:01:44,580 --> 00:01:48,030
soâ€‘called features with high correlation,

29
00:01:48,030 --> 00:01:50,820
which are features that add no value to our machine

30
00:01:50,820 --> 00:01:57,940
learning model, or even worse, it can make our regression tasks perform worse.

31
00:01:57,940 --> 00:01:58,630
Also,

32
00:01:58,630 --> 00:02:02,280
our data distribution might be malformed and not what the

33
00:02:02,280 --> 00:02:04,720
machine learning algorithms expect.

34
00:02:04,720 --> 00:02:07,450
As you have seen the list of the challenges we might face

35
00:02:07,450 --> 00:02:10,780
with the data is a very extensive list,

36
00:02:10,780 --> 00:02:15,610
so we will discuss these challenges throughout this module, but before

37
00:02:15,610 --> 00:02:18,820
discussing the specific challenges with our dataset,

38
00:02:18,820 --> 00:02:20,750
let's try to understand first,

39
00:02:20,750 --> 00:02:25,970
what is the root of all evil with the data issues we discussed earlier?

40
00:02:25,970 --> 00:02:32,150
Let's reflect and think. First and foremost, user and system errors,

41
00:02:32,150 --> 00:02:36,540
which can be narrowed down to human errors. Human either as a system user

42
00:02:36,540 --> 00:02:40,440
or even system developers are not perfect creatures.

43
00:02:40,440 --> 00:02:45,220
We make mistakes in data entry, we forget to add validation to our systems,

44
00:02:45,220 --> 00:02:47,890
or we might even develop a buggy systems that corrupt

45
00:02:47,890 --> 00:02:50,810
the downstream systems. For example,

46
00:02:50,810 --> 00:02:55,010
it is quite common to see systems that use a string type to store the

47
00:02:55,010 --> 00:02:58,860
date, instead of the native datetime construct,

48
00:02:58,860 --> 00:03:02,850
thus opening the door to many date formatting related data quality

49
00:03:02,850 --> 00:03:08,320
issues, such as missing data, duplicate rows, and so on.

50
00:03:08,320 --> 00:03:12,340
The second reason would be the usage of heterogeneous systems that

51
00:03:12,340 --> 00:03:15,930
have different business rules and this is quite common when it

52
00:03:15,930 --> 00:03:20,400
comes to the units of measurements. Different units of measurements

53
00:03:20,400 --> 00:03:24,470
are used for different cases in different cultures and surely for

54
00:03:24,470 --> 00:03:25,990
different systems.

55
00:03:25,990 --> 00:03:30,530
System A might have a human weight that's using pounds because it's

56
00:03:30,530 --> 00:03:35,540
developed in the US while system B from Sweden uses kilograms.

57
00:03:35,540 --> 00:03:38,860
Surely, we would not be comparing apples to apples,

58
00:03:38,860 --> 00:03:42,320
and we need to adjust to a single scale.

59
00:03:42,320 --> 00:03:43,140
Finally,

60
00:03:43,140 --> 00:03:46,360
sometimes we are having difficulties just because it's

61
00:03:46,360 --> 00:03:48,840
the nature of our business problem.

62
00:03:48,840 --> 00:03:51,530
We might want to do machine learning on new system,

63
00:03:51,530 --> 00:03:55,810
and it's a hard fact that we don't have enough data, or even we might

64
00:03:55,810 --> 00:04:02,000
be processing a video data with too many dimensions. I'm sorry, life is not ideal.

