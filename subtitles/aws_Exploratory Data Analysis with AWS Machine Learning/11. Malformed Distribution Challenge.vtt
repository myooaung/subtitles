WEBVTT
1
00:00:01.140 --> 00:00:04.810
Let's now discuss the final challenge we can have in our dataset,

2
00:00:04.810 --> 00:00:07.730
which is the malform distributions.

3
00:00:07.730 --> 00:00:10.930
One thing I have always talked about is that many machine

4
00:00:10.930 --> 00:00:14.340
learning algorithms are based on the fact that our dataset

5
00:00:14.340 --> 00:00:17.200
follows Gaussian distribution.

6
00:00:17.200 --> 00:00:17.960
In practice,

7
00:00:17.960 --> 00:00:21.700
there are many reasons why our dataset may not follow Gaussian

8
00:00:21.700 --> 00:00:26.040
distribution. To check whether a dataset follows a normal distribution

9
00:00:26.040 --> 00:00:30.520
that can be done either visually through visualizations or through a

10
00:00:30.520 --> 00:00:33.150
specific normality test techniques.

11
00:00:33.150 --> 00:00:35.620
A detailed discussion about normality tests,

12
00:00:35.620 --> 00:00:40.200
techniques is outside the scope, but you can think about it as specific

13
00:00:40.200 --> 00:00:44.230
measures that help us understand how close the model look to a normal

14
00:00:44.230 --> 00:00:47.510
distribution, and hence, the word normality.

15
00:00:47.510 --> 00:00:51.460
So if we apply specific techniques on a non‑Gaussian dataset,

16
00:00:51.460 --> 00:00:54.380
we might get misleading results, and hence, a poor

17
00:00:54.380 --> 00:00:57.640
machine learning model performance.

18
00:00:57.640 --> 00:01:01.030
So let's take a brief discussion on how can we fix

19
00:01:01.030 --> 00:01:03.340
the data distribution challenge.

20
00:01:03.340 --> 00:01:08.080
Fixing data distribution is more an art than science and it requires significant

21
00:01:08.080 --> 00:01:12.440
amount of judgment and sometimes domain expert involvement.

22
00:01:12.440 --> 00:01:17.470
We can threshold our dataset to remove long‑tailed values by removing any

23
00:01:17.470 --> 00:01:22.940
identified outliers or apply what's so‑called data transformations and

24
00:01:22.940 --> 00:01:27.830
this is usually when your dataset is hiding it's normal distribution

25
00:01:27.830 --> 00:01:32.500
structures and it requires some mathematical manipulations to make it

26
00:01:32.500 --> 00:01:37.720
match normal distribution. Two common transformation techniques are power

27
00:01:37.720 --> 00:01:40.040
and log transformations.

28
00:01:40.040 --> 00:01:44.670
Sometimes it might feel weird why specific data transformation technique

29
00:01:44.670 --> 00:01:50.430
works fine to shape our dataset to match the normal distribution and it can

30
00:01:50.430 --> 00:01:55.980
just be confusing. If you have the same thoughts, let's demystify the secret.

31
00:01:55.980 --> 00:02:00.040
By understanding how the log transformation works, you will be able to

32
00:02:00.040 --> 00:02:05.440
generalize for other types and you will not need to explain them. A skewed

33
00:02:05.440 --> 00:02:09.810
dataset occurs when we have specific values that are significantly different

34
00:02:09.810 --> 00:02:11.940
from the others.

35
00:02:11.940 --> 00:02:17.850
Remember the sale price we draw from Globomantics earlier. Let's see how

36
00:02:17.850 --> 00:02:22.620
the log transform can make the data range to smaller.

37
00:02:22.620 --> 00:02:28.020
Imagine that we have 100,000 and 100.

38
00:02:28.020 --> 00:02:30.810
The difference between them in the original linear

39
00:02:30.810 --> 00:02:33.900
scale would be the substructure result,

40
00:02:33.900 --> 00:02:42.820
which is 99,900, it's a larger spread range which can cause skewness. However,

41
00:02:42.820 --> 00:02:47.090
let's calculate that difference after taking the log. Here,

42
00:02:47.090 --> 00:02:54.360
I represented the numbers in terms of power of 10 and the result would be 5 ‑ 2,

43
00:02:54.360 --> 00:02:56.200
which is 3.

44
00:02:56.200 --> 00:02:56.920
As you can see,

45
00:02:56.920 --> 00:03:07.000
the logarithmic scale properties significantly penalize large numbers and make them smaller, and hence, removes the skewness.

