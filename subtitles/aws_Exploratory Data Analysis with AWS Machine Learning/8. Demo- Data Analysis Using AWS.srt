1
00:00:00,740 --> 00:00:04,610
And now let's proceed with a demo and see how we can do

2
00:00:04,610 --> 00:00:08,980
statistical data analysis using AWS SageMaker.

3
00:00:08,980 --> 00:00:12,080
We are back to the Jupyter Notebook,

4
00:00:12,080 --> 00:00:16,340
which we created in AWS SageMaker in the previous module.

5
00:00:16,340 --> 00:00:21,190
The first thing we are going to do as data analysts at Globomantics

6
00:00:21,190 --> 00:00:25,070
is to apply what's so called statistical data analysis on our

7
00:00:25,070 --> 00:00:27,900
dataset to understand its characteristics.

8
00:00:27,900 --> 00:00:28,790
To do that,

9
00:00:28,790 --> 00:00:33,490
we are going to use pandas built‑in method called describe that

10
00:00:33,490 --> 00:00:39,280
does descriptive statistics over dataset.

11
00:00:39,280 --> 00:00:42,680
And now we can see some interesting data.

12
00:00:42,680 --> 00:00:45,820
Let's try to reason over what we have got.

13
00:00:45,820 --> 00:00:50,090
The count of sale price element is 2930,

14
00:00:50,090 --> 00:00:54,760
which is equal to the total number of rows we have in the dataset.

15
00:00:54,760 --> 00:00:58,740
It's an indicator that we don't have any missing data.

16
00:00:58,740 --> 00:01:03,460
Sometimes we might have observations that are missing certain value due you

17
00:01:03,460 --> 00:01:08,440
to user entry errors or lack of validation in the front‑end systems or even

18
00:01:08,440 --> 00:01:11,640
faulty sensors that they didn't supply some data.

19
00:01:11,640 --> 00:01:13,390
If there is a missing data value,

20
00:01:13,390 --> 00:01:16,540
we will need to apply certain techniques to deal with them,

21
00:01:16,540 --> 00:01:20,060
and this is something we are going to discuss in future modules.

22
00:01:20,060 --> 00:01:24,230
The mean of the dataset is around 180,000.

23
00:01:24,230 --> 00:01:27,210
The mean itself alone doesn't tell us that much,

24
00:01:27,210 --> 00:01:30,030
but it becomes really powerful when we join this

25
00:01:30,030 --> 00:01:33,650
knowledge with other descriptive statistics.

26
00:01:33,650 --> 00:01:38,230
The standard deviation is around 80,000, which is somewhat high.

27
00:01:38,230 --> 00:01:41,360
This tells us that we should expect considerable

28
00:01:41,360 --> 00:01:43,940
differences in the prices in the dataset,

29
00:01:43,940 --> 00:01:47,230
as the dataset is spread out.

30
00:01:47,230 --> 00:01:50,280
The minimum value of the dataset is around 13,000.

31
00:01:50,280 --> 00:01:56,100
Notice that this is while the average is around 180,000.

32
00:01:56,100 --> 00:02:00,440
This gives us a hint that there are really large numbers in the dataset

33
00:02:00,440 --> 00:02:04,830
that are causing the dataset to tend towards the average.

34
00:02:04,830 --> 00:02:09,390
The 25 percentile is around 130,000.

35
00:02:09,390 --> 00:02:13,680
If we remember that the minimum value was at around 13,000,

36
00:02:13,680 --> 00:02:16,400
it tells us that there is a wide range of small

37
00:02:16,400 --> 00:02:21,750
numbers at the first quarter of data, while the 50 percentile,

38
00:02:21,750 --> 00:02:25,740
or the median, is around 160,000.

39
00:02:25,740 --> 00:02:30,020
If we compare it to the mean, which is around 180,000,

40
00:02:30,020 --> 00:02:32,450
we can say that they are close enough.

41
00:02:32,450 --> 00:02:33,620
In simple words,

42
00:02:33,620 --> 00:02:37,920
the average value of the dataset is close to the middle value when we

43
00:02:37,920 --> 00:02:41,130
order the dataset in ascending or descending fashion.

44
00:02:41,130 --> 00:02:46,110
The conclusion would be that the dataset is fairly symmetrical.

45
00:02:46,110 --> 00:02:54,660
The 75 percentile is at around 214,000, while the max is at very large number,

46
00:02:54,660 --> 00:02:57,140
755,000.

47
00:02:57,140 --> 00:03:00,780
We can withdraw the following conclusions about the dataset.

48
00:03:00,780 --> 00:03:04,550
There is fairly prices with large number between the 75

49
00:03:04,550 --> 00:03:07,720
percentile and the maximum value, in other words,

50
00:03:07,720 --> 00:03:10,510
the top 25 percentile.

51
00:03:10,510 --> 00:03:13,840
If we calculated the difference between the maximum value,

52
00:03:13,840 --> 00:03:21,620
which is 755‑214, which is the 75 percentile,

53
00:03:21,620 --> 00:03:26,890
we will find out that the difference is around 541,000.

54
00:03:26,890 --> 00:03:31,430
Compare it with the range with the minimum value at the 25 percentile,

55
00:03:31,430 --> 00:03:35,710
in other words, the lower 25 percentile values,

56
00:03:35,710 --> 00:03:42,410
you will find that is 130‑18 which is around 112,000.

57
00:03:42,410 --> 00:03:46,910
And this range is way smaller than the other range

58
00:03:46,910 --> 00:03:52,180
we found on the upper percentile, which was 541.

59
00:03:52,180 --> 00:03:57,940
The conclusion we can draw from that is that our dataset is skewed.

60
00:03:57,940 --> 00:04:04,250
Moreover, the maximum value, 755,000, is likely an outlier,

61
00:04:04,250 --> 00:04:08,630
since it is more than 3 standard deviation difference from the mean.

62
00:04:08,630 --> 00:04:13,620
We will discuss how to detect outliers later in the future modules.

63
00:04:13,620 --> 00:04:14,820
That's it.

64
00:04:14,820 --> 00:04:18,890
You can notice how much time we spent to explain these seven numbers

65
00:04:18,890 --> 00:04:22,180
and what conclusions we were able to draw from them.

66
00:04:22,180 --> 00:04:26,140
This should be a good hint for the value of descriptive statistics.

67
00:04:26,140 --> 00:04:29,980
Let's now calculate other two metrics, skewness and kurtosis.

68
00:04:29,980 --> 00:04:35,800
Luckily, they are also built in into pandas.

69
00:04:35,800 --> 00:04:40,440
The skewness value is around 5, which is much larger than 1.

70
00:04:40,440 --> 00:04:41,500
If you remember,

71
00:04:41,500 --> 00:04:46,700
an absolute value of skewness larger than 1 is an indication of a high skewness.

72
00:04:46,700 --> 00:04:52,120
And this is another confirmation of our analysis we did with mean and median,

73
00:04:52,120 --> 00:04:56,590
and from that we also found out that our dataset is skewed.

74
00:04:56,590 --> 00:05:01,480
Notice that kurtosis is 1.74, which is larger than 0.

75
00:05:01,480 --> 00:05:04,890
This indicates that the shape of our data is pointy.

76
00:05:04,890 --> 00:05:08,690
You might be wondering that I mentioned previously a kurtosis

77
00:05:08,690 --> 00:05:11,640
value more than 3 indicates a pointy distribution,

78
00:05:11,640 --> 00:05:14,960
but now I am saying that our value larger than 0

79
00:05:14,960 --> 00:05:16,940
indicates a pointy distribution.

80
00:05:16,940 --> 00:05:20,090
The reason is that pandas is using a different definition.

81
00:05:20,090 --> 00:05:25,240
Rather than considering kurtosis of the normal distribution being 3,

82
00:05:25,240 --> 00:05:27,520
it considers it as 0,

83
00:05:27,520 --> 00:05:32,110
which means that it's deducting 3 from it for mathematical convenience.

84
00:05:32,110 --> 00:05:36,510
You can read more about that here.

85
00:05:36,510 --> 00:05:41,010
The final thing we are going to explain in this demo is the correlation.

86
00:05:41,010 --> 00:05:43,410
I will take it here in a very brief manner,

87
00:05:43,410 --> 00:05:47,180
as the correlation matrix we will create will not be very readable.

88
00:05:47,180 --> 00:05:50,710
A better approach is the heat maps which are easier to deal with,

89
00:05:50,710 --> 00:05:53,680
and we are going to discuss that in the next module.

90
00:05:53,680 --> 00:05:57,150
To calculate the cross‑correlation of a dataset,

91
00:05:57,150 --> 00:06:00,850
which means the correlation of each element with the other elements,

92
00:06:00,850 --> 00:06:08,570
we use a pandas function called corr, as the following.

93
00:06:08,570 --> 00:06:12,850
As you can see, the values we are getting are not very readable,

94
00:06:12,850 --> 00:06:15,850
and they are just another complicated matrix of numbers.

95
00:06:15,850 --> 00:06:20,460
So let's not analyze that now, and let me jump to something interesting.

96
00:06:20,460 --> 00:06:27,940
As you can see, the dimensions of the correlation matrix are 38 x 38.

97
00:06:27,940 --> 00:06:32,200
How comes that while our original dataset has 82 columns.

98
00:06:32,200 --> 00:06:34,900
It should be 82 x 82.

99
00:06:34,900 --> 00:06:37,850
Hmm, you need to remember one thing.

100
00:06:37,850 --> 00:06:41,110
Correlation is defined for numerical variables,

101
00:06:41,110 --> 00:06:46,500
and hence all categorical variables are not calculated during correlation.

102
00:06:46,500 --> 00:06:49,140
Let me show you the dataset to remember that.

103
00:06:49,140 --> 00:06:53,420
As you can see, we have some columns that have categorical values,

104
00:06:53,420 --> 00:06:55,970
for example, the Utilities column.

105
00:06:55,970 --> 00:06:57,260
Also, luckily,

106
00:06:57,260 --> 00:07:02,460
pandas corr function is smart enough to exclude the non‑numeric values.

107
00:07:02,460 --> 00:07:04,030
And that's it for this demo.

108
00:07:04,030 --> 00:07:06,860
I hope that you now understand the value of descriptive

109
00:07:06,860 --> 00:07:12,000
statistics for us as data analysts at Globomantics. Thank you.

