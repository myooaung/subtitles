WEBVTT

00:00.570 --> 00:07.000
Hello and welcome back to the course and deep learning we're finally at the park on outturn cotters.

00:07.230 --> 00:12.300
So we've gone through quite a lot of different models in this course we've already talked about artificial

00:12.300 --> 00:17.790
neural networks convolutional neural networks and recurrent neural networks and that summerize to supervised

00:18.120 --> 00:19.760
de-planing side of things.

00:19.890 --> 00:24.630
And now we are in unsupervised branch of deep learning and we've already talked about self-organizing

00:24.630 --> 00:28.620
maps Balsall machines and now we're proceeding to our own cores.

00:28.860 --> 00:35.220
So congratulations that you got this far in the course and it's been a very exciting journey and now

00:35.220 --> 00:36.930
we're proceeding to the final stage.

00:37.050 --> 00:38.730
So let's get started.

00:39.060 --> 00:40.880
What is an ardent coater.

00:41.190 --> 00:44.120
Well this is what an artist called quarter looks like.

00:44.400 --> 00:51.390
And right away by the colors you can tell the good news that we are back to that directed types of neural

00:51.390 --> 00:51.990
networks.

00:52.110 --> 00:57.230
And this is a directed type of neural network the blue lines don't have arrows on the ends but we'll

00:57.270 --> 01:03.060
disagree that it is a directed type of network and everything is moving from left to right.

01:03.060 --> 01:07.900
So how does a outer enclosure work and what's what's the whole purpose of an auto encoder.

01:08.070 --> 01:13.340
Well just as the name suggests and adn coder encodes itself.

01:13.350 --> 01:20.190
That's the purpose of what it's doing or the whole philosophy behind the ancora that it takes some sort

01:20.190 --> 01:26.610
of inputs puts them through her hidden layer and then then gets outputs but it aims for the outputs

01:27.030 --> 01:28.940
to be identical to the inputs.

01:28.950 --> 01:30.620
That's what it's trying to do.

01:30.620 --> 01:36.840
That's why that's what how are you going to be training or and quoters to set them up in such a way

01:36.840 --> 01:43.710
that on the output you get values which are equivalent to your inputs and from that you can tell that

01:43.830 --> 01:50.270
other coders are not a pure type of unsupervised learning algorithm.

01:50.370 --> 01:56.830
They are actually a self supervised deploring algorithm because they are comparing it to something on

01:56.830 --> 02:02.310
the end remember in bosomed machines we didn't even have outputs we didn't have to compare to any kind

02:02.310 --> 02:04.000
of labels or anything.

02:04.050 --> 02:06.240
And in self-organising that's the same thing.

02:06.240 --> 02:11.580
We didn't have anything to compare to we were just looking for features we were just looking for things

02:11.970 --> 02:18.030
whereas here we are looking for things we're looking for this hidden layer which is also called the

02:18.360 --> 02:20.160
coding layer or the bottleneck.

02:20.160 --> 02:26.640
We're looking for how to structure it but at the same time we are comparing to the outputs to certain

02:26.640 --> 02:27.810
values which are the inputs.

02:27.810 --> 02:33.990
So it is this kind of on the verge between surprise and as a prize but we'll stick to unsupervised for

02:33.990 --> 02:34.940
our encoders.

02:35.130 --> 02:36.840
And that's pretty much how it works.

02:36.840 --> 02:41.910
So you have inputs they get encoded and then they get decoded and they're compared and through that

02:42.270 --> 02:43.400
the training happens.

02:43.650 --> 02:50.230
And as you'll see because of all the things we know in from the previous parts of this course it'll

02:50.250 --> 02:54.310
be very easy for you to understand or of is there quite a simple model.

02:54.510 --> 02:59.880
When you combine all of things are you know and are right away now we just talked about the process

02:59.910 --> 03:04.110
how they compare the outputs the inputs and you can already imagine how information is propagated this

03:04.110 --> 03:06.810
way and then you have gradient descend going the other way.

03:06.810 --> 03:11.250
So you talk about all those things but just be prepared that this section is going to be or this part

03:11.250 --> 03:15.750
of the Course is going to be pretty easy for you and you're going to most likely breeze through it especially

03:15.750 --> 03:18.610
if you've been through the other parts of the course already.

03:18.900 --> 03:23.700
And one more thing on on the slide is what our current code is used for.

03:23.700 --> 03:25.950
Well there's a couple things that they can be used for.

03:25.980 --> 03:27.630
They can be used for feature detection.

03:27.630 --> 03:35.160
So once you've encoded your data these nodes will present certain features which are important in your

03:35.160 --> 03:40.260
data and then you can just look at them and get those features out of them or basically use those features

03:40.620 --> 03:41.660
in the future.

03:41.670 --> 03:45.990
They can also be used to build powerful recommender systems as you'll see from the practical tutorials

03:45.990 --> 03:53.190
in this course and they can be used for encoding basically as as they're designed they are designed

03:53.190 --> 03:59.640
to encode data and you could take data with lots and lots of values encoded into a smaller representation

03:59.880 --> 04:05.900
and then all you would have to carry around is this or Ann Coulter or this decoder part and your encoded

04:05.910 --> 04:06.230
data.

04:06.240 --> 04:07.640
So it would take up less space.

04:07.650 --> 04:10.970
So that's that's another use case for our own Kotas.

04:11.070 --> 04:14.480
All right so that was a quick breakdown of the architecture of Arterton coders.

04:14.490 --> 04:20.700
And now let's have an example have a look at an example of how they actually work so we can understand

04:20.700 --> 04:22.890
them better on an intuitive level.

04:22.890 --> 04:29.490
So there's a simplified antoh encoder of just four input nodes and two nodes in the hidden lair as we

04:29.490 --> 04:34.100
can see we've got four movies here and four movies are here.

04:34.110 --> 04:37.740
And one of these movies Well these are just movies that a person has watched.

04:37.760 --> 04:44.660
And we're going to be encoding the rating that that people have left for those movies so one means a

04:44.670 --> 04:48.620
person like that movie and zero means a person didn't like that movie.

04:48.640 --> 04:51.290
And so that's how it basically works.

04:51.310 --> 04:56.570
And now let's have a look at how this information can be encoded through the hour and quarter.

04:56.580 --> 05:01.860
And before we start I just wanted to say that this example actually comes from this blog probably dance

05:02.090 --> 05:03.280
it's a it's a great blog.

05:03.300 --> 05:10.560
It's by a person who's actually a programmer who isn't a deep learning scientist as I understand but

05:10.890 --> 05:16.650
he really broke it down into good steps so we'll link to this at the end as additional reading.

05:16.650 --> 05:18.170
Very great read here.

05:18.330 --> 05:20.690
But for now I'll just walk through this example.

05:20.730 --> 05:21.590
So here we go.

05:21.720 --> 05:23.500
Let's have a look at these connections.

05:23.520 --> 05:29.820
First of all we follow training the hour and quarter which is going to come up with certain connections

05:29.820 --> 05:32.520
right away certain weights just to prove.

05:32.520 --> 05:38.880
So this whole example is to prove that it is possible it is possible to take four values and encode

05:38.880 --> 05:40.740
them into actually two values.

05:40.950 --> 05:48.110
And you know carried that around and basically save space and extract those features and is just examples

05:48.200 --> 05:52.170
just to show that this whole situation is possible so we're not going to worry about the training what

05:52.220 --> 05:55.980
we're going to be don't worry about anything else we're just going to see how is this possible this

05:55.980 --> 05:56.580
looks.

05:56.580 --> 05:57.960
This sounds like magic right.

05:57.960 --> 05:59.790
So let's see how this is possible.

05:59.790 --> 06:05.730
First of all we're going to color our synopses in two different colors blue and black or light blue

06:05.730 --> 06:11.160
and black where a light blue is basically a multiplication by one.

06:11.160 --> 06:17.310
So your weight is plus one and black is a multiplication by minus 1 so your weight is minus one and

06:17.520 --> 06:22.410
the other thing I wanted to mention here in out in code is you normally use that test the hyperbolic

06:22.410 --> 06:25.040
tangent activation function here and here.

06:25.290 --> 06:29.070
And we're not going to be doing that which is going to in this specific example we're going to just

06:29.070 --> 06:33.560
worry about the weights and we're just going to forget about the activation function for now completely.

06:33.570 --> 06:36.720
This will just help us understand everything on an intuitive level.

06:36.720 --> 06:42.570
So there we go that's how we're going to structure this specific network of specific out and coater

06:42.570 --> 06:47.850
we've already predefined the weights as such and this will help us understand that everything is possible

06:47.850 --> 06:49.890
this is just a specific example.

06:49.950 --> 06:52.620
So let's have a look at an input.

06:52.830 --> 06:55.840
Let's say as an input we've got 1 000.

06:55.920 --> 06:59.910
So the person just like moving number one and they disliked the rest of the movies.

06:59.910 --> 07:03.510
So what will the hidden nodes look like in that case.

07:03.520 --> 07:04.810
Well in that case he.

07:04.920 --> 07:12.270
Will be this one will turn into one here and this one will turn into one here because Blue is multiplying

07:12.270 --> 07:15.600
by one and these zeros they will always just add zero.

07:15.600 --> 07:18.400
So basically they're not going to contribute to the hidden nodes.

07:18.660 --> 07:20.840
And once you have that in.

07:20.910 --> 07:26.630
Now let's calculate each one of the output nodes says go into a tedious process but it's worth it.

07:26.640 --> 07:35.700
So there's our input going into these into the hit nose analyse what happens for the top node we multiply

07:35.700 --> 07:39.990
by a plus one by plus 1 and we get 2 for this knowledge we get.

07:39.990 --> 07:43.950
One times one equals one one times one is one because minus one.

07:43.980 --> 07:49.740
You add them up you get zero here you get minus one plus one equals zero and you get minus one minus

07:49.740 --> 07:51.360
one equals minus two.

07:51.360 --> 07:52.900
So those are your outputs.

07:53.100 --> 07:56.640
But those are actually preliminary outputs in a Aalto encoder.

07:56.640 --> 08:02.760
We also have a soft max function on end and we have a tutorial on the soft max function in one of the

08:02.760 --> 08:04.920
very early parts of the course.

08:04.930 --> 08:09.780
It's a bonus tutorial so now is the time to probably go revisit that tutorial if you skipped it at this

08:09.780 --> 08:10.560
stage.

08:10.560 --> 08:16.180
Basically what self-mocking function does in this case is it takes the highest value.

08:16.320 --> 08:20.340
So in this case are two and it turns that into one and everything else into zero.

08:20.340 --> 08:25.920
So if you apply a soft Max you will get one where you see the highest value and then zeros where you

08:25.920 --> 08:27.110
see everything else.

08:27.150 --> 08:34.380
And as you can see this result is indeed identical to what we input into our network.

08:34.380 --> 08:35.710
All right so that's a start.

08:35.730 --> 08:39.300
Let's have a look at some other cases we're not going to be as detailed.

08:39.300 --> 08:45.780
We're just going to look at the results so if you input 0 1 0 0 you will see that you get a 2 over here

08:45.930 --> 08:47.900
is 0 2 minus 2 0.

08:47.940 --> 08:51.950
And when you play this off Max you get 0 1 0 0 again identical.

08:52.270 --> 08:59.340
If your inputs are one over here you don't want to hear if you inputs a 1 or over here you get a one

08:59.370 --> 09:00.250
over here.

09:00.480 --> 09:06.960
So as you can see as long as in our dataset we've got rows with just three zeros and one one we can

09:06.960 --> 09:13.640
encode them into a small format where we just have two values so we just have to have these hidden nos

09:13.650 --> 09:20.340
as you can see every state is presented by hidden nodes so you have to have 1 1 you have 1 minus 1 you

09:20.340 --> 09:24.230
have 1 minus 1 1 and you have minus 1 minus 1.

09:24.390 --> 09:29.780
Every state is represented by Hitler and then you just need these weights.

09:29.780 --> 09:36.210
The knowledge about these weights to reconstruct your output is a very simplified example and it was

09:36.330 --> 09:43.470
even quite now looking back it is even quite straightforward that if you have four states here for possible

09:43.470 --> 09:49.080
states then you know you have to buy two combinations you are four in total but this gives a overview

09:49.080 --> 09:53.660
of how our encoders work in it again in a very simplified way.

09:53.670 --> 09:56.540
Now they of course are much more complex than that.

09:56.550 --> 10:02.830
So here you can see a much more sophisticated example where you way more so you have one two three four

10:02.830 --> 10:10.540
five six seven eight nine 10 11 inputs to hit a nose and 11 outputs and it still works totally fine.

10:10.540 --> 10:16.690
And this is a reference for additional reading to that same blog that we already mentioned.

10:16.720 --> 10:21.710
It's called neural networks are impressively good at compression by multi Scroop.

10:21.970 --> 10:26.170
And we'll include in the additional resources of course so definitely check it out.

10:26.170 --> 10:28.120
Very nice.

10:28.120 --> 10:32.620
Nicely written very easy introduction into our encounters.

10:32.620 --> 10:38.290
In fact it's I don't think it's even actually mentioned that it's an Kotas from the very start just

10:38.290 --> 10:43.660
neural networks and then in the comments you can read that indeed this was they were talking about order

10:43.660 --> 10:45.700
and courters so have a look at that.

10:45.700 --> 10:48.090
And I look forward to see you next time.

10:48.100 --> 10:49.860
Until then enjoy learning.
