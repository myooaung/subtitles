WEBVTT

00:00.780 --> 00:04.340
Hello and welcome back to the course on artificial intelligence.

00:04.530 --> 00:09.470
I hope you're excited about today's tutorial because we are taking our very first step into the world

00:09.470 --> 00:10.070
of way.

00:10.410 --> 00:13.100
And today we're talking about reinforcement learning.

00:13.230 --> 00:18.670
It's a very important story because it will underpin everything else is going to be happen in this course.

00:18.720 --> 00:20.970
So let's get started here.

00:20.970 --> 00:27.090
We've got a little maze and this maze is our representation of an environment and that's what we're

00:27.090 --> 00:29.160
going to be dealing with in this course.

00:29.160 --> 00:33.990
We're going to be dealing with certain environments in which our artificial intelligence is going to

00:33.990 --> 00:39.900
be performing is going to be taking actions is going to be looking to beat these enormous screens she'll

00:39.900 --> 00:41.910
be looking to win in these environments.

00:42.300 --> 00:44.130
And here we've got an agent.

00:44.310 --> 00:46.940
The agent is our artificial intelligence.

00:46.980 --> 00:52.860
That's the person or that's the mind that's going to be navigating these environments and learning from

00:52.950 --> 00:57.070
the feedback that one's going to be giving it in order to perform certain actions.

00:57.090 --> 01:02.130
And so the way it works is the agent perform certain actions in this environment.

01:02.310 --> 01:09.000
And as a result the state in which it is in will change so it might be further or closer or more to

01:09.000 --> 01:09.990
the left more to the right.

01:09.990 --> 01:15.140
It might have some of the other parameters that describe its state and those parameters and.

01:15.180 --> 01:20.920
The state is going to change because of the action takes and it will also get rewards based on the action

01:20.930 --> 01:24.900
so every time it takes an action the state will change and it will get a reward.

01:24.900 --> 01:29.120
Now bear in mind sometimes it might happen that it won't change the state the action won't change a

01:29.120 --> 01:33.030
state or there won't be a reward for taking that action.

01:33.060 --> 01:34.460
In that sense it was.

01:34.620 --> 01:38.430
But nevertheless the agent's going to keep doing that is going to be taking actions changing the state

01:38.430 --> 01:44.070
getting rewards changing action taking actions changing the state and getting rewards and by doing that

01:44.070 --> 01:48.810
process it's going to be learning about what is going to be exploring the environment understanding

01:48.840 --> 01:55.790
what actions lead to good rewards and favorable states and what actions the rewards an unfavorable state.

01:55.950 --> 01:59.640
And this is a very simplistic representational very global problem.

01:59.640 --> 02:04.350
So if you think about it environments actually don't have to be just Mayes's.

02:04.350 --> 02:09.120
It's not just about getting out of a maze or finding a treasure in a maze.

02:09.120 --> 02:11.690
An environment can be pretty much anything in life.

02:11.700 --> 02:15.120
So imagine you waking up in the morning and cooking an omelette.

02:15.360 --> 02:21.960
So in order to make that omelet you need to go through certain steps you need to get the salt get the

02:21.960 --> 02:27.590
eggs get the frying pans which to fire on and so on and it does sound like a routine mundane thing.

02:27.690 --> 02:29.910
But it's become routine because you've done it so many times.

02:29.910 --> 02:34.620
But in reality it's an environment where you're performing certain actions you're taking you putting

02:34.620 --> 02:40.380
the fire on you putting a frying pan on the fire you putting all the eggs into the frying pan and you

02:40.410 --> 02:43.140
putting some salt on the eggs and you're turning over and so on.

02:43.140 --> 02:49.920
So as you can see they are CRN actions actions which are taking in certain states and those actions

02:49.920 --> 02:52.410
lead to certain other states and sometimes reward.

02:52.410 --> 02:58.800
So for instance when you put the fire on and you wait wait wait wait wait wait wait wait wait too long

02:59.190 --> 03:01.810
and then you put the eggs in into the frying pan.

03:01.860 --> 03:03.510
The rewards are going to be very negative.

03:03.510 --> 03:04.960
It's all going to burn.

03:05.070 --> 03:10.100
On the other hand if you do all the all the correct actions in the correct time so it's also very important

03:10.100 --> 03:13.800
to understand that actions should be taken at the correct points in time.

03:13.800 --> 03:20.040
So for instance putting the salt in the frying pan before you put the eggs in might not be the best

03:20.040 --> 03:20.720
idea.

03:20.730 --> 03:26.160
You might want to take that action of putting a salt into the frying pan after the eggs are in there.

03:26.160 --> 03:28.210
So in the in a different state.

03:28.320 --> 03:29.570
So it's important to remember that.

03:29.730 --> 03:34.020
And at the same time so if you take all the correct actions in the correct order in the correct states

03:34.530 --> 03:38.790
your final reward could be that you get an omelet which you can eat.

03:38.850 --> 03:42.000
And so that's a very basic activity in your life.

03:42.000 --> 03:47.790
But if you think about it it is actually an environment and you are the agent going through this environment

03:47.880 --> 03:52.170
and perform a task you don't really need to learn anything because you already know it pretty well.

03:52.170 --> 03:56.110
But at same time you could learn maybe you could learn how to make a better omelet or especially if

03:56.300 --> 03:59.760
it's your first omelet that you're making you're probably going to screw it up but you will learn from

03:59.760 --> 04:05.910
that because you will understand what actions lead towards states and rewards and anything else in life.

04:06.000 --> 04:11.850
For instance even trading on the stock market and you know buying and selling and getting certain feedback

04:11.850 --> 04:16.340
from the market in the sense of return positive or negative returns.

04:16.380 --> 04:20.160
That's also an environment that's you participating in that environment as an aged.

04:20.160 --> 04:25.170
Driving a car is also an environment where you can turn the steering wheel you can accelerate you can

04:25.170 --> 04:30.180
break and so on and you're getting feedback from the environment and you one of those feedbacks is the

04:30.180 --> 04:35.970
policeman giving you a speeding fine if you're going above the acceptable or allowed speed limit on

04:35.970 --> 04:36.910
that highway.

04:36.990 --> 04:41.850
And therefore from there you learn that that's not something that should be done because it leads to

04:41.850 --> 04:43.080
a negative reward.

04:43.140 --> 04:45.540
So rewards don't have to be just at the very end of the process.

04:45.540 --> 04:47.970
They can be throughout the journey throughout the process.

04:47.970 --> 04:49.440
So those are a couple of examples.

04:49.440 --> 04:55.200
And in terms of I the simplest way to think of reinforcement learning is like training a dog when you

04:55.590 --> 04:58.140
train the dog you to give it certain commands.

04:58.140 --> 05:02.510
And if it obeys those commands then you give it a reach you give it like a biscuit or something if it

05:02.510 --> 05:06.710
doesn't Abeles Kamaz you tell it that it's a bad dog or you just don't give it a treat.

05:06.780 --> 05:13.770
And through that process it learns what certain commands or what it needs to do what action it needs

05:13.770 --> 05:18.380
to take in certain states and the states are the commands that you're giving it.

05:18.420 --> 05:22.620
And based on that it will get some certain rewards of course in the world of AI.

05:22.620 --> 05:24.540
It's not that complex.

05:24.540 --> 05:26.830
You don't have to give the treats.

05:26.910 --> 05:32.070
You don't have to have like a bag of biscuits with you every time you just give it a plus one or a minus

05:32.070 --> 05:37.240
one so it's a huge advantage that in the world of AI we've created these AIs ourselves.

05:37.260 --> 05:41.500
So the rewards that we're giving them if you think about this is really cool.

05:41.520 --> 05:47.280
The rewards are giving them they don't actually exist they're just a plus or minus one or plus a one

05:47.280 --> 05:48.440
or a zero or something.

05:48.450 --> 05:51.050
So it's all nonexistence all the Majerus stuff.

05:51.060 --> 05:56.250
But at the same time it leads to great results as we can create these amazing things these amazing artificial

05:56.250 --> 06:01.680
intelligence as by this amazing artificial intelligence by just providing rewards we don't really exist

06:01.740 --> 06:05.700
plus and minus one doesn't cost anything but some dumb elites results.

06:05.820 --> 06:08.100
So very similar to real world.

06:08.160 --> 06:14.880
And you know for example Dokes But here the rewards are digital and just numbers.

06:15.060 --> 06:20.880
And with that in mind we can talk about about robot dogs I love this example so this is just around

06:20.880 --> 06:26.910
in pictures not necessarily that exact robot dog that is trained through reinforcement learning some

06:26.970 --> 06:28.930
robot dogs especially the older ones.

06:29.070 --> 06:30.980
You'd have an algorithm in there.

06:31.320 --> 06:39.210
And this is actually a good example of the difference between preprogramed agents and reinforcement

06:39.210 --> 06:39.810
learning agent.

06:39.810 --> 06:47.370
So you could have a robot dog which is preprogrammed to how to walk it will say so in the in the algorithm

06:47.370 --> 06:54.270
behind the dog and the software will say OK so in order to walk you need to move your left leg forward

06:54.270 --> 06:58.950
left front leg forward then your back right leg forward then your front right leg forward then you back

06:58.950 --> 07:03.620
left leg forward and repeat that action and you know that's that's the definition of walking is a function.

07:03.630 --> 07:09.020
Inside this dog then it might have you know how to sit how to stand and things like that.

07:09.630 --> 07:16.660
Whereas in a robot dog that is trained through reinforcement learning what happens is you don't preprogrammed.

07:16.680 --> 07:23.760
This is the key concept to everything here that you don't have any algorithm inside that is hard coded

07:23.760 --> 07:24.800
into the dog.

07:24.810 --> 07:28.410
Instead you have what we'll be discussing in the future.

07:28.410 --> 07:36.660
You have this reinforcement learning algorithm which is told that OK so the goal is from to get from

07:36.810 --> 07:41.940
where you are now not knowing anything to that to the end of the room for example.

07:42.120 --> 07:44.220
And here are the certain actions you can take.

07:44.220 --> 07:48.900
You can move your right foot you can move your left foot you can move your right back foot you are left

07:48.900 --> 07:52.950
back foot so here all the degrees of freedom you can do you can move them like this you can move like

07:52.950 --> 07:59.130
that so like a list of actions you can take and your rewards are every time you take a step forward

07:59.140 --> 08:01.380
you get a plus one every time you fall over.

08:01.380 --> 08:04.050
You get a minus one and that's all there is to it.

08:04.080 --> 08:07.340
And then they just leave the dog and let it figure it out on its own.

08:07.350 --> 08:13.290
So the dog tries to stand up it falls and then it realizes that OK I shouldn't do that action that led

08:13.290 --> 08:17.620
to me falling because every time I fall I get a minus one which is not good for me then so does the

08:17.620 --> 08:18.990
other action that helped him stand up.

08:18.990 --> 08:23.970
And then it figures are just experiments experiments experiments tries things randomly and then figures

08:23.970 --> 08:29.990
out that it can make a step forward by moving its right front foot and then he gets a plus one and realize

08:30.060 --> 08:31.360
oh I should do more of that.

08:31.410 --> 08:35.560
Ok cool so it now learns that it should do more of this and less of that.

08:35.580 --> 08:42.240
And through this learning process it quickly very quickly understands how it can walk.

08:42.360 --> 08:49.080
And those those dogs that figured out on their own can actually sometimes walk better than dogs that

08:49.080 --> 08:53.880
are preprogramed because really preprogrammed things we look at the real live dogs and or you know we

08:53.880 --> 08:59.910
use our own imagination how to do it whereas a reinforcement learning dog can optimize things on its

08:59.910 --> 09:00.250
own.

09:00.270 --> 09:03.490
And because in AI sometimes it can get even better results.

09:03.630 --> 09:05.230
And that's how they can train these robot.

09:05.260 --> 09:07.260
The same robot dogs to play soccer.

09:07.470 --> 09:12.920
You can train a normal dog to play soccer because you know simply the whole approach is different.

09:12.930 --> 09:20.850
And it's not something that you know probably a normal dog has been trained to do or has ever done in

09:20.850 --> 09:22.980
its process of its evolution.

09:22.980 --> 09:28.140
Whereas a reinforcement learning robot dogs can very easily understand how to play soccer as long as

09:28.140 --> 09:32.690
you tell them what the rewards are what the goals are what the possible actions they can take are.

09:33.030 --> 09:36.320
So that is how reinforcement learning works.

09:36.330 --> 09:39.120
In general that's a quick overview of reinforcement learning.

09:39.120 --> 09:45.450
I hope that got you very excited about was going to come next because it's a completely different world

09:45.480 --> 09:51.930
compared to preprogramed solutions or hard program hardcoded solutions where you have the if else conditions.

09:51.930 --> 09:53.710
This is very different.

09:53.760 --> 09:55.960
And we're going to be talking more about that.

09:56.100 --> 10:03.360
In the meantime we've got some additional reading for you so if you'd like to have some supporting materials

10:03.640 --> 10:06.760
Here's a great article which you can look and look in.

10:06.790 --> 10:09.240
It's called simple reinforcement learning with tensor flow.

10:09.370 --> 10:10.510
It's got 10 parts.

10:10.510 --> 10:17.840
The link is here and you'll find the full clickable link on in the course resources by Arthur Giuliani's

10:17.920 --> 10:19.500
2016 article.

10:19.900 --> 10:24.720
And you can fall long discourse and also get additional information from that article.

10:24.730 --> 10:29.950
But bear in mind that that article is tends to flow where as in this course we are using pi torch so

10:30.460 --> 10:33.180
different implementation but implantations.

10:33.190 --> 10:39.580
But at the same time you might pick up a few things here and there that might supplement your learning

10:39.960 --> 10:41.220
that we're going to be doing in this course.

10:41.230 --> 10:44.860
So great articles follow you and if you're not considering following it for sure.

10:44.870 --> 10:50.940
Still just in case check out that that first part and see if you like it see if you'd like to read it

10:51.040 --> 10:51.840
a bit more.

10:52.150 --> 10:58.180
And then we've got specific to this tutorial a border enforcement learning there's a paper by Richard

10:58.180 --> 11:00.340
sudden which is called reinforcement learning.

11:00.400 --> 11:08.110
One introduction is the 1998 papers are quite old but at the same time you can learn a bit about reinforcement

11:08.110 --> 11:13.900
learning some of the examples like that omlet example and other examples of where reinforcement learning

11:13.900 --> 11:19.900
can be applied and just a general overview over and forth and learning if you are looking for some additional

11:19.990 --> 11:20.680
reading.

11:20.740 --> 11:23.160
And on that note we're going to wrap up this tutorial.

11:23.170 --> 11:24.610
Can't wait to see you next time.

11:24.610 --> 11:26.520
And until then enjoy AI.
