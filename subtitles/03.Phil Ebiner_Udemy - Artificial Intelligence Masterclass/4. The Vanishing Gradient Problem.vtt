WEBVTT

00:00.500 --> 00:05.520
Hello and welcome back to the course on deep learning I hope you enjoyed the introduction into our hands

00:05.600 --> 00:07.930
and today right away off the bat.

00:07.940 --> 00:12.560
We're going to jump into a huge problem that exists with our own ends.

00:12.560 --> 00:17.290
All right so what is this problem called the vanishing gradient.

00:17.320 --> 00:24.990
It is first discovered by Sep Hokke writer and I hope I'm pronouncing that right.

00:25.020 --> 00:31.170
I know we have students from Germany so you can comment if the pronunciation is incorrect but e he said

00:31.460 --> 00:44.260
Sep or Joeseph is his full name is a genius scientist who back in the 90s while he was still not a professor

00:44.280 --> 00:48.790
is a recent photo so he was much younger when he actually came up with this concept.

00:48.800 --> 00:54.140
He found that there was a big big problem and we'll talk about the the problem solved just now.

00:54.140 --> 01:00.290
Just wanted to point out who are the people that spotted this and basically as you'll learn from the

01:00.290 --> 01:09.170
further Charles SAP is one of the founding people in the modern way that we use our own ends and LACMA

01:09.200 --> 01:11.920
and we'll talk about that further down but this is hip hop right.

01:11.920 --> 01:18.980
So I just want to make sure you know who is behind all this and the second person is Yoshio banjo.

01:19.100 --> 01:24.260
He's a professor at the University of Montreal.

01:24.260 --> 01:32.960
He also discovered that this problem I think a bit later in 1994 so CEP was discovered in 1991 Yoshio

01:33.380 --> 01:35.130
wrote about this in 1994.

01:35.480 --> 01:41.860
But again this is another person driving this whole trip pushing the envelope in the space of our own

01:41.870 --> 01:42.170
ends.

01:42.200 --> 01:50.690
And if you go to the University of Miami and you look up Yoshio your profile you will find that Russia

01:50.720 --> 01:54.830
has over five hundred research papers.

01:54.830 --> 01:58.560
And by the way there is Yoshio banjo just hanging out with look.

01:58.850 --> 02:04.310
As you can see they all know each other it's a very tight knit community and it does sound like a conspiracy.

02:04.310 --> 02:08.470
These people that are driving are pushing the envelope in terms of deep learning.

02:08.740 --> 02:14.300
These are just like a select group of people who are all all in it together they all know what's going

02:14.300 --> 02:14.840
on there.

02:14.900 --> 02:19.040
Been doing it since the 80s or 90s and now it's all popping out into the world.

02:19.310 --> 02:25.070
So you know that's just to give you a quick idea of who's behind all this and today and now we're proceeding

02:25.070 --> 02:28.050
to the vanishing problem itself.

02:28.070 --> 02:33.950
So as you remember this is the gradient descent algorithm we're trying to find the Loek the global minimum

02:34.250 --> 02:40.620
of your cost function and that's going to be the optimal solution optimal set up for your neural network.

02:40.910 --> 02:49.670
As you also recall your gradient or your information travels through your neural network to your answer

02:49.970 --> 02:55.400
to get your outputs and then the error is calculated and is propagated back to the network to update

02:55.430 --> 02:56.810
the weights.

02:56.810 --> 02:58.610
And here we've got some waitron out.

02:58.610 --> 03:03.250
So what happens in a recurrent neural network is a similar thing.

03:03.260 --> 03:06.970
But here you've got a bit more going on right.

03:06.980 --> 03:10.740
So when your information trials through the network it travels like that.

03:10.760 --> 03:17.420
You've got lots of travels through time and information from previous timepoint goes keeps coming through

03:17.420 --> 03:21.860
the network and remember that every note here it's always very important to remember neural networks

03:21.860 --> 03:30.350
that every single node here is not just a node it's a representation of a whole layer of nodes remember

03:30.350 --> 03:33.280
we're looking at from like from the top or from the bottom.

03:33.320 --> 03:41.080
They are they actually extend through this chart down there into into this presentation you can see

03:41.250 --> 03:46.700
there's lots more neurons behind the ones that we can actually see because each one represents Alair

03:47.020 --> 03:48.310
very important to remember that.

03:48.440 --> 03:53.930
And so at each point in time you can calculate your cost function or your error.

03:54.110 --> 03:59.990
So basically your cost function compares your output which is in the right circle to your desired output

03:59.990 --> 04:02.450
to what you should be getting.

04:02.450 --> 04:04.180
And this happens during the training.

04:04.370 --> 04:08.760
And so and you have these values for throughout the time series.

04:08.780 --> 04:15.950
So for every single one of these red circles you can calculate the cost function and let's focus on

04:15.950 --> 04:16.310
one.

04:16.310 --> 04:20.930
Just to understand what's going on let's focus on this one specifically at time t.

04:20.960 --> 04:27.020
So you've calculated the cost function Epsilon team and now you want to propagate your cost function

04:27.020 --> 04:28.120
back through the network.

04:28.130 --> 04:29.120
How is this going to look.

04:29.120 --> 04:30.420
Because you need to update the weight.

04:30.440 --> 04:35.370
So every single neuron which participated in the calculation of the output associated with this cost

04:35.390 --> 04:42.500
function should be should have their weight updated to in order to help it better calculate the output

04:42.530 --> 04:44.020
to minimize that error.

04:44.030 --> 04:50.240
And the thing here is that it's not just the neurons in directly below excellente directly below this

04:50.240 --> 04:52.420
red circle it's all the neurons that contributed.

04:52.580 --> 04:53.830
And there's many more of them.

04:53.840 --> 04:59.150
There's all of these neurons as far back as you go depending on how many times steps you take you might

04:59.150 --> 05:04.840
take one and take two I take 50 ton steps depending on how you set up your network.

05:04.980 --> 05:10.980
And this is where the problem lies that you have to update or you have to propagate all the way back

05:11.640 --> 05:13.750
through time to these neurons.

05:13.830 --> 05:17.700
And we talk when we talk about time it's not that the problem is that we can travel through time not

05:17.700 --> 05:20.240
at all that we've unraveled this network.

05:20.250 --> 05:22.630
So this whole propagation can be facilitated.

05:22.830 --> 05:27.990
The problem lies in something else and as much I don't like putting math on the slides on intuition

05:27.990 --> 05:32.130
slides we're not going to go through the math but I'll point out one thing here so this is the math

05:32.130 --> 05:39.200
behind our Arnalds and we'll definitely direct you to more additional reading which you can do to upskilling

05:39.260 --> 05:40.660
on all of these math.

05:40.740 --> 05:47.730
So here we've got w rec and W stands for weight and recurring and that is the weight that is used to

05:48.150 --> 05:57.690
connect the hidden layers to themselves in the unrolled temporal loop and as you can see here on the

05:57.690 --> 06:04.690
left you're in order to get from XTi minus history from this layer remember this is a letter to you

06:04.690 --> 06:04.940
might.

06:04.940 --> 06:08.810
Do you need to apply double check then from here to here.

06:08.820 --> 06:09.590
Appliable right.

06:09.690 --> 06:16.740
And in simple terms into intuitive terms what you're doing is you're simply multiplying the values here

06:16.740 --> 06:20.890
as you remember to get from one large index we multiply the output by the way.

06:20.910 --> 06:24.410
And then we get to the next line and then multiply the output by the way and get to the next.

06:24.420 --> 06:25.930
Then we must the output from here.

06:25.950 --> 06:26.910
By the way get here.

06:27.150 --> 06:32.190
And the thing here is that we're multiplying with the same exact weight multiple times many times as

06:32.190 --> 06:34.820
many times as we need to go through this temporal loop.

06:34.830 --> 06:39.450
And this can be set this you can set this in your network do you want to do it once you want to look

06:39.450 --> 06:43.170
back one step you'll look back three steps you'll look back 50 steps.

06:43.170 --> 06:46.930
But as many times as we do it we have to multiply by the weight.

06:47.070 --> 06:55.350
And this is where the problem arises because when you multiply by something small your value decreases

06:55.350 --> 06:59.620
very quickly and this multiplication comes from this.

06:59.640 --> 07:03.740
Here you can see that PS stands for multiplication so we have to multiply.

07:03.810 --> 07:06.850
And that's what it's representing as far as you go back you multiplying.

07:07.080 --> 07:12.930
And as you remember weights are assigned at the start of your neural network.

07:12.990 --> 07:18.810
As you see in the practical tutorials your weights are assigned as a random value but run of those clusters

07:18.810 --> 07:19.860
or and from there.

07:19.920 --> 07:23.400
The network trains them up and identifies what they should be.

07:23.610 --> 07:30.360
But if you start with random double random w close to zero then because you're multiplying but many

07:30.360 --> 07:34.110
times the more you multiply the lower the value gets.

07:34.110 --> 07:39.900
So if you start off you might have a certain gradient going through your network being back propagated

07:39.900 --> 07:41.020
through your network.

07:41.490 --> 07:47.040
Then you move backwards your gradient becomes less then your brain becomes less and then your brain

07:47.040 --> 07:48.270
becomes even less.

07:48.270 --> 07:51.540
And what does that mean for the network and this is the important part too.

07:51.780 --> 07:53.150
Kind of like get your head around that.

07:53.160 --> 07:57.400
What does a vanishing gradient like that why is it bad for the network.

07:57.410 --> 08:03.150
Well because the gradient as it goes back through the network it is used to update the ways and we know

08:03.150 --> 08:03.880
that already.

08:04.140 --> 08:09.390
Well the lower the gradient is the harder it is for the network to update the weights.

08:09.390 --> 08:12.490
It cannot like it.

08:12.630 --> 08:19.710
Understand what kind of contribution each of these outputs has towards the error and therefore we can

08:19.710 --> 08:20.380
update the weights.

08:20.400 --> 08:25.650
But the lower the gradient the slower is going to update the weights and the higher the gradient the

08:25.650 --> 08:29.030
fosters going to update the weights and get to the final result.

08:29.220 --> 08:36.330
And so therefore if you had like for instance a thousand packs these weights for for this layer might

08:36.330 --> 08:42.420
be updated towards the end of the thousand and you'll have some final results but these weights because

08:42.420 --> 08:49.170
the greens are so much smaller they're going to be up there it's slower and therefore by the end of

08:49.170 --> 08:54.460
the thousand bucks you might not have the final results there and therefore this part of the strain

08:54.630 --> 08:58.640
this part of network is not trained based on this cost function.

08:58.680 --> 09:04.980
But the problem here is not just that half your network is not trained properly but also that these

09:05.430 --> 09:12.080
weigh these weights are this Lehre it's outputs are being used as inputs for further Lares.

09:12.120 --> 09:16.070
So the training here has been happening all along.

09:16.200 --> 09:22.620
Based on vore based off of inputs that are coming from untrained neurons untrained Lares So it's kind

09:22.620 --> 09:27.040
of like a vicious cycle you're you're training here and you think you're getting good results.

09:27.060 --> 09:34.320
But because you're And so small here this is training so slow and outputs is giving is so are are incorrect

09:34.410 --> 09:40.110
are not final outputs and therefore your training on the non-final outputs so because of this disbalance

09:40.140 --> 09:48.060
because of vanishing grading you have a a problem in your network which is not just that these weights

09:48.060 --> 09:52.110
are not being trained properly the whole network has not been trained properly because these weights

09:52.110 --> 09:53.350
are not being trained properly.

09:53.460 --> 09:59.820
And and that's there's like a domino effect like this wherever you look in the network always the ones

09:59.820 --> 10:01.780
at the very back are going to have that problem.

10:01.920 --> 10:08.240
And this is in a nutshell what the vanishing gradient problem for recurrent neural networks is.

10:08.470 --> 10:15.930
And that's was kind of the main roadblock to using recurrent neural networks.

10:15.990 --> 10:24.870
And let's summarize this in a in a short slide so if Jurek is small then your you have a vanishing grade

10:24.870 --> 10:29.150
problem if W is large you have an exploding Green problem because same thing.

10:29.160 --> 10:31.150
But then it's just going to explode.

10:31.200 --> 10:36.630
And here's an important thing to point out here is that of course there's more to it.

10:36.630 --> 10:36.870
Right.

10:36.870 --> 10:39.900
There's as you can see there's this formula.

10:39.910 --> 10:43.010
There's other things like the activation function and so on.

10:43.020 --> 10:46.340
So it's not as simple as small or large or less than one greater than one.

10:46.380 --> 10:50.400
But as a rule of thumb if you have very small weights you're going to have a Vashon gradient if you

10:50.400 --> 10:51.530
have very large weights.

10:51.590 --> 10:57.600
We put a in there the value of a hundred and then you're going to buy this you're going to have 10000.

10:57.600 --> 11:00.150
By Step three you're going to have a million.

11:00.150 --> 11:03.650
So then you have an exploding graded problem.

11:03.870 --> 11:09.360
So hopefully this summarizes explosion grade problem on an intuitive level because.

11:09.480 --> 11:13.590
So in short because you're unraveling the temporal loop.

11:13.770 --> 11:19.020
The further you go through your network the lower your gradient is and the harder it is to train the

11:19.020 --> 11:24.380
ways which has a domino effect on all of the further weights throughout the network as well.

11:24.420 --> 11:27.090
And so how do you combat vanishing ingrained problem.

11:27.120 --> 11:28.800
There's a couple of solutions.

11:29.010 --> 11:34.320
For instance for the exploding gradient you can have truncated BRACHT back propagation so you stop back

11:34.320 --> 11:38.610
propagating after a certain point but as you can imagine that's that's probably not optimal because

11:38.610 --> 11:40.760
then you're not updating all the weights.

11:41.130 --> 11:46.590
But if you don't stop then you're just going to have a completely irrelevant network so it's better

11:46.590 --> 11:48.790
than the original approach.

11:49.050 --> 11:55.530
Then you can have penalties so you could have the gradient being penalize and Beanery is artificially

11:55.530 --> 12:01.740
reduced you're going to have gradient clipping so you could have like a maximum limit for the gradient

12:01.740 --> 12:05.040
you could say never never have the gradient go over this value.

12:05.070 --> 12:10.680
And then if it's over that value just stays at that value as it propagates further down to the network.

12:10.680 --> 12:17.580
You can have the Vanish Ingredion problem you have CRN other solutions you have weight initialization

12:18.420 --> 12:25.450
where you are smart about how you initialize your weights to minimize the potential for vanishing gradient.

12:25.530 --> 12:33.030
You can have there's a type of network called the Echo state networks and we're not going to talk about

12:33.030 --> 12:39.430
that but they do somehow solve that or they are designed to solve the vanishing problem.

12:39.480 --> 12:44.570
But there's also a different type of network called The Long short term memory networks or the Elliston's

12:44.580 --> 12:53.580
for short which are extremely popular which are considered to be the go to network for implementing

12:53.640 --> 12:58.770
recurrent neural networks and that's exactly what we're going to be talking about in the rest of this

12:58.860 --> 12:59.430
section.

12:59.430 --> 13:01.620
So there's going to be very exciting.

13:01.620 --> 13:06.380
We're going to look at a brand new architecture for recurrent neural networks.

13:06.450 --> 13:08.190
Conway to get start on that.

13:08.610 --> 13:11.760
It's a very interesting topic to get into.

13:12.480 --> 13:19.050
But for now this is this brings us to the end of the vanishing Green problem to Tauriel and some additional

13:19.050 --> 13:19.560
reading.

13:19.620 --> 13:26.280
So additional reading you can definitely reference the original works by self harm writer and social

13:26.280 --> 13:26.570
change.

13:26.580 --> 13:30.980
So this is Cephus paper in 1991 from 1991.

13:31.140 --> 13:36.390
It's completely in German so I'm not even going to read the name but if you understand and can read

13:36.390 --> 13:39.920
German then definitely this could be a good read for you.

13:39.990 --> 13:45.210
Then there's Yoshua banjo's paper which is called Learn learning long term dependency of green dissent

13:45.240 --> 13:48.090
is difficult 1994.

13:48.090 --> 13:53.880
Also you can reference that but what I would recommend looking into is a paper called on the difficulty

13:53.880 --> 13:57.340
of training recurrent neural networks by Razvan Pascal.

13:57.390 --> 13:58.920
No it's just newer.

13:58.920 --> 14:02.970
It's 2013 and it's also got a banjo as the co-author.

14:03.000 --> 14:05.120
So probably you're supervising some of this research.

14:05.340 --> 14:12.480
And here it summarizes a lot of things that Yoshio banjo talks about in his paper from 1994.

14:12.480 --> 14:18.170
So why not look at something you or I would recommend checking this paper out.

14:18.630 --> 14:19.530
So there we go.

14:19.590 --> 14:22.110
That's the vanishing grading problem.

14:22.140 --> 14:23.050
Hope this was fun.

14:23.100 --> 14:26.600
Can wait to see on the next tutorial and until then enjoy deep learning.
