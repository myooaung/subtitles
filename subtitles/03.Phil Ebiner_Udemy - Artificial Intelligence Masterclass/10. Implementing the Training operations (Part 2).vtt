WEBVTT

00:00.240 --> 00:07.260
Hello and welcome to the final tutorial of this implementation and also step in our journey to the full

00:07.260 --> 00:08.460
world model.

00:08.460 --> 00:14.490
So we already implemented a big part of the training operations and actually we implemented the first

00:14.640 --> 00:20.470
half which consisted of getting the final last function and the cost.

00:20.520 --> 00:26.850
And now that we have it well we're ready to first compute the gradients of this loss with respect to

00:26.850 --> 00:27.540
the weights.

00:27.630 --> 00:32.410
And second of course apply the gradients to update the weights.

00:32.550 --> 00:36.710
And so we're gonna do basically exactly the same as what we adhere.

00:36.720 --> 00:42.920
Meaning following these steps you of course with our new nucleus here for the V.A. either was the callback

00:42.960 --> 00:48.440
label less but now for the engine Arnon we have this different list that we've just computed.

00:48.630 --> 00:53.070
But then it's going to be the same we're going to get the learning rate than the optimizer which will

00:53.070 --> 01:00.270
still be the atom optimizer then we will computed gradients from that same optimizer and then just before

01:00.360 --> 01:06.420
applying the gradient to update the weights we'll have to clip the gradients to avoid exploding gradients.

01:06.420 --> 01:09.630
That's something that we must do here for the R9.

01:09.630 --> 01:15.300
As you saw in the intuition lecturers of the island right remember we have to avoid the problems such

01:15.300 --> 01:21.490
as the vanishing gradient or the exploding regions and that we can do things to the clipping.

01:21.600 --> 01:26.690
So let's do this let's do this final big step of the training operations.

01:26.760 --> 01:34.410
And so now back in our Arnon let's start the same way by checking that we are indeed in training mode

01:34.830 --> 01:41.600
and that training mode we're going to get it again from our HP s hyper parameter container.

01:41.610 --> 01:46.760
And the name of the hyper parameter for that is is training.

01:46.770 --> 01:56.190
So just like for the V.A. itself that HP as that is training equals equals one then we will get that

01:56.190 --> 02:00.870
learning rate get that optimizer compute the gradients and apply them to obey the weights.

02:00.960 --> 02:04.610
So let's do this and let's start by getting the learning rate.

02:04.860 --> 02:10.770
So we're going to do that efficiently because we're going to have a lot of lines of code in common between

02:10.800 --> 02:13.390
our V.A. and our Indian Island.

02:13.410 --> 02:16.690
So for the learning rate it's going to be exactly the same.

02:16.770 --> 02:20.820
We're going to get it this way with the tensor flow variable class.

02:20.940 --> 02:27.090
So I'm copying this and I'm going to basically right here as the first line of this if condition.

02:27.090 --> 02:34.530
But the learning rate this time is going to be obtained directly from again our hyper parameter container

02:34.860 --> 02:40.980
and therefore instead of just bring cell that learning right here we have to put self that HP is that

02:41.010 --> 02:41.980
learning rate.

02:42.000 --> 02:43.620
Now we have the learning rate.

02:43.650 --> 02:52.140
All right then next step the next step is to this time get optimizer and actually it's going to be again

02:52.170 --> 02:58.860
exactly the same as this line of code here you know we take the optimizer we introduce a new object

02:58.860 --> 03:05.730
variable the optimizer and we get it by creating it as an object the Adam optimizer class from the train

03:05.730 --> 03:12.210
module by tensor flow and it's taking as usual one argument the learning rate which we've just created

03:12.210 --> 03:15.470
with the same object variable name self that are.

03:15.480 --> 03:22.200
So we can just copy this and not change anything here for the optimizer but don't worry it's not gonna

03:22.230 --> 03:23.160
be that easy.

03:23.160 --> 03:29.100
Then we'll need to change some things for what's left to code so let's base that and he would go we

03:29.100 --> 03:33.060
have our optimizer great and now next step.

03:33.060 --> 03:38.040
So now the next step which I think again is the same as the next step of the VAB.

03:38.190 --> 03:39.020
Yes indeed.

03:39.150 --> 03:41.060
The next step is to compute the gradient.

03:41.100 --> 03:47.160
So it's good that we do it twice so that you can remember better the process of the training operations

03:47.160 --> 03:51.110
which is very similar from the V to the ion n.

03:51.120 --> 03:54.170
So indeed right now we have to compute the gradients.

03:54.180 --> 03:57.010
And so let's just copy that again.

03:57.030 --> 03:59.210
We need to make a few changes.

03:59.280 --> 04:00.690
You might already guess what they are.

04:00.990 --> 04:09.240
So going by here pasting that here we're not going to call this time the gradients grads but G V S and

04:09.240 --> 04:14.330
that stands for Create and variables because indeed I've prepared the duck for you.

04:14.430 --> 04:17.360
Here it is the duck of the Adam optimizer class.

04:17.370 --> 04:21.750
I'm not sure I showed you this before but it's good that we see it now.

04:21.750 --> 04:28.200
Better later than never once again you have not of the old arguments that you have to input when creating

04:28.200 --> 04:29.220
an object of this class.

04:29.250 --> 04:32.640
So we just input the learning rate argument.

04:32.640 --> 04:38.370
But what's also very interesting is that you get all the duck of all the methods that you can come from

04:38.370 --> 04:44.730
this class and there is the apply gradient method which we will use later on to update the weights.

04:44.850 --> 04:52.470
But the one we're about to use right now is the other one the compute gradient method and this compute

04:52.470 --> 04:58.980
grading method as you can see returns not only the gradients but the list of the gradients and variable

04:58.980 --> 04:59.790
pairs.

04:59.800 --> 05:05.380
And so it was important to call that GV s to remind us that and that's also important because indeed

05:05.770 --> 05:11.050
what we're about to do right afterwards is to clip the gradients and to clip the gradients.

05:11.050 --> 05:14.900
We're going to need both the gradient and the variables of all these pair.

05:14.920 --> 05:17.740
So this time it's better to call that GV.

05:17.760 --> 05:18.630
Yes.

05:18.640 --> 05:26.380
And so here we go GV Yes the list of gradients and virals which are returned by this compute gradient

05:26.500 --> 05:32.920
method of the optimizer object created from the Adam optimizer class by tend to flow.

05:32.950 --> 05:38.860
And the other little change we had to do is of course here the less self that loss was the name of the

05:38.860 --> 05:45.640
less we gave for the V but now for the Indian origin we called that the cost which is exactly the same

05:45.640 --> 05:51.700
cost and thus are just some synonyms of the less error that you have between the predictions and the

05:51.700 --> 05:52.120
target.

05:52.150 --> 05:55.910
So we're just going to replace that by cost.

05:55.960 --> 05:56.700
And here we go.

05:56.740 --> 06:02.180
We have are gradients not only gradients but are gradients and variables.

06:02.200 --> 06:02.470
All right.

06:02.470 --> 06:08.890
So moving on to the next step now you might guess what this next step is I've just mentioned it once

06:08.890 --> 06:09.540
or twice.

06:09.670 --> 06:16.560
The next step is to clip the gradients so as to avoid exploding regions and so we're going to clip that

06:16.570 --> 06:22.540
meaning we're going to keep only some gradients in a certain range and this range is going to be decided

06:22.810 --> 06:27.100
by some parameters in our hyper parameter HP as container.

06:27.100 --> 06:30.690
Usually the range is from minus one to plus one.

06:30.700 --> 06:35.770
You can check that online if you type the clip by value function by tens.

06:35.780 --> 06:41.470
So which we're about to use we will see that the gradients are usually clipped between minus one plus

06:41.470 --> 06:48.760
one here we will clip them between minus graph clip and plus grep clip where graphically is indeed parameter

06:48.760 --> 06:51.660
of our hyper parameters container right.

06:51.670 --> 06:52.570
We can actually see it.

06:52.570 --> 06:54.490
I can show it to you right away.

06:54.490 --> 06:59.580
Remember we have all the values of the hyper parameters and on and values I'll give it to you.

06:59.590 --> 07:03.100
So there should be a greatly parameter here it is.

07:03.130 --> 07:05.310
And actually it is equal to 1.

07:05.320 --> 07:11.020
So exactly what I was just telling you but in case you want to change that you know you don't always

07:11.020 --> 07:13.730
have to clip it between minus 1 and plus one.

07:13.840 --> 07:20.170
You can clip it by another value here thanks to this hyper parameter but indeed by default it is going

07:20.170 --> 07:24.520
to be clipped as common practice between minus 1 and plus 1.

07:24.520 --> 07:31.960
All right so let's do this gradient clipping I'm going to close this and let's introduce a new variable.

07:32.050 --> 07:38.080
So this new variable is going to be of course the new clipped gradients which in English would also

07:38.080 --> 07:44.950
mean capped gradient so we're going to call that kept GV Yes and they're going to be the same genius

07:45.070 --> 07:51.880
here but without the ones being outside of the range so kept CBS equals.

07:51.880 --> 07:56.830
And now remember that Jesus is actually a list of several gradients and variables.

07:56.980 --> 08:02.680
And so in order to get the capped gradients for all the variables in that list.

08:02.680 --> 08:09.250
Well we'll have to do a full loop meaning we'll return again these kept Jesus in the same list of the

08:09.250 --> 08:16.130
same number of elements but for this since we're going to clip each one of the gradients in that list.

08:16.240 --> 08:22.680
Well we have to recreate that list again by doing a full loop within a pair of square brackets.

08:22.690 --> 08:25.360
So that's classic trick to know in Python.

08:25.360 --> 08:32.140
We're going to do a full loop now looping over all the elements in that list of the gradients and variable

08:32.140 --> 08:32.650
pairs.

08:32.830 --> 08:39.400
So speaking of pairs well that's what we need to get for it so I'm going to introduce some parentheses

08:39.670 --> 08:41.830
which will contain the pairs.

08:41.830 --> 08:49.180
And so the first element of the pair is the creation itself which will clip right now thanks to this

08:49.360 --> 08:51.270
clip by value function.

08:51.370 --> 08:57.070
And so speaking of this clip by value function well that's exactly what we need to do right now calling

08:57.070 --> 08:57.470
it.

08:57.610 --> 09:06.250
So I'm calling tensor flow first then from tenth floor I'm calling the clip by value function which

09:06.250 --> 09:08.200
takes an argument first.

09:08.200 --> 09:15.190
What you want to clip by value which are of course the gradient so all meaning all the gradients in

09:15.210 --> 09:15.670
Java.

09:15.700 --> 09:22.210
So we're gonna call each of the gradients in GV as grad so this is going to be the looping variable

09:22.390 --> 09:23.130
in the for loop.

09:23.620 --> 09:24.280
So great.

09:24.370 --> 09:30.370
And then the next two arguments we have to input are the values of course you know the values in the

09:30.370 --> 09:33.620
range inside which want to clip the gradients.

09:33.640 --> 09:41.530
And so as we've just said these values are going to be minus self that HP at a great clip and plus self

09:41.530 --> 09:43.080
that HP has that correctly.

09:43.090 --> 09:50.770
Which are the minus and plus values of this great clip parameter equal to one point zero by default.

09:50.770 --> 09:51.030
All right.

09:51.040 --> 10:01.210
So here we go for the lower bound of the range which is minus self that HP is that grad clip and then

10:01.540 --> 10:02.440
the upper bound.

10:02.440 --> 10:12.430
The range which is the opposite value the plus one that is self that HP is that grad clip.

10:12.530 --> 10:12.890
All right.

10:12.890 --> 10:13.280
Perfect.

10:13.280 --> 10:14.360
Now be careful.

10:14.360 --> 10:20.940
This is the first element of the pair of great and inviolable right.

10:20.960 --> 10:21.920
This element.

10:21.920 --> 10:25.370
So now we have to add the second one which is a.

10:25.460 --> 10:32.810
And so I'm adding coming here and I'm adding the variable which is the other looping variable which

10:32.810 --> 10:38.530
will loop over the second elements of the list NGV is right.

10:38.570 --> 10:42.670
And now speaking of the loop that's exactly what we need to add here.

10:42.680 --> 10:46.090
You know that's how a loop in a pair of square brackets work.

10:46.100 --> 10:50.070
You know when you want to build the list through a loop directly.

10:50.150 --> 10:54.920
And so here we have to add 4 and then are two looping variables.

10:54.920 --> 10:57.770
First one is grad you know this one.

10:57.770 --> 10:59.790
And the second one is of course.

10:59.870 --> 11:00.650
Var.

11:00.650 --> 11:04.100
So this is the first element of the pair in the G.P.S. list.

11:04.100 --> 11:06.840
And this is the second element of the bear in G.P.S. list.

11:07.190 --> 11:11.040
So for Grat for in of course this list.

11:11.080 --> 11:11.760
Yes.

11:11.840 --> 11:12.200
All right.

11:12.200 --> 11:13.730
And then here we go.

11:13.730 --> 11:16.050
We've just kept our credits.

11:16.760 --> 11:20.320
And now now we're ready to apply our gradients.

11:20.330 --> 11:25.000
And we're gonna do that exactly the same way as we did for our VAB.

11:25.010 --> 11:31.880
So I'm going back to our V.A. again and I'm just going to take this line of code.

11:32.060 --> 11:32.980
Copy it.

11:33.200 --> 11:38.250
And just to be efficient or because I'm lazy I'm going to face that again.

11:38.300 --> 11:45.650
And now we're just going to replace the grads here by of course are kept gradients.

11:45.750 --> 11:49.330
Rights are here replacing capped genius.

11:49.490 --> 11:49.910
All right.

11:49.910 --> 11:52.050
And then we have the same global step.

11:52.160 --> 11:54.440
That's what we've introduced here.

11:54.470 --> 11:59.630
Remember if HP yes that is training meaning equals equals one then self.

11:59.630 --> 12:03.440
Global step was created as a tensor flow variable.

12:03.470 --> 12:03.770
All right.

12:03.770 --> 12:04.670
So that's all good.

12:04.670 --> 12:07.690
And then the name is still going to be train step.

12:08.150 --> 12:09.010
So all good.

12:09.050 --> 12:10.880
We have nothing to change here.

12:11.120 --> 12:13.760
And now finally final line of code.

12:13.760 --> 12:19.670
Final line of code of this implementation which once again is going to be exactly the same as what we

12:19.670 --> 12:21.080
did for the V E.

12:21.170 --> 12:27.080
This is going to be this one which will initialize to global variables of the training.

12:27.140 --> 12:30.650
So I'm going to copy this line once again.

12:30.860 --> 12:36.500
Then pasted right here so be careful we have to get out of the if condition.

12:36.640 --> 12:38.460
And congratulations.

12:38.470 --> 12:46.930
You have implemented the engine Arnon so you can save and move on to the next step which is let's see.

12:46.930 --> 12:48.760
Well step 9.

12:48.880 --> 12:50.200
Reinforcement learning.

12:50.200 --> 12:56.440
So this is now that you are going to understand this part with you know the control of playing the action

12:56.770 --> 13:01.180
inside the environment then getting the reward and then reaching the next stage.

13:01.210 --> 13:04.900
So we will see that in detail in this next step.

13:04.960 --> 13:10.570
Step night reinforcement learning you will have an intuition lecture explaining how reinforcement learning

13:10.570 --> 13:16.720
works in general and we'll also implement functions that you can clearly understand what's happening

13:16.780 --> 13:19.240
in this part of the full world model.

13:19.240 --> 13:22.330
So take a break after this big step.

13:22.360 --> 13:25.100
Aides implementing the engine aren't and congrats again.

13:25.150 --> 13:30.420
That was actually a very advanced code section so make sure you take a good break and I'll see you in

13:30.430 --> 13:32.770
step 9 for reinforcement learning.

13:32.770 --> 13:34.300
Until then enjoy.
