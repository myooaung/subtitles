WEBVTT

00:00.180 --> 00:00.510
All right.

00:00.510 --> 00:01.260
Here we go.

00:01.280 --> 00:04.260
Let's implement the training operations.

00:04.260 --> 00:09.780
So we're going to make a series of functions before we get to that final loss function because indeed

00:09.780 --> 00:14.240
the formula is quite complex so we're going to split that into several steps.

00:14.310 --> 00:19.350
And the first thing we're going to do here is just get a special number which results from a complex

00:19.470 --> 00:20.340
operation.

00:20.340 --> 00:24.620
But that special number is something around 0 point ninety two point ninety four.

00:24.630 --> 00:26.290
We'll check that right away.

00:26.430 --> 00:30.990
And this special number is just the luck of the square root of two times by.

00:31.020 --> 00:40.650
So we're going to put that number into a variable which we're gonna call luck s q r t two pi which as

00:40.650 --> 00:43.920
we've just said is going to be equal to the following formula.

00:44.130 --> 00:52.620
The lug which I'm getting things to known pi num pi lug that's a function by num PI of the square root

00:52.770 --> 00:58.140
which again I'm going to get thanks to none pi and P that R T.

00:58.230 --> 01:07.110
Another function by name by of as we said two point zero times pi which again we're gonna get through

01:07.220 --> 01:11.130
num by with M P that P I.

01:11.130 --> 01:11.850
Here we go.

01:12.060 --> 01:17.100
And that's just the number I would like to get which is of course part of the last function that we're

01:17.100 --> 01:24.840
going to use to train our DNA on it and let's just check that if my memory is all correct if we execute

01:24.840 --> 01:25.640
this.

01:25.860 --> 01:27.480
Here we go we have our luck.

01:27.510 --> 01:28.890
Square root 2 Pi.

01:28.980 --> 01:32.730
And now let's just select this that execute again.

01:32.790 --> 01:33.480
And here we go.

01:33.480 --> 01:34.090
Yes.

01:34.170 --> 01:35.480
0 point ninety two.

01:35.480 --> 01:40.450
Right so that's a special number which is part of the last function.

01:40.500 --> 01:44.410
And speaking of the last function well we're not ready to get it yet.

01:44.430 --> 01:50.880
I would like to do an intermediary step which is to compute another formula which will include that

01:51.000 --> 01:58.050
look square 2 pi in which we're going to include inside a new function you know like this one here which

01:58.050 --> 02:02.080
we're gonna call def T F lug normal.

02:02.130 --> 02:02.500
All right.

02:02.520 --> 02:08.370
So we're making it because it is not part of the Tentacle tools right now for the tenth floor version

02:08.370 --> 02:09.420
we're using.

02:09.420 --> 02:13.500
And therefore we have to make this formula from scratch ourselves.

02:13.500 --> 02:16.570
And so this function is going to take three arguments.

02:16.680 --> 02:23.130
The first one is the target which will go here just as an argument y but then which will be later on

02:23.460 --> 02:26.090
the target in the MDA and training.

02:26.100 --> 02:31.670
Then the second one is one of the parameters of the NBN which is going to be the mean.

02:31.680 --> 02:37.200
So we're just going to input mean here as the name of the argument and then the third one is going to

02:37.200 --> 02:42.750
be another of the MDL coefficients or parameters which will be the log FTD.

02:42.900 --> 02:46.960
And actually we will keep that name for the argument log FTD.

02:47.120 --> 02:47.560
All right.

02:47.760 --> 02:54.630
And now inside the function we're just simply going to include that major part of that last function

02:54.660 --> 03:00.240
but which will still not be the final does function but it will be indeed the big part of the computation

03:00.450 --> 03:02.700
which is like a log normal distribution.

03:02.700 --> 03:13.260
And so that big operation well let's return it directly it is going to be minus open five times a fraction

03:13.530 --> 03:23.370
which will be first at the numerator why the target minus the mean divided by the exponential which

03:23.390 --> 03:28.500
will get from tensor flows to f that exp exponential curve.

03:28.590 --> 03:32.330
Well that empty and parameter the lug SSD.

03:33.290 --> 03:39.210
All right then we're gonna take the square of all this and to do this we just need to add here a double

03:39.210 --> 03:41.090
star then too.

03:41.280 --> 03:48.990
And then finally we have to subtract the low FTD and subtract again well that special number which we've

03:49.110 --> 03:52.880
introduced before the log square root of 2 Pi.

03:52.890 --> 03:53.160
All right.

03:53.160 --> 04:01.140
So pasting that here and here we go that's the major part of the less computation which we're going

04:01.140 --> 04:06.330
to include now and a final function which will be that loss function itself.

04:06.330 --> 04:14.850
And so I'm introducing a new function here def and we're going to call that less get less function get

04:14.850 --> 04:22.080
less func which will take for argument's first one will be of course one Indian parameter which we haven't

04:22.080 --> 04:29.620
used yet in this big operation which is of course the outlook makes mixing coefficients.

04:29.850 --> 04:33.310
And so that's our first argument here lug mix.

04:33.360 --> 04:34.850
So this is just the name of the argument.

04:34.860 --> 04:39.650
But then of course will input the real variable for that outlook mix.

04:39.990 --> 04:46.050
Then the next two parameters are going to be the other two empty MTM parameters which are the mean and

04:46.050 --> 04:47.950
the low as 2D.

04:48.090 --> 04:54.600
And then finally well why here in the T F look normal function that we've made ourselves which is the

04:54.600 --> 04:55.240
target.

04:55.290 --> 04:57.000
So we're simply going to call it.

04:57.000 --> 05:01.530
Why now and then we'll introduce a new viable or that target.

05:01.690 --> 05:06.130
And so here we go for the final round of that loss function.

05:06.130 --> 05:12.220
So I can already start to tell you what is going to be the final operation of that last function.

05:12.220 --> 05:18.850
Well it is going to be minus the mean which we're going to compute thanks to math reduce mean which

05:18.850 --> 05:27.790
I prepared for you again math reduced mean minus the mean of the lug of the sum of the exponential of

05:28.270 --> 05:32.230
the lug mix variable plus that operation here.

05:32.290 --> 05:32.720
Right.

05:32.750 --> 05:34.840
The elements resulting from the operations here.

05:34.870 --> 05:37.850
So no worries we will write that in several steps.

05:37.900 --> 05:42.880
So the first step is actually to get this outcome here resulting from this formula and we're going to

05:42.880 --> 05:47.670
put that outcome in a local variable which I'm going to call Ft.

05:47.680 --> 05:48.690
So V equals.

05:49.060 --> 05:56.140
Then we're gonna call that function T F like normal and actually also its arguments because we used

05:56.290 --> 06:03.970
the same names for the arguments that get lost function so copying this and pasting that here.

06:04.000 --> 06:10.480
Then remember I told you that the elements of which we're taking the lug of this summit exponential

06:10.870 --> 06:16.400
are actually that plus the lug mix right.

06:16.510 --> 06:21.760
These are going to be the elements inside that lug some exp.

06:21.850 --> 06:22.170
Right.

06:22.180 --> 06:27.820
And therefore now we're ready to take again that T F reduced lug some X function.

06:27.820 --> 06:34.990
So taking that again and just below Well we're still going to call that V because we're just updating

06:35.150 --> 06:37.750
this local variable and so here we go.

06:37.810 --> 06:46.990
I'm pasting again that T F reduce look some X taking as arguments the same one first the elements of

06:46.990 --> 06:52.360
which we're computing the log of the sum of the exponential which we called V you know exactly what

06:52.360 --> 06:53.380
we had here.

06:53.380 --> 07:00.640
Those are the elements inside this big lug of the sum of the exponential operation then remember we

07:00.640 --> 07:04.910
have to specify the axis and as usual we are making this operation long.

07:04.920 --> 07:12.940
The observations you know the rows of the horizontal axis which has an X one and then remember again

07:12.940 --> 07:16.460
since we're using the alias on a deprecated version.

07:16.540 --> 07:24.550
Well we have to take again that argument and not keep its default value of none but true.

07:24.580 --> 07:29.440
So copying this again and based that here.

07:29.470 --> 07:30.080
All right.

07:30.160 --> 07:36.820
So almost there we have one final step before we get the final outcome of the loss function which is

07:37.030 --> 07:44.500
remember to take the mean of all this or more specifically minus the mean of all this and therefore

07:44.830 --> 07:50.430
since we're ready to return the final outcome well we can end this function with a return.

07:50.440 --> 07:58.000
And then as we said it's going to be minus the mean of what we've just computed which we're gonna get.

07:58.030 --> 08:05.320
Thanks to this math reduce main function or more precisely the areas T F that reduce me.

08:05.500 --> 08:10.530
Right so I'm taking this and pasting the final outcome here.

08:10.660 --> 08:13.300
The mean of the.

08:13.320 --> 08:20.410
And congratulations you made this pretty complex last function which indeed we had to compute in several

08:20.530 --> 08:21.820
intermediary steps.

08:21.820 --> 08:22.180
Right.

08:22.300 --> 08:26.870
It's easier to do it this way it's more clear and not too overwhelming.

08:26.950 --> 08:27.580
All right.

08:27.580 --> 08:33.760
And now so now that we have our function ready let's get the final less and to do this we're gonna have

08:33.760 --> 08:37.720
to call that get lost function with the right arguments.

08:37.720 --> 08:42.610
So we already have three arguments we have to lug mix which we've prepared here.

08:42.640 --> 08:46.290
Outlook mix as a result of the Get em the inchoate function.

08:46.300 --> 08:53.080
And we also have the mean again still as another output resulting from the get and the encoder function

08:53.290 --> 09:00.610
out mean we also have the lug LCD again the third element the third and parameter resulting from the

09:00.970 --> 09:03.430
split of the output.

09:03.430 --> 09:05.340
But we don't have yet.

09:05.470 --> 09:05.950
Why.

09:05.950 --> 09:08.410
Which is I remind the target.

09:08.500 --> 09:10.630
And so that's exactly what we're gonna get right now.

09:10.630 --> 09:18.910
We're gonna get that target which we're gonna call inside a new viable flat target data.

09:18.920 --> 09:19.700
All right.

09:19.710 --> 09:20.640
And together.

09:20.650 --> 09:23.270
Target Well we're gonna have to do another reshape.

09:23.370 --> 09:26.600
Remember that we already prepared something for the target.

09:26.670 --> 09:33.100
It was I remind this output X which we have prepared thanks to a sensible place holder.

09:33.100 --> 09:40.290
And so we're gonna take that place holder again output X and even we're going to take self that output

09:40.320 --> 09:48.510
X because this flat target data meaning this Y in the get lost function is going to be that output X

09:48.540 --> 09:54.510
but just a little bit reshaped and the way it's going to be reshaped is by doing the following.

09:54.510 --> 10:01.440
First we're going to call of course the reshape function by tens of floaty f that reshape inside which

10:01.440 --> 10:03.580
we're gonna do something different this time.

10:03.630 --> 10:09.660
The first argument is going to be of course what we want to reshape which is this output x so that output

10:09.750 --> 10:16.170
X that place holder we prepared for the target and then exactly as what we did here we're going to reshape

10:16.170 --> 10:18.220
that with the following format.

10:18.450 --> 10:23.850
With that minus 1 which will flatten some dimensions but here it's not going to be of course HP has

10:23.850 --> 10:28.890
that on insights going to be a full flattening because we're just going to input one and I'm going to

10:28.890 --> 10:31.950
show you the before and after the reshape.

10:32.460 --> 10:38.610
So I'm taking this basically here inside the reshape function as the second argument and here as we

10:38.610 --> 10:45.050
said it's going to be a simple one for you know how we group the elements resulting from that reshape.

10:45.270 --> 10:48.780
And so now let's see what the new target becomes.

10:48.780 --> 10:54.930
So first we're going to execute this but because of the self I can execute that here to see the shape

10:55.170 --> 10:56.640
because it is inside of function.

10:56.640 --> 11:03.420
So that's why you know I prepared this aren't and values Python implementation which contains all the

11:03.780 --> 11:06.270
values of the engine on it.

11:06.390 --> 11:07.500
And so here we go.

11:07.530 --> 11:12.090
Now I can select this just output X and execute.

11:12.240 --> 11:12.900
And here we go.

11:12.930 --> 11:13.980
We have the shape.

11:13.980 --> 11:16.560
So it is a 3D tensor.

11:16.650 --> 11:20.580
So that's the original shape of the target we prepared before.

11:20.580 --> 11:28.140
It is a 3D tensor of remember 100 samples in this first dimension of the batch 1000 frames which are

11:28.140 --> 11:34.860
the successive frames to learn the time related dependencies happening in the game and more specifically

11:34.860 --> 11:41.170
these are the reconstructed frames resulting from the V and then 32 for.

11:41.250 --> 11:44.730
Remember the output seq width.

11:44.940 --> 11:49.710
And so that's the original shape and now what happens after the.

11:49.710 --> 11:55.800
So we have to scroll down a bit and find that flat target data.

11:55.800 --> 11:57.000
Here it is.

11:57.000 --> 12:00.090
So I'm going to first execute this.

12:00.090 --> 12:00.780
Here we go.

12:01.350 --> 12:05.030
And now I'm going to execute only this flat target data.

12:05.040 --> 12:12.270
Now let's see the new shape it is going to be as we've just said you know a full flattening of all the

12:12.270 --> 12:19.200
elements one by one with therefore three million and two hundred thousand rows that result indeed from

12:19.410 --> 12:27.990
the multiplication of 32 times 1000 times 100 because it was indeed a full flattening along one dimension

12:28.020 --> 12:29.440
one single dimension.

12:29.640 --> 12:36.030
And of course one column because indeed all the elements in the 32 elements of the output in all the

12:36.030 --> 12:43.080
1000 frames were constructed frames of the sequence length of all the 100 samples of the batch where

12:43.230 --> 12:50.130
grouped one by one and flattened into this one single one dimensional vector which can be seen as a

12:50.130 --> 12:51.550
vertical vector.

12:51.560 --> 12:56.940
All right so once again you clearly see what we're doing here with all the reshapes and now now that

12:56.950 --> 13:04.380
the reshape is done we can move on to the next step here which is to finally call that get lost function

13:04.410 --> 13:10.650
because indeed now we have all the arguments ready to be input inside this get lost function that is

13:11.040 --> 13:12.990
going to give us the final us.

13:13.440 --> 13:13.940
All right so.

13:14.040 --> 13:14.730
Well here we go.

13:14.750 --> 13:20.100
Let's copy this and we'll just replace the right names of the arguments.

13:20.160 --> 13:24.470
So since this get lost function returns the final loss.

13:24.570 --> 13:29.580
Well we have to introduce a new void bill which will contain that final death and which therefore will

13:29.580 --> 13:37.860
call loss func and that variable is going to be the result of the get loss function which is going to

13:37.860 --> 13:45.290
take as arguments of course the parameters of the empty end plus that target which would be here.

13:45.330 --> 13:50.070
So let's put the right names of these variables which we've already prepared.

13:50.130 --> 13:55.350
The first one log makes we call that actually out log mix.

13:55.530 --> 14:03.780
Then second one the mean and the Indian parameters we called that out mean and then again the log as

14:03.780 --> 14:11.480
the we call that out log city and why the target we just call that flat target data.

14:11.490 --> 14:15.720
So I'm copying this basing that year and here we go.

14:15.720 --> 14:19.110
Of course let's remove the Colin perfect.

14:19.630 --> 14:24.170
So that gives us the last function but now to get exactly the less.

14:24.280 --> 14:26.160
Which is also called cost.

14:26.170 --> 14:30.780
Well we have to more precisely get the mean of that outcome.

14:30.790 --> 14:34.810
You know that outcome returned by the get lost func function.

14:35.140 --> 14:40.960
And so to finish this well to finish the big first part of the training operations because then we'll

14:40.960 --> 14:43.210
have to introduce our optimizer.

14:43.210 --> 14:49.330
Well we're going to introduce a new variable which will be exactly the final less error or also called

14:49.870 --> 14:57.960
cost and which as we said is going to be the mean which again we get by sense of flow right and flow

14:58.000 --> 15:08.680
that reduce mean of that loss func variable which is the result of that get lost func function.

15:08.680 --> 15:09.450
All right.

15:09.490 --> 15:15.010
And now we can move on to the final part of the training operations with you know the introduction of

15:15.010 --> 15:20.410
the learning rate the optimizer and then of course we're going to apply the gradients from the optimizer

15:20.410 --> 15:23.040
exactly as what we did for the V.

15:23.260 --> 15:28.150
But of course now let's take a little break and let's do that final part of the training operations

15:28.240 --> 15:29.440
in the next soil.

15:29.490 --> 15:31.260
And so until then enjoy a.
