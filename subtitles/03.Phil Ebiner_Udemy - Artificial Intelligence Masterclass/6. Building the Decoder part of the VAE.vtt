WEBVTT

00:00.380 --> 00:06.320
Highs and welcome to this very exciting tutorial Y very exciting.

00:06.330 --> 00:13.220
That's because we're about to get the final outcome of the dream you know right now by building this

00:13.350 --> 00:15.070
variational our encoder.

00:15.180 --> 00:22.050
We're basically making an artificial intelligence dream and right now we're about to implement the decoded

00:22.050 --> 00:30.900
part of the Wii which will decode the latent vectors the obtained skeptically to reconstruct the input

00:31.080 --> 00:37.750
images that are same age I was able to see things through these four convolutions here.

00:37.770 --> 00:39.440
So that's a very big deal.

00:39.440 --> 00:44.730
This is actually the first time I am implementing an IDE that has the ability to dream.

00:44.820 --> 00:48.430
And actually this is the first time for anybody in 2018.

00:48.600 --> 00:51.150
So let's celebrate this special moment.

00:51.150 --> 00:56.080
We're about to make an AI dream with the decoded part of the Wii.

00:56.130 --> 00:57.160
So here we go.

00:57.210 --> 01:05.700
In order to make that happen we have to implement everything from here to the final reconstructed images

01:06.150 --> 01:07.890
which has the same damage.

01:08.280 --> 01:12.140
And so the first thing we have to do is another fully connected layer.

01:12.300 --> 01:18.510
All right so let's do this let's go back to Python and let's create this let's add it in the full architecture

01:18.510 --> 01:24.670
of the Wii and since this is going to be basically a new hit and there we're going to call it age again

01:25.170 --> 01:31.720
and age is going to be a fully connected layer which we can get again by contents of flow then by cutting

01:31.740 --> 01:33.910
the layers Mudgal Blyton's flow.

01:33.930 --> 01:38.280
And from that module we call again the dense function.

01:38.280 --> 01:43.470
And so this Den's function exactly last year is going to take three arguments that we need to answer

01:43.770 --> 01:51.140
the first one is the input vector to which this new dense layer is going to be fully connected.

01:51.330 --> 01:58.500
And of course as we can clearly see in the architecture it's going to be late and vector we caught in

01:58.500 --> 01:59.630
the previous tutorial.

01:59.880 --> 02:06.430
So let's enter this and put self dot zed as it is a variable of our object.

02:06.720 --> 02:09.960
And then remember the second argument is the number of units.

02:10.130 --> 02:17.460
And then there and as we can clearly see here it's going to be 1024 units.

02:17.460 --> 02:22.950
Don't worry about the one times one times here this is because we need to reshape it but we'll do it

02:23.040 --> 02:24.300
right after that.

02:24.300 --> 02:33.170
First of all let's get these 1024 units in the second argument one thousand and twenty four are right.

02:33.180 --> 02:38.440
And then as a final argument we can give a name to that new layer.

02:38.460 --> 02:43.750
This new fully connected layer and since it is indeed a fully connected layer we're going to keep the

02:43.830 --> 02:45.000
F.C. here.

02:45.000 --> 02:52.320
However since we're now dealing with the decoder I'm going to replace ink by Dec and then I'm going

02:52.320 --> 02:58.730
to remove dimmu because now we're simply dealing with a simple fully connected layer.

02:58.740 --> 03:07.440
All right so basically that smashes the first step of building the decoder that is this step here getting

03:07.440 --> 03:09.580
the new fully connected layer.

03:09.660 --> 03:18.150
Then we have to do this reshape here to get indeed a one dimensional vector of one by one by 1024 dimensions

03:18.300 --> 03:20.380
with 1024 elements.

03:20.400 --> 03:27.810
So the way we're going to do this is by adding age again and then as we did before we're going to take

03:27.810 --> 03:31.410
the reshape function by tens of flow.

03:31.410 --> 03:32.280
Here we go.

03:32.470 --> 03:34.170
Then first we need input.

03:34.170 --> 03:40.860
What we want to reshape which is age which right now is just a fully connected layer and nodes get this

03:40.980 --> 03:47.280
one dimensional vector in a one dimensional batch and composed of 1024 elements.

03:47.280 --> 03:52.380
Well we're going to do kind of the same thing as with with you here but this time there's going to be

03:52.640 --> 03:59.850
two more dominations which are corresponding Of course to these dimensions here.

03:59.850 --> 04:07.410
One by one so these new dimensions are just to be 100 percent aligned with the format of the future

04:07.470 --> 04:10.880
reconstructed images because no order to reconstruct them.

04:11.010 --> 04:15.420
We need to have the right dimensions for the future inverted convolutions.

04:15.450 --> 04:21.270
So we're just getting this format by creating these additional dimensions so that we can apply them

04:21.450 --> 04:24.320
inverted or transposed convolutions.

04:24.330 --> 04:29.760
All right so let's add these new diamond in year one by one.

04:29.760 --> 04:30.760
Here we go.

04:30.960 --> 04:38.520
And now we can just give the same or you know replace that by 1024 because indeed that's the number

04:38.520 --> 04:43.210
of elements we want to get in that one dimensional outwardly.

04:43.650 --> 04:51.720
All right so now let's move on to the next step which is finally the start of the deconvolution.

04:51.750 --> 04:55.730
Or more correctly the inverted convolutions.

04:55.920 --> 04:59.600
So we're going to still update age the way we did here.

04:59.860 --> 05:06.580
And we are going to get a sense of of course now from tons of flow we're going to get the letters Mudgal

05:06.760 --> 05:10.450
because it isn't this layer's module that will find what we need.

05:10.450 --> 05:20.680
That is the current 2D transpose function which will do exactly this inverted convolution taking as

05:20.690 --> 05:27.370
input one dimensional vector in a one dimensional batch and then recreating some images which we're

05:27.370 --> 05:32.310
going to do with several inverted convolutions to reconstruct the final images.

05:32.320 --> 05:40.480
So I prepared for you now the comes to the transpose function by tensor of flow but we can see that

05:40.660 --> 05:42.850
the arguments are kind of the same.

05:42.850 --> 05:48.780
They have the same names same inputs filters girl sized dried bedding.

05:48.940 --> 05:55.840
So they are the same and therefore it will be a piece of cake to implement these inverted convolutions.

05:55.840 --> 05:57.670
All right so here we go with the first one.

05:57.670 --> 06:04.030
The first inverted convolution is let's go back to our world models article.

06:04.270 --> 06:13.200
The first word convolution is going to be an inverted convolution with 128 filters of size five by five.

06:13.210 --> 06:19.320
So let's put that exactly the same way we did for the basic convolutions first.

06:19.360 --> 06:24.360
Let's not forget to of course input the input tensor which is of course H.

06:24.490 --> 06:33.880
Right which so far is just a one dimensional vector and then that's where the 128 filters come into

06:33.880 --> 06:37.480
play with a kernel size of five by five.

06:37.480 --> 06:47.020
And then again we're going to have tried to then as we can see if we look at it again a rectifier activation

06:47.020 --> 06:47.650
function.

06:47.920 --> 06:56.810
So we can just actually copy this to be more efficient activation because of that and end up reglue

06:57.490 --> 07:04.210
and then eventually we can give a name to that new first inverted convolutional layer.

07:04.270 --> 07:05.290
And here we go.

07:05.330 --> 07:06.870
We're going to go that this time.

07:07.090 --> 07:13.920
Well first not Anke but tech and then not count but D-Conn. one right.

07:13.930 --> 07:17.410
The first deconvolution all there that's got this way.

07:17.800 --> 07:25.030
All right so now is going to be the real piece of cake because to create the other inversed convolutions

07:25.540 --> 07:31.400
we'll just copy this and we will paste it just below just like that.

07:31.430 --> 07:36.780
And now of course we don't have to replace the input because it is keeping data like a domino.

07:36.940 --> 07:43.450
But we have to replace the number of filters and the name here because indeed we're going to keep the

07:43.450 --> 07:47.530
Belu activation functions and the same kernel size.

07:47.800 --> 07:55.510
So let's see in the previous inverted convolution we had 128 filters and now we're going to have 64

07:55.510 --> 07:56.090
of them.

07:56.290 --> 07:58.640
So let's put that right here.

07:58.650 --> 08:07.590
And the second inverted convolutional layer 64 filters and then deconvolution to our right.

08:07.600 --> 08:09.970
And then let's paste that again.

08:10.150 --> 08:16.350
We can guess the new number of filters which is going to be of course 32 right 32 right here.

08:16.510 --> 08:17.590
But be careful.

08:17.590 --> 08:20.500
A new kernel size of six by six.

08:20.500 --> 08:21.790
I didn't catch that before.

08:21.790 --> 08:30.160
So let this time replace 128 by 32 and 5 by 6.

08:30.160 --> 08:30.740
All right.

08:30.790 --> 08:35.600
Pretty easy right then D-Conn. one we replace it by decomp 3.

08:35.860 --> 08:37.960
And now I'm going to copy this.

08:38.030 --> 08:41.400
I want to lower my options to make any mistake.

08:41.500 --> 08:42.270
Paste.

08:42.310 --> 08:49.360
And what is going to be the final number of filters in the last inverted convolution right.

08:49.360 --> 08:50.790
This is the last one here.

08:50.860 --> 08:54.640
Where are you getting so close to the reconstructed image.

08:54.640 --> 09:02.140
We have to make first of final inverse convolution and this one has three only three filters of size

09:02.230 --> 09:03.270
six by six.

09:03.400 --> 09:05.830
So let's implement that right away.

09:06.040 --> 09:09.630
Three filters and a current size of six.

09:09.670 --> 09:10.500
Perfect.

09:10.660 --> 09:17.150
Then final thing we have to do is to replace the name they're going to come three by take on this Gorgie

09:17.170 --> 09:20.380
come for however.

09:20.560 --> 09:27.100
Now since this is actually the last convolution it is not a hidden layer anymore.

09:27.250 --> 09:36.100
It is the final layer and also wait for it the final reconstructed images and that's why we're not going

09:36.100 --> 09:43.780
to call that age but self that Y which is a new variable of our object that we're introducing and which

09:43.780 --> 09:51.400
is of course the final output of this whole convolutional variational our encoder but now be careful

09:51.460 --> 09:59.560
as most of the final layer is the activation function used is not rectify activation function but indeed

09:59.920 --> 10:06.390
a sigmoid activation function you will see that in most of the art visual neural network architecture

10:06.390 --> 10:11.220
is whether it is a fully connected neural network or a convolutional neural network or even a record

10:11.220 --> 10:12.320
or a neural network.

10:12.390 --> 10:18.260
The final activation function used for the output layer is a sigmoid activation function.

10:18.270 --> 10:27.210
So we're going to replace that reglue activation function by the sigmoid one which is also available

10:27.390 --> 10:30.890
from the neural network and module by tence afloat.

10:31.200 --> 10:33.810
And now everything is all good.

10:33.870 --> 10:39.980
Congratulations you have just given an artificial intelligence the ability to dream.
