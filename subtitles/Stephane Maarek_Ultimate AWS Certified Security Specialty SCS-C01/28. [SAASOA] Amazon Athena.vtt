WEBVTT
1
1

00:00:02.340  -->  00:00:04.620
<v Instructor>So now let's talk about Amazon Athena.</v>
2

2

00:00:04.620  -->  00:00:07.110
So Athena is a serverless query service
3

3

00:00:07.110  -->  00:00:10.980
to help you analyze the data stored in Amazon S3 buckets.
4

4

00:00:10.980  -->  00:00:12.720
And to analyze this data,
5

5

00:00:12.720  -->  00:00:15.180
you're going to use the standard SQL language
6

6

00:00:15.180  -->  00:00:16.590
to query the files.
7

7

00:00:16.590  -->  00:00:19.320
Behind the scenes, Athena is built on the Presto engine
8

8

00:00:19.320  -->  00:00:21.030
which uses the SQL language.
9

9

00:00:21.030  -->  00:00:23.700
So the idea is that users are going to load data
10

10

00:00:23.700  -->  00:00:24.600
into your S3 bucket
11

11

00:00:24.600  -->  00:00:27.330
or you are going to load data into your S3 bucket
12

12

00:00:27.330  -->  00:00:29.070
and then you would use the Athena service
13

13

00:00:29.070  -->  00:00:32.640
to query and analyze this data in Amazon S3
14

14

00:00:32.640  -->  00:00:33.660
without moving it.
15

15

00:00:33.660  -->  00:00:36.990
So Athena is serverless and it analyzes directly your data
16

16

00:00:36.990  -->  00:00:38.790
living in your S3 buckets.
17

17

00:00:38.790  -->  00:00:41.610
So it supports different formats such as CSV,
18

18

00:00:41.610  -->  00:00:45.660
JSON, ORC, Avro, and Parquet, and possibly others.
19

19

00:00:45.660  -->  00:00:47.520
And the pricing is very simple.
20

20

00:00:47.520  -->  00:00:49.650
You're just going to pay a fixed amount
21

21

00:00:49.650  -->  00:00:52.050
per terabyte of data scanned.
22

22

00:00:52.050  -->  00:00:54.060
You don't need to provision any database again
23

23

00:00:54.060  -->  00:00:56.580
because the whole service is serverless.
24

24

00:00:56.580  -->  00:00:59.640
So Athena is commonly used with another tool
25

25

00:00:59.640  -->  00:01:03.630
called Amazon Quicksight to create reports and dashboards.
26

26

00:01:03.630  -->  00:01:06.150
So we'll see Quicksight later in depth,
27

27

00:01:06.150  -->  00:01:08.670
but Amazon Quicksight connects to Athena
28

28

00:01:08.670  -->  00:01:11.100
which connects to your S3 buckets.
29

29

00:01:11.100  -->  00:01:13.440
Now, the use cases for Amazon Athena
30

30

00:01:13.440  -->  00:01:16.260
are to do ad hoc queries, business intelligence,
31

31

00:01:16.260  -->  00:01:19.830
analytics, reporting, and to analyze and query
32

32

00:01:19.830  -->  00:01:24.540
any kind of logs that originates from your AWS services.
33

33

00:01:24.540  -->  00:01:26.370
So it could be your VPC Flow Logs,
34

34

00:01:26.370  -->  00:01:30.120
your load balancer logs, your CloudTrail trails, and so on.
35

35

00:01:30.120  -->  00:01:33.480
So as an exam tip, anytime you need to analyze data
36

36

00:01:33.480  -->  00:01:37.230
in Amazon S3 using a serverless SQL engine,
37

37

00:01:37.230  -->  00:01:38.823
you can think about Athena.
38

38

00:01:39.690  -->  00:01:41.940
Now, I've talked about performance improvements
39

39

00:01:41.940  -->  00:01:44.790
and you can actually improve Athena performance
40

40

00:01:44.790  -->  00:01:47.820
and the exam will test you on this as well.
41

41

00:01:47.820  -->  00:01:48.870
So first of all,
42

42

00:01:48.870  -->  00:01:53.520
because you pay for the amount of data scanned per terabyte,
43

43

00:01:53.520  -->  00:01:56.040
you need to use a type of data
44

44

00:01:56.040  -->  00:01:58.290
where you're going to scan less data.
45

45

00:01:58.290  -->  00:02:01.290
And for this, you can use a columnar data type
46

46

00:02:01.290  -->  00:02:04.770
for cost savings because you only scan the columns you need.
47

47

00:02:04.770  -->  00:02:09.090
So therefore, the recommended formats for Amazon Athena
48

48

00:02:09.090  -->  00:02:12.243
are going to be Apache Parquet and ORC.
49

49

00:02:13.080  -->  00:02:15.780
And it's going to give you a huge performance improvement.
50

50

00:02:15.780  -->  00:02:20.370
And to get your files into the Apache Parquet or ORC format,
51

51

00:02:20.370  -->  00:02:21.630
you must use a service
52

52

00:02:21.630  -->  00:02:23.310
that we'll see as well in this section.
53

53

00:02:23.310  -->  00:02:24.660
For example, Glue.
54

54

00:02:24.660  -->  00:02:29.130
Glue can be very helpful to convert your data as an ETL job
55

55

00:02:29.130  -->  00:02:32.640
between, for example, CSV and Parquet.
56

56

00:02:32.640  -->  00:02:35.850
Now also because we want to scan less data,
57

57

00:02:35.850  -->  00:02:38.160
we need to compress data for smaller retrieval.
58

58

00:02:38.160  -->  00:02:41.190
So there are different compression mechanisms you can use
59

59

00:02:41.190  -->  00:02:43.680
that I've listed right here.
60

60

00:02:43.680  -->  00:02:46.890
Next, if you know you're going to query all the time
61

61

00:02:46.890  -->  00:02:50.940
on some specific columns, you can partition your datasets.
62

62

00:02:50.940  -->  00:02:54.180
And partitioning datasets means that in your S3 bucket,
63

63

00:02:54.180  -->  00:02:57.150
you're going to have the full path with slashes
64

64

00:02:57.150  -->  00:03:00.240
and each slash will be a different column name
65

65

00:03:00.240  -->  00:03:01.920
with a specific value.
66

66

00:03:01.920  -->  00:03:03.060
And so you're organizing,
67

67

00:03:03.060  -->  00:03:05.370
you're partitioning your data in Amazon S3
68

68

00:03:05.370  -->  00:03:06.510
so that when you query it,
69

69

00:03:06.510  -->  00:03:08.160
you can know exactly in which folder
70

70

00:03:08.160  -->  00:03:11.400
at which path in Amazon S3 you need to scan for data.
71

71

00:03:11.400  -->  00:03:13.740
So here's an example of data partitions.
72

72

00:03:13.740  -->  00:03:16.770
So we have flight data in Parquet format
73

73

00:03:16.770  -->  00:03:20.070
and then we do /year-=1991.
74

74

00:03:20.070  -->  00:03:21.750
So we partition by year
75

75

00:03:21.750  -->  00:03:24.000
and we'll have one folder for each year.
76

76

00:03:24.000  -->  00:03:26.370
Then within each year, we'll have month,
77

77

00:03:26.370  -->  00:03:27.480
so month equals one.
78

78

00:03:27.480  -->  00:03:30.210
And within each month, we'll have days, day equals one.
79

79

00:03:30.210  -->  00:03:33.090
And so when I do a query on Athena
80

80

00:03:33.090  -->  00:03:35.250
and I filter for a specific year,
81

81

00:03:35.250  -->  00:03:37.740
a specific month, and a specific day,
82

82

00:03:37.740  -->  00:03:41.340
then we'll know exactly to which folder in Amazon S3
83

83

00:03:41.340  -->  00:03:42.570
to get the data from.
84

84

00:03:42.570  -->  00:03:45.630
And therefore, we'll only recover a subset of the data.
85

85

00:03:45.630  -->  00:03:48.660
Therefore, we'll have really, really good partitioning.
86

86

00:03:48.660  -->  00:03:51.570
Finally, the last performance improvement you need to do
87

87

00:03:51.570  -->  00:03:54.600
is to use bigger files to minimize the overhead.
88

88

00:03:54.600  -->  00:03:58.230
So if you have many, many, many small files in Amazon S3,
89

89

00:03:58.230  -->  00:04:00.150
Athena is not going to be as performant
90

90

00:04:00.150  -->  00:04:01.890
as if you had larger files,
91

91

00:04:01.890  -->  00:04:04.620
for example, 128 megabytes and over,
92

92

00:04:04.620  -->  00:04:06.600
because larger files are easier to scan
93

93

00:04:06.600  -->  00:04:08.130
and easier to retrieve.
94

94

00:04:08.130  -->  00:04:12.510
Another feature of Amazon Athena is the federated query.
95

95

00:04:12.510  -->  00:04:15.540
So you know that Athena can query data in S3,
96

96

00:04:15.540  -->  00:04:18.030
but actually you can query data anywhere,
97

97

00:04:18.030  -->  00:04:21.390
for example, in relational or non-relational databases,
98

98

00:04:21.390  -->  00:04:24.300
you can query objects and custom data sources
99

99

00:04:24.300  -->  00:04:27.570
whether it be on AWS or on-premises.
100

100

00:04:27.570  -->  00:04:31.290
How? Well, you use what's called a Data Source Connector.
101

101

00:04:31.290  -->  00:04:32.640
It's a Lambda function
102

102

00:04:32.640  -->  00:04:34.590
and that Lambda function is going to run
103

103

00:04:34.590  -->  00:04:38.100
the federated queries in other services.
104

104

00:04:38.100  -->  00:04:40.350
So that could be, for example, CloudWatch Logs,
105

105

00:04:40.350  -->  00:04:43.560
DynamoDB, RDS, and so on, so it's very powerful.
106

106

00:04:43.560  -->  00:04:45.390
For example, we have Athena here
107

107

00:04:45.390  -->  00:04:46.800
and we have a Lambda function
108

108

00:04:46.800  -->  00:04:48.840
and you'll have one Lambda function
109

109

00:04:48.840  -->  00:04:50.940
per Data Source Connector.
110

110

00:04:50.940  -->  00:04:52.950
And then through Amazon Athena,
111

111

00:04:52.950  -->  00:04:56.017
you can run the query across ElastiCache, DocumentDB,
112

112

00:04:56.017  -->  00:05:00.150
DynamoDB, Redshift, Aurora, SQL Server, MySQL,
113

113

00:05:00.150  -->  00:05:04.560
HBase on the EMR service or any on-premises database
114

114

00:05:04.560  -->  00:05:08.010
directly from Athena, as well as, of course, Amazon S3.
115

115

00:05:08.010  -->  00:05:09.330
And you can do your joins
116

116

00:05:09.330  -->  00:05:11.730
and you can do your competitions and so on.
117

117

00:05:11.730  -->  00:05:14.250
That's why it's called a Federated Query.
118

118

00:05:14.250  -->  00:05:16.980
And then the results of this query can be stored
119

119

00:05:16.980  -->  00:05:20.550
into your Amazon S3 buckets for later analysis.
120

120

00:05:20.550  -->  00:05:21.810
So that's it for Amazon Athena.
121

121

00:05:21.810  -->  00:05:24.210
As you can see, it's a very powerful service.
122

122

00:05:24.210  -->  00:05:27.243
I hope you liked it, and I will see you in the next lecture.
