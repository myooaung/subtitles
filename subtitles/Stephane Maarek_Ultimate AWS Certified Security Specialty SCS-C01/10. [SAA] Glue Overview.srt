1
1

00:00:00,360  -->  00:00:03,450
<v Narrator>So now let's talk about AWS Glue.</v>
2

2

00:00:03,450  -->  00:00:06,150
So Glue is a managed extract,
3

3

00:00:06,150  -->  00:00:08,580
transform, and load service also called
4

4

00:00:08,580  -->  00:00:10,680
commonly ETL service.
5

5

00:00:10,680  -->  00:00:12,270
It's very useful to prepare
6

6

00:00:12,270  -->  00:00:15,150
and transform data for analytics.
7

7

00:00:15,150  -->  00:00:17,670
So this is a fully serverless service,
8

8

00:00:17,670  -->  00:00:19,800
and you're just going to submit whatever you want
9

9

00:00:19,800  -->  00:00:20,633
and it will achieve it.
10

10

00:00:20,633  -->  00:00:23,820
For example, say you had data in an S3 bucket
11

11

00:00:23,820  -->  00:00:25,680
or an Amazon RDS database
12

12

00:00:25,680  -->  00:00:30,680
and you wanted to load this into a Redshift data warehouse.
13

13

00:00:31,140  -->  00:00:33,420
So you could extract it using Glue.
14

14

00:00:33,420  -->  00:00:34,590
Then you would transform it,
15

15

00:00:34,590  -->  00:00:36,840
if need be to maybe filter some data
16

16

00:00:36,840  -->  00:00:39,060
or add some columns and so on, whatever you want.
17

17

00:00:39,060  -->  00:00:42,270
And then you would load the final output data
18

18

00:00:42,270  -->  00:00:44,670
into a Redshift data warehouse.
19

19

00:00:44,670  -->  00:00:47,520
So all of this happened from within the Glue ETL Service.
20

20

00:00:47,520  -->  00:00:49,050
You just have to write some code,
21

21

00:00:49,050  -->  00:00:51,840
launch your ETL job and off you go.
22

22

00:00:51,840  -->  00:00:53,310
So that's one example.
23

23

00:00:53,310  -->  00:00:54,650
Another example that's gonna come up
24

24

00:00:54,650  -->  00:00:58,350
in the exam is how to convert data into the Parquet formats.
25

25

00:00:58,350  -->  00:00:59,340
So why would you do this?
26

26

00:00:59,340  -->  00:01:03,540
Because, well, the Parquet formats is a columnar data format
27

27

00:01:03,540  -->  00:01:06,510
and, therefore, it is much better when in use, for example,
28

28

00:01:06,510  -->  00:01:08,670
with services like Athena.
29

29

00:01:08,670  -->  00:01:12,330
So say for example that you are doing inserts
30

30

00:01:12,330  -->  00:01:15,510
into your S3 buckets and then these files
31

31

00:01:15,510  -->  00:01:17,370
are in the CSV formats.
32

32

00:01:17,370  -->  00:01:21,780
Then you would use the Glue ETL Service to import the CSV
33

33

00:01:21,780  -->  00:01:24,840
and convert it into a Parquet format
34

34

00:01:24,840  -->  00:01:26,430
from within the Glue Service.
35

35

00:01:26,430  -->  00:01:29,790
Then you would send it into an output S3 bucket.
36

36

00:01:29,790  -->  00:01:31,620
And when in Parquet format
37

37

00:01:31,620  -->  00:01:34,530
then Amazon Athena is going to analyze this file
38

38

00:01:34,530  -->  00:01:36,480
in a much better fashion.
39

39

00:01:36,480  -->  00:01:39,000
So the other thing you can do to automate this
40

40

00:01:39,000  -->  00:01:43,410
entire process is that anytime a file is inserted
41

41

00:01:43,410  -->  00:01:47,430
into the S3 bucket, then you can send events notifications
42

42

00:01:47,430  -->  00:01:51,630
to a Lambda function which will then trigger a Glue ETL job.
43

43

00:01:51,630  -->  00:01:53,460
But you could replace the Lambda function
44

44

00:01:53,460  -->  00:01:55,200
with Event Bridge as well.
45

45

00:01:55,200  -->  00:01:57,810
This would work as an alternative.
46

46

00:01:57,810  -->  00:01:58,643
Okay.
47

47

00:01:58,643  -->  00:02:00,390
So there's another feature of Glue called
48

48

00:02:00,390  -->  00:02:03,990
the Glue Data Catalog, which is to catalog data sets.
49

49

00:02:03,990  -->  00:02:08,070
So the Glue Data Catalog will run Glue Data Crawlers
50

50

00:02:08,070  -->  00:02:11,880
and they will be connected to various data sources
51

51

00:02:11,880  -->  00:02:16,110
such as Amazon S3, Amazon RDS, Amazon DynamoDB
52

52

00:02:16,110  -->  00:02:19,440
or a compatible JDC database that you own
53

53

00:02:19,440  -->  00:02:21,210
on premises for example.
54

54

00:02:21,210  -->  00:02:25,350
So the Glue Data Crawler is going to crawl these databases
55

55

00:02:25,350  -->  00:02:29,010
and is going to write all the metadata of your tables,
56

56

00:02:29,010  -->  00:02:31,650
of your columns, of your data types, and so on
57

57

00:02:31,650  -->  00:02:33,540
into the Glue Data Catalog.
58

58

00:02:33,540  -->  00:02:36,750
And so it will have all the databases, the tables,
59

59

00:02:36,750  -->  00:02:39,270
and the metadata, and that will be leveraged
60

60

00:02:39,270  -->  00:02:42,780
by the Glue jobs to perform ETF.
61

61

00:02:42,780  -->  00:02:46,050
Now, also when you use Amazon Athena behind the scene
62

62

00:02:46,050  -->  00:02:48,690
to do the data discovery and the SQL discovery,
63

63

00:02:48,690  -->  00:02:51,060
Amazon Athena is going to be leveraging
64

64

00:02:51,060  -->  00:02:54,060
the AWS Glue Data Catalog.
65

65

00:02:54,060  -->  00:02:56,370
So will Amazon Redshift Spectrum.
66

66

00:02:56,370  -->  00:02:58,590
And so will Amazon EMR.
67

67

00:02:58,590  -->  00:03:01,830
So as you can see, the Glue Data Catalog service is central
68

68

00:03:01,830  -->  00:03:04,740
to many other AWS services.
69

69

00:03:04,740  -->  00:03:06,930
So other features that can appear at the exam
70

70

00:03:06,930  -->  00:03:09,750
on Glue and that you should know at a high level,
71

71

00:03:09,750  -->  00:03:12,330
the first one is Glue Job Bookmarks.
72

72

00:03:12,330  -->  00:03:14,700
And so this is to prevent you from
73

73

00:03:14,700  -->  00:03:19,380
reprocessing all data in case you run a new ETL job.
74

74

00:03:19,380  -->  00:03:20,310
So this is very important,
75

75

00:03:20,310  -->  00:03:21,960
and it can come up in the exam.
76

76

00:03:21,960  -->  00:03:24,060
Then you have Glue Elastic Views.
77

77

00:03:24,060  -->  00:03:26,010
And this is to combine and replicate data
78

78

00:03:26,010  -->  00:03:28,710
across multiple data stores using SQL.
79

79

00:03:28,710  -->  00:03:30,270
So you would, for example, create a view
80

80

00:03:30,270  -->  00:03:32,250
across an RDS database
81

81

00:03:32,250  -->  00:03:34,830
and an aura database and Amazon S3.
82

82

00:03:34,830  -->  00:03:36,990
So there's no custom code for this.
83

83

00:03:36,990  -->  00:03:40,050
And Glue is going to monitor for changes in the source data
84

84

00:03:40,050  -->  00:03:42,210
and it's going to be serverless.
85

85

00:03:42,210  -->  00:03:44,490
So this way you will create a virtual table
86

86

00:03:44,490  -->  00:03:47,310
which is a matureless view that spreads
87

87

00:03:47,310  -->  00:03:49,590
across multiple data store.
88

88

00:03:49,590  -->  00:03:52,080
You have Glue DataBrew which is used to clean
89

89

00:03:52,080  -->  00:03:55,170
and normalize data using prebuilt transformation.
90

90

00:03:55,170  -->  00:03:58,290
You have Glue Studio which is a GUI to create,
91

91

00:03:58,290  -->  00:04:01,020
run and monitor ETL jobs in Glue.
92

92

00:04:01,020  -->  00:04:03,420
And then you have Glue Streaming ETL,
93

93

00:04:03,420  -->  00:04:04,710
and it's actually built on top
94

94

00:04:04,710  -->  00:04:07,740
of Apache Spark Structured Streaming,
95

95

00:04:07,740  -->  00:04:10,860
and it's instead of running ETL jobs, as you know,
96

96

00:04:10,860  -->  00:04:14,280
batch jobs, you can run them as streaming jobs.
97

97

00:04:14,280  -->  00:04:16,530
And so, therefore, you can read data
98

98

00:04:16,530  -->  00:04:21,360
using Glue Streaming ETL from Kinesis Data Streams or Kafka
99

99

00:04:21,360  -->  00:04:26,130
or MSK as we'll see which is managed Kafka on AWS.
100

100

00:04:26,130  -->  00:04:26,963
Okay.
101

101

00:04:26,963  -->  00:04:28,380
So that's it for this lecture.
102

102

00:04:28,380  -->  00:04:29,400
I hope you liked it.
103

103

00:04:29,400  -->  00:04:31,350
And I will see you in the next lecture.
