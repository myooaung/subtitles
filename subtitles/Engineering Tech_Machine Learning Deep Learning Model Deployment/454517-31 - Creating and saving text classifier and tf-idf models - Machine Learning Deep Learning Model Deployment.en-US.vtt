WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.380 --> 00:00:06.510
Let's understand how to build a text classifier using the techniques that you've

00:00:06.510 --> 00:00:08.880
just learned will also understand some of

00:00:08.880 --> 00:00:12.495
the core concepts of NLP or Natural Language Processing.

00:00:12.495 --> 00:00:15.840
Go to Google collab and create a new notebook.

00:00:15.840 --> 00:00:19.155
We'll call it text classifier.

00:00:19.155 --> 00:00:23.220
There are various libraries available for natural language processing.

00:00:23.220 --> 00:00:28.739
Will be preprocessing our text using a popular library called NLTK.

00:00:28.739 --> 00:00:32.250
Will understand NLTK and some of the core concepts

00:00:32.250 --> 00:00:36.165
of Natural Language Processing by looking at some examples.

00:00:36.165 --> 00:00:39.490
First, we need to import NLTK.

00:00:41.730 --> 00:00:50.990
After that, we need to download NLTK libraries and will download all the liabilities.

00:00:51.600 --> 00:00:55.840
While it is downloading. Let's look at the text file that we'll

00:00:55.840 --> 00:01:00.290
be working on to understand NLP and build a text classifier.

00:01:01.410 --> 00:01:05.350
Will be looking at this restaurant review data.

00:01:05.350 --> 00:01:09.130
This is available on Kaggle and many other places online.

00:01:09.130 --> 00:01:12.910
This is restaurant reboot data and whether customers

00:01:12.910 --> 00:01:18.350
like the restaurant dot naught one means they've like did gentlemen step not like.

00:01:18.510 --> 00:01:23.035
You can see some of the positive sentences like the phrase, we're good.

00:01:23.035 --> 00:01:25.065
That is one that is positive.

00:01:25.065 --> 00:01:26.390
Who would not go back?

00:01:26.390 --> 00:01:27.875
That's a negative sentence,

00:01:27.875 --> 00:01:31.400
that's a negative review. So that is marked as 0.

00:01:31.400 --> 00:01:35.375
So based on these data will have to build it text classifier,

00:01:35.375 --> 00:01:39.980
using which we can predict whether a new sentences positive or not.

00:01:39.980 --> 00:01:44.370
We'll click on the tab to get the path of this file.

00:01:45.280 --> 00:01:48.200
We need Pandas to load the file.

00:01:48.200 --> 00:01:50.795
So we'll first import numpy as np,

00:01:50.795 --> 00:01:53.670
then Pandas as pd.

00:01:54.880 --> 00:02:02.070
Using pandas read_csv will read this CSV from our GitHub repository.

00:02:02.350 --> 00:02:06.410
We got an error because this is not Comma-separated,

00:02:06.410 --> 00:02:10.460
tab-separated, so you have to specify that delimiter.

00:02:10.460 --> 00:02:16.190
So the delimiter would be tab and then capture coating equal three,

00:02:16.190 --> 00:02:19.535
which means double-quotes should be ignored.

00:02:19.535 --> 00:02:22.025
Once it is loaded to a Pandas DataFrame,

00:02:22.025 --> 00:02:24.425
we can see the top records.

00:02:24.425 --> 00:02:28.310
Now this restaurant advertisement loaded to a Pandas DataFrame.

00:02:28.310 --> 00:02:30.319
In natural language processing,

00:02:30.319 --> 00:02:32.870
we remove some of the commonly occurring words lake,

00:02:32.870 --> 00:02:36.770
even though they might not tell us whether a sentence is positive or negative,

00:02:36.770 --> 00:02:38.465
but they would occupy space.

00:02:38.465 --> 00:02:40.490
Those words are called stop words.

00:02:40.490 --> 00:02:44.660
And using NLTK, we can easily get rid of all the stop words.

00:02:44.660 --> 00:02:50.000
There is another concept called stemming using which we can derive root form of words.

00:02:50.000 --> 00:02:52.295
For example, for both running at run,

00:02:52.295 --> 00:02:55.010
we can have word run for totally and total.

00:02:55.010 --> 00:02:56.630
We can have worked total.

00:02:56.630 --> 00:03:00.950
That we, we limit the number of words in our analysis.

00:03:00.950 --> 00:03:02.975
Let's understand how that would work.

00:03:02.975 --> 00:03:06.875
First, we'll import stopwords library from NLTK.

00:03:06.875 --> 00:03:08.960
Then we'll import porter stemmer,

00:03:08.960 --> 00:03:11.615
using which you can derive route for the words,

00:03:11.615 --> 00:03:14.915
will instantiate the stemmer class.

00:03:14.915 --> 00:03:17.750
Now let's look at our dataset in detail.

00:03:17.750 --> 00:03:19.475
It is 1000 entries,

00:03:19.475 --> 00:03:23.450
will have to loop through these touted entries and remove all the stop

00:03:23.450 --> 00:03:27.680
words and apply stemming and create a corpus of clean tech.

00:03:27.680 --> 00:03:32.840
First we'll declare a empty list which will contain the corpus of text.

00:03:32.840 --> 00:03:36.380
Now for i in range 0 to thousand,

00:03:36.380 --> 00:03:41.285
we'll declare a customer review variable which will contain data for each row,

00:03:41.285 --> 00:03:46.175
which we can fetch using dataset review I.

00:03:46.175 --> 00:03:54.364
Next, we'll get rid of all the stop words and apply stemming using this syntax.

00:03:54.364 --> 00:03:57.845
So we'll get all the words which are there in customer review.

00:03:57.845 --> 00:04:04.520
And if the word is not in the English stopword list off NLTK library, you apply stemming.

00:04:04.520 --> 00:04:09.455
Then you can concatenate the words to get the sentence back.

00:04:09.455 --> 00:04:13.160
And then finally, we'll append that to the corpus list,

00:04:13.160 --> 00:04:15.815
will also do some further data cleaning.

00:04:15.815 --> 00:04:17.270
If we look at this reboot,

00:04:17.270 --> 00:04:20.915
there are certain characters like exclamation mark,

00:04:20.915 --> 00:04:23.510
which we can also get rid of using Python.

00:04:23.510 --> 00:04:29.075
Regular expression will keep only alphabets in smaller capital letters.

00:04:29.075 --> 00:04:33.185
And you can easily do that in Python using regular expression.

00:04:33.185 --> 00:04:35.960
And the syntax for that is something like this.

00:04:35.960 --> 00:04:40.790
Should, this should get rid of all the characters which are not alphabet and will

00:04:40.790 --> 00:04:46.490
also convert all the sentences to lowercase for consistency.

00:04:46.490 --> 00:04:51.890
Now we'll split the sentence on space to derive the words.

00:04:51.890 --> 00:04:55.564
So the first line is removing all the junk characters.

00:04:55.564 --> 00:04:59.015
Then we are converting the sentences to lowercase,

00:04:59.015 --> 00:05:01.055
splitting it by space.

00:05:01.055 --> 00:05:05.075
For each word. If it is not in stopwords,

00:05:05.075 --> 00:05:08.750
then we are taking that word and applying stemming.

00:05:08.750 --> 00:05:13.050
And then finally we're joining all the watch to get the sentence back.

00:05:13.120 --> 00:05:16.890
So let's run it and see the output.

00:05:17.260 --> 00:05:22.080
We need to import the regular expression also.

00:05:26.740 --> 00:05:34.595
This has to be lower. Now after this,

00:05:34.595 --> 00:05:37.415
we should have a corpus of clean sentences.

00:05:37.415 --> 00:05:39.980
Let's check out the values.

00:05:39.980 --> 00:05:44.330
We'll take the first sentence is you can see now we

00:05:44.330 --> 00:05:49.055
have all the dots removed and the entire sentence convert it to lowercase.

00:05:49.055 --> 00:05:50.570
Let's say chord line seven,

00:05:50.570 --> 00:05:52.610
which is an index six.

00:05:52.610 --> 00:05:55.980
You can see the parentheses have been removed.

00:05:56.080 --> 00:05:59.555
And also all the stop words like a

00:05:59.555 --> 00:06:03.005
in the and other commonly occurring words have been removed.

00:06:03.005 --> 00:06:07.775
And the tamer helped us derive the root form up each word.

00:06:07.775 --> 00:06:10.770
Let's look at another example.

00:06:14.650 --> 00:06:20.270
So this is another sentence where words have been changed to their root form.

00:06:20.270 --> 00:06:23.420
Note that the root form may or may not have any meaning.

00:06:23.420 --> 00:06:26.210
But then that would help us reduce the number of

00:06:26.210 --> 00:06:30.350
words so that we can do the processing much faster.

00:06:30.350 --> 00:06:37.470
Next, let's convert the sentences to numeric format using TFIDF vector treasure.

00:06:37.810 --> 00:06:41.690
Scikit-learn is it TFIDF vector Egypt class.

00:06:41.690 --> 00:06:44.585
And we can specify how many words we want,

00:06:44.585 --> 00:06:47.000
tau 01500 or whatever number.

00:06:47.000 --> 00:06:49.850
Using mean DAF, we are specifying that the word should

00:06:49.850 --> 00:06:53.270
occur at lease price for that to be considered.

00:06:53.270 --> 00:06:56.120
So you can get rid of words that are.

00:06:56.120 --> 00:06:59.165
Cutting infrequently using the mean df.

00:06:59.165 --> 00:07:05.134
Using max D if you can get rid of words that occur frequently in all the documents.

00:07:05.134 --> 00:07:08.390
So for example, MAX da 0.6 would get rid of

00:07:08.390 --> 00:07:12.574
any word which occur in more than 60% of the documents.

00:07:12.574 --> 00:07:18.575
Next, using the vectorized or we can convert the corpus to a numeric carrier.

00:07:18.575 --> 00:07:21.540
Let's print takes now.

00:07:21.760 --> 00:07:25.310
So these are the TF-IDF values.

00:07:25.310 --> 00:07:28.775
There will be some nonzero values which are not displayed in this notebook.

00:07:28.775 --> 00:07:31.235
Let's check a sample record.

00:07:31.235 --> 00:07:36.750
And we can see that some of the words have nonzero values.

00:07:38.950 --> 00:07:43.235
So this victimizer is created a two-dimensional

00:07:43.235 --> 00:07:47.990
numeric carry from all the sentences in the restaurant review file.

00:07:47.990 --> 00:07:53.240
In this dataset, like does the dependent variable which contains one or 0.

00:07:53.240 --> 00:07:56.135
So let's create a dependent variable,

00:07:56.135 --> 00:07:59.280
y, which will have data for this column.

00:07:59.440 --> 00:08:03.350
So we'll get all the rows and the second column,

00:08:03.350 --> 00:08:06.600
convert that to a NumPy array.

00:08:07.930 --> 00:08:12.935
And when you print y, you can see all the values one or 0.

00:08:12.935 --> 00:08:16.190
After this, the stapes to create

00:08:16.190 --> 00:08:20.495
machine-learning model is same as what we have seen earlier for numeric data.

00:08:20.495 --> 00:08:22.580
We'll do train test, split,

00:08:22.580 --> 00:08:26.120
keep 80% data for training, 20% for testing.

00:08:26.120 --> 00:08:30.050
Let's use the K never technique to build a classifier.

00:08:30.050 --> 00:08:33.920
So you can also use any other classification technique like maybe

00:08:33.920 --> 00:08:38.270
so which is a popular classifier for text-based data.

00:08:38.270 --> 00:08:42.060
Now let's predict using the classifier.

00:08:42.220 --> 00:08:45.890
Let's derive the confusion matrix.

00:08:45.890 --> 00:08:48.785
Will now print the equity issue.

00:08:48.785 --> 00:08:53.780
Next, let's have a sample sentence and predict whether it is positive or negative.

00:08:53.780 --> 00:08:58.970
We use the same vector leisure to convert this sentence to numeric format.

00:08:58.970 --> 00:09:03.785
So this is now the TF-IDF representation of the sentence.

00:09:03.785 --> 00:09:09.440
After that, we can predict the sentiment using the predict method of the classifier.

00:09:09.440 --> 00:09:12.500
So we got one which is positive.

00:09:12.500 --> 00:09:15.635
Let's have another sample sentence.

00:09:15.635 --> 00:09:18.800
Convert that to TFIDF format.

00:09:18.800 --> 00:09:22.775
Now predict the sentiment and we got to 0.

00:09:22.775 --> 00:09:24.830
So this is a negative sentence.

00:09:24.830 --> 00:09:27.920
This is how we can build a text classifier which can read

00:09:27.920 --> 00:09:32.850
different sentences and determine whether it is positive or negative.

00:09:33.190 --> 00:09:38.180
Now if anybody wants to predict using this classifier,

00:09:38.180 --> 00:09:39.800
they would need the classifier.

00:09:39.800 --> 00:09:42.230
They would also need the victory measure.

00:09:42.230 --> 00:09:45.290
Let's export this two files in pickled format.

00:09:45.290 --> 00:09:47.855
So this is our classifier.

00:09:47.855 --> 00:09:50.314
We'll call it text classifier.

00:09:50.314 --> 00:09:53.840
And we'll create a pickle file for that TF-IDF model.

00:09:53.840 --> 00:09:59.810
Now we have both the pickle files and we can download from the colab environment and

00:09:59.810 --> 00:10:02.375
take it to another environment where we can

00:10:02.375 --> 00:10:06.600
use this buckle files to predict sentiment of text.
