WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.380 --> 00:00:06.735
Let's now understand how to create dot text classifier using by touch.

00:00:06.735 --> 00:00:13.095
The stapes work takes preprocessing and cleansing is same as what we have done earlier.

00:00:13.095 --> 00:00:15.870
Once you have the corpus of clean tech,

00:00:15.870 --> 00:00:21.630
you can use TFIDF vector Asia to create a numeric array.

00:00:21.630 --> 00:00:26.969
And then after that you can do train test split using scikit-learn.

00:00:26.969 --> 00:00:31.755
After that, instead of creating a model using k-nearest neighbor technique,

00:00:31.755 --> 00:00:35.820
we'll use Python to build it text classifier.

00:00:35.820 --> 00:00:40.180
Import the required library for touch.

00:00:43.510 --> 00:00:49.070
You need to convert x and y variable to tensor format.

00:00:49.070 --> 00:00:54.125
One thing to note here is we have total 1000 sentences in the corpus.

00:00:54.125 --> 00:00:56.270
They have 467 features.

00:00:56.270 --> 00:01:03.120
So these are the vectorizes towards now that determine our input node Cij.

00:01:03.130 --> 00:01:09.320
So earlier we had seen an example of a Python model with two input nodes,

00:01:09.320 --> 00:01:10.910
that is for Asian salary.

00:01:10.910 --> 00:01:14.840
But this time we have an input node of size 467 because there are

00:01:14.840 --> 00:01:21.170
467 different words that are features for this model building.

00:01:21.170 --> 00:01:23.840
Output size would be two because you are

00:01:23.840 --> 00:01:27.990
predicting with the sentiment is positive or negative.

00:01:28.090 --> 00:01:32.675
Or we can try with different hidden size.

00:01:32.675 --> 00:01:35.939
Let me try with 500.

00:01:36.820 --> 00:01:39.230
Similar to the previous example,

00:01:39.230 --> 00:01:40.790
we have two hidden layers,

00:01:40.790 --> 00:01:43.850
will have fully-connected layers input to hidden,

00:01:43.850 --> 00:01:47.280
hidden to hidden, and then he did do final output.

00:01:47.530 --> 00:01:52.595
So the only change here is the input size n perihelion sage.

00:01:52.595 --> 00:01:56.345
Rest of the steps are discussed earlier.

00:01:56.345 --> 00:01:59.130
Defined a model class.

00:01:59.470 --> 00:02:05.160
Then you define, optimize your learning rate.

00:02:05.920 --> 00:02:09.150
A 100 epochs this time.

00:02:10.180 --> 00:02:14.510
And now let's train the neural network.

00:02:14.510 --> 00:02:17.910
You will see the losses getting minimized.

00:02:19.180 --> 00:02:23.810
And now the model is trained and ready for prediction.

00:02:23.810 --> 00:02:28.010
We can predict the way we predicted our layer.

00:02:28.010 --> 00:02:34.290
Let's have a sample sentence will convert it to numeric format.

00:02:36.220 --> 00:02:41.730
And we need to convert that sentence to dodge denser format.

00:02:42.040 --> 00:02:47.420
After that, you can predict using the biters model class.

00:02:47.420 --> 00:02:51.020
From this, we can see that it's

00:02:51.020 --> 00:02:56.520
a positive sentence because the second element is higher than the first one.

00:02:56.980 --> 00:03:01.670
If we have another sentence similar to the one that we had earlier,

00:03:01.670 --> 00:03:03.290
which is a negative sentence,

00:03:03.290 --> 00:03:09.215
then will get the output in which the first element will be higher than the second one.

00:03:09.215 --> 00:03:11.645
So this is a negative sentence.

00:03:11.645 --> 00:03:15.080
And now you can create a dictionary from this model,

00:03:15.080 --> 00:03:18.665
the way you have done earlier using state underscore dictionary method.

00:03:18.665 --> 00:03:22.370
Then you can save the model and export that model

00:03:22.370 --> 00:03:27.210
and use it in another environment or created SDP from this model.
