WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.740 --> 00:00:06.300
Next we'll create a serverless API for our machine learning model.

00:00:06.300 --> 00:00:11.130
Heavier we saw how to create a virtual machine and deploy the model.

00:00:11.130 --> 00:00:13.080
If we ever own hardware,

00:00:13.080 --> 00:00:15.270
our infrastructure to run the machine learning model that

00:00:15.270 --> 00:00:17.565
you'll have to pay for the maintenance cost,

00:00:17.565 --> 00:00:19.260
even if nobody's using your modern,

00:00:19.260 --> 00:00:22.185
you still need to pay for the cost for the VM.

00:00:22.185 --> 00:00:24.660
With a serverless cloud model,

00:00:24.660 --> 00:00:28.200
you do not need to worry about the dandelion infrastructure,

00:00:28.200 --> 00:00:30.180
the maintenance, and the cost.

00:00:30.180 --> 00:00:34.620
You can focus on writing your business logic in a function or a method.

00:00:34.620 --> 00:00:37.440
And you will be charged based on the number of

00:00:37.440 --> 00:00:41.935
times that function method is getting invoked.

00:00:41.935 --> 00:00:44.450
If nobody is using that functional method,

00:00:44.450 --> 00:00:45.935
you will not be charged.

00:00:45.935 --> 00:00:47.450
That is the serverless model,

00:00:47.450 --> 00:00:50.370
which is the next big thing in cloud computing.

00:00:50.740 --> 00:00:55.445
Let's now see how to create a serverless function on Google,

00:00:55.445 --> 00:00:58.025
Cloud environment and random machine-learning model.

00:00:58.025 --> 00:01:04.925
You can also try the same thing on Azure cloud using Azure functions or AWS Lambda.

00:01:04.925 --> 00:01:08.915
First thing we need is we need to store the modal somewhere on Google Cloud.

00:01:08.915 --> 00:01:11.255
Google Cloud provides something called buckets,

00:01:11.255 --> 00:01:14.300
using which you can store objects.

00:01:14.300 --> 00:01:17.420
It's Lake S3 bucket on AWS cloud.

00:01:17.420 --> 00:01:22.550
Our blob storage or the job creating a bucket is really easy.

00:01:22.550 --> 00:01:27.300
You give a name to the bucket and it has to be globally unique.

00:01:37.600 --> 00:01:41.765
Click Continue. You can select one reason.

00:01:41.765 --> 00:01:44.450
You have to select a location and you'd have to make sure

00:01:44.450 --> 00:01:48.060
Google Cloud Function runs in the same location.

00:01:48.450 --> 00:01:50.920
Stories tape can be standard,

00:01:50.920 --> 00:01:52.810
you can leave it default.

00:01:52.810 --> 00:01:56.090
Fine-grained access is fine.

00:01:56.310 --> 00:01:59.905
You can have Google-managed ski for your bucket.

00:01:59.905 --> 00:02:02.724
Anyway, we are not going into the details of storage,

00:02:02.724 --> 00:02:07.040
but just leave everything as default and hit cleared.

00:02:10.230 --> 00:02:14.770
Now the rocket has been created and we can

00:02:14.770 --> 00:02:18.475
see the bucket and under that we could clear it folder directly upload files.

00:02:18.475 --> 00:02:25.040
Let's create a folder. Will say models.

00:02:34.170 --> 00:02:39.500
And then we'll upload files to this bucket folder.

00:02:42.960 --> 00:02:51.080
Let's applaud the classified pickle and a CDART buckled to this bucket folder.

00:02:54.120 --> 00:02:58.070
We can see that both the files have been uploaded.

00:03:04.170 --> 00:03:06.820
You can go to the bucket rule directly,

00:03:06.820 --> 00:03:08.230
see all your buckets.

00:03:08.230 --> 00:03:10.945
So I have few buckets which are already created.

00:03:10.945 --> 00:03:13.095
This the new one that I just created.

00:03:13.095 --> 00:03:15.695
I also bucket effects skills,

00:03:15.695 --> 00:03:18.230
which is a models folder.

00:03:18.230 --> 00:03:21.350
And under the same files that are available,

00:03:21.350 --> 00:03:23.870
classified or equivalent ac dot vehicle.

00:03:23.870 --> 00:03:27.125
Next we'll search for Cloud Functions.

00:03:27.125 --> 00:03:30.240
Select Cloud Functions.

00:03:33.730 --> 00:03:36.650
Here I've already created some functions,

00:03:36.650 --> 00:03:38.360
but if you have to create a new function,

00:03:38.360 --> 00:03:40.950
just click on this Create function.

00:03:43.990 --> 00:03:48.080
You can give a function, a name, select the region.

00:03:48.080 --> 00:03:51.860
And it is showing the default version which is matching with the bucket.

00:03:51.860 --> 00:03:54.860
Sit regard type is HTTP,

00:03:54.860 --> 00:04:00.170
so that you can get an HTTP endpoint to access this function from anywhere.

00:04:00.170 --> 00:04:03.560
Allow unauthenticated invocations because you are just

00:04:03.560 --> 00:04:07.160
testing something and we don't need to part authentication

00:04:07.160 --> 00:04:14.580
or get into access management for this lab will leave everything else as default.

00:04:15.100 --> 00:04:17.690
To 56 MB is fine,

00:04:17.690 --> 00:04:20.135
but if you want, you can allocate more memory.

00:04:20.135 --> 00:04:23.520
We can also specify the timeout deletion.

00:04:24.760 --> 00:04:32.225
Let's click Next. And here you have to select your runtime.

00:04:32.225 --> 00:04:34.715
We'll use Python 3.7.

00:04:34.715 --> 00:04:37.100
And then we can write our code here.

00:04:37.100 --> 00:04:39.965
You can see a sample HelloWorld program here.

00:04:39.965 --> 00:04:43.520
If we're logging into Google Cloud Function for the first time,

00:04:43.520 --> 00:04:46.025
you might be prompted to enable APIs.

00:04:46.025 --> 00:04:48.260
If you're prompted, just do that.

00:04:48.260 --> 00:04:50.975
Here you see a sample HelloWorld program.

00:04:50.975 --> 00:04:52.865
You can deploy and test it.

00:04:52.865 --> 00:04:57.830
In the requirements.txt, you specify what are the packages required.

00:04:57.830 --> 00:05:01.670
So I've already written a function for the machine learning code.

00:05:01.670 --> 00:05:02.900
Let me open

00:05:02.900 --> 00:05:04.560
that function,

00:05:11.410 --> 00:05:14.220
help replicate it.

00:05:14.440 --> 00:05:17.390
And it will take me to the same screen which you

00:05:17.390 --> 00:05:21.000
saw when you click on create a new function.

00:05:23.590 --> 00:05:28.910
This function was also created with it triggered type is, is to, to be.

00:05:28.910 --> 00:05:34.970
The function has a rest API and you get HTTP URL endpoint to access that function.

00:05:34.970 --> 00:05:36.965
Click next.

00:05:36.965 --> 00:05:40.085
So this is the function that I've already written.

00:05:40.085 --> 00:05:41.735
Let me quickly go through it.

00:05:41.735 --> 00:05:47.510
You have to import requests the way we did the awhile importing flask for our other demo.

00:05:47.510 --> 00:05:49.940
And this also works based on flask.

00:05:49.940 --> 00:05:52.160
For any Python functions are method3,

00:05:52.160 --> 00:05:55.025
which is flashed to export HTTP end point.

00:05:55.025 --> 00:05:56.930
You have to import pickle.

00:05:56.930 --> 00:06:02.360
You have to import storage because we need to access topical files from the bucket.

00:06:02.360 --> 00:06:06.680
For that, we need the Google Cloud SDK storage.

00:06:06.680 --> 00:06:10.220
You're pretty import numpy as np.

00:06:10.220 --> 00:06:13.205
You have to get the request JSON.

00:06:13.205 --> 00:06:15.980
This is similar code, what we did earlier.

00:06:15.980 --> 00:06:21.125
Additional code that you have to write is get an instance of the storage client.

00:06:21.125 --> 00:06:23.630
Get an instance of the bucket by doing

00:06:23.630 --> 00:06:26.615
stories Glenn dot get bucket and specifying the bucket name.

00:06:26.615 --> 00:06:31.175
Then you load the classifier and scalar specifying the path here.

00:06:31.175 --> 00:06:32.765
It has to be folded Naaman,

00:06:32.765 --> 00:06:38.945
the pickle file names unit to download the pickled files and you get it.

00:06:38.945 --> 00:06:40.100
Pre.

00:06:40.100 --> 00:06:43.460
Gold chloride function gives you a slash temp directory,

00:06:43.460 --> 00:06:47.105
PMP, under which you can download the pickled files.

00:06:47.105 --> 00:06:51.485
Then you have to load the pickle files in scalar the way you did earlier.

00:06:51.485 --> 00:06:55.385
After that, the code is pretty much similar to what we had done earlier.

00:06:55.385 --> 00:06:58.850
You read the agent salary and using those variables

00:06:58.850 --> 00:07:02.645
you can predict and you return that prediction.

00:07:02.645 --> 00:07:08.255
There's a requirements.txt file in which you have to include the required packages.

00:07:08.255 --> 00:07:12.410
You have to specify the package name and which version we need request,

00:07:12.410 --> 00:07:15.980
we need scikit-learn, we need cool cloud storage, and we need numpy.

00:07:15.980 --> 00:07:18.080
That's all you have to do.

00:07:18.080 --> 00:07:21.060
And simply click Deploy.

00:07:21.880 --> 00:07:24.260
It will deploy the function.

00:07:24.260 --> 00:07:27.080
You can come back, make changes and again deploy.

00:07:27.080 --> 00:07:30.995
It takes a few seconds for the function to get deployed.

00:07:30.995 --> 00:07:37.350
Once deployed, you can click on the function name and get into the details.

00:07:39.640 --> 00:07:43.130
You can see various metrics about that function.

00:07:43.130 --> 00:07:46.640
How many times it is getting invoked here in last one hour,

00:07:46.640 --> 00:07:48.290
six hour, 12 hour or whatever,

00:07:48.290 --> 00:07:51.170
and you get charged for the number of requests.

00:07:51.170 --> 00:07:53.960
But since we are using a Google Cloud of fried bread,

00:07:53.960 --> 00:07:58.970
that amount will get deducted from the credit available in our account.

00:07:58.970 --> 00:08:02.310
You can see the source here.

00:08:02.530 --> 00:08:07.055
You'll find the HTTP end point down to the triggered tab.

00:08:07.055 --> 00:08:11.690
And you can go to the testing tab and paste dysfunction.

00:08:11.690 --> 00:08:15.200
You do not need to write any external program to test this.

00:08:15.200 --> 00:08:18.290
So we'll send Asian salary that sort our function

00:08:18.290 --> 00:08:22.295
expects and did to predict whether a customer will buy or not.

00:08:22.295 --> 00:08:25.490
You would request has to be a valid JSON format.

00:08:25.490 --> 00:08:28.025
Click on paste the function to test it.

00:08:28.025 --> 00:08:30.710
And we can see the prediction here, 0.2.

00:08:30.710 --> 00:08:32.780
And then it will also display the log.

00:08:32.780 --> 00:08:40.520
And that will change h to 40 to 50 thousand.

00:08:40.520 --> 00:08:45.215
And we should get the prediction is 0.8.

00:08:45.215 --> 00:08:48.410
That's the probability of customer buying.

00:08:48.410 --> 00:08:51.095
We can see the prediction is 0.8.

00:08:51.095 --> 00:08:55.010
This is how you can apply a machine learning model in a serverless environment.

00:08:55.010 --> 00:08:58.130
Now this can scale up to millions or billions of users.

00:08:58.130 --> 00:09:00.605
You don't have to worry about the underlying infrastructure.

00:09:00.605 --> 00:09:02.975
Google Cloud function would take care of it.

00:09:02.975 --> 00:09:05.450
All you need to worry about is the business logic

00:09:05.450 --> 00:09:08.480
or the code that you're writing in a serverless environment.

00:09:08.480 --> 00:09:11.995
And we add some print statement in the log that you can see.

00:09:11.995 --> 00:09:16.410
This will help in debugging though, function whenever required.
