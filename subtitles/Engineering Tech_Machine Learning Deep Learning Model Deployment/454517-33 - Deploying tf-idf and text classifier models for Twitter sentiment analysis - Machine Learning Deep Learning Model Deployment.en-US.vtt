WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.710 --> 00:00:05.490
Let's now go to the text classifier notebook on

00:00:05.490 --> 00:00:10.690
Google Columbian download the pickle files that we generated in the previous level.

00:00:11.030 --> 00:00:14.715
First we need to import the file stability.

00:00:14.715 --> 00:00:22.180
Then we can save file store download and specify the file name in courts,

00:00:25.580 --> 00:00:31.484
and download the bcl files faster download the classifier.

00:00:31.484 --> 00:00:35.710
Then we'll download the TF-IDF modelling

00:00:37.040 --> 00:00:41.860
will upload the pickled files to GitHub repository.

00:00:48.130 --> 00:00:55.650
Now let's create a new notebook for Twitter sentiment analysis. We'll save this.

00:00:56.470 --> 00:01:00.839
We'll name it as Drew doesn't demand analysis.

00:01:01.030 --> 00:01:03.170
This is a new notebook,

00:01:03.170 --> 00:01:06.020
so the pickle files will not be present here.

00:01:06.020 --> 00:01:09.590
Will copy them from GitHub repository.

00:01:09.590 --> 00:01:12.540
Copy link address.

00:01:14.830 --> 00:01:19.550
Then, first get dot TF-IDF model,

00:01:19.550 --> 00:01:25.260
Copy link address, and then get the text classifier.

00:01:29.560 --> 00:01:33.360
Now both the files have been copied.

00:01:33.640 --> 00:01:39.900
To do Twitter sentiment analysis from a Python program will use to be liability.

00:01:40.360 --> 00:01:43.980
First clinically important 3p.

00:01:45.220 --> 00:01:50.480
Then we need to declare forward variables to store the consumer key, consumer secret,

00:01:50.480 --> 00:01:52.325
access token and access secret.

00:01:52.325 --> 00:01:55.505
Let's copy them from our developer account.

00:01:55.505 --> 00:02:02.930
We'll select the app that we just created and copy

00:02:02.930 --> 00:02:10.580
this key secretes and access token and access secret and regenerate these keys.

00:02:10.580 --> 00:02:14.810
After this lab, you will not be able to use these keys.

00:02:14.810 --> 00:02:19.670
Next, we'll write those turned out to be core to get

00:02:19.670 --> 00:02:23.795
outraged to Twitter using the consumer key consumer secret,

00:02:23.795 --> 00:02:26.060
access token and access secret.

00:02:26.060 --> 00:02:32.990
Next declared an APA variable with certain timeout, specified 22nd timeout.

00:02:32.990 --> 00:02:36.930
If there is no tweet for 20 seconds, then it will timeout.

00:02:37.900 --> 00:02:44.210
Next, let's fetch tweets for a particular text.

00:02:44.210 --> 00:02:47.135
Will be fetching for vaccine,

00:02:47.135 --> 00:02:49.590
which is a popular topic.

00:02:49.780 --> 00:02:54.395
Now we'll create an empty list to store all the tweets.

00:02:54.395 --> 00:02:57.500
And then using standard 2pi chord,

00:02:57.500 --> 00:02:58.550
we can fetch all dot,

00:02:58.550 --> 00:03:01.685
which the only thing that you need to pay attention to is

00:03:01.685 --> 00:03:05.910
how many tweets you want to fetch have specified 500 here.

00:03:11.620 --> 00:03:16.235
This will keep running until it reaches 500 tweets.

00:03:16.235 --> 00:03:19.250
You can verify the length of number of goods,

00:03:19.250 --> 00:03:21.275
phase two, which is 500.

00:03:21.275 --> 00:03:24.695
You can check some sample two, it's also,

00:03:24.695 --> 00:03:31.200
so these are some real grids that people are tweeting right now on covert vaccine.

00:03:31.990 --> 00:03:37.130
As you can see, that tweet said lord of special characters like cohosh.

00:03:37.130 --> 00:03:39.890
At the rate, we can use Python, relay,

00:03:39.890 --> 00:03:43.680
periodic let-expression, two pins that weights.

00:03:44.860 --> 00:03:49.385
So we didn't really look. We'll get tweets one-by-one,

00:03:49.385 --> 00:03:51.350
converted them to lowercase,

00:03:51.350 --> 00:03:54.304
will remove all John characters.

00:03:54.304 --> 00:03:57.710
You can read more about regular expression and

00:03:57.710 --> 00:04:01.925
understand how to deal with different types of text.

00:04:01.925 --> 00:04:05.600
We can take a sample to eat after cleansing.

00:04:05.600 --> 00:04:07.860
Let's check out this one.

00:04:12.700 --> 00:04:16.340
See that it's gained at all.

00:04:16.340 --> 00:04:18.665
The special characters are gone.

00:04:18.665 --> 00:04:22.295
We have learned videos techniques to deploy the pickled files,

00:04:22.295 --> 00:04:28.100
like having risky IPAs are serverless EPAs for this lab,

00:04:28.100 --> 00:04:38.670
let's simply Lord the pickle files to two variables and use them to import topical.

00:04:41.050 --> 00:04:46.580
And we lowered our TF-IDF model to another variable.

00:04:46.580 --> 00:04:52.730
Let's declare two variables to keep track of positive and negative tweets.

00:04:52.730 --> 00:04:56.510
Next we look to the Twitter list and using

00:04:56.510 --> 00:05:02.285
classifier dot predict method will predict sentiment for each tweet.

00:05:02.285 --> 00:05:06.410
And before fitting that takes to the classifier will have

00:05:06.410 --> 00:05:11.340
to apply the TF-IDF model to convert it to numeric format.

00:05:13.810 --> 00:05:21.140
Let's run this. After that we will get the positive and negative uidCount.

00:05:21.140 --> 00:05:25.040
Let's see how many positive but two, it's on vaccine,

00:05:25.040 --> 00:05:30.560
it's 97 and then 403 negative two.

00:05:30.560 --> 00:05:36.840
So this is the sentiment of the text analyzed for last 500 tweets.
