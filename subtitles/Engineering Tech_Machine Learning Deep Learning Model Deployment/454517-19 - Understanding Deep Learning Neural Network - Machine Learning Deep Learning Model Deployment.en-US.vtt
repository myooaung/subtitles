WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.260 --> 00:00:03.870
Deep learning is a subfield of machine learning,

00:00:03.870 --> 00:00:08.895
and neural networks are the most common category of deep learning algorithm.

00:00:08.895 --> 00:00:12.435
Let's understand the core concepts of neural network.

00:00:12.435 --> 00:00:15.945
In traditional machine learning would take good data

00:00:15.945 --> 00:00:19.065
and feed it to an algorithm and then get the output.

00:00:19.065 --> 00:00:23.205
For example, we could have A's in salary and other information of the customer,

00:00:23.205 --> 00:00:25.530
and we feed it to a classifier

00:00:25.530 --> 00:00:28.290
and then predict whether the customer is going to buy or not.

00:00:28.290 --> 00:00:35.190
Let's understand how the same thing would work in a neural network or deep learning.

00:00:35.190 --> 00:00:38.160
Let's start with a simple neutron.

00:00:38.160 --> 00:00:40.810
Neutron is a individual learning unit.

00:00:40.810 --> 00:00:44.120
It would read the input parameters and

00:00:44.120 --> 00:00:47.840
apply an activation function and then get the output.

00:00:47.840 --> 00:00:49.760
Typically in deep learning,

00:00:49.760 --> 00:00:54.830
you create many layers of neurons and build a neural network.

00:00:54.830 --> 00:00:57.320
Each neuron reads to a weighted sum of

00:00:57.320 --> 00:01:00.230
inputs from the previous layer from all the neurons,

00:01:00.230 --> 00:01:03.050
applies an activation function and passes

00:01:03.050 --> 00:01:06.780
the output to all the neurons in the subsequent layer.

00:01:06.910 --> 00:01:11.015
Let's understand activation function.

00:01:11.015 --> 00:01:16.430
In neuron receives weighted sum of all the inputs and

00:01:16.430 --> 00:01:18.290
applies an activation function to get

00:01:18.290 --> 00:01:22.355
the output which is passed to the next neuron in the next layer.

00:01:22.355 --> 00:01:24.575
Typically the hidden layer,

00:01:24.575 --> 00:01:28.760
we use RL2 activation function that is rectified linear unit.

00:01:28.760 --> 00:01:32.210
A railway line plot looks something like this.

00:01:32.210 --> 00:01:34.130
The one highlighted in red.

00:01:34.130 --> 00:01:37.730
For certain variables up to a point,

00:01:37.730 --> 00:01:40.520
the neutron ignores that input.

00:01:40.520 --> 00:01:44.990
For example, a customer might not be buying up to is 30.

00:01:44.990 --> 00:01:48.740
So it will not give any output up, please 30.

00:01:48.740 --> 00:01:52.100
And moment h crosses 30,

00:01:52.100 --> 00:01:54.530
the chances of buying is higher.

00:01:54.530 --> 00:01:59.839
So with h, the output would go up linearly.

00:01:59.839 --> 00:02:02.840
So that is the RL2 activation that is typically

00:02:02.840 --> 00:02:06.740
applied in the hidden layer of neural network.

00:02:06.740 --> 00:02:12.995
Softmax activation function is applied in the output layer of a classification model.

00:02:12.995 --> 00:02:17.255
Softmax gives the probability of the output.

00:02:17.255 --> 00:02:19.010
So it might read,

00:02:19.010 --> 00:02:21.200
let's say age multiplied by weight,

00:02:21.200 --> 00:02:22.610
salary multiplied by weight.

00:02:22.610 --> 00:02:27.395
But the output would be the probability if you apply the softmax activation function,

00:02:27.395 --> 00:02:32.180
whichever class has, the higher probability that class is the predicted class.

00:02:32.180 --> 00:02:35.180
It could be whether the customer is going to buy or not,

00:02:35.180 --> 00:02:36.665
whether it's a cat or dog.

00:02:36.665 --> 00:02:39.290
And it can be applied to more than two classes.

00:02:39.290 --> 00:02:45.440
Also, generally, softmaxes used with cross entropy loss calculation.

00:02:45.440 --> 00:02:50.045
Crossing defines how different two distributions are,

00:02:50.045 --> 00:02:53.884
the predicted distribution and the actual distribution.

00:02:53.884 --> 00:02:59.330
Low cross entropy means the predicted and the actual values are in sync.

00:02:59.330 --> 00:03:01.520
And the classifier tries to minimize

00:03:01.520 --> 00:03:04.835
the difference between predicted value and the actual value.

00:03:04.835 --> 00:03:11.360
Applying softmax activation function and cross entropy loss calculation.

00:03:11.360 --> 00:03:13.520
In a neural network,

00:03:13.520 --> 00:03:17.180
input is received from the input layer with its sum is calculated.

00:03:17.180 --> 00:03:19.640
An activation function is applied in

00:03:19.640 --> 00:03:22.970
all the neurons and the output is passed to the next layer.

00:03:22.970 --> 00:03:24.365
Again, in the next layer,

00:03:24.365 --> 00:03:25.970
weighted sum is calculated,

00:03:25.970 --> 00:03:28.535
an activation function is applied.

00:03:28.535 --> 00:03:31.595
Put is passed to the next layer and so on.

00:03:31.595 --> 00:03:33.950
And finally, we get the output.

00:03:33.950 --> 00:03:40.760
It could be probability for classification models or it could be the actual value.

00:03:40.760 --> 00:03:42.635
In a regression model.

00:03:42.635 --> 00:03:46.580
This process repeats several times until the loss is

00:03:46.580 --> 00:03:50.585
minimized and the data moves end-to-end from input to the output layer.

00:03:50.585 --> 00:03:52.190
It's called one epoch.

00:03:52.190 --> 00:03:57.950
We can define multiple epoxy until we get certain desired accuracy.

00:03:57.950 --> 00:04:00.095
For a classification model,

00:04:00.095 --> 00:04:06.090
we use cross entropy loss minimization technique to adjust the weights.

00:04:06.550 --> 00:04:12.890
This arrow denotes the feedback loop or backpropagation neural network.

00:04:12.890 --> 00:04:16.070
The loss is passed to the input layer so

00:04:16.070 --> 00:04:19.520
that the weights can be adjusted to minimize the loss.

00:04:19.520 --> 00:04:23.900
So this is our deep learning artificial neural network Lance.

00:04:23.900 --> 00:04:26.400
By adjusting the weights.

00:04:27.130 --> 00:04:29.960
With more number of epochs,

00:04:29.960 --> 00:04:33.170
the accuracy goes up and the loss gets minimized.

00:04:33.170 --> 00:04:34.640
Because with each epoch,

00:04:34.640 --> 00:04:37.295
the neurons learn something new about the data.

00:04:37.295 --> 00:04:41.255
And based on that, they keep on adjusting the weights.

00:04:41.255 --> 00:04:46.385
There are several deep learning libraries available to construct neural network,

00:04:46.385 --> 00:04:48.680
TensorFlow carriage, biter chart,

00:04:48.680 --> 00:04:50.620
some of the popular liabilities.

00:04:50.620 --> 00:04:52.890
The time of this recording.
