WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.620 --> 00:00:06.690
Let's understand how to create a rest API for the TensorFlow model that we just created.

00:00:06.690 --> 00:00:11.470
We can deploy a TensorFlow model using tensorflow model server.

00:00:12.410 --> 00:00:17.370
It can read the model file that we generated in protocol format and

00:00:17.370 --> 00:00:21.660
expose the model is erased APA or over g RPC,

00:00:21.660 --> 00:00:24.340
that is Google Remote Procedure Call

00:00:25.280 --> 00:00:29.340
will focus on how to create a risk APA for the model.

00:00:29.340 --> 00:00:31.120
In this lab.

00:00:33.170 --> 00:00:35.655
We will go to Google Cloud,

00:00:35.655 --> 00:00:37.575
create a virtual server,

00:00:37.575 --> 00:00:42.460
then installed Docker, and using docker will instruct the model server.

00:00:43.620 --> 00:00:48.220
We'll deploy the model on the VM in the dark raise environment,

00:00:48.220 --> 00:00:53.215
then clear the rest API using which will access the model from Google collab environment.

00:00:53.215 --> 00:00:55.550
Let's see that in action,

00:00:56.430 --> 00:00:59.230
go to GCP Console.

00:00:59.230 --> 00:01:03.320
Search for VM instance are Compute Engine.

00:01:03.810 --> 00:01:06.850
Click on the VM instance.

00:01:06.850 --> 00:01:09.040
Click Create.

00:01:09.040 --> 00:01:13.610
Let's let the VM with aid CPU and target USB memory.

00:01:15.600 --> 00:01:20.240
And this time we'll select the o12 instance.

00:01:24.550 --> 00:01:29.630
Let's select one to 18.400 GB

00:01:29.630 --> 00:01:38.730
space will allow us to DPS to DBS axis.

00:01:39.820 --> 00:01:48.900
Click cleared. It will take few seconds for the VM to get created.

00:01:51.400 --> 00:01:55.760
Elliptic go and open all the ports as shown earlier.

00:01:55.760 --> 00:01:58.820
Because we'll be creating a rest API at

00:01:58.820 --> 00:02:02.375
a particular port and that port should be accessible to as shown earlier.

00:02:02.375 --> 00:02:06.180
Make sure all the ports are open for this VM.

00:02:07.060 --> 00:02:10.170
Click on SSH.

00:02:11.770 --> 00:02:18.330
Let's do sudo apt-get update to ensure all packages that up-to-date.

00:02:19.720 --> 00:02:26.040
Next, we'll insert Docker with sudo apt-get install Docker command.

00:02:27.850 --> 00:02:31.110
Then fire this command.

00:02:34.420 --> 00:02:38.430
After that update all the packages again.

00:02:42.310 --> 00:02:45.870
Then they live it to root two j.

00:02:46.780 --> 00:02:50.880
At this point your Docker should be installed.

00:02:51.120 --> 00:02:55.550
Check the Docker version using Docker wasn't command.

00:02:55.890 --> 00:03:00.625
Then run a docker HelloWorld program to ensure it is installed correctly.

00:03:00.625 --> 00:03:04.400
You should see this message hello Chrome Docker.

00:03:07.140 --> 00:03:10.195
Now using docker pull tensorflow serving,

00:03:10.195 --> 00:03:13.250
you can install that TensorFlow models over.

00:03:13.890 --> 00:03:20.980
After that, let's get the model that we created earlier to this environment.

00:03:20.980 --> 00:03:29.420
Customer module.js will install unzip to unzip this file.

00:03:30.180 --> 00:03:32.485
Next, unzip the file,

00:03:32.485 --> 00:03:33.990
and we can see the

00:03:33.990 --> 00:03:42.750
Customer model here, and that we can see the protocol file.

00:03:44.110 --> 00:03:49.830
So the model has been copied to the Google VM environment.

00:03:51.610 --> 00:03:55.640
Next to start the TensorFlow model server,

00:03:55.640 --> 00:03:58.820
you'll have to specify the path of the model file which is

00:03:58.820 --> 00:04:01.820
present working directory slash customer behavior model.

00:04:01.820 --> 00:04:09.650
In this case, any target directory which is modeled plus the model name.

00:04:09.650 --> 00:04:12.690
And you have to specify the model name.

00:04:14.080 --> 00:04:17.970
Then TensorFlow serving.

00:04:19.300 --> 00:04:22.640
And the default port for risk.

00:04:22.640 --> 00:04:25.440
Epa's 8x 501.

00:04:28.240 --> 00:04:35.820
This PAF model server will expose the Martin lazy rest API at port 8501.

00:04:36.730 --> 00:04:39.635
This fire, this command.

00:04:39.635 --> 00:04:42.870
And USDA EPA would be ready.

00:04:44.830 --> 00:04:50.750
Let us now understand how to access this model from the Google collab environment.

00:04:50.750 --> 00:04:54.035
Will go to collaborate research dot google.com,

00:04:54.035 --> 00:04:58.730
and create a new Python three notebook will give it a name.

00:04:58.730 --> 00:05:01.505
Use TF models serving.

00:05:01.505 --> 00:05:04.190
Now we create a risk client and it would

00:05:04.190 --> 00:05:07.500
be similar to the steps that we have seen earlier.

00:05:08.920 --> 00:05:12.935
We have to import JSON request.

00:05:12.935 --> 00:05:17.885
Let's import numpy and will get go a seeds.rb file

00:05:17.885 --> 00:05:23.550
to do the standard scaling for the input data will import piccolo also.

00:05:26.470 --> 00:05:29.885
Now let's lower the standards killer pickle.

00:05:29.885 --> 00:05:31.475
We're variable.

00:05:31.475 --> 00:05:33.830
Before we call the TensorFlow models,

00:05:33.830 --> 00:05:37.320
having APA you leapt to standard skilled or data.

00:05:40.750 --> 00:05:48.740
We'll copy the public IP of the Google Cloud VM instance and create a URL variable.

00:05:48.740 --> 00:05:51.575
It should be V1 because we are only worried someone

00:05:51.575 --> 00:05:55.530
that model class the model name colon predict.

00:05:57.670 --> 00:06:01.710
This is the end point for the PAF model server.

00:06:03.250 --> 00:06:05.810
Now we can send the retinal list.

00:06:05.810 --> 00:06:10.650
We are sending data for both the set h 2040 thousand, which is this.

00:06:11.710 --> 00:06:15.750
And there's 42 thousand.

00:06:17.920 --> 00:06:22.860
You can send it up for one set of input parameters also.

00:06:24.490 --> 00:06:27.020
So you've created a JSON object,

00:06:27.020 --> 00:06:29.760
let's use that to post-trip.

00:06:31.150 --> 00:06:34.280
These are similar steps which you have done earlier,

00:06:34.280 --> 00:06:37.260
just that the endpoint is different this time.

00:06:39.490 --> 00:06:43.830
And after that, we can now print output.

00:06:45.220 --> 00:06:50.430
We can see output for both the input datasets.

00:06:52.000 --> 00:06:55.655
For the first one for age 20 and salary 40 thousand,

00:06:55.655 --> 00:06:57.965
the probability is 0.4, which is very low.

00:06:57.965 --> 00:07:04.410
And for the second one is 42 and salary 50 thousand the probabilities very I 0.98.

00:07:06.070 --> 00:07:09.605
So this is how you can create a rest API

00:07:09.605 --> 00:07:13.350
for your TensorFlow model using tensorflow model server.
