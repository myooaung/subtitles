WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:01.010 --> 00:00:05.940
Storing models in a database is a common practice in the real world.

00:00:05.940 --> 00:00:10.425
Data scientists can create the models converted to pick color binary format,

00:00:10.425 --> 00:00:12.840
and they can store it in a database from where

00:00:12.840 --> 00:00:16.785
other applications can access the model and use it for prediction.

00:00:16.785 --> 00:00:22.215
Let's see an example of how to store and retrieve models from a database.

00:00:22.215 --> 00:00:25.200
There are various ways we can try this.

00:00:25.200 --> 00:00:27.390
One of the easiest ways is to create

00:00:27.390 --> 00:00:32.230
a Postgres SQL Database on Google Colab and then use that as a model store.

00:00:32.230 --> 00:00:34.685
Let's see how that works.

00:00:34.685 --> 00:00:39.390
We'll go to Google Colab and create a new notebook.

00:00:40.270 --> 00:00:44.280
Let's call it ML models in dB.

00:00:45.550 --> 00:00:49.535
First, let's install Postgres equal on Google Colab.

00:00:49.535 --> 00:00:53.060
Colab is a Linux environment and we can execute postgres

00:00:53.060 --> 00:00:57.590
SQL Linux installation commands to install Postgres in this notebook.

00:00:57.590 --> 00:01:01.100
First, we'll make sure all the packages are up to date.

00:01:01.100 --> 00:01:04.040
This is a Linux command which will make sure

00:01:04.040 --> 00:01:08.700
all the libraries or packages on the Linux environment is up-to-date.

00:01:09.100 --> 00:01:11.960
Then to install Postgres sequence,

00:01:11.960 --> 00:01:14.280
we'll execute this command.

00:01:15.070 --> 00:01:17.540
Now installation is complete.

00:01:17.540 --> 00:01:20.850
Let's start the Postgres equal service.

00:01:22.420 --> 00:01:27.330
Postgresql database server is started on this notebook.

00:01:28.150 --> 00:01:33.960
Will alter the Postgres sequel user password to Postgres.

00:01:34.750 --> 00:01:37.880
By default will get a user id postgres,

00:01:37.880 --> 00:01:40.250
and we can set the password to the password.

00:01:40.250 --> 00:01:42.665
I've set it to Postgres.

00:01:42.665 --> 00:01:45.470
Next we'll create a database Future x,

00:01:45.470 --> 00:01:49.009
but before that, we'll drop any existing database.

00:01:49.009 --> 00:01:52.895
Using this command, you can execute any SQL query.

00:01:52.895 --> 00:01:54.770
So let's try it out.

00:01:54.770 --> 00:01:57.845
It says the database doesn't exist. That is okay.

00:01:57.845 --> 00:01:59.840
We can create it, but in future,

00:01:59.840 --> 00:02:01.100
if you have to read on this notebook,

00:02:01.100 --> 00:02:04.145
it will drop any existing database and then create a new one.

00:02:04.145 --> 00:02:07.590
Now we'll create a new database Future x.

00:02:10.690 --> 00:02:14.735
Next we'll create a table which will store all the models.

00:02:14.735 --> 00:02:16.895
The easiest way to do that is

00:02:16.895 --> 00:02:21.690
Habit text file with the create statement and then executed from Columbia.

00:02:22.660 --> 00:02:27.350
We have a simple text file which contains one create statement.

00:02:27.350 --> 00:02:29.330
And before that there is a drop statement.

00:02:29.330 --> 00:02:32.075
Also, if the table already exists, it will drop it.

00:02:32.075 --> 00:02:35.240
And it'll create a table future ex model catalog,

00:02:35.240 --> 00:02:36.965
which you'll have three fields.

00:02:36.965 --> 00:02:39.140
Model ID, modeled name,

00:02:39.140 --> 00:02:40.565
and the model file,

00:02:40.565 --> 00:02:42.530
which will store the actual model.

00:02:42.530 --> 00:02:44.540
So we have kept the modal ideas.

00:02:44.540 --> 00:02:46.790
Integer model name is varchar,

00:02:46.790 --> 00:02:48.725
that can store any string.

00:02:48.725 --> 00:02:51.230
And the model file format is byte.

00:02:51.230 --> 00:02:54.320
That is the format that we'll use to store to the pickle file.

00:02:54.320 --> 00:02:59.030
Now we need to upload this create model SQL file to collab and executed.

00:02:59.030 --> 00:03:06.680
So let's do that. Click on this icon and then upload the file.

00:03:06.680 --> 00:03:08.970
Will upload the file create models,

00:03:08.970 --> 00:03:11.550
table SQL, which we just saw.

00:03:14.260 --> 00:03:16.970
The SQL file has been uploaded.

00:03:16.970 --> 00:03:18.665
Now to execute the file,

00:03:18.665 --> 00:03:23.915
we need to connect to the database and then specify the file limb.

00:03:23.915 --> 00:03:26.090
So here local host is the host,

00:03:26.090 --> 00:03:28.325
which is the host for the server.

00:03:28.325 --> 00:03:31.550
This is the local host for the Colab environment.

00:03:31.550 --> 00:03:34.820
Then we are connecting by specifying the port number 5, 4, 3, 2,

00:03:34.820 --> 00:03:36.185
which is the default port,

00:03:36.185 --> 00:03:38.375
password and the database name,

00:03:38.375 --> 00:03:40.580
and then calling the script.

00:03:40.580 --> 00:03:49.100
Let's execute this. So this is fine.

00:03:49.100 --> 00:03:51.800
It failed to drop it because the table doesn't exist,

00:03:51.800 --> 00:03:54.270
but it would have created the table.

00:03:55.120 --> 00:04:00.270
Our scripted one drop statement and then one create statement.

00:04:00.490 --> 00:04:04.160
Let's now check if the table got created or not.

00:04:04.160 --> 00:04:09.620
There are various libraries available to interact with Postgres database from Python.

00:04:09.620 --> 00:04:13.340
We'll use psycopg2, which is one of the libraries

00:04:13.340 --> 00:04:17.610
or packages to work with PostgreSQL from Python.

00:04:17.860 --> 00:04:20.945
Import that in the Colab environment,

00:04:20.945 --> 00:04:23.600
psycopg2 is already available in other environment.

00:04:23.600 --> 00:04:27.090
You might have to install it by doing pip install.

00:04:27.910 --> 00:04:33.020
Then we need to connect to the database using psychopg2 connect method.

00:04:33.020 --> 00:04:35.585
You need to specify the user AD,

00:04:35.585 --> 00:04:40.020
password, the host name, and the database name.

00:04:42.070 --> 00:04:45.215
Now a connection has been established.

00:04:45.215 --> 00:04:47.840
Next, we need to create a cursor.

00:04:47.840 --> 00:04:51.515
Using coarser, we can execute different queries.

00:04:51.515 --> 00:04:57.240
Let's write a simple select query to get all the records from the model catalog table.

00:04:57.400 --> 00:05:02.280
Coursera is an execute method using which we can execute any query.

00:05:03.880 --> 00:05:08.705
Coursera's various methods to retrieve one or multiple decades.

00:05:08.705 --> 00:05:11.840
Let's do concert dot-dot-dot and get

00:05:11.840 --> 00:05:16.230
all the records and store it in a variable called models.

00:05:17.200 --> 00:05:20.930
Currently, because you've created a model catalog table,

00:05:20.930 --> 00:05:23.225
but we have not stored in the model.

00:05:23.225 --> 00:05:27.210
Let's now create a model in this Colab notebook.

00:05:28.330 --> 00:05:31.700
We will use the same code that we tried earlier

00:05:31.700 --> 00:05:34.730
got to build a classifier using KNN technique.

00:05:34.730 --> 00:05:38.400
It would read data from store purchase CSV,

00:05:38.860 --> 00:05:41.420
and then create a model.

00:05:41.420 --> 00:05:45.500
We need stored particular CSV in this Colab notebook.

00:05:45.500 --> 00:05:47.550
Let's upload that.

00:05:53.290 --> 00:05:55.550
So it is done now.

00:05:55.550 --> 00:05:57.980
We'll run this code to create a model.

00:05:57.980 --> 00:06:00.840
This is the same code that we have seen earlier.

00:06:02.370 --> 00:06:05.350
Now the classifier has been created and we

00:06:05.350 --> 00:06:08.230
also use the standard scaler to scale the data.

00:06:08.230 --> 00:06:13.130
We can try to predict using the model to ensure everything went okay.

00:06:13.200 --> 00:06:18.530
Accuracy is 0.875, so it ran fine.

00:06:19.650 --> 00:06:22.870
Next, we need to store the classifier and

00:06:22.870 --> 00:06:26.665
the standard scaler in the Postgres database model catalog table.

00:06:26.665 --> 00:06:32.425
Using pickle will convert the classifier and standard scaler to binary string format.

00:06:32.425 --> 00:06:34.990
This time will not be writing it to a file.

00:06:34.990 --> 00:06:39.355
Instead, we'll store the binary string in local variables,

00:06:39.355 --> 00:06:44.660
pickled classifier string and pickled standard scaler stream. Let's execute it.

00:06:46.380 --> 00:06:50.365
Now we need to store this pickle strings in the modal catalog.

00:06:50.365 --> 00:06:54.880
They will impose grids, will

00:06:54.880 --> 00:06:59.410
create an insert statement which will populate into the model catalog table.

00:06:59.410 --> 00:07:01.375
It has three fields, ID,

00:07:01.375 --> 00:07:05.035
name, and the gel model binary string.

00:07:05.035 --> 00:07:08.630
Next we'll pass the values using a tuple.

00:07:09.060 --> 00:07:13.645
For the classifier will use model ideas one name as classifier,

00:07:13.645 --> 00:07:16.045
and then we'll specify the classifier string.

00:07:16.045 --> 00:07:18.460
Next we create a cursor.

00:07:18.460 --> 00:07:21.460
Next we'll execute the query by calling

00:07:21.460 --> 00:07:25.820
cursor dot execute and then passing the insert statement and the tuple.

00:07:26.380 --> 00:07:30.350
Let's do the same thing for the standard scaler.

00:07:30.350 --> 00:07:38.510
This time we'll use modal ideas to name as C and then specify the standard scaler string.

00:07:38.510 --> 00:07:42.240
Will execute the insert statement again.

00:07:43.390 --> 00:07:48.330
And finally, we'll close the cursor and commit the connection.

00:07:50.950 --> 00:07:53.990
Now the models had been stored here,

00:07:53.990 --> 00:07:56.570
the standard scaler and the classifier been stored here.

00:07:56.570 --> 00:08:01.880
Any application which has access to this database can retrieve the models and use them.

00:08:01.880 --> 00:08:06.110
What prediction will retrieve

00:08:06.110 --> 00:08:10.290
the models from the Postgres table in the same Colab notebook.

00:08:10.770 --> 00:08:13.360
First we create a cursor.

00:08:13.360 --> 00:08:16.540
Then we'll write a select query to select all the models from

00:08:16.540 --> 00:08:20.995
the model catalog and then execute it and store the output in a model's variable.

00:08:20.995 --> 00:08:22.330
Let's do that.

00:08:22.330 --> 00:08:25.765
Now we can see that classifier and standards killers,

00:08:25.765 --> 00:08:29.210
they're getting phased from the Postgres table.

00:08:32.190 --> 00:08:37.760
So this is our any application can access the models from a table.

00:08:38.190 --> 00:08:41.440
In a real-world scenario that would be shared database

00:08:41.440 --> 00:08:45.530
which data scientist and other applications can access.

00:08:45.870 --> 00:08:48.804
Models is a list of tuples.

00:08:48.804 --> 00:08:52.670
Now we have two elements and each element is a tuple.

00:08:53.170 --> 00:08:58.565
Get the classifier from the first element of the list, that is index 0.

00:08:58.565 --> 00:09:01.505
And the third element of the tuple that is index two.

00:09:01.505 --> 00:09:06.245
Similarly get the scalar from the models by specifying one,

00:09:06.245 --> 00:09:07.880
which is the second element of the list,

00:09:07.880 --> 00:09:12.180
and then two, which is the third element of the tuple.

00:09:12.190 --> 00:09:14.465
Using pickled got loads,

00:09:14.465 --> 00:09:19.260
we can read the binary string and store the data and local variables.

00:09:20.890 --> 00:09:23.870
Now you've classified and scalar objects

00:09:23.870 --> 00:09:26.810
retrieved from the database and stored in local variable.

00:09:26.810 --> 00:09:29.790
And we can use this to predict,

00:09:30.790 --> 00:09:35.554
let's predict for age 40 and salary 20000.

00:09:35.554 --> 00:09:38.780
Using the same technique that we have done earlier.

00:09:38.780 --> 00:09:43.580
This time we are using classifier from dv and then scalar from dv.

00:09:43.580 --> 00:09:51.154
It works fine. Similarly, let's predict for age 42 and salary 50000.

00:09:51.154 --> 00:09:53.870
It gives expected results.

00:09:53.870 --> 00:09:57.050
This is our machine-learning models can be stored in

00:09:57.050 --> 00:10:00.890
a database and retrieved and used in other applications.

00:10:00.890 --> 00:10:03.095
Instead of storing the models,

00:10:03.095 --> 00:10:05.660
you might decide to store the prediction so that

00:10:05.660 --> 00:10:09.440
other application did not run the models that can directly take the prediction.

00:10:09.440 --> 00:10:11.945
It could be probability or whatever your model is,

00:10:11.945 --> 00:10:15.120
giving us an output, and then use that.
