WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.865
We have the store purchase data.

00:00:02.865 --> 00:00:06.300
We have data for different customers.

00:00:06.300 --> 00:00:10.335
There is in their salary and whether they purchase or not.

00:00:10.335 --> 00:00:12.765
Based on this data,

00:00:12.765 --> 00:00:16.125
we'll build a machine learning classification model,

00:00:16.125 --> 00:00:22.870
which will predict whether a new customer with a certain age and salary would buy or not.

00:00:24.440 --> 00:00:29.355
So in this is in salary or independent variables.

00:00:29.355 --> 00:00:33.375
We'll build a machine learning classification model using kNN,

00:00:33.375 --> 00:00:36.900
which will get rid with distort parties data.

00:00:36.900 --> 00:00:41.885
Let's understand k-nearest neighbor or k-NN machine-learning algorithm

00:00:41.885 --> 00:00:44.015
through a very simple example.

00:00:44.015 --> 00:00:49.835
Imagine we have cats and dogs shown in this diagram.

00:00:49.835 --> 00:00:54.425
On the x-axis we have weight and on the y-axis we have height.

00:00:54.425 --> 00:00:58.490
All the green ones are cats because obviously

00:00:58.490 --> 00:01:03.575
they would have less weight and laicite and all the blue ones are dogs.

00:01:03.575 --> 00:01:07.715
And if we know the height and weight of a new animal,

00:01:07.715 --> 00:01:09.875
let's say this new one in the center.

00:01:09.875 --> 00:01:13.020
Can we predict whether it's a cat or dog?

00:01:13.540 --> 00:01:17.135
Knn algorithm? Besides on that,

00:01:17.135 --> 00:01:21.050
based on the characteristics of the nearest neighbors.

00:01:21.050 --> 00:01:23.420
Little k value is five.

00:01:23.420 --> 00:01:25.970
We look at the five nearest neighbors can,

00:01:25.970 --> 00:01:31.265
based on that, we decide which class the animal could be lumped two.

00:01:31.265 --> 00:01:33.620
For example, in this case,

00:01:33.620 --> 00:01:35.825
there are three greens and two blues.

00:01:35.825 --> 00:01:38.825
That means there are three cats and

00:01:38.825 --> 00:01:43.025
dogs who have similar characteristics as the new animal.

00:01:43.025 --> 00:01:46.730
So this AnyVal is more likely to be a cat because

00:01:46.730 --> 00:01:52.280
the majority of the animals belong to the cat class in the nearest neighbourhood.

00:01:52.280 --> 00:01:56.660
So this is k nearest neighbor technique where outcome

00:01:56.660 --> 00:02:00.650
is predicted based on characteristics shown by the nearest neighbors.

00:02:00.650 --> 00:02:03.275
And the kava Louis typically five.

00:02:03.275 --> 00:02:07.685
Let's apply this technique on the store purchase data.

00:02:07.685 --> 00:02:11.165
We have the data in the project folder.

00:02:11.165 --> 00:02:16.520
We can spidery up to select your project folder here.

00:02:16.520 --> 00:02:21.320
And then we can go to files and see all the source code and files.

00:02:21.320 --> 00:02:24.185
So this is the stored purchase data we have

00:02:24.185 --> 00:02:28.070
using which will build a machine learning classification model.

00:02:28.070 --> 00:02:31.260
Let's create a new Python file.

00:02:33.490 --> 00:02:38.220
Will nematodes ML Pipeline.

00:02:45.190 --> 00:02:49.505
We'll import the standard libraries.

00:02:49.505 --> 00:02:53.360
We're assuming you are familiar with NumPy and pandas,

00:02:53.360 --> 00:02:56.280
which is a prerequisite for this course.

00:02:59.050 --> 00:03:04.595
In spider, as soon as you type you get all the errors or warnings.

00:03:04.595 --> 00:03:08.165
It saying we are not using Numpy pandas, that's fine.

00:03:08.165 --> 00:03:11.760
We'll be writing the code for the same shortly.

00:03:12.610 --> 00:03:18.840
Now let's load the store purchase data to a Pandas DataFrame.

00:03:19.000 --> 00:03:21.380
We live it training data,

00:03:21.380 --> 00:03:24.725
dataframe, which will store the store purchase data.

00:03:24.725 --> 00:03:27.695
Note that will not be cleaning with the entire data.

00:03:27.695 --> 00:03:31.415
We'll have some records for training and for testing, which we'll see next.

00:03:31.415 --> 00:03:38.220
But training data pandas DataFrame would store the entire CSV file data.

00:03:39.460 --> 00:03:43.625
You can run the entire file by selecting the cycle,

00:03:43.625 --> 00:03:46.220
or you can run the selection.

00:03:46.220 --> 00:03:48.810
Let's run the selection.

00:03:49.390 --> 00:03:52.355
You can go to variable explorer,

00:03:52.355 --> 00:03:55.640
click on cleaning data and we can see that is

00:03:55.640 --> 00:04:00.455
salary purchases have been loaded to the training data dataframe.

00:04:00.455 --> 00:04:05.310
Let's get some statistical info, boat cleaning data.

00:04:08.830 --> 00:04:13.610
We can see various statistical information about the data.

00:04:13.610 --> 00:04:17.285
How many records? We have, 40 records.

00:04:17.285 --> 00:04:19.145
We can see the mean,

00:04:19.145 --> 00:04:23.000
standard deviation and some other statistics about

00:04:23.000 --> 00:04:27.980
the data will store the independent variables in an IRA.

00:04:27.980 --> 00:04:33.605
Will take rose up to the last column and stored them in a dependent variable X,

00:04:33.605 --> 00:04:37.670
which is a NumPy array. Let's do that.

00:04:37.670 --> 00:04:41.030
So this should populate agents salaried.

00:04:41.030 --> 00:04:49.550
Next. Let's go to

00:04:49.550 --> 00:04:52.370
variable explorer and checkout.

00:04:52.370 --> 00:04:56.165
We can see that agent salary have now

00:04:56.165 --> 00:05:01.835
populated in NumPy array will populate the purchase column,

00:05:01.835 --> 00:05:06.740
which is the prediction to and at the Numpy array away.

00:05:06.740 --> 00:05:15.650
So this should populate the last column and store it in way too.

00:05:15.650 --> 00:05:21.080
This is our y, which is the dependent variable or the one we're trying to predict.

00:05:21.080 --> 00:05:23.915
We have aids in salary and x NumPy array.

00:05:23.915 --> 00:05:27.680
And we have y, which is the purchase data.

00:05:27.680 --> 00:05:29.300
For not purchased.

00:05:29.300 --> 00:05:30.710
One is where parties.

00:05:30.710 --> 00:05:34.235
So that is stored in a Numpy array.

00:05:34.235 --> 00:05:37.670
Now we have the independent variables and dependent

00:05:37.670 --> 00:05:40.835
variables in two separate Numpy arrays.

00:05:40.835 --> 00:05:48.155
Next, using scikit-learn will separate the data into training set and test set.

00:05:48.155 --> 00:05:51.080
And we'll huge 80-20 ratio,

00:05:51.080 --> 00:05:55.310
80% of the data for training and 20% for testing.

00:05:55.310 --> 00:05:59.570
Scikit-learn is a very popular library for machine learning using Python.

00:05:59.570 --> 00:06:02.960
Scikit-learn comes pre-installed with Anaconda spider.

00:06:02.960 --> 00:06:05.660
If I'm using a different Python environment,

00:06:05.660 --> 00:06:08.840
you might have to install scikit-learn using pip install

00:06:08.840 --> 00:06:13.850
SKLearn style is the command to install any Python libraries.

00:06:13.850 --> 00:06:18.830
Anaconda spider comes with scikit-learn, numpy, pandas,

00:06:18.830 --> 00:06:20.750
and many other libraries that are

00:06:20.750 --> 00:06:24.230
required for scientific competition and machine learning.

00:06:24.230 --> 00:06:26.780
We're using scikit-learn, train,

00:06:26.780 --> 00:06:31.655
test split class to split the dataset into two parts.

00:06:31.655 --> 00:06:33.695
Now once we do this,

00:06:33.695 --> 00:06:35.990
we let the training set and the test set.

00:06:35.990 --> 00:06:39.020
The training set will have 32 records.

00:06:39.020 --> 00:06:42.350
We said 80% data will be used for training.

00:06:42.350 --> 00:06:47.430
So we've totaled 40 records of which 32 will be used for cleaning.

00:06:48.010 --> 00:06:51.150
So this is extreme.

00:06:51.400 --> 00:06:57.420
And weight train 32 records for trading.

00:06:59.530 --> 00:07:04.249
And x-test has heat records.

00:07:04.249 --> 00:07:07.175
Similarly weight this will have eight records.

00:07:07.175 --> 00:07:09.890
This is the data for testing the model.

00:07:09.890 --> 00:07:12.320
Next, we'll feature skill that data.

00:07:12.320 --> 00:07:16.040
So that is it, salary are in the same bridge

00:07:16.040 --> 00:07:20.120
and the machine learning model could not get influenced by salary,

00:07:20.120 --> 00:07:22.040
which is in a higher range.

00:07:22.040 --> 00:07:29.150
Let's run this. Now we can see the scale data.

00:07:29.150 --> 00:07:32.450
Standard scaler distributes the data in

00:07:32.450 --> 00:07:36.200
a way so that the mean is 0 and standard deviation is one.

00:07:36.200 --> 00:07:39.725
Now both the A's and salary or in the same bridge.

00:07:39.725 --> 00:07:46.760
Next, we'll build a classification model using the K nearest neighbor technique.

00:07:48.090 --> 00:07:50.770
Will have five neighbors.

00:07:50.770 --> 00:07:53.425
We lose the Minkowski metrics.

00:07:53.425 --> 00:07:55.495
To build this classifier.

00:07:55.495 --> 00:08:01.180
Minkowski metrics works based on the Euclidian distance between two points.

00:08:01.180 --> 00:08:05.455
Euclidean distance is nothing but the shortest distance between two points.

00:08:05.455 --> 00:08:09.220
That's how it decides which neighbors are the nearest.

00:08:09.220 --> 00:08:14.305
Next will fit the training data to the classifier to clean it.

00:08:14.305 --> 00:08:18.530
So this is where the model is getting drained.

00:08:25.710 --> 00:08:31.975
This is the classifier object which is been trained with certain cleaning data,

00:08:31.975 --> 00:08:34.825
which is, is it salary is the input variable,

00:08:34.825 --> 00:08:37.585
head purchases the output variable.

00:08:37.585 --> 00:08:40.165
The classifier is our model.

00:08:40.165 --> 00:08:42.820
Will quickly check the accuracy of

00:08:42.820 --> 00:08:47.305
the classifier by trying to predict. For the test data.

00:08:47.305 --> 00:08:50.320
Classifier has a predict method which takes

00:08:50.320 --> 00:08:55.820
a Numpy arrays input and returns as output in another number.

00:09:00.840 --> 00:09:06.870
So this is our x and this is the weight.

00:09:06.870 --> 00:09:10.190
And let's see what is the prediction.

00:09:10.190 --> 00:09:15.095
Wavelet six set for one record.

00:09:15.095 --> 00:09:17.555
The model predicted accurately.

00:09:17.555 --> 00:09:19.370
For all of the records.

00:09:19.370 --> 00:09:25.080
We can also check the probability of prediction for all the test data.

00:09:34.720 --> 00:09:40.025
Here we can see that wherever we have more than 0.5 probability,

00:09:40.025 --> 00:09:46.230
the model is predicting that the customer owed by the customer would not buy.

00:09:47.530 --> 00:09:52.220
Mobility is helpful when he loved to sort data from

00:09:52.220 --> 00:09:57.330
the prediction and the customers were more likely to purchase.

00:09:57.370 --> 00:10:05.090
The history. The third one is more likely to purchase because the probabilities 0.8 or

00:10:05.090 --> 00:10:12.960
80% will check the accuracy of the model using Confusion Matrix.

00:10:16.630 --> 00:10:20.210
Confusion Matrix is a statistical technique to

00:10:20.210 --> 00:10:23.285
predict it courtesy of a classification model.

00:10:23.285 --> 00:10:25.385
The way it works is pretty simple.

00:10:25.385 --> 00:10:30.960
If the actual value is one and the model predicted one PRINCE2 project.

00:10:30.960 --> 00:10:35.335
We lose 10, it's false-negative.

00:10:35.335 --> 00:10:40.120
Similarly, 00 is true negative and 01 is false positive.

00:10:40.120 --> 00:10:43.300
It can also be represented in this format.

00:10:43.300 --> 00:10:45.490
So once we know all four types,

00:10:45.490 --> 00:10:48.320
we can easily determine the accuracy.

00:10:48.420 --> 00:10:52.915
So they couldn't see is true positive plus true negative divided way.

00:10:52.915 --> 00:10:55.315
All four types of predictions.

00:10:55.315 --> 00:10:58.660
No matter which classification technique you are using,

00:10:58.660 --> 00:11:04.700
kNN or any other Confusion Matrix can be used to calculate the accuracy of the model.

00:11:04.860 --> 00:11:08.875
Cyclic learn and other machine learning libraries.

00:11:08.875 --> 00:11:14.900
The built-in classes to Jen bit confusion matrix permit Julian predicted data.

00:11:15.630 --> 00:11:21.265
Let's create the confusion metrics will pass the actual value of the test set,

00:11:21.265 --> 00:11:23.950
that is weight test and the predicted values,

00:11:23.950 --> 00:11:25.255
that is white bread.

00:11:25.255 --> 00:11:30.565
And get the confusion metrics from the cyclic land confusion matrix class.

00:11:30.565 --> 00:11:34.660
Go to spider variable explorer.

00:11:34.660 --> 00:11:38.150
And we can see the confusion matrix over here.

00:11:38.700 --> 00:11:41.935
We have three true negatives.

00:11:41.935 --> 00:11:44.140
For true positives.

00:11:44.140 --> 00:11:49.100
Only one false negative and false positive.

00:11:49.380 --> 00:11:52.405
So this model is very good it,

00:11:52.405 --> 00:11:57.710
because we have only one false positive or negative from eight records.

00:11:59.110 --> 00:12:02.970
Let's calculate the accuracy of the model.

00:12:04.930 --> 00:12:10.505
And we'll print the quiescent 0.875.

00:12:10.505 --> 00:12:14.150
So our model is 87.5% occurred.

00:12:14.150 --> 00:12:19.550
So this model can predict whether a customer with a particular agent salary,

00:12:19.550 --> 00:12:24.840
goodbye or not with 87% accuracy.

00:12:26.920 --> 00:12:31.460
You can also get the intact classification report to

00:12:31.460 --> 00:12:36.275
understand more about precision recall and F1 score.

00:12:36.275 --> 00:12:39.800
So we've taken this toward purchase data and created

00:12:39.800 --> 00:12:43.325
a classifier which can predict whether somebody would by R naught.

00:12:43.325 --> 00:12:46.430
That model or classifier can be used to

00:12:46.430 --> 00:12:51.410
predict whether a customer with a particular agents salary would BYOD naught.

00:12:51.410 --> 00:12:55.160
So let's try to predict whether a customer

00:12:55.160 --> 00:12:59.315
with H porter Sal day to day 1000 good biochar.

00:12:59.315 --> 00:13:03.230
Note that this model takes a NumPy array and returns

00:13:03.230 --> 00:13:08.945
an compare Europe to create a Numpy array from agents salary,

00:13:08.945 --> 00:13:10.805
feature skill that data,

00:13:10.805 --> 00:13:13.085
and then feed it to the classifier.

00:13:13.085 --> 00:13:16.820
Because the classifier is trained on feature skill data should have

00:13:16.820 --> 00:13:20.915
been shirt to data you are fitting is also feature scaled.

00:13:20.915 --> 00:13:25.650
Same technique, which is standard scaler In our case.

00:13:27.640 --> 00:13:29.975
And the prediction is 0,

00:13:29.975 --> 00:13:34.340
the customer or not by somebody with age 40.

00:13:34.340 --> 00:13:37.340
And cell D2, D3 budget would not buy is.

00:13:37.340 --> 00:13:44.870
But this model, we can check the probability of the prediction for the same data.

00:13:44.870 --> 00:13:51.779
Classified as a predictor parameters using which you can get the probability.

00:13:55.060 --> 00:13:58.339
So the probability is 0.2 or 20%.

00:13:58.339 --> 00:14:01.650
That's why the model set to customer would not buy.

00:14:03.610 --> 00:14:10.410
Let's try to predict for a customer who is age 42 and salary 50 thousand.

00:14:17.410 --> 00:14:21.185
This time the model set the customer or buyer.

00:14:21.185 --> 00:14:24.060
Let's check out the probability.

00:14:25.900 --> 00:14:31.070
It's 0.880%. So there are 80% chances

00:14:31.070 --> 00:14:35.435
of the customer buying a machine learning model, greedy.

00:14:35.435 --> 00:14:37.460
It's a classification model.

00:14:37.460 --> 00:14:41.675
It can predict whether a customer with a certain agent cell D would by R naught.

00:14:41.675 --> 00:14:45.785
So this is the classifier we have, which is the model,

00:14:45.785 --> 00:14:50.540
and we are fitting data to this model to get output.

00:14:50.540 --> 00:14:54.845
Next will see various model deployment techniques.

00:14:54.845 --> 00:15:00.889
How we can save this model and deploy this model in other environments,

00:15:00.889 --> 00:15:05.039
including some of the cloud provider environments.
