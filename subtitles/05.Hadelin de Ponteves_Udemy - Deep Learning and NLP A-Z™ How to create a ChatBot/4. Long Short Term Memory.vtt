WEBVTT
1
00:00:00.340 --> 00:00:04.680
Hello and welcome back to the course on deep learning today we've got a very exciting tutorial ahead.

2
00:00:04.680 --> 00:00:09.570
We're talking about a long a short term memory or elist PMS for short.

3
00:00:09.720 --> 00:00:10.910
So let's get started.

4
00:00:11.040 --> 00:00:16.730
It's actually going to be quite a saturated trial so we go to our own little plan of attack for today.

5
00:00:16.720 --> 00:00:19.460
Today we're going to first of all look at a bit of history.

6
00:00:19.480 --> 00:00:23.620
So word came from what was the main idea behind it why people invented it.

7
00:00:24.040 --> 00:00:27.410
And then we're going to be talking about Ellis the same architecture.

8
00:00:27.410 --> 00:00:30.630
There's going to be the bulk of our tutorial today so be prepared for that.

9
00:00:30.750 --> 00:00:33.470
And then we're going to have an example walk through.

10
00:00:33.480 --> 00:00:36.120
Hopefully we'll have enough time for that as well.

11
00:00:36.120 --> 00:00:36.640
All right.

12
00:00:36.690 --> 00:00:38.550
So let's get started here.

13
00:00:38.550 --> 00:00:46.230
We've got a problem which we've identified in the previous tutorials we talked about the vanishing gradient

14
00:00:46.230 --> 00:00:47.530
problem.

15
00:00:47.550 --> 00:00:53.550
So in short what happens is as we propagate the error through the network it has to go through the unravelled

16
00:00:53.700 --> 00:01:01.250
temporal loop and as it does that it goes through these layers of neurons which are connected to themselves.

17
00:01:01.350 --> 00:01:06.090
These hidden layers which are connected to themselves and they're connected by the means of a always

18
00:01:06.090 --> 00:01:13.740
called the w w recurrent weight and because the weight is applied many many times on top of itself that

19
00:01:13.740 --> 00:01:20.110
causes the gradient to decline rapidly meaning that the weight of the layers on very far left are a

20
00:01:20.120 --> 00:01:25.590
bit dated much slower than the waist on the other layers on the far right and this creates a domino

21
00:01:25.590 --> 00:01:32.430
effect because the weights of the far left legs are very important because they dictate the outputs

22
00:01:32.430 --> 00:01:36.900
of those layers which are the inputs to the far right layers and therefore the whole training of the

23
00:01:36.900 --> 00:01:37.870
network suffers.

24
00:01:37.950 --> 00:01:40.350
And that is called The problem of the vanishing gradient.

25
00:01:40.500 --> 00:01:47.450
And as a rule of thumb we can see here that if we record small then we have vanishing gradient is large

26
00:01:47.450 --> 00:01:49.660
and we have exploding gradient.

27
00:01:50.010 --> 00:01:54.900
So now how do you solve this problem we talked about we talked about a couple of possible solutions

28
00:01:54.900 --> 00:02:01.260
we talked about clipping the gradient or penalizes the gradient for exploding gradients we talked about

29
00:02:01.590 --> 00:02:09.510
smartly selecting the weights or the ex-state networks which we didn't go into detail on for the vanishing

30
00:02:09.510 --> 00:02:10.080
gradient.

31
00:02:10.200 --> 00:02:15.750
And also we talked about the LACMA for the vanishing brain just if you separate yourself from all this

32
00:02:15.750 --> 00:02:18.490
information on all of this theory and knowledge.

33
00:02:18.720 --> 00:02:20.390
How would you solve a problem like this.

34
00:02:20.410 --> 00:02:22.580
What's the easiest and Fossa solution to.

35
00:02:22.580 --> 00:02:29.600
So if we are w Reyk is in simple terms less than one that we have vanishing gradient if we are w wreck

36
00:02:29.730 --> 00:02:36.170
greater than one we've got exploding green What's the first thing that comes to mind to solve this problem.

37
00:02:36.420 --> 00:02:40.100
Well the first thing that comes to mind is to make W Raechel one.

38
00:02:40.620 --> 00:02:43.770
And that's exactly what was done in the Ellis DM's.

39
00:02:43.770 --> 00:02:49.660
This is a very simplified explanation and there's definitely more to it than just w brekky cause one.

40
00:02:49.920 --> 00:02:55.830
But in general that's the idea and that's all it took.

41
00:02:55.830 --> 00:03:01.020
And when I saw this for the first time I was so excited that you know the solution is so simple it's

42
00:03:01.020 --> 00:03:06.570
really a genius solution that completely gets rid of the vanishing gradient problem.

43
00:03:06.570 --> 00:03:09.960
And so who are the people behind this.

44
00:03:09.960 --> 00:03:18.060
Here are two gentlemen we've talked about Sepah writer and the second person is Jurgen Schmidt Hoeber

45
00:03:18.300 --> 00:03:26.130
who is who was his supervisor during SEPs research or Ph.D..

46
00:03:26.310 --> 00:03:33.840
And so basically they wrote a paper on it in 1987 about Ellis dams and that's when LACMA were introduced

47
00:03:34.200 --> 00:03:35.690
to the world for the first time.

48
00:03:35.730 --> 00:03:36.990
Very exciting topic.

49
00:03:36.990 --> 00:03:39.860
So let's have a look at what they actually are.

50
00:03:40.080 --> 00:03:45.650
So we've got a recurrent neural network right here on ravelled temporal loop.

51
00:03:45.660 --> 00:03:47.120
This is what it looks like.

52
00:03:47.280 --> 00:03:54.930
If you dig in inside the recurrent roll network and right here I want it to do a quick shout out to

53
00:03:55.820 --> 00:03:57.530
Christopher all.

54
00:03:57.660 --> 00:04:02.990
So here is his blog right here a very well-written blog.

55
00:04:03.000 --> 00:04:05.520
Amazing images as you can see.

56
00:04:05.520 --> 00:04:10.620
And we're going to be using the images from this blog in our explanation here so thanks very much to

57
00:04:11.310 --> 00:04:20.270
Christopher for making this publicly available and allowing the use of his images in other works.

58
00:04:20.430 --> 00:04:24.120
So we'll definitely reference this at the end of today's tutorial.

59
00:04:24.600 --> 00:04:27.090
And going back to our presentation.

60
00:04:27.150 --> 00:04:33.050
So here we've got the recurrent neural network and this is what it looks like inside.

61
00:04:33.060 --> 00:04:34.380
And this is where the problem lies.

62
00:04:34.380 --> 00:04:41.100
So this operation that happens here is actually a neural network or operation as we will see further

63
00:04:41.100 --> 00:04:41.750
down.

64
00:04:42.030 --> 00:04:50.520
But simply put as you have outputs coming into your module into this module this operation is applied

65
00:04:50.550 --> 00:04:53.010
and then goes into the next module the pressure is applied and so on.

66
00:04:53.020 --> 00:04:57.630
So as you apply this approach and when you back propagate it goes through all of this and that's where

67
00:04:57.630 --> 00:05:02.750
the weights are applied that's where the wreck is sitting as that weight is applied as it's applied

68
00:05:02.750 --> 00:05:08.570
is applied the gradient vanishes vanishes and vanishes and that means that the weights cannot be updated

69
00:05:08.570 --> 00:05:16.250
properly or fast enough to train the network properly and so the further away you try to look the more

70
00:05:16.670 --> 00:05:19.760
problems you have and the more of the bashin gradient you have.

71
00:05:19.760 --> 00:05:27.680
This is standard Arnon and this is what the T.M. version looks like and I know you might be thinking

72
00:05:28.010 --> 00:05:30.320
that this is super complex.

73
00:05:30.520 --> 00:05:32.080
This There's so much going on here.

74
00:05:32.090 --> 00:05:37.390
And indeed it is a bit more complex than the standard Arnon But check this out.

75
00:05:37.400 --> 00:05:44.690
This is how elist teams are normally displayed in literature and in papers and so on.

76
00:05:44.690 --> 00:05:53.150
So definitely this is the same thing as you saw before previous before but is just a much more convoluted

77
00:05:53.150 --> 00:05:54.920
explanation or representation.

78
00:05:54.920 --> 00:06:01.550
So definitely this option is better and we're going to stick with this one and more so the it looks

79
00:06:01.670 --> 00:06:07.450
it looks difficult now but the goal is that by the end of this tutorial you are completely comfortable

80
00:06:07.450 --> 00:06:12.380
whats going on here and I think thats pretty exciting at this even though this may it may seem a bit

81
00:06:12.380 --> 00:06:13.410
complex now.

82
00:06:13.910 --> 00:06:18.460
Towards the end of the trial hopefully you'll be able to navigate LACMA quite well.

83
00:06:18.770 --> 00:06:25.250
And so before we go deep into what is going on here I wanted to highlight the main point so remember

84
00:06:25.250 --> 00:06:27.140
we said Jurek equals 1.

85
00:06:27.290 --> 00:06:33.410
Well that's this line over here that pipeline at the top as you can see there's nothing much not much

86
00:06:33.410 --> 00:06:33.890
going on here.

87
00:06:33.890 --> 00:06:42.620
There's only like 2 pointwise operations as well understand for the down and there's no complex neural

88
00:06:42.620 --> 00:06:48.200
network led operations are all brought out to this part and that's this is the essence if you're going

89
00:06:48.200 --> 00:06:55.710
to take away one thing from today's tutorial then this is that that you LACMA have a memory cell which

90
00:06:55.710 --> 00:06:56.930
is called the memory cell.

91
00:06:56.990 --> 00:07:02.250
I call the memory pipeline which just goes through time.

92
00:07:02.280 --> 00:07:10.280
And this is as going through time and it can very freely flow through time sometimes it might be removed

93
00:07:10.430 --> 00:07:11.300
and erased.

94
00:07:11.300 --> 00:07:14.650
Sometimes things might be added into it but that's pretty much it.

95
00:07:14.660 --> 00:07:19.430
Otherwise it flows through times freely and therefore when you back propagate through these Alice jams

96
00:07:19.760 --> 00:07:24.490
you don't have that problem of your vanishing gradient.

97
00:07:24.500 --> 00:07:29.660
That's that's the essence of Alice tamps.

98
00:07:29.690 --> 00:07:35.150
All right so now let's dig in a bit more detail so we're going to replace these modules on the left

99
00:07:35.150 --> 00:07:35.680
on the right.

100
00:07:35.670 --> 00:07:37.860
We have something more simple.

101
00:07:38.130 --> 00:07:49.190
This is going to replace them with our representation so C stands for memory a memory cell I guess.

102
00:07:49.460 --> 00:07:50.460
Is your output.

103
00:07:50.470 --> 00:07:53.650
So you can see there's the output going out into the world.

104
00:07:53.660 --> 00:07:58.640
And here you've got your output going into the next module in the next block.

105
00:07:58.790 --> 00:08:00.710
And then here you've got your input XTi.

106
00:08:00.800 --> 00:08:05.960
So basically this is the output from the Prius module which also went into the world but also is coming

107
00:08:05.960 --> 00:08:06.760
here.

108
00:08:06.800 --> 00:08:07.240
So there we go.

109
00:08:07.240 --> 00:08:14.450
So and elist GM module takes in three inputs and has two outputs.

110
00:08:14.450 --> 00:08:21.500
So City NHT because these are the same and the important thing to understand and remember is that everything

111
00:08:21.500 --> 00:08:23.320
here is a vector.

112
00:08:23.330 --> 00:08:28.220
So all of these are this is not just one neuron not just one rally.

113
00:08:28.220 --> 00:08:32.780
There are lots and lots and lots of values here behind hiding behind this war.

114
00:08:32.810 --> 00:08:37.910
This letter XTi and here as well and here as well and everywhere these are all

115
00:08:40.570 --> 00:08:41.160
layers.

116
00:08:41.270 --> 00:08:45.540
So just remember that and we're going to reference them as vectors because that's pretty much the same

117
00:08:45.530 --> 00:08:49.460
thing just lots of different values in one vector.

118
00:08:49.460 --> 00:08:56.420
Remove that and that will make it will allow you not to go down the wrong path in the intuitive understanding

119
00:08:56.420 --> 00:08:58.520
of this just remember that these are all vectors.

120
00:08:58.880 --> 00:09:00.620
And let's go through the legend.

121
00:09:00.620 --> 00:09:09.050
So we've got vector transfers so any line here is a vector being transferred or kind of moving around

122
00:09:09.050 --> 00:09:12.460
in this in this scenario in this architecture.

123
00:09:12.590 --> 00:09:16.100
And yes just kind of to reiterate that it is a vector.

124
00:09:16.460 --> 00:09:18.140
Then you've got a concatenation here.

125
00:09:18.380 --> 00:09:23.000
So here you can see that there's two lines combining into one.

126
00:09:23.000 --> 00:09:31.040
And important to understand that this is done here just to save space and make it less convoluted than

127
00:09:31.040 --> 00:09:32.390
it possibly could be.

128
00:09:32.600 --> 00:09:38.090
But the way the best way to imagine it is you that these two lines are running in parallel logic not

129
00:09:38.090 --> 00:09:42.740
actually combining concatenation means that you combine these two actors on top of each other but I

130
00:09:42.740 --> 00:09:47.570
think it's even easier to understand if you just think of it as in these there are two pipes running

131
00:09:47.570 --> 00:09:49.100
here but they're running parallel to each other.

132
00:09:49.100 --> 00:09:50.750
You've got one pipe and then it goes here.

133
00:09:50.760 --> 00:09:51.990
And the second pipe goes here.

134
00:09:52.070 --> 00:09:54.410
Then these same pipes go here and then they touch that.

135
00:09:54.420 --> 00:10:02.560
So basically you have two pipes running in parallel feeding into these neural network layer layer operations

136
00:10:03.310 --> 00:10:04.460
then you've got copy.

137
00:10:04.480 --> 00:10:10.270
So where do we have copy you have copy here the memories copies go ahead and just copy it here then

138
00:10:10.390 --> 00:10:12.570
this output is copied over here.

139
00:10:12.850 --> 00:10:14.230
Then you've got pointwise operations.

140
00:10:14.230 --> 00:10:15.650
Now we get to the interesting stuff.

141
00:10:15.730 --> 00:10:23.590
So you've got a couple of pointwise operations here that's five of them and the first thing we're going

142
00:10:23.590 --> 00:10:29.200
to talk about these ones the x's the Xs are valves and they all the names this is the forget valve the

143
00:10:29.200 --> 00:10:30.600
memory valve and the output file.

144
00:10:30.640 --> 00:10:35.630
And in literature you will see like letters f v and O.

145
00:10:36.070 --> 00:10:45.240
In the actual formulas representing these valves and so a valve looks like this in the plumbing valve

146
00:10:45.250 --> 00:10:48.040
looks like this and we're going to kind of think of it that way as well.

147
00:10:48.100 --> 00:10:54.670
So you've got water or basically something flowing through and then you can either close it or you can

148
00:10:54.730 --> 00:10:57.040
open it or you can close it to some extent.

149
00:10:57.040 --> 00:10:57.700
Same thing here.

150
00:10:57.700 --> 00:11:05.500
So you've got the forget valve is basically controlled by this layer operation and we'll talk about

151
00:11:05.500 --> 00:11:06.640
that in a second.

152
00:11:06.790 --> 00:11:11.760
And based on the output of that based on the decision made here this wall was either closed or open

153
00:11:11.770 --> 00:11:18.820
so if it's open memory flows through freely if it's closed then memory is cut off and therefore it's

154
00:11:18.820 --> 00:11:24.880
not dozens not transferred further and then new memory just is will be added here probably based on

155
00:11:24.880 --> 00:11:25.750
the results here.

156
00:11:25.750 --> 00:11:29.260
Then you've got the memory valve which again is controlled by a Sigma.

157
00:11:29.290 --> 00:11:34.450
See my sense for the sigmoid activation function that means that that's the activation function used

158
00:11:34.450 --> 00:11:37.680
in these léa operations.

159
00:11:37.780 --> 00:11:46.420
And as the decision is made here this value which is another layer operation which we'll get to in a

160
00:11:46.420 --> 00:11:52.900
second is either added to the memory or is somewhat added to the memory or is not added to the memory

161
00:11:53.050 --> 00:11:56.160
depending on the value that is decided in this valve.

162
00:11:56.560 --> 00:12:01.210
And then again another valve controlled by sigmoid as remember sigmoid or we use the same was because

163
00:12:01.210 --> 00:12:05.440
they're from 0 to 1 and therefore zero sense for no memory no.

164
00:12:05.440 --> 00:12:07.270
Nothing goes through one stands for something goes through.

165
00:12:07.270 --> 00:12:13.600
And then here you've got them valve which is the forget valve since they are not forgetful This is the

166
00:12:13.600 --> 00:12:15.600
output valve and we'll get to that in a second.

167
00:12:15.610 --> 00:12:19.210
We're pretty sure what you're going through the whole network already.

168
00:12:19.420 --> 00:12:27.000
So and then we've got T-shaped kind of like pipe or T-shaped joint.

169
00:12:27.040 --> 00:12:27.840
Why don't we.

170
00:12:27.960 --> 00:12:28.940
I'll show you where that is.

171
00:12:28.960 --> 00:12:30.010
That is over here.

172
00:12:30.190 --> 00:12:34.690
So where you have memory going through and then you can add additional memory if we go back you've got

173
00:12:34.690 --> 00:12:39.790
memory flowing through the joint and maybe some additional memory will be added into it maybe not depending

174
00:12:39.790 --> 00:12:41.330
on the ball.

175
00:12:42.520 --> 00:12:47.920
And as I said you've got the engine operation here that's more kind of like mathematical behind it why

176
00:12:47.920 --> 00:12:50.060
you want to be had between minus 1 and 1.

177
00:12:50.200 --> 00:12:53.250
We won't get into details of that but that's another pointwise operation here.

178
00:12:53.270 --> 00:12:59.730
You have and you've got the neural network layer operations we're here.

179
00:12:59.760 --> 00:13:01.480
That's that's representation.

180
00:13:01.480 --> 00:13:07.720
So basically just think of them as you've got and you've got instead of pointwise pointwise like element

181
00:13:07.720 --> 00:13:09.180
by element of a vector.

182
00:13:09.220 --> 00:13:14.170
You know if you want to multiply vector by zero you multiply every element by 0 or by one or by a certain

183
00:13:14.170 --> 00:13:14.620
amount.

184
00:13:14.630 --> 00:13:20.740
Kind of think of it that way that's a very simplistic way to think about these pointless operations

185
00:13:20.840 --> 00:13:25.000
whereas these ones are a bit more complex you've got layer coming in.

186
00:13:25.000 --> 00:13:30.070
And then you've got a flare coming out or like a lot because again everything here is a vector so you've

187
00:13:30.070 --> 00:13:37.710
got a lair of the sigmoid coming out controlling the valve for each one of these elements in the vector

188
00:13:37.810 --> 00:13:38.590
of memory.

189
00:13:38.590 --> 00:13:43.990
So there's not just one scene where there's many different and that's why you've got a whole layer coming

190
00:13:43.990 --> 00:13:44.260
in.

191
00:13:44.260 --> 00:13:46.430
And then you've got a layer coming on.

192
00:13:46.540 --> 00:13:48.450
These are layer operations.

193
00:13:48.490 --> 00:13:50.280
So just remember that.

194
00:13:50.440 --> 00:13:57.040
And so we're ready to look into this and in step by step and is going to very simple because we've discussed

195
00:13:57.040 --> 00:14:04.260
everything already in terms of hard work so we've got a new value coming in.

196
00:14:04.300 --> 00:14:12.340
You've got memory you've got non-memory you've got value coming from previous nodes and ropes and together

197
00:14:12.880 --> 00:14:20.590
they are combined to try to decide whether this valve should go ahead or should be closed or should

198
00:14:20.590 --> 00:14:22.990
be open though it was somewhat closed or open.

199
00:14:22.990 --> 00:14:28.450
Then you've got these two again combined together on a combined like they again flow in parallel and

200
00:14:28.450 --> 00:14:32.420
then they're combined in in here or in this operation.

201
00:14:32.620 --> 00:14:37.960
And basically it's just them combined it's like there's lots of layers here lots of layers here one

202
00:14:37.980 --> 00:14:45.520
like lots of neurons here not some neurons here and then all of that is basically in one layer operation

203
00:14:45.550 --> 00:14:49.940
is used to decide what value we're going to pass through.

204
00:14:49.990 --> 00:14:55.690
And then also if that value is going to pass or not and to what extent then here we've got the memory

205
00:14:55.690 --> 00:14:56.710
flowing through.

206
00:14:56.710 --> 00:15:02.870
We've got the forget valve if it's closed or and we've got memory valve close or open and we're adding

207
00:15:02.870 --> 00:15:05.620
in some memory possibly if we want to update.

208
00:15:05.620 --> 00:15:10.070
So basically we can let this whole memory fall through then keep this close keep us and open the memory

209
00:15:10.100 --> 00:15:16.370
and change keep this one or we can keep this one close and keep this one open then we can update the

210
00:15:16.370 --> 00:15:17.850
memory completely.

211
00:15:18.500 --> 00:15:27.530
And here finally we've got these two values combined to decide what part of the memory memory pipeline

212
00:15:28.190 --> 00:15:37.660
is going to be output become output of this module is it going to go as fully as the output or to some

213
00:15:37.660 --> 00:15:38.560
extent and so on.

214
00:15:38.560 --> 00:15:40.590
So again these decide that.

215
00:15:40.870 --> 00:15:42.280
So that's how it all works.

216
00:15:42.280 --> 00:15:45.110
Pretty straightforward architecture.

217
00:15:45.130 --> 00:15:49.680
Let's have a look at a specific example to understand this a bit better.

218
00:15:49.840 --> 00:15:55.120
So the example we talked about I'm a boy who likes to learn is that it at cheat intersection to check

219
00:15:55.540 --> 00:15:57.480
if this were girl.

220
00:15:57.500 --> 00:16:00.810
Then here he is Sam Holker caetera rather.

221
00:16:00.850 --> 00:16:06.730
OK it meaning that these two words not just this word would change but also it would affect these two

222
00:16:06.730 --> 00:16:10.640
words so different to in check rather than in English.

223
00:16:10.640 --> 00:16:16.030
So these words are affected by the gender of the subject and therefore in our enlistee and we might

224
00:16:16.030 --> 00:16:19.900
want to store the subject boy in this case in the memory.

225
00:16:19.900 --> 00:16:27.160
So for instance let's say boys start here and then it's just flowing through freely and nothing is changing.

226
00:16:27.160 --> 00:16:31.660
Like if we gaf if our new information doesn't tell us that there's a new subject.

227
00:16:31.660 --> 00:16:35.300
We just have boy flowing through freely and keeps flowing again.

228
00:16:35.330 --> 00:16:42.220
If for instance we have a new subject we have girl or we have we have a name like Amanda or something

229
00:16:42.220 --> 00:16:48.660
else and we understand that we've got a new subject then rules or we'll let this evolve.

230
00:16:49.150 --> 00:16:54.820
We'll close this while say you know destroy the memory that we had then open this wall put a new memory

231
00:16:55.150 --> 00:17:00.880
and then put the name here put the subject on just put in the gender one just put in like female I should

232
00:17:00.880 --> 00:17:03.220
put the whole as much information as we can.

233
00:17:03.220 --> 00:17:07.210
For instance this could be the Arcata is it doesn't have to be like this but as an example we could

234
00:17:07.210 --> 00:17:15.310
put in for instance girl now into this into this pipeline and why would we do that.

235
00:17:15.340 --> 00:17:19.690
Well because then from that we can extract different elements of information we can extract that it's

236
00:17:19.690 --> 00:17:23.170
female we can extract that singular right.

237
00:17:23.200 --> 00:17:27.670
So not just that it's there there's additional information the word girl that is single we can extract

238
00:17:27.670 --> 00:17:29.130
more information we can extract that.

239
00:17:29.380 --> 00:17:35.200
The word girl for instance has four letters were that it was capitalized or wasn't capitalized just

240
00:17:35.200 --> 00:17:39.810
as these are all very very intuitive examples.

241
00:17:39.840 --> 00:17:43.090
Doesn't have to work this way but this is how it could work.

242
00:17:43.150 --> 00:17:45.600
And so then we have the word girl in here.

243
00:17:45.700 --> 00:17:47.650
And so that's how this world works.

244
00:17:47.680 --> 00:17:48.910
And this wall works.

245
00:17:48.910 --> 00:17:53.740
And so what this valve does is it extracts certain information from which you have in the memory.

246
00:17:53.730 --> 00:17:59.830
So for instance if we have no girl in here and for the purposes of the current of the next word or next

247
00:17:59.830 --> 00:18:04.300
sentence that's coming up you might need to know like we saw you need to know the gender.

248
00:18:04.300 --> 00:18:13.810
So then this valve will facilitate the extraction of the gender and that will go as an input into your

249
00:18:13.810 --> 00:18:18.100
next module and it'll help the next module like it will be here.

250
00:18:18.130 --> 00:18:27.280
Decide and decide how to deal with the information that's coming its way how to put it into the correct

251
00:18:27.670 --> 00:18:30.420
form for the female gender for example.

252
00:18:30.430 --> 00:18:35.740
And so that's how the output valve works and what it does.

253
00:18:35.740 --> 00:18:40.170
So there we go that's how the long short term memory works.

254
00:18:40.180 --> 00:18:47.710
And I hope this was a quite an intuitive explanation and now you have a bit of a better understanding

255
00:18:47.970 --> 00:18:53.290
what is what elist hams are like and those of additional reading you can definitely reference the original

256
00:18:53.290 --> 00:18:58.010
paper by our two offers who created elist yams.

257
00:18:58.030 --> 00:19:02.480
Alternatively if you don't want to get that deep into mathematics and into the technical stuff there

258
00:19:02.490 --> 00:19:06.890
is a great great blog by Christopher Ola which we've already mentioned.

259
00:19:06.910 --> 00:19:09.130
Great explanation of Ellis DMD's.

260
00:19:09.280 --> 00:19:10.940
I highly recommend to check it out.

261
00:19:10.960 --> 00:19:15.360
There's a bit of mathematics not too heavy and there's another blog by.

262
00:19:15.740 --> 00:19:22.060
She yawned and asked him about SDM and its diagrams diagrams are a bit more in-depth.

263
00:19:22.060 --> 00:19:24.530
So they're a bit more.

264
00:19:24.830 --> 00:19:29.650
There's a bit more less space saving but there's have been more death might be easier to understand

265
00:19:29.650 --> 00:19:30.460
in some cases.

266
00:19:30.580 --> 00:19:33.490
No mathematics whatsoever just plain intuition.

267
00:19:33.500 --> 00:19:37.370
So also highly recommend this blog to check it out.

268
00:19:37.390 --> 00:19:44.170
If you like to get additional information on elist hymns and on that note I hope you enjoy Teresa Tauriel

269
00:19:44.380 --> 00:19:45.670
and I look forward to you next time.

270
00:19:45.730 --> 00:19:47.420
Until then enjoy deep learning.
