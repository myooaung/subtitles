1
00:00:00,720 --> 00:00:04,040
Hello I'll come back to the course on deep natural language processing.

2
00:00:04,110 --> 00:00:08,470
And today we're talking about classical versus deep learning models.

3
00:00:08,580 --> 00:00:10,040
So let's have a look.

4
00:00:10,290 --> 00:00:16,090
As we discussed about this diagram and on the left what the natural language processing models are overall

5
00:00:16,540 --> 00:00:23,430
and the on the right we have that deep learning models or Also there overlap is that deep natural language

6
00:00:23,430 --> 00:00:28,770
processing those are the very advanced types of natural language processing models which use deep learning

7
00:00:29,190 --> 00:00:32,910
and then find at the bottom here we've got the sequence to sequence models which we'll be interested

8
00:00:32,910 --> 00:00:33,950
in at the end.

9
00:00:34,070 --> 00:00:41,160
And so I thought that this tutorial would be fun if we had a look at a comparison between a couple So

10
00:00:41,160 --> 00:00:44,010
we'll look at a couple of examples from here.

11
00:00:44,190 --> 00:00:51,240
We'll look at an example from here and then finally will image sequence sequence just to kind of lay

12
00:00:51,240 --> 00:00:56,520
out the playing field so we know what we're dealing with and what types exist there because we could

13
00:00:56,520 --> 00:01:00,810
definitely just jump into sequence to sequence and look at the most powerful things.

14
00:01:00,990 --> 00:01:06,480
But then it would still be a mystery what our natural language processing models how the different deep

15
00:01:06,480 --> 00:01:11,960
natural language processing was and how the different sequence sequence with applications and so on.

16
00:01:12,180 --> 00:01:15,680
So let's have a look at a couple examples.

17
00:01:15,690 --> 00:01:20,760
All right some examples we're going to have our diagram on the right and we're going to start going

18
00:01:20,760 --> 00:01:22,380
through these examples one by one.

19
00:01:22,620 --> 00:01:32,550
So number one is that if Alice rules or and this is a way that we used to create chat bots back in the

20
00:01:32,550 --> 00:01:32,760
day.

21
00:01:32,760 --> 00:01:42,280
So if Alce rules are located over here on our diagram in just the A.P. part and what they entail is

22
00:01:42,490 --> 00:01:48,130
a huge list of possible questions and answers to those questions.

23
00:01:48,130 --> 00:01:55,300
And so once somebody in the chat asks a question or we can identify that part of the sentence is the

24
00:01:55,300 --> 00:02:01,660
question that we have proof it then we will give them the correct answer the answer that is associated

25
00:02:01,660 --> 00:02:02,830
with that question.

26
00:02:02,890 --> 00:02:11,650
But as you can imagine such mechanic is such a mechanical approach to answering questions or chatting

27
00:02:11,650 --> 00:02:19,810
with people does not result in anything human like anything realistic and it results in something like

28
00:02:19,810 --> 00:02:24,330
this rather where you have questions and then you have predetermined answers.

29
00:02:24,340 --> 00:02:29,560
But people want something tailored to them something specific they are asking about something else that

30
00:02:29,680 --> 00:02:32,940
doesn't fit into that list of questions and answers.

31
00:02:33,030 --> 00:02:43,900
And just very quickly becomes a mess but that's how we use to create chapel's the next type or the next

32
00:02:44,470 --> 00:02:45,950
natural language processing model.

33
00:02:45,970 --> 00:02:46,720
We will look at.

34
00:02:46,720 --> 00:02:54,940
It's called an audio frequency components analysis and it's used for speech recognition.

35
00:02:54,940 --> 00:03:01,120
And since we're here so we're specifically looking like we're purposefully looking at different examples

36
00:03:01,120 --> 00:03:06,400
and not just chat but we're looking at different applications so you can see that LP can be applied

37
00:03:06,400 --> 00:03:07,760
to speech recognition.

38
00:03:07,900 --> 00:03:15,460
And there used to be or still are algorithms that use non deep learning methods for speech recognition.

39
00:03:15,520 --> 00:03:18,980
And one of them is audio frequency components analysis.

40
00:03:19,150 --> 00:03:25,780
So in essence what happens is we look at the sound waves of somebody talking some proof chorded or real

41
00:03:25,780 --> 00:03:34,540
time audio over human speech and then we try to identify what waveforms exist in there.

42
00:03:34,690 --> 00:03:41,260
So in a very kind of simple way of explaining it let's have a look at.

43
00:03:41,280 --> 00:03:48,430
For example here are the four year transformation and with God there are frequencies where there's this

44
00:03:48,430 --> 00:03:51,570
like one of these He's going over there.

45
00:03:51,730 --> 00:03:54,340
Of course this is this doesn't even look like human speech.

46
00:03:54,340 --> 00:04:01,420
To be frank but the concepts say so like we look at what frequencies it can.

47
00:04:01,420 --> 00:04:02,550
It consists of.

48
00:04:02,740 --> 00:04:11,950
And then we compare them to like the pre recorded frequencies so we know that for instance certain certain

49
00:04:11,950 --> 00:04:16,930
combination of frequencies means this type of word or this type of work and it's not just about the

50
00:04:16,930 --> 00:04:23,050
frequencies of course it's it's much more complex than that but this is a very general overview of what

51
00:04:23,050 --> 00:04:26,580
happens we look at the frequencies a certain mathematical approach.

52
00:04:26,590 --> 00:04:33,840
This is the key point that we're not doing any neural confutation we're not creating neural networks

53
00:04:33,940 --> 00:04:40,870
just doing mathematical calculations around the frequencies that we can observe comparing them to the

54
00:04:40,870 --> 00:04:47,980
mathematical calculations we have in our library all pre analyzed frequencies and then we're matching

55
00:04:47,980 --> 00:04:54,100
it up with finding what word a person is saying what question they're asking or what the sentence is

56
00:04:54,100 --> 00:04:54,890
meaning.

57
00:04:55,030 --> 00:04:58,280
And then that is how we recognize speech.

58
00:04:58,720 --> 00:05:04,300
So there's another version of natural language processing which is in the green area.

59
00:05:04,780 --> 00:05:13,420
Next one is the bag of words model which is used for classification of very very popular approach for

60
00:05:14,410 --> 00:05:17,230
text analysis or natural language processing.

61
00:05:17,350 --> 00:05:24,910
And what it does is basically it also sits in the end all the parts and just the green area.

62
00:05:25,540 --> 00:05:31,540
Although as we will see further on it can see both in the green area and in the purple area over here.

63
00:05:31,750 --> 00:05:34,780
But that's something for a later discussion.

64
00:05:34,810 --> 00:05:38,440
What's the bag of words model does it is there is a bag of words.

65
00:05:38,800 --> 00:05:48,220
And the way it works is you might have like lots of text for instance here we've got tutors or lectures

66
00:05:48,310 --> 00:06:00,910
or examiners who marked grades who gave a pass or a failed grade to different papers or different assignments

67
00:06:00,910 --> 00:06:06,520
that were submitted and then they also left the comment so somebody said great job and left a comment

68
00:06:06,520 --> 00:06:11,820
or one meaning a past somebody said amazing work left to come out of one well done was really really

69
00:06:11,830 --> 00:06:14,770
rich and one poor effort zero could have that better zero.

70
00:06:14,770 --> 00:06:15,920
Try harder next time.

71
00:06:15,940 --> 00:06:16,530
And so on.

72
00:06:16,830 --> 00:06:24,200
And so from this what the bag awards model will do is it will put all these words into a bag and it

73
00:06:24,210 --> 00:06:28,640
will remember so how often does the word grace come up with a one.

74
00:06:28,810 --> 00:06:30,940
And how often does the word great come up with a zero.

75
00:06:30,940 --> 00:06:36,090
This is again a very general explanation a very high level explanation of what happens.

76
00:06:36,220 --> 00:06:43,230
But in essence this is the concept that it will look at the words and Ill look ill try to classify these

77
00:06:43,230 --> 00:06:44,920
words will I.

78
00:06:45,380 --> 00:06:52,540
Ill try to associate these words with either positive result or negative result in our case.

79
00:06:52,560 --> 00:06:57,900
And so in this case like amazing would be most likely associate with both of you don't often see amazing

80
00:06:58,290 --> 00:07:03,300
when you're trying to say when when the sentiment is negative when you try it when the person was trying

81
00:07:03,300 --> 00:07:09,240
to say something like that the work wasn't well done or well would be associated with words like the

82
00:07:09,240 --> 00:07:11,740
word work could be associated with both.

83
00:07:11,970 --> 00:07:18,090
But then these other words they would be mostly associate you pass something like words like poor or

84
00:07:18,360 --> 00:07:23,330
harder or who would have maybe would be associate zero.

85
00:07:23,520 --> 00:07:28,110
And so then will remember that I'll keep these words in a bag and next time something comes up.

86
00:07:28,110 --> 00:07:32,990
For instance somebody says Good.

87
00:07:33,240 --> 00:07:34,540
Good job.

88
00:07:34,800 --> 00:07:36,430
Keep keep it up or something.

89
00:07:36,430 --> 00:07:36,920
Got it.

90
00:07:36,930 --> 00:07:42,840
We'll analyze the words that are in that new sentence by pulling them out of the bag and looking at

91
00:07:42,840 --> 00:07:46,420
them and understanding are they mostly associated ones or zeros.

92
00:07:46,650 --> 00:07:53,730
And then I'll be able to predict they'll be able to classify the new new comments even without knowing

93
00:07:53,760 --> 00:07:58,800
what the mark was pacified will be able to say based on the comment or they will say is it a pass or

94
00:07:58,800 --> 00:07:59,560
fail.

95
00:08:00,000 --> 00:08:03,840
So that's like a very simplistic application of the bag of words.

96
00:08:03,960 --> 00:08:05,540
But there's another one.

97
00:08:05,660 --> 00:08:06,930
OK.

98
00:08:08,130 --> 00:08:11,890
OK so let's look at a deep natural language processing model.

99
00:08:12,180 --> 00:08:15,150
And this one is going to be OK over here.

100
00:08:15,150 --> 00:08:19,260
It's called convolutional neural networks for text recognition.

101
00:08:19,410 --> 00:08:26,070
So they like the model we're going to be looking at further down is indeed a deep natural language processing

102
00:08:26,070 --> 00:08:26,490
model.

103
00:08:26,550 --> 00:08:32,100
But I always wanted to give an example of a different model so not not just the one that we looked at

104
00:08:32,130 --> 00:08:35,730
but also a deep natural language processing model but a different one.

105
00:08:35,910 --> 00:08:43,250
And when I came across is when we use convolutional neural networks for texture cognition and then for

106
00:08:43,350 --> 00:08:44,440
classification.

107
00:08:44,940 --> 00:08:51,270
And so if if you're familiar with conventional networks or even if you're not what they're used for

108
00:08:51,270 --> 00:08:58,890
is a neural network that is used for mostly image recognition for like videos self-driving cars use

109
00:08:58,890 --> 00:09:01,770
them to detect obstacles on roads people and so on.

110
00:09:01,950 --> 00:09:05,560
Mostly it's used for image processing or video processing.

111
00:09:05,610 --> 00:09:11,100
So it is very interesting to see how convolutional neural networks can actually be used for text processing

112
00:09:11,370 --> 00:09:22,920
and the way it works is it's these words are transformed into a matrix and that's done through an operation

113
00:09:22,920 --> 00:09:24,580
called embedding of words.

114
00:09:24,810 --> 00:09:31,950
And then once they're in a matrix the same principles as kind of solution as we were apply for images

115
00:09:31,950 --> 00:09:33,870
and coalitional neural networks are applied.

116
00:09:33,870 --> 00:09:42,150
So there's a convolution operation going through these images then they're they're called Max gold I

117
00:09:42,150 --> 00:09:44,590
mean pulled some pole.

118
00:09:45,090 --> 00:09:50,300
And then they're flattened and then you know then we have the prediction.

119
00:09:50,310 --> 00:09:56,540
So we won't go into detail right now of how convolution convolutional operations work.

120
00:09:56,570 --> 00:10:02,400
There's lots of tutorials out there and so on and it's just not in the scope of the score because in

121
00:10:02,400 --> 00:10:07,480
this course we'll be looking at mostly recurrent neural networks which is RNA in a different type.

122
00:10:07,740 --> 00:10:13,620
But I just wanted to put it out there that you could technically use convolutional neural networks for

123
00:10:13,620 --> 00:10:18,300
text recognition just as you would do with images and very interesting concept.

124
00:10:18,300 --> 00:10:24,900
That was one of the earlier ones that was explored and then we moved on to recurrent neural networks

125
00:10:24,900 --> 00:10:26,990
as we see down the track.

126
00:10:27,420 --> 00:10:28,950
So there we go that's another one.

127
00:10:29,100 --> 00:10:34,710
And then finally the main model that we'll be looking for working with sequence the sequence has many

128
00:10:34,710 --> 00:10:37,910
many many different applications as we see down.

129
00:10:37,920 --> 00:10:39,450
And that's the one that's over there.

130
00:10:39,480 --> 00:10:42,530
So I'll give you a short preview of what's coming up.

131
00:10:42,580 --> 00:10:50,340
Looks something like this and don't worry it might look very complex right now but in a couple of tutorials

132
00:10:50,340 --> 00:10:57,540
from here will be very very comfortable with what's going on here and how this whole construct is working

133
00:10:57,540 --> 00:11:02,210
and why it's called sequence to sequence and what exactly it allows us to do.

134
00:11:02,580 --> 00:11:03,140
So there we go.

135
00:11:03,140 --> 00:11:12,360
I hope you enjoy this quick overview and comparison of the different types of models and different applications

136
00:11:12,360 --> 00:11:13,270
that they have.

137
00:11:13,440 --> 00:11:16,150
And we'll continue next and I look forward to seeing you there.

138
00:11:16,170 --> 00:11:19,300
And until then enjoy deep and O.P..
