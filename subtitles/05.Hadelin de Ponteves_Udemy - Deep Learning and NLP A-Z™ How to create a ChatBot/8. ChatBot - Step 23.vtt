WEBVTT
1
00:00:00.610 --> 00:00:08.230
Hello and welcome to this very exciting tutorial where we will create the decoder Arnon as opposed to

2
00:00:08.230 --> 00:00:13.090
what we did before when we created the encoder Arnon.

3
00:00:13.150 --> 00:00:19.870
Remember in this code section here we made the encoder arland function by the way I just replaced the

4
00:00:19.870 --> 00:00:25.150
previous name encoder Arnon layer by anchor Arnon at the end of the previous toile.

5
00:00:25.240 --> 00:00:31.180
And so in today's Statoil we're going to make another record a new work which will be this time to record

6
00:00:31.180 --> 00:00:36.400
a new network of the decoder and therefore let's scroll down back.

7
00:00:36.450 --> 00:00:41.790
We're going to make a new function here that we're going to call decoder underscore our.

8
00:00:41.800 --> 00:00:44.600
And then record a neural network of the decoder.

9
00:00:44.740 --> 00:00:47.590
So if you're ready let's begin.

10
00:00:47.590 --> 00:00:50.840
So this day go to Arlan function is going to take several arguments.

11
00:00:50.840 --> 00:00:52.460
Actually lots of them.

12
00:00:52.600 --> 00:00:55.540
So we're going to put them one by one quite quickly.

13
00:00:55.540 --> 00:00:59.840
Now that we are starting to get comfortable with all these different objects.

14
00:00:59.860 --> 00:01:00.160
All right.

15
00:01:00.160 --> 00:01:08.330
So the first argument is going to be the decoder underscore and deaded underscore inputs.

16
00:01:08.410 --> 00:01:10.150
We already covered that.

17
00:01:10.150 --> 00:01:18.140
Let's move on to the next one then the next one is going to be the decoder embeddings matrix.

18
00:01:18.150 --> 00:01:24.090
So again I will provide the resources in the description of this tutorial if you want to have more details

19
00:01:24.090 --> 00:01:24.740
on this.

20
00:01:24.870 --> 00:01:32.190
Then the next one is going to be the and coater underscore state which I remind that a very important

21
00:01:32.190 --> 00:01:39.100
thing to keep in mind is that it's the output of the encoder but that becomes the input of the decoder.

22
00:01:39.150 --> 00:01:45.890
So in current state we will definitely need it for our decoder Arnon then we're going to take again

23
00:01:46.140 --> 00:01:55.210
the total number of words in our corpus of answers then sequence lengths.

24
00:01:55.290 --> 00:01:58.540
Again we've already seen that sequence length.

25
00:01:58.550 --> 00:02:06.960
Then the Arnon underscore size which is another one of our previous arguments then the number of layers

26
00:02:06.960 --> 00:02:13.490
that we called Number layers so that this time the number of layers we want to have inside the record

27
00:02:13.490 --> 00:02:22.320
of a neural network of our decoder nonlawyers than what else we're going to need our word to in a dictionary.

28
00:02:22.330 --> 00:02:25.550
But that can actually be any dictionary it's just an argument.

29
00:02:25.560 --> 00:02:31.920
But that will of course be our answers words to any dictionary that we made in the previous part but

30
00:02:31.930 --> 00:02:36.390
one data pre-processing then we're going to apply some drop out again because there are going to be

31
00:02:36.390 --> 00:02:42.390
some training in the implementation of these Dakoda Arnon and therefore we are going to use to keep

32
00:02:42.600 --> 00:02:46.310
underscore prob parameter to control the dropout rate.

33
00:02:46.530 --> 00:02:48.780
And finally one last one.

34
00:02:48.840 --> 00:02:50.160
Try to guess what it is.

35
00:02:50.160 --> 00:02:51.170
It's very important.

36
00:02:51.170 --> 00:03:00.070
You know it's at the heart of any neural network it is the batch size because the inputs of neural networks

37
00:03:00.070 --> 00:03:06.250
whether it is in terms of flow or by torch are in two batches and we need to specify a batch size to

38
00:03:06.580 --> 00:03:08.710
choose the size of these batches.

39
00:03:08.710 --> 00:03:09.030
All right.

40
00:03:09.040 --> 00:03:11.190
So that's it for our arguments.

41
00:03:11.230 --> 00:03:14.660
And now let's start implementing this function.

42
00:03:14.800 --> 00:03:21.970
So we're going to start with actually with the with syntax of Python supersized that we're making this

43
00:03:21.970 --> 00:03:24.570
decode Arlan in a decoding scope.

44
00:03:24.580 --> 00:03:26.760
Remember the decoding scope I explain that.

45
00:03:26.780 --> 00:03:33.700
And I think tutorials ago it's a more advanced terms of the variable that will contain several data

46
00:03:33.700 --> 00:03:37.800
entities and therefore what we need to start here is introduce this code.

47
00:03:37.840 --> 00:03:43.810
And we're going to do it this way with a sense of flow and then from TransFair we're going to get that

48
00:03:43.990 --> 00:03:47.030
variable underscore scope.

49
00:03:47.080 --> 00:03:49.610
I will put that again in the resources of this lecture.

50
00:03:49.840 --> 00:03:59.440
And now in quotes we're going to input that coding as the decoding underscore scope.

51
00:03:59.450 --> 00:03:59.900
All right.

52
00:03:59.950 --> 00:04:01.730
Now we have to add some Kallen.

53
00:04:01.870 --> 00:04:05.500
And now inside the scope we're going to make everything happen.

54
00:04:05.500 --> 00:04:05.740
All right.

55
00:04:05.740 --> 00:04:10.480
So the first thing we need to start with is create our LSD layer.

56
00:04:10.570 --> 00:04:18.590
And so as for my encoder I'm going to call it L S T M equals and then exactly the same as our encoder

57
00:04:18.610 --> 00:04:24.790
we're going to get our sense of the library then from this library we're going to get the contrib module

58
00:04:25.030 --> 00:04:31.140
then from the country Mulier we're going to get the Arnon submodule and from this arland submodule we're

59
00:04:31.150 --> 00:04:41.560
going to get the basic LSD and sell the basic LACMA cell class that takes as inputs on you one argument

60
00:04:41.860 --> 00:04:45.480
which is the Arnon size.

61
00:04:45.610 --> 00:04:54.130
Remember that's exactly what we did here for the encoder we created as a layer which was same an object

62
00:04:54.250 --> 00:04:58.840
of the basic LACMA cell class that takes us and put the r in incise perfect.

63
00:04:58.840 --> 00:05:04.420
So we now have our Alice GM and so now that we have Arliss GM What do we do.

64
00:05:04.510 --> 00:05:11.400
Well it's time to apply some drop out regularisation to reduce overfilling and improve the accuracy.

65
00:05:11.500 --> 00:05:19.790
And so we're going to introduce the same variable name and Esty and underscore dropout meaning an LACMA

66
00:05:19.810 --> 00:05:22.250
layer on which dropout is applied.

67
00:05:22.420 --> 00:05:24.030
So LSD and dropout.

68
00:05:24.190 --> 00:05:30.060
And that again will be an object of another class that we used for our encoder.

69
00:05:30.070 --> 00:05:31.080
There it is.

70
00:05:31.150 --> 00:05:37.240
And so we can basically copy this because it's going to be exactly the same it is going to be an object

71
00:05:37.240 --> 00:05:44.320
of the drop out wrapper that needs to take the previous LACMA that we created as an object of the basic

72
00:05:44.320 --> 00:05:50.540
Elsje himself last and that uses the key probe parameter controlling the dropout rate.

73
00:05:50.740 --> 00:05:59.550
So let's do this let's that here T.F. contrib Arnon dropout rapper LACMA and put riprap equals keep.

74
00:05:59.980 --> 00:06:00.780
Perfect.

75
00:06:00.940 --> 00:06:06.980
So now we have our LACMA there with our applied but that gives us one LS gem there.

76
00:06:07.090 --> 00:06:09.200
And of course we're not building a kid stuff.

77
00:06:09.230 --> 00:06:15.310
Arnon we're building a stacked Ellis GM and therefore now we're going to use to multiply Arnon sell

78
00:06:15.310 --> 00:06:20.110
to stack several LACMA layers with drop out applied.

79
00:06:20.290 --> 00:06:23.870
And again that's exactly what we did for our encoder.

80
00:06:24.040 --> 00:06:32.140
So I'm going to simply well copy this line and just below for our decoder Arnon.

81
00:06:32.170 --> 00:06:37.510
Well I'm going to introduce this new variable here that will be our decoder cell.

82
00:06:37.510 --> 00:06:43.540
I remind that the decoder cell is what contains the stacked LACMA layers with about applied.

83
00:06:43.540 --> 00:06:51.010
So that's basically a cell LACMA layers and therefore this occurs there is going to be the taste of

84
00:06:51.010 --> 00:06:57.550
what I just copied that is an object of the multi Arnon cell taken from the Arlon module of the country

85
00:06:57.550 --> 00:07:04.600
module from the tents of the library which takes as input the previews LACMA layer in which drug was

86
00:07:04.600 --> 00:07:12.010
applied times the number of layers we want to have in this stacked LSD layer inside the Dakota cell.

87
00:07:12.160 --> 00:07:17.520
Then next step the next step is actually something new something we haven't done before.

88
00:07:17.560 --> 00:07:24.510
We need to initialize some weights that will be associated to the neurons of the fully connected layers

89
00:07:24.580 --> 00:07:26.010
inside our decoder.

90
00:07:26.020 --> 00:07:31.620
That is the last layer and to initialize them well first we need to introduce a new variable that I'm

91
00:07:31.630 --> 00:07:38.020
calling waits and we're going to initialize them by using an initializer.

92
00:07:38.020 --> 00:07:44.580
That is the T.F. that truncated normal initializer.

93
00:07:44.890 --> 00:07:49.570
And that will generate a truncated normal distribution of the weights.

94
00:07:49.570 --> 00:07:52.980
This function takes only one argument.

95
00:07:53.010 --> 00:07:58.930
We're going to choose a standard deviation which will be 0.1 and to specify this we need to answer the

96
00:07:58.930 --> 00:08:02.850
following argument here which is as t d d e.

97
00:08:02.860 --> 00:08:07.560
The standard deviation and that is equal to point 1.

98
00:08:07.660 --> 00:08:14.800
All right so now we have our fully connected the weights initialized then next step naturally after

99
00:08:14.800 --> 00:08:20.170
the weights the next step is to get the bias's so I'm introducing a new voice on here.

100
00:08:20.170 --> 00:08:27.970
Biases biases and now much more simple we just need to initialize them as zeros and to do that.

101
00:08:27.970 --> 00:08:36.640
We used the sense of flow function that is called zeros underscore initialiser and just some parenthesis

102
00:08:36.640 --> 00:08:38.530
with nothing in sight.

103
00:08:38.530 --> 00:08:43.530
All right so now we have our weights our biases for the fully connected layers.

104
00:08:43.540 --> 00:08:44.140
Great.

105
00:08:44.290 --> 00:08:49.150
We're ready to move on to the next step the next step is to make the output function.

106
00:08:49.300 --> 00:08:55.990
And so I'm getting it out put on this core function so I'm introducing a new variable that I choose

107
00:08:55.990 --> 00:09:00.880
to have the same name as the argument given in the previous function's output function.

108
00:09:00.880 --> 00:09:08.020
And now we're going to use the classic Python syntax to make a function which starts with lambda.

109
00:09:08.230 --> 00:09:12.130
Then the variable of this function is going to be X then.

110
00:09:12.310 --> 00:09:16.500
And what comes after this column is what this function will return.

111
00:09:16.510 --> 00:09:22.900
So what this function will return is some fully connectedly that will be used for the last layer of

112
00:09:22.900 --> 00:09:28.950
the Arnon you know the fully connected layer comes at the very end of the record neural network.

113
00:09:29.050 --> 00:09:33.300
You have the elist himself first you know the stacked LACMA in the cell.

114
00:09:33.340 --> 00:09:38.080
But then at the end of the record a neural network you have the fully connected layer and therefore

115
00:09:38.110 --> 00:09:43.630
that's exactly what we're making right now we already have our decoder cell composed of the stacked

116
00:09:43.890 --> 00:09:46.980
S.M. layers with the drop out applied for each one.

117
00:09:47.020 --> 00:09:50.890
But now we need to make effectively that fully connected layer.

118
00:09:51.010 --> 00:09:55.630
And we're making this with this output function that we're making right now.

119
00:09:55.630 --> 00:09:58.180
All right so let's make that good news.

120
00:09:58.180 --> 00:10:01.380
We have a of function that will do that for us.

121
00:10:01.420 --> 00:10:05.320
And so the first thing I'm going to do here is get of flow first.

122
00:10:05.380 --> 00:10:13.930
Then again the contrib module and then this time since we are building the most classic neural network

123
00:10:13.930 --> 00:10:21.240
layer it's not going to be taken from Arnon it's going to be taken from simply layers layers and from

124
00:10:21.250 --> 00:10:27.130
layers we get the fully underscore connected function.

125
00:10:27.390 --> 00:10:28.050
All right.

126
00:10:28.120 --> 00:10:31.530
And this function now is going to take several arguments.

127
00:10:31.540 --> 00:10:31.840
All right.

128
00:10:31.840 --> 00:10:38.950
So the first argument is the inputs and the inputs are of course are viable X at this stage.

129
00:10:38.980 --> 00:10:44.190
So here I will share with you the fully connected function in the resources of this lecture.

130
00:10:44.200 --> 00:10:45.520
So check it out.

131
00:10:45.520 --> 00:10:49.030
Then the second argument is going to be the number of outputs.

132
00:10:49.030 --> 00:10:54.150
And that is the number of words and all the answers which is non-words here.

133
00:10:54.180 --> 00:11:02.280
So non-words then the next argument is the activation function and will keep the default value which

134
00:11:02.280 --> 00:11:11.320
is a loop then for the normalizer we want to normalization to specify here none then eventually we have

135
00:11:11.320 --> 00:11:20.620
to enter the scope which is our decoding and this course scope that we define here as this able scope

136
00:11:20.680 --> 00:11:21.980
rappings decoding.

137
00:11:22.150 --> 00:11:30.090
So decoding scope then next variable is going to be Waite's underscore initializers.

138
00:11:30.190 --> 00:11:35.920
So that's detents of flow argument name of this fully connected function Waite's initializers and that's

139
00:11:35.920 --> 00:11:43.570
of course equal to our Waite's that we just initialize here as a truncated normal distribution with

140
00:11:43.660 --> 00:11:45.820
standard deviation of 0.1.

141
00:11:45.890 --> 00:11:50.660
So weights then naturally comes to biases biases.

142
00:11:50.680 --> 00:11:57.190
But on the score initialiser to respect that tends to flow name biases initialiser and that's equal

143
00:11:57.190 --> 00:12:07.420
to our biases that we initialized as zeros biases and that's it that makes our function that I remind

144
00:12:07.420 --> 00:12:13.000
we'll create a fully connected layer which will be the last layer of our record neural network the one

145
00:12:13.000 --> 00:12:14.020
of the decoder.

146
00:12:14.230 --> 00:12:19.510
And if you want to take a step back you need to understand that this fully connected layers will get

147
00:12:19.660 --> 00:12:25.330
the features from the previous part of the deeper and more all the previous part of the neural network

148
00:12:25.330 --> 00:12:27.880
which is the stacked LACMA.

149
00:12:28.180 --> 00:12:33.780
And we'll return the final scores and then using a legit like a soft Max method.

150
00:12:33.850 --> 00:12:37.450
We will return the final prediction which will be the final answer.

151
00:12:37.510 --> 00:12:39.840
Just realized a little typo here.

152
00:12:39.910 --> 00:12:41.500
Waits like that.

153
00:12:41.560 --> 00:12:43.330
And now we're ready to go.

154
00:12:43.630 --> 00:12:43.980
All right.

155
00:12:43.990 --> 00:12:47.800
Now the next step is to get our training predictions.

156
00:12:47.910 --> 00:12:53.330
And that's going to be very easy because we have the right function for this which is not this time

157
00:12:53.380 --> 00:12:56.920
function by tens of global a function that we made ourself.

158
00:12:57.100 --> 00:13:05.020
And that's exactly the decode trainings that function that returns the output when decoding the training

159
00:13:05.020 --> 00:13:05.530
set.

160
00:13:05.530 --> 00:13:07.730
And that's exactly what we need right now.

161
00:13:07.750 --> 00:13:16.090
So I'm going to introduce a new variable here that will be our training underscore predictions and that

162
00:13:16.180 --> 00:13:22.010
will be equal to what is returned by this de-code training set function that we met.

163
00:13:22.180 --> 00:13:29.370
And therefore I'm calling this function to code underscore training and underscore course set.

164
00:13:29.400 --> 00:13:34.340
Now we simply need to enter all the arguments that we defined for this function.

165
00:13:34.510 --> 00:13:37.680
And so now I'm going to input them one by one very quickly.

166
00:13:37.720 --> 00:13:41.440
The first one is the interstate.

167
00:13:41.480 --> 00:13:44.420
The second one is the decoder cell.

168
00:13:44.420 --> 00:13:46.040
And of course cell.

169
00:13:46.070 --> 00:13:52.940
The third one is the decoder underscore embedded in this court input.

170
00:13:53.000 --> 00:13:58.090
The fourth one is sequence underscore length.

171
00:13:58.370 --> 00:14:09.610
Then the next one is the recoating scope next one output function then they keep drub parameter.

172
00:14:09.750 --> 00:14:14.150
And finally are that on score sites.

173
00:14:14.400 --> 00:14:14.900
Great.

174
00:14:14.910 --> 00:14:17.910
So that gives us our training predictions.

175
00:14:17.910 --> 00:14:20.190
And now guess what we're going to do.

176
00:14:20.310 --> 00:14:26.040
We're going to get our test predictions because we're going to do in the trainings some cross-validation

177
00:14:26.370 --> 00:14:32.970
that's going to keep separately 10 percent of the training data to test demurral on it's observations.

178
00:14:33.030 --> 00:14:35.800
But without using them back for the training.

179
00:14:35.800 --> 00:14:37.570
So that's for the cross-validation part.

180
00:14:37.620 --> 00:14:44.010
And also what we're about to make will be used to infer to predict our ultimate predations that is the

181
00:14:44.100 --> 00:14:45.310
answers of the chair.

182
00:14:45.330 --> 00:14:47.550
But after its training so there we go.

183
00:14:47.550 --> 00:14:55.400
Let's get our test predictions but just before that we have to take our decoding scope and specify that

184
00:14:55.400 --> 00:15:01.510
we want to reuse the variables that were introduced in this decoding scope.

185
00:15:01.580 --> 00:15:02.980
That's important to do it.

186
00:15:03.290 --> 00:15:06.410
And now we're ready to get the test predictions.

187
00:15:06.560 --> 00:15:13.580
And therefore I'm introducing a new variable here test underscore predictions and this time we will

188
00:15:13.580 --> 00:15:19.640
get them with the second function that we made which is the de-code tested function and that returns

189
00:15:19.640 --> 00:15:21.600
exactly the test predictions.

190
00:15:21.830 --> 00:15:28.340
So let's call this function right now decode underscored test and this core set.

191
00:15:28.490 --> 00:15:34.870
And again we're going to input all the arguments that we defined for this function here.

192
00:15:34.880 --> 00:15:36.580
That is all these guys here.

193
00:15:36.650 --> 00:15:42.200
I'm going to put them one by one efficiently and so feel free to inspect the Dakotas at function if

194
00:15:42.290 --> 00:15:45.320
you need more information on what we made before.

195
00:15:45.530 --> 00:15:51.260
So Dakota said the first arguments we need to answer here is like for the dico trainings that function

196
00:15:51.680 --> 00:15:55.690
it's the encoder state and Kirner underscore state.

197
00:15:55.880 --> 00:16:01.830
Then next one the decoder cell the carousel next one.

198
00:16:01.830 --> 00:16:05.790
Be careful it's not going to be the decoder embedded input.

199
00:16:06.000 --> 00:16:11.860
As for the training set but I highlighted that in the previous to toile it's going to be this time of

200
00:16:11.860 --> 00:16:18.120
the Khoder underscore and this underscore matrix.

201
00:16:18.140 --> 00:16:24.570
All right then next argument it's going to be the key identifier of the star of string token which we're

202
00:16:24.570 --> 00:16:29.150
going to get with our word to end dictionary.

203
00:16:29.370 --> 00:16:38.010
And now we need answer the key of this token which is in quotes and these symbols here S O S and that

204
00:16:38.040 --> 00:16:43.620
will give us exactly the key identifier mapped by the words in dictionary.

205
00:16:43.800 --> 00:16:52.290
And I'm going to copy this because our next argument is going to be the key identifier of this time

206
00:16:52.510 --> 00:16:56.090
that the O S tokened end of string token.

207
00:16:56.460 --> 00:17:07.290
Then our next argument will be the sequence underscore length but minus 1 to not include the last token

208
00:17:07.410 --> 00:17:08.540
as we did before.

209
00:17:08.850 --> 00:17:14.820
All right then next argument is going to be the total number of words which we get thanks to our arguments

210
00:17:15.160 --> 00:17:16.530
number words.

211
00:17:16.630 --> 00:17:25.030
Then it's going to be our day coding scope which we introduced at the very beginning of this decoder

212
00:17:25.080 --> 00:17:26.250
Arnon function.

213
00:17:26.370 --> 00:17:34.830
The code in scope and then almost a rule we have three arguments to go the next one is the out put on

214
00:17:34.860 --> 00:17:41.650
this core function then it's going to be to keep underscore prob drop out parameter.

215
00:17:41.700 --> 00:17:48.260
And finally the last one we always end up with this one it is the batch underscore size.

216
00:17:48.270 --> 00:17:51.560
We are always working with batch of inputs.

217
00:17:51.570 --> 00:17:54.550
That's a key element to apply here.

218
00:17:54.600 --> 00:17:55.830
When doing deep learning.

219
00:17:56.240 --> 00:17:56.690
All right.

220
00:17:56.720 --> 00:17:59.190
Bedsides that was our last argument.

221
00:17:59.190 --> 00:18:02.290
Now let's do a quick check to see if everything's all right.

222
00:18:02.310 --> 00:18:08.340
I'm seeing another typo that I made in this output function code subsection.

223
00:18:08.340 --> 00:18:13.330
It doesn't take and as you wait with an S initializer with no s.

224
00:18:13.410 --> 00:18:13.870
All right.

225
00:18:13.890 --> 00:18:16.230
And the same should be fine.

226
00:18:16.230 --> 00:18:23.130
So now what we're going to do is get rid of these two warnings here because basically they tell us that

227
00:18:23.130 --> 00:18:26.950
we haven't used training predictions and desperate actions.

228
00:18:27.030 --> 00:18:32.430
So it's time to use them and to use them there's nothing more simple because the exact thing that we

229
00:18:32.430 --> 00:18:38.010
have to do now and be careful to go back to the with level and not inside the with.

230
00:18:38.070 --> 00:18:47.730
Right now what we have to do is simply return our training predictions and our test predictions.

231
00:18:47.820 --> 00:18:48.860
And there we go.

232
00:18:48.910 --> 00:18:51.850
We're done with the decoder Arnon function.

233
00:18:52.140 --> 00:18:59.820
So now I think everything is all good so we can select the whole code section to define effectively

234
00:19:00.040 --> 00:19:02.330
decode Arnon function.

235
00:19:02.580 --> 00:19:04.440
And there we go it's now on the console.

236
00:19:04.440 --> 00:19:05.400
It's defined.

237
00:19:05.520 --> 00:19:09.240
So we're ready to build our final set to sec model.

238
00:19:09.240 --> 00:19:10.580
So congratulations.

239
00:19:10.620 --> 00:19:16.090
We just made the Dakoda on and function that builds the record on one work at the decoder.

240
00:19:16.200 --> 00:19:18.750
And so now have some excellent news for you.

241
00:19:18.790 --> 00:19:25.470
We are ready to assemble everything that is basically to record neural network of the encoder and the

242
00:19:25.470 --> 00:19:32.010
record renewal network and the decoder to form the whole the final sect to sect model.

243
00:19:32.100 --> 00:19:35.000
And that's exactly what we'll do in the next tutorial.

244
00:19:35.100 --> 00:19:36.880
Until then Id been on P.
