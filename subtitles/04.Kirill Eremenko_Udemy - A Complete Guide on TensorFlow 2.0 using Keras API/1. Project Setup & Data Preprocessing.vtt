WEBVTT
1
00:00:00.510 --> 00:00:07.650
Hello and welcome to Section 5 now on recurrent neural networks and how to build them with denser flow

2
00:00:07.660 --> 00:00:08.250
2.0.

3
00:00:08.760 --> 00:00:14.670
So we're gonna use again one of these benchmark data sets which allow to compare the performance of

4
00:00:14.670 --> 00:00:20.730
several models and this dataset is going to be a classic application of what you can do with an island

5
00:00:21.030 --> 00:00:23.140
which is text classification.

6
00:00:23.250 --> 00:00:31.200
Indeed since a record a new network is used for data sets having sequences of data well since a text

7
00:00:31.380 --> 00:00:37.350
is actually a sequence of words we can very well use an iron n for text classification.

8
00:00:37.470 --> 00:00:40.530
And indeed you will see in the end that the results are really good.

9
00:00:40.980 --> 00:00:48.570
And so this text classification is going to be about predicting if a review of a movie taken from the

10
00:00:48.690 --> 00:00:52.500
Internet Movie Database is positive or negative.

11
00:00:52.500 --> 00:00:54.810
More precisely here is the dataset.

12
00:00:54.810 --> 00:01:00.900
This is the I am to be movie reviews dataset and more precisely we'll have to predict if the rating

13
00:01:00.960 --> 00:01:06.510
of a movie on a scale from 1 to 10 is above 5 or below 5.

14
00:01:06.510 --> 00:01:12.000
So it will be a binary classification because it's like we're predicting if a review is positive if

15
00:01:12.000 --> 00:01:15.070
it is over five or negative if it is below five.

16
00:01:15.390 --> 00:01:16.530
So that's the story.

17
00:01:16.560 --> 00:01:22.380
That's what we have to predict and the inputs will simply be reviews which are therefore sequences of

18
00:01:22.380 --> 00:01:27.570
words and which we will be able to feed in a record and new network.

19
00:01:27.570 --> 00:01:30.570
So check this out for more details if you want.

20
00:01:30.570 --> 00:01:37.440
It's basically a data set that contains 50000 reviews split evenly into 25000 reviews in the training

21
00:01:37.440 --> 00:01:40.270
set and 25000 reviews in a test.

22
00:01:40.320 --> 00:01:46.830
And so we will first train that record a new network to predict if each of these 25000 reviews is positive

23
00:01:46.860 --> 00:01:53.850
or negative and then we'll evaluate it on the set and we'll compute the accuracy on these 25000 reviews

24
00:01:54.030 --> 00:01:55.250
of the test set.

25
00:01:55.320 --> 00:02:01.500
So let's do this let's build our record Renewal Network with denser flow 2.0 starting with the first

26
00:02:01.500 --> 00:02:05.380
step where we install the dependencies and set up GPO environment.

27
00:02:05.380 --> 00:02:07.680
That's because we install tens of loads of you.

28
00:02:07.710 --> 00:02:12.900
So that's still the same Pip command but actually you don't have to install it because it is now already

29
00:02:12.900 --> 00:02:16.090
included in the Jupiter notebooks of Google collab.

30
00:02:16.560 --> 00:02:22.540
Then step two is to import the libraries and for this we actually only need tensor flow.

31
00:02:22.620 --> 00:02:24.720
And once again the carries datasets.

32
00:02:24.810 --> 00:02:25.380
Right.

33
00:02:25.380 --> 00:02:29.290
Same as the artificial new one network in the convolution or new network.

34
00:02:29.310 --> 00:02:35.910
We're taking this I am deeply movie reviews data set from the datasets module by Caris library taken

35
00:02:35.910 --> 00:02:38.100
from tensor flow and once again.

36
00:02:38.100 --> 00:02:42.870
Well yes just to check the version we are in tend to flow 2.0 Pono.

37
00:02:43.380 --> 00:02:50.980
So let's now move on to the real beginning of this application which is always the same phase data processing.

38
00:02:51.000 --> 00:02:51.290
All right.

39
00:02:51.300 --> 00:02:57.760
So for tax classification we have of course a different kind of data pricing to perform.

40
00:02:57.780 --> 00:03:01.650
Indeed with our artificial new networking convolution all new network.

41
00:03:01.650 --> 00:03:03.190
We worked with images.

42
00:03:03.300 --> 00:03:04.830
Now we're working with text.

43
00:03:04.890 --> 00:03:11.340
So it's actually another technique to pre process the reviews and the main thing we'll have to do is

44
00:03:11.460 --> 00:03:18.420
to pad all the sequences to be the same length because you know when we load the reviews of the MTV

45
00:03:18.500 --> 00:03:19.340
data set.

46
00:03:19.350 --> 00:03:22.680
Well of course the reviews don't have the same length.

47
00:03:22.800 --> 00:03:29.470
You know you can have a reviews with only three or forwards and large reviews with 20 words or 30 words.

48
00:03:29.610 --> 00:03:36.690
And whenever you're feeding a record a new network with text well all the input text must have the same

49
00:03:36.690 --> 00:03:37.440
length.

50
00:03:37.440 --> 00:03:43.530
So the main thing that we have to do is this padding which consists of adding some extra sales in the

51
00:03:43.530 --> 00:03:50.130
future tense of inputs containing all the texts so that all the input reviews have the same length and

52
00:03:50.370 --> 00:03:56.430
you have here in the parameters which is our first step step here we choose the number of words we want

53
00:03:56.430 --> 00:04:04.230
to import from the aim to be movie reviews data set and this max length which is exactly the total length

54
00:04:04.410 --> 00:04:06.330
of the sequence after the padding.

55
00:04:06.390 --> 00:04:12.990
So the maximum length will be 100 which means that for example if you have a review with only five words

56
00:04:13.360 --> 00:04:19.860
Well the review will still be represented in a sequence of 100 elements where the first five elements

57
00:04:20.040 --> 00:04:25.800
are the five words of the review and then the other ninety five elements are just the same sales with

58
00:04:25.980 --> 00:04:33.870
usually a bad token just to specify that the rest of the sequence you know from five to 100 was applied

59
00:04:33.960 --> 00:04:34.770
the padding.

60
00:04:34.770 --> 00:04:35.070
OK.

61
00:04:35.100 --> 00:04:42.510
So basically all our reviews will be sequences of 100 elements and we will specify this later on when

62
00:04:42.510 --> 00:04:44.350
padding the sequences to be the same length.

63
00:04:44.370 --> 00:04:50.790
But first we of course have to load the IMF data set and you will recognize the format and that's of

64
00:04:50.790 --> 00:04:57.550
course because we use the same low data method from I MTBE which is taken from the same carries data

65
00:04:57.540 --> 00:04:58.220
sets.

66
00:04:58.230 --> 00:05:05.150
So once again very easy to load the data set here but in the low data we have to specify the maximum

67
00:05:05.150 --> 00:05:06.710
number of words we want to have.

68
00:05:06.710 --> 00:05:12.980
Among our reviews and that is twenty thousand which means that we're not taking all the reviews from

69
00:05:12.980 --> 00:05:14.890
the I am to be data set.

70
00:05:14.990 --> 00:05:21.050
We're actually taking all the reviews that have the 20000 most frequent words right.

71
00:05:21.050 --> 00:05:23.010
So that will ease a bit.

72
00:05:23.030 --> 00:05:24.560
The training process.

73
00:05:24.680 --> 00:05:31.400
All right so once again using this low data method gets us the training set separately an extra in the

74
00:05:31.460 --> 00:05:37.880
input reviews and why train their labels saying whether or not the reviews are positive or negative.

75
00:05:38.000 --> 00:05:44.150
And same for the test set composed of X test containing all the input reviews and Y tests containing

76
00:05:44.150 --> 00:05:47.660
the labels positive or negative of these reviews.

77
00:05:47.710 --> 00:05:50.540
Okay so we loaded the data set and now here we go.

78
00:05:50.540 --> 00:05:56.720
Time to pad all the sequences to be the same length and so as we said we were going to make sure that

79
00:05:57.170 --> 00:06:04.000
our reviews after being battered will have 100 elements and the rest of the elements of the review doesn't

80
00:06:04.010 --> 00:06:09.840
have 100 elements will just be bad tokens just to finish the sequence up to 100.

81
00:06:10.010 --> 00:06:11.150
And so how do we pad.

82
00:06:11.150 --> 00:06:18.500
Well we have once again a very easy to use function taken from the sequence sub module by the pre processing

83
00:06:18.500 --> 00:06:25.760
module by the caris library through tensor flow and this batch sequences function takes all your reviews

84
00:06:25.820 --> 00:06:32.150
in extreme therefore with different lengths and of course the maximum length you want your reviews to

85
00:06:32.150 --> 00:06:33.830
have after the padding.

86
00:06:33.830 --> 00:06:34.440
OK.

87
00:06:34.580 --> 00:06:39.350
So very easy thanks to carers of course and then same for the test set.

88
00:06:39.350 --> 00:06:47.280
Well we pad all the sequences so that all the reviews index tests are sequences of 100 elements.

89
00:06:47.500 --> 00:06:48.140
All right.

90
00:06:48.140 --> 00:06:49.760
And that's actually it.

91
00:06:50.030 --> 00:06:55.880
That's all you need to do when working with of course a carer's data set in the MLP branch of artificial

92
00:06:55.880 --> 00:06:56.890
intelligence.

93
00:06:56.900 --> 00:06:58.850
So now we're going to move on to the next step.

94
00:06:58.850 --> 00:07:01.640
Step for building a renewal network.

95
00:07:01.760 --> 00:07:07.850
Make sure to see the theory first because you have to understand how Ireland's work and learn and what

96
00:07:07.910 --> 00:07:09.830
LSD and tsar and otherwise.

97
00:07:10.010 --> 00:07:15.470
If you already know everything about this well feel free to jump to the next story or where we'll start

98
00:07:15.470 --> 00:07:18.830
building or recommend new network with tensor flow 2.0.
