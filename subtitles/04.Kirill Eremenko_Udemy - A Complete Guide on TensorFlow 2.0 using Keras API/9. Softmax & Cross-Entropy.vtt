WEBVTT
1
00:00:00.330 --> 00:00:02.990
Hello and welcome back to the course on deep learning.

2
00:00:03.030 --> 00:00:08.540
This is an additional tutorial to talk about the soft Maxim cross entropy functions.

3
00:00:08.610 --> 00:00:15.300
It is not 100 percent necessary in order for you to go through all of the parts that we've been through

4
00:00:15.300 --> 00:00:21.510
in the main part of this section where we're talking mother convolution on neural networks but at the

5
00:00:21.510 --> 00:00:26.520
same time I thought it would be a good addition to your bag of knowledge and skill set.

6
00:00:26.520 --> 00:00:30.610
So let's go ahead and dig into these functions.

7
00:00:30.780 --> 00:00:37.470
So to start off with what we have here is the conclusion of neural network that we built in the main

8
00:00:37.470 --> 00:00:44.220
part of the section and then at the end it pops out some probabilities for zero point ninety five for

9
00:00:44.220 --> 00:00:47.970
a dog and 0 0 5 5 4 5 percent for a cat.

10
00:00:47.970 --> 00:00:53.160
Given that photo in the left as an input this is after the train has been conducted this is actually

11
00:00:53.450 --> 00:00:57.270
it's running and it's classifying a certain image.

12
00:00:57.330 --> 00:01:03.030
And so the question here is how come these two values add up to one because as far as we know from everything

13
00:01:03.080 --> 00:01:09.990
we've learned about artificial neural networks there is nothing to say that these two final neurons

14
00:01:10.020 --> 00:01:11.670
are connected between each other.

15
00:01:11.670 --> 00:01:16.530
So how would they know what the value of the hold each one of them know what the value of the other

16
00:01:16.530 --> 00:01:17.230
one is.

17
00:01:17.340 --> 00:01:19.890
And how would they know to add their values up to one.

18
00:01:20.280 --> 00:01:22.200
Well the answer is they wouldn't.

19
00:01:22.200 --> 00:01:29.160
In the classic version of artificial neural network and the only way that they do is because we introduce

20
00:01:29.160 --> 00:01:33.870
a special function called the soft max function in order to help us out of the situation.

21
00:01:33.900 --> 00:01:40.700
So normally what would happen is the dog and the cat neurons would have any kind of real values.

22
00:01:41.430 --> 00:01:48.090
They don't have to be they don't have to add up to one but then we would apply the soft max function

23
00:01:48.420 --> 00:01:50.710
which is written up over there at the top.

24
00:01:50.880 --> 00:01:56.330
And that would bring these values to be between 0 and 1 and it would make them add up to 1.

25
00:01:56.340 --> 00:02:03.210
And to quote Wikipedia the source max function or the normalized exponential function is a generalization

26
00:02:03.210 --> 00:02:10.230
of the logistic function that quote unquote squashes a k dimensional vector of arbitrary real values

27
00:02:10.230 --> 00:02:15.300
to a k dimensional vector of real values in the range of 0 to 1 that add up to 1.

28
00:02:15.300 --> 00:02:17.610
So basically it does exactly what we want.

29
00:02:17.640 --> 00:02:22.640
It brings these values to be between 0 and 1 and makes sure that they add up to 1.

30
00:02:22.890 --> 00:02:27.720
And the way it works is that the way that is this is possible is that because at the bottom we're here

31
00:02:27.720 --> 00:02:29.940
you can see that there is a summation.

32
00:02:29.940 --> 00:02:38.130
So it takes the exponent and puts it in the power of Z and adds it up so Z wants it to across all of

33
00:02:38.130 --> 00:02:38.760
your classes.

34
00:02:38.790 --> 00:02:39.830
All of these values.

35
00:02:39.930 --> 00:02:43.420
And so that's your normalization happening right there.

36
00:02:44.340 --> 00:02:51.240
So that's how the soft max function works and it makes sense to introduce the soft max function into

37
00:02:51.510 --> 00:02:53.040
convolution all neural networks.

38
00:02:53.040 --> 00:03:01.590
Because how strange would it be if you had a possible classes of a dog and a cat and for the dog class

39
00:03:01.590 --> 00:03:08.590
you had probability of 80 percent and for the cat class you had a probability of 45 percent right.

40
00:03:08.600 --> 00:03:14.700
It just doesn't make sense like that and therefore it's much better when you introduce a soft max function

41
00:03:14.700 --> 00:03:19.660
and that's what you'll find happening most of the time in convolution all neural networks.

42
00:03:19.680 --> 00:03:25.950
Now the other thing is that the soft max function comes hand-in-hand with something called The Cross

43
00:03:26.040 --> 00:03:30.560
entropy function and it's a very handy thing for us so let's first look at the formula.

44
00:03:30.570 --> 00:03:32.810
This is what the cross entropy function looks like.

45
00:03:33.090 --> 00:03:38.760
We're actually going to be using a different calculation we're gonna be using this representation of

46
00:03:38.760 --> 00:03:39.360
the Cross century.

47
00:03:39.360 --> 00:03:40.620
But the result is basically the same.

48
00:03:40.620 --> 00:03:42.300
This is just easier to calculate.

49
00:03:42.510 --> 00:03:49.170
And what I know this might sound very unrelated to anything right now just formulas on your screen but

50
00:03:49.810 --> 00:03:54.270
there will be some additional recommended reading at the end of this section so don't worry if you're

51
00:03:54.540 --> 00:03:56.370
not picking up on the math.

52
00:03:56.540 --> 00:03:58.320
Even if we haven't explained the math right now.

53
00:03:58.320 --> 00:04:05.430
But the point here is that what is the cross entropy will across entropy function remember how we previously

54
00:04:05.430 --> 00:04:13.380
in artificial neural networks we had a function called the mean a squared error function which we used

55
00:04:13.410 --> 00:04:21.450
as the cost function for assessing our network performance and our goal was to minimize the MSE in order

56
00:04:21.450 --> 00:04:23.870
to optimize our network performance.

57
00:04:23.880 --> 00:04:25.400
Well that was our cost function.

58
00:04:25.420 --> 00:04:34.170
Then there there and in convolution of neural networks we can still use MSE but a better option in convolution

59
00:04:34.170 --> 00:04:39.450
or neural networks after you apply the soft max function turns out to be the cross entropy function

60
00:04:39.780 --> 00:04:45.660
and in convolution on neural networks when you apply the cross injury functions not cost called the

61
00:04:45.660 --> 00:04:49.410
cost function anymore it's called the loss function and they they're very similar.

62
00:04:49.410 --> 00:04:55.050
They're just little terminal logical differences and little like little bit different and all what they

63
00:04:55.260 --> 00:04:55.460
mean.

64
00:04:55.470 --> 00:04:58.370
But for all purposes it's pretty much the same thing.

65
00:04:58.380 --> 00:05:07.440
And what happens is the lost function is again something that we want to minimize in order to maximize

66
00:05:07.500 --> 00:05:09.600
the performance of our network.

67
00:05:09.630 --> 00:05:15.180
So let's have a look look at a quick example on how of how this function can be applied.

68
00:05:15.210 --> 00:05:19.260
So let's say we've put an image of a dog entire network.

69
00:05:19.590 --> 00:05:24.440
The predicted value for dog is zero point nine and this is during the training.

70
00:05:24.450 --> 00:05:27.230
So we know that we know the label that is a dog.

71
00:05:27.270 --> 00:05:29.400
So the predictive value is zero point nine.

72
00:05:29.400 --> 00:05:32.260
The predictive value for cat is zero point one.

73
00:05:32.310 --> 00:05:37.920
Then here we have the label so we know it's a dog because this is training and one for dogs or for Cat.

74
00:05:37.920 --> 00:05:47.760
And so in this case you need to use use to plug these numbers into your formula for the cross entropy.

75
00:05:47.760 --> 00:05:55.230
So how you do it is the values on the left go into the variable Q The one that is under the logarithm

76
00:05:55.260 --> 00:06:00.600
in the on the right side and the values from the right would go into P and so it's important to remember

77
00:06:01.080 --> 00:06:06.000
which one goes there where because if you get them wrong you'd want to be taking a logarithm from a

78
00:06:06.570 --> 00:06:09.390
from a zero value and or a log in from a one.

79
00:06:09.570 --> 00:06:11.550
So you just want to plug them in.

80
00:06:11.670 --> 00:06:16.980
Make sure you plug them into the correct places and then you basically add that up.

81
00:06:16.980 --> 00:06:20.370
So that's how the cross century works and we'll look at a.

82
00:06:20.430 --> 00:06:26.250
Actually right now we're just going to look at a specific step by step example of applying this function

83
00:06:26.250 --> 00:06:28.600
in real life and it'll kind of make make more sense.

84
00:06:28.620 --> 00:06:35.760
What's cross entropy is and it'll be less like my goal in this material is to make you more comfortable

85
00:06:35.760 --> 00:06:42.310
of cross century because it can sound very convoluted and no pun intended.

86
00:06:42.780 --> 00:06:45.990
It can like convolution or neural networks.

87
00:06:46.050 --> 00:06:50.790
It can sound very complex right scary but it's not.

88
00:06:50.790 --> 00:06:51.600
That's that's the point.

89
00:06:51.600 --> 00:06:54.030
So let's go ahead and apply it just so we know that it's not scary.

90
00:06:54.030 --> 00:07:00.720
So here's the neural net and also this will explain why we're doing this why we're looking into different

91
00:07:00.720 --> 00:07:01.340
class functions.

92
00:07:01.710 --> 00:07:06.600
So neural network y neural network to let's say we have two neural networks and then we pass an image

93
00:07:06.600 --> 00:07:07.780
of a dog.

94
00:07:08.400 --> 00:07:12.020
And we know that this is a dog and not a cat.

95
00:07:12.120 --> 00:07:17.870
And then we have another image of a cat this time an animal and it's a cat not a dog.

96
00:07:17.870 --> 00:07:22.410
And here we have a weird looking animal which is in fact a dog and not a cat.

97
00:07:22.410 --> 00:07:24.090
If you look very closely.

98
00:07:24.270 --> 00:07:29.400
So we want to see what our neural networks were it will predict in the first case neural network one

99
00:07:29.460 --> 00:07:36.990
90 percent dog 10 percent cats correct neural network number two 60 percent dog 40 percent cat still

100
00:07:36.990 --> 00:07:37.550
correct.

101
00:07:37.620 --> 00:07:40.170
Worse but correct.

102
00:07:40.230 --> 00:07:42.670
Second option.

103
00:07:42.840 --> 00:07:48.450
First neural network 10 percent cat dog 90 percent cat correct neural network number.

104
00:07:48.550 --> 00:07:53.470
30 percent dog 70 percent cat worse but still correct.

105
00:07:53.520 --> 00:07:58.320
And then finally neural network one in image 3 neural network one.

106
00:07:58.380 --> 00:08:02.990
40 percent dog 60 percent cat incorrect neural network Number Two.

107
00:08:03.510 --> 00:08:08.220
10 percent dog 90 percent cat incorrect and worse.

108
00:08:08.220 --> 00:08:14.430
So the key here is that even though both networks got it wrong in the last one through out all three

109
00:08:14.670 --> 00:08:18.830
images neural network while one was outperforming neural network two.

110
00:08:18.840 --> 00:08:26.940
So even in the last case it was very it had a it gave dog like a 40 percent chance as opposed to neural

111
00:08:26.940 --> 00:08:29.070
network to only give dog a 10 percent chance.

112
00:08:29.070 --> 00:08:34.800
So neural network 1 is outperforming across the board when compared to a neural network too.

113
00:08:35.540 --> 00:08:41.730
And so now we're going to look at the functions that they can measure performance that we've kind of

114
00:08:41.730 --> 00:08:42.910
talked about the.

115
00:08:42.990 --> 00:08:44.780
So let's put these into a table.

116
00:08:44.790 --> 00:08:48.300
So there is a neural network one you have the wrong number.

117
00:08:48.300 --> 00:08:49.480
So that's the image number.

118
00:08:49.490 --> 00:08:53.860
And then for image one you have what it predicted 90 percent dog temps and cat.

119
00:08:54.060 --> 00:08:57.240
So there's the hat variables and then you have the actual value.

120
00:08:57.270 --> 00:09:00.420
So Dog correct cat incorrect.

121
00:09:00.510 --> 00:09:03.340
Same thing for image number two.

122
00:09:03.390 --> 00:09:05.010
And same thing for a minimum of three.

123
00:09:05.190 --> 00:09:07.650
And same for neural network number two.

124
00:09:07.650 --> 00:09:10.980
So a dog 60 percent cats 40 percent in the first image.

125
00:09:10.980 --> 00:09:12.050
That's what it predicted.

126
00:09:12.090 --> 00:09:18.000
Correct answer as dog not a cat and so on and so now let's see what errors we can actually get.

127
00:09:18.000 --> 00:09:24.860
So what errors we can calculate to estimate the performance and monitor the performance of our networks.

128
00:09:24.900 --> 00:09:33.000
So one type of error is called the classification error and that is basically just asking it did you

129
00:09:33.000 --> 00:09:37.620
get it right or not regardless of the probabilities is just did you get it right or did you not get

130
00:09:37.620 --> 00:09:37.890
it right.

131
00:09:37.920 --> 00:09:44.790
So in both cases for both neural networks each of them they've got one 0.

132
00:09:44.790 --> 00:09:46.290
So this is how many they go wrong.

133
00:09:46.290 --> 00:09:48.410
So they got one out of three wrong.

134
00:09:48.420 --> 00:09:54.630
So thirty three percent error rate for neural network 1 and 33 percent error rate for neural network

135
00:09:54.630 --> 00:09:59.700
2 as a basically from this standpoint both neural networks perform at the same level but we know that's

136
00:09:59.700 --> 00:10:00.170
not true.

137
00:10:00.180 --> 00:10:04.960
We know that neural network Guan is outperforming neural network too.

138
00:10:05.070 --> 00:10:10.770
That's why a classification error is not a good measure especially for the purposes of back propagation

139
00:10:11.640 --> 00:10:13.720
mean squared error different.

140
00:10:13.740 --> 00:10:19.050
And by the way I did these calculations in Excel I just didn't want to bore you with them but you can

141
00:10:19.050 --> 00:10:21.930
totally just sit down and do them on a paper or an Excel.

142
00:10:21.930 --> 00:10:28.710
These are very straightforward calculations is basically take the sum of squared errors and then just

143
00:10:28.710 --> 00:10:32.910
take the average across your across your observations.

144
00:10:32.910 --> 00:10:34.890
And that's blessed pretty much it.

145
00:10:35.010 --> 00:10:42.420
So for the hearing at network 1 you get 25 percent for neural network to you get seventy one percent

146
00:10:42.820 --> 00:10:43.280
error rate.

147
00:10:43.300 --> 00:10:48.670
So as you can see this one is more accurate it's telling us that neural net everyone has a much lower

148
00:10:48.670 --> 00:10:52.920
error rate the neural network 2 and then cross entropy again.

149
00:10:52.930 --> 00:10:53.800
We've seen the formula.

150
00:10:53.800 --> 00:10:58.300
You can also calculate this this is actually even easier to calculate than the mean squared error cross

151
00:10:58.300 --> 00:11:05.200
error across entropy gives you 38 percent for neural network 1 and 1.0 6 for neural network 2.

152
00:11:05.470 --> 00:11:07.940
So you can see the results are a bit different.

153
00:11:08.260 --> 00:11:16.160
When you look at them like that when you look at the mean square error and cross entropy.

154
00:11:16.180 --> 00:11:26.050
The question of why would you use cross entropy over mean the squared error isn't just about the kind

155
00:11:26.050 --> 00:11:27.880
of like the numbers that this but this.

156
00:11:27.880 --> 00:11:33.850
These calculations were just to show you that this is all it's all doable you can just do it on a paper.

157
00:11:33.940 --> 00:11:34.780
It's not.

158
00:11:34.780 --> 00:11:37.830
It is a not very intense mathematics.

159
00:11:37.840 --> 00:11:41.020
These are pretty pretty simple straightforward things.

160
00:11:41.140 --> 00:11:46.180
But the question of why would you use Minsk cross entropy over mean squared error.

161
00:11:46.180 --> 00:11:48.190
It's a very very good question to ask.

162
00:11:48.190 --> 00:11:49.720
I'm glad you asked it.

163
00:11:50.110 --> 00:12:00.550
The answer to that is like there's several advantages of cross entropy over mean squared era which are

164
00:12:00.550 --> 00:12:01.350
not obvious.

165
00:12:01.390 --> 00:12:07.080
And so I'll I'll mention a couple but other then I'll I'll let you know where you can find out more

166
00:12:07.080 --> 00:12:07.860
so.

167
00:12:08.320 --> 00:12:18.820
One of them is that if if for instance your at the very start of your back propagation your output value

168
00:12:18.820 --> 00:12:22.330
is very very very very tiny very tiny.

169
00:12:22.330 --> 00:12:25.660
So it's much smaller than the actual value that you want.

170
00:12:25.660 --> 00:12:32.860
Then at the very start the gradient in your gradient descent will be very very low and you it won't

171
00:12:32.860 --> 00:12:40.180
be enough it'll be very hard for the neural network to actually start doing something and start moving

172
00:12:40.180 --> 00:12:44.800
around and start adjusting those weights and start moving start actually moving in the right direction

173
00:12:45.070 --> 00:12:50.830
whereas when you use something like the cross entropy because it's got that logarithm in it it actually

174
00:12:51.370 --> 00:12:57.460
helps the network assess even a small area like that and do something about it.

175
00:12:57.490 --> 00:12:58.450
Here's how to think about it.

176
00:12:58.450 --> 00:13:03.190
So let's say in again this is very very intuitive approach.

177
00:13:03.190 --> 00:13:08.200
There's there's there's gonna be a link to the mathematics and you can derive these things through the

178
00:13:08.200 --> 00:13:11.130
mathematics in more detail but a very intuitive approach.

179
00:13:11.170 --> 00:13:17.600
Let's say your like your outcome that you want is is one.

180
00:13:17.650 --> 00:13:22.720
And right now you are at 1 one millionth of one.

181
00:13:22.730 --> 00:13:25.090
You're right zero point zero zero zero zero zero one.

182
00:13:25.240 --> 00:13:32.750
And then you improve next time you improve your outcome from from one millionth to one thousandth.

183
00:13:32.770 --> 00:13:40.210
And in terms of if you calculate the squared error you just subtracting one from the other or basically

184
00:13:40.210 --> 00:13:44.890
in each case you calculate the square error and you'll see that the squared errors when you compare

185
00:13:44.890 --> 00:13:46.720
one case versus the other.

186
00:13:46.720 --> 00:13:48.130
It didn't change that much.

187
00:13:48.150 --> 00:13:52.590
You didn't improve your network that much when you're looking at the mean squared error but if you're

188
00:13:52.600 --> 00:13:59.590
looking at the cross entropy because you're taking a logarithm and then you're comparing the two dividing

189
00:13:59.590 --> 00:14:06.910
one to the other you will see that you have actually improved your network significantly so you that

190
00:14:07.230 --> 00:14:13.000
that jump from one millionth to one thousandth in means squared error terms will be very low it will

191
00:14:13.000 --> 00:14:20.890
be insignificant and it won't it won't guide your gradient boosting process or your back propagation

192
00:14:20.890 --> 00:14:25.450
in the right direction it will it will it will guide it in the right direction but it'll be like a very

193
00:14:25.450 --> 00:14:29.390
slow guidance it won't have enough power.

194
00:14:29.620 --> 00:14:34.900
Whereas if you do that across entropy cross entropy will understand that oh even though these are very

195
00:14:34.900 --> 00:14:42.190
small adjustments that are just you know making a tiny change in absolute terms in relative terms it's

196
00:14:42.190 --> 00:14:43.680
a huge improvement.

197
00:14:43.810 --> 00:14:46.060
And we we are definitely going in the right direction.

198
00:14:46.060 --> 00:14:47.170
Let's keep going that way.

199
00:14:47.170 --> 00:14:57.490
So cross entropy will help your neural network get to the right gets to the optimal state is a better

200
00:14:57.490 --> 00:15:01.030
way for the neural network to get to get to an optimal state.

201
00:15:01.030 --> 00:15:08.200
But bear in mind that this only works when the cross entropy is only the preferred method only for classification.

202
00:15:08.200 --> 00:15:14.170
So if you're talking about things like regression like which we had an artificial neural networks then

203
00:15:14.200 --> 00:15:20.690
you would rather go with me and squared error whereas cross entropy is better for classification and

204
00:15:20.740 --> 00:15:23.630
again it has to do with the fact that we're using soft make functions.

205
00:15:23.640 --> 00:15:26.830
So that's a kind of intuitive explanation of that.

206
00:15:26.950 --> 00:15:32.110
A good place to learn a bit more about that if you're really interested in you know why are we using

207
00:15:32.630 --> 00:15:35.140
cross entropy versus mean squared error.

208
00:15:35.260 --> 00:15:42.840
Google a video by Geoffrey Hinton called the soft max output function and he explains it very well.

209
00:15:42.870 --> 00:15:48.830
And you know being the godfather of deep learning who can explain it better anyway.

210
00:15:48.840 --> 00:15:51.630
And by the way any video by Geoffrey Hinton is golden.

211
00:15:51.630 --> 00:15:58.540
He's just got a huge talent for explaining things anyway so that's that's soft Max versus cross entropy

212
00:15:58.560 --> 00:16:02.070
I hope that gives you kind of like an intuitive understanding of what's going on here.

213
00:16:02.070 --> 00:16:07.950
But more importantly that you're not put off by the term cross entropy because hard line will mention

214
00:16:07.950 --> 00:16:12.540
it in the practical serials and I wanted to make sure that you're prepared for that and it's it's just

215
00:16:12.540 --> 00:16:18.780
another way of calculating your loss function and another way of optimizing your network which is specifically

216
00:16:18.780 --> 00:16:25.560
tailored to classification problems and therefore can evolutionary neural networks and comes in hand-in-hand

217
00:16:25.860 --> 00:16:27.570
with the soft max function.

218
00:16:28.230 --> 00:16:35.430
So additional reading if you'd like a light introduction into cross entropy if you're interested in

219
00:16:35.430 --> 00:16:40.290
the cross entropy a bit more of course a good article to check out is called a friendly introduction

220
00:16:40.290 --> 00:16:43.740
to cross entropy loss by Rob DiPietro.

221
00:16:44.190 --> 00:16:47.100
2016 here's the link below.

222
00:16:47.100 --> 00:16:51.260
Very very nice very soft to nothing.

223
00:16:51.270 --> 00:16:54.420
No no super complex math.

224
00:16:54.420 --> 00:16:59.610
Good analogies good examples using analogies of cars and you look at cars and talks about information

225
00:16:59.610 --> 00:17:04.300
and bits and restrictions and you know how would you tackle this how would you call that it's.

226
00:17:04.400 --> 00:17:09.990
So it's a good article to have a look at and we'll give you a good overview of across entropy like from

227
00:17:09.990 --> 00:17:16.620
an introductory standpoint if you want to dig into the heavy math like what you see here then check

228
00:17:16.620 --> 00:17:23.970
out an article by or a blog by how to implement a neural network Intermezzo to so interim like is like

229
00:17:23.970 --> 00:17:31.230
an intermediary thing like a int. intermediates in you know like when you go to a theater and you have

230
00:17:31.230 --> 00:17:37.770
like a break between the first part and the second part so that because he's like going through all

231
00:17:37.770 --> 00:17:42.070
these steps and then he's like and then he says I got to explain this first.

232
00:17:42.420 --> 00:17:47.340
And yeah so that's why it's called Intermezzo no other reason as far as I understand the articles by

233
00:17:47.340 --> 00:17:55.530
Peter Rowland's 2016 as well so both are quite recent and Yo check out this if you would like to dig

234
00:17:55.530 --> 00:18:02.220
into the mathematics behind cross entropy behind a soft Max and cross entropy in this article actually.

235
00:18:02.850 --> 00:18:03.730
So there we go.

236
00:18:03.780 --> 00:18:07.300
That's all there is to these two.

237
00:18:07.320 --> 00:18:12.750
Hopefully I was able to add some more additional clarity and good luck with that.

238
00:18:12.760 --> 00:18:16.920
It's gonna be fun and enjoy the practical tutorials.

239
00:18:16.920 --> 00:18:18.030
I'll see you next time.

240
00:18:18.030 --> 00:18:19.650
Until then enjoy deep learning.
