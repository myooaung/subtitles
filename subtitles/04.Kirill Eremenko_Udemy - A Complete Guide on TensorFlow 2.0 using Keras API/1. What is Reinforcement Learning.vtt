WEBVTT
1
00:00:00.780 --> 00:00:04.530
Hello and welcome back to the courts on artificial intelligence.

2
00:00:04.530 --> 00:00:09.480
I hope you're excited about today's tutorial because we are taking a very first step into the world

3
00:00:09.480 --> 00:00:13.190
of A.I. and today we're talking about reinforcement learning.

4
00:00:13.230 --> 00:00:18.720
It's a very important story because it will underpin everything else is going to be happen in this course.

5
00:00:18.720 --> 00:00:22.960
So let's get started here we've got a little maze.

6
00:00:22.980 --> 00:00:27.990
And this maze is our representation of an environment and that's what we're going to be dealing with

7
00:00:28.560 --> 00:00:29.160
in this course.

8
00:00:29.160 --> 00:00:33.990
We're going to be dealing with certain environments in which our artificial intelligence is going to

9
00:00:33.990 --> 00:00:35.090
be performing.

10
00:00:35.100 --> 00:00:40.050
It's going to be taking actions it's going to be looking to beat these environments going she'll be

11
00:00:40.050 --> 00:00:42.110
looking to win in these environments.

12
00:00:42.300 --> 00:00:44.170
And here we've got an agent.

13
00:00:44.280 --> 00:00:46.980
The agent is our artificial intelligence.

14
00:00:46.980 --> 00:00:52.860
That's the person or that's the mind that's going to be navigating these environments and learning from

15
00:00:52.920 --> 00:00:57.100
the feedback that their minds are going to be giving it in order to perform certain actions.

16
00:00:57.120 --> 00:01:02.310
And so the way it works is the agent performs certain actions in this environment.

17
00:01:02.310 --> 00:01:06.180
And as a result the state in which it is in will change.

18
00:01:06.180 --> 00:01:11.400
So it might be further or closer or more to the left more to the right it might have a certain other

19
00:01:11.400 --> 00:01:15.150
other parameters that describe its state and those parameters are going to change.

20
00:01:15.180 --> 00:01:21.020
The state is going to change because of the action takes and it will also get rewards based on the actions

21
00:01:21.030 --> 00:01:24.870
every time it takes an action the state will change and it'll get a reward.

22
00:01:24.870 --> 00:01:28.230
Now bear in mind sometimes it might happen that it won't change the state.

23
00:01:28.240 --> 00:01:33.970
The action won't change a state or there won't be a reward for taking that action in that certain state

24
00:01:33.970 --> 00:01:34.390
it was.

25
00:01:34.620 --> 00:01:38.400
But nevertheless the agents going to keep doing that is going to be taking actions changing the state

26
00:01:38.400 --> 00:01:40.850
getting rewards changing action taking actions.

27
00:01:40.860 --> 00:01:45.950
Changing the state and getting rewards and by doing that process it's going to be learning about the

28
00:01:45.960 --> 00:01:51.030
environment's going to be exploring the environment understanding what actions lead to good rewards

29
00:01:51.120 --> 00:01:55.940
and favorable states and what actions lead to bad rewards and unfavorable states.

30
00:01:55.950 --> 00:01:59.610
And this is a very simplistic representation of a very global problem.

31
00:01:59.640 --> 00:02:04.340
So if you think about it environments actually don't have to be just mazes.

32
00:02:04.350 --> 00:02:10.680
It's not just about getting out of a maze or finding a treasure in a maze an environment can be pretty

33
00:02:10.680 --> 00:02:11.700
much anything in life.

34
00:02:11.700 --> 00:02:15.330
So imagine you waking up in the morning and cooking an omelet.

35
00:02:15.330 --> 00:02:21.960
So in order to make that omelet you need to go through certain steps you need to get the salt get the

36
00:02:21.960 --> 00:02:25.310
eggs get the frying pan switch the fire on and so on.

37
00:02:25.440 --> 00:02:29.800
It does sound like a routine mundane thing but it's become routine because you've done it so many times.

38
00:02:29.880 --> 00:02:34.410
But in reality it's an environment where you're performing certain actions you're taking that you're

39
00:02:34.410 --> 00:02:39.120
putting the fire on you're putting a frying pan on the fire you're putting all the eggs into the frying

40
00:02:39.120 --> 00:02:43.110
pan and you're putting some salt on the eggs and turning over and so on.

41
00:02:43.140 --> 00:02:49.920
So as you can see there are certain actions actions which are taking in certain states and those actions

42
00:02:49.920 --> 00:02:52.420
lead to certain other states and sometimes reward.

43
00:02:52.420 --> 00:02:57.870
So for instance when you put the fire on and you wait wait wait wait wait you take an action Wait wait

44
00:02:57.870 --> 00:03:01.810
wait wait too long and then you put the eggs into the frying pan.

45
00:03:01.860 --> 00:03:03.510
The rewards are going to be very negative.

46
00:03:03.510 --> 00:03:08.310
Is it all going to burn on the other hand if you do all that at all the correct actions in the correct

47
00:03:08.310 --> 00:03:08.910
times.

48
00:03:08.910 --> 00:03:13.530
So it's also very important to understand that the actions should be taken at the correct points in

49
00:03:13.530 --> 00:03:13.800
time.

50
00:03:13.800 --> 00:03:20.010
So for instance putting the salt in the frying pan before you put the eggs in might not be the best

51
00:03:20.040 --> 00:03:20.660
idea.

52
00:03:20.730 --> 00:03:26.160
You might want to take that action of putting the salt into the frying pan after the eggs are in there.

53
00:03:26.160 --> 00:03:28.220
So in the in a different state.

54
00:03:28.320 --> 00:03:29.700
So it's important to remember that.

55
00:03:29.700 --> 00:03:30.800
And at the same time.

56
00:03:30.810 --> 00:03:35.610
So if you take all the correct actions in the correct order in the correct states your final reward

57
00:03:35.610 --> 00:03:38.070
could be that you get an omelet which you can eat.

58
00:03:38.820 --> 00:03:42.000
And so that's a very basic activity in your life.

59
00:03:42.000 --> 00:03:47.790
But if you think about it it is actually an environment and you are the agent going through this environment

60
00:03:47.880 --> 00:03:52.120
and performing tasks you don't really need to learn anything because you really know it pretty well.

61
00:03:52.140 --> 00:03:56.100
But at same time you could learn maybe you could learn how to make a better omelet or especially if

62
00:03:56.270 --> 00:03:59.760
it's your first omelet that you're making you're probably going to screw it up but you will learn from

63
00:03:59.760 --> 00:04:05.900
that because you will understand what actions lead towards states and rewards and anything else in life.

64
00:04:05.970 --> 00:04:12.030
For instance even trading on the stock market and buying and selling and getting certain feedback from

65
00:04:12.030 --> 00:04:16.330
the market in the sense of return positive or negative returns.

66
00:04:16.350 --> 00:04:21.330
That's also an environment that's you participating in that moment as an agent driving a car is also

67
00:04:21.330 --> 00:04:26.100
an environment where you can turn the steering wheel you can accelerate you can break and so on and

68
00:04:26.100 --> 00:04:30.870
you're getting feedback from the environment and you're one of those feedbacks Is that policeman giving

69
00:04:30.870 --> 00:04:37.290
you a speeding fine if you're going above the acceptable or allowed speed limit on that highway and

70
00:04:37.290 --> 00:04:39.200
therefore from there you learn that OK.

71
00:04:39.240 --> 00:04:43.050
That's not something that should be done because it leads to a negative reward.

72
00:04:43.170 --> 00:04:46.980
So rewards don't have to be just at the very end of the process they can be throughout the journey throughout

73
00:04:46.980 --> 00:04:47.940
the process.

74
00:04:47.940 --> 00:04:49.460
So those are a couple of examples.

75
00:04:49.470 --> 00:04:54.900
And in terms of A.I. the simplest way to think of reinforcement learning is like training a dog when

76
00:04:54.900 --> 00:05:00.580
you train a dog you to give it certain commands and if it obeys those commands then you give it a treat

77
00:05:00.600 --> 00:05:02.150
you give it like a biscuit or something.

78
00:05:02.400 --> 00:05:06.680
If it doesn't obey those commands you tell it that it's a bad dog or you just don't give it a treat.

79
00:05:06.780 --> 00:05:13.560
And through that process it learns what certain commands or what what it needs to do what action it

80
00:05:13.560 --> 00:05:18.390
needs to take in certain states and the states are the commands that you are giving it.

81
00:05:18.420 --> 00:05:21.480
And based on that it will get some certain rewards.

82
00:05:21.480 --> 00:05:26.970
Of course in the world of A.I. it's not that complex that you don't have to give the A.I. treats you

83
00:05:26.970 --> 00:05:32.070
don't have to have like a bag of biscuits with you every time you just give it a plus one or a minus

84
00:05:32.070 --> 00:05:32.190
one.

85
00:05:32.190 --> 00:05:38.940
So it's a huge advantage that in the world of A.I. we've created these eyes ourselves so the rewards

86
00:05:38.940 --> 00:05:40.080
that we're giving them.

87
00:05:40.080 --> 00:05:41.540
If you think wow this is really cool.

88
00:05:41.580 --> 00:05:43.460
The rewards you're giving them they don't actually exist.

89
00:05:43.470 --> 00:05:48.420
There's just a plus or minus one or a plot on a one or a zero or something.

90
00:05:48.450 --> 00:05:51.060
So it's all nonexistence all imaginary stuff.

91
00:05:51.060 --> 00:05:53.130
But at the same time it leads to great resources.

92
00:05:53.130 --> 00:05:59.010
We can create these amazing things these amazing artificial intelligence has by this amazing artificial

93
00:05:59.010 --> 00:06:03.120
intelligence by just providing rewards which don't really exist the plus and minus one that doesn't

94
00:06:03.120 --> 00:06:05.710
cost us anything about Sametime release results.

95
00:06:05.820 --> 00:06:08.070
So very similar to real world.

96
00:06:08.100 --> 00:06:09.720
And you know that example dogs.

97
00:06:09.720 --> 00:06:14.890
But here the rewards are digital and just numbers.

98
00:06:15.060 --> 00:06:18.750
And with that in mind we can talk about a little bit about robot dogs.

99
00:06:18.750 --> 00:06:24.240
I love this example so this is just a random picture it's not necessarily that exact robot dog that

100
00:06:25.020 --> 00:06:29.700
is trained through reinforcement learning somebody at robot dogs especially the older ones you'd have

101
00:06:29.760 --> 00:06:36.660
an algorithm in there and this is this actually a good example of the difference between pre-programmed

102
00:06:36.960 --> 00:06:44.670
agents and reinforcement learning agents so you could have a Robo dog which is pre-programmed to how

103
00:06:44.670 --> 00:06:49.280
to walk it will say so in the in the algorithm behind the dog and the software will say OK.

104
00:06:49.290 --> 00:06:56.250
So in order to walk you need to test move your left leg forward left front leg forward then your back

105
00:06:56.250 --> 00:07:00.390
right leg forward then your front right leg forward then your back left leg forward and repeat that

106
00:07:00.390 --> 00:07:05.610
action and you know that's that's the definition of walking is a function inside this dog then it might

107
00:07:05.610 --> 00:07:13.020
have you know how to sit how to stand and things like that whereas in a robot dog that is trained through

108
00:07:13.020 --> 00:07:16.590
reinforcement learning what happens is you don't pre program.

109
00:07:16.650 --> 00:07:23.730
This is the key concept to everything you're that you don't have any algorithm inside that is hardcoded

110
00:07:23.760 --> 00:07:30.420
into the dog instead you have what we'll be discussing in the future you have this reinforcement learning

111
00:07:30.450 --> 00:07:39.420
algorithm which is told that OK so the goal is for it to get from where you are now not knowing anything

112
00:07:39.480 --> 00:07:41.920
to that to the end of the room for example.

113
00:07:42.120 --> 00:07:44.220
And here are the certain actions you can take.

114
00:07:44.220 --> 00:07:48.630
You can move your right foot you can move your left foot you can move your right back foot you later

115
00:07:48.660 --> 00:07:49.310
left back foot.

116
00:07:49.320 --> 00:07:52.890
So here are all the degrees of freedom and you can do you can move them like this you can move and I

117
00:07:52.890 --> 00:07:59.220
got to make a list of actions you can take and your rewards are every time you take a step forward you

118
00:07:59.220 --> 00:08:04.620
get a plus one every time you fall over you get a minus one and that's all there is to it and then they

119
00:08:04.620 --> 00:08:07.160
just leave the dog and let it figure it out on its own.

120
00:08:07.320 --> 00:08:13.290
So the dog tries to stand up it falls and then realizes that OK I shouldn't do that action that led

121
00:08:13.290 --> 00:08:17.540
to me falling because every time I fall I get a minus one which is not good for me then so does that

122
00:08:17.550 --> 00:08:22.110
other action that helped it stand up and then it figures out it just experiments experiments experiments

123
00:08:22.110 --> 00:08:26.850
tries things randomly and then figures out that you can make a step forward by moving its right its

124
00:08:26.850 --> 00:08:31.330
front foot and it gets a plus one and it realized I should do more of that.

125
00:08:31.420 --> 00:08:36.690
Okay cool so it now learns that it should do more of this and less of that and through this learning

126
00:08:36.690 --> 00:08:44.910
process it quickly very quickly understands how it can walk and those those dogs that figure it out

127
00:08:44.910 --> 00:08:50.940
on their own can actually sometimes walk better than dogs that are pre-programmed because when we pre-programmed

128
00:08:50.940 --> 00:08:56.190
things we look at the real live dogs and or you know we use our own imagination how to do it whereas

129
00:08:56.310 --> 00:09:02.400
a reinforcement learning dog can optimize things on its own and because it's an A.I. sometimes it can

130
00:09:02.400 --> 00:09:06.570
get even better results and that's how they can train these robot dog these are all the dogs to play

131
00:09:06.570 --> 00:09:12.900
soccer you can't train a normal dog to play soccer because you know simply the whole approach is different

132
00:09:12.900 --> 00:09:20.610
and it's not something that that you know probably a normal dog has been trained to do or has ever done

133
00:09:20.670 --> 00:09:27.090
in its process of its evolution whereas a reinforcement learning robot dogs can very easily understand

134
00:09:27.090 --> 00:09:31.350
how to play soccer as long as you tell them what the rewards are what the goals are what the possible

135
00:09:31.350 --> 00:09:32.410
actions they can take.

136
00:09:33.030 --> 00:09:39.090
So that is how reinforcement learning works in general that's a quick overview of reinforcement learning.

137
00:09:39.120 --> 00:09:44.400
I hope that got you very excited about what's going to come next because it's a completely different

138
00:09:44.790 --> 00:09:51.060
world compared to pre-programmed solutions or hard program hard coded solutions where you have the if

139
00:09:51.060 --> 00:09:57.150
else conditions this is very different and we're going to be talking more about that in the meantime

140
00:09:57.870 --> 00:10:03.940
we've got some additional reading for you so if you'd like to have some supporting materials here's

141
00:10:03.940 --> 00:10:06.360
a great article which you can look at.

142
00:10:06.400 --> 00:10:09.250
Look into it's called simple reinforcement learning with tensor flow.

143
00:10:09.370 --> 00:10:10.510
It's got 10 parts.

144
00:10:10.510 --> 00:10:17.290
The link is here and you'll find the full the clickable link on in the course resources is by Arthur

145
00:10:17.290 --> 00:10:17.830
Giuliani.

146
00:10:17.840 --> 00:10:24.160
The 2016 article and you can follow along this course and also get additional information from that

147
00:10:24.190 --> 00:10:24.700
article.

148
00:10:24.700 --> 00:10:30.290
But bear in mind that that article is tenser flow whereas in this course we are using pi torch so a

149
00:10:30.490 --> 00:10:35.770
different implementation but in plantations but at the same time you might pick up a few things here

150
00:10:35.770 --> 00:10:41.240
and there that might supplement your learning that we're going to be doing in this course.

151
00:10:41.240 --> 00:10:42.580
So great article to follow.

152
00:10:42.580 --> 00:10:45.780
Even if you're not considering following it for sure it is still just in case.

153
00:10:45.790 --> 00:10:52.450
Check out that that first part and see if you like it see if you would like to read it a bit more and

154
00:10:52.570 --> 00:10:56.600
then we've got specific to this tutorial about reinforcement learning.

155
00:10:56.690 --> 00:11:00.340
There's a paper by Richard Sutton which is called reinforcement learning.

156
00:11:00.380 --> 00:11:08.110
One introduction is the 1998 papers are quite old but at the same time you can learn a bit about reinforcement

157
00:11:08.110 --> 00:11:08.910
learning.

158
00:11:09.160 --> 00:11:14.050
Some of the examples like that omelet example and other examples of where reinforcement learning can

159
00:11:14.050 --> 00:11:17.650
be applied and just a general overview of reinforcement learning.

160
00:11:17.650 --> 00:11:23.140
If you are looking for some additional reading and on that note we're going to wrap up this tutorial.

161
00:11:23.170 --> 00:11:24.590
Can't wait to see you next step.

162
00:11:24.590 --> 00:11:26.490
And until then enjoy a.
