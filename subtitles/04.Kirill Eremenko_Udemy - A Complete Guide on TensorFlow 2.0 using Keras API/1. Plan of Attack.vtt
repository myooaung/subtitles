WEBVTT
1
00:00:00.390 --> 00:00:03.090
Hello and welcome back to the course on deep learning.

2
00:00:03.150 --> 00:00:08.590
I hope you're as excited about this section of the course on recurrent neural networks as I am.

3
00:00:08.670 --> 00:00:17.970
We're slowly venturing into the very complex very forward looking and cutting edge areas of deep learning

4
00:00:17.970 --> 00:00:19.920
and this is going to be very very fun.

5
00:00:20.250 --> 00:00:24.960
So today we're going to talk about how we're going to approach this section which contains so many different

6
00:00:24.960 --> 00:00:30.100
complex topics so many concepts that we need to get our head around.

7
00:00:30.150 --> 00:00:30.390
All right.

8
00:00:30.390 --> 00:00:36.630
So in this section we will learn first of all the idea behind a recurrent neural networks will see how

9
00:00:36.630 --> 00:00:43.530
they compare to the human brain will understand what makes them unique and special as compared to regular

10
00:00:43.560 --> 00:00:45.330
artificial neural networks.

11
00:00:45.400 --> 00:00:52.620
And then we'll talk about the vanishing gradient problem something that has been a major roadblock in

12
00:00:52.890 --> 00:00:58.740
or had been a major roadblock in the development and utilization of recurrent neural networks something

13
00:00:58.740 --> 00:01:01.990
that prevented them from being what they are now.

14
00:01:02.040 --> 00:01:08.820
And then we will move on to the solution that one of the most popular solutions to the vanishing green

15
00:01:08.820 --> 00:01:14.820
problem the long short term memory or else T.M. neural networks and we'll talk about their architecture

16
00:01:14.960 --> 00:01:22.020
be a very exciting tutorials one of my very topics and we will find out exactly how they work and what

17
00:01:22.200 --> 00:01:25.020
what that complex structure is inside them.

18
00:01:25.020 --> 00:01:32.040
We'll break it down into simple terms and you will be able to walk away with a pretty solid understanding

19
00:01:32.130 --> 00:01:33.450
of Ms.

20
00:01:33.870 --> 00:01:35.850
And then we'll talk about the practical intuition.

21
00:01:35.850 --> 00:01:40.340
So in that prehistory we will have a practical example of using Alzheimer's.

22
00:01:40.350 --> 00:01:47.040
But in this practical intuition to will look at some great examples posted by one of the researchers

23
00:01:47.040 --> 00:01:53.130
one of the most well-known researchers in the space and we'll understand even better on an intuitive

24
00:01:53.130 --> 00:01:59.070
level how Ellis teams actually work how they think will be like neuroscientists trying to understand

25
00:01:59.100 --> 00:02:02.900
what's going on in the brain of an Ellis team is going to be very exciting as well.

26
00:02:03.510 --> 00:02:09.420
And then at the end we'll have an extra tutorial on Alistair variations something special something

27
00:02:10.080 --> 00:02:15.540
you don't really have to take this material but it's very quick just to get you up to speed on what

28
00:02:15.540 --> 00:02:19.170
other options of illustrations exist out there in the world.

29
00:02:19.170 --> 00:02:26.550
What other architectures you might come across in your work so hopefully you're excited and ready to

30
00:02:26.550 --> 00:02:27.340
get started.

31
00:02:27.450 --> 00:02:29.650
And I can't wait to see on the next material.

32
00:02:29.730 --> 00:02:31.640
Until then enjoy deep learning.
