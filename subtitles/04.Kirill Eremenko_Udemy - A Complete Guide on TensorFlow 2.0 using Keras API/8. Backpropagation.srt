1
00:00:01,800 --> 00:00:04,120
Hello and welcome back to the course on deep learning.

2
00:00:04,140 --> 00:00:05,370
Today we're going to wrap up.

3
00:00:05,380 --> 00:00:07,290
We're back propagation.

4
00:00:07,290 --> 00:00:07,520
All right.

5
00:00:07,530 --> 00:00:12,050
So we already know pretty much everything we need to know about what happens in the neural net.

6
00:00:12,060 --> 00:00:18,240
We know that there's a process called forward propagation where information is entered into the input

7
00:00:18,240 --> 00:00:24,900
layer and then it's propagated forward to get our white hats our output values and then we compare those

8
00:00:24,900 --> 00:00:32,610
to the actual values that we have in our training set and then we calculate the errors then the errors

9
00:00:32,610 --> 00:00:40,170
are back propagated through the network in the opposite direction and that allows us to train the network

10
00:00:40,200 --> 00:00:41,550
by adjusting the weights.

11
00:00:41,610 --> 00:00:49,590
So the one key important thing to remember here is that back propagation is an advanced algorithm driven

12
00:00:49,590 --> 00:00:59,460
by very interesting and sophisticated mathematics which allows us to adjust the weights all of them

13
00:00:59,490 --> 00:01:02,420
at the same time all the way to adjusted simultaneously.

14
00:01:02,460 --> 00:01:08,940
So if we were doing this manually or if we were coming up for a different type of algorithm then even

15
00:01:08,940 --> 00:01:14,100
if we calculate the error and then we were trying to understand what effect each of the weights has

16
00:01:14,100 --> 00:01:22,300
an error we'd have to somehow adjust each of the weights independent independently or individually the

17
00:01:22,390 --> 00:01:29,110
huge advantage of back irrigation and this key thing to remember is that during the process of back

18
00:01:29,110 --> 00:01:39,740
propagation simply because of the way the algorithm is structured you are able to adjust all of the

19
00:01:39,740 --> 00:01:45,410
weight at the same time so you basically know which part of the error each of your weights in the neural

20
00:01:45,410 --> 00:01:47,270
network is responsible for.

21
00:01:47,360 --> 00:01:58,010
Now that is the key fundamental underlying principle of back propagation and this was why it picked

22
00:01:58,010 --> 00:02:02,660
up so rapidly in the 1980s and this was the major breakthrough.

23
00:02:02,720 --> 00:02:08,810
And if you'd like to learn more about that and how exactly the mathematics works in the background then

24
00:02:09,140 --> 00:02:14,690
a good article which we've already mentioned is the neural where networks and deep learning is actually

25
00:02:14,690 --> 00:02:16,460
a book by Michael Nielsen.

26
00:02:16,500 --> 00:02:23,600
There you'll find the mathematics written out and it will help you understand how exactly this is possible.

27
00:02:23,600 --> 00:02:30,430
But for now for our purposes it from an intuition point of view the important part is to remember that

28
00:02:31,190 --> 00:02:33,230
that's what back propagation does.

29
00:02:33,230 --> 00:02:36,550
It adjusts all of the weights at the same time.

30
00:02:36,890 --> 00:02:43,220
And now we're going to just wrap everything up with a step by step walk through of what happens in the

31
00:02:43,220 --> 00:02:45,330
training of a neural network.

32
00:02:45,350 --> 00:02:45,580
All right.

33
00:02:45,590 --> 00:02:50,960
So step one we randomly initialize the weights to small numbers close to zero but not zero.

34
00:02:50,960 --> 00:02:56,540
We didn't really focus on the initialization of weights during the Egyptian tutorials but the weights

35
00:02:56,540 --> 00:03:02,550
have to start somewhere and they're initialized with random values near zero.

36
00:03:02,570 --> 00:03:09,440
And from there through the process for propagation back irrigation these weights are adjusted until

37
00:03:09,440 --> 00:03:13,620
the error is minimized until the cost function is minimized.

38
00:03:13,770 --> 00:03:15,500
Then step to inputs.

39
00:03:15,500 --> 00:03:21,380
The first observation of your data sets to the first row into the input layer each feature is one input.

40
00:03:21,410 --> 00:03:25,260
So basically you take the columns and put them into the input nodes.

41
00:03:25,670 --> 00:03:27,820
Step 3 for propagation from left to right.

42
00:03:27,830 --> 00:03:32,730
The neurons are activated in a way that the impact of each neurones activation is limited by the weight

43
00:03:32,730 --> 00:03:33,250
to the weights.

44
00:03:33,240 --> 00:03:40,490
Basically determine how important each neurones activation is then propagate the activations until getting

45
00:03:40,760 --> 00:03:43,670
the predicted result y hat in this case.

46
00:03:43,940 --> 00:03:46,630
So basically you propagate from left to right.

47
00:03:46,640 --> 00:03:50,100
You go all the way until you get to the end and you get your way ahead.

48
00:03:50,240 --> 00:03:52,670
Then compare the predictor result to the actual result.

49
00:03:52,700 --> 00:03:58,570
Measure the generated error then you do the back propagation from Rachel if the error is back irrigated.

50
00:03:58,580 --> 00:04:02,170
Update the weights according to how much they're responsible for the error.

51
00:04:02,180 --> 00:04:08,240
Again you are able to calculate that because of the way the back propagated by approximation algorithm

52
00:04:08,270 --> 00:04:13,640
is structured the learning rate decides by how much we update the weights learning rate as parameter

53
00:04:13,910 --> 00:04:17,630
you can control in your neural network.

54
00:04:17,660 --> 00:04:24,440
Step 6 repeat steps 1 to 5 and update the weights after each observation that is called the reinforcement

55
00:04:24,440 --> 00:04:29,510
learning and in our case that was stochastic gradient descent.

56
00:04:29,510 --> 00:04:31,410
Or repeat steps one to 5.

57
00:04:31,430 --> 00:04:37,190
But update weights only after a batch of observations so batch learning it's either a full gradient

58
00:04:37,190 --> 00:04:40,910
descent or badge gradient descent or mini batch gradient descent.

59
00:04:40,910 --> 00:04:46,970
And Step 7 when the whole training set pass through the artificial neural network that makes an epoch

60
00:04:47,570 --> 00:04:48,980
redo more epochs.

61
00:04:48,980 --> 00:04:55,040
So basically you just keep doing that and doing that and doing that to allow your neural network to

62
00:04:55,040 --> 00:05:02,240
train better and better and better and constantly adjust itself as you minimize the cost function.

63
00:05:02,690 --> 00:05:04,360
So there we go.

64
00:05:04,370 --> 00:05:09,810
Those are the steps you need to take to build your artificial neural networks and train it.

65
00:05:09,980 --> 00:05:16,010
And these are the steps that you will be taken to get you've had learn in the practical tutorials.

66
00:05:16,040 --> 00:05:19,490
Wish you the best of luck and I look forward to seeing you next time.

67
00:05:19,490 --> 00:05:21,230
Until then enjoy deep learning.
