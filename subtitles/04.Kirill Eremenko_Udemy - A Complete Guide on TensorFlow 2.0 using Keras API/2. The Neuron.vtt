WEBVTT
1
00:00:00.270 --> 00:00:02.270
Hello and welcome back to the course and deep learning.

2
00:00:02.340 --> 00:00:07.950
Today we're talking about the neuron which is the basic building block of artificial neural networks.

3
00:00:07.950 --> 00:00:09.300
So let's get started.

4
00:00:09.300 --> 00:00:11.280
Previously we saw an image which looked like this.

5
00:00:11.280 --> 00:00:17.840
And these are actual real life neurons which have been smeared onto a glass and colored a little bit

6
00:00:17.860 --> 00:00:19.920
and they are observed through a microscope.

7
00:00:19.920 --> 00:00:21.030
So this is what they look like.

8
00:00:21.030 --> 00:00:28.950
As you can see quite an interesting structure a body and then lots of different tails kind of branches

9
00:00:29.160 --> 00:00:30.180
coming out of them.

10
00:00:30.270 --> 00:00:32.270
And this is very interesting.

11
00:00:32.310 --> 00:00:38.700
But the question is how can we recreate that in a machine because we really need to recreate it machine

12
00:00:38.940 --> 00:00:48.240
since the whole purpose of deep learning is to mimic how the human brain works in the hope that by doing

13
00:00:48.240 --> 00:00:54.000
so we're going to create something amazing we're going to create an amazing infrastructure for machines

14
00:00:54.000 --> 00:00:55.060
to be able to learn.

15
00:00:55.170 --> 00:00:56.750
And why do we hope for that.

16
00:00:56.760 --> 00:01:03.420
Well because the human brain is well just happens to be one of the most powerful learning learning tools

17
00:01:03.600 --> 00:01:07.260
in on the planet or like learning mechanisms on the planet.

18
00:01:07.260 --> 00:01:11.280
And we just hope that if we recreate that we'll have something as awesome as that.

19
00:01:11.280 --> 00:01:17.610
So our challenge right now our very first step to creating artificial neural networks is to recreate

20
00:01:17.640 --> 00:01:18.320
a neuron.

21
00:01:18.330 --> 00:01:19.050
So how do we do that.

22
00:01:19.050 --> 00:01:23.690
Well first of all let's have a closer look at what it actually is.

23
00:01:23.820 --> 00:01:34.080
This image was first created by Spanish neuroscientists and Jagger among you Karl in 1899 and what he

24
00:01:34.080 --> 00:01:39.710
did was he died neurons in actual brain tissue and look at them under a microscope.

25
00:01:39.840 --> 00:01:43.470
And while he was looking at them he actually drew what he saw and this is what he saw.

26
00:01:43.470 --> 00:01:49.620
He saw two neurons or two large neurons over there at the top which had all these branches coming out

27
00:01:49.620 --> 00:01:52.220
of them towards their top parts.

28
00:01:52.230 --> 00:01:59.460
And then each one you had this a rod or like thread coming out towards the bottom very long one.

29
00:01:59.460 --> 00:02:01.560
And yes so that's what he saw.

30
00:02:01.590 --> 00:02:07.740
And now you know technology has advanced quite a lot and we have seen neurons much closer in more detail

31
00:02:07.740 --> 00:02:13.250
and now we can actually draw what it looks like dog grammatically so let's have a look at that.

32
00:02:13.350 --> 00:02:20.970
Here's a neuron This is what looks like very similar to what Santiago or someone drew over here and

33
00:02:21.060 --> 00:02:21.490
here.

34
00:02:21.500 --> 00:02:23.900
And this neuron what we can see is that it's got a body.

35
00:02:24.510 --> 00:02:29.040
That's the main part of the neuron and then it's got some branches at the top which are called dendrites

36
00:02:29.130 --> 00:02:33.250
and it's also got an axon which is that long tail of the neuron.

37
00:02:33.300 --> 00:02:36.660
And so what are these dendrites and went for and what's the axon for.

38
00:02:36.660 --> 00:02:43.980
Well the key point to understand here is that neurons by themselves are pretty much useless.

39
00:02:43.980 --> 00:02:45.520
It's like it's like an ant.

40
00:02:45.540 --> 00:02:45.960
Right.

41
00:02:45.960 --> 00:02:52.280
An ant on its own can do much like five ants together maybe they can pick something up but again.

42
00:02:52.420 --> 00:02:55.370
They don't they can't build an anthill they're calling establish a colony.

43
00:02:55.380 --> 00:02:59.310
They can't work together as a as a huge organism.

44
00:02:59.310 --> 00:03:03.450
But at the same time when you have lots and lots of ads like you have a million and so they can build

45
00:03:03.450 --> 00:03:05.700
a whole colony and they can build an anthill.

46
00:03:05.700 --> 00:03:06.530
Same thing with neurons.

47
00:03:06.540 --> 00:03:12.450
By itself it's not that strong but when you have lots of neurons together they work together to do magic.

48
00:03:12.450 --> 00:03:13.740
And how do they work together.

49
00:03:13.740 --> 00:03:14.370
That's a question.

50
00:03:14.400 --> 00:03:16.620
Well that's what the dendrites and acts on are for.

51
00:03:16.650 --> 00:03:21.540
So the dendrites are kind of like the receivers of the signal for the neuron and Axon is the transmitter

52
00:03:21.540 --> 00:03:23.040
of the signal for the neuron.

53
00:03:23.160 --> 00:03:26.460
And here's an image of how it all works conceptually.

54
00:03:26.490 --> 00:03:32.490
So at the top you've got a neuron and you can see that it's dead rides are connected to axons of other

55
00:03:32.490 --> 00:03:35.940
neurons that are like even further away above it.

56
00:03:35.940 --> 00:03:42.810
And then the signal from this neuron travels down its axon and connects or passes onto the dendrites

57
00:03:42.810 --> 00:03:43.550
of the next neuron.

58
00:03:43.560 --> 00:03:44.850
And that's how they're connected.

59
00:03:44.970 --> 00:03:52.440
And in that small image over there you can see that the axon doesn't actually touch the dendrites a

60
00:03:52.450 --> 00:03:52.990
lot.

61
00:03:53.340 --> 00:03:58.830
A lot of machine learning or like a few Machine Learning scientists are very adamant about that the

62
00:03:58.830 --> 00:04:06.030
fact that it doesn't touch it like the room it doesn't touch it has been proven that there is a physical

63
00:04:06.030 --> 00:04:06.590
connection there.

64
00:04:06.900 --> 00:04:13.950
But the point that we're interested in is that that connection between them that the whole concept of

65
00:04:13.950 --> 00:04:15.220
the signal being passed.

66
00:04:15.220 --> 00:04:16.280
That's called the sign ups.

67
00:04:16.300 --> 00:04:18.520
You can see over there in that little image.

68
00:04:19.030 --> 00:04:23.760
That's a figure bracket is a sign ups and that's a term we're going to be using.

69
00:04:23.760 --> 00:04:30.180
So instead of calling our artificial neurons the lines that were going to have the connectors for artificial

70
00:04:30.180 --> 00:04:34.890
neurones we're going to be calling them axons or dendrites because then the question is whose connection

71
00:04:34.890 --> 00:04:39.180
is this is that that neurons are this neurons we just call and we're just going to call them sinuses.

72
00:04:39.450 --> 00:04:42.650
And that's kind of just answers all questions right away.

73
00:04:42.660 --> 00:04:48.030
That's just basically where the signals pass doesn't matter who that element belongs to they're just

74
00:04:48.180 --> 00:04:51.090
a representation of the signal being passed and we'll see that just now.

75
00:04:51.930 --> 00:04:55.120
So basically that's how a neuron works.

76
00:04:55.120 --> 00:04:56.170
And yeah.

77
00:04:56.190 --> 00:05:03.300
So let's move on to how are we going to represent neuron are going to create neurons in the machine.

78
00:05:03.330 --> 00:05:09.270
So we're moving away now we're moving away from neuroscience and moving into technology.

79
00:05:09.340 --> 00:05:10.090
And here we go.

80
00:05:10.330 --> 00:05:11.620
So here is our neuron.

81
00:05:11.680 --> 00:05:18.280
Also sometimes called the node then your own gets some input signals and it has an output signal.

82
00:05:18.340 --> 00:05:23.620
So dendrites and axons remember but again we're going to call these sentences.

83
00:05:23.740 --> 00:05:29.020
And then the these input signals we're going to represent them of other neurons as well.

84
00:05:29.020 --> 00:05:35.410
So in this specific case you can see that this neuron this green neuron is getting signals from yellow

85
00:05:35.410 --> 00:05:35.790
neurons.

86
00:05:35.800 --> 00:05:41.740
And in this course we're going to try and stick to a certain color coding regime where yellow means

87
00:05:41.770 --> 00:05:42.490
an input layer.

88
00:05:42.490 --> 00:05:50.650
So basically all of the neurons that are on the outer layer or in the first front of where are the signals

89
00:05:50.650 --> 00:05:52.440
coming in and buy signal.

90
00:05:52.500 --> 00:05:58.710
It might be like a bit of an over overkill to call this a signal it's just basically input values.

91
00:05:58.720 --> 00:06:03.520
So so you know how even like in a simple linear regression you have input values and then you have a

92
00:06:04.210 --> 00:06:05.010
predicted value.

93
00:06:05.010 --> 00:06:05.560
Same thing here.

94
00:06:05.560 --> 00:06:08.920
So you have input values and there they are the yellow ones.

95
00:06:09.040 --> 00:06:11.200
And then on the right we will see just now it'll be red.

96
00:06:11.200 --> 00:06:13.440
It'll be the output value.

97
00:06:13.510 --> 00:06:17.080
The other thing that I wanted to point out here is that in this specific example we're looking at a

98
00:06:17.080 --> 00:06:21.220
neuron which is getting its signals from the input layer neuron.

99
00:06:21.220 --> 00:06:27.070
So there are also neurons but their input layer neurons sometimes you'll have neurons which get their

100
00:06:27.070 --> 00:06:30.430
signal from other hidden layer neurons.

101
00:06:30.460 --> 00:06:35.290
So from other greenhorns and the concept is going to be exactly the same and just in this case we for

102
00:06:35.290 --> 00:06:37.560
simplicity's sake we're portraying this example.

103
00:06:37.560 --> 00:06:44.590
And in terms of the input layer the way to think about it is in the in the analogy of the human brain

104
00:06:45.280 --> 00:06:47.900
the input layer is your sensors.

105
00:06:47.900 --> 00:06:52.280
Right so whatever you can see here feel touch or smell.

106
00:06:52.450 --> 00:06:57.160
And of course it's like there's there's a lot of things you can see there's a lot of information coming

107
00:06:57.160 --> 00:06:58.810
in but those are your.

108
00:06:58.840 --> 00:07:05.970
That's what your brain is limited to IS PRETTY MUCH A is free much lives in a box made out of bones.

109
00:07:06.040 --> 00:07:11.920
And it's only just it's it's a mind blowing concept to think about that your brain is just locked in

110
00:07:11.920 --> 00:07:16.810
a black box and the only thing like it can't see you can hear the only thing is getting his electrical

111
00:07:16.810 --> 00:07:24.130
impulses coming from these organs that you have which are called Your Ears nose eyes you know your sense

112
00:07:24.130 --> 00:07:28.160
of touch and whatever and you and your and your taste.

113
00:07:28.150 --> 00:07:34.120
Right so you're just getting signals but it basically lives in this dark black box and it's making making

114
00:07:34.120 --> 00:07:36.210
sense of the world through your senses.

115
00:07:36.220 --> 00:07:38.280
It's it's phenomenal.

116
00:07:38.440 --> 00:07:38.890
And so yeah.

117
00:07:38.920 --> 00:07:42.970
So you have these inputs that are coming in in terms of human brain.

118
00:07:42.970 --> 00:07:49.450
Those are your five senses and in terms of machine learning or deep learning that is basically your

119
00:07:49.870 --> 00:07:56.500
input value so you're independent parables and we'll get that in a second so your input values the signal

120
00:07:56.670 --> 00:08:01.390
is passed through sign ups as to the neuron and then your neuron has an output value that it passes

121
00:08:01.390 --> 00:08:03.470
it further on down the chain.

122
00:08:03.490 --> 00:08:07.960
In this specific case in terms of color coding again yellow means input layer so we kind of simplifying

123
00:08:07.960 --> 00:08:11.740
everything here we're saying we're only going to have like the input layer and then we're going to have

124
00:08:11.770 --> 00:08:16.420
one hidden layer with the green which is the hinterland then we're going to have the output there right

125
00:08:16.420 --> 00:08:16.880
away.

126
00:08:17.500 --> 00:08:21.060
So just so that we can get used to these colors for now.

127
00:08:21.550 --> 00:08:22.110
So there we go.

128
00:08:22.110 --> 00:08:28.150
That's the basic structure so now let's look in a bit more detail at these different elements that we

129
00:08:28.150 --> 00:08:28.360
have.

130
00:08:28.360 --> 00:08:31.030
So we've got the input layer and what do we have here.

131
00:08:31.030 --> 00:08:36.700
Well we have these inputs which are in fact independent variables soon different variable one into better

132
00:08:36.730 --> 00:08:38.500
variable to independent variable.

133
00:08:38.650 --> 00:08:44.680
The important thing to remember here is that these independent variables are all for one single observation.

134
00:08:44.680 --> 00:08:50.590
So think of it as just one row in your database one observation you just take all of the independent

135
00:08:50.590 --> 00:08:57.940
variables you know maybe it's the age of the person there the amount of money in the bank accounts and

136
00:08:57.940 --> 00:09:02.970
then how how do they drive or walk to work what true method of Trump's protection do they use.

137
00:09:03.010 --> 00:09:08.410
So but that's all descriptors of one specific person that you are either you're training your model

138
00:09:08.410 --> 00:09:14.740
on or you're performing some prediction on and the other thing you need to know about these variables

139
00:09:14.770 --> 00:09:16.290
is that you need to standardize them.

140
00:09:16.320 --> 00:09:20.980
So you need to either standardize them which means make sure that they have a mean of zero variance

141
00:09:20.980 --> 00:09:28.450
one or you can also sometimes and headland will point out these situations in a bit more detail perhaps

142
00:09:28.450 --> 00:09:33.130
in the practical terms you might come across these sometimes you might want to not standardize you might

143
00:09:33.130 --> 00:09:38.920
want to normalize them meaning that instead of making sure the mean and immunization variance is one

144
00:09:38.950 --> 00:09:44.470
you just take you know to subtract the minimum value and then you divide by the maximum minus a minimum

145
00:09:44.470 --> 00:09:52.330
sum by the range of your values and the four you get values between 0 and 1 and it depends on this scenario

146
00:09:52.330 --> 00:09:56.890
you might want to do one or the other but basically you want all of these variables to be quite similar

147
00:09:56.890 --> 00:10:04.400
in in about the same a range of values and Y ways that well all of these values are going to go into

148
00:10:04.400 --> 00:10:10.010
a neural network where as we'll see just now there'll be added up and multiplied by weights added up

149
00:10:10.010 --> 00:10:15.620
and so on and just going to be just going to be easier for the neural network to process them if they're

150
00:10:15.620 --> 00:10:17.130
all about the same.

151
00:10:17.200 --> 00:10:23.960
And and in fact you know that's that's just how it is going to be able to work properly.

152
00:10:24.170 --> 00:10:29.150
And if you want to read more about standardization normalization and other things that you can do if

153
00:10:29.150 --> 00:10:36.340
you input variables are a good additional reading paper is called efficient back prob by young looking

154
00:10:36.850 --> 00:10:38.110
1998.

155
00:10:38.290 --> 00:10:44.750
The links over there so young AE and we're actually going to talk more about this phenomenal person

156
00:10:44.750 --> 00:10:49.790
in the space of deep learning in the part of the course we were talking about can delusional neural

157
00:10:49.790 --> 00:10:55.140
networks and you will you'll see that this is definitely a person who knows what he's talking about.

158
00:10:55.220 --> 00:11:02.420
He's a close friend of Geoffrey Hinton who we've already seen who've already mentioned some in this

159
00:11:02.420 --> 00:11:07.310
paper you'll learn more about sanitization and normalization but also you can pick up lots of other

160
00:11:07.310 --> 00:11:11.750
different tips and tricks and you'll be a good a good source for additional reading as you go through

161
00:11:11.750 --> 00:11:12.230
this quarter.

162
00:11:12.390 --> 00:11:15.320
So yeah check it out if you're interested in some additional reading.

163
00:11:15.450 --> 00:11:16.870
And there we go.

164
00:11:16.870 --> 00:11:20.160
So that's what we do if the variables.

165
00:11:20.330 --> 00:11:23.030
And here we've got the output value.

166
00:11:23.150 --> 00:11:25.070
So what can our output value be.

167
00:11:25.070 --> 00:11:26.260
Well we've got a couple of options.

168
00:11:26.270 --> 00:11:28.850
Output value can be it can be continuous.

169
00:11:28.850 --> 00:11:35.360
Like for instance price it can be binary for this as a personal exit or we'll stay or it can be a categorical

170
00:11:35.360 --> 00:11:39.190
variable and physical a categorical variable.

171
00:11:39.200 --> 00:11:44.750
The important thing to remember here is that in that case your output value only just one it'll be several

172
00:11:44.750 --> 00:11:51.560
output values because these will be a dummy variables which will be representing your categories and

173
00:11:51.560 --> 00:11:57.500
that just this how it works and it just important to remember that in that case that's how you're going

174
00:11:57.500 --> 00:12:02.420
to be getting your categories out of the artificial neural network.

175
00:12:02.750 --> 00:12:05.260
But let's go back to a simple case of one output value.

176
00:12:05.690 --> 00:12:11.720
And now let's one more point or kind of like the point I really made I just wanted to reiterate this

177
00:12:11.720 --> 00:12:18.260
point on the left you've got a single observation so one or off human from your dataset and on the right

178
00:12:18.260 --> 00:12:19.700
you have a single observation as well.

179
00:12:19.700 --> 00:12:22.130
And that is the same observation.

180
00:12:22.130 --> 00:12:27.740
So important to remember that like whatever inputs you're putting in that's for one row and then the

181
00:12:27.740 --> 00:12:32.540
output you get that is for that same exact row or if you're training a neural network then you know

182
00:12:32.600 --> 00:12:36.350
you're putting the inputs in for that one row you're putting the output in for that one row.

183
00:12:36.390 --> 00:12:42.050
So like if you want to simplify the complexity think of it as a you know like a simple linear regression

184
00:12:42.230 --> 00:12:43.940
or a multivariate linear regression.

185
00:12:43.940 --> 00:12:49.820
So you're putting in your values you have the output there's like there's no question about it when

186
00:12:49.820 --> 00:12:54.080
we're talking about things like regression because we're so used to it same thing here it's nothing

187
00:12:54.080 --> 00:12:54.910
too complex.

188
00:12:54.920 --> 00:12:59.120
We're just putting in values we're getting output but just remember that every time it's one row you're

189
00:12:59.120 --> 00:13:04.910
dealing with so you don't get confused and start putting in like thinking that these are different different

190
00:13:04.910 --> 00:13:10.520
rows that you're putting into your artificial neural network or something this is all just values in

191
00:13:10.520 --> 00:13:15.980
that one rows a different observation different characteristics or or attributes relating to that one

192
00:13:16.040 --> 00:13:18.740
observation every single time.

193
00:13:19.090 --> 00:13:19.330
Okay.

194
00:13:19.370 --> 00:13:25.100
So next thing I want to talk about here is the or are the sign ups is is the sign of says here we've

195
00:13:25.100 --> 00:13:31.100
got sciences and they all actually get assigned weights weights we're going to talk more about weights

196
00:13:31.100 --> 00:13:38.900
for the down but in short weights are crucial to our official neural network networks functioning because

197
00:13:38.930 --> 00:13:45.890
the weights are how neural networks learn by adjusting the weights and the neural network decides in

198
00:13:45.890 --> 00:13:51.050
every single case what's seeing a signal is important what signal is not important to a certain neuron

199
00:13:51.080 --> 00:13:55.520
what signal gets passed along and what signal doesn't get passed along or two what strength.

200
00:13:55.520 --> 00:13:57.710
To what extent signals get passed along.

201
00:13:57.710 --> 00:14:03.530
So weights are crucial they are and they are the things that get adjusted through the process of learning.

202
00:14:03.530 --> 00:14:08.150
Like when when you're training on artificial neural network you're basically adjusting all of the weights

203
00:14:08.510 --> 00:14:10.900
in all the sinuses across this whole neural network.

204
00:14:11.240 --> 00:14:17.720
And that's where a gradient descent and back propagation come into play and those are concepts that

205
00:14:17.730 --> 00:14:19.170
will also discuss.

206
00:14:19.170 --> 00:14:23.010
And so basically those are the weights that's how we need to know for now.

207
00:14:23.120 --> 00:14:26.600
And here we've got the neuron so signals go into the neuron.

208
00:14:26.840 --> 00:14:28.270
And what happens in the neuron.

209
00:14:28.370 --> 00:14:30.510
So this is the the interesting part.

210
00:14:30.510 --> 00:14:33.830
Like we're talking about the neuron today what happens inside the neuron.

211
00:14:33.830 --> 00:14:40.640
So a few things happen first thing and the first step is that all of these values that is getting get

212
00:14:40.910 --> 00:14:41.290
added up.

213
00:14:41.300 --> 00:14:43.780
So it takes the added.

214
00:14:43.790 --> 00:14:49.840
So the weighted sum of all of the input values that is getting very simple right.

215
00:14:49.910 --> 00:14:55.670
It's very very straightforward just add up multiplied by the way add them up and then it applies an

216
00:14:55.700 --> 00:14:57.140
activation function.

217
00:14:57.140 --> 00:15:01.480
Now we're going to talk more about activation functions the number is basically a function that is assigned

218
00:15:01.480 --> 00:15:11.140
to this neuron or to this whole layer and it is applied to this weighted sum and then from that the

219
00:15:11.140 --> 00:15:18.640
neuron understands if it needs to pass on a signal like that's the signal that it passes on that the

220
00:15:18.700 --> 00:15:22.210
function applied to the weight at some.

221
00:15:22.240 --> 00:15:26.470
But basically depending on the function the neuron will either pass on a signal or it won't pass that

222
00:15:26.470 --> 00:15:31.090
signal on and that's exactly what happened here in step 3.

223
00:15:31.420 --> 00:15:36.910
The neuron passes on that signal to the next neuron down the line and that's what we're going to talk

224
00:15:36.910 --> 00:15:43.600
about in the next material because it is quite an important topic we want to delve deeper into the activation

225
00:15:43.600 --> 00:15:48.850
function but hopefully for now everything is should be pretty clear how you have got input values you've

226
00:15:48.850 --> 00:15:53.410
got weights you've got the sinuses you've got something that you know happens in Neuron you've got weight

227
00:15:53.490 --> 00:15:57.670
at some and then activation function applied and then that is passed down the line and that is just

228
00:15:57.670 --> 00:16:01.940
repeated throughout the whole neural network on and on and on and on.

229
00:16:02.230 --> 00:16:06.910
You know thousands hundreds of thousands of times depending on how big how many neurons you have how

230
00:16:06.910 --> 00:16:08.980
many sentences you have in your neural network.

231
00:16:09.490 --> 00:16:10.060
So there we go.

232
00:16:10.060 --> 00:16:13.270
Hope you enjoyed it too this will come we'll see you next time.

233
00:16:13.270 --> 00:16:14.950
And until then enjoy deep learning.
