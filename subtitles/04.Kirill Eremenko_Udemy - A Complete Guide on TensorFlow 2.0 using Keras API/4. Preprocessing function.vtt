WEBVTT
1
00:00:00.490 --> 00:00:03.910
Hello everyone and welcome to this python tutorial.

2
00:00:03.940 --> 00:00:10.790
In this video we will be creating our pre processing function in the DNA flow transform.

3
00:00:10.810 --> 00:00:17.460
The goal is to create an automatic data set pre processing pipeline to help that process.

4
00:00:17.560 --> 00:00:22.840
We can create an datasets pre processing function that takes a single input.

5
00:00:22.840 --> 00:00:24.930
In our case that's a dictionary.

6
00:00:25.120 --> 00:00:29.210
But each value will be pre processed in some way.

7
00:00:29.230 --> 00:00:33.120
Let's start by defining the function called pre processing.

8
00:00:33.160 --> 00:00:41.180
We can add underscore our fan to the name to mention it's a function for arguments just specify inputs

9
00:00:41.960 --> 00:00:47.690
our inputs argument as we said is going to take a single line from our list of dictionaries that we

10
00:00:47.690 --> 00:00:55.070
created to ity of the go since we have our dictionary we need to separate each value to a different

11
00:00:55.070 --> 00:00:56.760
variable.

12
00:00:56.860 --> 00:01:06.290
For example let's write and or two is equal to inputs of an up to and now we created a variable for

13
00:01:06.290 --> 00:01:07.730
and to value.

14
00:01:07.730 --> 00:01:13.460
We are going to repeat this process for each variable in our inputs because we are going to perform

15
00:01:13.490 --> 00:01:17.510
different pre processing steps on each variable.

16
00:01:17.510 --> 00:01:25.610
For example we could kill all variables from 0 to 1 but we can scale and 2 from 0 to 1 and we can do

17
00:01:25.700 --> 00:01:28.430
something else to suit for example and so on.

18
00:01:29.750 --> 00:01:37.360
Then we are going to do the same thing 4 p.m. 10 so 2 and suit.

19
00:01:37.390 --> 00:01:44.500
Now we have separated every feature from our inputs to a separate variable will pre process every one

20
00:01:44.500 --> 00:01:50.260
of them differently so that you can get a sense of the whole pre processing function thing.

21
00:01:51.190 --> 00:01:57.670
And then we are going to return the data in the dictionary format as well but each value will be pre

22
00:01:57.670 --> 00:02:05.790
processed 1 A Quick side note here these operations that we are going to perform in this with you are

23
00:02:05.910 --> 00:02:09.990
randomly chosen for just for demonstration purposes.

24
00:02:10.260 --> 00:02:16.350
Since we don't have any model to train after this we are going to pre process every column differently

25
00:02:16.620 --> 00:02:22.260
so that you can see how it tends to flow transform can be used in your product.

26
00:02:22.320 --> 00:02:30.450
Let us define a new variable called as to normalized and here we are going to normalize or scale and

27
00:02:30.610 --> 00:02:37.800
to feature in some way for this one we are going to scale it in a way that we are going to subtract

28
00:02:37.800 --> 00:02:42.360
the mean from an R to a column from this value.

29
00:02:42.560 --> 00:02:49.730
That would be simple just use DFT dot me function to calculate the mean and provide a variable called

30
00:02:50.090 --> 00:02:58.910
and or to and subtract that value from end to we're going to perform this step for or two as well.

31
00:02:59.060 --> 00:03:11.620
So right as to a normalized is equal to ESA to minus DFT that mean of s to it now we normalized n to

32
00:03:11.620 --> 00:03:19.590
feature an s or two but we are still left with the intent and suits to be normalized 4 p.m. 10.

33
00:03:19.620 --> 00:03:26.730
We are going to name it normalized as well and for scaling we are going to scale it to the range from

34
00:03:26.730 --> 00:03:35.920
0 to 1 and there is a very helpful function called scale to 0 1 and provide a variable to it.

35
00:03:36.060 --> 00:03:38.490
In this case B M that.

36
00:03:38.730 --> 00:03:45.560
Lastly we have to normalize sort of variable and for suit we are going to scale it in a different way.

37
00:03:45.780 --> 00:03:51.750
This time we are going to use Min Max Keller which can be found in s killer as well.

38
00:03:52.020 --> 00:03:57.890
But in this case we have it in terms of flow transform which is very cool stuff.

39
00:03:57.920 --> 00:04:04.490
Now we have all of our variables normalized and ready to be exported and used in some model.

40
00:04:04.490 --> 00:04:06.810
We are going to return a Python dictionary.

41
00:04:06.920 --> 00:04:08.440
But first we need to create one.

42
00:04:09.110 --> 00:04:16.100
So each key in this dictionary that we are going to return can be named whatever you like but in this

43
00:04:16.100 --> 00:04:20.480
case I'm just going to name it the same way that we named our variables.

44
00:04:20.480 --> 00:04:27.240
So enter to normalized is a key and the value is the value of that feature.

45
00:04:27.260 --> 00:04:33.730
Just copy and paste these free more times and rename the starting position for our different.

46
00:04:33.740 --> 00:04:40.370
Now we have our dictionary that contains all the skilled features and it's time to return it so we can

47
00:04:40.430 --> 00:04:44.400
execute the cell in our function is defined.

48
00:04:44.500 --> 00:04:46.850
Here is the whole pre processing function.

49
00:04:46.900 --> 00:04:48.340
It's very simple.

50
00:04:48.340 --> 00:04:50.890
It can get really complicated really fast.

51
00:04:50.890 --> 00:04:54.490
It really depends on the stuff that you want to do with your data.

52
00:04:54.730 --> 00:05:02.080
In the next video we will talk about Apache beam but just about high level of it and how everything

53
00:05:02.080 --> 00:05:03.040
connects together.

54
00:05:03.040 --> 00:05:09.580
From this point on if you have any questions or comments so far please post them in the comments section

55
00:05:09.950 --> 00:05:11.950
otherwise I'll see you in the next tutorial.
