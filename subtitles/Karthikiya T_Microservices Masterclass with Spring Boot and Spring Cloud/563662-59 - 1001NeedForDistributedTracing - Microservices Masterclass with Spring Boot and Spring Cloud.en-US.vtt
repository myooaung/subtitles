WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.730
Let's talk about
distributed tracing

00:00:02.730 --> 00:00:04.560
with a real-world example.

00:00:04.560 --> 00:00:07.710
Assume that you have simple
client server architecture.

00:00:07.710 --> 00:00:09.060
And assume that we are running

00:00:09.060 --> 00:00:12.510
a monolithic application
on the server side.

00:00:12.510 --> 00:00:15.120
In case of monolithic
application,

00:00:15.120 --> 00:00:16.320
all the components would be

00:00:16.320 --> 00:00:18.240
residing under single codebase.

00:00:18.240 --> 00:00:22.260
And we'll run all of them
as a single application.

00:00:22.260 --> 00:00:25.245
Now in case of an
error, for example,

00:00:25.245 --> 00:00:28.410
maybe customer is experienced
in lag in response,

00:00:28.410 --> 00:00:30.510
your website matter
becomes slow.

00:00:30.510 --> 00:00:32.295
Or they might be a memory leak

00:00:32.295 --> 00:00:34.605
which is causing
performance issue.

00:00:34.605 --> 00:00:36.330
User is getting some kind of

00:00:36.330 --> 00:00:39.090
an error while
performing an operation.

00:00:39.090 --> 00:00:42.080
Whatever the issue is,
you know, where to go,

00:00:42.080 --> 00:00:43.670
how to analyze the problem,

00:00:43.670 --> 00:00:45.545
and how to fix the problem.

00:00:45.545 --> 00:00:48.485
Or at least you would know
whom to assign the defect to.

00:00:48.485 --> 00:00:50.600
For example, you know
that you need to dig into

00:00:50.600 --> 00:00:52.190
the logs and look at

00:00:52.190 --> 00:00:54.665
the stack trace to
figure out the problem.

00:00:54.665 --> 00:00:57.620
You would analyze it and fix it,

00:00:57.620 --> 00:01:00.050
would ascend the defect
to relevant team,

00:01:00.050 --> 00:01:03.125
whoever is responsible
for this application.

00:01:03.125 --> 00:01:06.455
However, in case
of microservices,

00:01:06.455 --> 00:01:08.825
things would get little tricky.

00:01:08.825 --> 00:01:10.835
In case of an error.

00:01:10.835 --> 00:01:12.335
You don't know where to go,

00:01:12.335 --> 00:01:13.880
how to analyze the problem,

00:01:13.880 --> 00:01:15.425
how to fix the problem,

00:01:15.425 --> 00:01:17.795
or whom to assign
the defect too.

00:01:17.795 --> 00:01:19.880
Because there could be

00:01:19.880 --> 00:01:23.720
multiple microservices involved
to handle each request.

00:01:23.720 --> 00:01:26.930
And error might be coming
in on microservice or

00:01:26.930 --> 00:01:30.455
multiple microservices
throughout the request flow.

00:01:30.455 --> 00:01:33.455
One obvious way to
figure out the problem

00:01:33.455 --> 00:01:37.130
is to dig into logs of each
and every micro-service,

00:01:37.130 --> 00:01:38.840
aggregate all of them,

00:01:38.840 --> 00:01:41.990
and somehow try to figure
out what's going wrong.

00:01:41.990 --> 00:01:44.135
And of course, as
you would guess,

00:01:44.135 --> 00:01:45.710
it's not feasible to dig

00:01:45.710 --> 00:01:47.450
through the logs
of each and every

00:01:47.450 --> 00:01:51.440
microservice and try to
figure out the problem.

00:01:51.440 --> 00:01:53.795
It will take lot of hours, days,

00:01:53.795 --> 00:01:56.150
or sometimes human never
be able to resolve

00:01:56.150 --> 00:01:59.180
the issue and add it
to the complexity.

00:01:59.180 --> 00:02:01.070
There could be
multiple instances of

00:02:01.070 --> 00:02:03.815
each microservice on
a Cloud enrollment.

00:02:03.815 --> 00:02:07.025
It can be residing on
multiple different platforms,

00:02:07.025 --> 00:02:08.000
could be written in

00:02:08.000 --> 00:02:09.994
multiple different
programming languages.

00:02:09.994 --> 00:02:11.540
I'm talking from
the perspective of

00:02:11.540 --> 00:02:14.495
a person who's trying to
figure out the issue.

00:02:14.495 --> 00:02:16.490
If they would analyze the issue,

00:02:16.490 --> 00:02:19.040
they have to know each
and every technology out

00:02:19.040 --> 00:02:21.875
there that is being used
in these microservices.

00:02:21.875 --> 00:02:23.990
You can't expect an
individual to be

00:02:23.990 --> 00:02:26.765
proficient in each and
every programming language.

00:02:26.765 --> 00:02:29.210
You can't expect them to
know how to dig through

00:02:29.210 --> 00:02:31.895
the logs of each and
every microservice.

00:02:31.895 --> 00:02:35.000
And it becomes difficult
even to raise an issue.

00:02:35.000 --> 00:02:36.560
He can't just randomly make

00:02:36.560 --> 00:02:38.060
a guess on where the error might

00:02:38.060 --> 00:02:41.090
be and raise a
defect on the team.

00:02:41.090 --> 00:02:42.680
They're going to
come back and say,

00:02:42.680 --> 00:02:45.140
we've tested everything and
everything is working fine.

00:02:45.140 --> 00:02:46.625
You just wasted our time.

00:02:46.625 --> 00:02:48.785
Not a pleasant situation.

00:02:48.785 --> 00:02:51.290
It creates that
communication gap within

00:02:51.290 --> 00:02:54.710
teams and cause a
lot of trouble.

00:02:54.710 --> 00:02:58.430
Obviously, we need a
solution to this problem.

00:02:58.430 --> 00:03:00.710
And that's where
distributed tracing

00:03:00.710 --> 00:03:02.675
would come into picture.

00:03:02.675 --> 00:03:05.000
Distributed tracing offers

00:03:05.000 --> 00:03:07.010
a user-friendly diagnostic tool.

00:03:07.010 --> 00:03:09.770
It allows you to see a
chain of events that

00:03:09.770 --> 00:03:13.865
occurred for each request
throughout the request flow.

00:03:13.865 --> 00:03:16.370
It allows you to dig deep into

00:03:16.370 --> 00:03:19.145
individual operation,
you need service.

00:03:19.145 --> 00:03:21.260
And it also offers the ability

00:03:21.260 --> 00:03:23.555
to search and narrow
down the problem.

00:03:23.555 --> 00:03:27.080
You'll have a better picture
of this in later videos.

00:03:27.080 --> 00:03:29.250
I'll see you in next one.
