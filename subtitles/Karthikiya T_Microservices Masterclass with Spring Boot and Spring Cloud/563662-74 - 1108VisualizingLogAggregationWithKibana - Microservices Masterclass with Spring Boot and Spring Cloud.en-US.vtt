WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.760
Let us get back to
our original scenario

00:00:02.760 --> 00:00:04.350
of storing all the data inside

00:00:04.350 --> 00:00:05.970
the elastic search and

00:00:05.970 --> 00:00:09.435
then visualizing the
data using Kibana.

00:00:09.435 --> 00:00:11.519
What I've done here inside

00:00:11.519 --> 00:00:14.750
the log slash config is
I've just simply replace

00:00:14.750 --> 00:00:16.415
the file plugin with

00:00:16.415 --> 00:00:18.470
Elastic Search because that's

00:00:18.470 --> 00:00:20.330
where I wanted to
send the data to.

00:00:20.330 --> 00:00:23.570
Then we will ask banner
to fetch the data from

00:00:23.570 --> 00:00:28.175
Elasticsearch to see
the aggregated logs.

00:00:28.175 --> 00:00:32.135
In addition to that, I've also
started all our services,

00:00:32.135 --> 00:00:35.960
and I've also started
all the silvers,

00:00:35.960 --> 00:00:39.740
started Elasticsearch Kibana, as

00:00:39.740 --> 00:00:43.400
well as Log Stash and
of course zip canals.

00:00:43.400 --> 00:00:46.970
Well. Now this process will

00:00:46.970 --> 00:00:50.704
take a lot of time here to
have a lot of patients.

00:00:50.704 --> 00:00:53.960
It will take awhile to see
your service up and running.

00:00:53.960 --> 00:00:55.415
Have patients.

00:00:55.415 --> 00:00:57.560
But once after they're
up and running,

00:00:57.560 --> 00:01:02.180
just go to Kibana dashboard.

00:01:02.180 --> 00:01:05.150
It runs on the portfolios is 01.

00:01:05.150 --> 00:01:07.820
Once you visit this URL,

00:01:07.820 --> 00:01:10.505
you would see a dashboard.

00:01:10.505 --> 00:01:12.230
When you're doing it
for the first time,

00:01:12.230 --> 00:01:15.200
you might see a prompt
asking you whether to

00:01:15.200 --> 00:01:18.545
use the sample data you want
to explore on your own.

00:01:18.545 --> 00:01:22.985
Choose the option that
says explore on your own.

00:01:22.985 --> 00:01:24.875
Because we already have

00:01:24.875 --> 00:01:27.755
elastic search where
the data is residing.

00:01:27.755 --> 00:01:31.175
You can choose that option
to explore on your own.

00:01:31.175 --> 00:01:37.230
Once you do that, you need
to search for index pattern.

00:01:40.260 --> 00:01:42.205
Go there.

00:01:42.205 --> 00:01:44.890
Basically index pattern
will tell us Cabana,

00:01:44.890 --> 00:01:48.220
which elastic search
indexes we want to explore.

00:01:48.220 --> 00:01:49.870
In our case, we want to explore

00:01:49.870 --> 00:01:53.590
all the logs information
from elastic search.

00:01:53.590 --> 00:01:58.225
For that, I have
specified this pattern.

00:01:58.225 --> 00:01:59.830
Human or be seeing this for

00:01:59.830 --> 00:02:01.420
the first time, you
need to create one.

00:02:01.420 --> 00:02:03.580
And the way you created
this by clicking

00:02:03.580 --> 00:02:07.045
this button that says
Create index pattern.

00:02:07.045 --> 00:02:12.250
And you specify Log
Stash, hyphen star.

00:02:12.250 --> 00:02:14.940
It shows a timestamp as well.

00:02:14.940 --> 00:02:18.205
Then click on Create
index pattern

00:02:18.205 --> 00:02:21.230
and you get something like this.

00:02:21.510 --> 00:02:23.920
Now obviously there are a lot

00:02:23.920 --> 00:02:25.480
of things that we can discuss in

00:02:25.480 --> 00:02:29.320
Cabana that it would take an
entire course on its own.

00:02:29.320 --> 00:02:33.160
Most of the time we don't
get to use all its features.

00:02:33.160 --> 00:02:35.785
It's more or less like
a repeated root task

00:02:35.785 --> 00:02:38.575
in your day-to-day job
to analyze the issue.

00:02:38.575 --> 00:02:41.140
Once I'd used to it, you
should feel comfortable

00:02:41.140 --> 00:02:45.260
analyzing the problem. Kibana.

00:02:45.570 --> 00:02:49.690
Let's now go to Postman and send

00:02:49.690 --> 00:02:54.460
the request to product service.

00:02:54.460 --> 00:02:57.350
Now let's go to zip Ken.

00:02:58.500 --> 00:03:04.670
Let us try to get the
trace ID through ZIP can.

00:03:05.050 --> 00:03:07.220
I'm going to choose
the service name

00:03:07.220 --> 00:03:10.470
asked product service run query.

00:03:10.810 --> 00:03:15.600
This event happened
a few seconds ago.

00:03:16.270 --> 00:03:20.010
Then we're going to
copy the trace ID.

00:03:22.540 --> 00:03:30.510
Then we go to discover option
here on the left-hand menu.

00:03:32.380 --> 00:03:34.700
There you get to see

00:03:34.700 --> 00:03:38.704
the logs propagating
from elastic search.

00:03:38.704 --> 00:03:44.340
Let's now the trace ID
that we just copied.

00:03:45.130 --> 00:03:48.425
Once you search,

00:03:48.425 --> 00:03:52.310
you get that aggregated
view of all the logs.

00:03:52.310 --> 00:03:55.715
And each will belong
to individual service.

00:03:55.715 --> 00:03:57.650
Have products, service pricing,

00:03:57.650 --> 00:03:59.360
service, inventory, service.

00:03:59.360 --> 00:04:01.520
These are all the
services involved in

00:04:01.520 --> 00:04:03.950
this request that corresponds

00:04:03.950 --> 00:04:06.500
to this particular trace ID.

00:04:06.500 --> 00:04:08.315
Now if you have an issue,

00:04:08.315 --> 00:04:10.790
you can go through
all these logs.

00:04:10.790 --> 00:04:12.995
Try to figure out the problem.

00:04:12.995 --> 00:04:15.860
This says a lot of time.

00:04:15.860 --> 00:04:18.410
Now let us get inside

00:04:18.410 --> 00:04:23.015
individually event and see
how they got populated.

00:04:23.015 --> 00:04:25.620
Let's go to JSON tab.

00:04:30.300 --> 00:04:38.180
Let me zoom in a bit.
This is adjacent output.

00:04:40.380 --> 00:04:43.000
But if you notice,
even though we have

00:04:43.000 --> 00:04:46.345
added few additional
fields using grok plug-in,

00:04:46.345 --> 00:04:48.175
they're not seen here.

00:04:48.175 --> 00:04:50.110
That's because that pattern

00:04:50.110 --> 00:04:52.945
didn't match with
this log messages.

00:04:52.945 --> 00:04:56.440
That's why Log Stash has added

00:04:56.440 --> 00:04:59.995
this additional tax saying
grok parse failure.

00:04:59.995 --> 00:05:02.395
This is what I was
talking about.

00:05:02.395 --> 00:05:04.570
The reason for this is we have

00:05:04.570 --> 00:05:09.535
this additional text which
is not part of the pattern.

00:05:09.535 --> 00:05:12.165
Now you can take this
as an assignment,

00:05:12.165 --> 00:05:16.115
try to figure out a pattern
for this text right here,

00:05:16.115 --> 00:05:20.280
and then populate all
those fields and Gabbana.

00:05:20.350 --> 00:05:28.560
Fixed pattern right here
between log level and bytes.

00:05:29.500 --> 00:05:33.030
Alright, I'll see
you in the next one.
