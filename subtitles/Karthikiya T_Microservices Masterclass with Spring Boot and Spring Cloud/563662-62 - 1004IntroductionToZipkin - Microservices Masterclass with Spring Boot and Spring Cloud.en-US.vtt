WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:02.385
Let us talk about Zipcar.

00:00:02.385 --> 00:00:05.445
Earlier we had talked
about springs loop,

00:00:05.445 --> 00:00:08.130
which has somewhat
solved the problem of

00:00:08.130 --> 00:00:11.595
including tracing information
in all the services.

00:00:11.595 --> 00:00:14.715
But practically speaking,
in case of an issue,

00:00:14.715 --> 00:00:18.930
it is still a lot of manual
work to identify the problem.

00:00:18.930 --> 00:00:21.570
You still have to go
region every service.

00:00:21.570 --> 00:00:23.700
Take a look at the logs and find

00:00:23.700 --> 00:00:26.385
the logs that correspond
to a particular trace ID.

00:00:26.385 --> 00:00:28.140
Just to be able to understand

00:00:28.140 --> 00:00:29.700
what are all the
services included in

00:00:29.700 --> 00:00:33.735
the request and where
the problem might be?

00:00:33.735 --> 00:00:36.865
We definitely need
a better solution.

00:00:36.865 --> 00:00:39.380
We need a tool that will help us

00:00:39.380 --> 00:00:41.780
visualize the transaction
flow as they move

00:00:41.780 --> 00:00:44.780
across multiple
microservices that

00:00:44.780 --> 00:00:46.760
we get a complete
picture of matter.

00:00:46.760 --> 00:00:50.810
All the microservices involved
in the request ability to

00:00:50.810 --> 00:00:52.400
look at response times

00:00:52.400 --> 00:00:55.430
of each and every
individual microservice.

00:00:55.430 --> 00:00:58.100
Microservices taking
way too long.

00:00:58.100 --> 00:00:59.720
We know that we need
to go there and

00:00:59.720 --> 00:01:01.850
do something to fix it.

00:01:01.850 --> 00:01:05.120
Help us identify any
performance issues.

00:01:05.120 --> 00:01:07.430
Are the ability to look at

00:01:07.430 --> 00:01:08.795
the total amount of time

00:01:08.795 --> 00:01:11.420
it took to complete
the transaction.

00:01:11.420 --> 00:01:15.065
Well, Zipcar is dancer for that.

00:01:15.065 --> 00:01:17.900
Zip can receives the
trace information from

00:01:17.900 --> 00:01:21.650
all the services and it
aggregates them based

00:01:21.650 --> 00:01:23.990
on the trace IMD provide

00:01:23.990 --> 00:01:28.700
multiple views for lookup to
identify a request status.

00:01:28.700 --> 00:01:31.055
So that in case
there is an error,

00:01:31.055 --> 00:01:33.950
maybe one of the services
returning find the data.

00:01:33.950 --> 00:01:35.870
Then we know that
we need to go to

00:01:35.870 --> 00:01:38.330
that service and do
something to fix it.

00:01:38.330 --> 00:01:42.980
It will help us to identify
service dependencies so that

00:01:42.980 --> 00:01:44.360
you don't have to
manually look at

00:01:44.360 --> 00:01:47.570
the logs to figure out all
the services involved,

00:01:47.570 --> 00:01:50.900
which will also help us
identify the failure points and

00:01:50.900 --> 00:01:54.650
latencies in the request flow.

00:01:54.650 --> 00:01:56.480
It will help us to identify

00:01:56.480 --> 00:01:58.864
calls to duplicate IT services,

00:01:58.864 --> 00:02:02.480
and to identify any
performance issues like

00:02:02.480 --> 00:02:07.174
memory leaks are all
utilization of CPU, etc.

00:02:07.174 --> 00:02:11.015
Internally, zipcar has
all these modules.

00:02:11.015 --> 00:02:13.610
The first of which is
the collector module,

00:02:13.610 --> 00:02:15.920
which is responsible
for collecting

00:02:15.920 --> 00:02:17.030
all the trace information

00:02:17.030 --> 00:02:19.400
sent from multiple
microservices.

00:02:19.400 --> 00:02:21.184
And then we have storage.

00:02:21.184 --> 00:02:23.090
This is where all the
tracing information

00:02:23.090 --> 00:02:25.130
will be stored and
would be indexed in

00:02:25.130 --> 00:02:27.920
a manner that can be
fetched at later point of

00:02:27.920 --> 00:02:31.655
time using the Search API
provided by a Zipcar.

00:02:31.655 --> 00:02:36.140
Zipcar also has a search API
and we can send requests

00:02:36.140 --> 00:02:37.280
to it to get

00:02:37.280 --> 00:02:40.580
relevant details to help
us identify the problem.

00:02:40.580 --> 00:02:43.670
For example, we could
send a request to get

00:02:43.670 --> 00:02:45.230
all the requests to

00:02:45.230 --> 00:02:48.920
a particular service
in past five minutes.

00:02:48.920 --> 00:02:50.555
You will know what I mean.

00:02:50.555 --> 00:02:52.400
Once we take a look
at an example,

00:02:52.400 --> 00:02:54.830
then we have web UI,

00:02:54.830 --> 00:02:57.410
which is a convenience
provided by

00:02:57.410 --> 00:03:00.845
zip Ken for us to send
request to the Search API.

00:03:00.845 --> 00:03:02.780
That we don't have
to manually send

00:03:02.780 --> 00:03:05.165
the request to the rest API.

00:03:05.165 --> 00:03:09.300
Instead, we can use the
UI divided by ZIP can.

00:03:09.730 --> 00:03:13.835
Hit are all the steps
involved to set up zip Ken.

00:03:13.835 --> 00:03:16.355
First of all, we need
a zip code server

00:03:16.355 --> 00:03:19.350
and there are multiple
ways to get that.

00:03:19.350 --> 00:03:22.165
We can use an
executable jar file.

00:03:22.165 --> 00:03:25.855
We can also have the
conserver as a microservice.

00:03:25.855 --> 00:03:28.375
By adding the dependency.

00:03:28.375 --> 00:03:31.780
We can also have it
as a Docker image.

00:03:31.780 --> 00:03:36.430
Then waiting fluid sleuth
ZIP can dependency in

00:03:36.430 --> 00:03:39.130
all the services
that the services

00:03:39.130 --> 00:03:42.505
can send the tracing
information to zip in server.

00:03:42.505 --> 00:03:45.640
This is what would glue
both services and the zip

00:03:45.640 --> 00:03:48.834
in silver to track the
tracing information.

00:03:48.834 --> 00:03:52.345
Finally, we need to
add 11 conflicts.

00:03:52.345 --> 00:03:54.985
To point to the zip conserver.

00:03:54.985 --> 00:03:56.875
The services would be aware

00:03:56.875 --> 00:03:59.560
where does it
conserver is residing?

00:03:59.560 --> 00:04:02.510
I'll see you in the next one.
