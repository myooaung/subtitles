WEBVTT
1
00:00:05.420 --> 00:00:06.490
Welcome back everyone.

2
00:00:06.530 --> 00:00:14.250
This lecture on time resampling will usually get financial data that has a date time index on a smaller

3
00:00:14.250 --> 00:00:15.380
time scale.

4
00:00:15.420 --> 00:00:19.910
That is something like every row corresponds to every day or every hour.

5
00:00:19.920 --> 00:00:25.830
However it's often a good idea for analysis purposes to aggregate that data based off some sort of frequency

6
00:00:25.920 --> 00:00:31.250
such as doing an aggregation on monthly data or quarterly data.

7
00:00:31.250 --> 00:00:34.970
Now you may think that using a group buy could solve part of the issue.

8
00:00:35.000 --> 00:00:39.380
You would just simply do a group buy and then buy every three months and that will give you quarterly

9
00:00:39.380 --> 00:00:40.210
data.

10
00:00:40.220 --> 00:00:45.440
However a simple GROUP BY statement isn't really smart enough to understand things like business quarters

11
00:00:45.470 --> 00:00:50.350
or business year starts or the fact that Business Week starts on Monday instead of Sunday.

12
00:00:50.360 --> 00:00:51.550
That sort of thing.

13
00:00:51.590 --> 00:00:58.210
Luckily Pandurs has frequency sampling tools built into it to actually help with this sort of problem.

14
00:00:59.040 --> 00:01:03.620
To learn about this we're going to be using a stock market data set of Wal-Mart prices.

15
00:01:03.630 --> 00:01:09.430
It's located under the time data folder under the five dash panels with time series folder that corresponds

16
00:01:09.430 --> 00:01:10.920
to the section of the course.

17
00:01:10.920 --> 00:01:13.290
Let's open up into Jupiter notebook and get started.

18
00:01:14.320 --> 00:01:19.080
OK here I am at my Jupiter notebook I've imported the usual suspects here num PI Panchos and map plot

19
00:01:19.130 --> 00:01:22.250
lib and we're going to continue by reading in our data.

20
00:01:22.250 --> 00:01:24.290
I want to show you two methods of reading in the data.

21
00:01:24.290 --> 00:01:27.560
One is a little longer and it's a little more customizable.

22
00:01:27.560 --> 00:01:29.660
The other one is a little faster.

23
00:01:29.840 --> 00:01:37.700
So we're going to say is DMF is equal to PD the read CSFB and then we'll passen the actual file path

24
00:01:37.820 --> 00:01:42.200
depending on where you are your file path may be different but aren't going to say time data Wal-Mart's

25
00:01:42.290 --> 00:01:43.140
doxies V.

26
00:01:43.250 --> 00:01:48.990
This matches the notes and I will read this in and if I check out the head of my data frame I can see

27
00:01:48.990 --> 00:01:54.110
you have a couple of columns here and note here that there's a date column and the date column is what

28
00:01:54.110 --> 00:01:56.050
I actually want to be the index.

29
00:01:56.390 --> 00:01:58.860
So if I take a look at the date column.

30
00:01:58.860 --> 00:02:02.020
Actually if we take a look at the entire data frame we just dig up the info.

31
00:02:02.300 --> 00:02:08.300
I can see the date column is actually says non-nil object which indicates that this is still just a

32
00:02:08.300 --> 00:02:08.860
string.

33
00:02:08.900 --> 00:02:10.740
It's not a date time object.

34
00:02:10.790 --> 00:02:15.230
So we need to convert this to a Date Time object and that's actually quite simple of pandas.

35
00:02:15.230 --> 00:02:23.320
There's different ways of doing this but the simplest way is to say DSF date and set it equal to PD.

36
00:02:23.920 --> 00:02:30.040
And there's a special To date time function and you can passen an entire series such as the original

37
00:02:30.040 --> 00:02:30.730
date.

38
00:02:30.850 --> 00:02:35.050
And what this is going to do is you're passing in the original series of the date which is just strings

39
00:02:35.400 --> 00:02:42.790
and PD to date time is going to reformat it to be a Date Time object and then set that to date.

40
00:02:42.790 --> 00:02:46.890
Now luckily for us this string is already in the standard notation.

41
00:02:46.930 --> 00:02:50.540
It expects a year dash a month dash a day.

42
00:02:50.710 --> 00:02:56.650
But in case you have a different notation for your specific date string what you can do is under PD

43
00:02:56.650 --> 00:02:57.480
to date time.

44
00:02:57.490 --> 00:03:02.860
If you do shifts have here you'll notice there's a format argument and this format argument basically

45
00:03:02.860 --> 00:03:07.710
takes in a string that essentially tells Pancho's what format it's in.

46
00:03:07.720 --> 00:03:11.500
So if you scroll down here there's an example what that string would look like.

47
00:03:11.530 --> 00:03:13.390
So it keeps rolling downhill format.

48
00:03:13.420 --> 00:03:17.510
So this is just a string telling you what format to expect that in.

49
00:03:17.700 --> 00:03:24.580
So in this case the sort of string says Okay expect a day then a forward slash then a month than a forward

50
00:03:24.580 --> 00:03:25.070
slash.

51
00:03:25.070 --> 00:03:26.010
Then a year.

52
00:03:26.200 --> 00:03:30.910
And there's actually different types of special characters that start with those percent signs that

53
00:03:30.910 --> 00:03:34.300
tells those what kind of day or what kind of month to expect.

54
00:03:34.300 --> 00:03:39.720
So for instance the difference between percent signed lowercase Y are the last two digits of a year.

55
00:03:39.760 --> 00:03:41.110
Percent sign uppercase y.

56
00:03:41.110 --> 00:03:46.270
Those are the fool for just a year etc. and you can look this up in the Panas documentation or on the

57
00:03:46.270 --> 00:03:50.080
provided note books for what these key terms are this per cent sign.

58
00:03:50.070 --> 00:03:50.910
Keep term.

59
00:03:51.430 --> 00:03:55.420
Luckily for us we don't really need to provide that information because saurian the standard format.

60
00:03:55.540 --> 00:04:00.210
So it's going to hit enter here and then check out the head of our data frame.

61
00:04:00.550 --> 00:04:08.490
Notice it looks the same but when I say that info I can see that it's now a date time object.

62
00:04:08.490 --> 00:04:10.330
So we have that ready to go.

63
00:04:10.620 --> 00:04:15.090
Now another way you could do this possibly is instead of saying PD to date time and then pass in the

64
00:04:15.090 --> 00:04:15.810
series.

65
00:04:15.810 --> 00:04:18.070
You could also just apply P-T to date times.

66
00:04:18.090 --> 00:04:19.480
Every element in the series.

67
00:04:19.620 --> 00:04:26.840
So do something like dates apply and then call PD to that time.

68
00:04:26.850 --> 00:04:28.560
I've seen that as well.

69
00:04:28.560 --> 00:04:31.260
And then you just set something like this.

70
00:04:31.390 --> 00:04:33.950
So it's really up to you these perform the exact same function.

71
00:04:34.000 --> 00:04:38.710
I would recommend doing it this way with PD to date time and then pass on the series to me that's a

72
00:04:38.710 --> 00:04:39.490
little more readable.

73
00:04:39.550 --> 00:04:41.600
But either way should be fine.

74
00:04:42.220 --> 00:04:45.330
And let me delete everything in the cell and then clear it out.

75
00:04:45.590 --> 00:04:45.920
OK.

76
00:04:45.940 --> 00:04:50.620
So we have our new date column and now no longer just a string but it's actually a date time object

77
00:04:51.010 --> 00:04:59.360
which means we should be able to set it as the index so I can say set index and then in date.

78
00:04:59.390 --> 00:05:02.760
I also want call this in place.

79
00:05:02.870 --> 00:05:04.120
True.

80
00:05:04.160 --> 00:05:10.120
So if I take a look at my data frame I can see I have now the date time index and then do various columns.

81
00:05:10.340 --> 00:05:13.970
So that's kind of the longer more manual way but it gives you a lot more control especially of this

82
00:05:13.970 --> 00:05:14.570
PD today.

83
00:05:14.600 --> 00:05:18.200
Time lets you have full control over whatever format that dates in.

84
00:05:18.200 --> 00:05:23.570
If you actually want to read in the dates as an index since the very beginning you could have just appear

85
00:05:24.200 --> 00:05:32.090
specified PD that read V and you can also add in an argument index underscore column and specify that

86
00:05:32.090 --> 00:05:34.610
the index column is the date.

87
00:05:34.610 --> 00:05:39.920
However keep in mind that this will still read it in as a string and you have to reformat the actual

88
00:05:39.920 --> 00:05:42.470
index to be a Date Time object.

89
00:05:42.490 --> 00:05:47.550
Now one additional thing to note here is that pandas actually comes with another parameter you can add

90
00:05:47.550 --> 00:05:51.680
in to read CSP that will actually do this parsing of data for you.

91
00:05:51.950 --> 00:05:56.930
And that's going to be parse underscore dates and you can set it to true.

92
00:05:57.050 --> 00:06:02.570
And what this does is if you provide an index arguments such as set the date column to be the index

93
00:06:02.570 --> 00:06:11.120
location parse dates set to true will automatically Trius best to parse this column as a Date Time object.

94
00:06:11.120 --> 00:06:12.440
So let's try this out.

95
00:06:12.740 --> 00:06:19.480
Well Re-Read our data frame and then if I check out the index it is now a date time index.

96
00:06:19.530 --> 00:06:20.790
So this is the faster way.

97
00:06:20.850 --> 00:06:25.400
It's not as controllable as the way we just showed up the step by step.

98
00:06:25.500 --> 00:06:30.600
But if your data is already an expected format this will save you a lot of time just passing your data

99
00:06:30.990 --> 00:06:34.140
set the column date and then say parse dates equal to true.

100
00:06:34.140 --> 00:06:38.430
There's other things you can do have parse dates such as adding in an actual list of columns to try

101
00:06:38.430 --> 00:06:40.080
to parse things of that nature.

102
00:06:40.080 --> 00:06:42.550
You can always check the documentation for more details.

103
00:06:42.630 --> 00:06:46.790
So we'll just stick it this way in this column parse that's equal to true.

104
00:06:46.950 --> 00:06:53.240
And I'm going to scroll all the way down here come back toward the F enter and everything's the same.

105
00:06:53.280 --> 00:06:57.150
So let's continue along and start talking about time resampling.

106
00:06:57.270 --> 00:07:01.920
So to do any sort of time resampling we're going to need a date time index.

107
00:07:01.920 --> 00:07:04.210
So if we scroll down here all the weights.

108
00:07:04.470 --> 00:07:05.910
This next empty cell.

109
00:07:05.940 --> 00:07:13.490
Again just a reminder we have a date time index what I can then do is off this data frame say re sample

110
00:07:15.410 --> 00:07:20.210
and then provide what's known as a rule and the rule is just indicating how do you actually want to

111
00:07:20.210 --> 00:07:22.070
read sample this data.

112
00:07:22.070 --> 00:07:27.410
So for instance I can say rule is equal to the letter A.

113
00:07:27.620 --> 00:07:31.770
And then if I run this I'll say OK a date time index rose sampler.

114
00:07:31.940 --> 00:07:39.200
And this is essentially acting as a group by method specific to time series data that is Date Time objects

115
00:07:39.560 --> 00:07:45.110
and there's keywords for every kind of possible series offset that you could think of.

116
00:07:45.110 --> 00:07:49.340
So I want to bring in a little table that's available to you in the corresponding notes for this lecture.

117
00:07:49.520 --> 00:07:52.490
So we bring in from my right hand side of the screen.

118
00:07:52.490 --> 00:07:55.590
So there's a little table here that is available to you in your notes.

119
00:07:55.760 --> 00:08:00.530
And there's an alias corresponding to every single type of description and you can see there's dozens

120
00:08:00.530 --> 00:08:02.230
here that are very very useful.

121
00:08:02.240 --> 00:08:03.490
We're just using a.

122
00:08:03.500 --> 00:08:05.310
Which is year and the frequency.

123
00:08:05.420 --> 00:08:09.320
So we can expect the last day of the year as our resampling.

124
00:08:09.320 --> 00:08:11.400
So we'll bring this aside now.

125
00:08:11.730 --> 00:08:16.940
And you'll notice here you have a time index free sampler saying OK it's resampling based on the end

126
00:08:16.940 --> 00:08:22.190
of the year but any sort of resampling is going to need an aggregate method to actually give some sort

127
00:08:22.190 --> 00:08:27.540
of result which means essentially operating like a group by you can then say something like mean.

128
00:08:27.830 --> 00:08:33.270
And now what this will do is it gives you the mean value based off the resampling of the end of year.

129
00:08:33.500 --> 00:08:43.730
So everything before 2012 12:31 had a mean of 67 everything between 2012 12:31 and 2013 12:31 at the

130
00:08:43.820 --> 00:08:49.360
end of year had a mean of 75 etc. and you can just keep doing this for different resampling.

131
00:08:49.490 --> 00:08:53.030
So I can do another resampling rule let's try doing a quarterly one.

132
00:08:53.060 --> 00:08:56.690
So you would look up in that table and then you would realize it's you.

133
00:08:56.810 --> 00:08:59.560
Some of them have obvious Lutter some of them don't.

134
00:08:59.630 --> 00:09:01.700
And then you would get now quarterly sampling.

135
00:09:01.700 --> 00:09:06.550
So here we have quarterly resampling and then you can also do business quarter and frequencies.

136
00:09:06.560 --> 00:09:09.640
You can do something like BQ for business quarters.

137
00:09:09.670 --> 00:09:14.420
That means based off of your business quarters which may slightly end the dates if it's the end of quarters

138
00:09:14.420 --> 00:09:17.110
or Friday that sort of thing instead of just they ran them Saturday.

139
00:09:17.390 --> 00:09:21.830
So that's important to know as far as the different rules that you can apply and you can actually also

140
00:09:21.830 --> 00:09:27.290
apply your own custom functions to any of these samples just as you could if any group by method.

141
00:09:27.290 --> 00:09:29.030
So let me show you what I mean by that.

142
00:09:29.060 --> 00:09:30.180
Scroll down here.

143
00:09:31.130 --> 00:09:40.610
If I say read sample and I give it a rule to be sure to end the year again and ask for the max that's

144
00:09:40.610 --> 00:09:46.820
going to return the max price per that year basically ending it on the last day of the year.

145
00:09:47.000 --> 00:09:52.190
But let's say I actually wanted to return the first instance of the period regardless of any sampling

146
00:09:52.190 --> 00:09:56.260
rate what I can then do is instead of Max I'll define my own function.

147
00:09:56.480 --> 00:10:01.610
Or you could even do this as a lamb expression depending on the complexity of it all create a function

148
00:10:01.610 --> 00:10:05.810
called first day and it takes in some entry.

149
00:10:05.940 --> 00:10:08.000
In this case we kind of expect the series here.

150
00:10:08.310 --> 00:10:12.890
And then it's just going to return entry zero.

151
00:10:13.050 --> 00:10:15.330
So indexing the very first entry off of this.

152
00:10:15.330 --> 00:10:17.060
So this is called first day.

153
00:10:17.190 --> 00:10:23.400
And instead of doing Max here once I've ran this and the find the first thing I can say OK take my data

154
00:10:23.400 --> 00:10:30.400
frame resampling based off the year end and then instead of calling Max or mean or deviation that sort

155
00:10:30.400 --> 00:10:37.150
of thing I will just apply my own kind of time series function which is called first day and no I'm

156
00:10:37.150 --> 00:10:40.680
not calling it I'm just passing it in so I'm not executing it of princes.

157
00:10:40.780 --> 00:10:43.000
I'm just passing it in like this.

158
00:10:43.000 --> 00:10:50.440
So now what this does is it returns the very first sample or entry for this particular time period.

159
00:10:50.890 --> 00:10:51.360
OK.

160
00:10:51.610 --> 00:10:57.330
So let's show you what you could do with visualization with these samplings.

161
00:10:57.340 --> 00:11:00.340
It's not super common to have to do your own applications this way.

162
00:11:00.400 --> 00:11:05.020
As far as doing it to apply because most of the things you want already built in but just keep that

163
00:11:05.020 --> 00:11:11.080
in mind it is possible that moving along to the visualisations want to show you kind of two quick visualizations

164
00:11:11.170 --> 00:11:13.260
illustrating why you would want to do something like this.

165
00:11:13.450 --> 00:11:16.790
Let's say you actually wanted the kind of yearly and the mean.

166
00:11:16.810 --> 00:11:17.520
How would you do that.

167
00:11:17.530 --> 00:11:25.570
Well you could ask for the close column and then you can call the sample not just offer data frame but

168
00:11:25.570 --> 00:11:31.090
actually off the series or the column itself and then we'll sample with a.

169
00:11:31.660 --> 00:11:36.900
And then I'm going to ask for the mean of the closing price and then it's going to do a bar plot.

170
00:11:37.000 --> 00:11:45.780
So same plots kind as equal to bar so you can see our output here gives us a nice little bar plot indicating

171
00:11:45.780 --> 00:11:49.670
the mean or average for those years themselves.

172
00:11:49.860 --> 00:11:53.940
And we can kind of edit this to sample for instance by the month.

173
00:11:54.270 --> 00:12:00.940
So let's say I'm here and if I run this now I get this monthly kind of average.

174
00:12:00.940 --> 00:12:04.990
So you can see there's a big dip in sales there now formatting wise you're going to use all your map

175
00:12:04.990 --> 00:12:13.970
Cutlip skills to reformat this but also do something like just a bigger fix size specific size 16 Comus

176
00:12:13.980 --> 00:12:14.570
6.

177
00:12:14.620 --> 00:12:16.710
I don't know that may be quite large but OK here a year.

178
00:12:17.020 --> 00:12:21.220
And it kind of a nice little figure here can see the dip in monthly sales et cetera.

179
00:12:21.610 --> 00:12:25.070
So that's the kind of thing you would be using for resampling.

180
00:12:25.180 --> 00:12:30.550
Now note that I'm using a bar plot because technically these are all categorical points.

181
00:12:30.550 --> 00:12:36.400
This is not a continuous value because the dates are actually holding some sort of aggregate method

182
00:12:36.400 --> 00:12:37.140
on them.

183
00:12:37.180 --> 00:12:38.400
So keep that in mind.

184
00:12:38.530 --> 00:12:40.090
This is not a continuous plot.

185
00:12:40.090 --> 00:12:41.380
This is a bar plot.

186
00:12:41.530 --> 00:12:44.320
So it's a little different than something like a histogram.

187
00:12:44.370 --> 00:12:47.230
OK thanks everyone and I'll see you at the next lecture.
