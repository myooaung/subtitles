WEBVTT
1
00:00:00.420 --> 00:00:05.430
Now we are going to launch our system for that we will use to overcome demand.

2
00:00:05.970 --> 00:00:08.520
And we will launch our system.

3
00:00:09.090 --> 00:00:18.930
So this may well promote this image from Docker registry, and it may also pull those exporters ElasticSearch

4
00:00:18.930 --> 00:00:25.230
exporter and PostgreSQL exporter from Docker registry.

5
00:00:25.230 --> 00:00:32.040
Once it is done doing that, then it will start all our containers once it starts our container.

6
00:00:32.580 --> 00:00:39.720
Once they start all of them, then we can post this video because it's going to take five minutes for

7
00:00:39.720 --> 00:00:40.610
all of them to come up.

8
00:00:40.620 --> 00:00:43.590
There are quite a few containers that we have.

9
00:00:45.520 --> 00:00:47.020
So let's wait for this now.

10
00:00:47.650 --> 00:00:49.450
We need the load balancers are remaining.

11
00:00:52.600 --> 00:00:52.900
OK.

12
00:00:53.440 --> 00:00:59.080
It's done at least all the containers have been started, so they are booting up right now.

13
00:01:00.010 --> 00:01:02.970
Let's check one of these services so good.

14
00:01:03.070 --> 00:01:04.420
So this is a good check.

15
00:01:04.570 --> 00:01:07.930
We can check logs of gate reserves.

16
00:01:11.240 --> 00:01:13.000
So right now, there is nothing over there.

17
00:01:13.790 --> 00:01:19.280
It has not even started, the container has been created, but service hasn't started yet.

18
00:01:20.090 --> 00:01:22.760
You can check all the containers there.

19
00:01:24.050 --> 00:01:25.640
They look good as of now.

20
00:01:26.330 --> 00:01:29.780
The Prometheus container has been added.

21
00:01:30.380 --> 00:01:38.270
And apart from this, we have Baiji exporter container and we also have ElasticSearch export containers.

22
00:01:38.270 --> 00:01:42.170
So now we have quite a few containers in our application.

23
00:01:42.170 --> 00:01:46.220
So a little later we will have to run this through.

24
00:01:46.220 --> 00:01:48.590
Kubernetes will not be able to run it like this.

25
00:01:49.040 --> 00:01:54.830
There are far too many containers for us to handle, so we will go to when it is in later sections.

26
00:01:54.830 --> 00:01:57.650
But right now we'll have to bear with this.

27
00:01:58.520 --> 00:01:58.850
OK.

28
00:01:59.270 --> 00:02:03.650
So let's check Gateway service log once again.

29
00:02:04.760 --> 00:02:05.660
So nothing is there.

30
00:02:06.500 --> 00:02:08.090
When you get the service, it's OK.

31
00:02:09.280 --> 00:02:11.650
So it just all of a sudden it came.

32
00:02:14.020 --> 00:02:18.200
So it has also started and it hasn't taken much time.

33
00:02:18.220 --> 00:02:19.830
About ninety nine seconds.

34
00:02:20.150 --> 00:02:21.190
That's all it has taken.

35
00:02:21.250 --> 00:02:24.220
You can check some of this service also in that case.

36
00:02:25.330 --> 00:02:29.920
Let's have admin services up if this service is up, OK?

37
00:02:29.950 --> 00:02:30.710
That's also up.

38
00:02:30.760 --> 00:02:32.140
So that looks pretty good.

39
00:02:33.040 --> 00:02:35.830
Then we can connect to our system without waiting at all.

40
00:02:37.300 --> 00:02:41.830
So for that, we need external IP address of this machine.

41
00:02:45.020 --> 00:02:45.800
Let's take that.

42
00:02:45.960 --> 00:02:46.250
OK.

43
00:02:47.890 --> 00:02:50.170
Let's first check if.

44
00:02:52.770 --> 00:02:54.820
Web application, of course, will get a response.

45
00:02:54.840 --> 00:02:56.130
Let's check databases.

46
00:02:57.180 --> 00:03:04.350
So so far, this has been our way of checking if the services are up or not.

47
00:03:05.400 --> 00:03:11.640
Now, from now on, because we have Prometheus, we have a different way of checking if our services

48
00:03:11.690 --> 00:03:16.020
are up or not, and this is actually should be the standard way of checking.

49
00:03:16.410 --> 00:03:17.790
So this is Prometheus, you.

50
00:03:18.570 --> 00:03:22.680
I have launched it by clicking on this monitoring Prometheus.

51
00:03:23.160 --> 00:03:30.120
You can directly go to Prometheus by going to four nine zero nine zero.

52
00:03:30.120 --> 00:03:34.440
But so that we don't have to remember, I put links over here.

53
00:03:35.640 --> 00:03:39.090
OK, so we are into Prometheus right now.

54
00:03:39.480 --> 00:03:45.180
So first thing that we are going to do over here, we will look at the targets that we have.

55
00:03:45.780 --> 00:03:47.190
So let's look at the targets.

56
00:03:48.060 --> 00:03:52.710
If I a collapse all of this information, then it looks like this.

57
00:03:53.160 --> 00:03:55.980
So the first one is a database matrix.

58
00:03:56.580 --> 00:04:01.150
So this is ElasticSearch exporter, NPG, Postgres exporter.

59
00:04:01.170 --> 00:04:03.750
They say that they are up so good.

60
00:04:04.380 --> 00:04:05.940
Then you go to Matrix Matrixed.

61
00:04:05.940 --> 00:04:07.350
They are all saying that they are up.

62
00:04:08.960 --> 00:04:17.090
Then services matrix here we have admin service, product service, Yurek Discovery Service, Inventory

63
00:04:17.090 --> 00:04:18.350
Authorization Gateway.

64
00:04:18.770 --> 00:04:20.060
So all of them are up.

65
00:04:20.630 --> 00:04:23.540
So this is a good way of knowing if our services are up.

66
00:04:24.560 --> 00:04:27.230
And this is the application.

67
00:04:29.060 --> 00:04:34.360
Now we want to see for how long they have been up so that we can go to graph.

68
00:04:35.360 --> 00:04:40.010
And yet this is where we can specify, let's hit this up.

69
00:04:42.380 --> 00:04:45.050
Request all weekend, we can execute it.

70
00:04:45.800 --> 00:04:53.000
So this is that all of them are up, but if we want to see the timeline for how long they have been

71
00:04:53.000 --> 00:04:54.980
up, let's reduce the skin.

72
00:04:56.200 --> 00:04:59.920
So this is 15 minutes, and so they have been up.

73
00:05:01.420 --> 00:05:04.510
This is probably the time it is showing.

74
00:05:04.960 --> 00:05:13.270
So this way we can see when something actually came up and if something goes down, then this graph

75
00:05:13.270 --> 00:05:19.660
will show that if let's say something goes down and comes up through this graph, we can find out if

76
00:05:19.660 --> 00:05:21.370
that kind of event happened.

77
00:05:21.970 --> 00:05:29.860
Another thing that we can do is we can get alerts because we have configured alerts of some instances

78
00:05:29.860 --> 00:05:30.160
don't.

79
00:05:30.790 --> 00:05:37.570
This alert will go up if our system is experiencing higher latency than according to the configuration

80
00:05:37.570 --> 00:05:38.650
that we have put over here.

81
00:05:39.010 --> 00:05:44.620
We will get that alert, too, but this one is very simple to check in stands down so we can quickly

82
00:05:44.620 --> 00:05:45.220
check that.

83
00:05:45.790 --> 00:05:50.170
For that, let's bring down one of the instances.

84
00:05:50.410 --> 00:05:50.880
So let's.

85
00:05:50.980 --> 00:05:52.960
These are the instances we have.

86
00:05:54.880 --> 00:05:58.840
Let's stop, let's say, inventory service.

87
00:06:07.660 --> 00:06:08.770
And let's say.

88
00:06:10.340 --> 00:06:13.010
When is this thing going to pick up, that's a.

89
00:06:14.900 --> 00:06:15.830
Service is down.

90
00:06:19.240 --> 00:06:22.660
OK, so this alert has gone.

91
00:06:23.200 --> 00:06:24.190
And as a.

92
00:06:26.470 --> 00:06:30.070
This alert says that an instance is down and it will.

93
00:06:30.700 --> 00:06:33.220
It is showing which instance is down.

94
00:06:33.250 --> 00:06:35.950
This is inventory service which has gone down.

95
00:06:37.150 --> 00:06:39.430
So that's the web alerts work.

96
00:06:40.030 --> 00:06:42.670
Let's bring up our inventory service.

97
00:06:50.210 --> 00:06:55.160
Then there are quite a few other things that we can check from here, so let's say we want to check

98
00:06:55.190 --> 00:07:06.650
CPU usage, then we can check CPU usage of our services by tracking this particular CPU usage began.

99
00:07:09.560 --> 00:07:09.920
OK.

100
00:07:10.870 --> 00:07:14.840
The Disney CPU usage, and let's say if.

101
00:07:17.600 --> 00:07:19.470
We start bringing our services.

102
00:07:21.080 --> 00:07:23.030
And let me create some data.

103
00:07:23.900 --> 00:07:26.180
Then these CPU usage will change.

104
00:07:29.860 --> 00:07:32.080
So we should be able to monitor that as we get.

105
00:07:33.410 --> 00:07:37.160
I think it takes five seconds for it to reverse, that's the configuration we have.

106
00:07:41.970 --> 00:07:42.700
We're just fine.

107
00:07:42.720 --> 00:07:44.590
In the meantime, let's look at alerts.

108
00:07:44.610 --> 00:07:48.570
Alerts has switched to no instance down, so that's pretty good.

109
00:07:50.910 --> 00:07:52.380
Let's go back to CBO.

110
00:07:56.150 --> 00:08:00.830
So this is something, OK, so this has come up over here.

111
00:08:01.370 --> 00:08:04.970
So this is where our system CPU has gone up.

112
00:08:05.300 --> 00:08:07.070
So this is how we can.

113
00:08:09.660 --> 00:08:15.150
Track system, CPU usage for our services, this is what our services, these are the services listed

114
00:08:15.150 --> 00:08:15.600
over here.

115
00:08:15.960 --> 00:08:24.630
So another question is how is Prometheus able to do this now under Prometheus is able to do this by

116
00:08:25.050 --> 00:08:30.150
virtue of these pages that each component is exposing.

117
00:08:30.150 --> 00:08:31.290
So let's copy this.

118
00:08:33.450 --> 00:08:44.020
IP address, and let's put it over here, so it's zero zero and the four services, the URL was actually

119
00:08:44.310 --> 00:08:45.120
Prometheus.

120
00:08:48.390 --> 00:08:54.090
So if you see this, this is how this page will get refreshed every few seconds.

121
00:08:54.510 --> 00:09:00.870
And this is how our services are exposing the matrix.

122
00:09:01.140 --> 00:09:03.060
So these are various metrics that are available.

123
00:09:03.060 --> 00:09:06.540
So let's say you want to look for garbage collection.

124
00:09:06.550 --> 00:09:14.670
How many bytes have been garbage collected or different garbage collector memory areas, how much memory

125
00:09:14.670 --> 00:09:15.270
they are using?

126
00:09:15.720 --> 00:09:18.900
So we can do all that monitoring over here.

127
00:09:19.560 --> 00:09:21.780
So let's say memory.

128
00:09:22.590 --> 00:09:31.410
So JVM memory committed by its medicine, so we can get that, we can monitor that.

129
00:09:31.950 --> 00:09:40.980
So it will monitor every area of heap, which is separately managed by Jawa Garbage Collector.

130
00:09:40.980 --> 00:09:43.640
So let's say this is CMS old generation.

131
00:09:43.650 --> 00:09:45.480
This has given it in space.

132
00:09:45.930 --> 00:09:51.690
So like that, different memory areas, how much memory is being used by each service, that kind of

133
00:09:51.690 --> 00:09:53.430
information we can get through this.

134
00:09:55.870 --> 00:10:00.160
Now, did he last one that probably we are interested is in request?

135
00:10:00.820 --> 00:10:05.110
So this study, GDP will help facilitate this.

136
00:10:05.890 --> 00:10:10.170
These rules, which are related to Django, so Django is deep.

137
00:10:10.480 --> 00:10:18.850
So let's say request total towns for total, so Django stop requests total by transport brutality.

138
00:10:19.390 --> 00:10:27.430
This will give us an idea of the request flow to web application, so this keeps track of the request.

139
00:10:27.770 --> 00:10:30.730
The total requests that have come to our system.

140
00:10:31.540 --> 00:10:39.370
So there are various such parameters like CPU memory requests count, which is basically to port and

141
00:10:39.370 --> 00:10:44.610
the health of our system, which we can easily monitor from Prometheus.

142
00:10:44.620 --> 00:10:52.300
And it can even give us alerts if some instance goes down or any other configuration shows some alert.

143
00:10:53.170 --> 00:10:57.270
So these are pretty useful things for a large-scale application.

144
00:10:57.280 --> 00:11:02.920
Otherwise, it is very difficult to know when there are loads of components and lots of instances that

145
00:11:02.920 --> 00:11:11.320
which one is up or down and what's the resource utilization and what's the system to put with these

146
00:11:11.320 --> 00:11:13.090
components with these when even put in?

147
00:11:13.090 --> 00:11:15.790
So there's something for which people use Prometheus.

148
00:11:16.240 --> 00:11:19.960
So this is how we can integrate Prometheus into our system.
