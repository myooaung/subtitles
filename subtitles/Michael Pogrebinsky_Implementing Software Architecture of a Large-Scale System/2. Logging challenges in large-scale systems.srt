1
00:00:01,050 --> 00:00:08,610
So let's first start with logging, so logging something that is potentially generated by each company.

2
00:00:09,240 --> 00:00:15,510
So our web application will generate log gateway service will generate log authorizations of all these

3
00:00:15,510 --> 00:00:22,980
services, they will generate log, in fact, database log also we may be interested in at times, but

4
00:00:22,980 --> 00:00:23,760
not generally.

5
00:00:24,660 --> 00:00:27,580
There are other ways of monitoring databases.

6
00:00:27,600 --> 00:00:33,870
Databases will provide their own ways of effectively monitoring databases, but we are generally responsible

7
00:00:33,870 --> 00:00:37,290
for the components that we are building into our system.

8
00:00:37,890 --> 00:00:44,040
We may be interested in logs of load balancers as well just to see where, whether certain requests

9
00:00:44,040 --> 00:00:50,370
are hitting load balances and if they're passing beyond the load balances or they are just getting dropped

10
00:00:50,370 --> 00:00:51,850
from the load balance that itself.

11
00:00:52,830 --> 00:00:59,730
All of our focus will mainly be on the application load that our components are generating, so that's

12
00:00:59,730 --> 00:01:00,750
what we will focus on.

13
00:01:01,260 --> 00:01:08,550
But generally, this applies to every component where there are logs and we are interested in those

14
00:01:08,550 --> 00:01:08,970
logs.

15
00:01:10,150 --> 00:01:17,710
So let's see how we can make sense out of those logs are challenge right now is that these logs are

16
00:01:17,710 --> 00:01:24,310
distributed on multiple machines and for each component which is dead on a different machine, we can

17
00:01:24,310 --> 00:01:31,390
have multiple instances which will add more machines to our system so we can easily have 100 machines

18
00:01:31,390 --> 00:01:32,110
in our system.

19
00:01:32,440 --> 00:01:39,580
So we need some way of centralizing the law because then only if we collect these logs and make a cohesive

20
00:01:39,580 --> 00:01:46,630
log out of it, then only we can draw some meaningful sense out of it because same user request, same

21
00:01:46,630 --> 00:01:53,350
user interaction, which may let it go on for five minutes that may be done on multiple services and

22
00:01:53,350 --> 00:01:57,760
on different instances of each service or each request can go on different instances.

23
00:01:58,150 --> 00:02:05,320
So we may have to collect all that information, which is distributed in our logs, and then only we

24
00:02:05,320 --> 00:02:09,130
will be able to draw some meaningful inferences out of our logs.

25
00:02:09,940 --> 00:02:12,910
So for that, we will need centralized logging.

26
00:02:14,850 --> 00:02:21,240
Now, in order to achieve centralized logging, this strategy that we are going to follow is and this

27
00:02:21,240 --> 00:02:25,260
is generally the strategy that is followed in any distributed system.

28
00:02:25,980 --> 00:02:32,130
And that strategy is that on each machine there are application components are generating logs.

29
00:02:32,610 --> 00:02:38,460
We will have some agent which will read these logs as and when they are generated.

30
00:02:38,460 --> 00:02:46,200
So on a real time basis, it can tell that log or whatever other mechanism it can use, it will transfer

31
00:02:46,350 --> 00:02:49,770
the log contents to a log collector.

32
00:02:49,770 --> 00:02:56,220
So as these logs are being generated by an application, this agent, which will be running on the same

33
00:02:56,220 --> 00:03:03,930
machine, will read these logs as they are being generated, and it will transfer these logs to a log

34
00:03:03,930 --> 00:03:04,590
collector.

35
00:03:05,040 --> 00:03:11,250
So we'll have to port this agent everywhere, wherever we have log files that that are of interest to

36
00:03:11,250 --> 00:03:11,550
us.

37
00:03:12,750 --> 00:03:20,040
So all these log files, we warned these agents to send to a log collector on a real time basis was

38
00:03:20,040 --> 00:03:24,810
the job of this law collector is to get all these logs from different agents.

39
00:03:25,260 --> 00:03:33,120
It should actually send these logs to a storage after processing them and giving them a meaningful structure.

40
00:03:34,080 --> 00:03:40,170
This storage should be such where we should be able to analyze these lots of first requirement is that

41
00:03:40,170 --> 00:03:42,110
it should be a permanent storage.

42
00:03:42,120 --> 00:03:44,910
We should not lose anything in this storage.

43
00:03:44,910 --> 00:03:48,780
Once a log has been sent to the storage and the second requirement is.

44
00:03:49,930 --> 00:03:54,010
We should be able to analyze these logs, we should be able to retrieve them.

45
00:03:54,250 --> 00:03:59,260
We should be able to find the information that we are looking for and we should be able to analyze it.

46
00:04:00,100 --> 00:04:06,820
Then we need analytics component, which can actually read all the log information, and it can present

47
00:04:06,820 --> 00:04:09,160
it in a meaningful way.

48
00:04:09,550 --> 00:04:15,700
One of them would be that we should be able to create reports out of these logs, which are stored in

49
00:04:15,700 --> 00:04:16,540
the logs storage.

50
00:04:17,590 --> 00:04:25,120
So with that as a goal, now what we need to do is we need to decide on what products we need that can

51
00:04:25,120 --> 00:04:27,190
help us achieving this.
