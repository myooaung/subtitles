WEBVTT
1
00:00:01.890 --> 00:00:07.320
Continuing from our previous session where we saw high availability property of our deployment.

2
00:00:07.650 --> 00:00:10.950
Now let's see if we can auto scale our system.

3
00:00:13.320 --> 00:00:15.150
Let's go to our console.

4
00:00:15.150 --> 00:00:22.560
But before we go there, let's check this what's the number of instances for our services?

5
00:00:22.560 --> 00:00:27.780
Let me put a filter on that so that they are clearly visible.

6
00:00:27.810 --> 00:00:28.110
OK.

7
00:00:28.530 --> 00:00:35.100
So all of these services, they have got one one instance, an Excel Gateway service parts which are

8
00:00:35.100 --> 00:00:38.610
called three instances because we scale them up manually.

9
00:00:39.270 --> 00:00:40.170
So let me do this.

10
00:00:40.770 --> 00:00:45.990
Let me scale them back to one instance.

11
00:00:47.100 --> 00:00:51.780
Let's revert back that manual scaling that we did early on.

12
00:00:53.860 --> 00:00:55.390
OK, so this should.

13
00:00:56.750 --> 00:01:02.720
Actually, terminate some of the notes from Google service now the target is one or two of them will

14
00:01:02.720 --> 00:01:03.200
come down.

15
00:01:03.620 --> 00:01:04.820
So while this is happening?

16
00:01:05.990 --> 00:01:10.760
Let's apply horizontal port or scalar configuration to our cluster.

17
00:01:10.790 --> 00:01:15.860
So for that, we will use this cube deployed script.

18
00:01:16.340 --> 00:01:18.080
And over here we will use

19
00:01:21.350 --> 00:01:22.850
scripts in autos.

20
00:01:22.850 --> 00:01:28.310
Those five scalar directly an auto scan that is the only YAML file which is there right now.

21
00:01:28.310 --> 00:01:34.490
And we have seen that during configuration that we have provided horizontal powered auto scaling definition

22
00:01:34.490 --> 00:01:34.970
over there.

23
00:01:35.300 --> 00:01:39.500
So let's use this to configure auto scalar for our services.

24
00:01:39.500 --> 00:01:40.880
So let me use this.

25
00:01:43.230 --> 00:01:49.940
OK, so these horizontal part order scales have been attached to our workloads.

26
00:01:49.980 --> 00:01:50.970
Let's verify that.

27
00:01:51.690 --> 00:01:59.670
And we have attached them to services and also to a web application and service page application.

28
00:01:59.670 --> 00:02:06.720
So they should all get auto scan, but they will scale up only if there is some law.

29
00:02:07.200 --> 00:02:13.620
So what we are going to do over here is we are going to simulate some law so that they can be scaled

30
00:02:13.620 --> 00:02:13.830
up.

31
00:02:15.520 --> 00:02:20.860
So let me remove this filter now, all the services, they have got one one instances.

32
00:02:22.260 --> 00:02:29.730
And that single page application and BP WebApp also has got one one instances, but let's verify if

33
00:02:29.730 --> 00:02:39.900
the scalar horizontal part or the skin that has been added so that is min max replica is three and mini

34
00:02:39.900 --> 00:02:40.860
replica is one.

35
00:02:41.220 --> 00:02:48.000
I think it is unable to read all matrix, probably because I have not set up the resources for web application.

36
00:02:49.430 --> 00:02:50.720
That should be OK.

37
00:02:51.170 --> 00:02:58.880
But we will get that warning, but I think we have all of these identity sources for all our other services,

38
00:02:58.880 --> 00:03:01.490
so this model killer looks absolutely fine.

39
00:03:02.120 --> 00:03:05.500
So he had a minimum number of replicas is one maximum is three.

40
00:03:06.350 --> 00:03:08.720
We can similarly verify this for.

41
00:03:10.120 --> 00:03:11.890
Other services, so let's say.

42
00:03:12.950 --> 00:03:13.810
Or this.

43
00:03:17.670 --> 00:03:24.600
So for order service also or to scale, it is attached and mini replica is one and makes replica is

44
00:03:24.600 --> 00:03:24.930
three.

45
00:03:25.350 --> 00:03:28.030
And we have configured CPU metric.

46
00:03:28.050 --> 00:03:34.590
If it goes CPU utilization goes above 50 percent, then this shirt scale up our system.

47
00:03:35.130 --> 00:03:41.730
Now, in order to scale up our system, we have got some geometry tests already configured and in order

48
00:03:41.730 --> 00:03:43.210
to run those geometry tests.

49
00:03:43.230 --> 00:03:50.070
What we'll have to do is we'll have to go to Kubernetes directly and there is a script called Cube Test.

50
00:03:50.070 --> 00:03:52.020
So there's a script that we're going to use.

51
00:03:52.020 --> 00:03:57.540
This script basically uses a configuration file, which is that in agree, let's open that.

52
00:03:59.910 --> 00:04:06.720
So this geometry, or Yemen file has got a definition of Jamie to run.

53
00:04:07.080 --> 00:04:13.080
So this will run a load test and these are the parameters we have asked this to connect to gateway service

54
00:04:14.700 --> 00:04:19.560
provider, deport the number of users and create a number of times.

55
00:04:19.560 --> 00:04:24.090
If you'd run the test and the lower level sort of sending adjustment of parameters is needed.

56
00:04:24.510 --> 00:04:25.660
It can be done over here.

57
00:04:26.100 --> 00:04:27.510
Right now, we're good to go.

58
00:04:27.990 --> 00:04:31.740
So we will just run this test for that.

59
00:04:31.890 --> 00:04:38.610
We will use this to test and this queue just creates this.

60
00:04:38.760 --> 00:04:45.960
Let me I forgot to tell you one thing that this has a Kubernetes object of type, job or run.

61
00:04:46.290 --> 00:04:49.520
It runs once and its job is finished.

62
00:04:49.830 --> 00:04:54.900
So this is if we're going to this object of type job, which will run once.

63
00:04:55.320 --> 00:04:59.430
So we are going to run that using the script cube test.

64
00:05:02.460 --> 00:05:05.370
And again, this script takes care of the parameters.

65
00:05:05.370 --> 00:05:12.540
And in case we are applying this script, what it does is it substitutes those environment variables

66
00:05:12.540 --> 00:05:14.460
inside that script and it runs.

67
00:05:14.460 --> 00:05:19.470
This command cube will create minus have over the modified finds.

68
00:05:19.470 --> 00:05:25.890
It is similar to how we have been doing this with the help of other scripts like you volumes or cube.

69
00:05:25.890 --> 00:05:28.720
The same thing is being done in cube desktops.

70
00:05:28.730 --> 00:05:30.540
Also nothing different.

71
00:05:30.990 --> 00:05:36.150
Now it's time to run this so cube test that's run this.

72
00:05:39.990 --> 00:05:45.580
So this is going to create a new workload off type job.

73
00:05:45.600 --> 00:05:46.350
Let's see that.

74
00:05:55.870 --> 00:05:58.270
OK, so it should be OK, it's over here.

75
00:05:58.990 --> 00:06:05.440
Jamie Burnett So this is a job that got created, so this is running right now.

76
00:06:05.470 --> 00:06:09.880
Let's see the logs for this.

77
00:06:10.270 --> 00:06:12.400
The logs for this is we can.

78
00:06:13.470 --> 00:06:15.390
I painted the command.

79
00:06:16.510 --> 00:06:21.760
So that it is easy to view the locks of evil, and this commander is nothing special.

80
00:06:21.790 --> 00:06:27.130
We just need to look at the logs and we have seen that how to check logs, the same similar command

81
00:06:27.130 --> 00:06:28.090
we are used to here it.

82
00:06:28.540 --> 00:06:33.070
And they're saying that the load test is in progress right now.

83
00:06:33.880 --> 00:06:41.590
So we have some 18 requests or 15 requests per second are being processed right now on an average.

84
00:06:42.310 --> 00:06:48.190
Let's see if this is resulting in two scaling of our services because this Nord is going to services.

85
00:06:49.000 --> 00:06:52.840
So if you see this authorization service part, there is a disturbance over there.

86
00:06:53.410 --> 00:06:59.140
It needs to scale up to three boards, get the service boards, need to scale up to two parts.

87
00:06:59.680 --> 00:07:00.730
We have set it to one.

88
00:07:00.730 --> 00:07:04.270
So now this also needs to be scaled up and order.

89
00:07:04.270 --> 00:07:07.450
So it seems like that needs to be scaled up to three boards.

90
00:07:07.960 --> 00:07:15.520
And they are being scaled up right now, so that's auto scaling, which is working for us right now.

91
00:07:18.190 --> 00:07:23.020
And we will let this auto scaling to complete.

92
00:07:24.270 --> 00:07:31.500
And then we will run another, Jamie, this looks like this is complete and Jamie protesters running

93
00:07:31.500 --> 00:07:31.980
right now.

94
00:07:32.820 --> 00:07:41.160
It has a throughput of about 18 to 15 requests per second, so we will wait for this to finish and then

95
00:07:41.160 --> 00:07:45.570
we will run this again and see if we can increase this throughput.

96
00:07:47.180 --> 00:07:48.660
There are some errors over here.

97
00:07:49.130 --> 00:07:55.850
There are very low in number, they are probably related to concurrency because there are some users

98
00:07:55.850 --> 00:08:01.760
that may be clashing for the same card item so that this percentage is really somewhat less than one

99
00:08:01.760 --> 00:08:02.210
percent.

100
00:08:02.480 --> 00:08:06.530
I need to fix this, but this is not going to stop us from running.

101
00:08:06.530 --> 00:08:10.970
This load test with this load test right now is it's complete.

102
00:08:11.570 --> 00:08:20.270
And its throughput initially started with 15, 18 14 and finished somewhere in the region of 21 request

103
00:08:20.270 --> 00:08:20.900
per second.

104
00:08:21.440 --> 00:08:23.330
Let's run this once again.

105
00:08:23.360 --> 00:08:26.270
Let's run this queue test once again.

106
00:08:27.680 --> 00:08:35.540
This will create delete the previous run and will create a new job and the new job you can monitor from

107
00:08:35.540 --> 00:08:35.810
here.

108
00:08:38.270 --> 00:08:39.710
By executing this command.

109
00:08:42.110 --> 00:08:44.300
So this has created a new front.

110
00:08:45.770 --> 00:08:49.970
And that has started with a total of 21 requests for a second.

111
00:08:51.260 --> 00:08:53.810
Let's see if that improves.

112
00:08:55.770 --> 00:08:57.000
So we'll have to look at one.

113
00:09:01.290 --> 00:09:10.290
So now, if you see this, the throughput has gone up by almost two and a half times the kind of hard

114
00:09:10.470 --> 00:09:17.070
because we have now augmented the number of boards which actually got augmented automatically, so are

115
00:09:17.070 --> 00:09:18.750
to port has also increased.

116
00:09:19.410 --> 00:09:20.430
So we can see this.

117
00:09:20.430 --> 00:09:27.840
This is a good demonstration of our system scaling up because of the law and not only it has scaled

118
00:09:27.840 --> 00:09:32.640
up in terms of the number of boards that are there right now.

119
00:09:35.470 --> 00:09:43.660
But it that scaling up has also resulted in the increase of throughput, so we are now able to process

120
00:09:44.170 --> 00:09:46.090
more number of requests in lesser time.

121
00:09:46.090 --> 00:09:49.980
So this run finished in much less time than the previous.

122
00:09:50.010 --> 00:09:52.540
The previous round took much longer.

123
00:09:52.900 --> 00:09:55.330
And this one finished in much less time.

124
00:09:55.780 --> 00:10:01.120
And that happened because we were able to auto scale our system.

125
00:10:01.870 --> 00:10:05.580
So with this, we are done with auto scaling.

126
00:10:05.590 --> 00:10:08.890
Our system has the property of model getting.

127
00:10:08.890 --> 00:10:12.250
Whenever we are subjecting to load, it is scaling up.
