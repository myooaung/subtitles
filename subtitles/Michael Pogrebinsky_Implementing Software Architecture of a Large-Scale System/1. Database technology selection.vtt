WEBVTT
1
00:00:00.960 --> 00:00:07.350
Now we will talk about database, so we will try to look for which database is suitable for us, and

2
00:00:07.350 --> 00:00:11.100
in particular, we are looking for open source databases.

3
00:00:12.260 --> 00:00:15.920
Now, there are two kinds of databases of broadly that are available to us.

4
00:00:15.950 --> 00:00:19.870
One is IBM's and another one is no sequel.

5
00:00:20.730 --> 00:00:26.360
So they have to work on a limited skill and no sequel databases.

6
00:00:26.360 --> 00:00:28.460
They offer unlimited scalability.

7
00:00:29.330 --> 00:00:35.810
Now let's look at their differences to understand which database is really suitable for our purpose.

8
00:00:36.710 --> 00:00:44.900
Now we are looking for a database which our services can use for doing transactions, so our database

9
00:00:44.930 --> 00:00:50.360
is going to experience both read Lord and Right Lord, so we will read data from our database.

10
00:00:50.840 --> 00:00:57.320
And then we will process that data, and then we will write back that data to our database.

11
00:00:57.320 --> 00:00:59.000
So that's that's called transactions.

12
00:00:59.000 --> 00:01:03.870
So that that's the kind of load our database is going to take for that purpose.

13
00:01:03.920 --> 00:01:09.320
R.D. BBMAs is a perfectly suitable choice because they provide full fledged asset transactions.

14
00:01:10.280 --> 00:01:16.760
We can also do asset transactions in no secret databases, depending upon which database we are talking

15
00:01:16.760 --> 00:01:17.130
about.

16
00:01:17.630 --> 00:01:23.750
But generally, they allow asset transaction, the only problem being that they allow asset transaction

17
00:01:23.750 --> 00:01:26.120
only on the aggregate data.

18
00:01:26.750 --> 00:01:32.800
So if you have a record, a single record, you can actually do an asset transaction on that.

19
00:01:32.810 --> 00:01:40.280
So a general strategy is to normalize data and put as much data as possible in a single record.

20
00:01:40.280 --> 00:01:48.960
So whatever data that needs to go transaction at a time that needs to be available in a single record.

21
00:01:49.040 --> 00:01:50.540
We often call it as aggregate.

22
00:01:50.990 --> 00:01:55.010
So that's the notion of transactions in no secret databases.

23
00:01:55.040 --> 00:01:56.060
It works pretty well.

24
00:01:56.570 --> 00:02:06.080
But then your schema will be designed as per your transactions, so you lose some flexibility over there

25
00:02:06.080 --> 00:02:08.560
in terms of how you want to design your ischemia.

26
00:02:09.200 --> 00:02:11.510
Schema should be strictly in that area.

27
00:02:11.510 --> 00:02:13.340
Insert your transaction requirements.

28
00:02:14.030 --> 00:02:20.510
Now there are a couple of other things that we lose when we work with no security databases and those

29
00:02:20.510 --> 00:02:22.930
are things like we can all do joints.

30
00:02:22.940 --> 00:02:29.510
There are no foreign keys, so data integrity, we have to take care of data integrity from application

31
00:02:29.510 --> 00:02:29.870
site.

32
00:02:30.200 --> 00:02:33.800
And then there is limited indexing that we can do.

33
00:02:33.800 --> 00:02:39.290
And that's because our data is distributed across multiple instances.

34
00:02:39.650 --> 00:02:48.740
And the same reason also doesn't allow us to do general transactions on multiple records, which are

35
00:02:48.980 --> 00:02:51.320
distributed over multiple instances.

36
00:02:51.710 --> 00:02:59.030
So for these reasons, no sequel database, they are seen as databases with limitations.

37
00:03:00.020 --> 00:03:08.570
Their strength lies in extreme scalability that they provide and the flexible schema so we can add as

38
00:03:08.570 --> 00:03:15.770
many columns on the fly as we want and return depends on the kind of new security database we are talking

39
00:03:15.770 --> 00:03:16.010
about.

40
00:03:16.010 --> 00:03:21.470
But generally we can take that that this schema is flexible and on demand we can extend it.

41
00:03:21.710 --> 00:03:29.240
So these are the two things that are a unique selling point for nowsecure databases and the one that

42
00:03:29.240 --> 00:03:37.400
we are most concerned about is extreme scale, because what we want is that we should be able to go

43
00:03:37.400 --> 00:03:41.020
to the ultimate scale that is required for our solution.

44
00:03:41.060 --> 00:03:50.150
Remember, we are doing this for large scale systems, so rich database we shall choose now if we choose,

45
00:03:50.150 --> 00:03:53.090
our teams are and gets limited.

46
00:03:53.840 --> 00:04:01.940
What that means is that if our data goes beyond one terabytes of data, then generally it seems in need

47
00:04:01.940 --> 00:04:02.960
about in that range.

48
00:04:03.960 --> 00:04:13.050
The RBM says they are not able to give good performance on those databases where we are trying to do

49
00:04:13.050 --> 00:04:16.890
transactions and the database has gone beyond one terabyte.

50
00:04:18.300 --> 00:04:22.620
That's a limitation on scale, but now we have to be realistic over here.

51
00:04:23.750 --> 00:04:30.440
Well, our data ever cross one terabyte limit on our transaction database, the database that we are

52
00:04:30.440 --> 00:04:31.460
doing transactions.

53
00:04:32.360 --> 00:04:36.530
It generally doesn't go beyond that size.

54
00:04:36.530 --> 00:04:43.160
And the reason is that when we do transactions on our database as the data become old and all.

55
00:04:43.670 --> 00:04:49.700
So let's say we have bought some ADRs and after sometimes these orders will be delivered to the customer

56
00:04:49.700 --> 00:04:51.170
and they really become history.

57
00:04:51.740 --> 00:04:59.210
So also, it's data we do not need to keep it in our IBM's instance where we are doing transactions

58
00:04:59.210 --> 00:05:05.780
and that kind of IBM's instances callers or LTV database because there we are doing online transaction

59
00:05:05.780 --> 00:05:06.380
processing.

60
00:05:06.980 --> 00:05:14.780
So from this kind of database, we extract the data to ideally process, and we put that data into another

61
00:05:14.780 --> 00:05:17.240
IBM's, which is called data warehouse.

62
00:05:17.930 --> 00:05:23.060
Now in this data warehouse, we can get data from other databases also.

63
00:05:23.070 --> 00:05:29.480
So it becomes one point where we can integrate our data and generally we keep historical data over.

64
00:05:29.480 --> 00:05:34.130
We're here for generating reports, and that process is called as an entity.

65
00:05:34.940 --> 00:05:36.250
So we do it in.

66
00:05:36.350 --> 00:05:42.020
We get data from multiple Worldpay databases, whatever databases they are there in our system.

67
00:05:42.470 --> 00:05:46.700
We integrate that data and we put that into data arrows for reports.

68
00:05:47.090 --> 00:05:48.110
And that is the reason.

69
00:05:48.110 --> 00:05:55.910
Generally, we see that the data size really goes very high in all TB databases.

70
00:05:57.470 --> 00:06:05.030
The reason we do not do analytics on these same or LTV database, the reason is that the workload,

71
00:06:05.030 --> 00:06:09.590
the right load is pretty high on all TB databases.

72
00:06:09.980 --> 00:06:17.330
And if we have heavy query load, the kind of load that reports report generation will create, then

73
00:06:17.330 --> 00:06:19.810
we need to set up our database differently.

74
00:06:19.820 --> 00:06:27.140
We need to do unit differently and that's why we work on data warehouses, which are like which are

75
00:06:27.140 --> 00:06:31.340
already obsolete, but they are configured and tuned differently.

76
00:06:31.820 --> 00:06:39.980
So this is how we solve the large data size problem that if data size becomes very large, then how

77
00:06:39.980 --> 00:06:40.880
do we handle that?

78
00:06:40.890 --> 00:06:42.140
So this is the way it is handled.

79
00:06:42.560 --> 00:06:46.160
So rarely we will cross one terabytes of data.

80
00:06:46.610 --> 00:06:50.120
So IBM's becomes a perfect choice for us.

81
00:06:50.930 --> 00:06:51.260
Right?

82
00:06:51.470 --> 00:07:00.140
And when we are talking about IBM's minus and PostgreSQL are two very good choice in terms of open source

83
00:07:00.410 --> 00:07:01.310
IBM's.

84
00:07:01.670 --> 00:07:04.040
So here we will choose any one of them.

85
00:07:04.040 --> 00:07:07.010
So I'm going to choose PostgreSQL over here.

86
00:07:07.610 --> 00:07:10.730
So there's the database that we are going to go ahead with.

87
00:07:11.930 --> 00:07:20.690
Now, a side note over here is that wherever we are not doing convictions in our skin is becoming bigger

88
00:07:20.690 --> 00:07:21.470
and bigger.

89
00:07:22.130 --> 00:07:29.930
Those are the places where we can actually start removing our BMWs and we can actually replace it with

90
00:07:29.930 --> 00:07:31.610
some new sequel database.

91
00:07:31.730 --> 00:07:35.030
So that is how generally we progress with databases.

92
00:07:35.450 --> 00:07:42.740
We can start with our devices, and if our skin grows, then we can embrace for eventual consistency.

93
00:07:43.310 --> 00:07:46.470
We can do it with IBM's multiple instances.

94
00:07:46.490 --> 00:07:50.090
And then finally, we can go for no secret databases.

95
00:07:50.690 --> 00:07:58.130
So we are going to use PostgreSQL security database, but because this is mainly for learning, so we

96
00:07:58.130 --> 00:08:04.190
will also involve Cassandra at some stage where we are microservices, we have multiple services at

97
00:08:04.190 --> 00:08:05.150
that point in time.

98
00:08:05.570 --> 00:08:07.580
We will also bring in Cassandra.

99
00:08:09.610 --> 00:08:12.550
So with that, we know what database we are going to use.

100
00:08:12.880 --> 00:08:17.320
Now let's take a quick look at the schema that we have.
