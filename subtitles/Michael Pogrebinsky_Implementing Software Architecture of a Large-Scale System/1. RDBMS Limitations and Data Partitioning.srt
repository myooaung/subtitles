1
00:00:02,150 --> 00:00:10,040
So far, we have discussed how we can limit the load on our BBMAs by taking care of some of the load

2
00:00:10,220 --> 00:00:18,020
through caching and by distributing the load over a period of time, by using message queue, by using

3
00:00:18,020 --> 00:00:18,420
message.

4
00:00:18,440 --> 00:00:25,250
You will not reduce the overall load on our database, but we will be able to distribute it in time.

5
00:00:26,350 --> 00:00:34,270
So in spite of all these things, once our scale goes up, our teams will not be able to take up the

6
00:00:34,270 --> 00:00:41,770
look, especially because the way our teams are designed, it is extremely hard for them to support

7
00:00:42,220 --> 00:00:47,230
more than one terabytes of data, and in some cases, it can go up to five dynamite.

8
00:00:47,800 --> 00:00:56,500
But generally, if that's the kind of data you are playing with, then IBM's will not be a good choice

9
00:00:56,500 --> 00:00:58,690
and it will have limitations with scalability.

10
00:00:58,690 --> 00:01:05,590
And the primary reason is that in Idlib BMWs, we are generally dealing with single instance.

11
00:01:06,080 --> 00:01:15,730
Now we can improve the situation with IBM's a bit by adding read replicas to the single instance, which

12
00:01:15,730 --> 00:01:17,020
we can call as master.

13
00:01:17,380 --> 00:01:24,190
Because this master will be responsible for all the right load and whatever transactions are being done

14
00:01:24,190 --> 00:01:30,460
on master, they'll be propagated to replica, and these replicas can be used for a read only queries.

15
00:01:31,030 --> 00:01:37,510
So it will bring in some eventual consistency, as in the replicas, may not be completely up to date

16
00:01:37,510 --> 00:01:38,140
with master.

17
00:01:38,150 --> 00:01:43,120
So if you query the master in replica at the same time, you may get different results.

18
00:01:43,960 --> 00:01:49,120
But with that tradeoff, at least one thing we will be able to achieve that.

19
00:01:50,080 --> 00:01:51,610
And that's a problem with cash also.

20
00:01:51,670 --> 00:01:55,090
It's a similar problem, so the way you deal with it is no different.

21
00:01:55,450 --> 00:01:59,250
Or at least it makes the situation a bit better.

22
00:01:59,270 --> 00:02:04,990
At least we are able to scale up the load, which is coming from read queries.

23
00:02:05,500 --> 00:02:07,460
So that's what we get.

24
00:02:07,480 --> 00:02:15,010
And the other thing that we get is higher variability in case if one of the replica or the master crashes,

25
00:02:15,010 --> 00:02:21,160
so it gives most decorations, we can promote a replica as a monster and we can be back in operations

26
00:02:21,160 --> 00:02:27,570
in some time after a delay because there will be some time that will be consumed in promoting a replica

27
00:02:27,610 --> 00:02:28,300
to a master.

28
00:02:28,900 --> 00:02:33,790
So there will be some delay, but it will not be that we are completely out.

29
00:02:34,240 --> 00:02:39,460
So there will be a higher amount of availability as compared to a single instance.

30
00:02:40,420 --> 00:02:46,870
But if we see this are right, Lord is still a problem because it is going to a single instance to this

31
00:02:46,870 --> 00:02:47,740
single instance.

32
00:02:47,740 --> 00:02:54,010
Master instance is responsible for the entire load that is coming from all our services.

33
00:02:54,670 --> 00:02:57,520
So this will still be highly loaded.

34
00:02:57,530 --> 00:03:03,640
And if the Lord of Great Lord, then this is still not a solution for us.

35
00:03:05,660 --> 00:03:11,780
In that case, what we can do is we can vertically partition our system so we can partition them into

36
00:03:13,040 --> 00:03:14,780
microservices architecture.

37
00:03:14,780 --> 00:03:19,400
So this is true microservices architecture where we have done vertical partitioning.

38
00:03:19,880 --> 00:03:27,680
Each service has got its own database and with this architecture, we can independently deploy and develop

39
00:03:27,680 --> 00:03:34,010
our services apart from taking care of the higher amount of node, because now we have got multiple

40
00:03:34,010 --> 00:03:36,890
instances that can take care of the load.

41
00:03:37,280 --> 00:03:43,910
The two different colors over here represent one, let's say master, and the other one is a lead replica.

42
00:03:44,540 --> 00:03:52,010
But from a right load perspective, we still have one instance serving a particular service.

43
00:03:52,010 --> 00:03:53,560
Well, it's in this inventory.

44
00:03:53,630 --> 00:03:57,290
So this deals with only one master instance.

45
00:03:57,830 --> 00:04:01,790
But what we have achieved over here, at least we have four different masters.

46
00:04:02,150 --> 00:04:07,070
In our previous situation, we only had one master for all our four services.

47
00:04:07,460 --> 00:04:09,920
Here we have four different masters.

48
00:04:10,610 --> 00:04:11,840
So this is a good approach.

49
00:04:12,050 --> 00:04:20,660
The issue with this approach is that across the database now, we cannot have integrity constraints

50
00:04:20,660 --> 00:04:25,310
because the data is being managed by different database machines.

51
00:04:25,940 --> 00:04:30,680
There can't be any great egoyan's we cannot get as a transaction.

52
00:04:30,680 --> 00:04:39,020
So if order service, let's say, needs to modify data, but in order IBM's and inventory IBM's, then

53
00:04:39,020 --> 00:04:40,310
that's not possible.

54
00:04:40,850 --> 00:04:46,910
Although we never started with that kind of design, but just letting you know that because now they

55
00:04:46,910 --> 00:04:49,100
are in two separate machines.

56
00:04:49,100 --> 00:04:53,690
So there is a hard constraint that we cannot do as transactions.

57
00:04:53,690 --> 00:04:58,760
If this was just one single machine and we could have clubbed them into a single database, we could

58
00:04:58,760 --> 00:05:00,500
have still achieved that.

59
00:05:00,500 --> 00:05:02,450
But now here, it's not possible.

60
00:05:02,960 --> 00:05:04,960
So we have lost integrity constraint.

61
00:05:04,970 --> 00:05:07,880
We can't do 80 joints and we can't do asset transactions.

62
00:05:08,930 --> 00:05:14,990
But what we can still get is within a particular database, let's say, if we are just creating the

63
00:05:14,990 --> 00:05:22,030
tables which are there in order it BMW's schema, then we can do as a transaction joins and constraints.

64
00:05:22,030 --> 00:05:28,280
So within a database, it's still not an issue, but across the database, we will have to embrace the

65
00:05:28,310 --> 00:05:29,750
eventual consistency.

66
00:05:30,260 --> 00:05:36,020
The other thing that we have over here is now that we have the complexity of maintaining multiple databases.

67
00:05:36,020 --> 00:05:42,380
But I guess that's not a big problem because when you go to scale, you do not have many options.

68
00:05:44,300 --> 00:05:51,910
Now that we have to deal with eventual consistency when we are dealing with multiple services, let's

69
00:05:51,920 --> 00:05:54,920
report a transaction that's if would create order transaction.

70
00:05:54,920 --> 00:05:58,340
We have to deal with inventory service and order service.

71
00:05:58,970 --> 00:06:03,110
So here we have to deal with different databases.

72
00:06:03,110 --> 00:06:09,680
Then in those situations, if we have to do those kind of transaction, the other kind of option becomes

73
00:06:09,680 --> 00:06:10,730
very appealing is.

74
00:06:11,880 --> 00:06:19,770
Distributed database, no distributor database here we can do partitioning at record level, so in our

75
00:06:19,770 --> 00:06:26,650
previous case, we did partitioning vertical partitioning at table levels or some tables got into inventory

76
00:06:26,650 --> 00:06:31,770
tables which were related to inventory and order related service order, serviceability tables.

77
00:06:32,370 --> 00:06:37,200
They were located in order already beams and likewise for product.

78
00:06:37,200 --> 00:06:39,640
And also there was separation of tables.

79
00:06:39,750 --> 00:06:45,960
Now, in this particular case, what we can do is, let's say our order table itself is very large and

80
00:06:45,960 --> 00:06:47,850
it cannot fit into one database.

81
00:06:48,270 --> 00:06:52,170
So here we can partition the table itself.

82
00:06:52,170 --> 00:06:59,550
So the partitioning is at a record level and this limit level that we can go for scalability.

83
00:07:00,850 --> 00:07:09,800
Some of the benefits of this approach is that now we also get data replication as part of new sequel

84
00:07:09,880 --> 00:07:11,150
distributed databases.

85
00:07:11,170 --> 00:07:18,010
They manage it pretty well and there is very little setup that we need to do from our site.

86
00:07:18,580 --> 00:07:23,500
If you want to bring up a cluster over a distributed database with replication, it's pretty easy and

87
00:07:23,500 --> 00:07:25,960
we will see that how easy it is.

88
00:07:26,290 --> 00:07:32,560
And if you have to do the same thing for another BMS, we would be reading manual for it.

89
00:07:32,700 --> 00:07:39,400
If that is not something we usually do, then that's the kind of effort that is required for setting

90
00:07:39,400 --> 00:07:44,260
up replication or even partitioning those databases these days.

91
00:07:44,620 --> 00:07:47,590
They do claim our means that they can do some kind of partitioning.

92
00:07:47,590 --> 00:07:53,650
So not a popular option, because why would you use to be a mess if you have to do partitioning in that?

93
00:07:54,220 --> 00:08:00,580
So nevertheless, the point is that so some of the features like partitioning and replication, they

94
00:08:00,580 --> 00:08:05,980
come out of the box in these databases and they are pretty easy to implement.

95
00:08:07,130 --> 00:08:13,970
If we have to send a request to this kind of distributor database, we can send it to any instance or

96
00:08:14,030 --> 00:08:19,820
outer over here managed by the database, and that will take care of the routing of the request in a

97
00:08:19,820 --> 00:08:20,600
previous case.

98
00:08:21,050 --> 00:08:27,170
Each service has to know which database is this or look, in this case, it looks pretty easy because

99
00:08:27,170 --> 00:08:31,940
we have good distinction of responsibility across its services.

100
00:08:32,330 --> 00:08:38,970
So the point is that here, let's say, if if there are multiple instances, well, as a product service

101
00:08:38,970 --> 00:08:45,020
is dealing with multiple instances of product database, then product service will have to take care

102
00:08:45,020 --> 00:08:46,640
of routing it.

103
00:08:46,640 --> 00:08:51,620
To one of those instances, it will have to figure out whether it's the right requested to read request

104
00:08:52,010 --> 00:08:55,630
and accordingly, it will have to choose an instance over here.

105
00:08:55,640 --> 00:08:58,460
This is something that is seamlessly managed by the database.

106
00:08:58,940 --> 00:09:02,810
Yeah, the benefit is that here we can create schema or demand.

107
00:09:02,810 --> 00:09:06,290
If you want to add some columns, we can do that any time.

108
00:09:06,920 --> 00:09:11,930
In fact, some of them will allow to create the tables also on the fly.

109
00:09:12,830 --> 00:09:19,280
So those are some of deep disadvantage advantages of using distributed database.

110
00:09:19,730 --> 00:09:27,110
The primary advantage is that we are getting data partitioning and there are a lot of other advantages

111
00:09:27,110 --> 00:09:27,560
as well.

112
00:09:28,010 --> 00:09:38,090
The the issue here is that we get limited indexing over here because mostly every record has to be addressed

113
00:09:38,090 --> 00:09:40,270
as if we are facing a primary key.

114
00:09:40,280 --> 00:09:46,220
We can have secondary indexes also, but they do not provide the kind of performance that we are looking

115
00:09:46,220 --> 00:09:46,460
for.

116
00:09:46,460 --> 00:09:53,720
So we have two designer schema sets that most of the stuff we can query using a key.

117
00:09:54,250 --> 00:10:00,020
If you can think of that as a primary keys of that, that's the kind of limitation we put on schema

118
00:10:00,770 --> 00:10:03,390
when we use a distributed database.

119
00:10:03,390 --> 00:10:06,170
So that puts a limitation on query burden.

120
00:10:06,170 --> 00:10:12,950
So the way you design schema over here is you first look at how you are going to query the database

121
00:10:13,130 --> 00:10:17,330
and then you come up with this schema as opposed to in the BBMAs.

122
00:10:17,330 --> 00:10:21,740
You can create this schema and then you can come up with any query burden.

123
00:10:22,100 --> 00:10:23,330
So those are the tradeoffs.

124
00:10:23,840 --> 00:10:28,460
So if you do not want to trade off everything, the other option that we have is that we can follow

125
00:10:28,460 --> 00:10:33,290
an idea of a hybrid approach wherein we can use our PBMs.

126
00:10:33,290 --> 00:10:39,230
For those services which need strong consistency, so does the burden that we can follow.

127
00:10:39,620 --> 00:10:48,590
And for the services where eventual consistency is just fine, we do not need complex query joins and

128
00:10:48,590 --> 00:10:52,880
all that stuff that we can use a distributed database.

129
00:10:52,880 --> 00:10:55,700
So those kind of approaches are pretty popular.

130
00:10:56,450 --> 00:11:04,370
In our case, what we are going to do is we are going to replace the only databases, Postgres databases.

131
00:11:05,890 --> 00:11:11,980
With Cassandra database, so that that's what we are going to do in this demonstration, but the court

132
00:11:12,010 --> 00:11:16,180
actually allows you to actually do something of this kind.

133
00:11:16,600 --> 00:11:20,500
The debt is something that the court that we have can do.

134
00:11:20,770 --> 00:11:26,470
But for this demonstration, we will just add Cassandra database and we will just focus on that.
