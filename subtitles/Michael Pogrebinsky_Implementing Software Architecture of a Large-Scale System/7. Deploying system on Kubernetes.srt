1
00:00:01,600 --> 00:00:05,560
In this session, we are going to deploy our system on Kubernetes.

2
00:00:06,430 --> 00:00:10,540
What we have done so far is that we have set up container registry.

3
00:00:10,540 --> 00:00:12,320
We have uploaded all the images over there.

4
00:00:12,880 --> 00:00:18,280
We have created a when it is clustered and we have seen the configuration files.

5
00:00:18,880 --> 00:00:24,670
So now we will use these configuration files for uploading or deploying our system on.

6
00:00:25,730 --> 00:00:26,780
Using it is.

7
00:00:27,290 --> 00:00:31,160
So let's go ahead and do that, so this is the cluster that we have.

8
00:00:31,940 --> 00:00:37,970
And now I'm connected to our workstation machine on Google, so.

9
00:00:38,210 --> 00:00:41,930
So let's go to our project directory and then Kubernetes.

10
00:00:42,620 --> 00:00:46,790
So here we have some scripts that will help us in deploying our system.

11
00:00:47,600 --> 00:00:53,330
The first step for the deployment is that we will need to create volumes for our databases.

12
00:00:53,330 --> 00:00:59,360
So we said that our databases need to store their data in a volume so that they remain.

13
00:00:59,360 --> 00:01:00,590
The data remain permanent.

14
00:01:00,980 --> 00:01:03,200
So first, we are going to make arrangement for that.

15
00:01:04,070 --> 00:01:08,510
For that, we have the configuration files and volume directory.

16
00:01:08,600 --> 00:01:15,380
So let's just look at my local directly to get a better view of this plans.

17
00:01:15,380 --> 00:01:17,120
So let's go to volume.

18
00:01:17,480 --> 00:01:21,950
It has got this persistent volume stored yammer directly.

19
00:01:22,370 --> 00:01:29,000
Here we are asking Kubernetes to create an object of kind, persistent volume.

20
00:01:30,050 --> 00:01:39,020
This persistent volume should have capacity of five GB, and we want this to be located on the local

21
00:01:39,020 --> 00:01:39,260
level.

22
00:01:39,710 --> 00:01:42,000
So this local that is highlighted over here.

23
00:01:42,020 --> 00:01:48,590
It means that we want to create a volume of type local, which means that the cluster nodes that we

24
00:01:48,590 --> 00:01:52,070
are using on that this storage will be resolved.

25
00:01:52,640 --> 00:01:59,180
So we have these that we show you what I really mean by that.

26
00:02:01,220 --> 00:02:07,430
On the compute engine, the three nodes that we have, the Cuban, it is cluster creation process was

27
00:02:07,430 --> 00:02:09,110
created on these three networks.

28
00:02:09,830 --> 00:02:12,390
This five GB of storage will be resolved.

29
00:02:12,390 --> 00:02:15,710
So when all of these nodes, there are three of them.

30
00:02:16,760 --> 00:02:23,920
So that that's what we are asking for and it will be mounted on some directly.

31
00:02:23,930 --> 00:02:27,020
You can put any amount directly over there.

32
00:02:27,770 --> 00:02:36,740
And this is our way of saying that create this on all the hostname and the value of the hostname we

33
00:02:36,740 --> 00:02:37,790
are providing will a hit.

34
00:02:38,330 --> 00:02:42,590
So it is right now as a variable called node name.

35
00:02:42,980 --> 00:02:47,510
So we will have to really substitute it with the actual node name.

36
00:02:49,160 --> 00:02:51,220
And how do we do that?

37
00:02:51,230 --> 00:02:58,070
So we will see that there is another variable that needs to be substituted and that is node index.

38
00:02:58,610 --> 00:03:06,290
And this is should be one, two and three so that we want to create three volumes disk volume one,

39
00:03:06,290 --> 00:03:08,330
disk volume two, this column three.

40
00:03:08,630 --> 00:03:10,820
So we need this variable to be substituted.

41
00:03:11,270 --> 00:03:14,390
And we also need this variable to be substituted.

42
00:03:14,900 --> 00:03:21,590
So after substituting, we'll have to run this three times so that we don't have to do this.

43
00:03:22,310 --> 00:03:26,900
What we are going to do is we're going to run a script for this.

44
00:03:27,800 --> 00:03:31,700
So let's see, what do I mean by that, that we'll have to run a script?

45
00:03:33,280 --> 00:03:40,960
Let's first make sure that we are connected to the cluster so that all our Kulkarni and commands that

46
00:03:42,550 --> 00:03:45,820
when we execute them, they connect to this cluster and execute.

47
00:03:46,600 --> 00:03:57,160
OK, so let's look at the script that will use this volume configuration, and that script is cube volume

48
00:03:57,160 --> 00:03:57,850
stored as it.

49
00:03:58,780 --> 00:04:05,480
Pure volume sort of sets when we are trying to create volumes, when argument does not delete and it

50
00:04:05,480 --> 00:04:10,030
does actually create, then what it does is and the creators actually default.

51
00:04:10,270 --> 00:04:15,850
It goes into the volume directly where this file persistence volume or Yamal is there.

52
00:04:16,540 --> 00:04:24,850
So it will use, said editor, and it will substitute the node name variable with actual no nib.

53
00:04:25,330 --> 00:04:32,920
And the node index variable with actual value of deep also led to one two three, whatever it is.

54
00:04:33,580 --> 00:04:39,880
So here, if you see, how do we get the Nord names, we are using this common CubeSat, it'll get nodes

55
00:04:40,180 --> 00:04:41,470
minus or name.

56
00:04:42,100 --> 00:04:47,950
So this is the command there that we are executing, so I can even show you that if I execute their

57
00:04:47,950 --> 00:04:48,790
command over here.

58
00:04:51,120 --> 00:04:56,460
It should give us all the names of on to these other names that this command will fetch.

59
00:04:58,680 --> 00:05:04,290
And it will then loop over these nodes.

60
00:05:05,270 --> 00:05:09,720
So we're here, actually, we are doing some substitution that did this.

61
00:05:10,770 --> 00:05:16,170
We are removing this node slash from it because this has only the node name.

62
00:05:16,890 --> 00:05:18,000
We have to remove this.

63
00:05:18,390 --> 00:05:22,380
So there is a little bit of said didn't work over here.

64
00:05:22,920 --> 00:05:26,340
So this is just to remove that signal.

65
00:05:26,790 --> 00:05:31,740
Then once we get the only node names over here, then we are looping over them.

66
00:05:32,190 --> 00:05:39,180
And after substituting these node names in node index in persistent volume start.

67
00:05:39,480 --> 00:05:39,720
Yeah.

68
00:05:40,410 --> 00:05:48,150
We are applying CubeSat and applying minus have come command on the substituted contents of persistent

69
00:05:48,150 --> 00:05:49,500
volume start Yemen.

70
00:05:49,860 --> 00:05:51,330
So that is what is happening over here.

71
00:05:51,720 --> 00:05:58,860
I explain this because this is something that we will do going forward also with Cube Deploy File as

72
00:05:58,860 --> 00:05:59,080
well.

73
00:05:59,100 --> 00:06:00,300
So I'll show you that later.

74
00:06:00,990 --> 00:06:02,940
But let's run this.

75
00:06:02,940 --> 00:06:03,570
Let's create

76
00:06:06,300 --> 00:06:08,400
this run cube volumes.

77
00:06:10,640 --> 00:06:11,900
So this has run.

78
00:06:12,260 --> 00:06:21,200
It has created persistent volume, disk volume zero, disk volume one and disk volume two on these respective

79
00:06:21,980 --> 00:06:22,550
nodes.

80
00:06:24,260 --> 00:06:31,730
So we reserve this space for our databases, and that was important because if we don't do that, we

81
00:06:31,730 --> 00:06:35,360
directly start our application, then our databases won't start.

82
00:06:35,360 --> 00:06:40,010
And if our databases won't start, then our application will not start.

83
00:06:41,210 --> 00:06:48,470
So that then now let's start our cluster or before we start our cluster, let's take a look at the script

84
00:06:48,470 --> 00:06:50,240
that we have for starting our cluster.

85
00:06:51,560 --> 00:07:01,250
So we have this GCP, not GCP, this cube deployed or to let's first look at the parameters that it

86
00:07:01,250 --> 00:07:01,550
need.

87
00:07:01,880 --> 00:07:08,510
One thing that it needs is it needs to know what is the registry host so we can either provide this

88
00:07:08,510 --> 00:07:15,020
as an environment variable outside of the script or if we have not done that, then we can uncovering

89
00:07:15,020 --> 00:07:17,150
this and we can provide it over here.

90
00:07:17,150 --> 00:07:21,200
So if it is EU, then you can put it as EU.

91
00:07:21,230 --> 00:07:25,010
If it is Asia, U.S., whatever it is, you can accordingly enter this.

92
00:07:25,700 --> 00:07:34,760
If you do not do any of that, then the default when you were here that it sets is Asia or GCR or Diageo.

93
00:07:35,360 --> 00:07:41,750
And similarly, the default value for revision ideas latest in case you have not specified it.

94
00:07:42,290 --> 00:07:46,160
The first part of this script really validates all of the arguments.

95
00:07:47,090 --> 00:07:55,670
Then the next part of the script substitute, or it identifies the environment variables, and if they

96
00:07:55,670 --> 00:07:59,300
are missing it, the whites default values for them.

97
00:08:00,170 --> 00:08:04,460
In the last part of this script is where the real action is.

98
00:08:05,060 --> 00:08:11,390
What the script does is it fetches all the configuration files that we have provided.

99
00:08:11,600 --> 00:08:17,480
So let's say when we run this, let me clear this.

100
00:08:19,460 --> 00:08:27,980
So the way we run this script is you deploy it, or as such, we will provide it directly.

101
00:08:28,220 --> 00:08:30,590
So here we are, providing the directly config.

102
00:08:31,100 --> 00:08:36,080
And by providing this, we are seeing all the YAML files which are there under this category.

103
00:08:36,530 --> 00:08:44,950
They have to be executed in alphanumeric order and along with that, we can provide commands, apply

104
00:08:45,560 --> 00:08:46,870
or delete.

105
00:08:47,360 --> 00:08:52,120
If we do not provide any of them, then the default values apply.

106
00:08:52,430 --> 00:09:00,970
And if you do not provide any, but then also they deploy the default value of all parties dart config

107
00:09:00,980 --> 00:09:08,000
only, which has got all the files so we can simply run this and this should create our entire system.

108
00:09:08,630 --> 00:09:11,330
So what this does is it takes all these.

109
00:09:11,330 --> 00:09:17,240
It identifies all the conflict files in the conflict file, but that we have provided it loops on them

110
00:09:17,540 --> 00:09:18,320
one by one.

111
00:09:18,800 --> 00:09:26,600
And then in the end, it applies this command cubicle minus f on the file contents.

112
00:09:27,350 --> 00:09:33,740
But before we give file contents to this, we have to substitute these this image registry.

113
00:09:33,740 --> 00:09:40,140
But this is where Docker registry is located, and we have to substitute the Revision ID over here.

114
00:09:40,160 --> 00:09:43,820
So once that is done, then we are ready to run this command.

115
00:09:43,850 --> 00:09:44,960
Why do we need to do that?

116
00:09:45,470 --> 00:09:52,640
I think I've shown you this before, but just to be sure that just see this, that's when any file and

117
00:09:52,640 --> 00:09:59,450
you will see that it has got these placeholder variables image registry part and revision ID, these

118
00:09:59,450 --> 00:09:59,950
can change.

119
00:09:59,960 --> 00:10:03,500
So we are substituting them at runtime.

120
00:10:03,500 --> 00:10:08,210
We could have hardcoded them over here and that would have made our script simpler.

121
00:10:08,900 --> 00:10:11,450
But then we would have lost the flexibility.

122
00:10:12,440 --> 00:10:16,520
So whichever way you prefer, you can do it right now.

123
00:10:16,520 --> 00:10:20,900
The automation does not assume any hardcoded value again substituted.

124
00:10:21,560 --> 00:10:23,450
So that's what we have right now.

125
00:10:24,080 --> 00:10:26,330
So let's run this script.

126
00:10:29,040 --> 00:10:37,770
So this scripture actually now create all the configurations that are there in the config, directly

127
00:10:37,860 --> 00:10:41,220
in the conflict, directly via these directories under them.

128
00:10:41,610 --> 00:10:43,680
They have these YAML files.

129
00:10:43,950 --> 00:10:46,410
So all of them will be executed.

130
00:10:47,400 --> 00:10:48,250
So let's just go.

131
00:10:48,270 --> 00:10:48,660
OK.

132
00:10:48,930 --> 00:10:50,670
So all of them were executed.

133
00:10:51,240 --> 00:10:55,200
Right now, we do not want this horizontal part or the scalar.

134
00:10:55,350 --> 00:10:56,550
We will need that later.

135
00:10:56,880 --> 00:11:05,940
So in case if you are to delete something, the way to do that is you deploy and we will say over here

136
00:11:06,930 --> 00:11:12,750
dot config and the directory name and the parameters delete.

137
00:11:13,290 --> 00:11:19,890
So once we do that, these auto scaling or horizontal auto scales have been deleted, they won't be

138
00:11:19,890 --> 00:11:21,090
applied to upwards.

139
00:11:21,420 --> 00:11:22,140
Let's go to our.

140
00:11:23,680 --> 00:11:25,870
Parts which will be there under workloads.

141
00:11:27,930 --> 00:11:31,950
So if you see this, all of them are getting created right now.

142
00:11:32,430 --> 00:11:34,910
Prometheus Radius, they are independent.

143
00:11:34,920 --> 00:11:36,090
They got created first.

144
00:11:36,600 --> 00:11:40,140
ElasticSearch board is getting created once it is created.

145
00:11:40,140 --> 00:11:45,110
Only after that, Yeager part will be created and flew into.

146
00:11:45,120 --> 00:11:48,480
Parts will be created because they have a dependency on ElasticSearch.

147
00:11:49,140 --> 00:11:58,890
Similarly, services they will be created only after Cassandra and Postgres database is unavailable

148
00:11:59,400 --> 00:12:01,120
because they have dependency on them.

149
00:12:01,140 --> 00:12:02,130
Let me refresh this.

150
00:12:06,110 --> 00:12:13,430
So are ElasticSearch still not up, that's the reason Yegor Collector Park is not running right now,

151
00:12:13,430 --> 00:12:14,880
it is in initializing work.

152
00:12:14,900 --> 00:12:18,140
You can go inside and see what's happening here.

153
00:12:18,890 --> 00:12:26,660
If you click this, it will show you that it is not in the district and initialized, so it is just

154
00:12:26,660 --> 00:12:27,350
starting it.

155
00:12:28,040 --> 00:12:34,180
So now the question is how does when it is figured out, what is the dependency?

156
00:12:34,460 --> 00:12:38,060
If you see this ElasticSearch board is running now very soon?

157
00:12:39,550 --> 00:12:43,330
You'll see a good collector bought and flew when reports are also available.

158
00:12:44,170 --> 00:12:45,220
Let me refresh this.

159
00:12:48,890 --> 00:12:56,810
So while these are coming up, we will end the Cassandra report is also coming up, let me explain you

160
00:12:57,740 --> 00:12:59,350
how this dependency will.

161
00:13:00,950 --> 00:13:05,360
So now if you see that ElasticSearch board is up, fluid board is also up.

162
00:13:05,840 --> 00:13:10,280
It came up pretty quickly, but only after ElasticSearch board became available.

163
00:13:10,970 --> 00:13:13,310
Yegor Connector board is still coming up.

164
00:13:14,510 --> 00:13:19,820
And you see this admin service boards or service boards, these are all the services or the service

165
00:13:19,820 --> 00:13:20,210
boards.

166
00:13:20,660 --> 00:13:24,770
They are actually waiting for these Kassandra boards to come up.

167
00:13:26,030 --> 00:13:28,490
Now once Cassandra boards are available.

168
00:13:30,060 --> 00:13:31,800
Let me go to Cassandra for.

169
00:13:33,080 --> 00:13:35,930
So, Cassandra, part two parts have already come up.

170
00:13:36,500 --> 00:13:40,700
And Cassandra, part two is coming up if you want to see what's happening there.

171
00:13:41,360 --> 00:13:45,440
You can go to inside the bar and click on logs.

172
00:13:47,910 --> 00:13:53,160
And this will show you actually what is going on over here, so you can look at these log files and

173
00:13:53,160 --> 00:14:00,030
that will tell you what is going on over here, so this is still coming up, so it's going to take some

174
00:14:00,030 --> 00:14:01,830
time to even go back.

175
00:14:02,310 --> 00:14:05,550
And in the meantime, our system is coming up.

176
00:14:05,940 --> 00:14:12,990
Let's understand how this dependencies manage now, it seems, because Cassandra, some of the parts

177
00:14:12,990 --> 00:14:14,550
were available, they were ready.

178
00:14:14,820 --> 00:14:16,440
So services also came up.

179
00:14:17,310 --> 00:14:23,320
So this works beautifully over here, and this works because of a configuration.

180
00:14:23,350 --> 00:14:25,260
And let me show you that configuration.

181
00:14:26,430 --> 00:14:29,820
So we're going to confirm that the services were waiting for databases.

182
00:14:29,820 --> 00:14:31,580
So let's go to any of the service.

183
00:14:31,590 --> 00:14:32,670
That's the order service.

184
00:14:34,890 --> 00:14:38,850
In order service, there are two in containers.

185
00:14:39,210 --> 00:14:41,480
So there the question, I said that I'll explain this.

186
00:14:41,480 --> 00:14:42,510
So now is the time.

187
00:14:43,110 --> 00:14:46,420
There are two container containers in this unit.

188
00:14:46,440 --> 00:14:47,640
Containers are special.

189
00:14:47,790 --> 00:14:53,990
They are special in the sense that the main containers about which we discuss here, we have orders

190
00:14:54,000 --> 00:14:54,780
of this container.

191
00:14:55,290 --> 00:14:57,210
This is our main container.

192
00:14:57,630 --> 00:15:03,600
This will not be started by Kubernetes in these containers are executed.

193
00:15:04,230 --> 00:15:13,120
So the job of these containers is to come up and execute some command and then their job is finished.

194
00:15:13,140 --> 00:15:14,190
Once they do that.

195
00:15:14,640 --> 00:15:23,070
So what commander executing over here, they are running in a loop and they'll continue to run the loop

196
00:15:23,970 --> 00:15:26,220
in the process is not available.

197
00:15:26,220 --> 00:15:31,020
So this has worked great for Postgres in a container is doing similarly.

198
00:15:31,440 --> 00:15:39,990
Wait for Cassandra is also running an infinite loop and this loop will end once it is able to connect

199
00:15:39,990 --> 00:15:44,790
to Cassandra, and it will be able to connect to Cassandra once Cassandra database comes up.

200
00:15:45,330 --> 00:15:52,410
So once these two init containers, they come up and they give a thumbs up that they are done.

201
00:15:52,620 --> 00:15:58,320
They executed successfully, then only automated service container will be started.

202
00:15:58,330 --> 00:16:00,480
So that is how we manage dependency.

203
00:16:00,810 --> 00:16:06,450
So let's say for some reason, if this is dependent on ElasticSearch also, then we can bring in another

204
00:16:06,450 --> 00:16:09,150
in a container to include ElasticSearch as well.

205
00:16:10,050 --> 00:16:16,860
So that is how this dependency is managed and are clustered now should be fully up and running.

206
00:16:19,040 --> 00:16:24,830
So with the help of readiness for a cabinet, this is able to detect that all of these services are

207
00:16:24,830 --> 00:16:25,160
running.

208
00:16:25,610 --> 00:16:27,370
There is also aliveness brought.

209
00:16:27,740 --> 00:16:34,370
If that province, then that service will be declared that service or this workload will be declared

210
00:16:34,370 --> 00:16:34,910
unhealthy.

211
00:16:35,420 --> 00:16:37,700
It will be carried and restarted.

212
00:16:38,390 --> 00:16:44,900
So if your workload, for some reason, they are getting killed and restarted again and again, you

213
00:16:44,900 --> 00:16:46,400
may like to see the reason.

214
00:16:47,180 --> 00:16:53,750
One Some of the times the reason is that the hardware capacity is not enough and they are not able to

215
00:16:53,750 --> 00:16:57,030
come up in the time that we have configured.

216
00:16:57,350 --> 00:17:02,630
We have configured 180 seconds for most of the boards, so they take more time than that to come up.

217
00:17:03,020 --> 00:17:06,530
Then when it is, we'll kill them and it will restart them.

218
00:17:06,540 --> 00:17:09,200
So that's something that we need to watch out for.

219
00:17:09,590 --> 00:17:15,920
But if you have a decent amount of hardware, these services should be able to come in the workload,

220
00:17:16,400 --> 00:17:17,120
the components.

221
00:17:17,120 --> 00:17:20,750
All these component parts should be able to come up in one or two seconds.

222
00:17:22,400 --> 00:17:23,990
So why not workloads are ready?

223
00:17:25,250 --> 00:17:30,650
Now the question is how do we connect to these workloads?

224
00:17:32,510 --> 00:17:41,090
So the answer to that is that we'll have to do that to services, and we kind of discussed this that

225
00:17:41,090 --> 00:17:46,590
if get service has to connect to, let's say, order service, it will have to go through whoever it

226
00:17:46,640 --> 00:17:51,800
is service component that we have created, which is order service or there, not the order service

227
00:17:51,800 --> 00:17:55,160
instance or the service which is implementing it.

228
00:17:55,790 --> 00:17:58,700
But the reverse proxy in between.

229
00:17:58,700 --> 00:18:04,070
So when right now I'm saying so this, I mean, I am talking about reverse proxy.

230
00:18:04,070 --> 00:18:07,370
So similarly, a web app has to exist gateway service.

231
00:18:07,850 --> 00:18:10,970
It has to go through a reverse proxy service component.

232
00:18:11,660 --> 00:18:19,220
So those reverse proxy service components we have created, but they work fine if the request is coming

233
00:18:19,460 --> 00:18:22,610
from within the network in which these components are.

234
00:18:23,660 --> 00:18:30,260
Now, if I have to connect from my desktop, so let's say if I have to open a browser and I have to

235
00:18:30,260 --> 00:18:38,480
connect to Google Cloud for connecting to my system, then that would be that will require an internet

236
00:18:38,480 --> 00:18:39,410
kind of access.

237
00:18:39,740 --> 00:18:43,070
And for that, we need special arrangement in our services.

238
00:18:44,620 --> 00:18:47,740
So let's understand that and then connect to our system.

239
00:18:48,790 --> 00:18:55,870
So how does our system allow services to let internet traffic come in?

240
00:18:56,350 --> 00:18:57,040
Let's see that.

241
00:18:58,410 --> 00:19:06,870
Most of the services that you see that are defined as cluster IP, there can be contacted only by those

242
00:19:07,020 --> 00:19:09,690
planes which are within this system.

243
00:19:10,350 --> 00:19:16,960
So let's say we have to connect to gateway service and the IP address is this.

244
00:19:16,980 --> 00:19:29,520
Let me try to connect using coal utility and board zero and status, but we will not be able to connect

245
00:19:29,520 --> 00:19:34,320
because we are outside of Cuba and it is created by our network in which ports is right.

246
00:19:34,740 --> 00:19:40,320
So we have to get into a port to be able to connect to other parts or services.

247
00:19:41,010 --> 00:19:44,430
So let's go ahead and try to get inside a port.

248
00:19:45,150 --> 00:19:47,550
So let's go to work, Lourdes.

249
00:19:50,080 --> 00:19:55,870
Over here, let let's decide to connect to, let's say, web boards.

250
00:19:58,620 --> 00:20:06,060
So the baby then entered inside their borders, tubes, hidden exec, it is very much like Dr. On.

251
00:20:09,550 --> 00:20:12,640
And this is actually not the name of the address of this bar.

252
00:20:13,030 --> 00:20:17,770
Let me forward that part if we get the actual name by clicking on that.

253
00:20:19,620 --> 00:20:22,800
So does the actual name of the board will make use of that?

254
00:20:28,510 --> 00:20:36,790
We're getting into the park over here, and we'll also have to get the name space over here, which

255
00:20:36,790 --> 00:20:37,180
is you.

256
00:20:40,350 --> 00:20:44,430
This won't work, I think the order of the arguments is.

257
00:20:49,940 --> 00:20:52,600
Think it is a space that is required over here.

258
00:20:53,390 --> 00:21:00,260
OK, so we have entered inside the container baobab container from here.

259
00:21:00,290 --> 00:21:11,720
If I try to access the service, I can even use the name over here and I should be able to do this.

260
00:21:14,900 --> 00:21:22,940
Well, could was obvious is not accessible because it's in namespace service and this web app is in

261
00:21:22,940 --> 00:21:24,470
namespace UI.

262
00:21:24,800 --> 00:21:29,590
So we'll have to add dart service to this now existence.

263
00:21:30,440 --> 00:21:32,600
I am getting a response over here.

264
00:21:33,200 --> 00:21:42,020
So what I'm trying to tell you over here is that here I can access my parts through the service button.

265
00:21:42,260 --> 00:21:46,400
The client has to be there within the same network.

266
00:21:47,120 --> 00:21:55,580
So I'm not in the same network if I'm trying to connect to internet, if I try to connect like this,

267
00:21:55,580 --> 00:21:57,510
if I provide a you are a little.

268
00:21:58,100 --> 00:22:03,950
I'm definitely outside of the school when it is network, so I need some way to get into this network.

269
00:22:05,870 --> 00:22:11,690
So the way to do that is through a configuration, there are two configuration that will allow us to

270
00:22:11,690 --> 00:22:12,110
do that.

271
00:22:12,650 --> 00:22:17,720
Let's look at these configurations, so let's go into the config.

272
00:22:18,140 --> 00:22:24,860
Let's say we want to connect to the web from internet, and we do want to do that.

273
00:22:25,370 --> 00:22:31,880
So this service configuration that we see this here, we do not have clustered IP.

274
00:22:31,880 --> 00:22:36,080
So if I look at I'll show you one of the component cluster IP is up.

275
00:22:36,680 --> 00:22:41,060
So let's hear service and let's take inventory service.

276
00:22:42,710 --> 00:22:46,700
So we see the service component of inventory service here.

277
00:22:47,300 --> 00:22:56,720
This service is off type cluster IP, which means only those IP addresses claims with IP addresses that

278
00:22:56,720 --> 00:22:58,550
are within the same network.

279
00:22:59,150 --> 00:23:01,600
They will be able to access this service.

280
00:23:01,610 --> 00:23:04,130
So that's what this cluster IP means.

281
00:23:05,240 --> 00:23:12,230
And that's what we see over here if we look at different services.

282
00:23:14,540 --> 00:23:22,010
This is what Lester IP means, that they are internally accessible, not the ones which I wanted to

283
00:23:22,010 --> 00:23:25,310
actually access from the internet from my workstation.

284
00:23:25,820 --> 00:23:28,840
I have configured them as port.

285
00:23:29,450 --> 00:23:36,440
The other option that I had was I can configure them as load balancer or load balance that generally

286
00:23:36,440 --> 00:23:37,310
will cost you.

287
00:23:37,790 --> 00:23:40,990
So it is a good practice to start with no port.

288
00:23:41,030 --> 00:23:47,870
And if your plan state is working fine, then after that you can convert it into load balancer.

289
00:23:47,870 --> 00:23:56,180
Because once I convert it into a load balancer, Google will actually allocate a load balancer for my

290
00:23:56,180 --> 00:24:00,980
system, and that will mean that I'll have to pay for that load balancer.

291
00:24:00,980 --> 00:24:03,070
So it's better to start with more port.

292
00:24:05,040 --> 00:24:07,160
Well, OK, so here the Northport is.

293
00:24:07,490 --> 00:24:10,880
Instead of cluster IP, we have specified type as what port.

294
00:24:10,890 --> 00:24:13,100
Later on, we will make it as load balancer.

295
00:24:13,670 --> 00:24:15,080
But right now it doesn't work port.

296
00:24:15,080 --> 00:24:17,600
And then I have specified Northport.

297
00:24:17,930 --> 00:24:21,440
Then I'll also have to specify Northport.

298
00:24:21,530 --> 00:24:24,080
In this case, it is three two one zero zero.

299
00:24:25,400 --> 00:24:29,600
Now let's take a look at how this Northport Northport works.

300
00:24:30,290 --> 00:24:32,000
So as the name indicates.

301
00:24:33,650 --> 00:24:35,660
We can go to any note.

302
00:24:35,720 --> 00:24:43,520
So let's go to the north in compute engine and get the IP address, public IP address, external IP

303
00:24:43,520 --> 00:24:45,260
address of any of these nodes.

304
00:24:45,260 --> 00:24:47,480
So let's pick up this note.

305
00:24:47,490 --> 00:24:52,430
Let's copy this public IP, and let's put it over here.

306
00:24:52,910 --> 00:25:00,050
Here we can use that Northport three two one zero zero that we just saw in our configuration and will

307
00:25:00,050 --> 00:25:01,970
be able to connect to our application.

308
00:25:02,660 --> 00:25:05,150
We can, in fact, choose any of them.

309
00:25:05,150 --> 00:25:06,470
So let's hit this one.

310
00:25:06,620 --> 00:25:07,370
We can pick up.

311
00:25:08,920 --> 00:25:12,280
And we can connect to.

312
00:25:13,190 --> 00:25:14,450
Using this new report.

313
00:25:14,870 --> 00:25:22,730
So what this means is that this Northport is a mechanism where a Port Northport is opened on all of

314
00:25:22,730 --> 00:25:29,750
these nodes, and we can use that port to get into our system to access services off type Northport.

315
00:25:30,500 --> 00:25:33,800
So in this case, with this configuration?

316
00:25:40,700 --> 00:25:49,280
Well, this new report, this Northport is tied up with this Web component in BABIP service.

317
00:25:49,730 --> 00:25:55,510
So when we connect to this particular port, we are actually directed to this web service.

318
00:25:55,550 --> 00:25:56,900
So that's what purpose.

319
00:25:57,380 --> 00:26:00,470
And we have provided this Northport for.

320
00:26:01,820 --> 00:26:09,680
Most of the components which require internet access, so let's go to Kubernetes engine and then we'll

321
00:26:09,680 --> 00:26:10,670
go to services.

322
00:26:12,860 --> 00:26:19,610
So they're their single page application at you, Prometheus.

323
00:26:20,750 --> 00:26:28,700
Cubana for ElasticSearch and Yegor for tracing all of them, they have no port.

324
00:26:32,520 --> 00:26:35,080
Let's hear their components if we are able to access.

325
00:26:35,100 --> 00:26:37,720
Definitely, we are able to access you.

326
00:26:37,720 --> 00:26:38,010
Wait.

327
00:26:38,310 --> 00:26:39,720
It's close one of the debt.

328
00:26:40,560 --> 00:26:42,180
And let's try to sign him.

329
00:26:45,530 --> 00:26:47,270
OK, so we have signed it.

330
00:26:49,130 --> 00:26:50,780
Right now, there are no products.

331
00:26:52,450 --> 00:26:59,620
So we'll have to put some data over here, so what we will do is we will create some data, let's create

332
00:27:00,070 --> 00:27:05,020
hundred users and find products, so let's do that.

333
00:27:07,780 --> 00:27:13,240
Data, while this is getting created, it's going to take a long time to create this a little longer.

334
00:27:13,630 --> 00:27:15,400
So this may actually time out.

335
00:27:16,120 --> 00:27:17,380
You don't need to worry about that.

336
00:27:17,680 --> 00:27:20,500
You can always check the data by looking at these tables.

337
00:27:21,890 --> 00:27:23,450
OK, they're done.

338
00:27:23,810 --> 00:27:27,350
Let's go back to OK, this this time, though, not a problem.

339
00:27:27,920 --> 00:27:30,770
Let's go and check if the data was created.

340
00:27:30,810 --> 00:27:33,050
So we look at user or table.

341
00:27:33,980 --> 00:27:36,440
We go all the way down to the bottom.

342
00:27:37,400 --> 00:27:41,210
Then we have about one hundred users.

343
00:27:41,210 --> 00:27:44,150
Can you get this 100 user, which God created?

344
00:27:44,150 --> 00:27:45,050
Let's go all the way up.

345
00:27:46,240 --> 00:27:50,380
Let's check inventory data this is created in the end, so this also was created.

346
00:27:50,910 --> 00:27:53,530
We have inventory for our products.

347
00:27:53,560 --> 00:27:54,100
We are done.

348
00:27:54,790 --> 00:27:58,090
So we have quite a few products with inventory over here.

349
00:27:58,450 --> 00:28:03,010
Now we can test our system if it is working fine or not.

350
00:28:03,010 --> 00:28:05,430
Let's add this desperate up to two cart.

351
00:28:06,940 --> 00:28:09,790
Now get in the car and let's give the order.

352
00:28:10,540 --> 00:28:11,860
It is in process right now.

353
00:28:11,860 --> 00:28:13,500
It should have gone to messaging you.

354
00:28:14,200 --> 00:28:17,500
If I look at orders right now, there is an order.

355
00:28:17,980 --> 00:28:20,940
And if I want to check the messaging queue, how do I monitor that?

356
00:28:20,950 --> 00:28:21,730
Come over here?

357
00:28:22,430 --> 00:28:24,070
Click message you over here.

358
00:28:25,570 --> 00:28:28,630
This will take you to message queue.

359
00:28:30,850 --> 00:28:31,540
This is.

360
00:28:36,360 --> 00:28:43,710
OK, so I am in now, if you see this in the last minute, this is the transaction that took place.

361
00:28:43,890 --> 00:28:50,910
This was so one request came and it was published and then it was delivered.

362
00:28:50,910 --> 00:28:53,550
So our message queue was working perfectly fine.

363
00:28:54,210 --> 00:28:57,630
Let's look at our Yeager trace.

364
00:29:00,340 --> 00:29:04,930
So let's look at the two order service we have created just now in order.

365
00:29:05,690 --> 00:29:07,570
Let's look at this create out of one card.

366
00:29:08,440 --> 00:29:09,540
Do we have the address?

367
00:29:10,000 --> 00:29:17,350
So we have our address that shows us that dressing infrastructure is working fine.

368
00:29:17,900 --> 00:29:22,030
Now let's check our last component, which is ElasticSearch.

369
00:29:23,170 --> 00:29:25,360
So that also seems to be working fine.

370
00:29:27,640 --> 00:29:29,050
Explode on my own.

371
00:29:30,370 --> 00:29:34,630
And that's what we discover there will have to create an index pattern.

372
00:29:34,990 --> 00:29:42,400
It's clear to you that if you see if Luann de Jager yago the service in Spain over here, that means

373
00:29:42,970 --> 00:29:48,760
boats were in the that are active and they are putting their name to ElasticSearch so we can use that

374
00:29:48,760 --> 00:29:51,700
data for log monitoring.

375
00:29:51,880 --> 00:29:58,750
Let's create index on fluently because that's what we are interested in right now.

376
00:30:02,330 --> 00:30:07,290
OK, so let's create the index patent, we have seen this before.

377
00:30:07,790 --> 00:30:10,700
There's nothing new that I'm doing over here.

378
00:30:12,040 --> 00:30:16,270
So we have all these log messages here, let's select message.

379
00:30:16,930 --> 00:30:22,030
You can select level and also host name, which should be good for us.

380
00:30:23,140 --> 00:30:29,200
So whatever is happening right now, whatever is getting logged in, our system is coming over here.

381
00:30:29,200 --> 00:30:35,620
If I want to check that any messages which has any error.

382
00:30:37,360 --> 00:30:41,110
So there was some error, so we were not able to get product.

383
00:30:41,560 --> 00:30:49,000
I think this happened when initially our system didn't have any data and I tried executing get products

384
00:30:49,360 --> 00:30:50,280
to you.

385
00:30:50,290 --> 00:30:52,840
I going, I click get products.

386
00:30:52,840 --> 00:30:53,860
And there was no data.

387
00:30:54,520 --> 00:30:56,710
That's when this error happened.

388
00:30:56,710 --> 00:31:06,400
And so this way we can use this logging very effectively to find out what is going inside our cluster.

389
00:31:06,400 --> 00:31:06,700
So.

390
00:31:08,200 --> 00:31:13,300
This way, we can look at the messages, if you want to see what orders were processed, we can do full

391
00:31:13,300 --> 00:31:18,280
text search, all sorts of process order like that we can search.

392
00:31:19,120 --> 00:31:22,630
So one order we a process that has shown up over here.

393
00:31:22,630 --> 00:31:27,040
So this becomes important if you're getting any errors and you are.

394
00:31:27,040 --> 00:31:32,620
Fortunately, ElasticSearch logging framework is working fine with your chart.

395
00:31:33,160 --> 00:31:37,810
If your system is stable, then you should be able to solve those problems by coming over here.

396
00:31:37,810 --> 00:31:39,280
So this becomes very important.

397
00:31:39,700 --> 00:31:46,390
Otherwise, it's really difficult to get into each part and figure out what was the log.

398
00:31:47,260 --> 00:31:50,290
You can do that, but it is very tedious thing to do here.

399
00:31:50,770 --> 00:31:52,900
Log centralized, so it makes it easier.

400
00:31:53,380 --> 00:31:57,700
And if there is any performance problem, we have this tracing solution with us.

401
00:31:59,140 --> 00:32:06,280
So with that, we were able to access our entire system.

402
00:32:06,790 --> 00:32:14,590
So one of our goal was this that we should be able to deploy our entire system on Kubernetes and it

403
00:32:14,590 --> 00:32:16,090
should functionally work well.

404
00:32:16,450 --> 00:32:18,100
So we have achieved that goal.

405
00:32:18,760 --> 00:32:24,850
What we will do in the next session is we will look at some of these operational capabilities of when

406
00:32:24,850 --> 00:32:28,960
it is like higher reliability, auto scaling and rolling upgrades.

407
00:32:28,970 --> 00:32:30,640
So let's do that in the next session.
