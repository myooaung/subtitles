WEBVTT
1
00:00:00.300 --> 00:00:02.910
Hello and welcome back to the courts on deep learning.

2
00:00:02.940 --> 00:00:04.640
Today we've got a very exciting tutorial here.

3
00:00:04.650 --> 00:00:09.660
We're talking about long short term memory or else teams for short.

4
00:00:09.660 --> 00:00:10.980
So let's get started.

5
00:00:10.980 --> 00:00:12.860
It's actually going to be quite a saturated trial.

6
00:00:12.870 --> 00:00:16.610
So we've got our own little plan of attack for today.

7
00:00:16.620 --> 00:00:19.370
Today we're going to first of all look at a bit of history.

8
00:00:19.410 --> 00:00:23.990
So where it came from what was the main idea behind it why people invented it.

9
00:00:24.210 --> 00:00:27.320
Then we're going to talking about Ellis they asked him architecture.

10
00:00:27.340 --> 00:00:31.680
There's gonna be the bulk of our tutorial today so be prepared for that and then we're going to have

11
00:00:31.680 --> 00:00:36.090
an example walk through hopefully you'll have enough time for that as well.

12
00:00:36.090 --> 00:00:36.520
All right.

13
00:00:36.630 --> 00:00:43.490
So let's get started here we've got a problem which we identified in the previous tutorials.

14
00:00:43.530 --> 00:00:47.230
We talked about the vanishing gradient problem.

15
00:00:47.490 --> 00:00:53.460
So in short what happens is as we propagate the error through the network it has to go through the unravelled

16
00:00:53.610 --> 00:01:01.180
temporal loop and as it does that it goes through these layers of neurons which are connected to themselves.

17
00:01:01.290 --> 00:01:05.990
These hidden layers which are connected to themselves and they're connected by the means of a weight

18
00:01:06.030 --> 00:01:12.630
called the W record the W recurrent weight and because this weight is applied many many times on top

19
00:01:12.630 --> 00:01:18.810
of itself that causes the gradient to decline rapidly meaning that the weight of the layers on the very

20
00:01:18.810 --> 00:01:24.150
far left are a bit dated much slower than the weight on the of the layers on the far right and this

21
00:01:24.150 --> 00:01:31.140
creates a domino effect because the weights of the fire left layers are very important because they

22
00:01:31.320 --> 00:01:36.270
dictate the outputs of those layers which are the inputs to the far right layers and therefore the whole

23
00:01:36.270 --> 00:01:40.440
training of the network suffers and that is called the problem of the vanishing gradient.

24
00:01:40.440 --> 00:01:46.110
And as a rule of thumb we could see here that if doubly records small then we have vanishing gradient

25
00:01:46.110 --> 00:01:49.870
if w wreck is large then we have exploding gradient.

26
00:01:49.950 --> 00:01:54.870
So now how do you solve this problem we've talked about we talked about a couple of possible solutions

27
00:01:54.870 --> 00:02:01.230
we talked about clipping the gradient or penalizing the gradient for exploding gradients we talked about

28
00:02:01.530 --> 00:02:09.450
smartly selecting the weights or the echo state networks which we didn't go into detail on for the vanishing

29
00:02:09.450 --> 00:02:12.710
gradient and also we talked about the L stands for the vanishing gradient.

30
00:02:13.050 --> 00:02:18.990
Just if you separate yourself from all of this information on all this theory and knowledge how would

31
00:02:18.990 --> 00:02:20.160
you solve a problem like this.

32
00:02:20.160 --> 00:02:22.550
What's the easiest and faster solution to it.

33
00:02:22.550 --> 00:02:28.560
So if we have w req is in simple terms less than 1 then we have vanishing gradient.

34
00:02:28.560 --> 00:02:35.070
If we have w req greater than one we've got exploding green what's the first thing that comes to mind

35
00:02:35.070 --> 00:02:36.310
to solve this problem.

36
00:02:36.360 --> 00:02:42.630
Well the first thing comes to mind is to make w recce call 1 and that's exactly what was done in the

37
00:02:42.630 --> 00:02:43.590
LSD Ms.

38
00:02:43.590 --> 00:02:50.280
This is a very simplified explanation and there's definitely more to it than just w rectangles one but

39
00:02:50.610 --> 00:02:55.750
in general that's the idea and that's all it took.

40
00:02:55.770 --> 00:03:00.990
And when I saw this for the first time I was so excited that you know the solution is so simple it's

41
00:03:00.990 --> 00:03:06.110
really a genius solution and that completely gets rid of the vanishing gradient problem.

42
00:03:06.510 --> 00:03:09.900
And so who are the people behind this.

43
00:03:09.900 --> 00:03:14.300
Here are two gentlemen we've talked about separator.

44
00:03:14.630 --> 00:03:25.160
And the second person is Jurgen Schmid Huber who is who was his supervisor during SAP's research or

45
00:03:25.200 --> 00:03:26.170
pitch D.

46
00:03:26.220 --> 00:03:27.530
And yeah.

47
00:03:27.540 --> 00:03:33.780
So basically they wrote a paper on 9th in 1987 about the stamp and that's when Ellis jams were introduced

48
00:03:34.110 --> 00:03:36.830
to Earth for the first time very exciting topic.

49
00:03:36.930 --> 00:03:40.000
So let's have a look at what they actually are.

50
00:03:40.020 --> 00:03:45.520
So we've got a recurrent neural network right here unravelled a temporal loop.

51
00:03:45.600 --> 00:03:47.080
This is what it looks like.

52
00:03:47.220 --> 00:03:54.840
If you dig in inside the recurring role rhetoric and right here I wanted to do a quick shout out to

53
00:03:55.730 --> 00:03:56.440
Christopher.

54
00:03:56.490 --> 00:03:57.560
All are.

55
00:03:57.600 --> 00:04:00.750
So here is his blog right here.

56
00:04:00.870 --> 00:04:02.970
Very well-written blog.

57
00:04:02.970 --> 00:04:05.420
Amazing images as you can see.

58
00:04:05.460 --> 00:04:09.530
And we're going to be using the images from this blog in our explanation here.

59
00:04:09.530 --> 00:04:17.400
So thank you very much to Christopher for making this publicly available and allowing the use of his

60
00:04:17.400 --> 00:04:20.320
images in other works.

61
00:04:20.370 --> 00:04:24.540
So we'll definitely reference this at the end of today's tutorial.

62
00:04:24.540 --> 00:04:27.090
And going back to our presentation.

63
00:04:27.090 --> 00:04:33.020
So here we've got the recurrent neural network and this is what it looks like inside.

64
00:04:33.030 --> 00:04:34.320
And this is where the problem lies.

65
00:04:34.320 --> 00:04:41.050
So this operation that happens here is actually a neural network layer operation as we'll see further

66
00:04:41.060 --> 00:04:41.710
down.

67
00:04:41.940 --> 00:04:50.460
But simply put as you have outputs coming into your module into this module this operation is applied

68
00:04:50.490 --> 00:04:52.970
and then goes into the next module the pressure is applied and so on.

69
00:04:52.970 --> 00:04:57.570
So as you apply this operation when you back propagate it goes through all of this and that's where

70
00:04:57.570 --> 00:05:02.690
the weights are applied that's where the W record's sitting as that weight is applied as it's applied

71
00:05:02.690 --> 00:05:08.480
as applied the gradient vanishes vanishes and vanishes and that means that the weights cannot be updated

72
00:05:08.480 --> 00:05:16.190
properly or fast enough to train the network properly and so the further away you try to look the more

73
00:05:16.610 --> 00:05:22.490
promise you have in the more of the vanishing gradient you have this is the standard Arnon and this

74
00:05:22.490 --> 00:05:29.780
is what the Ellis T M version looks like and I know you might be thinking that while this is super complex

75
00:05:30.440 --> 00:05:32.050
this there's so much going on here.

76
00:05:32.060 --> 00:05:35.740
And indeed it is a bit more complex than the standard Arnon.

77
00:05:35.750 --> 00:05:37.300
But check this out.

78
00:05:37.340 --> 00:05:45.770
This is how LSA teams are normally displayed in literature and in papers and so on and so definitely

79
00:05:45.800 --> 00:05:53.810
this is the same thing as you saw before previous before but it's just a much more convoluted explanation

80
00:05:53.810 --> 00:05:54.890
or representation.

81
00:05:54.890 --> 00:06:01.520
So definitely this option is better and we're going to stick with this one and more so the it looks

82
00:06:01.640 --> 00:06:07.400
it looks difficult now but the goal is that by the end of this tutorial you are completely comfortable

83
00:06:07.400 --> 00:06:12.200
what's going on here and I think that's pretty exciting that this even though this might may seem a

84
00:06:12.200 --> 00:06:18.720
bit complex now towards the end of the tour hopefully you'll be able to navigate LACMA quite well.

85
00:06:18.720 --> 00:06:24.470
And so before we go deep into what's going on here then I wanted to highlight the main point.

86
00:06:24.470 --> 00:06:27.160
So remember we said W Iraq equals one.

87
00:06:27.200 --> 00:06:33.590
Well that's this line over here that pipeline at the top as you can see there's nothing not much going

88
00:06:33.590 --> 00:06:33.820
on here.

89
00:06:33.830 --> 00:06:42.560
There's only like two point wise operations as we understand for the done and there's no complex neural

90
00:06:42.560 --> 00:06:47.960
network Laird operations they're all brought out to this bar and that's this is the essence if you're

91
00:06:47.960 --> 00:06:54.020
going to take away one thing from today's tutorial that then this is that that you Dallas teams have

92
00:06:54.500 --> 00:06:56.850
a memory cell it's called the memory cell.

93
00:06:56.930 --> 00:07:05.030
I call the memory pipeline which just goes through time and this is us going through time and it can

94
00:07:05.210 --> 00:07:12.830
very freely flow through time sometimes it might be removed and erased sometimes things might be added

95
00:07:12.830 --> 00:07:14.580
into it but that's pretty much it.

96
00:07:14.600 --> 00:07:19.400
Otherwise it flows through times freely and therefore when you back propagate through these Alice jams

97
00:07:19.700 --> 00:07:29.640
you don't have that problem of your vanishing gradient that's that's the essence of Ellis stamps.

98
00:07:29.660 --> 00:07:29.870
All right.

99
00:07:29.870 --> 00:07:32.350
So now let's dig in in a bit more detail.

100
00:07:32.360 --> 00:07:35.570
So we're going to replace these modules on the left and the right.

101
00:07:35.570 --> 00:07:41.950
We have something more simple is going to replace them with our.

102
00:07:42.140 --> 00:07:50.350
These representations so C stands for memory a memory cell I guess H is your output.

103
00:07:50.360 --> 00:07:55.220
So there as you can see there's the output going out into the world and here you got your output going

104
00:07:55.220 --> 00:08:00.580
into the next module into next block and then here you've got your input.

105
00:08:00.740 --> 00:08:05.660
So basically this is the output from the previous module which also went into the world but also is

106
00:08:05.660 --> 00:08:06.720
coming here.

107
00:08:06.740 --> 00:08:07.190
So there we go.

108
00:08:07.190 --> 00:08:14.350
So an Ellis team module takes in three inputs and has two outputs.

109
00:08:14.390 --> 00:08:21.440
So C.T. NHT because these are the same and the important thing to understand and remember is that everything

110
00:08:21.440 --> 00:08:23.260
here is a vector.

111
00:08:23.270 --> 00:08:28.160
So all of these are this is not just one neuron not just one value.

112
00:08:28.160 --> 00:08:34.090
There are lots and lots and lots of values here behind hiding behind this war this letter equity.

113
00:08:34.190 --> 00:08:36.910
And here as well and here as well and everywhere.

114
00:08:37.100 --> 00:08:37.880
These are all

115
00:08:40.450 --> 00:08:41.210
layers.

116
00:08:41.210 --> 00:08:42.370
So just remember that.

117
00:08:42.500 --> 00:08:47.150
And we're going to reference them as vectors because that's pretty much same thing just lots of different

118
00:08:47.150 --> 00:08:49.420
values in one vector.

119
00:08:49.430 --> 00:08:56.390
Remove that and that will make it will allow you not to go down the wrong path in the intuitive understanding

120
00:08:56.390 --> 00:09:00.560
of this just remember that these are all vectors and let's go through the legend.

121
00:09:00.560 --> 00:09:02.420
So we've got vector transfers.

122
00:09:02.450 --> 00:09:11.990
So any line here is a vector being transferred or kind of moving around in this in this in this architecture

123
00:09:12.500 --> 00:09:17.900
and yes that's just kind of to reiterate that it is a vector then we've got a concatenation here.

124
00:09:18.320 --> 00:09:22.940
So here you can see that there's two lines combining into one.

125
00:09:22.940 --> 00:09:31.010
And important to understand that this is done here just to save space and make it less convoluted than

126
00:09:31.010 --> 00:09:32.150
it possibly could be.

127
00:09:32.510 --> 00:09:37.850
But the way the best way to imagine it is you that these two lines are running in parallel let you're

128
00:09:37.850 --> 00:09:42.680
not actually combining concatenation means that you combine these two Xs on top of each other but I

129
00:09:42.680 --> 00:09:47.660
think it's even easier to understand if you just think of it as in these they're two pipes running here

130
00:09:47.690 --> 00:09:49.040
but they're running parallel to each other.

131
00:09:49.040 --> 00:09:53.300
You've got one pipe and then goes here and the second pipe goes here then these same pipes go here and

132
00:09:53.300 --> 00:09:54.350
then they attach that.

133
00:09:54.350 --> 00:10:03.100
So basically you have two pipes running in parallel feeding into these neural network layer layer operations.

134
00:10:03.230 --> 00:10:04.430
Then you've got copy.

135
00:10:04.430 --> 00:10:06.700
So where do we have copy you have copy here.

136
00:10:06.740 --> 00:10:09.900
The memories copies go straight ahead and just copy it over here.

137
00:10:09.900 --> 00:10:12.770
Then this output is copied over here.

138
00:10:12.770 --> 00:10:14.180
Then you've got point wise operations.

139
00:10:14.180 --> 00:10:15.610
Now we get to the interesting stuff.

140
00:10:15.650 --> 00:10:18.770
So you've got a couple of point wise operations here.

141
00:10:18.800 --> 00:10:26.330
That's five of them and the first thing we're going to talk about these ones to access the Xs are evolves

142
00:10:26.660 --> 00:10:29.020
and they all have names this is the forget valve.

143
00:10:29.030 --> 00:10:36.890
The memory valve and the output valve and in literature you will see like letters F V and O in the actual

144
00:10:36.890 --> 00:10:38.570
formulas representing these balls.

145
00:10:39.110 --> 00:10:46.490
And so involved looks like this in the in the plumbing involved looks like this and we're going to kind

146
00:10:46.490 --> 00:10:48.050
of think of it that way as well.

147
00:10:48.050 --> 00:10:54.590
So you've got water or basically something flowing through and then you can either close it or you can

148
00:10:54.680 --> 00:10:56.960
open it or you can close it to some extent.

149
00:10:56.960 --> 00:10:57.630
Same thing here.

150
00:10:57.660 --> 00:11:05.450
So you've got the forget valve is basically controlled by this layer operation and we'll talk about

151
00:11:05.450 --> 00:11:06.640
that in a second.

152
00:11:06.740 --> 00:11:11.720
And based on the output of that based on the decision made here this wall was either closed or open

153
00:11:11.720 --> 00:11:14.960
so if it's open memory flows through freely.

154
00:11:15.140 --> 00:11:20.810
If it's closed then memory is cut off and therefore it's not dozens not transferred further and then

155
00:11:21.050 --> 00:11:25.700
new memory just is will be added here probably based on the results here.

156
00:11:25.700 --> 00:11:31.940
Then you've got the memory valve which again is controlled by a Sigma Sigma stands for the sigmoid activation

157
00:11:31.940 --> 00:11:32.290
function.

158
00:11:32.290 --> 00:11:41.090
That means that that's activation function using these layer operations and as the decision is made

159
00:11:41.090 --> 00:11:48.920
here this value which is another layer operation which we'll get to in a second is either added to the

160
00:11:49.640 --> 00:11:54.500
memory or is somewhat added to the memory or is not added to the memory depending on the value that

161
00:11:54.500 --> 00:11:56.430
is decided in this valve.

162
00:11:56.510 --> 00:11:59.860
And then again another valve controlled by sigmoid is remember sigmoid.

163
00:12:00.110 --> 00:12:05.780
Why we using sigmoid because they're from 0 to 1 and therefore 0 sense for no memory no nothing goes

164
00:12:05.780 --> 00:12:10.970
through 1 stands for something goes through and then here you've got the valve which is the forget valve

165
00:12:11.930 --> 00:12:13.210
since they are not the forgetful.

166
00:12:13.220 --> 00:12:15.560
This is the output valve and we'll get to that in a second.

167
00:12:15.560 --> 00:12:18.950
We're pretty sure you going through the whole network already.

168
00:12:19.400 --> 00:12:27.810
So and then we've got the t shaped kind of like pipe or T shape joint it.

169
00:12:27.980 --> 00:12:28.880
I'll show you where that is.

170
00:12:28.880 --> 00:12:30.080
That is over here.

171
00:12:30.140 --> 00:12:34.480
So where you have memory going through and then you can add additional memory so if you go back you've

172
00:12:34.480 --> 00:12:39.290
got memory flowing through this joint and maybe some additional memory will be added into it maybe not

173
00:12:39.410 --> 00:12:42.010
depending on the both.

174
00:12:42.470 --> 00:12:46.820
And that's very much Ed you've got the tangent operation here that's this more kind of like mathematical

175
00:12:47.030 --> 00:12:51.800
behind it why you wanted to be between minus one and one we won't get into details and that but that's

176
00:12:51.800 --> 00:12:59.420
another point wise operation here you have and you've got the neural network layer operations over here

177
00:12:59.720 --> 00:13:01.420
that's that's their representation.

178
00:13:01.430 --> 00:13:07.670
So basically just think of them as you've got and you've got instead of point wise point wise like element

179
00:13:07.670 --> 00:13:12.830
by element of a vector know if you want to multiply of extra by zero you multiply every element by 0

180
00:13:12.830 --> 00:13:18.710
or by 1 or by a certain amount kind of think of it nowhere that's a very simplistic way to think about

181
00:13:18.710 --> 00:13:25.040
these pointers operations whereas these ones are a bit more complex you've got a layer coming in and

182
00:13:25.040 --> 00:13:29.990
then you going to layer coming out or like a layer because again everything here is a vector so you've

183
00:13:29.990 --> 00:13:37.130
got a layer of these sigmoid is coming out controlling the valve for each one of these elements in the

184
00:13:37.130 --> 00:13:42.650
vector of memory so there's not just one sigmoid there's many different and that's why you've got a

185
00:13:42.650 --> 00:13:48.390
whole layer coming in and then you're going to layer coming on these are layer operations.

186
00:13:48.440 --> 00:13:50.190
So just remember that.

187
00:13:50.420 --> 00:13:51.560
And yeah.

188
00:13:51.580 --> 00:13:56.990
So we're ready to look into this in in step by step and it's gonna be pretty simple because we've discussed

189
00:13:56.990 --> 00:14:00.560
everything already in terms of how it works.

190
00:14:00.570 --> 00:14:08.510
So we've got a new value coming in you've got memory you've got no memory you've got a value coming

191
00:14:08.510 --> 00:14:18.800
from previous node and loops and together they are combined two to decide whether this file should go

192
00:14:18.800 --> 00:14:23.330
should go ahead or should be closer should be open though it was somewhat closed or open then you've

193
00:14:23.330 --> 00:14:28.670
got these two again combined together or not go combined again the flow in parallel and then they're

194
00:14:28.670 --> 00:14:32.160
combined in in here or in this operation.

195
00:14:32.460 --> 00:14:37.100
And and basically it's not just them combined it's like there's lots of layers here lots of layers here

196
00:14:37.540 --> 00:14:44.960
one layer lots of neurons here not some neurones here and then all of that is basically in one layer

197
00:14:44.960 --> 00:14:51.950
operation is used to decide what value we're going to pass through and then also if that value is gonna

198
00:14:51.980 --> 00:14:57.770
pass through or not and to what extent then here we've got the memory flowing through we've got the

199
00:14:57.770 --> 00:15:03.050
forget valve if it's closed or open we've got memory valve closed are open and we're adding in some

200
00:15:03.050 --> 00:15:05.540
memory possibly if we want to update.

201
00:15:05.570 --> 00:15:09.740
So basically we can let this whole memory flow through then keep this one closed keep this an open the

202
00:15:09.740 --> 00:15:11.150
memory won't change.

203
00:15:11.150 --> 00:15:18.470
Keep this one or we can keep this one close and keep this one open then we can update the memory completely.

204
00:15:18.470 --> 00:15:27.470
And here finally we've got these two values combined to decide what part of the memory of memory pipeline

205
00:15:28.130 --> 00:15:33.060
is going to be output he's going to become output of this module.

206
00:15:33.080 --> 00:15:38.510
Is it going to go as fully as the output of it or to some extent and so on.

207
00:15:38.510 --> 00:15:40.760
So again these decide that.

208
00:15:40.820 --> 00:15:42.200
So that's how it all works.

209
00:15:42.200 --> 00:15:45.050
Pretty straightforward architecture.

210
00:15:45.050 --> 00:15:47.770
Let's have a look at a specific example.

211
00:15:48.050 --> 00:15:49.750
To understand this a bit better.

212
00:15:49.790 --> 00:15:54.590
So the example we talked about I'm a boy who likes to learn is some click that it achieved in translation

213
00:15:54.590 --> 00:15:57.800
to check if this were were girl then.

214
00:15:57.800 --> 00:15:59.660
Here he is Sam Holcomb.

215
00:15:59.690 --> 00:16:06.020
There are the oh cheat meaning that these two words not just this word would change but also it would

216
00:16:06.020 --> 00:16:10.580
affect these two words so different two in check rather than in English.

217
00:16:10.580 --> 00:16:16.160
So these words are affected by the gender of the subject and therefore in our l esteem we might want

218
00:16:16.160 --> 00:16:19.820
to store the subject boy in this case in the memory.

219
00:16:19.820 --> 00:16:27.130
So for instance let's say boys stored here and then is just flowing through freely and nothing is changing.

220
00:16:27.130 --> 00:16:32.960
Like if we GAF if our new information doesn't tell us that there is a new subject we just have boy flowing

221
00:16:33.230 --> 00:16:35.130
through freely and keeps flowing again.

222
00:16:35.300 --> 00:16:42.170
If for instance we have a new subject we have girl or we have we have a name like Amanda or something

223
00:16:42.170 --> 00:16:49.760
else and we understand that we've got a new subject then Will's door will let this evolve will close

224
00:16:49.760 --> 00:16:55.180
this while we'll say you know destroy the memory that we had then open this wall put a new memory and

225
00:16:55.190 --> 00:17:00.590
then put the name here put the subject won't just put in the gender won't just put in like female we'll

226
00:17:00.590 --> 00:17:03.170
actually put the whole as much information as we can.

227
00:17:03.170 --> 00:17:04.880
For instance this could be the architect.

228
00:17:04.900 --> 00:17:11.870
It doesn't have to be like this but as an example we could put in for instance girl now into this into

229
00:17:11.870 --> 00:17:15.280
this pipeline and why would we do that.

230
00:17:15.290 --> 00:17:19.640
Well because then from that we can extract different elements of information we can extract that it's

231
00:17:19.640 --> 00:17:22.670
female we can extract that it's singular.

232
00:17:22.700 --> 00:17:23.060
Right.

233
00:17:23.150 --> 00:17:27.620
So not just that it's there there's additional information the word girl that is single we can extract

234
00:17:27.620 --> 00:17:33.560
more information we can extract that the word girl for instance has four letters or that it was capitalized

235
00:17:33.560 --> 00:17:40.910
it wasn't capitalized just as these are all very very intuitive examples that doesn't have to work this

236
00:17:40.910 --> 00:17:43.020
way but this is how it could work.

237
00:17:43.100 --> 00:17:45.500
And so then we have the word girl in here.

238
00:17:45.650 --> 00:17:48.850
And so that's how this world works and this world works.

239
00:17:48.860 --> 00:17:53.700
And so what this world does is it extracts certain information from what you have in the memories.

240
00:17:53.700 --> 00:17:59.600
So for instance if we have now girl in here and for the purposes of the current of the next word or

241
00:17:59.600 --> 00:18:04.230
next sentence that's coming up you might need to know like we saw you need to know the gender.

242
00:18:04.250 --> 00:18:13.730
So then this valve will facilitate the extraction of the gender and that will go as an input into your

243
00:18:13.730 --> 00:18:16.930
next module and it will help the next module.

244
00:18:17.120 --> 00:18:26.060
It will be here decide and decide how to deal with the information that's coming its way how to put

245
00:18:26.060 --> 00:18:30.380
it into the correct form for the female gender for example.

246
00:18:30.380 --> 00:18:33.440
And so that's that's how the output valve works.

247
00:18:33.440 --> 00:18:35.690
And what what it does.

248
00:18:35.690 --> 00:18:36.410
So there we go.

249
00:18:36.410 --> 00:18:44.750
That's how the long short term memory works and I hope this was a quite an intuitive explanation and

250
00:18:45.530 --> 00:18:51.650
now you have a bit of a better understanding what what else dams are like in terms of additional reading.

251
00:18:51.800 --> 00:18:57.720
You could definitely reference the original paper by our two officers who created Atlas dams.

252
00:18:57.980 --> 00:19:02.480
Alternatively if you don't want to get that deep into mathematics and into the technical stuff there

253
00:19:02.480 --> 00:19:06.880
is a great great blog by Christopher flow which we've already mentioned.

254
00:19:06.890 --> 00:19:09.470
Great explanation of LSD Ms.

255
00:19:09.500 --> 00:19:10.880
I highly recommend to check it out.

256
00:19:10.910 --> 00:19:17.030
There's a bit of mathematics not too heavy and there's another blog by she young understanding I asked

257
00:19:17.030 --> 00:19:25.430
him a SDM and it's diagrams diagrams are a bit more in-depth so they're a bit more there there's a bit

258
00:19:25.430 --> 00:19:28.130
more or less space saving but Daniel has been more in depth.

259
00:19:28.130 --> 00:19:34.550
Might be easier to understand in some cases no mathematics whatsoever just plain intuition and so also

260
00:19:35.340 --> 00:19:40.400
highly recommend this blog to check it out if you would like to get a bit of additional information

261
00:19:40.490 --> 00:19:45.570
on the stamps and on that note I hope you enjoyed today's tutorial and I look forward to you next time.

262
00:19:45.680 --> 00:19:47.330
Until then enjoy deep learning.
