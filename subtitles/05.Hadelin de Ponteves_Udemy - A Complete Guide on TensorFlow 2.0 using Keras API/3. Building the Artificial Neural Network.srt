1
00:00:00,390 --> 00:00:01,790
All right let's do this.

2
00:00:01,830 --> 00:00:07,470
Let's build the artificial neural network with tens of low 2.0.

3
00:00:07,470 --> 00:00:08,130
All right.

4
00:00:08,130 --> 00:00:13,830
So first of all the first thing we have to do whenever building an artificial new network which in this

5
00:00:13,890 --> 00:00:20,130
section will be a fully connected neural network is to create the model object itself.

6
00:00:20,310 --> 00:00:25,860
And this object will actually be an instance of a class called a sequential class.

7
00:00:25,860 --> 00:00:33,030
And that's because we are building a fully connected new network which is a series of dance layers and

8
00:00:33,150 --> 00:00:35,830
as opposed to being a computational graph.

9
00:00:35,850 --> 00:00:41,670
Well it is a sequence of layers and that's why we use the sequential class and that sequential class

10
00:00:41,670 --> 00:00:48,350
is taken from the morals module from the caris library that we take from the tensor flow library.

11
00:00:48,360 --> 00:00:48,870
All right.

12
00:00:48,900 --> 00:00:54,150
So we have our moral basically it is initialized as a sequence of layers.

13
00:00:54,210 --> 00:00:59,940
And now we're going to add one by one the different layers that are going to be in this new network.

14
00:00:59,940 --> 00:01:06,240
So once again if you're new to new one that works you absolutely have to close distance only right now

15
00:01:06,270 --> 00:01:13,080
and jump down to the annex parts at the end of the course where we provide the whole theory of artificial

16
00:01:13,080 --> 00:01:14,090
neural networks.

17
00:01:14,100 --> 00:01:18,990
However if you do know about neural networks well you will understand what I'm going to explain in this

18
00:01:18,990 --> 00:01:24,960
tutorial meaning how we're going to add the different layers inside this artificial neural network.

19
00:01:24,960 --> 00:01:25,250
All right.

20
00:01:25,260 --> 00:01:32,040
So let's start with the first one which is a first fully connected hidden layer.

21
00:01:32,040 --> 00:01:32,240
All right.

22
00:01:32,250 --> 00:01:38,460
So I specified here in the text the different hyper parameters there are going to be in this first fully

23
00:01:38,460 --> 00:01:45,450
connected hidden layer and we're gonna have one hundred and twenty eight neurons which are called units

24
00:01:45,480 --> 00:01:46,180
by tensor flow.

25
00:01:46,170 --> 00:01:47,620
You know that's the units here.

26
00:01:47,730 --> 00:01:54,780
We'll get to that very quickly then the activation function you know which breaks the linearity inside

27
00:01:54,780 --> 00:02:01,410
your new networks that indeed your new network can learn the nonlinear relationships that are between

28
00:02:01,680 --> 00:02:07,680
the pixels of the images and the output saying which kind of clothes it is.

29
00:02:07,680 --> 00:02:11,970
You know there is no linear relationship between the pixels and the output.

30
00:02:11,970 --> 00:02:19,710
So we have to break the linearity that happens in the way that some of the inputs and then of the neurons

31
00:02:19,710 --> 00:02:20,880
inside the neural network.

32
00:02:21,000 --> 00:02:26,880
In order for the new network to be able to learn the nonlinear relationships and the activation function

33
00:02:26,910 --> 00:02:31,640
that we're using for this first fully connected hidden layer is the realm.

34
00:02:31,680 --> 00:02:31,980
Right.

35
00:02:31,980 --> 00:02:38,100
Classic one this is most of the time the one we use when building a dense layer a dense low by the way

36
00:02:38,100 --> 00:02:41,160
means nothing other than a fully connected layer.

37
00:02:41,160 --> 00:02:47,100
And for this we used to relook also called direct fire activation function which is of course part of

38
00:02:47,100 --> 00:02:50,740
the annexes in the artificial neural network annex.

39
00:02:50,780 --> 00:02:51,090
Okay.

40
00:02:51,090 --> 00:02:52,440
And then the input shape.

41
00:02:52,470 --> 00:02:54,620
Well you know what input shape is.

42
00:02:54,720 --> 00:03:02,940
We understood that the input of the new network are the single vectors which contain all the pixels

43
00:03:03,030 --> 00:03:06,390
of the images flattened into one dimension.

44
00:03:06,480 --> 00:03:15,120
And that's why the input shape here is indeed a vector of 784 elements corresponding to 784 pixels of

45
00:03:15,210 --> 00:03:16,180
each image.

46
00:03:16,290 --> 00:03:21,510
And the fact that we have a colon here and nothing after just means that indeed this is a vector of

47
00:03:21,510 --> 00:03:24,320
seven hundred and eighty four elements.

48
00:03:24,360 --> 00:03:32,160
And so when we add this first layer Well what we have to do is first take our model object which I remind

49
00:03:32,160 --> 00:03:34,470
is an instance of the sequential class.

50
00:03:34,620 --> 00:03:40,560
And as in every class where we have some method and a method we use to add a new fully connected layer

51
00:03:40,770 --> 00:03:46,950
or any layer for that matter is the add method and in the add method well we enter indeed which kind

52
00:03:46,950 --> 00:03:49,590
of layer we want to add to our model.

53
00:03:49,920 --> 00:03:55,470
So right now we're adding a first fully connected here and there and therefore the class with this is

54
00:03:55,710 --> 00:03:57,210
the dance class.

55
00:03:57,210 --> 00:04:04,500
So actually what is being added exactly is an object of the dance class which represents a first fully

56
00:04:04,500 --> 00:04:05,520
connected hidden layer.

57
00:04:06,000 --> 00:04:10,780
And inside this dance class well we have to specify three important arguments.

58
00:04:10,920 --> 00:04:16,740
The first one is the number of units meaning how many hidden neurons we want to have in this first fully

59
00:04:16,740 --> 00:04:18,330
connected hidden layer.

60
00:04:18,390 --> 00:04:22,240
Then the second one is the activation function to break the narrative.

61
00:04:22,440 --> 00:04:28,680
And as we saw this is the relook or rectify activation function which has the string released with no

62
00:04:28,680 --> 00:04:29,830
capital letters.

63
00:04:29,970 --> 00:04:37,110
And the third argument is the input shape which contains basically the input layer or your input vector

64
00:04:37,320 --> 00:04:39,440
that is fed into the new network.

65
00:04:39,510 --> 00:04:40,010
And right.

66
00:04:40,020 --> 00:04:48,330
Remember this input vector contains the 784 pixels that were all flattened from the to the array of

67
00:04:48,330 --> 00:04:48,900
the image.

68
00:04:49,430 --> 00:04:49,790
OK.

69
00:04:49,800 --> 00:04:51,600
So very simple.

70
00:04:51,750 --> 00:04:54,370
But again you don't have to remember this.

71
00:04:54,540 --> 00:05:00,970
The purpose of this course is just to give you some notebooks that you can keep with close to you on

72
00:05:00,970 --> 00:05:06,130
your machine and that you can use any time you want to build a new network with tons of load 2.0.

73
00:05:06,130 --> 00:05:11,410
For one of your projects so you don't have to remember for example that the dance class has taken from

74
00:05:11,410 --> 00:05:15,690
the layers module from the Cairns library and from the Townsville library.

75
00:05:15,790 --> 00:05:22,300
No you can just copy this and then make sure to change the parameters especially this one for the right

76
00:05:22,390 --> 00:05:25,790
input shape and then you can keep this architecture if you want.

77
00:05:25,820 --> 00:05:30,180
You know there is always big work on finding the optimal architecture.

78
00:05:30,250 --> 00:05:31,900
All right so there you go.

79
00:05:31,900 --> 00:05:36,390
Great stuff with added Our first fully connected hidden layer.

80
00:05:36,430 --> 00:05:38,640
Now moving onto the next one.

81
00:05:38,800 --> 00:05:43,750
What we're going to add now is going to be a second layer with drop out.

82
00:05:43,780 --> 00:05:45,770
So what is dropout dropout.

83
00:05:45,790 --> 00:05:46,800
I specified it here.

84
00:05:46,810 --> 00:05:53,110
I explained everything dropout is a regularization technique where we randomly set neurons in a layer

85
00:05:53,110 --> 00:05:53,770
to zero.

86
00:05:53,770 --> 00:05:59,860
In other words you know you're gonna have a layer inside which some neurons are gonna be deactivated

87
00:06:00,160 --> 00:06:06,070
therefore not learning during back propagation you know their weights won't be updated.

88
00:06:06,100 --> 00:06:10,120
That's why I say here while training those neurons won't be updated.

89
00:06:10,120 --> 00:06:14,950
And so how many neurons are gonna be deactivated and therefore not dated.

90
00:06:14,950 --> 00:06:21,250
Well that's a certain percentage of them which we choose when adding this layer will drop out and usually

91
00:06:21,250 --> 00:06:24,010
which is 20 to 50 percent.

92
00:06:24,010 --> 00:06:30,610
All right so let's do this to add this new layer was true about Well of course we use again the add

93
00:06:30,610 --> 00:06:37,570
method inside which we're going to input what we want to add and a new layer with drop out is simply

94
00:06:37,870 --> 00:06:41,380
given by the drop out class which takes as input.

95
00:06:41,410 --> 00:06:48,030
Exactly this percentage of neurons that we want to deactivate basically that want to drop out.

96
00:06:48,100 --> 00:06:54,820
And so that's why here I specified 0.01 meaning 20 percent of neurons are gonna be deactivated from

97
00:06:54,820 --> 00:06:59,170
time to time of course they're not going to be the same deactivated neurons.

98
00:06:59,170 --> 00:07:05,020
Every iteration you know just 20 percent of them will be activated from time to time to block the learning

99
00:07:05,020 --> 00:07:05,920
process.

100
00:07:05,920 --> 00:07:12,520
And the reason why we do this the reason why we added about is to indeed avoid overfishing because by

101
00:07:12,520 --> 00:07:18,910
preventing some neurons from learning well intuitively we understand that this will prevent from learning

102
00:07:18,910 --> 00:07:19,390
too much.

103
00:07:19,390 --> 00:07:24,800
No learning to close the training set and you know when you learn to learn on the training set you'll

104
00:07:24,810 --> 00:07:26,890
have poor results on the test.

105
00:07:26,890 --> 00:07:28,360
And that's what we want to avoid.

106
00:07:28,540 --> 00:07:30,950
Thanks to this drop out technique.

107
00:07:30,950 --> 00:07:33,670
All right so that's for the second layer was drop out.

108
00:07:33,670 --> 00:07:38,350
Now you know the syntax to add a new layer withdrew drop out with sense of flow.

109
00:07:38,350 --> 00:07:40,410
You understand that it's very easy.

110
00:07:40,510 --> 00:07:46,060
I recommend to include this in your new networks drop out as always very efficient and can improve the

111
00:07:46,060 --> 00:07:48,520
results on the evaluation set.

112
00:07:48,520 --> 00:07:51,880
So just make sure to keep that close to you.

113
00:07:51,880 --> 00:07:57,670
And now we're going to add the output layer which is of course the final layer of your new network that

114
00:07:57,670 --> 00:08:04,440
contains the output meaning that contains the final class that the new network predicts on the image

115
00:08:04,450 --> 00:08:11,070
you know the new network predicts which clothes basically is the input image that was fed to it.

116
00:08:11,260 --> 00:08:18,280
And so well here I specify the info the units argument which is how many neurons you want to have in

117
00:08:18,280 --> 00:08:19,420
your output layer.

118
00:08:19,450 --> 00:08:25,360
It's gonna be ten because we have ten classes and we're actually using the soft Max activation function

119
00:08:25,720 --> 00:08:29,070
in order to get probabilities for each of the class.

120
00:08:29,200 --> 00:08:34,230
And you know the final output the final prediction will be the class that has the highest probability.

121
00:08:34,240 --> 00:08:40,300
So that's why instead of having one neuron in the output layer as the final predicted class well we

122
00:08:40,300 --> 00:08:44,130
have 10 classes with their predicted probabilities.

123
00:08:44,230 --> 00:08:45,450
And so there you go.

124
00:08:45,550 --> 00:08:51,610
That's how you add this final output layer using again the add method and then the dance class.

125
00:08:51,610 --> 00:08:57,130
And that's because you want some full connections between your second layer with about and the output

126
00:08:57,130 --> 00:09:03,190
layer and this dance class takes again as argument unit which is how many neurons you're gonna have

127
00:09:03,190 --> 00:09:09,790
in the output layer meaning ten because we have ten classes and the activation function that will get

128
00:09:09,790 --> 00:09:16,180
you the final output which is set Max meaning that before getting the final output we will return some

129
00:09:16,180 --> 00:09:22,850
probabilities for each of the classes and return the final class that has the highest probability.

130
00:09:22,870 --> 00:09:23,640
All right.

131
00:09:23,680 --> 00:09:24,620
And that's it.

132
00:09:24,700 --> 00:09:29,830
That's how you build the full architecture of the new network intensive flow.

133
00:09:29,890 --> 00:09:36,250
And by having this you know I included enough so that then you can build any other architecture of any

134
00:09:36,250 --> 00:09:41,170
neural network you want because now you know how to add any type of layers.

135
00:09:41,170 --> 00:09:46,030
I'm not talking about the ones we're having a convolution on the one network I'm talking about the layers

136
00:09:46,030 --> 00:09:48,030
and fully connected neural network.

137
00:09:48,100 --> 00:09:54,100
We know how to add a simple first fully connected here and there with the dance class you also know

138
00:09:54,100 --> 00:09:58,990
how to add another layer with drop out and you know how to add the output layer.

139
00:09:59,230 --> 00:10:04,380
So for if you want to add a second fully connected here and there and actually this will be one of the

140
00:10:04,380 --> 00:10:05,490
exercises.

141
00:10:05,490 --> 00:10:12,180
Well you know you just need to use the add method again and input the dance class inside with a certain

142
00:10:12,180 --> 00:10:13,230
number of hidden neurons.

143
00:10:13,230 --> 00:10:18,720
You want to have in this new hidden layer and then I'm giving you an extra and you have to remove this

144
00:10:18,900 --> 00:10:24,780
because you need this is only entered when adding the first fully connected hidden layer therefore connected

145
00:10:24,900 --> 00:10:26,030
to the input layer.

146
00:10:26,100 --> 00:10:26,480
Right.

147
00:10:26,490 --> 00:10:28,980
Otherwise you can just simply include this.
