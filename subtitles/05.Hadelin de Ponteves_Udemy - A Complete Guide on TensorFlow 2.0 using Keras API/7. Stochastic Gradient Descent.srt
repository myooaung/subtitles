1
00:00:01,050 --> 00:00:03,300
Hello and welcome back to of course on deep learning.

2
00:00:03,300 --> 00:00:07,140
Today we're talking about stochastic gradient descent.

3
00:00:07,140 --> 00:00:14,400
Previously we learned about gradient descent and we found out that it is a very efficient method to

4
00:00:14,400 --> 00:00:19,560
solve our optimization problem where we are trying to minimize the cost function.

5
00:00:19,560 --> 00:00:28,980
It basically takes us from 10 to the power of 57 years to solving a problem within minutes or hours

6
00:00:29,460 --> 00:00:30,690
or within a day or so.

7
00:00:31,020 --> 00:00:37,440
And it really helps speed things up because we can see which way is downhill and we can just go in that

8
00:00:37,440 --> 00:00:41,160
direction and take steps and get to the minimum faster.

9
00:00:41,550 --> 00:00:49,950
But the thing with the stick of gradient descent is that this method requires for the class function

10
00:00:49,950 --> 00:00:51,010
to be convex.

11
00:00:51,090 --> 00:00:57,660
And as you can see here we've specifically chosen a convex cost function basically convex means that

12
00:00:58,140 --> 00:01:05,820
the function looks similar to what we are seeing now that it's just convex into one direction and that

13
00:01:05,820 --> 00:01:09,300
it in essence has one global minimum.

14
00:01:09,300 --> 00:01:11,580
And that's the one that we're going to find.

15
00:01:11,580 --> 00:01:13,980
But what if our function is not convex.

16
00:01:13,980 --> 00:01:17,830
What if our cost function is not corrects what if it looks something this.

17
00:01:17,970 --> 00:01:19,800
Well first of all how could that happen.

18
00:01:19,800 --> 00:01:27,900
Well that could happen because if we first of all choose a cost function which is not the squared difference

19
00:01:27,930 --> 00:01:33,770
between white hat and why or if we do choose the cost function which is like that.

20
00:01:33,780 --> 00:01:39,620
But then in a multidimensional space it can actually turn into something that is not convex.

21
00:01:39,720 --> 00:01:45,360
And so what would happen in this case if we just tried to apply our normal gradient descent method something

22
00:01:45,360 --> 00:01:51,180
like this could happen we could find a local minimum of the cost function rather than the global one.

23
00:01:51,180 --> 00:01:53,210
So this one was the best one.

24
00:01:53,220 --> 00:01:54,610
And we found the wrong one.

25
00:01:54,750 --> 00:01:57,650
And therefore we don't have the correct weights.

26
00:01:57,660 --> 00:01:59,970
We don't have an optimized neural network.

27
00:02:00,210 --> 00:02:02,520
We have a subpar neural network.

28
00:02:02,520 --> 00:02:04,620
And so what do we do in this case.

29
00:02:04,620 --> 00:02:10,020
Well the answer here is stochastic gradient descent.

30
00:02:10,020 --> 00:02:15,060
And it turns out the sarcastic gradient descent doesn't require for the cost function to be convex.

31
00:02:15,330 --> 00:02:20,100
So let's have a look at the two differences between the normal gradient descent that we talked about

32
00:02:20,100 --> 00:02:21,400
and the stochastic readings.

33
00:02:21,780 --> 00:02:27,690
So normal gradient descent is when we take all of our rows we plug them into our neural network and

34
00:02:27,690 --> 00:02:33,630
once again here we've got the neural network copied over several times but the rows are being plugged

35
00:02:33,630 --> 00:02:35,940
into that same neural network every time.

36
00:02:35,970 --> 00:02:39,140
So there's only one neural network this is just four viz. purposes.

37
00:02:39,300 --> 00:02:43,350
And then once we've plugged them in we've calculated our cost function based on the forming on the right

38
00:02:43,350 --> 00:02:48,630
and looking at the chart on the at the bottom and then we adjust the weights then this is called the

39
00:02:48,660 --> 00:02:54,410
gradient descent method or it's also the proper term is the batch gradient descent method.

40
00:02:54,420 --> 00:03:01,890
So we take the whole batch of from our sample we apply it and then we run that the stochastic gradient

41
00:03:01,890 --> 00:03:05,900
descent method is a bit different here we take the rows one by one.

42
00:03:05,900 --> 00:03:13,320
So we take this row we run our neural network and then we adjust the weights then we move onto the second

43
00:03:13,320 --> 00:03:18,900
row we take the second row we run our neural network we look at the cost function and then we adjust

44
00:03:18,900 --> 00:03:24,300
the weights again and then we take another 0 take or 3 we run our neural network we look at the cost

45
00:03:24,300 --> 00:03:25,360
function we're just away.

46
00:03:25,380 --> 00:03:32,610
So basically we're looking at we're adjusting the weights after every single row rather than doing everything

47
00:03:32,610 --> 00:03:38,640
together and then adjusting weights to different approaches and now we're going to just compare the

48
00:03:38,640 --> 00:03:39,660
two side by side.

49
00:03:39,660 --> 00:03:40,560
So here they are.

50
00:03:40,560 --> 00:03:42,860
This is how to visually remember them.

51
00:03:42,870 --> 00:03:49,440
So you've got the batch gradient descent where you are adjusting the weights after you've run them after

52
00:03:49,440 --> 00:03:55,230
you've run all of the rows in your neural network and then basically you just the weights and you run

53
00:03:55,230 --> 00:03:59,670
the whole thing again iteration iteration iteration in the grip stochastic gradient descent method you

54
00:03:59,670 --> 00:04:05,160
run one row at a time and you are just the weights you are just a way to adjust the weights and then

55
00:04:05,160 --> 00:04:11,000
you do everything again and again and that is called discussing gradient descent method.

56
00:04:11,160 --> 00:04:19,170
The main two differences are that the stochastic gradient descent method helps you avoid the problem

57
00:04:19,230 --> 00:04:28,940
where you find those local extremes or local minimums rather than the overall overall global minimum.

58
00:04:28,980 --> 00:04:34,770
And the reason for that in simple terms is that the as GDR or the stochastic gradient descent method

59
00:04:35,100 --> 00:04:38,150
has much higher fluctuations because it can afford them.

60
00:04:38,160 --> 00:04:43,620
It's doing one iteration or one row at a time and therefore the fluctuations are much higher and it

61
00:04:43,620 --> 00:04:49,390
is much more likely to find the global minimum rather than just the local minimum.

62
00:04:49,410 --> 00:04:54,630
And the other thing about the stochastic gradient descent nothing compared to the batch gradient is

63
00:04:55,530 --> 00:04:59,930
that is faster like the first impression that you might have is because it's doing every single roll

64
00:04:59,930 --> 00:05:04,800
one at a time it is slower but actually in fact it is faster because it is.

65
00:05:04,910 --> 00:05:12,580
It doesn't have to load up all the data into memory and run and wait until all those rows are un altogether.

66
00:05:12,650 --> 00:05:16,760
You can just roll around them one by one so it's a much later algorithm is much faster in that sense

67
00:05:16,760 --> 00:05:24,770
so though it has way more in that's in those senses it has more advantages over the batch gradient descent

68
00:05:24,770 --> 00:05:25,360
method.

69
00:05:25,370 --> 00:05:30,920
The main advantage of all of the main kind of like pro for the batch gradient descent method is that

70
00:05:30,920 --> 00:05:35,300
it is a deterministic algorithm or other than stochastic gradient descent.

71
00:05:35,390 --> 00:05:41,390
Being a sarcastic algorithm meaning it's random and the batch gradient descent method as long as you

72
00:05:41,390 --> 00:05:47,570
have the same starting weights for your neural network every time you run the batch gradient descent

73
00:05:47,570 --> 00:05:55,680
method you will get the same iterations the same results for yo for the way your weights are being updated.

74
00:05:55,820 --> 00:06:00,770
For us to have for the stochastic gradient descent method you won't get that because it is a stochastic

75
00:06:00,770 --> 00:06:07,580
method you are picking your rows possibly at random and you are updating your neural network in a sarcastic

76
00:06:07,580 --> 00:06:12,950
manner and therefore you're just going to every single time you run the sarcastic gradient descent method

77
00:06:12,980 --> 00:06:18,920
even if you have the same weights at the start you're going to have a different process and different

78
00:06:19,100 --> 00:06:20,550
iterations to get there.

79
00:06:20,720 --> 00:06:28,040
So that's in a nutshell what's to cast a gradient descent is also there's a method in between the tool

80
00:06:28,040 --> 00:06:34,730
called the Mini batch gradient descent method where you combine the two new basically run rather than

81
00:06:34,730 --> 00:06:41,110
running a whole batch or running one at a time you run batches of rows maybe five 10 100 of them in

82
00:06:41,120 --> 00:06:46,460
euros you decide to set you run those that number of rows at a time then you update your weights and

83
00:06:46,460 --> 00:06:50,330
you obliterates and so on and that's called the Mini batch gradient descent method.

84
00:06:50,510 --> 00:06:56,270
If you'd like to learn more about gradient descent there's a great article which you can have a look

85
00:06:56,270 --> 00:06:56,590
at.

86
00:06:56,600 --> 00:07:00,920
It's called a neural network in 13 lines of Python part 2.

87
00:07:00,920 --> 00:07:03,660
Gradient descent by Andrew trust.

88
00:07:04,520 --> 00:07:08,670
And the links below it's on GitHub 2015 article.

89
00:07:09,530 --> 00:07:12,680
Very well-written very in very simple terms.

90
00:07:12,860 --> 00:07:21,440
It's got some interesting philosophical or just interesting thoughts on how to apply green descent.

91
00:07:21,440 --> 00:07:28,280
What are the advantages and disadvantages and how to be how to do things in certain situations so he's

92
00:07:28,280 --> 00:07:31,310
got some very cool tips tricks and hacks.

93
00:07:31,310 --> 00:07:33,570
Very easy read so definitely check that out.

94
00:07:33,740 --> 00:07:36,890
And another one a bit more heavier read.

95
00:07:36,950 --> 00:07:42,260
For those of you who are into mathematics who want to get to the bottom of the mathematics why gradient

96
00:07:42,260 --> 00:07:44,290
descent is that specific.

97
00:07:45,230 --> 00:07:49,150
What are the formulas that are driving greenness and how is it calculate and so on.

98
00:07:49,160 --> 00:07:51,570
Check out the article or actually the book.

99
00:07:51,570 --> 00:07:58,040
It's a free online book called neural networks and deep learning by Michael Nielsen 2015 book it's just

100
00:07:58,040 --> 00:08:03,010
basically it's all online you can go ahead and check it out there and there.

101
00:08:03,020 --> 00:08:05,800
Again very soft introduction to the mathematics.

102
00:08:05,810 --> 00:08:07,030
But then for a mother.

103
00:08:07,250 --> 00:08:13,270
But the mathematics are pretty heavy as you go along as you read through the article.

104
00:08:13,550 --> 00:08:17,290
But at the same time it gets you into into that movie.

105
00:08:17,300 --> 00:08:22,310
I think you mean it has like a warm up chapter where you first warm up with the math and then you jump

106
00:08:22,310 --> 00:08:22,590
into them.

107
00:08:22,610 --> 00:08:27,160
So interested in math then this is the article to go to and there we go.

108
00:08:27,160 --> 00:08:29,120
So that's in a nutshell.

109
00:08:29,120 --> 00:08:38,000
The difference between green descent and stochastic gradient descent and how to work and on that note

110
00:08:38,000 --> 00:08:39,850
we're going to wrap up today Central.

111
00:08:39,860 --> 00:08:41,990
I look forward to seeing you on the next one.

112
00:08:41,990 --> 00:08:44,030
And until then enjoy deep learning.
