WEBVTT
1
00:00:00.450 --> 00:00:01.750
Welcome back.

2
00:00:01.800 --> 00:00:07.230
In this lesson we're going to talk about one of the key inputs to the gradient descent function.

3
00:00:07.230 --> 00:00:09.100
And this is the learning rate.

4
00:00:09.150 --> 00:00:12.490
So let's add a big section heading in a markdown cell.

5
00:00:12.540 --> 00:00:21.930
So when I click on this cell here changed from code to markdown at one pound symbol and then write the

6
00:00:21.930 --> 00:00:22.680
learning rate

7
00:00:25.720 --> 00:00:29.170
scrolling back up to where we've defined our gradient descent function.

8
00:00:29.170 --> 00:00:35.500
Let's take another look at our inputs previously with modified our Python code so that the inputs would

9
00:00:35.500 --> 00:00:43.090
include not only the derivative function and the initial guess but also a multiplier precision.

10
00:00:43.090 --> 00:00:46.360
And the maximum number of iterations.

11
00:00:46.360 --> 00:00:54.640
So far we've changed up the initial guesses and analyzed the impact that this had on the algorithm in

12
00:00:54.640 --> 00:00:58.530
different situations on different cost functions.

13
00:00:58.900 --> 00:01:03.690
But we've not really messed with the multiplier and this is what we're gonna do now.

14
00:01:03.700 --> 00:01:08.440
So what does the learning rate actually do in our algorithm.

15
00:01:08.440 --> 00:01:15.370
If we look at our update step namely this line right here we can see that our learning rate which I've

16
00:01:15.370 --> 00:01:23.260
called multiplier is multiplied with the gradient the gradient is the value of the slope of the cost

17
00:01:23.260 --> 00:01:25.530
function and the multiplier.

18
00:01:25.690 --> 00:01:32.590
It's just a constant but together they determine how big of a step we take.

19
00:01:32.590 --> 00:01:38.040
So if you remember with the gradient when the slope was steep then the gradient was a large number.

20
00:01:38.140 --> 00:01:39.960
And we take a big step.

21
00:01:40.060 --> 00:01:44.860
And similarly if the multiplier is launch we also take a big step.

22
00:01:44.920 --> 00:01:50.920
So at the moment we've got the multiplier set to a default value of zero point zero two.

23
00:01:50.920 --> 00:01:56.590
Now this might be a good time to maybe pause the video and think about what would happen if we picked

24
00:01:56.590 --> 00:02:03.220
a different value say what would happen if this value was really really small and what would happen

25
00:02:03.640 --> 00:02:07.430
if the multiplier value was really really large.

26
00:02:07.440 --> 00:02:12.170
Now I'm going to illustrate the effect of the multiplier taking our second example.

27
00:02:12.240 --> 00:02:14.230
This was the G of X function.

28
00:02:14.460 --> 00:02:16.790
I'm going to take this cell right here.

29
00:02:16.800 --> 00:02:23.420
This was the cell that generated our graphs and plotted our scatter plot for our gradient descent.

30
00:02:23.430 --> 00:02:30.570
On that you have X function going to copy this cell right here with the shortcut and a scroll all the

31
00:02:30.570 --> 00:02:32.590
way down to our learning rate.

32
00:02:32.970 --> 00:02:36.240
And I'm going to paste the cell below.

33
00:02:36.480 --> 00:02:41.430
I probably don't need the cell up here right at the moment so I'm just going to take this one and move

34
00:02:41.430 --> 00:02:44.370
it up and I'm gonna modify it a little bit.

35
00:02:44.370 --> 00:02:48.870
I'm gonna add some print statements below our charts at the bottom.

36
00:02:48.870 --> 00:02:52.770
The first print statement I'm going to add is the number of steps

37
00:02:57.960 --> 00:03:04.670
and if you recall this was the length of this list the length of list underscore x.

38
00:03:04.950 --> 00:03:07.800
So we'll write alien parentheses.

39
00:03:07.800 --> 00:03:09.610
List underscore X..

40
00:03:09.660 --> 00:03:13.170
The other thing that I want to modify is the initial guest that we've got.

41
00:03:13.550 --> 00:03:17.240
So I'm going to change this to one point nine.

42
00:03:17.550 --> 00:03:19.320
And I'm also going to add a multiplier here.

43
00:03:19.830 --> 00:03:26.610
So when I add multiplier equals I'm not gonna change the default value just yet.

44
00:03:26.610 --> 00:03:29.210
Let's just have it at zero point zero two.

45
00:03:29.400 --> 00:03:35.490
And since I haven't run my notebook in a little while I'm actually going to go to cell and run all instead

46
00:03:35.490 --> 00:03:42.640
of running just my latest cell and then I won't have to scroll all the way down to the bottom and there's

47
00:03:42.650 --> 00:03:43.370
our output.

48
00:03:43.370 --> 00:03:44.720
So it works as expected.

49
00:03:44.720 --> 00:03:50.690
So if I start at one point nine which is right here then the first step that we take is fairly large

50
00:03:51.020 --> 00:03:58.340
right here and then we slowly move down to our minimum and at the moment all it takes is 14 steps to

51
00:03:58.340 --> 00:03:59.580
get to the bottom.

52
00:03:59.730 --> 00:04:00.190
Okay.

53
00:04:00.230 --> 00:04:02.480
So this works as expected.

54
00:04:02.480 --> 00:04:04.130
Now let's have a little bit more fun with this.

55
00:04:04.490 --> 00:04:09.020
So when to change our function call to have a maximum of five iterations.

56
00:04:09.020 --> 00:04:16.040
So only one hour loop to run five times and then I'm also gonna change our multiplier from zero point

57
00:04:16.040 --> 00:04:19.690
zero two to zero point to five.

58
00:04:20.030 --> 00:04:24.100
And then I want to rerun the cell and then we can have a look at our chart.

59
00:04:24.200 --> 00:04:26.100
So what are we seeing here now.

60
00:04:26.250 --> 00:04:32.820
Our multiplier is increasing our step size and we see our algorithm bouncing around on this function.

61
00:04:32.930 --> 00:04:36.740
So this is very very different behavior from what we've seen before.

62
00:04:36.890 --> 00:04:38.680
But let's take this to an extreme.

63
00:04:38.720 --> 00:04:44.990
I'm going to scroll back up to our function call and change the maximum number of times that our loop

64
00:04:44.990 --> 00:04:54.740
will run from five to 500 and we hit shift into and scrolling down we see this the whole chart is turning

65
00:04:54.740 --> 00:04:55.370
red.

66
00:04:55.370 --> 00:05:01.790
Yeah we're bouncing around all over the place but our algorithm is never converging.

67
00:05:01.910 --> 00:05:07.130
In fact our loop at this point has run 500 times.

68
00:05:07.130 --> 00:05:11.940
In contrast we were at what 14 earlier in the number of steps.

69
00:05:12.020 --> 00:05:13.900
So this is very interesting right.

70
00:05:13.910 --> 00:05:16.010
What can we learn from this example.

71
00:05:16.310 --> 00:05:22.940
In almost every situation that we've illustrated previously we've converged between like 20 or 60 steps

72
00:05:22.940 --> 00:05:31.280
remember this time our loop ran 500 times and the algorithm still didn't find the minimum.

73
00:05:31.310 --> 00:05:38.000
So what we're seeing here is that if we're not careful we can get into a situation where our algorithm

74
00:05:38.270 --> 00:05:44.900
isn't converging and it just might continue going and going and going and this kind of brings us back

75
00:05:44.900 --> 00:05:51.350
to our discussion about Python for loops and Python while loops.

76
00:05:51.350 --> 00:05:56.580
Let me scroll back up to where we wrote that code.

77
00:05:56.590 --> 00:05:58.140
Here we go.

78
00:05:58.170 --> 00:06:05.280
Now you might remember that with a while loop you as the developer have to take extra care with your

79
00:06:05.280 --> 00:06:06.910
terminating condition.

80
00:06:07.200 --> 00:06:09.040
What's the terminating condition.

81
00:06:09.180 --> 00:06:13.950
It's whatever follows the while keyword right here.

82
00:06:13.950 --> 00:06:19.770
So if the logic in this terminating condition isn't formulated well then it's very easy to get into

83
00:06:19.770 --> 00:06:26.460
a scenario where you're never exiting the loop and then your python program accidentally ends up in

84
00:06:26.460 --> 00:06:27.990
an infinite loop.

85
00:06:27.990 --> 00:06:35.040
In other words with a Y loop you as the programmer need to think of the corner cases and include the

86
00:06:35.040 --> 00:06:39.070
logic to stop your loop from running longer than you intended to.

87
00:06:40.020 --> 00:06:44.790
I know in this example if you're looking at this code this seems really really trivial but infinite

88
00:06:44.790 --> 00:06:48.090
loops happen maybe more often than you expect.

89
00:06:48.090 --> 00:06:49.680
Let me show you an example.

90
00:06:49.920 --> 00:06:55.040
We could have written our gradient descent function with a Y loop instead of a for loop.

91
00:06:55.050 --> 00:07:01.560
This is our gradient descent function as it currently stands with a for loop and a cutoff point determined

92
00:07:01.770 --> 00:07:04.850
by the maximum number of iterations.

93
00:07:04.950 --> 00:07:10.730
And this is how one could imagine running our gradient descent with a Y loop.

94
00:07:10.920 --> 00:07:14.710
At first glance this Y loop would seem to make sense.

95
00:07:14.790 --> 00:07:21.840
The condition for running the loop is run the loop as long as the step size is greater than the precision.

96
00:07:21.990 --> 00:07:27.420
The step size is always the difference between the new X and the previous X..

97
00:07:27.540 --> 00:07:34.320
So at some point that calculation would get more and more precise at which point we would exit loop.

98
00:07:34.320 --> 00:07:41.040
Fair enough but this is exactly the kind of code that risks running into the infinite loop problem because

99
00:07:41.280 --> 00:07:48.930
in the case where one isn't converging and the step sizes between the values get larger this loop just

100
00:07:48.930 --> 00:07:50.060
continues running.

101
00:07:50.880 --> 00:07:57.750
So I'm not going to keep this cell around negative edit delete cell and this is why the for loop provides

102
00:07:57.750 --> 00:07:59.900
quite a quite a big contrast right.

103
00:07:59.910 --> 00:08:07.020
It's got this safety net by forcing you to explicitly state the number of times it will run ahead of

104
00:08:07.020 --> 00:08:08.340
time.

105
00:08:08.340 --> 00:08:11.540
Now let's crank up that learning rate even more.

106
00:08:11.940 --> 00:08:21.660
Scroll back down here and I'm going to increase our learning rate from zero point to five to zero point

107
00:08:22.140 --> 00:08:24.970
three when I leave everything else the same.

108
00:08:25.200 --> 00:08:26.430
And rerun this thing.

109
00:08:26.670 --> 00:08:29.060
See what happens.

110
00:08:29.880 --> 00:08:34.440
This is our old friend the overflow era.

111
00:08:34.450 --> 00:08:36.510
The result is too large.

112
00:08:36.520 --> 00:08:43.540
We've shut off to infinity and beyond what this little exercise is showing us is how there's another

113
00:08:43.540 --> 00:08:48.190
quirk that we have to be aware of with our optimization algorithms.

114
00:08:48.280 --> 00:08:53.470
We at the machine learning experts have to choose an appropriate learning rate.

115
00:08:54.250 --> 00:08:59.890
So that begs the question how do we know what the right learning rate actually is.

116
00:08:59.890 --> 00:09:01.480
Because that's what we've seen.

117
00:09:01.540 --> 00:09:06.850
If we pick a very large learning rate then our algorithm doesn't converge.

118
00:09:06.970 --> 00:09:12.340
And if we pick a very very small learning rate then our algorithm might actually take forever.

119
00:09:12.430 --> 00:09:13.840
What I mean by forever.

120
00:09:14.080 --> 00:09:14.650
Let me show you.

121
00:09:15.430 --> 00:09:21.760
I want to scroll back up where we've called our function when change this again to something that doesn't

122
00:09:21.760 --> 00:09:30.510
crash our python program so zero point zero or two and then I gonna copy this bit of code right here

123
00:09:33.250 --> 00:09:42.650
so that's the code where we're calling our function and the code were plotting our first shot.

124
00:09:42.820 --> 00:09:43.960
No I don't like seeing that error.

125
00:09:43.990 --> 00:09:50.210
So I'm going to rerun the cell and then I'm going to paste our code down here.

126
00:09:50.260 --> 00:09:51.860
Now I'm going to change this comment here.

127
00:09:51.880 --> 00:09:58.760
I'm going to call it run gradient descent three times.

128
00:09:59.710 --> 00:10:01.630
But before we run it three times.

129
00:10:01.720 --> 00:10:03.100
Let's run it at one time.

130
00:10:03.160 --> 00:10:03.570
So.

131
00:10:04.350 --> 00:10:05.600
So here's what I'm going to do.

132
00:10:05.890 --> 00:10:12.990
Instead of having the max iteration set to 500 here I'm going to set this equal to a variable.

133
00:10:13.030 --> 00:10:20.980
So going to say and is equal to one hundred and then instead of having a max iterations set to 500 and

134
00:10:20.980 --> 00:10:27.010
then set it equal to and it's going to specify the maximum number of iterations up here.

135
00:10:27.880 --> 00:10:40.750
Then I'm going to change the multiplier from zero point zero two to zero point zero zero zero five and

136
00:10:40.750 --> 00:10:46.660
then I'm going to add a precision I'm going to make it quite precise the calculation in the C precision

137
00:10:46.660 --> 00:10:56.080
should be equal to zero point zero zero zero one comma and then have the maximum number of iterations

138
00:10:56.680 --> 00:11:01.670
back here and for the initial guess we'll start off at three.

139
00:11:01.690 --> 00:11:05.280
So this is gonna be our first call to the gradient descent function.

140
00:11:06.280 --> 00:11:15.490
But instead of having this sequence unpacking code here we're gonna store all of this information in

141
00:11:15.520 --> 00:11:17.350
a single tuple.

142
00:11:17.350 --> 00:11:22.250
So this is gonna be called low Gamma Gamma.

143
00:11:22.300 --> 00:11:23.960
It's often used for learning rate.

144
00:11:24.130 --> 00:11:27.390
So we'll call our tuple low Gamma.

145
00:11:27.400 --> 00:11:36.550
Now let's change up the code for our Plot to Change the comment here to the plotting reduction in cost

146
00:11:37.510 --> 00:11:40.450
for each iteration.

147
00:11:40.450 --> 00:11:44.410
So this is gonna be our goal in terms of the figure size.

148
00:11:44.410 --> 00:11:45.730
We're gonna go with a single plot.

149
00:11:45.820 --> 00:11:53.680
So I'm going to size this plot differently from before to make it 20 by 10 it's going to be quite large.

150
00:11:55.240 --> 00:12:01.120
And then when it delete this subplot code here we don't need that.

151
00:12:01.390 --> 00:12:06.330
And then four axes we're gonna go on the y axis.

152
00:12:06.340 --> 00:12:10.940
We're gonna go from zero to 50.

153
00:12:11.050 --> 00:12:12.480
So this is gonna be our cost.

154
00:12:12.490 --> 00:12:14.610
Cost is gonna be in the y axis.

155
00:12:14.650 --> 00:12:20.880
See this in a bit and our x axis is gonna go from zero to the number of iterations.

156
00:12:20.980 --> 00:12:28.370
So it's gonna go from zero to n and it's gonna be a hundred gonna have the iterations on our x axis.

157
00:12:28.370 --> 00:12:36.350
So our x axis is gonna go from zero to n the title of the chart is gonna be the effect of the learning

158
00:12:36.350 --> 00:12:42.360
rate and for the X label will have the number of iterations.

159
00:12:42.360 --> 00:12:50.550
And for the Y label we're gonna have the cost now we need the data to populate our charts so we need

160
00:12:50.550 --> 00:12:55.590
the values for our charts and we need two things.

161
00:12:55.590 --> 00:13:00.660
The first thing is going to be what we have on our y axis.

162
00:13:00.660 --> 00:13:02.570
That's going to require us to.

163
00:13:04.020 --> 00:13:14.580
And that's going to require us to convert the the lists to name pi arrays.

164
00:13:14.640 --> 00:13:20.880
Reason being we can feed an array into our geophysics function but we cannot feed a list into our G

165
00:13:20.880 --> 00:13:22.120
of X function.

166
00:13:22.170 --> 00:13:24.130
So we've done this previously.

167
00:13:24.180 --> 00:13:36.290
I'm gonna call our first array low values set that equal to NPR Ray low gamma.

168
00:13:37.070 --> 00:13:41.840
And it was the second item in our tuple.

169
00:13:41.840 --> 00:13:43.580
Why did I just put a one there.

170
00:13:43.790 --> 00:13:49.090
Because the second item is at index 1 because we start counting from zero.

171
00:13:49.130 --> 00:13:50.980
The first item is at index 0.

172
00:13:50.990 --> 00:13:53.290
The second item is that index 1.

173
00:13:54.320 --> 00:13:54.590
Okay.

174
00:13:54.620 --> 00:13:56.710
So that's our y axis data.

175
00:13:56.750 --> 00:14:02.570
Time to get our x axis data.

176
00:14:02.600 --> 00:14:06.850
Now we just need our x axis to go from 0 to like 100.

177
00:14:06.860 --> 00:14:07.850
Right.

178
00:14:08.180 --> 00:14:18.590
So too is when I create a list from 0 2 and plus 1 y and plus 1 because we've got that extra initial

179
00:14:18.590 --> 00:14:19.070
guess.

180
00:14:19.160 --> 00:14:24.320
So even though our loop is running run 100 times our extra guests it's going to mean we should have

181
00:14:25.100 --> 00:14:26.990
100 plus one.

182
00:14:27.050 --> 00:14:34.340
And when a store has information in a variable called iteration on the score list.

183
00:14:34.340 --> 00:14:37.230
So how do we create a list in the past.

184
00:14:37.280 --> 00:14:44.930
Let me scroll down here a little bit in the past we've had our square brackets and we said 0 1 2 3.

185
00:14:44.930 --> 00:14:45.110
Right.

186
00:14:45.110 --> 00:14:48.560
We just populated that list with values.

187
00:14:48.560 --> 00:14:55.700
But this isn't what we're gonna do because we're not going to type out 100 different values in this

188
00:14:55.700 --> 00:14:56.120
list.

189
00:14:56.210 --> 00:15:00.230
Instead of creating this list manually what we're going to do instead is going to make use of this range

190
00:15:00.230 --> 00:15:05.240
function that we solve the for loop our range has a starting value and an ending value.

191
00:15:05.270 --> 00:15:12.620
So it's going to create all our values from 0 to and plus 1 but we can't do it like this.

192
00:15:12.620 --> 00:15:13.550
Exactly.

193
00:15:13.670 --> 00:15:22.460
The reason is is that if I press shift tab on this function I can see that this thing here will actually

194
00:15:22.460 --> 00:15:25.300
spit out a Range object.

195
00:15:25.310 --> 00:15:30.310
So this range function will give us a Range object but what we need is a list.

196
00:15:30.350 --> 00:15:34.010
So how do we convert a Range object to a list.

197
00:15:34.010 --> 00:15:45.600
Well we can call the list function and then next the call to the range function inside our call to the

198
00:15:45.600 --> 00:15:46.740
list function.

199
00:15:46.740 --> 00:15:51.080
So now we'll have a list starting from zero going to end plus 1.

200
00:15:51.450 --> 00:15:56.710
That's gonna be stored inside our variable called iteration list.

201
00:15:57.240 --> 00:16:02.970
And this is what we're going to take and we're gonna put that right here on our plot.

202
00:16:02.970 --> 00:16:11.850
So when I put that here and then for our y axis on this plot we're gonna use our well lo underscore

203
00:16:12.480 --> 00:16:18.550
values we're going to use our array of values for our cost function.

204
00:16:19.440 --> 00:16:21.750
And we also don't have to stick to the blue color.

205
00:16:21.750 --> 00:16:27.530
There's a lot of colors available including like light green for example and we can make the line of

206
00:16:27.530 --> 00:16:28.890
it thicker.

207
00:16:29.340 --> 00:16:34.130
Change the line with from three to five and we're going to get rid of the Alpha as well.

208
00:16:34.630 --> 00:16:39.810
Now I'm going to come on out the plot dots scattered code and just show our plot for a change.

209
00:16:39.810 --> 00:16:48.730
So I'm going to say peel teed off show parentheses at the end and hit shift enter to see what we get.

210
00:16:49.210 --> 00:16:53.920
Okay so we get a nice line plot right here.

211
00:16:53.920 --> 00:16:55.370
Just like this.

212
00:16:55.390 --> 00:16:58.360
So this is cool but you know what.

213
00:16:58.540 --> 00:17:02.920
We're gonna take our scatter functionality and use it as well.

214
00:17:02.920 --> 00:17:08.590
That week we get a little bubble each time our loop was run so that way we can see the step size a bit

215
00:17:08.590 --> 00:17:09.960
more clearly.

216
00:17:10.000 --> 00:17:18.460
So our x axis is going to be the iteration on the score list again and for our y axis it was going to

217
00:17:18.460 --> 00:17:24.030
be our low underscore values array.

218
00:17:24.030 --> 00:17:29.920
When I change it to the same color as with a hard line plot.

219
00:17:29.920 --> 00:17:32.720
So it's gonna be color light green.

220
00:17:33.000 --> 00:17:39.520
And for the DOT size we'll change it from 100 to 80 and get rid of the Alpha as well.

221
00:17:39.550 --> 00:17:40.770
So let me rerun this.

222
00:17:40.760 --> 00:17:43.500
See what it looks like huh.

223
00:17:43.540 --> 00:17:44.330
This is pretty good.

224
00:17:44.410 --> 00:17:51.860
So large a step size in the beginning getting smaller until the steps are very very close at the end.

225
00:17:51.880 --> 00:18:00.810
So what we're looking at here is the decrease in the cost with each iteration of our loop.

226
00:18:00.830 --> 00:18:07.950
I think this is really really neat but it'll be even neat if we can plot two different learning rates

227
00:18:08.040 --> 00:18:14.930
or three different learning rates on the same chart next to each other and see how they compare to scroll

228
00:18:14.930 --> 00:18:24.170
back up here and I'm going to correct my typo in the title to have effect of learning rate with a space

229
00:18:24.650 --> 00:18:27.700
I'm going to add a little comment here as well.

230
00:18:27.730 --> 00:18:36.440
I going to say plotting low learning rates and then I'm going to scroll back up to the beginning and

231
00:18:36.440 --> 00:18:40.770
put my cursor right here.

232
00:18:41.060 --> 00:18:47.420
Now as a challenge can you plot two more learning rates on our chart.

233
00:18:47.420 --> 00:18:56.840
So pause the video add a tuple called mid gamma and keep all the inputs to the call to the gradient

234
00:18:56.840 --> 00:19:03.800
descent function the same but change the multiplier to double the learning rate to zero point zero zero

235
00:19:03.800 --> 00:19:13.410
one and then create another tuple called high gamma and change the learning rate there to zero point

236
00:19:13.530 --> 00:19:15.240
zero zero two.

237
00:19:15.240 --> 00:19:21.930
Now you're gonna have to extract the values from the tuple that you get back and throw those onto our

238
00:19:21.930 --> 00:19:22.930
chart.

239
00:19:22.990 --> 00:19:33.450
I'll give you a few seconds to pause the video and give this a try and here's the solution so the quickest

240
00:19:33.450 --> 00:19:43.440
way to do this is to copy this code here paste it two times change the name of the tuple to mid gamma

241
00:19:43.530 --> 00:19:50.340
change the name of this tuple to high gamma and then this multiplier is gonna be zero point zero zero

242
00:19:50.340 --> 00:19:51.000
one.

243
00:19:51.150 --> 00:19:58.620
This multiplier is gonna be zero point zero zero to and then we're to come down here.

244
00:19:58.620 --> 00:20:01.400
Take this code right here.

245
00:20:01.440 --> 00:20:12.960
Copy it paste it twice and it changed my comment plotting mid learning rate plotting high learning rate

246
00:20:13.860 --> 00:20:21.210
and then instead of feeding the low values into our array here I want to make a call to NPR rate and

247
00:20:21.210 --> 00:20:24.500
then put an R tuple MIT gamma

248
00:20:29.020 --> 00:20:30.430
I'm going to take this.

249
00:20:30.430 --> 00:20:31.700
Copy it.

250
00:20:32.230 --> 00:20:33.790
Put it here as well.

251
00:20:33.880 --> 00:20:35.620
MIT Gamma One.

252
00:20:35.890 --> 00:20:36.850
And this is going to be

253
00:20:40.710 --> 00:20:41.850
NPR Ray.

254
00:20:42.000 --> 00:20:46.060
Hi Gamma 1 and same with this one.

255
00:20:46.090 --> 00:20:53.210
It's gonna be NPR a high gamma 1 but one thing that I'm gonna change as well is the color.

256
00:20:53.440 --> 00:20:57.090
I don't want all of these graphs to have the very very same color.

257
00:20:57.100 --> 00:20:59.990
Otherwise I can't tell them apart from the chart.

258
00:21:00.250 --> 00:21:06.590
One nice color to contrast the light green is steel blue.

259
00:21:06.730 --> 00:21:10.120
This is gonna be for.

260
00:21:10.350 --> 00:21:16.320
This is gonna be for the slightly higher learning rate and then for the highest learning rate on this

261
00:21:16.320 --> 00:21:17.140
chart.

262
00:21:17.190 --> 00:21:20.460
Pick my favorite color hot pink.

263
00:21:20.460 --> 00:21:21.130
Why not.

264
00:21:23.890 --> 00:21:24.480
Tarragon.

265
00:21:24.480 --> 00:21:26.020
I'm not making these color names up.

266
00:21:26.030 --> 00:21:26.190
Yeah.

267
00:21:26.210 --> 00:21:31.320
I haven't taking them from the map plot lib official documentation.

268
00:21:31.320 --> 00:21:37.120
So these are the names that I can put into that argument.

269
00:21:37.380 --> 00:21:43.970
So there's a predefined number of names that you can use and spelling has to match exactly.

270
00:21:43.970 --> 00:21:45.700
Otherwise they won't recognize them.

271
00:21:45.830 --> 00:21:47.740
Case I've got my chance now.

272
00:21:47.780 --> 00:21:50.290
The proof is in the pudding as they say.

273
00:21:50.370 --> 00:21:54.720
Hit shift enter and let's see what we get.

274
00:21:54.720 --> 00:22:00.840
So I think this chart is beautiful because it illustrates so nicely how our learning a trade affects

275
00:22:00.840 --> 00:22:02.200
our algorithm.

276
00:22:02.280 --> 00:22:07.410
We've run our gradient descent three separate times with different learning rates and now we can see

277
00:22:07.410 --> 00:22:14.610
how the cost decreases with each iteration of the loop and what we're seeing here is that the highest

278
00:22:14.610 --> 00:22:18.870
learning rate the one in hot pink converges the fastest.

279
00:22:18.870 --> 00:22:23.650
In other words the higher the multiplier the faster the convergence.

280
00:22:23.650 --> 00:22:29.910
And in contrast we've got the really low multiplier converging very very slowly to the minimum.

281
00:22:29.970 --> 00:22:36.090
Now if we've picked an even lower multiplier then it would reach the minimum even more slowly but as

282
00:22:36.090 --> 00:22:41.940
we've shown before the higher multiplier is only better up to a point right.

283
00:22:41.940 --> 00:22:46.110
Remember how in our earlier example our entire graph was red.

284
00:22:46.410 --> 00:22:52.920
Clearly having like a really high multiplier you either get an overflow error or we don't converge on

285
00:22:52.920 --> 00:22:53.660
our minimum.

286
00:22:53.700 --> 00:22:57.180
So higher multipliers don't always work out.

287
00:22:57.390 --> 00:23:04.640
In fact let's plot this crazy behavior that we had early on on this chart as well so when we go up here

288
00:23:05.420 --> 00:23:17.240
and had a common to call it experiment and then I'm going to take my Python code copy it pasted here

289
00:23:17.810 --> 00:23:21.690
and maybe call this insane Gamma.

290
00:23:21.860 --> 00:23:26.510
Now my initial guess was one point nine and an early example.

291
00:23:26.510 --> 00:23:33.180
And the multiplier was zero point to five if I remember correctly.

292
00:23:33.200 --> 00:23:42.520
Now all I need to do is come down here take this bit of code here paste it and I want to modify it so

293
00:23:42.550 --> 00:23:48.520
read plotting insane loading rate and this is gonna be r in sane gamma

294
00:23:51.810 --> 00:23:52.380
hand.

295
00:23:52.410 --> 00:24:00.990
In terms of color let's go for just good old red to uh show that this is not a good thing.

296
00:24:01.250 --> 00:24:05.420
How can head shift enter and see how that behaves.

297
00:24:05.440 --> 00:24:07.220
So what have we got.

298
00:24:07.250 --> 00:24:11.520
Whew that looks that looks really bad.

299
00:24:11.540 --> 00:24:17.840
So in their fourth example we started out much closer to the minimum.

300
00:24:17.890 --> 00:24:24.760
We started out with an initial guess of one point nine and that's compared to the initial guess of three

301
00:24:25.210 --> 00:24:28.520
which is where we started out with the other functions.

302
00:24:28.540 --> 00:24:35.950
So even though on our initial guess was actually much better and our cost was much lower it doesn't

303
00:24:35.950 --> 00:24:42.280
actually help us when our learning rate is all screwed up so we can see here that our cost doesn't actually

304
00:24:42.280 --> 00:24:43.280
come down.

305
00:24:43.410 --> 00:24:49.990
He even though that algorithm has been running 100 times we just have our cost bouncing around decreasing

306
00:24:50.020 --> 00:24:58.210
increasing decreasing again and in the end after a hundred iterations we can see that our cost here

307
00:24:58.330 --> 00:25:05.440
at the end is actually much higher than what the cost is for all the other examples.

308
00:25:05.440 --> 00:25:10.910
So all the other learning rates after a hundred iterations had a lower cost.

309
00:25:10.950 --> 00:25:17.390
Even then are really really high multiplier of zero point two five.

310
00:25:17.410 --> 00:25:22.750
So I think that illustrates the problem really nicely when the algorithm is bouncing around with no

311
00:25:22.750 --> 00:25:29.950
clear direction and I think a takeaway from this exercise is that picking the right learning rate is

312
00:25:29.950 --> 00:25:36.970
both a bit of an art and a science and so it shouldn't come really as a surprise that there isn't really

313
00:25:36.970 --> 00:25:41.460
one perfect solution for picking a good learning rate either.

314
00:25:41.470 --> 00:25:46.480
So even if you go to the literature on machine learning you'll find that there are different approaches

315
00:25:46.660 --> 00:25:48.910
for picking a good learning rate.

316
00:25:49.100 --> 00:25:54.340
Now I've talked a little bit about two of the things that this optimization algorithm is sensitive to

317
00:25:54.880 --> 00:25:56.020
these two knobs right.

318
00:25:56.020 --> 00:26:01.690
That we had that we could turn the initial guess and the learning rate and you might come away thinking

319
00:26:01.690 --> 00:26:09.850
that oh you know gradient descent it's you know it's a bad algorithm because it can have certain problems.

320
00:26:09.850 --> 00:26:16.750
And yet you know it it has pros and cons but one of the really really big advantages of gradient descent

321
00:26:17.350 --> 00:26:25.840
is that it is incredibly simple is an incredibly simple algorithm and is also quite fast so it's quite

322
00:26:25.840 --> 00:26:31.390
a fast thing to run to train your machine learning model and that's an advantage that not every other

323
00:26:31.390 --> 00:26:34.600
competing algorithm can claim for themselves.

324
00:26:35.290 --> 00:26:41.920
So given its relative simplicity and speed you can actually try out different learning rates for your

325
00:26:41.920 --> 00:26:45.890
cost function and see what works see what works best.

326
00:26:46.120 --> 00:26:50.630
But of course there's more elegant approaches to picking a learning rate as well.

327
00:26:50.650 --> 00:26:58.570
For example we could adjust our learning rate while the algorithm runs meaning the learning rate doesn't

328
00:26:58.570 --> 00:27:00.140
have to be fixed.

329
00:27:00.340 --> 00:27:05.170
The learning rate could chop and change after each step in our loop.

330
00:27:05.740 --> 00:27:12.550
So why would we do that much or ask why would we have a learning rate that's not fixed for a particular

331
00:27:13.030 --> 00:27:13.720
value.

332
00:27:13.720 --> 00:27:19.410
Why would you want to update the learning rate every time the loop runs well.

333
00:27:19.480 --> 00:27:26.950
So the idea behind that is that the further you are from an optimal value the faster you should move

334
00:27:26.980 --> 00:27:35.300
towards that minimum and thus the ideal value of your learning rate should be larger but on the other

335
00:27:35.300 --> 00:27:41.900
hand once you start getting closer and closer to that minimum then the learning rate should come down

336
00:27:41.900 --> 00:27:48.240
as well because you don't want to overshoot and this is why some machine learning practitioners create

337
00:27:48.270 --> 00:27:52.340
like a predefined schedule for the learning rate ahead of time.

338
00:27:52.650 --> 00:27:58.520
And in the schedule the learning rate starts off large and then gradually gets smaller but there is

339
00:27:58.520 --> 00:28:02.990
a whole host of other techniques as well that people are using.

340
00:28:02.990 --> 00:28:09.900
For example one quite simple technique is called the bold driver and it works like this.

341
00:28:09.920 --> 00:28:11.660
If your error rate.

342
00:28:11.740 --> 00:28:15.780
Yeah your cost was reduced since the last iteration.

343
00:28:15.970 --> 00:28:19.730
Then you can try increasing the learning rate by 5 percent.

344
00:28:20.870 --> 00:28:27.500
And if your error rate was in fact increased meaning that you skipped over the optimal point then you

345
00:28:27.500 --> 00:28:34.010
should reset the values of your parameters to the values of the previous iteration and decrease the

346
00:28:34.010 --> 00:28:41.660
learning rate by 50 percent so you can see with this kind of rule how the learning rate would change

347
00:28:41.980 --> 00:28:44.000
increased by 5 percent.

348
00:28:44.000 --> 00:28:50.510
If you had a reduction in your error rate and go back one step and decrease the learning rate by 50

349
00:28:50.510 --> 00:28:51.240
percent.

350
00:28:51.380 --> 00:28:54.560
If you had an increase in your cost.

351
00:28:54.700 --> 00:29:01.220
Okay well so this was quite a theoretical lesson and we've covered quite a bit we've covered how our

352
00:29:01.220 --> 00:29:04.670
algorithm is sensitive both to the initial guests and the learning rate.

353
00:29:05.210 --> 00:29:13.040
And now we can start to tackle some more complicated cost functions in particular.

354
00:29:13.040 --> 00:29:17.510
So far we've only been working with estimating a single value right.

355
00:29:18.140 --> 00:29:23.370
When you look at geophysics there's only one thing there is hex right.

356
00:29:23.480 --> 00:29:25.540
But this is only one dimension.

357
00:29:25.550 --> 00:29:31.940
Let's turn our attention to how we can tackle two dimensions in our gradient descent algorithm and then

358
00:29:31.940 --> 00:29:36.280
you'll see how you can tackle more than two very easily.

359
00:29:36.290 --> 00:29:38.400
I'll see you in the next lesson.

360
00:29:38.420 --> 00:29:39.410
Have a good one.
