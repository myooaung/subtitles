WEBVTT
1
00:00:00.270 --> 00:00:07.500
In the previous lessons we've imported our Boston house price data set into our Jupiter notebook.

2
00:00:07.500 --> 00:00:15.260
Now that we've successfully gathered our data it is time to take a good hard look at it.

3
00:00:15.620 --> 00:00:18.090
And this is really the next step in our workflow.

4
00:00:18.090 --> 00:00:20.490
We've formulated our question.

5
00:00:20.490 --> 00:00:23.040
We've gathered our data.

6
00:00:23.100 --> 00:00:26.750
Now it's time to explore our data set in depth.

7
00:00:27.030 --> 00:00:32.940
And oftentimes we're going to be exploring visualizing and cleaning this data more or less at the same

8
00:00:32.940 --> 00:00:41.100
time simply because the problems that the data has only becomes apparent after you start digging into

9
00:00:41.100 --> 00:00:43.480
the data set okay.

10
00:00:43.500 --> 00:00:50.650
So imagine that you're back at your real estate job and that the office intern has just plonked down

11
00:00:50.650 --> 00:00:54.150
a big data set on your desk.

12
00:00:54.160 --> 00:00:59.680
What are the first things that you'd want to understand about a fresh dataset.

13
00:00:59.770 --> 00:01:05.690
What's a good starting point to get to grips with a data set that you've never seen before.

14
00:01:05.710 --> 00:01:10.560
What are the kind of questions that you'd want to ask when you're first starting out with your work.

15
00:01:10.570 --> 00:01:12.430
Let me show you my own starting point.

16
00:01:12.610 --> 00:01:17.680
Let me show you the first six questions that I ask myself when I first start working with a new dataset

17
00:01:17.950 --> 00:01:20.280
that I haven't seen before.

18
00:01:20.320 --> 00:01:24.310
The first question I'm going to ask myself is Why does the data come from.

19
00:01:24.310 --> 00:01:26.940
What's the source of the data.

20
00:01:27.080 --> 00:01:32.880
The second question is can I find some sort of short description of what's in the data set.

21
00:01:33.080 --> 00:01:39.680
And this is important for understanding the all important context under which the data was collected

22
00:01:40.070 --> 00:01:43.600
and also how the data was collected.

23
00:01:43.610 --> 00:01:47.060
Third how big is the data set actually.

24
00:01:47.090 --> 00:01:51.740
How many individual data points are there in the data set.

25
00:01:51.740 --> 00:01:56.060
Am I dealing with an enormous data set or a small one.

26
00:01:56.150 --> 00:02:01.880
This is important from a practical point because working with a dataset with 10 million data points

27
00:02:01.880 --> 00:02:07.910
will require very different techniques than working with a data set of say 10 data points.

28
00:02:07.910 --> 00:02:14.210
For starters my aging laptop will totally struggle to crunch like a huge data set so it's important

29
00:02:14.210 --> 00:02:17.360
to figure out what sort of beast you're gonna be dealing with.

30
00:02:17.360 --> 00:02:23.810
But this aspect of data sets size it's not just important from a practical point of view but it's also

31
00:02:23.810 --> 00:02:30.080
important from a theoretical perspective because many statistical tests that you're going to be using

32
00:02:30.350 --> 00:02:39.320
will become a lot more powerful as the sample size increases the fourth question is how many features

33
00:02:39.470 --> 00:02:41.630
are there in the dataset.

34
00:02:41.720 --> 00:02:50.750
Now what I mean by features well for each data point how many aspects were measured how many entries

35
00:02:50.840 --> 00:02:58.250
are there for each row in the table how many columns all the this is what I mean by features you and

36
00:02:58.250 --> 00:03:01.390
I are going to be looking at house prices shortly.

37
00:03:01.400 --> 00:03:04.760
And each house is gonna be in a row in this dataset.

38
00:03:04.970 --> 00:03:12.200
The number of features will tell us how much information we have about each House the number of features

39
00:03:12.230 --> 00:03:17.660
will help us figure out how many characteristics we're going to be basing our prediction of the house

40
00:03:17.660 --> 00:03:24.170
value on the next two questions we're going to ask ourselves are what are the names of the features

41
00:03:24.530 --> 00:03:28.100
and what are the descriptions of each feature.

42
00:03:28.100 --> 00:03:35.030
The reason these questions are very crucial is because we need to understand what the dataset is actually

43
00:03:35.030 --> 00:03:36.150
measuring.

44
00:03:36.320 --> 00:03:42.710
So sometimes you'll get datasets with pretty unintuitive measurements so it's important to kind of dig

45
00:03:42.710 --> 00:03:48.380
in and understand what exactly is contained in the data.

46
00:03:48.380 --> 00:03:53.750
For starters you'll probably want to check the units that are being used in each column.

47
00:03:53.780 --> 00:04:00.800
So for example is our price given in dollars or in thousands of dollars.

48
00:04:01.200 --> 00:04:05.090
And these are just some of the basics to get right okay.

49
00:04:05.130 --> 00:04:13.810
So now that we've got our To Do list let's return to the Python code and see if we can answer these

50
00:04:13.900 --> 00:04:18.040
initial questions and check them off one by one.

51
00:04:18.180 --> 00:04:27.750
There is a handy little python function called DIY R that we can use to look at a Python objects attributes.

52
00:04:27.750 --> 00:04:36.960
Check it out Diaw parentheses Boston underscored dataset and shift enter.

53
00:04:37.350 --> 00:04:39.940
Well bring up the following output.

54
00:04:40.220 --> 00:04:48.990
They'll show us a list of attributes what we're looking at here is actually a list of attributes for

55
00:04:48.990 --> 00:04:57.160
this python object the first attribute here is a shorthand for I'm guessing description.

56
00:04:57.250 --> 00:04:59.280
So let's pull this out.

57
00:04:59.290 --> 00:05:00.380
Let's print this out.

58
00:05:00.580 --> 00:05:12.610
Once a print Boston on a school data sent dot D E S C are all caps and let's take a look at what we

59
00:05:12.610 --> 00:05:13.180
see here.

60
00:05:14.800 --> 00:05:16.470
So using this python attribute.

61
00:05:16.600 --> 00:05:21.450
We do indeed get a description of this dataset.

62
00:05:21.460 --> 00:05:28.210
This description was included in the python object if we were able to access it with this attribute

63
00:05:28.450 --> 00:05:31.070
that we discovered through the desire.

64
00:05:31.090 --> 00:05:35.570
Function OK so let's take a look at these notes.

65
00:05:35.870 --> 00:05:43.940
We've already seen that there are five hundred and six instances or rows in this dataset and there are

66
00:05:43.940 --> 00:05:50.000
13 attributes 13 categories or columns.

67
00:05:50.030 --> 00:05:53.530
Now these 13 columns are as follows.

68
00:05:53.810 --> 00:05:55.760
We've got per capita crime.

69
00:05:55.940 --> 00:06:02.050
We've got concentration of nitric oxides so this is a proxy for pollution.

70
00:06:02.120 --> 00:06:06.410
We've got the average number of rooms per dwelling.

71
00:06:06.410 --> 00:06:10.570
We've got a pupil teacher ratio by town.

72
00:06:10.760 --> 00:06:17.910
So this is a proxy for the quality of the schools and a whole bunch of other attributes.

73
00:06:18.410 --> 00:06:24.770
Scrolling down a little more we see that the two researchers that collated this dataset are Harrison

74
00:06:24.860 --> 00:06:30.270
and Rubenfeld and that this is based on a research paper.

75
00:06:30.710 --> 00:06:35.210
In fact we can actually see the title of the original research paper here.

76
00:06:35.210 --> 00:06:43.670
Hedonic prices and the demand for clean air and this was published in The Journal of environment economics

77
00:06:43.760 --> 00:06:49.040
and management Vol. 5 in 1978.

78
00:06:49.040 --> 00:06:54.540
So this already answers a lot of the initial questions about our dataset.

79
00:06:54.680 --> 00:07:00.500
It's actually fairly interesting that the original purpose of the researchers was to figure out how

80
00:07:00.830 --> 00:07:04.080
high the demand was for clean air in Boston.

81
00:07:04.130 --> 00:07:06.440
That's what they were trying to accomplish.

82
00:07:06.500 --> 00:07:12.890
They were trying to figure out how much more people are willing to pay to be able to breathe clean air

83
00:07:12.980 --> 00:07:15.080
in the city.

84
00:07:15.080 --> 00:07:22.820
The other thing that's important is that this housing data dates back to 1978 and we're working with

85
00:07:23.000 --> 00:07:31.760
five hundred and six different entries so let's check off the questions on our list.

86
00:07:31.800 --> 00:07:35.210
We've figured out the source of the data.

87
00:07:35.340 --> 00:07:39.430
We've read a brief description of the data sent.

88
00:07:39.480 --> 00:07:44.240
We've also managed to figure out the number of data points in the dataset which was five hundred and

89
00:07:44.240 --> 00:07:49.480
six and the number of features which was 13.

90
00:07:49.830 --> 00:07:53.770
And lucky for us the descriptions of the features was also given.

91
00:07:53.790 --> 00:07:59.400
They call them attributes if you remember and the names of these attributes were given in all caps they

92
00:07:59.400 --> 00:08:06.660
were abbreviations and finally we had a very brief description of all the features as well.

93
00:08:07.370 --> 00:08:10.080
So I think for starters this is pretty good going.

94
00:08:10.100 --> 00:08:17.570
But let's crack on how as an aside if you're curious on how this dataset was originally used you can

95
00:08:17.570 --> 00:08:23.240
actually pull up the original research paper that is mentioned in the description.

96
00:08:23.240 --> 00:08:30.680
So I just googled it and I got sent to the University of Michigan library Web site and then I was able

97
00:08:30.680 --> 00:08:35.170
to pull up the PDA for the original paper for free.

98
00:08:35.480 --> 00:08:42.080
And I think that's because Daniel Rubenfeld was actually at the University of Michigan while his co-author

99
00:08:42.260 --> 00:08:45.820
David Harrison was at Harvard at the time.

100
00:08:45.830 --> 00:08:53.570
Now if you Google this as well let me show you how you can embed a link in your Jupiter notebook very

101
00:08:53.570 --> 00:08:55.080
very easily.

102
00:08:55.080 --> 00:08:57.410
So I can copy the euro here.

103
00:08:57.410 --> 00:09:04.480
Go back to my Jupiter notebook and then going into one of the markdown cells say the gathered data cell

104
00:09:04.480 --> 00:09:15.070
I've got here I can use some square brackets and some parentheses to insert my your URL so the euro

105
00:09:15.490 --> 00:09:17.980
goes between the two parentheses.

106
00:09:17.980 --> 00:09:19.260
So I'm gonna paste it in here.

107
00:09:19.270 --> 00:09:25.630
This is the euro that I copied from the other tab in my browser and then in the square brackets I can

108
00:09:25.630 --> 00:09:31.760
include the text that I want to display instead of this long and unwieldy your.

109
00:09:31.880 --> 00:09:41.030
So I want to write source original research paper and when I hit shift enter it's gonna be displayed

110
00:09:41.240 --> 00:09:47.930
like so and this is an active link that we have now in our Jupiter notebook.

111
00:09:47.990 --> 00:09:54.440
Now you might not always get a nice description along with your data set like this.

112
00:09:54.440 --> 00:10:00.230
So let's have a think about how we might look at the number of data points and the number of features

113
00:10:00.530 --> 00:10:07.870
manually in case it wasn't presented to us on a silver plate like this going down to the bottom.

114
00:10:07.910 --> 00:10:15.140
I'm going to insert another markdown cell and when I had a subheading here and this is going to read

115
00:10:16.160 --> 00:10:25.550
data points and features to look at the number of features in this dataset I'm going to first access

116
00:10:25.820 --> 00:10:29.330
the bunch objects data attribute.

117
00:10:29.330 --> 00:10:30.690
I remember seeing this up above.

118
00:10:30.800 --> 00:10:38.960
When we use that the IRR function so we can write Boston underscore data set to hot data.

119
00:10:39.980 --> 00:10:46.610
And this is what this would look like from the output we can see that it's an array and we could verify

120
00:10:46.610 --> 00:10:54.250
this by writing type and then surrounding Boston on a score dataset dot data by parentheses.

121
00:10:54.320 --> 00:10:59.510
And here we can see that it is in fact a name pi and dimensional array.

122
00:10:59.510 --> 00:11:06.140
This is the type of object that we would be accessing like so now if we wanted to see the number of

123
00:11:06.140 --> 00:11:16.160
rows and columns the easiest way to do this is by writing Boston on a score dataset set data dot shape

124
00:11:16.940 --> 00:11:21.460
shape is an attribute of a number pi array.

125
00:11:21.470 --> 00:11:31.310
If you recall so hitting shift into we can see that this array has 506 rows and 13 columns which is

126
00:11:31.310 --> 00:11:36.680
good because it ties out we have the documentation that we read earlier.

127
00:11:36.920 --> 00:11:43.840
The thing that you'll also notice in this line of Python code is that we are changing our attributes.

128
00:11:43.940 --> 00:11:47.310
I'll just add this as a commentator on the right hand side.

129
00:11:47.330 --> 00:11:55.130
Now this is something quite important to understand because this is a good example that shows how objects

130
00:11:55.400 --> 00:12:00.820
and data can be nested inside one other scrolling to the very top.

131
00:12:00.830 --> 00:12:08.030
We see that when we are working with the Boston underscored dataset we are in fact working with an object

132
00:12:08.480 --> 00:12:18.300
of type bunch and this bunch has a number of attributes including that data attribute this data attribute

133
00:12:18.870 --> 00:12:27.440
is in fact an object of type Endy array a num PI N dimensional array and the n dimensional array in

134
00:12:27.440 --> 00:12:32.950
turn also has attributes including a shape attribute.

135
00:12:33.330 --> 00:12:42.820
And when we call on an NDE arrays shape attribute we get back a tuple so when you see this dot notation

136
00:12:43.060 --> 00:12:48.670
being used to chain things together you can think of it almost like a Russian material Suga doll where

137
00:12:48.670 --> 00:12:54.580
each doll contains another object and if that's not your kind of thing then think of it maybe like the

138
00:12:54.580 --> 00:13:01.940
movie Inception or you had dreams within dreams just with less gunfire and on that note I'll see you

139
00:13:01.940 --> 00:13:02.740
on the next lesson.
