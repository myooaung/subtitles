WEBVTT
1
00:00:00.550 --> 00:00:06.640
In this lesson what we're gonna do is going to look at our regression coefficients are theta is in more

2
00:00:06.670 --> 00:00:08.110
detail.

3
00:00:08.110 --> 00:00:13.900
So far we've discussed how to interpret the value of our coefficients in both the original linear model

4
00:00:14.170 --> 00:00:15.900
and the log linear model.

5
00:00:16.030 --> 00:00:21.670
After our data transformations the thing is if we're looking to predict house prices we're not just

6
00:00:21.670 --> 00:00:25.120
going to be interested in the sign and the size of our coefficients.

7
00:00:25.120 --> 00:00:31.030
We're also going to be interested in their significance just because there's a number next to a particular

8
00:00:31.030 --> 00:00:35.770
feature does not mean that this feature has much explanatory power.

9
00:00:36.010 --> 00:00:41.180
Just because there's a number here doesn't mean that this feature is actually significant.

10
00:00:41.200 --> 00:00:46.780
Remember how we talked about how it doctor checks your vital stats of regression coefficients vital

11
00:00:46.780 --> 00:00:49.350
stat that tells you about its significance.

12
00:00:49.360 --> 00:00:51.900
It's called the p value.

13
00:00:51.900 --> 00:00:58.540
Now most academic research papers that you come across analyze the significance of their findings using

14
00:00:58.540 --> 00:01:01.000
this metric of p values.

15
00:01:01.000 --> 00:01:03.420
And here's how this metric is typically used.

16
00:01:03.580 --> 00:01:11.770
If the p value is less than a certain threshold that threshold being zero point zero five then the result

17
00:01:12.040 --> 00:01:20.350
is deemed statistically significant and when the p value is greater than zero point zero five then this

18
00:01:20.350 --> 00:01:25.040
result is considered to be not statistically significant.

19
00:01:25.060 --> 00:01:30.130
This threshold of zero point zero five is kind of where the consensus amongst academics is regarding

20
00:01:30.130 --> 00:01:36.730
significance and for better or was there's a little bit of a cult around this particular value when

21
00:01:36.730 --> 00:01:39.170
it comes to calculating the p values.

22
00:01:39.430 --> 00:01:44.800
The psychic learns linear regression model isn't actually much help.

23
00:01:45.460 --> 00:01:51.580
So what we're gonna do is we're going to look beyond the cycle learn module to calculate a hard statistics

24
00:01:51.610 --> 00:01:53.210
for our regression.

25
00:01:53.410 --> 00:01:58.660
We're gonna go beyond the simple linear regression provided by our machine learning module in these

26
00:01:58.660 --> 00:02:02.830
lessons we're gonna be looking at some detailed statistics of our module.

27
00:02:02.830 --> 00:02:08.380
And this is why we're gonna be importing a different python module to take us further here and this

28
00:02:08.380 --> 00:02:11.470
python module is called stats model.

29
00:02:11.470 --> 00:02:19.600
Now let's add a section heading somewhere to change itself to markdown and put a section heading here

30
00:02:19.630 --> 00:02:30.740
that reads p values and evaluating coefficients what we're gonna be doing next is we're gonna be using

31
00:02:30.740 --> 00:02:38.330
the stats model module to run our linear regression and we will run it so that we get the same results

32
00:02:38.420 --> 00:02:40.580
as we would with psychic learn.

33
00:02:40.580 --> 00:02:46.490
However we will be able to use this python stats model module to pull up detailed statistics that we

34
00:02:46.490 --> 00:02:49.640
can't easily get with cyclone.

35
00:02:49.760 --> 00:02:54.240
The first thing that we need to do of course is import our stats model.

36
00:02:54.350 --> 00:03:03.110
So we're gonna go to the top of our notebook and in our first cell we're gonna say import stats models

37
00:03:03.470 --> 00:03:13.190
dot API as s and then I'm going to hit shift enter on the cell and what I see when I do this is there

38
00:03:13.190 --> 00:03:15.960
is a deprecation warning here.

39
00:03:16.190 --> 00:03:22.820
This deprecation warning refers to my stats models module and what it's saying is that at the time of

40
00:03:22.820 --> 00:03:31.880
recording this module is still using a component that is outdated so it's using this Panda's data tools

41
00:03:31.880 --> 00:03:39.510
component which is outdated or deprecated now I'm really not too concerned about this for two reasons.

42
00:03:39.510 --> 00:03:46.350
One is that we're not gonna be using any functionality to do with dates from the stats models API and

43
00:03:46.380 --> 00:03:53.880
two is that these stats models API will also be updated by the people who created it and what they will

44
00:03:53.880 --> 00:04:00.360
do is they will make sure that it doesn't break and that it's maintained in good working order.

45
00:04:00.360 --> 00:04:05.170
So by the time you're running this you may or may not see this deprecation warning.

46
00:04:05.190 --> 00:04:07.930
Now let's run our regression with this new module.

47
00:04:07.980 --> 00:04:14.040
The thing to note is that in order to make our regression tie out with socket learn we're going to have

48
00:04:14.040 --> 00:04:19.950
to add an intercept because as you can see there is an intercept here from our regression with cycle

49
00:04:19.950 --> 00:04:20.340
line.

50
00:04:21.360 --> 00:04:29.130
So what I'm going to do is take our features from the training data set and add an intercept cell right

51
00:04:29.460 --> 00:04:40.530
S.M. dot and underscore constant parentheses x on the school train and what I'll do is I'll store this

52
00:04:40.530 --> 00:04:43.320
modified data frame in a new variable.

53
00:04:43.320 --> 00:04:54.240
So what I'm gonna say is X on the score include constant equals S.M. don't add on the score constant

54
00:04:54.270 --> 00:04:56.700
parentheses x on a score train.

55
00:04:57.780 --> 00:05:04.350
Now what we can do is call on the stats models over less function which will give us back a model object

56
00:05:04.710 --> 00:05:07.930
which we can then use to fit our regression.

57
00:05:07.950 --> 00:05:18.990
So I want to see model is equal to S m dot or less parentheses y on a school train comma X on a score

58
00:05:19.350 --> 00:05:22.650
include on a score constant.

59
00:05:23.490 --> 00:05:31.410
What we're doing here is we're calling the old s function o Atlas stands for ordinary least squares

60
00:05:32.160 --> 00:05:34.050
and just like Sackett learn.

61
00:05:34.050 --> 00:05:39.570
This gives us a linear regression model which we're storing here has arguments.

62
00:05:39.570 --> 00:05:46.890
We've provided our target values and our features and these features include this constant that we've

63
00:05:46.890 --> 00:05:47.890
added.

64
00:05:48.150 --> 00:05:54.660
Now we can use the stats models API to fit our regression fitting our regression will give us some results.

65
00:05:54.780 --> 00:06:02.400
So I'll create a new variable called results and set that equal to model don't fit.

66
00:06:02.520 --> 00:06:09.680
In other words calling the fit method on the model will return to us our regression results.

67
00:06:09.750 --> 00:06:12.470
So the question is how do we take a look at these results.

68
00:06:12.510 --> 00:06:18.110
For starters let's see if we can print out the coefficients like we have here to get these coefficients.

69
00:06:18.150 --> 00:06:21.730
We'll use the results parameters attribute.

70
00:06:21.810 --> 00:06:28.230
So results dot grams and shift and to show us the coefficients.

71
00:06:28.230 --> 00:06:36.010
So here they are and these values tie out with what we saw in cyclone comment this out.

72
00:06:36.050 --> 00:06:41.840
And now we can take a look at the p values that I keep harping on about to show these.

73
00:06:41.870 --> 00:06:50.000
We also use our results object put a dot after it and access the p values attribute some results like

74
00:06:50.000 --> 00:06:56.990
p values and shift into will show us the p values for all r coefficients.

75
00:06:57.020 --> 00:07:01.610
I don't think this is formatted particularly nicely so when want to do is I'm going to comment this

76
00:07:01.610 --> 00:07:07.490
out again and both of these two series into a data frame.

77
00:07:07.490 --> 00:07:22.880
So with PD dot data frame parentheses curly braces single quotes QF colon results dot Grams comma single

78
00:07:22.880 --> 00:07:30.320
quotes p value colon results dot p values.

79
00:07:30.320 --> 00:07:35.600
We can look at our coefficients and their p values formatted nicely side by side.

80
00:07:35.600 --> 00:07:37.780
Check it out.

81
00:07:39.290 --> 00:07:41.720
Now this is starting to look pretty good but you know what.

82
00:07:41.900 --> 00:07:46.670
I found these p values really really hard to read in scientific notation.

83
00:07:46.670 --> 00:07:51.050
So when I want to do this I'm going around the I'm going to come up here where I'm creating our data

84
00:07:51.050 --> 00:07:59.210
frame and when I called the round function so round open parentheses closing parentheses before the

85
00:07:59.210 --> 00:08:06.630
curly brace and then comma and then a number of decimals that we should round to.

86
00:08:06.680 --> 00:08:11.160
So I'm going to go with 3 and refresh our output.

87
00:08:11.160 --> 00:08:14.050
Now let's talk about how to interpret these results.

88
00:08:14.190 --> 00:08:22.170
Well remember the rule of thumb that any p value over zero point zero five is not significant.

89
00:08:22.670 --> 00:08:32.040
In our case two of our features failed this test namely the Indus feature and our age feature.

90
00:08:32.270 --> 00:08:36.320
These two features do not appear to add much additional information.

91
00:08:37.250 --> 00:08:41.660
All the others are indeed statistically significant.

92
00:08:41.660 --> 00:08:49.490
Now let's make a note of this for later because maybe maybe we could remove the Indus and the H features

93
00:08:49.670 --> 00:08:52.790
from our model in the next lesson.

94
00:08:52.790 --> 00:08:56.420
We're going to discuss a potential problem in our regression.

95
00:08:56.420 --> 00:09:00.870
Remember how we had high correlations between our features in the next lesson.

96
00:09:00.920 --> 00:09:06.820
We're going to check formally if our regression suffers from the problem of multicolored the charity.
