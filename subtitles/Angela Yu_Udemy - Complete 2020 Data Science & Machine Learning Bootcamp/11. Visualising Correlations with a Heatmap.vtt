WEBVTT
1
00:00:00.890 --> 00:00:08.180
In the meantime let's do a little bit of work in our Python code to make this table a little bit more

2
00:00:08.180 --> 00:00:09.110
clear.

3
00:00:09.170 --> 00:00:18.130
Let's visualize our correlations in a way that we could put into a really snazzy report and to do this.

4
00:00:18.190 --> 00:00:24.610
We're going to represent our correlations as a triangle instead of this whole table here.

5
00:00:24.640 --> 00:00:29.560
We don't need to show all these duplicate values showing all these duplicate values doesn't really add

6
00:00:29.560 --> 00:00:30.090
anything.

7
00:00:30.100 --> 00:00:33.710
And it just makes the whole thing look really really busy.

8
00:00:33.730 --> 00:00:42.160
So my goal is to hide half of this table and to accomplish this I will create an array which will help

9
00:00:42.160 --> 00:00:47.850
me filter out the values that I don't want to show and the values that I want to show.

10
00:00:48.020 --> 00:00:55.090
I'm going to call this filter array mask and I'm going to set it equal to an array that's identical

11
00:00:55.300 --> 00:01:00.000
in size to this table of correlations.

12
00:01:00.010 --> 00:01:02.710
This correlation matrix that we've got up here.

13
00:01:03.160 --> 00:01:07.740
The module that I will use to help me do this is called num pi.

14
00:01:07.750 --> 00:01:13.740
I'm going to have to add this to my notebook imports at the top in order to use it.

15
00:01:13.830 --> 00:01:25.300
So when I see import num Pi has an P had shift enter on this import this module scroll back down here

16
00:01:26.110 --> 00:01:35.640
and then I'm going to use the zeros like function from the NUM pi module so right end p dot zeros on

17
00:01:35.640 --> 00:01:43.300
a school like and this function will create an array of zeros that is like whatever array is passed

18
00:01:43.690 --> 00:01:51.550
into this function as a parameter in our case that's gonna be the return value from calling the correlation

19
00:01:51.550 --> 00:01:54.790
method on our data frame.

20
00:01:54.790 --> 00:02:01.330
So let's have a look at what this mask array looks like at the moment let's shift into here and we can

21
00:02:01.330 --> 00:02:07.790
see that we have an array of well just zeros.

22
00:02:07.930 --> 00:02:14.070
Now I need to make another modification to filter on the values in the top triangle.

23
00:02:14.080 --> 00:02:19.940
First each know the indices of these cells in my array.

24
00:02:20.020 --> 00:02:25.020
Thankfully there is another number pi function that will help me find these.

25
00:02:25.020 --> 00:02:34.000
So going to say triangle underscore indices which is going to hold on to all the indices in the top

26
00:02:34.000 --> 00:02:45.340
triangle of my array and then set that equal to num pi dot p r i you try to underscore indices on the

27
00:02:45.560 --> 00:02:56.550
score from and then when I pass in my mask this will retrieve the indices for the top triangle of the

28
00:02:56.670 --> 00:02:57.780
array.

29
00:02:57.780 --> 00:03:07.090
And now that I've got my indices I can use my mascara to select just those cells and change their values.

30
00:03:07.290 --> 00:03:09.030
So as a mask.

31
00:03:09.030 --> 00:03:19.280
Square brackets and then triangle underscore indices and set those equal to true.

32
00:03:19.360 --> 00:03:21.720
Let me show you what our filter looks like now.

33
00:03:21.740 --> 00:03:31.230
So when I say mask it shift enter and then you can see here that the top triangle in this array has

34
00:03:31.230 --> 00:03:36.680
the value 1 and the bottom triangle has the value 0.

35
00:03:36.930 --> 00:03:46.080
And that's because true is mapped to the value 1 and False is mapped to an American value of 0 so with

36
00:03:46.080 --> 00:03:54.140
this in hand I can now move on to creating this beautiful visualization that I keep talking about.

37
00:03:54.240 --> 00:04:00.300
We're gonna use our old friends Seabourn and map plot lib to accomplish this.

38
00:04:00.460 --> 00:04:04.320
The first thing I want to do is I'm going to set the size of our figure.

39
00:04:04.370 --> 00:04:05.610
So I'm going to say PDT.

40
00:04:05.670 --> 00:04:15.940
Don't figure parentheses fixed size is equal to 16 by 10.

41
00:04:15.940 --> 00:04:23.010
And then we use our C Bourne's heat map function to generate a heat map of our correlations.

42
00:04:23.080 --> 00:04:30.580
We had the seaborne modulus S.A. and then I could get a dot after it and write heat map and then within

43
00:04:30.580 --> 00:04:35.220
the parentheses I'm going to provide our correlations.

44
00:04:35.230 --> 00:04:42.590
So this was the value returned by calling the core method on our data frame.

45
00:04:42.610 --> 00:04:50.230
So when I leave it like this as an as thought a heat map parentheses data dot core and then I'm going

46
00:04:50.230 --> 00:04:52.400
to show our plot with PDT.

47
00:04:52.660 --> 00:04:57.860
Don't show let me hit shift into to see what this looks like.

48
00:04:57.890 --> 00:05:00.220
Voila look at that.

49
00:05:00.290 --> 00:05:02.000
We're almost there.

50
00:05:02.120 --> 00:05:08.900
What we can see already is that the different colors show us a strong positive correlations have a dark

51
00:05:09.080 --> 00:05:15.140
red color and the negative strong correlations have a dark blue color.

52
00:05:15.320 --> 00:05:22.280
Anything that's close to zero is pale or white to this color scheme is actually conveying quite a lot

53
00:05:22.280 --> 00:05:28.860
of information already which is really really neat on the visualization front.

54
00:05:28.910 --> 00:05:36.560
Now if you're having trouble reading what it says down the sides and at the bottom of this chart we

55
00:05:36.560 --> 00:05:49.860
can increase the font size of these labels with BLT dot X takes parentheses font size equals 14 and

56
00:05:49.880 --> 00:05:53.250
I can do the same for the y axis with BLT Dot.

57
00:05:53.420 --> 00:06:02.030
Y ticks parentheses font size equals 14 hitting shift enter.

58
00:06:02.420 --> 00:06:04.490
We see it updated like so.

59
00:06:04.730 --> 00:06:07.640
So now it's a bit easier to read.

60
00:06:07.700 --> 00:06:10.940
Now it's time to add that mask that we created.

61
00:06:11.360 --> 00:06:19.340
We want to hide the correlations on this chart that are duplicates coming back up him inside the heat

62
00:06:19.340 --> 00:06:20.490
map method.

63
00:06:20.530 --> 00:06:28.640
I'm going to add another argument we're gonna say mask so the argument called mask is equal to

64
00:06:31.510 --> 00:06:41.110
the mask that we've so painstakingly created in the cell above so mask is equal to mask and this might

65
00:06:41.230 --> 00:06:51.580
look very confusing but this mask here refers to our variable in this cell here and this python code

66
00:06:51.580 --> 00:06:58.980
reading mask equals refers to the name of the key word in this function.

67
00:06:58.990 --> 00:07:01.810
Let me shift into and show you what this looks like.

68
00:07:03.140 --> 00:07:08.850
Well now we've effectively hidden half of our chart.

69
00:07:09.640 --> 00:07:17.710
So when it modifies even further I'm gonna add the actual values of our correlations on our heat map

70
00:07:18.150 --> 00:07:26.050
because what I want to do is what I want to display these numbers here on our chart with the colors

71
00:07:26.560 --> 00:07:36.910
so messy and not is equal to true and head shift into how you'll see the values of the correlations

72
00:07:37.240 --> 00:07:40.570
being displayed in the heat map.

73
00:07:40.570 --> 00:07:46.050
Of course by default these numbers actually get really small and difficult to read.

74
00:07:46.050 --> 00:07:46.600
No why.

75
00:07:46.600 --> 00:07:55.220
It's just how it is so we can increase their font size with another keyword argument so we can see not

76
00:07:55.650 --> 00:07:56.730
underscore k.

77
00:07:56.780 --> 00:08:04.200
Double U S is equal to and then curly braces quotes.

78
00:08:04.360 --> 00:08:10.530
Size colon and then 14 14.

79
00:08:10.570 --> 00:08:15.580
It's gonna be the font size of our annotations.

80
00:08:15.580 --> 00:08:22.550
The value of this are not on the score kW s argument is given as a dictionary.

81
00:08:22.560 --> 00:08:28.750
It's a python dictionary that we're looking at here and you can always spot Python dictionaries very

82
00:08:28.750 --> 00:08:39.640
very easily with this kind of curly bracket notation and a key value pair or some key value pairs inside.

83
00:08:39.790 --> 00:08:48.900
The key here is the string size and the value is 14.

84
00:08:48.980 --> 00:08:52.530
These are always separated by this colon.

85
00:08:52.730 --> 00:08:54.440
Let me shift into an update.

86
00:08:54.440 --> 00:08:55.270
The heat map now.

87
00:08:57.150 --> 00:08:59.060
What a brilliant.

88
00:08:59.120 --> 00:09:06.380
Now the only thing I find a little bit strange is who hired this background here is not all white because

89
00:09:06.380 --> 00:09:09.840
I expected the styling to be a little bit different.

90
00:09:09.860 --> 00:09:14.850
I expected this to be a white background instead of this gray hair.

91
00:09:15.020 --> 00:09:19.790
Now if you're also seeing something a little bit unexpected like this and the styling front you can

92
00:09:19.790 --> 00:09:29.030
always set the style manually of seaborne with S.A. stunt set on a school style parentheses and then

93
00:09:29.180 --> 00:09:31.260
provide the name of a style.

94
00:09:31.280 --> 00:09:39.540
So I'm going to go with white and it shift into that line of code should force this background color

95
00:09:39.540 --> 00:09:42.580
here to be set to white.

96
00:09:42.660 --> 00:09:48.210
But you know the thing is all in all writing this python code with the mask and with Seabourn and the

97
00:09:48.210 --> 00:09:51.040
heat map it's kind of like the easy part actually.

98
00:09:51.870 --> 00:09:57.900
The much harder part is making sense of what it is that we're actually looking at here.

99
00:09:59.080 --> 00:10:01.880
What is it that we can learn from this correlation matrix.

100
00:10:02.740 --> 00:10:10.570
So first off you and I we said we're gonna be looking at two things strength and direction.

101
00:10:10.840 --> 00:10:19.770
An example of a strong positive correlation would be something like an O x and Indus.

102
00:10:20.170 --> 00:10:28.750
Now this Indus feature measures the proportion of non retail business acres per town and this new X

103
00:10:28.750 --> 00:10:35.640
feature measures the nitric oxide concentration in parts per 10 million.

104
00:10:35.660 --> 00:10:42.070
At least that's me reading it off the documentation on the feature descriptions.

105
00:10:42.070 --> 00:10:47.840
These two features have a correlation of zero point seven six.

106
00:10:47.920 --> 00:10:52.160
So the question is does this make sense and I think.

107
00:10:52.510 --> 00:10:52.750
Yeah.

108
00:10:52.780 --> 00:10:54.070
Yeah it does.

109
00:10:54.070 --> 00:10:59.200
I would expect the pollution to be higher in industrial areas.

110
00:10:59.200 --> 00:11:07.200
The amount of industry and the amount of pollution should be positively correlated but looking at this

111
00:11:07.200 --> 00:11:08.310
table a little bit more.

112
00:11:08.370 --> 00:11:10.170
You know what I found quite interesting.

113
00:11:10.890 --> 00:11:21.690
It's the correlation of tax and the industry variable higher tax levels are apparently associated with

114
00:11:21.690 --> 00:11:23.810
more industrial areas.

115
00:11:23.820 --> 00:11:26.250
I actually found this quite surprising.

116
00:11:26.250 --> 00:11:34.950
So coming across these kind of relationships is why the correlation matrix is a useful tool for data

117
00:11:34.980 --> 00:11:44.420
exploration but there are of course has with everything some limitations looking at this heat map here.

118
00:11:44.630 --> 00:11:53.330
We can see that the highest correlation of all is the correlation between tax and rad access to radio

119
00:11:53.330 --> 00:11:54.480
highways.

120
00:11:54.500 --> 00:12:01.190
This is a positive correlation of zero point nine one which seems super high.

121
00:12:01.260 --> 00:12:01.860
How.

122
00:12:01.880 --> 00:12:08.010
Remember how we looked at the documentation of this correlation function.

123
00:12:08.120 --> 00:12:16.730
We went up here and we had shift tab and we learned that the default method for calculating this correlation

124
00:12:16.880 --> 00:12:19.640
is the Pearson method.

125
00:12:19.640 --> 00:12:27.950
Now it turns out that one of the things that you have to know about this type of correlation is that

126
00:12:27.950 --> 00:12:32.160
it makes some assumptions about the kind of data that it's running on.

127
00:12:32.390 --> 00:12:39.620
This correlation calculation is actually only valid for continuous variables and this means that it's

128
00:12:39.620 --> 00:12:46.790
not valid for C like a dummy variable like whether a property is on the Charles River or not because

129
00:12:46.790 --> 00:12:51.050
this is not a continuous variable it's only got two values right.

130
00:12:51.050 --> 00:12:59.390
0 or 1 and looking back up here where we've created our histogram for accessibility to radial highways

131
00:12:59.930 --> 00:13:05.060
we can also see that this is not a continuous variable.

132
00:13:05.120 --> 00:13:07.100
This feature was an index.

133
00:13:07.100 --> 00:13:13.070
If you remember and what this means is that our correlation calculation is actually not valid for the

134
00:13:13.070 --> 00:13:21.710
RFD feature because RAD is not a continuous variable which goes to show that it's very important to

135
00:13:21.710 --> 00:13:28.250
know how the individual features are measured what units they're in and what the distribution of the

136
00:13:28.250 --> 00:13:35.750
data looks like for these features because we can only use statistical tools that are appropriate for

137
00:13:35.750 --> 00:13:37.220
the kind of data you're working with.

138
00:13:37.950 --> 00:13:40.920
Okay so let's look at this last row down here.

139
00:13:41.000 --> 00:13:49.310
The row that reads price which is our target value on this row you see the correlation of all the features

140
00:13:49.820 --> 00:13:54.610
in our model with the price with our target.

141
00:13:54.890 --> 00:14:00.170
One of the things that I'm interested in looking for here is four which features we we don't find a

142
00:14:00.170 --> 00:14:07.700
relationship for which of the features is the correlation close to zero the lowest correlation of course

143
00:14:07.820 --> 00:14:11.600
is with the Charles River dummy variable.

144
00:14:11.600 --> 00:14:17.600
But as we've just said Charles is a dummy variable with only values between 1 and 0.

145
00:14:17.870 --> 00:14:21.340
So the correlation measure is actually not appropriate.

146
00:14:21.620 --> 00:14:23.660
But what about the next lowest one.

147
00:14:24.050 --> 00:14:35.000
The next lost one is this one called D I S and D I S is defined as the distance from employment centers

148
00:14:36.410 --> 00:14:37.010
now.

149
00:14:37.280 --> 00:14:37.930
That's interesting.

150
00:14:37.930 --> 00:14:47.360
So DHS is not very correlated with price but DHS is very highly correlated with the industry feature.

151
00:14:47.660 --> 00:14:56.250
Looking here we see that there is a correlation of minus zero point seven one between DHS and index.

152
00:14:56.270 --> 00:15:04.340
The reason I suspect this is the case is because many industrial areas are probably employment centers

153
00:15:04.700 --> 00:15:14.660
so being far away from an employment center is associated with a low amount of industry and this discovery

154
00:15:15.710 --> 00:15:22.030
adds something to my to do list for the regression analysis stage.

155
00:15:22.130 --> 00:15:32.720
What we should probably do is we should check if our distance feature adds explanatory value to our

156
00:15:32.720 --> 00:15:42.160
regression model in other words does having both the industry feature and the distance feature included

157
00:15:42.310 --> 00:15:46.850
in the regression make our model better or worse.

158
00:15:47.740 --> 00:15:52.830
Can we get away with just having the industry feature for example.

159
00:15:53.140 --> 00:16:00.220
Because the thing is if a feature is not adding any explanatory value it's often better to excluded

160
00:16:00.490 --> 00:16:07.990
and trying to run the regression without it because by excluding features you might end up with a simpler

161
00:16:07.990 --> 00:16:12.570
model and simplicity is usually a good thing.

162
00:16:12.580 --> 00:16:15.670
Okay so where does this leave us.

163
00:16:15.730 --> 00:16:21.520
The correlation matrix is no silver Data Exploration bullet.

164
00:16:21.640 --> 00:16:28.990
While it may not answer all our questions it can give us a bit more perspective and the correlation

165
00:16:28.990 --> 00:16:30.800
matrix has its pros and cons.

166
00:16:30.910 --> 00:16:33.520
It has strengths and it has limitations.

167
00:16:33.520 --> 00:16:40.280
Just like every other tool regarding the pros we've learned something about our data.

168
00:16:40.330 --> 00:16:46.810
We've learned that the amount of tax and the amount of industry are correlated and we've added something

169
00:16:46.810 --> 00:16:53.530
to our to do list for later namely that we should investigate if we really need the DHS feature in our

170
00:16:53.530 --> 00:16:55.630
model or not.

171
00:16:55.630 --> 00:17:02.380
Another pro is that we've learned that certain features with high correlations are possible sources

172
00:17:02.680 --> 00:17:04.830
of multicore linearity.

173
00:17:04.870 --> 00:17:10.270
Now I emphasize the word possible and this is another thing for our To Do list.

174
00:17:10.420 --> 00:17:17.920
High correlations don't necessarily imply this problem of multicore linearity but we will revisit this

175
00:17:17.920 --> 00:17:26.150
issue during the regression analysis stage by running a formal test for this problem.

176
00:17:26.160 --> 00:17:32.950
Now we're also learning a few things about some weaknesses of looking at correlations.

177
00:17:33.030 --> 00:17:39.420
For example we've learned that the correlation calculations assume continuous data.

178
00:17:39.480 --> 00:17:44.700
This Pearson correlation calculation that we've looked at is not valid.

179
00:17:44.820 --> 00:17:52.620
If the data is not continuous as it is the case with our accessibility index or our Charles River dummy

180
00:17:52.620 --> 00:18:00.690
variable and a second limitation that everybody likes to harp on about is that correlation does not

181
00:18:00.840 --> 00:18:02.790
imply causation.

182
00:18:03.000 --> 00:18:09.210
Just because two things move together doesn't mean that one thing causes another.

183
00:18:09.210 --> 00:18:14.300
In other words everybody who drank water in 1850 is now dead.

184
00:18:14.520 --> 00:18:17.940
But this doesn't mean that drinking water will kill you.

185
00:18:17.970 --> 00:18:24.330
In fact if you look at enough data and you look hard enough you will find that there are all sorts of

186
00:18:24.330 --> 00:18:26.770
weird correlations out there.

187
00:18:26.970 --> 00:18:34.740
Just google funny correlations or spurious correlations and you'll find a bunch of great examples of

188
00:18:34.740 --> 00:18:40.270
completely unrelated things that move together purely by chance.

189
00:18:40.420 --> 00:18:46.680
And if you do this you'll probably come across Tyler Vision's Web site who uses census data and data

190
00:18:46.710 --> 00:18:54.360
from the U.S. Department of Agriculture to show that divorce rates in Maine and margarine consumption

191
00:18:54.690 --> 00:18:59.490
are in fact highly correlated so the earlier chart of mine showing a zero correlation between these

192
00:18:59.490 --> 00:19:05.890
two things was in fact Hey lie Tyler's chart shows us how it actually works now.

193
00:19:05.940 --> 00:19:14.190
Another limitation of correlations is that they only check for linear relationships and it turns out

194
00:19:14.400 --> 00:19:20.460
just because there's a low Pearson correlation coefficient does not mean that there is no relationship

195
00:19:20.550 --> 00:19:22.940
between two variables.

196
00:19:22.950 --> 00:19:26.960
Let me show you some examples so you can actually see what I mean.

197
00:19:27.000 --> 00:19:35.070
Here's some fictional data on a chart showing the x and y values X and Y have a correlation of zero

198
00:19:35.070 --> 00:19:37.950
point eight 1 6.

199
00:19:37.980 --> 00:19:44.980
And let me tell you a different chart this is some more fictional data and the correlation between X

200
00:19:45.030 --> 00:19:52.030
2 and Y 2 is in fact also zero point 8 1 6.

201
00:19:52.490 --> 00:19:54.170
And on this third chart here.

202
00:19:54.450 --> 00:20:02.880
You guessed it the correlation is also zero point eight 1 6 and the same goes for this 4th chart here

203
00:20:03.690 --> 00:20:09.800
x 4 and y 4 also have a correlation of zero point 8 1 6.

204
00:20:09.810 --> 00:20:12.860
In fact these photographs are very famous.

205
00:20:12.960 --> 00:20:18.960
They're called hence Combs quartered and they're named after an English statistician who came up with

206
00:20:18.960 --> 00:20:19.830
them.

207
00:20:20.010 --> 00:20:26.670
These four graphs actually have very very similar descriptive statistics and a very very similar regression

208
00:20:26.670 --> 00:20:26.940
line.

209
00:20:27.960 --> 00:20:32.100
But of course they're showing us completely different relationships.

210
00:20:32.220 --> 00:20:41.130
They're showing us that outliers and non-linear relationships often only become apparent after visualizing

211
00:20:41.280 --> 00:20:42.470
the data.

212
00:20:42.690 --> 00:20:44.440
And this is what this implies.

213
00:20:44.580 --> 00:20:51.750
It means that it's important to look at these correlations and these descriptive statistics in conjunction

214
00:20:51.960 --> 00:20:59.020
with some charts and with this in mind we're gonna be complementing our analysis of the correlation

215
00:20:59.410 --> 00:21:02.040
with some more graphical analysis.

216
00:21:02.080 --> 00:21:08.620
That way we can discover if there's any hidden linear relationships or outliers in our data.

217
00:21:08.620 --> 00:21:16.210
As such we're gonna be visiting our old friend again the scatter plot but before we move on I can't

218
00:21:16.210 --> 00:21:20.900
resist showing you this infamous comic strip from X KCET.

219
00:21:20.950 --> 00:21:24.480
This is the kind of humor that appeals to you more than you'd care to admit.

220
00:21:24.520 --> 00:21:31.960
Then I highly recommend subscribing to X Casey D RSS feed and get your dose of geeky web comics on a

221
00:21:31.960 --> 00:21:33.590
regular basis.

222
00:21:33.670 --> 00:21:35.360
I'll see you in the next lessons.

223
00:21:35.380 --> 00:21:35.950
Take care.
