WEBVTT
1
00:00:00.270 --> 00:00:06.180
Throughout the next lessons I want to do the final steps of the data preparation for our base classifier

2
00:00:06.630 --> 00:00:13.830
and also show you a common format for data sets that you'll often encounter out there in the wild.

3
00:00:14.040 --> 00:00:19.530
Over the next lessons we'll be writing some Python code to create our feature vectors but not only that

4
00:00:19.980 --> 00:00:23.850
we'll create our features as a sparse matrix.

5
00:00:23.850 --> 00:00:25.420
What I mean by that.

6
00:00:25.440 --> 00:00:28.140
Well let's take it one step ahead of time.

7
00:00:28.320 --> 00:00:31.050
Let's consider a full matrix first.

8
00:00:31.080 --> 00:00:38.110
In that case we have our documentaries in one column and then we'll have our words in another column.

9
00:00:39.020 --> 00:00:44.490
Now of course these will be the word I.D. but for illustration I've written an actual wood here on the

10
00:00:44.490 --> 00:00:45.830
slide.

11
00:00:45.830 --> 00:00:52.490
Then in that third column we'll have our label her label it will be equal to one if the e-mail is spam

12
00:00:52.940 --> 00:00:54.680
and it will be equal to zero.

13
00:00:54.710 --> 00:00:56.740
If our email is non spam.

14
00:00:56.960 --> 00:01:04.280
So in this case email number five thousand seven hundred and ninety five is a non spam email and the

15
00:01:04.280 --> 00:01:05.840
label is equal to zero.

16
00:01:06.110 --> 00:01:13.190
And that last column where it says occurrence we will capture how often the word in the word column

17
00:01:13.400 --> 00:01:15.040
appears in the email.

18
00:01:15.080 --> 00:01:20.480
So if the word free appears three times then occurrence will be equal to three.

19
00:01:20.510 --> 00:01:22.010
Makes sense right.

20
00:01:22.010 --> 00:01:29.330
The reason that this kind of table is called a full matrix is because for each email there's an entry

21
00:01:29.450 --> 00:01:30.890
for each word.

22
00:01:30.890 --> 00:01:38.390
Even if that wood doesn't occur in the email the full matrix has an entry for each word in the vocabulary

23
00:01:38.780 --> 00:01:41.360
for each and every single email.

24
00:01:41.430 --> 00:01:45.110
Now we set our vocabulary size at 2500.

25
00:01:45.140 --> 00:01:52.430
Remember therefore for each document I.D. There will be two thousand and five hundred entries many of

26
00:01:52.430 --> 00:01:54.450
which will be zero.

27
00:01:54.530 --> 00:01:59.700
And this is where the sparse matrix comes in with a sparse matrix.

28
00:01:59.760 --> 00:02:03.320
We will remove the entries which are zero.

29
00:02:03.790 --> 00:02:12.250
And that means we actually only include the rows which have a word that occurs in the email.

30
00:02:12.270 --> 00:02:18.750
In this example the word mortgage and the word pay does not appear in email number five thousand seven

31
00:02:18.750 --> 00:02:20.150
hundred ninety five.

32
00:02:20.190 --> 00:02:28.300
Therefore those rows will not be present in the sparse matrix so you can see how the sparse matrix is

33
00:02:28.310 --> 00:02:32.480
just a compressed version of the full matrix with fewer rows.

34
00:02:32.480 --> 00:02:36.390
Now let's head over to Jupiter notebook and write some python code.

35
00:02:36.620 --> 00:02:41.410
First let me add a markdown cell to commemorate what we're gonna do.

36
00:02:41.570 --> 00:02:52.040
We're going to generate features and a sparse matrix and the first part of that is going to be creating

37
00:02:52.160 --> 00:02:59.240
a data frame with one word per column.

38
00:02:59.520 --> 00:03:02.340
I say creating a data frame with one word per column.

39
00:03:02.540 --> 00:03:08.900
But what are we creating the data frame from where are we at in terms of our data.

40
00:03:08.900 --> 00:03:16.980
We're going to be working with our stemmed nested list are stemmed nested list looks like this.

41
00:03:17.060 --> 00:03:21.230
So we've got a list of words for each document for each email.

42
00:03:21.770 --> 00:03:29.310
This thing about this is that Haas 10 nested list is a series All right.

43
00:03:29.720 --> 00:03:34.790
It's a panda series that holds on to individual lists.

44
00:03:34.790 --> 00:03:41.360
So if I want to access the e-mail at position two then I would see that even though our STEM nested

45
00:03:41.360 --> 00:03:47.240
list as a whole is a panda series it contains individual lists.

46
00:03:47.450 --> 00:03:53.270
So it's actually a series of lists each entries a list that makes it a little bit of an unwieldy data

47
00:03:53.270 --> 00:03:56.420
structure but it's one we're gonna work with.

48
00:03:56.420 --> 00:04:04.130
And what we're gonna do is we're going to convert it from a series containing lists to one of a list

49
00:04:04.370 --> 00:04:05.840
containing lists.

50
00:04:05.840 --> 00:04:11.330
And the reason for doing that is that there exists a very very handy method to convert list of lists

51
00:04:11.450 --> 00:04:13.090
into a data frame.

52
00:04:13.160 --> 00:04:20.810
So let me quickly copy and paste the cell and show you the method we're gonna use to convert our series

53
00:04:20.810 --> 00:04:21.850
to a list.

54
00:04:22.130 --> 00:04:29.090
And it's simply called to list our panda series has a method called to list which converts the whole

55
00:04:29.090 --> 00:04:31.460
thing to a Python list.

56
00:04:31.490 --> 00:04:32.450
Fair enough.

57
00:04:32.630 --> 00:04:37.640
The way this looks is what you might expect namely two square brackets.

58
00:04:37.670 --> 00:04:41.520
And then in each pair of square brackets you have an individual email.

59
00:04:41.520 --> 00:04:42.830
So the first e-mail ends here.

60
00:04:42.830 --> 00:04:44.800
The second one starts here.

61
00:04:44.840 --> 00:04:45.370
It's.

62
00:04:45.850 --> 00:04:51.650
It's kind of messy actually but we don't have to worry about this so much because we're going to create

63
00:04:51.740 --> 00:04:56.570
a panda's data frame using this code here.

64
00:04:56.720 --> 00:05:04.560
And the way we're gonna do that is with PD dot data frame and instead of the parentheses and enclosing

65
00:05:04.560 --> 00:05:13.890
it like so what we're gonna do instead is we're gonna put a dot and call a method called from records

66
00:05:14.130 --> 00:05:21.330
from Underscore Records that is and this is where we're going to feed in our stemmed unescorted nested

67
00:05:21.350 --> 00:05:30.320
underscore list dot to list now before I hit shift enter on this let's store this whole thing this whole

68
00:05:30.320 --> 00:05:39.470
data frame in a variable called Word on the score columns on a score date f or a data frame and we'll

69
00:05:39.470 --> 00:05:47.730
set that equal to PD dot data frame dot from records and then below we're going to print out the head

70
00:05:47.880 --> 00:05:54.130
of this data frame so we're going a script columns underscore def dot head.

71
00:05:54.210 --> 00:06:00.750
Now let's take a look at the first five rows and what we see here is as the index we've got our document

72
00:06:00.780 --> 00:06:08.880
ideas we'll add a label for this shortly and then we have each word split up as an individual data point

73
00:06:09.330 --> 00:06:16.890
in a column and it looks like we have a total of seven thousand six hundred and sixty one columns.

74
00:06:17.660 --> 00:06:21.940
The overall shape of our data frame looks like this.

75
00:06:22.280 --> 00:06:28.970
We've got five thousand seven hundred and ninety six rows and seven thousand six hundred and sixty one

76
00:06:29.540 --> 00:06:30.800
columns.

77
00:06:30.830 --> 00:06:32.230
Now here's a question to you.

78
00:06:32.480 --> 00:06:40.940
Why does the data frame have this shape well five thousand seven hundred and ninety six is the total

79
00:06:40.940 --> 00:06:48.440
number of emails that we have and seven thousand six hundred and sixty one is the number of words stem

80
00:06:48.440 --> 00:06:56.210
words that is in the longest email and we know this to be true because we've worked this out in a previous

81
00:06:56.480 --> 00:07:09.990
exercise now it's time to take the next step and that is splitting the data into a training and testing

82
00:07:10.170 --> 00:07:11.340
dataset.

83
00:07:11.790 --> 00:07:14.540
It's time to shuffle and split the data.

84
00:07:14.580 --> 00:07:18.450
Now that we've got our word on the score columns data frame.

85
00:07:18.860 --> 00:07:20.530
Now we've actually done this before.

86
00:07:20.550 --> 00:07:24.310
So I want to throw this over to you as a challenge.

87
00:07:24.390 --> 00:07:31.470
Can you split the data into a training and testing dataset using psychic learn as they're doing this

88
00:07:31.950 --> 00:07:35.820
set the test size at 30 percent.

89
00:07:35.820 --> 00:07:44.190
That means that the training data should include around 4000 and 57 emails and also as you're shuffling

90
00:07:44.430 --> 00:07:47.300
set the seed value to 42.

91
00:07:47.400 --> 00:07:53.460
And as you pause this video and try to solve this challenge how we think about what the target values

92
00:07:53.670 --> 00:07:55.170
should be as you're doing this

93
00:07:58.490 --> 00:07:58.890
all right.

94
00:07:58.890 --> 00:08:00.470
So here's the solution.

95
00:08:00.660 --> 00:08:05.830
We're gonna be using psychic learns train test split function to accomplish this.

96
00:08:05.910 --> 00:08:15.200
So we have to import this whole thing into our notebook so we'll see from S.K. learn don't model on

97
00:08:15.200 --> 00:08:16.390
a school selection.

98
00:08:16.410 --> 00:08:18.540
This is where the whole thing lives.

99
00:08:18.720 --> 00:08:28.180
Import train test split hitting tab on your keyboard after typing a few of the letters should help you

100
00:08:28.270 --> 00:08:32.570
avoid any typos on this relatively long import statement.

101
00:08:32.620 --> 00:08:40.160
Now hit shift enter on the cell and let's continue where we left off at the bottom of the notebook the

102
00:08:40.160 --> 00:08:41.550
train test split.

103
00:08:41.570 --> 00:08:47.800
Function will give us four outputs so that store them in four separate variables.

104
00:08:47.990 --> 00:08:55.620
The first one I'll call X on a school train the next one I'll call X on a school test then lowercase

105
00:08:55.620 --> 00:08:58.020
y on a school train.

106
00:08:58.050 --> 00:08:58.720
Come on.

107
00:08:58.930 --> 00:09:01.250
Why on the school test.

108
00:09:01.250 --> 00:09:10.720
And that's gonna be equal to train test split open parentheses word columns data frame.

109
00:09:10.760 --> 00:09:12.830
This is going to be our first argument.

110
00:09:13.280 --> 00:09:19.160
Then we have to supply our y values the data frame that we just created after all was just the features

111
00:09:19.240 --> 00:09:25.610
Right the different words the y values that we're trying to predict are actually our categories and

112
00:09:25.610 --> 00:09:32.800
I'm going to grab those from our data data frame it has a column called category and this is what I'll

113
00:09:32.950 --> 00:09:34.530
supply here.

114
00:09:35.020 --> 00:09:46.720
Next I'll set the test size to zero point three so 30 percent and then I'll set my random under school

115
00:09:46.960 --> 00:09:51.140
state to 42.

116
00:09:51.200 --> 00:09:54.500
This is where I'm specifying a seed value.

117
00:09:54.500 --> 00:10:01.460
And if you and I specify the same seed value we'll get the same results we'll get exactly the same shuffle.

118
00:10:01.460 --> 00:10:08.770
So let me run this and then we'll look at our analytics first let me print out the number of training

119
00:10:08.770 --> 00:10:17.690
samples and I'll print out x train dot shape square brackets.

120
00:10:18.000 --> 00:10:18.730
Zero.

121
00:10:18.760 --> 00:10:25.090
These are the number of training samples and we've got 30 percent of the total which is four thousand

122
00:10:25.150 --> 00:10:28.340
and fifty seven next.

123
00:10:28.930 --> 00:10:36.480
Let's just verify the fraction of the training set for the fraction of the training set.

124
00:10:36.680 --> 00:10:44.690
It's gonna be this number four thousand and fifty seven divided by the total number of entries say in

125
00:10:44.690 --> 00:10:46.840
our features data frame.

126
00:10:46.880 --> 00:10:59.370
So this was word on the score of columns on a school def shape 0 and that's 70 percent or very close

127
00:10:59.370 --> 00:11:07.440
to that meaning the tests said it's gonna be one minus this number which is 30 percent which we've specified

128
00:11:07.620 --> 00:11:08.850
here.

129
00:11:08.850 --> 00:11:11.290
Now let's take a look at what we've actually got.

130
00:11:11.370 --> 00:11:16.200
So I'll take X train dot head.

131
00:11:16.200 --> 00:11:22.330
These are the first five rows of our shuffled word on the score columns data frame.

132
00:11:22.400 --> 00:11:26.230
Now I said that these numbers here would refer to the index.

133
00:11:26.240 --> 00:11:26.470
Right.

134
00:11:26.480 --> 00:11:30.690
Our document I.D. and we can add this label very very easily.

135
00:11:30.950 --> 00:11:40.610
So we'll see X on a school train dot index dot name is equal to single the quotes you'll see on the

136
00:11:40.610 --> 00:11:44.290
school I.D. shift enter.

137
00:11:44.610 --> 00:11:51.210
We'll have our index name show up in the output right here but say we wanted to add this index name

138
00:11:51.720 --> 00:11:55.830
to both the training data set and the testing dataset.

139
00:11:55.860 --> 00:11:57.860
So X underscore a test as well.

140
00:11:58.200 --> 00:12:03.900
We can actually do that in the very same line of code by inserting another equal sign before we assign

141
00:12:03.900 --> 00:12:14.880
this string value here and write X on the school test dot index tot name in this case we're setting

142
00:12:14.880 --> 00:12:22.950
both of these indices equal to talk on a school I.D. then we had shift enter and show you that document

143
00:12:22.980 --> 00:12:32.310
I.D. actually match up after shuffling let's pull up y on the school train dot head and then we see

144
00:12:32.310 --> 00:12:38.610
the first five rows of our target values you can see here that the document I.D. match up with what

145
00:12:38.610 --> 00:12:44.450
we see in the training dataset how of course that should be true regardless of whether this says X in

146
00:12:44.450 --> 00:12:51.720
Moscow train or X in the school test the order of the features and the target values will be the same

147
00:12:52.770 --> 00:12:54.450
but proof is in the pudding right.

148
00:12:54.540 --> 00:13:03.930
Excellent the school test looks like this and Y on the school test looks like this in the next lesson

149
00:13:04.140 --> 00:13:11.550
we're going to create a sparse matrix from our training dataset and we're gonna do that by transforming

150
00:13:11.550 --> 00:13:14.840
the values in our data frame I'll see you there.
