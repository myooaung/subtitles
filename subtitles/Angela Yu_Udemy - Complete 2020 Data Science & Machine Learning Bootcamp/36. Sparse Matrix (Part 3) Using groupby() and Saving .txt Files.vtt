WEBVTT
1
00:00:00.330 --> 00:00:04.920
We're going to group all words by email and to do that.

2
00:00:05.250 --> 00:00:09.120
We'll use the pandas group by method.

3
00:00:09.120 --> 00:00:15.630
Let me add a markdown cell here that reads combine can't spell.

4
00:00:15.630 --> 00:00:25.800
Combine currencies with the pandas group buying method.

5
00:00:26.020 --> 00:00:32.340
Now I'm guessing that you will have used Excel Microsoft Excel at some point in the past and in excel

6
00:00:32.340 --> 00:00:36.300
as a very powerful functionality called a pivot table.

7
00:00:36.300 --> 00:00:42.850
And this group by method works in a very very similar way it will allow us to group the occurrences

8
00:00:42.970 --> 00:00:49.470
by document I.D. wordy and label and then we can sum up our occurrences.

9
00:00:49.480 --> 00:00:50.500
Let me show you.

10
00:00:50.680 --> 00:01:00.190
Train underscore grouped as equal to sparse on the squat train underscore the F dot group by open synthesis

11
00:01:00.850 --> 00:01:02.910
and I will provide a list.

12
00:01:03.040 --> 00:01:04.480
Square brackets.

13
00:01:04.480 --> 00:01:05.830
Single quotes.

14
00:01:05.970 --> 00:01:13.920
On a school I.D. comma single quotes word on a school I.D. comma.

15
00:01:13.920 --> 00:01:18.910
Single quotes label and the very end.

16
00:01:18.910 --> 00:01:28.060
We're going to chain another method called namely the summation so dot some at the end will sum up our

17
00:01:28.060 --> 00:01:31.430
occurrences after they've been grouped but I think.

18
00:01:31.430 --> 00:01:32.330
Seeing is believing.

19
00:01:32.330 --> 00:01:34.000
So let me show you what this looks like.

20
00:01:34.500 --> 00:01:37.470
So train on a score grouped dot head.

21
00:01:37.520 --> 00:01:39.610
Well show us the result.

22
00:01:39.620 --> 00:01:40.940
Here we go.

23
00:01:40.940 --> 00:01:49.130
What we see here is that for our document without zero our first document we've got a bunch of words

24
00:01:49.190 --> 00:01:52.610
in here that are grouped together by ideas.

25
00:01:52.760 --> 00:01:58.370
The word with Idea Number Zero occurs twice in this first email.

26
00:01:58.430 --> 00:02:01.420
This is all that this table is showing us right here.

27
00:02:01.430 --> 00:02:06.160
Now you might say All right well what's what's what's that word idea number zero.

28
00:02:06.170 --> 00:02:08.030
What's what's what's zero.

29
00:02:08.030 --> 00:02:18.760
So we can pull that up right Goodhart vocabulary and we can say dot at square brackets zero comma single

30
00:02:18.760 --> 00:02:22.910
quotes vocab and a score.

31
00:02:23.130 --> 00:02:28.810
Would this if you recall was the column name in our data frame.

32
00:02:28.810 --> 00:02:35.680
And this here is the index name which corresponded to our word I.D. the actual word that occurs twice

33
00:02:35.980 --> 00:02:38.990
in our vocabulary is h TTP.

34
00:02:39.080 --> 00:02:40.750
Why does this occur twice.

35
00:02:40.750 --> 00:02:45.400
Well it's because there's two hyperlinks in our email the original email.

36
00:02:45.400 --> 00:02:45.910
Right.

37
00:02:46.020 --> 00:02:47.350
A document added zero.

38
00:02:47.380 --> 00:02:56.800
We can pull up with data dot message square brackets zero and this e-mail reads Dear homeowner interest

39
00:02:56.800 --> 00:02:58.490
rates are at their lowest level blah blah.

40
00:02:59.170 --> 00:03:06.800
If I look further down in the text I see the first hyperlink here and I see the second hyperlink here.

41
00:03:06.830 --> 00:03:11.990
This is why the word HDP appears twice in this document.

42
00:03:12.620 --> 00:03:17.620
So grouped by function combined with the summation method seems to have worked really well.

43
00:03:17.750 --> 00:03:23.270
The thing I would quite like though is to have less of this pivot table feel to it and repeat the document

44
00:03:23.270 --> 00:03:30.890
daddy on every single row we can do that with train grouped equals.

45
00:03:30.890 --> 00:03:40.340
So we're just gonna overrated right train grouped don't reset index reset index will make our document

46
00:03:40.370 --> 00:03:47.770
D appear on every single row train grouped don't head.

47
00:03:47.790 --> 00:03:49.990
We'll show you exactly that.

48
00:03:50.850 --> 00:03:52.440
Fantastic.

49
00:03:52.470 --> 00:03:56.400
Let's take a quick look at the tail of this data frame as well.

50
00:03:56.490 --> 00:04:05.010
Train on a score grouped dot tail parentheses gives us this result we're get you to the same very quick

51
00:04:05.100 --> 00:04:13.410
sense check on this result as well in particular let's take a look at what word corresponds to one thousand

52
00:04:13.410 --> 00:04:15.460
nine hundred and twenty three.

53
00:04:15.510 --> 00:04:19.360
It appears to occur twice in this e-mail.

54
00:04:19.380 --> 00:04:22.030
Now you can either work ahead or follow along with me.

55
00:04:22.290 --> 00:04:30.660
But we've done this already vocab dot and square brackets one thousand nine hundred and twenty three

56
00:04:31.170 --> 00:04:36.150
comma single quotes vocab underscore word.

57
00:04:36.210 --> 00:04:37.620
This gives us the result.

58
00:04:37.770 --> 00:04:46.500
Welch which is a very odd word right and doesn't quite seem like a like a real word but it could be

59
00:04:46.500 --> 00:04:47.520
a stem word right.

60
00:04:47.520 --> 00:04:50.280
Me So maybe that's why it's a bit strange.

61
00:04:50.280 --> 00:04:58.980
Let's pull up the actual message and see why this word appears twice data dot message square brackets

62
00:05:00.000 --> 00:05:03.980
five thousand seven hundred and ninety five.

63
00:05:04.020 --> 00:05:11.780
This brings up quite a short e-mail and it appears that Welch is actually a name.

64
00:05:11.850 --> 00:05:17.510
It's the last name of this guy Brent Welch a software architect.

65
00:05:17.640 --> 00:05:24.540
And the word appears again in his email address which is at the very end of this e-mail.

66
00:05:24.540 --> 00:05:27.330
So this is why it's here.

67
00:05:27.330 --> 00:05:28.590
So I'm quite happy about this.

68
00:05:28.590 --> 00:05:30.180
I think this is this is good.

69
00:05:30.180 --> 00:05:33.140
This seems to pass the sense check.

70
00:05:33.450 --> 00:05:40.350
The only thing I'd be quite curious to find out now is how big of a reduction we've achieved in the

71
00:05:40.350 --> 00:05:42.810
number of rows from before.

72
00:05:43.410 --> 00:05:50.730
So if I say train on a score grouped dot shape then I can see that we've reduced the size of our data

73
00:05:50.730 --> 00:05:52.650
frame quite a bit.

74
00:05:52.650 --> 00:05:58.560
We've gone from four hundred and fifty thousand all to approximate two hundred and sixty five thousand

75
00:05:58.560 --> 00:05:59.760
rows.

76
00:05:59.760 --> 00:06:03.020
That's still a lot but it's about a 40 percent reduction.

77
00:06:03.240 --> 00:06:13.500
And I think this puts us in a really good place to save our work so let's do that now that a very quick

78
00:06:13.650 --> 00:06:15.580
markdown cell here.

79
00:06:15.680 --> 00:06:16.880
US 1 c.

80
00:06:16.980 --> 00:06:27.820
Training data as Dot T X T file in the previous lessons we've saved our files once before as a dump.

81
00:06:27.840 --> 00:06:34.650
Jason File and as a dot CSP file this is how we saved our files to our disk previously.

82
00:06:34.650 --> 00:06:38.520
Now let's save a file as a plain text file.

83
00:06:38.520 --> 00:06:41.250
And for this we're going to use num PIs functionality.

84
00:06:41.430 --> 00:06:47.500
But before we do that we're gonna need a relative file path at the top of our notebook.

85
00:06:47.760 --> 00:06:51.440
That way sits nicely with its friends.

86
00:06:51.450 --> 00:06:55.080
Now I'm planning to save this file in a slightly different folder right.

87
00:06:55.560 --> 00:06:57.320
But first let's give it a name.

88
00:06:57.330 --> 00:07:07.920
I'm want to call it training on a school data on a school file and I'll set that equal to spam data

89
00:07:08.670 --> 00:07:25.600
forward slash 0 2 on the school training forward slash train hyphen data Dot T X T I'll be adding our

90
00:07:25.600 --> 00:07:29.620
text file to this folder right here now.

91
00:07:29.620 --> 00:07:35.380
Be sure to hit shift enter on your cell with your constants and then join me down here at the bottom

92
00:07:35.380 --> 00:07:51.860
of the notebook with NDP dot save t x t parentheses training data file comma train grouped comma F.M.

93
00:07:51.860 --> 00:07:57.630
t is equal to single quotes percent.

94
00:07:58.040 --> 00:08:04.400
D If I hit shift tab on my keyboard to bring up the quick documentation we see that this is the file

95
00:08:04.400 --> 00:08:08.000
name including the relative file path.

96
00:08:08.150 --> 00:08:16.040
This here the second argument is the data and the third argument is f empty which stands for format

97
00:08:17.160 --> 00:08:23.970
if I hit the plus sign here and scroll down then I can see that our format argument requires a string

98
00:08:24.060 --> 00:08:26.130
or a sequence of strings.

99
00:08:26.160 --> 00:08:29.790
This essentially allows us to specify the number format.

100
00:08:29.790 --> 00:08:32.650
Lucky for us we're just dealing with integers.

101
00:08:32.700 --> 00:08:39.780
If I bring up my folder here side by side and hit shift enter now then I should see my text file appear

102
00:08:40.020 --> 00:08:47.120
right here before I open this text file and people signed let me show you what the columns are called

103
00:08:47.480 --> 00:08:48.830
in Jupiter notebook.

104
00:08:49.100 --> 00:08:53.050
So train underscore grouped dot columns.

105
00:08:53.120 --> 00:08:58.330
We'll bring up our column names that stock I.D. where daddy label an occurrence

106
00:09:00.970 --> 00:09:02.970
now let's look at this text file.

107
00:09:03.190 --> 00:09:08.350
If I open it up in my text that it's here then I can see the four columns clearly outlined.

108
00:09:08.350 --> 00:09:16.090
The first one is my document I.D. The second number here is the word I.D. The third number is the category

109
00:09:16.150 --> 00:09:17.030
or label.

110
00:09:17.050 --> 00:09:21.080
It's the first message it is in fact a spam email.

111
00:09:21.220 --> 00:09:26.810
And that fourth number here is the occurrence of the word with this I.D..

112
00:09:27.160 --> 00:09:36.090
Looking at line 16 here the word with I.D. 105 occurs twice in this spam email.

113
00:09:36.100 --> 00:09:41.440
So I think that almost wraps it up except we haven't done this for our test data yet.

114
00:09:41.740 --> 00:09:46.560
And that's where I want to throw it over to you as a challenge.

115
00:09:46.840 --> 00:09:54.760
Can you create a sparse matrix for the test data and then group all the occurrences of the same word

116
00:09:54.790 --> 00:10:02.000
in the same email together just like we did with the training data after you've done all that saved

117
00:10:02.000 --> 00:10:05.140
a data as a text file.

118
00:10:05.150 --> 00:10:09.680
Now I realize you're gonna have to save that data to some plays and give it a file name.

119
00:10:09.680 --> 00:10:15.230
So let's do that right now so that you and I have the same file names going forward scrolling up to

120
00:10:15.230 --> 00:10:16.250
our constants.

121
00:10:16.300 --> 00:10:26.060
I'm just quickly going to copy this relative path pasted in and then change the file name here to test

122
00:10:27.080 --> 00:10:36.980
and change the constant name from training to test as well to test on the score data on this file is

123
00:10:36.980 --> 00:10:43.970
equal to this relative path file name and extension right here.

124
00:10:44.120 --> 00:10:49.670
Now I don't have to ask you to pause the video because I want to show you the solution in the next lesson

125
00:10:50.770 --> 00:10:51.490
I'll see you there.
