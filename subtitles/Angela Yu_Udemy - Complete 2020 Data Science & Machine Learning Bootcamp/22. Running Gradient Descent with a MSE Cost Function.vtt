WEBVTT
1
00:00:00.590 --> 00:00:07.620
Sort of slowly coming up to the best part namely the pot were about to run our gradient descent algorithm

2
00:00:08.100 --> 00:00:15.630
on our means squared error cost function but before we dive into the python code and calculate in which

3
00:00:15.630 --> 00:00:22.950
direction our algorithm should move we need to work out the slope of our cost function first namely

4
00:00:23.070 --> 00:00:29.400
our gradient and this is where I've got some great news for you because we just have to apply some of

5
00:00:29.400 --> 00:00:35.520
the same calculus tricks that we've covered so far and these partial derivatives that are coming up

6
00:00:35.880 --> 00:00:38.130
are really not that hard to figure out.

7
00:00:38.550 --> 00:00:39.970
So let's dive in.

8
00:00:40.260 --> 00:00:49.080
Now you'll recall that our means quite Ara function looks like this hits one over n times the sum of

9
00:00:49.440 --> 00:00:52.520
Y minus y hat squared.

10
00:00:52.650 --> 00:00:59.550
So the actual values minus the predicted values squared you son them all up and you take the average.

11
00:00:59.730 --> 00:01:05.400
But the thing is since we're running a very simple linear regression on this with one variable only

12
00:01:05.670 --> 00:01:14.630
namely X are Y that actually takes the form theta 0 plus theta one times X..

13
00:01:14.910 --> 00:01:21.810
This is the linear regression model that we're using currently it's got one variable and two parameters

14
00:01:21.930 --> 00:01:24.630
theta zero and theta one.

15
00:01:24.660 --> 00:01:27.840
So what does this mean about our means squared error.

16
00:01:27.840 --> 00:01:35.790
Well if we take our equation and we simply substitute our model our linear regression model into this

17
00:01:35.790 --> 00:01:39.020
equation for y hat then we get something like this.

18
00:01:39.180 --> 00:01:45.810
And by removing those parentheses we can simplify this to the following form are means squared error

19
00:01:45.960 --> 00:01:50.020
for this particular linear regression model actually looks like this.

20
00:01:50.040 --> 00:01:59.510
We've got one over n times the sum of Y minus theta zero minus theta one times X squared.

21
00:01:59.670 --> 00:02:02.270
But you know what we can take this even further.

22
00:02:02.370 --> 00:02:08.860
Check it out when we're going to do now is we're gonna write out all the terms in this equation.

23
00:02:08.970 --> 00:02:14.550
So this is the opposite of simplifying it but it's going to make calculating our partial derivatives

24
00:02:14.670 --> 00:02:16.260
a lot easier.

25
00:02:16.260 --> 00:02:23.940
We can figure out all the terms in this sum like so firstly we can multiply out all the terms in this

26
00:02:23.940 --> 00:02:28.100
equation and that means we get something like this.

27
00:02:28.320 --> 00:02:32.160
We get quite a few terms starting with Y squared.

28
00:02:32.160 --> 00:02:39.120
But of course as a few terms in this long list that we can combine so we'd actually get something like

29
00:02:39.120 --> 00:02:41.580
this when we simplified a little bit.

30
00:02:41.580 --> 00:02:47.880
Now I know that that doesn't look very pretty but the great thing about having the equation written

31
00:02:47.880 --> 00:02:54.390
out like this is that we can calculate partial derivatives very very easily.

32
00:02:54.390 --> 00:02:59.760
So what I want to do is I want to start this lesson out on a challenge I would like you to get out pencil

33
00:02:59.760 --> 00:03:08.370
and paper and try to apply the power rule to the equation above to our mean square area equation and

34
00:03:08.370 --> 00:03:13.770
take the partial derivative with respect to theta zero.

35
00:03:14.040 --> 00:03:20.570
So our int. I'll give you a few seconds to pause the video before I show you the solution.

36
00:03:20.640 --> 00:03:21.720
Ready.

37
00:03:21.720 --> 00:03:29.280
Here we go now looking at this equation the first thing you'll notice is that there are quite a few

38
00:03:29.280 --> 00:03:39.000
terms that don't depend on theta zero namely Y squared to theta one x y and theta one squared times

39
00:03:39.090 --> 00:03:42.180
X squared for a partial derivative.

40
00:03:42.270 --> 00:03:47.450
These terms are treated as constants and they drop out of the equation.

41
00:03:47.460 --> 00:03:49.640
This is what we've talked about before.

42
00:03:49.680 --> 00:03:51.150
So what are we left with.

43
00:03:51.150 --> 00:04:02.260
Well we're left with the following sum minus two y plus two times theta zero plus two times theta one

44
00:04:02.290 --> 00:04:03.800
times X..

45
00:04:04.000 --> 00:04:08.930
And this is simply from applying the power rule that we covered in a previous lesson.

46
00:04:09.100 --> 00:04:14.270
Looking at this equation we can simplify it a little bit to make it look a bit prettier.

47
00:04:14.350 --> 00:04:19.110
The first thing I'm going to do I'm going to factor out the two that all these three terms in the sum

48
00:04:19.120 --> 00:04:20.460
have in common.

49
00:04:20.560 --> 00:04:26.650
In fact I'm actually going to factor out a minus two and I'm left with something like this.

50
00:04:26.650 --> 00:04:35.380
I've got minus two times why minus theta zero minus theta one and then what I can do is I can simply

51
00:04:35.380 --> 00:04:45.350
move this constant outside of the sum sorry equation would actually look like this and that's really

52
00:04:45.350 --> 00:04:45.870
it.

53
00:04:45.950 --> 00:04:51.150
That's the partial derivative with respect to theta zero.

54
00:04:51.200 --> 00:04:57.620
One thing that you might have noticed is that I've left out the little eyes in the superscript in this

55
00:04:57.620 --> 00:05:01.570
derivation hand and I'm going to put those back now for you.

56
00:05:01.610 --> 00:05:07.140
I left them out earlier because otherwise the notation would have just gotten too busy on the slide.

57
00:05:07.220 --> 00:05:13.130
So now that we've worked out the partial derivative with respect to our first parameter we can work

58
00:05:13.130 --> 00:05:19.910
out the partial derivative with respect to our second parameter namely theta 1.

59
00:05:19.940 --> 00:05:26.120
Once again I'm going to pose this as a challenge to you because you've worked out this one.

60
00:05:26.120 --> 00:05:28.280
Working out the other one is very very similar.

61
00:05:28.310 --> 00:05:33.370
You go through exactly the same steps but you'll get a slightly different result.

62
00:05:33.380 --> 00:05:39.490
I'll give you a few seconds to pause the video and scribble this down with pencil and paper.

63
00:05:41.120 --> 00:05:42.310
Ready.

64
00:05:42.320 --> 00:05:43.220
Here's the solution

65
00:05:46.430 --> 00:05:51.620
the equation that you get at the end when you go through all the steps and you simplify it will look

66
00:05:51.800 --> 00:05:53.060
something like this.

67
00:05:53.090 --> 00:05:57.950
It'll be very very similar to the partial derivative with respect to theta zero.

68
00:05:57.950 --> 00:06:05.120
Except that you're multiplying the entire thing by the x values at the end of the sum with these two

69
00:06:05.120 --> 00:06:06.480
equations in front of us.

70
00:06:06.560 --> 00:06:13.400
We can now add them to Jupiter notebook once again the first thing I'm gonna do is add a section heading

71
00:06:13.730 --> 00:06:16.390
with some markdown an hour later.

72
00:06:16.400 --> 00:06:17.480
Equations.

73
00:06:17.480 --> 00:06:29.030
This section heading I'm going to call partial derivatives of the mean squared error with respect to.

74
00:06:29.030 --> 00:06:36.140
Then I'm gonna add a dollar sign a backslash and write theta on the school 0 and then another dollar

75
00:06:36.140 --> 00:06:47.000
sign and dollar sign backslash theta underscore one dollar sign using the dollar signs.

76
00:06:47.060 --> 00:06:54.080
I'm including some later notation in line for our section heading and it's going to look like this when

77
00:06:54.080 --> 00:06:54.980
I press shift enter.

78
00:06:55.460 --> 00:07:04.040
But let's add our partial derivatives in latex notation as well so I'm going to add two hashtags to

79
00:07:04.040 --> 00:07:09.590
pound symbols than to Dollar Signs and write are fraction.

80
00:07:09.680 --> 00:07:17.240
So it's gonna be backslash F R A C pair of curly braces and then another pair of curly braces.

81
00:07:17.400 --> 00:07:24.650
And within the first pair of curly braces I'm gonna write backslash partial MSE and then in the second

82
00:07:24.650 --> 00:07:33.680
pair of curly braces I want to write backslash partial backslash theta underscore zero and that whole

83
00:07:33.680 --> 00:07:39.710
thing is gonna be equal to but before I add that bit Let's take a quick look what this looks like so

84
00:07:39.710 --> 00:07:41.640
I'm going to add my two dollar signs at the end.

85
00:07:41.690 --> 00:07:50.600
Press shift enter and there I can see my fraction with the partial derivative symbols in front okay

86
00:07:51.050 --> 00:07:54.270
so just so we have our equation in ha.

87
00:07:54.340 --> 00:07:55.770
Jupiter notebook as well.

88
00:07:55.940 --> 00:07:57.470
Let's write it out here together.

89
00:07:57.480 --> 00:08:08.220
So it's gonna be minus and then backslash Frank curly braces to curly braces and spacing.

90
00:08:08.440 --> 00:08:14.710
And then backslash some underscore curly braces I equals 1.

91
00:08:14.780 --> 00:08:16.150
Curly braces.

92
00:08:16.160 --> 00:08:18.750
Carrie curly braces.

93
00:08:18.770 --> 00:08:26.610
And then backslash big open parentheses y carry curly braces.

94
00:08:26.660 --> 00:08:39.290
And then inside parentheses I minus backslash theta zero minus backslash theta on the school one X Carrie

95
00:08:39.830 --> 00:08:41.170
curly braces.

96
00:08:41.240 --> 00:08:43.600
And then inside parentheses I.

97
00:08:43.710 --> 00:08:50.350
And then the last bit is gonna be backslash big closing parentheses.

98
00:08:50.420 --> 00:08:51.380
Let's see what this looks like.

99
00:08:53.570 --> 00:08:55.820
I might say that's pretty spot on.

100
00:08:55.820 --> 00:08:59.180
I'm just gonna click inside here and add the second one as well.

101
00:08:59.180 --> 00:09:00.190
This is the easy part.

102
00:09:00.200 --> 00:09:04.330
This is where we just copy what we've just written pasted below.

103
00:09:04.520 --> 00:09:12.950
And then change this thing here to theta one and then at the end we're going to add backslash big open

104
00:09:12.950 --> 00:09:20.500
parentheses x Carrie curly braces parentheses.

105
00:09:20.810 --> 00:09:25.000
And then backslash big closing parentheses.

106
00:09:25.370 --> 00:09:26.260
That's it.

107
00:09:26.300 --> 00:09:32.450
Now we've got our partial derivative equations displayed beautifully and Jupiter notebook.

108
00:09:32.450 --> 00:09:39.380
So one thing that I'll note here is that you know these partial derivatives are gonna depend on what

109
00:09:39.380 --> 00:09:44.660
kind of equation we're using for y hat at this point.

110
00:09:44.660 --> 00:09:50.150
We're using linear regression with one variable so we substituted that in the.

111
00:09:50.270 --> 00:09:57.620
And then we derived our partial derivatives from that if we had a different model say linear regression

112
00:09:57.620 --> 00:10:05.390
with two variables or something that estimates are Y had a little differently then we'd simply substitute

113
00:10:05.600 --> 00:10:12.550
that equation into our means squared era and then we can do the same derivation if we're so inclined.

114
00:10:12.740 --> 00:10:20.180
Because that means quite error cost function lends itself very well to all sorts of regression problems.

115
00:10:20.180 --> 00:10:24.730
And it will adapt very very well to all kinds of models as well.

116
00:10:24.770 --> 00:10:32.090
So having written out the partial derivatives in this form we can create a function where we calculate

117
00:10:32.090 --> 00:10:39.320
the slopes of the parameters in Python code so I might add a little section heading here and I call

118
00:10:39.320 --> 00:10:45.150
it MSE and gradient descent.

119
00:10:45.500 --> 00:10:49.050
And then that python function I'm going to add here.

120
00:10:49.160 --> 00:10:56.690
Now what I'm going to do is I'm going to create a function called grad s IDF grad and I want to give

121
00:10:56.690 --> 00:11:04.490
it three inputs and give it the X the y values and an array of factors.

122
00:11:04.640 --> 00:11:12.320
And when I say colon and then inside the body of this function I want to work out these two partial

123
00:11:12.470 --> 00:11:13.840
derivatives.

124
00:11:14.060 --> 00:11:15.690
So what are my inputs here.

125
00:11:15.770 --> 00:11:21.860
My inputs are the x values so the data the y values.

126
00:11:21.860 --> 00:11:23.880
Again this is also data.

127
00:11:24.410 --> 00:11:31.460
And then I have an array of theta parameters.

128
00:11:31.580 --> 00:11:36.110
These are the bits that we're actually optimizing in our gradient descent algorithm.

129
00:11:36.110 --> 00:11:44.720
This array is gonna have theta 0 and index 0 and theta one at index 1.

130
00:11:44.930 --> 00:11:49.770
So this is gonna be my function the number of samples.

131
00:11:49.820 --> 00:11:57.110
So n I can work out by saying and it's equal to why not sides dysfunction is going to get a whole list

132
00:11:57.110 --> 00:12:04.880
of y values and by calling white outsides I can work out how many samples were given to this function.

133
00:12:04.880 --> 00:12:11.180
Now as a challenge can you create two variables theta zero underscore slope and theta one underscore

134
00:12:11.180 --> 00:12:19.100
a slope and what I want you to do is I want you to translate these latex equations into Python code

135
00:12:19.960 --> 00:12:25.010
I'll give you a few seconds to pause the video and work this out.

136
00:12:25.100 --> 00:12:25.490
Ready.

137
00:12:26.210 --> 00:12:27.360
Here's the solution.

138
00:12:27.440 --> 00:12:30.730
So theta zero underscore slope.

139
00:12:30.950 --> 00:12:45.440
It's gonna be equal to minus two divided by n times the sum of Y minus fetus square brackets zero minus

140
00:12:46.310 --> 00:12:51.270
fetus square brackets 1 times x.

141
00:12:51.350 --> 00:12:57.170
So we're expecting that this function will receive an array of theta parameters and I'll have to theta

142
00:12:57.170 --> 00:12:58.760
zero at index zero.

143
00:12:58.790 --> 00:13:03.660
So this is what we're using here and we have theta one at index 1.

144
00:13:03.740 --> 00:13:05.900
So this is what we're using here.

145
00:13:06.230 --> 00:13:12.020
Now working out our theta 1 underscore slope is gonna be trivial because I can just copy this.

146
00:13:12.260 --> 00:13:20.120
Change the name here and then simply add another set of parentheses to our sum and multiply the whole

147
00:13:20.120 --> 00:13:21.610
thing by X.

148
00:13:21.680 --> 00:13:29.870
Again that way I can capture this term in the equation here so that's really it.

149
00:13:29.890 --> 00:13:37.570
The only thing left to do is output these values and we're going to output this stuff as an array.

150
00:13:37.570 --> 00:13:40.040
I'll show you three ways we can do this.

151
00:13:40.110 --> 00:13:48.970
It's a little bit of a review so we can write return and p dot array open parentheses square brackets

152
00:13:49.690 --> 00:13:52.120
theta zero underscore slope.

153
00:13:52.630 --> 00:13:59.590
And because this is an array as well we just have to pull out the first value of it and then write a

154
00:13:59.590 --> 00:14:09.160
comma and then theta one underscore slope and then grab the first value of that as well.

155
00:14:09.160 --> 00:14:10.870
Now this is one way you can do it.

156
00:14:10.930 --> 00:14:16.860
We've calculated these two things separately so we can combine them into an array like so.

157
00:14:17.560 --> 00:14:26.830
But I'm going to comment this out and show you a second way so we can also return NDP don't append open

158
00:14:26.830 --> 00:14:35.980
parentheses the first argument is gonna be our theta zero underscore slope and then our second argument

159
00:14:36.130 --> 00:14:43.150
that's gonna be theta 1 underscore slope.

160
00:14:43.380 --> 00:14:45.430
So this is a second way we can do it.

161
00:14:45.630 --> 00:14:51.050
We can append this array to this one and return that as well.

162
00:14:51.090 --> 00:14:56.250
So that way we can combine these two pieces of data that were calculated separately and append them

163
00:14:56.580 --> 00:14:57.510
to each other.

164
00:14:57.720 --> 00:15:01.980
The last way I want to show you is with the CONCATENATE function.

165
00:15:01.980 --> 00:15:04.930
So this is also from num pi.

166
00:15:04.980 --> 00:15:13.230
I mean num pi dot concatenate parentheses and then another set of parentheses where we're gonna supply

167
00:15:13.830 --> 00:15:23.370
theta zero underscore slope comma theta one on the school slope and then we just have to supply how

168
00:15:23.370 --> 00:15:26.090
we're gonna concatenated namely along the roads.

169
00:15:26.130 --> 00:15:29.710
So axis equals zero.

170
00:15:29.820 --> 00:15:34.820
So these are three ways you can write the python code to achieve the very same output.

171
00:15:34.830 --> 00:15:38.370
Now it's time to run our gradient descent and actually call this function.

172
00:15:38.940 --> 00:15:41.610
So I didn't make any typos so let's do that now.

173
00:15:41.970 --> 00:15:43.910
This is where the rubber meets the road.

174
00:15:43.920 --> 00:15:53.070
As they say I'm going to get my multiplier to zero point zero one and I'm going to set my initial guesses.

175
00:15:53.070 --> 00:16:04.050
So my theta is equal to an NPR Ray where our initial guesses are two point nine comma two point nine

176
00:16:04.350 --> 00:16:08.560
all in square brackets and then our gradient descent is gonna look like this.

177
00:16:08.560 --> 00:16:14.220
It's gonna be for i in range when I run this a thousand times.

178
00:16:14.220 --> 00:16:21.210
Colon and then in the body of for loop we're gonna have some very terse Python code that calculates

179
00:16:21.210 --> 00:16:24.360
our gradient and then updates are theta as Array.

180
00:16:24.510 --> 00:16:25.530
All in one go.

181
00:16:25.590 --> 00:16:35.260
It's gonna be fetus is equal to fetus minus multiplier times Grant.

182
00:16:35.280 --> 00:16:37.170
This is where calling our function.

183
00:16:37.190 --> 00:16:38.550
Now we have to supply our data.

184
00:16:38.550 --> 00:16:38.830
Right.

185
00:16:38.850 --> 00:16:43.170
X underscore five comma Y on the scroll five.

186
00:16:43.170 --> 00:16:47.280
This was the data that we generated earlier and then last.

187
00:16:47.280 --> 00:16:58.860
Input is gonna be our status array just like that after a loop has run and uh print out the results.

188
00:16:59.050 --> 00:17:02.110
So this is where we can check if our thing actually works.

189
00:17:02.110 --> 00:17:05.950
And I'm going to find out if I'm had any typos along the way.

190
00:17:05.950 --> 00:17:17.890
So I'm going to print and I'm going to say the min occurs at theta zero colon comma.

191
00:17:18.160 --> 00:17:24.390
And this is gonna be Peter's square brackets zero because that's where our intercept is gonna live.

192
00:17:24.400 --> 00:17:36.730
It's gonna live at index 0 first value in our theta array and let's print out the minimum at theta one

193
00:17:36.730 --> 00:17:38.500
as well.

194
00:17:38.500 --> 00:17:46.690
So it's gonna that one's gonna be in a print statement min at theta one and then comma Theatre's square

195
00:17:46.690 --> 00:17:48.480
brackets one.

196
00:17:48.850 --> 00:17:53.190
And finally we're gonna print out our means squared error.

197
00:17:53.220 --> 00:17:57.180
So when I use that M S E function that we created earlier.

198
00:17:57.250 --> 00:18:08.230
So I'm gonna say print MSE is colon comma and S E this is a function call parentheses and then we have

199
00:18:08.230 --> 00:18:10.030
to supply two things here.

200
00:18:10.030 --> 00:18:12.250
Remember why underscore five.

201
00:18:12.370 --> 00:18:17.200
So the actual y values and then y hat what s y hat.

202
00:18:18.070 --> 00:18:31.150
Well after our loop runs it's gonna be they square brackets 0 plus theta has square brackets 1 times

203
00:18:31.780 --> 00:18:33.710
are X data.

204
00:18:33.760 --> 00:18:37.170
So X underscore 5.

205
00:18:37.390 --> 00:18:37.720
All right.

206
00:18:37.750 --> 00:18:43.710
So we've just written a whole bunch of code without having tested it in a little while.

207
00:18:43.720 --> 00:18:52.630
Let's see if it works and it shift into now and I'm pleasantly surprised.

208
00:18:52.780 --> 00:19:01.860
We get after a thousand iterations we get theta zero value of point eight five and theta one value of

209
00:19:02.040 --> 00:19:09.490
point 1 to 2 and mean square error of zero point nine five approximately.

210
00:19:09.630 --> 00:19:14.890
And this very much ties out with all the calculations we've done previously.

211
00:19:15.090 --> 00:19:17.130
So we've definitely done this correctly.

212
00:19:17.370 --> 00:19:22.620
We've worked out the partial derivatives of our cost function and then we've run our gradient descent

213
00:19:22.650 --> 00:19:30.180
algorithm and this gradient descent algorithm started pretty far off started at two point nine for both

214
00:19:30.270 --> 00:19:32.790
our theta zero and our theta one values.

215
00:19:32.790 --> 00:19:40.380
And then in that for loop having run a thousand times it converged on the values that minimized the

216
00:19:40.380 --> 00:19:43.370
means squared error that minimized our cost function.

217
00:19:43.380 --> 00:19:45.840
So this is brilliant.

218
00:19:45.840 --> 00:19:48.600
Now all that's left to do is to plot it.
