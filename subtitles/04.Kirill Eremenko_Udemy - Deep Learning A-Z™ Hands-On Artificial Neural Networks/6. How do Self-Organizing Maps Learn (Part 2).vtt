WEBVTT
1
1

00:00:00.630  -->  00:00:01.463
<v Voiceover>Hello and welcome back</v>
2

2

00:00:01.463  -->  00:00:03.080
to the course on deep learning.
3

3

00:00:03.080  -->  00:00:05.390
In today's tutorial, we will continue exploring
4

4

00:00:05.390  -->  00:00:07.910
how self-organizing maps learn.
5

5

00:00:07.910  -->  00:00:11.170
So, previously we stopped at this image.
6

6

00:00:11.170  -->  00:00:14.920
We found out how the best-matching units or BMUs
7

7

00:00:14.920  -->  00:00:18.510
are updated, and how everything around them is also updated
8

8

00:00:18.510  -->  00:00:21.420
when they wind rows which they match up to.
9

9

00:00:21.420  -->  00:00:23.090
And today we're going to have a look at
10

10

00:00:23.090  -->  00:00:24.860
a bit of a more sophisticated example
11

11

00:00:24.860  -->  00:00:27.210
where we have several best-matching units
12

12

00:00:27.210  -->  00:00:28.950
for instance five, in this case.
13

13

00:00:28.950  -->  00:00:30.450
So let's get started.
14

14

00:00:30.450  -->  00:00:33.950
So as we discussed, these BMUs are going to be updated
15

15

00:00:33.950  -->  00:00:37.340
to be even closer to the rows that they matched up to,
16

16

00:00:37.340  -->  00:00:39.950
and then each one of these BMUs is going to be assigned
17

17

00:00:39.950  -->  00:00:43.590
a area around it, and that area is going to be calculated
18

18

00:00:43.590  -->  00:00:47.340
through a radius and this is what it looks like.
19

19

00:00:47.340  -->  00:00:49.700
And by the way, here we can see that there's some values
20

20

00:00:49.700  -->  00:00:51.550
that don't fall under the radius,
21

21

00:00:51.550  -->  00:00:53.690
that usually doesn't happen in self-organizing maps
22

22

00:00:53.690  -->  00:00:55.780
this is just our visual example.
23

23

00:00:55.780  -->  00:00:58.880
Normally the radius at the start is selected as quite large
24

24

00:00:58.880  -->  00:01:03.270
so it encompasses nearly the whole self-organizing map.
25

25

00:01:03.270  -->  00:01:07.260
And the thing that happens here is, first of all
26

26

00:01:07.260  -->  00:01:12.110
all of the nodes that fall into these areas
27

27

00:01:12.110  -->  00:01:13.500
are updated, as we saw previously.
28

28

00:01:13.500  -->  00:01:16.610
So let's go through these from the top to the bottom.
29

29

00:01:16.610  -->  00:01:18.960
So, for example, for the purple node
30

30

00:01:18.960  -->  00:01:20.850
there we go, all of those nodes are updated.
31

31

00:01:20.850  -->  00:01:22.890
So first of all, the purple node itself
32

32

00:01:22.890  -->  00:01:26.060
is updated to be closer to that row that it matched up to
33

33

00:01:26.060  -->  00:01:29.410
as the BMU, and then all of those other nodes
34

34

00:01:29.410  -->  00:01:31.080
that formed that radius are updated
35

35

00:01:31.080  -->  00:01:33.710
so it drags them along with itself.
36

36

00:01:33.710  -->  00:01:35.710
Then for the blue node, same thing happens
37

37

00:01:35.710  -->  00:01:37.190
same thing happens for the green node
38

38

00:01:37.190  -->  00:01:38.877
same thing happens for the red node
39

39

00:01:38.877  -->  00:01:40.640
and same thing happens for the orange node.
40

40

00:01:40.640  -->  00:01:43.130
So they are all dragged closer and closer
41

41

00:01:43.130  -->  00:01:45.440
and there's a bit of a resistance
42

42

00:01:45.440  -->  00:01:47.406
or push and pull between them because
43

43

00:01:47.406  -->  00:01:50.850
for instance the blue node is dragging this row
44

44

00:01:50.850  -->  00:01:53.970
this node that way, the orange node is dragging it that way
45

45

00:01:53.970  -->  00:01:55.390
and the red node is dragging it that way
46

46

00:01:55.390  -->  00:01:57.480
green node is dragging it that way.
47

47

00:01:57.480  -->  00:01:59.800
It's a bit of a battle between them
48

48

00:01:59.800  -->  00:02:03.151
but that's normal, that's what happens in the SOM
49

49

00:02:03.151  -->  00:02:06.240
and that happens, for instance, in your...
50

50

00:02:06.240  -->  00:02:08.450
So you go through all of your rows and your dataset
51

51

00:02:08.450  -->  00:02:09.960
and all of these updates happen.
52

52

00:02:09.960  -->  00:02:12.011
And then, as you have a new epoch
53

53

00:02:12.011  -->  00:02:14.680
or so you go through your rows again
54

54

00:02:14.680  -->  00:02:17.930
what happens is a unique feature
55

55

00:02:17.930  -->  00:02:21.700
of the Kohonen Learning Algorithm is applied
56

56

00:02:21.700  -->  00:02:25.300
and what that is is that your radiuses actually shrink.
57

57

00:02:25.300  -->  00:02:27.950
So in your next situation, what happens is the radiuses
58

58

00:02:27.950  -->  00:02:31.690
become a bit smaller, and this time
59

59

00:02:31.690  -->  00:02:36.690
when you're going through your dataset, your BMUs
60

60

00:02:36.870  -->  00:02:41.674
they're actually pulling, not on as many nodes in your SOMs
61

61

00:02:41.674  -->  00:02:44.097
so this time only these nodes are pulled, these ones
62

62

00:02:44.097  -->  00:02:48.000
and these ones, these ones, these ones.
63

63

00:02:48.000  -->  00:02:50.500
And then, again, the radiuses shrink.
64

64

00:02:50.500  -->  00:02:54.460
And this time these nodes are pulled, these nodes are pulled
65

65

00:02:54.460  -->  00:02:56.530
these nodes are pulled, these nodes are pulled
66

66

00:02:56.530  -->  00:02:57.830
and these nodes are pulled.
67

67

00:02:57.830  -->  00:02:59.920
So the process becomes more and more accurate
68

68

00:02:59.920  -->  00:03:03.100
as you go through your dataset again and again and again
69

69

00:03:03.100  -->  00:03:06.550
and if at the start you were just trying to get
70

70

00:03:06.550  -->  00:03:10.700
all of your SOMs very close to your data
71

71

00:03:10.700  -->  00:03:13.260
and roughly on the surface of your data
72

72

00:03:13.260  -->  00:03:14.980
as we can see in this example.
73

73

00:03:14.980  -->  00:03:16.360
So at the start, you were just trying
74

74

00:03:16.360  -->  00:03:19.070
to get anywhere near your data.
75

75

00:03:19.070  -->  00:03:23.220
Then as you go through more and more and more iterations
76

76

00:03:23.220  -->  00:03:25.613
what happens is, you are adjusting your SOM
77

77

00:03:26.696  -->  00:03:30.080
in a more precise and a more laser-specific manner
78

78

00:03:30.080  -->  00:03:32.510
you're adjusting, okay little edge over here
79

79

00:03:32.510  -->  00:03:34.350
little bump over here, I need to...
80

80

00:03:34.350  -->  00:03:35.780
This needs to adjust a little bit
81

81

00:03:35.780  -->  00:03:37.300
this needs to adjust a little bit, so you're becoming
82

82

00:03:37.300  -->  00:03:40.750
more and more specific and targeted in your approach.
83

83

00:03:40.750  -->  00:03:45.673
And that's exactly how your SOM slowly but surely
84

84

00:03:46.820  -->  00:03:51.320
goes over your data and becomes kind of like a mosque
85

85

00:03:51.320  -->  00:03:54.620
for your data, as we can see in this example over here.
86

86

00:03:54.620  -->  00:03:56.850
And so, in our visual representation
87

87

00:03:56.850  -->  00:03:58.380
what that would look like is...
88

88

00:03:58.380  -->  00:04:01.050
Here is our data, and then through lots and lots
89

89

00:04:01.050  -->  00:04:04.980
of iterations, our SOM might look something like this
90

90

00:04:04.980  -->  00:04:08.700
where there's some battle between the different nodes
91

91

00:04:08.700  -->  00:04:11.270
and where different nodes should be assigned to
92

92

00:04:11.270  -->  00:04:14.890
should they be green or blue or purple or orange or red?
93

93

00:04:14.890  -->  00:04:17.890
And then, eventually we've come to settle it
94

94

00:04:17.890  -->  00:04:20.600
to some kind of representation.
95

95

00:04:20.600  -->  00:04:21.970
So hopefully that was useful and now
96

96

00:04:21.970  -->  00:04:24.060
you have a good intuitive understanding
97

97

00:04:24.060  -->  00:04:27.490
of how SOMs work and learn.
98

98

00:04:27.490  -->  00:04:29.730
And now let's go through a couple of things
99

99

00:04:29.730  -->  00:04:32.921
that are important to remember as takeaways
100

100

00:04:32.921  -->  00:04:34.940
from this tutorial.
101

101

00:04:34.940  -->  00:04:37.340
So, a couple of things that are important to know:
102

102

00:04:37.340  -->  00:04:40.620
Number one, SOMs retain topology of your input set.
103

103

00:04:40.620  -->  00:04:42.984
As we could see from the image
104

104

00:04:42.984  -->  00:04:46.700
where the map is slowly becoming like a mosque of your data
105

105

00:04:46.700  -->  00:04:48.410
your data might have some topology
106

106

00:04:48.410  -->  00:04:51.340
there might be some interrelations in your data.
107

107

00:04:51.340  -->  00:04:56.340
Well, the SOM does everything it possibly can to be
108

108

00:04:58.365  -->  00:05:00.310
as close to your data as possible
109

109

00:05:00.310  -->  00:05:03.410
and become like a mosque for your data
110

110

00:05:03.410  -->  00:05:05.450
and therefore it will retain the topology of your input set
111

111

00:05:05.450  -->  00:05:09.230
and that is very very important, very valuable for us
112

112

00:05:09.230  -->  00:05:12.620
in terms of understanding our dataset better.
113

113

00:05:12.620  -->  00:05:15.840
Next is that SOMs actually reveal correlations
114

114

00:05:15.840  -->  00:05:17.680
that are not easily identified.
115

115

00:05:17.680  -->  00:05:20.350
As you can imagine, if you have twenty or thirty or fifty
116

116

00:05:20.350  -->  00:05:24.210
or a hundred or even hundreds of columns in your dataset
117

117

00:05:24.210  -->  00:05:27.480
it can be very challenging to assess
118

118

00:05:27.480  -->  00:05:30.620
any kind of correlations or similarities
119

119

00:05:30.620  -->  00:05:32.570
that might be present in your dataset
120

120

00:05:32.570  -->  00:05:36.370
whereas a SOM can neatly put it all--
121

121

00:05:36.370  -->  00:05:39.140
analyze all that for you, and then put all of that for you
122

122

00:05:39.140  -->  00:05:41.890
into a map, and you will be able to see those things
123

123

00:05:41.890  -->  00:05:43.630
on the map very easily.
124

124

00:05:43.630  -->  00:05:47.690
SOMs classify data without supervision
125

125

00:05:47.690  -->  00:05:51.610
and this is an important aspect as we discussed.
126

126

00:05:51.610  -->  00:05:53.920
We are now venturing into the space
127

127

00:05:53.920  -->  00:05:57.500
of unsupervised deep learning, and as you can see SOMs
128

128

00:05:58.520  -->  00:06:01.270
don't actually need any labels.
129

129

00:06:01.270  -->  00:06:03.350
For instance, in convolutional neural networks
130

130

00:06:03.350  -->  00:06:06.685
we discussed that we need to train our dataset
131

131

00:06:06.685  -->  00:06:09.120
in order for it to recognize objects
132

132

00:06:09.120  -->  00:06:11.510
it has to first be told that these are cats
133

133

00:06:11.510  -->  00:06:13.330
and these are dogs, and then after lots and lots
134

134

00:06:13.330  -->  00:06:16.770
of iterations, it'll know how to recognize a cat from a dog
135

135

00:06:16.770  -->  00:06:18.850
whereas here we don't need any labels
136

136

00:06:18.850  -->  00:06:22.960
we don't need any supervision, the SOM will, on its own
137

137

00:06:22.960  -->  00:06:26.251
extract features, and very often it will extract features
138

138

00:06:26.251  -->  00:06:31.240
or show us features, dependencies, and similarities
139

139

00:06:31.240  -->  00:06:34.320
correlations which we aren't even expecting
140

140

00:06:34.320  -->  00:06:37.130
and therefore we'll be very very surprised.
141

141

00:06:37.130  -->  00:06:40.283
So that's a very important aspect of SOMs.
142

142

00:06:41.260  -->  00:06:43.190
They can be used in scenarios where
143

143

00:06:43.190  -->  00:06:45.530
you don't actually know what you're looking for
144

144

00:06:45.530  -->  00:06:47.420
but you know that you want to find
145

145

00:06:47.420  -->  00:06:49.970
any kind of correlations in your data.
146

146

00:06:49.970  -->  00:06:53.020
SOMs don't require a target vector
147

147

00:06:53.020  -->  00:06:56.210
that's the same thing as they don't need supervision
148

148

00:06:56.210  -->  00:06:59.260
and as a result of that, there is no
149

149

00:06:59.260  -->  00:07:00.260
process of backpropagation.
150

150

00:07:00.260  -->  00:07:04.326
There is no backpropagation in the training of a SOM
151

151

00:07:04.326  -->  00:07:07.077
as you remember in artificial neural networks
152

152

00:07:07.077  -->  00:07:09.860
the data would go through the network, we'd get a result
153

153

00:07:09.860  -->  00:07:12.297
we'd compare it to the target vector, we'd find the error
154

154

00:07:12.297  -->  00:07:14.190
and then we'd backpropagate that error
155

155

00:07:14.190  -->  00:07:16.020
through the metric to update the weights.
156

156

00:07:16.020  -->  00:07:17.760
Well here, none of that happens because
157

157

00:07:17.760  -->  00:07:20.020
we don't actually have a target vector.
158

158

00:07:20.020  -->  00:07:22.870
There is no error to backpropagate.
159

159

00:07:22.870  -->  00:07:25.280
And also there is no lateral connections
160

160

00:07:25.280  -->  00:07:27.260
between your output nodes.
161

161

00:07:27.260  -->  00:07:29.460
It's important to just remember that, as you can see
162

162

00:07:29.460  -->  00:07:32.630
we didn't actually need any connection between the nodes
163

163

00:07:32.630  -->  00:07:36.630
the only thing that happens between the nodes is that
164

164

00:07:36.630  -->  00:07:39.510
when you pull on one node, the other ones get pulled
165

165

00:07:40.950  -->  00:07:44.020
the close ones get pulled, but that is through the radius
166

166

00:07:44.020  -->  00:07:47.330
and the area that we outlined in yellow
167

167

00:07:47.330  -->  00:07:51.940
that allows us, facilitates this whole process.
168

168

00:07:51.940  -->  00:07:53.450
When we say that there's no connections
169

169

00:07:53.450  -->  00:07:55.440
no lateral connections between the output nodes means that
170

170

00:07:55.440  -->  00:07:58.750
there's no actual neural network type of connections
171

171

00:07:58.750  -->  00:08:01.310
there's no activation functions between them and so on.
172

172

00:08:01.310  -->  00:08:03.160
And what is important to mention is
173

173

00:08:03.160  -->  00:08:04.570
because sometimes you'll see images
174

174

00:08:04.570  -->  00:08:07.333
where the nodes are connected in the output
175

175

00:08:07.333  -->  00:08:09.800
there's a grid behind the nodes.
176

176

00:08:09.800  -->  00:08:12.100
The reason for that is they're just showing
177

177

00:08:12.100  -->  00:08:15.450
that these are indeed nodes on a SOM
178

178

00:08:16.640  -->  00:08:20.950
these are the output nodes, and they might just be
179

179

00:08:20.950  -->  00:08:22.970
nicely neatly organized into a map
180

180

00:08:22.970  -->  00:08:26.150
but that's pretty much it, there is no actual formulas
181

181

00:08:26.150  -->  00:08:28.960
or equations going on between those nodes.
182

182

00:08:28.960  -->  00:08:30.490
That's also important to remember.
183

183

00:08:30.490  -->  00:08:33.970
So there we go, that's how self-organizing maps learn.
184

184

00:08:33.970  -->  00:08:36.740
And if you'd like to study this a bit more
185

185

00:08:36.740  -->  00:08:40.050
with some soft introduction into mathematics
186

186

00:08:40.050  -->  00:08:41.910
the mathematics behind self-organizing maps
187

187

00:08:41.910  -->  00:08:44.482
as you can see is very straightforward, it's very simple.
188

188

00:08:44.482  -->  00:08:45.970
There's a couple of additional things
189

189

00:08:45.970  -->  00:08:50.970
about how the radius changes and how the weights are updated
190

190

00:08:51.360  -->  00:08:53.260
based on how close on the proximity
191

191

00:08:53.260  -->  00:08:55.960
to your best-matching unit, but generally
192

192

00:08:55.960  -->  00:08:58.960
the mathematics is, I would say, the simplest out of all
193

193

00:08:58.960  -->  00:09:02.410
of the neural networks that we're discussing in this course.
194

194

00:09:02.410  -->  00:09:04.940
So if you'd like to study the math a little bit
195

195

00:09:04.940  -->  00:09:08.366
and also get some introduction to programming SOMs
196

196

00:09:08.366  -->  00:09:12.690
a great blog, or several blogs actually, on this topic
197

197

00:09:12.690  -->  00:09:15.610
are available at ai-junkie dot com
198

198

00:09:15.610  -->  00:09:19.030
so here's a link by Mat Buckland, not sure about the year
199

199

00:09:19.030  -->  00:09:21.267
if it's 2004 or not, and the post is called
200

200

00:09:21.267  -->  00:09:24.200
"Kohonen's Self Organizing Feature Maps"
201

201

00:09:24.200  -->  00:09:28.480
Great read, and you will get a very gentle introduction
202

202

00:09:28.480  -->  00:09:31.050
into self-organizing maps from there.
203

203

00:09:31.050  -->  00:09:32.760
So hopefully you enjoyed today's tutorial
204

204

00:09:32.760  -->  00:09:34.880
and I look forward to seeing you next time.
205

205

00:09:34.880  -->  00:09:36.833
Until then, enjoy deep learning.
