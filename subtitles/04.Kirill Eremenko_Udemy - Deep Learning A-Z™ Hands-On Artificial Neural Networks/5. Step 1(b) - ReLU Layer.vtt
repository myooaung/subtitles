WEBVTT
1
1

00:00:00.550  -->  00:00:01.383
<v Instructor>Hello and welcome back</v>
2

2

00:00:01.383  -->  00:00:02.870
to the course on deep learning.
3

3

00:00:02.870  -->  00:00:04.250
Today we're talking about ReLU,
4

4

00:00:04.250  -->  00:00:06.130
which is a Rectified Linear Unit,
5

5

00:00:06.130  -->  00:00:08.710
and this is an additional step
6

6

00:00:08.710  -->  00:00:12.100
on top of our convolution step.
7

7

00:00:12.100  -->  00:00:14.500
So it's not a separate big step, it's a small step.
8

8

00:00:14.500  -->  00:00:16.140
It's step 1B, basically.
9

9

00:00:16.140  -->  00:00:18.140
And what is going on here?
10

10

00:00:18.140  -->  00:00:19.977
Well, we have our input image
11

11

00:00:19.977  -->  00:00:22.870
and we have our Convolution Layer, which we've discussed,
12

12

00:00:22.870  -->  00:00:26.300
and then on top of that, we're going to apply,
13

13

00:00:26.300  -->  00:00:31.030
wait for it, our favorite Rectifier function.
14

14

00:00:31.030  -->  00:00:33.330
And you're already familiar with the Rectified function
15

15

00:00:33.330  -->  00:00:38.330
from the previous section on Artificial Neural Networks,
16

16

00:00:38.700  -->  00:00:43.700
and in our, so sometimes authors or instructors separate
17

17

00:00:46.500  -->  00:00:48.940
the Convolution and the Rectifier as two separate steps.
18

18

00:00:48.940  -->  00:00:50.550
In our examples, we're just going
19

19

00:00:50.550  -->  00:00:55.200
to consider them just one big step
20

20

00:00:55.200  -->  00:00:57.130
for second Convolution, then the Rectifier.
21

21

00:00:57.130  -->  00:01:00.220
And the reason why we're apply the Rectifier,
22

22

00:01:00.220  -->  00:01:04.260
is because we want to increase non-linearity in our image,
23

23

00:01:04.260  -->  00:01:08.020
or in our network, in our Convolutional neural network,
24

24

00:01:08.020  -->  00:01:12.540
and a Rectifier acts as that filter,
25

25

00:01:12.540  -->  00:01:15.740
or acts as that function, which breaks up linearity.
26

26

00:01:15.740  -->  00:01:18.650
And the reason why we want to increase non-linearity
27

27

00:01:18.650  -->  00:01:23.130
in our network is because images themselves
28

28

00:01:23.130  -->  00:01:24.870
are highly non-linear, especially
29

29

00:01:24.870  -->  00:01:28.750
if you're recognizing different objects next to each other,
30

30

00:01:28.750  -->  00:01:31.220
or just on this background and stuff like that,
31

31

00:01:31.220  -->  00:01:34.490
like the image is going to have lots of non-linear elements,
32

32

00:01:34.490  -->  00:01:35.960
and the transition between pixels,
33

33

00:01:35.960  -->  00:01:37.950
adjacent pixels, is often gonna be non-linear.
34

34

00:01:37.950  -->  00:01:40.450
That's because of borders, there's different colors,
35

35

00:01:40.450  -->  00:01:44.210
there's different elements in your images.
36

36

00:01:44.210  -->  00:01:45.043
But at the same time,
37

37

00:01:45.043  -->  00:01:46.000
when we're applying mathematical operations
38

38

00:01:46.000  -->  00:01:51.000
such as convolution, and running this feature detection
39

39

00:01:51.100  -->  00:01:53.750
to create our feature maps, we risk
40

40

00:01:53.750  -->  00:01:57.180
that we might create something linear,
41

41

00:01:57.180  -->  00:01:59.890
and therefore we need to break up the linearity.
42

42

00:01:59.890  -->  00:02:02.480
So, let's have a look at an example.
43

43

00:02:02.480  -->  00:02:05.860
Here is a imagine, an original image.
44

44

00:02:05.860  -->  00:02:10.860
Now when we apply a feature detector to this image
45

45

00:02:11.260  -->  00:02:13.190
we get something like this.
46

46

00:02:13.190  -->  00:02:14.990
So you can see here that black is negative,
47

47

00:02:14.990  -->  00:02:15.900
white is positive values.
48

48

00:02:15.900  -->  00:02:19.100
Well, when you apply a feature detector
49

49

00:02:19.100  -->  00:02:22.630
to a proper image, which as not just zeros and ones,
50

50

00:02:22.630  -->  00:02:23.780
but has lots of different values
51

51

00:02:23.780  -->  00:02:26.310
and you apply, as we saw previously,
52

52

00:02:26.310  -->  00:02:28.970
feature detectors can have negative values in themselves.
53

53

00:02:28.970  -->  00:02:30.960
Sometimes you will get negative values,
54

54

00:02:30.960  -->  00:02:33.640
and here their black ones are negative,
55

55

00:02:33.640  -->  00:02:34.690
white ones are positive.
56

56

00:02:34.690  -->  00:02:39.690
And what a Rectified linear unit function does,
57

57

00:02:41.390  -->  00:02:44.590
is it removes all the black, right?
58

58

00:02:44.590  -->  00:02:46.450
Anything below zero it turns into zero,
59

59

00:02:46.450  -->  00:02:49.170
and so from this, it turns to this, right?
60

60

00:02:49.170  -->  00:02:53.610
And so it's pretty hard to see what exactly
61

61

00:02:53.610  -->  00:02:58.610
is the benefit in terms of breaking up linearity.
62

62

00:02:59.280  -->  00:03:00.920
I'll try to explain.
63

63

00:03:00.920  -->  00:03:04.580
I'll try to show an example on this image,
64

64

00:03:04.580  -->  00:03:05.991
but at the end of the day
65

65

00:03:05.991  -->  00:03:08.180
this is a very mathematical concept,
66

66

00:03:08.180  -->  00:03:11.780
and we would have to go into a lot of math to really explain
67

67

00:03:11.780  -->  00:03:13.730
what is going on, but try this, have a look.
68

68

00:03:13.730  -->  00:03:16.180
So, for instance, let's look
69

69

00:03:16.180  -->  00:03:17.990
at this building here, all right?
70

70

00:03:17.990  -->  00:03:19.693
So this is a building on its own.
71

71

00:03:20.630  -->  00:03:23.400
Then you can see this shadow, this black part,
72

72

00:03:23.400  -->  00:03:24.530
this shadow right here.
73

73

00:03:24.530  -->  00:03:28.790
Well, you see that it's white, the reflection of the light,
74

74

00:03:28.790  -->  00:03:31.210
and then it's a gray, and then it gets darker,
75

75

00:03:31.210  -->  00:03:33.210
and then it gets darker, all right?
76

76

00:03:33.210  -->  00:03:35.800
So, when when we take it out, we take out that black part.
77

77

00:03:35.800  -->  00:03:38.160
So think of it in terms of linearity, right?
78

78

00:03:38.160  -->  00:03:42.010
So it looks like when you go from white to gray,
79

79

00:03:42.010  -->  00:03:43.880
the next step would be black, right?
80

80

00:03:43.880  -->  00:03:44.870
The next step would be black.
81

81

00:03:44.870  -->  00:03:49.560
It's a linear progression from bright to dark,
82

82

00:03:49.560  -->  00:03:53.490
and therefore this is kind of like a linear situation.
83

83

00:03:53.490  -->  00:03:56.570
When you take out the black, you break up the linearity.
84

84

00:03:56.570  -->  00:03:57.940
Let's try another one.
85

85

00:03:57.940  -->  00:03:59.070
Let's have a look here.
86

86

00:03:59.070  -->  00:04:02.028
And at the same time, it's still that same building, right?
87

87

00:04:02.028  -->  00:04:07.028
It's not like you're blending two buildings
88

88

00:04:07.670  -->  00:04:09.730
into each other, but that is secondary.
89

89

00:04:09.730  -->  00:04:12.110
The main point is breaking up the linearity.
90

90

00:04:12.110  -->  00:04:13.480
So let's have a look here, same thing.
91

91

00:04:13.480  -->  00:04:18.480
So, you see white, gray, black, gray, white,
92

92

00:04:19.460  -->  00:04:20.950
and when you break it up,
93

93

00:04:20.950  -->  00:04:22.430
you don't have that anymore, right?
94

94

00:04:22.430  -->  00:04:25.330
You don't have that profession, the gradual progression
95

95

00:04:25.330  -->  00:04:29.610
that you just have like an abrupt change,
96

96

00:04:29.610  -->  00:04:33.450
and that helps introduce non-linearity into you image.
97

97

00:04:33.450  -->  00:04:37.676
So it's a very rough explanation, very, kind of like,
98

98

00:04:37.676  -->  00:04:42.600
on the fingers explanation, rather than technical,
99

99

00:04:42.600  -->  00:04:45.260
but hopefully it kind of helps you understand
100

100

00:04:45.260  -->  00:04:47.280
a bit better what we're talking about here.
101

101

00:04:47.280  -->  00:04:49.700
So here again, you can see white, gray,
102

102

00:04:49.700  -->  00:04:50.890
it's a better example even.
103

103

00:04:50.890  -->  00:04:54.350
See bright, darker, darker, darker,
104

104

00:04:54.350  -->  00:04:55.590
darker, darker, darker, darker.
105

105

00:04:55.590  -->  00:04:58.170
So this part looks like it's linear,
106

106

00:04:58.170  -->  00:04:59.720
then you break it up like that.
107

107

00:05:00.650  -->  00:05:04.370
Again, so this is a very rough explanation.
108

108

00:05:04.370  -->  00:05:05.590
It's not absolutely perfect,
109

109

00:05:05.590  -->  00:05:08.610
but at least it gives you some idea of what's going on.
110

110

00:05:08.610  -->  00:05:11.743
But if you'd like to learn more, there's a good paper.
111

111

00:05:12.880  -->  00:05:15.770
There's always a paper, this one is by C. C. Jay Kuo,
112

112

00:05:15.770  -->  00:05:17.880
from the University of California,
113

113

00:05:17.880  -->  00:05:21.040
and it's called Understanding Convolutional Neural Networks
114

114

00:05:21.040  -->  00:05:23.060
with A Mathematical Model.
115

115

00:05:23.060  -->  00:05:26.350
And basically there he answers two questions,
116

116

00:05:26.350  -->  00:05:28.283
and you need to just look at the first one,
117

117

00:05:28.283  -->  00:05:31.830
and the questions why a non-linear activation function
118

118

00:05:31.830  -->  00:05:33.540
is essential at the filter output
119

119

00:05:33.540  -->  00:05:36.090
of all intermediate layers.
120

120

00:05:36.090  -->  00:05:39.890
So that kind of explains it in a bit more detail,
121

121

00:05:39.890  -->  00:05:42.590
both in terms of intuition and mostly
122

122

00:05:42.590  -->  00:05:44.220
in terms of mathematics.
123

123

00:05:44.220  -->  00:05:45.270
So that's an interesting paper
124

124

00:05:45.270  -->  00:05:47.150
where you can get some more additional information
125

125

00:05:47.150  -->  00:05:48.870
on this topic, and if you really
126

126

00:05:48.870  -->  00:05:53.260
want to dig in and explore some cool stuff here,
127

127

00:05:53.260  -->  00:05:55.590
then there's another paper that you might be interested in.
128

128

00:05:55.590  -->  00:05:57.690
It's called Delving Deep
129

129

00:05:57.690  -->  00:06:00.790
into Rectifiers: Surpassing Human-Level Performance
130

130

00:06:00.790  -->  00:06:02.800
on ImageNet classification,
131

131

00:06:02.800  -->  00:06:07.640
and here the authors, Kaiming He and others
132

132

00:06:07.640  -->  00:06:09.290
from Microsoft Research.
133

133

00:06:09.290  -->  00:06:13.650
They propose a different type
134

134

00:06:13.650  -->  00:06:17.540
of Rectified Linear Unit function.
135

135

00:06:17.540  -->  00:06:20.500
They propose the Parametric Rectified Linear Unit,
136

136

00:06:20.500  -->  00:06:21.930
which you see here on the right,
137

137

00:06:21.930  -->  00:06:24.200
and they argue that is delivers better results
138

138

00:06:24.200  -->  00:06:26.640
without sacrificing performance.
139

139

00:06:26.640  -->  00:06:28.280
So interesting read if you'd like
140

140

00:06:28.280  -->  00:06:30.370
to get a bit more into this topic.
141

141

00:06:30.370  -->  00:06:31.780
And that's all for today.
142

142

00:06:31.780  -->  00:06:35.229
The ReLU Layer is pretty simple, pretty straight forward,
143

143

00:06:35.229  -->  00:06:37.740
just applying the Rectified function.
144

144

00:06:37.740  -->  00:06:39.140
And I look forward to seeing you next time,
145

145

00:06:39.140  -->  00:06:40.853
until then, enjoy deep learning.
