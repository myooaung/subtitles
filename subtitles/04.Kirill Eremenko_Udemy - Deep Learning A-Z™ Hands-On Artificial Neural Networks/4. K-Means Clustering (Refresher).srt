1
1

00:00:00,290  -->  00:00:01,670
<v Instructor>Hello and welcome back</v>
2

2

00:00:01,670  -->  00:00:03,390
to the course on machine learning.
3

3

00:00:03,390  -->  00:00:04,730
In this section we're talking about
4

4

00:00:04,730  -->  00:00:07,790
the K-Means clustering algorithm.
5

5

00:00:07,790  -->  00:00:09,540
In this tutorial we're going to talk about
6

6

00:00:09,540  -->  00:00:11,630
the intuition behind K-Means.
7

7

00:00:11,630  -->  00:00:15,260
K-Means is a algorithm that allows you to cluster your data
8

8

00:00:15,260  -->  00:00:18,230
and as we'll see it's a very convenient tool
9

9

00:00:18,230  -->  00:00:21,670
for discovering categories or groups in your data set
10

10

00:00:21,670  -->  00:00:24,470
that you wouldn't have otherwise thought of yourself.
11

11

00:00:24,470  -->  00:00:27,720
In this section, or in this specific tutorial,
12

12

00:00:27,720  -->  00:00:31,570
we'll learn how to understand K-Means on an intuitive level,
13

13

00:00:31,570  -->  00:00:33,670
and we'll see an example of how it works.
14

14

00:00:33,670  -->  00:00:36,730
So, lets dive straight into it, lets make this topic,
15

15

00:00:36,730  -->  00:00:39,150
this complex topic, simple.
16

16

00:00:39,150  -->  00:00:41,680
All right, so here we've got a scatterplot
17

17

00:00:41,680  -->  00:00:43,650
and how does this scatterplot come to be?
18

18

00:00:43,650  -->  00:00:46,940
Well, lets imagine that we've got two variables
19

19

00:00:46,940  -->  00:00:49,730
in our data set and we decided to plot those two variables
20

20

00:00:49,730  -->  00:00:51,080
on the X and Y axis.
21

21

00:00:51,080  -->  00:00:52,930
So, here we go, that's one variable,
22

22

00:00:52,930  -->  00:00:54,970
and that's the other one, and this is how
23

23

00:00:54,970  -->  00:00:57,860
our observations are configured
24

24

00:00:57,860  -->  00:01:00,110
according to these two variables.
25

25

00:01:00,110  -->  00:01:02,850
So, the question here is can we identify
26

26

00:01:02,850  -->  00:01:05,030
certain groups among our variables,
27

27

00:01:05,030  -->  00:01:07,280
and how would we go about doing it.
28

28

00:01:07,280  -->  00:01:10,800
Can we maybe identify this as one, this is one group,
29

29

00:01:10,800  -->  00:01:13,007
and this is a second group, or maybe this is one group,
30

30

00:01:13,007  -->  00:01:15,030
the second, or maybe there's four groups,
31

31

00:01:15,030  -->  00:01:16,830
or five groups, maybe there's three groups.
32

32

00:01:16,830  -->  00:01:19,230
How do we identify the number of groups?
33

33

00:01:19,230  -->  00:01:22,080
How do we identify the groups themselves?
34

34

00:01:22,080  -->  00:01:24,040
So, what the K-Means does for you
35

35

00:01:24,040  -->  00:01:27,370
is it takes out the complexity
36

36

00:01:27,370  -->  00:01:29,660
from this decision making process
37

37

00:01:29,660  -->  00:01:32,100
and allows you to very easily identify
38

38

00:01:32,100  -->  00:01:33,640
those clusters are actually called
39

39

00:01:33,640  -->  00:01:36,740
clusters of data points in your data set.
40

40

00:01:36,740  -->  00:01:39,190
So, there we go, in this case we have three clusters.
41

41

00:01:39,190  -->  00:01:41,150
And, of course, this a very simplified example.
42

42

00:01:41,150  -->  00:01:43,570
We only have two dimensions here, so two variables.
43

43

00:01:43,570  -->  00:01:46,550
K-Means can work with a multi dimensional object,
44

44

00:01:46,550  -->  00:01:48,990
so it can work with three, four, five, 10 dimensions.
45

45

00:01:48,990  -->  00:01:51,730
This example is just for illustrational purposes
46

46

00:01:51,730  -->  00:01:53,760
so that we can visually see what's going on,
47

47

00:01:53,760  -->  00:01:55,880
but in reality there can be as many as 10,
48

48

00:01:55,880  -->  00:01:57,850
a hundred, any number of variables,
49

49

00:01:57,850  -->  00:02:00,540
and K-Means will do that complex calculation for you.
50

50

00:02:00,540  -->  00:02:03,620
So, its an algorithm designed to find
51

51

00:02:03,620  -->  00:02:05,180
these clusters for you.
52

52

00:02:05,180  -->  00:02:07,310
And, today we're going to find out a bit more
53

53

00:02:07,310  -->  00:02:10,310
on how the K-Means algorithm works.
54

54

00:02:10,310  -->  00:02:12,200
So, what is the step-by-step process?
55

55

00:02:12,200  -->  00:02:13,280
This is my favorite part.
56

56

00:02:13,280  -->  00:02:15,480
We're going to break it down to such a simple
57

57

00:02:15,480  -->  00:02:17,620
step-by-step process that it will be
58

58

00:02:17,620  -->  00:02:20,230
extremely intuitive and simple to understand.
59

59

00:02:20,230  -->  00:02:22,730
All right, so step one for the K-Means algorithm
60

60

00:02:22,730  -->  00:02:25,490
is to choose the number of clusters, K.
61

61

00:02:25,490  -->  00:02:27,870
And, we'll talk more about how to select
62

62

00:02:27,870  -->  00:02:29,280
the optimal number of clusters
63

63

00:02:29,280  -->  00:02:31,140
further down in this section of the course,
64

64

00:02:31,140  -->  00:02:33,290
but for now lets imagine that we've agreed
65

65

00:02:33,290  -->  00:02:35,850
on a number of clusters for a certain challenge,
66

66

00:02:35,850  -->  00:02:38,460
and lets say we've agreed that there's three clusters,
67

67

00:02:38,460  -->  00:02:40,960
or there's five clusters, or two clusters.
68

68

00:02:40,960  -->  00:02:43,960
And once you've done that then you proceed to step two,
69

69

00:02:43,960  -->  00:02:47,250
which is to select at random K points
70

70

00:02:47,250  -->  00:02:49,660
which will be the centroids of your clusters.
71

71

00:02:49,660  -->  00:02:52,090
And, not necessarily these points have to
72

72

00:02:52,090  -->  00:02:53,220
be from your dataset.
73

73

00:02:53,220  -->  00:02:55,040
So, as you saw we had a scatterplot.
74

74

00:02:55,040  -->  00:02:57,510
You could select any points on that scatterplot.
75

75

00:02:57,510  -->  00:02:59,630
They don't have part of the observations
76

76

00:02:59,630  -->  00:03:00,463
that are plotted on the scatterplot,
77

77

00:03:00,463  -->  00:03:04,700
they can be any random X and Y values on your scatterplot.
78

78

00:03:04,700  -->  00:03:08,130
As long as you just select a certain number of centroids
79

79

00:03:08,130  -->  00:03:10,270
that are going to equate to the number of clusters
80

80

00:03:10,270  -->  00:03:12,180
that you have decided upon.
81

81

00:03:12,180  -->  00:03:14,480
Again, we'll see this in practice just now.
82

82

00:03:14,480  -->  00:03:16,480
Then, you move on to step three,
83

83

00:03:16,480  -->  00:03:21,210
where you assign each data point to the closest centroid.
84

84

00:03:21,210  -->  00:03:23,690
And, that'll form K clusters right away immediately.
85

85

00:03:23,690  -->  00:03:25,370
So, that's your starting clusters,
86

86

00:03:25,370  -->  00:03:27,490
and then there's gonna be an iterative process
87

87

00:03:27,490  -->  00:03:28,883
to refine those clusters.
88

88

00:03:29,760  -->  00:03:30,620
We'll see this in action,
89

89

00:03:30,620  -->  00:03:32,870
but basically so you just check for every point
90

90

00:03:32,870  -->  00:03:34,990
in your dataset which, out of the centroids
91

91

00:03:34,990  -->  00:03:36,500
that you've identified in step two,
92

92

00:03:36,500  -->  00:03:41,100
which of them is the closest to your specific data point,
93

93

00:03:41,100  -->  00:03:43,610
then that's where that data point will be assigned to.
94

94

00:03:43,610  -->  00:03:46,750
And, closest is a kind of vague term here
95

95

00:03:46,750  -->  00:03:49,740
because what kind of distance you're measuring it by.
96

96

00:03:49,740  -->  00:03:52,970
Is it Euclidean distance, is it some other sort of distance?
97

97

00:03:52,970  -->  00:03:55,180
That can be better defined in your business challenge,
98

98

00:03:55,180  -->  00:03:56,860
but for the purposes of these tutorials
99

99

00:03:56,860  -->  00:03:59,610
we are going to talk about Euclidean distances,
100

100

00:03:59,610  -->  00:04:02,610
and so that's basically geometrical distances
101

101

00:04:02,610  -->  00:04:04,830
in simple terms.
102

102

00:04:04,830  -->  00:04:07,320
All right, step four where you proceed to us.
103

103

00:04:07,320  -->  00:04:10,220
Step three is to compute and place the new centroid
104

104

00:04:10,220  -->  00:04:13,680
of each cluster, and we'll see how that's done just now.
105

105

00:04:13,680  -->  00:04:14,980
So, once you've found your clusters
106

106

00:04:14,980  -->  00:04:16,870
you recalculate the centroids.
107

107

00:04:16,870  -->  00:04:19,000
Again, it's gonna be much easier to understand this
108

108

00:04:19,000  -->  00:04:20,620
when we look at an example.
109

109

00:04:20,620  -->  00:04:23,290
Then, in step five you reassign each data point
110

110

00:04:23,290  -->  00:04:25,200
to the new closest centroid,
111

111

00:04:25,200  -->  00:04:28,750
so basically you perform step three again
112

112

00:04:28,750  -->  00:04:30,530
but we will just call it step five here,
113

113

00:04:30,530  -->  00:04:32,330
and then if any reassignment took place
114

114

00:04:32,330  -->  00:04:33,640
you repeat step four.
115

115

00:04:33,640  -->  00:04:37,420
So, it becomes an iterative process of step four, step five,
116

116

00:04:37,420  -->  00:04:39,410
and if no reassignment took place
117

117

00:04:39,410  -->  00:04:41,530
you go to finish and your model is ready.
118

118

00:04:41,530  -->  00:04:43,260
I know this might seem a bit complex,
119

119

00:04:43,260  -->  00:04:46,370
but lets go through a visual example now
120

120

00:04:46,370  -->  00:04:49,360
so that we understand this on a very intuitive level.
121

121

00:04:49,360  -->  00:04:51,620
Then, you can always reference this slide
122

122

00:04:51,620  -->  00:04:54,330
or this step-by-step rule further down
123

123

00:04:54,330  -->  00:04:57,040
when you're actually performing K-Means clustering,
124

124

00:04:57,040  -->  00:04:59,680
or when you want to understand how its happening,
125

125

00:04:59,680  -->  00:05:02,220
or what's happening in the background of K-Means clustering.
126

126

00:05:02,220  -->  00:05:05,260
So, this slide will be a great reference point
127

127

00:05:05,260  -->  00:05:08,460
after we discuss this visual exercise
128

128

00:05:08,460  -->  00:05:09,780
that we're going to have just now.
129

129

00:05:09,780  -->  00:05:11,950
So, lets move on.
130

130

00:05:11,950  -->  00:05:14,630
All right, so there is our scatterplot,
131

131

00:05:14,630  -->  00:05:18,150
and here we've got the observations in our dataset.
132

132

00:05:18,150  -->  00:05:20,240
They're plotted against two variables.
133

133

00:05:20,240  -->  00:05:22,800
And, right away, the first question is
134

134

00:05:22,800  -->  00:05:27,240
can you visually just quickly identify the final clusters
135

135

00:05:27,240  -->  00:05:29,190
that you think that we'll end up with?
136

136

00:05:29,190  -->  00:05:30,230
It's pretty tough isn't it?
137

137

00:05:30,230  -->  00:05:32,860
And, that is, even when we just have two variables,
138

138

00:05:32,860  -->  00:05:36,450
imagine how complex this situation or this challenge
139

139

00:05:36,450  -->  00:05:38,850
would be if we had three variables, or five variables.
140

140

00:05:38,850  -->  00:05:40,890
We wouldn't even be able to plot
141

141

00:05:40,890  -->  00:05:43,770
a five dimensional scatterplot like that.
142

142

00:05:43,770  -->  00:05:46,530
So, that's where K-Means clustering comes in to play,
143

143

00:05:46,530  -->  00:05:49,330
and that's where this algorithm will help us
144

144

00:05:49,330  -->  00:05:50,720
simplify the process.
145

145

00:05:50,720  -->  00:05:54,260
So, lets see how it will perform in action.
146

146

00:05:54,260  -->  00:05:58,000
In this case, we're actually going to manually perform
147

147

00:05:58,000  -->  00:06:00,290
the K-Means clustering algorithm.
148

148

00:06:00,290  -->  00:06:02,250
So, something that would normally be done
149

149

00:06:02,250  -->  00:06:05,210
by tools of your choice, such as R or Python,
150

150

00:06:05,210  -->  00:06:07,440
or any other tool, but, in this case
151

151

00:06:07,440  -->  00:06:09,870
just so that we can get very comfortable
152

152

00:06:09,870  -->  00:06:11,960
with this algorithm, we're going to do it manually
153

153

00:06:11,960  -->  00:06:13,670
and see exactly how it works.
154

154

00:06:13,670  -->  00:06:15,300
All right, so lets go through the steps.
155

155

00:06:15,300  -->  00:06:18,760
Step one, choose the number of clusters, K,
156

156

00:06:18,760  -->  00:06:21,300
and lets say we somehow identified
157

157

00:06:21,300  -->  00:06:24,100
that the optimal number of clusters is equal to two.
158

158

00:06:24,100  -->  00:06:25,840
Again, we'll discuss how to find
159

159

00:06:25,840  -->  00:06:27,960
the optimal number of clusters further down in the section,
160

160

00:06:27,960  -->  00:06:30,140
but for now lets agree that for this challenge
161

161

00:06:30,140  -->  00:06:32,050
it's just two clusters.
162

162

00:06:32,050  -->  00:06:35,180
All right, and then step two, select at random
163

163

00:06:35,180  -->  00:06:38,620
K points which will be the centroids of your clusters,
164

164

00:06:38,620  -->  00:06:40,450
and not necessarily from your dataset.
165

165

00:06:40,450  -->  00:06:42,580
So, what that means is that they can be
166

166

00:06:42,580  -->  00:06:44,970
actual points in your dataset,
167

167

00:06:44,970  -->  00:06:47,820
or they can just be random points on your scatterplot.
168

168

00:06:47,820  -->  00:06:49,920
It doesn't matter where you select them.
169

169

00:06:49,920  -->  00:06:51,837
So, lets say we selected these two centroids,
170

170

00:06:51,837  -->  00:06:54,250
the blue centroid, and the red centroid.
171

171

00:06:54,250  -->  00:06:57,100
Next step is step three, assign each data point
172

172

00:06:57,100  -->  00:07:00,520
to the closest centroid and that'll form K clusters.
173

173

00:07:00,520  -->  00:07:03,243
So, basically for every data point in our data set
174

174

00:07:03,243  -->  00:07:06,260
what we need to do is identify which of the two centroids,
175

175

00:07:06,260  -->  00:07:07,880
blue or red, is the closest.
176

176

00:07:07,880  -->  00:07:09,810
So, lets say for this data point,
177

177

00:07:09,810  -->  00:07:11,330
obviously, blue is closer than red
178

178

00:07:11,330  -->  00:07:14,040
so it would be assigned to the blue cluster.
179

179

00:07:14,040  -->  00:07:16,570
For this data point, blue is again closer than red.
180

180

00:07:16,570  -->  00:07:19,180
For this data point, red is closer than blue.
181

181

00:07:19,180  -->  00:07:22,020
So, we could keep going like that for every data point
182

182

00:07:22,020  -->  00:07:25,230
and assign it to one of the two centroids,
183

183

00:07:25,230  -->  00:07:27,890
but we're going to use a quick hack here.
184

184

00:07:27,890  -->  00:07:31,020
We're going to use something that we learned from geometry.
185

185

00:07:31,020  -->  00:07:32,990
So, what we're going to do is we're going to just
186

186

00:07:32,990  -->  00:07:35,920
connect these two centroids with a line, like that,
187

187

00:07:35,920  -->  00:07:37,420
and then we'll find the center of the line,
188

188

00:07:37,420  -->  00:07:40,230
which is over here, and we'll put a perpendicular line
189

189

00:07:40,230  -->  00:07:42,730
exactly through that central like that.
190

190

00:07:42,730  -->  00:07:46,570
And, so from geometry we know, from high school geometry,
191

191

00:07:46,570  -->  00:07:50,500
or maybe at university, or even if you didn't have geometry,
192

192

00:07:50,500  -->  00:07:52,920
it's a very straightforward concept
193

193

00:07:52,920  -->  00:07:55,290
that any point on this green line
194

194

00:07:55,290  -->  00:07:57,300
is equidistant from the blue and the red.
195

195

00:07:57,300  -->  00:08:00,550
So, if I take this point, it'll be the same distance
196

196

00:08:00,550  -->  00:08:03,160
to the blue centroid and to the red centroid.
197

197

00:08:03,160  -->  00:08:05,170
If I take this point, it'll be same distance
198

198

00:08:05,170  -->  00:08:06,180
to blue and the red.
199

199

00:08:06,180  -->  00:08:09,280
So, this whole line is equidistant to the blue and the red,
200

200

00:08:09,280  -->  00:08:10,780
and that's how we constructed it.
201

201

00:08:10,780  -->  00:08:13,000
And so, based on that, it now is obvious
202

202

00:08:13,000  -->  00:08:16,300
that any of our points on this scatterplot
203

203

00:08:16,300  -->  00:08:18,450
above the green line is closer to the blue,
204

204

00:08:18,450  -->  00:08:21,560
and any of the points below the green line
205

205

00:08:21,560  -->  00:08:22,980
is closer to the red.
206

206

00:08:22,980  -->  00:08:25,290
And, that's how we're going to just color our points.
207

207

00:08:25,290  -->  00:08:28,470
It's a very quick way of coloring them.
208

208

00:08:28,470  -->  00:08:30,410
Again, you could totally go through every point
209

209

00:08:30,410  -->  00:08:34,130
and individually identify which centroid is closest,
210

210

00:08:34,130  -->  00:08:36,380
but, just for our sake while we're doing this exercise,
211

211

00:08:36,380  -->  00:08:40,580
it's gonna be faster if we apply this quick hack method.
212

212

00:08:40,580  -->  00:08:43,040
So, there we go, we've colored our centroids,
213

213

00:08:43,040  -->  00:08:45,490
or our points on our scatterplot.
214

214

00:08:45,490  -->  00:08:48,150
Before we continue, I do want to mention something here,
215

215

00:08:48,150  -->  00:08:51,520
that closest is a very, even though it seems
216

216

00:08:51,520  -->  00:08:53,590
straightforward, it's quite an ambiguous term,
217

217

00:08:53,590  -->  00:08:56,030
because closest, when you're visualizing things
218

218

00:08:56,030  -->  00:08:58,680
on a scatter plot, yes it's pretty straightforward
219

219

00:08:58,680  -->  00:09:01,850
that it's the distance, right, but at the same time
220

220

00:09:01,850  -->  00:09:04,388
in mathematics and in data science,
221

221

00:09:04,388  -->  00:09:06,610
there's lots of different type of distances,
222

222

00:09:06,610  -->  00:09:08,370
like the one we're using is Euclidean,
223

223

00:09:08,370  -->  00:09:10,650
and the question is, should you be using
224

224

00:09:10,650  -->  00:09:12,490
Euclidean distances, or should you be using
225

225

00:09:12,490  -->  00:09:16,050
some other type of distances defined for your challenge.
226

226

00:09:16,050  -->  00:09:18,100
And, that is something up to you to decide,
227

227

00:09:18,100  -->  00:09:20,660
and that's something for you to specify for the algorithm,
228

228

00:09:20,660  -->  00:09:24,650
what type of distance you're going to be using.
229

229

00:09:24,650  -->  00:09:26,580
But, for our sake, we're just going to be using
230

230

00:09:26,580  -->  00:09:29,550
Euclidean distances for the illustration purposes,
231

231

00:09:29,550  -->  00:09:33,170
and basically that's just a very straightforward,
232

232

00:09:33,170  -->  00:09:36,710
simple geometrical distances between two different points.
233

233

00:09:36,710  -->  00:09:38,730
So, that's just a caveat there and if you'd like
234

234

00:09:38,730  -->  00:09:40,870
to research more about distances and what other
235

235

00:09:40,870  -->  00:09:42,280
kind of distances you can use,
236

236

00:09:42,280  -->  00:09:43,640
just make sure to look into that
237

237

00:09:43,640  -->  00:09:46,630
because for some challenges sometimes you might want to use
238

238

00:09:46,630  -->  00:09:48,720
non-Euclidean distances that are defined
239

239

00:09:48,720  -->  00:09:50,690
in other specific ways.
240

240

00:09:50,690  -->  00:09:55,300
All right, so with that in mind, lets proceed to step four.
241

241

00:09:55,300  -->  00:09:57,420
Step four, compute and place the new centroid
242

242

00:09:57,420  -->  00:09:58,370
of each cluster.
243

243

00:09:58,370  -->  00:10:01,230
So basically, right now we've got the new points,
244

244

00:10:01,230  -->  00:10:03,600
all the blue ones, excluding the centroid itself,
245

245

00:10:03,600  -->  00:10:06,630
and all the red ones, excluding the centroid itself.
246

246

00:10:06,630  -->  00:10:09,383
We need to find out where the actual centroid,
247

247

00:10:09,383  -->  00:10:11,610
the new centroid for the blue points is,
248

248

00:10:11,610  -->  00:10:14,010
and the new centroid for the red points is.
249

249

00:10:14,010  -->  00:10:15,540
And the way to think about it is
250

250

00:10:15,540  -->  00:10:18,940
imagine that centroids themselves, they are weightless,
251

251

00:10:18,940  -->  00:10:22,360
but these other points, they have a certain weight,
252

252

00:10:22,360  -->  00:10:24,530
so lets say one kilo each.
253

253

00:10:24,530  -->  00:10:26,640
Then, you need to find the center of mass
254

254

00:10:26,640  -->  00:10:29,610
or the center of gravity of these centroids,
255

255

00:10:29,610  -->  00:10:31,960
and you need to pinpoint it on your scatterplot.
256

256

00:10:31,960  -->  00:10:34,710
So, for the blue cluster, it'll be somewhere here.
257

257

00:10:34,710  -->  00:10:37,419
For the red cluster, it will be somewhere over here.
258

258

00:10:37,419  -->  00:10:41,120
The way to think about it, on a two dimensional scatterplot,
259

259

00:10:41,120  -->  00:10:44,070
you kind of like can just visually see where it is,
260

260

00:10:44,070  -->  00:10:47,480
or you can just look at the X, let's say the Y coordinates
261

261

00:10:47,480  -->  00:10:51,490
for all of the blue points and find the center of gravity
262

262

00:10:51,490  -->  00:10:53,460
for the Y coordinates, which will be somewhere here,
263

263

00:10:53,460  -->  00:10:55,520
and then do the same thing for the X coordinates,
264

264

00:10:55,520  -->  00:10:56,740
find the center of gravity,
265

265

00:10:56,740  -->  00:10:59,550
or like the average of the X coordinates,
266

266

00:10:59,550  -->  00:11:00,560
which will be over here.
267

267

00:11:00,560  -->  00:11:02,850
So, that's how you get your new centroid
268

268

00:11:02,850  -->  00:11:04,170
for the red and blue clusters,
269

269

00:11:04,170  -->  00:11:07,300
and so we just move our centroids into them.
270

270

00:11:07,300  -->  00:11:09,830
Next, you perform step five.
271

271

00:11:09,830  -->  00:11:14,140
So, reassign each data point to the new closest centroid.
272

272

00:11:14,140  -->  00:11:15,910
And, if any reassignment took place,
273

273

00:11:15,910  -->  00:11:18,260
then go back to step four, otherwise,
274

274

00:11:18,260  -->  00:11:19,760
you may finish your algorithm,
275

275

00:11:19,760  -->  00:11:21,500
meaning that it has converged.
276

276

00:11:21,500  -->  00:11:22,730
So, let's do that.
277

277

00:11:22,730  -->  00:11:26,080
Let's see how now the data points will reassign
278

278

00:11:26,080  -->  00:11:27,960
to the new centroids.
279

279

00:11:27,960  -->  00:11:31,210
So, if we put our line through the scatterplot,
280

280

00:11:31,210  -->  00:11:32,950
you'll see that there is one point,
281

281

00:11:32,950  -->  00:11:34,340
or no, there's actually three points,
282

282

00:11:34,340  -->  00:11:36,760
this point is closer to the blue than to the red,
283

283

00:11:36,760  -->  00:11:39,110
and these two points are actually closer to the red
284

284

00:11:39,110  -->  00:11:40,160
than the blue.
285

285

00:11:40,160  -->  00:11:43,070
So, now we will recolor them, there we go.
286

286

00:11:43,070  -->  00:11:46,540
So, now we've got a new clustering
287

287

00:11:46,540  -->  00:11:48,810
and some reassignment did take place,
288

288

00:11:48,810  -->  00:11:52,040
so we're going to proceed back to step four.
289

289

00:11:52,040  -->  00:11:53,210
So, we're going back to step four,
290

290

00:11:53,210  -->  00:11:55,680
compute and place the new centroids for each cluster.
291

291

00:11:55,680  -->  00:11:58,120
Same thing, find the center of mass for this centroid,
292

292

00:11:58,120  -->  00:12:00,440
find the center of mass for this centroid.
293

293

00:12:00,440  -->  00:12:03,550
Place the centroids in those locations
294

294

00:12:03,550  -->  00:12:05,600
and now repeat step five.
295

295

00:12:05,600  -->  00:12:08,450
Reassign each data point to the new closest centroid.
296

296

00:12:08,450  -->  00:12:10,480
So, we're gonna to put the line through the charts,
297

297

00:12:10,480  -->  00:12:13,290
so the equidistant line between the two centroids.
298

298

00:12:13,290  -->  00:12:15,080
As you can see, this point is actually
299

299

00:12:15,080  -->  00:12:17,650
now in the blue cluster rather then being in the red,
300

300

00:12:17,650  -->  00:12:21,260
reassign that point, and what happens next
301

301

00:12:21,260  -->  00:12:23,860
is, again, we turn to step four.
302

302

00:12:23,860  -->  00:12:26,280
So, if I go back here you'll see that
303

303

00:12:26,280  -->  00:12:27,510
a reassignment did take place,
304

304

00:12:27,510  -->  00:12:29,320
so we're going back to step four.
305

305

00:12:29,320  -->  00:12:32,440
And, that's where our new clusters are going to be located,
306

306

00:12:32,440  -->  00:12:35,140
our new centroids, move them there.
307

307

00:12:35,140  -->  00:12:36,860
Repeat step five.
308

308

00:12:36,860  -->  00:12:38,470
As you can see, it's an iterative process.
309

309

00:12:38,470  -->  00:12:40,260
We're just going to keep going like this
310

310

00:12:40,260  -->  00:12:42,350
until the algorithm converges.
311

311

00:12:42,350  -->  00:12:43,940
So, there's our equidistant line.
312

312

00:12:43,940  -->  00:12:46,290
As you can see, one point needs to be reassigned.
313

313

00:12:46,290  -->  00:12:49,180
It gets reassigned, so now we go back to step four.
314

314

00:12:49,180  -->  00:12:52,450
Compute and place the new centroids for each cluster.
315

315

00:12:52,450  -->  00:12:56,470
Move our centroids, and now reassign each data point.
316

316

00:12:56,470  -->  00:12:58,030
So, repeat step five.
317

317

00:12:58,030  -->  00:13:01,070
And, as you can see, this time the equidistant line
318

318

00:13:01,070  -->  00:13:04,290
does not make any points reassign.
319

319

00:13:04,290  -->  00:13:06,010
So, as you can see, all the points
320

320

00:13:06,010  -->  00:13:08,700
are already in their correct clusters.
321

321

00:13:08,700  -->  00:13:11,770
That means no reassignment took place during this step
322

322

00:13:11,770  -->  00:13:15,520
so we can proceed to completing our algorithm.
323

323

00:13:15,520  -->  00:13:18,050
This means that the algorithm has converged.
324

324

00:13:18,050  -->  00:13:20,410
So, there we go, those are our clusters
325

325

00:13:20,410  -->  00:13:22,530
and the model is ready.
326

326

00:13:22,530  -->  00:13:24,190
Now, we can just remove the centroids
327

327

00:13:24,190  -->  00:13:26,750
and the equidistant line, and there we go,
328

328

00:13:26,750  -->  00:13:28,070
that's our final result.
329

329

00:13:28,070  -->  00:13:30,320
So, as you can see, that's how the K-Means algorithm works.
330

330

00:13:30,320  -->  00:13:32,300
It's a very very intuitive and straightforward,
331

331

00:13:32,300  -->  00:13:34,920
just an iterative process, and if we compare it to
332

332

00:13:34,920  -->  00:13:37,560
what we had at the start, this is what we had.
333

333

00:13:37,560  -->  00:13:39,730
As you can see, it's not that obvious
334

334

00:13:39,730  -->  00:13:41,800
how the clusters would have been arranged.
335

335

00:13:41,800  -->  00:13:43,360
For instance, you might have thought
336

336

00:13:43,360  -->  00:13:45,930
that maybe this could be a cluster on its own,
337

337

00:13:45,930  -->  00:13:47,490
and this could be a cluster,
338

338

00:13:47,490  -->  00:13:49,477
or maybe this bottom part would be a cluster,
339

339

00:13:49,477  -->  00:13:51,350
and this top part would be a cluster.
340

340

00:13:51,350  -->  00:13:53,760
But, based on the K-Means algorithm,
341

341

00:13:53,760  -->  00:13:57,350
what we got as a result is these two clusters.
342

342

00:13:57,350  -->  00:13:59,120
So, the points in each cluster
343

343

00:13:59,120  -->  00:14:01,250
are not exactly very similar to each other,
344

344

00:14:01,250  -->  00:14:04,930
but this is what the K-Means algorithm is suggesting.
345

345

00:14:04,930  -->  00:14:06,190
Hopefully you enjoyed this tutorial,
346

346

00:14:06,190  -->  00:14:10,090
and hopefully now we've demystified the K-Means algorithm,
347

347

00:14:10,090  -->  00:14:12,900
and we've broken it down into simple terms.
348

348

00:14:12,900  -->  00:14:15,110
I look forward to seeing you in the next tutorial.
349

349

00:14:15,110  -->  00:14:17,173
Until then, enjoy machine learning.
