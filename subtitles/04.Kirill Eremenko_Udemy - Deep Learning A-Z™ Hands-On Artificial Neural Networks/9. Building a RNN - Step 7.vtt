WEBVTT
1
1

00:00:00.530  -->  00:00:02.680
<v Instructor>Hello and welcome to the second step</v>
2

2

00:00:02.680  -->  00:00:05.210
of part two, building the RNN.
3

3

00:00:05.210  -->  00:00:09.070
In this second step, we're gonna add the first LSTM layer
4

4

00:00:09.070  -->  00:00:11.367
of our regular neural network which was introduced
5

5

00:00:11.367  -->  00:00:13.760
as a sequence of layers
6

6

00:00:13.760  -->  00:00:17.010
and also some Dropout regularisation.
7

7

00:00:17.010  -->  00:00:19.020
Why do we add some Dropout regularisation?
8

8

00:00:19.020  -->  00:00:21.700
That's, of course, to avoid Overfitting.
9

9

00:00:21.700  -->  00:00:24.240
We don't want any Overfitting when predicting
10

10

00:00:24.240  -->  00:00:26.750
the stock price and on a general rule.
11

11

00:00:26.750  -->  00:00:30.160
So, let's do this, let's add this first LSTM layer
12

12

00:00:30.160  -->  00:00:31.750
and Dropout regularisation.
13

13

00:00:31.750  -->  00:00:33.640
It's gonna take two lines of code.
14

14

00:00:33.640  -->  00:00:36.630
One line to add this first LSTM layer,
15

15

00:00:36.630  -->  00:00:40.240
and one line to add the Dropout regularisation.
16

16

00:00:40.240  -->  00:00:44.210
So, let's start with adding our first LSTM layer.
17

17

00:00:44.210  -->  00:00:47.263
That's really easy, we take our regressor.
18

18

00:00:48.420  -->  00:00:51.230
Regressor is an object of the sequential class.
19

19

00:00:51.230  -->  00:00:54.320
The sequential class contains the add method.
20

20

00:00:54.320  -->  00:00:57.440
That allows to add some layers of neural network,
21

21

00:00:57.440  -->  00:01:00.657
so we're adding here, dot and then this add method.
22

22

00:01:00.657  -->  00:01:02.120
There we go.
23

23

00:01:02.120  -->  00:01:05.232
And inside the add method, we need to input the layer,
24

24

00:01:05.232  -->  00:01:07.570
the type of layer we want to add.
25

25

00:01:07.570  -->  00:01:11.500
And of course, what we want to add is an LSTM layer.
26

26

00:01:11.500  -->  00:01:14.120
That's where we use the LSTM class,
27

27

00:01:14.120  -->  00:01:17.180
because actually what we are adding in this add method
28

28

00:01:17.180  -->  00:01:19.920
will be an object of the LSTM class.
29

29

00:01:19.920  -->  00:01:22.280
And this subject will represent nothing else
30

30

00:01:22.280  -->  00:01:24.510
than this LSTM layer.
31

31

00:01:24.510  -->  00:01:25.420
So, let's do this.
32

32

00:01:25.420  -->  00:01:28.260
We create this LSTM layer by creating
33

33

00:01:28.260  -->  00:01:32.090
an object of the L-S-T-M class,
34

34

00:01:32.090  -->  00:01:34.770
which will take several arguments now
35

35

00:01:34.770  -->  00:01:37.930
and now, please focus, this is very important.
36

36

00:01:37.930  -->  00:01:40.110
We need to input three arguments,
37

37

00:01:40.110  -->  00:01:42.720
and it's very important to understand what they are.
38

38

00:01:42.720  -->  00:01:46.720
These three arguments are, first, the number of units,
39

39

00:01:46.720  -->  00:01:50.306
which is the number of LSTM cells or units
40

40

00:01:50.306  -->  00:01:53.600
you want to have in this LSTM layer.
41

41

00:01:53.600  -->  00:01:55.480
We will choose a relevant number,
42

42

00:01:55.480  -->  00:01:57.360
and I will explain in a minute
43

43

00:01:57.360  -->  00:01:59.260
what this relevant number will be.
44

44

00:01:59.260  -->  00:02:02.090
Then, the second argument of this LSTM class,
45

45

00:02:02.090  -->  00:02:05.247
is return sequences and we will have to
46

46

00:02:05.247  -->  00:02:08.290
set it equal to true, because we are building
47

47

00:02:08.290  -->  00:02:10.710
a stacked LSTM which therefore will
48

48

00:02:10.710  -->  00:02:15.672
have several LSTM layers and when you add another LSTM layer
49

49

00:02:15.672  -->  00:02:18.070
after the one you are creating right now,
50

50

00:02:18.070  -->  00:02:21.970
well you have to set the return sequences argument to true.
51

51

00:02:21.970  -->  00:02:24.060
Once you are done with your LSTM layers,
52

52

00:02:24.060  -->  00:02:26.733
you are not gonna add another one after that,
53

53

00:02:26.733  -->  00:02:28.420
you will set it equal to false,
54

54

00:02:28.420  -->  00:02:30.120
but you won't even have to do this
55

55

00:02:30.120  -->  00:02:31.710
because this is the default value
56

56

00:02:31.710  -->  00:02:33.730
of the return sequences parameter.
57

57

00:02:33.730  -->  00:02:35.870
Alright, that's for the second argument.
58

58

00:02:35.870  -->  00:02:39.890
Now, the third and last argument is input_shape,
59

59

00:02:39.890  -->  00:02:43.712
and that is exactly the shape of the
60

60

00:02:43.712  -->  00:02:46.763
input containing x_train that we created
61

61

00:02:46.763  -->  00:02:50.180
in the last step of the data preprocessing part.
62

62

00:02:50.180  -->  00:02:53.690
It's an input shape in three dimensions, in 3-D,
63

63

00:02:53.690  -->  00:02:55.390
corresponding to the observations,
64

64

00:02:55.390  -->  00:02:57.860
the time steps, and the indicators.
65

65

00:02:57.860  -->  00:03:01.530
But in this third argument of the LSTM class,
66

66

00:03:01.530  -->  00:03:03.670
we won't have to include the three dimensions,
67

67

00:03:03.670  -->  00:03:05.690
only the two last ones corresponding
68

68

00:03:05.690  -->  00:03:07.710
to the time steps and the indicators,
69

69

00:03:07.710  -->  00:03:10.710
because the first one, corresponding to the observations,
70

70

00:03:10.710  -->  00:03:12.800
will be automatically taken into account.
71

71

00:03:12.800  -->  00:03:17.070
So, you will see, we will only specify x_train.shape[1],
72

72

00:03:17.070  -->  00:03:19.700
corresponding to the time steps, and [1] corresponding
73

73

00:03:19.700  -->  00:03:22.053
to the predictors, the indicators.
74

74

00:03:23.352  -->  00:03:25.080
That's our three arguments
75

75

00:03:25.080  -->  00:03:27.450
and so therefore let's input them now,
76

76

00:03:27.450  -->  00:03:30.120
starting with the units.
77

77

00:03:30.120  -->  00:03:34.970
Which I remind is the number of LSTM cells, or memory units,
78

78

00:03:34.970  -->  00:03:38.340
but for simplicity's sake, I will just call them neurons,
79

79

00:03:38.340  -->  00:03:42.363
that you want to have in this first LSTM layer.
80

80

00:03:43.748  -->  00:03:45.890
What is this number going to be?
81

81

00:03:45.890  -->  00:03:49.720
Well, even if we are going to stack many layers,
82

82

00:03:49.720  -->  00:03:53.730
we want our model to have a very high dimensionality.
83

83

00:03:53.730  -->  00:03:57.010
So, indeed we're making this dimensionality high
84

84

00:03:57.010  -->  00:03:59.870
thanks to the multiple LSTM layers that we're gonna add,
85

85

00:03:59.870  -->  00:04:03.704
but we can increase even more this dimensionality
86

86

00:04:03.704  -->  00:04:06.400
by including a large number of neurons
87

87

00:04:06.400  -->  00:04:08.500
in each of the LSTM layers.
88

88

00:04:08.500  -->  00:04:10.430
Since capturing the trends
89

89

00:04:10.430  -->  00:04:13.300
of a stock price is pretty complex,
90

90

00:04:13.300  -->  00:04:15.400
we need to have this high dimensionality
91

91

00:04:15.400  -->  00:04:18.440
and therefore we also need to have a large number of neurons
92

92

00:04:18.440  -->  00:04:21.220
in each of the multiple LSTM layers.
93

93

00:04:21.220  -->  00:04:24.160
And therefore, number of neurons we'll choose
94

94

00:04:24.160  -->  00:04:28.070
for this first LSTM layer is gonna be 50.
95

95

00:04:28.070  -->  00:04:30.880
50 neurons in this first LSTM layer and then
96

96

00:04:30.880  -->  00:04:33.700
in the next ones, they will also have 50 neurons,
97

97

00:04:33.700  -->  00:04:36.763
will get us a model with high dimensionality.
98

98

00:04:36.763  -->  00:04:39.410
And if we had chosen a too small number
99

99

00:04:39.410  -->  00:04:41.200
of neurons in each of the LSTM layers,
100

100

00:04:41.200  -->  00:04:43.730
like, for example, three to five neurons,
101

101

00:04:43.730  -->  00:04:45.660
well, that would be too little and it wouldn't
102

102

00:04:45.660  -->  00:04:48.770
be able to capture very well the upward and downward trends.
103

103

00:04:48.770  -->  00:04:51.760
But with 50 neurons, this will lead us to better results.
104

104

00:04:51.760  -->  00:04:55.650
That's the first argument, this is what we
105

105

00:04:55.650  -->  00:04:57.800
have to input here, so that's done.
106

106

00:04:57.800  -->  00:04:59.400
And now the second argument,
107

107

00:04:59.400  -->  00:05:02.730
as you remember, is return sequences.
108

108

00:05:02.730  -->  00:05:07.400
So return_sequences, spelled like that.
109

109

00:05:07.400  -->  00:05:10.060
That we have to set equal to true,
110

110

00:05:10.060  -->  00:05:11.990
and I remind us because we're gonna add
111

111

00:05:11.990  -->  00:05:14.570
another LSTM layer after this one
112

112

00:05:14.570  -->  00:05:17.500
because we're making a stacked LSTM neural network
113

113

00:05:17.500  -->  00:05:19.620
and therefore, when you're adding another one,
114

114

00:05:19.620  -->  00:05:23.470
you have to set this return sequence parameter to true.
115

115

00:05:23.470  -->  00:05:27.677
Perfect, and now final argument, input_shape,
116

116

00:05:30.090  -->  00:05:34.010
which, as we said, only contains the last two dimensions
117

117

00:05:34.010  -->  00:05:37.920
corresponding to the time steps and the indicators.
118

118

00:05:37.920  -->  00:05:40.770
Therefore, I'm simply going to copy this
119

119

00:05:41.720  -->  00:05:45.810
and I'm gonna paste it inside some new parenthesis
120

120

00:05:45.810  -->  00:05:49.230
because they will contain these two dimensions,
121

121

00:05:49.230  -->  00:05:51.770
x_train.shape one corresponding to the time steps,
122

122

00:05:51.770  -->  00:05:55.130
and one corresponding to the predictors, the indicators.
123

123

00:05:55.130  -->  00:05:57.151
But make sure to understand,
124

124

00:05:57.151  -->  00:05:58.820
we still have this 3-D structure
125

125

00:05:58.820  -->  00:06:01.830
that we made in this first part data preprocessing.
126

126

00:06:01.830  -->  00:06:05.860
Alright, so, that's done for the first LSTM layer.
127

127

00:06:05.860  -->  00:06:08.580
Now we're gonna take care of the second sub-step
128

128

00:06:08.580  -->  00:06:10.860
of this first step building the architecture
129

129

00:06:10.860  -->  00:06:11.980
of the neural network,
130

130

00:06:11.980  -->  00:06:14.483
that is adding some Dropout regularisation.
131

131

00:06:14.483  -->  00:06:17.154
We're gonna do that in a new line of code,
132

132

00:06:17.154  -->  00:06:19.930
and to do this, well, that's very simple.
133

133

00:06:19.930  -->  00:06:22.297
We take again our regressor,
134

134

00:06:23.445  -->  00:06:26.080
then we use the add method
135

135

00:06:26.080  -->  00:06:29.670
of the sequential class, and now, as you can imagine,
136

136

00:06:29.670  -->  00:06:32.550
it works exactly the same way as for the LSTM.
137

137

00:06:32.550  -->  00:06:35.710
We are gonna create an object of the Dropout class,
138

138

00:06:35.710  -->  00:06:37.140
which we already imported,
139

139

00:06:37.140  -->  00:06:40.290
to include this Dropout regularisation.
140

140

00:06:40.290  -->  00:06:42.360
And therefore, exactly for the LSTM,
141

141

00:06:42.360  -->  00:06:45.613
we need to specify here the name of this class, Dropout,
142

142

00:06:48.492  -->  00:06:50.080
which will take only one argument this time,
143

143

00:06:50.080  -->  00:06:51.550
which is the Dropout rate.
144

144

00:06:51.550  -->  00:06:54.770
You saw in the Improving and Tuning the Models
145

145

00:06:54.770  -->  00:06:57.549
of Parts 1 and 2A and seeing in them how Dropout works.
146

146

00:06:57.549  -->  00:07:00.820
You need to specify this Dropout rate,
147

147

00:07:00.820  -->  00:07:03.060
which is the rate of neurons you wanna drop,
148

148

00:07:03.060  -->  00:07:04.460
that is you wanna ignore,
149

149

00:07:04.460  -->  00:07:07.260
in the layers to do this regularisation.
150

150

00:07:07.260  -->  00:07:08.860
And the classic number to use,
151

151

00:07:08.860  -->  00:07:11.850
and which I recommend to use because it's quite relevant,
152

152

00:07:11.850  -->  00:07:15.072
is to drop 20% of your neurons in the layer.
153

153

00:07:15.072  -->  00:07:19.510
And this 20% is exactly what we need to input here.
154

154

00:07:19.510  -->  00:07:24.010
I'm gonna add 0.2, corresponding to exactly 20%.
155

155

00:07:24.010  -->  00:07:25.860
So, we're gonna have 20% Dropout,
156

156

00:07:25.860  -->  00:07:29.080
that is the 20% of the neurons of the LSTM layer,
157

157

00:07:29.080  -->  00:07:30.860
will be ignored during the training,
158

158

00:07:30.860  -->  00:07:34.370
that is during the forward propagation and back propagation
159

159

00:07:34.370  -->  00:07:36.740
happening in each iteration of the training.
160

160

00:07:36.740  -->  00:07:40.490
Therefore, since 20% of 50 is 10 neurons,
161

161

00:07:40.490  -->  00:07:43.434
that means that 10 neurons will be ignored and dropped out
162

162

00:07:43.434  -->  00:07:45.693
during each iteration of the training.
163

163

00:07:46.780  -->  00:07:50.180
So, perfect, we are done with our first LSTM layer,
164

164

00:07:50.180  -->  00:07:52.980
to which we added some Dropout regularisation.
165

165

00:07:52.980  -->  00:07:55.980
Perfect, our LSTM is getting into shape.
166

166

00:07:55.980  -->  00:07:58.410
Now we're gonna add some more LSTM layers
167

167

00:07:58.410  -->  00:08:02.270
until we're gonna add a total of four LSTM layers,
168

168

00:08:02.270  -->  00:08:04.110
so that will make a big, stacked LSTM
169

169

00:08:04.110  -->  00:08:06.870
and we will do it very efficiently, of course,
170

170

00:08:06.870  -->  00:08:10.760
because it's basically almost the same as what we did here,
171

171

00:08:10.760  -->  00:08:12.710
we will just need to remove this.
172

172

00:08:12.710  -->  00:08:15.320
Alright, so let's do that in the next tutorial.
173

173

00:08:15.320  -->  00:08:19.590
We will add these other LSTM layers in one same tutorial.
174

174

00:08:19.590  -->  00:08:21.700
And now just before I say goodbye to you,
175

175

00:08:21.700  -->  00:08:24.410
I'm going to select these two lines
176

176

00:08:24.410  -->  00:08:27.711
to add this first LSTM and the Dropout regularisation.
177

177

00:08:27.711  -->  00:08:32.430
So let's execute, there we go, first LSTM layer added
178

178

00:08:32.430  -->  00:08:35.780
and Dropout regularisation added as well.
179

179

00:08:35.780  -->  00:08:39.020
Perfect, now let's move on to the next LSTM layers
180

180

00:08:39.020  -->  00:08:41.090
and we'll do that in the next tutorial.
181

181

00:08:41.090  -->  00:08:43.213
Until then, enjoy Deep Learning.
