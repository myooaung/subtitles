1
1

00:00:00,520  -->  00:00:02,800
<v Narrator>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,800  -->  00:00:04,770
So, in the previous tutorial we introduced
3

3

00:00:04,770  -->  00:00:07,920
Convolutional Neural Networks and we set the right folder
4

4

00:00:07,920  -->  00:00:10,220
as working directory, which is this folder
5

5

00:00:10,220  -->  00:00:13,900
containing our cnn.py file that we're about to make
6

6

00:00:13,900  -->  00:00:17,480
and this dataset, that we will explain in great details
7

7

00:00:17,480  -->  00:00:20,660
and that we will also pre-process in this tutorial.
8

8

00:00:20,660  -->  00:00:23,620
So, as you can see, this is no longer a CSV file
9

9

00:00:23,620  -->  00:00:26,510
because of course you cannot create a CSV file
10

10

00:00:26,510  -->  00:00:29,530
where you have your images separated by some commas,
11

11

00:00:29,530  -->  00:00:31,000
that wouldn't make any sense.
12

12

00:00:31,000  -->  00:00:34,050
So, instead of having a CSV, I have a dataset.
13

13

00:00:34,050  -->  00:00:36,870
I'm not going to open it right now, but basically
14

14

00:00:36,870  -->  00:00:39,060
what there is in this dataset are all
15

15

00:00:39,060  -->  00:00:41,430
our images of cats and dogs.
16

16

00:00:41,430  -->  00:00:43,860
But this dataset has a special structure,
17

17

00:00:43,860  -->  00:00:46,120
a structure that will make our life easier
18

18

00:00:46,120  -->  00:00:49,110
and the life of the CNN model easier as well.
19

19

00:00:49,110  -->  00:00:51,420
Because, let's think about it for a minute,
20

20

00:00:51,420  -->  00:00:54,380
how can we make a training set and a test set
21

21

00:00:54,380  -->  00:00:57,160
where the independent variables are now the pixels
22

22

00:00:57,160  -->  00:01:00,650
distributed in 3D arrays, and therefore not distributed
23

23

00:01:00,650  -->  00:01:03,190
like in the previous dataset where we had our independent
24

24

00:01:03,190  -->  00:01:05,930
variables in columns, next to the final column
25

25

00:01:05,930  -->  00:01:07,830
that contained the dependent variable.
26

26

00:01:07,830  -->  00:01:10,420
Here, since our dataset no longer has the structure
27

27

00:01:10,420  -->  00:01:13,290
where the rows are the observations and the columns
28

28

00:01:13,290  -->  00:01:14,750
are the independent variables
29

29

00:01:14,750  -->  00:01:17,100
and the dependent variable next to each other,
30

30

00:01:17,100  -->  00:01:19,770
then we cannot add explicitly the dependent variable
31

31

00:01:19,770  -->  00:01:23,130
in our dataset because it wouldn't make much sense to add
32

32

00:01:23,130  -->  00:01:26,270
this dependent variable column along the 3D arrays
33

33

00:01:26,270  -->  00:01:28,110
representing the images.
34

34

00:01:28,110  -->  00:01:30,200
And, you know, when we train a machinery model
35

35

00:01:30,200  -->  00:01:32,090
we always need the dependent variable
36

36

00:01:32,090  -->  00:01:35,030
to have the real results that are required
37

37

00:01:35,030  -->  00:01:36,290
to understand the correlations
38

38

00:01:36,290  -->  00:01:38,410
between the information contained in the
39

39

00:01:38,410  -->  00:01:40,840
independent variables, and the real result
40

40

00:01:40,840  -->  00:01:42,780
contained in the dependent variable.
41

41

00:01:42,780  -->  00:01:46,280
But here, since we cannot add this dependent variable
42

42

00:01:46,280  -->  00:01:49,320
column in the same table, how can we extract
43

43

00:01:49,320  -->  00:01:51,770
the info of this dependent variable?
44

44

00:01:51,770  -->  00:01:53,750
Well, we have several solutions.
45

45

00:01:53,750  -->  00:01:56,210
A classic solution is to only have a dataset
46

46

00:01:56,210  -->  00:01:59,530
containing our images, separated in two different folders,
47

47

00:01:59,530  -->  00:02:00,880
a folder for the training set
48

48

00:02:00,880  -->  00:02:02,540
and a folder for the test set.
49

49

00:02:02,540  -->  00:02:05,520
And then since each of the images has a name,
50

50

00:02:05,520  -->  00:02:07,950
the name of the JPEG or PNG file,
51

51

00:02:07,950  -->  00:02:10,340
well you know what we can do, and that's the first solution,
52

52

00:02:10,340  -->  00:02:14,520
is to name each of these images by the category
53

53

00:02:14,520  -->  00:02:18,380
of the image, that is cat or dog, and add, for example,
54

54

00:02:18,380  -->  00:02:21,020
a number to differentiate all the images.
55

55

00:02:21,020  -->  00:02:23,210
So that means, that in each folder the training set
56

56

00:02:23,210  -->  00:02:26,400
and the test set, we would get, for example, 5,000 images
57

57

00:02:26,400  -->  00:02:29,480
of cats that we would name Cat One, Cat Two, Cat Three,
58

58

00:02:29,480  -->  00:02:32,300
Cat Four, Cat Five, up to Cat 5,000.
59

59

00:02:32,300  -->  00:02:34,100
And then, Dog One, Dog Two, Dog Three,
60

60

00:02:34,100  -->  00:02:37,040
Dog Four, Dog Five, up to Dog 5,000.
61

61

00:02:37,040  -->  00:02:39,400
And then, that's where the solution takes place.
62

62

00:02:39,400  -->  00:02:43,240
We can write some kind of code to extract the label name
63

63

00:02:43,240  -->  00:02:46,630
Cat or Dog from the name of the image file
64

64

00:02:46,630  -->  00:02:49,640
to specify to the algorithm whether this image
65

65

00:02:49,640  -->  00:02:53,530
belongs to the class Cat or belongs to the class Dog.
66

66

00:02:53,530  -->  00:02:56,550
And in some way, we get the our dependent variable vector,
67

67

00:02:56,550  -->  00:02:59,870
because we can fill in this dependent variable vector
68

68

00:02:59,870  -->  00:03:02,630
with the label names that we managed to extract
69

69

00:03:02,630  -->  00:03:06,480
from the image file names of all our images.
70

70

00:03:06,480  -->  00:03:08,230
So that's the first solution, but that's
71

71

00:03:08,230  -->  00:03:10,240
not the one we're gonna use because we have
72

72

00:03:10,240  -->  00:03:12,970
a better one, and a more efficient one.
73

73

00:03:12,970  -->  00:03:16,320
And, this better solution comes with Keras,
74

74

00:03:16,320  -->  00:03:18,170
of course, we're using Keras.
75

75

00:03:18,170  -->  00:03:20,570
Keras is an amazing library for deep learning
76

76

00:03:20,570  -->  00:03:22,850
and computer vision, and of course, it contains
77

77

00:03:22,850  -->  00:03:26,460
some tricks and tools to import some images
78

78

00:03:26,460  -->  00:03:28,080
in a very efficient way.
79

79

00:03:28,080  -->  00:03:29,970
And that's the solution we'll use
80

80

00:03:29,970  -->  00:03:32,840
and you will see, you will be very happy to use it.
81

81

00:03:32,840  -->  00:03:36,180
Because, basically, to import the images with Keras,
82

82

00:03:36,180  -->  00:03:39,100
what we only need to do is to prepare
83

83

00:03:39,100  -->  00:03:41,300
a special structure for our dataset.
84

84

00:03:41,300  -->  00:03:42,950
But it's a very simple structure
85

85

00:03:42,950  -->  00:03:44,600
as you are about to see right now.
86

86

00:03:44,600  -->  00:03:47,970
So what I'm gonna do, is open this dataset
87

87

00:03:47,970  -->  00:03:51,030
and you're going to find out about this special structure.
88

88

00:03:51,030  -->  00:03:53,580
So here we go, dataset opened.
89

89

00:03:53,580  -->  00:03:56,910
Okay, so the first pillar of the structure is to separate
90

90

00:03:56,910  -->  00:04:00,320
your images into two separate folders, we already said that,
91

91

00:04:00,320  -->  00:04:03,180
a training set folder and a test set folder.
92

92

00:04:03,180  -->  00:04:05,810
Alright, but that's not the trick, remember we want
93

93

00:04:05,810  -->  00:04:09,690
to have a simple way to differentiate the class labels,
94

94

00:04:09,690  -->  00:04:13,300
that is, differentiate the cat images and the dog images.
95

95

00:04:13,300  -->  00:04:16,680
And so, basically, the simple trick that we can use here
96

96

00:04:16,680  -->  00:04:20,950
is to, again, make two different folders
97

97

00:04:20,950  -->  00:04:24,070
one folder for the cats and one folder for the dogs.
98

98

00:04:24,070  -->  00:04:27,820
And that's how Keras will understand how to differentiate
99

99

00:04:27,820  -->  00:04:29,890
the labels of your dependent variable,
100

100

00:04:29,890  -->  00:04:32,000
that is the real results, whether each
101

101

00:04:32,000  -->  00:04:35,960
of your images is the image of a dog or the image of a cat.
102

102

00:04:35,960  -->  00:04:38,420
Because then, if you go into this folder, for example
103

103

00:04:38,420  -->  00:04:41,080
the dogs folder, well as you can see,
104

104

00:04:41,080  -->  00:04:43,930
I have some different images of dogs.
105

105

00:04:43,930  -->  00:04:47,460
This is the first dog, this is a second dog here
106

106

00:04:47,460  -->  00:04:51,980
and a third dog, fourth dog,
107

107

00:04:51,980  -->  00:04:53,700
alright, this one is cute.
108

108

00:04:53,700  -->  00:04:56,530
Well, anyway, I have some images as you know
109

109

00:04:56,530  -->  00:04:58,540
them because they have the JPEG format.
110

110

00:04:58,540  -->  00:05:01,450
So that can be any kind of images you have on your computer,
111

111

00:05:01,450  -->  00:05:03,520
you know you can take some pictures of your friends
112

112

00:05:03,520  -->  00:05:06,320
and replace these dog's pictures by the pictures
113

113

00:05:06,320  -->  00:05:07,970
of your friends and then you'll be able
114

114

00:05:07,970  -->  00:05:10,050
to train an algorithm that will predict,
115

115

00:05:10,050  -->  00:05:12,540
which friend of yours is in the pictures.
116

116

00:05:12,540  -->  00:05:14,740
So that can be pretty fun to do, but remember,
117

117

00:05:14,740  -->  00:05:16,610
you need a lot of images.
118

118

00:05:16,610  -->  00:05:19,180
Alright, so basically we have pictures of dogs here,
119

119

00:05:19,180  -->  00:05:21,510
and then if we go back to the other folder
120

120

00:05:21,510  -->  00:05:23,450
that's the same for the cats folder.
121

121

00:05:23,450  -->  00:05:27,360
In this folder we have some images of cats, so that's
122

122

00:05:27,360  -->  00:05:30,750
the first cat, second cat,
123

123

00:05:30,750  -->  00:05:34,070
third cat and et cetera.
124

124

00:05:34,070  -->  00:05:36,520
So, this dataset is a very well-known dataset
125

125

00:05:36,520  -->  00:05:39,220
in computer vision, it can be used as a performance
126

126

00:05:39,220  -->  00:05:42,400
benchmark, to test your Deep Learning models
127

127

00:05:42,400  -->  00:05:45,720
on this to simply see if you get some good accuracy
128

128

00:05:45,720  -->  00:05:47,690
and so it's a very useful dataset.
129

129

00:05:47,690  -->  00:05:50,740
So, this dataset here is actually a subset
130

130

00:05:50,740  -->  00:05:53,720
of the whole dataset, that you can find on Kaggle.
131

131

00:05:53,720  -->  00:05:55,920
Because the original whole dataset contains
132

132

00:05:55,920  -->  00:05:58,780
25,000 images, but here is just a subset
133

133

00:05:58,780  -->  00:06:01,340
since we have much less images than that.
134

134

00:06:01,340  -->  00:06:03,730
So, let's see how many images we have.
135

135

00:06:03,730  -->  00:06:07,430
Well, it's going to be kind of the same
136

136

00:06:07,430  -->  00:06:11,150
as the business problem we had in the previous section.
137

137

00:06:11,150  -->  00:06:13,090
Because remember in the previous section we had,
138

138

00:06:13,090  -->  00:06:16,520
10,000 observations corresponding to 10,000 customers
139

139

00:06:16,520  -->  00:06:19,270
of a bank and remembering the data pre-processing part,
140

140

00:06:19,270  -->  00:06:21,580
we divided these 10,000 customers into
141

141

00:06:21,580  -->  00:06:23,280
the training set and the test set.
142

142

00:06:23,280  -->  00:06:26,350
And remember the training set contained 8,000 customers
143

143

00:06:26,350  -->  00:06:29,070
and the test set contained 2,000 customers.
144

144

00:06:29,070  -->  00:06:32,010
Well, here it's exactly the same for our cats and dogs
145

145

00:06:32,010  -->  00:06:35,210
images, we have 10,000 images in total,
146

146

00:06:35,210  -->  00:06:39,810
then I put 8,000 images of dogs and cats in the training set
147

147

00:06:39,810  -->  00:06:44,810
and 2,000 images of dogs and cats in the test set.
148

148

00:06:44,830  -->  00:06:49,400
And then in the training set, I put 4,000 images of dogs
149

149

00:06:49,400  -->  00:06:54,400
and same for the cats, 4,000 images of cats.
150

150

00:06:54,710  -->  00:06:57,710
Alright, and that's the same for the test set,
151

151

00:06:57,710  -->  00:07:01,040
since the test set contains 2,000 images of cats and dogs.
152

152

00:07:01,040  -->  00:07:05,120
Well, we have here 1,000 images of dogs, you see it's
153

153

00:07:05,120  -->  00:07:08,740
going from 4,001 to 5,000,
154

154

00:07:08,740  -->  00:07:12,973
so 1,000 images of dogs and 1,000 images of cats.
155

155

00:07:16,950  -->  00:07:19,470
Alright, so to summarize, we have 10,000 images
156

156

00:07:19,470  -->  00:07:23,120
in total in the dataset, 8,000 images in the training set
157

157

00:07:23,120  -->  00:07:25,190
and 2,000 images in the test set.
158

158

00:07:25,190  -->  00:07:28,650
So that's an 80% 20% split, then in the training set
159

159

00:07:28,650  -->  00:07:32,490
we have 4,000 images of dogs and 4,000 images of cats.
160

160

00:07:32,490  -->  00:07:34,610
And in the test set we have 1,000 images
161

161

00:07:34,610  -->  00:07:37,210
of dogs and 1,000 images of cats.
162

162

00:07:37,210  -->  00:07:38,900
And basically, right now we did
163

163

00:07:38,900  -->  00:07:41,610
the big part of data pre-processing.
164

164

00:07:41,610  -->  00:07:43,800
I'm talking about the data pre-processing
165

165

00:07:43,800  -->  00:07:46,110
that we used to do in the first part,
166

166

00:07:46,110  -->  00:07:47,140
in the previous models.
167

167

00:07:47,140  -->  00:07:50,280
Because, basically, we already imported the dataset.
168

168

00:07:50,280  -->  00:07:52,810
Then we don't need to encode any categorical data
169

169

00:07:52,810  -->  00:07:54,980
because, of course, our independent variables
170

170

00:07:54,980  -->  00:07:58,160
are in someway the pixels and the three channels.
171

171

00:07:58,160  -->  00:08:01,040
So, there is no categorical data here and therefore
172

172

00:08:01,040  -->  00:08:03,030
we don't need to do any encoding.
173

173

00:08:03,030  -->  00:08:05,250
Then, next section, splitting the datasets into
174

174

00:08:05,250  -->  00:08:06,860
the training set and the test set,
175

175

00:08:06,860  -->  00:08:08,200
well, that's the same here,
176

176

00:08:08,200  -->  00:08:10,150
thanks to the structure of this folder.
177

177

00:08:10,150  -->  00:08:12,320
Our dataset is already splitted into
178

178

00:08:12,320  -->  00:08:14,430
a test set and a training set.
179

179

00:08:14,430  -->  00:08:17,480
So all good here and then, finally, we get to the next
180

180

00:08:17,480  -->  00:08:19,630
and last section, feature scaling.
181

181

00:08:19,630  -->  00:08:21,760
And so, do we need to apply feature scaling?
182

182

00:08:21,760  -->  00:08:24,970
Yes we do, remember what I said from the previous section.
183

183

00:08:24,970  -->  00:08:28,060
Feature scaling is 100% compulsory in deep learning
184

184

00:08:28,060  -->  00:08:29,710
and especially computer vision.
185

185

00:08:29,710  -->  00:08:32,590
But, you know, since this feature scaling section
186

186

00:08:32,590  -->  00:08:36,360
is associated to this whole data pre-processing part,
187

187

00:08:36,360  -->  00:08:39,230
and since we're not using this data pre-processing part,
188

188

00:08:39,230  -->  00:08:41,640
well, we will take care of feature scaling later,
189

189

00:08:41,640  -->  00:08:45,620
just before we fit our CNN to our images.
190

190

00:08:45,620  -->  00:08:48,200
And so the conclusion of all this, is that,
191

191

00:08:48,200  -->  00:08:51,110
there is no part one data pre-processing.
192

192

00:08:51,110  -->  00:08:53,780
Part one data pre-processing was done manually
193

193

00:08:53,780  -->  00:08:55,730
and then we just need to to come feature scaling
194

194

00:08:55,730  -->  00:08:58,540
and image augmentation, so that our deep learning models
195

195

00:08:58,540  -->  00:09:01,230
can run the most efficiently as possible.
196

196

00:09:01,230  -->  00:09:03,310
And, therefore, since we won't use this
197

197

00:09:03,310  -->  00:09:05,370
part one data pre-processing, well,
198

198

00:09:05,370  -->  00:09:07,350
that means that the the first part
199

199

00:09:07,350  -->  00:09:10,400
of our CNN model won't be data pre-processing
200

200

00:09:10,400  -->  00:09:13,210
but directly, and that's the exciting thing about it,
201

201

00:09:13,210  -->  00:09:15,980
because the data pre-processing part is the boring part,
202

202

00:09:15,980  -->  00:09:18,680
well, the first part here is building
203

203

00:09:19,890  -->  00:09:23,840
the Convolutional Neural Network.
204

204

00:09:23,840  -->  00:09:26,400
And here we go, we are ready to build it
205

205

00:09:26,400  -->  00:09:28,460
and that's what we'll do in the next tutorial.
206

206

00:09:28,460  -->  00:09:30,153
Until then, enjoy deep learning.
