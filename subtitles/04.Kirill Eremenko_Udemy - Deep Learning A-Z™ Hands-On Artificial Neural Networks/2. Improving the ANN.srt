1
1

00:00:00,140  -->  00:00:02,460
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,460  -->  00:00:05,230
So, before we take the challenge to improve our
3

3

00:00:05,230  -->  00:00:08,480
relevance accuracy of 83% that we that we just computed
4

4

00:00:08,480  -->  00:00:11,230
in the previous tutorial with k-fold cross-validation,
5

5

00:00:11,230  -->  00:00:13,280
I'd like to cover a very important technique
6

6

00:00:13,280  -->  00:00:16,790
in deep learning, which is the dropout regularization.
7

7

00:00:16,790  -->  00:00:19,860
So the dropout is the solution
8

8

00:00:19,860  -->  00:00:22,040
for overfitting in deep learning.
9

9

00:00:22,040  -->  00:00:24,950
Overfitting is when your model was trained too much
10

10

00:00:24,950  -->  00:00:27,820
on the training set, too much that it becomes much less
11

11

00:00:27,820  -->  00:00:30,930
performance on the test set and we can observe this
12

12

00:00:30,930  -->  00:00:33,700
when we have a large difference of accuracies
13

13

00:00:33,700  -->  00:00:36,350
between training set and the test set.
14

14

00:00:36,350  -->  00:00:39,140
Generally, when overfitting happens, you have a much
15

15

00:00:39,140  -->  00:00:42,170
higher accuracy on the training set than the test set.
16

16

00:00:42,170  -->  00:00:44,940
And another way to detect overfitting is when you observe
17

17

00:00:44,940  -->  00:00:48,030
a high variance when applying k-fold cross-validation
18

18

00:00:48,030  -->  00:00:51,470
because indeed, when it's overfitted on the training set,
19

19

00:00:51,470  -->  00:00:54,080
that is when your model learn too much, well sometimes
20

20

00:00:54,080  -->  00:00:57,070
it will succeed on new observations in one test set,
21

21

00:00:57,070  -->  00:00:58,643
when the correlations are similar to what the model
22

22

00:00:58,643  -->  00:01:00,120
learned too much.
23

23

00:01:00,120  -->  00:01:03,550
And sometimes your model won't succeed on other test sets
24

24

00:01:03,550  -->  00:01:05,950
because the correlations that it learned too much,
25

25

00:01:05,950  -->  00:01:08,800
unfortunately don't apply to these test sets.
26

26

00:01:08,800  -->  00:01:11,880
And therefore, in your vector of accuracies that you obtain
27

27

00:01:11,880  -->  00:01:15,530
with k-fold cross-validations, you have some high accuracies
28

28

00:01:15,530  -->  00:01:17,900
and some low accuracies and therefore you have
29

29

00:01:17,900  -->  00:01:21,020
high variance and that's how you detect overfitting.
30

30

00:01:21,020  -->  00:01:24,030
So, we definitely didn't get overfitting in the previous
31

31

00:01:24,030  -->  00:01:26,770
tutorial, because indeed when we look at our
32

32

00:01:26,770  -->  00:01:30,070
vector of accuracies, we get our 10 accuracies with the
33

33

00:01:30,070  -->  00:01:34,060
very small variance, that is by the way equal to 0.9%,
34

34

00:01:34,060  -->  00:01:36,870
so that's a small variance and as you can see
35

35

00:01:36,870  -->  00:01:41,080
our accuracies are more or less around 83%,
36

36

00:01:41,080  -->  00:01:43,450
the relevant accuracy that we obtained
37

37

00:01:43,450  -->  00:01:45,230
with k-fold cross-validation.
38

38

00:01:45,230  -->  00:01:47,740
But just in case you used the deep learning models
39

39

00:01:47,740  -->  00:01:50,040
you learned in this course, on your datasets
40

40

00:01:50,040  -->  00:01:52,370
to accomplish your goals and you observed some
41

41

00:01:52,370  -->  00:01:55,620
overfitting, or high variance, well I want you to have
42

42

00:01:55,620  -->  00:01:57,730
the solution to solve this problem.
43

43

00:01:57,730  -->  00:02:01,720
And this solution is the dropout regularization.
44

44

00:02:01,720  -->  00:02:05,450
So simply in this tutorial we will implement dropouts.
45

45

00:02:05,450  -->  00:02:07,020
And so how does it work?
46

46

00:02:07,020  -->  00:02:10,640
Where do we need to apply dropout to our artificial
47

47

00:02:10,640  -->  00:02:12,870
neural network and how does it work?
48

48

00:02:12,870  -->  00:02:15,530
Well dropout works this way.
49

49

00:02:15,530  -->  00:02:18,110
At each iteration of the training some neurons
50

50

00:02:18,110  -->  00:02:21,200
of your artificial neural network are randomly disabled
51

51

00:02:21,200  -->  00:02:24,090
to prevent them from being too dependent on each other
52

52

00:02:24,090  -->  00:02:26,690
when they learn the correlations and therefore,
53

53

00:02:26,690  -->  00:02:29,460
by over-writing these neurons the artificial neural
54

54

00:02:29,460  -->  00:02:34,370
network learns several independent correlations in the data,
55

55

00:02:34,370  -->  00:02:36,980
because each time there is not the same configuration
56

56

00:02:36,980  -->  00:02:38,060
of the neurons.
57

57

00:02:38,060  -->  00:02:41,250
And the fact that we get these independent correlations
58

58

00:02:41,250  -->  00:02:43,980
of the data, thanks to the fact that the neurons work
59

59

00:02:43,980  -->  00:02:47,030
more independently, that prevents the neurons from learning
60

60

00:02:47,030  -->  00:02:50,200
too much and therefore that prevents overfitting.
61

61

00:02:50,200  -->  00:02:53,120
Okay, so that's how it works and now let's see how
62

62

00:02:53,120  -->  00:02:57,490
we can implement dropout within this implementation of our
63

63

00:02:57,490  -->  00:02:59,190
artificial neural network.
64

64

00:02:59,190  -->  00:03:02,400
Okay so first we need to import a new class besides
65

65

00:03:02,400  -->  00:03:04,720
the sequential class and the dense class.
66

66

00:03:04,720  -->  00:03:08,490
This class is the dropout class, very simply.
67

67

00:03:08,490  -->  00:03:10,770
So we import this class from
68

68

00:03:10,770  -->  00:03:15,050
keras.layers, same as for dense.
69

69

00:03:15,050  -->  00:03:19,730
And from this layers module we import dropout.
70

70

00:03:19,730  -->  00:03:21,490
Alright, so that's dropout.
71

71

00:03:21,490  -->  00:03:24,900
We can actually try to import it.
72

72

00:03:24,900  -->  00:03:27,700
Here we go, dropout imported.
73

73

00:03:27,700  -->  00:03:29,950
And now where exactly in the neural network
74

74

00:03:29,950  -->  00:03:31,720
are we going to apply dropout?
75

75

00:03:31,720  -->  00:03:34,590
So as you understood, dropout is applied to the neurons
76

76

00:03:34,590  -->  00:03:37,100
so that some of them randomly become disabled
77

77

00:03:37,100  -->  00:03:38,640
at each iteration.
78

78

00:03:38,640  -->  00:03:41,620
And so basically what we need to do is to apply dropout
79

79

00:03:41,620  -->  00:03:43,070
to the layers.
80

80

00:03:43,070  -->  00:03:46,040
It can be to one layer, or it can be to several layers.
81

81

00:03:46,040  -->  00:03:48,990
So just to show you, I'm going to apply to several layers.
82

82

00:03:48,990  -->  00:03:53,080
I'm going to apply it to the first hidden layer
83

83

00:03:53,080  -->  00:03:55,740
and I'm going to apply it to the second hidden layer.
84

84

00:03:55,740  -->  00:03:58,500
And my advice is that when you have overfitting,
85

85

00:03:58,500  -->  00:04:01,610
you should apply dropout to all the layers, because that
86

86

00:04:01,610  -->  00:04:04,300
will give you more chance to reduce overfitting.
87

87

00:04:04,300  -->  00:04:07,790
Alright so let's apply dropout to the first hidden layer.
88

88

00:04:07,790  -->  00:04:11,980
So I'm gonna add here with dropout
89

89

00:04:13,070  -->  00:04:15,980
and we're gonna add a new line
90

90

00:04:15,980  -->  00:04:18,270
right after we added the hidden layer
91

91

00:04:18,270  -->  00:04:19,730
with the dense function.
92

92

00:04:19,730  -->  00:04:22,810
And actually that's very simple, we take our classifier
93

93

00:04:23,750  -->  00:04:27,850
then dot and then again add because we're actually adding
94

94

00:04:27,850  -->  00:04:31,750
dropout and then the add function, you probably guess what
95

95

00:04:31,750  -->  00:04:34,620
we're going to input, well of course we have to input
96

96

00:04:34,620  -->  00:04:39,620
dropout parentheses and now we have several arguments.
97

97

00:04:40,010  -->  00:04:45,010
So let's inspect dropout by pressing command of Ctrl I
98

98

00:04:45,470  -->  00:04:47,780
and let's have a look at the arguments.
99

99

00:04:47,780  -->  00:04:51,350
Okay so the first argument is P, which is a code between
100

100

00:04:51,350  -->  00:04:54,210
zero and one, and that's the fraction of the input you need,
101

101

00:04:54,210  -->  00:04:55,390
you want to drop.
102

102

00:04:55,390  -->  00:04:58,040
So basically that's the fraction of the neurons
103

103

00:04:58,040  -->  00:05:02,530
you want to drop, or as I said disable at each iteration.
104

104

00:05:02,530  -->  00:05:04,850
So let's suppose we have 10 neurons.
105

105

00:05:04,850  -->  00:05:09,410
Well if we choose P equals 0.1, 10%, then that means
106

106

00:05:09,410  -->  00:05:13,180
that at each iteration, one neuron will be disabled.
107

107

00:05:13,180  -->  00:05:16,720
If P equals 0.2, two neurons will be disabled.
108

108

00:05:16,720  -->  00:05:20,040
And so which value of P should we input?
109

109

00:05:20,040  -->  00:05:23,370
Well my advice is that when you have overfitting,
110

110

00:05:23,370  -->  00:05:27,490
that you start trying with P equals 0.1, 10%,
111

111

00:05:27,490  -->  00:05:30,050
and then if it doesn't solve the problem, if you still
112

112

00:05:30,050  -->  00:05:33,430
have overfitting, then you try a higher value of P.
113

113

00:05:33,430  -->  00:05:35,790
And you increment it for example by 0.1.
114

114

00:05:35,790  -->  00:05:39,140
So you first try with .01 and then if you still have
115

115

00:05:39,140  -->  00:05:41,180
overfitting, you try with 0.2.
116

116

00:05:41,180  -->  00:05:44,050
If you still have overfitting you try with 0.3.
117

117

00:05:44,050  -->  00:05:46,430
And until you manage to reduce overfitting.
118

118

00:05:46,430  -->  00:05:49,840
Because of course when you reach one all your neurons
119

119

00:05:49,840  -->  00:05:52,320
will be disabled and then what you'll get is not
120

120

00:05:52,320  -->  00:05:55,490
overfitting but underfitting because nothing will be learnt.
121

121

00:05:55,490  -->  00:05:58,180
There will be no neurons to learn anything.
122

122

00:05:58,180  -->  00:06:01,940
So in general don't try to go over 0.5 because then
123

123

00:06:01,940  -->  00:06:03,740
you'll get too close to underfitting.
124

124

00:06:03,740  -->  00:06:06,640
And so what I recommend is to try with 0.1,
125

125

00:06:06,640  -->  00:06:10,200
then try 0.2, 0.3, 0.4 and that should do it.
126

126

00:06:10,200  -->  00:06:13,930
So here, just to give you a start, I'm going to input here
127

127

00:06:13,930  -->  00:06:18,620
P equals 0.1 and that's your first try.
128

128

00:06:18,620  -->  00:06:23,620
Alright, and that adds dropout to your first hidden layer.
129

129

00:06:24,240  -->  00:06:27,630
And then very simply, if you want to apply dropout
130

130

00:06:27,630  -->  00:06:31,770
to your other layers, for example this second hidden layer,
131

131

00:06:31,770  -->  00:06:34,070
well that's very simple, you just paste that here
132

132

00:06:34,070  -->  00:06:36,430
and you don't change anything.
133

133

00:06:36,430  -->  00:06:37,410
That's exactly the same.
134

134

00:06:37,410  -->  00:06:40,790
You add dropout here to your second hidden layer
135

135

00:06:40,790  -->  00:06:42,810
and you specify that you want to overwrite,
136

136

00:06:42,810  -->  00:06:46,850
disable 10% of the neurons in the second hidden layer
137

137

00:06:46,850  -->  00:06:49,070
at each iteration.
138

138

00:06:49,070  -->  00:06:51,690
Alright, so now you know what to do in case you have to deal
139

139

00:06:51,690  -->  00:06:54,770
with overfitting and now we're gonna get to the fun part
140

140

00:06:54,770  -->  00:06:56,570
which is parameter tuning.
141

141

00:06:56,570  -->  00:07:00,080
This is going to be the real deal about improving your
142

142

00:07:00,080  -->  00:07:02,650
model's performance because basically we're going to
143

143

00:07:02,650  -->  00:07:05,330
tune our model and we're gonna find the best
144

144

00:07:05,330  -->  00:07:08,180
hyper parameters, like the best of number of epoch,
145

145

00:07:08,180  -->  00:07:11,530
the best batch size, the best optimizer, so that
146

146

00:07:11,530  -->  00:07:14,530
we get the best artificial neural network
147

147

00:07:14,530  -->  00:07:18,400
that will allow us to reach that 86% accuracy
148

148

00:07:18,400  -->  00:07:20,460
and make our challenge successful.
149

149

00:07:20,460  -->  00:07:22,790
That's exactly what we're gonna do in the next tutorial.
150

150

00:07:22,790  -->  00:07:24,540
And until then enjoy deep learning.
