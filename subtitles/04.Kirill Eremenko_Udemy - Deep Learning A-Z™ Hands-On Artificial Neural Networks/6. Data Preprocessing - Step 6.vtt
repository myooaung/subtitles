WEBVTT
1
1

00:00:00.400  -->  00:00:01.510
<v Instructor>Hello and welcome</v>
2

2

00:00:01.510  -->  00:00:05.560
to this almost final tutorial of data pre-processing.
3

3

00:00:05.560  -->  00:00:07.220
I look forward to being well prepared
4

4

00:00:07.220  -->  00:00:10.750
with our data to start making our machine learning models.
5

5

00:00:10.750  -->  00:00:11.940
It's gonna be very fun.
6

6

00:00:11.940  -->  00:00:14.000
We just have to hold one for two more tutorials
7

7

00:00:14.000  -->  00:00:15.770
and then we're good to go.
8

8

00:00:15.770  -->  00:00:18.500
So, today we're gonna talk about feature scaling
9

9

00:00:18.500  -->  00:00:21.030
which is very important in machine learning.
10

10

00:00:21.030  -->  00:00:23.250
I'm going to explain you why right now.
11

11

00:00:23.250  -->  00:00:25.340
Let's explain what is feature scaling
12

12

00:00:25.340  -->  00:00:27.169
and why we need to do it.
13

13

00:00:27.169  -->  00:00:31.090
As you can see, we have these two columns, Age and Salary
14

14

00:00:31.090  -->  00:00:33.720
that contains numerical numbers.
15

15

00:00:33.720  -->  00:00:35.720
You notice that the variables
16

16

00:00:35.720  -->  00:00:37.500
are not on the same scale
17

17

00:00:37.500  -->  00:00:42.250
because the age are going from 27 to 50
18

18

00:00:44.170  -->  00:00:48.500
and the salary's going from 40 K to like 90 K.
19

19

00:00:48.500  -->  00:00:52.230
So, because this Age variable and the Salary variable
20

20

00:00:52.230  -->  00:00:53.860
don't have the same scale,
21

21

00:00:53.860  -->  00:00:55.990
this will cause some issues
22

22

00:00:55.990  -->  00:00:58.570
in your machine learning models and why is that?
23

23

00:00:58.570  -->  00:01:00.830
It's because your machine learning models,
24

24

00:01:00.830  -->  00:01:02.180
a lot of machine learning models
25

25

00:01:02.180  -->  00:01:05.610
are based on what is called the Euclidean Distance.
26

26

00:01:05.610  -->  00:01:07.250
If you remember that back from high school,
27

27

00:01:07.250  -->  00:01:09.460
the Euclidean Distance between two data points,
28

28

00:01:09.460  -->  00:01:11.830
between two points is the square root
29

29

00:01:11.830  -->  00:01:14.120
of the sum of the squared coordinates.
30

30

00:01:14.120  -->  00:01:16.250
Well, actually here it's the same.
31

31

00:01:16.250  -->  00:01:18.860
We have two variables, Age and Salary,
32

32

00:01:18.860  -->  00:01:21.304
so you can picture Age as the x-coordinate
33

33

00:01:21.304  -->  00:01:23.940
and the salary as the y-coordinate
34

34

00:01:23.940  -->  00:01:26.480
and in the machine learning model's equations,
35

35

00:01:26.480  -->  00:01:30.010
some Euclidean Distances between observation points,
36

36

00:01:30.010  -->  00:01:32.440
for example, this one and this one
37

37

00:01:32.440  -->  00:01:35.820
are computed based on these two coordinates
38

38

00:01:35.820  -->  00:01:39.310
and actually since the salary has a much wider range
39

39

00:01:39.310  -->  00:01:42.870
of values, it's because it's going from zero to 100 K,
40

40

00:01:42.870  -->  00:01:47.870
the Euclidean Distance will be dominated by the Salary,
41

41

00:01:47.920  -->  00:01:49.680
because for example, if we take two observations,
42

42

00:01:49.680  -->  00:01:52.460
for example, this one, the ninth one,
43

43

00:01:52.460  -->  00:01:55.210
and the third one,
44

44

00:01:55.210  -->  00:01:57.860
well, the Euclidean Distance will compute the difference
45

45

00:01:57.860  -->  00:02:02.000
between this salary and this salary, so let's compute it.
46

46

00:02:02.000  -->  00:02:07.000
That's about, so this minus this one.
47

47

00:02:08.640  -->  00:02:11.590
As you can see, this is 31,000.
48

48

00:02:11.590  -->  00:02:12.973
If you put that in square,
49

49

00:02:14.160  -->  00:02:17.680
let's see, up square, that gives this.
50

50

00:02:17.680  -->  00:02:21.570
And now let's take for the same two observations the ages.
51

51

00:02:21.570  -->  00:02:23.210
So, let's compute.
52

52

00:02:23.210  -->  00:02:28.210
Equals 48 minus, it was this one, right?
53

53

00:02:28.441  -->  00:02:30.840
27, that's the difference
54

54

00:02:30.840  -->  00:02:32.890
and now let's take the square
55

55

00:02:32.890  -->  00:02:36.253
equals this square.
56

56

00:02:36.253  -->  00:02:38.270
441.
57

57

00:02:38.270  -->  00:02:39.640
So, you can see very clearly
58

58

00:02:39.640  -->  00:02:44.640
how this square difference dominates this square difference
59

59

00:02:44.780  -->  00:02:46.730
and that's because these two variables
60

60

00:02:46.730  -->  00:02:48.230
are not on the same scale.
61

61

00:02:48.230  -->  00:02:50.480
So, in the machine learning equations,
62

62

00:02:50.480  -->  00:02:53.000
it will be like this doesn't exist
63

63

00:02:53.000  -->  00:02:55.080
because it will be dominated by this,
64

64

00:02:55.080  -->  00:02:58.950
so that's why we absolutely need to put the variables
65

65

00:02:58.950  -->  00:03:00.170
in the same scale,
66

66

00:03:00.170  -->  00:03:03.080
that is that we're gonna transform these two variables
67

67

00:03:03.080  -->  00:03:06.530
and they're gonna have values in the same range,
68

68

00:03:06.530  -->  00:03:07.760
for example, they're gonna have values
69

69

00:03:07.760  -->  00:03:09.750
from minus one to plus one here
70

70

00:03:09.750  -->  00:03:11.910
and same here, minus one to plus one
71

71

00:03:11.910  -->  00:03:13.600
so that we don't get this sort of problem
72

72

00:03:13.600  -->  00:03:18.600
with a huge number here dominating a smaller number here
73

73

00:03:18.710  -->  00:03:21.810
so that eventually the sampler number doesn't exist.
74

74

00:03:21.810  -->  00:03:24.800
There are several ways of scaling your data.
75

75

00:03:24.800  -->  00:03:26.640
A very common one is the standardization
76

76

00:03:26.640  -->  00:03:28.730
which means that for each observation
77

77

00:03:28.730  -->  00:03:32.000
and each feature, you withdraw the mean value
78

78

00:03:32.000  -->  00:03:33.810
of all the values of the feature
79

79

00:03:33.810  -->  00:03:36.170
and you divide it by the standard deviation.
80

80

00:03:36.170  -->  00:03:38.310
So, that's the first type of feature scaling
81

81

00:03:38.310  -->  00:03:40.570
and another type is normalization
82

82

00:03:40.570  -->  00:03:43.780
which means that you substract your observation feature x
83

83

00:03:43.780  -->  00:03:46.530
by the minimum value of all the feature values
84

84

00:03:46.530  -->  00:03:47.970
and you divide it by the difference
85

85

00:03:47.970  -->  00:03:50.510
between the maximum of your feature values
86

86

00:03:50.510  -->  00:03:52.360
and the minimum of your feature values.
87

87

00:03:52.360  -->  00:03:53.810
Don't worry if you're not very comfortable
88

88

00:03:53.810  -->  00:03:54.840
with the mathematics here
89

89

00:03:54.840  -->  00:03:56.430
but what you need to understand
90

90

00:03:56.430  -->  00:03:59.030
is that we are putting our variables in the same range,
91

91

00:03:59.030  -->  00:04:01.820
in the same scale so that no variable
92

92

00:04:01.820  -->  00:04:04.090
is dominated by the other.
93

93

00:04:04.090  -->  00:04:05.160
So, let's do it right now.
94

94

00:04:05.160  -->  00:04:07.410
Anyway, you're gonna see how the variables
95

95

00:04:07.410  -->  00:04:08.360
are gonna be transformed.
96

96

00:04:08.360  -->  00:04:10.920
You're gonna see how they go from having large
97

97

00:04:10.920  -->  00:04:12.480
and very different values
98

98

00:04:12.480  -->  00:04:16.600
to small and same values, so let's start right now.
99

99

00:04:16.600  -->  00:04:19.810
So, as usual, we're gonna import a library to do this
100

100

00:04:19.810  -->  00:04:22.840
and we know this library, we already saw it,
101

101

00:04:22.840  -->  00:04:24.870
it's the preprocessing library
102

102

00:04:24.870  -->  00:04:27.100
and from this library, we're going to import
103

103

00:04:27.100  -->  00:04:30.550
the standard scalar class, so let's do it.
104

104

00:04:30.550  -->  00:04:33.960
We're going to import sklearn.preprocessing,
105

105

00:04:35.780  -->  00:04:38.250
so that's the preprocessing library
106

106

00:04:38.250  -->  00:04:40.283
import StandardScalar,
107

107

00:04:45.900  -->  00:04:48.360
so that's the class we need to import,
108

108

00:04:48.360  -->  00:04:51.110
then as usual, we're going to create an object
109

109

00:04:51.110  -->  00:04:55.310
of this class and we're going to call it sc for scale
110

110

00:04:56.974  -->  00:04:59.920
_X because you will see later
111

111

00:04:59.920  -->  00:05:03.470
that we might need to create another object
112

112

00:05:03.470  -->  00:05:07.100
that will scale the dependent variable vector
113

113

00:05:07.100  -->  00:05:08.520
but so far we will only need
114

114

00:05:08.520  -->  00:05:11.850
to scale the x matrix of features.
115

115

00:05:11.850  -->  00:05:16.510
So, sc_X and then here StandardScalar
116

116

00:05:17.810  -->  00:05:20.890
which is a class and parenthesis
117

117

00:05:22.670  -->  00:05:26.580
and now very simply we will directly fit and transform
118

118

00:05:26.580  -->  00:05:29.170
our training set and our test set.
119

119

00:05:29.170  -->  00:05:30.090
So, let's do this.
120

120

00:05:30.090  -->  00:05:34.890
We're gonna transform X_train, so we will recompute X_train
121

121

00:05:34.890  -->  00:05:36.420
because we want it to be scaled
122

122

00:05:36.420  -->  00:05:39.300
and so, to do this, we're taking our object sc_X
123

123

00:05:41.364  -->  00:05:45.300
and then we're calling the method fit and transform,
124

124

00:05:45.300  -->  00:05:47.700
so that's very important to understand that.
125

125

00:05:47.700  -->  00:05:51.800
When you are applying your object, StandardScalar object
126

126

00:05:51.800  -->  00:05:53.700
to your training set,
127

127

00:05:53.700  -->  00:05:55.890
you have to fit the object to the training set
128

128

00:05:55.890  -->  00:05:57.630
and then transform it
129

129

00:05:57.630  -->  00:05:58.810
and you're gonna see that it's not going
130

130

00:05:58.810  -->  00:06:00.380
to be the same thing for the test set
131

131

00:06:00.380  -->  00:06:02.880
because we will only transform the test set
132

132

00:06:02.880  -->  00:06:03.890
but for the training set,
133

133

00:06:03.890  -->  00:06:07.744
we need to fit it and then transform the training set.
134

134

00:06:07.744  -->  00:06:12.744
So, here we will just need to add as an argument X_train
135

135

00:06:13.480  -->  00:06:14.640
and that's it
136

136

00:06:14.640  -->  00:06:16.430
and now same for the test set.
137

137

00:06:16.430  -->  00:06:18.263
So we are going to copy this line,
138

138

00:06:19.540  -->  00:06:23.430
paste it here and here test
139

139

00:06:25.440  -->  00:06:28.410
and then X_test here as well
140

140

00:06:28.410  -->  00:06:29.930
and now as I just said,
141

141

00:06:29.930  -->  00:06:34.300
for the test set, we don't need to fit the sc_X object
142

142

00:06:34.300  -->  00:06:35.200
to the test set
143

143

00:06:35.200  -->  00:06:37.530
because it's already fitted to the training set,
144

144

00:06:37.530  -->  00:06:40.450
so here I will remove fit
145

145

00:06:42.290  -->  00:06:43.703
and we're good to go.
146

146

00:06:44.647  -->  00:06:48.160
Now, there are two questions that we can ask ourselves.
147

147

00:06:48.160  -->  00:06:50.330
The first one is really important
148

148

00:06:50.330  -->  00:06:53.310
and you will find a lot of these questions on Google.
149

149

00:06:53.310  -->  00:06:55.840
This question is do we need to fit
150

150

00:06:55.840  -->  00:06:58.050
and transform the dummy variables?
151

151

00:06:58.050  -->  00:06:59.670
Because as you can see,
152

152

00:06:59.670  -->  00:07:03.370
dummy variables, let's change the format here for a second,
153

153

00:07:03.370  -->  00:07:07.280
zero, as you can see, the dummy variables takes values
154

154

00:07:07.280  -->  00:07:09.030
either zero or one,
155

155

00:07:09.030  -->  00:07:10.220
so do we need to scale that
156

156

00:07:10.220  -->  00:07:12.440
because it seems to be already scaled, right?
157

157

00:07:12.440  -->  00:07:14.440
So, if you ask this question to Google,
158

158

00:07:14.440  -->  00:07:16.020
you will find several answers.
159

159

00:07:16.020  -->  00:07:18.300
You will find several teams for that.
160

160

00:07:18.300  -->  00:07:21.040
Some will say that no, you don't need to scale
161

161

00:07:21.040  -->  00:07:23.190
these dummy variables, some say that yes,
162

162

00:07:23.190  -->  00:07:25.210
you need to do it because you want some accuracy
163

163

00:07:25.210  -->  00:07:28.740
in your predictions and if you're interested in my opinion,
164

164

00:07:28.740  -->  00:07:31.560
I would sat that it depends on the context.
165

165

00:07:31.560  -->  00:07:34.700
It depends on how much you wanna keep interpretation
166

166

00:07:34.700  -->  00:07:35.950
in your models
167

167

00:07:35.950  -->  00:07:37.620
because if we scale this,
168

168

00:07:37.620  -->  00:07:38.453
it will be good
169

169

00:07:38.453  -->  00:07:40.240
because everything will be on the same scale,
170

170

00:07:40.240  -->  00:07:41.550
we will be happy with that
171

171

00:07:41.550  -->  00:07:43.520
and it will be good for our predictions
172

172

00:07:43.520  -->  00:07:45.290
but we will lose the interpretation
173

173

00:07:45.290  -->  00:07:47.590
of knowing which observations belongs
174

174

00:07:47.590  -->  00:07:49.680
to which country, et cetera.
175

175

00:07:49.680  -->  00:07:51.100
So, it's as you want.
176

176

00:07:51.100  -->  00:07:52.410
It won't break your model
177

177

00:07:52.410  -->  00:07:54.640
if you don't scale the dummy variables
178

178

00:07:54.640  -->  00:07:57.230
because they will be actually on the same scale
179

179

00:07:57.230  -->  00:07:59.800
as the future scaled variables here
180

180

00:07:59.800  -->  00:08:02.810
which we'll take values between minus one and one I think,
181

181

00:08:02.810  -->  00:08:03.960
we will see
182

182

00:08:03.960  -->  00:08:06.030
but here since this is our final tutorial
183

183

00:08:06.030  -->  00:08:08.610
and we won't have any interpretation to make,
184

184

00:08:08.610  -->  00:08:10.543
we will scale those dummy variables.
185

185

00:08:11.830  -->  00:08:13.060
So, that's how you feature scale,
186

186

00:08:13.060  -->  00:08:15.330
now let's not forget to select this
187

187

00:08:15.330  -->  00:08:18.160
and Command and Control + Enter to execute
188

188

00:08:18.160  -->  00:08:19.190
and here we go.
189

189

00:08:19.190  -->  00:08:22.600
So, now let's check our X_train and X_test.
190

190

00:08:22.600  -->  00:08:24.870
Let's double click on X_train.
191

191

00:08:24.870  -->  00:08:26.750
Here's X_train, as you can see,
192

192

00:08:26.750  -->  00:08:28.050
all the variables now belong
193

193

00:08:28.050  -->  00:08:30.690
to the same range, to the same scale.
194

194

00:08:30.690  -->  00:08:32.370
As you can see, all the variables seems
195

195

00:08:32.370  -->  00:08:35.550
to be between minus one and plus one,
196

196

00:08:35.550  -->  00:08:38.130
so that's perfect, that will largely improve
197

197

00:08:38.130  -->  00:08:39.500
your machine learning models
198

198

00:08:39.500  -->  00:08:40.950
and besides, I forgot to mention
199

199

00:08:40.950  -->  00:08:44.060
that even if sometimes machine learning models
200

200

00:08:44.060  -->  00:08:46.890
are not based on Euclidean Distances,
201

201

00:08:46.890  -->  00:08:48.830
we will still need to do feature scaling
202

202

00:08:48.830  -->  00:08:51.750
because the algorithm will converge much faster.
203

203

00:08:51.750  -->  00:08:53.730
That will be the case for decision trees.
204

204

00:08:53.730  -->  00:08:56.660
Decision trees are not based on Euclidean Distances
205

205

00:08:56.660  -->  00:08:58.690
but you will see that we will need to do feature scaling
206

206

00:08:58.690  -->  00:08:59.970
because if we don't do it,
207

207

00:08:59.970  -->  00:09:02.381
they will run for a very long time.
208

208

00:09:02.381  -->  00:09:06.110
And let's have a quick look at X_test, that's X_test,
209

209

00:09:06.110  -->  00:09:07.860
it's also feature scaled
210

210

00:09:07.860  -->  00:09:10.020
and you have to understand that the feature scaling here
211

211

00:09:10.020  -->  00:09:12.820
next test is the same as the feature scaling
212

212

00:09:12.820  -->  00:09:16.640
on X_train simply because the object StandardScalar
213

213

00:09:16.640  -->  00:09:18.740
was fitted to X_train.
214

214

00:09:18.740  -->  00:09:22.900
That's why it's important to fit the object to X_train first
215

215

00:09:22.900  -->  00:09:27.310
so that X_train and X_test are scaled on the same basis.
216

216

00:09:27.310  -->  00:09:28.970
And now I promised you another question.
217

217

00:09:28.970  -->  00:09:31.980
The other question was do we need to apply feature scaling
218

218

00:09:31.980  -->  00:09:34.060
to y, the dependent variable vector?
219

219

00:09:34.060  -->  00:09:36.360
So, to Y_train and to Y_test?
220

220

00:09:36.360  -->  00:09:39.320
Here as you can see, the dependent variable vector
221

221

00:09:39.320  -->  00:09:40.540
is a category called variable
222

222

00:09:40.540  -->  00:09:43.470
because it's taking only two values, zero and one,
223

223

00:09:43.470  -->  00:09:45.700
and now the question is do we need to apply feature scaling
224

224

00:09:45.700  -->  00:09:46.930
on this one?
225

225

00:09:46.930  -->  00:09:48.650
The answer is no this time.
226

226

00:09:48.650  -->  00:09:51.350
We don't need to apply feature scaling to this one.
227

227

00:09:51.350  -->  00:09:53.010
However, here we don't need to do it
228

228

00:09:53.010  -->  00:09:55.490
because this is a classification problem
229

229

00:09:55.490  -->  00:09:57.940
with a category called dependent variable
230

230

00:09:57.940  -->  00:10:00.180
but you will see that for regression,
231

231

00:10:00.180  -->  00:10:02.040
when the dependent variable will take
232

232

00:10:02.040  -->  00:10:04.320
a huge range of values,
233

233

00:10:04.320  -->  00:10:06.110
we will need to apply feature scaling
234

234

00:10:06.110  -->  00:10:09.060
to the dependent variable y as well.
235

235

00:10:09.060  -->  00:10:10.800
So, that's it for feature scaling.
236

236

00:10:10.800  -->  00:10:13.150
Now you know how to apply feature scaling
237

237

00:10:13.150  -->  00:10:15.580
to your data and mostly congratulations
238

238

00:10:15.580  -->  00:10:18.410
because we did all the required steps
239

239

00:10:18.410  -->  00:10:20.390
to pre-process our data,
240

240

00:10:20.390  -->  00:10:21.670
feature scaling was the last one
241

241

00:10:21.670  -->  00:10:23.070
because the next tutorial
242

242

00:10:23.070  -->  00:10:25.840
will be about this data pre-processing template
243

243

00:10:25.840  -->  00:10:28.330
and I will just explain how we are going to use it
244

244

00:10:28.330  -->  00:10:29.610
in our machine models,
245

245

00:10:29.610  -->  00:10:32.170
it's gonna be very fast and practical,
246

246

00:10:32.170  -->  00:10:34.310
so we are done with the data pre-processing.
247

247

00:10:34.310  -->  00:10:36.840
Congratulations, you did the most difficult part
248

248

00:10:36.840  -->  00:10:38.670
and now it's time to have fun.
249

249

00:10:38.670  -->  00:10:40.440
It's time to start making the models
250

250

00:10:40.440  -->  00:10:42.950
and I can't wait to start them with you.
251

251

00:10:42.950  -->  00:10:44.980
So, thank you for watching this tutorial.
252

252

00:10:44.980  -->  00:10:46.870
I look forward to seeing you on the next one
253

253

00:10:46.870  -->  00:10:48.820
and until then, enjoy machine learning.
