WEBVTT

00:00.390 --> 00:05.760
Hello and welcome back to the course on a deep learning I hope you're as excited about this section

00:05.760 --> 00:08.670
of the course on recurrent neural networks as I am.

00:08.710 --> 00:17.990
We're slowly venturing into the very complex very forward looking and cutting edge areas of deep learning.

00:18.000 --> 00:20.280
And this is going to be very very fun.

00:20.280 --> 00:24.960
So today we're going to talk about how we're going to approach this section which contains so many different

00:24.960 --> 00:30.150
complex topics so many concepts that we need to get our head around.

00:30.150 --> 00:35.700
All right so in this section we will learn first of all the idea behind a recurrent neural networks.

00:35.700 --> 00:42.120
We'll see how they compare to the human brain will understand what makes them unique and special as

00:42.120 --> 00:45.330
compared to regular artificial neural networks.

00:45.420 --> 00:53.070
Then we'll talk about the vanishing gradient problem something that has been a major roadblock in or

00:53.070 --> 00:58.740
had been a major roadblock in the development and utilization of recurrent neural networks something

00:58.740 --> 01:01.830
that prevented them from being what they are now.

01:02.130 --> 01:08.850
And then we will move onto the solution that one of the most popular solutions to the vanishing Green

01:08.850 --> 01:14.850
problem the long short term memory or ellis T.M. neural networks and we'll talk about their architecture

01:15.060 --> 01:21.600
be very exciting to Charles one of my favorite topics and we will find out exactly how they work and

01:21.660 --> 01:27.480
what what that complex structure is inside them we'll break it down into simple terms and you will be

01:27.480 --> 01:35.280
able to walk away with a pretty solid understanding of elist teams and then we'll talk about the practical

01:35.280 --> 01:40.330
intuition so in that pre-install we will have a practical example of using LACMA.

01:40.350 --> 01:47.060
But in this practical intuition Troil We'll look at some great examples posted by one of the researchers.

01:47.060 --> 01:53.130
One of the most well-known researchers in the space and we all understand even better on an intuitive

01:53.130 --> 01:59.100
level how elist teams actually work how they think will be like neuroscientists trying to understand

01:59.100 --> 02:03.480
what's going on in the brain of an Ellis team is going to be very exciting as well.

02:03.510 --> 02:09.420
And then at the end we'll have an extra tutorial on elist him variations something special something

02:10.050 --> 02:15.510
you don't really have to take this tutorial but it's very quick just to get you up to speed on what

02:15.540 --> 02:22.950
other options of LACMA exist out there in the world what other architectures you might come across in

02:22.950 --> 02:23.780
your work.

02:23.850 --> 02:27.350
So hopefully you're excited and ready to get started.

02:27.450 --> 02:29.720
And I can't wait to see the next tutorial.

02:29.730 --> 02:31.650
Until then enjoy deep learning.
