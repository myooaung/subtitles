WEBVTT

00:00.210 --> 00:07.240
Hello and welcome to this new tutorial now it is time to implement the Wii part of the Wii meaning we're

00:07.260 --> 00:14.910
going to implement this the classic way of getting the final late and vector Z because indeed right

00:14.910 --> 00:23.440
now we are at this specific step where we've just flattened the result of these four convolutions into

00:23.440 --> 00:30.180
a one dimensional vector of two times two times two hundred and fifty six elements and death would be

00:30.480 --> 00:34.310
the final latent vector of a classic encoder.

00:34.350 --> 00:41.610
But here we were building a variational our encoder and therefore instead of returning this latent vector

00:41.610 --> 00:46.250
Z in a deterministic way as a result of these four convolutions.

00:46.380 --> 00:54.480
Well as you can see we're going to sample it from a factor Gaussian distribution of mean new and variance

00:54.480 --> 00:55.420
Sigma.

00:55.440 --> 01:02.370
So the way we're going to do this is we're going to make two separate fully connected layers MU and

01:02.370 --> 01:03.020
sigma.

01:03.300 --> 01:07.450
And we're going to build them with the dense function by tensor flow.

01:07.460 --> 01:13.710
That's why you see dense here and dense is always the name of the classes and functions to build fully

01:13.710 --> 01:21.090
connected layers and then using the random normal distribution by sense of flow we're going to generate

01:21.150 --> 01:28.130
or you know simple some numbers from a normal distribution meaning Gaussian distribution of zero and

01:28.140 --> 01:29.120
variance one.

01:29.340 --> 01:36.180
And we're going to add all this to obtain the final latent vector which is the result of some stochastic

01:36.420 --> 01:37.730
operations.

01:37.740 --> 01:43.090
Therefore it's given in a stochastic way as opposed to a deterministic way.

01:43.350 --> 01:50.140
So let's do this let's build these two fully connected layers and then let's sample our final Late in

01:50.170 --> 01:53.960
fact z from the fact that Gaussian distribution.

01:54.000 --> 01:56.000
So back on Python here we go.

01:56.040 --> 02:00.320
Let's start with the first fully connected layer which is me.

02:00.480 --> 02:08.400
So I'm going to introduce a new variable here self that new which is going to be well a fully connected

02:08.400 --> 02:12.330
layer which we can create thanks to a of the following way.

02:12.360 --> 02:17.970
We're calling for instance flow and from tons of flow we're getting first the layers module and from

02:17.970 --> 02:21.350
the layers module we're going to get that dense function.

02:21.620 --> 02:26.370
And this function is going to take three arguments that we have to specify here.

02:26.460 --> 02:31.410
The first one is going to be the input vector of this fully connected layer.

02:31.440 --> 02:37.590
You know as an artificial neural network which you saw in step 1 artificial neural networks.

02:37.650 --> 02:44.160
So why do you think is going to be this input vector here of this fully connected layer Well of course

02:44.190 --> 02:51.300
that's going to be the flattened vector resulting from the four convolutions and therefore it is going

02:51.300 --> 02:52.670
to be h.

02:53.160 --> 02:59.610
And then as a second argument we need to specify how many neurons we want to have and this fully connected

02:59.610 --> 03:06.120
layer or how many units as it is the name of the argument here and we'll to figure out what this number

03:06.120 --> 03:07.020
is going to be.

03:07.050 --> 03:11.330
We just need to look again at our architecture made by the others.

03:11.430 --> 03:18.420
And since actually we're going to some these two dense layers with this normal distribution here multiply

03:18.420 --> 03:19.670
two sigma.

03:19.890 --> 03:27.000
Well actually the size of this first fully connected layer is going to be exactly that size here because

03:27.000 --> 03:32.520
data size is the size of that here and that is the sum of these two layers.

03:32.700 --> 03:37.420
And therefore what you want is 1024 units.

03:37.500 --> 03:46.100
In this first fully connected layer However this number is actually the xit size argument here.

03:46.170 --> 03:52.420
That is one of the arguments of the method meaning of the future instances of the company A-Class.

03:52.530 --> 04:00.120
When we create them and therefore we don't have to specify 1024 right now we will specify that later

04:00.270 --> 04:08.760
when we enter the argument value here and therefore we can just input self taught that size.

04:08.760 --> 04:09.770
Here we go.

04:10.110 --> 04:10.400
Right.

04:10.400 --> 04:11.020
Perfect.

04:11.040 --> 04:15.400
And then as usual we can you know add a name to that specific letter.

04:15.450 --> 04:19.850
So I'm just going to get this and we're going to replace here.

04:19.920 --> 04:21.310
Come one by.

04:21.450 --> 04:27.680
Well we can still say that we're in the encoder and we're going to replace one by fully connected.

04:27.780 --> 04:34.590
Because we're building the first fully connected live MMU in the variational part of the V so F C and

04:34.590 --> 04:37.960
then we can just say you are right.

04:37.980 --> 04:41.940
So we got the first one when we got this one.

04:42.030 --> 04:44.250
And now let's build this one sigma.

04:44.490 --> 04:47.610
So it's going to be the same technique.

04:47.610 --> 04:50.990
So I'm going to copy this then pasted below.

04:51.000 --> 04:59.340
We're going to replace me by not Sigma directly but by LUGG variants locavore because actually LUGG

04:59.340 --> 05:08.180
voice what's inside the exponential distribution right and Lavaur is going to be exactly the fully connected

05:08.180 --> 05:15.050
layer inside the exponential which is going to take again to late and vector age as input which when

05:15.080 --> 05:21.740
we turn here as a result of the four convolutions then same set size again for the same reason because

05:22.130 --> 05:31.130
we are summing these two layers to get the final latent to Z with the 1024 elements which corresponds

05:31.130 --> 05:33.140
exactly to that size.

05:33.470 --> 05:36.840
And now for the name we're going to replace that on.

05:37.020 --> 05:38.850
Seamew of course.

05:38.920 --> 05:40.830
FC look for.

05:40.850 --> 05:45.640
All right so that gives us what's going to be inside exponential of this.

05:45.680 --> 05:52.120
Second part of the some here new Plus Sigma multiply by the normal distribution.

05:52.250 --> 05:58.850
But since you actually want a Gaussian distribution of mean new and variants Sigma Well of course now

05:58.850 --> 06:02.820
we get to take the exponential and so to get it.

06:02.930 --> 06:05.430
Well now is the time to get Sigma.

06:05.630 --> 06:13.520
And therefore I'm introducing this new Sigma variable as a variable or object to solve that Sigma and

06:13.760 --> 06:21.150
Zelda Sigma is exactly going to be the exponential of love R but you know divided by two because in

06:21.290 --> 06:25.810
the formula after Galchen distribution in the denominator you actually have two.

06:26.150 --> 06:28.130
So let's take that exponential.

06:28.130 --> 06:29.700
We're going to get a sense of flow.

06:29.720 --> 06:36.290
So I'm calling sense of flow T.F. from which I'm going to take the exponential function inside which

06:36.350 --> 06:37.160
I'm going to enter.

06:37.160 --> 06:43.340
Of course self that Lugovoy are divided by two point zero.

06:43.340 --> 06:47.090
Because this is a float operation and perfect.

06:47.090 --> 06:52.100
Now we get the second fully connected layer Sigma.

06:52.340 --> 06:59.630
And so now we can just some these two but Sigma is of course going to be multiplied by the random normal

06:59.630 --> 07:05.900
distribution and the random normal distribution is actually a function of by by of flow which is the

07:05.900 --> 07:08.590
random underscore normal function.

07:08.660 --> 07:13.140
And that's exactly what we're going to use to compute that final sum.

07:13.160 --> 07:20.720
So let's do this let's get that final sum But first if you want we can you know get separately that

07:21.500 --> 07:29.540
and 0 1 you know the normal distribution meaning the in distribution of 0 and variance 1 so that you

07:29.540 --> 07:31.660
can clearly see how to get it.

07:31.700 --> 07:37.780
And so we're going to introduce that as Epsilon a new horrible object.

07:37.880 --> 07:47.090
And as we've just said we can get it by using from tens of flow the random normal function of a list

07:47.090 --> 07:49.460
of two elements in square brackets.

07:49.490 --> 07:54.920
The first one is going to be the batch size so here we go that's where we specify the batch size not

07:55.160 --> 07:56.090
right here.

07:56.090 --> 08:02.280
We're going to specify this in Epsilon here and the batch size is actually a variable of our object

08:02.610 --> 08:06.720
self-edit batch size which we initialized in the method.

08:06.860 --> 08:08.620
So that's the first element here.

08:08.720 --> 08:17.210
And the second one is going to be the size of the late and vector Z which is 1024 and which is given

08:17.450 --> 08:21.540
by also a viable or object self set size.

08:21.740 --> 08:24.970
So let's say that right here.

08:25.160 --> 08:31.370
And so now the fact that we create the random normal distribution this way will allow us to get not

08:31.370 --> 08:36.480
only a single simpled latent vector Z as the result of this.

08:36.530 --> 08:40.390
Some of these two fully connected letters MU and sigma.

08:40.490 --> 08:48.320
But the fact that we created this way will allow us to give us a batch of latent vectors Zee's all resulting

08:48.320 --> 08:52.280
from the sum of these two fully connected letters new and sigma.

08:52.610 --> 08:58.690
All right so now we're finally ready to get actually now the batch of latent vectors.

08:58.790 --> 09:05.120
And so this is going to be the simple part because now we have all the elements of this operation right

09:05.120 --> 09:10.710
here which is going to sum the first fully connected layer that we created you.

09:10.970 --> 09:17.990
And this product of the second fully integrated layer Sigma which we created as well and this normal

09:17.990 --> 09:18.910
distribution.

09:19.190 --> 09:20.270
So let's do this.

09:20.450 --> 09:22.810
We can now get the final late.

09:22.850 --> 09:34.460
Fact that and it is going to be as we've just said the sum of self that you plus self that Sigma multiplied

09:34.700 --> 09:39.510
by the normal distribution self that Absolom.

09:39.590 --> 09:40.310
Here we go.

09:40.340 --> 09:47.780
Perfect that implements the VI Part of the V which you have to understand will allow us to get in a

09:47.820 --> 09:53.840
sticky lastic way as opposed to a deterministic way to late and that uses which result therefore is

09:53.870 --> 09:57.790
to classically from our previous four convolutions.

09:57.800 --> 10:05.280
All right so we have now one more part to go it is of course the decoded part of the D and we'll do

10:05.280 --> 10:06.790
that in the next tutorial.

10:06.840 --> 10:08.890
You can actually practice to do it yourself.

10:09.120 --> 10:12.690
I'm just going to tell you the function that you have to use by sense of flow.

10:12.810 --> 10:16.000
It's going to the underscore transpose.

10:16.110 --> 10:20.840
So try to do it yourself by just following the architecture here.

10:20.970 --> 10:26.460
It's going to be exactly the same as what we've just done for the encoder except that this time you

10:26.460 --> 10:30.060
have to use it comes to the underscore transpose function.

10:30.060 --> 10:34.720
So you have all you need to do it yourself with all the information here.

10:34.770 --> 10:38.990
So try it for practice and I'll give you the solution in the next tutorial.

10:39.060 --> 10:40.640
Until then enjoy AI.
