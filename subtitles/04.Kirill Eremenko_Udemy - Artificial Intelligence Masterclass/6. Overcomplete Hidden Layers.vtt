WEBVTT

00:00.690 --> 00:04.980
Hello and welcome back to the course on deep learning in today's tutorial we're going to talk about

00:05.010 --> 00:06.800
over complete hit and Lares.

00:06.840 --> 00:11.550
But before we start I wanted to mention that this tutorial and next couple of terms are going to be

00:12.180 --> 00:20.460
quick because they are very kind of overview high level tutorials on different types of encoders and

00:20.480 --> 00:26.490
curves are quite a powerful technique or method in deep learning and they are popular and there are

00:26.490 --> 00:32.700
lots of variations of our encoders and we unfortunately couldn't go into detail and depth into all of

00:32.700 --> 00:33.360
them.

00:33.630 --> 00:38.220
But instead what we're going to do is we're going to show you the underlying concepts and principles

00:38.220 --> 00:46.080
of these variations and point you at specific papers and blogs and articles that you can read if you

00:46.080 --> 00:51.180
want to upscale on certain one of certain types of articles at the same time.

00:51.180 --> 00:59.100
Even these high level overuse will give you some very valuable insights and awareness about the different

00:59.100 --> 01:04.350
types of our encounters that exist and you'll be able to not only when you see them recognize them but

01:04.350 --> 01:10.290
also you will be able to operate with the terms with their names easily and and you won't be caught

01:10.320 --> 01:16.650
of guard when you come across them in literature or in different types of works that you'll be studying

01:16.680 --> 01:17.850
or performing.

01:17.850 --> 01:22.980
So that's the first thing that we talk about is over complete hidden layers.

01:22.980 --> 01:29.940
This is a underlying concept in a lot of or most of the variations of our encounters.

01:29.970 --> 01:30.690
So let's have a look.

01:30.690 --> 01:38.620
So here we've got an hour and quarter for input values to nodes in the hidden lair and four nodes in

01:38.630 --> 01:39.330
the output.

01:39.510 --> 01:45.570
The question is here what if we wanted to increase the number of nodes in the hidden lair.

01:45.570 --> 01:50.630
What if we wanted actually to have more nodes in the hidden lair than in the input there.

01:50.640 --> 01:52.050
Something like this.

01:52.440 --> 01:56.760
And the obvious question is here like why why would we do it.

01:56.940 --> 01:59.250
But the answer is Why not.

01:59.250 --> 02:04.300
We said that and on and out and color can be used as a feature extraction tool.

02:04.380 --> 02:06.240
But what if we want more features.

02:06.310 --> 02:10.110
Remember in artificial neural networks it was very easy for us to do.

02:10.110 --> 02:17.550
We had a certain number of inputs then we could have a whole layer of whatever number of inputs and

02:17.610 --> 02:21.920
nodes we wanted it could be six it could be 10 it could be 100 doesn't matter.

02:21.920 --> 02:24.740
We could have as many as we want we could add more letters and so on.

02:25.020 --> 02:29.790
But the point was we weren't restricted to how many nodes would have in here and there.

02:30.120 --> 02:35.650
And that allowed us to extract more features and that would be great in the case of an auto encoder

02:35.670 --> 02:40.980
as well given that we are advocating it as a feature extraction tool.

02:41.460 --> 02:42.750
But we have a problem.

02:42.930 --> 02:48.700
If we were to do this obviously the encoder can cheat the OT encoder.

02:48.750 --> 02:53.670
This goal is to get the outputs the outputs to equate to the inputs.

02:53.790 --> 02:59.130
As soon as you give it 4 or more hidden nodes in case you have four.

02:59.220 --> 03:04.530
Basically as soon as you give it a hidden layer which is the same size or greater than the input layer

03:04.770 --> 03:10.320
it just basically is able to cheat and just say all right this note is always going to be equal to this

03:10.320 --> 03:10.870
node.

03:11.040 --> 03:15.090
And then this node is equal to this not so information is going to fly through like that and then you'll

03:15.090 --> 03:17.480
even have some extra nodes that are not being used.

03:17.760 --> 03:25.890
And that could be the end process and state that this encoder would end up in after the training and

03:25.920 --> 03:30.720
it's going to be just useless is not going to extract any valuable information any valuable features

03:30.720 --> 03:31.970
for us through that process.

03:31.970 --> 03:34.190
So this is definitely a problem.

03:34.410 --> 03:36.820
And how are we going to solve that problem.

03:36.930 --> 03:41.580
Oh that's what we're going to look at the next couple of tutorials we're going to look at three different

03:41.580 --> 03:43.670
approaches that are used to solve that problem.

03:43.890 --> 03:46.970
And therefore three different variations of iron cutters.

03:47.070 --> 03:50.050
So let's proceed with the next tutorial and I will see you there.

03:50.100 --> 03:51.170
Until next time.

03:51.220 --> 03:52.030
Enjoyed learning.
