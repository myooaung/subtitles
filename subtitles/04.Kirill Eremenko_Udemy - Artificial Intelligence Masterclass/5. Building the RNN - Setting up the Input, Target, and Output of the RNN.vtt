WEBVTT

00:00.300 --> 00:02.420
Hello and welcome to this new tutorial.

00:02.430 --> 00:07.540
So now that we've tackled the creation of the Elysium cell and drop out.

00:07.590 --> 00:13.350
Time to wrap the whole thing connecting all this to the input outputs and the targets into the whole

00:13.530 --> 00:18.560
record a neural network model and the way we're going to do this is we're going to create some tensor

00:18.570 --> 00:25.470
flow placeholders for the input the target and the output meaning which will be returned by the island.

00:25.560 --> 00:30.900
And that's why I've prepared for you a new resource which will be useful for this tutorial which is

00:31.020 --> 00:35.160
the tensor flow place all the function from the tensor flow documentation.

00:35.160 --> 00:36.240
So let's have a look at this function.

00:36.240 --> 00:42.210
Let's have a look at what it does first it insert a placeholder for a tensor that will always be fit

00:42.580 --> 00:49.880
and that means that the record new will network that we will build expects as soon format of its input.

00:50.190 --> 00:53.490
And it will return the output in this same format as well.

00:53.550 --> 00:58.770
And therefore the target will need to have the same format when we compute the less error between the

00:58.830 --> 01:00.300
output and the target.

01:00.300 --> 01:06.510
So it's super important that you notice the transfer place holder because it is the compulsory format

01:06.750 --> 01:10.330
to have when you are working with neural networks in general.

01:10.410 --> 01:16.490
So let's do this let's create the tens of loop placeholder for our inputs our targets and our outputs

01:16.770 --> 01:19.370
and we're going to start with these inputs.

01:19.410 --> 01:21.850
But first we're going to do something very quick.

01:21.930 --> 01:26.860
We're going to create an object void for that maximum sequence length.

01:26.970 --> 01:29.730
And that's because we're going to need it to create a placeholder.

01:29.760 --> 01:31.330
So let's do that quickly first.

01:31.410 --> 01:33.780
We're gonna create a new object variable self.

01:33.840 --> 01:40.520
That sequence length which will be equal exactly to that variable.

01:40.590 --> 01:44.540
That is a simple variable and not an object variable.

01:44.610 --> 01:50.610
So we will base that here and then now is the time to create the place holders and we're going to start

01:50.850 --> 01:58.080
with the input placeholder to create a tensor of input in the right format for the record a new network.

01:58.350 --> 02:04.100
And we're gonna call that input placeholder self that input x.

02:04.140 --> 02:10.170
And of course solve that because it needs to be an object variable then that's where we can call tend

02:10.170 --> 02:16.350
to flow and from tensor flow we're going to call the place holder function here it is to which we're

02:16.350 --> 02:18.550
going to input these three arguments.

02:18.550 --> 02:25.260
So first the deed type is simply the type of the values that will mean the cells of the sensors.

02:25.260 --> 02:31.110
And since these values are just the result of all the computations with all the normalization and everything

02:31.350 --> 02:38.850
of course the Tea type is a tensor flow float and we can take the thirty two one to have enough fingers

02:39.180 --> 02:45.230
so float 32 and then the next argument well we cannot see it anymore but we can see it here.

02:45.300 --> 02:51.000
The next argument is the shape and this will be exactly the shape of the tensor is for the inputs the

02:51.000 --> 02:56.460
targets and outputs and these sensors are all going to have the same dimensions.

02:56.460 --> 03:00.030
The first one will correspond to the batch so it will be the batch size.

03:00.090 --> 03:05.820
The second one will correspond to well exactly what we've just said as an object viable the sequence

03:05.820 --> 03:06.540
length.

03:06.540 --> 03:10.110
And the third one will be the number of channels in the tensor.

03:10.110 --> 03:13.400
That is its dimension and that's what we gathered here.

03:13.400 --> 03:19.620
First remember when gathering the parameters that's the dimension of the input in width which remember

03:19.620 --> 03:25.540
corresponded to 35 channels 32 for this first input here.

03:25.620 --> 03:30.120
The latent vector z and 3 for the second input the action.

03:30.120 --> 03:31.980
So thirty five channels in total.

03:32.070 --> 03:38.730
And then the out with the dimension of the output will be 32 because this time it corresponds to the

03:38.910 --> 03:42.600
output of the Indian Ocean and that has 32 channels.

03:42.600 --> 03:48.720
So that's going to be the third element of this shape and therefore that's exactly what we're going

03:48.720 --> 03:50.080
to input now.

03:50.130 --> 03:56.310
And so I'm adding the shape equals to these three dimensions that will enter in square brackets the

03:56.310 --> 04:03.780
first one as we said corresponding to the batch size in which we're gonna get from r hp s object variable

04:03.840 --> 04:09.000
containing all the hyper parameters from which we're going to get the batch size.

04:09.150 --> 04:10.480
Here we go then.

04:10.560 --> 04:17.280
Second Dimension This is again we can take it from our HP as parameter object from which we're gonna

04:17.310 --> 04:20.630
get the max sec Len.

04:20.790 --> 04:26.820
That is exactly this 1 HP as Mac signaling or length or even self that sequence things that we've just

04:26.820 --> 04:27.630
created.

04:27.780 --> 04:35.850
And the third one as we said the dimension of the input which is in width corresponding to 35 channels

04:36.150 --> 04:40.970
or of course we could have put self that HP s that input SEC with.

04:40.980 --> 04:41.380
All right.

04:41.640 --> 04:43.220
So good thing done.

04:43.230 --> 04:49.530
We created the tens of took place order for our input meaning the right format of our input to be fed

04:49.530 --> 04:56.010
into the neural network and now we're gonna do the same for this time the target which we will call

04:56.010 --> 04:58.640
this time output x.

04:58.770 --> 05:01.550
So I'm going to place that in here by out.

05:01.690 --> 05:07.930
And then same it's gonna be the sense of a place holder with a float 32 type for itself.

05:08.140 --> 05:12.910
Then almost the same shape we're gonna have the same batch size of course the same maximum sequence

05:12.910 --> 05:18.310
length of course but the right dimension of the target which is the same dimension as the output that

05:18.310 --> 05:23.300
is out width corresponding to 32 channels.

05:23.440 --> 05:30.310
So good we have created or two tenths of loop placeholders for the input and the target we will do the

05:30.310 --> 05:36.760
same later on for the output meaning what will be returned by the Arnon but first we will take care

05:36.760 --> 05:43.480
now of gathering all the inputs that will need to create our final Ireland in which we will do with

05:43.780 --> 05:46.420
this dynamic ion and function.

05:46.420 --> 05:47.270
So let's have a look.

05:47.350 --> 05:50.580
What are the arguments of this dynamic on function.

05:50.590 --> 05:54.370
Well the first one is the cell and that we've already created.

05:54.370 --> 06:00.490
That was the LSC himself which we've created in the previous Statoil then these are going to be the

06:00.690 --> 06:04.640
input and which again we have already created just now.

06:04.660 --> 06:11.830
But we're not going to enter this cell that input x because we will create a separate viable it is safer

06:12.100 --> 06:18.220
when we do some further modifications of the input and therefore we're just going to create a new variable

06:18.220 --> 06:26.980
here that will call actual input x and which will be of course equal to the self that input x object

06:26.980 --> 06:33.490
variable and that's just to create separately variable which will be exactly the input of this dynamic

06:33.530 --> 06:36.490
ion function that is this input argument here.

06:36.490 --> 06:36.810
All right.

06:36.820 --> 06:40.090
So done for this one then sequence length.

06:40.090 --> 06:45.460
Well since we've specified a sequence length in the inputs then we don't have to specify here so we

06:45.460 --> 06:51.040
will just keep the non default value but then for the initial state that's the next step here that we

06:51.040 --> 06:57.290
have to do we have to initialize the state was all zeros as opposed to just inputting and none.

06:57.310 --> 07:03.010
So that's exactly what we're going to do here we are going to create the initial state initialized with

07:03.220 --> 07:08.950
only zeros but of course with the same batch size because again we're not going to get a single initial

07:08.950 --> 07:12.580
state but we're going to create a batch of initial states.

07:12.580 --> 07:19.470
So let us create this new required argument initial state for our dynamic or an end function which we

07:19.470 --> 07:22.840
will call self that initial state.

07:22.840 --> 07:28.740
And so that's gonna be the initial state inside the A-list himself so we can just take our cell here.

07:28.750 --> 07:35.230
So copying this based on that here and from which we are gonna get one of the attributes of the layer

07:35.240 --> 07:41.950
Norm basic Elysium cell because I remind that cell is nothing else and cell even here and cell F in

07:41.950 --> 07:48.460
here is nothing else than an object of this layer Norm basic Elysium cell class and therefore from cell

07:48.460 --> 07:52.770
here we can get all the attributes of the lesser known basic asked himself.

07:53.120 --> 07:59.950
And right now the one we're interested in is the zero state function attribute which will allow us to

08:00.070 --> 08:06.460
initialize the state with only zeros and that's why as I've just told you we have to specify the batch

08:06.460 --> 08:13.390
size here because we're gathering a batch of initial states to be 100 percent aligned with our other

08:13.390 --> 08:16.450
formats of the input the targets and the outputs.

08:16.450 --> 08:24.790
All right so here we're going to enter a batch size argument as HP as that batch size.

08:24.790 --> 08:25.450
Here we go.

08:25.600 --> 08:31.450
So HP is or so that HP is that's the same because HP has no argument that the init method.

08:31.450 --> 08:38.200
And then what is the other argument we can no longer see it but it was D type again and which of course

08:38.200 --> 08:44.590
will be again tensor float float 32 the float 32 type by tensor float.

08:44.590 --> 08:45.490
All right perfect.

08:45.490 --> 08:52.480
We are getting very close to the final call of the dynamic in function but first we have to take care

08:52.480 --> 08:57.160
of the output and we actually have a lot of work to do on that because you know remember that we have

08:57.160 --> 09:03.940
to distinguish two outputs to deterministic output returned by the iron and only and then the stochastic

09:03.970 --> 09:11.050
output returned by the MGM and in order to create that that stochastic output well we have to prepare

09:11.050 --> 09:17.680
now the tenses of weights and biases which will be required for the MGM because indeed the next step

09:17.950 --> 09:21.220
after getting that deterministic output of the iron in.

09:21.400 --> 09:28.210
Well what we'll do is do a matrix multiplication and that's why I've prepared for you a function that

09:28.210 --> 09:36.520
we'll use soon which is this matrix multiplication function x w plus B which will multiply deterministic

09:36.580 --> 09:43.570
output at the iron in by this tensor of whites that we're about to create plus this tensor of biases

09:43.570 --> 09:48.880
that we're about to create as well in order to get prepared for the stochastic operations by the MGM

09:49.300 --> 09:54.580
and therefore that's the important part of the work we have to do for the output right now we will create

09:54.580 --> 10:00.450
these two tenses of weights and biases and now it's important to understand that the sense of it will

10:00.450 --> 10:08.100
be a matrix with ion in size number of rows and iron and size remember is the number of units we input

10:08.100 --> 10:13.910
here for the Elysium cell which we've created with the low known basic Elysium cell class.

10:13.950 --> 10:18.620
So that is exactly going to be the number of rows in this matrix of weight.

10:18.660 --> 10:25.500
And then for the number of columns in this matrix of weight we'll have 32 times five times three columns.

10:25.500 --> 10:30.520
And the reason for that is that the dimension of the weights that we'll need for each of the Elysium

10:30.520 --> 10:38.220
gym unit is the multiplication of the dimensions of the two inputs that we have here which are the action

10:38.400 --> 10:39.820
and plates in fact easy.

10:39.990 --> 10:44.600
And this remember has dimension of Z size equals by default 32.

10:44.640 --> 10:50.120
This the action has a dimension of three because at each time we can play three possible actions and

10:50.130 --> 10:55.830
therefore a vector of three elements but then let's not forget that we'll use five Gaussian mixtures

10:56.160 --> 11:02.070
and therefore to get the total number of dimensions of the weights and therefore also the basis we will

11:02.070 --> 11:08.820
need to multiply 32 here by three and by also five the total number of Gaussian mixtures and therefore

11:08.970 --> 11:14.550
let's get that second dimension of the tensor of weight in a separate variable here which we will call

11:14.780 --> 11:21.930
an out and in which as we said is equal to the dimension of the output out which which I'm Kobe year

11:22.950 --> 11:32.630
out with multiplied by the number of Gaussian mixtures K mix and multiplied by three.

11:32.780 --> 11:34.360
The dimension of the actions.

11:34.360 --> 11:40.990
And that is the second dimension will have an arc tensor of weights meaning the number of columns we'll

11:40.990 --> 11:42.450
have in our tensor of weight.

11:42.460 --> 11:48.970
And also it will also be the total number of elements in the one dimensional tensor biases because indeed

11:49.300 --> 11:56.500
then what we'll do is once we multiply the deterministic output of the iron and by the tenths of weight

11:56.620 --> 12:02.110
well we will add the biases and therefore there will be the same and outnumber of biases.

12:02.110 --> 12:02.460
All right.

12:02.470 --> 12:07.450
So now that we've computed this number well let's create these two sensors and we're going to create

12:07.450 --> 12:12.850
them inside a tensor flow variable scope which will be exactly the variable scope used to build the

12:12.850 --> 12:19.560
Iron in and therefore I'm introducing it now with the sense of flow variable.

12:19.590 --> 12:25.170
Here we go scope inside which we just have to input the name of the scope in quotes which we'll call

12:25.230 --> 12:32.640
the Orient and variable scope and then inside we're first going to define the 2D tensor of weights and

12:32.850 --> 12:35.050
the one dimensional tensor basis.

12:35.130 --> 12:36.670
So let's start with intensive weights.

12:36.690 --> 12:43.770
We're gonna call that output W and we will create it thanks to its sense of flow function which will

12:43.770 --> 12:51.420
be the get variable function inside which we will input the name of the tensor which will be well we

12:51.420 --> 12:58.920
can just call it again output W and then of course here comes the most important argument the shape

12:59.280 --> 13:04.950
which is the shape we've just mentioned that is with the total number of Elysium units are in in size

13:05.220 --> 13:07.830
and an out number of columns.

13:07.830 --> 13:10.070
So here we go let's input the shape here.

13:10.140 --> 13:14.060
We are going to input then square brackets with the two dimensions.

13:14.160 --> 13:21.540
The first one is as we said self that HP s or hyper parameter container from which we're gonna get the

13:21.590 --> 13:27.720
iron in size that's for the first dimension of the matrix of weights the number of rows and then for

13:27.720 --> 13:31.890
the second dimension Well that's what we've just computed here and out.

13:31.950 --> 13:33.000
And so here we go.

13:33.000 --> 13:39.000
We're gonna have an out number of columns and that's the dimensions of the matrix of weight which we

13:39.000 --> 13:46.200
will multiply to the deterministic output of the ion so that we can get later on the stochastic Indian

13:46.350 --> 13:47.590
output layer.

13:47.610 --> 13:48.040
All right.

13:48.150 --> 13:55.500
So now we're gonna take care of the other tensor for the biases which will correspond to biases here.

13:55.530 --> 14:01.530
We just initialize that and now we're about to initialize this biases and therefore that's why I've

14:01.530 --> 14:09.610
copy this I'm going to paste that just below we're gonna replace output w by output B for the basis

14:09.810 --> 14:10.610
then same here.

14:10.680 --> 14:12.660
Output w by output B.

14:12.840 --> 14:19.320
And now however we're not gonna have a 2D tensor but a one day tensor which will of course have an out

14:19.560 --> 14:20.010
element.

14:20.040 --> 14:25.670
So I'm just removing the first dimension which was only for the weights so that we only keep an eye

14:25.680 --> 14:26.030
out.

14:26.040 --> 14:27.030
And here we go.

14:27.030 --> 14:33.750
Now we have are two tenths of weights and bases and so we're ready to get the deterministic output of

14:33.750 --> 14:41.640
the ironing in and then we'll be ready to compute the matrix multiplication of weights plus basis after

14:41.640 --> 14:49.110
which the mixture density network will come into play to get that final stochastic Indian output layer.

14:49.530 --> 14:53.660
So we'll do all this in the next couple of tutorials and until then enjoy a.
