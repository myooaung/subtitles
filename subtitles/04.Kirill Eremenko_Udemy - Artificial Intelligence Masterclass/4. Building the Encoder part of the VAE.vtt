WEBVTT

00:00.180 --> 00:07.170
Hello and welcome to this new tutorial now we're going to make that method that creates the model architecture

00:07.170 --> 00:12.830
itself and also the method that will integrate the training operations.

00:13.020 --> 00:18.110
So let's do this you already know the name of that method because we already covered.

00:18.110 --> 00:26.280
Actually it's exactly this method so I'm going to copy this because in order to introduce a method here

00:26.550 --> 00:31.770
we need to start with death to define the method and then just give the name of the method which will

00:31.770 --> 00:33.910
be build graph.

00:33.960 --> 00:39.900
However inside we have to specify the argument self because we're going to use Of course the variables

00:39.900 --> 00:43.940
of our object and no other argument is needed here.

00:44.160 --> 00:50.730
And therefore we are already ready to start implementing the bill graft method and the first thing we're

00:50.730 --> 00:56.970
going to do before you know making the whole architecture of the convolutional variational our encoder

00:57.150 --> 01:04.890
is to initiate the graph because the E will be constructed as a tensor flow graph and therefore now

01:04.890 --> 01:10.670
what we're about to do is create an object of the graph class by tenths of love.

01:10.860 --> 01:19.080
So that are the amoral gets done McGruff advanced structure which will be super useful when it is time

01:19.080 --> 01:20.640
to train the Wii.

01:20.670 --> 01:21.600
All right.

01:21.600 --> 01:26.160
Diamond graphs are a great innovation in the AI platforms.

01:26.160 --> 01:27.950
You also have them in by torch.

01:28.080 --> 01:33.510
And basically they allow faster computations of the Graylands and more specifically the gradients of

01:33.510 --> 01:37.290
composition functions in the training computation.

01:37.290 --> 01:44.490
So that's a must do we have to start here by creating this graph and therefore I'm introducing a new

01:44.760 --> 01:47.320
object variable which will be self-taught.

01:47.410 --> 01:54.120
G so g will be the graph itself and to create this graph as we've just said we're going to create it

01:54.210 --> 01:58.550
as an object of the graph class by sense of focus.

01:58.570 --> 02:04.130
I'm calling it sense of flow and Frampton's of flow and calling the graph class.

02:04.140 --> 02:05.170
Here it is.

02:05.280 --> 02:09.590
And we just need add some parentheses because this is indeed a class.

02:09.720 --> 02:12.360
All right and no argument is needed in the class.

02:12.390 --> 02:16.010
You have your graph ready it's basically initialized.

02:16.140 --> 02:21.460
And later on it will be used inside the tens of flow sessions that will run for either the inference

02:21.540 --> 02:22.770
or the training.

02:22.770 --> 02:23.130
All right.

02:23.130 --> 02:25.320
So here we go we have our graph.

02:25.320 --> 02:26.790
That was the first step.

02:26.790 --> 02:32.720
And now the second step is to specify that we want to have the whole architecture of the V.A. model

02:32.730 --> 02:38.910
that we're about to build inside the graph and therefore in order to specify that we're going to integrate

02:38.940 --> 02:45.690
indeed this architecture inside the graph we have to start with with here and set the graph which we

02:45.690 --> 02:50.080
called self g as default.

02:50.130 --> 02:50.560
Right.

02:50.580 --> 02:56.340
The first one here then let's not forget some parenthesis and then Gullane And then as you can see when

02:56.340 --> 03:00.910
you press enter We are inside this graph set as default.

03:00.930 --> 03:05.880
Right that's where we're going to build the whole architecture of the V8 emo.

03:05.940 --> 03:09.950
You know inside this graph and so here we go where you start.

03:10.320 --> 03:18.090
And so now what we're going to do is go back to the appendix section of the world knows article written

03:18.090 --> 03:19.390
by the original authors.

03:19.650 --> 03:26.850
Because indeed as we saw and bruised toils it contains the fool architecture the exact same one as the

03:26.850 --> 03:28.130
one we're about to build.

03:28.410 --> 03:33.790
And what we're going to do is simply follow this architecture from the top to the bottom because then

03:33.870 --> 03:39.750
we have to start here with the input images and we're simply going to follow the architecture with the

03:39.750 --> 03:46.890
same numbers here of importance or meaning future maps here and elements in the latent vector and then

03:46.980 --> 03:49.820
input sensors into the convolutions.

03:49.830 --> 03:52.440
So let's start with the very top.

03:52.500 --> 03:59.400
The first step here which is to get the input image with the following dimensions 64 by 64 by three

03:59.610 --> 04:05.720
and 64 by 64 are basically the height and the width of the images.

04:05.750 --> 04:09.240
There are going to be squares of 64 pixels.

04:09.300 --> 04:12.080
All right so that's the first thing we're going to do here.

04:12.120 --> 04:18.510
And the way we're going to do this is by first introducing a variable that we're going to call X and

04:18.540 --> 04:21.300
which of course will be a variable of the object.

04:21.570 --> 04:30.300
And this ex-boy will will be exactly the input images of 64 by 64 by three dimensions.

04:30.300 --> 04:36.840
And so we're going to create them inside a tensor placeholder that's usually the way we do things when

04:36.840 --> 04:37.940
dealing with images.

04:38.130 --> 04:43.260
Because this densify a placeholder will create the right format for these input images with the right

04:43.260 --> 04:44.130
dimensions.

04:44.390 --> 04:49.860
And the way we're going to do this is my first of course going tends to flow has a shot at T.F. then

04:49.860 --> 04:56.400
Frampton's of all we're going to go as we've just mentioned the place holder function inside which we

04:56.450 --> 05:03.540
are giving input its arguments and the first one is did type meaning the type of the elements that you

05:03.540 --> 05:09.470
all have in the cells of the arrays which are of course going to be the pixels of the images.

05:09.600 --> 05:17.850
And of course since we're going to normalize pixels which originally are between 0 and 255 but after

05:17.850 --> 05:20.410
normalization there will be basically float.

05:20.550 --> 05:24.250
So we have to specify the type of the cells as floats.

05:24.420 --> 05:30.540
And the way we do that with sense of flow is by common sense of flow and from tense flow we get the

05:30.540 --> 05:33.380
float 32 type.

05:33.450 --> 05:34.160
All right.

05:34.180 --> 05:40.200
So that was for the first argument and then I don't know if you saw but the second argument is actually

05:40.200 --> 05:42.390
the shape of the images.

05:42.540 --> 05:50.250
And as we saw well we can see that again the shape of the images are going to be exactly 64 by 64 by

05:50.250 --> 05:51.110
three.

05:51.110 --> 05:51.600
All right.

05:51.600 --> 05:59.310
And that's exactly what we have to specify in the second argument here in brackets inside which we have

05:59.310 --> 06:00.930
to input four elements.

06:00.930 --> 06:07.320
The first one is the diamond ssion of the batch in case we want to specify that we want our placeholder

06:07.320 --> 06:10.810
holding the input images inside batches already.

06:10.860 --> 06:13.710
But no we won't get them into some batches yet.

06:13.830 --> 06:16.370
That's not right now that we do this we we'll do that later.

06:16.500 --> 06:20.530
And therefore we will specify none as first element.

06:20.670 --> 06:25.380
And then of course the three other elements are the diamond sions of the images.

06:25.530 --> 06:33.210
And therefore as we've just seen they are 64 for the first one sixty fourth for the second one and three

06:33.450 --> 06:40.230
because we are working with RGV colored images as opposed to black and white images in which case the

06:40.240 --> 06:42.620
diamonds we have been here one.

06:42.810 --> 06:44.390
All right here we go.

06:44.400 --> 06:51.390
We took care of the input images meaning we took care of the first step here in input image sixty four

06:51.390 --> 06:52.760
by sixty four by three.

06:53.070 --> 07:00.630
And now we're going to tackle the first part of the V8 the meaning the point where we build the encoder

07:01.020 --> 07:01.400
right.

07:01.410 --> 07:07.170
You have to understand that we're going to build this convolutional the IMO in three main parts.

07:07.290 --> 07:14.190
The first part is going to be the encoder that's the encoder you know encoding the input images into

07:14.340 --> 07:15.540
the latent vector.

07:15.600 --> 07:21.690
And speaking of the late invective that's actually the second part of the implementation of the V and

07:21.690 --> 07:25.480
then the third part which is of course the decoder.

07:25.490 --> 07:33.540
Right here we are decoding the latent vector Z to reconstruct the final output that is the final output

07:33.600 --> 07:38.630
images which are going to be the representation of the environment inside the dream.

07:38.910 --> 07:46.620
So of course I'm going to structure our code now in these three parts starting with the first one which

07:46.620 --> 07:55.240
will be building the encoded part of the the variational are encoded.

07:55.530 --> 08:02.640
And so now here we go let's build exactly the same architecture as we see here starting with the first

08:02.640 --> 08:03.620
convolution.

08:03.620 --> 08:04.410
I really do.

08:04.500 --> 08:11.340
Convolution meaning it's going to be a convolutional layer of 32 times for feature maps with a realm

08:11.460 --> 08:14.260
meaning rectifier activation function.

08:14.460 --> 08:18.370
And this is exactly what we're about Intiman right now.

08:18.480 --> 08:24.610
You'll see that we will only add one additional info that is not here which will be destroyed but this

08:24.630 --> 08:27.250
trial you will see is most of the time too.

08:27.360 --> 08:29.570
That's why they didn't specify it here.

08:29.580 --> 08:31.170
All right let's do this.

08:31.170 --> 08:33.780
It's actually very easy with tons of flow.

08:33.820 --> 08:40.110
We're going to call our successive layers of the encoder and also the decoder by the way H because there

08:40.120 --> 08:41.740
are like hidden layers.

08:42.000 --> 08:43.380
Here we go H.

08:43.440 --> 08:48.930
And so now to create these convolutional layers Well you might guess that the first natural step is

08:48.930 --> 08:54.540
to call tens of flow and then from Frampton's of flow we're going to get a first module which is very

08:54.540 --> 09:01.410
easy to remember because indeed right now we are creating some convolutional layers and therefore layers.

09:01.470 --> 09:03.270
That's the model that tends to flow.

09:03.450 --> 09:10.860
And inside these layers Mudgal we're going to get the come to the function.

09:10.860 --> 09:17.400
And so here we go we can see very well the arguments of the come to the function but I've prepared in

09:17.590 --> 09:18.630
the resources.

09:18.660 --> 09:24.600
Another one which is not as important as the ones I've given you but pretty useful of what we're about

09:24.600 --> 09:25.600
to do right now.

09:25.680 --> 09:31.230
It's the T.F. players come from that sense of flow documentation.

09:31.290 --> 09:32.560
So how did I get that.

09:32.580 --> 09:39.000
I basically just typed layers to the sense sensor on Google and I directed got it right.

09:39.000 --> 09:43.070
That's how you can get easily and fast the information.

09:43.110 --> 09:50.280
And so this is useful because indeed you get all the detailed informations about the arguments of this

09:50.540 --> 09:55.580
comes to the function we are about to use to build our first convolutional layer.

09:55.800 --> 09:59.070
And so let's input these arguments one by one.

09:59.130 --> 10:06.890
The first one is inputs and that of course is going to be our input images which we created here so

10:06.900 --> 10:14.270
we can just copy this and enter this as the first input to come to the function.

10:14.460 --> 10:20.700
Then let's see the next argument is going to be the number of filters or the number of feature maps.

10:20.700 --> 10:26.670
That's the same thing right as you can see them or filters in the convolution as you saw in the intuition

10:26.670 --> 10:28.050
Nectars as now.

10:28.050 --> 10:31.300
Question is how many fields is that we want right now.

10:31.470 --> 10:41.860
Well the answer is in the architecture provided in Oracle and that is 32 32 feature maps or filters.

10:41.880 --> 10:45.740
All right let's do this let's implement that 32 filters.

10:45.800 --> 10:49.800
And now next argument you can see how all this is pretty easy.

10:49.950 --> 10:53.210
Let's see what the next argument is.

10:53.250 --> 10:58.700
That's the kernel size no the size of the kernel which you saw in the integration lectures.

10:58.860 --> 11:07.020
And again the answer of what kernel size we want is here right here it is for right 32 feature maps

11:07.050 --> 11:11.010
or 32 filters of size 4 by 4.

11:11.130 --> 11:12.280
That's what it means here.

11:12.510 --> 11:19.050
So let's do this let's go back to Python and let's input for here and then what is going to be the next

11:19.050 --> 11:19.750
argument.

11:19.860 --> 11:25.460
Well let's see here all the answers are here on the several resources we have.

11:25.470 --> 11:32.190
The next argument is going to be destroyed which again you saw is by how many cells are shifted.

11:32.190 --> 11:37.120
The filters are 32 filters of size four by four.

11:37.140 --> 11:40.250
And so as we've seen this trait is not given here.

11:40.470 --> 11:46.730
But as I've told you it's usually strides equal to then next argument.

11:46.890 --> 11:47.440
Let's have a look.

11:47.450 --> 11:50.600
Again a sense of loaded sense of documentation.

11:50.700 --> 11:57.660
The next argument is well you have many of them you have the padding the data format the deletion rate

11:57.950 --> 11:59.550
the activation and more of them.

11:59.640 --> 12:05.120
So no worries we don't have to answer all of them we are just going to keep their default values.

12:05.280 --> 12:10.840
However we have to specify the activation function because we want to use the rectifier activation function.

12:10.920 --> 12:13.110
And as you can see this is not the default value here.

12:13.110 --> 12:14.630
The default value is none.

12:14.670 --> 12:20.460
Therefore we're going to specify this in the arguments and remember in the world no rules it specifies

12:20.520 --> 12:25.680
which activation function it uses for this first convolutional layer.

12:25.800 --> 12:31.760
And that is a real activation function or in other words a rectifier activation function.

12:31.770 --> 12:32.570
So here we go.

12:32.610 --> 12:38.130
Let's implemented as for strides we have to specify the name of the argument because this is not the

12:38.130 --> 12:40.370
next one in the list of argument.

12:40.480 --> 12:42.900
So activation equals.

12:43.100 --> 12:49.800
And to get the rectify activation function we need to get it from sense of flow then from the and then

12:49.980 --> 12:53.350
Mudgal of tens of flow and from an image will tend to flow.

12:53.400 --> 12:59.090
Well here we go we get the real function activation function.

12:59.130 --> 12:59.490
All right.

12:59.490 --> 13:00.150
Perfect.

13:00.150 --> 13:06.330
So now we have everything we need but we're just going to add one final argument just to give a name

13:06.330 --> 13:12.210
to that specific convolutional layer which is the first one with the last argument that we can use here

13:12.450 --> 13:14.740
which is simply the name right.

13:14.790 --> 13:21.930
And so the name for this first convolution layer Well we can call it and encoder with Anke convolution

13:22.200 --> 13:29.280
can one the first convolutional layer of the encoder because then we'll get Also some convolutions or

13:29.490 --> 13:30.890
inverted convolutions.

13:31.050 --> 13:37.790
But for the decoder then no worries about the warning is only because we haven't used it yet but we're

13:37.800 --> 13:43.320
about to use it right now because age is first convolutional layer.

13:43.440 --> 13:47.910
It's going to become the input of the second convolutional there.

13:48.090 --> 13:55.230
So actually what we're going to do to be efficient in order to create that second convolutional layer

13:55.500 --> 14:01.950
is the first one and then the first very important thing is not to forget to place some of that X which

14:01.950 --> 14:08.220
was of course the input of the first convolutional layer but the first convolutional layer only because

14:08.220 --> 14:16.410
now the input sensor of the second convolutional layer is going to be the first one edge and we can

14:16.470 --> 14:21.400
override h here because what only matters to us is the final outcome.

14:21.480 --> 14:27.080
After the convolutions of Basically year we're just applying the convolutions one after the other by

14:27.090 --> 14:29.220
putting on the new one the previous one.

14:29.370 --> 14:33.500
And so I'm replacing the self-doubt X here by H.

14:33.720 --> 14:39.310
And now the second thing that we need to do is check the architecture here.

14:39.330 --> 14:46.020
What we want for the number of future maps or folders and same do we want the same size the same stride

14:46.170 --> 14:46.920
extra.

14:47.070 --> 14:53.700
So the answer to that question is again right here and the architecture of the original variational

14:53.700 --> 14:55.470
are in code are made by the authors.

14:55.620 --> 15:03.530
And as we can see the second convolutional layer has 6 for filters of size four by four.

15:03.670 --> 15:04.700
So here we go.

15:04.720 --> 15:12.790
We're going to implement that right away by simply replacing the 32 here by 64 and we already have the

15:12.790 --> 15:14.640
right size of the filters.

15:14.830 --> 15:17.910
And then let's check if we have the same activation function.

15:17.980 --> 15:22.300
And yes it is the same it is the rectify activation function once again.

15:22.450 --> 15:26.900
So nothing to change here and here of course we're going to replace UNC.

15:26.950 --> 15:32.710
Can one buy on come too because this is the second convolutional there.

15:32.710 --> 15:38.650
And here we go and there you're going to see how we're going to be so efficient at finishing the creation

15:38.740 --> 15:45.610
of this encoder because for the next one well this time we don't even have to replace a chip because

15:45.970 --> 15:52.690
now this new convolutional layer is going to take as input to previous convolutional layers of age here

15:52.720 --> 15:55.020
corresponds to that age here and now.

15:55.060 --> 15:59.920
Let's have a look at the architecture to see what number of feature maps or filters we want.

15:59.980 --> 16:09.730
For this third convolutional layer and we want 128 filters of size four by four and we also want a rectifier

16:09.790 --> 16:11.160
activation function.

16:11.260 --> 16:12.800
So it's going to be very quick now.

16:12.820 --> 16:22.650
Basically what we want to do is to replace 64 here by 128 and can to by on count three.

16:22.680 --> 16:24.910
The third convolutional layer.

16:25.120 --> 16:26.520
And then here we go again.

16:26.530 --> 16:34.510
I'm going to copy this new convolutional layer and paste it here and now the fourth convolutional layer

16:34.510 --> 16:39.850
is going to get as a tensor and puts the previous convolutional there the third one.

16:39.850 --> 16:43.350
But this time this new convolutional layer is going to get.

16:43.510 --> 16:47.300
Well you might guess that it's going to get the double of 128.

16:47.500 --> 16:53.650
Indeed reglue Gamp 256 filter's size four by four.

16:53.650 --> 16:54.590
So here we go.

16:54.670 --> 17:00.340
Here we only have to replace 128 by 256.

17:00.460 --> 17:01.910
Then size four by four.

17:01.930 --> 17:08.740
And again we have a rectifier activation function to let the signal pass inside the convolutional neural

17:08.740 --> 17:09.850
network.

17:09.850 --> 17:10.240
All right.

17:10.240 --> 17:14.860
And let's not forget to replace UNC comp 3 by on com 4.

17:15.010 --> 17:15.730
And here we go.

17:15.760 --> 17:18.960
We have our fourth convolutional layer.

17:19.060 --> 17:20.900
And does that mean that we have everything.

17:21.070 --> 17:23.530
Yes we do have everything.

17:23.590 --> 17:31.060
We are ready for the next step of the creation of this variational our encoder which is to flatten the

17:31.060 --> 17:38.620
result of all these conclusions here that happens into a one dimensional vector of two times two times

17:38.640 --> 17:46.240
two hundred and fifty six elements and so the way we're going to do this is actually very simple with

17:46.240 --> 17:52.150
sense of flow because you know I already did that with some other AI platforms and I actually had to

17:52.150 --> 18:00.130
make a function here the way we can do this very easily as tensor flow allows us to do because to flatten

18:00.370 --> 18:06.130
the result of these four convolutions Well the only thing that we have to do is get of course first

18:06.310 --> 18:11.690
sense of flow and then from tenso flow we're going to get the reshape function.

18:11.800 --> 18:19.420
And the trick to reshape the result of convolutions inside a one dimensional vector is simply to put

18:19.420 --> 18:26.050
first as you can see the final answer that you got as a result of the four convolutions here which is

18:26.050 --> 18:27.300
of course age.

18:27.310 --> 18:28.660
So here we go H.

18:28.900 --> 18:35.290
And then as you can see the next argument is the shape which you have to enter in square brackets inside

18:35.290 --> 18:37.740
which you're going to input two elements.

18:37.810 --> 18:44.890
The first one is minus one to say that you want a one dimensional vertical vector and then of course

18:44.890 --> 18:52.720
the second argument is going to be this exact same number here two times two times.

18:52.720 --> 18:54.780
Two hundred and fifty six.

18:54.970 --> 18:59.740
And that's of course the number of elements you want to have in this flattened vector.

18:59.740 --> 19:00.670
So here we go.

19:00.700 --> 19:02.320
That's the second element here.

19:02.350 --> 19:05.110
Two times two times 256.

19:05.110 --> 19:12.070
And congratulations you are done with the first part of the creation of this V immortal the encoded

19:12.070 --> 19:13.270
part is done.

19:13.360 --> 19:20.930
So now we're going to move on to the second part which is going to be the VI Part of the V model to

19:20.950 --> 19:29.530
simple stochastic the late and vector Z from a Gaussian distribution of mean and variance Sigma.

19:29.530 --> 19:31.830
So let's do that in the next tutorial.

19:31.870 --> 19:33.730
And until then enjoy AI.
