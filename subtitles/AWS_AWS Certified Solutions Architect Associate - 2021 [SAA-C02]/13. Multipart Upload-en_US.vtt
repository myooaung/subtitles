WEBVTT
1
00:00:02.030 --> 00:00:07.970
In this lab we're going to cover multipart upload so when you have larger objects that you want to upload

2
00:00:07.970 --> 00:00:17.060
into AWS you'll find that you need to break the files into pieces and upload them in parallel.

3
00:00:17.060 --> 00:00:19.300
And there's a couple of ways that this can be done.

4
00:00:19.370 --> 00:00:25.510
You can split the file yourself into much multiple pieces or you can allow AWS to do it.

5
00:00:25.510 --> 00:00:33.950
If you use the CLI multipart upload is typically a use case for when your uploads are or your fault

6
00:00:33.950 --> 00:00:41.780
your individual objects are 100 megabytes in size or more and you can upload objects as large as five

7
00:00:41.780 --> 00:00:49.430
gigabytes which is free now the benefits of doing it using multipart upload are that it improves throughput

8
00:00:49.430 --> 00:00:54.260
because you're uploading different parts of the file in parallel.

9
00:00:54.500 --> 00:00:58.470
You can also recover better from any network issues that might take place.

10
00:00:58.640 --> 00:01:02.390
So it will minimize the impacts of restarting a failed upload.

11
00:01:02.390 --> 00:01:08.120
If you have a network error you can also pause and reduce resume the uploads.

12
00:01:08.240 --> 00:01:14.990
So that means that you can do it over time choose the best times to perform your uploads and pause and

13
00:01:14.990 --> 00:01:19.710
then started again when you need to stop it for some reason.

14
00:01:19.770 --> 00:01:23.790
You can also start and upload before you know the final object size

15
00:01:26.470 --> 00:01:31.600
which means you can actually be uploading an object as you are creating it.

16
00:01:31.610 --> 00:01:38.390
There are several SDKs that you can work with to programmatically implement multipart upload and those

17
00:01:38.390 --> 00:01:43.160
include Java .NET, PHP, Ruby and REST.

18
00:01:43.160 --> 00:01:49.190
Now this is a good article you can have a look through and find quite a lot of information about what

19
00:01:49.190 --> 00:01:56.240
multipart upload is and then what we're actually going to do in this exercise is use the CLI.

20
00:01:56.720 --> 00:02:01.670
So it's easy to perform a multipart upload of a file.

21
00:02:01.670 --> 00:02:09.570
Now if you look for this article it tells you how to do it and there's quite a bit of information here.

22
00:02:09.590 --> 00:02:15.740
Quite a few different command lines and the key thing I want you to take from this is that AWS is telling

23
00:02:15.740 --> 00:02:24.970
you that you shouldn't use the S3 API unless the S3 command doesn't support the upload requirements.

24
00:02:25.250 --> 00:02:33.320
So that's the AWS S3 API which is more low level than the AWS S3 commands and therefore AWS

25
00:02:33.320 --> 00:02:39.230
would recommend using AWS S3 wherever possible so you can have a look through this and it gives

26
00:02:39.230 --> 00:02:43.420
you quite a bit of detail on how to do it using even those command line options.

27
00:02:43.430 --> 00:02:51.210
So we're just gonna head over to a command line and we're just going to issue the AWS S3 ls command

28
00:02:52.840 --> 00:02:56.620
and we can see that we have a bucket or two buckets there.

29
00:02:56.710 --> 00:03:04.590
I want to have a look inside the DCT company bucket just to see what we have in there and so currently

30
00:03:04.590 --> 00:03:11.350
we just have the my document document and then we have a couple of folders as well.

31
00:03:11.700 --> 00:03:18.540
So I'm just going to see what I have here and I know I have a file a large file so this mp3 here.

32
00:03:19.260 --> 00:03:27.330
Let's just look at a bit more detail this mp3 for excuse me is about 165 MBs in size.

33
00:03:27.620 --> 00:03:35.150
So what I want to do is upload that file to AWS so I'm going to type the command aws s3 cp

34
00:03:35.290 --> 00:03:43.700
for copy and then I'm gonna put the file name in and then I'm going to let it know which

35
00:03:43.700 --> 00:03:45.110
bucket I want to put it into

36
00:03:50.760 --> 00:03:52.600
and we hit enter

37
00:03:52.720 --> 00:03:57.210
Now what you'll find is for a second it looks like it's pausing but what it will do is because it's

38
00:03:57.210 --> 00:04:03.960
a large file it's actually going to automatically break up this file into pieces and send them in parallel

39
00:04:04.470 --> 00:04:13.930
and in a second we'll see that popping up on the screen.

40
00:04:13.990 --> 00:04:20.170
So now we can see that there are 20 parts that the file is being broken into and it's going to upload

41
00:04:20.170 --> 00:04:21.160
those in parallel

42
00:04:29.290 --> 00:04:33.000
so that took a couple of minutes and the file was uploaded.

43
00:04:33.070 --> 00:04:40.060
Another way that you would be able to if you want to play around with using the S3 API command and uploading

44
00:04:40.060 --> 00:04:48.310
pieces of your file on a Mac or Linux system you can use the split command so I can say split the 

45
00:04:49.000 --> 00:04:56.350
60 megabytes and then I can choose my file and then if I do an ls I need.

46
00:04:56.370 --> 00:05:03.180
So what I need to do is put in the file name again and then it will give me a different version of that

47
00:05:03.180 --> 00:05:09.900
file so then we have the files we have my large file and then my large folder and before a b and I see.

48
00:05:10.230 --> 00:05:15.240
So that's a way that you can split it up in this case just a few files if you had a really huge file

49
00:05:15.360 --> 00:05:22.330
gigabytes in size then you'd put it broken in some more pieces so that's multiplied upload.

50
00:05:22.330 --> 00:05:27.640
The main thing to remember for the exam is that it's recommended to use multipart upload when your files

51
00:05:27.640 --> 00:05:30.820
are 100 megabytes or more in size.

52
00:05:30.820 --> 00:05:36.940
Typically in a production scenario you would use one of the SDK and do it programmatically but as you

53
00:05:36.940 --> 00:05:40.600
can see even if you use the client will automatically break files up for you.

