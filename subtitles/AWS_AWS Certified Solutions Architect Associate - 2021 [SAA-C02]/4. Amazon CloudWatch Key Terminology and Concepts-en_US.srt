1
00:00:02,100 --> 00:00:06,740
Let's have a look now at some of the key terminology and concepts associated with Cloud Watch.

2
00:00:07,110 --> 00:00:12,840
So firstly, you have what's called a namespace and the namespace is a container for your cloud, which

3
00:00:12,840 --> 00:00:17,850
metrics and metrics and different namespace spaces are isolated from each other.

4
00:00:18,390 --> 00:00:22,740
So they cannot be mistakenly aggregated into the same set of statistics.

5
00:00:22,920 --> 00:00:27,580
So this is what namespace is looked like, and this is just a few of the name spaces that are available.

6
00:00:27,930 --> 00:00:34,980
So, for instance, the format is Awassa and then the name of the service like API, Gateway or Cloud

7
00:00:34,980 --> 00:00:36,890
HSM or Dinamo DB.

8
00:00:36,990 --> 00:00:38,670
We then have metrics themselves.

9
00:00:38,670 --> 00:00:41,670
So metrics are the fundamental concepts in cloud watch.

10
00:00:42,000 --> 00:00:46,770
They represent a time ordered set of data points that get published into cloud watch.

11
00:00:47,010 --> 00:00:53,520
So your services will send metrics to cloud watch and you can also send custom metrics to cloud, which

12
00:00:53,700 --> 00:00:55,500
will see how to do this a bit later on.

13
00:00:55,680 --> 00:00:58,020
And we'll actually do this in a hands on lab as well.

14
00:00:58,560 --> 00:01:04,320
Metrics exist within a region, so they're regional and they cannot be deleted, but they automatically

15
00:01:04,320 --> 00:01:06,420
expire after 15 months.

16
00:01:06,780 --> 00:01:12,420
Metrics are uniquely defined by name, a namespace and then zero or more dimensions.

17
00:01:12,870 --> 00:01:17,820
So you can see an example here of a few different AWASSA services sending in different metrics.

18
00:01:18,120 --> 00:01:25,200
So we got CPU utilization and discrete ops and then we got four EC2 and then we've got consumed rate

19
00:01:25,200 --> 00:01:26,670
capacity for Dynomite eBay.

20
00:01:26,670 --> 00:01:32,010
We might have bucket sized bytes for as free and read latency for ideas.

21
00:01:32,400 --> 00:01:34,170
Next up, we have dimensions.

22
00:01:34,350 --> 00:01:36,600
A dimension is a name value pair.

23
00:01:36,600 --> 00:01:41,760
That's part of the identity of a metric you can assign up to 10 to each metric.

24
00:01:42,090 --> 00:01:46,380
And they're basically categories for the characteristics of each metric.

25
00:01:46,680 --> 00:01:48,030
So let's look at that visually.

26
00:01:48,360 --> 00:01:51,780
So, for instance, here we have the two namespace.

27
00:01:52,200 --> 00:01:57,870
And then if you clicked on that AC2 namespace, you'd then see that you have a series of metrics and

28
00:01:57,870 --> 00:01:59,670
they're organized by dimensions.

29
00:01:59,970 --> 00:02:04,440
So you have by also scanning group as a dimension, per instance, metrics.

30
00:02:04,440 --> 00:02:05,430
That's a dimension.

31
00:02:05,610 --> 00:02:08,430
And within each of those, you then have a number of metrics.

32
00:02:09,030 --> 00:02:10,680
Next up, we have statistics.

33
00:02:10,890 --> 00:02:16,890
So statistics are metric data aggregations over a specific period of time and cloud, which provides

34
00:02:16,890 --> 00:02:23,400
statistics based on the metric data points provided by your custom data or provided by other services,

35
00:02:23,400 --> 00:02:23,710
the cloud.

36
00:02:24,510 --> 00:02:27,140
So let's just have a look at what the statistics might be.

37
00:02:27,450 --> 00:02:32,580
So, for example, you have minimum, which is the lowest value observed during the specified period,

38
00:02:32,790 --> 00:02:35,670
maximum, the highest value in a specified period.

39
00:02:35,880 --> 00:02:42,240
Then you've got the sum all values submitted, the average, which is the sum divided by the sample

40
00:02:42,240 --> 00:02:46,500
count and the sample count, is the number of data points used for the calculation.

41
00:02:47,410 --> 00:02:49,300
And then you have percentiles at the bottom there.

42
00:02:49,660 --> 00:02:54,940
Next up, we have alarms, so we've used alarms a couple of times in this course already and we'll use

43
00:02:54,940 --> 00:03:00,310
them again in this section and use an alarm to automatically initiate actions on your behalf.

44
00:03:00,520 --> 00:03:06,070
So it's configured to watch over a metric and look for a specific threshold and then it will trigger

45
00:03:06,070 --> 00:03:08,200
some kind of action based on that metric.

46
00:03:09,560 --> 00:03:15,710
And we did this earlier in the course where we had an auto scaling group configured with an alarm,

47
00:03:15,950 --> 00:03:23,300
that when the aggregate CPU usage went over 80 percent, it triggered a action to then launch a new

48
00:03:23,300 --> 00:03:23,780
instance.

49
00:03:23,960 --> 00:03:28,020
And action is a notification sent to an SRS topic on auto scaling group.

50
00:03:28,430 --> 00:03:34,760
You can also add alarms to dashboards and alarms, invoke actions for sustained state changes only.

51
00:03:35,540 --> 00:03:39,270
They don't invoke actions simply because they're in a particular state.

52
00:03:39,530 --> 00:03:44,640
So in other words, the state must have changed and been maintained for a specific number of periods.

53
00:03:44,660 --> 00:03:50,300
So, for instance, your metric needs to go over 80 percent CPU aggregation for a specific period of

54
00:03:50,300 --> 00:03:53,570
time, not just for a very short period of time.

55
00:03:54,080 --> 00:03:56,810
So remember this diagram from earlier in the course?

56
00:03:57,170 --> 00:04:01,280
In this one, we have an auto scaling group with four Ekta instances.

57
00:04:01,610 --> 00:04:03,310
We have Amazon cloud watch here.

58
00:04:03,890 --> 00:04:07,700
Now it is two status checks are different to cloud watch.

59
00:04:08,000 --> 00:04:13,520
But what we showed before was that an AC two status check might fail and then auto scanning group would

60
00:04:13,520 --> 00:04:16,070
initiate a new instant replacement.

61
00:04:16,700 --> 00:04:23,960
However, what we want to cover now is that we have also an alarm monitoring when the CPU utilization

62
00:04:23,960 --> 00:04:29,510
is greater than 80 percent and that's aggregated across the auto scaling group in that case, cloud,

63
00:04:29,510 --> 00:04:31,770
which notifies IHG to scale.

64
00:04:32,240 --> 00:04:34,760
So we have an alarm wheel on cloud watch.

65
00:04:35,030 --> 00:04:39,680
It's monitoring the incoming metrics from all of these different instances.

66
00:04:39,980 --> 00:04:46,280
And then when it sees that the aggregate CPU is over 80, it sends a notification to the ASG to scale.

67
00:04:46,490 --> 00:04:48,950
And in this case, another instance can then be installed.

