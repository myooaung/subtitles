1
00:00:02,250 --> 00:00:07,600
My guys in this lab were going to go into the console and have a quick look at Amazon EMR.

2
00:00:07,860 --> 00:00:10,080
Now EMR does feature on the exam.

3
00:00:10,200 --> 00:00:16,170
However usually it's doesn't come up very often and when it does the questions are typically quite basic

4
00:00:16,170 --> 00:00:18,450
so you won't need to go into all the details.

5
00:00:18,450 --> 00:00:22,800
There's a lot of open source technologies that are associated with them and you don't really need to

6
00:00:22,800 --> 00:00:24,870
know the ins and outs of all of those.

7
00:00:24,870 --> 00:00:31,560
You just need to know what it is what it's used for and a few key facts which may be tested on in the

8
00:00:31,560 --> 00:00:32,910
exam.

9
00:00:32,910 --> 00:00:35,100
So what is Amazon EMR.

10
00:00:35,100 --> 00:00:40,860
Well it's the leading cloud native Big Data platform so we've all heard of big data and what this does

11
00:00:40,920 --> 00:00:46,200
is it allows you to process vast amounts of data at pretty much any scale.

12
00:00:46,320 --> 00:00:47,980
So it's highly elastic.

13
00:00:48,120 --> 00:00:56,850
It can scale to thousands of nodes and it uses or you use open source tools with it such as Apache Spark

14
00:00:57,150 --> 00:01:03,780
Apache hive H base flank Presto all these open source tools that you may or may not have heard of but

15
00:01:03,780 --> 00:01:10,800
those are the tools that you would use to analyze data on Amazon EMR. EMR uses the concepts of clusters

16
00:01:11,280 --> 00:01:18,010
and in each cluster you have one master and then a number of core nodes and the core nodes do the processing.

17
00:01:18,030 --> 00:01:24,600
You can have up to thousands of core nodes and you always have the one master per cluster and you also

18
00:01:24,600 --> 00:01:30,750
have the option of scaling by adding additional clusters so you can even resize your cluster or you

19
00:01:30,750 --> 00:01:32,290
can add additional clusters.

20
00:01:32,310 --> 00:01:39,630
Now the way that you provision EMR is through two methods. Either you use a cluster and then you have the

21
00:01:39,630 --> 00:01:45,750
options of core Hadoop each base Presto or SPARC or you create a step execution we'll see this in the

22
00:01:45,750 --> 00:01:48,810
console and in this case you have a few different options.

23
00:01:48,810 --> 00:01:53,350
One of the things often comes up in the exam is root access to the cluster instances.

24
00:01:53,460 --> 00:02:00,690
So these instances are based on EC2 so your nodes are all easy to nodes and you have full root access

25
00:02:00,690 --> 00:02:02,010
to those nodes.

26
00:02:02,190 --> 00:02:07,500
So that's very important for some of the tools the open source tools that you would use with EMR data

27
00:02:07,770 --> 00:02:17,070
comes from a number of sources so you can use S3, Glacier, redshift, dynamoDB, RDS or the Hadoop file system

28
00:02:17,100 --> 00:02:23,830
HDFS. Typically the nodes are provisioned with instance stores if you might remember from the section

29
00:02:23,830 --> 00:02:29,430
of course where we cover EBS and you may remember that instance stores are ephemeral storage so that

30
00:02:29,430 --> 00:02:35,880
means the data is lost when you shut down your instance. Not when you reboot it, but when you shut it

31
00:02:35,880 --> 00:02:41,340
down so you can't store data long term but it's very high performance which is why it's really useful

32
00:02:41,580 --> 00:02:49,350
for analytics but you can also attach EBS volumes. Now typically EBS volumes have persistent data however

33
00:02:49,650 --> 00:02:52,580
with EMR when you shut down your clusters you do lose it.

34
00:02:52,610 --> 00:02:56,160
So the data is actually scrubbed from the EBS volume.

35
00:02:56,160 --> 00:03:02,120
You have an option with EMR to not just load data but to access data directly in S3.

36
00:03:02,130 --> 00:03:08,760
So you could have multiple clusters actually pointing to the same S3 Buckets and analyzing the same

37
00:03:08,760 --> 00:03:09,450
data.

38
00:03:09,450 --> 00:03:15,660
Alternatively you can load data in through one of these services and you can use data pipeline.

39
00:03:15,810 --> 00:03:19,260
Another AWS service to help you migrate data.

40
00:03:19,350 --> 00:03:26,710
So let's head into the console and we're just going to provision and EMR cluster. So I'm in the AWS management

41
00:03:26,710 --> 00:03:27,450
console.

42
00:03:27,610 --> 00:03:30,680
I'm going to go to services, analytics, EMR.

43
00:03:30,910 --> 00:03:33,880
So on the splash screen we just want to go and create a cluster.

44
00:03:33,880 --> 00:03:36,000
And I'm just going to leave the default name.

45
00:03:36,190 --> 00:03:42,150
We can say that it comes up with a default location for logging and then we have these two options.

46
00:03:42,150 --> 00:03:43,270
So the cluster.

47
00:03:43,610 --> 00:03:48,510
And it tells us we have cluster EMR creates a cluster with a set of specified applications so these

48
00:03:48,510 --> 00:03:50,660
are the applications core Hadoop.

49
00:03:51,000 --> 00:03:52,890
Each base Presto or spark.

50
00:03:52,920 --> 00:03:54,900
So you need to choose one of these options.

51
00:03:54,900 --> 00:04:01,810
I'm just going to leave it on core Hadoop alternatively you step execution and in this case a cluster

52
00:04:01,810 --> 00:04:07,330
is still created but the steps are executed and then the cluster is terminated once they're complete.

53
00:04:08,440 --> 00:04:14,890
So if you two step execution you have these options here streaming program, pick SPARC application

54
00:04:14,890 --> 00:04:19,810
or custom jar and then you have to fill out a few of those details depending which one you choose.

55
00:04:19,810 --> 00:04:21,160
So let's go back to a cluster.

56
00:04:21,280 --> 00:04:23,500
We're going to leave it selected as core Hadoop.

57
00:04:23,500 --> 00:04:29,800
We're going to leave it on the latest release of EMR and then you get to choose your instance size.

58
00:04:29,800 --> 00:04:32,770
Now note that you have one master and two core nodes.

59
00:04:32,890 --> 00:04:37,630
If I just up this amount you'll notice that you still have one master and they have two hundred ninety

60
00:04:37,620 --> 00:04:38,820
nine core nodes.

61
00:04:38,950 --> 00:04:42,280
So wherever you include there's always gonna be one master.

62
00:04:42,400 --> 00:04:46,060
You can even put that at 3000 and it's still just going to add core nodes.

63
00:04:46,570 --> 00:04:52,280
So we'll leave that it three and let's go and choose the smallest instance so we can reduce our costs.

64
00:04:52,450 --> 00:04:54,480
And in this case are going to see one medium.

65
00:04:54,610 --> 00:04:57,880
Again this is not a free to air incident so be careful.

66
00:04:57,880 --> 00:05:04,290
I understand that you will incur some costs if you run this instance and because this is easy too and

67
00:05:04,290 --> 00:05:10,590
we have the option of actually accessing it via route we can choose our keeper and you could then choose

68
00:05:11,340 --> 00:05:13,400
a role for EMR or if you like as well.

69
00:05:13,410 --> 00:05:18,810
We're just going to leave this on default and we're going to choose create cluster and so interestingly

70
00:05:18,810 --> 00:05:24,540
I got a message terminated of errors and in this case it says the instance type C one medium is not

71
00:05:24,540 --> 00:05:25,650
supported.

72
00:05:25,650 --> 00:05:26,820
So let's go back.

73
00:05:26,820 --> 00:05:34,890
I'm just going to try and create a cluster again and this time I'm just going to choose a different

74
00:05:34,890 --> 00:05:35,310
cluster.

75
00:05:35,310 --> 00:05:43,300
Let's try a C three extra large of compute optimized instance, still and leave the other settings as

76
00:05:43,300 --> 00:05:48,210
defaults and choose create cluster.

77
00:05:48,240 --> 00:05:50,330
I'm going to need to change the name on here.

78
00:05:50,580 --> 00:05:53,490
So just call my cluster 2 and create the cluster

79
00:05:56,390 --> 00:05:58,200
don't get to choose your key pair.

80
00:05:59,290 --> 00:06:03,850
So my cluster's in the starting state and we don't need to wait for this to complete because we're not

81
00:06:03,850 --> 00:06:07,360
actually going to be doing any work on the actual cluster itself but we're just going to have a look

82
00:06:07,360 --> 00:06:08,710
around and see what's what.

83
00:06:09,190 --> 00:06:13,570
So what I've done is I've opened up the east to management console as well because I want you to see

84
00:06:13,570 --> 00:06:15,820
that we now have these free instances.

85
00:06:15,970 --> 00:06:21,310
So clearly this is not like one of those managed services where you don't see the underlying infrastructure

86
00:06:21,310 --> 00:06:24,680
you can access this infrastructure.

87
00:06:24,810 --> 00:06:28,860
Now if you do want to access it you don't need to think about the security groups.

88
00:06:28,920 --> 00:06:35,370
Let's have a look at one of these instances and go to actions networking and change security groups

89
00:06:35,880 --> 00:06:39,450
and we can see that there's a couple of groups here that were created automatically for us.

90
00:06:39,810 --> 00:06:43,950
So we've got the elastic map produced slightly so we know that this must be a slave no because this

91
00:06:43,950 --> 00:06:48,130
was selected and then we had the elastic map reduce master.

92
00:06:48,360 --> 00:06:50,630
So that will be for the master node obviously.

93
00:06:50,670 --> 00:06:56,070
So let's look at the next one and try and work out which ones to master so we can see that this one

94
00:06:56,070 --> 00:07:00,620
is the master because it's using this security group.

95
00:07:00,800 --> 00:07:05,480
Now let's go down to security groups and just see what the rules are so we can see some rules here for

96
00:07:05,510 --> 00:07:12,800
inbound access to the slaves and this all DCP and UDP rules and they come from different security groups

97
00:07:14,030 --> 00:07:21,020
and we can also see ICMP rules to allow the ICMP protocol to work for communication and then if we go

98
00:07:21,020 --> 00:07:30,820
to the master we can also see some rules here that are locked down to specific security groups and then

99
00:07:30,820 --> 00:07:37,910
it's a bunch of IP addresses here which are Amazon IP addresses that allow the cluster manager to communicate

100
00:07:37,910 --> 00:07:39,320
with the master node.

101
00:07:39,320 --> 00:07:41,990
So one of the things that's missing is Port 22.

102
00:07:41,990 --> 00:07:48,140
If we did want to connect into these instances we would need to have port 20 to say let's just add an

103
00:07:48,140 --> 00:07:52,240
additional security group to our slave instance.

104
00:07:52,490 --> 00:07:57,530
So we'll go about we've got two slave instances I'm just gonna choose one of them go to networking change

105
00:07:57,530 --> 00:07:59,960
security groups and I'll just add in web access.

106
00:08:00,080 --> 00:08:05,410
Let's now head over to a command line with the IP address of the server copy to our clipboard.

107
00:08:05,510 --> 00:08:06,810
So I'm at the command line.

108
00:08:06,830 --> 00:08:07,620
Let's SSH.

109
00:08:07,640 --> 00:08:08,780
Into this instance

110
00:08:12,770 --> 00:08:17,870
and sure enough we're logged in and we can see EMR written on the screen in big letters here.

111
00:08:17,870 --> 00:08:23,990
So we know we have access to log in and work on this server.

112
00:08:23,990 --> 00:08:31,110
Let's head back to the console and have a look at a few more features of EMR some back in the AWS console

113
00:08:31,110 --> 00:08:31,850
for EMR.

114
00:08:32,250 --> 00:08:38,600
Let's just refresh waiting cluster ready so this cluster is now ready to perform operations on its actually

115
00:08:38,610 --> 00:08:41,240
start loading some data and running some analytics.

116
00:08:41,550 --> 00:08:45,450
And you can see some information about the master and the core nodes.

117
00:08:45,450 --> 00:08:53,620
You can choose to view the cluster details and look at the applications that are being used.

118
00:08:53,670 --> 00:09:00,420
You have lots of monitoring information here from cloud watch and the hardware you can now see the instances

119
00:09:00,420 --> 00:09:05,850
that are running so gives you a bit of a breakdown and you could go in here then and actually resigns

120
00:09:05,880 --> 00:09:10,680
these instances so you can go in and choose the amount of instances that you want to run.

121
00:09:10,890 --> 00:09:14,160
You then also have events can see what's going on.

122
00:09:14,160 --> 00:09:15,560
And you also have steps.

123
00:09:15,600 --> 00:09:22,050
So here you can add additional steps now a step is a programmatic task for performing a process on the

124
00:09:22,050 --> 00:09:22,940
data.

125
00:09:22,950 --> 00:09:29,220
So if you wanted to count the words in your data or perform an operation to modify data you can add

126
00:09:29,220 --> 00:09:36,480
a step to perform that operation under security configurations you can go in and create a security configuration

127
00:09:36,480 --> 00:09:44,550
you can enable encryption at rest encryption in transit authentication using cobras and you can also

128
00:09:44,550 --> 00:09:51,420
use IAM rules for the EMR file system the EMR file system is what is use when you want to access data

129
00:09:51,420 --> 00:09:57,540
directly from S3 just going to cancel on there under VPC subnets.

130
00:09:57,550 --> 00:10:04,370
This is where you can configure your subnets so that you can add an S3 endpoint or access to S3 or net

131
00:10:04,410 --> 00:10:05,220
instance.

132
00:10:05,290 --> 00:10:10,660
So your instances are running in subnets within your VPC.

133
00:10:10,660 --> 00:10:16,510
You may have noticed in the diagram that all of our clusters here are running in a single availability

134
00:10:16,510 --> 00:10:17,220
zone.

135
00:10:17,230 --> 00:10:22,270
Now you can run them across different availability zones but all the nodes of your cluster will be within

136
00:10:22,270 --> 00:10:23,880
one availability zone.

137
00:10:23,920 --> 00:10:30,120
So if you actually want your EMR instances to be able to access data on a S3 you need to add your

138
00:10:30,260 --> 00:10:33,450
S3 endpoint to that subnet.

139
00:10:33,640 --> 00:10:39,370
And if you want them to access public AWS service endpoints then you need to enter a NAT device and

140
00:10:39,370 --> 00:10:40,650
you'll see it here.

141
00:10:40,660 --> 00:10:42,880
If you actually have one present.

142
00:10:42,880 --> 00:10:47,290
So for these subnets we don't have that but we're not trying to enable any outbound access at this stage

143
00:10:47,290 --> 00:10:50,180
anyway so we'll give that a miss.

144
00:10:50,260 --> 00:10:55,690
Again we can see events across all our clusters under notebooks you can create multi-user notebooks

145
00:10:56,470 --> 00:11:02,410
and these are used by data scientists analysts and developers to prepare and visualize data and also

146
00:11:02,410 --> 00:11:04,230
collaborate with their peers.

147
00:11:04,450 --> 00:11:12,250
So you can head in here and you can create your notebook and specify your cluster your security groups

148
00:11:12,880 --> 00:11:20,730
and the location where the files for this notebook are actually stored so that's just a very quick overview

149
00:11:20,790 --> 00:11:21,960
of EMR.

150
00:11:22,050 --> 00:11:24,990
What we're going to do is we're just going to terminate this cluster

151
00:11:28,390 --> 00:11:32,740
and don't forget to do this if you have run it up because remember these are not free tier so you

152
00:11:32,740 --> 00:11:35,440
will pay something for running these instances.

153
00:11:35,590 --> 00:11:42,430
And if I were you I'd make sure that once this is terminated just go into your instances and

154
00:11:42,430 --> 00:11:44,710
just make sure that nothing is running that everything's gone.

