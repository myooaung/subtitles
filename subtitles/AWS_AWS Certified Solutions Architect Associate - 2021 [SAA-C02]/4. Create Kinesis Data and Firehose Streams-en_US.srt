1
00:00:02,160 --> 00:00:02,640
Hi guys.

2
00:00:02,640 --> 00:00:07,460
In this lab we're just going to go and use the console to create a couple of streams.

3
00:00:07,470 --> 00:00:14,350
So we're going to create a data stream and a Firehose delivery stream so going to click data stream

4
00:00:14,680 --> 00:00:21,940
and I'm just gonna call this my data stream and then we get to choose the number of shards and you can

5
00:00:21,940 --> 00:00:28,110
even use this option to put in your record sizes and the number of consumer applications you're going

6
00:00:28,110 --> 00:00:28,680
to use.

7
00:00:28,690 --> 00:00:30,690
And then you can provision your shards.

8
00:00:30,700 --> 00:00:36,340
So it says here you can provision up to 200 more shards before heading your account limit of two hundred.

9
00:00:36,400 --> 00:00:38,050
So let's just put in two.

10
00:00:38,050 --> 00:00:40,330
I don't need two hundred and that's it.

11
00:00:40,450 --> 00:00:45,220
That's the only option that you need to put in and then you just click Create a stream

12
00:00:51,280 --> 00:00:52,500
so that's being created.

13
00:00:52,510 --> 00:00:56,170
What I'm going to do is just go back to the dashboard and we're going to have a look at what the options

14
00:00:56,170 --> 00:00:57,830
are with Firehose.

15
00:00:58,030 --> 00:01:06,280
So I'm going to call this one my delivery stream and then you get to choose your source.

16
00:01:06,550 --> 00:01:08,360
So it's even a direct put.

17
00:01:08,430 --> 00:01:13,420
And it says choose this option to send records directly to the delivery stream or to send records from

18
00:01:13,450 --> 00:01:17,560
eight of us IO T cloud which logs or CloudWatch events.

19
00:01:17,560 --> 00:01:21,230
And the other option is to use a can ease stream.

20
00:01:21,250 --> 00:01:29,010
So now I could choose my data stream and then I can click next.

21
00:01:29,120 --> 00:01:34,190
We now have this option to transform records with AWS Lambda.

22
00:01:34,190 --> 00:01:38,990
So if I enabled this I could go in and I could choose a Lambda function and then whatever I can do with

23
00:01:38,990 --> 00:01:43,840
my code in Lambda i can use to transform these records in some way.

24
00:01:44,060 --> 00:01:46,420
Let's just leave that disabled.

25
00:01:46,490 --> 00:01:53,810
You can also then enable this option to convert the record format and you can choose to use the glue

26
00:01:53,810 --> 00:01:54,440
service

27
00:01:57,380 --> 00:02:03,000
and you can choose between the output formats Apache Park or Apache org.

28
00:02:03,200 --> 00:02:08,410
So we'll just disable those options and we'll just click on Next.

29
00:02:08,510 --> 00:02:15,380
And now you get the option to deliver to a destination and the wizard changes depending obviously on

30
00:02:15,380 --> 00:02:20,960
what you're going to use as a destination if I choose an entry destination I get to put in my bucket

31
00:02:20,960 --> 00:02:28,430
name but choose redshift it's going to ask me for a cluster and then the same with elastic search to

32
00:02:28,430 --> 00:02:34,160
specify the parameters there and then you can also use Splunk as a destination and it has the details

33
00:02:34,160 --> 00:02:36,980
here to lead your Splunk configuration.

34
00:02:36,980 --> 00:02:40,410
And as we'll see you then have this option to back up.

35
00:02:40,640 --> 00:02:45,470
So I'm just gonna go back up here choose Amazon S3 and choose one of my buckets.

36
00:02:45,470 --> 00:02:46,350
I think I'll leave it there.

37
00:02:46,350 --> 00:02:54,240
DCT company you could then choose a prefix and even an error prefix and then you click next.

38
00:02:54,240 --> 00:02:57,200
So now we have the option to set the buffer conditions.

39
00:02:57,200 --> 00:03:03,060
It says that Firehose buffers incoming records built before delivering them to you as free bucket.

40
00:03:03,060 --> 00:03:09,420
So you choose your buffer size and your buffer interval and you can then enable encryption so you can

41
00:03:09,420 --> 00:03:10,250
choose how you.

42
00:03:10,290 --> 00:03:17,040
Excuse me compression and encryption so you could choose how to compress your data and you could choose

43
00:03:17,040 --> 00:03:25,260
to enable encryption and choose the CMK then have logging which will go to CloudWatch Logs

44
00:03:25,620 --> 00:03:28,660
and you can enable tanks and then I am wrong.

45
00:03:28,830 --> 00:03:32,580
Let's just click next and we have to choose the Role

46
00:03:36,040 --> 00:03:41,770
and yeah we don't actually have a role for this now so let's just see what it's configuring and it's

47
00:03:42,760 --> 00:03:43,840
enabling glue.

48
00:03:43,840 --> 00:03:44,980
Get table versions.

49
00:03:44,980 --> 00:03:49,540
That's just leaves the default options here and click allow

50
00:03:52,610 --> 00:03:59,210
and we've now chosen all wrong and as click next and we can review and then create the delivery stream

51
00:04:01,110 --> 00:04:06,130
so that's just to show you a brief overview of the console and what's here.

52
00:04:06,280 --> 00:04:08,120
We don't really need these.

53
00:04:08,230 --> 00:04:09,780
We're just going to delete.

54
00:04:09,970 --> 00:04:21,040
We go in to data streams as well and just select my stream and delete.

55
00:04:21,430 --> 00:04:27,730
And what we're going to do in the next lab is create a Kinesis stream and have Lambda consume events

56
00:04:27,850 --> 00:04:33,700
from the Kinesis stream using an event source mapping and then log events to CloudWatch Logs.

