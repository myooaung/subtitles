WEBVTT
1
00:00:01.880 --> 00:00:04.530
Hi, guys, I'm in the AWB management console.

2
00:00:04.760 --> 00:00:09.580
I'm going to go to services and go down here to analytics and this is where you can find a FENA.

3
00:00:09.590 --> 00:00:14.540
Now, once you've logged into a FENA, you may see a different screen to me if you've not used it before.

4
00:00:14.750 --> 00:00:17.870
I've already gone through a couple of tutorials, so I don't see the splash screen.

5
00:00:18.020 --> 00:00:23.690
I just go straight in here to the query editor now actually running queries and creating tables and

6
00:00:23.690 --> 00:00:27.620
databases and so on is not something you're going to need to know for the exam.

7
00:00:27.920 --> 00:00:32.120
This is going a little bit beyond and it's also something which I'm not an expert at.

8
00:00:32.130 --> 00:00:38.570
So to make it easy for everyone, what I just want to do is go through this tutorial here and you'll

9
00:00:38.570 --> 00:00:39.170
find this.

10
00:00:39.170 --> 00:00:45.830
You'll just click on this tutorial and it actually takes you for a wizard and uses a free URL to load

11
00:00:45.830 --> 00:00:46.400
some data.

12
00:00:46.400 --> 00:00:50.000
And so this is a good way just to have a play around and see what it is.

13
00:00:50.000 --> 00:00:55.010
But again, probably beyond the scope of the exam, really, for the exam, you need to understand what

14
00:00:55.010 --> 00:00:57.640
these services are and in what context they use.

15
00:00:57.680 --> 00:00:59.180
So let's click on next.

16
00:00:59.180 --> 00:01:06.050
And it says it's going to create a table and we're going to query some Elby log files and say the first

17
00:01:06.050 --> 00:01:09.530
thing we need to do is to find the corresponding table in a FENA.

18
00:01:09.530 --> 00:01:13.850
So I chose next announced says choose create a new database.

19
00:01:14.210 --> 00:01:17.500
So we've got create a new database and then type default for the database.

20
00:01:17.720 --> 00:01:25.470
We then need to type SLB, underscore logs for the table name, and then I'm just going to copy this

21
00:01:25.550 --> 00:01:29.010
free URL here and this is where the data is going to be loaded from.

22
00:01:29.420 --> 00:01:34.480
So this is some Elby log data and we can then choose next on either of these.

23
00:01:34.490 --> 00:01:40.490
We now need to choose the data format, it says, to use Apache weblogs and then for the regular expression.

24
00:01:40.510 --> 00:01:45.620
So this helps to sort of pass the data and understand the format that the data is in.

25
00:01:46.040 --> 00:01:50.910
So we can literally just copy that paste into here and then click on next.

26
00:01:51.110 --> 00:01:56.930
Now, in this case, you have to fill out the column name so it understands the data and can put the

27
00:01:56.930 --> 00:01:59.240
data into columns in our database.

28
00:01:59.900 --> 00:02:05.040
And we can just click on click here and it will fill those out for us.

29
00:02:05.040 --> 00:02:08.950
So you can see it's created all these different columns and you can see what they relate to.

30
00:02:08.960 --> 00:02:14.390
So you got the request timestamp, the name of the Elby, the request IP address and the port, the

31
00:02:14.390 --> 00:02:15.680
back end IP and so on.

32
00:02:16.100 --> 00:02:21.110
So these will be columns within the database and then the data will get loaded into those columns.

33
00:02:21.740 --> 00:02:26.840
So let's choose next says you can actually create a partition for a Fener, but right now we don't need

34
00:02:26.840 --> 00:02:29.450
to configure partitions for the EOP logs table.

35
00:02:30.610 --> 00:02:34.450
So we just choose next and we can see now that it's created this query.

36
00:02:35.350 --> 00:02:40.540
And it says that the above DDL statement for The Epilogues Table was generated by the entries in The

37
00:02:40.540 --> 00:02:44.200
Wizard, and when we choose one query, the table will be created.

38
00:02:44.800 --> 00:02:49.550
So I can just close out of this now and choose and run, and that's successful.

39
00:02:49.750 --> 00:02:56.140
So we now have this table here and we can see the headers that correspond to those columns that are

40
00:02:56.140 --> 00:02:57.220
going to be in the table.

41
00:02:57.340 --> 00:03:02.140
So what I'm going to do now is I'm just going to click here and choose Prevue Table, and that will

42
00:03:02.140 --> 00:03:06.760
create another query that selects everything from Defo and Elby logs.

43
00:03:07.360 --> 00:03:13.810
So if I just choose so close that one query and what we can see now, we have a results down here and

44
00:03:13.810 --> 00:03:18.700
you can see that information from the elastic load balancer logs.

45
00:03:19.060 --> 00:03:23.830
So you can see the different column names that we saw before, the name of the Elby, the request IP,

46
00:03:23.830 --> 00:03:25.920
the request for the backend IP and so on.

47
00:03:26.470 --> 00:03:31.720
So we can see all this information is now being put into this table.

48
00:03:32.090 --> 00:03:36.820
And what we could then do is run any Escorial queries we want on that data.

49
00:03:36.970 --> 00:03:38.110
So that's Amazon.

50
00:03:38.110 --> 00:03:44.440
Athena, let's head over now to Adewusi Glew so you can find Golu here.

51
00:03:44.770 --> 00:03:49.690
I'm going to open it in a new tab and we can see that already has the Elby Logs table, which we've

52
00:03:49.690 --> 00:03:50.320
just created.

53
00:03:50.350 --> 00:03:52.680
So this is this is a correct time stamp.

54
00:03:52.690 --> 00:03:56.650
This is the one that I just created now a few minutes ago as part of this lab.

55
00:03:56.650 --> 00:04:02.290
And what I want to do is go down to the bottom here and there's a tutorials and I'm going to choose

56
00:04:02.290 --> 00:04:03.990
the add crawler tutorial.

57
00:04:04.540 --> 00:04:09.970
So this says you're asked to analyze arrival data for major air carriers to calculate the popularity

58
00:04:09.970 --> 00:04:12.810
of departure airports month over month.

59
00:04:13.270 --> 00:04:18.190
So you get flights, data for the year 2016 in CSV format stored in as free.

60
00:04:18.850 --> 00:04:23.590
And then before you transform your data, you catalog its metadata in the glue data catalog.

61
00:04:24.190 --> 00:04:30.220
So let's add the crawler and that's going to infer metadata from the flight logs Innisfree and create

62
00:04:30.220 --> 00:04:31.960
a table in your data catalog.

63
00:04:32.740 --> 00:04:36.700
So we choose next and it wants us to call it flight data crawler.

64
00:04:36.880 --> 00:04:41.930
I'm just going to make life easy and copy that and make sure I get it all and choose next for this year.

65
00:04:42.250 --> 00:04:44.340
Select data stores as the crawler types.

66
00:04:44.350 --> 00:04:45.130
That's the default.

67
00:04:45.190 --> 00:04:46.890
Now let's point the crawler to your data.

68
00:04:46.900 --> 00:04:48.510
Choose the S3 data store.

69
00:04:49.180 --> 00:04:54.820
So what I need to do is I need to pull out this as free URL, make sure you don't get the full stop

70
00:04:54.820 --> 00:04:59.290
on the end there and I'm going to put that into the path here.

71
00:04:59.740 --> 00:05:06.640
So data store is as free specified path in my account that should be in another account and then choose

72
00:05:06.640 --> 00:05:07.170
next.

73
00:05:07.180 --> 00:05:12.370
And you can add another data store, but we're not going to do that now in this case for you, you might

74
00:05:12.370 --> 00:05:13.360
need to create this role.

75
00:05:13.360 --> 00:05:19.780
I've run through this tutorial before, so I already have the role so I can actually come down and just

76
00:05:19.780 --> 00:05:26.560
select the roll and choose next and we can run this on demand now, says Crullers, create tables in

77
00:05:26.560 --> 00:05:27.340
your data catalog.

78
00:05:27.340 --> 00:05:28.990
Tables are contained in a database.

79
00:05:28.990 --> 00:05:31.870
Let's choose or create the Flight Stoebe database.

80
00:05:32.170 --> 00:05:39.850
So I'm going to choose that you had a database put in the name and click on CREATE and we need to enter

81
00:05:39.850 --> 00:05:44.210
flights for the prefix for the table and choose next.

82
00:05:44.230 --> 00:05:44.860
So that's it.

83
00:05:44.860 --> 00:05:49.300
We can then just choose finish and we can then go in and run our crawler.

84
00:05:49.300 --> 00:05:50.890
So the crawler is now running.

85
00:05:51.010 --> 00:05:55.840
So as it says up here, Crawler connects to a datastore, progresses through a list of classifiers to

86
00:05:55.840 --> 00:06:00.490
determine the schema for your data, and then creates metadata tables in your data catalog.

87
00:06:00.640 --> 00:06:05.410
So, again, a lot of this is beyond the scope of the exam, but it's just to get you into the console

88
00:06:05.560 --> 00:06:10.330
and start actually using the service and it'll help you gain a little bit more understanding about what

89
00:06:10.330 --> 00:06:11.050
it's all about.

90
00:06:11.800 --> 00:06:16.090
Now, you can see on the left here, we are concentrating on the data catalog, but then you've got

91
00:06:16.090 --> 00:06:17.200
the ETEL as well.

92
00:06:17.200 --> 00:06:21.190
So this is where you can create those extract, transform and load jobs.

93
00:06:21.580 --> 00:06:25.270
So that completed and you can have a look at the log files in cloud watch logs.

94
00:06:25.780 --> 00:06:29.050
You can also go to tables and you can see the flight CSV table.

95
00:06:29.560 --> 00:06:35.140
And if you come down to tutorials, if you want to continue now, I'm going to leave it here because

96
00:06:35.140 --> 00:06:37.240
I think I've gone a bit too far already.

97
00:06:37.240 --> 00:06:40.870
This is just to give you an idea of what these two services can do.

98
00:06:41.230 --> 00:06:47.470
But if you want to continue and play around a bit more, you can now go and do the explore table tutorial.

99
00:06:47.470 --> 00:06:52.840
And this carries on kind of where we left off because it starts exploring that flights CSFI table.

100
00:06:53.140 --> 00:06:57.850
So go through that step by step and you'll learn a little bit more about.

