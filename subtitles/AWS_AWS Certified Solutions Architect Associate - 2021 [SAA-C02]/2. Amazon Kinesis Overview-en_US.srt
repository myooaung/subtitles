1
00:00:02,000 --> 00:00:06,670
Guys, the next service I'm going to talk about in this section is Amazon Kinesis.

2
00:00:06,920 --> 00:00:10,090
So this is a service that deals with streaming data.

3
00:00:10,430 --> 00:00:15,230
So it allows you to collect, process and analyze that streaming data in real time.

4
00:00:16,070 --> 00:00:19,710
And what you can see on the screen here are free of the services.

5
00:00:19,940 --> 00:00:23,820
So there's Kinesis, data streams, data analytics and data firehose.

6
00:00:24,320 --> 00:00:28,210
Now, they don't necessarily work together in this pattern, but they can do.

7
00:00:28,520 --> 00:00:30,520
And in this case, we have producers.

8
00:00:30,740 --> 00:00:36,500
So this could be Iot data or some form of streaming data that's coming into data streams.

9
00:00:36,740 --> 00:00:41,270
And then data analytics allows you to run Esquibel queries on that data.

10
00:00:41,780 --> 00:00:47,630
And you could then put the data into firehose, which can load it into a data store, in this case,

11
00:00:47,630 --> 00:00:49,150
redshift, or as free.

12
00:00:49,310 --> 00:00:53,980
So it's a collection of services for processing streams of various types of data.

13
00:00:54,440 --> 00:01:00,640
Kinesis family includes data streams, data firehose, data analytics and video streams.

14
00:01:01,040 --> 00:01:05,820
Now, video streams is definitely out of scope and data analytics doesn't come up much.

15
00:01:06,200 --> 00:01:12,140
The key thing to know about data analytics is it's a way that you can run escudo queries on your streaming

16
00:01:12,140 --> 00:01:12,630
data.

17
00:01:13,190 --> 00:01:15,630
Other than that, I don't think you need to know much more.

18
00:01:15,800 --> 00:01:20,030
So what we're going to focus on is data streams and data firehose.

19
00:01:20,600 --> 00:01:28,490
So Kinesis deals with streaming data and that means use cases such as purchases from online stores could

20
00:01:28,490 --> 00:01:34,580
be stock price information, could be game data like the statistics and results of a game.

21
00:01:35,130 --> 00:01:40,800
It could be social network data, geospatial data, Iot sensor data and so on.

22
00:01:41,270 --> 00:01:45,560
So let's look at data streams and a bit more detail with data streams.

23
00:01:45,560 --> 00:01:49,700
You have producers and they capture and send data to Kinesis.

24
00:01:50,000 --> 00:01:51,080
You then have a stream.

25
00:01:51,110 --> 00:01:53,300
So this blue box here is the stream.

26
00:01:53,630 --> 00:01:59,960
And within that stream you have what are called shards and shards actually carry the data and data is

27
00:01:59,960 --> 00:02:04,870
stored in those shards for 24 hours by default or up to seven days.

28
00:02:05,210 --> 00:02:09,010
So the data is captured and then it can be stored in assured for processing.

29
00:02:09,350 --> 00:02:15,530
So then you have the consumers in this case, two instances which are consuming data from the shards.

30
00:02:16,070 --> 00:02:22,190
So they make an API calls to Kinesis to pull that data down and they can use something called the Kinesis

31
00:02:22,190 --> 00:02:23,590
Client Library to do that.

32
00:02:23,780 --> 00:02:30,920
You then have destination's and these can be Amazon S3, Dinamo, DB Redshift, EMR and firehose.

33
00:02:31,160 --> 00:02:34,120
And you could then run analytics on your data once it's there.

34
00:02:34,430 --> 00:02:40,430
So consumers take the data and process it and the data can be saved into another service.

35
00:02:40,880 --> 00:02:47,870
One Shard provides a capacity of one make per second data invert and two megabytes a second data output

36
00:02:48,380 --> 00:02:52,240
and one shard can support up to a thousand records per second.

37
00:02:52,580 --> 00:02:55,760
So you need to remember that there are limits at the Shahd level.

38
00:02:56,090 --> 00:03:02,390
So the actual capacity of your stream depends on the number of shards that you have and you pay per

39
00:03:02,390 --> 00:03:02,930
Shahd.

40
00:03:03,080 --> 00:03:04,760
So that's another consideration.

41
00:03:04,760 --> 00:03:10,670
When designing Kinesis streams, the total capacity of the stream is the sum of the capacities of its

42
00:03:10,670 --> 00:03:17,450
shards, because each of those shards has a maximum capacity and data stream supports something called

43
00:03:17,450 --> 00:03:20,720
resodding, which lets you adjust the number of shards.

44
00:03:21,200 --> 00:03:27,170
Now, this is used to adapt to changes in the rate of data flow in your stream, and it is important

45
00:03:27,170 --> 00:03:28,420
to know this for the exam.

46
00:03:29,060 --> 00:03:30,890
So there are two types of resodding.

47
00:03:30,980 --> 00:03:33,660
You've got shot, split and a shard merge.

48
00:03:34,130 --> 00:03:41,000
So in a sharp split on the left there, you divide a single shard into two shots and that means you're

49
00:03:41,000 --> 00:03:43,480
increasing the capacity of your stream.

50
00:03:43,850 --> 00:03:45,670
Otherwise you can merge Assad.

51
00:03:45,950 --> 00:03:49,310
And in this case, you're combining shards into a single shard.

52
00:03:49,520 --> 00:03:54,860
So you might do that if the shards are underutilized and now you're going to decrease cost and also

53
00:03:54,860 --> 00:03:56,900
decrease the capacity of the stream.

54
00:03:57,290 --> 00:04:04,580
So splitting increases the number of shards in your stream and consequently increases the data capacity

55
00:04:04,580 --> 00:04:06,980
of the stream and also the cost.

56
00:04:07,310 --> 00:04:11,900
So you pay per shard, which means that the more shards you have, the more you're going to pay.

57
00:04:12,590 --> 00:04:19,330
Now merging will decrease the data capacity and also therefore decrease the cost of the stream.

58
00:04:19,880 --> 00:04:26,320
So you've got to be very careful about how you allocate shards and monitor their utilization.

59
00:04:26,540 --> 00:04:31,550
So the next service is data firehose and it looks something like this.

60
00:04:31,550 --> 00:04:36,360
You have producers that send data to firehose and then you have your stream.

61
00:04:36,680 --> 00:04:38,090
Now there are no shards.

62
00:04:38,090 --> 00:04:44,120
It's completely automated, which means scalability is elastic, whereas with data streams you actually

63
00:04:44,120 --> 00:04:50,510
have to add shards manually, which means scalability is something that you've got to keep on top of

64
00:04:50,720 --> 00:04:57,890
from the stream teaches, then loaded to the destination's so it can go to another AWG service and it

65
00:04:57,890 --> 00:05:01,460
can also optionally be processed by LAMDA as it's.

66
00:05:01,500 --> 00:05:06,030
Being moved over there, it's a fully managed service and it's near real time.

67
00:05:06,270 --> 00:05:13,650
There's about 60 seconds of licencee, so the destinations are as free Redshift ElasticSearch and Splunk.

68
00:05:14,280 --> 00:05:15,690
To finish off this lesson.

69
00:05:15,930 --> 00:05:24,390
I just want to compare Kinesis with Excuse and S.A. So with Kinesis, consumers pull data that's the

70
00:05:24,390 --> 00:05:25,460
same excuse.

71
00:05:25,830 --> 00:05:30,270
Whereas with S.A.S., data is pushed to a subscriber now a kinesis.

72
00:05:30,270 --> 00:05:32,090
You've got as many consumers as you need.

73
00:05:32,850 --> 00:05:34,380
There's a few key advantages.

74
00:05:34,380 --> 00:05:37,940
You can route related records to the same record processor.

75
00:05:38,040 --> 00:05:43,320
You can have multiple applications accessing the stream concurrently and you get ordering at the sharp

76
00:05:43,320 --> 00:05:48,450
level and you can also consume records in the correct order at a later time as well.

77
00:05:49,260 --> 00:05:54,300
The caveat with Kinesis is you do have to manage your stream throughput, so you've got to constantly

78
00:05:54,300 --> 00:05:58,740
consider how many shots you've got and how much data is going through them.

79
00:05:58,920 --> 00:06:05,090
With Ishikawa's data is deleted after it's consumed, you can have many, as many workers as you need.

80
00:06:05,550 --> 00:06:09,600
There's no ordering guaranteed, though, except for with FIFO queues.

81
00:06:09,600 --> 00:06:12,840
And as you know, they have some limitations in terms of throughput.

82
00:06:13,140 --> 00:06:15,960
Escalus does provide messaging semantics.

83
00:06:16,200 --> 00:06:21,630
It also gives you individual message delay and it scales dynamically with S.A.S..

84
00:06:21,630 --> 00:06:23,400
It's a publisher subscriber model.

85
00:06:23,970 --> 00:06:28,980
It integrates with us for that financial architecture pattern that we looked at earlier in the section.

86
00:06:29,550 --> 00:06:30,950
And it does have some limits.

87
00:06:30,960 --> 00:06:35,850
So there are 10 million subscribers maximum and up to 10000 topics.

88
00:06:36,300 --> 00:06:39,450
That is not persistent and you don't need sufficient throughput.

89
00:06:39,450 --> 00:06:41,130
So it's another dynamic service.

90
00:06:41,400 --> 00:06:47,220
So those are a few key considerations between these services to help you understand the use cases and

91
00:06:47,220 --> 00:06:48,360
the limitations of each.

92
00:06:48,510 --> 00:06:50,370
And they could be useful for you in exam.

