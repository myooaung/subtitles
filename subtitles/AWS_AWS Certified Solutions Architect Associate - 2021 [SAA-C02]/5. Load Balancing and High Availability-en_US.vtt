WEBVTT
1
00:00:05.610 --> 00:00:08.880
You just learned about auto scaling, and we've also scaling.

2
00:00:09.030 --> 00:00:12.960
We end up with lots of different instances of the same application.

3
00:00:13.710 --> 00:00:16.290
So there's a question how do we direct traffic to them?

4
00:00:16.920 --> 00:00:23.220
So what I mean is, let's say in our cloud, we have these free web servers running the Engine X web

5
00:00:23.220 --> 00:00:25.080
service and on the internet.

6
00:00:25.080 --> 00:00:28.260
We have some users and they want to connect to our application.

7
00:00:28.740 --> 00:00:32.190
Now they can connect to any one of these because they're all identical.

8
00:00:32.850 --> 00:00:33.780
But how do they do that?

9
00:00:33.810 --> 00:00:35.220
How do they know where to go?

10
00:00:35.310 --> 00:00:39.570
Now, what we really want to do is spread the load between our instances.

11
00:00:40.350 --> 00:00:47.640
So we have something called a load balancer and the load balancer has an address and the user's computers

12
00:00:47.640 --> 00:00:50.040
will then connect to the load balancer address.

13
00:00:50.640 --> 00:00:54.300
So, for instance, each one will connect two example dot com.

14
00:00:54.300 --> 00:01:00.330
That's the address associated with this load balancer, and the load balance will forward the connection

15
00:01:00.330 --> 00:01:02.130
on to one of the web servers.

16
00:01:02.520 --> 00:01:07.680
When another user connects, it sends the connection to another web server and so on.

17
00:01:08.130 --> 00:01:10.280
So that's how we distribute the load.

18
00:01:10.290 --> 00:01:17.310
We have one end point address which is publicly available, and then we're able to distribute the connections

19
00:01:17.520 --> 00:01:21.510
to multiple instances and they can be in multiple availability zones.

20
00:01:22.320 --> 00:01:27.840
Now, if an instance fails, the load balancer will also reroute the connection.

21
00:01:27.840 --> 00:01:33.360
So now we have a connection going to web server to for use of one instead of web server one.

22
00:01:33.660 --> 00:01:38.490
So this is how we enable high availability now as well as high availability.

23
00:01:38.490 --> 00:01:40.080
You might have heard of fault tolerance.

24
00:01:40.740 --> 00:01:47.130
Fault tolerance is where we have the ability to recover in the case of something like a component failure.

25
00:01:47.580 --> 00:01:54.270
So here's a computer that has CPU memory, hard disk drive, a network card and the network has just

26
00:01:54.270 --> 00:01:54.690
failed.

27
00:01:55.230 --> 00:01:56.760
Then maybe a hard drive fails.

28
00:01:57.150 --> 00:02:03.750
Now, even one of these will cause an outage of the system because this system doesn't have any built

29
00:02:03.750 --> 00:02:04.680
in redundancy.

30
00:02:05.370 --> 00:02:12.000
Now, another way we could do this is to have multiple hard drives and multiple network interface cards.

31
00:02:12.390 --> 00:02:19.320
And in that case, if we lose one of either of those components, it's OK because we have fault tolerance

32
00:02:19.320 --> 00:02:23.400
in the system and we're able to continue to operate.

33
00:02:23.700 --> 00:02:25.650
Now we can mix these things together.

34
00:02:26.280 --> 00:02:33.000
So IWC provides a lot of fault tolerance built in to their infrastructure, and we're able to spread

35
00:02:33.000 --> 00:02:38.730
the load between different availability zones so that we have fault tolerance across the availability

36
00:02:38.730 --> 00:02:44.790
zones because each one is one or more data centers, so they're physically distinct from each other.

37
00:02:45.090 --> 00:02:50.820
And then we can spread the load for high availability to make sure that all users are constantly connected

38
00:02:50.820 --> 00:02:53.580
to working instances of our application.

39
00:02:53.850 --> 00:02:57.360
Now you should think of the availability zone as a separate data center.

40
00:02:57.540 --> 00:03:02.820
As I said, they are actually composed of one or more data centers, so remember that for the exam,

41
00:03:03.390 --> 00:03:08.880
but they're physically distinct from other availability zones and have high speed connectivity between

42
00:03:08.880 --> 00:03:09.030
them.

43
00:03:09.540 --> 00:03:15.270
Now again, in this instance, we can still have things like a web server failing, and if that happens,

44
00:03:15.270 --> 00:03:18.930
the connection gets dropped and rerouted to another web server.

45
00:03:19.260 --> 00:03:26.310
If we add in also scaling as well, then we get an even better solution because that failed web server

46
00:03:26.580 --> 00:03:28.890
can now actually be replaced by also scaling.

47
00:03:29.580 --> 00:03:36.720
So we regularly use auto scaling and load balancing together to make sure that we have the right number

48
00:03:36.720 --> 00:03:44.400
of EC2 instances that we can recover from failure and that we can direct the incoming connections across

49
00:03:44.400 --> 00:03:45.480
all of our instances.

50
00:03:45.870 --> 00:03:49.140
And in this case, User two ends up on web server four.

51
00:03:49.560 --> 00:03:52.140
Now, of course, we get a lot of resiliency here.

52
00:03:52.380 --> 00:03:54.630
Also at the availability zone level.

53
00:03:55.050 --> 00:03:58.920
So even if we lost an entire availability zone, it can happen.

54
00:03:59.160 --> 00:04:02.130
Sometimes connectivity to an agency might fail.

55
00:04:02.580 --> 00:04:08.490
But if you have your resources deployed across multiple availability zones, you're still operational

56
00:04:08.730 --> 00:04:13.620
and you can launch more instances into that AC to make sure you've got enough capacity.

57
00:04:13.950 --> 00:04:16.770
So now again, our connections are remounted.

58
00:04:17.190 --> 00:04:23.250
So those are the concepts around high availability and fault tolerance and how we use load balancing

59
00:04:23.460 --> 00:04:24.840
and also scaling together.

60
00:04:25.440 --> 00:04:30.330
And in the next lesson, we're going to look at the Amazon Elastic Load balancing service.

