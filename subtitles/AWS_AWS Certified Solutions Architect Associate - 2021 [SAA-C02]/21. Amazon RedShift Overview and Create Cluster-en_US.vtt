WEBVTT
1
00:00:01.830 --> 00:00:02.390
Hi guys.

2
00:00:02.550 --> 00:00:08.460
Amazon Redshift is a data warehouse that is used with structured query language,

3
00:00:08.480 --> 00:00:14.030
SQL, and business intelligence tools to perform analytics on data.

4
00:00:14.040 --> 00:00:21.630
If you remember this slide from earlier in this section we have operational and transactional databases

5
00:00:21.870 --> 00:00:27.080
and analytical databases redshift falls squarely in the analytical space.

6
00:00:27.180 --> 00:00:34.050
So it's an online analytics processing or OLAP type of database with data warehouses you typically

7
00:00:34.050 --> 00:00:41.430
take your data from elsewhere and load it into the data warehouse and then you can perform your analytics

8
00:00:41.430 --> 00:00:43.080
on that data.

9
00:00:43.110 --> 00:00:50.700
So in the case of redshift we can see we have Amazon S3 you can also ingest from dynamoDB and

10
00:00:50.790 --> 00:00:58.830
you ingest that data into redshift and redshift has the concept of LIDAR nodes which coordinate the

11
00:00:58.830 --> 00:01:02.340
query execution and then the actual compute nodes.

12
00:01:02.340 --> 00:01:04.080
And there are two different types of compute nodes.

13
00:01:04.080 --> 00:01:08.670
There are dense compute and dense storage nodes so depends on your use case.

14
00:01:08.670 --> 00:01:10.040
Which ones you use.

15
00:01:10.230 --> 00:01:17.500
But the compute nodes actually perform the execution of the queries and they do so in parallel.

16
00:01:17.640 --> 00:01:26.180
You can take snapshots of your redshift database and you can also have those snapshots replicated across

17
00:01:26.180 --> 00:01:27.470
to another region.

18
00:01:27.470 --> 00:01:30.320
And that can happen automatically and we'll see that in the console

19
00:01:33.850 --> 00:01:35.700
redshift is not multi easy.

20
00:01:35.710 --> 00:01:39.270
So you can see here it's all fitting within one availability zone.

21
00:01:39.490 --> 00:01:45.190
But snapshots just like with EBS are actually stored on S3 which means they're within the region

22
00:01:45.250 --> 00:01:47.440
not within the availability zone.

23
00:01:47.560 --> 00:01:54.880
And that means if you want to back up your database and then restore it into another AZ you can do

24
00:01:54.880 --> 00:01:56.570
S3 snapshots.

25
00:01:57.740 --> 00:02:03.560
In terms of the exam you often see redshift coming out where there's a requirement for analyzing data

26
00:02:03.920 --> 00:02:08.610
in an analytics way using SQL clients or business intelligence tools.

27
00:02:08.660 --> 00:02:13.940
So you'll often see business intelligence tools mentioned and when you hear that you can start thinking

28
00:02:13.940 --> 00:02:15.590
about redshift.

29
00:02:15.590 --> 00:02:21.380
Another thing have redshift is you can use something called redshift spectrum and with redshift spectrum

30
00:02:21.440 --> 00:02:29.400
you don't need to load the data in to redshift from S3 you can actually query data directly on S3.

31
00:02:29.480 --> 00:02:35.240
So that's something that's possibly going to come up if you see spectrum that could relate to performing

32
00:02:35.240 --> 00:02:36.560
analytics on data.

33
00:02:36.590 --> 00:02:38.290
That's sitting in S3.

34
00:02:38.300 --> 00:02:43.100
Another thing to mention about redshift is that it uses columnar data storage.

35
00:02:43.160 --> 00:02:51.740
So that means data is stored sequentially in columns instead of rows using column based storage methods

36
00:02:51.770 --> 00:02:55.270
is ideal for warehousing and analytics it's much much faster.

37
00:02:55.280 --> 00:02:59.980
It requires fewer IOs and therefore is extremely fast.

38
00:03:01.050 --> 00:03:07.140
Additionally because of the way that the architecture is designed you have what's called massively parallel

39
00:03:07.140 --> 00:03:07.850
processing.

40
00:03:07.860 --> 00:03:15.330
So you can execute your processes on lots of nodes and you can have up to 128 of these nodes running

41
00:03:15.330 --> 00:03:18.210
in parallel executing against your data.

42
00:03:18.540 --> 00:03:24.180
So that's enough theory let's head over to the console and we'll just create a redshift cluster.

43
00:03:24.320 --> 00:03:31.830
So I'm in the AWS management console I can just search for redshift and it comes up there and you've

44
00:03:31.830 --> 00:03:37.740
now got a couple of options you can do a quick launch cluster you could open a query Ed which is if

45
00:03:37.740 --> 00:03:45.330
you have an existing database and it even shows you a visual method of working out the cluster configuration

46
00:03:45.330 --> 00:03:49.920
you want so you can go in here and say well you know this scale so big that you've got petabytes here

47
00:03:50.310 --> 00:03:51.080
so you could choose.

48
00:03:51.090 --> 00:03:56.220
You know I've had 500 petabytes that data and it's going to tell us what type of compute node we need

49
00:03:56.220 --> 00:03:58.080
to use and what the prices.

50
00:03:58.080 --> 00:03:59.700
And obviously that would be extremely costly.

51
00:03:59.710 --> 00:04:03.310
It would take a lot of hours to actually process that data.

52
00:04:03.690 --> 00:04:09.270
But if you come back down into the realms of the 500 terabytes then it drops right down.

53
00:04:09.570 --> 00:04:14.520
So now we go to a different type of instance which is going to run at two hundred and twenty dollars

54
00:04:14.520 --> 00:04:21.210
an hour to go down to gigabytes and just put it right the way down then you know the price just drops

55
00:04:21.210 --> 00:04:25.220
down to a very small amount now 66 cents an hour.

56
00:04:25.260 --> 00:04:31.380
So let's go in and obviously this might cost a few dollars to run but what I'm going to do is I'm going

57
00:04:31.380 --> 00:04:36.600
to choose the smallest instance available and in fact it tells you up here so as she tells you what

58
00:04:36.600 --> 00:04:41.580
the per hour rate is so you can see DC to large as the smallest one.

59
00:04:41.580 --> 00:04:44.460
I'm going to choose that and that's only 33 cents an hour.

60
00:04:44.550 --> 00:04:45.900
So that's pretty good.

61
00:04:45.900 --> 00:04:47.580
I'll go with two nodes.

62
00:04:47.580 --> 00:04:49.380
I'll leave the default name.

63
00:04:49.380 --> 00:04:51.120
I need to put a password in here

64
00:04:57.110 --> 00:04:59.220
and we need to choose an IAM role.

65
00:04:59.370 --> 00:05:02.140
Now what I should have done is set this up ahead of time.

66
00:05:02.150 --> 00:05:03.530
But let's go and do that now.

67
00:05:03.530 --> 00:05:12.470
So let's go into I am and I also have an article here and you can find this actually just by clicking

68
00:05:12.470 --> 00:05:18.730
back in redshift here you just click on learn more and it will open this article and this will tell

69
00:05:18.730 --> 00:05:20.300
you how to create this role.

70
00:05:20.500 --> 00:05:25.540
And all I'm going to do here is I'm just going to copy this name because I'm going to use the same name

71
00:05:25.540 --> 00:05:31.840
for the role that AWB has suggest but we just need to go in and choose the redshift service choose the

72
00:05:31.840 --> 00:05:39.490
use cases redshift customizable and then we're going to load permissions for S3 read only access so

73
00:05:39.490 --> 00:05:45.490
that we can ingest data if you wanted to use redshift spectrum which I mentioned before which is gives

74
00:05:45.490 --> 00:05:51.850
you the capability to analyze data directly in as free rather than ingesting it first you would then

75
00:05:51.850 --> 00:05:53.710
also need to give it some additional permissions.

76
00:05:53.710 --> 00:06:00.950
So you need to use glue AWB as glue or Amazon Athena a couple of other services for performing analytics.

77
00:06:01.300 --> 00:06:04.080
So let's just go and create this role.

78
00:06:04.150 --> 00:06:13.940
I mean I am I'm going to choose roles create role I'm going to select redshift and then the use cases

79
00:06:13.960 --> 00:06:17.220
redshift customizable click next.

80
00:06:17.410 --> 00:06:21.640
And what I want to do is just search for Amazon s3.

81
00:06:21.640 --> 00:06:29.100
I'm going to choose the read only access option and then go through to paste in the name suggested by AWS

82
00:06:29.100 --> 00:06:33.750
and we'll just create that role.

83
00:06:33.780 --> 00:06:34.950
Let's go back to redshift.

84
00:06:34.950 --> 00:06:36.340
I'm just gonna cancel out.

85
00:06:36.570 --> 00:06:39.330
So let's click quick launch cluster.

86
00:06:39.330 --> 00:06:41.760
I'm gonna re-enter that password

87
00:06:45.850 --> 00:06:47.850
and then we're going to choose our role.

88
00:06:48.220 --> 00:06:49.600
And that's what we need to do.

89
00:06:49.600 --> 00:06:51.340
We can now click on launch cluster

90
00:06:57.090 --> 00:07:00.780
and it tells us here a bit about setting up the query editor.

91
00:07:00.780 --> 00:07:04.940
Now we don't actually have any data so we're not going to set this up because we don't have anything

92
00:07:04.950 --> 00:07:06.390
to actually query.

93
00:07:06.390 --> 00:07:10.890
But if you wanted to set up the query editor you also need to setup some IAM policies to allow that

94
00:07:10.890 --> 00:07:18.830
access let's just choose to view clusters and we can see that the cluster status is creating.

95
00:07:19.060 --> 00:07:25.600
It's been a few minutes now and I just refreshed and the cluster is now available and healthy couple

96
00:07:25.600 --> 00:07:30.970
of things we can do at this level you can select the cluster you can see the end point and you can see

97
00:07:30.970 --> 00:07:37.510
some information about the cluster including the port and you can launch a new cluster obviously you

98
00:07:37.510 --> 00:07:45.610
can go to cluster and query modify resize delete reboot you can configure audit logging say encryption

99
00:07:45.610 --> 00:07:49.720
case we didn't enable encryption but if you had done you'd be able to work with the encryption keys

100
00:07:50.560 --> 00:07:57.770
and you can take a snapshot and configure your retention period and then also you can configure across

101
00:07:57.770 --> 00:07:58.850
region snapshots.

102
00:07:59.180 --> 00:08:05.390
So when you do this you can then go and select your region that you want to copy your snapshots to and

103
00:08:05.390 --> 00:08:12.290
redshift automatically copies the automated and manual snapshots to the destination region course you

104
00:08:12.290 --> 00:08:19.170
then incur the data transfer costs and the storage costs in the destination region you can then choose

105
00:08:19.170 --> 00:08:25.530
your retention period up to 35 days and that's the automated snapshots and then for the manual snapshots

106
00:08:25.530 --> 00:08:30.800
you can specify the total number of days that you want those to be kept full.

107
00:08:30.810 --> 00:08:36.650
So those are a few things you can do there and you can also then go and click on the Cluster again you

108
00:08:36.660 --> 00:08:43.410
at the end point here get some information about the cluster you've also got the JDBC and ODBC connectivity

109
00:08:43.410 --> 00:08:51.600
information here and underneath at the very bottom hey you can see the leader node and to compute nights.

110
00:08:51.600 --> 00:08:55.840
Remember there's always one leader and then you've got multiple compute nodes and the compute nodes

111
00:08:55.840 --> 00:09:05.020
are executing the data for analytics and you can see the public and private eyepiece for those under

112
00:09:05.020 --> 00:09:05.890
status.

113
00:09:05.890 --> 00:09:11.410
You can see some recent events and then you've got cluster performance and database performance so you

114
00:09:11.410 --> 00:09:13.600
can see a lot of information about what's going on.

115
00:09:13.600 --> 00:09:19.060
If you're running long jobs you're going to be watching in here to see what's going on how your clusters

116
00:09:19.060 --> 00:09:25.230
performing and whether you need to add additional nodes you can look at the queries that have been run

117
00:09:25.290 --> 00:09:27.900
the loads and the restores.

118
00:09:27.900 --> 00:09:33.030
Just go back up to this level we can resize the cluster.

119
00:09:33.030 --> 00:09:38.300
So in here you have two options you've got the elastic resize and you've got the classic resize and

120
00:09:38.370 --> 00:09:42.750
just cancel out of there and then on the left you have the query editor and if you select this it asks

121
00:09:42.750 --> 00:09:44.190
you for some credentials

122
00:09:49.110 --> 00:09:57.150
and you can then go look at Save queries snapshots that you've taken this workload management and you

123
00:09:57.150 --> 00:09:58.430
can reserve nodes as well.

124
00:09:58.430 --> 00:10:05.080
So like many other services you could go in here and select the type of node you want to reserve.

125
00:10:05.090 --> 00:10:08.720
And then as always it's one year or three years nothing in between.

126
00:10:08.720 --> 00:10:09.800
It's just one or three.

127
00:10:09.800 --> 00:10:14.780
Those are the options and then you can choose whether you want to pay all upfront no upfront or partial

128
00:10:14.780 --> 00:10:16.890
upfront and that affects your discounts.

129
00:10:17.300 --> 00:10:24.470
Amazon Redshift advisor can give you some tips on what you can do to improve your cluster performance

130
00:10:24.470 --> 00:10:27.210
and decrease costs.

131
00:10:27.220 --> 00:10:33.460
We then have the events here which is obviously this is across clusters whereas with you one within

132
00:10:33.460 --> 00:10:40.150
the cluster you just see the events for that specific cluster and then you can download drive us to

133
00:10:40.160 --> 00:10:46.260
JDBC and DBC as well so that's just a really quick overview of redshift.

134
00:10:46.290 --> 00:10:50.850
I think we need to go in any more detail or actually execute queries or anything like that and load

135
00:10:50.850 --> 00:10:55.020
data in because that's beyond the scope of the architecture exam.

136
00:10:55.020 --> 00:11:00.900
You really just need to understand this architecture so you need to know that it's that redshift is

137
00:11:00.900 --> 00:11:08.730
for analytics type workloads so overlap that is a data warehouse that it's going to execute long complex

138
00:11:08.730 --> 00:11:09.570
queries.

139
00:11:09.570 --> 00:11:11.960
That's the purpose of a data warehouse.

140
00:11:12.060 --> 00:11:18.960
The architecture includes a leader node and then compute nodes up to 128 of these and they run in parallel.

141
00:11:18.960 --> 00:11:24.330
You need to remember it's within an availability zone it's not multi AZ but you can take snapshots

142
00:11:24.750 --> 00:11:31.110
and then you could potentially have multiple redshift clusters running in parallel and different availability

143
00:11:31.110 --> 00:11:36.300
zones or you can just restore a redshift cluster that's failed or that needs to be moved for some reason

144
00:11:36.690 --> 00:11:43.320
into a never availability zone and then you also have cross region snapshots so you can safeguard your

145
00:11:43.320 --> 00:11:47.390
snapshots and move them out of the region as well for even extra redundancy.

146
00:11:47.700 --> 00:11:54.140
And the key thing to remember is whenever you see anything about business intelligence tools or analytics

147
00:11:54.140 --> 00:12:01.190
using MySQL then redshift is the way to go so let's just go back and we're just going to clean

148
00:12:01.190 --> 00:12:01.720
this up.

149
00:12:01.730 --> 00:12:08.360
So all I need to do is go in and delete cluster and I'm going to select no for snapshot and acknowledge

150
00:12:08.600 --> 00:12:09.780
and delete and that's it.

