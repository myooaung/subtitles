1
00:00:01,860 --> 00:00:02,370
Hi guys.

2
00:00:02,370 --> 00:00:08,350
In this lab we're going to explore scaling policies in a bit more detail for auto scaling groups.

3
00:00:08,490 --> 00:00:12,930
And we're going to watch how an auto scaling group reacts to load as well.

4
00:00:13,050 --> 00:00:13,770
So we.

5
00:00:14,010 --> 00:00:19,710
I've kept my configuration running from the previous lab so I have my auto scaling group in my elastic

6
00:00:19,710 --> 00:00:27,990
load balancer and if I go into scaling policies I can see that I have this target tracking scaling policy.

7
00:00:27,990 --> 00:00:29,440
Now with target tracking.

8
00:00:29,580 --> 00:00:37,440
It tries to maintain the average CPM utilization at the value specified which for me is 70 percent.

9
00:00:37,440 --> 00:00:41,690
Now there are two other types of scaling policy that we can use.

10
00:00:41,790 --> 00:00:48,820
And if I actually click ad policy here we get to choose from a simple scaling policy or a policy with

11
00:00:48,840 --> 00:00:57,750
steps now with a simple scaling policy the ISG will increase or decrease the capacity of the group based

12
00:00:57,750 --> 00:01:00,020
on a single scaling adjustment.

13
00:01:00,060 --> 00:01:09,930
Now AWB W.S. actually recommend that for most use cases you use a step scaling policy rather than a

14
00:01:09,930 --> 00:01:12,930
simple scaling policy with a step scaling policy.

15
00:01:12,930 --> 00:01:19,470
The ISG will increase or decrease the capacity of the group based on a set of scaling adjustments known

16
00:01:19,470 --> 00:01:24,390
as step adjustments and these vary based on the size of the alarm breach.

17
00:01:24,420 --> 00:01:27,840
So let's have a look at a scaling policy with steps.

18
00:01:27,840 --> 00:01:34,310
So I'm going to call this maintain load and what I need to do is specify an alarm.

19
00:01:34,320 --> 00:01:37,440
So execute the policy when I choose an alarm.

20
00:01:37,440 --> 00:01:38,860
So let's create an alarm.

21
00:01:39,120 --> 00:01:42,310
So I'm going to say and I don't need to send a notification.

22
00:01:42,510 --> 00:01:49,950
I'm going to say when the average speed or utilization hits 70 percent or greater for at least five

23
00:01:49,950 --> 00:01:50,780
minutes.

24
00:01:51,000 --> 00:01:57,420
And you can specify different values set off an alarm and then what we do is we come back and specify

25
00:01:57,420 --> 00:02:00,660
what happens when that alarm is executed.

26
00:02:00,660 --> 00:02:06,150
So we want to take the action of adding one instance and let's choose create.

27
00:02:06,330 --> 00:02:11,130
So what I can do now is I'm going to get rid of my target tracking scaling policy.

28
00:02:11,550 --> 00:02:18,360
And we just have this maintain load one which is going to set off an alarm when our C4 utilization hits

29
00:02:18,540 --> 00:02:20,490
70 percent or greater.

30
00:02:20,490 --> 00:02:23,190
So what we need to do now is actually generate some load.

31
00:02:23,820 --> 00:02:30,360
So what I'm going to do is go to instances and I'm going to take the IP address of my instance and I'm

32
00:02:30,360 --> 00:02:35,760
going to log on through secure shell and I'm going to install the stress utility again that we installed

33
00:02:36,090 --> 00:02:37,880
back a couple of sections ago.

34
00:02:37,920 --> 00:02:42,540
So I'm going to run this command which you can find in the course download and then actually install

35
00:02:42,540 --> 00:02:48,880
the stress utility I have stress running on both my instances now.

36
00:02:48,880 --> 00:02:54,820
So let's go back down to auto scaling groups and let's have a look under monitoring.

37
00:02:54,910 --> 00:03:00,520
Now you can enable group metric connection by choosing this option here and then we'll get some more

38
00:03:00,520 --> 00:03:01,990
statistics after a while.

39
00:03:02,290 --> 00:03:08,090
But what we can see now is just the minimum and maximum group size the desired capacity and so on.

40
00:03:08,140 --> 00:03:15,160
Another way we can monitor these instances is just going back up to easy to and choosing the monitoring

41
00:03:15,160 --> 00:03:21,220
tab and it's going to take a few minutes because we're using basic monitoring so we won't see the load

42
00:03:21,220 --> 00:03:25,020
being reflected in the metrics for a few minutes yet.

43
00:03:25,390 --> 00:03:31,000
So it's been a few minutes and I can now see that the load is certainly above 70 percent it's way over

44
00:03:31,000 --> 00:03:33,970
80 percent on instance one.

45
00:03:34,030 --> 00:03:40,150
And if I go to my other instance I can see it's also got the load going a bit higher but it's not quite

46
00:03:40,150 --> 00:03:42,200
reached plus 70 percent yet.

47
00:03:42,550 --> 00:03:48,280
So what we need to do is just keep an eye on it and might take a few more minutes for all the information

48
00:03:48,280 --> 00:03:54,250
to be received and then we should see a scaling event occur in a few more minutes and we can now see

49
00:03:54,250 --> 00:03:58,260
that both of instances are well above 80 percent.

50
00:03:58,270 --> 00:04:01,630
So one of them there is showing us as 100 percent.

51
00:04:01,630 --> 00:04:05,400
So we should see a scaling event happen any minute now.

52
00:04:05,560 --> 00:04:06,280
And there it is.

53
00:04:06,310 --> 00:04:09,850
We can now see an additional instance initializing.

54
00:04:09,850 --> 00:04:16,310
So let's go down to auto scaling groups and let's have a look at activity history.

55
00:04:16,390 --> 00:04:22,930
And here we go we have a new instance so it says launching a new easy to instance and it tells us that

56
00:04:22,930 --> 00:04:31,790
the alarm was triggered and it triggered the policy changing the design capacity from two to three we

57
00:04:31,800 --> 00:04:33,810
head over to monitoring.

58
00:04:33,810 --> 00:04:35,660
Again you can see some information here.

59
00:04:35,670 --> 00:04:39,240
You can see the design capacity of two has now gone up to free.

60
00:04:39,420 --> 00:04:42,870
And you can see the pending instances count has gone up to 1.

61
00:04:42,870 --> 00:04:43,560
So there we go.

62
00:04:43,560 --> 00:04:44,670
That's how it works.

63
00:04:44,670 --> 00:04:51,300
We can now see that we have a new instance being launched now of course another thing that an auto skating

64
00:04:51,300 --> 00:04:58,390
group will do is respond to failed easy two instances.

65
00:04:58,520 --> 00:05:03,450
So for instance what I could do is go in and terminate an instance.

66
00:05:03,530 --> 00:05:08,840
So let's just terminate one of these instances and that's going to shut down and then the auto scanning

67
00:05:08,840 --> 00:05:12,830
group is going to realize that it's lost an instance.

68
00:05:12,830 --> 00:05:19,590
So now it's going to have to replace that instance to get back up to the desired minimum.

69
00:05:19,610 --> 00:05:20,780
So that's being terminated.

70
00:05:20,810 --> 00:05:26,520
Let's head down to auto scaling groups and have a look at activity history.

71
00:05:26,720 --> 00:05:33,050
In fact what I'm going to do is go back up and terminate my up and running instance with the reason

72
00:05:33,050 --> 00:05:39,680
being that I want to make sure that we're under the threshold and we don't have to wait too long for

73
00:05:40,070 --> 00:05:48,230
the auto screening group to evaluate whether the load is above 80 percent or not or about 70 percent.

74
00:05:48,250 --> 00:05:53,070
So if I terminate this one we're now in the state where we've only got one running instance.

75
00:05:53,110 --> 00:05:57,700
So it's definitely going to have to relaunch at least one instance straight away.

76
00:05:57,720 --> 00:06:03,640
I'm gonna have a look here and we can see that an instance is being terminated and we now get the message

77
00:06:03,640 --> 00:06:06,880
saying that we're waiting for You'll Be connection draining.

78
00:06:06,880 --> 00:06:12,160
And so what this is this is probably the first instance where we can see that the instance was taken

79
00:06:12,160 --> 00:06:15,730
out of service but it's saying it was in response to an easy to health check.

80
00:06:15,760 --> 00:06:17,310
Well actually it was me terminating it.

81
00:06:17,320 --> 00:06:18,850
So that's not really the case.

82
00:06:18,940 --> 00:06:22,790
But the easy to health check did not come back as positive.

83
00:06:22,810 --> 00:06:28,450
So the auto scanning group is then trying to make sure that any existing connections are drained before

84
00:06:28,450 --> 00:06:30,090
it forces a termination.

85
00:06:30,100 --> 00:06:38,440
And now we have a message not yet in service for launching a new easy to instance so this was in response

86
00:06:38,470 --> 00:06:46,750
to a difference between the desired an actual capacity increasing the capacity from two to free.

87
00:06:46,750 --> 00:06:51,790
So now let's go back to instances and we can see we have a new instance launching.

88
00:06:51,790 --> 00:06:56,350
So that's just a simple lap guys to show you how things work.

89
00:06:56,350 --> 00:07:01,930
Now the other thing that I just want to show you before we finish this off is that we can still connect

90
00:07:01,930 --> 00:07:10,300
to the instances that are behind our life balance so let's go to load balancing let's choose to copy

91
00:07:10,360 --> 00:07:14,230
our DNS name and I'm going to close this tab and just reopen it.

92
00:07:15,770 --> 00:07:16,790
And then we'll put this in.

93
00:07:16,810 --> 00:07:17,690
And we've got Rosalind.

94
00:07:17,690 --> 00:07:24,530
So Rosalind will be the instance which we just launched a few minutes ago this one here.

95
00:07:24,530 --> 00:07:31,060
So this is the one that was launched in response to the load that we placed on the auto scanning group.

96
00:07:31,100 --> 00:07:34,450
We now have a second one initializing so that may or may not be ready.

97
00:07:34,490 --> 00:07:37,420
Let's have a look.

98
00:07:37,430 --> 00:07:41,960
We've just seen Rosalind at the moment so if we give that a minute we should then see that we can connect

99
00:07:41,960 --> 00:07:44,280
to our second instance as well.

100
00:07:44,330 --> 00:07:49,030
So I thought a couple of minutes and let's see if it's in service and it is we've got a destiny.

101
00:07:49,040 --> 00:07:50,150
How about that.

102
00:07:50,150 --> 00:07:56,420
So we can now cycle through Rosalyn and destiny and in actual fact while that was happening another

103
00:07:56,420 --> 00:07:57,880
instance was launched.

104
00:07:57,890 --> 00:08:02,540
So if we just head back to our auto scaling group we should get some more messages in here.

105
00:08:02,990 --> 00:08:08,020
So we had another instance which it then terminated and then launched a new one.

106
00:08:08,030 --> 00:08:14,360
Now of course I terminated both instances but the auto scanning group didn't receive the easy to status

107
00:08:14,360 --> 00:08:16,370
check that it wanted to.

108
00:08:16,370 --> 00:08:22,340
And that's why it then took the initiative to try and shut these down and then it launched two instances

109
00:08:22,340 --> 00:08:25,250
to replace the instances that I terminated.

110
00:08:25,250 --> 00:08:30,430
So we should now have a bunch of healthy instances behind our load balancer.

111
00:08:30,560 --> 00:08:36,440
And as you've seen the low balance off because it's connected to the auto scaling group automatically

112
00:08:36,440 --> 00:08:40,880
picks up that there's new instances and low balances to those instances.

113
00:08:40,880 --> 00:08:42,700
So that's it for this lab guys.

114
00:08:42,830 --> 00:08:47,680
What we're going to do now is just delete auto scaling group.

115
00:08:47,810 --> 00:08:50,320
So let's just go in and choose Delete.

116
00:08:50,450 --> 00:08:52,850
We can also delete our launch configuration

117
00:08:56,920 --> 00:09:06,230
we can go to load balances and Alito our load balancer and we can also delete our target creep and we'll

118
00:09:06,230 --> 00:09:08,630
just create another one when we need to.

119
00:09:08,630 --> 00:09:16,550
And then if you go back to instances just go in and terminate all of these instances and that's all

120
00:09:16,750 --> 00:09:17,710
cleaned up.

121
00:09:17,710 --> 00:09:23,410
What we're going to do in the next lab is I'm going to show you how cross own load balancing works and

122
00:09:23,410 --> 00:09:24,670
also sticky sessions.

