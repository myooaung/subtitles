WEBVTT
1
00:00:03.090 --> 00:00:08.610
Hi, guys, in this lecture, I'm going to cover Amazon, Athena and AWEX Blue, and it's also going

2
00:00:08.610 --> 00:00:13.890
to be followed up by a lab where I give you a brief demonstration of each of these products and they're

3
00:00:13.890 --> 00:00:18.540
together because they are closely related, which you'll see as I go through the lesson.

4
00:00:18.940 --> 00:00:24.870
Now, these are definitely being covered on the exam and a little bit more regularity so that they're

5
00:00:24.870 --> 00:00:28.100
starting to be seen a bit more often now in people's exam papers.

6
00:00:28.440 --> 00:00:30.040
So it could win you a few points.

7
00:00:30.040 --> 00:00:31.320
So it's definitely worth covering.

8
00:00:32.040 --> 00:00:39.840
So Amazon, Athena, is a service which you can use to perform queries on data on S3.

9
00:00:44.130 --> 00:00:50.040
So this is a super simple slide here, because these are the fundamental pieces of knowledge that you

10
00:00:50.040 --> 00:00:50.530
need to know.

11
00:00:50.550 --> 00:00:57.750
So we have an S free data link, so data sitting on is free and then we're able to query that data using

12
00:00:57.750 --> 00:01:02.370
a FENA and the queries I use in the structured query language, Eskew will.

13
00:01:06.030 --> 00:01:11.370
So a bit more background on Athena, it's an interactive query service that makes it easy to analyze

14
00:01:11.370 --> 00:01:14.640
data in S3 using standard escudo.

15
00:01:15.360 --> 00:01:18.360
It's Cervalis and you only pay for the queries that you run.

16
00:01:19.270 --> 00:01:26.920
It also uses a managed data catalog, so this is where Awassa glue comes in and the data catalog stores

17
00:01:26.920 --> 00:01:32.290
information and schemas about the databases and tables that you create, your data that is stored on

18
00:01:32.290 --> 00:01:32.770
S3.

19
00:01:32.890 --> 00:01:40.270
It uses Presto with standard SQL support and supports a variety of data formats, including CSP, Jason

20
00:01:40.600 --> 00:01:42.800
Orch, Apache, PAKHI and Avro.

21
00:01:43.000 --> 00:01:46.110
It's also optimized for fast performance with S3.

22
00:01:47.020 --> 00:01:50.530
So with a Fener there's no need for complex ECL jobs.

23
00:01:50.530 --> 00:01:52.840
So that's extract, transform and load.

24
00:01:53.290 --> 00:01:57.180
Now Excel is normally where you have heterogeneous databases.

25
00:01:57.190 --> 00:01:59.770
Maybe you've got a proprietary database technology.

26
00:02:00.220 --> 00:02:06.130
You need to pull some data out of that so you extract it, you then need to transform it so it's ready

27
00:02:06.130 --> 00:02:09.160
to be loaded into a different type of database.

28
00:02:09.700 --> 00:02:14.860
So ECL jobs are where you do that extract, transform and load now with a feeling you don't need to

29
00:02:14.860 --> 00:02:15.340
do that.

30
00:02:15.580 --> 00:02:22.140
It will actually query the data in place on free and therefore it makes it easy for anyone with standard

31
00:02:22.170 --> 00:02:27.150
S2 Eliscu skills to quickly analyze large scale data sets.

32
00:02:27.370 --> 00:02:29.640
So moving on to Adewusi glue.

33
00:02:29.860 --> 00:02:34.180
So remember, aqueous glue provides that data catalog service.

34
00:02:34.630 --> 00:02:35.620
So you have Amazon.

35
00:02:35.620 --> 00:02:41.500
Athena and Athena can query data using scale that's sitting on a data lake unnecessary.

36
00:02:42.160 --> 00:02:49.270
And then glue is able to also discover that data transform and prepare it for analytics so it can perform

37
00:02:49.270 --> 00:02:50.460
that ECL function.

38
00:02:50.570 --> 00:02:56.650
It will then provide a unified view of the data through the glue data catalog that Fener and other services

39
00:02:56.650 --> 00:02:57.230
can use.

40
00:02:57.340 --> 00:03:04.510
So Glew is a fully managed pay as you go ECL service that automates the steps of data preparation for

41
00:03:04.510 --> 00:03:05.290
analytics.

42
00:03:05.290 --> 00:03:11.500
It will automatically discover and profile data via the data catalog, the glue data catalog and recommends

43
00:03:11.500 --> 00:03:16.470
and generates ECL code to transform source data into the target schema.

44
00:03:16.570 --> 00:03:21.610
So it runs on a fully managed, scalable Apache Spark environment to load your data to its destination.

45
00:03:22.150 --> 00:03:28.390
And you simply point glue to your data stored on Adewusi and glue discovers data and stores the associated

46
00:03:28.390 --> 00:03:30.850
metadata in the glue data catalog.

47
00:03:30.850 --> 00:03:35.520
And then once it's catalog, the data is immediately searchable, credible and available for ECL.

48
00:03:36.280 --> 00:03:37.920
So it works with daylight's on.

49
00:03:37.930 --> 00:03:40.260
That's free, but it also works with data warehouses.

50
00:03:40.270 --> 00:03:42.850
So think about redshift and data stores.

51
00:03:42.850 --> 00:03:47.710
So think about AIDS or perhaps a database that's running on Amazon, etc..

52
00:03:47.860 --> 00:03:49.380
So that's really it for the ferry.

53
00:03:49.840 --> 00:03:55.660
We now have a lab where I'm going to take you into the console and I'm going to show you some simple

54
00:03:55.660 --> 00:04:01.690
tutorials on how to create some configurations in both Rafina and Golu as well.

