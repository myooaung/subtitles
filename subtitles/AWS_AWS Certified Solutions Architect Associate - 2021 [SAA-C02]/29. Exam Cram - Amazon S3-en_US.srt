1
00:00:02,150 --> 00:00:04,640
Welcome to the exam cram for Section seven.

2
00:00:04,640 --> 00:00:07,100
We're starting off with Amazon S3.

3
00:00:07,310 --> 00:00:12,950
So entries are distributed architecture and objects and redundantly stored on multiple devices across

4
00:00:12,950 --> 00:00:16,850
multiple facilities in an Amazon S3 region.

5
00:00:16,850 --> 00:00:22,850
It's a simple key based object store and uses a standard rest based interface.

6
00:00:22,850 --> 00:00:29,210
Your files can be zero bytes to five terabytes in size and the largest object that can be uploaded in

7
00:00:29,210 --> 00:00:36,320
a single put is five gigabytes if you use objects larger than one hundred Meg it's recommended to use

8
00:00:36,320 --> 00:00:47,350
the multiple upload the capability. Event notifications are a way that you can send triggers or alerts

9
00:00:47,950 --> 00:00:54,220
based on specific events that happen event notifications can be triggered for specific actions to send

10
00:00:54,220 --> 00:01:01,120
alerts or trigger actions in another service and they can be sent to SNS topics SQS queues, Lambda

11
00:01:01,120 --> 00:01:07,780
functions and you need to configure SQS queues and Lambda before you configure S3.

12
00:01:07,940 --> 00:01:14,490
There's no extra charges but you do pay for SQS queues and Lambda. S3 provides

13
00:01:14,530 --> 00:01:21,370
read after write consistency for puts of new objects and eventual consistency for overwrite puts and

14
00:01:21,370 --> 00:01:29,230
deletes so because the data is replicated across multiple Availability Zones.

15
00:01:29,380 --> 00:01:35,950
The eventual consistency just means that there's a little but a time delay where the data must be synchronized

16
00:01:35,950 --> 00:01:38,040
across those different copies of the data.

17
00:01:39,130 --> 00:01:46,670
So remember eventual consistency when you overwrite an object with a put or you delete an object as

18
00:01:46,820 --> 00:01:49,040
free data is made up of a key.

19
00:01:49,050 --> 00:01:55,640
That's the name of the object and then you have value which is the data version i.e. metadata and an

20
00:01:55,640 --> 00:01:57,680
ACL.

21
00:01:57,680 --> 00:02:03,530
There's a few extra capabilities about S3 that you need to know about so transfer acceleration

22
00:02:03,530 --> 00:02:10,070
speeds up uploads using cloud front so it's uploading data and it makes it faster to upload that data

23
00:02:10,820 --> 00:02:16,430
request space means that rather than the bucket owner paying for the requests and the data transfer

24
00:02:16,880 --> 00:02:23,120
the requester pays and that means another AWB account.

25
00:02:23,490 --> 00:02:29,820
You can assign tags you can trigger events as we talked about before and you can create a static website

26
00:02:29,850 --> 00:02:30,880
on S3.

27
00:02:30,960 --> 00:02:36,010
You can even use the BitTorrent protocol to retrieve publicly available objects.

28
00:02:36,030 --> 00:02:41,640
Now I don't go into these in more detail here because ESRI is a really big subject and go into detail

29
00:02:41,640 --> 00:02:47,220
on all of these would take us a long time but there is more information obviously on the IWC website

30
00:02:47,250 --> 00:02:55,350
but also on the training notes typical use cases for S3 include backup and storage application hosting

31
00:02:55,800 --> 00:03:00,020
media hosting software delivery and static websites.

32
00:03:00,060 --> 00:03:05,940
So an S3 bucket remember your files or your objects are stored in a bucket so it's a container for the

33
00:03:05,940 --> 00:03:07,260
objects.

34
00:03:07,260 --> 00:03:08,670
It's a flat container.

35
00:03:08,670 --> 00:03:12,320
You don't have a hierarchy so you have a single bucket and that's it.

36
00:03:12,360 --> 00:03:18,990
And then you put your objects inside the bucket but you can use a folder and that's essentially it.

37
00:03:19,170 --> 00:03:24,630
It's actually an object key name or prefix but it looks like a folder so it looks like you're creating

38
00:03:24,630 --> 00:03:27,510
a hierarchy as you would file system.

39
00:03:27,510 --> 00:03:34,000
You can have 100 buckets per account by default and you can store unlimited objects in your buckets.

40
00:03:34,050 --> 00:03:35,910
You can create folders as mentioned before.

41
00:03:35,940 --> 00:03:38,670
This is only available through the console.

42
00:03:38,670 --> 00:03:43,040
You can't nest buckets so it's just one bucket at the top level.

43
00:03:43,050 --> 00:03:48,270
You could also create lots of buckets but you can't have a bucket within a bucket and each bucket is

44
00:03:48,270 --> 00:03:49,740
created in a region.

45
00:03:49,740 --> 00:03:52,380
Again remember this one very important for the exam.

46
00:03:52,590 --> 00:03:54,020
S3 as a global service.

47
00:03:54,030 --> 00:03:57,540
So when you administer it you don't select a region it's just global.

48
00:03:57,540 --> 00:04:02,970
But when you create a bucket you specify a region and your data is held within that region and it's

49
00:04:02,970 --> 00:04:09,340
never replicated out of that region unless you create a replication configuration yourself.

50
00:04:09,390 --> 00:04:13,880
So each object is stored and retrieved using the key name.

51
00:04:13,880 --> 00:04:19,950
And so you can identify and connect to your objects using a service end point the bucket name the object

52
00:04:19,950 --> 00:04:23,450
key name and then potentially the object version as well.

53
00:04:23,550 --> 00:04:28,800
As mentioned before objects stored in a bucket never leave the region in which they're stored unless

54
00:04:28,800 --> 00:04:32,800
you move them to another region or you enable cross region replication.

55
00:04:32,850 --> 00:04:37,590
You can define permissions and objects when you upload them at any time afterwards using the console.

56
00:04:37,680 --> 00:04:38,670
Some resources.

57
00:04:38,670 --> 00:04:42,960
This is another one where I've got a lot more information in the training notes I don't go into too

58
00:04:42,960 --> 00:04:44,570
much detail right here.

59
00:04:44,580 --> 00:04:49,980
You do need to understand these though you need to understand lifecycle rules so make sure you understand

60
00:04:49,980 --> 00:04:51,570
lifecycle management.

61
00:04:51,630 --> 00:04:58,800
You need to understand that you can use S3 as a static website that you can enable versioning to

62
00:04:58,800 --> 00:05:04,960
keep multiple versions of an object access controls which control permissions to access the bucket.

63
00:05:04,980 --> 00:05:10,590
You need to understand those as well for the exam and then bucket policies which are more like an I

64
00:05:10,600 --> 00:05:17,280
am policy with Jason code and you apply those two buckets and control access then have cross origin

65
00:05:17,280 --> 00:05:21,550
resource sharing which can be enabled or restricted and logging.

66
00:05:21,630 --> 00:05:23,600
So definitely as I mentioned before.

67
00:05:23,670 --> 00:05:25,800
Check the training notes and our e-book.

68
00:05:25,800 --> 00:05:30,410
We've got lots more information about each of these so you can understand them at a deeper level.

69
00:05:30,450 --> 00:05:32,840
It'll be quite tedious to sort of go into it now.

70
00:05:33,240 --> 00:05:37,620
So I'd rather that you just go and have a look at the training nights and just read up on these and

71
00:05:37,620 --> 00:05:39,930
understand what they are for storage classes.

72
00:05:39,930 --> 00:05:45,960
There are six storage classes you got the standard the intelligent tiering which automatically moves

73
00:05:45,960 --> 00:05:52,710
data to the most cost effective tier you've got standard infrequently accessed you've got one zone II

74
00:05:53,070 --> 00:05:59,150
which is one zone infrequently accessed so only stores data in one availability zone.

75
00:05:59,310 --> 00:06:05,040
You've got glacier and then you've got deep Archive which is the version essentially is the original

76
00:06:05,040 --> 00:06:05,850
glacier.

77
00:06:05,940 --> 00:06:12,090
It's the one where you get the lowest cost and it takes at least 24 hours to get your data back whereas

78
00:06:12,090 --> 00:06:17,100
with S3 glacier it's a little bit more expensive but you can get your data back faster.

79
00:06:17,100 --> 00:06:21,660
This is a table from the AWS website and there's a few things you definitely need to know.

80
00:06:21,660 --> 00:06:23,850
So these are the different storage tiers.

81
00:06:23,850 --> 00:06:29,820
One thing that's easy to remember is that the durability is 11 nines for all of these tiers.

82
00:06:29,820 --> 00:06:35,950
So that really equates to the likelihood of AWS is actually losing an object so it's very low.

83
00:06:35,990 --> 00:06:41,340
It's very very resilient and then you have availability so this is really more about you being able

84
00:06:41,340 --> 00:06:47,820
to access your objects so not whether they're lost or not they're still on AWS but can you actually

85
00:06:47,820 --> 00:06:48,960
connect to them.

86
00:06:49,200 --> 00:06:56,090
And in this case we can see we have some differences so we have four nines for S3 standard for it.

87
00:06:56,310 --> 00:07:02,850
Also for S3 glacier and deep archive then we have three nines for these to the intelligent tearing in

88
00:07:02,850 --> 00:07:09,740
the standard infrequently access and we have ninety nine point five for one zone IA.

89
00:07:09,890 --> 00:07:17,430
There's even an SLA the availability zones is free for everyone except one zone IA.

90
00:07:17,640 --> 00:07:24,750
And then you do need to understand that there is a minimum capacity charge for some tier so for these tiers

91
00:07:25,320 --> 00:07:27,660
a minimum storage duration for those tiers.

92
00:07:27,660 --> 00:07:33,480
So basically for everything except S3 standard and there's also a retrieval fee that's associated

93
00:07:33,480 --> 00:07:35,740
with these four tiers here.

94
00:07:35,880 --> 00:07:39,150
Make sure you know those because they can come up in exam questions.

95
00:07:39,180 --> 00:07:45,000
There are quite a lot of exam questions on S3 so it is quite a big subject and you do need to really

96
00:07:45,000 --> 00:07:45,930
understand it well.

97
00:07:46,680 --> 00:07:53,340
So again multipart upload is where you have bigger uploads or bigger files that you need to upload and

98
00:07:53,340 --> 00:08:00,240
it will upload the objects into different in different parts in parallel and in a specific order and

99
00:08:00,240 --> 00:08:07,380
it's performed using the multi-part upload API. Recommended for objects of 100 MB or more and can be

100
00:08:07,380 --> 00:08:10,810
used for objects from five MB up to five terabytes.

101
00:08:10,830 --> 00:08:14,950
You absolutely have to use it if your objects are bigger than five gigabytes.

102
00:08:15,300 --> 00:08:21,210
Then we got S3 copy so this is for creating a copy of objects up to five given size in a single

103
00:08:21,210 --> 00:08:25,310
atomic operation for files bigger than five gig.

104
00:08:25,320 --> 00:08:31,560
You have to use multiple upload so you can copy files up to 5 gig but you have to use multipart upload

105
00:08:31,560 --> 00:08:39,630
for bigger than 5 gigabytes and for this you can use the SDK or the rest api so you can use the copy

106
00:08:39,630 --> 00:08:45,540
operation to create copies of objects rename objects change the storage class or encryption at rest

107
00:08:45,540 --> 00:08:54,510
status move them between locations or regions and change the object metadata so a bit more and transfer

108
00:08:54,510 --> 00:08:55,540
acceleration.

109
00:08:55,560 --> 00:09:01,530
This enables fast easy and secure transfers of files over long distances between your client and the

110
00:09:01,530 --> 00:09:04,680
S3 bucket and it does not using cloud front.

111
00:09:04,680 --> 00:09:08,170
So think about latency here and accelerating uploads.

112
00:09:08,230 --> 00:09:13,620
There's no difference in the security is just as secure as uploading directly to S3 and you're only

113
00:09:13,620 --> 00:09:19,620
charged if is a benefit in transfer times compared to just uploading straight to S3.

114
00:09:19,650 --> 00:09:24,330
You have to enable this on the bucket and you can then disable it you can only suspend it.

115
00:09:24,360 --> 00:09:25,830
Let's have a look at encryption.

116
00:09:25,830 --> 00:09:28,470
There are four key encryption options.

117
00:09:28,470 --> 00:09:36,600
You've got the server side encryption S3 the server side encryption with client provided keys the

118
00:09:36,600 --> 00:09:42,450
server side encryption with KMS provided keys and then the client side where everything's encrypted

119
00:09:42,570 --> 00:09:45,640
outside of AWS. For S3 performance

120
00:09:45,730 --> 00:09:51,960
AWS recommend that you measure your performance that you scale your storage connections horizontally.

121
00:09:51,960 --> 00:09:59,580
You use byte range fetches and you use retry of requests for licensee sensitive applications and you

122
00:09:59,580 --> 00:10:05,360
can combine S3 and EC2 in the same AWS region so that's what you should do.

123
00:10:05,370 --> 00:10:10,890
Also it's recommended to use transfer acceleration to minimize latency caused by distance.

124
00:10:10,950 --> 00:10:16,470
Now again this is one where there's quite a lot of information to read up on here to understand the

125
00:10:16,470 --> 00:10:17,180
difference.

126
00:10:17,310 --> 00:10:19,260
Best Practices for performance.

127
00:10:19,260 --> 00:10:22,260
Please check the training notes and the AWS documentation.

128
00:10:22,260 --> 00:10:23,400
I'll put a link in.

129
00:10:23,520 --> 00:10:29,940
There's definitely exam questions coming up asking about bite range fetches or where the answer is to

130
00:10:29,940 --> 00:10:31,250
use byte range feature.

131
00:10:31,260 --> 00:10:33,010
So you need to understand what that is.

132
00:10:33,060 --> 00:10:38,580
It's really just a way of creating multiple parallel requests for your data to improve performance.

