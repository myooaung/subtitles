WEBVTT
1
1

00:00:00.790  -->  00:00:04.715
Hey guys welcome to Lecture 35.
2

2

00:00:04.715  -->  00:00:08.618
This is gonna be about
Duplicates in DataFrames.
3

3

00:00:12.054  -->  00:00:13.430
All right.
4

4

00:00:13.430  -->  00:00:15.760
So import the usual.
5

5

00:00:15.760  -->  00:00:16.754
NumPy is np.
6

6

00:00:18.418  -->  00:00:27.146
Pandas as pd and then from pandas
we'll import Series and DataFrame.
7

7

00:00:27.146  -->  00:00:31.700
All right, so let's go ahead and
make a data frame.
8

8

00:00:32.820  -->  00:00:36.974
We'll call it dframe and we're gonna
make it with duplicates on purpose.
9

9

00:00:38.255  -->  00:00:39.435
So we'll have data frame.
10

10

00:00:40.905  -->  00:00:44.395
We're gonna pass a dictionary,
so we'll just say key one.
11

11

00:00:48.022  -->  00:00:55.189
And then we'll have it be
let's say We'll pass A and
12

12

00:00:55.189  -->  00:00:59.968
we'll have two instances of A.
13

13

00:00:59.968  -->  00:01:05.258
And then we'll also add three
14

14

00:01:05.258  -->  00:01:11.190
instances of B, how about that?
15

15

00:01:11.190  -->  00:01:12.900
Whoops, that should be a comma.
16

16

00:01:14.420  -->  00:01:19.330
And then we'll make A second key and
17

17

00:01:19.330  -->  00:01:25.960
we'll pass just a list,
which is kind of the reverse.
18

18

00:01:25.960  -->  00:01:31.510
So this is different ways of making it
a list key, so this instance I just
19

19

00:01:31.510  -->  00:01:36.770
multiplied a by two to get AA and
then I added three Bs on top of that.
20

20

00:01:36.770  -->  00:01:41.210
Here I just wrote out manually 2,
2, 2, 3, 3, 3.
21

21

00:01:41.210  -->  00:01:44.100
Okay, and then let's go ahead and
show that data frame.
22

22

00:01:46.380  -->  00:01:50.438
Nice, so we have key1, key2,
letter AA, and then BBB and
23

23

00:01:50.438  -->  00:01:55.040
then key1 and key2, and
their values that match up.
24

24

00:01:55.040  -->  00:01:58.360
So, how can we find duplicates?
25

25

00:01:58.360  -->  00:02:00.670
It's actually really easy with pandas.
26

26

00:02:00.670  -->  00:02:07.360
We can pass the term duplicated,
with no additional arguments and
27

27

00:02:07.360  -->  00:02:11.720
it'll tell us if rows are duplicates.
28

28

00:02:11.720  -->  00:02:12.720
So for instance,
29

29

00:02:14.020  -->  00:02:18.305
that first row is gonna yield you a false
since it hasn't seen duplicate yet.
30

30

00:02:18.305  -->  00:02:19.840
A2.
31

31

00:02:19.840  -->  00:02:24.600
And then the second row, the index one
row, it's gonna say that it is duplicated
32

32

00:02:24.600  -->  00:02:30.180
true because it's previous
row was also key1 A key2 2.
33

33

00:02:30.180  -->  00:02:34.955
And if we look at the very last row,
B3, it's also
34

34

00:02:34.955  -->  00:02:40.335
true duplicated because it's
the second instance of the B and 3 row.
35

35

00:02:40.335  -->  00:02:42.595
So that's how you can find duplicates.
36

36

00:02:42.595  -->  00:02:44.499
Let's say you wanted to get
rid of those duplicates.
37

37

00:02:45.950  -->  00:02:52.540
You can drop them, and you can just
simply pass the method drop_duplicates
38

38

00:02:52.540  -->  00:02:56.820
with no additional argument, and
it'll get rid of any duplicates it found.
39

39

00:02:56.820  -->  00:03:01.830
So now we only have those false values
from before, the zero-index row,
40

40

00:03:01.830  -->  00:03:04.440
and then the second and
third index row, zero, two, three.
41

41

00:03:04.440  -->  00:03:06.930
And now we have those rows.
42

42

00:03:07.930  -->  00:03:09.640
So we were able to drop the duplicates.
43

43

00:03:09.640  -->  00:03:12.670
You can also filter out which
duplicates to drop by a single column.
44

44

00:03:13.760  -->  00:03:21.438
The way to do that is again just
say dframe_drop_duplicates,
45

45

00:03:21.438  -->  00:03:28.417
and you can pass the column name
that you want to drop from.
46

46

00:03:31.275  -->  00:03:34.146
So now I drop duplicates specifically
from the key1 column, so
47

47

00:03:34.146  -->  00:03:38.041
that means anything that was duplicate, as
far as the letters, has now been dropped.
48

48

00:03:39.600  -->  00:03:43.080
So, anything that had an additional
A after the first one, or
49

49

00:03:43.080  -->  00:03:45.790
an additional B after the first
B showed up, has been dropped.
50

50

00:03:45.790  -->  00:03:49.810
So, you only end up being left
with the zero and two index rows.
51

51

00:03:49.810  -->  00:03:52.250
So, that makes sense.
52

52

00:03:53.920  -->  00:03:56.775
Let's go ahead and
show the original dframe.
53

53

00:03:58.482  -->  00:04:03.060
All right, by default the first value
was taken for the duplicates, but
54

54

00:04:03.060  -->  00:04:05.880
we can also take the last value instead.
55

55

00:04:05.880  -->  00:04:11.485
To show you what I mean by that, we can
say dframe, let's bring this to center,
56

56

00:04:11.485  -->  00:04:20.015
dframe.drop_duplicates we'll
pass it on key1.
57

57

00:04:23.200  -->  00:04:28.450
So, if you notice last time,
we just took the first instance and
58

58

00:04:28.450  -->  00:04:32.250
then dropped later duplicates.
59

59

00:04:32.250  -->  00:04:35.850
Let's say, instead of doing that,
we wanted to take the very last duplicate.
60

60

00:04:37.140  -->  00:04:44.630
So we can do that by saying,
passing the argument take_last=True.
61

61

00:04:44.630  -->  00:04:47.210
So what do we expect to
happen in this case?
62

62

00:04:47.210  -->  00:04:50.690
So that means the very last time,
since we're calling it by key1,
63

63

00:04:50.690  -->  00:04:53.290
that it sees the duplicate it'll keep it.
64

64

00:04:53.290  -->  00:04:55.418
So before we press enter,
let's go ahead and
65

65

00:04:55.418  -->  00:04:58.320
try to imagine what the result's gonna be.
66

66

00:04:58.320  -->  00:05:04.150
If we look at key1,
the last time an A shows up is in index 1,
67

67

00:05:04.150  -->  00:05:06.840
so it's gonna keep that row.
68

68

00:05:06.840  -->  00:05:11.740
And if we keep going down key1,
the last time a duplicate of B shows up
69

69

00:05:11.740  -->  00:05:16.790
is at the fourth indexed row, so
we should expect rows 1 and 4 to be kept.
70

70

00:05:16.790  -->  00:05:21.140
Since we're taking the last setting
equal to true, let's see if that works.
71

71

00:05:21.140  -->  00:05:22.440
And that's what we got.
72

72

00:05:22.440  -->  00:05:25.370
We got the first and fourth index rows.
73

73

00:05:25.370  -->  00:05:26.663
Awesome.
74

74

00:05:26.663  -->  00:05:28.775
So that's about it for finding duplicates.
75

75

00:05:28.775  -->  00:05:29.799
It's pretty straightforward.
76

76

00:05:29.799  -->  00:05:34.873
You just pass the method either
duplicated to find the Boolean value if
77

77

00:05:34.873  -->  00:05:40.033
they're duplicates or not, and
you can just say drop_duplicates and
78

78

00:05:40.033  -->  00:05:43.685
you can pass specific
columns as an argument.
79

79

00:05:43.685  -->  00:05:48.455
And you can also pass take_last=True if
you wanna get the last duplicate value.
80

80

00:05:48.455  -->  00:05:49.845
That's about it for duplicates.
81

81

00:05:49.845  -->  00:05:52.045
Next up, we're gonna learn about mapping.
82

82

00:05:52.045  -->  00:05:52.715
I'll see you there.
