1
1

00:00:01,880  -->  00:00:03,830
Hello everyone, and
welcome to the next video in
2

2

00:00:03,830  -->  00:00:06,950
the Supervised Learning
Linear Regression Series.
3

3

00:00:06,950  -->  00:00:09,870
So in this video we're gonna be
doing is I'm gonna hop over back
4

4

00:00:09,870  -->  00:00:12,800
to the completed IPython Notebook.
5

5

00:00:12,800  -->  00:00:16,880
So we set up our data frame in the last
video with all different coefficients for
6

6

00:00:16,880  -->  00:00:19,830
that linear regression we
made using scikit-learn.
7

7

00:00:19,830  -->  00:00:23,130
So we did a multi-variable
linear regression.
8

8

00:00:23,130  -->  00:00:27,910
And now what we're gonna do is move on to
step 7, using training and validation.
9

9

00:00:27,910  -->  00:00:31,770
So, usually in a data set,
what you're gonna do, especially for
10

10

00:00:31,770  -->  00:00:35,950
our regression model, is is have
a training set implemented to build
11

11

00:00:35,950  -->  00:00:40,070
up the model and then a validation
set you use to validate the model.
12

12

00:00:41,170  -->  00:00:46,230
So usually you don't use the entire
data set to do your analysis.
13

13

00:00:46,230  -->  00:00:49,880
You're gonna split it up
into mostly training and
14

14

00:00:49,880  -->  00:00:52,370
then some testing or validation points.
15

15

00:00:52,370  -->  00:00:56,360
So the correct way to do this to
pick out your samples that for
16

16

00:00:56,360  -->  00:00:59,800
testing and training is to do it randomly.
17

17

00:00:59,800  -->  00:01:03,946
Fortunately, scikit-learn has
a built in function specifically for
18

18

00:01:03,946  -->  00:01:06,050
this called train_test_split.
19

19

00:01:06,050  -->  00:01:10,430
So what it does is it takes in your data
and it splits it into a training set and
20

20

00:01:10,430  -->  00:01:11,752
a testing set.
21

21

00:01:11,752  -->  00:01:14,720
So you'll run your analysis
on your training set and
22

22

00:01:14,720  -->  00:01:18,520
see how well it matches up with
your testing or validation set.
23

23

00:01:18,520  -->  00:01:23,299
So, this is how you can tell how good
your regression and your model was.
24

24

00:01:24,940  -->  00:01:32,000
So the parameters passed are your x and y,
and then, optionally, you can also pass
25

25

00:01:32,000  -->  00:01:37,250
a test size parameter as far as the train
test split method and scikit-learn.
26

26

00:01:37,250  -->  00:01:41,210
And what the test size parameter does is
it represents the portion of the data set
27

27

00:01:41,210  -->  00:01:44,410
to include in the test split,
so that test chunk.
28

28

00:01:44,410  -->  00:01:48,230
And then you can also either
do a train size parameter,
29

29

00:01:48,230  -->  00:01:52,400
which is how much of the data do
you want to be the training set.
30

30

00:01:52,400  -->  00:01:56,470
So let's say you had a data set of
100 points, you could set, hey,
31

31

00:01:56,470  -->  00:02:00,110
I want my testing size to
be 70 of those points, and
32

32

00:02:00,110  -->  00:02:02,930
I want my training size to be 30
of those points, or vice versa.
33

33

00:02:02,930  -->  00:02:05,626
You want a large training
set to be 70 points, and
34

34

00:02:05,626  -->  00:02:07,580
then a testing set to be 30 points.
35

35

00:02:07,580  -->  00:02:13,780
Usually you want your training set to be
as large as you reasonably can have it and
36

36

00:02:13,780  -->  00:02:17,580
your testing set to be large enough
that you feel validated with your model.
37

37

00:02:18,670  -->  00:02:20,172
So let's go ahead and
jump into the codes so
38

38

00:02:20,172  -->  00:02:22,194
we can get a better idea of
what this actually looks like.
39

39

00:02:24,551  -->  00:02:27,802
And again, there's also a link to
the documentation about the different
40

40

00:02:27,802  -->  00:02:30,750
parameters you can check out as far
as the train test split method.
41

41

00:02:32,330  -->  00:02:37,563
So what we're gonna do is we're gonna say,
we're gonna call sklearn.
42

42

00:02:41,138  -->  00:02:46,556
Sklearn and
then we're gonna call the cross
43

43

00:02:46,556  -->  00:02:51,829
validation library from scikit-learn,
44

44

00:02:51,829  -->  00:02:58,712
and this is used to do our
train test split analysis, and
45

45

00:02:58,712  -->  00:03:06,880
then the data we wanna do this with
is that x data we made earlier.
46

46

00:03:06,880  -->  00:03:07,690
Remember that array?
47

47

00:03:07,690  -->  00:03:10,470
You can go ahead and
check back to the previous videos.
48

48

00:03:12,420  -->  00:03:16,300
And then the next data set we
want is actually the target.
49

49

00:03:16,300  -->  00:03:20,410
So that was just the price.
50

50

00:03:23,130  -->  00:03:27,360
So what this will do is scikit-learn is
going to take all your X information, all
51

51

00:03:27,360  -->  00:03:31,729
your y information, and in this case, it's
gonna be the multi-variable information,
52

52

00:03:31,729  -->  00:03:36,090
and then split it up into
a training set and a testing set.
53

53

00:03:37,310  -->  00:03:40,050
And the way it's gonna output this
is it's gonna output four things.
54

54

00:03:40,050  -->  00:03:44,120
It's gonna output your X training and
your X testing, and
55

55

00:03:44,120  -->  00:03:46,820
then your Y training and your Y testing.
56

56

00:03:46,820  -->  00:03:50,820
So what we're gonna do is
now set up the outputs.
57

57

00:03:50,820  -->  00:03:55,160
So we did this a little backwards just
to show you how to construct this.
58

58

00:03:56,720  -->  00:03:58,981
But this is the order
it spits them out in.
59

59

00:03:58,981  -->  00:04:02,604
And you can always check the documentation
in case you're confused.
60

60

00:04:09,081  -->  00:04:10,200
Okay, great.
61

61

00:04:10,200  -->  00:04:15,440
So, what we've done here is we've split
up all our data into a training set and
62

62

00:04:15,440  -->  00:04:20,350
a testing set, and there's an X and
a Y for each of those respectively.
63

63

00:04:20,350  -->  00:04:23,550
So let's go ahead and see what
the output of the train test split was.
64

64

00:04:25,950  -->  00:04:27,330
And I'll do that just
with a print command.
65

65

00:04:29,550  -->  00:04:34,458
So what I'm gonna do is I'm
gonna check the shape of each of
66

66

00:04:34,458  -->  00:04:36,771
those that I just printed.
67

67

00:04:36,771  -->  00:04:40,098
Or just set,
depending on how you wanna think about it.
68

68

00:04:40,098  -->  00:04:42,514
Shape, and then for the Ys.
69

69

00:04:51,615  -->  00:04:55,629
Okay, so I'm gonna print the shape
of my X training and my X testing,
70

70

00:04:55,629  -->  00:04:57,930
and then my Y training and my Y testing.
71

71

00:05:00,210  -->  00:05:06,530
So you can see here that
the majority of the data is
72

72

00:05:06,530  -->  00:05:11,500
in the training sets so 379 points are in
the X training set and the Y training set.
73

73

00:05:11,500  -->  00:05:14,720
And then 127 points are in
the testing data set.
74

74

00:05:14,720  -->  00:05:18,665
So usually you want as much of your
data into the training set, and
75

75

00:05:18,665  -->  00:05:22,590
scikit-learn does this automatically for
you.
76

76

00:05:22,590  -->  00:05:27,360
It chooses what it thinks is the best
split, but you can go ahead as you learn
77

77

00:05:27,360  -->  00:05:31,440
more about machine learning, or have
a particular data set you wanna work with.
78

78

00:05:31,440  -->  00:05:35,430
You can decide how you
want that split to occur.
79

79

00:05:35,430  -->  00:05:38,830
But again, I encourage you to check
out that link to the documentation
80

80

00:05:38,830  -->  00:05:40,290
in your IPython Notebook.
81

81

00:05:40,290  -->  00:05:44,000
So moving on, now that we have
our training and testing sets,
82

82

00:05:44,000  -->  00:05:48,248
we can continue to predict the prices
based on the multiple of variables.
83

83

00:05:48,248  -->  00:05:55,129
So in this next section, step 8,
we're going to predict prices.
84

84

00:05:55,129  -->  00:05:57,637
So we have our training and testing sets.
85

85

00:05:57,637  -->  00:06:00,770
Let's go ahead and
try to predict house prices with them.
86

86

00:06:00,770  -->  00:06:04,050
So we'll use our training set for
87

87

00:06:04,050  -->  00:06:08,370
the prediction and then use our testing
set for validation of that prediction.
88

88

00:06:08,370  -->  00:06:11,194
So let's go ahead and
create a regression object again.
89

89

00:06:13,548  -->  00:06:15,196
And we do this just by, whoops.
90

90

00:06:18,610  -->  00:06:21,395
Calling that linear regression
that we imported earlier.
91

91

00:06:25,296  -->  00:06:28,090
And then once again,
I'm gonna do a linear regression.
92

92

00:06:28,090  -->  00:06:29,129
So I'm gonna do a fit.
93

93

00:06:35,922  -->  00:06:40,182
But in this case,
I'm going to pass my training set first.
94

94

00:06:40,182  -->  00:06:42,225
[COUGH] Excuse me.
95

95

00:06:50,422  -->  00:06:53,070
Okay, so I'm fitting my training
data to this linear regression.
96

96

00:06:54,380  -->  00:06:55,260
So let's go ahead and do that.
97

97

00:06:56,710  -->  00:06:58,250
All right, fantastic.
98

98

00:06:58,250  -->  00:07:04,190
Now I'm going to run what's called a
prediction on both the X training set and
99

99

00:07:04,190  -->  00:07:04,810
the testing set.
100

100

00:07:06,240  -->  00:07:09,480
So the way we can do that is just
by calling a prediction method.
101

101

00:07:14,739  -->  00:07:15,866
On my X train.
102

102

00:07:18,849  -->  00:07:24,675
And I'm gonna set that equal to Something,
103

103

00:07:24,675  -->  00:07:28,650
just an object called PRED for
prediction_train.
104

104

00:07:28,650  -->  00:07:33,120
So that's the prediction
from my X training sets, and
105

105

00:07:33,120  -->  00:07:37,510
then I'm gonna set a prediction for
my testing set.
106

106

00:07:39,730  -->  00:07:43,900
So we're gonna be using these four is to,
if we run a prediction with my training
107

107

00:07:43,900  -->  00:07:45,990
set, how well does it
match with my testing set?
108

108

00:07:48,440  -->  00:07:50,320
And hopefully the numbers
will be pretty close.
109

109

00:07:51,550  -->  00:07:52,217
All right.
110

110

00:07:52,217  -->  00:07:54,888
So if you remember, we've already
learned how to calculate the error.
111

111

00:07:54,888  -->  00:07:59,013
We want the mean squared error, or
sometimes the root mean squared error.
112

112

00:07:59,013  -->  00:08:01,146
Depending how you wanna calculate it.
113

113

00:08:01,146  -->  00:08:03,950
So this case,
we'll just take the mean square error.
114

114

00:08:05,150  -->  00:08:09,504
So what I'm gonna do here is
just print out a statement.
115

115

00:08:09,504  -->  00:08:14,894
So I'm gonna say, fit a model x_train,
116

116

00:08:14,894  -->  00:08:19,822
so fit a model with my x training sets,
117

117

00:08:19,822  -->  00:08:24,915
and calculate the mean squared error.
118

118

00:08:24,915  -->  00:08:28,141
In this case I'm not gonna do the root
mean squared error, it's up to you,
119

119

00:08:28,141  -->  00:08:30,260
depending how you want
to evaluate your errors.
120

120

00:08:30,260  -->  00:08:32,760
But in this case it
probably makes a little
121

121

00:08:32,760  -->  00:08:34,390
more sense to use the mean squared error.
122

122

00:08:34,390  -->  00:08:39,810
And you'll see why in a second
when there see the actual results.
123

123

00:08:39,810  -->  00:08:42,960
And calculate the mean square
error with your Y training set.
124

124

00:08:45,330  -->  00:08:51,625
In this case I will- Call
a floating point there.
125

125

00:08:54,621  -->  00:08:56,880
And so
what do I actually wanna print out here?
126

126

00:08:56,880  -->  00:09:00,097
Well, I wanna print out
the mean squared error.
127

127

00:09:00,097  -->  00:09:06,494
So I wanna take the difference
between my Y training and
128

128

00:09:06,494  -->  00:09:12,764
that prediction I made
earlier from my training set.
129

129

00:09:12,764  -->  00:09:17,352
So I wanna take that, and
then what I wanna do is square it, and
130

130

00:09:17,352  -->  00:09:19,925
then I wanna take the mean of that.
131

131

00:09:19,925  -->  00:09:24,080
So what I can do is use NumPy's
mean method to do that,
132

132

00:09:24,080  -->  00:09:27,670
np.mean.
133

133

00:09:27,670  -->  00:09:28,845
Okay.
134

134

00:09:28,845  -->  00:09:32,008
So I'm sorry I'm going off
a little bit off the cell here but
135

135

00:09:32,008  -->  00:09:35,173
I'm taking the difference
between my Y training object and
136

136

00:09:35,173  -->  00:09:39,430
my prediction training object, squaring
it, and then figuring out the mean.
137

137

00:09:39,430  -->  00:09:42,660
And then you could square root that if
you wanted, the root mean square error.
138

138

00:09:42,660  -->  00:09:45,420
But in this case it's not necessary.
139

139

00:09:48,070  -->  00:09:56,165
And then the next thing I wanna do is fit
a model with my training data set and
140

140

00:09:56,165  -->  00:10:00,094
calculate the mean squared error.
141

141

00:10:03,497  -->  00:10:09,044
With my X testing sets and
my Y testing set.
142

142

00:10:13,341  -->  00:10:19,710
And I'll leave that up to two places past
the decimal and so now what do I want?
143

143

00:10:19,710  -->  00:10:23,950
So now we're gonna do basically
the exact same thing we did earlier
144

144

00:10:23,950  -->  00:10:27,080
except we're gonna do it
with the Y testing data and
145

145

00:10:27,080  -->  00:10:33,070
my linear regression prediction from
my testing on the X testing data so
146

146

00:10:33,070  -->  00:10:37,435
that pred_test object we made earlier.
147

147

00:10:37,435  -->  00:10:41,596
So I want my Y test data
148

148

00:10:45,426  -->  00:10:46,970
Minus that prediction test data.
149

149

00:10:51,054  -->  00:10:52,253
I'm gonna square it,
150

150

00:11:00,177  -->  00:11:02,050
And then I'm gonna grab the mean.
151

151

00:11:02,050  -->  00:11:04,530
So let's go ahead and
break down real quick before I print this.
152

152

00:11:04,530  -->  00:11:07,432
Well, let me print it, and then I'll break
it down, just so it's easier to read.
153

153

00:11:09,595  -->  00:11:10,095
Okay.
154

154

00:11:12,110  -->  00:11:14,886
So let's go ahead and
break down what we just did here.
155

155

00:11:14,886  -->  00:11:20,100
We took the scikit-learn's ability
156

156

00:11:20,100  -->  00:11:23,470
to split our data into a training set and
a testing set.
157

157

00:11:23,470  -->  00:11:26,760
So this is always really important
to do in your regression models.
158

158

00:11:28,070  -->  00:11:32,430
And then we printed out the shape
to see what scikit-learn
159

159

00:11:32,430  -->  00:11:33,830
decided how to split up your data.
160

160

00:11:33,830  -->  00:11:37,460
So you have your X_training and
your X_testing.
161

161

00:11:37,460  -->  00:11:39,860
Then we made a linear regression object.
162

162

00:11:39,860  -->  00:11:44,310
We fit our training sets X and
Y train using the same methods we
163

163

00:11:44,310  -->  00:11:48,080
described in previous lectures,
and then we ran a prediction.
164

164

00:11:48,080  -->  00:11:52,980
We ran one linear regression prediction
using the training of the X data set.
165

165

00:11:52,980  -->  00:11:58,060
And then we ran another prediction
using the testing set and
166

166

00:11:58,060  -->  00:12:01,227
we set these as objects pred_train,
pred_test.
167

167

00:12:03,310  -->  00:12:08,413
And then we made a little print statement
here for the mean squared error,
168

168

00:12:08,413  -->  00:12:13,490
which is your Y training
minus your prediction.
169

169

00:12:13,490  -->  00:12:14,417
So remember,
170

170

00:12:14,417  -->  00:12:19,450
this is just basically the length of that
D line that we talked about earlier.
171

171

00:12:19,450  -->  00:12:24,170
So if I go back to the IPython Notebook
here and scroll up a bit.
172

172

00:12:27,431  -->  00:12:32,267
Remember this D object here, the length
of that line between your data and
173

173

00:12:32,267  -->  00:12:34,630
that best fit curve?
174

174

00:12:34,630  -->  00:12:35,554
That's what this is here.
175

175

00:12:36,875  -->  00:12:41,205
It's basically, what was
the difference between your prediction
176

176

00:12:41,205  -->  00:12:45,889
on that training set and
that actual Y training value?
177

177

00:12:45,889  -->  00:12:49,860
And then square it, and find the average.
178

178

00:12:49,860  -->  00:12:50,367
And remember,
179

179

00:12:50,367  -->  00:12:52,902
you could also take the square root if
you wanted the root mean squared error of
180

180

00:12:52,902  -->  00:12:53,838
this, like we did earlier.
181

181

00:12:56,012  -->  00:12:58,672
And now,
we also wanna do the same thing, but
182

182

00:12:58,672  -->  00:13:02,670
we wanna do it with the testings data
set and see how well they match.
183

183

00:13:04,520  -->  00:13:09,180
So, we took our Y testing,
did a prediction on that testing data and
184

184

00:13:09,180  -->  00:13:10,320
did the exact same thing.
185

185

00:13:11,970  -->  00:13:15,830
So in this case with
this particular split,
186

186

00:13:15,830  -->  00:13:21,250
it looks like we're not super close but
we're also not super far off.
187

187

00:13:21,250  -->  00:13:26,790
And the question is how do you actually
know how significant this number is?
188

188

00:13:28,260  -->  00:13:31,380
Is there a way to visualize how close
189

189

00:13:31,380  -->  00:13:35,050
our prediction actually is between
our training set and our testing set?
190

190

00:13:35,050  -->  00:13:39,790
Because these numbers can be a bit
nebulous if you're in different fields.
191

191

00:13:39,790  -->  00:13:44,190
So what might be a good training and
testing model in neuroscience might not be
192

192

00:13:44,190  -->  00:13:49,470
such a good testing and
training model for other fields like
193

193

00:13:49,470  -->  00:13:55,330
if you're doing some sort of oil and
gas machine learning thing.
194

194

00:13:55,330  -->  00:13:59,540
So what we're gonna do is do
something called the residual plot
195

195

00:14:00,590  -->  00:14:05,590
to visualize how good our analysis was.
196

196

00:14:05,590  -->  00:14:08,280
So I'm gonna hop over back to the IPython
Notebook that's already completed here.
197

197

00:14:08,280  -->  00:14:11,669
And I'm gonna scroll down to step 9.
198

198

00:14:15,631  -->  00:14:16,620
So keep going.
199

199

00:14:16,620  -->  00:14:17,810
Okay.
200

200

00:14:17,810  -->  00:14:18,950
So residual plots.
201

201

00:14:18,950  -->  00:14:20,950
All right.
So in regression analysis the difference
202

202

00:14:20,950  -->  00:14:25,180
between the observed value, the dependent
variable, that was our Y, and
203

203

00:14:25,180  -->  00:14:27,320
the predicted value,
sometimes called Y hat,
204

204

00:14:27,320  -->  00:14:31,040
with that little mark there,
is called the residual E.
205

205

00:14:31,040  -->  00:14:34,290
So each data point has one residual.
206

206

00:14:34,290  -->  00:14:37,790
It's the observed value
minus the predicted value.
207

207

00:14:37,790  -->  00:14:41,270
So again, you can think of these
residuals in the same way as that D
208

208

00:14:41,270  -->  00:14:42,620
value we discussed earlier.
209

209

00:14:43,900  -->  00:14:47,110
However, remember we are looking
at multiple features.
210

210

00:14:47,110  -->  00:14:51,288
So it's not just a simple line,
but it's still the residual.
211

211

00:14:51,288  -->  00:14:55,490
You can just think of it in your mind as
that D value, but just keep in mind that
212

212

00:14:55,490  -->  00:15:00,490
we are looking at multiple variables,
so that room size, that crime rate, etc.
213

213

00:15:01,630  -->  00:15:06,430
So what a residual plot is a graph to show
the residuals on the vertical axis and
214

214

00:15:06,430  -->  00:15:10,780
the independent variable, in this case
the price, on the horizontal axis.
215

215

00:15:10,780  -->  00:15:14,870
So if the points and residual plot are
randomly dispersed around the horizontal
216

216

00:15:14,870  -->  00:15:18,318
axis, a linear regression model
is appropriate for the data.
217

217

00:15:18,318  -->  00:15:22,570
Otherwise a nonlinear
model is more appropriate.
218

218

00:15:22,570  -->  00:15:27,230
So residual plots are a really nice way
to visualize the errors in your data.
219

219

00:15:27,230  -->  00:15:28,570
And if you've done a good job,
220

220

00:15:28,570  -->  00:15:32,210
then your data should be randomly
scattered around line zero.
221

221

00:15:32,210  -->  00:15:34,688
So if there's some sort of
major structure or pattern,
222

222

00:15:34,688  -->  00:15:37,610
that means your model's
not capturing something.
223

223

00:15:37,610  -->  00:15:41,440
And there may be an interaction between
two variables that you're not considering.
224

224

00:15:41,440  -->  00:15:45,010
Or you might be measuring
some time-dependent data.
225

225

00:15:45,010  -->  00:15:47,454
So if this is the case,
you gotta go back to your model and
226

226

00:15:47,454  -->  00:15:48,765
check your data set closely.
227

227

00:15:48,765  -->  00:15:53,735
So, to learn a little more about the
residual plot, I left this nice link here,
228

228

00:15:53,735  -->  00:15:57,145
and if you click it here,
I encourage you to read this,
229

229

00:15:57,145  -->  00:16:02,428
it's basically why you need to check your
residual plots for regression analysis.
230

230

00:16:02,428  -->  00:16:06,912
So this is what we're hoping
our residual plot looks like So
231

231

00:16:06,912  -->  00:16:12,211
we have that residual and then we
have our prediction or fitted value.
232

232

00:16:12,211  -->  00:16:14,120
And you can see they're kind
of randomly scattered about.
233

233

00:16:14,120  -->  00:16:16,110
That's exactly what you're looking for
234

234

00:16:16,110  -->  00:16:18,560
to know that your regression
was the right choice.
235

235

00:16:19,730  -->  00:16:21,800
If you get something that
looks a little more like this,
236

236

00:16:21,800  -->  00:16:24,580
you can see it almost
has a structure to it.
237

237

00:16:24,580  -->  00:16:27,660
Almost like a squared function or
something like that.
238

238

00:16:27,660  -->  00:16:30,810
Then you know this residual plot is
239

239

00:16:30,810  -->  00:16:33,530
telling you that regression
was not the way to go.
240

240

00:16:33,530  -->  00:16:37,010
So I encourage you to go ahead and
read through this article, and
241

241

00:16:37,010  -->  00:16:40,200
it gives you a really nice understanding
of what we're actually doing here with
242

242

00:16:40,200  -->  00:16:41,290
this residual plot.
243

243

00:16:41,290  -->  00:16:45,370
So once you read that, come back and
we'll do the live coding.
244

244

00:16:45,370  -->  00:16:48,060
So go ahead and
pause the video if you want and
245

245

00:16:48,060  -->  00:16:51,330
check out that link, but in this case,
I'm just gonna continue.
246

246

00:16:52,390  -->  00:16:55,640
So, what we're gonna do is
make that scatter plot or
247

247

00:16:55,640  -->  00:16:59,090
that residual plot for the training data.
248

248

00:16:59,090  -->  00:17:04,441
So I'm gonna say train equals
249

249

00:17:04,441  -->  00:17:09,380
plt.scatter.
250

250

00:17:09,380  -->  00:17:12,860
And I want my prediction data for
my training set.
251

251

00:17:12,860  -->  00:17:17,875
And then what I wanna do is on the y,
252

252

00:17:17,875  -->  00:17:21,430
is do my prediction training
minus the y training.
253

253

00:17:21,430  -->  00:17:22,510
That residual value.
254

254

00:17:22,510  -->  00:17:31,210
So remember that was
the definition of our residuals.
255

255

00:17:35,540  -->  00:17:39,510
I'm going to set the color to blue,
just so my passing c equals the string b.
256

256

00:17:39,510  -->  00:17:42,260
I'm going to say alpha = 0.5.
257

257

00:17:42,260  -->  00:17:45,930
So hopefully you remember from
the visualization lectures,
258

258

00:17:45,930  -->  00:17:51,170
alpha's just how see through something is,
and then I'm gonna set
259

259

00:17:51,170  -->  00:17:56,220
a scatter plot with the testing data,
so I'm gonna say test plt.scatter.
260

260

00:17:56,220  -->  00:17:59,930
And in this case, we're gonna do the same
thing, but with our testing data.
261

261

00:17:59,930  -->  00:18:03,950
So we have our test prediction,
that pred test object we made, and
262

262

00:18:08,870  -->  00:18:13,850
then I'm gonna say prediction test
minus the Y test, so that residual.
263

263

00:18:15,090  -->  00:18:16,240
And I'm going to color this red.
264

264

00:18:18,980  -->  00:18:23,620
And again, set alpha to about half way,
see through.
265

265

00:18:23,620  -->  00:18:26,630
And what I'm going to do is I'm going
to plot a horizontal line access at
266

266

00:18:26,630  -->  00:18:27,410
line equals zero.
267

267

00:18:27,410  -->  00:18:31,250
And I can do that by calling plt.
268

268

00:18:31,250  -->  00:18:34,260
H lines, so horizontal lines.
269

269

00:18:34,260  -->  00:18:41,870
Set my y equal to 0 as an argument, and
the I'm going to set an x min and x max,
270

270

00:18:41,870  -->  00:18:45,620
you'll have to play around and see what
the correct x min and x max are for you.
271

271

00:18:48,060  -->  00:18:49,480
Finally, I wanna label my plot.
272

272

00:18:51,860  -->  00:18:52,740
I'll give it a legend.
273

273

00:18:55,130  -->  00:19:01,370
I'll have the legend be equal to those
train and test plots I just made.
274

274

00:19:01,370  -->  00:19:05,320
So if you noticed, I set those
plots to objects, and I did that so
275

275

00:19:05,320  -->  00:19:08,790
I can actually make a legend for them, and
276

276

00:19:08,790  -->  00:19:13,020
then you'll also pass a tuple
argument here in your legend.
277

277

00:19:13,020  -->  00:19:15,780
And you can go ahead and
just look up MatPlotLib
278

278

00:19:15,780  -->  00:19:20,550
legend on the documentation to get all
the different things you can pass here.
279

279

00:19:20,550  -->  00:19:27,420
But in this case, I just want to
pass two things: training and test.
280

280

00:19:28,860  -->  00:19:33,390
And I'll set the location of
this equal to lower left.
281

281

00:19:33,390  -->  00:19:36,860
And you the documentation and
set up the legend wherever you want.
282

282

00:19:38,430  -->  00:19:43,604
Scroll down a bit and then finally my
283

283

00:19:43,604  -->  00:19:48,961
plot title equal to residual plots.
284

284

00:19:52,040  -->  00:19:52,870
Okay, and there you have it.
285

285

00:19:52,870  -->  00:19:57,620
So if you notice,
my horizontal line value was
286

286

00:19:57,620  -->  00:20:02,880
actually not big enough as far as the x
max, so let me go ahead and change that.
287

287

00:20:02,880  -->  00:20:03,920
Looks like it goes up to 40.
288

288

00:20:03,920  -->  00:20:06,460
And there we go, this is a little better.
289

289

00:20:06,460  -->  00:20:09,520
So If you notice here, for the most part,
290

290

00:20:09,520  -->  00:20:11,510
it looks like there
isn't any major pattern.
291

291

00:20:12,870  -->  00:20:17,240
There's maybe this kind of line here
that's interesting to look at, but really,
292

292

00:20:17,240  -->  00:20:21,790
if you think about how many data points
we had, we had over 500 data points.
293

293

00:20:21,790  -->  00:20:25,580
Quite the majority is just scattered
randomly around this horizontal line.
294

294

00:20:25,580  -->  00:20:28,290
So that's exactly the kind
of thing we're looking for.
295

295

00:20:28,290  -->  00:20:32,250
So, it might be interesting to check out
what these variables were if you take out
296

296

00:20:33,370  -->  00:20:35,130
certain features.
297

297

00:20:35,130  -->  00:20:37,220
Does this line go away?
298

298

00:20:37,220  -->  00:20:41,230
That's definitely something to consider
as you move on further in data science.
299

299

00:20:41,230  -->  00:20:45,570
But as far as was the regression
the right choice?
300

300

00:20:45,570  -->  00:20:46,740
Definitely.
301

301

00:20:46,740  -->  00:20:48,605
Looks like this residual
plot supports that.
302

302

00:20:48,605  -->  00:20:53,210
So, that's actually it for
this lesson in linear regression.
303

303

00:20:53,210  -->  00:20:56,015
Linear regression is a super broad topic.
304

304

00:20:56,015  -->  00:21:01,350
There's actually much more to it
than just our simple analysis here.
305

305

00:21:01,350  -->  00:21:04,800
So there's tons of great information
on the scikit learn documentation.
306

306

00:21:04,800  -->  00:21:07,380
So I encourage you to check out this
link that I left at the bottom here.
307

307

00:21:07,380  -->  00:21:12,990
And if you check it out here, it'll go
over tons of generalized linear models.
308

308

00:21:12,990  -->  00:21:14,250
So we went over.
309

309

00:21:14,250  -->  00:21:16,950
Right here,
the ordinary least squares method.
310

310

00:21:16,950  -->  00:21:18,140
It gives you a little example here,
311

311

00:21:18,140  -->  00:21:20,840
but there's actually tons of
different types of regressions.
312

312

00:21:20,840  -->  00:21:24,290
There's lasso, bridge regression, and
313

313

00:21:24,290  -->  00:21:28,780
you can just keep going and scrolling
down, elastic net, multi-task lasso.
314

314

00:21:28,780  -->  00:21:32,720
Least angle regression is another one,
and then different types of lassos.
315

315

00:21:32,720  -->  00:21:36,560
So, there's a lot of information here and
documentation.
316

316

00:21:36,560  -->  00:21:40,600
Like I said earlier, the scikit learn
has pretty amazing documentation and
317

317

00:21:40,600  -->  00:21:43,310
there's examples for most everything here.
318

318

00:21:43,310  -->  00:21:45,590
So I really encourage you to check it out.
319

319

00:21:45,590  -->  00:21:47,460
As well as this linear
regression example here,
320

320

00:21:47,460  -->  00:21:52,290
in case you want more examples as far
as linear regression is concerned.
321

321

00:21:52,290  -->  00:21:54,800
But for now, I'll stop the video here and
322

322

00:21:54,800  -->  00:21:58,850
let you explore on your own the different
worlds of linear regression.
323

323

00:21:58,850  -->  00:22:01,230
And thank you so much and
congratulations for
324

324

00:22:01,230  -->  00:22:04,540
finishing your first machine
learning lecture exercises.
325

325

00:22:04,540  -->  00:22:05,280
All right guys.
326

326

00:22:05,280  -->  00:22:06,780
Thanks, and
I'll see you at the next lecture.
