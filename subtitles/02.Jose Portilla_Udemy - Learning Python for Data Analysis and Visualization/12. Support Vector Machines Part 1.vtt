WEBVTT
1
00:00:01.530 --> 00:00:06.330
Halo semua dan selamat datang di mesin vektor dukungan kuliah di bagian pembelajaran

2
00:00:06.330 --> 00:00:07.630
mesin kursus ini.

3
00:00:07.630 --> 00:00:12.160
Jadi dukung mesin Dr atau dukung mesin vektor yang disebut SBM.

4
00:00:12.550 --> 00:00:18.830
Ini adalah metode yang menggunakan titik dalam mentransformasikan ruang masalah untuk memisahkan kelas menjadi dua kelompok.

5
00:00:19.650 --> 00:00:26.240
Dan seperti yang kami lakukan untuk regresi logistik jika kami ingin menggunakan metode ini untuk klasifikasi beberapa kelas,

6
00:00:26.240 --> 00:00:28.940
kami dapat menggunakan metode satu versus semua.

7
00:00:28.940 --> 00:00:32.990
Jadi seri kuliah ini akan dipecah menjadi bagian-bagian berikut.

8
00:00:32.990 --> 00:00:37.200
Bagian 1 hanya akan memiliki pengantar dasar untuk mendukung mesin vektor.

9
00:00:37.200 --> 00:00:41.820
Kemudian di bagian dua akan ada penjelasan umum tentang SVM dan bagaimana cara kerjanya.

10
00:00:42.520 --> 00:00:46.400
Bagian 3 akan membahas komputasi hyperplane yang akan menjadi bagian dari VM ini.

11
00:00:47.910 --> 00:00:52.300
Dan kemudian di bagian 4 kita akan memiliki beberapa sumber matematika tambahan jika Anda ingin melakukan

12
00:00:52.300 --> 00:00:53.890
lebih dalam ke matematika SBA.

13
00:00:54.660 --> 00:00:59.360
Bagian Lima akan mulai melihat SVM untuk melihat bagaimana kita dapat menggunakannya dengan Python dan belajar.

14
00:00:59.360 --> 00:01:03.900
Dan kemudian di bagian 6 kami akan memiliki beberapa sumber daya tambahan untuk Anda lihat.

15
00:01:04.350 --> 00:01:09.420
Jadi dimulai dengan bagian satu akan memiliki mesin dukungan vektor pengantar.

16
00:01:09.420 --> 00:01:16.650
Jadi penjelasan formal dari mesin-mesin dukungan vektor ada di mesin-mesin pendukung pembelajaran mesin adalah model pembelajaran yang

17
00:01:16.680 --> 00:01:21.950
diawasi dengan algoritma pembelajaran yang terkait dan mereka dapat menganalisis data dan mengenali

18
00:01:21.950 --> 00:01:26.090
pola dan mereka digunakan untuk klasifikasi dan analisis regresi.

19
00:01:26.730 --> 00:01:31.390
Jadi, jika Anda diberi satu set contoh pelatihan masing-masing milik salah satu dari dua kategori.

20
00:01:31.390 --> 00:01:34.080
Ingat ini harus seperti regresi logistik di sini.

21
00:01:35.380 --> 00:01:42.060
Algoritma pelatihan VM apa selanjutnya yang membangun sebuah model yang memberikan contoh baru ke dalam salah satu

22
00:01:42.060 --> 00:01:43.260
dari dua kategori.

23
00:01:43.650 --> 00:01:50.130
Jadi itulah yang merupakan pembatas linear biner non probabilistik. Jadi dengan perkawinan karena Anda memiliki

24
00:01:50.130 --> 00:01:52.000
satu dari dua kelas.

25
00:01:52.000 --> 00:01:56.780
Dan model SPL adalah representasi dari contoh-contoh sebagai titik-titik dalam spasi

26
00:01:56.780 --> 00:02:02.300
dan peta sehingga contoh-contoh kategori terpisah dibagi oleh celah yang jelas selebar mungkin dan

27
00:02:02.300 --> 00:02:08.690
akan melihat seperti apa bentuk fisiknya dalam sedetik. dan contoh-contoh baru kemudian dipetakan ke dalam ruang yang

28
00:02:08.690 --> 00:02:13.520
sama dan diprediksi termasuk dalam kategori berdasarkan pada sisi mana mereka jatuh.

29
00:02:13.520 --> 00:02:20.450
Jadi penjelasan formal mungkin sedikit membingungkan tetapi kami akan memecahnya sebagian untuk memberi Anda gambaran umum yang

30
00:02:20.450 --> 00:02:24.760
bagus tentang apa itu PM dan bagaimana mereka bekerja.

31
00:02:25.330 --> 00:02:31.330
Sesuatu yang perlu dicatat dengan cepat di sini kelebihan dan kekurangan mesin vektor dukungan.

32
00:02:31.330 --> 00:02:36.650
Jadi beberapa keunggulan sangat efektif dalam ruang berdimensi tinggi dan masih efektif dalam kasus di

33
00:02:36.650 --> 00:02:39.780
mana jumlah mereka lebih besar daripada jumlah sampel.

34
00:02:40.270 --> 00:02:45.110
Dan apa yang baik tentang mesin vektor dukungan adalah mereka menggunakan subset poin pelatihan dalam fungsi yang disebut

35
00:02:45.150 --> 00:02:47.970
vektor dukungan dan akan belajar tentang mereka dalam hitungan detik.

36
00:02:48.370 --> 00:02:50.060
Dan itu juga fisi memori yang cantik.

37
00:02:51.000 --> 00:02:56.700
Beberapa kelemahan dari mesin dukungan vektor adalah jika jumlah fitur jauh lebih besar dari jumlah

38
00:02:56.700 --> 00:03:03.230
sampel Anda mungkin akan mendapatkan kinerja yang buruk dari warga sipil dan satu hal lagi yang perlu

39
00:03:03.230 --> 00:03:08.190
diketahui adalah bahwa karena idiom tidak secara langsung memberikan perkiraan probabilitas .

40
00:03:08.700 --> 00:03:12.060
Dan akan membahas lebih lanjut nanti.

41
00:03:12.060 --> 00:03:15.390
Jadi mari kita lanjutkan dan pelajari ikhtisar dasar.

42
00:03:15.390 --> 00:03:18.830
Agak penjelasan yang lebih sederhana tentang apa itu SVM Zar.

43
00:03:18.830 --> 00:03:23.520
Jadi kita akan mulai dengan membayangkan situasi di mana Anda ingin memisahkan set pelatihan

44
00:03:23.520 --> 00:03:24.850
dengan dua kelas.

45
00:03:25.140 --> 00:03:30.570
Jadi jika Anda melihat gambar di sini dan saya mengimpornya menggunakan I Python yang menampilkan.

46
00:03:30.570 --> 00:03:32.500
Kami memiliki dua kelas di set kami.

47
00:03:32.500 --> 00:03:36.110
Kami memiliki kotak merah dan lingkaran biru ini.

48
00:03:36.270 --> 00:03:41.050
Jadi kita akan merencanakannya di ruang fitur dan membayangkan bahwa Anda mencoba menempatkan

49
00:03:41.050 --> 00:03:43.930
garis hijau untuk memisahkan kedua kelas ini.

50
00:03:43.930 --> 00:03:48.750
Jadi jika kita melihat angka ini di sini kita memiliki set pelatihan kita.

51
00:03:48.750 --> 00:03:52.010
Kami memiliki kotak merah dan lingkaran biru.

52
00:03:52.010 --> 00:03:59.040
Dan sekarang Anda mencoba mencari tahu di mana saya bisa menempatkan garis hijau di sini yang dapat

53
00:03:59.040 --> 00:04:01.020
memisahkan dua kelas ini.

54
00:04:01.020 --> 00:04:06.910
Jadi sekarang yang ingin Anda ketahui adalah Anda dapat melihat dari gambar Anda memiliki banyak cara menggambar garis ini dan garis ini

55
00:04:07.380 --> 00:04:10.400
di masa depan yang akan kita lakukan adalah memanggil hyperplane.

56
00:04:11.350 --> 00:04:16.200
Dan jika Anda perhatikan semua baris di sini sebenarnya benar-benar memisahkan kedua kelas.

57
00:04:16.510 --> 00:04:21.320
Jadi yang ingin Anda lakukan adalah mencoba menemukan garis optimal dan seterusnya bagaimana Anda memutuskan

58
00:04:21.320 --> 00:04:23.330
apa garis optimal atau hyperplane.

59
00:04:23.670 --> 00:04:29.150
Jadi kami memutuskan bahwa hyperplane optimal adalah yang memisahkan dua kelas ini dengan margin

60
00:04:29.150 --> 00:04:30.760
maksimum antara dua kelas.

61
00:04:30.760 --> 00:04:32.860
Jadi jika kita melihat gambar ini sekarang.

62
00:04:33.720 --> 00:04:39.250
Ini adalah garis hyperplane optimal karena memiliki margin maksimum antara kedua kelas.

63
00:04:39.250 --> 00:04:45.020
Jadi, Anda ingin memaksimalkan ruang antara hyperplane Anda dan dua kelas.

64
00:04:45.710 --> 00:04:50.350
Jadi bagaimana kita menghitung secara matematis bidang ketinggian optimal.

65
00:04:50.350 --> 00:04:56.140
Jadi yang akan kita lakukan adalah memiliki gambaran singkat tentang hal itu di bagian 3 dan kemudian saya sangat menyarankan Anda memeriksa

66
00:04:56.140 --> 00:04:57.840
penjelasan lengkap yang kami bisa PDA.

67
00:04:57.840 --> 00:04:59.350
Dan saya meninggalkan Anda tautan di sini.

68
00:04:59.350 --> 00:05:03.780
Anda ingin memeriksanya dengan PDA Anda memiliki bagian yang bagus di mesin vektor dukungan.

69
00:05:04.450 --> 00:05:05.550
Tapi kembali.

70
00:05:07.330 --> 00:05:10.610
Kita sekarang dapat membahas bagaimana menghitung hyperplane a par 3.

71
00:05:10.610 --> 00:05:16.170
Tetapi sejauh ide keseluruhan dasar dari mesin vektor dukungan adalah bahwa Anda memiliki dua kelas

72
00:05:16.200 --> 00:05:21.720
ini dan apa yang ingin Anda lakukan adalah menempatkan hyperplane ini sehingga Anda memiliki margin

73
00:05:21.720 --> 00:05:24.720
maksimum di sini di antara kedua kelas.

74
00:05:24.720 --> 00:05:26.760
Jadi sekarang kita telah melompat ke Bagian 3.

75
00:05:26.760 --> 00:05:31.500
Komputasi hyperplane dan kita akan pergi ke depan dan mulai dengan mendefinisikan hyperplane dalam hal

76
00:05:31.500 --> 00:05:33.030
ini dengan persamaan garis.

77
00:05:33.030 --> 00:05:36.710
Dan ini akan terlihat sedikit familiar dari Anda regresi linier.

78
00:05:36.710 --> 00:05:38.110
Tapi di sini kita punya pertanyaan C.

79
00:05:38.110 --> 00:05:45.030
Fungsi dan beta ini ditransposisikan sebagai vektor bobot yang diketahui dari fitur yang telah kita lihat sebelumnya dan

80
00:05:45.030 --> 00:05:47.770
beta ini bukan yang kita sebut bias.

81
00:05:47.770 --> 00:05:52.660
Jadi ada banyak cara kita dapat skala vektor gelombang dan bias itu.

82
00:05:52.660 --> 00:05:56.740
Tapi ingat kami ingin memaksimalkan margin antara kedua kelas.

83
00:05:56.740 --> 00:06:01.930
Jadi jika Anda melakukan analisis matematis dan dijelaskan beberapa sumber di bawah ini, Anda bisa.

84
00:06:01.950 --> 00:06:10.090
Bahwa sebagai nilai absolut dari fungsi itu sama dengan 1 dan X di sini melambangkan contoh pelatihan

85
00:06:10.090 --> 00:06:13.140
yang paling dekat dengan hyperplane.

86
00:06:13.140 --> 00:06:18.390
Jadi secara umum contoh pelatihan yang paling dekat dengan hyperplane disebut vektor dukungan dan

87
00:06:18.390 --> 00:06:19.550
vektor dukungan.

88
00:06:19.550 --> 00:06:24.870
Jika kita gulir ke atas itu yang diisi dengan warna di sini.

89
00:06:24.870 --> 00:06:32.400
Jadi Anda menempatkan titik data Anda yang paling dekat dengan hyperplane atau hubungi faktor dukungan Anda dan itulah yang

90
00:06:32.400 --> 00:06:34.890
memungkinkan Anda untuk membuat keputusan itu.

91
00:06:34.890 --> 00:06:36.070
Begitu.

92
00:06:36.640 --> 00:06:37.510
Vektor dukungan.

93
00:06:37.510 --> 00:06:43.890
Jika Anda melihat gambar di atas bidang warna kami dan representasi ini di sini

94
00:06:43.920 --> 00:06:46.270
dikenal sebagai hyperplane kanonik.

95
00:06:46.270 --> 00:06:52.510
Dan kemudian dari beberapa geometri kita tahu bahwa jarak antara X dan hyperplane beda beda ini tidak

96
00:06:53.740 --> 00:06:56.750
diberikan oleh persamaan jarak ini di sini.

97
00:06:56.850 --> 00:07:04.280
Jadi khususnya untuk hyperplane kanonik pembilang sama dengan 1 dan jarak ke vektor dukungan diberikan

98
00:07:04.280 --> 00:07:07.420
oleh persamaan ini di sini.

99
00:07:11.020 --> 00:07:15.430
Dan sesuatu yang perlu diketahui di sini adalah bahwa margin yang kita bicarakan ini

100
00:07:15.430 --> 00:07:20.900
telah kita perkenalkan pada bagian sebelumnya yang kita tunjukkan mendengar bahwa jaraknya dua kali lipat dari sampel terdekat.

101
00:07:20.900 --> 00:07:27.190
Jadi yang saya maksud dengan itu adalah jika kita naik ke visualisasi ini di sini ingat seluruh

102
00:07:27.190 --> 00:07:32.450
M ini akan menjadi dua kali jarak dari pesawat terbang ke dukungan yang ada.

103
00:07:32.450 --> 00:07:38.140
Semoga itu masuk akal karena Anda memilikinya di kedua sisi hyperplane.

104
00:07:38.140 --> 00:07:45.010
Dan akhirnya masalah pemaksimalan bahwa margin itu setara dengan masalah meminimalkan fungsi

105
00:07:45.010 --> 00:07:53.430
yang tunduk pada beberapa kendala dan kendala memodelkan persyaratan untuk hydroplane untuk mengklasifikasikan dengan benar semua

106
00:07:53.430 --> 00:07:59.860
contoh pelatihan X secara formal diberikan oleh persamaan ini di sini.

107
00:08:00.360 --> 00:08:05.110
Atau subset Weiss mewakili masing-masing label dari contoh kereta.

108
00:08:05.110 --> 00:08:08.350
Dan masalah ini dikenal sebagai optimasi Lagrangian.

109
00:08:08.350 --> 00:08:14.000
Dan saya memiliki tautan di sini jika Anda ingin memeriksanya dan itu sebenarnya dapat diselesaikan dengan

110
00:08:14.030 --> 00:08:18.650
menggunakan pengganda untuk mendapatkan vektor gelombang beta dan bias dari hyperplane optimal.

111
00:08:20.570 --> 00:08:25.300
Jika Anda ingin melakukan klasifikasi nonlinier, kami dapat menggunakan sesuatu yang disebut trik

112
00:08:25.300 --> 00:08:30.830
Kolonel yang juga saya tinggalkan tautannya untuk Anda dan menggunakan trik kernel yang dapat kami

113
00:08:30.830 --> 00:08:33.590
lakukan adalah mengiris ruang fitur dengan hyperplane.

114
00:08:33.590 --> 00:08:39.120
Jadi hanya gambaran singkat dari apa yang telah kami lakukan di sini kami telah melalui beberapa

115
00:08:39.120 --> 00:08:44.700
gambaran matematika yang sangat mendasar dan kami belum mendapatkan apa pun akan mencari sumber untuk derivasi dan kedua

116
00:08:45.070 --> 00:08:50.590
Tetapi pada dasarnya yang ingin kita lakukan adalah kita memiliki hyperplane optimal ini yang memisahkan fitur-fitur ini menjadi

117
00:08:50.590 --> 00:08:51.320
dua kelas.

118
00:08:51.320 --> 00:09:01.530
Kami ingin memaksimalkan AM ini margin ini di sini atau ini akan dua kali panjang hyperplane ke salah satu faktor pendukung di

119
00:09:01.530 --> 00:09:03.640
sini salah satu kelas.

120
00:09:06.220 --> 00:09:13.580
Melanjutkan kita dapat menggunakan trik Kernal jika Anda ingin mengiris fitur dengan cara non-linear.

121
00:09:13.580 --> 00:09:15.890
Jadi saat ini kami hanya memiliki garis lurus.

122
00:09:15.890 --> 00:09:22.210
Tetapi bagaimana jika kita ingin melakukan semacam kurva jika itu adalah pemisahan terpisah yang optimal.

123
00:09:22.210 --> 00:09:29.710
Jadi yang bisa kita lakukan adalah masuk ke ruang fitur dan apa yang kita lihat sekarang di gambar-gambar di atas

124
00:09:29.710 --> 00:09:31.150
adalah ruang input.

125
00:09:31.150 --> 00:09:33.360
Jadi di mana kami memiliki garis lurus itu.

126
00:09:33.360 --> 00:09:40.490
Tetapi Anda dapat membayangkan jika Anda melemparkan semua poin ini hampir seperti bola ke udara, Anda dapat

127
00:09:40.490 --> 00:09:43.060
membuat hyperplane memotong ke ruang fitur.

128
00:09:43.290 --> 00:09:47.610
Dan cara terbaik untuk menunjukkan ini adalah melalui animasi.

129
00:09:47.820 --> 00:09:51.790
Jadi saya akan pergi ke depan dan memainkan animasi ini dengan cepat untuk kalian.

130
00:09:51.790 --> 00:09:53.490
Itu dibuat oleh orang ini.

131
00:09:54.150 --> 00:09:57.540
Tetapi pergi ke layar penuh sehingga Anda bisa melihat.

132
00:09:57.850 --> 00:10:02.410
Jadi pada dasarnya yang kita miliki di sini adalah Anda memiliki

133
00:10:02.410 --> 00:10:11.800
titik biru dan titik merah ini dan apa yang akan Anda lakukan adalah menggunakan apa yang disebut trik Kolonel untuk mengiris titik-titik tersebut menggunakan hyperplane.

134
00:10:11.800 --> 00:10:17.080
Dan pada dasarnya Anda dapat melengkungkan ruang fitur di sana dan di sini Anda menurunkan bidang hiper

135
00:10:18.220 --> 00:10:19.800
Anda dan sekarang Anda terpisah.

136
00:10:20.170 --> 00:10:23.730
Kelas Blue Point Anda menunjuk dari yang merah di sana.

137
00:10:23.730 --> 00:10:25.570
Dan di sana Anda bisa mendapatkan lingkaran itu.

138
00:10:25.570 --> 00:10:31.760
Dan jika Anda melihat ke belakang dan melihat ruang input sekarang Anda memiliki klasifikasi nonlinear.

139
00:10:31.760 --> 00:10:34.420
Jadi saya akan membiarkan Anda melanjutkan dan memeriksa tautan itu sendiri.

140
00:10:36.580 --> 00:10:40.670
Mudah-mudahan Anda punya ide umum tentang apa yang kami lakukan untuk mendukung mesin vektor.

141
00:10:40.910 --> 00:10:45.900
Basis trik dasar hanyalah kita memiliki hyperplane dan kami mencoba untuk memaksimalkan margin antara

142
00:10:45.900 --> 00:10:50.330
dua kelas dan kemudian poin yang paling dekat dengan bidang yang lebih

143
00:10:50.360 --> 00:10:52.380
tinggi disebut dukungan yang muncul.

144
00:10:53.360 --> 00:10:57.670
Dan kemudian jika kita ingin melakukan klasifikasi nonlinier kita dapat melakukan sesuatu

145
00:10:57.670 --> 00:11:03.650
yang disebut trik kernel yang pada dasarnya menempatkan semua poin ke ruang fitur Anda dan kemudian mengiris ruang

146
00:11:03.650 --> 00:11:08.570
fitur dengan hyperplane dan mudah-mudahan visualisasi itu membantu Anda membayangkan itu sedikit lebih baik.

147
00:11:09.570 --> 00:11:16.430
Per untuk sumber daya matematika tambahan sehingga kami mendapatkan gambaran yang sangat mendasar hampir seperti sebuah konsep dari

148
00:11:16.430 --> 00:11:18.020
matematika di balik SVM.

149
00:11:18.020 --> 00:11:22.160
Jika Anda mencari menyelam lebih dalam ke matematika dari mesin vektor dukungan saya punya beberapa sumber daya

150
00:11:22.160 --> 00:11:23.370
untuk Anda di sini.

151
00:11:23.980 --> 00:11:30.160
Ada dukungan klasik dari kuliah mesin Dr di sini tentang cara menggunakan SVM.

152
00:11:30.430 --> 00:11:35.010
Dan kemudian jika Anda mencari sesuatu yang sedikit lebih berat di Maff lebih detail, Anda dapat

153
00:11:35.010 --> 00:11:36.180
melihat kuliah MIT.

154
00:11:36.180 --> 00:11:37.190
Ini sekitar satu jam.

155
00:11:38.340 --> 00:11:44.640
Pasti layak untuk belajar lebih banyak tentang matematika umum di belakang mesin faktor pendukung.

156
00:11:44.640 --> 00:11:48.910
OK jadi hanya ulasan singkat sebelum kita melanjutkan.

157
00:11:49.130 --> 00:11:53.310
Kami belajar definisi formal dari mesin vektor dukungan.

158
00:11:53.310 --> 00:11:59.630
Kami mendapat gambaran umum konseptual yang bagus tentang apa yang bekerja SVM dan apa yang sebenarnya dilakukannya adalah mencoba memisahkan kedua

159
00:11:59.930 --> 00:12:00.770
kelas ini.

160
00:12:00.770 --> 00:12:07.840
Kedua kelas poin ini digunakan untuk klasifikasi linier sebagai garis lurus dan coba lakukan adalah

161
00:12:07.840 --> 00:12:15.700
mengoptimalkan hyperplane ini yang sejalan dengan margin maksimum antara dua kelas dan titik-titik yang paling dekat

162
00:12:15.700 --> 00:12:18.700
dengan hydroplane disebut vektor dukungan.

163
00:12:18.910 --> 00:12:23.030
Jadi itulah yang digunakannya untuk menilai margin maksimum itu.

164
00:12:23.030 --> 00:12:24.860
Dan kemudian kita juga melihat kita bisa menggunakan.

165
00:12:24.860 --> 00:12:29.930
Kita dapat menggunakan trik kernal untuk melakukan klasifikasi nonlinier dan benar-benar akan melakukannya sehingga saya

166
00:12:29.930 --> 00:12:36.130
bisa belajar sedikit dan kemudian jika Anda ingin memiliki lebih banyak sumber matematika tambahan Anda bisa masuk dan

167
00:12:36.130 --> 00:12:38.540
memeriksa dua kuliah yang bagus ini.

168
00:12:38.540 --> 00:12:40.820
Saya sangat merekomendasikan Anda melakukannya.

169
00:12:40.820 --> 00:12:45.850
Tetapi bergerak jika Anda hanya ingin melompat langsung ke mesin-mesin vektor psych, ia belajar bagaimana

170
00:12:45.850 --> 00:12:47.370
untuk diimplementasikan dengan Python.

171
00:12:47.370 --> 00:12:49.910
Kemudian tetap disini untuk kuliah berikutnya dan sampai jumpa di sana.

172
00:12:49.910 --> 00:12:50.850
Baiklah, terima kasih kawan.
