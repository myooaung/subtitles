1
1

00:00:01,810  -->  00:00:05,290
Hey everyone, and welcome to the third
video in the Supervised Learning
2

2

00:00:05,290  -->  00:00:07,440
linear regression series.
3

3

00:00:07,440  -->  00:00:10,970
So in the previous video, we just
completed a single variable regression, or
4

4

00:00:10,970  -->  00:00:15,480
a univariate regression, using
the least squares method with Python.
5

5

00:00:15,480  -->  00:00:18,210
And remember we created this plot up here.
6

6

00:00:20,030  -->  00:00:23,330
So now let's see if we can find
the error in our fitted line.
7

7

00:00:23,330  -->  00:00:28,120
So if we check out the documentation
here on this link, checking out this
8

8

00:00:28,120  -->  00:00:32,950
documentation, we see that the resulting
array has the total squared error.
9

9

00:00:32,950  -->  00:00:35,930
So for each element, it checks
the difference between the line and
10

10

00:00:35,930  -->  00:00:37,390
the true value.
11

11

00:00:37,390  -->  00:00:39,255
Remember that was our original d value.
12

12

00:00:39,255  -->  00:00:45,790
So it basically looks around and sees how
long that green line or that d value was.
13

13

00:00:48,430  -->  00:00:52,050
So it takes that, squares it and
returns the sum of all these.
14

14

00:00:52,050  -->  00:00:55,690
So that was the sum d squared
value we discussed earlier.
15

15

00:00:55,690  -->  00:00:58,450
So all those distances
between your data point and
16

16

00:00:58,450  -->  00:01:00,590
your fit line squared and then summed.
17

17

00:01:01,920  -->  00:01:05,580
It might a little easier though to
understand the root mean squared error,
18

18

00:01:05,580  -->  00:01:09,120
which is really similar to
the standard deviation.
19

19

00:01:09,120  -->  00:01:12,050
So in this case,
to find the root mean square error,
20

20

00:01:12,050  -->  00:01:15,740
we divide by the number of elements and
then take the square root.
21

21

00:01:15,740  -->  00:01:18,230
There's also an issue of biased and
unbiased regression, but
22

22

00:01:18,230  -->  00:01:20,040
we'll delve into these
topics a little later.
23

23

00:01:21,580  -->  00:01:22,320
For now let's go ahead and
24

24

00:01:22,320  -->  00:01:25,690
see how we can get the root mean squared
error of the line we just fitted.
25

25

00:01:25,690  -->  00:01:26,810
And you can go ahead and
26

26

00:01:26,810  -->  00:01:31,260
check out the normal distribution
lectures in the statistics appendix
27

27

00:01:31,260  -->  00:01:36,240
if you need a little bit more review
as far as the standard deviation.
28

28

00:01:36,240  -->  00:01:41,250
But we're basically kind of getting a type
of standard deviation using this method.
29

29

00:01:41,250  -->  00:01:43,460
So, let's go ahead and
jump back to the live coding.
30

30

00:01:46,070  -->  00:01:50,900
So what we're gonna do now is
in order to get the error,
31

31

00:01:51,960  -->  00:01:55,800
I'm gonna get the resulting array, so I'm
gonna say make an object called results.
32

32

00:01:58,160  -->  00:02:03,553
I then call again the n
p dot linear algebra
33

33

00:02:03,553  -->  00:02:10,960
dot least squares method on those x and
y we made earlier.
34

34

00:02:13,330  -->  00:02:16,310
And my total error is stored at index one.
35

35

00:02:16,310  -->  00:02:18,930
If you look up the documentation,
it'll let you know that.
36

36

00:02:22,280  -->  00:02:25,650
And a lot of machine learning,
as far as getting to know your tools,
37

37

00:02:25,650  -->  00:02:27,090
is looking up the documentation.
38

38

00:02:27,090  -->  00:02:31,090
So, it's gonna be a lot of reading
throughout these machine learning lectures
39

39

00:02:31,090  -->  00:02:33,365
that you're gonna have to do
through the documentation.
40

40

00:02:33,365  -->  00:02:40,250
So we're gonna set the total
error as the result index one.
41

41

00:02:40,250  -->  00:02:45,580
And then in order to get the root mean
squared error, root mean squared error,
42

42

00:02:45,580  -->  00:02:48,890
or rmse, I can take
43

43

00:02:51,020  -->  00:02:58,560
the square roots of that total error or
error_total object I just made.
44

44

00:02:59,830  -->  00:03:02,020
And divide by the length of x.
45

45

00:03:03,680  -->  00:03:06,680
So this is, if you're familiar with
what a standard deviation looks like,
46

46

00:03:06,680  -->  00:03:09,790
you can kinda see how this
equation looks pretty familiar.
47

47

00:03:09,790  -->  00:03:11,280
Now, let's just go ahead and
print our results.
48

48

00:03:11,280  -->  00:03:16,589
I'll print the root mean square error was,
and
49

49

00:03:16,589  -->  00:03:23,500
in this case, I'll cut off
the floating point at two points.
50

50

00:03:26,750  -->  00:03:27,810
R, s, m, e.
51

51

00:03:33,211  -->  00:03:36,720
And oops.
52

52

00:03:36,720  -->  00:03:38,360
Root mean squared error.
53

53

00:03:38,360  -->  00:03:42,530
Okay, it look our root mean
squared error was 6.60,
54

54

00:03:42,530  -->  00:03:48,090
as far as up to two
places past the decimal.
55

55

00:03:48,090  -->  00:03:51,790
So since the root mean square error or
rmse,
56

56

00:03:51,790  -->  00:03:56,590
corresponds approximately
to the standard deviation,
57

57

00:03:56,590  -->  00:04:01,390
we can now say the price of
a house won't vary more than two
58

58

00:04:01,390  -->  00:04:04,796
times the rmse value 95% of the time.
59

59

00:04:04,796  -->  00:04:07,870
And so what I mean by that if we hop
over back to the iPython notebook.
60

60

00:04:10,240  -->  00:04:12,230
This is really similar to
the normal distribution.
61

61

00:04:12,230  -->  00:04:14,940
So go ahead and check out this link.
62

62

00:04:14,940  -->  00:04:18,272
It will just take you here to the 68, 95,
63

63

00:04:18,272  -->  00:04:23,040
99.7 rule, it has to do with the normal
distribution and the standard deviations.
64

64

00:04:23,040  -->  00:04:27,287
So what we could basically make out
this root mean square error to be,
65

65

00:04:27,287  -->  00:04:32,480
is that 95% of the time,
66

66

00:04:32,480  -->  00:04:38,920
your value's gonna be at least within
two times that root mean squared error.
67

67

00:04:40,030  -->  00:04:49,100
So you can reasonably expect a house price
to be within $13,200 of our fit line.
68

68

00:04:49,100  -->  00:04:53,600
Now remember,
this is in the unit of thousands.
69

69

00:04:55,485  -->  00:04:59,370
Okay, so go ahead and review your
normal distribution appendix lecture.
70

70

00:04:59,370  -->  00:05:02,485
If this didn't quite make sense to you,
but hopefully it did.
71

71

00:05:02,485  -->  00:05:08,355
Remember we're just basically
getting this error that we discussed
72

72

00:05:08,355  -->  00:05:12,975
up here earlier, the distance between all
these lines, squaring them, summing them.
73

73

00:05:12,975  -->  00:05:15,885
And then dividing it by the number
of data points we had, and
74

74

00:05:15,885  -->  00:05:17,045
then square rooting that whole thing.
75

75

00:05:17,045  -->  00:05:22,300
And that basically corresponds
to a standard deviation here.
76

76

00:05:22,300  -->  00:05:26,478
And since we're within two standard
deviations of it or 6.6 times 2,
77

77

00:05:26,478  -->  00:05:29,000
95% of the time, you're only
78

78

00:05:30,350  -->  00:05:34,560
gonna be off by two times that rmse,
that root mean square error value.
79

79

00:05:34,560  -->  00:05:36,470
All right.
80

80

00:05:36,470  -->  00:05:38,340
So that's how you get your error.
81

81

00:05:38,340  -->  00:05:42,640
Now let's move on to the cool part
using scikit learn to implement
82

82

00:05:42,640  -->  00:05:43,820
a multivariate regression.
83

83

00:05:44,890  -->  00:05:46,420
So, we're gonna keep moving along.
84

84

00:05:48,880  -->  00:05:51,820
And we'll keep moving along
with using scikit learn
85

85

00:05:51,820  -->  00:05:53,280
to a multivariable regression.
86

86

00:05:53,280  -->  00:05:56,650
So, it's gonna be a similar approach to
the above example that we just did, but
87

87

00:05:56,650  -->  00:05:59,290
scikit learn will be able to
take into account more than just
88

88

00:05:59,290  -->  00:06:01,950
the single data variable
effect on the target.
89

89

00:06:01,950  -->  00:06:04,290
So we'll start by importing
the linear regression library.
90

90

00:06:06,380  -->  00:06:07,800
Let's go ahead and
check out this link here.
91

91

00:06:09,460  -->  00:06:12,680
So again, I really encourage you to
really fully read and understand
92

92

00:06:12,680  -->  00:06:17,490
the documentation as we go along through
these machine learning lectures.
93

93

00:06:17,490  -->  00:06:21,410
You'll really get the most out if
you completely read through all this
94

94

00:06:21,410  -->  00:06:26,600
documentation, but we're importing
from sklearn.linear_model library,
95

95

00:06:26,600  -->  00:06:27,610
the linear regression.
96

96

00:06:29,160  -->  00:06:29,970
So this does,
97

97

00:06:29,970  -->  00:06:34,400
as you can see here, an ordinary,
least-squares, linear regression.
98

98

00:06:34,400  -->  00:06:35,170
All right.
99

99

00:06:35,170  -->  00:06:35,996
So hopping back here.
100

100

00:06:35,996  -->  00:06:43,650
The sklearn.linear_model.LinearRegression
class is what's known as an estimator.
101

101

00:06:43,650  -->  00:06:47,640
So what an estimator does, is estimators
predict a value based on observed data.
102

102

00:06:48,880  -->  00:06:54,510
So in sklearn, all estimators
implement the fit and predict methods.
103

103

00:06:54,510  -->  00:06:58,190
So the fit method is used to learn
the parameters of the model.
104

104

00:06:58,190  -->  00:07:02,914
And the later method, the predict method
is used to predict the value of a response
105

105

00:07:02,914  -->  00:07:07,024
variable for an explanatory variable
used in the learning parameter,
106

106

00:07:07,024  -->  00:07:09,711
such as the crime rate or
the number of houses.
107

107

00:07:09,711  -->  00:07:13,510
And your prediction would
be the price of that house.
108

108

00:07:13,510  -->  00:07:17,080
So now it's easy to experiment with
different models using scikit learn,
109

109

00:07:17,080  -->  00:07:20,630
because all the estimators implement
the fit and predict methods.
110

110

00:07:22,300  -->  00:07:24,630
So go ahead and
read through the documentation
111

111

00:07:24,630  -->  00:07:29,790
to get a clear idea of what that really
means as far as making estimator class.
112

112

00:07:29,790  -->  00:07:32,250
And implementing the fit and
predict methods.
113

113

00:07:32,250  -->  00:07:33,870
But hopefully for a linear regression,
114

114

00:07:33,870  -->  00:07:38,960
it's pretty intuitive
as far as giving data.
115

115

00:07:38,960  -->  00:07:40,120
All this different housing data and
116

116

00:07:40,120  -->  00:07:44,600
then trying to predict a house price from
that data based on what you already know.
117

117

00:07:45,920  -->  00:07:49,325
Okay.
So, let's go ahead and
118

118

00:07:49,325  -->  00:07:53,544
jump back to the live coding and
119

119

00:07:53,544  -->  00:08:00,686
we're gonna import sklearn,
or scikit learn, and
120

120

00:08:00,686  -->  00:08:08,826
from sklearn.linear_model
import LinearRegression.
121

121

00:08:08,826  -->  00:08:15,380
Okay, so now we're going to create
a linear regression object.
122

122

00:08:15,380  -->  00:08:21,240
Afterwards, we'll go ahead and
123

123

00:08:22,290  -->  00:08:26,950
do some more stuff on it so,
gonna make a linear regression object.
124

124

00:08:26,950  -->  00:08:33,380
Call it lreg, and then I could
do that just by setting it equal
125

125

00:08:33,380  -->  00:08:38,477
to LinearRegression close parentheses.
126

126

00:08:38,477  -->  00:08:42,430
Great, so if we take a quick look
over at the documentation here,
127

127

00:08:44,480  -->  00:08:47,110
the linear regression has a bunch
of different methods on it.
128

128

00:08:47,110  -->  00:08:51,740
Fit, get parameters, predict, score,
subparameters, decision function.
129

129

00:08:51,740  -->  00:08:54,510
So you can go ahead and read throughout
the documentation on this, and
130

130

00:08:54,510  -->  00:08:56,490
there's actually, what's really,
131

131

00:08:56,490  -->  00:08:59,990
really nice about the scikit learn,
is there's tons and tons of examples.
132

132

00:08:59,990  -->  00:09:01,310
So you can actually see here,
133

133

00:09:01,310  -->  00:09:05,840
there's already a linear regression
example done it out for you.
134

134

00:09:05,840  -->  00:09:09,490
So in case you want even more, besides
what this lecture offers, you can come
135

135

00:09:09,490  -->  00:09:14,050
here, and they're gonna do a very
similar thing using a diabetes data set.
136

136

00:09:14,050  -->  00:09:17,960
So you can always come back to
the documentation and check out examples.
137

137

00:09:17,960  -->  00:09:22,360
There's examples for most everything
in the scikit learn library.
138

138

00:09:22,360  -->  00:09:24,630
It's a really, really nice documentation.
139

139

00:09:24,630  -->  00:09:27,070
So I encourage you to check that out
after you're done with these lectures
140

140

00:09:27,070  -->  00:09:30,020
just to get another set
of examples to look at.
141

141

00:09:32,080  -->  00:09:38,295
But if we just take a look
at pressing tab, oops.
142

142

00:09:42,090  -->  00:09:44,270
You can look at the different methods.
143

143

00:09:44,270  -->  00:09:44,770
So.
144

144

00:09:47,750  -->  00:09:49,330
We have all these different methods.
145

145

00:09:49,330  -->  00:09:52,870
We have fit, fit_intercept,
normalize, predict, score.
146

146

00:09:52,870  -->  00:09:56,676
And what we want to do, is we're gonna
be really using just a few of them.
147

147

00:09:56,676  -->  00:10:02,310
So hopping back over, the functions
148

148

00:10:02,310  -->  00:10:07,340
we're gonna be using are the fit, which
fits a linear model, the predict, which
149

149

00:10:07,340  -->  00:10:11,420
is used to predict the y using the linear
model with the estimated coefficients.
150

150

00:10:13,000  -->  00:10:16,940
And then the score, which returns
the coefficient of the termination,
151

151

00:10:16,940  -->  00:10:19,560
otherwise known as an r squared value.
152

152

00:10:19,560  -->  00:10:22,510
And really all that gives is just
the measure how well the observed
153

153

00:10:22,510  -->  00:10:24,430
outcomes are replicated by the model.
154

154

00:10:24,430  -->  00:10:27,400
If you wanna learn more about
the coefficient determination,
155

155

00:10:27,400  -->  00:10:29,390
again Wikipedia is your friend here.
156

156

00:10:29,390  -->  00:10:31,050
Click the link I provided and
157

157

00:10:31,050  -->  00:10:36,610
it'll basically give you more context
of what the r squared value means.
158

158

00:10:36,610  -->  00:10:39,100
But a perfect r squared
value would just be 1, and
159

159

00:10:39,100  -->  00:10:41,030
in that case its a perfectly linear fit.
160

160

00:10:42,160  -->  00:10:44,840
And you can judge how
well your data fits to
161

161

00:10:44,840  -->  00:10:49,660
your line based on that coefficient of
determination, your r squared values.
162

162

00:10:49,660  -->  00:10:50,160
All right.
163

163

00:10:52,230  -->  00:10:55,580
So, lets go ahead and start the multi
variable regression analysis
164

164

00:10:55,580  -->  00:11:00,380
by separating our Boston data frame back
into data columns and the target columns.
165

165

00:11:02,080  -->  00:11:05,930
So, the way I'm going to
do that is like this.
166

166

00:11:08,770  -->  00:11:14,298
I'm gonna say my data columns as X_multi.
167

167

00:11:14,298  -->  00:11:21,344
And I'm gonna set that as
my data frame boston_df.
168

168

00:11:25,230  -->  00:11:26,210
And drop the price.
169

169

00:11:28,900  -->  00:11:35,200
Remember, in order to drop a column,
you gotta pass a one index and then my y
170

170

00:11:37,490  -->  00:11:42,840
target is gonna be equal
to boston_df.Price.
171

171

00:11:42,840  -->  00:11:47,870
And now we have our X_multi
equal to everything
172

172

00:11:47,870  -->  00:11:52,010
in the data frame minus the price and the
Y_target is now just equal to the price.
173

173

00:11:53,680  -->  00:11:55,040
And notice we didn't use in play so
174

174

00:11:55,040  -->  00:11:58,830
we haven't effected our original
Boston data frame permanently.
175

175

00:11:59,900  -->  00:12:03,640
So finally we're ready to pass the X and
Y using the linear regression object.
176

176

00:12:05,880  -->  00:12:10,116
The way to do that is just lreg,
that linear regression option we made.
177

177

00:12:11,215  -->  00:12:15,045
And now we just pass those
into the fit method.
178

178

00:12:24,750  -->  00:12:28,151
And now I've created a linear
regression fit and we can go ahead and
179

179

00:12:28,151  -->  00:12:31,390
check the intercept and
the number of coefficients.
180

180

00:12:31,390  -->  00:12:34,380
So, the way to do that is
by calling those methods.
181

181

00:12:34,380  -->  00:12:41,950
So I can say, print, and I'll type
it out and then break it down here.
182

182

00:12:53,500  -->  00:12:55,520
So the estimated intercept coefficient is.
183

183

00:12:56,570  -->  00:12:58,879
And I'll leave it to two
places after the decimal.
184

184

00:13:02,920  -->  00:13:06,776
And then I call the intercept
method underscore off it,
185

185

00:13:06,776  -->  00:13:08,670
and then I'll also print.
186

186

00:13:15,930  -->  00:13:17,350
The number of coefficients we used.
187

187

00:13:18,420  -->  00:13:20,961
Which should have been 13 if
you remember from the dataset.
188

188

00:13:29,580  -->  00:13:33,760
And I'm just gonna call the length of that
linear regression coefficient method.
189

189

00:13:35,080  -->  00:13:39,629
And remember, just check the documentation
to get a true understanding of what these
190

190

00:13:39,629  -->  00:13:42,420
intercepts and
coefficient methods return to you.
191

191

00:13:42,420  -->  00:13:46,884
So come over here and
learn about the actual
192

192

00:13:46,884  -->  00:13:51,350
linear regression class in scikit learn.
193

193

00:13:53,910  -->  00:13:59,712
So, whoops,
put the b in the wrong spot here, number,
194

194

00:13:59,712  -->  00:14:05,650
so the number of coefficients was 13,
as we expected.
195

195

00:14:05,650  -->  00:14:09,991
It's the length of that coefficient
method, and the number, the estimated
196

196

00:14:09,991  -->  00:14:14,801
intercept coefficient is 36.49, so
what we've done is we've basically, and
197

197

00:14:14,801  -->  00:14:17,830
I'm hopping over back to
the iPython notebook here.
198

198

00:14:19,870  -->  00:14:22,430
We've basically made an equation for
a line.
199

199

00:14:22,430  -->  00:14:24,030
But instead of just one coefficient, m,
200

200

00:14:24,030  -->  00:14:27,720
and an intercept b,
we now have 13 coefficients.
201

201

00:14:27,720  -->  00:14:29,250
So to get an idea of what this looks like,
202

202

00:14:29,250  -->  00:14:32,470
you can check out the documentation
link here for linear regression.
203

203

00:14:33,520  -->  00:14:37,830
And if you notice here they have
a following set of methods for regression.
204

204

00:14:38,840  -->  00:14:42,280
And they have an equation here as well
as a coefficient in the intercept.
205

205

00:14:44,220  -->  00:14:50,188
But what you've done is, hopping
over back to that iPython notebook,
206

206

00:14:50,188  -->  00:14:56,353
is you've made an equation of a line,
but instead of y equals m x plus b, or
207

207

00:14:56,353  -->  00:15:01,523
in this case it's written as y
equals w not plus w 1 times x 1,
208

208

00:15:01,523  -->  00:15:05,700
you've gone all the way
up to 13 coefficients,
209

209

00:15:05,700  -->  00:15:11,126
because you're using 13 different
types of data variables.
210

210

00:15:11,126  -->  00:15:14,220
Okay, so what we're going to do next
211

211

00:15:14,220  -->  00:15:19,550
is set up a data frame showing all the
features and their estimated coefficients
212

212

00:15:19,550  -->  00:15:23,276
obtained from that fit that we just did,
that linear regression.
213

213

00:15:23,276  -->  00:15:28,300
So basically scikit learn is doing
everything we did manually in Python
214

214

00:15:28,300  -->  00:15:31,530
using numpie in one simple
step with this fit method.
215

215

00:15:31,530  -->  00:15:35,360
So let's go ahead and
set up that data frame.
216

216

00:15:35,360  -->  00:15:37,211
I'm just gonna type it out and
then break it down for you.
217

217

00:15:40,211  -->  00:15:45,129
I'll make a data frame called
coefficient or coeff_df.
218

218

00:16:09,045  -->  00:16:11,970
Okay, so I set a data frame from Features.
219

219

00:16:11,970  -->  00:16:15,033
Now I'm going to set a new column lining
up the coefficients from the linear
220

220

00:16:15,033  -->  00:16:15,620
regression.
221

221

00:16:24,910  -->  00:16:27,128
I'll call it a coefficient
estimate column.
222

222

00:16:31,130  -->  00:16:35,080
And I'll have it equal
to a series of pandas.
223

223

00:16:35,080  -->  00:16:39,730
And I don't need to call pd
224

224

00:16:39,730  -->  00:16:44,660
here since I imported series
of the coefficient method.
225

225

00:16:46,900  -->  00:16:48,253
And I'll show that data frame.
226

226

00:16:52,800  -->  00:16:54,470
So let me break down what I did here.
227

227

00:16:54,470  -->  00:16:56,850
I create a coefficient data frame
228

228

00:16:56,850  -->  00:17:00,240
based on the columns of my
original Boston data frame.
229

229

00:17:01,420  -->  00:17:05,660
Renamed that columns just features so
made this columns a features.
230

230

00:17:05,660  -->  00:17:07,180
All those different
features from our data set.
231

231

00:17:08,800  -->  00:17:11,830
And then made a new column
called coefficient estimate and
232

232

00:17:11,830  -->  00:17:14,460
I just passed all those coefficients here.
233

233

00:17:15,950  -->  00:17:19,470
So when were in this last print statement,
we just printed out the length for
234

234

00:17:19,470  -->  00:17:21,200
those coefficients.
235

235

00:17:21,200  -->  00:17:22,760
So what your fit did.
236

236

00:17:22,760  -->  00:17:25,070
You use scikit learn.
237

237

00:17:25,070  -->  00:17:30,285
You fit a multiple regression, multiple
variable regression, using this X_multi.
238

238

00:17:31,442  -->  00:17:34,300
And scikit learn was able to return
239

239

00:17:34,300  -->  00:17:38,900
basically these coefficients that are used
to build up this equation of a line.
240

240

00:17:41,010  -->  00:17:41,880
Okay, great.
241

241

00:17:43,450  -->  00:17:48,540
So just like we initially plotted out,
it seems if we look at these coefficients,
242

242

00:17:48,540  -->  00:17:53,210
the highest one, 3.8,
actually corresponds with the room,
243

243

00:17:53,210  -->  00:17:55,010
the average number of rooms.
244

244

00:17:55,010  -->  00:17:57,930
So it seems like its
the highest correlated feature
245

245

00:17:57,930  -->  00:18:02,930
between a particular data feature and
the house price was the number of rooms.
246

246

00:18:02,930  -->  00:18:06,340
So in the next video what we're gonna
do is move on to predicting prices and
247

247

00:18:06,340  -->  00:18:10,480
using training and validation datasets.
248

248

00:18:10,480  -->  00:18:12,100
So just a quick overview
of what we did here.
249

249

00:18:14,210  -->  00:18:19,480
We were able to now use the scikit learn
library to perform a linear regression.
250

250

00:18:19,480  -->  00:18:23,110
There was a lot of documentation
reading to be done.
251

251

00:18:23,110  -->  00:18:27,083
But it was definitely worth it, and
we were able to get a result and
252

252

00:18:27,083  -->  00:18:29,336
set them into a panda's data frame.
253

253

00:18:29,336  -->  00:18:33,692
Okay, so I'll see you at the next video,
where we'll be performing step seven and
254

254

00:18:33,692  -->  00:18:38,237
step eight, where we'll use training and
validation sets, predict our prices, and
255

255

00:18:38,237  -->  00:18:40,930
then, finally,
we'll see some residual plots.
256

256

00:18:40,930  -->  00:18:43,253
All right, thanks guys,
and I'll see you there.
