1

00:00:01,440  -->  00:00:02,550
Welcome back.

2

00:00:02,550  -->  00:00:05,080
How did you go of the exercise with number of products.

3

00:00:05,220  -->  00:00:06,800
It wasn't difficult was it.

4

00:00:06,800  -->  00:00:09,240
Here is the result that I got.

5

00:00:09,240  -->  00:00:11,790
And here you can see right away a couple things.

6

00:00:11,790  -->  00:00:14,260
First of all there are a few anomalies.

7

00:00:14,280  -->  00:00:21,330
So you would expect that as the number of products increases the customer is more likely to stay off

8

00:00:21,330  -->  00:00:21,720
the bank.

9

00:00:21,720  -->  00:00:29,880
So in in banking term I think is called stickiness or the share of the wallets of the customer so as

10

00:00:29,930  -->  00:00:32,320
you know the bank has a greater share of the wallet.

11

00:00:32,400  -->  00:00:36,360
It is harder for the customer to leave and that is what we're observing in the first two.

12

00:00:36,360  -->  00:00:43,440
So people who have two products with the bank are less likely to exit than people have one product.

13

00:00:43,440  -->  00:00:48,840
In fact as you can see we can see the labels here because the font is too big so why don't we just take

14

00:00:48,840  -->  00:00:54,900
out the text label for now for our analysis and that way we'll see all of the labels.

15

00:00:54,900  -->  00:01:01,560
So here you can see that only 8 percent of customers with two products left and 28 percent of customers

16

00:01:02,010  -->  00:01:04,700
with one product left.

17

00:01:04,710  -->  00:01:07,080
So there a massive drop.

18

00:01:07,080  -->  00:01:09,160
It is much harder for these customers to leave.

19

00:01:09,330  -->  00:01:17,280
But then we see an anomaly here so we see how the as yet more products are three and four products it

20

00:01:17,280  -->  00:01:24,570
is customers are leaving at a rapid rate at a very very high rate 83 percent and 100 percent for products

21

00:01:24,870  -->  00:01:28,930
which does not apply for intuition whatsoever.

22

00:01:29,100  -->  00:01:38,310
So those are massive anomalies or very distinct anomalies and we need to understand what we can do about

23

00:01:38,310  -->  00:01:38,520
them.

24

00:01:38,520  -->  00:01:44,310
So the first thing that comes to mind and the first thing that you should always remember when you have

25

00:01:44,680  -->  00:01:52,020
anomalies that great there might be something wrong with the data are not wrong with that it might be

26

00:01:52,020  -->  00:01:55,670
it just might be a insignificant result.

27

00:01:55,680  -->  00:01:57,380
That's that's what I'm looking for here.

28

00:01:57,420  -->  00:02:01,680
It might be an insignificant result and therefore it might not be worth reporting what if there are

29

00:02:01,710  -->  00:02:08,580
only five people that have for products or if if because this is a large sample it's 10000 people together

30

00:02:08,580  -->  00:02:08,720
.

31

00:02:08,780  -->  00:02:14,220
What if there's like only 100 people that had a 100 had for product and they all just happened to live

32

00:02:14,220  -->  00:02:14,250
.

33

00:02:14,250  -->  00:02:18,360
So it's kind of like a random effect rather than something that you can rely on.

34

00:02:18,360  -->  00:02:23,490
And that is why we have to be cautious that these tests that we're doing they're not statistically fully

35

00:02:23,490  -->  00:02:25,710
fledged test full fledged statistical tests.

36

00:02:25,710  -->  00:02:28,450
They're more ad hoc visual tests.

37

00:02:28,470  -->  00:02:34,290
So in these cases we would need to check the actual number of people.

38

00:02:34,410  -->  00:02:36,720
So now I'm going to show you how to do that.

39

00:02:37,470  -->  00:02:40,290
And in order to check the number of people that left.

40

00:02:40,380  -->  00:02:46,290
We'll talk more about this kind of approach or a more thorough approach on this in the next section

41

00:02:46,500  -->  00:02:47,100
of the course.

42

00:02:47,100  -->  00:02:54,690
But for now I'll just show you a quick way to do it so I'm going to take the number of records over

43

00:02:54,690  -->  00:03:04,890
here and I'm going to drag a number of records into label just as a text so rather than doing a table

44

00:03:04,890  -->  00:03:07,160
calculation I'm just dragging in as it is.

45

00:03:07,410  -->  00:03:12,630
And so here you can see now we will just take out the table calculation for now because once again we

46

00:03:12,630  -->  00:03:14,150
can see what's in there.

47

00:03:14,700  -->  00:03:21,030
So here what you'll see is that these these two categories they have quite a lot of people in in these

48

00:03:21,030  -->  00:03:27,000
categories but these categories they have fewer people in the category so there is only two hundred

49

00:03:27,030  -->  00:03:32,310
and sixty six people in with three products is only 60 people with four products.

50

00:03:32,310  -->  00:03:38,000
So that can possibly explain why we're seeing a tsunami it just happens that we.

51

00:03:38,190  -->  00:03:45,120
The samples were randomly selected among them all the people that had for products they were very high

52

00:03:45,120  -->  00:03:49,130
list of leaving and they actually did leave over the observed period of time.

53

00:03:49,620  -->  00:03:54,120
So sometimes when things like this happen you have to investigate further and you have to conduct proper

54

00:03:54,120  -->  00:03:57,960
statistical tests to make sure that these results are valid.

55

00:03:57,960  -->  00:04:04,440
So in this case you can say that going from one product to products it's harder for people to live for

56

00:04:04,440  -->  00:04:07,430
three or four products you would need to investigate this further.

57

00:04:07,440  -->  00:04:09,690
So that's what I wanted to show you with this chart.

58

00:04:09,690  -->  00:04:14,790
We will return it back to its previous state now.

59

00:04:14,870  -->  00:04:19,500
And maybe you want to add a comment for yourself here so that in the future remember that there is some

60

00:04:19,500  -->  00:04:20,520
investigation to do.

61

00:04:20,520  -->  00:04:26,970
So you go and state and then you just select area for instance and here you want to say something like

62

00:04:27,670  -->  00:04:34,800
low observations in a last two categories.

63

00:04:34,800  -->  00:04:39,780
So just a label for Celso we're not going to go into the details of these investigations.

64

00:04:39,780  -->  00:04:42,040
Our goal is to understand how to work with Tablo.

65

00:04:42,210  -->  00:04:46,040
But nevertheless you want to have some information for the future.

66

00:04:46,040  -->  00:04:46,710
So there you go.

67

00:04:46,710  -->  00:04:53,760
I just added a label so whenever I go back to this chart I'll be able to see that there is a low number

68

00:04:53,760  -->  00:04:58,820
observations in these categories and I need to do something about it.

69

00:04:59,040  -->  00:05:00,250
So that's all for this chart.

70

00:05:00,270  -->  00:05:03,660
What I wanted to show you today is how to validate your approach.

71

00:05:03,690  -->  00:05:05,750
And in fact validate your daughter.

72

00:05:05,880  -->  00:05:09,990
So let's go ahead and create another test.

73

00:05:09,990  -->  00:05:12,450
I'm going to duplicate this one over here.

74

00:05:12,450  -->  00:05:20,500
So duplicate sheet and this one will call validation and we can get rid of this comment.

75

00:05:20,520  -->  00:05:28,520
So the idea here is to find a variable that you know for sure does not affect your end result.

76

00:05:28,830  -->  00:05:38,220
So something that is definitely not a important variable in the choice of customers to stay if the bank

77

00:05:38,220  -->  00:05:43,320
or exit the bank and a good variable here would be a number.

78

00:05:43,320  -->  00:05:45,850
So like a digit one digit number.

79

00:05:45,870  -->  00:05:52,320
So if say the customer My favorite is if the customers if you have four mobile phones and the data and

80

00:05:52,500  -->  00:05:58,080
you can just take say the last digit of the mobile phone or in this case we have customer I.D. So let's

81

00:05:58,080  -->  00:06:03,870
take the last digit of the customer I.D. as you can imagine it has nothing to do with whether or not

82

00:06:03,870  -->  00:06:06,470
the customer of the bank it's not a predictor.

83

00:06:06,600  -->  00:06:12,930
It doesn't matter what last digit in fact doesn't matter what customer I.D. you have at all in terms

84

00:06:12,930  -->  00:06:16,020
of your decision for staying or leaving leaving the bank.

85

00:06:16,170  -->  00:06:23,220
And so once we run the test for the laws the age of the customer I.D. We can then check that yes everything

86

00:06:23,220  -->  00:06:24,230
is fine.

87

00:06:24,300  -->  00:06:29,550
It has to be all of them all of the hostages have to have the same possibility of leaving.

88

00:06:29,550  -->  00:06:33,750
So let's go ahead and check that we're going to Right-Click customer I.D. and we're going to create

89

00:06:33,810  -->  00:06:35,280
a calculated field.

90

00:06:35,340  -->  00:06:37,470
Haven't done one of these in a while have we.

91

00:06:37,470  -->  00:06:38,930
So we're going to say run.

92

00:06:39,300  -->  00:06:49,290
We're going to say last digit of customer I.D. And here we're just going to say right as in the last

93

00:06:49,290  -->  00:06:54,950
character one and from here you can see that table calculations are correct.

94

00:06:54,960  -->  00:07:01,080
That's because customer ID is still deemed as a number we need to change it to a string so we'll just

95

00:07:01,080  -->  00:07:05,580
say SDR and we'll close the brackets.

96

00:07:05,610  -->  00:07:07,440
So that looks good.

97

00:07:07,440  -->  00:07:13,250
And now we have a new variable here last digit of customer I.D. as you can see is a calculated variable

98

00:07:13,440  -->  00:07:18,420
and we can take this and replace number of products as we go ahead and do that.

99

00:07:18,490  -->  00:07:25,950
And so right away you can see a beautiful very uniform picture here that approximately on average every

100

00:07:25,950  -->  00:07:31,830
customer ID that you have or the last digit of the customer I.D. It has the same people that digit have

101

00:07:31,830  -->  00:07:37,800
the same probability of living as average about 20 percent so plus or minus some fluctuations exist

102

00:07:38,100  -->  00:07:39,120
always in your daughter.

103

00:07:39,300  -->  00:07:44,010
But in this case it's plus or minus a little tiny bit which is insignificant.

104

00:07:44,010  -->  00:07:47,120
So on average it's 20 percent.

105

00:07:47,250  -->  00:07:50,380
So that's how you check your data another way you could do it.

106

00:07:50,440  -->  00:07:56,850
You could look at the last or first letter of the customer surname but that is a bit more tricky because

107

00:07:57,270  -->  00:08:02,310
lenders are generally less uniformly distributed they're not as randomly so you'll find that letters

108

00:08:02,310  -->  00:08:11,610
like q are rarer letters like TR more frequent and that can affect your results so try to find something

109

00:08:11,940  -->  00:08:16,350
that is uniformly distributed across your population or your sample.

110

00:08:16,410  -->  00:08:22,920
And at the same time it does not affect the end outcome at all whatsoever and that will allow you to

111

00:08:22,920  -->  00:08:23,900
validate two things.

112

00:08:23,900  -->  00:08:25,360
First of all our approach.

113

00:08:25,560  -->  00:08:31,890
Now we can see that our approach is valid because if it wasn't or if we did see some some skewness here

114

00:08:31,920  -->  00:08:34,010
then maybe we would question our approach.

115

00:08:34,140  -->  00:08:42,270
And also we can see that the data is there are no errors or issues with the data because let's say if

116

00:08:42,270  -->  00:08:51,840
we saw that one or a couple of these is too high then that would mean that perhaps maybe there are some

117

00:08:51,840  -->  00:08:57,950
problems with the data why is why is there this observation this anomaly that shouldn't be there.

118

00:08:58,440  -->  00:09:00,180
And that's the end of this tutorial.

119

00:09:00,420  -->  00:09:06,030
You can come up with other similar ways to validate your data and I encourage you to use them just to

120

00:09:06,030  -->  00:09:10,850
make sure that along the way while you're doing your analysis everything is running smoothly.

121

00:09:10,860  -->  00:09:12,120
I look forward to seeing you next time.

122

00:09:12,130  -->  00:09:13,610
And until then happy analyzing
