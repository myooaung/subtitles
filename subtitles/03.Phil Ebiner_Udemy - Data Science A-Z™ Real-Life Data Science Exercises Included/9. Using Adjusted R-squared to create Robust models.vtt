WEBVTT
1

00:00:01.380  -->  00:00:02.520
Great to see you back here.

2

00:00:02.550  -->  00:00:08.550
And today we're going to look at the handy trick that will help your models become more robust.

3

00:00:08.550  -->  00:00:14.160
Before we continue though I wanted to quickly recap on what we did last night and what we did is we

4

00:00:14.160  -->  00:00:22.350
use the backward elimination method to construct a multiple linear regression using our data and through

5

00:00:22.350  -->  00:00:26.430
that method we constructed four separate models you can see them on the in the background here so Model

6

00:00:26.430  -->  00:00:28.950
1 to 3 and then 4.

7

00:00:29.100  -->  00:00:34.980
And when we get to model 4 we were only left with one independent variable spend on research and development

8

00:00:35.220  -->  00:00:37.640
because all of the other ones were eliminated.

9

00:00:37.860  -->  00:00:41.070
And basically we completed the backward elimination process.

10

00:00:41.130  -->  00:00:48.350
However we were left with kind of a feeling that maybe we should have excluded the last variable and

11

00:00:48.350  -->  00:00:48.820
why is that.

12

00:00:48.840  -->  00:00:56.190
Well first of all we saw that this P-value is not that much bigger than the significance level that

13

00:00:56.190  -->  00:01:03.730
we selected we select a significant level of 0.05 the P-value 0.06 So just a bit greater than the threshold

14

00:01:03.740  -->  00:01:03.860
.

15

00:01:04.020  -->  00:01:10.170
And also when we looked at the charts or the plot of The Profit by marketing spend we could see that

16

00:01:10.860  -->  00:01:16.140
even visually there is kind of a correlation here so if you just look at the red dots and ignore the

17

00:01:16.140  -->  00:01:23.160
blue ones you can see that there is kind of like a leaner looking correlation here in that direction

18

00:01:23.160  -->  00:01:23.650
.

19

00:01:23.670  -->  00:01:29.160
So that kind of leaves us with a feeling maybe we shouldn't have excluded that variable.

20

00:01:29.160  -->  00:01:35.260
The problem here is that this these methods this step was a regression methods.

21

00:01:35.370  -->  00:01:36.630
They are very arbitrary.

22

00:01:36.630  -->  00:01:42.870
So once you select your significance level threshold you've got to stick to that right.

23

00:01:42.870  -->  00:01:44.770
You use the 0.05.

24

00:01:44.780  -->  00:01:50.820
This one 0.06 so there's greater and we just gotta cut it off and not look not look back anymore and

25

00:01:50.820  -->  00:01:52.420
just proceed with the method.

26

00:01:52.890  -->  00:02:02.250
So how can we improve our method of building models to assess situations like that and give you like

27

00:02:02.280  -->  00:02:08.460
an extra opinion or have another criteria to tell us whether or not we should have actually kept this

28

00:02:08.640  -->  00:02:09.550
variable.

29

00:02:09.660  -->  00:02:14.190
And there is a way and we're going to talk about it right now.

30

00:02:14.580  -->  00:02:18.660
So let's look at the top part over here.

31

00:02:19.200  -->  00:02:25.860
The top part is responsible for the variable so you have the coefficients and you've got the p values

32

00:02:25.950  -->  00:02:26.340
and so on.

33

00:02:26.340  -->  00:02:29.370
So we've talked about it quite quite extensively.

34

00:02:29.370  -->  00:02:30.860
But let's look at the bottom part.

35

00:02:30.870  -->  00:02:37.380
The second part of our report and here as we discussed before we've got the stats that are are for the

36

00:02:37.380  -->  00:02:38.040
model as a whole.

37

00:02:38.040  -->  00:02:41.220
So how well has been fitted how well it's working and so on.

38

00:02:41.220  -->  00:02:47.920
So the stats that we'll be looking at today are are squared and adjusted R-squared.

39

00:02:47.970  -->  00:02:57.480
And they will help us come up with a cool approach and great approach to improve our backward elimination

40

00:02:57.480  -->  00:02:58.060
method.

41

00:02:58.320  -->  00:03:03.990
So we already talked about both the R-squared and adjusted R-squared in the stats refresher section

42

00:03:03.990  -->  00:03:05.060
of course.

43

00:03:05.070  -->  00:03:09.420
However if you chose to skip that section because you're not interested in the formulas and so on then

44

00:03:09.420  -->  00:03:11.620
I'll quickly recap for you now.

45

00:03:11.700  -->  00:03:20.910
So R-squared here is basically a characteristic or a parameter of your model which tells you about the

46

00:03:20.910  -->  00:03:21.630
goodness of fit.

47

00:03:21.630  -->  00:03:27.540
So how will your model has been fitted and R squared can never be greater than one and you wanted to

48

00:03:27.540  -->  00:03:34.740
be as close to one as possible the closer to 1 it is the better your model is deemed to be fitted.

49

00:03:34.740  -->  00:03:43.230
However we also discussed this part in the refresher R-Squared is biased and it's biased in a way that

50

00:03:44.810  -->  00:03:47.650
it is constructed and the way these models are run.

51

00:03:47.670  -->  00:03:56.730
So the ordinary least squared method it doesn't allow R-squared to ever decrease so the more variables

52

00:03:56.730  -->  00:03:59.680
you add to your model the greater R-squared will be.

53

00:03:59.690  -->  00:04:05.790
So basically what we're what this means is that as long as you keep adding variables R-squared will

54

00:04:05.790  -->  00:04:08.460
always grow and we can observe that here.

55

00:04:08.460  -->  00:04:12.660
So if we start from the end where we have only one variable you can see that R-Squared is zero point

56

00:04:12.660  -->  00:04:13.600
ninety four.

57

00:04:13.800  -->  00:04:19.710
Then R-squared became while if you wish to go this way it's zero point ninety five point ninety five

58

00:04:19.710  -->  00:04:20.310
or four.

59

00:04:20.310  -->  00:04:24.820
There is zero point ninety five or seven and then it's zero point ninety 95 1 0.

60

00:04:24.930  -->  00:04:29.730
So as you can see the more variables we have the greater R-squared gets and that's that's always going

61

00:04:29.730  -->  00:04:34.440
to be the case just because of the way R-Squared is derived.

62

00:04:34.440  -->  00:04:39.930
And moreover you can even include completely around a variable so if I throw into this model I throw

63

00:04:39.930  -->  00:04:46.680
another variable which is basically the temperature outside like air temperature outside right now then

64

00:04:46.720  -->  00:04:46.870
.

65

00:04:46.920  -->  00:04:49.340
And I throw that in as an independent variable.

66

00:04:49.350  -->  00:04:54.390
Of course it's not a predictor it can predict profit of a company that works in New York or California

67

00:04:54.390  -->  00:04:54.690
.

68

00:04:54.870  -->  00:05:01.550
But R-Squared is still going to grow and is going to imply that our model is now even better fitted

69

00:05:01.570  -->  00:05:01.860
.

70

00:05:02.160  -->  00:05:06.320
So that way our square is biased and that's where adjusted R-squared comes into play.

71

00:05:06.510  -->  00:05:14.040
Adjusted R-squared is very similar to Our Square has got a very simple formula but it actually has a

72

00:05:14.460  -->  00:05:15.860
penalization factor.

73

00:05:15.930  -->  00:05:23.360
So basically just like R squared would grow if you add more variables that are square would also grow

74

00:05:23.400  -->  00:05:29.970
but there is a penalization factor which makes it small which reduces adjusted R-squared as you add

75

00:05:29.970  -->  00:05:30.870
more variables.

76

00:05:30.870  -->  00:05:35.040
So there's kind of these two effects battling each other on one hand it's growing because of the way

77

00:05:35.040  -->  00:05:40.730
it's constructed on the other hand the penalization factor is penalizing you or penalizing that just

78

00:05:40.740  -->  00:05:43.350
in R-squared and reducing it every time you add a variable.

79

00:05:43.350  -->  00:05:50.460
So basically if the variable that you added doesn't make adjusted or doesn't make R-squared grow that

80

00:05:50.460  -->  00:05:55.860
much like for instance here you can see 0.9 5 0 4 2 0 9 5 0 7.

81

00:05:55.860  -->  00:05:58.370
So it only grew by a fraction.

82

00:05:58.380  -->  00:05:59.970
Very very small amount.

83

00:05:59.970  -->  00:06:05.600
Well if that happens then the penalization factor is going to overwhelm this growth and therefore the

84

00:06:05.610  -->  00:06:07.860
adjusted R-squared is actually going to decrease.

85

00:06:07.950  -->  00:06:10.480
In that scenario and that way we can use.

86

00:06:10.500  -->  00:06:17.760
And we will use the adjusted R-squared to watch the goodness of fit our models and how it changes.

87

00:06:17.750  -->  00:06:19.010
So let's let's go ahead and do that.

88

00:06:19.010  -->  00:06:21.510
Let's observe that just that R-squared in our method.

89

00:06:21.780  -->  00:06:23.010
What was the adjusted R-squared.

90

00:06:23.000  -->  00:06:26.270
Here it was Sir point nine 4 6 6.

91

00:06:26.310  -->  00:06:34.340
Then when we excluded administration expenses adjusted R-squared went from zero point 9 4 6 6 to 0.9

92

00:06:34.350  -->  00:06:35.670
4 7 5.

93

00:06:35.670  -->  00:06:42.360
So as you can see here adjusted R-squared went up where it just basically what that means is that the

94

00:06:42.500  -->  00:06:44.490
model is now better.

95

00:06:44.510  -->  00:06:46.080
It's been it's fitted better.

96

00:06:46.130  -->  00:06:54.420
It works these variables in this combination fit the profit variable fit this model to explain the profit

97

00:06:54.420  -->  00:06:59.130
variable better than these variables in this combination to explain the profit variable which is good

98

00:06:59.130  -->  00:06:59.220
.

99

00:06:59.250  -->  00:07:00.210
It's a good step.

100

00:07:00.440  -->  00:07:01.200
Okay.

101

00:07:01.290  -->  00:07:06.170
So that means we improved Amahl here adjusted R-squared is 0.9 475.

102

00:07:06.210  -->  00:07:10.470
Let's see what had happened to where we moved to the next step and the next step adjusted R-squared

103

00:07:10.530  -->  00:07:10.560
.

104

00:07:10.560  -->  00:07:18.930
Is there a point 9 4 8 3 so it went up again meaning that once again we improved our model so altogether

105

00:07:18.930  -->  00:07:26.330
these variables are doing a better job explaining profit then these variables together are doing a job

106

00:07:26.340  -->  00:07:27.200
explaining profit.

107

00:07:27.360  -->  00:07:28.020
That's great.

108

00:07:28.010  -->  00:07:29.590
We improved our model again.

109

00:07:30.130  -->  00:07:34.410
And now let's see what happens where we take out the last variable marketing spend.

110

00:07:34.410  -->  00:07:42.590
So we went from adjusted R-squared 0.9 4 8 3 to adjusted R-squared 0.9 for 5 4.

111

00:07:42.810  -->  00:07:44.080
And what does that tell us.

112

00:07:44.090  -->  00:07:52.980
Our risk adjusted R-squared went down and went down by from by zero point zero 03 approximately.

113

00:07:52.980  -->  00:08:01.170
So that means that this model this new model is actually worse than this model so this model was better

114

00:08:01.170  -->  00:08:09.060
fitted to predict or explain the variance in profit than this model is doing so.

115

00:08:09.660  -->  00:08:10.230
So there you go.

116

00:08:10.230  -->  00:08:17.270
So even though we excluded a variable according to our Bacary elimination method this may this variable

117

00:08:17.270  -->  00:08:21.430
shouldn't have been excluded because it was Improve it was.

118

00:08:21.670  -->  00:08:23.520
We're faced with this measure of variable.

119

00:08:23.510  -->  00:08:29.250
This model is actually working better and that is your takeaway.

120

00:08:29.460  -->  00:08:35.800
Handy trick how to observe your models and how to create them.

121

00:08:35.820  -->  00:08:42.500
You don't only just follow the backwards elimination or whatever method that you are using and arbitrarily

122

00:08:42.540  -->  00:08:50.630
follow the rules just instead follow the rules but also watch the adjusted R-squared and see if it's

123

00:08:50.630  -->  00:08:51.440
actually improving.

124

00:08:51.450  -->  00:08:53.280
So if it's growing then you're doing the right thing.

125

00:08:53.490  -->  00:08:57.010
As soon as you see that our square drop then you have to stop and question.

126

00:08:57.100  -->  00:08:58.650
I just do the right thing or not.

127

00:08:58.640  -->  00:08:59.610
And you know what.

128

00:08:59.610  -->  00:09:05.440
What is the tradeoff of excluding a certain variable or opposite of including this variable.

129

00:09:05.460  -->  00:09:09.910
So I just had R-Squared is kind of your indicator how you're going along the way.

130

00:09:10.160  -->  00:09:12.150
And that's it for today.

131

00:09:12.140  -->  00:09:18.680
I just wanted to say that I just that R-Squared is not the only model parameter is because it is quite

132

00:09:18.690  -->  00:09:21.840
a lot of them here and I've already mentioned that Cauchy criterion.

133

00:09:22.010  -->  00:09:29.700
I take it criterion is is this one over here and it also works in a small way but it's got a different

134

00:09:29.730  -->  00:09:30.410
calculation.

135

00:09:30.410  -->  00:09:36.780
We won't go into detail but if you want to you can look things up a bit more and understand which one

136

00:09:36.870  -->  00:09:42.750
suits which of these criterions or parameters works better in your case for instance or K-Q criterion

137

00:09:42.750  -->  00:09:43.120
.

138

00:09:43.380  -->  00:09:48.620
You're going to find is better when it's lower So here it's you can see it's big and become smaller

139

00:09:48.620  -->  00:09:50.220
a smaller and then goes up again.

140

00:09:50.340  -->  00:09:55.430
So they're KDK right here and also confirms that this model is doing a better job.

141

00:09:55.670  -->  00:10:03.300
So if you like have a look at other criterions Schwartz criterion for instance and see how you can maybe

142

00:10:03.300  -->  00:10:04.640
apply them as well.

143

00:10:05.390  -->  00:10:07.200
But that's it for today.

144

00:10:07.230  -->  00:10:13.320
Hope you find this useful and hope you can apply it you'll find ways to apply this in your actual work

145

00:10:13.330  -->  00:10:13.600
.

146

00:10:13.860  -->  00:10:15.140
I look forward to seeing you next time.

147

00:10:15.210  -->  00:10:16.740
And until then happy be analyzing
