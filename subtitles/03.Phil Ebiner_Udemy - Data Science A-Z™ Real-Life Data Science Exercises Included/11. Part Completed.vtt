WEBVTT
1

00:00:00.960  -->  00:00:07.500
Welcome back and right away Congratulations you have completed the error handling in e-tail Phase 3

2

00:00:07.500  -->  00:00:15.510
section of the course also means that you have completed the whole of the EPL process part of the course

3

00:00:15.510  -->  00:00:22.450
which is a really very massive So Caressa that it's been a massive very long journey.

4

00:00:22.530  -->  00:00:24.090
So well done.

5

00:00:24.090  -->  00:00:30.840
And today we're going to quickly sum up what we learned in this phase 3 or a linked part of the course

6

00:00:31.230  -->  00:00:34.500
and then I'll wave you off to your homework.

7

00:00:34.670  -->  00:00:35.160
OK.

8

00:00:35.250  -->  00:00:45.060
So in this section what we learned is how to add filters to make sure that our conversion from a profile

9

00:00:45.780  -->  00:00:51.750
into a working file works well that it goes smoothly with 40 errors.

10

00:00:51.870  -->  00:00:56.700
As you remember we originally had a lot of errors because the wrong values were in the wrong columns

11

00:00:56.730  -->  00:01:00.420
and the conversions the implicit conversions weren't going through well.

12

00:01:00.690  -->  00:01:07.640
So we had to exclude a lot of rows and then the conversion went through OK.

13

00:01:07.650  -->  00:01:14.430
Also we learned how to do additional quality assurance checks and perform additional exclusions based

14

00:01:14.430  -->  00:01:16.350
on intrinsic knowledge of our data.

15

00:01:16.350  -->  00:01:21.480
So basically domain knowledge so we know that for instance balance cannot be less than zero or from

16

00:01:21.480  -->  00:01:28.640
the data we can see that the zip code has to have a certain format and the birthday can be over 100

17

00:01:28.830  -->  00:01:34.720
years ago or they can be less than five years ago so things like that.

18

00:01:34.770  -->  00:01:42.660
And these were just examples you don't have to use the same exact same elements of code in your specific

19

00:01:42.660  -->  00:01:50.150
case your case will be different but this is showing you that you do have this power to fish out nonconforming

20

00:01:50.160  -->  00:01:56.910
records and records that are considered anomalies from your daughter because it is important for your

21

00:01:56.910  -->  00:01:58.560
daughter to be clean for analysis.

22

00:01:58.740  -->  00:02:04.680
And I wanted to mention two things on this whole topic.

23

00:02:04.680  -->  00:02:09.710
So number one is this is a very simple data set.

24

00:02:09.720  -->  00:02:17.010
It doesn't have lots of Kolb's has only a few columns only has a couple of them and you remember previously

25

00:02:17.010  -->  00:02:22.800
in the previous sections we worked with a data set that had less rows but had so many more columns it

26

00:02:22.800  -->  00:02:26.200
had like over 20 columns I think.

27

00:02:26.370  -->  00:02:33.210
And in that case you can't really go through all of it will take you ages to go through every single

28

00:02:33.210  -->  00:02:33.790
column.

29

00:02:33.900  -->  00:02:38.790
Yes you do need to do the filters if you're experiencing issues then you need to do the filters but

30

00:02:39.030  -->  00:02:44.460
these additional exclusions you should only do them realistically for the columns that you require in

31

00:02:44.460  -->  00:02:45.810
your analysis.

32

00:02:45.960  -->  00:02:50.270
It is important to understand that it is not part of your responsibility.

33

00:02:50.400  -->  00:02:56.760
Well I assume that it's not part of your responsibility to make sure that there are no errors at all

34

00:02:56.760  -->  00:02:58.250
in this whole dataset.

35

00:02:58.320  -->  00:03:04.920
That is somebody else's responsibility what your responsibility is is to perform the analysis to come

36

00:03:04.920  -->  00:03:11.820
up with the great insights and conclusions and and things like that from the door of his Alsatians and

37

00:03:11.820  -->  00:03:14.850
reports and whatever whatever you're working on.

38

00:03:14.940  -->  00:03:23.760
So you will ultimately need a subset of your daughter you won't need all 115 columns in your daughter

39

00:03:23.760  -->  00:03:26.790
said so why not just focus on the ones you need.

40

00:03:26.790  -->  00:03:33.630
Why not if you need balance zipcode and birthday to create a Joe demographic segmentation of a dashboard

41

00:03:33.630  -->  00:03:33.770
.

42

00:03:33.900  -->  00:03:39.440
So you know where the people live how old they are and what their balance is and you can see the distributions

43

00:03:39.460  -->  00:03:39.950
there.

44

00:03:39.990  -->  00:03:48.000
Then in your analysis perhaps you're not even going to address things like what their city street addresses

45

00:03:48.300  -->  00:03:54.330
or what's what their interest rate is that they've been offered.

46

00:03:54.330  -->  00:04:01.110
So in that case you just need to focus on the columns that you will be using and check for anomalies

47

00:04:01.110  -->  00:04:04.960
there and make sure that these columns are clean and then proceed.

48

00:04:05.310  -->  00:04:11.640
And then if you do further on down the track on another call and then you can also perform the Q8 check

49

00:04:11.640  -->  00:04:12.320
here.

50

00:04:12.320  -->  00:04:21.300
So in some cases when you're working for very high profile element like report or something where everything

51

00:04:21.300  -->  00:04:26.580
has to be top notch which is going to a conference which is going to the CEO or something like that

52

00:04:26.850  -->  00:04:31.010
then yes then in that case then you have to probably look at every single column.

53

00:04:31.020  -->  00:04:37.860
But also in that case your daughter said would probably be smaller if if you're focusing on something

54

00:04:37.860  -->  00:04:41.780
very very specific some specific result that you want to choose.

55

00:04:41.820  -->  00:04:44.160
So that's that's part one.

56

00:04:44.170  -->  00:04:51.080
A I guess part one is that once again if your data set is huge if you rotatable is has hundreds of columns

57

00:04:51.530  -->  00:04:56.270
then from here you can see that you can limit the number of columns that you want to invoke in your

58

00:04:56.270  -->  00:05:03.110
analysis in your working table so let's say Mike my table had 100 columns but realistically I only needed

59

00:05:03.800  -->  00:05:07.940
these specific columns then that's how I create my working table.

60

00:05:07.940  -->  00:05:12.190
I would create more table with only the columns that I need in mind.

61

00:05:12.380  -->  00:05:18.420
Then I would insert into those columns and I would select only the associated columns from a row table

62

00:05:18.430  -->  00:05:18.760
.

63

00:05:18.980  -->  00:05:23.770
And we haven't been doing that because I wanted to show you how to work with all the callers but it's

64

00:05:23.780  -->  00:05:29.180
very simple you just basically from here you just delete these three columns delete them from here delete

65

00:05:29.180  -->  00:05:30.010
them from here.

66

00:05:30.170  -->  00:05:34.890
And for this script it would feel like as if those columns don't even exist.

67

00:05:35.030  -->  00:05:36.070
And so you can also do that.

68

00:05:36.080  -->  00:05:42.440
I don't recommend doing that because perhaps one day you might need the column that you haven't converted

69

00:05:43.060  -->  00:05:52.880
or Also while converting on a column you say if we had not included the balance column then we wouldn't

70

00:05:52.900  -->  00:05:59.420
have not got in that era where the balance was actually a letter and that helped us avoid like exclude

71

00:05:59.420  -->  00:06:05.840
a whole row which might have been a corrupt throw in a fully corrupt row so that we could including

72

00:06:05.840  -->  00:06:09.100
more columns helps you excludes more arose.

73

00:06:09.100  -->  00:06:14.270
Well that's that's quite interesting including more columns helps to exclude more rows that have anomalies

74

00:06:14.260  -->  00:06:14.650
.

75

00:06:14.660  -->  00:06:18.190
So in that sense it is beneficial to include more calls.

76

00:06:18.200  -->  00:06:24.740
But however if you know for a fact that you don't need 100 columns sometimes you can include just the

77

00:06:24.740  -->  00:06:25.980
ones you need.

78

00:06:26.500  -->  00:06:29.860
So that's the first thing I wanted to mention.

79

00:06:30.130  -->  00:06:35.490
And the second thing I wanted to mention is the Q&A that we have been doing all along.

80

00:06:35.500  -->  00:06:41.660
As you can see here we're counting the rows we're counting them carefully so we're avoiding any duplication

81

00:06:41.690  -->  00:06:48.040
of counting or double counting So in total we've actually verified that it adds up.

82

00:06:48.130  -->  00:06:51.290
Here recounting the number of rows there were actually every time.

83

00:06:51.290  -->  00:06:57.200
And if you ever need to find this specific row again then you can always revert back to the original

84

00:06:57.200  -->  00:07:05.630
file and look for that specific row using these same conditions.

85

00:07:05.720  -->  00:07:11.690
And you can always if you haven't analyzed the errors you can always revert back to them and find them

86

00:07:11.690  -->  00:07:12.080
again.

87

00:07:12.100  -->  00:07:16.880
If worst comes to worst you can just rerun the scripts are you looking for these fellows to run the

88

00:07:16.880  -->  00:07:20.820
script all the way from here like that.

89

00:07:20.840  -->  00:07:32.380
You just run it and then you instead of Delete to put select star here and then you run this part and

90

00:07:32.380  -->  00:07:34.030
you'll find these four rows again.

91

00:07:34.190  -->  00:07:37.790
So it's all repeatable as you can see it was very easy for me to repeat.

92

00:07:37.810  -->  00:07:41.450
Now if I want to continue the process just run this and there we go.

93

00:07:41.450  -->  00:07:49.430
So I've run that I've run that and the select statement so actually to replay sorrowfully.

94

00:07:49.940  -->  00:07:50.960
So it's all repeatable.

95

00:07:50.960  -->  00:07:54.430
So just be careful with that because if somebody is using your working file while you're doing that

96

00:07:54.440  -->  00:07:55.730
that might cause some issues.

97

00:07:55.970  -->  00:08:00.500
You know you can always create a duplicate copy and redo that into a different file.

98

00:08:00.640  -->  00:08:07.490
But the point of this comment is that all the Q8 checks that we're doing there they're really really

99

00:08:07.490  -->  00:08:08.300
helpful.

100

00:08:08.300  -->  00:08:11.530
They will allow you to trace any work that you've done.

101

00:08:11.540  -->  00:08:17.240
And in case there's a mess up somewhere you'll always be able to come back and say No I actually checked

102

00:08:17.240  -->  00:08:17.800
for that.

103

00:08:17.810  -->  00:08:23.600
Here's his proof I actually confirm that it wasn't there or that that wasn't the case the problem was

104

00:08:23.600  -->  00:08:24.840
with the original data.

105

00:08:24.920  -->  00:08:30.320
Or you could use these comments to you know investigate things down the track or for other people that

106

00:08:30.320  -->  00:08:31.170
pick up your work.

107

00:08:31.220  -->  00:08:37.420
They'll be able to look at your quality assurance checks and understand exactly what you did and also

108

00:08:37.510  -->  00:08:39.520
invoke those same checks in their work.

109

00:08:39.520  -->  00:08:45.080
So there are lots of benefits from having this way through all your analysis and that's how this blueprint

110

00:08:45.080  -->  00:08:51.720
is designed it is designed that you minimize the potential for any errors whatsoever.

111

00:08:52.150  -->  00:08:58.310
So I hope you enjoyed this whole blueprint on e-tail for data science as you can see it is not probably

112

00:08:58.310  -->  00:09:05.240
not 100 percent bullet proof but I would say 90 to 95 percent bullet proof helps you pick up as many

113

00:09:05.240  -->  00:09:06.160
errors as possible.

114

00:09:06.160  -->  00:09:13.670
Moreover it gives you tools and pathways for investigating additional errors based on your domain knowledge

115

00:09:13.670  -->  00:09:14.060
.

116

00:09:14.210  -->  00:09:19.600
And now that we've done with this part of the course what is coming up is the homework.

117

00:09:19.610  -->  00:09:23.300
And I really don't envy you.

118

00:09:23.300  -->  00:09:24.020
It's a challenge.

119

00:09:24.020  -->  00:09:26.030
I challenge you to do the homework.

120

00:09:26.060  -->  00:09:27.710
You don't have to but I challenge you.

121

00:09:27.880  -->  00:09:29.360
It's going to be hectic.

122

00:09:29.360  -->  00:09:33.400
It is going to really really hurt.

123

00:09:33.400  -->  00:09:36.740
It's going to hurt your brain doing that.

124

00:09:36.740  -->  00:09:43.340
But at the same time if you can do that homework you probably good for the first year of your career

125

00:09:43.340  -->  00:09:48.050
in terms of the crappy data that you'll ever come across.

126

00:09:48.050  -->  00:09:52.660
And on that note I look forward to seeing you in that homework tutorial.
