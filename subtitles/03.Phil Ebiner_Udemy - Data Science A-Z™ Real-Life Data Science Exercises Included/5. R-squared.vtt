WEBVTT
1

00:00:00.660  -->  00:00:03.690
R-squared this is a very interesting parameter.

2

00:00:03.720  -->  00:00:05.090
A lot of people use it.

3

00:00:05.220  -->  00:00:10.800
And sometimes the user of not knowing the underlying principles of Our Square.

4

00:00:10.800  -->  00:00:17.110
So let's quickly get a understanding of what R-Squared is because we will be using it too.

5

00:00:17.220  -->  00:00:18.680
So we stopped over here.

6

00:00:18.690  -->  00:00:24.630
We talked about the simple introgression been constructed through the ordinary least squares method

7

00:00:24.660  -->  00:00:28.480
where we are Mizzy minimizing the sum of squares of those differences.

8

00:00:28.560  -->  00:00:31.820
Why I minus why I have it.

9

00:00:32.580  -->  00:00:39.840
And basically we are looking at what the model is predicting and what is actually happening at a certain

10

00:00:40.980  -->  00:00:44.720
level of experience or at a certain level for the independent variable.

11

00:00:44.790  -->  00:00:46.310
And we look at that difference.

12

00:00:46.320  -->  00:00:47.730
We're squaring it.

13

00:00:47.730  -->  00:00:51.550
We're summing it for all of the observations we have.

14

00:00:51.920  -->  00:00:53.530
We were counting out some.

15

00:00:53.580  -->  00:01:01.470
And then the line that has them smallest sum will be the best fitting line or will be the simple regression

16

00:01:01.470  -->  00:01:02.000
model.

17

00:01:02.280  -->  00:01:12.420
So let's get rid of that call out for a second and let's remember this value so we'll need it just now

18

00:01:12.440  -->  00:01:12.610
.

19

00:01:12.720  -->  00:01:16.160
And in fact this value has a name.

20

00:01:16.170  -->  00:01:19.420
It's called the sum of squares of residuals.

21

00:01:19.860  -->  00:01:21.060
So we'll just label.

22

00:01:21.090  -->  00:01:27.210
SS Yes and now let's do something else before Chuck we'll get rid of those lines so this is our rule

23

00:01:27.210  -->  00:01:31.950
chart with those are patients and instead of drawing the regression line let's draw the average line

24

00:01:31.970  -->  00:01:32.030
.

25

00:01:32.070  -->  00:01:36.440
So this is your average salary across all of your observations.

26

00:01:36.450  -->  00:01:41.430
Now let's do the same exercise and project our observations onto this line.

27

00:01:41.910  -->  00:01:50.220
And now if we calculate the distances the read distances and we square them then we'll it will look

28

00:01:50.220  -->  00:01:57.150
like this will be a Y minus Y average squared and then we'll take the sum of all these distances.

29

00:01:57.150  -->  00:02:02.630
Now this value also has a name and it's called the total sum of squares.

30

00:02:02.970  -->  00:02:10.800
And what are squared is our R-squared equals to 1 minus sum of squares of residuals divided by total

31

00:02:10.860  -->  00:02:12.660
sum of squares.

32

00:02:12.730  -->  00:02:20.630
And so what this is saying is that there will always be a sum of our total sum of squares right.

33

00:02:20.640  -->  00:02:26.170
So the average not all your records are not always going to be all equal to the average although it's

34

00:02:26.170  -->  00:02:27.330
a very boring data set.

35

00:02:27.330  -->  00:02:33.050
So some of the total sum of squares will you know will always be some sort of value.

36

00:02:33.420  -->  00:02:39.150
And what you're trying to do with your regression is you're trying to fiddle a line to minimize as we

37

00:02:39.150  -->  00:02:44.480
discussed previously to minimize the sum of squares of residuals to make it as small as possible.

38

00:02:44.550  -->  00:02:44.790
Right.

39

00:02:44.790  -->  00:02:52.170
So in a way the way to think of it intuitively is the average line which we have on the Chye right now

40

00:02:52.260  -->  00:02:58.050
is also a type of trendline is just horizontal right.

41

00:02:58.050  -->  00:03:03.840
It's not sloped it's just a horizontal trendline but it kind of also is so you can think of it as a

42

00:03:04.970  -->  00:03:10.420
as a model that is fit to a dot it's not the best model but you know it's just why average.

43

00:03:10.500  -->  00:03:16.170
So what you're trying to do by fitting a sloped line and minimizing the sum of squares of residuals

44

00:03:16.350  -->  00:03:18.530
is you're trying to fit the best line.

45

00:03:18.750  -->  00:03:26.370
And what R-Squared is telling us is it's telling us how good is your line compared to the you know average

46

00:03:26.370  -->  00:03:28.300
line which anybody can think of.

47

00:03:28.290  -->  00:03:32.270
Right so it's not hard to take the average you take a calculator you get the average.

48

00:03:32.280  -->  00:03:36.770
But in order to fit the best fitting line you have to actually run a linear regression.

49

00:03:36.900  -->  00:03:38.970
And if you look at the formula.

50

00:03:39.120  -->  00:03:45.570
So the you tried to minimize s s residual right so some somewhat because of residual So on the right

51

00:03:45.570  -->  00:03:47.160
over there.

52

00:03:47.310  -->  00:03:54.580
That's part of the formula as you minimize SS residual it goes smaller it always becomes smaller.

53

00:03:54.840  -->  00:04:01.580
And that way as this residual boy I do bias's total become smaller and one minus SS residual divided

54

00:04:01.590  -->  00:04:03.990
by his total becomes greater.

55

00:04:03.990  -->  00:04:07.640
So ideally if your SS residual is zero.

56

00:04:07.680  -->  00:04:14.340
So basically your trend line that you're modeling goes through all your records then in that case R-Squared

57

00:04:14.340  -->  00:04:16.650
is equal to 1 and that's the ideal scenario.

58

00:04:16.950  -->  00:04:19.440
But that not normally never happens.

59

00:04:19.440  -->  00:04:26.790
So the closer R-Squared is to 1 the better the further away it is from 1 so the lower It is the worse

60

00:04:26.790  -->  00:04:27.370
.

61

00:04:27.380  -->  00:04:29.760
Our good question is can R-squared ever be negative.

62

00:04:29.760  -->  00:04:37.020
The answer is yes of course our can be negative and that is if your for instance says residual actually

63

00:04:37.530  -->  00:04:44.920
fits your daughter worse than your average line it's hard to do.

64

00:04:45.240  -->  00:04:50.160
You've got to you've got to try because if you don't know if you just draw a random line and yes then

65

00:04:51.060  -->  00:04:56.940
facing the wrong way so here would be going downwards and probably yes you would get in a negative R-squared

66

00:04:57.050  -->  00:04:57.090
.

67

00:04:57.090  -->  00:04:58.230
So that's.

68

00:04:58.230  -->  00:05:00.680
That means that the model is completely broken.

69

00:05:00.690  -->  00:05:03.390
It's better to the average and even to the model.

70

00:05:03.540  -->  00:05:08.100
But normally so our score is normally between zero and one never gets greater than one.

71

00:05:08.340  -->  00:05:10.390
And the closer is to one the better
