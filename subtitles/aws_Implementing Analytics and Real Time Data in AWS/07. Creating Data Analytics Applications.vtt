WEBVTT
1
00:00:00.940 --> 00:00:05.140
[Autogenerated] Let's start looking at Ken is is analytic applications.

2
00:00:05.140 --> 00:00:08.830
We just had a brief overview on what they are and how they're used,

3
00:00:08.830 --> 00:00:12.680
but let's go through actually creating one so that we can see how it's done

4
00:00:12.680 --> 00:00:15.780
and how they're actually used in real life scenarios.

5
00:00:15.780 --> 00:00:19.070
So I'm gonna create an analytics application and we

6
00:00:19.070 --> 00:00:20.720
have to create our application.

7
00:00:20.720 --> 00:00:21.710
So I have to give it a name.

8
00:00:21.710 --> 00:00:23.740
I'll call it Pluralsight we can.

9
00:00:23.740 --> 00:00:27.300
Give a description and then we choose our runtime so you can see

10
00:00:27.300 --> 00:00:31.140
we either can do SQL or we can do Apache Flink.

11
00:00:31.140 --> 00:00:35.080
Now SQL is obviously structured query language and then Flink is its

12
00:00:35.080 --> 00:00:39.100
own open source framework for distributed processing.

13
00:00:39.100 --> 00:00:41.140
So I'll do SQL for this.

14
00:00:41.140 --> 00:00:43.140
I'll create our application.

15
00:00:43.140 --> 00:00:43.840
And there you go.

16
00:00:43.840 --> 00:00:47.340
I just made our Data analytics application.

17
00:00:47.340 --> 00:00:51.340
Now what this does is it brings us to our source screen,

18
00:00:51.340 --> 00:00:56.310
and our source in this aspect is some type of streaming data that

19
00:00:56.310 --> 00:00:59.540
we're pushing into our analytic application.

20
00:00:59.540 --> 00:01:03.980
And what happens is when we connect are streaming data it in

21
00:01:03.980 --> 00:01:07.940
just data from the stream into a virtual table,

22
00:01:07.940 --> 00:01:12.240
an input stream within the Data Analytics application,

23
00:01:12.240 --> 00:01:15.810
we run our SQL analytic commands on that data,

24
00:01:15.810 --> 00:01:20.340
and then we can output data through an output stream on the back end.

25
00:01:20.340 --> 00:01:24.340
So if I click on connect streaming data we can choose a source now

26
00:01:24.340 --> 00:01:27.740
we can configure a new stream if we wanted to,

27
00:01:27.740 --> 00:01:30.140
or we can choose an existing one.

28
00:01:30.140 --> 00:01:36.440
Now, I went ahead and I created a data firehose and I called it test.

29
00:01:36.440 --> 00:01:40.540
And what I'll do is I'll start sending data over this delivery stream.

30
00:01:40.540 --> 00:01:44.560
Let me go back to my analytics, and then I'm gonna choose firehose.

31
00:01:44.560 --> 00:01:46.010
So this is important.

32
00:01:46.010 --> 00:01:50.580
The sources are either a data stream or a fire hose delivery stream.

33
00:01:50.580 --> 00:01:52.840
That's important to know for the exam.

34
00:01:52.840 --> 00:01:56.040
So let me choose my test delivery stream.

35
00:01:56.040 --> 00:02:00.370
You can see in our SQL queries We would reference this source as

36
00:02:00.370 --> 00:02:04.520
this value right here and then we can do some pre processing of

37
00:02:04.520 --> 00:02:07.440
records with Lambda if we had one to do,

38
00:02:07.440 --> 00:02:11.210
and then we have to grant access permission so we can create or update an

39
00:02:11.210 --> 00:02:15.030
existing role or choose a different role that can be assumed.

40
00:02:15.030 --> 00:02:18.240
So I already have one created, and I'll use that.

41
00:02:18.240 --> 00:02:22.650
And then the last part down here is pretty meat we can discover our schema.

42
00:02:22.650 --> 00:02:23.650
So if I click this,

43
00:02:23.650 --> 00:02:28.050
what's going on is that Data Analytics is going through

44
00:02:28.050 --> 00:02:30.640
this fire hose that we just connected.

45
00:02:30.640 --> 00:02:35.240
It's looking at that ingested data, and it's trying to discover a schema.

46
00:02:35.240 --> 00:02:35.810
And there you go.

47
00:02:35.810 --> 00:02:39.460
You can see it's detected JSON format and apply to schema.

48
00:02:39.460 --> 00:02:40.820
And if you recall,

49
00:02:40.820 --> 00:02:45.930
the data that gets passed in is basically information about ticker symbols for

50
00:02:45.930 --> 00:02:51.210
the stock market so we can see that it was able to distinguished ticker symbol

51
00:02:51.210 --> 00:02:57.610
sector change and then price as different fields that we can now perform SQL

52
00:02:57.610 --> 00:03:01.440
Analytics on as if it was relational table.

53
00:03:01.440 --> 00:03:06.370
So I'll save and continue, and that brings us to our SQL editor now.

54
00:03:06.370 --> 00:03:09.440
I'm not going to run it right now because I don't want to incur any cost,

55
00:03:09.440 --> 00:03:11.940
but we would start our application,

56
00:03:11.940 --> 00:03:16.360
and then you can save and run SQL from the console directly,

57
00:03:16.360 --> 00:03:20.540
and it's using that source data that we connected from our fire hose.

58
00:03:20.540 --> 00:03:24.480
So after I start the application, I can go through in just that,

59
00:03:24.480 --> 00:03:29.420
streaming data are input stream in Data Analytics puts it

60
00:03:29.420 --> 00:03:33.000
into a virtual table that essentially allows us to perform

61
00:03:33.000 --> 00:03:34.940
relational work on there.

62
00:03:34.940 --> 00:03:36.320
We perform that work,

63
00:03:36.320 --> 00:03:39.690
and then we output that result to an output stream

64
00:03:39.690 --> 00:03:42.040
that could go to any destination.

65
00:03:42.040 --> 00:03:46.360
So if we look at destinations, you can see we have three options.

66
00:03:46.360 --> 00:03:50.780
We have another data stream we can go back into fire hose

67
00:03:50.780 --> 00:03:53.220
or we can pass it to a lambda function.

68
00:03:53.220 --> 00:03:55.940
So depending on your use case,

69
00:03:55.940 --> 00:04:00.340
they offer three different options at this point in time,

70
00:04:00.340 --> 00:04:03.650
and you can see right here we have our in application stream.

71
00:04:03.650 --> 00:04:08.640
So this is for that continuous flow that we get from ingesting our records.

72
00:04:08.640 --> 00:04:12.650
We would create an in applications stream name and then we configure

73
00:04:12.650 --> 00:04:16.890
the output format so don't get tripped up over the name in

74
00:04:16.890 --> 00:04:20.320
application does not necessarily mean input.

75
00:04:20.320 --> 00:04:22.100
It means input and output.

76
00:04:22.100 --> 00:04:27.940
So there's an in application input stream and an in application output stream.

77
00:04:27.940 --> 00:04:31.440
And these error, what allow us to perform their relation will work.

78
00:04:31.440 --> 00:04:34.730
So I'm gonna cancel out of this, and that's actually going to do it.

79
00:04:34.730 --> 00:04:37.040
So it was a pretty short and sweet walk through.

80
00:04:37.040 --> 00:04:41.540
We just looked at how we can connect a source which we did with fire hose.

81
00:04:41.540 --> 00:04:44.750
We allowed it to automatically discover a schema.

82
00:04:44.750 --> 00:04:50.190
And then we looked at how we can run SQL commands on those streaming data,

83
00:04:50.190 --> 00:04:54.540
that real time data as if it was a relation all table.

84
00:04:54.540 --> 00:04:57.720
And we talked about how the in application streams

85
00:04:57.720 --> 00:04:59.640
are what allowed us to do that.

86
00:04:59.640 --> 00:05:06.000
So let's end here, and then we'll pick back up with video streams in the next clip.

