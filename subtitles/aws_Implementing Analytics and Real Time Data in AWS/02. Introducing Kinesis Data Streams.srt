1
00:00:00,740 --> 00:00:02,050
[Autogenerated] all right, welcome back.

2
00:00:02,050 --> 00:00:03,300
Let's dive into Ken.

3
00:00:03,300 --> 00:00:05,350
Is is data streams now.

4
00:00:05,350 --> 00:00:08,990
Can is is data streams are used to collect and process

5
00:00:08,990 --> 00:00:12,940
large streams of data records in real time,

6
00:00:12,940 --> 00:00:16,330
and the beauty of this services that it's accessible via

7
00:00:16,330 --> 00:00:19,390
of VPC or even the public internet.

8
00:00:19,390 --> 00:00:21,260
If you open it up now,

9
00:00:21,260 --> 00:00:25,750
Another major factor is that it has the ability to scale

10
00:00:25,750 --> 00:00:28,840
to keep up with data being produced.

11
00:00:28,840 --> 00:00:30,250
So based off metrics,

12
00:00:30,250 --> 00:00:35,230
we collect we can set our stream sizes to a certain amount of shards,

13
00:00:35,230 --> 00:00:38,990
which we'll talk about here in a little bit in order to keep up with

14
00:00:38,990 --> 00:00:42,040
the massive amounts of data we might be ingesting.

15
00:00:42,040 --> 00:00:46,790
Now can is is data streams also allow us to ingest large amounts of data,

16
00:00:46,790 --> 00:00:49,560
but it also provides durable storage,

17
00:00:49,560 --> 00:00:53,640
and there's a default retention period of 24 hours.

18
00:00:53,640 --> 00:00:55,380
So if there's any failures,

19
00:00:55,380 --> 00:01:00,070
you'll have up to 24 hours by default to replay those records and

20
00:01:00,070 --> 00:01:03,840
maybe re process the data that's within them.

21
00:01:03,840 --> 00:01:05,890
Now, as far as structure goes,

22
00:01:05,890 --> 00:01:10,010
groups of data records and data records are what they sound like.

23
00:01:10,010 --> 00:01:13,640
They're pieces of data that we put into our streams.

24
00:01:13,640 --> 00:01:16,440
They form data streams,

25
00:01:16,440 --> 00:01:19,740
and those get distributed into different shards and shards.

26
00:01:19,740 --> 00:01:19,850
Error.

27
00:01:19,850 --> 00:01:22,930
How we scale our streams for processing.

28
00:01:22,930 --> 00:01:25,450
And we'll look at that here in the console, coming up.

29
00:01:25,450 --> 00:01:29,090
So now that we have a general understanding of data streams,

30
00:01:29,090 --> 00:01:30,410
let's look at an example.

31
00:01:30,410 --> 00:01:33,650
Architecture er So in this diagram, we haven't E.

32
00:01:33,650 --> 00:01:33,850
C.

33
00:01:33,850 --> 00:01:36,900
Two instance hosting a Java application,

34
00:01:36,900 --> 00:01:39,890
and we'll go ahead and assume that this application,

35
00:01:39,890 --> 00:01:43,630
in easy to instantiate as an appropriate I am real

36
00:01:43,630 --> 00:01:46,740
attached to it that allows can, is, is actions.

37
00:01:46,740 --> 00:01:49,500
So what we're doing is, as we're processing data,

38
00:01:49,500 --> 00:01:53,190
our job application is putting records in to arcane

39
00:01:53,190 --> 00:01:56,040
is is data streams in real time.

40
00:01:56,040 --> 00:01:59,370
And with that, we have our consumers on the right,

41
00:01:59,370 --> 00:02:04,590
which are Lambda Functions, and they to get the record in real time.

42
00:02:04,590 --> 00:02:09,840
So a soon as it's put into the stream were able to pull it immediately out,

43
00:02:09,840 --> 00:02:12,670
and then those lamb does go ahead and take that data,

44
00:02:12,670 --> 00:02:16,210
perform some type of action and then store it in S3.

45
00:02:16,210 --> 00:02:18,040
Now, a few things.

46
00:02:18,040 --> 00:02:21,520
The job a application on the EEC, to instance,

47
00:02:21,520 --> 00:02:28,040
is what is known as a producer, so it produces records to put into the streams.

48
00:02:28,040 --> 00:02:30,570
And then our lambda czar What error called consumers.

49
00:02:30,570 --> 00:02:33,840
So they consume the records off of the stream.

50
00:02:33,840 --> 00:02:38,540
So three critical components of a data stream are a producer.

51
00:02:38,540 --> 00:02:42,260
The stream itself with the different shards and then

52
00:02:42,260 --> 00:02:44,740
our consumers on the back end.

53
00:02:44,740 --> 00:02:45,050
Now,

54
00:02:45,050 --> 00:02:47,420
one more thing before we wrap this up and move on to

55
00:02:47,420 --> 00:02:51,840
the console in this part here, where we're getting the records,

56
00:02:51,840 --> 00:02:57,470
these error two separate shards so were scaling out across multiple lamb does,

57
00:02:57,470 --> 00:03:01,940
based on our shard configuration that we made within our data stream.

58
00:03:01,940 --> 00:03:13,000
Let's go ahead and cut here and in the next clip will pick back up within the console and start creating data streams within AWS.

