WEBVTT
1
00:00:00.040 --> 00:00:02.030
- [Instructor] Let's take a look at the use cases

2
00:00:02.030 --> 00:00:04.060
where either a container,

3
00:00:04.060 --> 00:00:07.030
which is running as part of your service, fails,

4
00:00:07.030 --> 00:00:10.080
or a node on which the container is running, fails.

5
00:00:10.080 --> 00:00:14.030
Now, a happy scenario is not the usual scenario.

6
00:00:14.030 --> 00:00:17.060
A typical scenario is where container or node will fail.

7
00:00:17.060 --> 00:00:19.010
Now Docker Swarm Mode

8
00:00:19.010 --> 00:00:21.040
has really cool functionality for that.

9
00:00:21.040 --> 00:00:23.020
Let's take a look at that.

10
00:00:23.020 --> 00:00:25.040
So the first scenario that we're going to look at

11
00:00:25.040 --> 00:00:28.060
is, let's say, my container fails.

12
00:00:28.060 --> 00:00:29.040
Okay.

13
00:00:29.040 --> 00:00:32.020
In this case, the container on the bottom node

14
00:00:32.020 --> 00:00:34.050
in the middle is failing.

15
00:00:34.050 --> 00:00:38.040
Swarm Mode recognizes that the desired number of replicas,

16
00:00:38.040 --> 00:00:41.050
which was three, and the actual, which was two,

17
00:00:41.050 --> 00:00:43.020
is not matching.

18
00:00:43.020 --> 00:00:47.020
So it'll reschedule the container on a different node.

19
00:00:47.020 --> 00:00:48.030
As simple as that.

20
00:00:48.030 --> 00:00:50.030
So that is exactly the declarative state

21
00:00:50.030 --> 00:00:53.040
of the service that we were talking about earlier.

22
00:00:53.040 --> 00:00:55.020
Now, what happens if a node fails?

23
00:00:55.020 --> 00:00:56.070
Sure, node can fail as well.

24
00:00:56.070 --> 00:00:59.010
A network connectivity, there are hundreds of reasons

25
00:00:59.010 --> 00:01:00.070
by which a node may fail.

26
00:01:00.070 --> 00:01:03.070
And, let's say, our container was running on that node.

27
00:01:03.070 --> 00:01:06.020
Okay. Once again, Swarm Mode recognizes

28
00:01:06.020 --> 00:01:09.040
that the desired number of replicas of a service

29
00:01:09.040 --> 00:01:13.020
is not matching the actual number of replicas of the service

30
00:01:13.020 --> 00:01:14.080
and it'll just reconcile

31
00:01:14.080 --> 00:01:19.020
and schedule a new container on a different node.

32
00:01:19.020 --> 00:01:22.040
And if that original node comes back in the cluster,

33
00:01:22.040 --> 00:01:24.050
it's treated as just under the node then.

34
00:01:24.050 --> 00:01:27.090
Okay, so let's take a look at some of this in action.

35
00:01:27.090 --> 00:01:29.070
I'm here in my terminal

36
00:01:29.070 --> 00:01:32.090
and if you recall, we left where we were looking at

37
00:01:32.090 --> 00:01:36.000
the state of containers that are running across

38
00:01:36.000 --> 00:01:38.010
my Docker Machine cluster here.

39
00:01:38.010 --> 00:01:40.010
So let's clear up the screen here.

40
00:01:40.010 --> 00:01:43.030
I'm already ssh into my manager1 here.

41
00:01:43.030 --> 00:01:44.070
So let's take a look at

42
00:01:44.070 --> 00:01:47.030
where are my containers really running.

43
00:01:47.030 --> 00:01:50.000
So I'm going to say, docker service ps web.

44
00:01:50.000 --> 00:01:54.080
Now it says I have web.1, web.2, web.3

45
00:01:54.080 --> 00:01:57.060
running on these different nodes here.

46
00:01:57.060 --> 00:01:58.090
Okay. That's good.

47
00:01:58.090 --> 00:02:01.050
Our use case is really to say that,

48
00:02:01.050 --> 00:02:03.090
okay, my container is failing.

49
00:02:03.090 --> 00:02:06.030
Now, since we are on manager1,

50
00:02:06.030 --> 00:02:09.010
we will have to exit out of manager1

51
00:02:09.010 --> 00:02:12.020
and then what we'll do is, for our convenience,

52
00:02:12.020 --> 00:02:16.020
we will run into worker1 or, say, manager3.

53
00:02:16.020 --> 00:02:20.010
So we will ssh into manager3 here

54
00:02:20.010 --> 00:02:22.030
and we can say,

55
00:02:22.030 --> 00:02:24.040
show me the list of containers that are running.

56
00:02:24.040 --> 00:02:25.080
And rightly so,

57
00:02:25.080 --> 00:02:28.070
it says that this container is running over here.

58
00:02:28.070 --> 00:02:31.010
Okay? Now, we can kill the container

59
00:02:31.010 --> 00:02:32.040
but in order to show you

60
00:02:32.040 --> 00:02:34.000
that the container has actually died

61
00:02:34.000 --> 00:02:36.080
and the number of replicas for a service are less,

62
00:02:36.080 --> 00:02:39.040
let's get out of this shell here.

63
00:02:39.040 --> 00:02:42.060
And what we will do is we will say,

64
00:02:42.060 --> 00:02:46.040
docker-machine ssh manager1.

65
00:02:46.040 --> 00:02:48.090
And now to that we will say,

66
00:02:48.090 --> 00:02:51.030
docker service ls.

67
00:02:51.030 --> 00:02:55.010
So this says three of three replicas are running.

68
00:02:55.010 --> 00:02:58.040
I can give the command, now, to manager3,

69
00:02:58.040 --> 00:03:00.060
which is where one of my replicas is running.

70
00:03:00.060 --> 00:03:03.080
I can say, docker container rm

71
00:03:03.080 --> 00:03:05.060
and I have to brute force it.

72
00:03:05.060 --> 00:03:08.020
So if we scroll up here,

73
00:03:08.020 --> 00:03:09.050
here is my container ID.

74
00:03:09.050 --> 00:03:10.070
I'm going to copy it,

75
00:03:10.070 --> 00:03:12.080
I'm going to paste it like this.

76
00:03:12.080 --> 00:03:15.020
Now I'm purposely killing it.

77
00:03:15.020 --> 00:03:17.070
Now, this may happen by accident in your case

78
00:03:17.070 --> 00:03:20.030
but this is a good simulation of that.

79
00:03:20.030 --> 00:03:22.030
So the container has been killed.

80
00:03:22.030 --> 00:03:24.060
Let's take a look at the number of replicas.

81
00:03:24.060 --> 00:03:28.010
You can see that only two out of three replicas are running.

82
00:03:28.010 --> 00:03:30.060
So the Docker Swarm Manager kicks in.

83
00:03:30.060 --> 00:03:34.080
It says the actual versus desired is not matching.

84
00:03:34.080 --> 00:03:36.080
It's going to download the jboss/wildfly image,

85
00:03:36.080 --> 00:03:38.050
if not already downloaded.

86
00:03:38.050 --> 00:03:40.030
It's going to run the container

87
00:03:40.030 --> 00:03:42.090
and it's going to look good for us again.

88
00:03:42.090 --> 00:03:45.090
Let's take a look at the status of service.

89
00:03:45.090 --> 00:03:48.070
And rightly so, it is back to three and three.

90
00:03:48.070 --> 00:03:52.060
And now show me the tasks for the service web.

91
00:03:52.060 --> 00:03:54.070
It says the container on the manager

92
00:03:54.070 --> 00:03:56.080
failed about 35 seconds ago.

93
00:03:56.080 --> 00:03:59.090
And now the workers one, two and three,

94
00:03:59.090 --> 00:04:01.090
are running a container each.

95
00:04:01.090 --> 00:04:04.070
So see how easy it is, the containers

96
00:04:04.070 --> 00:04:07.000
and the number of replicas for a service

97
00:04:07.000 --> 00:04:09.010
are maintained consistently.

98
00:04:09.010 --> 00:04:11.010
Now let's take a look at the other use case

99
00:04:11.010 --> 00:04:13.090
where we can simulate if a node goes down.

100
00:04:13.090 --> 00:04:15.050
Okay.

101
00:04:15.050 --> 00:04:19.030
So I'm going to take a look at docker-machine ls.

102
00:04:19.030 --> 00:04:22.040
It's showing me three managers and three workers.

103
00:04:22.040 --> 00:04:25.020
And if you recall from the previous output,

104
00:04:25.020 --> 00:04:29.070
my containers are running on worker one, two and three.

105
00:04:29.070 --> 00:04:32.020
Now to simulate a node failure,

106
00:04:32.020 --> 00:04:36.060
what I'll do is I will try to shut down my worker1.

107
00:04:36.060 --> 00:04:39.060
But before that, I want to make sure

108
00:04:39.060 --> 00:04:42.000
I can see the number of replicas of the service.

109
00:04:42.000 --> 00:04:43.050
Rightly so, three.

110
00:04:43.050 --> 00:04:46.070
So now let's take docker-machine

111
00:04:46.070 --> 00:04:49.080
and stop worker1.

112
00:04:49.080 --> 00:04:51.050
It's trying to stop worker1,

113
00:04:51.050 --> 00:04:54.020
which is basically simulating a node failure.

114
00:04:54.020 --> 00:04:55.050
And that was too fast.

115
00:04:55.050 --> 00:04:57.000
As soon as we stopped the node,

116
00:04:57.000 --> 00:04:59.000
the container was automatically fired up

117
00:04:59.000 --> 00:05:00.070
on a different node.

118
00:05:00.070 --> 00:05:03.030
The node went down and we didn't really get time

119
00:05:03.030 --> 00:05:06.020
to show that the desired versus actual

120
00:05:06.020 --> 00:05:08.000
are not really matching.

121
00:05:08.000 --> 00:05:09.080
But let's see what might have happened.

122
00:05:09.080 --> 00:05:14.050
So if you look at ps web,

123
00:05:14.050 --> 00:05:18.040
and what happened is, now, one of the containers

124
00:05:18.040 --> 00:05:20.040
is running on manager1.

125
00:05:20.040 --> 00:05:23.030
And the reason this change was so quick and rapid

126
00:05:23.030 --> 00:05:25.070
is because on manager1,

127
00:05:25.070 --> 00:05:28.060
the image had already been downloaded.

128
00:05:28.060 --> 00:05:30.030
Had this been the case where the container

129
00:05:30.030 --> 00:05:32.020
was started on a new node,

130
00:05:32.020 --> 00:05:34.030
where the image has not been downloaded,

131
00:05:34.030 --> 00:05:35.090
it would have taken some time.

132
00:05:35.090 --> 00:05:37.030
But that's the beauty of it.

133
00:05:37.030 --> 00:05:40.010
You are completely transparent to the part,

134
00:05:40.010 --> 00:05:44.040
the Docker Swarm Manager will create the service for you,

135
00:05:44.040 --> 00:05:46.010
maintain the service for you,

136
00:05:46.010 --> 00:05:48.060
it'll download the image, if need be.

137
00:05:48.060 --> 00:05:51.030
But in this section, essentially what we saw,

138
00:05:51.030 --> 00:05:54.040
if a container fails or if a node fails,

139
00:05:54.040 --> 00:05:57.040
either way Docker Swarm reconciles

140
00:05:57.040 --> 00:05:59.040
and makes sure the set number of replicas

141
00:05:59.040 --> 00:06:00.000
are running for you.

