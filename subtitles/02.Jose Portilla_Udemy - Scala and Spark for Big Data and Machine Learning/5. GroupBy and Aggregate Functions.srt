1
00:00:05,270 --> 00:00:10,760
Hello everyone and welcome to the data framed group by an aggregate functions lecture this lecture.

2
00:00:10,760 --> 00:00:15,530
We're going to show you how to use group by way of a data frame as well as actually perform aggregate

3
00:00:15,530 --> 00:00:18,950
functions after a GROUP BY statement on your data frame.

4
00:00:18,950 --> 00:00:22,220
Let's go ahead and jump to our text editor to get started.

5
00:00:22,220 --> 00:00:27,220
All right so here I am at the text editor the Skala a script that goes along with this lecture.

6
00:00:27,230 --> 00:00:27,920
Isn't there a spark.

7
00:00:27,930 --> 00:00:34,310
Data frame's folder and it's called by underscore and underscore A-G for aggregate methods or functions.

8
00:00:34,460 --> 00:00:38,870
And you can check it out here we're basically going to be showing you how to use some basic group by

9
00:00:38,870 --> 00:00:45,800
functionality meaning to actually group rows by a column and then performing some sort of every function

10
00:00:45,800 --> 00:00:46,170
on it.

11
00:00:46,190 --> 00:00:50,540
We'll show you that and then we'll also show you how to actually perform some other aggregate functions

12
00:00:50,810 --> 00:00:56,150
using the select statement with some other function names there and then we also show you briefly how

13
00:00:56,150 --> 00:01:02,510
did the use ORDER BY NOW something to note here is we're going to be using the sales that CSC file that's

14
00:01:02,540 --> 00:01:04,570
also located in the spark data frames.

15
00:01:04,610 --> 00:01:09,920
So go ahead and just copy these import lines in the DFA setup line that we have everything set up the

16
00:01:09,920 --> 00:01:11,360
same way I do.

17
00:01:11,360 --> 00:01:14,460
I went ahead and created a new Skala script called L.E..

18
00:01:14,590 --> 00:01:20,420
For a lecture that Skala and I copied the first three code lines so starting a spark session importing

19
00:01:20,420 --> 00:01:25,130
that and then I'm going to go ahead and read the sales that see s feed data.

20
00:01:25,340 --> 00:01:29,810
Let me go ahead and print the schema and show you what this data frame looks like so we understand we're

21
00:01:29,810 --> 00:01:31,370
actually working with here.

22
00:01:31,450 --> 00:01:35,850
Now go ahead and say DFI print schema and then see just say DFT.

23
00:01:35,920 --> 00:01:39,120
Sure because this is a relatively small sales file.

24
00:01:39,140 --> 00:01:41,320
Or C every file we should be able to see it.

25
00:01:41,390 --> 00:01:45,300
Let's go ahead and load Elly's see that Skala and run that.

26
00:01:45,300 --> 00:01:47,870
And as you can see already started my Sparc shell.

27
00:01:47,900 --> 00:01:50,220
OK expanding this terminal to see the output.

28
00:01:50,570 --> 00:01:56,120
You can see that we have a company is a string a person as a string and then sales as an integer.

29
00:01:56,240 --> 00:01:57,530
And here we have company.

30
00:01:57,560 --> 00:01:59,840
So we have Google Microsoft and Facebook.

31
00:01:59,900 --> 00:02:04,490
We have a particular person that works at that company and then their sales number for that quarter

32
00:02:04,490 --> 00:02:05,500
or a year.

33
00:02:05,900 --> 00:02:11,600
So we're going to want to do is group by certain columns and then perform an aggregate function on the

34
00:02:11,600 --> 00:02:13,800
remaining numerical columns.

35
00:02:13,820 --> 00:02:19,220
So for instance let's say we wanted to group by the company column so grouped together everyone from

36
00:02:19,220 --> 00:02:23,150
Google everyone from Microsoft and own from maybe for Facebook.

37
00:02:23,150 --> 00:02:31,500
So we can do is say DMF dot and then you're going to say group capital B Y group by and then you put

38
00:02:31,860 --> 00:02:32,650
the column name.

39
00:02:32,670 --> 00:02:38,960
No dollar sign just a string of the column name for instance company let's go ahead and save that.

40
00:02:39,250 --> 00:02:42,660
And if we don't ask for anything on top of that group by you won't get anything out

41
00:02:45,950 --> 00:02:50,840
what spark will report back to you is that you have a sequel and what's known as a relational grouped

42
00:02:50,950 --> 00:02:51,720
data set.

43
00:02:51,890 --> 00:02:56,170
So that's what it calls this group by object a relational grouped data set.

44
00:02:56,210 --> 00:03:01,250
Once you have that group dataset where we can go ahead and do is call aggregate functions off of it.

45
00:03:01,250 --> 00:03:06,470
Now there's many functions you can use but some of the most common ones are things such as find the

46
00:03:06,470 --> 00:03:12,650
mean and we can go ahead and add show to this command so that we actually see it as an output.

47
00:03:12,650 --> 00:03:17,440
So if I run this again I mean and that show will go ahead and run this.

48
00:03:17,540 --> 00:03:23,030
And what SPARC will do is it will take any numerical columns it will go ahead and take the mean or average.

49
00:03:23,030 --> 00:03:25,550
And note how it actually changes the column name.

50
00:03:25,550 --> 00:03:26,830
Here it says AEG.

51
00:03:26,830 --> 00:03:27,660
Sales.

52
00:03:27,670 --> 00:03:30,830
And now we can see the mean sales across the company.

53
00:03:30,830 --> 00:03:35,960
And later on you can add options if you don't want to see this long of a floating point number but you

54
00:03:35,960 --> 00:03:38,090
can also do things such as count.

55
00:03:38,090 --> 00:03:43,880
So if we wanted to actually count the number of instances we can say group buy in off that group project

56
00:03:43,910 --> 00:03:49,430
called accounts so we have three employees from Google to Facebook and three at Microsoft and the other

57
00:03:49,430 --> 00:03:54,320
ones that are really common to use are things such as Macs that are going to go ahead and just copy

58
00:03:54,320 --> 00:03:54,560
this.

59
00:03:54,560 --> 00:03:57,310
We can see all of them at once.

60
00:03:57,320 --> 00:03:59,440
Men is another really common one.

61
00:03:59,930 --> 00:04:01,100
And then some.

62
00:04:01,100 --> 00:04:05,870
All right so all we're going to be doing is reporting back the max values and the minimum values as

63
00:04:05,870 --> 00:04:10,670
well as the sum of all these sales values are these numerical columns when you group the data by the

64
00:04:10,670 --> 00:04:11,600
company call.

65
00:04:11,900 --> 00:04:16,750
So let's go ahead and save that and run it and we'll see three little reports back.

66
00:04:16,940 --> 00:04:19,800
Here we have the Mac sales.

67
00:04:19,850 --> 00:04:25,430
So whoever had the Mac sale at google it was 340 whoever had the Mac sale at Facebook it was 8 7 the

68
00:04:25,580 --> 00:04:30,380
600 at Microsoft et cetera they can see the minimum sales amount and then we can see the total sum of

69
00:04:30,380 --> 00:04:35,900
sales because if Facebook had the total sum of the sales to be the largest.

70
00:04:35,930 --> 00:04:36,460
All right.

71
00:04:36,710 --> 00:04:39,540
So we can go ahead and show you a couple of more aggregate functions.

72
00:04:39,590 --> 00:04:43,240
If you go to the actual lecture notes there's a link here.

73
00:04:43,250 --> 00:04:50,250
So if you scroll to the left and then go down you will see a link to the aggregate functions page in

74
00:04:50,250 --> 00:04:51,660
the documentation.

75
00:04:51,660 --> 00:04:55,220
And here you can see a bunch of examples instead of just typing all of these out.

76
00:04:55,350 --> 00:04:58,500
I'm going to go ahead and copy these.

77
00:04:58,690 --> 00:05:02,450
And these are aggregate functions that you call off a column.

78
00:05:02,640 --> 00:05:09,880
And unlike a group by your just calling aggregate function off a particular column not by a group by

79
00:05:09,880 --> 00:05:10,900
statement.

80
00:05:10,900 --> 00:05:18,130
So what that means is for instance let's say I wanted to get the sum of all the sales not grouped by

81
00:05:18,130 --> 00:05:24,700
company then I would say something like this DFA that selects and here I would put in my aggregate function.

82
00:05:24,700 --> 00:05:27,230
So you can see here their standard deviation.

83
00:05:27,550 --> 00:05:34,090
But in my case I could say something like this if that selects the aggregate function I want SU M for

84
00:05:34,090 --> 00:05:40,840
some the column I want to perform it on such as the sales column and then in this case I can either

85
00:05:40,840 --> 00:05:44,850
collect it to return it back as an array or if I just want to show it in the terminal.

86
00:05:44,860 --> 00:05:48,530
I can say that show let's go ahead and run this.

87
00:05:48,530 --> 00:05:53,450
I'm going to comment some of these out so you don't get a whole lot of output but we can see the basic

88
00:05:53,450 --> 00:05:54,070
pattern.

89
00:05:54,220 --> 00:05:59,390
We're doing an aggregate function DFA that selects the aggregate method or a function and then that

90
00:05:59,390 --> 00:06:04,160
show or collect the painting what kind of action you want to perform on this transformation.

91
00:06:04,160 --> 00:06:06,060
So I'm going to go ahead and run this code.

92
00:06:06,300 --> 00:06:10,460
And we should see quite a few little data frames pop up as a result.

93
00:06:10,460 --> 00:06:11,720
Okay great.

94
00:06:12,140 --> 00:06:18,890
So here we're working of a numerical column can see things such as the sum of distinct sales count of

95
00:06:18,890 --> 00:06:23,140
distinct sales some of total sales which in this case are going to be the same.

96
00:06:23,210 --> 00:06:28,330
The variance if you want the standard deviation of the sample and you can even collect sets.

97
00:06:28,340 --> 00:06:33,800
So as I mentioned there's a link here which has the link to all the aggregate functions that you can

98
00:06:33,800 --> 00:06:39,370
use and going to go ahead and copy this and show you the aggregate function.

99
00:06:39,380 --> 00:06:41,970
So we've actually already explored the functions page a little bit.

100
00:06:42,110 --> 00:06:46,640
Let's go ahead and touch a little more detail on the aggregate functions you're going to go ahead and

101
00:06:46,640 --> 00:06:47,710
jump to this.

102
00:06:47,720 --> 00:06:49,320
You are LMI browser now.

103
00:06:49,540 --> 00:06:49,970
OK.

104
00:06:50,000 --> 00:06:55,100
Here I am at that functions page and you'll notice that the very first group of functions are these

105
00:06:55,160 --> 00:06:56,270
aggregate functions.

106
00:06:56,450 --> 00:07:01,280
And basically when you're using an error function without that group by statement all it means is that

107
00:07:01,280 --> 00:07:07,310
you're aggregating an entire column all the rows in that column into a single output.

108
00:07:07,310 --> 00:07:13,280
Now that output could be a single array of multiple values but the basic idea is that you're going to

109
00:07:13,310 --> 00:07:17,170
aggregate all those rows of information into a single result.

110
00:07:17,180 --> 00:07:21,890
So for something that's really obvious such as average you just take the average of all the values in

111
00:07:21,890 --> 00:07:28,100
that group and you can see the format that the aggregate function expects its input in either expects

112
00:07:28,100 --> 00:07:33,130
the column name as a string for using the select statement or you can just pasand the column itself

113
00:07:33,140 --> 00:07:38,540
so something like DPF open parentheses and then passen sales but hopefully you can see here that you

114
00:07:38,540 --> 00:07:44,450
have a lot of options for aggregate functions things such as counting distinct checking what is first.

115
00:07:44,450 --> 00:07:50,210
So the first value in a column grouping columns kurtosis a lot of statistical information here Max means

116
00:07:50,210 --> 00:07:55,550
men skewness stereo deviation and much more of the sum of the stink that cetera variance.

117
00:07:55,550 --> 00:07:57,240
So you can go ahead and check this out.

118
00:07:57,260 --> 00:08:01,700
A lot of these sort of terms will feel familiar if you're already familiar forking of sequel things

119
00:08:01,700 --> 00:08:04,110
such as first or count distinct.

120
00:08:04,250 --> 00:08:09,510
There's a very sequel like concept but hopefully you can see here the power of referencing these area

121
00:08:09,560 --> 00:08:14,200
functions in case you ever want to get more information from a particular column.

122
00:08:14,330 --> 00:08:15,890
But that's basically all there is to it.

123
00:08:15,920 --> 00:08:20,570
When you're dealing with either a group by statement or an aggregate function Let's go ahead and jump

124
00:08:20,570 --> 00:08:21,890
back to the text editor.

125
00:08:21,890 --> 00:08:22,250
All right.

126
00:08:22,250 --> 00:08:23,290
Here I am back.

127
00:08:23,420 --> 00:08:30,860
So just to briefly review what we covered you can either say DFA group by and then cause some sort of

128
00:08:30,920 --> 00:08:32,650
aggregate method off of this.

129
00:08:32,750 --> 00:08:38,630
And basically what this does for you is it performs the aggregate method in chunks based off of what

130
00:08:38,630 --> 00:08:39,320
you grouped by.

131
00:08:39,320 --> 00:08:44,980
So in this case grouping by the company performs the aggregate method in the chunks of the company.

132
00:08:45,350 --> 00:08:50,840
But if you don't want to use it group by statement instead you can just not worry about a distinction

133
00:08:50,840 --> 00:08:55,670
by another column and performant aggregate function off every single row in that column.

134
00:08:55,670 --> 00:09:01,070
So that's the difference between group buy versus just putting the arrogant function by itself.

135
00:09:01,070 --> 00:09:07,290
Now finally let's go ahead and show you how to order by with sparks and spark data frames.

136
00:09:07,490 --> 00:09:17,490
If you want to order your results actually quite easy you just say ADF dot order by so our lips order

137
00:09:17,490 --> 00:09:22,340
by capital B there and then you just input the colony on to order by.

138
00:09:22,350 --> 00:09:29,700
So for example I'm going to order by the sales and say Dot show and I will show the original data frame

139
00:09:29,820 --> 00:09:35,360
when we read it and so say DMF that show that we can see the order.

140
00:09:35,370 --> 00:09:42,160
So if I go ahead and clear this and run that scripts going to go ahead and expand the terminal so you

141
00:09:42,160 --> 00:09:43,040
can see the results.

142
00:09:43,060 --> 00:09:46,930
So the original See S We had everything already an order of the company.

143
00:09:46,930 --> 00:09:52,560
So Google Microsoft Facebook but we can actually order by a particular column such as sales here and

144
00:09:52,560 --> 00:09:58,690
now it goes from lowest to highest sales if you want the sales to be in descending order here we can

145
00:09:58,690 --> 00:10:00,420
see that they are in ascending order.

146
00:10:00,520 --> 00:10:03,200
You can just use some Skoll annotation for that.

147
00:10:03,340 --> 00:10:04,170
Well you end up doing.

148
00:10:04,180 --> 00:10:07,080
And it's a little funky but you can say dollar sign.

149
00:10:07,270 --> 00:10:13,410
And then the string of the column name that you say dot D E C for descending.

150
00:10:13,430 --> 00:10:18,470
Let's go ahead and save that and then we should see the results but in a descending order now based

151
00:10:18,470 --> 00:10:24,180
off the sales column for go ahead and run this we can see now we had the descending column of sales.

152
00:10:24,200 --> 00:10:24,570
All right.

153
00:10:24,590 --> 00:10:29,090
That's really all there is to it as far as using group by or aggregate functions.

154
00:10:29,090 --> 00:10:33,860
I definitely recommend you check out that link in the lecture notes just to see all the options that

155
00:10:33,860 --> 00:10:36,530
are available to you for aggregate functions.

156
00:10:36,670 --> 00:10:37,300
OK.

157
00:10:37,490 --> 00:10:38,710
Hopefully you enjoy this lecture.

158
00:10:38,720 --> 00:10:43,310
It's going to be a really useful method when you're dealing with large data sets to group them by a

159
00:10:43,310 --> 00:10:49,010
particular column and then get some more information or just perform aggregate functions on a particular

160
00:10:49,010 --> 00:10:51,440
column without the group by statement.

161
00:10:51,440 --> 00:10:53,350
Thanks everyone and I'll see if the next lecture.
