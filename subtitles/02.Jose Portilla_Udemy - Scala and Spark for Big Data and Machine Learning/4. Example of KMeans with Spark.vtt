WEBVTT
1
00:00:05.510 --> 00:00:10.070
Hello everyone and welcome to the documentation example for the clustering of spark section of this

2
00:00:10.100 --> 00:00:13.700
course and this lecture will walk through the documentation example.

3
00:00:13.730 --> 00:00:17.060
Show me how to actually implement k means clustering with Sparke.

4
00:00:17.090 --> 00:00:20.860
It's actually quite easy so we'll just walk through the code we will actually live.

5
00:00:20.870 --> 00:00:22.010
Code anything out.

6
00:00:22.190 --> 00:00:27.050
And then after that we'll jump to the official documentation web page so I can show you just a couple

7
00:00:27.050 --> 00:00:29.780
more features of Kamins algorithm.

8
00:00:29.780 --> 00:00:33.900
Now let's jump to the Adam text editor and walk through this example script.

9
00:00:34.100 --> 00:00:39.320
All right here are Adam text editor I've opened up the Kamins underscore example that's called a script

10
00:00:39.320 --> 00:00:40.010
file.

11
00:00:40.010 --> 00:00:44.290
And you can find that under the clustering folder under the machine learning sections folder.

12
00:00:44.300 --> 00:00:47.810
Let's just walk through it's actually a pretty simple script.

13
00:00:47.870 --> 00:00:50.180
You started sparks session as we've done before.

14
00:00:50.330 --> 00:00:52.310
And here there are lines 5 and 6.

15
00:00:52.310 --> 00:00:57.710
We set the following code to set the error reporting and then we have a spark session just as we've

16
00:00:57.710 --> 00:01:00.930
done in the past and then we import a clustering algorithm.

17
00:01:00.950 --> 00:01:06.710
In this case we're going to import from that Apache that sparked the Mehldau clustering the dot k means

18
00:01:06.790 --> 00:01:07.820
algorithm.

19
00:01:08.030 --> 00:01:13.510
Then you load in your data and as long as your data is in a different form it should be OK.

20
00:01:13.610 --> 00:01:18.830
But remember that in this case we're using an unsupervised learning algorithm which basically means

21
00:01:18.890 --> 00:01:21.800
we don't expect to have any labels in our data.

22
00:01:21.800 --> 00:01:28.370
In this case we're saying sample underscore Camy is report dated the text and that's the lib SVM file

23
00:01:28.370 --> 00:01:29.180
format.

24
00:01:29.180 --> 00:01:33.680
But in this case it's basically just a bunch of features which makes sense because it's unsupervised

25
00:01:33.680 --> 00:01:34.000
learning.

26
00:01:34.010 --> 00:01:35.240
We don't have those labels.

27
00:01:36.630 --> 00:01:44.120
Then we want to train a Kamins model so we will say vowel k means is equal to new K means object.

28
00:01:44.140 --> 00:01:45.600
You'll set the K value.

29
00:01:45.740 --> 00:01:47.790
That's probably the most important parameter here.

30
00:01:47.860 --> 00:01:49.860
Reviewed the theory lecture we just discussed.

31
00:01:49.870 --> 00:01:55.360
But basically you want to use your domain knowledge to set inappropriate k value the amount of clusters

32
00:01:55.360 --> 00:01:57.610
you expect and that is an optional argument.

33
00:01:57.610 --> 00:02:01.060
You can actually set a seed value in that set C value.

34
00:02:01.090 --> 00:02:05.310
It's just the seed on where to randomly initiate those centroid.

35
00:02:05.320 --> 00:02:09.520
Remember as I mentioned the theory lecture the centroid are randomly initiated.

36
00:02:09.520 --> 00:02:14.410
So if you want to try to get very repeatable results across multiple data sets to compare algorithms

37
00:02:14.440 --> 00:02:17.580
or settings etc. You may want to set a seed there.

38
00:02:17.770 --> 00:02:21.030
Then you say Vau model and actually train or fit your data.

39
00:02:21.160 --> 00:02:26.500
So we say k means that fits the data set and note that I'm not doing any sort of train to split here

40
00:02:26.680 --> 00:02:29.930
because that doesn't really make sense for an unsupervised learning algorithm.

41
00:02:29.950 --> 00:02:34.780
There would be no labels to compare predictions often since we're just clustering the actual data points

42
00:02:34.780 --> 00:02:40.270
we have and then we can actually evaluate these clusters by computing the within set sum of squared

43
00:02:40.330 --> 00:02:45.730
errors and remember that the standard Kamins algorithm aims at minimizing the sum of squares of the

44
00:02:45.730 --> 00:02:49.510
distance between the points of each of those sets.

45
00:02:49.510 --> 00:02:52.420
This is actually the objective of this algorithm.

46
00:02:52.420 --> 00:02:57.550
So once you've computed and trained the K means you can evaluate the result by using this within set

47
00:02:57.550 --> 00:03:02.730
some of squared errors which is essentially something like the sum of the distances of each observation

48
00:03:02.740 --> 00:03:05.060
and each k partition.

49
00:03:05.240 --> 00:03:07.830
And then there's the little convenience feature here.

50
00:03:07.850 --> 00:03:13.340
If you actually want to get those centers you can say model dot cluster centers and then dot for each

51
00:03:13.460 --> 00:03:15.060
passen prints Ellen.

52
00:03:15.380 --> 00:03:19.260
And basically what this is doing is the method of a quick hand for loop.

53
00:03:19.260 --> 00:03:23.320
So you're saying for each cluster center just print them all out.

54
00:03:23.600 --> 00:03:27.690
OK let's run this and see what happens when you open up a new terminal here.

55
00:03:27.950 --> 00:03:36.070
Make sure I'm under the clustering folder and then say Sparke bash shell and or write my Sparke shell

56
00:03:36.070 --> 00:03:36.790
is loaded.

57
00:03:36.910 --> 00:03:44.740
So we will say colon load and then this case Kamins underscore example does Skala enter.

58
00:03:44.770 --> 00:03:45.590
Let's run it.

59
00:03:45.970 --> 00:03:49.720
So we've run the model right now we're training it on that data set.

60
00:03:49.750 --> 00:03:54.330
We can see those stages showing up all right so that finished running.

61
00:03:54.340 --> 00:03:57.850
Now let me expand this so we can actually see the results we got.

62
00:03:57.930 --> 00:03:59.620
So here we see the actual model we train.

63
00:03:59.620 --> 00:04:01.340
We have a Kamins model.

64
00:04:01.470 --> 00:04:04.460
We can see that within some squared error here.

65
00:04:04.530 --> 00:04:10.990
This double so that some squirters 0.1 1 9 9 or 0.1 2.

66
00:04:11.250 --> 00:04:12.770
And then we have the cluster center.

67
00:04:12.770 --> 00:04:17.730
So this is the actual coordinates of the cluster centers and we're dealing with three dimensional data.

68
00:04:17.730 --> 00:04:19.700
So we have three coordinates.

69
00:04:19.710 --> 00:04:21.760
You need a coordinate for each feature.

70
00:04:21.780 --> 00:04:26.700
So for each mention you're going to have an actual coordinate of that Kamins cluster.

71
00:04:26.790 --> 00:04:30.960
In this case it's actually kind of easy to visualize because there's only three mentions you can kind

72
00:04:30.960 --> 00:04:36.600
of figure in your mind where these cluster centers are so x y and z axes as you get to higher that mentions

73
00:04:36.630 --> 00:04:39.560
going to be harder to visualize that within your mind.

74
00:04:39.880 --> 00:04:45.030
OK so we can see that these cluster centers are pretty well separated and we have a reasonable set of

75
00:04:45.030 --> 00:04:50.830
some squares given the actual values of the data set to see the values here.

76
00:04:50.850 --> 00:04:55.680
This is the live SVM format so it's a little strange to read but you can see here that we have this

77
00:04:56.100 --> 00:04:56.910
one two and three.

78
00:04:56.910 --> 00:05:01.320
These are all around nine point zero nine point one 9.2 along each three axes.

79
00:05:01.320 --> 00:05:07.470
And then we have these set of three points 0 1 and 2 which are zero point zero 0.1 0.2.

80
00:05:07.470 --> 00:05:12.270
So we can see here the cluster centers make a lot of sense in their fitting well now that we've explored

81
00:05:12.270 --> 00:05:13.920
this documentation example.

82
00:05:13.920 --> 00:05:20.130
Let's quickly jump to our browser and explore the actual machine learning clustering documentation going

83
00:05:20.130 --> 00:05:21.420
to jump to my browser now.

84
00:05:23.800 --> 00:05:27.570
All right here we are at the machine learning library or M-L guide.

85
00:05:27.610 --> 00:05:34.160
We visit this page many times before but I want you to do is click here on clustering and this page

86
00:05:34.160 --> 00:05:40.850
will describe the clustering algorithms in Madlib such as canings LDK a version of Kamins called bisecting

87
00:05:40.850 --> 00:05:43.640
Kamins and Gaussian a mixture model Kamins.

88
00:05:43.700 --> 00:05:48.080
By far the most commonly is clustering algorithms that we show here.

89
00:05:48.080 --> 00:05:50.720
And if you look at what I want you to notice is the input column.

90
00:05:50.720 --> 00:05:57.380
So k means expects your columns to be inputted as just features before for classification of regression.

91
00:05:57.380 --> 00:06:02.930
We had that label column and then the features column which was an array of features but for unsupervised

92
00:06:02.930 --> 00:06:03.470
learning.

93
00:06:03.560 --> 00:06:09.220
You don't have that label column so it really just expects a singular column of features as an array.

94
00:06:09.260 --> 00:06:14.690
So keep that in mind as you use Kamins of Scotland Spart And if you scroll them you can see the example

95
00:06:14.690 --> 00:06:16.270
that we essentially just walk through.

96
00:06:16.400 --> 00:06:17.670
It's a very simple example.

97
00:06:17.670 --> 00:06:20.120
Import k means load the data set.

98
00:06:20.120 --> 00:06:26.500
Train that Kamins model and evaluate it using compute cost on that same data set and computing that

99
00:06:26.510 --> 00:06:28.660
within set sum of squared errors.

100
00:06:28.710 --> 00:06:33.770
And if you want more information on this you can actually just click here to the Scala API docs and

101
00:06:33.770 --> 00:06:37.870
it will take you to this Kamins page where you can explore more parameters to do.

102
00:06:37.880 --> 00:06:41.510
But really k means is pretty simple as far as implementing it.

103
00:06:41.540 --> 00:06:45.980
If you scroll all the way down if you're a really advanced data scientist or maybe you are doing your

104
00:06:45.980 --> 00:06:51.400
Ph.D. and you're playing around with Scullin SPARC there some expert only parameters that you can initiate.

105
00:06:51.560 --> 00:06:57.200
But really if you don't have this experience these are really only for advanced expert only parameter

106
00:06:57.200 --> 00:06:59.000
keys that the algorithm can't take.

107
00:06:59.000 --> 00:07:04.160
So if you're familiar with that the options available here for you but really only experts should be

108
00:07:04.160 --> 00:07:06.200
using those parameters.

109
00:07:06.310 --> 00:07:06.910
All right.

110
00:07:07.040 --> 00:07:08.800
That's really all there is to discuss here.

111
00:07:08.810 --> 00:07:13.010
K means coming up next is going to be an overview of your Kamins project.

112
00:07:13.010 --> 00:07:14.750
Thanks everyone and I'll see at the next lecture.
