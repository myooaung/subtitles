WEBVTT
1
00:00:05.350 --> 00:00:11.140
Hello everyone and welcome to the project exercise solutions lecture for clustering with SPARK.

2
00:00:11.140 --> 00:00:16.570
Let's jump to the exercise script and start coding out the solutions for the project exercise.

3
00:00:16.600 --> 00:00:22.330
All right so here you have the exercise scripts and we'll be doing is just working through solutions

4
00:00:22.340 --> 00:00:27.010
and as a quick note you can always reference's Kamins underscore project underscore solution script

5
00:00:27.010 --> 00:00:30.700
right here for all the code that I will be actually typing out.

6
00:00:30.730 --> 00:00:32.940
So jumping back to the actual exercise.

7
00:00:33.190 --> 00:00:34.360
Let's start off.

8
00:00:34.630 --> 00:00:42.660
Let's go to the tasks and I will scroll down and start by importing a spark session.

9
00:00:42.680 --> 00:00:45.760
So I will say import or Apache.

10
00:00:45.760 --> 00:00:52.190
The spark thought sequel spark session.

11
00:00:52.230 --> 00:00:54.220
All right we have our imported sparks session.

12
00:00:54.300 --> 00:00:56.910
Here's the following code to use for error reporting.

13
00:00:56.910 --> 00:01:01.650
Set it to be a little lower and the next we want to create a Sparks session instance just as we've done

14
00:01:01.650 --> 00:01:02.610
in the past.

15
00:01:02.610 --> 00:01:11.830
I will say Val SPARC is equal to spark session builder and then get or create.

16
00:01:12.240 --> 00:01:15.330
Next I want to import the K means clustering algorithm.

17
00:01:15.330 --> 00:01:25.700
So say import or that Apache Sparke M-L we're using clustering algorithms and I want the K means the

18
00:01:25.700 --> 00:01:27.600
M is capitalized there.

19
00:01:27.650 --> 00:01:32.600
Next I want to load in the wholesale customers data so you could have downloaded it from the link that's

20
00:01:32.600 --> 00:01:33.510
posted here.

21
00:01:33.530 --> 00:01:39.620
Or you have it actually here as wholesale customers data that see every file will start off by saying

22
00:01:39.650 --> 00:01:40.650
Val.

23
00:01:40.840 --> 00:01:48.710
Data set is equal to spark the read option and I will set the headers to be true.

24
00:01:48.730 --> 00:01:53.840
Because if you look at the CSP file it does contain a header and then I also want to set the option

25
00:01:53.870 --> 00:01:55.760
of inferring schema to be true.

26
00:01:56.060 --> 00:01:57.680
So I'll say in first Kema

27
00:02:01.030 --> 00:02:06.890
and it's a C every file so say that C is free and then passen wholesale.

28
00:02:07.270 --> 00:02:13.770
Space customers Space Data CXXVI.

29
00:02:13.970 --> 00:02:18.140
All right so now that we have that Elisha's actually make sure everything worked.

30
00:02:18.140 --> 00:02:24.920
I'm going to save this Skala a script open up a new terminal here and under my clustering folder start

31
00:02:24.950 --> 00:02:36.130
a spark shell enter let the spark shell load and then I'm just going to load up that script load means

32
00:02:36.160 --> 00:02:46.410
wups make sure that's capital M Kamins underscore project underscore exorcized Scala enter and looks

33
00:02:46.410 --> 00:02:48.420
like everything's working so far.

34
00:02:48.540 --> 00:02:50.370
Make sure we read that dataset correctly

35
00:02:53.610 --> 00:03:01.640
and to confirm that we read it collect correctly I can just say data sets the current schema enter and

36
00:03:01.720 --> 00:03:03.370
we can see the schema here.

37
00:03:03.450 --> 00:03:07.770
So I have channel region and then these columns and I'm actually going to be using fresh milk grocery

38
00:03:07.770 --> 00:03:08.990
frozen etc..

39
00:03:09.360 --> 00:03:12.570
So up next is to select the following columns.

40
00:03:12.570 --> 00:03:18.060
These columns right here and to call this new subset feature data.

41
00:03:18.180 --> 00:03:18.920
So let's do that.

42
00:03:18.930 --> 00:03:29.660
I will say Val wups feature data feature underscore data is equal to my dataset selects and I'm going

43
00:03:29.660 --> 00:03:34.370
to select these Collinge right here so I can actually just copy and paste them make my life a little

44
00:03:34.370 --> 00:03:35.220
easier.

45
00:03:36.510 --> 00:03:40.360
And then make sure these are actually columns now.

46
00:03:40.360 --> 00:03:43.380
Not always do you have to use that dollar sign notation.

47
00:03:43.500 --> 00:03:46.680
I've personally noticed that across different operating systems.

48
00:03:46.680 --> 00:03:49.260
Some require it and some do not.

49
00:03:49.260 --> 00:03:52.080
Especially depending on what version of SPARC you're using.

50
00:03:52.080 --> 00:03:55.250
So basically for safety I would recommend that you put in these dollar signs.

51
00:03:55.300 --> 00:04:00.280
A lot of times I've noticed that it doesn't really need the dollar signs so it's up to you.

52
00:04:00.540 --> 00:04:07.320
But there we have it and the next I want to import vector assembler and vectors as we then the past.

53
00:04:07.320 --> 00:04:12.540
We actually put all these features into a single column of an array of all the features so say import

54
00:04:13.280 --> 00:04:24.110
or the Apache that spark that M-L that feature and then I'm going to say that assembler and then up

55
00:04:24.110 --> 00:04:29.880
next I will say import or Apache that SPARC the M-L.

56
00:04:29.960 --> 00:04:32.260
Lynn alge and vector's

57
00:04:34.910 --> 00:04:38.990
up next what I want to do is what we've always done is to create a new vector similar object called

58
00:04:38.990 --> 00:04:44.450
assembler for the feature columns as the input set the column output to be called just features.

59
00:04:44.460 --> 00:04:51.560
Then remember there's no label column because this is unsupervised learning so I will say Vau assembler

60
00:04:52.490 --> 00:04:56.150
is a new vector assembler object.

61
00:04:56.260 --> 00:05:04.090
I will set the input calls so set and put calls to be in array of these features here.

62
00:05:04.100 --> 00:05:08.810
So let me just copy and paste these and then we want these to be strings so I will delete these dollar

63
00:05:08.820 --> 00:05:09.510
signs.

64
00:05:13.170 --> 00:05:19.010
So we'll delete that one this one this one and this one.

65
00:05:19.050 --> 00:05:19.310
All right.

66
00:05:19.320 --> 00:05:23.680
So we have an array and then up next the last thing we want to do is actually set the output column.

67
00:05:24.060 --> 00:05:30.080
So we will say set output call or remember this is just lowercase features.

68
00:05:30.120 --> 00:05:35.340
So basically everything we've done in the past except this time it's just no label.

69
00:05:35.420 --> 00:05:40.060
Then we want to use that assembler object to transform the feature data and we call this new data.

70
00:05:40.070 --> 00:05:41.600
The training data.

71
00:05:41.600 --> 00:05:42.700
So I will say this.

72
00:05:42.740 --> 00:05:49.810
Val training underscore data is equal to assembler.

73
00:05:49.940 --> 00:05:55.280
Transform feature underscored data.

74
00:05:55.510 --> 00:05:56.890
And after we transformed.

75
00:05:56.920 --> 00:06:01.860
I really just want to select that features column.

76
00:06:02.020 --> 00:06:05.890
Next we want to create a Kamins model with K equal to 3.

77
00:06:05.950 --> 00:06:07.360
So that's quite easy.

78
00:06:07.360 --> 00:06:17.900
We just save Val some object in this case Kamins is a new Kamins model and then set K to be equal to

79
00:06:17.900 --> 00:06:18.680
3.

80
00:06:18.730 --> 00:06:19.300
And if you want.

81
00:06:19.300 --> 00:06:25.070
You can also see this but it's not terribly important so we'll just set a seed and then we will say

82
00:06:25.070 --> 00:06:27.450
fit that model to the training data.

83
00:06:27.570 --> 00:06:36.390
That's just going to be avowe model call came means and call it off of it onto your training data.

84
00:06:36.680 --> 00:06:40.690
And we're not doing any train to split here because unsupervised learning.

85
00:06:40.760 --> 00:06:46.520
So in order to evaluate we're going to compute the within set some of square errors and SPARC makes

86
00:06:46.520 --> 00:06:47.490
this quite easy.

87
00:06:48.570 --> 00:06:58.070
This is just like we did in the documentation example that we'll call model compute cost training underscore

88
00:06:58.070 --> 00:07:00.980
data and then show the result of this.

89
00:07:01.130 --> 00:07:05.620
So I can just say prints line and use some string interpellation here.

90
00:07:05.810 --> 00:07:08.560
So lowercase s quotes.

91
00:07:09.020 --> 00:07:14.360
I'm going to say something like the within said squirter.

92
00:07:14.360 --> 00:07:16.010
So just copy and paste this line

93
00:07:19.550 --> 00:07:27.770
was and then we can use this string interpellation notation and just passen the variable name when extra

94
00:07:27.770 --> 00:07:32.220
s there and if you want you can also report the cluster centers optional.

95
00:07:32.240 --> 00:07:40.370
It's up to you but let's actually run this I've saved it press of my arrow key and run load Kamins project

96
00:07:40.430 --> 00:07:41.570
exercise that Skala.

97
00:07:41.600 --> 00:07:45.710
Make sure when I have any errors here enter we should see everything.

98
00:07:45.730 --> 00:07:53.810
Load up get that data get those vectors create the Kamins object Traina and running it.

99
00:07:53.860 --> 00:07:55.500
All right so here are results.

100
00:07:55.720 --> 00:08:00.220
And depending on whether you set a seed here or not you may get slightly different results than what

101
00:08:00.220 --> 00:08:05.230
I've shown here but they should definitely be in the same order of magnitude and definitely around somewhere

102
00:08:05.230 --> 00:08:07.630
between 7 and 9.

103
00:08:07.630 --> 00:08:08.270
All right.

104
00:08:08.290 --> 00:08:10.540
Hope you enjoy this exercise project again.

105
00:08:10.540 --> 00:08:11.460
K means clustering.

106
00:08:11.460 --> 00:08:13.410
It's actually quite simple.

107
00:08:13.510 --> 00:08:19.120
We can report back here how well we were able to actually cluster the data and the larger the error.

108
00:08:19.270 --> 00:08:21.240
It means it wasn't the right value.

109
00:08:21.250 --> 00:08:25.030
So what I recommend doing now is actually experimenting with some other k value.

110
00:08:25.030 --> 00:08:26.860
So for instance putting in for here.

111
00:08:27.070 --> 00:08:29.520
See if you can separate those other regions.

112
00:08:29.830 --> 00:08:31.670
So I will load Kamins project again.

113
00:08:31.720 --> 00:08:33.690
Let's see for error goes up or down.

114
00:08:34.540 --> 00:08:35.530
After running this.

115
00:08:35.680 --> 00:08:37.530
So here it actually goes down.

116
00:08:37.540 --> 00:08:42.340
Which means it's actually a better fit with four clusters instead of three clusters.

117
00:08:42.340 --> 00:08:48.670
That makes sense because if you look back at the actual dataset remember we only had two defined regions

118
00:08:48.760 --> 00:08:55.600
and that third one was actually called other regions which means that there's more than one other region.

119
00:08:55.720 --> 00:09:01.960
So we can do is keep experimenting with K and I encourage you to do this and see what gives you eventually

120
00:09:02.110 --> 00:09:04.530
the lowest sum of squares.

121
00:09:06.240 --> 00:09:11.650
And this should give us lower and eventually you're going to get lower and lower but it will plateau.

122
00:09:11.660 --> 00:09:17.430
And that's basically that Elbot method that we're discussing over in the Kamins theory lecture if you

123
00:09:17.460 --> 00:09:20.350
keep upping the k your air will go down.

124
00:09:20.370 --> 00:09:27.350
But eventually the change is not going to be worth the extra computation of setting another cluster.

125
00:09:27.360 --> 00:09:32.580
So keep that in mind what you should do is using some other program or some sort of visualization maybe

126
00:09:32.640 --> 00:09:39.540
Excel is track this error amounts as you lower K and eventually you should see that elbow where it stops

127
00:09:39.540 --> 00:09:42.640
dropping as you go to a higher and higher value.

128
00:09:42.900 --> 00:09:48.630
So let's go to something like eight and run this see if we get a whole lot lower.

129
00:09:51.440 --> 00:09:53.030
So we get 3.7.

130
00:09:53.030 --> 00:09:57.220
Let's run this to something like 16 which is a whole lot of clusters.

131
00:09:57.470 --> 00:10:03.180
So run that again and make it something like 2.2 9.

132
00:10:03.210 --> 00:10:08.970
So you notice that the dropping is getting less and less so from 8 to 16 it only dropped about 1.

133
00:10:09.180 --> 00:10:14.190
And that's actually around the same drop that we're experiencing from something like four to five or

134
00:10:14.190 --> 00:10:15.030
five to six.

135
00:10:15.030 --> 00:10:20.490
So you would definitely see that elbow again I encourage you to play around these k values and actually

136
00:10:20.550 --> 00:10:25.040
check out that elbowing yourself in the set of the error results.

137
00:10:25.050 --> 00:10:25.590
OK.

138
00:10:25.770 --> 00:10:29.910
Hope you enjoyed this project and I will see at the next lecture if you have any questions feel free

139
00:10:29.910 --> 00:10:31.320
to post the Q&amp;A forums.
