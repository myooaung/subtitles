1
00:00:05,080 --> 00:00:10,180
Hello everyone and welcome to the data free operations lecture in this lecture we're going to continue

2
00:00:10,180 --> 00:00:15,610
our discussion of data from operations will show you how to do operations such as filtering data from

3
00:00:15,610 --> 00:00:17,950
a data frame based on certain conditions.

4
00:00:17,950 --> 00:00:20,660
Let's go ahead and jump toward sex ed. to get started.

5
00:00:20,920 --> 00:00:27,160
OK here I am at the text editor the skull a script for this lecture is data frame underscore operations

6
00:00:27,170 --> 00:00:32,740
that Skala and we're going to go ahead and start a simple Spark's session grab that same CACP felt that

7
00:00:32,740 --> 00:00:37,960
we've been working with this group 2006 2008 file and then we're going to show you how to do some basic

8
00:00:37,960 --> 00:00:43,220
operations based off of filtering data and let us discuss how to use Spark's sequel as well Scullin

9
00:00:43,230 --> 00:00:49,600
notation to filter and do some operations on that data frame and then we'll show you later on the very

10
00:00:49,600 --> 00:00:54,960
basics of some more complicated operations that we're going to be going over when we talk about grouped

11
00:00:54,970 --> 00:00:56,950
by aggregate functions.

12
00:00:56,980 --> 00:01:01,020
But let's go ahead and start a new script and go ahead and start the spark shell.

13
00:01:01,150 --> 00:01:06,550
I already created the script DFA that skull from the last lecture and I have these three lines of code

14
00:01:06,610 --> 00:01:11,800
just starting that sparked session creating the spark value either get or create that and then reading

15
00:01:12,020 --> 00:01:15,060
we have the options that CFE Citigroup.

16
00:01:15,130 --> 00:01:19,430
Let's go ahead and print the schema one more time so we can actually remember what this looks like one

17
00:01:19,450 --> 00:01:20,670
to say Dia.

18
00:01:21,010 --> 00:01:23,670
The print schema save that.

19
00:01:23,710 --> 00:01:25,700
And then let's go ahead and run this.

20
00:01:25,780 --> 00:01:33,220
Open this terminal a little more clear that and then say load the Skala and it's a quick note usually

21
00:01:33,220 --> 00:01:38,770
don't want to call your skull script the same name as a value or a variable inside of your script.

22
00:01:38,770 --> 00:01:40,640
I'm just doing this because it's really short.

23
00:01:40,660 --> 00:01:42,430
So is isn't going to be typing this.

24
00:01:42,640 --> 00:01:43,670
I want to keep it simple.

25
00:01:43,840 --> 00:01:44,190
OK.

26
00:01:44,200 --> 00:01:52,150
So again the CSP file itself in our data frame has a date the time stamp and then an open high low close

27
00:01:52,360 --> 00:01:52,830
price.

28
00:01:52,870 --> 00:01:55,630
And then the volume of stock that was traded that day.

29
00:01:55,750 --> 00:02:00,680
Let's go ahead and discuss how we can actually filter out data based off of some condition.

30
00:02:00,700 --> 00:02:02,530
So a lot of times you don't want all this data.

31
00:02:02,620 --> 00:02:05,650
You only want data that meets a certain criteria.

32
00:02:05,690 --> 00:02:11,810
So there's two ways or two main ways of filtering out data and one way is to use Sparke sequel syntax

33
00:02:12,110 --> 00:02:17,810
and the other the other way is just easy Skala syntax in order to use that Skala syntax with the dollar

34
00:02:17,810 --> 00:02:26,390
sign notation you're going to need to import Sparke implicit dot underscore and that will allow us to

35
00:02:26,390 --> 00:02:29,510
use the dollar sign Skoll annotation by default.

36
00:02:29,510 --> 00:02:33,080
You can go ahead and use the sequel notation without importing everything.

37
00:02:33,080 --> 00:02:36,620
Now we don't assume that you understand or have any sequel experience.

38
00:02:36,680 --> 00:02:41,660
While it's very likely that you do if you're already this far into discourse and are interested in Sparc

39
00:02:41,660 --> 00:02:45,430
and Skala I'm not going to assume that you have that siecle experience.

40
00:02:45,470 --> 00:02:50,210
So we're going to mainly stick with these Scullin notation although if you do have sequel experience

41
00:02:50,240 --> 00:02:52,150
you'll probably be a big fan of Sparke sequel.

42
00:02:52,160 --> 00:02:53,230
So just keep it in mind.

43
00:02:53,360 --> 00:02:58,300
I'll show you both examples but later on in the course we're going to stick with the scal annotation.

44
00:02:58,340 --> 00:03:03,080
Let's go ahead and show you an example of grabbing all the rows where a column meets a certain condition

45
00:03:03,650 --> 00:03:04,430
in order to do this.

46
00:03:04,430 --> 00:03:08,720
You go ahead and call your data frame object and then call the filter method off of it.

47
00:03:09,510 --> 00:03:11,700
And the skull annotation looks like this.

48
00:03:11,760 --> 00:03:14,960
You put a dollar sign and then the name of the column you want.

49
00:03:14,960 --> 00:03:20,340
For instance the close column and then you go ahead and put in whatever comparison operator you want

50
00:03:20,340 --> 00:03:22,440
to grab some sort of condition.

51
00:03:22,440 --> 00:03:27,900
So for example let's say I want to filter out the data frame where the close column was greater than

52
00:03:27,900 --> 00:03:28,980
$480.

53
00:03:28,980 --> 00:03:35,330
So I can say we're close greater than 80 and then I'm going to go ahead and show that.

54
00:03:35,340 --> 00:03:38,670
So sometimes you would set this as a new value or new data frame.

55
00:03:38,670 --> 00:03:42,120
But since we're just working the Wycliffe's the terminal right now and I want to see these results to

56
00:03:42,120 --> 00:03:46,590
understand how they work I'm just going to go ahead and say that show at the end of this let's go ahead

57
00:03:46,590 --> 00:03:47,540
and run this again.

58
00:03:49,000 --> 00:03:56,080
And here I can see the top 20 rows where the close price here happens to be greater than $480.

59
00:03:56,110 --> 00:03:57,050
Great.

60
00:03:57,100 --> 00:03:59,320
So we can also use equal notation.

61
00:03:59,320 --> 00:04:03,850
Let me go in and show you what that looks like it's going to feel really familiar if you already know

62
00:04:03,850 --> 00:04:04,720
sequel.

63
00:04:04,990 --> 00:04:10,930
But basically in sequel when you have the where statement and then some sort of condition that where

64
00:04:10,930 --> 00:04:13,270
condition is what you're going to pass here.

65
00:04:13,270 --> 00:04:18,250
So for instance if this was a sequel I would say where close.

66
00:04:18,400 --> 00:04:22,900
Greater than 480 and that should give us the exact same result.

67
00:04:22,930 --> 00:04:29,580
If I run the script again and there it is there are the top 20 rows where the closing price is greater

68
00:04:29,580 --> 00:04:31,610
than $480.

69
00:04:31,620 --> 00:04:32,230
OK.

70
00:04:32,430 --> 00:04:35,990
So let's go ahead and discuss how we can do multiple filters.

71
00:04:36,030 --> 00:04:38,670
So you don't always want to just filter by one column.

72
00:04:38,670 --> 00:04:42,350
You want to filter by multiple columns in the skull annotation.

73
00:04:42,360 --> 00:04:43,290
This is really easy.

74
00:04:43,290 --> 00:04:47,390
You just use the and ampersands or the or pipe operators.

75
00:04:47,400 --> 00:04:53,730
So for instance if I want to go ahead and maybe check where the close price was less than 40.

76
00:04:53,790 --> 00:05:02,960
So for instance I want to say close ups less than 480 and then I use Scalia's and operator which is

77
00:05:02,960 --> 00:05:14,270
2 percent so EON's dollar sign Hi is let's say let's go ahead I'll say less than 40 I can go ahead and

78
00:05:14,270 --> 00:05:20,800
show this animal control as to save this and then I'm going to go ahead and enter there to run this.

79
00:05:20,900 --> 00:05:22,410
And here I can see the results.

80
00:05:22,430 --> 00:05:26,930
So you notice that close lesson of 480 and high is less than 40.

81
00:05:27,290 --> 00:05:32,030
OK so that's how you can use multiple conditions there and you can use the pipe operator to do the or

82
00:05:32,390 --> 00:05:34,120
this is the scal annotation.

83
00:05:34,280 --> 00:05:37,040
Let me go and issue the spark siecle notation.

84
00:05:37,040 --> 00:05:39,070
So I'm going to go and just comment this out.

85
00:05:39,080 --> 00:05:40,110
So we don't lose it.

86
00:05:40,430 --> 00:05:47,920
I will say if that filter and again if your experience of sequel you can go ahead and just say close

87
00:05:48,990 --> 00:05:51,520
less than 480 and it's equal you're going to say.

88
00:05:51,540 --> 00:05:54,380
And and usually capitalize it you don't have to.

89
00:05:54,390 --> 00:05:58,120
But by sequel conventions usually capitalized those key words.

90
00:05:58,320 --> 00:06:03,690
And then you can say something like high also less than 480.

91
00:06:03,730 --> 00:06:04,810
Let's go ahead and save that.

92
00:06:04,830 --> 00:06:07,920
And let's actually show this result as well.

93
00:06:07,980 --> 00:06:12,750
Let's make sure this runs and you see here that this also works.

94
00:06:12,750 --> 00:06:16,280
So again these are all based off of Sparke sequel objects in those.

95
00:06:16,310 --> 00:06:17,620
Or that Apache.

96
00:06:17,640 --> 00:06:23,100
So the data Freman data sets and spark sessions all fall under Sparke sequel which is why we're able

97
00:06:23,100 --> 00:06:26,700
to use this sort of sequel syntax with these data frames.

98
00:06:26,700 --> 00:06:29,810
Now again it's up to you what you want to use here.

99
00:06:29,820 --> 00:06:34,490
I'm a fan of sequel and it's makes sense to me so a lot of times my own personal work of using this

100
00:06:34,490 --> 00:06:39,840
sequel iteration but this course does not assume that knowledge but we know that we showed you how to

101
00:06:39,930 --> 00:06:41,470
actually program a scholar.

102
00:06:41,580 --> 00:06:46,050
So we're going to be using the sort of Skoll notation but when it's appropriate I'll show you the alternative

103
00:06:46,050 --> 00:06:48,170
sequence notations in case you're interested.

104
00:06:48,520 --> 00:06:55,680
OK let's go ahead and show you a couple more actions that you can perform on a different.

105
00:06:55,730 --> 00:07:01,430
Right now the main action that we've been showing so the filter's the transformation and shows the action.

106
00:07:01,550 --> 00:07:06,590
But this main action of show basically just shows the results and a lot of times you actually don't

107
00:07:06,590 --> 00:07:12,630
want to see the results you want to maybe save it as something else or maybe count the results etc..

108
00:07:12,710 --> 00:07:19,250
What we can do is instead of showing these results I can use the action collect in order to collect

109
00:07:19,250 --> 00:07:21,780
these results into a scholar object.

110
00:07:21,940 --> 00:07:26,660
And because I'm actually collecting these results into a skull object I'm going to go ahead and save

111
00:07:26,660 --> 00:07:32,090
them as some sort of new or new variables so I will save them all.

112
00:07:32,150 --> 00:07:41,680
Let's go ahead and say S.H. underscore low or something whatever you want to actually do this let's

113
00:07:41,680 --> 00:07:45,400
go in and save that and then I'm going to clear that terminal and run again.

114
00:07:46,520 --> 00:07:51,950
And I should see now instead of actually seeing the results as a data frame I can actually see I've

115
00:07:51,950 --> 00:07:54,720
collected these results as an array.

116
00:07:54,740 --> 00:08:02,660
So now this array contains these row objects where the rows are the rows where this condition happened

117
00:08:02,660 --> 00:08:03,570
to be true.

118
00:08:03,770 --> 00:08:09,410
And this is the way that you can grab the sort of result back and then perform all your Skala operation

119
00:08:09,410 --> 00:08:10,230
knowledge on it.

120
00:08:10,400 --> 00:08:14,420
So then once you have this array you can do whatever you want that you could do if a normal array in

121
00:08:14,420 --> 00:08:18,210
Scotland you can run a for loop through it print out every row.

122
00:08:18,360 --> 00:08:21,910
Scam it for certain objects or certain elements etc..

123
00:08:21,950 --> 00:08:27,140
But the stock collect action is the way that you transform data frames into arrays.

124
00:08:27,140 --> 00:08:30,820
Now keep in mind this array the objects inside of it are still rows.

125
00:08:30,890 --> 00:08:35,930
You can index them just like you would with a normal skull array OK.

126
00:08:35,940 --> 00:08:42,930
Couple more closing points I want to discuss is a rather common method or action here excuse me.

127
00:08:43,020 --> 00:08:45,750
Instead of collect It's actually just do count.

128
00:08:45,780 --> 00:08:51,720
Now we're going to discuss aggregate functions a lot more in detail and we actually get to that by an

129
00:08:51,780 --> 00:08:53,180
aggregation lecture.

130
00:08:53,220 --> 00:08:55,480
But let's go and just show you an example of this.

131
00:08:55,500 --> 00:09:00,810
So instead of actually grabbing the results and saving them as an array let's say only wanted to know

132
00:09:00,840 --> 00:09:06,780
how many results are how many rows or returns I can use the dot count to actually get that result back.

133
00:09:06,780 --> 00:09:13,510
So if I go in and save this clear my terminal and run it I will see at the very end some sort of number.

134
00:09:13,510 --> 00:09:19,390
So in this case S.H. low is 397 in this P-word long just in the case.

135
00:09:19,400 --> 00:09:20,900
This is a long integer.

136
00:09:20,920 --> 00:09:23,360
And if you're unfamiliar of that don't worry about it too much.

137
00:09:23,380 --> 00:09:27,830
It just has to do with the way computers and machines store integers in memory.

138
00:09:27,850 --> 00:09:30,880
But this is just a basic integer 397.

139
00:09:30,940 --> 00:09:34,350
So I know I got 397 results back for my S.H. low.

140
00:09:34,360 --> 00:09:36,550
So that's the counts action.

141
00:09:36,550 --> 00:09:41,440
So again we can think of this as some sort of transformation on this data frame and then some sort of

142
00:09:41,500 --> 00:09:46,480
action on this data frame so we can separate out in our mind that we can stack transformations on top

143
00:09:46,480 --> 00:09:51,940
of each other and they're lazily evaluated so they don't actually occur until you actually perform the

144
00:09:51,940 --> 00:09:53,460
action on them.

145
00:09:53,470 --> 00:09:53,970
OK.

146
00:09:54,250 --> 00:09:59,730
Couple more little tidbits I want to close off with here is if you're trying to filter for equality.

147
00:09:59,800 --> 00:10:07,250
So for instance the filter and you want to figure out where Hy's equal to a specific number.

148
00:10:07,400 --> 00:10:14,520
If you end up saying something like for instance high equals equals what's going to say four eighty

149
00:10:14,520 --> 00:10:15,900
four point four Xerox.

150
00:10:15,900 --> 00:10:22,070
I know that's true and I'll say show you'll get an error at this point in time if you actually run this

151
00:10:23,700 --> 00:10:30,240
so using spark 2.0 point one this returns an error and you'll say something like cannot be applied to

152
00:10:30,240 --> 00:10:31,080
boolean.

153
00:10:31,160 --> 00:10:37,890
What you end up having to do to fix this using Skala notation is to use triple equal signs and then

154
00:10:37,890 --> 00:10:39,080
go ahead and save that.

155
00:10:39,210 --> 00:10:41,540
And then you'll get the results you want.

156
00:10:41,550 --> 00:10:47,890
So if you go ahead and run that you'll notice that now you get the Roback where high happened to be

157
00:10:47,890 --> 00:10:50,940
exactly equal to four hundred eighty four dollars and 40 cents.

158
00:10:50,940 --> 00:10:53,310
This is kind of a weird quirk right now.

159
00:10:53,350 --> 00:10:55,910
Spark data frame's hopefully in future versions.

160
00:10:55,910 --> 00:11:00,630
The double sign or double equal sign notation actually works like you would expect it to.

161
00:11:00,640 --> 00:11:05,210
But for now keep that in mind that you're going to have to use those triple equal signs with the scarlet

162
00:11:05,210 --> 00:11:06,220
notation.

163
00:11:06,220 --> 00:11:07,440
Well you can do instead.

164
00:11:07,450 --> 00:11:16,620
Also if you're familiar with sequel is say equal to and in this case it was forty four point four zero.

165
00:11:16,810 --> 00:11:20,370
Let's go ahead and run that show it.

166
00:11:20,550 --> 00:11:24,390
And you'll notice that this also works exactly the same as you would expect.

167
00:11:24,390 --> 00:11:29,160
So for me if you're familiar a sequel you don't have to worry about that kind of sort of weird quirk.

168
00:11:29,160 --> 00:11:31,290
It'll just work like you expect it to work.

169
00:11:31,650 --> 00:11:32,400
All right.

170
00:11:32,400 --> 00:11:37,620
Now later on in another lecture we're going to learn about aggregate functions in a group by.

171
00:11:37,650 --> 00:11:42,930
But I want to go ahead and show you the various operations and functions that you can perform with Scala

172
00:11:42,990 --> 00:11:47,810
on a data frame an example is correlation.

173
00:11:47,830 --> 00:11:52,620
So let's say you wanted to actually get the Pearson correlation between two columns.

174
00:11:52,690 --> 00:11:59,220
You can say diea that selects and then you're going to passen a built in function.

175
00:11:59,230 --> 00:12:04,990
So in this case the built in function is c o r r for correlation and this is the Pearson correlation

176
00:12:04,990 --> 00:12:07,450
which is going to tell us how correlated are two columns.

177
00:12:07,570 --> 00:12:09,350
And then you can just pass in a.

178
00:12:09,700 --> 00:12:14,140
No need to do any special formatting or a dollar sign in this function.

179
00:12:14,140 --> 00:12:19,750
You pass them in strings high low and then you can go ahead and say that show or collect whatever action

180
00:12:19,750 --> 00:12:20,900
you want to perform.

181
00:12:20,940 --> 00:12:26,850
You can say DFA that select and perform functions on your actual columns and then say that show.

182
00:12:26,980 --> 00:12:28,680
Let's go ahead and see this.

183
00:12:28,850 --> 00:12:33,520
And we should expect after the filter statement a number there and there it is.

184
00:12:33,530 --> 00:12:36,550
It's the correlation between the high column and the low column.

185
00:12:36,550 --> 00:12:40,010
And here we get an extremely high correlation it's almost perfectly correlated.

186
00:12:40,120 --> 00:12:45,610
And that actually makes a lot of sense because that means within a certain day the high price and low

187
00:12:45,610 --> 00:12:52,660
price are roughly going up and down together which makes sense because you don't get such drastic differences

188
00:12:52,660 --> 00:12:53,340
on a day.

189
00:12:53,440 --> 00:12:59,200
So for a relatively stable stock so it makes a lot of sense that for almost all stocks you'll have high

190
00:12:59,200 --> 00:13:01,830
correlation between the high and low column.

191
00:13:01,990 --> 00:13:06,130
If you want to go ahead and check out all the functions that are available to you in all the operations

192
00:13:06,490 --> 00:13:10,940
you can click here on data frame underscore operations at Skala the notes for this lecture.

193
00:13:10,960 --> 00:13:17,990
Scroll down and you should see various examples everything and we just discussed if you keep scrolling

194
00:13:17,990 --> 00:13:21,300
down you'll see here operations and useful functions and commented out.

195
00:13:21,360 --> 00:13:23,330
But go ahead and take this.

196
00:13:23,330 --> 00:13:24,450
Is this your l.

197
00:13:24,530 --> 00:13:27,870
And this takes you straight to the documentation of the latest API.

198
00:13:28,100 --> 00:13:29,850
Go ahead and copy this.

199
00:13:29,870 --> 00:13:35,410
Let's copy this and then I'm going to put this in my browsers are going to go ahead and jump to my browser

200
00:13:35,980 --> 00:13:41,470
and here you are at the URL and you can see that there's objects functions and these are all the functions

201
00:13:41,470 --> 00:13:46,290
available for a data frame and you should note that some of these will have an experimental tag on them

202
00:13:46,300 --> 00:13:50,640
so if you scroll down eventually see some stuff this is experimental so keep that in mind.

203
00:13:50,980 --> 00:13:54,970
If you're working in a production environment you don't want to use experimental stuff on things that

204
00:13:54,970 --> 00:13:56,620
are really important and can't break.

205
00:13:56,620 --> 00:14:01,300
But other than that you have a ton a ton of functions and operations here and we're going to be discussing

206
00:14:01,300 --> 00:14:04,460
a lot of these as we continue on through this section of the course.

207
00:14:04,540 --> 00:14:09,520
But I just wanted to let you know that you can basically use all these functions just like I should

208
00:14:09,520 --> 00:14:11,380
that simple correlational on.

209
00:14:11,590 --> 00:14:17,740
So let's say for some reason you wanted to get the inverse cosign or the arc cosign of a particular

210
00:14:17,740 --> 00:14:18,170
column.

211
00:14:18,180 --> 00:14:20,500
Well you come here to the math functions section.

212
00:14:20,600 --> 00:14:24,530
Check out see that they have a S.O.S for cosign.

213
00:14:24,550 --> 00:14:29,560
See the description here computes the cosign inverse of the given column the return angle etc..

214
00:14:29,590 --> 00:14:35,290
Some information about that but more importantly it says how it expects to actually get the input and

215
00:14:35,290 --> 00:14:37,210
just like the functions that you've developed.

216
00:14:37,210 --> 00:14:39,630
So here the function is called a S.O.S.

217
00:14:39,790 --> 00:14:42,570
And it expects a column name as a string.

218
00:14:42,580 --> 00:14:48,190
So if we actually check out the correlation function that we used earlier I went ahead and searched

219
00:14:48,190 --> 00:14:48,850
for here.

220
00:14:48,850 --> 00:14:50,360
It's c o r r.

221
00:14:50,530 --> 00:14:53,590
And here we can see that there's the Pearson correlation.

222
00:14:53,620 --> 00:14:56,190
So this actually works two ways.

223
00:14:56,410 --> 00:15:03,430
One way is just say C or R and pass in the column name one is a string column name two as a string or

224
00:15:03,430 --> 00:15:06,430
what you can actually do is pass in the column itself.

225
00:15:06,640 --> 00:15:12,460
So I remember from one of the previous lectures we show that one way of just calling a column is say

226
00:15:12,510 --> 00:15:13,250
DSF.

227
00:15:13,300 --> 00:15:17,940
And then in parentheses pass in the column name that returns the actual column object.

228
00:15:17,950 --> 00:15:23,830
So this particular function works on both the string or the actual column however you want to pass it

229
00:15:24,170 --> 00:15:28,850
and you can check out the various operations and functions that are available to you.

230
00:15:28,900 --> 00:15:32,340
So we don't have enough time to go through all of these because there are so many.

231
00:15:32,350 --> 00:15:34,590
Hopefully you can see how useful some of these are.

232
00:15:34,870 --> 00:15:40,420
And for certain of these says Date Time functions we actually have whole lectures devoted to exploring

233
00:15:40,420 --> 00:15:41,770
these sections a little more.

234
00:15:41,980 --> 00:15:44,990
But what I would encourage you to do is kind of play around these check them out.

235
00:15:45,040 --> 00:15:46,810
There's a ton of math functions.

236
00:15:46,930 --> 00:15:51,250
Miscellaneous functions non area functions you can check out string functions.

237
00:15:51,280 --> 00:15:55,570
So if you ever have a lot of string data you want to compute things such as the length of the string

238
00:15:55,570 --> 00:15:58,490
or lowercase the string uppercase etc..

239
00:15:58,630 --> 00:16:02,700
Regular Expressions it's all available to hear it's built in and it's ready to go.

240
00:16:02,710 --> 00:16:07,660
So hopefully now you're really starting to see how powerful the data frame structure in Scala is for

241
00:16:07,660 --> 00:16:13,300
you and how a lot of the work has already been put into pre-built functions and you can check out the

242
00:16:13,300 --> 00:16:15,460
link in the documentation.

243
00:16:15,520 --> 00:16:19,130
It's right here and there SPARC that Apache that org and it's in your notes.

244
00:16:19,370 --> 00:16:22,890
OK thanks everyone and I will see you at the next lecture.
