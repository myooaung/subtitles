1
00:00:04,970 --> 00:00:10,190
Hello everyone and welcome to the data frame project exercise solutions lecture and this lecture.

2
00:00:10,220 --> 00:00:13,780
We're going to code through the solutions for your data frame project.

3
00:00:13,790 --> 00:00:16,420
Let's go ahead and jump to the text editor to get started.

4
00:00:16,620 --> 00:00:16,910
OK.

5
00:00:16,910 --> 00:00:20,630
Here I am at the text editor for the data frame project.

6
00:00:20,630 --> 00:00:25,140
Go ahead and check out the different project underscore solution that scul a script.

7
00:00:25,190 --> 00:00:29,750
If you ever want to reference what I'm typing out here that basically we're going to go along and just

8
00:00:29,750 --> 00:00:30,590
explain the code.

9
00:00:30,590 --> 00:00:33,680
All of these various answers for the tasks.

10
00:00:34,150 --> 00:00:36,460
OK let's go ahead and get started.

11
00:00:36,470 --> 00:00:39,500
First thing we want to do is start a simple Spark's session.

12
00:00:39,560 --> 00:00:51,840
So in order to do that you're going to have to say import or Apache SPARC that sequel that SPARC session.

13
00:00:52,080 --> 00:00:58,650
And then we want to go ahead and create a value for Sparc just as we've done in all the sparks session

14
00:00:58,650 --> 00:01:05,460
Ensberg data frame lecture's will start Sparke session and say builder get or create.

15
00:01:05,490 --> 00:01:10,020
And as I mentioned before there's a lot more options you can add to this such as giving her app a name

16
00:01:10,050 --> 00:01:12,230
or specifying how many cores you want to use.

17
00:01:12,240 --> 00:01:16,030
But for right now we'll keep it simple just add the builder and get or create.

18
00:01:16,170 --> 00:01:19,740
And then finally we're going to load the Netflix stock CAC file.

19
00:01:19,820 --> 00:01:21,710
So to do that a creative value.

20
00:01:21,950 --> 00:01:28,300
F say spark that read and I'm going to set some options here as we've done in the past.

21
00:01:28,360 --> 00:01:38,530
I'm going to set the hetter to true and then I'm going to also set the option of in first schema it

22
00:01:39,390 --> 00:01:41,440
be true.

23
00:01:41,570 --> 00:01:46,590
And then finally going to say that CXXVI and you could also say format if you wanted to.

24
00:01:46,780 --> 00:01:51,220
Well let's go out and then call the Netflix underscore or 2011 underscore 2016.

25
00:01:51,390 --> 00:01:55,390
And if you look here the extension is not see that we see that in as well.

26
00:01:56,340 --> 00:01:57,070
Okay.

27
00:01:57,560 --> 00:01:58,870
So there you have it.

28
00:01:58,990 --> 00:02:04,340
And as a quick note sometimes depending on where you save this you may get an error if you ever have

29
00:02:04,430 --> 00:02:07,950
a Mac computer and you have spaces in the name of certain files.

30
00:02:07,970 --> 00:02:13,820
You may get an error when opening a CAC file so if you got the error make sure there's no spaces in

31
00:02:13,820 --> 00:02:17,190
the file path to the actual CSP file.

32
00:02:17,190 --> 00:02:17,740
All right.

33
00:02:17,740 --> 00:02:19,070
Going to go ahead and save that.

34
00:02:19,120 --> 00:02:21,660
And now let's ask what are the column names.

35
00:02:21,730 --> 00:02:27,820
So hopefully remember that you can just say f doc columns are going to say this and let's go ahead and

36
00:02:28,210 --> 00:02:29,390
run this.

37
00:02:29,440 --> 00:02:36,730
This is the data frame project's going to say data frame underscore project Skala.

38
00:02:37,120 --> 00:02:43,330
I'm going to run load all that up and then for the rest of the solutions lecture I'm actually going

39
00:02:43,330 --> 00:02:46,290
to type straight into the terminal.

40
00:02:46,330 --> 00:02:52,660
So here I can see my first result is date open high low close volume and then adjust it close.

41
00:02:52,660 --> 00:02:57,070
So these are my columns and then I'm going to go ahead and just go straight into the terminal to answer

42
00:02:57,070 --> 00:03:00,820
the rest of those questions because I already have data framed here so I can go ahead and just reference

43
00:03:00,820 --> 00:03:06,330
it so I can say something like that show you enter in I'll see the top 20 rows.

44
00:03:06,400 --> 00:03:11,110
So we're going to go ahead and just use this given that DFA has already defined that we don't continuously

45
00:03:11,110 --> 00:03:13,160
run the script over and over again.

46
00:03:13,210 --> 00:03:15,490
So we want to know is what does the schema look like.

47
00:03:15,510 --> 00:03:20,830
Hopefully remember that you can just say death print schema and here in the terminal you can actually

48
00:03:20,830 --> 00:03:23,840
use tab to help autocomplete some of this.

49
00:03:24,340 --> 00:03:27,910
So there's the schema looks like we have the date is the time stamp.

50
00:03:27,910 --> 00:03:32,830
Everything else is either a double and then the volume is an integer which makes sense because you can't

51
00:03:32,830 --> 00:03:35,280
trade half of a stock.

52
00:03:35,290 --> 00:03:38,680
Next we want to print out the first five columns.

53
00:03:38,680 --> 00:03:42,970
Hopefully you remember that you can just say head for any number of the first columns you want to see

54
00:03:43,420 --> 00:03:48,320
and we can see that head there and we see an array of the axle Rose returned to us.

55
00:03:48,370 --> 00:03:51,890
Next we wanted to use describe to learn about the data frame.

56
00:03:51,970 --> 00:03:58,270
That's basically the answer right there you can just say ZF describe and this will report back the statistical

57
00:03:58,270 --> 00:04:01,980
information if you just say that the scribe you may not see it.

58
00:04:02,050 --> 00:04:05,570
So you need to say is that describe that show.

59
00:04:05,740 --> 00:04:11,650
And this will actually report back the max min standard deviation mean and count for the various columns

60
00:04:11,650 --> 00:04:12,530
in your data frame.

61
00:04:12,550 --> 00:04:17,080
And if you have some issues here with the actual formatting you can either print one line at a time

62
00:04:17,380 --> 00:04:22,920
or even try expanding your terminal to try to get the formatting to show a little nicer.

63
00:04:23,230 --> 00:04:23,820
All right.

64
00:04:24,370 --> 00:04:30,520
Let's go ahead and create a new data frame of a column called HIV ratio that is the ratio of the high

65
00:04:30,520 --> 00:04:34,770
price versus the volume of stock traded for that day.

66
00:04:34,780 --> 00:04:35,110
OK.

67
00:04:35,140 --> 00:04:39,730
So whenever you see create a new data frame with a new column you should be thinking the with column

68
00:04:39,730 --> 00:04:44,900
command going to say thou Ze'ev to.

69
00:04:44,940 --> 00:04:53,260
We're creating a new data frame Beldi 2 and we're going to say DMF with column and then I want the column

70
00:04:53,260 --> 00:04:56,640
to be called HIV ratio.

71
00:04:56,790 --> 00:05:03,360
So we pass in the new column name first and then as the second argument we pass in the actual calculations

72
00:05:03,360 --> 00:05:08,760
we want to do we grab the columns in this case I want the ratio of high versus price so that's going

73
00:05:08,760 --> 00:05:13,470
to be high column divided by the volume column.

74
00:05:13,470 --> 00:05:14,390
So that's a ratio

75
00:05:17,920 --> 00:05:19,810
and then make sure you have the balance parentheses.

76
00:05:19,810 --> 00:05:21,890
Let's go ahead and run that.

77
00:05:21,970 --> 00:05:23,400
Here is DFI 2.

78
00:05:23,410 --> 00:05:29,840
And if we go ahead and just check the two columns we should see now that we have this A V ratio.

79
00:05:29,860 --> 00:05:35,580
And let's go ahead and just check out the head to make sure everything looks OK and there's the ratio.

80
00:05:35,770 --> 00:05:36,820
Okay perfect.

81
00:05:36,820 --> 00:05:41,380
So you notice that it may be a very small number and that makes sense because you may have a lot of

82
00:05:41,380 --> 00:05:46,780
stock or volume trades that day as the price gets higher and the high price is limited to about three

83
00:05:46,780 --> 00:05:50,690
digits but you can have thousands and thousands of stock transactions.

84
00:05:50,830 --> 00:05:56,230
Also if you wanted to you could have reversed this to be DSF volume divided by DFI depending which way

85
00:05:56,230 --> 00:05:58,420
you want to actually read that ratio.

86
00:05:58,840 --> 00:06:04,540
OK moving on to the next question let's go ahead and clear this one to ask what day had the peak high

87
00:06:04,540 --> 00:06:06,010
in price.

88
00:06:06,010 --> 00:06:10,830
Well one very easy way to do this is just order by the high price.

89
00:06:10,830 --> 00:06:20,380
So I'll say if the order by make sure that's a lowercase order by high

90
00:06:23,130 --> 00:06:29,610
and then I'm going to say D E S C for descending and then I'm going to go ahead and just show and I

91
00:06:29,610 --> 00:06:32,700
can pass on a number here to show to see how many Rosemount return.

92
00:06:32,790 --> 00:06:34,380
And I just want the first row.

93
00:06:34,670 --> 00:06:35,040
OK.

94
00:06:35,040 --> 00:06:37,950
And here we get the high price or the high date.

95
00:06:37,950 --> 00:06:44,550
So that happened in 2015 of June 13th or excuse me July 13th.

96
00:06:44,550 --> 00:06:44,920
OK.

97
00:06:45,000 --> 00:06:46,850
So that was the peak high.

98
00:06:47,010 --> 00:06:48,300
That was the day that it happened.

99
00:06:48,330 --> 00:06:51,630
Let's go ahead and get the mean of the close column.

100
00:06:51,630 --> 00:06:56,760
So for the mean of the close column all you have to do is remember the aggregate function lecture where

101
00:06:56,760 --> 00:07:00,490
we say DFT selects passen the aggregate function.

102
00:07:00,490 --> 00:07:06,080
This case its mean pass in the column you want to get the Mino's sum this case it's close.

103
00:07:06,330 --> 00:07:08,250
And let's go ahead and show that result.

104
00:07:09,640 --> 00:07:14,740
So there it is the mean of the close column or the average is about $230.

105
00:07:14,800 --> 00:07:19,240
Let's go ahead and find the max and min of the volume column.

106
00:07:19,370 --> 00:07:28,220
So we'll say DFA but selects and it's going to be very similar will pass and Max for volume and then

107
00:07:28,220 --> 00:07:31,480
we'll go ahead and show that.

108
00:07:31,480 --> 00:07:35,170
So that's the max volume Let's go ahead and just click up and then show them in

109
00:07:38,830 --> 00:07:39,520
run that.

110
00:07:39,730 --> 00:07:43,900
And there we have the men so you can see that there's a pretty drastic difference between the max and

111
00:07:43,900 --> 00:07:44,510
the men.

112
00:07:44,770 --> 00:07:51,880
And what's fun to do is actually grab the dates of where these max or min values happen and then explore

113
00:07:51,880 --> 00:07:55,420
using Google if anything interesting happened in that stock's history.

114
00:07:55,420 --> 00:07:59,830
In this case you can go ahead and check out the maximum volume traded and see if there was like a good

115
00:07:59,830 --> 00:08:07,120
quarter for Netflix or what actually happened during that day of the men or max values of volume trading.

116
00:08:07,120 --> 00:08:07,480
All right.

117
00:08:07,480 --> 00:08:13,370
Moving along there is a quick note here that for the Scullin Spark's syntax you need to import something.

118
00:08:13,390 --> 00:08:21,370
So the thing you had to import or go ahead and write here is Sparke dot implicit stuff underscore this

119
00:08:21,370 --> 00:08:25,410
is if you want to use the Skala or Spark's syntax for these filter commands.

120
00:08:25,510 --> 00:08:29,770
Or you could have also then the spark siecle syntax in which case you didn't actually need to import

121
00:08:29,770 --> 00:08:33,960
this going to go ahead and just copy that line and imported here.

122
00:08:34,030 --> 00:08:37,160
So say import sparked the implicit

123
00:08:39,880 --> 00:08:42,960
underscore and we'll go ahead and import that.

124
00:08:42,980 --> 00:08:46,900
That way we can use these Skala spark syntax for filtering.

125
00:08:47,440 --> 00:08:52,450
OK first question was how many days was the close lower than 600 dollars.

126
00:08:52,690 --> 00:09:03,620
Well that's easy I can just say DMF philtre dollar sign close column less than 600.

127
00:09:04,130 --> 00:09:08,480
And then I can use an aggregate function to actually count how many days this occurred.

128
00:09:08,570 --> 00:09:10,230
This case is going to be counted.

129
00:09:10,240 --> 00:09:11,510
That was a little tricky for this problem.

130
00:09:11,510 --> 00:09:14,410
Try to remember that aggregate function.

131
00:09:14,570 --> 00:09:20,150
But here we can see that 1218 days the price was lower than $600.

132
00:09:20,180 --> 00:09:27,140
And if you wanted to using Sparke sequel you could have just said close less than 600 as a string.

133
00:09:27,440 --> 00:09:29,460
And then the call don't counts on that.

134
00:09:29,480 --> 00:09:30,820
Now we're giving you the same answer.

135
00:09:30,860 --> 00:09:32,300
So use whatever you prefer.

136
00:09:32,420 --> 00:09:36,110
We won't assume any simple knowledge although it's very likely that if you're this far into the course

137
00:09:36,440 --> 00:09:39,230
you probably have some database experience.

138
00:09:39,230 --> 00:09:39,480
Kate.

139
00:09:39,470 --> 00:09:42,430
Coming up next what percentage of the time was the high.

140
00:09:42,440 --> 00:09:44,120
Greater than $500.

141
00:09:44,150 --> 00:09:51,260
So it's a bit of a tricky question it's asking not just how much or how many values you get that the

142
00:09:51,260 --> 00:09:52,890
high or is greater than 500.

143
00:09:52,990 --> 00:09:56,790
It wants to know what percentage of the time was the high greater than $500.

144
00:09:56,870 --> 00:10:00,830
And that's in reference to the actual entire data frame.

145
00:10:00,950 --> 00:10:05,630
So we're going to do is say DMF and I'm going to type this out here in the script and then copy and

146
00:10:05,630 --> 00:10:09,720
paste it so we can see a little bit of the syntax highlighting.

147
00:10:09,720 --> 00:10:17,910
I want to say DFT filter where my column is greater than 500.

148
00:10:17,970 --> 00:10:19,560
I'm going to count that.

149
00:10:19,590 --> 00:10:24,850
I remember that returns an integer long integer and then one I'm going to go ahead and do it.

150
00:10:24,950 --> 00:10:26,720
This is the percentage of the time.

151
00:10:26,790 --> 00:10:34,020
So I want to divide that by the total number of rows in the data frame.

152
00:10:34,190 --> 00:10:35,570
So let's go ahead and break this down.

153
00:10:35,570 --> 00:10:38,500
We're not quite finished yet but I just want to explain what we have so far.

154
00:10:38,510 --> 00:10:42,890
The question is asking what percentage of the time was the high greater than $500.

155
00:10:42,890 --> 00:10:49,490
So the way to answer this is to grab the total number of times using count and filter that the price

156
00:10:49,490 --> 00:10:51,540
of the high was greater than $500.

157
00:10:51,650 --> 00:10:55,570
Then divide it by the total number of rows in that data frame.

158
00:10:55,580 --> 00:10:59,210
So if you interpreted this question a little differently and came up with a different solution that's

159
00:10:59,210 --> 00:10:59,820
OK too.

160
00:10:59,840 --> 00:11:02,600
But let's go ahead and understand what we have to do next.

161
00:11:02,600 --> 00:11:03,870
So we want a percentage.

162
00:11:03,980 --> 00:11:07,270
Meaning we're eventually going to have to multiply this by 100

163
00:11:10,810 --> 00:11:14,770
end just doing this is actually going to give us the answer quite yet.

164
00:11:14,840 --> 00:11:16,560
So I'm going to show you why.

165
00:11:16,730 --> 00:11:18,430
Go ahead and copy this

166
00:11:21,260 --> 00:11:23,360
and then I will go ahead and paste.

167
00:11:23,630 --> 00:11:24,440
Let's run this.

168
00:11:24,440 --> 00:11:26,170
Make sure would have any errors.

169
00:11:26,690 --> 00:11:26,910
OK.

170
00:11:26,900 --> 00:11:29,140
You notice this gets long is equal to zero.

171
00:11:29,180 --> 00:11:33,500
So if you're were doing a division and you notice something goes straight to zero that's because you're

172
00:11:33,500 --> 00:11:39,020
probably doing classic divisions that have true division since these are both integers and they're both

173
00:11:39,050 --> 00:11:40,190
long integers.

174
00:11:40,190 --> 00:11:43,180
We need to get one of them to be a double to actually get the true value.

175
00:11:43,310 --> 00:11:49,380
In this case I want to just multiply the top number by one point zero in order to convert it to a double.

176
00:11:49,400 --> 00:11:52,070
And then that should fix the actual problem.

177
00:11:52,070 --> 00:11:57,150
So let's go ahead and copy this paste it and then run it.

178
00:11:57,280 --> 00:11:59,260
Now I can see the actual percentage of the time.

179
00:11:59,260 --> 00:12:04,430
So around 5 percent of the time the high price is greater than $500.

180
00:12:04,450 --> 00:12:07,250
This is probably one of the trickier questions in this project assignment.

181
00:12:07,260 --> 00:12:11,190
So go ahead and take the time to really understand what's going on the solutions here.

182
00:12:11,200 --> 00:12:14,460
Again the question is a little tricky especially if you interpret it differently.

183
00:12:14,580 --> 00:12:17,010
We're basically asking out of the entire dataset.

184
00:12:17,020 --> 00:12:20,520
What percent of the time was that high price greater than $500.

185
00:12:20,530 --> 00:12:26,950
So we grabbed the total count of the high price being great and the $500 divided by the count of all

186
00:12:26,950 --> 00:12:28,200
the rows in the data frame.

187
00:12:28,290 --> 00:12:30,320
Multiply it by 100 to get a percentage.

188
00:12:30,400 --> 00:12:34,690
And if you get that zero error the trick was to convert one of these to a simple double.

189
00:12:35,140 --> 00:12:35,640
OK.

190
00:12:35,890 --> 00:12:40,360
Now we want to know what is the Pearson correlation between high and volume.

191
00:12:40,360 --> 00:12:44,590
Now we've actually shown this before in an exercise of using the correlation function but you just say

192
00:12:44,950 --> 00:12:51,710
DFP selects c o r r and then passen the column names as just straight strings.

193
00:12:51,760 --> 00:12:58,700
In this case I want to say core of volume and let's go in and show the results.

194
00:12:58,730 --> 00:13:00,760
So high volume T.F. select.

195
00:13:01,010 --> 00:13:05,810
And there's a correlation there's basically no correlation either negative or positive between the high

196
00:13:05,810 --> 00:13:10,470
price and the volume column which kind of makes sense given their unit difference.

197
00:13:10,790 --> 00:13:11,690
OK.

198
00:13:11,690 --> 00:13:14,880
Now finally we want to know what is the max height per year.

199
00:13:15,050 --> 00:13:20,590
This is going to be a multiverse that process so let's go ahead and write this out in the scripts will

200
00:13:20,610 --> 00:13:29,220
save Val year death and the first thing I want to do is actually create a year column.

201
00:13:29,450 --> 00:13:30,980
So I will say with column

202
00:13:33,510 --> 00:13:40,990
year I'm going to capitalize this and then I'm going to use that time stamp function or that date function

203
00:13:40,990 --> 00:13:48,700
year and pass the actual date column and then make sure my princes are balanced there.

204
00:13:49,020 --> 00:13:55,020
And then using that year if I'm going to go ahead and select the year and the high column then group

205
00:13:55,020 --> 00:14:00,340
by the year and then call the max off of that and then you can technically do this all in one giant

206
00:14:00,340 --> 00:14:00,750
step.

207
00:14:00,760 --> 00:14:03,870
But this is a little nicer as far as being able to read it.

208
00:14:03,970 --> 00:14:10,600
So I'm going to create a new frame called The Year Maxxis and it's going to be year DMF.

209
00:14:10,780 --> 00:14:21,290
I'm going to select that year column and then I'm going to selects the high column and then I'm going

210
00:14:21,290 --> 00:14:26,410
to say Group By then I will say year

211
00:14:29,550 --> 00:14:37,590
and then call the max office OK and then let's go ahead and just select what we want to show.

212
00:14:37,720 --> 00:14:44,350
In this case go ahead and save this will comment out some of these earlier commands looks like everything

213
00:14:44,350 --> 00:14:45,500
else is good.

214
00:14:45,610 --> 00:14:48,010
Come out this command for the columns.

215
00:14:48,010 --> 00:14:51,580
And let's actually just run this whole script to make sure it's all working.

216
00:14:51,580 --> 00:14:53,590
So we'll load that data frame up again.

217
00:14:53,590 --> 00:15:04,590
Well let's go ahead and clear the console and say load data frame underscore project Skala run that.

218
00:15:04,730 --> 00:15:07,960
OK so it looks like you're the year Max has worked so far.

219
00:15:07,960 --> 00:15:11,230
So we haven't actually shown any thing as far as the results yet.

220
00:15:11,350 --> 00:15:12,920
But let's go and are you done.

221
00:15:12,950 --> 00:15:15,790
I want to show what is the max high per year.

222
00:15:15,910 --> 00:15:22,920
So we want to say a new year column using the time stamp function say Dhia with column year year the

223
00:15:22,930 --> 00:15:26,460
F set that equal to year the F and then off of you.

224
00:15:26,680 --> 00:15:29,200
I'm going to just select the columns I'm concerned about.

225
00:15:29,200 --> 00:15:34,000
I'm only concerned about the year column in the high column that a group that by year and asked for

226
00:15:34,000 --> 00:15:42,990
the max and since that's year Maxi's I can always just say year Maxi's And let's go ahead and say this

227
00:15:45,100 --> 00:15:46,930
your Maxie's that selects

228
00:15:49,870 --> 00:15:51,770
year.

229
00:15:51,910 --> 00:16:02,100
Remember I have to add in the aggregate function so max of high.

230
00:16:02,290 --> 00:16:03,550
And then I'm going to just say show

231
00:16:07,160 --> 00:16:11,360
and there it is there are at the max high prices per year.

232
00:16:11,360 --> 00:16:15,240
So you can see the high price of 2011 2016 2012 et cetera.

233
00:16:15,400 --> 00:16:19,660
And if you actually want to order this what you can do is just do something like this.

234
00:16:19,670 --> 00:16:22,270
You could if you wanted to you could this all in one step.

235
00:16:22,270 --> 00:16:28,650
So it might be nicer to say something like that of results is equal to this.

236
00:16:28,670 --> 00:16:37,250
And let's go ahead and not show it and then I'm going to say results dot order by and I will order it

237
00:16:37,250 --> 00:16:37,910
by the year

238
00:16:40,740 --> 00:16:44,780
and let's actually show that results Okay.

239
00:16:44,810 --> 00:16:51,150
And there it is ordered by the year so we can see how the max price per year has changed so 120 133

240
00:16:51,240 --> 00:16:53,800
389 49 716.

241
00:16:53,830 --> 00:16:59,110
And then we got a drop to 129 in 2016 and if you're wondering why that it dropped so much.

242
00:16:59,110 --> 00:17:04,660
Well this is most likely due to something like a stock split.

243
00:17:04,700 --> 00:17:09,830
Finally the very last question we wanted to answer was What does the average close for each calendar

244
00:17:09,830 --> 00:17:10,500
month.

245
00:17:10,550 --> 00:17:11,900
This is going to be really similar.

246
00:17:11,900 --> 00:17:13,640
So let's go ahead and break it down.

247
00:17:15,050 --> 00:17:19,860
I'm going to go ahead and separate out this into multiple steps by saying that all month.

248
00:17:20,660 --> 00:17:32,070
It's going to be death with column and then we can say month and then call the Month function off the

249
00:17:32,070 --> 00:17:39,330
date column and then I don't want every single column I just want the month and the close columns now.

250
00:17:39,500 --> 00:17:42,450
So let's go ahead and say thou month.

251
00:17:42,480 --> 00:17:51,540
Avi G-S is equal to month that selects and then I'm going to say month.

252
00:17:51,580 --> 00:17:59,290
Let's go to make sure that's capitalized and then I'm going to also ask for the close call that I'm

253
00:17:59,290 --> 00:18:09,270
going to group by the month column and ask for the means and I want the average And then finally let's

254
00:18:09,270 --> 00:18:10,350
go ahead and show that.

255
00:18:10,350 --> 00:18:13,040
So I will say month EVGA Yes.

256
00:18:13,290 --> 00:18:18,800
Select the columns I want right now it has the high low close volumes etc..

257
00:18:18,990 --> 00:18:27,750
Let's go ahead and just grabbed that month column and then also the year average of the close column

258
00:18:29,440 --> 00:18:31,080
and then we'll show that.

259
00:18:31,180 --> 00:18:38,090
Regarding comments out these two lines here save this and let's go ahead and run the script.

260
00:18:39,210 --> 00:18:43,620
So we will load data from their score project about Skala and see the results.

261
00:18:44,040 --> 00:18:44,480
Okay.

262
00:18:44,580 --> 00:18:46,030
And there it is.

263
00:18:46,200 --> 00:18:49,760
Here are the results so for the calendar month we get the average close price.

264
00:18:49,800 --> 00:18:51,780
Now right now this is again not sorted.

265
00:18:51,780 --> 00:18:54,770
So if you wanted to you could again use ORDER BY.

266
00:18:54,810 --> 00:18:55,920
So let's go ahead and do that.

267
00:18:55,920 --> 00:19:02,990
We're going to go ahead and say that ORDER BY WHEN TO ORDER BY THE MONTH.

268
00:19:02,990 --> 00:19:09,210
Now last time we did this in a separate step but it's also okay to just do it all in one step.

269
00:19:09,310 --> 00:19:14,720
Go ahead and run that again and he can see it's been ordered by the month.

270
00:19:14,720 --> 00:19:18,710
So you can compare the average close price for a calendar month.

271
00:19:18,710 --> 00:19:23,010
Now keep in mind this is the trend across all the years in the data set.

272
00:19:23,030 --> 00:19:27,480
So this is for January of all the years in the Netflix data set.

273
00:19:27,650 --> 00:19:33,710
In this case those years we go ahead and look at that dataset name it goes from 2011 to 2016 and you

274
00:19:33,710 --> 00:19:41,000
can see here that this is going to represent the average closing price for every January 2011 to 2016.

275
00:19:41,000 --> 00:19:46,550
So all the days for all those years sorted or grouped by the month.

276
00:19:46,580 --> 00:19:47,700
All right.

277
00:19:47,780 --> 00:19:52,940
So hopefully this was a fun challenging exercise for you.

278
00:19:52,940 --> 00:19:56,330
Some of these questions are way trickier than others especially these last two.

279
00:19:56,360 --> 00:19:57,850
What is Max high per year.

280
00:19:57,860 --> 00:19:59,840
What is average close for each calendar month.

281
00:19:59,840 --> 00:20:02,200
Those are probably some of the much trickier questions.

282
00:20:02,210 --> 00:20:07,130
And then the other tricky question was this what percentage of the time was height greater than 500.

283
00:20:07,130 --> 00:20:12,340
Those three questions are much harder than the rest of them but hopefully had a good stab at it and

284
00:20:12,350 --> 00:20:14,120
had fun while you're doing the exercise.

285
00:20:14,160 --> 00:20:18,590
If you have any questions feel free to post the Q&amp;A forums and I'll be happy to help you out.

286
00:20:18,590 --> 00:20:20,420
Thanks everyone and I'll see you at the next lecture.
