WEBVTT
1
00:00:05.500 --> 00:00:10.820
Hello everyone and welcome to the data Brick's overview lecture in this lecture.

2
00:00:10.840 --> 00:00:15.160
We're going to be showing you how to set up a data Brick's notebook and how to sign up for the free

3
00:00:15.160 --> 00:00:17.250
community edition of data bricks.

4
00:00:17.500 --> 00:00:21.220
Then we'll show you how to upload data to your notebook environment.

5
00:00:21.520 --> 00:00:26.410
Afterwards in the next lecture we're going to create a recommender system using the canonical movie

6
00:00:26.410 --> 00:00:27.750
lines data set.

7
00:00:27.820 --> 00:00:33.850
This dataset contains information about user IDs movie IDs and then the rating they gave that particular

8
00:00:33.850 --> 00:00:36.480
movie from one to five stars.

9
00:00:37.630 --> 00:00:41.060
This lecture contains a resource see every file free to download.

10
00:00:41.170 --> 00:00:46.450
This file contains the movie ratings data although we're also going to visit the Web site of movie Lions

11
00:00:46.690 --> 00:00:50.090
in case you want to play with the full data 20 million ratings set.

12
00:00:51.650 --> 00:00:52.790
Let's get started.

13
00:00:53.050 --> 00:00:57.020
Open your browser and go to w w w thought to Brigstock com.

14
00:00:57.180 --> 00:01:06.090
I'll see there OK here I am at data Brick's dot com and you can go to HTP as Colon slash slash data

15
00:01:06.110 --> 00:01:09.550
Brigstock com or just google for data Brick's to find this web site.

16
00:01:09.650 --> 00:01:14.090
And then somewhere along this site you should see trial data Brick's for free.

17
00:01:14.090 --> 00:01:17.360
You can click on one of these boxes where it says try data bricks.

18
00:01:17.360 --> 00:01:22.790
Or if you can't find that particular link just go to Datarock saw slash tried cache data bricks and

19
00:01:22.860 --> 00:01:27.740
you see this where you have the full platform trial and the community edition.

20
00:01:27.860 --> 00:01:34.340
Basically data bricks are going to be notebooks that are connected to clusters on a W-S or Amazon Web

21
00:01:34.340 --> 00:01:37.320
Services and what the full platform trial.

22
00:01:37.370 --> 00:01:43.270
You get a two week free trial that's excluding those Amazon Web Services charges and it has things such

23
00:01:43.270 --> 00:01:49.190
as business intelligence tools an interactive guide to spark and data Brick's notebooks dashboards unlimited

24
00:01:49.190 --> 00:01:55.340
clusters etc. We're going to be using the free community edition that involves just the mini 6 gigabyte

25
00:01:55.340 --> 00:01:59.800
cluster an interactive notebook and dashboard environment which we're going to be playing around with.

26
00:01:59.900 --> 00:02:02.660
And then the public environment the share your work.

27
00:02:02.660 --> 00:02:06.250
Now keep in mind this six gigabyte cluster is a whole lot.

28
00:02:06.260 --> 00:02:09.800
In fact most modern computers have more than six gigabytes of RAM.

29
00:02:09.920 --> 00:02:13.060
So this really makes sense to do a real spark analysis.

30
00:02:13.190 --> 00:02:18.360
But this should give you a little taste of what it's like to run a notebook on a real cluster.

31
00:02:18.650 --> 00:02:20.830
Click here where it says start today.

32
00:02:21.620 --> 00:02:27.230
And that should take you to the Sign-Up page in this sign up page Pessin your first name last name your

33
00:02:27.230 --> 00:02:28.520
email company name.

34
00:02:28.520 --> 00:02:30.510
You can just type in whatever you want there.

35
00:02:30.650 --> 00:02:33.970
Set up a password and then a Sign-Up will send you an email.

36
00:02:34.010 --> 00:02:36.160
And in that e-mail will be a confirmation link.

37
00:02:36.260 --> 00:02:41.450
So make sure you use a function e-mail that you can check once you've signed up click on the e-mail

38
00:02:41.450 --> 00:02:45.370
link and it should take you to the notebook page and Lloyd to jump to that notebook page.

39
00:02:45.390 --> 00:02:50.800
Now OK here I am at the welcome page for the data Brick's community edition.

40
00:02:50.960 --> 00:02:54.310
And you can find this a community dot cloud data Brigstock com.

41
00:02:54.380 --> 00:02:58.340
Remember this should pop up after you sign in with the e-mail link.

42
00:02:58.340 --> 00:03:00.900
Check your e-mail for that confirmation link.

43
00:03:00.970 --> 00:03:03.250
And once you sign that it should look like this.

44
00:03:03.260 --> 00:03:07.780
Now there's some featured notebooks here such as introduction to pacis spark on data bricks made of

45
00:03:07.800 --> 00:03:11.260
bricks for data scientists an introduction to structures streaming.

46
00:03:11.300 --> 00:03:13.190
I would really recommend they check this out.

47
00:03:13.190 --> 00:03:15.040
There's a lot of great documentation here.

48
00:03:15.140 --> 00:03:18.390
For more information and how to actually use the data brig's platform.

49
00:03:18.590 --> 00:03:23.290
So for instance I would recommend that you check out this introduction to Apache Sparke on data bricks.

50
00:03:23.450 --> 00:03:24.860
It says it's written in Python.

51
00:03:25.070 --> 00:03:26.900
Data Brick's supports multiple languages.

52
00:03:26.900 --> 00:03:30.710
Java Python Scala all the languages that SPARC runs.

53
00:03:30.920 --> 00:03:36.470
But in this case this particular notebook is actually more text than in the code and also has links

54
00:03:36.470 --> 00:03:39.210
to other notebooks in this gentle introduction series.

55
00:03:39.380 --> 00:03:43.380
And this really allows you to be familiar with the data Brick's terminology.

56
00:03:43.420 --> 00:03:48.170
There's workspaces and that basically allows you to organize all the work you're doing and most of the

57
00:03:48.170 --> 00:03:51.260
work you're going to be doing isn't what's known as a notebook.

58
00:03:51.260 --> 00:03:56.780
Now if you're familiar of Jupiter notebooks from other courses or languages such as Python or are you

59
00:03:56.780 --> 00:03:58.630
probably already understand what a notebook is.

60
00:03:58.640 --> 00:04:01.010
But if you don't have experience notebooks that's OK.

61
00:04:01.160 --> 00:04:04.780
In this lecture we're going to be walking you through what a notebook is.

62
00:04:05.030 --> 00:04:09.710
And there's also libraries and then tables and remember tables are just going to be the structure data

63
00:04:09.950 --> 00:04:13.430
that we're going to be reading from for our data frames.

64
00:04:13.460 --> 00:04:17.870
Then there's clusters and clusters are the groups of computers there were going to be treating conveniently

65
00:04:17.990 --> 00:04:23.030
as a single computer and basically with the state of snowpack environment that's going to mean that

66
00:04:23.030 --> 00:04:29.100
we can effectively treat 20 computers just as if it was a single computer connected to a single notebook.

67
00:04:29.510 --> 00:04:35.300
What you're doing here is you're paying data breaks a little extra on top of Amazon Web Services or

68
00:04:35.300 --> 00:04:41.180
the convenience of this interface of a notebook directly connected to a cluster.

69
00:04:41.180 --> 00:04:45.980
These clusters are then going to execute jobs and then you have apps which are third party integrations

70
00:04:45.980 --> 00:04:47.560
with the data Brick's platform.

71
00:04:47.600 --> 00:04:50.020
So go ahead and read through this a little more.

72
00:04:50.030 --> 00:04:54.650
But basically this just describes data bricks in a little more detail stuff that we're not going to

73
00:04:54.650 --> 00:05:01.250
really cover such as pacis sparks history or context or environment or data interfaces such as data

74
00:05:01.250 --> 00:05:03.700
sets or data frames we're already pretty familiar that.

75
00:05:03.860 --> 00:05:07.370
But then it shows you how to create a cluster starting in notebook etc..

76
00:05:07.430 --> 00:05:11.210
Now it also talks about the Apache SPARC architecture and how this works.

77
00:05:11.210 --> 00:05:14.040
We discuss this a little bit but it's always nice to review.

78
00:05:14.420 --> 00:05:14.880
OK.

79
00:05:14.990 --> 00:05:17.560
So I would recommend you take the time to read through this notebook.

80
00:05:17.600 --> 00:05:21.800
Don't worry that it's coded in Python there's really not that much code here that's just showing you

81
00:05:21.800 --> 00:05:25.800
some examples of what's possible on the data Brick's platform.

82
00:05:25.810 --> 00:05:31.180
All right what I want you to do now is click here on home and you should see something that looks like

83
00:05:31.180 --> 00:05:34.460
this on home or we're going to do is start a new notebook.

84
00:05:34.540 --> 00:05:39.850
Now we can see here you have a user's documentation release notes training tutorials shared then you

85
00:05:39.850 --> 00:05:44.980
have users that should be your username at whatever e-mail you have and then some notebooks that you

86
00:05:44.980 --> 00:05:50.240
open in this case a very open introduction to a patch spark and data Brick's for data scientists and

87
00:05:50.240 --> 00:05:51.860
note books that came with it.

88
00:05:51.880 --> 00:05:58.520
If you come back here to data Brick's that's going to allow you to create a new notebook.

89
00:05:58.520 --> 00:06:00.980
Now first we actually need to create a cluster.

90
00:06:00.980 --> 00:06:05.750
So I'm going to start a new notebook and show you how concerned you closer from a notebook in case you

91
00:06:05.750 --> 00:06:08.750
ever accidentally started you know books about starting a cluster.

92
00:06:08.750 --> 00:06:09.900
Here we have our new notebook.

93
00:06:09.900 --> 00:06:16.030
We'll call this my first notebook you can call it whatever you want.

94
00:06:16.050 --> 00:06:19.440
It makes you pick the language scholars and that's what we've been working in.

95
00:06:19.500 --> 00:06:25.160
Click creates and this creates this Scullin up book and you'll notice that it says detached.

96
00:06:25.160 --> 00:06:31.500
And basically here in this notebook environment we have cells where we can type in Scala or Sparke code

97
00:06:31.530 --> 00:06:34.980
such as print Ellen and we will say hello.

98
00:06:35.010 --> 00:06:37.820
What's world.

99
00:06:38.660 --> 00:06:42.670
And I can either click play here to run this or hit shift.

100
00:06:42.710 --> 00:06:44.090
Plus enter to run.

101
00:06:44.180 --> 00:06:50.140
Now if I run the cell we should get a little alert telling us we have no cluster OK.

102
00:06:50.140 --> 00:06:52.210
So here is this note because not attached to a cluster.

103
00:06:52.210 --> 00:06:54.260
Would you like to launch a new cluster.

104
00:06:54.340 --> 00:07:00.220
Notice here that the default is the older version of SPARC one point 6.2 instead of actually clicking

105
00:07:00.220 --> 00:07:07.350
launching and run click Cancel here and Ill say detached or we're going to do is under the tach say

106
00:07:07.710 --> 00:07:12.690
could that create a cluster or you could just go back to data bricks and instead of saying you know

107
00:07:12.690 --> 00:07:14.410
books a new cluster.

108
00:07:14.430 --> 00:07:21.570
Now here we have a new cluster Let's get this cluster name we'll call it like cluster and you can here

109
00:07:21.570 --> 00:07:30.480
choose the version of SPARC and Skala you want this case we want SPARC 2.0 0.2 or 2.0 0.1.

110
00:07:30.870 --> 00:07:33.820
Or are the latest versions of SPARC at this moment in time.

111
00:07:33.930 --> 00:07:39.220
I'm going to choose the latest version available to me which is SPARC 2.0 point two way scholar two

112
00:07:39.220 --> 00:07:41.450
point one one or two point eleven.

113
00:07:41.790 --> 00:07:44.190
And in this instance the reason the community.

114
00:07:44.340 --> 00:07:50.920
We get free six gigabytes of memory and then you can also have advanced settings such as AWOS where

115
00:07:50.920 --> 00:07:52.350
it's going to be organized.

116
00:07:52.420 --> 00:07:54.270
So the availability zone et cetera.

117
00:07:54.340 --> 00:07:56.590
And then you have Spart configuration.

118
00:07:56.620 --> 00:08:00.460
Most of these advanced settings are not going be able to change much because you're working on the community

119
00:08:00.460 --> 00:08:03.480
edition which is just six gigabytes of memory.

120
00:08:03.550 --> 00:08:10.290
So once you've set up the Apache SPARC version to be 2.0 does it have to be 2.0 0.2 can be 2.0 one or

121
00:08:10.300 --> 00:08:11.390
just 2.0.

122
00:08:11.460 --> 00:08:13.920
I recommend using probably the latest version 2.0.

123
00:08:14.050 --> 00:08:21.720
Click here on create cluster and this will eventually create that cluster and attach it to this notebook.

124
00:08:21.730 --> 00:08:26.920
So here if we go to the attached now we will see attached to my cluster.

125
00:08:26.920 --> 00:08:29.890
This cluster is being initiated and created.

126
00:08:29.890 --> 00:08:32.250
Click here and it will say pending.

127
00:08:32.260 --> 00:08:36.870
And once the cluster is done being created and run on AWOS it will connect to this notebook.

128
00:08:37.060 --> 00:08:40.070
You may have to wait about 30 seconds for that to connect.

129
00:08:40.120 --> 00:08:43.200
So I'm going to fast forward in time for that finishing pending.

130
00:08:43.420 --> 00:08:43.810
OK.

131
00:08:43.870 --> 00:08:44.820
It just finished.

132
00:08:44.840 --> 00:08:50.980
I actually have asked for those more about 10 seconds and now I can say Prince Ellen hello world and

133
00:08:50.980 --> 00:08:52.380
run the cell.

134
00:08:52.420 --> 00:08:53.000
All right.

135
00:08:53.140 --> 00:08:56.860
So here we can see after running that cell we get the output directly below.

136
00:08:57.130 --> 00:08:59.460
And if you want a new cell you have a couple of options.

137
00:08:59.470 --> 00:09:05.950
You can click here on this and it down to where it says add cell below or add cell above and you will

138
00:09:05.950 --> 00:09:07.270
end up getting a new cell.

139
00:09:07.270 --> 00:09:08.850
There's actually a bunch of shortcuts.

140
00:09:08.920 --> 00:09:13.600
If you ever want to refer to them click here on shortcuts and this will review all the shortcuts that

141
00:09:13.750 --> 00:09:19.630
are available to such as shortcuts for inserting a inserting a cell above running a command than inserting

142
00:09:19.630 --> 00:09:25.120
a cell below just running command that cetera definitely as you use notebooks more and more these will

143
00:09:25.120 --> 00:09:30.130
become second nature to you right now will basically just be worrying about shift enter which is running

144
00:09:30.130 --> 00:09:35.040
a cell you know what's really cool about a notebook environment is the fact that you get to see the

145
00:09:35.040 --> 00:09:36.820
output directly below the cell.

146
00:09:36.900 --> 00:09:41.220
There's no need to load up a spark shell and run an entire script.

147
00:09:42.130 --> 00:09:49.930
For instance if I say vallÃ©e x is equal to hello and do shift enter this will run this it create that

148
00:09:49.930 --> 00:09:51.880
string x.

149
00:09:52.000 --> 00:09:56.680
And what's nice about this notebook environment is you can basically treat each cell as if it was just

150
00:09:56.680 --> 00:10:01.130
a single line or multiple lines being inputted into your SPARC shell.

151
00:10:01.300 --> 00:10:08.290
For instance we can make another variable My gnome equal to to shift and try to run that cell and it

152
00:10:08.290 --> 00:10:09.110
runs it there.

153
00:10:09.120 --> 00:10:11.340
We see the output below.

154
00:10:11.350 --> 00:10:16.900
Now if I referenced it here with myname shift enter I can see that it was the second result.

155
00:10:16.900 --> 00:10:18.750
Integer is equal to two.

156
00:10:18.760 --> 00:10:19.110
All right.

157
00:10:19.120 --> 00:10:21.910
So that's really the basics of what we want to cover for notebooks.

158
00:10:22.000 --> 00:10:27.070
For now we're going to get a lot more practice with them and we actually build a recommender system.

159
00:10:27.070 --> 00:10:32.950
Essentially you can weigh out the pros and cons of a notebook by saying the pros is this cell interface

160
00:10:32.950 --> 00:10:39.130
which quickly allows you to play around with variables and data and make quick little chunks of code

161
00:10:39.280 --> 00:10:43.080
easily accessible to you and run chunks of code at a time.

162
00:10:43.350 --> 00:10:49.250
Because being this isn't an ideal programming environment where you want a much longer sparker Scalia's

163
00:10:49.260 --> 00:10:55.900
script usually you'd probably want to go back to Intelli J and run your code there because these cell

164
00:10:55.900 --> 00:11:00.110
formats isn't going to be the best thing for running large chunks of code.

165
00:11:00.400 --> 00:11:04.380
All right now let me show you how to actually upload data.

166
00:11:04.450 --> 00:11:11.680
If you click here on the tables icon and then click here or create a table you will see the data import

167
00:11:11.680 --> 00:11:17.510
menu and basically data Brick's allows you to import files from a variety of sources.

168
00:11:17.530 --> 00:11:24.010
In this case we have it just as file and we can't upload the file but I want you to do is download the

169
00:11:24.010 --> 00:11:29.250
resource file that contains the movie ratings and then upload it here.

170
00:11:29.290 --> 00:11:32.870
But before I upload it I want to show you the different options.

171
00:11:32.900 --> 00:11:39.340
There's S-3 and S-3 is a very common storage especially for large data and it's basically a storage

172
00:11:39.370 --> 00:11:41.170
of Amazon Web Services.

173
00:11:41.170 --> 00:11:46.560
So familiar if if you're familiar of Amazon Web Services all you have to do is passing your key id your

174
00:11:46.570 --> 00:11:49.550
secret access key as well as the S3 bucket name.

175
00:11:49.600 --> 00:11:54.560
And you'd be able to browse that bucket and import data directly from your S-3 bucket.

176
00:11:54.610 --> 00:12:00.360
Usually you're not going to be importing or uploading huge files of directly from your computer.

177
00:12:00.370 --> 00:12:02.050
You would do that from S-3.

178
00:12:02.050 --> 00:12:06.640
The other file format available to you is this data Brick's file system.

179
00:12:06.640 --> 00:12:11.470
And basically what this is is if you upload a file at a data Brick's it's going to turn it into this

180
00:12:11.470 --> 00:12:14.130
data Brick's file system or format.

181
00:12:14.410 --> 00:12:19.660
And then there's also Jaydee B.C. And this is basically going to allow you to connect to a typical sequel

182
00:12:19.660 --> 00:12:24.610
type database you'd have to put into your URL and you can connect to that database to your username

183
00:12:24.610 --> 00:12:29.770
the password and then either the table you want or some sort of sequel query to actually connect to

184
00:12:29.770 --> 00:12:31.270
that table.

185
00:12:31.270 --> 00:12:37.680
Let's go back to file and upload the actual movie ratings file that came as a resource for this lecture.

186
00:12:37.840 --> 00:12:41.450
We're going to just have to drag it and drop here.

187
00:12:41.490 --> 00:12:46.350
OK once that's done uploading make sure you check out this link right here.

188
00:12:46.360 --> 00:12:54.160
This is the link for the data bricks file system and it's going to save it onto your data file system.

189
00:12:54.160 --> 00:12:58.630
Copy this link and then go back to your workspace to your notebook.

190
00:12:58.630 --> 00:13:01.240
In this case we're going to go to my first notebook.

191
00:13:01.360 --> 00:13:02.450
Double click on that.

192
00:13:02.660 --> 00:13:07.780
And let's actually grab this and read the CACP file.

193
00:13:07.780 --> 00:13:11.090
In this case we're going to do what we always do.

194
00:13:11.180 --> 00:13:16.070
We're going to just say thou is equal to what.

195
00:13:16.190 --> 00:13:17.370
Well data.

196
00:13:17.390 --> 00:13:21.790
Scuse me it's equal to spark da read.

197
00:13:22.140 --> 00:13:29.840
And since this is a C as we file we can just say option hetter So be true.

198
00:13:29.940 --> 00:13:36.950
And then we will also say Option 2 and for schema so that we don't just get them all strings to be true

199
00:13:37.700 --> 00:13:46.110
and then I will say that cxxviii and I will pass that string of the file store movie ratings CACP.

200
00:13:46.190 --> 00:13:50.390
So all you have to do is say validate is equal to and just like we've done in the past spark that read

201
00:13:50.390 --> 00:13:55.970
option etc. the options for the CAC file something that's really interesting to note here however is

202
00:13:55.970 --> 00:14:01.370
that they have to actually import Sparke the Daybreak's notebook format understands that when you say

203
00:14:01.370 --> 00:14:05.320
SPARC here you're referring to this cluster connection that you already did.

204
00:14:05.600 --> 00:14:11.640
So I will do alt enter in Ultegra is going to run the cell and create a cell below it.

205
00:14:12.720 --> 00:14:13.060
OK.

206
00:14:13.080 --> 00:14:21.330
So it's done those SPARC jobs and now we have data which is a data frame we take out data and say print

207
00:14:21.450 --> 00:14:29.350
schema shift and we can see here that we have a user id a movie Id and a rating.

208
00:14:29.400 --> 00:14:32.310
And basically what this is is a particular user ID.

209
00:14:32.310 --> 00:14:38.370
For instance you can imagine this being on Netflix saw a particular movie with a numerical ID number

210
00:14:38.700 --> 00:14:42.570
and a numerical ID number is also related to a particular title.

211
00:14:42.570 --> 00:14:47.790
So for instance movie one is equal to Star Wars movie 2 is equal to Indiana Jones etc. and then they

212
00:14:47.790 --> 00:14:49.550
gave a rating a rating.

213
00:14:49.560 --> 00:14:55.680
It's just a number between one to five as a double and we can actually check that out by asking for

214
00:14:55.680 --> 00:14:57.140
the head of the data.

215
00:14:57.270 --> 00:15:04.160
So I will say data head well it's grabbed maybe the first five rows of that data that we can see here

216
00:15:04.160 --> 00:15:10.440
we have user id one for the movie the 31 gave it two and a half stars movie Id.

217
00:15:10.460 --> 00:15:17.120
Or you squeeze me the one for the movie The 1029 gave this rating gave this particular movie three stars

218
00:15:17.160 --> 00:15:22.460
etc. and all the ones in the beginning are for that first user and then it goes to user number to the

219
00:15:22.460 --> 00:15:27.920
movies they've watched the rating and gave it user number three etc. So make sure that you've been able

220
00:15:27.920 --> 00:15:34.700
to successfully upload your data to the tables and been able to read it using this option and really

221
00:15:34.700 --> 00:15:37.970
make sure that you save wherever this CSP file is.

222
00:15:38.150 --> 00:15:40.230
So copy this and write it down somewhere.

223
00:15:40.250 --> 00:15:42.650
That way you always have it as a reference.

224
00:15:42.650 --> 00:15:43.720
OK.

225
00:15:43.790 --> 00:15:48.980
Coming up next we're going to actually build out our recommender system on the movie lence data sets.

226
00:15:50.830 --> 00:15:55.310
If you're interested in finding out more about the actual movie lens dataset besides a small sample

227
00:15:55.310 --> 00:15:56.300
we have here.

228
00:15:56.480 --> 00:16:02.930
You can click here or actually go to group lens dot org slash data sites slash movie lines or basically

229
00:16:02.930 --> 00:16:08.120
just google movie lens and this will take you to the actual official research page for a group lends

230
00:16:08.120 --> 00:16:13.640
research and group blends research is basically the creator of this canonical data set.

231
00:16:13.700 --> 00:16:18.500
If you ever see any tutorials on recommender systems they're pretty much always benchmarked by this

232
00:16:18.500 --> 00:16:20.040
particular dataset.

233
00:16:20.400 --> 00:16:24.380
Now the large data set is actually 20 million ratings.

234
00:16:24.380 --> 00:16:30.530
So if you want to actually push it all the way AWOS maybe you purchased the actual data Brick's not

235
00:16:30.530 --> 00:16:31.570
the community version.

236
00:16:31.610 --> 00:16:33.290
You can download the zip file.

237
00:16:33.290 --> 00:16:36.930
It's 190 megabytes zips it's quite a bit larger when you add zip it.

238
00:16:37.070 --> 00:16:42.560
But for instance right now we're actually just using this much smaller data set we're using this latest

239
00:16:42.560 --> 00:16:47.460
small which was about 1 megabyte unzipped or 1.4 megabytes.

240
00:16:47.550 --> 00:16:50.430
So it's really up to you if you want to play around a larger data set.

241
00:16:50.510 --> 00:16:51.940
It really is the power of SPARC.

242
00:16:52.040 --> 00:16:54.860
But remember you're not to pay for a larger cluster.

243
00:16:54.860 --> 00:16:58.200
Right now we're keeping things small to keep everything in the free tier.

244
00:16:58.320 --> 00:17:04.090
You can find that more information just by googling movie lens or going to group lens or flash slash

245
00:17:04.180 --> 00:17:05.160
movie lines.

246
00:17:05.330 --> 00:17:10.490
But all we did in this particular lecture was show you how to set up your data account set up our first

247
00:17:10.490 --> 00:17:15.050
notebook set up the first cluster and upload the movie ratings data now we're going to be using in the

248
00:17:15.050 --> 00:17:17.930
next lecture to actually build the recommendation system.

249
00:17:18.010 --> 00:17:18.610
OK.

250
00:17:18.710 --> 00:17:21.620
Do you have any questions feel free to post them to the Q&amp;A forums.

251
00:17:21.620 --> 00:17:23.420
Thanks everyone and I'll see you at the next lecture.
