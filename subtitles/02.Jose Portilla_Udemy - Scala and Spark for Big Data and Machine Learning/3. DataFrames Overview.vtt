WEBVTT
1
00:00:05.400 --> 00:00:09.410
Hello everyone and welcome to the data frame's overview lecture.

2
00:00:09.420 --> 00:00:14.080
This is going to be our very first lecture we actually get started with programming with data frame

3
00:00:14.080 --> 00:00:15.840
spark and Skala.

4
00:00:15.870 --> 00:00:21.750
Let's go ahead and jump to the Adam text editor but I want you to do is actually find the folder Sparke

5
00:00:21.750 --> 00:00:27.410
data frames from the download for this course and then add that as a project folder.

6
00:00:27.420 --> 00:00:31.160
If you haven't done so already let's go ahead and jump to the editor now.

7
00:00:31.290 --> 00:00:33.770
OK here I am at Adam text editor.

8
00:00:33.780 --> 00:00:38.610
Now you may be wondering why haven't we gone to a full idea yet for already working a spark and data

9
00:00:38.610 --> 00:00:39.320
frames.

10
00:00:39.540 --> 00:00:44.920
You can jump to a full ID such as the Skala the eclipse or intelligent.

11
00:00:45.120 --> 00:00:49.830
Right now I think you'll still maximize your learning if you use a simple text editor and really force

12
00:00:49.830 --> 00:00:52.010
yourself to type out every single command.

13
00:00:52.140 --> 00:00:56.760
So we're going to stick with text editors maximize our learning regardless if it's a maybe a little

14
00:00:56.760 --> 00:00:59.190
more difficult than using a fool either.

15
00:00:59.220 --> 00:01:00.260
So just keep that in mind.

16
00:01:00.270 --> 00:01:04.410
You can use whatever you want but I'm going to stick with the text editor right now for this section

17
00:01:04.410 --> 00:01:05.450
of the course.

18
00:01:05.790 --> 00:01:06.070
OK.

19
00:01:06.090 --> 00:01:10.540
Hopefully you went ahead and added spark data frames as a project folder.

20
00:01:10.620 --> 00:01:15.960
You can see here that there's a data frame underscore overview that Skala script I have open right here.

21
00:01:15.990 --> 00:01:19.860
This is the script there we're actually going to be working through and explaining and we're going to

22
00:01:19.950 --> 00:01:23.820
just show you some very basic operations you can perform on a data frame.

23
00:01:24.090 --> 00:01:26.960
Let's go ahead and start a new file.

24
00:01:26.960 --> 00:01:30.920
So the right click on sparc data frames say new file.

25
00:01:31.070 --> 00:01:34.950
I'm going to call it something short like DFA Scala.

26
00:01:34.980 --> 00:01:38.650
OK so let's go ahead and start with a simple Sparke session.

27
00:01:39.480 --> 00:01:43.710
So Sparke session is what you're going to use when you're working if Sparke data frames and you can

28
00:01:43.710 --> 00:01:56.870
grab it by saying import or apache not SPARC that sequel and then Sparke session OK and then we're going

29
00:01:56.870 --> 00:02:05.940
to go ahead and say vow of SPARC is equal to spark session and then we'll say builder close print sees

30
00:02:05.940 --> 00:02:12.780
gets or creates and that will either get the current Spark's session or actually create one.

31
00:02:12.780 --> 00:02:16.680
Now there's a lot more options you can add here on the spark session such as telling how many course

32
00:02:16.680 --> 00:02:19.010
you want to use or give it an application name.

33
00:02:19.020 --> 00:02:23.490
Right now we're just keeping things simple so we're going to just focus line by line and actually creating

34
00:02:23.490 --> 00:02:24.130
values.

35
00:02:24.240 --> 00:02:28.830
We're not going to do anything fancy like create a whole function or an object just line by line to

36
00:02:29.030 --> 00:02:31.320
hopefully maximize our learning or actually work.

37
00:02:31.380 --> 00:02:32.900
These data frames.

38
00:02:32.910 --> 00:02:36.140
So you can create data frame from a spark session.

39
00:02:36.210 --> 00:02:39.550
Remember this is known as a class data set.

40
00:02:39.720 --> 00:02:44.280
So when you're looking at that documentation link from the previous lecture you may be a little computer

41
00:02:44.280 --> 00:02:45.150
that says data set.

42
00:02:45.150 --> 00:02:51.390
There is a set of data frames remember a data frame is just a data set organized with column names.

43
00:02:51.980 --> 00:02:59.310
So we're going to go ahead and say Valdai Yeff and then I'm going to go ahead and say spark that read

44
00:02:59.910 --> 00:03:05.510
and then I can say CXXVI and pasan a CSFB file.

45
00:03:05.520 --> 00:03:09.920
So right now we're just working off simple CSP files since we're just dealing on a local computer.

46
00:03:09.930 --> 00:03:15.000
Later on we're going to have a whole lecture on data input and output because big data sets usually

47
00:03:15.000 --> 00:03:17.160
have very particular file formats.

48
00:03:17.160 --> 00:03:18.660
Right now we're dealing everything locally.

49
00:03:18.660 --> 00:03:21.270
So a very simple format to use ESV.

50
00:03:21.360 --> 00:03:25.260
It's also a very common format for data to become or be shipped in.

51
00:03:25.260 --> 00:03:29.170
So what we're going to do is work with this Citigroup 2006 2008.

52
00:03:29.340 --> 00:03:34.180
Now this is basically stock information for Citigroup from the year 2006 2008.

53
00:03:34.260 --> 00:03:35.610
So has some interesting trends in it.

54
00:03:35.610 --> 00:03:39.240
We're going to be working with this dataset throughout the section of the course.

55
00:03:39.270 --> 00:03:43.520
But basically this is from the financial crisis and Citigroup had a big dive.

56
00:03:43.530 --> 00:03:45.900
So it's kind of an interesting data set to work with.

57
00:03:45.930 --> 00:03:48.070
We'll go ahead and say read that CSB.

58
00:03:48.090 --> 00:03:54.030
Citigroup 2006 underscore 2008 and the way I saved that here doesn't actually have the dot CACP file

59
00:03:54.030 --> 00:03:57.050
extensions so keep that in mind.

60
00:03:57.080 --> 00:04:01.920
Let's go ahead and just show you how you can print out the first five rows.

61
00:04:01.950 --> 00:04:06.710
So if you want to print out the first five rows of a data frame you can use the head method for that.

62
00:04:06.710 --> 00:04:09.610
And then you just passen how many rows you want to print.

63
00:04:09.620 --> 00:04:16.030
So in this case we'll say DFA head kilts go ahead and open up a terminal to actually run this simple

64
00:04:16.030 --> 00:04:20.590
script in a click right here plus and then what I'm going to do since my fonts a little larger than

65
00:04:20.590 --> 00:04:25.030
probably what you're using to actually see the full displays in them to grab this and try to expand

66
00:04:25.030 --> 00:04:27.060
the terminal as large as I can.

67
00:04:27.370 --> 00:04:32.170
And under-sea Sparke I have my programs etc. The sparked data frame's folder and I'm going to go ahead

68
00:04:32.170 --> 00:04:40.030
and say Sparke bash shell enter and that way we get the interactive Skoll environment that we can just

69
00:04:40.030 --> 00:04:41.830
call simple scripts from.

70
00:04:41.830 --> 00:04:47.260
So again we're going to say Spark's session build or create that session and read this that see a c

71
00:04:47.260 --> 00:04:48.030
file.

72
00:04:48.070 --> 00:04:56.720
So let's go ahead and say colon load and say DFT DFT scholar and then enter is going to go and import

73
00:04:56.720 --> 00:04:59.790
Sparke session and then you may see some warnings here.

74
00:04:59.810 --> 00:05:03.570
But don't worry about that depending on whether or not you turned off warnings.

75
00:05:03.650 --> 00:05:07.470
You'll see him or not see him but let's go in and see the results here.

76
00:05:07.490 --> 00:05:14.330
So here we can see that we have a data frame from that the variable value there and it say this is the

77
00:05:14.330 --> 00:05:20.020
data type it is or that Apache that sparked that sequel that data frame and then it has.

78
00:05:20.030 --> 00:05:26.140
You'll notice underscore 0 string underscore C-One street etc. and for more fields.

79
00:05:26.150 --> 00:05:31.590
This is basically Spark's most default way of telling you this is the zero column.

80
00:05:31.610 --> 00:05:38.140
This is the one column and the data type that I expect in the 0 column are strings by default Sparke

81
00:05:38.210 --> 00:05:41.970
data is going to teach every treat everything as a string.

82
00:05:42.030 --> 00:05:48.140
You'll notice here when we say DFA ahead we actually get to see in array output of a single row or a

83
00:05:48.140 --> 00:05:54.050
single rows I should say depending on what value you put in here for head here I can see that the array

84
00:05:54.050 --> 00:05:55.650
contains this information.

85
00:05:55.700 --> 00:06:01.220
It's a date in open price a high price a low price for that day the closing price and then the volume

86
00:06:01.490 --> 00:06:03.530
of stock traded for that day.

87
00:06:03.620 --> 00:06:08.420
And right now this looks a little strange as far as the actual formatting.

88
00:06:08.420 --> 00:06:11.800
So let's go ahead and try to print each of these lines.

89
00:06:12.090 --> 00:06:15.150
So we're going to go ahead and write a little bit of a simple script here.

90
00:06:15.290 --> 00:06:18.340
I'll say for wine.

91
00:06:18.780 --> 00:06:29.610
And then Simon operator DFI head five I'm going to go ahead and then say Prince L.N. and then print

92
00:06:29.610 --> 00:06:30.420
line.

93
00:06:30.420 --> 00:06:33.600
Actually a better term for this sort of line will go in and say Roche.

94
00:06:33.600 --> 00:06:35.270
So it's a little more clear.

95
00:06:35.410 --> 00:06:38.370
It says line in your notes but Rose maybe a little more clear than that.

96
00:06:38.370 --> 00:06:42.900
And let's go ahead and not just showed the head but actually print out every row in that that should

97
00:06:42.900 --> 00:06:45.480
actually format this a little nicer.

98
00:06:45.570 --> 00:06:50.220
So if we say load that the Skala here this is a nice printing format.

99
00:06:50.490 --> 00:06:52.460
Every row in that array objects.

100
00:06:52.470 --> 00:06:58.770
They open high low close volume and we can see that we have the dates and then some high price low price

101
00:06:58.770 --> 00:07:01.260
et cetera and the volume of stock trading.

102
00:07:01.560 --> 00:07:07.500
Now we have an issue here and that's that spark data frame and spark itself thinks that all of these

103
00:07:07.500 --> 00:07:15.240
are strings and if one thinks that this date here is a string itself spark can actually infer the schema

104
00:07:15.480 --> 00:07:20.190
based off of the objects or elements inside of their data source.

105
00:07:20.190 --> 00:07:24.740
Let's go ahead and show you how you can do that and set options when you're reading in data sources.

106
00:07:24.760 --> 00:07:30.390
We're going to have a lot more this when we actually go over various data inputs and outputs in a lecture

107
00:07:30.390 --> 00:07:30.850
later on.

108
00:07:30.850 --> 00:07:33.300
But right now we're dealing with simple CXXVI files.

109
00:07:33.390 --> 00:07:38.730
So it's going to show you how you can add options to spark that read which you do say spark that read

110
00:07:38.970 --> 00:07:45.870
and then clarify option and then you pass in two arguments here you pass the option the actual name

111
00:07:45.870 --> 00:07:51.210
of that option and then you say comma and whether you want a certain number there or you want a boolean

112
00:07:51.210 --> 00:07:56.390
value whatever happens to go if that option you pass it in and you pass these both in as strings suddenly

113
00:07:56.390 --> 00:07:59.850
pass in for the header option comma.

114
00:07:59.900 --> 00:08:06.360
I'm going to say true and what that is going to do is actually treat this very first row here as the

115
00:08:06.360 --> 00:08:06.800
header.

116
00:08:06.840 --> 00:08:13.320
So instead of actually saying underscore zero for the first column and then see one for the second column.

117
00:08:13.320 --> 00:08:17.880
It's actually going to say oh the first row is the actual column name.

118
00:08:17.940 --> 00:08:22.580
So instead of this default it should be dates and instead of this column it should be open.

119
00:08:22.590 --> 00:08:25.490
So let's go ahead and make sure you have a dot there.

120
00:08:26.460 --> 00:08:30.590
We're going to go ahead and run that and see if that makes a difference for us.

121
00:08:30.600 --> 00:08:30.830
OK.

122
00:08:30.830 --> 00:08:31.530
And it really does.

123
00:08:31.530 --> 00:08:37.890
You can see now that the first row is just pure information and that Hetter has actually been made the

124
00:08:37.890 --> 00:08:38.480
column name.

125
00:08:38.490 --> 00:08:41.890
So you can see dates is a string open as a string etc..

126
00:08:41.940 --> 00:08:45.210
Now open is not actually a string it's a double.

127
00:08:45.210 --> 00:08:48.120
It looks like it's 490 point zero on that very first day.

128
00:08:48.120 --> 00:08:54.460
Let's go ahead and add a new option in so we can say option dot and then we're going to go ahead and

129
00:08:54.460 --> 00:09:00.520
say this option is called in first schema and you want this to be true.

130
00:09:03.510 --> 00:09:07.330
Now I'm going to say that the letter is true and in first Kema is true and I'm going to look to make

131
00:09:07.330 --> 00:09:09.510
sure that these sort of things change.

132
00:09:09.530 --> 00:09:14.470
Let's go in and say control s to save that up on your arrow key in the terminal and then go ahead and

133
00:09:14.470 --> 00:09:16.310
load that simple script again.

134
00:09:16.390 --> 00:09:19.690
And if I expand this a little bit just to see the results I can see here.

135
00:09:19.690 --> 00:09:25.550
Now the date is type time stamp open as a double acceptor and have four more fields.

136
00:09:25.750 --> 00:09:30.770
So these two options are going to be really useful when you're working with and reading CACP data.

137
00:09:30.850 --> 00:09:34.980
Now as a quick no go ahead and check out the data from overview lecture.

138
00:09:35.030 --> 00:09:36.350
We go ahead and scroll up here.

139
00:09:36.440 --> 00:09:41.890
There's a link to the API notes and you can go ahead and visit all the various options that you can

140
00:09:41.890 --> 00:09:43.660
say when you're reading in data.

141
00:09:43.690 --> 00:09:47.750
But we'll go over those in much more detail when we actually talk about data input output.

142
00:09:48.030 --> 00:09:48.360
OK.

143
00:09:48.430 --> 00:09:54.580
So we learn the very basics that actually read in a CSP file and display some of the information off

144
00:09:54.580 --> 00:09:55.210
of it.

145
00:09:55.240 --> 00:09:57.220
Let me show you just a few more useful methods.

146
00:09:57.230 --> 00:10:00.710
You're going to want to know when working of a data frame.

147
00:10:00.810 --> 00:10:01.970
We talked about head.

148
00:10:01.980 --> 00:10:06.510
Now we actually showed you how you could print every row as far as wanting to see it with something

149
00:10:06.600 --> 00:10:08.150
you actually want to get the column names.

150
00:10:08.160 --> 00:10:10.520
For instance here I see that I have four more fields.

151
00:10:10.680 --> 00:10:13.390
Let's actually explore what those fields are.

152
00:10:13.440 --> 00:10:22.640
So I'm going to call DMF dot columns and if I save it and run that I can go ahead and see that my columns.

153
00:10:22.680 --> 00:10:23.400
Is this array.

154
00:10:23.400 --> 00:10:25.210
So it's an array of data.

155
00:10:25.230 --> 00:10:29.070
Open High Low close and volume and these are all string names.

156
00:10:29.100 --> 00:10:35.100
So now that I have these columns it can actually use those and select particular columns from the data

157
00:10:35.100 --> 00:10:35.790
frame.

158
00:10:35.790 --> 00:10:36.270
All right.

159
00:10:36.270 --> 00:10:41.130
Now let's go ahead and walk through just a few more useful operations he can do on a data frame.

160
00:10:41.250 --> 00:10:46.440
If you already worked with data frames in a language such as are or with Python and pandas you're probably

161
00:10:46.440 --> 00:10:51.870
familiar with the describe operation or method on a data frame which shows you some basic statistical

162
00:10:51.870 --> 00:10:57.780
information for numerical columns and you can just do that with describe off your data frame and then

163
00:10:57.780 --> 00:10:58.950
to actually see the results.

164
00:10:58.950 --> 00:11:01.430
We're going to go ahead and say show.

165
00:11:01.620 --> 00:11:07.650
So let me clear this console with control and then I'm going to go ahead and save this script and run

166
00:11:07.650 --> 00:11:09.120
it.

167
00:11:09.130 --> 00:11:12.030
So now you should see actually pace of this.

168
00:11:12.040 --> 00:11:15.500
So keep in mind this DFA is actually stored in memory here in the Skala.

169
00:11:15.680 --> 00:11:21.140
Let me actually run the scripts loads of Skala then I can see everything.

170
00:11:21.180 --> 00:11:21.730
All right.

171
00:11:21.990 --> 00:11:26.940
So my Faunce a little large if you wanted to you could print row by row like we'd done earlier for this

172
00:11:26.940 --> 00:11:33.900
data frame but here you can see that there's a summary it goes goes ahead and gives you a count a mean

173
00:11:33.900 --> 00:11:37.850
a standard deviation and min and max of all numerical columns.

174
00:11:37.860 --> 00:11:40.500
So here I can see as far as the counts.

175
00:11:40.590 --> 00:11:46.940
They should all be the same there's 750 five rows in this data frame so I can see that fifty 755 open

176
00:11:46.950 --> 00:11:48.190
high etc..

177
00:11:48.300 --> 00:11:52.060
The mean of the open the high of the open standard deviation min max.

178
00:11:52.080 --> 00:11:55.230
So that's a really useful method if you didn't fill out the numerical data

179
00:11:58.040 --> 00:12:03.260
now as you just saw when I accidentally wrote a line appear or this line right here and so the consul

180
00:12:04.120 --> 00:12:14.870
DMF I can actually say F dot and say describe not show directly into this terminal and it'll still work.

181
00:12:14.880 --> 00:12:17.840
So we're going to go ahead and do now sort of constantly editing the script.

182
00:12:17.870 --> 00:12:23.130
Just play around the data frame here directly and score since it's actually saved in memory as an object

183
00:12:23.670 --> 00:12:27.230
going to show you how you can actually select a column.

184
00:12:27.450 --> 00:12:32.720
So to select the column you actually just say Deia dot selects.

185
00:12:32.740 --> 00:12:34.240
Then you pass in the column name.

186
00:12:34.240 --> 00:12:36.590
For instance let's say I wanted volume.

187
00:12:36.910 --> 00:12:38.130
I'll go ahead and pass that.

188
00:12:38.170 --> 00:12:42.470
And if you're looking at the text editor you'll get helpful syntax highlighting here.

189
00:12:42.520 --> 00:12:45.200
So if I go go ahead and type this here just to show what that would look like.

190
00:12:45.490 --> 00:12:51.700
I can say selects and then a volume so that's a string but we're not going to do this straight into

191
00:12:51.700 --> 00:12:54.490
the terminal just to kind of play around that a little more.

192
00:12:54.500 --> 00:12:59.060
The more hands on it will say show to actually show that result.

193
00:12:59.060 --> 00:13:02.590
And if you have a lot of or a really large data frame which you will because you're working of spark

194
00:13:02.590 --> 00:13:07.690
because you have large data it's only going to show the top 20 rows if you order something with a lot

195
00:13:07.690 --> 00:13:08.350
of rows back.

196
00:13:08.350 --> 00:13:14.290
So here we can actually select the single column FTF that select pass in the column names a string and

197
00:13:14.290 --> 00:13:18.590
say that show if you want to select multiple columns that's easy too.

198
00:13:18.760 --> 00:13:22.480
You can just say DMF dot select.

199
00:13:22.510 --> 00:13:26.980
And then here you're going to use that sort of dollar sign notation on the string class and the string

200
00:13:26.980 --> 00:13:27.320
name.

201
00:13:27.310 --> 00:13:33.160
So you want it to select the dates comma and then you're going to say dollar sign whatever other column

202
00:13:33.160 --> 00:13:36.950
names you want for instance close comma.

203
00:13:37.040 --> 00:13:38.420
In other column name if you want it.

204
00:13:38.430 --> 00:13:45.270
So if I just want to select these two columns I can say that show and it's going to show me only the

205
00:13:45.270 --> 00:13:46.920
top 20 rows of this.

206
00:13:46.950 --> 00:13:51.920
Here's the date column and here's the close and know how the date now since it's a time stamp since

207
00:13:51.930 --> 00:13:55.290
I said in first schema actually has that full time statement affirmation.

208
00:13:55.290 --> 00:13:59.590
Now we actually have a whole lecture on how to deal time stamp information of data frames.

209
00:13:59.730 --> 00:14:03.270
So if you're interested in that sort of thing just keep that in mind as we continue on through this

210
00:14:03.270 --> 00:14:04.470
section of the course.

211
00:14:04.470 --> 00:14:08.550
But that's how you can select multiple columns you just need to add in the dollar sign.

212
00:14:08.550 --> 00:14:12.800
Now let's go ahead and show you how you can create new columns in is it do this.

213
00:14:12.810 --> 00:14:15.810
I'm actually going to now come back to this script.

214
00:14:15.900 --> 00:14:24.000
So to create new columns used to command decaff dots and then you say with column so you say with column

215
00:14:25.360 --> 00:14:29.980
in the first three passen as a string is the name of the new column you want.

216
00:14:30.350 --> 00:14:39.770
So in this case let's say I want a high plus low to be them I call them and then I'm going to go ahead

217
00:14:39.830 --> 00:14:41.920
and pass and the column names I want.

218
00:14:42.080 --> 00:14:50.810
So I will say DMF of high plus DMF of low

219
00:14:53.640 --> 00:14:59.120
and this is just a quick way to actually create a new column based off of old columns so you put in

220
00:14:59.320 --> 00:15:06.120
DFA that with Kollin the name of the new column you wants and then based off some sort of formula.

221
00:15:06.380 --> 00:15:12.830
The old columns and then whatever operations you want to do you can actually call columns singular columns

222
00:15:12.890 --> 00:15:14.130
the same way here.

223
00:15:14.180 --> 00:15:18.770
So I know I showed you select this the more formal method of doing this but if I scroll back down here

224
00:15:18.950 --> 00:15:26.590
I could also say DMF hi and that will return back that actual column objects.

225
00:15:26.590 --> 00:15:29.080
So it doesn't actually show the column itself.

226
00:15:29.230 --> 00:15:32.790
It actually grabs and saves as a result that actual column there.

227
00:15:32.860 --> 00:15:34.410
And it says here's a column.

228
00:15:34.420 --> 00:15:36.380
It's the high column from the high.

229
00:15:36.610 --> 00:15:42.220
So then you can perform operations of those columns and then say DFA with column and it'll show that.

230
00:15:42.220 --> 00:15:47.710
Now usually when you're doing this you're going to kind of say this is a new data frame so say thou

231
00:15:47.710 --> 00:15:53.950
DFI to DFA with column have a new column here it's the high price plus the low price.

232
00:15:53.950 --> 00:15:58.230
Now financially speaking that doesn't really make any sense as far as what this operation represents.

233
00:15:58.390 --> 00:16:02.290
But go ahead it kind of play along with this just so you can learn how to use with column.

234
00:16:02.440 --> 00:16:08.890
And then I'm going to go ahead and say the two and I'm going to print the schema of this just so we

235
00:16:08.890 --> 00:16:10.450
can see that new column.

236
00:16:10.450 --> 00:16:13.250
So let's go ahead and save that.

237
00:16:13.440 --> 00:16:19.620
And then at points you clear this and actually lower DFT Scala run this.

238
00:16:19.660 --> 00:16:23.100
And here we can see that we have a new column at typeless low.

239
00:16:23.110 --> 00:16:26.170
It knows that it's a double since these two are doubles right here.

240
00:16:26.320 --> 00:16:30.400
And it went ahead and created a new column and again you do that with the dot with column and we'll

241
00:16:30.400 --> 00:16:33.350
see some more examples of that later on this section of the course.

242
00:16:35.030 --> 00:16:38.200
And the very last thing I want to show you is how to rename it column.

243
00:16:38.210 --> 00:16:45.500
So if you want to rename a column you can just say the as method so DMF to in this case let's go ahead

244
00:16:45.500 --> 00:16:53.270
and say I plus low and you can say Dot as HPL.

245
00:16:53.280 --> 00:16:59.460
So instead of just saying hi plus say HBL for high plus low this stop as is probably familiar to you

246
00:16:59.460 --> 00:17:00.620
if you use sequel.

247
00:17:00.630 --> 00:17:02.880
It's kind of like an alias for this column.

248
00:17:02.880 --> 00:17:09.160
And we'll go ahead and say then we can actually reference this using a select method so I can say something

249
00:17:09.160 --> 00:17:20.530
like if two selects select the high plus slow column as HPL and then go ahead and show this.

250
00:17:20.610 --> 00:17:26.680
So it's make sure that all worked out well run our script here and here I can see the top 20 rows of

251
00:17:26.680 --> 00:17:30.460
this new HPL column which is just renaming the high plus low.

252
00:17:30.460 --> 00:17:32.560
So it's just an alias for this column.

253
00:17:32.560 --> 00:17:38.730
If I wanted to I couldn't put in a comma here and say DFA to close.

254
00:17:38.770 --> 00:17:42.610
And this is kind of showing you another way to select multiple columns.

255
00:17:42.610 --> 00:17:49.480
You can either use that dollar sign notation or use the F 2 or D F and then the name of the column itself.

256
00:17:49.480 --> 00:17:53.880
So if you're going to run this I can see that I'll get these two columns back.

257
00:17:53.890 --> 00:17:54.590
All right.

258
00:17:54.700 --> 00:17:56.040
That's all there is for now.

259
00:17:56.050 --> 00:17:59.150
We're going to learn a lot more about basic functions and operations.

260
00:17:59.140 --> 00:18:01.780
The very next lecture feel free to post any questions.

261
00:18:01.780 --> 00:18:06.790
The Kewney forums and make sure you actually review this data frame underscore overview Skala script

262
00:18:07.120 --> 00:18:09.730
before moving on to the next lecture.

263
00:18:09.730 --> 00:18:11.980
Thanks everyone and I'll see you at the next lecture.
