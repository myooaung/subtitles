WEBVTT
1
00:00:05.420 --> 00:00:10.900
Hello everyone and welcome to the classification section of the machine learning part of this course.

2
00:00:11.610 --> 00:00:16.500
Our next machine learning task to perform with Sparke is classification or we are actually trying to

3
00:00:16.500 --> 00:00:20.370
predict a categorical value instead of a continuous value.

4
00:00:20.460 --> 00:00:26.640
For instance trying to predict whether or not a tumor is benign or malignant will also begin to show

5
00:00:26.640 --> 00:00:31.590
you how to work with categorical data using spark a lot of times you're not going to have continuous

6
00:00:31.590 --> 00:00:34.520
numerical values for a feature column.

7
00:00:34.550 --> 00:00:39.990
Instead you may have a feature column of actual categories as values we will show you in this section

8
00:00:39.990 --> 00:00:45.450
of the course how to actually deal with those values that may just present themselves sometimes as strings

9
00:00:45.840 --> 00:00:52.170
Sparke actually has a lot of tools to help you encode those categorical values into actual encodings

10
00:00:52.350 --> 00:00:57.370
that a machine learning algorithm can understand our classification algorithm for this section.

11
00:00:57.390 --> 00:00:59.960
The course is going to be logistic regression.

12
00:01:00.120 --> 00:01:01.480
Now don't let the name fool you.

13
00:01:01.500 --> 00:01:03.670
It's not really a regression algorithm.

14
00:01:03.780 --> 00:01:10.800
It's using a specific type of logistic regression curve in order to perform a classification.

15
00:01:10.800 --> 00:01:15.540
Let's go ahead and discuss logistic regression in more detail and give you a little bit of background

16
00:01:15.720 --> 00:01:18.080
on the theory of logistic regression.

17
00:01:18.300 --> 00:01:21.970
If you want to understand the full mathematics behind logistic regression.

18
00:01:22.170 --> 00:01:27.500
Go ahead and read sections 4 through 4.3 of introduction to statistical learning curve.

19
00:01:27.510 --> 00:01:35.360
James moving along we want to learn about logistic regression as a method for classification in machine

20
00:01:35.360 --> 00:01:36.470
learning and statistics.

21
00:01:36.470 --> 00:01:41.930
Classification is the problem of identifying to which of a set of categories.

22
00:01:41.960 --> 00:01:46.550
A new observation belongs to based off of training data.

23
00:01:46.550 --> 00:01:53.690
Some examples of classification problems include trying to detect spam versus ham emails trying to detect

24
00:01:53.690 --> 00:01:59.740
whether someone's going to default on their loan or not or even trying to diagnose a disease for example

25
00:01:59.750 --> 00:02:03.320
trying to tell if someone has cancer or not.

26
00:02:03.320 --> 00:02:11.100
These above examples are all examples of a binary classification meaning we have two classes so far

27
00:02:11.130 --> 00:02:16.320
in our studies we've only seen regression problems where we try to predict a continuous value such as

28
00:02:16.320 --> 00:02:20.370
the price of a house although the name may be confusing at first.

29
00:02:20.370 --> 00:02:26.040
Logistic Regression allows us to solve classification problems where we're trying to predict discrete

30
00:02:26.130 --> 00:02:27.270
categories.

31
00:02:27.330 --> 00:02:34.970
The convention for binary classification is to have two classes zero and one.

32
00:02:35.000 --> 00:02:39.940
However we can't use a normal linear regression model on a binary group.

33
00:02:39.950 --> 00:02:42.690
It won't lead to a good fit.

34
00:02:42.790 --> 00:02:48.550
For example if we take a look at this plot below we have a Y axis which represents the probability of

35
00:02:48.550 --> 00:02:50.740
belonging to a particular group.

36
00:02:50.740 --> 00:02:56.920
Let's go ahead and imagine that this example plot is trying to predict likelihood of paying back a loan.

37
00:02:56.920 --> 00:03:02.680
We'll go ahead and label 0 percent probability as defaulting on their loan meaning they have a zero

38
00:03:02.680 --> 00:03:08.800
percent probability of being able to pay back their loan and at the top we have one or a 100 percent

39
00:03:08.830 --> 00:03:10.840
probability is fully paying back.

40
00:03:10.840 --> 00:03:16.520
That alone will go ahead and mark the X axis as some sort of paycheck value.

41
00:03:16.660 --> 00:03:22.750
That means if we go ahead and look at the data as your paycheck goes lower you have a closer to zero

42
00:03:22.750 --> 00:03:28.170
percent probability that you're going to be able to pay back your loan as your paycheck value gets higher.

43
00:03:28.180 --> 00:03:32.290
You don't have close to 100 percent probability of paying back your loan.

44
00:03:32.290 --> 00:03:37.930
The reason these yellow dashes are all either on 0 percent or 100 percent is because this is training

45
00:03:37.930 --> 00:03:39.470
data.

46
00:03:39.490 --> 00:03:44.830
Now if this was trading data and we try to use a linear regression model on it we would get a very bad

47
00:03:44.830 --> 00:03:45.360
fit.

48
00:03:45.370 --> 00:03:50.510
We would actually end up predicting probabilities below zero percent which doesn't really make any sense.

49
00:03:52.070 --> 00:03:58.430
Instead we can transform our linear regression to logistic regression curve and you'll notice our logistic

50
00:03:58.430 --> 00:04:05.520
regression curve can only go between 0 and 1 and that's going to be the key to understanding classification.

51
00:04:05.540 --> 00:04:09.240
Using a logistic regression curve the sigmoid.

52
00:04:09.260 --> 00:04:14.780
Also known as logistic function is going to be the key to understanding the using logistic regression

53
00:04:14.780 --> 00:04:17.410
to perform a classification.

54
00:04:17.450 --> 00:04:23.480
The key secret to this function is that it can take in any value and its output is going to be between

55
00:04:23.600 --> 00:04:25.730
0 and 1.

56
00:04:25.770 --> 00:04:31.970
If we take a look at the equation here on this plot we have the sigmoid function plotted out on the

57
00:04:31.970 --> 00:04:35.480
z axis is going to be the bottom line.

58
00:04:35.480 --> 00:04:36.730
Usually the x x.

59
00:04:36.790 --> 00:04:41.970
They're here without noting it as theta of Z and the formula is theta of Z.

60
00:04:41.990 --> 00:04:49.080
So the function of z is equal to 1 over 1 plus E to the power of negative Z.

61
00:04:49.130 --> 00:04:56.030
The key thing to notice here is that it doesn't matter what value of Z you put in to the logistic function

62
00:04:56.030 --> 00:04:57.580
or the sigmoid function.

63
00:04:57.620 --> 00:05:01.140
You'll always get a value between 0 and 1.

64
00:05:01.190 --> 00:05:06.440
So again if you take a look at this plot it doesn't matter that whatever value you put in for Z the

65
00:05:06.530 --> 00:05:12.520
output along the vertical axis is always going to be between 0 and 1 and that's the key.

66
00:05:12.530 --> 00:05:19.420
The sigmoid function This means that we can take a linear regression solution and place it into this

67
00:05:19.420 --> 00:05:23.010
sigmoid function and that's going to look like this.

68
00:05:23.020 --> 00:05:28.410
Remember our linear model followed a basic y equals x plus B principle.

69
00:05:28.450 --> 00:05:34.800
Here we have a linear model as y equals beta plus beta 1 times X..

70
00:05:34.990 --> 00:05:42.010
If we take that linear model and put it into the sigmoid function we finally are able to transform this

71
00:05:42.070 --> 00:05:48.820
linear regression to a logistic model meaning it doesn't matter whatever the value of the linear model

72
00:05:48.880 --> 00:05:50.550
output actually is.

73
00:05:50.590 --> 00:05:58.160
It's always going to be between 0 and 1 when we place it into the logistic model or the sigmoid function.

74
00:05:58.210 --> 00:06:04.150
Again if you want more of you on this mathematics make sure to read sections 4 through 4.3 of introduction

75
00:06:04.350 --> 00:06:06.620
to Stichel learning.

76
00:06:06.790 --> 00:06:12.430
But the basic premise of all of this is that this results in a probability from 0 to 1 of belonging

77
00:06:12.450 --> 00:06:14.200
in the one class.

78
00:06:14.410 --> 00:06:19.660
Again doesn't matter what we put in on this horizontal access on the vertical axis will always get some

79
00:06:19.660 --> 00:06:22.170
sort of probability between 0 and 1.

80
00:06:23.160 --> 00:06:32.190
That means we can set a cutoff point usually at 0.5 and we'll say if anything below results in 0.5 or

81
00:06:32.190 --> 00:06:35.710
below 0.5 that will go to class 0.

82
00:06:35.730 --> 00:06:38.500
Anything above belongs to class 1.

83
00:06:38.730 --> 00:06:44.910
So we're going to transform that 0.5 probability as a cutoff point.

84
00:06:44.920 --> 00:06:49.560
Let's go ahead and do a quick recap overview of what we just discussed.

85
00:06:49.600 --> 00:06:55.030
We can use the logistic function to output a value ranging from 0 to 1.

86
00:06:55.030 --> 00:07:01.720
Again it doesn't matter what we put along the horizontal axis we get a value from 0 to 1 based off of

87
00:07:01.720 --> 00:07:09.640
this probability we will assign a class by putting a cutoff point 0.5 which makes sense because we want

88
00:07:09.640 --> 00:07:16.360
to say if the probability is 50 percent or less of belonging to class 1 then we should classify this

89
00:07:16.420 --> 00:07:19.870
as Class 0 in our binary classification.

90
00:07:20.230 --> 00:07:26.440
If we have a probability of 0.5 or above of belonging to class 1 we'll go ahead and assign this new

91
00:07:26.440 --> 00:07:29.480
point to class 1.

92
00:07:29.480 --> 00:07:30.200
All right.

93
00:07:30.200 --> 00:07:36.920
So let's go ahead and talk about model evaluation and using it confusion matrix after you train a logistic

94
00:07:36.920 --> 00:07:40.070
regression model to classify some training data.

95
00:07:40.070 --> 00:07:44.150
You're going to want to evaluate your model's performance on some test data.

96
00:07:44.390 --> 00:07:52.080
You can use the confusion matrix to evaluate classification models a confusion matrix is a table that

97
00:07:52.080 --> 00:07:57.360
is often used to describe the performance of a classification model on a set of test data for which

98
00:07:57.360 --> 00:07:59.910
the true values are already known.

99
00:07:59.910 --> 00:08:05.700
The confusion matrix itself is actually relatively simple to understand but sometimes the related terminology

100
00:08:05.700 --> 00:08:07.200
can be confusing.

101
00:08:07.200 --> 00:08:09.760
Let's go ahead and walk through this example.

102
00:08:09.810 --> 00:08:13.540
In this case we have a binary classification problem.

103
00:08:13.600 --> 00:08:19.630
So in this example we're testing for the presence of a disease where no is a negative test which is

104
00:08:19.630 --> 00:08:20.140
false.

105
00:08:20.230 --> 00:08:21.440
Equals zero.

106
00:08:21.730 --> 00:08:26.760
Yes as a positive test where true is equal to one.

107
00:08:26.830 --> 00:08:28.480
What can we learn from this matrix.

108
00:08:28.510 --> 00:08:30.820
Well there are two possible predicted classes.

109
00:08:30.880 --> 00:08:31.810
Yes and no.

110
00:08:32.080 --> 00:08:36.990
If we were predicting the presence of a disease for example yes it would mean that they have the disease.

111
00:08:37.000 --> 00:08:39.100
No it would mean that they don't have disease.

112
00:08:39.460 --> 00:08:46.150
The classifier made a total of a hundred and sixty five predictions meaning 165 patients were tested

113
00:08:46.150 --> 00:08:48.070
for the presence of the disease.

114
00:08:48.130 --> 00:08:54.210
Out of those 165 cases the classifier predicted Yes 110 times and no.

115
00:08:54.210 --> 00:08:55.640
Fifty five times.

116
00:08:55.780 --> 00:08:59.310
In reality meaning we already have label test data.

117
00:08:59.310 --> 00:09:04.480
A hundred and five patients in the sample have the disease and 60 patients do not.

118
00:09:04.480 --> 00:09:06.990
Now let's go ahead and define the most basic terms.

119
00:09:07.830 --> 00:09:14.910
The basic terms are the whole number terms so not rates just hold numbers and those terms are true positives

120
00:09:15.180 --> 00:09:18.690
true negatives false positives and false negatives.

121
00:09:18.690 --> 00:09:20.810
You may already be familiar of this terminology.

122
00:09:20.910 --> 00:09:27.780
If you've ever had a deal of studies related to vaccine checks or disease checks a true positive.

123
00:09:27.900 --> 00:09:31.550
Are the cases in which we predicted yes that they have disease.

124
00:09:31.770 --> 00:09:36.660
And in reality they do have a disease that's T.P. true positive.

125
00:09:36.930 --> 00:09:43.140
In this case you can find that here at the bottom quadrant where T-P is equal to 100 true negatives

126
00:09:43.500 --> 00:09:46.800
are where we predicted and know that they don't have the disease.

127
00:09:47.040 --> 00:09:53.100
And in reality again they don't actually have Zeese that's on the upper left hand corner T.N..

128
00:09:53.120 --> 00:09:54.480
And that's equal to 50.

129
00:09:54.690 --> 00:09:57.470
So those are true positives and true negatives.

130
00:09:57.700 --> 00:10:03.380
They don't we have false positives and false negatives false positives are where we predicted Yes.

131
00:10:03.600 --> 00:10:04.890
That they have Zeese.

132
00:10:04.980 --> 00:10:08.040
In reality they don't actually have the disease.

133
00:10:08.040 --> 00:10:14.810
This is also known as a type 1 error then we have false negatives where we predicted.

134
00:10:14.850 --> 00:10:17.030
No they do not have the disease.

135
00:10:17.160 --> 00:10:25.730
But in reality they actually did have a disease that's not as a type 2 error.

136
00:10:25.810 --> 00:10:28.050
Then we can talk about rates.

137
00:10:28.330 --> 00:10:31.770
The first rate we can discuss is accuracy.

138
00:10:31.820 --> 00:10:35.890
What this is actually getting at is overall how often is it correct.

139
00:10:36.020 --> 00:10:41.450
A lot of times when you hear reports on studies they'll just tell you the accuracy and the accuracy

140
00:10:41.450 --> 00:10:47.070
is calculated by the number of true positives plus the number of true negatives over the total.

141
00:10:47.150 --> 00:10:52.470
In this case our model is 91 percent accurate.

142
00:10:52.490 --> 00:10:56.480
Then we have the misclassification rate which is answering the question.

143
00:10:56.480 --> 00:10:59.970
Overall how often is the model wrong.

144
00:11:00.050 --> 00:11:05.270
This is going to be calculated by the number of false positives plus a number of false negatives divided

145
00:11:05.270 --> 00:11:06.650
by that total.

146
00:11:06.680 --> 00:11:10.430
So that's 15 divided by 165 overall.

147
00:11:10.700 --> 00:11:14.660
This is 9 percent error rate or misclassification rate.

148
00:11:14.660 --> 00:11:20.900
Now let me discuss a nice quick way to remember false positives vs. false negatives are your type 1

149
00:11:20.900 --> 00:11:27.200
error versus your type 2 error remember are false positives that I'm pointing out here of the laser

150
00:11:27.390 --> 00:11:32.490
P is equal to 10 that's where we predicted Yes but they didn't actually have the disease.

151
00:11:32.570 --> 00:11:36.970
That's Type 1 error false negatives as we predicted.

152
00:11:36.970 --> 00:11:41.920
No but they actually do have this disease known as type 2 error.

153
00:11:41.980 --> 00:11:44.600
Go ahead and move along.

154
00:11:44.670 --> 00:11:51.270
We can think of type 1 error and type 2 errors as this funny little diagram as a nice way to remember

155
00:11:51.270 --> 00:11:51.940
it.

156
00:11:51.960 --> 00:11:55.880
So a type 1 error is where you're telling a man they're pregnant again.

157
00:11:55.890 --> 00:12:01.530
You're predicting Yes they're pregnant but they actually don't have the pregnancy type 2 error is a

158
00:12:01.530 --> 00:12:02.530
false negative.

159
00:12:02.670 --> 00:12:05.820
Here you're saying someone that's obviously pregnant that they're not pregnant.

160
00:12:05.970 --> 00:12:06.660
So you're predicting.

161
00:12:06.660 --> 00:12:12.870
No but they actually are pregnant and that's a difference at Type 1 air type to air false positives

162
00:12:12.870 --> 00:12:14.320
vs. false negatives.

163
00:12:14.430 --> 00:12:19.590
In statistics they are commonly referred to as type 1 or Type 2 instead of their actual names false

164
00:12:19.590 --> 00:12:21.210
positive or false negative.

165
00:12:21.600 --> 00:12:26.160
Hopefully this is a nice helpful and funny reminder on how to actually memorize these terms.

166
00:12:26.160 --> 00:12:26.670
All right.

167
00:12:26.730 --> 00:12:32.190
Now we understand some of the background behind using logistic regression for classification tasks.

168
00:12:32.190 --> 00:12:37.140
Let's begin our understanding of classification general of Sparke by starting off of a documentation

169
00:12:37.140 --> 00:12:39.630
example and then building off of that.

170
00:12:39.660 --> 00:12:41.990
Thanks everyone and we'll see you at the next lecture.
