WEBVTT
1
00:00:05.560 --> 00:00:10.180
Hello everyone and welcome to the regression example lecture for the model evaluation section of the

2
00:00:10.180 --> 00:00:13.810
course now that we've walked through the documentation example.

3
00:00:13.840 --> 00:00:16.310
Let's take a look back at that housing data set.

4
00:00:16.360 --> 00:00:21.460
We originally worked with in the Russian section of the course and applied the train validation split

5
00:00:21.700 --> 00:00:24.870
and parameter grid builder objects that we've been working with.

6
00:00:25.050 --> 00:00:27.200
We're going to jump to a text editor now.

7
00:00:27.210 --> 00:00:27.550
All right.

8
00:00:27.550 --> 00:00:28.070
Here I am.

9
00:00:28.090 --> 00:00:34.030
Adam text editor and the a script that goes along with this lecture is under the model evaluation folder

10
00:00:34.150 --> 00:00:37.300
it's called Train test in grids that Skala.

11
00:00:37.450 --> 00:00:42.400
And basically as I mentioned that we're going to be doing is following through of Parambrata build or

12
00:00:42.400 --> 00:00:47.590
train validation split and running very simple examples so you can get an idea of how to work with real

13
00:00:47.590 --> 00:00:52.990
cxxviii data or whatever data format you're working with with this train validation split and parameter

14
00:00:52.990 --> 00:00:53.980
grid builder.

15
00:00:54.010 --> 00:00:58.480
And we're going try to keep things very simple and not actually setting any options and parameter grid

16
00:00:58.750 --> 00:01:01.650
so we can focus more on the train validation split.

17
00:01:01.780 --> 00:01:03.090
I'm going to explain that in more.

18
00:01:03.130 --> 00:01:09.130
So let's start a new script going to you right click on model evaluation start a new file and call this

19
00:01:09.170 --> 00:01:12.170
the next stop Skala enter.

20
00:01:12.210 --> 00:01:13.560
Now let's begin.

21
00:01:13.600 --> 00:01:18.760
First thing I want to do is actually a couple of imports I need the evaluator the model and then the

22
00:01:18.760 --> 00:01:21.700
parameter grid builder and the train invalidations split.

23
00:01:21.940 --> 00:01:32.530
So I can say something like this import or Apache that spark animal evaluation and then I want the regression

24
00:01:32.530 --> 00:01:37.990
evaluator because I'm running a regression algorithm Linaria session and then I need to import my models.

25
00:01:37.990 --> 00:01:46.830
That just comes again from that Apache that spark M-L regression and then I want to call linear regression.

26
00:01:47.020 --> 00:01:49.340
And then finally I want to say import.

27
00:01:49.410 --> 00:01:57.820
And in this case I want to say or Apache that sparked the M-L and I'm going to grab it from tuning in

28
00:01:57.820 --> 00:02:01.010
I want to grab multiple things like putting curly brackets here.

29
00:02:01.180 --> 00:02:06.550
I want to grab that parameter great builder and I also want to grab that train validation split which

30
00:02:06.550 --> 00:02:10.190
allows us to do that training test and hold up a set.

31
00:02:10.540 --> 00:02:20.350
And then as an option I can also say import or Apache the log for J underscore and then say capital

32
00:02:20.530 --> 00:02:34.660
logger logger type in Oregon as a string and then I set the level to be level dot and in all caps here

33
00:02:34.750 --> 00:02:40.670
and this is just to reduce the amount of output we see as far as logging when you run SPARC.

34
00:02:40.720 --> 00:02:46.150
Now the first thing I want to do is start a simple Sparke session so I'll add one more import as we've

35
00:02:46.150 --> 00:02:55.400
done before in the past or the Apache that spark that sequel spark session and then we say Val Sparke

36
00:02:56.840 --> 00:03:03.130
start a spark session called builder on it and then get or create.

37
00:03:03.370 --> 00:03:05.030
So we have our bases covered.

38
00:03:05.050 --> 00:03:09.450
Next thing we want to do is actually prepare the training and test data in order to do that.

39
00:03:09.460 --> 00:03:12.570
We need to read in the test data so read the data.

40
00:03:12.790 --> 00:03:14.250
We're working on Facey every file.

41
00:03:14.260 --> 00:03:22.870
So I will say that data is equal to spark read and we're working of CCV so I'm going to pass on two

42
00:03:22.870 --> 00:03:23.460
options.

43
00:03:23.460 --> 00:03:27.990
We've done in the past header and then true.

44
00:03:28.170 --> 00:03:39.160
And then I also want to say option in first schema and then true the form I'm working with is CXXVIII.

45
00:03:39.160 --> 00:03:41.230
So we'll type that in C S V.

46
00:03:41.230 --> 00:03:46.360
And then finally I actually want to load that file and I'm using the same file that we used in the regression

47
00:03:46.360 --> 00:03:47.540
section of the course.

48
00:03:47.590 --> 00:03:49.670
That's the USA housing file.

49
00:03:49.720 --> 00:03:52.470
So I will just say this in strings.

50
00:03:53.020 --> 00:03:56.420
Slash regression slash.

51
00:03:56.440 --> 00:04:00.140
And then USA underscore housing that CAC.

52
00:04:00.220 --> 00:04:06.190
And basically what this does is these two dots here tell it to look beyond its own folder so go outside

53
00:04:06.190 --> 00:04:10.830
model the valuation go to regression and then get that U.S. housing that we file.

54
00:04:10.870 --> 00:04:12.820
Now just to make sure everything worked well.

55
00:04:12.850 --> 00:04:21.540
To add one more line here that says Data print schema and it actually works with or without close parentheses.

56
00:04:21.540 --> 00:04:22.900
But let's load that in.

57
00:04:22.900 --> 00:04:25.590
I already have my terminal and actual load it up.

58
00:04:25.660 --> 00:04:30.580
So I will say load and the X-Type Skala enter.

59
00:04:30.580 --> 00:04:32.840
And we should at the end of this see the schema printed out.

60
00:04:32.860 --> 00:04:33.920
Let me expand this.

61
00:04:34.120 --> 00:04:34.900
And there it is.

62
00:04:35.030 --> 00:04:40.450
And we've already seen this data sets we have the average area income average area house age average

63
00:04:40.450 --> 00:04:44.220
number of rooms number of bedrooms area of population price and address.

64
00:04:44.240 --> 00:04:48.070
Remember we're trying to predict the price in a regression model and eventually we won't actually use

65
00:04:48.070 --> 00:04:50.290
that addresses it's just string data.

66
00:04:50.710 --> 00:04:54.700
So coming up next we want to make sure we set up the data frame for machine learning.

67
00:04:54.860 --> 00:04:59.250
And I know I mentioned this constantly but going to do some imports not at the top.

68
00:04:59.290 --> 00:05:01.380
So everything is kind of organized by idea.

69
00:05:01.600 --> 00:05:05.780
But in this case we need to do some imports for vector assembler and vector.

70
00:05:05.820 --> 00:05:15.410
So we'll say or that Apache SPARC Emel feature dot and then call vector assembler and then I will say

71
00:05:15.440 --> 00:05:27.670
import or Apache SPARC M-L and then Lynn alge vector's that I want to rename the label column.

72
00:05:27.670 --> 00:05:34.950
So remember that we have the price column we want to rename as label which means we say yvel set up

73
00:05:34.950 --> 00:05:44.920
our data frame data select and going to select that price column capital P price and I will set it as

74
00:05:46.310 --> 00:05:47.970
it will say label.

75
00:05:48.340 --> 00:05:51.290
And then I want to select the couple of columns here.

76
00:05:51.460 --> 00:05:58.150
Since I'm dealing with the spark shell I can easily just say data that columns enter and reports back

77
00:05:58.150 --> 00:05:58.810
the columns.

78
00:05:58.820 --> 00:06:02.050
This is just a convenience so I can actually grab the ones I want.

79
00:06:02.050 --> 00:06:07.990
In this case you don't to copy and paste those features one of the type of I'm out just reformat them

80
00:06:08.470 --> 00:06:10.130
and then I will paste them here.

81
00:06:11.370 --> 00:06:14.600
And then I want to say dollar sign quotes.

82
00:06:14.600 --> 00:06:19.230
They don't always need the dollar sign here but when you're dealing with Skala sometimes you get errors

83
00:06:19.230 --> 00:06:20.430
depending on your operating system.

84
00:06:20.430 --> 00:06:21.590
You don't have it there.

85
00:06:21.600 --> 00:06:24.540
So a lot of times it's better to include it just to be on the safe side.

86
00:06:25.760 --> 00:06:29.730
All right then let's put in another quote here.

87
00:06:29.890 --> 00:06:35.520
There are dollar sign quote there and this got cut off so we will bring this back down.

88
00:06:36.100 --> 00:06:42.220
And one more all the way to the right.

89
00:06:42.230 --> 00:06:43.140
All right.

90
00:06:43.140 --> 00:06:45.750
So let's make sure that we have that working.

91
00:06:45.870 --> 00:06:48.240
I'm going to save this and run this again.

92
00:06:48.410 --> 00:06:54.380
So I will load Ixtoc Skala make sure my princes are balanced and I didn't miss anything up to expand

93
00:06:54.380 --> 00:06:55.040
this.

94
00:06:55.040 --> 00:06:55.310
All right.

95
00:06:55.310 --> 00:06:56.500
Looks like we have a deal.

96
00:06:56.540 --> 00:06:57.610
Ready to go.

97
00:06:57.620 --> 00:07:01.170
And I can say the F print schema.

98
00:07:03.100 --> 00:07:07.490
In effect spand this I have my label and then all my features.

99
00:07:07.490 --> 00:07:12.800
And as always we need to put all these features into a single column that's an array called features.

100
00:07:12.800 --> 00:07:18.560
Next thing you need to do in order to actually do that step is to create the vector Sembler which means

101
00:07:18.560 --> 00:07:29.350
we'll type all assembler is equal to a new vector assembler object going to set the input columns as

102
00:07:29.350 --> 00:07:30.210
an array.

103
00:07:31.700 --> 00:07:36.190
In the input column just the ones we've been working with here these guys.

104
00:07:36.200 --> 00:07:44.330
So copy and paste that right click copy scroll to the left down here paste it don't need the dollar

105
00:07:44.330 --> 00:07:47.090
signs in this case since we just want strings.

106
00:07:47.090 --> 00:07:49.290
We'll remove those.

107
00:07:49.410 --> 00:07:50.820
And one more over here.

108
00:07:52.130 --> 00:07:52.580
All right.

109
00:07:52.670 --> 00:07:53.440
That's an array.

110
00:07:53.470 --> 00:07:55.790
And we want to also set the output column.

111
00:07:55.810 --> 00:08:05.680
So here we will say that set output call to us lower case features.

112
00:08:05.840 --> 00:08:06.840
That's our assembler.

113
00:08:06.860 --> 00:08:11.740
And the next thing I want to do is actually transform the data frame using assembler which means we'll

114
00:08:11.740 --> 00:08:28.380
type Vau output equal to assembler transform to frame say selects and then say label and then say features

115
00:08:32.630 --> 00:08:37.610
save this and I always like writing stuff especially if you're working on a small dataset like this

116
00:08:37.610 --> 00:08:41.950
one just to make sure everything's balanced out as far as print season we don't have any typos.

117
00:08:42.110 --> 00:08:43.120
If you don't get any errors.

118
00:08:43.130 --> 00:08:48.220
Looks like we're still good here we have her output is just that label and the features vectors.

119
00:08:48.260 --> 00:08:55.380
Everything's working perfectly so far we've said data frame select the creator or assembler transforms

120
00:08:55.450 --> 00:08:58.310
and we have the output which is just the label and the features.

121
00:08:58.320 --> 00:09:02.920
Now we're ready to actually create an array of training and test data.

122
00:09:03.180 --> 00:09:04.620
So now I'm just saying.

123
00:09:04.630 --> 00:09:10.770
Training and test data in order to do that.

124
00:09:11.080 --> 00:09:20.380
I will say yvel color Ray and pasand training and test as to values inside of the Saray is equal to

125
00:09:20.380 --> 00:09:30.570
the output that selects I'm going to select the label column and select the features column and do a

126
00:09:30.570 --> 00:09:32.190
random split off of it.

127
00:09:32.210 --> 00:09:40.470
This is random split and typically you do a 70 30 percent split between training and test set.

128
00:09:40.510 --> 00:09:42.730
It really depends on what your data looks like.

129
00:09:42.730 --> 00:09:47.290
I can't give you a 100 percent correct answer all the time on how you should split between training

130
00:09:47.290 --> 00:09:48.270
and test.

131
00:09:48.280 --> 00:09:56.750
It's really often really common to do a 17:30 but things such as 75 25 80 20 60 40 are also very common.

132
00:09:56.800 --> 00:10:01.990
Obviously the lower your training set the less your model is going to have to go off.

133
00:10:02.020 --> 00:10:04.790
So we'll probably get a larger error at the end.

134
00:10:04.850 --> 00:10:12.560
But you don't want to give it so much training data that testing on a single point is basically useless.

135
00:10:13.240 --> 00:10:17.920
We'll do 70 30 split and you can set a seed if you want.

136
00:10:18.040 --> 00:10:22.840
I just set one you find that way your results are more or less the same as mine.

137
00:10:22.840 --> 00:10:25.590
Next up is the actual linear regression model.

138
00:10:25.990 --> 00:10:28.030
Remember we have these basically three steps.

139
00:10:28.030 --> 00:10:31.800
The model the parameter great build there and then that train validations lit.

140
00:10:31.840 --> 00:10:37.580
Right now we just have that training set and that testing set we have to actually create the model.

141
00:10:37.690 --> 00:10:42.970
I will save L-R is equal to new linear regression.

142
00:10:43.100 --> 00:10:45.550
Close print sees there is our model.

143
00:10:45.550 --> 00:10:53.220
And up next we want to do the parameter grid now for this particular example I'm actually going to leave

144
00:10:53.220 --> 00:10:59.930
the parameter grid empty just to show you that if you want to do the train test and hold out data set

145
00:11:00.480 --> 00:11:05.760
and you're worried that you don't want to pick any parameters you just want to do that three tier testing

146
00:11:05.760 --> 00:11:10.440
instead of just a simple train split like we have here you actually want to have that whole that data.

147
00:11:10.440 --> 00:11:14.790
You need that parameter grid but luckily you can actually leave that parameter grid blank if you want

148
00:11:14.790 --> 00:11:22.770
to you can say Program grid is equal to new Sharam grid builder.

149
00:11:22.770 --> 00:11:28.830
And instead of saying I'm setting options here for parameter griots or adding anything you can just

150
00:11:28.920 --> 00:11:37.860
at the end of this essentially build it we can go down here and then say something like dots build and

151
00:11:37.860 --> 00:11:43.320
that's a functioning parameter grid it's kind of useless in that you're not actually testing any grade

152
00:11:43.320 --> 00:11:48.900
or any parameters you're not adding to this but you do need some sort of parameter grid object for this

153
00:11:48.900 --> 00:11:49.710
next step.

154
00:11:49.890 --> 00:11:59.210
And I'm going to create the next step of the train validation split object train blitz and essentially

155
00:11:59.210 --> 00:12:03.390
this is the hold out train revalidation split I will say.

156
00:12:03.390 --> 00:12:18.800
Val we can say train validation split or train Val split is equal to a new train validation split close

157
00:12:18.820 --> 00:12:24.080
parentheses and then I will say and I have Princie surrounding this so I can write this in multiple

158
00:12:24.080 --> 00:12:30.560
lines but the first thing to do is set the estimator So the estimator remember that's just the actual

159
00:12:30.560 --> 00:12:31.860
model we created.

160
00:12:31.880 --> 00:12:33.650
So there it is estimators set.

161
00:12:33.670 --> 00:12:37.340
We put the dot here so it's a little more clear that these are extra options.

162
00:12:38.300 --> 00:12:45.500
And then the next thing I want to do is actually say Dot set the evaluator number the evaluator we just

163
00:12:45.560 --> 00:12:46.410
imported.

164
00:12:46.410 --> 00:12:50.210
Now I can say new regression evaluator here.

165
00:12:51.030 --> 00:12:56.770
Goes Prince sees and then the next thing I want to do is actually set the estimator program mapping.

166
00:12:56.810 --> 00:13:01.560
And in this case for using an empty parameter grid and later on we'll explore who can add more Gritz

167
00:13:01.580 --> 00:13:10.490
that right now we'll just see Dot set estimated Primm maps and passen that Pareene great.

168
00:13:10.490 --> 00:13:13.190
We just made Graham grid.

169
00:13:13.410 --> 00:13:21.130
And then finally I want to see here dots set the train ratio remember that train races just how much

170
00:13:21.130 --> 00:13:25.550
of this are you going to use for that training and test and how much of it is left for the holdout.

171
00:13:25.740 --> 00:13:33.240
And here a very typical number something like 0.8 percent and now that we have that train of split you

172
00:13:33.240 --> 00:13:36.600
can essentially treat that object as if it were the new model.

173
00:13:36.600 --> 00:13:39.980
We can say well we make sure these Princie are balanced real quick.

174
00:13:40.000 --> 00:13:51.390
Know they are Velle model is equipped to train vowels splits dots fit to the training data

175
00:13:54.400 --> 00:14:05.100
and then same model transform the test data and selects basically the features the label and the prediction

176
00:14:05.100 --> 00:14:05.680
which are them.

177
00:14:05.770 --> 00:14:07.000
Are there more important things you want to know.

178
00:14:07.010 --> 00:14:10.320
So select features

179
00:14:12.940 --> 00:14:13.630
label

180
00:14:16.220 --> 00:14:22.020
and predictions actually prediction and show that off.

181
00:14:22.190 --> 00:14:22.560
All right.

182
00:14:22.560 --> 00:14:27.180
Let me say this and let's run it to make sure we have everything correctly shown.

183
00:14:27.420 --> 00:14:33.180
So will load X thought Skala run this and make sure that we get the outputs we expect.

184
00:14:33.200 --> 00:14:36.950
There's a train of split and here are our results.

185
00:14:36.960 --> 00:14:40.140
Here we have the features the label and the prediction.

186
00:14:40.140 --> 00:14:44.210
These are pretty okay predictions but we want to actually get some sort of measurement.

187
00:14:44.430 --> 00:14:49.260
Let's go ahead and calculate the air from our label column in our prediction column.

188
00:14:49.260 --> 00:14:49.610
All right.

189
00:14:49.620 --> 00:14:55.420
So so far we've been able to actually run a model using train validation split but there's a couple

190
00:14:55.420 --> 00:14:58.160
of things we haven't really explored yet in more detail.

191
00:14:58.170 --> 00:15:01.950
For one thing we actually haven't added any grids for parameters.

192
00:15:01.950 --> 00:15:04.230
So that's something we're going to want to play with.

193
00:15:04.260 --> 00:15:10.050
And another thing is still unclear what are we actually evaluating with what is the metric we are using

194
00:15:10.260 --> 00:15:15.600
for this particular evaluator in our case a regression evaluator in order to fully understand this.

195
00:15:15.650 --> 00:15:18.270
Well let's take a quick look at the documentation.

196
00:15:18.270 --> 00:15:23.610
When you go to the documentation now and here we are at Sparks that Apache the org you're already very

197
00:15:23.610 --> 00:15:27.280
familiar as we've been visiting documentation many times throughout the course.

198
00:15:27.300 --> 00:15:33.030
Click on the latest docs and then come here to API docs and click on Skala and what I want to show you

199
00:15:33.060 --> 00:15:34.630
is under evaluator.

200
00:15:34.740 --> 00:15:39.480
And here you can begin to search for evaluator and we can see here under M-L evaluation.

201
00:15:39.480 --> 00:15:42.090
We have our binary classification evaluator.

202
00:15:42.090 --> 00:15:44.520
So that's classification with two classes.

203
00:15:44.520 --> 00:15:47.910
We have a general evaluator or multiclass classification evaluator.

204
00:15:48.000 --> 00:15:50.200
And then here are regression evaluator.

205
00:15:50.310 --> 00:15:55.950
Click on the regression evaluator and come down here and you'll notice that there's this parameter metric

206
00:15:56.070 --> 00:15:59.490
name and last parameter for the metric name and evaluation.

207
00:15:59.490 --> 00:16:02.230
Now if we expand this we have a couple of options here.

208
00:16:02.250 --> 00:16:08.970
And the default option is the root mean square error which means we currently valuated those models

209
00:16:08.970 --> 00:16:10.560
using root mean squared error.

210
00:16:10.710 --> 00:16:16.440
Well we can set the metric name to be mean square there are squared metric or the mean absolute air

211
00:16:16.980 --> 00:16:19.650
in order to actually compare metrics.

212
00:16:19.680 --> 00:16:21.680
We need to have more parameters.

213
00:16:21.870 --> 00:16:27.780
So to do all this let's go back at a couple grids and then play around with the different metric names

214
00:16:27.930 --> 00:16:31.770
and see if we can fully understand the entire process here.

215
00:16:31.860 --> 00:16:34.620
Let me pull back the Adam text editor.

216
00:16:34.620 --> 00:16:36.950
All right so he came back from the documentation.

217
00:16:36.960 --> 00:16:42.390
Now the first thing we need to do is actually add a couple of grid of parameters we want to add a grid.

218
00:16:42.510 --> 00:16:46.650
And that way we can use this regression evaluator metric to compare models.

219
00:16:46.680 --> 00:16:56.070
In this case we'll come back here to pram great builder we will say at one dot add a grid and let's

220
00:16:56.070 --> 00:17:03.990
say our Reg program Nazi regularisation parameter will put an array here and we're going to put two

221
00:17:04.020 --> 00:17:05.270
very different values.

222
00:17:05.280 --> 00:17:11.360
One of them is going to be more extreme we'll say something like 10000 as a regular ization parameter.

223
00:17:11.490 --> 00:17:16.450
And the second one is going to be much smaller we'll say something like 0.1 that we can really tell

224
00:17:16.450 --> 00:17:18.840
the difference when we compare these two models.

225
00:17:18.990 --> 00:17:23.080
I'm going to save this and run this script again.

226
00:17:23.490 --> 00:17:26.340
Load X Skala enter.

227
00:17:26.370 --> 00:17:31.410
And we should now be able to call the actual validation metrics off of our model object.

228
00:17:31.410 --> 00:17:32.430
Once it's running.

229
00:17:32.490 --> 00:17:35.310
And we shouldn't notice a significant difference between them.

230
00:17:35.520 --> 00:17:42.120
OK here are actual results this label on prediction here Sparke telling us hey I ran two models here

231
00:17:42.330 --> 00:17:46.960
one with a very large regularisation parameter one the much smaller ones are 0.1.

232
00:17:47.310 --> 00:17:52.550
If I click model dot tab I can see here I get the validation metrics as an option.

233
00:17:52.590 --> 00:17:55.440
Let's actually call that validation metrics.

234
00:17:56.450 --> 00:18:00.140
Enter and notice now I have two different values here.

235
00:18:00.140 --> 00:18:05.610
I have 97000 zeros or a five and ninety six thousand nine hundred ninety four.

236
00:18:05.780 --> 00:18:09.580
Still pretty close specially given how different regularisation was for these two.

237
00:18:09.800 --> 00:18:14.680
And that's just the basic overall hint that regularisation doesn't really affect this particular problem

238
00:18:14.690 --> 00:18:15.660
given this data.

239
00:18:15.800 --> 00:18:18.770
But we can still see some sort of difference here between the two.

240
00:18:18.800 --> 00:18:20.450
Regularisation parameters.

241
00:18:20.480 --> 00:18:24.250
Now imagine that I actually wanted to choose a different validation metric.

242
00:18:24.290 --> 00:18:29.600
Remember in this case by default our validation metric is root mean squared error.

243
00:18:29.780 --> 00:18:35.150
Let's try switching it up to our Square and show you how to actually set those parameters in order to

244
00:18:35.150 --> 00:18:35.800
do that.

245
00:18:36.020 --> 00:18:41.560
We will scroll this back down and revisit this object this new regression evaluator.

246
00:18:41.720 --> 00:18:47.590
OK we can set this metric to actually compare these by another metric that's not root mean squared error

247
00:18:47.650 --> 00:18:52.880
by coming back to this new regression evaluator and saying dot sets.

248
00:18:53.150 --> 00:18:59.300
In this case it's metric name and then passing in a string code for the particular metric name we want

249
00:18:59.300 --> 00:19:00.200
to evaluate.

250
00:19:00.350 --> 00:19:02.530
In our case we want the R-squared metric.

251
00:19:02.570 --> 00:19:05.340
So we'll just passen our two.

252
00:19:05.510 --> 00:19:07.280
And again you can reference that documentation.

253
00:19:07.280 --> 00:19:12.410
We were just exploring if you want these actual code names and there's code names for the binary classification

254
00:19:12.410 --> 00:19:17.290
or multiclass classification evaluators as well to actually choose what metric you want to evaluate.

255
00:19:17.300 --> 00:19:19.570
I'm going to save this.

256
00:19:19.600 --> 00:19:24.680
And now let's load this script again and see what happens when we call validation metrics this time

257
00:19:24.680 --> 00:19:28.750
around going to load X that's gonna run this.

258
00:19:28.790 --> 00:19:34.080
And now we're running the same train validations split except we're using a different metric name.

259
00:19:34.100 --> 00:19:37.070
In this case we're using R-squared as our metric.

260
00:19:37.070 --> 00:19:45.630
And now that we have run the model we can say model dots validation metric or metrics enter.

261
00:19:46.020 --> 00:19:48.060
And here we can see the R-squared values.

262
00:19:48.090 --> 00:19:53.320
No these are squared values are almost exactly the same because in this case regularisation doesn't

263
00:19:53.320 --> 00:19:54.990
make a big difference.

264
00:19:55.100 --> 00:20:00.020
If we want to see a more extreme example we can really pump up regularisation here on one of them and

265
00:20:00.020 --> 00:20:02.150
make one of these quite a bit smaller.

266
00:20:02.180 --> 00:20:05.830
We're going to save this and see if we can get any difference in our square.

267
00:20:05.900 --> 00:20:11.160
Basically what this is telling you is that both of these models fit almost exactly the same.

268
00:20:11.240 --> 00:20:12.080
Let's say that.

269
00:20:12.080 --> 00:20:12.910
Run it again.

270
00:20:13.040 --> 00:20:14.680
See if we get any difference.

271
00:20:14.720 --> 00:20:19.130
If not you can explore on a different regularisation or different regression dataset and see if you

272
00:20:19.130 --> 00:20:22.260
get anything different with these metrics.

273
00:20:22.270 --> 00:20:22.490
All right.

274
00:20:22.490 --> 00:20:29.070
Training it it's done training now it's called Model validation metrics enter.

275
00:20:29.240 --> 00:20:32.450
And here we can see that one of these now has a really horrible fit.

276
00:20:32.480 --> 00:20:34.730
This is a horrible R-squared value.

277
00:20:34.820 --> 00:20:43.280
It's point 0 0 0 5 point 1 2 4 et cetera versus a pretty good R-squared fit 92 percent of variance explainer

278
00:20:43.340 --> 00:20:44.860
0.9 to.

279
00:20:45.230 --> 00:20:45.940
OK.

280
00:20:46.010 --> 00:20:48.200
That's basically it for this lecture.

281
00:20:48.200 --> 00:20:53.180
Let me do a quick review of everything we've just done since we were working with a real data set.

282
00:20:53.210 --> 00:20:56.350
Obviously hopefully you don't do such extreme grid's.

283
00:20:56.360 --> 00:20:57.900
But let's start from the beginning.

284
00:20:57.910 --> 00:20:59.620
Review everything we just did.

285
00:20:59.630 --> 00:21:00.200
All right.

286
00:21:00.260 --> 00:21:06.110
For the quick review we set up regression evaluator since we're dealing if a regression problem in whatever

287
00:21:06.110 --> 00:21:07.100
case you're working with.

288
00:21:07.100 --> 00:21:13.310
You want to actually import the correct evaluator such as a classification evaluator a binary classification

289
00:21:13.310 --> 00:21:16.960
or multiclass classification evaluator depending on your dataset in your problem.

290
00:21:17.120 --> 00:21:21.620
Then we actually called in the model in this case we're using regression linear regression and then

291
00:21:21.620 --> 00:21:25.190
we called in two objects from tuning the parameter grid builder.

292
00:21:25.190 --> 00:21:28.790
And that train validation split everything else we've seen before.

293
00:21:28.790 --> 00:21:33.920
Things such as setting the error level starting your Spark's session reading in your data setting up

294
00:21:33.920 --> 00:21:38.540
your data for machine learning where we're that's the label features column set up and you do that using

295
00:21:38.540 --> 00:21:39.800
vector assembler.

296
00:21:39.800 --> 00:21:45.350
Then we split our data into a training set and a test set using random split.

297
00:21:45.350 --> 00:21:49.600
We create our model and we set up our parameter great builder.

298
00:21:49.700 --> 00:21:56.150
We say new program great builder dot add a grid and based off of whatever model you're using you can

299
00:21:56.150 --> 00:22:00.800
set up different parameters and you can always reference documentation for what specific parameters

300
00:22:00.830 --> 00:22:04.400
you can set in your model and you add an array of values.

301
00:22:04.400 --> 00:22:08.560
In this case quite an extreme difference of array and it doesn't just have to be two values.

302
00:22:08.660 --> 00:22:13.910
You can for instance put a comma here and now compare three different models and you can keep adding

303
00:22:14.150 --> 00:22:18.350
more parameters to test out more different models.

304
00:22:18.350 --> 00:22:20.900
Then finally you build that parameter grid.

305
00:22:21.080 --> 00:22:22.880
Then you have your train validation split.

306
00:22:22.880 --> 00:22:25.310
You create a new train validation split object.

307
00:22:25.400 --> 00:22:26.440
Set your estimator.

308
00:22:26.450 --> 00:22:27.860
Set your evaluator.

309
00:22:27.860 --> 00:22:33.460
In this case of calling new regression evaluator and if you want you can set a particular metric name.

310
00:22:33.530 --> 00:22:38.240
If you're not satisfied with the default metric and if you ever want to look up these metric names you

311
00:22:38.240 --> 00:22:40.560
just go to the documentation page we showed earlier.

312
00:22:40.700 --> 00:22:46.200
You set up your estimated parameter grid and set up your train ratio for the train test hold out ratio.

313
00:22:46.250 --> 00:22:48.190
Then you call a model.

314
00:22:48.260 --> 00:22:53.650
In this case it's just train vals split as if it were a normal model fit onto your training set.

315
00:22:53.660 --> 00:22:59.090
Use your model to transform your test set of data and actually select features label and prediction

316
00:22:59.120 --> 00:23:03.590
show and later on you can also do things such as select your validation metrics.

317
00:23:03.650 --> 00:23:07.070
All right if you have any questions feel free to post the Kewney forums.

318
00:23:07.070 --> 00:23:11.910
This is a pretty advanced stuff and it's the absolute latest stuff with SPARC 2.0.

319
00:23:12.080 --> 00:23:15.410
Hope you enjoyed this lecture and I will see you at the next one.

320
00:23:15.410 --> 00:23:16.060
Thanks everyone.
