1
00:00:05,950 --> 00:00:12,440
Hello everyone and welcome to the model evaluation section of the Course model evaluation is a fundamental

2
00:00:12,440 --> 00:00:16,560
topic of understanding your model's performance and the machine learning in general.

3
00:00:16,730 --> 00:00:21,800
If you want a more in-depth look at this topic go ahead and review chapter two of an introduction to

4
00:00:21,800 --> 00:00:27,610
statistical learning in this section of the course we're going to be discussing the following topics

5
00:00:28,010 --> 00:00:31,930
we'll review train splits and talk about holdout datasets.

6
00:00:32,110 --> 00:00:37,870
Then we'll review parameter Gritz and talk about how we can actually use Scullin spark for model evaluation

7
00:00:38,380 --> 00:00:43,090
then we'll have a brief discussion of the bias variance trade off and then we'll explore some documentation

8
00:00:43,090 --> 00:00:46,550
examples and also code out through some Skala and spark examples.

9
00:00:48,120 --> 00:00:52,680
Let's begin by talking about train splits which you've previously mentioned when discussing machine

10
00:00:52,680 --> 00:00:58,110
learning in general bullets for the concept you will always train the machine learning algorithm on

11
00:00:58,110 --> 00:01:03,720
some data but afterwards you're usually going to want some measure of how well it performed some sort

12
00:01:03,720 --> 00:01:09,900
of performance metric and each made machine learning task or idea has different metrics for evaluation.

13
00:01:09,900 --> 00:01:13,910
Let's show examples of just a few of them for regression tasks.

14
00:01:13,920 --> 00:01:19,260
You have metrics such as R-squared values for Explain variance or other metrics such as the root mean

15
00:01:19,260 --> 00:01:22,080
squared error for classification tasks.

16
00:01:22,110 --> 00:01:27,730
Through your confusion matrix you usually get metrics such as precision accuracy or recall.

17
00:01:27,980 --> 00:01:31,960
And then for clustering tasks you have things such as within sum of squares error.

18
00:01:31,980 --> 00:01:35,100
These are definitely not all the metrics but just a few examples of them.

19
00:01:36,480 --> 00:01:39,830
Well you could get these measurements or metrics using the same data.

20
00:01:39,840 --> 00:01:43,230
You trained your model on that's usually not a good idea.

21
00:01:43,230 --> 00:01:45,600
Your model has already seen this training data.

22
00:01:45,720 --> 00:01:49,070
Meaning it's not a good choice for evaluating your model's performance.

23
00:01:49,200 --> 00:01:54,390
What you really want to do is get these sort of metrics off test data which your model has not seen

24
00:01:54,390 --> 00:01:54,780
yet.

25
00:01:54,960 --> 00:02:00,180
And this is the main idea known as train test split you split your data into two sets your training

26
00:02:00,180 --> 00:02:01,610
set and you're testing set.

27
00:02:01,830 --> 00:02:07,090
You fit your model to the training set and evaluate it using the data hasn't seen before the test set

28
00:02:08,730 --> 00:02:11,750
in expansion of this idea is the hold up dataset.

29
00:02:11,980 --> 00:02:14,740
And this is separate from the training and test sets.

30
00:02:14,740 --> 00:02:20,230
Basically in this process use the training data to fit your model and then you use the test set to evaluate

31
00:02:20,260 --> 00:02:21,420
and adjust your model.

32
00:02:21,520 --> 00:02:25,980
So now you have not just training and test sets of data but you have three sets of data.

33
00:02:26,050 --> 00:02:32,330
The training set the test set and the holdout set and you can use that test set over and over again.

34
00:02:32,380 --> 00:02:37,840
So you train on the training data fit your model use the test set to evaluate and then adjust back to

35
00:02:37,840 --> 00:02:42,760
your original model and the training data to fix parameters or play around your model and see how you

36
00:02:42,760 --> 00:02:43,830
can improve it.

37
00:02:43,960 --> 00:02:49,510
Then before you deploy your model you check it against the holdout data set to get some final metrics

38
00:02:49,570 --> 00:02:50,440
on performance.

39
00:02:50,470 --> 00:02:53,800
And once you've tested it on the whole dataset there's no going back.

40
00:02:53,800 --> 00:02:59,530
That's your final measure of how well your model perform or how well you expect your model to perform

41
00:02:59,770 --> 00:03:05,090
right before you deploy your model onto some new data.

42
00:03:05,090 --> 00:03:11,230
Now let's talk about parameter grids as I just mentioned usually needs some sort of parameters to adjust

43
00:03:11,260 --> 00:03:13,050
in your machine learning algorithms.

44
00:03:13,100 --> 00:03:17,690
And as we've already seen we can have optional parameters in machine learning algorithms of skull and

45
00:03:17,690 --> 00:03:22,730
Sparke however many times it's difficult to know what are good values for these parameters.

46
00:03:22,880 --> 00:03:24,520
And there's no right or wrong answer.

47
00:03:24,560 --> 00:03:27,240
It really depends on your particular dataset.

48
00:03:27,260 --> 00:03:33,530
So a lot of times people want to know what's a good value for parameter x or parameter y in this specific

49
00:03:33,530 --> 00:03:37,650
machine learning algorithm and there's no real good answer in those situations.

50
00:03:37,790 --> 00:03:42,600
However SPARC makes it possible to set up a grid of parameters to train across.

51
00:03:42,770 --> 00:03:48,770
That way you can create multiple models train them across the grid and SPARC will report back which

52
00:03:48,770 --> 00:03:56,370
model perform the best based off of some sort of evaluation metric or some performance metric.

53
00:03:56,430 --> 00:04:01,860
Now let's talk about spark and Skala in a general sense for model evaluation Sparke makes all of these

54
00:04:01,860 --> 00:04:06,000
processes generally easy with the use of three object types.

55
00:04:06,090 --> 00:04:12,060
It has evaluators Parama grid builders and train validation split objects and later on this section

56
00:04:12,060 --> 00:04:17,100
of the course we're going to have coded out examples where we're going to be exploring how to use these

57
00:04:17,100 --> 00:04:24,430
object types to implement the ideas we just discussed an important aspect to understanding all of this

58
00:04:24,430 --> 00:04:30,340
is the bhai's various tradeoff we won't directly explored spark and Skala because it pertains more to

59
00:04:30,340 --> 00:04:32,190
theory than data engineering.

60
00:04:32,200 --> 00:04:37,280
However with that being said let's take the time now to at least understand the concept so we can have

61
00:04:37,300 --> 00:04:39,660
full context for this section of the course.

62
00:04:40,540 --> 00:04:46,000
The bias variance tradeoff is the point where we are just adding noise by adding model complexity or

63
00:04:46,000 --> 00:04:47,290
flexibility.

64
00:04:47,290 --> 00:04:52,000
The training error goes down as it has to but the test error will start to go up.

65
00:04:52,210 --> 00:05:00,660
The model after the bias tradeoff begins to overfit Let's go ahead and discuss this topic by imagining

66
00:05:00,750 --> 00:05:02,190
a dartboard.

67
00:05:02,310 --> 00:05:07,560
So it measures the center of this target or dartboard is a model that perfectly predicts the correct

68
00:05:07,680 --> 00:05:10,880
values as we move away from the bullseye.

69
00:05:10,890 --> 00:05:16,710
Our predictions will get worse and worse and we're going to go ahead and make a quadrant of low variance

70
00:05:16,710 --> 00:05:20,450
versus high variance and high bias versus low bias.

71
00:05:20,520 --> 00:05:28,110
So we can get an understanding of what the bias in variance terms mean generally imagine we can repeat

72
00:05:28,170 --> 00:05:34,260
our entire model building process to get a number of separate hits on the target each represents an

73
00:05:34,350 --> 00:05:36,430
individual realization of our model.

74
00:05:36,690 --> 00:05:43,180
Given the chance variability in the training data we gather sometimes we will get a good distribution

75
00:05:43,180 --> 00:05:44,290
of training data.

76
00:05:44,290 --> 00:05:47,040
So we predict very well and we are close to the bull's eye.

77
00:05:47,320 --> 00:05:53,530
While sometimes our training data might full of outliers or nonstandard values resulting in poor predictions

78
00:05:54,460 --> 00:06:00,400
and these different realizations result in a scatter of hits on the target what were aiming for is something

79
00:06:00,400 --> 00:06:02,600
for low bias and low variance.

80
00:06:02,830 --> 00:06:07,660
But realistically you'll have to do is tradeoff variance or bias.

81
00:06:07,720 --> 00:06:13,960
And here we can see in the quadrant of the target a low variance low bias model will predict correct

82
00:06:13,960 --> 00:06:20,920
values on the bullseye a low bias high variance model will predict values around the bullseye.

83
00:06:20,950 --> 00:06:27,400
But with a high degree of variance versus a high bias low variance model in that lower quadrant will

84
00:06:27,400 --> 00:06:31,760
have a higher bias to a certain location but low variance.

85
00:06:31,810 --> 00:06:37,900
All your models predictions are in a certain area and in the worst high variance high bias means you

86
00:06:37,900 --> 00:06:45,460
just all over the place basically a common temptation for beginners is to continually add complexity

87
00:06:45,460 --> 00:06:48,690
to a model until it fits the training set very well.

88
00:06:48,690 --> 00:06:53,410
And let's go ahead and begin to understand this in a machine learning algorithm to learn about linear

89
00:06:53,410 --> 00:06:54,870
regression.

90
00:06:54,880 --> 00:06:59,100
What you may want to do is let's say you're given a set of red training data.

91
00:06:59,110 --> 00:07:04,450
Now you're testing data your training data you might have a simple model such as the blue line and you

92
00:07:04,450 --> 00:07:09,880
get a certain error on your training data and you the site as a beginner hey maybe I should just make

93
00:07:09,880 --> 00:07:15,130
the model more and more complex or flexible so that it hits all those training points.

94
00:07:15,130 --> 00:07:19,930
However if you're hitting all those training points you're going to your model is going to fail to predict

95
00:07:20,170 --> 00:07:27,960
for new test points which is why we do that train test split doing that can cause a model to overfit

96
00:07:27,950 --> 00:07:29,040
to your training data.

97
00:07:29,060 --> 00:07:34,790
And like I mention cause large errors on new data such as the tests that we're going to do is take a

98
00:07:34,790 --> 00:07:40,970
look at an example model on how we can see overfitting occur from an air standpoint using test data.

99
00:07:41,120 --> 00:07:47,960
We'll use a black curve with some noise points off of it to represent the true shape the data follows.

100
00:07:48,110 --> 00:07:48,710
All right.

101
00:07:48,710 --> 00:07:50,540
We have three images here.

102
00:07:50,600 --> 00:07:53,560
The first one is the X versus the Y.

103
00:07:53,750 --> 00:08:00,050
And here we have model flexibility as different linear fits a linear fit a quadratic fit or a spline

104
00:08:00,050 --> 00:08:02,950
fit with each one getting more complex.

105
00:08:02,960 --> 00:08:07,550
So the simplest is just a linear fit more complicated that is quadratic.

106
00:08:07,550 --> 00:08:12,410
And then you can have a spline fit and you'll notice that the black curve is the truth that the model

107
00:08:12,410 --> 00:08:13,560
actually follows.

108
00:08:13,580 --> 00:08:18,360
So all the points are just noise around the actual black curve which is the truth.

109
00:08:18,890 --> 00:08:24,560
In order to evaluate your models and compare the complexities to each other well you'll have to do is

110
00:08:24,560 --> 00:08:27,990
plot out the complexity or flexibility of the model.

111
00:08:28,010 --> 00:08:35,260
For instance the polynomial level of a regression fit versus the error metric such as mean square error.

112
00:08:35,270 --> 00:08:39,920
And you can see here in that middle plot that's exactly what we've done and we've plotted out for the

113
00:08:39,920 --> 00:08:42,560
training data versus the test data.

114
00:08:42,560 --> 00:08:44,270
You'll notice all the way to the left.

115
00:08:44,270 --> 00:08:48,530
We have a simple model that yellow linear model for this model.

116
00:08:48,530 --> 00:08:54,350
We have a high error on both the test data and the training data as we begin to get more complicated

117
00:08:54,350 --> 00:08:56,330
with the quadratic model in blue.

118
00:08:56,540 --> 00:09:03,020
We begin to lower the error for the test data and the training data as we get even more complex however

119
00:09:03,050 --> 00:09:07,300
we begin to lower the lower the error for the training data significantly.

120
00:09:07,370 --> 00:09:11,730
But as a result you begin to have raised the error on the test data.

121
00:09:11,750 --> 00:09:18,800
The new data show you want to find the point that's going to balance out the bias and the variance in

122
00:09:18,830 --> 00:09:23,570
this particular case that's going to be closer to the quadratic fit that Bluepoint.

123
00:09:23,600 --> 00:09:28,790
So you want to balance out the bias and variance of your model to the point where your test data and

124
00:09:28,790 --> 00:09:32,840
training data have reached some sort of minimum and grouping together.

125
00:09:34,850 --> 00:09:41,640
This is the classic plot to show this as a general stance where you have low versus high model complexity

126
00:09:41,640 --> 00:09:45,730
on the x axis and some sort of prediction error on the y axis.

127
00:09:45,840 --> 00:09:50,010
And as you moved to the left you get a higher bias but lower variance.

128
00:09:50,100 --> 00:09:55,560
And as you move to the right to a higher complexity model you get a lower bias but high variance.

129
00:09:55,560 --> 00:10:00,980
And what you want to do is pick a point where you're comfortable with the bias tradeoff.

130
00:10:01,080 --> 00:10:05,130
If you go to the left of it you'll start to under fit to the data.

131
00:10:05,130 --> 00:10:09,990
And if you go to the right of it you'll start to overfit the data meaning you're hitting all those points

132
00:10:09,990 --> 00:10:15,430
in your training data and new data is cutting a larger error because of that.

133
00:10:15,430 --> 00:10:20,060
All right let's continue by going through some examples and exploring the documentation.

134
00:10:20,080 --> 00:10:22,030
Thanks everyone and I'll see you at the next lecture.
