WEBVTT
1
00:00:05.180 --> 00:00:10.220
Hello everyone and welcome to the recommender system implementation of lecture and this lecture we're

2
00:00:10.220 --> 00:00:14.960
going to be using the same data set that we just uploaded in the last lecture and actually build a movie

3
00:00:14.960 --> 00:00:20.570
recommendation system using Sparc and Skala are going to jump straight to my data notebook where we

4
00:00:20.570 --> 00:00:21.900
left off last time.

5
00:00:22.130 --> 00:00:25.130
OK here I am at a data Brick's notebook.

6
00:00:25.130 --> 00:00:31.750
I'm going to import a couple of things that we're going to be needing for actual recommendation system.

7
00:00:32.120 --> 00:00:39.990
We want to import or Apache SPARC M-L recommendation

8
00:00:43.580 --> 00:00:50.510
alias and then we can shift and try and run that cell and then the next thing I want to do is actually

9
00:00:50.510 --> 00:00:52.240
import the ratings.

10
00:00:52.250 --> 00:00:56.330
Now you've already done this in the previous lecture but in case you want a reminder we just say Val

11
00:00:56.330 --> 00:01:04.890
ratings as it will to spark read the option that we're going to have the better be true.

12
00:01:05.140 --> 00:01:16.760
And then we're also going to have the option of infer schema the true cxxviii and then the file path

13
00:01:16.760 --> 00:01:18.440
to that data file.

14
00:01:18.460 --> 00:01:26.100
We're going to copy and paste that see every path that I saved earlier so I'm going to copy and paste

15
00:01:26.100 --> 00:01:26.960
that here.

16
00:01:28.390 --> 00:01:31.800
And then run shift and try to make sure everything worked.

17
00:01:31.850 --> 00:01:36.820
The cell mammo a little zoomed in which is why it looks like it's on multiple lines but this is all

18
00:01:36.820 --> 00:01:38.170
just one line.

19
00:01:38.220 --> 00:01:38.880
OK.

20
00:01:39.040 --> 00:01:44.560
To confirm that's looking all right we can say something like rating's head and check up the head of

21
00:01:44.560 --> 00:01:45.330
the ratings.

22
00:01:45.430 --> 00:01:51.100
So for instance the very first row we have that user id the movie the rating that user gave that particular

23
00:01:51.100 --> 00:01:51.940
movie.

24
00:01:52.270 --> 00:01:56.660
And just to confirm we can also say current schema shift enter.

25
00:01:56.680 --> 00:02:02.200
And you can use tab autocomplete here OK since we're using the notebook system.

26
00:02:02.200 --> 00:02:07.930
I want to mention real quickly that in the data Brick's notebook system if you say rating's dot you

27
00:02:07.930 --> 00:02:13.750
can click the tab key on your keyboard and actually get a dropdown list of all the methods available

28
00:02:13.750 --> 00:02:19.420
to you on that particular object so we can see here all the methods available on this rating's data

29
00:02:19.420 --> 00:02:20.060
frame.

30
00:02:20.140 --> 00:02:24.820
We've seen this before in the spark shell where we clicked that tab and were able to see various options

31
00:02:24.850 --> 00:02:30.730
but just know that it's also available here in this notebook format and users of the Jupiter notebook

32
00:02:30.970 --> 00:02:33.940
will feel this to be really familiar.

33
00:02:33.950 --> 00:02:39.160
All right so we have our ratings now what we want to do is actually try to build a recommendation system

34
00:02:40.060 --> 00:02:45.730
now by their nature recommendation systems are pretty hard to evaluate how well they actually perform

35
00:02:46.010 --> 00:02:47.380
especially on new users.

36
00:02:47.380 --> 00:02:48.630
You have no data.

37
00:02:48.760 --> 00:02:55.330
We're going to do is split our data into a training and test as we've done before in the past so we'll

38
00:02:55.330 --> 00:03:03.370
say thou array training and test those two variables are two values is rating's dog.

39
00:03:03.460 --> 00:03:08.260
And then we will do a random split and you can see here I'm just clicking tab and it gives me the options

40
00:03:08.260 --> 00:03:15.780
of what to choose will say random split on this array and we'll do an 80 percent 20 percent split.

41
00:03:15.880 --> 00:03:21.050
So 80 percent training data 20 percent test data shift enter to run that.

42
00:03:21.100 --> 00:03:25.700
And here we have as the output our two data frame's training and testing.

43
00:03:25.760 --> 00:03:34.750
Now it's time to actually build the recommendation model so I will say vowel a ls and we create a new

44
00:03:35.580 --> 00:03:36.850
HLS model.

45
00:03:38.030 --> 00:03:40.490
And you can set lots of parameters here.

46
00:03:40.640 --> 00:03:45.870
You can check out the actual documentation page for the various parameters you can set.

47
00:03:45.920 --> 00:03:53.240
But a couple of the ones you can set is for instance at the max iterations and we're going to be following

48
00:03:53.240 --> 00:03:55.130
along with the documentation.

49
00:03:55.430 --> 00:04:03.550
So we'll say five max iterations and we also set a regularization parameter for this matrix factorization

50
00:04:03.550 --> 00:04:07.390
essentially and we'll say 0.01.

51
00:04:07.630 --> 00:04:14.860
And then the three main settings are parameters you want to set as the user call the item column and

52
00:04:14.860 --> 00:04:16.560
then the rating column.

53
00:04:16.660 --> 00:04:25.390
So I'll say set user call and this is to be a numeric column that identifies the actual users and in

54
00:04:25.390 --> 00:04:31.810
our case that's the user id column and then the other parameter you're usually going to have for a recommendation

55
00:04:31.810 --> 00:04:34.850
system is the actual item you're trying to recommend.

56
00:04:35.020 --> 00:04:39.520
So in our case it's movies that if you're dealing with something like Amazon this could be anything

57
00:04:39.520 --> 00:04:42.040
like the actual Amazon item code.

58
00:04:43.000 --> 00:04:49.630
So in this case we're going to say the movie ID and keep in mind this particular item column also needs

59
00:04:49.630 --> 00:04:51.710
to be a numeric form.

60
00:04:52.150 --> 00:04:59.020
And then finally the rating call and in our case that's just going to be the rating call.

61
00:04:59.230 --> 00:05:04.250
And then this is what we're going to do we're going to shift enter and create that new Aeolus object

62
00:05:04.990 --> 00:05:14.590
and then I'm going to say that all model is equal to a less fit that on the training set.

63
00:05:16.760 --> 00:05:21.360
And that's going to actually run most of the spark jobs there as it's being trained.

64
00:05:21.500 --> 00:05:27.110
And now we have a trained model and we can evaluate the model by actually performing some transformation

65
00:05:27.140 --> 00:05:28.190
of the test data.

66
00:05:28.310 --> 00:05:39.270
So I will say this that all predictions is equal to model does transform.

67
00:05:39.510 --> 00:05:44.930
And then I'm going to pass that test data and then shift and try to run that.

68
00:05:45.090 --> 00:05:46.780
And now I have my predictions.

69
00:05:46.950 --> 00:05:53.700
So if I see my predictions or if I just say predictions show shift enter we'll should see the output

70
00:05:54.030 --> 00:05:55.800
of that predictions data frame

71
00:05:58.920 --> 00:05:59.640
and there it is.

72
00:05:59.640 --> 00:06:05.150
So here I have my user ID the movie ID the rating and the prediction.

73
00:06:05.370 --> 00:06:08.010
So this is just the actual test data.

74
00:06:08.040 --> 00:06:13.140
This is the training data that I train our model on these first three columns are the actual test data

75
00:06:13.370 --> 00:06:19.190
and then the fourth column is the prediction I had based off of that user ID in that movie.

76
00:06:19.530 --> 00:06:27.990
So I predicted that user to 42 when watching movie 4 6 3 would have a three and a half or 3.6 star rating

77
00:06:28.350 --> 00:06:31.290
when in reality they had a four star rating.

78
00:06:31.320 --> 00:06:34.700
So how can we actually evaluate how well our model did.

79
00:06:35.010 --> 00:06:41.100
Well what do we that's an easy metric to more or less check it just check how far off on average are

80
00:06:41.100 --> 00:06:43.750
we from the rating versus the prediction.

81
00:06:44.610 --> 00:06:48.900
So let's show you how to do that and something that's nice about the notebook is you can delete the

82
00:06:48.900 --> 00:06:55.170
content in a cell but until you run it you won't see the output disappear which is kind of nice.

83
00:06:55.330 --> 00:06:57.710
In this case we're going to need to import one more thing.

84
00:06:57.760 --> 00:07:07.000
We'll need to import from org Patchi that spark that sequel dot functions dot underscore and that's

85
00:07:07.000 --> 00:07:12.540
where we have the capability to actually use functions such as absolute value and average.

86
00:07:12.680 --> 00:07:19.320
So I'm going to say all error is equal to predictions member predictions.

87
00:07:19.330 --> 00:07:21.690
Is the data Freemans showing right here below.

88
00:07:21.800 --> 00:07:33.160
And I'm going to select the rating column minus the prediction column and you can do this either way

89
00:07:33.160 --> 00:07:36.460
you can say prediction minus rating rating minus prediction.

90
00:07:36.480 --> 00:07:42.720
But what I want to do is calculate a new column which is going to be the rating minus the prediction

91
00:07:42.840 --> 00:07:48.540
and then take the average of that column to see on average how far off is our prediction that our rating.

92
00:07:48.660 --> 00:07:51.130
But there's one more thing that I need to keep in mind.

93
00:07:51.360 --> 00:07:56.430
And that's the fact that my prediction may be lower or higher than the true rating.

94
00:07:56.640 --> 00:08:03.300
So for instance here my prediction on the very first one was lower but on the third row My prediction

95
00:08:03.330 --> 00:08:04.310
is higher.

96
00:08:04.650 --> 00:08:12.180
And if on average we get half of them to be off by minus 1 and the other half off by Plus 1 if I just

97
00:08:12.180 --> 00:08:16.410
take the average of that it's going to look like I have a very good system because negative one and

98
00:08:16.410 --> 00:08:18.200
one the average is going to be zero.

99
00:08:18.330 --> 00:08:25.390
So I'm going to fool myself into thinking I have an absolute or excuse me that I have a zero error or

100
00:08:25.390 --> 00:08:27.760
an absolutely perfect recommendation system.

101
00:08:27.760 --> 00:08:32.840
And in that case I need to take the absolute value of this and this should be reminiscent to things

102
00:08:32.840 --> 00:08:39.740
such as the root mean squared error where you do things such as squaring the actual rating or true value

103
00:08:39.740 --> 00:08:44.510
minus a prediction in order to get rid of that sort of averaging below and above.

104
00:08:44.570 --> 00:08:49.880
And if you wanted to you could also do something like a root mean squared error on this sort of rating

105
00:08:49.880 --> 00:08:51.140
and prediction value.

106
00:08:51.140 --> 00:08:53.500
But here we're going to take the absolute value of this.

107
00:08:53.780 --> 00:08:58.370
And then we're going to say shift and try to run that sell.

108
00:08:58.490 --> 00:09:01.310
And now that's going to put this error data frame.

109
00:09:01.310 --> 00:09:04.890
So if I check out the air here and just show that

110
00:09:07.810 --> 00:09:15.150
this is going to give me the absolute value of rating minus prediction is the output.

111
00:09:15.220 --> 00:09:16.100
And there it is.

112
00:09:16.100 --> 00:09:20.980
So we have the absolute value of reading May's prediction and here I can see how off I was for each

113
00:09:20.980 --> 00:09:25.370
of those predictions and what I want to know on average how far off am I.

114
00:09:25.720 --> 00:09:35.070
And a very quick way to do this is to just say the scribe and then show description but you're going

115
00:09:35.070 --> 00:09:40.800
to notice something when I say that the scribe we won't be able to get that mean or the average immediately

116
00:09:41.340 --> 00:09:46.690
and I'm going to explain why in just a second once we see that output.

117
00:09:46.710 --> 00:09:53.690
OK so here we can see that the output shows a count and a minimum of value but mean instead of deviation

118
00:09:53.710 --> 00:09:56.520
have NULL or an end values.

119
00:09:56.530 --> 00:10:02.440
Now the reason for that is because when we did the split on the training data and the testing data there

120
00:10:02.440 --> 00:10:09.640
were some user IDs that were able to be in both the training set and the testing set which means we

121
00:10:09.640 --> 00:10:11.870
have no values in our output.

122
00:10:12.010 --> 00:10:20.590
That means the basically the model had never even seen the user id or movie Id combination before had

123
00:10:20.590 --> 00:10:24.330
never even heard of this user and it wasn't able to do any sort of output.

124
00:10:24.430 --> 00:10:36.860
So we can do here is say in a drop described show and this one showing the average for where we do have

125
00:10:36.860 --> 00:10:41.630
information it's going to drop those null values as we learned about when we learned how to deal with

126
00:10:41.630 --> 00:10:43.280
missing data and data frames.

127
00:10:43.280 --> 00:10:49.490
OK and here we can see on average we are off by zero point eighty four.

128
00:10:49.700 --> 00:10:56.600
So based off the data split that we did on training data versus testing data on average our model is

129
00:10:56.660 --> 00:11:00.560
off by about a little less than 1 star which isn't so bad.

130
00:11:00.590 --> 00:11:06.230
It means if you really liked the movie The model was able to correctly predict that you were more or

131
00:11:06.230 --> 00:11:08.750
less would like that movie within one star.

132
00:11:08.840 --> 00:11:13.930
And it's the same thing for if you really dislike the movie or just a movie was OK.

133
00:11:14.030 --> 00:11:21.640
OK so were able to implement our full recommendation system on a real cluster on the data bricks platform.

134
00:11:21.650 --> 00:11:27.110
Now as I mentioned the very beginning of this section of the Course recommender systems by themselves

135
00:11:27.110 --> 00:11:31.130
are actually a very complex topic and they deal for a whole host of problems.

136
00:11:31.130 --> 00:11:34.390
The other machine learning algorithms don't usually have to deal with.

137
00:11:34.400 --> 00:11:41.330
And those can be things such as the cold start problem which is when you have a user that's never actually

138
00:11:41.840 --> 00:11:44.480
initiated with the platform that you're trying to recommend.

139
00:11:44.480 --> 00:11:49.970
So for instance if you're on Netflix a very new user who just signed up for the service for that movie

140
00:11:49.970 --> 00:11:55.280
service What movies do you recommend to them if they've never seen any movies on your platform and you

141
00:11:55.280 --> 00:11:56.570
have no information about them.

142
00:11:56.570 --> 00:12:01.770
Similarly if it's an AM if it's a customer's very first time on a web site such as Amazon.

143
00:12:01.890 --> 00:12:07.010
Well the products you recommend for this person to buy if you have no purchasing history even though

144
00:12:07.010 --> 00:12:12.800
browsing history that's a cold start problem you're have a cold start before you can actually grab any

145
00:12:12.800 --> 00:12:14.270
data from this person.

146
00:12:14.270 --> 00:12:17.050
Now there's a variety of ways to actually solve that problem.

147
00:12:17.120 --> 00:12:20.780
And one easy way is to recommend the top 10 most popular items.

148
00:12:20.810 --> 00:12:26.320
Overall two new people and then seeing what the data says from their.

149
00:12:26.410 --> 00:12:32.080
Now with all that being said I hope you realize just how powerful Skala and Sparke is in that we were

150
00:12:32.080 --> 00:12:38.350
able to basically do a full blown recommendation system of just a few lines of code on a cluster and

151
00:12:38.350 --> 00:12:44.710
it's fully parallelizable meaning you can grab the larger cluster off the data Brick's professional

152
00:12:44.710 --> 00:12:47.550
platform and deal with much larger data sets.

153
00:12:47.560 --> 00:12:53.080
And if you want to do that you can feel free to purchase or do the 14 day free trial of data bricks

154
00:12:53.320 --> 00:12:57.820
and then download the larger movie lens datasets and play around with this further.

155
00:12:58.100 --> 00:12:58.700
OK.

156
00:12:58.840 --> 00:13:01.440
If you have any questions feel free to post them to the Q&amp;A forum.

157
00:13:01.540 --> 00:13:06.760
But as a quick review of what we've done basically we just imported from Sparta and Mehldau recommendation.

158
00:13:06.870 --> 00:13:14.470
LS And then we said grab our data as a ratings and most importantly we do the training test split as

159
00:13:14.470 --> 00:13:17.120
well as set up this new HLS data set.

160
00:13:17.290 --> 00:13:22.300
And we had to set the user column the item column and the rating column and really measure of these

161
00:13:22.300 --> 00:13:28.240
two parameters and see if to even really make a difference off our average error on that reading.

162
00:13:28.240 --> 00:13:31.130
I'll leave that to you as a little exercise play around.

163
00:13:31.360 --> 00:13:32.200
OK.

164
00:13:32.260 --> 00:13:32.920
Thanks everyone.

165
00:13:32.920 --> 00:13:34.010
Hope you enjoyed the lecture.

166
00:13:34.050 --> 00:13:34.930
Then I'll see you at the next one.
