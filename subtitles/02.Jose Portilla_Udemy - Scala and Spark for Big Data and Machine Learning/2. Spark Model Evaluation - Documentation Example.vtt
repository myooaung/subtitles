WEBVTT
1
00:00:05.350 --> 00:00:09.010
Hello everyone and welcome back to the model evaluation section of the course.

2
00:00:09.080 --> 00:00:14.320
And in this lecture we're going to be discussing the documentation example and basically as documentation

3
00:00:14.320 --> 00:00:20.170
example shows you how to use those evaluator Parambrata builder and then that train validation split

4
00:00:20.230 --> 00:00:21.100
object.

5
00:00:21.100 --> 00:00:27.300
These are specialized objects that allow you to formally set up your train test split later on the course

6
00:00:27.310 --> 00:00:30.210
we actually won't be using such formal syntax.

7
00:00:30.250 --> 00:00:33.710
We'll just do a random split for training sets and test sets.

8
00:00:33.930 --> 00:00:39.700
But you should know how to formalize a parameter grid and train validation split in order to show you

9
00:00:39.710 --> 00:00:41.910
this more formal way of doing things.

10
00:00:42.000 --> 00:00:46.900
And going to jump to the documentation and show you how you can actually find the documentation examples

11
00:00:46.990 --> 00:00:51.070
and then we'll jump to the out and text editor and actually code out these examples.

12
00:00:51.070 --> 00:00:54.130
Let me jump to documentation now and explore it with you.

13
00:00:54.160 --> 00:00:54.460
All right.

14
00:00:54.460 --> 00:00:59.880
Here I am at spark that a patch to the north to find the documentation I'm inferring to come here go

15
00:00:59.890 --> 00:01:06.380
to the latest release some 2.0 version or 2.1 later on come here to programming guides M-L live for

16
00:01:06.380 --> 00:01:11.440
machine learning and where we're going to be looking at is the main guide and L-EB and their model selection

17
00:01:11.530 --> 00:01:17.310
and tuning and this is the section that I want you to basically browse through what this section does

18
00:01:17.320 --> 00:01:22.310
describes how to use M-L lives tooling for tuning machine learning algorithms.

19
00:01:22.360 --> 00:01:26.740
Things such as choosing what parameters are the correct choice and you can do that through things such

20
00:01:26.740 --> 00:01:30.520
as a parameter grid or later on will see also pipelines.

21
00:01:30.520 --> 00:01:36.970
And we want to also check out the built in train validation split tools in order to do a train test

22
00:01:36.970 --> 00:01:38.640
split on our model.

23
00:01:38.680 --> 00:01:42.400
So I encourage you to actually read all of this and just browse through it.

24
00:01:42.520 --> 00:01:48.490
But basically we're going to be doing is understanding how this M-L lib supports model selection through

25
00:01:48.490 --> 00:01:50.110
a train validation split.

26
00:01:50.110 --> 00:01:56.470
There's also a cross-validation model or tool you can use that's very similar to train validation split

27
00:01:56.590 --> 00:02:03.040
except instead of just doing two splits into a training set and a testing set it does a number of splits

28
00:02:03.040 --> 00:02:05.320
so it cross-validation of all the splits.

29
00:02:05.350 --> 00:02:09.190
It's a very simple similar concept to train validation splits.

30
00:02:09.190 --> 00:02:13.500
So once you understand one of them you pretty much understand how to use the other.

31
00:02:13.510 --> 00:02:18.550
Now this train validation split tool and you can click on this and open up in a new link if you want

32
00:02:18.550 --> 00:02:19.750
to explore more.

33
00:02:19.810 --> 00:02:26.050
Require a couple of items one that requires an estimate or some sort of algorithm or a pipeline object

34
00:02:26.050 --> 00:02:28.760
which will work with more later on in the future.

35
00:02:28.930 --> 00:02:33.570
And then it also requires a set of parameter maps and these are parameters to choose from and you are

36
00:02:33.570 --> 00:02:39.270
sometimes called a parameter grid to search over and then it also requires any Valuator object.

37
00:02:39.430 --> 00:02:44.980
And this is the object that's going to light have a metric to measure how well a fitted model does on

38
00:02:44.980 --> 00:02:47.030
some holdout test data set.

39
00:02:47.380 --> 00:02:47.940
OK.

40
00:02:48.220 --> 00:02:54.640
So the reason we're not going to be using train validation is split in this format is because of these

41
00:02:54.640 --> 00:02:59.650
three requirements it requires an estimator cram into maps and evaluators and a lot of times and we're

42
00:02:59.650 --> 00:03:01.910
just trying to learn how to use Skala and SPARC.

43
00:03:02.020 --> 00:03:05.180
We don't want to mess around for a whole parameter grid unfortunately.

44
00:03:05.200 --> 00:03:10.000
To use this train validation of split object you do need a set of parameter maps.

45
00:03:10.000 --> 00:03:14.350
Now that's not to say we're not going to be doing training and test splits later on in the course but

46
00:03:14.350 --> 00:03:20.410
later on the course we're actually going to simplify this by just using a simple random split on train

47
00:03:20.410 --> 00:03:21.210
test.

48
00:03:21.280 --> 00:03:27.430
What train validation split here does is it does the training test and the holdout data set which is

49
00:03:27.430 --> 00:03:28.130
a little different.

50
00:03:28.150 --> 00:03:33.370
As we discussed in the previous lecture but what I encourage you to do is just browse through this read

51
00:03:33.370 --> 00:03:36.210
through it read through cross-validation if you're interested in it.

52
00:03:36.250 --> 00:03:40.210
Basically just that training test split with more folds on your data.

53
00:03:40.300 --> 00:03:45.580
So you set some sort of k value there but scroll down until you come over here to train validations

54
00:03:45.580 --> 00:03:46.370
split.

55
00:03:46.390 --> 00:03:52.750
This is the actual coding example that we're going to code out and text editor and explain how the parameter

56
00:03:52.750 --> 00:03:58.090
grid the train validation split object the fitting the transformation etc. so you can find the full

57
00:03:58.090 --> 00:04:01.130
example code here in the documentation.

58
00:04:01.200 --> 00:04:05.020
But you can also find it in your folder under your zip download.

59
00:04:05.080 --> 00:04:09.690
I'm going to jump to the Adam text editor now and actually code out this example from scratch.

60
00:04:09.820 --> 00:04:11.390
So you can learn how to use it.

61
00:04:11.770 --> 00:04:13.610
Let's jump to Adam text editor.

62
00:04:13.950 --> 00:04:20.020
OK so here am I opened up Adam text editor and under the machine learning folder there's a folder called

63
00:04:20.050 --> 00:04:25.960
Motl underscore evaluation and they call the script we're going to be working through is this doc model

64
00:04:26.030 --> 00:04:31.470
evil acts and this is the documentation example for model evaluation.

65
00:04:31.480 --> 00:04:36.430
Now instead of just walking through this I'm actually going to code it out so you can get a better idea

66
00:04:36.430 --> 00:04:38.840
of how this whole actually works.

67
00:04:38.920 --> 00:04:39.760
So let's.

68
00:04:39.760 --> 00:04:49.540
Right click here and say new file and I'm going to just type in X Skala enter and then start typing

69
00:04:49.540 --> 00:04:50.810
stuff out.

70
00:04:50.830 --> 00:04:51.090
All right.

71
00:04:51.100 --> 00:05:00.490
First thing I want to do is are imports and what we have to do is import org Apache Sparke M-L and then

72
00:05:00.490 --> 00:05:05.530
we need an evaluator in order to use the parameter grid builder in the train validation object.

73
00:05:05.740 --> 00:05:12.980
We need evaluator we need evaluators So we want to say from Mehldau evaluation dot and then the pest

74
00:05:13.060 --> 00:05:17.740
depending on whether using classification algorithm regression algorithm etc. you're going to need a

75
00:05:17.740 --> 00:05:19.500
specific evaluator.

76
00:05:19.630 --> 00:05:22.470
Are examples going to use the regression evaluator.

77
00:05:22.540 --> 00:05:27.880
Since we're going to show how to do all these things on a regression task since we just learn about

78
00:05:27.880 --> 00:05:33.450
regression with Skala and Sparke then we need to actually import the model itself.

79
00:05:33.520 --> 00:05:43.100
Say import or Apache that SPARC M-L and then from the regression family will say linear regression.

80
00:05:43.420 --> 00:05:48.180
And finally we need to actually import the parameter grid build there from the train validation split

81
00:05:48.190 --> 00:05:49.930
objects.

82
00:05:50.050 --> 00:06:00.850
So I'm going to say import or Petchey that spark M-L tuning loops tuning and then some importing multiple

83
00:06:00.850 --> 00:06:01.330
things.

84
00:06:01.330 --> 00:06:03.590
I can just use these curly brackets.

85
00:06:03.670 --> 00:06:09.860
I want Paramo great builder and then I also want the train validation split object.

86
00:06:10.000 --> 00:06:10.570
OK.

87
00:06:11.140 --> 00:06:13.090
So so far that's what I have.

88
00:06:13.090 --> 00:06:17.080
Up next I want to prepared the training and test data in this case.

89
00:06:17.080 --> 00:06:23.670
We're essentially going to redo the examples to show in the regression section in this particular documentation

90
00:06:23.670 --> 00:06:24.280
example.

91
00:06:24.340 --> 00:06:32.140
I'm going to show the documentation sample linear regression data text so I will just say Val data is

92
00:06:32.140 --> 00:06:41.710
equal to spark Hoopes spark the read format and then I'm going to be using this lib SVM data and you

93
00:06:41.710 --> 00:06:44.890
can use the data that's in the regression folder.

94
00:06:44.890 --> 00:06:49.360
So if you go back to the regression folder remembered that you had this sample underscore at linear

95
00:06:49.360 --> 00:06:51.380
regression underscore data that text.

96
00:06:51.520 --> 00:06:53.850
That's exactly the same text file We're going to be working with.

97
00:06:53.860 --> 00:06:59.030
So you've actually already done this before going back to X Scala.

98
00:06:59.260 --> 00:07:06.370
I will say load and then I'll just type and sample underscore linear regression underscore data dot

99
00:07:06.790 --> 00:07:11.690
cxi and then I want to split that up.

100
00:07:11.690 --> 00:07:14.180
So I want to prepare the training and test data.

101
00:07:14.390 --> 00:07:19.550
So this is our train test splits and that looks like this.

102
00:07:19.550 --> 00:07:29.120
I say Val and I create an array here and I'm basically going to be unpacking the results into an array

103
00:07:29.480 --> 00:07:32.980
where one item is the training set and the other item is a test set.

104
00:07:33.500 --> 00:07:41.060
And then I just call my data and offer this data Frank object I can call random split so you can see

105
00:07:41.060 --> 00:07:46.760
that data frames in Skala and Sparke really kept in mind when you're dealing with machine learning so

106
00:07:46.760 --> 00:07:51.600
much so that a random split method is actually built into these data frames so I say data that random

107
00:07:51.650 --> 00:07:54.070
split noticed something as capitalized.

108
00:07:54.380 --> 00:08:01.710
And then as arguments here are parameters I an array and this array is going to have two numbers in

109
00:08:01.710 --> 00:08:04.250
it as essentially percentages.

110
00:08:04.320 --> 00:08:07.540
The percent I want for training and the I want for test.

111
00:08:07.650 --> 00:08:11.810
So I'm going to do a 0.9 0.1 train test split.

112
00:08:11.820 --> 00:08:16.590
Now what this basically means is that 90 percent of my data is going to be training data and 10 percent

113
00:08:16.590 --> 00:08:21.090
of my data is going to be test data and you should make sure that those two numbers essentially add

114
00:08:21.090 --> 00:08:22.400
up to one.

115
00:08:22.570 --> 00:08:25.990
And if you want as an additional argument you can set a seed.

116
00:08:26.040 --> 00:08:27.930
So this is just a random seed value.

117
00:08:27.930 --> 00:08:32.910
So for instance I can just put one two three four five or really any integer number you want and that

118
00:08:32.910 --> 00:08:36.000
essentially sets a random seed for this random split.

119
00:08:36.030 --> 00:08:39.610
Remember since this split is happening random for different rows.

120
00:08:39.720 --> 00:08:42.030
If you want your results to match mine exactly.

121
00:08:42.090 --> 00:08:47.650
You may want to provide an actual seed if I happened to provide a seed number for you to follow.

122
00:08:47.700 --> 00:08:51.750
Usually it's not necessary to provide a seed but it's really up to you the pending on whether you're

123
00:08:51.750 --> 00:08:56.220
going to be passing this code along to colleagues and they'll be nice to compare results on the exact

124
00:08:56.220 --> 00:08:57.770
same random split of data.

125
00:08:59.240 --> 00:09:03.670
All right let's just explore the state so we can say data print schema.

126
00:09:03.860 --> 00:09:10.300
I'm going to save this open up a terminal and then it will open up Sparke cells.

127
00:09:10.310 --> 00:09:14.590
I will say Sparke bash shell and hit enter.

128
00:09:14.600 --> 00:09:17.180
Load that up.

129
00:09:17.420 --> 00:09:18.860
OK so that slowed it up.

130
00:09:18.860 --> 00:09:20.270
Now let's load that file.

131
00:09:20.270 --> 00:09:25.160
I'll say call and load and then start Skala enter.

132
00:09:25.200 --> 00:09:28.740
And we should see the scheme a printout.

133
00:09:28.740 --> 00:09:34.370
All right so the schema printed out and notice that we have it in our classic format label and features.

134
00:09:34.380 --> 00:09:35.840
Now this is the example text data.

135
00:09:35.850 --> 00:09:38.610
So it's already conveniently in that format.

136
00:09:39.060 --> 00:09:40.390
So let's delete this.

137
00:09:40.710 --> 00:09:46.830
And now let's show you how to actually set up the parameter grid so we can use parameter grid builder

138
00:09:46.830 --> 00:09:51.090
or program grid builder to construct a grid of parameters to search over.

139
00:09:51.090 --> 00:09:56.940
And in this case we're going to add a grid of parameters for the linear regression object and train

140
00:09:56.940 --> 00:10:02.760
validation split is going to try all the combinations of values and determine the best model using the

141
00:10:02.760 --> 00:10:05.150
evaluator that we imported.

142
00:10:05.220 --> 00:10:13.920
So I will say Velle Parama grid is equal to a new program great builder object.

143
00:10:14.800 --> 00:10:24.520
And then I say add a grid and I passen a linear regression model so LR In order to do that let's actually

144
00:10:24.520 --> 00:10:25.610
build out that model.

145
00:10:25.690 --> 00:10:34.210
So after the train split going to add a new line here I will say Val L-R is equal to a new linear regression

146
00:10:34.240 --> 00:10:36.730
object so has this new linear regression object.

147
00:10:37.060 --> 00:10:40.930
And then I can actually add it to the grid the parameter.

148
00:10:40.930 --> 00:10:47.260
So I'm saying add grid L-R and then I call off L.R. ready program.

149
00:10:47.280 --> 00:10:49.860
So ready Grecian parameters.

150
00:10:50.040 --> 00:10:58.470
And in this case we can say something like array 0.1 and zero point zero one.

151
00:10:58.680 --> 00:11:03.030
And basically we're going to be doing is calling different parameters they can add to a linear regression

152
00:11:03.060 --> 00:11:03.750
object.

153
00:11:03.880 --> 00:11:08.590
And if you want an actual list of all the parameters you can add to a linear regression object.

154
00:11:08.760 --> 00:11:13.750
Basically what I mean by that is figuring out well what codes can I call off of l r.

155
00:11:13.830 --> 00:11:16.080
You can check that out in the documentation.

156
00:11:16.080 --> 00:11:22.290
So what you do is you just say in a general sense add a grid call your model whatever you happen to

157
00:11:22.290 --> 00:11:22.840
call it.

158
00:11:22.850 --> 00:11:31.370
LR dot and then whatever parameter you want to say so in this case we can also do something like intercept.

159
00:11:33.110 --> 00:11:35.970
Or you could say something like add a grid.

160
00:11:35.990 --> 00:11:42.810
And these are just the exact same choices for parameters done in the actual documentation example.

161
00:11:43.040 --> 00:11:51.040
So for instance L-R dot elastic net program and I can passen an array of value here.

162
00:11:51.540 --> 00:11:52.770
So I will say array.

163
00:11:52.800 --> 00:11:55.570
And there's no easy way to choose the actual value.

164
00:11:55.770 --> 00:12:01.250
The For instance in the documentation example they choose three values here.

165
00:12:01.290 --> 00:12:09.810
It will say zero point zero for the elastic net parameter 0.5 and then 1.0 and then at the end of all

166
00:12:09.810 --> 00:12:13.970
this you actually build this.

167
00:12:13.970 --> 00:12:17.160
So let's actually make sure this all works.

168
00:12:17.160 --> 00:12:22.450
I'm going to save this and run this file again so the X that Skala.

169
00:12:22.630 --> 00:12:23.350
And notice here.

170
00:12:23.350 --> 00:12:26.570
Now I have this parameter grid so I can see here.

171
00:12:26.740 --> 00:12:32.590
And the reason I had just intercept with no array is because the intercept is an actual boolean.

172
00:12:32.590 --> 00:12:33.730
So it's true.

173
00:12:33.880 --> 00:12:40.310
Or if you scroll up here vs we can see less Cygnet actually has these numbers.

174
00:12:40.560 --> 00:12:43.040
So if I wanted to fit the intercept I'd have to add that in.

175
00:12:43.050 --> 00:12:47.610
But it doesn't take an array because we're not searching for an array of actual values.

176
00:12:47.610 --> 00:12:55.560
Now if I type LR here dots and then hit tab I should be able to see the actual some of the actual parameters

177
00:12:55.560 --> 00:12:56.310
here.

178
00:12:56.310 --> 00:13:02.760
So for example there's an elastic net program I can explain Paramor extract the parameter map and I

179
00:13:02.760 --> 00:13:06.720
can do set feature columns that fit in their separate label column.

180
00:13:06.720 --> 00:13:13.440
Max adorations prediction column the regulates regularisation parameters for things such as Lasser regularization

181
00:13:13.880 --> 00:13:14.580
etc..

182
00:13:14.730 --> 00:13:19.380
So if you really want to know all of this you really have to just take the time and read through the

183
00:13:19.380 --> 00:13:20.300
documentation.

184
00:13:20.310 --> 00:13:24.850
But right now we'll just keep things simple by following along with the documentation example.

185
00:13:25.020 --> 00:13:31.500
But the basic premise of this is you say new program great builder and you say Dot add grid call your

186
00:13:31.500 --> 00:13:36.900
model call whatever parameter you want and if you actually want to explore everything that's available

187
00:13:36.900 --> 00:13:40.800
to you you can just say L.R. dot tab and you should see a bunch of options.

188
00:13:40.800 --> 00:13:43.430
Now keep in mind that all of these are parameters.

189
00:13:43.440 --> 00:13:46.300
A lot of them are just methods such as Save or set.

190
00:13:46.380 --> 00:13:51.210
But a lot of these are parameters and you should be able to just check the documentation and see which

191
00:13:51.210 --> 00:13:52.310
ones are parameters.

192
00:13:53.220 --> 00:13:58.410
And then if it's a parameter it takes in an American value you set an array of the different values

193
00:13:58.410 --> 00:14:00.520
you want to test over is the actual grid.

194
00:14:00.750 --> 00:14:06.100
And then what it ends up doing is if we check this out again this grid site I'm going to type here.

195
00:14:06.210 --> 00:14:09.450
Program grid enter.

196
00:14:09.510 --> 00:14:12.160
It essentially makes a grid of all the possible combinations.

197
00:14:12.160 --> 00:14:17.980
So this is 1.0 true or 0.1 0.5 True's 3.0 1 etc..

198
00:14:18.090 --> 00:14:22.800
So we have all these possible combinations and eventually just say dot dot dot because it doesn't want

199
00:14:22.800 --> 00:14:24.280
to put out the entire grid there.

200
00:14:24.480 --> 00:14:27.410
But that's how we perform grid object works.

201
00:14:27.480 --> 00:14:31.310
Then up next you can just run the train validation split.

202
00:14:31.710 --> 00:14:45.480
So here WIPs back in our script I'm going to say Val train revalidation split is equal to a new train

203
00:14:45.500 --> 00:14:55.410
validation split object and then I will say set estimator an estimator we actually set is the model

204
00:14:55.410 --> 00:14:55.830
itself.

205
00:14:55.830 --> 00:14:59.540
So the estimators just another way of saying machine learning algorithm.

206
00:14:59.550 --> 00:15:02.230
So in this case I just pasan L-R.

207
00:15:02.450 --> 00:15:08.180
And then I need to set my evaluator and I do that with set evaluator.

208
00:15:08.180 --> 00:15:11.320
So I have set the estimator and I need to set the evaluator.

209
00:15:11.330 --> 00:15:16.360
Since Max role model is a regression model and we're going to need to set the regression evaluator.

210
00:15:16.520 --> 00:15:18.310
And in this case I can just call it here.

211
00:15:18.320 --> 00:15:29.720
New regression evaluator and then off of this I can say set and make sure I actually close Princie here.

212
00:15:31.310 --> 00:15:36.510
And here I can say set estimator program maps.

213
00:15:36.580 --> 00:15:39.010
So this is the parameter mappings I want to set.

214
00:15:39.040 --> 00:15:43.750
I can just say Parama grid which was the one we just created.

215
00:15:43.750 --> 00:15:53.010
And then finally for the training set test set versus the whole that set I say set train ratio.

216
00:15:53.140 --> 00:15:56.680
And in this case will fall the documentation and put 0.8.

217
00:15:56.680 --> 00:15:59.680
So this is what's going to allow us to do that holdout testing.

218
00:15:59.770 --> 00:16:01.930
So that train validation is split object.

219
00:16:01.930 --> 00:16:05.020
So I call a new training validation split object.

220
00:16:05.020 --> 00:16:10.660
I set the estimator basically set the actual model set the evaluator and it needs to line up with a

221
00:16:10.660 --> 00:16:12.360
type of model you're using.

222
00:16:12.370 --> 00:16:18.590
So regression model calls for a regression evaluator classification is a classification evaluator etc..

223
00:16:18.700 --> 00:16:24.630
Then I set the estimator parameter grid so set S-meter pram mappings and then I set the train ratio

224
00:16:24.730 --> 00:16:28.600
which is basically what are you going to use for training and testing versus what you're going to use

225
00:16:28.600 --> 00:16:30.340
for the holdout set.

226
00:16:30.340 --> 00:16:30.810
All right.

227
00:16:31.060 --> 00:16:36.610
And then once we have that we can actually run the train validation split and it will automatically

228
00:16:36.610 --> 00:16:39.110
choose the best set of parameters.

229
00:16:39.490 --> 00:16:46.570
And this is going to look really similar to a normal testing so say model train of validation splits

230
00:16:47.830 --> 00:16:55.490
that fit and then pass the training set and then finally we can make predictions on the test data.

231
00:16:55.520 --> 00:16:59.780
So the model is that models the combination parameters that perform the best.

232
00:16:59.810 --> 00:17:13.170
So I will say model the transform the test and then I can select off these results the features the

233
00:17:13.180 --> 00:17:20.920
label and then the prediction and then finally I'm going to just show all that and we can explore a

234
00:17:20.920 --> 00:17:25.120
lot of these objects later on in the actual Sparke show.

235
00:17:25.330 --> 00:17:26.570
So let me save that.

236
00:17:26.990 --> 00:17:30.000
Let's load it up and run it and make sure we typed in everything correctly.

237
00:17:32.950 --> 00:17:34.140
Right so that finished running.

238
00:17:34.150 --> 00:17:37.900
Let me expand on this and actually show it to you.

239
00:17:37.900 --> 00:17:41.890
So here we can see the features label and prediction.

240
00:17:41.890 --> 00:17:46.360
And this looks really similar to what we've seen in the past except now we're running it with a parameter

241
00:17:46.360 --> 00:17:49.520
grid where the model is the actual best fit.

242
00:17:49.780 --> 00:17:57.520
So if I say model dots and then hit tab I can see that I can actually call the best model so I can say

243
00:17:58.710 --> 00:18:05.990
best model eps and this is going to basically return back a linear regression object that represents

244
00:18:05.990 --> 00:18:08.780
that best model in case I ever want to do stuff with it.

245
00:18:08.900 --> 00:18:14.700
And then I can get things to extract a parameter mapping get the estimator itself get the estimated

246
00:18:14.720 --> 00:18:15.640
parameter mapping.

247
00:18:15.640 --> 00:18:24.580
So things such as model estimator Coram maps.

248
00:18:24.590 --> 00:18:27.890
And this is going to return back the actual parameter mapping that it was trained with.

249
00:18:27.890 --> 00:18:31.640
In case you ever want to see that information back straight from the model.

250
00:18:31.970 --> 00:18:32.680
All right.

251
00:18:32.780 --> 00:18:34.630
So I know we went through a lot here.

252
00:18:34.670 --> 00:18:39.280
And again I really recommend that you take the time to read the documentation page so we first showed.

253
00:18:39.430 --> 00:18:45.140
But let me break down everything we just did in this documentation example and then we're going to revisit

254
00:18:45.200 --> 00:18:51.920
the housing data again following the formatting of parameter grid and train validation split.

255
00:18:51.920 --> 00:18:57.500
So you have to do is you need basically three main things you need some sort of evaluator that you grab

256
00:18:57.500 --> 00:18:58.780
from revaluation.

257
00:18:58.850 --> 00:19:02.930
You need some sort of model which in this case we're grabbing it from regression but it can be from

258
00:19:02.960 --> 00:19:04.920
anything else such as classification.

259
00:19:05.000 --> 00:19:09.710
And then you need these tuning objects which is your parameter great builder and your train validation

260
00:19:09.710 --> 00:19:10.730
split.

261
00:19:10.730 --> 00:19:16.010
You read in your data you do a random split on your data to get it into a training set and a testing

262
00:19:16.010 --> 00:19:16.610
set.

263
00:19:16.800 --> 00:19:20.430
And in most cases throughout the course we're actually just going to stop here.

264
00:19:20.480 --> 00:19:25.070
So you'll notice in the classification section this is where we will just stop and we'll go straight

265
00:19:25.070 --> 00:19:27.870
into actually train the model.

266
00:19:27.900 --> 00:19:32.460
But if you want to continue or something such as the hold out data set this is where Paramo grade builder

267
00:19:32.460 --> 00:19:34.710
and train validation split come into play.

268
00:19:34.710 --> 00:19:40.410
You can call a linear regression object and then you call a Parambrata builder and then you add grids

269
00:19:40.410 --> 00:19:41.900
to it and the grids.

270
00:19:41.940 --> 00:19:47.070
It's just a parameter and the pending what type information that parameter gets in array of numerical

271
00:19:47.070 --> 00:19:52.560
values or to actually just say something like Yes fit the entire set for this linear regression model

272
00:19:52.920 --> 00:19:53.570
etc..

273
00:19:53.670 --> 00:19:58.560
And if you want to know more about these for the theoretical understanding and the statistical information

274
00:19:58.920 --> 00:20:03.670
I suggest you do the reading in introduction to testicle learning on these specific algorithms.

275
00:20:03.810 --> 00:20:08.520
And if you want the actual parameters or are available to you then you have to basically just look up

276
00:20:08.520 --> 00:20:11.910
this model in the documentation and see what parameters are available to you.

277
00:20:11.970 --> 00:20:17.700
Or you can click tab as I showed earlier in the spark shell and see the options yourself and then you

278
00:20:17.700 --> 00:20:23.160
need to say train validation split so you create a new train validation split object you pass in your

279
00:20:23.160 --> 00:20:23.770
s later.

280
00:20:23.780 --> 00:20:25.970
Remember the estimators just the actual model.

281
00:20:25.980 --> 00:20:30.750
Then you set your evaluator remember the evaluator needs to match the type of model or you type of estimator

282
00:20:30.750 --> 00:20:31.700
you're using.

283
00:20:31.740 --> 00:20:37.740
And then after that you pass in the estimator parameter mapping which is just the grids we created before.

284
00:20:37.740 --> 00:20:42.990
And then finally you set the train ratio of your training and testing set versus the actual hold out

285
00:20:42.990 --> 00:20:43.710
data set.

286
00:20:44.550 --> 00:20:49.990
Then finally you take your model and run train invalidations split as if it was just a normal inner

287
00:20:49.990 --> 00:20:51.000
regression object.

288
00:20:51.060 --> 00:20:55.530
You fit it to your training data and then you transform your test data and you can grab information

289
00:20:55.530 --> 00:21:00.060
off of it such as the features the label in the prediction and show it or you can say this to another

290
00:21:00.090 --> 00:21:02.700
object and explore it that way.

291
00:21:02.700 --> 00:21:03.420
All right.

292
00:21:03.420 --> 00:21:04.650
Hope you enjoyed this lecture.

293
00:21:04.650 --> 00:21:05.640
I know it's quite a lot.

294
00:21:05.640 --> 00:21:08.650
And I know it's a little complicated skull spark.

295
00:21:08.760 --> 00:21:14.900
Basically you're paying a price for its ability to quickly be paralyzed two more computers later on

296
00:21:14.900 --> 00:21:18.310
when we show you things such as working at this on Amazon Web Services.

297
00:21:18.390 --> 00:21:23.100
But it is worth the extra effort of looking stuff in the documentation because we're essentially just

298
00:21:23.100 --> 00:21:24.680
a few lines of code here.

299
00:21:24.690 --> 00:21:29.040
We've made something that can be run on a multiple computer system.

300
00:21:29.090 --> 00:21:29.570
OK.

301
00:21:29.880 --> 00:21:30.960
Thanks Ron.

302
00:21:30.960 --> 00:21:32.190
And I will see at the next lecture.
