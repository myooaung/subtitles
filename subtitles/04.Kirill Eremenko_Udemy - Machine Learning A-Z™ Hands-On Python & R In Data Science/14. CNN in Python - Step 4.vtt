WEBVTT
1
00:00:00.360 --> 00:00:01.140
Hello, my friends.

2
00:00:01.200 --> 00:00:06.300
Welcome back to this implementation and mostly welcome to Part three, training the CNN.

3
00:00:06.770 --> 00:00:11.680
In the previous to Tokyo, we built the brain which contained the eyes of the A.I..

4
00:00:11.730 --> 00:00:13.830
You know, thanks to the convolutional layers.

5
00:00:14.160 --> 00:00:21.120
And now we're gonna make that brains smart with that training of CNN on all our training set images.

6
00:00:21.450 --> 00:00:26.880
And at the same time, as you see, we're going to evaluate our same model on the test, set over the

7
00:00:26.940 --> 00:00:27.360
epics.

8
00:00:27.480 --> 00:00:30.610
You know, we're going to train our CNN over 25 bucks.

9
00:00:30.850 --> 00:00:36.840
And at each book, we'll actually see how our model is performing on our test set images.

10
00:00:37.260 --> 00:00:42.240
So this is a different kind of training as what we had before, because we always used to separate the

11
00:00:42.240 --> 00:00:43.830
training and the evaluation.

12
00:00:44.190 --> 00:00:46.260
But here, this will happen at the same time.

13
00:00:46.290 --> 00:00:50.310
That's because we're doing some specific application, which is computer vision.

14
00:00:51.000 --> 00:00:52.290
All right, I'm ready.

15
00:00:52.380 --> 00:00:53.190
Let's do this.

16
00:00:53.220 --> 00:00:57.020
Let's start with the first step, compiling the CNN.

17
00:00:57.630 --> 00:00:57.870
All right.

18
00:00:57.930 --> 00:01:03.870
So in a new code cell, we're going to compile the CNN, meaning we're going to connect it to an optimizer,

19
00:01:04.030 --> 00:01:06.870
a lost function and some metrics.

20
00:01:07.620 --> 00:01:11.670
And, well, you know, here once again, we're doing binary classification.

21
00:01:11.820 --> 00:01:19.450
And so very simply, we're going to compile our CNN exactly the same way as how we compiled our A and

22
00:01:19.460 --> 00:01:24.120
then in the previous section, because indeed, we're going to choose once again, an item optimizer

23
00:01:24.180 --> 00:01:29.610
to, you know, perform stochastic gradient descent, update the weights in order to reduce the loss

24
00:01:29.760 --> 00:01:32.870
error between the predictions and the target.

25
00:01:33.420 --> 00:01:35.650
Then we're going to choose the same loss.

26
00:01:35.790 --> 00:01:41.550
You know, the binary cross entropy loss once again, because we're doing exactly the same task, binary

27
00:01:41.550 --> 00:01:42.390
classification.

28
00:01:42.930 --> 00:01:44.540
And then same for the metrics.

29
00:01:44.550 --> 00:01:50.010
We're going to choose the accuracy metrics, because that's, you know, the most relevant way to measure

30
00:01:50.010 --> 00:01:54.480
the performance of a classification model, which is exactly the case of our CNN.

31
00:01:55.110 --> 00:01:59.640
And therefore, to compile the CNN, well, it's gonna be a piece of cake because indeed, we're going

32
00:01:59.640 --> 00:02:01.650
to do exactly the same as before.

33
00:02:01.830 --> 00:02:09.960
So we're gonna start by taking our CNN from which we're gonna call the compile method, which will take

34
00:02:10.110 --> 00:02:10.880
as input.

35
00:02:11.070 --> 00:02:17.430
Well, first, our optimizer, which will choose to be the atom optimizer.

36
00:02:18.090 --> 00:02:25.860
Then the last function, which will choose to be the binary cross entropy.

37
00:02:26.640 --> 00:02:27.210
All right.

38
00:02:27.570 --> 00:02:32.160
And finally, final argument, the metrics we will choose only one.

39
00:02:32.190 --> 00:02:34.920
But remember, we have the choice to take several of them.

40
00:02:35.250 --> 00:02:36.720
But just the accuracy is fine.

41
00:02:36.720 --> 00:02:40.740
And therefore, in these pair of square brackets, we're going to input in quotes.

42
00:02:40.950 --> 00:02:46.480
Well, that accuracy and done this compound successfully this year.

43
00:02:46.480 --> 00:02:49.290
CNN to the optimizer, the last function and the metric.

44
00:02:49.710 --> 00:02:51.420
So exactly the same as before.

45
00:02:51.720 --> 00:02:58.470
However, now to train CNN on the training set and evaluating it at the same time on the test set,

46
00:02:58.920 --> 00:03:01.150
well, it will not be exactly the same as before.

47
00:03:01.170 --> 00:03:03.540
But once again, very, very similar.

48
00:03:03.810 --> 00:03:04.580
Let's check this out.

49
00:03:04.650 --> 00:03:06.140
Let's create a new code cell.

50
00:03:06.600 --> 00:03:09.580
And now you can actually guess the first two steps.

51
00:03:09.600 --> 00:03:10.710
There are always the same.

52
00:03:10.980 --> 00:03:14.770
The first step is to take, of course, our CNN from which.

53
00:03:14.880 --> 00:03:15.990
And that's the second step.

54
00:03:16.380 --> 00:03:18.540
What method do we need to call here?

55
00:03:18.720 --> 00:03:20.760
Well, once again, this never changes.

56
00:03:21.090 --> 00:03:23.610
This is, of course, the fit method.

57
00:03:23.970 --> 00:03:28.620
The fit method, which, as always, will train the CNN on the training set.

58
00:03:29.210 --> 00:03:29.510
OK.

59
00:03:29.760 --> 00:03:32.910
So this time, what are going to be the inputs?

60
00:03:33.420 --> 00:03:35.800
Well, the first input is always the same.

61
00:03:35.820 --> 00:03:37.720
It's going to be, of course, the set.

62
00:03:37.740 --> 00:03:41.640
You know, the data set on which you're going to train your model here to CNN.

63
00:03:42.000 --> 00:03:43.590
And that's, of course, the training set.

64
00:03:43.740 --> 00:03:50.270
And the name of the parameter for that is simply X and therefore will specify X to be, you know, are

65
00:03:50.280 --> 00:03:56.970
training set that exact same training set which we created in part one.

66
00:03:57.240 --> 00:04:04.230
You know, right here, that training set, that training set to which we've applied this image data

67
00:04:04.230 --> 00:04:07.500
generated two to indeed perform image augmentation.

68
00:04:08.100 --> 00:04:08.370
All right.

69
00:04:08.400 --> 00:04:13.440
That training, that's the first input of our parameter in this fit method.

70
00:04:13.950 --> 00:04:14.360
OK.

71
00:04:14.560 --> 00:04:16.080
Then next parameter.

72
00:04:16.560 --> 00:04:16.790
All right.

73
00:04:16.830 --> 00:04:22.110
And the next parameter is this time, you know, the difference, the difference with what we did before.

74
00:04:22.560 --> 00:04:26.940
So it has to do, of course, with the fact that we're not only training the CNN on the train set,

75
00:04:26.970 --> 00:04:30.750
but also evaluating it at the same time on the test set.

76
00:04:31.110 --> 00:04:34.050
And that second parameter corresponds exactly to this.

77
00:04:34.290 --> 00:04:37.830
We have to specify here the validation data.

78
00:04:38.080 --> 00:04:39.220
That's the name of the parameter.

79
00:04:39.270 --> 00:04:44.330
But that's, of course, the set on which we want to evaluate our CNN.

80
00:04:44.570 --> 00:04:47.370
And that's, of course, a test set that will be the value of the parameter.

81
00:04:47.730 --> 00:04:54.040
But the name of that parameter is, as I just said, vally dacian underscore data.

82
00:04:54.810 --> 00:04:58.710
And this is equal, of course, to the test set.

83
00:04:58.890 --> 00:04:59.730
Once again, that.

84
00:04:59.990 --> 00:05:06.830
Exact same test set which we created here in part one when progressing to set this one to which, of

85
00:05:06.830 --> 00:05:10.820
course, no transformation was applied, only feature scaling.

86
00:05:11.430 --> 00:05:12.650
OK, is a test set.

87
00:05:13.100 --> 00:05:13.640
Good.

88
00:05:14.000 --> 00:05:19.010
And now we have one final argument, which is, of course, you can totally guess what it is.

89
00:05:19.220 --> 00:05:19.480
Right.

90
00:05:19.490 --> 00:05:23.840
This is the inevitable argument when training a deep neural network.

91
00:05:24.410 --> 00:05:31.370
I'm talking, of course, about the Epic's parameter, which is a number of epochs and well, you know,

92
00:05:31.730 --> 00:05:34.250
to justify to you which number I chose.

93
00:05:34.550 --> 00:05:40.940
Well, I actually started with 10 bucks and I noticed that the accuracy was not converging.

94
00:05:41.270 --> 00:05:46.010
So then I tried with 50 netbooks, because you're going to see that one book is actually pretty slow.

95
00:05:46.020 --> 00:05:47.360
You know, it's actually a bit long.

96
00:05:47.480 --> 00:05:51.770
I mean, much longer than the epics and the training of our previous neural network, you know, the

97
00:05:51.770 --> 00:05:52.210
Eynon.

98
00:05:52.760 --> 00:05:53.810
So I started with 10.

99
00:05:53.860 --> 00:05:54.530
It was not enough.

100
00:05:54.560 --> 00:05:55.190
Then fifteen.

101
00:05:55.190 --> 00:05:57.200
Still not enough, you know, still not converging.

102
00:05:57.410 --> 00:06:00.320
And then 25 and 25 was perfect.

103
00:06:00.800 --> 00:06:06.380
I had an accuracy that pretty much converged not only on the training set, but also on the test set.

104
00:06:06.380 --> 00:06:06.710
You'll see.

105
00:06:07.160 --> 00:06:10.000
So for the number of ebooks here, we're going to choose 25.

106
00:06:10.340 --> 00:06:12.080
Feel free to increase it if you have time.

107
00:06:12.080 --> 00:06:17.740
You know, if you want to let your computer run for now or more with 25 bucks here, it will be fine.

108
00:06:17.760 --> 00:06:19.820
It we'll just take ten to fifteen minutes.

109
00:06:20.060 --> 00:06:20.930
So that will be all good.

110
00:06:20.960 --> 00:06:23.090
We'll get our results pretty fast.

111
00:06:23.750 --> 00:06:24.100
All right.

112
00:06:24.320 --> 00:06:25.580
Well, actually, that's it.

113
00:06:25.790 --> 00:06:29.750
That's only what we need to train our CNN on a training set.

114
00:06:29.780 --> 00:06:35.640
Well, evaluating it on the tests, it so perfect, we smashed by three.

115
00:06:35.780 --> 00:06:41.900
And we can now already move on to point four where we're going to make our single prediction, which

116
00:06:41.960 --> 00:06:49.860
I remind will consist of deploying our moral on the two images of this single prediction for.

117
00:06:50.450 --> 00:06:54.720
This one for which, of course, our model will have to recognize that there is a dog.

118
00:06:55.160 --> 00:06:58.790
And this one where our model will have to recognize there is a cat.

119
00:06:59.240 --> 00:07:00.730
So let us hope that it is right.

120
00:07:00.980 --> 00:07:02.390
But we won't have the predictions.

121
00:07:02.390 --> 00:07:03.280
And next story all.

122
00:07:03.280 --> 00:07:09.170
We will have them in the one after that, because remember, we're going to run our implementation from

123
00:07:09.380 --> 00:07:11.930
Juber notebook because we conjugating Google Kulab.

124
00:07:11.930 --> 00:07:14.440
The data set is to be all right.

125
00:07:14.570 --> 00:07:18.830
So well, as soon as you're ready for part four, let's do this together.

126
00:07:19.100 --> 00:07:21.080
And until then, enjoy machine learning.
