WEBVTT
1
00:00:00.300 --> 00:00:02.290
Hello and welcome back to the course and deep learning.

2
00:00:02.430 --> 00:00:07.980
Today we're talking about the neuron which is the basic building block of artificial neural networks.

3
00:00:08.010 --> 00:00:09.390
So let's get started.

4
00:00:09.390 --> 00:00:11.340
Previously we saw an image which looked like this.

5
00:00:11.340 --> 00:00:18.300
And these are actual real life neurons which have been smeared onto a gloss and color a little bit and

6
00:00:18.360 --> 00:00:19.950
they are observed through a microscope.

7
00:00:19.950 --> 00:00:22.140
So this is what they look like as you can see.

8
00:00:22.140 --> 00:00:29.730
Quite an interesting structure a body and then lots of different tails kind of branches coming out of

9
00:00:29.730 --> 00:00:30.250
them.

10
00:00:30.330 --> 00:00:32.350
And this is very interesting.

11
00:00:32.370 --> 00:00:38.400
But the question is how can we recreate that in a machine because we really need to recreate it and

12
00:00:38.400 --> 00:00:47.610
machine since the whole purpose of deep learning is to mimic how the human brain works in the hope that

13
00:00:47.820 --> 00:00:51.000
by doing so we're going to create something amazing.

14
00:00:51.000 --> 00:00:55.200
We're going to create an amazing infrastructure for machines to be able to learn.

15
00:00:55.230 --> 00:00:56.800
And why do we hope for that.

16
00:00:56.820 --> 00:01:03.480
Well because the human brain is well just happens to be one of the most powerful learning learning tools

17
00:01:03.880 --> 00:01:07.300
on the planet or like learning mechanisms on the planet.

18
00:01:07.320 --> 00:01:11.310
And we just hope that if we recreate that we'll have something as awesome as that.

19
00:01:11.310 --> 00:01:17.670
So our challenge right now our very first step to creating artificial neural networks is to recreate

20
00:01:17.700 --> 00:01:18.380
a neuron.

21
00:01:18.390 --> 00:01:19.090
So how do we do it.

22
00:01:19.110 --> 00:01:23.840
Well first of all let's have a closer look at what it actually is.

23
00:01:23.880 --> 00:01:33.180
This image was first created by a Spanish neuroscientist and Chagga Ramon Yi Kajal in 1899.

24
00:01:33.180 --> 00:01:37.780
And what he did was he died in neurons in actual brain tissue.

25
00:01:37.780 --> 00:01:39.850
And look at them under a microscope.

26
00:01:39.900 --> 00:01:43.530
And while he was looking at them he actually drew what he saw and this is what he saw.

27
00:01:43.530 --> 00:01:49.560
He saw it to your hands or two large neurons over there at the top which had all these branches coming

28
00:01:49.560 --> 00:01:57.930
off of them towards their top parts and then each one had this Araud or like thread coming out towards

29
00:01:57.930 --> 00:01:59.410
the bottom very long one.

30
00:01:59.520 --> 00:02:01.510
And so that's what he saw.

31
00:02:01.660 --> 00:02:07.800
And now you know technology has advanced quite a lot and we have seen neurons much closer in more detail

32
00:02:07.800 --> 00:02:11.890
and now we can actually draw what it looks like diagrammatic.

33
00:02:11.910 --> 00:02:13.220
So let's have a look at that.

34
00:02:13.440 --> 00:02:14.190
Here's a neuron.

35
00:02:14.190 --> 00:02:21.810
This is what it looks like very similar to what Santiago around one drew over here and here and this

36
00:02:21.810 --> 00:02:24.310
year and what we can see is that it's got a body.

37
00:02:24.570 --> 00:02:29.100
That's the main part of the neuron and then it's got some branches at the top which is called dendrites

38
00:02:29.160 --> 00:02:33.200
and it's also got an X on which is that long tail of the euro.

39
00:02:33.300 --> 00:02:38.030
And so what are these dendrites and when foreign was the axen for well.

40
00:02:38.130 --> 00:02:44.040
The key point to understand here is that neurons by themselves are pretty much useless.

41
00:02:44.040 --> 00:02:45.570
It's like it's like an ant.

42
00:02:45.600 --> 00:02:46.140
Right.

43
00:02:46.170 --> 00:02:49.640
And on its own can do my psych five ads together.

44
00:02:49.830 --> 00:02:51.170
Maybe they can pick something up.

45
00:02:51.190 --> 00:02:55.830
But again they they don't they can build an anthill or they call them establish a colony that can't

46
00:02:56.430 --> 00:02:59.340
work together as a as a huge organism.

47
00:02:59.370 --> 00:03:03.510
But at the same time when you have lots and lots of ads like you have in a million and they can build

48
00:03:03.510 --> 00:03:05.680
a whole colony they can build an anthill.

49
00:03:05.680 --> 00:03:06.600
Same thing with neurons.

50
00:03:06.600 --> 00:03:12.320
By itself it's not that strong but when you have lots of neurons together they work together to do magic.

51
00:03:12.510 --> 00:03:13.820
And how do they work together.

52
00:03:13.820 --> 00:03:14.430
That's the question.

53
00:03:14.440 --> 00:03:19.140
Well that's what the dendrites and Aksenov for so the dendrites are kind of like the receivers of the

54
00:03:19.140 --> 00:03:22.980
signal for the neuron and the axon is the transmitter of the signal for the neuron.

55
00:03:23.220 --> 00:03:26.520
And here's an image of how it all works conceptually.

56
00:03:26.520 --> 00:03:32.550
So at the top you've got on your own and you can see that it's dendrites are connected to axons of other

57
00:03:32.550 --> 00:03:35.990
neurons that are like even further away above it.

58
00:03:36.000 --> 00:03:42.930
And then the signal from your own travels down its axon and connects or passes on to the dendrites of

59
00:03:42.930 --> 00:03:44.960
the next neuron and that's how they're connected.

60
00:03:45.030 --> 00:03:53.040
And in that small image over there you can see that the axon doesn't actually touch the dendrite lot.

61
00:03:53.310 --> 00:03:59.130
A lot of machine learning or like a few machine learning scientists are very adamant about the fact

62
00:03:59.130 --> 00:04:03.650
that it doesn't touch it like the room it doesn't touch.

63
00:04:03.660 --> 00:04:06.890
It has been proven that there is no physical connection there.

64
00:04:06.960 --> 00:04:14.010
But the point that we're interested in is that that connection between them that the whole concept of

65
00:04:14.010 --> 00:04:19.590
the signal being passed that recall the sign ups you can see over there in that little image that's

66
00:04:20.300 --> 00:04:22.210
figure bracket is a sign up.

67
00:04:22.230 --> 00:04:23.820
And that's the trend we're going to be doing.

68
00:04:23.820 --> 00:04:29.820
So instead of calling our artificial neurons the lines that we're going to have or the connectors for

69
00:04:29.820 --> 00:04:34.200
artificial neurons we're now going to be calling them axons or dendrites because then the question is

70
00:04:34.200 --> 00:04:36.880
whose connection is this is it that neurons are neurons.

71
00:04:36.990 --> 00:04:39.340
We just call that good which is good to call them sign of cells.

72
00:04:39.510 --> 00:04:42.680
And that's kind of just answers all questions in a.

73
00:04:42.690 --> 00:04:47.610
That's just basically where the signal is passed doesn't matter who that element belongs to.

74
00:04:47.610 --> 00:04:51.550
They're just a representation of the signal pass and we'll see that just now.

75
00:04:51.960 --> 00:04:55.210
So basically that's how a neuron works.

76
00:04:55.210 --> 00:05:03.390
And yeah so let's move on to how are we going to represent neuron going to create neurons in a machine

77
00:05:03.390 --> 00:05:04.690
so we're moving away.

78
00:05:04.690 --> 00:05:09.420
Now we're moving away from neuroscience and moving into technology.

79
00:05:09.460 --> 00:05:10.260
And here we go.

80
00:05:10.360 --> 00:05:17.260
So here's our neuron also sometimes called the node then your own gets some input signals and it has

81
00:05:17.260 --> 00:05:18.400
an output signal.

82
00:05:18.400 --> 00:05:21.040
So dendrites and axons remember.

83
00:05:21.040 --> 00:05:27.490
But again we're going to call these sign ups and then these input signals we're going to present them

84
00:05:27.490 --> 00:05:29.040
of other neurons as well.

85
00:05:29.080 --> 00:05:35.500
So in this specific case you can see that this neuron is green you're on is getting signals from yellow

86
00:05:35.500 --> 00:05:35.850
neurons.

87
00:05:35.860 --> 00:05:41.800
And in this course we're going to try and stick to a certain color coding regime where yellow means

88
00:05:41.830 --> 00:05:42.540
an input layer.

89
00:05:42.540 --> 00:05:50.700
So basically all of the neurons that are on the outer layer on the first front of where are the signals

90
00:05:50.710 --> 00:05:52.300
coming in and by signal.

91
00:05:52.300 --> 00:05:59.200
It might be like a bit of an over overkill to call this a signal it's just basically input values so.

92
00:05:59.340 --> 00:06:04.720
So you know how even like in a simple linear regression you have input values and then you have a predicted

93
00:06:04.720 --> 00:06:05.620
value Same thing here.

94
00:06:05.620 --> 00:06:10.720
So you have input values and there they are the yellow ones and then on the right you'll see just now

95
00:06:10.720 --> 00:06:11.260
it will be red.

96
00:06:11.260 --> 00:06:12.690
It'll be the output value.

97
00:06:13.570 --> 00:06:17.140
The other thing that I wanted to point out here is that in this specific example we're looking at a

98
00:06:17.140 --> 00:06:21.320
neuron which is getting its signals from the input layer neurons.

99
00:06:21.320 --> 00:06:24.220
There are also neurons but their input their neurons.

100
00:06:24.520 --> 00:06:31.450
Sometimes you'll have neurons which get their signal from other hidden layer neurons so from other green

101
00:06:31.450 --> 00:06:35.860
neurons and the concept is going to be exactly the same I mean just in this case we use for simplicity's

102
00:06:35.860 --> 00:06:42.830
sake we're portraying this example and in terms of the input layer the way to think about it is in the

103
00:06:42.970 --> 00:06:49.900
in the analogy of the human brain the input layer is your senses right so whatever you can see hear

104
00:06:49.900 --> 00:06:52.280
feel touch or smell.

105
00:06:52.510 --> 00:06:57.220
And of course it's like there's there's a lot of things you can see there's a lot of information coming

106
00:06:57.220 --> 00:06:57.540
in.

107
00:06:57.730 --> 00:07:02.870
But those are your That's what your brain is limited to is pretty much a life.

108
00:07:03.010 --> 00:07:09.160
Israel lives in a box made out of bones and it's only just it's it's a mind blowing concept to think

109
00:07:09.160 --> 00:07:15.430
about that your brain is just locked in a black box and the only thing it can see can hear.

110
00:07:15.430 --> 00:07:20.920
The only thing is getting is electrical impulses coming from these organs that you have we should call

111
00:07:20.920 --> 00:07:28.210
your ears nose eyes you know your sense of touch and whatever and you and your and your taste.

112
00:07:28.220 --> 00:07:34.150
Right so it's just getting signals but it basically lives in this dark black box and it's making making

113
00:07:34.150 --> 00:07:38.460
sense of the world through your senses it's phenomenal.

114
00:07:38.500 --> 00:07:38.930
And so yeah.

115
00:07:38.950 --> 00:07:43.030
So you have these inputs that are coming in in terms of human brain.

116
00:07:43.030 --> 00:07:49.540
Those are your five senses and in terms of machine learning or deep learning that is basically your

117
00:07:49.900 --> 00:07:55.520
input values are your independent variables and we'll get that in a second so your input values they

118
00:07:56.400 --> 00:08:01.090
the signal is passed through sign sinuses to the neuron and then your own has an output value that it

119
00:08:01.090 --> 00:08:03.190
passes further on down the chain.

120
00:08:03.550 --> 00:08:07.990
In this specific case in terms of color coding again yellow means input layer so we kind of simplifying

121
00:08:07.990 --> 00:08:11.830
everything here we're saying we're only going to have like the input layer and then we're going to have

122
00:08:11.830 --> 00:08:16.510
one hidden layer with the green which is the hinterland then we're going to have the output there right

123
00:08:16.510 --> 00:08:17.530
away.

124
00:08:17.530 --> 00:08:21.360
So just so that we can get used to these calls for now.

125
00:08:21.580 --> 00:08:24.030
So there we go that's the basic structure.

126
00:08:24.030 --> 00:08:28.390
So now let's look in a bit more detail at these different elements that we have.

127
00:08:28.390 --> 00:08:31.090
So we've got the input layer and what do we have here.

128
00:08:31.090 --> 00:08:37.090
Well we have these inputs which are in fact independent variable so depend variable one and a bit variable

129
00:08:37.090 --> 00:08:42.760
to independent variable and the important thing to remember here is that these independent variables

130
00:08:42.790 --> 00:08:44.740
are all for one single observation.

131
00:08:44.740 --> 00:08:47.620
So think of it as just one row in your database.

132
00:08:47.620 --> 00:08:54.790
One observation you just take all of the independent variables you know maybe it's the age of the person

133
00:08:54.820 --> 00:09:01.270
there the amount of money in the bank account and then how how do they drive or walk work what method

134
00:09:01.270 --> 00:09:03.060
of Shampoo's protection do they use.

135
00:09:03.070 --> 00:09:08.800
So but that's all descriptors of one specific person that you are either your training your model on

136
00:09:09.130 --> 00:09:12.520
or you're performing some prediction on.

137
00:09:12.610 --> 00:09:16.900
And the other thing you need to know about these variables is that you need to standardize them so you

138
00:09:16.900 --> 00:09:21.310
need to either standardize them which means make sure that they have a mean of zero and a variance one

139
00:09:21.340 --> 00:09:29.080
or you can also sometimes and headland will point out these traces in a bit more detail perhaps in the

140
00:09:29.080 --> 00:09:33.310
practical terms you might come across these sometimes you might want to know standardising might want

141
00:09:33.310 --> 00:09:34.800
to normalize them.

142
00:09:34.990 --> 00:09:41.250
Meaning that instead of making sure the mean and very Muser variance is one you just take you know to

143
00:09:41.260 --> 00:09:46.480
subtract the minimum value and then you divide by the maximum minus minimums by the range of your values

144
00:09:46.480 --> 00:09:49.210
and the four you get values between 0 and 1.

145
00:09:49.510 --> 00:09:53.580
And it depends on this scenario you might want to do one or the other.

146
00:09:53.590 --> 00:10:00.700
But basically you want all of these variables to be quite similar in about the same a range of values

147
00:10:00.760 --> 00:10:01.670
and why.

148
00:10:01.690 --> 00:10:02.160
Why is that.

149
00:10:02.180 --> 00:10:06.890
Well all of these values are going to go into a neural network where as we'll see just now they'll be

150
00:10:06.890 --> 00:10:13.190
added up and multiplied by it's added up and so on and just going to be is going to be easier for the

151
00:10:13.190 --> 00:10:17.140
neural network to process them if they're all about the same.

152
00:10:17.290 --> 00:10:23.770
And and in fact you know that's that's just how it is going to be able to work properly.

153
00:10:24.230 --> 00:10:29.210
And if you want to read more about standardization normalization and other things that you can do if

154
00:10:29.210 --> 00:10:36.440
you know what variables a good additional reading paper is called efficient back prob by young Licken

155
00:10:37.030 --> 00:10:39.780
1998 the links over there.

156
00:10:39.860 --> 00:10:47.570
So we're actually going to talk more about this phenomenal person in the space of deep learning in the

157
00:10:47.680 --> 00:10:52.160
part of the course we're talking about convolutional neural networks and you'll you'll see that this

158
00:10:52.160 --> 00:10:55.240
is definitely a person who knows what he's talking about.

159
00:10:55.280 --> 00:11:00.830
He's a close friend of Jeffrey Hinton who we've already seen who are very dim..

160
00:11:00.860 --> 00:11:07.070
So in this paper you'll learn more about centers ization of normalization but also you can pick up lots

161
00:11:07.070 --> 00:11:11.510
of other different tips and tricks and you'll be a good a good source for additional reading as you

162
00:11:11.510 --> 00:11:12.120
go through this.

163
00:11:12.120 --> 00:11:14.510
So check it out.

164
00:11:15.830 --> 00:11:22.090
If you are interested in so you check it out if you're interested in some additional reading.

165
00:11:22.160 --> 00:11:26.840
And there we go so that's what we do for the variables.

166
00:11:26.840 --> 00:11:29.610
And here we've got the output value.

167
00:11:29.660 --> 00:11:31.610
So what can our output value be.

168
00:11:31.610 --> 00:11:34.400
Well we've got a couple of options.

169
00:11:34.760 --> 00:11:35.900
Well we've got a couple of options.

170
00:11:35.930 --> 00:11:41.740
Output value can be it can be continuous Like for instance price it can be binary for instance a person

171
00:11:41.740 --> 00:11:48.890
will exit or will stay or it can be categorical verbal and physical wriggler categorical verbal.

172
00:11:48.890 --> 00:11:53.690
The important thing to remember here is that in that case your output value won't be just one it'll

173
00:11:53.690 --> 00:12:00.020
be several output values because these will be a dummy variables which will be representing your categories

174
00:12:00.990 --> 00:12:07.220
and that just this how it works and just important to remember that in that case that's how you're going

175
00:12:07.220 --> 00:12:11.910
to be getting your categories out of the artificial neural network.

176
00:12:12.440 --> 00:12:15.150
But let's go back to a simple case of one output volume.

177
00:12:15.380 --> 00:12:21.410
And now let's one more point or kind of like the point the ready made I just wanted to reiterate this

178
00:12:21.410 --> 00:12:24.950
point on the left you've got a single observation.

179
00:12:25.100 --> 00:12:29.360
So I wonder if you mean from your data set and on the right you have a single generation as well.

180
00:12:29.390 --> 00:12:31.820
And that is the same observation.

181
00:12:31.820 --> 00:12:37.730
So important to remember that like whatever inputs you putting in that's for one row and then the output

182
00:12:37.730 --> 00:12:39.600
you get that is for that same exact row.

183
00:12:39.710 --> 00:12:44.080
Or if you're training your neural network then you know you're putting the inputs in for that one role

184
00:12:44.090 --> 00:12:46.080
you're putting the output in for that one row.

185
00:12:46.100 --> 00:12:52.760
So like if you want to simplify the complexity think of it as a like a simple regression or a multivariate

186
00:12:52.910 --> 00:12:53.630
linear regression.

187
00:12:53.630 --> 00:12:57.730
So you're putting in your values you have the output.

188
00:12:57.770 --> 00:12:59.330
There's there's like there's no question about it.

189
00:12:59.330 --> 00:13:02.420
When we're talking about things like regression because we're so used to it.

190
00:13:02.420 --> 00:13:04.600
Same thing here it's nothing too complex.

191
00:13:04.610 --> 00:13:06.370
We're just putting in values we are getting output.

192
00:13:06.380 --> 00:13:10.940
But just remember that every time it's one row you're dealing with so you don't get confused and start

193
00:13:10.940 --> 00:13:17.570
putting in like thinking that these are different different rows that you're putting into your artificial

194
00:13:17.570 --> 00:13:22.340
neural network or something this is all just values in that one Rowse a different observation different

195
00:13:22.340 --> 00:13:28.600
characteristics or attributes relating to that one observation every single time.

196
00:13:28.790 --> 00:13:35.030
OK so next thing we want to talk about here is are the sign UPSs is a sign that says Here we've got

197
00:13:35.030 --> 00:13:38.580
souses and they all actually get assigned weights weights.

198
00:13:38.610 --> 00:13:46.760
Going to talk more about weights further down but in short weights are crucial to artificial neural

199
00:13:46.760 --> 00:13:53.690
network and it works functioning because weights are how neural networks learn by adjusting the weights

200
00:13:54.100 --> 00:13:59.900
the neural network decides in every single case what single signal is poor and what signal is not important

201
00:13:59.900 --> 00:14:00.770
to certain neuron.

202
00:14:00.770 --> 00:14:05.930
What single gets passed along and what signal doesn't get passed along or what strength to what extent

203
00:14:05.930 --> 00:14:07.400
signals get passed along.

204
00:14:07.420 --> 00:14:08.950
So weights are crucial.

205
00:14:08.960 --> 00:14:13.220
They are and they are the things that get adjusted through the process of learning.

206
00:14:13.220 --> 00:14:17.840
Like when when you're training an artificial neural network you're basically adjusting all of the weights

207
00:14:18.200 --> 00:14:20.660
in all of the sign says across this whole neural network.

208
00:14:20.840 --> 00:14:26.320
And that's where gradient descent and back propagation come into play.

209
00:14:26.320 --> 00:14:29.160
And those are concepts that we also discussed.

210
00:14:29.300 --> 00:14:31.070
So basically those are the weights.

211
00:14:31.070 --> 00:14:32.520
That's what we need to know for now.

212
00:14:32.810 --> 00:14:38.050
And we've got the neurons so signals go into the neuron and what happens in the euro.

213
00:14:38.060 --> 00:14:40.160
So this is the interesting part.

214
00:14:40.160 --> 00:14:43.300
Like we're talking about the neuron today what happens inside the neuron.

215
00:14:43.520 --> 00:14:49.730
So a few things happen first thing and the first step is that all of these values that it's getting

216
00:14:49.820 --> 00:14:50.980
gets added up.

217
00:14:50.980 --> 00:14:59.750
So it takes that added so the weighted sum of all of the input values that is getting very simple it's

218
00:14:59.750 --> 00:15:06.080
very very straightforward just add up multiply by the way add them up and then it applies an activation

219
00:15:06.080 --> 00:15:06.830
function.

220
00:15:06.830 --> 00:15:10.580
Now we're going to talk more about activation functions further down but it's basically a function that

221
00:15:10.580 --> 00:15:19.060
is assigned to this neuron or to this whole layer and it is applied to this weighted some.

222
00:15:19.220 --> 00:15:26.810
And then from that you don't understand if it needs to pass on a signal if you like and that's the signal

223
00:15:26.810 --> 00:15:31.910
that passes on that the function applied to the way that some.

224
00:15:31.910 --> 00:15:36.050
But basically depending on the function the neuron will either pass on a signal it or it won't pass

225
00:15:36.050 --> 00:15:37.160
the signal on.

226
00:15:37.520 --> 00:15:40.970
And that's exactly what happened here in step three.

227
00:15:41.090 --> 00:15:46.580
The neuron passes on that signal to the next neuron down the line and that's what we're going to talk

228
00:15:46.580 --> 00:15:49.580
about in the next tutorial because it is quite an important topic.

229
00:15:49.580 --> 00:15:56.000
We want to delve deeper into the activation function but hopefully for now everything is it should be

230
00:15:56.000 --> 00:16:01.250
pretty clear how you know got input values you've got weights you've got design houses you've got something

231
00:16:01.310 --> 00:16:06.050
you know happens in neuron you've got weighted Sarmad and activation function applied and then that

232
00:16:06.050 --> 00:16:10.700
is passed on line and that is just repeated throughout the whole neural network on and on and on and

233
00:16:10.790 --> 00:16:16.490
on you know thousands hundreds of thousands of times depending on how big how many neurons you have

234
00:16:16.490 --> 00:16:19.200
how many situps silences you have your neural network.

235
00:16:19.200 --> 00:16:19.740
So there we go.

236
00:16:19.740 --> 00:16:22.780
Hope you enjoyed today's Tauriel Coates.

237
00:16:22.860 --> 00:16:24.660
And until then enjoy deep learning.
