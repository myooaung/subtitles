WEBVTT
1
00:00:00.210 --> 00:00:05.970
Hello my friends welcome to this new practical activity and the first practical activity actually of

2
00:00:05.970 --> 00:00:08.310
Part Three classification.

3
00:00:08.390 --> 00:00:13.650
I'm super excited as you might hear by the sound of my voice because this is one of my favorite parts.

4
00:00:13.710 --> 00:00:17.910
And actually you will see that the case study we will do together is pretty fun.

5
00:00:17.910 --> 00:00:19.500
So I can't wait to start.

6
00:00:19.500 --> 00:00:20.520
Welcome back.

7
00:00:20.520 --> 00:00:21.630
And so now here we go.

8
00:00:21.630 --> 00:00:28.230
We are about to enter a new branch of machine learning where at this time we won't predict a continuous

9
00:00:28.290 --> 00:00:33.760
numerical value like in part two regression but this time we will predict a category.

10
00:00:33.810 --> 00:00:40.320
You know a class like for example you know a binary variable 0 or 1 1 The classic example is actually

11
00:00:40.380 --> 00:00:42.800
two classification to predict a tumor.

12
00:00:42.840 --> 00:00:45.540
You know whether a tumor is benign or malignant.

13
00:00:45.570 --> 00:00:51.150
And this is actually the case study we'll do a DNA you know when I will teach you on how to select the

14
00:00:51.150 --> 00:00:54.340
best classification model but that will come at the end.

15
00:00:54.450 --> 00:01:00.560
First we will do a fun case study with which we will learn how to build each of our classification models.

16
00:01:00.570 --> 00:01:01.950
And there you go my friends.

17
00:01:01.950 --> 00:01:06.730
It is and destroy all that I will explain the problem of this case study.

18
00:01:06.780 --> 00:01:07.100
All right.

19
00:01:07.110 --> 00:01:12.220
So before we start let's just make sure once again everyone here is on the same page.

20
00:01:12.240 --> 00:01:18.010
So right before this tutorial I give you the link to this folder containing all the codes and data sets

21
00:01:18.240 --> 00:01:19.360
in the 10 parts.

22
00:01:19.410 --> 00:01:21.630
So make sure to connect to that link.

23
00:01:21.630 --> 00:01:24.390
And now now we should all be on the same page.

24
00:01:24.420 --> 00:01:25.350
And there you go.

25
00:01:25.350 --> 00:01:33.090
We're going to go into Part Three classification to tackle our first model logistic regression.

26
00:01:33.090 --> 00:01:39.300
I hope you enjoyed the intuition lectures and mostly that you are now ready to put your well learned

27
00:01:39.300 --> 00:01:41.030
theory into practice.

28
00:01:41.070 --> 00:01:47.610
And we're going to put it into practice first with Python with which we're going to reemployment from

29
00:01:47.610 --> 00:01:48.110
scratch.

30
00:01:48.150 --> 00:01:52.830
And step by step the whole logistic regression implementation.

31
00:01:52.920 --> 00:01:56.500
So as you can see in this python folder you have two files first.

32
00:01:56.570 --> 00:02:02.520
Well that logistic regression implementation in IP one B format which you can open with either Google

33
00:02:02.530 --> 00:02:10.390
collaboratively or duper notebook and you have the data set which is called social network ads.

34
00:02:10.470 --> 00:02:16.780
So let's open it and now let me explain what the problem is about.

35
00:02:16.920 --> 00:02:17.370
All right.

36
00:02:17.370 --> 00:02:23.160
So let's imagine our favorite car company I won't mention a name here because I don't want to do any

37
00:02:23.160 --> 00:02:24.400
kind of advertising.

38
00:02:24.540 --> 00:02:31.140
But let us imagine your favorite car company and let's imagine that you are a data scientist for that

39
00:02:31.140 --> 00:02:38.850
company and your mission should you choose to accept it is to predict which of your previous customers

40
00:02:39.090 --> 00:02:46.790
will buy a brand new beautiful SUV just created by your favorite car company.

41
00:02:46.800 --> 00:02:53.580
All right so your favorite car company has just released this brand new beautiful irresistible SUV and

42
00:02:53.580 --> 00:02:58.860
the general manager of this car company has asked you you know the most talented data scientists at

43
00:02:58.860 --> 00:03:05.670
the company to predict which customers will buy that new SUV with the highest conversion rate.

44
00:03:06.240 --> 00:03:11.550
And to help you because you know this general manager has some minimum data science skills and knows

45
00:03:11.880 --> 00:03:15.910
and understands that in order to predict this you need data right.

46
00:03:15.940 --> 00:03:23.010
Data on which to train your classification model to predict what needs to be predicted meaning which

47
00:03:23.010 --> 00:03:25.470
customers will buy that brand new SUV.

48
00:03:26.060 --> 00:03:26.780
And so there you go.

49
00:03:26.790 --> 00:03:31.080
That's exactly the data that your general manager gave you.

50
00:03:31.110 --> 00:03:32.270
And in this data set.

51
00:03:32.280 --> 00:03:39.410
Well each row corresponds to different customers and for each of these customers there you go.

52
00:03:39.420 --> 00:03:44.210
I'm about to reveal the features ended up being viable for each of these customers.

53
00:03:44.220 --> 00:03:50.880
Well this general manager collected the age and it collected the estimated salary because you know when

54
00:03:50.880 --> 00:03:54.400
a customer buys a new car with some kind of credit or whatever.

55
00:03:54.420 --> 00:03:57.510
Well it has to provide the estimated salary in the form.

56
00:03:57.510 --> 00:03:59.910
So that's how it got the estimated salary.

57
00:03:59.940 --> 00:04:07.080
And finally that's your dependent variable of course the chase variable telling whether or not these

58
00:04:07.080 --> 00:04:12.900
customers have put previously some older SUV use of this car company.

59
00:04:12.960 --> 00:04:13.230
Right.

60
00:04:13.230 --> 00:04:19.680
So this car company has basically many makes and models of SUV and all the zeros and the ones that you

61
00:04:19.680 --> 00:04:26.040
see here for each customer is seeing whether or not these customers have bought one of these previous

62
00:04:26.160 --> 00:04:33.870
SUV so that your model will be trained on these data sets and for new customers you know having a different

63
00:04:33.930 --> 00:04:36.180
age and different estimated salary.

64
00:04:36.180 --> 00:04:40.720
Well we will predict if yes or no they will buy that new SUV.

65
00:04:41.240 --> 00:04:41.990
OK.

66
00:04:42.030 --> 00:04:47.880
So of course in this dependent variable purchased zero means that the customer didn't buy any previous

67
00:04:47.880 --> 00:04:52.580
SUV and one means that the customer but some previous SUV.

68
00:04:52.620 --> 00:04:53.220
All right.

69
00:04:53.220 --> 00:04:59.910
And therefore all the future predictions that will be equal to one will probably mean that the customer

70
00:05:00.000 --> 00:05:05.050
has a high chance to buy the new SUV if of course we offer a great deal.

71
00:05:05.070 --> 00:05:11.070
And finally once we predict the customers that are going to buy the SUV while the final step of the

72
00:05:11.070 --> 00:05:18.510
strategy will be for the advertising team to post ads of this brand new SUV on social networks and these

73
00:05:18.600 --> 00:05:23.900
ads will be targeted to the customers where we predict one you know where we predict that they're going

74
00:05:23.900 --> 00:05:25.710
to buy that new SUV.

75
00:05:25.710 --> 00:05:31.170
All right so you see the idea the predictive model will target your customers and then the advertising

76
00:05:31.170 --> 00:05:37.430
team will use the results of this breaks of model to optimize the targeting of future customers.

77
00:05:37.470 --> 00:05:43.160
And that is why you know the name of the dataset is called social network ad that's easy.

78
00:05:43.430 --> 00:05:43.760
OK.

79
00:05:44.340 --> 00:05:44.610
All right.

80
00:05:44.610 --> 00:05:46.200
So that's the problem I hope you like it.

81
00:05:46.200 --> 00:05:48.240
I hope you're excited to work on it.

82
00:05:48.600 --> 00:05:50.090
And so now we're going to.

83
00:05:50.160 --> 00:05:58.050
Without further ado start are logistic regression implementation on your favorite I.D. whether it is

84
00:05:58.080 --> 00:06:00.870
Google laboratory or Jupiter notebook.

85
00:06:00.960 --> 00:06:05.160
You have the choice but my favorite is by far Google collaborators.

86
00:06:05.160 --> 00:06:12.990
So if you love it to follow me here and now let's re implement this logistic regression implementation

87
00:06:13.260 --> 00:06:17.370
step by step right now it is laying out the notebook and we're about to have it in a second.

88
00:06:17.370 --> 00:06:18.410
There we go.

89
00:06:18.420 --> 00:06:18.680
All right.

90
00:06:18.710 --> 00:06:20.670
So that's the whole notebook.

91
00:06:20.670 --> 00:06:22.180
It is in read only mode.

92
00:06:22.190 --> 00:06:28.380
So right now what we have to do is to create a copy of this notebook and to do this we just have to

93
00:06:28.380 --> 00:06:31.410
click Save a copy and drive.

94
00:06:31.410 --> 00:06:37.440
And this will create a copy as you can see of this notebook in which we will be able to re-employment

95
00:06:37.650 --> 00:06:40.130
the whole model from scratch.

96
00:06:40.140 --> 00:06:41.000
All right great.

97
00:06:41.010 --> 00:06:44.900
So as usual the first thing we're going to do is to delete all the code cells.

98
00:06:44.940 --> 00:06:45.140
Right.

99
00:06:45.150 --> 00:06:47.130
Because I want you to take action.

100
00:06:47.130 --> 00:06:52.760
I want you to learn by doing so I really really want you to re implement all these code cells from scratch.

101
00:06:52.790 --> 00:06:57.690
So we're going to delete all of them to do this we just have to click them and then click the trust

102
00:06:57.690 --> 00:06:58.700
button here.

103
00:06:58.740 --> 00:07:00.780
Just do as I do.

104
00:07:00.810 --> 00:07:06.720
All right and make sure not to delete the text sales because we want to keep that well highlighted structure

105
00:07:07.350 --> 00:07:09.120
right features killing.

106
00:07:09.120 --> 00:07:14.890
So yes there will be feature scaling for logistic regression and I will explain why.

107
00:07:14.910 --> 00:07:15.310
All right.

108
00:07:15.310 --> 00:07:19.440
And now we train the logistic regression model breaking new result.

109
00:07:19.440 --> 00:07:19.890
All right.

110
00:07:20.010 --> 00:07:24.960
And you really have everything in its implementation you'll see that you will learn how to predict an

111
00:07:24.960 --> 00:07:29.760
ensemble of results you know in the desert to also learn how to predict a single result like you know

112
00:07:29.760 --> 00:07:34.200
when you deploy your model in production when you want to predict a single observation.

113
00:07:34.210 --> 00:07:39.410
So now confusion matrix that is to evaluate your model and of course the visualizations at GM.

114
00:07:39.420 --> 00:07:46.050
And once again I chose a data set of only two features right The Age and the estimated salary so that

115
00:07:46.110 --> 00:07:50.700
we can indeed visualize the results Indian on the training set and on the test set.

116
00:07:50.700 --> 00:07:56.670
Because remember in the plot each dimension corresponds to one feature and therefore there are as many

117
00:07:56.670 --> 00:07:58.930
dimensions as there are features.

118
00:07:58.950 --> 00:08:04.290
And so since we have two features we'll have a nice 2D plot and that's exactly the reason why I needed

119
00:08:04.290 --> 00:08:10.500
to take two features but no worries the implementations were about to make works for any data set regardless

120
00:08:10.680 --> 00:08:11.800
the number of features.

121
00:08:11.880 --> 00:08:17.490
And I will prove this to you at the end of this part when deploying all our classification models on

122
00:08:17.490 --> 00:08:21.490
a brand new generic dataset with more features.

123
00:08:21.570 --> 00:08:26.150
And this is how I will also teach you on how to select the best model.

124
00:08:26.160 --> 00:08:26.790
All right.

125
00:08:26.880 --> 00:08:28.110
So there you go.

126
00:08:28.110 --> 00:08:33.200
I hope you're excited both by the problem case study and this implementation.

127
00:08:33.330 --> 00:08:36.340
And now before we finish and move onto the next tutorial.

128
00:08:36.420 --> 00:08:42.380
Well I would like you to do a little exercise now that you saw the dataset and understands it.

129
00:08:42.450 --> 00:08:48.810
And since you also have your data processing template Well there you go the exercise is I would like

130
00:08:48.810 --> 00:08:55.200
you to implement on your own the data repricing phase up to this step you know feature scaling.

131
00:08:55.230 --> 00:09:00.590
So basically I would like to implement on your own this step empowering the libraries and this step

132
00:09:00.640 --> 00:09:05.290
point the data set and the steps bringing the data set into the training set and to set.

133
00:09:05.370 --> 00:09:09.570
And finally this last step of the data repricing phase feature scaling.

134
00:09:09.570 --> 00:09:10.220
All right.

135
00:09:10.260 --> 00:09:17.370
So please try to use of course your data repricing template and of course your data repricing toolkit

136
00:09:17.400 --> 00:09:22.970
because indeed in order to implement that step you will need to grab a tool of your data progressing

137
00:09:22.980 --> 00:09:23.430
toolkit.

138
00:09:23.430 --> 00:09:27.620
And I'm sure you will find it so you can only do this on your own.

139
00:09:27.630 --> 00:09:28.560
There is no trap.

140
00:09:28.560 --> 00:09:30.070
It's actually super easy.

141
00:09:30.180 --> 00:09:34.560
And of course we will implement the solution together in an extra trial.

142
00:09:34.740 --> 00:09:37.140
So I can't wait to see what you end up with.

143
00:09:37.230 --> 00:09:40.570
And I'm sure we will end up with the same thing.

144
00:09:40.580 --> 00:09:41.520
So let's see.

145
00:09:41.520 --> 00:09:43.320
And until then enjoy machine learning.
