1
00:00:00,240 --> 00:00:01,160
Hello my friends.

2
00:00:01,170 --> 00:00:07,050
I hope you digested well the previous tutorial where we tackle this big but yet important tool of our

3
00:00:07,050 --> 00:00:08,700
data repressing toolkit.

4
00:00:08,730 --> 00:00:09,260
Indeed.

5
00:00:09,270 --> 00:00:15,210
Now you know how to handle the case where you have some category called data in your data set which

6
00:00:15,210 --> 00:00:19,720
is a situation you will encounter many times in your future mission or in Korea.

7
00:00:19,830 --> 00:00:25,860
And now we have two tools to cover the first one being splitting the data set into the training set

8
00:00:25,920 --> 00:00:29,520
and the test set and a second one being feature scaling.

9
00:00:29,550 --> 00:00:36,750
So before we start I'm about to enter one of the most frequently asked questions in the data science

10
00:00:36,750 --> 00:00:40,040
community which is be ready for it.

11
00:00:40,080 --> 00:00:47,100
Do we have to apply feature scaling before splitting the dataset into the training set and to set or

12
00:00:47,460 --> 00:00:53,730
after I've seen this question many times and you will find that question in many forums of the data

13
00:00:53,730 --> 00:00:54,820
science community.

14
00:00:54,840 --> 00:00:58,930
Some people will say that we have to apply features killing before it split.

15
00:00:58,950 --> 00:01:05,370
Some people will say after the split and now I'm about to reveal the right answer there is only one

16
00:01:05,460 --> 00:01:08,680
right answer which is by the way totally obvious.

17
00:01:08,730 --> 00:01:15,780
After you get the explanation so the answer is we have to apply feature scaling.

18
00:01:15,780 --> 00:01:22,470
After splitting the dataset into the training set and a test set and now let me explain.

19
00:01:22,470 --> 00:01:24,710
So first just to make sure everybody understands.

20
00:01:24,720 --> 00:01:25,800
Let me explain the what.

21
00:01:25,800 --> 00:01:27,270
First and then I'll explain the why.

22
00:01:27,390 --> 00:01:33,540
So of course splitting the dataset into the training set in a test consists of making two separate sets

23
00:01:33,800 --> 00:01:39,090
one training set where you're going to train your missionary model on existing observations and one

24
00:01:39,090 --> 00:01:44,730
test set where you're going to evaluate the performance of your model on new observations.

25
00:01:44,730 --> 00:01:50,790
And it's important to understand that these new observations are exactly like you know some future data

26
00:01:50,940 --> 00:01:54,700
that you're going to get and on which you're going to deploy your machine learning model.

27
00:01:54,720 --> 00:01:55,150
All right.

28
00:01:55,200 --> 00:02:02,730
So that's this first tool and now feature scaling simply consists of scaling all your variables or your

29
00:02:02,730 --> 00:02:07,760
features actually to make sure they all take values in the same scale.

30
00:02:07,770 --> 00:02:13,560
And we do this so as to prevent one feature to dominate the other which therefore would be neglected

31
00:02:13,710 --> 00:02:15,410
by the machine learning model.

32
00:02:15,450 --> 00:02:15,870
All right.

33
00:02:15,900 --> 00:02:18,170
So that's the what for both of these tools.

34
00:02:18,180 --> 00:02:25,620
Now let me explain the why we have to apply feature scaling after splitting the data set into the training

35
00:02:25,620 --> 00:02:32,160
set and instead it's really obvious it is for the simple reason that the test set is supposed to be

36
00:02:32,160 --> 00:02:37,770
a brand new set on which you are going to evaluate your machine learning model.

37
00:02:37,770 --> 00:02:43,710
So it's exactly like you know you're training your machinery moral on your training set and then later

38
00:02:43,710 --> 00:02:48,420
on you know after it is trained you are going to deploy it on new observations.

39
00:02:48,420 --> 00:02:54,360
So what this means is that the test set is something you're not supposed to work with for the training

40
00:02:54,660 --> 00:03:01,920
and features killing is as you will see a technique that will get the mean and standard deviation of

41
00:03:02,070 --> 00:03:03,240
your feature.

42
00:03:03,300 --> 00:03:05,580
You know in order to perform the scaling.

43
00:03:05,580 --> 00:03:13,740
So if we apply feature scaling before the split then it will actually get the mean and standard deviation

44
00:03:13,800 --> 00:03:19,530
of all the values including the ones in the test set and since the test set is something you're not

45
00:03:19,530 --> 00:03:23,220
supposed to have you know like some future data in production.

46
00:03:23,220 --> 00:03:28,710
Well you know applying pictures killing on the original day you said before the split would cause some

47
00:03:28,830 --> 00:03:32,030
what we call information leakage on the test set.

48
00:03:32,040 --> 00:03:37,560
You know we would grab some information from the test set which we're not supposed to get because it

49
00:03:37,560 --> 00:03:41,250
is supposed to be new data with new observations.

50
00:03:41,280 --> 00:03:47,760
So remember this the essential reason why you should not apply features scaling before the split is

51
00:03:47,760 --> 00:03:54,450
to prevent information leakage on the test set which you're not supposed to have until the training

52
00:03:54,480 --> 00:03:55,380
is done.

53
00:03:55,470 --> 00:03:55,920
All right.

54
00:03:55,950 --> 00:03:57,700
So I think I have explained enough.

55
00:03:57,780 --> 00:04:02,490
Now I'm relieved and we're relieved that at least it's 100 percent clear for everyone.

56
00:04:02,790 --> 00:04:04,150
And so there you go my friends.

57
00:04:04,210 --> 00:04:11,190
Let's implement one of the last tools of this data processing toolkit which is indeed the split of the

58
00:04:11,190 --> 00:04:14,430
data set into the training set and the test.

59
00:04:14,580 --> 00:04:14,950
All right.

60
00:04:14,970 --> 00:04:16,650
So how are we going to do this.

61
00:04:16,650 --> 00:04:23,310
Well we're gonna do it with a function a function by psychic learn you know the most popular and useful

62
00:04:23,340 --> 00:04:29,040
data science library because once again this library contains a module that is called model selection

63
00:04:29,370 --> 00:04:33,310
which contains itself a function called trained test splits.

64
00:04:33,480 --> 00:04:38,070
And this function will exactly do what we want which is to create four separate sets.

65
00:04:38,100 --> 00:04:43,890
Actually not two but four because we will actually create a pair of matrix of features independent variable

66
00:04:43,890 --> 00:04:48,330
for the training set and another pair of matrix of features and opinion been invaluable for the test

67
00:04:48,330 --> 00:04:49,020
set.

68
00:04:49,020 --> 00:04:54,090
All right so we're basically going to get four sets X strain which is a matrix of features of the training

69
00:04:54,090 --> 00:04:59,580
set x test which is the matrix of features of the test set y train which is a dependent variable of

70
00:04:59,580 --> 00:05:03,800
the set and white test which is a dependent variable of the test.

71
00:05:03,810 --> 00:05:05,090
That's exactly what we want.

72
00:05:05,310 --> 00:05:07,030
And now why do we want this.

73
00:05:07,080 --> 00:05:08,130
While it's not us.

74
00:05:08,130 --> 00:05:14,550
It's actually the future machine learning model that we will build in the next part which will be all

75
00:05:14,550 --> 00:05:22,020
of them expecting this format as inputs you know for the training it will expect extra train and Y train

76
00:05:22,080 --> 00:05:27,870
as inputs in a method actually called the fifth method and for the predictions also called inference

77
00:05:28,240 --> 00:05:30,360
these models will predict x test.

78
00:05:30,360 --> 00:05:31,100
All right.

79
00:05:31,110 --> 00:05:36,690
So that's the reason it is simply the format expected by the future imaginary models.

80
00:05:36,720 --> 00:05:38,790
And now let's get these four sets.

81
00:05:38,970 --> 00:05:47,100
So we're gonna get them from Cyclone learn of course there you go from which we're going to get access

82
00:05:47,100 --> 00:05:49,990
to model selection.

83
00:05:50,010 --> 00:05:51,830
I really like Google collab.

84
00:05:52,080 --> 00:06:00,180
And then from which we're going to import that train underscore tests clit function perfect.

85
00:06:00,180 --> 00:06:04,530
You see how we can be so efficient thanks to the assistance of Google collab.

86
00:06:04,590 --> 00:06:07,290
I hope you really like it as well.

87
00:06:07,290 --> 00:06:07,730
All right.

88
00:06:07,770 --> 00:06:10,180
So now that we have this functional Well we're going to use it.

89
00:06:10,290 --> 00:06:14,790
And since we already know what this function will return as I've just explained.

90
00:06:14,790 --> 00:06:19,830
Well let's create these four variables returned by this trend tested function.

91
00:06:19,830 --> 00:06:25,930
And as we said they are first X train the matrix of features of the training set.

92
00:06:26,040 --> 00:06:34,170
Therefore containing all the countries one hotel encoded ages and salaries of the training set so extreme

93
00:06:34,470 --> 00:06:39,620
then accessed the matrix of features of the test set.

94
00:06:39,720 --> 00:06:46,860
Then why train which is the dependent variable of the training set meaning all the poor chaste decisions

95
00:06:47,070 --> 00:06:49,590
of the customers in the training set.

96
00:06:49,590 --> 00:06:57,680
Why train and then why test which same content all the purchase decisions of the customers in the test.

97
00:06:57,690 --> 00:06:58,230
All right.

98
00:06:58,560 --> 00:07:04,680
So that's the four variables returned by this train tested function and since it is the function that

99
00:07:04,680 --> 00:07:12,150
returns is variable Well let's take that function right away and let's add here and equals and train

100
00:07:12,150 --> 00:07:14,460
test pit and then some parenthesis.

101
00:07:14,580 --> 00:07:19,760
And now the question is what do we have to input inside this function.

102
00:07:19,770 --> 00:07:20,150
All right.

103
00:07:20,310 --> 00:07:24,350
So actually there are some parameters that we can guess right.

104
00:07:24,720 --> 00:07:28,300
Because indeed this train test split is supposed to split something.

105
00:07:28,350 --> 00:07:33,450
So one of the inputs will be that's something which we're about to split and which is of course our

106
00:07:33,600 --> 00:07:34,690
data set.

107
00:07:34,710 --> 00:07:38,490
However of course this function does not expect the data set as a whole.

108
00:07:38,580 --> 00:07:44,390
It expects Well the combination of the matrix of Features X and that have been invaluable vector Y in

109
00:07:44,430 --> 00:07:51,630
that the first two inputs of this function so let's input them here x4 as the matrix of features and

110
00:07:51,750 --> 00:07:54,400
why the dependent variable vector.

111
00:07:54,750 --> 00:07:55,930
Great.

112
00:07:56,020 --> 00:07:59,370
Y yes then come up and then next arguments.

113
00:07:59,400 --> 00:08:07,680
So we still have to input two more arguments which are going to be first the split size you know because

114
00:08:07,920 --> 00:08:13,100
we're not going to split this dataset into a training set and a test of the same size.

115
00:08:13,200 --> 00:08:18,720
Actually we need a lot of observations in a training set and a few in the test set but we need a lot

116
00:08:18,720 --> 00:08:24,240
of them in the training set so as to give the future machinery model more chance to understand and learn

117
00:08:24,360 --> 00:08:26,430
the correlations in the dataset.

118
00:08:26,430 --> 00:08:30,750
So let me just tell you the recommended size of the split.

119
00:08:30,750 --> 00:08:37,440
Well I recommend to have 80 percent observation in the training set and 20 percent in the test.

120
00:08:37,440 --> 00:08:37,790
All right.

121
00:08:37,800 --> 00:08:45,450
This is a very good split and therefore here we are going to input a new parameter which is test size

122
00:08:45,990 --> 00:08:49,220
and we'll set that equal to 0 point 2.

123
00:08:49,270 --> 00:08:49,680
Right.

124
00:08:49,680 --> 00:08:56,580
20 percent observations will go into the test and therefore here since we have 10 observations in this

125
00:08:56,580 --> 00:09:02,010
data set that means that eight observations will go into the training set many eight customers will

126
00:09:02,010 --> 00:09:04,620
go into the training set and two in the test.

127
00:09:04,710 --> 00:09:09,510
And this is not necessarily the last two you know they will be taken randomly but eight of them will

128
00:09:09,510 --> 00:09:12,180
go into the training set and two in the test.

129
00:09:12,180 --> 00:09:12,870
All right.

130
00:09:13,050 --> 00:09:19,710
And now we'll add one final argument just for teaching purposes so that we can have the same results

131
00:09:19,860 --> 00:09:26,070
displayed in here you know in the notebook because then I'm going to run some prints to show you these

132
00:09:26,070 --> 00:09:30,600
four elements returned by this train test dysfunction you know the training set in the test set and

133
00:09:30,600 --> 00:09:36,240
since there are some random factors there are going to happen during this split right because the observations

134
00:09:36,240 --> 00:09:41,040
will be randomly split it into the training set in the test well to make sure we have the same random

135
00:09:41,040 --> 00:09:50,730
factors will just at year random state equals one right we're just fixing the seat here so that we'll

136
00:09:50,730 --> 00:09:55,240
get the same split and therefore the same training set and same tested.

137
00:09:55,260 --> 00:09:56,010
All right.

138
00:09:56,010 --> 00:09:56,910
And that's it.

139
00:09:56,910 --> 00:09:57,150
Right.

140
00:09:57,150 --> 00:10:01,230
This is the code to split the data set into the training set in a test.

141
00:10:01,230 --> 00:10:04,890
Let me zoom out a bit so that you can see it.

142
00:10:04,980 --> 00:10:05,210
All right.

143
00:10:05,210 --> 00:10:06,750
So that's the full code.

144
00:10:06,750 --> 00:10:08,630
This will return indeed.

145
00:10:08,820 --> 00:10:15,630
These four new sets composed of the training set an extra and white train and a test set in excess and

146
00:10:15,630 --> 00:10:16,530
white test.

147
00:10:16,530 --> 00:10:17,790
Let me show you this right away.

148
00:10:17,850 --> 00:10:22,780
So we're going to add four new code cells here right.

149
00:10:22,830 --> 00:10:26,360
And we're going to print each of these created sets.

150
00:10:26,370 --> 00:10:29,930
So first we're going to print x train.

151
00:10:30,090 --> 00:10:31,450
Let me copy this.

152
00:10:31,530 --> 00:10:38,050
Then we're going to print x test then we're going to print.

153
00:10:38,310 --> 00:10:40,830
Why train.

154
00:10:40,890 --> 00:10:45,710
And finally we're going to print why test.

155
00:10:45,870 --> 00:10:46,560
Perfect.

156
00:10:46,560 --> 00:10:52,700
All right so now let's execute everything starting with this cell here spreading the dataset into transit

157
00:10:52,700 --> 00:10:53,850
and has it done.

158
00:10:53,850 --> 00:10:54,440
Perfect.

159
00:10:54,450 --> 00:10:55,930
Run successfully.

160
00:10:55,930 --> 00:10:59,140
Now let's run the cell to print x train.

161
00:10:59,280 --> 00:11:03,560
And as you can see indeed we have now eight observations industry and.

162
00:11:03,660 --> 00:11:03,930
Right.

163
00:11:03,930 --> 00:11:10,860
One two three four five six seven eight which correspond to eight customers taken randomly from this

164
00:11:10,860 --> 00:11:18,570
data set and we clearly recognize the features here with first the three columns being the one hot encoded

165
00:11:18,990 --> 00:11:25,360
variables that encode the country categorical variable we also call that dummy variables.

166
00:11:25,590 --> 00:11:30,560
Then we clearly have here the age as the second variable as the second feature you know.

167
00:11:30,570 --> 00:11:31,770
And then the salary.

168
00:11:31,800 --> 00:11:37,090
So we clearly have a great matrix of features for the training set.

169
00:11:37,110 --> 00:11:37,440
All right.

170
00:11:37,440 --> 00:11:37,900
Perfect.

171
00:11:37,900 --> 00:11:44,450
Now let's print access to we'll get here two observations containing the same features here as here

172
00:11:44,460 --> 00:11:44,690
right.

173
00:11:44,700 --> 00:11:46,810
This is the matrix of features still.

174
00:11:46,920 --> 00:11:53,130
So we have the dummy variables here in the first three columns then the age and the two salaries of

175
00:11:53,130 --> 00:11:57,620
our two customers taken randomly from the data set into this test set.

176
00:11:57,750 --> 00:11:58,900
Then why train.

177
00:11:58,890 --> 00:12:05,670
So here we'll get eight per chased decisions right with the zeros and ones here that were encoded before

178
00:12:05,670 --> 00:12:07,040
with label encoder.

179
00:12:07,380 --> 00:12:13,680
And of course make sure to understand this these eight per chased decisions correspond of course to

180
00:12:13,860 --> 00:12:19,410
the eight same customers of this matrix of features extra in the training set right.

181
00:12:19,410 --> 00:12:22,440
These features correspond to these purchase decisions.

182
00:12:22,440 --> 00:12:24,670
These are the same customers here.

183
00:12:24,750 --> 00:12:32,310
And finally why test which will output to result meaning to purchase decisions right 0 and 1 corresponding

184
00:12:32,310 --> 00:12:37,840
of course to the same customers as in this matrix of features of the test.

185
00:12:37,860 --> 00:12:38,340
All right.

186
00:12:38,520 --> 00:12:39,540
So there you go.

187
00:12:39,540 --> 00:12:40,320
Congratulations.

188
00:12:40,320 --> 00:12:45,930
Now you have a new tool in your data processing tool kit splitting the data set into the training said

189
00:12:46,010 --> 00:12:52,710
Ed said not only you have this tool but also you have the final answer to the ultimate question.

190
00:12:52,710 --> 00:12:58,980
Do we have to apply feature scaling before or after the split and it's clearly after the split to avoid

191
00:12:59,010 --> 00:13:04,590
indeed information leakage because simply the test set is supposed to be something you write something

192
00:13:04,890 --> 00:13:09,450
on which we evaluate our model on you observations.

193
00:13:09,450 --> 00:13:10,750
All right great.

194
00:13:10,770 --> 00:13:16,650
So I'm glad that you are really making progress here with new tools and new knowledge that actually

195
00:13:16,650 --> 00:13:18,930
reduce any kind of confusion.

196
00:13:18,930 --> 00:13:25,470
So now we're going to move on to our final tool right feature scaling which now you know must be applied

197
00:13:25,530 --> 00:13:31,470
after the split and you will see what we'll get with some other prints after we deploy this tool on

198
00:13:31,590 --> 00:13:32,600
our data set.

199
00:13:32,610 --> 00:13:38,400
So I can't wait to show this to you and I can't wait to give you this last final tool in your toolkit

200
00:13:38,730 --> 00:13:40,110
because then what does it mean.

201
00:13:40,170 --> 00:13:46,290
That means that we will be 100 percent ready to start building our future machine learning models.
