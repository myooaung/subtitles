1
00:00:00,330 --> 00:00:01,260
Hello my friend.

2
00:00:01,260 --> 00:00:02,490
Happy to see you again.

3
00:00:02,490 --> 00:00:08,100
Now for this new section and mostly this new practical activity where we're going to build together

4
00:00:08,220 --> 00:00:10,710
the random forest regression.

5
00:00:10,740 --> 00:00:16,800
So this is our last model of the regression part and we're going to build it very quickly and efficiently

6
00:00:16,830 --> 00:00:21,630
because actually it looks very much like the decision tree regression model.

7
00:00:21,690 --> 00:00:27,300
So we will be very efficient and besides right after this section comes the most important section for

8
00:00:27,300 --> 00:00:34,020
you it is the section where Kiril and I will explain to you how to not only evaluate your regression

9
00:00:34,020 --> 00:00:37,000
models but also how to select the best one.

10
00:00:37,050 --> 00:00:42,420
So right after we're done with the random forest regression we will actually have a new dataset with

11
00:00:42,420 --> 00:00:49,020
you know multiple features you know like more real world data set and I will show you how to use your

12
00:00:49,110 --> 00:00:55,950
regression code templates to quickly plug your regression models onto the dataset and quickly find the

13
00:00:55,950 --> 00:00:56,580
best one.

14
00:00:56,580 --> 00:00:56,870
All right.

15
00:00:56,880 --> 00:01:00,140
So that will be very important for you to learn how to handle.

16
00:01:00,360 --> 00:01:05,270
But now let's smashed our last regression model the random forced regression.

17
00:01:05,400 --> 00:01:09,050
And before we start let's just make sure everyone here is on the same page.

18
00:01:09,090 --> 00:01:13,240
I give you the link to this folder right before this material in the article.

19
00:01:13,380 --> 00:01:15,270
So make sure to connect to that link.

20
00:01:15,270 --> 00:01:17,100
And now we should all be on the same page.

21
00:01:17,130 --> 00:01:20,260
So we're all going to go into part two regression.

22
00:01:20,400 --> 00:01:25,050
Then we're going to go this time to the last regression model which is our Random Forest regression

23
00:01:25,350 --> 00:01:31,650
and then we're going to start as usual with Python and this folder contains once again the position

24
00:01:31,650 --> 00:01:38,040
salaries data set and of course the random forest regression implementation in the API and before match

25
00:01:38,040 --> 00:01:42,800
which can therefore open with either Google collaboratively or Jupiter notebook.

26
00:01:42,870 --> 00:01:48,200
And as far as I'm concerned as usual I'm going to open it with Google collaboratively.

27
00:01:48,210 --> 00:01:49,230
So there we go.

28
00:01:49,260 --> 00:01:53,960
Let's start implementing the random forest regression model.

29
00:01:54,030 --> 00:02:00,000
Now it is you know laying out the notebook and in a second we should have it.

30
00:02:00,000 --> 00:02:01,230
There we go.

31
00:02:01,230 --> 00:02:01,800
All right.

32
00:02:01,830 --> 00:02:04,830
So as usual this is an read only mode.

33
00:02:04,830 --> 00:02:09,400
We're going to quickly create a copy so that we can re implement it.

34
00:02:09,690 --> 00:02:15,900
We're not going to be implemented from scratch because it is really similar as The Decision Tree regression

35
00:02:15,900 --> 00:02:16,540
model.

36
00:02:16,620 --> 00:02:23,070
So you will see that we will only re implement one code cell and if you know you come here in this regression

37
00:02:23,070 --> 00:02:25,950
folder for the first time with random force regression.

38
00:02:25,950 --> 00:02:30,880
Well I encourage you to have a look at Decision Tree regression first because indeed all these code

39
00:02:30,880 --> 00:02:32,160
cells were explained right.

40
00:02:32,190 --> 00:02:35,920
But this time we're only going to delete this one.

41
00:02:36,000 --> 00:02:41,670
That's where we you know train the random for regression model on the whole data set and then all the

42
00:02:41,670 --> 00:02:42,540
rest of the same.

43
00:02:42,570 --> 00:02:45,560
It is all the same as Decision Tree regression.

44
00:02:45,570 --> 00:02:49,250
We import first the libraries then we import the data set.

45
00:02:49,250 --> 00:02:53,730
Then after you know the training of the random force regression model on the whole there is said we

46
00:02:53,730 --> 00:02:57,570
predict this new result which is the exact same syntax.

47
00:02:57,580 --> 00:03:00,090
Actually I'm going to hide this now.

48
00:03:00,090 --> 00:03:04,040
So this is the exact same syntax as The Decision Tree regression model.

49
00:03:04,080 --> 00:03:05,610
And then here that's the same.

50
00:03:05,610 --> 00:03:11,790
This is the exact same code that we implement visualize the random forest regression results.

51
00:03:11,790 --> 00:03:17,370
All right so let's just keep this because we did it many times and I'm sure you're looking forward to

52
00:03:17,610 --> 00:03:22,860
that final section where you know everything is going to make sense because indeed you will learn how

53
00:03:22,860 --> 00:03:28,680
to use this regression folder containing all these code templates for regression and you will learn

54
00:03:28,680 --> 00:03:33,930
how to understand which model to choose and you know select the best one for your data set.

55
00:03:33,930 --> 00:03:36,480
I will explain everything in this last section.

56
00:03:36,600 --> 00:03:43,200
But for now let's implement that only missing code cell to train the random forest regression model

57
00:03:43,440 --> 00:03:45,030
on the whole dataset.

58
00:03:45,060 --> 00:03:50,700
So let's add a new code cell here and now there you go you could once again totally do it yourself by

59
00:03:50,910 --> 00:03:56,700
looking at some documentation online and actually well let's do it together this time we're gonna pretend

60
00:03:56,790 --> 00:04:01,770
that you know we would like to build a random forest regression model and turn it on the dataset and

61
00:04:01,770 --> 00:04:07,650
that I have absolutely no clue on how to build it or you know on which side could learn class to use

62
00:04:07,800 --> 00:04:08,850
to build it.

63
00:04:08,880 --> 00:04:09,630
So let's see.

64
00:04:09,630 --> 00:04:14,630
Well what I would do as I said would be to go to Google or being.

65
00:04:14,850 --> 00:04:17,610
So here is Google and I would type here in a search bar.

66
00:04:17,880 --> 00:04:21,100
Well you know for example cited learn.

67
00:04:21,150 --> 00:04:21,630
All right.

68
00:04:21,630 --> 00:04:24,610
And then random forest regression.

69
00:04:24,720 --> 00:04:25,790
This one right.

70
00:04:25,920 --> 00:04:27,320
Even the suggestion helps.

71
00:04:27,330 --> 00:04:28,230
That's perfect.

72
00:04:28,290 --> 00:04:34,590
Then pressing enter and then I would go for the link cycle learn you know the cycle learn web site and

73
00:04:34,590 --> 00:04:37,830
this will usually be the first link as it is the case right here.

74
00:04:37,950 --> 00:04:39,270
So let's click this.

75
00:04:39,510 --> 00:04:46,590
And normally I should find site the exact name of the random forest regression class and also the name

76
00:04:46,590 --> 00:04:48,840
of the module that contains this class.

77
00:04:48,840 --> 00:04:52,590
And indeed that is exactly what we see in big here.

78
00:04:52,590 --> 00:04:54,650
This is the whole library cycle learn.

79
00:04:54,660 --> 00:04:59,460
This is the module that contains the class we want and this is the name of the class.

80
00:04:59,460 --> 00:05:00,580
So there you go.

81
00:05:00,590 --> 00:05:06,890
Well let's actually take everything here and we will get this bitch because this is not exactly what

82
00:05:06,890 --> 00:05:10,790
we have to write in Python but I'm sure you'll know how to adapt this.

83
00:05:10,790 --> 00:05:11,270
There you go.

84
00:05:11,270 --> 00:05:12,460
That's why I wanted to show you.

85
00:05:12,460 --> 00:05:18,980
You know it's very very easy to find online the name of a class that allows to build the model you want

86
00:05:19,130 --> 00:05:19,420
right.

87
00:05:19,430 --> 00:05:24,860
I just had to type cycled learn and random forest regression and just go to the first link so you see.

88
00:05:24,890 --> 00:05:26,280
Very very easy.

89
00:05:26,280 --> 00:05:31,670
So let's go back to our implementation and let's start building this random forest regression model

90
00:05:31,880 --> 00:05:34,010
and train it on the whole dataset.

91
00:05:34,010 --> 00:05:40,220
Now I'm gonna based where I've just copied because indeed in order to import this class I simply need

92
00:05:40,220 --> 00:05:47,400
to add here at the beginning of from right from cyclone and then from the end symbol module of psychic

93
00:05:47,400 --> 00:05:47,840
return.

94
00:05:48,230 --> 00:05:54,440
Well I'm going to import that random forest regressive class and perfect.

95
00:05:54,560 --> 00:05:57,440
And now you know what the next natural step is.

96
00:05:57,440 --> 00:06:03,500
Well it is of course to create a new object of this random forest regressive class and we're going to

97
00:06:03,500 --> 00:06:05,560
call this object as usual right.

98
00:06:05,630 --> 00:06:06,850
Correct sir.

99
00:06:07,100 --> 00:06:09,110
Which will be equal to.

100
00:06:09,320 --> 00:06:12,180
Well you know an instance of this class.

101
00:06:12,230 --> 00:06:17,510
That's why I'm copying it and basing it here and adding some parentheses.

102
00:06:17,510 --> 00:06:18,020
All right.

103
00:06:18,050 --> 00:06:22,640
So this time do you think we have to input something in this class.

104
00:06:22,640 --> 00:06:26,860
Well let's not fall into the trap of too much easiness.

105
00:06:26,870 --> 00:06:32,810
Let's say remember that for a decision tree regression we actually did not input you know an essential

106
00:06:32,810 --> 00:06:33,610
parameter.

107
00:06:33,650 --> 00:06:38,840
We just input a random state factor in order to fix the seeds so that we can all have the same results

108
00:06:38,840 --> 00:06:40,730
displayed in the output.

109
00:06:40,730 --> 00:06:45,460
But this time there is actually a parameter that is very important.

110
00:06:45,470 --> 00:06:47,220
I'm sure you guess which one it is.

111
00:06:47,330 --> 00:06:52,790
And not only it is important but also you can tune it you know you can try several values of this parameter

112
00:06:52,790 --> 00:06:58,980
to see which value leads to the best accuracy and therefore the best random forest regress.

113
00:06:59,600 --> 00:07:05,690
And that parameter is well of course as you might guess the number of trees because remember a random

114
00:07:05,690 --> 00:07:10,400
forest is a bunch of trees teaming up to output a final operation.

115
00:07:10,420 --> 00:07:16,370
You know each tree makes its own prediction and then the random forest itself judges the final ones

116
00:07:16,460 --> 00:07:17,000
output.

117
00:07:17,030 --> 00:07:23,330
And so that number of trees is important and that's why we will specify we will choose actually a number

118
00:07:23,330 --> 00:07:24,090
of trees here.

119
00:07:24,110 --> 00:07:27,680
And I usually recommend to start with 10 trees.

120
00:07:27,680 --> 00:07:30,110
So that's exactly what we're going to input here.

121
00:07:30,110 --> 00:07:36,200
The name of that parameter is an underscore estimate or is just like that.

122
00:07:36,200 --> 00:07:40,020
And as we said we're going to say equal to 10 10 trees.

123
00:07:40,070 --> 00:07:40,340
Right.

124
00:07:40,340 --> 00:07:41,280
Ten estimates.

125
00:07:41,330 --> 00:07:42,950
Each tree is an estimate.

126
00:07:43,670 --> 00:07:44,030
Okay.

127
00:07:44,030 --> 00:07:48,670
And then as usual we're going to add the random underscore state parameter.

128
00:07:48,700 --> 00:07:55,160
And once again we'll set that equal to zero so that we can fix the seeds and get the same output displayed

129
00:07:55,310 --> 00:07:56,780
in our notebook.

130
00:07:56,780 --> 00:07:57,650
Okay great.

131
00:07:57,650 --> 00:08:00,660
And now final step you know this step by heart.

132
00:08:00,680 --> 00:08:08,620
Now it is so obvious for you it is of course to fit the regress or object to the whole dataset.

133
00:08:08,630 --> 00:08:12,910
In other words that means we are training the regress or on the whole dataset.

134
00:08:12,920 --> 00:08:13,160
Right.

135
00:08:13,160 --> 00:08:20,090
So here we have to add a dot and then fit applied to X and Y.

136
00:08:20,180 --> 00:08:20,880
Perfect.

137
00:08:21,050 --> 00:08:26,690
So and that's all we had to do you know for this random forest regression implementation all the rest

138
00:08:26,930 --> 00:08:29,420
is the same as the decision tree.

139
00:08:29,420 --> 00:08:34,260
So we can just execute all the cells here and observe comfortably in our chair.

140
00:08:34,280 --> 00:08:35,180
The final result.

141
00:08:35,190 --> 00:08:36,080
So let's have a look.

142
00:08:36,110 --> 00:08:38,530
I just want to warn you again that it's not going to be pretty.

143
00:08:38,570 --> 00:08:43,610
And that's of course for the exact same reason as decision trees a random forest regression model is

144
00:08:43,610 --> 00:08:49,790
way better adapted to high dimensional data sets or you know datasets with multiple features which you

145
00:08:49,790 --> 00:08:56,180
will see in the final section of this part to regression when not only learning how to evaluate your

146
00:08:56,180 --> 00:09:02,960
regression models but also on how to select the best model for any dataset you know for a particular

147
00:09:02,960 --> 00:09:04,760
dataset you're working with.

148
00:09:04,760 --> 00:09:05,120
All right.

149
00:09:05,150 --> 00:09:06,010
So let's do this.

150
00:09:06,030 --> 00:09:11,780
Let's execute each of these cells starting with important libraries.

151
00:09:12,020 --> 00:09:17,100
Then I'm not going to forget I almost forgot to upload the data set.

152
00:09:17,150 --> 00:09:22,230
You know I was about to execute the cell but that were never returned to Nara because indeed the dataset

153
00:09:22,250 --> 00:09:23,530
needs to be applauded.

154
00:09:23,600 --> 00:09:25,400
So let's upload it now.

155
00:09:25,400 --> 00:09:25,750
All right.

156
00:09:25,760 --> 00:09:29,220
So as usual we have to go into our machinery is that folder.

157
00:09:29,390 --> 00:09:35,540
Wherever you put it on your machine then part two regression then Section 9 and last section of this

158
00:09:35,540 --> 00:09:38,870
part to random forest regression and python.

159
00:09:38,930 --> 00:09:39,500
And there we go.

160
00:09:39,500 --> 00:09:40,460
Position salaries.

161
00:09:40,460 --> 00:09:43,540
This is still of course the same dataset.

162
00:09:43,540 --> 00:09:44,200
Okay.

163
00:09:44,220 --> 00:09:45,200
So that's all good.

164
00:09:45,620 --> 00:09:49,730
Now we have the data sets and now we can import it inside the notebook.

165
00:09:49,730 --> 00:09:56,610
Well actually inside our python program and now we're going to train the running for his regression

166
00:09:56,610 --> 00:09:58,480
more on the whole data set.

167
00:09:58,550 --> 00:10:04,850
So let's do this and this will output the running for its regressive model with all the parameters and

168
00:10:04,850 --> 00:10:12,230
at this stage the only parameter that I recommend to tune is indeed that number of estimates which we

169
00:10:12,230 --> 00:10:13,810
chose to be equal to 10.

170
00:10:13,910 --> 00:10:15,640
All right and don't worry too much about the rest.

171
00:10:15,650 --> 00:10:19,150
Now this will already give you an excellent model.

172
00:10:19,310 --> 00:10:26,340
And then now let us predict the final result by you know just calling this predict method from our regress

173
00:10:26,340 --> 00:10:29,570
or object and predict method just has to take as input.

174
00:10:29,570 --> 00:10:36,140
Well that position level number six point five which remember you have to input in a double pair of

175
00:10:36,140 --> 00:10:41,730
square brackets because the brick method expect a 2D array as its input.

176
00:10:41,840 --> 00:10:42,080
Right.

177
00:10:42,080 --> 00:10:43,630
So that's very important for you to know.

178
00:10:43,640 --> 00:10:45,210
But we saw this many time.

179
00:10:45,230 --> 00:10:48,730
I'm sure it has become also very obvious for you.

180
00:10:48,830 --> 00:10:54,730
So let's do this let's get this prediction and we get while we get a pretty good prediction actually.

181
00:10:54,740 --> 00:11:00,740
One hundred and sixty seven thousand dollars which is very close to you know that salary that this person

182
00:11:01,100 --> 00:11:06,830
mentioned to earn in the previous company which was 160 K so that's very very good.

183
00:11:07,220 --> 00:11:09,560
And now let's visualize the final result.

184
00:11:09,740 --> 00:11:13,790
Oh I actually forgot to delete that output but that's fine.

185
00:11:13,790 --> 00:11:17,090
Let's run the sale and we'll get that output again.

186
00:11:17,090 --> 00:11:23,900
And here is the regression curve of the random for us regression model and of course it looks like a

187
00:11:23,900 --> 00:11:30,590
lot the one of the decision trees although this time there are more steps of the stairs you know remember

188
00:11:30,590 --> 00:11:35,300
for the decision tree regression model we had a step for each of the position level.

189
00:11:35,540 --> 00:11:39,480
And here for example we have two steps between two position levels.

190
00:11:39,490 --> 00:11:45,440
So that's of course because we have more trees this time and therefore more splits of you know the features

191
00:11:45,440 --> 00:11:47,730
where you have the same prediction.

192
00:11:47,810 --> 00:11:49,690
You know the average of the predicted salary.

193
00:11:50,030 --> 00:11:52,940
So it only makes sense that there are more steps.

194
00:11:53,000 --> 00:11:53,390
Right.

195
00:11:53,390 --> 00:11:54,350
Same here as well.

196
00:11:54,350 --> 00:11:55,790
And same here as well.

197
00:11:56,480 --> 00:11:59,770
But once again this is not really relevant in 2D.

198
00:11:59,780 --> 00:12:01,830
I just wanted you to see still.

199
00:12:01,880 --> 00:12:07,820
Well the regression curve in 2D which is the only way we can see it unless we put something in 3D.

200
00:12:07,910 --> 00:12:13,640
Well you know over three dimensions it's clearly impossible to visualize a regression hyper plain so

201
00:12:14,090 --> 00:12:16,250
that's why I kept two dimensions here.

202
00:12:16,280 --> 00:12:18,110
But once again don't worry.

203
00:12:18,110 --> 00:12:23,750
In this last section we will have a real world data set with multiple features and we will try each

204
00:12:23,840 --> 00:12:30,170
and every single one of our regression models on this data set and I will teach you how to figure out

205
00:12:30,320 --> 00:12:31,640
which ones to select.

206
00:12:31,640 --> 00:12:35,420
Indeed that's very often ask questions in the data science community.

207
00:12:35,430 --> 00:12:40,340
You know you have all these models but which one should you go for you know which one should you select

208
00:12:40,550 --> 00:12:41,560
for your data sets.

209
00:12:41,570 --> 00:12:46,640
And that's exactly what I'll teach you in this last section of Part Two regression.

210
00:12:46,640 --> 00:12:47,150
All right.

211
00:12:47,180 --> 00:12:48,930
So congratulations.

212
00:12:48,950 --> 00:12:52,750
That was your final regression model of part two.

213
00:12:52,760 --> 00:12:58,190
You now have a complete toolkit of regression models which gives you a lot of different options and

214
00:12:58,190 --> 00:13:02,260
solutions for your future data set and future machinery problems.

215
00:13:02,270 --> 00:13:05,370
So I'm really happy for you that we built together this toolkit.

216
00:13:05,390 --> 00:13:06,950
Make sure to use it the right way.

217
00:13:07,040 --> 00:13:12,230
And now we will finish with this last section to teach you how to use this regression tool kit.

218
00:13:12,290 --> 00:13:17,070
The best way you know by selecting the best model for any dataset.

219
00:13:17,240 --> 00:13:17,750
All right.

220
00:13:17,750 --> 00:13:19,600
So join me in this next section.

221
00:13:19,610 --> 00:13:21,460
I can't wait to see you again.

222
00:13:21,500 --> 00:13:23,300
And until then enjoy machine learning.
