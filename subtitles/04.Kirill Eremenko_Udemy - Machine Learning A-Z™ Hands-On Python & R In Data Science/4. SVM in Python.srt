1
00:00:00,150 --> 00:00:06,000
Hello my friends and welcome to this new practical activity on support vector machines.

2
00:00:06,000 --> 00:00:07,950
For clarification of course.

3
00:00:07,950 --> 00:00:08,260
All right.

4
00:00:08,280 --> 00:00:14,440
So we already built to classification models logistic regression and K and then we got the best result

5
00:00:14,440 --> 00:00:15,890
so far with Ken.

6
00:00:15,990 --> 00:00:20,440
And now let's see if we can beat it or even the one after that.

7
00:00:20,560 --> 00:00:21,880
Colonel SVM.

8
00:00:22,140 --> 00:00:22,370
All right.

9
00:00:22,380 --> 00:00:27,240
So before we start as usual let's make sure everyone here is on the same page right before this tutorial

10
00:00:27,300 --> 00:00:29,550
I give you the link to this whole folder.

11
00:00:29,610 --> 00:00:31,490
So make sure to connect to that link.

12
00:00:31,490 --> 00:00:38,550
And if that's the case then follow me into Part 3 classification and then Section 16 support vector

13
00:00:38,550 --> 00:00:40,100
machine as VM.

14
00:00:40,350 --> 00:00:43,020
And we're going to start with Python of course as usual.

15
00:00:43,020 --> 00:00:45,770
And in this python folder you will get two files.

16
00:00:45,780 --> 00:00:47,920
The first is the same data set.

17
00:00:47,920 --> 00:00:55,410
Social network adds that C is v containing 400 observations where each observation is actually a customer

18
00:00:55,650 --> 00:00:57,560
who but yes or no.

19
00:00:57,720 --> 00:01:01,290
SUV that is advertised on social networks.

20
00:01:01,290 --> 00:01:06,960
And for each of these customers you have the age the estimated salaries of these are the two features.

21
00:01:07,050 --> 00:01:12,600
And with these two features you will predict the dependent viable purchased meaning whether or not the

22
00:01:12,600 --> 00:01:14,860
customers but the SUV.

23
00:01:14,880 --> 00:01:21,720
So one means yes the customer but previous SUV and zero means no the customer didn't buy any SUV.

24
00:01:21,720 --> 00:01:22,340
All right.

25
00:01:22,380 --> 00:01:23,590
So same data set.

26
00:01:23,670 --> 00:01:29,550
And of course the second factor is this support vector machine implementation and the API and B format

27
00:01:29,580 --> 00:01:33,480
which can either open with Google collaboratively or GP a notebook.

28
00:01:33,480 --> 00:01:37,170
And as far as I'm concerned I'm going to open it with Google collaborator.

29
00:01:37,200 --> 00:01:39,840
But feel free to choose your favorite ideas.

30
00:01:40,530 --> 00:01:41,030
All right.

31
00:01:41,070 --> 00:01:44,660
So let's put that out here actually here.

32
00:01:44,730 --> 00:01:47,390
And right now it is loading the notebook laying it out.

33
00:01:47,460 --> 00:01:50,040
And in a second we should have it open.

34
00:01:50,040 --> 00:01:51,240
There we go.

35
00:01:51,240 --> 00:01:51,450
All right.

36
00:01:51,450 --> 00:01:54,660
So that's the whole support vector machine implementation.

37
00:01:54,660 --> 00:01:57,570
And of course it is exactly the same as before.

38
00:01:57,570 --> 00:02:03,930
In order to re implement this we will only have to change the code cell where we build and train this

39
00:02:03,930 --> 00:02:10,020
model because indeed this implementation results from the exact same classification template that we

40
00:02:10,020 --> 00:02:15,870
made when we built the logistic regression model and we saw clearly when implementing the Kenya's neighbours

41
00:02:15,870 --> 00:02:17,360
model how indeed.

42
00:02:17,400 --> 00:02:22,170
We only had to change one cell and how this template worked super well for that model.

43
00:02:22,440 --> 00:02:25,770
So here for SVM we're going to do exactly the same.

44
00:02:25,770 --> 00:02:30,210
We're just going to leave all the cells as they are as they actually were in the logistic regression

45
00:02:30,210 --> 00:02:35,430
model and we will only re implement the cell where we build the SVM.

46
00:02:35,430 --> 00:02:35,830
All right.

47
00:02:35,850 --> 00:02:41,310
So let's do this let's create a new copy of this file because this file isn't read on mode.

48
00:02:41,590 --> 00:02:44,170
Let's click here save a copy and drive.

49
00:02:44,280 --> 00:02:50,430
And this will create a copy inside which we will indeed be able to modify the implementation and mostly

50
00:02:50,430 --> 00:02:54,570
to re implement that could sell to be the SVM model.

51
00:02:54,570 --> 00:02:55,130
All right.

52
00:02:55,140 --> 00:02:55,970
Perfect.

53
00:02:55,980 --> 00:03:02,310
So at the beginning of course we start with the data repricing phase with all the same outputs displayed

54
00:03:02,370 --> 00:03:03,430
on the notebook.

55
00:03:03,450 --> 00:03:04,770
So that's all good.

56
00:03:04,770 --> 00:03:08,520
Then we apply feature scaling because you know it improves the training performance.

57
00:03:08,640 --> 00:03:11,510
And anyway it's never bad to apply feature scaling.

58
00:03:11,550 --> 00:03:13,470
And finally there we go.

59
00:03:13,560 --> 00:03:20,790
That's the cell we have to re implement together because indeed it is the one that defers with respect

60
00:03:20,790 --> 00:03:22,560
to the previous implementations.

61
00:03:22,560 --> 00:03:28,360
So let's click this trust button here to you know read implement it again let's create a new code cell.

62
00:03:28,380 --> 00:03:34,050
And now my friends over to you once again I would like you to please press buzz on the video and try

63
00:03:34,050 --> 00:03:36,380
to implement that could sell yourself.

64
00:03:36,390 --> 00:03:41,160
And that's because I not only want to train you in machine learning but also train you on how to be

65
00:03:41,160 --> 00:03:43,410
independent with machine learning.

66
00:03:43,410 --> 00:03:50,280
So right now the exercise I want you to do is to do some research in the cycle cyclone an API to figure

67
00:03:50,280 --> 00:03:53,880
out which class allows to build the SVM model.

68
00:03:53,940 --> 00:03:59,220
So you will find it very easily actually because there is no trap in the name of the class or the name

69
00:03:59,220 --> 00:04:00,090
of the module.

70
00:04:00,090 --> 00:04:06,240
So I trust you will totally be able to do this exercise successfully and mostly know which method to

71
00:04:06,240 --> 00:04:10,770
use at the end to train that SVM or on the training set.

72
00:04:10,770 --> 00:04:11,210
All right.

73
00:04:11,220 --> 00:04:12,260
So please press buzz.

74
00:04:12,300 --> 00:04:16,970
And now in two seconds I'm gonna give you the solution.

75
00:04:17,200 --> 00:04:18,620
All right let's do this.

76
00:04:18,630 --> 00:04:22,010
So I already have the cyclone API open.

77
00:04:22,020 --> 00:04:26,100
You know that was for the nearest neighbors the key nearest neighbours.

78
00:04:26,100 --> 00:04:28,650
Which we implemented previously in the previous section.

79
00:04:28,650 --> 00:04:35,880
We used this class K neighbors classifier and now the next thing we would like to find in this API documentation

80
00:04:36,210 --> 00:04:41,010
is the module that contains the class that allows to build the SVM model.

81
00:04:41,400 --> 00:04:43,520
So naturally where could we find it.

82
00:04:43,530 --> 00:04:46,770
You know here should we scroll back up or scroll down.

83
00:04:47,100 --> 00:04:50,750
Well let's hope that you know the name of the module starts with an S..

84
00:04:50,790 --> 00:04:54,550
Because here you know the modules are organized by alphabetical order.

85
00:04:54,720 --> 00:04:57,250
So since here we are at n n neighbors.

86
00:04:57,290 --> 00:05:02,900
Let us hope that the name of the module we're looking for starts with a net like support vector machine.

87
00:05:02,910 --> 00:05:06,250
So let's scroll down and random projections.

88
00:05:06,270 --> 00:05:07,400
Mean supervised learning.

89
00:05:07,400 --> 00:05:08,970
And there we go.

90
00:05:08,970 --> 00:05:10,800
Support vector machines.

91
00:05:10,800 --> 00:05:11,570
Hello.

92
00:05:11,580 --> 00:05:15,000
That's exactly what we were looking for support vector machines.

93
00:05:15,000 --> 00:05:18,390
That's not the name of the module the name of the module is as the m the same.

94
00:05:18,450 --> 00:05:21,040
That stands for support vector machines.

95
00:05:21,210 --> 00:05:21,600
All right.

96
00:05:21,600 --> 00:05:22,020
And then.

97
00:05:22,020 --> 00:05:23,870
Well you know the hardest part is done.

98
00:05:24,030 --> 00:05:29,490
Now according to you which estimated as you know because here you have all the basically support back

99
00:05:29,490 --> 00:05:34,470
to machines based machinery morals and so according to you which one do we need to take here.

100
00:05:34,710 --> 00:05:37,550
Well we actually have two options.

101
00:05:37,560 --> 00:05:43,410
We could either take the linear as we see which will directly build the linear support vector machine

102
00:05:43,410 --> 00:05:49,450
model or we can take this one as we see and choose a linear kernel.

103
00:05:49,470 --> 00:05:50,100
All right.

104
00:05:50,100 --> 00:05:56,610
And we will actually go for this option because in the next section we will study the kernel as VM models

105
00:05:56,910 --> 00:06:02,610
which as you might guess allow us to choose some different kernels in our SVM including the linear one

106
00:06:02,790 --> 00:06:07,330
and the nonlinear ones like for example the very famous one orbit.

107
00:06:07,440 --> 00:06:12,860
Right radial basis function which is actually the one we used in as we are you know that's the class

108
00:06:12,870 --> 00:06:18,100
you recognize it that we used to build the edge of your model in the path to regression.

109
00:06:18,120 --> 00:06:18,380
All right.

110
00:06:18,390 --> 00:06:19,530
So there we go.

111
00:06:19,530 --> 00:06:20,820
Let's go inside.

112
00:06:20,820 --> 00:06:24,230
And here is the name of the module as VM of course.

113
00:06:24,240 --> 00:06:30,210
And the class that we must now import in order to build R as your model.

114
00:06:30,210 --> 00:06:37,040
All right so let's start with this as usual let's get you know this path and let's adapt it inside or

115
00:06:37,040 --> 00:06:40,740
coattail you know how to do this we need to start with from.

116
00:06:40,740 --> 00:06:50,330
So from the cyclone library and then the SVM module of the cyclone library we will import the SBC class.

117
00:06:50,570 --> 00:06:51,020
OK.

118
00:06:51,330 --> 00:06:52,440
So that's a first step.

119
00:06:52,440 --> 00:06:57,840
And then the next natural step you know it by heart it is of course to create an object of this class.

120
00:06:57,840 --> 00:07:04,050
And we need to keep here the name classifier because in order to not have anything to change afterwards

121
00:07:04,320 --> 00:07:08,580
so classifier which will be nothing else then the SVM model itself.

122
00:07:08,670 --> 00:07:11,840
And so to create such an object we need to call this class.

123
00:07:11,910 --> 00:07:19,200
I can just type it as V C and then add some parenthesis and now the question is what do we need to input

124
00:07:19,200 --> 00:07:20,580
here as parameters.

125
00:07:21,090 --> 00:07:27,060
Well I hope you have the reflex to think of course we need to input the kernel because as I've just

126
00:07:27,060 --> 00:07:32,910
explained this as we see class allow us to build kernel as and models with either a linear kernel which

127
00:07:32,910 --> 00:07:36,140
is the classic as your model or a nonlinear kernel.

128
00:07:36,150 --> 00:07:41,040
So here in our parameters we'll have to specify that we want a linear kernel because we're starting

129
00:07:41,040 --> 00:07:43,030
with the classic SVM model.

130
00:07:43,080 --> 00:07:48,760
And then in the next section you know on kernel as VM will choose a nonlinear kernel like RBI for polynomial.

131
00:07:48,780 --> 00:07:49,530
We'll see.

132
00:07:49,530 --> 00:07:50,460
But there you go.

133
00:07:50,460 --> 00:07:51,930
That's our first argument.

134
00:07:51,960 --> 00:07:57,540
And indeed when we have a look at the documentation we can see that indeed the second parameter here

135
00:07:57,630 --> 00:07:59,970
is Colonel and the default value is RBI.

136
00:08:00,000 --> 00:08:05,790
So we'll have to specify that we want linear to start with and then as you can see you have many many

137
00:08:05,790 --> 00:08:11,040
other arguments you know parameters and you can check out the description for each of them.

138
00:08:11,130 --> 00:08:15,720
But no worries we won't change their default value we'll just keep their default value.

139
00:08:15,720 --> 00:08:19,830
Like for example the regularization parameter for which we'll keep the default value of 1.

140
00:08:19,890 --> 00:08:24,220
That's fine you'll see in the visualization that there is not much we can do in order to improve the

141
00:08:24,220 --> 00:08:26,130
moral or to avoid or fitting.

142
00:08:26,130 --> 00:08:27,000
So there we go.

143
00:08:27,000 --> 00:08:33,390
The only parameter that we'll take is this one kernel equals not IBF but linear.

144
00:08:33,390 --> 00:08:35,100
All right let's do this.

145
00:08:35,100 --> 00:08:40,380
Kernel equals in quotes linear.

146
00:08:40,440 --> 00:08:41,280
All right.

147
00:08:41,340 --> 00:08:46,890
And then I know I've just said that we won't have to input any other parameters but let's add anyway

148
00:08:46,920 --> 00:08:51,700
you know this one the random state parameter and set it equal to zero.

149
00:08:51,810 --> 00:08:56,760
Just to make sure that we get the same results displayed on our notebook you know because there are

150
00:08:56,760 --> 00:09:02,370
some run factors when we build this SVM model and therefore for teaching purposes it's better if we

151
00:09:02,370 --> 00:09:05,850
all have the same results displayed in our notebook.

152
00:09:05,850 --> 00:09:06,570
All right.

153
00:09:06,570 --> 00:09:07,490
And that's it.

154
00:09:07,500 --> 00:09:08,330
Congratulations.

155
00:09:08,340 --> 00:09:10,820
That builds the SVM morale.

156
00:09:10,820 --> 00:09:12,860
The classic one with the linear kernel.

157
00:09:13,050 --> 00:09:15,280
And now you know how to finish this.

158
00:09:15,420 --> 00:09:23,310
Of course our last here is to take our classifier from which we're going to call the fit method to train

159
00:09:23,580 --> 00:09:30,660
our SVM classifier on the training set and we have to input that here in two parts first matrix of features

160
00:09:30,660 --> 00:09:36,780
of the training set which is x train and then the dependent variable vector of the train set which is

161
00:09:36,780 --> 00:09:38,790
why train all right.

162
00:09:38,790 --> 00:09:41,920
So you know this perfectly well now it's like your native language.

163
00:09:41,970 --> 00:09:42,310
Right.

164
00:09:42,750 --> 00:09:43,050
Okay.

165
00:09:43,050 --> 00:09:44,700
I hope I'm right but there you go.

166
00:09:44,700 --> 00:09:45,660
Congratulations.

167
00:09:45,660 --> 00:09:47,500
This implementation is now over.

168
00:09:47,550 --> 00:09:51,000
Thanks to this very efficient code template.

169
00:09:51,120 --> 00:09:51,700
And so now.

170
00:09:51,720 --> 00:09:55,770
Well we'll just run everything and observe the final results.

171
00:09:55,770 --> 00:09:57,660
Indian All right so let's do this.

172
00:09:57,660 --> 00:10:04,650
Let's not forget to you know import the data set here you know upload it in the notebook so click this

173
00:10:04,650 --> 00:10:05,730
folder here.

174
00:10:05,730 --> 00:10:12,240
Then you will have to wait a few seconds because your notebook will be connecting to a runtime to enable

175
00:10:12,240 --> 00:10:15,840
file browsing and in a second and we should see.

176
00:10:15,840 --> 00:10:16,340
There we go.

177
00:10:16,350 --> 00:10:18,110
The upload button.

178
00:10:18,180 --> 00:10:23,040
So we're going to click that and well you know that's the data set but I'll show you the whole path

179
00:10:23,040 --> 00:10:23,430
again.

180
00:10:23,640 --> 00:10:28,050
So you'll find your machinery is at for code and data sets which you could download in the previous

181
00:10:28,050 --> 00:10:29,580
tutorial in the oracle.

182
00:10:29,580 --> 00:10:36,000
Then you're going to go to part three classification then to Section 16 support vector machine in Python

183
00:10:36,210 --> 00:10:38,190
and social network at that.

184
00:10:38,200 --> 00:10:38,880
Yes.

185
00:10:39,000 --> 00:10:42,660
You click open and this will upload the data set inside the notebook.

186
00:10:42,660 --> 00:10:44,160
There we go now.

187
00:10:44,190 --> 00:10:48,450
Now you can run all the cells by clicking runtime here and then.

188
00:10:48,450 --> 00:10:49,320
Are you ready.

189
00:10:49,320 --> 00:10:50,080
Run.

190
00:10:50,280 --> 00:10:53,220
Oh and this will run all the cell.

191
00:10:53,500 --> 00:10:54,570
And perfect.

192
00:10:54,570 --> 00:11:00,540
Now we have are as VM Morrow with a linear kernel with all the default values of the parameters here

193
00:11:00,540 --> 00:11:06,750
except this one linear kernel and so there we go then we have the rest of the cells when we predict

194
00:11:06,840 --> 00:11:07,620
the new result.

195
00:11:07,620 --> 00:11:12,600
Indeed we get the right prediction remember it's 0 for that particular first customer of the test set

196
00:11:12,690 --> 00:11:16,560
of age 30 an estimated salary eighty seven thousand dollars.

197
00:11:16,560 --> 00:11:20,940
Indeed it is predicted not to by the SUV as is the case in reality.

198
00:11:20,940 --> 00:11:27,060
Okay then when we predict the test results once again we see that we have many correct predictions except

199
00:11:27,060 --> 00:11:27,630
some of them.

200
00:11:27,630 --> 00:11:33,600
This one this one and you know this one okay but it looks very good.

201
00:11:33,720 --> 00:11:39,360
However what we're mostly interested in and we're about to get it right now is to see the accuracy on

202
00:11:39,360 --> 00:11:44,270
the test that you know the number of correct predictions or if you want the number of incorrect predictions.

203
00:11:44,390 --> 00:11:45,000
Are you ready.

204
00:11:45,000 --> 00:11:46,470
We're about to get it right now.

205
00:11:46,660 --> 00:11:47,120
Right.

206
00:11:47,160 --> 00:11:51,580
That code sell prints the computing matrix and display the accuracy and wow.

207
00:11:51,620 --> 00:11:52,250
OK.

208
00:11:52,320 --> 00:11:52,900
Interesting.

209
00:11:52,890 --> 00:11:56,490
So it beat actually the logistic regression model right.

210
00:11:56,490 --> 00:12:00,290
Remember the logistic regression model had an accuracy of 89 percent.

211
00:12:00,780 --> 00:12:04,320
And here the SVM slightly beat it by 1 percent.

212
00:12:04,320 --> 00:12:11,140
However it doesn't beat that low key and a model which remember had an accuracy of 93 percent.

213
00:12:11,160 --> 00:12:11,430
All right.

214
00:12:11,430 --> 00:12:19,020
So it doesn't beat it and now we're actually going to understand why why didn't the SVM moral beat the

215
00:12:19,020 --> 00:12:19,850
K and then.

216
00:12:19,960 --> 00:12:20,300
Okay.

217
00:12:20,310 --> 00:12:22,610
So we'll actually figure that out in a bit.

218
00:12:22,650 --> 00:12:28,410
Or actually you know I can just show it to you here on the original implementation of the SVM but you're

219
00:12:28,440 --> 00:12:31,510
going to understand now why it didn't beat it.

220
00:12:31,530 --> 00:12:32,760
Well there you go.

221
00:12:32,760 --> 00:12:38,800
It didn't beat it because once again since we chose a linear kernel Well the prediction boundary or

222
00:12:38,800 --> 00:12:42,600
you know the decision boundary is actually once again a straight line.

223
00:12:42,600 --> 00:12:46,600
And therefore even if you rotate it either this way this way.

224
00:12:46,650 --> 00:12:52,020
Well it won't be able to catch the right prediction for you know these green customers here which should

225
00:12:52,020 --> 00:12:53,450
belong to the green regions.

226
00:12:53,460 --> 00:12:54,390
And same for this one.

227
00:12:54,390 --> 00:12:56,310
You know if we rotate it this way.

228
00:12:56,430 --> 00:13:00,770
Well we will catch these ones but then we will get more incorrect predictions around here.

229
00:13:00,840 --> 00:13:03,620
And if we rotate this way you know from here to here.

230
00:13:03,630 --> 00:13:08,640
Well indeed we will catch these ones in the right prediction region but we will get more incorrect ones

231
00:13:08,640 --> 00:13:09,240
here.

232
00:13:09,240 --> 00:13:12,240
So that's the problem of linear models here.

233
00:13:12,240 --> 00:13:17,730
Indeed it's much better to have a prediction boundary that does some kind of a curve here in order to

234
00:13:17,730 --> 00:13:24,420
catch only the red customers who should be predicted not by the SUV and leave the green ones in the

235
00:13:24,420 --> 00:13:25,620
green region.

236
00:13:25,620 --> 00:13:26,210
All right.

237
00:13:26,280 --> 00:13:30,620
That's exactly what we got with our K nearest neighbors right.

238
00:13:30,630 --> 00:13:36,630
It is not a smooth curve but it did the job of selecting the right red customers here still leading

239
00:13:36,630 --> 00:13:41,290
some green ones but here catching the right green ones in the right region the green region.

240
00:13:41,400 --> 00:13:44,880
And here of course since we have the straight line it's impossible to do.

241
00:13:44,880 --> 00:13:52,020
But no worries I'm sure you have the intuition that once we use a nonlinear kernel well we'll be able

242
00:13:52,020 --> 00:13:57,550
to catch the right green observations here in the right green region and well that's exactly what we'll

243
00:13:57,570 --> 00:14:02,790
find out in the next section when implementing the kernel SVM mode.

244
00:14:03,300 --> 00:14:03,750
All right.

245
00:14:03,750 --> 00:14:05,250
So let's see if we're done here.

246
00:14:05,250 --> 00:14:05,950
Yes we are.

247
00:14:06,000 --> 00:14:11,730
So it's finished to execute and of course we get the same results and let's see for the test set still

248
00:14:11,730 --> 00:14:15,660
running but and Oh wow.

249
00:14:15,670 --> 00:14:17,490
Okay so funny timing.

250
00:14:17,490 --> 00:14:22,220
It just populated here and well you know still the same because of this straight line here.

251
00:14:22,220 --> 00:14:28,260
Well we have some green customers and customers who but in reality a new SUV that couldn't be bricked

252
00:14:28,260 --> 00:14:34,340
it correctly because they fell in the wrong region because of this brazen boundary that can not separate

253
00:14:34,350 --> 00:14:36,560
well are two classes.

254
00:14:36,600 --> 00:14:36,930
All right.

255
00:14:36,960 --> 00:14:37,710
Same problem.

256
00:14:37,770 --> 00:14:43,710
And you know you might guess that this problem will be fixed once we choose a nonlinear kernel for our

257
00:14:43,830 --> 00:14:45,360
support vector machine model.

258
00:14:45,360 --> 00:14:50,150
All right so let's find out about this in the next section on kernel as SVM.

259
00:14:50,160 --> 00:14:52,000
And until then enjoy machine learning.
