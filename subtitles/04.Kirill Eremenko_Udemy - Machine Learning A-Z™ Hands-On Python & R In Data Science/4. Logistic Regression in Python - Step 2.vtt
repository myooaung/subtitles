WEBVTT
1
00:00:00.390 --> 00:00:01.740
Hello my friends all right.

2
00:00:01.770 --> 00:00:03.460
Let's see if you got this right.

3
00:00:03.480 --> 00:00:08.940
So I asked you at the end of the previous tutorial to implement yourself the data pricing phase for

4
00:00:08.940 --> 00:00:14.670
this logistic regression model and now we're about to implement that solution together.

5
00:00:14.670 --> 00:00:15.040
All right.

6
00:00:15.060 --> 00:00:16.200
So what did we have to do.

7
00:00:16.200 --> 00:00:22.890
Well first the first natural step was to go to our data pricing template to grab all the essential tools

8
00:00:22.890 --> 00:00:23.760
which are indeed.

9
00:00:23.760 --> 00:00:27.830
Well the first steps of the data pricing phase so let's do this.

10
00:00:27.840 --> 00:00:31.030
That's the first thing you were supposed to do.

11
00:00:31.080 --> 00:00:39.500
So copying this cell and then I'm basing it right here in a new code cell for importing the libraries.

12
00:00:39.570 --> 00:00:40.290
There we go.

13
00:00:40.290 --> 00:00:40.860
First step.

14
00:00:40.860 --> 00:00:48.120
Done now to import the data set while same let's go to our data processing template which indeed is

15
00:00:48.120 --> 00:00:50.500
meant for us to be efficient.

16
00:00:50.520 --> 00:00:55.170
So I'm copying this and then we'll see what we have to replace.

17
00:00:55.170 --> 00:00:59.180
But once again it is supposed to help you so that you don't have to do much.

18
00:00:59.190 --> 00:00:59.550
All right.

19
00:00:59.550 --> 00:01:06.540
So just pasted it and let's do you know just for less tip of the data pressing phase you know in any

20
00:01:06.540 --> 00:01:10.500
situation regardless of whether or not is needed feature scaling.

21
00:01:10.500 --> 00:01:10.790
All right.

22
00:01:10.800 --> 00:01:20.440
So let's take that cell here and let's copy that right here in a new Kotel.

23
00:01:20.460 --> 00:01:20.720
All right.

24
00:01:20.730 --> 00:01:25.300
So that's the first essential steps you know which happened most of the time when building a machine

25
00:01:25.300 --> 00:01:26.250
learning model.

26
00:01:26.250 --> 00:01:30.990
All right so now next step what do we have to change inside this data repressing phase.

27
00:01:30.990 --> 00:01:33.500
Well let's take it step by step first step.

28
00:01:33.510 --> 00:01:35.700
Of course we don't have anything to change.

29
00:01:35.760 --> 00:01:37.170
Now let's see second step.

30
00:01:37.440 --> 00:01:41.270
Well there you go of course here we have to change the name of the data set.

31
00:01:41.310 --> 00:01:48.100
It is not data that that to be but it is now social network adds that CSB.

32
00:01:48.120 --> 00:01:50.460
So let's do this let's just replace this.

33
00:01:50.460 --> 00:01:52.830
That was what you had to do next.

34
00:01:52.830 --> 00:01:57.780
Social network on this court adds that C is V.

35
00:01:57.870 --> 00:01:59.400
And now next question.

36
00:01:59.460 --> 00:02:01.650
Do we have to change anything here.

37
00:02:01.650 --> 00:02:08.880
Well of course the answer is no no because this data repressing template was meant for you to not have

38
00:02:08.880 --> 00:02:15.750
to change anything in most situations as long as of course you make sure that in your data set you have

39
00:02:15.870 --> 00:02:21.090
the features in the first columns you know these are the two features here and the dependent variable

40
00:02:21.210 --> 00:02:22.360
in the last column.

41
00:02:22.410 --> 00:02:29.370
And since this automatically select all the columns except the last one and this automatically selects

42
00:02:29.430 --> 00:02:30.350
the last column.

43
00:02:30.390 --> 00:02:34.830
And this of course regardless of the number of features you have in your dataset set.

44
00:02:34.860 --> 00:02:35.940
Well there you go.

45
00:02:35.970 --> 00:02:41.610
This will simply select the age and salary in the matrix of features.

46
00:02:41.820 --> 00:02:48.590
And this line will simply select Well the dependent variable in a nice one dy vector.

47
00:02:48.600 --> 00:02:49.410
All right.

48
00:02:49.410 --> 00:02:49.940
So there you go.

49
00:02:49.940 --> 00:02:53.040
That was the reason why I said it was so easy.

50
00:02:53.130 --> 00:02:58.970
You only had to change the name of the dataset here social network ad starts yes.

51
00:02:59.320 --> 00:02:59.610
Okay.

52
00:02:59.610 --> 00:03:04.050
And actually picking up the dataset now that we have the code to import it.

53
00:03:04.320 --> 00:03:10.350
Well let's not forget to upload it in a notebook because in this same tutorial we will actually run

54
00:03:10.440 --> 00:03:11.780
these four cells.

55
00:03:11.910 --> 00:03:13.020
But before this.

56
00:03:13.020 --> 00:03:15.060
Make sure to finish implementing them.

57
00:03:15.060 --> 00:03:19.950
So here we are connecting to a runtime to enable file browsing and in a second.

58
00:03:19.950 --> 00:03:20.810
There you go.

59
00:03:20.820 --> 00:03:22.950
Which should have the upload button.

60
00:03:22.950 --> 00:03:26.880
So now we're going to click it to find our whole machine learning.

61
00:03:26.880 --> 00:03:32.520
It is that folder and we're going to go inside and we're going to go to part three classification then

62
00:03:32.580 --> 00:03:36.240
Section 14 logistic regression python.

63
00:03:36.240 --> 00:03:41.000
And we're going to select our social network at data set.

64
00:03:41.010 --> 00:03:45.920
It is exactly the same with of course the h here the estimated salary and the purchase columns.

65
00:03:45.930 --> 00:03:46.750
There we go.

66
00:03:46.770 --> 00:03:52.890
Let's click open it is going to upload it in our notebook as we go into here.

67
00:03:52.890 --> 00:03:53.700
There you go.

68
00:03:53.700 --> 00:03:54.090
Perfect.

69
00:03:54.090 --> 00:03:55.020
We have it.

70
00:03:55.020 --> 00:03:56.520
So now we're ready to import it.

71
00:03:56.550 --> 00:04:00.650
But first I want to finish the implementation of this therapy pressing phase.

72
00:04:00.660 --> 00:04:06.050
So let's move on here to the next step splitting the data set into the training set and test set and

73
00:04:06.060 --> 00:04:07.710
here do we have anything to change.

74
00:04:07.710 --> 00:04:09.600
Well there you go again of course.

75
00:04:09.600 --> 00:04:12.890
No it is not compulsory to change anything here.

76
00:04:12.900 --> 00:04:18.840
This will automatically create your training set and the test set out of the whole dataset.

77
00:04:18.840 --> 00:04:23.970
You know the whole dataset composed of the matrix of features x and the dependent variable back to Y.

78
00:04:24.100 --> 00:04:29.130
We're going to do some prints right to show you everything because then you know the next step is to

79
00:04:29.370 --> 00:04:30.770
apply feature scaling.

80
00:04:31.080 --> 00:04:32.240
And before we get to this.

81
00:04:32.400 --> 00:04:34.430
Well I just would like to change something here.

82
00:04:34.440 --> 00:04:37.100
But again that was absolutely not necessary.

83
00:04:37.100 --> 00:04:38.970
You could truly live it like that.

84
00:04:39.150 --> 00:04:46.020
But I just want to change the test size because since actually we have four hundred observations in

85
00:04:46.020 --> 00:04:49.930
total you know we have 400 customers in this dataset.

86
00:04:49.980 --> 00:04:55.230
Well you know if we choose a test size of all point twenty five it will be nice because we will have

87
00:04:55.230 --> 00:05:00.880
actually 300 customers in the training set and 100 tumors in the test.

88
00:05:01.080 --> 00:05:01.400
Right.

89
00:05:01.400 --> 00:05:02.980
We'll have nice round numbers.

90
00:05:03.060 --> 00:05:09.900
So just exceptionally here I usually recommend indeed open to as a test site but open 25 is actually

91
00:05:09.900 --> 00:05:11.090
totally fine.

92
00:05:11.100 --> 00:05:16.920
You will see that we will totally be able to train our logistic regression model in the future classification

93
00:05:16.920 --> 00:05:21.290
models on this dataset with 300 observations in the training set.

94
00:05:21.300 --> 00:05:21.760
Okay.

95
00:05:21.930 --> 00:05:24.040
So open 25 is totally fine.

96
00:05:24.060 --> 00:05:30.270
We will have a nice training set of 300 customers and one nice test set of 100 customers.

97
00:05:30.300 --> 00:05:31.140
So there you go.

98
00:05:31.140 --> 00:05:33.250
Now we can move on to the next step.

99
00:05:33.360 --> 00:05:34.980
Feature scaling.

100
00:05:35.130 --> 00:05:36.190
Okay.

101
00:05:36.300 --> 00:05:36.650
All right.

102
00:05:36.660 --> 00:05:41.700
So first of all let me explain the why why do we have to apply features scaling here.

103
00:05:41.700 --> 00:05:46.610
Well that's very simple is actually not required for logistic regression.

104
00:05:46.650 --> 00:05:53.130
However still applying it will improve the training performance and therefore it will also improve the

105
00:05:53.130 --> 00:05:54.370
final predictions.

106
00:05:54.390 --> 00:06:00.720
So you have to understand that for some morals like as we are features killing is an absolute necessity.

107
00:06:00.720 --> 00:06:05.580
However for other models like logistic regression it is not a necessity.

108
00:06:05.730 --> 00:06:11.450
However still applying it will improve the training performance and therefore the final predictions.

109
00:06:11.470 --> 00:06:12.950
So that's the why we're playing it.

110
00:06:12.960 --> 00:06:16.470
But you can totally implement the same without feature scaling.

111
00:06:16.760 --> 00:06:17.240
All right.

112
00:06:17.520 --> 00:06:23.490
So now that we covered the way let's proceed to the how how we're going to implement features getting

113
00:06:23.490 --> 00:06:24.120
here.

114
00:06:24.120 --> 00:06:29.640
Well there is nothing more simple things to our data processing toolkit which I have to put here.

115
00:06:29.640 --> 00:06:30.520
And if you haven't.

116
00:06:30.570 --> 00:06:37.560
Well grab it in part 1 data pricing and now in our toolkit we're going to scroll down to the bottom

117
00:06:37.560 --> 00:06:41.850
to find that feature scanning tool which is actually the last one.

118
00:06:41.860 --> 00:06:43.680
Right here it is.

119
00:06:43.680 --> 00:06:44.790
That's feature scaling.

120
00:06:44.810 --> 00:06:45.750
So here we go.

121
00:06:45.750 --> 00:06:47.340
Let's grab it.

122
00:06:47.350 --> 00:06:49.110
No you just have to grab the first cell.

123
00:06:49.110 --> 00:06:50.670
Implementing the tool.

124
00:06:50.850 --> 00:06:54.310
And now let's go back to our logistic regression implementation.

125
00:06:54.360 --> 00:07:01.310
Let's create a new code cell here for feature scaling and let's paste it right inside.

126
00:07:01.310 --> 00:07:01.760
All right.

127
00:07:01.770 --> 00:07:05.490
And now the last step that you had to do was to figure out what to replace.

128
00:07:05.490 --> 00:07:09.680
Here in this feature scaling implementation well here that's super easy.

129
00:07:09.690 --> 00:07:15.030
We simply want to you know feature scale all the features we want to scale all the features we want

130
00:07:15.030 --> 00:07:20.820
to scale age and salary and of course we don't have to scale the dependent variable per chased because

131
00:07:20.880 --> 00:07:25.980
its values are 0 and 1 and therefore are already in the range of values we want.

132
00:07:25.980 --> 00:07:27.270
So all go with this.

133
00:07:27.270 --> 00:07:33.420
So basically we're only going to scale these two features age and estimated salary and therefore here

134
00:07:33.420 --> 00:07:39.420
what we just had to do you know as replacement was just to remove that selection of the index indexes

135
00:07:39.420 --> 00:07:39.660
here.

136
00:07:39.660 --> 00:07:42.930
Here we selected old indexes starting from three.

137
00:07:42.930 --> 00:07:48.490
But this time we don't have to do anything we can just scale the whole matrix of features.

138
00:07:48.570 --> 00:07:52.930
So I'm just removing here the index selections and there you go.

139
00:07:52.950 --> 00:07:57.170
We'll be ready to feature scale both our training set and our test it.

140
00:07:57.180 --> 00:08:03.240
And I remind that this is absolutely compulsory to do this after splitting the data set into the training

141
00:08:03.240 --> 00:08:09.060
set and to set in order to avoid information leakage from the test set.

142
00:08:09.060 --> 00:08:09.550
All right.

143
00:08:09.630 --> 00:08:10.800
So there you go my friends.

144
00:08:10.800 --> 00:08:16.740
That was what you needed to do for data repressing congratulations if you've got the same thing don't

145
00:08:16.740 --> 00:08:20.140
worry about that test size this is just for the form.

146
00:08:20.190 --> 00:08:21.060
There you go.

147
00:08:21.060 --> 00:08:23.420
This was simply what you had to do.

148
00:08:23.450 --> 00:08:30.240
All right so now we're gonna do a few prints to actually see the before and after feature scaling.

149
00:08:30.240 --> 00:08:36.900
So what I'm gonna do is right after this code cells bring the data set into the training center said.

150
00:08:36.900 --> 00:08:42.330
I'm going to make four prints just to show you if you don't need to look at that well feel free not

151
00:08:42.330 --> 00:08:50.400
to include these new CD sales but what I want to do is print first X train then I would like to print

152
00:08:50.730 --> 00:08:58.310
y trains here by train then next one I would like to print x test.

153
00:08:58.590 --> 00:09:04.050
And finally I would like to print well why test.

154
00:09:04.260 --> 00:09:04.680
OK.

155
00:09:05.160 --> 00:09:12.330
So that's an after feature scaling however we will just you know print to sales because we actually

156
00:09:12.330 --> 00:09:16.400
don't apply feature Skilling to Y and therefore we'll just print first.

157
00:09:16.440 --> 00:09:21.180
Well X train again and second X test.

158
00:09:21.180 --> 00:09:21.450
Right.

159
00:09:21.450 --> 00:09:25.410
Because these will be the only sets of data that will be changed.

160
00:09:25.410 --> 00:09:26.060
All right.

161
00:09:26.100 --> 00:09:26.520
Perfect.

162
00:09:26.570 --> 00:09:28.400
Now let's execute everything.

163
00:09:28.410 --> 00:09:29.460
We have the data set.

164
00:09:29.460 --> 00:09:30.060
All good.

165
00:09:30.060 --> 00:09:36.890
So let's do this starting by imploring the libraries library's good now importing the data set.

166
00:09:37.010 --> 00:09:37.870
Great.

167
00:09:37.900 --> 00:09:41.630
Now splitting the data set into the training set in the test set.

168
00:09:41.640 --> 00:09:42.930
There we go.

169
00:09:42.930 --> 00:09:43.430
All right.

170
00:09:43.440 --> 00:09:46.400
Now let's print x train and see what it looks like.

171
00:09:46.410 --> 00:09:46.830
All right.

172
00:09:46.830 --> 00:09:48.330
Let's scroll down a bit.

173
00:09:48.330 --> 00:09:48.660
All right.

174
00:09:48.690 --> 00:09:52.770
That's extra N was first the Kong age in the age feature.

175
00:09:52.860 --> 00:09:57.180
And second the column of the estimated salary the estimated salary future.

176
00:09:57.180 --> 00:09:57.540
All right.

177
00:09:57.540 --> 00:10:01.620
And of course we have 300 observations in this training set.

178
00:10:01.620 --> 00:10:02.880
You don't have to count them.

179
00:10:03.030 --> 00:10:03.900
But there you go.

180
00:10:03.900 --> 00:10:05.410
We have many of them.

181
00:10:05.430 --> 00:10:05.920
All right.

182
00:10:06.030 --> 00:10:11.250
So now let's print why train it just to have a look at what we create.

183
00:10:11.250 --> 00:10:13.210
You know this is not compulsory.

184
00:10:13.290 --> 00:10:19.560
So that's why train with all the poor chaste decisions on the previous SUV zero means that the customer

185
00:10:19.560 --> 00:10:27.930
did not buy any SUV and one means yes the customer but a previous SUV and now accessed scroll down a

186
00:10:27.930 --> 00:10:28.520
bit.

187
00:10:28.560 --> 00:10:29.080
Right.

188
00:10:29.120 --> 00:10:34.710
Extend so same it contains 100 observations corresponds to one hundred customers.

189
00:10:34.710 --> 00:10:37.920
And for each of them their age and their estimated salary.

190
00:10:38.100 --> 00:10:43.200
And you know since x test is actually supposed to be some new data in production.

191
00:10:43.200 --> 00:10:48.930
Well we're actually going to suppose that x test is actually the set of customers who purchased yes

192
00:10:48.930 --> 00:10:50.280
or no the new SUV.

193
00:10:50.280 --> 00:10:55.800
You know we're going to pretend that x test is actually some data when we deploy our model in production

194
00:10:56.030 --> 00:11:01.710
so that we can evaluate it on the new observations many on the new customers buying Yes I know that

195
00:11:01.710 --> 00:11:03.130
new SUV.

196
00:11:03.270 --> 00:11:03.660
All right.

197
00:11:03.660 --> 00:11:08.610
It's more fun if we imagine X does this way because it is indeed supposed to be some new observations.

198
00:11:08.730 --> 00:11:10.800
And now why test.

199
00:11:10.800 --> 00:11:12.000
Let's see.

200
00:11:12.000 --> 00:11:17.640
All right and why test contains of course all the chaste decisions of the customers in this test.

201
00:11:17.760 --> 00:11:21.330
Meaning whether or not they bought the new SUV.

202
00:11:21.330 --> 00:11:21.970
All right.

203
00:11:22.050 --> 00:11:22.470
Perfect.

204
00:11:22.500 --> 00:11:29.270
So now let's apply feature scaling and let's see how extreme an excess are transformed.

205
00:11:29.280 --> 00:11:29.610
All right.

206
00:11:29.640 --> 00:11:31.670
So let's do this let's play.

207
00:11:31.980 --> 00:11:34.140
And now let's print x train.

208
00:11:34.140 --> 00:11:37.530
All right so now we have some scaled values between.

209
00:11:37.560 --> 00:11:42.280
Well you know minus two and plus three.

210
00:11:42.280 --> 00:11:46.470
You know this one is two point oh six minus one point seven.

211
00:11:46.470 --> 00:11:49.930
Anyway should be somewhere between minus three and plus three.

212
00:11:49.980 --> 00:11:50.220
Okay.

213
00:11:50.250 --> 00:11:56.460
But now we can clearly see that we have both the two features in the same range the Transformers Age

214
00:11:56.550 --> 00:12:01.170
and The transformed estimated salary are now indeed in the same range.

215
00:12:01.200 --> 00:12:05.190
And that's exactly what we're supposed to get with feature scaling.

216
00:12:05.190 --> 00:12:05.910
All right.

217
00:12:05.910 --> 00:12:14.610
So now let's crawl down to print x test and the same we get the two features age and salary taking values

218
00:12:14.700 --> 00:12:18.170
in the same range between somewhere around minus three and plus three.

219
00:12:18.180 --> 00:12:23.010
So this will improve the training performance of joystick regression model.

220
00:12:23.010 --> 00:12:25.950
Well you know for the training set of course only for the training set.

221
00:12:25.950 --> 00:12:32.610
But then when we will deploy our model to predict whether the customers of the test set but yes or no

222
00:12:32.850 --> 00:12:38.310
the new SUV Well we will have indeed to apply the predict method on these skilled values.

223
00:12:38.370 --> 00:12:40.260
Otherwise operations will be nonsense.

224
00:12:40.290 --> 00:12:40.820
Right.

225
00:12:40.830 --> 00:12:46.920
The predict method has to be called on a set of features with the same scale as the one that was applied

226
00:12:47.010 --> 00:12:48.180
during the training.

227
00:12:48.180 --> 00:12:49.800
Okay perfect.

228
00:12:49.800 --> 00:12:53.250
So now we can move on to the next exciting step.

229
00:12:53.430 --> 00:12:59.430
I'm talking of course about the step where we build and train our logistic regression well on the training

230
00:12:59.430 --> 00:13:04.830
set and we're going to do this of course with our best friend cycled learn which therefore leads you

231
00:13:04.830 --> 00:13:06.690
to a new exercise.

232
00:13:06.690 --> 00:13:12.240
I would like to figure out on your own how you can build and train that logistic regression model using

233
00:13:12.240 --> 00:13:19.320
psychic turn and I encourage you to have a look at the API you know the whole cycling API to find the

234
00:13:19.320 --> 00:13:25.050
class that allows us to build the logistic regression model and then I'm sure that you will know which

235
00:13:25.050 --> 00:13:29.710
method to use to train this logistic regression model on the training set.

236
00:13:29.790 --> 00:13:35.900
So please try to do it on your own and of course we will implement a solution together in a next tutorial.

237
00:13:35.940 --> 00:13:37.790
And until then enjoy machine learning.
