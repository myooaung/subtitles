WEBVTT
1
00:00:00.570 --> 00:00:02.600
Welcome back to your machine learning.

2
00:00:02.610 --> 00:00:06.710
A to Z course a super excited to have you back on board.

3
00:00:06.720 --> 00:00:12.800
And today we're kicking off the Support Vector regression intuition super pumped about these tutorials

4
00:00:12.810 --> 00:00:15.930
got some very exciting slides coming up for you.

5
00:00:15.960 --> 00:00:17.670
So what is Support Vector regression.

6
00:00:17.910 --> 00:00:25.260
Well Support Vector regression was invented back in the 90s by Vladimir Von Sponeck and his colleagues

7
00:00:25.290 --> 00:00:28.150
who were working at the Bell Labs at the time.

8
00:00:28.150 --> 00:00:30.390
That was the AT&amp;T Bell Labs.

9
00:00:30.390 --> 00:00:38.400
Now they're not Nokia Bell Labs and a lot of support vector machine and Support Vector regression are

10
00:00:38.400 --> 00:00:41.700
discussed in the demo of Up next book.

11
00:00:41.820 --> 00:00:49.260
The nature of statistical learning 1992 in this course we will be covering both support vector machine

12
00:00:49.770 --> 00:00:53.880
in the part of the course to do classification and Support Vector regression we will be talking about

13
00:00:53.880 --> 00:00:54.590
it here.

14
00:00:54.750 --> 00:01:02.130
And in addition to that we will also be talking about kernel support of extra machine and kernel Support

15
00:01:02.130 --> 00:01:06.620
Vector regression and the kernel trick and many other things like that.

16
00:01:06.690 --> 00:01:11.100
So there is a lot of exciting stories on these topics but for now we're just going to limit ourselves

17
00:01:11.130 --> 00:01:16.140
to support vector regression specifically linear Support Vector regression.

18
00:01:16.170 --> 00:01:17.380
So here we go.

19
00:01:17.700 --> 00:01:24.740
Here we've got two plots and we need that in order to compare SVR to the simple linear regression that'll

20
00:01:24.750 --> 00:01:26.400
really help us understand things.

21
00:01:26.430 --> 00:01:29.900
So here on the left we've got some random plots our random dots.

22
00:01:30.090 --> 00:01:31.580
I'm going to copy them over to the right.

23
00:01:31.590 --> 00:01:36.680
So we know that these are identical there's no tricks involved there's a absolutely identical.

24
00:01:36.720 --> 00:01:38.960
This is absolutely identical set of data.

25
00:01:39.420 --> 00:01:45.090
And let's start with on the left we're going to apply a simple linear regression we've already discussed

26
00:01:45.090 --> 00:01:47.310
what it's like but let's quickly refresh.

27
00:01:47.310 --> 00:01:52.850
So basically we're going to have this line go through the data and how is this line derived.

28
00:01:52.850 --> 00:01:59.880
Well a method called the ordinary least squares method will be applied to find this line and basically

29
00:01:59.880 --> 00:02:08.880
we want to minimize the distance between the this value Y the actual value in the data and y hat.

30
00:02:08.910 --> 00:02:12.510
Basically what it would have been on a trend line we take the difference here where difference here

31
00:02:12.510 --> 00:02:14.330
we square it and we want to minimize that.

32
00:02:14.320 --> 00:02:16.760
That's ordinarily squares method f.

33
00:02:16.800 --> 00:02:19.370
Essentially what we're doing is minimizing the error.

34
00:02:19.380 --> 00:02:27.840
We want to have a line with the minimum error possible so that's that's the intuition behind a simple

35
00:02:27.900 --> 00:02:30.150
regression something we already talked about.

36
00:02:30.150 --> 00:02:34.560
Now how does the support vector regression work.

37
00:02:34.560 --> 00:02:36.000
Well let's have a look on the right.

38
00:02:36.010 --> 00:02:43.640
We've SVR instead of a simple line you'll see a tube and here you have the regression line in the middle

39
00:02:43.650 --> 00:02:46.430
but then there's this tube around and what does this tube do.

40
00:02:46.440 --> 00:02:52.110
Well this tube has a width of epsilon and the width as Bush measured vertically is important and along

41
00:02:52.110 --> 00:02:59.820
this axis not perpendicular to the tube but vertically and this tube itself is called the Epsilon insensitive

42
00:02:59.820 --> 00:03:00.390
2.

43
00:03:00.420 --> 00:03:01.500
And what does that mean.

44
00:03:01.500 --> 00:03:10.530
Well that means that any points in our dataset that fall inside the tube they won't be we'll be disregarding

45
00:03:10.530 --> 00:03:11.040
their error.

46
00:03:11.070 --> 00:03:19.920
So basically this tube think of it as a margin of error that we are allowing our model to have and not

47
00:03:19.920 --> 00:03:22.050
care about any error inside here.

48
00:03:22.050 --> 00:03:30.030
So any discrepancy between or any like distance between this point over here and the line as in like

49
00:03:30.030 --> 00:03:33.940
for instance here we could see let's look at which point is that.

50
00:03:33.960 --> 00:03:36.480
That's 1 2.

51
00:03:36.510 --> 00:03:38.600
This third point you can see the line is even different right.

52
00:03:38.590 --> 00:03:40.800
The results can be different and probably will be different.

53
00:03:40.800 --> 00:03:45.460
So this third point here there is a distance between the line here and we care about this area here.

54
00:03:45.480 --> 00:03:50.210
We don't care about this area because it falls within this Epsilon insensitive tube.

55
00:03:50.220 --> 00:03:52.680
So we're disregarding any kind of errors in here.

56
00:03:53.030 --> 00:03:59.160
And that's kind of the key behind support vector regression and it gives a little bit of movement or

57
00:03:59.160 --> 00:04:03.520
a little bit of buffer to our model.

58
00:04:03.780 --> 00:04:09.500
And at the same time we have points that are outside the Epsilon insensitive tube.

59
00:04:09.780 --> 00:04:10.650
There they are.

60
00:04:10.650 --> 00:04:13.180
And for them we do care about the error.

61
00:04:13.260 --> 00:04:18.180
And here will be measured as the distance between the the point and the tube itself.

62
00:04:18.180 --> 00:04:20.720
So not the trend line but the tube itself.

63
00:04:20.970 --> 00:04:22.580
These distances have names.

64
00:04:22.590 --> 00:04:29.850
They're either see star if the point is below the tube or see if the point is above the tube.

65
00:04:30.090 --> 00:04:31.840
And they're called.

66
00:04:31.980 --> 00:04:34.650
So these values circles like variables.

67
00:04:34.650 --> 00:04:39.140
So it's advocacy star if it's below sea for a star if it's above.

68
00:04:39.150 --> 00:04:45.030
And we do care about the air so we care about these distances and the way we care about it so we're

69
00:04:45.030 --> 00:04:49.410
going to try to avoid formulas that will give additional readings something you can look into further

70
00:04:49.410 --> 00:04:51.300
down at the end of this tutorial.

71
00:04:51.300 --> 00:04:55.960
But just for for completeness sake here is the formula.

72
00:04:56.130 --> 00:05:01.620
So in the case of oil as it was a simple ordinarily squared like that.

73
00:05:01.620 --> 00:05:03.380
Here is a bit more complex.

74
00:05:03.510 --> 00:05:08.760
We're not going to talk about this part of a here but what we're focusing on is this and we can see

75
00:05:08.760 --> 00:05:15.690
that we're minimizing we want these distances said the sum of these distances to be minimal.

76
00:05:15.690 --> 00:05:17.780
Once again there'll be additional reading at the end.

77
00:05:17.790 --> 00:05:24.390
If you'd like to go into it but effectively these points the ones that are outside our tube are dictating

78
00:05:24.630 --> 00:05:30.690
what the tube will look like how the tube will be positioned so the error within the tube is completely

79
00:05:30.690 --> 00:05:34.230
disregarded we don't care about the era unlike in the ordinary square.

80
00:05:34.240 --> 00:05:42.690
So we're giving some kind of buffer of flexibility to our tube Maureen like allowing it to accounting

81
00:05:42.690 --> 00:05:47.310
for some kind of error that we might expect in the data.

82
00:05:47.340 --> 00:05:52.240
It's normal some for there to be error but these ones they are important to us.

83
00:05:52.470 --> 00:05:55.370
And also one final thing.

84
00:05:55.370 --> 00:05:58.370
Why is this a method called Support Vector regression.

85
00:05:58.380 --> 00:06:04.860
Well because effectively these points all of these points outside by any point actually any point on

86
00:06:04.860 --> 00:06:10.650
this plot is a vector can be represented as a vector in this two dimensional space or a multidimensional

87
00:06:10.650 --> 00:06:13.150
space if you have more features.

88
00:06:13.170 --> 00:06:18.690
So in this case is can presented by two dimensional vector so they are all these points of Xs but the

89
00:06:18.690 --> 00:06:24.540
ones that we've highlighted in red the ones outside the tube they're the support vectors because they

90
00:06:24.540 --> 00:06:27.720
are dictating how this tube is great.

91
00:06:27.720 --> 00:06:34.200
So basically they're supporting the structure or formation of this tube and that's why they called support

92
00:06:34.560 --> 00:06:38.190
vectors and that's why this is a support vector regression.

93
00:06:38.310 --> 00:06:43.100
And so there we go that's what it's all about that just important to remember the Epsilon insensitive

94
00:06:43.160 --> 00:06:48.690
tube and that support vector regression just cares about the errors of anything that's lying outside

95
00:06:48.690 --> 00:06:52.980
this tube and to finish off as promised here's the additional reading.

96
00:06:52.980 --> 00:06:58.440
So if you'd like to learn a bit more have a look at Chapter 4 support vector regression in a book called

97
00:06:58.440 --> 00:07:03.630
efficient learning machines theories concepts and implications for engineers and system designed by

98
00:07:03.630 --> 00:07:13.560
Maria to widen the Rahul Hana and here's a link here where it's aggregated on this portal for published

99
00:07:13.650 --> 00:07:20.400
work something you'll notice that it might be a little bit confusing here they say these are potential

100
00:07:20.400 --> 00:07:26.070
support vectors they're referring only to the ones close I've had a look at different literature and

101
00:07:26.120 --> 00:07:31.920
as the literature I prefer is the one that says in this respect is the one that says that the support

102
00:07:31.920 --> 00:07:38.790
vectors are the ones that are outside and they are outside any any basically any point outside the tube

103
00:07:38.790 --> 00:07:45.750
is a support vector so that's how we talk to discuss it inside this tutorial but have a look at this

104
00:07:45.750 --> 00:07:52.260
different nomenclature maybe there'll be a good perspective to have a different view but overall the

105
00:07:52.620 --> 00:07:57.810
first couple of paragraphs describing the whole problem they're very well written I liked how they were

106
00:07:57.810 --> 00:08:03.180
written and I think I can be a valuable addition if you're looking for additional reading so we go that's

107
00:08:03.180 --> 00:08:07.310
a support vector regression hope you enjoy the tutorial and look forward to you next time.

108
00:08:07.380 --> 00:08:09.060
Until then happy analyzing.
