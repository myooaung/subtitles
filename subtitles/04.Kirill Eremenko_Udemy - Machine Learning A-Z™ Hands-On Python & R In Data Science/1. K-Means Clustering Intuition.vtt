WEBVTT
1

00:00:00.390  -->  00:00:03.140
Hello and welcome back to the course on machine learning.

2

00:00:03.300  -->  00:00:07.560
And in this section we're talking about the K means clustering algorithm.

3

00:00:07.560  -->  00:00:11.490
And in this tutorial we're going to talk about the intuition behind Kamins.

4

00:00:11.490  -->  00:00:17.850
So Kamins is a algorithm that allows you to closter your data and as we will see it's a very convenient

5

00:00:17.850  -->  00:00:23.550
tool for discovering categories of groups in your data set that you wouldn't have otherwise thought

6

00:00:23.550  -->  00:00:24.390
of yourself.

7

00:00:24.600  -->  00:00:31.320
And in this section or in this specific tutorial we'll learn how to understand k means on an intuitive

8

00:00:31.320  -->  00:00:33.780
level and we'll see an example of Hardwick's.

9

00:00:33.780  -->  00:00:35.240
So let's dive straight into it.

10

00:00:35.280  -->  00:00:39.230
Let's make this topic this complex topic simple.

11

00:00:39.240  -->  00:00:41.610
All right so here we've got a scatterplot.

12

00:00:41.790  -->  00:00:43.740
And how does this robot come to be.

13

00:00:43.740  -->  00:00:49.810
Well let's imagine that we've got two variables in our data set and we decided to plot those two variables

14

00:00:49.810  -->  00:00:52.910
on x and y axis so here we go that's one variable.

15

00:00:53.010  -->  00:00:53.910
That's the other one.

16

00:00:53.970  -->  00:01:00.210
And this is how our observations are configured according to these two variables.

17

00:01:00.210  -->  00:01:05.060
So the question here is can we identify certain groups among all variables.

18

00:01:05.190  -->  00:01:07.360
And how would we go about doing this too.

19

00:01:07.370  -->  00:01:09.780
Can we maybe identify this as one.

20

00:01:09.810  -->  00:01:14.820
This is one group and this is the second group or maybe this is one group the second maybe there's four

21

00:01:14.820  -->  00:01:16.950
groups or five of these three groups.

22

00:01:16.950  -->  00:01:22.190
How do we identify that number of groups how do we identify the groups themselves.

23

00:01:22.200  -->  00:01:29.700
So what the Kamins does for you is it takes out the complexity from this decision making process and

24

00:01:29.730  -->  00:01:36.210
allows you to very easily identify those clusters are actually called clusters of data points in your

25

00:01:36.210  -->  00:01:36.850
data set.

26

00:01:36.870  -->  00:01:41.220
So they would go in this case we have three classes and of course this is a very simplified example

27

00:01:41.220  -->  00:01:46.860
we only have two dimensions here so two variables Kamins can work with multidimensional objects so it

28

00:01:46.860  -->  00:01:49.050
can work with three four five 10 dimensions.

29

00:01:49.050  -->  00:01:54.630
This example is just for illustration purposes so that we can visually see what is going on but in reality

30

00:01:54.630  -->  00:02:00.360
they can be as many as 10 or 100 any number of variables and Kamins will do that complex calculation

31

00:02:00.360  -->  00:02:00.620
for you.

32

00:02:00.620  -->  00:02:05.240
So it's an algorithm designed to find these clusters for you.

33

00:02:05.310  -->  00:02:10.100
And today we're going to find out a bit more on how the Kamins algorithm works.

34

00:02:10.440  -->  00:02:12.480
So what is the step by step process.

35

00:02:12.480  -->  00:02:13.340
My favorite part.

36

00:02:13.350  -->  00:02:19.110
We get to break you down as such a simple step by step process that will be extremely intuitive and

37

00:02:19.110  -->  00:02:20.340
simple to understand.

38

00:02:20.340  -->  00:02:26.670
All right so step one for the Kamins algorithm is to choose the number of clusters K and we'll talk

39

00:02:26.670  -->  00:02:29.360
more about how to select the optimal number of clusters.

40

00:02:29.360  -->  00:02:31.120
Further down in this section of the course.

41

00:02:31.230  -->  00:02:36.420
But for now let's imagine that we've agreed on a number of clusters for a certain challenge and let's

42

00:02:36.420  -->  00:02:40.820
say we have to agree that there's three clusters of this five clusters or two classes.

43

00:02:41.070  -->  00:02:47.610
And once you've done that then you proceed to step two which is to select at random k points which will

44

00:02:47.610  -->  00:02:53.280
be the centroid of your clusters and not necessarily these points have to be from your dataset.

45

00:02:53.280  -->  00:02:57.480
So as you saw we had a scatterplot you could select any points and that scatterplot.

46

00:02:57.630  -->  00:03:02.430
They don't have to be part of the observations that A-plot on the scalp but they can be any random x

47

00:03:02.430  -->  00:03:08.610
and y values on your scatterplot as long as you just selects a certain number of centroid that are going

48

00:03:08.610  -->  00:03:12.130
to equate to the number of clusters that you have decided upon.

49

00:03:12.290  -->  00:03:14.580
Yes we'll see this in practice just now.

50

00:03:14.580  -->  00:03:22.050
Then you move on to Step 3 where you assign each data point to the closest centroid and that will form

51

00:03:22.080  -->  00:03:23.780
K clusters right away immediately.

52

00:03:23.780  -->  00:03:28.770
So yes you're starting clusters and then there's going to be an iterative process to refine those clusters

53

00:03:29.220  -->  00:03:30.690
and we'll see this in action.

54

00:03:30.690  -->  00:03:35.370
But basically so you just check for every point in a data set which out of the centroid that you've

55

00:03:35.370  -->  00:03:42.390
identified and step 2 which of them is the closest to your Pacific data point and then that's where

56

00:03:42.390  -->  00:03:48.500
that data point will be assigned to and closest is a kind of vague term here because what kind of distance

57

00:03:48.530  -->  00:03:54.540
you're measuring it by is it Euclidean distances at some other sort of distance that can be better defined

58

00:03:54.540  -->  00:03:55.240
in your business.

59

00:03:55.260  -->  00:03:59.580
But for the purposes of these tutorials we're going to talk about Euclidean distances.

60

00:03:59.730  -->  00:04:02.670
And so that's basically geometrical distances.

61

00:04:02.700  -->  00:04:04.930
In simple terms.

62

00:04:04.950  -->  00:04:05.370
All right.

63

00:04:05.370  -->  00:04:11.990
Step Four where you proceed to step three is to compute and place the new centroid which cluster and

64

00:04:12.060  -->  00:04:13.770
we'll see how that's done just now.

65

00:04:13.770  -->  00:04:18.930
So once you find your clusters you recalculate the centroid again it's going to be much easier to understand

66

00:04:18.930  -->  00:04:20.580
this when we look at an example.

67

00:04:20.740  -->  00:04:25.280
Then in step 5 you reassigned each data point to the new courses centroid.

68

00:04:25.290  -->  00:04:32.020
So basically you perform step 3 again but we'll just call it Step 5 here and then if any assignment

69

00:04:32.090  -->  00:04:33.660
place you repeat step 4.

70

00:04:33.750  -->  00:04:37.330
So it becomes an iterative process of Step 4 survive.

71

00:04:37.560  -->  00:04:41.370
And if Noria someone to place to go to finish and your models.

72

00:04:41.640  -->  00:04:47.520
I know this might seem a bit complex but let's go through a visual example now so that we understand

73

00:04:47.520  -->  00:04:49.320
this all in a very intuitive level.

74

00:04:49.320  -->  00:04:53.680
And then you can always reference the slide or the step by step rule.

75

00:04:53.760  -->  00:04:59.280
Further down when you're actually performing k means clustering or when you want to understand how it's

76

00:04:59.340  -->  00:05:02.310
happening or what's happening in the background of Kamins clustering.

77

00:05:02.310  -->  00:05:08.820
So this slide will be a great reference point after we discussed this visual exercise that we're going

78

00:05:08.820  -->  00:05:09.620
to have just now.

79

00:05:09.880  -->  00:05:11.920
So let's move on.

80

00:05:12.060  -->  00:05:18.750
All right so there is our scatterplot And here we've got the observations in our data set they're plotted

81

00:05:18.750  -->  00:05:26.040
against two variables and right away the first question is can you visually just quickly identify the

82

00:05:26.280  -->  00:05:29.220
final clusters that you think that will end up with.

83

00:05:29.310  -->  00:05:30.300
It's pretty tough isn't it.

84

00:05:30.300  -->  00:05:36.660
And that is even when we just have two variables imagine how complex a situation or this challenge would

85

00:05:36.660  -->  00:05:42.180
be if we had three verbs or five variables we couldn't we wouldn't even be able to plot a five dimensional

86

00:05:42.540  -->  00:05:43.680
scatterplot like that.

87

00:05:43.890  -->  00:05:49.340
So that's where it came in is clustering comes into play and that's where this algorithm will help us

88

00:05:49.340  -->  00:05:50.850
simplify the process.

89

00:05:50.850  -->  00:05:53.970
So let's see how it will perform in action.

90

00:05:53.970  -->  00:06:00.300
So in this case we're actually going to manually perform the same means clustering algorithm.

91

00:06:00.400  -->  00:06:05.880
So something that would normally be done by tools of your choice for such as our Python or any other

92

00:06:05.880  -->  00:06:06.410
tool.

93

00:06:06.570  -->  00:06:10.800
But in this case just so that we can get very comfortable with this algorithm.

94

00:06:10.830  -->  00:06:13.750
We're going to do it manually and see exactly how it works.

95

00:06:13.770  -->  00:06:14.060
All right.

96

00:06:14.070  -->  00:06:15.340
So let's go through the steps.

97

00:06:15.420  -->  00:06:22.530
Step 1 choose the number of clusters K and let's say we somehow identify that the optimal number of

98

00:06:22.530  -->  00:06:24.070
clusters is equal to 2.

99

00:06:24.210  -->  00:06:27.960
Again we'll discuss how to find the opposite number of clusters further down in S..

100

00:06:28.050  -->  00:06:32.130
But for now let's agree that for this challenge it's just two clusters.

101

00:06:32.160  -->  00:06:32.580
All right.

102

00:06:32.610  -->  00:06:39.870
And then a step to select at random k points which will be the centroid of your clusters and not necessarily

103

00:06:39.870  -->  00:06:40.530
from a data set.

104

00:06:40.530  -->  00:06:46.650
So what that means is that they can be actual points in your data set or they can just be random points

105

00:06:46.680  -->  00:06:50.000
on your scatterplot it doesn't matter where you select them.

106

00:06:50.010  -->  00:06:53.920
So let's say we selected these two centroid is a blue centroid and the red cent..

107

00:06:54.360  -->  00:06:55.810
Next step is step three.

108

00:06:55.830  -->  00:07:00.490
Assign each dot a point to the closest centroid and that will form K clusters.

109

00:07:00.630  -->  00:07:06.200
So basically for every data point in our data set what we need do is identify which of the two centroid

110

00:07:06.200  -->  00:07:07.930
is blue or red is the closest.

111

00:07:07.980  -->  00:07:11.400
So let's say for this data point obviously blue is closer than red.

112

00:07:11.400  -->  00:07:15.000
So it would be assigned to the blue cluster for this data point.

113

00:07:15.000  -->  00:07:19.140
Blue is again closer than red for this data point red is closer than blue.

114

00:07:19.320  -->  00:07:25.150
So we could keep going like that for every data point and assign it to one of the two central.

115

00:07:25.290  -->  00:07:31.530
But we're going to use a quick hack here we going to use something that we learned from geometry so

116

00:07:31.530  -->  00:07:36.090
what are we going to do is we're going to just connect these two centroid with a line like that and

117

00:07:36.090  -->  00:07:41.070
then we'll find the center of the light which is so here and I'll put a perpendicular light exactly

118

00:07:41.070  -->  00:07:42.600
through that central like that.

119

00:07:42.870  -->  00:07:49.980
And so from Jones-Drew we know from high school geometry or maybe I'm at university or even if you didn't

120

00:07:49.980  -->  00:07:56.370
have geometry it's it's a very straightforward concept that any point on this green line is equidistant

121

00:07:56.370  -->  00:07:57.360
from the blue and the red.

122

00:07:57.360  -->  00:08:03.110
So if I take this point will be the same distance to the blue centroid and to the red centroid.

123

00:08:03.240  -->  00:08:06.270
If I take this point it'll be same distance to blue and the red.

124

00:08:06.270  -->  00:08:10.860
So this whole line is equidistant to the blue in there and that's how we constructed it.

125

00:08:10.890  -->  00:08:17.550
And so based on that it is obvious that any of our points on the scatterplot above the green line is

126

00:08:17.550  -->  00:08:23.760
close to the blue and any of the points below are the green line is closer to the red and that's how

127

00:08:23.760  -->  00:08:25.390
we're going to just color all points.

128

00:08:25.410  -->  00:08:28.400
It's a very quick way of coloring them.

129

00:08:28.590  -->  00:08:33.390
Again you could totally go through every point and individually identify which one to which centroid

130

00:08:33.390  -->  00:08:34.110
is closest.

131

00:08:34.200  -->  00:08:39.480
But just for our sake while we're doing this exercise it's going to be faster if we apply this quick

132

00:08:39.480  -->  00:08:40.540
hack method.

133

00:08:40.710  -->  00:08:47.250
So there we go we've colored our centroid or our points on our scatterplot before we continue I do want

134

00:08:47.250  -->  00:08:52.740
to mention something here that's closest is a very even though it seems straightforward it's quite an

135

00:08:52.740  -->  00:08:58.740
ambiguous term because closest when you're visualizing things on a scatterplot Yes it's pretty straightforward

136

00:08:58.740  -->  00:09:00.350
that it's the distance.

137

00:09:00.390  -->  00:09:06.690
Right but at the same time in mathematics and in data science there's lots of different type of distances

138

00:09:06.690  -->  00:09:12.030
like the one we're using is Euclidean and the question is should you be using Euclidean distances or

139

00:09:12.030  -->  00:09:17.370
should be using some other type of distances defined for your challenge and that is something up to

140

00:09:17.370  -->  00:09:18.180
you to decide.

141

00:09:18.180  -->  00:09:23.490
And there's something for you to specify for the algorithm what type of distance you're going to be

142

00:09:23.490  -->  00:09:24.170
using.

143

00:09:24.480  -->  00:09:29.480
And but for our sake which is going to be using Euclidean distances for illustration purposes.

144

00:09:29.670  -->  00:09:36.090
And basically that's just the very straightforward simple geometrical distances between two different

145

00:09:36.090  -->  00:09:36.680
points.

146

00:09:36.810  -->  00:09:41.130
So there's just a caveat there and if you'd like to research more about distances and what other kind

147

00:09:41.130  -->  00:09:45.750
of distances you can use just make sure to look into that because for some challenges sometimes you

148

00:09:45.750  -->  00:09:50.790
might want to use non-Euclidian non-Euclidean distances are defined in other specific ways.

149

00:09:50.790  -->  00:09:51.060
All right.

150

00:09:51.060  -->  00:09:54.940
So with that in mind let's proceed to step four.

151

00:09:55.190  -->  00:09:58.360
So for compute and place the new centroid of each cluster.

152

00:09:58.500  -->  00:10:03.540
So basically right now we've got the new points the old the blue ones excluding the centroid itself

153

00:10:03.810  -->  00:10:06.310
and all the red ones excluding the centroid itself.

154

00:10:06.480  -->  00:10:12.120
And we need to find out where the actual centroid the new centroid for the Blue Points is and the new

155

00:10:12.120  -->  00:10:13.920
centroid for the red point is.

156

00:10:14.210  -->  00:10:20.100
And the way to think about it is imagine that the centroid themselves they're weightless but these other

157

00:10:20.100  -->  00:10:22.740
points they have a certain weight say.

158

00:10:22.830  -->  00:10:24.540
Let's say one kilo each.

159

00:10:24.630  -->  00:10:30.300
Then you need to find the center of mass or the center of gravity of these centroid and you need to

160

00:10:30.300  -->  00:10:31.970
pinpoint it on your scatterplot.

161

00:10:32.070  -->  00:10:35.730
So for the blue closer it'll be somewhere here for the red cluster.

162

00:10:35.730  -->  00:10:37.170
It'll be somewhere right here.

163

00:10:37.320  -->  00:10:43.620
And the way to think about it on a two dimensional scatterplot you kind of like can just visually see

164

00:10:43.620  -->  00:10:51.090
where it is or you can just look at the x y coordinates for all of the blue points and find the center

165

00:10:51.090  -->  00:10:55.580
of gravity for the y coordinates which will be somewhere here and then do the same thing for the x coordinates

166

00:10:55.590  -->  00:11:00.620
find the center of gravity which Or like the average of the X Cornus which will be our here.

167

00:11:00.660  -->  00:11:04.210
So that's how you get your new centroid for the red blue clusters.

168

00:11:04.260  -->  00:11:07.520
And so we just move our centroid into them.

169

00:11:07.520  -->  00:11:09.890
Next you perform step 5.

170

00:11:09.930  -->  00:11:14.050
So re-assign each data point to the new closest centroid.

171

00:11:14.280  -->  00:11:17.150
And if any re-assignment took place and go back to Step 4.

172

00:11:17.250  -->  00:11:21.630
Otherwise you may finish algorithm meaning that it has converged.

173

00:11:21.630  -->  00:11:27.870
So let's do that let's see how now the data points will re-assign to the new centroid.

174

00:11:28.080  -->  00:11:33.870
So if we put our line through the scatterplot you'll see that there is one point on it is actually three

175

00:11:33.870  -->  00:11:34.440
points.

176

00:11:34.440  -->  00:11:39.530
This point is close to the blue and the red and these two points are actually close to the red to the

177

00:11:39.540  -->  00:11:40.000
blue.

178

00:11:40.260  -->  00:11:42.180
So now we will recolored them.

179

00:11:42.510  -->  00:11:43.170
There we go.

180

00:11:43.170  -->  00:11:48.970
So now we've got a new clustering and some re-assignment did take place.

181

00:11:48.990  -->  00:11:51.820
So we're going to proceed back to Step 4.

182

00:11:52.140  -->  00:11:55.800
So we're going back to Step 4 compute and place the new centroid for each cluster.

183

00:11:55.800  -->  00:11:56.280
Same thing.

184

00:11:56.280  -->  00:12:01.930
Find the center mass for this centroid find the center of mass for this centroid place the centroid

185

00:12:02.040  -->  00:12:08.430
in those locations and now repeat step 5 re-assign each data point to the new closer centroid.

186

00:12:08.580  -->  00:12:13.070
So we're going to put the line through the chart so the equidistant line between the two centroid.

187

00:12:13.410  -->  00:12:18.390
As you can see this point is actually now in the Blue Cross rather than being in the red re-assign that

188

00:12:18.390  -->  00:12:19.210
point.

189

00:12:19.470  -->  00:12:26.900
And what happens next is again we return to step 4 so if I go back here you'll see that area some someone

190

00:12:26.910  -->  00:12:33.060
did take place going back to Step 4 and that's where our new clusters are going to be located are new

191

00:12:33.150  -->  00:12:34.000
centroid.

192

00:12:34.080  -->  00:12:35.240
Move them there.

193

00:12:35.250  -->  00:12:40.650
Repeat step 5 as you can see it's an iterative process which is going to keep going like this until

194

00:12:40.650  -->  00:12:42.430
the algorithm converges.

195

00:12:42.450  -->  00:12:46.340
So there's our equidistant line as it can see one point needs to be reassigned.

196

00:12:46.410  -->  00:12:47.600
It gets reassigned.

197

00:12:47.730  -->  00:12:54.290
So now we go back to Step 4 compute in place the new centroid for each cluster move our centroid and

198

00:12:54.300  -->  00:12:57.900
now assign each data points or repeats the five.

199

00:12:58.170  -->  00:13:03.790
And as you can see this time there could distant line does not make any points.

200

00:13:03.840  -->  00:13:10.230
Re-assign says he can see all the points are already in their correct clusters and that means no re-assignment

201

00:13:10.560  -->  00:13:15.600
took place during this step so we can proceed to completing our algorithm.

202

00:13:15.600  -->  00:13:18.150
This means that the algorithm has converged.

203

00:13:18.150  -->  00:13:18.930
So there we go.

204

00:13:18.930  -->  00:13:25.560
Those are our clusters and the model is ready and so now we can just remove the centroid and equidistant

205

00:13:25.560  -->  00:13:28.200
line and there we go that's our final result.

206

00:13:28.200  -->  00:13:32.340
So as you can see that's how the Kamins algorithm works it's very very intuitive and straight forward

207

00:13:32.350  -->  00:13:33.900
just an iterative process.

208

00:13:34.080  -->  00:13:37.660
And if we compare to what we had at the start this is what we had.

209

00:13:37.680  -->  00:13:41.880
As you can see it's not that obvious how the Klaas's would have been arranged.

210

00:13:41.880  -->  00:13:47.100
For instance you might have thought that maybe this could be a cluster on its own and this could be

211

00:13:47.100  -->  00:13:52.110
a cluster or maybe this bottom part would be a cluster in this top part would be a cluster but based

212

00:13:52.110  -->  00:13:59.220
on the Kamins algorithm what we got as a result is these two clusters so the points in each cluster

213

00:13:59.220  -->  00:14:05.310
are not exactly very similar to each other but this is what the K means algorithm is suggesting hopefully

214

00:14:05.340  -->  00:14:06.520
enjoyed the storyline.

215

00:14:06.600  -->  00:14:13.350
Hopefully now we've demystified the Kamins algorithm and we've broken down into simple terms and I look

216

00:14:13.350  -->  00:14:15.200
forward to seeing an ex tutorial.

217

00:14:15.210  -->  00:14:17.130
Until then insuree machine learning
