1
00:00:00,540 --> 00:00:04,020
Hello and welcome back to the course on artificial intelligence.

2
00:00:04,020 --> 00:00:08,930
I hope you in during the course so far and today we're talking about action selection policies.

3
00:00:09,000 --> 00:00:09,340
All right.

4
00:00:09,360 --> 00:00:10,950
Let's dive straight into it.

5
00:00:10,960 --> 00:00:15,960
Previously we talked about adding a neural network to our simple core learning.

6
00:00:15,960 --> 00:00:21,180
And so far we are getting quite into deep Q learning.

7
00:00:21,180 --> 00:00:26,600
We've talked about the learning part quite a bit including adding some elements to it.

8
00:00:26,610 --> 00:00:29,970
And today we're talking about this part we're talking about the acting.

9
00:00:29,970 --> 00:00:31,250
So let's have a look.

10
00:00:31,260 --> 00:00:38,550
So here we've got what we discussed about the acting that once you input the values the parameters or

11
00:00:38,550 --> 00:00:42,420
the vector of describing the state the agent is currently in in that environment.

12
00:00:42,420 --> 00:00:47,420
Then that is after all the learning is done or even before the learning is done.

13
00:00:47,430 --> 00:00:51,580
Basically we get all the cue values so we're not interested in the learning right now we instrument

14
00:00:51,630 --> 00:00:51,960
acting.

15
00:00:51,960 --> 00:00:57,300
So once we have these key values how do we understand which one we need to use.

16
00:00:57,300 --> 00:00:58,830
Well if you think about it.

17
00:00:58,830 --> 00:01:05,400
Q values are simply these are predictions for the key values so as we did in the simple Q learning algorithm

18
00:01:05,430 --> 00:01:10,380
what did we do we just selected the one with the best of the highest Q value.

19
00:01:10,380 --> 00:01:15,630
Once we have the one with the highest value we just take that action because it just brings us the highest

20
00:01:15,820 --> 00:01:20,760
values and we know that Q values calculators the immediate reward that we expect to receive plus the

21
00:01:20,850 --> 00:01:24,740
decay factor terms the value of the next states and it's a recursive calculation.

22
00:01:24,750 --> 00:01:25,380
So why not.

23
00:01:25,380 --> 00:01:28,480
Why wouldn't you take the best Q value.

24
00:01:28,560 --> 00:01:30,510
That's kind of the end of it.

25
00:01:30,720 --> 00:01:34,550
But as you can see here it's not as simple here we're using a soft max function.

26
00:01:34,560 --> 00:01:37,560
And this is where we're going to talk about action selection policies.

27
00:01:37,890 --> 00:01:41,190
So here in reality we don't have to have just a soft max function.

28
00:01:41,250 --> 00:01:44,730
We can have different action selection policies.

29
00:01:44,730 --> 00:01:50,540
For example we've got Epsilon greedy Epsilon soft and we've got the soft Max.

30
00:01:50,820 --> 00:01:56,220
And those are kind of like the most commonly used action selection policies of course there are others

31
00:01:56,220 --> 00:02:01,850
for instance the most basic one is here is very simple actually selection policy just select the best.

32
00:02:02,060 --> 00:02:03,730
The one with the highest Q value.

33
00:02:03,930 --> 00:02:09,750
But why doesn't that action policy fly and why do we have different types of action policy action selection

34
00:02:09,750 --> 00:02:10,140
policies.

35
00:02:10,470 --> 00:02:15,510
Well it all boils down to exploration versus exploitation.

36
00:02:15,510 --> 00:02:21,840
And that is the core of reinforcement learning because we've already talked about this a little bit

37
00:02:22,200 --> 00:02:28,170
that your agent when it's operating an environment it might predict certain key values which which might

38
00:02:28,170 --> 00:02:31,800
be good and it might and might not turn out great.

39
00:02:31,800 --> 00:02:34,920
It might turn out that those core values are bad and we'll be forced to explore.

40
00:02:34,920 --> 00:02:40,560
So if we for instance in this case predict that Q2 is the best one and then it takes two to take action

41
00:02:40,560 --> 00:02:42,420
to add it.

42
00:02:42,450 --> 00:02:48,180
So from here takes action too and then it gets it gets a very negative reward then the environment is

43
00:02:48,180 --> 00:02:54,090
forcing the agent to go and explode because now it's going to learn that oh actually I thought Q2 is

44
00:02:54,090 --> 00:02:56,070
going to be very good but it turned out very bad.

45
00:02:56,630 --> 00:02:58,310
So so the results are not very bad.

46
00:02:58,310 --> 00:03:02,560
So the network is going to update itself so next time he's in the state he's going to probably EMI my

47
00:03:02,550 --> 00:03:08,220
soldiers Q2 if it was you know like if it was a very very favorable so you might think that that's like

48
00:03:08,880 --> 00:03:14,180
you know he might need a couple of times a couple of penalties or punishments in order to learn a futures

49
00:03:14,190 --> 00:03:19,710
about action but maybe he'll already soon learn that I'm going to take a different action and take this

50
00:03:19,710 --> 00:03:25,890
action because now it has the best Q value so sometimes the environment forces the agent to take different

51
00:03:26,130 --> 00:03:33,150
to explore different actions but sometimes the agent might get it find itself stuck in a local maximum

52
00:03:33,510 --> 00:03:38,850
it might find that it falls like through through its initial exploration and found that oh this is a

53
00:03:38,850 --> 00:03:43,850
political action like I'm going to go right here and that desperate collection.

54
00:03:43,880 --> 00:03:49,710
But the problem is that it thinks is the best action simply because it hasn't explored is explored going

55
00:03:49,710 --> 00:03:55,800
up is ghost blood going left as explored going right but it hasn't explored going down from that specific

56
00:03:56,310 --> 00:04:01,650
state that it's in and now that it's kind of like biased towards this action and thinks too good actually

57
00:04:01,650 --> 00:04:05,160
is gonna keep taken is going to keep getting it's going to keep taking this action is going to keep

58
00:04:05,160 --> 00:04:11,310
getting a good reward but what if this action would have been even better if this action would have

59
00:04:11,310 --> 00:04:18,210
been so much better that if it knew about this action it would actually switch to this action but because

60
00:04:18,210 --> 00:04:23,700
it got stuck in a local maximum it getting these good rewards it's just going to be reinforced this

61
00:04:23,700 --> 00:04:27,690
is going to keep reinforcing itself that or the environment's going to reinforce it that this is a good

62
00:04:27,690 --> 00:04:32,790
action to take keep doing that but real in the reality is that there's this other action that hasn't

63
00:04:32,790 --> 00:04:38,760
found yet or hasn't even explored that would have been much better as a what we want to do is we want

64
00:04:38,760 --> 00:04:45,750
to come up with an action selection policy that allows our agent not to get stuck in a local maximum.

65
00:04:45,780 --> 00:04:51,270
Yes it's important to keep doing the good actions that's the exploitation part we want to exploit what

66
00:04:51,270 --> 00:04:56,100
we've found but at the same time we still want to explore we never want to stop exploring as like in

67
00:04:56,100 --> 00:05:00,670
life you never want to stop learning you stop learning you die that's there's a saying like that that

68
00:05:00,980 --> 00:05:03,260
when you're not growing you're dying or something.

69
00:05:03,320 --> 00:05:09,110
So you want to keep learning and your agent wants to keep learning and that's where these action selection

70
00:05:09,110 --> 00:05:10,340
policies come in.

71
00:05:10,340 --> 00:05:12,270
So we've got three listed here.

72
00:05:12,270 --> 00:05:14,340
So the first one is Epsilon greedy.

73
00:05:14,440 --> 00:05:20,600
It's a very simple one it sounds pretty complex in the sense that like it's got a cool name and to do

74
00:05:20,600 --> 00:05:22,260
things with several names are complex.

75
00:05:22,310 --> 00:05:22,950
It's actually not.

76
00:05:23,150 --> 00:05:31,350
So basically what it does is it will select the one with the best Q value and absolute like Epsilon.

77
00:05:31,370 --> 00:05:32,810
You might hear in other places.

78
00:05:32,830 --> 00:05:34,740
There's just like a selection policy.

79
00:05:35,180 --> 00:05:40,430
So in this case we're using it to select sucked out of our queue values our actions so you'll select

80
00:05:40,430 --> 00:05:45,910
the one with the highest Q value all the time except for Epsilon percent of the time.

81
00:05:45,920 --> 00:05:53,060
So for instance if you set epsilon to 10 percent then you're going to or zero point one then 10 percent

82
00:05:53,060 --> 00:05:56,610
of the time the action is going to be selected at random.

83
00:05:56,690 --> 00:06:01,790
So 90 percent of the time you're still going to be selecting the best action based on the highest value

84
00:06:02,030 --> 00:06:07,640
but 10 percent of the time is going to be selecting a random action uniform which is going to be absolutely

85
00:06:07,640 --> 00:06:13,150
randomly taking an action or if you said Epsilon to zero point 5 4 0 0 5.

86
00:06:13,160 --> 00:06:18,730
That means that 95 percent of the time the agent is going to be taking that action with the highest

87
00:06:18,830 --> 00:06:22,430
value but 5 percent of the time it's still going to be selecting and a random action.

88
00:06:22,430 --> 00:06:25,690
So it's going to be going out there and exploring.

89
00:06:25,730 --> 00:06:31,160
So Epsilon soft is very similar no way the way that that's kind of like why it's called Epsilon greedy

90
00:06:31,160 --> 00:06:39,890
because they're your greedily selecting the action the good action except for that little Epsilon percent

91
00:06:39,890 --> 00:06:45,080
of the time so that the lower the Epsilon they'll lower the left Epsilon they're more greedily you're

92
00:06:45,080 --> 00:06:53,240
selecting that kind of the action that is the optimal action and the less you're leaving the less chances

93
00:06:53,240 --> 00:06:55,940
you're leaving for exploration Epsilon soft is the opposite.

94
00:06:55,940 --> 00:07:01,970
So basically you're selecting at random you're selecting one minus epsilon percent of the time.

95
00:07:01,970 --> 00:07:06,890
So if you're absent like zero point one or 10 percent then only 10 percent of the time you're taking

96
00:07:07,160 --> 00:07:09,280
this action and why.

97
00:07:09,370 --> 00:07:12,310
And 90 percent of the time you're selecting a random action.

98
00:07:12,350 --> 00:07:19,250
So very very simple just inverted algorithms and a soft Max is kind of like the next step from or it's

99
00:07:19,370 --> 00:07:24,620
a more advanced version I would say of epsilon of the Epsilon greedy algorithm although they both have

100
00:07:24,620 --> 00:07:26,440
merit and they both have place.

101
00:07:26,480 --> 00:07:30,820
We're going to be using soft Max in our coding in our practical side of things.

102
00:07:30,820 --> 00:07:35,150
So that's why we're going to talk in a bit more detail about soft Max.

103
00:07:35,270 --> 00:07:36,320
So let's have a look.

104
00:07:36,320 --> 00:07:38,340
So let's move on to soft Max hopefully.

105
00:07:38,420 --> 00:07:42,750
It's pretty clear about Epsilon greedy so it's a pretty straightforward algorithm.

106
00:07:42,770 --> 00:07:47,740
Select the this one most of the time except for sometimes go and explore.

107
00:07:47,750 --> 00:07:53,750
And now we also see why it's important to do that exploration so that we don't end up in local maximums

108
00:07:53,780 --> 00:07:55,970
in our in our optimization process.

109
00:07:55,970 --> 00:07:58,820
So now we're going to talk a bit more about South Max.

110
00:07:58,830 --> 00:08:05,870
There's a tutorial on soft Max at the end of the course in I think it's an annex number two where we

111
00:08:05,870 --> 00:08:08,380
talk about the concept behind soft Max.

112
00:08:08,390 --> 00:08:13,100
I'm just going to refresh a little bit here so there we're talking about or neural networks and by the

113
00:08:13,100 --> 00:08:17,660
way we're all going to be covering conversion and we're not covering coalition neural networks in this

114
00:08:17,660 --> 00:08:23,330
section of the course in this section we're still using a vector but in the next section of the course

115
00:08:23,420 --> 00:08:29,570
when we're we're creating an age play Doom we are going to be using conventional neural networks so

116
00:08:29,930 --> 00:08:36,230
it could be beneficial for you to look at conventional neural networks and then take a soft Max function

117
00:08:36,260 --> 00:08:41,710
or you can learn a bit more about soft Max after you take that and evolutionary neural networks annex

118
00:08:41,750 --> 00:08:43,190
of course later on.

119
00:08:43,190 --> 00:08:48,110
But here's a quick refresher so here we've got an evolution of neural network which decides whether

120
00:08:48,110 --> 00:08:48,890
it's a dog or a cat.

121
00:08:48,890 --> 00:08:55,940
So here we've got the voting process between these neurons and this one that says that it's a it's got

122
00:08:55,940 --> 00:08:57,850
the features you know the fluffy ears.

123
00:08:58,190 --> 00:09:06,110
What the pointed pointed face type of thing and the kind of like the features that other types of eyes

124
00:09:06,110 --> 00:09:09,890
the eye the way their eyes look all these features that belong to dogs.

125
00:09:09,890 --> 00:09:13,490
So it's a five percent chance that it's dog and the five percent chance that's a cat.

126
00:09:13,850 --> 00:09:15,790
But the question is how did we get.

127
00:09:15,950 --> 00:09:20,740
And in that Tora we're talking about how do we get these values to add up to one.

128
00:09:20,810 --> 00:09:27,590
Well whatever the emotional or our whole neural networks are they can relational neural network plus

129
00:09:27,590 --> 00:09:32,720
the fully connected layers wherever it's bad out whatever the values it spat out we applied is soft

130
00:09:32,720 --> 00:09:33,870
makes functional here.

131
00:09:33,930 --> 00:09:38,650
This is where we introduced them and found a formula for the soft max function is what it looks like.

132
00:09:38,720 --> 00:09:40,450
And then we've got these values.

133
00:09:40,550 --> 00:09:43,400
And so basically that's a quick refresher.

134
00:09:43,400 --> 00:09:45,940
This is the formula for the soft Max.

135
00:09:46,050 --> 00:09:50,870
It's what it does is it takes however many outputs you have doesn't matter.

136
00:09:50,870 --> 00:09:58,070
It will take them and it will squash them all into values between 0 and 1 regardless of how big they

137
00:09:58,070 --> 00:10:03,780
are just its form you can see that there is a total sum at the bottom so there's always going to be

138
00:10:03,780 --> 00:10:10,050
zero bridges are in one and also the all these values are going to add up to one always And so that's

139
00:10:10,440 --> 00:10:18,210
that's very beneficial for us because when we're using the soft max function what happens is we get

140
00:10:18,210 --> 00:10:24,870
these Q values we select this best Q value but in reality what happens is these Q values that we get

141
00:10:24,970 --> 00:10:26,700
there there are actual numbers right.

142
00:10:26,700 --> 00:10:32,040
So this some kind of numbers they don't have to add up to 1 and I have to be between 0 and 1 just some

143
00:10:32,040 --> 00:10:38,160
numbers but when we apply soft Max we don't just select the best one we actually get numbers like that

144
00:10:38,170 --> 00:10:44,240
so we get numbers in the range between 0 and 1 and that are also that also add up to 1.

145
00:10:44,250 --> 00:10:47,280
And so what other thing do we know that add up to 1.

146
00:10:47,280 --> 00:10:50,100
Well probabilities we know that probabilities always have to add up to 1.

147
00:10:50,130 --> 00:10:56,550
So that is why we can say here we've got key values but here all of a sudden we've got soft or we've

148
00:10:56,550 --> 00:10:57,920
got probabilities.

149
00:10:57,930 --> 00:11:02,820
So we can say that the likelihood of this being the best action is 90 percent.

150
00:11:02,820 --> 00:11:08,160
This best being the other sections 5 percent 2 percent 3 percent because we know the higher your Q value

151
00:11:08,160 --> 00:11:09,280
the better the action.

152
00:11:09,290 --> 00:11:14,520
And so if we squash them to 0 to 1 then these become probabilities and we can deal with them as such

153
00:11:15,060 --> 00:11:22,830
and therefore now is when the action is selected and that's how we come up with Q2.

154
00:11:22,830 --> 00:11:28,530
But if you look at it closely this isn't a strict 100 percent and these are not strict zero percent.

155
00:11:28,530 --> 00:11:30,750
So this is a 5 percent 2 percent 3 percent.

156
00:11:30,750 --> 00:11:41,040
So the the most natural way to apply the soft Max in order to preserve exploration in the algorithm

157
00:11:41,370 --> 00:11:48,540
is to use these exact probabilities as how often we're going to be taking that action.

158
00:11:48,540 --> 00:11:54,420
So these probabilities actually represent the distribution of these actions that we're taking.

159
00:11:54,420 --> 00:12:01,680
So basically soft Max makes it very easy for us to come up with a way to combine exploitation and exploration.

160
00:12:01,700 --> 00:12:06,360
So the best the best action will always have the highest probability because it has high school value

161
00:12:06,690 --> 00:12:10,890
and therefore here we're going to be just going to use these as our distribution we're going to say

162
00:12:10,920 --> 00:12:15,690
okay we're going to be taking Q2 90 percent of the time but 5 percent of the time we're still gonna

163
00:12:15,690 --> 00:12:20,820
be taking q 1 and 2 percent of time we get to do 3 and 3 percent of the time we're gonna be taking Q4.

164
00:12:21,360 --> 00:12:27,030
And the beauty here is also that as these values UPDATE As then as the agent goes through the network

165
00:12:27,030 --> 00:12:35,420
more and more and more it becomes more familiar with the environment and therefore these updates so

166
00:12:35,430 --> 00:12:41,700
this value for instance might become like it might might ascertain that this values actually less or

167
00:12:41,700 --> 00:12:42,620
this actually is higher.

168
00:12:42,620 --> 00:12:47,010
And so these abilities will also change as an agent goes through.

169
00:12:47,010 --> 00:12:52,920
So even though here we've got you to nobody is to say that sometimes 5 percent of the time to be more

170
00:12:52,920 --> 00:12:59,580
precise we'll be selecting Q1 as the action to take and sometimes or action one will be taking action

171
00:12:59,580 --> 00:13:05,310
one sometimes we'll be taking action through a 2 action 3 2 percent of the time and action 4 will be

172
00:13:05,310 --> 00:13:06,340
taking about 3 percent.

173
00:13:06,390 --> 00:13:13,770
So every action has a chance to play in this process as long as we have enough iterations an agent goes

174
00:13:13,770 --> 00:13:17,880
through lots and lots of times through these states that they're in.

175
00:13:17,880 --> 00:13:23,820
And that's that's how this that's how any kind of deep learning algorithm works that you want to do

176
00:13:23,820 --> 00:13:29,970
this many many times so that you learn from experience and therefore as you can see here it's a very

177
00:13:29,970 --> 00:13:35,100
natural transition to we're not just randomly like an absolute grid algorithm and not just randomly

178
00:13:35,100 --> 00:13:42,770
selecting the actions we're selecting them based on their soft max values which makes it makes it like

179
00:13:42,780 --> 00:13:47,370
has some logic behind it not just not just at random 10 percent of the time we're selecting a random

180
00:13:47,370 --> 00:13:52,500
action but there's some logic behind how we're doing it and based on their the Q values that we've explored

181
00:13:53,220 --> 00:13:58,560
and so that's the action selection policy that we're going to be using in this course.

182
00:13:58,560 --> 00:14:04,530
You're welcome to definitely check out Epsilon greedy action section policy if you like but we're going

183
00:14:04,530 --> 00:14:10,860
to be predominately using the soft Max action social policy and I've got an interesting reading for

184
00:14:10,860 --> 00:14:11,400
you.

185
00:14:11,460 --> 00:14:17,370
So this is called adaptive Epsilon greedy exploration in reinforcement learning based on value differences

186
00:14:17,380 --> 00:14:24,420
it's the 2010 article and it's interesting because Mike Michelle I'm not sure how to pronounce it Michelle

187
00:14:24,420 --> 00:14:32,790
and me Karl toxic introduces a different type of algorithm so an adjusted Epsilon greedy algorithm and

188
00:14:33,390 --> 00:14:40,920
called the video video algorithm or Epsilon greedy VTB algorithm you can see it over here and he actually

189
00:14:40,920 --> 00:14:44,160
compares computers to the absolute reading itself Max.

190
00:14:44,160 --> 00:14:53,160
And it's an absolute greedy algorithm which basically the main idea behind it is to adjust the value

191
00:14:53,160 --> 00:14:55,250
of epsilon depending on the state.

192
00:14:55,290 --> 00:15:00,970
The agent is the answer if if the agent is very certain about this there and then Epsilon should be

193
00:15:00,970 --> 00:15:03,940
smaller so there should be less exploration if the agent is uncertain.

194
00:15:04,030 --> 00:15:08,700
Absence should be higher should be more exploration so it is a 2010 article.

195
00:15:09,220 --> 00:15:17,920
I'm not sure if it's if this new proposed algorithm is widely used or is as being accepted in the community

196
00:15:17,920 --> 00:15:22,230
or if artificial terms has moved kind of away from this.

197
00:15:22,240 --> 00:15:28,990
This suggestion but nevertheless it will definitely help you reinforce your knowledge about action selection

198
00:15:28,990 --> 00:15:32,090
policies which would discuss the absolute ingredient the soft Max I'll help you.

199
00:15:32,090 --> 00:15:36,580
It'll give you an opportunity compare them side by side and also see in which direction people actually

200
00:15:36,580 --> 00:15:39,270
think when they want to improve artificial intelligence.

201
00:15:39,270 --> 00:15:46,910
So if you're ever planning on creating really interesting algorithms that are pushing the edge of ultra

202
00:15:46,920 --> 00:15:52,780
artificial intelligence and pushing the envelope in this space then this could be a good way for you

203
00:15:52,780 --> 00:16:00,820
to see in which direction people think sometimes when they're trying to improve the norms of artificial

204
00:16:00,820 --> 00:16:03,990
intelligence or the norms that existed back then in 2010.

205
00:16:04,000 --> 00:16:04,600
So there we go.

206
00:16:04,750 --> 00:16:10,990
Hopefully you enjoyed today's tutorial about the action selection policies and we learned about Epsilon

207
00:16:10,990 --> 00:16:13,550
greedy apps on soft and the soft Macs.

208
00:16:13,720 --> 00:16:18,090
And now you're even more prepared for the practical side of things.

209
00:16:18,250 --> 00:16:20,700
And on that note I look forward to seeing you next time.

210
00:16:20,790 --> 00:16:22,510
And until then enjoy a.
