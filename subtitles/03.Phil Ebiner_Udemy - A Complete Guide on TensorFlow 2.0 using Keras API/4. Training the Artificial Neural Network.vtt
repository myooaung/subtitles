WEBVTT
1
00:00:01.000 --> 00:00:04.180
Okay so I hope you'll have fun in the exercises.

2
00:00:04.180 --> 00:00:08.220
You know you build some other cool architectures of new networks.

3
00:00:08.230 --> 00:00:10.630
And now now that we're done building it.

4
00:00:10.690 --> 00:00:12.740
Well we have to compile it.

5
00:00:12.760 --> 00:00:14.260
What does it mean to compile it.

6
00:00:14.620 --> 00:00:21.670
Well that means that we have to connect it to an optimizer and choose a loss in the optimizer is the

7
00:00:21.670 --> 00:00:28.270
tool that will update the weights during stochastic gradient descent when back propagating your less

8
00:00:28.480 --> 00:00:30.080
back into the neural network.

9
00:00:30.100 --> 00:00:31.360
And so here we go.

10
00:00:31.360 --> 00:00:33.260
That's how you can pallet.

11
00:00:33.430 --> 00:00:38.950
You take your model again and this time you're gonna use another method which is the compound method.

12
00:00:38.950 --> 00:00:43.720
Right this is not the method of the sequential class and inside this compound method you're going to

13
00:00:43.720 --> 00:00:45.970
enter the following arguments.

14
00:00:45.970 --> 00:00:51.210
The first one is Europe's miser which will update the way through stochastic gradient descent.

15
00:00:51.340 --> 00:00:56.350
And one of the best optimizer stochastic gradient descent is called the atom optimizer.

16
00:00:56.350 --> 00:01:00.070
I recommend to choose this one as a default option.

17
00:01:00.070 --> 00:01:05.110
Then you're gonna have to input the loss which is how you're going to compute you know the error between

18
00:01:05.110 --> 00:01:10.810
your predictions and the targets which you have in the training set right in a training set we have

19
00:01:10.810 --> 00:01:17.020
the answers for each of the images and we're going to compare these right answers to the predictions

20
00:01:17.360 --> 00:01:23.680
meaning the answers of the new network to compute basically how correct or incorrect our new network

21
00:01:23.680 --> 00:01:27.160
was with its predictions and then the metrics.

22
00:01:27.160 --> 00:01:28.900
And that's the accuracy right.

23
00:01:28.930 --> 00:01:34.130
You know the number of correct predictions divided by the total number of observations which is sixty

24
00:01:34.310 --> 00:01:37.790
thousand and training set and ten thousand test.

25
00:01:38.140 --> 00:01:43.780
And why do we have this complex named for the accuracy as sparse categorical accuracy.

26
00:01:43.780 --> 00:01:46.300
Well that's because we have several categories right.

27
00:01:46.300 --> 00:01:47.320
We have ten of them.

28
00:01:47.620 --> 00:01:51.790
I think when you do binary classification it's only accuracy.

29
00:01:51.790 --> 00:01:57.550
And however when you have more than two categories or classes to predict you're going to use this metric

30
00:01:57.640 --> 00:02:00.190
the sparse categorical accuracy.

31
00:02:00.250 --> 00:02:00.520
Right.

32
00:02:00.520 --> 00:02:02.500
Very simply that's how you can value model.

33
00:02:02.530 --> 00:02:04.630
And then there you go.

34
00:02:04.630 --> 00:02:06.400
You're ready to train it.

35
00:02:06.490 --> 00:02:12.530
But first before I show you how to train it there is this cool method that you can use again to get

36
00:02:12.530 --> 00:02:14.030
the summary of your model.

37
00:02:14.090 --> 00:02:20.080
You know if you want to visualize this better because there you go when you run model that summary you

38
00:02:20.080 --> 00:02:20.760
get this.

39
00:02:20.770 --> 00:02:24.880
So indeed this is a sequential model as opposed to computational graph.

40
00:02:25.060 --> 00:02:30.390
And then you get the information that each of the layers inside your new network that you've just built.

41
00:02:30.400 --> 00:02:38.000
So indeed we have a first fully connected layer or dense layer with 1 128 neurons inside.

42
00:02:38.020 --> 00:02:46.030
Then we have our second layer with drop out which once again has 128 neurons inside.

43
00:02:46.060 --> 00:02:49.700
And finally a final fully connected layer.

44
00:02:49.840 --> 00:02:53.410
And finally our output layer that is fully connected.

45
00:02:53.530 --> 00:02:58.800
That's why we have dense here again that is fully connected to this layer which are about right.

46
00:02:58.930 --> 00:03:05.860
And we have indeed 10 output neurons in our output layer corresponding to the 10 classes among which

47
00:03:05.950 --> 00:03:08.590
are neural network has to break the correct one.

48
00:03:08.590 --> 00:03:08.980
All right.

49
00:03:08.980 --> 00:03:11.420
And then you have some extra info here.

50
00:03:11.500 --> 00:03:17.440
So all parameters we have this all number of parameters that's interesting and that corresponds to all

51
00:03:17.440 --> 00:03:20.080
the weights inside your neural network.

52
00:03:20.290 --> 00:03:25.540
Then trainable parameters you know meaning the parameters you can update through the training.

53
00:03:25.570 --> 00:03:31.270
In other words the weights are the same because indeed in our new network we don't have any hyper parameter

54
00:03:31.680 --> 00:03:36.940
you know hyper parameter is something that is not obtained through the training but which you can tune

55
00:03:37.120 --> 00:03:41.470
then by experimenting several trainings of your neural network.

56
00:03:41.470 --> 00:03:47.380
But here we don't have any anyway and there we go yes non trainable parameters these are the hyper parameters

57
00:03:47.670 --> 00:03:48.570
zero.

58
00:03:48.640 --> 00:03:50.080
Okay perfect.

59
00:03:50.080 --> 00:03:57.190
So now that we have a moral build and that is also compiled with the best stochastic gradient descent

60
00:03:57.280 --> 00:04:03.640
optimizer the right looks for you know a multi class classification and the right metrics the accuracy

61
00:04:03.820 --> 00:04:05.710
when having several classes.

62
00:04:05.800 --> 00:04:13.450
Well we are ready for the next step which is to train the moral and there is nothing more simple to

63
00:04:13.450 --> 00:04:17.610
train the model with denser flow it is actually in just one line of code.

64
00:04:17.770 --> 00:04:23.110
And besides the arguments are so simple you're just going to take your model again.

65
00:04:23.110 --> 00:04:27.700
Then from it you're going to call the fifth method which once again is another method of the sequential

66
00:04:27.700 --> 00:04:32.290
class and you're going to input just three compulsory arguments.

67
00:04:32.290 --> 00:04:39.820
First your input which are your reshaped images you know in this 2D array containing all the images

68
00:04:39.820 --> 00:04:43.640
and rows and all the pixels of the images in columns.

69
00:04:43.750 --> 00:04:49.570
Then why train which are the real answers or in other words the targets which are the right classes

70
00:04:49.570 --> 00:04:51.870
for each of the images in extra.

71
00:04:52.090 --> 00:04:58.000
And then finally the number of epochs meaning the number of times you're going to train on the full

72
00:04:58.210 --> 00:05:04.900
amount of images in a strain you know basically a number of times the full training set will enter the

73
00:05:04.900 --> 00:05:05.950
neural network.

74
00:05:05.970 --> 00:05:12.190
Get the predictions including the laws back propagate the laws and this for all the observations in

75
00:05:12.190 --> 00:05:17.050
the training set and then there is an optional argument which you can add which is the batch size and

76
00:05:17.060 --> 00:05:23.090
that's if you want to do mini batch learning which is the combination of you know remembers to castigate

77
00:05:23.200 --> 00:05:27.780
descent and that's learning everything is explained in the annex parts.

78
00:05:27.910 --> 00:05:33.150
But here you're going to see that with these simple arguments and with no batch size we're gonna get

79
00:05:33.560 --> 00:05:38.140
a very good accuracy in the end on the evaluations that you'll see.

80
00:05:38.140 --> 00:05:40.420
All right so when you execute this what do we get.

81
00:05:40.420 --> 00:05:47.100
Well we get our five epochs with for each the accuracy obtained on the training sets.

82
00:05:47.110 --> 00:05:48.950
That's not the final victory.

83
00:05:49.060 --> 00:05:54.430
It will be of course on the evaluation side but on the training set we already have a good sign.

84
00:05:54.430 --> 00:06:02.560
We get 81 percent accuracy on the first epic 85 percent accuracy on the second epic 80 almost seven

85
00:06:02.560 --> 00:06:08.710
percent accuracy on the third epic eighty seven point thirty three percent on the fourth epic and eighty

86
00:06:08.710 --> 00:06:11.630
seven point seventy two percent on the fifth epic.

87
00:06:11.650 --> 00:06:17.390
So you see the purpose of training it on several epochs is because over a certain number of bugs.

88
00:06:17.560 --> 00:06:22.780
Well the accuracy will improve because the new network remembers the previous updates of the weights

89
00:06:23.050 --> 00:06:28.960
in order to make some new updates and improve the accuracy as the new network clearly understands that

90
00:06:28.960 --> 00:06:29.660
it is its goal.

91
00:06:29.660 --> 00:06:31.060
You know that it is what it has to do.

92
00:06:31.450 --> 00:06:37.090
So you can actually try with more bugs and I'm sure you can improve a bit more the accuracy but not

93
00:06:37.090 --> 00:06:43.030
too much because indeed we can see that it's about to convert but then maybe who knows with other architectures

94
00:06:43.330 --> 00:06:49.940
you can get some higher accuracy and that will actually be of course the exercises of this section.

95
00:06:49.960 --> 00:06:50.520
All right.

96
00:06:50.620 --> 00:06:56.800
So we're going to take a little break now and we're going to keep the surprise for what we get in terms

97
00:06:56.890 --> 00:07:00.480
of accuracy on the evaluation set right.

98
00:07:00.490 --> 00:07:03.550
This is really the final result we're interested in.

99
00:07:03.550 --> 00:07:07.900
So I'll see you in the next story all and we'll find out about that final accuracy.
