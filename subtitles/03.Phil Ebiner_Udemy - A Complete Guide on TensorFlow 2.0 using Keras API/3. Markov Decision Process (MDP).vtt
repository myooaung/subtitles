WEBVTT
1
00:00:01.030 --> 00:00:03.760
Hello and welcome back to the course on artificial intelligence.

2
00:00:03.760 --> 00:00:08.630
And today we're talking about Markov Decision Processes or MDD.

3
00:00:08.680 --> 00:00:10.740
Let's have a look what we've got today.

4
00:00:11.350 --> 00:00:13.990
So last time we stopped on the concept of a map.

5
00:00:14.020 --> 00:00:19.420
So because we have calculated the values based on the Belmont equation we can derive this map for our

6
00:00:19.420 --> 00:00:21.190
agent on this maze.

7
00:00:21.190 --> 00:00:27.430
And basically what that means is wherever the NJ agent starts so let's say it starts over there.

8
00:00:27.490 --> 00:00:30.820
It knows exactly which steps to take in order to get to the finish line.

9
00:00:30.820 --> 00:00:32.670
So it just goes up up.

10
00:00:32.680 --> 00:00:33.250
Right.

11
00:00:33.250 --> 00:00:33.880
Right.

12
00:00:33.880 --> 00:00:34.930
And done.

13
00:00:35.020 --> 00:00:37.540
And so the question here is is that it.

14
00:00:37.540 --> 00:00:43.930
Is it really that simple is reinforcement learning really that you know for the lack of a better word

15
00:00:43.930 --> 00:00:44.660
boring.

16
00:00:44.740 --> 00:00:49.570
It's it's you once you have the map that's it all you have to do is you're done.

17
00:00:49.690 --> 00:00:51.000
You just follow the map.

18
00:00:51.010 --> 00:00:55.410
Well the reality is that it's not actually that simple.

19
00:00:55.450 --> 00:01:00.970
And that's a good thing because it makes this course more interesting for us and we can actually solve

20
00:01:00.970 --> 00:01:02.290
much more complex problems.

21
00:01:02.560 --> 00:01:05.400
So this is where a mark of a process is coming.

22
00:01:05.440 --> 00:01:10.810
But first we're going to talk about two things we don't into a deterministic search versus non deterministic

23
00:01:10.810 --> 00:01:11.140
search.

24
00:01:11.620 --> 00:01:14.680
So let's talk about the concept of deterministic search.

25
00:01:14.740 --> 00:01:21.520
This is our agent in the maze and deterministic search means that if the agent decides to go up then

26
00:01:21.520 --> 00:01:26.930
what will happen is with 100 percent probability it will go up.

27
00:01:26.950 --> 00:01:28.630
That's exactly what will happen.

28
00:01:28.630 --> 00:01:29.820
There's no other options.

29
00:01:29.850 --> 00:01:33.640
Once it says go up or clicks the up arrow it will go up.

30
00:01:33.640 --> 00:01:35.010
There's no other options.

31
00:01:35.220 --> 00:01:41.420
Now on the other hand non deterministic search is when our agent says it wants to go up.

32
00:01:42.120 --> 00:01:44.380
They are actually couple of options.

33
00:01:44.380 --> 00:01:48.520
For example there could be three options and we're going to be looking at example where there are three

34
00:01:48.520 --> 00:01:50.860
options but it doesn't have to be limited to three before.

35
00:01:50.920 --> 00:01:56.410
It could be different depending on depends on the problem the randomness could be different but in our

36
00:01:56.410 --> 00:02:01.610
case it could be three options with an 80 percent chance he does go up.

37
00:02:01.810 --> 00:02:07.810
But then with a 10 percent chance when he was to go up he'll actually go to the left just because because

38
00:02:07.810 --> 00:02:10.630
that's how the environment works that's the world that he lives in.

39
00:02:11.380 --> 00:02:17.680
And if another 10 percent chance he'll actually go right and in this case he'll fall into the firepit.

40
00:02:17.800 --> 00:02:20.700
So that is how it all works.

41
00:02:20.710 --> 00:02:26.710
That's an example of a non deterministic sure search a stochastic process and what the point of this

42
00:02:26.710 --> 00:02:34.990
is is to make a more realistic model of what could actually happen even in a real world in a real world

43
00:02:34.990 --> 00:02:40.120
type of problem because very rarely do you get situations like this when when you do something and it

44
00:02:40.120 --> 00:02:41.390
happens exactly that way.

45
00:02:41.470 --> 00:02:47.860
And even if you think about it in terms of games that say you've got an agent playing Pacman well not

46
00:02:47.860 --> 00:02:53.020
always Is it the case that if he's standing in the square he goes up he will get the same exact result

47
00:02:53.020 --> 00:02:54.550
every time he will.

48
00:02:54.640 --> 00:03:00.190
He will indeed go up but it may be in one case he won't get eaten by a ghost in another case he will

49
00:03:00.190 --> 00:03:01.260
get eaten by a ghost.

50
00:03:01.540 --> 00:03:05.920
So as you can see there's some randomness to it because it depends on how the ghosts are moving and

51
00:03:05.920 --> 00:03:07.300
they don't always move the same way.

52
00:03:07.300 --> 00:03:09.430
They don't always start in the same locations.

53
00:03:09.430 --> 00:03:16.090
So it's very logical is very it is fair that there is some random is there's something that is not under

54
00:03:16.090 --> 00:03:21.430
the control of the agent and that is this is just a way for us to represent that in order for us to

55
00:03:21.430 --> 00:03:27.190
learn how we can deal with it and how that affects the Belmont equation how it affects the whole reinforcement

56
00:03:27.190 --> 00:03:28.640
learning process.

57
00:03:29.020 --> 00:03:33.730
But at the same time the randomness is of course not limited to if you go up there's a 10 percent chance

58
00:03:33.730 --> 00:03:38.020
you'll go right or attempts and just go left or if you go down there the temperatures chances go right

59
00:03:38.020 --> 00:03:42.340
or left or if you're right there's attempts and chances go up or down sort of limited to where you're

60
00:03:42.340 --> 00:03:46.600
going to end up sometimes you might have a problem that is exactly sometimes the probabilities might

61
00:03:46.600 --> 00:03:47.380
be different.

62
00:03:47.380 --> 00:03:52.850
Sometimes the randomness might boil down to something else it might be boiled boiled down like an example

63
00:03:52.850 --> 00:03:59.290
of Pacman of the ghosts eating you or not eating you or it might boil down to something different for

64
00:03:59.290 --> 00:04:06.280
instance like there is there's like if the agent is playing Doom and then there's something like a monster

65
00:04:06.280 --> 00:04:11.500
which is going to shoot him in one case another came there's like there's a probability with which it

66
00:04:11.500 --> 00:04:14.860
will get shot and with which it won't get shot and so on.

67
00:04:14.860 --> 00:04:19.660
So something that is out of the control the agents something I cannot predict.

68
00:04:19.660 --> 00:04:25.690
That's what we're modelling here in non deterministic search and this is where we have directly approached

69
00:04:25.840 --> 00:04:32.730
two new concepts of Markov processes and or a Markov process and mark a mark Markov decision process.

70
00:04:32.740 --> 00:04:38.650
So let's have a look at these and you know how much I don't like to put definitions and lots of text

71
00:04:38.650 --> 00:04:42.250
on the sides but in this case it is necessary for us to go through them.

72
00:04:42.250 --> 00:04:46.180
So let's have a look a stochastic process has a mark of property.

73
00:04:46.180 --> 00:04:51.700
If the conditional probability distribution of future states of the process conditional on both past

74
00:04:51.700 --> 00:04:57.820
and present state depends only upon the present state not on the sequence of events that preceded it.

75
00:04:58.180 --> 00:05:04.050
A process with this property is called a market very complex definition and kind of like giving credit

76
00:05:04.530 --> 00:05:08.820
a little bit not only contradicts itself but feels like it contradicts itself so here is as conditional

77
00:05:08.820 --> 00:05:13.650
both a positive presence that depends only upon but at the same time it only depends upon the present

78
00:05:13.650 --> 00:05:14.030
state.

79
00:05:14.460 --> 00:05:17.650
So don't get too bogged down in that.

80
00:05:17.780 --> 00:05:22.980
I'll break it down in simple terms so a mark of property is when your future states.

81
00:05:23.010 --> 00:05:27.160
So not just your choice but the whole thing your choice and the environment.

82
00:05:27.240 --> 00:05:32.580
It will only like the results of a world of you of the action you take in that environment will only

83
00:05:32.580 --> 00:05:33.870
depend on where you are now.

84
00:05:33.870 --> 00:05:36.510
It will not depend on how you got there and that's it.

85
00:05:36.510 --> 00:05:40.550
So as a matter of property and a process which has this property is called the market process.

86
00:05:40.830 --> 00:05:41.500
So in.

87
00:05:41.530 --> 00:05:47.940
To put it into an example so if your agent is here and if he goes if he decides to go up he might go.

88
00:05:47.970 --> 00:05:53.640
He in our case in our non deterministic search example you actually might go left and right or the right.

89
00:05:53.640 --> 00:05:57.530
That's because we have that sticker still city inside our environment.

90
00:05:57.540 --> 00:06:01.770
We have that randomness inside our environment so anything one of these three might happen.

91
00:06:01.770 --> 00:06:07.130
But the key here is that this is a mark of process because we don't care how you got here.

92
00:06:07.170 --> 00:06:10.630
He could have come from the Top End and up here he could have come from the left and get up here he

93
00:06:10.650 --> 00:06:12.330
could have come from the bottom ended up here.

94
00:06:12.330 --> 00:06:16.590
He could have like play and moved around here like a hundred thousand times and then got here.

95
00:06:16.650 --> 00:06:18.600
It does not matter what happened before.

96
00:06:18.780 --> 00:06:22.250
Only what matters is what which state is he in now.

97
00:06:22.470 --> 00:06:29.850
And so that the probabilities of going left or right or up they will always be the same.

98
00:06:29.880 --> 00:06:36.550
If he is in this state now and so that's basically just saying doesn't matter what happened before we

99
00:06:36.550 --> 00:06:37.590
are here now.

100
00:06:37.650 --> 00:06:42.270
This is the state you're in and don't go figure that state doesn't just mean where he's standing.

101
00:06:42.270 --> 00:06:46.620
The state is the state of the whole of the whole of the agent in the environment.

102
00:06:46.620 --> 00:06:52.260
So is that like monsters on the right or the monsters on the left or is the ghost coming from the top

103
00:06:52.260 --> 00:06:54.400
of the bottom whatever state you're in now.

104
00:06:54.480 --> 00:06:58.260
Doesn't matter how you got there it doesn't matter how it how it all came to be that you're there in

105
00:06:58.260 --> 00:07:03.390
that state now what will happen in the future is only determined by the state you're in now plus the

106
00:07:03.390 --> 00:07:07.220
actions you will take then plus of course the randomness that is overlaid on top of that.

107
00:07:07.410 --> 00:07:14.670
So that's a mark of process and a market decision process or a MVP or market decision processes provide

108
00:07:14.670 --> 00:07:20.370
a mathematical framework for model modelling decision making in situations where outcomes are partly

109
00:07:20.370 --> 00:07:23.220
random and partly under control over decision maker.

110
00:07:23.490 --> 00:07:29.520
So important I understand that Markov decision processes processes are different different whole concept

111
00:07:29.530 --> 00:07:35.400
Markov process to a Markov process they're kind of like a mathematical framework so but at the same

112
00:07:35.400 --> 00:07:39.870
time I thought it was important for us to understand what a Markov process is because I think it still

113
00:07:40.170 --> 00:07:45.900
helps in the understanding of a market market decision process so a market decision process is.

114
00:07:46.040 --> 00:07:48.240
This is exactly what we've been discussing up till now.

115
00:07:48.270 --> 00:07:53.850
So that the agent lives in this environment where it has control like room previously had full control

116
00:07:53.850 --> 00:07:55.020
of the of what's going on.

117
00:07:55.020 --> 00:07:57.430
But now it has a little bit less control.

118
00:07:57.540 --> 00:08:00.230
It can decide to go up but it actually knows.

119
00:08:00.250 --> 00:08:04.800
Okay so if I go up there's an 80 percent chance I'll go up there's attempts and chances on the left

120
00:08:04.800 --> 00:08:06.080
temps and chances go right.

121
00:08:06.090 --> 00:08:08.880
So not everything is fully under its control.

122
00:08:08.880 --> 00:08:10.650
There is some randomness in this environment.

123
00:08:10.650 --> 00:08:15.570
That's exactly what a Markov Decision Processes and Markov decision process is the framework that the

124
00:08:15.570 --> 00:08:19.380
agent will use in order to understand what to do in this environment.

125
00:08:19.380 --> 00:08:23.760
So we've got an environment with some stochastic cities some randomness and now the agent has to choose

126
00:08:23.760 --> 00:08:28.490
for instance should go up down left or right has to make that decision.

127
00:08:28.500 --> 00:08:34.440
It doesn't know what to do and in order to make that decision is going to apply a framework is going

128
00:08:34.440 --> 00:08:39.890
to be using Markov decision process in order to make that decision what's going to happen where it's

129
00:08:39.900 --> 00:08:40.920
going to go.

130
00:08:40.920 --> 00:08:47.550
And so basically this environment that poses this problem it is referred to the mark of decision process.

131
00:08:47.550 --> 00:08:52.770
So it's the framework that agent using at the same time the environment is referred to that the agent

132
00:08:52.770 --> 00:08:55.960
is operating in a market decision process environment.

133
00:08:56.280 --> 00:08:57.950
And so basically here we've got two concepts.

134
00:08:57.960 --> 00:09:01.710
We've got the mark of process is the way this environment is designed.

135
00:09:01.710 --> 00:09:03.840
That the part that does the work.

136
00:09:03.870 --> 00:09:06.930
What happens from where you are now doesn't depend on the past.

137
00:09:07.080 --> 00:09:11.070
And at the same time we've got the market decision process is the framework that the agent is going

138
00:09:11.070 --> 00:09:13.790
to be using in order to solve this environment.

139
00:09:13.920 --> 00:09:18.780
And the good news is that the Markov decision process on that framework that we're talking about is

140
00:09:18.780 --> 00:09:23.150
actually just an add on to our Belman equation is the Belmont equation.

141
00:09:23.160 --> 00:09:24.710
But just a bit more sophisticated.

142
00:09:24.720 --> 00:09:26.980
So let's have a look at that.

143
00:09:27.030 --> 00:09:28.970
This is our Belman equation so far.

144
00:09:28.980 --> 00:09:30.970
It's the maximum of all possible actions.

145
00:09:30.990 --> 00:09:35.100
So the value over being an estate is the maximum of all possible actions that you can take from that

146
00:09:35.100 --> 00:09:35.990
state.

147
00:09:36.210 --> 00:09:41.880
The maximum is taken from the reward that you would get by taking that action in that state plus a discount

148
00:09:41.880 --> 00:09:45.320
factor times the value of the next state which is s prime.

149
00:09:45.360 --> 00:09:47.190
So that's what we've had so far.

150
00:09:47.370 --> 00:09:52.500
Now because we have some randomness in our whole process this this pie will change because we don't

151
00:09:52.500 --> 00:09:57.570
actually know which state will end up and we don't know what s prime will be will it be if we're going

152
00:09:57.570 --> 00:09:59.850
up will it be up or we'll be left or will be right.

153
00:09:59.850 --> 00:10:04.470
So we actually have to place this with the expected value of the next state.

154
00:10:04.920 --> 00:10:06.360
So here we're going to replace this.

155
00:10:06.360 --> 00:10:08.510
So there's three possible states we can end up in.

156
00:10:08.760 --> 00:10:12.670
And so we're going to replace that with some value there.

157
00:10:12.720 --> 00:10:15.420
That state has a value of as one prime.

158
00:10:15.420 --> 00:10:18.230
That said it has a v of s prime two as to prime.

159
00:10:18.420 --> 00:10:21.570
And this state has a value of V of three.

160
00:10:21.600 --> 00:10:22.490
Brian.

161
00:10:22.590 --> 00:10:28.710
So now we're going to multiply the state that we actually are intending to go into by 80 percent because

162
00:10:28.710 --> 00:10:33.990
that's our probability of getting to that state plus the probability of getting into this state 10 percent

163
00:10:33.990 --> 00:10:35.310
plus approval of getting in state.

164
00:10:35.310 --> 00:10:37.950
So this is just our expected value.

165
00:10:37.950 --> 00:10:45.720
So if from statistics if we take the expected value of getting into the state that we'll get into it

166
00:10:45.960 --> 00:10:46.980
kind of like the average.

167
00:10:46.980 --> 00:10:52.880
What's what's the average of what will get and then we replace that over here then we get the equation.

168
00:10:52.880 --> 00:10:56.820
No it jumps very quickly and just because there's a is bigger but if you look at it carefully you'll

169
00:10:56.820 --> 00:10:57.930
see exactly the same thing.

170
00:10:57.940 --> 00:11:05.610
You've got Max here Max here and then you've got R of S and a r of S and a here you've got gamma you've

171
00:11:05.610 --> 00:11:06.290
got gamma.

172
00:11:06.360 --> 00:11:08.570
And then finally here you've got V.

173
00:11:08.580 --> 00:11:13.560
So you knew exactly it was a deterministic search you knew which states you'll get into.

174
00:11:13.560 --> 00:11:16.020
Now you don't know which state you'll get into instead of taking V.

175
00:11:16.020 --> 00:11:18.810
You're taking the expected value of the states.

176
00:11:18.820 --> 00:11:25.080
You'll get into or of the future state or just in simpler terms you're just taking the average of what

177
00:11:25.080 --> 00:11:25.980
you'll get into.

178
00:11:25.980 --> 00:11:31.020
So you know if like it was a in a in like for as a 30 plus 3 percent chance it will be like this plus

179
00:11:31.020 --> 00:11:32.840
this plus is divide by three basically.

180
00:11:32.850 --> 00:11:38.700
But in this case it's not it's not exactly like average average it's it's a weighted average because

181
00:11:38.700 --> 00:11:40.370
of the probabilities here.

182
00:11:40.380 --> 00:11:45.930
So here you've got the probability of it when you're in this state you take this action of getting into

183
00:11:45.990 --> 00:11:48.330
state as prime time the value as prime.

184
00:11:48.360 --> 00:11:51.750
And some across all this primes that you could possibly get into over here.

185
00:11:51.780 --> 00:11:57.270
So exactly what we have three here one two three add them up multiply by parolees add them up.

186
00:11:57.270 --> 00:11:57.880
Same here.

187
00:11:57.990 --> 00:11:58.750
One two three.

188
00:11:58.770 --> 00:12:01.510
Multiply them by both probabilities and add them up.

189
00:12:02.040 --> 00:12:05.130
And that is your new Belmont equation.

190
00:12:05.130 --> 00:12:06.370
Congratulations.

191
00:12:06.390 --> 00:12:08.880
This is what we're going to be working with going forward.

192
00:12:09.090 --> 00:12:13.530
And that is the framework that is used in Markov Decision Processes.

193
00:12:13.530 --> 00:12:20.700
So that is the framework that solves this that agents used to solve this whole stochastic non deterministic

194
00:12:20.700 --> 00:12:25.400
search problem where there is random events that are happening that they cannot control.

195
00:12:25.410 --> 00:12:26.870
So it's much more complex.

196
00:12:26.880 --> 00:12:33.390
But as you can see because we built up slowly to it now we already know about this we know about there's

197
00:12:33.420 --> 00:12:35.070
worry about this.

198
00:12:35.070 --> 00:12:36.090
We know about this.

199
00:12:36.090 --> 00:12:36.670
We know what they are.

200
00:12:36.660 --> 00:12:42.450
So all we did is we just introduced this part over here because there are probabilities involved in

201
00:12:42.870 --> 00:12:49.170
the action or the consequences of your action and on deterministic they are based on some probabilities.

202
00:12:49.170 --> 00:12:50.510
And so there we go.

203
00:12:50.550 --> 00:12:54.790
That's how a mark of decision process works.

204
00:12:54.810 --> 00:12:58.100
And the underlying equation behind it.

205
00:12:58.290 --> 00:13:04.260
Once again it is something that is more that more closely resembles the real world problems real world

206
00:13:04.260 --> 00:13:08.640
scenarios or even game scenarios because not everything is straightforward.

207
00:13:08.640 --> 00:13:14.110
There is some randomness of all involved and not always will.

208
00:13:14.160 --> 00:13:18.830
Taking an action in a certain state will always not will not always will it lead to the same outcome.

209
00:13:18.840 --> 00:13:23.070
And so this is what we're going to be dealing with going forward and that's going to make things way

210
00:13:23.070 --> 00:13:24.290
more interesting.

211
00:13:24.330 --> 00:13:29.560
So hopefully you're excited for that and excited to see what's going to come next.

212
00:13:29.640 --> 00:13:35.810
And in the meantime I've found a really cool paper for you to have a look at this time.

213
00:13:35.810 --> 00:13:37.390
It's a very applied paper.

214
00:13:37.410 --> 00:13:40.100
So this was actually really interesting to read through.

215
00:13:40.110 --> 00:13:46.760
It's called a survey of applications of mark of decision processes processes and it was ruled by white.

216
00:13:46.770 --> 00:13:55.290
In 1993 there's link and it'll show you examples of where market decision processes actually are used

217
00:13:55.290 --> 00:13:56.970
to model real life scenarios.

218
00:13:56.970 --> 00:13:59.510
I think I was very excited by this.

219
00:13:59.510 --> 00:14:03.750
I was impressed by some examples so population harvesting for instance.

220
00:14:03.760 --> 00:14:08.730
So let's say you have some fish and you know what the population of the fishing is you need to decide

221
00:14:08.730 --> 00:14:13.220
how many fish can we fish out this year and what.

222
00:14:13.220 --> 00:14:14.250
So that's your current state.

223
00:14:14.250 --> 00:14:15.570
That's the action that you're taking.

224
00:14:15.570 --> 00:14:17.150
How many can we've shot at this year.

225
00:14:17.160 --> 00:14:18.270
So what are the odds.

226
00:14:18.270 --> 00:14:20.460
What are the possible outcomes of that.

227
00:14:20.490 --> 00:14:22.060
How many fish will we have next year.

228
00:14:22.110 --> 00:14:25.050
How many official here we have the year after and the year after and so on.

229
00:14:25.050 --> 00:14:30.180
And it's not deterministic because it's not like if you take out and about 90 percent of the population

230
00:14:30.180 --> 00:14:34.590
the next year you will have you know back to 100 percent it's not not exactly surrealistic.

231
00:14:34.590 --> 00:14:39.510
There are certain random factors involved which are out of our control and therefore we have to understand

232
00:14:39.690 --> 00:14:42.600
what's what's going to happen we have to model what's going to happen.

233
00:14:42.600 --> 00:14:45.800
That's where a market decision process is used agriculture.

234
00:14:46.000 --> 00:14:49.790
There's an example like something like harvesting crops how much crops do we harvest how much.

235
00:14:49.980 --> 00:14:51.420
How many do we not harvest.

236
00:14:51.420 --> 00:14:58.140
Another one which I looked at finance and investment like an insurance company needs to decide how much

237
00:14:58.140 --> 00:15:04.940
of its funds it will invest in any I think day or year or some period of time and there are certain

238
00:15:04.940 --> 00:15:06.410
factors that are of its control.

239
00:15:06.410 --> 00:15:09.180
For instance you know the market movements it doesn't know what can happen.

240
00:15:09.260 --> 00:15:13.940
So it needs to actually model that somehow and a market decision process is used for that.

241
00:15:14.270 --> 00:15:16.840
So here you can see lots and lots of examples.

242
00:15:16.850 --> 00:15:20.720
And this is the number of examples given I think for each one.

243
00:15:20.820 --> 00:15:28.730
And even sports two examples for sports and epidemics and motor insurance claims inspections and maintenance

244
00:15:28.730 --> 00:15:29.280
and repair.

245
00:15:29.300 --> 00:15:31.840
It's also very interesting have a look at that.

246
00:15:31.880 --> 00:15:40.340
Just to give you an understanding of hey this is not just all made up stuff hypothetical and the Matrix

247
00:15:40.340 --> 00:15:41.060
type of thing.

248
00:15:41.060 --> 00:15:42.520
This is actually a real world scenario.

249
00:15:42.550 --> 00:15:47.360
So it'll give you a better understanding and this is what we talked about in the promotional video for

250
00:15:47.360 --> 00:15:52.880
this course that or description of the course that we're going to inspire you and your intuition to

251
00:15:52.880 --> 00:15:55.850
give you ideas for how to use A.I. in real life.

252
00:15:55.850 --> 00:15:57.800
This is your opportunity.

253
00:15:57.800 --> 00:15:59.590
Look at this paper to understand.

254
00:15:59.850 --> 00:16:02.840
Okay so we're gonna be dealing with Markov decision process is going forward.

255
00:16:02.840 --> 00:16:03.950
That's really cool.

256
00:16:03.950 --> 00:16:05.220
What do they look like in real life.

257
00:16:05.240 --> 00:16:10.100
And this possibly could trigger some ideas for you how you could apply A.I. in the future to make the

258
00:16:10.100 --> 00:16:13.580
world a better place and we'd be super happy about that.

259
00:16:13.640 --> 00:16:14.500
We'd be super happy.

260
00:16:14.840 --> 00:16:18.650
If you could use what you learned in this course to make the world a better place we're failing.

261
00:16:18.680 --> 00:16:20.320
How fantastic would that be.

262
00:16:20.330 --> 00:16:23.120
So on that note I hope you enjoyed today's tutorial.

263
00:16:23.120 --> 00:16:24.560
I look forward see you next time.

264
00:16:24.560 --> 00:16:26.310
And until then enjoy a.
