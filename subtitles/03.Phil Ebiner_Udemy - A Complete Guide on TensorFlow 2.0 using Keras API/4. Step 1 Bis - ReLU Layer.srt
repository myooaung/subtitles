1
00:00:00,600 --> 00:00:02,900
Hello and welcome back to the course on deep learning.

2
00:00:02,910 --> 00:00:06,150
Today we're talking about a real Lou which is rectified linear units.

3
00:00:06,180 --> 00:00:08,780
And this is an additional step.

4
00:00:08,790 --> 00:00:12,150
On top of our convolution step.

5
00:00:12,150 --> 00:00:16,230
So it's not a separate big step it's a small step it's step one b basically.

6
00:00:16,230 --> 00:00:18,150
And what is going on here.

7
00:00:18,210 --> 00:00:20,340
Well we have our input image.

8
00:00:20,340 --> 00:00:22,820
We have our convolution layer which we've discussed.

9
00:00:22,920 --> 00:00:26,170
And then on top of that we're going to apply.

10
00:00:26,340 --> 00:00:27,150
Wait for it.

11
00:00:27,180 --> 00:00:31,080
Our favorites rectify our function.

12
00:00:31,080 --> 00:00:37,680
And you're really familiar with the rest of our function from the previous section on artificial neural

13
00:00:37,680 --> 00:00:47,650
networks and in our so sometimes authorized or instructors instructor's separate the convolution and

14
00:00:47,650 --> 00:00:52,900
the rectified as two separate steps in our examples we're just going to consider them the.

15
00:00:53,130 --> 00:00:56,820
Just one big step for a second solution then rectify.

16
00:00:57,210 --> 00:01:03,720
And the reason why we're applying the rector fire is because we want to increase non linearity in our

17
00:01:03,720 --> 00:01:13,330
image or in our network and our commercial neural network and direct fire acts as that filter or access

18
00:01:13,380 --> 00:01:15,840
that function which breaks up linearity.

19
00:01:15,840 --> 00:01:23,490
And the reason why we want to increase non linearity in our network is because images themselves are

20
00:01:23,640 --> 00:01:30,010
highly nonlinear especially if you're recognizing different objects next to each other or just on this

21
00:01:30,060 --> 00:01:35,070
background and stuff like that like the image is going to have lots of nonlinear elements and the transition

22
00:01:35,070 --> 00:01:37,980
between pixels adjacent pixels is often would be non-linear.

23
00:01:37,980 --> 00:01:43,730
That's because as borders is different colors is different there's different elements in your images.

24
00:01:43,730 --> 00:01:50,040
And but at the same time when we're applying a mathematical operations such as convolution and to running

25
00:01:50,040 --> 00:01:57,360
this feature detection to create our feature maps we risk that we might create something linear and

26
00:01:57,360 --> 00:01:59,730
therefore we need to break up the linearity.

27
00:01:59,910 --> 00:02:02,090
So let's have a look at an example.

28
00:02:02,550 --> 00:02:05,910
Here is a image an original image.

29
00:02:05,910 --> 00:02:13,240
Now when we apply a feature detection detector to this image we get something like this.

30
00:02:13,240 --> 00:02:19,080
So you can see here that black is negative White is positive values well when you apply a feature detector

31
00:02:19,110 --> 00:02:24,000
to a like a proper image which has not just zeros and ones but has lots of different values and you

32
00:02:24,540 --> 00:02:26,610
apply as we saw previously featured.

33
00:02:26,630 --> 00:02:28,930
Texas can have negative values in themselves.

34
00:02:29,010 --> 00:02:31,020
Sometimes you'll get negative values.

35
00:02:31,020 --> 00:02:34,690
And here there are black ones are negative white ones are positive.

36
00:02:34,740 --> 00:02:45,470
And what a rectified linear unit function does is it removes all the black rated anything below zero

37
00:02:45,480 --> 00:02:46,230
turns into zero.

38
00:02:46,500 --> 00:02:48,650
And so from this it turns into this.

39
00:02:48,720 --> 00:02:48,990
Right.

40
00:02:49,230 --> 00:02:57,840
And so it's it's pretty hard to see what exactly is the benefit in terms of four in terms of breaking

41
00:02:57,840 --> 00:02:59,240
up linearity.

42
00:02:59,340 --> 00:03:00,950
I'll try to explain.

43
00:03:01,050 --> 00:03:04,300
I'll try to like show an example on this image.

44
00:03:04,680 --> 00:03:09,570
But at the end of the day it's this is a very mathematical concept and we'd have to go into a lot of

45
00:03:09,570 --> 00:03:12,410
math to really explain what is going on.

46
00:03:12,420 --> 00:03:13,260
But let's let's try.

47
00:03:13,260 --> 00:03:13,770
Let's have a look.

48
00:03:13,770 --> 00:03:16,830
So for instance let's look at this.

49
00:03:16,830 --> 00:03:17,550
This building here.

50
00:03:17,570 --> 00:03:17,790
Right.

51
00:03:18,030 --> 00:03:20,700
So this is a building on its own.

52
00:03:20,710 --> 00:03:24,550
Then you can see this shadow this black part of this shadow over here.

53
00:03:24,570 --> 00:03:28,800
Well you see that it's white the reflection of the light.

54
00:03:28,800 --> 00:03:32,950
Then it's a gray and then it gets darker and then it gets darker again.

55
00:03:33,000 --> 00:03:33,240
Right.

56
00:03:33,240 --> 00:03:35,850
So and when we take it out we take out that black part.

57
00:03:35,850 --> 00:03:38,190
So think of it in terms of linearity right.

58
00:03:38,190 --> 00:03:43,680
So it looks like when you go from white to gray the next step would be black.

59
00:03:43,680 --> 00:03:43,890
Right.

60
00:03:43,890 --> 00:03:51,360
The next step would be black it's it's linear progression from bright to dark and therefore this is

61
00:03:51,360 --> 00:03:56,320
kind of like a linear situation when you take out the black you break up the linearity.

62
00:03:56,640 --> 00:03:57,990
Let's try another one.

63
00:03:57,990 --> 00:04:01,920
Let's have a look here and at the same time it's still that same building right.

64
00:04:01,920 --> 00:04:08,280
It's not it's not like you or you're like It's not like you're blending two buildings into each other

65
00:04:08,460 --> 00:04:09,750
but that is secondary.

66
00:04:09,750 --> 00:04:12,000
The main point is breaking up the linearity.

67
00:04:12,150 --> 00:04:13,050
So let's have a look here.

68
00:04:13,050 --> 00:04:13,530
Same thing.

69
00:04:13,530 --> 00:04:19,560
So you see white gray black gray white.

70
00:04:19,560 --> 00:04:22,220
And when you break it up you don't have that anymore.

71
00:04:22,230 --> 00:04:22,490
Right.

72
00:04:22,500 --> 00:04:29,670
You don't have that progression a gradual progression that you just have like a an abrupt change.

73
00:04:29,670 --> 00:04:33,480
And that helps introduce non linearity into your image.

74
00:04:33,510 --> 00:04:42,450
So it's a very rough explanation very kind of like on the on the fingers explanation rather than technical

75
00:04:42,600 --> 00:04:47,310
but hopefully it kind of helps you understand a bit better what we're talking about here.

76
00:04:47,310 --> 00:04:54,750
So here again you can see white gray is a better example even see bright darker darker darker darker

77
00:04:54,770 --> 00:04:55,590
darker and darker.

78
00:04:55,620 --> 00:04:58,130
So this part looks like it's linear.

79
00:04:58,230 --> 00:05:05,810
Then you break it up like that again so this is a very rough explanation is not absolutely perfect but

80
00:05:05,810 --> 00:05:08,750
at least gives you some idea of what's going on.

81
00:05:08,750 --> 00:05:14,120
But if you'd like to learn more there's a good paper as always there's always a paper.

82
00:05:14,120 --> 00:05:15,320
This one is by C.S. J.

83
00:05:15,320 --> 00:05:17,830
Cole from the University of California.

84
00:05:17,930 --> 00:05:23,150
And it's called Understanding evolution all neural networks have a mathematical model.

85
00:05:23,150 --> 00:05:28,780
And basically there he answers to questions and you need to just look at the first one.

86
00:05:28,790 --> 00:05:34,070
And the question is why in a non-linear activation function is essential at the filter output of all

87
00:05:34,070 --> 00:05:36,170
intermediate layers.

88
00:05:36,170 --> 00:05:44,240
So that kind of explains it in a bit more detail both in terms of intuition and mostly in terms of mathematics.

89
00:05:44,270 --> 00:05:47,960
So that's an interesting paper where you can get some more additional information on this topic.

90
00:05:48,050 --> 00:05:54,590
And if you really want to dig in and explore some some cool stuff here then there's another paper that

91
00:05:54,590 --> 00:06:00,800
you might be interested in so-called delving deep into rectifying surpassing human 11 level performance

92
00:06:00,830 --> 00:06:02,870
on image and that classification.

93
00:06:02,870 --> 00:06:09,320
And here the author is combing hair and others from Microsoft Research.

94
00:06:09,320 --> 00:06:19,790
They propose a different type of rectified linear unit function or they propose the parametric rectified

95
00:06:19,790 --> 00:06:21,980
linear function which you see here on the right.

96
00:06:21,980 --> 00:06:26,680
And they argue that it delivers better results with faults sacrificing performance.

97
00:06:26,690 --> 00:06:30,210
So interesting read if you'd like to get a bit more into this topic.

98
00:06:30,440 --> 00:06:31,880
And that's all for today.

99
00:06:31,880 --> 00:06:38,120
The real you layer is pretty simple preset for adjusting just applying the rectify function and I look

100
00:06:38,120 --> 00:06:39,200
forward to seeing you next time.

101
00:06:39,200 --> 00:06:40,700
Until then enjoy deep learning.
