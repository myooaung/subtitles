WEBVTT
1
00:00:00.990 --> 00:00:03.930
Hello and welcome back to the course on artificial intelligence.

2
00:00:03.930 --> 00:00:06.960
Today we are finally talking about cute learning.

3
00:00:07.020 --> 00:00:07.470
All right.

4
00:00:07.500 --> 00:00:13.290
So we've already got this equation the Belmont equation which we've added lots of components to we've

5
00:00:13.290 --> 00:00:19.840
got the reward here which can be not just at the very end but it can be at any given step.

6
00:00:19.890 --> 00:00:21.900
We've got the discount factor.

7
00:00:21.900 --> 00:00:26.850
We've got the probability because now we're looking at Markov Decision Processes.

8
00:00:26.850 --> 00:00:32.730
And here we've got the probability of ending up in different states regardless of what action we take

9
00:00:33.300 --> 00:00:38.640
or actually given the action we take there can be multiple states that we can help end up in and then

10
00:00:38.640 --> 00:00:43.530
we've got the value of the next states because he was kind of like a recursive function and so on.

11
00:00:43.650 --> 00:00:46.550
But you probably still have one question.

12
00:00:46.740 --> 00:00:54.150
The question is where in all of this is the letter Q Why is it all called Q learning.

13
00:00:54.300 --> 00:00:58.670
So whereas the Q and that's the question that we're going to be answering today.

14
00:00:58.860 --> 00:01:04.560
So far we've been dealing with values the value of being in a certain state.

15
00:01:04.620 --> 00:01:10.010
And now we're going to look at how Q fits into all of that as well.

16
00:01:10.020 --> 00:01:14.550
So here we've got two examples on the left is what we've be doing so far.

17
00:01:14.550 --> 00:01:16.650
Our agent has been analyzing okay.

18
00:01:16.920 --> 00:01:18.150
I'm over here.

19
00:01:18.150 --> 00:01:23.520
This is a mark of decision process so doesn't matter how I got here the rest of the environment doesn't

20
00:01:23.520 --> 00:01:30.270
care of the steps that it took me to get here from now on the I have to make the optimal decision where

21
00:01:30.270 --> 00:01:30.720
to go.

22
00:01:30.720 --> 00:01:31.980
Here here here.

23
00:01:31.980 --> 00:01:37.220
Based on the current state and all the future states that come from here but not from the past.

24
00:01:37.440 --> 00:01:39.650
And so he can see that there's three options.

25
00:01:39.650 --> 00:01:46.110
There's state one state to state three and based on his experience he has calculated the values in these

26
00:01:46.110 --> 00:01:49.830
states and now he's going to using the Belmont equation.

27
00:01:49.830 --> 00:01:53.330
So even though this is a stochastic process so he knows that he'll go here.

28
00:01:53.340 --> 00:01:57.990
But there's a chance that he'll go left or right and so on so based on these values going to make a

29
00:01:57.990 --> 00:02:03.440
decision that's what we've been do so far and that is totally the legitimate approach here.

30
00:02:03.510 --> 00:02:05.610
But now we're going to modify it a little bit.

31
00:02:05.610 --> 00:02:12.780
We're going to take this same exact concept same exact problem but here instead of looking at the values

32
00:02:12.900 --> 00:02:21.390
of each state that he can end up in we're going to look at the values or the value of each action.

33
00:02:21.390 --> 00:02:25.470
So we're we're not going to use the letter V anymore because views for the value of the state we're

34
00:02:25.470 --> 00:02:30.660
going to use Q And the way you might have a question why the letter Q well.

35
00:02:30.660 --> 00:02:37.500
Q Some people speculate that Q Well I read this I think on Quora somebody mentioned that Q is because

36
00:02:37.500 --> 00:02:42.600
of quality but at the same time I couldn't find any other references to that so it might not be because

37
00:02:42.600 --> 00:02:47.460
of that might just because that's the letter that was used at the time and now it became super popular

38
00:02:47.460 --> 00:02:50.730
because it's all called Q learning because of that.

39
00:02:50.730 --> 00:02:57.810
So no exact reason why hold Q but nevertheless this helps us distinguish between V and Q so.

40
00:02:57.810 --> 00:03:03.870
Q here represents a rather than the value of the state it represents let's go off quality it represents

41
00:03:03.870 --> 00:03:06.210
the quality of the action reserve represents.

42
00:03:06.210 --> 00:03:08.250
Okay so I've got four actions.

43
00:03:08.250 --> 00:03:13.380
What are the different qualities of these actually what what is the or the value of the action or the

44
00:03:13.380 --> 00:03:15.690
quality of the action which action is more lucrative.

45
00:03:15.690 --> 00:03:20.940
So I need a metric telling me all right how do I quantify this action and then I can compare them and

46
00:03:20.940 --> 00:03:22.980
that is exactly what Q is.

47
00:03:23.250 --> 00:03:31.110
And so here he got four possible actions as always go up right left or down and based on the action

48
00:03:31.590 --> 00:03:35.970
there's gonna be a formula which tells us the quantifiable value of that action which we're calling

49
00:03:35.970 --> 00:03:38.480
the Q The Q value of that action.

50
00:03:38.580 --> 00:03:43.860
So let's have a look at how we're going to derive this formula for Q What how does it actually relate

51
00:03:43.860 --> 00:03:44.420
to these.

52
00:03:44.430 --> 00:03:51.240
Because as you can imagine because actions lead to states there has to be some sort of link between

53
00:03:51.240 --> 00:03:51.710
the two.

54
00:03:51.810 --> 00:03:56.010
Right we've got we've very determined how to color calculate this and we're pretty good at it.

55
00:03:56.010 --> 00:04:02.180
We know how to use the Belmont equation in very different environments with lots of different complications.

56
00:04:02.190 --> 00:04:06.010
Well let's leverage that knowledge to understand how we can now calculate.

57
00:04:06.010 --> 00:04:11.500
Q In order to make the same predictions because as you imagine the investor environment doesn't change

58
00:04:11.520 --> 00:04:16.480
depending depending on what approach we use the environment is going to be the same regardless.

59
00:04:16.500 --> 00:04:22.080
So therefore this approach and this approach should always give the same result and therefore that's

60
00:04:22.410 --> 00:04:24.930
another reason why these two should be linked.

61
00:04:25.050 --> 00:04:26.240
So let's have a look.

62
00:04:26.250 --> 00:04:31.170
So here's our view approach where we're just going to look at the value of any given state this state

63
00:04:31.170 --> 00:04:32.340
or any other state.

64
00:04:32.340 --> 00:04:37.110
And here we're going to we're just using the letter as here because that's the current state.

65
00:04:37.110 --> 00:04:40.560
And so therefore the terminology will be the same in both equations.

66
00:04:40.590 --> 00:04:42.710
And here we're using q essay.

67
00:04:42.720 --> 00:04:45.480
Q Is the of the state s and the action.

68
00:04:45.480 --> 00:04:51.060
A because action is up but in which state do we perform that action we performed that action in the

69
00:04:51.060 --> 00:04:51.800
state.

70
00:04:51.820 --> 00:04:56.410
S OK so now we're going to ride out the Belmont equation for the first approach.

71
00:04:56.430 --> 00:05:04.700
As you can see here we've got the of s so the value of any given state s is the maximum of the reward

72
00:05:04.700 --> 00:05:06.770
that you get so maximum bet.

73
00:05:07.010 --> 00:05:11.800
Based on the actions we have three in this case actually have four actions so maximum out of all the

74
00:05:11.810 --> 00:05:15.250
possible actions of this park which we've already discussed many times.

75
00:05:15.250 --> 00:05:20.870
So this is our reward that we get from performing that action in that state.

76
00:05:20.870 --> 00:05:26.990
Plus a discount in fact multiplied by the expected value of the new state that we're going to be in

77
00:05:27.110 --> 00:05:32.120
an expected value because it is a stochastic process we don't know exactly for sure that we're going

78
00:05:32.120 --> 00:05:35.950
to end up over here we might end up on the left or the right with a certain probability.

79
00:05:35.990 --> 00:05:37.890
That's why these probabilities are in here.

80
00:05:38.210 --> 00:05:38.450
All right.

81
00:05:38.450 --> 00:05:40.140
So that's our value.

82
00:05:40.300 --> 00:05:43.480
And now let's look at Q So Q is going to be defined.

83
00:05:43.520 --> 00:05:49.420
We're going to use this to define Q So let's say the agent from this location from this state perform

84
00:05:49.420 --> 00:05:50.780
the action up.

85
00:05:50.780 --> 00:05:54.140
What is the Q value going to be equal to.

86
00:05:54.470 --> 00:06:00.040
Well first of all let's see what will get in return for performing this action up first thing that you'll

87
00:06:00.050 --> 00:06:01.960
get is a reward right.

88
00:06:02.000 --> 00:06:04.050
That knows no doubt about it.

89
00:06:04.190 --> 00:06:06.260
There's going to be some sort of reward might be zero.

90
00:06:06.290 --> 00:06:12.560
But we know that the whole this the way this reinforcement learning process works is that sometimes

91
00:06:12.560 --> 00:06:15.830
for performing certain actions from a given state there's a reward.

92
00:06:15.830 --> 00:06:19.790
So we're gonna add that in here and then we're going to add what we're going to add.

93
00:06:19.790 --> 00:06:21.050
Well let's think about it.

94
00:06:21.050 --> 00:06:24.430
What is the next thing that happens after he's gone.

95
00:06:24.800 --> 00:06:29.990
Well next thing that happens is that now the agent is in a certain state.

96
00:06:29.990 --> 00:06:35.930
He could end up here with a 80 percent probability or some probability but actually you can up here

97
00:06:35.960 --> 00:06:39.730
or here but wherever he ends up now there's.

98
00:06:39.740 --> 00:06:47.040
We already have a quantified metric for that state he's in and that is actually the value of that state.

99
00:06:47.090 --> 00:06:52.280
But because he can up in many different states in three of the possible different states we have to

100
00:06:52.310 --> 00:06:56.090
look at the expected value of the state that he'll be in.

101
00:06:56.150 --> 00:07:00.950
And so we're going to add that in we're going to add of course discount and factor as we previously

102
00:07:00.950 --> 00:07:03.960
had because that is somewhere in the future.

103
00:07:04.130 --> 00:07:11.150
And then we're going to add the sum of across all possible states across all possible states that he

104
00:07:11.150 --> 00:07:13.730
could end up by taking this action times a probability.

105
00:07:14.180 --> 00:07:16.520
So what we're saying here is that OK.

106
00:07:16.550 --> 00:07:22.160
So by performing an action you going to get a reward plus which is a quantified metric plus you're going

107
00:07:22.160 --> 00:07:26.690
to get you end up in a state we don't know which one it could be here it could be here it could be here

108
00:07:26.960 --> 00:07:32.020
but here is the expected value of the state that you're going to end up in.

109
00:07:32.210 --> 00:07:36.320
And now we're going to multiply by discounting factor because that is one move away.

110
00:07:36.320 --> 00:07:44.120
So that is our cue value for this for performance action and what you will notice here right away is

111
00:07:44.120 --> 00:07:51.590
that Q The Q value is actually exactly identical to what's inside these brackets over here.

112
00:07:51.920 --> 00:07:52.610
And why is that.

113
00:07:52.610 --> 00:07:59.870
Well if you think about it here we're taking the maximum of the result we will get the maximum across

114
00:07:59.900 --> 00:08:00.920
all possible actions.

115
00:08:00.920 --> 00:08:04.970
So we've got four actions that we're taking the maximum across all possible actions of the result that

116
00:08:04.970 --> 00:08:08.120
we'll get by taking each of those actions.

117
00:08:08.300 --> 00:08:13.940
And in Q We're defining interesting what will we get by taking a certain action.

118
00:08:13.940 --> 00:08:19.260
So if you think about it it makes sense that the value of a state.

119
00:08:19.310 --> 00:08:25.890
So for instance this state is the maximum of all of the possible Q values right.

120
00:08:25.940 --> 00:08:32.480
So here in the States by being in the state the agent has one key value to Cuba value 3Q value for Q

121
00:08:32.480 --> 00:08:32.800
value.

122
00:08:32.810 --> 00:08:37.610
So yes positive for possible Q values while the value of the state it makes sense that the value of

123
00:08:37.610 --> 00:08:42.360
the state is the maximum of all of those four Q values.

124
00:08:42.370 --> 00:08:44.360
That is exactly what we can see here.

125
00:08:44.360 --> 00:08:48.020
That's a good confirmation of this new formula that we derived.

126
00:08:48.040 --> 00:08:53.870
If that wasn't the case if that if that did match up then we would have questions would be like So why

127
00:08:53.900 --> 00:08:55.000
why doesn't it matter.

128
00:08:55.100 --> 00:09:05.240
Why doesn't it match up if Q value is a quantified metric of performing an action and V depends on the

129
00:09:05.240 --> 00:09:12.440
for is like is the maximum of the possible results of the four actions that he can perform over that

130
00:09:12.440 --> 00:09:17.020
makes sense and that confirms the formula that we've just arrived.

131
00:09:17.330 --> 00:09:23.090
And now we're going to make it even more interesting we're going to get rid of the V entirely because

132
00:09:23.090 --> 00:09:28.670
you can see here you've got V is a recursive function of V so and then you've got V and then V and then

133
00:09:28.670 --> 00:09:29.720
V and then V and so on.

134
00:09:29.720 --> 00:09:36.620
So you can express this view through all of the following vs the most optimal vs that will come up here

135
00:09:36.710 --> 00:09:42.600
we are expressing Q As a font a recursive function of V or as a function of the next V.

136
00:09:42.830 --> 00:09:45.140
And then we'd have to plug in this V and then we'd get back to the V.

137
00:09:45.170 --> 00:09:51.200
So what are we going to do is we're actually going to take this V and we're going to we're going to

138
00:09:51.200 --> 00:09:57.560
replace it with a Q Right so let's have a look at that we're going to take this V of the next state

139
00:09:57.980 --> 00:10:01.520
and we're going to plug this into that formula over here.

140
00:10:01.520 --> 00:10:04.260
And as you can see it now.

141
00:10:04.310 --> 00:10:13.150
So this part doesn't change this probability doesn't change but as we just discussed V of S is the maximum.

142
00:10:13.370 --> 00:10:16.830
By all actions of Q of S and a right over here.

143
00:10:16.910 --> 00:10:19.130
So that's what we're going to replace in here.

144
00:10:19.130 --> 00:10:24.230
So we're going to say a maximum of of course is the new action the action that we're going to take because

145
00:10:24.230 --> 00:10:26.590
here we've got the view of s prime.

146
00:10:26.720 --> 00:10:30.660
So here now we've got the maximum across all a primes.

147
00:10:30.680 --> 00:10:34.460
So the actions that we're going to take from this state or from wherever whichever other state we end

148
00:10:34.460 --> 00:10:35.240
up in.

149
00:10:35.430 --> 00:10:41.120
But the action that we're going to take from there and maximum across all those and the maximum is of

150
00:10:41.210 --> 00:10:50.120
all of the Q values that will that are available to us in that new state as prime comma a prime.

151
00:10:50.120 --> 00:10:51.230
And that's the action.

152
00:10:51.230 --> 00:10:51.890
So that's the.

153
00:10:52.130 --> 00:10:54.530
So there's gonna be another four Q values there.

154
00:10:54.530 --> 00:10:56.840
So now as you can see let's go through that again.

155
00:10:56.990 --> 00:11:02.780
So as from what we derive this word would we discussed just through logic and intuition so that we can

156
00:11:02.780 --> 00:11:09.140
see that VNS are actually V of S and Q of us and a r linked the S is the maximum across all actions

157
00:11:09.140 --> 00:11:11.100
of Kyogle s and you can see it right here.

158
00:11:11.100 --> 00:11:16.460
So this this part is identical to this part and then we're going to leverage that and we're going to

159
00:11:16.460 --> 00:11:23.120
replace this bit with VNS from here but not this exact funnel we're going to take this internal part

160
00:11:23.120 --> 00:11:25.880
and we're going to replace it with Kilborn s and a.

161
00:11:26.030 --> 00:11:34.130
So we're gonna plug that in here and this part is going to be Q of s prime a prime so maximal Q by cross

162
00:11:34.130 --> 00:11:37.010
all a primes of q s prime a prime.

163
00:11:37.010 --> 00:11:39.680
And now we have our formula.

164
00:11:39.740 --> 00:11:43.380
So now we have a recursive formula for the Q value.

165
00:11:43.400 --> 00:11:48.740
So now the agent can think what's the value of this action what's the quality of this action was the

166
00:11:48.740 --> 00:11:50.390
Q value of this action.

167
00:11:50.390 --> 00:11:56.540
Well it depends on the reward I get in the immediate step after that plus it depends on the discounted

168
00:11:56.540 --> 00:12:02.170
factor times the maximum of all the possible Q actions in that state.

169
00:12:02.330 --> 00:12:06.710
But I don't know if I'm going to get there so I need to also look at that state and that state and that's

170
00:12:06.710 --> 00:12:12.830
why we have this expected value over here so we have some probability times the maximum that's our expected

171
00:12:12.830 --> 00:12:13.340
value.

172
00:12:13.400 --> 00:12:18.590
So very similar formula as you can see but this time we're expressing things through the Q values and

173
00:12:18.590 --> 00:12:27.440
that's why this whole algorithm is called Kill earning because this is what is looked at this is what

174
00:12:27.440 --> 00:12:31.970
the agents actually use they don't look at the states they look at their possible actions and then based

175
00:12:31.970 --> 00:12:35.720
on the actions on the Q values of actions they will decide which action to take.

176
00:12:35.720 --> 00:12:40.280
So they'll just look at the maximum Q value in this given state it has four actions.

177
00:12:40.280 --> 00:12:45.250
What is the best action to take so you can compare sort of comparing the different states it can end

178
00:12:45.250 --> 00:12:51.770
up end up in is going to compare the possible actions that it currently has then by finding the optimal

179
00:12:51.770 --> 00:12:56.930
one is going to take that action and then it is going to repeat that process repeat that process and

180
00:12:56.930 --> 00:12:57.440
so on.

181
00:12:57.500 --> 00:13:03.890
So now you can see how all of this comes together how the reward the discounting factor the stochastic

182
00:13:04.310 --> 00:13:10.280
market decision processes and the v values and the Q values all come together in order to give us this

183
00:13:10.640 --> 00:13:18.320
one super powerful Belmont equation for Q values which we can now apply and let our agents learn how

184
00:13:18.320 --> 00:13:20.360
to beat the environment.

185
00:13:20.360 --> 00:13:23.330
And so that is a intuitive explanation of what's going on.

186
00:13:23.330 --> 00:13:28.460
I know we went through the formulas but it is necessary because this is like our formula that we've

187
00:13:28.460 --> 00:13:36.740
been going through this whole chapter and I think it's a good transition from V to Q and it illustrates

188
00:13:36.740 --> 00:13:38.610
how they're linked between each other.

189
00:13:38.750 --> 00:13:46.940
And if you'd like to get a bit more of a rigorous approach mathematical approach and like you see the

190
00:13:46.940 --> 00:13:52.970
mathematics behind it and learn a bit more about Q values and how they work then we've got some additional

191
00:13:52.970 --> 00:14:00.590
reading for you this paper is called Mark of decision processes concepts and algorithms by Martine von

192
00:14:00.830 --> 00:14:02.900
Otter lo 2009.

193
00:14:02.900 --> 00:14:05.480
So you've got the link here as always.

194
00:14:05.480 --> 00:14:12.290
And here you can read in a bit more detail to understand all the nitty gritty behind Q values and so

195
00:14:12.290 --> 00:14:17.330
on and now that we've discussed all of these things relating to the Belmont equation now we are ready

196
00:14:17.330 --> 00:14:24.380
to look at something more complex such as this paper in order if if we want to get some additional information

197
00:14:24.380 --> 00:14:27.620
on this in order to kind of get a deeper understanding.

198
00:14:27.620 --> 00:14:34.040
But even if you don't read the newspaper already you should have a good working knowledge of what Q

199
00:14:34.040 --> 00:14:40.810
learning is all about and how agents come up with the actions that they need to take in a certain environment.

200
00:14:40.820 --> 00:14:43.960
So I hope you enjoyed today's tutorial and I look forward to seeing you next time.

201
00:14:43.970 --> 00:14:45.560
Until then enjoy a.
