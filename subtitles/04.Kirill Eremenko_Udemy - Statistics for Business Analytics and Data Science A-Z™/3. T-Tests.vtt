WEBVTT
1
00:00:00.580 --> 00:00:02.770
Hello and welcome back to the course on statistics.

2
00:00:02.790 --> 00:00:05.220
Today we're talking about the T-test.

3
00:00:05.310 --> 00:00:06.740
It's been a long time coming.

4
00:00:06.740 --> 00:00:11.850
And finally it's here and we are all prepared because we've talked about the t distribution and also

5
00:00:11.850 --> 00:00:17.280
we have all that knowledge about the standard hypothesis testing and we've been doing for so long.

6
00:00:17.400 --> 00:00:25.090
So we will definitely know what is going on when the t test when we're scientists so let's have a look.

7
00:00:25.110 --> 00:00:26.360
Here's our challenge.

8
00:00:26.400 --> 00:00:29.480
It is recommended to walk 10000 steps a day to be healthy.

9
00:00:29.520 --> 00:00:34.860
You are wondering if on average Americans are meeting this recommendation you surveyed 10 random people

10
00:00:34.860 --> 00:00:40.470
about how many steps each of them takes per day on average and got the following responses and thousand

11
00:00:40.470 --> 00:00:44.760
nine hundred eight thousand two hundred eleven dollars 350 and so on.

12
00:00:44.850 --> 00:00:51.900
It's evident that within your sample on average respondents are taking a less than 10000 steps but can

13
00:00:51.900 --> 00:00:56.060
you infer the same results for the whole population of America.

14
00:00:56.400 --> 00:00:58.820
Very interesting question very interesting challenge.

15
00:00:58.920 --> 00:01:05.700
And yes so basically one thing I wanted to point out here is that you will see what will you see this

16
00:01:05.700 --> 00:01:13.770
on average and on average and that where they're coming from is that it is Riggleman to walk 10000 steps

17
00:01:13.890 --> 00:01:15.160
a day to be healthy.

18
00:01:15.300 --> 00:01:17.690
That's assumed like implied on average.

19
00:01:17.700 --> 00:01:22.490
Obviously sometimes will be days when you are glass sometimes walk these days when you will walk more.

20
00:01:22.500 --> 00:01:26.040
So 10000 means on average per day.

21
00:01:26.220 --> 00:01:31.680
And that's why this average is popping up that turn around to people how many steps that take per day

22
00:01:31.680 --> 00:01:36.840
on average meaning like in a week how many days how many steps do they take on average per day or in

23
00:01:36.900 --> 00:01:38.230
one month or in a year.

24
00:01:38.430 --> 00:01:39.360
And same thing here.

25
00:01:39.360 --> 00:01:45.670
So this average what I'm trying to say here is that this average is has nothing to do with our means

26
00:01:45.670 --> 00:01:49.010
our standard deviation and so on is just the way the parameter is constructed.

27
00:01:49.050 --> 00:01:54.960
So if it could have been we could have just had a parameter where you have to it's recommended to walk

28
00:01:55.590 --> 00:01:57.350
this many steps per year.

29
00:01:57.600 --> 00:02:02.160
So what would it be three hundred three million 650000 steps.

30
00:02:02.420 --> 00:02:05.300
Me On million 650000 steps per year.

31
00:02:05.520 --> 00:02:11.280
But we don't have that we have per day so that's why this extra average is popping up but don't let

32
00:02:11.280 --> 00:02:14.010
it confuse you or get you off the track.

33
00:02:14.010 --> 00:02:18.960
We know what we're supposed to do in terms of the T-test and we're going to stick to that and the parameter

34
00:02:18.960 --> 00:02:21.500
can be whatever it needs to be.

35
00:02:21.510 --> 00:02:28.920
So let's just copy the important formation here and let's state all hypotheses so our null hypothesis

36
00:02:28.980 --> 00:02:32.480
is that the number of steps is at least met.

37
00:02:32.700 --> 00:02:38.640
So we want to prove that we can see that it's from here we can just see that there's only two people

38
00:02:39.070 --> 00:02:42.870
with over 10000 means the mean is going to be less than 10000.

39
00:02:43.200 --> 00:02:46.760
So we want to prove that it's less than 10000 for the whole population.

40
00:02:46.770 --> 00:02:52.860
And as we discussed before we need to assume a normal hypothesis which we're going to try to get to

41
00:02:52.860 --> 00:02:56.320
a contradiction and then therefore will be able to reject it.

42
00:02:56.370 --> 00:03:01.140
And therefore the only alternative will be possible so what you want to prove should always be your

43
00:03:01.140 --> 00:03:02.570
alternative hypothesis.

44
00:03:02.580 --> 00:03:07.070
And here we want to prove that the number of steps is not met meaning that it's less than 8000 just

45
00:03:07.070 --> 00:03:12.930
like as we see here and therefore our real hypothesis is that it is at least met so people in America

46
00:03:12.930 --> 00:03:15.570
on average say 10000 or more sets per day.

47
00:03:15.570 --> 00:03:22.440
And our alternative is Will is and hopefully we'll be able to reject the null and keep the alternative

48
00:03:22.800 --> 00:03:25.140
is that they take less than 10000 steps per day.

49
00:03:25.590 --> 00:03:30.390
And what's interesting here first of all I hope that you know now you're more comfortable with this

50
00:03:30.390 --> 00:03:35.790
logic after we've discussed what it means that you cannot reject the null hypothesis previously and

51
00:03:35.820 --> 00:03:40.740
about the contradictions and about what happens if the neural and when we reject and all we keep the

52
00:03:40.740 --> 00:03:42.500
all new possible turnover and so on.

53
00:03:42.510 --> 00:03:47.900
So hopefully you're getting more comfort with that logic and remember to practice that intuition behind

54
00:03:47.900 --> 00:03:49.190
what everything we're doing.

55
00:03:49.530 --> 00:03:54.480
But the other thing I wanted to point out is that it's pretty cool it's a pretty cool notion that you

56
00:03:54.480 --> 00:04:01.740
can just take 10 people survey them and find out something about the whole population of America and

57
00:04:01.740 --> 00:04:04.860
you don't have to or whatever country you live in.

58
00:04:04.860 --> 00:04:10.920
You just you basically you don't have to worry more than that as you'll see you will be able to will

59
00:04:10.920 --> 00:04:19.470
be able to derive some insights from or any 10 and it might be a bit over a like a hard to believe concept

60
00:04:20.190 --> 00:04:27.180
especially if you think of it in the way that oh well I'll just go out and survey my parents my siblings

61
00:04:27.450 --> 00:04:33.150
my children my friends and I'll definitely get 10 people and there's no way that that is going to be

62
00:04:33.150 --> 00:04:34.670
representative of the whole America.

63
00:04:34.800 --> 00:04:35.790
Well that is true.

64
00:04:35.790 --> 00:04:36.740
That is correct.

65
00:04:36.750 --> 00:04:42.060
But the thing there is that that sample isn't random that samples your family and friends and we're

66
00:04:42.060 --> 00:04:48.210
talking about a random sample and even more so if you just go outside on the street and you survey 10

67
00:04:48.210 --> 00:04:52.710
people that samples and random either because those are people who live in your neighborhood those are

68
00:04:52.710 --> 00:04:56.820
people who live in the street and maybe you'll be able to infer something about the people who live

69
00:04:56.820 --> 00:04:57.580
in your street.

70
00:04:57.750 --> 00:05:01.450
But you definitely won't be able to infer something about population of America because that's just

71
00:05:01.450 --> 00:05:02.460
not a random sample.

72
00:05:02.550 --> 00:05:07.310
And remember the assumption is one of the assumptions is that it's a random sample.

73
00:05:07.310 --> 00:05:13.400
It's very important to make sure the sample selected at random and it's much harder to select a random

74
00:05:13.400 --> 00:05:15.420
sample from the whole of America.

75
00:05:15.740 --> 00:05:20.480
Even if you were able to do it then you'd have to like fly to their location or call them up or you

76
00:05:20.480 --> 00:05:26.250
know how would you even select the random sample you need to have all the information all these people.

77
00:05:26.420 --> 00:05:33.320
But as long as you can select a random sample somehow then like somehow magically then this is a pretty

78
00:05:33.320 --> 00:05:39.270
cool notion that turns you then can be in even ten can be enough because we have the t test.

79
00:05:39.320 --> 00:05:42.860
So that's that's a quick comment on the sample there.

80
00:05:42.860 --> 00:05:48.070
And another thing that I can see here is that this distribution has wider tails this time.

81
00:05:48.080 --> 00:05:51.650
So usually our tails go down to the bottom very pretty quickly.

82
00:05:51.650 --> 00:05:57.710
Now they're pretty wide and that's because what we're told before this is a distribution is not a standard

83
00:05:57.730 --> 00:05:59.660
or is this not a normal distribution.

84
00:05:59.810 --> 00:06:02.670
Is a t distribution for the test to stick.

85
00:06:02.840 --> 00:06:08.720
And the key point here as well is that the TCT statistics so as you can see when we draw these We're

86
00:06:08.720 --> 00:06:14.930
not putting the new X bar or the Sigma X bar or P bar to explore in this case.

87
00:06:15.140 --> 00:06:18.890
We're not putting all of those action actually putting a 0 here because we're going to be looking at

88
00:06:19.190 --> 00:06:23.360
the distribution of the t statistic remember like we were told it will calculate Nozette score and we

89
00:06:23.360 --> 00:06:29.840
were saying that the Zed Zed parameter or that statistic has its own distribution which is centered

90
00:06:29.840 --> 00:06:32.340
around 0 and has a standard deviation of one.

91
00:06:32.450 --> 00:06:38.080
Well a t t statistic has its own distribution as well and that's exactly what's going on with it.

92
00:06:38.210 --> 00:06:45.290
And because we are constructing a t distribution which is going to do the inferences for the t distribution

93
00:06:45.290 --> 00:06:52.940
here or from the t distribution rather than from the original sampling distribution and you'll see exactly

94
00:06:52.940 --> 00:06:59.390
why we'll when we talk about the tea table so for knowledge of say this is the t distribution is derived

95
00:06:59.390 --> 00:06:59.900
in the same way.

96
00:06:59.900 --> 00:07:01.790
So it comes from the sampling distribution.

97
00:07:01.790 --> 00:07:04.660
So the central limit theorem still holds holds.

98
00:07:04.670 --> 00:07:10.120
It's just going to be easier for us deal if the t distribution rather than plotting sampling to the

99
00:07:10.130 --> 00:07:11.420
original sampling distribution.

100
00:07:11.420 --> 00:07:15.200
So this is the drive to distribution which we're going to drive just now.

101
00:07:15.200 --> 00:07:15.490
All right.

102
00:07:15.500 --> 00:07:19.260
So let's do this as we discussed some time ago.

103
00:07:19.260 --> 00:07:26.000
Now the best way for us to reject the null hypothesis like the strongest assumption we can make is that

104
00:07:26.300 --> 00:07:29.060
from here that new is actually equal to 10000.

105
00:07:29.180 --> 00:07:31.710
We want to prove that it's less than 10000 based on this data.

106
00:07:31.760 --> 00:07:36.500
If we were just like mue for instance it could be anywhere we could target 15000 but it is going to

107
00:07:36.500 --> 00:07:41.500
be a weaker proof than if we selected a 10000 we want are the strongest proof ever.

108
00:07:41.630 --> 00:07:47.240
And that's why we're going to select 10000 then we're going to say we're going to calculate now.

109
00:07:47.240 --> 00:07:50.990
Now the thing here is we don't know anything about the population.

110
00:07:50.990 --> 00:07:57.140
Usually we at least know the standard deviation or some some parameters of the population here we don't

111
00:07:57.140 --> 00:07:57.880
know anything.

112
00:07:57.950 --> 00:08:00.030
We don't even have any data on that.

113
00:08:00.110 --> 00:08:06.130
What we do know is our dataset here so we can calculate the standard deviation in our dataset.

114
00:08:06.140 --> 00:08:09.730
And as you remember four samples we don't use sigmoid.

115
00:08:09.830 --> 00:08:16.010
So there we go so that's s That's our sample standard deviation and BMI that's not it has nothing to

116
00:08:16.010 --> 00:08:21.620
do with the sampling distribution because for the sampling distribution we have Sigma X bar Sigma bar.

117
00:08:21.660 --> 00:08:26.090
This is s this is just for this one sample we have one sample.

118
00:08:26.190 --> 00:08:28.610
You can write or actually do this in Excel.

119
00:08:28.840 --> 00:08:31.080
So it might contain errors over there.

120
00:08:31.340 --> 00:08:33.960
So you can see that interview is for the population.

121
00:08:34.040 --> 00:08:37.940
That's that's our assumption that's the only thing we we don't actually know it about a bit and then

122
00:08:37.950 --> 00:08:43.430
we're assuming no other relation then S is some variation of our sample.

123
00:08:43.580 --> 00:08:47.900
So you can just take these put these in Excel or into a calculator and based on the formula we discussed

124
00:08:48.680 --> 00:08:56.960
at the start of the course you can calculate the sample standard deviation and then X bar is 8 5 and

125
00:08:56.960 --> 00:09:03.680
I find that's the mean of your move up from your sample.

126
00:09:03.680 --> 00:09:07.850
Basically that's the mean on your sample and as you can see we're not using again we're not using myu

127
00:09:07.850 --> 00:09:08.360
here.

128
00:09:08.390 --> 00:09:12.710
We're not using the X bar anything is not the sampling distribution I mean this is just the mean of

129
00:09:12.710 --> 00:09:14.300
your one sample that you picked out.

130
00:09:14.300 --> 00:09:17.000
Normally we don't calculate these thing that's important to remember.

131
00:09:17.150 --> 00:09:20.310
Normally we don't do calculations about the sample or anything.

132
00:09:20.450 --> 00:09:23.800
We just straight ahead straightaway proceed to the sampling distribution.

133
00:09:23.990 --> 00:09:28.550
But in this case because we don't know that much about the population we have to make inferences about

134
00:09:28.550 --> 00:09:33.620
the population in fact which is going to be using that statistic is going to do all of that for us.

135
00:09:33.620 --> 00:09:38.080
We don't have to make inferences about the population from the sample and then do all those samples.

136
00:09:38.080 --> 00:09:43.430
We're just going to use a t statistic and is going to do all of the he's going to have all of that groundwork

137
00:09:43.430 --> 00:09:45.070
built into the former.

138
00:09:45.140 --> 00:09:47.300
Speaking of which here's a format.

139
00:09:47.330 --> 00:09:53.420
So that is just a formula very similar to that statistic you can see that it's it's and unadjusted version

140
00:09:53.420 --> 00:09:54.290
of that said statistic.

141
00:09:54.290 --> 00:10:02.130
So here we've got X bar VAM put it in MMU which is where same exact same that statistic though we are

142
00:10:02.130 --> 00:10:04.810
calculating for that from sampling distribution.

143
00:10:04.830 --> 00:10:08.810
So that will be myu X bar minus here with X bar minus me.

144
00:10:08.820 --> 00:10:14.000
So that's why we said we're going to be dealing with a T distribution rather than the sampling distribution

145
00:10:14.000 --> 00:10:18.790
with your hand because that's that's the way we're calculating.

146
00:10:18.870 --> 00:10:24.330
That's the way we're structuring the calculations the formulas that we're using when we are applying

147
00:10:24.330 --> 00:10:25.930
the distribution.

148
00:10:26.010 --> 00:10:32.910
There we go at the bottom we've got as divide by square root of number of degrees of freedom as is the

149
00:10:32.910 --> 00:10:34.770
sample standard deviation done.

150
00:10:34.830 --> 00:10:38.810
Put it in New is a number of degrees of freedom.

151
00:10:38.910 --> 00:10:42.540
It's always just the number of samples minus one.

152
00:10:42.540 --> 00:10:46.670
That's just that's just the way it's calculated and voila.

153
00:10:46.860 --> 00:10:49.970
Eight point five nine five minutes 10000 divided.

154
00:10:50.020 --> 00:10:53.210
So on and so on it gives us minus two point ninety five.

155
00:10:53.400 --> 00:10:55.890
So let's plot this t value.

156
00:10:55.920 --> 00:11:00.430
So here we know it's 0 so this these are the standard deviations.

157
00:11:00.450 --> 00:11:05.910
This is just like as in the normal hypothesis testing that the ones we've been doing it has been the

158
00:11:05.910 --> 00:11:08.460
same as if we were plotting is that statistic.

159
00:11:08.460 --> 00:11:11.430
That's that's what it would mean and we're pretty much doing that.

160
00:11:11.430 --> 00:11:17.820
We're just kind of like mixing in the sampling distribution chart and the statistic together because

161
00:11:17.820 --> 00:11:20.470
we kind of it was very intuitive here which is going to plot this.

162
00:11:20.500 --> 00:11:23.610
Now there we go T minus two point ninety five.

163
00:11:23.820 --> 00:11:30.870
So once enervation to stand creation's boom point ninety five standard missions over here and now we

164
00:11:30.870 --> 00:11:33.170
need to find out.

165
00:11:33.180 --> 00:11:39.760
So if we want to reject the null hypothesis we're fine 95 percent confidence.

166
00:11:39.780 --> 00:11:43.240
Now we just need to find our critical t value.

167
00:11:43.260 --> 00:11:46.110
So I'll just show you the tea table now and you'll see what I mean.

168
00:11:46.120 --> 00:11:52.280
So here's a tea table as you can see it's actually smaller than the Zad table.

169
00:11:52.410 --> 00:11:53.150
The score.

170
00:11:53.280 --> 00:12:00.750
But at the same time that is only because they shrunk it to make it legible that if if you were to have

171
00:12:00.780 --> 00:12:06.090
all of the possible values here it would explode would be huge because of the extra variable that we

172
00:12:06.090 --> 00:12:07.590
have the degrees of freedom.

173
00:12:07.590 --> 00:12:15.180
So now you don't have that luxury of actually just looking up a zed value and then finding out the probability.

174
00:12:15.180 --> 00:12:22.110
In fact this table works just like when we were talking about rejection regions.

175
00:12:22.110 --> 00:12:30.000
So remember when we said there's a second way to use the Zad table score when you can just look up a

176
00:12:30.030 --> 00:12:31.880
critical score from your table.

177
00:12:31.880 --> 00:12:37.080
Kind of like reverse engineer your table and plot that critical score because at its core and then compare

178
00:12:37.080 --> 00:12:41.260
your score to the griddles Criddle girls at school for that degree of certainty that you want.

179
00:12:41.280 --> 00:12:46.260
Well that's exactly how the t temples work they don't work in the in the normal way that we do what

180
00:12:46.260 --> 00:12:48.440
we originally discussed for dessert tables.

181
00:12:48.480 --> 00:12:54.840
You cannot based on a t value you cannot just pick out the probability for all for that value it works

182
00:12:54.840 --> 00:12:55.230
either way.

183
00:12:55.230 --> 00:13:03.770
So you say what level of confidence you want so in our case we won t 95 we want 95 percent confidence

184
00:13:03.810 --> 00:13:10.110
that our result is correct or you know that where we want to reject the null hypothesis with a 95 percent

185
00:13:10.170 --> 00:13:11.650
confidence if we can.

186
00:13:11.730 --> 00:13:16.370
And then what we do is we go down so we'll were doing the OneTel tests so 95.

187
00:13:16.360 --> 00:13:24.060
So that means the 5 there's 5 percent we need to find the area with a 5 percent chance of the observation

188
00:13:24.090 --> 00:13:25.810
or our sample falling in there.

189
00:13:26.040 --> 00:13:30.780
And so now basically we look for all degrees of freedom in our case we have how many do we have.

190
00:13:30.810 --> 00:13:34.070
It was nine nine degrees of freedom so we scroll down.

191
00:13:34.530 --> 00:13:40.980
So nine degrees of freedom there it is that's nine degrees of freedom at a 95 percent confidence interval

192
00:13:40.980 --> 00:13:42.390
at nine degrees of freedom.

193
00:13:42.420 --> 00:13:47.550
The tea Krugel is one point eighty eight three three and that's all we have to know.

194
00:13:47.580 --> 00:13:54.420
So we put this into our calculations to critical at 95 degrees confidence and nine degrees of freedom

195
00:13:54.810 --> 00:13:57.240
is minus one point A-330.

196
00:13:57.240 --> 00:14:03.300
So the thing about the tables there we need to know is the way they're set up is the tea table gives

197
00:14:03.300 --> 00:14:07.010
you so this is this is the critical value.

198
00:14:07.170 --> 00:14:12.100
So for 95 degree 95 percent confidence and nine degrees of freedom.

199
00:14:12.270 --> 00:14:14.970
This is the critical value that's on the right here.

200
00:14:14.970 --> 00:14:22.770
So for instance it would be somewhere here 1.8 3:03 and basically or a 1.6 one to 1.3 it's sitting over

201
00:14:22.770 --> 00:14:23.110
here.

202
00:14:23.130 --> 00:14:26.340
So that means that anything to the right of the critical value you reject.

203
00:14:26.340 --> 00:14:32.610
That's the way the tables are set up so whatever value see that means if you're doing their testing

204
00:14:32.610 --> 00:14:36.170
the right tail over here then you rejected if it's greater than the critical value.

205
00:14:36.390 --> 00:14:42.870
But because the tea table the tea distribution is symmetrical that means you can just metrically look

206
00:14:42.870 --> 00:14:47.940
at it over here meaning that our tea critical because we will be dealing with the left tail is minus

207
00:14:47.940 --> 00:14:49.000
one point eighty three.

208
00:14:49.080 --> 00:14:55.200
So it's actually somewhere over here that's pretty critical and therefore we reject anything to the

209
00:14:55.200 --> 00:15:01.710
left as you can see our value falls to the left so TI's less than the critical it falls into this rejection

210
00:15:01.710 --> 00:15:02.500
tale.

211
00:15:02.760 --> 00:15:09.720
And yeah that basically means that in our case we can reject the null hypothesis and therefore So we've

212
00:15:09.720 --> 00:15:11.050
come to a contradiction.

213
00:15:11.220 --> 00:15:19.170
Basically what this is saying is that it is so unlikely if if this is if this were true that on average

214
00:15:19.170 --> 00:15:25.350
in America people are taking more than 10000 steps or more per day then this is where the distribution

215
00:15:25.350 --> 00:15:26.170
would have.

216
00:15:26.310 --> 00:15:32.970
And it would be so unlikely for us to pick out this random sample this exact this random sample of these

217
00:15:32.970 --> 00:15:40.710
parameters that we've calculated it would be so unlikely that we can with a 95 percent chance a degree

218
00:15:40.710 --> 00:15:46.350
of confidence reject the null hypothesis and therefore sort of come to a contradiction that it's so

219
00:15:46.380 --> 00:15:52.590
unlikely we can reject the null hypothesis and therefore the only officer only possible alternative

220
00:15:52.620 --> 00:15:58.080
is the alternative hypothesis where they are taking less than 10000 steps.

221
00:15:58.110 --> 00:16:03.110
And what I wanted to point out here is that as you can see the tails are thicker here.

222
00:16:03.120 --> 00:16:08.780
Well there's just a just abysmal zation But what is this means is that tails I think are here and the

223
00:16:08.790 --> 00:16:15.090
distribution actually allows for values to go further away and we discuss the distribution of values

224
00:16:15.090 --> 00:16:20.280
to go further away and it's just the way that we've drawn it and also based on the parameters that we've

225
00:16:20.280 --> 00:16:26.820
calculated that in this case we're still we've still found a sample that doesn't conform to the distribution

226
00:16:26.820 --> 00:16:29.450
and therefore this distribution cannot be the true case.

227
00:16:29.520 --> 00:16:33.840
And the reason for that is that you can see just even on the sample that a lot of the people here are

228
00:16:33.840 --> 00:16:41.640
taking way less than 10000 steps 6000 6000 and so on that if this is a random sample it would be very

229
00:16:41.730 --> 00:16:47.730
unlikely given the distribution would be very unlikely to pick out this random sample even in a T-test

230
00:16:47.970 --> 00:16:55.680
not just a zed test but even in a t test which is even harder to prove which is which allows for more

231
00:16:55.980 --> 00:17:03.150
variation in your data for more values to be in the tails it still is not something that is plausible

232
00:17:03.360 --> 00:17:05.730
and we rejecting this null hypothesis.

233
00:17:05.730 --> 00:17:06.440
So there we go.

234
00:17:06.480 --> 00:17:08.050
I hope this clarifies up.

235
00:17:08.130 --> 00:17:14.850
What's a t test is and how how to apply it as you can see it's a very similar process we just calculate

236
00:17:14.850 --> 00:17:20.910
a t statistic instead of that statistic and then we compare it to the critical statistic for that level

237
00:17:20.910 --> 00:17:24.990
of conference we want and the number of number of degrees of freedom.

238
00:17:25.200 --> 00:17:28.670
And by the way if you look at the table.

239
00:17:28.740 --> 00:17:34.210
So if you look at this table you'll see that what happens when you increase the degrees of freedom.

240
00:17:34.350 --> 00:17:37.340
You can see that the values change at the start so they change rapidly.

241
00:17:37.350 --> 00:17:38.060
Quite a lot.

242
00:17:38.160 --> 00:17:41.850
But then they start to change slower and slower and slower.

243
00:17:41.850 --> 00:17:47.430
So when you get to 30 you can see that the values are barely changing at all and that's why after 30

244
00:17:47.540 --> 00:17:53.400
this table or this table specifically it skips and goes to 40 to 60 because just to show us that you

245
00:17:53.400 --> 00:17:57.630
know what is it converging to that's that's used that statistic over there you can see.

246
00:17:57.630 --> 00:18:03.620
So it's it's it's like pretty much becomes a statistic already after 30 1 6 9 7 1 6 4 5.

247
00:18:03.780 --> 00:18:09.810
No big change happening after that and that's what we mean that's after 30 degrees of freedom there's

248
00:18:09.810 --> 00:18:15.890
not much point in using a tea table that you can just use as a statistic approach as before.

249
00:18:16.140 --> 00:18:19.830
So there we go I hope you enjoyed today's tutorial and I look forward to seeing you next time.

250
00:18:19.830 --> 00:18:21.420
Until then have analyzing.
