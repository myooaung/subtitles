1
00:00:00,730 --> 00:00:07,060
Hello and welcome to this new tutorial and this new code section because now that we're done with the

2
00:00:07,060 --> 00:00:12,790
section on normalizing the states we're going to move on to a new code section an important one that's

3
00:00:12,790 --> 00:00:15,230
the section where we build the AI.

4
00:00:15,460 --> 00:00:23,530
So the first thing to keep in mind and that is very important is that our AI is a policy a policy which

5
00:00:23,530 --> 00:00:27,220
is a function taking as input states of the environment.

6
00:00:27,220 --> 00:00:33,650
We explained what they were and returning some actions the actions to play in order to work.

7
00:00:33,700 --> 00:00:39,970
So the AI is a policy and then the second very important thing to keep in mind and to understand is

8
00:00:39,970 --> 00:00:48,420
that the algorithm that we are doing here that we are implementing is based on an exploration on the

9
00:00:48,430 --> 00:00:50,420
space of pulses.

10
00:00:50,470 --> 00:00:56,530
In other words we're going to be exploring a lot of different policies and will converge to the one

11
00:00:56,800 --> 00:01:00,330
that will return the best actions in order to work.

12
00:01:00,340 --> 00:01:07,800
So since now we are about build an AI AI not training yet but build its foundations.

13
00:01:07,990 --> 00:01:14,800
Well you might guess that we will do it into a class again because we're not only going to build the

14
00:01:14,860 --> 00:01:22,090
architecture of the policy we will build the policy and we will give it its tools that will update it

15
00:01:22,160 --> 00:01:23,510
there in all the training.

16
00:01:23,680 --> 00:01:30,370
You know in the different steps of the training we will update the policy by applying some perturbations

17
00:01:30,400 --> 00:01:36,760
and updating the weight of the policy because its a perception in the direction that increases the rewards

18
00:01:37,180 --> 00:01:43,930
and thats why a class is perfect for that because a class can contain different methods which are actually

19
00:01:43,930 --> 00:01:44,930
different tools.

20
00:01:45,040 --> 00:01:50,860
And of course these measures will be to apply some perturbations on the weight of the perception of

21
00:01:50,860 --> 00:01:51,700
the policy.

22
00:01:52,060 --> 00:01:59,080
So we'll make a class that will call policy we will initialize all the variables of our future policy

23
00:01:59,090 --> 00:02:06,490
object which will be nothing else than some AI's represented by perception and we will integrate in

24
00:02:06,490 --> 00:02:12,220
this class several methods which will update the policy all along the training.

25
00:02:12,220 --> 00:02:13,430
All right so lets do this.

26
00:02:13,450 --> 00:02:15,740
Lets start by defining the class.

27
00:02:15,970 --> 00:02:24,610
So as usual we always class and then the name of this class will be either actually a I for I prefer

28
00:02:24,610 --> 00:02:31,810
to call it policy just so that I can remember that the AI is a policy function of the input states and

29
00:02:31,810 --> 00:02:33,350
returning the actions to play.

30
00:02:33,670 --> 00:02:35,620
No argument here we go.

31
00:02:35,800 --> 00:02:45,090
We are ready to define this class as a job we need to start with the in its method which will take as

32
00:02:45,150 --> 00:02:45,950
arguments.

33
00:02:46,050 --> 00:02:53,870
Well first of course self our future policy objects or future AI's and also input size.

34
00:02:54,090 --> 00:03:00,570
So thats just the size of the inputs you know basically the number of elements in the vector of input

35
00:03:00,570 --> 00:03:09,160
states and output size which is of course the number of actions to play I remind this is another important

36
00:03:09,160 --> 00:03:09,480
point.

37
00:03:09,490 --> 00:03:16,540
But I remind that the AI is not only returning one action it is returning several actions because indeed

38
00:03:16,540 --> 00:03:22,740
in order to walk you need to do several movements in your body and only one movement.

39
00:03:22,780 --> 00:03:24,420
So what are these actions.

40
00:03:24,480 --> 00:03:32,110
Are basically the angles between the axes of the virtual robot and also the muscles impulsion.

41
00:03:32,380 --> 00:03:38,770
Right which impulsion should you make on a certain muscle of a certain member of your body in order

42
00:03:38,770 --> 00:03:39,360
to walk.

43
00:03:39,580 --> 00:03:41,050
So here we go.

44
00:03:41,060 --> 00:03:44,250
That's our arguments input size output size.

45
00:03:44,290 --> 00:03:52,480
Now let's define the variables property the objects are future policy objects are future AI's and these

46
00:03:52,480 --> 00:03:55,750
variables are actually only one variable.

47
00:03:55,750 --> 00:03:57,210
Can you guess what it is.

48
00:03:57,430 --> 00:04:03,190
If you understand that what we're building here is the policy and the policy is nothing else than a

49
00:04:03,190 --> 00:04:10,570
perception and a perception is nothing else than a one layer neural network composed of several neurons

50
00:04:10,690 --> 00:04:12,660
therefore composed of several weights.

51
00:04:12,940 --> 00:04:20,710
Well the only variable that would really be essential to initialize here and to introduce is of course

52
00:04:20,980 --> 00:04:25,360
the matrix of weight which we're going to call theta.

53
00:04:25,450 --> 00:04:30,990
So the variable that I'm going to introduce here and the only one of our function.

54
00:04:30,980 --> 00:04:36,590
Basically the only one proper to the future object is self theta.

55
00:04:36,750 --> 00:04:44,560
The metrics of weights of the neurons of the perception of the policy are to solve that data and as

56
00:04:44,680 --> 00:04:53,710
how we usually do in the init method we are going to initialize this matrix of weights theta to zeros

57
00:04:54,280 --> 00:04:54,990
only zeros.

58
00:04:55,000 --> 00:05:01,960
But remember this zeros function by non-Thai needs to take one essential argument that you can see here

59
00:05:01,960 --> 00:05:06,370
the shape and that is a couples that's why I'm adding new parenthesis here.

60
00:05:06,510 --> 00:05:11,320
A couple containing the dimensions of this matrix of zeros you want to build.

61
00:05:11,530 --> 00:05:13,340
So here zeros.

62
00:05:13,360 --> 00:05:17,050
We just entered one argument that means it's going to be a vector.

63
00:05:17,290 --> 00:05:23,410
But here since we want to initialize matrix all we have to do with these parenthesis here to specify

64
00:05:23,410 --> 00:05:25,100
the dimensions of the matrix.

65
00:05:25,450 --> 00:05:26,670
And these are going to be.

66
00:05:26,830 --> 00:05:30,170
Be careful it's not going to be input size and output size.

67
00:05:30,280 --> 00:05:37,420
It's actually going to be I'll put size for the number of lines so there are going to be output size

68
00:05:37,540 --> 00:05:46,880
lines therefore same number of lines as a number of actions to play and in what size columns.

69
00:05:46,870 --> 00:05:52,870
So there is going to be as many columns as the number of elements in the vector input states.

70
00:05:52,900 --> 00:05:56,160
And why do we have to take it in this order.

71
00:05:56,350 --> 00:06:01,160
That's because we're simply following what they do in the article in the paper.

72
00:06:01,300 --> 00:06:04,130
They do a multiplication by the left side.

73
00:06:04,210 --> 00:06:09,770
You know they do a matrix multiplication by the left side and therefore it must be the output size that

74
00:06:09,790 --> 00:06:10,840
goes first.

75
00:06:10,840 --> 00:06:15,520
You can check that on the paper but we could have totally done in precise first and output size but

76
00:06:15,520 --> 00:06:18,370
then it would have been a multiplication by the right.

77
00:06:18,370 --> 00:06:21,090
Basically it's just a conventional order.

78
00:06:21,400 --> 00:06:23,230
All right and that's it.

79
00:06:23,380 --> 00:06:27,910
That's only what we need to initialize here for our policy.

80
00:06:27,910 --> 00:06:31,950
So it's quite simple for now but it's going to get slightly more challenging now.

81
00:06:31,990 --> 00:06:38,230
And that's why we're going to take a little break and move on to the next tutorial to make this first

82
00:06:38,230 --> 00:06:44,650
function this essential function for the are as you know it's at the heart of the IRS and that is going

83
00:06:44,650 --> 00:06:49,570
to be the function that applies to perturbations on the weights.

84
00:06:49,570 --> 00:06:54,580
You know we will take several directions for each direction will apply a perturbation on the weights

85
00:06:55,000 --> 00:07:01,870
and we will measure the rewards that we get with this specific perturbation in this specific direction.

86
00:07:02,170 --> 00:07:08,980
And if you want to see peril with the article Well basically in the next tutorial we will be implementing

87
00:07:08,980 --> 00:07:10,510
exactly this.

88
00:07:10,510 --> 00:07:17,230
You know the papers suggest two ways of doing things the one which is applying the perturbations in

89
00:07:17,380 --> 00:07:20,480
one direction here and the opposite direction year.

90
00:07:20,590 --> 00:07:22,750
So we will call this by the way the positive direction.

91
00:07:22,780 --> 00:07:27,250
And this one the negative direction which is to say that these are opposite directions.

92
00:07:27,250 --> 00:07:34,840
So V-1 is applying these perturbations without normalizing the state and V-2 is saying applying those

93
00:07:34,930 --> 00:07:40,470
perturbation positive direction negative direction but with normalization applied.

94
00:07:40,630 --> 00:07:42,820
And as you can see this is exactly what we did.

95
00:07:42,880 --> 00:07:49,490
We subtract by the mean here and we divide by the standard deviation the root of the variance.

96
00:07:49,570 --> 00:07:55,230
And of course what we're going to do is V-2 because we already normalized the states.

97
00:07:55,240 --> 00:08:01,890
OK so we're going to apply those positive perturbations here and those negative perturbations here.

98
00:08:01,900 --> 00:08:08,760
But keep in mind that positive negative just means that we're playing the opposite direction for a specific

99
00:08:09,060 --> 00:08:10,120
perturbation here.

100
00:08:10,350 --> 00:08:11,000
OK.

101
00:08:11,160 --> 00:08:12,660
So we'll do that in the next tutorial.

102
00:08:12,680 --> 00:08:17,160
Actually since you already have the formula the exact formula in the paper.

103
00:08:17,250 --> 00:08:23,190
Maybe you could try to make this future function before the next to Tokyo and that could be a nice exercise

104
00:08:23,190 --> 00:08:25,690
and you can get the solution in the next it's also.

105
00:08:25,800 --> 00:08:31,730
Basically you only have to apply this in a new function that we're going to call evaluate.

106
00:08:31,740 --> 00:08:35,630
All right so good luck and I'll see you in the next it's oil to make that function.

107
00:08:35,730 --> 00:08:36,980
Until then enjoy.

