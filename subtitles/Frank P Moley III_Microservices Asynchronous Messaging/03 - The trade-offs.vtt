WEBVTT
1
00:00:00.004 --> 00:00:02.006
- [Instructor] As with every technical decision,

2
00:00:02.006 --> 00:00:05.000
there are trade-offs when making decisions

3
00:00:05.000 --> 00:00:07.001
on how to develop software.

4
00:00:07.001 --> 00:00:11.007
Asynchronous communications is no exception to this rule.

5
00:00:11.007 --> 00:00:13.006
One of the single biggest trade-offs

6
00:00:13.006 --> 00:00:16.002
with a microservices implementation

7
00:00:16.002 --> 00:00:18.005
that includes asynchronous messaging

8
00:00:18.005 --> 00:00:21.008
is the overall complexity of the system.

9
00:00:21.008 --> 00:00:25.002
Some of the complexity comes from artifact sprawl.

10
00:00:25.002 --> 00:00:28.004
Usually consumers of messages are individual artifacts

11
00:00:28.004 --> 00:00:31.005
that have their own repositories, build pipelines,

12
00:00:31.005 --> 00:00:35.003
deployment pipelines, and configuration management.

13
00:00:35.003 --> 00:00:39.005
You now have to account for all of this in your system.

14
00:00:39.005 --> 00:00:44.002
Another complexity issue comes from disconnected code paths.

15
00:00:44.002 --> 00:00:46.009
This adds complexity to many areas of your system,

16
00:00:46.009 --> 00:00:49.002
as we will discuss, but for now,

17
00:00:49.002 --> 00:00:52.003
considering the debugging pathway alone,

18
00:00:52.003 --> 00:00:55.008
having disconnected paths makes debugging,

19
00:00:55.008 --> 00:00:59.001
troubleshooting, and evaluating outcomes significantly

20
00:00:59.001 --> 00:01:04.001
more complex than in a standard synchronous system.

21
00:01:04.001 --> 00:01:07.001
Multiple paths through asynchronous messaging systems

22
00:01:07.001 --> 00:01:10.003
can also increase complexity.

23
00:01:10.003 --> 00:01:13.003
Not all communications, as we will see,

24
00:01:13.003 --> 00:01:15.006
follow a point-to-point model.

25
00:01:15.006 --> 00:01:18.007
Once you add in fanouts or conditional jobs,

26
00:01:18.007 --> 00:01:22.007
your system increases in complexity for all aspects

27
00:01:22.007 --> 00:01:26.000
from development to operations.

28
00:01:26.000 --> 00:01:28.007
Observability is one area that the complexity

29
00:01:28.007 --> 00:01:33.000
dramatically increases because, as you might imagine,

30
00:01:33.000 --> 00:01:35.002
the disconnect in call stacks

31
00:01:35.002 --> 00:01:39.001
makes traditional observability that much harder.

32
00:01:39.001 --> 00:01:41.008
Lack of immediate response means

33
00:01:41.008 --> 00:01:45.002
that your inspection of issues has a disconnect.

34
00:01:45.002 --> 00:01:48.002
You lose the ability to log the error or success

35
00:01:48.002 --> 00:01:50.003
in the client immediately.

36
00:01:50.003 --> 00:01:54.002
As such, you have to inspect other logs

37
00:01:54.002 --> 00:01:58.001
to complete the view of the whole process.

38
00:01:58.001 --> 00:02:01.006
Obviously, logging is critical for observability

39
00:02:01.006 --> 00:02:03.005
of the health of a system.

40
00:02:03.005 --> 00:02:05.007
Often, in synchronous services,

41
00:02:05.007 --> 00:02:08.004
the logs and errors are unified.

42
00:02:08.004 --> 00:02:12.008
Well, in asynchronous styles, this isn't the case.

43
00:02:12.008 --> 00:02:16.005
When dealing with messaging, it is even worse.

44
00:02:16.005 --> 00:02:20.001
Often, you need to add things like correlation IDs

45
00:02:20.001 --> 00:02:23.007
to your log messages to help you aggregate the logs

46
00:02:23.007 --> 00:02:26.004
and reassemble calls.

47
00:02:26.004 --> 00:02:28.006
Much like log aggregation,

48
00:02:28.006 --> 00:02:32.001
correlation of metrics becomes a challenge.

49
00:02:32.001 --> 00:02:36.003
This is critical for chains of calls in asynchronous styles

50
00:02:36.003 --> 00:02:39.007
because of the isolation of components.

51
00:02:39.007 --> 00:02:42.008
There still may be a need to figure out the upstream

52
00:02:42.008 --> 00:02:46.005
and downstream causes of an abnormal load.

53
00:02:46.005 --> 00:02:49.006
As such, this metrics correlation can require

54
00:02:49.006 --> 00:02:54.001
a deeper evaluation of the code as a whole.

55
00:02:54.001 --> 00:02:56.007
There is more complexity at play in a system

56
00:02:56.007 --> 00:02:59.005
that is based on asynchronous communications

57
00:02:59.005 --> 00:03:01.004
than we are discussing here,

58
00:03:01.004 --> 00:03:04.002
but I do want to point out a few more issues

59
00:03:04.002 --> 00:03:06.009
that need to be taken into account.

60
00:03:06.009 --> 00:03:10.002
The sheer fact that components will increase,

61
00:03:10.002 --> 00:03:13.003
especially in an asynchronous messaging system,

62
00:03:13.003 --> 00:03:17.005
and that adds to complexity to several areas.

63
00:03:17.005 --> 00:03:19.000
We already pointed out a few,

64
00:03:19.000 --> 00:03:21.006
but you need to remember that each new component

65
00:03:21.006 --> 00:03:24.003
not only brings the overhead of that component

66
00:03:24.003 --> 00:03:27.003
across all of your build and deployment pipelines,

67
00:03:27.003 --> 00:03:29.004
but also in your resource utilization

68
00:03:29.004 --> 00:03:31.007
within the system as a whole.

69
00:03:31.007 --> 00:03:35.007
New artifacts, especially in containerized environments,

70
00:03:35.007 --> 00:03:39.001
have their own overhead with respect to resources.

71
00:03:39.001 --> 00:03:42.000
You potentially have more database connections,

72
00:03:42.000 --> 00:03:46.004
more RAM and CPU utilization, and other costs.

73
00:03:46.004 --> 00:03:50.001
If you work in a commercialized containerized environment,

74
00:03:50.001 --> 00:03:52.002
you will have more containers running,

75
00:03:52.002 --> 00:03:56.002
which also impacts your operational costs.

76
00:03:56.002 --> 00:03:59.000
I talked about observability being a trade-off

77
00:03:59.000 --> 00:04:01.001
with asynchronous systems.

78
00:04:01.001 --> 00:04:04.009
Well, that trade-off has impacts on operations.

79
00:04:04.009 --> 00:04:09.002
As you saw, some of the key indicators an operations team

80
00:04:09.002 --> 00:04:12.002
would use to evaluate the health of a system,

81
00:04:12.002 --> 00:04:13.008
like logging and metrics,

82
00:04:13.008 --> 00:04:17.003
are impacted by asynchronous messaging.

83
00:04:17.003 --> 00:04:20.005
As such, overall operational evaluations

84
00:04:20.005 --> 00:04:23.000
become a little bit more tricky.

85
00:04:23.000 --> 00:04:25.005
The asynchronous nature of your systems

86
00:04:25.005 --> 00:04:30.001
and the expected hops must be well-documented.

87
00:04:30.001 --> 00:04:34.001
Operations must be able to trace a task through the system

88
00:04:34.001 --> 00:04:36.002
without the need for debugging code

89
00:04:36.002 --> 00:04:39.001
of possibly hundreds of artifacts.

90
00:04:39.001 --> 00:04:42.004
In addition, you need to provide as much information

91
00:04:42.004 --> 00:04:45.008
via logs to allow for aggregation

92
00:04:45.008 --> 00:04:50.003
and quick inspection of potentially troubled workloads.

93
00:04:50.003 --> 00:04:53.000
This isn't a trivial exercise,

94
00:04:53.000 --> 00:04:55.006
but it is inherently critical to ensure

95
00:04:55.006 --> 00:04:58.009
the system can be run and maintained.

96
00:04:58.009 --> 00:05:01.003
You need to build and keep well-maintained

97
00:05:01.003 --> 00:05:06.001
operational runbooks for every system and component.

98
00:05:06.001 --> 00:05:10.002
Finally, I want to discuss an ethereal trade-off.

99
00:05:10.002 --> 00:05:12.008
From my experiences with these patterns,

100
00:05:12.008 --> 00:05:14.003
finding the source of an issue

101
00:05:14.003 --> 00:05:17.005
is significantly more complex.

102
00:05:17.005 --> 00:05:20.009
While an issue may still arise in a single component

103
00:05:20.009 --> 00:05:23.003
and the logs of that component can help,

104
00:05:23.003 --> 00:05:26.004
the root cause is not so easy to find.

105
00:05:26.004 --> 00:05:29.006
There are times where contracts may be validated

106
00:05:29.006 --> 00:05:32.009
or new use cases not originally intended

107
00:05:32.009 --> 00:05:36.002
cause a message, and subsequent consumer,

108
00:05:36.002 --> 00:05:39.007
to be fired in a not so good state.

109
00:05:39.007 --> 00:05:43.004
This new use case or modification of an existing one

110
00:05:43.004 --> 00:05:48.000
can make identifying the issue source significantly harder.

111
00:05:48.000 --> 00:05:50.004
In a choreographed event model, for instance,

112
00:05:50.004 --> 00:05:52.004
which we will talk about later,

113
00:05:52.004 --> 00:05:56.004
the downstream impact of an issue may be several hops

114
00:05:56.004 --> 00:05:59.003
down the message chain, therefore,

115
00:05:59.003 --> 00:06:03.002
tracing the issue through the call stacks is harder.

116
00:06:03.002 --> 00:06:07.003
Your development staff must learn solid debugging skills,

117
00:06:07.003 --> 00:06:10.003
which is becoming a challenge in some circles,

118
00:06:10.003 --> 00:06:13.000
so take this into consideration.

