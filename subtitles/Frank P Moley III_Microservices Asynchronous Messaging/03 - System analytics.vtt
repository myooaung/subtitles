WEBVTT
1
00:00:00.004 --> 00:00:02.005
- After log aggregation,

2
00:00:02.005 --> 00:00:05.005
one of the key uses, is analytical exercises

3
00:00:05.005 --> 00:00:08.001
on the stream data platform.

4
00:00:08.001 --> 00:00:11.007
This exercise is performed after the internal processing

5
00:00:11.007 --> 00:00:15.006
of the messages, and in the aggregations.

6
00:00:15.006 --> 00:00:17.009
In the real world of analytics tools and engines,

7
00:00:17.009 --> 00:00:20.008
you may be curious why the stream data platform

8
00:00:20.008 --> 00:00:24.003
is an optimal place to do system analytics.

9
00:00:24.003 --> 00:00:27.001
The reason is speed, efficiency

10
00:00:27.001 --> 00:00:31.002
and ultimately near real-time activity.

11
00:00:31.002 --> 00:00:33.008
When it comes to near real-time activity,

12
00:00:33.008 --> 00:00:36.001
short of in-process activity,

13
00:00:36.001 --> 00:00:38.005
the stream data platform offers

14
00:00:38.005 --> 00:00:41.000
the first place analytics can occur.

15
00:00:41.000 --> 00:00:45.005
And often is a bigger view of the data.

16
00:00:45.005 --> 00:00:48.007
In an in-process solution you would be restricted

17
00:00:48.007 --> 00:00:52.005
to the data available to the individual microservice.

18
00:00:52.005 --> 00:00:54.006
But in a stream data analytics solution

19
00:00:54.006 --> 00:00:56.008
you would have multiple microservices

20
00:00:56.008 --> 00:01:01.003
and systems to capture and analyze the data from.

21
00:01:01.003 --> 00:01:03.003
This near real-time activity means

22
00:01:03.003 --> 00:01:05.007
your reaction times are quicker as well.

23
00:01:05.007 --> 00:01:08.003
If you are building analytics that will trigger

24
00:01:08.003 --> 00:01:12.004
system events, for instance, this becomes critical.

25
00:01:12.004 --> 00:01:14.004
We're going to talk about eventing later because

26
00:01:14.004 --> 00:01:16.001
its one of the most exciting uses

27
00:01:16.001 --> 00:01:18.007
of the platform, in my opinion.

28
00:01:18.007 --> 00:01:21.001
Because the data is analyzed In-stream,

29
00:01:21.001 --> 00:01:24.000
using tools like Apache Spark that are designed to

30
00:01:24.000 --> 00:01:27.000
handle the size and scope of data possible.

31
00:01:27.000 --> 00:01:29.008
Your data is not only analyzed sooner,

32
00:01:29.008 --> 00:01:33.006
but also quicker. Consider if you pushed all of that data

33
00:01:33.006 --> 00:01:36.002
first to a relational data warehouse,

34
00:01:36.002 --> 00:01:39.009
and then had to create queries on that data.

35
00:01:39.009 --> 00:01:43.002
The data either has to be optimized for use cases, which

36
00:01:43.002 --> 00:01:47.001
restrict others, or stored in a way that makes indicesing

37
00:01:47.001 --> 00:01:51.003
efficient. As such, real-time analytics engines are much

38
00:01:51.003 --> 00:01:54.009
quicker at handling the disjointed data.

39
00:01:54.009 --> 00:01:57.007
All of this leads to quicker responses.

40
00:01:57.007 --> 00:02:00.006
Dashboards can be built quickly and efficiently,

41
00:02:00.006 --> 00:02:03.009
That show real time introspection of the system.

42
00:02:03.009 --> 00:02:07.000
You can get answers to questions as quick as you can write

43
00:02:07.000 --> 00:02:12.000
and execute the analytics job, making its agility king.

44
00:02:12.000 --> 00:02:13.008
If you have to write to a database

45
00:02:13.008 --> 00:02:16.008
de-normalize it or optimize a query,

46
00:02:16.008 --> 00:02:22.009
your time to delivery rapidly increases with each pivot.

47
00:02:22.009 --> 00:02:24.009
So now, to the how?

48
00:02:24.009 --> 00:02:28.002
the first thing you really need, is a question to answer.

49
00:02:28.002 --> 00:02:31.008
But once you have it, you write a job to look for answers.

50
00:02:31.008 --> 00:02:33.006
Since you have the data,

51
00:02:33.006 --> 00:02:37.000
you know where to look. Once the job is written,

52
00:02:37.000 --> 00:02:41.007
you execute it, as a consumer of the stream data.

53
00:02:41.007 --> 00:02:44.002
You then evaluate the results.

54
00:02:44.002 --> 00:02:46.005
Control groups can really help here

55
00:02:46.005 --> 00:02:49.007
to make sure that you are getting the most valid answers.

56
00:02:49.007 --> 00:02:52.007
Visualizations again, help the human

57
00:02:52.007 --> 00:02:55.002
comprehend The results.

58
00:02:55.002 --> 00:02:59.002
The exercise usually takes several iterations to determine

59
00:02:59.002 --> 00:03:01.007
if the analytics is correct.

60
00:03:01.007 --> 00:03:04.003
Finally, you are ready to ship it.

61
00:03:04.003 --> 00:03:06.003
If the analytics was worth it,

62
00:03:06.003 --> 00:03:08.009
there's value in collecting these results

63
00:03:08.009 --> 00:03:11.005
over a period of time.

64
00:03:11.005 --> 00:03:13.009
The analyzed and massage data is now ready to ship

65
00:03:13.009 --> 00:03:18.001
in a new format to a data lake for future use.

66
00:03:18.001 --> 00:03:22.003
The original source data is left intact, but in this model,

67
00:03:22.003 --> 00:03:27.007
we now represent the data in a new form in our lake.

68
00:03:27.007 --> 00:03:30.004
So the question about value is a valid one.

69
00:03:30.004 --> 00:03:33.000
There's a lot of work involved in building an analytic

70
00:03:33.000 --> 00:03:36.006
system on top of a stream data platform.

71
00:03:36.006 --> 00:03:40.001
One thing to note is that maturity matters to really achieve

72
00:03:40.001 --> 00:03:43.005
optimal results. You need to commit to it,

73
00:03:43.005 --> 00:03:45.003
and allow it to mature.

74
00:03:45.003 --> 00:03:47.009
At the same time, you'd be flexible

75
00:03:47.009 --> 00:03:51.000
and pivot as needed within the model.

76
00:03:51.000 --> 00:03:53.002
The insights, however hard This

77
00:03:53.002 --> 00:03:56.000
are powerful tools.

78
00:03:56.000 --> 00:03:58.006
As you will see, when we discuss eventing.

79
00:03:58.006 --> 00:04:00.006
You can really do yourself a lot of good

80
00:04:00.006 --> 00:04:03.000
by building an analytics platform,

81
00:04:03.000 --> 00:04:06.002
especially in a microservices architecture,

82
00:04:06.002 --> 00:04:08.008
where there are a lot of moving parts.

83
00:04:08.008 --> 00:04:11.008
As I mentioned, with maturity, you need dedication.

84
00:04:11.008 --> 00:04:15.003
Your organization must dedicate real efforts

85
00:04:15.003 --> 00:04:16.007
to making this work.

86
00:04:16.007 --> 00:04:18.002
If you do it halfway,

87
00:04:18.002 --> 00:04:21.009
your chances of succeeding are reduced dramatically.

88
00:04:21.009 --> 00:04:27.008
This dedication must be from top down, as well as bottom up.

89
00:04:27.008 --> 00:04:29.008
The team needs to buy into the value

90
00:04:29.008 --> 00:04:33.000
of such a platform and, turn it into a reality.

91
00:04:33.000 --> 00:04:36.001
Now you won't find significant value immediately,

92
00:04:36.001 --> 00:04:38.008
especially considering the investment.

93
00:04:38.008 --> 00:04:43.004
Once you allow it to mature and grow, you will see value,

94
00:04:43.004 --> 00:04:47.000
but the return on investment takes time.

