1
00:00:01,710 --> 00:00:04,054
When you're ready to scale the number of pods in your deployment,

2
00:00:04,054 --> 00:00:07,656
you'll use the kubectl scale deployment command.

3
00:00:07,656 --> 00:00:13,392
For example, kubectl scale deployment, deployment name,

4
00:00:13,392 --> 00:00:18,105
--replicas=, then the desired number of replicas; in this case 3.

5
00:00:18,105 --> 00:00:22,122
To scale the number of nodes you'll use the az aks scale command,

6
00:00:22,122 --> 00:00:24,616
passing in the name of the resource group,

7
00:00:24,616 --> 00:00:28,019
the name of the cluster, and the desired number of nodes,

8
00:00:28,019 --> 00:00:29,552
in this case, 5.

9
00:00:29,552 --> 00:00:33,208
Your node pool will then scale up or down to match

10
00:00:33,208 --> 00:00:35,749
the requested number of nodes.

11
00:00:35,749 --> 00:00:37,386
Right now there's only one pod.

12
00:00:37,386 --> 00:00:40,285
Let's scale that up to three pods.

13
00:00:40,285 --> 00:00:44,044
We can check the status with kubectl get pods and

14
00:00:44,044 --> 00:00:47,107
eventually see all three pods running.

15
00:00:47,107 --> 00:00:49,296
The original one that's been up for a few minutes and the new

16
00:00:49,296 --> 00:00:51,663
ones that have been up for a few seconds.

17
00:00:51,663 --> 00:00:53,547
If I hit the app in a loop,

18
00:00:53,547 --> 00:00:56,980
notice that two of the responses have started over at 1,

19
00:00:56,980 --> 00:01:01,916
this would be the new pods; then the existing pod picked it up at 6.

20
00:01:01,916 --> 00:01:03,033
Again, in our simple app,

21
00:01:03,033 --> 00:01:05,893
the counter is unique to each pod and starts over

22
00:01:05,893 --> 00:01:08,358
at 1 when the app is restarted.

23
00:01:08,358 --> 00:01:10,200
Let's take a look at our nodes.

24
00:01:10,200 --> 00:01:11,648
We have three nodes.

25
00:01:11,648 --> 00:01:14,727
Let's say I wanted to scale it down to one node.

26
00:01:14,727 --> 00:01:18,543
We'll run the az aks scale command, then when it's finished,

27
00:01:18,543 --> 00:01:20,735
we should see the succeeded message.

28
00:01:20,735 --> 00:01:25,820
If we run get nodes again, we can see that only one node is in the ready state.

29
00:01:25,820 --> 00:01:29,934
If we look at the pods, we can see that one has been running for a while,

30
00:01:29,934 --> 00:01:32,330
probably the one that was on this node to begin with,

31
00:01:32,330 --> 00:01:35,603
then two other pods have been started on this single

32
00:01:35,603 --> 00:01:38,590
node to maintain our replica set of 3.

33
00:01:38,590 --> 00:01:43,116
We can hit the app again to see that two of the pods are indeed new pods,

34
00:01:43,116 --> 00:01:46,097
while a third has already been running for a while,

35
00:01:46,097 --> 00:01:48,549
because the counter started at 8.

36
00:01:48,549 --> 00:01:50,495
Let's scale it back up to three nodes.

37
00:01:50,495 --> 00:01:53,989
Again, these commands may take a while when you run them,

38
00:01:53,989 --> 00:01:55,806
but here we'll just cut to the success.

39
00:01:55,806 --> 00:01:58,389
Now we can see that all three nodes are ready,

40
00:01:58,389 --> 00:02:00,459
but the pods appear to be the same.

41
00:02:00,459 --> 00:02:03,955
If we hit the pods, we could see that none of the counters restarted.

42
00:02:03,955 --> 00:02:07,235
We can look into this a little bit further and see that all

43
00:02:07,235 --> 00:02:10,330
three pods are still on a single node.

44
00:02:10,330 --> 00:02:11,962
Depending on the implementation of your cloud

45
00:02:11,962 --> 00:02:14,147
provider and what the defaults are,

46
00:02:14,147 --> 00:02:20,046
it may not automatically rebalance pods across all of your nodes.

47
00:02:20,046 --> 00:02:23,700
To update the application you'll need to build and tag a new Docker image,

48
00:02:23,700 --> 00:02:25,434
then push it to the repository.

49
00:02:25,434 --> 00:02:30,951
In this example, we create a new image for demo with a 2.0 tag, then push it.

50
00:02:30,951 --> 00:02:33,614
Once the image is in the repository,

51
00:02:33,614 --> 00:02:37,289
you can use kubectl to set the demo image for the existing

52
00:02:37,289 --> 00:02:40,540
demo app deployment to the new 2.0 image.

53
00:02:40,540 --> 00:02:46,196
Kubernetes will then automatically spin up new pods with the new image.

54
00:02:46,196 --> 00:02:49,792
Let's update the application to say Goodbye instead of Hello.

55
00:02:49,792 --> 00:02:54,530
On this workstation, we still just have the 1.0 version of this image.

56
00:02:54,530 --> 00:02:59,174
We could go ahead and run docker build on our workstation and tag a 2.0 version,

57
00:02:59,174 --> 00:03:00,799
then push it to the registry,

58
00:03:00,799 --> 00:03:05,506
but there's also an option to initiate a remote build on a cloud agent so I'm

59
00:03:05,506 --> 00:03:08,990
going to show you that for the 2.0 version of this app.

60
00:03:08,990 --> 00:03:12,898
If your machine has an OS that does not currently support Docker,

61
00:03:12,898 --> 00:03:15,020
this feature could be an alternative to spinning up a

62
00:03:15,020 --> 00:03:16,820
VM just to build Docker images.

63
00:03:16,820 --> 00:03:21,215
The command is az acr build, then the namespace,

64
00:03:21,215 --> 00:03:24,373
image name, and tag for the image you'd like to build,

65
00:03:24,373 --> 00:03:28,417
the registry where the image needs to be pushed upon successful build,

66
00:03:28,417 --> 00:03:30,524
the name of the Docker file,

67
00:03:30,524 --> 00:03:34,068
then the directory that contains all the source code and

68
00:03:34,068 --> 00:03:36,705
other information needed to build the image.

69
00:03:36,705 --> 00:03:40,119
First it will pack all of the source files into a tar,

70
00:03:40,119 --> 00:03:41,679
then upload them to the cloud.

71
00:03:41,679 --> 00:03:43,835
Once a cloud agent becomes available,

72
00:03:43,835 --> 00:03:47,509
it will download that source code and execute the Docker build.

73
00:03:47,509 --> 00:03:50,688
We can see all the output like we would have if we

74
00:03:50,688 --> 00:03:52,554
had built it on our own machine.

75
00:03:52,554 --> 00:03:57,218
Now we can see the push of the new image to our cloud repository.

76
00:03:57,218 --> 00:04:00,396
Once that's complete, we get a message that it was successful.

77
00:04:00,396 --> 00:04:03,384
Again, all of this happened on cloud machines.

78
00:04:03,384 --> 00:04:06,434
I don't have the 2.0 version on this workstation,

79
00:04:06,434 --> 00:04:09,310
but it is in our cloud repository.

80
00:04:09,310 --> 00:04:15,473
Let's run kubectl set image for the 2.0 version of the application.

81
00:04:15,473 --> 00:04:16,465
Once the deployment is updated,

82
00:04:16,465 --> 00:04:21,138
we can check the progress of the pods to see the 2.0 versions getting

83
00:04:21,138 --> 00:04:24,239
created and the 1.0 versions getting terminated.

84
00:04:24,239 --> 00:04:30,119
After about a minute, we have all three pods running with the 2.0 version.

85
00:04:30,119 --> 00:04:31,706
Notice how the three pods are distributed across

86
00:04:31,706 --> 00:04:33,982
only two nodes in the node pool.

87
00:04:33,982 --> 00:04:37,662
If you have specific affinity or distribution requirements,

88
00:04:37,662 --> 00:04:41,078
it's important to understand the default behavior of each cloud provider and

89
00:04:41,078 --> 00:04:45,415
where you might need to be more specific with your deployment.

90
00:04:45,415 --> 00:04:53,000
Let's hit our cluster again and now we can see the Goodbye message and that all three pods are indeed new and have started over at 1.

