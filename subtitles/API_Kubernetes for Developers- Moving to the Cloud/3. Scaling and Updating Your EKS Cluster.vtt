WEBVTT
1
00:00:01.655 --> 00:00:03.757
When you're ready to scale the number of pods in your deployment,

2
00:00:03.757 --> 00:00:07.228
you'll use the kubectl scale deployment command.

3
00:00:07.228 --> 00:00:12.363
For example, kubectl scale deployment, deployment name,

4
00:00:12.363 --> 00:00:15.766
--replicas=, then the desired number of replicas,

5
00:00:15.766 --> 00:00:17.496
in this case 3.

6
00:00:17.496 --> 00:00:21.950
To scale the number of nodes, you'll use the eksctl scale nodegroup command,

7
00:00:21.950 --> 00:00:25.936
passing in the name of the cluster, the desired number of nodes,

8
00:00:25.936 --> 00:00:29.538
in this case 5, and the name of the node group to scale.

9
00:00:29.538 --> 00:00:35.259
Your node pool will then scale to match the requested number of nodes.

10
00:00:35.259 --> 00:00:38.093
We're going to scale the deployment up to three replicas.

11
00:00:38.093 --> 00:00:41.636
We can run kubectl get pods to check the progress.

12
00:00:41.636 --> 00:00:45.555
Notice that the two new pods have only been running a few seconds,

13
00:00:45.555 --> 00:00:49.157
whereas the original pod has been up for a while longer.

14
00:00:49.157 --> 00:00:52.950
If we hit the application, we'll see the original pod starting at 6,

15
00:00:52.950 --> 00:00:56.322
and the two new pods are starting at 1.

16
00:00:56.322 --> 00:01:01.741
We can also see the three pods distributed over the two nodes in our node pool.

17
00:01:01.741 --> 00:01:06.409
To find the name of your node group, run eksctl get nodegroup,

18
00:01:06.409 --> 00:01:10.931
then pass in the name of your cluster.

19
00:01:10.931 --> 00:01:15.944
I'll scale the node group down to 1 using the eksctl scale nodegroup command.

20
00:01:15.944 --> 00:01:20.116
Notice the age of the pods before we scaled the node pool.

21
00:01:20.116 --> 00:01:24.964
When we reduced it to 1 and all three pods needed to be on the same node,

22
00:01:24.964 --> 00:01:28.714
notice how two pods had to be created.

23
00:01:28.714 --> 00:01:31.382
Let's scale the nodes up to 3.

24
00:01:31.382 --> 00:01:35.758
Again, we'll use the eksctl scale nodegroup command.

25
00:01:35.758 --> 00:01:38.687
Now we can see there are three nodes in the node pool.

26
00:01:38.687 --> 00:01:43.719
Notice that the three pods are still running on the same node.

27
00:01:43.719 --> 00:01:47.294
The default settings for rebalancing pods across nodes may vary

28
00:01:47.294 --> 00:01:51.489
from provider to provider so be sure to understand that if you

29
00:01:51.489 --> 00:01:54.921
have any special requirements.

30
00:01:54.921 --> 00:01:58.643
To update the application you'll need to build and tag a new Docker image,

31
00:01:58.643 --> 00:02:00.646
then push it to the repository.

32
00:02:00.646 --> 00:02:05.320
In this example, we will create a new image for demo with a 2.0 tag,

33
00:02:05.320 --> 00:02:06.415
then push it.

34
00:02:06.415 --> 00:02:08.208
Once the image is in the repository,

35
00:02:08.208 --> 00:02:12.686
you can use kubectl to set the demo image for the existing

36
00:02:12.686 --> 00:02:17.080
demo-app deployment to the new 2.0 image.

37
00:02:17.080 --> 00:02:21.852
Kubernetes will then automatically spin up new pods with the new image.

38
00:02:21.852 --> 00:02:28.002
We'll change the message from Hello to Goodbye, then build the 2.0 image.

39
00:02:28.002 --> 00:02:33.968
Once the image is built, we'll go ahead and push it to the repository.

40
00:02:33.968 --> 00:02:39.349
Next, run kubectl set image to set the 2.0 image as the one we want to use.

41
00:02:39.349 --> 00:02:41.457
If you run kubectl get pods,

42
00:02:41.457 --> 00:02:45.360
you can see the three 1.0 versions terminating and

43
00:02:45.360 --> 00:02:48.938
the three 2.0 versions running.

44
00:02:48.938 --> 00:02:50.022
When we hit the application,

45
00:02:50.022 --> 00:02:59.000
we can see the new message and that the counters have restarted. With the new deployment, the pods are distributed across the three nodes.

