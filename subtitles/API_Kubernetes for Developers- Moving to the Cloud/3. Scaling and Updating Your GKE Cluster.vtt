WEBVTT
1
00:00:01.642 --> 00:00:03.767
When you're ready to scale the number of pods in your deployment,

2
00:00:03.767 --> 00:00:07.576
you'll use the kubectl scale-deployment command.

3
00:00:07.576 --> 00:00:12.993
For example, kubectl scale deployment, deployment name,

4
00:00:12.993 --> 00:00:15.925
--replicas=, then the desired number of replicas,

5
00:00:15.925 --> 00:00:18.466
in this case, 3.

6
00:00:18.466 --> 00:00:19.410
To scale the number of nodes,

7
00:00:19.410 --> 00:00:22.665
you'll use the gcloud container clusters resize command,

8
00:00:22.665 --> 00:00:26.860
passing in the name of the cluster, then --num-nodes,

9
00:00:26.860 --> 00:00:30.751
followed by the desired number of nodes,

10
00:00:30.751 --> 00:00:32.493
in this case, 5.

11
00:00:32.493 --> 00:00:34.641
Let's add a couple of more pods.

12
00:00:34.641 --> 00:00:40.896
We'll use kubectl scale deployment, deployment name, and set the replicas to 3.

13
00:00:40.896 --> 00:00:42.567
Now when we hit the application,

14
00:00:42.567 --> 00:00:45.031
we can see that there are three different independent

15
00:00:45.031 --> 00:00:47.320
pods responding to our requests.

16
00:00:47.320 --> 00:00:51.158
Notice that the original pod has been running for longer than the two

17
00:00:51.158 --> 00:00:54.859
that were just started up to make a total of 3.

18
00:00:54.859 --> 00:00:58.703
Let's say that you decided that your app does not need three nodes.

19
00:00:58.703 --> 00:01:02.656
You can scale it down to 1 by running gcloud container clusters resize,

20
00:01:02.656 --> 00:01:06.663
name of the cluster, --num-nodes 1.

21
00:01:06.663 --> 00:01:09.122
This will remove two of the nodes from the node pool

22
00:01:09.122 --> 00:01:12.134
so that you are left with only 1.

23
00:01:12.134 --> 00:01:13.921
Notice that we still have three pods,

24
00:01:13.921 --> 00:01:17.260
but two of them have been running for a shorter period of time.

25
00:01:17.260 --> 00:01:21.636
When there were three nodes, it balanced one pod on each node.

26
00:01:21.636 --> 00:01:24.679
When we'd scaled the node pool from three nodes to one,

27
00:01:24.679 --> 00:01:28.141
it had to spin up two new pods on the single node.

28
00:01:28.141 --> 00:01:32.182
Let's scale it back up to three nodes.

29
00:01:32.182 --> 00:01:33.392
Now we have three nodes.

30
00:01:33.392 --> 00:01:36.241
Notice that two of them are very new.

31
00:01:36.241 --> 00:01:39.753
Now we're back to three pods again.

32
00:01:39.753 --> 00:01:44.198
To update the application, you'll need to build and tag a new Docker image,

33
00:01:44.198 --> 00:01:45.199
then push it to the repository.

34
00:01:45.199 --> 00:01:51.717
In this example, we create a new image for demo with a 2.0 tag, then push it.

35
00:01:51.717 --> 00:01:52.922
Once the image is in the repository,

36
00:01:52.922 --> 00:01:57.147
you can use kubectl to set the demo image for the existing

37
00:01:57.147 --> 00:01:59.879
demo app deployment to the new image.

38
00:01:59.879 --> 00:02:04.457
Kubernetes will then automatically spin up new pods with the new image,

39
00:02:04.457 --> 00:02:07.367
drain traffic from the old pods over to the new pods,

40
00:02:07.367 --> 00:02:08.858
then remove the old pods.

41
00:02:08.858 --> 00:02:13.116
Let's change the message from Hello to Goodbye so that we can create

42
00:02:13.116 --> 00:02:15.969
a new image and update the running application.

43
00:02:15.969 --> 00:02:20.766
We'll tag this one as 2.0, then push it to the repository.

44
00:02:20.766 --> 00:02:26.261
Now we can run the kubectl set image and change the image that we have deployed.

45
00:02:26.261 --> 00:02:27.410
When we hit the URL,

46
00:02:27.410 --> 00:02:32.467
we can see that the new message from our 2.0 version is there.

47
00:02:32.467 --> 00:02:38.000
We have three new pods that have started up with the new version of the application.

