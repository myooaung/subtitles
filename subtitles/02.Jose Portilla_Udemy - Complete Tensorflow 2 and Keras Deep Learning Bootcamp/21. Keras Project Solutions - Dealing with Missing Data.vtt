WEBVTT
1
00:00:05.210 --> 00:00:06.620
Welcome back everyone.

2
00:00:06.620 --> 00:00:11.990
It's now time to begin data pre processing and we'll start off by trying to figure out how to work with

3
00:00:11.990 --> 00:00:14.470
the missing data in our dataset.

4
00:00:14.540 --> 00:00:20.120
This probably the hardest part of the entire project because it's more nuanced and there's no 100 percent

5
00:00:20.120 --> 00:00:21.440
correct answer.

6
00:00:21.440 --> 00:00:26.660
Remember our options are to either keep the missing data drop the missing data or fill in the missing

7
00:00:26.660 --> 00:00:27.620
data.

8
00:00:27.650 --> 00:00:31.170
Let's review what data is missing and how we can work with it.

9
00:00:31.190 --> 00:00:31.530
All right.

10
00:00:31.580 --> 00:00:35.840
So to be in Section 2 on data pre processing we have a couple of goals and right now we're going to

11
00:00:35.840 --> 00:00:39.210
focus on either removing or filling in any missing data.

12
00:00:39.470 --> 00:00:44.300
And this very first cell actually isn't a task it's just the head of the data frame that we currently

13
00:00:44.300 --> 00:00:45.170
have.

14
00:00:45.170 --> 00:00:49.220
So we'll move on to missing data and we want to ask ourselves what is the length of the entire data

15
00:00:49.220 --> 00:00:49.950
frame.

16
00:00:49.990 --> 00:00:55.730
And that's easy to answer it just by saying LPN on the data frame you can also use DFT info and it looks

17
00:00:55.730 --> 00:00:59.860
like we have about about four hundred thousand points a little less than that.

18
00:00:59.870 --> 00:01:04.640
So next we want to create a series that displays the total count of missing values per column and we've

19
00:01:04.640 --> 00:01:09.950
shown you how to do this before you simply say DLF dot is null.

20
00:01:09.950 --> 00:01:16.640
And recall that returns back a data frame that looks like this and we can take the sum of this across

21
00:01:16.640 --> 00:01:24.290
the rows here and then we get the actual columns and their numbers for how many points are missing.

22
00:01:24.320 --> 00:01:30.020
So it looks like we have around twenty two thousand missing for EMP title which will later find that

23
00:01:30.040 --> 00:01:31.430
as employment title.

24
00:01:31.430 --> 00:01:34.020
Also employment length is missing quite a bit.

25
00:01:34.130 --> 00:01:40.220
Then we have this titles missing just a few missing on this particular feature we're missing quite a

26
00:01:40.220 --> 00:01:46.790
bit on Mort Ach so we'll have to see what that is and then where you're also missing a couple on any

27
00:01:46.790 --> 00:01:48.750
public record of bankruptcies.

28
00:01:48.740 --> 00:01:56.390
OK so what would be interesting is to get these numbers in terms of percentage of this total data frame

29
00:01:56.480 --> 00:02:00.110
just to see how significant of a problem is it.

30
00:02:00.110 --> 00:02:05.180
So I want to see it in this fashion of I'm missing five point seven percent of all employment titles

31
00:02:05.270 --> 00:02:09.340
or I'm missing almost 10 percent of these mortgage accounts.

32
00:02:09.440 --> 00:02:11.810
So let's go ahead and figure out how to do that.

33
00:02:11.960 --> 00:02:20.090
One way to do it is to get our previous series recall that is DFS No not some.

34
00:02:20.090 --> 00:02:26.210
And then if I simply divide every item there by the length of the data frame I can then multiply that

35
00:02:26.210 --> 00:02:31.430
by 100 to get a percentage and that is how we can get this exact same result.

36
00:02:31.430 --> 00:02:33.590
So again it's 100 times DFS No.

37
00:02:33.770 --> 00:02:36.210
That sum divided by the length the data frame.

38
00:02:36.530 --> 00:02:42.800
So it looks like the ones we're gonna have to really focus on is this mortgage accounts because we can't

39
00:02:42.800 --> 00:02:44.510
just really drop 10 percent of our data.

40
00:02:44.900 --> 00:02:50.480
But some of these are so minor such as this one and this one these are less than actually less than

41
00:02:50.480 --> 00:02:51.640
half a percent of our data.

42
00:02:51.650 --> 00:02:53.780
So it should be fine to drop some of these.

43
00:02:53.780 --> 00:02:58.610
So we're gonna do is we're gonna start from the top to the bottom we'll start off with this employment

44
00:02:58.610 --> 00:03:00.480
title and employment length.

45
00:03:00.710 --> 00:03:02.220
So let's scroll down here.

46
00:03:02.270 --> 00:03:06.920
So when I first examine this employment title and employment length to see whether it will be OK to

47
00:03:06.920 --> 00:03:10.670
drop them so we can use ft underscore info.

48
00:03:11.090 --> 00:03:16.580
That's the feature information function that we set up for you at the beginning that simply reports

49
00:03:16.580 --> 00:03:18.590
back what these are.

50
00:03:18.620 --> 00:03:25.220
So the feature information on the employment title that's simply the job title supplied by the bar when

51
00:03:25.220 --> 00:03:29.800
applying for the loan and the employment length one we actually have it free here.

52
00:03:29.810 --> 00:03:31.560
It's the employment length in years.

53
00:03:31.580 --> 00:03:37.190
So possible values are between 0 and 10 where 0 means less than one year and 10 means 10 or more years.

54
00:03:37.610 --> 00:03:42.740
So next question that we want to start thinking about is well how many unique employment job titles

55
00:03:42.800 --> 00:03:47.630
are there because when you start thinking Are we just going to drop employment title or fill it in with

56
00:03:47.630 --> 00:03:48.440
something.

57
00:03:48.500 --> 00:03:56.740
So it might be adjusting to actually explore the employment title so we'll say def employment title

58
00:03:57.430 --> 00:04:00.120
and let's just see how many unique employment titles there are.

59
00:04:01.910 --> 00:04:06.620
So after running that you should see there's a ton of unique employment titles and factors.

60
00:04:06.680 --> 00:04:12.740
One hundred seventy three thousand unique employment titles recall that are dataset itself was around

61
00:04:12.740 --> 00:04:13.790
400000.

62
00:04:13.850 --> 00:04:18.700
So it looks like there's almost half of that are all unique employment titles.

63
00:04:18.860 --> 00:04:25.840
And then we can kind of check it out with the value counts to explore that further which I would recommend

64
00:04:27.600 --> 00:04:29.340
although you don't have to do this for the project.

65
00:04:29.370 --> 00:04:34.260
We can simply run this and we can see here that we have some teachers managers registered nurses et

66
00:04:34.260 --> 00:04:38.240
cetera and we have a ton of titles that are actually just unique to that particular person.

67
00:04:38.700 --> 00:04:44.430
So realistically there are just way too many titles to convert this to some sort of dummy variable feature

68
00:04:44.760 --> 00:04:50.830
we can't add in an extra hundred and seventy three thousand columns of billions.

69
00:04:50.850 --> 00:04:57.960
Now what you could do is maybe with extensive feature engineering start categorizing these maybe as

70
00:04:58.320 --> 00:05:03.870
high income jobs versus medium income jobs but again you have to make a lot of assumptions and you have

71
00:05:03.870 --> 00:05:10.090
to figure out how to do this and map it to over one hundred seventy three thousand different job titles.

72
00:05:10.110 --> 00:05:15.420
So this is just so many unique job titles that it's probably not going to be very informative because

73
00:05:15.420 --> 00:05:17.970
half of all people have some unique job title.

74
00:05:18.090 --> 00:05:22.980
So instead of we're going to do is we'll simply remove that title because it's not realistically going

75
00:05:22.980 --> 00:05:25.510
to be useful to us in any manner.

76
00:05:25.530 --> 00:05:37.370
So we'll say DCF is going to be equal to the FDA drop the employment title and we do this a lot.

77
00:05:37.370 --> 00:05:43.730
Access is equal to 1 and be careful with these dropping statements because you can only run them once

78
00:05:44.090 --> 00:05:45.500
if you try running them more than once.

79
00:05:45.500 --> 00:05:48.620
You'll end up getting error because you already remove that feature.

80
00:05:48.620 --> 00:05:53.540
OK next we want to create account plot of the employment link feature column.

81
00:05:53.540 --> 00:05:59.720
So again we want to try to sort the order of the values so one way to do that is to try to grab a list

82
00:05:59.720 --> 00:06:08.870
here in order to actually get that order so we can say just as before the F employment length should

83
00:06:08.870 --> 00:06:10.340
be an underscore.

84
00:06:10.610 --> 00:06:15.050
Go ahead and say drop N A.

85
00:06:15.110 --> 00:06:21.850
And after that we should be able to call unique if we don't drop any sometimes that returns an error.

86
00:06:21.860 --> 00:06:29.090
So here are the actual unique titles and I'm going to just go ahead and sort them and now I have the

87
00:06:29.150 --> 00:06:30.810
sorted titles.

88
00:06:30.950 --> 00:06:38.440
And this actually looks almost in order except that excellently sorted one year right before this 10

89
00:06:38.450 --> 00:06:40.170
plus years.

90
00:06:40.180 --> 00:06:45.290
So what I can do is simply copy this which is what I was doing down here.

91
00:06:45.420 --> 00:06:53.260
Paste it into the cell and we'll go ahead and say employ length order is equal to this list and then

92
00:06:53.320 --> 00:06:59.190
I'll simply justice by grabbing this guy sticking it down here.

93
00:06:59.280 --> 00:07:06.790
Let's go ahead and make sure that formatting is correct and then we'll grab this guy and stick it there

94
00:07:07.600 --> 00:07:09.960
and then fix the commas OK.

95
00:07:10.180 --> 00:07:12.940
So after running that we now have this nice ordering.

96
00:07:12.940 --> 00:07:17.620
Again this is technically optional but it'll be nice for our account plot that we create right here

97
00:07:17.740 --> 00:07:19.860
to have this actually in order.

98
00:07:19.990 --> 00:07:28.780
And now I can do this by simply saying as an S count plot where x is the employment length my data is

99
00:07:28.780 --> 00:07:35.060
equal to my current data frame and my order is equal to that employment length order.

100
00:07:35.140 --> 00:07:37.390
So if you run that you get to see this right here.

101
00:07:37.420 --> 00:07:44.470
We'll go ahead and stretch it out so we don't get that overlapping by saying Pulte figure fig size is

102
00:07:44.470 --> 00:07:49.260
equal to go in and say twelve by four and we get this nice plot here.

103
00:07:49.390 --> 00:07:53.950
So it looks like quite the majority of people have been working in their employment for 10 plus years

104
00:07:54.310 --> 00:07:58.220
which makes sense if you're taking a loan you're very likely to have a job.

105
00:07:58.240 --> 00:08:00.130
Otherwise how are you going to pay it back.

106
00:08:00.130 --> 00:08:04.730
So most people have been working for more than a year and it looks like we messed up the order here

107
00:08:04.730 --> 00:08:06.470
of one year versus less than one year.

108
00:08:06.520 --> 00:08:12.380
So let's go ahead and take this guy and put it right there where it belongs.

109
00:08:12.380 --> 00:08:15.510
And then that should fix the ordering for us.

110
00:08:15.530 --> 00:08:15.890
There we go.

111
00:08:15.890 --> 00:08:18.760
So now we see the less than one year one year two year etc..

112
00:08:18.770 --> 00:08:25.250
OK so something I may also want to do is just view how useful a feature this is based off what we're

113
00:08:25.250 --> 00:08:26.030
trying to predict.

114
00:08:26.030 --> 00:08:29.890
Recall that we're trying to predict whether or not someone's actually going to pay back their loan.

115
00:08:29.900 --> 00:08:32.480
So what is their loan status.

116
00:08:32.510 --> 00:08:33.650
We run that.

117
00:08:33.650 --> 00:08:39.800
And really what I'm interested in here is the relationship between fully paid off versus charged off

118
00:08:40.280 --> 00:08:42.200
per employment length.

119
00:08:42.200 --> 00:08:49.010
If there's an extreme difference in one of these categories of fully paid off versus charged off for

120
00:08:49.010 --> 00:08:54.050
example maybe if you work less than one year everyone there charged that their loan didn't pay it back

121
00:08:54.320 --> 00:08:56.240
then it's a fairly important feature.

122
00:08:56.240 --> 00:09:02.060
If the ratio of this blue bar to this orange bar is essentially the same across all these employment

123
00:09:02.060 --> 00:09:05.870
length categories then this isn't a very informative feature.

124
00:09:05.870 --> 00:09:11.960
So what I want to do is I really want to figure out the ratio between the fully paid off versus charge

125
00:09:11.960 --> 00:09:18.740
off people per category of employment link and that's what this later task we just completed this one

126
00:09:18.740 --> 00:09:19.550
down here.

127
00:09:19.610 --> 00:09:20.320
This task we're here.

128
00:09:20.330 --> 00:09:21.660
That's what it's essentially describing.

129
00:09:22.010 --> 00:09:23.330
So that does really inform us.

130
00:09:23.420 --> 00:09:30.050
Well we really want to know is the percentage of charge offs per employment length category.

131
00:09:30.050 --> 00:09:30.420
OK.

132
00:09:30.440 --> 00:09:33.310
So how can we actually do this.

133
00:09:33.370 --> 00:09:37.900
This is a bit of a challenge task but if you just think of this logically step by step especially when

134
00:09:37.900 --> 00:09:40.870
you're using pandas it should be pretty straightforward.

135
00:09:40.900 --> 00:09:46.900
So what I want to do is I essentially want to build out a series that looks something like this the

136
00:09:46.930 --> 00:09:52.380
percent of people that don't pay back their loan per employment category.

137
00:09:52.390 --> 00:10:02.140
So what I'm going to do is I'll say go ahead and grab the subset of the data frame where my loan status

138
00:10:04.880 --> 00:10:08.700
is equal to charged off.

139
00:10:10.080 --> 00:10:16.020
So these are the people that did not pay back their loans and I'm going to copy paste this down here

140
00:10:16.050 --> 00:10:22.230
because we'll do the same thing for people that did fully pay off their loan.

141
00:10:22.380 --> 00:10:26.670
So I have right now one data frame where people didn't pay back their loan charged off and the one where

142
00:10:26.670 --> 00:10:27.790
they fully paid it.

143
00:10:27.870 --> 00:10:33.630
And what I'm going to do is I'm interested in grouping it by this employment length category.

144
00:10:33.810 --> 00:10:35.610
So I will say group by

145
00:10:38.710 --> 00:10:47.730
employment underscore length and I'll do that for both cases and next what I want to do is actually

146
00:10:47.730 --> 00:10:49.980
want to grab the counts for each of them.

147
00:10:50.070 --> 00:10:54.210
Want to figure out how many people are actually in this blue bar and how many people are in this orange

148
00:10:54.210 --> 00:10:54.850
bar.

149
00:10:54.870 --> 00:10:57.690
So this is I am just trying to the math behind these bars.

150
00:10:57.690 --> 00:10:59.850
So they charged off people.

151
00:10:59.880 --> 00:11:01.250
Group them by employment length.

152
00:11:01.320 --> 00:11:03.720
Go ahead and do the count there.

153
00:11:03.840 --> 00:11:07.650
That essentially now is this orange count column.

154
00:11:07.650 --> 00:11:11.640
And to get the blue count column we'll just do that on the fully paid statuses.

155
00:11:11.640 --> 00:11:16.810
And in this case I'm really only interested in the lone status column for each of these.

156
00:11:16.920 --> 00:11:22.120
So instead of just running this and getting the amounts it's gonna be the same for all these was going

157
00:11:22.120 --> 00:11:26.300
to pass in a column of lone status and get those numbers.

158
00:11:26.340 --> 00:11:31.410
So here I can see the lone status for the charged off per employment category.

159
00:11:31.470 --> 00:11:34.950
And then if I do this on this guy I'll get the same thing.

160
00:11:34.950 --> 00:11:41.920
But for people that fully paid back their loans and I'm going to assign these by saying employment or

161
00:11:42.040 --> 00:11:46.830
ENP underscore I'll go ahead and say see 0 4 charge off.

162
00:11:46.900 --> 00:11:55.600
So that's one series and then we'll say ENP underscore FP for fully paid and then all image sit in is

163
00:11:55.600 --> 00:11:56.730
the ratio between them.

164
00:11:56.740 --> 00:12:06.360
So that would be the employee length for people that charged off divided by the fully paid.

165
00:12:06.360 --> 00:12:15.380
So we can run these and now I can see the percent of people that charged off versus the fully paid.

166
00:12:15.470 --> 00:12:18.410
Keep in mind this is a direct ratio.

167
00:12:18.410 --> 00:12:31.240
If I actually wanted the percent I could say ENP C O plus SMP FP run that and then this is the actual

168
00:12:31.240 --> 00:12:34.660
percent per category instead of the direct ratio.

169
00:12:34.900 --> 00:12:40.290
And we can see here that across the extremes it looks to be extremely similar.

170
00:12:40.300 --> 00:12:43.930
So I can see zero point one nine zero point one eight.

171
00:12:43.980 --> 00:12:47.740
Mostly is was a point nine versus the max of zero point two.

172
00:12:47.740 --> 00:12:53.800
So it looks like this particular feature of employment length doesn't actually have some extreme differences

173
00:12:54.070 --> 00:12:55.670
on the charge off rates.

174
00:12:55.780 --> 00:13:01.480
So it looks like regardless of what actual employment length you have if you were to pick someone about

175
00:13:01.480 --> 00:13:07.870
20 percent of them somewhere between nineteen percent or nineteen point nine nine and 20 percent are

176
00:13:07.870 --> 00:13:10.060
going to have not paid back their loans.

177
00:13:10.240 --> 00:13:17.950
And we can just exemplify this further by if we say employment underscore at length I can actually make

178
00:13:18.010 --> 00:13:28.260
a bar plot for this by saying plot kind is equal to bar run that I can see the percentages or the ratios

179
00:13:28.590 --> 00:13:29.930
essentially the same information.

180
00:13:29.970 --> 00:13:34.950
Really what I'm looking for here is the fact that since all these bars are almost the same height there

181
00:13:34.950 --> 00:13:40.050
really isn't that much information or differentiation between the employment length columns which is

182
00:13:40.080 --> 00:13:45.960
kind of surprising but we can see here the main difference is that people who work for 10 years are

183
00:13:46.080 --> 00:13:52.230
have a slightly smaller charge off rate than people who worked less like one year or less than a year.

184
00:13:52.290 --> 00:13:57.360
But the difference is not extreme enough to really validate keeping this feature.

185
00:13:57.450 --> 00:14:02.370
So since they're so extremely similar across all employments well we'll go ahead and do it just drop

186
00:14:02.370 --> 00:14:03.560
that column.

187
00:14:03.600 --> 00:14:13.300
We'll say the f is equal to DFT drop employment length access equal to one.

188
00:14:13.540 --> 00:14:19.030
So run that and next revisit the data frame to see what feature columns still have missing data.

189
00:14:19.090 --> 00:14:29.020
So say the f is no that some go ahead and run that you'll get back this series and it looks like we

190
00:14:29.020 --> 00:14:37.220
still have the title this Revel you till this mortgage accounts and the public record bankruptcies so

191
00:14:37.220 --> 00:14:43.700
we'll go ahead and continue dealing with these remaining missing features and the continuation of this

192
00:14:43.700 --> 00:14:44.450
lecture.

193
00:14:44.540 --> 00:14:45.410
I'll see you there.
