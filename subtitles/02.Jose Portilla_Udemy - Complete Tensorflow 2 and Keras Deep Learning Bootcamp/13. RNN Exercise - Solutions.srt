1
00:00:05,240 --> 00:00:06,180
Welcome back everyone.

2
00:00:06,200 --> 00:00:10,680
In this lecture we're going to go over the solutions for the recurrent neural network exercise questions.

3
00:00:10,700 --> 00:00:12,030
Let's get started.

4
00:00:12,030 --> 00:00:14,020
Okay here we are at the exercise notebook.

5
00:00:14,030 --> 00:00:16,160
Let's go ahead and begin.

6
00:00:16,160 --> 00:00:20,720
First we need to import pandas as PD and then we're gonna need to do a little bit of plotting so we'll

7
00:00:20,720 --> 00:00:28,570
import map plot lib dot pi plot pie plot as BLT.

8
00:00:28,570 --> 00:00:32,500
And then just in case depending on your version of notebook you may need to set map plot lib.

9
00:00:33,570 --> 00:00:34,090
In line

10
00:00:37,030 --> 00:00:40,480
and I don't believe we use num pi but you can go ahead and import it anyways.

11
00:00:40,560 --> 00:00:41,880
Okay so the dataset.

12
00:00:41,940 --> 00:00:48,230
Remember our goal is to forecast into the future the actual production of frozen dessert first thing.

13
00:00:48,290 --> 00:00:55,380
First me going to read in this dataset we'll say the f is equal to PD read CSB and it's located underneath

14
00:00:55,410 --> 00:01:00,230
our data folder so we can provide the full file path depending where you're located on your computer.

15
00:01:00,350 --> 00:01:02,030
It's called frozen dessert production.

16
00:01:02,040 --> 00:01:08,220
You should be able to tab autocomplete this and then just as mentioned in the lecture you actually want

17
00:01:08,220 --> 00:01:18,530
to go ahead and set our index column equal to date and then we'll say past dates is equal to true.

18
00:01:19,040 --> 00:01:25,550
So that allows me to to then just read it in L in one step with the dates as a date timestamp as the

19
00:01:25,550 --> 00:01:26,440
index.

20
00:01:26,480 --> 00:01:29,580
Now this series of letters and numbers kind of hard to remember.

21
00:01:29,690 --> 00:01:32,270
So we'll go ahead and change the column name to production.

22
00:01:32,270 --> 00:01:41,100
So say the F columns is equal to and as a list we press in production.

23
00:01:41,100 --> 00:01:41,550
There we go.

24
00:01:41,760 --> 00:01:43,230
So we get that result.

25
00:01:43,230 --> 00:01:45,340
Go ahead and plot at the time series.

26
00:01:45,570 --> 00:01:47,310
You could just simply say the F plot.

27
00:01:47,310 --> 00:01:49,050
You can also edit the figure size if you want.

28
00:01:49,110 --> 00:01:55,800
So if you wanted to you could say fig size is equal to and then play around if this till it's something

29
00:01:55,800 --> 00:01:56,510
meaningful to you.

30
00:01:56,520 --> 00:01:58,980
But obviously there's clear seasonality during the summer months.

31
00:01:58,980 --> 00:02:01,370
So let's explore that further.

32
00:02:01,380 --> 00:02:03,870
Next we want to figure out our train test split.

33
00:02:03,870 --> 00:02:08,770
So the first task is to figure out well how much data will actually have to play with so we'll say.

34
00:02:08,800 --> 00:02:10,110
What's the length of my data frame.

35
00:02:10,150 --> 00:02:11,980
It's five hundred and seventy three points.

36
00:02:11,980 --> 00:02:18,160
So our next task is to split the data into a train test split and in our particular case we told you

37
00:02:18,160 --> 00:02:20,920
the test set should be the last 24 months of data.

38
00:02:20,920 --> 00:02:24,510
That's something you could play around with but it should at least include 12 months of data which is

39
00:02:24,510 --> 00:02:25,720
a seasonal cycle.

40
00:02:25,720 --> 00:02:33,000
We'll go ahead and make it larger to be two years of data so we'll say test size is equal to twenty

41
00:02:33,000 --> 00:02:40,680
four and then our test index is then equal to the length of the data frame minus the test size and what's

42
00:02:40,680 --> 00:02:44,970
nice about this equation is then you can play around if your test size and edit the test index as you

43
00:02:44,970 --> 00:02:46,410
see fit.

44
00:02:46,860 --> 00:02:52,540
And then our training set is simply starting from the very beginning.

45
00:02:52,740 --> 00:02:58,860
Go all the way up to that test index and then our test set is essentially the end of that.

46
00:02:58,860 --> 00:03:01,950
So it says starting at the test index go all the way to the end.

47
00:03:02,570 --> 00:03:05,310
OK so we have our training set and our test set.

48
00:03:05,340 --> 00:03:08,050
If you check the length of the test set it's twenty four months.

49
00:03:08,070 --> 00:03:12,690
So essentially what a model's going to be doing is based off most data set.

50
00:03:12,690 --> 00:03:17,940
Essentially everything up to the last two years see if you can forecast its future two years which is

51
00:03:17,940 --> 00:03:18,480
our test set.

52
00:03:19,020 --> 00:03:23,580
Okay so next it's up to us to scale the data so we'll say

53
00:03:26,700 --> 00:03:33,850
S.K. learned pre processing import min max scalar.

54
00:03:34,190 --> 00:03:43,930
Go ahead and run that create our scalar object scalar is equal to an instance of min max scalar and

55
00:03:43,930 --> 00:03:48,040
then we only fit to the training data because we don't want to assume that we have prior information

56
00:03:48,130 --> 00:03:49,360
about the future.

57
00:03:49,690 --> 00:03:52,910
So we'll say fit to our training data.

58
00:03:52,930 --> 00:03:56,020
Technically this could be a fit transform if you wanted to do it all at once.

59
00:03:56,440 --> 00:03:59,380
But I like separating out the two steps just for readability.

60
00:03:59,380 --> 00:04:06,370
So skilled trained scalar that transform transform our training set and we'll start to make sure to

61
00:04:06,370 --> 00:04:07,710
transform our test set.

62
00:04:08,500 --> 00:04:12,530
So scalar thought transform on the test set.

63
00:04:13,300 --> 00:04:15,490
OK so we have our skilled data.

64
00:04:15,520 --> 00:04:18,390
Next step is to create our time series generator.

65
00:04:18,430 --> 00:04:25,180
So the first generator to create is for the actual training data we'll say from tensor flow thought

66
00:04:25,180 --> 00:04:29,980
Kerry's stop pre processing thought sequence.

67
00:04:29,980 --> 00:04:35,690
Import time series generator and we'll say there we go.

68
00:04:36,330 --> 00:04:42,650
I just had auto completed that and the length of our actual training is up to us.

69
00:04:42,660 --> 00:04:48,450
But we said at a minimum it should be 18 months to give length to capture can the full year seasonality.

70
00:04:48,660 --> 00:04:53,580
So we'll say that the length of the batches is 18 months.

71
00:04:53,580 --> 00:04:55,110
How many actual features are we working with.

72
00:04:55,140 --> 00:04:58,730
Well it's just one feature and you can define it here define it later.

73
00:04:58,740 --> 00:05:05,450
Remember we eventually passed that in upon creating the model and now it's time to create our time series

74
00:05:05,450 --> 00:05:07,890
generator.

75
00:05:07,950 --> 00:05:09,870
We simply pass our scale training set.

76
00:05:09,960 --> 00:05:16,680
That's a source of both our X and Y the length is just equal to the length of the batch.

77
00:05:16,680 --> 00:05:19,560
Remember that's how many months are included in each batch.

78
00:05:19,560 --> 00:05:22,560
And then the batch size will just be one.

79
00:05:22,560 --> 00:05:29,100
We'll just do kind of one set at a time and you can do generator index at zero to see what the example

80
00:05:29,100 --> 00:05:30,170
batches look like.

81
00:05:30,260 --> 00:05:31,750
We did that in the lectures.

82
00:05:31,960 --> 00:05:32,600
Okay.

83
00:05:32,730 --> 00:05:34,660
Next it's time to create the model.

84
00:05:34,770 --> 00:05:39,630
So we'll say from tensor flow not care stop models.

85
00:05:39,630 --> 00:05:42,700
Import sequential.

86
00:05:43,030 --> 00:05:46,000
And then from tensor flow.

87
00:05:46,120 --> 00:05:47,790
Dark hair start layers.

88
00:05:47,790 --> 00:05:49,560
We need to import two types of layers.

89
00:05:49,560 --> 00:05:52,030
One is the LACMA unit layer that we're going to be using.

90
00:05:52,200 --> 00:05:56,100
And for the final output layer it needs to be a dense layer.

91
00:05:56,310 --> 00:05:57,690
And now we'll define our model.

92
00:05:57,690 --> 00:06:02,330
So a model is equal to sequential model the add.

93
00:06:02,460 --> 00:06:09,940
And I'll add in 100 Ellis Tam units here and then we'll make sure that the input shape is defined correctly

94
00:06:09,940 --> 00:06:13,910
or say input shape is equal to length.

95
00:06:14,050 --> 00:06:22,650
Make sure it's in a tuple length by any number of features finally you have to make sure to add in the

96
00:06:23,340 --> 00:06:26,050
dense layer so same model.

97
00:06:26,050 --> 00:06:27,450
Add in art then slayer.

98
00:06:27,460 --> 00:06:33,310
This should just be one neuron since we're predicting just one output and then after that we'll compile

99
00:06:33,310 --> 00:06:33,840
the model.

100
00:06:34,300 --> 00:06:45,040
So we say model dot compile optimizer go ahead and choose Adam and then the loss is means squared error.

101
00:06:45,090 --> 00:06:46,740
You can play around the optimizer as well.

102
00:06:46,770 --> 00:06:47,460
Let's not.

103
00:06:47,490 --> 00:06:48,990
Doesn't have to be the Adam string.

104
00:06:49,130 --> 00:06:50,220
We have a lot of options there.

105
00:06:50,220 --> 00:06:55,440
Let's run this make sure we didn't commit any typos it's created the model and to see a summary of the

106
00:06:55,440 --> 00:07:00,160
model can say model that summary and we see the output here.

107
00:07:00,190 --> 00:07:05,130
OK next we want to create a generator for the scale test slash validation set.

108
00:07:05,130 --> 00:07:14,170
So just as we did in a lecture I will say My validation generator is equal to a time series generator

109
00:07:14,440 --> 00:07:17,810
except this time it's based off the scaled test data.

110
00:07:18,190 --> 00:07:24,880
So scale test data and then the length is the same as before and then the batch size will also be the

111
00:07:24,880 --> 00:07:30,020
same as before and whips make sure batch size.

112
00:07:30,020 --> 00:07:33,300
I forgot to define it so I'll just hardcoded as one.

113
00:07:33,320 --> 00:07:38,330
OK so probably good ideas to just set a variable batch size up above that we can play around the batch

114
00:07:38,330 --> 00:07:42,100
sizes but typically one works really well for a time series data.

115
00:07:42,110 --> 00:07:46,650
Next we want to create an early stopping callback based off validation loss.

116
00:07:46,700 --> 00:07:56,750
We do this by saying from tensor flow Harris that callbacks import early stopping and then we'll create

117
00:07:56,810 --> 00:08:04,680
our early stop is equal to early stopping and what we're going to be moderating here is validation loss

118
00:08:05,800 --> 00:08:09,890
and I recommend to have a little higher patients when it comes to recurrent know networks.

119
00:08:09,930 --> 00:08:13,350
Obviously there's no right answer here you can always play around with this but I'll go ahead and say

120
00:08:13,500 --> 00:08:19,020
just wait for two epochs then we want to fit the model to the generator and let the early stopping dictate

121
00:08:19,110 --> 00:08:21,170
how long we're actually going to train for.

122
00:08:21,420 --> 00:08:29,430
So we'll say model that fit to a generator passing that training generator created to some arbitrarily

123
00:08:29,550 --> 00:08:36,910
kind of high number of epochs the validation data should be linked to your validation generator and

124
00:08:36,910 --> 00:08:42,220
then finally don't forget your callbacks list to make sure we actually implement the early stop.

125
00:08:42,220 --> 00:08:44,360
So let's go ahead and run this.

126
00:08:44,360 --> 00:08:50,120
I'm going to comment out that previous cell to make sure we don't get any typos here run that.

127
00:08:50,720 --> 00:08:54,870
OK so now it's fitting to the generator in the solutions notebook.

128
00:08:54,880 --> 00:09:00,040
We went ahead and the only difference that we've done so far is we called the Ellis TAM to use an activation

129
00:09:00,040 --> 00:09:02,790
unit of recurrent rectified linear units.

130
00:09:02,920 --> 00:09:06,960
Here I'm just using the default which is the hyperbolic tangent I believe if we do shift type here it'll

131
00:09:07,030 --> 00:09:07,960
tell us.

132
00:09:07,960 --> 00:09:12,520
But what we're going to do is we'll fast forward into the future until this is done training on whatever

133
00:09:12,520 --> 00:09:14,530
epochs are dictated by early stop.

134
00:09:14,830 --> 00:09:18,330
So let me hop forward into the future until this is done training.

135
00:09:18,330 --> 00:09:18,610
All right.

136
00:09:18,610 --> 00:09:19,920
This is finished training for me.

137
00:09:19,940 --> 00:09:24,790
Train pretty fast once and for epochs depending on the activation functions in the number of neurons

138
00:09:24,790 --> 00:09:25,270
you choose.

139
00:09:25,270 --> 00:09:30,180
It may take longer but we'll go going to just plot our losses so let's go ahead and do that.

140
00:09:30,190 --> 00:09:34,120
We'll say loss is equal to PD.

141
00:09:35,530 --> 00:09:44,630
Data frame model our history the history and then say Lost Plot.

142
00:09:44,630 --> 00:09:50,780
Run that and we can see here that it looks like our training loss was starting to go down more but our

143
00:09:50,780 --> 00:09:55,040
validation lost was starting to peak and due to our patients value instead of just stopping training

144
00:09:55,070 --> 00:09:56,290
they're okay.

145
00:09:56,330 --> 00:09:58,950
The next step is to evaluate on our test data.

146
00:09:59,120 --> 00:10:04,400
So we're going to forecast predictions into the test data range and that may be slightly different depending

147
00:10:04,400 --> 00:10:05,860
on what you chose.

148
00:10:05,890 --> 00:10:11,020
But recall that our test data set if we come back up here they should actually be 24 months.

149
00:10:11,030 --> 00:10:15,740
So we come back up here and we decided that the test length was 24 months.

150
00:10:15,740 --> 00:10:21,650
So let's go ahead and do that and we'll type out here we'll be the following and I'll I'll do this by

151
00:10:21,650 --> 00:10:26,540
hand because this is probably the trickiest part to understand that the entire lecture.

152
00:10:26,540 --> 00:10:27,770
The rest is pretty straightforward.

153
00:10:27,770 --> 00:10:28,730
Once you get used to it.

154
00:10:29,090 --> 00:10:32,360
But I sit an empty holder for my test predictions.

155
00:10:32,360 --> 00:10:36,320
Let me comment that that cell so we get a bunch of room here and create an empty cell.

156
00:10:36,660 --> 00:10:42,140
Okay so first thing I wanna keep track my test predictions and then the next idea is I have to figure

157
00:10:42,140 --> 00:10:45,380
out what does the first evaluation batch look like.

158
00:10:45,380 --> 00:10:50,940
So what do I first pass in to my model and say hey give me one month into the future.

159
00:10:50,960 --> 00:10:56,050
Well that should be the very last possible batch in my scale the training set.

160
00:10:56,050 --> 00:11:03,570
So I'll be scaled training data minus the length of my batch all the way to the end.

161
00:11:03,580 --> 00:11:09,430
So for example if I was predicting if my model took in 24 months to predict the 25th month one month

162
00:11:09,430 --> 00:11:14,380
into the future then this would be the last 24 months of my training data so that the next month is

163
00:11:14,380 --> 00:11:16,200
the first point in my test data.

164
00:11:16,240 --> 00:11:21,430
So there's my first evaluation batch and I'll set it to my current batch.

165
00:11:21,520 --> 00:11:26,260
The one thing I need to make sure to do is to make sure it's in the correct shape.

166
00:11:26,260 --> 00:11:31,360
So that's first evaluation batch reshaped to the following and we pass on a tuple here.

167
00:11:31,360 --> 00:11:32,840
Notice the double parentheses.

168
00:11:32,920 --> 00:11:40,840
It's just one time series per batch so it's essentially batch size by the length which in our case was

169
00:11:40,840 --> 00:11:46,510
that 18 months and then the number of features is just one.

170
00:11:46,540 --> 00:11:50,520
And if you want to play around the batch size this one would be batch size.

171
00:11:50,550 --> 00:11:51,890
So that's what this represents.

172
00:11:53,070 --> 00:12:01,740
Then we have for I in range length of test because we're only forecasting onto the test set.

173
00:12:02,900 --> 00:12:05,770
Then I need to get a prediction one timestamp ahead.

174
00:12:05,930 --> 00:12:12,740
So I'll tell my model go ahead and predict based off the current batch information at the very first

175
00:12:12,740 --> 00:12:13,190
pass.

176
00:12:13,220 --> 00:12:18,320
That's essentially saying hey given the last 24 months of the training data what does the first month

177
00:12:18,350 --> 00:12:19,970
of the test data look like.

178
00:12:19,970 --> 00:12:24,290
And to get the first month the test data because the way the formatting works out and the way it returns

179
00:12:24,290 --> 00:12:26,280
it we index just zero there.

180
00:12:26,540 --> 00:12:29,870
And you can print these out in case you're ever confused.

181
00:12:29,870 --> 00:12:32,210
So say my current prediction is this.

182
00:12:32,240 --> 00:12:36,290
So I would recommend that if this is still confusing you just go ahead and put a bunch of print statements

183
00:12:36,290 --> 00:12:39,800
here print out the entire prediction and just do a range of one.

184
00:12:39,830 --> 00:12:45,300
See what happens in one loop OK then I want to store that prediction.

185
00:12:45,330 --> 00:12:53,210
So I say my test predictions go ahead and append my current prediction.

186
00:12:53,380 --> 00:12:59,260
And then finally I need to update my current batch to now drop that very first value and then add in

187
00:12:59,500 --> 00:13:01,630
the current prediction value.

188
00:13:01,630 --> 00:13:09,520
So now my current batch is essentially moving one month into the future so I'll say N.P. append and

189
00:13:09,520 --> 00:13:16,170
this is essentially where the NUM pi indexing gets a little tricky so explain it one more time for you.

190
00:13:16,860 --> 00:13:24,600
So it's a three dimensional object where we have batch size by length by number of features the batch

191
00:13:24,600 --> 00:13:25,680
size is actually changing.

192
00:13:25,680 --> 00:13:30,960
So we'll say grab everything from the entire batch because it's just one time series comma and then

193
00:13:30,960 --> 00:13:37,530
for the length I want to say grab everything but index zero because that's essentially what I'm dropping.

194
00:13:37,530 --> 00:13:41,640
So I say one colon and then the number of features hasn't changed.

195
00:13:41,640 --> 00:13:43,580
So just grab everything there.

196
00:13:43,920 --> 00:13:53,830
And then what I'm going to do here is I want to append my current prediction so say current prediction.

197
00:13:54,290 --> 00:13:59,030
And the reason I pass it with these double set of brackets is to make sure the dimensions make sense

198
00:13:59,330 --> 00:14:06,920
for where I'm actually inserting it and I need to insert it in index one here of my shape.

199
00:14:06,920 --> 00:14:11,260
So we'll say axes is equal to 1.

200
00:14:11,300 --> 00:14:11,700
OK.

201
00:14:11,780 --> 00:14:16,880
So this part of the hardest line to understand but really it's just reaching into the NUM pi array and

202
00:14:16,880 --> 00:14:22,100
make sure that the dimensions lineup we're not editing the batch we're not adding the features or editing

203
00:14:22,250 --> 00:14:28,340
the shape at index 1 which is this guy right here and we say 1 colon because I drop that very first

204
00:14:28,370 --> 00:14:34,050
item in my true train data and then I append then my current prediction.

205
00:14:34,100 --> 00:14:34,280
All right.

206
00:14:34,350 --> 00:14:38,630
So that's my current batch and that's actually it for the loop so we'll go in and delete these spare

207
00:14:38,630 --> 00:14:44,690
lines and let's run this make sure I didn't commit any typos and whips we for actually forgot to important

208
00:14:44,720 --> 00:14:45,120
umpire.

209
00:14:45,390 --> 00:14:46,440
So we'll do that as well.

210
00:14:46,440 --> 00:14:50,860
Important I'm pi as MP now we'll run it and there we have it.

211
00:14:50,910 --> 00:14:54,900
Okay so we can see your test predictions.

212
00:14:55,020 --> 00:14:58,260
Recall there a text our test predictions are still scaled.

213
00:14:58,350 --> 00:15:00,480
So we need to inverse transform them.

214
00:15:00,480 --> 00:15:02,000
So we'll go ahead and do that.

215
00:15:02,190 --> 00:15:10,920
We'll say scalar that inverse transform our test predictions and then I'll call these my true predictions

216
00:15:12,920 --> 00:15:14,480
so they are my true predictions.

217
00:15:14,540 --> 00:15:23,020
And let's append that to our original test data frame by creating a new column called predictions is

218
00:15:23,020 --> 00:15:26,020
equal to true predictions.

219
00:15:26,020 --> 00:15:27,680
You may get a warning here again.

220
00:15:27,700 --> 00:15:28,750
You can ignore it.

221
00:15:29,230 --> 00:15:37,450
So if we insert one more cell below and then take a look at test I now have my true production numbers

222
00:15:37,540 --> 00:15:39,030
against my predictions.

223
00:15:39,060 --> 00:15:41,490
Okay so my predictions.

224
00:15:41,500 --> 00:15:45,940
It looks like more or less they're getting the same trend but these look like they're pretty noisy.

225
00:15:45,970 --> 00:15:48,300
So let's see how well we actually performed here.

226
00:15:48,340 --> 00:15:53,230
So we'll come back will plot our true predictions versus our true test values here.

227
00:15:53,260 --> 00:16:01,620
So we'll say test the plot run that and it looks like our predictions are not really matching up that

228
00:16:01,620 --> 00:16:03,780
well to our production.

229
00:16:03,780 --> 00:16:07,360
So we also want to calculate our route mean squared error.

230
00:16:07,440 --> 00:16:08,250
Let's do this.

231
00:16:08,250 --> 00:16:09,870
So how do how can we actually do this.

232
00:16:09,870 --> 00:16:20,730
We simply say from S.K. learn that metrics import mean squared error and then you can say the mean squared

233
00:16:20,730 --> 00:16:21,840
error.

234
00:16:21,840 --> 00:16:30,840
Compare your production versus your predictions.

235
00:16:30,870 --> 00:16:31,920
That's the mean square there.

236
00:16:31,920 --> 00:16:38,130
So the route means quarter would be NDP obscurity run that.

237
00:16:38,190 --> 00:16:43,530
And so right now I mean square is twenty nine units and you can check out the units by coming up here

238
00:16:43,530 --> 00:16:46,270
and seeing what the actual units are in.

239
00:16:46,460 --> 00:16:47,240
OK.

240
00:16:47,280 --> 00:16:53,370
So is this a good enough model in our case just visually we can see that the model's not really good.

241
00:16:53,370 --> 00:16:56,460
So what can we begin to do to try to fix this model.

242
00:16:56,460 --> 00:16:58,280
Well there's lots of different options.

243
00:16:58,290 --> 00:17:05,670
If we come back up here to edit our model itself so won't thing we can do is we can edit the activation

244
00:17:05,670 --> 00:17:11,550
function in other thing we can do is edit the amount of Ellis time units so maybe we the side page one

245
00:17:11,550 --> 00:17:16,980
hundred units isn't enough to basically grab this behavior especially since we're trying to predict

246
00:17:17,520 --> 00:17:19,350
24 months to the future.

247
00:17:19,350 --> 00:17:21,030
Now that's another thing can edit.

248
00:17:21,030 --> 00:17:26,520
So coming back up here maybe using an entire set of twenty four is just too much.

249
00:17:26,580 --> 00:17:29,040
It's trying to predict two years into the future each time.

250
00:17:29,130 --> 00:17:33,570
So maybe decide to change that to 18 for a test size and that's what's really nice about using these

251
00:17:33,570 --> 00:17:38,880
variables is you can easily start playing around to your data so you decide I'll change my test size

252
00:17:38,880 --> 00:17:47,430
to 18 and then to make sure that's correct when I'm actually choosing my validation generator.

253
00:17:47,430 --> 00:17:54,390
I'll have to edit this because right now if we take a look at the size of the test set it's also equal

254
00:17:54,390 --> 00:17:54,960
to 18.

255
00:17:54,990 --> 00:17:57,760
So maybe I just choose 12 as my length here.

256
00:17:57,840 --> 00:18:04,950
And so what I'm going to do is I set my test size the last 18 months and my batch size is now twelve

257
00:18:04,950 --> 00:18:05,700
months.

258
00:18:05,730 --> 00:18:10,110
And so what I can do is I can start rerunning all these and maybe add in some more Ellis time units

259
00:18:10,110 --> 00:18:11,010
if I think that will help.

260
00:18:11,430 --> 00:18:13,560
So maybe add one to 50.

261
00:18:13,560 --> 00:18:16,410
Keep in mind too many neurons you're going to be over to the data.

262
00:18:16,740 --> 00:18:20,850
So there's another value can play around with maybe 150 is too much et cetera.

263
00:18:20,850 --> 00:18:23,910
So let's just go ahead and see if those things fix it.

264
00:18:23,910 --> 00:18:28,650
Again this isn't part of the actual exercise this is just me exploring further and trying to show you

265
00:18:28,650 --> 00:18:29,760
what you do in real life.

266
00:18:29,760 --> 00:18:33,600
The thing is you can play around with an edit to try to improve your performance.

267
00:18:33,600 --> 00:18:39,750
And luckily we already have this metric that I can improve on so I can see visually and my grabbing

268
00:18:39,750 --> 00:18:40,470
that seasonality.

269
00:18:40,470 --> 00:18:44,320
It looks like the seasonality is being obtained to some degree.

270
00:18:44,640 --> 00:18:49,830
And now what I want to make sure is that I obtain that seasonality behavior and try to reduce this number.

271
00:18:49,830 --> 00:18:53,360
So a perfect fit would have zero here as a root means squared error.

272
00:18:53,520 --> 00:18:54,730
So let's do this.

273
00:18:54,780 --> 00:19:01,020
I'm going to basically run all these changes so I'll change my test size to something smaller essentially

274
00:19:01,320 --> 00:19:08,750
allowing my model to have more data to train on and then we'll scale everything the time series generator.

275
00:19:08,760 --> 00:19:14,070
What I'm going to do is I set my batch now just to one year and this is pretty much the absolute smallest

276
00:19:14,070 --> 00:19:14,980
I can make this.

277
00:19:15,030 --> 00:19:19,860
So we'll go ahead and keep it as 12 if I try to make it smaller that I'm not grabbing really that full

278
00:19:19,920 --> 00:19:21,600
year information of seasonality.

279
00:19:21,780 --> 00:19:22,900
We'll recreate our model.

280
00:19:23,040 --> 00:19:24,820
See if adding more neurons helps.

281
00:19:24,900 --> 00:19:30,600
Notice that curious actually updating and saying hey this is the next model you're creating redefine

282
00:19:30,600 --> 00:19:36,320
our validation generator we still have our early stopping and let's go ahead and try this again and

283
00:19:36,320 --> 00:19:37,410
see if this works.

284
00:19:37,790 --> 00:19:41,970
So I'll fast forward on time until the second attempt works.

285
00:19:42,050 --> 00:19:42,350
OK.

286
00:19:42,380 --> 00:19:46,940
So what's interesting here is even if more neurons that only needed three epochs of fit said something

287
00:19:46,940 --> 00:19:47,700
that's interesting.

288
00:19:47,900 --> 00:19:49,550
Let's go ahead and plot out the loss.

289
00:19:49,640 --> 00:19:51,770
Check out that behavior.

290
00:19:51,770 --> 00:19:52,670
And here we have it.

291
00:19:52,700 --> 00:19:57,380
So there's our validation loss and may even be worth trying to increase that patients because it's not

292
00:19:57,380 --> 00:20:01,810
clear to me that that validation loss was going to keep going down or keep going up.

293
00:20:01,820 --> 00:20:03,330
So that's something you can also play around with.

294
00:20:03,350 --> 00:20:05,920
Maybe we're actually not training for enough epochs on this.

295
00:20:05,940 --> 00:20:07,890
So now it's time to evaluate our test data.

296
00:20:07,970 --> 00:20:14,150
We'll go and just run the same series and we can see here production predictions.

297
00:20:14,210 --> 00:20:19,100
Notice the actual values have not changed because now my entire test length isn't as large.

298
00:20:19,100 --> 00:20:22,140
I'm only doing 18 months instead of 24 months.

299
00:20:22,160 --> 00:20:27,530
And now let's go ahead and do the test plot and now it looks like we're fitting much better to the data

300
00:20:28,220 --> 00:20:29,530
still not definitely perfect.

301
00:20:29,540 --> 00:20:32,860
We can't actually grab that little spike trend that looks to be repeating.

302
00:20:32,960 --> 00:20:34,490
It's kind of too smooth right now.

303
00:20:34,640 --> 00:20:35,670
But let's go into Keckley.

304
00:20:35,720 --> 00:20:37,480
Our means squared error.

305
00:20:37,490 --> 00:20:38,360
I'll rerun these cells.

306
00:20:38,360 --> 00:20:40,530
Keep in mind our previous value was twenty nine.

307
00:20:40,610 --> 00:20:43,970
So if we get lower we improved on it and now we get seven.

308
00:20:44,000 --> 00:20:45,650
So a major improvement.

309
00:20:45,770 --> 00:20:53,030
Again the things that I changed were I gave my data more or excuse me I gave my model more data to train

310
00:20:53,030 --> 00:20:59,930
on by decreasing the length of that test set and I gave the model more neurons to have available to

311
00:20:59,930 --> 00:21:00,020
it.

312
00:21:00,020 --> 00:21:05,270
So at 150 another thing we can do is kind of retrain everything but see if we give it more patients

313
00:21:05,300 --> 00:21:10,460
because it's kind of surprising that with three epochs that's all I need for training given the sort

314
00:21:10,460 --> 00:21:11,510
of lost chart.

315
00:21:11,600 --> 00:21:16,130
Now maybe the case that validation loss ends up spiking beyond this but it's worth actually exploring

316
00:21:16,130 --> 00:21:17,630
that with more patients.

317
00:21:17,630 --> 00:21:22,700
Main thing I want to clarify for you is there's still a lot more you can do to improve your models.

318
00:21:22,730 --> 00:21:28,250
So if you get kind of a crappy fit the first time around you can always play around with a lot more

319
00:21:28,460 --> 00:21:33,200
such as the length of the test set the number of neurons in your Elyse Tam layer the patients in your

320
00:21:33,200 --> 00:21:38,870
early stopping et cetera to see how you can continually improve your fit and for time series information.

321
00:21:38,900 --> 00:21:44,620
I would always encourage you to do both a visual check and a quantitative check such as Ruby square.

322
00:21:44,720 --> 00:21:45,710
I hope that helps you.

323
00:21:45,890 --> 00:21:48,560
And I will see you in the next section of the course.
