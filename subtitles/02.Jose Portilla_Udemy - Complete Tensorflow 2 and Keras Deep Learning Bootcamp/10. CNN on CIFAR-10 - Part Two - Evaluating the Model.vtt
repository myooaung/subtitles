WEBVTT
1
00:00:05.370 --> 00:00:06.570
Welcome back everyone.

2
00:00:06.570 --> 00:00:11.520
In the previous lecture we went ahead and read in the data pre processed it and then we train our model.

3
00:00:11.520 --> 00:00:13.410
Let's go ahead and evaluate our model.

4
00:00:13.430 --> 00:00:16.430
I'm gonna head back to that Uber notebook where we left off last time.

5
00:00:16.590 --> 00:00:16.860
All right.

6
00:00:16.860 --> 00:00:18.050
We just train our model.

7
00:00:18.060 --> 00:00:23.680
Let's go ahead and read in those metrics are those losses based off the model history by saying PD.

8
00:00:23.690 --> 00:00:32.160
That data frame and call it on Model history that history and we're going to do here is we could just

9
00:00:32.160 --> 00:00:37.380
check out the metrics themselves by running it as such but it's probably more thing to plot them out

10
00:00:38.270 --> 00:00:39.960
let's say metrics.

11
00:00:39.970 --> 00:00:43.070
It's called columns real quick to save ourselves a little bit of typing.

12
00:00:43.260 --> 00:00:49.720
We'll say metrics on and let's go ahead and grab you can grab all of them and then just delete some

13
00:00:49.720 --> 00:00:58.060
of these so we'll paste that in and we'll compare accuracy versus validation accuracy and let's go ahead

14
00:00:58.060 --> 00:01:00.850
and plot that out.

15
00:01:00.990 --> 00:01:01.610
OK.

16
00:01:01.670 --> 00:01:06.440
So it looks like the training actually continues to rise and the validation accuracy it looks like it

17
00:01:06.440 --> 00:01:08.650
was also continuing to go up a little bit flattening out.

18
00:01:08.930 --> 00:01:12.500
But remember we indicated that we wanted to stop based off the loss.

19
00:01:12.530 --> 00:01:19.520
So that's going to be more informative of why we stopped training early so we'll say loss compared to

20
00:01:20.810 --> 00:01:23.090
validation loss.

21
00:01:23.170 --> 00:01:24.650
Go ahead and plot that out.

22
00:01:25.120 --> 00:01:30.250
And here's a really indicative chart so we can see the training loss going down but we weren't getting

23
00:01:30.250 --> 00:01:33.000
any improvement on the validation loss.

24
00:01:33.010 --> 00:01:34.650
And in fact it looks like it were starting to go up.

25
00:01:34.900 --> 00:01:38.980
So probably good idea to start ending training at around 70 epochs.

26
00:01:39.010 --> 00:01:46.060
As always we can get the final evaluation metrics either by just looking at this last row in our data

27
00:01:46.060 --> 00:01:57.040
frame or by saying model evaluate and we could evaluate on X test y cat test save or both is equal to

28
00:01:57.040 --> 00:01:59.400
zero so you don't see a big printing output.

29
00:02:00.090 --> 00:02:02.750
And if you run that it basically shows you back the same results.

30
00:02:02.770 --> 00:02:05.560
So zero point nine eight five come back up here.

31
00:02:05.590 --> 00:02:07.420
Notice zero point ninety five.

32
00:02:07.420 --> 00:02:11.050
And then you have zero point six nine and there's zero point six nine.

33
00:02:11.080 --> 00:02:13.720
So same results that are in the actual history.

34
00:02:13.840 --> 00:02:21.730
Let's create a classification report and confusion matrix to further evaluate this scalar metrics in

35
00:02:21.730 --> 00:02:30.020
import classification report confusion matrix it's actually get our predictions.

36
00:02:30.110 --> 00:02:41.260
So that's model predict classes of X test and then we'll print out a classification report comparing

37
00:02:41.920 --> 00:02:44.530
our true known values from my test.

38
00:02:44.620 --> 00:02:46.660
In this case we don't pass on the categorical.

39
00:02:46.660 --> 00:02:51.250
We just need the actual numbers versus our predictions.

40
00:02:51.250 --> 00:02:56.950
Print that out and we can see our precision recall an f1 score as well as our overall accuracy.

41
00:02:56.950 --> 00:03:00.210
So the things we want to think about is how good of an accuracy is.

42
00:03:00.210 --> 00:03:02.320
Sixty nine percent accuracy.

43
00:03:02.350 --> 00:03:07.480
Well the first thing you compare it to is just random guessing and because we have 10 classes here a

44
00:03:07.480 --> 00:03:10.610
random guess has a 10 percent chance of being correct.

45
00:03:10.630 --> 00:03:15.490
So we're getting sixty nine percent accuracy overall which means our models actually performing quite

46
00:03:15.490 --> 00:03:17.310
well given the circumstances.

47
00:03:17.350 --> 00:03:22.840
And here what's really nice about this classification report is I can take a look at what classes it's

48
00:03:22.840 --> 00:03:27.830
actually not performing that well on you can see it tends to perform poorly on class number three.

49
00:03:27.850 --> 00:03:31.780
And we can take a look at this and see 0 1 2 3.

50
00:03:31.840 --> 00:03:34.120
So it looks like it has trouble with cats.

51
00:03:34.240 --> 00:03:38.010
And if we take a look at if we have three here that's four five.

52
00:03:38.020 --> 00:03:39.190
We have dogs.

53
00:03:39.190 --> 00:03:40.110
Take a look back here.

54
00:03:40.110 --> 00:03:42.220
You also know doesn't do that great on dogs.

55
00:03:42.230 --> 00:03:44.730
In fact they perform very similar to each other.

56
00:03:44.860 --> 00:03:48.820
And that's because when you blur an image so much that it's only 32 by 32.

57
00:03:48.970 --> 00:03:51.100
Cats and dogs can look really similar to a computer.

58
00:03:51.100 --> 00:03:53.590
So we actually would expect it to perform quite poorly.

59
00:03:53.590 --> 00:03:58.870
Here you can see it performs quite well on class number one which is an automobile which apply the most

60
00:03:58.870 --> 00:04:01.000
distinctive out of all these how reach.

61
00:04:01.010 --> 00:04:02.540
Notice there's also trucks.

62
00:04:02.590 --> 00:04:07.360
So the fact that it can perform automobiles so well given that there's already a truck category which

63
00:04:07.360 --> 00:04:11.990
is quite similar to automobiles it's pretty impressive so we'll come back here.

64
00:04:12.030 --> 00:04:20.340
Do you think can do as before is kind of you the actual predictions themselves how many you got right.

65
00:04:20.360 --> 00:04:29.550
What it's actually mis classifying things as and as always you can say import seaborne as S.A. and then

66
00:04:29.550 --> 00:04:40.280
say S.A. heat map on this confusion matrix pie make it a little larger will say BLT figure fig size

67
00:04:40.340 --> 00:04:44.720
is equal to something like let's go ahead and say 10 by 6 run that.

68
00:04:44.930 --> 00:04:50.000
And here what's kind of interesting is based off the color you can quickly see what it's misclassified

69
00:04:50.000 --> 00:04:50.360
things.

70
00:04:50.390 --> 00:04:52.900
So it looks like there is confusion between 3 and 5.

71
00:04:53.480 --> 00:04:59.710
And we can get an annotation to see the actual number of misclassification here.

72
00:04:59.840 --> 00:05:03.050
So you can see based off the color there is confusion between 3 and 5.

73
00:05:03.140 --> 00:05:08.210
But come back here that was 0 1 2 3 cats versus dogs.

74
00:05:08.210 --> 00:05:12.530
So it has trouble with these kind of small furry animals make sense they're quite similar and a 32 by

75
00:05:12.530 --> 00:05:13.330
32 image.

76
00:05:13.670 --> 00:05:17.750
But let's go ahead and see how it performs on just a single image.

77
00:05:17.840 --> 00:05:23.270
So if we wanted to predict the single image we would just before grab a single image.

78
00:05:23.510 --> 00:05:25.280
So someone returns back.

79
00:05:25.490 --> 00:05:29.500
Here's my image data and recall this data has already been scaled.

80
00:05:29.500 --> 00:05:33.560
So if they gave you a raw image you'd have to scale it by saying divide by two fifty five.

81
00:05:33.920 --> 00:05:39.460
But what I can do here say guilty go ahead and show my image.

82
00:05:39.460 --> 00:05:41.020
So here's my image.

83
00:05:41.080 --> 00:05:42.010
I'm actually not sure what this is.

84
00:05:42.040 --> 00:05:48.200
So we have to say why test at zero and that's three.

85
00:05:48.200 --> 00:05:50.020
So that should be a cat.

86
00:05:50.030 --> 00:05:51.690
We'll see how well it does.

87
00:05:51.840 --> 00:05:58.930
Let's go to say model predict classes and thing is the only thing we have to make sure it was reshape

88
00:05:58.930 --> 00:05:59.890
this correctly.

89
00:05:59.890 --> 00:06:04.480
So it's one image 32 by 32 with three color channels.

90
00:06:04.480 --> 00:06:05.240
Go ahead and run that.

91
00:06:05.290 --> 00:06:06.730
And it looks like it predicted it quite well.

92
00:06:06.730 --> 00:06:10.000
So that's kind of surprising because I myself wouldn't be able to predict this.

93
00:06:10.330 --> 00:06:10.990
OK.

94
00:06:11.020 --> 00:06:17.800
And I think the one we show in the actual data set or these solutions notebook is image number 16 which

95
00:06:17.800 --> 00:06:19.890
is much clearer that it's a dog here.

96
00:06:20.110 --> 00:06:23.950
And then we can run that against white tests or 16.

97
00:06:23.950 --> 00:06:26.410
So that's a dog for the classes.

98
00:06:26.410 --> 00:06:27.540
It also performs correctly.

99
00:06:27.550 --> 00:06:31.200
So a pretty good performance considering the circumstances here.

100
00:06:31.300 --> 00:06:35.340
Well we're going to cover next is how to deal with quote unquote real image data.

101
00:06:35.440 --> 00:06:40.210
So typically your images you're not gonna be loading them with just a simple Kerry's call.

102
00:06:40.240 --> 00:06:44.830
Instead you're probably going to be loading them from a folder a directory on your computer that has

103
00:06:44.830 --> 00:06:48.640
some sort of J peg or P G files in the next lecture.

104
00:06:48.640 --> 00:06:52.990
We're going to show you how to download the dataset that we work with and then the next subsequent lectures

105
00:06:53.020 --> 00:06:59.800
will show you how to use Kerry's built in image generator and flow from directory functionalities to

106
00:06:59.800 --> 00:07:03.130
actually work with real j peg and P G files.

107
00:07:03.130 --> 00:07:04.990
Thanks and we'll see you at the next lecture.
