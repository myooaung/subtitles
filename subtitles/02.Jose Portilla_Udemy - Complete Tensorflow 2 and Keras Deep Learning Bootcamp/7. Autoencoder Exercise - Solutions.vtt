WEBVTT
1
00:00:05.300 --> 00:00:06.260
Welcome back everyone.

2
00:00:06.290 --> 00:00:11.040
In this lecture we're going to go over an example set of solutions for the auto encoders exercise.

3
00:00:11.090 --> 00:00:11.690
Let's get started.

4
00:00:12.610 --> 00:00:12.890
OK.

5
00:00:12.900 --> 00:00:15.670
Here we are at the auto encoder exercise notebook.

6
00:00:15.670 --> 00:00:21.270
We went ahead and ran the imports here and then we'll read in the data by running that cell.

7
00:00:21.760 --> 00:00:23.670
Let's go ahead and display the data frame.

8
00:00:23.680 --> 00:00:25.290
Just say data to show that.

9
00:00:25.720 --> 00:00:30.810
And here we can see the various food categories as well as the four countries.

10
00:00:30.820 --> 00:00:33.490
Next task was to transpose the data frame.

11
00:00:33.550 --> 00:00:39.520
So if you just Google search data frame transpose plus pandas or if you look rather documentation you'll

12
00:00:39.520 --> 00:00:41.940
realize there's a transpose method.

13
00:00:41.980 --> 00:00:44.920
And we previously discussed this when talking about the scribe.

14
00:00:44.920 --> 00:00:49.150
So if you run that they'll basically kind of rotate or transpose the data frame.

15
00:00:49.150 --> 00:00:52.210
So now we have these four countries as my rows.

16
00:00:52.210 --> 00:00:53.430
So I had this nice little heat map.

17
00:00:53.470 --> 00:00:58.270
However it's pretty hard to actually distinguish the different countries from each other.

18
00:00:58.270 --> 00:01:04.390
So let's go ahead and complete this task in order to actually plot out this heat map and we can do this

19
00:01:04.390 --> 00:01:10.780
a variety of ways but essentially we're just going to call essence the heat map and you can either call

20
00:01:10.780 --> 00:01:14.590
it on the original data frame or the transposed data frame.

21
00:01:14.770 --> 00:01:17.640
If you just run S.A. heat map you'll get kind of a smaller figure.

22
00:01:18.010 --> 00:01:26.620
So you can always enlarge it by saying Pulte figure fig size is equal to and then say something a little

23
00:01:26.620 --> 00:01:32.320
larger because there's something like five by four or 10 by eight etc. Actually I think 10 by eight

24
00:01:32.320 --> 00:01:33.550
will make it larger here.

25
00:01:33.640 --> 00:01:34.550
It's going to run that.

26
00:01:35.620 --> 00:01:36.490
And there we go.

27
00:01:36.490 --> 00:01:44.710
So you can either run it this way or as discussed you can run the transposed way ADF transpose and see

28
00:01:44.710 --> 00:01:50.170
it as such but either way it's a little hard to distinguish the differences between these four countries

29
00:01:50.170 --> 00:01:51.600
just through a heat map.

30
00:01:51.670 --> 00:01:57.330
So we're going to do is we'll go ahead and run the imports necessary to build out our auto encoder.

31
00:01:57.400 --> 00:02:06.820
So let's first say import tensor flow or we'll say from tensor flow that carries top models import a

32
00:02:06.820 --> 00:02:13.750
sequential model and then from tensor flow that carries the layers import dense.

33
00:02:14.440 --> 00:02:20.000
And if you want to use the stochastic gradient descent optimization you can import that as well.

34
00:02:20.260 --> 00:02:22.850
We'll go ahead and just use atom as our optimizer.

35
00:02:23.230 --> 00:02:30.190
So let's go ahead and and we'll go ahead and use stochastic gradient descent as our optimizer and to

36
00:02:30.190 --> 00:02:36.920
use that will simply say from tensor flow that curious cares to optimize ours import as she.

37
00:02:37.090 --> 00:02:38.590
Go ahead and run that.

38
00:02:38.590 --> 00:02:39.920
And so now we have our imports.

39
00:02:39.940 --> 00:02:42.970
Let's go ahead and create the encoder lots of ways.

40
00:02:42.970 --> 00:02:49.540
Good on this but here I'll just say encoder is equal to an instance of sequential and we'll follow what

41
00:02:49.540 --> 00:02:55.760
the task 7 which is a dense layer starting with eight neurons as a unit.

42
00:02:55.870 --> 00:03:01.870
Then the activation function of rectified linear unit and then we have to specify here the input shape

43
00:03:03.580 --> 00:03:05.710
is equal to starting at 17.

44
00:03:05.740 --> 00:03:12.060
So you go from 17 straight to eight and then next here for the next layer we'll go from that eight.

45
00:03:13.030 --> 00:03:19.150
So add another then Slayer down from that eight to the four activation function equal to rectified linear

46
00:03:19.150 --> 00:03:25.840
unit and it's up to you if you want you can continue specifying the input shape from before but if you

47
00:03:25.840 --> 00:03:29.140
just stack these right on top of each other you don't necessarily have to.

48
00:03:29.350 --> 00:03:34.810
So optionally you can keep specifying input shape like this from the previous layer just to make sure

49
00:03:35.020 --> 00:03:40.660
you keep your layers in order kind of in your own mind but really it's not necessary because we're adding

50
00:03:40.660 --> 00:03:43.970
it in order right after the previous layer.

51
00:03:44.050 --> 00:03:47.120
So say encoder and then finally add in dance.

52
00:03:47.170 --> 00:03:52.650
Let's go ahead bring it down the two units and an activation function of rectified layer unit.

53
00:03:53.490 --> 00:03:54.010
OK.

54
00:03:54.130 --> 00:03:55.180
So there's our encoder.

55
00:03:55.240 --> 00:03:59.140
Next we're gonna create the decoder essentially operates the exact opposite.

56
00:03:59.650 --> 00:04:06.450
So we'll say a decoder is equal to the sequential model and then we'll add in those dense layers will

57
00:04:06.460 --> 00:04:08.680
say dance units here.

58
00:04:08.680 --> 00:04:16.180
It's gonna go starting at 4 with the activation function for a rectified linear unit taking in our input

59
00:04:16.180 --> 00:04:23.920
shape as to so we're call that this too right here is the final layer of the encoder and it's kind of

60
00:04:23.920 --> 00:04:31.750
the middle hidden layer throughout the entire auto encoder and then we'll add in basically in reverse

61
00:04:32.380 --> 00:04:33.400
what the encoder is.

62
00:04:33.400 --> 00:04:41.560
So then units here is going to be equal to 8 activation is equal to rectified linear unit and then he

63
00:04:41.560 --> 00:04:55.350
will say decoder add will add in dance and here we're gonna expand it back out to the final 17 neurons.

64
00:04:55.530 --> 00:05:05.040
Same as before activation function equal to rectified linear unit and then what we'll do here is complete

65
00:05:05.040 --> 00:05:06.640
that decoder.

66
00:05:06.750 --> 00:05:15.150
Then after that we're going to be doing is will simply say create the auto encoder combination which

67
00:05:15.150 --> 00:05:23.610
is auto encoder is equal to sequential and then encoder with decoder

68
00:05:26.220 --> 00:05:31.130
then beyond that will say auto encoder and we'll compile it.

69
00:05:31.380 --> 00:05:34.890
The loss here because it's continuous value should be mean squared error.

70
00:05:35.020 --> 00:05:40.250
And let's go ahead and say our optimizer is equal to stochastic gradient descent and we'll start up

71
00:05:40.260 --> 00:05:42.990
the learning rate of one point five.

72
00:05:42.990 --> 00:05:44.370
Go ahead and have that.

73
00:05:44.610 --> 00:05:50.610
Then we'll create our min max scalar to scale the data and we're going to do is make sure we transpose

74
00:05:50.610 --> 00:05:55.110
the data since really what we have here are 14 feature columns and only four rows.

75
00:05:55.110 --> 00:05:56.760
So one per country.

76
00:05:56.760 --> 00:06:01.260
So to do this we'll say from S.K. learn not pre processing.

77
00:06:01.410 --> 00:06:11.850
Go ahead and import min max scalar create an instance of our scalar a scalar is equal to men Max scalar

78
00:06:12.900 --> 00:06:20.130
create the scaled version of our data frame so say scaled the f is equal to scalar and we'll simply

79
00:06:20.130 --> 00:06:21.900
just fit transform on everything.

80
00:06:21.930 --> 00:06:22.370
So fit.

81
00:06:22.370 --> 00:06:29.190
Transform except recall here will say fit transform on transpose and to make sure it's an empire it

82
00:06:29.190 --> 00:06:31.130
will say values after that.

83
00:06:31.320 --> 00:06:39.840
So then note here the shape of our scaled D F The asked for the shape here it's for rose by 17 features

84
00:06:40.280 --> 00:06:44.920
not the time to fit the auto encoder to the scale data for fifteen epochs.

85
00:06:45.090 --> 00:06:54.810
Go ahead and do that will say auto encoder fit scaled the F scale D F epochs go ahead and run it for

86
00:06:54.810 --> 00:07:01.470
fifteen run that should be very fast because it's extremely small data set and then next we'll run the

87
00:07:01.470 --> 00:07:07.440
scale data through only the encoder and predict the reduced dimensionality output.

88
00:07:07.650 --> 00:07:13.590
So in that case just as we did in the very first couple lecturer we said encoded two dimensional version

89
00:07:13.680 --> 00:07:21.060
is equal to encoder that predict scaled data frame.

90
00:07:21.060 --> 00:07:25.260
So after running that encoded to that marginality you should get probably something that looks like

91
00:07:25.260 --> 00:07:31.170
this on the pending on how many epochs you ran for and your particular random initialization you may

92
00:07:31.170 --> 00:07:33.620
actually see an entire column B zeros.

93
00:07:34.020 --> 00:07:40.410
So keep that in mind let's go ahead and join the encoded two dimensional data of the original countries

94
00:07:40.470 --> 00:07:41.490
index.

95
00:07:41.490 --> 00:07:44.570
So when all we can do that is by simply saying the following.

96
00:07:44.730 --> 00:07:51.840
Recall that if I say DIA transpose that index is the same as grabbing the actual columns.

97
00:07:51.840 --> 00:07:54.320
So have England Wales Scotland and Northern Ireland.

98
00:07:54.840 --> 00:08:02.010
So I'll say my final results are equal to P the data frame where my data is the encoded 2 dimensional

99
00:08:02.010 --> 00:08:13.740
version my index is equal to the F transpose that index and then finally lets say my columns are C1

100
00:08:13.800 --> 00:08:14.410
and C2.

101
00:08:14.420 --> 00:08:18.860
So two subcomponents that are from the very middle hidden layer.

102
00:08:19.200 --> 00:08:21.700
So C want to see two those are just generic names I chose.

103
00:08:22.380 --> 00:08:28.260
OK so if I take a look at my results I have this data frame and what I'm going to do is I'm going to

104
00:08:28.260 --> 00:08:30.410
say results is equal to results.

105
00:08:30.750 --> 00:08:38.520
Resets index in order to have something that looks like this.

106
00:08:38.520 --> 00:08:45.240
Which means I can actually now plot C1 C2 and then labeled by index if a scatter plot so I can do that

107
00:08:45.240 --> 00:08:54.860
we'll see more to say seaborne scatter plot x equal to see one y equal to C2.

108
00:08:54.930 --> 00:09:02.010
That's the reduced dimensionality we'll say theta is equal to results and let's say Hugh is equal to

109
00:09:02.040 --> 00:09:11.670
index run that and here we can see that Northern Ireland very different from England Wales and Scotland

110
00:09:11.700 --> 00:09:16.160
and we can see Wales is also quite different from England and Scotland.

111
00:09:16.170 --> 00:09:20.820
So you should've gotten something that's pretty similar behaviour essentially showing you that Northern

112
00:09:20.820 --> 00:09:22.830
Ireland quite different from the other three.

113
00:09:23.040 --> 00:09:28.410
And if you got differentiation between the remaining three as well that's OK as well which you should

114
00:09:28.410 --> 00:09:34.710
notice is the two most similar ones on your plot should hopefully have been England and Scotland.

115
00:09:34.740 --> 00:09:39.750
So two closest points on your plot should be England Scotland and then the furthest away from that from

116
00:09:39.750 --> 00:09:43.310
the other three should be Northern Ireland depending on your auto encoder.

117
00:09:43.350 --> 00:09:49.070
Wales might have been differentiated further or closer to England and Scotland.

118
00:09:49.110 --> 00:09:53.680
OK so keep mine mind once you actually go back and look at the data and table it makes sense.

119
00:09:53.730 --> 00:09:58.500
If you go back to the data table you'll notice that the Northern Irish actually eat way more grams of

120
00:09:58.500 --> 00:10:04.050
fresh potatoes and far fewer grams of things like fresh fruits cheese fish and alcoholic drink.

121
00:10:04.050 --> 00:10:08.650
So things that are actually difficult to interpret just by looking at the raw data.

122
00:10:08.760 --> 00:10:14.790
So it's a good sign that the structure we visualise actually reflects a big fact and real world geography

123
00:10:15.060 --> 00:10:18.740
since Northern Ireland is the only of those four countries.

124
00:10:18.740 --> 00:10:21.270
It's not on the actual island of Great Britain.

125
00:10:21.270 --> 00:10:21.610
Okay.

126
00:10:21.660 --> 00:10:26.010
And we have a little link to this video in case you're confused over the differences between England

127
00:10:26.010 --> 00:10:27.660
UK and Great Britain.

128
00:10:27.660 --> 00:10:28.160
All right.

129
00:10:28.380 --> 00:10:30.240
So that's it for this exercise.

130
00:10:30.270 --> 00:10:34.290
I hope you enjoyed it as a practical application for dimensionality reduction.

131
00:10:34.290 --> 00:10:37.380
With autumn encoders thanks and I'll see you at the next lecture.
