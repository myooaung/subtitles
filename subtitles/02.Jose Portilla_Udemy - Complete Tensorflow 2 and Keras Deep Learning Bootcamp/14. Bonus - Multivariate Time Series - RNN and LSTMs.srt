1
00:00:05,350 --> 00:00:07,540
Hello everyone and welcome back.

2
00:00:07,540 --> 00:00:12,390
We just finished our exercise our current neural networks with time series and the bonus.

3
00:00:12,470 --> 00:00:18,490
I want to have a quick discussion on multivariate time series with Ellis team Recurrent Neural Networks

4
00:00:18,890 --> 00:00:21,160
dizzy often requested topic from students.

5
00:00:21,190 --> 00:00:25,050
So we'll go ahead and discuss it so we have this quick bonus notebook.

6
00:00:25,050 --> 00:00:28,020
It actually comes after the exercise solution.

7
00:00:28,020 --> 00:00:33,000
So we're going to quickly discuss how to use Ellis Tan's recurrent Real Networks to predict multivariate

8
00:00:33,000 --> 00:00:34,080
time series.

9
00:00:34,230 --> 00:00:38,760
Something to keep in mind though is that there's a few cons to using Ellis stamps for this approach.

10
00:00:39,750 --> 00:00:45,450
So as with all neural networks the model is essentially a black box that makes it really difficult to

11
00:00:45,450 --> 00:00:48,310
interpret what's the model actually doing internally.

12
00:00:48,420 --> 00:00:53,910
In order to predict this multivariate time series and that's a problem that extends not just to multivariate

13
00:00:53,910 --> 00:00:59,310
time series but using recurrent neural networks for time series in general it's essentially a black

14
00:00:59,310 --> 00:01:04,890
box so you can't see what factors are important to the network especially for a deeper and larger network

15
00:01:05,260 --> 00:01:09,650
that actually relay information to forecasting into the future.

16
00:01:09,660 --> 00:01:15,690
Now the larger con is time series is actually very well studied field of mathematics and there's many

17
00:01:15,690 --> 00:01:22,080
well studied alternatives that are actually simpler to use such as scream X or seasonal auto aggressive

18
00:01:22,110 --> 00:01:28,420
integrated moving average exoplanets models and also via emacs or Varma X models.

19
00:01:28,440 --> 00:01:33,660
So I would highly encourage people to check out those well studied models first and you can apply those

20
00:01:33,660 --> 00:01:40,020
not just to multivariate time series but also single time series and they work very well and more importantly

21
00:01:40,050 --> 00:01:42,420
they're very interpretable.

22
00:01:42,420 --> 00:01:47,800
So I highly recommend they try those more conventional approaches before settling on Ellis tans or current

23
00:01:47,800 --> 00:01:53,430
narrow networks for multivariate time series data or time series data in general while it can feel very

24
00:01:53,430 --> 00:01:58,410
cool to be using state of the art recurrent annual that works for your time series forecasting again

25
00:01:58,500 --> 00:02:03,000
the cost of not being able to interpret what's going on is a large enough con that you should really

26
00:02:03,030 --> 00:02:06,540
fall back this well studied Rima based models.

27
00:02:06,770 --> 00:02:07,440
OK.

28
00:02:07,740 --> 00:02:12,390
Now fortunately let's say you actually do want to check out recurring your networks for your time series

29
00:02:12,390 --> 00:02:17,850
data and you happen to have multivariate time series so there's actually only two real main changes

30
00:02:17,850 --> 00:02:22,020
you have to do to essentially the notebooks that we've been working with and the general structure we've

31
00:02:22,020 --> 00:02:23,610
been working with.

32
00:02:23,610 --> 00:02:29,430
So for a multivariate time series the two main changes are the first as you may expect you need to change

33
00:02:29,430 --> 00:02:34,010
the input shape in your Elyse Tam layer to now reflect that two dimensional structure.

34
00:02:34,080 --> 00:02:38,580
You may recall previously we had a variable called an underscore feature and we were just saying that

35
00:02:38,670 --> 00:02:43,650
equal to one since we just had a single variable or a single feature in our time series in order to

36
00:02:43,650 --> 00:02:48,690
reflect a multivariate time series will simply change that to however many features we have in that

37
00:02:48,690 --> 00:02:53,670
multivariate time series and then the second main change is that final dense layer.

38
00:02:53,700 --> 00:02:58,470
Previously it was just one neuron because we were just forecasting a single value into the future for

39
00:02:58,470 --> 00:03:03,070
a multivariate time series before forecasting multiple features into the future.

40
00:03:03,090 --> 00:03:05,970
So within one neuron per feature or variable.

41
00:03:05,970 --> 00:03:11,220
So let's go out and explore these changes since essentially everything else stays exactly the same.

42
00:03:11,220 --> 00:03:14,690
We're not actually going to manually program through this bonus notebook.

43
00:03:14,690 --> 00:03:19,620
Instead we'll just go ahead and walk through and point out where the two main changes happen to occur.

44
00:03:19,620 --> 00:03:20,160
OK.

45
00:03:20,220 --> 00:03:22,070
Let's open up that notebook and walk you through it.

46
00:03:22,650 --> 00:03:22,940
OK.

47
00:03:22,950 --> 00:03:29,190
Here we are underneath the Arnolds folder at the very end you should see bonus multivariate Arnon.

48
00:03:29,370 --> 00:03:33,660
And if you don't see it make sure you've downloaded the latest version of our zip file.

49
00:03:33,720 --> 00:03:38,060
Let's go ahead and open up that notebook and basically just walk through the major changes.

50
00:03:38,250 --> 00:03:41,820
So we have a little warning here that basically tells you that you should definitely check out these

51
00:03:41,910 --> 00:03:45,220
arraignment based models before jumping to Ellis team.

52
00:03:45,240 --> 00:03:50,250
But let's imagine that you do have a multivariate time series and you do want to forecast these multiple

53
00:03:50,250 --> 00:03:52,160
wearables into the future.

54
00:03:52,200 --> 00:03:58,560
So we're working with here is a multivariate time series that essentially it's a data set that occurs

55
00:03:58,650 --> 00:04:02,440
of recording every 10 minutes from a wireless sensor network.

56
00:04:02,460 --> 00:04:07,110
So this wireless sensor network is actually recording a bunch of different features and it's every 10

57
00:04:07,110 --> 00:04:09,360
minutes for 4.5 months.

58
00:04:09,570 --> 00:04:14,760
As you can imagine recording something every 10 minutes for that much time four point five months.

59
00:04:14,760 --> 00:04:16,800
It basically creates a huge dataset.

60
00:04:16,890 --> 00:04:22,260
So later on in this notebook we're also going to take part of that dataset because we want to forecast

61
00:04:22,470 --> 00:04:24,270
24 hours into the future.

62
00:04:24,360 --> 00:04:29,730
You don't need 4.5 months of information a 10 minute intervals to learn that behavior.

63
00:04:29,730 --> 00:04:30,660
So we'll go ahead.

64
00:04:31,020 --> 00:04:33,490
We actually have here linked the original source of the dataset.

65
00:04:33,990 --> 00:04:35,360
Let's zoom in a little bit.

66
00:04:35,370 --> 00:04:40,620
Again we're not actually going to code this notebook out manually and so we'll just walk you through

67
00:04:40,620 --> 00:04:40,770
it.

68
00:04:41,340 --> 00:04:43,960
So let's get just a little bit of a better view.

69
00:04:44,040 --> 00:04:47,280
I'm going to toggle the header here so we can really focus on the code.

70
00:04:47,850 --> 00:04:54,570
So first thing first I did my imports and I read in my CSP file I said inferred they time format equal

71
00:04:54,570 --> 00:05:01,590
true so we can actually infer here that not only is this date stamped but it's also has a timestamp.

72
00:05:02,070 --> 00:05:05,040
And then we set the index column to the date there.

73
00:05:05,040 --> 00:05:07,230
So if you run that line you should see something it looks like this.

74
00:05:07,230 --> 00:05:13,510
We have this index called Date and their date time objects date time because not only are they they

75
00:05:13,710 --> 00:05:15,340
can see it's every 10 minutes.

76
00:05:15,360 --> 00:05:18,480
It's a recording of how many appliances are on the lights.

77
00:05:18,480 --> 00:05:23,400
And then these various features and you can check out more information about various features in the

78
00:05:23,400 --> 00:05:25,950
actual description and link here below.

79
00:05:26,040 --> 00:05:32,850
So we have all this variable data that was recorded and what we want to do is theoretically at the end

80
00:05:33,090 --> 00:05:38,100
on a forecast all these multiple variables together so we can see the information here.

81
00:05:38,160 --> 00:05:39,740
Luckily I'm not missing any information.

82
00:05:39,750 --> 00:05:41,210
So Nanos.

83
00:05:41,430 --> 00:05:44,760
And then if you want you can explore the various features themselves.

84
00:05:44,760 --> 00:05:46,720
So here we can see wind speed.

85
00:05:46,740 --> 00:05:48,890
And here we can see appliances etc..

86
00:05:48,900 --> 00:05:56,890
Notice it looks very noisy and the x axis overlaps because these are 10 minute increments for 4.5 months

87
00:05:56,890 --> 00:05:59,460
so there's tons and tons of data points here.

88
00:05:59,520 --> 00:05:59,810
Okay.

89
00:06:00,210 --> 00:06:04,680
So clearly there's some sort of behavior because you can see here there's looks like there's empty spaces

90
00:06:04,680 --> 00:06:09,840
so you can imagine things like occurring during night or occurring on the weekend that could change

91
00:06:09,840 --> 00:06:11,630
some of these cases.

92
00:06:11,640 --> 00:06:14,970
So what we do is we're going to do a train test split just as we did before.

93
00:06:15,360 --> 00:06:21,060
But before that since we're only predicting 24 hours into the future so as noticed here says we just

94
00:06:21,060 --> 00:06:24,380
don't predict 24 hours in the future we don't need three months of data.

95
00:06:24,540 --> 00:06:27,930
So we're going to do is save a little bit of train time and we're just going to select the last month's

96
00:06:27,930 --> 00:06:28,950
data.

97
00:06:28,950 --> 00:06:38,170
So if we take a look at the head here it starts at 2016 01 eleven and then goes to 2016 may 27.

98
00:06:38,190 --> 00:06:43,720
So 05 twenty seven let's just go ahead and say grab the entire month of May and we can do that with

99
00:06:44,010 --> 00:06:50,840
LLC by saying starting at 2016 05 01 and then go all the way to the end of the colon.

100
00:06:51,390 --> 00:06:57,340
So that just grabs essentially the month of May going for the first all the way to a twenty seven optionally

101
00:06:57,360 --> 00:07:01,860
if you want you can use all the data but you'll save some train time by just grabbing that chunk of

102
00:07:01,860 --> 00:07:02,600
the data.

103
00:07:02,880 --> 00:07:07,740
So we may we went ahead and reset our data is that and what else we want to do is you'll notice that

104
00:07:07,770 --> 00:07:11,060
it has really high precision but a lot of this is unnecessary.

105
00:07:11,100 --> 00:07:15,780
So we can see here it has lots of zeros and for such high precision especially after we normalize the

106
00:07:15,780 --> 00:07:17,960
data it's not gonna be helpful to the network.

107
00:07:18,010 --> 00:07:19,320
Essentially it's going to be noise.

108
00:07:19,740 --> 00:07:24,720
So we'll also do as we round off the data to either one decimal point or two decimal points it's really

109
00:07:24,720 --> 00:07:25,260
up to you.

110
00:07:25,650 --> 00:07:32,230
So here we went ahead and said data from the two and the next thing is to figure out well what's in

111
00:07:32,230 --> 00:07:37,540
our batch sizes look like if there's 10 minute increments that this is being recorded.

112
00:07:37,540 --> 00:07:39,520
We know there's 24 hours in a day.

113
00:07:39,580 --> 00:07:41,710
And then there's 60 minutes per hour.

114
00:07:42,130 --> 00:07:47,980
So by that then by 10:00 which means every one hundred and forty four rows is one day of information.

115
00:07:47,980 --> 00:07:51,690
So you can imagine that if I wanted my test range to be two days.

116
00:07:51,760 --> 00:07:55,990
So here we have this little variable that can play around with test days is to that I just multiply

117
00:07:55,990 --> 00:08:00,090
that by 144 to get the actual amount of rows.

118
00:08:00,100 --> 00:08:05,230
So here we have this little test index that represents how many rows back and have to reach in order

119
00:08:05,230 --> 00:08:09,010
to have my test set at the end be equal to this many days.

120
00:08:09,010 --> 00:08:10,960
So two times 144.

121
00:08:11,050 --> 00:08:16,230
That means the last two hundred eighty eight rows represent those last two days of information.

122
00:08:16,270 --> 00:08:22,660
So setting that as my test set what I can do is just to use this integer based location by saying starting

123
00:08:22,660 --> 00:08:28,570
from the beginning colon all the way up to 1 not including negative test index essentially minus those

124
00:08:28,570 --> 00:08:29,640
two hundred eighty eight rows.

125
00:08:30,040 --> 00:08:36,010
As my training set and then the test set is starting from the end and going back negative two hundred

126
00:08:36,000 --> 00:08:38,290
eighty eight rows colon all the way to the end.

127
00:08:38,800 --> 00:08:41,780
So notice that minus sign in the indexing it won't work for you.

128
00:08:41,800 --> 00:08:42,850
Try it the other way round.

129
00:08:43,690 --> 00:08:50,440
OK so now we have our training set starting at May 1st going all the way to here May 25th and the test

130
00:08:50,440 --> 00:08:56,530
set starting at May 25th just 10 minutes later and then going all the way to May 27.

131
00:08:56,590 --> 00:08:57,480
Perfect.

132
00:08:57,550 --> 00:09:00,060
Then we'll go ahead and scalar they do just as we did before.

133
00:09:00,160 --> 00:09:01,660
We only do it on a training data.

134
00:09:01,660 --> 00:09:07,180
We don't want to assume prior knowledge of this test set and then we transform both train and tests

135
00:09:07,180 --> 00:09:10,300
to get the skilled versions the time series generator object.

136
00:09:10,300 --> 00:09:11,560
This as we did before.

137
00:09:11,560 --> 00:09:12,850
Very useful to us.

138
00:09:13,000 --> 00:09:18,130
So important just as we did before and notice here I set my length to 144.

139
00:09:18,250 --> 00:09:21,250
So I want to get that full daily cycle in the length.

140
00:09:21,250 --> 00:09:26,070
So that's a logical choice that size that's still equal to one.

141
00:09:26,110 --> 00:09:31,100
And so this essentially looks the same as before skill train skill train length and batch size.

142
00:09:31,180 --> 00:09:34,930
The only thing in play around a fear really is the length and 144.

143
00:09:34,990 --> 00:09:38,330
It's logical sense that's a full day's information for the batch.

144
00:09:38,410 --> 00:09:43,150
So given one full day go ahead and tell me in the forecast what the next 10 minutes are going to look

145
00:09:43,150 --> 00:09:44,260
like.

146
00:09:44,260 --> 00:09:44,560
OK.

147
00:09:44,980 --> 00:09:46,890
So we have the length of skill training set.

148
00:09:46,930 --> 00:09:48,510
We know what the generator does.

149
00:09:48,520 --> 00:09:53,330
However if you take a look at the first batch you'll notice that it's quite a bit larger because the

150
00:09:53,330 --> 00:10:00,640
first batch is now essentially passing in every single column and you have a multivariate time series.

151
00:10:00,700 --> 00:10:06,820
So these batches are a lot larger and a lot more complex but the same idea holds that given these last

152
00:10:06,860 --> 00:10:13,540
one hundred and forty four data points for each of the feature columns show me and predict the next

153
00:10:13,540 --> 00:10:18,440
10 minutes worth of data essentially one row into the future of all those variables.

154
00:10:18,460 --> 00:10:22,330
So that's why why is so large it's one y per feature.

155
00:10:22,330 --> 00:10:24,560
Because they multivariate data set.

156
00:10:24,580 --> 00:10:24,870
OK.

157
00:10:25,480 --> 00:10:28,990
So you can edit the length so that it makes sense for your time series.

158
00:10:28,990 --> 00:10:30,890
Then we create the model suite that before.

159
00:10:31,210 --> 00:10:34,320
And this is where the first change occurs.

160
00:10:34,360 --> 00:10:41,920
We have this Ellis Tam layer here the input shape is going to be again defined by the length of that

161
00:10:41,920 --> 00:10:43,320
batch comma.

162
00:10:43,870 --> 00:10:47,200
And then scale train that shape of one.

163
00:10:47,200 --> 00:10:48,730
So what is that shape of one.

164
00:10:49,000 --> 00:10:54,340
Well if we take a look at the shape of scale the train currently it's three thousand five hundred sixty

165
00:10:54,340 --> 00:10:58,240
five rows by twenty eight features twenty eight features.

166
00:10:58,270 --> 00:11:01,230
That's just the number of columns we had in the original data frame.

167
00:11:01,270 --> 00:11:06,940
So if you scroll all the way back up here and take a look at our original data frame and you take a

168
00:11:06,940 --> 00:11:13,660
look at these 28 columns we're right now predicting each of these columns ten minutes into the future

169
00:11:13,660 --> 00:11:14,530
step by step.

170
00:11:14,770 --> 00:11:19,300
So this is what makes it multivariate is that instead of predicting for instance appliances into the

171
00:11:19,300 --> 00:11:22,500
future we're predicting these 28 columns into the future.

172
00:11:22,540 --> 00:11:25,060
So that is kind of main change.

173
00:11:25,060 --> 00:11:32,130
Number one they have to take into account is when you created that model the shape here for this second

174
00:11:32,140 --> 00:11:36,790
I mentioned the input shape has to reflect the amount of features you're considering.

175
00:11:37,060 --> 00:11:44,230
And I've actually put it here as scaled train that shape index 1 because is now the generalized version

176
00:11:44,710 --> 00:11:46,170
of any other data set.

177
00:11:46,570 --> 00:11:49,390
So you could input your own multivariate data set.

178
00:11:49,390 --> 00:11:53,680
Check out the shape and you still be able to use this code as a plug and play because as the generalized

179
00:11:53,680 --> 00:12:00,280
version and same the second change here is the final prediction layer you should have one neuron per

180
00:12:00,280 --> 00:12:00,830
feature.

181
00:12:01,120 --> 00:12:04,750
So it should be the dense layer of skilled train again that shape one.

182
00:12:05,170 --> 00:12:07,990
So if you wanted to you could hard code these as twenty eight.

183
00:12:08,230 --> 00:12:12,970
But here we have the generalized versions and those are the two main changes you need to consider when

184
00:12:12,970 --> 00:12:14,770
dealing with multivariate data.

185
00:12:14,860 --> 00:12:20,200
And as mentioned in the lecture I would highly recommend you check out a Rima based models first because

186
00:12:20,200 --> 00:12:24,530
they're interpret ability and they actually perform very well on multivariate datasets.

187
00:12:24,550 --> 00:12:26,200
So you can check that out.

188
00:12:26,380 --> 00:12:31,520
After that you get a summary of the model and you can also implement early stopping if you want just

189
00:12:31,520 --> 00:12:33,520
a week before essentially nothing changes here.

190
00:12:33,530 --> 00:12:34,670
It all looks exactly the same.

191
00:12:35,120 --> 00:12:40,940
And then we just fit to that generator we to some epochs we can do early stopping we only train for

192
00:12:40,940 --> 00:12:46,850
two epochs because as you can imagine dealing with a multivariate dataset you're training time is going

193
00:12:46,850 --> 00:12:52,100
to go up quite a bit soon as here we only train for two epochs and it was still taking about eight minutes

194
00:12:52,190 --> 00:12:54,200
per epoch more or less.

195
00:12:54,200 --> 00:12:58,070
And if you only train for two epochs there's still plenty of room for improvement.

196
00:12:58,100 --> 00:13:02,770
You can see both training validation the loss of training set and your validation that test set.

197
00:13:02,810 --> 00:13:03,620
We're still going down.

198
00:13:03,670 --> 00:13:07,890
So it's probably good train for a lot more epochs so early stopping was actually never hit.

199
00:13:08,060 --> 00:13:13,100
So you can go ahead and play around that dataset and then train it for longer if you have the patience

200
00:13:13,100 --> 00:13:18,410
for it or if you have some free time to just let your computer run and crunch those numbers then the

201
00:13:18,410 --> 00:13:19,230
evaluation.

202
00:13:19,430 --> 00:13:22,990
All of this code is essentially exactly the same as before.

203
00:13:23,060 --> 00:13:28,070
The only things you need to consider here are when you're reshaping those evaluation batches.

204
00:13:28,070 --> 00:13:30,340
It's one by length.

205
00:13:30,470 --> 00:13:34,400
And then here it's against skill train that shape one.

206
00:13:34,400 --> 00:13:35,850
So that's the main difference here.

207
00:13:35,930 --> 00:13:40,910
Because now that first batch has to include those 28 feature columns.

208
00:13:40,910 --> 00:13:43,430
So that's during the evaluation.

209
00:13:43,430 --> 00:13:49,940
And then here you'll notice that we're essentially just going to set and features equal to scale train

210
00:13:50,060 --> 00:13:55,090
that shape one for all our previous problems and no features we just hardcoded as one.

211
00:13:55,460 --> 00:13:59,780
But here we want to make sure it equals to the same number of feature columns we had in the beginning

212
00:14:00,350 --> 00:14:02,790
and then that loop is exactly the same.

213
00:14:02,810 --> 00:14:08,350
And here we're just predicting for the test range because expand that number to forecast into the future

214
00:14:08,690 --> 00:14:10,280
especially if you just retrained on everything.

215
00:14:10,280 --> 00:14:13,030
So all this code pretty much exactly the same.

216
00:14:13,040 --> 00:14:17,400
The main thing to keep in mind here is your number of features is going to be higher than just one.

217
00:14:17,540 --> 00:14:20,330
And then here we have the test predictions scale tests.

218
00:14:20,330 --> 00:14:23,880
And if you want you can inverse those transformations and begin to compare.

219
00:14:23,930 --> 00:14:26,800
And here we have the real test data.

220
00:14:27,990 --> 00:14:30,100
Versus our true predictions.

221
00:14:30,100 --> 00:14:34,430
And consider if your predictions are a lot noisier and there's only room for improvement here.

222
00:14:34,530 --> 00:14:42,330
Now again it really depends on the actual situation for how you would actually evaluate this model.

223
00:14:42,330 --> 00:14:48,750
You could do mean squared error but recall that you have all these features so we could do is do a routine

224
00:14:48,750 --> 00:14:55,380
squared error comparing for instance appliances versus the true appliances prediction and see how that

225
00:14:55,380 --> 00:14:56,040
performed.

226
00:14:56,370 --> 00:15:01,290
If you just are concerned about your routine squared error on appliances and your forecasting capability

227
00:15:01,410 --> 00:15:02,820
on that particular feature.

228
00:15:02,970 --> 00:15:05,520
But if you're forecasting on everything as a whole.

229
00:15:05,820 --> 00:15:10,320
Maybe you want to do some sort of weighted average of your root mean square errors across all these

230
00:15:10,320 --> 00:15:11,190
features.

231
00:15:11,190 --> 00:15:15,870
It really depends on what features are important to predict which features are not that important and

232
00:15:16,710 --> 00:15:22,230
also you'll notice that the scale here is different for all these so it probably make more sense to

233
00:15:22,560 --> 00:15:25,920
start calculating that sort of error term on maybe the skilled version.

234
00:15:25,950 --> 00:15:31,290
Again it really depends on the exact situation and your exact dataset on how you should be evaluating

235
00:15:31,560 --> 00:15:33,540
these multivariate predictions.

236
00:15:33,550 --> 00:15:33,990
OK.

237
00:15:34,110 --> 00:15:40,860
But again the main differences here are during the forecasting section is edit the number of features

238
00:15:40,860 --> 00:15:45,790
here and during the construction of the model to edit the number of features.

239
00:15:46,110 --> 00:15:52,800
In this simple R9 layer to reflect the input shape and then one dense neuron or one neuron in the sense

240
00:15:52,800 --> 00:15:56,960
layer per feature and those are essentially the main changes.

241
00:15:56,970 --> 00:15:57,590
OK.

242
00:15:57,810 --> 00:15:58,980
I hope that was helpful to you.

243
00:15:59,040 --> 00:16:03,930
And as always again I would highly suggest checking out those Auriemma based models first before jumping

244
00:16:03,990 --> 00:16:06,450
into this kind of black box of deep learning.

245
00:16:06,510 --> 00:16:08,180
But this notebook is here for you.

246
00:16:08,220 --> 00:16:09,500
In case you want ease it.

247
00:16:09,680 --> 00:16:10,430
Thanks.

248
00:16:10,440 --> 00:16:11,960
We'll see you at the next section of the course.
