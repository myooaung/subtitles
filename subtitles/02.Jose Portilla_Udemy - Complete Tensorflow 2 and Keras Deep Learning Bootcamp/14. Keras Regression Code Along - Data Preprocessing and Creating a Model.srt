1
00:00:05,230 --> 00:00:10,660
Welcome back everyone to part two or second phase of the Care's regression coda long project.

2
00:00:10,660 --> 00:00:14,050
You've already taken a look at the features then a little bit of feature engineering and even dropped

3
00:00:14,050 --> 00:00:15,160
some features.

4
00:00:15,160 --> 00:00:20,920
Now it's time to scale along for train test split and then create a model and train the model.

5
00:00:20,950 --> 00:00:21,730
Let's get started.

6
00:00:22,140 --> 00:00:28,390
OK after we finish all our feature engineering the next step is to separate our features from our label.

7
00:00:28,390 --> 00:00:38,920
We can do this by assigning X to D F drop price along axes equal to one and then the other thing we're

8
00:00:38,920 --> 00:00:47,320
going to do is say Y is equal to the F price and to make sure there's no issues between the Pansies

9
00:00:47,320 --> 00:00:49,810
data types and the NUM pine Americas.

10
00:00:49,810 --> 00:00:52,180
We can also just request the values.

11
00:00:52,180 --> 00:00:58,240
So if you say that values this returns back then umpiring underneath the actual data frames at risk

12
00:00:58,250 --> 00:00:59,510
panel series.

13
00:00:59,680 --> 00:01:04,850
So you run those and now that we have separator features from our label it's time to do a train test

14
00:01:04,850 --> 00:01:05,730
split.

15
00:01:05,920 --> 00:01:16,840
We will say from S.K. learn that model selection import a train test split run that and the way I like

16
00:01:16,840 --> 00:01:21,730
to do this to save myself a little bit of typing time is just grab the example from the dock string

17
00:01:22,030 --> 00:01:23,350
and copy and paste it in here.

18
00:01:23,830 --> 00:01:28,480
So we'll just go ahead and grab that copy at paste it in.

19
00:01:28,530 --> 00:01:30,170
I would encourage you to do the same.

20
00:01:30,330 --> 00:01:36,420
And what I'm going to do is I'm going to set my test size to just be 30 percent and I'll also set my

21
00:01:36,420 --> 00:01:40,580
random state to be 1 0 1 again arbitrary no choice.

22
00:01:40,590 --> 00:01:43,360
It's just that we're consistent across the notebook.

23
00:01:43,410 --> 00:01:44,850
So go ahead and use the same random stay.

24
00:01:44,850 --> 00:01:48,250
I do that way you can compare your results directly.

25
00:01:48,270 --> 00:01:50,720
OK so we'll go ahead and run this.

26
00:01:50,730 --> 00:01:52,230
We've done our split.

27
00:01:52,230 --> 00:01:54,960
Now it's time to actually perform the scaling.

28
00:01:54,960 --> 00:01:57,520
Remember we want to do the scaling post split.

29
00:01:57,600 --> 00:02:03,360
That way we only fit to the training set to prevent data leakage from the test set.

30
00:02:03,360 --> 00:02:11,250
We'll do this just as we've done before by saying SDK learn that pre processing import min max scalar

31
00:02:11,760 --> 00:02:13,370
you can technically use any scaling you like.

32
00:02:13,380 --> 00:02:16,500
But min max scalar it's simpler to understand.

33
00:02:16,500 --> 00:02:23,220
So we'll use it in this case min max scalar and then we will redefine our training set as the scaled

34
00:02:23,220 --> 00:02:30,670
versions and I can save a little bit of time by both fitting and transforming all my training set in

35
00:02:30,670 --> 00:02:31,540
one step.

36
00:02:31,540 --> 00:02:37,060
So previously we saw fit and transform in two steps but scaling actually has this convenience fixture

37
00:02:37,180 --> 00:02:40,450
feature of transforming it and fitting at one step.

38
00:02:40,720 --> 00:02:47,260
So we'll go ahead do that and then forward the test set recall here we're just going to transform we

39
00:02:47,260 --> 00:02:54,370
don't fit to our test set because we don't want to assume prior information about our test set so there's

40
00:02:54,370 --> 00:02:56,160
a training set and there's a test set.

41
00:02:56,160 --> 00:02:57,700
Now they've been scaled.

42
00:02:57,750 --> 00:03:00,430
Coming up next we're gonna do is actually create the model.

43
00:03:00,570 --> 00:03:11,470
So we'll do the following we'll say from tensor flow that care stop models import sequential and then

44
00:03:11,470 --> 00:03:17,530
we'll also say from tensor flow that carries dot layers import.

45
00:03:18,160 --> 00:03:22,030
And we're going to import dense layer.

46
00:03:22,030 --> 00:03:28,400
So we run that and let's create our model so we create a sequential model.

47
00:03:28,480 --> 00:03:35,830
And typically what we do is we try to base the number of neurons or units in our layers from the size

48
00:03:35,890 --> 00:03:37,630
of the actual feature data.

49
00:03:37,630 --> 00:03:41,320
So let's take a quick look at the shape of our data.

50
00:03:41,530 --> 00:03:47,860
So it looks like we have 19 incoming features and that's probably a good range to then have 19 neurons

51
00:03:48,310 --> 00:03:50,070
in our layer.

52
00:03:50,170 --> 00:04:02,920
So same model add dense 19 and we'll also say that the activation layer will be a rectified linear unit.

53
00:04:02,920 --> 00:04:06,120
Now let's make sure we actually spell this correctly sequential.

54
00:04:06,460 --> 00:04:12,340
Now something that we're going to explore later on is to see how we can use early stopping in order

55
00:04:12,340 --> 00:04:17,740
to try to choose the correct number of epochs to train for but also to try to choose the correct number

56
00:04:17,770 --> 00:04:25,530
of layers to train for for right now though I'm just going to copy and paste this line a couple of times

57
00:04:26,190 --> 00:04:29,060
to add in and make this a deep learning network.

58
00:04:29,070 --> 00:04:33,900
Now this may be overkill and we may end up over fitting slightly to training data but we'll be able

59
00:04:33,900 --> 00:04:39,780
to explore whether or not that's happening by passing in validation data along for our training.

60
00:04:39,780 --> 00:04:47,290
So what I will do here is add one more final layer and this last final layer just gonna have one neuron

61
00:04:47,410 --> 00:04:51,680
as its output since that's going to be directly outputting our predicted price.

62
00:04:52,060 --> 00:04:59,370
And then we are going to compile this model and previously in the theory sections of these lectures

63
00:04:59,400 --> 00:05:04,470
we went ahead and talked about atom optimizer being a good optimizer so we'll choose it by passing the

64
00:05:04,470 --> 00:05:06,070
string code atom.

65
00:05:06,300 --> 00:05:11,850
And since this is a regression problem we're choosing a continuous label such as price we'll go ahead

66
00:05:11,850 --> 00:05:15,000
and choose our lost metric as mean squared error.

67
00:05:15,000 --> 00:05:21,430
Okay so we go ahead and run that and now it's time to train the model we'll say model that fit.

68
00:05:21,540 --> 00:05:25,940
And then what I'm going to pass here is my training data just as we've done before.

69
00:05:26,040 --> 00:05:37,500
So X train and then Y train and the next thing going to do is also pass in validation data reduce shift

70
00:05:37,500 --> 00:05:44,250
tab here you'll notice that you can pass in the data to train on x and y and you can also passin validation

71
00:05:44,250 --> 00:05:44,540
data.

72
00:05:44,970 --> 00:05:51,180
And what that means is after each epoch of training on the training data will quickly run the test data

73
00:05:51,390 --> 00:05:53,640
and check our loss on the test data.

74
00:05:53,640 --> 00:05:59,070
So that way we can keep a tracking of how well performing not just on our training data but also on

75
00:05:59,070 --> 00:06:00,210
our test data.

76
00:06:00,210 --> 00:06:05,500
Keep in mind this test data will not actually affect the weights or biases of our network.

77
00:06:05,670 --> 00:06:10,690
So cares isn't going to update your model based off the test data or validation data.

78
00:06:10,690 --> 00:06:18,180
Instead it will only use the training data as it's updating the weights and biases and continue to essentially

79
00:06:18,180 --> 00:06:23,070
check how well it's doing and not just the training data but also the validation data and the will we

80
00:06:23,070 --> 00:06:30,520
pass this in is by saying validation data is equal to and then we pass in our values here.

81
00:06:30,540 --> 00:06:40,120
So we're going to say X test and then Y test and we want to make sure that we actually consider the

82
00:06:40,120 --> 00:06:41,170
values here.

83
00:06:41,170 --> 00:06:47,650
So something to make sure of is a call called values here because tensor flow may complain if you don't

84
00:06:47,650 --> 00:06:52,420
pass in a numeric array because it can't work with Pan this series or data frames that well.

85
00:06:52,420 --> 00:06:57,880
So make sure you've done the values and again what we're doing here is we're training on X train y train

86
00:06:58,160 --> 00:07:03,250
but as we go along we want to be checking against our test set and that will give us some nice plots

87
00:07:03,550 --> 00:07:05,860
to basically realize whether or not we're overfishing.

88
00:07:06,460 --> 00:07:08,010
So add in the validation data.

89
00:07:08,560 --> 00:07:15,330
And finally because it's a larger data set we're going to feed in our data in batches and we're gonna

90
00:07:15,350 --> 00:07:19,700
call batch size of one hundred and twenty eight.

91
00:07:19,720 --> 00:07:27,710
It's very typical to do batch sizes in powers of two so 64 128 256 the smaller the batch size.

92
00:07:27,760 --> 00:07:34,330
The longer training is going to take but the less likely you're going to over fit to your data because

93
00:07:34,330 --> 00:07:36,670
you're not passing in your entire training set at once.

94
00:07:36,670 --> 00:07:42,490
Instead you're focusing on these smaller batches and then fine let's go ahead and choose kind of an

95
00:07:42,490 --> 00:07:44,260
arbitrary large number of epochs.

96
00:07:44,280 --> 00:07:45,240
So a four hundred.

97
00:07:45,400 --> 00:07:47,480
We don't have any early stopping mechanisms yet.

98
00:07:47,530 --> 00:07:51,330
We'll learn about those later on in the course but right now we'll do 400.

99
00:07:51,370 --> 00:07:57,130
That way I can see those nice curves and also compare my training performance to my test performance.

100
00:07:57,130 --> 00:08:01,990
So go ahead and run this and hopefully if you've done everything correctly you should be getting to

101
00:08:01,990 --> 00:08:04,920
see the output if you get some sort of error code here.

102
00:08:04,990 --> 00:08:10,270
Make sure you reference our notebook you can go ahead and run our notebook directly in order to prevent

103
00:08:10,270 --> 00:08:11,730
any simple typos errors.

104
00:08:12,080 --> 00:08:18,580
OK so this is training on our data set right now coming up in the next lecture we'll go ahead and finish

105
00:08:18,580 --> 00:08:23,920
his training so kind of fast forward through this and then we'll start evaluating on our test data set

106
00:08:24,340 --> 00:08:27,020
as well as predict on brand new data.

107
00:08:27,100 --> 00:08:32,470
So again always in this lecture is we've scaled our data after doing the train test split created our

108
00:08:32,470 --> 00:08:37,690
model and then kind of the new thing that we saw here is adding in validation data during the fitting

109
00:08:38,110 --> 00:08:40,020
as well as choosing a batch size.

110
00:08:40,030 --> 00:08:42,070
Ok thanks I'll see you at the next lecture.
