1
00:00:05,260 --> 00:00:06,320
Welcome back everyone.

2
00:00:06,320 --> 00:00:11,620
In part three we're going to work on setting up our training batches so we're going to be using the

3
00:00:11,620 --> 00:00:14,520
built in TAF data set object.

4
00:00:14,560 --> 00:00:17,630
And again we'll be using the from tensor slices method.

5
00:00:17,680 --> 00:00:22,000
We actually already saw how to do this in the MLP lecture series.

6
00:00:22,000 --> 00:00:28,030
So let's go ahead and do a quick review and set up our batches for our again and head over back to the

7
00:00:28,030 --> 00:00:29,110
notebook.

8
00:00:29,110 --> 00:00:29,470
All right.

9
00:00:29,500 --> 00:00:32,560
I'm back at the notebook where I left off last time.

10
00:00:32,590 --> 00:00:38,200
First thing we need to do is when setting up our batches we to decide on a batch size.

11
00:00:38,410 --> 00:00:43,780
So I will go ahead and choose a relatively small batch size since we're dealing with technically image

12
00:00:43,780 --> 00:00:49,870
data but only if a dense network will slow down our learning training time a little bit and choose a

13
00:00:49,870 --> 00:00:51,020
smaller batch size.

14
00:00:51,100 --> 00:00:54,520
If you want training to go faster you can choose a larger batch size.

15
00:00:54,610 --> 00:00:55,400
Okay.

16
00:00:55,450 --> 00:01:03,740
And then I will say my dataset is equal to only Zero's object that we made before.

17
00:01:03,850 --> 00:01:08,920
And the reason I'm setting it up this way is so you can optionally try to run something like my data

18
00:01:09,040 --> 00:01:11,570
is equal to x train.

19
00:01:11,590 --> 00:01:18,070
But for now we'll just operate on zeros that way we can really tell if our generator is actually generating

20
00:01:18,190 --> 00:01:20,620
numbers such as zero or not.

21
00:01:20,620 --> 00:01:21,170
Okay.

22
00:01:21,340 --> 00:01:27,580
Next they're going to do is set up our data set so to do this we simply say data set is equal to TAF

23
00:01:28,230 --> 00:01:40,130
dot data dot data set and then we call from tensor underscore slices and then we pass in my data and

24
00:01:40,130 --> 00:01:46,020
then we're gonna do here is we call shuffle even though it doesn't matter too much since it's already

25
00:01:46,020 --> 00:01:48,090
kind of just random zeros.

26
00:01:48,180 --> 00:01:50,940
You should always shuffle just in case it's an order.

27
00:01:51,120 --> 00:01:57,150
And then we're going to do here is choose a buffer size the buffer size is just so it doesn't load everything

28
00:01:57,150 --> 00:02:00,600
up into memory we set a limit in case we're dealing for really large data set.

29
00:02:00,780 --> 00:02:03,090
And usually we just choose a thousand.

30
00:02:03,300 --> 00:02:08,600
So we'll go ahead and run that and then we're going to do is if you want to just confirm the type of

31
00:02:08,600 --> 00:02:15,830
this object it is a data set and that's a shuffled data sets are actually going to shuffle it and then

32
00:02:15,830 --> 00:02:22,550
we're gonna do here is will say data set is equal to data set and we'll start setting up the batches

33
00:02:22,550 --> 00:02:29,810
for it by sending dot batch and then you pass in the batch size that you wanted and then we say drop

34
00:02:31,420 --> 00:02:39,700
remainder is equal to true recall from the actual MLP lecture series that drop remainder just means

35
00:02:39,850 --> 00:02:45,340
if those last few items is less than your batch size we'll go ahead and just drop those.

36
00:02:45,340 --> 00:02:53,080
So if I take a look at the size of my batch it's thirty two and then if I take a look at my dataset

37
00:02:53,350 --> 00:02:57,860
the shape of it is five thousand nine hundred twenty three.

38
00:02:57,880 --> 00:03:05,420
So that means 30 to if we try to divide this up into batches of 32 we get a little bit of leftover we

39
00:03:05,420 --> 00:03:08,880
get a hundred eighty five clean batches with some images left over.

40
00:03:09,000 --> 00:03:12,660
And so all drop remainder does is it drops that last remainder.

41
00:03:12,660 --> 00:03:18,170
So we get one hundred eighty five clean batches each of thirty two and for our dataset one hundred and

42
00:03:18,240 --> 00:03:22,980
eighty five batches should be more than enough for at most going to drop just 32 images and out of almost

43
00:03:22,980 --> 00:03:23,790
6000.

44
00:03:24,030 --> 00:03:25,660
That's essentially inconsequential.

45
00:03:25,680 --> 00:03:29,220
So we can go ahead and feel safe dropping that remainder.

46
00:03:29,220 --> 00:03:36,540
And then what we can do here is call pre fetch 1 and then we'll just train this for one epoch after

47
00:03:36,540 --> 00:03:37,040
one epoch.

48
00:03:37,050 --> 00:03:40,270
We should be able to see it start generating zeros.

49
00:03:40,950 --> 00:03:49,230
OK so we set up everything for our batch training now it's time to actually create our training loop.

50
00:03:49,440 --> 00:03:56,850
So if we take a look at the Gan object we created it's this sequential model and I can actually call

51
00:03:57,420 --> 00:04:04,380
layers off of this and essentially get back a list of two sequential models because recall when we created

52
00:04:04,650 --> 00:04:09,210
our again it's essentially just a sequential of the generator and discriminator which ends up being

53
00:04:09,330 --> 00:04:16,380
this concatenated list of the generator and discriminator which means if I call the first item off of

54
00:04:16,380 --> 00:04:23,310
this I can see my first sequential model and then I can call layers off of this and then I see the actual

55
00:04:23,310 --> 00:04:29,460
layers here of that first sequential model and then I can do the same for the second one which is the

56
00:04:29,460 --> 00:04:32,460
discriminator and I can see my discriminator here.

57
00:04:32,460 --> 00:04:36,710
So notice my generator starts off dense and then reshapes everything.

58
00:04:36,840 --> 00:04:44,160
And what I can also do is then start calling things off like a summary of my generator model versus

59
00:04:44,160 --> 00:04:47,250
a summary of my discriminator model.

60
00:04:47,700 --> 00:04:53,610
And this is all just to show you that I can access the generator model by itself or the discriminator

61
00:04:53,610 --> 00:04:59,730
model by itself by just calling off of these actual layers since the layers of the scan are sequential

62
00:04:59,730 --> 00:05:00,750
models themselves.

63
00:05:00,750 --> 00:05:05,220
So that's all I really want to show here with this dot layers because I'm going to do is I'm actually

64
00:05:05,220 --> 00:05:12,330
going to grab the separate components before we actually start the loop by saying generator comma discriminator

65
00:05:13,500 --> 00:05:17,760
equals again and I'm essentially impacting that list of layers.

66
00:05:17,760 --> 00:05:22,920
So I'll go ahead and this is all now going to go in a single cell but have Gan about layers here and

67
00:05:22,920 --> 00:05:26,970
I'm grabbing the generator component from it and the discriminator component and we can see that layers

68
00:05:26,970 --> 00:05:27,420
itself.

69
00:05:27,420 --> 00:05:30,100
I could just call the actual models itself.

70
00:05:30,130 --> 00:05:30,490
All right.

71
00:05:30,840 --> 00:05:34,620
So our training loop is going to be a little different here than we usually do.

72
00:05:34,620 --> 00:05:35,640
Typically we just fit.

73
00:05:35,670 --> 00:05:43,350
But since we're operating with essentially two distinct phases we need to do a say for epoch in range

74
00:05:43,380 --> 00:05:44,560
epochs.

75
00:05:44,610 --> 00:05:48,540
In this case we're just training for one after one epoch we should be able to actually see results that

76
00:05:48,870 --> 00:05:53,400
basically make sense and we're going to do is print ourselves a little message just to make sure we're

77
00:05:53,580 --> 00:06:02,020
running we'll see currently on epoch and then we'll print out epoch we'll do plus one that we don't

78
00:06:02,020 --> 00:06:08,350
start to epoch 0 and so we'll keep eyes a little counter so I's equal to zero in our case.

79
00:06:08,470 --> 00:06:17,620
Essentially just run once and then for every batch in our dataset what we're going to do here is we'll

80
00:06:17,620 --> 00:06:20,770
say I is equal to I plus 1

81
00:06:24,610 --> 00:06:33,260
so we able to run and then we'll say if I model hundred is equal to zero.

82
00:06:33,310 --> 00:06:35,170
Essentially we've done a hundred batches.

83
00:06:35,170 --> 00:06:36,970
We're gonna print out a little message.

84
00:06:36,970 --> 00:06:42,250
This is now so we can track on the current batch so we'll say it's actually put a little tab here just

85
00:06:42,250 --> 00:06:51,850
for spacing we'll say currently on batch number and then we'll input I and then I can actually calculate

86
00:06:51,850 --> 00:06:58,880
how many batches by saying of lengths of my data and then just do classic division.

87
00:06:58,880 --> 00:07:05,630
So to forward slashes divided by batch size this is just for kind of printing out what we're currently

88
00:07:05,630 --> 00:07:08,610
on and let's make sure we spell that right.

89
00:07:08,660 --> 00:07:09,050
There we go.

90
00:07:09,750 --> 00:07:10,330
OK.

91
00:07:10,370 --> 00:07:14,780
So again all I'm doing here is I'm just using eyes a tracker to essentially count what batch I'm on

92
00:07:14,910 --> 00:07:16,620
in every 100 batches.

93
00:07:16,640 --> 00:07:19,970
I'll go ahead and print something out to tell me what batch I'm currently on.

94
00:07:19,970 --> 00:07:24,470
Otherwise we actually won't see any information while this is training so we can print out that I'm

95
00:07:24,470 --> 00:07:28,250
currently on some epoch and then print out what batch I'm currently on.

96
00:07:28,250 --> 00:07:34,420
And if you scroll up again recall that we should be having a hundred eighty five batches.

97
00:07:34,460 --> 00:07:35,050
OK.

98
00:07:35,180 --> 00:07:41,390
So now we set this up in two phases during the training to make sure we close that off.

99
00:07:41,780 --> 00:07:43,650
Go ahead and enter here.

100
00:07:43,790 --> 00:07:49,280
So outside of this if statement I'm going to start off by the discriminator phase.

101
00:07:49,280 --> 00:07:49,870
OK.

102
00:07:49,970 --> 00:08:00,420
So first I need to create some random noise so it's ATF random normal noise and the shape of this noise

103
00:08:00,450 --> 00:08:08,980
should be equal to the batch size by the coatings size we chose earlier recall coating sizes essentially

104
00:08:08,980 --> 00:08:17,600
that latent layer that is at the very beginning of the what is essentially almost like a decoder that

105
00:08:17,600 --> 00:08:20,350
the generator is operating as OK.

106
00:08:20,360 --> 00:08:25,670
So we have that random noise and then what we're gonna do is we'll generate some numbers based off just

107
00:08:25,670 --> 00:08:26,890
noise input.

108
00:08:26,960 --> 00:08:34,970
So this is technically the discriminator training phase but in order to discriminate between real images

109
00:08:34,970 --> 00:08:37,530
versus fake images we have to actually produce some images.

110
00:08:37,550 --> 00:08:45,540
So the generator what it's going to do on this very first run is it's going to try to generate images.

111
00:08:45,590 --> 00:08:46,880
So that is what's happening here.

112
00:08:47,090 --> 00:08:53,660
All the generator gets to see is this random noise and then given that we're gonna noise see if based

113
00:08:53,660 --> 00:08:59,900
off this model shape of going from 100 it's 150 784 and then reshaping it's 20 to twenty eight what

114
00:09:00,050 --> 00:09:04,520
is going to do is can you actually produce an image you can imagine that the very first images that

115
00:09:04,520 --> 00:09:06,760
it's gonna produce are just gonna be absolute noise.

116
00:09:07,010 --> 00:09:09,560
So it generates some images.

117
00:09:09,560 --> 00:09:14,950
Next we want to do is we want to concatenate generated images along with real images.

118
00:09:15,140 --> 00:09:25,460
So what we'll do here is we'll say X fake versus real so we have our fake generated images and our real

119
00:09:25,460 --> 00:09:27,140
images from our dataset.

120
00:09:27,140 --> 00:09:31,910
So the way we're going to do this is if a little bit of sensor flow commands they look pretty similar

121
00:09:31,910 --> 00:09:36,050
to some panels and then like code that we have Comcast which is concatenate.

122
00:09:36,250 --> 00:09:41,070
And what I want to do is I need a pass and a list of what I actually want to concatenate together.

123
00:09:41,180 --> 00:09:43,760
I want to concatenate together my fake images.

124
00:09:43,760 --> 00:09:47,930
Those are the ones that the generator is generating on the very first pass this should be horrible but

125
00:09:47,930 --> 00:09:49,560
hopefully over time they get better.

126
00:09:49,730 --> 00:09:56,480
And then what I need to do is I need to actually grab my current x batch and to make sure this works

127
00:09:56,510 --> 00:10:02,720
as far as data types action to cast the data type to be float 32 bit.

128
00:10:02,990 --> 00:10:16,720
So we'll say F D types cast and I take in my X batch and cast it to float 32 bit and then we can cat.

129
00:10:16,710 --> 00:10:20,310
Nate this along axis is equal to zero.

130
00:10:20,310 --> 00:10:26,520
Okay so all we're doing here is I have these fake generated images and then I have my real images from

131
00:10:26,520 --> 00:10:33,030
X batch which I'm casting to float 32 that way later on I don't get errors if you don't cast to float

132
00:10:33,030 --> 00:10:34,960
through to some versions of tense flow.

133
00:10:34,980 --> 00:10:39,720
Also the penny on your operating system will show errors that it can't concatenate things that are a

134
00:10:39,720 --> 00:10:40,850
different data types.

135
00:10:40,900 --> 00:10:43,750
Okay so we can get those guys together.

136
00:10:43,800 --> 00:10:50,070
Now I have my fake versus real then what I need to do is I need to actually set the target label for

137
00:10:50,070 --> 00:10:50,640
them.

138
00:10:50,640 --> 00:10:53,670
Essentially what is the Y for the discriminator.

139
00:10:53,670 --> 00:11:04,230
So the distributor is going to take TAF constant while we're gonna have the following we'll say zero

140
00:11:04,230 --> 00:11:08,180
point zero times the batch size.

141
00:11:08,190 --> 00:11:14,820
Notice that this is a list so you might recall that if you take a list of a single item and multiply

142
00:11:14,820 --> 00:11:17,660
it by some number it essentially just expands that list.

143
00:11:17,690 --> 00:11:20,760
So I'll show you a quick example of that in case you're not familiar.

144
00:11:20,760 --> 00:11:25,930
This is just basic python but let's imagine I have that zero point zero times it by ten.

145
00:11:25,950 --> 00:11:28,310
The result is here 10 of these.

146
00:11:28,360 --> 00:11:34,290
Okay so that's what I'm doing here I'm just taking all these zeroes times that batch size and then what

147
00:11:34,290 --> 00:11:41,220
I'm going to do is I'll then go ahead and say plus and that is these are just Python lists.

148
00:11:41,220 --> 00:11:42,420
These are num pi res.

149
00:11:42,470 --> 00:11:44,730
I'm not actually adding zero plus one here.

150
00:11:44,730 --> 00:11:51,400
I'm just taking my list of zeros and then I'm essentially joining it or concatenated along with.

151
00:11:51,510 --> 00:11:55,580
Here we have one point zero times the batch size

152
00:11:58,440 --> 00:12:02,370
OK so what does this actually look like when it's all said and done.

153
00:12:02,400 --> 00:12:10,910
Let's go ahead and grab it here grab these guys and then let's paste this here and see what we actually

154
00:12:10,910 --> 00:12:11,830
get.

155
00:12:11,840 --> 00:12:18,860
So if I run this I essentially get a bunch of zeros first and then a bunch of ones.

156
00:12:18,860 --> 00:12:20,900
So does that actually make sense.

157
00:12:20,900 --> 00:12:27,500
Well we come back here we'll see that it does because the zeros should be zero.

158
00:12:27,500 --> 00:12:29,610
Not true for the generated images.

159
00:12:29,780 --> 00:12:38,190
The ones should be true for the actual real images from the X batch dataset and we have to make sure.

160
00:12:39,110 --> 00:12:39,650
This little

161
00:12:42,400 --> 00:12:45,490
cursor there should be right here.

162
00:12:45,490 --> 00:12:46,170
There we go.

163
00:12:46,180 --> 00:12:53,160
So it's essentially a list of single list zeros and a list of single ones so let me copy and paste them.

164
00:12:53,200 --> 00:12:55,840
That's why we actually didn't get those brackets correctly.

165
00:12:56,190 --> 00:12:57,310
Pace set in.

166
00:12:57,460 --> 00:12:58,240
Run that again.

167
00:12:58,270 --> 00:12:59,970
And just to show you what it looks like.

168
00:12:59,980 --> 00:13:03,000
So this is essentially what we're using as our labels.

169
00:13:03,040 --> 00:13:07,690
So all these zeros correspond to that first batch of generated images.

170
00:13:07,690 --> 00:13:08,320
It's zero.

171
00:13:08,320 --> 00:13:09,460
They're not real.

172
00:13:09,460 --> 00:13:14,770
And then ones here correspond to the real images the ones that we actually know are true zeros written

173
00:13:14,770 --> 00:13:16,990
by human eyes ones.

174
00:13:16,990 --> 00:13:20,860
And that is going to be joined up with this ex fake versus real.

175
00:13:20,860 --> 00:13:22,530
So we have those labels now.

176
00:13:22,570 --> 00:13:31,890
Next we need to do is in order to get rid of a curse morning we say discriminator not trainable.

177
00:13:31,930 --> 00:13:34,820
We actually do want the discriminator to be trained here.

178
00:13:34,870 --> 00:13:41,140
We'll say it's equal to true recall that previously we said it was false and that was basically to let

179
00:13:41,140 --> 00:13:42,580
the gang compile.

180
00:13:42,580 --> 00:13:48,820
So if you have discriminated that trainable is equal to False here that's only false for this compiling

181
00:13:48,910 --> 00:13:51,580
it doesn't mean you can never train discriminator again.

182
00:13:51,580 --> 00:13:54,590
So that's why we have to essentially reset it as OK.

183
00:13:54,610 --> 00:13:56,310
Now it's time to train the discriminator.

184
00:13:56,380 --> 00:13:57,670
So we said that the true.

185
00:13:57,670 --> 00:14:10,660
And then we say take my discriminator and then train on batch on that X fake versus real.

186
00:14:11,090 --> 00:14:14,060
And then the why is that why when we just created.

187
00:14:14,480 --> 00:14:17,530
So this was actually pretty straightforward for this commuter train phase.

188
00:14:17,550 --> 00:14:22,620
All we're doing is we start off with some random noise given that random noise to the generator and

189
00:14:22,620 --> 00:14:23,630
telling the generator.

190
00:14:23,700 --> 00:14:26,000
Given this random noise produced some images.

191
00:14:26,040 --> 00:14:32,800
So it's really interesting here is the generator never actually sees any real M digits images.

192
00:14:32,820 --> 00:14:39,010
All it gets to see here is noise being fed into it and then increases generated images.

193
00:14:39,090 --> 00:14:45,270
We get those fake images these generated images and concatenate them together with the real images from

194
00:14:45,270 --> 00:14:50,400
our current ex batch which recall we're getting from that data set object that created before.

195
00:14:50,400 --> 00:14:55,170
Next we need to actually create labels for these and we know the first ones do talk talking carbonation

196
00:14:55,290 --> 00:14:56,160
are all the fake images.

197
00:14:56,190 --> 00:15:00,360
So we say those will all be zeros since they're not real.

198
00:15:00,360 --> 00:15:05,700
And then we assign one labels to all the ones that are real and you do this regardless of how many classes

199
00:15:05,700 --> 00:15:07,920
you have because they're called discriminator.

200
00:15:07,920 --> 00:15:11,730
All it's trying to do is say what is real versus what is fake.

201
00:15:11,970 --> 00:15:15,960
And then we have screaming that trainable is equal to true and then we're gonna go ahead and train on

202
00:15:15,960 --> 00:15:20,000
that batch and now all that's left to do is to train the generator.

203
00:15:20,000 --> 00:15:27,750
So for this we create some random noise again we'll just say TAF thought random that normal for random

204
00:15:27,750 --> 00:15:34,980
normal noise and then we'll say shape is equal to the batch size by whatever the encoding size you want

205
00:15:34,980 --> 00:15:35,370
in here.

206
00:15:35,370 --> 00:15:37,530
So it will say coding size.

207
00:15:37,830 --> 00:15:39,280
So there's a random noise again.

208
00:15:39,330 --> 00:15:39,590
Go ahead.

209
00:15:39,600 --> 00:15:47,260
Just Label this as train generator so there's the random noise and then after that what we're going

210
00:15:47,260 --> 00:15:50,280
to do is we actually in this phase.

211
00:15:50,350 --> 00:15:52,280
What's happening here is a generator.

212
00:15:52,330 --> 00:15:57,010
It's going to start by producing fake images and then discriminator again tries to guess whether these

213
00:15:57,010 --> 00:15:59,080
images are fake or real.

214
00:15:59,080 --> 00:16:01,810
However in phase two is actually happening here.

215
00:16:01,990 --> 00:16:05,170
We want to discriminate or to believe that these fake images are real.

216
00:16:05,200 --> 00:16:09,580
So what we end up doing is we set those targets all equal to one.

217
00:16:09,610 --> 00:16:19,870
So what we do here is we say why two is equal to TMF dot constant and we have this one point zero times

218
00:16:19,870 --> 00:16:21,370
the size of the batch.

219
00:16:21,460 --> 00:16:26,170
So say at one point zero and make sure we have princes here.

220
00:16:26,170 --> 00:16:35,790
So one point zero times batch size parentheses close that off and then to avoid an error we say set

221
00:16:35,790 --> 00:16:40,580
discriminator is actually really to avoid a point a warning not an error.

222
00:16:41,620 --> 00:16:46,730
What happens is Caris if you put your trainable is true and then say trainable is false.

223
00:16:46,840 --> 00:16:47,980
Chris would essentially warn you.

224
00:16:47,980 --> 00:16:53,010
Are you sure you want to do this because you compiled it with a false versus true or vice versa.

225
00:16:53,020 --> 00:16:54,870
So that's why we're actually doing this.

226
00:16:54,880 --> 00:16:59,110
These two lines are technically to avoid these warnings.

227
00:16:59,500 --> 00:17:05,780
So we have discriminator now trainable sequel to false and then we end up taking the entire game we

228
00:17:05,780 --> 00:17:12,930
call train on batches or train on batch and we pass in the noise versus Y 2.

229
00:17:13,100 --> 00:17:16,420
So eventually hopefully what's gonna happen is at the end of all this training.

230
00:17:16,670 --> 00:17:23,240
Once these epochal loops are done running we can just take our again and then say based off some noise

231
00:17:23,270 --> 00:17:26,860
go ahead and give me some sort of generated output.

232
00:17:26,900 --> 00:17:33,530
OK so we'll go ahead and zoom out and we can see the entire process here.

233
00:17:33,620 --> 00:17:39,230
Again all we're doing is in Phase 1 we feed in that Gaussian noise the generator produces fake images

234
00:17:39,680 --> 00:17:44,840
and we're completing that batch by concatenate in an equal number of those real images and we set the

235
00:17:44,840 --> 00:17:50,720
target accordingly where zero corresponds with those fake generated images and then one corresponds

236
00:17:50,720 --> 00:17:52,880
to those real images from the batch.

237
00:17:52,880 --> 00:17:57,890
OK then we train the discriminator on that batch and then essentially in Phase 2 what we're doing is

238
00:17:57,890 --> 00:18:03,840
we feed the Gan itself some Gaussian noise it's generator will start by producing fake images then this

239
00:18:03,840 --> 00:18:07,380
criminal will try to guess whether or not these images are fake or real.

240
00:18:07,400 --> 00:18:09,690
And again we want in this phase 2.

241
00:18:09,830 --> 00:18:12,450
We want that discriminate to believe that the fake images are real.

242
00:18:12,470 --> 00:18:15,700
So we set the Y two targets to one.

243
00:18:15,740 --> 00:18:16,150
All right.

244
00:18:16,370 --> 00:18:17,860
So let's go ahead and run this.

245
00:18:17,870 --> 00:18:24,180
This is just running for one epoch but as you run this what you should see is our little batch indicator.

246
00:18:24,230 --> 00:18:29,600
And if you want you can set the smaller so instead of 100 go ahead and report on every 10 batches or

247
00:18:29,600 --> 00:18:33,270
something like that but we'll go ahead and just let this run.

248
00:18:33,310 --> 00:18:35,810
I'm going to fast forward in time until this is done running.

249
00:18:35,960 --> 00:18:36,260
All right.

250
00:18:36,290 --> 00:18:38,240
So this is just finished training for me.

251
00:18:38,270 --> 00:18:43,730
Now it's time to actually see if the generator components of the pan can produce something that more

252
00:18:43,730 --> 00:18:45,610
or less looks like a zero.

253
00:18:45,620 --> 00:18:54,110
So when you first need to do is ask for some noise we'll say noise is equal to T F random normal shape

254
00:18:54,260 --> 00:19:00,980
is equal to and then however many fake images you want to go out and ask for a 10 fake images by the

255
00:19:00,980 --> 00:19:08,980
encoding size so we run that and if we take a look at our noise space is just 10 by hundred were called

256
00:19:08,980 --> 00:19:14,040
the encoding size was one hundred and licious show what this noise actually looks like.

257
00:19:14,170 --> 00:19:16,030
So that's all it is it's just noise.

258
00:19:16,030 --> 00:19:23,590
So given this series of noise essentially 10 of these series of 100 noise points passes into the generator

259
00:19:24,310 --> 00:19:32,050
to get some images will say images is equal to the generator being fed in that noise and now if I take

260
00:19:32,050 --> 00:19:36,860
a look at the shape of my images it's ten by twenty eight by twenty eight.

261
00:19:36,880 --> 00:19:39,670
So I have ten twenty eight by twenty images.

262
00:19:39,670 --> 00:19:46,490
Let's grab the first one see what it looks like so notice has a lot of zeros which makes sense but kind

263
00:19:46,490 --> 00:19:50,270
of in the middle hopefully starts to actually label some data.

264
00:19:50,870 --> 00:19:55,370
So let's say Keelty M show and see what this visibly looks like.

265
00:19:55,370 --> 00:20:00,420
So you run that and you get something that more or less should have some sort of circular shape.

266
00:20:00,620 --> 00:20:04,230
And if you're getting just something that's very very noisy or totally blank.

267
00:20:04,340 --> 00:20:08,060
Go ahead and restart your colonel and train for a couple of epochs.

268
00:20:08,060 --> 00:20:13,400
Sometimes it's dependent on your hardware and if you're really just getting a lot of noise or just really

269
00:20:13,400 --> 00:20:14,280
bad results.

270
00:20:14,420 --> 00:20:17,780
Try downloading our notebook and running our notebook directly.

271
00:20:17,810 --> 00:20:23,570
We've actually added a little bit of code to set some random seeds for you in our notebook to make sure

272
00:20:23,570 --> 00:20:25,190
you get some good results.

273
00:20:25,190 --> 00:20:30,290
So what we did here is notice this does kind of look like a doughnut so clearly it's learning some kind

274
00:20:30,290 --> 00:20:31,360
of zero.

275
00:20:31,640 --> 00:20:37,880
Now you should also notice with your model it's highly likely you'll encounter this error known as mode

276
00:20:37,880 --> 00:20:39,230
collapse.

277
00:20:39,230 --> 00:20:41,060
That was the first image it generated.

278
00:20:41,060 --> 00:20:43,070
Let's take a look at the second one.

279
00:20:43,070 --> 00:20:44,350
Image number one.

280
00:20:44,600 --> 00:20:47,780
Notice that it essentially looks exactly the same.

281
00:20:47,780 --> 00:20:54,020
And out of this batch of 10 images you'll notice they all look either exactly the same or extremely

282
00:20:54,020 --> 00:20:54,960
similar.

283
00:20:55,010 --> 00:20:57,440
You can see that maybe the values are slightly different.

284
00:20:57,470 --> 00:21:02,030
So if I take a look at this little green point that little green point exists here.

285
00:21:02,030 --> 00:21:03,980
But the actual value is slightly different.

286
00:21:03,980 --> 00:21:07,130
This one's a little brighter and you can take a look at maybe here.

287
00:21:07,160 --> 00:21:12,440
These bottom points they are in the same coordination or formation but these happen to have slightly

288
00:21:12,440 --> 00:21:18,760
different values so they are technically different arrays but they look almost exactly the same.

289
00:21:18,800 --> 00:21:22,940
And this is extremely common with generative adversarial networks.

290
00:21:22,940 --> 00:21:25,540
And this is known as mode collapse.

291
00:21:25,670 --> 00:21:27,320
So it's actually happening here.

292
00:21:27,320 --> 00:21:32,320
Well what happens is the generators outputs are becoming less diverse.

293
00:21:33,050 --> 00:21:39,080
And so what's actually happening is the generator basically figured out that it could produce one single

294
00:21:39,080 --> 00:21:44,210
image that would always fool the discriminator and that single image was essentially something that

295
00:21:44,210 --> 00:21:45,430
looks like this.

296
00:21:45,440 --> 00:21:47,890
So the generator is starting to think so to speak.

297
00:21:47,990 --> 00:21:52,610
Why would it tried to bother to learn other types of zeroes when it figured out the type of zero that

298
00:21:52,610 --> 00:21:54,030
can fool the discriminator.

299
00:21:54,140 --> 00:22:00,530
So it gets cut and this feedback loop of consistently producing the same image and then sometimes this

300
00:22:00,530 --> 00:22:06,230
can oscillate as a discriminator gets smart this image suddenly a generator collapses back down to noise

301
00:22:06,260 --> 00:22:12,200
and then thinks of another zero it can generate et cetera and it's almost like this cycle of noise versus

302
00:22:12,470 --> 00:22:17,620
a single zero noise versus a single zero where that single zero is the image it figured out that could

303
00:22:17,620 --> 00:22:18,880
fool discriminator.

304
00:22:18,950 --> 00:22:24,350
So there's this kind of weird oscillation that can occur due to this mode collapse.

305
00:22:24,440 --> 00:22:30,770
Now this is essentially a state of the art as far as trying to figure out how we can prevent mode collapse.

306
00:22:30,770 --> 00:22:35,840
There's a couple of different ways that people can go about it and I've linked a couple of research

307
00:22:35,840 --> 00:22:43,190
papers where researchers are actually trying to do things like propose new cost functions or different

308
00:22:43,190 --> 00:22:46,880
techniques to stabilize training or avoid this mode collapse.

309
00:22:46,880 --> 00:22:53,270
So things are actually trying to do is maybe techniques like experienced replay which actually consists

310
00:22:53,360 --> 00:22:59,420
of storing the images produced by the generator at each iteration into what they call a replay buffer

311
00:22:59,870 --> 00:23:05,510
and then it gradually drops those older generator images that way discriminator gets a C newer stuff

312
00:23:05,600 --> 00:23:11,240
and you don't get this kind of repeat image that the generator understands it feels discriminator And

313
00:23:11,240 --> 00:23:18,110
the thing to notice here is we actually used a completely dense network to produce to the images it

314
00:23:18,110 --> 00:23:23,960
might make more sense to try to create a convolution volitional network based game.

315
00:23:23,990 --> 00:23:29,180
And these are known as Deep convolution all Ganz or DC Ganz.

316
00:23:29,240 --> 00:23:33,770
So we're gonna do is in the next series of lectures essentially going to edit our network.

317
00:23:33,770 --> 00:23:39,110
So instead of using dense layers since we're dealing with to the image information let's see if we can

318
00:23:39,110 --> 00:23:41,000
use convolution volitional layers.

319
00:23:41,030 --> 00:23:41,580
OK.

320
00:23:41,600 --> 00:23:43,700
Hope you found this project really interesting.

321
00:23:43,730 --> 00:23:48,050
Again we're dealing with state of the art stuff here and it's highly likely that we're not to be able

322
00:23:48,050 --> 00:23:49,880
to produce perfect zeros.

323
00:23:49,880 --> 00:23:55,250
But if you were able to produce something that is at least circular in shape then you know your generator

324
00:23:55,370 --> 00:23:59,540
has at least attempted to learn something from the images of zeros.

325
00:23:59,660 --> 00:24:02,540
And what's really cool is it never even got to see the images.

326
00:24:02,540 --> 00:24:04,890
All it gets to see is noise.

327
00:24:04,970 --> 00:24:08,810
I'll see you in the next room of lectures where you begin to discuss deep convolution Ganz.
