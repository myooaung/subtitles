1
00:00:05,290 --> 00:00:10,540
Welcome everyone to this section of the course on natural language processing with deep learning.

2
00:00:10,590 --> 00:00:16,050
We're going to be exploring how to work with text data in conjunction with deep learning and this topic

3
00:00:16,050 --> 00:00:20,730
is a natural extension of the time series and recurrent neural networks topics that we just discussed

4
00:00:20,790 --> 00:00:27,150
since we'll be using a lot of those similar topics in the section of the course we're going to be creating

5
00:00:27,180 --> 00:00:32,580
a neural network that will generate new text based on a corpus of existing text data and I would highly

6
00:00:32,580 --> 00:00:38,350
suggest they check out the article called the unreasonable effectiveness of recurrent neural networks.

7
00:00:38,460 --> 00:00:41,820
We've actually linked it for you as a resource in this lecture.

8
00:00:41,820 --> 00:00:46,620
So definitely check it out because we're essentially going through the project that's described in the

9
00:00:46,620 --> 00:00:47,580
article.

10
00:00:47,580 --> 00:00:49,680
So what are we actually going to do and how will this work.

11
00:00:50,990 --> 00:00:57,170
Basically we're going to be doing is the following task given an input string sequence predict the sequence

12
00:00:57,230 --> 00:00:59,380
shifted forward by one character.

13
00:00:59,420 --> 00:01:05,810
So if I give you the sequence h e l l shift it over forward and predict that the next sequence of four

14
00:01:05,810 --> 00:01:13,250
letters should be E L L O to spell out Hello So the character based recurrent your network will actually

15
00:01:13,250 --> 00:01:17,750
end up learning the structure of the text and the example that we're going to be working through is

16
00:01:17,750 --> 00:01:22,280
going to use the complete works of William Shakespeare and what's really cool is that you're going to

17
00:01:22,280 --> 00:01:27,230
see the network clearly learn play writing structure and spacing and things like characters entering

18
00:01:27,230 --> 00:01:29,840
and exiting just from a character level.

19
00:01:29,840 --> 00:01:35,330
So we'll be exploring that and how this network is actually able to learn basic structure of a much

20
00:01:35,390 --> 00:01:42,020
larger piece of text by just predicting character to character so essentially we're going to be doing

21
00:01:42,140 --> 00:01:44,690
is building a model that looks something like this.

22
00:01:44,690 --> 00:01:49,250
We have our sequence of input characters and notice that we have some input layer where it's one hot

23
00:01:49,250 --> 00:01:54,350
and coded and then throughout maybe a hidden layer or multiple hidden layers we end up attaching weights

24
00:01:54,350 --> 00:01:59,570
to these one hot and coatings and eventually the output layer is going to give us well.

25
00:01:59,750 --> 00:02:05,540
We'll be treating as a probability distribution for all the letters essentially giving us the highest

26
00:02:05,540 --> 00:02:09,650
probability character that should show up next.

27
00:02:09,760 --> 00:02:12,350
So let's go through the steps that we're going to be doing in this project.

28
00:02:12,350 --> 00:02:17,330
Step number one is just to read in the text data and for this we can use the basic built in Python commands

29
00:02:17,330 --> 00:02:22,490
to read in a corpus of text a string data note however if you plan on doing this with your own text

30
00:02:22,500 --> 00:02:28,070
dataset you should have at least a large dataset that contains about 1 million or more characters for

31
00:02:28,070 --> 00:02:35,710
realistic results step number two is text processing and factorization since the neural network can't

32
00:02:35,710 --> 00:02:41,370
take in raw strings and you can't multiply a raw string by a digit or a floating point number.

33
00:02:41,380 --> 00:02:43,960
What we need to do is to encode them as integers.

34
00:02:44,140 --> 00:02:46,540
So we'll be doing this for all the characters not just letters.

35
00:02:46,540 --> 00:02:52,300
So things like a b c will encode those to each have a representative index number and we'll do the same

36
00:02:52,300 --> 00:02:54,460
thing for punctuation even empty space.

37
00:02:54,490 --> 00:03:00,060
We'll have an encoding then step number three will be to create batches.

38
00:03:00,120 --> 00:03:05,730
So we're using tense flows data set object to easily create batches of text sequences so something like

39
00:03:06,000 --> 00:03:11,700
Hello space M and then the next sequence that is the target sequence will be starting at E and then

40
00:03:11,700 --> 00:03:12,630
go Ello space.

41
00:03:12,630 --> 00:03:20,390
My second understand that the complete sequence should be Hello my so we'll want to use sequence lengths

42
00:03:20,420 --> 00:03:25,460
that are long enough to capture a structure and previous words but not so long that the sequence is

43
00:03:25,460 --> 00:03:31,460
just capturing also historical noise that isn't relevant to the sequence shifted by one character forward.

44
00:03:33,360 --> 00:03:37,650
Step number four we'll be actually creating the model and we'll be using three main layers for our example

45
00:03:37,650 --> 00:03:38,040
model.

46
00:03:38,040 --> 00:03:43,920
In this lecture series and that is embedding layer a gated recurrent unit layer and then a final dense

47
00:03:43,920 --> 00:03:51,850
layer so the embedding layer what it does is essentially embeds positive integers or indices into dense

48
00:03:51,880 --> 00:03:53,620
vectors of a fixed size.

49
00:03:53,620 --> 00:04:00,280
So for example we could imagine that we have similar letter encoded to number four and another letter

50
00:04:00,280 --> 00:04:01,780
encoded to number 20.

51
00:04:01,960 --> 00:04:07,420
And what we're going to do is we're gonna choose some number of embedding dimensions and the embedding

52
00:04:07,420 --> 00:04:16,840
layer will actually cast or turn those single integers into dense vectors where there is a number per

53
00:04:16,840 --> 00:04:20,790
dimension and it's up to the user to choose the number of embedding dimensions.

54
00:04:20,800 --> 00:04:27,950
And this is has a lot to do with the way things like word to vector actually works after that will work

55
00:04:27,950 --> 00:04:29,740
with a gated recurrent unit layer.

56
00:04:29,870 --> 00:04:33,740
And that's a special type of recurrent neuron unit we discussed in the previous section of the course

57
00:04:34,250 --> 00:04:40,670
and the gated recurrent unit is like a long short term memory unit with forget gate but has fewer parameters

58
00:04:40,670 --> 00:04:44,280
than LSD m as it lacks an output gate.

59
00:04:44,320 --> 00:04:48,850
So again we discussed the gate at our current unit and you can review this in the previous section of

60
00:04:48,850 --> 00:04:55,240
the course and then finally we have the dense layer where there's essentially one year on per character

61
00:04:55,570 --> 00:04:57,630
and character labels will be one encoded.

62
00:04:57,640 --> 00:05:01,370
So the final dense layer essentially produces a probability per character.

63
00:05:01,570 --> 00:05:05,980
And what's really nice about that probability per character is that means we can play around with what's

64
00:05:05,980 --> 00:05:07,110
known as temperature.

65
00:05:07,210 --> 00:05:13,330
Essentially we can play around the fact where we can actually manipulate the probability drawing and

66
00:05:13,330 --> 00:05:19,720
maybe not just choose the highest probability character but actually assign it where we may end up playing

67
00:05:19,720 --> 00:05:25,270
or a temperature so that less probable characters happen either more or less often depending on that

68
00:05:25,270 --> 00:05:25,890
factor.

69
00:05:25,900 --> 00:05:31,560
So we'll be discussing that as we could along as well then step 5 is to train the model.

70
00:05:31,590 --> 00:05:36,840
So we'll set up our batches and make sure to one hot encode our character labels and then step six will

71
00:05:36,840 --> 00:05:42,190
be generating new text similar to how we generate the forecast in the previous section of the course.

72
00:05:42,300 --> 00:05:46,950
But it's really cool here is we'll show you how to save our models weights and then she had a reload

73
00:05:47,250 --> 00:05:52,660
that models weights with a different batch size in order to pass in just a single example.

74
00:05:52,700 --> 00:05:54,710
Okay so let's go ahead and get started.

75
00:05:54,950 --> 00:05:57,470
I'll see you at the next lecture or begin coding all of this out.
