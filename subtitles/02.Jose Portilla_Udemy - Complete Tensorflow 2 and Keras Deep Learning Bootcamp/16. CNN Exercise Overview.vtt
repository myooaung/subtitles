WEBVTT
1
00:00:05.660 --> 00:00:06.570
Welcome back everyone.

2
00:00:06.590 --> 00:00:11.620
In this lecture we're going to have an overview for the accomplished all neural network exercise notebook.

3
00:00:11.620 --> 00:00:16.970
It's a pretty simple exercise is basically having you run through a set of data that's very similar

4
00:00:16.970 --> 00:00:18.830
to the endless digital dataset.

5
00:00:19.190 --> 00:00:21.230
So let's take a look at the notebook and what you need to do.

6
00:00:21.650 --> 00:00:21.950
OK.

7
00:00:21.970 --> 00:00:24.020
Here we are at the exercise notebook.

8
00:00:24.020 --> 00:00:28.850
So for this you're going to be using what's known as the fashion and this dataset and this data set

9
00:00:28.850 --> 00:00:33.510
includes 10 labels of different clothing types along with twenty eight by twenty eight grayscale images.

10
00:00:33.590 --> 00:00:37.850
So really similar to the M this dataset for the digits we have sixty thousand images and 10000 test

11
00:00:37.850 --> 00:00:38.560
images.

12
00:00:38.780 --> 00:00:44.000
And they have a variety of descriptions and labels so it's like things like pullover dresses coats sandals

13
00:00:44.300 --> 00:00:46.580
sneakers ankle boots et cetera.

14
00:00:46.740 --> 00:00:50.950
So you can go ahead and load up the data it's built into curves by running these two lines and then

15
00:00:50.960 --> 00:00:55.310
we'll just have you essentially go through the same workflow that we've been going through for the previous

16
00:00:55.310 --> 00:00:55.820
lectures.

17
00:00:55.820 --> 00:00:57.560
Things like visualizing the data.

18
00:00:57.650 --> 00:01:02.660
So here we can see an example image and then it will have you pre processed the data by normalizing

19
00:01:02.660 --> 00:01:08.180
it by dividing by the max values then reshape the data to include that singular color channel things

20
00:01:08.180 --> 00:01:12.770
like convert the Y train a y test values to be one hot encoded and then we'll have you build out the

21
00:01:12.770 --> 00:01:17.240
model you actually have a lot of flexibility you don't necessarily need to follow out the layout we

22
00:01:17.240 --> 00:01:17.810
did here.

23
00:01:17.810 --> 00:01:21.980
This is just what the solution itself and the solution up book went through and you can always edit

24
00:01:21.980 --> 00:01:26.510
these things you can't edit things like the loss but feel free to play around with the optimizer you

25
00:01:26.510 --> 00:01:31.130
don't need to use arms propagation you can use something like Adam et cetera.

26
00:01:31.210 --> 00:01:33.800
Okay so after that you go ahead and then train the model.

27
00:01:33.800 --> 00:01:38.280
So if you want you can add an early stopping to choose the amount of epochs for you.

28
00:01:38.330 --> 00:01:40.870
But here we just went ahead and trained it for 10 epochs again.

29
00:01:40.880 --> 00:01:45.120
Feel free to add in that early stopping and then we want you to evaluate the model.

30
00:01:45.140 --> 00:01:49.460
So essentially at the end of all this basically we just want you to be able to print out a classification

31
00:01:49.460 --> 00:01:51.750
report on your model's performance.

32
00:01:51.750 --> 00:01:57.920
You should get pretty good performance somewhere around overall ninety one or ninety percent accuracy

33
00:01:57.920 --> 00:01:59.300
or recall.

34
00:01:59.330 --> 00:02:04.970
OK so feel free to edit any of the parameters you want here inside of this network.

35
00:02:05.060 --> 00:02:09.200
But coming up next we'll go ahead and walk through an example run through the solutions.

36
00:02:09.260 --> 00:02:09.770
I'll see you there.
