WEBVTT
1
00:00:05.700 --> 00:00:06.870
Welcome back everyone.

2
00:00:06.960 --> 00:00:12.450
In this lecture we're going to discuss group by the group by method it's allow us to group rows of data

3
00:00:12.450 --> 00:00:15.280
together and call aggregate functions.

4
00:00:15.660 --> 00:00:19.650
Often we want to explore how values are distributed or aggregated across groups.

5
00:00:19.860 --> 00:00:23.650
To do this we use the group by method call on a pan this data frame.

6
00:00:23.760 --> 00:00:29.010
If you're familiar of sequel or ask who all this is very very similar to a group by call.

7
00:00:29.010 --> 00:00:33.040
This process is also often referred to as a split apply combine.

8
00:00:33.090 --> 00:00:34.200
Let me show you step by step.

9
00:00:34.220 --> 00:00:37.590
But a group by actually does behind the scenes.

10
00:00:37.590 --> 00:00:44.100
Let's imagine that we have a data frame with two columns column 1 which is categorical A B or C and

11
00:00:44.100 --> 00:00:51.440
then column 2 which is some continuous value what we do is we select a column of categorical values.

12
00:00:51.540 --> 00:00:52.530
So that's step number one.

13
00:00:52.530 --> 00:00:57.300
The column should be categorical set number two is it then split this up.

14
00:00:57.300 --> 00:00:58.530
Based off this category.

15
00:00:58.530 --> 00:01:02.000
So we get all the A's all the B's all the seas and so on.

16
00:01:02.160 --> 00:01:05.540
Then we choose some sort of function to apply.

17
00:01:05.670 --> 00:01:11.640
In this case we'll go ahead and apply a sum in which case we grab all the items in column two and we

18
00:01:11.640 --> 00:01:13.140
go ahead and sum them together.

19
00:01:13.140 --> 00:01:14.550
Per category.

20
00:01:14.550 --> 00:01:16.110
So essentially split.

21
00:01:16.230 --> 00:01:20.640
And then we've applied an aggregate function such as a summation.

22
00:01:20.670 --> 00:01:25.860
After that we recombine everything together to get the final result which is why it's sometimes referred

23
00:01:25.860 --> 00:01:26.910
to as a split.

24
00:01:27.060 --> 00:01:28.420
Apply combine.

25
00:01:28.590 --> 00:01:30.060
So this is what a group buy does.

26
00:01:30.060 --> 00:01:35.730
We choose a categorical column split based off those categories apply the function per category and

27
00:01:35.730 --> 00:01:42.100
then recombine it to get the final result so keep in mind we can choose any sort of aggregation function.

28
00:01:42.240 --> 00:01:46.080
So the operation chosen with the group by call has to be an aggregation method.

29
00:01:46.500 --> 00:01:49.110
So what does an aggregation method actually mean.

30
00:01:49.110 --> 00:01:53.930
Well it means that we can take in multiple values and combine them to return a singular value.

31
00:01:53.940 --> 00:02:00.120
So for example taking the sum of an array or a sum of many values returns back a single value.

32
00:02:00.120 --> 00:02:05.490
Same with a standard deviation or a mean or just counting how many values there are can also take in

33
00:02:05.580 --> 00:02:10.470
as many values as you want you count them up and you return back one single count things such as finding

34
00:02:10.470 --> 00:02:16.050
the max or min are also aggregation functions since they take in many values and return back a single

35
00:02:16.050 --> 00:02:17.040
result.

36
00:02:17.040 --> 00:02:19.860
Let's go ahead and learn how to use Group II with pandas.

37
00:02:19.860 --> 00:02:22.250
Here you have an empty Jupiter notebook.

38
00:02:22.260 --> 00:02:25.090
Keep in mind we're actually going to be reading in a C as we file.

39
00:02:25.140 --> 00:02:33.060
So after you've imported pandas as PD what you need to do is you have to make sure you locate the CSP

40
00:02:33.060 --> 00:02:33.540
file.

41
00:02:33.540 --> 00:02:38.890
So what I would suggest you do is inside the pandas Crash Course folder.

42
00:02:39.060 --> 00:02:40.820
Go ahead and create a new notebook.

43
00:02:40.830 --> 00:02:46.620
I've created one here called Untitled and inside of this folder pan this crash course there's a C as

44
00:02:46.620 --> 00:02:52.730
we file called universities that CSP in order to call it the exact same way I'm going to call it.

45
00:02:52.860 --> 00:02:58.890
We need to do is make sure your notebook is located in the exact same location as the start CSP file.

46
00:02:58.890 --> 00:03:01.740
We're going to cover radiant data and a lot more detail later on.

47
00:03:01.740 --> 00:03:05.570
But for right now to keep things simple underneath your pad this crash course folder.

48
00:03:05.580 --> 00:03:07.150
Go ahead create a new notebook.

49
00:03:07.190 --> 00:03:12.780
Label it untitled and make sure it's in the same directory as its universities that CSC file.

50
00:03:12.780 --> 00:03:16.040
Once you've done that you should be able to run the following command.

51
00:03:16.140 --> 00:03:24.390
DFA is equal to PD that read underscore C S V and then simply provide the name of that and what I suggest

52
00:03:24.390 --> 00:03:28.200
you do is hit tab here and it should autocomplete for you.

53
00:03:28.200 --> 00:03:34.470
So universities that CSC and then run that if it does not tab autocomplete it means your notebook is

54
00:03:34.470 --> 00:03:35.580
in the wrong directory.

55
00:03:35.580 --> 00:03:38.070
So please go back and watch carefully how I set this up.

56
00:03:40.140 --> 00:03:45.700
What I can do is call ahead method on the state of free to show five rows by default.

57
00:03:45.730 --> 00:03:51.680
Now here we have four columns sector university year completions and geography.

58
00:03:51.810 --> 00:03:56.940
It looks like sector University geography and even here are actually categories.

59
00:03:57.060 --> 00:04:02.670
So yea you can think of this as continuous but it's actually just categories such as the year 2016.

60
00:04:02.670 --> 00:04:05.200
Next category year 2017 etc..

61
00:04:05.400 --> 00:04:11.580
So don't be confused that even if a value in a column is not a string it can still be categorical or

62
00:04:11.580 --> 00:04:13.380
treated as category.

63
00:04:13.380 --> 00:04:18.070
So what we're going to do is step one which is called the group by method.

64
00:04:18.120 --> 00:04:24.490
So Def group by and then we decide what column to actually want to group by.

65
00:04:24.530 --> 00:04:30.500
So might be interesting to see the total completions per year across all universities.

66
00:04:30.530 --> 00:04:33.890
So what this data set is actually showing us is the sector of university.

67
00:04:33.890 --> 00:04:38.780
That particular university the year how many completions that particular university had all these universities

68
00:04:38.810 --> 00:04:40.590
are actually located in Nevada.

69
00:04:40.820 --> 00:04:46.520
So we're gonna do is let's go out and see how many completions are happening on average per year so

70
00:04:46.590 --> 00:04:50.150
you can say good bye and then pass in the column.

71
00:04:50.160 --> 00:04:55.800
Now if you just run this what happens is you get back this data frame group by object and what this

72
00:04:55.800 --> 00:05:00.000
means is this group by object is waiting for some extra aggregation call.

73
00:05:00.210 --> 00:05:06.830
So you end up doing is you can do something like Some and this will return back the total number of

74
00:05:06.830 --> 00:05:12.530
completions across each year or maybe you're interested in the average number of completions such as

75
00:05:12.530 --> 00:05:14.690
the mean you can run that.

76
00:05:14.690 --> 00:05:23.210
And notice what happens is the year right here ends up being your index and the columns that can actually

77
00:05:23.240 --> 00:05:24.120
be operated on.

78
00:05:24.500 --> 00:05:28.820
So for example completions those end up being resulted here.

79
00:05:28.820 --> 00:05:33.670
Now you may be wondering why don't I see sector or university or geography.

80
00:05:33.680 --> 00:05:39.800
Well that's because it doesn't actually make sense to calculate the mean of these strings or the mean

81
00:05:39.860 --> 00:05:41.360
of this university.

82
00:05:41.360 --> 00:05:46.340
So keep that in mind that not every column is you're gonna be able to apply this aggregate function

83
00:05:46.340 --> 00:05:47.060
to it.

84
00:05:47.060 --> 00:05:52.250
And again typically what you do is you grouped by some sort of categorical column and then perform the

85
00:05:52.250 --> 00:05:57.140
allegations expecting that to occur on the continuous value columns.

86
00:05:57.150 --> 00:06:02.190
Now keep in mind this also just returns back a data frame and it can confirm that by checking the type

87
00:06:02.190 --> 00:06:02.960
of this.

88
00:06:03.060 --> 00:06:08.720
And here we can see the data frame so there are a lot of possible aggregate functions for you to use

89
00:06:09.140 --> 00:06:15.080
such as Count some mean median etc. and I would encourage you to check out the notebook that we provide

90
00:06:15.080 --> 00:06:16.320
for you in it.

91
00:06:16.550 --> 00:06:17.900
If you actually open it up.

92
00:06:17.900 --> 00:06:25.340
So if I go back here and open up my group by notebook you'll scroll down and eventually you'll see right

93
00:06:25.340 --> 00:06:27.200
here all the other area functions.

94
00:06:27.200 --> 00:06:29.440
You can call so I list them out here for you.

95
00:06:29.570 --> 00:06:32.080
So you can check them out coming back to our notebook.

96
00:06:32.090 --> 00:06:39.030
What I wanted to point out is if I call again maybe mean or let's go ahead and do some.

97
00:06:39.040 --> 00:06:44.280
So we can see the total number of students going through year number completions per year.

98
00:06:44.330 --> 00:06:49.240
It makes sense that it's getting bigger so more students populations grow et cetera.

99
00:06:49.250 --> 00:06:51.340
It should probably be increasing by year.

100
00:06:51.530 --> 00:06:57.020
What I could do is if I wanted the actual index to be in the other way essentially not ascending but

101
00:06:57.020 --> 00:07:08.900
descending I could say here dot sort underscore index and then say ascending is equal to False run that

102
00:07:09.080 --> 00:07:12.170
now actually sort based off the index.

103
00:07:12.170 --> 00:07:18.890
OK so we can also group by multiple columns and that essentially gives us the SEC multi tiered index.

104
00:07:18.890 --> 00:07:23.410
So let's take a look at our data frame again.

105
00:07:23.530 --> 00:07:29.440
So here you have the sector the university that year what I could do is maybe group by the year and

106
00:07:29.440 --> 00:07:31.810
then group by the sector.

107
00:07:31.810 --> 00:07:38.490
So what I could do is something like this the F group by passing a list of what I want to group by.

108
00:07:38.650 --> 00:07:44.690
So my outer grouping to be year and then my inner grouping to be by sector.

109
00:07:45.040 --> 00:07:51.070
And then let's go ahead and check the sum so I can see the actual number of completions per year per

110
00:07:51.070 --> 00:07:51.940
sector.

111
00:07:51.970 --> 00:07:58.240
Keep in mind this data frame actually only has one column completions and now has a multi tiered index

112
00:07:58.330 --> 00:08:04.110
or a multi hierarchy index or the outer index is year and the inner index is called sector.

113
00:08:04.420 --> 00:08:09.860
So keep that in mind we won't really be dealing that much with multi tier indices throughout this course.

114
00:08:09.880 --> 00:08:15.010
But if you perform a group by where you're calling multiple columns that will be the result.

115
00:08:15.010 --> 00:08:20.530
Finally I want to show you how you can easily combine group by with The described method for a very

116
00:08:20.530 --> 00:08:24.260
quick but useful summary statistics across a category.

117
00:08:24.520 --> 00:08:32.290
So we can call the F group by year and then just say describe and recall describe returns back the actual

118
00:08:32.380 --> 00:08:37.950
summary statistics across a variety of things like count mean standard deviation min max etc..

119
00:08:37.960 --> 00:08:44.260
So here we get this nice table and again what happens here is now we actually have a multi tier across

120
00:08:44.320 --> 00:08:45.290
our columns.

121
00:08:45.430 --> 00:08:51.280
So we have this overall completions and then different columns like count mean standard deviation.

122
00:08:51.280 --> 00:08:59.700
What I like to do however is call a transpose on this run that and this actually then makes the categories

123
00:08:59.820 --> 00:09:02.400
or the year the columns themselves.

124
00:09:02.400 --> 00:09:05.020
And then we can see this completions index.

125
00:09:05.070 --> 00:09:09.740
So I personally prefer this method of calling that transpose after calling describe.

126
00:09:10.020 --> 00:09:15.480
But the main thing I want to point out is not describe is very useful of group buy to quickly get some

127
00:09:15.630 --> 00:09:20.020
aggregate summary statistics across particular categories in your dataset.

128
00:09:20.100 --> 00:09:25.290
The main thing you should take away from this lecture is you called the F group by on some categorical

129
00:09:25.290 --> 00:09:31.110
column and then call some sort of method to be done across the continuous values such as Max min or

130
00:09:31.110 --> 00:09:33.920
if you want to run a gamut of summary statistics.

131
00:09:33.930 --> 00:09:37.940
Then just call dot describe and for convenience you can always transpose this.

132
00:09:38.220 --> 00:09:39.840
Thanks and I'll see you at the next lecture.
