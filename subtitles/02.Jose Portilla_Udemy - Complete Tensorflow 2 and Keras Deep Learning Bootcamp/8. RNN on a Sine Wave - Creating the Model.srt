1
00:00:05,520 --> 00:00:07,620
OK here we are where we left off last time.

2
00:00:07,620 --> 00:00:13,260
Previously we learned how the time series generator works and what we're going to do is we'll go ahead

3
00:00:13,590 --> 00:00:19,770
and make sure we create a generator object for us to train off of and what we want to do is make sure

4
00:00:19,770 --> 00:00:25,620
that we get basically a full cycle or full seasonality within one length of one of our batches.

5
00:00:25,800 --> 00:00:31,650
So we know that there's about five hundred points between zero and 50 and we can see here that a cycle

6
00:00:31,650 --> 00:00:36,330
goes from about zero to maybe a little greater than five.

7
00:00:36,360 --> 00:00:43,650
So we think about this we probably want at least somewhere between 50 and 100 points within our batches.

8
00:00:43,650 --> 00:00:45,350
So let's go ahead and just say 50 here.

9
00:00:45,930 --> 00:00:48,610
So say 50 and we'll send one batch at a time.

10
00:00:48,750 --> 00:00:50,470
We'll go ahead and create this generator.

11
00:00:50,470 --> 00:00:52,560
And now it's time to create our model.

12
00:00:52,800 --> 00:01:02,380
We will say from tensor flow that care stop models import sequential and then we'll say from tensor

13
00:01:02,380 --> 00:01:07,720
flow dark hair start layers import.

14
00:01:08,170 --> 00:01:13,330
And we'll be using a dense layer and we'll begin by using a simple are an end layer and then we'll expand

15
00:01:13,330 --> 00:01:15,190
this to an LSD him.

16
00:01:15,940 --> 00:01:20,560
OK so the last we need to define is how many features are we actually training off of.

17
00:01:20,590 --> 00:01:25,300
And in our case we just have one feature input which is x trying to predict Y.

18
00:01:25,510 --> 00:01:27,890
So the number of features is just one.

19
00:01:28,270 --> 00:01:35,290
And then to begin creating our models say models equal to sequential and then we'll say model the ad

20
00:01:36,010 --> 00:01:42,070
and we'll add in a simple aren't N and the simple aren't N has a lot of parameter calls that you can

21
00:01:42,070 --> 00:01:46,810
pass in but here we'll just focus on the basics such as the number of units how many neurons are here

22
00:01:47,230 --> 00:01:50,600
and then the other important one is the input shape.

23
00:01:50,680 --> 00:01:54,750
So you actually can't see here but we have to define what the actual input shape.

24
00:01:54,850 --> 00:01:58,990
And luckily that's actually pretty easy given that we've already defined the generator.

25
00:01:58,990 --> 00:02:04,480
So the actual number of neurons you want it somehow related to the length of the batches since I'm passing

26
00:02:04,480 --> 00:02:05,200
in 50.

27
00:02:05,230 --> 00:02:11,530
Let's go ahead and just choose 50 neurons here and then the other thing I need to put in here is the

28
00:02:11,530 --> 00:02:16,760
input shapes so we'll say input shape is equal to.

29
00:02:16,790 --> 00:02:25,130
And we'll say length by number of features and then after this we want our final prediction to just

30
00:02:25,130 --> 00:02:26,570
be a single number.

31
00:02:26,600 --> 00:02:32,090
So say that all goes into a single dense output and then we will compile this.

32
00:02:32,090 --> 00:02:36,980
And this is essentially a continuous value or continuous label.

33
00:02:36,980 --> 00:02:42,530
So we'll say optimizer is equal to atom but more importantly our lost function is just going to be mean

34
00:02:42,530 --> 00:02:47,510
square error just as we always do for continuous values then go ahead and run that.

35
00:02:48,020 --> 00:02:52,760
And then your model should be created and you can confirm a summary or a model by saying model that

36
00:02:52,760 --> 00:02:56,180
summary and he can see the different layers.

37
00:02:56,180 --> 00:03:01,370
So what we're going to do is instead of saying models that fit since we're using a generator we say

38
00:03:01,370 --> 00:03:09,010
models that fit to generator and then we pass in the generator that we just created and then we choose

39
00:03:09,100 --> 00:03:11,020
a specific number of epochs.

40
00:03:11,020 --> 00:03:15,520
And later on in this lecture we'll show you had actually implement early stopping by creating an early

41
00:03:15,520 --> 00:03:22,000
stopping generator based off your validation set but for this I'll go ahead and just train this for

42
00:03:22,540 --> 00:03:26,250
let's say five epochs run this.

43
00:03:26,320 --> 00:03:31,690
And keep in mind recurrent neural networks they definitely take a lot longer to train than typical artificial

44
00:03:31,690 --> 00:03:32,550
neural networks.

45
00:03:32,590 --> 00:03:35,200
So expect this to take quite a bit longer.

46
00:03:35,200 --> 00:03:42,910
Even if our only training for five epochs I'm going to fast forward in time until this is done training.

47
00:03:42,940 --> 00:03:47,320
All right I just fast forward in time and now I fit in five epochs.

48
00:03:47,330 --> 00:03:49,250
Let's go ahead and evaluate our losses.

49
00:03:49,250 --> 00:03:52,260
We can do this by simply saying losses equal to PD.

50
00:03:52,630 --> 00:04:00,970
That data frame model history that history and then Let's plot these out

51
00:04:04,080 --> 00:04:08,260
and we can see here we're definitely reducing on the first couple of epochs and almost looks like we're

52
00:04:08,280 --> 00:04:13,560
increasing so later on we'll see how we can compare this to validation data and probably institute an

53
00:04:13,590 --> 00:04:15,190
early stopping mechanism.

54
00:04:15,300 --> 00:04:18,750
But let's go ahead and evaluate on our test data.

55
00:04:18,960 --> 00:04:26,360
So to do this I'm going to show you our logic here before we write it into a for loop.

56
00:04:26,520 --> 00:04:32,850
So let's imagine what is the first batch I should be passing in to my model to predict something on

57
00:04:32,850 --> 00:04:34,070
the test set.

58
00:04:34,110 --> 00:04:44,010
Well that should be coming from my scaled training data all the way from minus length to the end.

59
00:04:44,010 --> 00:04:49,980
Essentially we're going to take the last twenty five points from our training set to predict one point

60
00:04:49,980 --> 00:04:53,150
into the future which is the very first point in our test set.

61
00:04:54,060 --> 00:04:59,280
So I have my first evaluation batch which I can take a look at if I want and I can see it here.

62
00:04:59,370 --> 00:05:05,070
And essentially what I do is in order to pass this into the Model X need to reshape it to the shape

63
00:05:05,520 --> 00:05:15,990
the model expects which to generalize it will be one by length by any number of features and then we'll

64
00:05:15,990 --> 00:05:22,810
say first if our batch is equal to the reshaping of it and now the model should be able to predict this

65
00:05:22,840 --> 00:05:29,270
will say model predict one point into the future given the last twenty five points in the training data.

66
00:05:29,440 --> 00:05:35,500
So its model I predict first if our batch we run that and we get back our prediction.

67
00:05:35,500 --> 00:05:43,840
Now we should recall that we are predicting scaled predictions so far so I can compare this to my scaled

68
00:05:44,020 --> 00:05:49,030
test point and this is the very first point here in my scale test and we can see we're actually doing

69
00:05:49,030 --> 00:05:50,020
pretty good.

70
00:05:50,020 --> 00:05:56,160
I predicted zero point nine for eight and my first scale test point is zero point nine for nine.

71
00:05:56,170 --> 00:05:57,860
So definitely not bad.

72
00:05:58,130 --> 00:06:04,600
Well we need to do is we need to put this logic now in a for loop to keep moving essentially 25 points

73
00:06:04,990 --> 00:06:10,540
and as we keep predicting we'll add an additional predicted point until we're essentially predicting

74
00:06:10,600 --> 00:06:12,970
or forecasting off predictions.

75
00:06:12,970 --> 00:06:17,380
So what I will do is essentially create all this logic that we did here in these four cells and put

76
00:06:17,380 --> 00:06:19,350
it into a for loop.

77
00:06:19,390 --> 00:06:30,810
So what I want to do is say my test predictions is an empty list and then I have my first evaluation

78
00:06:30,810 --> 00:06:40,980
batch equal to that scale training data where I have minus length all the way to the end and then my

79
00:06:41,190 --> 00:06:49,140
current batch that essentially I am currently using will start off as my first evaluation batch reshaped

80
00:06:49,950 --> 00:07:01,140
to 1 length and features and then what I need to do is essentially create a for loop that keeps appending

81
00:07:01,150 --> 00:07:05,710
this but something I want to show you is if I run this.

82
00:07:06,040 --> 00:07:11,530
Notice that my first evaluation batch is right here.

83
00:07:11,740 --> 00:07:18,140
But the shape of my current batch essentially after I reshape this looks like this.

84
00:07:18,140 --> 00:07:26,390
Notice the extra arrays and what I need to be doing is I need to keep appending all this data except

85
00:07:26,420 --> 00:07:32,990
for that first original point because I'm replacing that with my new predicted point at the end.

86
00:07:33,050 --> 00:07:35,150
Essentially moving one step forward.

87
00:07:35,180 --> 00:07:39,980
So what I'm going to be doing and this is where the code gets a little complex because it's a lot of

88
00:07:39,980 --> 00:07:43,120
num pi indexing but I will be running this command.

89
00:07:43,130 --> 00:07:51,770
I will be saying end p append current batch and this is where the indexing gets really complicated.

90
00:07:51,780 --> 00:07:56,410
So watch carefully will pass in the following commands.

91
00:07:56,410 --> 00:08:01,480
Essentially saying based off the dimensions grab everything the first I and everything in this last

92
00:08:01,480 --> 00:08:08,530
dimension but in the actual dimension that's holding the data go from index 1 all the way to the end

93
00:08:08,800 --> 00:08:15,010
essentially dropping that first original point because we don't need it anymore since we're not actually

94
00:08:17,050 --> 00:08:17,630
using it.

95
00:08:17,830 --> 00:08:20,670
And so I will then put in my predicted point.

96
00:08:20,710 --> 00:08:23,200
So imagine we have some predicted value.

97
00:08:23,200 --> 00:08:24,930
This is just to make it obvious.

98
00:08:25,000 --> 00:08:27,660
Let's say my predicted value is ninety nine.

99
00:08:27,670 --> 00:08:32,500
Notice I have three braces since that's going to be the same format that predictions are in.

100
00:08:32,560 --> 00:08:34,040
If we take a look here.

101
00:08:34,090 --> 00:08:37,330
Notice I have two braces and then my number.

102
00:08:37,420 --> 00:08:43,140
So it's gonna happen is I need it to have an extra set of brackets here so I can use N.P. append.

103
00:08:43,330 --> 00:08:55,620
So if I say here ninety nine along axis is equal to 1 so make the say this is my predicted value in

104
00:08:55,620 --> 00:08:57,330
passing in it right here.

105
00:08:57,330 --> 00:09:06,290
Go ahead and run this and let's compare this now from our current batch to our first evaluation batch.

106
00:09:06,390 --> 00:09:15,470
So our first evaluation batch goes eight point three seven seven point nine eight what I want in my

107
00:09:15,470 --> 00:09:20,620
current batch as I keep moving is go ahead and drop the very first point.

108
00:09:20,930 --> 00:09:22,710
And now I'm at seven point nine eight.

109
00:09:22,830 --> 00:09:26,780
So notice I dropped eight point three seven and move straight here.

110
00:09:26,780 --> 00:09:27,830
And after dropping that.

111
00:09:27,890 --> 00:09:30,180
Go ahead and add our predicted point to the end.

112
00:09:30,190 --> 00:09:36,050
So instead of ending at nine point two five we scroll down here we go from nine point two five to that

113
00:09:36,110 --> 00:09:38,300
ninety nine that I inserted.

114
00:09:38,300 --> 00:09:42,080
So a lot of this essentially isn't that complicated.

115
00:09:42,080 --> 00:09:45,530
The complex part is actually implementing it with NUM pi.

116
00:09:45,590 --> 00:09:51,980
So all this code is doing is it saying go ahead and move that current batch along by one time step which

117
00:09:51,980 --> 00:09:57,110
we talked about in the theory lectures that we just keep moving this forward and forward the complex

118
00:09:57,110 --> 00:10:02,480
part is because we're dealing with so many dimensions the calls are actually kind of a little weird

119
00:10:02,730 --> 00:10:08,270
you only grabbing from one here all the way to the end and the middle dimension and then you're gonna

120
00:10:08,270 --> 00:10:15,140
have to set the predicted value with a set amount of dimensions because it's technically three dimensional

121
00:10:15,890 --> 00:10:17,050
because it's a tensor.

122
00:10:17,210 --> 00:10:22,750
And then we append along X equal to one because we're spending along the information axis here.

123
00:10:22,760 --> 00:10:25,040
Okay so I know that's a little bit complicated.

124
00:10:25,050 --> 00:10:29,810
I just wanted to kind of walk through that logic and then we're going to essentially make a for loop

125
00:10:29,810 --> 00:10:30,910
out of that logic.

126
00:10:31,070 --> 00:10:36,980
So I'm going to copy and paste what I have here because this actually doesn't change the initiation

127
00:10:37,730 --> 00:10:43,670
of the logic doesn't change what we do then it was we say for I in range and we're gonna do this for

128
00:10:43,670 --> 00:10:52,000
the entire test that will say for length the test the go ahead and say my current prediction based off

129
00:10:52,420 --> 00:11:00,700
the current batch is model that predict current batch and I'm going to index an item off of that to

130
00:11:00,700 --> 00:11:06,700
be my current prediction and then I will take my test predictions which remember starts off as an empty

131
00:11:06,700 --> 00:11:15,270
list and I simply append my current prediction and what I have to do is then move my current batch forward

132
00:11:15,360 --> 00:11:21,570
one time step and that's what we do is kind of complicated logic here so we'll copy this and say and

133
00:11:21,660 --> 00:11:27,720
update my current batch is equal to NPA pen current batch.

134
00:11:27,870 --> 00:11:33,840
This stays the same except when I'm inserting is not some arbitrary value of ninety nine instead it's

135
00:11:33,840 --> 00:11:41,420
going to be notice I'm deleting the ninety nine and the braces and say my current prediction.

136
00:11:41,580 --> 00:11:45,900
So keep a really careful eye here on the braces in fact I would highly recommend use copy and paste

137
00:11:45,900 --> 00:11:47,020
this from our notes.

138
00:11:47,070 --> 00:11:51,720
So what I'm doing here is to walk you through it is I have an entry list for my test predictions.

139
00:11:51,810 --> 00:11:56,910
I grab my first evaluation batch which is the last essentially in our case twenty five points of the

140
00:11:56,910 --> 00:12:01,530
training batch and then I reshape it to make sure my model can accept it based off the length the number

141
00:12:01,530 --> 00:12:05,000
of features and then I set that to my current batch.

142
00:12:05,040 --> 00:12:06,930
I do my very first prediction.

143
00:12:06,930 --> 00:12:09,190
That's my current prediction right here.

144
00:12:09,390 --> 00:12:15,900
And then what I'm going to do is I say OK go ahead and grab that current prediction and dependent to

145
00:12:15,900 --> 00:12:16,730
that list.

146
00:12:16,860 --> 00:12:22,080
But now to keep predicting into the future I have to I have to move my current batch one step forward

147
00:12:22,440 --> 00:12:28,500
which is essentially what this does is it gets rid of that first item and instead replaces that by adding

148
00:12:28,500 --> 00:12:31,210
the current prediction all the way to the very end.

149
00:12:31,270 --> 00:12:37,530
Now if I run this for loop what's going to happen is I should essentially get now pure test predictions

150
00:12:37,530 --> 00:12:38,910
by the end.

151
00:12:38,910 --> 00:12:43,380
So this is now my predictions for the entire test range.

152
00:12:43,380 --> 00:12:44,080
OK.

153
00:12:44,190 --> 00:12:51,600
And now I'm going to essentially be comparing that to my scaled test data and we'll hopefully see it

154
00:12:51,600 --> 00:12:52,790
work well.

155
00:12:52,830 --> 00:12:57,060
Remember that this is scaled so let's go ahead and inverse that transformation.

156
00:12:57,060 --> 00:13:06,180
We can do this easily by saying my true predictions are equal to scalar dot inverse transform these

157
00:13:06,180 --> 00:13:09,030
test predictions that we just created.

158
00:13:09,550 --> 00:13:12,330
And now my true predictions are in the right scale.

159
00:13:12,370 --> 00:13:19,830
OK so once that's done all I need to do is figure out ways compare this recall that I still have my

160
00:13:19,830 --> 00:13:22,450
test data frame with that test set.

161
00:13:22,470 --> 00:13:30,540
So what I can do is say add in a column called predictions and set that equal to true predictions.

162
00:13:30,540 --> 00:13:35,040
Keep in mind you may get a warning about something saying a value is trying to be set on a copy of a

163
00:13:35,040 --> 00:13:35,740
slice.

164
00:13:35,830 --> 00:13:40,740
It's essentially warning you hey you're editing permanently your current test data frame so you can

165
00:13:40,740 --> 00:13:41,880
go ahead and ignore that warning.

166
00:13:41,940 --> 00:13:43,470
That's actually the behavior we want.

167
00:13:43,590 --> 00:13:47,430
It's essentially painless telling you hey you're editing this permanently by running this but that's

168
00:13:47,430 --> 00:13:48,170
exactly what I want.

169
00:13:48,180 --> 00:13:50,470
I want something that looks like this.

170
00:13:50,580 --> 00:13:53,980
And then once that's done let's go ahead and plot it out and see how he did.

171
00:13:53,970 --> 00:14:00,960
We'll say test that plot and we can expand on this figure size a little bit just so it's clear and we'll

172
00:14:00,960 --> 00:14:05,280
say oops fig size is equal to twelve by eight.

173
00:14:05,280 --> 00:14:08,540
Go ahead and run that and you should see something that looks like this.

174
00:14:08,550 --> 00:14:14,220
Notice our predictions are definitely not perfect but it's really interesting to see that our model

175
00:14:14,310 --> 00:14:19,450
is actually able to start to mimic the behavior of a sine wave.

176
00:14:19,470 --> 00:14:24,510
However you'll notice that as we're predicting further and further into the future our values are getting

177
00:14:24,510 --> 00:14:29,200
further and further off which makes sense because we're predicting off predictions.

178
00:14:29,220 --> 00:14:34,310
So this very first point should be the most accurate because we're using real data those last twenty

179
00:14:34,310 --> 00:14:39,760
five turning points to predict my first point into the test set and then as I move to the next point.

180
00:14:39,900 --> 00:14:46,110
Now I'm using twenty four points in the train data plus my predicted point in the test range to predict

181
00:14:46,110 --> 00:14:50,940
a new point and then I'm using two predictions to predict the next point then three plus the old ones

182
00:14:51,240 --> 00:14:57,230
until all the way to the end where I'm using essentially twenty four train points or excuse me twenty

183
00:14:57,230 --> 00:15:02,710
four points in my test range that I predicted plus one original trend data predict the last one.

184
00:15:02,850 --> 00:15:07,050
So it kind of makes sense that we're getting further and further off because we're doing essentially

185
00:15:07,050 --> 00:15:10,950
true forecasting at the very end but our behavior is looking pretty good.

186
00:15:10,980 --> 00:15:14,230
There's a pretty smooth prediction line and it looks like a sine wave.

187
00:15:14,700 --> 00:15:19,440
So coming up next we're going to see if we can improve on this by using an Ellis team unit.

188
00:15:19,440 --> 00:15:20,040
I'll see you there.
