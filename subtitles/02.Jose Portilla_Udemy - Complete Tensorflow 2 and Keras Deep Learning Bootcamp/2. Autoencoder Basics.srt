1
00:00:05,540 --> 00:00:06,660
Welcome back everyone.

2
00:00:06,710 --> 00:00:13,050
In this lecture we'll be discussing the very basics of the auto encoder network architecture so the

3
00:00:13,050 --> 00:00:17,820
auto encoder is actually a very simple neural network and we'll feel very similar to the multilayer

4
00:00:17,820 --> 00:00:22,760
perception models that we built during the artificial neural network section of the course.

5
00:00:23,010 --> 00:00:29,190
And the main idea is that an auto encoder is designed to simply reproduce its input at the output layer

6
00:00:30,480 --> 00:00:36,510
so that means that the key difference between an auto encoder and the typical MLP multilayer perception

7
00:00:36,600 --> 00:00:42,120
or artificial neural network is that the number of input neurons will actually be equal to the exact

8
00:00:42,120 --> 00:00:45,960
same number of output neurons typically in our use cases.

9
00:00:45,960 --> 00:00:51,960
We've been either having the output be equal to the certain number of classes were classified four or

10
00:00:52,170 --> 00:00:53,390
equal to one neuron.

11
00:00:53,400 --> 00:00:59,310
If we're dealing with a continuous output however in kind of a strange case here for that I don't coder

12
00:00:59,580 --> 00:01:03,310
the number of input neurons is exactly equal to the number of output neurons.

13
00:01:03,900 --> 00:01:09,450
Let's explore what this actually looks like and why we would even want to build a network this way.

14
00:01:09,580 --> 00:01:12,400
Here we can see an example auto encoder.

15
00:01:12,430 --> 00:01:19,540
Notice the shape we start with five neurons and then we slowly reduces down to a certain specified number

16
00:01:19,540 --> 00:01:21,840
of neurons throughout the hidden layers.

17
00:01:21,880 --> 00:01:27,340
So you go from five to three to two and the main idea here is right in the middle of your auto encoder

18
00:01:27,610 --> 00:01:34,090
you're going to reduce to some desired amounts of neurons essentially trying to capture what information

19
00:01:34,150 --> 00:01:35,200
is important.

20
00:01:35,200 --> 00:01:40,960
And then in the second half of the auto encoder essentially the decoding part you expand this back out

21
00:01:41,290 --> 00:01:45,130
to equal the same number of neurons that you started with.

22
00:01:45,130 --> 00:01:49,960
So we go from five to three to two and then back out to three to five again.

23
00:01:49,960 --> 00:01:54,910
So we start at five and then after going through the entire auto encoder we actually end up back at

24
00:01:54,910 --> 00:02:01,490
five so when we walk through these layers and explain this basic idea and to simplify this even further

25
00:02:01,730 --> 00:02:09,010
let's imagine that we just have a single hidden layer to go directly 5 to 2 to 5 so the idea here is

26
00:02:09,010 --> 00:02:15,700
that in order to produce the same output at the final layer that internal hidden layer or specifically

27
00:02:15,700 --> 00:02:21,610
the internal multiple hidden layers for a real auto encoder example those hidden layers must actually

28
00:02:21,610 --> 00:02:23,220
learn what features are important.

29
00:02:23,650 --> 00:02:29,890
And this kind of extreme example basically shows you that in order for this input to be reproduced as

30
00:02:29,950 --> 00:02:36,220
output those two neurons in the hidden layer they're going to have to learn which inputs are actually

31
00:02:36,220 --> 00:02:38,590
important in order to reproduce the output.

32
00:02:39,100 --> 00:02:42,320
So they're reducing the amount of dimensions from five to two.

33
00:02:43,150 --> 00:02:48,730
So again here we see a design of five dimensions reduced to two dimensions and then expand that back

34
00:02:48,730 --> 00:02:50,360
out to the original five.

35
00:02:50,380 --> 00:02:52,500
Notice my use of the word dimensions.

36
00:02:52,530 --> 00:02:56,430
I'm actually saying the word features here OK.

37
00:02:56,560 --> 00:03:01,630
So again the feed for network trained to produce its input at the output layer is all an auto encoder

38
00:03:01,630 --> 00:03:07,150
is we have some input goes and gets reduced to a hidden layer in the middle and then expand it back

39
00:03:07,150 --> 00:03:13,650
out to the outputs the output size again is the same as the input layer and that hidden representation

40
00:03:13,650 --> 00:03:14,540
in the middle.

41
00:03:14,550 --> 00:03:20,250
Attempts to maintain the important input information before being decoded or expand that backup.

42
00:03:21,540 --> 00:03:26,940
And then later on we'll see how we can actually take advantage of that hidden layer to extract meaningful

43
00:03:26,940 --> 00:03:27,930
insights.

44
00:03:27,930 --> 00:03:32,970
So when we do things like dimensionality reduction we'll actually be able to train a full auto coder

45
00:03:33,150 --> 00:03:37,620
and then split it in half and directly extract results from that middle hidden layer.

46
00:03:37,650 --> 00:03:43,080
But the full auto encoder is actually inputs shrunken down to hidden and then expanded back out outputs

47
00:03:44,370 --> 00:03:48,690
so this idea is actually extremely similar to principal component analysis.

48
00:03:48,690 --> 00:03:55,740
We are trying to reduce the dimensionality into a few principal components and keep in mind we can explore

49
00:03:55,740 --> 00:04:01,200
stack auto encoders if more hidden layers will actually be doing this ourselves throughout the exercise.

50
00:04:01,200 --> 00:04:04,980
Notebooks now a really important note.

51
00:04:05,110 --> 00:04:09,700
A lot of students get confused by this term reduction in dimensionality.

52
00:04:09,700 --> 00:04:15,850
Keep in mind those hidden layers is not simply sub selecting only certain features so it's not like

53
00:04:15,850 --> 00:04:21,370
you start off with 10 features and then choose eight of those 10 to continue on with what you're actually

54
00:04:21,370 --> 00:04:27,820
doing is you're calculating combinations of the original features to represent the original data in

55
00:04:27,820 --> 00:04:30,080
a reduced dimensional space.

56
00:04:30,130 --> 00:04:35,140
So again you're not sub selecting features you're actually taking a little bit of all the features some

57
00:04:35,140 --> 00:04:38,060
percent depending on the way the auto coders train.

58
00:04:38,110 --> 00:04:44,740
So for example you make take 5 percent of feature 1 maybe 10 percent of feature 2 et cetera and you

59
00:04:44,740 --> 00:04:51,760
decide how important is each feature as you slowly reduce this dimensionality down and then that hopefully

60
00:04:51,820 --> 00:04:57,040
that hidden layer in the middle is going to learn which features are the most important and how to organize

61
00:04:57,040 --> 00:05:01,380
them in a way to reduce the dimensionality and then to check if it's working or not.

62
00:05:01,600 --> 00:05:05,690
We decode or expand the spec out to the output layer.

63
00:05:05,980 --> 00:05:12,080
So again we're not sub selecting only certain features so what is sexual look like in practice.

64
00:05:12,080 --> 00:05:17,660
Well later on in the next series of lectures we're going to explore how we can actually use an auto

65
00:05:17,660 --> 00:05:18,220
encoder.

66
00:05:18,230 --> 00:05:24,500
Split it up into two parts a encoder and a decoder to perform dimensionality reduction.

67
00:05:24,500 --> 00:05:29,750
So we'll be able to actually take a three dimensional data set and then reduce it down to either 2 or

68
00:05:29,750 --> 00:05:31,370
even 1 dimensions.

69
00:05:31,380 --> 00:05:37,430
So notice here we have these two clusters and we're actually able to map them to a lower dimensionality

70
00:05:37,430 --> 00:05:44,140
space or lower dimensional space and still maintain that separation in this particular use case of going

71
00:05:44,140 --> 00:05:45,720
down from 3 to 2.

72
00:05:45,760 --> 00:05:48,220
That may not look actually very useful to you.

73
00:05:48,280 --> 00:05:52,870
However you can imagine that we've been dealing with datasets that are much much larger than just three

74
00:05:52,870 --> 00:05:53,770
features.

75
00:05:53,770 --> 00:06:00,940
So if you have a dataset that is maybe 20 or 30 features something very large you can visualize 20 or

76
00:06:00,940 --> 00:06:07,570
30 features because we can only visualize three dimensional space but which could do is reduce those

77
00:06:07,570 --> 00:06:14,630
20 or 30 features into some lower dimensional space like 2D or 3D and then visualize that.

78
00:06:14,680 --> 00:06:18,720
And then that will you to see just how clearly separated those classes are.

79
00:06:18,820 --> 00:06:24,220
Here we can see that in three dimensions yellow and purple here have clear separation and that clear

80
00:06:24,220 --> 00:06:28,460
separation is maintained as we reduce this down dimensionality wise.

81
00:06:28,600 --> 00:06:33,820
You can imagine that if we start off with a high high dimension dataset that was too highly dimensional

82
00:06:33,820 --> 00:06:39,160
to visualize we could reduce this down to 3 the or to the and then visualize the classes and see just

83
00:06:39,160 --> 00:06:44,800
how separable our dataset is and then later on we'll see how we can apply this to just reveal hidden

84
00:06:44,800 --> 00:06:46,540
insights from our data.

85
00:06:46,540 --> 00:06:53,040
So the main idea behind an Auto encoder is the center hidden layer reduce the dimensionality to learn

86
00:06:53,050 --> 00:06:58,120
the most important combinations of original features and we're gonna see two applications of the auto

87
00:06:58,120 --> 00:07:03,340
encoder and they both use an auto encoder but use them in very different ways which is why this is such

88
00:07:03,340 --> 00:07:08,650
an interesting network architecture because even though you have the same network architecture of inputs

89
00:07:08,650 --> 00:07:14,080
being reduced and then matching the output size you can use them in a wide variety of tasks.

90
00:07:14,200 --> 00:07:15,990
So we're gonna split to use cases.

91
00:07:16,090 --> 00:07:20,770
One is the more obvious use case of dimensionality reduction where you essentially after training an

92
00:07:20,800 --> 00:07:26,510
auto encoder split it in half and just extract the hidden layer outputs and then the second use case.

93
00:07:26,510 --> 00:07:28,740
There's a really interesting one for noise removal.

94
00:07:28,810 --> 00:07:34,120
You'll actually have that hidden layer essentially learn how to filter out noise when reproducing the

95
00:07:34,120 --> 00:07:34,620
output.

96
00:07:35,170 --> 00:07:39,690
OK so both of these use out encoders but these are really different ways and they're super interesting.

97
00:07:39,700 --> 00:07:40,820
Problem sets.

98
00:07:40,840 --> 00:07:46,810
So let's go ahead and in the next lecture learn how we can detach an auto encoder into two separate

99
00:07:46,810 --> 00:07:50,320
components in order to perform dimensionality reduction.

100
00:07:50,320 --> 00:07:50,920
I'll see it there.
