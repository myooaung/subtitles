WEBVTT
1
00:00:05.380 --> 00:00:06.540
Welcome back everyone.

2
00:00:06.550 --> 00:00:11.920
In this lecture we're going to briefly discuss the idea behind two main issues and those are explored

3
00:00:11.920 --> 00:00:17.520
ingredients and vanish ingredients as our networks grow deeper and more complex.

4
00:00:17.520 --> 00:00:22.290
There's two issues that arise and again as I mentioned those are explored ingredients and varnish ingredients

5
00:00:22.650 --> 00:00:27.590
and recall that the gradient is used in our calculation to adjust weights and biases in our network.

6
00:00:27.690 --> 00:00:33.630
As we discussed in the back propagation theory lectures so let's think about just the basic feed forward

7
00:00:33.630 --> 00:00:35.700
network in a feed for network.

8
00:00:35.700 --> 00:00:39.240
Here we have the input layer over on the left hand side.

9
00:00:39.240 --> 00:00:44.580
We feed it through some hidden layers and then we have our final output layer on the right.

10
00:00:44.940 --> 00:00:50.760
Now for complex data we've already encountered complex data such as complex image data or we're gonna

11
00:00:50.760 --> 00:00:50.920
do.

12
00:00:50.930 --> 00:00:54.060
This section is work with complex sequence data.

13
00:00:54.060 --> 00:00:55.700
We end up needing deeper network.

14
00:00:55.710 --> 00:01:01.400
So we need more hidden layers in order to actually learn the patterns that are in our data.

15
00:01:01.430 --> 00:01:07.220
Now what happens is there's this vanishing or exploding gradient issue that arises during the back propagation

16
00:01:07.220 --> 00:01:07.970
step.

17
00:01:07.970 --> 00:01:12.710
So recall that we're going to calculate some sort of lost metric on the output layer and then back propagate

18
00:01:12.710 --> 00:01:15.220
our error all the way back to the input layer.

19
00:01:15.350 --> 00:01:20.930
And if we're have lots of hidden layers then we're having the update to the weights and biases be a

20
00:01:20.930 --> 00:01:27.780
function of many many other derivatives that we're calculating along the way back so back propagation

21
00:01:27.780 --> 00:01:33.000
is going backwards from the output layer to the employer propagating that error gradient and for deeper

22
00:01:33.000 --> 00:01:35.850
networks the issues can arise from back propagation.

23
00:01:35.850 --> 00:01:39.220
And again those are vanishing and exploding gradients.

24
00:01:39.290 --> 00:01:44.030
So as you go back to those lower layers close to the input layer the gradients are often actually getting

25
00:01:44.030 --> 00:01:44.590
smaller.

26
00:01:44.600 --> 00:01:48.500
So a vanishing gradient is usually the more common problem although they can technically explode on

27
00:01:48.500 --> 00:01:49.280
the way back.

28
00:01:49.460 --> 00:01:54.440
But as you're typically going back and back closer to those input layers there's gradients are getting

29
00:01:54.440 --> 00:01:59.420
smaller and eventually what happens is they're so small by the time he gets the input layer that the

30
00:01:59.420 --> 00:02:02.900
weights never really change that much at those lower input levels.

31
00:02:02.900 --> 00:02:07.910
And that's actually a big problem because we want to be able to detect large basic patterns in our data

32
00:02:08.240 --> 00:02:13.730
right up close to the input layer and have the deeper layers focus on smaller details or in smaller

33
00:02:13.730 --> 00:02:15.020
patterns.

34
00:02:15.020 --> 00:02:19.040
And again the opposite can also occur that the grants and the exploding on the way back causing these

35
00:02:19.040 --> 00:02:24.230
issues so let's discuss why this is actually occurring and how we can fix it.

36
00:02:24.230 --> 00:02:28.760
And then in the next lecture we'll discuss how these issues specifically affect the recurrent Neel networks

37
00:02:28.970 --> 00:02:34.070
and how we can use the long short term memory unit and get a recurrent unit to also fix this to understand

38
00:02:34.070 --> 00:02:35.410
why this is actually happening.

39
00:02:35.420 --> 00:02:40.580
Let's take a look at a really common activation function such as the sigmoid activation function and

40
00:02:40.580 --> 00:02:46.340
we also know that other activation functions such as logarithmic SOF Max actually use sigmoid as part

41
00:02:46.340 --> 00:02:47.210
of their calculation.

42
00:02:47.270 --> 00:02:53.240
So this is also relevant to that now as we know the sigmoid activation function basically squeezes any

43
00:02:53.240 --> 00:02:54.220
input.

44
00:02:54.230 --> 00:03:00.290
So here we can see the input on the x axis and it squeezes that input to fit between 0 and 1.

45
00:03:00.290 --> 00:03:05.840
However let's take a closer look at what happens when your input starts to get further away from zero.

46
00:03:06.110 --> 00:03:08.190
The further away your input is from zero.

47
00:03:08.330 --> 00:03:13.880
The rate of change in the sigmoid function is actually decreasing rapidly and that rate of change that

48
00:03:13.880 --> 00:03:16.190
is the derivative of the sigmoid function.

49
00:03:16.300 --> 00:03:21.200
And we already know that back propagation and the gradient calculation is essentially just calculating

50
00:03:21.200 --> 00:03:27.630
that derivative in multiple dimensions as you go back through into the hidden layers so let's actually

51
00:03:27.630 --> 00:03:32.820
see the derivative on top of the sigmoid function and we can see here that the scale of that derivative

52
00:03:32.910 --> 00:03:35.780
is much smaller than the actual sigmoid function.

53
00:03:35.940 --> 00:03:41.310
And in fact here you can deviate not that much off the input of zero and eventually you're getting really

54
00:03:41.310 --> 00:03:48.140
really small and close to a zero output so what ends up happening is when you're dealing with any number

55
00:03:48.140 --> 00:03:53.540
of hidden layers you end up using an activation function like the sigmoid function which ends up resulting

56
00:03:53.570 --> 00:03:57.920
in you have any number of small derivatives being multiplied together and by the time you're really

57
00:03:57.920 --> 00:04:02.710
close to the input layer that gradient could decrease exponentially as we propagate down.

58
00:04:02.780 --> 00:04:06.800
And that means you're actually not affecting the weights and biases that much of those first couple

59
00:04:06.800 --> 00:04:11.390
of initial layers which are actually really important for finding the basic patterns in our data.

60
00:04:11.390 --> 00:04:15.290
So we just mentioned that this is an issue of the activation function.

61
00:04:15.320 --> 00:04:21.430
So an opportunity arises where we could just edit and change our activation function so we could use

62
00:04:21.430 --> 00:04:26.650
different activation functions and we can use the rectified linear unit which doesn't actually saturate

63
00:04:26.650 --> 00:04:31.660
those larger positive values we previously discussed that the rectified linear unit is a great choice

64
00:04:31.660 --> 00:04:33.910
for activation and we use that quite a bit.

65
00:04:33.910 --> 00:04:40.840
But here is one of its main benefits is that doesn't matter how large your input value is going to be

66
00:04:40.840 --> 00:04:41.860
beyond zero.

67
00:04:42.010 --> 00:04:48.340
You're not going to exponentially decrease the rate of change there and there's variations off the rectified

68
00:04:48.340 --> 00:04:48.960
linear unit.

69
00:04:48.970 --> 00:04:53.800
There's the leaky rectified linear unit that for negative input values instead of just flatten the amount

70
00:04:53.800 --> 00:04:54.670
to zero.

71
00:04:54.670 --> 00:05:01.640
It slowly leaks them into a very slow gradient going towards a negative number and then we also saw

72
00:05:01.640 --> 00:05:05.010
the exponential linear unit which is called E L you for short.

73
00:05:05.110 --> 00:05:09.160
And that basically kind of starts off a negative one for those larger negative values and then slowly

74
00:05:09.160 --> 00:05:15.500
again continues and the gradient doesn't end up flattening out here which is again good for our calculations

75
00:05:15.520 --> 00:05:21.580
as we won't get that vanishing gradient in other possible solution is to perform what's known as batch

76
00:05:21.580 --> 00:05:26.890
normalization where your model will normalize each batch using that particular batches mean and standard

77
00:05:26.890 --> 00:05:32.470
deviation and that has also been found to alleviate the issue of vanishing gradient.

78
00:05:32.500 --> 00:05:37.120
There's also options of choosing different initialization the weights that can again also help alleviate

79
00:05:37.180 --> 00:05:44.670
these vanishing gradient issues and a very popular method is called Xavier initialization so apart from

80
00:05:44.670 --> 00:05:49.830
things like nationalization we also mentioned that there's sometimes the issue of exploding gradients

81
00:05:50.220 --> 00:05:54.180
where essentially if you're putting on what activation function using you may actually have really a

82
00:05:54.200 --> 00:05:58.560
large gradient values being multiplied by larger values and larger values and you end up just getting

83
00:05:58.680 --> 00:06:02.010
no convergence as you go closer to the input layers.

84
00:06:02.010 --> 00:06:06.760
So we could do is just use what's known as gradient clipping which is kind of a dirty trick.

85
00:06:06.760 --> 00:06:11.280
It's kind of a very quick fix but essentially what you just decide is gradients are gonna be cut off

86
00:06:11.520 --> 00:06:13.390
before reaching some predetermined limit.

87
00:06:13.680 --> 00:06:17.730
And for example you're going to cut off gradients to be always between negative one and one so you're

88
00:06:17.730 --> 00:06:20.480
not going to let them get too small or get too large.

89
00:06:21.910 --> 00:06:26.050
So the recurrent neural network for a time series present their own gradient challenges.

90
00:06:26.050 --> 00:06:31.300
So we're going to do is we're going to explore these special long short term memory neuron unit that

91
00:06:31.300 --> 00:06:34.060
helps actually alleviate a lot of these issues as well.

92
00:06:34.060 --> 00:06:37.190
And it's really good for dealing with sequence data.

93
00:06:37.360 --> 00:06:41.560
So the next lecture let's go to discuss this very special neuron that's very popular for a current year

94
00:06:41.560 --> 00:06:45.760
on networks which is the long short term memory or else time neuron unit.

95
00:06:45.820 --> 00:06:46.330
I'll see you there.
