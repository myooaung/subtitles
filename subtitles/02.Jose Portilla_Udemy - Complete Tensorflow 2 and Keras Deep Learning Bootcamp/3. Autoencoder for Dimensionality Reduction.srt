1
00:00:05,730 --> 00:00:06,930
Welcome back everyone.

2
00:00:06,930 --> 00:00:12,120
In this lecture we're going to show you how you can use python codes create order encoders for the purpose

3
00:00:12,120 --> 00:00:18,070
of dimensionality reduction we can use an auto encoder for dimensionality reduction by separating out

4
00:00:18,070 --> 00:00:25,590
the auto encoder into two parts the encoder and the decoder so here we have the full auto encoder recall

5
00:00:25,620 --> 00:00:26,530
the auto encoder.

6
00:00:26,550 --> 00:00:32,160
The main idea is that we have the same number inputs as we do outputs and then inside of the auto encoder

7
00:00:32,430 --> 00:00:36,170
we do a dimensionality reduction and then expand that backup.

8
00:00:37,320 --> 00:00:44,490
So we end up doing is we separate this out into two parts in encoder and decoder where the idea of the

9
00:00:44,580 --> 00:00:51,280
encoder is to take something that's highly dimensional and then encode it into a lower dimensional space.

10
00:00:51,420 --> 00:00:57,330
And basically the decoder job is to expand back out that low dimensionality in order to make sure that

11
00:00:57,330 --> 00:01:03,330
that hidden layer in the middle of the auto encoder is actually learning correctly what features are

12
00:01:03,330 --> 00:01:07,860
important because if it's not able to learn correctly what combination of features is important then

13
00:01:07,860 --> 00:01:09,960
you're going to have a really poor output.

14
00:01:09,960 --> 00:01:11,670
So that's the job of the decoder.

15
00:01:11,670 --> 00:01:16,680
So what we do is we train this all together until the hidden layer becomes really good at extracting

16
00:01:16,680 --> 00:01:20,510
useful information and then the decoder can expand that backup.

17
00:01:20,550 --> 00:01:26,460
Once you've trained the full auto encoder then we just grab the encoder section and now we have something

18
00:01:26,730 --> 00:01:29,280
that can successfully reduce dimensionality.

19
00:01:29,400 --> 00:01:34,150
So we train them together that when we actually want to use it for dimensionality reduction we only

20
00:01:34,140 --> 00:01:35,430
need to grab the encoder part.

21
00:01:36,690 --> 00:01:41,940
So this encoder cannot successfully reduce dimensionality of the original data by taking combinations

22
00:01:41,970 --> 00:01:43,670
of the original features.

23
00:01:43,680 --> 00:01:49,710
Now you may be asking what use cases does this actually have dimensionally reduction has a wide variety

24
00:01:49,740 --> 00:01:50,640
of use cases.

25
00:01:50,670 --> 00:01:56,730
It can be used for things like compressing your data set visualizing data in lower dimensions and most

26
00:01:56,730 --> 00:02:01,890
importantly revealing hidden relationships that are not clearly seen in higher dimensions when we're

27
00:02:01,890 --> 00:02:07,230
dealing with feature data that goes beyond three dimensions then it becomes very difficult to actually

28
00:02:07,230 --> 00:02:12,930
visualize this and understand the relationship especially if you're performing something like a classification

29
00:02:12,930 --> 00:02:17,980
tasks it may be quite useful to understand just how separated are these classes.

30
00:02:18,180 --> 00:02:23,490
And if you're able to visualize this either in 3D or to the you may be able to get an understanding

31
00:02:23,580 --> 00:02:25,590
of just how separable these classes are.

32
00:02:26,220 --> 00:02:32,040
So for example here we have an example of dimensionality reduction where we're starting from three dimensions

33
00:02:32,370 --> 00:02:36,810
we can see here that these two clusters are highly separable and we're actually able to visualize three

34
00:02:36,810 --> 00:02:37,440
dimensions.

35
00:02:37,440 --> 00:02:41,760
We can imagine that this could have been a case where we have eleven dimensions and so there's no way

36
00:02:41,760 --> 00:02:45,810
of actually visualizing the classes themselves or all the features at once.

37
00:02:45,930 --> 00:02:52,050
So what we could do is reduce that dimensions into either to these space or one d space or 3d space

38
00:02:52,050 --> 00:02:57,270
for operating from something much higher and then see if we're actually able to separate those classes

39
00:02:57,270 --> 00:03:02,000
or not just to get some intuition of how separable these two groups are.

40
00:03:02,470 --> 00:03:05,460
OK so it's actually cut out this exact problem set.

41
00:03:05,580 --> 00:03:07,830
I'm going to hop over to a notebook and get started.

42
00:03:08,420 --> 00:03:13,960
All right here I am at a notebook I've already imported no pie pan this seaboard in map plot lib and

43
00:03:13,980 --> 00:03:20,040
I'm going to do one more import that we actually haven't seen before which is from S.K. learn thought

44
00:03:20,130 --> 00:03:28,110
datasets import make underscore blobs which is a really useful function for just quickly creating a

45
00:03:28,170 --> 00:03:29,300
dataset.

46
00:03:29,430 --> 00:03:34,770
So we're going to be just creating a dataset so you can get an idea of how an auto encoder can be used

47
00:03:35,070 --> 00:03:40,590
for reduced dimensionality and then later on in your exercise you'll work with a real dataset to actually

48
00:03:40,620 --> 00:03:41,430
apply this.

49
00:03:42,060 --> 00:03:47,700
So let's create a dataset with this make blobs function and we're gonna call make blobs.

50
00:03:47,700 --> 00:03:53,160
And if you want explorer make blobs in more detail the ship type here and see the various functionality

51
00:03:53,190 --> 00:03:54,200
that is available for it.

52
00:03:55,400 --> 00:04:00,930
So we're going to do just walk you through the main parameters we're going to be passing in.

53
00:04:01,410 --> 00:04:07,350
First we're going to decide the end samples which is essentially how many rows do you want in your dataset.

54
00:04:07,860 --> 00:04:14,730
So this case will last for three hundred rows and then we'll say N features and this where you can decide

55
00:04:14,760 --> 00:04:17,210
how many features you actually want.

56
00:04:17,220 --> 00:04:23,890
So I'm just going to create that first two features for this dataset and then we can decide on centres.

57
00:04:23,910 --> 00:04:26,830
So how many clusters they actually want in this case.

58
00:04:26,850 --> 00:04:27,720
I want to.

59
00:04:27,720 --> 00:04:33,230
So we can think of this as a kind of binary dataset belonging to one of two clusters.

60
00:04:33,480 --> 00:04:39,630
And then finally we can say cluster underscore standard deviation essentially how much noise is there

61
00:04:39,630 --> 00:04:40,940
for these clusters.

62
00:04:40,940 --> 00:04:43,260
All set that to just one point zero.

63
00:04:43,740 --> 00:04:46,830
And then to make sure you get the exact same data set that I do.

64
00:04:46,950 --> 00:04:47,220
Go ahead.

65
00:04:47,220 --> 00:04:48,650
Is that your random state.

66
00:04:48,680 --> 00:04:50,580
2 1 0 1.

67
00:04:50,610 --> 00:04:57,060
Because this sets you're going to make 300 samples part of two clusters with just two features.

68
00:04:57,280 --> 00:05:02,520
We go ahead and run that and then we can separate out our data because if we take a look at this data

69
00:05:02,520 --> 00:05:11,100
object it's a tuple of two arrays or the first item is these X features and then the second item is

70
00:05:11,100 --> 00:05:13,440
your actual labels.

71
00:05:13,440 --> 00:05:16,160
So we take a look at this by this tuple unpacking.

72
00:05:16,170 --> 00:05:22,230
We have this X feature data and then if we took a look at why we have the actual corresponding labels

73
00:05:22,320 --> 00:05:28,610
either into cluster zero or cluster one remember that in this case we're actually worrying about any

74
00:05:28,610 --> 00:05:30,280
sort of classification.

75
00:05:30,300 --> 00:05:33,830
Instead we're going to see how to reduce the number of dimensions here.

76
00:05:33,910 --> 00:05:37,390
Let's say they only created a two dimensional dataset.

77
00:05:37,530 --> 00:05:41,510
What are going to do is add a third feature that is just noise.

78
00:05:41,630 --> 00:05:49,460
So we're gonna say and P go ahead and put this into yourself and P random seed and go ahead and set

79
00:05:49,460 --> 00:05:58,190
your scene to 1 0 1 and in the exact same cell seek at the same noise I do create Z noise and say P

80
00:05:58,940 --> 00:06:09,950
random normal and say the size is equal to the length of X and then say Z noise and convert that noise

81
00:06:09,950 --> 00:06:13,380
array into a series.

82
00:06:13,610 --> 00:06:21,170
So all I did here is with these three lines I now have this series which is just a bunch of random noise

83
00:06:21,170 --> 00:06:28,760
for every single row from zero to ninety nine but I'm going to do next is create a data frame called

84
00:06:28,760 --> 00:06:34,020
features by saying PD data frame on X.

85
00:06:34,160 --> 00:06:41,990
So if I take a look at my features right now I have 0 and 1 which is the original features created by

86
00:06:42,080 --> 00:06:43,250
make blobs.

87
00:06:43,280 --> 00:06:48,560
These two features actually have some sort of correlation or effect to separating out all these data

88
00:06:48,560 --> 00:06:51,180
points into their respective clusters.

89
00:06:51,200 --> 00:06:59,390
What I'm going to do is say features is equal to PD and I'm going to concatenate my original features

90
00:06:59,510 --> 00:07:05,650
with that noisy series and then I'll say concatenate this a lot.

91
00:07:05,660 --> 00:07:17,420
Access is equal to 1 and let's go ahead and just relabeled those columns to be equal to some x1 x2 and

92
00:07:17,420 --> 00:07:19,310
then x.

93
00:07:19,400 --> 00:07:27,560
So it is actually look like I say feature head and I see X1 X2 and X3 and I can actually perform a scatter

94
00:07:27,560 --> 00:07:40,190
plot here by saying party scatter and let's go ahead and scatter plot X1 versus features x2 recall these

95
00:07:40,190 --> 00:07:41,600
are our original features.

96
00:07:41,600 --> 00:07:43,430
So they should be informative.

97
00:07:43,430 --> 00:07:45,010
So here I see the two clusters.

98
00:07:45,230 --> 00:07:51,350
There's clear separation here and I can actually specify what to colour them by by saying c is equal

99
00:07:51,350 --> 00:07:54,500
to y which means just colour them by the zeros and ones.

100
00:07:54,530 --> 00:08:01,460
So clearly in my original data I have these very clearly separated clusters.

101
00:08:01,460 --> 00:08:07,100
What I can do is I can actually visualize this in three dimensions but again visualizing the extra noise

102
00:08:07,130 --> 00:08:21,010
we added in as our third made up feature so we can say from NPL underscore toolkits that M plot 3D the

103
00:08:22,280 --> 00:08:27,790
import axes 3 d.

104
00:08:27,900 --> 00:08:28,330
Okay.

105
00:08:28,360 --> 00:08:32,830
So we have NPL toolkits and plot 3D import axis 3D.

106
00:08:33,410 --> 00:08:39,950
And then I'm going to create a three dimensional figure by saying figures equal to peel figure and I'm

107
00:08:39,950 --> 00:08:48,080
actually then going to say axes physical to fit add underscore subplot and you can check out our auto

108
00:08:48,080 --> 00:08:48,920
encoder notebook.

109
00:08:49,100 --> 00:08:54,680
So if you actually go to the auto encoders notebook that this lecture is based upon you scroll down

110
00:08:54,680 --> 00:08:55,250
here.

111
00:08:55,250 --> 00:09:00,290
I blinked for you the actual information from the online documentation to don't think that I have this

112
00:09:00,380 --> 00:09:02,030
scatter plot memorized.

113
00:09:02,210 --> 00:09:04,670
Instead I just went ahead and referenced that link.

114
00:09:04,790 --> 00:09:06,290
So keep that in mind.

115
00:09:06,530 --> 00:09:07,850
That's how I know this code.

116
00:09:07,910 --> 00:09:16,850
We're gonna say add some plot one 1 one and then most importantly we say projection is 3D and then I'm

117
00:09:16,850 --> 00:09:25,980
going to call a scatter plot here X that scatter and will say scatter feature x 1 against feature x

118
00:09:25,980 --> 00:09:34,170
2 against feature x 3 and then we'll go ahead and color that by the correct labels again.

119
00:09:34,880 --> 00:09:38,600
So I run that and I see this nice three dimensional scatter.

120
00:09:38,600 --> 00:09:43,580
Now what's really cool is if you're operating in the latest version of Jupiter you should be able to

121
00:09:44,600 --> 00:09:54,380
say percent sine map plot lib space notebook run that cell and then rerun the plotting and you should

122
00:09:54,380 --> 00:09:57,800
see that this is now an interactive plot that you can explore.

123
00:09:57,800 --> 00:10:04,520
So notice here along this third dimension that we created here this kind of noisy one along the z axis

124
00:10:04,970 --> 00:10:07,790
that is actually just straight noise.

125
00:10:07,820 --> 00:10:14,070
Well we originally saw was the scatter that looked like this so if we looked at it straight on top.

126
00:10:14,090 --> 00:10:16,250
Notice that we have these type clusters.

127
00:10:16,250 --> 00:10:22,410
But this third dimension is kind of adding a lot of noise that actually does not separate out versus

128
00:10:22,410 --> 00:10:23,160
yellow.

129
00:10:23,170 --> 00:10:25,510
There's no separation between purple versus yellow.

130
00:10:25,680 --> 00:10:31,320
In this third dimension of noise that we created there's clear separation of purple vs. Yellow.

131
00:10:31,320 --> 00:10:34,040
Along these two original features.

132
00:10:34,110 --> 00:10:39,720
So what we're going to explore is if we can create an auto encoder that can essentially try to filter

133
00:10:39,720 --> 00:10:42,200
out which features are important and which are not.

134
00:10:42,350 --> 00:10:49,530
And we would assume that it'll take information from X1 and X2 and it won't take information from x3

135
00:10:49,620 --> 00:10:56,160
since x3 really does nothing to tell us about these two blobs since it's just noise.

136
00:10:56,160 --> 00:10:56,810
Okay.

137
00:10:56,970 --> 00:11:02,100
So if you want to kind of turn off this plot you can just come up here and then click this little off

138
00:11:02,100 --> 00:11:02,820
button.

139
00:11:02,820 --> 00:11:09,150
I'm Zune dense you can't see the whole thing but here we have now those two clusters X1 and X2 informative

140
00:11:09,150 --> 00:11:11,730
features x3 essentially just noise.

141
00:11:11,850 --> 00:11:16,350
It shouldn't be very informative especially for informing you on how separable these classes are.

142
00:11:16,950 --> 00:11:23,400
So now let's explore creating an encoder and decoder going to come back down here and we'll say from

143
00:11:23,400 --> 00:11:30,350
tensor flow not curious thought models import sequential.

144
00:11:31,620 --> 00:11:38,570
And then from tensor flow thought curve stop layers import dense.

145
00:11:39,210 --> 00:11:44,400
And then lastly when I want to show here is we're actually going to import stochastic gradient descent

146
00:11:44,820 --> 00:11:51,990
as our optimizer and when we can do that the same from tensor flow that carries the optimizes import

147
00:11:52,600 --> 00:11:58,470
s g d and what's nice about stochastic gradient descent is you can play a lot with the learning rate

148
00:11:58,650 --> 00:12:02,600
of the stochastic gradient descent to try to help out your auto encoder.

149
00:12:02,600 --> 00:12:08,430
Now in our particular case there's a very simple dataset so affecting the learning rate shouldn't really

150
00:12:08,430 --> 00:12:12,780
make much of a difference but it's quite common in auto encoders to use something like stochastic gradient

151
00:12:12,780 --> 00:12:16,130
descent and then play around with that learning rate and make it a little slower.

152
00:12:16,140 --> 00:12:21,180
If you notice that hidden layer is having trouble understanding what features are important.

153
00:12:21,180 --> 00:12:25,830
So how do we actually do this or we're gonna do is we're going to create an auto encoder that starts

154
00:12:25,830 --> 00:12:32,550
off with the three inputs reduces it down to two neurons in the hidden layer and then expands it back

155
00:12:32,550 --> 00:12:34,800
out to three neurons.

156
00:12:34,800 --> 00:12:40,370
So we want to make sure that the input 3 matches the output and we're going to do this in two steps.

157
00:12:40,440 --> 00:12:46,560
We'll first create an encoder and we defined this as its own separate model by saying coder is equal

158
00:12:46,560 --> 00:12:57,870
to sequential and saying encoder ad and we add in a dense layer and we'll say units is equal to to specify

159
00:12:57,870 --> 00:13:04,380
the activation function to be rectified linear unit and most importantly when you specify that are input

160
00:13:04,380 --> 00:13:08,910
shape is equal to the original input shape which was 3.

161
00:13:09,210 --> 00:13:15,060
So we're going from 3 as our input down to 2 dense units.

162
00:13:15,480 --> 00:13:19,050
So that's our encoder and then we need to match a decoder to it.

163
00:13:19,680 --> 00:13:26,640
So the decoder is going to be equal to again its own sequential model and then we'll say decoders equal

164
00:13:26,660 --> 00:13:31,400
to excuse me decoder add panel will add dense layers.

165
00:13:31,470 --> 00:13:37,420
And in this case the decoder ends with three units go ahead and use the same activation function rectified

166
00:13:37,500 --> 00:13:43,520
linear unit but it's input shape is coming from the output of this encoder.

167
00:13:43,590 --> 00:13:51,460
So we're going from 3 to 2 and then the input shape is going to be equal to that 2 and then it's going

168
00:13:51,450 --> 00:13:53,320
to come out as three.

169
00:13:53,400 --> 00:13:57,450
So that's what's happening here between the encoder and the decoder.

170
00:13:57,450 --> 00:14:03,810
We start off with three features go down to two and then from that to as the input for the decoder we

171
00:14:03,810 --> 00:14:05,130
expand back out to three.

172
00:14:05,130 --> 00:14:12,510
So very simple auto encoder here to define the actual auto encoder will say it's simply a sequential

173
00:14:12,510 --> 00:14:18,510
model and this is what's really nice about the care syntax is we just passed a list of encoder and decoder

174
00:14:18,830 --> 00:14:21,120
and we've essentially combined them together.

175
00:14:21,120 --> 00:14:29,070
So we're gonna do is we'll train the auto encoder together that way we can have the encoder learn the

176
00:14:29,100 --> 00:14:36,690
best way to reduce from 3 to 2 in order to reproduce on the decoder side a similar output and then once

177
00:14:36,690 --> 00:14:42,510
the auto encoder has learned this will separate out the encoder section of it in order to just go from

178
00:14:42,540 --> 00:14:44,310
three to two.

179
00:14:44,310 --> 00:14:53,760
So the last we need to do is compile our auto encoder so we'll say auto encoder compile and we'll go

180
00:14:53,760 --> 00:14:56,460
ahead and say a loss is mean square error.

181
00:14:56,460 --> 00:15:01,980
Since these are all continuous values and here's we can play around the atomizer if you really want

182
00:15:01,980 --> 00:15:04,110
to you could just pass in the string code.

183
00:15:04,180 --> 00:15:09,630
Adam but typically what's really nice is to play around a stochastic gradient descent and then play

184
00:15:09,630 --> 00:15:10,830
around your learning rates.

185
00:15:10,830 --> 00:15:14,350
So people just pass on one point five okay.

186
00:15:14,380 --> 00:15:19,770
So we have our auto encoder completely setup last we need to do is actually scale our dataset.

187
00:15:20,110 --> 00:15:29,050
We'll save from that scalar and pre processing just as we've done before import min max scalar.

188
00:15:29,530 --> 00:15:34,660
And notice what I'm going to do here is since there's technically no right answer as far as how you

189
00:15:34,660 --> 00:15:40,780
should reduce this dimensionality I'm just going to fit and transform on my entire dataset because I

190
00:15:40,870 --> 00:15:48,000
don't really have a right answer to compare this to soc I have a correct version of this dataset in

191
00:15:48,010 --> 00:15:49,750
two dimensions.

192
00:15:49,750 --> 00:15:56,230
So we'll go out and say min max scalar and then say our scale data is equal to scalar.

193
00:15:56,320 --> 00:16:01,780
And here I'll just call it transform on my entire feature dataset.

194
00:16:01,810 --> 00:16:07,810
So this is what's making this so far different from the supervisory process because there was no trained

195
00:16:07,810 --> 00:16:11,590
test split here because it doesn't really make sense for doing something where we don't actually know

196
00:16:11,590 --> 00:16:15,570
the correct answer we're going from three dimensions onto two dimensions.

197
00:16:15,610 --> 00:16:16,220
OK.

198
00:16:16,410 --> 00:16:23,230
Finally let's go ahead and take our auto encoder and we're gonna fit this and we're gonna take our scale

199
00:16:23,230 --> 00:16:28,600
data and then pass our scale data as well as both X and Y.

200
00:16:28,600 --> 00:16:29,630
Here really.

201
00:16:29,860 --> 00:16:33,190
And then we'll go and train us for five epochs this should be pretty fast because it's a very simple

202
00:16:33,190 --> 00:16:33,670
dataset.

203
00:16:34,210 --> 00:16:34,550
OK.

204
00:16:34,570 --> 00:16:42,070
There we go just kind of a few microseconds per epoch and then we're gonna do is we'll say the encoded

205
00:16:43,120 --> 00:16:45,820
2 dimensional version is equal to.

206
00:16:46,240 --> 00:16:50,250
And notice here I'm just going to grab the encoder not the full auto encoder.

207
00:16:50,410 --> 00:16:57,340
So just the encoder section which goes from three to two and I'll ask it to predict based off my scaled

208
00:16:57,400 --> 00:17:07,750
data run that and now if we check out encoded two dimensional run this nation notice some behavior that

209
00:17:07,750 --> 00:17:09,270
looks something like this.

210
00:17:09,280 --> 00:17:16,780
So if we check out the shape of this it's taken from three hundred to three so if we take a look at

211
00:17:17,710 --> 00:17:23,680
the original scaled data shape it's three hundred rows by three features and we have successfully reduced

212
00:17:23,680 --> 00:17:26,360
that down to just two dimensions.

213
00:17:26,530 --> 00:17:28,120
So let's see what that actually looks like.

214
00:17:28,120 --> 00:17:37,330
I'm going to call peal t scatter and we'll call encoded two dimensions and say grab all the rows from

215
00:17:37,330 --> 00:17:44,910
the first column and plot them against encoded two dimensions all the rows in the second column.

216
00:17:45,280 --> 00:17:49,960
We go ahead and run that and you should see something that maybe looks something like this.

217
00:17:50,020 --> 00:17:53,520
It may not be so noisy here you may actually just see two straight lines.

218
00:17:53,920 --> 00:17:59,880
But note that we have this little group cluster and then kind of these points up above here.

219
00:17:59,890 --> 00:18:05,110
Now the question is how well did this actually do at separating out the classes that we know to be the

220
00:18:05,110 --> 00:18:06,200
underlying truth.

221
00:18:06,700 --> 00:18:13,930
Well what I can do is turn this off here and say c is equal to y then actually color these run that

222
00:18:14,020 --> 00:18:19,960
and you notice that it was able to separate this out and it got some of the noise here but it reduced

223
00:18:19,960 --> 00:18:25,390
it back down to two dimensions where all the L points are down here and all the purple points are up

224
00:18:25,390 --> 00:18:26,050
here.

225
00:18:26,200 --> 00:18:29,590
So why would this be useful in a real world situation.

226
00:18:29,590 --> 00:18:34,870
Well in a real world situation you dealing with high dimensional data so it may not just be three features

227
00:18:35,140 --> 00:18:39,670
that it may be something like eleven features something that's just impossible to visualize as we did

228
00:18:39,670 --> 00:18:40,120
before.

229
00:18:40,600 --> 00:18:46,780
So appear when we actually visualize the two three dimensional clusters that we did appear we are pretty

230
00:18:46,780 --> 00:18:51,160
lucky we can actually visualize this and there's no real need for dimensionality reduction what the

231
00:18:51,160 --> 00:18:56,410
purpose of this which is to show you that we're going from 3 down to 2 that in a real world situation

232
00:18:56,740 --> 00:19:01,000
you're going to be dealing with an extremely high dimensional data set something that's four or five

233
00:19:01,000 --> 00:19:05,000
six ten et cetera something that's just impossible to visualize.

234
00:19:05,290 --> 00:19:11,020
So what you would then do is train your auto encoder to go from something like 10 dimensions down to

235
00:19:11,020 --> 00:19:17,410
three dimensions and then you'd be able to plot this out in a three dimensional space and you can imagine

236
00:19:17,410 --> 00:19:23,230
if you plot it out in a three dimensional space and saw a clear separation by class because maybe you

237
00:19:23,230 --> 00:19:30,370
just wanted to explore the data and see how intermingled these classes were and just general sense and

238
00:19:30,370 --> 00:19:35,130
you saw that there was clear separation then you would know that these two classes are very distinct.

239
00:19:35,320 --> 00:19:41,230
If after reducing it down to three dimensions or two dimensions there was a lot of crossover between

240
00:19:41,230 --> 00:19:46,450
these two classes or multiple classes then you would know which classes are really similar to each other

241
00:19:46,810 --> 00:19:50,250
and which classes are very separable from each other.

242
00:19:50,260 --> 00:19:55,540
So this is just to reveal hidden insights visually that you wouldn't be able to see before and your

243
00:19:55,540 --> 00:20:00,940
exercise is really going to focus on that and your exercise will give you a real dataset that has 17

244
00:20:00,940 --> 00:20:06,310
dimensions and will be your job to actually reduce it down to either three dimensions or two dimensions

245
00:20:06,640 --> 00:20:12,000
and see what components or what categories are highly separable from the other ones.

246
00:20:12,370 --> 00:20:12,680
OK.

247
00:20:12,680 --> 00:20:19,440
So all we did here is we created an auto encoder by separately creating our encoder first and then our

248
00:20:19,530 --> 00:20:20,560
decoder.

249
00:20:20,610 --> 00:20:26,970
Coming up next we're going to analyze how we can perform this same process but on even higher dimensional

250
00:20:26,970 --> 00:20:28,820
data such as image data.

251
00:20:28,890 --> 00:20:34,170
Recall a 28 by 28 image is going to have seven hundred eighty four feature points or seven hundred eighty

252
00:20:34,170 --> 00:20:35,760
four pixels or dimensions.

253
00:20:35,950 --> 00:20:37,590
So you're going to reduce that down.

254
00:20:37,710 --> 00:20:38,370
OK.

255
00:20:38,460 --> 00:20:41,240
If you have any questions feel free to post the CUNY forums.

256
00:20:41,380 --> 00:20:42,990
Thanks and I'll see you at the next lecture.
