WEBVTT
1
00:00:05.290 --> 00:00:06.140
Welcome everyone.

2
00:00:06.160 --> 00:00:10.810
Now that we understand the theory behind convolution on neural networks let's go ahead and begin training

3
00:00:11.170 --> 00:00:17.530
one on the very famous m this data set in part one of the series of lectures we're going to focus on

4
00:00:17.530 --> 00:00:22.920
just reading in the data and also making sure that we understand the one hot encoding of the labels.

5
00:00:22.930 --> 00:00:23.940
Let's get started.

6
00:00:23.950 --> 00:00:24.150
All right.

7
00:00:24.160 --> 00:00:26.850
Here I am at a notebook I've already imported pansies pizza.

8
00:00:26.880 --> 00:00:32.840
No pies and MP as well as my polyp and map polyp in line to load up the M this dataset.

9
00:00:32.860 --> 00:00:41.230
It's actually built into carriers so we can say from tensor flow carries that datasets go ahead and

10
00:00:41.260 --> 00:00:43.010
import amnesty.

11
00:00:44.220 --> 00:00:47.300
And then what we're going to be able to do is actually load in the data.

12
00:00:47.490 --> 00:00:51.790
So m this now has a load data method call.

13
00:00:51.930 --> 00:00:55.360
And essentially what this does is just load the m this data set for us.

14
00:00:55.560 --> 00:01:01.560
So we're going to use some tuple unpacking here because it's actually already organized for us in both

15
00:01:01.620 --> 00:01:04.230
x train y train and x test White test.

16
00:01:04.290 --> 00:01:06.460
We don't need to do any splits ourselves.

17
00:01:06.690 --> 00:01:18.620
So we'll say X train y train and then create another tuple for X test y test okay.

18
00:01:18.900 --> 00:01:24.360
So then go ahead and load in that data and then we're going to do is especially explaining the shapes

19
00:01:24.360 --> 00:01:26.480
of the data and how to visualize the data.

20
00:01:26.490 --> 00:01:28.240
So if I take a look at X train.

21
00:01:28.260 --> 00:01:30.370
It looks like just a bunch of values here.

22
00:01:30.380 --> 00:01:32.430
So let's check out the shape of it.

23
00:01:32.550 --> 00:01:39.030
You'll notice the X train is essentially sixty thousand images and each image is 28 by 28 pixels.

24
00:01:39.030 --> 00:01:41.800
Let's go ahead and grab a single image from this.

25
00:01:41.860 --> 00:01:48.800
So it's a single image and we'll say X train 0 and grabbed the very first item.

26
00:01:48.850 --> 00:01:52.940
So if I take a look at a single image now its shape is 28 by 28.

27
00:01:54.220 --> 00:01:58.040
And here actually have all the raw values from that single image.

28
00:01:58.060 --> 00:02:06.280
So luckily if I have if I have a two dimensional image map plot lib has something called M show which

29
00:02:06.280 --> 00:02:09.030
can display these two dimensional array objects.

30
00:02:09.040 --> 00:02:14.920
So we say peel t the M show and I get back what is essentially the number five.

31
00:02:14.920 --> 00:02:23.170
Now keep in mind that the m this dataset is grayscale so it only goes from zero to the max value of

32
00:02:23.390 --> 00:02:24.770
that 255.

33
00:02:24.820 --> 00:02:29.780
So somewhere around here we can see kind of these lighter points or darker points are labeled fifty

34
00:02:29.780 --> 00:02:35.530
five or close to it something that sometimes confuses students is they see back a color image here and

35
00:02:35.530 --> 00:02:41.490
they're wondering hey I thought this dataset wasn't grayscale Why do I see purples and yellows et cetera.

36
00:02:41.500 --> 00:02:45.800
Well technically you can choose any color mapping to display this image.

37
00:02:45.800 --> 00:02:51.460
And if you recall from our data visualization crash course we talked about the color maps that are available

38
00:02:51.580 --> 00:02:58.060
in map what lit the default one is this very this one that goes from purple to blue to green to yellow

39
00:02:58.540 --> 00:03:05.000
and technically what Matt Portillo is doing is just setting zero at this purple point and then one at

40
00:03:05.110 --> 00:03:11.290
or 255 actually at this yellow point and then all the values are sequenced in between and you could

41
00:03:11.410 --> 00:03:16.510
replace this with any color mapping you wanted to and you could come down here and replace it with grays

42
00:03:16.900 --> 00:03:19.340
or replace it with kind of a black to white.

43
00:03:19.360 --> 00:03:24.790
So there's gray which is black to white or grays which is kind of confusing because this one is with

44
00:03:24.790 --> 00:03:25.090
it.

45
00:03:25.090 --> 00:03:27.280
E and then it goes from white to black.

46
00:03:27.310 --> 00:03:31.010
So technically you could display this in any sort of color mapping you want.

47
00:03:31.120 --> 00:03:36.590
The main thing to just be aware of right now is the values go from zero to two hundred and fifty five

48
00:03:36.610 --> 00:03:38.380
which is typical of images.

49
00:03:38.380 --> 00:03:41.810
Basically each color channel usually gets a value between 0 to 255.

50
00:03:42.150 --> 00:03:42.940
OK.

51
00:03:42.970 --> 00:03:46.110
So we're able to show the data of a single image.

52
00:03:46.120 --> 00:03:48.880
Now let's explore the actual labels themselves.

53
00:03:48.970 --> 00:03:56.080
If we take a look at white train you'll notice that the array right now the very first value is five

54
00:03:56.170 --> 00:03:59.230
which corresponds to the very first value in our X training set.

55
00:03:59.230 --> 00:04:02.320
So we were correct in assuming that this was a five here.

56
00:04:02.320 --> 00:04:08.260
But what's going to happen is right now our labels are just literally the number they represent.

57
00:04:08.500 --> 00:04:15.940
And what's going to happen here is if we were to pass in our training the labels as such the network

58
00:04:15.940 --> 00:04:21.410
would assume it was some sort of continuous value and it would try to predict stuff like five point

59
00:04:21.410 --> 00:04:23.780
five or five point six et cetera.

60
00:04:23.920 --> 00:04:25.950
And really these are categories.

61
00:04:25.960 --> 00:04:31.220
So five is not a continuous value but it's actually the category five.

62
00:04:31.600 --> 00:04:36.070
So really what we're doing here is a classification problem and we need to make sure that our network

63
00:04:36.160 --> 00:04:43.060
understands that as discussed earlier in the artificial neural network section we talked about the theory

64
00:04:43.060 --> 00:04:48.570
behind multi class classification problems and what we need to do is we need to one hot encode this

65
00:04:48.870 --> 00:04:52.060
and locally tense flow actually has utilities for this.

66
00:04:52.270 --> 00:05:01.760
We can say from tensor flow doctors that utilize import to categorical.

67
00:05:02.220 --> 00:05:03.750
We'll go ahead and run that.

68
00:05:03.900 --> 00:05:11.040
And then what we're going to do is if we take a look at the shape of Y train it just says sixty thousand

69
00:05:11.070 --> 00:05:14.130
because there's essentially 60000 labeled numbers there.

70
00:05:14.370 --> 00:05:19.830
But we need to do is we need to transform this to one hot encoding so that each of these numbers ends

71
00:05:19.830 --> 00:05:22.790
up one hot encoding to represent a category.

72
00:05:22.800 --> 00:05:34.150
So what we'll do here is we'll say why example too categorical and if we say then why train and then

73
00:05:34.150 --> 00:05:41.710
see the shape of why example it has now automatically with this two categorical function call which

74
00:05:41.710 --> 00:05:46.600
we just adhere essentially converts a class vector of integers to a binary class matrix.

75
00:05:46.600 --> 00:05:53.260
So it's now one hot and cold this so that if I actually take a look at why example is this super large

76
00:05:53.260 --> 00:05:53.930
array.

77
00:05:53.980 --> 00:06:00.120
But notice now if I take a look at the very first item notice what happens here.

78
00:06:00.150 --> 00:06:06.980
An index position 5 it's now 1 essentially indicating that this belongs to your class number 5.

79
00:06:07.020 --> 00:06:10.260
So it goes 0 1 2 3 4 and 5.

80
00:06:10.260 --> 00:06:14.600
So it's transform this 5 into this entire row.

81
00:06:14.730 --> 00:06:19.020
So that's why we have now sixty thousand by ten across everything.

82
00:06:19.020 --> 00:06:24.270
So what we're going to do is we'll convert both the test labels and the training labels into categorical

83
00:06:24.270 --> 00:06:33.070
labels so say white cat test is equal to two categorical why test.

84
00:06:33.080 --> 00:06:38.120
Now if you do shift type here you'll notice another possible parameter we can pass in is the number

85
00:06:38.120 --> 00:06:39.310
of classes.

86
00:06:39.410 --> 00:06:44.930
Now I didn't pass in the number of classes when I first called to categorical because two categorical

87
00:06:45.020 --> 00:06:50.600
is actually able to infer the number of classes and the way it does this is simply by checking what's

88
00:06:50.600 --> 00:06:54.290
the number of unique items within this labeled dataset.

89
00:06:54.710 --> 00:06:56.790
So in this case the numbers are 0 through 9.

90
00:06:56.930 --> 00:07:00.480
So there's 10 possible numbers so 10 unique actual categories.

91
00:07:00.500 --> 00:07:03.080
So that's why I'd assume there was 10 here.

92
00:07:03.080 --> 00:07:09.510
Now there may be a rare case where for some reason you're missing an instance of a particular class.

93
00:07:09.530 --> 00:07:15.020
If that happens to be the case you can always just make sure the number of classes are what you expect

94
00:07:15.350 --> 00:07:17.840
by manually inputting the number of classes.

95
00:07:17.840 --> 00:07:28.350
And this case is equal to 10 and we'll go ahead and the same for train so we'll say why train and we

96
00:07:28.350 --> 00:07:30.140
can manually specify the number of classes.

97
00:07:30.150 --> 00:07:35.250
Now either way it should work given the fact that both our test set and our training set actually contain

98
00:07:35.280 --> 00:07:37.480
instances of all possible classes.

99
00:07:37.560 --> 00:07:39.680
So we'll go ahead and run that as well.

100
00:07:39.680 --> 00:07:46.050
And in general to actually have a well suited machine learning dataset there should be instances of

101
00:07:46.230 --> 00:07:48.840
each class within both the test and the train.

102
00:07:48.960 --> 00:07:53.700
And if that's not the case that's usually indicative of a problem with your actual dataset.

103
00:07:53.700 --> 00:07:57.450
OK now it's time to normalise the training data.

104
00:07:57.450 --> 00:08:00.240
Recall that our training data is just image data.

105
00:08:00.240 --> 00:08:06.300
So if I take a look at that single image again my values go from 0 to 255.

106
00:08:06.330 --> 00:08:14.760
So if I check the actual Max here it's two hundred fifty five and the minimum value is zero.

107
00:08:14.770 --> 00:08:20.710
And to make sure I don't run into any gradient problems these should be scale between 0 and 1.

108
00:08:20.740 --> 00:08:25.240
There's a couple of different approaches we could take we could take a classic approach of from S.K.

109
00:08:25.260 --> 00:08:31.510
learn that pre processing importing that min max scalar and then running it on both the test set and

110
00:08:31.510 --> 00:08:37.440
the training set and recall when we've been using that min max scalar object or any scalar from psychic

111
00:08:37.510 --> 00:08:43.290
learn we fit to the training data and then transform on the test data.

112
00:08:43.390 --> 00:08:49.240
However I keep mentioning that because we only fit on the training data because I don't want to assume

113
00:08:49.270 --> 00:08:55.000
prior knowledge of my test data but we have a special case here because we're dealing with images and

114
00:08:55.210 --> 00:09:00.190
I can make the assumption that the future images that I'm going to feed into this model that it hasn't

115
00:09:00.190 --> 00:09:07.160
seen yet should be scaled as just normal images that go from the range of 0 to 255.

116
00:09:07.300 --> 00:09:14.620
So an easy way to just instantly keep all the values between zero and one is to simply say X train is

117
00:09:14.620 --> 00:09:24.980
equal to x train divided by two fifty five and then I can perform the same function here on X test and

118
00:09:24.980 --> 00:09:31.420
say X test is equal to x test divided by 255.

119
00:09:31.490 --> 00:09:37.160
And the reason again I can do that is because I know that feature images just by the way they're constructed

120
00:09:37.190 --> 00:09:41.500
with red green blue channels should always have values green 0 and 255.

121
00:09:41.510 --> 00:09:47.540
So my scaling will essentially not change the features are essentially known both right now and for

122
00:09:47.540 --> 00:09:48.820
future images.

123
00:09:48.860 --> 00:09:56.090
And now if I take a look at a scaled image so let's go ahead as a scaled image is equal to x train at

124
00:09:56.090 --> 00:09:56.990
zero.

125
00:09:56.990 --> 00:10:01.880
Take a look at the scaled image now and you'll notice all the values essentially go from zero up to

126
00:10:01.910 --> 00:10:05.830
1 and that's going to work much better with our actual network.

127
00:10:06.350 --> 00:10:09.550
And if we take a look at the max value it's 1 now.

128
00:10:09.560 --> 00:10:15.170
And keep in mind that if you say peel team show since the ratio of everything is still the same.

129
00:10:15.200 --> 00:10:18.660
The scaled image should look pretty much exactly the same before.

130
00:10:18.660 --> 00:10:26.510
OK so the last step for our data processing is to reshape the data right now if we take a look at the

131
00:10:26.510 --> 00:10:31.230
shape of our training data it's sixty thousand by twenty eight by twenty eight.

132
00:10:31.790 --> 00:10:37.550
So this is correct for a convolution on your own network but we need to add one more dimension to let

133
00:10:37.550 --> 00:10:41.300
the network know that we're dealing with a single RG B channel.

134
00:10:41.330 --> 00:10:48.200
Essentially the images are in black and white so we have just a single image colour channel from 0 to

135
00:10:48.260 --> 00:10:48.700
1.

136
00:10:48.710 --> 00:10:55.310
In our case and before the actual normalization it was 0 to 255 in the next series of lectures when

137
00:10:55.310 --> 00:10:56.470
we deal of color images.

138
00:10:56.540 --> 00:11:02.330
We'll make sure to specify this as a dimension of 3 1 4 red 1 for green and 1 for blue.

139
00:11:02.330 --> 00:11:10.080
So all we're going to do here is we're going to say X train is equal to x train.

140
00:11:10.220 --> 00:11:20.480
Make sure I follow my same convention and say reshape sixty thousand twenty eight by twenty eight comma

141
00:11:20.940 --> 00:11:22.290
and then we're going to add one.

142
00:11:22.290 --> 00:11:27.580
So essentially what we have here is d batch size.

143
00:11:27.590 --> 00:11:30.160
In this case the batches all the images.

144
00:11:30.220 --> 00:11:33.560
So really it's just all images essentially how big is this batch.

145
00:11:33.680 --> 00:11:35.090
So the batch size.

146
00:11:35.330 --> 00:11:39.340
And then we have the width by height.

147
00:11:40.280 --> 00:11:45.530
By color channels in our case we only have one colour channel because these images are essentially in

148
00:11:45.530 --> 00:11:46.100
black and white.

149
00:11:46.760 --> 00:11:52.850
So we run this and since sixty thousand times twenty eight times twenty eight times one is the same

150
00:11:52.850 --> 00:11:55.030
as sixty thousand times twenty eight times 28.

151
00:11:55.070 --> 00:12:00.800
There's no issues with me reshaping this by adding in this other kind of blank colour channel or just

152
00:12:00.800 --> 00:12:07.910
a single instance of the colour channel and we'll do the same thing for X test x test is equal to x

153
00:12:07.910 --> 00:12:14.930
test and we reshape this to be ten thousand and you can confirm that that's the right number by just

154
00:12:15.050 --> 00:12:16.760
checking the current shape of accessed.

155
00:12:16.850 --> 00:12:18.940
So there's 10000 images in the test set.

156
00:12:19.160 --> 00:12:23.050
Of course there are also twenty eight by twenty eight and they're also in grayscale.

157
00:12:23.100 --> 00:12:29.330
OK so that's actually it for data pre processing when we're dealing with real image data such as PND

158
00:12:29.330 --> 00:12:32.610
and JPEG files which we'll do later on in this section of the course.

159
00:12:32.750 --> 00:12:37.760
There's going to be a little bit more involvement as far as data pre processing scaling and actually

160
00:12:37.760 --> 00:12:43.520
performing transformations on the dataset but luckily for us this dataset is essentially specifically

161
00:12:43.520 --> 00:12:46.460
designed for learning about compositional neural networks.

162
00:12:46.460 --> 00:12:52.100
So we have everything set up as far as that data processing and coming up next we're going to do is

163
00:12:52.100 --> 00:12:55.180
we'll discuss actually training and creating the model.

164
00:12:55.220 --> 00:12:56.900
Thanks and I'll see you at the next lecture.
