WEBVTT
1
00:00:05.310 --> 00:00:06.490
Welcome back everyone.

2
00:00:06.570 --> 00:00:10.760
In this lecture we're going to go over creating and train the model for our project.

3
00:00:10.800 --> 00:00:12.400
Let's head to the notebook and get started.

4
00:00:12.430 --> 00:00:12.680
Okay.

5
00:00:12.690 --> 00:00:16.230
Here I am under creating the model section of our exercise notebook.

6
00:00:16.290 --> 00:00:22.230
We'll go ahead and run this cell to run the imports necessary specifically sequential and dense and

7
00:00:22.320 --> 00:00:27.030
optionally you can add in dropout layers that we went over during the classification lecture of the

8
00:00:27.030 --> 00:00:28.470
section of the course.

9
00:00:28.470 --> 00:00:32.130
So you have a lot of flexibility here over the number of layers and neurons you want.

10
00:00:32.220 --> 00:00:40.170
We're going to follow a pretty simple approach here where if we take a look at X train's shape it starts

11
00:00:40.170 --> 00:00:41.730
off with 78 features.

12
00:00:41.730 --> 00:00:47.170
So what I'm going to do is have my first layer match that we'll say model the add dance.

13
00:00:47.250 --> 00:00:51.510
And it's a pretty good rule of thumb that you're very first layer should probably match the same number

14
00:00:51.510 --> 00:00:54.450
of features for just a normal artificial neural network.

15
00:00:54.570 --> 00:01:01.040
And then the activation function we will choose is rectified linear unit and then to try to help prevent

16
00:01:01.040 --> 00:01:01.790
over fitting.

17
00:01:01.830 --> 00:01:04.550
We'll go ahead and add in a dropout layer.

18
00:01:04.960 --> 00:01:06.420
And is something you can play around with.

19
00:01:06.470 --> 00:01:12.350
As far as the dropout rate but I'll go ahead and just do 20 percent here and then I'm going to copy

20
00:01:12.350 --> 00:01:18.400
this and add in a couple more hidden layers but what I'm going to do is I'm essentially going to reduce

21
00:01:18.400 --> 00:01:21.830
the number of neurons about half for each layer.

22
00:01:21.940 --> 00:01:28.810
So we'll go from seventy eight to thirty nine and then thirty nine to 19 and we'll have dropouts on

23
00:01:28.840 --> 00:01:34.540
each of those layers and then this is the layers that you do need to match up for is the output layer

24
00:01:35.020 --> 00:01:40.420
recall we're really just performing binary classification which means we should have one neuron at the

25
00:01:40.420 --> 00:01:40.960
very end.

26
00:01:41.500 --> 00:01:48.880
So one dense neuron so units is equal to one and it should be using an sigmoid activation function.

27
00:01:49.240 --> 00:01:55.400
So recall the sigmoid activation function pushes values to be between 0 and 1 and next we're going to

28
00:01:55.400 --> 00:02:01.990
do is we will compile the model and this is also where you need to match up so because we're performing

29
00:02:02.020 --> 00:02:03.570
a binary classification.

30
00:02:03.790 --> 00:02:11.410
The loss should be binary underscore cross entropy and then after that we'll go ahead and choose our

31
00:02:11.410 --> 00:02:14.930
optimizer and I'll choose Adam from out the miser.

32
00:02:15.100 --> 00:02:17.420
Go ahead and run that cell and make sure compiles.

33
00:02:17.560 --> 00:02:20.170
And then what's next is we need to fit the model.

34
00:02:20.170 --> 00:02:21.520
Here you have a lot of options.

35
00:02:21.580 --> 00:02:24.690
And if you wanted to you could also add in things like early stopping.

36
00:02:24.790 --> 00:02:28.600
But I will go ahead and just fit the model for twenty five epochs.

37
00:02:28.600 --> 00:02:31.570
So we first pass in our training data.

38
00:02:31.600 --> 00:02:40.230
So say X train and Y train and we'll say epochs is equal to twenty five.

39
00:02:40.420 --> 00:02:46.720
Now because this is a much larger dataset it's probably a good idea to feed this in in batches so as

40
00:02:46.720 --> 00:02:52.480
recommended by the task we'll say batch size is equal to two hundred and fifty six.

41
00:02:52.870 --> 00:02:57.880
And then finally we also want to pass in our validation data so we can plot out the losses and see for

42
00:02:57.880 --> 00:02:59.050
over fitting anywhere.

43
00:02:59.200 --> 00:03:03.670
So we'll pass an X test and Y test during the actual fit call.

44
00:03:04.180 --> 00:03:10.220
So if we go ahead and run this let me clear the old running and let me run this guy.

45
00:03:10.420 --> 00:03:12.070
Make sure we don't get any errors.

46
00:03:12.160 --> 00:03:12.910
And there we go.

47
00:03:12.910 --> 00:03:19.990
So I'm going to essentially end it here and in the very next lecture we're going to do is after this

48
00:03:19.990 --> 00:03:24.000
is done training we're going to move on to evaluating model performance.

49
00:03:24.100 --> 00:03:28.330
And again if you're on a slower computer or you're running this on google cloud you can always choose

50
00:03:28.360 --> 00:03:32.170
less epochs choose a larger batch size or just eliminate batch size.

51
00:03:32.170 --> 00:03:36.490
However this is enough samples where you should really be feeding it into batches and everything can

52
00:03:36.490 --> 00:03:41.800
do is if this is taking too long to train is as mentioned before you can actually before normalizing

53
00:03:41.820 --> 00:03:48.050
them for doing train test split just use DNA sample to take a sample fraction of the entire data set

54
00:03:48.370 --> 00:03:50.560
since we are dealing with quite a large dataset.

55
00:03:50.590 --> 00:03:51.090
OK.

56
00:03:51.280 --> 00:03:55.600
So this is currently training I will see you in the next lecture where we begin evaluating this model's

57
00:03:55.600 --> 00:03:56.250
performance.
