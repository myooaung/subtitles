WEBVTT
1
00:00:05.440 --> 00:00:07.030
Welcome back everyone.

2
00:00:07.030 --> 00:00:11.950
We've done a lot of discussion on supervised learning methods as well as evaluating the performance

3
00:00:11.950 --> 00:00:16.690
of supervised learning such as regression tasks or classification tasks.

4
00:00:16.690 --> 00:00:23.690
Now let's go ahead and finally discuss unsupervised learning So recall that when we're dealing with

5
00:00:23.690 --> 00:00:30.650
supervised learning the actual label was known due to historical labeled data for example with those

6
00:00:30.650 --> 00:00:32.250
images of dogs and cats.

7
00:00:32.330 --> 00:00:37.970
We actually had historical information where someone manually went in and labeled images as dogs and

8
00:00:37.970 --> 00:00:44.490
cats and then upon model deployment we could see a new image and predict if it's a dog or a cat.

9
00:00:44.630 --> 00:00:50.090
But what actually happens when we don't have historical labels for example we just have all those images

10
00:00:50.270 --> 00:00:53.710
but no one's gone ahead and actually labeled them dogs versus cats.

11
00:00:53.750 --> 00:00:59.720
We just have the RAW image files or we just have raw data in general that doesn't have any sort of label.

12
00:00:59.940 --> 00:01:02.070
This is unsupervised learning.

13
00:01:02.070 --> 00:01:05.470
And there are certain tasks that fall under the umbrella of supervised learning.

14
00:01:05.650 --> 00:01:10.530
There's tasks like clustering anomaly detection and dimensionality reduction.

15
00:01:10.530 --> 00:01:17.130
Let's talk about these individually clustering isn't we're trying to group together unlabeled data points

16
00:01:17.370 --> 00:01:23.610
into categories or clusters and data points are assigned to a cluster based on their similarity to other

17
00:01:23.610 --> 00:01:24.760
points within that cluster.

18
00:01:25.200 --> 00:01:31.170
So if we only had those images well we could attempt to do is cluster them together into two groups

19
00:01:31.500 --> 00:01:36.780
and then we could end up saying hey we have a cluster here that's similar to each other and another

20
00:01:36.780 --> 00:01:38.510
cluster it's similar to each other.

21
00:01:38.550 --> 00:01:44.460
But keep in mind since this is Unsupervised data we technically could end up with an approach that doesn't

22
00:01:44.460 --> 00:01:47.960
actually correspond with the labels we had in our minds beforehand.

23
00:01:47.970 --> 00:01:54.180
For example if we specified we wanted to cluster together these images into two main clusters we could

24
00:01:54.180 --> 00:02:01.020
accidentally end up clustering large dogs and large cats in one image cluster and then small dogs and

25
00:02:01.020 --> 00:02:03.500
small cats and another image cluster.

26
00:02:03.510 --> 00:02:09.810
So what's going to happen is your machinery model is going to attempt to cluster together based off

27
00:02:09.810 --> 00:02:15.030
similarity and you're not always guaranteed a correct result because remember you didn't have the label

28
00:02:15.030 --> 00:02:15.660
to begin with.

29
00:02:15.660 --> 00:02:20.940
So there's technically kind of no right answer which makes actually evaluating these models performance

30
00:02:21.090 --> 00:02:28.000
a lot more nuanced now than there's also anomaly detection which attempts to detect outliers in a data

31
00:02:28.000 --> 00:02:33.890
set for example fraudulent transactions on a credit card would be anomalies and we would like to detect

32
00:02:33.890 --> 00:02:39.920
those and technically often you're not going to have historical labels of fraudulent transactions because

33
00:02:39.920 --> 00:02:41.180
they're so rare to occur.

34
00:02:41.540 --> 00:02:47.030
So really what we're looking for here are points that are really dissimilar from everything else in

35
00:02:47.030 --> 00:02:52.130
the dataset and that also falls often under Unsupervised Learning because you don't have the historical

36
00:02:52.130 --> 00:02:55.880
label of fraudulent transactions.

37
00:02:56.230 --> 00:02:58.390
And then there's also a dimensionality reduction.

38
00:02:58.570 --> 00:03:03.520
And I mentioned earlier reduction are data processing techniques that reduce the number of features

39
00:03:03.520 --> 00:03:09.400
in the dataset either for the purposes of compression or to just better understand underlying trends

40
00:03:09.400 --> 00:03:13.930
within the dataset so an unsupervised learning.

41
00:03:13.940 --> 00:03:19.970
The most important thing to note here is that these are situations where we do not know and do not have

42
00:03:20.060 --> 00:03:22.450
a correct answer for this historical data.

43
00:03:22.460 --> 00:03:28.610
We don't have that label which means evaluation is much harder and more nuanced because there's technically

44
00:03:28.640 --> 00:03:31.140
no correct answer it's actually compare to.

45
00:03:31.190 --> 00:03:36.350
So you're not going to get things like accuracy precision or root mean square error because technically

46
00:03:36.650 --> 00:03:42.800
there's no correct answer to actually compare this to if you had that label to begin with then you would

47
00:03:42.800 --> 00:03:44.960
have just done supervised learning.

48
00:03:44.960 --> 00:03:50.810
So that's what makes unsupervised learning not necessarily harder but just the evaluation of it is a

49
00:03:50.810 --> 00:03:53.750
lot harder.

50
00:03:53.850 --> 00:03:59.190
And keep in mind for an unsupervised process we no longer have that test train split since we don't

51
00:03:59.190 --> 00:04:02.850
have that right answer to compare with on the test data.

52
00:04:02.850 --> 00:04:07.560
So we end up just acquiring the data cleaning the data and then we're going to train or fit our model

53
00:04:07.800 --> 00:04:12.900
on all the data and then afterwards perform some sort of transformation on that data set such as that

54
00:04:12.900 --> 00:04:17.880
dimensionality reduction and then once we do that we can deploy the model but again we don't have that

55
00:04:17.880 --> 00:04:19.580
correct answer to compare to.

56
00:04:19.650 --> 00:04:24.570
So we're not really able to do any sort of comparison to a test set when it comes to an unsupervised

57
00:04:24.570 --> 00:04:30.710
learning process now later on in this course we're gonna explore unsupervised learning processes with

58
00:04:30.710 --> 00:04:36.860
specialized neural network structures such as auto encoders but for the first major chunk of the course

59
00:04:36.860 --> 00:04:41.060
we're really going to focus on supervised learning and then later on we'll come back to unsupervised

60
00:04:41.060 --> 00:04:42.010
learning.

61
00:04:42.020 --> 00:04:42.650
All right.

62
00:04:42.770 --> 00:04:43.240
Thanks.

63
00:04:43.280 --> 00:04:44.390
And I'll see you at the next lecture.
