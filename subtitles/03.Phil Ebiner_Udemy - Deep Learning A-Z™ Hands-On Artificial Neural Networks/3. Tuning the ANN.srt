1
1

00:00:00,230  -->  00:00:02,430
<v Narrator>Hello and welcome to this python tutorial.</v>
2

2

00:00:02,430  -->  00:00:03,823
Alright so let's take this challenge
3

3

00:00:03,823  -->  00:00:06,399
to reach this 86% accuracy
4

4

00:00:06,399  -->  00:00:08,636
which is the relevant measure of the accuracy
5

5

00:00:08,636  -->  00:00:10,693
through careful cross validation
6

6

00:00:10,693  -->  00:00:12,550
and the solution to succeed
7

7

00:00:12,550  -->  00:00:15,054
the best tool to use to accomplish this goal
8

8

00:00:15,054  -->  00:00:17,280
is parameter tuning.
9

9

00:00:17,280  -->  00:00:19,220
So what is parameter tuning?
10

10

00:00:19,220  -->  00:00:20,910
Okay, so as you noticed
11

11

00:00:20,910  -->  00:00:23,519
in this first artificial neural network that we just built
12

12

00:00:23,519  -->  00:00:25,370
we have two type of parameters.
13

13

00:00:25,370  -->  00:00:29,090
We have the parameters that are learnt from the model
14

14

00:00:29,090  -->  00:00:31,660
during the training and these are the weights
15

15

00:00:31,660  -->  00:00:34,973
and we have some other parameters that stay fixed
16

16

00:00:34,973  -->  00:00:38,311
and these parameters are called the hyper parameters.
17

17

00:00:38,311  -->  00:00:40,459
So for example these hyper parameters
18

18

00:00:40,459  -->  00:00:44,223
are the number of a epoch, the batch size, the optimizer
19

19

00:00:44,223  -->  00:00:47,120
or the number of neurons in the layers.
20

20

00:00:47,120  -->  00:00:49,880
And when we trained our artificial neural network
21

21

00:00:49,880  -->  00:00:52,470
well, we trained it with some fixed values
22

22

00:00:52,470  -->  00:00:55,160
of these hyper parameters. But maybe that
23

23

00:00:55,160  -->  00:00:57,630
by taking some other values, we would get to
24

24

00:00:57,630  -->  00:01:00,656
a better accuracy overall with careful cross validation.
25

25

00:01:00,656  -->  00:01:03,572
And so that's what parameter tuning is all about.
26

26

00:01:03,572  -->  00:01:06,089
It consists of finding the best values
27

27

00:01:06,089  -->  00:01:09,580
of these hyper parameters and we are gonna do this
28

28

00:01:09,580  -->  00:01:11,277
with a technique called grid search
29

29

00:01:11,277  -->  00:01:14,740
that basically will test several combinations
30

30

00:01:14,740  -->  00:01:17,078
of these values and will eventually return
31

31

00:01:17,078  -->  00:01:20,179
the best selection, the best choice that leads
32

32

00:01:20,179  -->  00:01:24,580
to the best accuracy with careful cross validation.
33

33

00:01:24,580  -->  00:01:26,610
Alright, so let's implement parameter tuning
34

34

00:01:26,610  -->  00:01:29,811
with grid search on our artificial neural network.
35

35

00:01:29,811  -->  00:01:33,220
So the good news is that it's actually quite the same
36

36

00:01:33,220  -->  00:01:35,129
as implementing careful cross validation
37

37

00:01:35,129  -->  00:01:37,798
because we will use the kerasclassifier
38

38

00:01:37,798  -->  00:01:41,470
to wrap our artificial neural network in a classifier
39

39

00:01:41,470  -->  00:01:43,281
that can be used for scikit_learn
40

40

00:01:43,281  -->  00:01:46,004
but instead of using the cross_val score function
41

41

00:01:46,004  -->  00:01:48,448
we will use the grid search cv class
42

42

00:01:48,448  -->  00:01:50,753
and so basically we will create an object
43

43

00:01:50,753  -->  00:01:52,775
that we're gonna call gridsearch of this class
44

44

00:01:52,775  -->  00:01:55,770
and this is this object that will apply
45

45

00:01:55,770  -->  00:01:58,537
parameter tuning on our kerasclassifier
46

46

00:01:58,537  -->  00:02:01,220
that is our traditional neural network.
47

47

00:02:01,220  -->  00:02:03,550
So that's good news because, basically what we can do
48

48

00:02:03,550  -->  00:02:05,519
to implement this very efficiently
49

49

00:02:05,519  -->  00:02:09,918
is to take everything from here to the top,
50

50

00:02:09,918  -->  00:02:14,010
copy, and we're gonna paste that here in the section
51

51

00:02:14,010  -->  00:02:16,347
tuning the ANN. And now, as I just explained,
52

52

00:02:16,347  -->  00:02:19,360
we will not choose the cross_val_score function
53

53

00:02:19,360  -->  00:02:21,723
but we will use the GridSearchCV class.
54

54

00:02:24,940  -->  00:02:26,455
And then, we import this class from
55

55

00:02:26,455  -->  00:02:28,905
model_selection module. If, by any chance,
56

56

00:02:28,905  -->  00:02:30,779
model_selection doesn't work for you
57

57

00:02:30,779  -->  00:02:32,849
and that depends on the version you have,
58

58

00:02:32,849  -->  00:02:36,633
try to import it from scikitlearn.grid_search
59

59

00:02:36,633  -->  00:02:40,550
module. That's actually the old name of the module
60

60

00:02:40,550  -->  00:02:44,193
to import gridsearchcv. But it's either gridsearchcv
61

61

00:02:44,193  -->  00:02:46,390
or model_selection. Okay and now
62

62

00:02:46,390  -->  00:02:48,820
we need to import sequential and dense
63

63

00:02:48,820  -->  00:02:51,110
to make our artificial neural network
64

64

00:02:51,110  -->  00:02:52,833
and as I told you now it's exactly the same.
65

65

00:02:52,833  -->  00:02:55,570
We use this build classifier function
66

66

00:02:55,570  -->  00:02:58,640
to build the architecture of our ANN.
67

67

00:02:58,640  -->  00:03:00,830
So that's what we do here. Nothing has
68

68

00:03:00,830  -->  00:03:03,102
to be changed so far and then we wrap
69

69

00:03:03,102  -->  00:03:05,740
the whole architecture in this classifier
70

70

00:03:05,740  -->  00:03:07,757
using the kerasclassifier class
71

71

00:03:07,757  -->  00:03:09,609
but in this kerasclassifier class we will not
72

72

00:03:09,609  -->  00:03:13,542
input the batch size and number of epoch arguments
73

73

00:03:13,542  -->  00:03:15,675
because that's the arguments we're gonna tune
74

74

00:03:15,675  -->  00:03:17,858
and the arguments we're gonna tune
75

75

00:03:17,858  -->  00:03:19,887
will be put separately in the gridsearch object
76

76

00:03:19,887  -->  00:03:22,970
that we're going to create. So we have to remove
77

77

00:03:22,970  -->  00:03:26,686
these two arguments here and we'll use them
78

78

00:03:26,686  -->  00:03:29,423
in the gridsearch object. Okay, and now,
79

79

00:03:29,423  -->  00:03:32,330
that's when things start to change because
80

80

00:03:32,330  -->  00:03:34,950
now we have to code the part that is related
81

81

00:03:34,950  -->  00:03:38,878
to gridsearch. So, it's going to take about 10 lines
82

82

00:03:38,878  -->  00:03:40,763
but no worries, that will be simple.
83

83

00:03:40,763  -->  00:03:43,180
So, what do we need to start with?
84

84

00:03:43,180  -->  00:03:45,884
We need to start by creating a dictionary
85

85

00:03:45,884  -->  00:03:47,950
that will contain the hyper parameters
86

86

00:03:47,950  -->  00:03:50,250
that we want to optimize, that we want
87

87

00:03:50,250  -->  00:03:52,003
to find the best values.
88

88

00:03:52,003  -->  00:03:54,740
So, let's create a variable for this
89

89

00:03:54,740  -->  00:03:57,313
dictionary that we are gonna call parameters.
90

90

00:03:58,720  -->  00:04:00,881
And so parameters is going to be a dictionary.
91

91

00:04:00,881  -->  00:04:03,768
So, a dictionary in python is a structure of data
92

92

00:04:03,768  -->  00:04:06,585
that contains keys, that is identifiers,
93

93

00:04:06,585  -->  00:04:09,101
and some values for these keys
94

94

00:04:09,101  -->  00:04:11,296
and there can be several values for each key,
95

95

00:04:11,296  -->  00:04:13,867
and so basically each key is going to be
96

96

00:04:13,867  -->  00:04:15,955
the hyper parameter that we want to input
97

97

00:04:15,955  -->  00:04:18,850
and the values of the keys are going to be
98

98

00:04:18,850  -->  00:04:20,710
the values that we want to try, that is,
99

99

00:04:20,710  -->  00:04:23,226
to test for our artificial neural network.
100

100

00:04:23,226  -->  00:04:25,870
And then, all these values for all the keys
101

101

00:04:25,870  -->  00:04:28,588
will be combined and what gridsearch will do
102

102

00:04:28,588  -->  00:04:31,555
is it will train the artificial neural network
103

103

00:04:31,555  -->  00:04:33,310
using careful cross validation
104

104

00:04:33,310  -->  00:04:35,814
to get a relevant accuracy with the different combinations
105

105

00:04:35,814  -->  00:04:38,145
of these values and eventually in the end
106

106

00:04:38,145  -->  00:04:41,370
it will return best accuracy with
107

107

00:04:41,370  -->  00:04:43,580
the best selection of these values.
108

108

00:04:43,580  -->  00:04:45,090
So now what we have to do is to
109

109

00:04:45,090  -->  00:04:47,000
create this dictionary with all the combinations
110

110

00:04:47,000  -->  00:04:49,358
of the hyper parameters that we want to optimize.
111

111

00:04:49,358  -->  00:04:52,420
So how do we create a dictionary in Python?
112

112

00:04:52,420  -->  00:04:53,253
Well, that's very easy.
113

113

00:04:53,253  -->  00:04:56,250
We have to use some brackets and inside
114

114

00:04:56,250  -->  00:04:58,510
these brackets we're going to input the keys
115

115

00:04:58,510  -->  00:05:00,810
that is the hyper parameters, and their
116

116

00:05:00,810  -->  00:05:02,500
values that we want to test.
117

117

00:05:02,500  -->  00:05:05,210
Okay, so what would we like to optimize?
118

118

00:05:05,210  -->  00:05:07,217
Well, we have many hyperparameters
119

119

00:05:07,217  -->  00:05:09,340
in the architecture and the training
120

120

00:05:09,340  -->  00:05:12,457
of our ANN, so we can for example tune the batch size.
121

121

00:05:12,457  -->  00:05:16,054
You know, we can try several batch sizes because one of them
122

122

00:05:16,054  -->  00:05:18,008
will lead us to a better accuracy.
123

123

00:05:18,008  -->  00:05:21,830
We can also tune the number of epoch for that same reason.
124

124

00:05:21,830  -->  00:05:23,875
There is an optimal number of epoch.
125

125

00:05:23,875  -->  00:05:26,550
But we can also tune some hyper parameters
126

126

00:05:26,550  -->  00:05:29,658
in the architecture of the ANN, like the optimizer
127

127

00:05:29,658  -->  00:05:31,638
and some other hyper parameters.
128

128

00:05:31,638  -->  00:05:34,559
So let's start with the batch size.
129

129

00:05:34,559  -->  00:05:36,870
We're going to input the batch size
130

130

00:05:36,870  -->  00:05:38,710
and different values of the batch size
131

131

00:05:38,710  -->  00:05:40,599
in the dictionary. So how do we do that?
132

132

00:05:40,599  -->  00:05:43,290
Well, we have to put the name of the hyper parameter
133

133

00:05:43,290  -->  00:05:45,932
we want to optimize in quotes, like this.
134

134

00:05:45,932  -->  00:05:48,510
And we put the exact same name as the one
135

135

00:05:48,510  -->  00:05:50,970
given in the model, that is, we input
136

136

00:05:50,970  -->  00:05:55,260
batch size. Alright, so that's the key.
137

137

00:05:55,260  -->  00:05:58,597
And now, we need to test several values for the batch size.
138

138

00:05:58,597  -->  00:06:02,193
So, we add here a column and in square brackets
139

139

00:06:02,193  -->  00:06:04,526
we input the values we want to test.
140

140

00:06:04,526  -->  00:06:06,435
So which one would you like to test?
141

141

00:06:06,435  -->  00:06:08,952
Well, we've seen in the British tutorial
142

142

00:06:08,952  -->  00:06:12,474
that a batch size of 10 led us to a relevant accuracy
143

143

00:06:12,474  -->  00:06:15,678
of 83% and we would like to beat 83%
144

144

00:06:15,678  -->  00:06:18,850
so we will not test 10 but some
145

145

00:06:18,850  -->  00:06:20,710
other values of the batch size.
146

146

00:06:20,710  -->  00:06:22,627
Let's test 2 values of the batch size
147

147

00:06:22,627  -->  00:06:27,032
and we're gonna try 25 and 32.
148

148

00:06:27,032  -->  00:06:31,008
So why 25 and 32? Well, that's based on my experience
149

149

00:06:31,008  -->  00:06:34,079
and that's also common practice to take powers of 2.
150

150

00:06:34,079  -->  00:06:36,824
Well, you can try other values of the batch size
151

151

00:06:36,824  -->  00:06:39,078
but let's see what we get with these ones.
152

152

00:06:39,078  -->  00:06:41,887
Okay, so, that's for the first parameter.
153

153

00:06:41,887  -->  00:06:44,951
Now, let's take care of the second hyper parameter.
154

154

00:06:44,951  -->  00:06:48,210
What is it going to be? Well, let's try
155

155

00:06:48,210  -->  00:06:51,540
to optimize the number of epoch.
156

156

00:06:51,540  -->  00:06:53,579
So same, we input the real name
157

157

00:06:53,579  -->  00:06:57,583
of the number of epoch parameter, that is, nb_epoch
158

158

00:06:58,455  -->  00:07:00,750
so basically it has to be the same name
159

159

00:07:00,750  -->  00:07:02,710
as the one used to train the ANN.
160

160

00:07:02,710  -->  00:07:06,118
So nb_epoch and here that's the same, a column,
161

161

00:07:06,118  -->  00:07:09,866
and in square brackets we input the values we want to test.
162

162

00:07:09,866  -->  00:07:13,470
So okay, let's try again with a hundred
163

163

00:07:13,470  -->  00:07:15,910
but we won't get the same result as what we got here
164

164

00:07:15,910  -->  00:07:17,590
because we used a batch size of 10
165

165

00:07:17,590  -->  00:07:20,879
and now we use a batch size of 25 and then 32.
166

166

00:07:20,879  -->  00:07:24,715
So let's try 100 and let's also try another value,
167

167

00:07:24,715  -->  00:07:28,070
like, for example, 500.
168

168

00:07:28,070  -->  00:07:30,627
You know, maybe we won't reach convergence with 100
169

169

00:07:30,627  -->  00:07:33,350
and maybe 500 will lead us to a better result.
170

170

00:07:33,350  -->  00:07:34,440
We never know.
171

171

00:07:34,440  -->  00:07:37,070
So okay, that's already 2 hyper parameters
172

172

00:07:37,070  -->  00:07:38,380
that we are tuning and now
173

173

00:07:38,380  -->  00:07:39,720
I am giving you a little challenge.
174

174

00:07:39,720  -->  00:07:41,359
What if we want to optimize
175

175

00:07:41,359  -->  00:07:44,584
a hyper parameter that is in the architecture here?
176

176

00:07:44,584  -->  00:07:47,657
For example, let's say we want to tune the optimizer.
177

177

00:07:47,657  -->  00:07:51,793
Well, what we would need to do is to add a third key here
178

178

00:07:51,793  -->  00:07:55,990
and, in quotes, we'll put the name of the hyper parameter
179

179

00:07:55,990  -->  00:08:00,130
that is, optimizer, and column, but then,
180

180

00:08:00,130  -->  00:08:01,980
how do we input some different values,
181

181

00:08:01,980  -->  00:08:04,220
considering that we already have a values
182

182

00:08:04,220  -->  00:08:06,630
of the optimizer here? You know we didn't have values
183

183

00:08:06,630  -->  00:08:08,650
for the number of epoch and the batch size,
184

184

00:08:08,650  -->  00:08:10,638
so we can try several of them here,
185

185

00:08:10,638  -->  00:08:13,738
but here we already have a value for the optimizer.
186

186

00:08:13,738  -->  00:08:16,017
So how can we test some other ones?
187

187

00:08:16,017  -->  00:08:18,303
Well there is something specific we have to do.
188

188

00:08:18,303  -->  00:08:22,792
We have to create a new parameter of the build classifier
189

189

00:08:22,792  -->  00:08:26,034
function, and this new argument is of course going to be,
190

190

00:08:26,034  -->  00:08:29,887
an argument that will give us choice for the optimizer.
191

191

00:08:29,887  -->  00:08:32,590
So how does it work? Well we're going to give a
192

192

00:08:32,590  -->  00:08:35,090
new name to this optimizer argument,
193

193

00:08:35,090  -->  00:08:38,210
and we're going to call it optimizer, and here
194

194

00:08:38,210  -->  00:08:43,210
we're going to copy this and replace the Adam optimizer here
195

195

00:08:44,374  -->  00:08:47,560
by this optimizer argument that now plays
196

196

00:08:47,560  -->  00:08:50,540
the role of a variable.
197

197

00:08:50,540  -->  00:08:54,190
And so now, we have the right to input different values
198

198

00:08:54,190  -->  00:08:57,020
that we want to test for our optimizer because
199

199

00:08:57,020  -->  00:09:00,700
what will happen is that this key here, will be associated
200

200

00:09:00,700  -->  00:09:03,820
to this optimizer and so when we give different values
201

201

00:09:03,820  -->  00:09:06,250
for this key, well the different values are going
202

202

00:09:06,250  -->  00:09:09,380
to be tested in this optimizer here.
203

203

00:09:09,380  -->  00:09:12,139
So that's the trick, and therefore if you want to tune
204

204

00:09:12,139  -->  00:09:15,159
and hyper parameters that are in this architecture here,
205

205

00:09:15,159  -->  00:09:17,403
that is the architecture of your ANN.
206

206

00:09:17,403  -->  00:09:21,113
Well what you have to do, is to create a new argument
207

207

00:09:21,113  -->  00:09:23,021
for the hyper parameter you want to tune
208

208

00:09:23,021  -->  00:09:28,021
and then, replace the value here by name of your argument.
209

209

00:09:28,080  -->  00:09:31,808
Okay, so lets try some different values of our optimizer.
210

210

00:09:31,808  -->  00:09:35,660
So still we use some square brackets and inside
211

211

00:09:35,660  -->  00:09:37,690
the square brackets we're going to try several
212

212

00:09:37,690  -->  00:09:41,011
optimizers, and so of course we're going to try Adam
213

213

00:09:41,011  -->  00:09:44,360
optimizer and then, lets try another one.
214

214

00:09:44,360  -->  00:09:48,219
Actually another very good optimizer that is sometimes
215

215

00:09:48,219  -->  00:09:51,405
a better one for deep learning models, is the
216

216

00:09:51,405  -->  00:09:56,116
rmsprop optimizer, which is another excellent optimizer
217

217

00:09:56,116  -->  00:09:58,201
based on stochastic gradient descent.
218

218

00:09:58,201  -->  00:10:01,100
Feel free to have a look at the Keras documentation
219

219

00:10:01,100  -->  00:10:03,410
to get more info and you'll see for example,
220

220

00:10:03,410  -->  00:10:05,437
that they recommend to use this rmsprop
221

221

00:10:05,437  -->  00:10:08,840
for regular neural networks, this is generally a better
222

222

00:10:08,840  -->  00:10:10,456
choice and indeed this is the optimizer
223

223

00:10:10,456  -->  00:10:13,121
that we're going to use for regular neural networks.
224

224

00:10:13,121  -->  00:10:15,831
But lets still try it for our ANN,
225

225

00:10:15,831  -->  00:10:18,970
maybe this will lead us to better results.
226

226

00:10:18,970  -->  00:10:23,336
Okay, so that's it for the parameters that we want to tune,
227

227

00:10:23,336  -->  00:10:25,529
that is try to find the optimal values.
228

228

00:10:25,529  -->  00:10:28,390
And now what we have to do is of course
229

229

00:10:28,390  -->  00:10:31,175
to start to implement grid search, and so remember
230

230

00:10:31,175  -->  00:10:35,470
we import the gridsearchcv class and so now you can guess
231

231

00:10:35,470  -->  00:10:36,303
what we're going to do.
232

232

00:10:36,303  -->  00:10:39,130
We're going to create an object of this class
233

233

00:10:39,130  -->  00:10:41,440
that will contain this parameter dictionary
234

234

00:10:41,440  -->  00:10:44,045
with all the hyper parameters that we want to tune,
235

235

00:10:44,045  -->  00:10:46,750
and the different values that we want to test.
236

236

00:10:46,750  -->  00:10:49,540
And of course this gridserch object will contain our
237

237

00:10:49,540  -->  00:10:51,001
estimator that is our classifier
238

238

00:10:51,001  -->  00:10:54,710
and eventually it will also contain the information
239

239

00:10:54,710  -->  00:10:56,399
related to kfold cross validation.
240

240

00:10:56,399  -->  00:10:59,209
Because remember, when we tune our model with gridsearch
241

241

00:10:59,209  -->  00:11:01,709
well kfold cross validation is going to be used
242

242

00:11:01,709  -->  00:11:05,430
to evaluate the accuracy and so it we'll need to enter
243

243

00:11:05,430  -->  00:11:08,750
for gridsearch is, a scoring metric which is going
244

244

00:11:08,750  -->  00:11:12,210
to be the accuracy, and the number of folds in
245

245

00:11:12,210  -->  00:11:15,108
kfold cross validation, and of course we will choose ten
246

246

00:11:15,108  -->  00:11:17,227
like we did for kfold cross validation
247

247

00:11:17,227  -->  00:11:19,130
in the previous tutorial.
248

248

00:11:19,130  -->  00:11:22,950
So here we go, let's create the grid search object
249

249

00:11:22,950  -->  00:11:25,940
that we're going to call grid-search and so this is
250

250

00:11:25,940  -->  00:11:29,022
an object from the GridSearchCV class,
251

251

00:11:29,022  -->  00:11:32,423
so I'm copying this and pasting it here
252

252

00:11:32,423  -->  00:11:35,509
and now some parenthesis and inside the parenthesis
253

253

00:11:35,509  -->  00:11:37,627
we're going to input the parameters of the
254

254

00:11:37,627  -->  00:11:41,600
GridSearchCV class, so lets inspect the object.
255

255

00:11:41,600  -->  00:11:44,088
So as I just said, the first argument we have to input is
256

256

00:11:44,088  -->  00:11:49,088
are estimator and that of course our classifier
257

257

00:11:49,430  -->  00:11:54,430
which is our ANN, then second argument is our parameters
258

258

00:11:54,740  -->  00:11:57,170
that we want to optimize, that we want to tune.
259

259

00:11:57,170  -->  00:11:59,873
And the name for this argument is param_grid,
260

260

00:12:01,470  -->  00:12:04,080
and so for this param grid argument we need to input
261

261

00:12:04,080  -->  00:12:06,984
are parameters dictionary, that contains
262

262

00:12:06,984  -->  00:12:10,010
all the combinations that we want to try
263

263

00:12:10,010  -->  00:12:12,283
and for each of these combinations, we will use
264

264

00:12:12,283  -->  00:12:15,710
kfold cross validation with a specific metric that is
265

265

00:12:15,710  -->  00:12:18,194
going to be the accuracy and a specific number of folds,
266

266

00:12:18,194  -->  00:12:20,434
that is going to be ten folds.
267

267

00:12:20,434  -->  00:12:23,400
And that's actually the last two arguments we need to input.
268

268

00:12:23,400  -->  00:12:27,090
That is the scoring metric, that is given by the
269

269

00:12:27,090  -->  00:12:30,883
argument scoring and so that is the accuracy.
270

270

00:12:32,870  -->  00:12:37,680
And finally the number of folds, which is ten.
271

271

00:12:37,680  -->  00:12:40,070
So we close the parenthesis and here we go.
272

272

00:12:40,070  -->  00:12:42,789
Our grid search object is ready, it contains all the
273

273

00:12:42,789  -->  00:12:46,966
information we need to apply grid search to our ANN.
274

274

00:12:46,966  -->  00:12:50,594
But now, as you can notice, this grid search objects
275

275

00:12:50,594  -->  00:12:53,104
is not yet fitted to the training set,
276

276

00:12:53,104  -->  00:12:55,195
so that's exactly what we have to do now.
277

277

00:12:55,195  -->  00:12:58,892
We need to fit grid search to the training set,
278

278

00:12:58,892  -->  00:13:01,340
so to do this, that's exactly the same
279

279

00:13:01,340  -->  00:13:03,079
as what we used to do with the other objects.
280

280

00:13:03,079  -->  00:13:05,510
For example when fitted this standard scaler objects
281

281

00:13:05,510  -->  00:13:08,535
to the training set, we need to take our object
282

282

00:13:08,535  -->  00:13:13,273
gridsearch, then dot the we use the fit method
283

283

00:13:13,273  -->  00:13:16,540
and inside the fit method we input the training set.
284

284

00:13:16,540  -->  00:13:18,480
And the training set is composed of the matrix of
285

285

00:13:18,480  -->  00:13:23,480
features x_train and the dependent variable vector y_train.
286

286

00:13:25,170  -->  00:13:28,997
And actually this grid search that fit x train, y train
287

287

00:13:28,997  -->  00:13:32,659
will create a new object and so we will give the same
288

288

00:13:32,659  -->  00:13:36,392
name grid_search to this new object and so here,
289

289

00:13:36,392  -->  00:13:39,010
I'm giving the same name grid search that
290

290

00:13:39,010  -->  00:13:41,430
will be fitted to the train set.
291

291

00:13:41,430  -->  00:13:45,011
Okay, so this will fit are ANN to the training set
292

292

00:13:45,011  -->  00:13:48,200
while running grid search to find the optimal
293

293

00:13:48,200  -->  00:13:50,520
hyper parameters that we're testing here,
294

294

00:13:50,520  -->  00:13:54,389
so we would already be ready to execute the code from now
295

295

00:13:54,389  -->  00:13:56,940
but let's just add two lines of code
296

296

00:13:56,940  -->  00:14:00,070
to get what we're mostly interested in, that is the
297

297

00:14:00,070  -->  00:14:01,572
best selection of parameters and
298

298

00:14:01,572  -->  00:14:04,400
of course the best accuracy.
299

299

00:14:04,400  -->  00:14:07,730
So we're going to create to new variables that will contain
300

300

00:14:07,730  -->  00:14:12,380
this information, so lets start with best parapmeters.
301

301

00:14:12,380  -->  00:14:14,540
So here I'm creating a new variable
302

302

00:14:15,380  -->  00:14:18,473
and then best accuracy.
303

303

00:14:20,030  -->  00:14:23,510
Alright, now how can we get these best parameters
304

304

00:14:23,510  -->  00:14:26,463
and this best accuracy? Well there is nothing more simple,
305

305

00:14:26,463  -->  00:14:30,121
gridsearch is an object from the class gridsearchcv,
306

306

00:14:30,121  -->  00:14:33,462
and if you look at the info of the gridsearchcv class
307

307

00:14:33,462  -->  00:14:36,203
you will see in the end that in contains some attributes
308

308

00:14:36,203  -->  00:14:40,070
and one of them is best params, that will get us exactly
309

309

00:14:40,070  -->  00:14:42,769
the best parameters and another one is best score
310

310

00:14:42,769  -->  00:14:46,890
which gives us the best accuracy that result from this best
311

311

00:14:46,890  -->  00:14:48,502
selection of hyper parameters.
312

312

00:14:48,502  -->  00:14:50,998
Okay, so let's get these two attributes?
313

313

00:14:50,998  -->  00:14:55,390
So to get an attribute from a class, we need take the object
314

314

00:14:55,390  -->  00:15:00,180
grid_search then dot and then we just need to add the name
315

315

00:15:00,180  -->  00:15:02,725
of the attribute, so the attribute for best parameters
316

316

00:15:02,725  -->  00:15:07,725
is best underscore params then underscore and same
317

317

00:15:09,410  -->  00:15:12,985
for the attribute for the best accuracy, we take our object
318

318

00:15:12,985  -->  00:15:17,985
grid search then dot and then we add the name of attribute
319

319

00:15:18,320  -->  00:15:23,125
which is best underscore score and underscore again.
320

320

00:15:23,125  -->  00:15:25,320
Alright, and with these two attributes
321

321

00:15:25,320  -->  00:15:27,880
we'll get the best selection of the hyper parameters
322

322

00:15:27,880  -->  00:15:30,279
that we're searching and the best accuracy
323

323

00:15:30,279  -->  00:15:32,309
resulting from this best selection.
324

324

00:15:32,309  -->  00:15:36,294
Okay and now that's the most exciting part because basically
325

325

00:15:36,294  -->  00:15:38,838
we don't have anything to do, everything will be automated.
326

326

00:15:38,838  -->  00:15:42,290
Grid search will find the best selection
327

327

00:15:42,290  -->  00:15:43,629
of the hyper parameters for us.
328

328

00:15:43,629  -->  00:15:45,965
So, you can totally do something else,
329

329

00:15:45,965  -->  00:15:48,761
you can have some lunch, have some dinner, have some fun
330

330

00:15:48,761  -->  00:15:51,188
work on another project, you can even go to sleep
331

331

00:15:51,188  -->  00:15:53,437
and actually that's what I'm about to do because,
332

332

00:15:53,437  -->  00:15:56,334
this will actually take quite some time,
333

333

00:15:56,334  -->  00:15:58,780
several hours maybe, that's why
334

334

00:15:58,780  -->  00:16:00,760
I'm recommending to do something else because
335

335

00:16:00,760  -->  00:16:02,330
this is going to take some time.
336

336

00:16:02,330  -->  00:16:05,300
So feel free to do whatever you want, but in the end
337

337

00:16:05,300  -->  00:16:09,617
you will get a much better result than this 83% accuracy.
338

338

00:16:09,617  -->  00:16:12,778
So lets set some goals with this accuracy.
339

339

00:16:12,778  -->  00:16:16,261
The absolute great goal would be 86%
340

340

00:16:16,261  -->  00:16:18,150
lets set this as the gold medal.
341

341

00:16:18,150  -->  00:16:22,215
Then another great goal less hard to reach would be 85%
342

342

00:16:22,215  -->  00:16:25,170
and lets say that if we reach this goal we have
343

343

00:16:25,170  -->  00:16:27,860
the silver medal and lets say that if we reach
344

344

00:16:27,860  -->  00:16:30,329
84% we have the bronze medal.
345

345

00:16:30,329  -->  00:16:33,821
And before 84% well we're at the same level
346

346

00:16:33,821  -->  00:16:36,530
as in the previous tutorial, which remember
347

347

00:16:36,530  -->  00:16:40,140
was 83%, so we get no medal.
348

348

00:16:40,140  -->  00:16:42,180
Alright, so let's see what happens, I actually
349

349

00:16:42,180  -->  00:16:44,540
don't know what will happen because this is the first
350

350

00:16:44,540  -->  00:16:46,410
time I have run this, so I will discover
351

351

00:16:46,410  -->  00:16:48,500
the results with you, so just like you
352

352

00:16:48,500  -->  00:16:51,120
I'm hoping to get at least one medal.
353

353

00:16:51,120  -->  00:16:52,932
Alright, so now we're going to execute the whole thing,
354

354

00:16:52,932  -->  00:16:56,850
so first we need to import the data set and run the
355

355

00:16:56,850  -->  00:16:59,130
data reprocessing phase, so we're taking
356

356

00:16:59,130  -->  00:17:01,660
everything from here to the top.
357

357

00:17:01,660  -->  00:17:04,459
Here we go, data set well imported and well processed
358

358

00:17:04,459  -->  00:17:08,706
and now let's finally execute the last section,
359

359

00:17:08,706  -->  00:17:11,788
parameter tuning, we have the whole tuning ready.
360

360

00:17:11,788  -->  00:17:16,163
Now we don't have anything to do so have fun here we go.
361

361

00:17:18,290  -->  00:17:20,760
Alright, this is running and this is going to take a while
362

362

00:17:20,760  -->  00:17:22,710
so, we'll let this run and we'll
363

363

00:17:22,710  -->  00:17:24,496
see each other back at the end of the tuning,
364

364

00:17:24,496  -->  00:17:26,133
so I'll see you soon.
365

365

00:17:28,070  -->  00:17:30,858
Alright, welcome back, I think this is about to end.
366

366

00:17:30,858  -->  00:17:35,330
According to my eta, so we are about to know on which
367

367

00:17:35,330  -->  00:17:39,779
step of the podium we are, remember gold is 86%,
368

368

00:17:39,779  -->  00:17:44,779
silver is 85% and bronze is 84% so at least lets
369

369

00:17:46,410  -->  00:17:47,589
hope we get a medal.
370

370

00:17:47,589  -->  00:17:50,478
Alright, so almost over we can see here the last round
371

371

00:17:50,478  -->  00:17:54,660
we are at 86%, that's a good sign but
372

372

00:17:54,660  -->  00:17:57,210
remember that's only the accuracy of one round
373

373

00:17:57,210  -->  00:17:59,881
and maybe that we won't the same good accuracy
374

374

00:17:59,881  -->  00:18:02,972
over ten rounds, and that's why we're computing the main
375

375

00:18:02,972  -->  00:18:05,755
of ten accuracies over ten train test folds.
376

376

00:18:05,755  -->  00:18:10,010
So that we get a relevant measure of the models performance.
377

377

00:18:10,010  -->  00:18:14,720
Okay and now this is about to end is 3 seconds, 3 2 1
378

378

00:18:14,720  -->  00:18:17,324
and here we go, its over we ended up with an accuracy
379

379

00:18:17,324  -->  00:18:22,324
of 86.26% so that's the accuracy of the last round, but
380

380

00:18:22,340  -->  00:18:24,877
lets have a look at the real measure of the accuracy.
381

381

00:18:24,877  -->  00:18:27,890
Which is contained in best accuracy here,
382

382

00:18:27,890  -->  00:18:31,230
and the medal we get is silver
383

383

00:18:31,230  -->  00:18:33,547
because we got an accuracy of 85%.
384

384

00:18:33,547  -->  00:18:36,726
So congratulations, sorry for those who were hoping
385

385

00:18:36,726  -->  00:18:40,313
for the gold medal but don't worry first of all silver
386

386

00:18:40,313  -->  00:18:44,190
is a good achievement, and second of all getting
387

387

00:18:44,190  -->  00:18:46,230
the gold medal is going to be the homework
388

388

00:18:46,230  -->  00:18:48,910
of this section, so that will be excellent for you
389

389

00:18:48,910  -->  00:18:51,921
because you'll get to practice with neural networks,
390

390

00:18:51,921  -->  00:18:54,420
because in order to reach the gold medal
391

391

00:18:54,420  -->  00:18:56,010
you will have several options.
392

392

00:18:56,010  -->  00:18:58,790
One of them is to change the architecture of your
393

393

00:18:58,790  -->  00:19:00,519
neural network, another option is to
394

394

00:19:00,519  -->  00:19:03,080
do some more parameter tuning.
395

395

00:19:03,080  -->  00:19:05,380
And now to finish this tutorial lets have a look at the
396

396

00:19:05,380  -->  00:19:07,767
best parameters that were selected by grid search,
397

397

00:19:07,767  -->  00:19:12,653
here they are in best parameters and the winners are
398

398

00:19:12,653  -->  00:19:16,492
bach size of 25 a number of epoch of 500 and
399

399

00:19:16,492  -->  00:19:21,172
an rmsprop optimizer and so it's with these parameters
400

400

00:19:21,172  -->  00:19:26,172
that we manage to get an 85% accuracy and a silver medal.
401

401

00:19:26,720  -->  00:19:29,170
And now it's your turn, try to get the gold medal
402

402

00:19:29,170  -->  00:19:32,267
put me one set down on the podium, I'll work on
403

403

00:19:32,267  -->  00:19:34,931
getting the gold medal too and as soon as I get it,
404

404

00:19:34,931  -->  00:19:37,174
I'll give you some hints and a solution.
405

405

00:19:37,174  -->  00:19:39,423
Until then enjoy Deep Learning.
