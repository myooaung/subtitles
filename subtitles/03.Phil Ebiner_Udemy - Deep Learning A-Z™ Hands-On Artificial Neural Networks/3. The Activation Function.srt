1
1

00:00:00,150  -->  00:00:00,983
<v Instructor>Hello, and welcome back</v>
2

2

00:00:00,983  -->  00:00:02,630
to the course on deep learning.
3

3

00:00:02,630  -->  00:00:05,060
Alright, today we're talking about the activation function.
4

4

00:00:05,060  -->  00:00:06,920
Let's get straight into it.
5

5

00:00:06,920  -->  00:00:08,560
So, this is where we left off previously.
6

6

00:00:08,560  -->  00:00:11,900
We talked about the structure of one neuron.
7

7

00:00:11,900  -->  00:00:13,180
So, there it is in the middle.
8

8

00:00:13,180  -->  00:00:15,670
We know that it has some inputs values coming in.
9

9

00:00:15,670  -->  00:00:17,010
It's got some weights.
10

10

00:00:17,010  -->  00:00:21,539
Then it calculates the weighted sum of those inputs
11

11

00:00:21,539  -->  00:00:23,640
and then applies the activation function
12

12

00:00:23,640  -->  00:00:28,440
and step three, it passes on the signal to the next neuron.
13

13

00:00:28,440  -->  00:00:29,690
And that's what we're talking about today.
14

14

00:00:29,690  -->  00:00:31,620
We're talking about the value
15

15

00:00:31,620  -->  00:00:32,770
that is going to be passed over.
16

16

00:00:32,770  -->  00:00:34,140
So, we're talking about the activation
17

17

00:00:34,140  -->  00:00:36,280
function that's being applied.
18

18

00:00:36,280  -->  00:00:39,200
So, what options do we have for the activation function?
19

19

00:00:39,200  -->  00:00:41,130
Well, we're gonna look at four different types
20

20

00:00:41,130  -->  00:00:43,320
of activation functions that you can choose from.
21

21

00:00:43,320  -->  00:00:44,750
Of course, there are more different types
22

22

00:00:44,750  -->  00:00:45,583
of activation functions.
23

23

00:00:45,583  -->  00:00:47,180
But these are the predominant ones
24

24

00:00:47,180  -->  00:00:48,170
that you'll be hearing about
25

25

00:00:48,170  -->  00:00:50,290
and that we'll be using in this course.
26

26

00:00:50,290  -->  00:00:53,010
So, here is the threshold function.
27

27

00:00:53,010  -->  00:00:54,220
This is what it looks like.
28

28

00:00:54,220  -->  00:00:58,830
So, on the x-axis, you have the weighted sum of inputs.
29

29

00:00:58,830  -->  00:01:03,830
On the y-axis you have just the values from zero to one.
30

30

00:01:03,870  -->  00:01:06,330
And basically, the threshold function
31

31

00:01:06,330  -->  00:01:07,800
is a very simple type of function
32

32

00:01:07,800  -->  00:01:12,800
where if the value is less than zero,
33

33

00:01:13,400  -->  00:01:16,760
then the threshold function passes on zero.
34

34

00:01:16,760  -->  00:01:20,170
If the value is more than zero or equal to zero
35

35

00:01:20,170  -->  00:01:22,840
then threshold function passes on a one.
36

36

00:01:22,840  -->  00:01:26,860
So, it's basically kind of like a yes no type of function.
37

37

00:01:26,860  -->  00:01:29,040
Very, very straightforward.
38

38

00:01:29,040  -->  00:01:32,060
Very kind of like rigid type of function.
39

39

00:01:32,060  -->  00:01:34,950
Either yes or no, no other options.
40

40

00:01:34,950  -->  00:01:35,783
So, there you go.
41

41

00:01:35,783  -->  00:01:37,330
That's how it works, very simple function.
42

42

00:01:37,330  -->  00:01:40,477
Let's move on to something a bit more complex now.
43

43

00:01:40,477  -->  00:01:42,310
The sigmoid function.
44

44

00:01:42,310  -->  00:01:43,990
A very interesting formula
45

45

00:01:44,850  -->  00:01:47,240
that we have here you'll see just now.
46

46

00:01:47,240  -->  00:01:51,410
There is one divided by one plus e to the power of minus x.
47

47

00:01:51,410  -->  00:01:52,710
Whereas in this case of course,
48

48

00:01:52,710  -->  00:01:57,710
x is the value of the weighted sums.
49

49

00:02:00,090  -->  00:02:02,490
So, this is what the sigmoid looks like.
50

50

00:02:02,490  -->  00:02:06,410
It's a function which is used in the logistic regression
51

51

00:02:06,410  -->  00:02:09,430
if you recall from the machine learning course.
52

52

00:02:09,430  -->  00:02:11,960
So, what is good about this function is that it is smooth.
53

53

00:02:11,960  -->  00:02:15,740
Unlike the threshold function this one
54

54

00:02:15,740  -->  00:02:17,960
doesn't have those kinks in its curve
55

55

00:02:17,960  -->  00:02:21,630
and therefore it's just nice and smooth gradual progression.
56

56

00:02:21,630  -->  00:02:25,570
So, anything below zero, it just like drops off.
57

57

00:02:25,570  -->  00:02:28,717
Above zero, it approximates towards one.
58

58

00:02:30,064  -->  00:02:33,400
And this sigmoid function is very useful
59

59

00:02:33,400  -->  00:02:35,540
in the final layer, in the output layer.
60

60

00:02:35,540  -->  00:02:38,830
Especially when you're trying to predict probabilities.
61

61

00:02:38,830  -->  00:02:41,060
And we'll see that throughout this course.
62

62

00:02:41,060  -->  00:02:43,090
And then we've got the rectifier function.
63

63

00:02:43,090  -->  00:02:45,980
Rectifier function, even though it has a kink
64

64

00:02:45,980  -->  00:02:48,890
is one of the most popular functions
65

65

00:02:48,890  -->  00:02:50,860
for artificial neural networks.
66

66

00:02:50,860  -->  00:02:53,760
So, it goes all the way to zero.
67

67

00:02:53,760  -->  00:02:56,320
It is zero, and then from there
68

68

00:02:56,320  -->  00:02:59,512
it gradually progresses as the input value
69

69

00:02:59,512  -->  00:03:01,630
increases as well.
70

70

00:03:01,630  -->  00:03:03,320
And we'll see that throughout the course.
71

71

00:03:03,320  -->  00:03:04,950
We'll see that in other intuition tutorials
72

72

00:03:04,950  -->  00:03:07,440
and we'll also see that how we use this function
73

73

00:03:07,440  -->  00:03:09,620
in the practical side of the course.
74

74

00:03:09,620  -->  00:03:11,980
And I will comment on this a bit more
75

75

00:03:11,980  -->  00:03:13,480
in a few slides from now.
76

76

00:03:13,480  -->  00:03:14,910
So, just remember that a rectifier function
77

77

00:03:14,910  -->  00:03:17,140
is one of the most used functions
78

78

00:03:17,140  -->  00:03:18,900
in artificial neural networks.
79

79

00:03:18,900  -->  00:03:21,200
And finally, we've got one more function
80

80

00:03:21,200  -->  00:03:22,710
that you will probably hear about.
81

81

00:03:22,710  -->  00:03:25,150
It's the hyperbolic tangent function.
82

82

00:03:25,150  -->  00:03:27,290
It's very similar to the sigmoid function
83

83

00:03:27,290  -->  00:03:30,130
but here the hyperbolic tangent function
84

84

00:03:30,130  -->  00:03:32,330
goes below zero.
85

85

00:03:32,330  -->  00:03:36,550
So, the values go from zero to one, or approximately to one.
86

86

00:03:36,550  -->  00:03:39,610
And go from zero to minus one on the other side.
87

87

00:03:39,610  -->  00:03:42,280
And that can be useful in some applications.
88

88

00:03:42,280  -->  00:03:44,700
So, we're not going to go into too much depth
89

89

00:03:44,700  -->  00:03:45,700
on each one of these functions.
90

90

00:03:45,700  -->  00:03:49,210
I just wanted to acquaint you with them
91

91

00:03:49,210  -->  00:03:50,050
so that you know what they look like
92

92

00:03:50,050  -->  00:03:51,410
and what they're called.
93

93

00:03:51,410  -->  00:03:54,300
And if you like to get some additional reading,
94

94

00:03:54,300  -->  00:03:58,600
then check out this paper by Xavier Glorot,
95

95

00:04:01,850  -->  00:04:05,660
called deep sparse rectifier neural networks, 2011 paper.
96

96

00:04:05,660  -->  00:04:07,930
And there you will find out
97

97

00:04:07,930  -->  00:04:10,910
exactly why the rectifier function
98

98

00:04:10,910  -->  00:04:14,170
is such a valuable function.
99

99

00:04:14,170  -->  00:04:16,270
Why it's so popularly used.
100

100

00:04:16,270  -->  00:04:19,250
But nevertheless, for now, you don't really
101

101

00:04:19,250  -->  00:04:20,570
need to know all of those things.
102

102

00:04:20,570  -->  00:04:22,410
For now, we're just going to start applying them.
103

103

00:04:22,410  -->  00:04:24,070
We're going to start using them more, more and more.
104

104

00:04:24,070  -->  00:04:27,210
And so, when you feel comfortable with the practical
105

105

00:04:27,210  -->  00:04:30,570
side of things, then you can go
106

106

00:04:30,570  -->  00:04:32,013
and refer to this paper, and then you'll
107

107

00:04:32,013  -->  00:04:35,240
be able to soak in that knowledge much quicker
108

108

00:04:35,240  -->  00:04:37,120
and it'll make much more sense.
109

109

00:04:37,120  -->  00:04:38,240
So, but just keep this in mind
110

110

00:04:38,240  -->  00:04:40,550
that when you're ready, when you feel that you're ready
111

111

00:04:40,550  -->  00:04:42,720
then you can go and refer to this paper
112

112

00:04:42,720  -->  00:04:45,400
and get some valuable knowledge from there.
113

113

00:04:45,400  -->  00:04:48,250
So, just to quickly recap.
114

114

00:04:48,250  -->  00:04:50,670
We have the threshold activation function
115

115

00:04:50,670  -->  00:04:52,106
which looks like this.
116

116

00:04:52,106  -->  00:04:54,060
The sigmoid activation function
117

117

00:04:54,060  -->  00:04:55,650
which looks like this.
118

118

00:04:55,650  -->  00:04:57,010
We have the rectifier function,
119

119

00:04:57,010  -->  00:05:00,410
and we have the hyperbolic tangent function.
120

120

00:05:00,410  -->  00:05:02,210
And now to finish off this tutorial
121

121

00:05:02,210  -->  00:05:04,940
let's quickly do a few exercises.
122

122

00:05:04,940  -->  00:05:06,860
So, we'll just do two quick exercises
123

123

00:05:06,860  -->  00:05:09,060
to help that knowledge sink in.
124

124

00:05:09,060  -->  00:05:11,530
So, first one is we've got an example here
125

125

00:05:11,530  -->  00:05:14,490
of a neural network with just one neuron
126

126

00:05:14,490  -->  00:05:16,050
and then right away, the output layer.
127

127

00:05:16,050  -->  00:05:18,270
And the question is, assuming that
128

128

00:05:18,270  -->  00:05:19,920
your dependent variable is binary,
129

129

00:05:19,920  -->  00:05:21,110
so it's either zero or one,
130

130

00:05:21,110  -->  00:05:23,650
which threshold function would you use?
131

131

00:05:23,650  -->  00:05:25,970
So, out of the one's that we discussed.
132

132

00:05:25,970  -->  00:05:30,970
We have the threshold function, the sigmoid function,
133

133

00:05:31,180  -->  00:05:33,370
the rectifier function, and we've got
134

134

00:05:33,370  -->  00:05:34,970
the hyperbolic tangent function.
135

135

00:05:36,290  -->  00:05:41,290
In their raw forms, which ones would you be able to use
136

136

00:05:41,760  -->  00:05:43,023
for a binary variable?
137

137

00:05:43,860  -->  00:05:46,110
Okay, so, the answers here are,
138

138

00:05:46,110  -->  00:05:49,250
there's two options that we can approach this with.
139

139

00:05:49,250  -->  00:05:52,290
So, number one is the threshold activation function.
140

140

00:05:52,290  -->  00:05:54,670
Because we know that it's between zero and one
141

141

00:05:54,670  -->  00:05:57,520
and it gives you a zero under certain values
142

142

00:05:57,520  -->  00:05:58,650
and then otherwise it gives you ones,
143

143

00:05:58,650  -->  00:06:00,000
so it only can give you two values.
144

144

00:06:00,000  -->  00:06:04,300
It fits perfectly, fits this requirement perfectly
145

145

00:06:04,300  -->  00:06:06,890
and therefore, you can just say y equals
146

146

00:06:07,980  -->  00:06:12,950
the threshold function of your weighted sum,
147

147

00:06:12,950  -->  00:06:13,870
and that's it.
148

148

00:06:13,870  -->  00:06:16,320
And in the second case, which you could use
149

149

00:06:16,320  -->  00:06:18,340
is the sigmoid activation function.
150

150

00:06:18,340  -->  00:06:20,450
It is actually also between zero and one.
151

151

00:06:20,450  -->  00:06:21,680
Just what we need.
152

152

00:06:21,680  -->  00:06:25,550
But at the same time, you want just zero or one right.
153

153

00:06:25,550  -->  00:06:28,900
So, it's not exactly what we need
154

154

00:06:28,900  -->  00:06:31,230
but in this case what you could use it as
155

155

00:06:31,230  -->  00:06:36,230
is the probability of y being yes or no.
156

156

00:06:37,430  -->  00:06:40,090
So, we want y to be zero or one.
157

157

00:06:40,090  -->  00:06:45,090
But instead we'll say that the sigmoid activation function
158

158

00:06:45,730  -->  00:06:50,730
tells us the probability of y being equal to one.
159

159

00:06:51,770  -->  00:06:55,090
So, basically, the closer you get to the top,
160

160

00:06:55,090  -->  00:06:57,890
the more likely it is that this is indeed
161

161

00:06:57,890  -->  00:07:00,640
a one or a yes rather than a no.
162

162

00:07:00,640  -->  00:07:02,590
And so, that's very similar
163

163

00:07:02,590  -->  00:07:04,810
to the logistic regression approach.
164

164

00:07:04,810  -->  00:07:07,233
And those are just two examples
165

165

00:07:07,233  -->  00:07:09,910
if you have a binary variable.
166

166

00:07:09,910  -->  00:07:12,740
And now, let's have a look at another practical application.
167

167

00:07:12,740  -->  00:07:14,823
Let's have a look at how all this will play out
168

168

00:07:14,823  -->  00:07:17,320
if we had a neural network like this.
169

169

00:07:17,320  -->  00:07:21,010
So, in the first input layer we have some inputs.
170

170

00:07:21,010  -->  00:07:23,710
They are sent off to our first hidden layer,
171

171

00:07:23,710  -->  00:07:26,010
and then an activation function is applied.
172

172

00:07:26,010  -->  00:07:27,840
And usually what you would apply here,
173

173

00:07:27,840  -->  00:07:29,610
and what you will see throughout this course is
174

174

00:07:29,610  -->  00:07:32,770
we're gonna apply a rectifier activation function.
175

175

00:07:32,770  -->  00:07:34,460
So, it would look something like that.
176

176

00:07:34,460  -->  00:07:36,500
We apply the rectifier activation function
177

177

00:07:36,500  -->  00:07:38,810
and then from there, the signals
178

178

00:07:38,810  -->  00:07:41,740
would be passed on to the output layer
179

179

00:07:41,740  -->  00:07:43,880
where the sigmoid activation function
180

180

00:07:43,880  -->  00:07:46,740
would be applied and that would be our final output,
181

181

00:07:46,740  -->  00:07:48,980
and that could predict a probability for instance.
182

182

00:07:48,980  -->  00:07:50,890
So, this combination is gonna be quite common
183

183

00:07:50,890  -->  00:07:53,874
where in the hidden layers we apply the rectifier function
184

184

00:07:53,874  -->  00:07:58,790
and then in the output layer we apply the sigmoid function.
185

185

00:07:58,790  -->  00:08:01,330
So, there we go, hope you enjoyed today's tutorial.
186

186

00:08:01,330  -->  00:08:03,100
Now, you are quite well versed
187

187

00:08:03,100  -->  00:08:04,940
in the four different types of activation functions
188

188

00:08:04,940  -->  00:08:08,030
and you will get some hands on practical experience
189

189

00:08:08,030  -->  00:08:09,330
with them throughout this course.
190

190

00:08:09,330  -->  00:08:12,160
We'll be using them all over place.
191

191

00:08:12,160  -->  00:08:13,880
So, you'll get to know them quite intimately
192

192

00:08:13,880  -->  00:08:16,033
and you should be quite comfortable with them.
193

193

00:08:16,033  -->  00:08:18,420
But for now, this is the knowledge
194

194

00:08:18,420  -->  00:08:20,531
that you need to progress and understand
195

195

00:08:20,531  -->  00:08:23,830
what is going to be happening further down in this course.
196

196

00:08:23,830  -->  00:08:26,810
And on that note, I look forward to seeing you next time.
197

197

00:08:26,810  -->  00:08:28,643
Until then, enjoy deep learning.
