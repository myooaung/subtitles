1
1

00:00:00,340  -->  00:00:02,480
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,480  -->  00:00:05,270
And congratulations for making it to Volume Two
3

3

00:00:05,270  -->  00:00:07,070
unsupervised deep learning.
4

4

00:00:07,070  -->  00:00:10,550
So today we will implement our very first unsupervised deep
5

5

00:00:10,550  -->  00:00:13,420
learning model, which is the self organizing map.
6

6

00:00:13,420  -->  00:00:16,330
And we will implement this model to solve a new kind of
7

7

00:00:16,330  -->  00:00:19,570
business problem, which is fraud detection.
8

8

00:00:19,570  -->  00:00:21,560
So this business problem goes this way.
9

9

00:00:21,560  -->  00:00:23,570
Let's imagine we are deep learning scientists
10

10

00:00:23,570  -->  00:00:26,020
working for a bank and we are given a data set
11

11

00:00:26,020  -->  00:00:29,180
that contains information of customers from this bank
12

12

00:00:29,180  -->  00:00:31,550
applying for an advanced credit card.
13

13

00:00:31,550  -->  00:00:34,580
So basically, these informations are the data that customers
14

14

00:00:34,580  -->  00:00:37,170
had to provide when filling the application form.
15

15

00:00:37,170  -->  00:00:40,510
And our mission, is to detect potential fraud
16

16

00:00:40,510  -->  00:00:42,370
within these applications.
17

17

00:00:42,370  -->  00:00:44,490
So that means that by the end of the mission,
18

18

00:00:44,490  -->  00:00:47,350
we have to give the explicit list,
19

19

00:00:47,350  -->  00:00:49,890
of the customers who potentially cheated.
20

20

00:00:49,890  -->  00:00:53,610
So our goal is very explicit, we have to return something
21

21

00:00:53,610  -->  00:00:56,220
however, to return this something, that is the list of
22

22

00:00:56,220  -->  00:00:58,290
potential fraudulent customers,
23

23

00:00:58,290  -->  00:01:01,230
we will not make a supervised deep learning model
24

24

00:01:01,230  -->  00:01:04,430
and try to predict if each customer potentially cheated,
25

25

00:01:04,430  -->  00:01:06,002
yes or no, you know, with a dependent variable
26

26

00:01:06,002  -->  00:01:08,280
that has binary values.
27

27

00:01:08,280  -->  00:01:09,670
No, this is not what we're gonna do.
28

28

00:01:09,670  -->  00:01:12,690
What we're gonna do is unsupervised deep learning,
29

29

00:01:12,690  -->  00:01:15,610
which means that we will identify some patterns
30

30

00:01:15,610  -->  00:01:17,350
in a high dimensional data sets
31

31

00:01:17,350  -->  00:01:19,150
full of nonlinear relationships.
32

32

00:01:19,150  -->  00:01:22,340
And one of these patterns will be the potential fraud.
33

33

00:01:22,340  -->  00:01:24,820
That is the customers who potentially cheated.
34

34

00:01:24,820  -->  00:01:27,580
So to understand better now what we're gonna do is to import
35

35

00:01:27,580  -->  00:01:30,180
the data set, so that you understand clearly,
36

36

00:01:30,180  -->  00:01:31,900
what those patterns are.
37

37

00:01:31,900  -->  00:01:33,930
So let's go to File Explorer.
38

38

00:01:33,930  -->  00:01:36,670
And let's go to the right working directory folder.
39

39

00:01:36,670  -->  00:01:39,330
So deep learning A to Z Volume Two
40

40

00:01:39,330  -->  00:01:40,910
unsupervised deep learning
41

41

00:01:40,910  -->  00:01:44,610
and now we are in the first part of Volume Two part four
42

42

00:01:44,610  -->  00:01:46,210
self organizing maps.
43

43

00:01:46,210  -->  00:01:48,804
By the way self organizing maps is the simplest
44

44

00:01:48,804  -->  00:01:50,860
unsupervised deep learning model we'll
45

45

00:01:50,860  -->  00:01:52,870
implement in this Volume Two.
46

46

00:01:52,870  -->  00:01:56,040
So that's why I put it first and then after will have these
47

47

00:01:56,040  -->  00:01:58,200
very powerful deep learning models
48

48

00:01:58,200  -->  00:02:01,220
that we will use to implement a recommended system.
49

49

00:02:01,220  -->  00:02:03,350
But for now let's make a fraud detector.
50

50

00:02:03,350  -->  00:02:07,830
And so we'll go to Part Four and section 16 building an SOM.
51

51

00:02:07,830  -->  00:02:11,370
Alright, so the data set that contains these applications
52

52

00:02:11,370  -->  00:02:13,590
for the advanced credit card is this one,
53

53

00:02:13,590  -->  00:02:16,230
credit card applications that CSV,
54

54

00:02:16,230  -->  00:02:18,110
so we're gonna start this mission by
55

55

00:02:18,110  -->  00:02:19,630
importing this data set.
56

56

00:02:19,630  -->  00:02:21,786
But before let's set the right folder as working directory
57

57

00:02:21,786  -->  00:02:23,400
that is this folder.
58

58

00:02:23,400  -->  00:02:25,440
And so we click on this tool button here,
59

59

00:02:25,440  -->  00:02:27,900
and set console working directory.
60

60

00:02:27,900  -->  00:02:29,220
Now everything's fine,
61

61

00:02:29,220  -->  00:02:32,330
I will explain afterwards what are these two Python files.
62

62

00:02:32,330  -->  00:02:34,190
All right, so two important data sets,
63

63

00:02:34,190  -->  00:02:37,703
first let's import the libraries, the essential libraries to
64

64

00:02:37,703  -->  00:02:41,650
implement model and now let's import the credit card
65

65

00:02:41,650  -->  00:02:43,160
application status set.
66

66

00:02:43,160  -->  00:02:48,160
So as usual, we'll call our data set data set equals then
67

67

00:02:48,640  -->  00:02:53,223
pd.read_csv and in parentheses and in quotes.
68

68

00:02:55,870  -->  00:02:58,681
we input the name of the data set which is
69

69

00:02:58,681  -->  00:03:01,181
credits_card_applications.csv.
70

70

00:03:05,770  -->  00:03:08,790
Alright, perfect, so now let's select this line
71

71

00:03:08,790  -->  00:03:10,470
and import it.
72

72

00:03:10,470  -->  00:03:13,180
Well import it, let's go to variable explorer
73

73

00:03:13,180  -->  00:03:14,720
and let's open it,
74

74

00:03:14,720  -->  00:03:17,279
So first this data set is taken from the
75

75

00:03:17,279  -->  00:03:20,530
UCI Machine Learning Repository,
76

76

00:03:20,530  -->  00:03:22,690
it is called the Statlog Australian Credit
77

77

00:03:22,690  -->  00:03:27,220
Approval Data Set, and it is available at this address here.
78

78

00:03:27,220  -->  00:03:28,970
So first, let's see the basic information
79

79

00:03:28,970  -->  00:03:30,210
about this data set,
80

80

00:03:30,210  -->  00:03:33,130
so this file concerns credit card applications,
81

81

00:03:33,130  -->  00:03:35,790
all attribute names and values has been changed to
82

82

00:03:35,790  -->  00:03:39,630
meaningless symbols to protect confidentiality of the data.
83

83

00:03:39,630  -->  00:03:42,530
Okay, so that makes this problem even more complex
84

84

00:03:42,530  -->  00:03:44,580
and difficult to solve for human,
85

85

00:03:44,580  -->  00:03:46,210
indeed when we see the data set,
86

86

00:03:46,210  -->  00:03:49,340
we feel totally incapable of detecting any fraud.
87

87

00:03:49,340  -->  00:03:51,760
So we clearly need a deep learning model
88

88

00:03:51,760  -->  00:03:53,580
to find the cheaters,
89

89

00:03:53,580  -->  00:03:56,160
and then we have some attributes informations here.
90

90

00:03:56,160  -->  00:03:58,400
Basically, the independent variables
91

91

00:03:58,400  -->  00:04:02,130
are some categorical and continuous independent variables.
92

92

00:04:02,130  -->  00:04:06,860
And inside all these variables are hidden SOM frauds
93

93

00:04:06,860  -->  00:04:08,490
that we have to detect.
94

94

00:04:08,490  -->  00:04:12,370
Okay, so let's go back to Python and let's make the link
95

95

00:04:12,370  -->  00:04:15,570
between this data set and what I've just explained to you.
96

96

00:04:15,570  -->  00:04:18,220
So the first thing important to understand here is that
97

97

00:04:18,220  -->  00:04:19,910
the columns are the attributes,
98

98

00:04:19,910  -->  00:04:22,760
that is the informations of the customers.
99

99

00:04:22,760  -->  00:04:24,830
And the lines are the customers.
100

100

00:04:24,830  -->  00:04:27,200
For example, this line seven here corresponds
101

101

00:04:27,200  -->  00:04:28,270
to one customer.
102

102

00:04:28,270  -->  00:04:31,140
And when I said earlier that the unsupervised deep learning
103

103

00:04:31,140  -->  00:04:33,860
model is going to identify some patterns.
104

104

00:04:33,860  -->  00:04:36,210
Well, by patterns, I mean customers,
105

105

00:04:36,210  -->  00:04:39,750
so it's going to do some kind of customer segmentation
106

106

00:04:39,750  -->  00:04:42,150
to identify segments of customers
107

107

00:04:42,150  -->  00:04:44,870
and one of the segments will contain the customers that
108

108

00:04:44,870  -->  00:04:46,230
potentially cheated.
109

109

00:04:46,230  -->  00:04:50,437
So after having seen intuition lectures made Kirill,
110

110

00:04:50,437  -->  00:04:54,170
can you try to guess what this segment is going to
111

111

00:04:54,170  -->  00:04:56,570
be on the self organizing map.
112

112

00:04:56,570  -->  00:04:58,890
It will actually be very explicit,
113

113

00:04:58,890  -->  00:05:01,580
it corresponds to one specific range of values
114

114

00:05:01,580  -->  00:05:03,580
in the self organizing map.
115

115

00:05:03,580  -->  00:05:04,740
So try to think about it,
116

116

00:05:04,740  -->  00:05:06,910
I'm going to provide a solution very soon.
117

117

00:05:06,910  -->  00:05:09,550
Now let's go deeper into the details of what
118

118

00:05:09,550  -->  00:05:11,440
this first unsupervised deep learning model
119

119

00:05:11,440  -->  00:05:14,250
is going to do, with all these customers.
120

120

00:05:14,250  -->  00:05:18,210
So first of all, all these customers are the input,
121

121

00:05:18,210  -->  00:05:21,440
these customers are the inputs of our neural network.
122

122

00:05:21,440  -->  00:05:24,900
And then what happens is that these input points
123

123

00:05:24,900  -->  00:05:27,980
are going to be mapped to a new output space.
124

124

00:05:27,980  -->  00:05:30,610
And between the input space and the output space,
125

125

00:05:30,610  -->  00:05:34,110
we have this neural network composed of neurons,
126

126

00:05:34,110  -->  00:05:37,280
each neuron being initialized as a vector of weights
127

127

00:05:37,280  -->  00:05:40,130
that is the same size as the vector of customer
128

128

00:05:40,130  -->  00:05:42,560
that is a vector of 15 elements,
129

129

00:05:42,560  -->  00:05:46,040
because we have the customer ID plus 14 attributes.
130

130

00:05:46,040  -->  00:05:49,470
And so for each observation point that is for each customer,
131

131

00:05:49,470  -->  00:05:52,257
the output of this customer, will be the neuron
132

132

00:05:52,257  -->  00:05:54,700
that is the closest to the customer.
133

133

00:05:54,700  -->  00:05:56,740
So basically, in the network, we pick the neuron
134

134

00:05:56,740  -->  00:05:58,400
that is the closest to the customer.
135

135

00:05:58,400  -->  00:06:01,690
And remember, this neuron is called the winning node,
136

136

00:06:01,690  -->  00:06:04,610
for each customer, the winning node is the most similar
137

137

00:06:04,610  -->  00:06:06,590
neuron to the customer, then, you know,
138

138

00:06:06,590  -->  00:06:08,530
we use a neighborhood function like
139

139

00:06:08,530  -->  00:06:10,340
the galch neighborhood function,
140

140

00:06:10,340  -->  00:06:13,900
to update the weight of the neighbors of the winning node
141

141

00:06:13,900  -->  00:06:15,920
to move them closer to the point.
142

142

00:06:15,920  -->  00:06:18,870
And we do this for all the customers in the input space.
143

143

00:06:18,870  -->  00:06:20,380
And we'll repeat that again.
144

144

00:06:20,380  -->  00:06:22,090
We'll repeat all this many times.
145

145

00:06:22,090  -->  00:06:25,520
And each time we'll repeat it, the output space decreases
146

146

00:06:25,520  -->  00:06:27,060
and loses dimensions.
147

147

00:06:27,060  -->  00:06:29,610
It reduces its dimension little by little.
148

148

00:06:29,610  -->  00:06:32,190
And then it reaches a point where the neighborhood
149

149

00:06:32,190  -->  00:06:35,870
stops decreasing, where the output space stops decreasing.
150

150

00:06:35,870  -->  00:06:37,920
And that's the moment where we obtained our
151

151

00:06:37,920  -->  00:06:40,690
self organizing map in two dimensions
152

152

00:06:40,690  -->  00:06:44,220
with all the winning nodes that were eventually identified.
153

153

00:06:44,220  -->  00:06:46,850
And so now we're getting closer to the frauds.
154

154

00:06:46,850  -->  00:06:49,200
Because indeed, when we think about frauds,
155

155

00:06:49,200  -->  00:06:52,760
we think about outliers because the fraud basicly is defined
156

156

00:06:52,760  -->  00:06:56,040
by something that is far from the general rules.
157

157

00:06:56,040  -->  00:06:57,950
The rules that must be respected when
158

158

00:06:57,950  -->  00:06:59,690
applying to the credit card.
159

159

00:06:59,690  -->  00:07:03,070
So the frauds are actually the outlying neurons
160

160

00:07:03,070  -->  00:07:05,700
in this two dimensional self organizing map,
161

161

00:07:05,700  -->  00:07:08,693
simply because the outline neurons are far from the majority
162

162

00:07:08,693  -->  00:07:11,440
of neurons that follow the rules.
163

163

00:07:11,440  -->  00:07:13,060
And so now the question is,
164

164

00:07:13,060  -->  00:07:15,450
how can we detect the outline neurons
165

165

00:07:15,450  -->  00:07:17,230
in the self organizing map?
166

166

00:07:17,230  -->  00:07:19,780
Well, for this, we need the MID,
167

167

00:07:19,780  -->  00:07:22,440
the Mean Interneuron Distance.
168

168

00:07:22,440  -->  00:07:25,480
That means that in our self organizing map for each neuron,
169

169

00:07:25,480  -->  00:07:29,000
we're going to compute the mean of the Euclidean distance
170

170

00:07:29,000  -->  00:07:32,770
between this neuron and the neurons in its neighborhood.
171

171

00:07:32,770  -->  00:07:34,410
And so what neighborhood is that?
172

172

00:07:34,410  -->  00:07:36,460
Well, we have to define it manually.
173

173

00:07:36,460  -->  00:07:39,350
But we define a neighborhood for each neuron.
174

174

00:07:39,350  -->  00:07:41,800
And we compute the mean of the Euclidean distance
175

175

00:07:41,800  -->  00:07:43,590
between this neuron that we picked
176

176

00:07:43,590  -->  00:07:46,110
and all the neurons in the neighborhood that we defined.
177

177

00:07:46,110  -->  00:07:48,690
And by doing that we can detect outliers,
178

178

00:07:48,690  -->  00:07:51,850
because outliers will be far from all the neurons
179

179

00:07:51,850  -->  00:07:53,100
in its neighborhood.
180

180

00:07:53,100  -->  00:07:56,500
And so this is this exact information that we'll get
181

181

00:07:56,500  -->  00:07:59,100
on the self organizing map at the end of this part
182

182

00:07:59,100  -->  00:08:01,930
that will allow us to detect outliers,
183

183

00:08:01,930  -->  00:08:04,090
and therefore, to detect fraud.
184

184

00:08:04,090  -->  00:08:06,560
And then we'll use an inverse mapping function
185

185

00:08:06,560  -->  00:08:11,200
to identify which customers originally in the input space
186

186

00:08:11,200  -->  00:08:14,820
are associated to this winning node, that is an outlier.
187

187

00:08:14,820  -->  00:08:16,550
Alright, so the mystery is solved.
188

188

00:08:16,550  -->  00:08:18,760
Now let's implement all this.
189

189

00:08:18,760  -->  00:08:21,120
So I'm gonna close the data set here.
190

190

00:08:21,120  -->  00:08:22,930
And we just need to do something now
191

191

00:08:22,930  -->  00:08:27,030
is to split the data sets into two subsets,
192

192

00:08:27,030  -->  00:08:30,260
the sets that contain all the variables from customer ID
193

193

00:08:30,260  -->  00:08:32,210
to attribute number 14,
194

194

00:08:32,210  -->  00:08:35,910
and the class that is the variable that tells if yes or no
195

195

00:08:35,910  -->  00:08:38,060
the application of the customer was approved.
196

196

00:08:38,060  -->  00:08:41,380
So zero is no, the application was not approved.
197

197

00:08:41,380  -->  00:08:44,190
And one means yes, the application was approved.
198

198

00:08:44,190  -->  00:08:46,820
And so we want to separate all these variables here
199

199

00:08:46,820  -->  00:08:48,350
and the variable class here,
200

200

00:08:48,350  -->  00:08:50,240
so that's on the self organizing map,
201

201

00:08:50,240  -->  00:08:53,300
we can clearly distinguish the customers who weren't
202

202

00:08:53,300  -->  00:08:54,940
approved their application
203

203

00:08:54,940  -->  00:08:56,760
and the customers who got approval,
204

204

00:08:56,760  -->  00:08:59,140
because then that will be useful, for example,
205

205

00:08:59,140  -->  00:09:02,590
if we want to detect in priority, the fraudulent customers
206

206

00:09:02,590  -->  00:09:04,670
who got their applications approved,
207

207

00:09:04,670  -->  00:09:06,130
that would make more sense.
208

208

00:09:06,130  -->  00:09:08,520
And therefore, now we're gonna do is
209

209

00:09:08,520  -->  00:09:11,020
create those two subsets.
210

210

00:09:11,020  -->  00:09:13,560
So we're gonna call the first one X.
211

211

00:09:13,560  -->  00:09:18,560
So x equals dataset.iloc to get the indexes
212

212

00:09:19,390  -->  00:09:22,000
of the observations we want to include an X,
213

213

00:09:22,000  -->  00:09:24,270
and so we start with the indexes of the lines.
214

214

00:09:24,270  -->  00:09:25,700
And since we want all the lines,
215

215

00:09:25,700  -->  00:09:27,600
because we want all the customers
216

216

00:09:27,600  -->  00:09:30,680
we're gonna use the column here, then come up
217

217

00:09:30,680  -->  00:09:33,250
and then remember we want all the columns
218

218

00:09:33,250  -->  00:09:37,360
except the last one, so we use here Colon minus one.
219

219

00:09:37,360  -->  00:09:41,160
And then as usual, that values which will return
220

220

00:09:41,160  -->  00:09:45,030
all the observations indexed by these indexes here.
221

221

00:09:45,030  -->  00:09:48,020
Alright, perfect now, let's create the last column.
222

222

00:09:48,020  -->  00:09:51,470
Let's call it Y, here we can take that again
223

223

00:09:51,470  -->  00:09:54,850
and we'll just change the index, right?
224

224

00:09:54,850  -->  00:09:59,110
And so here remember the trick to take the last column well,
225

225

00:09:59,110  -->  00:10:01,030
we just need to take minus one.
226

226

00:10:01,030  -->  00:10:04,130
Alright, so now let's select these two lines
227

227

00:10:04,130  -->  00:10:08,220
and execute, and now x and y are created.
228

228

00:10:08,220  -->  00:10:11,900
We can check x contains all the variables
229

229

00:10:11,900  -->  00:10:16,530
except the last one, and y contains the last variable
230

230

00:10:16,530  -->  00:10:19,700
that tells if yes or no, the application was approved.
231

231

00:10:19,700  -->  00:10:22,040
So now there is an important point to make,
232

232

00:10:22,040  -->  00:10:25,070
we splited our data sets into x and y.
233

233

00:10:25,070  -->  00:10:27,200
But be careful we did not do that because we're doing
234

234

00:10:27,200  -->  00:10:30,290
some supervised learning, we're not trying to make a model
235

235

00:10:30,290  -->  00:10:33,170
that will predict zero or one in the end.
236

236

00:10:33,170  -->  00:10:35,780
We're just doing this to make the distinction in the end
237

237

00:10:35,780  -->  00:10:37,500
between the customers who were approved
238

238

00:10:37,500  -->  00:10:39,680
and the customers who were not approved.
239

239

00:10:39,680  -->  00:10:43,000
You will see that when we train our self organizing map
240

240

00:10:43,000  -->  00:10:46,030
we will only use x because we are doing some
241

241

00:10:46,030  -->  00:10:47,610
unsupervised deep learning,
242

242

00:10:47,610  -->  00:10:51,360
that means that no dependent variable is considered.
243

243

00:10:51,360  -->  00:10:53,350
Alright, so it's important to make that clear
244

244

00:10:53,350  -->  00:10:57,260
and now just before we train our self organizing map,
245

245

00:10:57,260  -->  00:10:59,000
we just need to do one more thing,
246

246

00:10:59,000  -->  00:11:02,860
its feature scaling because, you know, most of the time
247

247

00:11:02,860  -->  00:11:05,980
feature scaling is compulsory for deep learning
248

248

00:11:05,980  -->  00:11:08,760
for that simple reason that they're high computations
249

249

00:11:08,760  -->  00:11:10,640
to make because we're starting with
250

250

00:11:10,640  -->  00:11:12,330
a high dimensional data set,
251

251

00:11:12,330  -->  00:11:14,550
with lots of nonlinear relationships.
252

252

00:11:14,550  -->  00:11:17,340
And so it will be much easier for our deep learning model
253

253

00:11:17,340  -->  00:11:20,230
to be trained if the features are scaled.
254

254

00:11:20,230  -->  00:11:22,180
So let's do that quickly.
255

255

00:11:22,180  -->  00:11:24,500
We're actually going to do the same as
256

256

00:11:24,500  -->  00:11:27,630
for regular neural networks, we will use normalization
257

257

00:11:27,630  -->  00:11:29,310
that means that we'll get all our features
258

258

00:11:29,310  -->  00:11:30,980
between zero and one.
259

259

00:11:30,980  -->  00:11:33,616
So let's do this, Remember, we need to import
260

260

00:11:33,616  -->  00:11:38,616
the MinMaxScaler class from a sklearn.preprocessing,
261

261

00:11:40,160  -->  00:11:43,587
here it is, import MinMaxScaler.
262

262

00:11:47,130  -->  00:11:49,730
All right, then we create an object of this class
263

263

00:11:49,730  -->  00:11:53,227
that we call SC equals MinMmaxScaler
264

264

00:11:54,694  -->  00:11:56,320
and inside this class well,
265

265

00:11:56,320  -->  00:11:59,540
let's inspect it here, it is MinMaxScaler
266

266

00:11:59,540  -->  00:12:02,740
the parameters our first feature range
267

267

00:12:02,740  -->  00:12:04,303
so feature range here,
268

268

00:12:05,770  -->  00:12:09,510
equals then in parentheses we specify that we want the
269

269

00:12:09,510  -->  00:12:13,210
range of our scales features between zero and one
270

270

00:12:13,210  -->  00:12:15,120
that's normalization
271

271

00:12:15,120  -->  00:12:18,400
and that's all, we don't need this copy parameter,
272

272

00:12:18,400  -->  00:12:21,660
we got everything we need with this feature range parameter.
273

273

00:12:21,660  -->  00:12:26,660
So now as we gotta do is first fit this SC object to x,
274

274

00:12:28,290  -->  00:12:32,340
so that SC gets all the informations of x like the minimum,
275

275

00:12:32,340  -->  00:12:35,470
the maximum, well all the informations that it needs
276

276

00:12:35,470  -->  00:12:38,040
to apply normalization to x.
277

277

00:12:38,040  -->  00:12:42,030
So we finished first sc.fit
278

278

00:12:42,030  -->  00:12:44,960
and remember after this object is fitted to x.
279

279

00:12:44,960  -->  00:12:47,410
Well, we need of course, to transform x
280

280

00:12:47,410  -->  00:12:50,100
that is to apply normalization to x
281

281

00:12:50,100  -->  00:12:54,160
and therefore we use here to fit transform
282

282

00:12:54,160  -->  00:12:57,490
method that we apply to x.
283

283

00:12:57,490  -->  00:13:00,930
So you know this and therefore since the fit transform
284

284

00:13:00,930  -->  00:13:04,340
method returns, the normalized version of x.
285

285

00:13:04,340  -->  00:13:09,247
Well, we need to add here x equals and sc.fit_transform(x).
286

286

00:13:11,119  -->  00:13:13,740
Alright, so that's will do the normalization
287

287

00:13:13,740  -->  00:13:15,257
it's ready to execute.
288

288

00:13:15,257  -->  00:13:19,950
Select like this and execute, perfect.
289

289

00:13:19,950  -->  00:13:24,425
Now if we look at x well you can see that x is
290

290

00:13:24,425  -->  00:13:27,650
all normalized, can indeed check that all the values
291

291

00:13:27,650  -->  00:13:29,930
are between zero and one.
292

292

00:13:29,930  -->  00:13:33,360
So perfect, now our first unsupervised deep learning model
293

293

00:13:33,360  -->  00:13:35,930
will be able to learn in the best conditions.
294

294

00:13:35,930  -->  00:13:37,200
And speaking of learning,
295

295

00:13:37,200  -->  00:13:40,030
that's exactly what will happen in the next tutorial.
296

296

00:13:40,030  -->  00:13:41,753
Until then, enjoy deep learning.
