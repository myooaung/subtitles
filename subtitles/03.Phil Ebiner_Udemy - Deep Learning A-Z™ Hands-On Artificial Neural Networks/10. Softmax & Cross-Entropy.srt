1
1

00:00:00,250  -->  00:00:01,083
<v Teacher>Hello and welcome back</v>
2

2

00:00:01,083  -->  00:00:02,980
to the course on deep learning.
3

3

00:00:02,980  -->  00:00:05,620
This is an additional tutorial
4

4

00:00:05,620  -->  00:00:08,550
to talk about the softmax and cross-entropy functions.
5

5

00:00:08,550  -->  00:00:12,010
It is not 100% necessary in order for you
6

6

00:00:12,010  -->  00:00:15,230
to go through all of the parts that we've been through
7

7

00:00:15,230  -->  00:00:18,470
in the main part of this section
8

8

00:00:18,470  -->  00:00:21,220
where we're talking about the convolutional neural networks
9

9

00:00:21,220  -->  00:00:22,488
but at the same time I thought it would be
10

10

00:00:22,488  -->  00:00:26,510
a good addition to your bag of knowledge and skill set
11

11

00:00:26,510  -->  00:00:30,740
so let's go ahead and dig in to these functions.
12

12

00:00:30,740  -->  00:00:32,420
So to start of with,
13

13

00:00:32,420  -->  00:00:35,850
what we have here is the convolutional neural network
14

14

00:00:35,850  -->  00:00:38,660
that we built in the main part of the section
15

15

00:00:38,660  -->  00:00:42,830
and then at the end it pops out some probabilities
16

16

00:00:42,830  -->  00:00:47,830
for 0.95 for a dog and 0.05, or 5%, for a cat,
17

17

00:00:47,930  -->  00:00:50,810
given that photo on the left has an input.
18

18

00:00:50,810  -->  00:00:51,960
This is after the train has been conducted.
19

19

00:00:51,960  -->  00:00:54,290
This is actually, it's running,
20

20

00:00:54,290  -->  00:00:57,230
and it's classifying a certain image.
21

21

00:00:57,230  -->  00:00:58,520
And so the question here is,
22

22

00:00:58,520  -->  00:01:00,810
how come these two values add up to one.
23

23

00:01:00,810  -->  00:01:03,000
Because as far as we know from everything
24

24

00:01:03,000  -->  00:01:05,318
that we've learned about our neural networks,
25

25

00:01:05,318  -->  00:01:09,980
there is nothing to say that these two final neurons
26

26

00:01:09,980  -->  00:01:11,610
are connected between each other.
27

27

00:01:11,610  -->  00:01:16,610
So how would they know what the value of the other one is
28

28

00:01:16,832  -->  00:01:20,230
and how would they know to add their values up to one.
29

29

00:01:20,230  -->  00:01:22,200
Well, the answer is they wouldn't,
30

30

00:01:22,200  -->  00:01:25,890
in the classic version of artificial neural network,
31

31

00:01:25,890  -->  00:01:27,550
and the only way that they do
32

32

00:01:27,550  -->  00:01:29,900
is because we introduce a special function,
33

33

00:01:29,900  -->  00:01:31,800
called the softmax function,
34

34

00:01:31,800  -->  00:01:33,860
in order to help us out of this situation.
35

35

00:01:33,860  -->  00:01:35,400
So normally, what would happen,
36

36

00:01:35,400  -->  00:01:38,900
is the dog and the cat neurons would have
37

37

00:01:38,900  -->  00:01:41,430
any kind of real values.
38

38

00:01:41,430  -->  00:01:45,100
They don't have to add up to one.
39

39

00:01:45,100  -->  00:01:48,360
But then, we would apply the softmax function,
40

40

00:01:48,360  -->  00:01:50,810
which is written up over there at the top,
41

41

00:01:50,810  -->  00:01:53,060
and that would bring these values
42

42

00:01:53,060  -->  00:01:54,320
to be between zero and one
43

43

00:01:54,320  -->  00:01:56,280
and it would make them add up to one.
44

44

00:01:56,280  -->  00:02:00,440
And to quote Wikipedia, the softmax function,
45

45

00:02:00,440  -->  00:02:02,160
or the normalized exponential function,
46

46

00:02:02,160  -->  00:02:04,290
is a generalization of the logistic function
47

47

00:02:04,290  -->  00:02:08,727
that quote unquote "squashes a K-dimensional vector
48

48

00:02:08,727  -->  00:02:11,467
"of arbitrary real values to a K-dimensional vector
49

49

00:02:11,467  -->  00:02:13,337
"of real values in the range of zero
50

50

00:02:13,337  -->  00:02:15,250
"to one that add up to one."
51

51

00:02:15,250  -->  00:02:17,580
So basically, it does exactly what we want.
52

52

00:02:17,580  -->  00:02:20,130
It brings these values to be between zero and one
53

53

00:02:20,130  -->  00:02:22,830
and makes sure that they add up to one.
54

54

00:02:22,830  -->  00:02:25,070
And the way it works is that,
55

55

00:02:25,070  -->  00:02:26,113
the way that this is possible is that,
56

56

00:02:26,113  -->  00:02:27,670
because at the bottom over here
57

57

00:02:27,670  -->  00:02:29,870
you can see that there's a summation
58

58

00:02:29,870  -->  00:02:34,870
so it takes the exponent and puts it in the power of z
59

59

00:02:35,460  -->  00:02:38,730
and adds it up so z1, z2, across all of your classes,
60

60

00:02:38,730  -->  00:02:39,860
all of these values,
61

61

00:02:39,860  -->  00:02:43,463
and so that's your normalization happening right there.
62

62

00:02:44,300  -->  00:02:47,300
So that's how the softmax function works
63

63

00:02:47,300  -->  00:02:50,760
and it makes sense to introduce a softmax function
64

64

00:02:50,760  -->  00:02:53,010
into convolutional neural networks
65

65

00:02:53,010  -->  00:02:55,770
because how strange would it be
66

66

00:02:55,770  -->  00:02:59,587
if you had possible classes of a dog and a cat
67

67

00:02:59,587  -->  00:03:04,587
and for the dog class you had probability of 80%
68

68

00:03:05,070  -->  00:03:08,580
and for the cat class you had a probability of 45%, right?
69

69

00:03:08,580  -->  00:03:11,300
It just doesn't make sense like that
70

70

00:03:11,300  -->  00:03:13,010
and therefore it's much better
71

71

00:03:13,010  -->  00:03:14,670
when you introduce the softmax function
72

72

00:03:14,670  -->  00:03:17,130
and that's what you will find happening most of the time
73

73

00:03:17,130  -->  00:03:19,640
in convolutional neural networks.
74

74

00:03:19,640  -->  00:03:23,280
Now the other thing is that the softmax function
75

75

00:03:23,280  -->  00:03:25,340
comes hand-in-hand with something called
76

76

00:03:25,340  -->  00:03:27,430
the cross-entropy function
77

77

00:03:27,430  -->  00:03:28,960
and it's a very handy thing for us.
78

78

00:03:28,960  -->  00:03:30,540
So let's first look at the formula.
79

79

00:03:30,540  -->  00:03:33,000
This is what the cross-entropy function looks like.
80

80

00:03:33,000  -->  00:03:36,970
We're actually going to be using a different calculation.
81

81

00:03:36,970  -->  00:03:38,650
We're going to be using this representation
82

82

00:03:38,650  -->  00:03:40,560
of the cross-entropy but the results basically the same.
83

83

00:03:40,560  -->  00:03:42,210
This is just easier to calculate.
84

84

00:03:43,900  -->  00:03:46,820
I know this might sound very unrelated
85

85

00:03:46,820  -->  00:03:48,980
to anything right now, just formulas on your screen,
86

86

00:03:48,980  -->  00:03:52,060
but there'll be some additional recommended reading
87

87

00:03:52,060  -->  00:03:52,990
at the end of this section
88

88

00:03:52,990  -->  00:03:55,812
so don't worry if you're not picking up on the math,
89

89

00:03:55,812  -->  00:03:58,270
if we haven't explained the math right now,
90

90

00:03:58,270  -->  00:04:01,690
but the point here is that, what is a cross-entropy?
91

91

00:04:01,690  -->  00:04:03,540
Well, a cross-entropy function,
92

92

00:04:03,540  -->  00:04:06,680
remember how we previously in artificial neural networks,
93

93

00:04:06,680  -->  00:04:11,680
we had a function called the mean squared error function
94

94

00:04:12,460  -->  00:04:14,770
which we used as the cost function
95

95

00:04:14,770  -->  00:04:17,690
for assessing our network performance
96

96

00:04:17,690  -->  00:04:20,940
and our goal was to minimize the MSE
97

97

00:04:20,940  -->  00:04:23,830
in order to optimize our network performance.
98

98

00:04:23,830  -->  00:04:25,780
Well, that was our cost function there,
99

99

00:04:26,690  -->  00:04:30,280
and in convolutional neural networks,
100

100

00:04:30,280  -->  00:04:33,360
we can still use MSE but a better option
101

101

00:04:33,360  -->  00:04:35,410
in convolutional neural networks,
102

102

00:04:35,410  -->  00:04:37,550
after you apply the softmax function,
103

103

00:04:37,550  -->  00:04:39,720
turns out to be the cross-entropy function,
104

104

00:04:39,720  -->  00:04:42,730
and in convolutional neural networks,
105

105

00:04:42,730  -->  00:04:44,193
when you apply the cross-entropy function,
106

106

00:04:44,193  -->  00:04:46,530
it's not called the cost function anymore,
107

107

00:04:46,530  -->  00:04:49,350
it's called the loss function and they're very similar
108

108

00:04:49,350  -->  00:04:52,160
and they're just little terminological differences
109

109

00:04:52,160  -->  00:04:55,430
and a little bit different in what they mean
110

110

00:04:55,430  -->  00:04:58,330
but for our purposes it's pretty much the same thing
111

111

00:04:58,330  -->  00:05:02,300
and what happens is the loss function
112

112

00:05:02,300  -->  00:05:05,820
is again something that we want to minimize
113

113

00:05:05,820  -->  00:05:09,570
in order to maximize the performance of our network.
114

114

00:05:09,570  -->  00:05:12,300
So let's have a look at a quick example
115

115

00:05:12,300  -->  00:05:15,180
on how, of how this function can be applied.
116

116

00:05:15,180  -->  00:05:18,010
So let's say we've put an image of a dog
117

117

00:05:18,010  -->  00:05:19,620
into our network.
118

118

00:05:19,620  -->  00:05:23,120
The predicted value for dog is 0.9
119

119

00:05:23,120  -->  00:05:25,152
and this is during the training so we know
120

120

00:05:25,152  -->  00:05:27,220
the label that is a dog.
121

121

00:05:27,220  -->  00:05:29,350
So the predicted value is 0.9,
122

122

00:05:29,350  -->  00:05:32,260
the predicted value for cat is 0.1.
123

123

00:05:32,260  -->  00:05:34,550
Then here we have the label so we know it's a dog
124

124

00:05:34,550  -->  00:05:36,890
because this is training and one for dog,
125

125

00:05:36,890  -->  00:05:40,020
zero for cat and so in this case you need
126

126

00:05:40,020  -->  00:05:45,020
to plug these numbers into your formula
127

127

00:05:45,360  -->  00:05:47,700
for cross-entropy.
128

128

00:05:47,700  -->  00:05:51,630
So how you do it is the values on the left
129

129

00:05:51,630  -->  00:05:53,310
go into the variable q.
130

130

00:05:53,310  -->  00:05:55,137
The one that is under the logarithm
131

131

00:05:55,137  -->  00:05:58,490
on the right side and the values from the right
132

132

00:05:58,490  -->  00:06:00,150
would go into p and so it's important
133

133

00:06:00,150  -->  00:06:02,070
to remember which one goes where
134

134

00:06:02,070  -->  00:06:04,030
because if you get them wrong
135

135

00:06:04,030  -->  00:06:05,331
you don't want to be taking a logarithm
136

136

00:06:05,331  -->  00:06:09,520
from a zero value or a logarithm from a one
137

137

00:06:09,520  -->  00:06:12,700
so you just want to make sure you plug them into
138

138

00:06:12,700  -->  00:06:16,940
the correct places and then you basically add that up.
139

139

00:06:16,940  -->  00:06:19,370
So that's how the cross-entropy works
140

140

00:06:19,370  -->  00:06:20,350
and we'll look at a,
141

141

00:06:20,350  -->  00:06:21,420
actually right now we're just going
142

142

00:06:21,420  -->  00:06:24,470
to look at a specific step by step example
143

143

00:06:24,470  -->  00:06:26,640
of applying this function in real life.
144

144

00:06:26,640  -->  00:06:28,580
And it will kind of make more sense
145

145

00:06:28,580  -->  00:06:31,855
what cross-entropy is and it will be less,
146

146

00:06:31,855  -->  00:06:35,690
my goal in this tutorial is to make you more comfortable
147

147

00:06:35,690  -->  00:06:39,210
with cross-entropy because it can sound very convoluted
148

148

00:06:39,210  -->  00:06:41,933
and (laughs) no pun intended.
149

149

00:06:42,770  -->  00:06:45,568
It can, convolutional neural networks,
150

150

00:06:45,568  -->  00:06:48,770
it can sound very sound complex, right?
151

151

00:06:48,770  -->  00:06:50,760
Scary. But it's not.
152

152

00:06:50,760  -->  00:06:51,593
That's the point.
153

153

00:06:51,593  -->  00:06:53,990
So let's go ahead and apply it so we know it's not scary.
154

154

00:06:53,990  -->  00:06:58,990
So here's, and also this will explain why we're doing this,
155

155

00:06:59,240  -->  00:07:01,680
why we're looking into different cross-functions.
156

156

00:07:01,680  -->  00:07:03,610
So neural network one, neural network two.
157

157

00:07:03,610  -->  00:07:05,180
Let's say we have two neural networks.
158

158

00:07:05,180  -->  00:07:07,784
And then we pass an image of a dog
159

159

00:07:07,784  -->  00:07:12,060
and we know that this is a dog and not a cat.
160

160

00:07:12,060  -->  00:07:15,070
And then we have another image of a cat
161

161

00:07:15,070  -->  00:07:17,830
this time an animal and it's a cat and not a dog.
162

162

00:07:17,830  -->  00:07:20,378
And here we have a weird looking animal
163

163

00:07:20,378  -->  00:07:22,400
which is in fact a dog and not a cat
164

164

00:07:22,400  -->  00:07:24,220
if you look very closely.
165

165

00:07:24,220  -->  00:07:27,590
So we want to see what our neural networks will predict.
166

166

00:07:27,590  -->  00:07:29,280
The first case, neural network one,
167

167

00:07:29,280  -->  00:07:33,250
90% dog, 10% cat, correct.
168

168

00:07:33,250  -->  00:07:36,650
Neural network number two, 60% dog dog, 40% cat.
169

169

00:07:36,650  -->  00:07:37,570
Still correct.
170

170

00:07:37,570  -->  00:07:39,023
Worse, but correct.
171

171

00:07:40,160  -->  00:07:45,160
Second option. First neural network 10% dog, 90% cat.
172

172

00:07:46,000  -->  00:07:47,230
Correct.
173

173

00:07:47,230  -->  00:07:51,450
Neural network number two, 30% dog, 70% cat.
174

174

00:07:51,450  -->  00:07:53,440
Worse, but still correct.
175

175

00:07:53,440  -->  00:07:55,280
And then finally neural network one,
176

176

00:07:55,280  -->  00:07:58,390
in image three, neural network one,
177

177

00:07:58,390  -->  00:08:00,590
40% dog, 60% cat.
178

178

00:08:00,590  -->  00:08:01,800
Incorrect.
179

179

00:08:01,800  -->  00:08:06,300
Neural network two, 10% dog, 90% cat.
180

180

00:08:06,300  -->  00:08:08,170
Incorrect and worse.
181

181

00:08:08,170  -->  00:08:11,310
So the key here is that even though both networks
182

182

00:08:11,310  -->  00:08:12,960
got it wrong in the last one,
183

183

00:08:12,960  -->  00:08:16,970
throughout all three images neural network one
184

184

00:08:16,970  -->  00:08:18,790
was outperforming neural network two.
185

185

00:08:18,790  -->  00:08:23,210
So even in the last case, it was very,
186

186

00:08:23,210  -->  00:08:27,520
it gave dog like a 40% as opposed to neural network two
187

187

00:08:27,520  -->  00:08:29,070
only gave dog a 10% chance.
188

188

00:08:29,070  -->  00:08:33,110
So neural network one is outperforming across the board
189

189

00:08:33,110  -->  00:08:35,480
when compared to neural network two.
190

190

00:08:35,480  -->  00:08:39,180
And so now we're going to look at the functions,
191

191

00:08:39,180  -->  00:08:40,960
that can measure performance
192

192

00:08:40,960  -->  00:08:42,930
that we've kind of talked about already.
193

193

00:08:42,930  -->  00:08:44,760
So let's put these into a table.
194

194

00:08:44,760  -->  00:08:46,790
So there's neural network one.
195

195

00:08:46,790  -->  00:08:49,450
You have the row number, so that's the image number,
196

196

00:08:49,450  -->  00:08:52,350
and then for image one you have what it predicted,
197

197

00:08:52,350  -->  00:08:55,420
90% dog, 10% cat, so those are the hat variables,
198

198

00:08:55,420  -->  00:08:57,260
and then you have the actual values,
199

199

00:08:57,260  -->  00:09:00,450
so dog, correct, cat, incorrect.
200

200

00:09:00,450  -->  00:09:03,320
Same thing for image number two
201

201

00:09:03,320  -->  00:09:05,150
and same thing for image number three.
202

202

00:09:05,150  -->  00:09:07,610
And same for neural network number two
203

203

00:09:07,610  -->  00:09:10,960
so dog 60%, cat 40% in first image,
204

204

00:09:10,960  -->  00:09:12,040
that's what it predicted,
205

205

00:09:12,040  -->  00:09:15,090
correct answer was dog not cat, and so on.
206

206

00:09:15,090  -->  00:09:17,990
And so now let's see what errors we can actually get.
207

207

00:09:17,990  -->  00:09:20,250
So what errors we can calculate
208

208

00:09:20,250  -->  00:09:22,150
to estimate the performance
209

209

00:09:22,150  -->  00:09:24,471
and monitor the performance of our networks.
210

210

00:09:24,471  -->  00:09:28,530
So one type of error is called the classification error
211

211

00:09:28,530  -->  00:09:32,380
and that is basically just asking,
212

212

00:09:32,380  -->  00:09:33,930
you did get it right or not.
213

213

00:09:33,930  -->  00:09:35,410
Regardless of the probabilities,
214

214

00:09:35,410  -->  00:09:37,870
it's just did you get it right or did you not get it right.
215

215

00:09:37,870  -->  00:09:41,980
So in both cases, for both neural networks,
216

216

00:09:41,980  -->  00:09:44,540
each of them, they got one,
217

217

00:09:44,540  -->  00:09:46,250
or so this is how many they got wrong.
218

218

00:09:46,250  -->  00:09:48,370
So they got one out of three wrong.
219

219

00:09:48,370  -->  00:09:52,260
So 33% error rate for neural network one
220

220

00:09:52,260  -->  00:09:54,837
and 33% error rate for neural network two.
221

221

00:09:54,837  -->  00:09:56,960
And so basically from this standpoint,
222

222

00:09:56,960  -->  00:09:59,060
both neural networks perform at the same level,
223

223

00:09:59,060  -->  00:10:00,130
but we know that's not true.
224

224

00:10:00,130  -->  00:10:02,685
We know that neural network one is outperforming
225

225

00:10:02,685  -->  00:10:04,083
neural network two.
226

226

00:10:05,010  -->  00:10:08,140
That's why classification error is not a good measure,
227

227

00:10:08,140  -->  00:10:10,913
especially for the purposes of back propagation.
228

228

00:10:11,770  -->  00:10:13,780
Mean squared error. Different.
229

229

00:10:13,780  -->  00:10:16,840
And by the way, I did these calculations in Excel.
230

230

00:10:16,840  -->  00:10:18,350
I just didn't want to bore you with them.
231

231

00:10:18,350  -->  00:10:19,910
But you can totally just sit down
232

232

00:10:19,910  -->  00:10:21,920
and do them on a paper or in Excel.
233

233

00:10:21,920  -->  00:10:23,590
These are very straight forward calculations,
234

234

00:10:23,590  -->  00:10:27,681
just basically take the sum of squared errors
235

235

00:10:27,681  -->  00:10:32,428
and then just take the average across your observations,
236

236

00:10:32,428  -->  00:10:34,950
and that's pretty much it.
237

237

00:10:34,950  -->  00:10:38,645
So for neural network one, you get 25%.
238

238

00:10:38,645  -->  00:10:43,240
For neural network two, you get 71% error rate.
239

239

00:10:43,240  -->  00:10:45,850
So as you can see, this one is more accurate.
240

240

00:10:45,850  -->  00:10:47,580
It's telling us that neural network one
241

241

00:10:47,580  -->  00:10:50,180
has a much lower error rate than neural network two.
242

242

00:10:51,080  -->  00:10:53,780
And then cross-entropy, again, we've seen the formula.
243

243

00:10:53,780  -->  00:10:54,880
You can also calculate this.
244

244

00:10:54,880  -->  00:10:56,660
This is actually even easier to calculate
245

245

00:10:56,660  -->  00:10:57,990
than the mean squared error.
246

246

00:10:57,990  -->  00:11:02,450
Cross-entropy gives you 38% for neural network one
247

247

00:11:02,450  -->  00:11:05,400
and 1.06 for neural network two.
248

248

00:11:05,400  -->  00:11:08,230
So you can see the results are a bit different
249

249

00:11:08,230  -->  00:11:10,130
when you look at them like that,
250

250

00:11:10,130  -->  00:11:12,440
when you look at, you know,
251

251

00:11:12,440  -->  00:11:14,623
the mean squared error and cross-entropy.
252

252

00:11:16,130  -->  00:11:20,390
The question of why would you use cross-entropy
253

253

00:11:20,390  -->  00:11:25,390
over mean squared error isn't just about, the kind of like,
254

254

00:11:26,250  -->  00:11:27,334
the numbers that they sprout.
255

255

00:11:27,334  -->  00:11:30,090
These calculations were just to show you
256

256

00:11:30,090  -->  00:11:32,410
that this is all, it's all doable,
257

257

00:11:32,410  -->  00:11:33,580
you can just do it on a paper.
258

258

00:11:33,580  -->  00:11:37,810
It's not, these are not very intense mathematics.
259

259

00:11:37,810  -->  00:11:41,110
These are pretty simple straight-forward things.
260

260

00:11:41,110  -->  00:11:44,900
But the question of why would you use cross-entropy
261

261

00:11:44,900  -->  00:11:48,140
over mean squared error is a very good question to ask,
262

262

00:11:48,140  -->  00:11:50,003
I'm glad you asked it.
263

263

00:11:50,003  -->  00:11:55,003
The answer to that is there's several advantages
264

264

00:11:55,410  -->  00:12:00,170
of cross-entropy over mean squared error
265

265

00:12:00,170  -->  00:12:02,210
which are not obvious.
266

266

00:12:02,210  -->  00:12:04,420
So I'll mention a couple,
267

267

00:12:04,420  -->  00:12:07,070
but then I'll let you know where you can find out more.
268

268

00:12:07,070  -->  00:12:12,070
So one of them is that if for instance your,
269

269

00:12:12,410  -->  00:12:16,960
at the very start of your back propagation,
270

270

00:12:16,960  -->  00:12:21,960
your output value is very, very, very, very tiny, very tiny.
271

271

00:12:22,220  -->  00:12:25,630
So it's much smaller than the actual value that you want.
272

272

00:12:25,630  -->  00:12:27,220
Then at the very start,
273

273

00:12:27,220  -->  00:12:29,840
the gradient in your gradient descent
274

274

00:12:29,840  -->  00:12:31,320
will be very, very low,
275

275

00:12:31,320  -->  00:12:33,730
and it won't be enough,
276

276

00:12:33,730  -->  00:12:36,870
it will be very hard for the neural network
277

277

00:12:36,870  -->  00:12:39,380
to actually start doing something
278

278

00:12:39,380  -->  00:12:41,710
and start moving around and start adjusting those waves
279

279

00:12:41,710  -->  00:12:45,030
and start actually moving in the right direction.
280

280

00:12:45,030  -->  00:12:47,720
Whereas when you use something like the cross-entropy
281

281

00:12:47,720  -->  00:12:49,910
because it's got that logarithm in it,
282

282

00:12:49,910  -->  00:12:54,180
it actually helps the network assess
283

283

00:12:54,180  -->  00:12:57,220
even a small error like that and do something about it.
284

284

00:12:57,220  -->  00:12:58,430
Here's how to think about it.
285

285

00:12:58,430  -->  00:13:03,258
So let's say in, again this is a very intuitive approach,
286

286

00:13:03,258  -->  00:13:05,930
there's gonna be a link to the mathematics
287

287

00:13:05,930  -->  00:13:08,760
and you can derive these things through the mathematics
288

288

00:13:08,760  -->  00:13:11,130
in more detail but a very intuitive approach.
289

289

00:13:11,130  -->  00:13:16,130
Let's say your outcome that you want is one
290

290

00:13:17,610  -->  00:13:22,610
and right now you are at one millionth of one, right?
291

291

00:13:23,100  -->  00:13:25,160
0.000001.
292

292

00:13:25,160  -->  00:13:29,820
And then, next time you improve your outcome
293

293

00:13:29,820  -->  00:13:32,730
from one millionth to one thousandth
294

294

00:13:32,730  -->  00:13:37,210
and in terms of, if you calculate the squared error,
295

295

00:13:37,210  -->  00:13:39,530
you're just subtracting one from the other,
296

296

00:13:39,530  -->  00:13:40,810
or basically in each case
297

297

00:13:40,810  -->  00:13:42,120
you're calculating the squared error
298

298

00:13:42,120  -->  00:13:43,680
and you'll see that the squared error
299

299

00:13:43,680  -->  00:13:46,650
is when you compare one case versus the other,
300

300

00:13:46,650  -->  00:13:48,050
it didn't change that much.
301

301

00:13:48,050  -->  00:13:50,250
You didn't improve your network that much
302

302

00:13:50,250  -->  00:13:52,030
when you're looking at the mean squared error.
303

303

00:13:52,030  -->  00:13:55,290
But if you're looking at the cross-entropy,
304

304

00:13:55,290  -->  00:13:57,310
because you're taking a logarithm,
305

305

00:13:57,310  -->  00:13:59,090
and then you're comparing the two,
306

306

00:13:59,090  -->  00:14:00,941
dividing one with the other,
307

307

00:14:00,941  -->  00:14:04,001
you will see that you have actually improved
308

308

00:14:04,001  -->  00:14:06,070
your network significantly.
309

309

00:14:06,070  -->  00:14:10,520
So that jump from one millionth to one thousandth
310

310

00:14:10,520  -->  00:14:12,730
in mean squared error terms will be very low.
311

311

00:14:12,730  -->  00:14:14,370
It will be insignificant
312

312

00:14:14,370  -->  00:14:19,330
and it won't guide your gradient boosting process
313

313

00:14:19,330  -->  00:14:22,030
or your back propagation in the right direction.
314

314

00:14:22,030  -->  00:14:24,150
It will guide it in the right direction
315

315

00:14:24,150  -->  00:14:26,660
but it will be like a very slow guidance,
316

316

00:14:26,660  -->  00:14:29,380
it won't have enough power,
317

317

00:14:29,380  -->  00:14:32,180
whereas if you do through cross-entropy,
318

318

00:14:32,180  -->  00:14:33,810
cross-entropy will understand that,
319

319

00:14:33,810  -->  00:14:36,060
oh even though these are very small adjustments
320

320

00:14:36,060  -->  00:14:38,030
that are just, you know,
321

321

00:14:38,030  -->  00:14:40,585
making a tiny change in absolute terms,
322

322

00:14:40,585  -->  00:14:43,760
in relative terms, it's a huge improvement,
323

323

00:14:43,760  -->  00:14:46,020
and we're definitely going in the right direction,
324

324

00:14:46,020  -->  00:14:47,140
let's keep going that way.
325

325

00:14:47,140  -->  00:14:50,810
So cross-entropy will help your neural network
326

326

00:14:52,660  -->  00:14:56,750
get to the right, get to the optimal state.
327

327

00:14:56,750  -->  00:14:59,170
It's a better way for the neural network
328

328

00:14:59,170  -->  00:15:01,020
to get to an optimal state.
329

329

00:15:01,020  -->  00:15:03,690
But bear in mind that this only works
330

330

00:15:03,690  -->  00:15:07,010
when the cross-entropy is the preferred method
331

331

00:15:07,010  -->  00:15:08,160
only for classification.
332

332

00:15:08,160  -->  00:15:11,330
So if you're talking about things like regression,
333

333

00:15:11,330  -->  00:15:13,670
like which we had in artificial neural networks,
334

334

00:15:13,670  -->  00:15:17,430
then you would rather go with mean squared error,
335

335

00:15:17,430  -->  00:15:20,520
whereas cross-entropy is better for classification
336

336

00:15:20,520  -->  00:15:22,430
and again it has to do with the fact
337

337

00:15:22,430  -->  00:15:23,600
that we're using softmax function.
338

338

00:15:23,600  -->  00:15:26,690
So that's a, kind of an intuitive explanation of that.
339

339

00:15:26,690  -->  00:15:29,270
A good place to learn a bit more about that
340

340

00:15:29,270  -->  00:15:31,100
if you're really interested in, you know,
341

341

00:15:31,100  -->  00:15:34,806
why are we using cross-entropy versus mean squared error,
342

342

00:15:34,806  -->  00:15:37,660
google a video by Geoffrey Hinton
343

343

00:15:37,660  -->  00:15:40,600
called "The Softmax Output Function."
344

344

00:15:40,600  -->  00:15:43,360
And he explains it very well, you know,
345

345

00:15:43,360  -->  00:15:46,290
being the godfather of deep learning,
346

346

00:15:46,290  -->  00:15:47,940
who can explain it better anyway.
347

347

00:15:48,780  -->  00:15:51,590
And by the way, any video by Geoffrey Hinton is golden.
348

348

00:15:51,590  -->  00:15:54,223
He's just got a huge talent for explaining things.
349

349

00:15:55,200  -->  00:15:58,540
Anyways, so that's softmax versus cross-entropy.
350

350

00:15:58,540  -->  00:16:00,210
I hope that gives you, kind of like,
351

351

00:16:00,210  -->  00:16:02,030
an intuitive understanding of what's going on here,
352

352

00:16:02,030  -->  00:16:03,240
but more importantly,
353

353

00:16:03,240  -->  00:16:06,470
that you're not put off by the term cross-entropy,
354

354

00:16:06,470  -->  00:16:09,020
because Hadland will mention in the practical tutorials,
355

355

00:16:09,020  -->  00:16:11,170
and I wanted to make sure that you're prepared for that,
356

356

00:16:11,170  -->  00:16:15,254
and it's just another way of calculating your loss function
357

357

00:16:15,254  -->  00:16:17,570
and another way of optimizing your network
358

358

00:16:17,570  -->  00:16:21,780
which is specifically tailored to classification problems
359

359

00:16:21,780  -->  00:16:24,000
and therefore convolutional neural networks
360

360

00:16:24,000  -->  00:16:28,200
and comes hand-in-hand with the softmax function.
361

361

00:16:28,200  -->  00:16:29,650
So, additional reading.
362

362

00:16:29,650  -->  00:16:34,072
If you'd like a light introduction into cross-entropy,
363

363

00:16:34,072  -->  00:16:36,663
if you're interested in cross-entropy a bit more of course,
364

364

00:16:36,663  -->  00:16:39,127
a good article to check out is called
365

365

00:16:39,127  -->  00:16:41,610
"A Friendly Introduction to Cross-Entropy Loss"
366

366

00:16:41,610  -->  00:16:45,240
by Rob DiPietro, 2016.
367

367

00:16:45,240  -->  00:16:47,080
Here's the link below.
368

368

00:16:47,080  -->  00:16:51,150
Very, very nice, very soft, nothing,
369

369

00:16:51,150  -->  00:16:56,080
no super complex math, good analogies, good examples.
370

370

00:16:56,080  -->  00:16:58,570
He uses analogies of cars, and you look at cars,
371

371

00:16:58,570  -->  00:17:01,070
and talks about information and bits restrictions,
372

372

00:17:02,006  -->  00:17:03,220
and you know, how would you encode this,
373

373

00:17:03,220  -->  00:17:04,053
how would you encode that.
374

374

00:17:04,053  -->  00:17:05,770
It's a good article to have a look at
375

375

00:17:05,770  -->  00:17:08,573
and will give you a good overview of cross-entropy,
376

376

00:17:09,500  -->  00:17:11,800
like from an introductory standpoint.
377

377

00:17:11,800  -->  00:17:13,810
If you want to dig into the heavy math,
378

378

00:17:13,810  -->  00:17:15,442
like what you see here,
379

379

00:17:15,442  -->  00:17:19,337
then check out an article by, or blog by,
380

380

00:17:19,337  -->  00:17:22,440
"How to implement a neural network Intermezzo 2"
381

381

00:17:22,440  -->  00:17:25,120
so intermezzo is like an intermediate thing,
382

382

00:17:25,120  -->  00:17:29,170
like a, intermittence in, you know,
383

383

00:17:29,170  -->  00:17:32,210
like when you to a theater and you have like a break
384

384

00:17:33,150  -->  00:17:36,650
between the first part and the second part,
385

385

00:17:36,650  -->  00:17:38,290
because he's like going through all these steps
386

386

00:17:38,290  -->  00:17:39,590
and then he's like, and then he says,
387

387

00:17:39,590  -->  00:17:41,433
oh I gotta explain this first.
388

388

00:17:42,360  -->  00:17:44,020
And yeah, so that's why it's called intermezzo,
389

389

00:17:44,020  -->  00:17:46,600
no other reason (laughs) as far as I understand.
390

390

00:17:46,600  -->  00:17:50,650
The article's by Peter Roelants, 2016, as well,
391

391

00:17:50,650  -->  00:17:53,690
so both are quite recent, and yeah,
392

392

00:17:53,690  -->  00:17:55,500
check out this if you'd like to dig
393

393

00:17:55,500  -->  00:17:59,124
into the mathematics behind cross-entropy,
394

394

00:17:59,124  -->  00:18:02,810
behind softmax and cross-entropy in this article actually.
395

395

00:18:02,810  -->  00:18:07,250
So there we go, that's all there is to these two.
396

396

00:18:07,250  -->  00:18:10,840
Hopefully I was able to add some additional clarity
397

397

00:18:10,840  -->  00:18:12,660
and good luck with that.
398

398

00:18:12,660  -->  00:18:16,870
It's going to be fun and enjoy the practical tutorials.
399

399

00:18:16,870  -->  00:18:17,970
I'll see you next time.
400

400

00:18:17,970  -->  00:18:19,793
Until then, enjoy deep learning.
