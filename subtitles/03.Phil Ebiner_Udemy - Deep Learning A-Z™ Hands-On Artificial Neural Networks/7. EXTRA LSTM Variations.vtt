WEBVTT
1
1

00:00:00.280  -->  00:00:01.113
<v Instructor>Hello and welcome back</v>
2

2

00:00:01.113  -->  00:00:02.600
to the course on deep learning.
3

3

00:00:02.600  -->  00:00:04.240
Today, we're just going to quickly cover
4

4

00:00:04.240  -->  00:00:09.240
off the variations of long short-term memory architectures.
5

5

00:00:09.720  -->  00:00:12.942
I think it's important so that you're at least aware
6

6

00:00:12.942  -->  00:00:17.330
that there are other alternatives to the LSTM version
7

7

00:00:17.330  -->  00:00:18.380
that we looked at
8

8

00:00:18.380  -->  00:00:20.190
and that you might encounter them sometime
9

9

00:00:20.190  -->  00:00:21.023
in your work.
10

10

00:00:21.023  -->  00:00:24.990
So here is the standard LSTM which we discussed.
11

11

00:00:24.990  -->  00:00:28.410
It should be a pretty comfortable fit by now
12

12

00:00:28.410  -->  00:00:31.130
and now let's have a look at a couple of variations.
13

13

00:00:31.130  -->  00:00:34.900
So variation number one is when you add peepholes.
14

14

00:00:34.900  -->  00:00:36.160
So I'm just gonna go back again
15

15

00:00:36.160  -->  00:00:38.782
so you see these lines are added
16

16

00:00:38.782  -->  00:00:43.782
connecting these sigmoid activation functions
17

17

00:00:44.030  -->  00:00:48.749
or these neural network layer operations.
18

18

00:00:48.749  -->  00:00:51.340
Addtitional input is being provided.
19

19

00:00:51.340  -->  00:00:54.480
It's the information about the current state
20

20

00:00:54.480  -->  00:00:56.140
of the memory cell.
21

21

00:00:56.140  -->  00:00:57.901
And that basically allows,
22

22

00:00:57.901  -->  00:01:00.307
that's why it's called peephole,
23

23

00:01:00.307  -->  00:01:05.200
allows these decisions about the valves to be made
24

24

00:01:05.200  -->  00:01:06.900
with taking into account
25

25

00:01:06.900  -->  00:01:09.860
what is actually sitting there in the memory.
26

26

00:01:09.860  -->  00:01:13.330
Modification number two is when you take these two valves,
27

27

00:01:13.330  -->  00:01:15.530
the forget valve and the memory valve,
28

28

00:01:15.530  -->  00:01:16.790
and you connect them.
29

29

00:01:16.790  -->  00:01:18.240
So let's go back.
30

30

00:01:18.240  -->  00:01:23.240
Instead of having a separate decision for the memory valve,
31

31

00:01:23.950  -->  00:01:25.590
now you have a combined decision
32

32

00:01:25.590  -->  00:01:27.230
for the forget valve and the memory valve.
33

33

00:01:27.230  -->  00:01:30.820
And basically whenever you add something into memory,
34

34

00:01:30.820  -->  00:01:33.810
so whenever you close the memory off whenever this is zero,
35

35

00:01:33.810  -->  00:01:35.240
this becomes the opposite.
36

36

00:01:35.240  -->  00:01:36.550
One minus zero becomes a one,
37

37

00:01:36.550  -->  00:01:38.080
so you have to put something in.
38

38

00:01:38.080  -->  00:01:40.200
If you make this a one,
39

39

00:01:40.200  -->  00:01:41.210
or close to one,
40

40

00:01:41.210  -->  00:01:44.200
this becomes a zero and therefore you don't put anything in.
41

41

00:01:44.200  -->  00:01:45.740
Here you just keep the old memory.
42

42

00:01:45.740  -->  00:01:49.760
So it just makes sense to combine them sometimes.
43

43

00:01:49.760  -->  00:01:52.070
And then another modification,
44

44

00:01:52.070  -->  00:01:54.810
a very popular modification
45

45

00:01:54.810  -->  00:01:58.843
called gated recurring units, GRUs, for short.
46

46

00:01:58.843  -->  00:02:03.630
What they do is completely get rid of the C pipeline,
47

47

00:02:03.630  -->  00:02:05.390
as you can see the memory pipeline,
48

48

00:02:05.390  -->  00:02:06.710
and they replace it with the H,
49

49

00:02:06.710  -->  00:02:08.790
which is the hidden pipeline,
50

50

00:02:08.790  -->  00:02:11.440
which we had before at the bottom.
51

51

00:02:11.440  -->  00:02:12.860
They just combine basically the two.
52

52

00:02:12.860  -->  00:02:16.090
So now instead of having two separate values,
53

53

00:02:16.090  -->  00:02:19.470
one for the memory and one for the hidden state,
54

54

00:02:19.470  -->  00:02:20.640
now you have one
55

55

00:02:20.640  -->  00:02:22.340
and it simplifies things.
56

56

00:02:22.340  -->  00:02:25.520
So it's probably a bit less flexible,
57

57

00:02:25.520  -->  00:02:29.660
but in terms of how many things are being controlled
58

58

00:02:29.660  -->  00:02:31.980
and monitored, there's actually less.
59

59

00:02:31.980  -->  00:02:34.222
And that's it.
60

60

00:02:34.222  -->  00:02:36.830
This is how the gated recurring unit works.
61

61

00:02:36.830  -->  00:02:38.040
As you can see,
62

62

00:02:38.040  -->  00:02:39.340
it might look a bit more convoluted,
63

63

00:02:39.340  -->  00:02:40.750
but in reality it is a bit simpler.
64

64

00:02:40.750  -->  00:02:44.493
You only have this valve,
65

65

00:02:45.670  -->  00:02:46.720
then you another valve here,
66

66

00:02:46.720  -->  00:02:47.620
another valve here
67

67

00:02:47.620  -->  00:02:49.818
and these two are connected as well.
68

68

00:02:49.818  -->  00:02:52.860
So this is in essence the gated recurring unit.
69

69

00:02:52.860  -->  00:02:57.720
The constant behind it is to get rid of the memory cell
70

70

00:02:57.720  -->  00:03:01.630
and just have this one pipeline going through there
71

71

00:03:01.630  -->  00:03:03.130
that takes care of everything.
72

72

00:03:04.050  -->  00:03:07.834
So there we go, that's the three LSTM variations.
73

73

00:03:07.834  -->  00:03:10.201
There's lots more other ones.
74

74

00:03:10.201  -->  00:03:13.270
A good paper to check out is called
75

75

00:03:13.270  -->  00:03:15.780
LSTM A Search Space Odyssey
76

76

00:03:15.780  -->  00:03:19.621
by Klaus Greff and others, 2015.
77

77

00:03:19.621  -->  00:03:23.240
So there they compared quite a few different LSTMs.
78

78

00:03:23.240  -->  00:03:27.000
You might like this research that they did.
79

79

00:03:27.000  -->  00:03:27.833
So there you go.
80

80

00:03:27.833  -->  00:03:31.447
That's a short intro into the alternative architectures
81

81

00:03:31.447  -->  00:03:33.450
of the LSTM.
82

82

00:03:33.450  -->  00:03:34.900
And I look forward to seeing you next time.
83

83

00:03:34.900  -->  00:03:36.543
Until then, enjoy deep learning.
