1
1

00:00:00,200  -->  00:00:02,690
<v Instructor>Hello, and welcome to this Python tutorial.</v>
2

2

00:00:02,690  -->  00:00:05,250
Alright, so now time to make the training
3

3

00:00:05,250  -->  00:00:07,270
of our restricted Boltzmann machine,
4

4

00:00:07,270  -->  00:00:09,320
and as I told you in the previous tutorial,
5

5

00:00:09,320  -->  00:00:11,380
the most difficult part is done.
6

6

00:00:11,380  -->  00:00:13,010
And even if the training is going
7

7

00:00:13,010  -->  00:00:14,800
to take a couple of lines of code,
8

8

00:00:14,800  -->  00:00:18,080
well you're gonna see that everything will be very logical
9

9

00:00:18,080  -->  00:00:20,050
and basically we will only have
10

10

00:00:20,050  -->  00:00:23,690
to include inside of a for loop, the different functions
11

11

00:00:23,690  -->  00:00:26,320
that we made in this RBM class.
12

12

00:00:26,320  -->  00:00:28,980
Because, indeed, this RBM class contains
13

13

00:00:28,980  -->  00:00:31,460
what is at the heart of the RBM model
14

14

00:00:31,460  -->  00:00:33,950
and so now we just need to make the training happen
15

15

00:00:33,950  -->  00:00:36,970
and that's exactly what we're gonna do in this tutorial.
16

16

00:00:36,970  -->  00:00:38,240
Alright, so let's start.
17

17

00:00:38,240  -->  00:00:41,240
We have to start by choosing a number of epochs.
18

18

00:00:41,240  -->  00:00:43,160
So, we're gonna call the variable
19

19

00:00:43,160  -->  00:00:45,163
for the number of epoch nb_epoch.
20

20

00:00:46,560  -->  00:00:47,393
Alright?
21

21

00:00:47,393  -->  00:00:51,070
Equals, and let's choose for now 10 epochs
22

22

00:00:51,070  -->  00:00:52,950
because we have a few observations.
23

23

00:00:52,950  -->  00:00:56,300
We have only 943 observations
24

24

00:00:56,300  -->  00:00:58,597
and besides we only have binary value zero
25

25

00:00:58,597  -->  00:01:02,010
and one so the convergence will be reached pretty fast.
26

26

00:01:02,010  -->  00:01:03,550
So nb_epoch will stand,
27

27

00:01:03,550  -->  00:01:05,920
seems to be a good choice to start with.
28

28

00:01:05,920  -->  00:01:07,970
Alright so number of epoch equals 10,
29

29

00:01:07,970  -->  00:01:11,210
then what we need to do is, of course, make the for loop.
30

30

00:01:11,210  -->  00:01:13,730
We're gonna make a for loop that will go through
31

31

00:01:13,730  -->  00:01:16,020
the 10 epochs and then each epoch,
32

32

00:01:16,020  -->  00:01:18,400
all our observations will go into the network
33

33

00:01:18,400  -->  00:01:20,790
and we will update the weights after the observations
34

34

00:01:20,790  -->  00:01:23,400
of each batch passed through the network.
35

35

00:01:23,400  -->  00:01:25,310
And then in the end we'll get our final
36

36

00:01:25,310  -->  00:01:28,410
visible note with the new ratings for the movies
37

37

00:01:28,410  -->  00:01:31,160
that were not originally rated.
38

38

00:01:31,160  -->  00:01:32,750
Alright, so let's make this for loop.
39

39

00:01:32,750  -->  00:01:36,010
We start with for, then we need to come up
40

40

00:01:36,010  -->  00:01:38,340
with a variable for epoch and so
41

41

00:01:38,340  -->  00:01:40,310
we're gonna simply take epoch,
42

42

00:01:40,310  -->  00:01:42,370
that's the name of the looping variable,
43

43

00:01:42,370  -->  00:01:45,760
in range and in parentheses,
44

44

00:01:45,760  -->  00:01:48,560
well let's start at epoch number one
45

45

00:01:48,560  -->  00:01:53,160
and then nb_epoch plus one.
46

46

00:01:53,160  -->  00:01:55,960
Alright, so this will go from one to 10
47

47

00:01:55,960  -->  00:01:58,660
because even if this is equal to 11,
48

48

00:01:58,660  -->  00:02:01,520
remember that the upper bound of a range is not included
49

49

00:02:01,520  -->  00:02:04,350
and therefore this will go from one to 10.
50

50

00:02:04,350  -->  00:02:09,350
Alright and then colon, and let's go inside the for loop.
51

51

00:02:09,410  -->  00:02:11,200
Alright, so what do we have to do first?
52

52

00:02:11,200  -->  00:02:13,720
Well, remember that for any deep learning
53

53

00:02:13,720  -->  00:02:16,450
algorithm that we make, we need a loss function
54

54

00:02:16,450  -->  00:02:17,620
to measure the error.
55

55

00:02:17,620  -->  00:02:19,160
To measure the error between
56

56

00:02:19,160  -->  00:02:22,020
the predictions and the real ratings.
57

57

00:02:22,020  -->  00:02:23,330
And so you know, in this training
58

58

00:02:23,330  -->  00:02:25,180
we will compare the predictions
59

59

00:02:25,180  -->  00:02:26,630
to the ratings we already have,
60

60

00:02:26,630  -->  00:02:28,820
that is the ratings of the training set.
61

61

00:02:28,820  -->  00:02:30,810
And so basically we will measure
62

62

00:02:30,810  -->  00:02:33,320
the difference between the predicted ratings,
63

63

00:02:33,320  -->  00:02:37,480
that is either zero or one, and the real ratings, zero one.
64

64

00:02:37,480  -->  00:02:40,860
And we have a couple of options for measuring the loss
65

65

00:02:40,860  -->  00:02:44,050
we have the RMSE, the Root Mean Square Error,
66

66

00:02:44,050  -->  00:02:46,080
which is the root of the mean
67

67

00:02:46,080  -->  00:02:47,830
of the square differences between
68

68

00:02:47,830  -->  00:02:49,880
the predicted ratings and the real ratings.
69

69

00:02:49,880  -->  00:02:52,410
So that's the most common loss measuring use
70

70

00:02:52,410  -->  00:02:54,620
and practice but then we have some other ones
71

71

00:02:54,620  -->  00:02:57,840
like the simple distance, the absolute distance
72

72

00:02:57,840  -->  00:03:00,240
that measures, simply, the absolute difference
73

73

00:03:00,240  -->  00:03:02,810
between the predicted rating and the real rating.
74

74

00:03:02,810  -->  00:03:04,390
And so in this deep learning course
75

75

00:03:04,390  -->  00:03:07,290
I want to introduce you to these two measures,
76

76

00:03:07,290  -->  00:03:09,860
and in fact for this RBM we'll go with
77

77

00:03:09,860  -->  00:03:12,180
the simple difference in absolute value,
78

78

00:03:12,180  -->  00:03:15,310
and for the auto-encoders we will use the RMSE.
79

79

00:03:15,310  -->  00:03:18,600
And so the point is that we need to introduce
80

80

00:03:18,600  -->  00:03:20,710
a loss variable, and we're gonna call it
81

81

00:03:20,710  -->  00:03:22,040
train loss, train_loss.
82

82

00:03:22,040  -->  00:03:24,540
Train, underscore, loss.
83

83

00:03:24,540  -->  00:03:26,910
And we will initialize it to zero
84

84

00:03:26,910  -->  00:03:28,880
of course because before we started
85

85

00:03:28,880  -->  00:03:31,350
training this loss is equal to zero.
86

86

00:03:31,350  -->  00:03:32,890
And then the slots will increase
87

87

00:03:32,890  -->  00:03:34,380
when we find some errors between
88

88

00:03:34,380  -->  00:03:36,323
the predictions and the real ratings.
89

89

00:03:37,160  -->  00:03:39,610
Then, we're gonna need a counter.
90

90

00:03:39,610  -->  00:03:41,030
Because what we're gonna do is
91

91

00:03:41,030  -->  00:03:43,610
we're going to normalize the train loss
92

92

00:03:43,610  -->  00:03:45,030
and to normalize the train loss
93

93

00:03:45,030  -->  00:03:47,430
we will simply divide the train loss
94

94

00:03:47,430  -->  00:03:51,560
by this counter, and I'm gonna call this counter s,
95

95

00:03:51,560  -->  00:03:55,220
and we initialize it to zero, as well,
96

96

00:03:55,220  -->  00:03:57,300
but we want it to be afloat and therefore
97

97

00:03:57,300  -->  00:04:01,740
I'm adding a dot here so that s has a type float.
98

98

00:04:01,740  -->  00:04:03,710
Alright, so we have our counter which
99

99

00:04:03,710  -->  00:04:06,660
we will increment after each epoch
100

100

00:04:06,660  -->  00:04:10,010
and then now time to do the real training.
101

101

00:04:10,010  -->  00:04:12,410
And the real training happens with
102

102

00:04:12,410  -->  00:04:16,210
these three functions, sample_h, sample_v, and train.
103

103

00:04:16,210  -->  00:04:19,160
And so, so far, when we made these functions
104

104

00:04:19,160  -->  00:04:22,070
it was regarding to one user
105

105

00:04:22,070  -->  00:04:24,240
and of course the samplings and the contrast
106

106

00:04:24,240  -->  00:04:26,640
of divergence algorithm have to be done
107

107

00:04:26,640  -->  00:04:29,000
over all the users, but remember
108

108

00:04:29,000  -->  00:04:31,120
over all the users in the batch.
109

109

00:04:31,120  -->  00:04:35,460
So what we'll do first is get these batches of users.
110

110

00:04:35,460  -->  00:04:37,830
And to do so we need another for loop.
111

111

00:04:37,830  -->  00:04:39,960
So let's do it, so it's a for loop inside
112

112

00:04:39,960  -->  00:04:42,410
the first for loop, and so for,
113

113

00:04:42,410  -->  00:04:45,430
and now since this for loop is about looping
114

114

00:04:45,430  -->  00:04:47,927
over all the users, I'm introducing the looping
115

115

00:04:47,927  -->  00:04:52,927
variable id, underscore user, in range.
116

116

00:04:54,410  -->  00:04:57,320
So, the indexes of the users start at zero
117

117

00:04:57,320  -->  00:05:00,910
so we start here with zero, comma,
118

118

00:05:00,910  -->  00:05:02,570
and now according to you what is going
119

119

00:05:02,570  -->  00:05:04,710
to be the stop index.
120

120

00:05:04,710  -->  00:05:05,646
That's an important point.
121

121

00:05:05,646  -->  00:05:09,110
Remember that we wanna take some batches of users,
122

122

00:05:09,110  -->  00:05:12,080
we don't wanna take each user one by one and then update
123

123

00:05:12,080  -->  00:05:14,040
the weight, we want to update the weight after
124

124

00:05:14,040  -->  00:05:16,970
each batch of users going through the network.
125

125

00:05:16,970  -->  00:05:20,330
And so right now we're not taking each user one by one,
126

126

00:05:20,330  -->  00:05:22,640
we're taking the batches of the users,
127

127

00:05:22,640  -->  00:05:26,550
and therefore since the batch size equals 100,
128

128

00:05:26,550  -->  00:05:28,100
well the first batch is going
129

129

00:05:28,100  -->  00:05:32,730
to contain all the users from index zero to index 99,
130

130

00:05:32,730  -->  00:05:34,340
then the second batch size is going
131

131

00:05:34,340  -->  00:05:39,340
to contain the users from index 100 to index 199
132

132

00:05:39,547  -->  00:05:43,230
and the third batch size from 200 to 299,
133

133

00:05:43,230  -->  00:05:44,740
excreta, until the end.
134

134

00:05:44,740  -->  00:05:48,130
And so the last batch that will go into the network
135

135

00:05:48,130  -->  00:05:53,130
will be the batch size of the users from index 943,
136

136

00:05:53,370  -->  00:05:57,050
minus 100, that is 843.
137

137

00:05:57,050  -->  00:05:59,170
And so the last batch of users will contain
138

138

00:05:59,170  -->  00:06:04,170
the users from index 843, to, 943.
139

139

00:06:04,400  -->  00:06:07,050
And therefore the stop of the range for
140

140

00:06:07,050  -->  00:06:10,420
the users is not nb users but
141

141

00:06:10,420  -->  00:06:15,420
nb, users, minus, batch, underscore, size.
142

142

00:06:16,320  -->  00:06:20,400
nb users minus batch size, that is 843.
143

143

00:06:20,400  -->  00:06:23,520
Okay, but then, it's not all, we need the step
144

144

00:06:23,520  -->  00:06:24,900
because we don't wanna go from one
145

145

00:06:24,900  -->  00:06:27,550
to one, we wanna go from one to 100,
146

146

00:06:27,550  -->  00:06:31,630
and then from 100 to 200, excreta, until the last batch.
147

147

00:06:31,630  -->  00:06:33,860
And so the step, that's the third argument
148

148

00:06:33,860  -->  00:06:37,030
we need to input, the step is going to be, not one,
149

149

00:06:37,030  -->  00:06:39,550
the default step, but 100,
150

150

00:06:39,550  -->  00:06:42,410
that is the batch size and therefore here as a third
151

151

00:06:42,410  -->  00:06:46,430
argument we need to input batch, size.
152

152

00:06:46,430  -->  00:06:49,810
That's how you implement batch learning from scratch.
153

153

00:06:49,810  -->  00:06:52,150
Everything happens in this loop.
154

154

00:06:52,150  -->  00:06:55,750
Alright, and now we're ready to go into the loop.
155

155

00:06:55,750  -->  00:06:58,140
Okay so now as a first step in this new
156

156

00:06:58,140  -->  00:07:00,050
for loop, what do we have to do?
157

157

00:07:00,050  -->  00:07:02,320
Well first, we will get separately,
158

158

00:07:02,320  -->  00:07:04,810
our input and our target.
159

159

00:07:04,810  -->  00:07:08,360
Our input is the ratings of all the movies
160

160

00:07:08,360  -->  00:07:10,820
by the specific user we're dealing with right now
161

161

00:07:10,820  -->  00:07:13,210
in the loop, and the target is going
162

162

00:07:13,210  -->  00:07:15,740
to be at the beginning the same as the input,
163

163

00:07:15,740  -->  00:07:18,100
but then since you know the input is gonna go
164

164

00:07:18,100  -->  00:07:20,460
into the Gibbs chain and will be updated
165

165

00:07:20,460  -->  00:07:23,840
to get the new ratings in each visible node
166

166

00:07:23,840  -->  00:07:25,610
then the input is going to change,
167

167

00:07:25,610  -->  00:07:28,890
but the target will keep its same initial value.
168

168

00:07:28,890  -->  00:07:31,300
Alright, so let's get first the input,
169

169

00:07:31,300  -->  00:07:33,600
and so as you understood, as I just said,
170

170

00:07:33,600  -->  00:07:36,170
the input is the vector that is gonna go
171

171

00:07:36,170  -->  00:07:40,090
into the Gibbs chain and will be updated at each round trip.
172

172

00:07:40,090  -->  00:07:42,060
And so actually, we're gonna call, very simply,
173

173

00:07:42,060  -->  00:07:46,320
this input, vk, because this is this vk that is
174

174

00:07:46,320  -->  00:07:48,940
going to be the output of Gibbs sampling, you know
175

175

00:07:48,940  -->  00:07:51,370
after the K steps of the random walk.
176

176

00:07:51,370  -->  00:07:55,440
But first, at the start this vk is actually
177

177

00:07:55,440  -->  00:07:57,770
the input batch of observations,
178

178

00:07:57,770  -->  00:08:00,040
that is the input batch of all the ratings
179

179

00:08:00,040  -->  00:08:01,720
of the users in the batch.
180

180

00:08:01,720  -->  00:08:04,310
The ratings that already existed.
181

181

00:08:04,310  -->  00:08:08,737
And so, this input is going to be the training_set,
182

182

00:08:09,890  -->  00:08:12,680
then brackets, and since right now we're dealing
183

183

00:08:12,680  -->  00:08:16,750
with a specific user that has the ID, id user,
184

184

00:08:16,750  -->  00:08:20,330
well the batch that we want to get is all the users
185

185

00:08:20,330  -->  00:08:25,330
from id_user, up to id_user, plus, batch_size.
186

186

00:08:30,420  -->  00:08:31,370
Why's that?
187

187

00:08:31,370  -->  00:08:35,610
It's because this id_user, colon, id plus batch_size
188

188

00:08:35,610  -->  00:08:40,610
is the range from the id_user up to the id_user plus 100.
189

189

00:08:41,890  -->  00:08:46,000
So we're taking all the users from this id_user here,
190

190

00:08:46,000  -->  00:08:50,300
up to the next 100 users in the training set.
191

191

00:08:50,300  -->  00:08:53,670
And so that gives us our batch of 100 users.
192

192

00:08:53,670  -->  00:08:57,480
So perfect, that gives us our input batch vk.
193

193

00:08:57,480  -->  00:09:00,100
Then later vk will go into the Gibbs chain.
194

194

00:09:00,100  -->  00:09:03,790
Then let's get our target, the target is the one thing
195

195

00:09:03,790  -->  00:09:05,910
we don't wanna touch, that is, it's the batch
196

196

00:09:05,910  -->  00:09:07,970
of original ratings that we don't wanna
197

197

00:09:07,970  -->  00:09:09,910
touch, and that we want to compare,
198

198

00:09:09,910  -->  00:09:12,250
in the end, to our predicted ratings.
199

199

00:09:12,250  -->  00:09:14,010
And we need it because actually we want to measure
200

200

00:09:14,010  -->  00:09:16,210
the error between the predicted ratings and the real
201

201

00:09:16,210  -->  00:09:19,270
ratings to get the loss, the train loss.
202

202

00:09:19,270  -->  00:09:22,173
So, we're gonna call this target v0,
203

203

00:09:23,220  -->  00:09:26,410
so v0 are our ratings of the movies that were
204

204

00:09:26,410  -->  00:09:30,750
already rated by the 100 users in this batch.
205

205

00:09:30,750  -->  00:09:34,730
And so basically, since, you know the target
206

206

00:09:34,730  -->  00:09:37,740
is the same as the input at the beginning,
207

207

00:09:37,740  -->  00:09:40,630
well we just need to paste what's above.
208

208

00:09:40,630  -->  00:09:44,070
At the beginning the input is the same as the target.
209

209

00:09:44,070  -->  00:09:46,380
But then the input will be updated, as you saw
210

210

00:09:46,380  -->  00:09:48,610
in the intuition lectures with Kiro.
211

211

00:09:48,610  -->  00:09:52,670
Okay, and then we have one third thing
212

212

00:09:52,670  -->  00:09:55,540
to get before we start the real deal.
213

213

00:09:55,540  -->  00:09:58,650
That is contrastive divergence with Gibbs sampling
214

214

00:09:58,650  -->  00:10:01,560
it is our initial probabilities.
215

215

00:10:01,560  -->  00:10:04,420
So according to you it is the initial probabilities,
216

216

00:10:04,420  -->  00:10:09,420
it is actually p, h, zero, remember ph0 is the probabilities
217

217

00:10:10,810  -->  00:10:14,090
that the hidden node at the start equal one
218

218

00:10:14,090  -->  00:10:16,410
given the real ratings.
219

219

00:10:16,410  -->  00:10:18,600
That is, given the ratings of the movies that
220

220

00:10:18,600  -->  00:10:22,060
were already rated by the users of our batch.
221

221

00:10:22,060  -->  00:10:25,960
And so here, in this for loop, I'm going to take as well ph0
222

222

00:10:27,210  -->  00:10:29,620
and now what do you think I'm going to add?
223

223

00:10:29,620  -->  00:10:31,600
Well, here we go, that's where we use
224

224

00:10:31,600  -->  00:10:35,090
our first function of one of these three functions
225

225

00:10:35,090  -->  00:10:37,220
according to you which one is it going to be?
226

226

00:10:37,220  -->  00:10:39,130
Well, we wanna get the probabilities
227

227

00:10:39,130  -->  00:10:41,770
that the hidden node equal one given the visible nodes
228

228

00:10:41,770  -->  00:10:44,160
at the beginning, and therefore that of course
229

229

00:10:44,160  -->  00:10:48,390
sample-h, that we want, because sample-h returns
230

230

00:10:48,390  -->  00:10:53,370
p_h given V, that is here it will return p_h given V, zero
231

231

00:10:54,380  -->  00:10:57,504
And since this sample_h function also returns
232

232

00:10:57,504  -->  00:11:00,730
the samples, the Bernoulli samples, well we have
233

233

00:11:00,730  -->  00:11:04,230
to use a Python trick to only get the first
234

234

00:11:04,230  -->  00:11:06,760
element of what this function returns,
235

235

00:11:06,760  -->  00:11:09,870
and the Python trick to do that is to add here
236

236

00:11:09,870  -->  00:11:14,100
a comma and then an underscore, so that it understands
237

237

00:11:14,100  -->  00:11:17,690
that we only want to return, you know, the first element
238

238

00:11:17,690  -->  00:11:19,640
of the function that we're gonna take right now,
239

239

00:11:19,640  -->  00:11:21,300
that is sample_h.
240

240

00:11:21,300  -->  00:11:24,350
So right now what we have to add is sample_h,
241

241

00:11:24,350  -->  00:11:28,810
but since sample-h is a method of our RBM class,
242

242

00:11:28,810  -->  00:11:30,700
well we need to use a sample-h function
243

243

00:11:30,700  -->  00:11:34,123
from our rbm object, and therefore we need to take here
244

244

00:11:34,123  -->  00:11:39,123
our rbm object, and then dot, and then now we can
245

245

00:11:39,280  -->  00:11:44,080
take our sample, underscore, h, function.
246

246

00:11:44,080  -->  00:11:46,320
Alright, and then parentheses of course,
247

247

00:11:46,320  -->  00:11:48,650
and now we have to input one argument
248

248

00:11:48,650  -->  00:11:50,360
as you can see here which is x.
249

249

00:11:50,360  -->  00:11:53,920
And remember, x corresponds to the visible node,
250

250

00:11:53,920  -->  00:11:55,980
because we want to sample the hidden node,
251

251

00:11:55,980  -->  00:11:59,310
and we do this with the probabilities p_h given
252

252

00:11:59,310  -->  00:12:02,030
the visible nodes, and so the variable here is
253

253

00:12:02,030  -->  00:12:04,580
the visible nodes, and so which visible nodes,
254

254

00:12:04,580  -->  00:12:08,470
well that's the visible nodes at the start, that is, v0.
255

255

00:12:08,470  -->  00:12:10,760
That is the original ratings of the movies
256

256

00:12:10,760  -->  00:12:13,590
for all the users of our batch.
257

257

00:12:13,590  -->  00:12:14,483
Alright, so, v0.
258

258

00:12:15,920  -->  00:12:18,420
Okay, things are starting to get shape.
259

259

00:12:18,420  -->  00:12:20,250
Then, next step.
260

260

00:12:20,250  -->  00:12:24,260
So the next step is, I'm sorry guys, another for loop.
261

261

00:12:24,260  -->  00:12:25,600
And, according to you what is
262

262

00:12:25,600  -->  00:12:28,060
this for loop going to be about?
263

263

00:12:28,060  -->  00:12:31,070
Well there is only one for loop that we can make,
264

264

00:12:31,070  -->  00:12:34,570
it's of course the for loop for the k steps
265

265

00:12:34,570  -->  00:12:36,400
of contrastive divergence.
266

266

00:12:36,400  -->  00:12:38,481
The K steps of contrastive divergence,
267

267

00:12:38,481  -->  00:12:40,050
that is in this for loop
268

268

00:12:40,050  -->  00:12:42,200
that we're gonna make the Gibbs chain,
269

269

00:12:42,200  -->  00:12:45,850
that we're gonna do our k steps of the random walk.
270

270

00:12:45,850  -->  00:12:47,270
Alright, so let's do this.
271

271

00:12:47,270  -->  00:12:49,940
Another for loop, so for, then let's call
272

272

00:12:49,940  -->  00:12:54,340
the looping variable of course, k in range, (10).
273

273

00:12:56,470  -->  00:12:59,540
Alright, and now, a little exercise,
274

274

00:12:59,540  -->  00:13:01,030
I would like you to continue here
275

275

00:13:01,030  -->  00:13:05,090
to implement what is going to be inside this for loop,
276

276

00:13:05,090  -->  00:13:07,040
that will be excellent practice for you,
277

277

00:13:07,040  -->  00:13:09,410
and you'll get the solution in the next tutorial.
278

278

00:13:09,410  -->  00:13:12,120
You have all the tools you need to apply now
279

279

00:13:12,120  -->  00:13:14,910
the contrastive divergence, so you should totally be able
280

280

00:13:14,910  -->  00:13:15,930
to do this.
281

281

00:13:15,930  -->  00:13:18,350
Alright, so good luck, guess what's inside this
282

282

00:13:18,350  -->  00:13:21,690
for loop, and we'll continue that in the next tutorial.
283

283

00:13:21,690  -->  00:13:23,290
Until then, enjoy deep learning!
