1
1

00:00:00,640  -->  00:00:01,473
<v Narrator>Hello and welcome back</v>
2

2

00:00:01,473  -->  00:00:03,170
to the course on deep learning.
3

3

00:00:03,170  -->  00:00:05,290
In today's tutorial we're going to talk about
4

4

00:00:05,290  -->  00:00:06,810
overcomplete hidden layers.
5

5

00:00:06,810  -->  00:00:08,690
But before we start, I wanted to mention
6

6

00:00:08,690  -->  00:00:10,890
that this tutorial and the next couple tutorials
7

7

00:00:10,890  -->  00:00:14,970
are going to be quick because they are
8

8

00:00:14,970  -->  00:00:18,110
very kind of overview, high level tutorials
9

9

00:00:18,110  -->  00:00:20,250
on different types of autoencoders.
10

10

00:00:20,250  -->  00:00:23,290
Autoencoders are quite a powerful technique
11

11

00:00:23,290  -->  00:00:25,300
or method in deep learning,
12

12

00:00:25,300  -->  00:00:26,810
and they are popular, and there are lots
13

13

00:00:26,810  -->  00:00:28,480
of variations of autoencoders.
14

14

00:00:28,480  -->  00:00:30,690
As we unfortunately couldn't go
15

15

00:00:30,690  -->  00:00:33,570
into detail and depth into all of them,
16

16

00:00:33,570  -->  00:00:34,403
but instead what we're going to do
17

17

00:00:34,403  -->  00:00:37,010
is we're going to show you the underlying
18

18

00:00:37,010  -->  00:00:40,410
concepts and principles of these variations
19

19

00:00:40,410  -->  00:00:43,580
and point you at specific papers
20

20

00:00:43,580  -->  00:00:45,300
and blogs and articles that you can
21

21

00:00:45,300  -->  00:00:48,490
read if you want to up-skill on certain ones,
22

22

00:00:48,490  -->  00:00:50,170
certain types of autoencoders.
23

23

00:00:50,170  -->  00:00:52,590
At the same time, even these high level overviews
24

24

00:00:52,590  -->  00:00:56,650
will give you some very valuable insights
25

25

00:00:56,650  -->  00:00:59,280
and awareness about the different types
26

26

00:00:59,280  -->  00:01:01,900
of autoencoders that exist and you'll be able to,
27

27

00:01:01,900  -->  00:01:04,170
not only when you see them, recognize them,
28

28

00:01:04,170  -->  00:01:07,900
but also you'll be able to operate with the terms,
29

29

00:01:07,900  -->  00:01:10,930
with their names easily and you won't be caught off-guard
30

30

00:01:10,930  -->  00:01:12,920
when you come across them in literature,
31

31

00:01:12,920  -->  00:01:15,420
or in different types of works that
32

32

00:01:15,420  -->  00:01:17,800
you'll be studying or preforming.
33

33

00:01:17,800  -->  00:01:19,530
So, let's get started!
34

34

00:01:19,530  -->  00:01:21,450
First thing that we need to talk about
35

35

00:01:21,450  -->  00:01:22,940
is overcomplete hidden layers.
36

36

00:01:22,940  -->  00:01:27,030
This is a underlying concept in a lot of
37

37

00:01:27,030  -->  00:01:29,910
or most of the variations of autoencoders.
38

38

00:01:29,910  -->  00:01:30,743
So let's have a look.
39

39

00:01:30,743  -->  00:01:33,360
So here we've got an autoencoder,
40

40

00:01:33,360  -->  00:01:36,990
four input values, two nodes in the hidden layer,
41

41

00:01:36,990  -->  00:01:39,470
and four nodes in the output layer.
42

42

00:01:39,470  -->  00:01:42,210
The question is here, what if we wanted
43

43

00:01:42,210  -->  00:01:45,550
to increase the number of nodes in the hidden layer?
44

44

00:01:45,550  -->  00:01:48,230
What if we wanted actually to have more nodes
45

45

00:01:48,230  -->  00:01:50,610
in the hidden layer than in the input layer?
46

46

00:01:50,610  -->  00:01:52,320
Something like this.
47

47

00:01:52,320  -->  00:01:55,640
And the obvious question is here like why?
48

48

00:01:55,640  -->  00:01:56,910
Why would we do that?
49

49

00:01:56,910  -->  00:01:59,200
But the answer is why not?
50

50

00:01:59,200  -->  00:02:02,500
We said that an autoencoder can be used
51

51

00:02:02,500  -->  00:02:04,340
as a feature extraction tool,
52

52

00:02:04,340  -->  00:02:06,250
but what if we want more features?
53

53

00:02:06,250  -->  00:02:08,740
Remember in artificial neural networks
54

54

00:02:08,740  -->  00:02:10,070
it was very easy for us to do,
55

55

00:02:10,070  -->  00:02:12,070
we had a certain number of inputs,
56

56

00:02:12,070  -->  00:02:15,010
then we could have a whole layer of
57

57

00:02:15,010  -->  00:02:17,090
whatever number of inputs that,
58

58

00:02:17,090  -->  00:02:18,450
of hidden nodes that we wanted.
59

59

00:02:18,450  -->  00:02:19,890
It could be six, it could be 10,
60

60

00:02:19,890  -->  00:02:21,890
it could be 100, doesn't matter,
61

61

00:02:21,890  -->  00:02:23,150
we could have as many as we want to.
62

62

00:02:23,150  -->  00:02:25,000
We could add more layers and so on.
63

63

00:02:25,000  -->  00:02:26,970
But the point was we weren't restricted
64

64

00:02:26,970  -->  00:02:30,070
to how many nodes we would have in the hidden layer.
65

65

00:02:30,070  -->  00:02:33,130
And that allowed us to extract more features
66

66

00:02:33,130  -->  00:02:35,150
and that would be great in the case of
67

67

00:02:35,150  -->  00:02:38,550
an autoencoder as well given that we are advocating
68

68

00:02:38,550  -->  00:02:41,440
it as a feature extraction tool.
69

69

00:02:41,440  -->  00:02:44,480
But we have a problem, if we were to do this,
70

70

00:02:44,480  -->  00:02:47,190
obviously the autoencoder can cheat.
71

71

00:02:47,190  -->  00:02:48,760
The autoencoder,
72

72

00:02:48,760  -->  00:02:53,740
this goal is to get the outputs to equate to the inputs.
73

73

00:02:53,740  -->  00:02:57,890
As soon as you give it four or more hidden nodes,
74

74

00:02:57,890  -->  00:02:59,500
in case you have four inputs, basically
75

75

00:02:59,500  -->  00:03:02,010
as soon as you give it a hidden layer which is
76

76

00:03:02,010  -->  00:03:04,690
the same size or greater than the input layer,
77

77

00:03:04,690  -->  00:03:07,987
it just basically is able to cheat and just say,
78

78

00:03:07,987  -->  00:03:10,887
"Alright this node is always going to be equal to this node,
79

79

00:03:10,887  -->  00:03:12,630
"and then this node is equal to this node."
80

80

00:03:12,630  -->  00:03:14,670
So information is just gonna fly through like that,
81

81

00:03:14,670  -->  00:03:16,010
and then you'll even have some extra
82

82

00:03:16,010  -->  00:03:17,700
nodes that are not being used.
83

83

00:03:17,700  -->  00:03:20,740
And that could be the end process,
84

84

00:03:20,740  -->  00:03:24,670
the end state that this autoencoder would end up in
85

85

00:03:24,670  -->  00:03:26,970
after the training and is going to be just useless
86

86

00:03:26,970  -->  00:03:29,600
it's not going to extract any valuable information,
87

87

00:03:29,600  -->  00:03:31,980
any valuable features for us through that process.
88

88

00:03:31,980  -->  00:03:34,330
So this is definitely a problem.
89

89

00:03:34,330  -->  00:03:36,920
And how are we going to solve that problem?
90

90

00:03:36,920  -->  00:03:38,240
Well that's what we're going to look at
91

91

00:03:38,240  -->  00:03:39,730
the next couple of tutorials.
92

92

00:03:39,730  -->  00:03:42,050
We're going to look at three different approaches
93

93

00:03:42,050  -->  00:03:43,820
that are used to solve that problem.
94

94

00:03:43,820  -->  00:03:47,010
And therefore three different variations of autoencoders.
95

95

00:03:47,010  -->  00:03:48,720
So lets proceed with the next tutorial
96

96

00:03:48,720  -->  00:03:50,040
and I will see you there.
97

97

00:03:50,040  -->  00:03:52,173
Until next time, enjoy deep learning.
