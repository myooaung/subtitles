WEBVTT
1
00:00:00.780 --> 00:00:03.160
Hi and welcome back to this end of course.

2
00:00:03.240 --> 00:00:08.340
So now we are ready to build our multi had attention sub layer that will heavily used.

3
00:00:08.340 --> 00:00:14.760
Of course the small scale that projects attention that we built just before right before starting to

4
00:00:14.760 --> 00:00:16.110
write the code.

5
00:00:16.110 --> 00:00:23.190
Let's have a look at the architecture once again of the sub layer so for each of the values queries

6
00:00:23.310 --> 00:00:26.810
and keys we will apply a big linear function.

7
00:00:26.820 --> 00:00:34.860
So which is exactly a dense layer intensive low and then splits the the the results into sub spaces

8
00:00:35.280 --> 00:00:37.530
for each of those sub spaces.

9
00:00:37.530 --> 00:00:44.070
We will apply the scale that product attention that we wrote just before then we will concatenate everything

10
00:00:44.070 --> 00:00:49.710
so that we get back to words that have the right dimension which is the model.

11
00:00:49.770 --> 00:00:51.250
And finally we apply.

12
00:00:51.260 --> 00:01:01.670
Lastly in our function to this whole computation so let's start right now by creating all class multi

13
00:01:01.980 --> 00:01:08.180
heads attention which inherits from layers.

14
00:01:08.210 --> 00:01:09.770
That's like

15
00:01:13.250 --> 00:01:16.310
as per usual we need this method.

16
00:01:16.310 --> 00:01:27.680
The initialization that will take self and the number of projection that we want to perform we call

17
00:01:27.680 --> 00:01:44.020
the Indian function from the layer layer class melty heads attention self and we call these units.

18
00:01:44.060 --> 00:01:53.030
Here we are the way we will proceed now is we will try to write the code methods and to just write the

19
00:01:53.030 --> 00:01:59.090
whole process the whole architecture of the sub layer and each time we need a new layer or a new viable

20
00:01:59.270 --> 00:02:03.190
we will go back and declare it's in the appropriate method.

21
00:02:03.200 --> 00:02:14.320
So let's define call that takes self queries keys values and the mask that we will want to apply.

22
00:02:15.460 --> 00:02:21.760
And so the first step was to apply a huge linear functions to the queries the keys and the values so

23
00:02:22.240 --> 00:02:29.660
let's write that queries equals self that query line of queries.

24
00:02:29.670 --> 00:02:32.330
Same thing for the keys self.

25
00:02:32.380 --> 00:02:42.120
That's those self that key line applied to keys and values equals self.

26
00:02:42.130 --> 00:02:46.690
That's value linked to values.

27
00:02:46.690 --> 00:02:53.080
So killing will equate really in killing and valuing of three dense layers that we need to declare right

28
00:02:53.110 --> 00:02:53.430
now.

29
00:02:54.160 --> 00:03:03.580
But if you remember well those dense layers they have a number of units which is exactly de model and

30
00:03:04.390 --> 00:03:09.370
we need to have access to this model but we can't have access to them.

31
00:03:09.370 --> 00:03:16.630
Indeed in this init method the reason for that is when we create an objects like Motorhead attention

32
00:03:17.350 --> 00:03:24.520
so we will create our layer at that particular moment the class we will be in all the layer or whatever

33
00:03:24.690 --> 00:03:27.460
they have no clue about to the inputs that it will get.

34
00:03:27.460 --> 00:03:36.430
So it has no clue about the dimension of all vectors whatsoever but there is a way to have access to

35
00:03:36.520 --> 00:03:47.480
those information where when we want to build sub layers of available US and this is the build methods

36
00:03:49.070 --> 00:03:50.630
what this build method does.

37
00:03:52.310 --> 00:03:58.040
Is that it's kind of the same thing as in its but instead of being called when we create the objects

38
00:03:58.340 --> 00:04:02.050
it is called when we use the object for the first time.

39
00:04:02.360 --> 00:04:08.960
So it kind of completes the whole initialization phase but it has access to much more information because

40
00:04:08.960 --> 00:04:11.000
it will have inputs when it is called.

41
00:04:11.600 --> 00:04:18.440
So just remember that built is kind of like initialization but instead of being called when the objects

42
00:04:18.440 --> 00:04:22.360
created it is called the first time the object is used.

43
00:04:22.370 --> 00:04:30.260
So this allows us to have access to more information when we want to declare layers of valuables or

44
00:04:30.350 --> 00:04:31.850
anything like that.

45
00:04:32.510 --> 00:04:39.110
And input shape is something that you can always use in these build methods and basically it's just

46
00:04:39.110 --> 00:04:43.770
a list that conveys the shape of the first inputs for call.

47
00:04:43.790 --> 00:04:47.360
So input shapes will exactly be the shape of queries right.

48
00:04:49.280 --> 00:05:02.030
So we can define now so if that's the model which is inputs shape minus one last dimension of the of

49
00:05:02.030 --> 00:05:16.320
the inputs and self that's really an equals layers that tense units equals self that the model.

50
00:05:17.000 --> 00:05:18.110
Same thing for.

51
00:05:18.560 --> 00:05:27.070
Actually let's be let's be lazy it will pass this one two times

52
00:05:29.790 --> 00:05:32.490
because they are exactly the same type of.

53
00:05:32.520 --> 00:05:41.170
So key and value I think we are good with this.

54
00:05:41.190 --> 00:05:45.630
So now let's go to the next step.

55
00:05:45.630 --> 00:05:53.330
After the after the linear function what we have to do is to split those inputs so queries equals let's

56
00:05:53.340 --> 00:05:57.660
say as we repeat the process three times that we will define a function to do that.

57
00:05:57.690 --> 00:06:07.730
So splits crush 4 projections of queries and we will give him the batch size you will see why later.

58
00:06:08.550 --> 00:06:23.770
And once again I'm going to be lazy and do that do that for the keys and for the values keys and values

59
00:06:24.850 --> 00:06:30.840
so let us now write this very function this method for all notes had.

60
00:06:30.840 --> 00:06:32.140
Attention layer.

61
00:06:32.140 --> 00:06:34.480
Def splits

62
00:06:37.510 --> 00:06:50.190
splits please pros and we need self of course and for any inputs and batch size OK.

63
00:06:50.210 --> 00:06:57.680
The reason we need this batch size is because we will basically reshape the inputs so that it's split

64
00:06:57.680 --> 00:07:04.370
it into safe spaces and so we need to have a batch size so that we have all the information for the

65
00:07:04.370 --> 00:07:11.320
shape we want to get so let's define this shape right now instead of having

66
00:07:14.340 --> 00:07:28.530
inputs of shape batch size sack the length and the model one we want to return is something of shape

67
00:07:28.620 --> 00:07:42.520
batch size the number of projections SEC length once again and the pros so even though this is the last

68
00:07:42.520 --> 00:07:49.900
dimension that we want to split the number of projections just come here because this is kind of like

69
00:07:49.900 --> 00:07:55.540
the same thing as channels when you make any artificial intelligence about images.

70
00:07:55.540 --> 00:08:01.670
When you do computer vision our big matrix that we had before for each sentence is divided into sub

71
00:08:01.670 --> 00:08:06.040
matrices and he we have a way to access each of those mattresses.

72
00:08:06.250 --> 00:08:13.180
So this is why the number of projection comes here but when we will do the reshape it will first be

73
00:08:13.690 --> 00:08:19.330
in this place instead of sequence length and sequence length will be that just because this is this

74
00:08:19.420 --> 00:08:24.400
deep model dimension that we will split into no approach and the approach.

75
00:08:24.400 --> 00:08:34.600
So let's see right now how it works by writing the shape batch size minus 1 will actually be the length

76
00:08:34.660 --> 00:08:46.500
of the sequence so sequence length and we want to have self that's no well no approach.

77
00:08:46.930 --> 00:08:47.560
Times

78
00:08:49.980 --> 00:08:54.480
vectors of size if that's D

79
00:08:57.550 --> 00:09:05.350
approach so this will be the shape we will use for the reshape which does the bigger parts of the splitting

80
00:09:05.350 --> 00:09:10.990
process and then we'll just need a small three hour increments of the dimensions to get exactly the

81
00:09:10.990 --> 00:09:12.400
shape we're all looking for right there.

82
00:09:12.400 --> 00:09:25.970
So let's define splits inputs as to F3 shape inputs and shape equals shape.

83
00:09:27.240 --> 00:09:39.430
So just to be clear what we what we have now is that instead of what we want as a result we have this

84
00:09:39.480 --> 00:09:40.090
shapes

85
00:09:43.140 --> 00:09:47.590
of these shapes so all we have to do is to invert those two dimensions.

86
00:09:47.600 --> 00:09:51.750
And this is exactly what we'll do here in the return.

87
00:09:51.750 --> 00:10:03.480
Instead of returning splitting inputs we return to efforts transpose splits its inputs and we give the

88
00:10:03.480 --> 00:10:05.100
permutation this way.

89
00:10:06.360 --> 00:10:09.480
So we just send it to the transpose methods.

90
00:10:09.510 --> 00:10:10.620
The permutations.

91
00:10:10.630 --> 00:10:11.030
So.

92
00:10:12.360 --> 00:10:18.040
So let's say that the dimension 0 stays at the index 0.

93
00:10:18.040 --> 00:10:19.910
Let's put the dimension number 2.

94
00:10:19.920 --> 00:10:25.310
So the third one in place of the dimension number one which is the second one.

95
00:10:25.740 --> 00:10:33.360
Let's do exactly the opposite right now for the dimension number one going to the instead of the dimension

96
00:10:33.360 --> 00:10:34.940
number two.

97
00:10:34.950 --> 00:10:39.870
And finally we keep the last dimension at its place.

98
00:10:40.350 --> 00:10:46.530
And here we are we have just inverted those two dimensions and we get exactly the results we wanted.

99
00:10:46.530 --> 00:10:54.890
So this is art speed methods if you paid attention we used he no projection and deep throat.

100
00:10:54.900 --> 00:11:04.920
So we have to define those the number of projections was given to the melted attention layer when we

101
00:11:04.920 --> 00:11:11.400
initialized it so we can just defining why that number approach equals then the approach.

102
00:11:11.940 --> 00:11:24.450
And we used the dimension of each subspace for the so let's define its inbuilt because we need the dimension

103
00:11:24.450 --> 00:11:32.730
of the model to do it because the dimension of each projection is equal to dimension of the model divided

104
00:11:32.730 --> 00:11:36.390
by the number of projections so let's do that right now.

105
00:11:36.390 --> 00:11:47.970
First we need to do one small thing which is to assess that self that the model can be divided as into

106
00:11:49.180 --> 00:11:50.420
by self.

107
00:11:50.430 --> 00:11:54.420
That's no approach.

108
00:11:54.530 --> 00:12:00.410
This just means that we want to make sure that when we divide us into goals the model by the number

109
00:12:00.410 --> 00:12:03.110
of projections the rest will be zero.

110
00:12:03.290 --> 00:12:10.400
And these assets words right there just demand to have this condition.

111
00:12:10.460 --> 00:12:14.750
So if it's not the we just stop and we know where it comes from.

112
00:12:14.910 --> 00:12:22.480
And now that we make sure that we can do it let's just say that self that the Droege equals self.

113
00:12:22.490 --> 00:12:27.170
That's the model divided.

114
00:12:27.270 --> 00:12:38.520
So two slashes so that we get an integer as a result by the number of projections okay perfect.

115
00:12:38.540 --> 00:12:44.610
So we are already done because we have been cleaning our functions that have been applied.

116
00:12:44.630 --> 00:12:52.730
We depleted all the Crow's keys and values into safe spaces and now all we have to do is to compute

117
00:12:53.360 --> 00:13:01.950
the attentions for each of those spaces so attention equals scaled that's projects.

118
00:13:02.010 --> 00:13:12.740
Attention applied to queries keys values and the musk that's who we are.

119
00:13:14.510 --> 00:13:20.840
So now we wants to perform the concatenation which is exactly the opposite of what we did in the split

120
00:13:20.840 --> 00:13:27.870
approach and more precisely this is the opposite of this reshape method that we use right there.

121
00:13:28.400 --> 00:13:32.910
So in order to do that we need to invert the dimensions once again.

122
00:13:33.290 --> 00:13:39.680
So we need to put the number of POWs right here in place of the length of the sequence so that we can

123
00:13:39.770 --> 00:13:46.070
match the number of projections with the dimension of the projections and go back to the dimension of

124
00:13:46.100 --> 00:13:46.600
our model.

125
00:13:46.610 --> 00:13:57.350
So let's start by making this either transpose thing to f that's transpose attention

126
00:14:00.730 --> 00:14:01.930
equals.

127
00:14:02.250 --> 00:14:12.560
Same thing as before we invert the dimension number 2 and 1 and we are ready to concatenate all those

128
00:14:12.560 --> 00:14:30.360
attentions equals T F that's reshape attention shape equals batch size minus one and said that the model

129
00:14:32.040 --> 00:14:33.300
lets make it cleaner.

130
00:14:33.870 --> 00:14:38.180
And as we use the batch size he we need to define it.

131
00:14:38.240 --> 00:14:44.000
Right there are actually we had to do it before we just forget because we needed for needed them for

132
00:14:44.000 --> 00:14:44.930
the speed projection.

133
00:14:44.930 --> 00:14:57.580
So let's say that batch size equals T F that shape queries and that's the first dimension that conveys

134
00:14:57.580 --> 00:15:00.850
the matching size.

135
00:15:01.490 --> 00:15:12.530
So now after the concatenation the last step is just to apply the ultimate linear function so that we

136
00:15:12.530 --> 00:15:20.550
will call finally now to come cats attention.

137
00:15:21.110 --> 00:15:25.910
And of course we have to define this one once again in the build method.

138
00:15:25.940 --> 00:15:36.850
So self that's fine now lean equals layers that's dense and the number of units as before is self.

139
00:15:36.860 --> 00:15:40.040
That's the model.

140
00:15:40.040 --> 00:15:41.530
Perfect.

141
00:15:41.600 --> 00:15:46.800
Well it seems like we went through the whole process of the melt had a tension layer.

142
00:15:47.060 --> 00:15:55.070
So we can finally return outputs and be pretty proud of ourselves.

143
00:15:55.070 --> 00:16:02.420
Well here we are merited attention sub layer is ready so we can finally go back to the main architect

144
00:16:02.660 --> 00:16:06.900
of the transformer and start building the encoder and decoder.

145
00:16:06.920 --> 00:16:08.330
This is exactly what we do next.

146
00:16:08.330 --> 00:16:09.220
So seizing.
