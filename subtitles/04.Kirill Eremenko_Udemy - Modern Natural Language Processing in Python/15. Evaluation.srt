1
00:00:00,180 --> 00:00:02,640
Hi guys and welcome back to this tutorial.

2
00:00:02,640 --> 00:00:06,450
We are finally here ready to evaluate options later.

3
00:00:06,450 --> 00:00:10,580
We have build the model we have trained IT AND NOW LET'S SEE HOW IT WORKS.

4
00:00:10,590 --> 00:00:15,960
Indeed we really have to do these spots because it good accuracy during the training doesn't mean that

5
00:00:16,020 --> 00:00:18,030
our translator works well.

6
00:00:18,030 --> 00:00:24,370
Quick story time while I was making these calls I spent almost a week trying to make this evaluation

7
00:00:24,450 --> 00:00:28,140
part work because I just didn't know why.

8
00:00:28,140 --> 00:00:34,140
But although I had great accuracy during training the translator didn't work well.

9
00:00:34,290 --> 00:00:36,110
Whatever the input I gave him.

10
00:00:36,300 --> 00:00:41,680
He just kept giving me a sentence with the same word repeated like 20 times.

11
00:00:41,730 --> 00:00:48,000
And it's just that in the scale that product forgets the minus sign that was not supposed to be the.

12
00:00:48,780 --> 00:00:55,950
And basically what he did is that's it multiplied the mask by zero instead of minus infinity.

13
00:00:55,950 --> 00:01:04,380
So the Look AHEAD mask wasn't applied and the decoder allowed itself to have a look at the future words

14
00:01:04,650 --> 00:01:06,000
instead of guessing them.

15
00:01:06,180 --> 00:01:11,820
So that's just an example of why we really need to test the translator by ourselves.

16
00:01:13,060 --> 00:01:15,160
So let's stop right now.

17
00:01:15,250 --> 00:01:22,690
We will start by creating this this function evaluates which will take us inputs a sentence in the string

18
00:01:22,690 --> 00:01:31,390
format and as output will give the translation but encoded so a sequence of numbers corresponding to

19
00:01:31,390 --> 00:01:33,570
the took a nice version of the translation.

20
00:01:35,140 --> 00:01:47,500
So let's define evaluates which puts which takes input sentence as inputs and that starts by encoding

21
00:01:47,500 --> 00:01:49,580
this sentence.

22
00:01:49,780 --> 00:01:51,550
So input sentence equals

23
00:01:54,480 --> 00:02:03,150
a NISA English doubts and codes in sentence.

24
00:02:03,520 --> 00:02:10,150
And as we have been doing this since the beginning we need to add the starting token and the ending

25
00:02:10,240 --> 00:02:16,150
token so let's adhere vocab size

26
00:02:18,750 --> 00:02:28,400
English minus one minus two so for the start tucking in here.

27
00:02:30,050 --> 00:02:39,640
The cup size English minus one for the ending token last thing we have to do is to simulate the batch

28
00:02:39,640 --> 00:02:41,040
dimensions so.

29
00:02:41,530 --> 00:02:44,020
So let's create Inc.

30
00:02:44,050 --> 00:02:56,950
Inputs equals T F tax expenditures in sentence along the first axis which is the axis that s corresponds

31
00:02:56,950 --> 00:03:01,820
to the batch here we are with our encoder inputs.

32
00:03:01,820 --> 00:03:07,460
Now we have to create the decoder input which will also be the output of the Dakota.

33
00:03:07,490 --> 00:03:14,510
So output which is also the decoder inputs will at the beginning just

34
00:03:17,320 --> 00:03:18,580
just be distorting tucking.

35
00:03:18,580 --> 00:03:32,260
So capsize size French this time minus two just realized that there is a e right here.

36
00:03:34,120 --> 00:03:42,410
And again we want to simulate the botching the budget process here we are.

37
00:03:42,450 --> 00:03:48,030
Now what we want to do is to make several iteration of the transformer because each iteration of the

38
00:03:48,030 --> 00:03:50,700
transformer will give us one new wood.

39
00:03:51,060 --> 00:03:59,100
So as we so if we give the decoder a sentence of length let's say 10 we will get as outputs a sentence

40
00:03:59,100 --> 00:04:01,470
of the same length but it will be shifted.

41
00:04:01,500 --> 00:04:07,950
So the first word which is the starting token will disappear and at the end of the sentence we will

42
00:04:07,950 --> 00:04:10,930
have the next predicted words.

43
00:04:11,040 --> 00:04:17,160
So let's make a loop and we don't want this group to be longer than the max length that we decided before

44
00:04:18,580 --> 00:04:23,590
so the Q all o output sentence would not be long longer than 20 words

45
00:04:26,400 --> 00:04:30,800
we don't need to skip the index here.

46
00:04:30,800 --> 00:04:39,470
So we don't care about that s just make a loop that will not be longer than March 10th and apply our

47
00:04:39,470 --> 00:04:50,560
transformer to encode and call the input Dakota in goods which is exactly the same hour as our goods

48
00:04:52,000 --> 00:05:01,180
its output of course something here and as we all know training right now we say forced to the training

49
00:05:01,180 --> 00:05:01,880
perimeter.

50
00:05:02,200 --> 00:05:04,400
So the job out is not applied here.

51
00:05:04,450 --> 00:05:11,260
Now we have the outputs of the transponder which let's recap the dimensions here.

52
00:05:11,260 --> 00:05:21,290
It's one for the batch size which gives one sake length which is the length of the output so far.

53
00:05:21,890 --> 00:05:33,620
And the last dimension lookup size are for each element of the sentence so far we have a vector of size

54
00:05:33,680 --> 00:05:42,140
vocab size and for a given index in the vocab size the higher the number the higher the probability

55
00:05:42,170 --> 00:05:51,270
that it is the would we are looking for so as we said we just want to take the last words of this predictions.

56
00:05:51,270 --> 00:06:04,310
So projection Q will be predictions that we only keep here the last words of the second dimension which

57
00:06:04,310 --> 00:06:12,980
represents the sequence the sentence basically anyone to keep the whole vector so all the numbers for

58
00:06:13,130 --> 00:06:15,980
each index in our vocabulary.

59
00:06:16,130 --> 00:06:22,160
Now let's get the index of our next words the way we will do that is other said the higher the number

60
00:06:22,670 --> 00:06:24,770
the higher the probability that is on next word.

61
00:06:24,770 --> 00:06:28,490
So the tool we need to use now it's just an odd Max.

62
00:06:28,500 --> 00:06:43,110
So predicted here will be t efforts our max of our prediction applied to the axis minus one because

63
00:06:43,110 --> 00:06:49,320
this is along the last axis that we want to apply these hallmarks Max and as our indices need to be

64
00:06:49,320 --> 00:06:57,720
integers we will cast this t after the costs to make sure this is in the right format.

65
00:06:57,780 --> 00:07:00,560
So in thirty two.

66
00:07:00,780 --> 00:07:01,260
Here we are.

67
00:07:01,290 --> 00:07:02,900
We have the index of our next words.

68
00:07:02,910 --> 00:07:11,370
So all we have to do is to add this new index to our outputs which are also the encoder inputs so outputs

69
00:07:11,730 --> 00:07:20,230
equals T F dot com cuts outputs and predicted i.e.

70
00:07:24,800 --> 00:07:32,920
in these concatenation is made along the axis minus one because outputs huge just the sequence of numbers

71
00:07:32,920 --> 00:07:39,430
that represents our sentence there is just one situation we need to handle is that if all turns Fama

72
00:07:39,430 --> 00:07:47,270
says that the translation is done it will just predicts the next token being the ending sentence stuck

73
00:07:47,270 --> 00:07:47,410
in.

74
00:07:47,440 --> 00:07:59,770
So there is this case that can happen if predicted i.e. these equals to vocab size F R minus 1 which

75
00:07:59,770 --> 00:08:05,530
is the ending token for the French language then it means that the translation is done so we can just

76
00:08:05,530 --> 00:08:11,260
right now return and return outputs but we don't want to return the output like this because we added

77
00:08:11,320 --> 00:08:17,980
a dimension that was supposed to simulate the batch so we will just squeeze these firsts which just

78
00:08:17,980 --> 00:08:24,730
means that we would get rid of the first dimension that doesn't represent anything useful for us outputs

79
00:08:25,120 --> 00:08:27,300
axis equals zero.

80
00:08:27,340 --> 00:08:30,170
So we get rid of the batch dimension.

81
00:08:30,190 --> 00:08:36,080
Ok so here we are our output should be ready to be returned by the function so you just get out of the

82
00:08:36,080 --> 00:08:40,630
soup and we do the same thing here.

83
00:08:40,680 --> 00:08:47,740
Return return to efforts squeeze of outputs axis equals zero.

84
00:08:48,280 --> 00:08:53,350
And here we are are evaluating functions that takes as inputs.

85
00:08:53,350 --> 00:08:58,770
The string sentence and produces the encoding version of its translation.

86
00:09:00,010 --> 00:09:01,180
Okay now we are almost done.

87
00:09:01,180 --> 00:09:04,100
We just need our final wrapper.

88
00:09:04,150 --> 00:09:15,180
If I may say so which will be these function translates takes a sentence as inputs.

89
00:09:15,420 --> 00:09:20,400
This sentence being a string we can divert we apply the alternate function right here so we can get

90
00:09:20,400 --> 00:09:26,010
our outputs as being evaluates of sentence

91
00:09:28,820 --> 00:09:34,070
we want it to be and we all want it to be a 10 so we want it to be an empire array because it's easier

92
00:09:34,070 --> 00:09:40,780
for us later to deal with it's just realized how those brackets we we don't know why.

93
00:09:41,630 --> 00:09:47,900
So we have the outputs but as we said this is an encoded version of the output so we have now to decode

94
00:09:47,900 --> 00:09:48,860
them.

95
00:09:48,890 --> 00:10:02,390
Let's say that all predicted sentence is equal to take in either a French that decode and we don't want

96
00:10:02,390 --> 00:10:06,710
to try to decode all of the numbers is among these numbers.

97
00:10:06,710 --> 00:10:13,880
We will have other tokens like this talking talking so we will not decode the whole sequence.

98
00:10:14,450 --> 00:10:33,380
We will loop over all our numbers for AI for AI inputs but only if I is lower than vocab capsize if

99
00:10:33,400 --> 00:10:35,870
ah minus two.

100
00:10:35,870 --> 00:10:42,470
So now we are sure that we only kept the numbers that corresponds to real words.

101
00:10:45,770 --> 00:10:46,550
And now we are done.

102
00:10:46,560 --> 00:10:54,770
So we just have to find and function with some fancy printing.

103
00:10:54,770 --> 00:10:58,150
Let's remind the user what was the input.

104
00:10:58,250 --> 00:11:11,810
So Prince inputs that formats and let's put the sentence the input sentence right here.

105
00:11:11,810 --> 00:11:20,030
And finally we give the user the translation of the sentence he wanted to get

106
00:11:23,660 --> 00:11:28,040
the formats predicted.

107
00:11:28,440 --> 00:11:33,210
Sentence here out so it should work.

108
00:11:33,290 --> 00:11:38,600
Guess you're ready to try outs this final version of our translator so let's go.

109
00:11:38,630 --> 00:11:39,750
Let's see if it works well.

110
00:11:41,180 --> 00:11:42,590
Let's try to translate.

111
00:11:42,590 --> 00:11:54,810
For instance a simple simple sentence like this is a problem we have to solve.

112
00:11:54,810 --> 00:11:56,240
For instance

113
00:12:00,360 --> 00:12:02,060
is that the same mistake I did before.

114
00:12:02,060 --> 00:12:03,980
Let's hope it's the only one

115
00:12:07,300 --> 00:12:13,260
and of course I did the same thing like.

116
00:12:13,380 --> 00:12:16,240
This is a problem you have to solve predicts in translation.

117
00:12:16,260 --> 00:12:20,730
Set up a device with which is the perfect translation of this one.

118
00:12:20,730 --> 00:12:26,610
Trust me if this is not your native language these words will well let's try another one.

119
00:12:30,050 --> 00:12:32,000
This is really

120
00:12:35,490 --> 00:12:38,290
sorry for what.

121
00:12:38,400 --> 00:12:42,540
For too.

122
00:12:42,600 --> 00:12:49,550
Let's try this one sit on this remote game of salt which is again the perfect translation.

123
00:12:49,560 --> 00:12:50,480
OK so that's it.

124
00:12:50,490 --> 00:12:55,630
We have a fully performing translator made from scratch.

125
00:12:55,750 --> 00:12:56,360
Collect collected.

126
00:12:56,380 --> 00:12:58,350
I was super excited to do this.

127
00:12:58,380 --> 00:13:04,020
I remember a few years ago I was wondering how it was possible and I never thought I would be able to

128
00:13:04,290 --> 00:13:05,530
do my own translator.

129
00:13:06,300 --> 00:13:07,470
But here we are.

130
00:13:07,470 --> 00:13:13,050
Of course it could be much better if you try some complicated sentences.

131
00:13:13,200 --> 00:13:14,850
It might not be accurate.

132
00:13:15,020 --> 00:13:17,730
But first the training phase was quite short.

133
00:13:18,600 --> 00:13:23,230
I think this is something you can train for days and days and it was to improve.

134
00:13:23,220 --> 00:13:32,490
Also are our data sets was maybe it would be light and quite biased because this is only Parliament's

135
00:13:32,520 --> 00:13:36,930
discussions so a lot of aspects of language are missing of course.

136
00:13:36,930 --> 00:13:41,030
So a lot of room for improvements if you want to play with it's just go.

137
00:13:41,030 --> 00:13:43,530
We'll have a lot of fun trying to make it even better.

138
00:13:43,830 --> 00:13:46,990
Hopefully it's have fun with this transformer and see you soon.
