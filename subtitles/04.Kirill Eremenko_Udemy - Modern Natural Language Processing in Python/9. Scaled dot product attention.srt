1
00:00:00,270 --> 00:00:03,510
Hi and welcome back to this course on MLP.

2
00:00:03,540 --> 00:00:08,010
I hope you are ready because we are about to write the most important part of our transformer which

3
00:00:08,010 --> 00:00:10,620
is about the attention mechanism.

4
00:00:10,740 --> 00:00:15,150
Let's just have a quick reminder of how it works with the paper.

5
00:00:16,080 --> 00:00:20,670
So here on the right you can see the general structure of the melt head.

6
00:00:20,680 --> 00:00:28,480
Attention layer that we will build later and here the detailed scale that predicts attention.

7
00:00:28,540 --> 00:00:30,640
That is implemented right here.

8
00:00:30,850 --> 00:00:39,200
If we remember well we have entries queries keys and values that each represents a sentence a sentence

9
00:00:39,200 --> 00:00:40,150
of being a matrix.

10
00:00:40,150 --> 00:00:46,880
Because at this points after the embedding we have the first dimension that corresponds to the sequence.

11
00:00:46,930 --> 00:00:48,570
So the different words.

12
00:00:48,700 --> 00:00:56,430
And the second dimension corresponds to the embedding so here is how these care product works.

13
00:00:56,500 --> 00:00:59,920
I read the I wrote the mathematical formula right here.

14
00:01:00,580 --> 00:01:04,810
So let's start writing the scales that predict function.

15
00:01:04,810 --> 00:01:08,060
So let's define this one.

16
00:01:08,050 --> 00:01:13,960
Def scaled dots projects attention.

17
00:01:14,130 --> 00:01:25,860
We will have as entries the queries the keys the values and the mask Musk can be the Look AHEAD mask

18
00:01:25,890 --> 00:01:32,460
which is the one that doesn't allow the decoder to see the next words all of the padding mask that we

19
00:01:32,460 --> 00:01:40,510
will see later that masks the zeros at the end of each sentence so the first thing we will do is the

20
00:01:40,510 --> 00:01:44,530
products that we see right here in the brackets.

21
00:01:44,530 --> 00:01:54,670
So let's say that product equals T F that's much more queries keys and we don't forget to transpose

22
00:01:56,130 --> 00:02:07,480
Trump sorry transpose B which means that the second matrix will be transposed equals through.

23
00:02:07,540 --> 00:02:14,040
Now what we need before we compute this scale is to get the value of decay.

24
00:02:14,040 --> 00:02:25,160
So we will call this keys team for dimension equals T F that casts T F that shape keys.

25
00:02:25,460 --> 00:02:33,220
So we get the full dimensions of the keys and of course we take the last one because the last dimension

26
00:02:33,230 --> 00:02:35,380
corresponds to the embedding.

27
00:02:35,390 --> 00:02:38,280
And this is what we are looking for.

28
00:02:38,720 --> 00:02:45,030
And we used to have that cast because we want this number to be afloat.

29
00:02:45,170 --> 00:02:56,370
Now we can scale this project so scaled projects equals projects divided by T F that math.

30
00:02:56,370 --> 00:03:05,860
That's as Q all t kissed him now there is the phase that we don't see in the formula but it's quite

31
00:03:05,860 --> 00:03:07,380
clear here in the paper

32
00:03:10,300 --> 00:03:13,900
these masks that they say optional because they don't consider that binding face.

33
00:03:13,900 --> 00:03:16,260
But we will use the mask anyway.

34
00:03:18,310 --> 00:03:24,290
But just to make sure we add this little condition if mask is not known.

35
00:03:24,290 --> 00:03:33,770
So if there is a real mask to apply then the best way to apply this mask is to multiply by minus infinity

36
00:03:33,800 --> 00:03:36,530
because we will apply a soft Max next.

37
00:03:36,770 --> 00:03:44,450
And what a soft Max does is that it puts all the values of a vector between zero and one making sure

38
00:03:44,450 --> 00:03:46,790
that the sum of all those values equal one.

39
00:03:47,000 --> 00:03:54,500
And so before the soft Max if a number is really really low like minus infinity in theory it will become

40
00:03:54,530 --> 00:03:56,450
a zero after the s Max.

41
00:03:56,480 --> 00:04:02,240
And this is why we want to do because we don't want this process to pay attention to the zeros at the

42
00:04:02,240 --> 00:04:02,990
end of the sentence.

43
00:04:03,020 --> 00:04:09,870
So if after these scaled products we have zero it means that it will not have any impact later.

44
00:04:10,310 --> 00:04:16,400
So applying minus infinity right now means after the off Max to have zero which means that there is

45
00:04:16,400 --> 00:04:17,240
no impact later.

46
00:04:17,720 --> 00:04:23,020
So we can't of course say that it will be equal to minus infinity.

47
00:04:23,030 --> 00:04:37,670
But the way we do this is that we just add adds really low numbers so mask times minus ten to the nine

48
00:04:37,670 --> 00:04:39,380
for instance.

49
00:04:39,380 --> 00:04:45,830
This is a really low number and we are sure that after the s Max it will be equal to zero which is what

50
00:04:45,830 --> 00:04:50,640
we are looking for now that we applied the mask.

51
00:04:50,650 --> 00:05:02,360
Let's make the lasts mathematical computation TAF the math more and we want to multiply this results

52
00:05:02,420 --> 00:05:04,100
from the soft Max.

53
00:05:04,100 --> 00:05:14,810
So soft Max scaled projects and we apply the soft Max along the plus axis because when we will then

54
00:05:14,810 --> 00:05:16,790
multiply it by V.

55
00:05:16,850 --> 00:05:19,140
This will simulate many debt products.

56
00:05:19,280 --> 00:05:25,400
And the important thing is that for a given vector V the vector we use in the left right here corresponds

57
00:05:25,400 --> 00:05:28,610
to weights which means we want their sum to be equal to 1.

58
00:05:28,610 --> 00:05:34,610
So that's why we apply this off Max along the last axis like that.

59
00:05:34,670 --> 00:05:43,050
And this is multiplied by the values that we had as inputs who we are.

60
00:05:43,180 --> 00:05:48,230
We have all sketched Doug for the attention function that is ready to be used.

61
00:05:48,340 --> 00:05:50,090
So let's return the results.

62
00:05:50,090 --> 00:05:50,680
Return

63
00:05:53,150 --> 00:05:59,440
attention and that's it for the scale that product attention that is ready to be used for the military

64
00:05:59,440 --> 00:05:59,660
head.

65
00:05:59,660 --> 00:06:00,780
Attention sub layer.

66
00:06:01,010 --> 00:06:02,820
This is exactly the topic of the next session.

67
00:06:02,820 --> 00:06:03,940
So see you then.
