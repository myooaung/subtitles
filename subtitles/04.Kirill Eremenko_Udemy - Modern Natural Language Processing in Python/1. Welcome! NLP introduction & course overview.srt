1
00:00:00,150 --> 00:00:00,560
Hi.

2
00:00:00,690 --> 00:00:05,400
I'm glad that you decided to join me on this course and I hope you are ready to dive into the world

3
00:00:05,430 --> 00:00:08,520
of how computers deal with all human language.

4
00:00:08,520 --> 00:00:15,270
But before that in this quick introduction we will we will first talk about what an IP is and why we

5
00:00:15,270 --> 00:00:16,920
finally need an LP.

6
00:00:16,920 --> 00:00:21,800
Then I will explain personally why I feel that energy is so interesting and fascinating.

7
00:00:21,840 --> 00:00:25,860
And finally we will quickly go through the plan of attack for this whole course.

8
00:00:25,890 --> 00:00:30,240
So first what is it not be so happy stands for natural language processing.

9
00:00:30,330 --> 00:00:32,220
So it speaks by itself.

10
00:00:32,220 --> 00:00:36,970
It just means that is the part of a I the feel of a I that deals with the human language.

11
00:00:37,020 --> 00:00:42,240
So the idea is that we want computers to be able to grasp the meaning which in words that's not obvious

12
00:00:42,240 --> 00:00:45,860
at all because computers only deal with sequences of zeros and ones.

13
00:00:45,900 --> 00:00:51,810
And how do we convey a meaning as complex as it can be sometimes we human interactions with those sequences

14
00:00:51,810 --> 00:00:53,130
of zeros and ones.

15
00:00:53,220 --> 00:00:56,740
But that's not an easy task but nonetheless entropy is still everywhere.

16
00:00:56,820 --> 00:01:02,390
Because of course our language whether it be as a text or as species is everywhere around us.

17
00:01:02,520 --> 00:01:06,950
And you maybe have not noticed it but you actually use energy all the time.

18
00:01:06,990 --> 00:01:13,140
For example Google recently implemented something in the search engine with an IP to improve the accuracy

19
00:01:13,140 --> 00:01:18,020
of their results instead of blindly looking for the results that have the same words.

20
00:01:18,030 --> 00:01:23,070
Daniel inputs research what they do now is that the grass the meaning of research and they try to find

21
00:01:23,210 --> 00:01:28,550
websites that correspond to this meaning or if you are part of those people who talk to their phones.

22
00:01:28,590 --> 00:01:33,870
Of course it's full of energy when you are glad because you don't receive all those spammers in your

23
00:01:33,870 --> 00:01:34,950
main inbox.

24
00:01:34,950 --> 00:01:36,620
Again you can think an LP.

25
00:01:36,660 --> 00:01:41,550
So it's really everywhere and you can think of many other applications like fraud detection in huge

26
00:01:41,550 --> 00:01:44,100
contracts or sentimental analyses.

27
00:01:44,130 --> 00:01:48,750
So it could apply for instance to Automated Moderation charts or in forums.

28
00:01:48,810 --> 00:01:53,970
And of course each time we use a translator It's an LP and all the chat bots that we now see on the

29
00:01:53,970 --> 00:01:57,330
Internet there are of course the result of this research in A.P..

30
00:01:57,720 --> 00:02:02,480
So now I hope that you are convinced that it's a very important and useful field of A.I..

31
00:02:02,540 --> 00:02:07,150
I will tell you two reasons why for me it's one of the most interesting and fascinating fields in the

32
00:02:07,150 --> 00:02:07,550
eye.

33
00:02:07,560 --> 00:02:13,080
The first one is that in LP we are really far from reaching the expectations that we had a few years

34
00:02:13,080 --> 00:02:16,110
ago but that's often the opposite with the other fields of A.I..

35
00:02:16,310 --> 00:02:21,600
If you take the example of a pledge to games there is this famous team in Google which is Google deep

36
00:02:21,600 --> 00:02:23,540
mind that achieves great results.

37
00:02:23,540 --> 00:02:28,830
A few years ago with the game of Go When machines were finally able to be better than humans while the

38
00:02:28,830 --> 00:02:32,380
community thought that we will need at least 10 years to achieve that.

39
00:02:32,420 --> 00:02:34,540
So we went really faster than expected.

40
00:02:34,680 --> 00:02:38,470
But that's not how it works in LP few years a few decades ago.

41
00:02:38,550 --> 00:02:43,560
People thought that we would have computers that would be able to talk like humans very soon.

42
00:02:43,650 --> 00:02:45,000
But we are still far from it.

43
00:02:45,180 --> 00:02:49,470
We realize that it's way more complicated than expected to deal with language AI.

44
00:02:50,190 --> 00:02:55,290
But that brings us to the second reason why they'll be so fascinating is that as we still have a lot

45
00:02:55,290 --> 00:03:00,930
of work to do it also means that we have a lot of things to discover and that itchy that new fascinating

46
00:03:00,990 --> 00:03:03,840
discoveries that appear in the research woods.

47
00:03:03,840 --> 00:03:08,510
So that's also exciting to follow this kind of adventure in this energy research.

48
00:03:08,870 --> 00:03:13,420
It's not that we can't wait to dive into it let's have a quick look at how we will do that.

49
00:03:13,500 --> 00:03:18,740
The course will be separated in two main parts corresponding to the two main types of tasks that exist

50
00:03:18,740 --> 00:03:19,440
in LP.

51
00:03:19,470 --> 00:03:25,590
The first one being classification tasks so it's spam detection all sentimental on the license for detections

52
00:03:25,620 --> 00:03:29,420
and many others and the seven part being sequence to sequence tasks.

53
00:03:29,580 --> 00:03:35,940
So it can be cancellations text memorization chat bots well anything that has a whole text as inputs

54
00:03:36,030 --> 00:03:38,660
and outputs while in the classification task.

55
00:03:38,730 --> 00:03:44,850
We have a text as inputs but we will have labels as outputs the first part will be about a classification

56
00:03:44,850 --> 00:03:45,170
task.

57
00:03:45,180 --> 00:03:48,090
And for that we will apply as CNN to it.

58
00:03:48,210 --> 00:03:50,720
So it's an instance for conversion on the will networks.

59
00:03:50,790 --> 00:03:56,760
And of course it was first used for compute divisions so to analyze images but actually it could also

60
00:03:56,760 --> 00:03:59,840
perform really well in an LP for the classification tasks.

61
00:03:59,850 --> 00:04:03,610
So that's what we will use to tackle those kind of problems.

62
00:04:03,700 --> 00:04:09,810
And in the first box we will go into the CNN intuition talking first about the origins of CNN so how

63
00:04:09,810 --> 00:04:15,330
they were applied to images to then make the link between image and text to see how we can apply it

64
00:04:15,330 --> 00:04:21,300
for an LP and then we will go for the applications which in our case will be a sentimental analyzer.

65
00:04:21,540 --> 00:04:27,330
So we will use a lot of tweets that has been labeled positive or negative using the images that were

66
00:04:27,330 --> 00:04:27,940
into it.

67
00:04:28,140 --> 00:04:33,870
And we will create our sentiments out in that saw out of its but of course the idea of the CNN architecture

68
00:04:33,990 --> 00:04:36,150
is really similar for any classification tasks.

69
00:04:36,180 --> 00:04:42,210
So it will fit any other follows with labels as outputs and in the second part we'll see what we call

70
00:04:42,210 --> 00:04:42,930
the transformer.

71
00:04:43,140 --> 00:04:48,060
So it's a recent model recent architecture that has been presented by Google and I'm really excited

72
00:04:48,060 --> 00:04:53,370
to present to this one because for me it's so interesting the way it works and is so so powerful you

73
00:04:53,370 --> 00:04:55,390
will see that's in the application phase.

74
00:04:55,530 --> 00:04:59,550
So same thing that for the CNN we would first go into the intuition.

75
00:04:59,550 --> 00:05:05,130
So I would expect the origins of this one where it comes from and in details how it's implemented how

76
00:05:05,130 --> 00:05:08,840
it works and what are the mathematical ideas behind.

77
00:05:09,090 --> 00:05:11,080
Then of course we will go for the application.

78
00:05:11,160 --> 00:05:11,950
And guess what.

79
00:05:11,970 --> 00:05:14,930
You will be able to build your own translator in a certain way.

80
00:05:14,930 --> 00:05:19,440
I can't believe that I'm saying that because not so long ago I was sure that it was impossible for me

81
00:05:19,440 --> 00:05:20,380
to build a translator.

82
00:05:20,380 --> 00:05:25,920
I thought that was something that only Google or big firms could do but you will see that we can get

83
00:05:25,920 --> 00:05:30,030
great results with AIDS and you will have your own translator that you can that you can beat for any

84
00:05:30,030 --> 00:05:31,170
language of course.

85
00:05:31,170 --> 00:05:36,870
If you have the appropriate data and sending that for CNN these architects are the transformer is very

86
00:05:36,870 --> 00:05:42,310
general so of course we will build a translator but you can use it for any sequence to sequence dusk

87
00:05:42,340 --> 00:05:43,940
so it will work perfectly fine.

88
00:05:43,950 --> 00:05:46,380
If you want to build a chatterbox for instance.

89
00:05:46,380 --> 00:05:48,260
So that was it for all plan of attack.

90
00:05:48,270 --> 00:05:49,920
What we will go through this course.

91
00:05:49,980 --> 00:05:52,410
I hope you are super excited to go into it with me.

92
00:05:52,560 --> 00:05:54,360
So let's not wait any longer and see you soon.
