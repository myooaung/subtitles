1
00:00:00,120 --> 00:00:02,130
Hi and welcome back to this email because.

2
00:00:02,280 --> 00:00:08,280
So now that you have seen how CNN is applied to images for comfort division let us see how we can apply

3
00:00:08,280 --> 00:00:14,400
them to texts for an AP and so the first part will be to see how we can transform a text so that it's

4
00:00:14,400 --> 00:00:15,900
a valid input for CNN.

5
00:00:16,380 --> 00:00:19,670
So the idea was to say OK so CNN for images.

6
00:00:19,680 --> 00:00:20,600
What does it do.

7
00:00:20,730 --> 00:00:23,100
It search for local features in an image.

8
00:00:23,100 --> 00:00:29,220
So why don't we want to process text or sentences the same way by looking for local features throughout

9
00:00:29,220 --> 00:00:30,030
the whole sentence.

10
00:00:31,380 --> 00:00:36,780
But in order to do that we have to have the same kind of inputs as images which were mattresses.

11
00:00:36,780 --> 00:00:39,550
So we wanted to get a matrix out of a sentence.

12
00:00:39,570 --> 00:00:44,250
We want to represent the sentence as a matrix because for now it's just a list of characters or a list

13
00:00:44,250 --> 00:00:47,400
of words but it doesn't really look like a metric so far.

14
00:00:47,670 --> 00:00:50,770
So what we would like to have is something like this.

15
00:00:50,850 --> 00:00:56,910
And the most intuitive way is to say that if we have a matrix each row will correspond to the words

16
00:00:57,300 --> 00:01:00,940
and each column New York corresponds to whatever we will find out later.

17
00:01:00,990 --> 00:01:04,860
But this representation means that its words will be a vector.

18
00:01:05,070 --> 00:01:11,800
So let's work around that the easier and maybe more natural way to represent a word with a vector is

19
00:01:11,800 --> 00:01:13,340
also the less effective one.

20
00:01:13,450 --> 00:01:15,520
It is what we call the 1 plot encoding.

21
00:01:15,550 --> 00:01:21,100
So basically if we say that all vocabulary has one hundred thousands words each word would be a vector

22
00:01:21,370 --> 00:01:24,340
of that size and it will be mostly zeros.

23
00:01:24,340 --> 00:01:26,090
It will only have one one.

24
00:01:26,200 --> 00:01:32,790
And for example for Doug if we say that Doug is wood number two thirty eight will add the index to thirty

25
00:01:32,800 --> 00:01:37,840
eight of all Victor we will see a one and all the other numbers will be zero.

26
00:01:37,840 --> 00:01:43,480
So that way we have a unique representation for each of all one hundred thousand words but there is

27
00:01:43,480 --> 00:01:46,350
absolutely no relation between each of them.

28
00:01:46,540 --> 00:01:51,610
There is no mathematical relation that appears there is no meaning that is conveyed to it.

29
00:01:51,640 --> 00:01:54,660
It shows how little meaning and information it conveys.

30
00:01:54,700 --> 00:02:00,250
Well it conveys information about which word we are dealing with but nothing about the relations between

31
00:02:00,250 --> 00:02:01,370
those words.

32
00:02:01,390 --> 00:02:05,420
So what we would like to have is a smaller representation of each words.

33
00:02:05,470 --> 00:02:12,100
So instead of having a vector of size one hundred thousands we would have a vector of smaller size let's

34
00:02:12,100 --> 00:02:14,130
say for instance sixty four.

35
00:02:14,170 --> 00:02:16,330
So that's a common thing in science.

36
00:02:16,330 --> 00:02:17,950
We want to make the vector smaller.

37
00:02:17,950 --> 00:02:21,940
So that means that we add more constraints to our information in a certain way.

38
00:02:21,940 --> 00:02:29,110
It has less liberty and that forces all system to create relations links or even meaning in the process

39
00:02:29,170 --> 00:02:31,540
if we do it the right way of course.

40
00:02:31,540 --> 00:02:37,330
So this time Doug for instance instead of being a vector of size one hundred thousands we have all zeros

41
00:02:37,330 --> 00:02:42,160
but one it would be a vector of size 64 and it won't be binary anymore.

42
00:02:42,190 --> 00:02:44,530
It would be numbers between 0 and 1.

43
00:02:44,530 --> 00:02:50,860
So before going into mathematical details or to see how we do that most of the time we would just have

44
00:02:50,860 --> 00:02:56,460
a look at some cool visual representations to see what it does and what it performs.

45
00:02:56,470 --> 00:03:03,310
So the first one is this quite well-known effects of the what embedding now that they are embedded in

46
00:03:03,310 --> 00:03:05,690
a smaller vector space.

47
00:03:05,890 --> 00:03:11,230
We have some mathematical relations which in the woods as vectors and we can actually send the vectors

48
00:03:11,530 --> 00:03:18,310
and summing vectors means summing meaning in all embedded space so we can actually get some very cool

49
00:03:18,310 --> 00:03:25,330
stuff like if we take the word King for instance and that we say King minus man plus woman we actually

50
00:03:25,330 --> 00:03:26,570
get the word queen.

51
00:03:26,620 --> 00:03:31,930
So if the embedding has been done properly we can actually subtract the parts of the meaning of the

52
00:03:31,930 --> 00:03:33,440
words and add another one.

53
00:03:33,490 --> 00:03:40,660
Like for King we can absolutely get rid of the male parts of King and add the female parts to this result

54
00:03:40,720 --> 00:03:41,890
in order to get Queen.

55
00:03:42,010 --> 00:03:47,080
And another example very simple is Paris minus France plus Italia.

56
00:03:47,200 --> 00:03:51,520
We get from so with Paris minus France we get the capital.

57
00:03:51,790 --> 00:03:55,300
And if we add Italia then we get the capital of Italy which is Rome.

58
00:03:55,510 --> 00:04:01,510
So that means that we are now able to apply mathematics to our words to all vectors and that is very

59
00:04:01,510 --> 00:04:03,610
very useful in the sense of course.

60
00:04:03,610 --> 00:04:05,040
So very good points.

61
00:04:05,050 --> 00:04:10,120
Second visual I.D. You may have to get a little bit closer to your screen.

62
00:04:10,230 --> 00:04:15,760
But this is a two dimensional representation of some words that have been embedded and the important

63
00:04:15,760 --> 00:04:22,130
thing here is to realize that words that have similar meanings will be very close in the embedded space.

64
00:04:22,150 --> 00:04:29,440
So for instance if you have a look at some words here we can see that's kingdom will be close to states

65
00:04:29,720 --> 00:04:36,190
or that data will be close to information also there is relations between the tenses of the verbs here

66
00:04:36,190 --> 00:04:43,090
we can see that there is become that is close to became close to being also close to was final you can

67
00:04:43,090 --> 00:04:46,980
see that he is very close to she which is close to it also.

68
00:04:47,230 --> 00:04:54,040
So by embedding vectors into a smaller dimensional vector space we added constraints as I said before

69
00:04:54,280 --> 00:05:01,090
and it forced of course if we do the embedding properly it forces the positions of vectors all the values

70
00:05:01,090 --> 00:05:07,090
of all vectors to convey meaning and so to make words of similar meaning close in all embedding space.

71
00:05:07,090 --> 00:05:07,730
So that's cool.

72
00:05:07,750 --> 00:05:09,700
We get a vector representation of.

73
00:05:09,970 --> 00:05:15,850
So that will allow us to have mattresses out of sentences and it seems to be a very powerful tool because

74
00:05:15,850 --> 00:05:19,870
it conveys meaning we can do mathematical stuff with it's so awesome.

75
00:05:19,990 --> 00:05:26,470
But how does it work exactly like the process the embedding process mathematically what do we do exactly.

76
00:05:26,620 --> 00:05:32,460
The idea is that with an input vector as a one called encoded vector as we said before.

77
00:05:32,500 --> 00:05:38,530
So all you want but only one one for each what we want to multiply it by a matrix and then building

78
00:05:38,530 --> 00:05:41,870
matrix and gets all embedded vector right.

79
00:05:41,890 --> 00:05:48,470
Actually the fact that we multiply a 1 hardcoded vector by a matrix it just means that if we have for

80
00:05:48,470 --> 00:05:54,580
instance the words I so there is only X eye which is activated that means that we just take the row

81
00:05:54,920 --> 00:05:55,720
and all matrix.

82
00:05:55,750 --> 00:06:02,280
So this matrix is just a list of all the embedded vectors all of the embittered words but that's a detail.

83
00:06:02,290 --> 00:06:04,710
So we want to multiply and you won hearts and got it.

84
00:06:04,710 --> 00:06:09,940
Victor why this match mix and how Victor but the question is how do we train these metrics.

85
00:06:09,940 --> 00:06:11,290
How do we train the weights.

86
00:06:11,290 --> 00:06:13,630
How do we learn the embedded vectors.

87
00:06:13,630 --> 00:06:18,490
We can't of course say that we want from these to get back to the original one because we will just

88
00:06:18,490 --> 00:06:23,040
have to apply the opposites cooperation just to see which way it corresponds to.

89
00:06:23,170 --> 00:06:24,900
And so get the this right here.

90
00:06:25,060 --> 00:06:31,070
So the idea that we want to multiply by another matrix which we call the context matrix you'll see why.

91
00:06:31,450 --> 00:06:38,260
And gets again a vector of size vocab size but we want to get something else that the original input

92
00:06:38,260 --> 00:06:44,360
vector but we still want it to have a meaning to have a coalition with the original words.

93
00:06:44,470 --> 00:06:46,180
We want these outputs.

94
00:06:46,270 --> 00:06:49,870
This vector to have a semantic coalition with this one.

95
00:06:49,900 --> 00:06:53,800
So one idea to do that is to use what we call contexts.

96
00:06:53,800 --> 00:07:00,520
So this is the keep grab model where it basically for an input words we will pick several other words

97
00:07:00,550 --> 00:07:06,020
which we'll call contexts and we want them to appear in this output vector.

98
00:07:06,100 --> 00:07:13,240
So let's say that's word number one corresponds to context words as Number 10 20 and 30 for instance

99
00:07:13,540 --> 00:07:14,730
at the end of this operation.

100
00:07:14,740 --> 00:07:21,160
So after embedding the the word number one and after retrieving information with the context matrix

101
00:07:21,550 --> 00:07:29,140
we want he to have many zeros but we want to have a high number in position 10 20 and 30 which are the

102
00:07:29,230 --> 00:07:34,590
indices that corresponds to the context would that be defined before for the Woods number one.

103
00:07:34,600 --> 00:07:40,810
So in order to get those context words we actually take a huge corpus of texts and for each words we

104
00:07:40,810 --> 00:07:45,850
will take for instance the two previous words and the two next words as context.

105
00:07:45,850 --> 00:07:51,970
So in this sentence in spite of everything I still believe people are really good at Hearts which comes

106
00:07:51,970 --> 00:07:53,790
from the diary of Anne Frank.

107
00:07:53,890 --> 00:07:59,590
If we take the word good it's surrounded by the words are really hot and hot.

108
00:07:59,590 --> 00:08:04,960
So this will produce four pairs of inputs and contexts which will be good.

109
00:08:04,960 --> 00:08:08,540
Are good really good arts and good hearts.

110
00:08:08,590 --> 00:08:15,670
So when we put the words good as inputs so there will be a one only in the position that corresponds

111
00:08:15,670 --> 00:08:16,490
to this word good.

112
00:08:16,840 --> 00:08:23,320
And after embedding it after applying it by the context matrix we want to have the indices corresponding

113
00:08:23,320 --> 00:08:30,010
to are really arts and hot activated of course as good will appear many times in our caucus we'll have

114
00:08:30,070 --> 00:08:32,540
a lot more context vectors as those ones.

115
00:08:32,710 --> 00:08:38,920
But the idea is that it's time we use a pair of input context words we will change those mattresses

116
00:08:39,130 --> 00:08:46,840
the w w prime we will tune the coefficients in order to activate the context words out of the input

117
00:08:46,840 --> 00:08:54,820
words so the global idea about dots was from a single words in one hot unguided version get a smaller

118
00:08:54,820 --> 00:09:02,320
vector and from this smaller vector get several words or activate several words that are often close

119
00:09:02,380 --> 00:09:04,750
to the initial words in our corpus.

120
00:09:04,840 --> 00:09:10,390
So that's pretty clever because we have all embedding face right here but it must convey meaning it

121
00:09:10,390 --> 00:09:17,840
must be semantically efficient in order to retrieve the context information after this operation and

122
00:09:17,870 --> 00:09:23,140
of course this spots was only used for training in order to have something that has meaning.

123
00:09:23,270 --> 00:09:25,240
That's when we embeds a vector.

124
00:09:25,250 --> 00:09:26,380
We only use these spots.

125
00:09:26,390 --> 00:09:28,220
This is the one that we are interested in.

126
00:09:28,280 --> 00:09:36,010
In the end importance an interesting thing to notice is that if two words have a close meaning this

127
00:09:36,010 --> 00:09:40,010
means that they probably in all copies will have similar contexts.

128
00:09:40,010 --> 00:09:43,130
And that also means that they will have a close embedding.

129
00:09:43,150 --> 00:09:49,120
So if we take this sentence again we could argue that the words people could be replaced with humans

130
00:09:49,180 --> 00:09:49,740
for instance.

131
00:09:49,750 --> 00:09:55,810
So as to believe humans are really good at Hearts and that makes sense because humans and people they

132
00:09:55,810 --> 00:09:57,310
convey a similar meaning.

133
00:09:57,340 --> 00:10:03,100
So in all copies we could expect humans and people to usually be surrounded by the same words.

134
00:10:03,100 --> 00:10:11,450
So that means that after we embed the words people all the words humans we get to different imbedded

135
00:10:11,470 --> 00:10:17,310
vectors and within words but when we apply the context metrics we want to get very close results.

136
00:10:17,310 --> 00:10:19,200
Actually pretty much the same results.

137
00:10:19,210 --> 00:10:24,580
So that means that the embedded versions of those two words have to be close and that's where it comes

138
00:10:24,580 --> 00:10:25,990
from what we saw earlier.

139
00:10:26,020 --> 00:10:30,730
That's two words of similar meaning we'll be close in all embedded space.

140
00:10:30,730 --> 00:10:37,030
So that was it for the global idea of what embedding which will allow us to get efficient vectors from

141
00:10:37,030 --> 00:10:40,330
words and so to get a matrix out of a sentence.

142
00:10:40,330 --> 00:10:46,120
If we just want to summarize this what embedding phase using this Skip grab module is that it finds

143
00:10:46,120 --> 00:10:51,790
that dimension reduction in vector representation affords while adding a semantic relation between them

144
00:10:52,060 --> 00:10:57,460
and I would add that this relation is mathematical so that we can use it which turned out operations

145
00:10:57,520 --> 00:10:59,200
in artificial intelligence.

146
00:10:59,330 --> 00:11:05,520
So that was it for the very first step of seeing and applied to entropy which is how to get value inputs

147
00:11:05,530 --> 00:11:07,170
for our CNN of course.

148
00:11:07,330 --> 00:11:13,540
And now we're ready to feed them to sentence and to see what happens in our module and what we will

149
00:11:13,540 --> 00:11:17,780
have to change in order to have properly working CNN full texts.

150
00:11:17,800 --> 00:11:20,140
So that's the topic of the next pots and see you soon.
