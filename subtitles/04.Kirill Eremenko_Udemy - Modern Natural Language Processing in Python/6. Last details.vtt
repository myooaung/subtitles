WEBVTT
1
00:00:00.180 --> 00:00:01.080
Hi and welcome back.

2
00:00:01.080 --> 00:00:07.450
This NFP that because so we are now the last step of the intuition explanation of the transformer.

3
00:00:07.530 --> 00:00:12.050
We'll just see the last details in the architecture that we will need to implement.

4
00:00:12.270 --> 00:00:18.690
The first one are the feet forward layers that we can see here at the end of each and coding sub layer

5
00:00:18.780 --> 00:00:20.220
and decoding sub layer.

6
00:00:20.700 --> 00:00:22.330
So nothing amazing about that.

7
00:00:22.440 --> 00:00:24.860
It's just a standard dense layer.

8
00:00:25.100 --> 00:00:31.200
Marcus is way too dense layers that we use in order to learn more and to allow our model to learn things

9
00:00:31.200 --> 00:00:32.140
by itself.

10
00:00:32.190 --> 00:00:37.930
So each feet forward sub layer is made of two linear transformations so two dense layers.

11
00:00:37.980 --> 00:00:39.650
This is what we can see right here.

12
00:00:39.810 --> 00:00:45.720
So here there is the first linear transformation so X is the inputs of the feet forward.

13
00:00:45.720 --> 00:00:48.930
We multiplied by a matrix and we are the bias.

14
00:00:49.080 --> 00:00:53.750
Then these marks of zero and the result is what we call a real activation.

15
00:00:53.760 --> 00:00:59.870
So at the end of a dense layer we can add a small nonlinear function and usually real you is used.

16
00:00:59.910 --> 00:01:04.230
It's just a function that removes all the negative values of our dense layer.

17
00:01:04.650 --> 00:01:08.670
And the result of that is used to compute the second dense layer.

18
00:01:08.790 --> 00:01:11.380
So we have here a second matrix to apply.

19
00:01:11.490 --> 00:01:16.510
And a second bias that we add and the output of all this will have the same shape.

20
00:01:16.510 --> 00:01:21.270
So again we will have a sequence of the same length and the same embedding dimension.

21
00:01:21.330 --> 00:01:22.800
So a few details.

22
00:01:22.810 --> 00:01:28.370
First it says that it's applied to each position separately and identically.

23
00:01:28.440 --> 00:01:31.590
What it means is that for a single sequence.

24
00:01:31.590 --> 00:01:36.650
We will apply those to linear transformations to each of the elements.

25
00:01:36.660 --> 00:01:39.450
So each of the words if I may say so.

26
00:01:39.510 --> 00:01:45.720
It's not was as I said before because we have computed some attention mechanism to it but you get me.

27
00:01:45.720 --> 00:01:51.690
So for each of the elements of the sequence we will apply this linear transformation so each elements

28
00:01:51.780 --> 00:01:58.740
as a reminder is a vector of size D models so the dimension of the embedding so X right.

29
00:01:58.740 --> 00:02:04.990
He represents the words and elements and we apply the same dance y as the assembly notch information

30
00:02:05.220 --> 00:02:06.800
to each of the elements.

31
00:02:06.810 --> 00:02:13.860
So each of the position of sequence and the second point is that this fit forward Sibila is different

32
00:02:13.860 --> 00:02:15.380
for each Sibila.

33
00:02:15.570 --> 00:02:23.910
So for each of the eights and causing layers of each of those eights decoding layers we apply a different

34
00:02:23.910 --> 00:02:24.470
foot forward.

35
00:02:24.500 --> 00:02:27.160
So actually we have eight times two.

36
00:02:27.160 --> 00:02:33.690
So 16 different feet forwards the blaze that all model has to learn which makes 32 dense layers because

37
00:02:33.750 --> 00:02:37.660
each feet forward sub layers is made of two dense lies.

38
00:02:37.770 --> 00:02:39.780
So that's it's very simple.

39
00:02:39.840 --> 00:02:46.440
At the end of each decoding or encoding sub layers we order really standouts feet forwards neural network

40
00:02:46.510 --> 00:02:53.280
in order to learn more second detail is that you can see that after each sub layer that can be the attention

41
00:02:53.280 --> 00:02:58.500
suddenly of the feet forward so layer you have this add a norm and this area right here.

42
00:02:58.740 --> 00:03:06.010
So what it means is that the outputs of those suppliers are actually added to the inputs of the sub

43
00:03:06.010 --> 00:03:09.210
layers and then we just do a standard normalization.

44
00:03:09.570 --> 00:03:11.980
So there are two reasons why we do that.

45
00:03:12.090 --> 00:03:17.460
The first one is that of course we have computed new things that gives us new information about all

46
00:03:17.460 --> 00:03:20.370
data but we don't want to forget the way it was before.

47
00:03:20.400 --> 00:03:25.770
For instance in the self attention mechanism after every composing a sentence with regards to the relations

48
00:03:25.770 --> 00:03:31.680
between each woods we wants to remember at his debates how the sentence was before so it could be really

49
00:03:31.680 --> 00:03:36.990
useful to add the original sentence to the output of this stuff attention symbol.

50
00:03:37.270 --> 00:03:42.690
And the second reason why we do that is that it's actually a common thing to do in artificial intelligence.

51
00:03:42.690 --> 00:03:48.420
This is called residual connections and it makes learning easier more precisely the back propagation

52
00:03:48.810 --> 00:03:54.870
as a reminder how it works to train and you will network is that we have a look at how our outputs is

53
00:03:54.900 --> 00:04:00.210
close to the targets we want to reach and according to the difference between our output and the real

54
00:04:00.210 --> 00:04:06.350
targets we can know in which direction we want to change each valuable or each weights in our network

55
00:04:06.480 --> 00:04:12.090
using some grazing methods but it's actually way easier to know how we need to change the weights at

56
00:04:12.090 --> 00:04:18.660
the end of model that's at the very beginning if we know that's the output is different from what we

57
00:04:18.660 --> 00:04:25.770
wanted to get it's easier to see how changing the last lean out layer for instance will affect the outputs

58
00:04:25.980 --> 00:04:32.700
but it's way way harder to know how changing for instance the way we embeds all words are the way we

59
00:04:32.700 --> 00:04:38.130
compute these attention at the very beginning will affect 0 outputs because there are a lot of mathematical

60
00:04:38.130 --> 00:04:40.220
operations in the way.

61
00:04:40.220 --> 00:04:47.510
So doing this residual connection makes it easier for the back propagation face to have access to the

62
00:04:47.520 --> 00:04:48.900
produce steps.

63
00:04:48.900 --> 00:04:54.390
So here for instance we have access to the outputs of these attentions sub layer.

64
00:04:54.690 --> 00:04:59.000
So we have access to how we train those weights right.

65
00:04:58.990 --> 00:05:06.230
He the all connections so we don't have to go through the whole dense layers right here in order to

66
00:05:06.240 --> 00:05:09.340
have access to how this output was computed.

67
00:05:09.360 --> 00:05:15.420
So that's also a good way to improve the learning phase and other elements that they mentioned in the

68
00:05:15.420 --> 00:05:18.070
paper that is quite important is the drop outs.

69
00:05:18.090 --> 00:05:23.730
So the drop outs is just a tool a small layer that we can add in a network that will shut down some

70
00:05:23.730 --> 00:05:24.420
units.

71
00:05:24.420 --> 00:05:25.380
So by shutting down.

72
00:05:25.380 --> 00:05:27.410
I mean that's doing a training phase.

73
00:05:27.420 --> 00:05:28.860
We will not use those new ones.

74
00:05:28.890 --> 00:05:32.950
So those valuables waits to computes the outputs.

75
00:05:33.030 --> 00:05:35.660
And of course we will not use them in the back propagation.

76
00:05:35.670 --> 00:05:37.580
So we've been that's trained them.

77
00:05:37.620 --> 00:05:42.810
This is something that we only apply in the training phase you know to improve the performance of our

78
00:05:42.810 --> 00:05:43.380
model.

79
00:05:43.620 --> 00:05:44.410
So we don't apply.

80
00:05:44.410 --> 00:05:49.500
It's when we use our model after it is trained because then we want to use everything we have learned

81
00:05:49.650 --> 00:05:55.710
the whole possibilities of Oh I usually we should down approximately 10 percent or 20 percent off on

82
00:05:55.710 --> 00:05:56.330
humans.

83
00:05:56.400 --> 00:05:59.580
And of course these are different ones of each training step.

84
00:05:59.790 --> 00:06:06.330
And the idea behind that is that by turning off certain units we force or AI to have good results to

85
00:06:06.330 --> 00:06:09.870
compute good outputs with a smaller part of its capacities.

86
00:06:09.870 --> 00:06:12.570
And that helps him keeping some kind of a generality.

87
00:06:12.750 --> 00:06:18.090
Oh I will always be able to perform well even if it can't use all its news and the consequence of that

88
00:06:18.180 --> 00:06:23.760
is that it will prevents the fitting of a fitting being Oh I am getting too close to our training sets.

89
00:06:23.760 --> 00:06:28.320
And so when an AI or face it means that it will perform well on a train sets.

90
00:06:28.320 --> 00:06:32.880
But it would perform poorly on the training sets because it's new data that it has never seen.

91
00:06:32.880 --> 00:06:37.500
So the playing your out helps keeping some kind of Virginia quality in training and in the paper to

92
00:06:37.500 --> 00:06:42.360
say that they oblige you about between each sub layer and the other no buts.

93
00:06:42.510 --> 00:06:48.300
So this means one drop out here another one here here here and here.

94
00:06:48.750 --> 00:06:54.090
And also they apply a positional dropouts right after the positional encoding so just before entering

95
00:06:54.090 --> 00:06:57.290
in those and cutting layers and decoding layers.

96
00:06:57.330 --> 00:07:02.490
So we also have a job of layer that we will implement here and there.

97
00:07:02.490 --> 00:07:08.850
And finally the last detail of the architecture is this simple linear function Y Hey it's just again

98
00:07:08.880 --> 00:07:13.920
a simple identifier that's the odds at the end of the whole encoding and decoding phase.

99
00:07:13.920 --> 00:07:19.890
It's applied independently to each elements of the sequence once again and the number of units.

100
00:07:19.920 --> 00:07:26.760
So the the output dimension of this dense layer will be the book upsize of all target language for instance

101
00:07:26.760 --> 00:07:33.090
right from English to French the dimension of the outputs to the number of units of all dense layer

102
00:07:33.180 --> 00:07:36.870
will be the number of words that we have in our French vocabulary.

103
00:07:36.870 --> 00:07:42.690
So that's after a soft Max we get actual probabilities for each word of our dictionary.

104
00:07:42.780 --> 00:07:49.320
So we just for each element gets the word that has the higher probability and we gets a sequence or

105
00:07:49.320 --> 00:07:52.090
a sentence that should be the output of transform.

106
00:07:52.560 --> 00:07:57.660
So that's it we have been through all of the steps all the small sub layers of the transformer.

107
00:07:57.990 --> 00:08:04.170
Hopefully life learning more about how it works and more especially about the attention mechanism and

108
00:08:04.270 --> 00:08:09.780
now that we have the information about this model I think we are ready to start implementing it and

109
00:08:09.780 --> 00:08:11.550
to build all on translator.

110
00:08:11.600 --> 00:08:12.030
See you soon.
