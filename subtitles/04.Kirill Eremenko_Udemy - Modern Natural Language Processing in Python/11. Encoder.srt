1
00:00:00,300 --> 00:00:05,770
Hi and welcome back to this e-mail because now that we have implemented all the attention mechanism

2
00:00:05,800 --> 00:00:11,240
we are ready to create office big parts of the transformer which will be the encoder.

3
00:00:11,370 --> 00:00:15,010
Again we'll have a quick look at the paper right here.

4
00:00:15,840 --> 00:00:19,640
So let's see this architecture.

5
00:00:19,650 --> 00:00:19,990
Hey.

6
00:00:20,190 --> 00:00:22,980
This is the block we will focus right now.

7
00:00:22,980 --> 00:00:28,680
So we will call and then call the layer just this small parts right here.

8
00:00:28,920 --> 00:00:36,260
While small actually it's the most important one but this sequence of operations will be called and

9
00:00:36,270 --> 00:00:37,180
called a layer.

10
00:00:37,290 --> 00:00:43,710
And we will have in order to build our encoder to add the embedding the positional encoding and as many

11
00:00:43,710 --> 00:00:46,430
times as we need the and causal layer.

12
00:00:46,710 --> 00:00:53,910
So a quick reminder why we need to do is to first apply a motorhead attention right here then to add

13
00:00:53,910 --> 00:01:01,650
the results to the inputs and to the playing on and then just apply some regular linear functions using

14
00:01:01,650 --> 00:01:09,460
the dense layer and again use these small adds and known sequence at the end you will see some drop

15
00:01:09,460 --> 00:01:12,060
outs between the operations.

16
00:01:12,090 --> 00:01:15,630
This is just a standard way to train our networks.

17
00:01:15,750 --> 00:01:20,190
We forget some weights during the training so that we avoid all the fitting.

18
00:01:20,190 --> 00:01:22,250
Now that carving heads how it will work.

19
00:01:22,260 --> 00:01:23,460
Let's get back to coding

20
00:01:26,300 --> 00:01:39,900
and we will start by class and call the layer which inherits from the Layer class as per usual.

21
00:01:39,950 --> 00:01:50,720
So let's define the unit function self and what we will have as inputs is this sorry f f and four feed

22
00:01:50,720 --> 00:01:58,010
forward networks units which are the number of units that we will use in the forward spots that we just

23
00:01:58,010 --> 00:02:06,800
mentioned earlier just after the attention and the Kennedys imports and the number of projections for.

24
00:02:06,830 --> 00:02:11,510
So let's just call it no approach for the attention mechanism.

25
00:02:11,600 --> 00:02:18,150
And finally the dropout rate that we mentioned before.

26
00:02:18,240 --> 00:02:34,130
So now let's call the super we applied to any cogent layer and self and call the Inuit method for mates.

27
00:02:34,600 --> 00:02:36,420
So let's make sure we could.

28
00:02:36,460 --> 00:02:40,830
We will be able to use the vile boats later in the code.

29
00:02:40,840 --> 00:02:55,500
So self that's fit for network units but nuts just units will be good enough f f and units we'll get

30
00:02:55,500 --> 00:03:06,510
some guess I'm quite hungry f f in units self that's no projection equals then the projection and for

31
00:03:06,510 --> 00:03:16,110
the last one self that drop outs is equal to drop outs that we Hotz as parameter here we are we will

32
00:03:16,110 --> 00:03:21,840
for sure needs the build function because we will need dimension of the models and many thing that we

33
00:03:21,840 --> 00:03:25,450
don't have when we just create the objects and call the layer.

34
00:03:25,800 --> 00:03:35,340
So here self and input shape as before we don't know exactly right now what we will use.

35
00:03:35,340 --> 00:03:42,780
So let us first try to define the call function and as we go through the process we will build what

36
00:03:42,810 --> 00:03:46,460
we need to build in the appropriate methods just above.

37
00:03:46,470 --> 00:03:48,830
So call will need self.

38
00:03:48,840 --> 00:03:51,070
Of course we will get inputs.

39
00:03:51,170 --> 00:03:58,320
We will get the mask that we will apply in the Motorhead attentional layer and training which is a parameter

40
00:03:58,350 --> 00:04:01,340
that is either equal to False or true.

41
00:04:01,470 --> 00:04:07,530
And it basically just says to objects if we are in the training phase in this case it will apply the

42
00:04:07,530 --> 00:04:09,940
drop outs so that we don't other fits.

43
00:04:09,960 --> 00:04:16,170
If we are not training it will not apply the job because we want to get our whole architects working

44
00:04:16,170 --> 00:04:17,990
at 100 persons.

45
00:04:18,870 --> 00:04:29,920
What we will do is that we want to do the first step which is to apply a multi heads attention to the

46
00:04:30,010 --> 00:04:30,580
inputs.

47
00:04:30,580 --> 00:04:35,080
So in this case let's be sure about that.

48
00:04:35,080 --> 00:04:38,150
We can see that in this motel attention some layer.

49
00:04:38,230 --> 00:04:42,380
The key is the values and the queries are all the same.

50
00:04:42,400 --> 00:04:44,460
And they are all just the inputs we get right here.

51
00:04:44,680 --> 00:04:56,820
So we can say that we send inputs inputs inputs and finally we give it's the mask.

52
00:04:57,670 --> 00:05:08,880
So we use the self had attentional layer let's define it for I hate self that's multi heads.

53
00:05:09,440 --> 00:05:21,140
Attention will be a multi heads attentional layer and self tuts number of projection.

54
00:05:21,150 --> 00:05:23,440
This is all we need to to give.

55
00:05:23,470 --> 00:05:26,200
So it's well defined.

56
00:05:26,490 --> 00:05:29,420
Now we will use some dropouts here and there.

57
00:05:29,800 --> 00:05:37,480
So let's apply one right now self that's dropouts.

58
00:05:37,680 --> 00:05:47,400
1 attention and training will be given by the training perimeter right here.

59
00:05:47,800 --> 00:05:54,310
And then let's apply the norm that we so the sum and the norm that we mentioned before in the article

60
00:05:54,310 --> 00:05:57,950
so attention equals self.

61
00:05:57,950 --> 00:06:07,610
That's known one applied to the sum of attention and the inputs we had before.

62
00:06:07,620 --> 00:06:11,540
So let's just declare those two layers that we are using right now.

63
00:06:11,540 --> 00:06:15,030
So self that's dropouts.

64
00:06:15,030 --> 00:06:18,910
1 Sorry equals layers.

65
00:06:18,920 --> 00:06:23,890
That's dropouts and the rates is just self.

66
00:06:23,890 --> 00:06:34,490
That's dropouts and self that's no one equals layers.

67
00:06:34,490 --> 00:06:40,340
That's like a normal normalization.

68
00:06:40,680 --> 00:06:48,150
And let's say that the normalization will use epsilon equals 1 e minus 6.

69
00:06:48,150 --> 00:06:50,790
This is just a standard one.

70
00:06:50,820 --> 00:06:53,420
You can try other accidents if you want to.

71
00:06:53,460 --> 00:06:55,800
Trust me this one will do the job.

72
00:06:55,800 --> 00:06:59,430
Now we have the feed forward parts that we want to deal with.

73
00:06:59,460 --> 00:07:03,560
So let's say that all outputs will be equal to self.

74
00:07:03,550 --> 00:07:13,800
That's tense one which is the first part of all feed forwards process attention and then we have to

75
00:07:13,800 --> 00:07:15,360
apply the second one.

76
00:07:15,360 --> 00:07:23,830
I forgot t self that dense two of our outputs.

77
00:07:24,300 --> 00:07:26,850
We apply a small about just as before.

78
00:07:27,330 --> 00:07:39,120
So another one about two of outputs and finally we apply the set and the norm as we did before so known

79
00:07:39,540 --> 00:07:47,390
to apply to two outputs plus attention that we computed before.

80
00:07:48,060 --> 00:07:55,330
Let's now just define those those different layers that we used self.

81
00:07:55,360 --> 00:07:56,680
That's dense.

82
00:07:56,720 --> 00:08:03,920
One Equals layers that tense units equals.

83
00:08:03,940 --> 00:08:06,060
And this is where we use self.

84
00:08:06,070 --> 00:08:17,560
That's f f and units and as they say in the paper we will use activation equals

85
00:08:20,040 --> 00:08:28,470
value and the second part of the fit forward phase is another dense layer.

86
00:08:30,180 --> 00:08:36,450
This time we want to go back to units equals self.

87
00:08:36,520 --> 00:08:46,480
That's the model and this is where we realize that we need this model and that we can only get it in

88
00:08:46,480 --> 00:08:47,610
the build method.

89
00:08:47,620 --> 00:08:57,190
So let's just define it right he said that the model equals inputs shape and that's along the last dimension

90
00:08:57,760 --> 00:09:03,310
and the next layer that we want to define is drop outs right here.

91
00:09:03,310 --> 00:09:13,500
So itself that's true about two equals layers that dropout rate equals self.

92
00:09:13,510 --> 00:09:18,790
That's that's drop outs.

93
00:09:19,240 --> 00:09:31,020
And finally the last one well actually let's let's be lazy and copy this one and just change this one

94
00:09:31,140 --> 00:09:32,750
by a two.

95
00:09:32,750 --> 00:09:33,990
Here we are.

96
00:09:34,280 --> 00:09:38,960
The whole process has been rewritten so I think the outputs afterwards we are looking for at the end

97
00:09:38,960 --> 00:09:46,790
of an encoder layer so we can just return outputs and I guess these and call the layer is ready to be

98
00:09:46,790 --> 00:09:47,520
used.

99
00:09:47,690 --> 00:10:00,920
So as we said before now we are ready to create the whole encoder using the encoder layer.

100
00:10:01,190 --> 00:10:02,390
Same thing.

101
00:10:02,690 --> 00:10:04,470
This inherits from the layer.

102
00:10:04,820 --> 00:10:10,960
Class def in its self.

103
00:10:11,100 --> 00:10:14,970
And now we will have a certain number of parameters to build this layer.

104
00:10:15,210 --> 00:10:18,120
So first thing is number of layers.

105
00:10:18,140 --> 00:10:22,890
So this is the number of time we will repeat the called the layer inside the layer.

106
00:10:23,330 --> 00:10:30,540
If I find units will we will just pass to the encoder layer.

107
00:10:30,540 --> 00:10:39,610
Same thing with the number of projections and for the drop outs we will need the vocab size because

108
00:10:39,610 --> 00:10:44,980
there is an encoding phase at the beginning of the encoder an emerging face sorry at the beginning of

109
00:10:44,980 --> 00:10:45,670
the encoder.

110
00:10:46,000 --> 00:10:55,090
So we need to know what is the size of the vocabulary in order to do the embedding and of course the

111
00:10:55,240 --> 00:11:00,460
model to tell the imaging which will be the dimension of our words.

112
00:11:00,490 --> 00:11:12,390
At the end of this process let's be fancy and given name to layer with name equals encoded so that's

113
00:11:12,390 --> 00:11:12,540
it.

114
00:11:12,540 --> 00:11:16,720
That's all the parents as we will need to build our encoder as usual.

115
00:11:16,730 --> 00:11:28,120
Let's call super and call the self and this time we need to pass the time it's a name because name is

116
00:11:28,120 --> 00:11:31,130
something that belongs to the lower class.

117
00:11:31,150 --> 00:11:38,460
So we have to say to the layer class that the initialization is made with this name right here.

118
00:11:38,470 --> 00:11:41,030
Let's define all the valuables that we will need later.

119
00:11:41,320 --> 00:11:48,180
So self that's number layers equals no layers.

120
00:11:48,450 --> 00:11:48,960
Self.

121
00:11:49,030 --> 00:11:50,140
That's f f.

122
00:11:50,190 --> 00:11:56,190
And units equals f offend units.

123
00:11:57,500 --> 00:12:05,540
Self that no protections equals to the projections set ups drop outs.

124
00:12:05,790 --> 00:12:11,370
You guessed it equals drop outs and self.

125
00:12:11,370 --> 00:12:16,900
That's the model finally equals D.

126
00:12:16,930 --> 00:12:26,560
Model and we'll see later for the different layers we need to define it's easier to start with the cold

127
00:12:26,590 --> 00:12:28,960
methods and to see along the way.

128
00:12:28,960 --> 00:12:37,750
What we'll need to define before so def call it using self and we get the inputs the mask and again

129
00:12:37,750 --> 00:12:42,110
the training to see if we need to apply the drop outs.

130
00:12:42,130 --> 00:12:52,670
So first thing we had to do in the encoding phase and call layer is to apply the embedding itself.

131
00:12:52,690 --> 00:13:01,200
That's embedding that we will define matter of inputs then it's a detail that they say in the paper

132
00:13:01,420 --> 00:13:15,190
but at this point we multiply by T F dots math that's askew all t t costs self.

133
00:13:15,220 --> 00:13:24,230
That's the model T F that's floats 32.

134
00:13:24,240 --> 00:13:28,030
So this is just a small normalization.

135
00:13:28,050 --> 00:13:37,170
This is detailed in the paper when they say how the train is so let's quickly have a look at this here

136
00:13:37,650 --> 00:13:43,290
in the emptying of self Max they say in the embedding layers emitted by those weights by square roots

137
00:13:43,350 --> 00:13:45,330
of D model.

138
00:13:45,420 --> 00:13:50,120
So that was the reason of this small multiplication right here.

139
00:13:52,400 --> 00:13:56,960
Now let's apply all that positional encoding layer.

140
00:13:56,990 --> 00:14:01,610
So outputs outputs equals self.

141
00:14:01,610 --> 00:14:07,110
That's pause and coding of all inputs.

142
00:14:07,320 --> 00:14:13,530
Because the way we built the position and coding layer is that it returns the sum of the outputs and

143
00:14:13,530 --> 00:14:14,880
the positional encoding.

144
00:14:14,910 --> 00:14:21,060
So this will work fine and outputs equals self.

145
00:14:21,060 --> 00:14:22,540
That's dropouts.

146
00:14:22,560 --> 00:14:32,640
Just to tell you about before before applying all the encoding sub layers outputs training.

147
00:14:32,780 --> 00:14:38,510
So now we need to apply the encoder layer a certain number of times the best way to do that is to have

148
00:14:38,600 --> 00:14:44,040
actually lists of layers so that we can go through each with a loop.

149
00:14:44,150 --> 00:14:46,430
And this is what we will do right here.

150
00:14:46,460 --> 00:14:49,430
So for let's put it in the middle.

151
00:14:49,430 --> 00:14:51,840
For I in range self.

152
00:14:51,850 --> 00:14:55,470
That's number of layers.

153
00:14:55,610 --> 00:15:02,720
We will simply call out outputs equals self.

154
00:15:02,740 --> 00:15:16,200
That's encoding layers No I and we'll apply it to outputs with this same mosque again and with a training

155
00:15:16,200 --> 00:15:16,890
parameter.

156
00:15:18,570 --> 00:15:19,380
And here we are.

157
00:15:19,380 --> 00:15:24,080
We have applied the encoding sub layer as many times as we need.

158
00:15:24,080 --> 00:15:32,250
So let's just return the outputs and the last thing we have to do of course is to declare all those

159
00:15:32,250 --> 00:15:34,860
layers that we used in the input method.

160
00:15:34,890 --> 00:15:37,100
So let's proceed.

161
00:15:37,110 --> 00:15:42,540
Self that's embedding will simply be layers.

162
00:15:42,600 --> 00:15:51,610
That's and Beijing with us parameters the vocab size and the dimension of the model.

163
00:15:51,610 --> 00:15:54,130
So we need to tell the embedding layers.

164
00:15:54,130 --> 00:15:56,340
What is the size of our vocabulary.

165
00:15:56,350 --> 00:15:57,010
So what.

166
00:15:57,130 --> 00:16:02,000
What is the maximum number that is used in the tokenization.

167
00:16:02,000 --> 00:16:05,480
And of course the dimension that we want to achieve with our module.

168
00:16:05,480 --> 00:16:08,770
So that was for the embedding layer.

169
00:16:08,770 --> 00:16:15,040
Now we use a positional encoding layer that we created just before.

170
00:16:15,040 --> 00:16:25,790
So let's say that this is positional sorry and coding we did not need any time to for this one.

171
00:16:25,990 --> 00:16:29,320
Then we used a dropouts.

172
00:16:29,800 --> 00:16:32,350
Let's quickly define its layers.

173
00:16:32,350 --> 00:16:38,000
That's dropouts rates equals dropouts.

174
00:16:38,290 --> 00:16:40,890
Then we have all our encoding layers.

175
00:16:40,890 --> 00:16:50,650
So as I said the best way to do that is to create a list of encouraging layers so encodes a layer.

176
00:16:52,270 --> 00:16:58,320
F and units you know it's perfect.

177
00:17:00,130 --> 00:17:09,090
We use the number of projections and finally we need to give it the drop out rate and this is done for

178
00:17:11,420 --> 00:17:16,500
something that we don't need in range no layers.

179
00:17:17,750 --> 00:17:20,270
So this is operation right.

180
00:17:20,270 --> 00:17:28,300
He just creates a certain number of encoding layers and this number has been given by the number layers

181
00:17:28,330 --> 00:17:29,250
valuable.

182
00:17:29,510 --> 00:17:37,280
And actually we just realized right here that we almost don't need those valuables anywhere else that

183
00:17:37,280 --> 00:17:41,420
in the end it functions so we can actually get rid of a lot of things here.

184
00:17:41,510 --> 00:17:51,060
So basically we can get rid of all those valuables as all we need outside of the in its method is the

185
00:17:51,300 --> 00:17:57,630
model right there and the set number layers right here.

186
00:17:57,930 --> 00:18:00,260
And so we just have to keep those two.

187
00:18:00,410 --> 00:18:03,840
And and that's it's so perfect.

188
00:18:03,840 --> 00:18:05,630
We have all anchored already.

189
00:18:05,670 --> 00:18:11,000
Now we can go to the second big parts of the transformer and build the decoder.

190
00:18:11,040 --> 00:18:15,120
And when this is done we will finally be able to build the whole architecture.

191
00:18:15,450 --> 00:18:18,870
So let's waste no more time and get into the decoder.

192
00:18:18,870 --> 00:18:19,370
See you soon.
