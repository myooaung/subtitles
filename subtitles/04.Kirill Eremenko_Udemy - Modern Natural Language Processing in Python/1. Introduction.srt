1
00:00:00,660 --> 00:00:03,980
Hi guys and welcome back to this course on MLP.

2
00:00:04,050 --> 00:00:10,200
I hope you are super excited because I am because finally we will be able to build on transformer and

3
00:00:10,230 --> 00:00:16,050
the way we will do that is that we will implement our own very own translator just like Google Translate

4
00:00:16,060 --> 00:00:22,920
for instance before we go into that's just a quick look at the plan of attack for this section first

5
00:00:22,920 --> 00:00:26,970
we will have to deal with the data processing of course as always.

6
00:00:27,560 --> 00:00:33,510
And after that we will be able to build the whole model starting with the whole embedding and positional

7
00:00:33,510 --> 00:00:38,070
encoding phase that we saw earlier which is the first like the first step.

8
00:00:38,070 --> 00:00:45,040
The first layer of the whole model then we will dive to the whole attention mechanism so first with

9
00:00:45,040 --> 00:00:52,480
these scaled stack products which is the the core of this whole mechanism and then this will allow us

10
00:00:52,720 --> 00:00:59,150
to build this civilly the multi attentional sub layer which will be the core for everything else.

11
00:00:59,250 --> 00:01:03,800
After that we will write the code for the whole encoder.

12
00:01:03,820 --> 00:01:10,510
Same thing will go for the decoder and we will finally be able to write the code for the for the transformer

13
00:01:10,540 --> 00:01:18,030
that will be ready to to work and to be trained then goes this very phase like the training phase.

14
00:01:18,070 --> 00:01:20,760
This will take about a few hours but no more.

15
00:01:20,770 --> 00:01:26,860
And just believe me in a few hours you will be able to have a performing translator and we will see

16
00:01:26,860 --> 00:01:33,130
that in the last phase the evaluating phase where we will give some homemade sentences to one translator

17
00:01:33,400 --> 00:01:36,000
just to assess how well it performs.

18
00:01:36,010 --> 00:01:37,660
So this was for the plan of attack.

19
00:01:37,810 --> 00:01:40,920
Before we start coding of course we will need to get the data.

20
00:01:41,170 --> 00:01:47,820
The data are used to take our a whole corpus of English sentences translate into French.

21
00:01:47,820 --> 00:01:51,050
I picked this language because this is my native language.

22
00:01:51,100 --> 00:01:57,700
It was of course easier for me to develop the the model and to assess how it works.

23
00:01:57,760 --> 00:02:00,900
You can of course speak any language you want.

24
00:02:00,910 --> 00:02:07,990
So I took the data from the euro Parliament's European Parliament that higher they have a really rich

25
00:02:08,020 --> 00:02:10,330
corpus in many languages.

26
00:02:10,330 --> 00:02:17,470
So you just have to take this in your favorite search engine and go to these websites stayed empty and

27
00:02:17,470 --> 00:02:22,850
you see that right here they have a lot of pairs of languages.

28
00:02:22,990 --> 00:02:28,810
So this one the French English one and once you don't know it it's all you have to do is to unzip the

29
00:02:28,810 --> 00:02:33,900
whole folder and you will get those files.

30
00:02:34,030 --> 00:02:38,170
The European poll English and the same one translated here.

31
00:02:38,200 --> 00:02:42,450
So those are just the same corpus in two different languages.

32
00:02:42,640 --> 00:02:46,920
Those two additional fighters don't come with those these four.

33
00:02:47,960 --> 00:02:50,180
I will just quickly explain what they are.

34
00:02:50,180 --> 00:02:57,830
These are just a quick lists of characters of words that are usually followed by points in the in the

35
00:02:57,950 --> 00:03:00,380
language without being forced ups.

36
00:03:00,410 --> 00:03:06,340
So what I mean by that is that you can sometimes find Mr. for instance followed by points.

37
00:03:06,350 --> 00:03:08,580
That doesn't mean that it's the end of the sentence.

38
00:03:08,660 --> 00:03:12,980
And later in the data processing the took an amazing face.

39
00:03:12,980 --> 00:03:19,880
We'll have to split a sentence into several words and we don one can either to thing that each point

40
00:03:19,910 --> 00:03:27,040
is the end of the sentence for instance if a sentence says that something happened at 10:00 a.m. this

41
00:03:27,040 --> 00:03:29,970
this will be written a dot and dots.

42
00:03:29,970 --> 00:03:35,960
This doesn't mean that those are full stops and that the sentence needs to end so this file we will

43
00:03:35,960 --> 00:03:40,760
use it in order to pinpoint the points that are not full stops.

44
00:03:41,020 --> 00:03:43,130
I will provide this file to you.

45
00:03:43,130 --> 00:03:48,530
You can Don on this as resources or you can write your own if you use different languages even if you

46
00:03:48,530 --> 00:03:55,970
don't use this Don where we are the performances will not be so bad it's just to increase the quality

47
00:03:55,970 --> 00:03:59,300
of the inputs but doesn't make a huge difference.

48
00:03:59,450 --> 00:04:05,930
Once you have all these files the best way to have access to your personal files in Google collab is

49
00:04:05,930 --> 00:04:07,430
to go through Google Drive.

50
00:04:07,490 --> 00:04:14,540
So just go to your personal drive megafauna with your data for instance former drag and drop all the

51
00:04:14,540 --> 00:04:19,850
files here and yeah everything will be set up to start coding.

52
00:04:19,880 --> 00:04:23,070
So now that everything's ready I hope you're super excited.

53
00:04:23,090 --> 00:04:24,970
Let's start coding and see you soon.
