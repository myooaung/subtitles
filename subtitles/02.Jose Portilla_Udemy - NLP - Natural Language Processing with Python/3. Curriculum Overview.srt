1
00:00:05,320 --> 00:00:09,610
Welcome back everyone the course curriculum lecture before you begin the course.

2
00:00:09,700 --> 00:00:12,910
Let's build an understanding of our overall learning path.

3
00:00:12,910 --> 00:00:17,340
This lecture is just going to be a brief overview of the main topics and the structure of the curriculum

4
00:00:17,350 --> 00:00:21,910
of course that way you can understand the way the Course is laid out and maybe you can jump to a particular

5
00:00:21,910 --> 00:00:25,750
section that interests you or understand the pathway from start to finish.

6
00:00:26,910 --> 00:00:33,960
So the overall course sections are pie on text basics in LP Python basics part of speech tagging Tex

7
00:00:33,960 --> 00:00:39,780
classification semantics and sentiment analysis topic modeling and then some chat bots and advance material

8
00:00:39,780 --> 00:00:41,760
that fall under the deep learning category.

9
00:00:43,520 --> 00:00:46,970
We're going to start off with Python text basics where we're just going to learn the very basics of

10
00:00:46,970 --> 00:00:49,210
Python's built in text capabilities.

11
00:00:49,380 --> 00:00:53,900
Those are things like learning how to open text files how to work with PDA files and the basics of regular

12
00:00:53,900 --> 00:00:56,050
expressions.

13
00:00:56,090 --> 00:00:58,450
Then we'll move on to the Python basics.

14
00:00:58,460 --> 00:01:03,290
This is when we actually start using specialized natural language processing libraries such as spacey

15
00:01:03,320 --> 00:01:04,380
and LDK.

16
00:01:04,490 --> 00:01:07,060
These are the two most popular libraries for Python.

17
00:01:07,250 --> 00:01:11,570
And we'll have a deeper discussion on the differences between them and why you may want to use another

18
00:01:11,660 --> 00:01:15,250
like spacing in certain situations or in LDK in other situations.

19
00:01:15,260 --> 00:01:18,650
So that's what I really begin to work with natural language processing libraries.

20
00:01:19,790 --> 00:01:21,660
There we're going to move on.

21
00:01:21,740 --> 00:01:27,440
And within that section we'll discuss things like Spacey pipeline's tokenization stemming limitations

22
00:01:27,530 --> 00:01:34,350
Stoppard's and vocabulary and phrase matching the next section is essentially going to be a continuation

23
00:01:34,410 --> 00:01:39,500
of learning how to work those libraries by talking about things like parts of speech tagging named entity

24
00:01:39,510 --> 00:01:46,820
recognition sentence segmentation or spacey and we'll also see how to visualize speech and named recognition.

25
00:01:48,790 --> 00:01:51,680
Now we'll switch gears and talk about teks classification.

26
00:01:51,850 --> 00:01:55,840
This is where we dive deeper into machine learning learning about how to use sikat learn for machine

27
00:01:55,840 --> 00:01:56,500
learning.

28
00:01:56,500 --> 00:02:01,510
Just an overall process and then we'll talk about text feature extraction conducting operations like

29
00:02:01,510 --> 00:02:07,480
Count vectorization and then T.F. idea vectorization term frequency inversed document frequency and

30
00:02:07,480 --> 00:02:11,140
then we'll have a text classification project for you.

31
00:02:11,360 --> 00:02:16,630
After that we'll talk about semantics and sentiment analysis semantics and things like word vectors

32
00:02:16,670 --> 00:02:21,380
such as the words Avik algorithm and then sentiment analysis understanding whether a piece of text is

33
00:02:21,380 --> 00:02:24,870
positive or negative sentiment.

34
00:02:24,890 --> 00:02:26,840
Then we move on to topic modeling.

35
00:02:26,840 --> 00:02:29,170
This is again on the machine learning side of things.

36
00:02:29,180 --> 00:02:31,890
But we'll be switching gears to unsupervised learning.

37
00:02:31,910 --> 00:02:37,940
We'll talk about things like later like allocation and non-negative matrix factorization and this at

38
00:02:37,940 --> 00:02:44,390
the end will have a topic modeling project with LDA and MF towards the end.

39
00:02:44,380 --> 00:02:48,670
We're going to have a general deep learning section where we talk about things like chat bots and advanced

40
00:02:48,670 --> 00:02:49,450
material.

41
00:02:49,630 --> 00:02:54,490
So here we're going to focus on more complex and more advanced topics like actually generating text

42
00:02:54,820 --> 00:03:00,040
using recurrent neural networks and long short term memory units and then will also talk about using

43
00:03:00,040 --> 00:03:04,280
deep learning to create your own chaplets OK.

44
00:03:04,440 --> 00:03:05,260
Let's get started.

45
00:03:05,400 --> 00:03:06,300
I'll see you at the next lecture.

