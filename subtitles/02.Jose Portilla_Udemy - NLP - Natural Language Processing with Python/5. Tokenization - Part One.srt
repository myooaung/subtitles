1
00:00:05,360 --> 00:00:11,780
Welcome to this lecture on tokenization tokenization is the process of breaking up the original raw

2
00:00:11,780 --> 00:00:15,580
text into component pieces or otherwise known as tokens.

3
00:00:15,590 --> 00:00:21,190
This was the very first step that occurred when we created our document objects using the pipeline was

4
00:00:21,210 --> 00:00:23,780
spacey.

5
00:00:23,890 --> 00:00:29,350
You can imagine that we have some original text and the very first step is to split on whitespace.

6
00:00:29,350 --> 00:00:35,680
So from where moving to L.A. We split on the white space then Spacey is smart enough to understand that

7
00:00:35,680 --> 00:00:39,250
it's going to separate prefix that is characters at the beginning.

8
00:00:39,460 --> 00:00:41,980
It's also going to two separate exceptions.

9
00:00:42,070 --> 00:00:46,620
Those are concatenations and it's also going to be able to separate a suffix.

10
00:00:46,630 --> 00:00:48,930
So those are characters towards the end.

11
00:00:49,240 --> 00:00:55,450
And then we can also realize that we have exceptions such as an exclamation point and then we're done.

12
00:00:55,450 --> 00:01:00,410
So there's lots of steps involved but luckily Spacey is taking care of all of that for us.

13
00:01:00,430 --> 00:01:05,440
We have prefix suffix infix and exceptions.

14
00:01:05,440 --> 00:01:08,760
So again notice that the tokens are just pieces of the original text.

15
00:01:08,800 --> 00:01:13,280
We don't see any conversion to words stems or lemmas the base forms of words.

16
00:01:13,390 --> 00:01:18,100
And we haven't seen anything yet about organizations places money etc. that's going to be named and

17
00:01:18,190 --> 00:01:23,890
the recognition we we're going to talk about later on for now you should understand that tokens are

18
00:01:23,890 --> 00:01:28,690
the basic building blocks of a dark object everything to help us understand the meaning of the text

19
00:01:28,990 --> 00:01:32,440
is derived from tokens and the relationship to one another.

20
00:01:34,290 --> 00:01:38,730
As I mentioned we had the prefix those were the characters at the beginning.

21
00:01:38,730 --> 00:01:41,910
We had the suffix characters at the end.

22
00:01:41,960 --> 00:01:47,660
We have infix which are the characters in between the prefix and the suffix and then we have the exception

23
00:01:47,750 --> 00:01:53,000
which is a special case rule to split a string into several tokens or prevent a token from being split

24
00:01:53,270 --> 00:01:55,190
when punctuation rules are applied.

25
00:01:56,710 --> 00:01:58,860
So we can see a few examples here such as.

26
00:01:58,900 --> 00:02:03,120
A prefix is a dollar sign prefixing an amount of money.

27
00:02:03,490 --> 00:02:10,880
A suffix can be something like K.M. suffix seeing at the end the kilometers travelled in unfix is just

28
00:02:10,880 --> 00:02:17,900
something between like a dash or a forward slash and the exceptions can be basically things are a special

29
00:02:17,900 --> 00:02:23,210
case rules to make sure when you're doing the split as the token you understand that it's actually part

30
00:02:23,210 --> 00:02:28,500
of a joint punctuation like you dot dot.

31
00:02:28,700 --> 00:02:31,490
Now tokens have a variety of useful attributes and methods.

32
00:02:31,580 --> 00:02:34,060
We're going to explore tokens of Spacey in further detail.

33
00:02:34,070 --> 00:02:35,770
But jumping over to Jupiter notebook.

34
00:02:35,900 --> 00:02:37,300
Let's get started.

35
00:02:37,370 --> 00:02:44,510
The first two are going to do in our note book is import Spacey and then load up our Spacey library

36
00:02:45,580 --> 00:02:56,940
will say Spacey load and then call it an underscore co core underscore Wehbe underscore S-M that load's

37
00:02:56,940 --> 00:02:58,340
or spacey library.

38
00:02:58,620 --> 00:03:03,940
Next let's go and create a string that includes opening and closing quotation marks.

39
00:03:04,230 --> 00:03:15,870
So say my string is equal to will have single strings on the outside and we're going to say where moving

40
00:03:16,530 --> 00:03:30,450
to a exclamation point and then we're actually going to have this in its own set of quotes.

41
00:03:30,480 --> 00:03:33,990
So those are the quotes I'm using on the very outside.

42
00:03:33,990 --> 00:03:39,450
I have this single set of quotes and then the reason I'm using a backslash here is the so that the single

43
00:03:39,450 --> 00:03:44,970
quote in between we and our ear isn't actually going to break up the string and then we have these double

44
00:03:44,970 --> 00:03:53,060
quotes which are going to be intact so if I took a look at my string I see we're moving to L.A. But

45
00:03:53,060 --> 00:03:58,500
if I actually print this out I will see we're moving to L.A..

46
00:03:58,880 --> 00:04:03,710
So again just including some information in there such as the double quotes and then using this backslash

47
00:04:03,740 --> 00:04:09,120
as an escape character for the single quote in order to not stop the string too early.

48
00:04:10,310 --> 00:04:20,650
Let's go ahead and say Doc is equal to an LP and will pass in my string and then for a token and doc.

49
00:04:20,920 --> 00:04:30,810
Let's go ahead and print the text of the token and we can see here we were able to grab the double quotes.

50
00:04:30,810 --> 00:04:35,150
We're able to separate we an apostrophe are we moving to.

51
00:04:35,280 --> 00:04:39,110
We understood the LS should be kept together and then we have an exclamation mark.

52
00:04:39,570 --> 00:04:45,400
So Spacey is going to isolate punctuation that does not form an integral part of a word.

53
00:04:45,450 --> 00:04:50,010
Things like quotation marks commas and punctuation at the end of the sentence will be assigned their

54
00:04:50,010 --> 00:04:50,930
own token.

55
00:04:51,150 --> 00:04:57,390
However punctuation that exists as part of the email address website or numerical value will be kept

56
00:04:57,450 --> 00:04:58,710
as part of that token.

57
00:04:58,710 --> 00:05:00,560
Let me show you an example.

58
00:05:00,730 --> 00:05:06,390
Let's create another string and pass it into a dock.

59
00:05:06,860 --> 00:05:18,050
We're going to create a unicode string that says something like We're here to help send snail mail email

60
00:05:18,100 --> 00:05:25,260
support at our site dot com or visit us at each.

61
00:05:25,270 --> 00:05:33,430
And it's going to make up a Web site here that E.W. our site dot com exclamation mark.

62
00:05:33,950 --> 00:05:38,790
So we run that and run if we can see this is actually a really complex string to analyze.

63
00:05:38,810 --> 00:05:42,090
We have a lot of punctuation here such as these exclamation points.

64
00:05:42,100 --> 00:05:45,080
This dash or hyphens separating snail mail.

65
00:05:45,230 --> 00:05:48,350
We have dots here which isn't really of a sentence.

66
00:05:48,360 --> 00:05:52,970
Instead it's part of this email address and then we also have these dots right here which are part of

67
00:05:52,970 --> 00:05:54,060
a Web site.

68
00:05:54,080 --> 00:05:56,320
Spacey is smart enough to understand that.

69
00:05:56,360 --> 00:06:08,010
So if we see 40 in doc to print see here we can see that the Web site and email are intact.

70
00:06:08,050 --> 00:06:12,850
Space is smart enough to understand that this particular period isn't going to end the sentence.

71
00:06:12,870 --> 00:06:18,210
What is interesting though is that the hyphen in between snail and mail is actually assigned its own

72
00:06:18,210 --> 00:06:24,370
token and spacially has its own internal rules for deciding what kind of token gets assigned where.

73
00:06:24,420 --> 00:06:25,940
Let's create another document.

74
00:06:26,030 --> 00:06:39,860
Well say Doc three is equal to an LP and here we're going to pass in a five kilometer NYC cab ride cab

75
00:06:39,860 --> 00:06:47,050
ride it costs $10 and 30 cents.

76
00:06:47,050 --> 00:06:51,750
Run that and then we'll save 40 in Doc 3.

77
00:06:51,880 --> 00:06:54,230
Go ahead and print t.

78
00:06:54,790 --> 00:07:00,390
And he Rissi a five K.M. being separated from five NYC cab ride costs.

79
00:07:00,390 --> 00:07:03,830
Dollar Sign and again it's smart enough to keep ten thirty.

80
00:07:03,840 --> 00:07:04,720
Together.

81
00:07:04,930 --> 00:07:08,720
So here that distance unit and the dollar sign are assigned their own tokens.

82
00:07:08,770 --> 00:07:13,820
Yet the dollar amount itself here and $10 30 cents is preserved.

83
00:07:14,800 --> 00:07:21,340
Punctuation that exists as part of a known creation will also be kept as part of the token.

84
00:07:21,340 --> 00:07:33,730
So for example will create no document and LP and will say something like let's visit St. Louis in the

85
00:07:34,390 --> 00:07:37,210
US next year.

86
00:07:37,210 --> 00:07:37,970
We run that.

87
00:07:38,100 --> 00:07:40,310
Well check out the tokens that spacey figured out.

88
00:07:42,210 --> 00:07:44,610
And I'll say let's visit.

89
00:07:44,620 --> 00:07:50,460
Notice it keeps St together Louis and it keeps us together again.

90
00:07:50,470 --> 00:07:53,260
Everything is happening under the hood for your convenience.

91
00:07:53,290 --> 00:07:56,150
You don't actually have to tell Spacey what rules to expect.

92
00:07:56,170 --> 00:07:59,160
It's smart enough to understand that on its own.

93
00:07:59,230 --> 00:08:04,360
Now if we ever want to count the number of tokens all we need to do is check the length of a document

94
00:08:04,390 --> 00:08:05,040
object.

95
00:08:05,040 --> 00:08:07,660
So if I want to know how many tokens doc 4 has.

96
00:08:07,660 --> 00:08:11,400
I just checked the length and it reports back that it has 11.

97
00:08:11,410 --> 00:08:20,590
Now we can actually also count vocab entry vocab objects contain a fool library of items so off of a

98
00:08:20,590 --> 00:08:25,900
document object you can call the vocab.

99
00:08:26,230 --> 00:08:31,870
And it's this large vocabulary object and this number is going to change based on the language library

100
00:08:31,870 --> 00:08:34,240
loaded at the start.

101
00:08:34,270 --> 00:08:40,990
So if I check the length of Doc for that vocab notice right now it's fifty seven thousand eight hundred

102
00:08:41,110 --> 00:08:42,250
fifty two.

103
00:08:42,250 --> 00:08:51,250
That means that when we loaded up this English core web as M for small that has a vocabulary of fifty

104
00:08:51,250 --> 00:08:57,640
seven thousand eight hundred fifty two different types of tokens and there is an LG or a much larger

105
00:08:57,640 --> 00:09:00,820
language library that you can use for more complex tasks.

106
00:09:00,880 --> 00:09:05,990
But for most of our use cases this language library is going to service fine.

107
00:09:06,180 --> 00:09:10,980
Now tokens can also be retrieved by index position and slice and we've actually shown this before.

108
00:09:11,030 --> 00:09:11,990
But let's reiterate.

109
00:09:12,090 --> 00:09:23,780
It will say duck 5 is equal to an LP pass and a unicode string that says it is better to give than receive

110
00:09:26,420 --> 00:09:30,140
and if I have docked 5 here it can simply index.

111
00:09:30,730 --> 00:09:38,650
And here I can grab one single token or I can grab a slice of tokens maybe 5 say give me everything

112
00:09:38,650 --> 00:09:45,830
from position 2 to 5 and I can see better to give and we kind of have our own little span there.

113
00:09:45,870 --> 00:09:52,080
Keep in mind tokens cannot be reassigned although document objects can be considered lists of tokens.

114
00:09:52,080 --> 00:09:54,470
They do not support item re-assignment.

115
00:09:54,630 --> 00:09:57,150
So once you created that document it's going to be fixed.

116
00:09:57,150 --> 00:10:03,340
You can't do something like re-assign something like that it's going to say object does not support

117
00:10:03,340 --> 00:10:03,790
assignment.

118
00:10:03,790 --> 00:10:07,960
So this document object you're not allowed to do this sort of reassignment which makes sense there's

119
00:10:07,960 --> 00:10:11,010
a lot of information here that spacey figured out from these tokens.

120
00:10:11,200 --> 00:10:14,970
It shouldn't make sense to be able to simply replace it for a simple python string.

121
00:10:16,950 --> 00:10:24,300
Now Spacey can actually go a step beyond simple tokens it can actually understand named entities named

122
00:10:24,360 --> 00:10:24,890
entities.

123
00:10:24,900 --> 00:10:27,000
Add another layer of context.

124
00:10:27,120 --> 00:10:32,880
The language model that you load in at the very top recognizes that certain words or organization names

125
00:10:32,970 --> 00:10:39,450
while others are locations and still other combinations relate to things like money or dates named entities

126
00:10:39,480 --> 00:10:43,970
are accessible through the E and T S property of a document object.

127
00:10:43,980 --> 00:10:52,300
I want to create a new document to show this will say Doc let's say eight is equal to an LP and we'll

128
00:10:52,300 --> 00:11:05,860
say the following string Apple to build a Hong Kong factory for 6 million dollars want to just say six

129
00:11:05,860 --> 00:11:08,150
million dollars.

130
00:11:08,260 --> 00:11:11,920
So we run this right now and we already know that we have token information.

131
00:11:11,920 --> 00:11:21,850
We can say for tokin and Doc date print out the token text and in fact I can print it all out along

132
00:11:21,850 --> 00:11:26,890
the same line by saying and is equal to some sort of separator.

133
00:11:26,890 --> 00:11:28,740
So I can do that to print out.

134
00:11:28,750 --> 00:11:30,460
Let's add a little space there.

135
00:11:31,030 --> 00:11:33,010
So here I can see all the various tokens.

136
00:11:33,070 --> 00:11:38,270
Apple to build a Hong Kong factory for the dollar sign than six and then million.

137
00:11:38,560 --> 00:11:40,620
But here's what's really cool.

138
00:11:40,960 --> 00:11:52,240
I can go ahead and say for entity in dock 8 but instead of now going through dock I'm going to say dock

139
00:11:52,270 --> 00:12:00,800
eight dot and Ts and we're not going to do is say print the entity.

140
00:12:01,220 --> 00:12:07,190
So off of this string space is smart enough to figure out that there's something special about Apple.

141
00:12:07,340 --> 00:12:13,130
Hong Kong and six million dollars and is able to figure this out without you telling it more information.

142
00:12:13,130 --> 00:12:19,430
It recognized that Apple Hong Kong and six million dollars are named entities kind of similar to proper

143
00:12:19,430 --> 00:12:20,600
nouns in a sense.

144
00:12:20,650 --> 00:12:24,860
Basically there's more context to these particular words.

145
00:12:24,860 --> 00:12:29,850
And in fact it's smart enough to know the falling off of each of these entities.

146
00:12:30,020 --> 00:12:36,650
We can actually print out a label so you can say label underscore and then we're going to print a new

147
00:12:36,650 --> 00:12:38,330
line here.

148
00:12:39,200 --> 00:12:40,890
And you'll notice that it has these labels.

149
00:12:40,880 --> 00:12:48,830
They'll say Apple is R.G. or organization or $6000 is money and you can check out the named entities

150
00:12:48,830 --> 00:12:55,040
list that we have inside of our notebook we have a little link here that will show you a table of what

151
00:12:55,040 --> 00:13:00,700
names and cities look like and what these actual codes stand for and then check out the link in our

152
00:13:00,730 --> 00:13:03,330
notebook for link the documentation.

153
00:13:03,350 --> 00:13:09,370
Now you can ask for more explanation on these labels directly by simply saying the following.

154
00:13:09,440 --> 00:13:14,540
Print the string representation of a Speccy.

155
00:13:14,730 --> 00:13:20,410
It has this really useful explain function and you can actually ask it to explain that label.

156
00:13:20,690 --> 00:13:26,480
So I know a little bit nested here but all I'm saying is explain that label pasand into a string and

157
00:13:26,480 --> 00:13:27,410
then print it out.

158
00:13:29,140 --> 00:13:29,700
Whoops.

159
00:13:30,050 --> 00:13:30,580
This should be.

160
00:13:30,600 --> 00:13:32,230
And to see.

161
00:13:33,120 --> 00:13:33,770
OK.

162
00:13:33,780 --> 00:13:38,850
So again I'm just going through each entity printing out the entity print printing out the label and

163
00:13:38,850 --> 00:13:41,800
then printing out the builtin explanation for that label.

164
00:13:41,820 --> 00:13:45,440
So orgy's stands for companies agencies institutions.

165
00:13:45,570 --> 00:13:52,110
GP is for countries cities and states 6 million are recognized as money which is monetary values including

166
00:13:52,110 --> 00:13:52,690
unit.

167
00:13:53,010 --> 00:13:55,710
So all of this is what space is doing for you.

168
00:13:55,710 --> 00:14:00,060
Lots of awesome work though we don't really have to be aware of.

169
00:14:00,190 --> 00:14:07,320
And the last thing I want to mention here is that Doc E.A. s or entities similar to that we have now

170
00:14:07,320 --> 00:14:11,340
and Chunk's which are another object property nown chunks.

171
00:14:11,410 --> 00:14:17,120
Our base noun phrases essentially flat phrases that have a noun as their head.

172
00:14:17,320 --> 00:14:22,210
So you can think of noun chunks as noun plus the words describing a particular noun.

173
00:14:22,210 --> 00:14:23,710
Let me show you what I mean by that.

174
00:14:23,990 --> 00:14:32,850
We're going to say Doc nine is equal to an LP and we'll create a sentence here called Autonomy's cars

175
00:14:36,530 --> 00:14:43,820
shift insurance liability toward manufacturers.

176
00:14:44,120 --> 00:14:44,540
OK.

177
00:14:44,540 --> 00:14:49,820
So kind of a complex sentence here and we have various chunks the chunks are going to be autonomous

178
00:14:49,820 --> 00:14:53,700
cars because we have a known cars and this word describing that.

179
00:14:53,830 --> 00:15:00,770
Now we also have insurance liability as another chunk and then manufactures itself is a now chunk is

180
00:15:00,800 --> 00:15:01,420
just a noun.

181
00:15:01,460 --> 00:15:03,860
But it doesn't really have the Scripter here.

182
00:15:04,010 --> 00:15:13,730
So again I can say for chunk in and I'll say Doc nine and I can call noun chunks off of this and then

183
00:15:13,730 --> 00:15:21,030
actually print out each chunk autonomous cars insurance liability and manufacturers.

184
00:15:21,300 --> 00:15:26,880
And I would encourage you to again check out the link that we provide in the notebook for a list of

185
00:15:26,880 --> 00:15:29,720
all the different types of noun chunks that are available.

186
00:15:29,760 --> 00:15:34,920
Coming up next for part two we're going to talk about visualizing with builtin visualizes that's basically

187
00:15:34,920 --> 00:15:36,120
also provides.

188
00:15:36,150 --> 00:15:37,170
We'll see you at the next lecture.

