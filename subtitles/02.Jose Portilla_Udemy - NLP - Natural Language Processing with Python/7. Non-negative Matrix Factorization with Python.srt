1
00:00:05,510 --> 00:00:06,470
Welcome back everyone.

2
00:00:06,560 --> 00:00:11,150
And this lecture we're now going to show you how you can perform non-negative matrix factorization with

3
00:00:11,150 --> 00:00:12,070
Python.

4
00:00:12,080 --> 00:00:17,510
We're going to work on the same NPR that CXXVI dataset and you'll realize how easy it is to swap out

5
00:00:17,570 --> 00:00:19,660
LDK for anti-meth.

6
00:00:19,670 --> 00:00:21,980
Let's head over to Jupiter notebook and get started.

7
00:00:22,850 --> 00:00:23,160
All right.

8
00:00:23,180 --> 00:00:25,430
I've created a brand new untitled notebook.

9
00:00:25,430 --> 00:00:27,900
It's located underneath my topic modeling folder.

10
00:00:27,980 --> 00:00:36,770
So I have easy access to the NPR CSFB file or going to do is just like before import Panthers PD and

11
00:00:36,770 --> 00:00:42,000
then read in the file and we're going to go through all of this quite a bit quicker than we did the

12
00:00:42,000 --> 00:00:47,940
first time around since we already know what the NPR that every file is how to extract features from

13
00:00:47,940 --> 00:00:49,740
it and how to display topics.

14
00:00:49,800 --> 00:00:52,360
Essentially there's two main things we need to change.

15
00:00:52,380 --> 00:01:01,220
The first one is the pre-processing feature extraction because of the dependence on per word count probabilities.

16
00:01:01,280 --> 00:01:03,970
There is still a probability distribution.

17
00:01:04,050 --> 00:01:10,540
We could only use count vector risers for LDK But since non-negative matrix factorization works have

18
00:01:10,560 --> 00:01:11,800
coefficient values.

19
00:01:11,910 --> 00:01:15,240
Here we can actually preprocess the text with T.F. idea.

20
00:01:15,300 --> 00:01:25,750
Vectorization I say from Eski learn the feature extraction the text import and we're going to import

21
00:01:25,860 --> 00:01:33,230
the IDF vectorize or and keep in mind T.F. idea of vectorize are actually performs a count vectorizing

22
00:01:33,280 --> 00:01:33,920
beforehand.

23
00:01:33,950 --> 00:01:41,450
So essentially it takes care of a lot of steps for us and we'll say T.F. IDF are going to create incidents

24
00:01:41,480 --> 00:01:43,040
of this T.F. idea of vectorize.

25
00:01:43,370 --> 00:01:48,040
And just like before we can ask for a maxim document frequency.

26
00:01:48,140 --> 00:01:55,040
So we're going to ask for words that show up in no more than 95 percent of the documents and then we

27
00:01:55,040 --> 00:02:01,190
can also have a minimum document frequency we could pass this in a percentage saying 0.1 something like

28
00:02:01,280 --> 00:02:04,040
it must show up in at least 10 percent of all the documents.

29
00:02:04,280 --> 00:02:06,880
But here a good filter is something like 2.

30
00:02:07,040 --> 00:02:11,330
That way you essentially filter out words that are unique to any particular document because they won't

31
00:02:11,330 --> 00:02:16,880
really be useful for topic modeling if they only show up in one singular document and then just like

32
00:02:16,880 --> 00:02:24,180
before we can provide a stop words equal to the string code English and T.F. idea vectorize will automatically

33
00:02:24,180 --> 00:02:31,160
remove English Stoppard's for us then we'll create our DTM the technically more than just the document

34
00:02:31,350 --> 00:02:33,280
matrix since we are performing T.F. idea.

35
00:02:33,290 --> 00:02:39,760
But just to keep things kind of in parallel as before we'll keep some of the same variable names then

36
00:02:39,770 --> 00:02:41,350
we're going to call a fit.

37
00:02:41,400 --> 00:02:49,940
Transform this NPR article column and then we're going to have our.

38
00:02:50,090 --> 00:02:54,890
Essentially what is our T.F. idea of matrix here we just call it the variable DTM.

39
00:02:54,890 --> 00:03:00,180
So it's a sparse matrix just like before where we have the articles by the number of words.

40
00:03:00,440 --> 00:03:01,080
OK.

41
00:03:01,370 --> 00:03:11,900
And then to perform and MF We simply say from as Kilbourn thought decomposition import and F and we'll

42
00:03:11,900 --> 00:03:18,680
say and M.F. model is equal to an instance of an M F and there's a couple of things to note here.

43
00:03:18,710 --> 00:03:21,790
The first one is the number of components.

44
00:03:21,800 --> 00:03:26,720
So this is essentially the K value that we need to choose which is going to correlate with how many

45
00:03:26,720 --> 00:03:31,560
topics we want we'll go ask for the same number of topics as we did last time.

46
00:03:31,730 --> 00:03:34,250
But I encourage you to play around this value.

47
00:03:34,250 --> 00:03:38,380
And then what I also would like to do is provide a random state.

48
00:03:38,420 --> 00:03:42,140
Remember we initialize that H and W matrix randomly.

49
00:03:42,320 --> 00:03:47,690
So if you want to make sure you get the exact same topics I do go ahead and put a random state that

50
00:03:47,690 --> 00:03:50,920
matches my random state of 42.

51
00:03:51,370 --> 00:03:59,390
Then we're going to do is call an MF underscore model and we all fit it to that DTM and something you

52
00:03:59,390 --> 00:04:04,970
should notice is an MF performs faster than LDA for this particular data set.

53
00:04:05,150 --> 00:04:07,320
That's not to say it's not going to take a little bit of time.

54
00:04:07,340 --> 00:04:13,060
But it should be quite a bit faster than LDA especially because of the way pi works.

55
00:04:13,100 --> 00:04:17,040
It's really well suited for these sort of matrix factorization problems.

56
00:04:17,150 --> 00:04:21,560
So now that the IMF model has been fitted it's time to actually display the topics.

57
00:04:21,560 --> 00:04:27,320
So we're going to kind of skip the introduction to get feature names but remember just like with vectorization

58
00:04:27,680 --> 00:04:35,290
I can call T.F. idea and then say get feature names pasand some index for any particular word and we'll

59
00:04:35,330 --> 00:04:36,650
get that word back.

60
00:04:36,650 --> 00:04:43,130
So we're now going to show you how to actually grab the top 15 words per topic.

61
00:04:43,130 --> 00:04:53,070
So what we're going to do here is say for index Khama topic in enumerate and just like before.

62
00:04:53,110 --> 00:04:59,500
Off to an MF model we can call component's underscore essentially the exact same process as LDA.

63
00:04:59,830 --> 00:05:10,350
We're just going to do some extreme printing here and say the top 15 words for a topic number and then

64
00:05:10,440 --> 00:05:16,950
we'll passen index and then we're going to say prints and we'll print out a list comprehension here

65
00:05:17,400 --> 00:05:30,560
will say T.F. IDF get the feature names I for I in and will say topic thought arke sorts just like we

66
00:05:30,560 --> 00:05:36,180
did before and we'll say go back 15 and then go all the way to the end.

67
00:05:37,150 --> 00:05:40,220
So starting at the end go back 15 and then go all the way to the end.

68
00:05:40,240 --> 00:05:43,800
So those are essentially the values with the highest coefficients.

69
00:05:43,810 --> 00:05:49,480
So previously with LDA ruling with words to have the highest probabilities of belonging to a topic.

70
00:05:49,480 --> 00:05:55,750
Now with an M F redialing before it's to have the highest coefficient values inside of that matrix and

71
00:05:55,750 --> 00:05:59,050
then we'll go ahead and prints a new line.

72
00:06:00,300 --> 00:06:00,640
OK.

73
00:06:00,710 --> 00:06:04,380
So when you run this you should begin to see these words show up.

74
00:06:04,400 --> 00:06:08,690
Keep in mind the order of these words may be slightly different for you but in general they should be

75
00:06:08,690 --> 00:06:13,850
the same or it's things like research virus Zika disease show up in the first topic.

76
00:06:13,890 --> 00:06:18,130
Another thing to keep in mind is that these topics aren't going to match the topic from LDA.

77
00:06:18,200 --> 00:06:19,670
It's a completely different process.

78
00:06:19,820 --> 00:06:22,260
So we should expect completely different topics.

79
00:06:22,260 --> 00:06:26,210
However there should be some topics that tend to match up especially if you're dealing with the same

80
00:06:26,210 --> 00:06:27,060
data set.

81
00:06:27,080 --> 00:06:33,050
So we should see some topics that tend to work for maybe politics here we can see kind of health disease

82
00:06:33,370 --> 00:06:34,070
et cetera.

83
00:06:34,130 --> 00:06:35,990
Here we can see stuff on the election.

84
00:06:35,990 --> 00:06:39,390
Here we see a similar topic like music albums songs.

85
00:06:39,530 --> 00:06:43,670
And then we see something on education schools students college et cetera.

86
00:06:43,880 --> 00:06:44,670
OK.

87
00:06:44,750 --> 00:06:49,880
So again it's up to you to interpret what these words actually mean for a topic.

88
00:06:49,970 --> 00:06:55,520
And usually it's pretty obvious if you're have a good domain experience with whatever you're working

89
00:06:55,520 --> 00:06:56,530
with.

90
00:06:56,630 --> 00:07:01,760
Now what we want to do is simply attach the discovery topic labels so the original articles.

91
00:07:01,760 --> 00:07:04,510
And that itself is pretty straight forward as well.

92
00:07:04,580 --> 00:07:14,480
We simply same topic results is equal to an IMF model and then we just call it transform on that original

93
00:07:14,510 --> 00:07:20,070
DTM variable that we created and then we same topic results.

94
00:07:20,800 --> 00:07:27,250
Recall that if we actually just grab one of these topic results it's essentially a kind of coefficient

95
00:07:27,250 --> 00:07:35,860
value for the top topic that's representative and what we want is the index position of the most representative

96
00:07:35,860 --> 00:07:37,360
target or topic.

97
00:07:37,360 --> 00:07:40,060
So we call RMX just as we did before.

98
00:07:40,300 --> 00:07:47,350
And if we want to do this across the entire array you can simply say RMX x is equal to 1 so that we

99
00:07:47,350 --> 00:07:51,830
run this along columns and then we have the index positions that match up to these topics.

100
00:07:51,850 --> 00:07:54,770
Same exact thing we did before for LDA.

101
00:07:55,270 --> 00:08:05,850
And then we're just simply going to say pure topic is equal to topic underscore results RMX access equal

102
00:08:05,850 --> 00:08:06,800
to one.

103
00:08:07,290 --> 00:08:13,720
We check out our data frame flips head of the data frame and there can see the science topics.

104
00:08:13,720 --> 00:08:18,800
Again these numbers 1 3 6 are essentially just arbitrary numbers that really mean anything.

105
00:08:18,820 --> 00:08:22,710
They only mean something in context of the words with the highest coefficient values.

106
00:08:22,900 --> 00:08:27,880
And so I mean to keep in mind topic number six if an F has nothing to do with topic number six with

107
00:08:27,910 --> 00:08:28,630
LDA.

108
00:08:28,630 --> 00:08:31,800
These are just kind of arbitrary topic number assignments.

109
00:08:31,810 --> 00:08:38,560
The main thing you have to do is decide OK I'm going to label topic number 6 as education and what you

110
00:08:38,560 --> 00:08:43,980
could do if you really wanted to you could create a little dictionary something like this.

111
00:08:43,980 --> 00:08:53,060
You could say one is to and then search here for Topic number one and say GOP Pence President Russia

112
00:08:53,070 --> 00:08:55,850
et cetera and decide what to do with politics.

113
00:08:55,890 --> 00:09:04,520
So you say topic one has to do with politics and then you'd go off topic number two and decide on something

114
00:09:04,520 --> 00:09:04,820
for that.

115
00:09:04,820 --> 00:09:11,230
So some other topic and you would essentially keep doing this dictionary for topics zero through six

116
00:09:11,630 --> 00:09:17,750
and then what you could do is if you wanted to once that dictionary was Creedon say topic and then say

117
00:09:18,050 --> 00:09:29,170
map and you could basically call this my topic dictionary and you would map my topic dictionary and

118
00:09:29,170 --> 00:09:35,070
then you could create this as the actual Like topic label.

119
00:09:35,290 --> 00:09:36,890
So let's actually show you what this would look like.

120
00:09:36,940 --> 00:09:38,310
Quickly create some topic.

121
00:09:38,320 --> 00:09:50,320
So these have 0 1 2 4 5 and 6 and I going to briefly just kind of try to summarize this.

122
00:09:50,330 --> 00:09:54,260
So looks like Topic number 6 positive education so I'll say edu.

123
00:09:54,330 --> 00:09:56,250
Topic number five looks like s do music.

124
00:09:56,280 --> 00:09:58,300
So I'll say music.

125
00:09:58,530 --> 00:10:02,330
Topic number four looks like s do the election kind of just generalizing here.

126
00:10:02,550 --> 00:10:06,980
So say election number three.

127
00:10:07,140 --> 00:10:08,030
Let's just guess.

128
00:10:08,050 --> 00:10:11,540
Looks like it has to do with Syria Middle East ISIS.

129
00:10:11,670 --> 00:10:13,810
So we'll go ahead and say politics here.

130
00:10:14,760 --> 00:10:15,440
I'll say poly.

131
00:10:15,480 --> 00:10:17,460
There's probably more than one politics.

132
00:10:17,460 --> 00:10:20,040
Number two Senate House people coverage.

133
00:10:20,040 --> 00:10:20,660
Medicaid.

134
00:10:20,830 --> 00:10:25,330
Oh go ahead and say this is health care legislation.

135
00:10:25,470 --> 00:10:26,540
So just say Ali G.

136
00:10:26,550 --> 00:10:28,440
I asked for legislation.

137
00:10:28,440 --> 00:10:30,140
Topic number one looks like this.

138
00:10:30,250 --> 00:10:34,770
Do the campaign so will say election.

139
00:10:34,990 --> 00:10:35,560
You can see it.

140
00:10:35,570 --> 00:10:41,750
Overall these articles tend to skew towards politics and then zero research virus study.

141
00:10:41,780 --> 00:10:45,960
So say it has to do with health.

142
00:10:46,040 --> 00:10:48,070
So now I have my topic dictionary.

143
00:10:48,140 --> 00:10:53,730
That essentially is mapping these numbers for the topic to some sort of string code.

144
00:10:54,020 --> 00:10:59,110
And then I'm going to create a new column topic label that maps that dictionary to the topic.

145
00:10:59,330 --> 00:11:06,050
So when I run this and then check out NPR ahead I get to see the topic labels that I created.

146
00:11:06,100 --> 00:11:11,740
So that's a quick and easy way to map whatever the side of the topic label to be to the actual number.

147
00:11:11,740 --> 00:11:12,360
All right.

148
00:11:12,580 --> 00:11:13,650
That's it for this lecture.

149
00:11:13,660 --> 00:11:18,340
I hope you saw that it's pretty easy that once you understand her perform LDA it should be pretty easy

150
00:11:18,340 --> 00:11:24,820
to then switch out for an MF and we could also see the MF was a little bit faster than LDA and that

151
00:11:25,090 --> 00:11:28,980
speed increase is going to be really obvious when you have a really huge data set.

152
00:11:28,990 --> 00:11:33,730
So coming up next we're going to have a project assessment working with a question dataset try to find

153
00:11:33,730 --> 00:11:38,680
topics for those questions and then we're going to use an MF to perform some topic modeling on that

154
00:11:38,680 --> 00:11:39,470
dataset.

155
00:11:39,520 --> 00:11:41,890
So we'll have an overview of that assessment coming up next.

156
00:11:41,950 --> 00:11:42,450
We'll see other.

