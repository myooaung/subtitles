WEBVTT
ï»¿1
00:00:05.370 --> 00:00:06.300
Welcome back everyone.

2
00:00:06.330 --> 00:00:09.130
This lecture on Spacey basics.

3
00:00:09.280 --> 00:00:11.530
There are a few key steps forward going to space.

4
00:00:11.640 --> 00:00:13.250
We're going to cover in this lecture.

5
00:00:13.540 --> 00:00:15.880
The first step is loading the language library.

6
00:00:15.880 --> 00:00:20.830
Remember you need to have install the language library as shown in the spacey setup lecture.

7
00:00:20.830 --> 00:00:26.050
Then once you've loaded in that language library we build the pipeline object and from that pipeline

8
00:00:26.050 --> 00:00:32.860
object we can use tokens perform parts speech tagging and understand different token attributes.

9
00:00:32.870 --> 00:00:39.230
So as I mentioned Spacey works if a pipeline object and the main idea here is that there is an end of

10
00:00:39.310 --> 00:00:41.560
the function that we're going to create from space.

11
00:00:41.720 --> 00:00:47.480
That is going to automatically take in raw text and perform a series of operations that is going to

12
00:00:47.480 --> 00:00:56.150
tag parse and describe the text data and those operations are known as tokenization parsing piece of

13
00:00:56.150 --> 00:00:57.940
speech recognition and so on.

14
00:00:58.010 --> 00:01:00.980
And we're going to cover each of those and a lot more detail later on.

15
00:01:02.150 --> 00:01:08.300
So our purpose for this lecture is is to discover the pipeline object and various series of operations

16
00:01:08.660 --> 00:01:09.950
in subsequent lectures.

17
00:01:09.950 --> 00:01:14.590
We're going to dive a lot deeper into each of these individual aspects of an LP and spacey.

18
00:01:14.630 --> 00:01:20.510
So we're going to talk about tokenization parts of speech or POS stemming limitation and a lot more

19
00:01:20.510 --> 00:01:21.380
detail.

20
00:01:21.410 --> 00:01:25.330
So keep in mind we're going to introduce those terms right now in this lecture but we're going to Diven

21
00:01:25.390 --> 00:01:28.080
to a lot more greater detail later on.

22
00:01:28.170 --> 00:01:29.500
OK let's get started.

23
00:01:29.520 --> 00:01:31.290
But opening up a Jupiter notebook.

24
00:01:31.480 --> 00:01:32.460
OK here I am.

25
00:01:32.510 --> 00:01:39.000
What is essentially the repo for the course underneath the python basics the very first note book is

26
00:01:39.000 --> 00:01:40.460
called space basics.

27
00:01:40.500 --> 00:01:43.360
That's the notebook we're going to be going over in this lecture.

28
00:01:43.410 --> 00:01:46.690
I highly encourage you to also check out that notebook on your own.

29
00:01:46.990 --> 00:01:51.320
There's lots of useful information including installation and set up information that we cover in the

30
00:01:51.320 --> 00:01:52.580
space the set up lecture.

31
00:01:52.680 --> 00:01:57.540
But there's also a table so if you start scrolling down you'll eventually find tables that talk a little

32
00:01:57.540 --> 00:02:00.510
bit more about tokens and tokenization.

33
00:02:00.690 --> 00:02:03.680
So definitely take a look at this as we go along here.

34
00:02:03.750 --> 00:02:09.510
I'm going to open up a new notebook just create a new untitled notebook here and we're going to do is

35
00:02:09.510 --> 00:02:11.160
essentially go along that lecture.

36
00:02:11.160 --> 00:02:12.160
So let's get started.

37
00:02:13.290 --> 00:02:16.100
The first thing we need to do is actually import Spacey.

38
00:02:16.440 --> 00:02:21.200
So we'll say import Spacey and then we're going to load the language library.

39
00:02:21.330 --> 00:02:30.870
We will say an LP is equal to Spacey load and then we're going to pasan a specific string an underscore

40
00:02:30.900 --> 00:02:37.290
core which stands for core English language and then Web S-M which is essentially the small version

41
00:02:37.290 --> 00:02:38.510
of this language library.

42
00:02:38.760 --> 00:02:40.490
And then we're going to hit enter here.

43
00:02:41.100 --> 00:02:46.670
And don't worry if this takes a long time to load the very first time you run it it usually takes a

44
00:02:46.670 --> 00:02:47.040
while.

45
00:02:47.040 --> 00:02:52.050
It's a fairly large library but it's also part of what makes space so efficient is that a lot of what

46
00:02:52.050 --> 00:02:55.920
it's running on top of it's preloaded into this library.

47
00:02:55.920 --> 00:03:00.930
The next step we're going to do is create a doc object or a document object.

48
00:03:00.930 --> 00:03:07.650
So we'll create a variable called the OSI and we're going to pasan a unicode string.

49
00:03:07.650 --> 00:03:16.110
That means we prefer that string with a U and we're just going to pass in a string that says Tesla is

50
00:03:16.110 --> 00:03:18.270
looking at buying

51
00:03:20.910 --> 00:03:28.340
U.S. startup for dollar sign 6 million.

52
00:03:28.830 --> 00:03:35.170
OK so what's actually going to happen here is using the language library that we just loaded that spacey

53
00:03:35.170 --> 00:03:35.980
developed.

54
00:03:36.160 --> 00:03:42.160
It's going to essentially parse this entire stream into separate components for us and it's going to

55
00:03:42.670 --> 00:03:45.640
parse it into what are known as tokens essentially.

56
00:03:45.790 --> 00:03:48.830
Each of these little words is going to become a token.

57
00:03:48.850 --> 00:03:57.630
So what it can do is say for token in doc so I can actually iterate through this document object.

58
00:03:57.830 --> 00:04:05.210
And then I can print out the token and there's various attributes I can actually grab from each token.

59
00:04:05.490 --> 00:04:12.810
So for example I can grab the token text and the token text is actually the raw text that it grabs notice

60
00:04:12.960 --> 00:04:21.550
that it's smart enough to actually treat you as Dot these capital U and S as a single token Spacey threw

61
00:04:21.570 --> 00:04:25.970
a lot of development is actually smart enough to realize that when we say something like capital you

62
00:04:25.970 --> 00:04:29.070
dot capital S dot we're talking about the country.

63
00:04:30.120 --> 00:04:35.190
It's also smart enough to realize that this dollar sign and the six should probably be separated the

64
00:04:35.190 --> 00:04:41.720
dollar sign stands for US dollars and the stands for an amount and then million stands for another amount.

65
00:04:41.850 --> 00:04:45.030
And later on we'll see that with named entity recognition.

66
00:04:45.030 --> 00:04:50.040
Spacey can actually tell things like Tesla as being part of a company.

67
00:04:50.110 --> 00:04:54.970
So we're going to hear it's a print token that XTi and then we're also going to print out some more

68
00:04:54.970 --> 00:04:55.320
stuff.

69
00:04:55.330 --> 00:05:01.080
Let's go ahead and print out token POS which stands for part of speech.

70
00:05:01.510 --> 00:05:07.480
So when we run that we now see this ninety five ninety nine ninety nine.

71
00:05:07.570 --> 00:05:11.920
And later on we're going to see that each of these numbers actually corresponds to the parts of speech

72
00:05:11.950 --> 00:05:20.560
like an adverb a verb a noun conjugation etc. if you actually want the Ronayne used in is say POS underscore

73
00:05:21.590 --> 00:05:24.320
Bruhn that and it will tell you what part of speech it is.

74
00:05:24.340 --> 00:05:31.470
So is it smart enough to know that Tesla is a proper noun is a verb looking a verb proper noun for us

75
00:05:31.480 --> 00:05:33.330
start up as a noun and so on.

76
00:05:33.470 --> 00:05:38.510
Still smart enough to realize that million here is a number and so is six.

77
00:05:38.560 --> 00:05:40.610
And then the star sign is a symbol.

78
00:05:40.810 --> 00:05:45.880
So already right off the bat Spacey knows a lot of information about each of these tokens.

79
00:05:47.290 --> 00:05:57.790
And then you can also do things like print tokens dot and if you do deeply underscore it's going to

80
00:05:57.790 --> 00:05:59.900
give you even more information.

81
00:06:01.040 --> 00:06:07.310
Anti-peace stands for syntactic dependency and we'll talk about that in more detail when we talk about

82
00:06:07.310 --> 00:06:08.720
specific tokens.

83
00:06:08.720 --> 00:06:14.810
But hopefully now you can see that it's really incredible the capabilities of spacey along of natural

84
00:06:14.810 --> 00:06:19.330
language processing to grab a lot of information just from a simple string.

85
00:06:19.520 --> 00:06:23.970
It recognize that Tesla is a proper noun not just a word at the start of a sentence.

86
00:06:23.990 --> 00:06:28.430
So Tesla here is capitalized not just because it's the start of a sentence but also because it's a proper

87
00:06:28.430 --> 00:06:28.770
noun.

88
00:06:28.940 --> 00:06:34.380
And it was able to tell that it's also understood that us these dots don't separate it.

89
00:06:34.430 --> 00:06:35.440
It's a single entity.

90
00:06:35.450 --> 00:06:36.780
It's a single token.

91
00:06:36.850 --> 00:06:41.210
And as we dive deeper into Spacey we're going to see where each of these abbreviations mean and how

92
00:06:41.210 --> 00:06:42.080
did arrives.

93
00:06:42.350 --> 00:06:48.380
We'll also see how Spacey can interpret the last three tokens combine to six million dollar sign that

94
00:06:48.380 --> 00:06:54.720
six million it's going to be able to understand that all of this is some sort of quantifier of money.

95
00:06:55.170 --> 00:06:59.760
But let's go ahead and move on and discuss the pipeline object.

96
00:06:59.780 --> 00:07:05.780
So after I'm putting the space module in the cell above we loaded a model and named an LP.

97
00:07:05.990 --> 00:07:11.930
So this actual line of code is known as loading a model and we called it an LP.

98
00:07:11.940 --> 00:07:19.520
Next we created a dock or a document object by applying this model in LP to our text.

99
00:07:19.560 --> 00:07:26.780
Spacey also builds a companion vocab object for vocabulary and we'll cover that in later lectures.

100
00:07:27.050 --> 00:07:33.430
This doc object that we created holds the process text and that's really the focus of this lecture.

101
00:07:33.650 --> 00:07:43.070
So off of this and help the object Well we can do is call DOT pipeline.

102
00:07:43.160 --> 00:07:49.400
So when we run an LP R-Texas entering a processing pipeline the first breaks down the text and then

103
00:07:49.400 --> 00:07:55.490
performs that series of operations of tagging parsing and describing that data so we can see here the

104
00:07:55.490 --> 00:08:02.540
basic a.p pipeline is a tagger a parser and then any R which stands for named entity recognizer and

105
00:08:02.540 --> 00:08:05.720
we'll talk about each of these in a lot more detail later on.

106
00:08:05.990 --> 00:08:14.030
And you can also just get the basic names by saying a.p LP pipe underscore names and you can play around

107
00:08:14.840 --> 00:08:20.260
with the various attributes here which we're going to talk about as we continue on throughout the course.

108
00:08:20.270 --> 00:08:24.010
The first one I want to discuss just quickly is tokenization.

109
00:08:24.010 --> 00:08:29.560
So the very first step in processing any text is to split it up all the component parts.

110
00:08:29.690 --> 00:08:36.410
That is the words and punctuation into tokens and these tokens are annotated inside the doc object to

111
00:08:36.440 --> 00:08:38.980
contain descriptive information.

112
00:08:39.050 --> 00:08:40.870
So let's go ahead and see an example.

113
00:08:41.750 --> 00:08:48.260
I'm going to create an other document talk to an We're going to Pessin a unicode string means it starts

114
00:08:48.260 --> 00:08:56.690
a few and we'll say Tesla isn't looking into startups anymore.

115
00:08:58.580 --> 00:08:59.290
We run that.

116
00:08:59.510 --> 00:09:06.640
And then let's go ahead and print out the tokens for token doc to and we're going to print out same

117
00:09:06.640 --> 00:09:15.930
information we printed out last time which was the text to speech and then the syntactic dependency.

118
00:09:15.940 --> 00:09:21.790
So if you're on that you can see again Tesla it's a proper noun noun subject and then it's actually

119
00:09:21.790 --> 00:09:29.890
able to understand that is and an apostrophe t is actually a conjunction and it's going to be able to

120
00:09:29.890 --> 00:09:34.870
keep them separate as well as know the relationship between these two parts of these tokens.

121
00:09:34.870 --> 00:09:37.650
So it's really advance what space is doing here.

122
00:09:37.660 --> 00:09:40.560
So again notice that isn't it really.

123
00:09:40.630 --> 00:09:47.530
Actually it's been split to two tokens and spatially recognizes both the root verb is and the negation

124
00:09:47.620 --> 00:09:48.320
attached to it.

125
00:09:48.370 --> 00:09:54.960
So it understands is the root verb and that this is a negation this an apostrophe T.

126
00:09:55.060 --> 00:10:00.970
Notice also that both the extended whitespace and the period at the end of the sentence are assigned

127
00:10:00.970 --> 00:10:02.090
their own tokens.

128
00:10:02.110 --> 00:10:09.790
So if I were to put in a lot of extended whitespace here and run this again this space would actually

129
00:10:09.790 --> 00:10:11.760
become a token with Speccy.

130
00:10:11.800 --> 00:10:14.080
And then the stop is a punctuation.

131
00:10:14.080 --> 00:10:19.410
Now I want to point out that here we're iterating for every token inside of this document object.

132
00:10:19.600 --> 00:10:24.130
But we can also use indexing taxi grab tokens individually.

133
00:10:24.190 --> 00:10:30.130
So if I take my document object I can use indexing to grab the very first token off of this and by default

134
00:10:30.160 --> 00:10:31.740
it returns that token text.

135
00:10:31.840 --> 00:10:38.440
But just as before or offer that token object I can ask for attributes like part of speech and it returns

136
00:10:38.440 --> 00:10:40.840
back prop and which stands for proper noun.

137
00:10:40.990 --> 00:10:46.930
And if you Cheka are spacey BASIX notebook and scroll to the section we're at right now which is learning

138
00:10:46.930 --> 00:10:50.570
about parts of speech tacking dependencies.

139
00:10:50.620 --> 00:10:55.960
Additional token attributes and so on we have a couple of useful links here for understanding the different

140
00:10:56.020 --> 00:10:56.890
notations.

141
00:10:56.890 --> 00:11:04.840
So here we saw that there's a part of speech like Propp and for proper noun verb space noun and so on.

142
00:11:05.020 --> 00:11:09.390
You can check out the link in the note book that we provide here for annotations specific.

143
00:11:09.610 --> 00:11:12.700
And there's also a variety of languages here English German et cetera.

144
00:11:13.060 --> 00:11:19.840
You can just expand English I can tell you the part of speech the morphology and description so you

145
00:11:19.840 --> 00:11:23.370
can take a look at all of this to see what all of this stands for.

146
00:11:23.380 --> 00:11:28.330
So if you see pro and that stands for a pronoun personal and then so on.

147
00:11:28.330 --> 00:11:32.700
You can keep going on in more detail and it does this for a wide variety of languages.

148
00:11:33.130 --> 00:11:37.660
So there's Parsa speech tagging that's available to us and we'll cover parts of speech in a lot more

149
00:11:37.660 --> 00:11:39.430
detail later on on its own.

150
00:11:39.430 --> 00:11:46.490
There's also syntactic dependencies for each token and that as I mention is just going to be the call.

151
00:11:46.510 --> 00:11:53.320
For example if we grab the first item here we can say the IP underscore this is the syntactic dependency.

152
00:11:53.370 --> 00:11:58.750
Again there's a link inside the Speccy notebooks you can go here dependencies get a full list of syntactic

153
00:11:58.750 --> 00:11:59.860
dependencies.

154
00:11:59.860 --> 00:12:06.010
So right now those are only available for English and German so you can expand English and see the label

155
00:12:06.280 --> 00:12:07.950
as well as description.

156
00:12:07.960 --> 00:12:13.390
We've also provided in Sodor Spacey notebook a good explanation of what these dependencies actually

157
00:12:13.390 --> 00:12:14.620
stand for.

158
00:12:14.620 --> 00:12:18.670
Again this has more to do if a really good understanding of the way the English language works which

159
00:12:18.670 --> 00:12:19.980
can be really complicated.

160
00:12:20.000 --> 00:12:24.880
But this isn't really an English course so we're not good dive too much into the details but this link

161
00:12:24.880 --> 00:12:29.740
here that we provide for you is the full paper explaining the various dependencies and what they stand

162
00:12:29.740 --> 00:12:37.300
for things like a causal subject a dependent a causal passive subject and so on this sort of level of

163
00:12:37.300 --> 00:12:41.830
detail is sometimes really important if you're trying to build a chat bot that can understand things

164
00:12:41.830 --> 00:12:44.590
like present versus past tense and so on.

165
00:12:44.650 --> 00:12:49.500
Keep mindspace basically doing this all this work for us under the hood.

166
00:12:49.580 --> 00:12:54.650
So we'll come back to our untitled notebook and the last thing I want to mention is that there's lots

167
00:12:54.650 --> 00:12:56.810
of other additional token attributes.

168
00:12:56.810 --> 00:13:01.020
So far we've seen parts of speech dependencies and a couple of more.

169
00:13:01.310 --> 00:13:06.680
If you come back to the Speccy basics notebook and scroll down here we have an entire list of the different

170
00:13:06.680 --> 00:13:08.680
tags and description.

171
00:13:08.990 --> 00:13:13.970
So for example if we call it that text that's is the original word text which in this case would be

172
00:13:13.970 --> 00:13:14.780
Tesla.

173
00:13:15.080 --> 00:13:20.660
If you want the lemma that's going to give you the limitation or the base form of the word and later

174
00:13:20.660 --> 00:13:23.200
on we're going to talk about limitation and a lot more detail.

175
00:13:23.330 --> 00:13:26.460
Lotusphere that essentially the result is just lower case thingness.

176
00:13:26.760 --> 00:13:32.840
There's the part of speech a simple speech tag if you tag underscore that actually gives you a detailed

177
00:13:32.840 --> 00:13:33.970
part of speech tag.

178
00:13:34.190 --> 00:13:39.500
So beyond just proper noun they'll tell you proper singular so understands that Tesla is actually a

179
00:13:39.740 --> 00:13:42.860
singular object then you can ask for the shape.

180
00:13:42.860 --> 00:13:46.130
So things like capitalization punctuation digits.

181
00:13:46.130 --> 00:13:48.640
So the way it shows that is it shows that capital X..

182
00:13:48.680 --> 00:13:53.540
So in the content the first letter is capitalized and then lowercase X for everything else being lowercase

183
00:13:53.770 --> 00:13:56.600
and you can also ask is Alpha or is stop.

184
00:13:56.600 --> 00:14:01.800
So is Alpha answer the question is the token alpha character so is alphanumeric.

185
00:14:01.840 --> 00:14:07.250
It's actually just an alphabetical character excuse me and that is stop asks Who is the token part of

186
00:14:07.250 --> 00:14:08.010
the stop list.

187
00:14:08.030 --> 00:14:11.350
So one of the most common words of whatever language you're working with.

188
00:14:11.400 --> 00:14:14.660
So Tesla is definitely not one of the most common words in English language.

189
00:14:14.810 --> 00:14:15.900
So it's false.

190
00:14:15.920 --> 00:14:19.380
So you have all these variety of tags you can call off of any of these tokens.

191
00:14:19.550 --> 00:14:25.440
And Spacey did this all automatically for us the very minute that he passed it into this an LP.

192
00:14:25.480 --> 00:14:30.200
And again it's doing this all based off the fact that we loaded in this library which is why it took

193
00:14:30.200 --> 00:14:32.880
some time but that's what makes Spacey so efficient.

194
00:14:32.900 --> 00:14:41.680
It loads up the library first for us now large stock objects can be hard to work if sometimes a span

195
00:14:41.770 --> 00:14:47.330
is a slice of a dark object in the form Some start versus a stop.

196
00:14:47.410 --> 00:14:50.980
So I'm going to copy and paste because it's quite a large string.

197
00:14:50.980 --> 00:15:01.320
If we scroll down here to span's and I'm going to copy paste this cell that way we don't need to write

198
00:15:01.320 --> 00:15:02.570
it all again.

199
00:15:02.730 --> 00:15:05.390
And essentially we're just passing in a really long string.

200
00:15:05.400 --> 00:15:12.380
So although comly actually to John Lennon from his song beautiful boy and so on so I have this really

201
00:15:12.380 --> 00:15:20.900
large documents and what I may want to do is actually just grab a span of it so I can say life quote

202
00:15:22.600 --> 00:15:24.960
so notice here there's a quote here.

203
00:15:24.960 --> 00:15:30.120
Life is what happens to us while we are busy making other plans and we're going to say is life quote

204
00:15:30.180 --> 00:15:39.990
is equal to Doc 3 and starting at index 16 up to but not including index 30 and then I can print out

205
00:15:41.120 --> 00:15:46.050
the life quote and it gives me this quote Life is what happens.

206
00:15:46.050 --> 00:15:51.250
That's why we are making other plans so this is now a span of that overall document because it's documents

207
00:15:51.260 --> 00:15:51.870
quite large.

208
00:15:51.890 --> 00:15:56.280
And maybe we're only interested in this particular quote inside of that document.

209
00:15:56.330 --> 00:16:03.260
So what's really interesting is that even though we're only grabbing a section of this document Spacey

210
00:16:03.260 --> 00:16:06.800
is smart enough to know that this life is a spam.

211
00:16:06.860 --> 00:16:13.580
So if you check out the type of this life quote space is doing a lot of work under the hood to understand

212
00:16:13.580 --> 00:16:21.510
that this is a particular span unlike the entire document which we checked type of doc three understands

213
00:16:21.510 --> 00:16:21.630
it.

214
00:16:21.640 --> 00:16:23.230
That's the entire document.

215
00:16:23.270 --> 00:16:27.320
So when you take a slice out of this space it's going to be smart enough to understand that it's the

216
00:16:27.320 --> 00:16:34.700
span of a larger document and then certain tokens inside a document or Doc object may also receive a

217
00:16:34.700 --> 00:16:36.800
start a sentence tag.

218
00:16:36.800 --> 00:16:42.110
While this doesn't immediately build the list of sentences these tags enable the generation of sentence

219
00:16:42.110 --> 00:16:44.480
segments through doc thought.

220
00:16:44.530 --> 00:16:47.890
E.A. Yes and later we're going to write our own segmentation rules.

221
00:16:47.990 --> 00:16:49.740
But let me just show you a simple example.

222
00:16:51.450 --> 00:17:00.090
Well say talk for is equal to an LP create a unicode string and say this is the first sentence.

223
00:17:00.120 --> 00:17:01.920
Period space.

224
00:17:02.000 --> 00:17:04.000
This is an other sentence.

225
00:17:06.550 --> 00:17:07.600
Period space.

226
00:17:07.600 --> 00:17:10.940
This is the last sentence.

227
00:17:12.090 --> 00:17:16.410
So here we have three sentences inside her documents.

228
00:17:16.860 --> 00:17:19.500
And again Spacey doing a lot of work for us.

229
00:17:19.560 --> 00:17:25.050
It actually understands and is going to separate each of these sentences throughout the documents so

230
00:17:25.050 --> 00:17:36.270
I can say for sentence in doc for and if I just sit duck for those to go through every token but I can

231
00:17:36.270 --> 00:17:46.260
call the SE A.S. or sense attribute in order to print out each particular sentence so spacey automatically

232
00:17:46.260 --> 00:17:54.950
understands that this period and the space is a sentence and then I can say something like Doc for at

233
00:17:56.090 --> 00:17:57.950
index position 6.

234
00:17:58.430 --> 00:18:00.840
So that's this right here.

235
00:18:00.940 --> 00:18:01.800
That's this word right here.

236
00:18:01.810 --> 00:18:03.500
This room or these are tokens.

237
00:18:03.620 --> 00:18:06.670
So we have various tokens here I can call.

238
00:18:06.680 --> 00:18:10.540
Thought is underscore sent underscore.

239
00:18:10.550 --> 00:18:14.050
Start asking Is this the start of a sentence.

240
00:18:14.210 --> 00:18:16.120
If you're in that will return true.

241
00:18:16.850 --> 00:18:21.100
If I check it out for another token it's good to check it out for tokens 7.

242
00:18:21.140 --> 00:18:24.030
That's is this is right here.

243
00:18:24.160 --> 00:18:27.270
Make it more obvious by token 8 the word another.

244
00:18:27.440 --> 00:18:32.660
I'm going to ask K is this word another the start of a sentence.

245
00:18:32.660 --> 00:18:37.040
And if you run this it's not going to return anything it's going to return none because it is not the

246
00:18:37.040 --> 00:18:42.180
start of a sentence unlike when he passed and six it returns back true.

247
00:18:42.230 --> 00:18:48.110
So it's really incredible the capabilities a Speccy to take in a raw string and completely understand

248
00:18:48.110 --> 00:18:50.000
things like parts of speech.

249
00:18:50.030 --> 00:18:56.060
Name recognition token attributes where the sentence starts and ends and a lot more.

250
00:18:56.120 --> 00:18:58.240
So we just touch the tip of the iceberg.

251
00:18:58.400 --> 00:19:03.150
But I really hope this gets you excited about the potential capabilities of spacey.

252
00:19:03.230 --> 00:19:07.840
Coming up next we're going to dive in and a lot more detail to each of these particular components.

253
00:19:07.850 --> 00:19:08.810
I'll see you at the next lecture.

