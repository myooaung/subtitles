WEBVTT
ï»¿1
00:00:05.230 --> 00:00:10.960
Welcome back to this lecture on sentence segmentation in space the basics we saw briefly how Doc objects

2
00:00:10.990 --> 00:00:13.780
are divided into sentences in this lecture.

3
00:00:13.780 --> 00:00:18.670
We're going to learn how sentence segmentation works and how to set our own segmentation rules to break

4
00:00:18.670 --> 00:00:21.700
up Dock's into sentences based off our own rules.

5
00:00:21.700 --> 00:00:22.820
Let's go ahead and check this out.

6
00:00:22.840 --> 00:00:24.090
In a Jupiter notebook.

7
00:00:24.340 --> 00:00:29.780
OK here I am at the notebook and right now I imported Spacey I've loaded up the English library set

8
00:00:30.220 --> 00:00:36.280
and also I've created a doc object out of this string right here where I have this is the first sentence.

9
00:00:36.300 --> 00:00:37.400
This is another sentence.

10
00:00:37.450 --> 00:00:38.680
And then this is the last sentence.

11
00:00:38.680 --> 00:00:40.320
So there's three sentences here.

12
00:00:40.390 --> 00:00:50.230
And as we know from space the basics what I can do is say four sent in doc since printout sent where

13
00:00:50.260 --> 00:00:54.550
Doc that sense actually separates everything into sentences.

14
00:00:54.550 --> 00:01:00.310
Keep in mind that this doc dot se A.S. is actually a generator.

15
00:01:00.430 --> 00:01:06.700
So if you were to try to index off of sense it would actually fail.

16
00:01:06.850 --> 00:01:11.890
You'd get an error saying a generator object is not subscript will because this generates the sentences

17
00:01:11.980 --> 00:01:13.990
instead of holding them all in memory.

18
00:01:14.020 --> 00:01:18.240
That's a safe space because as you can imagine this document object can be huge.

19
00:01:18.250 --> 00:01:24.640
So you don't want all of the saved duplicated in memory instead you can iterate through the sentences.

20
00:01:24.640 --> 00:01:30.370
However for the actual document itself since it knows from the language library how to quickly look

21
00:01:30.370 --> 00:01:37.480
up tokens you can easily grab individual tokens from the dock but you can't grab sentences individually

22
00:01:38.650 --> 00:01:40.120
from that sense.

23
00:01:40.240 --> 00:01:46.830
If you really need to do this all you need to do is just pass this into a list so you can say list thok

24
00:01:46.840 --> 00:01:52.390
sense and then you gonna have a list here of the sentences they can then index off of order to grab

25
00:01:52.390 --> 00:01:53.470
the sentences.

26
00:01:53.470 --> 00:01:56.560
However keep in mind that if you actually look at the type here

27
00:01:59.380 --> 00:02:04.630
these are going to be Spacey spand objects not just normal strings.

28
00:02:04.630 --> 00:02:10.570
Now typically Spacey is really good at separating out sentences itself based off things like the start

29
00:02:10.570 --> 00:02:12.950
of a sentence or the end of a sentence.

30
00:02:13.000 --> 00:02:18.700
However you may have a custom dataset for sentence segmentation you want to follow your own particular

31
00:02:18.700 --> 00:02:19.450
set of rules.

32
00:02:19.510 --> 00:02:24.190
Maybe you want to segment's based off a semi-colon which isn't technically the stop of a sentence.

33
00:02:24.190 --> 00:02:30.780
Well let's actually create a new object that is going to be a document object.

34
00:02:30.830 --> 00:02:35.630
It's going to be Unicode and I'm going to use single quotes on the outside because it's going to be

35
00:02:35.660 --> 00:02:38.600
kind of an interesting set of strings.

36
00:02:38.620 --> 00:02:46.290
Well we're going to do is say have a little quote here like management is doing the right things.

37
00:02:46.460 --> 00:02:52.080
Semi-colon leadership is doing the right things.

38
00:02:53.090 --> 00:02:56.220
And then this is itself going to be a quote inside of this text.

39
00:02:56.210 --> 00:02:58.620
I'm going to zoom out so you can see the effect here.

40
00:02:58.760 --> 00:03:05.520
And then I'm going to attribute this for dash to Peter Drucker.

41
00:03:05.710 --> 00:03:06.700
So we're going to run that.

42
00:03:06.940 --> 00:03:09.860
And if we take a look at the actual text of this document.

43
00:03:09.880 --> 00:03:14.450
So if I just say Doc text I can see management is doing the right things.

44
00:03:14.500 --> 00:03:16.990
Semi-colon leadership's doing right things.

45
00:03:16.990 --> 00:03:19.590
This first phrase is inside its own set of double quotes.

46
00:03:19.630 --> 00:03:25.480
And then we see that Peter Drucker first let's see what happens with the default settings of sentence

47
00:03:25.480 --> 00:03:26.980
segmentation.

48
00:03:26.980 --> 00:03:40.410
If I say force sent in doc since Prince the sentence and then Prince a new line if I run that notice

49
00:03:40.410 --> 00:03:46.180
it got separated out into two sentences the first one is the long quote and the second one is this dash

50
00:03:46.230 --> 00:03:48.370
Peter Drucker that may be right.

51
00:03:48.390 --> 00:03:51.810
That may be wrong depending on your particular use case.

52
00:03:51.840 --> 00:03:58.460
So let's show you how to add a new rule to the pipeline that gets created when you wrap this in an LP.

53
00:03:58.480 --> 00:04:01.690
So now we're going to do is show you basically two approaches.

54
00:04:01.690 --> 00:04:10.830
One is to add a segmentation rule basically adding a new thing to segment on and the other is essentially

55
00:04:10.830 --> 00:04:12.220
changed the rules entirely.

56
00:04:12.270 --> 00:04:21.060
So change segmentation rules so by default you can see from previous examples if we scroll up that we

57
00:04:21.060 --> 00:04:25.270
split essentially on periods and white spaces here to see the sentences.

58
00:04:25.380 --> 00:04:30.540
So maybe that's not the case in which case you'll need to replace all the rules but maybe you also want

59
00:04:30.540 --> 00:04:35.700
to separate on the normal separations but add in something like separating on semi-colons.

60
00:04:35.700 --> 00:04:39.660
So we're going to show you first how to add a segmentation rule that will show you how to change all

61
00:04:39.660 --> 00:04:41.090
the segmentation rules.

62
00:04:41.100 --> 00:04:45.520
Let's first show you how to add a segmentation rule and do that we need to create a function.

63
00:04:45.570 --> 00:04:51.800
We'll start off by calling our functions set custom underscore boundaries.

64
00:04:51.930 --> 00:04:57.450
And is going to accept a document object and I want to show you a couple of things that we can do with

65
00:04:57.450 --> 00:04:58.710
a document object.

66
00:04:58.710 --> 00:05:04.280
The first thing is that every token inside a document actually retains its index position.

67
00:05:04.290 --> 00:05:13.200
So for example I can say for token in documents print out token I and if we actually then run set custom

68
00:05:13.200 --> 00:05:20.460
boundaries on our current documents you'll see here that every token contains its index position.

69
00:05:20.460 --> 00:05:27.840
So for example I can print the token and the tokens so we can see the first set of double quotes is

70
00:05:27.850 --> 00:05:28.600
0.

71
00:05:28.600 --> 00:05:32.760
This management is token number one is a token number two and so on.

72
00:05:32.770 --> 00:05:33.510
So we have this.

73
00:05:33.590 --> 00:05:37.440
I for the index position which is going to be useful as you'll see in a second.

74
00:05:37.780 --> 00:05:43.480
So what we're going to do is we'll say for tokin in doc but we're going to do some indexing here on

75
00:05:43.480 --> 00:05:44.560
this doc object.

76
00:05:44.620 --> 00:05:47.330
Say colon negative 1.

77
00:05:47.470 --> 00:05:53.560
And the reason for that is if we actually take a look at document colon negative one this goes all the

78
00:05:53.560 --> 00:05:59.470
way from the beginning up to but not including the very last token and you'll see why we have to do

79
00:05:59.470 --> 00:06:00.360
that in just a second.

80
00:06:00.390 --> 00:06:07.090
We're going to do is we'll go through every tokin up to and including the last one and we'll say if

81
00:06:07.090 --> 00:06:13.180
tokin text is equal to and then we're going to add a new segmentation rule so we'll say if it's equal

82
00:06:13.180 --> 00:06:23.330
to a semi colon Well we're going to do is say that documents at that index position plus one.

83
00:06:23.440 --> 00:06:29.720
So that very next one will say is sentence are sent.

84
00:06:29.740 --> 00:06:33.070
Start is equal to true.

85
00:06:33.110 --> 00:06:38.080
So essentially all we're doing is we're going along through every tokin inside of this document.

86
00:06:38.120 --> 00:06:43.640
And once we see a semi colon we're going to say OK you just saw a semi colon the very next word or token

87
00:06:43.640 --> 00:06:49.200
after that that's going to be the start of a new sentence which is why we're saying token I plus one.

88
00:06:49.460 --> 00:06:55.370
And since we're saying I plus one we only need to go up to Peter because the index of Peter plus one

89
00:06:55.370 --> 00:06:56.820
is going to be his last name trucker.

90
00:06:56.990 --> 00:07:01.460
So that's why we're going from the beginning all the way up to the end minus the very last word because

91
00:07:01.460 --> 00:07:03.460
we're at ease this plus one.

92
00:07:03.740 --> 00:07:09.580
And we want to make sure that we don't actually go past the documents.

93
00:07:09.590 --> 00:07:12.870
Otherwise we'll get some index out of range here.

94
00:07:13.040 --> 00:07:17.420
So again we're going through each of these tokens once we says some I call and we're going to say that

95
00:07:17.420 --> 00:07:21.530
very next word such as leadership that's going to be the start of a sentence.

96
00:07:21.530 --> 00:07:23.980
We'll set that attribute equal to true.

97
00:07:24.080 --> 00:07:30.330
And then at the end once you're done going through for a loop we're going to return the documents so

98
00:07:30.330 --> 00:07:31.420
we know how to run that.

99
00:07:31.890 --> 00:07:38.550
And then you're going to call an LP and you're going to say add underscore pipe and you get a passing

100
00:07:38.870 --> 00:07:49.780
custom boundaries and you're going to say before equals parser and then we can say and O.P. that pipe

101
00:07:49.960 --> 00:07:54.460
underscore names and you can see here we have the tagger.

102
00:07:54.460 --> 00:08:00.160
So that happens first then our custom boundaries happens the pipeline then the parser and then the named

103
00:08:00.220 --> 00:08:01.360
entity recognition.

104
00:08:01.540 --> 00:08:07.290
Notice that tagger parser and the recognition those all happen on the pipeline automatically.

105
00:08:07.300 --> 00:08:10.390
So let's go ahead and rerun the document creation.

106
00:08:10.660 --> 00:08:17.350
So before we actually get to changing segmentation rules we're going to do is create a couple more cells

107
00:08:18.550 --> 00:08:22.050
here and say Doc 4.

108
00:08:22.540 --> 00:08:27.950
So brand new documents in LP and we're going to copy and paste this long quote.

109
00:08:29.240 --> 00:08:33.310
So copy that paste it in.

110
00:08:33.420 --> 00:08:38.900
So I had this new document for and then what I'm going to do is say the following.

111
00:08:39.850 --> 00:08:47.070
Four cents in doc for cents print sent.

112
00:08:47.660 --> 00:08:52.160
So when you run that notice now it's separating on that semi-colon.

113
00:08:52.160 --> 00:08:59.810
When I ran the original documents and asked it to print sentences it printed out the entire quote and

114
00:08:59.810 --> 00:09:01.050
then Peter Drucker.

115
00:09:01.250 --> 00:09:08.210
But now I've created a new pipeline set my custom boundaries inside of the pipeline and then again read

116
00:09:08.210 --> 00:09:13.320
the fine documents and then you can see here it's separating on the semi colon.

117
00:09:13.340 --> 00:09:16.220
So that's how you can add a segmentation rule.

118
00:09:17.130 --> 00:09:20.910
So what happens if you want to change the rules completely.

119
00:09:20.910 --> 00:09:25.500
So in some cases we want to replace spaces default sensitizer with our own set of rules.

120
00:09:25.590 --> 00:09:30.450
So we're going to see now how the default sensitizer breaks on periods and they're going to replace

121
00:09:30.450 --> 00:09:34.310
that behavior of the sense incisor to maybe break on something like line breaks.

122
00:09:35.080 --> 00:09:44.550
So again we're going to say an LP is equal to space e the load and I'm reloading the English library.

123
00:09:44.620 --> 00:09:48.760
So I kind of reset to the original default behavior.

124
00:09:48.970 --> 00:09:52.960
Otherwise my cousin boundaries that I created here is going to be messed up.

125
00:09:53.200 --> 00:09:54.740
So I've reloaded this.

126
00:09:54.760 --> 00:10:02.800
Ran that line again and then we're going to create a string will say my string is a unicode string and

127
00:10:02.800 --> 00:10:06.220
says this is a sentence.

128
00:10:06.220 --> 00:10:12.580
This is another and we're actually going to then add in some line breaks here so I add in some line

129
00:10:12.580 --> 00:10:13.690
breaks.

130
00:10:13.780 --> 00:10:19.400
Add two in a row and then say this is a we'll do another line break.

131
00:10:19.850 --> 00:10:23.010
We'll say third sentence.

132
00:10:23.300 --> 00:10:23.580
OK.

133
00:10:23.590 --> 00:10:29.500
So if I were actually prints my string here it says this is a sentence this is another.

134
00:10:29.500 --> 00:10:36.280
This is a third sentence you can imagine for something like a text dataset of poetry line breaks like

135
00:10:36.280 --> 00:10:40.320
this are actually more important than the periods.

136
00:10:40.360 --> 00:10:47.710
And you may want to the fine line breaks themselves as the actual end of a sentence instead of what

137
00:10:47.710 --> 00:10:50.030
is classically known as the end of a sentence which is a period.

138
00:10:50.050 --> 00:10:52.740
So for something like approach with data set this could be really important.

139
00:10:52.750 --> 00:10:56.790
Now let's see what happens if we check out the default behavior first.

140
00:10:56.830 --> 00:11:03.640
We're going to say Doc is equal to an LP will pass in my string and now take a look at what the default

141
00:11:03.670 --> 00:11:07.710
segmentation is will say for sentence.

142
00:11:08.050 --> 00:11:14.320
And talk about sense print out the sentence.

143
00:11:14.610 --> 00:11:20.140
And when we run this you'll notice that it's actually able to pick up on this double set of new lines

144
00:11:20.520 --> 00:11:22.680
but it automatically separates the first.

145
00:11:22.680 --> 00:11:24.280
This is a sentence from.

146
00:11:24.280 --> 00:11:25.470
This is another.

147
00:11:25.650 --> 00:11:31.820
And that's going to be an issue for us if we only want the new line to be the indicator of a new segment.

148
00:11:31.950 --> 00:11:36.800
So maybe you really want three segments here this one this one and this one.

149
00:11:37.080 --> 00:11:40.720
Instead of separating on standard segments.

150
00:11:40.980 --> 00:11:52.070
So if we want to change the rules we need to call from Spacey thought pipeline import sentence segments

151
00:11:52.070 --> 00:11:59.350
or and then we'll create a function that will basically tell Spacey and its pipeline what it should

152
00:11:59.350 --> 00:12:08.450
be using for segmentation and we'll call this function split on we'll call it split on new lines.

153
00:12:08.470 --> 00:12:12.900
It takes the documents we're going to find start at zero.

154
00:12:14.400 --> 00:12:19.620
And we'll say have I seen a new line or have I seen a new separator or whatever you want your segmentation

155
00:12:19.620 --> 00:12:25.530
to be and we'll start off as false and then we'll say for word in the documents

156
00:12:28.180 --> 00:12:32.370
we've seen a new line then this is going to be a generator.

157
00:12:32.380 --> 00:12:41.980
I will yield doc from start to the current words in the expedition and hear word you could replace it

158
00:12:41.980 --> 00:12:45.550
with token if that makes more sense to you it's just a variable name.

159
00:12:45.550 --> 00:12:52.330
And then we'll say the New START is equal to the word at that are the words in solution there and we'll

160
00:12:52.330 --> 00:13:05.400
reset seen new line equal to false and then we'll say oh if the word text and then we're going to call

161
00:13:05.400 --> 00:13:11.790
the starts with method which is just a normal method available often in Python string will say if the

162
00:13:11.790 --> 00:13:17.580
word text starts with a new line then we've just seen a new line.

163
00:13:17.620 --> 00:13:24.040
Set it to true and then at the end of this for loop we're going to yield

164
00:13:27.660 --> 00:13:31.860
from start all the way to the end.

165
00:13:31.860 --> 00:13:33.340
So do we actually doing here.

166
00:13:33.510 --> 00:13:34.550
We start at zero.

167
00:13:34.560 --> 00:13:38.250
We haven't seen a new line yet or a new segmentation so we'll set that equal to false.

168
00:13:38.460 --> 00:13:43.470
And then for every token or word in that document if we've already seen the new line then we're going

169
00:13:43.470 --> 00:13:50.520
to yield from the start up to that current tokens index will reset that current tokens index as a start

170
00:13:50.520 --> 00:13:53.770
position and we'll set senior line equals to false.

171
00:13:53.940 --> 00:14:01.150
Else if we actually see that that particular tokens text starts with and here backslash and you can

172
00:14:01.140 --> 00:14:07.620
replace that with whatever segmentation or separator you want then we reset the new line equals to true.

173
00:14:07.620 --> 00:14:12.420
So again the things that you would replace for your own segmentation are essentially whatever your indicator

174
00:14:12.420 --> 00:14:13.710
is here which starts with.

175
00:14:13.800 --> 00:14:19.160
And if you want you can change the variable names from senior line to seen my segments.

176
00:14:19.220 --> 00:14:19.750
OK.

177
00:14:20.040 --> 00:14:25.350
And then we're returning back or generating these chunks of the document.

178
00:14:26.400 --> 00:14:34.070
So then what we do is we just say create a variable like a speedy and then we call sentence segment

179
00:14:34.070 --> 00:14:34.550
later.

180
00:14:35.770 --> 00:14:37.340
Pasand the vocab.

181
00:14:38.320 --> 00:14:44.290
And then we say strategy we're basically replacing the default strategy for sentence segmentation and

182
00:14:44.290 --> 00:14:49.840
then we pass in our function split on new lines and then we're going to add to the pipeline or we'll

183
00:14:49.840 --> 00:15:00.650
say an LP at Pipe espie the and if you check out now when we say Doc is equal to an LP strings we need

184
00:15:00.650 --> 00:15:02.160
to redefine that document.

185
00:15:02.660 --> 00:15:12.830
Well now say for sentence in Doc thought sense Prince and there we're going to print out for example

186
00:15:13.580 --> 00:15:15.930
the actual sentence.

187
00:15:16.010 --> 00:15:21.080
So if you run this notice what happens we're saying this is a sentence this is another we are no longer

188
00:15:21.080 --> 00:15:24.990
splitting by default on this is a sentence and then period another line.

189
00:15:24.990 --> 00:15:26.360
This is another instead.

190
00:15:26.390 --> 00:15:30.640
Now this whole thing is being treated as a single segment's.

191
00:15:30.890 --> 00:15:35.900
So notice again the difference here by default Spacey was splitting on the periods.

192
00:15:35.900 --> 00:15:41.540
Now we've completely overwritten the sentence segmentation rules to our own strategy where we're splitting

193
00:15:41.540 --> 00:15:42.980
on new lines.

194
00:15:43.340 --> 00:15:44.230
OK.

195
00:15:44.240 --> 00:15:46.200
And that's it for this lecture.

196
00:15:46.250 --> 00:15:51.290
Hopefully now you realize that you can use the default sentence segmentation.

197
00:15:51.290 --> 00:15:53.640
You can add rules on what to segment on.

198
00:15:53.780 --> 00:15:57.190
Or you can completely replace the rules if you need so.

199
00:15:57.500 --> 00:15:59.180
Thanks and we'll see you at the next lecture.

