1
00:00:05,310 --> 00:00:07,440
Welcome back everyone in this lecture.

2
00:00:07,440 --> 00:00:11,590
We're going to show you how to actually perform latent Thursley application with Python.

3
00:00:11,820 --> 00:00:13,790
Let's open up a new notebook and get started.

4
00:00:15,150 --> 00:00:21,360
As a quick note underneath the topic modeling folder you'll notice that there is an NPR that CXVII.

5
00:00:21,530 --> 00:00:25,660
We're going to be using that and pure that CSFB data file to actually load up some articles.

6
00:00:25,790 --> 00:00:28,950
So make sure you actually look at it underneath the topic modeling folder.

7
00:00:29,180 --> 00:00:32,750
And if you want to follow any of the code that we show there in this lecture you can run it from the

8
00:00:32,870 --> 00:00:34,350
latent location.

9
00:00:34,550 --> 00:00:39,290
I've gone ahead and created a new notebook in the same folder so I can easily access this NPR that see

10
00:00:39,290 --> 00:00:40,090
every file.

11
00:00:40,370 --> 00:00:45,080
Let's go ahead and go to that untitled notebook now and the first we're going to do is actually load

12
00:00:45,080 --> 00:00:46,430
up that data.

13
00:00:46,670 --> 00:00:52,990
So we'll say import pand this is Preedy and then check out what that data actually holds by saying NPR

14
00:00:52,990 --> 00:01:01,680
is equal to PD that read underscore as we and NPR see as we.

15
00:01:01,700 --> 00:01:06,620
So let's take a look at the head of this data frame so we can understand what this actually is what

16
00:01:06,620 --> 00:01:07,850
this dataset is.

17
00:01:07,850 --> 00:01:14,450
It's a couple of thousand articles and each of these rules is the full text of one of the articles.

18
00:01:14,450 --> 00:01:18,320
Notice that right now we only have information on the article text.

19
00:01:18,320 --> 00:01:22,700
We don't actually have some sort of label column indicating what topic this particular article belongs

20
00:01:22,700 --> 00:01:23,540
to.

21
00:01:23,540 --> 00:01:30,460
So for example if we actually want to view one of the articles we simply say NPR at the article call

22
00:01:30,460 --> 00:01:32,700
them and then passen whatever role we want.

23
00:01:32,840 --> 00:01:37,580
So if all of the very first article I could index at zero and it return back the text of that entire

24
00:01:37,580 --> 00:01:38,310
article.

25
00:01:38,330 --> 00:01:40,360
This is essentially the first row.

26
00:01:40,610 --> 00:01:48,770
And if you look up the length of the actual dataset self-check of length NPR there's almost 12000 articles

27
00:01:48,770 --> 00:01:49,280
here.

28
00:01:49,370 --> 00:01:55,370
So you can have fun exploring just any basically any number between 1 and eleven thousand nine hundred

29
00:01:55,370 --> 00:01:55,960
ninety two.

30
00:01:55,970 --> 00:02:03,230
So article number 4000 we can run that was like the comment at that link line and here we can see an

31
00:02:03,230 --> 00:02:04,800
article on this.

32
00:02:04,800 --> 00:02:09,770
So what we don't know yet is what actual topics each of these articles belong to.

33
00:02:09,770 --> 00:02:14,110
So we're going to try to attempt to figure out and assign a topic to each of these articles.

34
00:02:14,120 --> 00:02:16,180
Let's go ahead and continue on.

35
00:02:16,310 --> 00:02:21,470
The first thing I need to do before I actually run LDA is perform a little bit of pre-processing and

36
00:02:21,490 --> 00:02:30,910
we can do that by saying from Eski learn the feature extraction text and we're going to do vectorization

37
00:02:31,090 --> 00:02:33,910
something we've already seen in the text classification lecture.

38
00:02:34,080 --> 00:02:41,150
So we're going to import vectorize or elysée C-v is equal to an instance of Count victimiser.

39
00:02:41,250 --> 00:02:45,050
Now we're going to pasan three parameters that are really useful.

40
00:02:45,060 --> 00:02:49,620
The first one is Max D F and Max the F.

41
00:02:49,620 --> 00:02:54,780
What it does is when you're building up the vocabulary we're going to ignore certain terms that have

42
00:02:54,780 --> 00:02:56,810
really high document frequency.

43
00:02:56,850 --> 00:03:03,030
So this essentially gets rid of terms that are really common across a lot of the documents.

44
00:03:03,150 --> 00:03:07,100
So you can pass in a number between 0 and 1 here.

45
00:03:07,110 --> 00:03:10,450
So such as 90 or 0.9.

46
00:03:10,680 --> 00:03:16,800
And what this is going to do is it's going to discard words that show up in 90 percent of the documents

47
00:03:17,280 --> 00:03:18,650
and you can play around this value.

48
00:03:18,780 --> 00:03:23,460
So if you want to be a little looser of what words get thrown out or not throw away as many words you

49
00:03:23,460 --> 00:03:28,220
can say something like Ok only words that show up in 95 percent of all the documents.

50
00:03:28,470 --> 00:03:36,820
So you can play around that ratio and it's counterpart is mindif which is essentially a minimum.

51
00:03:36,870 --> 00:03:39,560
So what's the minimum document frequency.

52
00:03:39,630 --> 00:03:46,350
So words that show up a minimum amount of times and you can actually passen for both Max T.F. and mindif

53
00:03:46,710 --> 00:03:52,980
either a ratio between 0 and 1 or if you pass an integer it will actually take that into account as

54
00:03:53,190 --> 00:03:55,080
a raw number of documents.

55
00:03:55,080 --> 00:04:02,420
So for example if I passen to here this says that the minimum frequency for a word to be counted into

56
00:04:02,450 --> 00:04:06,450
this count vectorize her it has to show up in at least two documents.

57
00:04:06,450 --> 00:04:09,900
So it can't be totally unique to a single article.

58
00:04:10,260 --> 00:04:14,880
So that's usually a good idea to throw out some really common words and also throw out random words

59
00:04:14,910 --> 00:04:16,450
maybe even typos or misspelling.

60
00:04:16,590 --> 00:04:19,870
So you have to guarantee that they show up at least in two documents.

61
00:04:20,130 --> 00:04:20,850
OK.

62
00:04:20,910 --> 00:04:26,400
And then also you're going to do is we can automatically remove Stoppard's simply by saying Stop words

63
00:04:26,400 --> 00:04:33,530
here and pass in the string English and vectorize or can actually automatically remove those Stoppard's.

64
00:04:33,750 --> 00:04:38,320
We have other methods of doing this as we know if we wanted to we could tokenize all of these Ramoo

65
00:04:38,320 --> 00:04:43,680
stoppers ourselves with spacey but we could also let victimiser take care of all that for us as well

66
00:04:43,680 --> 00:04:49,760
as take out really common words and make sure words are meeting some sort of minimum threshold.

67
00:04:50,150 --> 00:04:50,690
OK.

68
00:04:51,030 --> 00:04:52,480
So there's our count vectorize.

69
00:04:52,650 --> 00:04:57,090
Now it's time to actually apply this and transform to the article.

70
00:04:57,090 --> 00:05:01,590
Again notice here I'm not going to do any sort of train to split because this is unsupervised learning.

71
00:05:01,710 --> 00:05:06,110
I don't have any label to test against So this really makes sense to test train split and said what

72
00:05:06,110 --> 00:05:08,170
I'm doing is essentially unsupervised learning.

73
00:05:08,270 --> 00:05:12,800
We're going to transform to the entire dataset.

74
00:05:12,810 --> 00:05:21,730
OK so we're going to do now is we're going to say a document matrix is equal to C-v thought that transform

75
00:05:22,960 --> 00:05:26,560
and I'm going to do this on the article call them

76
00:05:29,830 --> 00:05:36,310
and then if we check out our document matrix then you should have a sparse matrix.

77
00:05:36,310 --> 00:05:39,970
Keep in mind depending on what computer you have this fit transform may take a while.

78
00:05:39,970 --> 00:05:41,530
This is a pretty big dataset.

79
00:05:41,740 --> 00:05:48,570
But right now it looks like we have almost about 55000 terms and here the number of articles.

80
00:05:48,730 --> 00:05:50,860
OK so there's our turn matrix.

81
00:05:50,860 --> 00:05:55,020
Up next what we need to do is actually perform the latent a.

82
00:05:55,390 --> 00:06:02,080
And this is built in to sikat learn all you need to say is from a scalar the composition

83
00:06:04,560 --> 00:06:05,370
import.

84
00:06:05,880 --> 00:06:07,480
And then you can actually just directly import.

85
00:06:07,490 --> 00:06:11,020
You could have autocomplete this latent there are allocation.

86
00:06:11,200 --> 00:06:15,510
Now we're going to say LDA as our variable name here.

87
00:06:15,510 --> 00:06:20,190
Typically you don't actually have variable names are capitalized but I like this because it kind of

88
00:06:20,190 --> 00:06:21,740
makes it sound like an acronym.

89
00:06:22,020 --> 00:06:28,830
And also it doesn't make the l be confused of a one sort of say LDA and then we're going to call instance

90
00:06:28,900 --> 00:06:33,460
a late since there is only allocation and we can explore some of the parameters.

91
00:06:33,480 --> 00:06:39,450
There's lots of different parameters here such as learning the K learning method et cetera number of

92
00:06:39,450 --> 00:06:46,110
iterations the most important ones that we actually want to go through are number of components and

93
00:06:46,110 --> 00:06:50,910
we also want to make sure we set a random state because there is some random this because remember we

94
00:06:50,910 --> 00:06:53,080
randomly assigned words in the beginning.

95
00:06:53,130 --> 00:06:57,210
So we want to make sure that our random state is a sign that we are going to get the same topics that

96
00:06:57,270 --> 00:06:57,910
I do.

97
00:06:58,940 --> 00:07:03,760
So we're going to do here is first decide on the number of components.

98
00:07:03,920 --> 00:07:06,700
So we're going to say number of components.

99
00:07:07,220 --> 00:07:10,270
And this is where there is no right or wrong answer.

100
00:07:10,280 --> 00:07:16,060
It really depends on your domain experience and your familiarity with the domain that data is from.

101
00:07:16,130 --> 00:07:19,990
I'm going to just say I want 7 general topics returned.

102
00:07:20,000 --> 00:07:25,640
If you want to get into more detail such as maybe subcategories of politics like international politics

103
00:07:25,970 --> 00:07:27,500
versus national politics.

104
00:07:27,500 --> 00:07:32,210
If you're hoping for that sort of level of information in subtopics you might want to make that number

105
00:07:32,210 --> 00:07:33,980
of components much larger.

106
00:07:34,070 --> 00:07:35,940
Again it's totally up to you.

107
00:07:36,470 --> 00:07:42,230
So then we're going to just choose subcomponents as seven essentially looking for seven topics and then

108
00:07:42,230 --> 00:07:47,820
I'm also going to say random state is equal to 42 and I would encourage you to do the same.

109
00:07:47,870 --> 00:07:53,990
That way the words return per topic are the same for you as they are for me.

110
00:07:53,990 --> 00:08:01,190
OK so we have LDA and now it's time to take LDA and fit it to our document turn matrix.

111
00:08:01,190 --> 00:08:04,820
So we'll go ahead and run that and I'm going to fast forward in time.

112
00:08:04,820 --> 00:08:05,870
This should take awhile.

113
00:08:05,870 --> 00:08:07,360
Again it depends on your hardware.

114
00:08:07,490 --> 00:08:12,730
But we are dealing with a large number of documents and remember LDA is an iterative process.

115
00:08:12,770 --> 00:08:19,190
So it's going to keep updating those weights per word per topic over and over again until they've stabilized.

116
00:08:19,190 --> 00:08:22,360
So I'm going to go and hop forward in time until that's done fitting.

117
00:08:22,930 --> 00:08:23,550
OK.

118
00:08:23,690 --> 00:08:25,180
We have finished fitting the model.

119
00:08:25,370 --> 00:08:27,800
Now there's only three steps left.

120
00:08:27,800 --> 00:08:34,480
The first step is to actually grab the vocabulary of words.

121
00:08:35,610 --> 00:08:46,980
The second step is to grab the topics and the first step is to grab the highest probability words per

122
00:08:46,980 --> 00:08:47,710
topic.

123
00:08:47,760 --> 00:08:50,690
That way we can actually interpret the topics themselves.

124
00:08:50,760 --> 00:08:54,000
So we're going to continue right off of this in the very next lecture.

125
00:08:54,000 --> 00:08:54,590
We'll see if there.

