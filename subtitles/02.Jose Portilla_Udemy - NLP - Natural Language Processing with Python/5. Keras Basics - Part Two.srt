1
00:00:05,240 --> 00:00:10,660
Now the way we do that is we first need to scale our test data or new incoming data.

2
00:00:10,680 --> 00:00:11,730
Now we've actually already done this.

3
00:00:11,730 --> 00:00:14,240
We have our skilled X test right here.

4
00:00:14,370 --> 00:00:19,830
But keep in mind if you had fresh data like you just went out and collected the information off a flower

5
00:00:19,830 --> 00:00:22,160
in the field you would have to scale that data first.

6
00:00:22,170 --> 00:00:28,110
Because our model is only train on the scaled versions of the sort of data and then we're going to do

7
00:00:28,260 --> 00:00:36,390
is if you actually take a look at model dot product just like you would for a psychic learn model the

8
00:00:36,390 --> 00:00:43,370
same model that predict and then pass that scaled data and then if you run this you'll notice that it

9
00:00:43,380 --> 00:00:47,170
actually spits out probabilities by default when you called up predict.

10
00:00:47,310 --> 00:00:53,430
If you want the raw classes themselves simply call predict underscore classes and it's going to predict

11
00:00:53,580 --> 00:00:58,650
the classes again something that's important to know here is that it doesn't predict the one encoded

12
00:00:58,680 --> 00:00:59,640
version of classes.

13
00:00:59,760 --> 00:01:05,250
It just reports back the exposition which is convenient in some senses kind of inconvenient because

14
00:01:05,940 --> 00:01:10,180
the original white tests we had to change it to be 100 coded.

15
00:01:10,260 --> 00:01:13,660
So that's a little annoying but we'll show you how to quickly fix that.

16
00:01:14,210 --> 00:01:14,590
OK.

17
00:01:14,760 --> 00:01:16,600
So we're able to not predict classes.

18
00:01:16,650 --> 00:01:23,610
We need to do is compare these predictions to our white test the way we're going to do that is by simply

19
00:01:23,610 --> 00:01:25,260
saying the following.

20
00:01:25,260 --> 00:01:38,650
We're going to say that our predictions is equal to model dot predicts classes on that scale x test

21
00:01:38,650 --> 00:01:51,340
data and then we're going to transform our Y test data by simply saying why test RMX across access is

22
00:01:51,340 --> 00:01:52,750
equal to one.

23
00:01:52,750 --> 00:01:58,190
And what that does is it reports back the actual classes because why test original.

24
00:01:58,190 --> 00:02:03,310
If we again take a look at it the max value is simply going to be at the expiration of the corresponding

25
00:02:03,310 --> 00:02:03,910
class.

26
00:02:03,910 --> 00:02:06,520
So that's kind of an easy fix.

27
00:02:06,520 --> 00:02:12,530
So now we have the predictions and our original Why test values in the exact same format.

28
00:02:12,550 --> 00:02:16,450
So I just need to compare these predictions to the truth values.

29
00:02:16,570 --> 00:02:17,580
And we already know how to do that.

30
00:02:17,640 --> 00:02:18,790
I sikat learn.

31
00:02:18,820 --> 00:02:21,890
We simply say from Eski learned that metrics import.

32
00:02:22,210 --> 00:02:27,090
And we're going to import the confusion matrix the classification report.

33
00:02:27,470 --> 00:02:32,490
And if you don't you can also import the accuracy score and then we can just play around with these.

34
00:02:32,510 --> 00:02:40,070
So the confusion matrix we pass why test RMX access equal to 1.

35
00:02:40,140 --> 00:02:46,910
So those are the true values corresponding in the same method as the predictions are going to be in.

36
00:02:47,150 --> 00:02:49,330
And it looks like we're doing pretty well in this confusion matrix.

37
00:02:49,340 --> 00:02:56,330
Let's go ahead and print out the full classification report and then just copy and paste this here

38
00:03:00,280 --> 00:03:01,040
from that.

39
00:03:01,480 --> 00:03:05,310
And we're getting a pretty good precision recall especially on this 0 class.

40
00:03:05,320 --> 00:03:06,880
A little rougher on this one class.

41
00:03:06,880 --> 00:03:10,540
And if you would actually visualize this is because there's a lot more overlap between classes 1 and

42
00:03:10,540 --> 00:03:11,310
2.

43
00:03:11,440 --> 00:03:14,220
And then finally we can print out the accuracy score.

44
00:03:14,380 --> 00:03:16,520
So accuracy score between these two.

45
00:03:16,720 --> 00:03:18,800
And we're getting around 88 percent accuracy.

46
00:03:18,850 --> 00:03:19,920
So not bad.

47
00:03:20,340 --> 00:03:20,850
OK.

48
00:03:21,100 --> 00:03:24,470
So we learn how to actually evaluate on unseen data.

49
00:03:24,520 --> 00:03:28,580
The last thing I want to do is especially for a really large network.

50
00:03:28,660 --> 00:03:30,580
We spent some time training it.

51
00:03:30,670 --> 00:03:33,710
This wasn't a big deal for this particular 150 epochs.

52
00:03:33,730 --> 00:03:35,290
It took just a few seconds to run.

53
00:03:35,530 --> 00:03:40,630
But if we're running a super large model especially for things like text generation that can take hours

54
00:03:40,630 --> 00:03:41,160
to run.

55
00:03:41,410 --> 00:03:47,530
We want to be able to save and load our model though if we do this is we simply grab our model and then

56
00:03:47,530 --> 00:03:48,790
call that save.

57
00:03:49,300 --> 00:03:51,100
And then you choose whatever you want to save it.

58
00:03:51,100 --> 00:03:56,130
So you say something like my first model and the extension is not H5.

59
00:03:56,200 --> 00:03:59,020
So this essentially saves all the weights of the network.

60
00:03:59,050 --> 00:04:03,420
So we're able to quickly load it up again if you want to load the model so that we.

61
00:04:03,460 --> 00:04:04,860
Let's imagine we shut down our computer.

62
00:04:04,870 --> 00:04:06,890
Next day we want to load this model that we train.

63
00:04:07,100 --> 00:04:17,600
All you need to do is say from cares that models import load model and we can make a new model is equal

64
00:04:17,600 --> 00:04:18,510
to.

65
00:04:18,680 --> 00:04:24,380
And then you just call the load model at the file location of where ever the H5 file is.

66
00:04:24,380 --> 00:04:30,770
So my first model age 5 the only thing to be really careful of is be really careful with this naming

67
00:04:30,770 --> 00:04:35,780
scheme because if I run this cell again multiple times after training maybe I'm just playing around

68
00:04:35,780 --> 00:04:36,420
of things.

69
00:04:36,500 --> 00:04:41,900
This will automatically overwrite any file with the exact same name that can be really dangerous especially

70
00:04:41,900 --> 00:04:46,310
if you have a model and your weights save that maybe took hours to run and you're just playing around

71
00:04:46,310 --> 00:04:48,400
and maybe do something else that took a minute to run.

72
00:04:48,500 --> 00:04:50,120
You will just have overwritten all that work.

73
00:04:50,120 --> 00:04:51,870
So be really careful this naming scheme.

74
00:04:51,920 --> 00:04:55,020
You don't want to overwrite a model that took forever to train.

75
00:04:55,040 --> 00:04:59,480
So now I have this new model essentially works the exact same way as the original simple same model.

76
00:04:59,720 --> 00:05:03,590
And then I can call it classes function for the generator.

77
00:05:03,590 --> 00:05:10,820
Anything I can passen that skill X test data run it and then get the classes predicted.

78
00:05:10,820 --> 00:05:11,270
OK.

79
00:05:11,570 --> 00:05:18,590
So that's it for evaluating model performance as well as exploring how to predict on new unseen data.

80
00:05:18,590 --> 00:05:20,440
All right we'll see you at the next lecture.

