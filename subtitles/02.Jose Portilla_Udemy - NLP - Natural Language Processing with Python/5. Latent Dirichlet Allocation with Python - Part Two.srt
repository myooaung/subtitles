1
00:00:05,230 --> 00:00:09,460
Welcome back to part two of late in Thursley allocation with Python.

2
00:00:09,460 --> 00:00:12,180
Let's continue where we left off in the previous lecture.

3
00:00:12,190 --> 00:00:16,870
All right here we are at the notebook where we left off last time we defined three steps last time we

4
00:00:16,870 --> 00:00:22,240
wanted to be able to grow the vocabulary of words grab the topics and then grab the highest probability

5
00:00:22,240 --> 00:00:23,510
words per topic.

6
00:00:23,770 --> 00:00:27,730
Let's begin by showing you how to grab the vocabulary of words I'm doing.

7
00:00:27,760 --> 00:00:33,790
Enter to put themselves below the cell and what I want to show you is that the count vectorize or object

8
00:00:33,820 --> 00:00:36,760
we created for the document matrix.

9
00:00:36,760 --> 00:00:42,220
You can actually do a get underscore a feature underscore names method and if you check the length of

10
00:00:42,220 --> 00:00:47,540
what's returned here it's fifty four thousand seven hundred and seventy seven.

11
00:00:47,650 --> 00:00:51,880
And if we take a look at our previous sparse matrix that's actually the other dimension.

12
00:00:51,880 --> 00:00:59,380
In fact this is documents by words or a document matrix so get feature names is actually just holding

13
00:00:59,410 --> 00:01:02,850
everything or pulling an instance of every single word.

14
00:01:03,070 --> 00:01:10,750
In fact if you check the type of CV docket feature names it's just a list of all the words that were

15
00:01:10,780 --> 00:01:18,050
in all your documents so we can randomly grab some words off of this so I can say Get feature names

16
00:01:21,490 --> 00:01:28,410
and then Passons some sort of index position off of this list so I can grab word number index location

17
00:01:28,480 --> 00:01:31,680
50000 and that happens to be the word transcribe.

18
00:01:31,680 --> 00:01:37,880
Or I can get the word at location 41000 and there happens to be reproductive.

19
00:01:37,890 --> 00:01:45,030
So just to make this clear I could print out a bunch of random words from my list of words so I could

20
00:01:45,030 --> 00:01:57,910
say import random and then say random word ID is equal to random random integer and then say grab a

21
00:01:57,910 --> 00:02:08,230
random integer from 0 up to 54000 777 and make sure we still ran the word ID correctly and then call

22
00:02:08,230 --> 00:02:13,600
see the get feature names and then pasan that random word ID.

23
00:02:13,600 --> 00:02:18,460
So as you keep running the cell you should see some different random words show up.

24
00:02:18,460 --> 00:02:22,890
So these are all words that are in our actual documents.

25
00:02:22,930 --> 00:02:28,840
So now we know I can simply call Seabee that get feature names and then pass in some ID number like

26
00:02:28,840 --> 00:02:33,190
10 and I will get some sort of string word that happened to be in the document.

27
00:02:33,220 --> 00:02:36,120
So now I understand how to grab the vocabulary of the words.

28
00:02:36,130 --> 00:02:38,890
Now it's time to actually grab the topics.

29
00:02:38,890 --> 00:02:46,820
So the way we do this is we actually now grab this information directly off the trained LDA so I can

30
00:02:46,820 --> 00:02:52,730
say LDA or whatever happened to call your model and call component's underscore.

31
00:02:53,150 --> 00:03:02,180
Then you can check the length here of these components and you notice there's 7 components and LDA components

32
00:03:02,240 --> 00:03:03,410
underscore.

33
00:03:03,410 --> 00:03:12,130
If we check the type of this it actually is just a name PI array containing probabilities for each word

34
00:03:12,940 --> 00:03:16,450
so that I can say grabbed the first item.

35
00:03:16,450 --> 00:03:19,350
Actually you can even describe the shape of the array.

36
00:03:19,600 --> 00:03:25,330
You'll notice it's seven topics by fifty four thousand seven hundred and seventy seven words and we

37
00:03:25,330 --> 00:03:28,440
take a look at the actual components.

38
00:03:28,450 --> 00:03:31,350
You just notice it's this giant array.

39
00:03:31,420 --> 00:03:37,510
So what we're going to be able to do is combine this information with our ability to grab vocabulary

40
00:03:37,930 --> 00:03:42,220
in order to show you the highest probability words per topic.

41
00:03:42,220 --> 00:03:48,370
So we're going to do here is we're going to start off by just grabbing a single topic off of these components.

42
00:03:48,370 --> 00:03:54,520
So going to create some new cells and let's grab one of these topics we'll say single topic is equal

43
00:03:54,520 --> 00:03:58,800
to LDA components underscore and going to index at zero.

44
00:03:59,080 --> 00:04:04,750
So it's the very first topic and we still again don't know what this topic actually represents.

45
00:04:05,990 --> 00:04:15,080
So I'm going to take single topic and I'm going to ask for ARG sort and Arg sort does it returns the

46
00:04:15,140 --> 00:04:18,000
index positions that would sort the story.

47
00:04:18,050 --> 00:04:21,010
So let me show you with a very simple example first.

48
00:04:21,020 --> 00:04:24,530
So this is the output of our sort.

49
00:04:24,650 --> 00:04:29,920
But I'm going to create a very simple array just to make sure you understand this sort process.

50
00:04:29,990 --> 00:04:41,760
So I'm going say a r r so the variable name and P array and I get a pass in 10 and then 200 and let's

51
00:04:41,760 --> 00:04:49,030
say 1 and if I take a look at my array I have 10 201.

52
00:04:49,140 --> 00:04:56,160
If I take a look at array and then call sore on it it returns to 0 1.

53
00:04:56,160 --> 00:05:02,790
So what that actually means is this is the index positions that would sort the array from the lowest

54
00:05:02,940 --> 00:05:08,860
value to the highest value because the index to that was 1 index 0 that was 10.

55
00:05:08,940 --> 00:05:10,830
And then our largest value was the index 1.

56
00:05:10,830 --> 00:05:11,930
So it was two hundred.

57
00:05:12,180 --> 00:05:17,350
So what we're doing is we're going to take these single topics and then figure out what index positions

58
00:05:17,370 --> 00:05:22,280
we should be looking at for high probability words for this particular single topic.

59
00:05:22,320 --> 00:05:28,560
So it's all our is doing is essentially showing us the locations of these larger values.

60
00:05:28,560 --> 00:05:30,400
So that's where we're using our sort here.

61
00:05:30,480 --> 00:05:32,340
So I can then use these in expedition's.

62
00:05:32,370 --> 00:05:39,420
So it's actually going to be directly correlated with the index positions of this CV docket feature

63
00:05:39,420 --> 00:05:44,140
names since LTA that we're actually trained on the result of this compact razor.

64
00:05:44,610 --> 00:05:45,180
OK.

65
00:05:45,510 --> 00:05:51,770
So what we're going to do then is now I understand that I can call single topic the arc sort of index

66
00:05:51,770 --> 00:05:55,870
positions for the high probability words for that particular topic.

67
00:05:55,970 --> 00:06:07,300
What I'm going to do then is the following I will say single topic and then I'm going to call our sort

68
00:06:10,780 --> 00:06:15,220
and let's go ahead and grab the top 10 words for this topic.

69
00:06:15,370 --> 00:06:19,630
So off of our sort I'm going to say negative 10 colon.

70
00:06:19,930 --> 00:06:24,070
So this line is a little tricky to understand at first but it's actually not so bad.

71
00:06:24,070 --> 00:06:37,320
All we're doing here is remember that our source returns back index positions sorted from least to greatest.

72
00:06:37,330 --> 00:06:43,220
And what we're looking for are the top 10 values.

73
00:06:43,300 --> 00:06:47,310
In other words the 10 greatest values.

74
00:06:47,500 --> 00:06:55,520
So that means I want the last 10 values of our sort.

75
00:06:55,930 --> 00:07:01,180
And so what this does right here is after you call Ark's sort it says starting next mission negative

76
00:07:01,180 --> 00:07:01,880
10.

77
00:07:02,260 --> 00:07:04,060
Colin go all the way to the end.

78
00:07:04,180 --> 00:07:14,470
All this is doing is saying grab the last 10 values of the result of our sort which returns the index

79
00:07:14,470 --> 00:07:20,530
positions of the 10 highest probability words for this particular single topic.

80
00:07:20,530 --> 00:07:33,090
So what we're going to do is say top 10 words is equal to single topic arc.

81
00:07:35,190 --> 00:07:43,040
Negative 10 all the way to the end and then those are the positions of my top 10 words and I'll say

82
00:07:43,040 --> 00:07:56,570
for index in top 10 words Prince CV get feature names and then I'm going to call that index.

83
00:07:56,630 --> 00:08:01,970
So when you're done this may take a little time but hopefully you should see words that tend to have

84
00:08:01,970 --> 00:08:04,460
some sort of generalized topic feeling.

85
00:08:04,460 --> 00:08:11,330
So this first single topic the 10 highest probability words show up in that particular topic are new

86
00:08:11,660 --> 00:08:17,620
percent's government company million care people health said and says.

87
00:08:17,930 --> 00:08:23,580
So here we can maybe tell that these look maybe like business articles government articles or health

88
00:08:23,580 --> 00:08:25,430
care issue articles.

89
00:08:25,430 --> 00:08:31,750
So what it can also do is start asking for more words like the top 20 words here.

90
00:08:32,120 --> 00:08:36,680
So let's go ahead and call this top 20.

91
00:08:36,850 --> 00:08:43,540
We'll run this and we'll make sure we call top 20 here.

92
00:08:43,550 --> 00:08:44,230
There we go.

93
00:08:44,420 --> 00:08:49,460
And now I get to see the top 20 words for this particular single topic and now it definitely starts

94
00:08:49,460 --> 00:08:54,230
to clear up that maybe has to do more with health insurance and politics so I can see President state

95
00:08:54,230 --> 00:08:57,920
tax insurance federal government per cent and more.

96
00:08:58,070 --> 00:09:05,060
But what's really incredible here is that LDA is now showing us some sort of underlying latent topic.

97
00:09:05,090 --> 00:09:11,780
It's trying to show us that these particular words have high probabilities of showing up in this particular

98
00:09:11,930 --> 00:09:13,310
single topic.

99
00:09:13,310 --> 00:09:20,840
Let's go ahead and now set up a little loop that prints out the top 15 words for each of the seven topics.

100
00:09:21,970 --> 00:09:29,530
So we're going to do here is the following we'll say for in the next column a topic a little bit a tuple

101
00:09:29,530 --> 00:09:37,870
and packing because we're going to call enumerates off L.D. components.

102
00:09:37,940 --> 00:09:49,370
I'm going to use some string formatting here to say the top 15 words for Topic number and then I'm going

103
00:09:49,370 --> 00:09:50,710
to insert here.

104
00:09:50,990 --> 00:09:53,700
Index essentially is just the count.

105
00:09:53,710 --> 00:09:54,560
In fact was going to call it.

106
00:09:54,590 --> 00:09:57,020
I sort of get confused here.

107
00:09:59,610 --> 00:10:08,240
And then what I'm going to do is say Prince and I'm going to create a list of C-v get feature names

108
00:10:10,100 --> 00:10:22,900
index for index and topic Arc's sorts and then grab the last 15 words after you done are sort and then

109
00:10:22,900 --> 00:10:29,480
we'll go ahead and print we'll print some new lines here.

110
00:10:29,640 --> 00:10:35,430
All right so what is is doing is essentially saying for each topic and LDA components are LDA components

111
00:10:35,490 --> 00:10:36,390
was an array.

112
00:10:36,420 --> 00:10:42,810
We come back up here of seven so seven topics by fifty four thousand seven hundred and seventy seven

113
00:10:42,810 --> 00:10:45,570
words and has a value associated to each of these words.

114
00:10:45,660 --> 00:10:50,180
The greater the value the higher probability has a value to that particular topic.

115
00:10:50,190 --> 00:10:54,800
So what we're doing is for each of those topics we're just grabbing the top 15 words and then we're

116
00:10:54,810 --> 00:11:01,270
using miscomprehension to basically grab those index actions off our Get feature names words.

117
00:11:01,290 --> 00:11:07,380
So if you run this loop you should end up seeing the falling top 15 words for topic zero companies money

118
00:11:07,380 --> 00:11:13,530
year federal etc. So it looks as if healthcare in the United States top 15 words for topic number one

119
00:11:14,010 --> 00:11:20,760
military how security Russia government Trump $250 for topping number two World Family Home day time

120
00:11:20,760 --> 00:11:21,690
water city.

121
00:11:21,690 --> 00:11:25,440
So maybe some infrastructure stuff here we can see medical disease patients health.

122
00:11:25,440 --> 00:11:27,600
So maybe just general health.

123
00:11:27,600 --> 00:11:29,700
Topic number for the election.

124
00:11:29,760 --> 00:11:32,700
Obama Clinton so looks like stuff do with the election.

125
00:11:33,690 --> 00:11:37,620
Number five music maybe lifestyle articles etc..

126
00:11:37,740 --> 00:11:42,380
And here we can see data science university students school science education topics.

127
00:11:42,390 --> 00:11:45,850
So here we only printed out seven topics zero through six.

128
00:11:46,020 --> 00:11:48,940
It's up to you to decide how many topics start beforehand.

129
00:11:49,080 --> 00:11:54,000
If some of these topics maybe aren't clear enough or maybe some of these topics seem too similar maybe

130
00:11:54,000 --> 00:11:59,370
you go for less topics or if you want to get some clarity and subdivide these into further topics you

131
00:11:59,370 --> 00:12:02,300
would just ask for more topics right off the top.

132
00:12:03,200 --> 00:12:09,730
So the last thing left to do is to attach these topic numbers to the original articles.

133
00:12:09,830 --> 00:12:11,780
And that's actually pretty straightforward.

134
00:12:11,780 --> 00:12:19,520
All we need to do is recall that we have our original document turned matrix as well as our original

135
00:12:19,850 --> 00:12:26,390
NPR data frame but we would like to do is essentially create a new column here that has a topic number

136
00:12:26,990 --> 00:12:35,700
and to do that all we really need to do is create a list of the actual topics off this document matrix

137
00:12:35,940 --> 00:12:51,970
and what we can do is call LDA dot transform on our original DTM so say topic results and go ahead and

138
00:12:51,970 --> 00:12:52,410
run that.

139
00:12:52,450 --> 00:12:54,380
Again this may take a little bit of time.

140
00:12:54,400 --> 00:12:59,980
Technically we've already done this but we call the fit transform instead of just a dot transform.

141
00:13:00,060 --> 00:13:05,830
And now if I take a look at topic results what this is is an array.

142
00:13:06,000 --> 00:13:11,600
And if you check the shape it's in array where we have the original eleven thousand nine hundred ninety

143
00:13:11,600 --> 00:13:13,030
two articles presented.

144
00:13:13,320 --> 00:13:15,780
And now we have by 7.

145
00:13:15,780 --> 00:13:21,020
So what this is if I take a look at the very first instance of topic results.

146
00:13:21,150 --> 00:13:25,410
These are essentially probabilities of belonging to a particular topic.

147
00:13:25,410 --> 00:13:29,910
Remember we previously saw probabilities of words belonging to a particular topic.

148
00:13:29,910 --> 00:13:36,000
Now we're seeing the probability of a document belonging to a particular topic where this topic is topic

149
00:13:36,000 --> 00:13:37,970
number 0 indexed 0.

150
00:13:37,980 --> 00:13:41,640
This is the probability of topic number one index 1.

151
00:13:41,640 --> 00:13:45,900
Notice here how these are essentially just values in percentage form.

152
00:13:45,900 --> 00:13:51,720
So somewhere between 0 and 1 and if it's a little hard for you to read that you can call that round

153
00:13:51,720 --> 00:13:55,110
off of this and you can see the actual percentages.

154
00:13:55,320 --> 00:14:00,600
So it looks like the very first article at index 0.

155
00:14:00,810 --> 00:14:07,410
It has a high its highest probability is around 68 percent belonging to topic number 1.

156
00:14:07,410 --> 00:14:08,610
So let's see if that makes sense.

157
00:14:08,610 --> 00:14:13,980
What was topic number one topic number one tended to include words like military House security Russia

158
00:14:14,010 --> 00:14:16,690
government President Trump police etc..

159
00:14:16,770 --> 00:14:21,120
So let's take a look back at the original documents.

160
00:14:21,120 --> 00:14:27,540
Remember we can print out any article we want so let's go ahead and print out the first article and

161
00:14:27,570 --> 00:14:29,040
it looks like it does have to do with that.

162
00:14:29,040 --> 00:14:35,400
So Washington of 2016 even pulsing the bipartisan politics cannot etc. so it does look like it has to

163
00:14:35,400 --> 00:14:36,260
do with politics.

164
00:14:36,270 --> 00:14:42,750
It has to do if Russia it has to do security Moscow etc. so that seems like a pretty fair topic.

165
00:14:42,750 --> 00:14:49,530
Now keep in mind because of recent news and the way NPR works and some of these articles particularly

166
00:14:49,530 --> 00:14:52,520
the way we scrape these articles a lot of them actually do of politics.

167
00:14:52,590 --> 00:14:58,680
So you may see feel that some of these topics overlap which OK we're going to do now is show you how

168
00:14:58,680 --> 00:15:01,670
to easily connect this with our original data frame.

169
00:15:01,700 --> 00:15:06,480
So what we're going to do is I'm actually not interested in these probabilities.

170
00:15:06,480 --> 00:15:10,720
I'm just interested in the index position of the highest probability.

171
00:15:10,980 --> 00:15:19,250
And I can grab that by calling RMX and RMX returns back the index position of the highest probability.

172
00:15:19,290 --> 00:15:22,310
So again RMX says index 1.

173
00:15:22,320 --> 00:15:24,340
That's where you had your highest probability.

174
00:15:24,340 --> 00:15:38,140
So going to do is say NPR topic is equal to topic results and then I'm going to call RMX across.

175
00:15:38,160 --> 00:15:42,190
Access is equal to one.

176
00:15:42,470 --> 00:15:44,280
And then I can see the assigned topics.

177
00:15:44,300 --> 00:15:50,030
So note that these first four looks like their topic one then from photography Illustration A video

178
00:15:50,030 --> 00:15:54,960
looks like topic to topic 3 2 3 2 5 5 and so on.

179
00:15:55,160 --> 00:15:59,600
And then if you ever want to figure out well what is topic 5 represent says who's the YouTube star of

180
00:15:59,600 --> 00:16:00,630
2016.

181
00:16:00,710 --> 00:16:03,970
We go ahead and see will or the high probability words for topic 5.

182
00:16:04,190 --> 00:16:09,290
We simply scroll up see topic 5 has to do with music people life etc..

183
00:16:09,530 --> 00:16:15,200
So looks like it makes sense that this story about YouTube stars 2016 and Adele singing has to do with

184
00:16:15,200 --> 00:16:16,240
top five.

185
00:16:16,610 --> 00:16:19,970
All right I hope you found this as fascinating as I did.

186
00:16:20,000 --> 00:16:25,370
If you have any questions feel free to ask in the Q and A forums and I would highly encourage you to

187
00:16:25,370 --> 00:16:29,810
check out our original site and Thursley allocation notebook so you can play around with those values

188
00:16:29,810 --> 00:16:30,460
as well.

189
00:16:30,700 --> 00:16:31,270
OK.

190
00:16:31,490 --> 00:16:33,400
Thanks and I'll see you at the next lecture.

