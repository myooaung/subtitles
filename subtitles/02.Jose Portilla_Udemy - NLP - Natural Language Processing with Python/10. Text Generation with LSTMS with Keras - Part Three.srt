1
00:00:05,470 --> 00:00:10,980
Welcome back everyone to three of texture narration of Python keris in part 3.

2
00:00:10,990 --> 00:00:14,860
We're going to learn how to generate new text based off of the seat input.

3
00:00:15,280 --> 00:00:16,300
Let's get started.

4
00:00:16,630 --> 00:00:21,260
All right so we have a Sade's trained model and we've also saved our tokenizer.

5
00:00:21,280 --> 00:00:23,860
What's next is to actually generate new text.

6
00:00:23,920 --> 00:00:28,890
So we're going to do is we're going to create a function that generates new text for US based off of

7
00:00:28,890 --> 00:00:34,480
a given model tokenizer sequence length a seed text and then the number words to be generated by the

8
00:00:34,480 --> 00:00:35,240
model.

9
00:00:35,710 --> 00:00:43,420
So I will say DLF generates underscore text and then it's going to take in a model.

10
00:00:43,820 --> 00:00:50,750
It's going to take in a tokenizer it's going to take in a sequence length some see text basically some

11
00:00:50,840 --> 00:00:56,180
text to start off on and then the number of words that we want to generate after this

12
00:01:00,330 --> 00:01:04,670
we're going to begin by preventing a placeholder for the final output text.

13
00:01:04,860 --> 00:01:07,290
And at the very end essentially up protect the list.

14
00:01:07,440 --> 00:01:12,750
So I'm going to at the very end join everything that list of space.

15
00:01:13,050 --> 00:01:15,000
So say output text.

16
00:01:15,240 --> 00:01:15,910
OK.

17
00:01:16,260 --> 00:01:20,290
Now comes the part of actually taking that model and generating new text from it.

18
00:01:21,250 --> 00:01:26,740
So the first we want to do is say input text and we're going to start off with some sweet text.

19
00:01:26,740 --> 00:01:31,300
So we actually need to feed it some sort of line of 25 tokens that we want to start off with and then

20
00:01:31,300 --> 00:01:35,920
it's going to generate one word after that then we're going to do is chop off the very first word of

21
00:01:35,920 --> 00:01:42,280
the seed taken our new word put it at the end and then we have our new sweet text or a new pretext for

22
00:01:42,280 --> 00:01:48,840
that and we're going to keep doing that however many times the user wants to generate words so I'll

23
00:01:48,920 --> 00:01:58,890
say for I in range number of generated words will take the input text string and encode it to a sequence.

24
00:01:59,090 --> 00:02:08,010
So I'll say in coded text is equal to the tokenizer and call text 2 sequences off of this tokenizer

25
00:02:08,010 --> 00:02:17,550
object so text the sequences and then we're going to pass in our input text and we're going to grab

26
00:02:17,550 --> 00:02:23,040
the first item here because it basically returns a tuple or a list so we're going to run the very first

27
00:02:23,040 --> 00:02:23,520
item.

28
00:02:23,660 --> 00:02:27,540
And want to encourage you to do is play around with print statements here so you can print out what

29
00:02:27,540 --> 00:02:28,690
impot text looks like.

30
00:02:28,800 --> 00:02:33,980
Print out what this tokenizer text sequences is etc. but we've seen this before previously.

31
00:02:34,010 --> 00:02:35,320
And one of the lecturers.

32
00:02:35,610 --> 00:02:42,300
And then we need to do is do some pad in coding so we're going to pad sequences to our trained very

33
00:02:44,200 --> 00:02:52,090
So say pad sequences pad sequences here and in order to do that we actually need to import it.

34
00:02:52,090 --> 00:02:57,060
So let's go ahead and do some imports.

35
00:02:57,070 --> 00:03:08,390
We'll say from Krista pre-processing from is that pre-processing we're going to import is sequencing

36
00:03:08,510 --> 00:03:13,790
and pet sequences so from care stop make sure he cares right.

37
00:03:13,860 --> 00:03:21,690
From Krista pre-processing that sequence import pad sequences.

38
00:03:21,960 --> 00:03:25,140
So we're going to now call Pead sequences here.

39
00:03:25,620 --> 00:03:31,060
We're going to pasand the encoded text as a list of just one.

40
00:03:31,500 --> 00:03:36,990
And then the max length here should be whatever the sequence length was that was passed then.

41
00:03:37,110 --> 00:03:43,030
This essentially makes mixer's that if you pass in a super long text it's it's really trained on 25

42
00:03:43,080 --> 00:03:43,770
tokens.

43
00:03:43,860 --> 00:03:47,430
We're going to pad it to make sure it's only 25 tokens.

44
00:03:47,460 --> 00:03:54,660
Or if you see text happens to be too short then we're going to pad it to fill up the 25 spaces again

45
00:03:54,870 --> 00:03:55,800
in order to get best results.

46
00:03:55,800 --> 00:04:00,170
I do recommend just passing a CTX that is actually the same expected length that your model has but

47
00:04:00,370 --> 00:04:06,640
I want to make this function a little more robust so give him pad them out and then for truncating we

48
00:04:06,650 --> 00:04:11,590
can either do a kind of pre or post beginning of string or after the string so will go ahead and do

49
00:04:11,590 --> 00:04:12,840
it at the beginning of the string.

50
00:04:12,920 --> 00:04:19,500
Since we're more dependent on the end of the string for a prediction purposes then we'll say predicted

51
00:04:19,650 --> 00:04:22,260
word index position.

52
00:04:22,320 --> 00:04:24,790
So is going to predict class probabilities for each word.

53
00:04:24,870 --> 00:04:33,170
So same model predicts classes of the padded encoded string.

54
00:04:33,410 --> 00:04:39,100
We'll see verbose is equal to zero so I don't see any output that will grab the very first index and

55
00:04:39,110 --> 00:04:45,360
return there which means the predicted word is just going to be off that tokenizer

56
00:04:48,600 --> 00:04:56,060
so tokenizer call index word and then grab that predicted word index and then we're going to update

57
00:04:56,600 --> 00:04:58,680
our input text.

58
00:04:58,890 --> 00:05:08,500
So say the input text is now going to have a blank space plus the predicted word and let's make sure

59
00:05:08,500 --> 00:05:10,900
we say a word here for a predicted word

60
00:05:13,780 --> 00:05:21,910
and then we'll take our output text and we'll append the predicate that word.

61
00:05:22,100 --> 00:05:22,720
And that's it.

62
00:05:22,970 --> 00:05:23,390
OK.

63
00:05:23,510 --> 00:05:28,700
So let's kind of go line by line here because there is a lot going on through a lot of function calls.

64
00:05:28,700 --> 00:05:30,630
So what does generate text function doing.

65
00:05:30,800 --> 00:05:35,640
It's taking in the model we just trained the tokenizer which has knowledge about the vocabulary in what

66
00:05:35,660 --> 00:05:38,760
ID number goes with what word the sequence link.

67
00:05:38,990 --> 00:05:41,100
Some see text you want to start off with.

68
00:05:41,300 --> 00:05:45,690
And this is robust enough to have shorter sweet text or longer see text and the sequence length.

69
00:05:45,740 --> 00:05:51,140
But for best purposes or best results you should try to make these essentially equal to each other you

70
00:05:51,140 --> 00:05:54,640
should make the text the same length as what was trained on.

71
00:05:54,680 --> 00:05:58,630
Otherwise you have to pad it and then the number of words we want to generate.

72
00:05:58,670 --> 00:06:01,580
So we have our output text which is the final output.

73
00:06:01,640 --> 00:06:07,010
We have our initial seed sequence that initial seeding text and then let's say I want to generate 10

74
00:06:07,010 --> 00:06:07,850
words.

75
00:06:07,850 --> 00:06:13,210
So for i in range number of words so I'm going to do this 10 times I want to generate 10 words.

76
00:06:13,580 --> 00:06:18,340
I'm going to first take the input text string and encode it to be a sequence.

77
00:06:18,440 --> 00:06:24,860
Essentially what we did earlier we transform those raw text data into sequences of numbers.

78
00:06:25,570 --> 00:06:31,420
Then if my CTX happens to be too short or too long I may need to pad it.

79
00:06:31,450 --> 00:06:34,600
I mean need to cut it off or I may need to add to it.

80
00:06:34,930 --> 00:06:41,530
So what I'm going to do is call pad and coding to make sure that it matches are trained rate for best

81
00:06:41,530 --> 00:06:42,030
results.

82
00:06:42,040 --> 00:06:44,830
Hopefully the CTX happens to be the same as a sequence like.

83
00:06:44,880 --> 00:06:46,880
But this makes it more robust.

84
00:06:46,990 --> 00:06:51,150
After that I'm going to predict the class probabilities for each word.

85
00:06:51,150 --> 00:06:55,270
So modeled classes is essentially going to throw out the entire vocabulary.

86
00:06:55,360 --> 00:06:58,210
Assign a probability to the most likely next word.

87
00:06:58,330 --> 00:07:04,270
So they'll maybe you'll say something like 6 percent probability that the next word is Ishmail 7 percent

88
00:07:04,270 --> 00:07:06,340
probability the next word is call et cetera.

89
00:07:06,340 --> 00:07:08,240
So we can do that across the entire vocabulary.

90
00:07:08,350 --> 00:07:12,000
So you can imagine most words are have a very small probability.

91
00:07:12,010 --> 00:07:14,450
Next we're going to have the actual predicted word.

92
00:07:14,470 --> 00:07:21,010
So the way that classes works when we index it with 0 it's going to return the index of that particular

93
00:07:21,010 --> 00:07:21,330
word.

94
00:07:21,350 --> 00:07:28,060
Essentially it's ID which if we can call tokenizer the index word from before we just pass in that index

95
00:07:28,090 --> 00:07:35,060
and it matches with the actual word then we're going to take in the input text and I'm going to add

96
00:07:35,060 --> 00:07:37,580
a space and then add on that perfect word.

97
00:07:37,880 --> 00:07:48,240
So if my input text in the very beginning was 25 words after running this here for the first loop were

98
00:07:48,290 --> 00:07:50,550
the first pass on this for a loop.

99
00:07:50,630 --> 00:07:55,670
It's now going to be 26 words which means I'm then going to pet it and that's why I'm going to truncate

100
00:07:55,670 --> 00:07:56,970
with PRI here.

101
00:07:57,020 --> 00:07:59,040
So it chops off the very first word.

102
00:07:59,090 --> 00:08:01,910
So essentially creating sequences as it goes along.

103
00:08:02,030 --> 00:08:05,210
But more and more of the sequence is going to be in my predicted words.

104
00:08:05,210 --> 00:08:10,700
And if you make numb generated words long enough eventually you'll just be predicting on your own predicted

105
00:08:10,700 --> 00:08:11,090
words.

106
00:08:11,090 --> 00:08:13,830
So true generation without even any seed.

107
00:08:14,060 --> 00:08:15,130
Well there is always a seed.

108
00:08:15,140 --> 00:08:20,540
But after you do this enough times if your number of generated words is longer than your CTX number

109
00:08:20,540 --> 00:08:24,190
of words then you'll be predicting off your predicted words.

110
00:08:24,200 --> 00:08:29,270
Now we still want actually a pen that predicted word so we'll say the up text and we'll append the predicted

111
00:08:29,270 --> 00:08:29,720
word.

112
00:08:29,960 --> 00:08:37,290
So this input text is for prediction purposes this output text is all I'm actually going to show some.

113
00:08:37,310 --> 00:08:39,620
Only going to show these predicted words of the up.

114
00:08:39,980 --> 00:08:40,430
OK.

115
00:08:40,700 --> 00:08:42,230
So we can go ahead and run that.

116
00:08:42,230 --> 00:08:44,170
So that function is ready to go for us now.

117
00:08:44,360 --> 00:08:46,640
And now it's time to generate a seed sequence.

118
00:08:46,730 --> 00:08:51,230
I'm first going to generate using the probably really poor network that we showed here and then are

119
00:08:51,230 --> 00:08:55,850
going to load in the much larger more robust network.

120
00:08:55,850 --> 00:09:00,150
So what I will do is just from my text sequences I only grab the very first one.

121
00:09:00,230 --> 00:09:02,310
Call me Ishmael some years ago et cetera.

122
00:09:02,660 --> 00:09:08,790
And then you can either grab the sequence of your choosing or you can grab a random sequence.

123
00:09:08,870 --> 00:09:17,540
So what I could also do is say import random random seed Possum's a number like 101 and then you could

124
00:09:17,540 --> 00:09:20,170
randomly pick out.

125
00:09:20,380 --> 00:09:31,210
So we can say random loops random dot Ranz integer from 0 to the length of text sequences and then just

126
00:09:31,210 --> 00:09:38,270
grab some random seed text by calling text sequences off of your random pick.

127
00:09:39,980 --> 00:09:45,430
So the random see Texan looks something like this is just a random sentence in our tech sequences.

128
00:09:45,440 --> 00:09:51,410
It's again up to you if you want to choose your own text sequence like the famous first lines or just

129
00:09:51,590 --> 00:09:53,630
randomly choose some see text.

130
00:09:53,660 --> 00:09:55,990
So once you've done that we'll go ahead and join this.

131
00:09:56,000 --> 00:10:07,290
So it's actually a sentence we'll say see text is equal to and we'll say join random CTX.

132
00:10:07,330 --> 00:10:10,100
So now our CTX just looks like a sentence like this.

133
00:10:10,120 --> 00:10:11,490
And throwing the clothes to one side.

134
00:10:11,500 --> 00:10:14,300
He really did this in not only a civil but it really kind and true.

135
00:10:14,320 --> 00:10:17,220
But the way I stood looking and so on.

136
00:10:17,710 --> 00:10:21,400
So I would recommend that because of the unique writing style of Moby Dick.

137
00:10:21,400 --> 00:10:27,970
It's an older book has a more unique writing style that you actually use the text from Moby Dick as

138
00:10:27,970 --> 00:10:28,920
your sweet text.

139
00:10:28,990 --> 00:10:32,830
But if you really wanted to you could technically just write your own sentence here.

140
00:10:32,830 --> 00:10:39,820
Like I went to the ship and then keep going until you hit 25 words.

141
00:10:39,890 --> 00:10:44,320
Again I would really recommend that because unless you're good at mimicking the style of Moby Dick writing

142
00:10:44,770 --> 00:10:46,200
in it it's up to you.

143
00:10:46,420 --> 00:10:50,900
But you see texts eventually at the end should be some string that looks like this.

144
00:10:50,950 --> 00:10:55,860
That is hopefully 25 tokens long or whatever your sequence length happens to be.

145
00:10:56,230 --> 00:11:03,130
Once you have that ready to go we're simply going to call generate text we Pastner model passenger tokenizer

146
00:11:04,180 --> 00:11:12,050
passenger sequence link all things that we defined earlier passen our text and it happens to be equal

147
00:11:12,050 --> 00:11:18,470
to see text just to make sure it happens to be the case soci testicle to see text and then the number

148
00:11:18,470 --> 00:11:23,360
of generated words will go ahead and generate 25 words off of this.

149
00:11:23,390 --> 00:11:27,650
Keep in mind this is a really poor model that we just train really train that on two epochs that horrible

150
00:11:27,650 --> 00:11:28,470
accuracy.

151
00:11:28,490 --> 00:11:30,870
So I would expect to get pretty bad results here.

152
00:11:30,950 --> 00:11:34,650
So let's go ahead and run this.

153
00:11:34,770 --> 00:11:36,150
It looks like I made a slight typo.

154
00:11:36,150 --> 00:11:42,120
I said text singular two sequences it should be text two sequences that will come back up here and fix

155
00:11:42,120 --> 00:11:44,760
that in our original file.

156
00:11:44,760 --> 00:11:47,310
So I kept mentioning kind of really easy to make a typo here.

157
00:11:47,580 --> 00:11:52,810
So it should be if we take a look at tab here it should be text with an s.

158
00:11:52,830 --> 00:11:56,490
So I'll say teks two sequences so run that again.

159
00:11:56,730 --> 00:11:58,080
Let's run these cells.

160
00:11:58,080 --> 00:12:02,090
So we have our CTX now generate Texican.

161
00:12:02,290 --> 00:12:06,060
It looks like there's one more typo that underscore actually shouldn't be there.

162
00:12:06,350 --> 00:12:09,440
So come back up here and fix that as well.

163
00:12:12,690 --> 00:12:14,580
Now you should be able to generate some text.

164
00:12:14,850 --> 00:12:15,960
And there we go.

165
00:12:15,960 --> 00:12:19,350
So if you ran this for two epochs you should see really similar results.

166
00:12:19,350 --> 00:12:24,400
It should is essentially just predict the most common word to be every single word.

167
00:12:24,480 --> 00:12:30,270
So since that performed poorly Let's now load in a model that I trained on the entirety of Moby Dick

168
00:12:30,270 --> 00:12:31,870
for about 300 epochs.

169
00:12:31,920 --> 00:12:36,510
It was still only getting around 60 percent accuracy after 300 epochs but the results are a lot better

170
00:12:36,510 --> 00:12:37,710
than what's shown here.

171
00:12:37,710 --> 00:12:38,610
Let's go ahead and load it in.

172
00:12:38,610 --> 00:12:48,480
We'll say from carious that models import load model and we'll say now are models equal to load model

173
00:12:48,810 --> 00:12:52,410
and the file is called IPAC and there's two of them here.

174
00:12:52,440 --> 00:12:55,070
The really large one is called IPAC big.

175
00:12:55,110 --> 00:13:01,140
So go ahead and import IPAC big five and then the next thing we need to do is load up the tokenizer

176
00:13:01,890 --> 00:13:09,350
that matches this larger file because this was tokenize thing the entirety of the Moby Dick text so

177
00:13:09,360 --> 00:13:15,840
say load and then I'm going to open and this one is the big with no.

178
00:13:15,910 --> 00:13:21,200
5 is the actual tokenizer and I'm going to read it in binary.

179
00:13:21,200 --> 00:13:25,190
So run that and then let's go ahead and copy and paste.

180
00:13:25,240 --> 00:13:26,100
Generate text.

181
00:13:26,180 --> 00:13:29,890
And when you run this you should get back much more reasonable results.

182
00:13:29,900 --> 00:13:35,150
So now that is generating text that says at that stub my frame Roman eyes of his own power for the whales

183
00:13:35,150 --> 00:13:37,100
green wrenched et.

184
00:13:37,160 --> 00:13:41,180
So whether or not this is a good prediction is kind of up to you.

185
00:13:41,440 --> 00:13:41,850
OK.

186
00:13:41,960 --> 00:13:47,510
I encourage you to play around with training your own models maybe on larger sequences or smaller sequences

187
00:13:47,780 --> 00:13:48,120
for more.

188
00:13:48,140 --> 00:13:52,880
E POCs as well as just playing around the different CTX maybe you write your own sweet text.

189
00:13:53,160 --> 00:13:54,270
Okay thanks.

190
00:13:54,290 --> 00:13:55,610
And we'll see you at the next lecture.

