WEBVTT
ï»¿1
00:00:05.600 --> 00:00:06.380
Welcome back.

2
00:00:06.500 --> 00:00:11.240
Let's quickly discuss an overview of what topic modeling is before we actually dive into the methods

3
00:00:11.240 --> 00:00:12.420
of topic modeling.

4
00:00:13.970 --> 00:00:19.480
So we're going to learn about latents there is a location and non-negative matrix factorization.

5
00:00:19.520 --> 00:00:26.580
The first to understand general topic of topic modeling topic modeling allows us to efficiently analyze

6
00:00:26.580 --> 00:00:33.330
large volumes of text by clustering documents together into topics a large amount of text data is actually

7
00:00:33.420 --> 00:00:39.150
unlabelled meaning we won't be able to apply our previous supervised learning approaches because those

8
00:00:39.150 --> 00:00:44.580
machine learning models would actually depend on historical labeled data and in the real world specifically

9
00:00:44.580 --> 00:00:45.470
for text data.

10
00:00:45.600 --> 00:00:51.960
You're not going to have a convenient label attached to a text dataset such as positive or negative

11
00:00:52.410 --> 00:00:54.710
or spam versus him.

12
00:00:54.740 --> 00:00:59.130
Instead you may have a variety of labels such as the different categories.

13
00:00:59.160 --> 00:01:03.200
A newspaper article could be in and you may just have the text itself unlabelled.

14
00:01:03.390 --> 00:01:08.600
So it's up to us to try to discover those labels through topic modeling.

15
00:01:08.890 --> 00:01:12.960
So again if we have unlabeled data then we can attempt to discover these labels.

16
00:01:13.000 --> 00:01:18.160
And in the case of text data this means really attempting to discover clusters of similar documents

17
00:01:18.370 --> 00:01:24.720
grouped together hopefully by some sort of topic a very important idea to keep in mind here is that

18
00:01:24.720 --> 00:01:31.080
it's actually very difficult to evaluate an unsupervised learning model's effectiveness because we didn't

19
00:01:31.080 --> 00:01:34.790
actually know the correct topic or the right answer to begin with.

20
00:01:34.800 --> 00:01:39.970
All we know is that the documents clustered together share some sort of similar topic ideas.

21
00:01:40.050 --> 00:01:43.820
It's up to the user to identify what these topics actually represent.

22
00:01:44.070 --> 00:01:46.840
So again these unsupervised learning algorithms.

23
00:01:46.950 --> 00:01:51.450
There's not really a good way to evaluate how well they did because we never really had a right answer

24
00:01:51.450 --> 00:01:52.290
to begin with.

25
00:01:53.630 --> 00:01:58.250
We're going to begin by examining how latents there Asli allocation works and how it can attempt to

26
00:01:58.250 --> 00:02:01.010
discover topics for a corpus of documents.

27
00:02:01.010 --> 00:02:01.930
Let's get started.

28
00:02:01.970 --> 00:02:02.890
Aasiya at the next lecture.

