1
00:00:05,250 --> 00:00:06,180
Welcome back everyone.

2
00:00:06,270 --> 00:00:10,590
And this lecture we're going to go through the solutions for the topic modeling Assessment Project.

3
00:00:10,590 --> 00:00:12,620
Let's head over to the notebook and get started.

4
00:00:13,440 --> 00:00:13,670
All right.

5
00:00:13,680 --> 00:00:16,510
Here I am at the topic modeling Assessment Project notebook.

6
00:00:16,710 --> 00:00:24,000
The first they're going to do is import pandas as PD and then read in that data set for Quora.

7
00:00:24,270 --> 00:00:31,450
So PD that read and it's a CSP file and it's called Korac questions you should be able to tap autocomplete

8
00:00:31,470 --> 00:00:32,080
this.

9
00:00:32,130 --> 00:00:37,130
If not you'll need to provide the entire file path and then we can check the head of the state of frame

10
00:00:39,270 --> 00:00:42,210
and you notice it's just a series of questions.

11
00:00:42,330 --> 00:00:47,750
So if you ever wanted to grab one of these singular questions where you could do is simply from the

12
00:00:47,750 --> 00:00:50,450
question column grab any index.

13
00:00:50,480 --> 00:00:53,120
So what is step by step guide to invest in share market.

14
00:00:53,120 --> 00:00:55,540
India is the first question.

15
00:00:55,580 --> 00:00:57,890
So for pre-processing it's pretty straightforward.

16
00:00:57,950 --> 00:01:07,660
We simply say from S-K learn the feature extraction thought text import T.F. IDF vectorize or then we'll

17
00:01:07,660 --> 00:01:08,730
create it.

18
00:01:09,430 --> 00:01:17,870
Let's create an instance of our vectorize or T.F. IDF and we'll say T.F. idea of vectorize or and you

19
00:01:17,870 --> 00:01:24,950
can play around with the max document frequency values will say we're looking for terms that show up

20
00:01:25,650 --> 00:01:28,040
at our max docking at frequencies 95 percent.

21
00:01:28,040 --> 00:01:32,930
So kind of drop the 5 percent most frequent terms since those probably aren't conducive to any particular

22
00:01:32,930 --> 00:01:33,910
topic.

23
00:01:33,950 --> 00:01:40,160
SR so common and then also essentially remove any words unique to any documents by saying that the words

24
00:01:40,340 --> 00:01:48,990
show up at least in two documents and also pasand upwards English English.

25
00:01:48,990 --> 00:01:49,980
There we go.

26
00:01:50,670 --> 00:01:53,420
And then we'll create our document matrix.

27
00:01:53,440 --> 00:01:58,560
Again it's not really a dominant term matrix we're also performing T.F. IDF on it but it's just to keep

28
00:01:58,560 --> 00:01:59,160
things in line.

29
00:01:59,160 --> 00:02:07,070
What we did before Deif IDF and then we fit transform on the article or on the question column here

30
00:02:09,550 --> 00:02:18,270
so run that and at the end you should see a matrix with 400 and 4000 ish questions and then around 40000

31
00:02:18,280 --> 00:02:19,760
words.

32
00:02:19,780 --> 00:02:22,310
Now it's time for non-negative matrix factorization.

33
00:02:22,390 --> 00:02:26,130
If you want you can also perform LDA but LDH takes quite a bit of time.

34
00:02:26,200 --> 00:02:34,220
So we're going to stick with an MF sual say from Skillern about the composition import and F. We'll

35
00:02:34,220 --> 00:02:42,350
create an instance of a MF model by calling an MF saying number of components.

36
00:02:42,380 --> 00:02:47,330
And here we specifically asked for 20 topics and if you want make sure the topics are lined up the same

37
00:02:47,330 --> 00:02:48,650
way I have them here.

38
00:02:48,650 --> 00:02:51,690
Go ahead and put in random state is equal to 42.

39
00:02:51,740 --> 00:02:54,160
Again the number 42 is completely arbitrary.

40
00:02:54,180 --> 00:02:56,980
It's just to make sure that your random values are the same as mine.

41
00:02:57,010 --> 00:03:06,340
Setting a random seed and then it's time to actually fit the model so we will fit the model to our documentary

42
00:03:06,390 --> 00:03:12,830
matrix and then we're going to print out our top 15 most common words for each of the topics.

43
00:03:12,930 --> 00:03:15,220
So I'll go ahead and get started on this while that's fitting.

44
00:03:15,300 --> 00:03:28,350
I will say for index topic in enumerate and IMF model and IMF underscore model components underscore

45
00:03:29,130 --> 00:03:35,220
we're going to do is we'll just print out that top 15 words a line using some string literals will say

46
00:03:36,150 --> 00:03:44,380
the top 15 words for topic number and then we'll pass an index and that's the current topic number we're

47
00:03:44,380 --> 00:03:45,060
on.

48
00:03:45,360 --> 00:03:48,390
And then we'll use less comprehension just like we did before.

49
00:03:48,750 --> 00:03:58,440
We simply grab that TAF IDF T.F. IDF object and then we're going to say get under score feature underscore

50
00:03:58,440 --> 00:04:09,300
names and then grab it at index eye for eye in topic loops in topic and recall we are sort.

51
00:04:09,640 --> 00:04:17,240
And then if we want the last 15 we say from negative 15 all the way to the end and then we're going

52
00:04:17,240 --> 00:04:20,390
to print out a new line.

53
00:04:20,390 --> 00:04:20,820
All right.

54
00:04:21,020 --> 00:04:25,400
So as I previously mentioned this is a much bigger file than before.

55
00:04:25,400 --> 00:04:29,080
So an IMF model is still going to take a while LDA will take even longer.

56
00:04:29,190 --> 00:04:32,390
Someone quickly hop forward in time until this is done training.

57
00:04:32,690 --> 00:04:34,390
OK so that's done training for me now.

58
00:04:34,400 --> 00:04:38,930
It took several minutes and I have a pretty fast computer so it may take you longer depending on your

59
00:04:38,930 --> 00:04:39,570
hardware.

60
00:04:39,650 --> 00:04:44,420
If you have a really slow or small hardware computer there's a possibility of getting a memory error

61
00:04:44,420 --> 00:04:44,880
here.

62
00:04:45,080 --> 00:04:50,210
And if that's the case there's really nothing you can do except either take a subset of that data set

63
00:04:50,630 --> 00:04:52,410
or get better hardware.

64
00:04:52,670 --> 00:04:57,110
So let me run this next topic and we can see here the top 15 words.

65
00:04:57,110 --> 00:04:59,730
So here are the various topics we found.

66
00:04:59,750 --> 00:05:00,460
Number two.

67
00:05:00,470 --> 00:05:02,400
Number three and so on.

68
00:05:02,420 --> 00:05:07,250
So the last thing to do is actually connect these topic numbers to the questions themselves.

69
00:05:07,260 --> 00:05:09,630
So remember our original data frame looks like this.

70
00:05:09,650 --> 00:05:11,470
And we want it to look something like that.

71
00:05:11,870 --> 00:05:12,640
And then we can do this.

72
00:05:12,650 --> 00:05:13,690
It's pretty straightforward.

73
00:05:13,850 --> 00:05:25,680
We simply say something like Topic results is equal to an MF model and we're going to transform our

74
00:05:25,680 --> 00:05:35,470
original DTM and then we're going to say topic results and Graham the max coefficient value along access

75
00:05:35,500 --> 00:05:44,450
equal to 1 and then we'll say Corra topic is equal to topic results.

76
00:05:44,460 --> 00:05:48,880
RMX is equal to 1.

77
00:05:48,960 --> 00:05:53,030
Actually we could just say that this is something else whip's kind of wrote this line twice there.

78
00:05:53,160 --> 00:05:57,040
So core topic topic results are maximal to 1.

79
00:05:57,060 --> 00:06:02,670
So if you run that and then check the head of the data frame that should have matched up the topic based

80
00:06:02,670 --> 00:06:04,480
on the index this for the questions.

81
00:06:04,580 --> 00:06:09,960
And we can see here if you use the same random state as I did the first three topics would be 5 16 and

82
00:06:09,960 --> 00:06:10,800
17.

83
00:06:10,830 --> 00:06:13,690
Again that's only if you happen to use the same or in them they did.

84
00:06:14,030 --> 00:06:14,560
OK.

85
00:06:14,730 --> 00:06:16,380
So that's it for this project.

86
00:06:16,440 --> 00:06:21,870
Hopefully you can see now it's pretty straightforward to perform topic modeling and psychic learns has

87
00:06:21,870 --> 00:06:23,740
a lot of convenient functions for you to use.

88
00:06:23,880 --> 00:06:27,840
And really all of this is essentially it looks like just around 15 cells.

89
00:06:27,900 --> 00:06:33,360
So just a few lines of code in order to perform topic modeling on a really large dataset.

90
00:06:33,390 --> 00:06:33,830
Thanks.

91
00:06:33,870 --> 00:06:37,940
And that concludes the topic modeling section will see at the next section of the course.

