1
00:00:05,250 --> 00:00:09,280
Welcome back everyone to this lecture on Keres basics.

2
00:00:09,310 --> 00:00:13,660
We're going to learn how to create a very simple neural network for classifying the famous ires.

3
00:00:13,660 --> 00:00:19,830
They set the goal of this particular lecture is to just get you comfortable with the syntax with.

4
00:00:20,170 --> 00:00:25,930
The data itself contains measurements of flower pedals and steeples and corresponding labels to one

5
00:00:25,930 --> 00:00:27,060
of three classes.

6
00:00:27,160 --> 00:00:29,360
That is one of three flower species.

7
00:00:29,380 --> 00:00:30,610
It's a really simple data set.

8
00:00:30,620 --> 00:00:33,310
It's super famous has its own Wikipedia page.

9
00:00:33,430 --> 00:00:37,090
So we're going to be working on this just to learn the syntax basics.

10
00:00:37,090 --> 00:00:38,900
Let's open up a notebook and get started.

11
00:00:39,680 --> 00:00:41,090
OK here I am at a notebook.

12
00:00:41,140 --> 00:00:47,030
I want to start off by importing some pie as N.P. because we may need to use some pie later and then

13
00:00:47,030 --> 00:00:48,730
I'm going to read in the data set.

14
00:00:48,770 --> 00:00:54,020
The state is famous enough that it's actually built into sikat learn so I can simply say from S-K learn

15
00:00:54,920 --> 00:01:00,910
that data sets import and there is a load Irus function.

16
00:01:01,100 --> 00:01:11,020
So load underscore Irus and then I'm going to create a variable called Irus and then run the load Irus

17
00:01:11,030 --> 00:01:11,740
function.

18
00:01:11,810 --> 00:01:17,660
And if you take a look at what type of object is returned here it's actually a specialized Bunche object

19
00:01:17,660 --> 00:01:18,780
from sikat learn.

20
00:01:19,070 --> 00:01:24,650
And if you check its attributes it has feature names Target Target names and the data.

21
00:01:24,710 --> 00:01:30,110
So it's actually already kind of restructured for us and you can print out the description attribute

22
00:01:30,800 --> 00:01:33,640
and it prints out an entire description of what this is.

23
00:01:33,650 --> 00:01:38,120
So is essentially just as I mentioned simple links and widths and centimeters and then pedal lengths

24
00:01:38,150 --> 00:01:44,720
and pedal with in centimeters and the corresponding class there's 150 instances and there's four attributes

25
00:01:45,170 --> 00:01:51,560
and the flowers belong to a species of Iris it's either iris a Tosa Iris versicolor or Iris virginica

26
00:01:51,970 --> 00:01:55,480
and some some are sticks and then famous Iris dataset.

27
00:01:55,520 --> 00:01:56,800
Lots more info here.

28
00:01:57,170 --> 00:02:02,210
OK so let's actually now grab the feature information and the label information.

29
00:02:02,210 --> 00:02:07,780
It's actually already organized for us with attribute calls from the spunge object.

30
00:02:07,890 --> 00:02:15,040
So I will say my X features is equal to Irus data and if we take a look at X it's essentially a name

31
00:02:15,060 --> 00:02:18,530
pyrite with the four measurements lined up per instance.

32
00:02:18,690 --> 00:02:22,430
So people length's people with Pedda length and pedal with.

33
00:02:22,470 --> 00:02:24,150
So we have X ready to go.

34
00:02:24,150 --> 00:02:27,130
Now what we need to do is grab our labels.

35
00:02:27,210 --> 00:02:32,390
So that's also organized for us in the form of a target attribute.

36
00:02:32,520 --> 00:02:35,890
Essentially the target variable or label target.

37
00:02:35,950 --> 00:02:39,210
If you check out what Y looks like it's sorted.

38
00:02:39,210 --> 00:02:43,100
So we can see we have class 0 class 1 and class 2.

39
00:02:43,380 --> 00:02:47,790
Now because of the way carriers and your own that works work in general we're actually going to want

40
00:02:47,790 --> 00:02:52,080
to convert this to a categorical labeling system.

41
00:02:52,080 --> 00:02:53,730
Essentially what encoding.

42
00:02:53,970 --> 00:03:03,000
So what that means is I want a vector that's going to be zero for everywhere that this class doesn't

43
00:03:03,000 --> 00:03:03,840
match up.

44
00:03:03,840 --> 00:03:11,190
So for example class zero is going to be one index zero and then zero everywhere else for the other

45
00:03:11,190 --> 00:03:11,990
two classes.

46
00:03:12,150 --> 00:03:15,850
So essentially these become indexed additions 0 1 and 2.

47
00:03:15,870 --> 00:03:18,670
So a class one instance.

48
00:03:18,690 --> 00:03:24,580
So if we had class actually this would be class 0 it would look like this.

49
00:03:24,790 --> 00:03:31,250
And then class of one would end up getting a one at that that exhibition.

50
00:03:32,590 --> 00:03:41,800
And then a class of two would end up getting a one or one hand-coded at this and exposition.

51
00:03:41,820 --> 00:03:43,750
So this is known as one encoding.

52
00:03:43,920 --> 00:03:48,390
And we're going to be able to do this quite easily using some built in tools of course.

53
00:03:48,390 --> 00:03:53,480
So all we're doing is transforming each of these into its own little vector of three.

54
00:03:53,550 --> 00:04:02,320
And the way we do this is we say From cares that you tildes import and we import to underscore categorical.

55
00:04:02,640 --> 00:04:07,830
So we can say from Keres import to underscore categorical.

56
00:04:08,160 --> 00:04:08,910
There we go.

57
00:04:08,910 --> 00:04:10,790
And then we simply just cast it.

58
00:04:10,790 --> 00:04:15,450
We say why is not equal to two categorical Y.

59
00:04:15,690 --> 00:04:19,460
And if you check out the shape of y now it's 150 instances.

60
00:04:19,500 --> 00:04:22,190
Now it has three values per label.

61
00:04:22,410 --> 00:04:26,940
So if we actually take a look at what it looks like now you can see it's essentially the one high encoding

62
00:04:27,030 --> 00:04:28,320
so ones are 0.

63
00:04:28,380 --> 00:04:30,990
Then we move onto the next last 0 or 1 0.

64
00:04:30,990 --> 00:04:33,650
Move on to the next last and then 0 0 1.

65
00:04:33,720 --> 00:04:39,630
So now that we have set up for cares what we can do is continue on to split the data into a training

66
00:04:39,630 --> 00:04:41,390
set and a test set.

67
00:04:41,400 --> 00:04:50,270
So we're going to do is just psychic learn from S-K learn that model selection import train test split.

68
00:04:50,480 --> 00:04:56,630
Then we simply call train test split do shift tab as I previously mentioned that we can quickly copy

69
00:04:56,960 --> 00:05:01,480
the tuple that gets created essentially unpacking the values.

70
00:05:01,760 --> 00:05:10,340
So we will copy and paste that X train x test Y train Y test and then just pass it my X features my

71
00:05:10,340 --> 00:05:13,330
y label the sign on a test size.

72
00:05:13,400 --> 00:05:18,760
I'll go ahead and say 33 percent of the data will be testing data and then random state.

73
00:05:18,860 --> 00:05:21,250
If you want the exact same split as I will get.

74
00:05:21,290 --> 00:05:23,630
Go ahead and use 42.

75
00:05:23,670 --> 00:05:31,930
So we run that and now we have the four components the training data the testing features as well as

76
00:05:31,930 --> 00:05:34,490
the labels for the training data.

77
00:05:34,720 --> 00:05:37,220
And then the labels for the test data.

78
00:05:37,270 --> 00:05:41,100
Notice now everything's been shuffled and randomized which is exactly what you want.

79
00:05:41,230 --> 00:05:45,910
You don't want to train on things in an order otherwise you'll get really good at classifying zeros

80
00:05:45,910 --> 00:05:48,950
first and then when it comes to classifying ones you'll get really bad.

81
00:05:49,030 --> 00:05:54,040
So we always want to shuffle that data which is done automatically for us with a train test split which

82
00:05:54,040 --> 00:05:57,310
is why we have a random state because it is randomly shuffled.

83
00:05:57,410 --> 00:06:01,320
So we want to make sure you get the same shuffling I do now for neural networks.

84
00:06:01,330 --> 00:06:05,420
It's usually a really good idea to scale or standardized your data.

85
00:06:05,710 --> 00:06:11,020
So for this particular instance we kind of don't need to but it's a good idea anyways so we're going

86
00:06:11,020 --> 00:06:14,010
to show you how to do it occasionally do it for your own problems.

87
00:06:15,400 --> 00:06:22,950
Simply say for him as he learned that pre-processing and we import min max scaler and what this does

88
00:06:23,010 --> 00:06:29,160
is it simply makes all the values fit between a range like zero and one or negative to 1 to 1.

89
00:06:29,250 --> 00:06:34,260
And what this does is essentially divided by the max value to make sure everything fits between 0 and

90
00:06:34,260 --> 00:06:34,810
1.

91
00:06:35,040 --> 00:06:40,830
So what I mean by that is let's imagine you had a list of values like five 10 15 20.

92
00:06:40,860 --> 00:06:47,010
So if you took this list and then divided it by the max value so this would actually have to be an array.

93
00:06:47,070 --> 00:06:52,440
So if you took this array of values and then divided it by the max value which is 20 when you run that

94
00:06:52,620 --> 00:06:57,220
you'll notice now everything is between 0 and 1 because you divided by the max value.

95
00:06:57,240 --> 00:06:59,540
That's essentially what minimax scalar does.

96
00:06:59,550 --> 00:07:02,100
There's lots of other scaling methods but this one is pretty simple.

97
00:07:02,100 --> 00:07:08,540
So we're going to kind of run on this ideology we just simply create a scalar object just like you would

98
00:07:08,540 --> 00:07:11,270
for anything else with sikat learn.

99
00:07:11,870 --> 00:07:13,680
And then we're going to take her scalar object.

100
00:07:13,700 --> 00:07:17,470
Now we're going to fit it and we're going to fit it to only the training data.

101
00:07:17,720 --> 00:07:21,080
We don't fit it on all the data because that would essentially be cheating.

102
00:07:21,080 --> 00:07:24,560
We don't want to assume prior knowledge of the test data.

103
00:07:24,620 --> 00:07:30,740
So we're only going to fit it to the training features and then we're going to create skilled versions

104
00:07:30,740 --> 00:07:31,650
of our trained data.

105
00:07:31,940 --> 00:07:36,850
We will say scaled X train is equal to the scalar object.

106
00:07:37,100 --> 00:07:45,950
And then we will transform our X training data and we'll do the same thing for the test data we'll call

107
00:07:45,950 --> 00:07:51,760
scalar object and then we will transform our test data.

108
00:07:52,930 --> 00:07:55,470
So if we take a quick peek at the training data.

109
00:07:55,470 --> 00:07:58,990
Now you'll notice all the values are between 0 and 1.

110
00:07:59,110 --> 00:08:00,670
So that converts everything.

111
00:08:00,730 --> 00:08:07,060
And typically that helps neural networks helps the weights and biases from growing to large.

112
00:08:07,090 --> 00:08:09,590
OK so we have the skilled versions of our data.

113
00:08:09,780 --> 00:08:15,060
What we're going to do now is actually build a network of course which is actually a pretty simple process.

114
00:08:15,060 --> 00:08:24,240
We simply say from Karris that models import sequential essentially a bunch of sequences of layers and

115
00:08:24,240 --> 00:08:29,180
then we'll say from cares that layers and we're doing a really simple model here.

116
00:08:29,200 --> 00:08:34,680
So we're just going to import a dense layer later on when we actually perform things like text generation.

117
00:08:34,690 --> 00:08:36,630
These models are going to get a lot more complicated.

118
00:08:36,730 --> 00:08:41,860
We'll import different types of units like Ellis T.M. units but keep things simple just sequential and

119
00:08:41,860 --> 00:08:43,320
dense.

120
00:08:43,460 --> 00:08:46,140
And this is really the syntax we want to get familiar with.

121
00:08:46,250 --> 00:08:51,650
We first create the sequential model and then we simply add layers to the model.

122
00:08:51,650 --> 00:08:52,410
So we say model.

123
00:08:52,460 --> 00:08:58,700
And in this case only important one kind of led to an added layer and each particular layer has different

124
00:08:58,700 --> 00:09:05,680
parameters so dense we want to describe how many neurons or units go into that sense layer.

125
00:09:05,810 --> 00:09:07,700
What should the activation function be.

126
00:09:07,790 --> 00:09:11,680
And then you can do other things like have a bias initialiser et cetera.

127
00:09:11,690 --> 00:09:17,150
A kernel initialiser often those sort of initializers it's fine if the default value is the only thing

128
00:09:17,150 --> 00:09:19,610
you should probably mention is the input dimensions.

129
00:09:19,730 --> 00:09:20,910
So we'll do that as well.

130
00:09:21,820 --> 00:09:25,370
So I'll say dance let's go ahead and use 8 neurons.

131
00:09:25,510 --> 00:09:30,650
I'm just using double the amount of features whether or not that's incorrect or correct.

132
00:09:30,740 --> 00:09:32,460
Literally no right answer for that.

133
00:09:32,560 --> 00:09:37,390
Often people have some sort of domain knowledge that helps them understand what's a good choice for

134
00:09:37,390 --> 00:09:38,080
neurons.

135
00:09:38,140 --> 00:09:42,430
It should probably be something based off the number of features like some multiple of the features.

136
00:09:42,430 --> 00:09:48,870
But again there really is no right or wrong answer for that and they will say input them insuance is

137
00:09:48,870 --> 00:09:56,130
for since we expect for features and then we'll say the activation function is r e l you for a rectified

138
00:09:56,220 --> 00:10:00,550
linear unit and let's go ahead and add in one more then Slayer

139
00:10:03,380 --> 00:10:10,860
so we have that input layer another set of eight neurons and then finally for output layer we're going

140
00:10:10,860 --> 00:10:17,880
to add in three neurons and it's going to have the soft Max activation.

141
00:10:18,410 --> 00:10:24,260
And the reason for that is essentially that three neurons what's going to happen is each neuron is going

142
00:10:24,260 --> 00:10:30,170
to have what looks like essentially a probability for belying in any particular class which is why we

143
00:10:30,170 --> 00:10:31,340
want encoded.

144
00:10:31,430 --> 00:10:33,270
So if we remember what we looked like.

145
00:10:33,560 --> 00:10:34,820
It looks something like this.

146
00:10:34,820 --> 00:10:40,370
So at the end the neurons get to return maybe like 30 percent chance for this exploration a 20 percent

147
00:10:40,370 --> 00:10:45,430
chance for this exploration and then 50 percent chance for this actually correct an expedition.

148
00:10:45,440 --> 00:10:51,380
So that's why at the end we have these three neurons and that's what we choose soft max.

149
00:10:51,380 --> 00:10:57,020
So kind of get some output that kind of looks something like this.

150
00:10:57,020 --> 00:10:58,370
It won't look exactly like this.

151
00:10:58,400 --> 00:11:02,900
Such nice probabilities but this is the general idea is going to tell you what it's leaning towards

152
00:11:02,900 --> 00:11:04,390
based off the expedition.

153
00:11:04,550 --> 00:11:06,250
And that's why we use the soft max.

154
00:11:06,530 --> 00:11:12,910
So at the end of that we simply compile our model.

155
00:11:12,990 --> 00:11:14,630
We choose some sort of loss.

156
00:11:14,640 --> 00:11:18,870
This loss is going to depend on what you're actually performing.

157
00:11:19,080 --> 00:11:29,600
So here we are performing categorical process so we want categories to say categorical underscore cross

158
00:11:30,230 --> 00:11:31,000
entropy.

159
00:11:31,250 --> 00:11:38,800
So that's how we're actually going to determine loss the optimizer will use the atom optimizer.

160
00:11:39,030 --> 00:11:45,240
And then for the metrics we're going to decide this on accuracy.

161
00:11:45,620 --> 00:11:49,030
So that's what we'll be training on and that's we'll be reporting back.

162
00:11:49,190 --> 00:11:54,030
Now if everything worked well if your model you should be able to then ask for a summary of the model

163
00:11:54,570 --> 00:12:00,180
which reports back where each layer is doing what the output shape and the number of parameters created.

164
00:12:00,180 --> 00:12:00,630
All right.

165
00:12:00,780 --> 00:12:06,330
So if you get anything for this lecture it really should be these few lines you import sequential models

166
00:12:06,450 --> 00:12:11,180
and then import the layers you want create your model and then add in the layers and the Pentagon what

167
00:12:11,180 --> 00:12:17,340
kind of import you may have to kind of tune these hyper parameters or the side more hyper parameters.

168
00:12:17,580 --> 00:12:20,690
And then at the end you compile it with some sort of choice of loss.

169
00:12:20,820 --> 00:12:24,780
And this last depends on exactly what kind of problem you're trying to solve.

170
00:12:25,730 --> 00:12:31,950
Then once you have that just like a psychic learn model you simply call models that fit on your training

171
00:12:31,950 --> 00:12:32,710
data.

172
00:12:32,790 --> 00:12:38,470
And remember we scale their training data so Passons scaled X train and then Y train.

173
00:12:38,730 --> 00:12:43,840
We don't need to scale train because it's labels that are already zeros and ones.

174
00:12:43,940 --> 00:12:50,170
So we're going to say pocks and pocks one epoch is running through the entire training data set.

175
00:12:50,390 --> 00:12:55,080
Well go ahead and run through 150 epochs and then there's a verbose.

176
00:12:55,080 --> 00:12:58,740
Verbose is just basically how much output you want information.

177
00:12:58,740 --> 00:13:01,620
So if you put in zero it's not going to report back anything.

178
00:13:01,770 --> 00:13:05,070
If you put in one ill actually show you like a little progress bar.

179
00:13:05,220 --> 00:13:10,200
So two it shows you the pockets on how much loss and what its current accuracy is.

180
00:13:10,200 --> 00:13:14,130
So it's go ahead and run this for 150 POCs this should be relatively fast.

181
00:13:14,130 --> 00:13:15,240
I'm on a really fast computer.

182
00:13:15,260 --> 00:13:17,210
But again the data sets are pretty small.

183
00:13:17,280 --> 00:13:20,090
So at the end of this you should get something around.

184
00:13:20,280 --> 00:13:21,820
I would say 90 percent.

185
00:13:21,830 --> 00:13:26,070
So it may not be exactly this because remember there is some random initialization to the weights and

186
00:13:26,070 --> 00:13:29,410
biases but you should be getting around 90 percent.

187
00:13:29,580 --> 00:13:32,820
You may get slightly higher slightly lower after 150 pucks.

188
00:13:32,820 --> 00:13:33,270
OK.

189
00:13:33,570 --> 00:13:39,920
So another question is how do we predict on new unseen data and how do we evaluate the model performance.

190
00:13:39,990 --> 00:13:42,820
We're going be covering that in the very next lecture.

191
00:13:42,840 --> 00:13:43,350
We'll see if there.

