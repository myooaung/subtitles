1
00:00:05,440 --> 00:00:09,260
Welcome back everyone to this lecture on feature extraction from text.

2
00:00:09,280 --> 00:00:16,300
This is part one we're going to be going over that basic theory of feature extraction most classic machine

3
00:00:16,300 --> 00:00:20,500
learning algorithms can't actually accept or take in raw text.

4
00:00:20,590 --> 00:00:26,650
Instead we need to perform some sort of feature extraction from the raw text in order to pass numerical

5
00:00:26,650 --> 00:00:30,010
features to the machine learning algorithm.

6
00:00:30,040 --> 00:00:35,770
For example we could count the occurrence of each word to map text to an actual number.

7
00:00:35,770 --> 00:00:43,660
Let's discuss counter vectorization along with term frequency an inverse document frequency.

8
00:00:43,870 --> 00:00:46,490
So come vectorization looks like this.

9
00:00:46,540 --> 00:00:49,190
Let's imagine we had a list of messages.

10
00:00:49,420 --> 00:00:51,160
So here we have three messages.

11
00:00:51,160 --> 00:00:54,310
And this looks really similar to the data that we previously had.

12
00:00:54,310 --> 00:01:02,240
So right now everything is in raw text or E-string in Python what we can do is using sikat learn we

13
00:01:02,240 --> 00:01:08,090
can perform count vectorization and notice the syntax here looks really similar to the same way we created

14
00:01:08,120 --> 00:01:14,570
machinery models of sikat learn we say from S-K learn and then some family of feature extraction models

15
00:01:14,870 --> 00:01:16,550
specifically from text.

16
00:01:16,610 --> 00:01:22,550
We import this vectorize are now what this is going to do is it's going to count the occurrences of

17
00:01:22,550 --> 00:01:25,050
all the unique words.

18
00:01:25,090 --> 00:01:31,200
So after you create your count vector isor you're going to vectorize those messages.

19
00:01:31,270 --> 00:01:38,950
And basically what happens is it treats each individual unique word as a feature and then it's going

20
00:01:38,950 --> 00:01:46,690
to count the occurrence of each unique feature or word throughout every single document and each document

21
00:01:47,020 --> 00:01:49,220
is essentially just each text message.

22
00:01:49,360 --> 00:01:54,560
So it can think of that term document as just another word for every documented text.

23
00:01:55,400 --> 00:01:56,900
So if we take a look here.

24
00:01:57,110 --> 00:02:02,290
Notice that the word call didn't show up in the first message or the third message.

25
00:02:02,360 --> 00:02:08,030
It only showed up in the middle message or the word dogs only shows up in the last message.

26
00:02:08,030 --> 00:02:11,290
Now some words do show it more than once like the word go.

27
00:02:11,360 --> 00:02:12,270
And the word too.

28
00:02:12,410 --> 00:02:19,140
But overall we should see a lot of zeros because most messages are not going to contain all the words.

29
00:02:19,220 --> 00:02:25,430
So you can imagine for a very large set of documents otherwise known as a corpus we're going to have

30
00:02:25,430 --> 00:02:29,870
what's known as a very sparse matrix a matrix of a lot of zeros.

31
00:02:29,870 --> 00:02:34,450
This sort of matrix is known as the document term matrix or DTM.

32
00:02:34,490 --> 00:02:40,210
Again we're just counting the number of times each unique word throughout the entire vocabulary of all

33
00:02:40,210 --> 00:02:43,610
the documents shows up in each particular document.

34
00:02:45,700 --> 00:02:50,410
Now in alternative to count vectorization is something called T.F. idea.

35
00:02:50,520 --> 00:02:55,590
Vector Isar or term frequency inverse document frequency of vectorize or.

36
00:02:55,810 --> 00:02:59,590
It's also going to create a document matrix from our messages.

37
00:02:59,620 --> 00:03:07,930
However instead of filling in the document term matrix or DTM with token counts it calculates term frequency

38
00:03:08,020 --> 00:03:11,230
inverse document frequency value for each word.

39
00:03:11,230 --> 00:03:12,940
So let's talk about what that actually means.

40
00:03:12,940 --> 00:03:15,090
What does T.F. IETF mean.

41
00:03:16,400 --> 00:03:22,820
Well let's first look at that first term T.F. or term frequency we can think of term frequency as a

42
00:03:22,820 --> 00:03:30,230
function of the term and the particular document and all it is it's the raw count of a term a document.

43
00:03:30,230 --> 00:03:36,080
Basically just answering the question the number of times that that particular term occurs in documentary.

44
00:03:36,080 --> 00:03:42,630
So for example we take a look at the term frequency for the word dogs.

45
00:03:42,640 --> 00:03:45,700
The term frequency for the first two messages would be zero.

46
00:03:45,880 --> 00:03:50,710
And then the term frequency for the word dogs in the last message would be one.

47
00:03:50,800 --> 00:03:55,990
Again the term frequency is just the raw count of a term in a document something that we just saw.

48
00:03:56,050 --> 00:04:01,320
However term frequency alone isn't enough for a thorough feature analysis of the text.

49
00:04:01,570 --> 00:04:10,780
Let's imagine very common terms like stop such as or the because the terms such as the so common term

50
00:04:10,780 --> 00:04:16,570
frequency will tend to incorrectly emphasize documents which happen to use words like the more frequently

51
00:04:17,020 --> 00:04:23,740
without giving enough weight to the more meaningful terms like Unique words such as red and dogs and

52
00:04:23,800 --> 00:04:29,470
inversed document frequency factor is incorporated which diminishes the weight of terms that occur very

53
00:04:29,470 --> 00:04:33,030
frequently in the documents set and increases the weight.

54
00:04:33,040 --> 00:04:40,140
The terms that occur rarely It is the logarithmically scaled inverse fraction of the documents that

55
00:04:40,140 --> 00:04:45,330
contain the word obtained by dividing the total number of documents by the number of documents containing

56
00:04:45,330 --> 00:04:50,560
the term and then taking the logarithmic of that quotient.

57
00:04:50,650 --> 00:04:57,370
So we know that T.F. IDF isn't the term frequency times one over the document frequency or otherwise

58
00:04:57,370 --> 00:04:59,950
known as the inverse document frequency.

59
00:04:59,950 --> 00:05:05,110
So here we can see the formula for term frequency inverse document frequency as well as the full equation

60
00:05:05,110 --> 00:05:08,830
for inverse document frequency.

61
00:05:09,030 --> 00:05:14,900
Now fortunately for us sikat learn can calculate all of these terms for us through the use of its API.

62
00:05:14,910 --> 00:05:19,080
Keep in mind you should notice how similar the syntaxes to our previous use of machine learning models

63
00:05:19,110 --> 00:05:25,870
and sikat learn here you can see that from Eski learned that feature traction the text we're able to

64
00:05:25,960 --> 00:05:27,270
import the TAF idea.

65
00:05:27,440 --> 00:05:32,310
Victimiser And again we simply call T.F. idea of victimiser.

66
00:05:32,440 --> 00:05:37,780
And in this case we're going to want to call it transform on the actual raw text messages which is going

67
00:05:37,780 --> 00:05:45,610
to perform turn frequency inversed document frequency vectorization on the actual text messages.

68
00:05:45,610 --> 00:05:51,790
So all this is doing is not only is it taking into account the term frequency how many times the terms

69
00:05:51,790 --> 00:05:53,340
show up in a single document.

70
00:05:53,560 --> 00:06:00,070
But as well as the inversed document frequency How often do these terms show up in all across all the

71
00:06:00,070 --> 00:06:00,900
documents.

72
00:06:00,970 --> 00:06:08,650
That way the very common words aren't weighed as much as the unique words TAF IDF allows us to understand

73
00:06:08,650 --> 00:06:13,510
the context of words across an entire corpus of documents instead of just its relative importance in

74
00:06:13,510 --> 00:06:14,920
a single document.

75
00:06:14,980 --> 00:06:19,730
Coming up next we're going to explore how to perform these operations with Python in psychic alert.

76
00:06:19,960 --> 00:06:21,040
We'll see it next lecture.

