1
00:00:05,380 --> 00:00:10,030
Hello everyone and welcome to the recurrent neural networks theory lecture here in this lecture we're

2
00:00:10,030 --> 00:00:15,960
just gonna give you a brief overview of the theory behind a recurrent neural net recurrent neural networks

3
00:00:15,990 --> 00:00:19,170
are specifically designed to work with sequence data.

4
00:00:19,230 --> 00:00:25,140
So examples of sequence data are things like time series data such as how many sales have happened over

5
00:00:25,140 --> 00:00:29,960
a previous time series or what the production has been over previous time series.

6
00:00:29,970 --> 00:00:34,760
There's also things like sentences which is what we're focusing on for natural language processing.

7
00:00:34,920 --> 00:00:37,760
So you can think of a sentence as just a sequence of words.

8
00:00:37,890 --> 00:00:44,580
Things like audio or music it's basically just a sequence of sounds or even things like car trajectories.

9
00:00:44,610 --> 00:00:48,350
There's just a sequence of instructions left right forward and back.

10
00:00:48,420 --> 00:00:55,050
So again recurrent neural networks when you understand them you're really going to be focusing on applying

11
00:00:55,050 --> 00:01:00,670
them to sequence based data and there's lots of different ways of applying recurrent year networks which

12
00:01:00,670 --> 00:01:08,770
you're going to learn about in this lecture so we can imagine a sequence as just a vector of information

13
00:01:08,830 --> 00:01:14,590
where the index location basically points out its points in time and then the actual values are the

14
00:01:14,920 --> 00:01:16,100
training values.

15
00:01:16,120 --> 00:01:21,640
So if we imagine a sequence that's one two three four five six a question that you could ask anybody's

16
00:01:21,700 --> 00:01:26,030
Are you able to predict a similar sequence shifted one tiny step into the future.

17
00:01:26,170 --> 00:01:27,900
And for most people it's actually pretty easy.

18
00:01:27,910 --> 00:01:32,560
You feel that there's a pattern here of increasing by one as you move along one time step.

19
00:01:32,830 --> 00:01:37,300
So if I asked you Hey what's the sequence that's similar to this except shift that over one times that

20
00:01:37,650 --> 00:01:42,400
you would hopefully build figure out well I'll just say two three four five six seven so we're going

21
00:01:42,400 --> 00:01:49,230
to try to later on training network to predict a time series shifted over one time step into the future.

22
00:01:49,310 --> 00:01:53,410
You can imagine that would be really beneficial if we're trying to predict sales for the next day

23
00:01:56,420 --> 00:01:59,270
so the way we can do this is by using a recurrent neural network.

24
00:01:59,270 --> 00:02:03,460
So let's review how a normal neuron works in a feed forward network.

25
00:02:03,500 --> 00:02:08,990
Remember that a normal neuron just takes in some input and it can be multiple inputs so it can aggregate

26
00:02:08,990 --> 00:02:09,580
them.

27
00:02:09,710 --> 00:02:14,190
And then once it aggregates those inputs it passes it through some sort of activation function.

28
00:02:14,300 --> 00:02:19,100
Here we're just using the symbol for a rectified linear unit that it can be things like a sigmoid function

29
00:02:19,400 --> 00:02:23,310
or a 10 H function etc. and then from that we have an output.

30
00:02:23,360 --> 00:02:27,870
So that's why a normal neuron works a recurrent neuron is a little different.

31
00:02:28,010 --> 00:02:32,030
What it's going to do is send the output back to itself.

32
00:02:32,030 --> 00:02:37,100
So the output goes back into input of the same neuron.

33
00:02:37,100 --> 00:02:40,260
So we can actually unroll this throughout time.

34
00:02:40,610 --> 00:02:44,400
So if we unroll this throughout time it ends up looking something like this.

35
00:02:44,620 --> 00:02:49,820
So we can see time here kind of represented on the x axis and we have this particular recurrent neuron

36
00:02:50,210 --> 00:02:56,870
where it's input at T minus one gives an output at T minus one and then that gets passed into the neuron

37
00:02:57,200 --> 00:03:03,590
in its state input at time t and then it has an output at time t and then we can take that output and

38
00:03:03,590 --> 00:03:09,260
pass it as input for the same neuron at time T plus 1 and then so on and so on.

39
00:03:09,290 --> 00:03:14,420
So this is called unrolling that recurrent neuron something that's important to note here is that the

40
00:03:14,420 --> 00:03:20,630
neuron is actually receiving both inputs from a previous timestamp as well as inputs from the current

41
00:03:20,630 --> 00:03:21,480
time step.

42
00:03:21,560 --> 00:03:26,360
So you can see each of these neurons has two sets of inputs they're these cells that are a function

43
00:03:26,360 --> 00:03:32,120
of input from previous time steps are also known as memory cells and recurrent neural networks are also

44
00:03:32,120 --> 00:03:36,710
flexible in their inputs and outputs for both sequences and single vector values.

45
00:03:36,710 --> 00:03:41,450
So we'll show a couple of examples of that in just a little bit but I want you to know it's also very

46
00:03:41,450 --> 00:03:44,060
easy to create a layer of recurrent neurons.

47
00:03:44,060 --> 00:03:49,370
Previously we just saw one recurrent neuron then we unrolled it through time but we could do the same

48
00:03:49,370 --> 00:03:51,490
thing for an entire layer.

49
00:03:51,590 --> 00:03:57,200
So if we want an entire layer of recurrent neurons then it would look something like this where we have

50
00:03:57,290 --> 00:04:01,040
input of X and then it goes through those recurrent neurons.

51
00:04:01,040 --> 00:04:02,300
Here we have three of them.

52
00:04:02,300 --> 00:04:08,090
Then we see the output at Y and then we take the output and then just pass it back in to all the neurons

53
00:04:08,120 --> 00:04:09,260
in that layer.

54
00:04:09,260 --> 00:04:14,930
And we could do the same idea and unroll this entire layer throughout time so that we get an input a

55
00:04:14,930 --> 00:04:22,910
T equals zero get an output it's equal zero and then pass that along with the output and input at time

56
00:04:22,970 --> 00:04:27,300
plus one and same idea except now we're doing it with an entire layer.

57
00:04:27,300 --> 00:04:33,350
Now since the output of these recurrent neurons at a time step t is technically a function of all the

58
00:04:33,350 --> 00:04:39,700
inputs from previous time steps you could then begin to think that has some form of memory because we're

59
00:04:39,710 --> 00:04:45,560
technically passing in historical information into that recurrent neuron or into that layer of recurrent

60
00:04:45,560 --> 00:04:46,480
neurons.

61
00:04:46,490 --> 00:04:53,030
So part of this neural network that preserves some sort of state across these time steps is called a

62
00:04:53,060 --> 00:04:58,340
memory cell and later on we're going to see much more complex examples of memory cells.

63
00:04:58,340 --> 00:05:04,790
But for now we have this basic recurrent neural neuron that just sends its output back into its input

64
00:05:06,610 --> 00:05:10,570
recurrent your own networks are also very flexible in their inputs and outputs.

65
00:05:10,570 --> 00:05:15,760
So let's walk through a few examples of different combinations of inputs and outputs we can use for

66
00:05:15,760 --> 00:05:23,340
recurrent neural networks so we can actually perform a sequence input to a sequence output and an example

67
00:05:23,340 --> 00:05:29,900
of that would be passing in a set of Time series information such as a year's worth of daily sales data

68
00:05:30,320 --> 00:05:36,200
and then wanting back a sequence of that same sales data shifted over a certain time period into the

69
00:05:36,200 --> 00:05:36,730
future.

70
00:05:36,750 --> 00:05:41,660
And that's basically the initial example we first thought of that one two three four five and then you

71
00:05:41,660 --> 00:05:49,940
asked back for two three four five six so a sequence however shifted over one step into the future another

72
00:05:50,000 --> 00:05:56,360
set of examples inputs and outputs would be to pass in a sequence input and request back a vector output

73
00:05:56,930 --> 00:06:03,980
so a common example of using recurring neural networks for this sort of input output is sentiment scores

74
00:06:04,130 --> 00:06:10,670
so you can feed in a sequence of words maybe a paragraph of a movie review and then request back a vector

75
00:06:10,760 --> 00:06:14,650
indicating whether it was a positive sentiment such as they really liked the movie.

76
00:06:14,690 --> 00:06:19,880
That's usually indicated by one versus they really dislike the movie which is indicated by usually negative

77
00:06:19,880 --> 00:06:20,980
1 or 0.

78
00:06:21,170 --> 00:06:23,060
So that would be an example of a sequence input.

79
00:06:23,120 --> 00:06:27,860
So that paragraph of words to just a single vector value and you essentially have a bunch of training

80
00:06:27,860 --> 00:06:33,050
data where you have a bunch of paragraphs and then some sort of sentiment score attached to them and

81
00:06:33,050 --> 00:06:38,740
you would train your recurrent your own network on that data to have a sequence input to a vector output.

82
00:06:38,910 --> 00:06:45,530
So on the opposite side you could also feed in a vector input so a single input just a first time step

83
00:06:45,860 --> 00:06:51,640
and basically passin zeros for the rest of your time steps and then let the output be a sequence.

84
00:06:51,680 --> 00:06:55,330
So that's a vector to sequence network.

85
00:06:55,370 --> 00:07:00,860
So an example of vectors a sequence could be just providing a single seed a word and then getting out

86
00:07:01,010 --> 00:07:06,560
an entire sequence of high probability sequence phrases that would actually come out.

87
00:07:06,590 --> 00:07:12,290
So you could have a word such as hello and then the sequence that gets generated something like How

88
00:07:12,290 --> 00:07:13,070
are you.

89
00:07:13,070 --> 00:07:19,520
Because after being trained on lots of introductory text maybe the neural network realizes that just

90
00:07:19,520 --> 00:07:20,210
based off the seed.

91
00:07:20,210 --> 00:07:20,960
Hello.

92
00:07:20,960 --> 00:07:24,080
One of the more common phrases is how are you.

93
00:07:24,080 --> 00:07:28,550
Now that we understand basic earnings we're gonna move on to understanding a particular cell structure

94
00:07:28,610 --> 00:07:33,620
known as Elysium or long short term memory in which is going to be necessary in order to actually generate

95
00:07:33,620 --> 00:07:40,100
text that makes sense because we want to have the network be aware of not just the most recent texts

96
00:07:40,100 --> 00:07:44,090
that scene but also the entire history of texts that it's seen.

97
00:07:44,180 --> 00:07:47,120
So we're gonna learn how that works in the very next lecture we'll see there.

