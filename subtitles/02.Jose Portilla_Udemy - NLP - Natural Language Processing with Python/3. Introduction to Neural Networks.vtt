WEBVTT
ï»¿1
00:00:05.720 --> 00:00:12.740
Welcome back everyone to an introduction to neural networks we've seen how a single perceptual behaves.

2
00:00:12.750 --> 00:00:16.650
Now let's expand on this concept to get the idea of a neural network.

3
00:00:16.650 --> 00:00:22.910
Let's see how to connect many perceptions together and then how to represent this mathematically.

4
00:00:22.930 --> 00:00:25.840
So what does a multiple perceptions network actually look like.

5
00:00:25.840 --> 00:00:27.540
Well essentially looks like this.

6
00:00:27.640 --> 00:00:32.890
Here we can see that we have various layers of single perceptions connected to each other through their

7
00:00:32.890 --> 00:00:38.810
inputs and outputs in this case we have an input layer all the way on the left which is purple.

8
00:00:38.810 --> 00:00:44.210
We have two hidden layers and hidden layers are the layers that are in-between the input layer and all

9
00:00:44.210 --> 00:00:49.370
the way on the right the output layer essentially hidden layers or layers that don't get to see the

10
00:00:49.370 --> 00:00:49.970
outside.

11
00:00:49.970 --> 00:00:56.060
That is all the way the inputs on the left or all the way the outputs on the right so input layers.

12
00:00:56.070 --> 00:00:57.660
Those are real values from the data.

13
00:00:57.670 --> 00:01:00.710
So they take in the actual data as their input.

14
00:01:00.790 --> 00:01:04.950
Then you have hidden layers and those are the layers between the input and output layers.

15
00:01:05.020 --> 00:01:10.990
And if you have three or more hidden layers that's considered a deep network and then an output layer

16
00:01:11.080 --> 00:01:17.820
where you have some sort of final estimate of whatever the output that you're trying to estimate is.

17
00:01:17.840 --> 00:01:22.520
So as you go forwards through more layers the level of abstraction increases.

18
00:01:22.790 --> 00:01:29.900
Let's not discuss the activation function and a little more detail previously our activation function

19
00:01:29.900 --> 00:01:33.580
was just a really simple function the output 0 or 1.

20
00:01:33.620 --> 00:01:38.600
Remember we were just taking the sum of the inputs in that simple perception model and then had an activation

21
00:01:38.600 --> 00:01:44.630
function that just had an output of either zero or one based off whether the number was positive or

22
00:01:44.630 --> 00:01:45.290
negative.

23
00:01:45.380 --> 00:01:47.120
And if we plot it out that looks like this.

24
00:01:47.120 --> 00:01:54.510
We can say the output is either 0 or 1 on the y axis and then the x axis we see are marking at zero.

25
00:01:54.620 --> 00:01:57.310
And if we have a negative value we output zero.

26
00:01:57.320 --> 00:02:04.730
And if we have a positive value we output one and we did this by having the x axis be w x plus B that

27
00:02:04.730 --> 00:02:07.970
is the weights times the inputs plus that bias.

28
00:02:07.970 --> 00:02:10.270
And it's really common to just set that equal to Z.

29
00:02:10.520 --> 00:02:15.150
So you can refer to Z instead of having to concentrate for it to w x plus B.

30
00:02:15.200 --> 00:02:22.390
So whenever I say C I'm essentially referring to those weights times those inputs plus that bias so

31
00:02:22.630 --> 00:02:27.190
unfortunately this is actually a pretty dramatic function since small changes aren't really reflected

32
00:02:27.550 --> 00:02:30.730
so you can have kind of really small changes in Z.

33
00:02:30.790 --> 00:02:36.420
For instance the change from zero point five and Z to zero point six doesn't really matter it's still

34
00:02:36.420 --> 00:02:40.960
going to output one as long as this positive or the change that are really dramatic from negative 1

35
00:02:40.960 --> 00:02:43.160
to negative 1000 doesn't really matter.

36
00:02:43.180 --> 00:02:48.340
It's gonna be zero so it'd be nice if we could have a more dynamic function.

37
00:02:48.350 --> 00:02:54.680
For example this red line here you can see here that our outputs now are no longer just 0 or 1.

38
00:02:54.680 --> 00:02:56.580
We have a little more detail to it.

39
00:02:56.600 --> 00:02:58.970
There's a little more nuance here.

40
00:02:58.970 --> 00:03:01.310
So lucky for us this is actually the sigmoid function.

41
00:03:01.310 --> 00:03:07.070
This red line function already exists and it's known as the sigmoid function and they said my function

42
00:03:07.100 --> 00:03:14.690
is just f of x is equal to 1 over 1 plus e to the negative x and we could replace X here with Z.

43
00:03:14.810 --> 00:03:17.440
So we have z is equal to w x plus B.

44
00:03:17.450 --> 00:03:23.210
So we just take the whole thing that Z and plug it into the sigmoid function so we'd have f of z is

45
00:03:23.210 --> 00:03:29.240
equal to one over one plus E to the negative z and that would be our sigmoid function for the activation

46
00:03:29.240 --> 00:03:35.310
function so changing the activation function used can actually be really beneficial depending on what

47
00:03:35.430 --> 00:03:40.530
the particular task we're trying to complete and we'll get into more detail on that later on when we

48
00:03:40.530 --> 00:03:42.850
actually implement our own neural networks.

49
00:03:44.080 --> 00:03:48.540
Let's discuss a few more activation functions that we're going to encounter throughout this course.

50
00:03:49.150 --> 00:03:54.370
So we'll really common function that's used as an activation function besides the sigmoid function is

51
00:03:54.370 --> 00:04:00.990
the hyperbolic tangent function or 10 H or Tench of Z where 0 members just w x plus B.

52
00:04:01.150 --> 00:04:06.910
And that looks really similar to the sigmoid function except note that the output can then be from negative

53
00:04:06.910 --> 00:04:10.360
1 to 1 unlike previously we had 0 to 1.

54
00:04:10.480 --> 00:04:16.030
So if we look at the sigmoid function again that went from 0 to 1 and that was equal to 1 over 1 plus

55
00:04:16.120 --> 00:04:22.900
eats a negative x or negative z whatever the input is and now for the Tench or hyperbolic tangent function

56
00:04:23.200 --> 00:04:26.800
it goes from negative 1 to 1 but still has its sort of same behavior shape.

57
00:04:26.800 --> 00:04:31.870
And if you're interested in how to express the hyperbolic tangent mathematically over here on the right

58
00:04:31.870 --> 00:04:34.110
hand side are the actual equations for it.

59
00:04:36.500 --> 00:04:43.610
Then there's also the rectified linear unit or R E L U and this is a really common activation function

60
00:04:43.910 --> 00:04:48.740
but it's actually relatively simple and the way it works is the following method.

61
00:04:48.740 --> 00:04:55.040
You basically passing your z value into this Max equation so you say Max of zero or Z.

62
00:04:55.490 --> 00:04:56.750
So what does that actually mean.

63
00:04:56.750 --> 00:05:01.450
Well the rectified linear unit that actual activation function works the following manner.

64
00:05:01.550 --> 00:05:06.830
It says Based off your z value and 0 just return the maximum value.

65
00:05:07.190 --> 00:05:13.430
And that actually means that if your value of Z is negative then the max between zero and any negative

66
00:05:13.430 --> 00:05:14.870
number is always going to be zero.

67
00:05:15.260 --> 00:05:17.800
So we can see here that is Z.

68
00:05:18.020 --> 00:05:24.530
If it's negative the function always returns zero and then between Max zero comma Z.

69
00:05:24.860 --> 00:05:28.670
If the Z is positive it's always going to return the value of Z.

70
00:05:28.700 --> 00:05:32.120
So essentially you of just that Y is equal to Z here.

71
00:05:32.120 --> 00:05:33.070
So on the right hand side.

72
00:05:33.080 --> 00:05:35.790
That's why you get that sort of straight linear line.

73
00:05:35.960 --> 00:05:38.020
So again relatively simple function.

74
00:05:38.030 --> 00:05:41.950
But this is kind of considered at certain points in time State of the art.

75
00:05:42.050 --> 00:05:46.760
So we're gonna definitely be seeing this activation function a lot throughout our lectures.

76
00:05:46.760 --> 00:05:52.640
R E L U or rectified linear unit actually tends to have the best performance in many situations.

77
00:05:52.640 --> 00:05:57.710
Now luckily for us the deep learning libraries were using specifically cares have all of these activation

78
00:05:57.710 --> 00:06:02.960
functions built in so it's gonna be really easy for us to swap them in and out or use certain activation

79
00:06:02.960 --> 00:06:04.770
functions in certain situations.

80
00:06:04.880 --> 00:06:09.560
Often we'll use something like a soft Max at the very end of a layer in order to actually get some sort

81
00:06:09.560 --> 00:06:13.200
of classification output either a zero or one.

82
00:06:13.340 --> 00:06:18.770
So we're going to do now is move on to understanding had actually implement and build our own neural

83
00:06:18.770 --> 00:06:23.070
network models with Caris now that we already understand the neural network theory.

84
00:06:23.240 --> 00:06:28.070
Once we understand neural network theory and Caris we'll move on to a more advanced type of neural network

85
00:06:28.430 --> 00:06:32.310
specifically designed for sequences called recurrent neural networks.

86
00:06:32.390 --> 00:06:34.270
The first let's move onto cares.

87
00:06:34.340 --> 00:06:35.320
I'll see you at the next lecture.

