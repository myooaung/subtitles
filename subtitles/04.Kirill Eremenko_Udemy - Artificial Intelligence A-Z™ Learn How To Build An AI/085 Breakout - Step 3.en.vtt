WEBVTT
1
00:00:00.480 --> 00:00:06.650
Hello and welcome to this tutorial now we're going to make this second function to initialize the weights.

2
00:00:06.840 --> 00:00:11.190
And this one will be used to get an optimal learning.

3
00:00:11.340 --> 00:00:13.160
Actually these weights.

4
00:00:13.230 --> 00:00:21.960
So the second function we're going to call it weights underscore in it and it will take as argument

5
00:00:22.200 --> 00:00:26.020
the object and which will represent the neural network.

6
00:00:26.220 --> 00:00:27.080
So that's all.

7
00:00:27.090 --> 00:00:32.300
And then colon and now let's get inside the function to define what we wanted to do.

8
00:00:32.580 --> 00:00:38.610
So basically what we wanted to do is initialize the weights of the new network in such a way that we

9
00:00:38.610 --> 00:00:39.920
get an optimal learning.

10
00:00:40.080 --> 00:00:43.590
So this will not seem particularly intuitive.

11
00:00:43.590 --> 00:00:46.740
This is based on research papers and experiments.

12
00:00:46.740 --> 00:00:52.440
We are going to initialize the weights in a specific way that we haven't seen before but believe me

13
00:00:52.440 --> 00:00:54.740
that will optimize the learning process.

14
00:00:54.930 --> 00:01:00.760
So we just implemented without getting into the details of why we initialize the weights this way.

15
00:01:00.960 --> 00:01:06.390
And so we're going to start by using a trick which will be used later to make the distinction between

16
00:01:06.660 --> 00:01:13.260
the convolution and the full connection because you know remember our AI will have eyes and therefore

17
00:01:13.260 --> 00:01:15.290
it will have some convolutional layers.

18
00:01:15.330 --> 00:01:20.550
And of course it will also have some fully connected layers and we will have a different initialization

19
00:01:20.550 --> 00:01:23.550
of the weights for these two types of connections.

20
00:01:23.820 --> 00:01:28.620
So we're going to use this trick to separate these two kinds of connections and then we'll use some

21
00:01:28.890 --> 00:01:34.160
conditions to get the different initialization for each of these connections.

22
00:01:34.170 --> 00:01:40.410
So this trick is to create a new variable that we're going to call last name and that will be called

23
00:01:40.410 --> 00:01:44.630
to an object so and represents the neural network.

24
00:01:44.670 --> 00:01:47.140
But it's an object we'll see that later.

25
00:01:47.430 --> 00:01:54.290
And we're going to get the special attribute from this object which will be Kolesnik with double underscore

26
00:01:54.390 --> 00:02:01.570
first class double underscore dot double underscore again name and almost there.

27
00:02:01.610 --> 00:02:03.100
Another double underscore.

28
00:02:03.360 --> 00:02:10.500
So that's a pretty ugly trick to look for the type of connection of our new network object but that

29
00:02:10.500 --> 00:02:12.100
will give us exactly what you want.

30
00:02:12.210 --> 00:02:15.300
You're going to see it's going to make sense when we start if conditions.

31
00:02:15.540 --> 00:02:19.710
And by the way speaking of conditions we can stop them right now.

32
00:02:19.710 --> 00:02:26.170
And so what we're going to do now is start the first condition which will get us the first case.

33
00:02:26.280 --> 00:02:34.770
That is if the connection is a convolution and so to write this condition right if class name that's

34
00:02:35.010 --> 00:02:35.510
fine.

35
00:02:35.510 --> 00:02:45.180
Here we use a method the find method find and inside when put in quotes for convolution.

36
00:02:45.300 --> 00:02:54.330
And so if class name does find can is we're going to do different than minus one that is actually if

37
00:02:54.540 --> 00:02:57.900
we have a convolution because Manasquan means no.

38
00:02:58.200 --> 00:03:03.360
Well in that case we will do a special initialization of the weights.

39
00:03:03.420 --> 00:03:07.830
So this condition here means if we have a convolution connection.

40
00:03:07.830 --> 00:03:13.910
So in that case what we do is run this specific initialization of the way we want to do.

41
00:03:14.040 --> 00:03:17.930
And so that's where all the not intuitive things will come.

42
00:03:18.060 --> 00:03:21.990
We will start by creating a variable that we're going to call it.

43
00:03:22.180 --> 00:03:29.260
And this court shape so weight underscores shape will be a list that will basically contains the shape

44
00:03:29.530 --> 00:03:31.310
of the weights in our new network.

45
00:03:31.580 --> 00:03:35.800
And so we to use the list function to create a list.

46
00:03:35.930 --> 00:03:42.880
And inside we're going to put in the neural network that weight which will be the weight of the neural

47
00:03:42.880 --> 00:03:43.500
network.

48
00:03:43.570 --> 00:03:50.890
But in the convolution connection and to get the shape of these weights we use another attribute which

49
00:03:50.890 --> 00:03:59.670
is that data and then size size will get us the shape of these weights in the convolution connection.

50
00:03:59.680 --> 00:04:06.580
So now weight shape contains in a list the shape of the weights and the convolution connections of our

51
00:04:06.880 --> 00:04:08.300
network and.

52
00:04:08.460 --> 00:04:14.510
All right so we have weight shape then to initialize the weights of this convolution connection.

53
00:04:14.560 --> 00:04:16.650
We're going to need two values.

54
00:04:16.650 --> 00:04:22.920
First is the product of the first dimension by the second nomination by this third dimension.

55
00:04:22.930 --> 00:04:27.960
So that's what we're going to get right now and then we'll also need to get the zeros time ancient times

56
00:04:27.960 --> 00:04:33.570
the second time and sometimes the third dimension and then we'll use these two values in the competition

57
00:04:33.660 --> 00:04:35.660
of how we initialize the weights.

58
00:04:35.670 --> 00:04:37.920
So let's get this through this first product.

59
00:04:37.920 --> 00:04:46.050
We call it fun in and that will be equal to the product and we going to use the prod function which

60
00:04:46.050 --> 00:04:55.270
is a function by non-Thai which has a shortcut and P So MP that PRUD and inside prod we input what we

61
00:04:55.260 --> 00:04:56.680
want to make the product.

62
00:04:57.180 --> 00:05:02.590
And so as we said that is the diamond and one two and three of our weight shape.

63
00:05:02.880 --> 00:05:10.430
And so to get this we can take a wave shape and get the indexes of these three line engines.

64
00:05:10.500 --> 00:05:15.800
And so we set it Simonton one up to 10 inch and three included.

65
00:05:16.020 --> 00:05:18.770
So a dungeon for excluded.

66
00:05:19.050 --> 00:05:24.230
And that's how we can get it for the upper bound here is not included.

67
00:05:24.440 --> 00:05:35.410
So that is what we want then Same for fun out as we said Fan out it's going to be the product of the

68
00:05:35.410 --> 00:05:39.650
damage in zero time dimension two times that I mentioned three.

69
00:05:39.730 --> 00:05:45.490
And so here we can get indexed from two included to four excluded.

70
00:05:45.730 --> 00:05:52.110
So that will get as the product of time and two and three and then we can multiply it by design and

71
00:05:52.180 --> 00:05:58.900
zero which we can access with whaleship zero of index zero.

72
00:05:59.230 --> 00:06:16.990
So to sum up this is the one time two times in three and just below we have zero times two times in

73
00:06:16.990 --> 00:06:20.320
three of our weight check list of weights.

74
00:06:20.320 --> 00:06:25.390
All right so now we're going to use these two values fan in and fan out to proceed to the initialization

75
00:06:25.870 --> 00:06:33.280
because we're going to compute a new value as we're going to call W bound and that's equal to the square

76
00:06:33.280 --> 00:06:39.530
root which we can get with a function and P from and by that as q r t.

77
00:06:39.520 --> 00:06:40.830
Second like before.

78
00:06:40.840 --> 00:06:46.710
So the square root of 6 divided by fanning out.

79
00:06:46.820 --> 00:06:57.340
So fan in let's fan out that we got this W down here represents in some way the size of the tens of

80
00:06:57.340 --> 00:06:58.140
weights.

81
00:06:58.240 --> 00:06:59.740
And why did we get this.

82
00:06:59.740 --> 00:07:06.130
It's because then what we were just about to do now is we want to generate some random weights that

83
00:07:06.130 --> 00:07:10.070
are inversely proportional to the size of the tensor of weights.

84
00:07:10.180 --> 00:07:18.580
Because indeed what we're about to do now is take our new network and then get its weight.

85
00:07:18.580 --> 00:07:25.260
So by still taking the attribute weight then access its data that is the tensor itself.

86
00:07:26.100 --> 00:07:33.330
And then from this tensor of weights we're going to generate some random weights that are inversely

87
00:07:33.330 --> 00:07:37.110
proportional to the size of the tensor weights.

88
00:07:37.180 --> 00:07:45.520
And so in this uniform function now we have to put a lower bound which will be minus W bound and the

89
00:07:45.520 --> 00:07:49.090
upper bound which will be plus W back.

90
00:07:49.750 --> 00:07:52.460
OK so that's for the weights.

91
00:07:52.510 --> 00:07:57.460
And now we need to initialize the bias and good news for the bias.

92
00:07:57.460 --> 00:07:59.130
It's going to be much more simple.

93
00:07:59.200 --> 00:08:07.650
We are going to initialize them all with zeros so to get these buys we take them from our model of course

94
00:08:08.070 --> 00:08:09.470
that is our new network.

95
00:08:09.930 --> 00:08:15.790
And then the attribute for the bias is bias then same with access to the data.

96
00:08:16.200 --> 00:08:23.430
And then we're going to use a method which is the Phil underscore method which as you might have guessed

97
00:08:23.580 --> 00:08:29.970
is used to fill the tensor of bias with zeros well with the rules we have to specify that we want to

98
00:08:29.970 --> 00:08:31.600
fill it with zeroes here.

99
00:08:31.700 --> 00:08:34.230
And so that's why I'm putting here Zero.

100
00:08:34.560 --> 00:08:40.210
All right so to summarize we generate some random weights inversely proportional to the size of the

101
00:08:40.210 --> 00:08:43.860
tensor weights and we initialized device with zeros.

102
00:08:43.860 --> 00:08:49.850
All right so that was for the Initialize action at the convolution connections.

103
00:08:49.880 --> 00:08:52.880
Now we need to do the same for the full connection.

104
00:08:53.300 --> 00:09:01.470
And so we're going to add new condition and if same we take this trick we use first class name that

105
00:09:01.470 --> 00:09:05.160
is this variable that contains the different names of the connections.

106
00:09:05.160 --> 00:09:14.440
So if class name that same we use the find method to which when put in quotes this time a full connection

107
00:09:14.580 --> 00:09:19.050
that is a classic linear connection in a classic artificial neural network.

108
00:09:19.390 --> 00:09:27.160
And so the name for that is linear and saying we're going to use this trick to say that we want it to

109
00:09:27.160 --> 00:09:35.810
be different than minus one so that's this end of class thing fine linen are different than minus one

110
00:09:35.810 --> 00:09:41.360
means if the connection is in the there that is if we have a classical connection.

111
00:09:41.360 --> 00:09:44.620
So in that case how are we going to initialize the weights.

112
00:09:44.840 --> 00:09:47.260
Well it's going to be quite the same.

113
00:09:47.270 --> 00:09:54.020
We're going to introduce whaleship Voivode which will not erase the first one because we will either

114
00:09:54.050 --> 00:10:01.430
in this case or that case so it will not be the same so we can totally reverse that then same we're

115
00:10:01.430 --> 00:10:08.600
going to introduce a fan in variable which this time will not be equal to the product of these three

116
00:10:08.600 --> 00:10:09.440
dimensions.

117
00:10:09.650 --> 00:10:17.090
But actually this time it will be equal to simply design mention one.

118
00:10:17.380 --> 00:10:23.870
And that's because for the full connection there is less connections than in a convolution connection.

119
00:10:23.880 --> 00:10:29.970
Now is this an intuition lectures in the end and in the CNN section there is less mention for a full

120
00:10:29.970 --> 00:10:32.640
connection than for a convolution.

121
00:10:32.640 --> 00:10:39.840
So basically we just take this time in one year then say we're going to have a fan out variable which

122
00:10:39.840 --> 00:10:48.780
will then use to compute W. bounds and this fan out domination is going to be weights shape of index

123
00:10:49.160 --> 00:10:49.890
0.

124
00:10:49.890 --> 00:10:51.470
That is the diamonds zero.

125
00:10:51.480 --> 00:10:55.290
All right then to GWB and it's going to be the same.

126
00:10:55.470 --> 00:11:01.780
It's going to be the square root of 6 divided by the sum offending and find out.

127
00:11:01.830 --> 00:11:04.660
So there we go.

128
00:11:04.880 --> 00:11:11.870
And then the good news is that it's exactly the same as previously we use the uniform function for the

129
00:11:11.870 --> 00:11:20.990
weights and the fill function for the bias to get the same kind of initialization this time with a different

130
00:11:20.990 --> 00:11:24.440
fan in and fan out and therefore different w that.

131
00:11:24.560 --> 00:11:27.520
So that's the same principle that's the same idea.

132
00:11:27.530 --> 00:11:32.660
The only thing that changes here is that we have less dominations for the full connection and therefore

133
00:11:32.780 --> 00:11:39.160
more simple consideration of this bound of the weights here to generate these random weights.

134
00:11:39.200 --> 00:11:45.210
But the good news is that now it's really not only these weights and it functions very.

135
00:11:45.380 --> 00:11:47.150
But now we have two tools.

136
00:11:47.330 --> 00:11:50.180
And so we are ready to start building the brain.

137
00:11:50.300 --> 00:11:51.280
So I can't wait.

138
00:11:51.290 --> 00:11:53.500
This will be of course the most exciting part.

139
00:11:53.510 --> 00:11:57.630
This was just to warm up and get us ready for the big thing.

140
00:11:57.650 --> 00:11:59.990
So we'll take care of that in the next tutorial.

141
00:12:00.080 --> 00:12:02.590
Well actually it's going to take several tutorials of course.

142
00:12:02.690 --> 00:12:04.350
We'll start by making the eyes.

143
00:12:04.520 --> 00:12:10.040
And then remember we will add an illustration to learn the temporal properties of the input and then

144
00:12:10.040 --> 00:12:12.170
we'll take care of the actor and the critic.

145
00:12:12.170 --> 00:12:17.000
And that's where we'll use this to function normalized comes initialiser and weights in it.

146
00:12:17.120 --> 00:12:18.590
So I can't wait to do that.

147
00:12:18.590 --> 00:12:20.630
We're going to make something very powerful now.

148
00:12:20.630 --> 00:12:22.510
So get ready for it.

149
00:12:22.790 --> 00:12:24.250
Until then enjoy AI.
