WEBVTT
1
00:00:00.600 --> 00:00:02.360
Hello and welcome to the story.

2
00:00:02.520 --> 00:00:07.920
Now we're going to make the for loop that to compute the policy loss and the value loss and once we

3
00:00:07.920 --> 00:00:13.260
have these two lessons we will be able to use our optimizer to place to get cigarette in the sand to

4
00:00:13.260 --> 00:00:14.820
reduce the losses.

5
00:00:14.820 --> 00:00:15.990
All right so there we go.

6
00:00:15.990 --> 00:00:22.740
We start here by the way in the previous tutorial we implemented this section and I forgot to remove

7
00:00:22.740 --> 00:00:24.280
the indents sorry about that.

8
00:00:24.420 --> 00:00:28.210
So starting from here is not in the fall.

9
00:00:28.530 --> 00:00:32.650
And now we're starting a new full loop so I'm starting here with four.

10
00:00:32.840 --> 00:00:38.340
And now what we're going to do is we're going to start from the last step that was done during the exploration

11
00:00:38.670 --> 00:00:40.420
and we're going to move backward in time.

12
00:00:40.470 --> 00:00:51.250
So that's why I'm doing for i in reversed range land rewards biggest rewards is the least.

13
00:00:51.260 --> 00:00:57.000
And since each step of the exploration is associated to where we work because at each step we get reward

14
00:00:57.230 --> 00:00:58.400
when then we words.

15
00:00:58.400 --> 00:01:05.090
Is this number of steps and this reverse here is used so that we can move back in time so that we go.

16
00:01:05.160 --> 00:01:10.890
And now what we're going to do is update the cumulative reward that is far and we're going to update

17
00:01:10.890 --> 00:01:11.570
it this way.

18
00:01:11.640 --> 00:01:14.110
That's actually the same as what we did for Doom.

19
00:01:14.170 --> 00:01:23.280
It's equal to gamma which we get from our parameters and taking from first programs that not times far

20
00:01:24.060 --> 00:01:31.290
plus the reward of this which we can get by taking the least reward and taking the index.

21
00:01:31.500 --> 00:01:36.680
So for us this will be the work of the last and then it will be the reward of the previous day and etc.

22
00:01:37.080 --> 00:01:43.320
and each time we update our By multiplying it by gamma and then adding this reward at the set.

23
00:01:43.650 --> 00:01:47.090
And so by doing this remember we will get in the end.

24
00:01:47.210 --> 00:01:53.070
So I'm going to write it as they come and we will get our community reward that will be cool at the

25
00:01:53.070 --> 00:01:55.910
end of the loop to our zero.

26
00:01:56.060 --> 00:02:02.100
The reward of step zero plus gamma times are one.

27
00:02:02.290 --> 00:02:09.040
We were the first that plus gamma squared times are to the word.

28
00:02:09.060 --> 00:02:21.960
The second step plus that plus gamma at the power of and minus one times the reward attained at step

29
00:02:22.800 --> 00:02:30.690
and minus 1 where any number of steps but then be careful at the end we'll have gamma at the power of

30
00:02:31.050 --> 00:02:32.250
number of steps.

31
00:02:33.530 --> 00:02:40.110
Times devalue the value of the function applied to the last state.

32
00:02:40.150 --> 00:02:42.450
That's what we should get at yet.

33
00:02:42.680 --> 00:02:49.130
And this we will get that because remember here we got this value and the last step because this was

34
00:02:49.130 --> 00:02:50.840
done at the end of this for loop here.

35
00:02:51.200 --> 00:02:56.140
And so we got the value and we set our to be equal to that value.

36
00:02:56.420 --> 00:03:02.710
So right now are at the beginning of the second full loop here will be equal to this value of the last

37
00:03:02.710 --> 00:03:03.310
date.

38
00:03:03.590 --> 00:03:10.300
But then by doing this this is what we'll get in the end are equal or zero percent or one tennis court

39
00:03:10.520 --> 00:03:16.620
or two plus can add the power and minus one times that we were at step and minus one plus game at the

40
00:03:16.620 --> 00:03:20.990
power of number of steps times this value of the Lastings.

41
00:03:21.020 --> 00:03:25.900
So that's the main thing to understand and this can be the action of the cumulative reward.

42
00:03:26.090 --> 00:03:33.560
And that's why it is important to start from it by initialising or with the here and doing this reversed

43
00:03:33.820 --> 00:03:37.390
loop to get this final equation perfect.

44
00:03:37.400 --> 00:03:43.430
And now now that we have the right value for the cumulative reward Well we will compute the advantage

45
00:03:43.850 --> 00:03:49.110
and the advantage here is just the advantage of getting this reward compared to them.

46
00:03:49.160 --> 00:03:56.600
So I'm going to introduce evolvable advantage and therefore it will be equal to this cumulative reward

47
00:03:57.020 --> 00:04:01.390
minus the value of the V function obtained at the stage.

48
00:04:01.610 --> 00:04:06.690
So therefore that is our minus values.

49
00:04:07.130 --> 00:04:07.740
Perfect.

50
00:04:07.760 --> 00:04:13.160
And now that we have the community we work and the advantage then we can get the value loss.

51
00:04:13.190 --> 00:04:15.170
This is the first that we can get now.

52
00:04:15.200 --> 00:04:20.940
So we're going to get our value does very well and this will be updated the following way.

53
00:04:20.960 --> 00:04:24.530
Remember so far that devalued us was initialized to zero.

54
00:04:24.800 --> 00:04:34.670
And so we're going to take the value loss again and at 0.5 times the square to the advantage so we can

55
00:04:34.670 --> 00:04:35.480
get it this way.

56
00:04:35.490 --> 00:04:39.410
Advantage thought out too.

57
00:04:39.470 --> 00:04:46.460
So that just means to square the advantage of the power to and that is exactly the value plus the loss

58
00:04:46.580 --> 00:04:52.830
generated by the predictions of the value of the function outputs by the creek.

59
00:04:53.120 --> 00:04:59.840
And so it makes sense that this is devalued just because remember the advantage of the action in the

60
00:04:59.840 --> 00:05:04.820
state s is the difference between the Q value and the value of the B function.

61
00:05:04.880 --> 00:05:14.660
And so when we play the optimal action Well we get the stationary state with Q optimal of the optimal

62
00:05:14.660 --> 00:05:20.080
action a star player in the state as equals the optimal value.

63
00:05:20.080 --> 00:05:22.000
Vistar of the state s.

64
00:05:22.180 --> 00:05:28.390
So it's quite intuitive to understand that when the advantage is not equal to zero then there will be

65
00:05:28.390 --> 00:05:29.990
a difference between these two.

66
00:05:30.220 --> 00:05:32.930
And therefore that's how the last is measured.

67
00:05:33.370 --> 00:05:34.120
OK.

68
00:05:34.120 --> 00:05:37.410
So very last computed one last down.

69
00:05:37.420 --> 00:05:39.000
We have now one more to go.

70
00:05:39.010 --> 00:05:44.560
It is the policy loss and that is what we're going to compute right now and to compute it we need to

71
00:05:44.560 --> 00:05:51.990
consider again the generalized advantage estimation because to compute the policy loss we need to generalized

72
00:05:52.000 --> 00:05:58.450
advantage estimation and to get the generalized advantages the nation we need first the temporal difference

73
00:05:58.540 --> 00:05:59.760
of the stage valves.

74
00:05:59.830 --> 00:06:06.220
So we have multiple things to compute here and we're going to start with this temporal difference once

75
00:06:06.220 --> 00:06:07.710
we get the temporal difference.

76
00:06:07.780 --> 00:06:12.850
We will get the generalized advantage estimation and once we get the generalized advantages to mention

77
00:06:13.180 --> 00:06:14.900
we will get the peninsulas.

78
00:06:14.920 --> 00:06:15.460
All right.

79
00:06:15.670 --> 00:06:28.100
So let's start with the temporal difference T.G. DD is equal to the reward of the step I plus Ghana

80
00:06:28.700 --> 00:06:39.200
which we get things to our programs list so Bromstad gamma times the value of this debt plus one and

81
00:06:39.200 --> 00:06:50.260
we add that data to access it minus the value of the step I and same we add the data.

82
00:06:50.280 --> 00:06:50.610
All right.

83
00:06:50.670 --> 00:06:53.900
That's the formula of the temporal difference and the state values.

84
00:06:54.180 --> 00:06:59.690
And now we can update the generalized advantage estimation and how is it dated.

85
00:06:59.880 --> 00:07:10.230
Well we take R-GA and we multiply it by gamma parameter gamma times so which we access with our parameters

86
00:07:10.230 --> 00:07:10.940
as well.

87
00:07:10.950 --> 00:07:18.410
So we take programs cell and we add this temporal difference of the state values.

88
00:07:18.510 --> 00:07:19.890
So be careful.

89
00:07:19.890 --> 00:07:21.280
We are in the loop.

90
00:07:21.450 --> 00:07:27.060
And each time we multiply the by and by and we add a temporal difference.

91
00:07:27.150 --> 00:07:33.720
So it's important to understand that at the end of this loop Well this generalized advantage estimation

92
00:07:34.170 --> 00:07:46.180
will be equal to the sum on all the steps of gamma times so that the power of i times the temporal difference

93
00:07:46.480 --> 00:07:50.360
at the step by are so important to keep that in mind.

94
00:07:50.700 --> 00:07:57.000
And now that we have the generalized advantage estimation and the general difference we can finally

95
00:07:57.000 --> 00:07:58.880
compute the policy.

96
00:07:59.070 --> 00:08:00.210
So let's do this.

97
00:08:00.210 --> 00:08:10.020
We are going to update our policy laws the following way by taking the old policy for us and we substract

98
00:08:10.230 --> 00:08:20.280
the LUGG probabilities obtained at the step that we multiply by this generalized advantage estimation

99
00:08:20.280 --> 00:08:23.920
that we have to put in a variable because then we'll compute the gradients.

100
00:08:24.150 --> 00:08:33.170
So it has to be attached to gradients in the graph and then we add minus 0.1 times the entropy.

101
00:08:33.170 --> 00:08:36.660
The entropy obtained at the step in the fall.

102
00:08:37.200 --> 00:08:38.130
And again.

103
00:08:38.220 --> 00:08:39.540
Now be careful.

104
00:08:39.540 --> 00:08:50.830
This is the inside the loop which means that at the end of the flu what you'll get is policy plus equals

105
00:08:51.280 --> 00:09:03.110
minus some over the steps of the product luggin of the policy at the step times to generalized advantage

106
00:09:03.110 --> 00:09:04.090
estimation.

107
00:09:04.160 --> 00:09:10.710
Plus this 0.01 times the entropy does the so that we get.

108
00:09:10.720 --> 00:09:13.240
And now what is the policy of the I.

109
00:09:13.280 --> 00:09:19.100
Well that's the soft Max probabilities of the actions and the entropy of this that I will you know what

110
00:09:19.100 --> 00:09:21.320
it is it's where we computed earlier.

111
00:09:21.430 --> 00:09:22.870
And what we intended to do list.

112
00:09:22.940 --> 00:09:24.130
So we already have that.

113
00:09:24.290 --> 00:09:29.950
But this year I hear it's the soft Max probability of the actions.

114
00:09:30.170 --> 00:09:32.080
And why do we put a minus here.

115
00:09:32.150 --> 00:09:37.220
That's because the luck of the probability and the entropy are negative values.

116
00:09:37.220 --> 00:09:43.340
And since we want to minimize their absolute value we must see this last as the LUGG likelihood as opposed

117
00:09:43.340 --> 00:09:44.370
to a distance.

118
00:09:44.370 --> 00:09:51.530
No we want to maximize the probability of the action that will maximize the advantage.

119
00:09:51.530 --> 00:09:53.130
That's the whole idea behind it.

120
00:09:53.210 --> 00:09:59.060
We want to maximize the probability of playing the action that will maximize the advantage and for those

121
00:09:59.060 --> 00:10:03.500
of you who might be wondering what is the purpose of this entropy efficient.

122
00:10:03.650 --> 00:10:05.720
There is this factor 0.01 here.

123
00:10:05.930 --> 00:10:13.430
Well the purpose of it is just to prevent it from falling too quickly into a trap where we have a distribution

124
00:10:13.430 --> 00:10:19.340
of probabilities with zeros for all the actions except one which has a probability of one.

125
00:10:19.550 --> 00:10:22.370
And if that happens that would minimize the entropy.

126
00:10:22.550 --> 00:10:29.060
So that's why we're adding this small revisions 0.01 year that will make the entropy increase in the

127
00:10:29.060 --> 00:10:30.940
Great in the sense.

128
00:10:31.020 --> 00:10:35.460
OK so now the good news is that the most difficult part is done.

129
00:10:35.500 --> 00:10:41.050
We have the two losses and therefore what we only need to do now and we already know how to do it is

130
00:10:41.050 --> 00:10:44.820
to perform just to get a grade in the sense to reduce these two classes.

131
00:10:45.190 --> 00:10:51.170
And so what we're going to do now is get out of this loop and we're going to take our optimizer.

132
00:10:51.390 --> 00:10:57.220
The one we made separately then remember the first thing we have to do is to initialize all the grading

133
00:10:57.220 --> 00:11:04.810
parameters to zero and to do this we add that then to zero and it's called a grad method.

134
00:11:04.810 --> 00:11:06.980
All right so that's done then.

135
00:11:07.000 --> 00:11:11.890
Now we're going to perform backward propagation but we're going to give twice as much importance to

136
00:11:11.890 --> 00:11:15.670
the policy last than the value lost because the policy is smaller.

137
00:11:15.970 --> 00:11:28.190
So to do this we're going to put in parenthesis policy and the score plus plus 0.5 value loss so 0.5

138
00:11:28.970 --> 00:11:37.400
times the value to us and we're going to add here that we applies the backward method to perform backward

139
00:11:37.400 --> 00:11:43.880
propagation and thanks to this trick here with the policy less plus half of the value that we have twice

140
00:11:43.880 --> 00:11:47.460
as much importance to the policy than the Vaness.

141
00:11:47.480 --> 00:11:53.630
OK then we're going to use another trick which is to prevent the gradient from taking extremely large

142
00:11:53.630 --> 00:11:56.890
values and therefore to generate the algorithm.

143
00:11:57.020 --> 00:12:03.530
And the trick to do that is to get first our torch library then the end and module from the torch library

144
00:12:04.220 --> 00:12:13.040
then the utils submodule and now we're going to use a function CLEP underscore grad's on the score norm

145
00:12:13.530 --> 00:12:20.830
and we are going to input our model parameters with a second input which will be 40.

146
00:12:21.200 --> 00:12:26.560
And that trick will basically make sure that the gradients won't take extremely large values and to

147
00:12:26.560 --> 00:12:28.000
generate the algorithm.

148
00:12:28.220 --> 00:12:30.770
And for those of you who might be wondering whether these 40 years.

149
00:12:30.800 --> 00:12:31.640
Exactly.

150
00:12:31.850 --> 00:12:37.130
Well that just means that we're using these values so that's the norm of the gradient stays between

151
00:12:37.130 --> 00:12:42.510
0 and 40 and therefore that's how we prevent the gradient from taking to large values.

152
00:12:43.000 --> 00:12:45.170
OK now we're almost done.

153
00:12:45.170 --> 00:12:52.550
Remember we made this and sure shared Gretz function at the beginning of the fall which is to ensure

154
00:12:52.670 --> 00:12:59.450
that the agent and the shared model share the same gradients and to do this to make sure that we can

155
00:12:59.450 --> 00:13:01.090
apply this function here.

156
00:13:01.220 --> 00:13:13.310
And so we are going to add and sure share grad's to make sure that the moral and the shared model share

157
00:13:13.310 --> 00:13:14.690
the same gradients.

158
00:13:14.690 --> 00:13:16.670
All right so that's just a precaution.

159
00:13:16.670 --> 00:13:21.620
I'm not sure that's totally necessary but you know at least we won't get an issue here.

160
00:13:22.040 --> 00:13:22.550
Okay.

161
00:13:22.550 --> 00:13:29.900
And finally last line of code we are of course going to perform the optimization step to reduce the

162
00:13:29.900 --> 00:13:38.900
losses and you know how to do it of course we take our optimizer and we add that step with parenthesis

163
00:13:39.320 --> 00:13:43.300
and then we go to training of our brains is over.

164
00:13:43.550 --> 00:13:44.780
So congratulations.

165
00:13:44.780 --> 00:13:47.030
I hope this wasn't too overwhelming.

166
00:13:47.060 --> 00:13:49.620
Don't worry I will provide the code with all the comments.

167
00:13:49.670 --> 00:13:53.330
So if you missed any detail you can have a look at the comments.

168
00:13:53.420 --> 00:13:57.260
And don't worry if you haven't understood anything this is very advanced.

169
00:13:57.410 --> 00:14:04.020
But rest assured this is also the most powerful remember visit made from the creator of pi.

170
00:14:04.100 --> 00:14:06.770
So we are really working with the best here.

171
00:14:06.850 --> 00:14:12.560
The state of the art so it's totally normal if you didn't get everything the first time but by working

172
00:14:12.560 --> 00:14:16.520
on it many times you will definitely get more and more comfortable.

173
00:14:16.520 --> 00:14:19.150
So now we're done with the training.

174
00:14:19.220 --> 00:14:26.080
So basically we made all the most important things you know we made the brains by building the architectures

175
00:14:26.120 --> 00:14:30.610
of the neural networks with the convolutions the LCN and the fully connected layers.

176
00:14:30.620 --> 00:14:34.360
We trained his brain by making this train code here.

177
00:14:34.520 --> 00:14:37.190
So basically the heart of the algorithm is done.

178
00:14:37.310 --> 00:14:39.800
You made the A3 see congratulations.

179
00:14:39.800 --> 00:14:43.940
Now we have a few more things to do but that is just to get the fun part.

180
00:14:43.940 --> 00:14:52.540
You know we need to make this test that we found which will test the agents and provide the videos and

181
00:14:52.550 --> 00:14:54.020
the airplane break out.

182
00:14:54.170 --> 00:15:00.860
So this will be very fun to watch we'll not code all the lines of this test that I fell because as we

183
00:15:00.860 --> 00:15:02.720
said we did the most important thing.

184
00:15:02.840 --> 00:15:10.310
All related 23C but I will of course explain the code and eventually we have this made up I found which

185
00:15:10.310 --> 00:15:11.770
will execute the code.

186
00:15:11.890 --> 00:15:15.900
And from the moment we execute this code all the code will be generated.

187
00:15:16.040 --> 00:15:18.150
So the brains will be made.

188
00:15:18.200 --> 00:15:24.080
The training will happen and the eye will play new games of breakout and we will get all the videos.

189
00:15:24.080 --> 00:15:26.480
So I can't wait to eventually watch them.

190
00:15:26.550 --> 00:15:29.810
We are going to see if he is smart enough to catch the ball.

191
00:15:29.990 --> 00:15:36.380
So now I'm going to see in the next tutorial for this desktop UI so that we can test the AI on some

192
00:15:36.380 --> 00:15:37.250
new games.

193
00:15:37.300 --> 00:15:38.980
And until then enjoy AI.
