WEBVTT
1
00:00:00.660 --> 00:00:03.540
Hello welcome back to the course on artificial intelligence.

2
00:00:03.540 --> 00:00:08.940
In today's tutorial we're going to cover up quite a complex tutorial called eligibility trace or and

3
00:00:08.940 --> 00:00:09.500
step.

4
00:00:09.500 --> 00:00:14.970
Q learning and this is something I going to implement in the practical side of things so that's why

5
00:00:14.970 --> 00:00:21.390
we need to come out of and at center it is quite a complex topic so we've got a very interesting approach

6
00:00:21.690 --> 00:00:24.880
to getting us up to speed with the intuition behind.

7
00:00:24.890 --> 00:00:29.760
So I have like a different approach in mind than we're used to solaced the simple look at that and see

8
00:00:29.760 --> 00:00:30.560
how that goes.

9
00:00:30.780 --> 00:00:34.190
So I want to give you an example to start off with.

10
00:00:34.260 --> 00:00:39.990
I'm going to give you an example in this tutorial and that will demonstrate the power of eligibility.

11
00:00:40.020 --> 00:00:42.470
And give us the intuition behind things.

12
00:00:42.540 --> 00:00:47.880
And then if you like to delve further into eligibility traits I'll give you the best place where you

13
00:00:47.880 --> 00:00:49.210
can read about it.

14
00:00:49.290 --> 00:00:52.560
I'll give you a reference to a book but otherwise.

15
00:00:52.560 --> 00:00:57.120
So while this is going to be different because we're into it first rather than delving into the intuition

16
00:00:57.420 --> 00:01:01.580
we're going to look at an example and the intuitional become obvious after we talk about.

17
00:01:01.580 --> 00:01:03.010
And that's that's my hope so.

18
00:01:03.130 --> 00:01:03.860
So let's have a look.

19
00:01:03.860 --> 00:01:06.000
Let's see let's see if we can do this.

20
00:01:06.000 --> 00:01:12.780
So here we've got two agents and they're navigating the same environment and we're going to see how

21
00:01:12.780 --> 00:01:13.740
these two agents work.

22
00:01:13.740 --> 00:01:16.260
First one is going to work with our eligibility trace.

23
00:01:16.260 --> 00:01:22.230
Second one is going to work with illegibly trace and hopefully we'll see why the second one is going

24
00:01:22.230 --> 00:01:24.450
to be so much more powerful than the first one.

25
00:01:24.630 --> 00:01:26.240
So let's have a look.

26
00:01:26.250 --> 00:01:28.040
We're going to look at this agent first.

27
00:01:28.320 --> 00:01:34.170
And the way he operates is the exact way that we've discussed deep circular thing so far.

28
00:01:34.530 --> 00:01:40.230
So the agent is going to take a step or is going to move take an action move into a new state.

29
00:01:40.260 --> 00:01:45.480
Good to get a certain reward is going to put that through or through its algorithm update the neural

30
00:01:45.480 --> 00:01:50.610
network that's running this agent or that's running in the mind of this agent.

31
00:01:50.610 --> 00:01:54.870
So that's basically how it's learning from that moment is going to take a new step.

32
00:01:54.870 --> 00:01:59.550
So from this new state is going to take a new action based on what its neural network is telling it

33
00:01:59.550 --> 00:02:03.930
to do is going to get rewards going up update and so on and is going to keep doing that.

34
00:02:03.930 --> 00:02:09.370
So obviously this is going to do quite a good job and as we've seen previously from the previous practical

35
00:02:09.400 --> 00:02:15.450
Squire to DROs we're going to get some quite good results here but now we're going to add a new feature.

36
00:02:15.480 --> 00:02:21.380
Now this agent number two this guy over here he's going to navigate the same environment.

37
00:02:21.570 --> 00:02:23.770
What's he's going to use legibility of trees.

38
00:02:23.940 --> 00:02:25.170
And this is what it means.

39
00:02:25.170 --> 00:02:30.280
What he's going to do is he's going to take any steps he's going to take in this case five forceps is

40
00:02:30.300 --> 00:02:38.730
going to take four steps and then only after taking these steps will he gets calculate the total reward

41
00:02:38.730 --> 00:02:42.730
that he got from those steps and he will put it through his network.

42
00:02:42.730 --> 00:02:48.420
He will put it through his neural network that's governing the decision making process and then the

43
00:02:48.420 --> 00:02:50.690
neural network will learn from that.

44
00:02:50.700 --> 00:02:51.600
So which one.

45
00:02:51.630 --> 00:02:54.050
Right away like Which one do you think is more powerful.

46
00:02:54.150 --> 00:02:59.070
The guy that is just taking it one step at a time and kind of like poking in the blind or in the dark

47
00:02:59.070 --> 00:03:01.550
and he's like OK so I'm going to take a step see what happens.

48
00:03:01.620 --> 00:03:02.830
Take a step see what happens.

49
00:03:02.850 --> 00:03:03.480
Take steps.

50
00:03:03.480 --> 00:03:04.020
What happens.

51
00:03:04.020 --> 00:03:10.680
The guy at the top or the guy that takes just very courageously Marsha's through four steps in a row

52
00:03:11.130 --> 00:03:17.610
and then he decides whether those were good steps or not altogether and why you can see here or why

53
00:03:17.610 --> 00:03:22.470
you're probably getting a sense for why the second guy is better or is more powerful is because the

54
00:03:22.470 --> 00:03:25.160
second guy actually knows what's at the end.

55
00:03:25.170 --> 00:03:30.030
The first guy when he's when he's assessing whether this step is good or not he's only looking at the

56
00:03:30.030 --> 00:03:31.170
reward that he's getting.

57
00:03:31.280 --> 00:03:34.430
And so he's only guided by the reward the environment is giving him.

58
00:03:34.440 --> 00:03:39.570
Same thing here he's only guided by the reward that this environment is giving him here.

59
00:03:39.620 --> 00:03:46.490
So every time that's his only kind of compass that he has the reward the reward the reward.

60
00:03:46.560 --> 00:03:51.800
Whereas here the he actually can assess after taking the steps he can assess.

61
00:03:51.820 --> 00:03:53.960
OK so I did get to the finish line.

62
00:03:54.000 --> 00:03:56.640
So this combination of steps was good.

63
00:03:56.700 --> 00:03:57.680
All of them were good.

64
00:03:57.840 --> 00:04:01.410
Or Oh no I ended up in the firepit or Ohno I.

65
00:04:01.500 --> 00:04:08.100
I did and when the the my car didn't get to the finish line or I crossed the sand wall or I lost the

66
00:04:08.100 --> 00:04:09.340
game of doom or something.

67
00:04:09.450 --> 00:04:13.330
And then he decides from that this whole combination of steps is bad.

68
00:04:13.650 --> 00:04:18.180
And therefore for these steps that are earlier on he has more information.

69
00:04:18.180 --> 00:04:23.490
He has more insights like in a very intuitive approaches.

70
00:04:23.490 --> 00:04:26.000
Again this is a much more complex topic than we're portraying here.

71
00:04:26.010 --> 00:04:32.370
But in an intuitive way for example if you take this step this step only has information to you to obtain

72
00:04:32.370 --> 00:04:34.990
it you only have information coming back from this reward here.

73
00:04:35.070 --> 00:04:38.580
And for this step in this case the same exact step.

74
00:04:38.640 --> 00:04:41.670
It has more information has information coming all the way from.

75
00:04:41.820 --> 00:04:45.500
OK so what was the outcome after four steps or five steps or whatever.

76
00:04:45.520 --> 00:04:51.930
Yes so that is that is how it works and why it's called eligibility trace is because during this process

77
00:04:51.960 --> 00:04:58.170
not only does he look at the computer reward of this of what's going on and then the cumulative loss

78
00:04:58.200 --> 00:05:00.460
and then all that is appropriate.

79
00:05:00.620 --> 00:05:05.210
But actually there is a trace of eligibility as what is called the disability trust.

80
00:05:05.210 --> 00:05:15.440
There is a trace that is kept in an algorithm which says OK so if we do get a let's say we get a punishment

81
00:05:15.470 --> 00:05:23.060
we get a negative reward then which of these steps is most likely to be eligible for that punishment.

82
00:05:23.090 --> 00:05:29.690
So not only do we know what overall this whole pattern or the school combination of steps but we also

83
00:05:29.690 --> 00:05:36.350
keep an a trace of eligibility which steps are we going to update if we get everyone.

84
00:05:36.350 --> 00:05:40.970
So for instance if as a negative reward we might have an eligibility trace that indicates to us that

85
00:05:41.030 --> 00:05:47.360
this is a step that is most responsible for what we got in the end or if it's a positive reward again

86
00:05:47.390 --> 00:05:54.800
we might know the algorithm helps us keep track this eligibility algorithm also helps us keep track

87
00:05:54.830 --> 00:06:03.170
of what's what step or what action needs to be is eligible eligible to be updated based on that reward

88
00:06:03.170 --> 00:06:03.820
that we get.

89
00:06:03.860 --> 00:06:05.820
And that's why it's called eligibility trace.

90
00:06:06.160 --> 00:06:11.810
And so that's the basic intuition behind eligibility and hopefully these two examples of these agents

91
00:06:11.810 --> 00:06:18.260
make it quite obvious or are quite intuitive in while these abilities can be so powerful.

92
00:06:18.440 --> 00:06:25.760
And if as promised if you'd like to delve further into topical eligibility traces or step learning then

93
00:06:26.330 --> 00:06:31.220
a wonderful amazing book which is which you can find is called reinforcement learning.

94
00:06:31.220 --> 00:06:36.590
An introduction is by Richard Sutton Andrew Barto 1998.

95
00:06:36.740 --> 00:06:40.770
I think they're in the process of creating a second edition or the very critical issue.

96
00:06:40.790 --> 00:06:49.210
But this is the most common or the most popular or the most referenced book on enforcement learning

97
00:06:49.260 --> 00:06:53.050
it's got a ridiculous number of citations.

98
00:06:53.300 --> 00:06:56.630
I think like tens of thousands if I'm not mistaken.

99
00:06:56.810 --> 00:07:01.120
And also the chapter you need for this is Chapter 7.

100
00:07:01.130 --> 00:07:06.900
So in order to look at eligibility choices there's a whole chapter about Chapter 7.

101
00:07:06.920 --> 00:07:10.100
You can read about it and it goes into lots of detail.

102
00:07:10.220 --> 00:07:17.660
Forward Backward eligibility traces and also how integral temporal difference on one hand and the other

103
00:07:17.660 --> 00:07:23.320
end of the spectrum you have Monte-Carlo methods in between you have eligibility traces allegedly traces

104
00:07:23.330 --> 00:07:27.280
or you link to go from temporal differences to Monte-Carlo methods.

105
00:07:27.290 --> 00:07:34.190
Very interesting read lots of pictures which I really really appreciated very intuitive explanations.

106
00:07:34.250 --> 00:07:40.550
So there's lots of things that you can learn from this book about artificial intelligence and reinforcement

107
00:07:40.550 --> 00:07:48.230
learning but specifically eligibility traces are like a very good place to go to is this book for eligibility

108
00:07:48.230 --> 00:07:49.190
traces.

109
00:07:49.350 --> 00:07:57.070
And the second reference for today is something that is going to show you in the practical trials the

110
00:07:57.440 --> 00:08:04.460
deep learning or the Google deep mind research paper on a synchronous methods for a deeper reinforcement

111
00:08:04.550 --> 00:08:05.120
learning.

112
00:08:05.270 --> 00:08:11.270
Yes that's the paper that's the one paper that the A-3 see paper that we're going to be discussing further

113
00:08:11.270 --> 00:08:12.240
down in the score.

114
00:08:12.240 --> 00:08:14.410
We're getting closer and closer to it.

115
00:08:14.510 --> 00:08:21.200
And as you can tell we're pretty excited about this so this is going to be looking a little bit about

116
00:08:21.500 --> 00:08:28.400
how they implemented eligibility traces in this paper so we're going to be using this more for the practical

117
00:08:28.400 --> 00:08:29.420
side of things.

118
00:08:29.420 --> 00:08:33.650
So hopefully you enjoyed today's tutorial and know you're a bit more comfortable with eligibility traces

119
00:08:34.010 --> 00:08:35.920
and I can't wait to see you next time.

120
00:08:35.930 --> 00:08:37.680
Until then enjoy a.
