WEBVTT
1
00:00:00.560 --> 00:00:02.560
Hello and welcome to this tutorial.

2
00:00:02.750 --> 00:00:04.610
Now we have the brain of tomorrow.

3
00:00:04.610 --> 00:00:06.350
We also have the optimizer.

4
00:00:06.350 --> 00:00:09.980
So basically we are ready to train our different agents.

5
00:00:10.010 --> 00:00:11.620
That is our different brains.

6
00:00:11.720 --> 00:00:18.650
So that's from now that will make this big train function which will contain all the A3 algorithm and

7
00:00:18.650 --> 00:00:24.920
therefore what we're about to implement in this train that I found is just this huge train function

8
00:00:25.230 --> 00:00:31.200
that will just be this big train function nothing else no class but who will use this train function.

9
00:00:31.250 --> 00:00:33.880
And the last step of this module with the main code.

10
00:00:34.100 --> 00:00:35.010
So there we go.

11
00:00:35.060 --> 00:00:37.340
But before we start you can notice that.

12
00:00:37.490 --> 00:00:42.020
Well first we import some libraries so that's the classic libraries with the torch module.

13
00:00:42.020 --> 00:00:49.040
I mean your torch library then ends library to create the environment which will break out.

14
00:00:49.220 --> 00:00:54.470
Then we will of course import are actually class from our model.

15
00:00:54.620 --> 00:00:55.980
File this one.

16
00:00:56.420 --> 00:01:00.070
And finally we will use a variable from TORCIDA.

17
00:01:00.150 --> 00:01:04.000
I regret to run highly performing competitions at the gradient.

18
00:01:04.100 --> 00:01:05.750
Thanks to the dynamic graphs.

19
00:01:05.990 --> 00:01:11.960
And then we have this ensure shared grad's function which I didn't want to spend too much time on this

20
00:01:11.960 --> 00:01:16.930
because well first this is just a function that will make sure everything works correctly.

21
00:01:17.030 --> 00:01:20.400
If the model used by the agent doesn't have any share gradient.

22
00:01:20.480 --> 00:01:25.640
That's why it's called short shared grad's and the other reason is that I don't think this function

23
00:01:25.640 --> 00:01:26.530
is necessary.

24
00:01:26.630 --> 00:01:32.700
But we never know and at least with this will be 100 percent sure that the code will execute properly

25
00:01:33.080 --> 00:01:34.790
but that's not really important.

26
00:01:34.880 --> 00:01:40.640
What we must focus on is this trend function that we all start making right now.

27
00:01:41.000 --> 00:01:41.600
So here we go.

28
00:01:41.610 --> 00:01:48.860
Def and train will soon become a train and this transformation will take several arguments.

29
00:01:49.010 --> 00:01:50.400
The first one is rank.

30
00:01:50.420 --> 00:01:56.460
I'm going to explain what it is and the second the second one is harams so that all the parameters are

31
00:01:56.460 --> 00:01:57.840
the environment.

32
00:01:57.920 --> 00:02:02.860
Then the third parameter is going to be shared moral.

33
00:02:03.170 --> 00:02:09.260
So you know the shared model is what the agent will get to run its little exploration on a certain number

34
00:02:09.260 --> 00:02:17.050
of steps and then finally the last argument is going to be the optimizer that is the one we made earlier.

35
00:02:17.770 --> 00:02:20.040
So perfect that's up for arguments.

36
00:02:20.090 --> 00:02:24.010
And now we are ready to start implementing the same function.

37
00:02:24.170 --> 00:02:30.890
So the first thing we'll do is you know you remember what A-380 stands for it stands for synchronous

38
00:02:31.010 --> 00:02:32.480
active Crilley agents.

39
00:02:32.540 --> 00:02:34.980
So in 8:3 see there is a synchronous.

40
00:02:34.980 --> 00:02:40.940
So as you understood we have to disenfranchise each training agent and to diseconomies then we're going

41
00:02:40.940 --> 00:02:49.010
to use the rank to shift each side with this rank so this rank parameter here is just to shift the seed

42
00:02:49.220 --> 00:02:52.230
so that each training agent is synchronized.

43
00:02:52.580 --> 00:02:59.390
So for example if there is any training agents then the ranks will go from 1 to 10 and there will be

44
00:02:59.390 --> 00:03:02.600
one integer per agent from 1 to 10.

45
00:03:02.630 --> 00:03:08.840
So when we shift the seed by one thread all the pseudo random numbers created by this thread will be

46
00:03:08.840 --> 00:03:11.340
totally independent from the other threads.

47
00:03:11.480 --> 00:03:14.250
However the seed or fixed numbers.

48
00:03:14.510 --> 00:03:19.640
So when we reproduce the experience we will find exactly the same events.

49
00:03:19.880 --> 00:03:23.450
And that's because it's deterministic with respect to the seat.

50
00:03:23.690 --> 00:03:30.470
So it's important to understand that and that's why we need to do is to synchronize each trainee agents

51
00:03:30.800 --> 00:03:34.260
by using the right here to shift the seed with the rank.

52
00:03:34.350 --> 00:03:36.110
So let's do this to do that.

53
00:03:36.140 --> 00:03:39.170
We're going to take our torche library.

54
00:03:39.170 --> 00:03:45.350
Then we're going to get the seed with manual underscores seed parenthesis.

55
00:03:45.350 --> 00:03:51.040
This is a function and now we're going to take the seeds of all the agents which we can access with

56
00:03:51.220 --> 00:03:55.620
from that seed and to shift them by the rank to synchronize.

57
00:03:55.660 --> 00:04:05.420
Each of these agents will just add here plus rec and that will shift the seed with the rank to disenfranchise

58
00:04:05.510 --> 00:04:09.970
each trainee agents because there is one seed for each training agency.

59
00:04:09.980 --> 00:04:15.390
All right first thing done and now next step the next step is to get the environment.

60
00:04:15.530 --> 00:04:21.470
So we're going to create a new variable that we're going to call and and now will use to create Atari

61
00:04:21.590 --> 00:04:26.120
and function from the end module to create the environment for breakout.

62
00:04:26.150 --> 00:04:28.130
That is to get the environment of break out.

63
00:04:28.250 --> 00:04:38.070
So we take this function create Terry and and now we have to input just one argument which are the parameters

64
00:04:38.070 --> 00:04:39.060
of the environment.

65
00:04:39.270 --> 00:04:42.730
And we have them because this is one of the inputs of the brain function.

66
00:04:42.750 --> 00:04:48.270
This is this parameter here which will be the parameters of the environment of breakout and therefore

67
00:04:48.330 --> 00:04:58.320
to get the breakout environments we take these programs argument then that and then we get an name which

68
00:04:58.320 --> 00:05:03.240
in the future that is in the next code with the main function that will execute the whole code will

69
00:05:03.240 --> 00:05:06.010
be breakout brachial Vizier.

70
00:05:06.010 --> 00:05:09.450
All right so that gets us the environment perfect.

71
00:05:09.610 --> 00:05:16.080
And now next step is to align the seat of the environment on the one of the agents.

72
00:05:16.180 --> 00:05:17.650
And why do we do that.

73
00:05:17.650 --> 00:05:24.220
It's because remember each agent of the A-3 Silmaril has its own vision of the environment like its

74
00:05:24.220 --> 00:05:31.000
own copy of the environment and therefore we need to line each of the agents on one specific version

75
00:05:31.000 --> 00:05:36.520
of the environment and to do that we're going to use the seat because each seat determines a specific

76
00:05:36.520 --> 00:05:37.320
environment.

77
00:05:37.450 --> 00:05:43.390
So by associating a different seed to each agent well we'll get exactly what we want that is that each

78
00:05:43.390 --> 00:05:46.340
agent will have its own environment.

79
00:05:46.500 --> 00:05:54.730
And so how can we do that we can take our environment then at that then use the seed function to you

80
00:05:54.770 --> 00:05:57.340
know choose the ones he gets for the environment.

81
00:05:57.520 --> 00:06:01.370
And so now to align the seat of the environment to the seed of the agent.

82
00:06:01.600 --> 00:06:08.460
Well we simply need to get this because this corresponds to the seat of the agent that was shifted things

83
00:06:08.530 --> 00:06:14.170
to rank to get decent organized training agents because they're all on a different set.

84
00:06:14.200 --> 00:06:20.020
So we just need to pay that here and this will align the seat of the environment on the one of the agent

85
00:06:21.210 --> 00:06:24.660
Okay now we are going to get our model.

86
00:06:24.670 --> 00:06:27.060
That is our A-3 see brains.

87
00:06:27.180 --> 00:06:32.320
And so that is now that we're going to use the active class from our model file.

88
00:06:32.350 --> 00:06:38.500
So we're basically going to create a new object of this activity class and we're going to call this

89
00:06:38.500 --> 00:06:40.960
object model or brain if you like.

90
00:06:41.200 --> 00:06:47.650
But basically this object will contain all the convolutions the CM The linear connection and the Ford

91
00:06:47.650 --> 00:06:49.500
function to propagate the signal.

92
00:06:49.600 --> 00:06:55.630
So it will basically contain the brains of the actor in the critic with the ability to propagate the

93
00:06:55.630 --> 00:06:59.120
signal throughout the brain to get the final output.

94
00:06:59.170 --> 00:07:06.850
So let's do this let's create our model so as we said we want to call this object model.

95
00:07:07.150 --> 00:07:15.040
And so we create an object of the Act create class and therefore we take a class actor critic and now

96
00:07:15.040 --> 00:07:17.250
remember what arguments when the two inputs.

97
00:07:17.350 --> 00:07:20.420
That's actually the arguments of the function.

98
00:07:20.640 --> 00:07:26.230
So self we have to input it you know that's what we have to do to use the object in the method.

99
00:07:26.530 --> 00:07:33.010
But then the arguments we have to put are nomine puts which is in bad shape that is done in chains of

100
00:07:33.010 --> 00:07:38.680
art in print images and the actual space that contains you know the set of actions.

101
00:07:38.680 --> 00:07:42.550
So let's put these arguments in the train function.

102
00:07:42.760 --> 00:07:51.580
So the first one we can get it with our environment and that and then we use observation space that's

103
00:07:51.580 --> 00:07:59.020
the space of observations then that and then you get the number of inputs we get shade bracket's zero.

104
00:07:59.190 --> 00:07:59.550
All right.

105
00:07:59.550 --> 00:08:01.120
That's for inputs.

106
00:08:01.290 --> 00:08:04.690
And now for action space.

107
00:08:04.860 --> 00:08:10.480
Well that's almost the same as we need to get it from our environment that we are important than that.

108
00:08:10.500 --> 00:08:12.920
And then action space.

109
00:08:12.920 --> 00:08:13.260
All right.

110
00:08:13.260 --> 00:08:17.860
And that gives us the arguments we need to input when creating an object.

111
00:08:17.860 --> 00:08:20.130
The model of the execrate class.

112
00:08:20.400 --> 00:08:25.150
OK so now we have our model and now the next step is to prepare our input states.

113
00:08:25.170 --> 00:08:31.230
So remember we're still doing deeper informal learning so the input states our input images and therefore

114
00:08:31.560 --> 00:08:37.080
this will be originally done by Ray which will contain one channel because we will work with black and

115
00:08:37.080 --> 00:08:40.670
white images and it will have time in oceans of 42 by 42.

116
00:08:40.980 --> 00:08:46.680
But it's important to understand and to keep in mind here that the input states are input images.

117
00:08:46.680 --> 00:08:51.940
So what we have to do is to get the non-power then we will convert it into a torture answer.

118
00:08:52.050 --> 00:08:57.770
But the first step as what we did previously is to get an umpire and to get it.

119
00:08:57.840 --> 00:08:58.970
It's actually quite simple.

120
00:08:58.980 --> 00:09:06.080
Well first we need to create a variable for the input state which will cross state and this to get an

121
00:09:06.080 --> 00:09:07.130
umpire array.

122
00:09:07.230 --> 00:09:13.000
We simply need to take our environment and then adapt and then use the reset function.

123
00:09:13.200 --> 00:09:19.940
And this will initialize States as an empire array of dimensions one by 42 by 42.

124
00:09:20.190 --> 00:09:27.170
One means 1 channel so black and white image and 42 by 42 is of course the dominations of the image.

125
00:09:27.210 --> 00:09:30.860
The number of pixels and the width and the number of pixels and the height.

126
00:09:30.870 --> 00:09:32.630
So basically that's just the time instance.

127
00:09:32.670 --> 00:09:34.660
And that's the ones we work with.

128
00:09:34.820 --> 00:09:40.830
And now now that we have this actually in umpiring because this will get us these images of such time

129
00:09:40.830 --> 00:09:42.670
insurance in Empire.

130
00:09:42.870 --> 00:09:48.510
Now we can convert them into torch dancers and to do this well we're going to a data state again because

131
00:09:48.510 --> 00:09:50.890
we don't need to keep the number arrays.

132
00:09:51.180 --> 00:09:55.030
And that's where we use torch the torch module.

133
00:09:55.260 --> 00:10:02.790
And remember we already did that to do with the function from underscore non-Thai parenthesis.

134
00:10:02.880 --> 00:10:08.350
And inside this function we need to input the number rate which we want to convert into a torch sensor.

135
00:10:08.610 --> 00:10:14.880
And that is the state the previous version of the state non-pay array will become pipelined from the

136
00:10:14.880 --> 00:10:20.510
pipe function a torch sensor so that just creates intenser from the state.

137
00:10:20.550 --> 00:10:24.870
And now we just need to initialize the done viable.

138
00:10:24.870 --> 00:10:30.650
Remember the variable is generally the variable that says if an episode is over or if the game is over.

139
00:10:30.870 --> 00:10:37.110
Well here we just want to introduce this done very well and initialiser to true to specify that this

140
00:10:37.160 --> 00:10:41.230
Don't Voivode will be equal to true when the game is done.

141
00:10:41.260 --> 00:10:46.790
That will be useful for later so that the AI doesn't play indefinitely to break out.

142
00:10:46.820 --> 00:10:47.350
All right.

143
00:10:47.390 --> 00:10:54.320
So that was basically the beginning of this trend function with some initialization and some things

144
00:10:54.320 --> 00:10:55.370
that we have to do.

145
00:10:55.370 --> 00:11:00.560
The most important part here was that we have to disenfranchise each trainee agents.

146
00:11:00.560 --> 00:11:04.890
That's one first principle of the A3 similar we have to apply.

147
00:11:05.160 --> 00:11:09.780
And now in the next tutorial we will proceed to the synchronization with the shared model.

148
00:11:09.830 --> 00:11:14.810
Let's not forget that there is a different model but also the share model which is a model that all

149
00:11:14.810 --> 00:11:16.180
the agents share.

150
00:11:16.190 --> 00:11:22.430
And so we have to synchronize with the show model so that each agent can get this shared model to proceed

151
00:11:22.520 --> 00:11:25.990
to a small exploration of a certain number of steps.

152
00:11:26.000 --> 00:11:28.080
So that's what we'll do in the next Statoil.

153
00:11:28.130 --> 00:11:29.710
And until then enjoy AI.
