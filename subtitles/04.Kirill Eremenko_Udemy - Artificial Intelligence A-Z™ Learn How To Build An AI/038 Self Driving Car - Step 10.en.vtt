WEBVTT
1
00:00:00.480 --> 00:00:03.160
Hello and welcome to this Python tutorial.

2
00:00:03.180 --> 00:00:03.520
All right.

3
00:00:03.520 --> 00:00:08.810
In this oil we're going to make the function that will select the right action and each time.

4
00:00:08.810 --> 00:00:13.650
So basically we're going to implement the part that will make the car the right move.

5
00:00:13.740 --> 00:00:18.780
And each time that it's going left going straight or going right to reach the goal and to avoid the

6
00:00:18.780 --> 00:00:20.640
obstacles that is descent.

7
00:00:21.000 --> 00:00:22.350
So let's do this right now.

8
00:00:22.370 --> 00:00:28.320
We are going to start as usual with the deaf to define a function and then we give a name to our function

9
00:00:28.320 --> 00:00:36.370
which we're going to call select action than some parenthesis and this select action function will take

10
00:00:36.550 --> 00:00:37.410
two arguments.

11
00:00:37.450 --> 00:00:43.960
The first one is self as you grow to refer to the object and the second argument which according to

12
00:00:43.960 --> 00:00:46.150
you is going to be which one.

13
00:00:46.390 --> 00:00:47.860
Well what could it be.

14
00:00:47.860 --> 00:00:54.220
If you think about it the action we select comes from the output of the neural network because the output

15
00:00:54.220 --> 00:00:59.890
of the neural network or the q values for each of the three possible actions and therefore the action

16
00:00:59.890 --> 00:01:05.980
that we play the action that will be the output of the neural network depends on the input state and

17
00:01:05.980 --> 00:01:11.610
the input states is exactly the second argument we need with select action function.

18
00:01:11.680 --> 00:01:16.310
It's because we're literally going to take the output of the neural network.

19
00:01:16.510 --> 00:01:22.180
And of course the output of the neural network directly depends on the input of the neural network.

20
00:01:22.360 --> 00:01:24.010
So that's going to be our argument.

21
00:01:24.280 --> 00:01:31.720
And now we can give it any name we will actually call it state because the input of the neural networks

22
00:01:31.960 --> 00:01:38.280
are the input states that are and coded by a vector of five dimensions to three signals orientation

23
00:01:38.480 --> 00:01:39.920
and minus orientation.

24
00:01:40.120 --> 00:01:42.120
And so now things are going to be easy.

25
00:01:42.160 --> 00:01:47.860
We are going to feed the input state into the neural network the one that we build right above right

26
00:01:47.860 --> 00:01:54.520
here with the next class and then then we're going to get the outputs which are the key values for each

27
00:01:54.520 --> 00:01:59.530
of the three possible actions and then using the soft Max method which I'm going to explain in this

28
00:01:59.530 --> 00:02:00.360
tutorial.

29
00:02:00.400 --> 00:02:03.140
We're going to get the final action to play.

30
00:02:03.280 --> 00:02:08.460
So let's do this let's go into the function and let's implement all this.

31
00:02:08.470 --> 00:02:14.980
So the first thing we need to start with is about what I've just mentioned soughed Max the idea of the

32
00:02:14.980 --> 00:02:20.590
soft Max is that we're going to try to get the best action to play at each time.

33
00:02:20.740 --> 00:02:25.000
But at the same time we will be exploring the different actions.

34
00:02:25.000 --> 00:02:25.950
And how do we do that.

35
00:02:25.960 --> 00:02:31.080
How can we get the best action to play while still exploring the other actions.

36
00:02:31.270 --> 00:02:39.250
Well we used this idea of stuff which consists of generating a distribution of probabilities for each

37
00:02:39.250 --> 00:02:40.370
of the q values.

38
00:02:40.420 --> 00:02:42.100
Q States action.

39
00:02:42.160 --> 00:02:46.600
Now we have one Q value for each action go left go straight or go right.

40
00:02:46.850 --> 00:02:49.680
But this q value also depends on the input state.

41
00:02:49.690 --> 00:02:52.720
That's exactly the Q function use on intuition lectures.

42
00:02:52.870 --> 00:02:56.280
This Q function is a function of the state and the action.

43
00:02:56.320 --> 00:03:02.540
So since we have here one input state which is the state here and three possible actions we have three

44
00:03:02.540 --> 00:03:09.070
new values Q. State action 1 Q state action 2 and two states action 3 and we are going to generate a

45
00:03:09.070 --> 00:03:13.760
distribution of probabilities with respect to these three key values.

46
00:03:13.930 --> 00:03:19.420
That is we're going to have one probability for the first Q value one of the probability for the second

47
00:03:19.420 --> 00:03:25.490
Q value and a third probability for the third Q And all the three probabilities will sum up to 1.

48
00:03:25.670 --> 00:03:31.840
And so we're going to do all this with soughed Max and soughed Max will attribute a large probability

49
00:03:32.170 --> 00:03:33.530
to the highest Q around.

50
00:03:33.820 --> 00:03:41.050
That's why an alternative to soft Max is a simple RMX no directly taking the maximum of the q values

51
00:03:41.530 --> 00:03:44.860
but in that case we're not exploring the other actions.

52
00:03:44.920 --> 00:03:50.500
Thanks to these probabilities we can explore somewhere else using a temperature parameter that we we're

53
00:03:50.500 --> 00:03:51.900
going to see very quickly.

54
00:03:52.210 --> 00:03:55.990
We can still explore them by configuring this temperature parameter.

55
00:03:56.020 --> 00:04:03.380
That's why in general for security I highly recommend to use a soft x rather than a simple RMX.

56
00:04:03.460 --> 00:04:06.990
All right so let's implements of X and therefore as you understood.

57
00:04:07.060 --> 00:04:12.990
Since soughed Max returns the probabilities of each of the three Q values for the three possible actions.

58
00:04:13.180 --> 00:04:20.120
Well the first variable we are going to create is probably referring of course to these probabilities.

59
00:04:20.450 --> 00:04:26.680
So props equals and now we're going to take our soughed next function and according to you where are

60
00:04:26.680 --> 00:04:28.070
we going to take it from.

61
00:04:28.330 --> 00:04:31.600
Well of course remember we imported the.

62
00:04:31.700 --> 00:04:38.130
And then does functional submodule which I remind is the module that contains most of the actions to

63
00:04:38.140 --> 00:04:39.790
implement a neural network.

64
00:04:39.820 --> 00:04:44.980
We gave it the shortcut F and so that's actually from this functional submodule that we're going to

65
00:04:44.980 --> 00:04:46.990
take our self next function.

66
00:04:47.290 --> 00:04:53.830
But since we gave it a shortcut f we start here with a Neph representing functional from which we take

67
00:04:54.040 --> 00:04:56.080
our soughed next function.

68
00:04:56.080 --> 00:04:56.920
Here it is.

69
00:04:56.980 --> 00:04:59.540
That's the first one and parenthesis.

70
00:04:59.770 --> 00:05:00.160
All right.

71
00:05:00.200 --> 00:05:03.920
Now what do we need to input in the next function.

72
00:05:04.150 --> 00:05:10.020
Well that's of course the entities for which we want to generate a probability distribution.

73
00:05:10.190 --> 00:05:11.430
And what are these entities.

74
00:05:11.550 --> 00:05:13.870
Well these are of course the key values.

75
00:05:13.870 --> 00:05:16.790
So now the question is how can we get the q values.

76
00:05:16.960 --> 00:05:22.720
Well of course the q values are the output of the neural network and to get these outputs of the neural

77
00:05:22.720 --> 00:05:23.410
network.

78
00:05:23.590 --> 00:05:24.560
Well here we go.

79
00:05:24.610 --> 00:05:26.830
We need to take our new network.

80
00:05:27.100 --> 00:05:33.520
But in fact we already have it because that's what initialized in the end it's function.

81
00:05:33.530 --> 00:05:39.980
Know we created a self-taught model which is nothing else that will not work because it is a new object

82
00:05:40.290 --> 00:05:41.540
of the network class.

83
00:05:41.600 --> 00:05:42.820
And so that's perfect.

84
00:05:42.830 --> 00:05:49.040
We can just take our model here and stuff next apply this model to the input state which is the argument

85
00:05:49.040 --> 00:05:52.950
here and that will return the outputs that we're looking for.

86
00:05:53.090 --> 00:05:54.440
That is the key values.

87
00:05:54.560 --> 00:06:00.260
And so now your intuition why we had to take the model here to introduce it in the function might get

88
00:06:00.260 --> 00:06:00.840
better.

89
00:06:00.920 --> 00:06:06.410
Those of you starting with object oriented programming you will see that all this will become natural

90
00:06:07.100 --> 00:06:08.780
so soft next then.

91
00:06:08.870 --> 00:06:16.840
So we take our models self model because this must be the model of the object that we created here.

92
00:06:17.180 --> 00:06:24.350
But then we need to get the output of our neural network model and therefore we're going to hear some

93
00:06:24.350 --> 00:06:30.400
parenthesis in which we are going to input Well the input state named state here.

94
00:06:30.620 --> 00:06:39.350
So what we want to do at first is enter state but now we must be careful to something state looks like

95
00:06:39.350 --> 00:06:40.560
a simple set right now.

96
00:06:40.790 --> 00:06:46.850
But remember that state is actually going to be a torch sensor because later we're going to use this

97
00:06:46.920 --> 00:06:52.190
cell at less state to put it as the argument of the Select action function.

98
00:06:52.190 --> 00:06:57.430
The state's argument that is here is actually going to become later this self-taught less state.

99
00:06:57.680 --> 00:07:01.680
And since this is a tortured answer world the model will accept it.

100
00:07:01.760 --> 00:07:02.690
So that's fine.

101
00:07:02.810 --> 00:07:05.000
But now we can improve the algorithm.

102
00:07:05.180 --> 00:07:12.490
So as soon as the state is a torch sensor and as we said earlier most of the sensors are wrapped into

103
00:07:12.500 --> 00:07:13.260
voivode.

104
00:07:13.320 --> 00:07:15.640
This will also contain a gradient.

105
00:07:15.650 --> 00:07:22.110
So right now what we're going to do first is wrap this input state that is a tensor into a torch very

106
00:07:22.110 --> 00:07:27.990
well but since this is the input states Well there is not going to be some differentiation.

107
00:07:28.160 --> 00:07:34.700
We will not be using the gradient of this state torch Voivode and that can be stations and therefore

108
00:07:34.880 --> 00:07:45.530
what we're going to do now is convert this torch sensor state into a torch variable like so.

109
00:07:45.780 --> 00:07:51.400
But then to specify that we don't want the gradients in the graph at all that can predations at the

110
00:07:51.400 --> 00:07:52.380
end of Mudgal.

111
00:07:52.570 --> 00:07:57.800
Well we will at here come up volatile equals true.

112
00:07:58.150 --> 00:08:06.160
So that now we have our state torched sensor into a torch very well but thanks to this Votel equals

113
00:08:06.160 --> 00:08:07.200
true barometer.

114
00:08:07.390 --> 00:08:14.950
Well we will be including the gradients associated to this input states to the graph of all the conditions

115
00:08:15.100 --> 00:08:16.530
of the end in that model.

116
00:08:16.840 --> 00:08:18.530
So that's another technical trick.

117
00:08:18.550 --> 00:08:23.130
This will save us some memory and therefore this will improve the performance.

118
00:08:23.170 --> 00:08:27.850
So I highly recommend to do this and now we're going to add something more fun.

119
00:08:27.910 --> 00:08:30.640
It's about this temperature parameter that I've just mentioned.

120
00:08:30.850 --> 00:08:36.190
So this temperature parameter is the parameter that would allow us to modulate how the neural network

121
00:08:36.190 --> 00:08:40.040
will be sure of which action it should decide to play.

122
00:08:40.210 --> 00:08:47.290
So this temperature parameter will be a positive number and the closer it is to zero the less sure the

123
00:08:47.290 --> 00:08:53.200
neural network will be when playing in action and the higher the temperature parameter is the more sure

124
00:08:53.410 --> 00:08:56.540
the neural network will be of the action it decides to play.

125
00:08:56.890 --> 00:09:04.480
And to add this parameter I'm going to multiply the outputs which are the Kugan used by this temperature

126
00:09:04.480 --> 00:09:05.250
parameter.

127
00:09:05.500 --> 00:09:13.440
So let's start for example with 7 and I'm going to specify here the little comment T equals 7.

128
00:09:13.460 --> 00:09:15.610
So that's the temperature parameter.

129
00:09:15.690 --> 00:09:17.210
I'm sorry go to 7.

130
00:09:17.260 --> 00:09:21.010
We're going to try some other ones but I just want to start with a small one because you're going to

131
00:09:21.010 --> 00:09:22.470
see that with a small one.

132
00:09:22.510 --> 00:09:28.150
Our car will still behave like some kind of an insect but then by increasing the temperature parameter

133
00:09:28.510 --> 00:09:34.340
our code will look more like a car and decides to sell driving will be much much better.

134
00:09:34.480 --> 00:09:40.450
And so that makes sense because the higher is this temperature parameter the higher will be the probability

135
00:09:40.450 --> 00:09:48.010
of the winning Juval me because for example if we have soft max of the q values.

136
00:09:48.190 --> 00:09:54.850
Let's take some simple numbers one two three if stuffed max of one to three equals.

137
00:09:54.850 --> 00:10:01.150
For example 0.04 0.11 and open eighty five.

138
00:10:01.270 --> 00:10:05.650
Then by increasing the temperature by taking higher temperature.

139
00:10:05.680 --> 00:10:13.360
Right now temperature equals one by taking a high temperature like for example Tussaud subtracts let's

140
00:10:13.360 --> 00:10:22.210
copy this and multiply it by for example two or three so next have the same values but multiplied by

141
00:10:22.210 --> 00:10:24.110
the temperature parameter of three.

142
00:10:24.370 --> 00:10:31.390
Well we will get something like zero for the first Q value because this had a very low probability that

143
00:10:31.530 --> 00:10:38.020
something around zero then something very small for the second probability because this was still a

144
00:10:38.020 --> 00:10:39.260
low probability.

145
00:10:39.410 --> 00:10:42.910
So let's say for example or point 0 2.

146
00:10:43.320 --> 00:10:49.910
But then this third probability since it was the largest one and a pretty high one.

147
00:10:50.140 --> 00:10:55.180
Well increasing the temperature this probability will be even larger because we're going to be even

148
00:10:55.180 --> 00:11:02.230
more sure that this is the right Q value corresponding to the action we must play and therefore this

149
00:11:02.230 --> 00:11:05.630
is going to be something like 0.2 98.

150
00:11:05.980 --> 00:11:11.800
Now by increasing the temperature parameter Well we are now even more sure that the third action here

151
00:11:11.800 --> 00:11:17.530
should be the action to play because the probability for the q value of this action is not only the

152
00:11:17.530 --> 00:11:19.590
largest one but also very high.

153
00:11:19.840 --> 00:11:22.600
So that's what this temperature parameter is all about.

154
00:11:22.660 --> 00:11:27.340
It's about the certainty of which direction we should decide to play.

155
00:11:27.340 --> 00:11:27.610
All right.

156
00:11:27.610 --> 00:11:29.450
So I'm going to remove this comment.

157
00:11:29.470 --> 00:11:31.000
This was just to explain.

158
00:11:31.200 --> 00:11:33.490
And now let's get our action.

159
00:11:33.490 --> 00:11:35.370
So how are you going to do that.

160
00:11:35.560 --> 00:11:41.440
Well the principle of the next method is not only to generate a probability distribution for each of

161
00:11:41.440 --> 00:11:46.390
the key values but also and that's the second step of the soft next method.

162
00:11:46.480 --> 00:11:51.820
We take a random draw from this distribution to get our final action.

163
00:11:52.010 --> 00:11:57.310
And of course we will have a high chance to get the action that corresponds to the Q value that has

164
00:11:57.310 --> 00:12:01.660
the highest probability because that's exactly how the distribution works.

165
00:12:01.660 --> 00:12:02.550
So there we go.

166
00:12:02.560 --> 00:12:04.040
Let's get our action.

167
00:12:04.060 --> 00:12:11.380
So we're going to introduce a new Voivode is we're going to call action and this action is going to

168
00:12:11.380 --> 00:12:17.460
be a random draw of the probability distribution that we just created at this time before.

169
00:12:17.510 --> 00:12:20.100
And so how do we get such a random draw.

170
00:12:20.200 --> 00:12:26.410
Well we're going to take our prop's probabilities of each of the key values we take props and then dart

171
00:12:26.650 --> 00:12:34.120
and then we're going to use the multi Gnomeo function and that will give us a random draw from this

172
00:12:34.120 --> 00:12:36.030
distribution process.

173
00:12:36.160 --> 00:12:38.420
So that's all that will get his reaction.

174
00:12:38.470 --> 00:12:39.280
Perfect.

175
00:12:39.490 --> 00:12:42.790
And now of course we are going to return the action.

176
00:12:42.790 --> 00:12:44.730
There is a little trick here.

177
00:12:44.810 --> 00:12:51.460
What is the fact that this Propst that multinomial returns the PI towards viable with a fake badge.

178
00:12:51.490 --> 00:12:57.210
You know this fake diamonds and corresponding to the batch and therefore to get the right result that

179
00:12:57.220 --> 00:13:00.540
we want that is the action in 0 1 or 2.

180
00:13:00.820 --> 00:13:08.200
We just need to add here data and then some brackets and the actions here are one or two that we're

181
00:13:08.230 --> 00:13:13.100
looking for is content and the index is 0 and 0.

182
00:13:13.570 --> 00:13:14.000
All right.

183
00:13:14.000 --> 00:13:14.730
And there we go.

184
00:13:14.740 --> 00:13:21.420
Now we have our action thanks to this select action function the AI will now know which action to play.

185
00:13:21.490 --> 00:13:22.440
And each time.

186
00:13:22.810 --> 00:13:23.460
Terrific.

187
00:13:23.500 --> 00:13:27.430
So now we can move on to the next function which will be the learn function.

188
00:13:27.520 --> 00:13:32.410
And that's where we will train the whole neural network you know with all the forward propagation and

189
00:13:32.410 --> 00:13:35.790
then the back propagation is is to categorize in the sense.

190
00:13:35.950 --> 00:13:41.500
Well basically we will implement the whole training of the deep learning model that is at the heart

191
00:13:41.560 --> 00:13:43.340
of our artificial intelligence.

192
00:13:43.480 --> 00:13:44.680
So I can't wait to do that.

193
00:13:44.680 --> 00:13:49.290
This is going to be an exciting tutorial and so I'll see you in the next Statoil.

194
00:13:49.510 --> 00:13:50.670
Until then enjoy.

195
00:13:50.720 --> 00:13:51.000
I.
