WEBVTT
1
1

00:00:00.140  -->  00:00:02.240
<v Instructor>In this demo, I am going to show you</v>
2

2

00:00:02.240  -->  00:00:05.223
how backup and restore works in kops.
3

3

00:00:07.100  -->  00:00:10.750
In my Kubernetes course I have a directory etcd,
4

4

00:00:10.750  -->  00:00:13.253
and in etcd I have a README file.
5

5

00:00:14.470  -->  00:00:19.470
To create a high available cluster, you can use this line:
6

6

00:00:20.090  -->  00:00:22.980
kops create cluster, still the kubernetes.newtech.academy
7

7

00:00:22.980  -->  00:00:26.820
cluster, state; and then you can specify the zones:
8

8

00:00:26.820  -->  00:00:31.300
eu-west-1a, eu-west-1b, eu-west-1c and the master zones.
9

9

00:00:31.300  -->  00:00:36.107
So we gonna have three masters, also 1a, 1b, and 1c,
10

10

00:00:36.107  -->  00:00:40.300
and a node count of three, node-size=t2.micro
11

11

00:00:40.300  -->  00:00:43.730
and then master-size=t2.micro and then dns-zone.
12

12

00:00:43.730  -->  00:00:47.060
This will spin up three masters, three workers,
13

13

00:00:47.060  -->  00:00:50.230
all in different AZs, availability zones,
14

14

00:00:50.230  -->  00:00:52.833
so that we have a high available cluster.
15

15

00:00:54.050  -->  00:00:57.560
You can execute this command, kops create cluster,
16

16

00:00:57.560  -->  00:01:00.610
and then kops update cluster minus minus yes
17

17

00:01:00.610  -->  00:01:02.730
with the state file; then you'll have
18

18

00:01:02.730  -->  00:01:04.393
a high available cluster running.
19

19

00:01:05.500  -->  00:01:08.250
And then we can go and test the backups.
20

20

00:01:08.250  -->  00:01:11.047
So, let's have a look.
21

21

00:01:11.047  -->  00:01:13.880
(keyboard typing)
22

22

00:01:15.130  -->  00:01:18.773
I have three masters; so, these are my masters.
23

23

00:01:19.680  -->  00:01:22.340
And we have etcd running in kops,
24

24

00:01:22.340  -->  00:01:26.060
and etcd is a bit different than in a Minikube etcd
25

25

00:01:26.060  -->  00:01:28.400
because now you have three nodes.
26

26

00:01:28.400  -->  00:01:31.708
But also kops has an etcd manager that it uses
27

27

00:01:31.708  -->  00:01:36.030
so that it can take backups automatically to S3 as well.
28

28

00:01:36.030  -->  00:01:39.560
So let's have a look at those, kube kubectl get pods
29

29

00:01:39.560  -->  00:01:42.673
in kube-system, and we're going to filter on etcd.
30

30

00:01:44.460  -->  00:01:48.360
Then you see we have three etcd manager events
31

31

00:01:48.360  -->  00:01:52.357
and three etcd manager main, and these we need to have
32

32

00:01:52.357  -->  00:01:55.140
to make our cluster running.
33

33

00:01:55.140  -->  00:01:56.530
If you don't have these,
34

34

00:01:56.530  -->  00:01:59.490
then we don't have a cluster basically.
35

35

00:01:59.490  -->  00:02:02.270
We don't have a way to persist our data.
36

36

00:02:02.270  -->  00:02:07.270
So let's log in into one of them, kubectl exec minus it.
37

37

00:02:09.210  -->  00:02:10.803
And we're going to open bash,
38

38

00:02:11.740  -->  00:02:14.373
and it'll be in the kubesystem namespace.
39

39

00:02:15.724  -->  00:02:19.720
Here we have etcd running, and in the current directory
40

40

00:02:19.720  -->  00:02:21.213
and slash we have etcd,
41

41

00:02:22.910  -->  00:02:27.253
etcd-manager command line utility, CTL.
42

42

00:02:28.470  -->  00:02:31.173
And we can do help to show the help.
43

43

00:02:32.010  -->  00:02:35.750
With this tool we can get the cluster specs,
44

44

00:02:35.750  -->  00:02:38.650
we can list the backups, and we can also restore a backup.
45

45

00:02:39.820  -->  00:02:44.820
So, let's list the backups; and to list the backups
46

46

00:02:45.690  -->  00:02:49.110
I need to add the backup store argument.
47

47

00:02:49.110  -->  00:02:52.950
So I'm gonna add backup-store and then this full URL,
48

48

00:02:52.950  -->  00:02:56.860
kops-state, your, this is basically your S3 bucket,
49

49

00:02:56.860  -->  00:03:00.400
kubernetes.newtech.academy backups-etcd-main.
50

50

00:03:00.400  -->  00:03:02.840
So that's where, in S3, the backups are stored;
51

51

00:03:02.840  -->  00:03:05.303
and I'm then going to enter list backups.
52

52

00:03:07.230  -->  00:03:09.620
So then it connects to S3 and then
53

53

00:03:09.620  -->  00:03:11.473
lists the backups that I have now.
54

54

00:03:12.400  -->  00:03:13.930
So, what I'm going to do now,
55

55

00:03:13.930  -->  00:03:15.480
I'm going to destroy my cluster
56

56

00:03:16.620  -->  00:03:19.770
and I will have to restore one of these backups;
57

57

00:03:19.770  -->  00:03:21.410
and then I'm going to check whether I still have
58

58

00:03:21.410  -->  00:03:25.560
the same information and whether my cluster still comes up.
59

59

00:03:25.560  -->  00:03:29.090
So I'm gonna exit here, and I'm still going to do
60

60

00:03:29.090  -->  00:03:32.143
one more command, kubectl get configmap.
61

61

00:03:33.600  -->  00:03:37.423
I actually created a configmap using the readme.
62

62

00:03:37.423  -->  00:03:39.640
The command is in the readme itself,
63

63

00:03:39.640  -->  00:03:43.760
so kubectl create configmap readme is a readme,
64

64

00:03:43.760  -->  00:03:46.520
but if you already have a Kubernetes object in your cluster,
65

65

00:03:46.520  -->  00:03:48.170
you don't have to do this command.
66

66

00:03:48.170  -->  00:03:51.130
You just need to have some object that you wanna check
67

67

00:03:51.130  -->  00:03:54.060
whether the backup restore was successful
68

68

00:03:54.060  -->  00:03:55.040
because if you create the object,
69

69

00:03:55.040  -->  00:03:56.370
you also have to wait 15 minutes
70

70

00:03:56.370  -->  00:03:58.193
to make sure that the backup runs.
71

71

00:03:59.570  -->  00:04:02.090
In my cluster it's like 15 minutes
72

72

00:04:02.090  -->  00:04:05.350
between them that the backup is taken.
73

73

00:04:05.350  -->  00:04:08.210
So, I'm gonna have a look at this object;
74

74

00:04:08.210  -->  00:04:11.163
but first, I'm going to delete my master nodes.
75

75

00:04:12.600  -->  00:04:14.710
So these are my instances.
76

76

00:04:14.710  -->  00:04:17.823
Three master nodes, so let's get rid of them.
77

77

00:04:18.990  -->  00:04:23.990
Instant state terminate, and my Kubernetes cluster is gone.
78

78

00:04:24.210  -->  00:04:25.773
My masters are deleted.
79

79

00:04:26.760  -->  00:04:28.623
It'll take a few moments,
80

80

00:04:31.620  -->  00:04:33.620
and then you'll also wanna go to volumes
81

81

00:04:34.910  -->  00:04:39.730
and then now we have these volumes here,
82

82

00:04:39.730  -->  00:04:44.460
etcd main event for A, B, and C.
83

83

00:04:44.460  -->  00:04:47.160
Once they become available, then we also,
84

84

00:04:47.160  -->  00:04:49.143
then we also want to delete them.
85

85

00:04:50.320  -->  00:04:52.540
So there's still two in use.
86

86

00:04:52.540  -->  00:04:54.440
I'll have to wait a little bit longer.
87

87

00:04:55.700  -->  00:04:59.653
I'll already delete those, delete volumes.
88

88

00:05:01.195  -->  00:05:03.313
I wanna make sure the data is completely gone.
89

89

00:05:09.694  -->  00:05:13.200
There is still one master shutting down.
90

90

00:05:13.200  -->  00:05:15.100
And we also have other scaling groups.
91

91

00:05:16.250  -->  00:05:19.053
If we have a look here, master, master, master.
92

92

00:05:21.790  -->  00:05:26.140
These are going to launch new instances to make sure
93

93

00:05:26.140  -->  00:05:29.640
that the masters are recreated, so this is done in AWS.
94

94

00:05:29.640  -->  00:05:31.230
There's an autoscaling group making sure
95

95

00:05:31.230  -->  00:05:34.660
that new instances will be created.
96

96

00:05:34.660  -->  00:05:38.623
So we already have one pending here, A, B, C;
97

97

00:05:39.460  -->  00:05:41.650
and this one's still shutting down.
98

98

00:05:41.650  -->  00:05:44.500
It's strange here that it takes a long time to shut down.
99

99

00:05:45.671  -->  00:05:48.853
It's a bit annoying because I don't want,
100

100

00:05:51.557  -->  00:05:54.370
because I also want to remove those two volumes.
101

101

00:05:58.506  -->  00:05:59.906
I'm gonna do a force detach.
102

102

00:06:00.968  -->  00:06:03.120
I just wanna make sure that Kubernetes is not going to
103

103

00:06:03.120  -->  00:06:07.493
reuse this volume here in a new master, so...
104

104

00:06:10.120  -->  00:06:12.290
Let me just remove all these tags here
105

105

00:06:15.180  -->  00:06:19.503
to make sure that it is not reused.
106

106

00:06:22.970  -->  00:06:27.970
There we go, and then now this one's already available.
107

107

00:06:29.850  -->  00:06:31.600
Now, we'll delete this one as well.
108

108

00:06:35.320  -->  00:06:37.470
So now that our Kubernetes cluster is actually
109

109

00:06:37.470  -->  00:06:42.470
a little bit broken, let's log in to one of these nodes.
110

110

00:06:42.640  -->  00:06:43.783
Let's try to fix it.
111

111

00:06:47.010  -->  00:06:49.970
So if you do kubectl get nodes or get anything,
112

112

00:06:49.970  -->  00:06:53.240
it will not work anymore, also because IP addresses
113

113

00:06:53.240  -->  00:06:56.080
have been changed and that takes time to change as well.
114

114

00:06:56.080  -->  00:06:58.082
So we'll just use ssh...
115

115

00:06:58.082  -->  00:07:01.360
(keyboard typing)
116

116

00:07:01.360  -->  00:07:03.203
to log into one of the nodes,
117

117

00:07:05.330  -->  00:07:08.820
and then we need to repair our etcd cluster.
118

118

00:07:08.820  -->  00:07:13.030
So let's have a look at docker ps etcd.
119

119

00:07:13.030  -->  00:07:15.290
About a minute ago it came up,
120

120

00:07:15.290  -->  00:07:17.410
so you see it takes a long time.
121

121

00:07:17.410  -->  00:07:19.910
And I need to restore this etcd manager.
122

122

00:07:19.910  -->  00:07:23.610
So, so this one's the main; this one's the events,
123

123

00:07:23.610  -->  00:07:25.423
but we need to restore both of them.
124

124

00:07:27.093  -->  00:07:30.940
Let me start bash.
125

125

00:07:30.940  -->  00:07:32.940
Actually, let me first check the logs of it,
126

126

00:07:32.940  -->  00:07:36.293
docker logs of this instance.
127

127

00:07:37.639  -->  00:07:39.889
It is actually still waiting for the volumes.
128

128

00:07:42.880  -->  00:07:44.830
Let's have a look at our volumes again.
129

129

00:07:46.500  -->  00:07:48.400
This is the last one I want to delete.
130

130

00:07:54.170  -->  00:07:59.170
Let's exit here, and let's run kops update cluster;
131

131

00:08:02.740  -->  00:08:04.530
and now it's going to make sure that
132

132

00:08:04.530  -->  00:08:07.493
everything is available again in our AWS account.
133

133

00:08:10.240  -->  00:08:14.610
So when you do kops update, it will then recreate
134

134

00:08:14.610  -->  00:08:18.120
these volumes because I deleted them.
135

135

00:08:18.120  -->  00:08:20.103
It's not etcd itself that creates them;
136

136

00:08:20.103  -->  00:08:21.810
it's actually kops that creates them.
137

137

00:08:21.810  -->  00:08:24.430
So we delete the masters, delete the volumes,
138

138

00:08:24.430  -->  00:08:27.000
because we want to test our restore;
139

139

00:08:27.000  -->  00:08:29.070
and then we need to run kops update.
140

140

00:08:29.070  -->  00:08:32.500
Kops update will then make those volumes
141

141

00:08:32.500  -->  00:08:34.610
available for Kubernetes cluster.
142

142

00:08:34.610  -->  00:08:38.870
Etcd waits until the volume becomes available, and then
143

143

00:08:38.870  -->  00:08:41.570
it will attach it; and then if we have another look...
144

144

00:08:43.351  -->  00:08:46.184
(keyboard typing)
145

145

00:08:49.350  -->  00:08:51.253
there we see it's going to attach it.
146

146

00:08:52.419  -->  00:08:55.252
(keyboard typing)
147

147

00:08:59.750  -->  00:09:00.853
And there we go.
148

148

00:09:02.440  -->  00:09:04.560
Now it's going to format it,
149

149

00:09:04.560  -->  00:09:06.803
and then it will discover that it's empty.
150

150

00:09:07.690  -->  00:09:10.430
So it's going to read our cluster specification.
151

151

00:09:10.430  -->  00:09:12.793
That's why we need this etcd manager.
152

152

00:09:14.060  -->  00:09:16.803
It manages the etcd cluster.
153

153

00:09:18.690  -->  00:09:21.260
And then we, it says unexpected error
154

154

00:09:21.260  -->  00:09:23.640
running etcd cluster reconciliation.
155

155

00:09:23.640  -->  00:09:25.660
Etcd has zero members registered,
156

156

00:09:25.660  -->  00:09:28.343
must issue restore backup command to proceed.
157

157

00:09:30.290  -->  00:09:31.923
So this one is then the leader.
158

158

00:09:33.940  -->  00:09:37.440
I am the leader, and you should try
159

159

00:09:37.440  -->  00:09:41.590
to restore it on the leader in the backup.
160

160

00:09:41.590  -->  00:09:43.540
So let's try to restore the backup now.
161

161

00:09:45.230  -->  00:09:50.230
Docker exec of this container ID,
162

162

00:09:52.980  -->  00:09:56.653
gonna run minus IT bash;
163

163

00:09:57.700  -->  00:10:02.700
and then we do etcd, etcd-manager-ctl list-backups.
164

164

00:10:07.450  -->  00:10:10.403
And let's restore lost backup, this one.
165

165

00:10:11.380  -->  00:10:16.380
So we do restore backup, and then it's going to send
166

166

00:10:16.590  -->  00:10:19.320
a command or add a command to the queue.
167

167

00:10:19.320  -->  00:10:22.733
We can also use list queue, list commands,
168

168

00:10:23.660  -->  00:10:26.910
to see what the commands are in the queue.
169

169

00:10:26.910  -->  00:10:29.723
So now this command is restored, backup is in the queue.
170

170

00:10:30.970  -->  00:10:35.970
And let's do the same for our events,
171

171

00:10:36.100  -->  00:10:38.570
docker ps grep etcd.
172

172

00:10:38.570  -->  00:10:40.260
The first one is events;
173

173

00:10:40.260  -->  00:10:44.393
these also need to be restored, docker exec IT bash.
174

174

00:10:45.272  -->  00:10:48.360
We can use the same command, but we need to change this
175

175

00:10:48.360  -->  00:10:50.300
backup store because now instead of main,
176

176

00:10:50.300  -->  00:10:55.120
it's going to be events, etcd events list backups.
177

177

00:10:56.250  -->  00:11:01.010
These are the events backups, restore backup.
178

178

00:11:01.010  -->  00:11:04.190
Then the last one, and then
179

179

00:11:04.190  -->  00:11:06.043
it's also going to add this command.
180

180

00:11:07.030  -->  00:11:09.100
Now you'll have to wait a little bit.
181

181

00:11:09.100  -->  00:11:14.100
We can check the logs to see what is happening, docker logs.
182

182

00:11:19.430  -->  00:11:22.220
And then if you read the command
183

183

00:11:23.290  -->  00:11:26.280
that needs to restore and then it should start
184

184

00:11:26.280  -->  00:11:28.753
at some point recovering the cluster.
185

185

00:11:31.010  -->  00:11:32.563
So maybe I do minus F,
186

186

00:11:35.770  -->  00:11:37.382
got a lot of output.
187

187

00:11:37.382  -->  00:11:40.550
Etcd has two members registered now.
188

188

00:11:40.550  -->  00:11:42.020
I'll try to expand the clusters,
189

189

00:11:42.020  -->  00:11:44.620
which will take time for the other nodes to come up.
190

190

00:11:46.130  -->  00:11:49.460
But at some point it will have three clusters again.
191

191

00:11:49.460  -->  00:11:52.710
So then the next step is actually that the etcd cluster
192

192

00:11:52.710  -->  00:11:56.653
will recover and then the API server also needs to recover.
193

193

00:11:57.630  -->  00:12:00.000
So we do ps aux API server.
194

194

00:12:00.000  -->  00:12:01.633
We don't have an API server yet.
195

195

00:12:02.960  -->  00:12:05.483
Docker ps -a grep kube-apiserver,
196

196

00:12:06.470  -->  00:12:09.870
so we have one of those exited; and I will show you why.
197

197

00:12:09.870  -->  00:12:11.140
Docker logs...
198

198

00:12:15.330  -->  00:12:20.330
It couldn't connect to 4,001 because that's where our etcd
199

199

00:12:21.460  -->  00:12:25.160
is running, and then it exited not very nicely even.
200

200

00:12:25.160  -->  00:12:27.330
It actually completely crashed,
201

201

00:12:27.330  -->  00:12:30.750
but a kubelet is going to restore the kube API server.
202

202

00:12:30.750  -->  00:12:35.183
So if we wait a little bit or we execute systemctl
203

203

00:12:36.380  -->  00:12:40.720
restart kubelet, then our API server also restarts.
204

204

00:12:40.720  -->  00:12:43.033
So I (mumbles) do docker ps grep kube-api,
205

205

00:12:46.490  -->  00:12:50.010
and now our kube-api is up five seconds ago.
206

206

00:12:50.010  -->  00:12:54.529
I do Docker logs minus F, and then it's still
207

207

00:12:54.529  -->  00:12:59.529
not being able to connect and it crashes again.
208

208

00:13:01.290  -->  00:13:05.550
We have to have another look at this endpoint,
209

209

00:13:05.550  -->  00:13:06.993
still connection refused.
210

210

00:13:07.940  -->  00:13:09.673
Let's have another look, etcd.
211

211

00:13:14.517  -->  00:13:16.684
Etcd manager docker logs,
212

212

00:13:21.680  -->  00:13:24.153
still only two members; we want three.
213

213

00:13:26.180  -->  00:13:28.941
And it's the B that is not connecting,
214

214

00:13:28.941  -->  00:13:30.353
connecting refused from B.
215

215

00:13:32.700  -->  00:13:34.740
What is happening here,
216

216

00:13:34.740  -->  00:13:39.740
our B volume is still doing something.
217

217

00:13:40.280  -->  00:13:43.940
So it will take some time before it can be attached.
218

218

00:13:49.260  -->  00:13:50.710
I actually think that there is an issue
219

219

00:13:50.710  -->  00:13:55.333
with the B availability zone in Amazon.
220

220

00:13:56.730  -->  00:14:00.010
That might actually happen because there seems
221

221

00:14:00.010  -->  00:14:02.243
to be an issue with this B master.
222

222

00:14:03.710  -->  00:14:07.290
One or more volumes attached to this instance is impaired.
223

223

00:14:07.290  -->  00:14:09.793
Please review the state of your volumes.
224

224

00:14:10.780  -->  00:14:11.653
That's not good.
225

225

00:14:13.190  -->  00:14:17.563
I can actually stop this B again and have it restarting.
226

226

00:14:19.330  -->  00:14:22.173
That's probably the easiest way to resolve this issue.
227

227

00:14:23.840  -->  00:14:26.950
I will shut down B again, terminate,
228

228

00:14:32.780  -->  00:14:34.123
delete this B volume,
229

229

00:14:38.580  -->  00:14:40.093
and delete this B volume.
230

230

00:14:46.260  -->  00:14:50.130
I'll again remove the labels, the tags;
231

231

00:14:50.130  -->  00:14:51.253
there's no tags here.
232

232

00:14:54.140  -->  00:14:55.040
Delete these tags.
233

233

00:15:00.737  -->  00:15:05.737
Delete these tags, save, refresh.
234

234

00:15:06.580  -->  00:15:07.653
It's available now.
235

235

00:15:08.600  -->  00:15:11.563
Delete again, run kops update.
236

236

00:15:17.250  -->  00:15:19.133
Kops will recreate the volumes.
237

237

00:15:24.810  -->  00:15:27.250
There we go; two available volumes,
238

238

00:15:27.250  -->  00:15:29.160
and then there will be a new one,
239

239

00:15:29.160  -->  00:15:32.373
master eu-west-1b attaching those.
240

240

00:15:33.440  -->  00:15:35.640
I only have to restore the backup once because then
241

241

00:15:35.640  -->  00:15:38.760
the leader will make sure the data gets everywhere.
242

242

00:15:38.760  -->  00:15:40.010
So now it's initializing.
243

243

00:15:42.680  -->  00:15:44.210
Let's have a look at the volumes.
244

244

00:15:44.210  -->  00:15:46.943
At some point they should start attaching.
245

245

00:15:50.350  -->  00:15:52.629
Let's have a look in this instance.
246

246

00:15:52.629  -->  00:15:55.462
(keyboard typing)
247

247

00:16:01.137  -->  00:16:04.970
So this is the B instance, up for two minutes.
248

248

00:16:06.810  -->  00:16:09.563
Etcd, up for five seconds, 20 seconds.
249

249

00:16:16.457  -->  00:16:20.540
And then it already attached the volume just now.
250

250

00:16:21.750  -->  00:16:23.853
As I look at the volumes again.
251

251

00:16:26.010  -->  00:16:30.470
So now it is in use, the first one, the events.
252

252

00:16:30.470  -->  00:16:33.073
And now the main also needs to be attached.
253

253

00:16:35.140  -->  00:16:39.790
This looks better; it has found the other A and C.
254

254

00:16:39.790  -->  00:16:42.580
So let's log again to our number A.
255

255

00:16:49.610  -->  00:16:54.610
And curl works now, curl to this 4001.
256

256

00:16:54.680  -->  00:16:58.793
It's using (mumbles) less, so it cannot really do anything.
257

257

00:17:00.240  -->  00:17:02.620
We get a bad, alert bad certificate
258

258

00:17:02.620  -->  00:17:04.890
because we don't have the certificates.
259

259

00:17:04.890  -->  00:17:08.620
You could log in to etcd only if you have the certificates,
260

260

00:17:08.620  -->  00:17:12.173
and the certificates are within the etcd container.
261

261

00:17:13.220  -->  00:17:15.923
So now the API server should be up.
262

262

00:17:17.550  -->  00:17:19.950
API server's up; let's take a looks.
263

263

00:17:19.950  -->  00:17:23.553
Look up ps grep kube-api docker logs,
264

264

00:17:27.610  -->  00:17:29.720
and it is up and running.
265

265

00:17:29.720  -->  00:17:31.660
We can use kubectl also here on the node
266

266

00:17:31.660  -->  00:17:35.310
because it will take some time before the DNS is updated.
267

267

00:17:35.310  -->  00:17:37.433
Kubectl get pods.
268

268

00:17:38.990  -->  00:17:41.058
No pods found in default namespace,
269

269

00:17:41.058  -->  00:17:43.243
but here is my configmap.
270

270

00:17:44.150  -->  00:17:47.120
So the restore actually worked.
271

271

00:17:47.120  -->  00:17:50.480
So in this lecture, in this demo, I showed you
272

272

00:17:50.480  -->  00:17:54.180
that there are backups by default in kops on S3.
273

273

00:17:54.180  -->  00:17:57.260
I deleted the master nodes, the volumes,
274

274

00:17:57.260  -->  00:18:00.410
used kops updates to recreate the volumes.
275

275

00:18:00.410  -->  00:18:02.690
The nodes itself were recreated automatically
276

276

00:18:02.690  -->  00:18:05.170
because of the autoscaling group.
277

277

00:18:05.170  -->  00:18:08.449
Then it attaches the volume, formats the volume.
278

278

00:18:08.449  -->  00:18:12.530
etcd says there is a problem; I cannot find my files.
279

279

00:18:12.530  -->  00:18:15.120
There's no state, so you need to restore a backup.
280

280

00:18:15.120  -->  00:18:19.340
I restored a backup for the main and for the events,
281

281

00:18:19.340  -->  00:18:22.521
and then the kube API server keeps on restarting
282

282

00:18:22.521  -->  00:18:25.690
until etcd is available again.
283

283

00:18:25.690  -->  00:18:28.700
Etcd is available; I can do kubectl get configmap,
284

284

00:18:28.700  -->  00:18:31.300
and I see my configmap that I created
285

285

00:18:31.300  -->  00:18:32.933
before destroying the volumes.
286

286

00:18:34.730  -->  00:18:36.400
Hopefully you don't really have to do that,
287

287

00:18:36.400  -->  00:18:38.900
but it's definitely good to know if your etcd cluster
288

288

00:18:38.900  -->  00:18:41.560
becomes corrupt how you have to restore your data
289

289

00:18:41.560  -->  00:18:45.383
and how all the systems kind of work together.
290

290

00:18:47.180  -->  00:18:48.380
So this is for kops.
291

291

00:18:48.380  -->  00:18:52.830
In the theory, I also showed you the common commands
292

292

00:18:52.830  -->  00:18:56.800
to use the etcd command line utility to take backups.
293

293

00:18:56.800  -->  00:18:58.500
We don't really need it here
294

294

00:18:58.500  -->  00:19:00.830
because we have the etcd manager.
295

295

00:19:00.830  -->  00:19:03.900
All different clusters have a different kind of setup,
296

296

00:19:03.900  -->  00:19:06.870
so it will depend a little bit on your setup
297

297

00:19:06.870  -->  00:19:09.323
of your cluster what kind of tooling that you are using
298

298

00:19:09.323  -->  00:19:11.730
to see how you have to take backups
299

299

00:19:11.730  -->  00:19:14.040
and restore backups of etcd.
300

300

00:19:14.040  -->  00:19:16.400
So for kops, it's pretty straightforward,
301

301

00:19:16.400  -->  00:19:18.950
especially because they already take the backups for you.
302

302

00:19:18.950  -->  00:19:21.320
In other systems, you might have to take the backups.
303

303

00:19:21.320  -->  00:19:23.230
Even in some managed systems,
304

304

00:19:23.230  -->  00:19:25.380
it might be done for (mumbles) for example.
305

305

00:19:26.340  -->  00:19:27.510
So this was it.
306

306

00:19:27.510  -->  00:19:31.282
It's very possible that you might get into some trouble
307

307

00:19:31.282  -->  00:19:33.360
trying out this lab yourself.
308

308

00:19:33.360  -->  00:19:36.360
You've seen me struggling with it already.
309

309

00:19:36.360  -->  00:19:38.450
So, if you have any questions,
310

310

00:19:38.450  -->  00:19:41.043
use the Q&amp;A and I'll be ready to help you.
