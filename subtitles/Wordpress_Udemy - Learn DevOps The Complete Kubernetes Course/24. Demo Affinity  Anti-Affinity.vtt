WEBVTT
1
1

00:00:00.040  -->  00:00:04.630
<v Edward>So here I have spun up a cluster using Kubs,</v>
2

2

00:00:04.630  -->  00:00:05.923
with three nodes.
3

3

00:00:08.214  -->  00:00:11.603
So here are three nodes running 1.9.8.
4

4

00:00:12.670  -->  00:00:14.340
And if I describe one node,
5

5

00:00:14.340  -->  00:00:16.000
I will be able to see the labels.
6

6

00:00:16.000  -->  00:00:17.513
So I'm going to describe node,
7

7

00:00:18.370  -->  00:00:20.490
and then I'm going to describe my first node,
8

8

00:00:20.490  -->  00:00:22.240
not the master, but the first node.
9

9

00:00:24.520  -->  00:00:27.223
And let's scroll up.
10

10

00:00:28.460  -->  00:00:30.490
And then we see the labels.
11

11

00:00:30.490  -->  00:00:33.770
So here we see the architecture amd64,
12

12

00:00:33.770  -->  00:00:36.910
instance-type t2.micro, OS Linux,
13

13

00:00:36.910  -->  00:00:41.660
region eu-west-1, and it's in the eu-west-1a zone,
14

14

00:00:41.660  -->  00:00:44.620
and the host name is the full alias host name.
15

15

00:00:44.620  -->  00:00:47.300
So these labels already exist.
16

16

00:00:47.300  -->  00:00:49.300
You can use them for affinity
17

17

00:00:50.570  -->  00:00:53.340
if you're going to make your own rules.
18

18

00:00:53.340  -->  00:00:56.360
I'm going to use the example I showed you in the theory.
19

19

00:00:56.360  -->  00:01:01.360
So I'm going to go to my Kubernetes course Git repository.
20

20

00:01:01.530  -->  00:01:03.363
And here I have a folder affinity.
21

21

00:01:05.000  -->  00:01:08.050
In this folder I have node-affinity.yaml,
22

22

00:01:09.610  -->  00:01:12.130
and I have a deployment that's called node-affinity
23

23

00:01:12.130  -->  00:01:15.400
that I will deploy to my Kubernetes cluster.
24

24

00:01:15.400  -->  00:01:17.730
So I need env def,
25

25

00:01:17.730  -->  00:01:21.453
and I need team engineering-project1, which is optional.
26

26

00:01:22.960  -->  00:01:26.700
So let me just create this one and see what happens.
27

27

00:01:26.700  -->  00:01:28.750
I'm going to create a node-affinity.yaml,
28

28

00:01:30.350  -->  00:01:32.103
and it's going to launch three pods.
29

29

00:01:33.311  -->  00:01:36.363
So I'll do kubectl get pod, and I get pending,
30

30

00:01:37.320  -->  00:01:39.730
because I haven't created labels yet.
31

31

00:01:39.730  -->  00:01:43.087
So if I do kubectl get describe pod,
32

32

00:01:48.343  -->  00:01:53.343
and the pod name, zero out of four nodes are available.
33

33

00:01:53.830  -->  00:01:55.590
The first one is the master,
34

34

00:01:55.590  -->  00:01:57.920
so it's not gonna scale on that one.
35

35

00:01:57.920  -->  00:01:58.920
And then all of them,
36

36

00:01:58.920  -->  00:02:01.070
they don't match my required affinity rule.
37

37

00:02:02.200  -->  00:02:07.200
So let's then start creating labels using kubectl label,
38

38

00:02:07.240  -->  00:02:08.603
so I can label a node.
39

39

00:02:09.770  -->  00:02:12.360
And I will do get nodes first.
40

40

00:02:12.360  -->  00:02:15.753
So label node.
41

41

00:02:15.753  -->  00:02:17.543
I will take the first node here,
42

42

00:02:18.580  -->  00:02:22.033
and I'm going to label it env=dev.
43

43

00:02:24.540  -->  00:02:29.540
And I'm also going to label the second node env=dev.
44

44

00:02:35.190  -->  00:02:38.730
And now you will see that all the pods have been scaled.
45

45

00:02:43.140  -->  00:02:44.793
It can take some time, obviously.
46

46

00:02:47.000  -->  00:02:52.000
And then you can see that now the pods are being scaled.
47

47

00:02:52.510  -->  00:02:54.900
Two running, one is still creating.
48

48

00:02:54.900  -->  00:02:58.803
If you now do describe this pod,
49

49

00:03:00.830  -->  00:03:02.760
then we see it was successfully
50

50

00:03:02.760  -->  00:03:05.950
assigned to one of our nodes.
51

51

00:03:05.950  -->  00:03:07.540
So now it is running.
52

52

00:03:07.540  -->  00:03:10.823
It is running on this node, the one that ends with 119.
53

53

00:03:12.250  -->  00:03:15.690
That is the first one here, the first node here.
54

54

00:03:15.690  -->  00:03:18.610
And you can see that these two nodes
55

55

00:03:18.610  -->  00:03:21.077
now have the label env=dev.
56

56

00:03:21.077  -->  00:03:24.100
But the other label, it doesn't really matter.
57

57

00:03:24.100  -->  00:03:26.190
The other label was not taken into account
58

58

00:03:26.190  -->  00:03:28.210
because it doesn't exist on either of them,
59

59

00:03:28.210  -->  00:03:30.260
so it got scaled anyways.
60

60

00:03:30.260  -->  00:03:35.260
So now if we label our node that ends with 196,
61

61

00:03:37.710  -->  00:03:41.540
we add label team=engineering-project1,
62

62

00:03:46.830  -->  00:03:50.540
and then let's have a look at our YAML file.
63

63

00:03:50.540  -->  00:03:51.590
Engineering-project1,
64

64

00:03:52.700  -->  00:03:56.010
team engineering-project1 is preferred.
65

65

00:03:56.010  -->  00:03:59.990
So now if you would stop this node here
66

66

00:03:59.990  -->  00:04:04.990
that is running on the node IP ending on 119,
67

67

00:04:06.420  -->  00:04:08.660
then it should be rescaled on the one
68

68

00:04:08.660  -->  00:04:09.900
that is ending with 196,
69

69

00:04:09.900  -->  00:04:12.390
because now this one is the only one
70

70

00:04:12.390  -->  00:04:14.413
with the label engineering-project1.
71

71

00:04:15.970  -->  00:04:17.650
So let's try that.
72

72

00:04:17.650  -->  00:04:22.040
Let's try to get the name of this pod.
73

73

00:04:22.040  -->  00:04:26.453
This is node-affinity with vk7tg at the end.
74

74

00:04:27.800  -->  00:04:31.000
And this is now running on the one with 119.
75

75

00:04:31.000  -->  00:04:34.703
So let's say kubectl delete pod, this one.
76

76

00:04:35.770  -->  00:04:37.693
And then we do kubectl get pods.
77

77

00:04:39.410  -->  00:04:41.710
Then we see that this one is terminating.
78

78

00:04:41.710  -->  00:04:43.510
A new one has been created.
79

79

00:04:43.510  -->  00:04:45.210
This one is five seconds old.
80

80

00:04:45.210  -->  00:04:47.563
Let's describe this one.
81

81

00:04:48.530  -->  00:04:52.593
Kubectl describe pod.
82

82

00:04:57.772  -->  00:04:59.000
And where is this one now running?
83

83

00:04:59.000  -->  00:05:01.610
On the one that ends with 196,
84

84

00:05:01.610  -->  00:05:03.980
because the one that ends with 196
85

85

00:05:03.980  -->  00:05:08.430
is the one that best matches our affinity rules,
86

86

00:05:08.430  -->  00:05:10.060
because this is the only node
87

87

00:05:10.060  -->  00:05:12.860
that also has the team key value pair,
88

88

00:05:12.860  -->  00:05:15.770
the team label that I am looking for.
89

89

00:05:15.770  -->  00:05:19.530
So now typically, all the pods that I restart
90

90

00:05:19.530  -->  00:05:23.090
are going to be first scaled on this node.
91

91

00:05:23.090  -->  00:05:24.290
And if this node is full
92

92

00:05:24.290  -->  00:05:26.320
or it cannot scale anything anymore,
93

93

00:05:26.320  -->  00:05:28.850
then it will be scaled on other nodes
94

94

00:05:28.850  -->  00:05:32.270
that have the tag env=dev.
95

95

00:05:32.270  -->  00:05:35.770
If it doesn't have that mandatory tag, that required tag,
96

96

00:05:35.770  -->  00:05:38.130
then it will not be scaled at all.
97

97

00:05:38.130  -->  00:05:39.900
So that's it for this lecture.
98

98

00:05:39.900  -->  00:05:41.660
In the next lecture, I will explain to you
99

99

00:05:41.660  -->  00:05:44.403
about pod affinity and anti-affinity.
