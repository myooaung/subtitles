WEBVTT
1
00:00:00.450 --> 00:00:01.690
Welcome everyone.

2
00:00:01.770 --> 00:00:08.270
Last portion we talked about transfer learning in this segment.

3
00:00:08.280 --> 00:00:16.290
We're going to move away from transfer learning but borrowing the same concept and moved into feature

4
00:00:16.290 --> 00:00:17.860
extraction.

5
00:00:18.030 --> 00:00:21.000
What exactly is feature extraction.

6
00:00:21.120 --> 00:00:31.110
One of the beauty of Deep Neural learning neural network is that it's able to actually extract or pull

7
00:00:31.110 --> 00:00:42.000
out the salient key features of images as we've learned so far and the question is that can you actually

8
00:00:42.000 --> 00:00:50.460
use the neural network architecture that we've been using so far to extract features much like feature

9
00:00:50.460 --> 00:00:58.230
engineering that you do in the machine learning in classical machine learning and and solve it that

10
00:00:58.230 --> 00:00:58.620
way.

11
00:00:58.620 --> 00:01:05.250
So used the neural network that you build extract the key features and then apply very simple

12
00:01:07.680 --> 00:01:16.950
shallow machine learning to classify the pictures the end again turns out to be yes you can do that.

13
00:01:16.950 --> 00:01:22.920
The technique is called feature extraction so I'm going to show you how to do that right now.

14
00:01:23.070 --> 00:01:25.990
We are again working with the cats and dogs data center.

15
00:01:26.030 --> 00:01:27.580
Since you're comfortable with it.

16
00:01:27.720 --> 00:01:30.760
So let's continue to work through with this.

17
00:01:31.320 --> 00:01:32.940
And I'm going to show you that as a demo.

18
00:01:32.970 --> 00:01:38.540
So the first few blocks of codes are exactly the same that aren't actually any changes with those.

19
00:01:38.670 --> 00:01:40.570
So we're gonna skip through them.

20
00:01:40.770 --> 00:01:43.590
We're still going to make use of e.g. the differences.

21
00:01:44.220 --> 00:01:50.880
There's no major difference as to removing the top meaning we don't have the flattened layer nor the

22
00:01:50.880 --> 00:01:53.370
funny fully connected layer.

23
00:01:53.700 --> 00:01:53.970
Right.

24
00:01:53.970 --> 00:01:56.960
The variation comes in here.

25
00:01:56.970 --> 00:01:59.920
Notice the data augmentation that we are doing.

26
00:01:59.950 --> 00:02:00.330
All right.

27
00:02:00.330 --> 00:02:07.350
So that's the first part and we are actually providing in terms of the binary request that we are doing.

28
00:02:07.470 --> 00:02:07.740
All right.

29
00:02:07.770 --> 00:02:16.020
So other things that we are doing here is this we are actually setting the sample size providing the

30
00:02:16.020 --> 00:02:19.890
features and basically loops through them all one by one.

31
00:02:19.890 --> 00:02:27.820
So let me walk through these in terms of what we're looking to do from the train that again that we

32
00:02:27.820 --> 00:02:31.780
did earlier this is this which is this one here.

33
00:02:32.320 --> 00:02:39.940
And that train that again we save it as a training sample size which is thousand.

34
00:02:39.970 --> 00:02:44.120
Second thing is the feature we store a whole bunch of 0.

35
00:02:44.140 --> 00:02:44.470
OK.

36
00:02:44.500 --> 00:02:46.170
So we have train sample size.

37
00:02:46.180 --> 00:02:48.990
So essentially we're creating a tensor here.

38
00:02:49.060 --> 00:02:55.090
So four thousand is the is the first number comma four by four.

39
00:02:55.090 --> 00:02:55.420
All right.

40
00:02:55.660 --> 00:02:57.840
And then followed by five 1 2.

41
00:02:57.850 --> 00:03:04.500
We're creating a shape that is that is that has that shape right.

42
00:03:04.510 --> 00:03:08.200
So followed by the training labels which we will save.

43
00:03:09.070 --> 00:03:11.920
So these are creating a whole bunch of zeros.

44
00:03:11.920 --> 00:03:12.230
OK.

45
00:03:12.250 --> 00:03:18.310
So these are using num pi to create zeros for both the feature that we're going to feed as well as to

46
00:03:18.310 --> 00:03:26.710
label the next thing is that we look through the data which are the images and extract both the features

47
00:03:26.740 --> 00:03:30.700
as well as these labels and we run them one by one.

48
00:03:31.080 --> 00:03:31.350
OK.

49
00:03:31.360 --> 00:03:40.880
So we extract the features and store it into the features to extract that features here is based on.

50
00:03:41.050 --> 00:03:41.710
We find that

51
00:03:45.500 --> 00:03:48.920
extract that features is this one here.

52
00:03:48.920 --> 00:03:50.090
Sorry.

53
00:03:50.150 --> 00:03:50.390
OK.

54
00:03:50.390 --> 00:03:59.900
This extract that features is based on the so-called images that we have the data that's generated and

55
00:03:59.900 --> 00:04:07.700
we feed it through our neural net which is the be G.G. without the head removing the that flatten layer

56
00:04:07.730 --> 00:04:10.810
as well as the full fully connected layer.

57
00:04:11.270 --> 00:04:18.770
And just basically taking the output of the model which is core again we save it and extract it features

58
00:04:19.220 --> 00:04:30.070
and then we run it through and save it into our train features and the labels of these features are

59
00:04:30.070 --> 00:04:32.650
stored in train level labels.

60
00:04:32.650 --> 00:04:38.500
So the basic idea is is that we are looping through all of the data as all of the images.

61
00:04:38.500 --> 00:04:41.000
In other words the images are being fed in.

62
00:04:41.080 --> 00:04:49.870
We perform data on plantation but we already know the label of these so we store it in the in a variable

63
00:04:49.870 --> 00:04:56.590
called Train labels and the features which are the images we feed it through the neural network CNN

64
00:04:56.620 --> 00:05:04.720
that was done before which is the BCG and then the output of the image plus to CNN is stored in train

65
00:05:04.720 --> 00:05:10.100
features and then we increase AI by one.

66
00:05:10.150 --> 00:05:18.520
And basically when we actually once we reached a data size or batch size and told the total size is

67
00:05:18.520 --> 00:05:21.690
greater than the training sample size we stopped OK.

68
00:05:21.700 --> 00:05:23.080
This was this break.

69
00:05:23.110 --> 00:05:32.710
As for right the next thing is that we need to scale our data in two divided by 2 5 5 and then again

70
00:05:32.710 --> 00:05:37.840
we need to have the data flow from the directory as well for all validation data.

71
00:05:38.730 --> 00:05:39.110
OK.

72
00:05:39.190 --> 00:05:44.850
So this is the path whereby again we need to do the same without validation data set.

73
00:05:46.550 --> 00:05:51.080
So with the validation again we need to know how many samples do we have.

74
00:05:51.080 --> 00:05:56.840
Again we create the features the shape is the number of samples that we have.

75
00:05:56.930 --> 00:06:00.530
And then four by four and then five one two.

76
00:06:00.560 --> 00:06:05.420
And we create the number of zeroes based on the label.

77
00:06:05.420 --> 00:06:07.590
How many sample size that we have.

78
00:06:07.820 --> 00:06:14.770
Again with performed the extracting features store the features store the labels and continue until

79
00:06:14.780 --> 00:06:18.160
we've finished all of our samples.

80
00:06:18.190 --> 00:06:18.490
All right.

81
00:06:18.520 --> 00:06:21.640
So the next part here is turning our features.

82
00:06:21.960 --> 00:06:28.130
We all need to reshape it instead of four comma four.

83
00:06:28.150 --> 00:06:28.600
Come on.

84
00:06:28.600 --> 00:06:28.950
Five.

85
00:06:28.960 --> 00:06:30.110
One two.

86
00:06:30.310 --> 00:06:38.320
We are reshaping it to become for training sample size which is four thousand four times four times

87
00:06:38.320 --> 00:06:38.620
five.

88
00:06:38.620 --> 00:06:39.030
One two.

89
00:06:39.030 --> 00:06:40.730
Essentially what what are we doing here.

90
00:06:40.740 --> 00:06:41.960
We're hasty flattening it.

91
00:06:42.480 --> 00:06:50.730
Okay we're reshaping it to become a 2D except that it is 2D but that defers first.

92
00:06:50.780 --> 00:06:55.740
So core value is actually the sample its sample itself.

93
00:06:55.930 --> 00:06:56.350
Okay.

94
00:06:56.630 --> 00:07:00.670
So we do that for the training we do there for the validation as well.

95
00:07:00.820 --> 00:07:01.120
Right.

96
00:07:01.120 --> 00:07:05.600
Come next is to define the so-called classification part now.

97
00:07:05.620 --> 00:07:13.790
So we import the model's optimizes the dens and the drop out and we create our sequential model we pheasant

98
00:07:13.800 --> 00:07:22.960
for Adelaide call dens 5 1 2 which is the input dimension and then we actually apply a densely connected

99
00:07:22.960 --> 00:07:28.150
layer the dense the input layer is basically you notice that is four by four by 5 1 2 is basically is

100
00:07:28.150 --> 00:07:35.350
extra flat and layer and we are applying a density connected layer but with dropout 50 percent.

101
00:07:35.650 --> 00:07:42.910
And finally apply a sigmoid for classification which is a cats and dogs that we're looking at.

102
00:07:43.480 --> 00:07:52.840
And the next one is to familiar compiling our data followed by training our data for 50 epochs and this

103
00:07:52.840 --> 00:07:54.760
is the outcome that we get.

104
00:07:55.230 --> 00:07:55.440
Right.

105
00:07:55.450 --> 00:08:03.060
So if you look at the model in terms of the validation accuracy hover around 87 percent.

106
00:08:03.060 --> 00:08:06.130
So it's range bound from 85 to 90.

107
00:08:06.130 --> 00:08:14.380
And these so-called loss X start to pick up from 20 onwards and in fact it didn't improve from up to

108
00:08:14.380 --> 00:08:15.580
10 epochs.

109
00:08:15.580 --> 00:08:21.340
So if you look at this carefully around 10 a box it were already over fitting our data and we should

110
00:08:21.490 --> 00:08:24.800
probably stop turning our model at that point.

111
00:08:24.790 --> 00:08:27.680
There's no need to continue on for another epochs.

112
00:08:27.820 --> 00:08:35.440
But again you can see that this is a really attractive way of using neural network the ingenious way

113
00:08:35.440 --> 00:08:36.730
of using a neural network.

114
00:08:36.730 --> 00:08:44.620
Basically what you've done is actually you have used a CNN to extract the key features and then apply

115
00:08:44.680 --> 00:08:52.880
a slightly different layout Des layout with a dropout with a dense layout for the problem that you are

116
00:08:52.880 --> 00:08:55.450
trying to actually solve here.

117
00:08:55.610 --> 00:08:55.910
Okay.

118
00:08:55.920 --> 00:09:02.330
So I guess the one question that you might have when you look through this V G G network and then later

119
00:09:02.330 --> 00:09:12.520
on you see that I have provided certain shaves such as this one here for comma for comma 5 1 2.

120
00:09:12.590 --> 00:09:18.380
I have a sneaking suspicion that you probably don't know what this is all you'll be asking that question

121
00:09:18.380 --> 00:09:19.010
right throughout.

122
00:09:19.220 --> 00:09:23.150
So let me put you out of your misery and come back to this model.

123
00:09:23.150 --> 00:09:28.280
This is the output of the base model of the V G.G. and our input.

124
00:09:28.310 --> 00:09:29.820
Remember our image size.

125
00:09:29.810 --> 00:09:33.070
We reshape the 128 by 128.

126
00:09:33.080 --> 00:09:43.460
Take a look at the output the output through this V g g sixteen is a height of four with a full and

127
00:09:43.460 --> 00:09:45.670
depth of five 1 2.

128
00:09:45.710 --> 00:09:49.480
And therein lies where the four by four by five.

129
00:09:49.480 --> 00:09:51.200
One two comes from.

130
00:09:51.200 --> 00:10:00.200
So in this situation at the max pulling the block five pull makes pulling to D you have a hydro full

131
00:10:01.290 --> 00:10:04.080
with a full and depth of 5 1 2.

132
00:10:04.080 --> 00:10:07.220
Basically it's kind of like a cube but not quite because it's 5 1 2.

133
00:10:07.220 --> 00:10:09.780
This is not exactly a cube and

134
00:10:12.810 --> 00:10:20.190
what we usually have is that we reshape it and flatten it and essentially flatten it is basically four

135
00:10:20.190 --> 00:10:22.430
times four times five one two.

136
00:10:22.470 --> 00:10:27.920
And that's technically what we did here when we actually reshape it here to four times four times five.

137
00:10:27.920 --> 00:10:29.560
One two.

138
00:10:29.670 --> 00:10:36.000
Of course you don't have to apply these sequential models which is plugging the neural network at the

139
00:10:36.000 --> 00:10:36.920
end of it.

140
00:10:37.320 --> 00:10:43.290
It is quite common and quite normal for people to actually apply logistic regression which is of course

141
00:10:44.010 --> 00:10:52.750
a sigmoid function rather than applying the all of these other techniques that we discussed in this

142
00:10:52.860 --> 00:10:54.130
one way to do it.

143
00:10:54.270 --> 00:10:58.220
You can of course logistic regression as one way.

144
00:10:58.230 --> 00:11:04.500
There's a few on there that you can actually apply to it but I'll let that lead that to you as an exercise

145
00:11:04.500 --> 00:11:06.360
for you to experiment with.

146
00:11:06.420 --> 00:11:09.610
I hope you found this useful and thank you once again for watching.
