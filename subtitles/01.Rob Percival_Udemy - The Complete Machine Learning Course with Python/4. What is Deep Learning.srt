1
00:00:01,050 --> 00:00:01,950
Welcome back everyone.

2
00:00:02,550 --> 00:00:06,120
So let's dig into what exactly is a deep learning.

3
00:00:06,120 --> 00:00:12,600
So in this segment we're going to talk about the architecture of deep learning and what does it actually

4
00:00:12,600 --> 00:00:13,830
look like.

5
00:00:13,830 --> 00:00:19,060
Now some basic fundamentals it is not deep in the sense of knowledge.

6
00:00:19,200 --> 00:00:24,170
The DPA actually refers to the depth of layers.

7
00:00:24,540 --> 00:00:32,310
Each layer is learned basically that each layer learned of a specific representation of the data.

8
00:00:32,310 --> 00:00:38,700
Now I appreciate that the picture that I'm showing here it's little bit not clear.

9
00:00:39,030 --> 00:00:42,980
And if you look at it this is the Alexa net.

10
00:00:43,080 --> 00:00:49,230
It's a computer vision deep learning and neural network architecture.

11
00:00:49,350 --> 00:00:55,800
Let me just walk through the network with you if you have a look at this we start from the left side

12
00:00:57,270 --> 00:00:58,110
from this side.

13
00:00:58,230 --> 00:01:01,450
This is where the input image comes in.

14
00:01:02,060 --> 00:01:02,560
OK.

15
00:01:02,610 --> 00:01:05,010
And followed by this is one layer.

16
00:01:05,640 --> 00:01:09,980
And these are these so-called dimensions of the layer fifty five by fifty.

17
00:01:09,990 --> 00:01:18,000
Don't be too concerned with that for now and then followed by another layer and then another layer by

18
00:01:18,000 --> 00:01:24,570
another layer followed by another layer and then the last few layers are called the dense layer fully

19
00:01:24,570 --> 00:01:26,010
connected dense layer.

20
00:01:26,310 --> 00:01:34,620
So don't be too concerned but those just understand that as this you have the input data and then you

21
00:01:34,620 --> 00:01:45,870
have the output in terms of what the target is and in between what you have is a whole lot of neural

22
00:01:45,870 --> 00:01:52,140
networks that actually learn each layer learn a specific presentation or representation.

23
00:01:52,290 --> 00:01:55,790
I mentioned before this is Layer number five.

24
00:01:56,430 --> 00:02:04,020
And if is first layer then we'll be most likely will be straight edges would be the right line or the

25
00:02:04,050 --> 00:02:08,550
left line and then the horizontal could be the second layer.

26
00:02:08,610 --> 00:02:11,810
And then thirdly you could be the circle and on and on it goes.

27
00:02:11,820 --> 00:02:15,080
So that's specifically what each layer learned.

28
00:02:15,600 --> 00:02:24,480
And each of these layer the depth of the layer actually provides the capacity for the network to learn

29
00:02:24,480 --> 00:02:30,020
more I guess if you think of it in statistics term is called degrees of freedom.

30
00:02:30,120 --> 00:02:37,110
If you think of it as a straightforward equation then instead of y you could do X plus C which is 1

31
00:02:37,170 --> 00:02:45,330
x day one variable then what you have here is many many X as an example not exactly the same but just

32
00:02:45,600 --> 00:02:53,290
just just just as an example to to give you an idea of what each of these are.

33
00:02:53,550 --> 00:02:59,190
Now I appreciate that this is probably still very raw.

34
00:02:59,190 --> 00:03:05,070
And in terms of your understanding but look at this as the architecture for now I'm going to show you

35
00:03:05,100 --> 00:03:07,250
another one right now.

36
00:03:07,290 --> 00:03:11,140
This is another computer vision layer.

37
00:03:11,160 --> 00:03:13,120
It's called BCG 16.

38
00:03:13,140 --> 00:03:17,580
This is an example of again another neural network architecture specifically.

39
00:03:17,640 --> 00:03:21,960
This is for again for convolution all neural network.

40
00:03:21,960 --> 00:03:27,720
If you look at this what you have is that you have the input layer where the raw data is being fed in

41
00:03:28,170 --> 00:03:35,670
and what you have then is sixty four block one block one has a convolution layer one second convolution

42
00:03:35,670 --> 00:03:37,640
there and then pulley.

43
00:03:37,710 --> 00:03:44,220
So this layer here consists of three three guest layer.

44
00:03:44,220 --> 00:03:47,100
You can call it this block consists of three layers.

45
00:03:47,100 --> 00:03:54,150
And then this is followed by another block which has two convolution or layer in a mix pulling and then

46
00:03:54,150 --> 00:03:57,570
followed by block three and block four and then block five.

47
00:03:57,840 --> 00:03:59,650
And on and on it goes.

48
00:04:00,000 --> 00:04:06,750
What is not shown here are the last three layers of fully connected a dense layer.

49
00:04:06,750 --> 00:04:14,160
This may not be three layers of fully connected dense layer but the basic idea is that the before the

50
00:04:14,160 --> 00:04:21,630
output there must be fully connected layer of to actually finish off the second neural network.

51
00:04:21,870 --> 00:04:27,340
And that's really what I wanted to show you in terms of the deep learning part of it.

52
00:04:27,390 --> 00:04:34,320
This is the architecture of what a deep learning neural network looks like and I hope that it gives

53
00:04:34,320 --> 00:04:36,900
you at least if you haven't seen it before.

54
00:04:36,900 --> 00:04:39,330
This is just a start of what it looks like.

55
00:04:39,360 --> 00:04:45,170
I hope that it helps you to actually at least have a better grasp of it.

56
00:04:45,210 --> 00:04:51,500
Now let me illustrate a little bit more of the architecture of what a neural network looks like.

57
00:04:51,660 --> 00:05:01,350
And quite often this is a toy example whereby of the architect of what a neural network looks like and

58
00:05:01,350 --> 00:05:07,950
it's not too different from the real world whereby what I've shown you earlier is a much more complex

59
00:05:08,070 --> 00:05:10,040
and more state of the art.

60
00:05:10,080 --> 00:05:16,320
In fact this is about three years on the state of the art neural network but in its infancy This is

61
00:05:16,320 --> 00:05:20,490
what it is a representation of what a neural net what looks like.

62
00:05:20,490 --> 00:05:21,900
So you have the input.

63
00:05:21,900 --> 00:05:23,500
These are your input.

64
00:05:23,690 --> 00:05:28,110
It could be in the form of your raw data sample data.

65
00:05:28,110 --> 00:05:32,070
And what is this is that is being fed into what's called an input layer.

66
00:05:32,310 --> 00:05:35,920
So these are the three layers that is actually facing the world.

67
00:05:36,090 --> 00:05:42,390
So you have these three layers and noticed that each of these are connected to the next layer and these

68
00:05:42,720 --> 00:05:50,790
one two three layers here are core the hidden layers these layers are hidden for a dozen next to interact

69
00:05:50,790 --> 00:05:52,680
with the outside world.

70
00:05:52,980 --> 00:06:00,570
I guess you can actually say that the data is actually being fed input in directly into the input layers.

71
00:06:00,570 --> 00:06:06,240
Now when you look at this it kind of reminds you of our brain.

72
00:06:06,450 --> 00:06:16,110
In fact the idea of neural network is that it is inspired from biology or from our brain or brain cells.

73
00:06:16,110 --> 00:06:21,410
Now if you look at it again this is the input features or the feature vector.

74
00:06:21,480 --> 00:06:26,520
If you're looking at lots of data is that it's coming in also called features.

75
00:06:26,520 --> 00:06:32,940
It goes through the input layer which directly feed into three hidden layers before it comes to the

76
00:06:33,240 --> 00:06:42,930
output layers and finally the output is the predictions that it actually that the neural network will

77
00:06:42,930 --> 00:06:45,150
actually generate for you.

78
00:06:45,300 --> 00:06:47,280
And that's generally the basic idea

79
00:06:52,450 --> 00:06:55,900
the basic idea of what a neural network is.

80
00:06:55,900 --> 00:06:58,590
So let me actually illustrate a little bit more.

81
00:06:58,810 --> 00:07:06,610
I think it's worthwhile to spend a little bit of time to understand how a neural network actually work.

82
00:07:06,970 --> 00:07:18,670
So let me start with the the bottom one which is directly from I mean just put it up directly from Stanford.

83
00:07:18,710 --> 00:07:26,570
These are Stanford class on convolution or neural networks for visual recognition or computer vision

84
00:07:26,600 --> 00:07:27,730
that I mentioned earlier.

85
00:07:28,250 --> 00:07:29,180
If you look at this.

86
00:07:29,210 --> 00:07:31,330
This is what I wanted to show you.

87
00:07:31,550 --> 00:07:38,440
And what they have very kindly and nicely shown here is the future which is also the layer itself.

88
00:07:38,510 --> 00:07:39,680
This is the second layer.

89
00:07:39,710 --> 00:07:45,890
If you look at it of this is Steve from Alex net and you can see that it actually is.

90
00:07:45,890 --> 00:07:47,500
Well they have the edges.

91
00:07:47,510 --> 00:07:52,310
It's kind of like horizontal or vertical as fairly smooth.

92
00:07:52,370 --> 00:07:52,640
All right.

93
00:07:52,790 --> 00:08:02,540
So let's look at the next one and we're going to go into this 2D convolution all network visualizations

94
00:08:02,540 --> 00:08:07,120
by Adam Holly and this is really quite something else.

95
00:08:07,230 --> 00:08:15,880
Okay let's wait for this to run now what he's done here is that this is a two layer neural network and

96
00:08:16,360 --> 00:08:18,130
it's asking you to draw the number here.

97
00:08:18,130 --> 00:08:21,610
So let's just draw this and I'm going to draw a four.

98
00:08:22,300 --> 00:08:23,230
OK.

99
00:08:23,590 --> 00:08:23,960
Right.

100
00:08:23,980 --> 00:08:24,980
What do you see here.

101
00:08:25,000 --> 00:08:28,120
What you can see here is that this is the input.

102
00:08:28,120 --> 00:08:30,240
This is the next layer.

103
00:08:30,270 --> 00:08:30,580
All right.

104
00:08:30,580 --> 00:08:32,230
It fits into this next layer.

105
00:08:32,680 --> 00:08:39,420
And it actually each and every one of these tells you what is actually going on and how it actually

106
00:08:39,490 --> 00:08:43,110
each of these pixel relate to which part.

107
00:08:43,390 --> 00:08:44,760
And it shows you.

108
00:08:44,800 --> 00:08:46,110
OK from here.

109
00:08:46,210 --> 00:08:48,210
Notice that it links to that layer.

110
00:08:48,280 --> 00:08:50,620
And notice these are the edges.

111
00:08:50,620 --> 00:08:54,370
And it goes on and on finally becomes a single layer.

112
00:08:54,460 --> 00:08:58,910
And then this is the fully connected layer and finally the prediction is for.

113
00:08:58,990 --> 00:09:06,690
And notice that this is quite remarkable and of course from the perspective of a phone from a person

114
00:09:06,680 --> 00:09:12,640
who is learning that this is quite remarkable because it's really easy to understand.

115
00:09:13,090 --> 00:09:16,190
You can actually show the employer or hide it.

116
00:09:16,240 --> 00:09:20,150
You can look at the convolution or layer one hired hide it.

117
00:09:20,320 --> 00:09:24,580
And this is the down sample which is also what's called Max pulling.

118
00:09:24,580 --> 00:09:26,430
And then this layer to.

119
00:09:26,580 --> 00:09:29,270
And this is the down sample also called Max pool.

120
00:09:29,290 --> 00:09:33,590
And then we have the fully connected Layer 1 fully connected Layer 2.

121
00:09:33,610 --> 00:09:40,810
And finally this is the output itself and and I think I really encourage you to play with it.

122
00:09:40,840 --> 00:09:48,580
It really helps to develop a lot of intuition and to see how and how it actually works from from the

123
00:09:49,620 --> 00:09:57,030
picture that I showed you earlier this is three simple layer has three nodes whereas for this you can

124
00:09:57,030 --> 00:10:00,070
see that it has one two three four five six nodes.

125
00:10:00,610 --> 00:10:01,360
OK.

126
00:10:01,380 --> 00:10:10,200
And from there to the next pool layer it has one to one but going to the next one it actually comes

127
00:10:10,200 --> 00:10:13,020
from multiple sources.

128
00:10:13,110 --> 00:10:13,430
OK.

129
00:10:13,440 --> 00:10:16,960
As you can actually see here right.

130
00:10:17,300 --> 00:10:18,790
Let's look at the next.

131
00:10:18,890 --> 00:10:25,670
And I also want to encourage you to play with this 10 sibling playground that's provided for by

132
00:10:28,510 --> 00:10:33,660
Google OK so if you look at this there are a few things.

133
00:10:34,060 --> 00:10:39,950
Now let's start with something extremely simple such as this.

134
00:10:39,970 --> 00:10:43,010
So it's a bit on the small side.

135
00:10:43,180 --> 00:10:45,570
So let's just start with OK.

136
00:10:45,730 --> 00:10:46,630
No hidden layer.

137
00:10:47,300 --> 00:10:47,620
OK.

138
00:10:47,650 --> 00:10:54,350
So in this case it can actually separate out the MI.

139
00:10:54,520 --> 00:10:55,760
We have zero.

140
00:10:56,110 --> 00:10:56,490
OK.

141
00:10:56,700 --> 00:10:59,480
We have two features OK.

142
00:10:59,510 --> 00:11:01,100
Notice that is a straight line.

143
00:11:01,100 --> 00:11:03,080
That's a vertical straight line for this.

144
00:11:03,080 --> 00:11:05,690
And this is a horizontal straight line.

145
00:11:05,690 --> 00:11:11,960
And you can see that the neural network was able to actually learn the segregation line separating the

146
00:11:11,960 --> 00:11:13,780
orange from the blue.

147
00:11:13,850 --> 00:11:14,690
So let us

148
00:11:17,660 --> 00:11:18,350
train this.

149
00:11:18,350 --> 00:11:19,130
We haven't trained it.

150
00:11:19,140 --> 00:11:22,320
There was just randomly generated let's just train this.

151
00:11:22,580 --> 00:11:23,420
OK.

152
00:11:23,450 --> 00:11:29,720
And after a few hundred epochs epochs is mostly means that it takes D.C. all of the data.

153
00:11:30,170 --> 00:11:30,480
OK.

154
00:11:30,560 --> 00:11:32,650
It sees all of the data.

155
00:11:32,660 --> 00:11:35,930
How many times or now it has in the same data seven hundred times.

156
00:11:35,930 --> 00:11:37,350
Let's just pause it.

157
00:11:37,350 --> 00:11:37,910
OK.

158
00:11:38,000 --> 00:11:43,810
And you can see that the learning rate is point zero three using this activation function called tangent

159
00:11:43,880 --> 00:11:44,900
hedge.

160
00:11:44,900 --> 00:11:50,780
And if you look at the Test loss and the training lost there's absolutely none whatsoever.

161
00:11:50,780 --> 00:11:59,000
If you think about loss as errors then you can understand that hey what we have is a scenario whereby

162
00:12:00,260 --> 00:12:09,320
this model by say we have two features here X1 and X2 and is able to actually predict the two classes

163
00:12:09,440 --> 00:12:15,440
of blobs or clusters very easily the blue one from the orange one.

164
00:12:15,800 --> 00:12:16,040
OK.

165
00:12:16,040 --> 00:12:18,720
So we can see that actually well learn that really well.

166
00:12:19,190 --> 00:12:24,630
Okay let's move on to regenerate another example.

167
00:12:24,730 --> 00:12:26,430
They're all in the same spot.

168
00:12:26,450 --> 00:12:28,800
Let me add a lot of noise.

169
00:12:29,580 --> 00:12:29,990
Yeah.

170
00:12:30,030 --> 00:12:34,090
Let that regenerate okay and restart the training again.

171
00:12:34,130 --> 00:12:41,300
Notice that again it's quickly learning the test losses down to zero the training loss is pretty low

172
00:12:41,300 --> 00:12:42,530
as well.

173
00:12:42,560 --> 00:12:44,820
And again let's pause this.

174
00:12:44,930 --> 00:12:52,130
It quickly learns the pattern and it finds the best line to actually segregate the two class.

175
00:12:52,430 --> 00:12:56,900
Right now you say OK that's straight straightforward is a simple pattern.

176
00:12:56,910 --> 00:12:58,210
It should be able to pick that out.

177
00:12:58,220 --> 00:12:59,740
So let's have a look at this.

178
00:12:59,750 --> 00:13:01,620
Now we have four.

179
00:13:01,830 --> 00:13:07,670
I can't you can't possibly draw a straight line to separate out the blue one from the orange one because

180
00:13:08,120 --> 00:13:14,090
no matter just convince yourself no matter how you draw one line you are not going to be able to segregate

181
00:13:14,330 --> 00:13:14,750
the two.

182
00:13:14,750 --> 00:13:21,380
So these two features enough let's run all the machine learning model and noticed that it's not getting

183
00:13:21,380 --> 00:13:22,030
any lower.

184
00:13:22,040 --> 00:13:28,680
It is just struggling and the losses are really high at point five and point five for both the test

185
00:13:28,700 --> 00:13:29,300
and train.

186
00:13:29,300 --> 00:13:29,630
OK.

187
00:13:29,660 --> 00:13:30,840
This is not working.

188
00:13:30,920 --> 00:13:36,090
Just basically having two features one vertical one one horizontal one is not working.

189
00:13:36,110 --> 00:13:44,090
So let's just add one more extra to power two and run this and see Is it working.

190
00:13:44,090 --> 00:13:44,730
No.

191
00:13:44,780 --> 00:13:45,160
OK.

192
00:13:45,170 --> 00:13:46,400
Again it's not working.

193
00:13:46,400 --> 00:13:49,290
Let's add another feature extra the power too.

194
00:13:49,520 --> 00:13:50,030
OK.

195
00:13:50,030 --> 00:13:53,360
It's trying very very hard.

196
00:13:54,530 --> 00:13:58,340
This is part of the thing that I really want to encourage you to do.

197
00:13:58,370 --> 00:13:59,570
Try this.

198
00:13:59,570 --> 00:14:07,270
Play around with it to see if you can actually get an intuition and understanding all of this.

199
00:14:07,280 --> 00:14:12,700
So again not working very well at least at another features x1 x2 8.

200
00:14:12,710 --> 00:14:14,800
This is looking better.

201
00:14:14,810 --> 00:14:20,670
They're looking much much better and noticed that the losses are now down to 1 percent.

202
00:14:20,940 --> 00:14:22,070
OK so all right.

203
00:14:22,070 --> 00:14:29,450
So I mean these are the traditional way of what's called feature engineering.

204
00:14:29,450 --> 00:14:36,130
Now via feature engineering providing different kind of features it's able to actually predict the data.

205
00:14:36,230 --> 00:14:39,280
So far what we've done is nothing deep about this.

206
00:14:39,560 --> 00:14:45,790
It's just very one layer of of or predicted there's nothing deep about this.

207
00:14:45,800 --> 00:14:50,090
So let's move on to something like this Circle K we have all of these.

208
00:14:50,090 --> 00:14:52,680
Let's see if it actually can learn right.

209
00:14:52,690 --> 00:14:59,000
Seems is actually very quickly be able to learn that pattern pretty well which is really good because

210
00:14:59,150 --> 00:15:02,260
we have five features it seems to be able to do that.

211
00:15:02,270 --> 00:15:04,890
Now what if I add a hidden layer.

212
00:15:05,280 --> 00:15:05,540
OK.

213
00:15:05,570 --> 00:15:08,600
Just two neurons to see if it actually can improve that.

214
00:15:08,610 --> 00:15:12,050
Remember the lowest earlier was zero point zero five.

215
00:15:12,640 --> 00:15:13,350
OK.

216
00:15:13,400 --> 00:15:16,870
And it seems to now hasn't done that well.

217
00:15:16,910 --> 00:15:17,320
OK.

218
00:15:17,330 --> 00:15:20,540
It seems to getting lower might need more.

219
00:15:20,570 --> 00:15:25,790
Ken maybe we'll add a few more hidden layer to see if it actually can lend the pattern better.

220
00:15:26,090 --> 00:15:32,660
And with that you can see that it's starting to drop let's see how it actually improve.

221
00:15:32,800 --> 00:15:34,650
It's not much better than before.

222
00:15:34,660 --> 00:15:43,440
So what we do is it maybe will add to hidden layer and three neurons five new rules for it and see what

223
00:15:43,440 --> 00:15:44,220
happened.

224
00:15:44,250 --> 00:15:44,530
OK.

225
00:15:44,550 --> 00:15:47,010
Now you can see that it actually is starting to drop.

226
00:15:47,010 --> 00:15:49,850
It seems to have a better capacity to learn.

227
00:15:49,890 --> 00:15:53,160
Still though the test loss is actually a little bit on the high side.

228
00:15:54,520 --> 00:15:55,870
In fact it's actually climbing.

229
00:15:55,860 --> 00:15:57,940
Now the more that we train.

230
00:15:57,940 --> 00:15:58,240
OK.

231
00:15:58,270 --> 00:16:01,450
So let's just do this and so.

232
00:16:01,720 --> 00:16:04,330
And again this is the part that I want to highlight.

233
00:16:04,360 --> 00:16:08,350
This is the part of neural network and deep learning.

234
00:16:08,350 --> 00:16:10,110
You do have to play around with it.

235
00:16:10,120 --> 00:16:12,350
It's not straightforward science here.

236
00:16:12,370 --> 00:16:17,080
There's a bit of tinkering play around with it to see if it actually works really well.

237
00:16:17,470 --> 00:16:22,570
And you might have to stop at some point to stop it from continue to learn because this is seems to

238
00:16:22,570 --> 00:16:23,870
start to over fit now.

239
00:16:24,310 --> 00:16:24,630
OK.

240
00:16:24,640 --> 00:16:27,580
And it doesn't seem to do so well in this.

241
00:16:27,580 --> 00:16:32,910
So maybe we actually try and really do and we'll try it is classification.

242
00:16:32,950 --> 00:16:33,190
OK.

243
00:16:33,190 --> 00:16:34,270
We're going to leave that one out.

244
00:16:34,320 --> 00:16:36,060
And let's try this.

245
00:16:36,100 --> 00:16:36,360
OK.

246
00:16:36,370 --> 00:16:41,950
So once I change Rallo the Test loss and the training lost job immediately.

247
00:16:41,950 --> 00:16:42,170
OK.

248
00:16:42,190 --> 00:16:44,190
It was hovering around 8 percent earlier.

249
00:16:44,200 --> 00:16:46,930
That dropped immediately and now it's starting to climb again.

250
00:16:46,930 --> 00:16:53,460
So we're starting to go into the overland territory so final pattern is this.

251
00:16:53,890 --> 00:16:55,990
I know that as you watch me play around with this.

252
00:16:55,990 --> 00:16:57,070
It looks really exciting.

253
00:16:57,070 --> 00:17:02,750
But trust me and this is much more fun as you play with it than me playing with it.

254
00:17:02,830 --> 00:17:08,080
You will actually get the intuition much much faster by playing with it yourself.

255
00:17:08,080 --> 00:17:14,050
So for this it's really well it seems to be improving but it seems to also struggle so we're going to

256
00:17:14,050 --> 00:17:17,530
add a few more layers to help it along.

257
00:17:17,880 --> 00:17:18,260
OK.

258
00:17:18,280 --> 00:17:20,920
Let's see if this many layers can actually help.

259
00:17:21,700 --> 00:17:22,020
OK.

260
00:17:22,030 --> 00:17:23,960
So let's see.

261
00:17:23,980 --> 00:17:25,240
So it might take a while

262
00:17:30,110 --> 00:17:30,550
yeah.

263
00:17:30,890 --> 00:17:31,280
That's right.

264
00:17:31,280 --> 00:17:31,730
There we go.

265
00:17:31,730 --> 00:17:32,910
It's learning the pattern.

266
00:17:33,100 --> 00:17:33,380
OK.

267
00:17:33,380 --> 00:17:40,470
The Test loss has dropped down to 5 percent now is 4 percent let's continue to learn.

268
00:17:40,710 --> 00:17:40,940
OK.

269
00:17:40,980 --> 00:17:46,700
But I think by now you probably get the gist of what I'm trying to actually convey here.

270
00:17:46,740 --> 00:17:48,090
You do need to actually.

271
00:17:48,250 --> 00:17:49,680
Well a couple of things that were done.

272
00:17:49,710 --> 00:17:50,880
Let's summarize.

273
00:17:50,880 --> 00:17:52,710
We have the input data.

274
00:17:52,800 --> 00:17:54,580
We have the features.

275
00:17:54,690 --> 00:17:56,940
We have the many layers.

276
00:17:56,940 --> 00:17:58,580
These are the hidden layers.

277
00:17:58,620 --> 00:18:04,980
And finally we have the output layer and there is the learning rate which will cover later lectures

278
00:18:05,370 --> 00:18:08,760
activation within used his regular lines Asian.

279
00:18:08,760 --> 00:18:09,630
We talk about that.

280
00:18:09,630 --> 00:18:10,980
This is classification.

281
00:18:11,520 --> 00:18:17,090
But again I want to stress that please spend some time to learn this and I'm going to pass this because

282
00:18:17,090 --> 00:18:19,640
it's probably getting a bit of a distraction for you.

283
00:18:20,310 --> 00:18:25,570
Now these are the few things that I wanted to highlight and want to actually demo to you.

284
00:18:25,590 --> 00:18:32,700
They are ready to is out there for you to help you to develop the intuition and really that is what

285
00:18:32,700 --> 00:18:34,320
deep learning is about.

286
00:18:34,320 --> 00:18:36,810
I showed you what Alex net looks like.

287
00:18:36,840 --> 00:18:42,750
This is the video G 16 which is 16 layers and this is what a neural network looks like.

288
00:18:42,750 --> 00:18:46,590
And finally we stop at the neural network visualization.

289
00:18:46,590 --> 00:18:48,450
I hope that has been is useful.

290
00:18:48,450 --> 00:18:53,130
But don't just listen to me talking go and play with it.

291
00:18:53,130 --> 00:18:54,270
You can't break it.

292
00:18:54,270 --> 00:18:57,060
All of these I have the link provided for you.

293
00:18:57,060 --> 00:19:00,350
This was the best way for you to develop intuition.

294
00:19:00,360 --> 00:19:01,770
Thank you once again for watching.

295
00:19:01,770 --> 00:19:04,160
I look forward to seeing you in the next video.
