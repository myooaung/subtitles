WEBVTT
1
00:00:01.060 --> 00:00:02.320
Welcome back everyone.

2
00:00:02.410 --> 00:00:05.500
So let's review a little bit of what we've done so far.

3
00:00:05.560 --> 00:00:13.270
We've shown you how to get the data from Cagle and I've shown you how to train your own CNN and the

4
00:00:13.290 --> 00:00:15.570
code is provided in this file.

5
00:00:15.690 --> 00:00:18.000
Cats underscore and underscore.

6
00:00:18.000 --> 00:00:23.300
Dogs though I Python notebook follow by how you actually save it.

7
00:00:23.320 --> 00:00:29.620
That's the last few line of codes and cats and dogs and also how do you load your model.

8
00:00:29.680 --> 00:00:32.540
So let's extend a little bit further.

9
00:00:32.750 --> 00:00:40.480
We've built some really fundamental understanding of how to train your CNN but let's go one step further

10
00:00:41.260 --> 00:00:46.930
one step further is what happens if I don't have enough images.

11
00:00:46.960 --> 00:00:48.330
That's the first question.

12
00:00:48.340 --> 00:00:56.830
The second one is I've got this many images can I actually make or extract more out of it.

13
00:00:56.830 --> 00:00:59.840
The answer is yes you can indeed.

14
00:00:59.950 --> 00:01:03.850
This technique is called data augmentation.

15
00:01:03.850 --> 00:01:10.930
Basically instead of just relying on the only data source that we have which is what we have just now

16
00:01:10.930 --> 00:01:18.580
which is two thousand you can actually create more data from the existing images that you have the technique

17
00:01:18.610 --> 00:01:26.590
that we're going to talk about his core data augmentation is under the image data generator and I have

18
00:01:26.590 --> 00:01:33.940
code that I'm going to show you in a minute but essentially the basic idea here is that you can actually

19
00:01:33.940 --> 00:01:39.250
provide you can perform all of these so-called transformation within.

20
00:01:39.310 --> 00:01:45.820
So let's just start with the so-called documentation if you have a look at this.

21
00:01:45.880 --> 00:01:51.550
It actually provide a few or rather fairly extensive explanation.

22
00:01:51.670 --> 00:01:57.460
Basically you can actually perform from a start feature wise on the score center.

23
00:01:57.460 --> 00:01:58.860
This is a boolean.

24
00:01:58.870 --> 00:01:59.130
OK.

25
00:01:59.140 --> 00:02:02.460
So zero means no one means yes.

26
00:02:02.560 --> 00:02:09.190
And then you have what this does is it is set input mean to zero over the data set feature wise and

27
00:02:09.190 --> 00:02:15.270
then on and on it goes to some of the specific one that I want to talk about is rotation range.

28
00:02:15.270 --> 00:02:15.580
All right.

29
00:02:15.580 --> 00:02:18.310
Degree range for random rotations.

30
00:02:18.370 --> 00:02:25.120
So this rotation basically means that you rotated 10 degrees 20 degrees all the other way minus 10 minus

31
00:02:25.120 --> 00:02:25.770
20.

32
00:02:25.780 --> 00:02:27.720
You can set the range yourself.

33
00:02:27.970 --> 00:02:32.020
The next one is the width shift range.

34
00:02:32.020 --> 00:02:35.530
What this is doing is that it shift the images right.

35
00:02:35.650 --> 00:02:41.590
And the flow here a fraction of total with less than 1 or pixels if it's graded than 1.

36
00:02:41.590 --> 00:02:44.500
So it allows you to shift the images level right.

37
00:02:44.520 --> 00:02:44.750
OK.

38
00:02:44.770 --> 00:02:52.220
So that's the with shifting You can also perform the height shifting upward down or if it's less than

39
00:02:52.220 --> 00:02:57.920
1 then as a fraction of his more than 1 is how many pixels you under shift and then you follow by sheer

40
00:02:57.920 --> 00:03:04.440
range the sheer intensity so the sheer angle in counterclockwise directions in degrees.

41
00:03:04.460 --> 00:03:06.210
So let me just show you this.

42
00:03:06.220 --> 00:03:11.370
Let me just Google OK.

43
00:03:11.410 --> 00:03:15.960
So image Cher ring OK.

44
00:03:15.980 --> 00:03:19.910
This is what image sharing actually means so let's borrow this

45
00:03:23.540 --> 00:03:23.840
OK.

46
00:03:23.850 --> 00:03:26.310
You can see this are the examples of sharing.

47
00:03:26.310 --> 00:03:30.340
So you can like instead of a cube you share it.

48
00:03:30.360 --> 00:03:34.080
This is kind of like I don't know what you call it x2 trapezoid.

49
00:03:34.350 --> 00:03:37.970
And then this is a more example of sharing.

50
00:03:38.280 --> 00:03:38.540
OK.

51
00:03:38.550 --> 00:03:43.850
So that's really the basic idea of sharing of images.

52
00:03:44.010 --> 00:03:50.280
Again what we're doing here is really simple transformation so that the image is not the same as what

53
00:03:50.280 --> 00:03:51.900
you have for a generally.

54
00:03:51.900 --> 00:03:56.770
And allows the CNN to learn all of these from the CNN perspective.

55
00:03:56.790 --> 00:03:58.380
These are just new data set.

56
00:03:58.380 --> 00:04:05.190
And indeed it is because the angle is a bit different or spin should or been zoom in or the pixel has

57
00:04:05.190 --> 00:04:06.450
been shift up or down.

58
00:04:06.720 --> 00:04:14.010
What you really want to do using these data augmentation technique is to actually forces the CNN to

59
00:04:14.010 --> 00:04:21.490
learn the general feature rather than learn pixel by pixel or memorizing the pixel by pixel.

60
00:04:21.800 --> 00:04:27.330
There's two more that I want to mention horizontal flip and also vertical flip basically flipping the

61
00:04:27.330 --> 00:04:31.600
image either horizontally or vertically.

62
00:04:32.070 --> 00:04:32.280
Right.

63
00:04:32.310 --> 00:04:34.500
So those are the few that I want to touch on.

64
00:04:34.500 --> 00:04:38.250
So now let's go into our codes within Jupiter lab.

65
00:04:38.280 --> 00:04:38.550
All right.

66
00:04:38.550 --> 00:04:43.470
So we start with this is our original cats and dogs.

67
00:04:43.470 --> 00:04:43.960
OK.

68
00:04:43.980 --> 00:04:51.300
And I also want to cover the other part which is the data augmentation.

69
00:04:51.360 --> 00:04:57.410
Now before we actually go into the data augmentation I've done a little bit more here in terms of the

70
00:04:57.550 --> 00:04:58.990
the code itself.

71
00:04:59.040 --> 00:05:00.800
Let me just highlight the difference.

72
00:05:00.930 --> 00:05:02.300
All of these are the same.

73
00:05:02.310 --> 00:05:02.560
OK.

74
00:05:02.600 --> 00:05:14.380
So importing of data to source data these are all exactly the same unchanged at OK so processing is

75
00:05:14.380 --> 00:05:18.280
the same in terms of the CNN architecture is exactly the same.

76
00:05:18.280 --> 00:05:19.380
What then is different.

77
00:05:19.690 --> 00:05:22.230
OK I want to highlight to you this part here.

78
00:05:22.420 --> 00:05:22.720
Right.

79
00:05:22.750 --> 00:05:29.030
So if you look at this this is the portion that's different you will remember when we were looking at

80
00:05:29.030 --> 00:05:40.580
the image data generated previously we don't have this portion there's no rotation range there's no

81
00:05:40.610 --> 00:05:48.330
with shift range there's no height shift range there's no sheer range zoom range horizontal flipping.

82
00:05:48.500 --> 00:05:53.300
Now this is application of data augmentation in reality now.

83
00:05:53.390 --> 00:05:55.920
So everything else is exactly the same.

84
00:05:56.000 --> 00:06:04.500
And the only difference are these you can say six line of code Yep six line of codes.

85
00:06:04.550 --> 00:06:11.690
By the way just one word a warning you don't ever perform data augmentation with your validation data

86
00:06:11.930 --> 00:06:12.630
you don't.

87
00:06:12.650 --> 00:06:13.910
OK.

88
00:06:14.000 --> 00:06:18.120
The only with the training data because you won the CNN to learn.

89
00:06:18.380 --> 00:06:21.830
So if you look at this these are all that were changed.

90
00:06:21.860 --> 00:06:23.700
Nothing else changed.

91
00:06:23.810 --> 00:06:28.420
The difference now also is that we're running hundreds epochs instead of 30.

92
00:06:28.420 --> 00:06:32.690
I guess I want to run a longer epochs to illustrate to you what actually happened.

93
00:06:33.030 --> 00:06:37.880
OK so let's go down to these so-called performance everything else the same.

94
00:06:37.880 --> 00:06:45.510
So this is the two key chart that we used to to assess the performance of our model.

95
00:06:46.040 --> 00:06:49.100
If we look carefully What do you see.

96
00:06:49.280 --> 00:06:54.230
Let me just put side by side the previous one which is 1 or 1.

97
00:06:54.230 --> 00:06:55.130
Cats and dogs

98
00:06:57.940 --> 00:07:01.080
killing me stick it side by side so you can actually see.

99
00:07:01.770 --> 00:07:02.170
OK.

100
00:07:02.370 --> 00:07:04.430
Let's go all the way to the bottom.

101
00:07:04.530 --> 00:07:13.050
Notice that previously the training and trade the validation accuracy plateau and doesn't extend beyond

102
00:07:13.080 --> 00:07:14.350
80 percent.

103
00:07:14.580 --> 00:07:21.870
And if you compare this to the training and validation accuracy I noticed that the validation continues

104
00:07:21.870 --> 00:07:25.090
to climb even at 100 epoch.

105
00:07:25.230 --> 00:07:33.490
It continues to climb and the actual accuracy is actually going up in tandem with our training data.

106
00:07:33.810 --> 00:07:43.170
And this is the key important lesson here is that did data augmentation forces to CNN to learn general

107
00:07:43.170 --> 00:07:50.400
features to actually focus on some of the key characteristics of the image so that it actually can generalize.

108
00:07:50.400 --> 00:07:56.640
In fact you'll notice that the green is exactly which is the validation it's generally higher than the

109
00:07:56.640 --> 00:07:58.350
training data.

110
00:07:58.350 --> 00:08:04.980
All right so the validation data or the performance accuracy is generally better than the training accuracy

111
00:08:05.220 --> 00:08:10.800
and that's really what you want to see is that this is not an example of a fitting neither is an example

112
00:08:10.800 --> 00:08:11.730
of under fitting.

113
00:08:11.730 --> 00:08:14.830
It just shows you that there are more room to grow.

114
00:08:14.880 --> 00:08:20.970
Likewise with the loss is exactly the same conclusion the validation lost continue to draw and the blue

115
00:08:20.970 --> 00:08:24.870
dot which is the training loss continue to drop they are going in tandem.

116
00:08:24.870 --> 00:08:27.920
So this is not a sign of over fitting at all.

117
00:08:28.000 --> 00:08:29.560
And this is encouraging.

118
00:08:29.730 --> 00:08:39.690
Reason for data augmentation is to do precisely that so that our data or our model can continue to learn

119
00:08:39.840 --> 00:08:44.640
the general features and perform better out of sample.

120
00:08:45.060 --> 00:08:45.360
OK.

121
00:08:45.390 --> 00:08:53.130
So all right I'm gonna stop there and the next video I'm going to show you some examples of WoW of images

122
00:08:53.130 --> 00:08:59.820
of data images that have been augmented meaning with perform data augmentation so that you can get a

123
00:09:00.090 --> 00:09:02.580
general sense of what what it has done.
