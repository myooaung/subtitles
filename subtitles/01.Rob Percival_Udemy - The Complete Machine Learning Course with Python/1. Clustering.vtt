WEBVTT
1
00:00:00.390 --> 00:00:01.140
Welcome back everyone.

2
00:00:01.140 --> 00:00:06.070
Let's continue without him on unsupervised learning and the specific topic.

3
00:00:06.150 --> 00:00:08.640
We're going to look at is clustering.

4
00:00:08.640 --> 00:00:11.260
What exactly is clustering.

5
00:00:11.880 --> 00:00:18.910
It's really the approach of grouping a set of objects in such a way that they are in the same group.

6
00:00:18.920 --> 00:00:20.280
And this is core class to

7
00:00:23.590 --> 00:00:27.820
in some way the purpose of it is that they have similar characteristics.

8
00:00:27.900 --> 00:00:28.180
OK.

9
00:00:28.210 --> 00:00:29.730
So that's really the basic idea.

10
00:00:29.750 --> 00:00:35.020
And we could be I say Is this the task of grouping a set of objects in such a way that objects in the

11
00:00:35.020 --> 00:00:39.990
same group are more similar to each other than those in other groups.

12
00:00:40.000 --> 00:00:43.180
So that's basically what Wikipedia actually highlighted.

13
00:00:43.230 --> 00:00:46.270
There are some examples in terms of application for this.

14
00:00:46.270 --> 00:00:52.200
There is of course the natural language processing and IP clustering them or similar documents together.

15
00:00:52.330 --> 00:00:57.470
Computer vision in terms of identifying cards dogs cars dolphins things are.

16
00:00:57.760 --> 00:01:04.690
Stock markets in terms of clustering similar companies together rather than the traditional labelling

17
00:01:04.690 --> 00:01:08.440
of IT market segmentation or customer segmentation.

18
00:01:08.470 --> 00:01:12.220
So that's really some of the application for clustering.

19
00:01:12.670 --> 00:01:19.150
As I mentioned briefly before Unsupervised Learning really is one that we don't provide it with Target

20
00:01:19.210 --> 00:01:25.690
which is allow the algorithm to identify the underlying patterns and classify them together.

21
00:01:25.720 --> 00:01:28.300
There are a few types of clustering.

22
00:01:28.300 --> 00:01:32.230
There is the connectivity base clustering which is based on distance.

23
00:01:32.410 --> 00:01:35.800
And this one we will talk about which is hierarchical clustering.

24
00:01:35.920 --> 00:01:39.920
There is centroid base clustering which is K Means Algorithm.

25
00:01:39.940 --> 00:01:44.770
These are done by representing each cluster by a single mean vector.

26
00:01:44.770 --> 00:01:46.970
There is also a distribution base clustering.

27
00:01:46.970 --> 00:01:54.420
These are model using statistical distribution using the expectation maximization algorithm E.M. its

28
00:01:54.460 --> 00:01:55.360
call.

29
00:01:55.450 --> 00:02:01.440
There is also a density base clustering which is D.B. scan for these as defined clusters as connected

30
00:02:01.830 --> 00:02:04.970
dense regions in the data space.

31
00:02:05.050 --> 00:02:05.290
All right.

32
00:02:05.290 --> 00:02:12.910
For a start I just want to highlight that we are using the email extent on machine learning extend this

33
00:02:12.910 --> 00:02:16.170
is created by our SBT.

34
00:02:16.180 --> 00:02:25.180
And if you just have a look at the his contact details Sebastian Rathke is the person who created this

35
00:02:25.750 --> 00:02:27.760
extension to the psychic line.

36
00:02:28.360 --> 00:02:28.630
All right.

37
00:02:28.630 --> 00:02:35.080
So it's really use for when it comes to the day to day data side tasks especially in machine learning

38
00:02:35.980 --> 00:02:36.970
tasks that you want to do.

39
00:02:36.970 --> 00:02:41.820
So if you look at this you can actually very quickly see on simple bolt classifier which combines the

40
00:02:41.870 --> 00:02:42.970
F One two three.

41
00:02:42.970 --> 00:02:44.500
Very very quickly.

42
00:02:44.500 --> 00:02:50.760
So it's a really excellent library and package that I strongly encourage you to make use of it.

43
00:02:50.770 --> 00:02:54.570
All right coming back to our Jupiter notebook on collaborate.

44
00:02:54.640 --> 00:03:00.280
The reason that I want to highlight the M.O. extends that because we are using the Parish Library itself

45
00:03:00.340 --> 00:03:06.880
and also somewhat the basic ideas here in terms of the combination of classifiers.

46
00:03:06.880 --> 00:03:11.930
So if you look carefully we have three classifiers we have classifier one logistic regression.

47
00:03:12.250 --> 00:03:18.550
We have classifier to which is random forest classifier and then we have classifier 3 which is a support

48
00:03:18.550 --> 00:03:26.800
vector classifier and we have an ensemble vote classifier which provided here and the weights are look

49
00:03:26.800 --> 00:03:35.250
at it as such two onto logistic regression one and one onto a rainforest as far as SBC.

50
00:03:35.290 --> 00:03:42.910
So in terms of the data that we have we just basically extract from Iris data and we are also performing

51
00:03:43.300 --> 00:03:48.230
the core plotting so you can actually visualize what transpired here.

52
00:03:48.730 --> 00:03:55.360
So in terms of the plotting we are putting in the one two three plus ensemble and big.

53
00:03:55.480 --> 00:03:59.330
This is a for loop so you can have four charts here.

54
00:03:59.360 --> 00:04:07.310
The actual fitting of it as well as plotting of the so called decision regions to allow you to actually

55
00:04:07.450 --> 00:04:10.650
see the various and so core.

56
00:04:12.340 --> 00:04:16.990
How does each of the algorithm separate the different classes.

57
00:04:16.990 --> 00:04:18.080
Okay.

58
00:04:18.280 --> 00:04:18.640
All right.

59
00:04:18.670 --> 00:04:21.390
So let's dive into this.

60
00:04:21.400 --> 00:04:24.750
So you have logistic regression images.

61
00:04:24.790 --> 00:04:26.050
Zoom in a little bit.

62
00:04:26.200 --> 00:04:31.270
So you have the logistic regression which are green and then you have the triangle and then you have

63
00:04:31.270 --> 00:04:31.660
the blue.

64
00:04:31.660 --> 00:04:33.430
There are some errors there.

65
00:04:34.000 --> 00:04:34.780
Random Forest.

66
00:04:34.790 --> 00:04:37.690
Notice is not a straight line so it's non-linear in nature.

67
00:04:37.690 --> 00:04:45.640
So the blues are clearly there's a bit of difficulties with the green ensemble is a combination of all

68
00:04:45.640 --> 00:04:46.450
three.

69
00:04:46.680 --> 00:04:50.950
As you can see is non-linear as well.

70
00:04:51.160 --> 00:04:56.560
Random for us it's very zigzag was very similar to a decision tree.

71
00:04:56.830 --> 00:04:58.060
Okay so let's run this

72
00:05:05.100 --> 00:05:05.420
okay.

73
00:05:05.520 --> 00:05:07.770
So that's all sorted.

74
00:05:07.860 --> 00:05:11.740
Don't worry too much about the deprecated module.

75
00:05:11.930 --> 00:05:14.550
It will be updated once there's a refresh come through.

76
00:05:15.180 --> 00:05:22.800
So the lesson based on these is these are what we call control to actually allow you to have some references.

77
00:05:22.800 --> 00:05:28.090
So let's look at what's gone rate of hierarchical clustering.

78
00:05:28.110 --> 00:05:32.910
There are a few types of aggregation or creation of clusters.

79
00:05:32.910 --> 00:05:39.120
You have the growth narrative which is bottom up each observation starts its own cluster and pairs of

80
00:05:39.120 --> 00:05:42.510
classes as much as one moves up to Haruki.

81
00:05:42.510 --> 00:05:49.260
You can also have the divisive which is a top down approach and the observation starts with one classes

82
00:05:49.350 --> 00:05:53.620
and splits are perform recursively as one moves down the hierarchy.

83
00:05:53.730 --> 00:05:59.160
If this sounds like decision tree then yeah maybe that will be the direction that will help you in terms

84
00:05:59.160 --> 00:06:01.700
of our understanding.

85
00:06:01.710 --> 00:06:07.410
So in terms of the step by step you can actually also refer to the notes as provided here.

86
00:06:07.410 --> 00:06:13.770
You can have in terms of the shortest distance which is single linkage distance between two clusters

87
00:06:13.770 --> 00:06:18.750
to be the minimum distance between any single data point the first class there's any single data point

88
00:06:18.750 --> 00:06:19.550
the second class.

89
00:06:19.570 --> 00:06:22.020
So it's based on shortest distance.

90
00:06:22.020 --> 00:06:27.810
The second one is complete linkage for the distance which is the distance between two classes to be

91
00:06:27.810 --> 00:06:32.970
the maximum distance between any single data point in the first class to any single data point the second

92
00:06:32.970 --> 00:06:33.440
cluster.

93
00:06:33.450 --> 00:06:36.650
So first is based on the shortest distance.

94
00:06:36.660 --> 00:06:38.910
Second line is based on the furthest distance.

95
00:06:39.030 --> 00:06:40.360
You can also look at average.

96
00:06:40.380 --> 00:06:45.790
You can use it centroid which is distance between two mean vectors mean being average.

97
00:06:45.960 --> 00:06:53.370
And there's also one more which is what's method which is based on ANOVA based approach and analysis

98
00:06:53.400 --> 00:06:56.730
of variance so it is an iterative process.

99
00:06:56.730 --> 00:07:02.910
What it does is that minimize the total within the cluster variance so it's looking at the overall at

100
00:07:03.060 --> 00:07:09.410
each step the pair of classes with minimum between class the distance are merged right.

101
00:07:09.450 --> 00:07:15.440
So let's look at these step by step where we import the usual library we create blobs.

102
00:07:16.090 --> 00:07:21.520
We store the x y and if we plot it out this is what you can see here.

103
00:07:21.520 --> 00:07:24.530
So there's 1 2 3 4 5 classes clearly.

104
00:07:24.640 --> 00:07:29.640
Now the question is can our hierarchical class strings pick that up.

105
00:07:29.680 --> 00:07:36.040
So to begin with we import this from say pi thought cluster hierarchy and then we're importing what

106
00:07:36.040 --> 00:07:44.170
dental Graham asteroid's linkage and the distance is based on linkage and we are using the award method

107
00:07:44.170 --> 00:07:44.800
here.

108
00:07:44.830 --> 00:07:47.150
So let's start with the dental graph.

109
00:07:47.230 --> 00:07:48.230
Let me run this.

110
00:07:48.250 --> 00:07:55.120
This may take a while because bearing in mind this is based on what distance so it needs to calculate

111
00:07:55.480 --> 00:07:58.660
the every single point versus the other.

112
00:07:59.320 --> 00:08:02.740
So looking at this this is a rather large dental Gram.

113
00:08:02.860 --> 00:08:04.960
It's a little bit hard to visualize.

114
00:08:04.990 --> 00:08:11.140
You start off with here and you split into two and then this further split into two and then continue

115
00:08:11.140 --> 00:08:12.520
to split from there.

116
00:08:13.360 --> 00:08:17.830
Okay technically this is how we will read it.

117
00:08:18.280 --> 00:08:27.070
And the other alternative way to view this is to actually turn it by 90 degrees rather than top down

118
00:08:27.250 --> 00:08:33.730
this way we can easily look at it and horizontally which is just as difficult to look at partially because

119
00:08:34.150 --> 00:08:38.050
well it basically exceeded our screen.

120
00:08:38.140 --> 00:08:46.200
One other technique that we tend to use is to actually truncate the DNA diagram coming back here.

121
00:08:46.300 --> 00:08:47.290
Let me zoom out a little

122
00:08:50.450 --> 00:08:59.750
one thing that you immediately you can hopefully get a sense of this is that look when do we actually

123
00:08:59.750 --> 00:09:01.960
stopped dividing.

124
00:09:01.970 --> 00:09:11.330
Of course at the end degree at the end of it they all individual dots or points on their own working

125
00:09:11.330 --> 00:09:17.340
class that them and then for example this portion here is a cluster on his own.

126
00:09:17.390 --> 00:09:20.390
If you combine them this is a cluster on his own.

127
00:09:20.390 --> 00:09:23.990
If you take these whole lot it's a cluster on its own.

128
00:09:24.440 --> 00:09:27.020
This is a matter of choice really.

129
00:09:27.740 --> 00:09:35.120
And you can either cut it this way which makes it three classes or we can actually cut it this way which

130
00:09:35.120 --> 00:09:36.380
makes it five classes.

131
00:09:36.380 --> 00:09:38.080
What do I mean by cut.

132
00:09:38.240 --> 00:09:47.770
When I say cut record that these are measured by distance Okay I can specify 75.

133
00:09:48.100 --> 00:09:48.960
Okay.

134
00:09:49.250 --> 00:09:55.910
If I specify 200 that means I only have one cluster.

135
00:09:55.910 --> 00:10:00.280
If I say okay the distance is one seventy five.

136
00:10:00.650 --> 00:10:00.920
Okay.

137
00:10:00.920 --> 00:10:09.160
Before I separate them anything there is greater than 175 in terms of the absolute.

138
00:10:09.170 --> 00:10:15.920
There's no unit here grade and 175 we split them into two which is what you have here.

139
00:10:15.950 --> 00:10:17.730
Anything that's above 150.

140
00:10:17.750 --> 00:10:19.740
Then we have three.

141
00:10:19.790 --> 00:10:20.230
Okay.

142
00:10:20.280 --> 00:10:23.400
Well you can say 75 and this also three as well.

143
00:10:23.430 --> 00:10:25.610
If it's 25 then we have five.

144
00:10:25.610 --> 00:10:31.340
So there's the middle of choice for you to decide whether you wanted to be split to three to five or

145
00:10:31.340 --> 00:10:32.150
more.

146
00:10:32.150 --> 00:10:39.200
That's totally down to the actual business decision that you have to utilize based on the core context

147
00:10:39.200 --> 00:10:41.140
that you're working on.

148
00:10:41.150 --> 00:10:41.950
All right.

149
00:10:42.560 --> 00:10:49.760
So come this point what we did is that we say I'm going to draw a horizontal line at 25 which is one

150
00:10:49.880 --> 00:10:53.300
which gives us 1 2 3 4 5 clusters.

151
00:10:53.300 --> 00:10:55.800
So this is a class on his own.

152
00:10:55.820 --> 00:10:57.610
This is a castle on his own.

153
00:10:57.620 --> 00:10:59.330
This is a cluster on his own.

154
00:10:59.330 --> 00:11:00.880
This is class on his own.

155
00:11:00.890 --> 00:11:01.810
This is another castle.

156
00:11:01.850 --> 00:11:12.010
So they're giving us five classes to zoom back in and they will give us if we plotted and look at it

157
00:11:12.010 --> 00:11:25.510
this way in terms of truncating it and what you get now is this you will get this much simplified hierarchical

158
00:11:25.660 --> 00:11:26.470
addendum Graham

159
00:11:29.510 --> 00:11:36.770
so truncating it this way makes it easier for you to identify in terms of how many.

160
00:11:37.200 --> 00:11:44.490
Rather than looking at a big hurricane or cluster 10 diagram now truncated is a lot easier to observe

161
00:11:44.490 --> 00:11:45.220
and read as well.

162
00:11:45.600 --> 00:11:47.180
Why is there six years.

163
00:11:47.190 --> 00:11:49.720
Because we specify piece it could do six.

164
00:11:49.740 --> 00:11:52.420
So that gives one two three four five six.

165
00:11:52.460 --> 00:12:00.050
All right you can of course specify a smaller number that would actually change d or core anagram self

166
00:12:00.060 --> 00:12:04.500
so to me I have not plot these run these cells.

167
00:12:04.500 --> 00:12:08.370
Let me run that and let me first plot it with six

168
00:12:14.280 --> 00:12:14.630
okay.

169
00:12:14.640 --> 00:12:16.950
So I think that if I change it to three

170
00:12:20.130 --> 00:12:25.120
they will automatically find where the point is and just cut it for you.

171
00:12:25.240 --> 00:12:28.550
You don't actually have to do all the work or do it all for you.

172
00:12:29.680 --> 00:12:35.320
Okay so that makes it so much easier for you to utilize this functionality.

173
00:12:35.360 --> 00:12:35.860
Great.

174
00:12:35.860 --> 00:12:37.160
The last part then is to.

175
00:12:37.190 --> 00:12:37.890
Okay fine.

176
00:12:37.900 --> 00:12:44.020
We have class to these into intelligence say we've been able to identify five six one two three four

177
00:12:44.020 --> 00:12:44.350
five.

178
00:12:44.350 --> 00:12:45.090
Class those here.

179
00:12:45.100 --> 00:12:51.150
The question then is that given a point how do you know which class does does it belong.

180
00:12:51.160 --> 00:12:58.650
The beauty of using the PSI pi is that you can actually retrieve the class by using the F classed as

181
00:12:58.650 --> 00:12:59.710
function.

182
00:12:59.710 --> 00:13:03.720
You can retrieve by either distance or the number of classes.

183
00:13:03.790 --> 00:13:08.860
So here what we've done is that we specify the maximum distance of 20 Phi.

184
00:13:08.860 --> 00:13:13.990
It will actually be based on the 25 and it will work backwards.

185
00:13:14.140 --> 00:13:18.150
OK so based on 25 we will have one two three four five.

186
00:13:18.550 --> 00:13:21.340
Okay minimum distance of twenty five.

187
00:13:21.610 --> 00:13:27.350
And these are basically the So core clusters or death.

188
00:13:27.400 --> 00:13:30.070
What is things at this point belong to cluster 1.

189
00:13:30.100 --> 00:13:32.510
Point number 3 belong to cluster 5.

190
00:13:32.740 --> 00:13:34.470
So on and so forth.

191
00:13:34.720 --> 00:13:40.490
And what we're doing next is just to plot it out so that you can actually visualize it.

192
00:13:40.510 --> 00:13:41.980
What do you have here.

193
00:13:41.980 --> 00:13:46.180
You have X and you have the 0 and 1.

194
00:13:46.180 --> 00:13:51.450
So there's basically zero on this one there and we actually for the classed as 1 2 3 4 5.

195
00:13:51.490 --> 00:13:58.540
We use the number to denote the casters individual identity.

196
00:13:58.540 --> 00:14:07.120
So because this is based on the color map prism you will have one being one color 2 being on color 3

197
00:14:07.120 --> 00:14:08.380
being color my color.

198
00:14:08.380 --> 00:14:12.450
So on and so forth so hands while you'll get here in the end is actually 5 colors.

199
00:14:14.070 --> 00:14:15.890
K continue on from there.

200
00:14:16.000 --> 00:14:17.730
You what we did earlier.

201
00:14:17.770 --> 00:14:18.740
It's by distance.

202
00:14:20.550 --> 00:14:22.640
This is OK.

203
00:14:22.650 --> 00:14:25.560
We can also separate them by clusters.

204
00:14:25.770 --> 00:14:28.830
OK so we just do it this way.

205
00:14:29.040 --> 00:14:30.900
Now I'm specifying 5.

206
00:14:31.130 --> 00:14:31.700
Okay.

207
00:14:31.710 --> 00:14:39.570
And they're the KS already been specify and what I will do is actually it will automatically calculate

208
00:14:39.570 --> 00:14:42.440
that yourself based on the clusters that we use.

209
00:14:42.720 --> 00:14:49.870
Okay so this is based on K being 5 clusters whereas previously this is based on Max distance that we

210
00:14:49.970 --> 00:14:53.730
provided the criterion being distance vs. here.

211
00:14:53.730 --> 00:14:57.750
The criterion being maximum number of clusters right.

212
00:14:57.770 --> 00:15:01.350
That's pretty much all that I want to cover for this portion here.

213
00:15:01.350 --> 00:15:08.700
We start off we're talking and basically an introduction to MLS extend for you to utilize it for ensemble.

214
00:15:08.730 --> 00:15:16.350
If you want to we can actually also have a lot of useful content for the purpose of clustering as well.

215
00:15:16.380 --> 00:15:19.560
Then we move on to hierarchical cost clustering.

216
00:15:19.560 --> 00:15:29.250
Using them was method and we're sure you how from pure generation of data using map blocks to using

217
00:15:29.310 --> 00:15:35.730
the addendum grand followed by the truncation of the underground and then finally retrieving the clusters

218
00:15:36.390 --> 00:15:37.650
of each of the points.

219
00:15:37.650 --> 00:15:45.570
So that's basically the whole core idea of clustering and using hierarchical clustering to do that.

220
00:15:45.630 --> 00:15:47.090
That's all on a cover for now.

221
00:15:47.100 --> 00:15:49.470
I look forward to coming to seeing you in the next lesson.
