WEBVTT
1
00:00:02.250 --> 00:00:03.830
Welcome back in this lesson.

2
00:00:04.020 --> 00:00:08.450
We're going to cover binary classification.

3
00:00:08.460 --> 00:00:10.350
The previous two lessons were looked at.

4
00:00:10.350 --> 00:00:15.560
Regression based problem where you are predicting continuous variable.

5
00:00:15.580 --> 00:00:22.980
We looked at the classification problem as well as 10 class classification problem which is slightly

6
00:00:22.980 --> 00:00:29.090
different from binary classification problem when it comes to binary classification problem.

7
00:00:29.100 --> 00:00:37.910
We have a two class structure so it's either this or that zero or 1 cats or dogs.

8
00:00:38.160 --> 00:00:45.900
In this case we are going to continue with the amnesty dataset with some slight variation the variation

9
00:00:45.930 --> 00:00:57.060
is that instead of predicting whether it is one of the 10 digits we can convert it into whether the

10
00:00:57.060 --> 00:01:00.750
digit is a 5 or not a 5.

11
00:01:00.960 --> 00:01:10.050
So hence turning it from what the dual core 10 class or multi class classification problem to just a

12
00:01:10.050 --> 00:01:12.150
binary class classification problem.

13
00:01:12.780 --> 00:01:13.950
So let's get started.

14
00:01:13.960 --> 00:01:22.100
We're looking at installing our basic libraries download the data and preparing the data.

15
00:01:22.140 --> 00:01:28.650
The one thing that we do want to HD bring to mind just couple of backend features of Carrasco is that

16
00:01:29.550 --> 00:01:39.000
you need to bear in mind carers by default text on what's called a channel last image data format.

17
00:01:39.000 --> 00:01:44.130
The funny thing about the image war was that there are two types of channel on channel first and channel

18
00:01:44.130 --> 00:01:52.620
loves the Channel 4 as basically means that the first channel is default to have the value come first

19
00:01:52.620 --> 00:01:58.710
and the image rows and columns channel last will have the one comes at the end of it.

20
00:01:59.250 --> 00:02:00.720
By default it is Channel last.

21
00:02:00.750 --> 00:02:03.190
So this is something that we need to keep in mind.

22
00:02:03.360 --> 00:02:05.050
So let's run this.

23
00:02:05.190 --> 00:02:07.090
So we do need to do what's core.

24
00:02:07.290 --> 00:02:09.570
If class.

25
00:02:09.570 --> 00:02:17.290
I should really changed is to last.

26
00:02:18.160 --> 00:02:18.520
OK.

27
00:02:18.540 --> 00:02:20.270
Then this should really be channel last.

28
00:02:20.280 --> 00:02:23.140
And it gives us the input shape of image.

29
00:02:23.140 --> 00:02:34.500
Roll image columns then one the next step then is to convert our data into what's called the converting

30
00:02:34.500 --> 00:02:39.730
it to a float rather than 0 2 5 4 5 and unsigned integers.

31
00:02:39.730 --> 00:02:42.000
And this is the part that I was talking about.

32
00:02:42.060 --> 00:02:47.040
If it is not digit 5 then Y is converted to zero evidence digit 5.

33
00:02:47.040 --> 00:02:51.960
Then why target is converted to 1.

34
00:02:52.050 --> 00:02:53.930
I'm going to skip this right now.

35
00:02:53.940 --> 00:02:59.400
Our y value is in the form of integer 64.

36
00:02:59.550 --> 00:03:05.510
That's it now for sufficient for our use rather than having to convert it to float 32.

37
00:03:05.650 --> 00:03:09.110
Let's come to the network architecture.

38
00:03:09.240 --> 00:03:17.190
We have the first layer being flatten followed by a dense layer with 128 neurons had the activation

39
00:03:17.190 --> 00:03:25.140
function we're using is really two and then we have another dance function with two class specifying

40
00:03:25.140 --> 00:03:27.570
one basic mean 0 or 1.

41
00:03:27.570 --> 00:03:36.550
And finally our activation function instead of a sigmoid which generate probability is now that was

42
00:03:36.550 --> 00:03:43.930
soft makes now as sigmoid sigmoid is basically default to either 0 or 1.

43
00:03:43.930 --> 00:03:49.210
And while we're running that we have the model of summary that's going to be printing out the model

44
00:03:49.240 --> 00:03:55.990
architecture and I'm going to run these are the backgrounds that we don't have to wait for it to run

45
00:03:56.830 --> 00:04:05.080
and the optimizer we use is the simple stochastic gradient descent and the loss here instead of the

46
00:04:05.140 --> 00:04:05.860
sparse

47
00:04:08.260 --> 00:04:10.350
categorical chaos entropy.

48
00:04:10.360 --> 00:04:14.490
You are using binary cross entropy because we are.

49
00:04:14.500 --> 00:04:18.170
We only have a binary system of binary class here.

50
00:04:18.190 --> 00:04:21.390
And finally we also put in the accuracy here.

51
00:04:21.430 --> 00:04:22.650
So this is the architecture.

52
00:04:22.660 --> 00:04:29.020
We have three layers the input layer that hidden densely and finding the output which is just out with

53
00:04:29.020 --> 00:04:33.010
the output shape of one moving down here.

54
00:04:33.010 --> 00:04:35.620
We are splitting our training data.

55
00:04:35.650 --> 00:04:46.860
Furthermore into chain validation and test this is something there will be more and more prominent as

56
00:04:46.860 --> 00:04:50.000
we are going for right now we're just keeping it really simple.

57
00:04:50.140 --> 00:04:58.300
The first fifty thousand is going to be used for training the remaining 10000 meaning from 50000 fifty

58
00:04:58.360 --> 00:05:06.140
thousand and one do a sixty thousand debt ten thousand will be used for the purpose of validation.

59
00:05:06.640 --> 00:05:15.520
This allow first and allow the algorithm to actually continuously tune the model will be trained using

60
00:05:15.520 --> 00:05:19.810
the training data and then the validation will be used to validate.

61
00:05:19.940 --> 00:05:29.590
And if this is to avoid the over fitting problem that we've mentioned before we also don't want to touch

62
00:05:29.680 --> 00:05:36.340
the test data because if you're touching the test data technically supposed to be only touch ones at

63
00:05:36.340 --> 00:05:37.910
the end of your model.

64
00:05:38.530 --> 00:05:44.230
If you're tested then you what you have done is actually you allow your model to peek into the answer

65
00:05:44.230 --> 00:05:45.090
already.

66
00:05:45.100 --> 00:05:51.350
So really what you want to do is actually set aside some of your training data for validation to verify

67
00:05:51.370 --> 00:05:57.730
that your model hasn't been all the fitted then run the test on the test data for one last time.

68
00:05:57.850 --> 00:06:04.430
So we're doing it for the validation as you can see here the first 50000 is for training as we need

69
00:06:04.430 --> 00:06:08.100
to train underscore one and then 10000 for validation.

70
00:06:08.110 --> 00:06:09.740
We'll run these.

71
00:06:09.920 --> 00:06:11.540
Now this is the actual training itself.

72
00:06:11.540 --> 00:06:13.040
We need to provide Angus probably.

73
00:06:13.050 --> 00:06:13.650
Why.

74
00:06:13.820 --> 00:06:16.510
How many epochs which we have already defined here.

75
00:06:16.520 --> 00:06:23.620
Twenty books and 128 is our bench to update the weight.

76
00:06:23.900 --> 00:06:30.980
And finally we have this new parameter called validation data which we will be fitting in to validate

77
00:06:30.980 --> 00:06:39.050
our data which is the terms of the paramount need to provide is in the form of a couple with x value

78
00:06:39.080 --> 00:06:42.140
and validate early station data.

79
00:06:42.140 --> 00:06:46.670
So as you run there you'll notice that there are more information than before you have to laws you have

80
00:06:46.670 --> 00:06:52.940
to accuracy as before but now you have additional information which is to validation loss as well as

81
00:06:52.940 --> 00:06:54.050
accuracy.

82
00:06:54.050 --> 00:07:03.200
Let's go to the end of our training you will see that the accuracy is 98 and the validation accuracy

83
00:07:03.200 --> 00:07:08.990
is also around ninety eight point two four so that's looking quite good.

84
00:07:08.990 --> 00:07:13.150
And because they are fairly close together so that's looking at the raw data.

85
00:07:13.160 --> 00:07:20.550
We can also visualize it by passing it out and when you see the plot this hour you can see our training

86
00:07:20.550 --> 00:07:27.470
losses continuing to decrease and the validation lost has plateaued a lot sooner.

87
00:07:27.510 --> 00:07:37.260
So around here about 10 epochs the decline of the training is much faster than the validation itself.

88
00:07:37.260 --> 00:07:47.740
What this is actually telling you is that we are starting to all the fit our training data.

89
00:07:47.960 --> 00:07:50.910
Actually correction the validation is the lot.

90
00:07:51.040 --> 00:07:55.440
That's correct validation is actually a solid line and that this training lost.

91
00:07:55.630 --> 00:08:01.030
And what this is really telling you is that we're starting to over fit our training data although the

92
00:08:01.030 --> 00:08:06.520
validation is still going downwards just not as fast as the training.

93
00:08:07.000 --> 00:08:12.910
And that's something that we can correct by using some of the regularization technique that we've talked

94
00:08:12.910 --> 00:08:16.060
about before drop out being one of them.

95
00:08:16.380 --> 00:08:16.720
Right.

96
00:08:16.720 --> 00:08:21.580
So let's plot the training accuracy versus the validation accuracy.

97
00:08:21.580 --> 00:08:23.560
So this is from outside a different point of view.

98
00:08:23.560 --> 00:08:24.940
This is plotting laws.

99
00:08:24.940 --> 00:08:32.290
This is the training and also validation accuracy from Accuracy point of view is pretty much the same

100
00:08:32.290 --> 00:08:33.500
picture.

101
00:08:33.730 --> 00:08:41.560
The accuracy for Belfast you climbing which is quite healthy we can actually continue to train others

102
00:08:42.010 --> 00:08:44.420
to actually see what the final outcomes are.

103
00:08:44.500 --> 00:08:51.850
But nevertheless this is actually very useful and yeah just to summarise what we've cover and this lesson

104
00:08:51.880 --> 00:09:00.790
we looked at binary classification rather than 10 class classification problem we looked at we prepared

105
00:09:00.790 --> 00:09:07.720
data as before the slight variation in terms of the introduction here is what's a new feature called

106
00:09:07.720 --> 00:09:14.650
The Cross back in just to check what is our image in terms of the data format we had channel last.

107
00:09:14.650 --> 00:09:15.550
Or channel first.

108
00:09:15.550 --> 00:09:17.710
That part is important.

109
00:09:18.220 --> 00:09:24.010
Well of course we convert this from 10 class to a binary class problems.

110
00:09:24.070 --> 00:09:32.650
We we looked at sigmoid instead of soft Max and we also look at as JD which for all and temper was for

111
00:09:32.650 --> 00:09:37.890
something like simple architecture and simple data set like this on much.

112
00:09:38.260 --> 00:09:40.960
Very very different than the other.

113
00:09:41.000 --> 00:09:47.530
So core optimization algorithm the part that I do want to highlight is that if something new is the

114
00:09:47.530 --> 00:09:54.430
training as well as to take validation and test data here this is where we actually have departed from

115
00:09:54.430 --> 00:10:01.480
the previous lesson and this is a new concept is that the baseline is that we always keep our training

116
00:10:01.480 --> 00:10:08.530
for training and then we set aside some data for validation and finally test data for the end of our

117
00:10:08.800 --> 00:10:12.330
modelling development or the model development.

118
00:10:12.400 --> 00:10:13.520
Thank you once again for watching.

119
00:10:13.520 --> 00:10:15.090
I look forward to seeing you in the next lesson.
