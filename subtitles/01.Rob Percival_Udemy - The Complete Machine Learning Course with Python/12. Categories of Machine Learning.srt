1
00:00:00,770 --> 00:00:01,640
Welcome back everyone.

2
00:00:01,640 --> 00:00:07,880
In the next few lessons we're going to look at some of the mechanics of machine learning some of the

3
00:00:07,910 --> 00:00:18,020
key part that we need to cover in terms of wrapping up this course is how do you evaluate the machine

4
00:00:18,020 --> 00:00:20,330
learning the performance of it.

5
00:00:20,330 --> 00:00:23,980
Of course we're going to touch on a little bit of the different categories of machine learning or those

6
00:00:23,980 --> 00:00:27,950
already gone through that extensively in another course.

7
00:00:27,950 --> 00:00:30,520
We're going to talk about how do you split your data.

8
00:00:30,530 --> 00:00:35,040
What used to be your data and to attain validation and test said.

9
00:00:35,540 --> 00:00:42,410
We'll also cover a data processing feature engineering and how to spot of an under fitting.

10
00:00:42,460 --> 00:00:47,150
And that's pretty much some of the key ideas that we're going to cover.

11
00:00:47,150 --> 00:00:55,200
And before we finish we're going to touch on regularization and dropout and finally workflow.

12
00:00:55,280 --> 00:01:02,990
So let's dig into it the first one that I want to cover with something that is hopefully a review for

13
00:01:02,990 --> 00:01:03,800
you.

14
00:01:03,800 --> 00:01:07,960
Nevertheless I do want to cover it for those who have not seen it before.

15
00:01:08,000 --> 00:01:11,550
There are really four different categories of machine learning.

16
00:01:11,600 --> 00:01:16,880
There is this supervised which is your typical classification and regression problem.

17
00:01:16,910 --> 00:01:23,720
It could include also sequence generation object detection and image segmentation by and large.

18
00:01:23,750 --> 00:01:30,710
This class is called supervised because you provide the actual target that no one else and also the

19
00:01:30,710 --> 00:01:39,820
input data so that the neural network learn the the expected output there is also the unsupervised which

20
00:01:39,820 --> 00:01:40,320
is.

21
00:01:40,750 --> 00:01:42,940
Well there's basically no Target here.

22
00:01:42,940 --> 00:01:51,250
The machine learning algorithm needs to distinguish the underlying pattern clusters the similar patterns

23
00:01:51,250 --> 00:01:57,180
together or reduced the dimension of the actual underlying pattern itself.

24
00:01:57,340 --> 00:02:00,400
And also you have self supervised.

25
00:02:00,400 --> 00:02:08,590
These are auto encoders whereby again these so core target is not provided it learns to categorize by

26
00:02:08,590 --> 00:02:14,020
itself using auto encoders as hen's core self supervise.

27
00:02:14,020 --> 00:02:20,650
This is a new class on his own is it's it's a real fairly new invention only really a couple of years

28
00:02:20,650 --> 00:02:27,130
old and it really deserve a class on its own because it is quite unique and quite different altogether.

29
00:02:27,160 --> 00:02:29,980
And the final one wonder has quite a lot.

30
00:02:30,510 --> 00:02:38,350
Whether receival or headline attention is reinforcement learning ever since the Alpha ego that beat

31
00:02:38,590 --> 00:02:40,620
the world champion.

32
00:02:40,990 --> 00:02:45,430
So those are the real basic four different categories of machine learning.

33
00:02:45,430 --> 00:02:52,590
And I just want to revise this once again in terms of machine learning performance evaluation.

34
00:02:52,690 --> 00:02:55,510
We've seen this many times before within this course.

35
00:02:55,600 --> 00:03:02,590
You have your input data there's multiple layers of For Your neural network but does the prediction

36
00:03:03,010 --> 00:03:11,020
and the comparing against the actual why you use lost function to measure the difference and generate

37
00:03:11,020 --> 00:03:18,910
a law score of course to improve your you know your neural network you use an optimizer that's that's

38
00:03:18,910 --> 00:03:26,710
based on the problem that you're faced with and finally continue to tune the weights which is really

39
00:03:26,710 --> 00:03:30,040
the memory of your neural network as well.

40
00:03:30,130 --> 00:03:37,120
So the goal of machine learning is that your mettle your model is able to generalize to problems that

41
00:03:38,080 --> 00:03:40,540
are Data's it has not seen before.

42
00:03:40,640 --> 00:03:47,710
And what exactly is the last function we've covered this really is is to Mr. function to calculate how

43
00:03:47,710 --> 00:03:54,940
far this your prediction deviates away from the actual and the lost functions depends on the problem

44
00:03:54,940 --> 00:04:00,430
that you face it could be binary multi class or regression problem.

45
00:04:01,430 --> 00:04:01,740
OK.

46
00:04:01,770 --> 00:04:05,910
So just want to touch on a little bit of train validate and test.

47
00:04:05,910 --> 00:04:11,470
This is really important concepts I did talk about this a little earlier.

48
00:04:11,970 --> 00:04:21,690
So just go straight to the key nodes here tests data set is meant for you to be used only once ones

49
00:04:21,690 --> 00:04:25,580
only and only at the end of the model building.

50
00:04:25,640 --> 00:04:32,520
So this is necessary because you want to prevent information like this is the only way that you can

51
00:04:32,520 --> 00:04:38,700
tell whether your model can actually perform well in the real world situation or not.

52
00:04:38,760 --> 00:04:41,790
So test is really the only way that you can actually tell.

53
00:04:41,790 --> 00:04:44,880
Hence you cannot touch it to be your model.

54
00:04:44,970 --> 00:04:47,450
That's the key point here.

55
00:04:47,460 --> 00:04:54,630
You cannot use the test to build your model your model building must be done using training data set

56
00:04:55,200 --> 00:05:01,800
of course you still need to check how good your model is this where the validation dataset comes in

57
00:05:03,360 --> 00:05:09,170
so for your model your neural net what model it would have seen these data sets the training data says

58
00:05:09,180 --> 00:05:12,900
many many times in order to learn the weight.

59
00:05:13,080 --> 00:05:18,780
But there's no way for you to tell if it all fit and they can generalize to unseen data.

60
00:05:18,890 --> 00:05:24,390
Hence we use validation a dataset to evaluate your model performance.

61
00:05:24,390 --> 00:05:27,840
With respect to various hyper power meters.

62
00:05:29,170 --> 00:05:29,420
OK.

63
00:05:29,460 --> 00:05:37,560
So this is an example of what's called a single holdout test set whereby you have a training set and

64
00:05:37,560 --> 00:05:39,750
then you have a test set.

65
00:05:39,750 --> 00:05:42,390
I'm going to show you this is the simplest of them all.

66
00:05:42,390 --> 00:05:46,970
I'm going to show you a difference a few variation of this.

67
00:05:47,190 --> 00:05:53,370
And the best practice as well so this is a single holdout set in.

68
00:05:53,530 --> 00:06:01,480
And this is useful but that does suffer from some flaws in that up to you train your data.

69
00:06:01,500 --> 00:06:05,930
There's no way for you to tell whether it's all fit or under fit.

70
00:06:06,050 --> 00:06:08,130
And and that's not good.

71
00:06:08,130 --> 00:06:10,660
So which is why this was developed.

72
00:06:10,710 --> 00:06:14,930
You have a single holdout validation and test set.

73
00:06:14,970 --> 00:06:20,380
What I mean by a single holdout here is actually we use a pen.

74
00:06:20,610 --> 00:06:22,980
Is this this portion here.

75
00:06:23,010 --> 00:06:25,150
So there's only one of these.

76
00:06:25,200 --> 00:06:27,750
So this why is core single.

77
00:06:28,680 --> 00:06:33,160
Earlier the previous slide the example was this one.

78
00:06:33,270 --> 00:06:33,630
Okay.

79
00:06:33,630 --> 00:06:43,470
Now instead of that week the training data we're split into maybe this is just 10 percent of your training

80
00:06:43,470 --> 00:06:43,970
data.

81
00:06:43,980 --> 00:06:47,460
The remaining 90 percent is used to train your data.

82
00:06:47,730 --> 00:06:57,310
And this is much better because now you have some way to validate to see if your data has indeed.

83
00:06:57,650 --> 00:07:02,450
Well your neural network model has indeed over learned of a fit.

84
00:07:02,490 --> 00:07:03,960
Your training data.

85
00:07:03,960 --> 00:07:06,300
So that's where the validation comes in.

86
00:07:06,390 --> 00:07:13,650
But one of the more common breast best practice is to do this is called K for validation whereby you

87
00:07:13,650 --> 00:07:18,140
have one fall to actually test your data.

88
00:07:18,270 --> 00:07:19,680
So there's a 10 4 Here.

89
00:07:19,680 --> 00:07:27,600
This is common in in the shadow machine learning for neural network.

90
00:07:27,690 --> 00:07:35,820
This is sufficient just one single holdout validation and also a test that is sufficient to to validate

91
00:07:35,820 --> 00:07:38,270
your neural network.

92
00:07:38,280 --> 00:07:43,290
All right I hope that there has been use for this just one more key point so a few key points that I

93
00:07:43,290 --> 00:07:46,670
want to actually highlight before we move on this.

94
00:07:46,800 --> 00:07:56,000
This slide on traps here please be mindful of them the data representativeness is important.

95
00:07:56,130 --> 00:08:06,780
You must have all the so-called target as well as features represented in your training data otherwise

96
00:08:07,020 --> 00:08:10,520
your neural network never get to seen an example.

97
00:08:10,540 --> 00:08:11,640
So it will never would.

98
00:08:11,640 --> 00:08:18,150
It would never would have learned and for time series data Don Chapel because the sequence in the order

99
00:08:18,150 --> 00:08:23,300
of how the data comes or comes true is important.

100
00:08:23,310 --> 00:08:30,630
You have to ensure that all your test data are posterior meaning in the future to the data in the training

101
00:08:30,630 --> 00:08:31,340
set.

102
00:08:31,470 --> 00:08:39,080
So to use simple English posterior here basically means future

103
00:08:42,440 --> 00:08:43,690
data redundancy.

104
00:08:43,730 --> 00:08:49,490
Also make sure that the same data is not repeated in both the train and test data.

105
00:08:49,490 --> 00:08:55,130
This is a mistake that I wish I demonstrated to you when we were looking at the binary classification

106
00:08:55,130 --> 00:08:56,000
1.

107
00:08:56,330 --> 00:09:04,130
We did some training on on the validation set before we actually created the validation set.

108
00:09:04,130 --> 00:09:07,960
So that's basically noticed that the arrow was significantly lower.

109
00:09:07,970 --> 00:09:14,510
You can try that out but the problem when you actually do that is that while your validation is no longer

110
00:09:14,510 --> 00:09:21,350
useful as a validation dataset because you have allowed your neural network model to see it

111
00:09:24,390 --> 00:09:30,870
and the other example is you have the same picture of the same CAD exactly the same picture in both

112
00:09:31,500 --> 00:09:36,180
your training as well as your validation as well as your tests either war.

113
00:09:36,180 --> 00:09:39,750
This is an A and a real war analogy.

114
00:09:39,750 --> 00:09:41,150
This is basically cheating.

115
00:09:41,280 --> 00:09:46,380
It's like a teacher giving the student sample exam question and then testing the same questions in the

116
00:09:46,380 --> 00:09:47,360
exam.

117
00:09:47,370 --> 00:09:51,750
All right so that's really what's the the traps that.

118
00:09:52,110 --> 00:09:57,370
You do need to be mindful of and it's critical to pay attention to that.

119
00:09:57,450 --> 00:10:02,130
Then I'm gonna stopped and the next video we're gonna go into a data pre processing.
