1
00:00:02,040 --> 00:00:05,910
In our previous lesson we talked about decision tree.

2
00:00:05,920 --> 00:00:12,030
Well really there's actually a very brief introduction to Decision Tree random for us as far as a boost

3
00:00:13,800 --> 00:00:19,060
as an added benefit of using those two models.

4
00:00:21,240 --> 00:00:22,680
Let me take a step back.

5
00:00:22,860 --> 00:00:27,600
Remember when we were looking into Boston housing there were a lot of features and see if I actually

6
00:00:27,600 --> 00:00:28,150
did provide.

7
00:00:28,160 --> 00:00:33,960
Yeah there are a lot of features you have no idea which is the important feature.

8
00:00:33,960 --> 00:00:42,820
I did talk about adding one by one by one and observe using ask where to tell if the actual data.

9
00:00:43,080 --> 00:00:50,760
Sorry the feature itself is important and provide any significant value add in the machine learning

10
00:00:51,210 --> 00:00:53,980
models prediction.

11
00:00:54,540 --> 00:01:00,540
There's a shortcut or walk around to get to those answer really really quickly.

12
00:01:00,540 --> 00:01:05,470
That's where this random forest as well as that of both comes in really handy.

13
00:01:05,490 --> 00:01:12,060
So this is the part whereby we want to revisit the feature importance which we did earlier.

14
00:01:12,240 --> 00:01:18,200
The answer the questions that we won the one to ask is that we have 13 features for the Boston Housing.

15
00:01:18,270 --> 00:01:20,700
Are they all equally important.

16
00:01:20,700 --> 00:01:23,970
Which features are more important.

17
00:01:25,290 --> 00:01:31,680
In Kentucky Lynn help us with this exercise rather than going through the process of elimination one

18
00:01:31,680 --> 00:01:32,970
by one.

19
00:01:33,030 --> 00:01:41,880
Well if we use the edibles we already perform the modeling earlier edibles to have this special feature

20
00:01:42,240 --> 00:01:50,800
or you can call it method call feature underscore importance is and generate this so values.

21
00:01:50,820 --> 00:02:00,030
And if I look at the columns of our underlying data I just need to well combine the two and view them

22
00:02:00,030 --> 00:02:06,450
concurrently which is why I'm using Panda's data frame here and label the column as feature and sort

23
00:02:06,450 --> 00:02:08,940
them in ascending order

24
00:02:11,940 --> 00:02:18,480
descending order because I said it to force then you can see that Alstead has the highest value followed

25
00:02:18,480 --> 00:02:22,550
by room number and all the way down to CHP errors.

26
00:02:22,620 --> 00:02:28,970
So this is the ranking order the importance or the feature importance.

27
00:02:29,310 --> 00:02:36,900
It can also plot them to see in terms of the value absolute of value relative to the rest and you can

28
00:02:36,900 --> 00:02:43,370
see that the first tree although this is not in percentage format provide subsidies or amount of the

29
00:02:43,680 --> 00:02:48,910
explanatory power towards the so-called medium value of the house.

30
00:02:49,170 --> 00:02:55,290
If we look at random forest it doesn't have the same feature importance and as we plotted it pretty

31
00:02:55,290 --> 00:03:02,840
much state the least same state R.M. and this is far although in slightly different.

32
00:03:02,850 --> 00:03:08,930
So core scale but there's still picked the first this tree is the most important.

33
00:03:09,210 --> 00:03:16,630
After that there's some shuffling and slightly defined in terms of order.

34
00:03:16,880 --> 00:03:17,290
Okay.

35
00:03:17,330 --> 00:03:23,760
So a bit of exercise for you looking at Decision Tree regress.

36
00:03:24,480 --> 00:03:32,280
And just to compare now what we did earlier is that we did the random forest and we also did edibles

37
00:03:32,460 --> 00:03:37,680
and what we did is that we spread the data to train and test it but we didn't do is to calculate that

38
00:03:37,680 --> 00:03:40,820
mean square error as well as ask where.

39
00:03:41,190 --> 00:03:49,920
What I'd like to do is actually do that exercise using decision tree regression and see what the actual

40
00:03:49,920 --> 00:03:51,370
outcomes are.

41
00:03:51,420 --> 00:04:01,020
I will provide the code for you to to you to view as a comparison bearing in mind there's one more thing

42
00:04:01,050 --> 00:04:06,540
that we cover in these two lessons is also the so-called feature importance.

43
00:04:06,690 --> 00:04:14,520
I'd like you to actually try that exercise you see the decision tree to help you with the you know to

44
00:04:14,550 --> 00:04:21,120
to not just watch me go through the whole process but also you actually get your hands dirty as well

45
00:04:27,510 --> 00:04:29,930
so let's walk through this exercise.

46
00:04:30,330 --> 00:04:36,750
We perform the prediction of course we create an instance of the model perform the prediction followed

47
00:04:36,750 --> 00:04:43,920
by running diminished area and then to ask where it performed far worse than both the random for us

48
00:04:43,920 --> 00:04:46,290
as well as to add a boost.

49
00:04:46,290 --> 00:04:53,350
And finally when you look at the feature importance it by the order is slightly different but they asked

50
00:04:53,350 --> 00:05:01,590
you the same three variable to arm and this state are in this okay with that I'm going to stop right

51
00:05:01,590 --> 00:05:04,820
now hopefully you've got the same result.

52
00:05:04,950 --> 00:05:08,860
Feel free to dig in a little bit more just to understand the whole exercise.
