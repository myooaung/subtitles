WEBVTT
1
00:00:01.410 --> 00:00:02.130
Welcome back everyone.

2
00:00:02.580 --> 00:00:05.250
Let's do some revision on neuro network.

3
00:00:05.310 --> 00:00:07.770
That's what the Indians stand for.

4
00:00:07.950 --> 00:00:10.890
Let's look at the basic architecture of what it looks like.

5
00:00:10.890 --> 00:00:12.600
So let's look at this.

6
00:00:12.640 --> 00:00:13.510
So for a start.

7
00:00:13.530 --> 00:00:20.640
When you look at this what you have is an example of a standard artificial neural network also called

8
00:00:20.640 --> 00:00:22.860
neural network for short.

9
00:00:22.860 --> 00:00:27.780
If you look at the basic architecture you can identify some basic characteristics.

10
00:00:27.780 --> 00:00:32.730
The first being input and also output.

11
00:00:32.730 --> 00:00:40.470
So you have your input which could be pictures could be features could be a two dimensional data.

12
00:00:41.010 --> 00:00:44.480
And this is the input layer and this is the output there.

13
00:00:44.940 --> 00:00:49.290
And then you have three layers of tree hidden layers.

14
00:00:49.290 --> 00:00:55.920
Now these are what's called fully connected because every one of these neural are connected to the next

15
00:00:55.920 --> 00:00:57.510
layer of neurons.

16
00:00:57.510 --> 00:01:01.930
And this is connected to the next layer of neurons.

17
00:01:01.940 --> 00:01:10.430
Now the secret sauce to all of these neural networks lie in the actual hidden layer itself.

18
00:01:10.430 --> 00:01:19.040
And what we can look at is the so-called convolution all neural networks where the distinguish fact

19
00:01:19.040 --> 00:01:25.190
features are that distinguish or the difference lies in the actual hidden layer.

20
00:01:26.130 --> 00:01:28.860
So for now let's revise some of these key concepts.

21
00:01:28.860 --> 00:01:35.200
And then in next and future lecture we're going to go into the architecture of CNN.

22
00:01:35.250 --> 00:01:40.830
Now what have we learned so far from the previous course is that machine learning is really about learning

23
00:01:41.250 --> 00:01:46.440
the rules that link inputs and also targets together.

24
00:01:46.440 --> 00:01:53.790
So if you provide the computer with pictures of cats it needs to learn some features of the cards to

25
00:01:53.790 --> 00:01:59.220
be able to identify that this is indeed a cat which is targets.

26
00:01:59.250 --> 00:02:05.760
And if you provided the pictures that you has never learned from before or a brand new picture for example

27
00:02:05.880 --> 00:02:13.380
a Mountain cat for example which it hasn't seen before and hopefully from the features that it has learnt

28
00:02:13.470 --> 00:02:21.510
in the past it is able to identify that the egg to a target or the prediction is indeed a cat and we'll

29
00:02:21.510 --> 00:02:25.590
give you the label that this is actually a cat.

30
00:02:25.590 --> 00:02:30.990
Now the question is really finding that features or the three question mark there to link the inputs

31
00:02:30.990 --> 00:02:34.370
to the actual target output.

32
00:02:34.560 --> 00:02:41.280
I provided the picture and or rather the flow diagram below just to establish or just to illustrate

33
00:02:41.280 --> 00:02:43.390
to you the link itself.

34
00:02:43.500 --> 00:02:50.250
Now in order to do that neural networks require many examples to achieve these two lenders very similar

35
00:02:50.250 --> 00:02:57.660
to how human learn if you have ever watch a child of 2 3 years old how they learn in terms of the naming

36
00:02:57.660 --> 00:03:01.330
apples oranges and identify objects.

37
00:03:01.410 --> 00:03:08.100
It's the same process right from the fetus slides we can see that there are many layers to deep neuro

38
00:03:08.190 --> 00:03:12.210
Learning Network or deep learning neural networks.

39
00:03:12.340 --> 00:03:17.100
Now these players learn the relationship between the inputs and the target from being exposed to many

40
00:03:17.100 --> 00:03:17.650
examples.

41
00:03:17.640 --> 00:03:21.270
That's really the basic ideas of all of these.

42
00:03:21.270 --> 00:03:29.820
So in this portion we're going to continue to look at somewhat the example here and terms of the of

43
00:03:29.820 --> 00:03:34.060
the CNN or the convolution or neural network example.

44
00:03:34.080 --> 00:03:36.110
So let's dive into it.

45
00:03:36.150 --> 00:03:44.100
So this is v g g sixteen also core very deep neural network.

46
00:03:44.100 --> 00:03:46.670
Now this has been around for quite some time.

47
00:03:46.920 --> 00:03:56.160
It's still consider one of the more established and well known neural network architecture as in an

48
00:03:56.160 --> 00:03:57.510
architecture.

49
00:03:57.510 --> 00:04:06.180
It's from the paper by Simona and the ISM and the the name of the papers go very deep convolution neural

50
00:04:06.480 --> 00:04:08.670
networks for large scale image recognition.

51
00:04:08.670 --> 00:04:10.890
This is published in 2014.

52
00:04:10.890 --> 00:04:19.800
It is still use today as the backbone for some of the object identification algorithm that's being used

53
00:04:19.800 --> 00:04:21.020
today.

54
00:04:21.090 --> 00:04:24.740
So let's look at the example of neural network architecture.

55
00:04:24.810 --> 00:04:27.400
I provided a link here feel free to actually browse through.

56
00:04:27.420 --> 00:04:30.520
We want things to go through that for now.

57
00:04:30.990 --> 00:04:37.280
What I have not remove or not shown here is the last few layers which is the fully connected layer and

58
00:04:37.280 --> 00:04:39.770
that has been removed.

59
00:04:39.960 --> 00:04:46.890
When we look at the so-called layers here you can see how deep this neural network or this CNN example

60
00:04:46.890 --> 00:04:47.620
is.

61
00:04:47.730 --> 00:04:50.660
You have the input layer you followed by block 1.

62
00:04:50.850 --> 00:04:54.450
If you look at the block one you'll notice that there's one two three.

63
00:04:54.500 --> 00:05:01.920
It has convolution or air another convolution or air and then it makes pulling layer followed by Block

64
00:05:01.920 --> 00:05:02.540
2.

65
00:05:02.730 --> 00:05:09.190
You see that is convolution or a coalition or air followed by a mix pool block 3.

66
00:05:09.210 --> 00:05:17.070
You have another convolution area convolution area followed by another mixed pool block for convolution

67
00:05:17.160 --> 00:05:23.700
convolution convolution followed by a mixed pool block five convolution convolution convolution followed

68
00:05:23.700 --> 00:05:25.870
by a max pool.

69
00:05:26.240 --> 00:05:35.480
I intentionally go through that repetitively to hopefully allow you to see the pattern.

70
00:05:35.520 --> 00:05:41.280
You notice that the first two blocks is two convolution followed by Max pool.

71
00:05:41.280 --> 00:05:48.260
Second convolution is second block is to convolution followed by Max pool and then the following block

72
00:05:48.280 --> 00:05:52.910
three or four five is three convolution followed by Max pool.

73
00:05:53.040 --> 00:05:56.820
This is five five blocks.

74
00:05:56.910 --> 00:06:03.300
Ultimately if you look carefully is actually a lot deeper than then five blocks is actually 16.

75
00:06:03.300 --> 00:06:06.760
That's why this is called BCG 16.

76
00:06:07.290 --> 00:06:10.070
This is an example of CNN.

77
00:06:10.350 --> 00:06:12.140
I'm going to start calling CNN.

78
00:06:12.150 --> 00:06:22.510
I'm having problem pronouncing convolution on a CNN as the as an example of CNN architecture let's again

79
00:06:22.510 --> 00:06:26.150
revise some One last key concept before we move on.

80
00:06:26.200 --> 00:06:35.560
You will recall from our previous course lectures that we provide the neural network with inputs and

81
00:06:35.800 --> 00:06:38.140
there are some ways that needs to be changed.

82
00:06:38.140 --> 00:06:42.330
Initially it's just randomize waves followed by two.

83
00:06:42.550 --> 00:06:48.550
In this example we have two hidden layers or two layers for this neural network followed by a prediction

84
00:06:49.810 --> 00:06:56.680
from prediction and compare it against the ground truth or the actual label and then measured using

85
00:06:56.680 --> 00:07:00.370
the lost function to measure the loss score losses.

86
00:07:00.370 --> 00:07:08.730
Another way of saying errors and obviously initially the neural network hasn't learned yet.

87
00:07:09.380 --> 00:07:09.850
Okay.

88
00:07:09.900 --> 00:07:17.920
We need to actually go through iteration a loop to actually continuously learn where admitted mistakes

89
00:07:17.930 --> 00:07:25.090
in need to actually make some adjustment to actually learn the actual features to be able to correctly

90
00:07:25.090 --> 00:07:32.800
identify and not so in the future predict what the actual images and that process of continuous loop

91
00:07:33.160 --> 00:07:41.320
is called learning and the the method or the instrument or the mechanics used to to teach the new or

92
00:07:41.320 --> 00:07:43.780
that what is called the optimizer.

93
00:07:44.460 --> 00:07:44.920
Okay.

94
00:07:44.950 --> 00:07:48.090
These are all revisions all revision concepts.

95
00:07:48.090 --> 00:07:51.480
Hopefully it's not too foreign to you.

96
00:07:51.610 --> 00:07:58.480
If not please go back to the other course neural network foundation or introduction to actually built

97
00:07:58.480 --> 00:08:01.030
on those foundation before you carry on.

98
00:08:01.030 --> 00:08:06.010
Because this whole course is actually built upon that but then I'm going to stop thank you once again

99
00:08:06.010 --> 00:08:06.870
for watching.

100
00:08:06.970 --> 00:08:08.470
I look forward to seeing the next lesson.
