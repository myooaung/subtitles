WEBVTT
1
00:00:01.170 --> 00:00:02.100
Welcome back everyone.

2
00:00:02.100 --> 00:00:10.710
Let's get started with training your own CNN and of learning all the contents only or the principles

3
00:00:10.710 --> 00:00:15.600
and concepts we're going to dig right into training your own CNN right now.

4
00:00:16.170 --> 00:00:21.320
So before we get started the first thing that we need to do is get some data.

5
00:00:21.840 --> 00:00:26.250
The Cagle is where we're going to head to for the data.

6
00:00:26.430 --> 00:00:28.970
It's called Dogs and cats redux.

7
00:00:28.980 --> 00:00:36.360
If you actually click onto the link which I provide as a PDA we'll bring to you this Web site here by

8
00:00:36.370 --> 00:00:40.000
Cagle cargo is owned by Google.

9
00:00:40.000 --> 00:00:47.340
Now what they have done here is that they provided images of dogs and cats as you know and our previous

10
00:00:47.340 --> 00:00:52.830
lessons is that if you can train any DB learning neural network you must have data.

11
00:00:53.220 --> 00:01:00.350
So we are heading to Keiko for this data.

12
00:01:00.430 --> 00:01:06.410
Now let's walk through the cargo dashboard so that you are comfortable with it.

13
00:01:06.430 --> 00:01:10.180
This is the cargo dogs versus cats redux.

14
00:01:10.180 --> 00:01:11.640
This is the colonel's edition.

15
00:01:11.650 --> 00:01:21.860
So you have the overview which provide the background in terms of what these tasks entails.

16
00:01:21.910 --> 00:01:27.670
You have the leaderboards the colonels as well as discussion topics so you can actually have look at

17
00:01:27.670 --> 00:01:34.750
the tunnels there's a few people who are happy to share the results with codes as well for you to look

18
00:01:34.750 --> 00:01:37.800
through in terms of the data.

19
00:01:37.800 --> 00:01:39.810
This is what we're interested in.

20
00:01:39.810 --> 00:01:47.760
You can actually click download and download before you don't know you do need to actually sign in to

21
00:01:47.760 --> 00:01:51.900
the cargo account that you have if you don't have an account.

22
00:01:51.900 --> 00:01:59.310
Feel free to register one you do unfortunately need to register before you can actually obtained a copy

23
00:01:59.310 --> 00:02:00.800
of it.

24
00:02:01.530 --> 00:02:04.850
Now the next thing is that there are kernels.

25
00:02:04.860 --> 00:02:07.910
There are few that that people are happy to share.

26
00:02:07.920 --> 00:02:11.010
I have my own work here and some favorites.

27
00:02:11.070 --> 00:02:14.310
There are few discussion boards that people want to discuss.

28
00:02:14.430 --> 00:02:17.820
This is always a great way to actually get started with learning.

29
00:02:17.940 --> 00:02:23.280
And finally the leader boards if you look at the public leader boards and you can look at some of these

30
00:02:23.280 --> 00:02:32.880
scores and benchmark yourself against that right now for this exercise what I have done is actually

31
00:02:33.150 --> 00:02:37.370
I've already downloaded this for you.

32
00:02:37.890 --> 00:02:44.790
If you go to the folder there will be the directory is there is actually a folder called data.

33
00:02:44.790 --> 00:02:54.150
If you go into data the actual download the data from Kaggle which is these data source here you have

34
00:02:54.150 --> 00:03:00.180
the test and then you have to train what I have done with the with the data is that I've done all these

35
00:03:00.180 --> 00:03:05.480
to the test and train only place the training data here.

36
00:03:05.580 --> 00:03:13.530
The training is the full set of data where that's provided by Kaggle that does take a while to open

37
00:03:13.530 --> 00:03:22.530
because there are a lot of data in the let me just go there from another direction.

38
00:03:22.600 --> 00:03:29.060
This is the photo and we bring it across.

39
00:03:29.160 --> 00:03:34.230
And if you go into the data under training these are all the pictures.

40
00:03:34.890 --> 00:03:35.270
OK.

41
00:03:35.280 --> 00:03:39.270
And what I have done with the God this has come up.

42
00:03:39.270 --> 00:03:43.910
These are all the data you can exchange scroll to the pictures if you like.

43
00:03:44.550 --> 00:03:46.780
Let's go back to data.

44
00:03:47.000 --> 00:03:48.110
We just closed it off

45
00:04:07.280 --> 00:04:13.100
you may ask that why is there two are the folders these two folders.

46
00:04:13.160 --> 00:04:20.910
The sample data that I extracted out of the training original data sets from Kaggle.

47
00:04:20.940 --> 00:04:22.570
Now this is too large.

48
00:04:22.580 --> 00:04:27.650
Rather it's it's it's not too large it's just the quite large and it takes a while for the training

49
00:04:28.220 --> 00:04:29.300
to be completed.

50
00:04:29.300 --> 00:04:34.790
What I wanted to do is actually just to illustrate to you the in terms of the process or how this can

51
00:04:34.790 --> 00:04:35.500
be done.

52
00:04:35.780 --> 00:04:42.200
And so there's no point especially when you're actually performing the investigation pod to actually

53
00:04:42.200 --> 00:04:47.690
use the whole dataset it's better that you actually get your model up and running before you actually

54
00:04:47.690 --> 00:04:50.640
run your complete training.

55
00:04:50.750 --> 00:04:52.970
You're training on the complete data set.

56
00:04:53.060 --> 00:04:59.060
It's better to use a small sample a couple of thousand pictures to play around with your neural network

57
00:04:59.090 --> 00:05:03.590
architecture and once you're happy with it then training on the full data set.

58
00:05:03.590 --> 00:05:11.720
The reason is that loading these pictures especially when you have a lot of them the training time can

59
00:05:11.720 --> 00:05:12.980
be quite substantial.

60
00:05:12.980 --> 00:05:17.570
In fact it can take up to weeks if you're online not mine for that.

61
00:05:17.570 --> 00:05:24.300
So when optimizing your time and being mindful of that is really important.

62
00:05:24.740 --> 00:05:33.500
So let me just control B to minimize step or command be in Mac so let's walk through these codes that

63
00:05:33.500 --> 00:05:34.540
I've provided.

64
00:05:34.580 --> 00:05:35.980
This is core 1 to 1.

65
00:05:36.110 --> 00:05:38.200
Cats and dogs.

66
00:05:38.260 --> 00:05:38.730
Dawn.

67
00:05:38.790 --> 00:05:41.620
Thought I Python notebook.

68
00:05:42.020 --> 00:05:46.060
So if you look at this these some of these comments are originally from cargo.

69
00:05:46.070 --> 00:05:53.440
I'm going to leave it there with imported name PI.

70
00:05:53.490 --> 00:05:59.420
This is for linear algebra and also computation of import appendages and also CV 2.

71
00:05:59.490 --> 00:06:02.720
This is computer vision too.

72
00:06:03.240 --> 00:06:06.850
I won't actually walk through and explain what is CV to it.

73
00:06:07.100 --> 00:06:08.520
It's an API on its own.

74
00:06:08.520 --> 00:06:11.760
It's actually open source library.

75
00:06:11.880 --> 00:06:18.870
There's lot of useful information in there especially when it comes to manipulating images and videos.

76
00:06:18.870 --> 00:06:26.220
Of course we make use of the usual map plot lip high plot for plotting purpose the style that we're

77
00:06:26.220 --> 00:06:36.690
using as DG plot popularized by R and we importing psychic land model selection to spit out data into

78
00:06:36.690 --> 00:06:49.080
training as well as test set and we are gonna work with images that has height of 128 pixels and width

79
00:06:49.150 --> 00:06:54.460
of hundred and twenty eight pixels you can ignore these comments.

80
00:06:54.460 --> 00:06:56.930
We are importing two more packages.

81
00:06:56.930 --> 00:07:05.880
One is import os the other one is in post as hedge you two forward file copying right.

82
00:07:06.170 --> 00:07:13.040
Your running and when you're running in this for the first time you do need to actually copy these into

83
00:07:13.040 --> 00:07:14.690
these two folders.

84
00:07:14.720 --> 00:07:16.830
Now you don't have to do that yourself.

85
00:07:16.850 --> 00:07:19.220
I've already written the code for you.

86
00:07:19.220 --> 00:07:24.290
First thing you need to do before you copy the file is that you do need to create the directories.

87
00:07:24.320 --> 00:07:26.650
That's what these are for.

88
00:07:26.960 --> 00:07:33.410
The source data underscore dir here which is this line of code.

89
00:07:33.600 --> 00:07:44.070
At points your code of informed Python that the source for that is within data and the phone to call

90
00:07:44.160 --> 00:07:45.400
chain.

91
00:07:45.540 --> 00:07:53.200
The next thing is to try when you actually tried to create a directory to already exist.

92
00:07:53.200 --> 00:07:55.300
You will get an error message.

93
00:07:55.440 --> 00:07:55.740
Okay.

94
00:07:55.750 --> 00:07:58.260
That's why I used to try.

95
00:07:58.390 --> 00:08:04.410
If there is an error it basically means that the directory already has been created.

96
00:08:04.450 --> 00:08:05.440
Then don't create.

97
00:08:05.440 --> 00:08:07.580
Skip the whole portion altogether.

98
00:08:07.600 --> 00:08:13.180
Skip those steps steps all the kinda so I have four things here.

99
00:08:13.220 --> 00:08:19.060
One is basic using the always package operating system to make directory.

100
00:08:19.070 --> 00:08:26.930
We're trying to make training set or split our data into training set as well as validation set we have

101
00:08:27.200 --> 00:08:33.830
within the training set another sub folder core cats and a supported called docs that's where we're

102
00:08:33.830 --> 00:08:41.350
going to place all of the images are ha cats and dogs and the validation set is for validating our model.

103
00:08:41.360 --> 00:08:43.300
Again we have cats and dogs for that.

104
00:08:43.310 --> 00:08:49.200
So this first portion of the code is try to make these directory.

105
00:08:49.210 --> 00:08:53.010
That's what the M Cadia stands for is try to make the directory.

106
00:08:53.110 --> 00:09:00.940
Failing that meaning the under the situation whereby the directory is already created an arrow will

107
00:09:00.940 --> 00:09:01.870
be drawn up.

108
00:09:01.960 --> 00:09:08.410
Then this is where the catching of the exception comes in to pass meaning paths passed the meaning of

109
00:09:08.410 --> 00:09:14.860
passes that don't perform the above codes at all.

110
00:09:14.860 --> 00:09:23.860
The next thing is we perform listing off the directory the files in the data folder.

111
00:09:23.930 --> 00:09:28.650
The sub photo trainings underscore set and Kent's folder.

112
00:09:28.800 --> 00:09:34.350
This list directory is such that we're running this line of code.

113
00:09:34.350 --> 00:09:38.870
If O S list directory is equal to blank all right.

114
00:09:38.870 --> 00:09:44.330
Meaning that for that doesn't have any pictures then perform the following.

115
00:09:44.370 --> 00:09:50.310
Again what you don't want to do is that every time when you run the codes this get run and then you

116
00:09:50.310 --> 00:09:55.320
copying many many pictures and quite often the same pictures being copied over.

117
00:09:55.830 --> 00:09:57.010
You don't want to do that.

118
00:09:57.090 --> 00:09:59.190
Hence I have this if statement.

119
00:09:59.520 --> 00:10:06.390
If the copying has already been done then don't copy any more so hence the comment there that says if

120
00:10:06.390 --> 00:10:09.360
the folder contains no picture then start copying.

121
00:10:09.630 --> 00:10:09.830
Right.

122
00:10:09.840 --> 00:10:10.770
What are we doing.

123
00:10:10.770 --> 00:10:12.740
Let me just minimized it.

124
00:10:12.750 --> 00:10:20.410
We are copying two thousand cat pictures and copying from the source data directory.

125
00:10:20.500 --> 00:10:22.220
Data dir which is defined here.

126
00:10:22.230 --> 00:10:31.140
Data on the score or backslash train we're copying cats picture from the source directory to the training

127
00:10:31.140 --> 00:10:32.400
directory.

128
00:10:32.670 --> 00:10:36.950
Again we'll copy another two thousand pictures of dogs.

129
00:10:37.180 --> 00:10:37.470
All right.

130
00:10:37.470 --> 00:10:43.860
From the source directory to the training set directory and the sub for the core dogs.

131
00:10:43.860 --> 00:10:47.380
Again we copy 500 pictures.

132
00:10:47.380 --> 00:10:48.690
All right.

133
00:10:48.690 --> 00:10:55.310
The pictures that has the name from two thousand to two thousand five hundred copying from the source

134
00:10:55.310 --> 00:11:01.360
directory to the cats validation some photo validation some for the cats.

135
00:11:01.520 --> 00:11:03.860
Again we do the same with the dogs.

136
00:11:03.860 --> 00:11:10.630
So essentially what are these few line of codes as doing what we are doing here is that we copying two

137
00:11:10.640 --> 00:11:18.200
thousand pictures put it in the training directory and the cats and dogs respectively and then we are

138
00:11:18.200 --> 00:11:23.750
copying another 500 for cats and dogs and pack them under the validation.

139
00:11:23.750 --> 00:11:28.070
Then the question is What is this line of code mean.

140
00:11:28.070 --> 00:11:28.520
Right.

141
00:11:28.520 --> 00:11:30.650
This is basically a false statement.

142
00:11:30.680 --> 00:11:39.430
For I in range 2000 when you use the range command basically it will list or provide iteration of 0

143
00:11:39.440 --> 00:11:40.280
1 2 3.

144
00:11:40.430 --> 00:11:50.740
Essentially a list so when you actually run the for loop I become 0 1 2 3 and this part here is just

145
00:11:50.740 --> 00:12:00.160
basically creating the file name core cat dot 0 dot J Peg Cat dot one dot j peg dot to dot JPA on and

146
00:12:00.160 --> 00:12:02.100
on it goes for the first two thousand.

147
00:12:02.680 --> 00:12:04.330
So we're creating these.

148
00:12:04.330 --> 00:12:07.480
This will become because you can see here there's a bracket.

149
00:12:07.480 --> 00:12:09.550
So that's a list comprehension.

150
00:12:09.550 --> 00:12:17.590
So you have created a list containing cat dot 0 dot GPA comma Cat 1 dot GPA on and on and goes for the

151
00:12:17.590 --> 00:12:18.750
first two thousand.

152
00:12:18.860 --> 00:12:22.150
It will stop at Cat dot 1 9 9 9 dot J PAC.

153
00:12:22.690 --> 00:12:28.660
So basically we have two thousand file names there and we're running another full loop here looping

154
00:12:28.660 --> 00:12:30.340
over the list.

155
00:12:30.400 --> 00:12:40.950
So basically the next part here is asking or telling Python to copy these files from the source directory

156
00:12:40.980 --> 00:12:44.370
to the training directory with cats.

157
00:12:44.550 --> 00:12:50.880
If we come over here you'll notice that the name is indeed kept on 0 dot JPA so we know what the file

158
00:12:50.880 --> 00:12:52.530
names looks like.

159
00:12:52.530 --> 00:12:57.850
And then we copy it over let's go to the training set.

160
00:12:57.850 --> 00:13:00.500
You notice the directory structure is that this.

161
00:13:00.570 --> 00:13:02.730
There's two s folders cats and dogs.

162
00:13:02.730 --> 00:13:08.730
If I click cards you'll notice that the pictures has been copied over and there are two thousand items

163
00:13:08.730 --> 00:13:10.540
in this folder for the dog.

164
00:13:10.560 --> 00:13:13.380
Likewise dot dot dot 0.

165
00:13:13.500 --> 00:13:19.110
Likewise two thousand is always good to visually check this just to make sure that everything has been

166
00:13:19.110 --> 00:13:22.740
done in accordance to your wish and what you will require.

167
00:13:22.740 --> 00:13:26.030
And that's basically what I want to cover here.

168
00:13:26.040 --> 00:13:33.570
This is really the preparation before the data pre processing what we've done here literally is create

169
00:13:33.960 --> 00:13:36.870
the data set for us to start training our model.

170
00:13:36.870 --> 00:13:42.030
But there are some more things that we need to do in terms of preparing our data as well as creating

171
00:13:42.030 --> 00:13:43.680
the neural network.

172
00:13:43.890 --> 00:13:45.210
We'll do that in the next video.

173
00:13:45.210 --> 00:13:46.620
Thank you once again for watching.
