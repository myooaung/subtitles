1
00:00:00,990 --> 00:00:01,890
Welcome back.

2
00:00:01,920 --> 00:00:06,200
In this segment we're going to look at a motivational example.

3
00:00:06,200 --> 00:00:12,160
Now this is from the other causes Markle Foundation or neural network or introduction to neural network.

4
00:00:12,540 --> 00:00:17,430
So what I have is the example of one an example too is this one here.

5
00:00:17,570 --> 00:00:18,860
Okay.

6
00:00:19,230 --> 00:00:20,400
All right.

7
00:00:20,650 --> 00:00:25,800
Did let me just walk through these codes just to refresh your memory and then we're going to go into

8
00:00:26,120 --> 00:00:36,090
a comparison between the neural network and artificial neural network vs. CNN and compare them in terms

9
00:00:36,090 --> 00:00:37,390
of performance.

10
00:00:37,470 --> 00:00:37,710
Right.

11
00:00:37,740 --> 00:00:45,030
So these text processes but as usual you input the data or rather you input num pi you import tariffs

12
00:00:45,210 --> 00:00:51,480
and from carers we are using the MNC data set and then we split our data sets into training set and

13
00:00:51,480 --> 00:00:53,510
also the test set.

14
00:00:53,520 --> 00:00:56,510
This is the basic usual data exploration.

15
00:00:56,520 --> 00:01:04,290
There are sixty thousand rows of data and each of them are 28 by 28 28 rows and 28 columns and they

16
00:01:04,290 --> 00:01:05,770
are all in the NUM pi array.

17
00:01:05,790 --> 00:01:06,860
And this is what it looks like.

18
00:01:06,870 --> 00:01:12,570
It doesn't make any sense hence what we do is that we usually convert it to an image format that looks

19
00:01:12,570 --> 00:01:15,540
like this to make some sense out of it.

20
00:01:15,540 --> 00:01:20,920
So looking at the the training why train privacies X train.

21
00:01:20,970 --> 00:01:25,720
This is the white train which is a target variable the shape is also sixty thousand.

22
00:01:26,040 --> 00:01:33,030
We're looking at the second which is the third item to remember the pattern start counting from zero.

23
00:01:33,090 --> 00:01:37,200
So we're looking at for the actual target.

24
00:01:37,260 --> 00:01:43,850
Yeah or the label was four and if you look at the corresponding X train two is the image looks that

25
00:01:43,860 --> 00:01:46,560
this is a four is a handwritten digit.

26
00:01:47,580 --> 00:01:47,850
Right.

27
00:01:47,850 --> 00:01:53,070
And then if you look at the X tests and that y test there are ten thousand rows of data that we're using

28
00:01:53,070 --> 00:02:00,800
to actually test the accuracy of the neural networks in terms of the architecture we're using Kerry's

29
00:02:00,820 --> 00:02:08,600
here and the actual model is the sequential model and there are two dense layer.

30
00:02:08,840 --> 00:02:16,170
There are 512 neurons so you can see that this is quite a quite a massive new network is is very wide

31
00:02:16,610 --> 00:02:20,490
but it's not very deep so a breath Y is it's fine and twelve.

32
00:02:20,510 --> 00:02:26,420
But depth wise is only one two because we have ten classes or ten digits.

33
00:02:26,450 --> 00:02:31,190
So that's why we actually have 10 dense core outcome here.

34
00:02:31,620 --> 00:02:37,250
And we're using the self mixed and cell predictor rather than sigmoid sigmoid for binary soft mixes

35
00:02:37,250 --> 00:02:39,410
for multi class.

36
00:02:39,860 --> 00:02:46,520
So those are the basic steps out the previous portion is really just reshaping our data in such a way

37
00:02:46,520 --> 00:02:54,920
that the sixty thousand row we're combining reshaping rather than 28 by 28 in terms of the 2D we come

38
00:02:55,190 --> 00:02:56,480
with community into

39
00:02:59,030 --> 00:03:10,100
one vector and we convert the data normalized it into between 0 and 1 hence we divide it by 2 5 5 because

40
00:03:10,100 --> 00:03:16,610
the numbers here is between 0 and 2 5 5 and we're using an optimizer which is armaments prop the more

41
00:03:16,610 --> 00:03:22,540
advanced version instead of stock that is stochastic gradient descent.

42
00:03:22,540 --> 00:03:23,300
All right Judy.

43
00:03:23,330 --> 00:03:30,590
And we're using categorical across in entropy rather than binary because we're at 10 digits here and

44
00:03:32,180 --> 00:03:40,000
the training shows that in terms of the performance accuracy starts from 91 percent and reached a peak

45
00:03:40,000 --> 00:03:45,560
of ninety nine point six percent and in terms of the prediction is ninety eight point one five.

46
00:03:45,800 --> 00:03:46,000
Right.

47
00:03:46,000 --> 00:03:50,010
So that's really a really quick tool revision of what we looked at.

48
00:03:50,230 --> 00:03:55,810
If we go over to the motivational example the first portion here is exactly the same.

49
00:03:55,810 --> 00:04:00,180
OK well I'm just combining all the codes into one and insulting.

50
00:04:00,220 --> 00:04:03,210
Instead of calling it model I call it an.

51
00:04:03,980 --> 00:04:09,640
And if you look at the performance that is exactly is exactly the same 98 and in terms of the prediction

52
00:04:09,640 --> 00:04:15,400
is ninety eight point 0 1 percent because see that architecture should look familiar to you it's fine

53
00:04:15,400 --> 00:04:21,880
until off in terms of the number of neurons or breadth and the depth there's only two rows one densely

54
00:04:21,880 --> 00:04:24,640
connected in two and another one.

55
00:04:24,640 --> 00:04:26,690
So there's actually two of them.

56
00:04:26,830 --> 00:04:32,560
Now let's have a look at CNN convolution all neural networks.

57
00:04:32,740 --> 00:04:36,910
You if you look at these first cell here they are exactly the same.

58
00:04:37,000 --> 00:04:39,970
If you are looking at the eye put a note here.

59
00:04:39,970 --> 00:04:41,740
This is without padding.

60
00:04:41,740 --> 00:04:42,940
Just take note of that.

61
00:04:42,940 --> 00:04:44,790
That's not important for now.

62
00:04:45,040 --> 00:04:49,070
Reward should cover the actual details of the architect a bit later on.

63
00:04:49,360 --> 00:04:49,600
Right.

64
00:04:49,630 --> 00:04:57,160
So in terms of the architecture of the CNN we have a convolution or layer followed by Mexico.

65
00:04:57,220 --> 00:05:03,190
Another convolution there might be followed by Mexico convolution there followed by a flatten and then

66
00:05:03,220 --> 00:05:05,150
another to densely connected there.

67
00:05:05,220 --> 00:05:06,810
And it's that that's all that we have.

68
00:05:06,810 --> 00:05:15,010
So we have one two three convolution there two makes poorly and then flatten it so that the these core

69
00:05:15,430 --> 00:05:20,470
soft Max the classifier can actually make prediction.

70
00:05:20,490 --> 00:05:26,030
All love it that there is this is an important process or step that they actually need to flatten it.

71
00:05:26,050 --> 00:05:32,350
Otherwise these soft mix would not be able to perform is what it needs to do.

72
00:05:32,350 --> 00:05:38,290
So the rest is exactly the same we are we need to compile it we need to fit it and then followed by

73
00:05:38,760 --> 00:05:39,430
prediction.

74
00:05:39,430 --> 00:05:45,520
So if you look at the training score this is the highest is ninety nine point eight 0 7 radians to outperform

75
00:05:46,600 --> 00:05:54,230
against the end and this is 98 percent and this is the actual print of the architecture itself you can

76
00:05:54,230 --> 00:06:00,920
see convolution next convolution Max convolution flatten to dance layer and if you look at the prediction

77
00:06:00,920 --> 00:06:03,010
is ninety nine point one six.

78
00:06:03,290 --> 00:06:08,650
It's far outperform the ninety eight point 0 1 offer and then.

79
00:06:09,230 --> 00:06:16,460
And that's the basic idea of CNN you can see that this is really an exciting space this is where you

80
00:06:16,460 --> 00:06:24,890
can see that the CNN outperform the and then in terms of the prediction and it actually able to learn

81
00:06:25,100 --> 00:06:29,840
not just because the actual prediction is more accurate but much what's more important is that it is

82
00:06:29,840 --> 00:06:37,940
actually you can see that it actually learn to distinguish the key features that they end and when able

83
00:06:37,940 --> 00:06:44,310
to learn before and then lies the problem with fully connected layer is that it just learn the universal

84
00:06:44,330 --> 00:06:52,010
features but it doesn't actually learn the intricacy of some of the key difference in terms of the shapes

85
00:06:52,670 --> 00:06:58,590
of the of the underlying pictures that we feed fit through.

86
00:06:59,110 --> 00:06:59,460
OK.

87
00:06:59,480 --> 00:07:06,080
So that's really to motivate you to understand why we actually embark on the journey to learn CNN which

88
00:07:06,080 --> 00:07:12,290
is the state of the art architecture of two for object detection and image recognition.

89
00:07:12,290 --> 00:07:19,970
And now when we come back we're going to start to get some another perspective or intuition of how CNN

90
00:07:19,970 --> 00:07:22,880
actually view images this time around.

91
00:07:22,880 --> 00:07:26,290
A lot more visual rather than code and I look forward to seeing you then.
