1
00:00:04,640 --> 00:00:06,140
Welcome back everyone.

2
00:00:06,140 --> 00:00:11,030
We're going to start looking at the actual architecture of a neural network.

3
00:00:11,030 --> 00:00:14,680
We're going to start with something that a little bit more complex.

4
00:00:14,750 --> 00:00:21,750
This is a simplified neural network but it's much more complex than the one that we just coded up in

5
00:00:21,770 --> 00:00:22,590
cold.

6
00:00:23,030 --> 00:00:25,700
Looking at there as you can see that there are three layers.

7
00:00:25,700 --> 00:00:27,620
There is the input layer.

8
00:00:27,620 --> 00:00:31,360
There is the hidden layer and there is the output layer.

9
00:00:31,370 --> 00:00:36,200
So let me walk through them one by one the employers are the features.

10
00:00:36,240 --> 00:00:37,340
Okay.

11
00:00:37,370 --> 00:00:40,960
In our example previously we only have one feature.

12
00:00:40,970 --> 00:00:48,490
So in that case you probably represent it as x 1 in this neural network architecture.

13
00:00:48,500 --> 00:00:50,120
There are three input.

14
00:00:50,120 --> 00:00:59,120
Okay so this x1 x2 as far as extreme moving on to the next layer which is called the hidden layer.

15
00:00:59,140 --> 00:01:05,950
The reason is called hidden layer is because it's not exposed to the input neither is it each day exposed

16
00:01:06,340 --> 00:01:08,440
exposed to the output hands.

17
00:01:08,440 --> 00:01:10,650
It is the hidden layer.

18
00:01:10,810 --> 00:01:19,020
Now with the hidden layer we don't actually we don't have any direct influence of the hidden day itself.

19
00:01:19,450 --> 00:01:24,150
And the output layer is the output of our machine learning model.

20
00:01:24,340 --> 00:01:31,120
And that's the layer that actually provide the outcome of all machine learning prediction.

21
00:01:31,120 --> 00:01:37,400
So if you look at this the upper layer has one output meaning it would generate one single value.

22
00:01:37,960 --> 00:01:43,960
So if you look at the whole architecture of this neural network it takes three inputs and will come

23
00:01:43,960 --> 00:01:46,320
up with one output.

24
00:01:46,330 --> 00:01:53,080
The other thing that I want to actually highlight is notice that the lines they each layer this layer

25
00:01:53,230 --> 00:01:59,130
each input is connected to the next layer fully.

26
00:01:59,200 --> 00:02:03,310
Hence this is also sometimes called the fully connected layers.

27
00:02:03,310 --> 00:02:09,040
And this layer is also called the dense less dense network.

28
00:02:09,040 --> 00:02:12,790
And likewise for this as well.

29
00:02:12,790 --> 00:02:17,230
And let me just walk through in terms of the mathematics of this.

30
00:02:17,230 --> 00:02:21,310
You don't need to understand this because you don't need to actually calculate it will be all done by

31
00:02:21,340 --> 00:02:26,730
extensive flow the estimation the computation will be all done by intensive flow.

32
00:02:26,830 --> 00:02:34,450
We start with the N1 which is this neuron here and it takes the input from x1 x2 expel x3.

33
00:02:34,450 --> 00:02:43,060
So these tree mut and X1 is multiplied by the weight of 1 1 which is this represented by this line here

34
00:02:43,750 --> 00:02:51,870
and X2 is multiplied by W2 of x3 is multiplied by W. 13 and then plus advised him B1 here stands for

35
00:02:51,880 --> 00:02:59,890
buys them much as same concept as the intercept in the Y is good and makes plus see from your high school

36
00:03:00,730 --> 00:03:01,420
all right.

37
00:03:01,510 --> 00:03:08,770
So the W eleven the beauty of and W thirteen are in the beginning as I mentioned in the previous lesson

38
00:03:08,770 --> 00:03:15,970
is that these weights are randomized okay when it's first the algorithm doesn't know what's the right

39
00:03:15,970 --> 00:03:24,250
way in which is initialized with any random weights and then or just compute the N1 likewise for N two

40
00:03:24,250 --> 00:03:32,070
as well the result of N1 and end to is being provided as an input to 4 and 3.

41
00:03:32,080 --> 00:03:38,290
So if you look at the formula right here is represented right here here entry is takes in the input

42
00:03:38,290 --> 00:03:46,210
from n 1 and multiplied by another weight and then 2 is multiplied by another weight plus another bias

43
00:03:46,290 --> 00:03:56,940
I'm looking at this you can see that this is actually a much more complex representation of our so-called

44
00:03:57,010 --> 00:04:06,670
propose problem the problem that we all originate and this which was why is the three x plus fifty there's

45
00:04:06,670 --> 00:04:15,460
only one input and there's only again one so called slope plus one intercept that we needed to estimate

46
00:04:15,880 --> 00:04:22,570
but when you look at this there are three inputs obviously it's it's not a problem that there is representation

47
00:04:22,600 --> 00:04:31,300
of the problem that we proposed and is far too complex to represent our problem anyway and is likely

48
00:04:31,300 --> 00:04:38,590
to not what it will be over q and we'll give you a false answer as well so let's look at something much

49
00:04:38,590 --> 00:04:47,320
simpler that we caught up in our coal lab earlier and this is the model that we propose and then compile

50
00:04:47,470 --> 00:04:48,900
and fit as well.

51
00:04:49,180 --> 00:04:57,300
For with this what we have was one input layer which would define it in our P.F. Crystal layer dot dense

52
00:04:57,760 --> 00:05:05,860
we had input shape of 1 which is this one here and for the upper layer we had the units equal to 1 Again

53
00:05:05,890 --> 00:05:14,770
that's 1 output which is what's shown here in our op layer so this is our dense layer of input to this

54
00:05:14,770 --> 00:05:21,910
that's our dense layer of course it's not very dense is only really one input here one output one one

55
00:05:21,910 --> 00:05:29,620
neuron and one output and in terms of the representation this is what it looks like and one is a good

56
00:05:29,620 --> 00:05:37,530
2 The X1 which is the input multiplied by the weight that eleven plus the bias term if you look at the

57
00:05:37,540 --> 00:05:45,720
representation of this is much much closer to the problem statements that we have of course the problems

58
00:05:45,760 --> 00:05:54,620
Edmonds were created by a real function and it's not like the reality is that in real life you're not

59
00:05:54,620 --> 00:05:57,030
going to be able to see that real function.

60
00:05:57,050 --> 00:06:04,190
So we are using the algorithm to actually estimate the function itself.

61
00:06:04,250 --> 00:06:09,650
The final point that I wanted to make here is that our neural network quite often a much more complex

62
00:06:09,650 --> 00:06:16,670
than either of these there can be multiple layers and each layer have many more neurons than one or

63
00:06:16,670 --> 00:06:20,750
two could be three could be substantially higher.

64
00:06:20,750 --> 00:06:29,320
And there are also other So core steps that you can perform in between layers as well.

65
00:06:29,390 --> 00:06:31,190
Right now we only have one hit in here.

66
00:06:31,220 --> 00:06:32,990
It can be four or five.

67
00:06:33,050 --> 00:06:40,070
And many many more but we'll see different architectures too for the flexibility to solve the problems

68
00:06:40,070 --> 00:06:41,120
at hand.

69
00:06:41,210 --> 00:06:42,430
With that I'm going to start.

70
00:06:42,470 --> 00:06:43,760
Thank you for watching.

71
00:06:43,760 --> 00:06:50,150
Just to summarize we've talked about a more complex neural network architecture vs. a much simpler one

72
00:06:50,180 --> 00:06:54,500
which solve our problem and look forward to seeing you in the next lesson.
