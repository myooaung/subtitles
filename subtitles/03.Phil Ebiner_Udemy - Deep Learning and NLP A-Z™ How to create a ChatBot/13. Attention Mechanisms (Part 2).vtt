WEBVTT
1
00:00:00.190 --> 00:00:03.320
So that's how attention you're are going to continue with the rest.

2
00:00:03.330 --> 00:00:07.260
Example let's have some look at something fun.

3
00:00:07.350 --> 00:00:16.530
So this is from a research paper where your show banjo was one of the co-authors and this is a translation

4
00:00:16.530 --> 00:00:24.030
example from English French So you've got the agreement on the European area was signed in August 1992.

5
00:00:24.030 --> 00:00:24.710
Full stop.

6
00:00:24.790 --> 00:00:30.310
And then this whole translated into French law called Soul zone economy.

7
00:00:30.450 --> 00:00:34.650
And I see it in me.

8
00:00:34.980 --> 00:00:43.860
But one of clubs of all dues for you and Paul Anderson's.

9
00:00:44.080 --> 00:00:44.610
There we go.

10
00:00:44.620 --> 00:00:46.210
That's me showing off.

11
00:00:46.600 --> 00:00:54.780
We haven't learned it in a while but so basically what is going on here.

12
00:00:55.100 --> 00:01:04.880
The trend this these white dots is where the neural network was paying attention to as it was translating.

13
00:01:04.890 --> 00:01:11.740
So remember like the way it's so those white dots they signify the way on every single word as it was

14
00:01:11.740 --> 00:01:12.420
translated.

15
00:01:12.520 --> 00:01:22.780
So we gave them all that for this luxury VIP super power of seeing what's in the in the whole input

16
00:01:22.780 --> 00:01:24.160
so there's a whole input.

17
00:01:24.310 --> 00:01:30.370
And so we gave the neural network a little luxury to look at the input every single time as we just

18
00:01:30.370 --> 00:01:31.050
discussed.

19
00:01:31.210 --> 00:01:33.330
And then we observed what did it do.

20
00:01:33.340 --> 00:01:37.720
So we kind of like in our stuff I love these a lot of the things we're studying in neural net as if

21
00:01:37.720 --> 00:01:46.500
it was an alien or some kind of another species and we're trying to see what it is thinking or doing.

22
00:01:46.710 --> 00:01:47.550
So let's have a look.

23
00:01:48.310 --> 00:01:49.810
You can see that here.

24
00:01:50.110 --> 00:01:51.250
So here it's pretty straightforward.

25
00:01:51.250 --> 00:01:52.270
It's frustrating.

26
00:01:52.460 --> 00:01:59.740
It is saying to construct things like word by word so first word is translated into first word second

27
00:01:59.740 --> 00:02:00.160
word.

28
00:02:00.250 --> 00:02:05.200
So the second word is the second or third word third or third word comes from the third word and so

29
00:02:05.200 --> 00:02:05.320
on.

30
00:02:05.320 --> 00:02:12.190
But then when you get to LA it's not just looking at the word but it's also looking at the word area.

31
00:02:12.280 --> 00:02:13.230
Very interesting.

32
00:02:13.430 --> 00:02:14.720
So they're quite far apart.

33
00:02:14.840 --> 00:02:24.960
Well because in French there are two genders law all female and male and each one of them has a different.

34
00:02:25.150 --> 00:02:29.240
So if a female is a lot for male he said Look eat.

35
00:02:29.470 --> 00:02:38.580
And then for plural you actually say something else you say like an eagle should be the areas in French

36
00:02:38.580 --> 00:02:42.490
should be left L E S results.

37
00:02:43.480 --> 00:02:47.860
Like with an S M and so areas there is but then this one would change to a plural so we actually have

38
00:02:48.180 --> 00:02:51.160
one for female one for male and one for plural.

39
00:02:51.310 --> 00:02:54.830
I know you like the French French supercup is not.

40
00:02:54.840 --> 00:02:59.320
It's a really fun language but it's it's aspecific of the language.

41
00:02:59.380 --> 00:03:03.140
And so the machine knows this algorithm knows this and it looks OK.

42
00:03:03.160 --> 00:03:04.800
So I want to translate the.

43
00:03:05.080 --> 00:03:09.560
But in order to just say I need more information so it's paying attention.

44
00:03:09.760 --> 00:03:16.280
As we saw through the weights to the war to the now and it's very clever it didn't go for the adjective.

45
00:03:16.310 --> 00:03:21.570
It is an adjective European economic I guess adjectives didn't pay attention to the adjectives.

46
00:03:21.580 --> 00:03:28.890
Maybe a little bit here maybe by accident but it paid a Aquatica significant attention to the word area.

47
00:03:28.990 --> 00:03:29.740
Very interesting.

48
00:03:29.740 --> 00:03:30.250
Right.

49
00:03:30.490 --> 00:03:31.060
OK.

50
00:03:31.070 --> 00:03:35.850
So and decided to put lot which is correct.

51
00:03:36.060 --> 00:03:38.240
Next is like zone.

52
00:03:38.840 --> 00:03:43.070
So here you can see it's paying attention to area again mostly like a little bit.

53
00:03:43.070 --> 00:03:45.610
To this I don't really know why.

54
00:03:45.620 --> 00:03:54.620
Maybe it's like anticipating something or not natural then economy.

55
00:03:54.830 --> 00:03:56.300
Again it's Pantages right.

56
00:03:56.300 --> 00:04:00.220
We're a little bit too being European interstage European.

57
00:04:00.500 --> 00:04:00.940
OK.

58
00:04:00.950 --> 00:04:03.350
So you can see it's kind of like and here's your it it's reversed.

59
00:04:03.350 --> 00:04:04.910
Right so here you go.

60
00:04:05.180 --> 00:04:10.730
In English you go the European Economic Area in France you go.

61
00:04:10.880 --> 00:04:18.200
LA area economic European So absolutely the opposite way and the machine is paying attention in the

62
00:04:18.200 --> 00:04:25.630
correct order of that with all OK then we have this part.

63
00:04:25.720 --> 00:04:32.740
This is interesting because here you have was assigned two words in French I have three words I see.

64
00:04:33.090 --> 00:04:40.840
It's the composer is the time like in English we have lots of times we have almost perfect 10 years

65
00:04:40.840 --> 00:04:45.040
past perfect present for the French that you also have a lot of time and this was called picosecond

66
00:04:45.090 --> 00:04:45.940
present and so

67
00:04:48.730 --> 00:04:49.930
was signed.

68
00:04:49.930 --> 00:04:57.430
So I had that is that part and here is really cool because if you study French You'll remember that

69
00:04:57.790 --> 00:05:02.330
you haven't been the cases that for it to say like this.

70
00:05:02.380 --> 00:05:13.090
This construct of say oh I hope this is in the if not you can correct me but this construct is in order

71
00:05:13.090 --> 00:05:13.780
to create it.

72
00:05:13.780 --> 00:05:21.120
You need like these words are interdependent so depending on what's what's

73
00:05:23.900 --> 00:05:30.260
what word do you have here is the supporting verb what Verby half years supporting war will be either

74
00:05:30.450 --> 00:05:35.170
avoir or être and this case is avoir because your main word is être.

75
00:05:35.230 --> 00:05:40.340
It's a bit it might sound a bit confusing but the point is that you in order to decide on these two

76
00:05:40.340 --> 00:05:46.100
words you cannot decide on them one by one you can say was translated this time as it is to decide on

77
00:05:46.100 --> 00:05:46.970
these two words.

78
00:05:47.210 --> 00:05:49.600
You have to look at these two words together.

79
00:05:49.670 --> 00:05:51.070
So it was like was signed.

80
00:05:51.090 --> 00:05:58.720
OK then we know that it's it's Iott and then so and then see here is.

81
00:05:58.810 --> 00:06:03.310
So there that's why you have this very very interesting stuff.

82
00:06:03.650 --> 00:06:07.990
And so that was the translation.

83
00:06:08.000 --> 00:06:10.860
That's the sources from the paper if you want to have a look.

84
00:06:11.150 --> 00:06:21.820
And then finally we have this other paper where Christopher Manning is one of the authors from Stanford

85
00:06:22.440 --> 00:06:23.400
not mistaken.

86
00:06:23.440 --> 00:06:27.860
So here is a comparison again this is related to recession as you can see.

87
00:06:27.860 --> 00:06:35.700
Attention is a very powerful mechanism very well use in translation because there differencing languages

88
00:06:35.980 --> 00:06:38.560
are quite can be quite significant.

89
00:06:38.560 --> 00:06:40.300
And that tension really helps.

90
00:06:40.330 --> 00:06:45.520
So here you just got a comparison of different models all of these models at the top.

91
00:06:45.610 --> 00:06:48.770
Yeah all of them have attention as far as I know.

92
00:06:48.780 --> 00:06:54.190
I'm not sure about this this Black want but I think it also has an edge and you're old and have attention

93
00:06:54.310 --> 00:06:56.420
and these this one doesn't have attention.

94
00:06:56.590 --> 00:07:04.390
And on the X-axis you have how long the sentences are how many sentence how many words and blow is the

95
00:07:05.260 --> 00:07:05.920
score.

96
00:07:05.920 --> 00:07:13.600
It's called the bilingual evaluation under study and it's a score that algorithms or even humans are

97
00:07:13.600 --> 00:07:20.200
used like that score is used to rate people and and my models on how they're well they're translating.

98
00:07:20.410 --> 00:07:24.910
And so like a score of 25 is 20 25 is pretty good.

99
00:07:25.150 --> 00:07:29.310
I'm not an expert in blood like as far as I know it should be like between 0 and 1.

100
00:07:29.350 --> 00:07:37.210
What I'm understanding here is they're like multiplying by 100 but it doesn't really matter at this

101
00:07:37.210 --> 00:07:39.870
stage like what we're interested in is the relative contrast.

102
00:07:39.880 --> 00:07:44.400
So like the state of that mortals right now they achieve like just over 30.

103
00:07:44.410 --> 00:07:47.980
But there I don't think I've seen a model that achieved over 40.

104
00:07:47.980 --> 00:07:52.540
And this paper actually was 2015 paper so it's really egregious.

105
00:07:52.780 --> 00:07:59.110
But the point is that if we compare them side by side against each other you will see that when you

106
00:07:59.110 --> 00:08:03.730
don't have attention you you're doing quite OK for up to three hours and then you drop off.

107
00:08:03.730 --> 00:08:08.150
So because the sentences get so long you don't you're not paying attention.

108
00:08:08.470 --> 00:08:14.460
You can just translate them easily but with attention even when the scientists get long Like been over

109
00:08:14.470 --> 00:08:24.130
40 you kind of flatlined so you get a more or less consistent output outcome of your translation.

110
00:08:24.190 --> 00:08:25.940
So that's almost it.

111
00:08:26.080 --> 00:08:30.260
Before I just remembered one more thing before we finish off.

112
00:08:30.340 --> 00:08:32.740
There's also this is called global attention.

113
00:08:32.740 --> 00:08:39.490
When you have this whole park there's also a concept called what's it called it's called local attention

114
00:08:39.850 --> 00:08:48.150
when instead of this whole Red Square you only look at like two or three words in the sun like that

115
00:08:48.160 --> 00:08:51.840
for example and then out of them you have the waves and so on.

116
00:08:52.090 --> 00:08:56.050
Or just a couple words and that is more similar to what humans do.

117
00:08:56.050 --> 00:09:01.510
Like if you have like a 20 word sentence and you're translating it or if you're trying to respond to

118
00:09:01.510 --> 00:09:07.280
it you don't look at the whole sentence every time you think I when you you just look at parts of this.

119
00:09:07.630 --> 00:09:09.490
And that's more close to what humans do.

120
00:09:09.490 --> 00:09:15.360
And it's actually better and better computationally for machines a bit harder to implement.

121
00:09:15.370 --> 00:09:24.250
Bit harder for you to like the the mathematics and the conceptually horas is easier to kind of brute

122
00:09:24.250 --> 00:09:28.590
force the global tetch rather than picking out where we're going to look at it locally and stuff.

123
00:09:28.810 --> 00:09:33.040
But it just that you are where there's two different concepts as global tension local tension.

124
00:09:33.040 --> 00:09:36.720
The one we talked about is global Tige.

125
00:09:37.120 --> 00:09:42.610
And if you'd like to learn more about attention then the paper is just having a look at this called

126
00:09:42.610 --> 00:09:48.060
effective approaches to attention based neuro translation by means tongue.

127
00:09:48.440 --> 00:09:56.890
If I'm pressing it correctly and others including crews from Manning and 2015 paper again this is about

128
00:09:56.890 --> 00:09:59.700
translation but very very similar conceptually.

129
00:09:59.740 --> 00:10:03.260
So everything you'll learn there is still applicable for chat boards.

130
00:10:03.570 --> 00:10:09.380
And and plus you'll get some additional examples of how sex sex is used in translation.

131
00:10:09.610 --> 00:10:13.620
All right so I've opened the door and I look forward to seeing an extent.

132
00:10:13.730 --> 00:10:15.520
Then enjoy.

133
00:10:15.580 --> 00:10:17.630
Natural Language Processing.
