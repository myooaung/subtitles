WEBVTT
1
00:00:00.600 --> 00:00:05.370
So there you go that's the vector we're working with now that we know how to construct it.

2
00:00:05.460 --> 00:00:15.300
It is possible to encode your text in the version of like a vector for again for the standing purposes.

3
00:00:15.300 --> 00:00:17.020
We're not going to work with the vector.

4
00:00:17.040 --> 00:00:18.480
We're going to work with the text.

5
00:00:18.480 --> 00:00:24.180
So we're going to create our neural network working with the text but just keep in mind that in a machine

6
00:00:24.270 --> 00:00:31.730
sense we do need to work numbers and it is possible to construct that vector of numbers from our email

7
00:00:33.080 --> 00:00:38.220
and the vector of numbers will be about the same size as the email itself.

8
00:00:38.630 --> 00:00:40.090
Several have this.

9
00:00:40.370 --> 00:00:41.140
At the start.

10
00:00:41.160 --> 00:00:48.180
Us and and the other thing here is you can see we drop the S.O.S.

11
00:00:48.180 --> 00:00:50.060
Because it's not that important.

12
00:00:50.060 --> 00:00:54.470
Every sentence will always start with our software so it won't make any difference if we see it in there.

13
00:00:54.470 --> 00:00:58.360
So as the start or not we know we're not going to even bother with it.

14
00:00:58.360 --> 00:01:04.310
We're just going to keep these as as important as we'll see why just now it's important because it will

15
00:01:04.310 --> 00:01:08.310
dictate when the output will terminate.

16
00:01:08.650 --> 00:01:15.020
OK so basically what we're doing is we're feeding these values into our recurrent neural network.

17
00:01:15.020 --> 00:01:24.020
So as we discussed here we've got so this value goes into our first element of our recurring neural

18
00:01:24.020 --> 00:01:28.980
network in the first layer and then it feeds into here.

19
00:01:28.990 --> 00:01:31.840
So we'll talk we'll talk about this in a second how this works.

20
00:01:31.850 --> 00:01:39.620
This this is just a classic rock or neural network but will mention LSD as well just now as you will

21
00:01:39.620 --> 00:01:40.150
see.

22
00:01:40.310 --> 00:01:47.420
But for now just imagine this is a classic recurring neural network feeding into itself through time

23
00:01:47.580 --> 00:01:52.670
as a time and so we're adding new information like some information about an output that we would like

24
00:01:53.030 --> 00:01:54.090
a memory cell.

25
00:01:54.200 --> 00:01:59.180
So it's kind of is feeding into itself plus we are putting in and you are in quotes and so on.

26
00:02:00.310 --> 00:02:05.680
Then here we are starting to see an output so soon as we have the Endal sentence that's what we hear

27
00:02:06.160 --> 00:02:12.740
that's when we're going to revert to that structure that we talked about.

28
00:02:12.750 --> 00:02:20.190
So if I jump back a couple of slides this one as you can see with eating and feeding in as it ends then

29
00:02:20.250 --> 00:02:24.520
we start to see how that's architecture creating.

30
00:02:24.540 --> 00:02:33.140
So here we've got the outputs then then it's then it's feeding into the next element that will get the

31
00:02:33.140 --> 00:02:36.020
output next element output next element.

32
00:02:36.080 --> 00:02:38.000
End of sentence and that's it.

33
00:02:38.000 --> 00:02:44.450
So let's discuss this in a bit more detail as you can see here's the text of the email.

34
00:02:44.450 --> 00:02:48.060
Hello Carol chicken if you're back back to us tomorrow.

35
00:02:48.280 --> 00:02:52.430
And so the ideal response is like yes Ambac and of sentence that's what we're aiming for.

36
00:02:52.430 --> 00:02:56.780
This is our ideal scenario how we're going to train it.

37
00:02:56.810 --> 00:02:59.200
We're going to talk about that in the next tutorial.

38
00:02:59.210 --> 00:03:03.980
For now we want to understand the architecture that will facilitate this ideas.

39
00:03:04.850 --> 00:03:07.350
So what happens here.

40
00:03:07.370 --> 00:03:08.780
We've got all that input.

41
00:03:08.780 --> 00:03:14.330
So as as I regarding neural network were able to feed in whatever number of words is going to be in

42
00:03:14.330 --> 00:03:21.410
this case one two three comma three four five six seven eight nine ten eleven words include commas and

43
00:03:21.410 --> 00:03:30.680
full stops and 12 including the US or we could fit in an email which is like a thousand words long.

44
00:03:30.710 --> 00:03:36.010
Same approach just keeps just the number of steps would be longer.

45
00:03:36.470 --> 00:03:42.860
And then once that process we want to start getting output so this this will be like a parameter in

46
00:03:42.860 --> 00:03:43.550
your algorithm.

47
00:03:43.550 --> 00:03:49.670
You'll see that so this will be a parameter in your algorithm which would relate to which like will

48
00:03:49.670 --> 00:03:58.700
be adjust to the length of your input which is your email would be a variable input and that's exactly

49
00:03:58.700 --> 00:03:59.510
what we're up to.

50
00:03:59.690 --> 00:04:03.380
And then once that's done the network is going to be start predicting approach.

51
00:04:03.380 --> 00:04:08.240
So how does this work how does it know that it has to have like 4 outputs.

52
00:04:08.240 --> 00:04:09.010
Why not three.

53
00:04:09.050 --> 00:04:09.750
Why not five.

54
00:04:09.770 --> 00:04:10.560
Why not.

55
00:04:10.820 --> 00:04:16.490
Well the way it works is with every step starting when it starts production and again is totally up

56
00:04:16.490 --> 00:04:23.630
to us to say when we wanted to start predicting and the way we are doing it is as soon as this input

57
00:04:23.630 --> 00:04:25.640
is over that's when the predictions.

58
00:04:25.910 --> 00:04:34.910
So this first prediction in fact any one of these predictions there's thousand options 20000 possible

59
00:04:35.300 --> 00:04:40.980
predictions that all 20000 possible results that the network can pick.

60
00:04:40.980 --> 00:04:43.500
So let's start with this first one here.

61
00:04:43.610 --> 00:04:49.150
Once all of this has come in in this make a prediction there's 20000 variance.

62
00:04:49.160 --> 00:04:53.690
So the way the neural network will do and again we're not talking about training right now we're talking

63
00:04:53.690 --> 00:05:03.980
about the end result the end result is the neural network will spit out a probability score for the

64
00:05:04.430 --> 00:05:05.750
options that it has.

65
00:05:05.930 --> 00:05:14.720
And so it'll assign probabilities to the different words that it can pick from from you know from A

66
00:05:14.720 --> 00:05:26.500
to two Apple to attend to all the way through to the Zabra and whatever other words they are in between.

67
00:05:26.510 --> 00:05:28.480
So you don't assign probabilities.

68
00:05:28.680 --> 00:05:34.920
And basically we will know that's likely it will.

69
00:05:35.180 --> 00:05:39.410
It will analyze all the words and it will pick the one with the highest probability.

70
00:05:39.410 --> 00:05:48.740
So in this case yes the world of believe it was yes out of the 20000 that's the where then is residual

71
00:05:48.740 --> 00:05:56.010
information from the network is propagate forward in inheres passed on to the next cell at here.

72
00:05:56.060 --> 00:06:01.590
The next one that is picked as I'm here next month is back here.

73
00:06:01.600 --> 00:06:07.660
Next one is is and I've said that so end of sentence as you remember is one of those 20000 options.

74
00:06:07.760 --> 00:06:12.380
And as soon as End of sentence is picked and that's when we know when to stop.

75
00:06:12.540 --> 00:06:14.450
So it could have been longer.

76
00:06:14.540 --> 00:06:22.700
In short so here is sort of just actually up to the network to generate the right response and hopefully

77
00:06:22.700 --> 00:06:26.580
we'll know how to do that based on the training that it's been all talk about the training again in

78
00:06:26.580 --> 00:06:27.150
the next trial.

79
00:06:27.150 --> 00:06:35.130
So once the response is generated by the this whole this is one of us is generate this whole thing and

80
00:06:36.660 --> 00:06:41.710
something Jannot time like if training was done well there will be an S.O.S.

81
00:06:41.730 --> 00:06:48.150
That just means it understands that sentences need to and at some point another way you do it is you

82
00:06:48.150 --> 00:06:52.920
can have a limit you could say not more than 50 words and they require something like that and then

83
00:06:52.920 --> 00:06:53.720
cut it off.

84
00:06:53.880 --> 00:06:56.210
So they just go forever.

85
00:06:56.310 --> 00:07:00.400
So here's an example of an LACMA image.

86
00:07:00.610 --> 00:07:13.730
And again this is from the R and R n n information and so here it's a type of recurrent neural network.

87
00:07:14.080 --> 00:07:19.120
And here what is happening why is why I brought this up is because in an LACMA you can see we've got

88
00:07:19.120 --> 00:07:27.040
that part of it here the memory cell but also an analysis T.M. we're feeding back as we remember feeding

89
00:07:27.040 --> 00:07:34.490
back the the value so this value that we have or is here is very you can see the value that we're putting

90
00:07:34.640 --> 00:07:35.170
up here.

91
00:07:35.180 --> 00:07:39.800
We're actually feeding back into the network as you can see that sort of happen here we just have the

92
00:07:39.800 --> 00:07:41.590
memory going ahead.

93
00:07:41.780 --> 00:07:49.340
The memory saw that top one is like in comparison you can see that there is cell there is a memory going

94
00:07:49.340 --> 00:07:52.470
through but we're not feeding this output back in.

95
00:07:52.850 --> 00:07:54.600
So that's what we're going to fix.

96
00:07:54.620 --> 00:07:57.890
We're going to take this output and feed it back in.

97
00:07:57.890 --> 00:08:01.910
So yes is the output but also it's going to go in here.

98
00:08:01.920 --> 00:08:10.280
So essentially we're using an Alice T.M. network over here to make sure that we're taking into account

99
00:08:10.280 --> 00:08:18.350
not just the memory was blowing through but we're using output that we generate before and that will

100
00:08:18.350 --> 00:08:21.650
help us help the network preserve understand the meaning.

101
00:08:21.650 --> 00:08:30.260
So it just works better this way just it just can create better sentences like that because it remembers

102
00:08:30.260 --> 00:08:36.500
what not only just the overall memory that's been passed through the network that's updated through

103
00:08:36.500 --> 00:08:43.640
the cells but also because it remembers the immediately preceding word and helps it formulate better

104
00:08:43.640 --> 00:08:44.950
sentences.

105
00:08:44.960 --> 00:08:52.600
So that's that's our architecture that's how we're going to construct it.

106
00:08:52.640 --> 00:09:01.700
Over here we've got our cells with feeding and information like creating that community of understanding

107
00:09:03.020 --> 00:09:12.860
creating that vector of meaning for us so by by here by the time we get here we've we accumulated a

108
00:09:13.460 --> 00:09:21.250
whole vector and this is going to be the meaning of the sentences basically contained in this cell here.

109
00:09:21.410 --> 00:09:25.120
And from here we're starting to extract the answer.

110
00:09:25.130 --> 00:09:30.980
And that's why these parts have their names in coater So we're taking all this information and we have

111
00:09:32.070 --> 00:09:42.090
at the start we're encoding it into this one cell and then we are decoding it or providing a response

112
00:09:42.210 --> 00:09:44.120
through this part so this is the encoder.

113
00:09:44.160 --> 00:09:45.620
This is the decode.

114
00:09:45.960 --> 00:09:48.540
So that's the overall kind of

115
00:09:51.000 --> 00:09:55.080
overview of the architecture of a sequence to sequence model.

116
00:09:55.080 --> 00:09:57.640
It's got a first part that encodes the meaning.

117
00:09:57.820 --> 00:10:04.050
Then it's got a second part that decodes the answer from the encoded meaning through and both of them

118
00:10:04.050 --> 00:10:05.490
work through an order.

119
00:10:05.490 --> 00:10:11.220
And so basically we have to recurrent neural networks attached to each other here of course is a very

120
00:10:11.220 --> 00:10:12.780
basic example.

121
00:10:12.860 --> 00:10:16.490
And like I know it's quite complex but once we talk about the training it takes to Tauriel hopefully

122
00:10:16.510 --> 00:10:19.270
will become a bit more intuitive how this works.

123
00:10:19.440 --> 00:10:25.920
But this is architecture and again this is a very basic example because in reality they actually made

124
00:10:26.970 --> 00:10:29.630
you can make them deep you can make them deeper than that.

125
00:10:29.760 --> 00:10:31.790
And the way it works is something like this.

126
00:10:31.860 --> 00:10:41.050
So you have to encode that vector meaning you have layers of recurring neural networks and then once

127
00:10:41.050 --> 00:10:42.640
that's done then you start decoding.

128
00:10:42.640 --> 00:10:47.110
So they not just feed into each other into themselves if you into the next layer is the next layer and

129
00:10:47.110 --> 00:10:47.680
so on.

130
00:10:47.890 --> 00:10:54.370
And as we know like more neurons means more flexibility for the network to assign different tasks to

131
00:10:54.370 --> 00:11:02.200
those neurons assigned different roles and encode the meaning better and understand basically in human

132
00:11:02.200 --> 00:11:04.530
language means understanding the senses.

133
00:11:04.900 --> 00:11:09.210
All right so that was the sequence to sequence architecture.

134
00:11:09.400 --> 00:11:15.630
I know it's quite a lot to take in but bear with me in the next term we'll talk about the training.

135
00:11:15.820 --> 00:11:20.950
And of course once you go into the practical tutorials all of this will come to you.

136
00:11:20.980 --> 00:11:25.750
Once you have your hands on it it will all make sense how you build it once you're able to actually

137
00:11:25.750 --> 00:11:29.570
constructed and set the parameters set the architecture.

138
00:11:29.620 --> 00:11:31.570
All of this will come in handy.

139
00:11:31.690 --> 00:11:37.860
So don't stress out if there are some parts that are bits like is still a bit vague.

140
00:11:38.020 --> 00:11:39.240
It will all come of time.

141
00:11:39.240 --> 00:11:39.750
It's all.

142
00:11:39.760 --> 00:11:43.820
It's definitely a worthwhile investment of time to go.

143
00:11:43.960 --> 00:11:49.930
To have gone through this tutorial and on the equitorial We'll talk about the sequence the sequence

144
00:11:49.930 --> 00:11:52.150
training and I look forward to seeing you there.

145
00:11:52.150 --> 00:11:55.460
Until then enjoy natural language processing.
