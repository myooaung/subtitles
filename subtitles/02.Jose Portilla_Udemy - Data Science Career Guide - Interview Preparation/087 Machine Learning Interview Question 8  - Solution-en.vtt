WEBVTT
1

00:00:05.280 --> 00:00:07.110

Welcome everyone to the solution for machine learning.



2

00:00:07.110 --> 00:00:12.550

Question number 8 this question was just in general asking what is overfitting causes and what ways



3

00:00:12.550 --> 00:00:14.810

can you attempt to avoid overfitting.



4

00:00:15.580 --> 00:00:20.980

So overfitting is when the machine learning model does not generalize well to data it has not seen before



5

00:00:21.040 --> 00:00:22.610

such as test data.



6

00:00:22.630 --> 00:00:28.390

What happens is overfit the training data and that's usually indicating it has too much complexity in



7

00:00:28.390 --> 00:00:31.980

regards to the training data size.



8

00:00:32.000 --> 00:00:37.310

So here we can see a simple visual example of underfeeding good fit and overfitting.



9

00:00:37.310 --> 00:00:44.600

So this is just some sort of fit on a continuous label and we can see here under fitting we have a high



10

00:00:44.660 --> 00:00:48.690

error on not just training points but also test points.



11

00:00:48.890 --> 00:00:55.400

So when we end up getting a new point on the actual continuous value here basically what to happen is



12

00:00:55.400 --> 00:01:00.800

going to receive a large error so under fitting that's exemplified when you have large errors in both



13

00:01:00.800 --> 00:01:06.360

your test set and your training set a good fit is when you have low errors in both your training set



14

00:01:06.590 --> 00:01:08.260

and your test set.



15

00:01:08.270 --> 00:01:14.060

Now overfitting what happens is you have a very complex model and on your training set it performs very



16

00:01:14.060 --> 00:01:19.110

well because basically it ended up fitting perfectly to every single point in your training set.



17

00:01:19.310 --> 00:01:24.090

But as you begin to test it on a test set essentially data it hasn't seen before.



18

00:01:24.170 --> 00:01:28.880

You end up getting a much larger error than on your training data and that's a prime example that your



19

00:01:28.880 --> 00:01:32.020

overfitting when you did really well on your training data.



20

00:01:32.120 --> 00:01:34.710

Perform poorly on your test data.



21

00:01:34.740 --> 00:01:40.150

So there's many ways to attempt to fix overfitting one thing is to increase the training data size that's



22

00:01:40.160 --> 00:01:44.900

not always an option the Pentagon how much data you have but it is a way to help alleviate overfitting



23

00:01:45.270 --> 00:01:48.470

in other great way to fix overfitting is regularization.



24

00:01:48.530 --> 00:01:50.300

And there's lots of different techniques for this.



25

00:01:50.300 --> 00:01:53.270

There's L-1 regularisation L2 regularisation.



26

00:01:53.480 --> 00:01:58.640

You can also try early stoppings so you can try for certain algorithms to stop them earlier.



27

00:01:58.640 --> 00:02:02.720

That way they don't get overly complex and overfit to your training data.



28

00:02:02.840 --> 00:02:05.730

And then there's also other things like different methods of evaluating.



29

00:02:05.750 --> 00:02:10.610

You can do things like k fold cross-validation although that won't directly be able to fix the issue



30

00:02:10.700 --> 00:02:13.940

or if fitting it is something you can take into account.



31

00:02:14.000 --> 00:02:15.320

OK I hope this was helpful.



32

00:02:15.320 --> 00:02:17.930

Definitely check out the resources in the guide book on this one.



33

00:02:17.930 --> 00:02:18.990

I'll see you at the next lecture.



