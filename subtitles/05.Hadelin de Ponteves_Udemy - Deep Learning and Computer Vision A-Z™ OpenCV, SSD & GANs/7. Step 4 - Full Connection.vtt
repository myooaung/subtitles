WEBVTT
1
00:00:00.520 --> 00:00:02.800
Hello and welcome back to the course and deep learning.

2
00:00:02.800 --> 00:00:06.140
Today we're finally at STEPNELL before full connection.

3
00:00:06.310 --> 00:00:08.210
So what is this step all about.

4
00:00:08.440 --> 00:00:16.990
Well in this step we're adding a whole artificial neural network to our convolutional neural network

5
00:00:17.000 --> 00:00:22.450
so to all of the things that we've done so far which are convolution pooling and flattening.

6
00:00:22.510 --> 00:00:24.910
Now we're adding a whole new.

7
00:00:24.930 --> 00:00:28.990
And then on the back of that how intense is that.

8
00:00:28.990 --> 00:00:32.350
That is just that is something that is definitely something.

9
00:00:32.590 --> 00:00:36.730
And so here we've got the input layer we've got a fully connected plan.

10
00:00:36.740 --> 00:00:42.580
I'll put there and by the way the fully connected Lehre in the artificial neural networks we used to

11
00:00:42.580 --> 00:00:47.650
call them hidden layers and here we're calling them fully connected because they are hidden lairs but

12
00:00:47.650 --> 00:00:53.500
at the same time they're a more specific type of fiddlers that are fully connected in artificial neural

13
00:00:53.500 --> 00:00:57.510
networks hidden letters don't have to be fully connected.

14
00:00:57.520 --> 00:01:02.230
Whereas in convolutional neural networks we're going to be using fully connected letters and that's

15
00:01:02.230 --> 00:01:05.640
why they're generally called fully connected Lares.

16
00:01:05.770 --> 00:01:11.620
And so basically that whole column or vector of outputs that we have after the flattening we're passing

17
00:01:11.620 --> 00:01:18.160
it into the input learned here we've got a very simplified example just for illustration purposes.

18
00:01:18.160 --> 00:01:26.770
And what the main purpose of the artificial neural network is is to combine our features into more attributes

19
00:01:26.770 --> 00:01:28.960
that predict the Klaas's even better.

20
00:01:28.960 --> 00:01:37.660
So we already in our vector of outputs in the Flaten of the flattened result from what we've really

21
00:01:37.660 --> 00:01:43.750
done we have some features encoded in the numbers in that vector and they can already do probably a

22
00:01:43.750 --> 00:01:51.730
pretty good job at predicting what's what's Clauss we're looking at whether it's a dog or a cat or whether

23
00:01:51.730 --> 00:01:53.840
it's a tumor or not a tumor and so on.

24
00:01:53.890 --> 00:02:00.610
But at the same time we know that we have this structure called artificial neural network which is designed

25
00:02:00.610 --> 00:02:07.810
which which has a purpose of dealing with attributes and coming out or dealing with features and coming

26
00:02:07.810 --> 00:02:16.120
up with new attributes and combining attributes together to even better predict things that we're trying

27
00:02:16.120 --> 00:02:20.360
to predict and we know that from the previous parts so why not leverage that.

28
00:02:20.440 --> 00:02:22.750
And that's exactly what the plan here is.

29
00:02:22.750 --> 00:02:29.140
So how about we pass on those values into an artificial neural network and let it even further optimize

30
00:02:29.140 --> 00:02:30.350
everything that we're doing.

31
00:02:30.640 --> 00:02:31.900
And so that's what we're going to be doing.

32
00:02:31.900 --> 00:02:36.390
But let's look at a more realistic example because this one is a bit too simple.

33
00:02:36.610 --> 00:02:43.990
So here we've got a better looking artificial neural network where we have five attributes on the inputs

34
00:02:43.990 --> 00:02:51.040
that we have in the first unless we have six neurons in the second or in the second fully connected

35
00:02:51.040 --> 00:02:55.510
Larry have eight neurons and then we have two outputs one for dog and one for cat.

36
00:02:55.630 --> 00:03:02.240
And so an important thing to talk of for us to talk about here is that why do we have two outputs.

37
00:03:02.240 --> 00:03:09.100
We're kind of used to having only one output in our artificial neural networks Well one output is for

38
00:03:09.100 --> 00:03:14.740
kind of when you're predicting a numerical value when you print when you're running a regression type

39
00:03:14.740 --> 00:03:15.480
of problem.

40
00:03:15.760 --> 00:03:22.840
But when you're doing classification you need an output Proclus except for the exception is when you

41
00:03:22.840 --> 00:03:27.940
have just two clusters like we have two classes here dog and cat and we could have just done one output

42
00:03:27.970 --> 00:03:33.790
and made it a binary output and said One is a dog and zeros a cat and that would have worked totally

43
00:03:33.790 --> 00:03:38.760
fine and actually what you'll see had lunch do that in the practical tutorials and that's how they'll

44
00:03:38.770 --> 00:03:39.250
be structured.

45
00:03:39.250 --> 00:03:46.090
But at the same time if you have more than two categories for instance dogs cats and birds then you

46
00:03:46.090 --> 00:03:52.420
have to have a neuron per every category and that's why we're going to practice with two categories

47
00:03:52.420 --> 00:03:58.320
in this example so that we know what to expect if we ever have more than two categories.

48
00:03:58.550 --> 00:04:00.010
And so what's going to be happening here.

49
00:04:00.010 --> 00:04:05.260
So we've already done all the groundwork we've done the convolution we've done the pooling and the flattening

50
00:04:05.620 --> 00:04:10.570
and now the information is going to go through the artificial neural network so let's have a look at

51
00:04:10.570 --> 00:04:12.300
how the other all happens.

52
00:04:12.340 --> 00:04:18.460
There is information going through from the very start from the moment when the image is processed and

53
00:04:18.610 --> 00:04:23.920
kind convoluted convolved then puled flattened and then through the artificial neural network all four

54
00:04:23.920 --> 00:04:30.720
steps and then a prediction is made and we'll see how this happens in a moment will be very very interesting.

55
00:04:30.730 --> 00:04:32.920
But for now let's just say a prediction is made.

56
00:04:32.920 --> 00:04:36.070
And for instance 80 percent that it's a dog.

57
00:04:36.070 --> 00:04:40.610
But it turns out to be a cat and then an error is calculated.

58
00:04:40.610 --> 00:04:40.990
A.

59
00:04:41.200 --> 00:04:47.720
Well what we used to call accosts cost function in a artificial neural network and we used mean square

60
00:04:47.740 --> 00:04:51.460
error there or in-common illusional neural networks.

61
00:04:51.460 --> 00:04:57.630
It's called a loss function and we use a cross entropy function for that.

62
00:04:57.640 --> 00:04:59.870
And we'll talk about cross entropy and mean squared errors.

63
00:05:00.130 --> 00:05:02.820
In a separate tutorial and how all that happens.

64
00:05:02.820 --> 00:05:08.730
But for knowledge you say we have a lost type of function which tells us how well our network is performing

65
00:05:08.730 --> 00:05:13.560
and we're trying to optimize optimize it or minimize that function to optimize our network.

66
00:05:13.750 --> 00:05:19.470
So the error is calculated and then it's back propagated through the network just like we had in artificial

67
00:05:19.470 --> 00:05:26.700
neural networks is back propagated and the some things are adjusted in the network to help optimize

68
00:05:27.000 --> 00:05:31.670
the performance and the things that are adjusted are as usual the weights in the artificial neural network

69
00:05:31.670 --> 00:05:34.910
are part of those the blue lines that you see here the Cynapsus.

70
00:05:35.340 --> 00:05:43.950
Then also another thing that is adjusted is the feature detectors so we know that we're looking for

71
00:05:43.950 --> 00:05:46.140
features but what if we're looking for the wrong features.

72
00:05:46.140 --> 00:05:51.570
What if this didn't work out because the features are incorrect and so the feature detectors those remember

73
00:05:51.570 --> 00:05:53.860
those little matrices that we had.

74
00:05:54.250 --> 00:05:57.270
That's the three by three matrices.

75
00:05:57.270 --> 00:06:03.240
They are adjusted so that maybe next time it'll be better and let's see what happens.

76
00:06:03.360 --> 00:06:03.860
Type of thing.

77
00:06:03.870 --> 00:06:11.040
And but of course it's all done with a lot of science in the background of a lot of math and it's all

78
00:06:11.040 --> 00:06:14.580
done through a gradient gradient descent of back propagation.

79
00:06:14.580 --> 00:06:20.880
So it's all it's all not just random perturbations it's actually very thought through how it's done.

80
00:06:21.210 --> 00:06:27.630
But nevertheless the feature detectors are adjusted the weights are adjusted and this whole process

81
00:06:27.630 --> 00:06:30.710
happens again and then again the errors back propagate.

82
00:06:30.720 --> 00:06:32.610
And this keeps going on and on and on.

83
00:06:32.760 --> 00:06:37.950
And that's how our network is optimized that's how our network trains on the data.

84
00:06:37.950 --> 00:06:43.800
So the important thing here is that the data goes through the whole area from the very start to the

85
00:06:43.800 --> 00:06:44.410
very end.

86
00:06:44.430 --> 00:06:49.950
Then the error is compared so the error is calculated and then is back propagated.

87
00:06:49.950 --> 00:06:56.520
So same story as with artificial neural networks just a bit longer because of that whole for the first

88
00:06:56.520 --> 00:06:58.320
three steps that we already had.

89
00:06:59.040 --> 00:07:04.440
And now let's have a look at the interesting part the really interesting part how do these two classes

90
00:07:04.440 --> 00:07:10.050
work because Or how do these two output neurons work because before we've always kind of had one output

91
00:07:10.050 --> 00:07:11.840
neuron what happens when we have two.

92
00:07:11.840 --> 00:07:17.490
How does how does this situation of classification or images play out.

93
00:07:17.670 --> 00:07:21.610
Well let's start with the top neuron first going to start with the dog.

94
00:07:22.080 --> 00:07:28.950
How do we the main purpose what we need to do first is we need to understand what weights to assign

95
00:07:28.950 --> 00:07:36.000
to all of these syllabuses that connect to the dog so that we know which of the previous neurons are

96
00:07:36.000 --> 00:07:38.910
actually important for the dog and let's see how that is done.

97
00:07:38.910 --> 00:07:46.460
So let's say hypothetically we've got these numbers in our previous layer of previous fully connected.

98
00:07:46.500 --> 00:07:47.980
In the final fully connected layer.

99
00:07:48.120 --> 00:07:51.010
And again these numbers can be absolutely anything.

100
00:07:51.030 --> 00:07:56.490
They don't have to be that they can be any numbers but just for argument's sake we're going to agree

101
00:07:56.490 --> 00:08:01.890
that we are looking specifically at numbers between 0 and 1.

102
00:08:02.280 --> 00:08:09.840
So it's easier for us to argue these things and understand and one means that that neuron was very confident

103
00:08:09.840 --> 00:08:15.960
that it found this feature and zero is going to mean that that neuron didn't find a feature is looking

104
00:08:15.960 --> 00:08:23.580
for so because at the end of the day these neurons like anything else on this from on this left side

105
00:08:23.610 --> 00:08:25.470
is just looking at features at an image.

106
00:08:25.470 --> 00:08:27.490
This is already very very process.

107
00:08:27.510 --> 00:08:32.940
But still it's detecting a certain feature or a combination of features on the image right before we

108
00:08:33.700 --> 00:08:34.590
can evolve step.

109
00:08:34.590 --> 00:08:39.060
We had kind of recognizable features in the pool set they're less recognizable than they become even

110
00:08:39.060 --> 00:08:40.850
less recognizable in the flattened image.

111
00:08:40.850 --> 00:08:42.550
And then they get combine and so on.

112
00:08:42.570 --> 00:08:48.720
But nevertheless this we're talking about here certain features that are present image or their combination.

113
00:08:48.720 --> 00:08:54.480
So and one which has been passed and this is important has been passed to both the dog and the cat at

114
00:08:54.480 --> 00:08:57.020
the same time to both the output neurons.

115
00:08:57.150 --> 00:09:06.180
So one means that for us for our argument it means that this this neuron has is firing up its It's really

116
00:09:06.180 --> 00:09:11.850
rapidly detecting that feature that you know might be an eyebrow it might be detecting this eyebrow

117
00:09:11.870 --> 00:09:15.170
for again for simplicity sake is detecting this eyebrow.

118
00:09:15.270 --> 00:09:20.310
And is communicate that to the dog run to the cat neuron saying I can see my eyebrow I can see my eyebrow.

119
00:09:20.310 --> 00:09:25.240
And then it's up to the dog and the cat neuron to understand what that means for them.

120
00:09:25.290 --> 00:09:25.860
Right.

121
00:09:25.890 --> 00:09:30.840
And so in this case which neurons are firing up these three neurons are firing up the eyebrow and that

122
00:09:30.830 --> 00:09:36.120
say the nose is saying I can see I can see a big nose and I can see floppy ears.

123
00:09:36.270 --> 00:09:40.540
So it and it's saying that to the dog and to the cat and then what the dog.

124
00:09:40.560 --> 00:09:43.390
And then what happens is we know that this is a dog.

125
00:09:43.440 --> 00:09:49.920
So the dog neuron knows that the answer is it is actually a dog because at the end we're comparing to

126
00:09:49.920 --> 00:09:53.640
the picture or to the label on the picture and when another dog.

127
00:09:53.640 --> 00:09:56.310
So basically the dog neuron is going to say Aha.

128
00:09:56.310 --> 00:09:58.820
So I should be triggered in this case.

129
00:09:58.830 --> 00:10:04.790
So these are neurons they're telling this signal that they're sending to both to me to the dog and the

130
00:10:04.790 --> 00:10:09.000
cat is actually a indication for me that it is a dog.

131
00:10:09.020 --> 00:10:13.940
And throughout these lots and lots and lots of iterations of this happens many times the dog will learn

132
00:10:13.940 --> 00:10:19.580
that these neurons do indeed fire up when the feature belongs to a dog.

133
00:10:19.670 --> 00:10:24.260
On the other hand the cat neuron will know that it's not a cat and it will know that this feature is

134
00:10:24.260 --> 00:10:28.210
firing up and this neuron is telling me it can see floppy ears floppy ears ears.

135
00:10:28.370 --> 00:10:31.040
But at the same time it's not a cat.

136
00:10:31.040 --> 00:10:36.980
So basically to me that's a signal that I should ignore this neuron like and the more that happens the

137
00:10:36.980 --> 00:10:41.960
more the cat neuron is going to ignore this neuron about the floppy ears.

138
00:10:42.440 --> 00:10:49.100
And so basically that that's how through lots and lots of iterations if this happens often.

139
00:10:49.100 --> 00:10:54.170
So this is just one example but if this happens often maybe a one maybe 0.8 0.9 maybe sometimes it won't

140
00:10:54.170 --> 00:11:02.090
fire but overall on average this neuron is lighting up very often when it is indeed a dog the dog neuron

141
00:11:02.090 --> 00:11:05.920
will start attributing higher importance to this neuron.

142
00:11:05.930 --> 00:11:06.590
And so there we go.

143
00:11:06.590 --> 00:11:08.430
That's that's how we're going to signify it.

144
00:11:08.450 --> 00:11:14.570
We're going to say that these three neurons through this iterative process with me with many many many

145
00:11:14.570 --> 00:11:20.210
many samples and many many a remember so a sample is a row in your data set and Apoc is when you go

146
00:11:20.210 --> 00:11:25.150
through your whole dataset again and again and again there are lots and lots of iterations.

147
00:11:25.220 --> 00:11:34.010
This dog neuron learned that this eyebrowed neuron and this big nose neuron and this floppy ear neuron

148
00:11:34.340 --> 00:11:43.040
they all seem to really contribute very well to the classification of what it's looking for and which

149
00:11:43.040 --> 00:11:44.350
is a dog.

150
00:11:44.480 --> 00:11:45.730
So that's how it works.

151
00:11:45.740 --> 00:11:55.130
And again these ears and nose and eyebrows those are very very approximate or like very far fetched

152
00:11:55.130 --> 00:12:01.640
examples because by this stage in this whole convolution conventional neural network it is completely

153
00:12:01.640 --> 00:12:07.400
unrecognizable what they're looking for but at the same time it is something in the features of dogs

154
00:12:07.400 --> 00:12:09.020
or cats or whatever you classify it.

155
00:12:09.410 --> 00:12:11.130
And then so let's move on to the next.

156
00:12:11.150 --> 00:12:15.860
Now we're going to look at the cat neuron but these We're going to remember that these weights are you

157
00:12:15.860 --> 00:12:17.900
know how we've sorted out the dog.

158
00:12:17.900 --> 00:12:22.970
So the dog is kind of like pretty much ignoring all these other neurons one two three four or five but

159
00:12:22.970 --> 00:12:26.510
it's really paying attention to what these three neurons are saying.

160
00:12:26.570 --> 00:12:28.330
Now what is the cat's listening to.

161
00:12:28.490 --> 00:12:30.830
Well whenever it is actually a cat.

162
00:12:30.970 --> 00:12:32.530
Right.

163
00:12:32.710 --> 00:12:35.600
This is this is an example of a situation when it's actually a cat.

164
00:12:35.600 --> 00:12:42.980
So you'll see that this these three neurons 0.9 0.9 and one they're saying something they're saying

165
00:12:42.980 --> 00:12:44.590
something to both the dog and the cat.

166
00:12:44.600 --> 00:12:49.510
And this is again important remember so this output signal goes both ways it's the same right.

167
00:12:49.520 --> 00:12:55.520
It's it's saying one to the dog saying to the cat but then it's up to the dog to the cat to decide whether

168
00:12:55.520 --> 00:13:00.220
to take into account that signal and learn from it or not.

169
00:13:00.500 --> 00:13:05.810
And both the dog and the cat can see that this is a photo I should of put a photo of a cat here but

170
00:13:05.810 --> 00:13:10.030
basically imagine a photo of a cat both a dog and the cat can see that this is actually a cat.

171
00:13:10.190 --> 00:13:20.150
So basically the dog is like OK so these whiskers and these pointy triangle ears and this small size

172
00:13:20.420 --> 00:13:28.250
yes or or maybe this type you know how cats have these things in their eyes their eyes are like little

173
00:13:28.310 --> 00:13:33.350
They're not circles or lines or something like cat eyes.

174
00:13:33.350 --> 00:13:37.460
Basically these cat eyes they're definitely not working for me.

175
00:13:37.460 --> 00:13:42.980
They're not helping me I'll predict because every time these neurons light up the prediction is not

176
00:13:42.980 --> 00:13:44.240
what I'm looking for.

177
00:13:44.240 --> 00:13:46.910
On the other hand the cat is like hmm that's interesting.

178
00:13:46.910 --> 00:13:51.620
Every time these this one lights up it's more most of the time it lights up.

179
00:13:51.620 --> 00:13:55.310
It matches my expectation it matches what I'm looking for.

180
00:13:55.310 --> 00:13:55.630
OK.

181
00:13:55.640 --> 00:13:58.050
I'm going to listen to this guy more than this one.

182
00:13:58.160 --> 00:14:02.710
This one same thing every time it lights up or most of the times it lights up.

183
00:14:02.810 --> 00:14:09.100
I happened to get a good I happened to be rewarded for my prediction because I get it right.

184
00:14:09.110 --> 00:14:09.760
It's a cat.

185
00:14:09.770 --> 00:14:10.080
OK.

186
00:14:10.130 --> 00:14:11.440
I'm going to listen to him more.

187
00:14:11.450 --> 00:14:17.930
You know this one useless to me because he's not actually you know like he's he's not even lighting

188
00:14:17.930 --> 00:14:21.040
up it's a cat but it's he's not lighting up so the opposite is happening.

189
00:14:21.050 --> 00:14:24.410
And this one is well it's a cad but he's not letting up so I'm not gonna listen to him.

190
00:14:24.410 --> 00:14:31.250
But this one when he went what was this the eyes the cat eyes light up we can see I can see that it's

191
00:14:31.250 --> 00:14:31.850
a cat.

192
00:14:31.850 --> 00:14:36.440
It matches most of the time so I'm going to learn from that and I'm going to listen to these three guys

193
00:14:36.980 --> 00:14:38.750
more often than not.

194
00:14:38.750 --> 00:14:44.810
And so basically the cat is listening to these three and it's ignoring the other five and that is how

195
00:14:45.350 --> 00:14:54.830
these final neurons learn which neurons in the final fully connected Lehre to listen to the output neurons

196
00:14:54.830 --> 00:14:58.460
learn which of the fully which are the final fully connected.

197
00:14:58.670 --> 00:15:00.030
There are neurons to listen to.

198
00:15:00.180 --> 00:15:02.530
And that's how they understand.

199
00:15:02.790 --> 00:15:08.930
Basically that's how the features are propagated through the network and conveyed to the output.

200
00:15:08.970 --> 00:15:14.070
And so even though these features of course don't have that much meaning to them like floppy ears or

201
00:15:14.070 --> 00:15:14.900
whiskers.

202
00:15:15.210 --> 00:15:21.860
At the same time they do have some distinctive they are a distinctive feature of that specific class

203
00:15:21.870 --> 00:15:27.270
and that's how the network is trained because we also during remember during the back propagation process

204
00:15:27.270 --> 00:15:33.750
we also adjust the feature detectors so if a feature is useless to the output it's going to it's going

205
00:15:33.750 --> 00:15:39.600
to probably be disregarded because this doesn't happen to one or two stories it happens through thousands

206
00:15:39.600 --> 00:15:41.000
and thousands of iterations.

207
00:15:41.040 --> 00:15:46.620
So with time a feature that is useless to the network is going to be disregarded and replaced with feature

208
00:15:46.620 --> 00:15:52.830
is useful and so at the end of the day in this final layer of neurons you are likely to have lots of

209
00:15:53.070 --> 00:15:59.730
features or combinations of features from the image that are indeed representative or descriptive of

210
00:15:59.730 --> 00:16:01.320
dogs and cats.

211
00:16:01.710 --> 00:16:06.660
And so then once your network is trained up then we this is how it's applied.

212
00:16:06.660 --> 00:16:09.340
So this is the next step like we were trained up our network will this happen.

213
00:16:09.350 --> 00:16:13.020
Let's see what happens when the this network is applied.

214
00:16:13.020 --> 00:16:15.660
So let's say we pass on an image of a dog.

215
00:16:16.410 --> 00:16:20.340
The values are propagated through a network we get certain values.

216
00:16:20.610 --> 00:16:26.880
And so this time the dog and the cat neurons don't know they don't have the image of the dog here they

217
00:16:26.880 --> 00:16:28.470
don't know that it's a dog or a cat.

218
00:16:28.470 --> 00:16:35.380
They have no idea what it is but they have learned to listen to what is being shown here.

219
00:16:35.380 --> 00:16:35.660
Right.

220
00:16:35.670 --> 00:16:40.440
They have learned to listen to dog and listens to these three neurons a cat neuron listens to these

221
00:16:40.440 --> 00:16:40.910
three.

222
00:16:40.950 --> 00:16:44.850
And so the dog neuron looks at one two three and says aha these are pretty high.

223
00:16:44.940 --> 00:16:50.430
So my probability is going to be high that is a dog the cat neuron looks at these three and says OK

224
00:16:50.470 --> 00:16:53.670
these this one is pretty high but these are pretty low.

225
00:16:53.670 --> 00:16:54.320
Interesting.

226
00:16:54.320 --> 00:16:56.990
So my probability is going to be 0.05.

227
00:16:57.130 --> 00:16:58.950
And and then and that's.

228
00:16:58.980 --> 00:17:00.110
And that's where you get your prediction.

229
00:17:00.120 --> 00:17:04.490
So then your first choice for this neural network is dog.

230
00:17:04.500 --> 00:17:06.900
Second choice is cat and that's pretty much it.

231
00:17:06.900 --> 00:17:11.690
So the answer is dog and same thing happens when you pass an image of a cat.

232
00:17:11.910 --> 00:17:16.580
You get new values and you can see that even though this one's high these ones are low.

233
00:17:16.770 --> 00:17:20.560
And for the cat This was high this was high and this one's a bit low.

234
00:17:20.670 --> 00:17:25.850
So the probability here might not be as great as previously but still you can see that it's a cat of

235
00:17:25.860 --> 00:17:26.810
79 percent.

236
00:17:26.940 --> 00:17:30.230
And so therefore the neural network is going to vote that it's a cat.

237
00:17:30.270 --> 00:17:33.240
And so basically all the neural networks going to conclude that it's a cat.

238
00:17:33.330 --> 00:17:40.710
Voting is a term that is used for these guys so these neurons in the final fully connected Lehre they

239
00:17:40.710 --> 00:17:41.510
get to vote.

240
00:17:41.520 --> 00:17:42.810
And these are their votes.

241
00:17:42.870 --> 00:17:47.160
And again we are just for argument's sake putting values between 0 and 1 here.

242
00:17:47.160 --> 00:17:54.480
These could be any values but they get to vote and then these weights are the importance of their vote.

243
00:17:54.480 --> 00:18:00.540
So this is these are these purple weights are how the dog neuron views their votes.

244
00:18:00.540 --> 00:18:04.820
How much importance is it assigns to these neurons and those votes.

245
00:18:04.830 --> 00:18:12.810
And this is how much importance the cat's neuron up size to these votes the votes of these neurons and

246
00:18:12.810 --> 00:18:18.840
so these neurons vote the dog and the cat based on their learned the weights they decide who to listen

247
00:18:18.840 --> 00:18:23.490
to and then they make their predictions and then hold neural network concludes that this is in this

248
00:18:23.490 --> 00:18:29.190
case a cat and then that's And then that's your conclusion and that's how you get images like this where

249
00:18:29.490 --> 00:18:37.200
you have a cheetah and then you have a cheetah claws who you know like a high high probability So this

250
00:18:37.200 --> 00:18:40.080
is you know the probability that the network has predicted.

251
00:18:40.080 --> 00:18:44.430
And these are laws but these still exist because they're still kind of like a small chance the other

252
00:18:44.430 --> 00:18:49.710
neurons are also listening to their voters and they're saying oh maybe it's actually a leopard and a

253
00:18:49.710 --> 00:18:50.580
bullet train.

254
00:18:50.580 --> 00:18:51.400
Very very probable.

255
00:18:51.400 --> 00:18:52.470
I hear scissors.

256
00:18:52.470 --> 00:18:57.600
You know this one one but hand-glass was a very close second and in fivepence stethoscope because you

257
00:18:57.600 --> 00:19:03.960
could see like this guy this this neuron the scissors neuron the output series neuron listened to its

258
00:19:03.960 --> 00:19:07.070
voters and it had the predominant vote overall.

259
00:19:07.080 --> 00:19:10.190
But then the hand-glass had a good outcome as well.

260
00:19:10.200 --> 00:19:16.450
So there we go that's how the full connection works and how this is all this all plays out together.

261
00:19:16.680 --> 00:19:18.810
I hope you enjoyed today's tutorial.

262
00:19:18.810 --> 00:19:21.320
We're going to summarize all of this in the summary as well.

263
00:19:21.420 --> 00:19:22.860
And I'll see you next time.

264
00:19:22.860 --> 00:19:24.720
Until then enjoy deep learning.
