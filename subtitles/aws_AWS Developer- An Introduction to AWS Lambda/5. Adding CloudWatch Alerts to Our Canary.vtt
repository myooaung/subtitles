WEBVTT
1
00:00:01.020 --> 00:00:02.450
So back over here in CloudWatch,

2
00:00:02.450 --> 00:00:05.160
we could take a closer look at the invocations of our Lambda

3
00:00:05.160 --> 00:00:07.820
function. Now, because there's three here, let's go ahead and

4
00:00:07.820 --> 00:00:10.220
pick the first two initially,

5
00:00:10.220 --> 00:00:12.650
and it looks like this is the first function invocation

6
00:00:12.650 --> 00:00:15.080
that I ran against my website when I still had my name

7
00:00:15.080 --> 00:00:17.100
configured as the expected value.

8
00:00:17.100 --> 00:00:17.310
Now,

9
00:00:17.310 --> 00:00:20.080
if I go back up to the log group and look at the log

10
00:00:20.080 --> 00:00:23.030
stream that happened right after that, about a minute later,

11
00:00:23.030 --> 00:00:25.710
it looks like this is the one that failed the check,

12
00:00:25.710 --> 00:00:27.480
and we can see that based on the errors that are

13
00:00:27.480 --> 00:00:30.010
recorded inside of the logs in here.

14
00:00:30.010 --> 00:00:34.220
So how should we proactively notify ourselves when things like this fail?

15
00:00:34.220 --> 00:00:37.060
Well, one of the easiest ways to do this with Lambda is to set up a

16
00:00:37.060 --> 00:00:40.590
CloudWatch alarm that is related to these logs and the metrics that

17
00:00:40.590 --> 00:00:43.450
are generated from them. So let's go over to the Alarms section

18
00:00:43.450 --> 00:00:47.900
inside of CloudWatch here, and let's create a new alarm.

19
00:00:47.900 --> 00:00:51.160
Now, when I click Create a new alarm, I'm going to need to specify a metric

20
00:00:51.160 --> 00:00:53.860
to create the alarm that's associated with the function.

21
00:00:53.860 --> 00:00:55.480
So I'll select a metric here,

22
00:00:55.480 --> 00:00:58.860
and I need to pick where I want to get this metric from, so

23
00:00:58.860 --> 00:01:00.640
I'm going to search for Lambda right here.

24
00:01:00.640 --> 00:01:03.240
And then I'll be able to find metrics related to each

25
00:01:03.240 --> 00:01:04.940
Lambda function by their function name.

26
00:01:04.940 --> 00:01:07.560
So in this case, I see there's the function name of lambda‑canary,

27
00:01:07.560 --> 00:01:09.160
which is the one we're working with,

28
00:01:09.160 --> 00:01:11.690
and I'm curious about the metric name. Now, in this case,

29
00:01:11.690 --> 00:01:14.240
I want to know if there's any errors, so I'll have to scroll through

30
00:01:14.240 --> 00:01:16.820
some of these different metrics that we see here.

31
00:01:16.820 --> 00:01:20.810
And until I find errors, I can click the checkbox right next to it.

32
00:01:20.810 --> 00:01:23.280
So, now that I've hit the checkbox here,

33
00:01:23.280 --> 00:01:25.600
it's highlighted all the errors that have happened on my

34
00:01:25.600 --> 00:01:27.610
function in the last few periods of time.

35
00:01:27.610 --> 00:01:31.490
Now, I'm going to just zoom in quite a bit here to the 1‑hour window,

36
00:01:31.490 --> 00:01:34.500
and this is showing me that pretty recently, within the last 5

37
00:01:34.500 --> 00:01:37.050
to 10 minutes, I had an error from earlier.

38
00:01:37.050 --> 00:01:40.420
Now that's because I intentionally triggered that one error before,

39
00:01:40.420 --> 00:01:42.870
and there haven't been any errors since.

40
00:01:42.870 --> 00:01:47.230
It looks a little bit weird because the count is 0.333, and that's

41
00:01:47.230 --> 00:01:50.830
because it's one out of three function invocations.

42
00:01:50.830 --> 00:01:52.040
And if I wanted to change this,

43
00:01:52.040 --> 00:01:54.980
I could go to the Graphed metrics section here. Rather than

44
00:01:54.980 --> 00:01:57.770
having this be an average of the errors of the period, I could

45
00:01:57.770 --> 00:02:00.200
change this to a different metric.

46
00:02:00.200 --> 00:02:01.550
Now, if I change it to Sum,

47
00:02:01.550 --> 00:02:04.140
this is going to be a bit more helpful to tell me how many errors are

48
00:02:04.140 --> 00:02:07.920
happening within a period. It looks like in the period around 4:10

49
00:02:07.920 --> 00:02:09.830
there was a single error, and then later,

50
00:02:09.830 --> 00:02:12.890
towards 4:15 and 4:20 there haven't been any errors yet.

51
00:02:12.890 --> 00:02:15.420
So this will be more straightforward when I want to set up

52
00:02:15.420 --> 00:02:16.830
an alert for this function.

53
00:02:16.830 --> 00:02:20.670
So let's go ahead and select this metric for the purposes of creating our alarm,

54
00:02:20.670 --> 00:02:23.360
and then we'll be able to configure it a little bit further.

55
00:02:23.360 --> 00:02:25.240
We could set up a different name for this metric or

56
00:02:25.240 --> 00:02:27.040
change other configuration around.

57
00:02:27.040 --> 00:02:29.240
Similarly, with the period that we're looking at,

58
00:02:29.240 --> 00:02:32.470
we could specify larger or smaller periods as appropriate to

59
00:02:32.470 --> 00:02:34.400
whatever we need in terms of our alerting.

60
00:02:34.400 --> 00:02:36.530
I'm just going to leave it on 5 minutes for right now.

61
00:02:36.530 --> 00:02:38.780
But maybe if Globomantics was really concerned about

62
00:02:38.780 --> 00:02:40.700
getting instantaneous information,

63
00:02:40.700 --> 00:02:43.170
we could start to configure it with smaller periods just in

64
00:02:43.170 --> 00:02:45.580
case anything else came in in those periods.

65
00:02:45.580 --> 00:02:49.430
So I'm going to close that out now, and we can go down to the Conditions here.

66
00:02:49.430 --> 00:02:52.930
Now we could use CloudWatch Alarms' anomaly detection to see if there's

67
00:02:52.930 --> 00:02:55.330
any anomalies in what's being reported, but instead,

68
00:02:55.330 --> 00:02:57.550
I'm just going to stick with the static threshold.

69
00:02:57.550 --> 00:03:00.380
And I'm going to say that whenever errors are greater than whatever I

70
00:03:00.380 --> 00:03:03.570
define as the threshold, I want to be notified immediately.

71
00:03:03.570 --> 00:03:03.820
Now,

72
00:03:03.820 --> 00:03:07.350
the threshold value I want in this case is going to be 0 because I

73
00:03:07.350 --> 00:03:10.590
want to know if there's anything greater than 0, to let me know

74
00:03:10.590 --> 00:03:13.730
immediately so that I can immediately check the website and see if

75
00:03:13.730 --> 00:03:15.080
there's any issues with it.

76
00:03:15.080 --> 00:03:18.040
I could do additional configuration by clicking the drop‑down here,

77
00:03:18.040 --> 00:03:20.580
and this would allow me to determine how many datapoints I

78
00:03:20.580 --> 00:03:23.930
need in order to have the alarm actually go into an alarm

79
00:03:23.930 --> 00:03:29.000
state and notify me. Also, I can specify what I want missing data to mean.

80
00:03:29.000 --> 00:03:32.150
So, if there's missing data from the alert, do I want to be

81
00:03:32.150 --> 00:03:35.740
alerted immediately, or in this case, am I not really concerned,

82
00:03:35.740 --> 00:03:38.970
and I just want to treat it as missing, so neither good nor bad.

83
00:03:38.970 --> 00:03:41.550
I'm going to leave all this additional configuration as

84
00:03:41.550 --> 00:03:44.390
is so that whenever I see any datapoints out of one

85
00:03:44.390 --> 00:03:46.680
datapoint going into a bad state,

86
00:03:46.680 --> 00:03:49.040
I want to be notified, and I also don't want to worry

87
00:03:49.040 --> 00:03:51.540
about if there's any missing data, for whatever reason.

88
00:03:51.540 --> 00:03:52.840
So let's click Next here,

89
00:03:52.840 --> 00:03:55.320
and we'll move on to the next stage of configuring this alarm.

90
00:03:55.320 --> 00:03:59.360
If I scroll back up to the top here, I want a notification for this alarm.

91
00:03:59.360 --> 00:04:00.070
Now, in this case,

92
00:04:00.070 --> 00:04:03.040
I just want to be notified whenever this goes into an Alarm state

93
00:04:03.040 --> 00:04:06.030
instead of an OK or an INSUFFICIENT_DATA state.

94
00:04:06.030 --> 00:04:09.250
And then I want to specify how I want to be notified.

95
00:04:09.250 --> 00:04:10.920
So I could either select an existing,

96
00:04:10.920 --> 00:04:14.630
Simple Notification Service topic, or I could create a new one. In this

97
00:04:14.630 --> 00:04:18.130
case, because this account is completely new and I don't have any SNS

98
00:04:18.130 --> 00:04:22.330
topic created, I'm going to go ahead and click Create a new topic. Now,

99
00:04:22.330 --> 00:04:24.240
it's going to create a new topic for me,

100
00:04:24.240 --> 00:04:26.800
and then I can say which email addresses I'd like to be

101
00:04:26.800 --> 00:04:30.330
able to get emails from SNS, so I'm going to add in my

102
00:04:30.330 --> 00:04:35.080
email now. With my email added in, I can go ahead and click Create topic.

103
00:04:35.080 --> 00:04:35.960
After doing that,

104
00:04:35.960 --> 00:04:39.820
the SNS topic should be created, and we can configure it with our alarm.

105
00:04:39.820 --> 00:04:42.400
But I still need to finish creating this alarm. So I can

106
00:04:42.400 --> 00:04:44.900
scroll down and click the Next button, and then I can add a

107
00:04:44.900 --> 00:04:47.040
name and description for this alarm.

108
00:04:47.040 --> 00:04:50.480
We'll call this our canaryAlarm, and then we'll add a description, which

109
00:04:50.480 --> 00:04:53.480
will just be the exact same, and says canaryAlarm.

110
00:04:53.480 --> 00:04:58.420
Next, we'll click the Next button and finally preview this alarm and create it.

111
00:04:58.420 --> 00:05:01.440
Now, this alarm is going to end up triggering whenever the blue line,

112
00:05:01.440 --> 00:05:04.460
which is the count of errors, goes above the red line,

113
00:05:04.460 --> 00:05:09.170
which is our top limitation of how many errors we could expect within 5 minutes.

114
00:05:09.170 --> 00:05:12.440
So, it looks like this would've triggered for that single

115
00:05:12.440 --> 00:05:14.450
error that I intentionally created earlier.

116
00:05:14.450 --> 00:05:16.650
And that's exactly what we want, so we should be

117
00:05:16.650 --> 00:05:18.440
happy with how this is set up now.

118
00:05:18.440 --> 00:05:21.250
So I'm going to scroll down, and let's go ahead and skim through the rest

119
00:05:21.250 --> 00:05:25.420
of the configuration and finally click Create alarm.

120
00:05:25.420 --> 00:05:28.970
So now our alarm is created, and we've successfully created an

121
00:05:28.970 --> 00:05:32.120
SNS topic to send things to us; however,

122
00:05:32.120 --> 00:05:34.730
SNS isn't going to send messages to our email until the

123
00:05:34.730 --> 00:05:36.870
subscription is actually confirmed.

124
00:05:36.870 --> 00:05:38.570
So if I scroll over to the right here,

125
00:05:38.570 --> 00:05:42.440
you'll see that the actions for this alarm are still pending confirmation.

126
00:05:42.440 --> 00:05:45.700
You'll also see that Amazon is reminding us to confirm this if I close

127
00:05:45.700 --> 00:05:50.210
these other two notifications out inside of the AWS SNS console. So

128
00:05:50.210 --> 00:05:52.830
let me click View SNS Subscriptions here,

129
00:05:52.830 --> 00:05:55.040
and let's go ahead and confirm that email.

130
00:05:55.040 --> 00:05:59.250
So this pending confirmation is for pluralsight.fernando@gmail.com,

131
00:05:59.250 --> 00:06:02.560
and what this means is I need to go to my email and confirm that I'm

132
00:06:02.560 --> 00:06:04.860
okay getting alerts for this alarm.

133
00:06:04.860 --> 00:06:09.390
So I've gone ahead and loaded up my pluralsight.fernando@gmail.com email here.

134
00:06:09.390 --> 00:06:10.810
Now there's nothing in this inbox yet,

135
00:06:10.810 --> 00:06:13.770
so I'm going to go back to the Simple Notification Service, make sure I

136
00:06:13.770 --> 00:06:18.130
have the pluralsight.fernando@gmail.com endpoint selected,

137
00:06:18.130 --> 00:06:20.900
and then I'm going to click Request confirmation.

138
00:06:20.900 --> 00:06:24.080
So this should've sent something off to my email to make sure I can confirm

139
00:06:24.080 --> 00:06:26.640
that I want to get emails whenever something goes wrong.

140
00:06:26.640 --> 00:06:28.680
So going back to pluralsight.fernando,

141
00:06:28.680 --> 00:06:32.090
we should see this email after we refresh the page.

142
00:06:32.090 --> 00:06:33.720
So now that I've refreshed my inbox,

143
00:06:33.720 --> 00:06:36.700
it looks like I got my initial notification here about 6

144
00:06:36.700 --> 00:06:40.320
minutes ago, and then I also got the notification that I was

145
00:06:40.320 --> 00:06:42.730
just asking to be resent to this email.

146
00:06:42.730 --> 00:06:45.230
I can use this second one or the first one to

147
00:06:45.230 --> 00:06:48.240
confirm my notification subscription.

148
00:06:48.240 --> 00:06:49.030
Now, once I've done this,

149
00:06:49.030 --> 00:06:52.160
I want to be careful not to click the click here to unsubscribe button,

150
00:06:52.160 --> 00:06:54.930
even though it looks like that might be what I'm supposed to do next.

151
00:06:54.930 --> 00:06:56.670
And once this subscription is confirmed,

152
00:06:56.670 --> 00:06:59.010
I should be able to close that out and go back to the Simple

153
00:06:59.010 --> 00:07:02.650
Notification Service console, and then I can refresh this page

154
00:07:02.650 --> 00:07:05.450
to see whether or not the pending confirmation will change from

155
00:07:05.450 --> 00:07:07.910
that to a Confirmed status.

156
00:07:07.910 --> 00:07:10.640
It looks like in this case I am now confirmed.

157
00:07:10.640 --> 00:07:12.960
And if I go back to the CloudWatch management console,

158
00:07:12.960 --> 00:07:15.900
I should be able to refresh this screen as well and see that

159
00:07:15.900 --> 00:07:18.760
now this alarm is all set up and is no longer pending

160
00:07:18.760 --> 00:07:20.910
confirmation in the Actions section.

161
00:07:20.910 --> 00:07:24.610
So you'll see that this canaryAlarm is currently okay with 0 of

162
00:07:24.610 --> 00:07:27.660
1 datapoints reporting errors in 5 minutes.

163
00:07:27.660 --> 00:07:28.640
Now, this is great.

164
00:07:28.640 --> 00:07:32.530
Hopefully, my function keeps running and always is in this state.

165
00:07:32.530 --> 00:07:34.460
But if we're really worried and we want to make sure

166
00:07:34.460 --> 00:07:36.340
this is going to send us an alarm,

167
00:07:36.340 --> 00:07:40.330
let's go back to our Lambda function and then click the Configuration section.

168
00:07:40.330 --> 00:07:42.500
And instead of selecting the CloudWatch Events that

169
00:07:42.500 --> 00:07:44.610
we just selected and configured,

170
00:07:44.610 --> 00:07:47.960
we're going to go back to the lambda‑canary and click that, and let's

171
00:07:47.960 --> 00:07:51.680
scroll down and change the expected value one more time.

172
00:07:51.680 --> 00:07:52.350
In this case,

173
00:07:52.350 --> 00:07:54.180
I'm just going to add some gibberish in here that I don't

174
00:07:54.180 --> 00:07:56.440
think will appear on my page in this sequence,

175
00:07:56.440 --> 00:07:58.340
and I'm going to save this function,

176
00:07:58.340 --> 00:08:02.060
and then let's test it. Now, this should trigger an alarm

177
00:08:02.060 --> 00:08:05.010
because we don't have this text on my website, and

178
00:08:05.010 --> 00:08:06.890
because this is giving an error,

179
00:08:06.890 --> 00:08:10.970
we should be able to go back over to the CloudWatch management console and wait

180
00:08:10.970 --> 00:08:14.260
for a few minutes here while that data propagates through the CloudWatch logs

181
00:08:14.260 --> 00:08:16.960
and the CloudWatch metrics and triggers this alarm.

182
00:08:16.960 --> 00:08:19.090
So let's wait for a minute while this happens.

183
00:08:19.090 --> 00:08:22.740
So I waited maybe 30 seconds and refreshed this page one more time, and my

184
00:08:22.740 --> 00:08:25.710
canaryAlarm was starting to show up in the alarm stage.

185
00:08:25.710 --> 00:08:26.580
So because of this,

186
00:08:26.580 --> 00:08:29.970
I went back to my email inbox here, and I had a new email

187
00:08:29.970 --> 00:08:32.440
letting me know that currently, the alarm that I just set

188
00:08:32.440 --> 00:08:34.110
up is in the alarm stage.

189
00:08:34.110 --> 00:08:36.970
This would be the exact same information that we might want to get if our

190
00:08:36.970 --> 00:08:40.840
website was down and would tell us how we could start to debug the problem

191
00:08:40.840 --> 00:08:44.310
by going directly to the AWS management console and taking a look at this

192
00:08:44.310 --> 00:08:47.860
alarm. There might be other remediation steps that we'd have inside of our

193
00:08:47.860 --> 00:08:51.020
organization if we knew this website was down and maybe wanted to know

194
00:08:51.020 --> 00:08:54.360
more about how to fix it. So nice work in creating your first Lambda

195
00:08:54.360 --> 00:08:55.030
function.

196
00:08:55.030 --> 00:08:58.800
Let's go ahead and review what we've done in this module.

197
00:08:58.800 --> 00:09:02.590
Congratulations! In this module, we've learned a lot of different

198
00:09:02.590 --> 00:09:06.380
information. We reviewed Lambda's technical limitations and what it can and

199
00:09:06.380 --> 00:09:10.240
can't be used for architecturally, and we also learned some workarounds for

200
00:09:10.240 --> 00:09:13.040
these limitations that we could apply if needed.

201
00:09:13.040 --> 00:09:16.980
We also deployed our first Lambda function inside of the AWS console

202
00:09:16.980 --> 00:09:21.270
using a modified AWS blueprint for the Lambda canary, and we set up a

203
00:09:21.270 --> 00:09:24.460
learning and monitoring on top of that Lambda canary so we can be

204
00:09:24.460 --> 00:09:26.940
notified whenever something fails.

205
00:09:26.940 --> 00:09:28.990
Now that we've practiced some of these essentials,

206
00:09:28.990 --> 00:09:32.430
what's next? We'll be tackling a more complicated example in the

207
00:09:32.430 --> 00:09:39.000
next module where we'll learn how to integrate Lambda with third‑party APIs, so let's get to it.

