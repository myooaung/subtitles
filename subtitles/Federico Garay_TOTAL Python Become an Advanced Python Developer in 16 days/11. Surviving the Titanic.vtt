WEBVTT
1
00:00:07.700 --> 00:00:10.430
So you've learned the theory about machine learning.

2
00:00:10.460 --> 00:00:14.210
Now let's put into practice with a decision tree exercise.

3
00:00:14.690 --> 00:00:20.150
In this exercise, we're going to put our project into practice, which will allow us to design a machine

4
00:00:20.150 --> 00:00:21.140
learning model.

5
00:00:21.380 --> 00:00:28.250
This model will analyze a database, and from that it will be able to predict the behavior of eventual

6
00:00:28.250 --> 00:00:29.030
new data.

7
00:00:29.300 --> 00:00:32.090
So how are we going to work in this lesson?

8
00:00:32.120 --> 00:00:36.860
You're going to download a file that I'm going to leave for you to download together with this lesson.

9
00:00:37.340 --> 00:00:44.810
It is this data frame that has about 715 records that correspond to the passengers that traveled on

10
00:00:44.810 --> 00:00:47.090
the fateful voyage of the Titanic.

11
00:00:47.870 --> 00:00:50.240
Download it and save it to your drive.

12
00:00:50.750 --> 00:00:55.580
This file belongs to a challenge that is quite well known among students of data science.

13
00:00:55.850 --> 00:01:01.490
Hey, you can go to the official site from where we got it, which is about how to design a decision

14
00:01:01.490 --> 00:01:09.080
tree which can evaluate according to the data of the people who survived or not, that that voyage in

15
00:01:09.080 --> 00:01:15.410
order to identify within the characteristics of those people, which were the factors that most influenced

16
00:01:15.410 --> 00:01:17.270
them to survive or not.

17
00:01:17.690 --> 00:01:23.390
And the idea of doing this is that based on what the machine learns by analyzing this information,

18
00:01:23.660 --> 00:01:28.130
it can identify in advance what would happen to eventual new passengers.

19
00:01:28.490 --> 00:01:32.420
So what you're going to find in this data frame is about six columns.

20
00:01:32.960 --> 00:01:39.350
The first one say is whether that person survived or not and reports it with a zero if they didn't survive

21
00:01:39.350 --> 00:01:40.760
or a one if they did.

22
00:01:41.210 --> 00:01:45.860
We have this with numbers so we can do mathematical calculations with this.

23
00:01:46.490 --> 00:01:49.670
Then we have which class each passenger belongs to.

24
00:01:50.060 --> 00:01:55.520
One corresponds to first class, two to second class, and three to third class.

25
00:01:56.120 --> 00:02:01.820
So remember this the people in third class were the people with the lowest socioeconomic status.

26
00:02:02.480 --> 00:02:06.290
Then we have gender zero for men and one for women.

27
00:02:06.980 --> 00:02:13.220
Finally, the age, which is obviously expressed by a number of years, if that person had siblings

28
00:02:13.220 --> 00:02:17.540
or spouses within the trip, we can see this with a number reflecting the number.

29
00:02:18.230 --> 00:02:25.370
Siblings or spouses indicates horizontal relationships, and if those people were accompanied by parents

30
00:02:25.370 --> 00:02:30.140
or children, i.e. vertical links, we have the number expressed in numbers.

31
00:02:30.590 --> 00:02:35.810
So as I told you, you are going to download this data frame and we're going to incorporate it into

32
00:02:35.810 --> 00:02:36.560
your disk.

33
00:02:36.800 --> 00:02:42.350
You're going to import it into the notebook so that we can work from there and that we can design a

34
00:02:42.350 --> 00:02:44.990
model that will allow us to draw a conclusions.

35
00:02:45.350 --> 00:02:49.340
And we're going to do it together, of course, because there's a lot to learn here.

36
00:02:50.000 --> 00:02:55.400
So if you want to work with another data frame, which has different data and you make other kinds of

37
00:02:55.400 --> 00:03:00.890
inquiries, of course you can do it and it will work if you adapt it to the characteristics of each

38
00:03:00.890 --> 00:03:01.700
data frame.

39
00:03:02.130 --> 00:03:08.450
But let's let's start by doing what needs to be done, which is to import the libraries we need, which

40
00:03:08.450 --> 00:03:10.520
in this case are not a few.

41
00:03:11.570 --> 00:03:17.840
We're going to first import pandas like PD and we're going to import NumPy as NP.

42
00:03:18.800 --> 00:03:24.830
We're also going to import matt plot lib dot pi plot and we're going to do it as PLT.

43
00:03:25.820 --> 00:03:27.500
These are the ones we already know.

44
00:03:27.860 --> 00:03:33.770
We're also going to import a new one which we haven't seen so far, which is C Bon, which is also a

45
00:03:33.770 --> 00:03:38.030
library that allows us to make graphs to represent graphs in the notebooks.

46
00:03:38.480 --> 00:03:43.910
We're going to import this one as sense, and you'll finally get to know it in a moment.

47
00:03:44.120 --> 00:03:48.470
Now we're going to go to the ones that are specifically for machine learning.

48
00:03:48.620 --> 00:03:55.550
And as we said in the introduction, it's the CI Kit Learn library, which has a lot of elements.

49
00:03:55.550 --> 00:03:57.320
And we're going to start with that.

50
00:03:58.100 --> 00:04:02.000
We're going to say that from S k learn tree.

51
00:04:02.030 --> 00:04:05.840
We're going to import an element called decision tree classifier.

52
00:04:07.280 --> 00:04:16.190
And from SRC Learn metric, we're going to import several elements import accuracy score and we're going

53
00:04:16.190 --> 00:04:18.830
to bring in confusion matrix.

54
00:04:19.520 --> 00:04:24.920
Going to know what this is and finally to plot confusion matrix.

55
00:04:26.270 --> 00:04:35.030
Finally from SK learn we are also going to import TRE and with this we have everything we need.

56
00:04:35.300 --> 00:04:39.590
So we run this cell and we already have all the elements at our disposal.

57
00:04:40.400 --> 00:04:46.340
We're also going to do something that we already know how to do, which is to link the drive disc with

58
00:04:46.340 --> 00:04:50.060
this notebook so that we can use the resources that are there.

59
00:04:50.660 --> 00:04:55.580
There we have the data frame, which I assume you've already downloaded and saved your own disk.

60
00:04:55.880 --> 00:05:04.820
So from Google Dot CoLab, lets import drive and then we're going to say down here the magic phrase

61
00:05:04.820 --> 00:05:10.850
drive mount and we're going to put in brackets slash, content slash, drive.

62
00:05:11.600 --> 00:05:15.770
We execute this line and here comes this authorization request.

63
00:05:16.070 --> 00:05:20.720
So I go to my account, I click on allow and our disk has been linked.

64
00:05:22.760 --> 00:05:25.670
Now we're going to read this file that's inside our disk.

65
00:05:25.970 --> 00:05:32.390
We save it into a variable and we're going to call it def for data frame and we're going to say that

66
00:05:32.390 --> 00:05:40.700
is equal to an object pandas read c csv and in the brackets we put this link we copy and paste it.

67
00:05:40.940 --> 00:05:42.350
This is the address.

68
00:05:43.010 --> 00:05:48.290
So we execute it and it's supposed to be already downloaded inside this variable def.

69
00:05:49.370 --> 00:05:52.910
Let's see the first five rows to make sure it's all right.

70
00:05:53.360 --> 00:05:56.060
We put df dot head and run it.

71
00:05:56.840 --> 00:06:01.220
Here we have the first five passengers of which only three have survived.

72
00:06:02.150 --> 00:06:10.520
Then we have the data class, gender, age, and if it had brothers, children, etc. Well, this is

73
00:06:10.520 --> 00:06:11.480
our data frame.

74
00:06:11.480 --> 00:06:12.980
We can now work with it.

75
00:06:13.610 --> 00:06:17.870
And as we saw in the introduction, in order to be able to analyze our information, we're going to

76
00:06:17.870 --> 00:06:20.750
need to separate what our data frame contains.

77
00:06:21.740 --> 00:06:28.190
On the other hand, we're going to put in the matrix x all the data from class to parents and children,

78
00:06:29.030 --> 00:06:36.770
and in another variable the variable Y only survivor in order to be separately the predictor data and

79
00:06:36.770 --> 00:06:38.030
the values to predict.

80
00:06:38.630 --> 00:06:40.280
That's what we're going to do now.

81
00:06:41.090 --> 00:06:47.240
So we're going to store in the variable X, but it's going to be a capital X this time to represent

82
00:06:47.240 --> 00:06:53.780
that it contains all the lowercase x of our data frame, and we're going to set it to be equal to d

83
00:06:53.780 --> 00:06:57.140
f, which is the object we already have with all the data.

84
00:06:57.380 --> 00:07:03.860
But applying the drop method and what we're going to take out of here is Survivor, which is the first

85
00:07:03.860 --> 00:07:04.760
of the columns.

86
00:07:05.270 --> 00:07:08.420
We don't want Survivor to be with our predictor data.

87
00:07:09.680 --> 00:07:14.960
And the second parameter of the drop method is to report which access survivor corresponds to.

88
00:07:15.350 --> 00:07:20.150
If it is a zero, it means it's a row, and if it's a one, it means it's a column.

89
00:07:21.290 --> 00:07:23.180
In our case, Survivor is a column.

90
00:07:23.180 --> 00:07:24.260
So we put one.

91
00:07:24.950 --> 00:07:30.770
Then we're going to store in the variable Y to the column survivor, which is the one we've left out.

92
00:07:31.550 --> 00:07:36.620
We do it as simple as DF Survivor and we've got it saved.

93
00:07:37.850 --> 00:07:42.200
We run this cell and our data is now stored in these two separate variables.

94
00:07:42.980 --> 00:07:46.400
Well, now let's visualize how our X has turned out.

95
00:07:46.850 --> 00:07:53.990
For that we put only X in capital letters, dot head, and here we see the first five lines of this

96
00:07:53.990 --> 00:07:57.260
data frame that does not have the column survivor.

97
00:07:58.190 --> 00:08:03.860
And if we visualize y head, we also see five rows but only of the column survivor.

98
00:08:04.700 --> 00:08:10.790
The first column has the indexes and the second one says zero or one, depending on whether it's survived

99
00:08:10.790 --> 00:08:11.450
or not.

100
00:08:12.290 --> 00:08:14.690
So we are now ready to create our tree.

101
00:08:15.930 --> 00:08:22.530
We're going to create a variable called My Tree, which is the same as this this method that has already

102
00:08:22.530 --> 00:08:25.620
been imported, which is decision tree classifier.

103
00:08:26.070 --> 00:08:31.710
And in its brackets, we're going to put two parameters that for now you won't be able to fully understand.

104
00:08:32.010 --> 00:08:38.070
But when we see what a tree looks like, you will be able to do not only understand it but also modify

105
00:08:38.070 --> 00:08:38.400
it.

106
00:08:38.790 --> 00:08:41.220
The first parameter is max depth.

107
00:08:41.610 --> 00:08:45.690
We're going to set the depth of our tree to two levels for now.

108
00:08:45.690 --> 00:08:48.570
This doesn't tell you much, but you'll see what I mean.

109
00:08:49.290 --> 00:08:52.410
And the second parameter is going to be the random state.

110
00:08:52.800 --> 00:08:56.910
This parameter basically sets how much randomness our tree is going to have.

111
00:08:57.450 --> 00:08:59.790
For now, I'm going to set it at 42.

112
00:09:00.180 --> 00:09:02.220
You can play around with this value later.

113
00:09:02.970 --> 00:09:08.070
For now, just note that there is a need to report what the values of the two parameters are going to

114
00:09:08.070 --> 00:09:08.550
be.

115
00:09:09.300 --> 00:09:14.110
Well, I've already executed this cell, but so far nothing has happened.

116
00:09:14.130 --> 00:09:19.350
We don't see any tree anywhere because all we've done so far is to create this object.

117
00:09:19.350 --> 00:09:20.760
Nothing more than that.

118
00:09:21.690 --> 00:09:27.330
So now we're going to start what we can call training the machine so that it can read the data that

119
00:09:27.330 --> 00:09:34.050
exists and understand the correlations that exist between those values and then be prepared to anticipate

120
00:09:34.050 --> 00:09:34.590
things.

121
00:09:35.130 --> 00:09:36.870
So how do we train the machine?

122
00:09:37.380 --> 00:09:39.240
So there's not a lot of signs here.

123
00:09:39.930 --> 00:09:47.010
It's as simple as calling my tree, which is the object we just created, and we apply the fit method.

124
00:09:47.970 --> 00:09:55.050
We're going to pass parameters to it first the X for the matrix of passenger characteristics, and then

125
00:09:55.050 --> 00:09:58.110
why of course, which is the one with the results.

126
00:09:58.860 --> 00:10:02.610
The one that tells us what the destination of each passenger has been.

127
00:10:03.300 --> 00:10:06.870
And that's the variable that in the future we want to be able to predict.

128
00:10:07.590 --> 00:10:10.380
If we run this, we don't see anything new either.

129
00:10:11.340 --> 00:10:17.040
Just this output here that says decision tree classifier with the data I just entered.

130
00:10:18.060 --> 00:10:24.750
So now I suggest you pause the video and read this short text, which explains in detail what has happened

131
00:10:24.750 --> 00:10:25.830
here so far.

132
00:10:26.470 --> 00:10:31.770
Okay, the model is trained, so we are in a position to start making predictions.

133
00:10:32.190 --> 00:10:38.850
That is to say we are in a position to see the relationships between X and Y and to establish which

134
00:10:38.850 --> 00:10:42.810
were the most influential factors for each person to have survived or not.

135
00:10:43.170 --> 00:10:46.830
Now we're going to make the first predictions about our data frame.

136
00:10:47.070 --> 00:10:54.030
We're going to create a variable, a variable that can be called prediction y, which is survival through

137
00:10:54.030 --> 00:11:00.510
my tree predict and in brackets X, this is the first step.

138
00:11:00.540 --> 00:11:04.530
The second step is to compare those predictions with the actual results.

139
00:11:04.770 --> 00:11:10.410
So we're going to be able to see this comparison through a print that says something like accuracy,

140
00:11:10.440 --> 00:11:17.910
colon space, and then accuracy score, which is that method we imported at the beginning of the lesson.

141
00:11:18.990 --> 00:11:25.170
So what do we want to know how accurate our program was when it identified the relationship between

142
00:11:25.170 --> 00:11:28.410
personal characteristics and the survival of each passenger?

143
00:11:29.280 --> 00:11:35.490
We can say that our program did its own guessing, and this method will compare its results with the

144
00:11:35.490 --> 00:11:39.510
actual data and tell us what percentage it got right.

145
00:11:40.710 --> 00:11:46.710
The values we need to pass here are going to be prediction Y on the one hand, and then the variable

146
00:11:46.710 --> 00:11:49.350
Y which contains the actual values.

147
00:11:50.070 --> 00:11:53.520
These are the variables the accuracy score is going to compare.

148
00:11:54.450 --> 00:12:00.150
So if we execute, this line informs us with a decimal number that the percentage of accuracy has been.

149
00:12:00.810 --> 00:12:03.270
This value could go from 0 to 1.

150
00:12:03.630 --> 00:12:05.580
So we have obtained a good result.

151
00:12:06.060 --> 00:12:08.820
Of course it is a result that can be improved.

152
00:12:09.330 --> 00:12:12.390
It would be good to get 85 or 90%.

153
00:12:12.780 --> 00:12:15.420
Getting 100% is almost impossible.

154
00:12:15.420 --> 00:12:21.360
But these values can be improved by refining our data frame and improving the way we analyze it.

155
00:12:21.690 --> 00:12:28.380
Now, in order to visualize something entertaining for our eyes, let's create a confusion matrix and

156
00:12:28.380 --> 00:12:29.820
look at that strange word.

157
00:12:29.850 --> 00:12:34.320
The idea is not to confuse us, but on the contrary, to clarify.

158
00:12:34.860 --> 00:12:36.180
But that's the name of it.

159
00:12:36.180 --> 00:12:38.100
Confusion Matrix.

160
00:12:39.180 --> 00:12:45.750
Let's put parameters y, which are the people who survived or not in the real list and the prediction

161
00:12:45.750 --> 00:12:50.670
y which are the people that the machine has predicted if they survived or not?

162
00:12:51.720 --> 00:12:57.240
If we run it, we're going to get this double entry table, which for now is not too clear either.

163
00:12:57.930 --> 00:13:03.450
Here we have some numbers that we are not able to understand, but we will understand them when we make

164
00:13:03.450 --> 00:13:06.270
a graph with this, which is what follows.

165
00:13:06.480 --> 00:13:11.460
And that will make it very clear to us what these numbers are that are matrix as crossed.

166
00:13:11.880 --> 00:13:15.210
So we're going to represent it in a graph, creating a confusion.

167
00:13:15.740 --> 00:13:19.310
Plot to which we have to pass several parameters.

168
00:13:19.700 --> 00:13:25.310
First, the estimate we already have an estimate of created, which is my tree.

169
00:13:26.090 --> 00:13:31.910
Then we have to pass it the capital X, which are the real variables that we have of the people in our

170
00:13:31.910 --> 00:13:32.750
data frame.

171
00:13:33.110 --> 00:13:40.010
The parameter y true refers to the database that contains the true results, not the predicted ones.

172
00:13:40.370 --> 00:13:41.870
So it is y.

173
00:13:42.770 --> 00:13:44.990
And then the other parameters are optional.

174
00:13:45.020 --> 00:13:46.700
None of them are mandatory.

175
00:13:47.060 --> 00:13:49.400
What we're going to do now is give this a color.

176
00:13:49.940 --> 00:13:55.190
We're going to call the C map parameter, which allows us to color map our graph.

177
00:13:55.910 --> 00:14:03.980
We're going to make it equal to PLT dot cm for color map dot blues, which is a type of color map in

178
00:14:03.980 --> 00:14:11.150
blue ranges so that we can visualize the results we get with the color and the value format is going

179
00:14:11.150 --> 00:14:14.450
to be equal to 0.0 f.

180
00:14:15.020 --> 00:14:18.140
This means that they are not going to have decimal numbers.

181
00:14:19.340 --> 00:14:24.710
Now let's run this and we get our first graph which is made up of this same table.

182
00:14:24.740 --> 00:14:31.760
c40 717 and 124 and 166.

183
00:14:32.090 --> 00:14:33.740
But let's see what it's telling us.

184
00:14:34.100 --> 00:14:40.010
On the vertical axis, we have the true label, and on the horizontal axis we have the predicted label

185
00:14:40.550 --> 00:14:42.680
and we have the zero and the one.

186
00:14:43.520 --> 00:14:45.680
So let's try to analyze this table.

187
00:14:46.250 --> 00:14:52.460
In this quadrant, the actual not survivors and the predicted not survivors are crossed.

188
00:14:53.030 --> 00:14:59.510
Here it says there were 407 and it's painted in a very dark blue, which means that the coincidence

189
00:14:59.510 --> 00:15:00.230
is high.

190
00:15:00.680 --> 00:15:06.500
Then in this other quadrant, we have the crossover between the predicted survivors and the real survivors.

191
00:15:07.520 --> 00:15:10.340
It matched in 166 cases.

192
00:15:10.820 --> 00:15:15.920
This gives us a value that is not as obscure as the previous one, but it is quite high.

193
00:15:16.250 --> 00:15:19.280
So these two quadrants which occupy this diagonal.

194
00:15:19.340 --> 00:15:23.960
Show us how accurate our tree has been in predicting who survived or not.

195
00:15:24.980 --> 00:15:30.620
And to make this a bit better, let's normalize this data so that instead of absolute values, we have

196
00:15:30.620 --> 00:15:31.940
percentage values.

197
00:15:32.420 --> 00:15:39.740
So let's copy this same line, because here we really just want to change the format to 0.2.

198
00:15:40.220 --> 00:15:46.970
And we're going to add one more parameter here which is normalized true attention that here true is

199
00:15:46.970 --> 00:15:48.050
with lowercase.

200
00:15:49.190 --> 00:15:51.860
So let's run this and we have the same picture.

201
00:15:52.070 --> 00:15:56.450
But now look how interesting we can see everything in percentages.

202
00:15:57.380 --> 00:16:03.410
There were 96% and 57% matches between the predictions and the actual data.

203
00:16:03.920 --> 00:16:07.280
This shows us how assertive our model can be.

204
00:16:08.000 --> 00:16:13.430
The chances of our model being wrong are quite low, especially for guessing who does not survive.

205
00:16:13.820 --> 00:16:16.040
That is where it's been most successful.

206
00:16:16.370 --> 00:16:22.070
We're going to show what is the tree that our system is used to make decisions and this is actually

207
00:16:22.070 --> 00:16:23.390
what we want to build.

208
00:16:23.900 --> 00:16:32.180
We're going to call PLT with the method figure, which we can give a size with the parameter fig size,

209
00:16:32.180 --> 00:16:35.300
which is equal to a tuple of ten and eight.

210
00:16:35.750 --> 00:16:42.440
Here I set the size in which my tree will be graphically represented in a new line I'm going to call

211
00:16:42.440 --> 00:16:43.940
tree dot plot.

212
00:16:44.420 --> 00:16:52.130
Remember that tree is a CI kit learn object and we can call plot tree, which is going to ask us for

213
00:16:52.130 --> 00:16:53.150
some parameters.

214
00:16:54.170 --> 00:17:02.210
First, the decision tree, which is my tree, and after that it doesn't ask for any mandatory parameter.

215
00:17:03.020 --> 00:17:08.360
The others are optional and I'm going to add what I want the colors of the tree to show.

216
00:17:08.960 --> 00:17:11.690
So I want filled to be equal to true.

217
00:17:11.720 --> 00:17:13.850
This time is with a capital letter.

218
00:17:15.320 --> 00:17:17.570
I also want to see the names of our attributes.

219
00:17:17.570 --> 00:17:21.440
So feature names is going to be equal to x columns.

220
00:17:22.160 --> 00:17:26.960
And with this we can see it, but we only need to make PLT show up with show.

221
00:17:28.010 --> 00:17:30.050
Do you want to see our decision tree?

222
00:17:30.950 --> 00:17:34.130
Here it is being built and here it has arrived.

223
00:17:34.610 --> 00:17:37.550
This is the decision tree that has been created.

224
00:17:37.850 --> 00:17:41.420
And here we are going to understand how our machine has thought.

225
00:17:41.690 --> 00:17:47.810
What was the process it went through to be able to solve each case and determine whether it is a survivor

226
00:17:47.810 --> 00:17:48.500
or not.

227
00:17:48.800 --> 00:17:54.260
First of all, you have to understand that it went through this tree from the top down here are two

228
00:17:54.260 --> 00:17:58.040
levels deep that we set up when we set up the tree.

229
00:17:58.940 --> 00:18:01.100
It has this level and then one more.

230
00:18:02.030 --> 00:18:08.390
It starts here and has established, based on its observations, that gender is divided into two values,

231
00:18:08.390 --> 00:18:09.710
zero and one.

232
00:18:10.250 --> 00:18:15.410
So at first checks if it has a gender less than 0.5, i.e. it's equal.

233
00:18:15.590 --> 00:18:16.160
Zero.

234
00:18:16.160 --> 00:18:18.550
That means it's male to the left.

235
00:18:18.560 --> 00:18:20.840
Go to the cases where it is true.

236
00:18:20.870 --> 00:18:22.190
All of these are men.

237
00:18:22.340 --> 00:18:25.010
And to the right, the cases where they are female.

238
00:18:25.520 --> 00:18:28.370
So now it asks different questions for each case.

239
00:18:28.370 --> 00:18:34.550
Based on the prevalence you have observed during your training for the cases that fell on the left side.

240
00:18:34.550 --> 00:18:35.180
The men.

241
00:18:35.220 --> 00:18:38.120
It asks if age is less than 6.5.

242
00:18:38.240 --> 00:18:39.890
Why did it ask this question?

243
00:18:39.890 --> 00:18:41.960
And why would this particular number?

244
00:18:42.470 --> 00:18:48.170
Because it has identified, according to the cause, the theory of the data frame that the survivors

245
00:18:48.170 --> 00:18:54.200
or not survivors are divided into two groups that are pretty much determined by this age number as a

246
00:18:54.200 --> 00:18:54.860
cutoff.

247
00:18:55.070 --> 00:18:57.650
So here it asks them each case.

248
00:18:58.310 --> 00:18:59.060
Listen.

249
00:18:59.090 --> 00:19:01.490
Are you younger than 6.5 years old?

250
00:19:02.180 --> 00:19:04.850
Those cases that are go to the left again.

251
00:19:05.060 --> 00:19:07.250
True values always go to the left.

252
00:19:07.580 --> 00:19:10.400
And here it determines their chances of survival.

253
00:19:10.760 --> 00:19:15.410
The blue labels belong to the survivors and the orange labels to the non survivors.

254
00:19:16.100 --> 00:19:17.870
So who survived the most?

255
00:19:18.290 --> 00:19:20.960
On the male side, the most survivors are children.

256
00:19:20.960 --> 00:19:27.440
Under six and a half years of age, men over six and a half have a lower chance of survival.

257
00:19:27.680 --> 00:19:31.730
And our decision tree applies a non survivor label to these people.

258
00:19:33.020 --> 00:19:38.570
Then if the people were women, those on the right, the tree observed that what most determine the

259
00:19:38.570 --> 00:19:41.390
fate of these people was not age but class.

260
00:19:42.320 --> 00:19:47.930
It has identified that those who traveled in first or second class have a better chance of survival.

261
00:19:48.470 --> 00:19:53.900
We can even see from this intensity of the blue that women in first class are more likely to survive

262
00:19:53.930 --> 00:19:56.720
than men under six and a half years of age.

263
00:19:57.770 --> 00:20:05.060
Our system can be wrong, of course, but here it tells us how likely it is to get it wrong or right.

264
00:20:05.450 --> 00:20:10.850
This Gini number is the level of impurity that this label has.

265
00:20:12.320 --> 00:20:18.980
The lower this number is, the more likely it is to get it right, because the less impurities, the

266
00:20:18.980 --> 00:20:20.690
fewer mistakes it is made.

267
00:20:21.350 --> 00:20:27.260
We see that the most pure label, because it has the lowest Gini number, is this one that says first

268
00:20:27.260 --> 00:20:33.440
and second class women are more survivors than the others, and it is the label that is most likely

269
00:20:33.440 --> 00:20:34.310
to be right.

270
00:20:34.670 --> 00:20:40.910
Now, let's see what happens if we give my tree three levels deep instead of two, we run it and then

271
00:20:40.910 --> 00:20:45.230
we run everything that follows so that the following cells are updated to this change.

272
00:20:45.620 --> 00:20:51.440
Here we run my tree again and see how it works with three levels of depth.

273
00:20:51.440 --> 00:20:58.310
After dividing the cases by gender, it divides again the males by age with the same limit and the females

274
00:20:58.310 --> 00:20:59.060
by class.

275
00:20:59.690 --> 00:21:04.910
In the case of boys, after classifying them by age, it divides the children according to whether or

276
00:21:04.910 --> 00:21:10.760
not they have brothers or partners, and finds that those who do have brothers or partners have survived

277
00:21:10.760 --> 00:21:12.590
longer than those who do not.

278
00:21:13.040 --> 00:21:20.480
In both cases, the Gini is very low, so these conclusions are quite reliable in the case of men over

279
00:21:20.480 --> 00:21:21.360
six and a half.

280
00:21:21.380 --> 00:21:27.590
It has divided them according to class and so it has made conclusions according to all these classifications

281
00:21:27.590 --> 00:21:28.730
for each case.

282
00:21:29.030 --> 00:21:34.670
You can look at this tree in detail to try and understand the reasoning of this tree for the particular

283
00:21:34.670 --> 00:21:35.510
data frame.

284
00:21:35.930 --> 00:21:42.890
I think this says a lot to keep playing with and I recommend you try different parameters not only randomness

285
00:21:42.890 --> 00:21:48.680
but also depth and others so that you can identify how our decision tree was working.

286
00:21:49.040 --> 00:21:55.010
This way you can identify how it builds its decisions, or which presentation mode is the one with the

287
00:21:55.010 --> 00:21:56.720
best suits your needs.

288
00:21:57.320 --> 00:22:00.980
If we want to look at this in a different way, let's go down a little further.

289
00:22:01.070 --> 00:22:06.740
And while we're at it, I'll ask you to read all of this, because I'm going to explain quite well and

290
00:22:06.740 --> 00:22:09.500
very precisely how this tree works.

291
00:22:09.920 --> 00:22:17.510
So now we're going to graph the importance of each of the categories we have age, gender, etc., to

292
00:22:17.510 --> 00:22:20.420
make a graph that shows us the importance of the categories.

293
00:22:20.600 --> 00:22:25.400
Let's first see how to create the two variables the X and the Y of this graph.

294
00:22:26.270 --> 00:22:31.430
The x variable is going to be the relevance and it is going to be equal to my tree.

295
00:22:31.820 --> 00:22:33.530
Feature importance.

296
00:22:34.190 --> 00:22:36.500
This is going to be the x variable.

297
00:22:37.250 --> 00:22:43.400
The Y variable is going to be the columns because there we have the names of the categories and then

298
00:22:43.400 --> 00:22:46.750
we take them from x dot columns.

299
00:22:46.760 --> 00:22:48.980
And here we have our second variable.

300
00:22:49.790 --> 00:22:54.050
Now we're going to create the chart itself with these variables that we already have.

301
00:22:55.250 --> 00:23:01.760
So here comes C, born with its acronym SE and S, which we import to create a chart.

302
00:23:02.360 --> 00:23:08.210
Well, now we're going to use it with the bar plot method, which is going to have columns as parameters

303
00:23:08.210 --> 00:23:10.520
on one side and relevance on the other.

304
00:23:11.570 --> 00:23:18.380
We're going to give it a title through PLT title, which is relevance of each attribute.

305
00:23:19.040 --> 00:23:22.360
And then we're going to show it with PLT Show.

306
00:23:23.540 --> 00:23:29.810
We do that and we run it and here comes our graph, which shows us that the decision tree has identified

307
00:23:29.810 --> 00:23:34.040
that gender is the most determinant factor in a person's survival.

308
00:23:34.280 --> 00:23:37.340
Then class, then age, and so on.

309
00:23:37.880 --> 00:23:43.760
This last category could not be analyzed because it would not be at the fourth level and we have only

310
00:23:43.760 --> 00:23:45.380
analysed up to a third level.

311
00:23:46.160 --> 00:23:51.290
If we were to include it, we can see how far this column goes, but logically it has to be lower than

312
00:23:51.290 --> 00:23:52.070
this one.

313
00:23:52.160 --> 00:23:53.540
What do we have here?

314
00:23:54.320 --> 00:23:57.560
On the Titanic, most of the people who survived were women.

315
00:23:58.010 --> 00:24:04.310
The next distinguishing factor of the survivors was that they were upper class and then that they were

316
00:24:04.310 --> 00:24:05.330
low of age.

317
00:24:05.900 --> 00:24:07.020
So I'm familiar.

318
00:24:07.040 --> 00:24:11.480
Women and children first for an accident with few boats.

319
00:24:11.480 --> 00:24:15.080
You would also have to say rich women and children first.

320
00:24:15.950 --> 00:24:18.590
Well, this was just one example.

321
00:24:18.710 --> 00:24:20.960
You can go much deeper if you like.

322
00:24:21.710 --> 00:24:26.930
So this has been a very intense day, and yet you have barely managed to peek into the fascinating world

323
00:24:26.930 --> 00:24:30.680
of data science, artificial intelligence and machine learning.

324
00:24:31.340 --> 00:24:37.940
This can really go much deeper, but this would require a specific course on that because it would be

325
00:24:37.940 --> 00:24:40.520
hours and hours and hours and hours and hours.

326
00:24:40.820 --> 00:24:45.080
So what I've shown you today is how we can apply Python to this world.

327
00:24:45.620 --> 00:24:47.990
Then you can go deeper if you're interested.

328
00:24:48.260 --> 00:24:50.210
You already have the basics.

329
00:24:50.540 --> 00:24:55.640
So it's time to rest and prepare yourself so that tomorrow you'll be full of energy and motivation.

330
00:24:55.940 --> 00:24:59.480
Because a project we're all looking for is coming for real.

331
00:24:59.630 --> 00:25:01.220
The Mecca of programming.

332
00:25:01.610 --> 00:25:02.510
Don't miss it.
