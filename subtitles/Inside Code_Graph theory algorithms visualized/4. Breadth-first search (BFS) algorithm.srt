1

00:00:00,229  -->  00:00:04,630
Welcome back to the course, in this lecture,
we will study the second traversal algorithm

2

00:00:04,630  -->  00:00:12,150
of this chapter: breadth-first search. Let's
take back our missions example. We started

3

00:00:12,150  -->  00:00:19,039
at mission A, we finished it, then it unlocked
3 new missions B C and D. After it we went

4

00:00:19,039  -->  00:00:25,260
to B, we unlocked E and F. In depth-first
search, we've put remaining missions of A

5

00:00:25,260  -->  00:00:31,340
on hold, which are C and D, we decided to
not go to them until we finish all the part

6

00:00:31,340  -->  00:00:37,480
of the graph by going from B, in other words
we kept going deeper and we didn't go back

7

00:00:37,480  -->  00:00:44,910
until we finished our work. But what if we
adopt a new strategy, after completing B,

8

00:00:44,910  -->  00:00:49,670
instead of going to unlocked missions of B,
we just write them somewhere and we continue

9

00:00:49,670  -->  00:00:55,110
unlocked missions of A. So we store E and
F, and we go to the next mission unlocked

10

00:00:55,110  -->  00:01:01,780
by A, which is C. Same thing again, we just
remember unlocked missions, none in this case,

11

00:01:01,780  -->  00:01:11,590
and we continue. Next mission is D, it unlocks
3 new missions H I and J, we remember them.

12

00:01:11,590  -->  00:01:18,890
And now what's the next mission? Take some
seconds to think. The next mission is E, because

13

00:01:18,890  -->  00:01:24,729
now that we finished missions of A, we can
move to missions of B. We go to E, it unlocks

14

00:01:24,729  -->  00:01:32,600
K, we remember it. Next mission is F, it unlocks
L, we remember it. Basically we're just following

15

00:01:32,600  -->  00:01:38,560
the chronological order of missions we were
remembering, so the next one is H. We go to

16

00:01:38,560  -->  00:01:44,549
it, and it doesn't unlock new missions, we
just have no new thing to remember, remembering

17

00:01:44,549  -->  00:01:51,820
means that we need to go to it after finishing
missions that come before it. Next mission

18

00:01:51,820  -->  00:01:58,380
is I. It unlocks D, J, and O, but D and J
have already been unlocked before, so the

19

00:01:58,380  -->  00:02:07,710
only new mission is O, we remember it. Next
mission is J, and it unlocks no new missions.

20

00:02:07,710  -->  00:02:15,020
Next mission is K, it unlocks P as a new mission,
we remember it. Next mission is L, it unlocks

21

00:02:15,020  -->  00:02:23,510
two new missions Q and R that we remember.
Next mission is O, it doesn't unlock new missions.

22

00:02:23,510  -->  00:02:31,530
Next mission is P, no new missions. And same
thing for Q, then for R. We don't have unlocked

23

00:02:31,530  -->  00:02:39,590
missions to complete anymore, we finished
traversing the graph. Okay what just happened

24

00:02:39,590  -->  00:02:44,640
here? This technique of traversing is called
breadth-first search, instead of going deep

25

00:02:44,640  -->  00:02:50,890
into one path, we decide to traverse the graph
level by level. Which is what happened here,

26

00:02:50,890  -->  00:02:57,480
at the first level we had A, then in the next
level we had B C and D, then in the next level

27

00:02:57,480  -->  00:03:07,560
we had E F H I J, then K L O, then finally
P Q R. And it's the idea of breadth-first

28

00:03:07,560  -->  00:03:13,830
search, we don't move to the next level until
we finish traversing all vertices of the actual

29

00:03:13,830  -->  00:03:19,910
level. And to be able to do so, when being
on a vertex, instead of directly going to

30

00:03:19,910  -->  00:03:25,780
its neighbors, we just remember unvisited
ones to be able to go back to them later,

31

00:03:25,780  -->  00:03:33,060
once we finish vertices of the actual level.
And remembering is important, because the

32

00:03:33,060  -->  00:03:38,700
structure determines the chronological order
of elements we still need to traverse, but

33

00:03:38,700  -->  00:03:44,230
it also saves their reference. And having
their reference is crucial, for example here

34

00:03:44,230  -->  00:03:49,920
we went from D to E, which would not be possible
without having the reference of E, you can

35

00:03:49,920  -->  00:03:55,200
see that there is no edge from D to E, so
we need to have the reference of E somewhere

36

00:03:55,200  -->  00:04:03,090
else to be able to jump. In brief, for example
for missions, at the beginning we have only

37

00:04:03,090  -->  00:04:09,320
one unlocked mission, the mission A. After
completing it, we unlock three new missions

38

00:04:09,320  -->  00:04:18,169
B C and D. And we follow this order, B, C,
D, so we start with B. B unlocks new missions

39

00:04:18,169  -->  00:04:24,600
E and F, but as said earlier, we don't go
to them, we just remember them. And to remember

40

00:04:24,600  -->  00:04:29,729
them, we put them at the end of the list of
missions that we remembered before, in our

41

00:04:29,729  -->  00:04:39,181
case C and D. Etcetera etcetera until this
structure becomes empty. By the way, what

42

00:04:39,181  -->  00:04:44,639
structure do we use? You can see that to remember
a mission, we're putting it at the end of

43

00:04:44,639  -->  00:04:49,800
the structure, and the next mission to be
done is extracted from the beginning of the

44

00:04:49,800  -->  00:04:57,130
structure. Basically this structure is organizing
items in chronological order, the first element

45

00:04:57,130  -->  00:05:02,540
that comes in is the first one that goes out,
in other words, the first mission that gets

46

00:05:02,540  -->  00:05:08,729
remembered is the first mission that gets
completed. And what structure works on First

47

00:05:08,729  -->  00:05:15,560
In First Out principle? It's the queue. Because
yes, we're kinda using a queue here, new missions

48

00:05:15,560  -->  00:05:21,530
that enter the structure have to wait until
missions that are already there get completed

49

00:05:21,530  -->  00:05:29,740
before their turn comes, like a real life
queue. Now we've seen how does breadth-first

50

00:05:29,740  -->  00:05:37,970
search work, we've discussed what structure
to use, let's try to write some pseudocode.

51

00:05:37,970  -->  00:05:44,610
Our function needs the initial mission, 'A'
in our example, and the set of visited missions.

52

00:05:44,610  -->  00:05:49,250
Inside the function, we've seen that we need
a queue where we put unlocked missions that

53

00:05:49,250  -->  00:05:55,130
we didn't complete yet. Initially, the only
unlocked mission is the one from where we

54

00:05:55,130  -->  00:06:01,729
start, we directly put it in the queue. And
here we consider the mission as visited as

55

00:06:01,729  -->  00:06:08,330
soon as it enters the queue, even if it's
still not its turn. The reason behind it is

56

00:06:08,330  -->  00:06:16,160
to avoid enqueuing it again. This is why we
put our initial mission as visited. Now we

57

00:06:16,160  -->  00:06:21,660
can start working, we said that we keep looping
while we still have unlocked missions that

58

00:06:21,660  -->  00:06:29,340
we didn't complete yet. We write, while the
queue is not empty. Inside the loop, we extract

59

00:06:29,340  -->  00:06:35,690
a mission, the mission that we will complete
next, for that we just pop from the queue.

60

00:06:35,690  -->  00:06:41,410
We complete the mission, then we need to enqueue
new missions it unlocks. For that, for each

61

00:06:41,410  -->  00:06:46,660
mission it unlocks, we first make sure that
it's not visited, to avoid enqueuing the same

62

00:06:46,660  -->  00:06:54,319
mission more than once. If it's not visited,
we just enqueue it, and we put it in visited,

63

00:06:54,319  -->  00:06:59,139
remember that we consider a mission as visited
as soon as it enters the queue, not until

64

00:06:59,139  -->  00:07:05,470
it gets popped. And we're done, this loop
will perform the process we've seen in the

65

00:07:05,470  -->  00:07:10,810
example, we pop a mission, we complete it,
and we remember new missions that we didn't

66

00:07:10,810  -->  00:07:19,520
unlock before. Let's try to write the pseudocode
with general terms now, not missions. We have

67

00:07:19,520  -->  00:07:25,360
as parameters the graph, the initial vertex,
and the set of visited vertices. We create

68

00:07:25,360  -->  00:07:31,490
the queue where we put the vertex. We set
our vertex as visited. Then while the queue

69

00:07:31,490  -->  00:07:37,180
is not empty, we pop a vertex, we process
it, we can print it for example, then for

70

00:07:37,180  -->  00:07:44,430
each neighbor, if the neighbor is not visited,
we enqueue it, and we put it in visited. That's

71

00:07:44,430  -->  00:07:50,810
it. Once again, if you still can't grasp the
concept of breadth-first search and want to

72

00:07:50,810  -->  00:07:56,240
visualize more examples, I made a code that
generates a random graph then shows you the

73

00:07:56,240  -->  00:08:06,020
execution step by step, you can run it as
much as you want. Let's try to implement this

74

00:08:06,020  -->  00:08:11,860
pseudocode in Python now. Here also, the way
we implement a graph matters, the code will

75

00:08:11,860  -->  00:08:17,979
differ depending on if we use an adjacency
list or an adjacency matrix. But the difference

76

00:08:17,979  -->  00:08:23,280
will be when traversing neighbors only, as
with depth-first search. With an adjacency

77

00:08:23,280  -->  00:08:29,270
list, we have a function bfs that takes as
parameters the graph, the initial vertex,

78

00:08:29,270  -->  00:08:35,279
and the visited set. Inside the function,
we create a queue, we put the initial vertex

79

00:08:35,279  -->  00:08:42,560
in it, we put the initial vertex in visited,
and we start looping. While the queue isn't

80

00:08:42,560  -->  00:08:48,120
empty, we pop a vertex, we print it, then
for each neighbor, we first check if it's

81

00:08:48,120  -->  00:08:54,260
not visited. If neighbor not in visited, we
put it in the queue, and we add it to the

82

00:08:54,260  -->  00:09:00,060
visited set, that's it. And you already know
that to visit neighbors of a vertex in an

83

00:09:00,060  -->  00:09:08,900
adjacency list, we just traverse the array
at key vertex. And for an adjacency matrix,

84

00:09:08,900  -->  00:09:13,839
same code, the only thing that changes is
the way we traverse neighbors, which is done

85

00:09:13,839  -->  00:09:23,240
by using the function we talked about previously.
Now remember that in dfs, we had the problem

86

00:09:23,240  -->  00:09:28,550
of the possibility of not traversing the whole
graph with one dfs call. And it's the same

87

00:09:28,550  -->  00:09:34,630
thing with bfs, if we have this graph for
example and we call bfs on A, some vertices

88

00:09:34,630  -->  00:09:41,180
will be visited but not all of them, because
not all vertices are reachable by A. Once

89

00:09:41,180  -->  00:09:50,100
again, we call bfs on every unvisited vertex.
In code, for each vertex, if it's not visited,

90

00:09:50,100  -->  00:10:00,700
we call bfs on it. Next we have time and space
complexity. For the time complexity, we first

91

00:10:00,700  -->  00:10:07,860
have the cost of traversing the vertices.
We have |V| vertices so the cost is |V|. We

92

00:10:07,860  -->  00:10:13,560
also have the cost of the while loop. Remember
that each vertex enters the queue only once,

93

00:10:13,560  -->  00:10:19,740
so in total, |V| elements will have been in
the queue. And at each iteration, one element

94

00:10:19,740  -->  00:10:24,930
is popped from the queue. We deduce that the
total number of iterations of the while loop,

95

00:10:24,930  -->  00:10:35,670
in all calls, is |V|, each vertex causes one
more iteration. But what's happening inside

96

00:10:35,670  -->  00:10:40,649
an iteration? We are popping from the queue,
costs O(1) time, then we're traversing neighbors

97

00:10:40,649  -->  00:10:46,290
of the popped vertex, whose the cost is the
degree of that vertex with an adjacency list.

98

00:10:46,290  -->  00:10:52,829
When calculating the time cost of a loop,
if the cost of one iteration remains the same,

99

00:10:52,829  -->  00:10:58,269
we just multiply it by the number of iterations.
Which is not the case here, here the cost

100

00:10:58,269  -->  00:11:03,480
of one iteration depends on the degree of
the popped vertex. And we have one iteration

101

00:11:03,480  -->  00:11:11,960
per vertex, so to get the cost, we just do
the sum of degrees of vertices. And we know

102

00:11:11,960  -->  00:11:18,380
that the sum of degrees of vertices is |E|
if the graph is directed or 2|E| if the graph

103

00:11:18,380  -->  00:11:29,420
is undirected. In total, we have |V|+|E| or
2|E|. In both cases, we get O(|V|+|E|), the

104

00:11:29,420  -->  00:11:36,070
time complexity of breadth-first search with
an adjacency list is O(|V|+|E|), same as depth-first

105

00:11:36,070  -->  00:11:43,459
search. And with an adjacency matrix, the
difference is that traversing vertices costs

106

00:11:43,459  -->  00:11:49,709
O(|V|), because we traverse the whole row.
So for the while loop, we have |V| iterations,

107

00:11:49,709  -->  00:11:59,149
each one of them costs |V|, the cost is |V|².
In total, we have |V|+|V|², which gives O(|V|²),

108

00:11:59,149  -->  00:12:07,490
we get a time complexity of O(|V|²) for breadth-first
search with an adjacency matrix. And for the

109

00:12:07,490  -->  00:12:13,020
space complexity, vertices are going in and
out of the queue, so we need to find what's

110

00:12:13,020  -->  00:12:17,630
the maximum number of vertices that can be
in the queue at the same time, to be able

111

00:12:17,630  -->  00:12:23,579
to measure its extra space. In the worst case,
in the queue we can have |V|-1 vertices at

112

00:12:23,579  -->  00:12:32,550
the same time. It occurs when we have a star
graph and we launch bfs from the center vertex.

113

00:12:32,550  -->  00:12:36,970
What happens is that at the beginning we have
only one vertex in the queue, the one from

114

00:12:36,970  -->  00:12:43,060
where we start. We pop it, we process it,
then we enqueue its unvisited neighbors. And

115

00:12:43,060  -->  00:12:50,390
here this center vertex has |V|-1 unvisited
neighbors, they all get added to the queue.

116

00:12:50,390  -->  00:13:00,089
This is why, the size of the queue becomes
|V|-1. And in bfs we also have the set of

117

00:13:00,089  -->  00:13:09,720
visited vertices, which holds all the vertices
at the end, |V|. In total, we have |V|+|V|-1,

118

00:13:09,720  -->  00:13:16,070
which gives 2|V|-1, which gives O(|V|), the
space complexity of breadth-first search is

119

00:13:16,070  -->  00:13:22,980
O(|V|), either for adjacency list or for adjacency
matrix. Now we know about depth-first search,

120

00:13:22,980  -->  00:13:31,120
we know about breadth-first search, but, which
one is better? Which one should we use? Well,

121

00:13:31,120  -->  00:13:36,870
each one has its pros and cons, so it depends
on the problem. For example dfs has an advantage

122

00:13:36,870  -->  00:13:42,750
over bfs when the vertex we're searching for
is probably deep, in other words, in the last

123

00:13:42,750  -->  00:13:49,390
levels of the graph. Why? Because dfs goes
to deep levels frequently, while bfs needs

124

00:13:49,390  -->  00:13:56,431
to traverse all previous levels before reaching
deep ones. And the opposite holds, bfs is

125

00:13:56,431  -->  00:14:02,259
more adapted to search for vertices that are
in shallow levels of the graph, not too far

126

00:14:02,259  -->  00:14:05,910
from the starting point, because shallows
levels are traversed before deep ones. Also,

127

00:14:05,910  -->  00:14:15,160
dfs is good when we want to explore paths
one by one. While bfs works on multiple paths

128

00:14:15,160  -->  00:14:21,810
at once edge by edge, it incrementally builds
multiple paths at once. Another thing, bfs

129

00:14:21,810  -->  00:14:26,860
has an advantage when we want to go from a
vertex to another with the fewest possible

130

00:14:26,860  -->  00:14:33,220
number of edges, when launching bfs from u
and it gets to v, it's guaranteed that bfs

131

00:14:33,220  -->  00:14:42,930
used the least possible amount of edges. Unlike
dfs, it's not guaranteed with dfs. In next

132

00:14:42,930  -->  00:14:48,769
lectures will use them to develop algorithms
and solve problems, which can give you a better

133

00:14:48,769  -->  00:14:54,069
understanding of dfs and bfs and their differences.
See you in the next lecture!
