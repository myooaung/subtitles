WEBVTT

00:01.350 --> 00:02.100
Hello everyone.

00:02.100 --> 00:03.220
Welcome back.

00:03.270 --> 00:08.370
In this video we're going to begin the model building process of this project and we're going to apply

00:09.000 --> 00:14.760
for a model data set and compared the results that we get out of each one and we're going to select

00:15.060 --> 00:19.320
one best model and we're going to move forward with that one in the coming years.

00:19.320 --> 00:24.120
We're also going to touch on grid search that we're going to apply to the model of our choosing.

00:24.120 --> 00:29.240
So let's begin first we're going to use logistic regression.

00:29.550 --> 00:33.010
So let's call this section logistic regression.

00:34.290 --> 00:37.400
Endless import the actual classifier.

00:37.620 --> 00:46.160
So we're going to get it from Skillern that linear model great import logistic regression.

00:46.680 --> 00:48.190
There you go.

00:48.210 --> 00:54.190
Now let's play the actual classifier and make it a Kaltech logistic regression.

00:54.390 --> 00:57.570
And we're going to add actually a couple of extra arguments.

00:57.570 --> 01:03.980
One being random state because as you guys know I like to put the random state in everything I do.

01:04.200 --> 01:06.090
And we're actually going to get a penalty.

01:06.200 --> 01:11.880
This is the same penalty that we use on the first project for quintic assignments.

01:11.880 --> 01:18.840
So this is the one penalty the last penalty is just going to make sure that our data is penalize one

01:18.840 --> 01:22.610
particular barabar has too much of a coefficient.

01:22.770 --> 01:23.830
This may not be needed.

01:23.880 --> 01:28.680
But if we add this to this model right now we're going to make sure that we're comparing the very best

01:28.680 --> 01:31.740
options of each of the different methods that we can use.

01:31.740 --> 01:40.890
So we're gonna actually fix this classifier to our trainset in our tests actually go to what training

01:40.920 --> 01:45.180
said and our response here of course.

01:45.180 --> 01:46.210
There you go.

01:46.230 --> 01:50.020
So let's create a classifier training.

01:50.100 --> 01:50.840
And there you go.

01:51.120 --> 01:52.170
That's it.

01:52.170 --> 01:54.190
So let's actually see the results that we get.

01:54.480 --> 02:02.790
So we're going to call the section predicating person and the first thing we care about is the white

02:02.810 --> 02:04.580
bread the actual preditions right.

02:04.740 --> 02:07.830
So add to that we simply get the.

02:08.850 --> 02:16.680
And we used to predict that on the test and test and there you go.

02:16.710 --> 02:19.160
We have our predictions now.

02:19.830 --> 02:24.360
What we care about of course is the accuracy but we also care about the precision the recall and the

02:24.360 --> 02:25.050
other metrics.

02:25.050 --> 02:25.490
Right.

02:25.650 --> 02:29.810
So we're going to import functions to actually those metrics for us.

02:29.820 --> 02:36.020
So from S-K learn that of course what do you guys think is this better X-ray.

02:36.240 --> 02:43.560
So from Eskil and that metrics we're going to import a confusion matrix which is a little bit later

02:45.060 --> 02:56.820
the accuracy the EF 1 score the precision and find the league position score.

02:56.820 --> 02:59.920
And finally the record.

03:00.090 --> 03:00.620
There you go.

03:00.720 --> 03:03.770
So we're going to import all these lines and we're going to move forward.

03:04.140 --> 03:11.610
Now the accuracy is now going to be a simple call to the accuracy function that we imported on the Y

03:11.610 --> 03:17.380
test and white.

03:18.110 --> 03:19.220
So there you go.

03:19.220 --> 03:20.690
That works.

03:20.830 --> 03:23.150
Now the next step is to get the precision.

03:23.150 --> 03:29.960
So we're going to call it Prak and precision is going to be what you guys expect a precision call on

03:30.000 --> 03:34.530
a white cast and the wide spread.

03:34.820 --> 03:36.590
Next we move with the recall.

03:36.660 --> 03:41.330
We've will it work and going to be a recall call.

03:41.480 --> 03:43.570
There you go.

03:49.050 --> 03:55.160
Now the last there is the one score which is going to be EF 1.

03:55.920 --> 04:00.450
And we're going to call it I guess EF 1 as well.

04:00.450 --> 04:02.800
So let's run this for.

04:03.240 --> 04:04.350
Excellent.

04:04.350 --> 04:08.390
We're going to put this rissoles in a Pandurs to Frank.

04:08.760 --> 04:10.180
So let's do that.

04:10.290 --> 04:13.900
We're going to create a panda state of frame.

04:13.960 --> 04:14.780
There you go.

04:15.110 --> 04:20.280
And he are going to be this results.

04:20.280 --> 04:26.950
So first we're going to take all the data from that is coming from a linear regression.

04:27.360 --> 04:29.870
So the first argument should be the name of the model.

04:29.970 --> 04:34.960
So there should be linear progression grant.

04:35.340 --> 04:43.650
And what we're going to add here in parentheses is Lasser because this is the lasso penalty.

04:43.740 --> 04:45.320
So that's the first one.

04:45.360 --> 04:50.280
The next thing is going to be in this band that the reframes going to be the accuracy.

04:50.380 --> 04:50.980
OK.

04:51.200 --> 04:56.360
The precision the ripple and the F-1 score.

04:57.090 --> 04:58.540
So we don't that part.

04:58.740 --> 05:01.120
And finally we're actually going to names is.

05:01.530 --> 05:04.810
The columns are going to be named depending on this.

05:04.890 --> 05:14.050
So first column is going to be modeled which is the name of the model and there as you get it in a real

05:14.090 --> 05:23.500
hell accuracy precision recall.

05:23.880 --> 05:24.820
And EF 1 score

05:28.550 --> 05:32.340
get a really tight and.

05:33.560 --> 05:35.930
So that's it.

05:35.930 --> 05:38.450
So let's run this line right here.

05:39.590 --> 05:40.380
Excellent.

05:40.400 --> 05:47.990
And if we look at the console we have our results our last role model has given us an accuracy of 56

05:47.990 --> 05:55.120
percent precision 57 percent a recall of 70 percent and the EF 1 score of 63.

05:55.180 --> 05:57.950
Now the accuracy.

05:58.020 --> 05:59.010
It's 55.

05:59.010 --> 06:01.140
It's not high at all.

06:01.170 --> 06:07.170
It's just slightly slightly off that it has the procession again 57 is good.

06:07.330 --> 06:09.850
It's it's like there is something there but it's not too high.

06:10.050 --> 06:12.190
And recall it's 70 percent.

06:12.480 --> 06:15.200
Now that means that there's some bias in this model.

06:15.300 --> 06:16.170
OK.

06:16.740 --> 06:22.920
Now if you guys want to remember the precision is the rate of true positives divided by the rate of

06:22.920 --> 06:24.630
true positives and false positives.

06:24.720 --> 06:25.710
What does that mean.

06:25.710 --> 06:33.090
That means that out of all the predictive positives we want to know how many have been predicted right

06:33.570 --> 06:34.170
and how many.

06:34.170 --> 06:36.130
Of course but they're wrong.

06:36.180 --> 06:37.850
Now for a record score.

06:38.280 --> 06:42.590
What it means is true positives divided by true positives and false negatives.

06:42.930 --> 06:47.970
That tells us a little bit different story tells us and of all the actual positives.

06:48.060 --> 06:50.300
How many do we predict to be actually true.

06:50.880 --> 06:58.380
So the recall is telling us that out of all the actual positives we have predicted them to be true around

06:58.380 --> 07:03.500
70 percent of the time which is good but for precision we have felt short.

07:03.660 --> 07:09.350
And we can see the one score is some in-between number here or on 63.

07:09.370 --> 07:13.360
Now this is how we're going to build our models going forward.

07:13.740 --> 07:19.500
So in the next video we're going to replicate this process with a couple of more models compare the

07:19.500 --> 07:25.290
results as opposed to the logistic regression model and see which one benefits the most.

07:25.290 --> 07:27.840
Thank you very much for watching and see in the next media.
