WEBVTT

00:04.260 --> 00:11.040
So in this section we're going to use a type of neural networks we're called convolutional neural networks.

00:11.040 --> 00:11.540
OK.

00:11.790 --> 00:16.200
So if you guys are not familiar with the term neural networks in general I just here are going to give

00:16.200 --> 00:20.670
you an overview of what you mean by a neural network and then we get to move forward to develop so-called

00:20.760 --> 00:22.490
convolutional neural network.

00:22.710 --> 00:28.050
And then afterwards we're going to just jump into the with a notebook and actually start to develop

00:28.050 --> 00:29.350
our code.

00:29.390 --> 00:29.940
OK.

00:30.360 --> 00:32.820
So the first step is OK how can humans learn.

00:32.820 --> 00:40.770
The whole idea of designing artificial neural network is to imitate our human brain how our human brain

00:40.770 --> 00:41.190
works.

00:41.190 --> 00:45.120
We're just going to take whatever whatever we have with the theory behind it.

00:45.270 --> 00:47.790
I was going to do it were presented in a mathematical form.

00:48.000 --> 00:49.110
It's really powerful.

00:49.110 --> 00:55.220
It's really like really really passionate personally about like neural networks in general and you know

00:55.230 --> 00:56.440
deep learning in general.

00:56.700 --> 01:03.420
So as you can see here this kind of image that shows how can we humans learn in general.

01:03.630 --> 01:06.360
So I wouldn't central nervous system in our brain in general.

01:06.360 --> 01:08.850
We have kind of billions of neurons.

01:08.850 --> 01:10.430
These neurons keeps firing.

01:10.490 --> 01:11.050
OK.

01:11.220 --> 01:17.220
So when you say fighting that means these neurons collect data or collect information from others what

01:17.220 --> 01:18.600
we call a dendrites.

01:18.660 --> 01:19.380
OK.

01:19.380 --> 01:23.400
Process information within the nucleus and generate the output.

01:23.520 --> 01:24.240
OK.

01:24.310 --> 01:29.730
Or the learnings out of it into what we call the axon which is kind of the output of the neuro in a

01:29.730 --> 01:30.500
very simple form.

01:30.600 --> 01:31.100
OK.

01:31.140 --> 01:36.210
And these neurons billions of neurons are talking to each other as we are learning as we go and the

01:36.210 --> 01:42.240
learning happens by changing kind of the trends of the what we call it the weight here.

01:42.300 --> 01:44.440
As we begin to describe it in mathematical form.

01:44.630 --> 01:45.280
OK.

01:45.700 --> 01:51.420
So then you can simply collect signal from input channels or call dendrites process the information

01:51.570 --> 01:54.930
in the nucleus and generate an output to the axon.

01:55.020 --> 01:55.580
OK.

01:55.980 --> 02:01.440
So human learners as we adaptively change the strengths of the bonds between the neurons.

02:01.440 --> 02:03.840
So we change the sense of the bonds between the neurons.

02:03.840 --> 02:04.990
That's how humans learn.

02:05.250 --> 02:06.870
And that's what we want to do it mathematically.

02:06.870 --> 02:08.270
OK so how can we do it.

02:08.270 --> 02:10.810
And how can we present a Meuron in a mathematical form.

02:10.830 --> 02:15.540
So we can do it in a code then we can put it on a microprocessor or computer or whatever and then we

02:15.540 --> 02:19.410
can actually kind of create our many brain Percey.

02:19.470 --> 02:19.890
All right.

02:19.890 --> 02:23.970
So as you can see here this is basically the model of our neuron in a very simple form.

02:24.300 --> 02:29.970
We took our human neuron and it presented it mathematical For OK.

02:30.100 --> 02:30.430
OK.

02:30.450 --> 02:34.670
Our neuron our biological neuron in a way it has a nucleus right.

02:34.680 --> 02:38.820
So let's sort of present this simply by having kind of a summation element.

02:38.820 --> 02:44.670
OK we can have certain inputs bunch of inputs these inputs and multiply them by weights.

02:44.820 --> 02:45.440
OK.

02:45.810 --> 02:47.900
And then we're going to do kind of an additional.

02:47.910 --> 02:54.600
So we can multiply the inputs by the way it's sum them up and add kind of a bias that can shift basically

02:54.600 --> 02:57.750
the learning or shift the values up and down.

02:57.810 --> 02:58.340
Okay.

02:58.560 --> 03:02.960
And then afterwards we can the output from here can just multiply by.

03:03.000 --> 03:06.960
Again I don't want to go into the math mathematics when applied because what's called activation function

03:06.990 --> 03:07.530
f.

03:07.740 --> 03:12.120
It might be let's say exponential value which might be let's say sigmoid function it can be whatever

03:12.120 --> 03:12.770
function.

03:13.050 --> 03:18.340
And then the output we're going to be connected possibly to another neuron and so on so forth let's

03:18.360 --> 03:22.680
get hundreds of them let's get thousands of these neurons connect them together.

03:22.800 --> 03:25.160
And congratulations you have just a brain.

03:25.170 --> 03:27.190
You can't teach that many brain whatever you want.

03:27.270 --> 03:29.180
OK so what does the network look like.

03:29.220 --> 03:34.590
That's actually a very simple form what a neural network or artificial neural network looks like.

03:34.740 --> 03:37.630
It has a bunch of inputs right.

03:37.770 --> 03:41.580
It has bunch of outputs and a bunch of what we call it hidden layers.

03:41.640 --> 03:46.380
We're just going to put whatever values or even numbers of people they are as much as we can in here

03:47.080 --> 03:51.670
and we simply connect all the inputs to the hidden layers.

03:51.900 --> 03:54.470
So each of the neurons connected to all the news is next.

03:54.810 --> 03:59.520
And then all the neurons connect to the current all the newest next and the next layer the hidden layer

03:59.610 --> 04:01.690
all of them will connect to the next layer and so on.

04:01.740 --> 04:04.890
So guys can see here connect the output one can operate to and so on.

04:04.930 --> 04:07.660
And that's how can we build our intelligence in a way.

04:07.950 --> 04:14.610
OK so the question is how can we learn in the same fashion as humans learn by changing again the bonds

04:14.610 --> 04:17.000
trends between the neurons.

04:17.040 --> 04:18.390
What we going to do simply.

04:18.390 --> 04:25.020
We're going to change the weights value once we change the weights value then we can kind of adjust

04:25.020 --> 04:28.530
this network to learn specific patterns to tell us.

04:28.530 --> 04:29.780
For instance in this case.

04:29.790 --> 04:30.480
OK.

04:30.630 --> 04:36.600
Once they see this kind of image OK I can tell you that this image tell us kind of a bag or maybe this

04:36.600 --> 04:39.090
image that's kind of a hat and so on so forth.

04:39.100 --> 04:39.520
OK.

04:39.690 --> 04:45.520
That's the overall idea of designing or building an artificial one that was obviously a lot of math.

04:45.660 --> 04:47.030
There's a lot of training behind that.

04:47.160 --> 04:51.480
But this is what I want you to know when it comes to how to build just a simple neuron and build the

04:51.480 --> 04:52.500
multiple layer of it.

04:52.500 --> 04:56.430
And we'll see it's really simple very easy but you have been out fishing or network in general.

04:56.760 --> 04:57.430
OK.

04:57.780 --> 04:58.400
All right.

04:58.590 --> 05:02.610
So again let's take a look at how can we perform kind of our training.

05:02.660 --> 05:05.980
So I think it's here we have our network OK on your network.

05:06.120 --> 05:10.740
Here we have our 10 12 classes which is again cendol code church whatever.

05:11.010 --> 05:11.540
OK.

05:11.670 --> 05:13.340
And here we have our Tany data sets.

05:13.350 --> 05:17.120
Here we have all our dataset which is like you know 70000 images.

05:17.340 --> 05:23.630
And our objective is we want to simply train the network to change these weights so we can tell us simply

05:23.840 --> 05:27.350
ok when I see this image then I can classify it one of these classes.

05:27.720 --> 05:27.990
OK.

05:27.990 --> 05:35.190
So someone can tell me OK we're going to take the 28 by 28 pixels or simply I'm going to go take a row

05:35.310 --> 05:39.630
which indicates an image and use it as an input directly here.

05:39.990 --> 05:41.530
And let's go and train the network.

05:41.600 --> 05:42.660
OK.

05:43.230 --> 05:45.410
If you do this you will get terrible results.

05:45.420 --> 05:45.990
OK.

05:46.050 --> 05:47.790
Why that.

05:48.030 --> 05:51.240
The point is we cannot treat networks in general.

05:51.330 --> 05:51.920
OK.

05:52.030 --> 05:57.750
We can see images in general in a in the same fashion as we treat let's say features or say let's say

05:58.800 --> 05:59.960
other data samples.

05:59.970 --> 06:00.590
OK.

06:00.730 --> 06:06.260
When we deal with images we need to preserve and call it the spatial dependence between pixels.

06:06.540 --> 06:12.720
OK so a bag when you specify an image of a bag or the pixels are independent.

06:12.720 --> 06:14.150
On the other pixels around it.

06:14.220 --> 06:14.700
OK.

06:14.940 --> 06:20.490
That's why we need to perform kind of another process beforehand before we actually feed all the pixels

06:20.490 --> 06:21.810
directly to our network.

06:21.990 --> 06:27.660
We want to perform well convolution and that's where the term convolutional neural network came into

06:27.660 --> 06:28.090
play.

06:28.230 --> 06:28.890
OK.

06:29.550 --> 06:31.820
So let's assume are going to start with our image again.

06:31.860 --> 06:36.900
Twenty eight pixel twenty eight pixels greyscale image and we want to do that we're going to we're going

06:36.900 --> 06:39.270
to run what we call convolutional layer.

06:39.430 --> 06:40.390
OK.

06:40.390 --> 06:41.790
Again it's very simple.

06:41.820 --> 06:45.880
And I know the term might be a little bit confusing or like looks intimidating but it's very simple.

06:46.040 --> 06:48.150
Well you know what we call it the feature detectors.

06:48.290 --> 06:49.240
OK.

06:49.410 --> 06:52.390
In a very simple form we're going to add these feature detectors.

06:52.440 --> 06:54.530
OK I'm going to show you in the next slides.

06:54.540 --> 06:55.860
What do you mean by this.

06:56.240 --> 07:01.780
Well you know kind of the speech of the doctors on our image but form kind of a problem called convolution.

07:02.120 --> 07:02.670
OK.

07:02.850 --> 07:07.660
We can create these feature detectors with these feature detectors and then we're going to perform a

07:07.660 --> 07:08.600
call of pooling.

07:08.750 --> 07:14.910
OK we'll create the smaller sets of all these like pulling features.

07:15.090 --> 07:18.240
And then afterwards we're going to flatten them and feed them to our network.

07:18.360 --> 07:22.950
This kind of an overview of what's the convolution run network are we going to look like OK obviously

07:23.060 --> 07:27.700
like you know what do you mean by pulling in by kernel's what they mean by all that we can discover

07:27.750 --> 07:30.120
we're going to discuss that in a little bit more details.

07:30.120 --> 07:34.250
This is a quick overview of what we want to do throughout the module.

07:34.320 --> 07:38.310
Again we have our input image we wanted to classify the different classes.

07:38.310 --> 07:41.160
We cannot simply take the pixel and feed them directly.

07:41.210 --> 07:45.200
OK because we each pixel has to be dependent on the other pixels.

07:45.260 --> 07:49.140
So that's why we're going to come up with all this mess we're going to do convolutions we're going to

07:49.140 --> 07:50.020
do pooling.

07:50.070 --> 07:51.810
We're going to flattening and so on.

07:51.810 --> 07:54.750
So let's take a look at what do you mean by all these terms.

07:54.780 --> 07:55.480
OK.

07:55.850 --> 07:56.820
All right.

07:57.000 --> 07:57.460
All right.

07:57.510 --> 08:01.270
So I hope you guys enjoyed that section and see you in part two.
