WEBVTT

00:01.200 --> 00:02.570
Welcome back everybody.

00:02.580 --> 00:07.230
To date we're going to begin with a day at pre-processing park or a model building process.

00:07.230 --> 00:08.550
We're going to start with EPA.

00:08.580 --> 00:13.480
We can explore a little bit more so that we are more familiar with what we're working with.

00:13.620 --> 00:19.090
So let's begin first always we have to run the libraries that we're going to use.

00:19.250 --> 00:25.030
We're going to be using Pandurs model at least not by CBer.

00:25.140 --> 00:30.140
We're also going to read that data said that we're going to be working with which is found in churn

00:30.160 --> 00:33.420
underscored data that seems to be great.

00:33.800 --> 00:38.300
So do you guys remember the first step to check your data is always going to be using the help function

00:38.940 --> 00:40.420
that you're familiar with.

00:40.600 --> 00:46.930
Structuralist Luckily for us in the previous video we actually saw it through CSB and excel.

00:47.180 --> 00:49.680
So these numbers are going to be familiar to you.

00:50.000 --> 00:55.940
Also if we want to see that call names who have used columns again pretty basic.

00:55.940 --> 01:03.190
And finally when we use set that graph to see the distributions of our cars.

01:03.740 --> 01:11.540
So this is a distribution of course you sir is a code so distribution cures and really matter churn

01:12.440 --> 01:18.290
tells us that 41 percent of the users have turn over the data that we have because we have been in a

01:18.290 --> 01:19.560
very balanced dataset.

01:19.760 --> 01:20.750
That's OK.

01:21.080 --> 01:25.430
H says that 32 years is the average age for users.

01:25.430 --> 01:26.050
OK.

01:26.160 --> 01:27.100
So and so forth.

01:27.250 --> 01:30.170
And we can see these distributions are a lot better.

01:30.170 --> 01:33.740
The moment we play to rent which is going to be mobile continuing to be.

01:34.290 --> 01:36.800
Now before we plug anything.

01:36.980 --> 01:42.980
Let's first clean up data a little bit and the first thing to clean up is nobody's going to see what

01:42.980 --> 01:44.700
Noble's we have on our calls.

01:44.700 --> 01:46.410
We've can around the following line.

01:46.460 --> 01:47.670
We're going to run it.

01:47.800 --> 01:49.040
And just started it.

01:49.280 --> 01:54.330
We will apply to it which tells us if each row is an AA or NA.

01:54.620 --> 02:02.490
And of course any is going to make sure that we only get the columns to have at least one in them.

02:02.570 --> 02:09.620
So if we run this we get most of them being false meaning that none of them have any asset for each

02:10.180 --> 02:14.010
grade score and rewards for these data set.

02:14.030 --> 02:15.850
It's 27000 rows.

02:16.410 --> 02:24.170
So if the nodes are just a few they will simply remove them or be fine if the nulls are a lot more then

02:24.440 --> 02:32.270
we now make sense to happen or to verify that what we do is we're going to run the asset that you say

02:33.260 --> 02:37.510
that some because that uses the sample how many labels there are.

02:37.790 --> 02:41.690
So if we run this we see that for H.

02:41.720 --> 02:47.920
There's only four nobodies which is pretty small for those eight thousand Malis which is almost a third

02:47.920 --> 02:49.230
of the entire set.

02:49.400 --> 02:50.220
That's a lot.

02:50.540 --> 02:57.160
Then we have rewards which is thirty two thousand which is a little more than a tenth of the rose.

02:57.170 --> 03:04.050
So based on these big numbers I think it does make sense for our purposes to remove rewards earned and

03:04.060 --> 03:09.050
remove credit score from our model building process because the nos are so big.

03:10.170 --> 03:15.330
We are going to also get rid of these four nos for the way we're going to do it is simply we're going

03:15.330 --> 03:20.670
to remove the rows that are over age because only four are almost $27.

03:20.850 --> 03:27.810
So that's the first thing to get one of those four rows that have an A for h is to use the following

03:27.940 --> 03:28.470
line.

03:28.800 --> 03:37.980
So we use dataset and then we use panderers not know this is going to be applied to that particular

03:38.010 --> 03:45.390
call is going to return all the rows for which that column is not know just anything except for those

03:45.390 --> 03:47.060
two rows that we found.

03:47.310 --> 03:50.880
And this is of course going to be applied to each other.

03:51.040 --> 03:51.920
Great.

03:51.930 --> 03:56.220
Now we want to return the data set to be equal to this.

03:56.280 --> 03:58.920
So we're going to run this great.

03:59.190 --> 04:05.490
Now the next step is to get rid of those two columns the house a big number of notes and a great score

04:05.540 --> 04:06.980
and rewards.

04:07.020 --> 04:07.880
So let's.

04:08.050 --> 04:17.100
We're going to set up and you later say you know plus dropping the columns columns and that's going

04:17.100 --> 04:19.730
to be credit score

04:23.390 --> 04:25.920
and rewards are

04:29.250 --> 04:29.690
great.

04:30.020 --> 04:35.840
And you oh we forgot and let's run actually.

04:35.840 --> 04:40.210
So now we have a dataset that is clean enough for a huge difference OK.

04:40.400 --> 04:46.400
So we have finished doing that that they say we are going to do the histogram on the next meal and to

04:46.400 --> 04:48.170
see the distributions of what.

04:48.530 --> 04:53.300
Normally we wouldn't be able to do the histogram unless all the calls had their nose been removed which

04:53.300 --> 04:53.990
which is they.

04:54.110 --> 04:55.940
So we're ready for that part.

04:56.360 --> 04:58.880
Thank you very much for watching and seeing the next week.
