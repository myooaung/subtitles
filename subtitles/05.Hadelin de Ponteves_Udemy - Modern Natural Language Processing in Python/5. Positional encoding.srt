1
00:00:00,330 --> 00:00:02,450
Hi and welcome back to this A.P. course.

2
00:00:02,460 --> 00:00:05,640
So let's start with the first detail of the architecture.

3
00:00:05,850 --> 00:00:11,340
After the attention mechanism and the one we will deal with right now is the positional encoding.

4
00:00:11,340 --> 00:00:12,630
So what is it.

5
00:00:12,660 --> 00:00:17,190
Let's have a quick look at the architecture that we can see in the paper.

6
00:00:17,250 --> 00:00:22,070
So we already notice that there is these huge encoder he and decoder there.

7
00:00:22,230 --> 00:00:28,400
But each one starts after an embedding layer with a positional encoding so we can see that there is

8
00:00:28,670 --> 00:00:34,700
a plus because we add to the embedded words what we call a positional encoding.

9
00:00:34,700 --> 00:00:37,140
So why do we do that.

10
00:00:37,160 --> 00:00:43,850
The thing is that's with CNN so conversational neural networks all our innards recommend to all networks.

11
00:00:43,850 --> 00:00:50,120
The order of the woods had an impact on the process for instance with the convolution on the networks.

12
00:00:50,210 --> 00:00:56,480
The fact that we use a small filter that goes through the whole sentence it means that's words that

13
00:00:56,510 --> 00:00:59,340
are close to each other in a sentence.

14
00:00:59,380 --> 00:01:00,930
Will it be in the same filter.

15
00:01:01,190 --> 00:01:08,060
So the order of the words has an importance and an inverting two words will change the outputs of all

16
00:01:08,060 --> 00:01:13,460
CNN Just because two words might maybe not appear anymore in the same filter.

17
00:01:13,670 --> 00:01:19,040
And for the audience and of course the fact that we feed the words one after another gives a real importance

18
00:01:19,040 --> 00:01:25,250
to the order of the woods words but here for a transformer it's not the case anymore because we computes

19
00:01:25,370 --> 00:01:31,040
the attention mechanism in a global way and we give the same role to each woods.

20
00:01:31,040 --> 00:01:37,250
So for instance let's say we don't use a positional encoding and let's say we have an English sentence

21
00:01:37,280 --> 00:01:42,790
as input for all and codes and we want to translate it into a French sentence so the inputs and the

22
00:01:42,800 --> 00:01:46,280
output of all decoder will be different sentence.

23
00:01:46,280 --> 00:01:54,260
So let's imagine that we invert the first and the third words for instance in our initial English sentence

24
00:01:54,950 --> 00:02:02,060
in all set attention mechanism we will just again invert the first and the third element of the sequence

25
00:02:02,120 --> 00:02:07,700
because words that were previously related to the first elements are not related to the third elements.

26
00:02:07,700 --> 00:02:14,090
So when we we compose the sentence at the end of the attention mechanism we we compose it's in the third

27
00:02:14,150 --> 00:02:15,630
element instead of the first.

28
00:02:15,650 --> 00:02:21,050
So we just keep the inversion and same thing for the first elements instead of the third.

29
00:02:21,050 --> 00:02:27,890
So basically at the very beginning in the inputs of all coda if we inverts the first and the third position

30
00:02:28,190 --> 00:02:32,070
it will just keep this inversion throughout the whole encoding phase.

31
00:02:32,120 --> 00:02:36,410
Now let's see what happens in this multi attention sub layer.

32
00:02:36,590 --> 00:02:41,980
We have a French sentence here and in the output of this sub layer each elements ie.

33
00:02:42,170 --> 00:02:49,190
As we said before is made of all the elements from the output of the encoder and we we compose them

34
00:02:49,220 --> 00:02:54,130
according to how they are related to the elements from the French sentence.

35
00:02:54,140 --> 00:03:00,200
So if in the initial sentence so we thought that the inversion the first element of our French sentence

36
00:03:00,470 --> 00:03:06,980
was related to the first elements of the English one and that the third then in the attention mechanism

37
00:03:07,130 --> 00:03:13,370
in the outputs we will get for the first elements and lots of the first elements of the English sequence

38
00:03:13,610 --> 00:03:19,070
and not a lot of the third elements of the English sequence because the first elements of the French

39
00:03:19,070 --> 00:03:25,820
sequence is really similar to the first elements of the English sentence and nuts to the third elements

40
00:03:25,910 --> 00:03:27,090
of the English sentence.

41
00:03:27,200 --> 00:03:33,650
But if we inverted the first and the third elements of the English sentence then it means that's all

42
00:03:33,710 --> 00:03:39,350
first elements of the French sentence is not related to the third element of the English sentence and

43
00:03:39,350 --> 00:03:40,320
that the first one.

44
00:03:40,340 --> 00:03:47,210
So when we will make the attention mechanism right here as output for the first elements we will get

45
00:03:47,330 --> 00:03:53,690
a lot of the third elements of the English sentence and not the first but the third elements of the

46
00:03:53,900 --> 00:03:58,850
English sentence right now was exactly the same thing as the first elements of the English sentence

47
00:03:59,090 --> 00:04:00,500
before the inversion.

48
00:04:00,500 --> 00:04:07,040
So basically at the outputs of this sub layer we get exactly the same thing as if we hadn't made the

49
00:04:07,040 --> 00:04:08,180
inversion.

50
00:04:08,180 --> 00:04:14,510
So basically any inversion in the order of the words of the input sentence will have no impact because

51
00:04:14,900 --> 00:04:20,640
in this operation right here we will just get the elements that are important for us with no regard

52
00:04:20,660 --> 00:04:22,880
to where they are in the sentence.

53
00:04:22,970 --> 00:04:28,270
And that's a shame because in all language of course the place of the words in our sentence has an importance.

54
00:04:28,270 --> 00:04:33,710
So we would like to find a way for all transformer to have an idea of the positions of the words in

55
00:04:33,710 --> 00:04:35,120
our sentence.

56
00:04:35,120 --> 00:04:40,700
So let's add a positional encoding so numbers after the embedding layer that helps keeping track of

57
00:04:40,700 --> 00:04:43,670
the positions of the words and what they do in the paper.

58
00:04:44,940 --> 00:04:49,230
Is that the use of the sine and the cosine functions in order to compute those numbers.

59
00:04:49,230 --> 00:04:50,640
That's the one to us.

60
00:04:50,700 --> 00:04:56,760
So there is not a lot of intrusion to explain right here because in the paper they say that they tried

61
00:04:56,940 --> 00:05:03,030
different numbers and the idea is just to add something as a numbers that will be different for each

62
00:05:03,030 --> 00:05:07,800
position in the sentence and for each dimension of the embedding so that we lose the symmetry between

63
00:05:07,950 --> 00:05:08,940
each position.

64
00:05:08,940 --> 00:05:13,980
So basically what we will do is that for each dimension of our embedding which is represented by the

65
00:05:13,980 --> 00:05:20,670
number I we will get assigned all the cosine function with respect to the position in the sequence.

66
00:05:20,670 --> 00:05:26,730
So here there is a graph some visual representations of some of those functions for different dimensions.

67
00:05:26,730 --> 00:05:32,790
So if we take for instance the fourth dimension of all embedding if we go through all the positions

68
00:05:32,850 --> 00:05:39,720
in a sentence of length one hundred we can see that we have a certain sine functions that has a certain

69
00:05:39,720 --> 00:05:40,450
frequency.

70
00:05:40,530 --> 00:05:47,520
But if we go to another dimension like dimension 6 we again gets the sine function with respect to the

71
00:05:47,520 --> 00:05:50,430
position in the sequence but the frequency has changed.

72
00:05:50,520 --> 00:05:56,580
So in all matrix representing our sequence we are a different number for each position in the sequence

73
00:05:56,790 --> 00:05:58,970
and each dimension in all embedding.

74
00:05:58,980 --> 00:06:04,060
So that was it for the positional encoding there is nothing really complicated or magical about that.

75
00:06:04,140 --> 00:06:09,450
It's just a tool that we had to add in order to give importance to the order of the words in all sequence

76
00:06:09,690 --> 00:06:15,150
because the attention mechanism is very global and is symmetrical with respect to the positions of the

77
00:06:15,150 --> 00:06:15,860
woods.

78
00:06:15,870 --> 00:06:18,840
So that was it for this first little step.

79
00:06:18,840 --> 00:06:23,640
We are almost done with explaining the transformer so we're getting closer and closer to implementing

80
00:06:23,670 --> 00:06:24,130
it.

81
00:06:24,210 --> 00:06:25,080
Let's finish this.

82
00:06:25,080 --> 00:06:27,360
In 2014 the next section and see you soon.
