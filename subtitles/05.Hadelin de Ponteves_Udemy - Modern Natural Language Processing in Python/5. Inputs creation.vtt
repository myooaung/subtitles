WEBVTT
1
00:00:00.210 --> 00:00:02.240
Hi and welcome back to this A.P. course.

2
00:00:02.250 --> 00:00:07.680
So now that we have all our tweets that husband cleans and so that we have a corpus made of a list of

3
00:00:07.680 --> 00:00:08.610
the different tweets.

4
00:00:08.650 --> 00:00:12.050
What you have to do now is to do what we call a tokenization face.

5
00:00:12.060 --> 00:00:17.400
That means from a sentence which is just a sequence of characters get a list of numbers each number

6
00:00:17.400 --> 00:00:18.900
corresponding to a word.

7
00:00:18.990 --> 00:00:24.630
So we could do that manually by just getting the list of all the words that are in all copies but that

8
00:00:24.630 --> 00:00:29.550
will be quite stages and we will have to face some difficulties like for instance we wouldn't know how

9
00:00:29.550 --> 00:00:32.980
to deal with a new word if I's input for I.

10
00:00:33.030 --> 00:00:35.690
Later we will have a word that we haven't seen yet.

11
00:00:35.700 --> 00:00:38.190
It would be hard to give a meaning to it.

12
00:00:38.190 --> 00:00:43.830
So we will use a tool that exists in the tensor flow that sets module that we loaded before it will

13
00:00:43.830 --> 00:00:44.940
do the whole job for us.

14
00:00:44.970 --> 00:00:50.730
So we will just give it a corpus also give it a target for capsize and it will build what we call an

15
00:00:50.730 --> 00:00:56.430
encoder which is an object that can transform any standout string sentence into the list of numbers

16
00:00:56.490 --> 00:00:58.470
that will be used later by 0 8.

17
00:00:58.620 --> 00:01:08.520
So the way we do that is by declaring our took a NISA equals T F G S for 10 simple datasets that features

18
00:01:08.790 --> 00:01:25.820
the texts that sub would texts and coda that build from Corpus corpus and as I said before we will give

19
00:01:25.820 --> 00:01:27.720
it all corpus.

20
00:01:27.740 --> 00:01:37.820
So the list of all strings and a target vocab size which is a number of words that we want to see in

21
00:01:37.820 --> 00:01:43.820
our vocabulary and it's OK if it's lower than the actual number of words in the language or even in

22
00:01:43.820 --> 00:01:44.660
the corpus.

23
00:01:44.660 --> 00:01:49.850
Actually it could even be positive to do that because with words that do not have a number attached

24
00:01:49.850 --> 00:01:54.810
to them with the you either the encoder will compose it with words that already exists.

25
00:01:54.830 --> 00:02:00.110
Well most of the time it will be letter by letter but that actually can be quite useful and sometimes

26
00:02:00.110 --> 00:02:03.740
it can be powerful with a word that only appears one time in all copies.

27
00:02:03.740 --> 00:02:08.840
For instance it's difficult to get meaning out of it when we don't have many data points with its so

28
00:02:08.870 --> 00:02:13.040
decomposing it might be a more efficient way to process it and to give meaning to it.

29
00:02:13.070 --> 00:02:19.940
So the vocab size that I will use he is around sixty four thousands words which is a pretty big number

30
00:02:20.110 --> 00:02:24.560
but it's too less than the total number of words that exist in English and probably the number of words

31
00:02:24.590 --> 00:02:26.180
that we have in our coffers.

32
00:02:26.180 --> 00:02:30.730
So I want to read it right now because it takes quite a long time to process this to to create that

33
00:02:30.730 --> 00:02:31.400
took an idea.

34
00:02:31.790 --> 00:02:37.010
But after that we will be able to transform each of our sentences each of our strings into a list of

35
00:02:37.010 --> 00:02:37.430
numbers.

36
00:02:37.430 --> 00:02:46.660
So let's say that data inputs will be equal to the list of all the noise that encode.

37
00:02:46.670 --> 00:02:55.880
So this is how we call the ENCODE methods of Arctic and ice applied to a sentence for each sentence

38
00:02:56.780 --> 00:02:59.510
being in data clean.

39
00:02:59.660 --> 00:03:04.430
So before in data clean we had a list of strings interesting being a twits.

40
00:03:04.550 --> 00:03:11.030
And now in data inputs we have a list of sequences each sequence being a list of numbers and corresponding

41
00:03:11.030 --> 00:03:11.870
to tweets.

42
00:03:12.650 --> 00:03:17.900
So now that we have all our sentences that our list of numbers so they are almost ready to be given

43
00:03:17.900 --> 00:03:19.910
to AI for the training.

44
00:03:19.910 --> 00:03:24.920
What we first need to do is the padding because doing the training in artificial intelligence most of

45
00:03:24.920 --> 00:03:27.710
the time leaves batches so we don't do the training.

46
00:03:27.710 --> 00:03:33.000
One example at the time we feed several examples at once to our network which is called batch.

47
00:03:33.260 --> 00:03:36.800
But in order to do that we need all our examples to have the same length.

48
00:03:36.860 --> 00:03:41.440
So we will simply add zeros at the end of each sentence so that they all have the same length.

49
00:03:41.450 --> 00:03:45.860
And the important thing is that zero is not a number that is used by auto organizer.

50
00:03:45.860 --> 00:03:46.940
It doesn't have any meaning.

51
00:03:46.940 --> 00:03:48.430
It doesn't correspond to any words.

52
00:03:48.440 --> 00:03:52.520
So we can safely use it and it won't alter the meaning of sentences.

53
00:03:53.000 --> 00:03:57.760
So first thing to do is to declare the maximum length of r sentences.

54
00:03:58.580 --> 00:04:07.430
So let's do that by creating the global valuable max length we'll be able to max of all the length of

55
00:04:07.430 --> 00:04:08.840
our sentences.

56
00:04:09.140 --> 00:04:12.320
So within sentence for sentence

57
00:04:15.240 --> 00:04:17.340
in data inputs.

58
00:04:17.340 --> 00:04:22.560
So this time each element of data inputs it's a list of numbers each number corresponding to a word.

59
00:04:22.590 --> 00:04:28.410
So that means that the length of sentence is actually the number of words in sequence whereas before

60
00:04:28.530 --> 00:04:33.150
if we took an element of data clean the length of this element would be the number of characters in

61
00:04:33.240 --> 00:04:34.140
a sentence.

62
00:04:34.140 --> 00:04:36.230
So here we have what we were looking for.

63
00:04:36.480 --> 00:04:46.800
And we can use these tools so data inputs equals T F that's clear us that's for processing that sequence

64
00:04:47.390 --> 00:04:49.930
that by the sequences.

65
00:04:50.040 --> 00:04:54.350
And what we have to give it is of course the list of sentences that we want to pass.

66
00:04:54.350 --> 00:05:03.150
So data inputs we also want to give it the value that we use for the budding so value equals zero then

67
00:05:03.150 --> 00:05:07.120
the way we wanted to but so we wanted to add the zeros at the end of sentences.

68
00:05:07.170 --> 00:05:10.110
So we say that padding equals busts.

69
00:05:10.320 --> 00:05:18.570
And finally of course the maximum length of all sentences after the padding so Max Len equals max length.

70
00:05:18.570 --> 00:05:18.930
That's it.

71
00:05:18.960 --> 00:05:21.800
After this we will have all our sentences that would have the same length.

72
00:05:21.810 --> 00:05:25.060
So then we will give it to the training method for I.

73
00:05:25.200 --> 00:05:29.040
It will be able to apply batches and to do it more efficiently.

74
00:05:29.040 --> 00:05:34.380
And finally last thing that we want to do is to speed our data into a training sets and the testing

75
00:05:34.380 --> 00:05:40.530
sets so that we can evaluate the performance of all I just after the training and actually all that

76
00:05:40.530 --> 00:05:45.690
the set has something special which is that we have one million six hundred thousand elements in it

77
00:05:46.110 --> 00:05:51.630
and actually in the order of our data are right now the first half of it have a negative sentiments.

78
00:05:51.630 --> 00:05:54.870
And the second half of it has a positive sentiments.

79
00:05:54.870 --> 00:05:56.820
So they are actually sorted.

80
00:05:56.820 --> 00:06:03.690
So a simple way to let's say get 1000 elements from the positive sets and 1000 elements from the negative

81
00:06:03.690 --> 00:06:11.310
sets in order to create 0 testing sets will be to just pick thousands random integers between 0 and

82
00:06:11.490 --> 00:06:13.260
800000 minus 1.

83
00:06:13.260 --> 00:06:27.180
So we will do that tests indices equals and beat that random that round int from 0 to 800 thousands

84
00:06:27.720 --> 00:06:28.290
minus one.

85
00:06:28.290 --> 00:06:35.500
So we can't have a hundred thousands with these methods and we will have eight thousands of them.

86
00:06:35.550 --> 00:06:41.460
So this gives us the indices that we will use among the first half of our data to create all testing

87
00:06:41.460 --> 00:06:42.060
sets.

88
00:06:42.090 --> 00:06:47.800
So to have the negative tweets and in order to have all all indices.

89
00:06:48.000 --> 00:06:56.150
So for the negative and the positive tweets we will simply concatenate so NPD that can cut tonight's

90
00:06:57.010 --> 00:06:58.760
the indices that we already have.

91
00:06:58.770 --> 00:07:13.790
So far the negative tweets and the same indices of course its tests and tests indices shifted by eight

92
00:07:13.810 --> 00:07:16.740
hundred thousands eight hundred thousand.

93
00:07:16.750 --> 00:07:17.710
Perfect.

94
00:07:17.730 --> 00:07:23.220
So with that we have the indices that we want to use in order to create our testing sets.

95
00:07:23.250 --> 00:07:32.480
So let's do that right now and say that tests inputs would be equal to data inputs in those indices.

96
00:07:32.480 --> 00:07:38.860
So the tests in this is and of course do the same OK.

97
00:07:38.910 --> 00:07:42.900
I keep writing text instead of tests but you've got me.

98
00:07:42.900 --> 00:07:53.220
And of course this is the same thing for test tests labels are all equal to data labels.

99
00:07:53.220 --> 00:07:57.810
In tests indices and now we have our testing sets.

100
00:07:57.810 --> 00:08:03.810
So of course we want to get rid of those data from the training sets and in order to do that we will

101
00:08:03.810 --> 00:08:15.600
use the delete method from the nearby module so train inputs equals and b that dilate and it will delete

102
00:08:15.660 --> 00:08:25.980
from data inputs the indices tests indices and us of course data inputs is a matrix and not just a vector

103
00:08:26.310 --> 00:08:28.330
because it's a list of lists.

104
00:08:28.350 --> 00:08:31.650
It's a list of sequences each sequence being a list of numbers.

105
00:08:31.650 --> 00:08:37.540
The list of what we need to specify and on which axes we want to apply this methods.

106
00:08:37.620 --> 00:08:43.020
And of course this is the first one because this is the first axis that corresponds to the index of

107
00:08:43.020 --> 00:08:44.800
the sentences in all copies.

108
00:08:44.820 --> 00:08:55.770
And now let's finally create or train labels so train labels equals and that deletes data labels and

109
00:08:55.770 --> 00:09:03.030
we deletes those indices and as labels are just a vector we don't need to specify the axes for the delete

110
00:09:03.050 --> 00:09:03.450
method.

111
00:09:03.660 --> 00:09:04.260
So that's it.

112
00:09:04.260 --> 00:09:09.480
Now we finally have our training and testing sets that are ready to be used to train 0 II.

113
00:09:09.570 --> 00:09:14.660
So of course all you have to do is to build AI and that's exactly the topic of the next part so see

114
00:09:14.680 --> 00:09:14.910
you soon.
