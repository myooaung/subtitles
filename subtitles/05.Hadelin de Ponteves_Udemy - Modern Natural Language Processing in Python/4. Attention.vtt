WEBVTT
1
00:00:00.180 --> 00:00:02.290
Hi and welcome back to this energy course.

2
00:00:02.360 --> 00:00:06.570
So we have just seen before briefly how the transformer generally works.

3
00:00:06.690 --> 00:00:11.160
And we will now dive into the details of it and we will start with the most important parts.

4
00:00:11.250 --> 00:00:12.870
The attention mechanism.

5
00:00:13.230 --> 00:00:15.840
So what is the general idea behind that.

6
00:00:15.870 --> 00:00:24.210
So we have two sequences or two sentences that can be equal in the case of the self attention be will

7
00:00:24.210 --> 00:00:29.310
be the contexts and A will be the sequence that we want to really work with.

8
00:00:29.310 --> 00:00:34.080
This is the sequence that conveys the information that we wanted to rearrange in a certain way.

9
00:00:35.250 --> 00:00:41.130
So there can be difference in certain cases like for instance if we are working with the translator

10
00:00:41.370 --> 00:00:48.750
we could have one sentence that is in English and the other one in French and we want to apply the attention

11
00:00:48.750 --> 00:00:54.720
to those two sentences together or they can be the same for instance in the encoding but we have only

12
00:00:54.720 --> 00:01:00.270
the input sentence and we just want to apply this this attention mechanism to a single sentence.

13
00:01:00.270 --> 00:01:02.820
And in this case A and B will be equal.

14
00:01:03.660 --> 00:01:10.190
So what we do with those two sentences is that we first wants to see how each element from a is related

15
00:01:10.190 --> 00:01:16.920
to elements from B essay elements and not words because at the very beginning of course we have words

16
00:01:16.950 --> 00:01:22.950
because the inputs is a sentence but later when we read and whinge all mattresses and we combine words

17
00:01:22.980 --> 00:01:29.790
and we do some mathematical operations we still have sequences of same length but this time it's not

18
00:01:29.790 --> 00:01:35.520
really correct to say that they are words the allergists elements mathematical elements that convey

19
00:01:35.760 --> 00:01:37.380
information in a more efficient way.

20
00:01:37.380 --> 00:01:40.880
So that's why I say elements and not words it's two sequences.

21
00:01:40.890 --> 00:01:43.340
And at the very beginning its sentences.

22
00:01:43.350 --> 00:01:45.000
So its words.

23
00:01:45.120 --> 00:01:51.880
So we just wanted to see how each element from a is related to the elements from B.

24
00:01:52.020 --> 00:01:57.770
And then we wanted to we combines all the elements from a according to those relations.

25
00:01:57.780 --> 00:02:05.250
So we want to rearrange the information in a according to how each element of A is related to the elements

26
00:02:05.250 --> 00:02:05.950
from B.

27
00:02:05.970 --> 00:02:14.550
So if we do a before after this attention mechanism we have a beginning a sequence a and in context

28
00:02:14.580 --> 00:02:19.270
B that tells us how we manipulate a and output.

29
00:02:19.320 --> 00:02:26.940
We have a new sequence where each element I will be a mix of elements from a that's well related to

30
00:02:26.940 --> 00:02:28.460
the elements B I.

31
00:02:29.220 --> 00:02:36.060
So generally speaking B tells us how we we combined elements from a or in the case of the self attention

32
00:02:36.510 --> 00:02:39.390
a tells us how we combine a.

33
00:02:39.390 --> 00:02:43.290
So we can see that there are two important parts to two steps.

34
00:02:43.290 --> 00:02:48.260
The first one being to compute the relations between elements from a and b.

35
00:02:48.360 --> 00:02:51.660
And the second one is to use that in order to recombine.

36
00:02:51.840 --> 00:02:58.440
So let's first talk about the first step getting the relations between the elements we know that's are

37
00:02:58.470 --> 00:03:04.320
its words and later each element will be a vector because we will apply an embedding layer at the very

38
00:03:04.320 --> 00:03:05.650
beginning of the model.

39
00:03:05.700 --> 00:03:12.150
So each word is represented by a vector and a good way in maths to compute the similarity between two

40
00:03:12.150 --> 00:03:14.220
vectors is to use that predicts.

41
00:03:14.250 --> 00:03:21.780
So a dot product is a mathematical operations that take as inputs to vectors and gives us a number we

42
00:03:21.780 --> 00:03:24.350
will say that it's between minus 1 and plus one.

43
00:03:24.570 --> 00:03:29.640
Actually it's between minus the product of the norms and plus the product of the norms.

44
00:03:29.910 --> 00:03:35.350
But in order to simplify this we will say that the norms of actors are ones so the outputs of the dot

45
00:03:35.350 --> 00:03:38.750
products will be written minus 1 and plus 1.

46
00:03:39.090 --> 00:03:46.260
And the idea is that two vectors will be similar if their dot product equals to 1 they will have no

47
00:03:46.260 --> 00:03:52.120
correlation if the dot product equals to 0 and they will be collated in an opposite manner.

48
00:03:52.260 --> 00:03:59.220
If the DOT predicts equals minus 1 and actually the way of computing those similarities focuses on the

49
00:03:59.220 --> 00:04:04.760
directions of the vectors have a small animation for you to see how it works.

50
00:04:04.800 --> 00:04:15.190
So we can see that the two vectors u and v and the direction of you is changing as time as time passes

51
00:04:15.640 --> 00:04:21.440
and you can see here how the dot products changes with the direction of Hue changing.

52
00:04:21.490 --> 00:04:26.960
So when you for instance here is starting to be perpendicular with the.

53
00:04:27.400 --> 00:04:33.040
This is what we call non correlation you and V are not completed and you can see that the foot equals

54
00:04:33.070 --> 00:04:35.420
equal to one 2 0.

55
00:04:35.420 --> 00:04:36.220
Sorry.

56
00:04:36.320 --> 00:04:39.500
Now you goes to the opposite direction.

57
00:04:39.740 --> 00:04:43.810
And this is where the dot product equals minus one.

58
00:04:43.940 --> 00:04:49.480
This just means that the Coalition is negative.

59
00:04:49.680 --> 00:04:54.270
Here again we go to and then correlating and non coalition phase.

60
00:04:54.270 --> 00:05:00.430
And now if you and we have a certain direction we say that they are highly correlated.

61
00:05:00.450 --> 00:05:07.800
So there is a huge similar similarity between them and that's why this was equal to plus 1 so the dot

62
00:05:07.810 --> 00:05:13.630
product is a very useful tool in order to capture the similarity between two words and that actually

63
00:05:13.630 --> 00:05:20.590
perfectly fits the AARP context because when we do embedding we actually make sure that two words that

64
00:05:20.590 --> 00:05:26.830
have similar meanings very close in all and meaning space which means that they have similar direction.

65
00:05:27.610 --> 00:05:34.690
So a quick and very simple example of that this could be a representation of the imaging of three words

66
00:05:34.930 --> 00:05:38.500
that would be joy despair and treat for instance.

67
00:05:38.500 --> 00:05:44.260
And we can see that joy and despair they kind of have opposite meaning and that's why the vectors are

68
00:05:44.290 --> 00:05:45.490
in the opposite direction.

69
00:05:45.490 --> 00:05:48.610
So the dot products between joy and despair would be minus one.

70
00:05:48.700 --> 00:05:54.600
And if I pick another random words like tree it is not really related to joy or despair.

71
00:05:54.730 --> 00:05:59.530
So they are perpendicular with those two vectors and the DOT predicts will be equal to zero.

72
00:05:59.930 --> 00:06:06.360
But it's true that we could argue that tree might be closer to joy than despair but whatever.

73
00:06:06.810 --> 00:06:07.890
And every hard.

74
00:06:07.890 --> 00:06:13.990
In other words like happiness for instance it will be very close to joy so depression will be very close

75
00:06:13.990 --> 00:06:16.060
to the direction of the vector Joy.

76
00:06:16.120 --> 00:06:20.980
So the DOT predicts between happiness and joy would be really close to one.

77
00:06:20.980 --> 00:06:26.910
So finally the dot product is a very powerful tool in order to compute the similarity between two words.

78
00:06:26.920 --> 00:06:33.680
So all we have to do now is to apply it to each page of words for all two sequences A and B.

79
00:06:33.920 --> 00:06:39.320
There is a very simple way to do that and although to understand it we will have a look at how dot for

80
00:06:39.330 --> 00:06:41.600
that and matrix multiplication work.

81
00:06:41.960 --> 00:06:45.340
So first the dot products we can see here that we have two vectors.

82
00:06:45.350 --> 00:06:52.850
The first one a B and the second one x y the first vector is horizontal and the other one is vertical.

83
00:06:52.850 --> 00:06:58.940
This is how we write our vectors when we want to apply dot products and what it does is that it takes

84
00:06:58.940 --> 00:07:03.350
the first element from the first vector and the first element from the second vector.

85
00:07:03.350 --> 00:07:07.970
We multiply them together and we do the same thing for the second elements.

86
00:07:08.180 --> 00:07:11.630
We multiply then also and we sum the total.

87
00:07:11.840 --> 00:07:14.330
That's basically what that product does.

88
00:07:14.330 --> 00:07:20.900
And what is interesting is that if instead of having a simple vector here we have a whole matrix.

89
00:07:20.900 --> 00:07:29.560
The result of this matrix multiplied by the vector x y is on the first elements a x plus b y.

90
00:07:29.600 --> 00:07:36.140
And the second is c x plus G Y and we can actually see that it really corresponds to the dot product

91
00:07:36.140 --> 00:07:42.940
between A B and x y here and at the products between C D and X Y.

92
00:07:43.370 --> 00:07:49.030
So multiplying a matrix by a vector is just to do multiple drug products at a time.

93
00:07:49.310 --> 00:07:54.260
So each row of the matrix will be dot predicted with the second vector.

94
00:07:54.620 --> 00:07:59.080
And the term that we could say that predicted but we will for the sake of this course.

95
00:07:59.210 --> 00:08:06.440
And if we go even further because we place the second vector by a matrix also so we will get all matrix

96
00:08:06.500 --> 00:08:16.950
ABC D and W X Y Z and again knowing that the first element is a W plus b y and so on we just can that

97
00:08:16.950 --> 00:08:19.440
is that's actually H elements.

98
00:08:19.450 --> 00:08:20.600
Is it the products.

99
00:08:20.600 --> 00:08:26.620
And so multiplying two mattresses together is actually the same thing as computing all the products

100
00:08:26.630 --> 00:08:32.480
with each row of the first Matrix with each column of the second matrix.

101
00:08:32.510 --> 00:08:40.850
So if we want to compute many dot products at a time we just have to multiply the first Matrix which

102
00:08:40.850 --> 00:08:44.150
is made of all the first vectors of the Bay of vectors.

103
00:08:44.150 --> 00:08:50.270
We wanted to apply the dot product too and we just make a second matrix made with the second vectors

104
00:08:50.570 --> 00:08:54.300
of the there are vectors that we wanted to apply the dot product to.

105
00:08:54.470 --> 00:09:00.170
The important thing to keep in mind is that we want for the first Matrix to have all vectors horizontally

106
00:09:00.590 --> 00:09:05.750
and for the second matrix we want all vectors to be vertical but we will come to that later.

107
00:09:06.350 --> 00:09:12.620
So we know that we wanted to apply that predicts to each pair of words or each pair of elements from

108
00:09:12.620 --> 00:09:19.040
all sequences in order to compute the similarity and we know that we can do that with the whole sequences

109
00:09:19.190 --> 00:09:25.760
all the time just by using matrix multiplication so we can go back to the formula in the paper and this

110
00:09:25.760 --> 00:09:31.280
figure that explains the whole process of the scale that predicts and have a closer look at it.

111
00:09:31.310 --> 00:09:36.170
So we can see here that we have three sequences as inputs.

112
00:09:36.170 --> 00:09:39.850
That's because it's a very general way to consider the scales drug products.

113
00:09:39.860 --> 00:09:43.020
But in our case we have only two sequences.

114
00:09:43.040 --> 00:09:51.590
As I said before sequence a sequence B and that's actually because K and V so the T's and the values

115
00:09:51.740 --> 00:09:58.010
will always be the same in r in our model in the transformer and Q Could be difference.

116
00:09:58.070 --> 00:10:06.740
So Q will be what I call the context is the sequence B and Q and V are the sequences that convey the

117
00:10:06.740 --> 00:10:10.100
information that we want to we arrange that we wanted to work with.

118
00:10:10.100 --> 00:10:14.360
So K envy are the sentence A that I talked about earlier.

119
00:10:15.200 --> 00:10:22.310
So the first step that we can see here is that we multiply Q which is all context sentence with the

120
00:10:22.370 --> 00:10:28.420
sequence K and after the embedding each sequence being a matrix we get all the products that we wanted

121
00:10:28.430 --> 00:10:35.870
and then we talked earlier and this t here just means transpose and it means that we simply flip the

122
00:10:35.960 --> 00:10:40.570
matrix in order to transform any horizontal vector into vertical vector.

123
00:10:40.820 --> 00:10:47.170
And of course any vertical vector into horizontal vector so rows become columns and columns become rows.

124
00:10:47.270 --> 00:10:53.780
And with that we make sure that the matrix multiplication actually gives us all the dot products between

125
00:10:53.900 --> 00:10:58.980
all the pairs of elements from the two sequences Q and K all be in a.

126
00:10:59.480 --> 00:11:07.550
So these two times Katie is a new metrics that conveys all the information about how Q is related to

127
00:11:07.550 --> 00:11:14.350
K or more precisely how each elements or words of Q is related to each elements of k.

128
00:11:15.080 --> 00:11:22.040
Now the next step we can he is these small scale it doesn't really have intuitively a meaning each tests

129
00:11:22.160 --> 00:11:28.280
something we do to improve the model and it stabilizes the whole process but there is nothing really

130
00:11:28.280 --> 00:11:29.930
to understand behind.

131
00:11:30.020 --> 00:11:35.790
So we just divide by decay which is the dimension of each element of each Woods from the keys.

132
00:11:35.960 --> 00:11:39.640
Then we can see that we apply a soft match so very quickly.

133
00:11:39.740 --> 00:11:45.940
Soft Max is an aberration that takes as inputs a vector and as outputs we get a vector of the same dimension

134
00:11:45.970 --> 00:11:49.220
that each element will be between 0 and 1.

135
00:11:49.430 --> 00:11:51.640
That's the first thing that is important.

136
00:11:51.650 --> 00:11:56.470
The second thing is that we keep the relations between each element of the initial vector.

137
00:11:56.480 --> 00:12:02.870
So if elements 2 was lower that element 3 H will be the same after the soft Max.

138
00:12:02.870 --> 00:12:07.460
So we just keep half each element is greater or lower than the others.

139
00:12:07.480 --> 00:12:12.920
That's the second point about soft Max and the third one is that the sum of all elements from the output

140
00:12:12.920 --> 00:12:15.180
of the S Max should be equal to 1.

141
00:12:15.650 --> 00:12:21.070
And the reason for that is that it perfectly suites what we call weights in mathematics.

142
00:12:21.080 --> 00:12:27.140
So we have just a sequence of numbers between 0 and 1 and their sum equals 1.

143
00:12:27.260 --> 00:12:33.530
And having those weights is really useful because it allows us to weaken those elements from vectors

144
00:12:33.620 --> 00:12:41.390
or to apply that products or to some elements from matrices of vectors or whatever in a very meaningful

145
00:12:41.390 --> 00:12:49.320
way because it just tells us how much of each element we want to take when we make a sum or when we

146
00:12:49.320 --> 00:12:54.900
re compose information and we are sure that there is nothing that that gets out of hands just because

147
00:12:54.900 --> 00:12:56.720
the sum of everything equals one.

148
00:12:56.730 --> 00:13:01.470
So there is also sort of a stability in the patients we will apply later.

149
00:13:01.740 --> 00:13:07.050
So this stuff Max is necessary in order in order to get valid weights.

150
00:13:07.050 --> 00:13:13.650
So now we have a perfect and valid representation of how each element from two was related to each element

151
00:13:13.650 --> 00:13:20.880
from K so how each element from our sentence a is related to each element from the context sentence

152
00:13:20.940 --> 00:13:21.440
B.

153
00:13:21.540 --> 00:13:28.410
And finally we will use those weights to read compose elements from our sentence a which here is called

154
00:13:28.470 --> 00:13:35.130
the to get in you rearranged version of the information in a in the values let's have a look at the

155
00:13:35.130 --> 00:13:39.600
first visual representation in order to make it even more simple.

156
00:13:39.780 --> 00:13:44.410
And this will be what's two times Katie looks like.

157
00:13:44.560 --> 00:13:52.080
So I said that with this matrix multiplication we get a new metrics that tells us how each element from

158
00:13:52.080 --> 00:13:54.310
two is related to each element from k.

159
00:13:54.780 --> 00:14:00.900
And here we can see an example of those mattresses we have key words of our sentence.

160
00:14:00.900 --> 00:14:03.090
Q It's a French sentence.

161
00:14:03.210 --> 00:14:07.480
And here we have each word of the sentence K which is an English sentence.

162
00:14:07.500 --> 00:14:11.450
So of course this is the attention mechanism in a translator.

163
00:14:11.730 --> 00:14:19.170
And in this metrics each square represents the value the score of the similarity between two words and

164
00:14:19.320 --> 00:14:19.640
the Y.

165
00:14:19.650 --> 00:14:25.000
So it is the more important the similarity is between those two words.

166
00:14:25.110 --> 00:14:31.710
And he of course as it is a translator and as the French language is very close to the English language

167
00:14:31.710 --> 00:14:34.660
in terms of how a sentence is constructed.

168
00:14:34.770 --> 00:14:40.950
We have a straight line here because we know that first words will most likely be the translation of

169
00:14:41.190 --> 00:14:43.420
the first words for the other sentence too.

170
00:14:43.500 --> 00:14:50.130
And that's the same for the words at the end of the sentence and for basically every local part of the

171
00:14:50.130 --> 00:14:51.160
sentences.

172
00:14:51.450 --> 00:14:53.340
But we can see some interesting behaviors.

173
00:14:53.340 --> 00:15:00.360
For instance he we can see a brewing some sort of a square where we have some similarities that are

174
00:15:00.660 --> 00:15:07.710
going on and that's normal because here we have in English it should be noted and in French we have

175
00:15:08.140 --> 00:15:13.930
equally on the note taker which is the exact translation but it's not a word by words translation.

176
00:15:13.950 --> 00:15:19.560
It's just that the global meaning of it should be noted that's corresponds to the global meaning of

177
00:15:19.830 --> 00:15:20.830
equal view not the note.

178
00:15:21.210 --> 00:15:26.940
And as it's not the word by words literal translation We can see that there is more than one square

179
00:15:27.030 --> 00:15:28.040
that is activated.

180
00:15:28.050 --> 00:15:36.060
So words are related to many other words in order to get the general meaning of this group of words.

181
00:15:36.120 --> 00:15:41.410
So that's for the representation of what these Q times K means.

182
00:15:41.420 --> 00:15:46.910
So now let's talk about the second main part and the second step in all attention mechanism which was

183
00:15:46.910 --> 00:15:55.940
to recombine the the the sequence values which we called a before and is equal to k as V and K are the

184
00:15:55.940 --> 00:15:58.590
same ones during all process in the transformer.

185
00:15:59.060 --> 00:16:06.200
So now that we get ha Q and K and so Q and V are related let's re compose v according to that in order

186
00:16:06.200 --> 00:16:13.310
to see what happens adjusts showed he a visual representation of how matrix multiplication works and

187
00:16:13.340 --> 00:16:14.320
he on the left.

188
00:16:14.600 --> 00:16:17.740
This matrix is the output of these operations.

189
00:16:17.750 --> 00:16:22.160
So it's exactly the same kind of matrix that we had before he.

190
00:16:22.250 --> 00:16:28.570
It's just the numerator similarity between the sequences and this matrix represents the sequence V.

191
00:16:28.880 --> 00:16:34.490
Also right the dimension of the matrix so here the dimension on the left is the length of the sequence

192
00:16:34.490 --> 00:16:43.040
Q hit the length of the sequence K because we made this operation q TIMES the transpose of K so this

193
00:16:43.040 --> 00:16:50.630
is dimension of our matrix and hey this matrix is the sequence V so this dimension is the length of

194
00:16:50.630 --> 00:16:58.160
the sequence V and this dimension is dimension of our model of course K and V being equal K length will

195
00:16:58.160 --> 00:16:59.590
be equal to Valence.

196
00:16:59.630 --> 00:17:05.870
So the matrix multiplication will be valid so let's say we wanted to know what we'll put in the second

197
00:17:05.870 --> 00:17:07.400
row of our results.

198
00:17:07.610 --> 00:17:14.450
This is done by Rico imposing elements from our sequence v according to this rule.

199
00:17:14.500 --> 00:17:21.770
Father second row of our results we will only use the second row of or attention metrics and as each

200
00:17:21.770 --> 00:17:29.720
soft Max has been applied real wise we actually know that this row will be a valid set of weights so

201
00:17:29.720 --> 00:17:35.840
let's say for instance that the second word of Q was really related to the first and the third words

202
00:17:35.870 --> 00:17:44.870
of K which is also V then it means that the coefficients a 2 1 and a 2 3 will be very very high.

203
00:17:45.140 --> 00:17:49.820
They will actually be close to 1 and the others will be close to zero.

204
00:17:49.820 --> 00:17:57.890
So when we do each vector multiplication here in this this row we know that we will use a lot of the

205
00:17:57.890 --> 00:18:00.750
first and the third row of the.

206
00:18:01.010 --> 00:18:05.890
And we will not use the others because this coefficient to 2 will be close to zero.

207
00:18:05.900 --> 00:18:09.980
So we don't use this one but the coefficient to 1 is close to 1.

208
00:18:10.370 --> 00:18:18.890
So we will use a lot of b 1 2 and third coefficient a 2 3 will be close to 1 so we will use a lots of

209
00:18:19.250 --> 00:18:20.000
B 3 2.

210
00:18:20.570 --> 00:18:30.230
So finally c 2 2 will be made of a lot of b 1 2 and b 3 2 and that will be the same for each other elements

211
00:18:30.320 --> 00:18:31.280
of this row.

212
00:18:31.490 --> 00:18:40.430
So that means that this row the second row of the outputs is made of lots of the first row of V and

213
00:18:40.430 --> 00:18:46.940
the third row of V which makes sense because because we said earlier that's our second element from

214
00:18:46.940 --> 00:18:52.130
Q The second word was related to the firsts was a V.

215
00:18:52.280 --> 00:19:00.780
And the third would of the so if you summarize we have a sequence Q and A SEQUENCE V.

216
00:19:00.780 --> 00:19:07.830
We will say V and it's the same as Q And the second word of Q is related to the first and the third

217
00:19:07.830 --> 00:19:09.470
world of V.

218
00:19:09.510 --> 00:19:17.130
So when we compute the attention mechanism at the end of its instead of Q we get a new sentence and

219
00:19:17.160 --> 00:19:23.730
in the second elements we have a mix of the first and the third elements are words from V.

220
00:19:23.730 --> 00:19:33.150
So this new sequence of sentence is a mix of elements from V according to how they were related.

221
00:19:33.150 --> 00:19:41.760
Q So Q is really a guide to how we want to we compose and rearrange information from the that was for

222
00:19:41.760 --> 00:19:44.100
how the attention mechanism works.

223
00:19:44.130 --> 00:19:50.880
And now we'll see how it is applied in the transform it so we can see that it appears three times in

224
00:19:50.910 --> 00:19:58.140
the transformer architecture it appears in the encoding phase and it appears twice in the decoding phase

225
00:19:58.470 --> 00:20:05.040
in the encoding phase and the first time in the decoding phase we can see that the three inputs are

226
00:20:05.040 --> 00:20:06.120
the same.

227
00:20:06.120 --> 00:20:07.770
And this is what we call self attention.

228
00:20:07.770 --> 00:20:12.190
So in this case Q will be equal to k and V.

229
00:20:12.300 --> 00:20:18.660
So the initial sentence and the context sentence are the same which means that we will compose a sentence

230
00:20:18.690 --> 00:20:24.520
according to how each word of the sentence is related to the other words of this very sentence.

231
00:20:24.660 --> 00:20:30.330
And that's actually the main point in replacing our own ends with a more global way of encoding sentences

232
00:20:30.750 --> 00:20:36.600
instead of encoding a sentence getting one word at a time we computes information globally with the

233
00:20:36.600 --> 00:20:38.030
whole sentence at once.

234
00:20:38.100 --> 00:20:43.630
And we repeat this process several times in the paper to say that they do that eight times in order

235
00:20:43.650 --> 00:20:49.860
to make sure that we get the most out of this weakened position of the information and in the decoding

236
00:20:49.860 --> 00:20:55.590
phase we start with the same thing we we composed information from the sentence that we give as inputs

237
00:20:55.590 --> 00:20:56.910
for the decoder.

238
00:20:57.330 --> 00:21:03.420
So we apply itself attentional layer and then there is this tension layer right here that doesn't use

239
00:21:03.570 --> 00:21:06.430
the same sentence for the three inputs.

240
00:21:06.600 --> 00:21:07.980
And in this case.

241
00:21:07.980 --> 00:21:13.310
Q So the context sentence will be the outputs of these sub layer.

242
00:21:13.440 --> 00:21:20.430
So it will be the sentence coming from the decoder and two and V which are equal will be the outputs

243
00:21:20.430 --> 00:21:21.720
of the encoder.

244
00:21:22.080 --> 00:21:28.170
So if we take the example of a translator let's say a translator That's transform an English sentence

245
00:21:28.380 --> 00:21:35.940
into a French sentence here we will have an English sentence that we want to decompose with the contexts

246
00:21:36.150 --> 00:21:38.010
made of a French sentence.

247
00:21:38.120 --> 00:21:44.520
So before this layer we have an English sentence or an English sequence because we already applied some

248
00:21:44.970 --> 00:21:51.140
mechanisms and maths to the English sentence but so we have an English sequence here that we want to

249
00:21:51.140 --> 00:21:56.850
we compose according to a context that is made of the French sentence that we have made so far.

250
00:21:57.120 --> 00:21:59.550
So we are in the middle of a translation.

251
00:21:59.560 --> 00:22:05.730
He we have what we have constructed so far like half of a French sentence for instance we work with

252
00:22:05.730 --> 00:22:11.970
it's a little bits and we get a contexts so all encoded version of the English sentence.

253
00:22:12.000 --> 00:22:18.660
We'll see how it is related to the French sentence that we have made so far element wise and so we will

254
00:22:18.660 --> 00:22:24.240
get a new real French version of the English sentence according to the work that we have already done

255
00:22:24.360 --> 00:22:30.420
with the translation and that's how we get new information that we can use for the translation in order

256
00:22:30.420 --> 00:22:33.930
to predict the next words in all translated sentence.

257
00:22:33.990 --> 00:22:38.760
So the last element we need to see in the attrition mechanism is what they call in the paper.

258
00:22:38.790 --> 00:22:44.730
Luke the head mask and this is something that appears that he in these very attention sub layer.

259
00:22:45.270 --> 00:22:49.900
What it does is that's it for a visit the decoder to have access to future words.

260
00:22:49.920 --> 00:22:56.220
And what I mean by that is doing the processing of the sequence indeed coda when we deal with the words

261
00:22:56.280 --> 00:23:02.400
or the elements ie we don't want to have access to the word I and the following words to understand

262
00:23:02.400 --> 00:23:04.310
that Let's simulate a training phase.

263
00:23:04.530 --> 00:23:10.350
So during the training we will have a pair of texts or a pair of sentences we will take the example

264
00:23:10.350 --> 00:23:11.380
of a translator.

265
00:23:11.580 --> 00:23:16.290
So we will have an English sentence and the French sentence that is supposed to be the translation of

266
00:23:16.290 --> 00:23:17.640
the English sentence.

267
00:23:17.640 --> 00:23:23.760
So to train with this pair of sentences we will use the English sentence as inputs for the encoder and

268
00:23:23.760 --> 00:23:30.470
we will use also the French sentence as targets as outputs for all decoder but we will also use this

269
00:23:30.470 --> 00:23:36.240
sentence the French sentence to the output sentence as inputs for our decoder right here.

270
00:23:36.360 --> 00:23:38.420
But as it is written here shifted right.

271
00:23:38.430 --> 00:23:45.120
So what it means is that if you have a sentence of let's say 20 words what we will use here as inputs

272
00:23:45.240 --> 00:23:47.670
is a sequence of 21 words.

273
00:23:47.730 --> 00:23:50.790
The first one being just a token that we will call the starting token.

274
00:23:50.790 --> 00:23:53.880
So it just says that the sentence starts.

275
00:23:53.970 --> 00:24:01.290
And so what we want or transformed to do is to have as output he the exact same sentence as he but shifted

276
00:24:01.290 --> 00:24:03.380
by one position to the left.

277
00:24:03.600 --> 00:24:09.990
But of course in order to predict the words I wanted to transform to have only access to the first I

278
00:24:09.990 --> 00:24:13.080
minus one words from the French sentence.

279
00:24:13.080 --> 00:24:18.270
So what we will do is that during the self attention mechanism right Hey when we compose the sentence

280
00:24:18.360 --> 00:24:24.510
I position I for instance we will only use the first ie elements from the sentence which are actually

281
00:24:24.900 --> 00:24:31.170
the starting token and the first I minus one words and the best way to do that is to multiply all out

282
00:24:31.170 --> 00:24:33.750
and matrix that we had before.

283
00:24:33.750 --> 00:24:41.310
So with the few times occasions posts by a triangular metrics that we see which is made of once in the

284
00:24:41.310 --> 00:24:48.670
bottom left parts of the matrix and zeros in this tricked top writes parts of the matrix environments

285
00:24:48.720 --> 00:24:51.980
application I don't mean a standard matrix multiplication.

286
00:24:51.990 --> 00:24:58.740
This is an element y's multiplication so we will multiply the top left value of this matrix by 1 then

287
00:24:58.740 --> 00:25:04.510
the next value by 0 and so on so the top right value of this matrix by 0.

288
00:25:04.650 --> 00:25:06.110
And that's how it works.

289
00:25:06.120 --> 00:25:12.330
So by doing this we keep all the items and values from these spots of our attention matrix and we set

290
00:25:12.330 --> 00:25:16.300
to 0 0 those values and to see exactly what does.

291
00:25:16.530 --> 00:25:22.010
Let's have a look back at the visual representation of the matrix multiplication right here.

292
00:25:22.530 --> 00:25:26.620
So this is all two times K transposed matrix.

293
00:25:26.760 --> 00:25:30.210
And if we multiply by the look ahead masked what we saw before.

294
00:25:30.210 --> 00:25:35.490
It just means that those coefficients stay the same in the bottom left parts.

295
00:25:35.640 --> 00:25:38.320
But those coefficients now are set to zero.

296
00:25:38.700 --> 00:25:44.970
So when we do the decomposition at the end so when we multiply by V which is again the same sentence

297
00:25:45.000 --> 00:25:47.670
because we are in a self attention process.

298
00:25:47.670 --> 00:25:54.420
If we go to second row for example here we have only the first two coefficients that are not equal to

299
00:25:54.420 --> 00:25:57.140
zero and all the others are set to zero.

300
00:25:57.330 --> 00:26:03.000
So that means that when we do that we composition we will multiply all those elements by zero and we

301
00:26:03.000 --> 00:26:06.390
will only keep the first two elements of our initial sequence.

302
00:26:06.960 --> 00:26:13.950
So our second elements in our output sequence will only pay attention to the first two elements of our

303
00:26:13.950 --> 00:26:18.210
initial sequence which means just the starting token and the first words.

304
00:26:18.210 --> 00:26:23.760
So that's exactly what we wanted element number I after the set attention mechanism in the decoder.

305
00:26:23.760 --> 00:26:28.770
I've only access to the first I minus one woods and that's how we make sure that during the training

306
00:26:28.920 --> 00:26:30.300
when we try to predict what.

307
00:26:30.300 --> 00:26:30.950
No I.

308
00:26:31.260 --> 00:26:34.910
We only have access to the previous words of the predicted sentence.

309
00:26:34.980 --> 00:26:41.220
So that allows us to train with a day of sentence in only one step by simulating the facts.

310
00:26:41.220 --> 00:26:44.730
That's to predict the next words in all I sentence.

311
00:26:44.790 --> 00:26:45.480
We don't have yet.

312
00:26:45.480 --> 00:26:47.200
The rest of the sentence.

313
00:26:47.300 --> 00:26:49.880
So that was for this look ahead mask.

314
00:26:49.980 --> 00:26:57.300
And finally now we can see in the paper that there is this other figure that explained how the use is

315
00:26:57.450 --> 00:27:03.690
in the items in sub layer because they don't just apply the scaled dot products directly on the sentences

316
00:27:03.990 --> 00:27:09.210
what they do is that's the first complete cleaning our projections and that then do the concatenation.

317
00:27:09.210 --> 00:27:15.210
So instead of applying the scales that predicts attention to the whole sentence they speech each factor

318
00:27:15.540 --> 00:27:16.260
from the sentence.

319
00:27:16.260 --> 00:27:19.540
So each element from the sentence into safe spaces.

320
00:27:19.650 --> 00:27:25.980
So let's say each words after embedding has a dimension of twelve for instance instead of applying the

321
00:27:25.980 --> 00:27:33.020
scale the products to sequences made off elements of dimension twelve we will split it into let's say

322
00:27:33.020 --> 00:27:39.540
it for sequences each element being of dimension 3 and then we compute this kid attention with that

323
00:27:39.870 --> 00:27:45.380
and we can cut the result in order to get back to our initial dimension for the model.

324
00:27:45.750 --> 00:27:51.000
And the reason for that is well explained in the paper they say no tell attention allows the module

325
00:27:51.000 --> 00:27:56.280
to turn fully attention to information from different representations of spaces at different positions

326
00:27:56.550 --> 00:27:59.520
with a single attention had averaging inhibits this.

327
00:27:59.520 --> 00:28:04.720
So what it means is let's get back to our example with and being dimension of twelve.

328
00:28:05.160 --> 00:28:06.980
We could say that's among the.

329
00:28:06.990 --> 00:28:12.990
For instance three first dimensions of all embedding there is something that we wanted to get between

330
00:28:12.990 --> 00:28:17.980
the first and the third words but it only appears in those first three dimensions.

331
00:28:18.210 --> 00:28:24.150
Then we would really like to have all attention mechanism to be able to focus on those three dimensions

332
00:28:24.420 --> 00:28:30.270
so that the information is not faded into the 12 initial dimension of the embedding.

333
00:28:30.270 --> 00:28:36.870
So splitting the space into set of spaces allows the attention mechanism to attend to more information

334
00:28:36.900 --> 00:28:41.640
and to be able to get more relations between the elements of a sequence.

335
00:28:41.640 --> 00:28:46.920
And finally how we do that mathematically it's important to notice that we don't split the space into

336
00:28:46.920 --> 00:28:51.160
safe spaces and then apply many different linear transformations.

337
00:28:51.180 --> 00:28:57.300
What we do is that we first apply the huge linear transformations so a dense layer intense flow and

338
00:28:57.300 --> 00:29:00.170
then we split the space into safe spaces.

339
00:29:00.180 --> 00:29:06.330
The reason why we do that is that it allows the architecture the module to learn more because the transformer

340
00:29:06.360 --> 00:29:13.350
could say that's it's better for him to make let's say the first and the eight and the tenth dimensions

341
00:29:13.530 --> 00:29:14.370
work together.

342
00:29:14.460 --> 00:29:20.310
So during the first big linear transformation he will just rearrange all the dimensions of our embedding

343
00:29:20.520 --> 00:29:23.970
to put together the ones that work the best together actually.

344
00:29:24.030 --> 00:29:30.090
And then we will split into safe spaces because if we split first then it means that's the first second

345
00:29:30.120 --> 00:29:35.630
and third dimensions will always work together and have not access to the other dimensions for the scale

346
00:29:35.660 --> 00:29:36.270
dot products.

347
00:29:36.270 --> 00:29:37.770
And maybe that's not optimal.

348
00:29:37.860 --> 00:29:43.330
So that's why we first apply a big linear formation and then we do this plating and of course after

349
00:29:43.360 --> 00:29:49.900
updating the scared the attentions to all those spaces we can get everything back in order to get back

350
00:29:49.900 --> 00:29:53.050
to the initial dimension of our embedding.

351
00:29:53.080 --> 00:29:57.540
So that was a huge part understanding this attention mechanism how it works.

352
00:29:57.550 --> 00:30:03.910
This composition of the elements of different sequences don't hesitate to go back to each to have a

353
00:30:03.910 --> 00:30:09.850
look at the figures but that was it for the hardest part which was actually also the more interesting

354
00:30:09.850 --> 00:30:15.990
one because in my opinion it's really interesting how this says a lot about how language works for humans.

355
00:30:16.000 --> 00:30:21.760
My interpretation is that's the transformer is getting closer to how we as humans actually process our

356
00:30:21.760 --> 00:30:22.490
language.

357
00:30:22.510 --> 00:30:27.290
I feel like that we process sentences globally and that's what after the other.

358
00:30:27.310 --> 00:30:33.010
And the fact that the word comes after the other in a sentence when we talk when we read is just the

359
00:30:33.010 --> 00:30:39.580
constraints of how our language is made by the way all brain gets the information and process a sentence

360
00:30:39.730 --> 00:30:42.890
is closer to how the transformer deals with it.

361
00:30:42.890 --> 00:30:48.570
With the attention mechanism I feel like we do it more globally and then the language working words

362
00:30:48.590 --> 00:30:55.420
after words is just maybe a none optimal way to convey information of the meaning of what we wanted

363
00:30:55.420 --> 00:30:56.460
to say.

364
00:30:56.590 --> 00:31:01.780
Anyway that was maybe a bit esoteric but it's just that I find it really interesting how the sequence

365
00:31:01.780 --> 00:31:08.470
to sequence models evolved to get to this one that seems closer in my opinion to how we really processed

366
00:31:08.470 --> 00:31:09.850
language as humans.

367
00:31:09.850 --> 00:31:14.710
So of course we are not done with the explanation of the transformer because there are some other details

368
00:31:14.710 --> 00:31:18.950
in the implementation that we have to deal with but for the main part this is done.

369
00:31:19.180 --> 00:31:23.830
So we only have a few other details to see before starting to implement transformer.

370
00:31:24.050 --> 00:31:25.580
Hopefully excited and see you soon.
