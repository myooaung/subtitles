WEBVTT
1
00:00:00.740 --> 00:00:03.470
[Autogenerated] Now that you have understood the basic building block,

2
00:00:03.470 --> 00:00:09.440
often artificial neuron, Let's look what a neural network is.

3
00:00:09.440 --> 00:00:13.160
It's a computing system that is made up off a number off simple,

4
00:00:13.160 --> 00:00:15.470
highly interconnected neurons,

5
00:00:15.470 --> 00:00:19.950
producing a certain output in a multi layered neural network.

6
00:00:19.950 --> 00:00:22.240
You will have input layer.

7
00:00:22.240 --> 00:00:27.440
What are more hidden layers on an output layer?

8
00:00:27.440 --> 00:00:29.790
Let's look at different types of neural network

9
00:00:29.790 --> 00:00:32.540
under typical business application.

10
00:00:32.540 --> 00:00:34.870
First one is artificial Neural Network,

11
00:00:34.870 --> 00:00:40.440
which is typically used to address patent recognition problems.

12
00:00:40.440 --> 00:00:44.640
Convolution Neural Network is used in image processing.

13
00:00:44.640 --> 00:00:47.520
Recommend neural networks is used in speech.

14
00:00:47.520 --> 00:00:50.010
Recognition on Deep Neural.

15
00:00:50.010 --> 00:00:54.740
Network is used in acoustic modeling on deep belief.

16
00:00:54.740 --> 00:00:58.740
Network is used in cancer detection.

17
00:00:58.740 --> 00:01:03.640
Let's see how a training process works in artificial neural network.

18
00:01:03.640 --> 00:01:07.840
In addressing a typical classifications problem.

19
00:01:07.840 --> 00:01:12.130
Let's imagine a simple network with three layers that has one

20
00:01:12.130 --> 00:01:17.940
input layer one hidden layer on one output layer.

21
00:01:17.940 --> 00:01:22.390
You feed the label training data at the input layer, along with the weights.

22
00:01:22.390 --> 00:01:24.640
For each connections.

23
00:01:24.640 --> 00:01:29.940
An activation function is executed at the hidden layer to produce an output.

24
00:01:29.940 --> 00:01:34.240
This is also called forward propagation.

25
00:01:34.240 --> 00:01:35.820
This output could be right.

26
00:01:35.820 --> 00:01:39.310
Prediction are around one on this value is compared to the

27
00:01:39.310 --> 00:01:42.940
actual value on the error is computer.

28
00:01:42.940 --> 00:01:49.440
This error, also called the cost function, needs to be minimized.

29
00:01:49.440 --> 00:01:53.820
There are many optimization techniques, like a stochastic Grady indecent.

30
00:01:53.820 --> 00:01:59.540
To achieve this, this error is fed back to the input layer,

31
00:01:59.540 --> 00:02:02.840
and weights unbiased are reregistered.

32
00:02:02.840 --> 00:02:03.070
On.

33
00:02:03.070 --> 00:02:06.740
This process is called back propagation.

34
00:02:06.740 --> 00:02:12.540
This is an iterative training process to get an optimal training score.

35
00:02:12.540 --> 00:02:14.940
One is the training process is perfected.

36
00:02:14.940 --> 00:02:17.910
You can feed in the test data and check how the

37
00:02:17.910 --> 00:02:22.440
prediction works on unseen test data.

38
00:02:22.440 --> 00:02:24.870
Let's start looking at convolution.

39
00:02:24.870 --> 00:02:27.140
Neural networks.

40
00:02:27.140 --> 00:02:32.140
CNN's can be compared to brain's visual cortex function.

41
00:02:32.140 --> 00:02:36.140
CNN's are primarily used in image processing,

42
00:02:36.140 --> 00:02:37.440
wise recognition,

43
00:02:37.440 --> 00:02:43.490
a natural language processing in the case off a and then every neuron is

44
00:02:43.490 --> 00:02:47.640
interconnected with every other neuron in the adjacent layer.

45
00:02:47.640 --> 00:02:52.140
But in CNN, only a small portion of the input is connected to its app.

46
00:02:52.140 --> 00:02:54.240
JSLint layer.

47
00:02:54.240 --> 00:02:57.540
This is a feed forward network.

48
00:02:57.540 --> 00:03:03.040
The convolution operation forms the basis off CNN.

49
00:03:03.040 --> 00:03:06.270
There are multiple layers involved in the convolution

50
00:03:06.270 --> 00:03:08.280
URL network on Let's get a quick,

51
00:03:08.280 --> 00:03:16.540
high level overview off each such layer first one is convolution layer.

52
00:03:16.540 --> 00:03:23.540
Convolution is a linear operation where you multiply the weights with inputs.

53
00:03:23.540 --> 00:03:28.540
In a typical image, processing the input will be a two dimensional array.

54
00:03:28.540 --> 00:03:33.340
The multiplication is performed between a two dimensional input RGBA

55
00:03:33.340 --> 00:03:36.360
and relatively smaller two dimensional weights.

56
00:03:36.360 --> 00:03:42.350
Are two, also called us, filter this element wise multiplication.

57
00:03:42.350 --> 00:03:44.640
Between the filter on the input,

58
00:03:44.640 --> 00:03:51.640
the cells in a single scaler value as a filter is multiplied multiple times.

59
00:03:51.640 --> 00:03:56.040
You end up in a two dimensional array off output values

60
00:03:56.040 --> 00:04:00.040
that represents filtering off the input.

61
00:04:00.040 --> 00:04:05.040
This two dimensional output array is called a feature map.

62
00:04:05.040 --> 00:04:08.940
Next one is Ray Lou Layer.

63
00:04:08.940 --> 00:04:09.890
In some cases,

64
00:04:09.890 --> 00:04:14.310
an additional layer called Raila Layer would be added to

65
00:04:14.310 --> 00:04:18.640
introduce non linearity in the feature man.

66
00:04:18.640 --> 00:04:21.460
A limitation with this feature map is that they're highly

67
00:04:21.460 --> 00:04:24.740
dependent on the location off the features.

68
00:04:24.740 --> 00:04:28.380
A small change in the image in the farm off image distortion

69
00:04:28.380 --> 00:04:31.440
would result in a totally different feature map.

70
00:04:31.440 --> 00:04:34.650
Down sampling is a common approach that is used to

71
00:04:34.650 --> 00:04:37.340
address this in signal processing.

72
00:04:37.340 --> 00:04:43.440
Where you lower the resolution of the input signal to reduce over fitting.

73
00:04:43.440 --> 00:04:48.690
A more robust approach is to use a pulling layer this

74
00:04:48.690 --> 00:04:52.240
operates on the feature map much like a filter,

75
00:04:52.240 --> 00:04:56.810
and reduce the size of the future map even further to

76
00:04:56.810 --> 00:04:59.740
common functions used in the pooling.

77
00:04:59.740 --> 00:05:03.490
Our average pulling that calculates the average values

78
00:05:03.490 --> 00:05:06.440
for each patch on the future man.

79
00:05:06.440 --> 00:05:11.620
A maximum pulling that calculates a maximum values for each part in the future.

80
00:05:11.620 --> 00:05:16.940
Man next one is fully connected layer.

81
00:05:16.940 --> 00:05:20.630
The objective of the fully connected layer is to take the results off,

82
00:05:20.630 --> 00:05:25.930
pulling layer on flatten, flattening the process off,

83
00:05:25.930 --> 00:05:28.610
converting all the result in two dimensions.

84
00:05:28.610 --> 00:05:37.340
Arrays from pull future map into a single, long, continuous linear vector.

85
00:05:37.340 --> 00:05:40.730
The flattened output represents the probability that a

86
00:05:40.730 --> 00:05:47.040
certain feature belongs to the label, usually in a deep neural network,

87
00:05:47.040 --> 00:05:53.000
there will be multiple convolution, layer rail, you layer on pulling layers.

