1
00:00:02,240 --> 00:00:06,740
[Autogenerated] Welcome to this module on evaluate ML models,

2
00:00:06,740 --> 00:00:10,340
we will be covering some off the performance metrics that are used to

3
00:00:10,340 --> 00:00:15,040
evaluate both classifications on regression models.

4
00:00:15,040 --> 00:00:19,740
If you are a beginner, I encourage you to understand this concepts clearly.

5
00:00:19,740 --> 00:00:22,140
But if you're already familiar with these,

6
00:00:22,140 --> 00:00:27,220
next few minutes are going to be a quick refresher for you.

7
00:00:27,220 --> 00:00:31,440
Let's begin with the metrics for classifications problems.

8
00:00:31,440 --> 00:00:33,780
If you have bean in the world of machine learning,

9
00:00:33,780 --> 00:00:38,610
you may have heard about confusion Metrics.

10
00:00:38,610 --> 00:00:42,440
Though Confusion Metrics is not a metric by its own,

11
00:00:42,440 --> 00:00:47,360
it forms the basis off multiple other performance metrics that are used for

12
00:00:47,360 --> 00:00:52,540
evaluating binary on multi class classification models.

13
00:00:52,540 --> 00:00:55,050
Let's consider a business case where you want to

14
00:00:55,050 --> 00:00:59,440
predict if the email is spam or not.

15
00:00:59,440 --> 00:01:03,240
This is a typical binary classification problem where the

16
00:01:03,240 --> 00:01:07,840
output label is a simple yes or no.

17
00:01:07,840 --> 00:01:12,330
Let's try it two by two matrix with rows in the confusion matrix

18
00:01:12,330 --> 00:01:17,060
representing what the machine learning predicted on the columns

19
00:01:17,060 --> 00:01:20,940
representing the actual values.

20
00:01:20,940 --> 00:01:25,840
The value in the top left quadrant represents the actual spam emails

21
00:01:25,840 --> 00:01:28,740
that are correctly predicted by the algorithm.

22
00:01:28,740 --> 00:01:34,040
This in machine learning language is called us true positive.

23
00:01:34,040 --> 00:01:38,550
The bottom right QUARANTE represents the non spam email that

24
00:01:38,550 --> 00:01:41,140
are correctly predicted by the algorithm.

25
00:01:41,140 --> 00:01:44,140
This is called true negative.

26
00:01:44,140 --> 00:01:47,830
Bottom Left Quadrant represents the actual number of the spam

27
00:01:47,830 --> 00:01:51,340
emails that the algorithm didn't predict,

28
00:01:51,340 --> 00:01:54,040
and it's also called is falsy Negative.

29
00:01:54,040 --> 00:01:58,800
On the top right is where the algorithm predicts it has a spam email,

30
00:01:58,800 --> 00:02:01,040
but in reality it is not.

31
00:02:01,040 --> 00:02:05,840
This is also called us a false positive.

32
00:02:05,840 --> 00:02:09,940
Now let's consider a multi class classification problem

33
00:02:09,940 --> 00:02:13,630
where you like the algorithm to predict if a particular

34
00:02:13,630 --> 00:02:18,640
fruit is apple banana are orange.

35
00:02:18,640 --> 00:02:21,230
Since there are three possible outcomes,

36
00:02:21,230 --> 00:02:26,700
the confusion matrix will be a three by three matrix on along the same lines.

37
00:02:26,700 --> 00:02:30,440
For a classification problem with n possible outcome,

38
00:02:30,440 --> 00:02:35,540
the confusion matrix will be end by in metrics.

39
00:02:35,540 --> 00:02:38,730
Now that we learn about the confusion metrics,

40
00:02:38,730 --> 00:02:42,540
let's look at the performance metrics that can be derived from this.

41
00:02:42,540 --> 00:02:45,340
First one is accuracy.

42
00:02:45,340 --> 00:02:50,240
Accuracy is defined as you overall correct predictions performed by the model,

43
00:02:50,240 --> 00:02:54,840
and here is a formula used for computing accuracy.

44
00:02:54,840 --> 00:02:57,940
Accuracy is the answer to the question.

45
00:02:57,940 --> 00:03:01,140
What percentage of predictions were correct?

46
00:03:01,140 --> 00:03:07,640
Next one is precision are positive predictive value.

47
00:03:07,640 --> 00:03:12,040
The formula to compute position is the number off positive

48
00:03:12,040 --> 00:03:15,740
predictions out off all the total predictions.

49
00:03:15,740 --> 00:03:19,160
A model with a higher position means that it will

50
00:03:19,160 --> 00:03:21,780
identify a higher percentage off.

51
00:03:21,780 --> 00:03:26,940
Positive class are a higher percentage off truthy positive.

52
00:03:26,940 --> 00:03:30,840
You can think of position as the answer to the question.

53
00:03:30,840 --> 00:03:35,040
What percentage off __________ predictions were correct?

54
00:03:35,040 --> 00:03:38,040
The next one is recall.

55
00:03:38,040 --> 00:03:43,310
Recall is also known as sensitivity and is computed with the formula.

56
00:03:43,310 --> 00:03:46,040
As shown here.

57
00:03:46,040 --> 00:03:49,040
Recall is the answer to the question.

58
00:03:49,040 --> 00:03:54,440
What percentage off positive cases did the moral catch?

59
00:03:54,440 --> 00:03:58,940
Specificity is computer with a formula that has shown here,

60
00:03:58,940 --> 00:04:01,520
and it helps in answering the question.

61
00:04:01,520 --> 00:04:07,240
What percentage of negative cases are correctly predicted?

62
00:04:07,240 --> 00:04:13,740
Let's say your organization goal is to capture maximum number of spam emails,

63
00:04:13,740 --> 00:04:15,960
and that means we need to increase the positive

64
00:04:15,960 --> 00:04:18,540
cases that the model is sketching,

65
00:04:18,540 --> 00:04:22,540
which means we need to increase our recall score.

66
00:04:22,540 --> 00:04:22,740
Now.

67
00:04:22,740 --> 00:04:28,140
As we increase this, our position score might suffer.

68
00:04:28,140 --> 00:04:31,640
That is another metric called F one score,

69
00:04:31,640 --> 00:04:36,540
which is a harmonic distribution between precision and recall.

70
00:04:36,540 --> 00:04:42,670
And here is a formula to compute JSON score an important

71
00:04:42,670 --> 00:04:46,520
visualization chart in the field of classifications is a

72
00:04:46,520 --> 00:04:49,540
receiver operator characteristic curve.

73
00:04:49,540 --> 00:04:52,540
Also called us a rows seeker.

74
00:04:52,540 --> 00:04:57,140
It is a curve that plots the relation between true positive rate.

75
00:04:57,140 --> 00:04:59,940
This is a false positive rate.

76
00:04:59,940 --> 00:05:03,770
PRA positive rate is also known as the sensitivity are

77
00:05:03,770 --> 00:05:08,550
recall on the false positive rate is also known as the false

78
00:05:08,550 --> 00:05:13,870
alarms are one minus specificity.

79
00:05:13,870 --> 00:05:18,430
A rows SQL summarize is all off the confusion matrices

80
00:05:18,430 --> 00:05:22,640
possibilities that each threshold produced.

81
00:05:22,640 --> 00:05:26,940
It doesn't provide us with a numerical value to compare model,

82
00:05:26,940 --> 00:05:31,460
but the metric area under the curve, also known as a U C,

83
00:05:31,460 --> 00:05:33,740
certainly does.

84
00:05:33,740 --> 00:05:35,640
Referring to the chart below,

85
00:05:35,640 --> 00:05:40,850
you can see that choosing the threshold a provided us with better error.

86
00:05:40,850 --> 00:05:45,800
You see value than choosing the threshold to be higher.

87
00:05:45,800 --> 00:05:54,000
The year you see better than model is in distinguishing between the spam email on non spam email

