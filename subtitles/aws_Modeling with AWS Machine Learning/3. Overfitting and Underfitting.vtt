WEBVTT
1
00:00:01.040 --> 00:00:04.640
[Autogenerated] before discussing about war fitting and under fitting.

2
00:00:04.640 --> 00:00:09.340
Let's understand what is bias and variance.

3
00:00:09.340 --> 00:00:14.140
Let's revisit the housing data that we saw in the previous clip,

4
00:00:14.140 --> 00:00:17.210
a data 0.1, which is for the house.

5
00:00:17.210 --> 00:00:22.950
With the size 1000 square feet you can see the actual value is

6
00:00:22.950 --> 00:00:28.640
100 on the predicted value is 110 So the difference between

7
00:00:28.640 --> 00:00:32.140
actual and predicted is 10 units.

8
00:00:32.140 --> 00:00:35.540
This difference is called is bias.

9
00:00:35.540 --> 00:00:39.840
A low bias means that the moral is spiriting accurately.

10
00:00:39.840 --> 00:00:44.420
The high bias means low accuracy.

11
00:00:44.420 --> 00:00:47.240
Now let's imagine that we repeat the model building

12
00:00:47.240 --> 00:00:50.640
process with a different dataset.

13
00:00:50.640 --> 00:00:55.270
Variance is a measure off how much the predictions vary for

14
00:00:55.270 --> 00:00:58.340
a fixed point between different ones.

15
00:00:58.340 --> 00:00:59.110
For example,

16
00:00:59.110 --> 00:01:03.750
it's take a different sample of data that has house with 1000 square

17
00:01:03.750 --> 00:01:06.220
feet and check the differences between predicted.

18
00:01:06.220 --> 00:01:08.340
On actual,

19
00:01:08.340 --> 00:01:11.190
though there are other factors that contribute to pricing

20
00:01:11.190 --> 00:01:16.400
off the house for simplicity's sake, we're going to disregard those.

21
00:01:16.400 --> 00:01:19.700
The chart that you see is for a different sample,

22
00:01:19.700 --> 00:01:22.710
and in this sample you can see the difference between the

23
00:01:22.710 --> 00:01:26.910
estimated on the actual price is nine units.

24
00:01:26.910 --> 00:01:31.540
That means the model didn't change a whole lot between samples.

25
00:01:31.540 --> 00:01:36.140
This model would be considered as low variance.

26
00:01:36.140 --> 00:01:39.640
If your moral changes drastically between sample sets,

27
00:01:39.640 --> 00:01:43.740
it's considered a high variance model.

28
00:01:43.740 --> 00:01:48.640
You must have seen this bull side diagram from many mission learning articles

29
00:01:48.640 --> 00:01:53.770
showing the bias on variant straight off in the abode diagram.

30
00:01:53.770 --> 00:01:59.140
The center off the bull side is a model that has perfect prediction school,

31
00:01:59.140 --> 00:02:04.540
which means the low bias as we move away from the center.

32
00:02:04.540 --> 00:02:09.190
The bias increases now as we repeat the modeling process.

33
00:02:09.190 --> 00:02:12.440
If the scores are scattered all over the place,

34
00:02:12.440 --> 00:02:15.920
then it is a model with high variance else.

35
00:02:15.920 --> 00:02:18.440
It is a model with the low variance.

36
00:02:18.440 --> 00:02:24.840
Your ideal scenario is to have a low bias on low variance,

37
00:02:24.840 --> 00:02:28.440
a model with low bias and high variance.

38
00:02:28.440 --> 00:02:31.340
It's called war footing.

39
00:02:31.340 --> 00:02:34.560
Error model, with the high bias on low variance,

40
00:02:34.560 --> 00:02:37.240
is often called us under fitting on.

41
00:02:37.240 --> 00:02:41.250
Most of the time it happens mainly because we have very less

42
00:02:41.250 --> 00:02:46.630
data to build accurate Morning are the data is nonlinear and

43
00:02:46.630 --> 00:02:49.540
error trying to build a linear model.

44
00:02:49.540 --> 00:02:54.030
So common suggestions to overcome under fitting are two use more features

45
00:02:54.030 --> 00:02:58.340
into the model are two improve the predicting capability.

46
00:02:58.340 --> 00:03:02.590
Try adding complexity to your model over.

47
00:03:02.590 --> 00:03:05.030
Fitting usually happens when the model tries too

48
00:03:05.030 --> 00:03:10.840
hard to fit into the training set, and it's very bad in generalizing.

49
00:03:10.840 --> 00:03:16.610
So common suggestions to overcome over fitting are two use fever features to

50
00:03:16.610 --> 00:03:22.240
decrease the variance and to increase training samples.

51
00:03:22.240 --> 00:03:26.640
Regularization is a technique that is often used to a wide war.

52
00:03:26.640 --> 00:03:32.310
Fitting in over fitting, the model captures all the noises.

53
00:03:32.310 --> 00:03:35.540
The model will show high accuracy for the training set,

54
00:03:35.540 --> 00:03:38.390
but will perform poorly on a test dataset,

55
00:03:38.390 --> 00:03:43.170
which means it shows high variance for any good.

56
00:03:43.170 --> 00:03:45.240
Performing gentle is model.

57
00:03:45.240 --> 00:03:49.840
The model ideally needs to be low bias on low variance,

58
00:03:49.840 --> 00:03:53.490
so the process of converting this high variance to low

59
00:03:53.490 --> 00:03:57.540
variance is often called regularization.

60
00:03:57.540 --> 00:03:59.250
To explain this better,

61
00:03:59.250 --> 00:04:02.760
let's assume a simple linear regression with just two

62
00:04:02.760 --> 00:04:06.140
data points as shown in the chart.

63
00:04:06.140 --> 00:04:09.320
With this minimum data, the model will be over fitting.

64
00:04:09.320 --> 00:04:11.140
As you can see,

65
00:04:11.140 --> 00:04:17.440
the some off Squires between the actual unpredicted values are zero,

66
00:04:17.440 --> 00:04:22.620
but the same model might perform very poorly on a test data on the value

67
00:04:22.620 --> 00:04:26.740
off the cost function of the error function is higher.

68
00:04:26.740 --> 00:04:32.740
To minimize this, we modify the error function as shown below.

69
00:04:32.740 --> 00:04:34.270
This is called a ridge.

70
00:04:34.270 --> 00:04:37.940
Regression are held to regularization.

71
00:04:37.940 --> 00:04:40.500
Lambda is a positive value.

72
00:04:40.500 --> 00:04:44.340
On em is a slope off the line.

73
00:04:44.340 --> 00:04:49.060
Now the L to value may be higher on a war fit model which

74
00:04:49.060 --> 00:04:53.840
dictates us to optimize the model even further.

75
00:04:53.840 --> 00:04:59.010
Though the actual and predicted value in this new line are not exactly the same,

76
00:04:59.010 --> 00:05:04.390
the overall error has come down in lasso regression.

77
00:05:04.390 --> 00:05:07.240
Also called us l one regularization.

78
00:05:07.240 --> 00:05:09.270
Instead of squiring the slope,

79
00:05:09.270 --> 00:05:13.840
we calculate the absolute value off the some of the slope.

80
00:05:13.840 --> 00:05:23.000
Lasso regression is also used in feature extraction. By removing all the features who slope value is, zero are closer to zero.

