WEBVTT
1
00:00:00.640 --> 00:00:03.940
[Autogenerated] as he prepared for machine learning specialty exam.

2
00:00:03.940 --> 00:00:06.810
It's very important to have a good understanding on the

3
00:00:06.810 --> 00:00:10.940
building algorithms offered by Amazon Sage Maker.

4
00:00:10.940 --> 00:00:13.010
Let's start with linear learner.

5
00:00:13.010 --> 00:00:18.740
Al got to Implementation of linear learner involves three steps.

6
00:00:18.740 --> 00:00:22.240
First one is pre process.

7
00:00:22.240 --> 00:00:25.940
You can perform the normalization process manually,

8
00:00:25.940 --> 00:00:29.440
or you can let the algorithm do it for you.

9
00:00:29.440 --> 00:00:32.540
If the normalization option is turned on,

10
00:00:32.540 --> 00:00:36.360
the algorithm studies a sample of data and learned

11
00:00:36.360 --> 00:00:39.240
their mean on standard division,

12
00:00:39.240 --> 00:00:45.140
and each of the feature is calibrated to have a mean of zero value.

13
00:00:45.140 --> 00:00:50.840
To get the good results you need to ensure that the data is shuffled properly.

14
00:00:50.840 --> 00:00:53.840
Second step is training.

15
00:00:53.840 --> 00:01:00.240
This uses SG a stochastic Grady in dissent during the training phase,

16
00:01:00.240 --> 00:01:05.340
and you can also use some off the optimization algorithms like Adah,

17
00:01:05.340 --> 00:01:09.140
Paragraph and SG.

18
00:01:09.140 --> 00:01:12.450
You can also apparently optimize multiple models,

19
00:01:12.450 --> 00:01:16.540
with each one of them having different objectives.

20
00:01:16.540 --> 00:01:21.770
The third step CSV ality and the training is run parallel e,

21
00:01:21.770 --> 00:01:24.740
the models error evaluated against their validation.

22
00:01:24.740 --> 00:01:29.030
SATA on the optimal model is selected by comparing

23
00:01:29.030 --> 00:01:32.640
against the appropriate metal.

24
00:01:32.640 --> 00:01:37.340
Linear learners are supervised learning algorithms,

25
00:01:37.340 --> 00:01:40.450
though the name songs it's a regression algorithm,

26
00:01:40.450 --> 00:01:45.540
it can be used both for classification on regression.

27
00:01:45.540 --> 00:01:50.740
One of the requirements is that the data be represented in a metrics format,

28
00:01:50.740 --> 00:01:55.500
with all the rows representing the observation on columns representing the

29
00:01:55.500 --> 00:02:00.590
features with an additional column that represents the labor in terms of

30
00:02:00.590 --> 00:02:05.420
channel linear learners supports train validation,

31
00:02:05.420 --> 00:02:12.440
untested SATA with validation and test channels being optional.

32
00:02:12.440 --> 00:02:17.030
Both the record I will wrap brought above and CSV data

33
00:02:17.030 --> 00:02:20.440
formats are supported in training face.

34
00:02:20.440 --> 00:02:25.840
If the data isn't CSV former, the first column must be the labor.

35
00:02:25.840 --> 00:02:26.220
However,

36
00:02:26.220 --> 00:02:29.910
during the influence face along with Record I Go and

37
00:02:29.910 --> 00:02:34.740
CSV JSON format is also supported.

38
00:02:34.740 --> 00:02:41.640
Linear learners supports both file more on pipe more during the training phase.

39
00:02:41.640 --> 00:02:45.590
Linear learner can run either on a single are a multi

40
00:02:45.590 --> 00:02:50.710
machine CPU on GPU instances the metrics that are

41
00:02:50.710 --> 00:02:53.070
reported by linear learner algorithm.

42
00:02:53.070 --> 00:03:02.380
Our last function Accuracy F one score position on Rico AWS recommends that

43
00:03:02.380 --> 00:03:06.270
the tuning be performed against a validation metric instead.

44
00:03:06.270 --> 00:03:08.850
Off training metric.

45
00:03:08.850 --> 00:03:13.450
The require hyper parameters that needs to be set up by the users are the

46
00:03:13.450 --> 00:03:18.540
number of features in the input data number off classes.

47
00:03:18.540 --> 00:03:20.540
The predictor type.

48
00:03:20.540 --> 00:03:23.220
There are a bunch of other hyper parameters that can be said,

49
00:03:23.220 --> 00:03:27.640
but these three are mandatory ones.

50
00:03:27.640 --> 00:03:29.090
To clarify the theory,

51
00:03:29.090 --> 00:03:32.930
we're going to take a quick look at a sample notebook that is referred,

52
00:03:32.930 --> 00:03:34.370
inevitably, a sage maker.

53
00:03:34.370 --> 00:03:36.030
Documentation on.

54
00:03:36.030 --> 00:03:39.440
See how linear learner is implemented.

55
00:03:39.440 --> 00:03:43.380
I would like you to understand how the algorithm is implemented on.

56
00:03:43.380 --> 00:03:44.110
Later on,

57
00:03:44.110 --> 00:03:49.000
we will launch a sage maker notebook and go over each step in detail

58
00:03:49.000 --> 00:03:53.940
on get a hands on exercise under data ingestion,

59
00:03:53.940 --> 00:03:58.270
the sample data is fetched from the URL and train with

60
00:03:58.270 --> 00:04:02.740
validation and test data a fading.

61
00:04:02.740 --> 00:04:05.920
Since linear learner Expect the input data in record.

62
00:04:05.920 --> 00:04:07.440
I will format.

63
00:04:07.440 --> 00:04:11.440
We're converting the data to the required farmer.

64
00:04:11.440 --> 00:04:14.540
If you are familiar with Python and Umpire,

65
00:04:14.540 --> 00:04:18.340
this code should look very familiar to you.

66
00:04:18.340 --> 00:04:23.940
Once the data is converted, it is uploaded to the S3 bucket.

67
00:04:23.940 --> 00:04:26.940
Now that the data is processed and ready,

68
00:04:26.940 --> 00:04:30.940
we're ready to perform the training process.

69
00:04:30.940 --> 00:04:32.530
We're using the get image.

70
00:04:32.530 --> 00:04:36.100
You are a method to invoke the linear learner method from the

71
00:04:36.100 --> 00:04:40.740
darker container that's maintained by sage maker.

72
00:04:40.740 --> 00:04:44.860
Then we will create the estimator object and you can see

73
00:04:44.860 --> 00:04:47.000
all three require hyper parameters,

74
00:04:47.000 --> 00:04:51.240
error said on the training process is started.

75
00:04:51.240 --> 00:04:55.140
Once the training is completed, the model is deployed.

76
00:04:55.140 --> 00:05:01.040
Now you can pass the test data on performed predictions.

77
00:05:01.040 --> 00:05:04.640
Let's switch our attention to X G boost.

78
00:05:04.640 --> 00:05:07.350
Xcode boost is an efficient open source

79
00:05:07.350 --> 00:05:11.240
implementation off grading boosting algorithm.

80
00:05:11.240 --> 00:05:15.500
It is a supervised learning algorithm that can be used effectively in

81
00:05:15.500 --> 00:05:19.840
handling both classifications under aggression problems.

82
00:05:19.840 --> 00:05:24.050
It's called Grady int boosting because it uses agrarian decent

83
00:05:24.050 --> 00:05:29.540
algorithm to minimize the loss while adding new models.

84
00:05:29.540 --> 00:05:32.910
Xcode Bush algorithm can be used as a built in algorithm

85
00:05:32.910 --> 00:05:37.190
artists of framework like tensorflow and run the training

86
00:05:37.190 --> 00:05:40.640
script in your local environments.

87
00:05:40.640 --> 00:05:45.800
Extra boost Uses CSV and lib S3 in farmer to read input

88
00:05:45.800 --> 00:05:48.190
data both in training on in France.

89
00:05:48.190 --> 00:05:56.600
Face Amazon recommends using CPI use are not GP Use for training phase as the

90
00:05:56.600 --> 00:06:02.540
algorithm is memory intensive are not compute intensive.

91
00:06:02.540 --> 00:06:08.940
Extra boost algorithm computes metrics like accuracy area under the curve.

92
00:06:08.940 --> 00:06:16.040
If one score mean absolute error, mean average position means quiet.

93
00:06:16.040 --> 00:06:22.740
Error on a route means squared error during the training process.

94
00:06:22.740 --> 00:06:27.200
Here is a quick example showing X G boost all garden to train a

95
00:06:27.200 --> 00:06:31.040
regression model in the data ingestion face,

96
00:06:31.040 --> 00:06:34.540
you connect to the URL Andre to you, Avalon.

97
00:06:34.540 --> 00:06:38.760
Data on upload the data to S3 buckets for the training process.

98
00:06:38.760 --> 00:06:41.730
To begin we're using Get image.

99
00:06:41.730 --> 00:06:44.340
You are a method of estimator object.

100
00:06:44.340 --> 00:06:48.940
To fetch the XSLT, webOS shall guard them from the docker container.

101
00:06:48.940 --> 00:06:52.940
Then you prepare the input data conflict,

102
00:06:52.940 --> 00:06:53.430
output,

103
00:06:53.430 --> 00:06:59.100
data conflict a resource conflict hyper parameter and pause them

104
00:06:59.100 --> 00:07:03.540
as a parameter To create the training job.

105
00:07:03.540 --> 00:07:05.660
You can see the required hyper parameter.

106
00:07:05.660 --> 00:07:12.340
Enum round is set to 50 on the content type is lib CSV.

107
00:07:12.340 --> 00:07:23.000
Then you create an endpoint that can serve the model. And finally you passed the test dataset and check prediction accuracy.

