WEBVTT
1
00:00:00.840 --> 00:00:04.290
[Autogenerated] next is K nearest neighbors also called us

2
00:00:04.290 --> 00:00:09.540
Cayenne training in Kane and runs in three phases.

3
00:00:09.540 --> 00:00:13.320
First face is sampling in sampling.

4
00:00:13.320 --> 00:00:19.440
The size of the initial dataset is optimized so that it fits in the memory.

5
00:00:19.440 --> 00:00:22.740
Next face is dimensionality reduction,

6
00:00:22.740 --> 00:00:26.310
where the algorithm tries to remove the noise around the features.

7
00:00:26.310 --> 00:00:31.200
Using the algorithms like random forest and ready is a footprint off.

8
00:00:31.200 --> 00:00:35.770
The model in the memory index building optimizes the

9
00:00:35.770 --> 00:00:39.590
efficient look up off distance between the sample points and

10
00:00:39.590 --> 00:00:43.120
it's k nearest neighbors it provides.

11
00:00:43.120 --> 00:00:46.540
Three different types of indexes were flat index,

12
00:00:46.540 --> 00:00:53.340
an inverter index and inverted index with part quant ization.

13
00:00:53.340 --> 00:00:57.720
Kanaan can be used in modeling both classifications under aggression.

14
00:00:57.720 --> 00:01:03.620
Problems in a classifications problem algorithm queries the K points

15
00:01:03.620 --> 00:01:06.930
that are closer to the sample point and returns.

16
00:01:06.930 --> 00:01:09.440
The frequently use labor.

17
00:01:09.440 --> 00:01:13.070
In the case of regression it where is a cake closest point

18
00:01:13.070 --> 00:01:16.340
and returned the average off their values.

19
00:01:16.340 --> 00:01:22.090
Kane and supports both train and test data channels, and it uses a record.

20
00:01:22.090 --> 00:01:26.340
I go on CSV as an input file farmer.

21
00:01:26.340 --> 00:01:28.680
Keep in mind if you're using CSV,

22
00:01:28.680 --> 00:01:33.240
the first column needs to be the label and can use both file

23
00:01:33.240 --> 00:01:39.020
more are pipe more to read the data que en en can be trained

24
00:01:39.020 --> 00:01:41.420
on a CPU instantiate like MF.

25
00:01:41.420 --> 00:01:47.840
IT are a GPU instantiate like Pete for a classifier predictor

26
00:01:47.840 --> 00:01:51.760
Kane and computes accuracy on for regression.

27
00:01:51.760 --> 00:01:57.820
It computes means squired center require hyper parameters are,

28
00:01:57.820 --> 00:02:00.440
of course, the value of K.

29
00:02:00.440 --> 00:02:04.840
The number of features in the input predictor type that

30
00:02:04.840 --> 00:02:09.940
identifies if it's a classification are a regression.

31
00:02:09.940 --> 00:02:13.680
The number of data points to be sampled on the

32
00:02:13.680 --> 00:02:16.540
target dimension production target,

33
00:02:16.540 --> 00:02:21.740
which is necessary if the parameter dimension reduction type is sync.

34
00:02:21.740 --> 00:02:25.710
Let's jump into a Jupyter notebook on see how we can train a

35
00:02:25.710 --> 00:02:31.280
model using cayenne this example uses you see,

36
00:02:31.280 --> 00:02:36.880
I machine learning covert type dataset We're using W.

37
00:02:36.880 --> 00:02:41.430
Gay to download the data on in pre processing face.

38
00:02:41.430 --> 00:02:48.140
The data is split into training on test data with a 90 10 ratio.

39
00:02:48.140 --> 00:02:52.720
Then the data is uploaded to two separate S3 buckets,

40
00:02:52.720 --> 00:02:57.440
one for training on the other one for testing.

41
00:02:57.440 --> 00:02:59.630
The data is written in record.

42
00:02:59.630 --> 00:03:03.500
I will pro to be farmer in the training face.

43
00:03:03.500 --> 00:03:07.310
An estimator object is created and you can see we error

44
00:03:07.310 --> 00:03:10.260
fetching the cane and algorithm from the Docker Container

45
00:03:10.260 --> 00:03:14.340
registry and error using em, for instance,

46
00:03:14.340 --> 00:03:17.930
and error sitting the value off K to 10 on setting

47
00:03:17.930 --> 00:03:21.740
the predictor type to classifier.

48
00:03:21.740 --> 00:03:23.500
Once the training is completed,

49
00:03:23.500 --> 00:03:28.940
the endpoint is created which can be used for future predictions.

50
00:03:28.940 --> 00:03:31.440
Since this is a classification problem,

51
00:03:31.440 --> 00:03:36.540
we're using accuracy as a metric during prediction.

52
00:03:36.540 --> 00:03:40.340
Next, we're going to jump into random cut forest,

53
00:03:40.340 --> 00:03:43.640
which is an algorithm for anomaly detection.

54
00:03:43.640 --> 00:03:47.540
And it is an unsupervised learning algorithm.

55
00:03:47.540 --> 00:03:51.550
This algorithm look for outliers are anomalies in

56
00:03:51.550 --> 00:03:55.140
the data like unexpected spikes.

57
00:03:55.140 --> 00:03:57.140
Picks in Period City.

58
00:03:57.140 --> 00:04:00.640
Our unclassifiable data points.

59
00:04:00.640 --> 00:04:05.150
The first step is to fetch a random sample of data on a

60
00:04:05.150 --> 00:04:06.850
technique called a reservoir error.

61
00:04:06.850 --> 00:04:10.740
Sampling is used for this purpose.

62
00:04:10.740 --> 00:04:14.270
Next step in the training process is to slice the data

63
00:04:14.270 --> 00:04:16.940
into number off equal partitions.

64
00:04:16.940 --> 00:04:20.640
Then each partition is sent to an individual tree.

65
00:04:20.640 --> 00:04:21.520
On the tree.

66
00:04:21.520 --> 00:04:27.340
Recursive Lee organizes its partition into a binary tree.

67
00:04:27.340 --> 00:04:32.630
The third step is to choose the hyper parameters enum please,

68
00:04:32.630 --> 00:04:36.040
a number of samples per tree.

69
00:04:36.040 --> 00:04:40.400
The recommendation is to begin with 100 trees in are two balance

70
00:04:40.400 --> 00:04:46.140
between the anomaly score noise on moral complexity.

71
00:04:46.140 --> 00:04:51.840
Anomaly detection supports both train and test data channels.

72
00:04:51.840 --> 00:04:53.100
It supports record.

73
00:04:53.100 --> 00:04:55.360
I will prote above on CSV file.

74
00:04:55.360 --> 00:04:58.940
Formats on the data can be read both in file.

75
00:04:58.940 --> 00:05:01.840
More unp IT more.

76
00:05:01.840 --> 00:05:07.370
Amazon recommends using one Lee CPU Instances to run this algorithm on the

77
00:05:07.370 --> 00:05:13.940
general recommendation is to use M four C four or C five.

78
00:05:13.940 --> 00:05:20.040
Random cut forest computes F one score during the training process.

79
00:05:20.040 --> 00:05:24.050
The number of features in a dataset is a required hyper parameter.

80
00:05:24.050 --> 00:05:27.540
If you're running the job through the console enum,

81
00:05:27.540 --> 00:05:32.290
please enum Samples per tree are an optional hyper parameters,

82
00:05:32.290 --> 00:05:38.040
with the default value off 102 56 respectively.

83
00:05:38.040 --> 00:05:43.740
There's jump into a quick demo on See how random cut forest is implemented.

84
00:05:43.740 --> 00:05:48.260
You start this example by defining S3 bucket location for

85
00:05:48.260 --> 00:05:52.440
storing the training data undertrained model.

86
00:05:52.440 --> 00:05:59.240
This example uses the NYC Taxi dataset as we have seen in previous cases.

87
00:05:59.240 --> 00:06:04.510
The first step is to fetch the data from the source and pandas read CSV

88
00:06:04.510 --> 00:06:09.940
method is being used to convert the data To see is reformer.

89
00:06:09.940 --> 00:06:13.370
Unlike other examples where we fetch the algorithm from the

90
00:06:13.370 --> 00:06:17.340
container registry and pastor to the estimated object.

91
00:06:17.340 --> 00:06:21.140
We error directly instantiate ing the random cut far as stop chip,

92
00:06:21.140 --> 00:06:25.140
which is part of the sage maker package.

93
00:06:25.140 --> 00:06:29.760
We're using em, for instance, to run this training job and error.

94
00:06:29.760 --> 00:06:34.290
Overrating the default values for numb trees on numb samples.

95
00:06:34.290 --> 00:06:36.540
Poetry.

96
00:06:36.540 --> 00:06:45.000
Once the training is completed, you can deploy the model so that it can be used for prediction purposes.

