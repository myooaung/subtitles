1
00:00:01,040 --> 00:00:02,860
[Autogenerated] Let's take a look at the dataset that

2
00:00:02,860 --> 00:00:06,200
we're going to use in this demo.

3
00:00:06,200 --> 00:00:11,200
This is a bank marketing dataset that's used for predicting if a

4
00:00:11,200 --> 00:00:16,140
customer is going to sign up for a term deposit or not.

5
00:00:16,140 --> 00:00:16,700
Right away,

6
00:00:16,700 --> 00:00:24,440
you can see this dataset is a combination off numbers bullying and text values,

7
00:00:24,440 --> 00:00:29,540
and it shows the profile of customers with details like is age,

8
00:00:29,540 --> 00:00:36,080
job, marital status, education on so on.

9
00:00:36,080 --> 00:00:40,750
The column campaign indicates the number of times the client

10
00:00:40,750 --> 00:00:44,840
was contacted during this campaign process.

11
00:00:44,840 --> 00:00:49,810
P days indicates the number of days passed by after the client was

12
00:00:49,810 --> 00:00:54,040
last contacted during the previous campaign.

13
00:00:54,040 --> 00:00:59,240
The column previous indicates the total number of contacts performed

14
00:00:59,240 --> 00:01:04,590
prior to this campaign and API outcome shows the outcome off the

15
00:01:04,590 --> 00:01:10,360
previous campaign on the output variable deposit indicate if the client

16
00:01:10,360 --> 00:01:14,410
subscribe to the term deposit are not.

17
00:01:14,410 --> 00:01:19,340
Let's launch into Jupyter notebook and begin our exercise.

18
00:01:19,340 --> 00:01:25,840
This piece of code gets the current execution role from the Sage maker package

19
00:01:25,840 --> 00:01:32,980
on the current region from Bo to three session like we have seen multiple

20
00:01:32,980 --> 00:01:36,250
times in the previous model we're using get image.

21
00:01:36,250 --> 00:01:38,930
You are a method to get the image off.

22
00:01:38,930 --> 00:01:43,400
X G boost algorithm from the container registry.

23
00:01:43,400 --> 00:01:46,840
These container registries whose these algorithms,

24
00:01:46,840 --> 00:01:48,430
in a highly available,

25
00:01:48,430 --> 00:01:54,120
secured on scalable environment each arable is account is

26
00:01:54,120 --> 00:01:58,490
provided with one container registries and you need to be

27
00:01:58,490 --> 00:02:04,840
authenticated before you can pull on, push images from the repositories,

28
00:02:04,840 --> 00:02:11,040
Click run and you will see the success message being printed.

29
00:02:11,040 --> 00:02:11,270
Now,

30
00:02:11,270 --> 00:02:14,070
let's go ahead and create a S3 bucket where we will

31
00:02:14,070 --> 00:02:18,240
store the training on validation data.

32
00:02:18,240 --> 00:02:22,610
We're using the create bucket app a call to create the

33
00:02:22,610 --> 00:02:26,940
bucket with a bucket name Globomantics.

34
00:02:26,940 --> 00:02:28,190
When I run this cell,

35
00:02:28,190 --> 00:02:31,940
I'm getting an error since I have already created this bucket

36
00:02:31,940 --> 00:02:34,540
and if you're creating it for the first thing,

37
00:02:34,540 --> 00:02:38,440
you would not be seeing this center.

38
00:02:38,440 --> 00:02:44,740
Now, let's use the URL retrieval method to download the banking dataset.

39
00:02:44,740 --> 00:02:47,240
Once the data is successfully downloaded,

40
00:02:47,240 --> 00:02:52,940
we're storing it in a raid iFrame named model Underscore Data.

41
00:02:52,940 --> 00:02:54,570
Let me run this cell on we can.

42
00:02:54,570 --> 00:02:59,440
See the success message being printed.

43
00:02:59,440 --> 00:03:02,230
I would like to check the top few rows and check the

44
00:03:02,230 --> 00:03:06,050
structure of the data from the output.

45
00:03:06,050 --> 00:03:10,440
It is very evident that the data we download it is a pre process

46
00:03:10,440 --> 00:03:14,880
data because you don't see their strength that we saw in the raw

47
00:03:14,880 --> 00:03:20,790
data anymore on the number of columns is very more than what we

48
00:03:20,790 --> 00:03:23,940
saw in the actual data.

49
00:03:23,940 --> 00:03:27,540
You can see one job column is convert OData Multiple bullion

50
00:03:27,540 --> 00:03:31,440
column Checking if the customer is an admin,

51
00:03:31,440 --> 00:03:37,140
are a blue collar, are an entrepreneur are two housemaid.

52
00:03:37,140 --> 00:03:39,270
Let me list all the data columns,

53
00:03:39,270 --> 00:03:43,700
and you can see similarly one Marital status column in the raw

54
00:03:43,700 --> 00:03:48,440
data is converted to four bullion columns.

55
00:03:48,440 --> 00:03:51,440
Marital underscored Die wurst,

56
00:03:51,440 --> 00:03:57,650
marital underscore Married marital underscores single on marital under

57
00:03:57,650 --> 00:04:03,540
school Unknown on all these columns carry Boolean values.

58
00:04:03,540 --> 00:04:08,940
Let me check if any off the columns have null values in it,

59
00:04:08,940 --> 00:04:19,000
and if so, they need to be filled with proper values. And as I expected, all the 61 columns have no null values.

