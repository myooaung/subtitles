1
00:00:00,940 --> 00:00:05,540
[Autogenerated] before we move forward and split the data for training purposes.

2
00:00:05,540 --> 00:00:09,040
Let's look at some off the data splitting strategies.

3
00:00:09,040 --> 00:00:13,440
This by no means is an exhaustive list.

4
00:00:13,440 --> 00:00:16,440
Let's consider a business case where we would like to

5
00:00:16,440 --> 00:00:21,380
understand how the new feature that we launched last year

6
00:00:21,380 --> 00:00:24,640
is being received by the customer.

7
00:00:24,640 --> 00:00:25,770
So in this case,

8
00:00:25,770 --> 00:00:31,740
we're focused on the reviews from last year to derive meaningful information.

9
00:00:31,740 --> 00:00:35,640
So for cases like these, we will use a time based splitting.

10
00:00:35,640 --> 00:00:39,860
And timestamp is an important attributes on the data needs to

11
00:00:39,860 --> 00:00:44,340
be sorted by time before splitting the data.

12
00:00:44,340 --> 00:00:50,040
Consider the case where the data we have is very limited.

13
00:00:50,040 --> 00:00:54,400
So if we split the data in a 80 20 or 70 30 ratio

14
00:00:54,400 --> 00:00:59,840
for training and testing purposes, they might end up war fitting the model.

15
00:00:59,840 --> 00:01:04,120
In orderto address these scenarios we use careful cross

16
00:01:04,120 --> 00:01:07,830
validation splitting in careful splitting.

17
00:01:07,830 --> 00:01:13,240
The entire dataset is split in to case subsets and K minus

18
00:01:13,240 --> 00:01:17,920
one subset is used for training purpose on the last dataset

19
00:01:17,920 --> 00:01:20,780
is used for testing purposes on.

20
00:01:20,780 --> 00:01:23,440
Then the score is evaluated.

21
00:01:23,440 --> 00:01:28,240
The next round at different subset is taken from the case subsets.

22
00:01:28,240 --> 00:01:31,590
I'm testing this perform and this can be continued

23
00:01:31,590 --> 00:01:34,940
until we test against all the subsets.

24
00:01:34,940 --> 00:01:35,880
And finally,

25
00:01:35,880 --> 00:01:39,450
the average of the score is calculated on the final score

26
00:01:39,450 --> 00:01:44,540
is derived randomly splitting the data.

27
00:01:44,540 --> 00:01:49,940
Consider the case where you don't need to maintain the order off your data.

28
00:01:49,940 --> 00:01:51,910
In cases like this,

29
00:01:51,910 --> 00:01:56,060
this is a very good strategy to consider in order to have a good

30
00:01:56,060 --> 00:02:00,560
distribution of data between training and test data is also

31
00:02:00,560 --> 00:02:03,620
recommended to shuffle the data well before splitting.

32
00:02:03,620 --> 00:02:09,790
It is pseudo random number is used to randomly split the data and

33
00:02:09,790 --> 00:02:14,840
this is a strategy will be using in our exercise.

34
00:02:14,840 --> 00:02:19,430
I'm going to use the number I split method on split the data in the

35
00:02:19,430 --> 00:02:25,140
70 30 ratio for training on validation purpose.

36
00:02:25,140 --> 00:02:31,250
We're using random splitting strategy as mentioned before the output

37
00:02:31,250 --> 00:02:34,740
shows the total number off rows on the columns,

38
00:02:34,740 --> 00:02:39,040
both for training on validation data.

39
00:02:39,040 --> 00:02:45,340
One of the data requirements off training a CSV data using XSLT bustan garden.

40
00:02:45,340 --> 00:02:50,200
Is it the target variable must be present as the first column on the

41
00:02:50,200 --> 00:02:56,000
CSV file must not have a header record for inference.

42
00:02:56,000 --> 00:03:03,040
The algorithm assumes that CSS3 input does not have the label column.

43
00:03:03,040 --> 00:03:06,080
We're dropping the last two columns that indicates if the

44
00:03:06,080 --> 00:03:09,140
customer signed up to a term deposit or not.

45
00:03:09,140 --> 00:03:16,140
And Perl fixing the dataset with a target column and header is removed as well.

46
00:03:16,140 --> 00:03:21,140
This modified data is written to train dot CSV on

47
00:03:21,140 --> 00:03:24,540
validation that CSV files respectively.

48
00:03:24,540 --> 00:03:25,460
Next,

49
00:03:25,460 --> 00:03:30,240
I'm going to use Border three API to upload these two files to two

50
00:03:30,240 --> 00:03:35,630
separate folders trained at CSV under train folder and validation

51
00:03:35,630 --> 00:03:39,340
that CSV under validation folder.

52
00:03:39,340 --> 00:03:46,440
Let me run this cell and make sure that the files are successfully uploaded.

53
00:03:46,440 --> 00:03:50,420
I'm going to log in back to edible is concerned on validate

54
00:03:50,420 --> 00:03:53,840
If these files are uploaded successfully,

55
00:03:53,840 --> 00:03:57,740
let me go to Amazon S3 dashboard.

56
00:03:57,740 --> 00:04:01,860
That is a bucket by the name Globomantics that we created at the beginning

57
00:04:01,860 --> 00:04:11,040
of our excise click on sage maker Demo Xcode Boost Diem.

58
00:04:11,040 --> 00:04:15,740
You can see there are two folders Train on validation.

59
00:04:15,740 --> 00:04:17,380
Do not worry about the output folder.

60
00:04:17,380 --> 00:04:22,640
At this point, we will talk about it in the subsequent models,

61
00:04:22,640 --> 00:04:29,840
click on train and you can see the CSV file that you just uploaded.

62
00:04:29,840 --> 00:04:35,660
Select the file and you can also see the object you are Click on

63
00:04:35,660 --> 00:04:43,640
permissions This list all the users that have access to this object

64
00:04:43,640 --> 00:04:47,840
go back and click on Validation folder.

65
00:04:47,840 --> 00:04:50,930
You can see validation that CSV file is being

66
00:04:50,930 --> 00:04:54,440
uploaded under this folder as well.

67
00:04:54,440 --> 00:04:55,370
In this module,

68
00:04:55,370 --> 00:04:59,010
we started the training process by pulling the X G boost

69
00:04:59,010 --> 00:05:02,740
algorithm image from the container registry.

70
00:05:02,740 --> 00:05:06,640
Then we don't know that the data and prepared it

71
00:05:06,640 --> 00:05:10,540
before passing to the training process.

72
00:05:10,540 --> 00:05:15,170
Then you saw different data splitting strategies to split the input data for

73
00:05:15,170 --> 00:05:20,940
the training and eventually uploaded them to the S3 buckets.

74
00:05:20,940 --> 00:05:22,540
In the next subsequent models,

75
00:05:22,540 --> 00:05:27,940
you will see how to use sage maker estimator object to train the model,

76
00:05:27,940 --> 00:05:32,330
evaluate the metrics, logging to CloudWatch console on,

77
00:05:32,330 --> 00:05:36,290
monitor the progress Before we wrap up this course,

78
00:05:36,290 --> 00:05:40,600
you will also see how to use sage maker automated tuning

79
00:05:40,600 --> 00:05:48,000
process to tune the hyper parameters on find the best training job recommended by the sage maker

