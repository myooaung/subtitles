WEBVTT
1
00:00:01.570 --> 00:00:03.640
Hello everyone and welcome to this election.

2
00:00:04.150 --> 00:00:08.560
I'm super excited because now we're getting a little bit closer to actually building our convolution

3
00:00:08.710 --> 00:00:09.540
network.

4
00:00:09.610 --> 00:00:12.740
So let's go ahead and see how can we do this.

5
00:00:12.760 --> 00:00:19.040
So first step is that we wanted to perform or Colin normalization to the data or data normalization.

6
00:00:19.990 --> 00:00:20.980
Why is that.

7
00:00:20.980 --> 00:00:28.240
First let's see the original image so let's go to the head to the extreme and that's upper case or extreme

8
00:00:28.260 --> 00:00:29.180
that's on it.

9
00:00:29.380 --> 00:00:33.730
And if you take a look here you would see that these are the images which is again the values of the

10
00:00:33.730 --> 00:00:34.940
actual pixels.

11
00:00:35.200 --> 00:00:42.610
If you remember we had each image was 64 by 64 by three which is three dimensions and each pixel has

12
00:00:42.610 --> 00:00:46.160
a value ranging from zero up to 255.

13
00:00:46.590 --> 00:00:52.720
Zero indicates Black value and twenty two hundred fifty five indicates collared image.

14
00:00:52.730 --> 00:00:53.660
All right.

15
00:00:54.010 --> 00:00:59.840
What we do in general before we perform the training we actually prefer to do what we call a normalization.

16
00:00:59.860 --> 00:01:05.650
So we one of this values to range from zero to one that would be make it much easier for the network

17
00:01:05.710 --> 00:01:08.720
and improve our accuracy significantly.

18
00:01:08.740 --> 00:01:10.800
So let's go ahead and normalize the details.

19
00:01:10.810 --> 00:01:17.110
When I say okay X under squirting the quilt's 2 x underscored today and you can simply divide by 2 5

20
00:01:17.110 --> 00:01:18.360
5 that's it.

21
00:01:18.550 --> 00:01:23.680
That we're simply going to go and take all of these values and divide by 24.

22
00:01:23.680 --> 00:01:30.580
Two hundred fifty five just two the numbers we're going to change only from 0 to 1 y 255 because we

23
00:01:30.580 --> 00:01:36.120
know that the maximum value within our pixel is 255 indicating a white collar.

24
00:01:36.850 --> 00:01:40.570
If you go to X underskirt test let's do that to the testing data set as well.

25
00:01:40.570 --> 00:01:45.640
So X of discourse tests divided by 255 lists on that end.

26
00:01:45.640 --> 00:01:46.340
Looks good.

27
00:01:46.360 --> 00:01:49.060
Let's go ahead and view extreme.

28
00:01:49.060 --> 00:01:50.740
And that's our extreme again.

29
00:01:51.480 --> 00:01:51.740
OK.

30
00:01:51.760 --> 00:01:52.510
Looks way better.

31
00:01:52.540 --> 00:01:57.150
Now we have the actual data which is changing again from zeo Till one point eight figures.

32
00:01:57.160 --> 00:02:03.210
But that indicates like you know kind of a white collar say because one that's our white collar.

33
00:02:03.760 --> 00:02:10.420
Let's go ahead and for example show the data or show one image show that image show.

34
00:02:10.630 --> 00:02:17.190
And let's go ahead and view any simple extreme for example of 10 let's say nonintuitive.

35
00:02:17.200 --> 00:02:22.660
The images still you know intergraded way in that image show as kind of a collared image even though

36
00:02:22.660 --> 00:02:24.200
the pixels are ringing from Cedar to.

37
00:02:24.220 --> 00:02:26.740
Even if still normalize it.

38
00:02:27.400 --> 00:02:33.300
Let's go ahead and check the extreme dimensions so extend or shape one more time.

39
00:02:33.370 --> 00:02:40.740
Again it's six hundred sixty four 64 four by three in the killing 600 images and each image 64 by 64.

40
00:02:40.750 --> 00:02:45.480
By three let's go ahead and view why train and why.

41
00:02:45.510 --> 00:02:47.990
But shape run it.

42
00:02:48.010 --> 00:02:49.730
And we have 600 samples okay.

43
00:02:49.750 --> 00:02:50.230
Looks good.

44
00:02:50.240 --> 00:02:52.100
That's the actual label.

45
00:02:52.510 --> 00:02:55.570
So let's go ahead and actually build our convolution and Eric.

46
00:02:55.600 --> 00:02:57.390
That's kind of the most exciting part.

47
00:02:57.790 --> 00:03:00.160
So we're going to do they're going to use Keris.

48
00:03:00.260 --> 00:03:00.600
OK.

49
00:03:00.670 --> 00:03:07.220
Which is really powerful kind of tool within python that we can use to build our deep networks build

50
00:03:07.240 --> 00:03:09.820
our you know machine learning algorithms.

51
00:03:10.120 --> 00:03:16.530
It's really powerful and actually sits kind of on top of penso floor which is Google's platform used

52
00:03:16.530 --> 00:03:19.790
to build very powerful position here and it works.

53
00:03:20.230 --> 00:03:25.570
So let's go ahead and important important packages were going to be using.

54
00:03:25.570 --> 00:03:34.500
So going to the okay from say Keris dart models we're going to import sequential.

55
00:03:35.820 --> 00:03:37.260
Sequential.

56
00:03:37.900 --> 00:03:38.570
END.

57
00:03:38.880 --> 00:03:42.330
We're going to say okay from I'm going to show you exactly what they mean by all this let's try them

58
00:03:42.360 --> 00:03:46.480
first from Keris dog models.

59
00:03:46.810 --> 00:03:49.050
We're going to import it as well.

60
00:03:49.520 --> 00:03:52.020
Well one imported kind of 2D.

61
00:03:52.350 --> 00:03:56.440
We're going to import Max pooling to dÃ­az one.

62
00:03:57.900 --> 00:04:06.360
Actually this an uppercase N we're going to imported thence we are going to import flaten N we're going

63
00:04:06.360 --> 00:04:07.690
to import drop out.

64
00:04:07.690 --> 00:04:11.010
I hope some of these names are pretty familiar to you.

65
00:04:11.070 --> 00:04:12.230
So we have drop out.

66
00:04:12.360 --> 00:04:13.280
Okay.

67
00:04:13.890 --> 00:04:18.310
And let's go ahead as well any import from Keris.

68
00:04:19.480 --> 00:04:21.290
Dot optimizers.

69
00:04:23.420 --> 00:04:25.930
We're going to import.

70
00:04:27.030 --> 00:04:29.760
Adam which is the Adam optimiser are going to be using.

71
00:04:30.140 --> 00:04:33.470
And then I'm going to hear an import as well of the lads last step.

72
00:04:33.570 --> 00:04:37.770
I'm going to imported pensa board which is you know kind of again.

73
00:04:38.420 --> 00:04:41.160
We're going to use it extensively to actually treat our model.

74
00:04:41.390 --> 00:04:43.030
Masi Oka from Keris.

75
00:04:44.530 --> 00:04:46.550
Dot call-backs.

76
00:04:49.550 --> 00:04:50.550
Import.

77
00:04:51.760 --> 00:04:54.530
Energon imports penso born.

78
00:04:56.290 --> 00:04:57.870
Are ready OK.

79
00:04:57.880 --> 00:04:59.460
So let's do that first.

80
00:04:59.470 --> 00:05:00.700
And it looks good.

81
00:05:00.700 --> 00:05:02.220
So we're using tensaw floor as Beck.

82
00:05:02.240 --> 00:05:04.890
Let's go ahead and actually see what you mean by this.

83
00:05:04.900 --> 00:05:09.700
So first we import the sequential So what we're going to do here is we're going to build old convolution

84
00:05:09.860 --> 00:05:16.170
network from the left hand side to the right hand side in a sequential form.

85
00:05:16.320 --> 00:05:16.990
OK.

86
00:05:17.190 --> 00:05:17.590
All right.

87
00:05:17.590 --> 00:05:19.100
What do you mean by this.

88
00:05:19.150 --> 00:05:20.760
He gently thing a little bit of air.

89
00:05:20.770 --> 00:05:24.780
So that's my in my bed so keep in the models we're going to say it's layers.

90
00:05:24.850 --> 00:05:25.370
OK.

91
00:05:25.690 --> 00:05:29.100
And the optimizers he had just missed an hour.

92
00:05:29.230 --> 00:05:30.610
Let's try that again.

93
00:05:30.610 --> 00:05:31.780
And it looks good now Chad.

94
00:05:31.780 --> 00:05:32.790
Perfectly.

95
00:05:32.920 --> 00:05:35.830
So let's go ahead and actually understand what you mean by this.

96
00:05:35.830 --> 00:05:41.780
First step is he would then weigh sequential so we would going to use then to start building our convolution

97
00:05:41.820 --> 00:05:46.380
our network from the left hand side to the right hand side in a sequential fashion.

98
00:05:46.630 --> 00:05:52.420
So again to build for example our input layer and then we're going to use our feature detectors afterwards

99
00:05:52.630 --> 00:05:57.910
and then I'm going to do the meck spooling layer after and then maybe drop out afterwards and then we're

100
00:05:57.910 --> 00:06:01.000
going to flatten it and then do with a fully connected layer.

101
00:06:01.000 --> 00:06:06.190
So it's very important to actually use the sequential to build in a sequential fashion I'm going to

102
00:06:06.190 --> 00:06:08.200
show you again how can we do this.

103
00:06:08.200 --> 00:06:12.910
And then from Kira's book layers we import the convolution to do that we'll get to build our convolution

104
00:06:12.910 --> 00:06:14.740
or layer.

105
00:06:14.740 --> 00:06:20.170
We had to the max pooling going into the dense food for the fully connected fission network which is

106
00:06:20.170 --> 00:06:22.880
you know on the right hand side of the network.

107
00:06:23.320 --> 00:06:26.320
We're going to flatten n or drop out as well.

108
00:06:26.620 --> 00:06:30.140
And these two are important for the optimization portion.

109
00:06:30.140 --> 00:06:35.280
So were going to use Adam optimizers to actually go ahead and perform our training our aid.

110
00:06:35.430 --> 00:06:36.100
OK.

111
00:06:36.400 --> 00:06:38.920
So let's go ahead and train our model.

112
00:06:38.920 --> 00:06:44.130
So first we can in a sense sheet CNN underscored models for our scale model.

113
00:06:44.250 --> 00:06:51.140
You can call it whatever from our sequential you in a sense she has an object from these sequential

114
00:06:51.910 --> 00:06:54.560
OK and let's run that.

115
00:06:54.600 --> 00:06:55.460
It looks good.

116
00:06:55.570 --> 00:06:58.860
And let's go ahead and start actually start building our network.

117
00:06:58.870 --> 00:07:01.100
So going to say okay CNN.

118
00:07:01.910 --> 00:07:04.910
On the schauder model dots.

119
00:07:05.030 --> 00:07:11.570
This started airds organists start basically adding different layers to our network in a second to d

120
00:07:12.500 --> 00:07:19.120
n he'll we have to specify the dimensions of our convolution and so we are going to do here.

121
00:07:19.190 --> 00:07:22.510
We agreed to use 64 kernels.

122
00:07:22.610 --> 00:07:23.050
Okay.

123
00:07:23.090 --> 00:07:27.920
And the dimension at each granolas six basics you can go ahead and choose whatever value you want as

124
00:07:27.920 --> 00:07:34.310
long as it works and then afterwards you have to specify our input shape and our activation.

125
00:07:34.610 --> 00:07:39.670
So again say okay input on discourse shape equals two.

126
00:07:39.920 --> 00:07:44.770
When you say input sheet that's basically the shape of the actual image that was feeding to our network.

127
00:07:44.780 --> 00:07:51.270
So if you us a call we have the dimensions was 64 64 64 by three because it was a coloured image.

128
00:07:51.340 --> 00:07:51.970
Right.

129
00:07:52.400 --> 00:07:53.970
And less set.

130
00:07:54.010 --> 00:07:54.820
Were going to do here.

131
00:07:54.860 --> 00:07:59.760
We have to specify the activation on activation function.

132
00:08:00.080 --> 00:08:03.400
So we're going to say okay activation equals two.

133
00:08:03.560 --> 00:08:07.390
We're going to use the elu activation here in this case.

134
00:08:07.580 --> 00:08:13.720
So bear in mind we're going to use the loo activation in all our hidden layers so all the layers and

135
00:08:13.730 --> 00:08:16.840
conversion layers as well were going to use to give us a call.

136
00:08:16.850 --> 00:08:19.950
We use that elude to actually add then in the edit it or not.

137
00:08:20.360 --> 00:08:26.040
However the output on the output lay were going to use the sigmoid function.

138
00:08:26.330 --> 00:08:26.660
Why.

139
00:08:26.660 --> 00:08:31.160
Because we actually want our output to be set to read and we're looking for the output to be either

140
00:08:31.160 --> 00:08:38.260
0 or 1 right or kind of you know smiling or not smiling it doesn't make sense to edry you at the aptly.

141
00:08:38.710 --> 00:08:39.140
All right.

142
00:08:39.260 --> 00:08:43.090
So that's the first layer Let's go ahead and add the additional layer.

143
00:08:43.100 --> 00:08:44.100
Let's go ahead though and say Okay.

144
00:08:44.120 --> 00:08:48.400
CNN underscored model and we're going to add as well.

145
00:08:48.420 --> 00:08:53.750
And let's add it Max spooling layer if you as a member if we do the convolutions first and then we're

146
00:08:53.750 --> 00:09:00.200
going to do the Mexifornia after the Tsuki Amex pulling to the end.

147
00:09:00.200 --> 00:09:08.930
We're going to specify what portion size pool underscore size equal to and we're going to use filters

148
00:09:08.960 --> 00:09:11.760
to both to filter to actually do the mext pulling for us.

149
00:09:11.880 --> 00:09:13.850
Let's action on that to make sure that we're actually running.

150
00:09:13.880 --> 00:09:15.550
Okay looks good.

151
00:09:16.170 --> 00:09:19.130
All right let's add a drop outlay to Mexico.

152
00:09:19.130 --> 00:09:26.330
CNN and the squid model dot Ed and we're going to air drop out.

153
00:09:26.330 --> 00:09:33.130
Drop out n or specify let's say point to kind of 20 per cent of the neurones were going to be dropped

154
00:09:33.130 --> 00:09:35.290
out randomly.

155
00:09:35.340 --> 00:09:35.980
All right.

156
00:09:35.990 --> 00:09:37.230
So that's pretty much it.

157
00:09:37.250 --> 00:09:39.410
That's pretty much how can we build our first layer.

158
00:09:39.550 --> 00:09:44.850
Firstly going and do what we're going to do here is going to add an additional convolution afterwards.

159
00:09:44.870 --> 00:09:45.230
Okay.

160
00:09:45.230 --> 00:09:48.470
That seems to improve the performance of our network.

161
00:09:48.710 --> 00:09:50.150
So let's go ahead actually do this.

162
00:09:50.150 --> 00:09:53.760
We're going to add again another convolution layer and the other mext pooling layer.

163
00:09:53.770 --> 00:09:56.100
So going to basically copy their base at here.

164
00:09:56.510 --> 00:09:59.090
And let's for example make it again six by six.

165
00:09:59.090 --> 00:09:59.930
No problem.

166
00:10:00.110 --> 00:10:03.320
But we don't where we don't want to add in it because it doesn't make sense.

167
00:10:03.320 --> 00:10:04.880
He we don't have an input to the network.

168
00:10:04.880 --> 00:10:05.180
Why.

169
00:10:05.180 --> 00:10:10.400
Because the output coming out from this layer is we're going to be fed as an input to this layer.

170
00:10:10.400 --> 00:10:17.620
So does it make sense to actually this sort of this endless keep the activation as Lulu and then exporting

171
00:10:17.630 --> 00:10:19.210
let's make it two way too.

172
00:10:19.210 --> 00:10:23.160
All right so here we added kind of two layers of convolutions Let's go ahead.

173
00:10:23.180 --> 00:10:24.920
We have two more steps to go.

174
00:10:25.100 --> 00:10:27.640
We wanted to add simply our flattening.

175
00:10:27.710 --> 00:10:36.010
So let's go ahead and flatten that CNN underscored model and we're going to Ed's flat and layer.

176
00:10:38.180 --> 00:10:44.770
Flatten and that we're simply going to flatten our matrix we're going to make it just one single layer

177
00:10:45.040 --> 00:10:48.770
to be ready to actually be fed into our fully connectedly.

178
00:10:49.300 --> 00:10:53.640
So let's go ahead and add a fully connected network as a mistake.

179
00:10:53.650 --> 00:10:55.360
CNN underscored model.

180
00:10:55.780 --> 00:10:56.690
We're going to see say Okay.

181
00:10:56.760 --> 00:10:57.650
Ed.

182
00:10:57.900 --> 00:11:01.200
Anyone want to add if we connected that's opening using dense.

183
00:11:01.220 --> 00:11:09.580
Okay which is the output most the open player or the opport network we just say dense silky opiate underscore

184
00:11:09.940 --> 00:11:15.520
dimensions equates to you can select whatever number we select one to eight.

185
00:11:15.540 --> 00:11:17.340
Anyone going to specify as well.

186
00:11:17.410 --> 00:11:21.770
Activation equals to really who in this case.

187
00:11:21.790 --> 00:11:25.130
Again please bear in mind that this is the hidden layer.

188
00:11:25.150 --> 00:11:31.040
One of the layers within our fully connected layer Let's go ahead and actually do the last network with

189
00:11:31.040 --> 00:11:32.910
the less output of our network.

190
00:11:33.000 --> 00:11:33.320
OK.

191
00:11:33.340 --> 00:11:39.900
So yes okay CNN on called good model that Ed let's go ahead and add dense as well.

192
00:11:40.360 --> 00:11:47.640
And we need we need to specify our output dimensions so our output underscored a dimension equal to

193
00:11:47.800 --> 00:11:53.230
one and that's very important to me to make sure to adjust that vett that premiere because here we only

194
00:11:53.230 --> 00:11:58.150
have one opiate which is you know kind of a kind of a binary output you get to zero in the killing that

195
00:11:58.150 --> 00:12:01.130
faces not smiling or the one in beginning's mining.

196
00:12:01.750 --> 00:12:06.110
And go ahead and do the activation sequence to sigmoid.

197
00:12:09.160 --> 00:12:13.950
Again it's important to make sure that all the activation functions in he'd within the network inside

198
00:12:13.950 --> 00:12:15.140
our values.

199
00:12:15.520 --> 00:12:15.850
Okay.

200
00:12:15.860 --> 00:12:17.380
Just to add non-linearity.

201
00:12:17.560 --> 00:12:22.030
However for the opiate if you're opiate is binary or category if it's OK if it's caddick What are the

202
00:12:22.030 --> 00:12:22.600
different story.

203
00:12:22.600 --> 00:12:28.420
But if it's if it's binary you need to make sure it's actually sigmoid output was sigmoid activation

204
00:12:28.420 --> 00:12:29.960
function in the opiate.

205
00:12:30.120 --> 00:12:30.690
All right.

206
00:12:30.700 --> 00:12:32.180
Let's grab them on that.

207
00:12:32.510 --> 00:12:34.580
And it looks pretty good to us.

208
00:12:34.580 --> 00:12:35.230
All right.

209
00:12:35.320 --> 00:12:36.240
Okay.

210
00:12:36.280 --> 00:12:40.590
And that's been much how can you actually build our network I used to enjoy that lecture in the future

211
00:12:40.630 --> 00:12:41.210
lecture.

212
00:12:41.410 --> 00:12:45.130
We're going to go ahead and actually start treating our model because now we haven't done any training

213
00:12:45.640 --> 00:12:46.750
or we have done so far.

214
00:12:46.750 --> 00:12:51.550
It just you know like building our network the next lecture going to see how can we treat our model

215
00:12:51.940 --> 00:12:56.260
and then afterwards we're going to start to test out what people get in to their lecture and see you

216
00:12:56.320 --> 00:12:57.760
in the next lecture.
