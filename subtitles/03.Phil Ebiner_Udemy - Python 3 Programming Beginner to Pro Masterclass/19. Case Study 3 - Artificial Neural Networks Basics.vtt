WEBVTT
1
00:00:00.640 --> 00:00:02.820
Hello everyone and welcome to this lecture.

2
00:00:03.220 --> 00:00:08.890
I'm super excited because now we're going to dig a little bit deeper and start building our evolutional

3
00:00:08.980 --> 00:00:14.650
neural network and understand what you mean by deep learning and Confucian nemuro network home it's

4
00:00:14.650 --> 00:00:18.630
super exciting and it's very interesting you can do tons of applications with it.

5
00:00:18.890 --> 00:00:19.390
OK.

6
00:00:19.630 --> 00:00:26.020
So before building our you know convolutedly network Let's take a look at some of the basics of artificial

7
00:00:26.320 --> 00:00:30.340
neural neuro networks or that fishell the basis of neurones.

8
00:00:30.470 --> 00:00:31.530
OK.

9
00:00:32.200 --> 00:00:34.910
So what do they mean by fissionables.

10
00:00:35.200 --> 00:00:43.780
Simply put fission you know networks are kind of models that are used mainly to mimic the human brain.

11
00:00:44.170 --> 00:00:52.510
So what scientists did in a very calm and very high level is just look at our human brain okay which

12
00:00:52.510 --> 00:00:58.810
is how the human brain works and these try to mimic the human brain in a mathematical format.

13
00:00:58.810 --> 00:01:01.290
And that's the power over Fission networks.

14
00:01:01.450 --> 00:01:06.880
They can actually build reveal kind of know like mimic the human brain you know b.b. kind of you know

15
00:01:06.880 --> 00:01:10.500
building your own mini brain in a way which is really powerful.

16
00:01:10.870 --> 00:01:16.500
So let's take a look at how humans think in general or what's how the human brain thing and then see

17
00:01:16.560 --> 00:01:19.940
how can we actually mimic that mathematically.

18
00:01:19.960 --> 00:01:27.370
So this is simply kind of an image that indicates that how humans how often Human think or how human

19
00:01:27.370 --> 00:01:34.660
brains think simply put our human brain consists of bunch of neurones neurones and you can see the neurones

20
00:01:34.660 --> 00:01:35.830
are kind of firing.

21
00:01:35.890 --> 00:01:36.470
Okay.

22
00:01:36.730 --> 00:01:40.980
Will mean finding that means the kind of processing information as it goes.

23
00:01:41.320 --> 00:01:48.010
So then neurone that again he was talking about the human neurone or how the human brain works.

24
00:01:48.100 --> 00:01:54.440
So the new loan simply collects different signals okay coming from different channels called dendrite.

25
00:01:54.470 --> 00:01:55.010
All right.

26
00:01:55.480 --> 00:02:01.320
And what happens is the human brain collect that information and process that information in we'll call

27
00:02:01.320 --> 00:02:03.020
it nucleus in here.

28
00:02:03.250 --> 00:02:03.850
OK.

29
00:02:04.090 --> 00:02:11.320
And then does you know some mathematical operations some operation somehow and generate certain output

30
00:02:11.440 --> 00:02:17.440
on what we call the accent here and that accent is connected to another neurone and that neurone again

31
00:02:17.530 --> 00:02:24.160
takes all the inputs from all dendrite before and processing within the nucleus and sends it to the

32
00:02:24.160 --> 00:02:24.760
exon.

33
00:02:25.090 --> 00:02:30.700
And all these neurones and billions of these neurones are connected to each other communicating to each

34
00:02:30.700 --> 00:02:31.340
other.

35
00:02:31.440 --> 00:02:32.710
There's just marvellous way.

36
00:02:32.710 --> 00:02:37.940
It's just an amazing way because there are billions of these neurones connected to each other.

37
00:02:38.400 --> 00:02:38.700
OK.

38
00:02:38.710 --> 00:02:40.510
So how do humans learn.

39
00:02:40.610 --> 00:02:45.770
So humans learn by adaptively changing the bonds tense between these neurones.

40
00:02:45.790 --> 00:02:48.200
So here is this Exon values.

41
00:02:48.250 --> 00:02:48.910
Okay.

42
00:02:48.940 --> 00:02:56.180
As you know this tense between this for example neurone and the next more on these ones genthe changes

43
00:02:56.230 --> 00:02:59.730
that's how humans start to learn as we move forward.

44
00:02:59.740 --> 00:03:04.620
For example we learn how to let's say you know categorize for example as a kid.

45
00:03:04.630 --> 00:03:09.190
Categorize Okay that's for example like a cup that's for instance like a dog.

46
00:03:09.220 --> 00:03:10.940
That's the cat and so on.

47
00:03:11.110 --> 00:03:13.840
And that's how humans learn as we go.

48
00:03:14.130 --> 00:03:15.090
Okay.

49
00:03:15.430 --> 00:03:19.770
That's kind of a quick overview of some of the biology we don't really care about that much.

50
00:03:19.810 --> 00:03:23.500
What we care about okay what how can we represent that mathematically.

51
00:03:23.500 --> 00:03:29.080
So what scientist did which is fascinating if we look at this basically in your own who say okay you

52
00:03:29.080 --> 00:03:36.000
know what let's go ahead and present that mathematically one are going to do here are going to say okay

53
00:03:36.160 --> 00:03:40.010
now I have a bunch of inputs P1 P2P three.

54
00:03:40.180 --> 00:03:40.910
OK.

55
00:03:41.170 --> 00:03:47.100
And here I have again my nucleus the same unit that I had before and he does have a bunch of weights.

56
00:03:47.200 --> 00:03:47.810
All right.

57
00:03:47.860 --> 00:03:50.390
Connected to this nucleus right.

58
00:03:50.410 --> 00:03:51.700
Bear with me.

59
00:03:51.700 --> 00:03:52.810
So what's going to happen is.

60
00:03:53.180 --> 00:03:59.540
And that's how the mathematics of it all works and that's how just can we did present one single neurone.

61
00:03:59.590 --> 00:04:06.050
What we're going to do here I'm going to say OK whatever input going to this mutant neurone P1 P2P three

62
00:04:06.510 --> 00:04:15.100
we can multiply these inputs by the Wheat's So here we have P1 P2 those W2 And P.S. those pines w3.

63
00:04:15.280 --> 00:04:15.930
OK.

64
00:04:16.090 --> 00:04:17.810
And then here we're just going to process.

65
00:04:17.820 --> 00:04:21.720
This is a very simple form that's the nucleus in a very simple form.

66
00:04:21.880 --> 00:04:26.050
It just sums up the inputs and the weights.

67
00:04:26.260 --> 00:04:31.260
And then at specific bias be just kind of you know shifting the signal up and down don't have otherwise

68
00:04:31.270 --> 00:04:32.090
so far.

69
00:04:32.380 --> 00:04:34.630
And what happens is that will generate an output.

70
00:04:35.170 --> 00:04:41.460
And then we apply what we call an activation function f okay to that N dujon that is an output.

71
00:04:41.800 --> 00:04:45.130
And that's pretty much how can you build one single neurone.

72
00:04:45.420 --> 00:04:45.880
OK.

73
00:04:45.910 --> 00:04:48.310
Tomorrow of course we have tons of questions.

74
00:04:48.310 --> 00:04:49.730
What do you mean by an F.

75
00:04:49.780 --> 00:04:51.630
What do you mean by activation function.

76
00:04:52.000 --> 00:04:57.070
What do you mean by a bias or that just we're going to discuss this cause that in lots of details in

77
00:04:57.070 --> 00:04:59.570
the future sections.

78
00:05:00.160 --> 00:05:01.040
You're only objective.

79
00:05:01.040 --> 00:05:02.630
You don't want you to learn it.

80
00:05:02.660 --> 00:05:06.110
How can we actually model the human brain.

81
00:05:06.110 --> 00:05:09.030
You know from the biological format into just Mathematica.

82
00:05:09.050 --> 00:05:12.670
For me it just has a bunch of inputs multiplied by Wheat's.

83
00:05:12.860 --> 00:05:13.790
We some them up.

84
00:05:13.790 --> 00:05:18.050
We are places and bias we apply specific activation function that will generate an output.

85
00:05:18.210 --> 00:05:22.440
And that would present one single neurone or a.

86
00:05:22.490 --> 00:05:27.010
So how could we build that artificial network so to build an efficient network.

87
00:05:27.050 --> 00:05:30.030
We're going to actually connect a bunch of these neurones together.

88
00:05:30.320 --> 00:05:32.810
We're going to take let's say like thousands of them.

89
00:05:33.200 --> 00:05:37.690
We can we organize them obviously to input layer to a hidden layer.

90
00:05:37.700 --> 00:05:43.550
So each of these bubbles that are present in Europe and this new one takes all the inputs coming from

91
00:05:43.580 --> 00:05:49.680
previous basically layer does kind of specific processing which is you know our planning activation

92
00:05:49.700 --> 00:05:52.490
function SORN and generates an output.

93
00:05:52.490 --> 00:05:54.560
And this new number the exact same thing.

94
00:05:54.560 --> 00:05:57.330
This neurone is connected to all the neurones beforehand.

95
00:05:57.350 --> 00:06:01.200
In the previous layer the same processing generates an output and so on.

96
00:06:01.550 --> 00:06:03.620
And that's how we process information.

97
00:06:03.830 --> 00:06:09.640
That's how we present an artificial neural network or you know kind of mimic the human brain in a mathematical

98
00:06:09.640 --> 00:06:10.340
format.

99
00:06:10.520 --> 00:06:13.830
And that's amazing because now we can actually build your own brain.

100
00:06:14.030 --> 00:06:20.300
Now you can you know a team you know for example they give a specific out a mini brain to detect smiling

101
00:06:20.300 --> 00:06:21.440
faces for instance.

102
00:06:21.440 --> 00:06:22.100
And so.

103
00:06:22.340 --> 00:06:23.020
All right.

104
00:06:23.120 --> 00:06:28.280
And that's a quick overview of how human brain works and how can we represent present mathematically

105
00:06:28.610 --> 00:06:29.540
in the next section.

106
00:06:29.570 --> 00:06:33.440
We're going to learn how can we build our convolution with a network.

107
00:06:33.650 --> 00:06:39.580
You know just kind of which is I'm kind of a more advanced form of simple not efficient or network here.

108
00:06:39.800 --> 00:06:45.060
And I'm going to learn how can we do what we call it feature detectors Max spooning and so on.

109
00:06:45.230 --> 00:06:48.520
I hope you get it during a lecture and see you in the next lecture.
