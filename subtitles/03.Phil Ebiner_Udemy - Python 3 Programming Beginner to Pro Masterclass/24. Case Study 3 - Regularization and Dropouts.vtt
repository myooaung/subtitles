WEBVTT
1
00:00:00.920 --> 00:00:04.200
Hello everyone and welcome this lecturer in the previous lectures.

2
00:00:04.250 --> 00:00:09.860
We discussed how can we build our convolution your network pretty much from scratch going through all

3
00:00:09.860 --> 00:00:15.380
the feature detectors or the next pooling layer or the loose layers or the flattening and so on and

4
00:00:15.380 --> 00:00:17.260
so forth here.

5
00:00:17.630 --> 00:00:22.750
I just wanted to discuss know just a quick give and give you kind of a quick hint of how can we improve

6
00:00:22.760 --> 00:00:27.250
for example the accuracy or how can we improve the performance of the network.

7
00:00:27.650 --> 00:00:33.590
We can improve the performance of the network by first by increasing the number of filters.

8
00:00:33.590 --> 00:00:37.640
So here for example or a number of feature detectors we can actually increase the number of feature

9
00:00:37.640 --> 00:00:38.720
detectors.

10
00:00:38.720 --> 00:00:43.300
So for example if we're using 32 feature detectors we can use 64 instead.

11
00:00:43.310 --> 00:00:49.190
That might improve the access Not necessarily but it adds more potential of expecting more data and

12
00:00:49.190 --> 00:00:56.240
more features out of our images and then hence results in a better improved performance.

13
00:00:56.810 --> 00:01:03.890
The next the next improvement that we could perform in college dropout OK and drop out is actually very

14
00:01:03.890 --> 00:01:05.450
powerful technique.

15
00:01:05.630 --> 00:01:06.200
Okay.

16
00:01:06.410 --> 00:01:12.770
Which is can be used to improve legs medically improves the general aviation capability of the network

17
00:01:13.250 --> 00:01:21.020
which is how they will be able to generalise and actually perform on it testing data better even though

18
00:01:21.020 --> 00:01:23.520
it hasn't seen this data before or during training.

19
00:01:23.650 --> 00:01:24.970
OK.

20
00:01:25.580 --> 00:01:27.050
So let's take a look at what this means.

21
00:01:27.050 --> 00:01:29.840
So let's assume that we have here our efficient network.

22
00:01:30.170 --> 00:01:34.420
What we do what we mean by top out we're actually going to select some of these new ons.

23
00:01:34.430 --> 00:01:39.710
So let's say this new and for example and let's say this neurone and let's say Come here this neurone

24
00:01:39.980 --> 00:01:44.410
and these neurones are going to do here is going to basically drop them out.

25
00:01:44.690 --> 00:01:48.100
We're going to do more than your aunt and remove all the dependencies.

26
00:01:48.110 --> 00:01:54.290
So actually we drop that neurone and we more of the area was connected to it which are then you're on.

27
00:01:54.300 --> 00:01:56.150
And we drop all the areas connected to it.

28
00:01:56.150 --> 00:01:57.620
It's just incredible.

29
00:01:57.820 --> 00:01:58.830
OK OK.

30
00:01:58.860 --> 00:01:59.480
What's the point.

31
00:01:59.480 --> 00:02:02.020
Like what's what's the advantages of having this.

32
00:02:02.030 --> 00:02:11.270
So what we actually has been proven that when we do drop out which is kind of one of the regulare lazy

33
00:02:11.930 --> 00:02:17.280
regularisation techniques Okay it can help to reduce overfitting of the network.

34
00:02:17.310 --> 00:02:22.730
It's kind of you know like we are trying to you know as an it has to learn in our studies to become

35
00:02:22.730 --> 00:02:24.820
better and better at what we do.

36
00:02:24.860 --> 00:02:32.420
We we drop parts of these neurones so the network we know we have such a train more afterwards it gets

37
00:02:32.420 --> 00:02:38.990
it gets back even more the stronger it gets back and try to avoid what we call the overfitting problem.

38
00:02:39.240 --> 00:02:44.870
OK what do you mean by overfitting what happen is if you need to be deficient in what it is for example

39
00:02:44.870 --> 00:02:50.660
using teeling dataset what happened is the network might try to go ahead and try to look at the different

40
00:02:50.660 --> 00:02:52.930
details of the teeling data.

41
00:02:53.090 --> 00:02:54.320
We do not want this.

42
00:02:54.320 --> 00:03:01.190
We do we want the now to generalize you on the net to even if how to classify for example like smiling

43
00:03:01.190 --> 00:03:05.620
faces even if has seen the smiling faces before during training.

44
00:03:05.720 --> 00:03:13.730
OK so drop out you first dropping out units in a traditional network nuance in in here when we perform

45
00:03:13.730 --> 00:03:19.300
training with our drop outs we've developed a co-dependency among each other doing training.

46
00:03:19.310 --> 00:03:23.540
So this new will depend on this new will on will depend on all the in as before.

47
00:03:23.560 --> 00:03:29.660
And what we could do during drop out of it which is again part of the regularisation technique is actually

48
00:03:29.660 --> 00:03:37.310
go ahead and we drop these neurones when we drop them net will become way better because the reduced

49
00:03:37.310 --> 00:03:43.760
the overfitting of the network becomes more generalized becomes more way better and then starts again

50
00:03:43.880 --> 00:03:44.730
as you move forward.

51
00:03:44.750 --> 00:03:50.270
These networks or these kind of neurones start to capture all the learnings again and the performance

52
00:03:50.580 --> 00:03:51.490
and hands again.

53
00:03:51.600 --> 00:03:52.090
Right.

54
00:03:52.320 --> 00:03:57.100
And then this just a quick overview of the two improvements that could be done to the conversion year

55
00:03:57.110 --> 00:03:57.660
network.

56
00:03:57.800 --> 00:03:59.370
Let's go ahead and implement.

57
00:03:59.370 --> 00:04:04.050
You know like one or two of these improvements in our network.
