WEBVTT
1
00:00:01.080 --> 00:00:04.860
Hello everyone and welcome to this lecture and this lecture.

2
00:00:04.860 --> 00:00:12.550
We're going to get started and analyze the Amazon Alexa reviews rating.

3
00:00:12.740 --> 00:00:13.830
It's actually really interesting.

4
00:00:13.920 --> 00:00:14.900
Let's get started.

5
00:00:14.950 --> 00:00:18.690
Maybe going to walk you through the data overview we have done that in the past in the previous lecture

6
00:00:19.050 --> 00:00:20.820
but I'm gonna walk you through it really quick.

7
00:00:20.850 --> 00:00:27.810
So now we have 3000 Amazon customer reviews the store readings and our objective here is to simply do

8
00:00:27.810 --> 00:00:31.710
some data analysis visualization some sentiment analysis and so on.

9
00:00:32.310 --> 00:00:34.930
Please please go ahead and do in the.

10
00:00:35.220 --> 00:00:37.590
Make sure that you installed world cloud.

11
00:00:37.590 --> 00:00:44.040
So pip install word cloud should install it and you're going to be ready to to go ahead.

12
00:00:44.820 --> 00:00:47.690
So first step we're going to import important libraries.

13
00:00:47.710 --> 00:00:51.960
We're going to import pandas as PD.

14
00:00:51.960 --> 00:01:00.420
We're going to import none pi as MP because remember pandas is mainly used for kind of data frame manipulations

15
00:01:00.460 --> 00:01:02.810
because remember think of it as excel.

16
00:01:02.850 --> 00:01:06.720
So this is basically as like you having excel within a python environment.

17
00:01:06.720 --> 00:01:07.410
That's num pi.

18
00:01:07.440 --> 00:01:11.520
That's pandas num pi is mainly used for numerical analysis.

19
00:01:11.580 --> 00:01:20.740
We're going to import as well seaborne as S.A. and seaborne is used again for data visualization as

20
00:01:20.750 --> 00:01:23.310
actually used for statistical data visualization.

21
00:01:23.330 --> 00:01:30.020
It's very helpful very powerful to show you tons of information using very very small commands and then

22
00:01:30.020 --> 00:01:42.090
we're going to import map plot lib map plot lib dot pie plot as BLT again map plot lib is useful for

23
00:01:42.090 --> 00:01:50.180
data visualization and plotting as well ships enter to run its gates take a little bit of a while and

24
00:01:50.180 --> 00:01:50.840
we're good to go.

25
00:01:50.840 --> 00:01:54.700
All right let's go ahead and actually load the data which is the actual reviews.

26
00:01:54.830 --> 00:02:02.540
So if you guys go back here to our project here which is analysing Amazon reviews you will find that

27
00:02:02.540 --> 00:02:07.640
there is a file here called Amazon underscored Alexa dot t as V.

28
00:02:07.670 --> 00:02:13.250
And that's simply the data you guys can download the data as well from Kaggle and this is the link for

29
00:02:13.250 --> 00:02:13.700
the data.

30
00:02:13.700 --> 00:02:18.420
Again this data is obviously like you know like open open source available data to everybody.

31
00:02:18.470 --> 00:02:24.320
So going to say OK let's read the data so we're going to use pen this data frame or pen this library

32
00:02:24.320 --> 00:02:26.030
to lead to load our data.

33
00:02:26.030 --> 00:02:30.800
So going to say PD dot read on the score see as V.

34
00:02:30.800 --> 00:02:38.760
And here we're going to pass along our Amazon underscore Alexa dot t as V.

35
00:02:38.760 --> 00:02:39.800
Right.

36
00:02:39.860 --> 00:02:45.100
And what we're going to do here we're going to say okay our separator which is the information in the

37
00:02:45.110 --> 00:02:54.080
file is separated by equals 2 backslash t all right.

38
00:02:54.160 --> 00:02:55.380
And we leave all that data.

39
00:02:55.450 --> 00:02:59.500
I'm going to put it into the F underscore Alexa which is gonna be our data frame.

40
00:02:59.530 --> 00:03:03.220
This is simply just treat our data and put it in our data frame the F Alexa let's run.

41
00:03:03.220 --> 00:03:04.510
That looks good.

42
00:03:04.510 --> 00:03:06.550
And let's take a look at the data.

43
00:03:06.580 --> 00:03:11.920
So what I'm going to ask you guys to do is to if you guys remember to use the head methods right and

44
00:03:11.920 --> 00:03:18.910
use the tail as well as a member to your data frame to actually try to go ahead and visualize the first

45
00:03:18.910 --> 00:03:19.620
couple of rows.

46
00:03:19.630 --> 00:03:27.750
And the last couple of rows please go ahead pause the video and I'll see you guys after the challenge.

47
00:03:27.790 --> 00:03:28.330
All right.

48
00:03:28.360 --> 00:03:32.030
I hope you guys were able to figure out the challenge you have done that several times in the past and

49
00:03:32.030 --> 00:03:37.390
if you guys who are maybe not familiar well maybe he went in here right away into that project.

50
00:03:37.450 --> 00:03:44.320
So what we going to do here we're going to say OK the F underscored Alexa which is our data frame dot

51
00:03:44.320 --> 00:03:44.880
head.

52
00:03:44.890 --> 00:03:45.990
That should give us.

53
00:03:46.120 --> 00:03:48.940
Let's actually move this the F underscore Alexa dot head.

54
00:03:48.940 --> 00:03:52.450
That's give us you know the first couple of rows within our data.

55
00:03:53.050 --> 00:03:54.040
So let's take a look at it.

56
00:03:54.040 --> 00:03:59.820
It's actually pretty interesting you would find that we have five columns here.

57
00:03:59.840 --> 00:04:04.060
Here it shows the rating which is what star rating the customer gave.

58
00:04:04.060 --> 00:04:07.590
So maybe five star reviews for start and so on.

59
00:04:07.630 --> 00:04:08.630
He'll chose the date.

60
00:04:08.680 --> 00:04:12.190
So it's actually pretty decent data we have July 2018.

61
00:04:12.280 --> 00:04:19.330
You know like 31 July 3 first and so on he is showing the variation which is charcoal fabric walnut

62
00:04:19.330 --> 00:04:20.410
finish and so on.

63
00:04:20.410 --> 00:04:22.750
So different variations within the product.

64
00:04:22.750 --> 00:04:23.360
OK.

65
00:04:23.380 --> 00:04:24.950
Why that information is important.

66
00:04:24.950 --> 00:04:32.350
Maybe like you know like all bad reviews are associated let's say with the walnut finish maybe so that

67
00:04:32.350 --> 00:04:37.870
we can get that information and that will you know can we feed it back to like let's say the product

68
00:04:37.870 --> 00:04:41.630
strategy to maybe remove the one that finish or maybe make it better.

69
00:04:41.650 --> 00:04:47.320
Or maybe like you know like because that has an impact on the overall product sales and reviews as well.

70
00:04:47.670 --> 00:04:48.670
OK.

71
00:04:48.870 --> 00:04:50.050
It looks good.

72
00:04:50.140 --> 00:04:52.170
And then here these are the verified reviews.

73
00:04:52.180 --> 00:04:55.720
That's actually what the customer wrote like love my eco loved it.

74
00:04:55.750 --> 00:05:01.750
You know like I had a lot of fun with it and so on and here we have our feedback so feedback is either

75
00:05:01.750 --> 00:05:05.440
1 or 0 1 that means positive feedback.

76
00:05:05.440 --> 00:05:06.370
That means two.

77
00:05:06.460 --> 00:05:10.860
That means three four or five stars and zero is bad feedback.

78
00:05:10.870 --> 00:05:13.390
That means one or two star.

79
00:05:13.420 --> 00:05:14.580
All right looks good.

80
00:05:15.220 --> 00:05:21.950
Let's go ahead and actually visualize the last couple of Rose of mystic IDF dot Alexa and you're gonna

81
00:05:21.970 --> 00:05:28.850
say my apologies the F underscored Alexa it's a dark tale that you showed the last couple of rows.

82
00:05:29.020 --> 00:05:29.980
And here we go.

83
00:05:30.040 --> 00:05:33.400
So it seems that we have around 3000 views which would make sense.

84
00:05:33.400 --> 00:05:38.380
And here it seems like there are more variations in here is like black dots that appeared you know which

85
00:05:38.380 --> 00:05:40.180
is we don't know how many variations.

86
00:05:40.300 --> 00:05:44.390
And that's kind of some of the visualization of the data we're gonna be doing.

87
00:05:44.530 --> 00:05:45.350
All right looks great.

88
00:05:46.210 --> 00:05:46.850
OK.

89
00:05:46.930 --> 00:05:53.430
The next step is I'm going to ask you guys to get the keys so going to take IDF underscored Alexa dot

90
00:05:53.460 --> 00:05:59.710
keys and that should give us you know what the columns and here's two choices the rating the date variation

91
00:05:59.710 --> 00:06:01.360
verifies reviews and feedback.

92
00:06:01.360 --> 00:06:01.580
OK.

93
00:06:01.690 --> 00:06:02.160
Looks good.

94
00:06:02.650 --> 00:06:04.340
Let's add a couple of neural new cells.

95
00:06:04.360 --> 00:06:08.520
Again adding cells you press eight to add cells.

96
00:06:08.520 --> 00:06:08.800
All right.

97
00:06:10.450 --> 00:06:15.940
And afterwards what we could do we could say OK maybe you can go ahead and take a look at the reviews.

98
00:06:16.090 --> 00:06:23.650
So you say okay under our data frame which is the F underscore Alexa we're going to grab our verified

99
00:06:23.660 --> 00:06:30.610
reviews verified underscore reviews that should grab us our our column right.

100
00:06:30.610 --> 00:06:32.720
So here we have like love my echo.

101
00:06:32.720 --> 00:06:33.220
Love that.

102
00:06:33.220 --> 00:06:35.330
You know I have a lot of fun with it.

103
00:06:35.350 --> 00:06:36.550
I received you know love it.

104
00:06:36.550 --> 00:06:39.490
I have listened to it seems like the word love you know is repeated right.

105
00:06:39.490 --> 00:06:40.660
Love love love.

106
00:06:40.660 --> 00:06:46.570
So maybe we can find a way to actually using the word count to plot which one is the most kind of repeated

107
00:06:46.570 --> 00:06:47.330
word.

108
00:06:47.610 --> 00:06:49.670
You know and it's really really powerful.

109
00:06:49.660 --> 00:06:50.710
You gonna see how can we do that.

110
00:06:50.880 --> 00:06:52.400
OK.

111
00:06:52.480 --> 00:06:52.780
All right.

112
00:06:52.780 --> 00:06:53.940
Looks good.

113
00:06:54.250 --> 00:07:02.550
Afterwards what we could do we could say OK maybe we can try to use the info method.

114
00:07:02.550 --> 00:07:08.780
We're going to say these f underscored Alexa dot info enter.

115
00:07:08.960 --> 00:07:10.130
And here we go.

116
00:07:10.160 --> 00:07:14.910
Info it gives you kind of a quick summary of OK how many entries were in the ratings who have three

117
00:07:14.930 --> 00:07:17.720
150 and they're all integers 64.

118
00:07:17.750 --> 00:07:21.550
We have the dates you know variations verify the views all of them.

119
00:07:21.550 --> 00:07:23.780
Obviously we have 30 150.

120
00:07:23.840 --> 00:07:26.090
And the good thing is that there are no NOLs which is amazing.

121
00:07:26.090 --> 00:07:31.040
That means there is no like there's no missing information or the information contained on the function

122
00:07:31.040 --> 00:07:31.790
is good.

123
00:07:31.800 --> 00:07:36.500
Then you don't need to go and actually do like feature engineering and you know like add maybe some

124
00:07:36.500 --> 00:07:40.250
new lines and stuff like that the data is pretty much intact that's good.

125
00:07:40.250 --> 00:07:43.650
And here we have the memory usage is around you know 123 terabytes.

126
00:07:43.650 --> 00:07:46.040
It's it's not too bad.

127
00:07:46.040 --> 00:07:47.440
All right.

128
00:07:47.570 --> 00:07:47.860
OK.

129
00:07:47.870 --> 00:07:52.790
What we could do as well who can say OK ADF underscored Alexa dot described.

130
00:07:54.650 --> 00:07:59.680
And here we go describe gives you kind of a quick summary of if all the numbers that are contained so

131
00:07:59.680 --> 00:08:00.910
here we have ratings.

132
00:08:01.070 --> 00:08:02.160
Here we have feedback.

133
00:08:02.240 --> 00:08:02.540
Right.

134
00:08:03.050 --> 00:08:08.560
So the rating shows you how many that count how many elements were in that column ratings we have sitting

135
00:08:08.560 --> 00:08:09.080
on 50.

136
00:08:09.110 --> 00:08:09.970
Right.

137
00:08:09.980 --> 00:08:11.800
The mean is that on four point four six.

138
00:08:11.810 --> 00:08:13.050
That's a good thing.

139
00:08:13.070 --> 00:08:13.370
OK.

140
00:08:13.370 --> 00:08:14.060
That's out of five.

141
00:08:14.060 --> 00:08:15.760
That's great feedback.

142
00:08:15.810 --> 00:08:19.830
You know as our point 9 1 again the feedback is either 0 or 1.

143
00:08:19.850 --> 00:08:24.530
If you guys take a look at it here you'll find the minimum value for the feedback was zero maximum value

144
00:08:24.530 --> 00:08:31.370
is 1 for the reviews its minimum value was 1 maximum value was 5 and the standard deviation which is

145
00:08:31.370 --> 00:08:34.070
the dispersion from the mean is around 1.

146
00:08:34.160 --> 00:08:39.050
Here the feedback that dispersion from the mean is that on point 2 and this is kind of 25 percentile

147
00:08:39.050 --> 00:08:40.160
50 at 75 percent.

148
00:08:40.160 --> 00:08:41.190
Don't worry about it.

149
00:08:41.270 --> 00:08:45.170
The overall idea is the mean here is four point four six.

150
00:08:45.180 --> 00:08:49.840
And that's kind of the most important information along with the min max values.

151
00:08:49.880 --> 00:08:50.560
All right.

152
00:08:50.570 --> 00:08:52.760
And that's all what I have for this lecture.

153
00:08:52.760 --> 00:08:57.950
So at this lectures what we have done is that we went through the data overview and problem statement

154
00:08:58.370 --> 00:09:02.280
and then we what we did that we imported our libraries here.

155
00:09:02.450 --> 00:09:08.370
We imported our data and we took a look at a couple of rows took a couple of rows at the end as well.

156
00:09:08.510 --> 00:09:15.830
We check a couple of reviews and then we're able as well to get some info and using the info and describe

157
00:09:15.830 --> 00:09:19.160
as well to get some information regarding our data frame.

158
00:09:19.160 --> 00:09:21.730
And that's pretty much all what I have for this lecture.

159
00:09:21.740 --> 00:09:23.110
I hope you guys enjoyed it.

160
00:09:23.170 --> 00:09:27.290
And the next lectures we're going to go ahead and visualize some data going to get way more interesting.

161
00:09:27.650 --> 00:09:29.630
And I'll see you guys in the next lecture.
