WEBVTT

00:00.580 --> 00:03.880
Hello and welcome back to the course on artificial intelligence.

00:03.880 --> 00:09.340
And finally we're on to the fun stuff we're on to deep learning.

00:09.370 --> 00:10.620
All right so let's have a look.

00:10.700 --> 00:14.060
Bruce we spoke about killer earning and what it's all about.

00:14.080 --> 00:20.120
And we learned about age and the environment and how the agent will look at the state.

00:20.160 --> 00:23.600
Or she is in take an action get a reward.

00:23.620 --> 00:28.570
Enter into a new state and based on that feedback loop they will continue taking actions and they will

00:28.570 --> 00:29.390
learn from that.

00:29.410 --> 00:32.230
Understand what are the best actions to take.

00:32.230 --> 00:34.930
And so we looked at this basic example of a maze.

00:34.990 --> 00:40.500
We understood that as Asian explorers environment understands what the values of the states are.

00:40.510 --> 00:45.100
Then we moved on from dealing with the values of the states to dealing with the values of the actions

00:45.100 --> 00:52.180
with the q values and then A-Basin that understood how plans in a non sarcastic environments work and

00:52.510 --> 00:57.000
how policies work in stochastic environments and this is an example of a policy.

00:57.040 --> 01:01.250
So that's a quick recap of everything we've discussed in the basic learning.

01:01.400 --> 01:07.180
And now let's have a look at how this can be taken to the next level through deep learning through adding

01:07.180 --> 01:08.030
deep learning.

01:08.210 --> 01:08.480
OK.

01:08.500 --> 01:16.060
So this is our environment and what we're going to do now is we're going to add instead of just doing

01:16.060 --> 01:21.800
basic calculations in this matrix that we have which is pretty simple.

01:21.820 --> 01:26.920
What we're going to do is we're going to add two axes which adds an x and y axis or we'll call them

01:27.020 --> 01:28.430
x1 and x2.

01:28.510 --> 01:30.380
Just to make things even more general.

01:30.430 --> 01:36.660
And here we've got the real number the columns 1 2 3 4 he'll share rule number the rows 1 to 3.

01:36.910 --> 01:44.680
And so now every single state can be described by a pair of two values x1 and x2 so any one of these

01:44.680 --> 01:50.890
squares in which the agent can possibly be in can be described by x1 x2.

01:50.890 --> 01:58.290
So for instance right now he's in the square with x y and equal to 1 and x 2 equal to 2.

01:58.420 --> 02:03.380
And therefore that's not some way we can escape in your square meaning we can describe any state.

02:03.400 --> 02:08.280
Then of course this is a very simplified version of an environment of describing States.

02:08.290 --> 02:10.240
But nevertheless it works in this case.

02:10.240 --> 02:17.210
And that means that now we can feed this these states into a neural network.

02:17.350 --> 02:21.790
And by the way here I would just like to mention that at the end of the course of good annexes we've

02:21.790 --> 02:27.190
got an x number one and attics and but two in order to proceed successfully with this section is highly

02:27.190 --> 02:32.230
advisable that you check out unaccessible one which is on artificial neural network so you understand

02:32.230 --> 02:37.390
how they work so that we can we don't have to delve into that here and we can just use the benefits

02:37.390 --> 02:43.750
of the knowledge of how artificial neural networks work and so we feed this information on the state

02:43.780 --> 02:51.820
into a neural network and then it will process this information the X1 and x2 depending on the structure

02:51.820 --> 02:55.330
of the neural network it might have multiple hidden layers and so on.

02:55.330 --> 03:00.940
So that's something that you'll figure out in practical tutorials but at the end we will structure in

03:00.940 --> 03:06.510
such a way that it spits out for values and these four values are actually going to be arke you value

03:06.520 --> 03:11.320
so the values which dictate which action we need to take and further down in this tutorial we'll see

03:11.320 --> 03:15.170
exactly how these key values are used to decide which action is taken.

03:15.190 --> 03:22.460
But the main point here is that we no longer look at just this maze from a real learning perspective.

03:22.600 --> 03:29.710
We're now taking the states of the maze and we're feeding them into a deep neural network in order to

03:29.770 --> 03:31.150
get these cubicles.

03:31.290 --> 03:35.020
And at the end of the day we're still going to come up with an action we're still going to understand

03:35.080 --> 03:39.850
what action we need to take and we'll discuss all this in more detail but the question right now is

03:39.850 --> 03:42.960
why why are we doing all of this why we called it.

03:43.150 --> 03:47.960
Why are making things so much more complicated when that initial approach of learning was working already

03:48.280 --> 03:49.010
well.

03:49.120 --> 03:54.940
The reason for that is the Gule learning was working in this very simplistic environment and we're continuing

03:54.940 --> 03:59.800
to deal for now with this very simplistic environment in order to better understand the concepts.

03:59.950 --> 04:06.790
But at the same time that simple key learning will no longer work in more complex environments and we're

04:06.790 --> 04:13.420
talking about for instance the self-driving cars which will be creating or playing Doom when the artificial

04:13.420 --> 04:21.190
intelligence is playing do or other Atari games like breakout or even self-driving cars and more advanced

04:21.580 --> 04:27.190
reinforcement learning things such as like robots walking around and performing actions in all of those

04:27.190 --> 04:34.000
cases basically learning is insufficient is not strong is not powerful enough to be able to master those

04:34.000 --> 04:34.650
challenges.

04:34.660 --> 04:41.200
And just like we've seen in the deep learning course if you've been in our discipline or if you've done

04:41.200 --> 04:47.770
the annex sections on x number one and X-2 you will where we know that deep learning is by far superior

04:47.770 --> 04:51.580
to any type of machine learning let alone simple CULE learning.

04:51.610 --> 04:55.750
And that's why we're leveraging the power of deep learning here so we're feeding in the information

04:55.750 --> 04:58.500
about the environment as a vector of values.

04:58.510 --> 05:04.170
In this case just to use into a deep neural network and then we're using that to perform the actions

05:04.170 --> 05:07.360
that we want to decide which actions are agents going to take.

05:07.380 --> 05:11.650
So that's kind of like a high level overview of why we're doing this.

05:11.800 --> 05:17.880
And now let's have a look at it a bit more detail what happens to the concepts of cool learning when

05:17.880 --> 05:24.050
we transfer when we make the transformation from or transition from simple learning into deep Killary.

05:24.090 --> 05:31.680
So as you saw in the previous intuition tutorials we had a slide like this which is the foundation of

05:31.920 --> 05:33.530
temporal difference learning.

05:33.660 --> 05:37.380
This is the formula for temporal difference and basically So let's go through.

05:37.380 --> 05:45.930
So basically we had an agent who was in this state over here which is indicated the blue arrow and we

05:45.930 --> 05:51.720
were understanding how temporal difference works for this q value of for instance going up.

05:51.720 --> 05:57.180
And so what we saw here was before this is in the simple Killary not the deep learning this is the simple

05:57.210 --> 06:05.040
Killary what we saw was before the agent had a subset queek hue value that he had learned about this

06:05.040 --> 06:06.200
action of going up.

06:06.270 --> 06:08.640
And so then he decided to take ception to go up.

06:08.820 --> 06:14.550
And right after he takes his action he gets a reward for taking this action in this state.

06:14.760 --> 06:21.030
And that is that reward plus now he can evaluate the value of the current state he's in which is the

06:21.030 --> 06:27.990
maximum of all of the new q values of all of the cube of the new action he can take a prime in the new

06:27.990 --> 06:32.380
state as print and read multiplied by the DK factor of gamma.

06:32.400 --> 06:41.340
So that is essentially the Q The new q value or kind of like the the empirical Q value that he has just

06:41.340 --> 06:43.130
received for taking that action.

06:43.230 --> 06:45.570
And ideally these two two should be the same.

06:45.570 --> 06:51.390
So the actually the Q value that he had in his memory about this action in this state should equate

06:51.390 --> 06:57.380
to the actual reward Plus the gamma times the value of the state that he ended up in.

06:57.570 --> 07:01.800
And therefore that's how we calculate the temporal difference we take what you are after minus what

07:01.800 --> 07:05.150
he got what he had in mind what he was expecting.

07:05.170 --> 07:06.640
You subtract one from the other.

07:06.720 --> 07:07.620
That's a temporal difference.

07:07.620 --> 07:12.710
And then you use your learning rate Alpha to adjust your q value.

07:12.730 --> 07:17.000
Your your new q value by the temporal difference but with a coefficient of Alpha.

07:17.070 --> 07:20.430
So that is the essence of the simple Q learning.

07:20.420 --> 07:25.940
Now let's have a look at how it changes in deep Killary and so is still going to work with the slide

07:25.950 --> 07:29.380
but we're going to see exactly what's happening.

07:29.550 --> 07:35.850
So in a deep learning the neural network will predict for Valis as we saw in the previous line is we'll

07:35.850 --> 07:41.700
see further down as to journal the neural network will predict for values or it might predict more values

07:41.700 --> 07:44.730
of more possible actions in a given state.

07:44.730 --> 07:49.320
But in this case when we know that there's only four actions upright left to done and so the neural

07:49.320 --> 07:56.860
network will predict four of these values so there will be no in in a deep kill or any situation is

07:56.860 --> 07:58.750
important to understand there's no before or after.

07:58.920 --> 08:01.680
And this is how we'll we'll get to know this a bit better.

08:01.680 --> 08:08.700
So the neural net will predict four of these values and it will compare not to what will happen after

08:08.960 --> 08:15.630
but the neural network will compare to this exact value but it was this value which was calculated in

08:15.630 --> 08:17.700
the previous step.

08:17.700 --> 08:24.810
So in the previous time when the agent was in this exact square so let's say I don't know.

08:24.990 --> 08:34.340
Some time ago the agent was again was in this exact square as well and it calculated this value previously.

08:34.350 --> 08:36.130
So in the previous time.

08:36.140 --> 08:38.640
Long time ago the agent calculated this value.

08:38.640 --> 08:41.970
Then the agent stored this value for the future.

08:41.970 --> 08:43.650
And now the future has come.

08:43.650 --> 08:48.750
So now he's in the square again and now he's got these Q values which is predicted and one of them is

08:48.750 --> 08:50.470
for the four going up.

08:50.640 --> 08:57.180
So now what he's going to do is going to compare the predicted value of Q to this value which he had

08:57.180 --> 09:02.430
recorded from the previous step and we'll understand exactly why this is important right now so just

09:02.490 --> 09:03.400
important on the set here.

09:03.430 --> 09:10.110
There is no before an officer in this specific square the specific time we're taking the Q value that

09:10.110 --> 09:17.970
he's predicted using the neural network this time and we comparing it to this value which he had from

09:18.270 --> 09:23.880
the previous time from the previous time he was in the square assessing all of the situation and you

09:23.880 --> 09:28.200
know like the previous time he actually performed this action.

09:28.200 --> 09:29.220
So there we go.

09:29.220 --> 09:33.320
Now let's have a look at how this all works out in the neural network and why.

09:33.330 --> 09:35.050
Why is it like.

09:35.070 --> 09:39.060
I know it sounds a bit complicated right now but we'll break it down into simple terms right.

09:39.270 --> 09:39.960
Just in a second.

09:39.960 --> 09:44.340
So this on your own network we're fitting in the states of the environment into the neural network is

09:44.340 --> 09:48.830
going through the hidden layers then it's coming out with these outputs Q1 Q2 Q3 Q4.

09:48.840 --> 09:57.360
In that specific state these are the q values that the neural network is predicting for possible actions.

09:57.360 --> 09:58.250
Those are the cubicles.

09:58.350 --> 10:04.210
So then we're comparing to target and these targets exist exactly so if we go back here this is the

10:04.210 --> 10:09.910
target so this is the value that was predicted and then but also we know that we have a target from

10:09.910 --> 10:11.660
the last time we were in the square.

10:11.770 --> 10:16.580
We have a target for this same action which is up for instance.

10:16.600 --> 10:21.430
So here we've got a target and we're going to compare we compare Q1 versus that target we're comparing

10:21.430 --> 10:28.340
Q2 versus that target the target that we had from previously Q3 versus a target Q4 versus the target.

10:28.360 --> 10:36.550
And so this is the part where the neural network or the agent is now learning through deep learning

10:36.550 --> 10:42.670
how to better go through Smit's and the key point here is that we're still applying cool learning but

10:42.910 --> 10:48.280
the concepts insist in simple learning you learn through temporal differences which are pretty straightforward

10:48.310 --> 10:50.650
which we've already discussed and we know quite well why now.

10:50.860 --> 10:56.030
But at the same time in deep learning how do neural networks learn neural networks learn through our

10:56.030 --> 10:56.940
adjusting the weights.

10:56.950 --> 11:07.060
So we have to adapt the concepts of reinforced the concepts of simple kill learning to the way neural

11:07.060 --> 11:10.900
networks actually work and that is through updating their weights.

11:10.900 --> 11:14.890
And so this is what we're trying to figure out here how do we adapt that concept of temporal difference

11:15.370 --> 11:21.160
to your own network so that we can leverage the full power of neural networks.

11:21.200 --> 11:27.730
So far we've got this so we enter our environment state here as a vector goes through a neural network

11:27.730 --> 11:33.170
we get predictions of q values and then from the previous time the agent was in that state.

11:33.190 --> 11:39.410
We have these cue target to target one two three and four for each of these respective actions.

11:39.430 --> 11:40.810
And so now we're up to.

11:40.810 --> 11:43.270
OK let's compare each one with each one.

11:43.550 --> 11:50.440
And from here it's it becomes pretty straightforward if you're up to speed with neural networks.

11:50.440 --> 11:53.190
Once again that's on a an X number one.

11:53.290 --> 12:00.340
We're going to calculate a loss which is Al here and we're going to be cute target this one minus Q

12:00.370 --> 12:01.630
minus this one.

12:01.780 --> 12:06.100
We're going to square that so the square difference of the of each one of these and we're going to sum

12:06.100 --> 12:06.700
them.

12:06.760 --> 12:12.250
So we take the sum of the squared differences of these values and their targets and we're going to send

12:12.250 --> 12:13.930
them up and that's going to be a loss.

12:13.960 --> 12:19.720
And so ideally just as we had into the temporal difference learning so if we go back for a second remember

12:19.720 --> 12:25.590
we said Ideally we want this to be equal to this so we want the temporal difference to be zero so that's

12:25.720 --> 12:32.410
that means basically the agent is predicting exactly correctly what you know the Q value is that the

12:32.410 --> 12:38.830
agent is predicting our Exactly or that he has a memory are exactly descriptive of the environment and

12:38.830 --> 12:44.920
therefore the agent can never get environed pretty well right there's no surprises there's no there's

12:44.920 --> 12:50.440
no answer as long as this temporal difference is highly positive or highly negative then then we've

12:50.440 --> 12:51.290
got some surprises.

12:51.300 --> 12:55.630
But if the general differences zero then he knows environment so well that he can predict what's going

12:55.630 --> 13:01.150
on and he can and therefore his policy is going to be very good and he's going to be able to navigate.

13:01.290 --> 13:02.100
So here.

13:02.140 --> 13:07.470
Same thing so we want this last to be as close to zero supposed as smallest possible.

13:07.660 --> 13:14.620
And that's why now we're going to this is the part where we are going to leverage the real true power

13:14.650 --> 13:20.770
of neural network so we could take this loss and we're going to use back propagation or stochastic gradient

13:20.770 --> 13:27.220
descent to take this loss and pass it through the network pass it back or back propagated through a

13:27.220 --> 13:33.130
network and through stochastic great and decent a date the weights of these synopses in the network

13:33.460 --> 13:39.940
so that next time we go through this network the way it's already a bit better descriptive of the environment

13:39.940 --> 13:40.990
and that's exactly hardware.

13:41.020 --> 13:48.020
So here you have if we go back this is calculated losses Kalka and guess prove propagator for the network

13:48.040 --> 13:49.270
the weights are updated.

13:49.270 --> 13:55.660
Then the next time we get here this happens again and again here this happens again and so on and so

13:55.720 --> 14:02.350
and it keeps happening and that's how this agent learns or basically now it's the neural network which

14:02.350 --> 14:09.820
is the brain of the agent is learning is becoming more and more descriptive of the environment and therefore

14:09.820 --> 14:12.010
the agent is able to navigate the environment.

14:12.060 --> 14:17.920
When we say descriptive environment basically means that when we put in the states of the environment

14:17.920 --> 14:24.900
that the agent is in we are more likely to get closer and closer to the actual values.

14:24.940 --> 14:30.310
And that happens because the acute values that we want to find the right action and that happens because

14:30.430 --> 14:36.850
these new targets are actually empirically derived so he every day how does he find these cute targets.

14:36.850 --> 14:40.050
That's that's actually this so he actually observes.

14:40.060 --> 14:42.840
OK so once I do take this step what's the reward I get.

14:43.000 --> 14:44.950
And then what's the values of this state.

14:45.010 --> 14:48.700
So same thing as we saw previously in Q learning and the simple learning intuition.

14:48.790 --> 14:51.240
So he learns this through trial and error.

14:51.460 --> 14:57.910
And then he constructs his network or that's the way it is in such a way that the predicted cube values

14:58.180 --> 15:01.270
are close and close up consummating that target.

15:01.290 --> 15:07.310
Q values so very similar to the concept we discussed here in the simple temporal difference learning

15:07.370 --> 15:09.800
of the simple learning algorithm.

15:09.860 --> 15:10.400
So there we go.

15:10.400 --> 15:12.490
That's that's how the agent learns.

15:12.500 --> 15:13.920
So we're up to here.

15:14.220 --> 15:15.300
And that's the learning part.
