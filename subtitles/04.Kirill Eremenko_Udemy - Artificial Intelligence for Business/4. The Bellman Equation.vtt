WEBVTT

00:00.980 --> 00:04.340
Hello and welcome back to the course on artificial intelligence.

00:04.400 --> 00:07.550
Today we're going to talk about the Belmont equation.

00:07.550 --> 00:12.530
It's quite a complex topic and we're going to introduce it in a step by step manner throughout this

00:12.530 --> 00:17.480
whole section of the course so I'm going to just jump straight into the most complex origin of the Belman

00:17.480 --> 00:18.110
equation right away.

00:18.110 --> 00:23.190
But instead we're going to introduce it slowly in order to gradually understand how it works.

00:23.360 --> 00:28.430
And I hope you're cool with that approach if you're G.R. Let's get straight into it.

00:28.640 --> 00:33.770
So we're going to have a couple of key concepts that we're going to be operating with and these concepts

00:33.770 --> 00:34.390
are.

00:34.550 --> 00:41.060
S stands for states so the state in which our agent is or any other possible state in which it can be

00:41.690 --> 00:45.430
a represents an action that a an agent can take.

00:45.440 --> 00:50.630
So an agent can have access to a certain list of actions and actions are very important when they're

00:50.630 --> 00:53.570
looked at in a state combination.

00:53.570 --> 00:57.830
So when you're in a certain state and then you look at actions and it starts to make sense what's going

00:57.830 --> 01:01.820
to be the result of those actions because you'll like an action by itself or a state doesn't really

01:01.820 --> 01:05.630
make sense because you don't know where you are and where you can possibly end up.

01:06.440 --> 01:12.560
Then we have we'll have our Which stands for reward and that's through Ward the agent gets for entering

01:12.560 --> 01:14.180
into a certain state.

01:14.290 --> 01:16.930
And gamma is the discount factor.

01:16.940 --> 01:21.430
And we'll talk about the discount factor in a second all make sense just now but they're just taking

01:21.430 --> 01:21.770
notes.

01:21.770 --> 01:26.520
Make a mental note that we are going to have this letter Gamelin that will be operating with later on.

01:26.570 --> 01:31.190
So the person behind the bellman equation is Richard Ernest bellman.

01:31.310 --> 01:39.350
He was a flight mathematician and came up with the concepts of dynamic programming which we're now which

01:39.350 --> 01:43.660
we now call reinforcement learning or which we call the Belman equation now.

01:44.010 --> 01:45.440
Well that's what we're called now.

01:45.440 --> 01:52.300
And in 1953 he came up with that concept and that's when the Belmont Belman equation came to me.

01:52.580 --> 01:56.490
So let's have a look at how this all works.

01:56.490 --> 02:02.360
There's our lovely agent in the bottom left corner and he is in a maze and this is quite a classical

02:02.450 --> 02:08.330
maze where you've got some blocks the white blocks are blocked in which the agent can step into the

02:08.330 --> 02:13.770
gray block is the one one that is just not accessible sets like a wall in this maze.

02:13.850 --> 02:20.100
The green is where the agent is should be aiming to end up in that's where we want the agent to go that's

02:20.100 --> 02:20.850
the finish.

02:21.140 --> 02:24.940
And the red is firepits or the engine falls into the fire pit.

02:25.010 --> 02:26.600
He will lose the game.

02:26.900 --> 02:31.280
So in the fire pit the reward which is R is minus 1.

02:31.280 --> 02:37.520
So that's our way of telling the agent that's not something we want you to do like remember in the example

02:37.520 --> 02:42.110
of when we're training dogs we want to tell them like bad dog if it's not doing the right thing that

02:42.110 --> 02:43.290
he wanted to do same thing here.

02:43.300 --> 02:43.660
We.

02:43.670 --> 02:47.930
Tell the agent that this is not something that you should be doing you shouldn't be ending up in the

02:47.930 --> 02:52.250
square so every time it doesn't happen the squirrel get a minus one reward so you'll be punished with

02:52.260 --> 02:53.480
minus one reward.

02:53.480 --> 02:57.560
On the other hand if it ends up in the Green Square it will get a plus one reward meaning that that

02:57.560 --> 02:59.280
is what we wanted to do.

02:59.540 --> 03:02.210
So those are the two rewards that the agent can't possibly get.

03:02.420 --> 03:06.160
And how does it learn how to operate in this maze.

03:06.320 --> 03:10.700
Just like in that example of the robot dogs that learned to walk we just going to let it know it will

03:10.700 --> 03:12.430
just tell it that here the action you can do.

03:12.440 --> 03:18.470
You can go up right left or down those are four possible actions that you can take and that's it.

03:18.520 --> 03:19.700
Have a play around with that.

03:19.700 --> 03:21.380
See what you can come up with.

03:21.380 --> 03:26.240
So the agent might go to the right then they might go two more to the right they might go back to the

03:26.240 --> 03:30.670
left they're just randomly pressing the button and they're trying to see what happens and they go back

03:30.670 --> 03:31.060
here.

03:31.110 --> 03:34.580
They go up go up go down go up go right.

03:34.580 --> 03:38.380
So for now they haven't learned anything they just so far nothing has happened.

03:38.390 --> 03:41.750
They go right and then bam they end up in the Green Square.

03:41.780 --> 03:48.080
So they realize wow I just got a plus one awar So as soon as I stepped into the Green Square they got

03:48.080 --> 03:48.990
a plus one reward.

03:49.040 --> 03:53.600
And that triggers the algorithm to say OK that's really cool.

03:53.780 --> 03:58.870
I am rewarded for ending up in the square so I want to end up in the square.

03:58.880 --> 04:00.580
So what does that mean for the agent.

04:00.860 --> 04:04.260
That means it starts to ask the question how did I get to this square.

04:04.260 --> 04:10.640
What was the preceding state I was in and what action that I take to get to square and then looks back

04:10.640 --> 04:14.900
and says OK so the preceding stage was this one.

04:14.900 --> 04:17.340
It turns out to be valuable in that state.

04:17.360 --> 04:19.170
The one that's part of the Red Arrow.

04:19.190 --> 04:26.180
Because from that state you're I'm I'm just one step away from getting the maximum reward I can possibly

04:26.180 --> 04:30.220
dream of of plus one like a biscuit for a dog.

04:30.440 --> 04:33.160
As soon as I know if I ever am in that state.

04:33.200 --> 04:35.090
That square marked with the Red Arrow.

04:35.150 --> 04:36.690
All I have to do is press right.

04:36.980 --> 04:44.000
So how do I tell myself to remember that that state is valuable Well to me there's no difference actually

04:44.480 --> 04:45.080
as the agent.

04:45.080 --> 04:50.300
There's no difference in whether I am in the Green Square or in the white square right in the Green

04:50.300 --> 04:50.560
Square.

04:50.560 --> 04:51.550
I get the reward of one.

04:51.560 --> 04:58.760
So I'm going to mark for myself that the Y Square is got for me it has the value of one because it leads

04:58.760 --> 05:03.260
exactly to reward one so soon as I'm in the White square I know I'll just take one more action.

05:03.320 --> 05:05.350
I'll be in the Green Square and I'll get a reward of one.

05:05.350 --> 05:11.290
So that's why I'm going to say that the value of this square is equal to one because it leads directly

05:11.290 --> 05:16.490
to if on any sort of subtractions as soon as I mean here I know my reward will be one so I'm going to

05:16.490 --> 05:22.100
mark this square as the call to one that's the value that's the perceived value of being in the state.

05:22.370 --> 05:24.710
Next the agent's going to be OK.

05:24.770 --> 05:26.860
So how do I get into this square.

05:26.990 --> 05:29.930
And you know he might walk around again and so on.

05:29.930 --> 05:33.580
And up in the square again and be like OK how did I get into this square before that.

05:33.740 --> 05:36.790
And the way I got into this square was from this square.

05:36.800 --> 05:37.470
Interesting.

05:37.490 --> 05:37.780
OK.

05:37.790 --> 05:40.180
So essentially get into this square.

05:40.220 --> 05:42.890
I know that all I have to do is go right.

05:42.950 --> 05:45.590
And then from here I already know that I'm going to win.

05:45.590 --> 05:49.910
I know exactly how everything is going to unravel from here and I know the value of being in this state

05:49.910 --> 05:50.910
is equal to one.

05:50.990 --> 05:58.280
And since there's no nothing is stopping me from going from here to here the value in this is going

05:58.280 --> 06:04.540
to a perceived value I'm good value being in here as equal to one as well because I mean here I know

06:04.580 --> 06:06.640
be here and I'll be here pretty quickly.

06:06.710 --> 06:10.380
So I'm going to win and then I get into this square before that.

06:10.470 --> 06:12.890
Well I got into the square from this square.

06:13.040 --> 06:19.160
So the value is similar approach the value of being here is also equal to one and so on.

06:19.160 --> 06:23.090
So the value of being here is equal to one or the value of being here is equal to one because each one

06:23.090 --> 06:26.070
of them leads to the next one and to the finish line.

06:26.180 --> 06:29.800
So that's all like pretty logical at this stage.

06:29.900 --> 06:36.380
This is us pretty much designing the Belman equation right now so this is we could possibly think about

06:36.380 --> 06:40.420
designing an equation that helps an agent go through the maze.

06:40.460 --> 06:45.830
So look at the reward then the preceding state give it a value of equal to reward the proceedings and

06:45.910 --> 06:51.890
Salzar kind of like creates this pathway is all great and well but the problem here is OK what happens

06:51.950 --> 06:59.170
if our agent for some reason starts in this state is of starting here and taking these actions and it

06:59.240 --> 07:00.530
actually starts in the state.

07:00.590 --> 07:06.920
How does it know how does it remember which action to take should it go right or should it go down or

07:06.920 --> 07:08.480
should maybe go left or should it go up.

07:08.480 --> 07:12.850
How does it remember which is the next continuation from here.

07:13.160 --> 07:18.580
If the only values it has is these values are the ones that kind of count and see what's further away

07:18.590 --> 07:19.630
it can only see.

07:19.640 --> 07:19.950
All right.

07:19.970 --> 07:21.870
What I have here and what I have here.

07:21.920 --> 07:23.470
How does it know which way to go.

07:23.600 --> 07:24.820
Well at this stage it doesn't.

07:24.850 --> 07:27.780
It's pretty identical for the age and which way to go.

07:27.880 --> 07:30.720
And so that's why this approach doesn't really work.

07:30.750 --> 07:32.860
It's a very simplistic explanation.

07:32.870 --> 07:38.600
Of course there's much more to it but in an intuitive way that's why we cannot just assign just carry

07:38.600 --> 07:40.510
on this value backwards like that.

07:40.730 --> 07:46.140
Because one of the reasons is once Agent is in between these two values whereas you'd and go.

07:46.160 --> 07:46.640
It doesn't.

07:46.640 --> 07:48.470
It can get confused like that.

07:48.560 --> 07:50.980
And so how do we solve this problem.

07:50.980 --> 07:52.270
What are we going to do.

07:52.340 --> 07:57.800
And this is where we're going to start introducing the Belman equation in its actual form slowly step

07:57.800 --> 07:58.580
by step.

07:58.610 --> 08:01.570
So the bellmen equation looks something like this.

08:01.580 --> 08:06.830
So we've already talked about the the value of being in a certain state as is your current state or

08:06.830 --> 08:10.220
any given state and there is as well.

08:10.340 --> 08:17.230
And as Prime is the state the following state the state that you will end up in after the state and

08:17.230 --> 08:18.870
Beiteinu cancer in action.

08:19.000 --> 08:24.200
But we know that there's many actions and a agent can take and that's why we've got this Max over here.

08:24.200 --> 08:27.170
So by taking an action what will happen to an agent.

08:27.170 --> 08:32.560
So does say we're in state as by taking an action in state assets and we take action.

08:32.560 --> 08:36.610
A what will happen is will instantly get a reward by getting into a new state.

08:36.710 --> 08:41.900
And remember that reward can be 1 or plus one or minus one if it's at the end of the game or it can

08:41.900 --> 08:43.580
be a zero if it's throughout the game.

08:43.580 --> 08:46.170
In this case our reward throughout the game is zero.

08:46.220 --> 08:55.110
So that's the reward Plus we will get into a new state which has value of s prime.

08:55.130 --> 08:57.770
So that's the value of the new state and gamma.

08:57.770 --> 08:58.760
We'll talk about them in a second.

08:58.760 --> 09:03.500
But the point I'm trying to raise here or the point I'm raising here is that we've got many different

09:03.500 --> 09:05.750
actions that we can take and that's why we've got the maximum.

09:05.750 --> 09:09.620
So by taking action we get a reward Plus we end up in a new state.

09:09.680 --> 09:14.600
And so for every move out of the in our case before our possible actions for every one of the possible

09:14.600 --> 09:17.780
4 actions we're going to have a equation like this.

09:17.780 --> 09:22.940
So this is going to have a value for they will have a different value for every one of the four actions

09:23.450 --> 09:28.700
and we're going to look at only the maximum because of course the agent wants to take the optimal state.

09:28.700 --> 09:33.830
So if he's in state s he's going to look at these values he's going to find the maximum based on the

09:33.830 --> 09:37.300
action I'm going to take that action that needs the maximum of these values.

09:37.580 --> 09:41.580
So hopefully that makes sense why we're taking the maximum here.

09:41.600 --> 09:45.620
Then once we get the reward and the value goes said why do we have this Gabaa première here.

09:45.620 --> 09:52.160
Well it's there exactly to solve that problem of where the agent doesn't know which way to go because

09:52.160 --> 09:56.530
it cannot is comparing the values of two states on the on both sides and they're the same.

09:56.750 --> 10:00.830
That's why gamblers called the discounting factor so we're going to have a look at that and it better

10:00.830 --> 10:02.020
understand.

10:02.030 --> 10:04.590
So let's take a formal put it here on the top right.

10:04.700 --> 10:09.030
And now we will analyze what the values of the different states are.

10:09.080 --> 10:11.760
And every state here is a square nomy.

10:11.780 --> 10:16.640
So one of these any one of these white squares is a state and we will going to calculate the value of

10:16.640 --> 10:18.200
being in that state.

10:18.230 --> 10:19.710
So let's start with the square.

10:19.730 --> 10:21.560
What is the value of being in this state.

10:21.800 --> 10:25.760
Well we need to take the maximum of this value across all actions.

10:26.090 --> 10:31.430
And we know that this value represents is maximized as we get closer to the finish line and that's how

10:31.430 --> 10:37.040
it's constructed and just by looking at you can see because here got the reward and here's got a discounting

10:37.040 --> 10:40.840
factor multiplied by the value of the next state.

10:41.000 --> 10:46.640
And it just makes sense that that's how we would construct that equation so it makes sense that from

10:46.640 --> 10:50.280
here the maximum of this value will be if we move to the right.

10:50.300 --> 10:56.060
So that's how we calculate the values that this value of this state is he calls the maximum or equals

10:56.240 --> 10:57.420
to this value.

10:57.440 --> 11:00.940
If we move to the right if we take an action of moving to the right.

11:00.980 --> 11:02.270
So what will this value be.

11:02.300 --> 11:04.780
Well the reward of moving to the right is equal to 1.

11:05.000 --> 11:10.460
And the result is what Kumba gamma is we don't have a value in the state because we are already in the

11:10.460 --> 11:11.660
best state possible.

11:11.660 --> 11:12.820
So this is the final state.

11:12.830 --> 11:16.220
It won't have a value we just get a reward here and that's the end of the game.

11:16.220 --> 11:20.300
So the value will be of this maximum will be equal to one.

11:20.450 --> 11:23.790
And that's why value of state as here is equal to 1.

11:23.810 --> 11:27.760
Now things get interesting when we move to the left when we move backwards a bit.

11:27.950 --> 11:34.020
So now calculate the value of this of being in this state and for that we're going to need Gabaa.

11:34.010 --> 11:39.890
So let's say our discounting factor is a zero point nine and it makes sense what a discounting factor

11:39.890 --> 11:40.910
is once we calculate that.

11:40.910 --> 11:47.370
So from here just based on our intuition and based because we know how this is working how this works.

11:47.390 --> 11:51.470
We know that the best possible action is go to the right because from here we go here.

11:51.500 --> 11:56.060
So that means the maximum will be achieved when in this state you go to the right.

11:56.210 --> 11:58.940
And so let's see what happens if we plug it in here.

11:58.940 --> 12:02.570
So if you go from here to here you don't get in your Wardle's will be zero.

12:02.690 --> 12:07.570
But then you'll get Gamma's who get 0.9 times the value of the new state which is one.

12:07.580 --> 12:13.970
So in this case the value the whole result of this is 1 times or a 0.9 times one equals 2.2.

12:13.970 --> 12:15.860
So that's all values are pointed.

12:16.190 --> 12:20.990
So if we calculate this now you'll see that from here we know just by looking at the maze we know because

12:21.140 --> 12:27.230
we as humans because we're understanding how this equation works of course and either agent would have

12:27.230 --> 12:32.120
to experiment with these things but because we have like a crystal ball we can see this whole maze.

12:32.120 --> 12:33.800
We have like the bird's eye view right now.

12:33.800 --> 12:36.230
We know that the best action is go to the go to the right.

12:36.260 --> 12:42.170
So if we plug it all in here it will be zero no reward Plus the report nine times the value in the state

12:42.170 --> 12:45.480
0.9 is zero point eighty one and so on.

12:45.480 --> 12:50.140
So here will be 0.72 three and he'll be 0.66.

12:50.390 --> 12:57.530
So you can see that the way the discounted factor works is it discounts the value of the state as you

12:57.530 --> 12:58.550
are further away.

12:58.550 --> 13:05.750
So if you are familiar with finance theory then it's something similar to time value of money like what

13:05.750 --> 13:13.550
would you think about it this way What would you prefer to have $5 today or $5 in 10 days from now if

13:13.550 --> 13:18.200
somebody was to give you a choice I will give you $5 a day all you $5 10 days from now.

13:18.310 --> 13:20.240
Well of course you would choose $5 today.

13:20.240 --> 13:20.810
Why is that.

13:20.840 --> 13:26.690
Well because you can take us $5 and you can invest them at a certain interest rate which is very similar

13:26.690 --> 13:32.900
to Gabaa and your $5 in 10 days will actually grow into maybe five dollars and seventy three cents or

13:32.900 --> 13:33.890
something like that.

13:34.010 --> 13:36.360
And that's how time value money works.

13:36.360 --> 13:38.240
And very similar concept here.

13:38.240 --> 13:43.200
And the important thing to understand here this is just a theory a way that reinforcement learning.

13:43.200 --> 13:48.930
So Richard Belman came up with this equation and from then now that's how we use it.

13:48.930 --> 13:51.380
So you could go ahead and come up with a different equation.

13:51.380 --> 13:55.370
It doesn't have to have Gamla it might have some other factor might not you know have a factor but this

13:55.430 --> 13:57.760
approach works and that's why we're using it.

13:57.890 --> 14:00.730
And this is what it looks like.

14:00.730 --> 14:06.610
So the further away you are the less value of it being in the state and in terms of time value of money.

14:06.620 --> 14:09.810
If I could say to you where would you rather be would you rather be here.

14:09.920 --> 14:11.050
Would you rather be here.

14:11.280 --> 14:12.860
You'd say I would rather be here.

14:12.860 --> 14:18.710
So we're creating that same phenomenon as time value of money we're artificially creating it through

14:18.710 --> 14:24.620
gamma so that in order to incentivize agents or inspire agents to be closer to the finish line.

14:24.620 --> 14:29.660
So if an agent were to be asked would you rather be here or here because of the way this equation works

14:29.870 --> 14:31.520
it would choose to be here.

14:31.580 --> 14:33.320
There's nothing more to that nothing less.

14:33.320 --> 14:35.780
It's not something that the world works this way.

14:35.780 --> 14:41.780
No it's just something that we were artificially creating in order for our agents to understand that

14:42.020 --> 14:47.900
this is this is good this is good this is good old good but this one is better than this one and this

14:47.900 --> 14:50.060
one is better than this one and this one has been in this one.

14:50.060 --> 14:54.730
And that way you can see that all the agent can see in which direction needs to go.

14:54.740 --> 15:00.290
So it can see that if I'm standing here remember that problem that we had or was standing here so if

15:00.290 --> 15:05.110
you standing here do I go down or if I'm standing here to go up or do I go down.

15:05.240 --> 15:10.030
Well now there's not a sound problem anymore because he can see that it's actually better to go up because

15:10.030 --> 15:11.380
the values are bigger here.

15:11.500 --> 15:14.430
And then from here has got to go right because the value is bigger here than here.

15:14.500 --> 15:17.950
And then from here is Bertschi go right because the value here is bigger than you know you are from

15:17.950 --> 15:18.480
here.

15:18.550 --> 15:22.550
He already knows that he needs to go right because he'll get a reward here of one.

15:22.630 --> 15:24.910
So that's how this whole approach works.

15:24.910 --> 15:27.550
Now let's have a quick look at the rest of the square.

15:27.550 --> 15:29.750
So how do we calculate the value in the square.

15:29.980 --> 15:32.380
Well here is where things get tricky.

15:32.410 --> 15:38.350
So from here you might not actually go left right you might actually go right so we can just keep going

15:38.350 --> 15:41.310
like that because it might actually be shorter to go this way.

15:41.470 --> 15:44.670
So what we're going to do is we're going to calculate the value in the square first.

15:44.950 --> 15:48.120
And because obviously from here the best ways to go is up.

15:48.160 --> 15:52.690
Again that's because we see the crew we have the crystal ball we can see things and you'll see further

15:52.690 --> 15:57.010
down in the section you'll see how the agent actually explores this understands this on their likes

15:57.010 --> 15:57.980
through experimentation.

15:58.030 --> 16:02.530
But for us we know that it's better to go this way so we're going to Kalka it's value here and that's

16:02.530 --> 16:06.360
why we're going to calculate the value in this square first.

16:06.370 --> 16:09.190
So here we have three possible actions.

16:09.190 --> 16:11.560
In reality we actually have four we can also go left.

16:11.560 --> 16:15.280
The agent could hypothetically press left and bump into the wall and stay here.

16:15.370 --> 16:19.060
But for simplicity sekret is going to show the actions that we.

16:19.060 --> 16:24.010
Knowing what we know and having the crystal ball we know which actions are the ones actually lead to

16:24.010 --> 16:26.490
something other than the same state again.

16:26.800 --> 16:31.960
And so here from here we know that again just because we have a crystal ball we know that the best way

16:31.960 --> 16:36.790
to go is this way an agent of course would have to experiment and find the best way and you'll see how

16:36.790 --> 16:37.470
that happens.

16:37.510 --> 16:42.220
Further down in the section you'll see actually how an agent walks around and how you would experiment

16:42.310 --> 16:43.530
trying to find these values.

16:43.540 --> 16:45.140
But for us we know that way.

16:45.310 --> 16:50.380
So here if we plug everything in one so the maximum the best output is when you show up.

16:50.440 --> 16:53.760
And here is a report nine so you put that in.

16:53.770 --> 16:55.840
You get zero point nine.

16:56.170 --> 16:58.720
OK so it Kalika that one that calculate this one.

16:58.720 --> 16:59.810
Same approach.

16:59.810 --> 17:02.020
This is you have three ways you can go.

17:02.020 --> 17:05.540
Actually four for the agent but for us we can see it's only three.

17:05.840 --> 17:08.800
So zero point eighty one for me here.

17:08.860 --> 17:14.800
You have 0.72 three and it actually ties in nicely with this value because in you if you discount again

17:14.800 --> 17:20.030
you put 66 and here you have 0.23 because this is the optimal route.

17:20.050 --> 17:21.140
So there you go.

17:21.160 --> 17:23.700
That is the values all of these states.

17:23.710 --> 17:30.400
And now you can see that because we've created this equation or created synthetically this whole concept

17:30.400 --> 17:38.050
of the closer you are to the finish line the more valuable that state is not because we're afraid that

17:38.080 --> 17:41.790
now it's pretty obvious for the agent which way it should go.

17:41.920 --> 17:44.050
And we'll talk more about that in coming.

17:44.070 --> 17:51.820
Charles I hope you enjoyed today's session and I know it's a bit it might sound a bit basic at this

17:51.820 --> 17:56.520
stage but as we go through this section we will add a bit more complexity to it.

17:56.650 --> 18:01.450
At the same time if you cannot wait if you want to jump into it then there is a paper which you can

18:01.450 --> 18:06.700
look at and it is the original paper by Richard Belman is called the theory of dynamic programming from

18:06.700 --> 18:08.110
1954.

18:08.320 --> 18:10.160
And you can find it at this link.

18:10.270 --> 18:16.530
And there you go so you can jump straight into it and read from the author of the Belman equation.

18:16.570 --> 18:20.810
But just bear in mind that this is quite a mathematically heavy paper.

18:20.920 --> 18:22.770
And on that note I'll look for your next.

18:22.800 --> 18:24.520
And until then enjoy AI.
