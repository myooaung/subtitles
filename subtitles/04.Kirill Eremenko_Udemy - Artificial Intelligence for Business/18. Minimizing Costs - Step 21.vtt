WEBVTT

00:00.510 --> 00:06.720
Hello welcome back to this second case study we already completed two important steps of our general

00:07.040 --> 00:07.850
blueprint.

00:07.920 --> 00:12.800
The first step was to build the whole environment into our environment class.

00:12.900 --> 00:18.900
And the second step was to build Of course the brain the brain of the eye that is going to take as inputs

00:18.960 --> 00:20.770
the state of the environment.

00:21.000 --> 00:26.190
And that's where we turn the actions to play and more precisely it will return the q values of these

00:26.190 --> 00:27.200
actions.

00:27.200 --> 00:31.210
And so now now that we have the environment defined and that we have a brain.

00:31.320 --> 00:34.850
Well time to implement the whole deal.

00:34.870 --> 00:42.990
CUELLAR an algorithm with experience replay and this is an advanced version of the deep learning algorithm

00:42.990 --> 00:46.320
and of course we we're going to implement the best version.

00:46.350 --> 00:46.740
All right.

00:46.740 --> 00:54.150
So I've prepared with me the deep Q learning algorithm the food process and so we're simply going to

00:54.150 --> 00:57.950
follow this step by step to help us implement this the best way.

00:58.110 --> 01:00.410
However you are going to see that some of the steps.

01:00.480 --> 01:06.720
Like for example computing the last here will actually not be done in this document bytes and Vauvert

01:06.840 --> 01:08.760
later on in the training.

01:08.760 --> 01:09.050
All right.

01:09.060 --> 01:13.640
So to warm us up let's get a quick refresher on the disappearing algorithm.

01:13.770 --> 01:15.360
It is divided into two parts.

01:15.360 --> 01:22.350
The first one is the initialization part where we have to initialize the memory of the experience replay

01:22.620 --> 01:28.620
as an empty list and which which will call memory to be more specific than we will choose a maximum

01:28.620 --> 01:34.710
size of the memory so this will be one of the object variables you know introduced and initialized in

01:34.710 --> 01:40.210
the init method because of course we're going to build this db Q learning algorithm within a class or

01:40.220 --> 01:45.960
we'll start with the init method and we will introduce initialises maximum size memory and then we'll

01:45.960 --> 01:51.690
start in a first state but that's more to explain the beginning of the process and then the second part

01:51.960 --> 01:56.610
which will be you know this whole loop in the training over the epochs too.

01:56.800 --> 02:02.550
And each time they do wait in the direction that will reduce the last error between your predictions

02:02.760 --> 02:05.070
returned by our brain and the target.

02:05.190 --> 02:09.140
So this is where the whole real key learning process happens.

02:09.170 --> 02:10.340
And so how does this start.

02:10.380 --> 02:15.990
Well we start by predicting the q values of the current state then we'll play the action and we'll get

02:15.990 --> 02:18.480
the reward then we'll reach the next stage.

02:18.660 --> 02:25.050
And all these happenings will make a transition because transition is composed of the current state.

02:25.050 --> 02:28.870
You know before we play the action the action played the we received.

02:29.010 --> 02:35.450
And the next state reached to all this is a transition and this is what will be appended to the memory.

02:35.460 --> 02:36.000
All right.

02:36.100 --> 02:43.050
And and then you know we'll do some batched learning meaning that instead of back propagating a single

02:43.050 --> 02:47.940
input and updating the weights based on the last error incurred by one prediction.

02:48.030 --> 02:54.660
Well we're going to do that on a batch of prediction a batch of predictions and targets.

02:54.660 --> 03:01.890
That's why here and this last step we will first get the predictions into a batch then the targets in

03:02.040 --> 03:07.740
a separate batch the same size as the batch of the predictions and then we'll compute the last air between

03:07.950 --> 03:10.420
the operations and the targets in that batch.

03:10.440 --> 03:16.410
That's why we have some on the whole batch that is for all the couple of preditions and Target.

03:16.410 --> 03:17.030
All right.

03:17.070 --> 03:23.370
And then of course we will back propagate this total last area of the whole batch back into the new

03:23.380 --> 03:26.180
network and through sarcastic graden descent.

03:26.250 --> 03:32.550
We will use our optimizer to update the weights according to how much they contributed to the air that

03:32.550 --> 03:35.930
is in the directions that will reduce this air.

03:36.030 --> 03:36.330
All right.

03:36.330 --> 03:40.020
So that was a quick refresher on the dequeue learning algorithm.

03:40.110 --> 03:43.200
And now if you're ready Well let's implement it.

03:43.200 --> 03:50.340
So I'm going to go back to bison we're quickly going to import the library that we need for this implementation.

03:50.490 --> 03:59.280
And the good news is that we will only need non-Thai so important by adding the shortcut a..

03:59.460 --> 04:00.350
Perfect.

04:00.360 --> 04:04.970
So that was the introduction to start off for this new board the general framework.

04:05.130 --> 04:11.370
And now in the next couple of tutorials I will highlight the structure of this new part of the steps

04:11.700 --> 04:17.940
which still belong to the general framework and we'll implement together the whole deal killer algorithm

04:18.300 --> 04:19.930
and then enjoy AI.
