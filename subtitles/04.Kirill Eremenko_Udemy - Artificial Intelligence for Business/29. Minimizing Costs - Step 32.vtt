WEBVTT

00:00.420 --> 00:03.060
All right here we go Here we go for the last step.

00:03.060 --> 00:08.700
The final step of the training where we have to compute the loss over the two whole batches of inputs

00:08.730 --> 00:12.060
and targets that we just gathered in the previous step.

00:12.210 --> 00:18.360
And now what we're about to do well make sure to remember this each time you are training a deep learning

00:18.360 --> 00:21.220
model or a deep reinforcement learning moral.

00:21.230 --> 00:28.080
We're going to use this amazing tool by the model class from the curiousest library which is the train

00:28.170 --> 00:30.030
on batch function.

00:30.180 --> 00:36.450
And why is this an amazing function that's because it allows to perform the most powerful version of

00:36.450 --> 00:42.900
great in the sense which is many but great in the sense combining stochastic great in the sense and

00:43.020 --> 00:49.170
Bache learning that is we're going to apply things to our at an optimizer stochastic great in the sense

00:49.410 --> 00:56.250
but not on single rows of observations but on batches of observations and the batches of observations

00:56.250 --> 00:58.760
will be of course well the two batches.

00:58.950 --> 01:02.640
We gather here in the previous step to make sure to check out the book.

01:02.700 --> 01:04.740
I explain all this in the first.

01:04.770 --> 01:08.110
And it's at the end of the book about artificial neural networks.

01:08.140 --> 01:13.230
So make sure to find many Bache great in the sense and the function we're about to use will allow us

01:13.230 --> 01:14.000
to perform that.

01:14.010 --> 01:14.740
Exactly.

01:14.910 --> 01:22.800
All right let's do this as I just told you it is a method from the moral class pre-build and defined

01:22.800 --> 01:23.920
by Karris.

01:23.940 --> 01:32.460
So since we already created our model object well to use this train on Bachche method Well we only need

01:32.460 --> 01:39.180
to take our model object from which we're going to call this train on batch method which it's important

01:39.180 --> 01:43.340
to understand is a method of this model class.

01:43.350 --> 01:44.490
So let's do this.

01:44.490 --> 01:46.180
I'm going to go back to training.

01:46.320 --> 01:54.060
And so we're going to get our model from which we're going to go the train on Bache method.

01:54.330 --> 01:59.340
And now this training method you can guess which arguments it is expecting.

01:59.350 --> 02:06.120
It's of course the two batches of inputs and the toilets which we exactly gather in the step.

02:06.270 --> 02:09.390
So since we called them input and toilet.

02:09.540 --> 02:17.770
Well in fact that's exactly what we need to enter here so I'm copying this and pasting that inside.

02:18.030 --> 02:18.610
All right.

02:18.610 --> 02:23.810
And now this will perform the training on our batches of inputs and target.

02:23.880 --> 02:29.640
So it will basically do everything you know it will for propagate inputs inside our brain.

02:29.640 --> 02:34.740
You know our neural network then it will get the predictions it will compare the predictions to the

02:34.740 --> 02:35.370
target.

02:35.520 --> 02:39.020
Then the last area between the inputs and the targets will be incurred.

02:39.120 --> 02:44.750
And then this last will be back propagated into the neural network to which many Bachche great innocent

02:44.850 --> 02:45.870
will be performed.

02:45.960 --> 02:53.900
Thanks to our Adam optimizer because indeed we can Baude also the model with our atom optimizer and

02:53.910 --> 02:56.760
with an MSCE Plus which is the main square.

02:57.000 --> 02:59.460
All right so everything will happen automatically.

02:59.550 --> 03:02.860
And that's the beauty of this train on Bachche method.

03:03.000 --> 03:03.690
All right.

03:03.750 --> 03:04.130
Perfect.

03:04.140 --> 03:05.720
But then that's not all.

03:05.760 --> 03:11.090
This train on Bache method will not only do this but also it will return something.

03:11.100 --> 03:12.780
Can you guess what it's going to return.

03:12.840 --> 03:19.530
Well of course it's going to return the last the last error and since right now we are dealing with

03:19.620 --> 03:26.320
a specific time step and we want to get the overall US over all the time steps of that specific epoch.

03:26.350 --> 03:29.390
You know we want to get the total last over the whole epoch.

03:29.550 --> 03:37.080
Well what I'm going to do now is take the last which we already introduced and initialized here initialized

03:37.080 --> 03:37.780
to zero.

03:37.940 --> 03:45.780
So I'm going to say that last and I'm going to increment it by the last returned at this specific time

03:45.990 --> 03:48.720
here that we're dealing with this one.

03:48.870 --> 03:52.180
And so over the time steps you know over the whole book.

03:52.350 --> 03:55.860
Well the loss will be incremented each time.

03:55.950 --> 03:59.500
Thanks to the result of this training batch method.

03:59.700 --> 04:06.270
All right so pretty amazing and powerful method is because it does everything dramatically for you and

04:06.300 --> 04:13.310
powerful because it applies the most powerful version of In percent which once again is mineable great

04:13.510 --> 04:14.340
isn't.

04:14.460 --> 04:15.030
All right.

04:15.060 --> 04:17.050
So that's a great thing done.

04:17.110 --> 04:21.560
And now we have to you know finish this while loop correctly.

04:21.660 --> 04:28.050
Indeed since we did a while loop Well you know the looping Vargo which is times that is not incremented

04:28.140 --> 04:28.990
dramatically.

04:29.010 --> 04:37.280
So we have to increment it ourselves and to do this well simply here we will just increment it by one.

04:37.290 --> 04:37.940
All right.

04:38.010 --> 04:43.420
And then one final thing to do before this loop is ended correctly.

04:43.440 --> 04:44.510
Can you guess what it is.

04:44.510 --> 04:46.510
It's very important not to forget it.

04:46.680 --> 04:53.340
It's about of course the current state because indeed we initialize the current state here by calling

04:53.340 --> 04:55.950
the observed method from our environment.

04:55.950 --> 05:00.830
But then at no time have we ever updated current state.

05:00.900 --> 05:04.350
And so this is the time now that we have to get it right.

05:04.350 --> 05:06.190
Otherwise that wouldn't make sense.

05:06.300 --> 05:12.420
This current state will always be the same and so can you guess how we're going to update our current

05:12.420 --> 05:13.180
state.

05:13.200 --> 05:19.560
Well very simply since we actually reached the next date well that means that we've also already reached

05:19.680 --> 05:27.030
the next timestep and therefore the next day here is nothing else than our new current state.

05:27.060 --> 05:32.130
And so the current state that was initialized here.

05:32.220 --> 05:40.060
Well we'll become inside the while loop the next state that we've just reached and here we go.

05:40.080 --> 05:47.430
Now we are really in the new timestep and we did everything correctly because in our current state has

05:47.430 --> 05:50.310
become that next state and perfect.

05:50.310 --> 05:52.560
That's the end of the whole training.

05:52.560 --> 05:53.790
Congratulations.

05:53.970 --> 05:55.810
The hard part is done now.

05:55.840 --> 06:01.910
We're going to end this beautifully by printing you know the turning result for each book.

06:02.010 --> 06:07.590
So we'll just do some prints and then of course we will do something very important which is saving

06:07.800 --> 06:08.670
them all.

06:08.790 --> 06:14.760
I will teach you how to do this which will allow us to save the weight of the model so that we can later

06:14.760 --> 06:19.460
on load them any time without having to train the model again.

06:19.470 --> 06:20.720
So that's really useful.

06:20.760 --> 06:22.330
And not only That's really useful.

06:22.350 --> 06:25.200
You will see that it's something very easy to do.

06:25.320 --> 06:27.460
And once again thanks to Chris.

06:27.690 --> 06:31.830
All right so let's do all this in the next tutorial I can't wait to see how the training is going to

06:31.830 --> 06:32.350
go.

06:32.430 --> 06:33.920
And until then enjoy a.
