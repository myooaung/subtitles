WEBVTT

00:00.800 --> 00:05.330
Hello and welcome back to the course and deep learning now that we've seen your own networks in action

00:05.390 --> 00:08.390
it's time for us to find out how they learn.

00:08.420 --> 00:10.460
So let's get straight into it.

00:10.460 --> 00:16.050
They are two fundamentally different approaches to getting a program to do what you want it to do.

00:16.190 --> 00:24.560
One is hard coded coding where you actually tell the program's specific rules and what outcomes you

00:24.560 --> 00:25.070
want.

00:25.070 --> 00:30.890
And you just guide it throughout the whole way and you account for all the possible options that the

00:30.890 --> 00:33.070
program has to deal with.

00:33.260 --> 00:41.270
On the other hand you have neural networks where you create a facility for the program to be able to

00:41.750 --> 00:43.480
understand what it needs to do on its own.

00:43.480 --> 00:50.030
So you basically create this neural network where you provided inputs you tell it what you want as outputs

00:50.060 --> 00:53.020
and then you let it figure everything out on its own.

00:53.300 --> 00:59.840
Two fundamentally different approaches and that is something to keep in mind as we go through these

00:59.840 --> 01:00.800
tutorials.

01:00.800 --> 01:06.070
Our goal is to create this network which then learns on its own.

01:06.070 --> 01:13.220
We we're going to avoid trying to put in the rules and a good example that I can give you right now

01:13.220 --> 01:18.630
is this will come further in the course but it's just a very visual example for instance.

01:18.650 --> 01:25.640
How do you distinguish between a dog and cat fur on the left side on the process depicted on the left

01:25.640 --> 01:33.200
you would program things like the cat's ears have to be like this look out for whiskers look out for

01:33.200 --> 01:39.480
this type of nose look out for this type of shape of face look out for these colors you kind of you'd

01:39.480 --> 01:45.260
describe all these things and you'd have conditions like if if the ears are pointy than cat if the ears

01:45.260 --> 01:49.330
are sloping down and possibly dog and so on.

01:49.550 --> 01:55.040
On the other hand for a neural network you just code the neural networks you code the architecture and

01:55.040 --> 02:00.980
then you point the neural network at a folder full all these cats and dogs with images of cats and dogs

02:00.980 --> 02:03.160
which are already categorized and you tell it.

02:03.180 --> 02:06.740
OK I've got you I've got some images of cats and dogs.

02:06.830 --> 02:08.810
Go and learn what a cat is.

02:08.810 --> 02:10.550
Go and learn what a dog is.

02:10.550 --> 02:15.950
And the neural network will on its own understand everything it needs to understand and then further

02:15.950 --> 02:21.080
down once is trained up when you give it a new image of a cat or dog it will be able to understand what

02:21.080 --> 02:21.550
it was.

02:21.560 --> 02:25.640
So there they are those are the two fundamentally different approaches.

02:25.640 --> 02:31.040
And today we're going to slowly start getting into how that second approach works.

02:31.040 --> 02:31.480
All right.

02:31.520 --> 02:33.290
So let's get straight to it.

02:33.350 --> 02:39.830
Here we have a very basic neural network with one layer is called a single layer feedforward neural

02:39.830 --> 02:42.710
network and it is also called a perception.

02:42.710 --> 02:47.260
Now before we proceed one thing that we do need to adjust is that output value.

02:47.330 --> 02:49.260
Right now you can see that it's just a Y.

02:49.280 --> 02:51.110
We need to put a y hat in there.

02:51.140 --> 02:56.440
And the reason for that is usually y stands for the actual value and that's what we're going to be using.

02:56.440 --> 03:03.650
So why is it going to be the actual value which we see inreality output value is the predicted value

03:03.650 --> 03:05.840
by the algorithm by the neural network.

03:05.840 --> 03:09.170
Why what is the output value.

03:09.170 --> 03:11.450
Basically that's the denomination for the output value.

03:11.690 --> 03:19.970
And the perception that was first invented in 9:57 by Frank Rosenblat and his whole idea was to create

03:20.120 --> 03:24.940
something that can actually learn and adjust itself.

03:25.160 --> 03:27.930
And this is what we're going to be looking at now.

03:27.950 --> 03:30.170
So we've got our perception.

03:30.170 --> 03:32.020
Let's see how our perception learns.

03:32.030 --> 03:39.090
So let's say we have some input values that have been supplied to the perception and or basically to

03:39.090 --> 03:40.150
our own network.

03:40.280 --> 03:44.140
Then the activation function is applied.

03:44.140 --> 03:49.110
We have an output and now we're going to plot the output on a chart.

03:49.160 --> 03:51.750
So there it is our output y hat.

03:51.790 --> 03:57.470
Now what we need to do is in order to be able to learn we need to compare the output value to the actual

03:57.470 --> 04:01.530
value that we want the neural network to get right.

04:01.550 --> 04:04.540
And that is the value y.

04:04.760 --> 04:08.350
And so if reported here you'll see that there's a bit of a difference.

04:08.370 --> 04:13.460
Now we're going to calculate a function called the cost function is calculated as one half of the difference

04:13.460 --> 04:17.140
of the squared difference between the actual value and output value.

04:17.150 --> 04:20.450
Now there are many ways you can come up for class function.

04:20.450 --> 04:23.250
There are many different cost functions that you can use.

04:23.270 --> 04:30.500
This is probably the most commonly used call function and why it is specifically this function we use

04:30.500 --> 04:34.220
will find out further down when we're talking about a gradient descent.

04:34.220 --> 04:39.050
But for now we're just going to agree that this is the cost function and basically what the cost function

04:39.050 --> 04:44.240
is telling us is what is the error that you have in your prediction.

04:44.240 --> 04:50.740
And our goal is to minimize the cost function because the lower the cost function the closer the Y had

04:50.750 --> 04:51.730
is to whine.

04:52.100 --> 04:54.380
OK so it's only bigger and that's let's proceed.

04:54.380 --> 05:00.710
So basically from here what happens is there is a cost function and from here what happens is now we're

05:00.710 --> 05:08.900
going to once we've compared now we're going to feed this information back into the neural network.

05:08.930 --> 05:14.270
So there we go there is information going back into the neural network and it goes to the weights and

05:14.270 --> 05:15.570
the weights get updated.

05:15.620 --> 05:20.820
Basically the only thing that we have control of in this very simple neural network are the weights

05:20.840 --> 05:23.500
w 1 W2 all the way to W..

05:23.930 --> 05:30.140
And our goal is to minimize the cost function so all we can do is update the weights so we update the

05:30.140 --> 05:33.650
weights and tweak them a little bit.

05:33.890 --> 05:40.040
And how exactly we'll find out for the down but for now we agree that we have the it and then we continue.

05:40.040 --> 05:48.210
So but here I've put up this screenshot of the data just to make some one point very clear that right

05:48.210 --> 05:53.500
now throughout this whole experiment everything we're doing right now we're dealing with just the one

05:53.540 --> 05:53.940
roll.

05:53.940 --> 06:00.300
So we're dealing with we have a dataset of one row where we have for instance we're dealing with how

06:00.300 --> 06:07.540
long you study it like the variable that we're predicting is what what results you're going to get on

06:07.570 --> 06:12.900
exam and the dependent independent variables that we have is how many hours did you study for how many

06:12.900 --> 06:16.740
hours did you sleep and what did you get on the quiz in the mid-semester.

06:16.740 --> 06:19.830
So in the middle of the semester as a quiz what percentage did you get there.

06:19.830 --> 06:26.070
So based on those variables we're trying to predict what score you'll get for the exam and exam the

06:26.070 --> 06:27.960
93 percent that's the actual value.

06:27.960 --> 06:30.250
So that's why so.

06:30.630 --> 06:36.510
So we feed these three values into on your own network again for the second time now and then we're

06:36.510 --> 06:38.930
going to be comparing the result to Y.

06:39.090 --> 06:40.740
So let's see how this works.

06:40.740 --> 06:43.630
We feed these values into the neural network.

06:43.770 --> 06:50.100
Everything gets adjusted and weights get it just so as you can see this is again going to feed the values

06:50.130 --> 06:50.490
again.

06:50.520 --> 06:55.600
The point here is that we're feeding in the same value so we only have one roll we're trying to we're

06:55.650 --> 06:59.580
training on one or this is because this is just a very simple basic example.

06:59.580 --> 07:01.730
Then we'll see what happens when there's morals.

07:01.740 --> 07:06.130
So again we feed these rows in our cars function get adjusted.

07:06.140 --> 07:10.460
As you can see everything happens along those lines again.

07:10.470 --> 07:16.140
So as you say every time our white hat is changing because we've tweaked the weights Oh my head is changing

07:16.440 --> 07:17.540
our cost function changing.

07:17.550 --> 07:20.400
Let's have a look again so we feed those in.

07:20.490 --> 07:22.810
Why how is changing cost function changing.

07:22.890 --> 07:26.960
We get information back feed back to the weights so that the weights get adjusted again.

07:26.970 --> 07:29.540
We feed in the same values every time.

07:29.730 --> 07:31.780
Everything gets adjusted goes back to the weights.

07:31.800 --> 07:36.630
And one more time feed in OK and another time.

07:36.630 --> 07:40.600
So we've adjust the way adjusted the way we feed in the information.

07:40.770 --> 07:41.310
And there we go.

07:41.310 --> 07:45.930
So now this time the white hat is equal to y cross-functional 0.

07:45.960 --> 07:50.690
Usually we won't get cross-functional equal to zero but this is a very simple example.

07:50.760 --> 07:57.420
So hopefully all that made sense every time we feed in exactly that same row because just in this case

07:57.420 --> 08:04.440
we're just dealing with that one row into our neural network well then the weights get the values get

08:04.440 --> 08:10.740
balanced by the ways activation function is applied we get y hat y had as compared to Y then we see

08:10.740 --> 08:12.260
how the cost function is changed.

08:12.360 --> 08:17.790
Feedback and the feed that information Bakker's on your own network and adjusts adjust the weights again.

08:17.790 --> 08:21.360
And then we repeat the same process again with the same exact row.

08:21.510 --> 08:23.240
We're trying to minimize that cost.

08:23.460 --> 08:26.940
So up until now we've been dealing with just that one role.

08:26.970 --> 08:29.430
Let's see what happens when you have multiple roles.

08:29.430 --> 08:31.280
So here's the full data set.

08:31.290 --> 08:38.550
We have eight rows of how many hours you slept or maybe these are different students in day taking the

08:38.550 --> 08:44.010
same exam how many other hours they studied how many hours of sleep before the exam would get on the

08:44.010 --> 08:44.410
quiz.

08:44.430 --> 08:47.320
And their final result on the test.

08:47.430 --> 08:51.260
And as you can see here on the left I've got eight of these.

08:51.300 --> 08:51.820
Perseverance.

08:51.840 --> 08:54.730
Actually they are all the same person.

08:54.740 --> 09:00.840
So this is also important to send I just multiplied it or like duplicated eight times just so that we

09:00.840 --> 09:03.270
can.

09:03.270 --> 09:04.260
Conception is that.

09:04.260 --> 09:09.800
But the important thing here is the same neural network we're going to be feeding these into one same

09:09.800 --> 09:10.320
human network.

09:10.320 --> 09:11.580
So let's go let's get started.

09:11.580 --> 09:20.730
So one pauca as you'll hear had lain mentioning one airpark is when we go through a whole dataset and

09:20.730 --> 09:26.250
we train our neural network on on all of these roles.

09:26.250 --> 09:27.360
So got this history.

09:27.360 --> 09:31.630
So there's our first row and there's Why had for the first stroke.

09:32.280 --> 09:35.200
There is a second row as why had for the second round.

09:35.220 --> 09:39.450
So again is being fed into the same neural network every time.

09:39.520 --> 09:45.010
I would just copy them several times so we can visually see how this is happening.

09:45.030 --> 09:50.250
Then again as it's happening again that's third row fourth row.

09:50.640 --> 09:55.770
There's our way ahead for the fourth row and so on basically then we get the same values for the remaining

09:55.830 --> 09:56.540
four rows as well.

09:56.540 --> 10:03.410
So every time we just feed in a row into our neural network we get about.

10:03.690 --> 10:06.870
Then we compare to the actual value.

10:06.870 --> 10:08.710
So they are the actual values.

10:08.710 --> 10:11.300
So for every single roll we have an actual value.

10:11.580 --> 10:18.240
And now based on all of these differences between why had and why we can calculate the cost function

10:18.240 --> 10:27.620
which is the sum of all of those squared differences between why and why and how all of that is halved.

10:28.170 --> 10:30.300
And there's our cost function.

10:30.300 --> 10:36.690
And basically now what we do after we have the full cost function we go back and we update the weights

10:37.110 --> 10:39.470
we update a 1 WTW 3.

10:39.480 --> 10:45.750
And the important thing to remember here is that all of these Perceptor has all of these neural networks

10:45.750 --> 10:49.620
is actually one neural network so there's not eight of them there's just one.

10:49.620 --> 10:54.440
And when we update the weights we're going to update the weights in that one neural network.

10:54.440 --> 10:57.870
So basically the weights are going to be the same for all of the rows.

10:57.870 --> 11:00.460
So it's not the case that every role has its own weight.

11:00.480 --> 11:02.700
Not all the rows share the weights.

11:02.850 --> 11:11.070
And so that's why we looked at the cost function which is the sum of the square differences and then

11:11.070 --> 11:15.200
we updated the weights and now from here that it was just one iteration.

11:15.210 --> 11:18.950
Next we're going to run this whole thing again.

11:18.960 --> 11:25.410
We're going to feed every single row into the neural network find out our cost function and do this

11:25.410 --> 11:26.310
whole process again.

11:26.310 --> 11:32.040
So just as we saw previously where we had just one row and we were doing everything again and again

11:32.110 --> 11:33.540
and again same thing here.

11:33.550 --> 11:39.120
But now we're going to be doing an 8 rows or 800 rows or eight thousand rows however many rows you have

11:39.120 --> 11:40.620
in your data set.

11:40.770 --> 11:47.160
You do this process and then you calculate the cost function and the goal here is to minimize the cost

11:47.160 --> 11:54.300
function and to get as soon as you found the middle of the cost function that is your final neural network

11:54.300 --> 12:05.640
that means your weights have been adjusted and you have found the optimal weights for this data set

12:05.640 --> 12:10.710
that you your training on and you're ready to proceed to the testing phase or to the application phase.

12:11.460 --> 12:14.870
And this whole process is called back propagation.

12:14.960 --> 12:21.890
So some additional reading that you might want to do for the cost function and I know we just talked

12:21.890 --> 12:24.800
about one and there are many different ones.

12:24.800 --> 12:28.670
A good article is located on cross validated.

12:28.670 --> 12:33.020
It's called a list of cost functions used in your own networks along side applications.

12:33.050 --> 12:39.800
So the euro is there but you can just google for that exact search term or a search phrase and you will

12:39.890 --> 12:42.070
that this one will be the first one that pops up.

12:42.110 --> 12:48.740
It's actually got some good examples and application or use cases for different functions so if you're

12:48.740 --> 12:51.810
interested to learn more about codifications Check out this article.

12:51.920 --> 12:54.350
And on that note I hope you enjoyed this tutorial.

12:54.350 --> 12:55.970
I look forward to seeing you next then.

12:56.000 --> 12:58.120
Until then enjoy learning.
