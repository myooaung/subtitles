WEBVTT

00:00.240 --> 00:04.740
Hello and welcome back to this case study Number two about minimizing costs.

00:04.810 --> 00:06.220
And welcome to this homework.

00:06.330 --> 00:12.720
More specifically the solution that we're about to implement and this homework is still in this perspective

00:12.870 --> 00:20.220
and spirit of minimizing even more the cost because it is about implementing early stopping which will

00:20.220 --> 00:27.030
allow us to train the model with a lower number of books meaning that as soon as the performance is

00:27.030 --> 00:33.390
not improved after a certain number of books well no need to continue the training and therefore we

00:33.390 --> 00:34.380
can break it.

00:34.380 --> 00:40.560
We can stop it and we'll still have a great moral and you know what we could still have a better model

00:40.680 --> 00:43.600
than the one we obtained without early stopping.

00:43.740 --> 00:48.890
And the reason for this is pretty classic in deep learning or deep reinforcement learning.

00:48.960 --> 00:53.540
If that happens that means that we have overfitting in the previous model.

00:53.550 --> 00:59.800
You know when you train your model too much well the ways are too adapted to the training set.

00:59.820 --> 01:05.640
And so when you run a new simulation with new observations and therefore new data well your model that

01:05.640 --> 01:10.100
was overfit to the training said might not do well on the test.

01:10.110 --> 01:15.780
So this early stopping technique that we were about to implement is not only about reducing the cost

01:15.810 --> 01:16.550
even more.

01:16.650 --> 01:20.450
It's also about making sure that we don't get overfitting.

01:20.490 --> 01:25.440
And speaking of overfitting there is another technique that is absolutely effective and that you must

01:25.440 --> 01:27.080
have a new tool kit.

01:27.120 --> 01:29.110
This technique is called drop out.

01:29.280 --> 01:36.120
And as a bonus of the section I will provide the code of the brain including the drop out because drop

01:36.190 --> 01:44.290
out consists of activating a certain percentage of your neurons inside your brain during for propagation.

01:44.490 --> 01:50.760
So that not all the neurons learn at every iteration and therefore we won't get in a situation where

01:50.880 --> 01:55.280
all the neurons are overfit to your training data.

01:55.470 --> 01:56.880
So that's very effective.

01:56.880 --> 01:58.900
I'll give that to you after the solution.

01:58.980 --> 02:04.950
But now in this solution we will implement the other technique that is early stopping in which is another

02:05.130 --> 02:05.900
must have.

02:05.910 --> 02:06.880
And you took it.

02:07.110 --> 02:07.980
So here we go.

02:08.040 --> 02:13.250
As you can notice I renamed our previous training for our training.

02:13.350 --> 02:14.620
No early stopping.

02:14.640 --> 02:19.290
So that's exactly what we implemented before you know in that training implementation.

02:19.290 --> 02:26.880
Then I computed which gave me this new file which I renamed training early stopping and that's in this

02:26.880 --> 02:33.570
file that will implement early stopping So so far this file contains the exact same implementation as

02:33.800 --> 02:34.350
training.

02:34.370 --> 02:35.230
At least being.

02:35.370 --> 02:43.320
But in this one we will now implement early stopping so that we can save on the machine and server resources

02:43.530 --> 02:50.550
which I remind are spending a lot of energy and therefore incurring cost when training some neural networks.

02:50.550 --> 02:51.310
So here we go.

02:51.360 --> 02:57.720
Let's do this early stopping is a feature of the training so that's why I'm directly going into the

02:57.780 --> 02:59.430
training code section.

02:59.760 --> 03:00.120
All right.

03:00.120 --> 03:02.680
And so you know what early stopping is about.

03:02.740 --> 03:09.690
It's a technique that will stop the training if the performance is not improved after a certain number

03:09.690 --> 03:15.630
of books and a certain number of books will be a parameter that will choose here and which will curl

03:15.750 --> 03:17.550
patients you know patients.

03:17.550 --> 03:21.310
How much do we have to be patient to improve the performance.

03:21.330 --> 03:22.420
So here we go.

03:22.530 --> 03:24.390
Just after we get our moral.

03:24.480 --> 03:27.160
We're going to introduce now four new parameters.

03:27.240 --> 03:35.230
The first one will be early stopping which will be a boolean because either true or false.

03:35.400 --> 03:41.640
And basically that will be useful for us to give us the option to apply yes or no early stopping so

03:42.390 --> 03:44.360
early stopping is equal to true.

03:44.580 --> 03:50.990
Well early stopping will be activated and if it falls early stopping will not be activated.

03:51.000 --> 03:51.690
All right.

03:51.690 --> 03:57.050
So we will of course initialize this to true because we want to do early stopping now.

03:57.060 --> 04:04.200
Then the next parameter is going to be patients which is exactly that number of books.

04:04.200 --> 04:09.480
We will wait for the performance to be improved and over which we will start training.

04:09.480 --> 04:16.110
And so that number of times is going to be well it's a small number of times which means we're not very

04:16.110 --> 04:16.840
patient.

04:16.950 --> 04:18.840
But I'm starting with this one.

04:18.840 --> 04:23.390
We might get some good results then next parameter is going to be.

04:23.430 --> 04:25.200
And that's a very important one.

04:25.290 --> 04:33.500
The best total reward and that's going to be a variable that will keep in memory and each time the best

04:33.510 --> 04:34.220
tool we want.

04:34.280 --> 04:39.990
And you know the best accumulated reward obtained over the books and so you know what we'll do with

04:39.990 --> 04:41.940
this bet will reward very well.

04:42.000 --> 04:49.470
We will compare each time the accumulated reward then if the community we would beat the best or we

04:49.500 --> 04:53.430
weren't well at best what we were will be updated to that new best value.

04:53.520 --> 05:00.960
And if the Akunyili reward doesn't beat the best tool we work well we will have a there which will be

05:00.960 --> 05:05.150
incremented by 1 and we will compare the counter to the patients.

05:05.370 --> 05:10.340
And if the counter goes over 10 well that's how we will start the trend.

05:10.590 --> 05:11.970
So at best all we want.

05:11.970 --> 05:13.640
How do we have to initialize it.

05:13.860 --> 05:14.670
Well be careful.

05:14.670 --> 05:20.640
We cannot really initialize it to zero because that's all the word can actually be a negative number.

05:20.730 --> 05:27.420
So we're going to use this very famous trick in Python which is initialized to something that can never

05:27.630 --> 05:36.810
be larger than any negative number and that's of course minus infinity which I took from non-pay because

05:36.810 --> 05:39.370
it's a mathematical object from the umpire.

05:39.600 --> 05:45.450
All right so that the best reward will make sure that even at the beginning when we get maybe our first

05:45.540 --> 05:50.950
negative accumulated we word that they won't be lower than this best or we were.

05:51.290 --> 05:51.600
OK.

05:51.600 --> 05:55.950
So that's how you initialize the parameters of early stopping.

05:55.950 --> 06:02.730
But then there is a final one that we have to initialize and which I've just mentioned it is that count

06:03.000 --> 06:08.760
that we'll have to compare to the patients below which we won't start the training and over which we

06:08.760 --> 06:15.090
will start the training and that count we will call it exactly you will see that it will make total

06:15.090 --> 06:18.050
sense to go this way patients count.

06:18.180 --> 06:18.770
OK.

06:18.870 --> 06:22.610
And of course this one will have to initialize it to zero.

06:22.680 --> 06:28.890
And so then what will happen is that at the end of each book we will compare the community reward to

06:29.010 --> 06:30.400
the best that we wanted.

06:30.420 --> 06:36.420
Then if the action related reward is larger than the best or reward Well the best or we will become

06:36.720 --> 06:37.800
this new best.

06:37.800 --> 06:38.890
Actually we work.

06:39.030 --> 06:45.600
However if the action later reward is lower than the best or worst than this patience count will be

06:45.600 --> 06:49.640
incremented by 1 and then it will be compared to the patient.

06:49.710 --> 06:53.800
And if it goes over the patient we will stop everything.

06:54.090 --> 06:55.860
All right let's do this.

06:55.860 --> 07:01.950
And so now we have to go down at the very end of the training section because of course we have to break

07:01.950 --> 07:02.640
the training.

07:02.640 --> 07:08.880
After all the updates of the environment and all you know the process of the training happening at each

07:08.880 --> 07:09.700
time step.

07:09.780 --> 07:16.950
Basically at the very end of the epical you know the one that is looping over the epoche not the timestep

07:17.160 --> 07:17.720
right.

07:17.820 --> 07:22.590
The performance improvement is assessed over the epochs and not the time steps.

07:22.590 --> 07:29.970
So speaking of the epoche loop Well I'm going to go exactly here just before the print I'm going to

07:29.970 --> 07:38.430
introduce a new code section which will be well let's call it simply early Stebbing still belonging

07:38.430 --> 07:42.300
to the general framework with early stuff being implemented.

07:42.540 --> 07:43.340
And here we go.

07:43.410 --> 07:45.830
Let's implement early stopping.

07:45.840 --> 07:52.440
So we're going to give ourselves the option to activate or deactivate early stopping and to do that.

07:52.520 --> 08:00.780
We're going to use this trick if then in parenthesis early stopping which means since early stopping

08:00.780 --> 08:05.750
is a Boolean that means nothing else than if early stuff being equal equals true.

08:05.970 --> 08:11.370
And so in that condition then we will activate early stopping.

08:11.460 --> 08:17.160
And so now again please press pause in the video and based on what I've just explained try to implement

08:17.160 --> 08:18.100
it yourself.

08:18.270 --> 08:19.750
And now I'm going to do it.

08:19.860 --> 08:27.750
So I'm just exactly going to follow what I just explained which is first to check if that's all we wanted.

08:28.410 --> 08:38.460
You know the actual reward of our AI is lower than the best total reward which we've just introduced.

08:38.460 --> 08:47.380
Well in that case of course what we have to do is increment our patience count by 1.

08:47.630 --> 08:56.210
And then as if Or else actually elevate this time to tool reward is well you know going to be this time

08:56.210 --> 08:59.030
larger than the best told word.

08:59.150 --> 09:06.030
So I'm just going to replace that long time by larger then in that case.

09:06.200 --> 09:08.760
Well what we have to do is twofold.

09:08.900 --> 09:15.920
First of course we have to update our best all we very well because it just got bitten by our accumulated

09:15.920 --> 09:16.560
reward.

09:16.730 --> 09:25.650
So it will be now equal to that new X million reward by the AI which has now become the best tool we

09:25.660 --> 09:26.320
wanted.

09:26.660 --> 09:33.880
But that's not all we have to do something else which is to reset the counter no patience count.

09:33.890 --> 09:40.090
Because indeed if we beat the best or we work well we'll be OK to wait for another 10 rounds.

09:40.100 --> 09:44.900
You know another 10 bucks to check if the performance will be improved.

09:44.900 --> 09:50.840
So we have to reset the patience count to zero.

09:51.170 --> 09:57.830
Okay perfect but then there is a last if that we have to do and which is of course about applying our

09:57.830 --> 10:01.220
final touch of early stopping if we must.

10:01.250 --> 10:04.120
In other words that's where we will break the training.

10:04.160 --> 10:11.360
If the performance is not improved and to do this well we're going to add this last if to check if our

10:11.360 --> 10:22.270
patients count just went over the patients that is over 10 bucks without any improvement which in that

10:22.270 --> 10:24.940
case will break everything.

10:25.090 --> 10:33.950
But before we break everything let's just print a nice early stopping in the will quote right.

10:33.970 --> 10:35.900
Just to explain that indeed.

10:35.950 --> 10:40.610
We had to break the training because there was no improvement of our AI performance.

10:40.630 --> 10:47.440
And then final touch we break everything or just the training not our AI of course.

10:47.440 --> 10:47.920
All right.

10:47.950 --> 10:48.800
And then that's it.

10:48.820 --> 10:54.730
We just implemented early stop in congratulations which I remind has two benefits.

10:54.730 --> 10:57.350
First saving the cost even more.

10:57.460 --> 10:59.750
And second preventing overfitting.

10:59.770 --> 11:04.060
And we're going to check if we indeed had some overfilling in the previous training.

11:04.060 --> 11:09.310
Remember we got 54 percent if here we improve that score even more.

11:09.430 --> 11:13.360
Well that means that we might have had overfilling in our previous training.

11:13.390 --> 11:18.480
So what we're going to do now is select everything as you can notice.

11:18.520 --> 11:23.650
I restarted the kernel here so basically everything is reset for a new training.

11:23.650 --> 11:29.400
We can actually delete the previous model because indeed we're about to get a model.

11:29.500 --> 11:35.950
And so now well I think everything is ready to launch that new training with early stopping.

11:35.950 --> 11:37.370
The train has just started.

11:37.600 --> 11:41.240
And here we go with the first epix.

11:41.260 --> 11:41.560
All right.

11:41.560 --> 11:44.410
So we can see that it's a good start indeed.

11:44.560 --> 11:50.860
It's probably the same story as before you know with the same first spending's of energy in the two

11:50.860 --> 11:53.810
separate simulation with the AI and with no AI.

11:53.980 --> 11:56.730
But the question is there are actually two questions.

11:56.770 --> 11:58.210
How long is it going to last.

11:58.210 --> 12:01.170
Is it going to last less than 100 bucks.

12:01.360 --> 12:04.560
And second question are we going to beat the score.

12:04.570 --> 12:07.660
But that will get the answer for the simulation.

12:07.660 --> 12:13.570
The first answer that we're going to get into straining is the number of books we will have reached

12:13.720 --> 12:14.860
at the end.

12:14.860 --> 12:18.790
So right now number 16 We're doing great.

12:18.790 --> 12:24.530
Our aim building indeed the ultimate cooling system except in some books like this one.

12:24.730 --> 12:26.920
Well that's no big deal.

12:26.950 --> 12:31.210
I think it's very similar to before and will let's do the usual.

12:31.210 --> 12:34.580
I'm going to put some little music and we'll see when we reach the end.

12:59.130 --> 13:01.680
And well that music didn't last long.

13:01.680 --> 13:07.750
Indeed the final epoch of the training with early stopping is epoche number 30.

13:07.770 --> 13:11.770
So it was relevant and easy to implement early stopping.

13:11.820 --> 13:16.950
We actually didn't need 100 folks but now we have not one yet.

13:17.100 --> 13:20.350
Let's check that with this new model.

13:20.370 --> 13:23.310
You know with these new weights of the neural network.

13:23.430 --> 13:29.200
But a neural network which was trained for a short number of times therefore with much less updates.

13:29.370 --> 13:32.060
Well let's check that even with this model.

13:32.100 --> 13:39.510
We get some good score on brand new observations and the testing that is over this one full year of

13:39.510 --> 13:42.080
simulation with brand new data.

13:42.090 --> 13:42.420
All right.

13:42.420 --> 13:46.170
So the good news now is that we don't have to change anything here.

13:46.170 --> 13:49.910
You know early stopping only had to be implemented for the training.

13:49.980 --> 13:51.720
And here everything is ready.

13:51.780 --> 13:56.780
Here we're going to load our numeral the one with the weights resulting from early stopping.

13:56.820 --> 13:58.640
So let's execute this.

13:58.710 --> 14:01.850
I'm just going to remove the bike before.

14:01.870 --> 14:03.070
Here we go.

14:03.190 --> 14:08.180
Now let's select everything and execute.

14:08.450 --> 14:08.790
Right.

14:08.790 --> 14:13.800
And again I'm going to put some music again I hope you like the music by the way because I've made you

14:13.800 --> 14:14.820
listen to a lot.

14:14.820 --> 14:20.610
Apologies if you don't like it but I think it's always better to wait for something with some nice music.

15:00.250 --> 15:08.220
And wow not only we had to train for a much less number of time but also we even beat the energy saved

15:08.280 --> 15:11.300
by the AI and that's exactly what I was saying.

15:11.300 --> 15:17.070
There was overfitting in our previous training because indeed in the previous training we trained our

15:17.070 --> 15:19.570
neural network over 100 books.

15:19.800 --> 15:23.850
And here with only three ebooks we beat the final score.

15:23.880 --> 15:28.460
So that's a great illustration of the benefits of early stopping.

15:28.560 --> 15:33.050
Meaning that it's reducing even more the cost in the training of neural networks.

15:33.120 --> 15:39.030
As I told you it is quite costly for companies today even if you know the AWOS servers or the other

15:39.030 --> 15:45.420
servers are reducing their price but still if you have a lot of servers and big data centers this is

15:45.540 --> 15:51.750
definitely cost saving and not only it is cost saving things to a lower number of e-book but also it

15:51.750 --> 15:58.410
is cost saving in terms of energy spent because indeed we save even more energy than before.

15:58.410 --> 16:06.460
So congratulations for tons of things now first that you finished this huge section on the second case

16:06.460 --> 16:07.990
study minimizing cost.

16:08.070 --> 16:12.960
Second congratulations for having completed the homework then the third.

16:12.960 --> 16:17.340
Congratulations for adding early stopping in you to get and forth.

16:17.400 --> 16:24.850
Congratulations for accomplishing our goal with this amazing performance saving 71 percent of energy.

16:25.110 --> 16:30.150
And now don't forget to check the bonus that you'll get in the next tutorial with an article where I

16:30.150 --> 16:36.960
will provide the code to implement dropout to reduce even more overfitting.

16:36.960 --> 16:39.610
So make sure to add it and you took it as well.

16:39.630 --> 16:41.280
And until then enjoy AI.
