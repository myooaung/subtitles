WEBVTT

00:00.460 --> 00:02.850
Highs and welcome to this new tutorial.

00:03.090 --> 00:05.490
All right so I hope you got the chance to do the homework.

00:05.580 --> 00:11.340
And congratulations if you not only succeeded it but also tried because you know practice does make

00:11.340 --> 00:12.360
it perfect.

00:12.390 --> 00:15.520
And so now together we're going to implement the solution.

00:15.870 --> 00:23.010
So the problem was that you know before we had to manually update the reward matrix inside the location

00:23.010 --> 00:23.830
where we want to go.

00:23.880 --> 00:30.120
So we remember we wanted to go to location because it's the top priority location and we manually updated

00:30.120 --> 00:35.320
the we were here changing it from 1 to 1000 then the Q learning happened.

00:35.460 --> 00:40.260
Computing the q values inside this matrix of values over 1000 irrigations.

00:40.470 --> 00:45.880
And then finally the root here got the q values and computed the optimal route.

00:45.960 --> 00:47.740
So that was a three step process.

00:47.850 --> 00:50.130
First we manually updated the reward.

00:50.130 --> 00:52.460
Second the learning process happened.

00:52.590 --> 00:58.360
And third the root function returned the optimal route from the starting location to the end location.

00:58.740 --> 01:00.590
And we want to improve that.

01:00.590 --> 01:06.320
We want to improve that process so that we don't even have to manually change do we want here.

01:06.360 --> 01:12.270
We want to leave it always like that so that when we call this function the root function with the starting

01:12.270 --> 01:16.740
location and the ending location well everything happens automatically.

01:16.740 --> 01:22.560
You know the Q learning happens automatically and the curing already integrated information that the

01:22.560 --> 01:25.940
high reward happens in the temporary location.

01:25.980 --> 01:33.060
Therefore the first step of our approach is to make sure that when we call the root function it automatically

01:33.060 --> 01:34.970
changes the high reward.

01:34.980 --> 01:40.340
Inside the ending location so basically the idea is that what we did manually here.

01:40.500 --> 01:43.170
Well now we won't be the ones to do it.

01:43.230 --> 01:45.550
The root function is going to do that itself.

01:45.810 --> 01:47.700
And that's the first step of our approach.

01:47.880 --> 01:49.840
So to that extent.

01:50.040 --> 01:57.570
Well what I'm going to do now what we have to do is create a copy of the matrix of rewards because you

01:57.570 --> 02:03.390
know we always want to leave the matrix of rewards initialized this way we never want to change it so

02:03.390 --> 02:04.040
that's why.

02:04.230 --> 02:10.290
Right now I'm making a copy of it new matrix of rewards and that is in this matrix that will update

02:10.650 --> 02:14.380
the reward in the top priority ending location where we want to go.

02:14.640 --> 02:21.930
So let's do this now we have to make a copy of our original matrix of rewards which will stay this way

02:22.440 --> 02:24.510
and to make a copy of an entire array.

02:24.570 --> 02:31.140
What you have to do is well first get 9:5 course because from month by we're going to use this copy

02:31.740 --> 02:41.010
function which will exactly create a copy of this original our matrix if you thought of you know creating

02:41.010 --> 02:48.480
a copy this way by just setting our new equal to our well that couldn't work because you know we want

02:48.480 --> 02:50.150
to keep our.

02:50.160 --> 02:57.210
As it is that is initialized with only zeros and ones and since afterwards we're going to do some modifications

02:57.210 --> 02:58.160
on our new.

02:58.320 --> 03:04.080
Well if you keep this like that when you do the modifications on our new Well the modifications will

03:04.080 --> 03:05.600
also happen are.

03:05.690 --> 03:11.970
So that's a trap you absolutely need to avoid in order to make a real copy so that the modifications

03:11.970 --> 03:15.010
on the copy don't affect your original array.

03:15.150 --> 03:20.850
Well you have to do it this way with N.P. that copy the array.

03:21.160 --> 03:21.720
All right.

03:21.720 --> 03:23.860
You can make some tests if you want.

03:23.860 --> 03:27.780
And now we have our new matrix of words are new.

03:27.910 --> 03:29.350
So now what is the next step.

03:29.760 --> 03:38.010
Well the next step of course is to now do this modification that is this update of the reward in the

03:38.010 --> 03:43.700
cell corresponding to the top priority location ending location where we want to go.

03:43.980 --> 03:50.370
And therefore Well there is nothing more simple to do that will just take our matrix rewards far into

03:50.430 --> 03:52.710
this copy of the original one.

03:53.040 --> 03:57.280
And now we're going to get the cell inside which we want to A the reward.

03:57.290 --> 03:59.940
That's the cell and the Raje and clingy.

03:59.960 --> 04:06.680
Remember we have two indexes here which are exactly two states and therefore to get the right index

04:06.760 --> 04:07.200
here.

04:07.290 --> 04:14.700
Well we need to take the ending state and the ending state we can get it by mapping the ending location

04:14.970 --> 04:16.280
to the ending state.

04:16.320 --> 04:22.530
Thanks to our dictionary that with me here right because this one location the state maps.

04:22.530 --> 04:29.820
The letters from a L to their corresponding indexes that is their current funding state from 0 to 11

04:30.510 --> 04:36.610
and therefore what we'll do first before dating the reward in our matrix of the word copy.

04:36.810 --> 04:40.610
Well let's first get this ending state.

04:40.710 --> 04:47.460
So I'm going to get it this way am introducing a new variable here which is obviously the index and

04:47.480 --> 04:51.160
then location index state and then location.

04:51.170 --> 04:51.970
And to get it.

04:52.010 --> 05:00.810
Well as we just said we need to get our location to state dictionary inside which we're of course the

05:00.810 --> 05:07.580
ending location because this will map ending location into the ending state.

05:07.800 --> 05:15.330
And now we can update the reward in the cell of location G because indeed we have what we need is ending

05:15.330 --> 05:16.050
state.

05:16.500 --> 05:23.570
And since the cell is in the row of index and state and in the column of index ending state as well.

05:23.790 --> 05:24.780
Well here we go.

05:24.840 --> 05:26.840
Now we managed to catch this.

05:27.120 --> 05:32.930
And so what we have to do is updated from 1 to 1000.

05:33.090 --> 05:39.300
Exactly as we did manually with now the process is automated so that you know when we call the root

05:39.300 --> 05:46.380
function with just the starting location in letter like as we did in the crude oil and the ending location

05:46.380 --> 05:48.960
the temporary location which is location G.

05:49.110 --> 05:56.640
Well you know we'll automatically get the index state corresponding to that ending location by mapping

05:56.640 --> 06:00.270
it with our location is state dictionary and then here.

06:00.330 --> 06:05.640
Since we have now the ending state indexes both in the row and the column.

06:05.790 --> 06:11.620
Well we managed to update our matrix of what copy without changing the original one.

06:11.700 --> 06:15.830
Thanks to what we did here with this copy trick by an umpire.

06:16.110 --> 06:16.860
All right.

06:17.040 --> 06:18.960
Perfect So that's a first step down.

06:18.990 --> 06:20.180
First important step.

06:20.370 --> 06:25.220
But now there is a second very essential step that we have to do of course.

06:25.220 --> 06:26.420
Can you see what it is.

06:26.670 --> 06:35.560
Well it's of course related to the fact that here the Q learning process happens on our not only it

06:35.550 --> 06:39.430
happens on our But also when we exited the code.

06:39.540 --> 06:42.940
It happens before we make that update here.

06:43.110 --> 06:50.550
So even if we you know replace our by our new here that won't work because the update of our new happens

06:50.670 --> 06:54.120
inside this root function so we don't have a choice.

06:54.120 --> 07:01.790
The only thing we can do here is integrate the learning process inside the root function.

07:01.800 --> 07:02.640
All right.

07:02.640 --> 07:08.660
And we'll integrate everything the whole cube learning algorithm including the initialization and the

07:08.720 --> 07:10.290
q values to zeroes.

07:10.620 --> 07:19.070
So what I'm going to do now is just first get rid of the commons here then this one as well.

07:19.120 --> 07:19.950
Right.

07:20.100 --> 07:26.850
And then I'm going to get all this which is the whole cube learning algorithm that I'm going to cut

07:26.850 --> 07:32.220
it and then I'm going to include it just below right here.

07:32.220 --> 07:36.150
That is right after we make the update of the reward in the cell.

07:36.150 --> 07:38.210
The top priority location.

07:38.370 --> 07:41.220
And so I'm going to base that here.

07:41.340 --> 07:45.640
But now be careful the copy paste just broke the alignment.

07:45.840 --> 07:48.780
So I'm going to fix this right away.

07:48.780 --> 07:50.810
Let's see let's stop making a mistake.

07:50.850 --> 07:52.830
This first line here was initialization.

07:52.830 --> 08:00.750
This happens in the main code section of the root function then starts to loop and then that's the first

08:01.110 --> 08:03.470
row of the loop that goes here.

08:03.480 --> 08:11.800
Same for the playable actions that goes here then happens this second for loop inside the first loop.

08:12.090 --> 08:19.290
Then this a condition that goes inside the second for loop and then that's what happens if indeed the

08:19.290 --> 08:21.410
reward is larger than zero.

08:21.480 --> 08:28.220
We append remember the action to the list of possible actions that we can play from the current state.

08:28.230 --> 08:28.680
All right.

08:28.680 --> 08:33.910
And then let's be careful again the next day here belongs to the first for loop.

08:33.930 --> 08:37.780
Before I loop and not the for loop.

08:37.800 --> 08:38.810
So here we are.

08:38.820 --> 08:39.760
Good.

08:39.840 --> 08:42.810
And then same for the temporal difference here.

08:42.810 --> 08:44.720
It happens in the first four.

08:44.720 --> 08:46.810
I loop and then eventually.

08:46.850 --> 08:48.840
Now we just got the tempo difference.

08:48.840 --> 08:56.360
We can update the Q value following the Belman equation and that happens still inside the first four

08:56.480 --> 08:56.900
to four.

08:56.900 --> 09:04.970
I do because that's indeed a date happens at each iteration and over 1000 iterations.

09:05.310 --> 09:06.160
OK.

09:06.360 --> 09:14.210
So we fixed the alignment and now we just have one last thing to change and we already mentioned it.

09:14.280 --> 09:15.910
We already mentioned what it was.

09:16.080 --> 09:17.100
Guess what it is.

09:17.260 --> 09:23.640
It's of course the fact that the q learning even if we put it at the right place while the Q learning

09:23.640 --> 09:31.560
still happens on the previous original version of our in which now we're not supposed to update manually

09:31.640 --> 09:32.520
we want it.

09:32.590 --> 09:40.250
So of course now we have to make the Q learning process happen on our brand new copy of The Matrix if

09:40.260 --> 09:46.500
we want that we just created and therefore the only thing that we have to do left is just replace are

09:46.500 --> 09:48.240
here by aren't you.

09:48.420 --> 09:53.400
And then same here for the temporal difference let's not forget that as well are you.

09:53.400 --> 09:54.260
All right.

09:54.300 --> 09:55.550
And now that's it.

09:55.560 --> 10:03.440
Now we're ready to make the ultimate test whether managed to improve the system by adding some extra

10:03.440 --> 10:04.680
hour of mation.

10:04.820 --> 10:09.410
All right so first of all just so that you get a nice cold well structured.

10:09.410 --> 10:10.970
I'm going to fix that.

10:10.970 --> 10:11.910
The comments here.

10:12.080 --> 10:19.130
So let's see now we basically integrated the curing process that is building the solution inside the

10:19.130 --> 10:23.850
root function that is the tool supposed to go into production.

10:23.870 --> 10:30.920
Let's just say that now going into production will just be you know the final test because indeed the

10:30.920 --> 10:37.550
real AI solution is built here inside this root function and therefore now Poythress is going to be

10:38.360 --> 10:45.170
just a test that is printing the final route from the primary location and then feel free to test any

10:45.170 --> 10:50.390
other routes you know with other temporary locations or other storing locations.

10:50.450 --> 10:51.650
Feel free to play around.

10:51.650 --> 10:54.530
You'll see that this will work very well.

10:54.530 --> 10:55.250
All right.

10:55.250 --> 10:57.610
So let's make some test first.

10:57.710 --> 11:01.690
We're going to save all this because we made quite a lot of changes here.

11:01.700 --> 11:07.130
We're going to then reset everything of course by you know clicking on this tool button here and then

11:07.490 --> 11:09.840
restart the kernel.

11:09.890 --> 11:10.850
And here we go.

11:10.880 --> 11:17.030
Now everything is reset all cleaned to perform some new tests and that's exactly what we're going to

11:17.030 --> 11:17.910
do now.

11:17.930 --> 11:24.130
So I remind that we're checking to see if when starting from location.

11:24.470 --> 11:32.480
While we manage to get to location G by the two possible optimal routes that is this one and this one.

11:32.480 --> 11:40.400
And of course without having to change anything in this original matrix we want let's do this let's

11:40.490 --> 11:47.090
select everything here and then let's press command control his answer to execute.

11:47.090 --> 11:51.810
And here we go we have our first possible optimal path.

11:51.830 --> 12:03.380
I J K L H G E I J K L h d perfect and the matrix of rewards we can just check was not modified.

12:03.470 --> 12:05.000
Let's check it out our.

12:05.180 --> 12:12.050
And indeed we have the original version of The Matrix Awards which means that we can just now compute

12:12.170 --> 12:17.620
any other optimal route from any starting location to any other ending location.

12:17.930 --> 12:23.630
But then let's compute this whole code again to see if we get the other possible optimal route.

12:23.860 --> 12:28.170
So I'm going to ask you here again that's still the first one.

12:28.310 --> 12:36.680
First one still is one still and second one the IJA A B C G E I J.

12:36.700 --> 12:38.500
B C G great.

12:38.510 --> 12:41.450
Amazing then if you want we can do another test.

12:41.480 --> 12:43.760
Let's see for example starting from B.

12:43.880 --> 12:45.140
Well we go to H.

12:45.140 --> 12:48.020
By this route and not for example this route.

12:48.170 --> 12:52.220
Let's check that the path is going to be B C G H.

12:52.220 --> 12:53.630
Just a quick check you know.

12:53.900 --> 12:57.960
And just to test also the purpose of making this copy every word.

12:57.980 --> 13:01.910
Because again we just did our equals are here that wouldn't work.

13:01.910 --> 13:11.020
So let's test if when starting from location B and going to a prior location age we do PCG age check

13:11.030 --> 13:12.130
it out.

13:12.200 --> 13:25.540
We start from B and we're your age and going to select all this and execute B C G H perfect.

13:25.560 --> 13:32.820
All right so it seems that we did it we did our first improvement that is better to with more automation

13:33.270 --> 13:37.420
but now comes the second challenge a bit more tricky.

13:37.620 --> 13:48.200
Well remember when we go from following our previous example when we go from eat to G Well we get two

13:48.200 --> 13:49.750
possible optimal path.

13:50.000 --> 13:58.370
But in fact remember that in our top priority location ranking the location K comes actually a second

13:58.370 --> 13:59.390
top priority.

13:59.600 --> 14:06.650
So it would be nice to optimize even more the warehouse close by you know making sure that when our

14:06.650 --> 14:13.430
ultimate warehouse Robert starts from this location and has to go to location G well on its way it can

14:13.490 --> 14:18.510
also collect the products of location K before going to locations.

14:18.770 --> 14:24.080
So that you know after collecting the practice of location g it doesnt have to go back to location K

14:24.410 --> 14:29.770
before leading to the exit and bringing the Perfect's to whatever next step in the process.

14:29.810 --> 14:37.430
So thats why the next great improvement of our tool would be an interruption or you know make a new

14:37.430 --> 14:45.200
function that offers the option to go by an intermediary location which of course should belong in the

14:45.350 --> 14:48.290
top three priority location.

14:48.290 --> 14:54.740
So thats the challenge again try to make it by yourself practice truly does make perfect.

14:54.770 --> 14:58.310
And of course we'll implement together the solution in the next toile.
