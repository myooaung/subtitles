WEBVTT

00:01.000 --> 00:03.720
Hello and welcome back to the course on artificial intelligence.

00:03.760 --> 00:08.270
And today we're talking about Mark of decision processes or M.D piece.

00:08.680 --> 00:11.060
Let's have a look what we've got today.

00:11.380 --> 00:14.010
So last time we stopped on the concept of a map.

00:14.020 --> 00:19.930
So because we've calculated the values based on the Belman equation we can derive this map for our agent

00:19.960 --> 00:20.990
of this maze.

00:21.190 --> 00:27.520
And basically what that means is wherever the ange a agent starts let's say it starts over there.

00:27.520 --> 00:30.820
It knows exactly which steps to take in order to get to the finish line.

00:30.820 --> 00:33.220
So it just goes up up right.

00:33.220 --> 00:34.790
Right and done.

00:35.020 --> 00:37.490
And so the question here is is that it.

00:37.540 --> 00:39.730
Is it really that simple.

00:39.730 --> 00:44.750
Is reinforcement learning really that you know for the lack of a better word boring.

00:44.750 --> 00:46.360
It's it's.

00:46.390 --> 00:50.760
Once you have the math that's it all you have to do is you've done it just full of them.

00:51.040 --> 00:55.410
Well the reality is that it's not actually that simple.

00:55.450 --> 01:00.970
And that's a good thing because it makes this course more interesting for us and we can actually solve

01:00.970 --> 01:02.560
much more complex problems.

01:02.560 --> 01:05.410
So this is where the mark of a process is coming.

01:05.440 --> 01:10.810
But first we're going to talk about two things we're into it deterministic search versus nondeterministic

01:10.810 --> 01:11.380
search.

01:11.620 --> 01:14.670
So let's talk about the concept of deterministic search.

01:14.740 --> 01:21.520
This is our agent in the maze and deterministic search means that if the agent decides to go up then

01:21.520 --> 01:26.930
what will happen is 100 percent probability it will go up.

01:26.980 --> 01:28.640
That's exactly what will happen.

01:28.640 --> 01:29.690
There's no other options.

01:29.680 --> 01:33.640
Once once it says go up or click the up arrow it'll go up.

01:33.640 --> 01:35.050
There's no other options.

01:35.200 --> 01:41.980
Now on the other hand nondeterministic search is when our agent says it wants to go up.

01:42.120 --> 01:44.380
They are actually a couple of options.

01:44.410 --> 01:48.740
For example there could be three options and we're going to look an example where there are three options

01:48.760 --> 01:53.260
but it doesn't have to be a limit to three could before it could be different depending on depends on

01:53.260 --> 01:59.590
the problem the randomness could be different but in our case it could be three options with an 80 percent

01:59.590 --> 02:01.550
chance he does go up.

02:01.780 --> 02:07.420
But then with a 10 percent chance when he wants to go up he'll actually go to the left just because.

02:07.420 --> 02:09.280
Because that's how the environment works.

02:09.280 --> 02:10.920
That's the world that he lives in.

02:11.380 --> 02:14.760
And with another check in 10 percent chance he'll actually go right.

02:14.830 --> 02:17.690
And in this case he'll fall into the firepit.

02:17.800 --> 02:20.690
So that is how it all works.

02:20.710 --> 02:26.710
That's an example of a nondeterministic sure search a stochastic process and what the point of this

02:26.710 --> 02:34.990
is is to make a more realistic model of what could actually happen even in a real world in a real world

02:34.990 --> 02:40.510
type of problem because very rarely do you get situations like this when you do something and it happens

02:40.510 --> 02:41.300
exactly that way.

02:41.470 --> 02:46.520
And even if you think about it in terms of games that say you've got an agent playing Pac-Man.

02:46.690 --> 02:51.220
Well not always is it the case that if he's standing in the square he goes up.

02:51.310 --> 02:54.140
He will get the same exact result every time.

02:54.400 --> 02:59.770
Well he will indeed go up but it may be in one case he won't get eaten by a ghost in another case.

02:59.770 --> 03:01.390
He will get eaten by a ghost.

03:01.540 --> 03:05.920
So as you can see there's some randomness to it because it depends on how the ghosts are moving and

03:05.920 --> 03:07.300
they don't always move the same way.

03:07.300 --> 03:09.360
They don't always start in the same locations.

03:09.460 --> 03:16.090
So it's very logical is very fair that there is some randomness there's something that is not under

03:16.090 --> 03:21.760
the control of the agent and that is this is just a way for us to present that in order for us to learn

03:21.760 --> 03:27.160
how we can deal with it and how that affects a Belman equation how it affects the whole reinforcement

03:27.190 --> 03:28.950
learning process.

03:29.020 --> 03:33.700
But at the same time the randomness is of course not limited to if you go up there's a 10 percent chance

03:33.700 --> 03:38.330
you'll go right or temp's and just go left or if you go down to 10 percent chance you go right or left

03:38.330 --> 03:42.640
or if you're right there's a 10 percent chance an up or down subtle limited to where you're going to

03:42.640 --> 03:45.510
end up sometimes you might have a problem that is exactly.

03:45.520 --> 03:47.380
Sometimes the problems might be different.

03:47.380 --> 03:52.900
Sometimes the randomness might boil down to something else it might be boiled down like in that example

03:52.930 --> 03:58.840
Pacman the ghosts eating you are not eating you or my boiled down to something different.

03:58.840 --> 04:05.500
For instance like there's there's like if the agent is playing Doom and then there's something like

04:05.650 --> 04:08.380
a monster which is going to shoot him in one case.

04:08.380 --> 04:14.590
In other cases there's like there's a probability if we should all get shot and we should get shot and

04:14.590 --> 04:19.660
so and so something that is out of the control of the agents is something I cannot predict.

04:19.660 --> 04:25.690
That's what we are modeling here in nondeterministic search and this is where we have directly approached

04:25.900 --> 04:32.730
two new concepts a mark of processes and or a marker of process and a marker mark of decision process

04:32.740 --> 04:38.650
so let's have a look at these and you know how much I don't like to put definitions on lots of text

04:38.650 --> 04:39.030
on the side.

04:39.040 --> 04:42.220
But in this case it is necessary for us to go through them.

04:42.220 --> 04:46.150
So let's have a look a stochastic process has the mark of property.

04:46.190 --> 04:51.820
If the conditional probability distribution of future states of the process conditional both past and

04:51.820 --> 04:58.150
present state depends only upon the present state not on the sequence of events that preceded it.

04:58.180 --> 05:00.350
A process of this property is called a marker.

05:00.990 --> 05:06.420
Very complex definition and it kind of like you introduce a little bit not only contradicts itself but

05:06.420 --> 05:10.980
feels like it contradicts itself so here is as conditional both for positive presence that depends on

05:10.980 --> 05:11.400
your point.

05:11.430 --> 05:14.400
But at the same time it only depends upon the present state.

05:14.460 --> 05:17.820
So don't get too bogged down in that.

05:17.850 --> 05:23.000
I'll break it down in simple terms so a mark of property is when your future states.

05:23.010 --> 05:25.240
So not just your choice but the whole thing.

05:25.260 --> 05:32.130
Your choice and the environment will only like the results of all of the action you take in that environment

05:32.130 --> 05:33.850
will only depend on where you are now.

05:33.870 --> 05:35.850
It will not depend on how you got there.

05:36.060 --> 05:36.520
And that's it.

05:36.510 --> 05:40.590
So that's a of froggery and a process which has this property is called the marker process.

05:40.830 --> 05:47.490
So to put it into an example so if your agent is here and if he goes if he decides to go up he might

05:47.490 --> 05:48.010
go.

05:48.010 --> 05:52.930
He in our case in our nondeterministic search example you actually might go left and right.

05:52.950 --> 05:53.640
All right.

05:53.640 --> 05:57.540
That's because we have that stick a this city inside our environment.

05:57.540 --> 05:59.610
We have that randomness inside our environment.

05:59.760 --> 06:01.730
So any one of these things might happen.

06:01.770 --> 06:07.200
But the key here is that this is a mark of process because we don't care how you got here.

06:07.200 --> 06:10.650
He could have come from the top ended up here he could have come from the left and that up here you

06:10.650 --> 06:12.330
could come from the bottom ended up here.

06:12.330 --> 06:16.570
He could have like play a moved around here like 100000 times and then got here.

06:16.650 --> 06:22.220
It does not matter what happened before only what matters is which state is he in now.

06:22.440 --> 06:29.720
And so the the probabilities of going left or right or up they will always be the same.

06:29.880 --> 06:32.120
If he's in this state now.

06:32.640 --> 06:36.900
And so that's basically just saying it doesn't matter what happened before we're here.

06:36.900 --> 06:39.090
Now this is the state you're in.

06:39.150 --> 06:42.270
And don't forget that state doesn't just mean where he's standing.

06:42.270 --> 06:48.060
The state is the state of the whole of the whole of the agent in the environment so is like monsters

06:48.060 --> 06:52.980
on the right or the monsters on the left or you know is the ghost coming from a top or bottom whatever

06:53.040 --> 06:54.490
state you're in now.

06:54.510 --> 06:58.410
Doesn't matter how you got there doesn't matter how and how it all came to be that you're there in that

06:58.410 --> 06:58.740
state.

06:58.740 --> 07:02.940
Now what will happen in the future is only determined by the state you're in now.

07:02.940 --> 07:07.160
Plus the actions you will take them plus of course the randomness that is overlaid on top of that.

07:07.410 --> 07:14.200
So that's a mark of process and a market decision process or an MVP or market decision process.

07:14.340 --> 07:19.680
Provide a mathematical framework for model modeling decision making in situations where outcomes are

07:19.680 --> 07:23.340
partly random and partly under control over decision making.

07:23.520 --> 07:29.500
So important understand that Markov decision process processes are different and different whole concept

07:29.660 --> 07:32.320
Markov process mark of process.

07:32.320 --> 07:34.820
There are like a mathematical framework so.

07:34.920 --> 07:39.030
But at the same time I thought it was important for us to understand what a mark of process is because

07:39.120 --> 07:44.760
I think it still helps an understanding of a mark mark of decision process and so a market decision

07:44.760 --> 07:50.460
process is there is exactly what we've been discussing Up till now so that the agent lives in this environment

07:50.490 --> 07:56.370
where it has control like him previously had had full control of what's going on but now it has a little

07:56.370 --> 07:57.480
bit less control.

07:57.540 --> 08:00.210
It can decide to go up but it actually knows.

08:00.250 --> 08:05.660
OK so if I go up there's an ape's and chancel go up attempts and chances go left and chance will go

08:05.670 --> 08:06.120
right.

08:06.120 --> 08:08.880
So not everything is fully under its control.

08:08.880 --> 08:10.670
There is some randomness in this environment.

08:10.660 --> 08:15.570
That's exactly what a market decision process and Markov decision process is the framework that the

08:15.570 --> 08:19.350
agent will use in order to understand what to do in this environment.

08:19.350 --> 08:22.340
So we've got an environment with some toxicity some randomness.

08:22.500 --> 08:26.930
And now the agent has to choose for instance should go up down left or right.

08:27.300 --> 08:28.480
He has to make that decision.

08:28.480 --> 08:29.760
He doesn't know what to do.

08:30.090 --> 08:36.150
And in order to make that decision is going to apply a framework is going to be using a mark of decision

08:36.150 --> 08:40.910
process in order to make that decision what what's going to happen where it's going to go.

08:40.920 --> 08:47.540
And so basically this environment that poses this problem it is referred to the mark of decision process

08:47.550 --> 08:52.770
so it's the framework the agent using at the same time the environment is referred to that the agent

08:52.770 --> 08:55.750
is operating in a market decision process environment.

08:56.250 --> 09:01.140
And so basically here we've got two concepts we've got the mark of process is the way this environment

09:01.140 --> 09:03.710
is designed that the PA does the work.

09:03.720 --> 09:06.970
What happens from where you are now doesn't depend on the past.

09:07.080 --> 09:11.190
And that same time we've got the mark of decision process is the framework that the agent is going to

09:11.190 --> 09:13.620
be using in order to solve this environment.

09:13.920 --> 09:18.780
And the good news is that the mark of decision process or that framework that we're talking about is

09:18.780 --> 09:24.680
actually just an add on to our Belman equation is the Belman equation but just a bit more sophisticated.

09:24.690 --> 09:26.920
So let's have a look at that.

09:27.000 --> 09:28.980
This is our Belman equation so far.

09:28.980 --> 09:30.980
It's the maximum of all possible actions.

09:30.990 --> 09:35.100
So the value of being in a state is the maximum of all possible actions that you can take from that

09:35.100 --> 09:35.960
state.

09:36.210 --> 09:41.880
The maximum is taken from the reward that you would get by taking that action in that state plus a discount

09:41.880 --> 09:45.360
factor times the value of the next state which is as prime.

09:45.360 --> 09:47.110
So that's what we've had so far.

09:47.360 --> 09:52.500
Now because we have some randomness in our whole process this this part will change because we don't

09:52.500 --> 09:57.550
actually know which state will end up and we don't know what s prime will be will it be if we're going

09:57.550 --> 10:03.630
up will it be up or will be left would be right we actually have to place this with the expected value

10:03.630 --> 10:04.910
of the next date.

10:04.920 --> 10:08.620
So here we are going to replace this so there's three possible states we can end up in.

10:08.760 --> 10:15.400
And so we're going to replace that with some value that state has a value of as one prime.

10:15.470 --> 10:18.370
That said it has a V as prime to prime.

10:18.420 --> 10:22.450
And this state has a value of the of us three Bryne.

10:22.590 --> 10:28.740
So now we're going to multiply the state that we actually are intending to go into by 80 percent because

10:28.740 --> 10:33.990
that's how probability of getting to that state plus the probability of getting to this state 10 percent

10:33.990 --> 10:40.620
plus of getting in-state So this is just our expected value so if from statistics if we take the expected

10:40.620 --> 10:47.970
value of getting into this state that we'll get into are kind of like the average What's the average

10:47.970 --> 10:51.810
of what we'll get and then we replace that over here.

10:51.960 --> 10:56.070
Then we get this aggression and it jumps very quickly just because there's a bigger but if you look

10:56.070 --> 10:59.880
at it carefully you'll see the same things have got Max here Max here.

10:59.890 --> 11:04.300
Then you've got r of S and A R of S and A.

11:04.400 --> 11:06.360
You've got gamma you've got gamma.

11:06.360 --> 11:08.550
And then finally here you've got the V.

11:08.580 --> 11:13.590
So you knew exactly it was a deterministic search you knew which states you'll get into.

11:13.590 --> 11:18.210
Now you don't know which state you'll get into since that of taking via You're taking the expected value

11:18.210 --> 11:24.600
of the state you'll get into or of the future state or just in simpler terms just taking the average

11:24.630 --> 11:25.860
of what you're getting into.

11:26.010 --> 11:32.390
So you know like it was like for 30 plus 3 percent chance it will be like this Plus's divide by three

11:32.530 --> 11:32.830
basically.

11:32.850 --> 11:37.310
But in this case it's not exactly like average average.

11:37.360 --> 11:40.360
It's a weighted average because of the probabilities here.

11:40.380 --> 11:45.930
So here you've got the probability of it when you're in this stage to take this action of getting into

11:45.990 --> 11:50.760
state as prime time of value as prime and some to cross all this primes that you could possibly get

11:50.760 --> 11:54.740
into where we are so exactly what we had three here one two three.

11:54.810 --> 11:57.260
Add them up multiply by probabilities add them up.

11:57.260 --> 11:57.990
Same here.

11:57.990 --> 11:58.770
One two three.

11:58.770 --> 12:01.610
Multiply them by the probabilities and add them up.

12:02.040 --> 12:05.110
And that is your new Belman equation.

12:05.130 --> 12:06.380
Congratulations.

12:06.420 --> 12:08.960
This is what we're going to be working with going forward.

12:09.090 --> 12:15.540
And that is the framework that is used in Markov decision processes so that is the framework that solves

12:15.540 --> 12:16.650
this.

12:16.740 --> 12:22.950
Agents used to solve this whole stochastic nondeterministic search problem where there's random events

12:22.950 --> 12:25.410
that are happening that they cannot control.

12:25.410 --> 12:26.870
So it's it's much more complex.

12:26.880 --> 12:30.100
But as you can see because we built up slowly to it.

12:30.240 --> 12:36.120
Now we already know about this worrying about there's worry about this we don't know about this.

12:36.120 --> 12:36.660
We know what they are.

12:36.660 --> 12:42.450
So all we did is we just introduced this part over here because there are probabilities involved in

12:42.870 --> 12:49.180
the action or the consequences of your action on nondeterministic they are based on some probabilities.

12:49.180 --> 12:50.510
And so there we go.

12:50.550 --> 12:58.230
That's how a marker of decision process works and the underlying equation behind it.

12:58.270 --> 13:04.270
Once again it is something that is more like more closely resembles a real world problems real Lords

13:04.270 --> 13:08.640
than ours or even games in ours because not everything is straightforward.

13:08.640 --> 13:14.110
There is some randomness of all involved and not always will.

13:14.190 --> 13:18.640
Taking an action in a certain state will always not will not always Will it lead to the same outcome.

13:18.810 --> 13:23.100
And so this is what we're going to be dealing with going forward and that's going to make things way

13:23.100 --> 13:24.260
more interesting.

13:24.330 --> 13:29.420
So hopefully you're excited for that and excited to see what's going to come next.

13:29.640 --> 13:35.820
And in the meantime I found a really cool paper for you to have a look at this time.

13:35.820 --> 13:37.400
It's a very applied paper.

13:37.410 --> 13:39.890
So this one is actually really interesting to read through.

13:40.110 --> 13:44.380
It's called a survey of applications of mark of decision processes proces.

13:44.400 --> 13:47.940
And it was written by white in 1993.

13:47.940 --> 13:55.950
There's the link and Ill show you examples of where Markov decision processes actually are used to model

13:55.950 --> 13:57.130
real life scenarios.

13:57.130 --> 13:59.510
I think I was very excited by this.

13:59.510 --> 14:03.680
I was impressed by some examples of population harvesting for instance.

14:03.680 --> 14:09.240
So let's say you have some fish and you know what the population of fish is you need to decide how many

14:09.240 --> 14:11.540
fish can we fish out this year.

14:11.940 --> 14:14.270
And so that's your current state.

14:14.280 --> 14:18.450
That's the action that you're taking How many can we've shot at this year so that what are the what

14:18.450 --> 14:20.330
are the possible outcomes of that.

14:20.490 --> 14:22.110
How many fish will we have next year.

14:22.110 --> 14:25.170
How many fish will we have the year after and the year after and so on.

14:25.210 --> 14:30.180
And it's not deterministic because it's not like if you take it at an hour and 90 percent of the population

14:30.180 --> 14:34.580
the next year you will have you know back to 100 percent is not not exactly sermonising.

14:34.590 --> 14:39.540
There are certain random factors involved which are out of our control and therefore we have to understand

14:39.710 --> 14:43.590
what's what's going to happen we have to model what's going to happen that's where a market decision

14:43.590 --> 14:45.900
processes is agriculture.

14:46.020 --> 14:50.190
There's an example like something like harvesting crops how much crops do we harvest how much money

14:50.220 --> 14:51.390
do we not harvest.

14:51.420 --> 14:58.140
Another one which I looked at finance and investment like an insurance company needs to decide how much

14:58.140 --> 15:04.940
of its funds it will invest in any given I think day or year or some period of time and there are certain

15:04.940 --> 15:06.440
factors are of US control.

15:06.440 --> 15:11.210
For instance you know the market movements it doesn't know what can happen so it needs to actually model

15:11.210 --> 15:14.140
that somehow and a mark of decision process is used for that.

15:14.300 --> 15:16.840
So here you can see lots and lots of examples.

15:16.850 --> 15:20.290
And this is the number of examples given I think for each one.

15:20.600 --> 15:27.950
And so you know even sports examples for sports and epidemics and motor insurance claims inspections

15:28.180 --> 15:30.980
maintenance and repairs it's also very interesting.

15:30.980 --> 15:31.850
Have a look at that.

15:31.880 --> 15:40.340
Just to give you an understanding of hey this is not just all made up stuff hypothetical The Matrix

15:40.340 --> 15:41.080
type of thing.

15:41.090 --> 15:45.530
This is actually the real world scenario so Ill give you a better understanding and this is what we

15:45.530 --> 15:50.360
talked about in the promotional video for the scores that or the description of the course that we're

15:50.360 --> 15:55.850
going to inspire you and your intuition to give you ideas for how to use AI in real life.

15:55.850 --> 15:57.770
This is your opportunity.

15:57.770 --> 15:59.740
Look at this paper to understand.

15:59.840 --> 16:02.790
OK so we're going to be dealing with mark of decision process going forward.

16:02.840 --> 16:03.950
That's really cool.

16:03.950 --> 16:08.090
What do they look like in real life and this possibly could trigger some ideas for you how you could

16:08.090 --> 16:11.200
apply in the future to make the world a better place.

16:11.690 --> 16:13.640
And we'd be super happy about that.

16:13.640 --> 16:18.530
We'd be happy if you could use what you learn in this course to make the world a better place.

16:18.680 --> 16:20.010
How fantastic with that.

16:20.330 --> 16:23.120
So on that note I hope you enjoyed today's tutorial.

16:23.120 --> 16:24.490
I look forward See you next time.

16:24.560 --> 16:26.280
And until then enjoy a.
