WEBVTT

00:00.240 --> 00:01.830
Hello hello and here we go.

00:01.890 --> 00:03.270
Let's start the training.

00:03.300 --> 00:12.150
So first of all we're going to make sure that the train variable of our environment is set to the train

00:12.150 --> 00:19.140
mode that is this train viable here which was already initialized to true meaning that indeed we are

00:19.230 --> 00:20.290
in the train mode.

00:20.400 --> 00:21.960
So that's the first thing we need to do.

00:22.190 --> 00:28.420
And now that we are in the train mode Well we are almost ready to start the big training for you.

00:28.440 --> 00:36.540
But before we have to get our moral because remember that this brain object is the whole object of the

00:36.540 --> 00:38.390
brain class that we built here.

00:38.400 --> 00:43.050
But inside this brain class and therefore inside these brain object.

00:43.140 --> 00:50.550
Well we have a variable you know a variable of the object that is the full model itself you know composed

00:50.550 --> 00:58.350
of not only the neural network the brain but also the optimizer which is the second most important element

00:58.650 --> 01:05.310
in a neural network because that's the optimizer which will update the weights at each iteration to

01:05.400 --> 01:07.500
reduce the last function over time.

01:07.500 --> 01:14.100
So it's important not to confuse the brain which is the neural network with the model composed of not

01:14.100 --> 01:17.710
only the neural network but also the optimizer and the US.

01:17.970 --> 01:24.210
So that's why right now we have to get our moral and to get our model Well it's very simple since we

01:24.210 --> 01:26.840
already created our brain object.

01:26.890 --> 01:28.740
We're going to get it this way.

01:28.740 --> 01:37.770
This model here is a new variable and introducing and it will be of course to the moral attribute of

01:37.860 --> 01:39.580
the brain object.

01:39.580 --> 01:40.270
All right.

01:40.440 --> 01:41.640
And now we have the full model.

01:41.640 --> 01:45.530
Now we have the new one at work plus plus plus the optimizer.

01:45.630 --> 01:47.970
And so we are ready to start the training.

01:48.330 --> 01:49.920
So let's do this story.

01:49.970 --> 01:51.620
We actually have two options.

01:51.690 --> 01:59.910
The safe option and the not safe option which is to make sure again to check we are indeed in the training

01:59.910 --> 02:07.290
mode and you know to do that we can simply start with an if on train just like that because you know

02:07.500 --> 02:13.290
this means if on the train the train mode of our environment is equal to true.

02:13.290 --> 02:18.710
And so if that's the case well we can train you know we can start to train.

02:18.720 --> 02:21.900
So this is the safe option but it's optional.

02:21.900 --> 02:28.260
I'm just adding this so that you know if you are implementing some complex files and playing with many

02:28.440 --> 02:34.440
servers at the time and combining some training in some simulations Well it's important to have this

02:34.620 --> 02:39.500
safe if condition that will check that you are indeed in training mode.

02:39.690 --> 02:40.320
OK.

02:40.560 --> 02:43.360
And then now we can start the big training for you.

02:43.610 --> 02:53.640
And so I'm going to loop over with a for loop or all the books in the range from 1 to.

02:53.930 --> 03:02.880
Of course the total number of epochs which I remind is a parameter we initialized at 1000.

03:02.940 --> 03:08.190
I'm still not sure if we're going to train for 1000 bucks that seems maybe a bit too much.

03:08.190 --> 03:09.630
Maybe we'll do 100.

03:09.660 --> 03:10.990
We'll decide that at the end.

03:11.130 --> 03:11.760
Okay.

03:11.760 --> 03:14.090
And then Colleen and here we go.

03:14.190 --> 03:17.460
Let's train our AI and let's make it smart.

03:17.910 --> 03:19.390
So what do we have to start with.

03:19.560 --> 03:26.400
Well I remind that this is going to be a long code section with no worries in the next tutorial I will

03:26.400 --> 03:33.090
reveal the structure of the general AI framework you know we'll have code section tiles inside this

03:33.090 --> 03:39.570
big training loop with capital letters which means that indeed inside this dreamful there's still the

03:39.570 --> 03:46.250
general steps of the process you have to follow to build a deeper enforcement learning kind of a way

03:46.500 --> 03:47.880
to solve a business problem.

03:48.120 --> 03:51.560
But before that we have to do some couple things first.

03:51.750 --> 03:57.430
You know initialize the toll reward obtained by the AI over the round.

03:57.570 --> 04:00.360
And of course I'm initializing this to zero.

04:00.420 --> 04:08.060
Then of course also we have to initialize the last to zero and we initialize this as a float because

04:08.140 --> 04:15.320
a loss is the result of some computations then we're going to initialize the first month of the training

04:15.330 --> 04:18.770
you know just the month at which we're going to start the training.

04:18.990 --> 04:20.900
And you know this can be any month.

04:21.000 --> 04:25.470
I wanted to start with the first month January but actually we can start anytime.

04:25.470 --> 04:31.810
So when I'm simply going to do is get a random integer between 0 and 12.

04:31.890 --> 04:33.740
And so I'm getting here first.

04:33.750 --> 04:41.150
Remember the random module by and by and then from this random module I'm going to get that once again

04:41.310 --> 04:47.690
the rand function which is going to take two arguments to integers.

04:47.790 --> 04:54.750
The first one is going to be zero and the second one is going to be 12 because it is excluded so that

04:54.930 --> 05:00.700
this will give me a random integer between 0 included and 12 excluded.

05:00.990 --> 05:01.500
All right.

05:01.500 --> 05:05.770
And that will allow us to start at any month of the year.

05:05.830 --> 05:06.700
Perfect.

05:06.700 --> 05:09.540
All right then next step what is next step.

05:09.550 --> 05:15.240
Well the next step is to make sure that we have a fully reset environment.

05:15.280 --> 05:20.860
You know in case this is not the first training you are running well your environment object might not

05:21.130 --> 05:21.970
be reset.

05:22.120 --> 05:27.550
So here we're going to make sure we reset our environment and to do this.

05:27.580 --> 05:34.330
I'm using of course the reset method which is indeed a method of our environment.

05:34.510 --> 05:36.890
Right this is the first method of date on.

05:36.940 --> 05:41.110
But if we scroll down Indeed we have the reset method here.

05:41.170 --> 05:42.500
This is what I just got.

05:42.720 --> 05:43.610
OK.

05:43.690 --> 05:50.920
And then so then remember this reset method let's have a look at it again this reset method takes one

05:50.920 --> 05:55.580
argument which is new month but that's is the month we're going to start the training.

05:55.810 --> 06:03.390
And so here in our training implementation we'll I'm going to input that this new month argument of

06:03.430 --> 06:12.330
the reset method is going to be exactly this Neumont here which is the result of a random integer taken

06:12.340 --> 06:14.120
from 0 to 11.

06:14.350 --> 06:15.580
Okay perfect.

06:15.580 --> 06:23.440
Now we're going to make sure as well that the game over variable is false because indeed if this is

06:23.440 --> 06:29.190
not the first training you're running well maybe your game over a variable is set to true.

06:29.290 --> 06:34.240
So before we started training we need to force it to be called false because indeed when we started

06:34.240 --> 06:36.760
training the game is just starting.

06:36.760 --> 06:44.140
Then we're going to get the actual state of the environment and to do this well we have the other method

06:44.710 --> 06:49.840
you know just below the reset method which is the observed method.

06:49.840 --> 06:55.840
And once again this is a method that gives us at any time the current state the last reward in whether

06:55.840 --> 06:56.950
the game is over.

06:57.090 --> 07:00.100
And so perfect that's exactly what we want right now.

07:00.130 --> 07:06.040
You know we're going to start the training in a specific state and that specific state we can get it

07:06.270 --> 07:07.810
thanks to the observer method.

07:08.040 --> 07:08.750
OK.

07:08.800 --> 07:15.340
However since right now we are just starting the training and therefore we have not received any reward

07:15.340 --> 07:15.800
yet.

07:16.030 --> 07:21.850
Well from this observed method we're only going to get the current state.

07:21.880 --> 07:27.100
We don't care about the last word because there is no last word and we don't care either about game

07:27.100 --> 07:31.060
over because we already initialized that so we just want the current state.

07:31.210 --> 07:38.330
And the trick in Python to just get one horrible among several ones one are returned by function.

07:38.350 --> 07:45.670
The trick to do this is for us to get the name of the variable you want so as to introduce it in a new

07:45.670 --> 07:47.260
variable also current state.

07:47.260 --> 07:55.930
Here is a new variable and then I'm going to add after a comma an underscore and then remember the observed

07:56.080 --> 08:00.020
method returns three elements the reward end game over.

08:00.130 --> 08:05.560
We just want this one so I'm going to add an underscore for reward and an underscore for Game over.

08:05.700 --> 08:09.050
So here I already add the for word.

08:09.070 --> 08:15.160
And now I'm adding the underscore game over meaning that we will only get the current state from the

08:15.160 --> 08:20.230
call of our observe method of the environment.

08:20.230 --> 08:20.530
Right.

08:20.530 --> 08:22.760
So that gives me the current state.

08:22.810 --> 08:26.860
And now we have almost everything initialized correctly.

08:26.860 --> 08:33.020
So I'm going to go by the way this section initialising all the variables of both the environment and

08:33.020 --> 08:36.690
the training so that will be in the general framework.

08:36.920 --> 08:43.210
And now the last thing we need to do before we run all these iterations to get the predictions compare

08:43.210 --> 08:48.050
them with Target and reduce the last with our optimized through stochastic rate and the.

08:48.210 --> 08:50.510
You know because we're using atom optimizer.

08:50.740 --> 08:59.430
Well the last thing we need to do here is just to initialize the variable which will loop over the iterations

08:59.440 --> 09:07.330
so not to confuse here epoch and iteration or timestep that's the same the epoch will be a period of

09:07.510 --> 09:15.010
five months and the iteration the times that we hear will be each minute happening during those five

09:15.010 --> 09:21.670
month and it is at each minute that our AI will predict an action to play which will compare to a target

09:21.880 --> 09:24.730
incur the loss and then update the weight with our optimizer.

09:24.730 --> 09:28.330
But remember that we're going to do that through experience replay.

09:28.330 --> 09:30.850
Therefore in some batches we're inside batches.

09:30.850 --> 09:34.590
You have to understand that the time step is the minute.

09:34.630 --> 09:34.960
All right.

09:34.960 --> 09:38.220
So maybe put that in common to remember this.

09:38.220 --> 09:44.630
The book will be a long period of five month and the timestep will be one minute.

09:44.860 --> 09:50.710
OK so now that everybody understands well we all know that we have to initialize this to zero.

09:50.710 --> 09:55.110
This will be the first minute of the fool five month Epoque.

09:55.270 --> 10:03.500
And so now that's when we can start this second which will loop over all the iterations there is all

10:03.500 --> 10:07.750
the minutes during that five month period which is our epoch.

10:07.760 --> 10:10.310
So according to you what are we going to do.

10:10.430 --> 10:15.570
Well our natural thinking would be to do that with a follow up you know we just do that with a follow

10:15.570 --> 10:24.220
up for I in range 0 and lifetime 30 times 24 times 60 because that's the number of minutes in five months.

10:24.410 --> 10:26.500
However there is a little trap.

10:26.570 --> 10:33.050
We also need to make sure that when we start again a new iteration we also need to make sure that the

10:33.050 --> 10:40.580
game is not over and that we can not specify this in a for loop so we can't use a follow up here and

10:40.580 --> 10:46.450
said we're going to use a while loop and therefore here is what we're going to do.

10:46.490 --> 10:52.970
We're going to do the following while loop which will not only iterate through all the time steps from

10:52.970 --> 11:01.600
0 to the number of minutes and five month with which we'll also check that indeed the game is not over.

11:01.880 --> 11:06.830
While not game over here because you know game over is a boolean variable.

11:06.850 --> 11:08.540
So this is equal to true or false.

11:08.630 --> 11:12.020
And writing this as not game over means exactly.

11:12.140 --> 11:15.970
Well Game over equals equals false.

11:15.980 --> 11:18.430
So while not game over.

11:18.590 --> 11:28.440
And then of course while timestep is lower than the total number of minutes in a full epoch and the

11:28.440 --> 11:29.950
netbook is five months.

11:29.960 --> 11:37.640
So we're going to get now the number of minutes and five month which is five times thirty because we

11:37.640 --> 11:40.190
have on average 30 days in one month.

11:40.190 --> 11:42.400
So five months 30 days.

11:42.620 --> 11:49.940
Then times the number of hours in a day which is 24 and then of course the number of minutes in an hour

11:50.270 --> 11:51.800
which is 60.

11:51.800 --> 11:57.530
All right so this gives us exactly the number of minutes the total number of minutes in five months

11:57.680 --> 12:00.830
and that's going to be the fool epoch time period.

12:00.830 --> 12:01.880
All right.

12:01.880 --> 12:09.620
And so now perfect we can move on to the next step which is to start the real deal that is doing the

12:09.620 --> 12:12.200
exploration versus exploitation.

12:12.200 --> 12:17.480
If we do exploration we're going to play some actions rationally and if we do some exploitation the

12:17.480 --> 12:20.610
actions will be the result of the brain's predictions.

12:20.810 --> 12:26.200
And in that case of course we'll compute the loss between the predictions and the targets and update

12:26.260 --> 12:29.520
our weight to reduce it through experience really.

12:29.720 --> 12:31.670
So let's do all this in the next tutorial.

12:31.670 --> 12:35.300
I can't wait to do this very important step with you.

12:35.300 --> 12:38.490
And of course I will highlight the general AI framework.

12:38.720 --> 12:39.750
So let's tackle this.

12:39.770 --> 12:41.510
And until then enjoy AI.
