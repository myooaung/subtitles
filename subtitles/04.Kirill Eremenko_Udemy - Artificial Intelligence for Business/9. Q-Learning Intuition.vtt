WEBVTT

00:00.960 --> 00:03.990
Hello and welcome back to the course on artificial intelligence.

00:03.990 --> 00:06.990
Today we are finally talking about Kule learning.

00:07.020 --> 00:12.840
All right so we've already got this equation the bellmen equation which we've added lots of components

00:12.840 --> 00:13.080
to.

00:13.080 --> 00:19.860
We've got the reward here which can be not just at the very end but it can be at any given step.

00:19.890 --> 00:21.900
We've got the discount factor.

00:21.900 --> 00:26.830
We've got the probability because now we're looking at mark of a decision processes.

00:26.850 --> 00:32.730
And here we've got the possibility of ending up in a different states regardless of what action we take

00:33.300 --> 00:35.160
or actually given the action we take.

00:35.160 --> 00:40.310
They can be multiple states and we can up and up in and then we've got the value of the next states

00:40.440 --> 00:43.510
because he kind of like a recursive function and so on.

00:43.650 --> 00:46.740
But you probably still have one question.

00:46.770 --> 00:53.110
The question is where in all of this is the letter Q Why is it all cold.

00:53.130 --> 00:55.760
Q learning so where is the cue.

00:55.860 --> 00:58.890
And that's the question that we're going to be answering today.

00:58.890 --> 01:05.910
So far we've been dealing with values the value of being in a certain state and now we are going to

01:06.270 --> 01:10.010
look at how Q fits into all that as well.

01:10.020 --> 01:14.510
So here we've got two examples on the left is what we would be doing so far.

01:14.550 --> 01:16.320
Our agent has been analyzing.

01:16.360 --> 01:18.110
Ok I'm over here.

01:18.180 --> 01:21.720
This is a mark of decision process so doesn't matter how we got here.

01:21.720 --> 01:28.210
The rest of the environment doesn't care of the steps that it took me to get here from now on.

01:28.410 --> 01:30.760
I have to make the optimal decision where to go.

01:30.760 --> 01:31.950
Hear hear hear.

01:32.010 --> 01:37.240
Based on the current state and all the future states that come from here but not from the past.

01:37.440 --> 01:42.070
And so he can see that there's three options there's state one state to state three.

01:42.210 --> 01:48.870
And based on his experience he has calculated the values in these states and now he's going to using

01:48.870 --> 01:49.830
the bellmen equation.

01:49.830 --> 01:54.170
So even though this is the classic Proceso he knows that he'll go here but there's a chance that he

01:54.210 --> 01:56.060
go left right and so on.

01:56.060 --> 02:02.400
So based on these values going to make a decision that's what we do so far and that is totally legitimate

02:02.400 --> 02:03.420
approach here.

02:03.540 --> 02:05.590
But now we're going to modify it a little bit.

02:05.610 --> 02:12.780
We're going to take the same exact concept same exact problem but here instead of looking at the values

02:12.900 --> 02:21.390
of each state that he can end up in we're going to look at the values or the value of each action.

02:21.390 --> 02:25.650
So we we're not going to use the letter V any more because for the value of the state we're going to

02:25.650 --> 02:30.690
use a Q and the quote you might have a question why the letter Q Well.

02:30.690 --> 02:32.250
Q Some people speculate that.

02:32.250 --> 02:33.710
Q Will I read this.

02:33.720 --> 02:40.500
I think on Quora somebody mentioned that Q is because of quality but at the same time I couldn't find

02:40.500 --> 02:44.400
any other references to that so it might not be because that might be just because that's the letter

02:44.400 --> 02:49.650
that was used at the time and now it became super popular because it's all called key learning.

02:49.650 --> 02:50.490
Because of that.

02:50.730 --> 02:52.470
So no exact reason was hold.

02:52.480 --> 02:58.800
Q But nevertheless at least it helps us distinguish between V and Q So Q here.

02:58.800 --> 03:03.290
There were presents rather than the value of the state it represents lets go of quality.

03:03.360 --> 03:06.210
It represents the quality of the action that represents.

03:06.210 --> 03:07.980
OK so I've got four actions.

03:08.250 --> 03:10.800
What are the different qualities of these action.

03:10.840 --> 03:15.690
What is the value of the action or the quality of the action which action is more lucrative.

03:15.690 --> 03:20.970
So I need a metric telling me all right how do I quantify this action and then I can compare them and

03:20.970 --> 03:23.470
that is exactly what Q is.

03:23.470 --> 03:29.190
And so here he's got four possible actions as always go up right left or down.

03:29.190 --> 03:35.430
And based on the action theres going to be a formula which tells us the quantifiable value of that action

03:35.430 --> 03:38.360
which we're calling the Q The Q value of that action.

03:38.580 --> 03:41.610
So let's have a look at how we're going to derive this formula.

03:41.610 --> 03:44.440
Q What how does it actually relate to these.

03:44.450 --> 03:51.240
Because as you can imagine because actions lead to states there has to be some sort of link between

03:51.240 --> 03:51.800
the two.

03:51.820 --> 03:56.010
Right we've got we've already determined how to calculate this and we're pretty good at it.

03:56.010 --> 04:01.960
We know how to use a Belman equation in very different environments with lots of different complications.

04:02.190 --> 04:06.000
Well let's leverage that knowledge to understand how we can now calculate.

04:06.000 --> 04:12.080
Q In order to make the same predictions because as you can imagine the environment doesn't change depending

04:12.450 --> 04:16.480
depending on what approach we use the environment is going to be the same regardless.

04:16.500 --> 04:21.080
So therefore this approach and this approach should always give the same result.

04:21.240 --> 04:24.630
And therefore that's another reason why these two should be linked.

04:25.050 --> 04:26.240
So let's have a look.

04:26.250 --> 04:31.230
So here's our view approach where we just got to look at the value of any given state this state or

04:31.230 --> 04:32.180
any other state.

04:32.340 --> 04:37.140
And here we going to be just using the lead here because that's the current state.

04:37.140 --> 04:43.260
And so therefore the terminology will be the same in both equations and here we're using q as a Q as

04:43.260 --> 04:45.470
the of the state s and an action.

04:45.490 --> 04:51.800
A because action is up but in which state did we perform that action we perform that action in the state.

04:51.930 --> 04:52.790
Yes.

04:52.950 --> 04:57.180
OK so now we're going to ride out the Belman equation for the first approach as you can see here we've

04:57.180 --> 05:06.130
got the of assets the value of any given state as is the maximum of the reward that you get a maximum

05:06.130 --> 05:10.410
bet based on the actions you have three in this case you actually have four actions.

05:10.580 --> 05:15.980
So maximum or all the possible actions of this part which we've very discussed many times so this is

05:16.370 --> 05:22.640
our reward that we get from performing that action in that state plus a discount in fact multiplied

05:22.640 --> 05:28.970
by the expected value of the new state that we're going to be in an expected value because it is a stochastic

05:28.970 --> 05:34.130
process we don't know exactly for sure that we're going to end up over here we might end up on the left

05:34.130 --> 05:35.990
or the right sort of probability.

05:35.990 --> 05:38.210
That's why these probabilities are in you.

05:38.210 --> 05:40.290
All right so that's our value.

05:40.290 --> 05:43.480
And now let's look at Q So Q is going to be defined.

05:43.520 --> 05:49.490
We're going to use this to define Q So let's say the agent from this location from this date perform

05:49.490 --> 05:50.690
the action up.

05:50.800 --> 05:54.300
What is the q value going to be equal to.

05:54.470 --> 05:58.910
Well first of all let's see what he will get in return for performing this action.

05:58.910 --> 06:04.000
Up first thing that you'll get is a reward right that knows no doubt about it.

06:04.190 --> 06:09.860
There's going to be some sort of or it might be zero but we know that the hole is the way the Forsman

06:09.860 --> 06:16.030
learning process works is that sometimes for performing certain actions from a given state or so we're

06:16.040 --> 06:19.790
going to add that in here and then we're going to add what we're going to add.

06:19.790 --> 06:21.030
Well let's think about it.

06:21.050 --> 06:24.590
What is the next thing that happens after he's going there.

06:24.800 --> 06:32.000
Well next thing that happens is that now the agent is in a certain state he could end up here with a

06:32.270 --> 06:34.610
80 percent probability or some probability.

06:34.700 --> 06:41.150
But actually you can up here right here but wherever he ends up now there's we already have a quantified

06:41.150 --> 06:47.050
metric for that state he's in and that is actually the v value of that state.

06:47.090 --> 06:52.280
But because he can up in many different states in three of the possible different states we have to

06:52.310 --> 06:55.670
look at the expected value of the state that he'll be in.

06:56.150 --> 07:00.950
And so we're going to add that in we're going to add of course the discounted factor as we previously

07:00.950 --> 07:04.010
had because that is somewhere in the future.

07:04.130 --> 07:11.150
And then we're going to add the some of across all possible states across all possible states that he

07:11.150 --> 07:12.850
could end up by taking this action.

07:12.850 --> 07:14.180
Terms of probability.

07:14.180 --> 07:19.880
So what we're saying here is that okay so by performing an action you're going to get a reward Plus

07:19.940 --> 07:24.530
which is a quantified metric Plus you're going to get you end up in a stage we don't know which one

07:24.950 --> 07:25.770
it could be here.

07:25.790 --> 07:31.430
Could be here it could be here but here is the expected value of the state that you're going to end

07:31.430 --> 07:31.990
up in.

07:32.210 --> 07:35.990
And now we're going to multiply by discounting factor because that is one move away.

07:36.350 --> 07:44.120
So that is our Q value for this for performance section and what you will notice here right away is

07:44.120 --> 07:44.780
that.

07:44.780 --> 07:51.450
Q The Q value is actually exactly identical to what's inside these brackets over here.

07:51.920 --> 07:52.630
And why is that.

07:52.640 --> 07:59.870
Well if you think about it here we're taking the maximum of the results will get the maximum across

07:59.870 --> 08:04.190
all possible actions so we've got four actions and we're taking the maximum across all possible actions

08:04.220 --> 08:11.100
of the result that we'll get by taking each of those actions and enqueue we're defining interesting

08:11.120 --> 08:13.930
what will we get by taking a certain action.

08:13.940 --> 08:19.280
So if you think about it it makes sense that the value of a state.

08:19.310 --> 08:25.650
So for instance this state is the maximum of all of the possible values.

08:25.730 --> 08:25.940
Right.

08:25.940 --> 08:32.800
So here in the States by being in the state the agent has one key value to keep the 3Q value for q value

08:32.810 --> 08:37.610
so yes positive for possible Q values while the value of this stay it makes sense that the value of

08:37.610 --> 08:42.400
the state is the maximum of all of those four key values.

08:42.430 --> 08:48.000
That is exactly what we can see here that's a good confirmation of this new formula that we drive.

08:48.020 --> 08:53.060
If that wasn't the case if that if that didn't match up then we would have questions would be like.

08:53.240 --> 08:55.100
So why why doesn't it match.

08:55.100 --> 09:05.270
Why doesn't it match up if Q value is a quantified metric of performing an action and V depends on the

09:05.270 --> 09:12.410
floor is like is the maximum of the possible results of the four actions that he can perform over that

09:12.410 --> 09:12.910
makes sense.

09:12.920 --> 09:21.010
And that confirms the formula that we've just arrived and now we're going to make it even more interesting.

09:21.050 --> 09:26.520
We're going to get rid of the Wii entirely because you can see here you've got Wii is a recursive function.

09:26.780 --> 09:29.720
So and then you got me and then B and then B and then B and so on.

09:29.720 --> 09:35.240
So you can express this view through all of the following Vee's the most optimal these that will come

09:35.240 --> 09:37.480
up here were expressing.

09:37.480 --> 09:43.790
Q As a funk a recursive function of the or is a function of the next D and then you'd have to plug in

09:43.790 --> 09:48.680
this V and that would get back to the beach so what are we going to do is we're actually going to take

09:48.680 --> 09:52.020
this V and we're going to we're going to replace it with.

09:52.160 --> 09:59.270
Q Right so let's have a look at that want to take this V of the next state and we're going to plug this

09:59.270 --> 10:01.360
into that formula over here.

10:01.520 --> 10:07.080
And as you can see now so this bar doesn't change this probably doesn't change.

10:07.130 --> 10:16.940
But as we just discussed the of s is the maximum by all actions of q of S and a right over here.

10:16.940 --> 10:19.120
So that's what we're going to replace in here.

10:19.120 --> 10:24.260
So we're going to say a maximum of of course is the new action the action that we're going to take because

10:24.260 --> 10:26.580
here we've got the view of as prime.

10:26.720 --> 10:32.300
So here now we've got the maximal Crucell a prime so the actions that we're going to take from this

10:32.300 --> 10:36.950
state or from wherever whichever other state we end up in but the action that we're going to take from

10:36.950 --> 10:45.440
there and Maximo across all those and the maximum is of all the q values that will that are available

10:45.440 --> 10:50.120
to us in that new state as prime comma a prime.

10:50.120 --> 10:51.230
And that's action.

10:51.230 --> 10:52.130
So that's the.

10:52.160 --> 10:53.450
So there's going to be another four.

10:53.450 --> 10:54.470
Q values there.

10:54.530 --> 10:56.620
So now as you can see let's go through again.

10:56.990 --> 11:02.050
So as from what we derive this word we just got through logic and intuition.

11:02.330 --> 11:07.390
So there because that VNS are actually view of AS and curious and they are linked.

11:07.390 --> 11:11.110
The of s is the maximum across all actions of Cuba S and you can see right here.

11:11.110 --> 11:13.720
So this this part is identical to this part.

11:14.240 --> 11:20.690
And then we're going to leverage that and we're going to replace this bit with VNS from here but not

11:20.690 --> 11:25.760
this exact funnel we're going to take this internal part and replace it with Kill venison.

11:26.030 --> 11:27.510
So we're going to plug that in here.

11:27.680 --> 11:35.930
And this part is going to be que of s prime a prime maximum of cube by Crucell a Priam's of Q As Prime

11:35.930 --> 11:36.750
a prime.

11:37.010 --> 11:39.620
And now we have our formula.

11:39.740 --> 11:46.830
So now we have a recursive formula for the q value so now the agent can think what's the value of the

11:46.830 --> 11:50.260
session what's the quality of the session was a Q value of this action.

11:50.420 --> 11:54.240
Well it depends on the reward I get in the immediate step after that.

11:54.320 --> 12:02.200
Plus it depends on the discounted factor times the maximum of all the possible Q actions in that state.

12:02.360 --> 12:06.710
But I don't know if I'm going to get their side need to also look at that state and that state and that's

12:06.710 --> 12:12.810
why we have this expected value over here so we have some probably times a maximum that's only expected

12:12.830 --> 12:13.250
value.

12:13.400 --> 12:15.420
So a very similar formula as you can see.

12:15.560 --> 12:22.430
But this time we're expressing things through the q values and that's why this whole algorithm is called

12:22.610 --> 12:28.970
Kill learning because this is what is looked at this is what the agents actually use they don't look

12:28.970 --> 12:33.920
at the states look at their possible actions and then based on the actions on the values of the actions

12:34.190 --> 12:39.080
they will decide which action to take so they'll just look at the maximum value in this given state

12:39.290 --> 12:40.270
it has four actions.

12:40.280 --> 12:45.110
What is the best action to take so we can compare and sort of comparing the different states that can

12:45.110 --> 12:51.320
end up end up in is going to compare the possible actions that it currently has then by finding the

12:51.380 --> 12:56.780
optimal one is going to take that action and then it is going to repeat that process repeat that process

12:56.810 --> 12:57.390
and so on.

12:57.530 --> 13:03.890
So now you can see how all this comes together how the reward the discounting factor or the stochastic

13:04.280 --> 13:11.150
market decision processes and the values and the q values all come together in order to us this one

13:11.210 --> 13:18.980
super powerful Belman equation for q values which we can now apply and let our agents learn how to beat

13:18.980 --> 13:20.050
the environment.

13:20.370 --> 13:23.330
And so that is a intuitive explanation of what's going on.

13:23.330 --> 13:28.460
I know we went through the formulas but it is necessary because this is like our formula that's we've

13:28.460 --> 13:34.460
been going through this whole chapter and I think it's a good transition from the To.

13:34.700 --> 13:43.400
Q And it illustrates how there are links between Yishun And if you'd like to get a bit more of a rigorous

13:43.400 --> 13:49.330
approach mathematical approach and like you see the mathematics behind it and learn a bit more about

13:49.340 --> 13:51.550
q values and how they work.

13:51.590 --> 13:53.980
Then we've got some additional reading for you.

13:54.080 --> 14:02.870
This paper is called mark of decision processes concepts and algorithms by martÃ­n von Autor low 2009.

14:02.900 --> 14:09.560
So you cut the link here as always and here you can read in a bit more detail to understand all the

14:09.770 --> 14:15.260
nitty gritty behind Q values and so on and now that we've discussed all of these things relating to

14:15.260 --> 14:21.960
the Belman equation now we are ready to look at something more complex such as this paper in order if

14:22.040 --> 14:27.620
if we want to get some additional information on this in order to kind of get a deeper understanding.

14:27.620 --> 14:34.340
But even if you don't read the newspaper or radio you should have a good working knowledge of what learning

14:34.340 --> 14:40.770
is all about and how agents come up with the actions that they need to take in a certain environment.

14:40.790 --> 14:43.920
So I hope you enjoy today Statoil and I look forward to your next them.

14:43.940 --> 14:45.320
Until then enjoy.

14:45.350 --> 14:45.590
I.
