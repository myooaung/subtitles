WEBVTT

00:00.730 --> 00:02.760
All right let's continue our journey.

00:02.820 --> 00:08.900
So the next step now and which is specific to our case study is to compute the reward.

00:09.000 --> 00:16.230
Indeed the word is defined as the difference between the energy spent when we have no AI minus the energy

00:16.230 --> 00:17.810
spent when we have the AI.

00:17.910 --> 00:23.190
And therefore since we've just computed the energy spend when we have no AI and since we also have the

00:23.190 --> 00:29.070
energy spent when we have the AI because we're doing this right after the ai ai plays in action Well

00:29.070 --> 00:32.430
we have everything we need to compute the reward.

00:32.430 --> 00:39.480
And so let's get it in is given by self that reward you know that's our object variable and that's equal

00:39.480 --> 00:44.850
to indeed the difference between the energy spent by that alternative system.

00:44.850 --> 00:54.270
When we have no AI minus the energy spent by our AI and I remind that these two energies are measured

00:54.360 --> 01:01.050
in two separate simulations but that will follow the same evolution of the intrinsic temperature which

01:01.070 --> 01:02.140
will compute soon.

01:02.280 --> 01:08.550
All right so here we go we have the reward we have received the reward right after the AI plays the

01:08.550 --> 01:13.650
action at a specific iteration when we apply the update method.

01:13.860 --> 01:20.670
And now the last step that we can do you know within this main step of the AI framework is to scale

01:20.670 --> 01:21.380
the reward.

01:21.540 --> 01:23.700
So that's not a compulsory step.

01:23.700 --> 01:30.480
However this is highly recommended and by the way this is recommended in the research papers of deep

01:30.480 --> 01:36.000
reinforcement learning and even poncey gradient which is another branch of artificial intelligence.

01:36.000 --> 01:42.660
But scaling the reward will basically stabilize the deep learning computations because if you remember

01:42.660 --> 01:44.380
in the deep learning process.

01:44.540 --> 01:48.880
Well at some point we'll get the targets and the reward is an element of that target.

01:49.020 --> 01:54.420
And so since in a neural network everything is killed Well it will be better for the deep Guler and

01:54.420 --> 01:55.950
computations to scale.

01:55.950 --> 01:56.980
Also the reward.

01:57.180 --> 02:04.770
And that's why we're going to take again here are we word very variable which we're going to multiply

02:05.070 --> 02:12.990
by a scaling factor a small one like one times 10 or the power of minus three which is a simple way

02:12.990 --> 02:19.410
to scale the reward and which is also related to the fact that a normalization of the reward would you

02:19.410 --> 02:23.600
know divided by the maximum absolute value that the can take.

02:23.610 --> 02:29.970
And since the reward is computed by this formula and since this can take values between minus three

02:29.970 --> 02:30.900
and plus three.

02:30.990 --> 02:36.570
Well the maximum value that we can take is approximately this maximum value of the energy spent when

02:36.570 --> 02:39.510
there is no AI and 4 for the approximation.

02:39.510 --> 02:45.000
Since the system when we have no AI brings back the surface temperature to the optimal range.

02:45.000 --> 02:51.150
Well we could say that you know this maximum value is the difference between the maximum temperature

02:51.420 --> 02:56.280
and the minimum temperature or the temperature of the optimal range of temperature.

02:56.490 --> 03:01.230
But if we consider the difference between the max and min temperatures which would correspond to some

03:01.230 --> 03:06.400
normalization Well we would get a factor of 10 of the power of minus 2.

03:06.570 --> 03:08.670
But for even more stabilization.

03:08.760 --> 03:14.980
Well we're actually considering an even lower scaling factor of 10 of the power of minus 3.

03:15.210 --> 03:21.300
And we're going to do the simple scaling of only multiplying this scaling factor which is like dividing

03:21.300 --> 03:28.410
the reward by 0.01 we just multiply that scaling factor by of course sell that reward.

03:28.530 --> 03:34.080
So that now our reward is scaled so that then the deteriorating conditions will be stabilized so that

03:34.080 --> 03:36.630
eventually we'll get better results.

03:36.630 --> 03:37.150
All right.

03:37.170 --> 03:43.620
And that's the first big step inside this environment of day that we're doing through this method which

03:43.620 --> 03:46.240
I remind belongs to the general framework.

03:46.320 --> 03:54.030
We got the reward and now the next step will be to get the next date which I remind is composed of first

03:54.350 --> 03:55.450
the number of users.

03:55.500 --> 03:58.340
You know that's the first element of the input state vector.

03:58.560 --> 04:02.620
Then the second element of the state is the rate of data transmission.

04:02.820 --> 04:10.850
And finally the last element of the state is the server's temperature when there is the AI.

04:11.100 --> 04:16.500
But in order to get that third element we'll need to have first the atmospheric temperature but also

04:16.500 --> 04:20.170
the number of users and the rate of data that's based on assumption 1.

04:20.400 --> 04:27.090
But also the delta of intrinsic temperature because indeed I remind that each of the two separate simulations

04:27.390 --> 04:34.050
when we have the AI and when we have no AI follow the same evolution of intrinsic temperature of the

04:34.050 --> 04:34.700
server.

04:34.710 --> 04:39.450
So that's why we need to get all this first and then that's not for the state but will also need to

04:39.460 --> 04:42.490
update the news of temperature when there is no AI.

04:42.630 --> 04:47.620
Because of course we're keeping track of the evolution of that temperature.

04:47.670 --> 04:48.280
All right.

04:48.450 --> 04:54.790
So let's do this new big step of the general framework getting the next stage in the next tutorial.

04:54.840 --> 04:56.520
And until then enjoy AI.
