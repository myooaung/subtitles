WEBVTT

00:01.040 --> 00:04.240
Hello and welcome back to the course on artificial intelligence.

00:04.250 --> 00:07.540
Today we're talking about the living penalty.

00:07.550 --> 00:13.490
All right so here we've got all Belman equation and as we've been going through this course we've been

00:13.490 --> 00:19.930
slowly making more and more complex so so far we've already added these probabilities in here.

00:20.120 --> 00:22.880
And also we've added the discounting factor.

00:22.880 --> 00:28.110
Now we're going to look in more detail at this side of the equation where we have the reward.

00:28.180 --> 00:34.360
Now remember previously when we talked about how reinforcement learning works we said we have an agent

00:34.400 --> 00:41.260
and it performs actions in the environment and in an exchange or as a result of that it gets a new state

00:41.270 --> 00:45.220
and we should now in and a reward for that action.

00:45.560 --> 00:52.160
Well so far in our example we've only been getting rewards at the very end either if we get to the finish

00:52.160 --> 00:58.590
line or if we for the agent ends up in the fire pit he gets a plus one or a minus one reward.

00:58.880 --> 01:05.690
But that is a very simplistic approach to reinforcement learning and in more realistic scenarios you

01:05.750 --> 01:11.000
will likely have rewards throughout the journey not just at the very end you might have rewards throughout

01:11.000 --> 01:11.330
the journey.

01:11.330 --> 01:20.630
For instance if it's an AI playing a game and if for example it's like shooting somebody in doom it

01:20.630 --> 01:26.390
might get points for killing that enemy or it might be a different other game.

01:26.390 --> 01:32.210
If it overtakes another car or something like that just because of the rules of the game not because

01:32.210 --> 01:39.350
of its way of analyzing the game but actually the game is structured in a way that it's reinforcing

01:39.350 --> 01:43.180
its giving points for doing certain actions even before the game is over.

01:43.490 --> 01:49.730
So Sinatras I got are very common and not just in games and also in real life and that's why we're going

01:49.730 --> 01:55.970
to introduce something similar into our example a simplified version of that but nevertheless a reward

01:56.030 --> 02:01.370
that is continuously given to the agent throughout the game not just at the end and the way we're going

02:01.370 --> 02:04.400
to do it is by looking at the other tiles.

02:04.400 --> 02:10.010
So right now we only have a reward plus one at the final tile and reward minus 1 at the other final

02:10.010 --> 02:11.470
tile the firepit.

02:11.750 --> 02:14.320
But now we're going to add rewards in every single time.

02:14.400 --> 02:17.710
We'll add a very small reward will be minus 0.04.

02:17.720 --> 02:23.360
And as you can see it's negative so every time the agent moves he'll get a negative reward and that's

02:23.360 --> 02:28.250
what's called a living penalty because no matter where he goes he will always get this negative reward

02:28.400 --> 02:30.900
except for these final tiles because that's the end of the game.

02:31.250 --> 02:37.240
And so you can see the reward even on this tile is minus 0.04 but that doesn't mean that he starts with

02:37.240 --> 02:37.910
that reward.

02:37.910 --> 02:39.480
He only gets this reward.

02:39.710 --> 02:44.810
And this is important to remember he only gets his reward when he enters a tile so whenever he he promised

02:44.810 --> 02:51.260
an action he goes here then he will get this reward minus 0.04 and then he comes back to this tile he'll

02:51.260 --> 02:53.620
get another and 0.04 award.

02:53.720 --> 03:00.320
And so the longer he walks around the more he accumulates this negative reward and therefore is an incentive

03:00.320 --> 03:03.800
for him to finish the game earlier so quickly as possible.

03:03.820 --> 03:10.340
And so now let's have a look at how our policy or how the agents policy is going to change depending

03:10.370 --> 03:14.100
on what value we set for this reward.

03:14.360 --> 03:19.460
So here are four environments and in each one we're going to explore a different issue we're not going

03:19.460 --> 03:20.940
to do the calculations.

03:21.070 --> 03:25.640
We're just going to project the results and you will see that intuitively they make total sense.

03:25.640 --> 03:31.690
So here we've got a reward for any step offer any for getting into any state.

03:32.000 --> 03:37.250
Is equal to zero or just as what we've seen before here the reward is going to be zero point zero for

03:37.340 --> 03:39.190
what we just did just now.

03:39.210 --> 03:44.900
You know the reward will be at minus 0.5 or giving penalty will be much as rapid fire so much higher

03:44.900 --> 03:47.650
you can see them here more than ten times greater.

03:47.750 --> 03:50.120
And here the living palatal will be minus two.

03:50.120 --> 03:59.000
So even more than the rewards you get for jumping or even less than the reward that you are the agent

03:59.000 --> 04:00.650
gets for ending up in the firepit.

04:00.650 --> 04:07.610
So let's have a look at how the actions or the optimal policy for parsing this environment will change

04:07.610 --> 04:09.190
depending on the software.

04:09.190 --> 04:11.530
So this is our original policy.

04:11.870 --> 04:18.230
And as you can remember we had these two very interesting and even a little bit weird a decisions by

04:18.230 --> 04:23.900
the agent but which totally makes sense if he can live for as long as he likes.

04:23.900 --> 04:29.480
If you can just travel around for as long as he wants without being penalized for staying alive very

04:29.480 --> 04:30.210
long.

04:30.620 --> 04:37.550
He why not why wouldn't he just go into the corner here into the wall and just keep doing that until

04:37.820 --> 04:38.420
it happens.

04:38.420 --> 04:42.140
It so happens that he goes this way and then he will walk around and same thing here.

04:42.140 --> 04:47.210
It's much safer for him to jump into the wall hoping that one of these will come up eventually and then

04:47.210 --> 04:52.610
he'll go to the finish line anyway because by choosing these two actions he doesn't risk getting into

04:52.610 --> 04:53.620
the fire pit.

04:53.630 --> 04:59.390
Now let's see what happens if we add a reward and negative reward for just being alive for making a

04:59.390 --> 05:04.920
step move here you can see that instantly these two changed.

05:04.920 --> 05:07.870
Now the agent doesn't want to jump into the wall.

05:07.870 --> 05:13.420
He is more likely to risk getting to the firepit having a 10 percent chance of jumping in here but he

05:13.420 --> 05:19.330
will go forward because every time he comes to watch here if he was going to be doing it here as well

05:19.810 --> 05:24.580
every time he jumps into well he performs an action he ends up into in this state with an 80 percent

05:24.580 --> 05:24.940
chance.

05:24.970 --> 05:31.150
And that means an 80 percent chance he will get a minus 0.04 reward meaning that a lot of the time he's

05:31.150 --> 05:34.870
going to be getting this accumulating this negative reward.

05:34.870 --> 05:41.560
Same thing here if he jumps into the wall waiting for that moment when he will actually be randomly

05:41.560 --> 05:42.940
move to the right.

05:42.940 --> 05:49.300
If he keeps doing that he will accumulate this negative reward and that the result of that if you perform

05:49.300 --> 05:55.600
the calculations you'll see that the result of that the expected value of that approach jumping to the

05:55.600 --> 06:02.800
wall is worse than taking the risk of going forward and actually ending up in the firepit.

06:02.800 --> 06:10.420
So he changes his decisions in these two blocks to instead move forward and move to the left even though

06:10.420 --> 06:15.820
there's a risk of jumping in the fire simply because now the longer he's alive the longer he will accumulate

06:15.820 --> 06:18.760
this living penalty in the next environment.

06:18.760 --> 06:22.420
Now we're increasing the living Pouncey to even a greater number.

06:22.420 --> 06:24.780
Mine is 3.5 and let's see what changes here.

06:24.790 --> 06:27.170
So now you can see that compared to this environment.

06:27.220 --> 06:31.540
The only thing a change here is that this arrow is pointing to the right.

06:32.020 --> 06:38.320
And what that means is that now it's no longer a good option for the agent or actually also this arrows

06:38.320 --> 06:42.290
pointing was pointing to a left and nozzles nose pointing upwards.

06:42.310 --> 06:48.700
So now it's no longer a good idea for the agent to go around from here or go around all the way because

06:49.060 --> 06:53.260
if he goes wrong all the way yes he's safer there's a lesser chance there's no chance of getting the

06:53.260 --> 06:54.280
firepit.

06:54.280 --> 06:57.540
But at the same time there's less chance of getting the firemen.

06:57.670 --> 07:03.070
But at the same time he will accumulate quite a substantial negative reward as he walks around.

07:03.070 --> 07:05.330
So it's just it's the part is too long.

07:05.500 --> 07:12.310
So that forces him whether he's here or here to take the shorter route to get here even though he has

07:12.310 --> 07:17.290
a much higher risk of getting into the firepit because as soon as he ends up in the square there's a

07:17.290 --> 07:21.750
10 percent chance of getting to the firepit according to his calculations.

07:21.760 --> 07:27.940
It's just the expected value of this approach is better than the expected value of going around simply

07:27.940 --> 07:30.590
because we've increased this living penalty.

07:30.670 --> 07:36.860
And finally we're getting to the example with the live in penalty of minus two point zero.

07:37.090 --> 07:42.970
So here I encourage you to pause the video now that you've seen how the policy has changed as we increase

07:42.970 --> 07:44.310
the loading pounds penalty.

07:44.410 --> 07:49.810
I encourage you to pause the video and think for yourself what will happen in this scenario.

07:49.810 --> 07:55.820
What do you think the optimal policy will be given that the living penalty is so high.

07:55.960 --> 07:58.340
So all this was the video if you'd like to.

07:58.420 --> 08:04.840
And now I'm going to jump into showing you the solution so in this case if you increase support penalty

08:04.840 --> 08:06.890
to minus two point zero.

08:06.940 --> 08:13.630
It's so high remember that the penalty here is only minus one preserved so high that the agent would

08:13.630 --> 08:18.480
just want to get out of the game in any way possible even if it's just by jumping into the fire pit.

08:18.520 --> 08:19.150
He will do it.

08:19.150 --> 08:25.390
He will be like every time I make a step every time I end up in a new in and you state or every time

08:25.390 --> 08:29.980
I make an action I end up getting a minus two reward.

08:29.980 --> 08:36.310
So what's the point of trying to get to the finish line from here will take me two extra steps.

08:36.310 --> 08:41.020
I'm just going to go here and then straight into the firepit because that way my reward is going to

08:41.020 --> 08:48.460
be less than the negative reward is going to be as bad as in the case of just making additional steps

08:49.000 --> 08:56.740
so you can see that adding this living reward and depending on the value of the living reward that we're

08:56.740 --> 08:59.240
adding the results are going to be different.

08:59.230 --> 09:06.220
And the agent is going to select different policies and that's basically what's how the reward value

09:06.370 --> 09:11.980
can be is incorporated by the Belmont an equation even when it's not just at the finish line or at the

09:11.980 --> 09:16.300
end of the game but even throughout the game and again once again it doesn't have to be on every single

09:17.070 --> 09:20.140
in every single state depending on the environment itself.

09:20.140 --> 09:26.470
It might be given to the agent at certain specific states not at every state but in our simplistic example

09:26.470 --> 09:29.870
we're just using rewards at every given state.

09:30.010 --> 09:34.390
To illustrate this concept so I hope you enjoyed today's tutorial.

09:34.510 --> 09:40.510
And as you can see we've already made our Belman equation quite sophisticated and now it can be applied

09:40.510 --> 09:44.300
to many different scenarios and I can't wait to see it in the next tutorial.

09:44.310 --> 09:46.120
And until then enjoy a I.
