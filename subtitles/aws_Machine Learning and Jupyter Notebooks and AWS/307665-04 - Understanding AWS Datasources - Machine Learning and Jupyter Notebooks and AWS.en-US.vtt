WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:05.040
everybody and welcome this lesson on understanding a little bit more detail that data

00:00:05.040 --> 00:00:09.660
sources and that are going to be used within Amazon machine learning now.

00:00:09.660 --> 00:00:13.810
Data source objects contain meta data about your input data.

00:00:13.810 --> 00:00:14.100
Now,

00:00:14.100 --> 00:00:15.950
when you create a data source,

00:00:15.950 --> 00:00:21.330
the Amazon machine Learning reads your input data computes to spring descriptive statistics

00:00:21.330 --> 00:00:23.230
on its attributes and stores.

00:00:23.230 --> 00:00:24.500
The statistics,

00:00:24.500 --> 00:00:28.840
which is called a schema and other information as part off the data source,

00:00:28.840 --> 00:00:33.650
object after you use or after you create a data source,

00:00:33.650 --> 00:00:35.580
you can use the data insights,

00:00:35.580 --> 00:00:37.800
which are provided by the Machine Learning Consul,

00:00:37.800 --> 00:00:42.530
which will take a look at to explore the statistical properties off your input data.

00:00:42.530 --> 00:00:45.960
And you can use the data source to train a machine learning model.

00:00:45.960 --> 00:00:51.150
Input data is like I mentioned the data used to create the data source,

00:00:51.150 --> 00:00:55.900
and you must save your input data as a CS format for AWS.

00:00:55.900 --> 00:00:59.460
No different programs and different software have different requirements,

00:00:59.460 --> 00:01:01.560
but for Amazon machine learning,

00:01:01.560 --> 00:01:03.650
it has to be in a CSB file.

00:01:03.650 --> 00:01:08.290
And each column in the CIA's We file contains an attributes of the observation.

00:01:08.290 --> 00:01:09.360
For example,

00:01:09.360 --> 00:01:10.750
in the figure that you guys see,

00:01:10.750 --> 00:01:16.850
it's basically a snapshot off a CS refile that has four observation each in its own rope,

00:01:16.850 --> 00:01:20.360
and each observation contains eight attributes,

00:01:20.360 --> 00:01:22.490
which are separated by a comma.

00:01:22.490 --> 00:01:27.510
Now the attributes represent information about each individual represented by an

00:01:27.510 --> 00:01:28.120
observation.

00:01:28.120 --> 00:01:28.690
For example,

00:01:28.690 --> 00:01:30.530
it starts off with a customer i d.

00:01:30.530 --> 00:01:33.560
Then it goes into a job i d the education,

00:01:33.560 --> 00:01:34.360
whether it's basic,

00:01:34.360 --> 00:01:35.620
whether it's high school,

00:01:35.620 --> 00:01:36.870
the housing,

00:01:36.870 --> 00:01:38.250
the loan campaign,

00:01:38.250 --> 00:01:44.570
the duration and whether they will respond to a campaign which is classified by either a

00:01:44.570 --> 00:01:47.100
zero or a 10 being no and one being.

00:01:47.100 --> 00:01:47.710
Yes,

00:01:47.710 --> 00:01:52.620
Now I'm some machine learning requires a name for h attributes,

00:01:52.620 --> 00:01:58.090
and you can specify these attribute names by either including them in the first line,

00:01:58.090 --> 00:02:01.100
which is again known as a header line of the CSP file,

00:02:01.100 --> 00:02:06.660
or including them in a separate schema file that's can be located or that should be located

00:02:06.660 --> 00:02:09.450
in the same S three bucket as input data.

00:02:09.450 --> 00:02:10.710
Now,

00:02:10.710 --> 00:02:16.290
the CSB file that contains your input data has to meet a certain set of requirements.

00:02:16.290 --> 00:02:17.050
For example,

00:02:17.050 --> 00:02:20.930
it must be in plain text consists of observations,

00:02:20.930 --> 00:02:23.870
and each line should have only one observation,

00:02:23.870 --> 00:02:26.540
and they should be separated by commas.

00:02:26.540 --> 00:02:29.750
And if an attribute value contains a comma,

00:02:29.750 --> 00:02:33.460
the entire attribute value must be enclosed in double quotes.

00:02:33.460 --> 00:02:39.630
He's observation must be terminated with an end of Floyd line character and attributes.

00:02:39.630 --> 00:02:42.150
Values cannot include end of line characters,

00:02:42.150 --> 00:02:44.870
even if the values enclosed in double quotes.

00:02:44.870 --> 00:02:48.340
Every observation must have the same number of attributes,

00:02:48.340 --> 00:02:51.850
and each observation must be no longer than 100 kilobytes.

00:02:51.850 --> 00:02:57.440
So these are requirements that must be met by the CSB file that you're going to be using as

00:02:57.440 --> 00:02:58.350
your data source.

00:02:58.350 --> 00:03:00.100
And like I mentioned,

00:03:00.100 --> 00:03:05.350
you can provide your input to machine learning in a single file or as a collection of files

00:03:05.350 --> 00:03:05.350
,

00:03:05.350 --> 00:03:08.430
and these must satisfy a few other conditions.

00:03:08.430 --> 00:03:11.230
Forgettable files must have the same data schema,

00:03:11.230 --> 00:03:17.670
and they must reside in the same s three bucket and and and path,

00:03:17.670 --> 00:03:18.240
for example.

00:03:18.240 --> 00:03:24.120
You guys see the three pats that I've given input data files are named input 12 and three

00:03:24.120 --> 00:03:25.010
and has three.

00:03:25.010 --> 00:03:25.710
Bucket is,

00:03:25.710 --> 00:03:26.140
for example,

00:03:26.140 --> 00:03:27.160
the example Bucket.

00:03:27.160 --> 00:03:31.080
Your paths should look like what you guys see on the screen.

00:03:31.080 --> 00:03:31.570
Lastly,

00:03:31.570 --> 00:03:33.410
when you create the CSB file.

00:03:33.410 --> 00:03:34.500
Each observation,

00:03:34.500 --> 00:03:35.980
like I mentioned as a requirement,

00:03:35.980 --> 00:03:40.380
will be terminated by a special end of line character of this character is not visible,

00:03:40.380 --> 00:03:44.880
but it's automatically included at the end of each operation when you press enter or the

00:03:44.880 --> 00:03:45.620
return keep.

00:03:45.620 --> 00:03:46.290
Now,

00:03:46.290 --> 00:03:50.460
the special character that represents the end of line varies depending on the toys that

00:03:50.460 --> 00:03:51.160
you're using.

00:03:51.160 --> 00:03:51.960
For Apple.

00:03:51.960 --> 00:04:00.380
Lennox uses a line feed character that's indicated by a slash and Windows uses to courage

00:04:00.380 --> 00:04:05.660
is called carriage return and line feed that are indicated by slash R slash.

00:04:05.660 --> 00:04:08.240
And so when you're saving those files,

00:04:08.240 --> 00:04:12.840
you have to make sure that you specify the correct format that those files should be saved

00:04:12.840 --> 00:04:16.410
in based on the operating system that you're using,

00:04:16.410 --> 00:04:21.970
and especially for using a Mac OS and using a Microsoft Excel on a Mac OS and you create

00:04:21.970 --> 00:04:23.400
your CSB file,

00:04:23.400 --> 00:04:27.550
you have to make sure that you save it as a Windows car comma,

00:04:27.550 --> 00:04:28.520
separated or ah,

00:04:28.520 --> 00:04:29.020
windows.

00:04:29.020 --> 00:04:33.270
See SV fall in order for it to work properly within Amazon machine learning.

00:04:33.270 --> 00:04:34.350
So it's very important,

00:04:34.350 --> 00:04:40.460
so you make sure that our data format is correct before feeding in the data to the machine

00:04:40.460 --> 00:04:41.090
learning model.

00:04:41.090 --> 00:04:41.800
Otherwise,

00:04:41.800 --> 00:04:43.940
if you started to format your data,

00:04:43.940 --> 00:04:45.950
especially if you have large amounts of data,

00:04:45.950 --> 00:04:48.860
it can become a hassle to correct it later on.

00:04:48.860 --> 00:04:51.660
So we want to make sure that we know what the requirements are,

00:04:51.660 --> 00:04:57.530
and we start following them when we're preparing the data for ingestion into the machine

00:04:57.530 --> 00:04:58.370
learning model.

00:04:58.370 --> 00:05:03.890
One of the fundamental goals off machine learning is to make accurate predictions on future

00:05:03.890 --> 00:05:07.890
data instances before using ah machine learning model.

00:05:07.890 --> 00:05:08.870
To make predictions,

00:05:08.870 --> 00:05:13.010
we need to obviously evaluate the predictive performance of the model to see if is actually

00:05:13.010 --> 00:05:17.820
working properly and to estimate the quality off machine learning model predictions with

00:05:17.820 --> 00:05:23.640
data it has not seen weaken reserve or what's called a split a portion of the data for

00:05:23.640 --> 00:05:28.640
which we already know the answer and define that as a proxy for future data and evaluate

00:05:28.640 --> 00:05:31.790
how well the machine learning models actually predicting it.

00:05:31.790 --> 00:05:34.710
So with Amazon machine learning,

00:05:34.710 --> 00:05:37.450
we have three options for splitting the data.

00:05:37.450 --> 00:05:39.910
We can pre split the data,

00:05:39.910 --> 00:05:45.000
which is you can split the data into two data input locations before uploading them to the

00:05:45.000 --> 00:05:48.110
S three bucket and create two separate data sources.

00:05:48.110 --> 00:05:51.320
You can use what's called a sequential split,

00:05:51.320 --> 00:05:56.510
which is basically telling Amazon machine learning to split your data sequentially.

00:05:56.510 --> 00:05:57.040
One.

00:05:57.040 --> 00:06:01.200
Creating the training and evaluation data sources or the random split,

00:06:01.200 --> 00:06:02.600
which is it will use it.

00:06:02.600 --> 00:06:03.100
It will.

00:06:03.100 --> 00:06:04.710
It will split it randomly,

00:06:04.710 --> 00:06:08.360
and it just for your purposes.

00:06:08.360 --> 00:06:09.330
By default,

00:06:09.330 --> 00:06:13.060
Amazon machine Learning splits the data in a 70 30 format,

00:06:13.060 --> 00:06:18.990
meaning that 70% will be used for training and 30% will be used for evaluation.

00:06:18.990 --> 00:06:19.790
Now,

00:06:19.790 --> 00:06:24.030
a simple way to split your input data for training and evil is to select the non

00:06:24.030 --> 00:06:28.580
overlapping subsets or for data while preserving the order of the records.

00:06:28.580 --> 00:06:33.040
This approach is useful if you want to value your models on data for a certain date or

00:06:33.040 --> 00:06:34.360
within a certain time rings.

00:06:34.360 --> 00:06:35.020
So,

00:06:35.020 --> 00:06:35.670
for example,

00:06:35.670 --> 00:06:40.650
say that you have a customer engagement data for the past five months and you want to use

00:06:40.650 --> 00:06:44.550
this historical data to predict customer engagement in the upcoming month.

00:06:44.550 --> 00:06:48.300
Are using the beginning of the range for training,

00:06:48.300 --> 00:06:53.380
and the data from the end of the range for evaluation purposes might produce a more

00:06:53.380 --> 00:07:00.270
accurate estimate off the models quality than using records data drawn from an entire data

00:07:00.270 --> 00:07:00.850
range.

00:07:00.850 --> 00:07:06.540
So the two figures you guys see given example of one,

00:07:06.540 --> 00:07:11.860
he should use a sequential splitting strategy versus when you should use a random splitting

00:07:11.860 --> 00:07:12.560
strategy.

00:07:12.560 --> 00:07:14.820
When you create a data source,

00:07:14.820 --> 00:07:18.460
you can choose to split your data source sequentially and again.

00:07:18.460 --> 00:07:19.310
Like I mentioned,

00:07:19.310 --> 00:07:25.170
Amazon will use the 1st 70% for training and the remaining 30% of your data for evaluation

00:07:25.170 --> 00:07:25.170
.

00:07:25.170 --> 00:07:29.770
So it's very important that we know what data were working with or predictions we want to

00:07:29.770 --> 00:07:33.290
make that will help us define what the right way off,

00:07:33.290 --> 00:07:36.160
either sequentially splitting it or random is blending.

00:07:36.160 --> 00:07:41.830
It was very important that we understand our data before we get to this point and will help

00:07:41.830 --> 00:07:45.710
us split the data properly because if we split it incorrectly,

00:07:45.710 --> 00:07:46.890
the machine learning model,

00:07:46.890 --> 00:07:50.140
regardless of how good the back end is,

00:07:50.140 --> 00:07:55.940
it will predict the wrong answers because the data that we have ingested into it and the

00:07:55.940 --> 00:08:00.790
way we're splitting it is incorrect and finally the data schema.

00:08:00.790 --> 00:08:05.600
So there's two different options we can use for the data schema and I had mentioned.

00:08:05.600 --> 00:08:11.410
This proves briefly before you can either allow AWS to infer the data Types of Fiji

00:08:11.410 --> 00:08:15.260
attributes and input data file and automatically create a schemer for you.

00:08:15.260 --> 00:08:21.830
Or you can provide a dot schema file when you upload your data into the Amazon as three

00:08:21.830 --> 00:08:22.240
bucket,

00:08:22.240 --> 00:08:25.850
and you have to make sure it's uploaded in the correct in the same bucket.

00:08:25.850 --> 00:08:29.830
So either or and depending on how your business is operating,

00:08:29.830 --> 00:08:34.670
how advance your machine learning capabilities are of your organization will depend on

00:08:34.670 --> 00:08:39.190
whether you provide your own schema file or whether you want A W s two and for

00:08:39.190 --> 00:08:40.300
automatically for you.

00:08:40.300 --> 00:08:44.070
For most of the small to even medium size organizations,

00:08:44.070 --> 00:08:47.590
it's probably best to let AWS and for the data types,

00:08:47.590 --> 00:08:52.260
If you are pretty heavy into data science and you have your data structured properly,

00:08:52.260 --> 00:08:55.200
you are able to provide your own schema file.

00:08:55.200 --> 00:08:58.190
And just for your information,

00:08:58.190 --> 00:08:59.210
schema,

00:08:59.210 --> 00:08:59.510
files,

00:08:59.510 --> 00:09:00.020
biscuits,

00:09:00.020 --> 00:09:05.050
skeleton skeleton structure that represents the logical view off the entire data.

00:09:05.050 --> 00:09:09.560
It was basically telling the machine learning model what the data is and what it's

00:09:09.560 --> 00:09:11.120
structured as and again,

00:09:11.120 --> 00:09:14.650
you can also either let it'll be Do it for you,

00:09:14.650 --> 00:09:16.110
which is an automated way of doing it,

00:09:16.110 --> 00:09:17.190
a simple way of doing it.

00:09:17.190 --> 00:09:21.480
Or if you have a more advanced need for machine learning,

00:09:21.480 --> 00:09:24.350
then you can also upload your own dot schema file.

00:09:24.350 --> 00:09:29.480
But please make sure that you upload it or it has to be uploaded in the same s three bucket

00:09:29.480 --> 00:09:32.340
that has your orginal data source.

00:09:32.340 --> 00:09:37.330
So I hope you guys got a good understanding off the data structures and how they're

00:09:37.330 --> 00:09:42.330
utilized by AWS machine learning and the importance of making sure that we have our data

00:09:42.330 --> 00:09:47.970
structured properly in terms of CS refile and following the certain requirements that are

00:09:47.970 --> 00:09:52.570
there that are put by AWS in terms of what the CSP file should be,

00:09:52.570 --> 00:09:54.390
where to be located and so on.

