WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:00.780
everybody.

00:00:00.780 --> 00:00:07.010
And welcome to this lesson on looking at our feature transformation feature transformation

00:00:07.010 --> 00:00:10.800
and the importance off it in the machine learning process.

00:00:10.800 --> 00:00:17.250
So let's just consider if a machine learning model whose task is to decide whether a credit

00:00:17.250 --> 00:00:20.160
card transaction is fraudulent or not.

00:00:20.160 --> 00:00:20.860
Now,

00:00:20.860 --> 00:00:21.980
based on your application,

00:00:21.980 --> 00:00:24.060
background knowledge and data analysis,

00:00:24.060 --> 00:00:28.740
you might decide which data fields or features are important to include in the input data.

00:00:28.740 --> 00:00:29.260
So,

00:00:29.260 --> 00:00:30.130
for example,

00:00:30.130 --> 00:00:34.750
you might include the transaction amount the merchant named The address and the credit card

00:00:34.750 --> 00:00:38.860
owners address are important to provide to the learning process.

00:00:38.860 --> 00:00:40.190
On the other hand,

00:00:40.190 --> 00:00:42.020
Iran only generated transaction.

00:00:42.020 --> 00:00:45.200
It I d carries no beneficial information.

00:00:45.200 --> 00:00:49.010
And once you have decided on which fields include,

00:00:49.010 --> 00:00:52.540
you transform these features to help the learning process.

00:00:52.540 --> 00:00:56.760
Transformations add background experience to the input data,

00:00:56.760 --> 00:00:59.690
enabling the model to benefit from this experience.

00:00:59.690 --> 00:01:00.410
So,

00:01:00.410 --> 00:01:01.150
for example,

00:01:01.150 --> 00:01:06.400
you guys see a merchant address has represented a string to,

00:01:06.400 --> 00:01:06.820
for example,

00:01:06.820 --> 00:01:09.050
the 16 25 32nd Avenue,

00:01:09.050 --> 00:01:10.270
New York New York,

00:01:10.270 --> 00:01:17.720
45678 Let's see if if that by itself the address has limited expressive power,

00:01:17.720 --> 00:01:22.850
it is useful only for learning patterns associated with that exact address.

00:01:22.850 --> 00:01:30.960
Breaking it up into ah constituent parts does create additional features such as address

00:01:30.960 --> 00:01:35.030
specifically being 16 25 32nd Avenue cities,

00:01:35.030 --> 00:01:37.110
specifically being New York state,

00:01:37.110 --> 00:01:39.920
specifically being New York and the ZIP court,

00:01:39.920 --> 00:01:43.710
specifically being 45678 No,

00:01:43.710 --> 00:01:49.650
the learning log or them can group more disparate transactions together and discover

00:01:49.650 --> 00:01:50.690
broader patterns.

00:01:50.690 --> 00:01:55.320
Perhaps some urgent ZIP codes experience more fraud activity than others,

00:01:55.320 --> 00:02:01.350
so it's very important that we learn and we transform our data properly.

00:02:01.350 --> 00:02:04.440
Another two ways to transform DataFeatures,

00:02:04.440 --> 00:02:09.620
either before creating the machine learning miles with machine Amazon machine learning you

00:02:09.620 --> 00:02:14.260
the transform your input data directly before showing it to AWS.

00:02:14.260 --> 00:02:19.120
Oregon is a built in data transformations off Amazon machine learning.

00:02:19.120 --> 00:02:26.370
Now AWS Machine ling recipes contain instructions for transforming your data as part off

00:02:26.370 --> 00:02:27.250
the process.

00:02:27.250 --> 00:02:28.430
No,

00:02:28.430 --> 00:02:31.310
they're just defined using Jason like syntax,

00:02:31.310 --> 00:02:35.100
but they have additional restrictions beyond the normal Jason restrictions,

00:02:35.100 --> 00:02:39.050
So there's three different sections that recipes have.

00:02:39.050 --> 00:02:39.790
First,

00:02:39.790 --> 00:02:44.680
we have groups which enable grouping off multiple variables for ease of applying

00:02:44.680 --> 00:02:45.620
transformations.

00:02:45.620 --> 00:02:45.810
So,

00:02:45.810 --> 00:02:46.450
for example,

00:02:46.450 --> 00:02:52.550
you can create a group off all variables having to do with free text parts off a Web page,

00:02:52.550 --> 00:02:53.470
for example,

00:02:53.470 --> 00:02:54.750
the title or the body,

00:02:54.750 --> 00:02:58.660
and then perform a transformation on all of these parts at once.

00:02:58.660 --> 00:03:04.410
Many of assignments which enabled the creation of intermediate named variables that can be

00:03:04.410 --> 00:03:07.190
re used in processing and then finally,

00:03:07.190 --> 00:03:12.960
outputs was define which variables will used in the learning process and what

00:03:12.960 --> 00:03:13.930
transformations,

00:03:13.930 --> 00:03:14.580
if any,

00:03:14.580 --> 00:03:16.480
apply to these variables.

00:03:16.480 --> 00:03:17.090
Now,

00:03:17.090 --> 00:03:20.030
when you create a new data source in Amazon,

00:03:20.030 --> 00:03:23.860
machine learning and statistics are computed for that daily resource.

00:03:23.860 --> 00:03:29.300
It will also create a suggested recipe that could be used to create a new model from that

00:03:29.300 --> 00:03:29.570
data.

00:03:29.570 --> 00:03:35.010
Source suggested data sources based on the data and the target attributes present in the

00:03:35.010 --> 00:03:40.550
data and provide useful starting born for creating and fine tuning your models.

00:03:40.550 --> 00:03:46.990
Other various different types of data transformations that are used by AWS.

00:03:46.990 --> 00:03:47.460
For example,

00:03:47.460 --> 00:03:48.560
there's the engram,

00:03:48.560 --> 00:03:55.500
which takes a text variable as input and provides strings corresponding to a sliding off a

00:03:55.500 --> 00:04:00.720
window off user configurable and words generating outputs in the process.

00:04:00.720 --> 00:04:00.830
So,

00:04:00.830 --> 00:04:01.500
for example,

00:04:01.500 --> 00:04:03.870
consider the string the texting.

00:04:03.870 --> 00:04:05.950
I really enjoyed reading this book,

00:04:05.950 --> 00:04:11.590
specifying the Engram transformation with the window size of one simply gives you all

00:04:11.590 --> 00:04:13.330
individual words in that strength.

00:04:13.330 --> 00:04:13.770
For example,

00:04:13.770 --> 00:04:15.940
each word will be by itself.

00:04:15.940 --> 00:04:19.750
I really enjoy reading this book.

00:04:19.750 --> 00:04:24.200
If you specify and size or window size of two,

00:04:24.200 --> 00:04:24.960
it'll grip.

00:04:24.960 --> 00:04:28.350
It'll group two words together and three and so on,

00:04:28.350 --> 00:04:31.750
and I won't discuss each one of these in detail.

00:04:31.750 --> 00:04:36.170
That gets a bit outside of the scope in a bit too involved in the details off the types of

00:04:36.170 --> 00:04:37.030
transformations.

00:04:37.030 --> 00:04:42.230
But just know that these are the main tribes of transformations that AWS uses.

00:04:42.230 --> 00:04:46.100
If you're using the AWS service for doing the transformations,

00:04:46.100 --> 00:04:49.580
if you're not going to do him by yourself or on your own now,

00:04:49.580 --> 00:04:52.040
it also has a data rearrangement functionality,

00:04:52.040 --> 00:04:56.830
which enables you to create a data source that is based on on Lee a portion off the input

00:04:56.830 --> 00:04:58.060
data that it points to.

00:04:58.060 --> 00:04:58.660
So,

00:04:58.660 --> 00:04:59.370
for example,

00:04:59.370 --> 00:05:04.710
when you create a model using the wizard and choose a default evaluation option,

00:05:04.710 --> 00:05:09.180
it automatically reserves 30% for evaluation and uses 70 for training.

00:05:09.180 --> 00:05:14.460
Now this functionality is enabled by the data rearrangement feature off Amazon machine

00:05:14.460 --> 00:05:14.860
learning.

00:05:14.860 --> 00:05:20.640
I can use the Amazon machine learning a P I or command line interface.

00:05:20.640 --> 00:05:25.620
If you want to change some of those parameters and it does allow you to do,

00:05:25.620 --> 00:05:28.240
it does allow you to change certain parameters such as,

00:05:28.240 --> 00:05:28.660
for example,

00:05:28.660 --> 00:05:34.030
of percent begin to indicate where the data for the data source starts or the percent,

00:05:34.030 --> 00:05:37.160
and indicate where the data for the data source ends.

00:05:37.160 --> 00:05:43.380
The compliment primer tells AWS to use data that is not included in the range of percent

00:05:43.380 --> 00:05:44.310
begin 2%.

00:05:44.310 --> 00:05:49.710
And and the Koplan parameter is useful if you need to create complementary data sources for

00:05:49.710 --> 00:05:51.560
training and for evaluation.

00:05:51.560 --> 00:05:53.550
And then there's also the strategy,

00:05:53.550 --> 00:05:59.830
which splits the data for our data sores from other than the 70 30 default that it does it

00:05:59.830 --> 00:05:59.830
.

00:05:59.830 --> 00:06:03.910
So if you want to use 50 50 50 for training and 50 for evaluation,

00:06:03.910 --> 00:06:08.440
you can use a strategy stream to go ahead and change those options because got a good or

00:06:08.440 --> 00:06:13.400
you off the types of data transformations that are offered by AWS and how important it is

00:06:13.400 --> 00:06:19.810
that we make sure that our data is transformed properly and correctly in order to make sure

00:06:19.810 --> 00:06:25.070
that our machine learning model predicts the correct answers and learns properly and trains

00:06:25.070 --> 00:06:26.110
itself properly

