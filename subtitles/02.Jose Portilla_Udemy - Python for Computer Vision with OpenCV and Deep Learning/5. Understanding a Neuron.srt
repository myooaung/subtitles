1
00:00:06,090 --> 00:00:12,240
Welcome everyone to this lecture on an introduction to the perception before we launch straight into

2
00:00:12,240 --> 00:00:18,880
neural networks we to understand the individual components first such as a single neuron artificial

3
00:00:18,880 --> 00:00:22,840
neural networks or Amen's actually have a basis in biology.

4
00:00:22,840 --> 00:00:27,580
So we're going to see how we can attempt to mimic biological neurons with an artificial neuron otherwise

5
00:00:27,580 --> 00:00:28,900
known as a perception.

6
00:00:29,080 --> 00:00:33,310
And then once we go through the process of how a simple perception works we'll go ahead and show you

7
00:00:33,310 --> 00:00:36,400
how you can represent that mathematically.

8
00:00:36,500 --> 00:00:40,630
But let's start off of a biological neuron such as a brain cell.

9
00:00:41,030 --> 00:00:46,070
So if biological neuron works as in a simplified way through the following manner.

10
00:00:46,250 --> 00:00:51,130
Basically you have dendrites that feed into the body of this cell and you can have many then drives.

11
00:00:51,320 --> 00:00:55,970
And what happens is an electrical signal gets passed through the dendrites to the body of the cell and

12
00:00:55,970 --> 00:01:01,850
then later on a single output or a single electrical signal is passed on through an axon to later on

13
00:01:01,850 --> 00:01:04,250
Connect to some other neuron.

14
00:01:04,250 --> 00:01:09,350
And that's the basic idea we have kind of these mini input Erlick of electro signals through the dendrites

15
00:01:09,440 --> 00:01:15,600
goes with the body and then a single actual signal output through the axon So the artificial neuron

16
00:01:15,720 --> 00:01:17,250
also has inputs and outputs.

17
00:01:17,300 --> 00:01:24,230
So an attempt to mimic the biological neuron so this simple model again is just known as a perception.

18
00:01:24,250 --> 00:01:26,040
In this case we have two inputs.

19
00:01:26,200 --> 00:01:29,010
So let's go ahead and see a simple example of how it can work.

20
00:01:29,800 --> 00:01:35,380
So we have two inputs and a single output and we start indexing at zero so we have input of zero input

21
00:01:35,380 --> 00:01:37,450
one.

22
00:01:37,510 --> 00:01:39,850
So the inputs can have values of features.

23
00:01:39,850 --> 00:01:43,900
So when you have your data set you're going to have various features and these features can be anything

24
00:01:43,900 --> 00:01:50,290
from how many rooms a house has or how dark an image is represented by some sort of pixel amount or

25
00:01:50,290 --> 00:01:55,210
some sort of Darkness number etc. So essentially Don't worry right now about what these numbers actually

26
00:01:55,210 --> 00:01:56,050
represent.

27
00:01:56,050 --> 00:01:59,420
Later on we're dealing with real data sets we'll have something more tangible.

28
00:01:59,470 --> 00:02:01,960
For right now these are just arbitrary number choices.

29
00:02:02,110 --> 00:02:04,330
But again we have input zero and Input 1.

30
00:02:04,330 --> 00:02:11,450
So again indexing starts at zero here and we're going to assign them values of let's say 12 and 4 then

31
00:02:11,450 --> 00:02:15,280
the next step is to have these inputs multiplied by some sort of way.

32
00:02:15,530 --> 00:02:22,540
So we have weight zero for input zero and weight one for Input 1 and typically the weights are actually

33
00:02:22,540 --> 00:02:25,240
initialized through some sort of random generation.

34
00:02:25,240 --> 00:02:28,620
So you just choose a random number for these weights.

35
00:02:28,730 --> 00:02:34,470
In this case we'll just go ahead and pretend that the random numbers chosen was 0.5 and negative 1.

36
00:02:34,520 --> 00:02:36,980
Again the numbers here are pretty much arbitrary.

37
00:02:36,980 --> 00:02:40,240
Really focus on the general process.

38
00:02:40,250 --> 00:02:43,750
So now these inputs are going to be multiplied by the weights.

39
00:02:43,860 --> 00:02:45,060
So that ends up looking like this.

40
00:02:45,060 --> 00:02:48,550
We have 12 times 0.5 or 1 1/2 and that equals six.

41
00:02:48,570 --> 00:02:54,500
And then we say four times negative one is equal to negative for the next step is to take these results

42
00:02:54,530 --> 00:02:59,910
of the inputs multiplied by their respective weights and pass them into an activation function.

43
00:03:00,950 --> 00:03:03,200
There's many activation functions to choose from.

44
00:03:03,200 --> 00:03:06,440
Now we're going to cover this in a lot more detail later on.

45
00:03:06,440 --> 00:03:13,150
For now our activation function is actually going to be very simple we're going to say the following.

46
00:03:13,290 --> 00:03:20,130
If the sum of the inputs is positive return one or output one if the sum of the inputs is negative then

47
00:03:20,190 --> 00:03:22,470
output 0.

48
00:03:22,470 --> 00:03:27,630
So in our case if we say six plus negative four that's the same thing as saying six minus four.

49
00:03:27,630 --> 00:03:29,510
So then we get to as a result.

50
00:03:29,530 --> 00:03:37,070
So since the sum of the inputs was positive the activation function returns 1 or outputs 1.

51
00:03:37,180 --> 00:03:39,340
So there's a possible issue here.

52
00:03:39,400 --> 00:03:41,130
And the issue is the following.

53
00:03:41,230 --> 00:03:42,640
What is the original inputs.

54
00:03:42,640 --> 00:03:46,010
Started off as zero.

55
00:03:46,040 --> 00:03:51,890
Any way you chose even if it was randomly chosen multiplied by the input would still result in zero.

56
00:03:51,890 --> 00:03:58,010
So if input 0 happened to be zero and input 1 happened to be zero as well instead of 12 and 4 then you

57
00:03:58,010 --> 00:03:58,860
could have a problem.

58
00:03:58,880 --> 00:04:05,190
You essentially always get out zero because zero multiplied by the way is still going to be zero.

59
00:04:05,210 --> 00:04:07,960
So we can actually fix this by adding in a biased term.

60
00:04:08,000 --> 00:04:09,700
And in this case we choose one.

61
00:04:09,890 --> 00:04:15,070
So we're going to go ahead and add in our own biased term here plus one.

62
00:04:15,090 --> 00:04:16,410
So what does this actually look like.

63
00:04:16,410 --> 00:04:22,250
Mathematically Now let's quickly think about how we can represent this perception model mathematically

64
00:04:22,430 --> 00:04:24,030
and it's actually quite simple.

65
00:04:24,050 --> 00:04:30,800
Basically we just say the following from equal zero to n that is the number of inputs we're gonna say.

66
00:04:30,830 --> 00:04:37,010
W If I were that specific weight for the input multiplied by X of II which is the input itself plus

67
00:04:37,010 --> 00:04:37,810
the bias term.

68
00:04:37,880 --> 00:04:40,370
And that's essentially it.

69
00:04:40,370 --> 00:04:46,830
So once we have many preset trons in a network we'll see how we can easily extend this to a matrix form.

70
00:04:46,860 --> 00:04:51,700
So as a quick review in this pretty simple lecture we just covered briefly the following.

71
00:04:51,840 --> 00:04:57,990
We discussed in the very basic terms how biological neuron works then how that can reflect to a perception

72
00:04:57,990 --> 00:05:03,730
model which is the artificial neuron and then the mathematical representation of that perception model.

73
00:05:03,870 --> 00:05:08,790
Coming up next we're going to continue discussing how we can build off this perception model to build

74
00:05:08,850 --> 00:05:10,140
a neural network.

75
00:05:10,140 --> 00:05:11,740
Thanks and I'll see at the next lecture.
