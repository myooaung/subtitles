1
00:00:06,430 --> 00:00:08,770
Welcome back everyone in this lecture.

2
00:00:08,770 --> 00:00:15,990
We're going to discuss cost functions we'll now explore how can we evaluate the performance of in Europe

3
00:00:16,560 --> 00:00:22,390
we can use a cost function to measure how far off we are from the expected value.

4
00:00:22,440 --> 00:00:25,080
So we're going to use the following variables and notation.

5
00:00:25,080 --> 00:00:30,000
Let's quickly review this so you don't get confused when we actually see the cost functions we'll use

6
00:00:30,000 --> 00:00:36,690
the variable y to represent the true value and we'll use the variable A to represent the neurons prediction.

7
00:00:36,960 --> 00:00:39,380
So what does this actually mean in terms of weights and biases.

8
00:00:39,390 --> 00:00:46,040
Well remember that we said W times x that is the wait times the input plus the bias is equal to Z.

9
00:00:46,140 --> 00:00:49,920
And remember that we pass Z into the activation function.

10
00:00:49,920 --> 00:00:56,450
So for example if we're using the sigmoid function we could say that sigma of Z is equal to eight.

11
00:00:56,670 --> 00:01:03,000
So that neuron's prediction when we actually put in the some of those weights times the inputs plus

12
00:01:02,990 --> 00:01:08,710
those biased terms we get the Z and then we can pass that in to the activation function sigma of Z.

13
00:01:08,880 --> 00:01:10,010
And that is equal to a.

14
00:01:10,050 --> 00:01:14,500
Which at the end of represents the neurons prediction.

15
00:01:14,520 --> 00:01:20,040
So the first cost function we're going to discuss is the quadratic cost function and that is equal to

16
00:01:20,430 --> 00:01:25,490
the sum of y minus a squared divided by N.

17
00:01:25,590 --> 00:01:27,530
And you can if you want pull out one over.

18
00:01:27,530 --> 00:01:30,130
And so what do we actually see here.

19
00:01:30,120 --> 00:01:35,520
Well we can see that larger errors are more prominent due to the squaring because remember why is that

20
00:01:35,520 --> 00:01:37,890
true value and A's are predictive value.

21
00:01:38,070 --> 00:01:42,960
So if there's a big difference there when we square it's going to really be prominent now.

22
00:01:43,050 --> 00:01:48,300
Unfortunately when you actually implement this quadratic costs function this collation could cause a

23
00:01:48,300 --> 00:01:53,700
slowdown in our learning speed due to the way that formulaically it actually works.

24
00:01:54,430 --> 00:02:00,190
So instead we're going to be using cross entropy as our cost function and this is defined in the following

25
00:02:00,190 --> 00:02:09,010
manner we say negative 1 over and and then we take the sum of Y times the log a plus one minus Y times

26
00:02:09,010 --> 00:02:10,820
the log of 1 minus a.

27
00:02:10,990 --> 00:02:14,740
And this actual cost function allows for faster learning.

28
00:02:14,770 --> 00:02:17,080
And the reason for that is the following.

29
00:02:17,140 --> 00:02:23,350
The larger the difference between y and a D faster than your on ends up actually learning due to this

30
00:02:23,350 --> 00:02:25,850
cross and should be cost function.

31
00:02:25,930 --> 00:02:30,910
So that means that if we end up at the beginning for really large difference between our predicted value

32
00:02:31,150 --> 00:02:37,120
and the true value we can essentially quickly jump forward using this cost function because the larger

33
00:02:37,120 --> 00:02:40,970
the difference the faster neuron's going to learn.

34
00:02:41,050 --> 00:02:46,450
So now that we have these two key aspects of learning of general networks the neurons with their activation

35
00:02:46,450 --> 00:02:49,780
function and the cost function we're still missing a key step.

36
00:02:49,780 --> 00:02:52,590
And that's the actual learning process.

37
00:02:53,080 --> 00:02:58,960
So we need to figure out how we can use our neurons and the measurements of our air our cost function

38
00:02:58,990 --> 00:03:02,950
basically a measurement of how wrong we are and then attempt to correct our prediction.

39
00:03:02,950 --> 00:03:04,980
In other words actually learn.

40
00:03:05,260 --> 00:03:11,260
So again so far we understand the neurons the precept trons how we can link them together to get a neural

41
00:03:11,260 --> 00:03:16,180
net and then we also understand now that we have these cost functions that are essentially measurements

42
00:03:16,180 --> 00:03:20,100
of how off we are how wrong we are what our error in general is.

43
00:03:20,320 --> 00:03:24,960
So now that we have these two components we need to understand well how do we actually fix that air

44
00:03:25,890 --> 00:03:30,310
and we're going to do is the next lecture we're going to briefly cover how we can do this with gradient

45
00:03:30,310 --> 00:03:32,570
descent which is probably a term you've heard before.

46
00:03:32,680 --> 00:03:34,920
If you've ever read anything on machine learning.

47
00:03:35,110 --> 00:03:35,600
OK.

48
00:03:35,740 --> 00:03:38,430
Let's go ahead and discuss gradient descent in the next lecture.

49
00:03:38,470 --> 00:03:39,210
I'll see it there.
