WEBVTT
1
00:00:05.720 --> 00:00:08.810
Welcome back everyone to an introduction to neural networks.

2
00:00:10.320 --> 00:00:12.750
We've seen how a single percept Trump behaves.

3
00:00:12.750 --> 00:00:16.480
Now let's expand on this concept to get the idea of a neural network.

4
00:00:16.680 --> 00:00:22.950
Let's see how to connect many perceptions together and then how to represent this mathematically.

5
00:00:22.960 --> 00:00:25.830
So what does a multiple perceptions trends that work actually look like.

6
00:00:25.840 --> 00:00:27.590
Well essentially looks like this.

7
00:00:27.640 --> 00:00:32.710
Here we can see that we have various layers of single percept trons connected to each other through

8
00:00:32.710 --> 00:00:35.080
their inputs and outputs.

9
00:00:35.100 --> 00:00:38.780
In this case we have an input layer all the way on the left which is purple.

10
00:00:38.820 --> 00:00:44.220
We have two hidden layers and hidden layers are the layers that are in between the input layer and all

11
00:00:44.220 --> 00:00:49.380
the way on the right the output layer essentially hidden layers are layers that don't get to see the

12
00:00:49.380 --> 00:00:49.980
outside.

13
00:00:49.980 --> 00:00:56.170
That is all the way the inputs on the left or all the way the outputs on the right so input layers those

14
00:00:56.170 --> 00:00:57.680
are real values from the data.

15
00:00:57.680 --> 00:01:00.760
So they take in the actual data as their input.

16
00:01:00.820 --> 00:01:04.950
Then you have hidden layers and those are the layers between the input and output layers.

17
00:01:05.020 --> 00:01:10.960
And if you have three or more hidden layers that's considered a deep network and then an output layer

18
00:01:11.080 --> 00:01:17.650
where you have some sort of final estimate of whatever the output that you're trying to estimate is.

19
00:01:17.870 --> 00:01:22.670
So as you go forwards through more layers the level of abstraction increases.

20
00:01:22.820 --> 00:01:26.090
Let's not discuss the activation function in a little more detail.

21
00:01:28.110 --> 00:01:33.600
Previously our activation function was just a really simple function the output 0 or 1.

22
00:01:33.630 --> 00:01:37.980
Remember we were just taking the some of the inputs in that simple percept Tron model and then had an

23
00:01:38.040 --> 00:01:44.430
activation function that just had an output of either or one based off whether that number was positive

24
00:01:44.460 --> 00:01:45.210
or negative.

25
00:01:45.370 --> 00:01:51.720
And if we plotted out that looks like this we can say the output is either 0 or 1 on the y axis and

26
00:01:51.720 --> 00:01:54.480
then the x axis we see our marking at zero.

27
00:01:54.630 --> 00:02:01.020
And if we have a negative value we output 0 and if we have a positive value we output 1 and we did this

28
00:02:01.020 --> 00:02:04.450
by having the x axis B W X plus B.

29
00:02:04.550 --> 00:02:07.810
That is the weights times the inputs plus that bias.

30
00:02:07.950 --> 00:02:10.410
And it's really common to just set that equal to Z.

31
00:02:10.530 --> 00:02:15.160
So you can refer to Z instead of having to constantly for two W X plus B.

32
00:02:15.180 --> 00:02:23.230
So whenever I say Z I'm essentially inferring to those wait times those inputs plus that bias So unfortunately

33
00:02:23.230 --> 00:02:27.350
this is actually a pretty dramatic function since small changes aren't really reflected.

34
00:02:27.550 --> 00:02:30.550
So you can have kind of really small changes in Z.

35
00:02:30.790 --> 00:02:37.270
For instance the change from 0.5 in Zeitz 0.6 doesn't really matter it's still going to output one as

36
00:02:37.270 --> 00:02:41.980
long as it's positive or the changes that are really dramatic from negative ones to negative a thousand

37
00:02:42.370 --> 00:02:43.160
doesn't really matter.

38
00:02:43.180 --> 00:02:45.350
It's going to be zero.

39
00:02:45.680 --> 00:02:48.370
So it'd be nice if we could have a more dynamic function.

40
00:02:48.380 --> 00:02:54.680
For example this red line here you can see here that our outputs now are no longer just 0 or 1.

41
00:02:54.680 --> 00:02:56.600
We have a little more detail to it.

42
00:02:56.630 --> 00:02:57.950
There's a little more nuance here.

43
00:02:58.980 --> 00:03:01.320
So lucky for us this is actually the sigmoid function.

44
00:03:01.320 --> 00:03:07.050
This red line function already exists and it's known as the sigmoid function and they said my function

45
00:03:07.110 --> 00:03:14.800
is just f of x is equal to 1 over 1 plus E to the negative x and we can replace X here with Z.

46
00:03:14.820 --> 00:03:17.450
So we have zees equal to W X plus be.

47
00:03:17.460 --> 00:03:21.310
So we just take the whole thing that Z and plug it into the sigmoid function.

48
00:03:21.390 --> 00:03:28.260
So we'd have f of Z is equal to 1 over 1 plus it's the negative Z and that would be our sigmoid function

49
00:03:28.410 --> 00:03:29.800
for the activation function.

50
00:03:30.930 --> 00:03:35.970
So changing the activation function used can actually be really beneficial depending on the particular

51
00:03:35.970 --> 00:03:41.220
task we're trying to complete and we'll get into more detail on that later on when we actually implement

52
00:03:41.280 --> 00:03:42.880
our own that works.

53
00:03:44.080 --> 00:03:49.000
Let's discuss a few more activation of functions that we're going to encounter throughout this course.

54
00:03:49.150 --> 00:03:54.490
So a really common function that's used as an activation function besides a Sigmond function is the

55
00:03:54.490 --> 00:04:01.420
hyperbolic tangent function or 10 H or tranche of Z where Z remembers just double x plus B and that

56
00:04:01.420 --> 00:04:08.050
looks really similar to the sigmoid function except that the output can then be from negative 1 to 1.

57
00:04:08.100 --> 00:04:10.480
And like previously we had 0 to 1.

58
00:04:10.480 --> 00:04:16.030
So if we look at the sigmoid function again that went from 0 to 1 and that was equal to 1 over 1 plus

59
00:04:16.110 --> 00:04:19.140
it's a negative x or negative Z whatever the input is.

60
00:04:19.360 --> 00:04:25.160
And now for the Tench or hyperbolic tangent function it goes from negative 1 to 1 but still has that

61
00:04:25.160 --> 00:04:26.730
sort of same behavior shape.

62
00:04:26.790 --> 00:04:31.870
And if you're interested in how to express the hyperbolic tangent mathematically over here on the right

63
00:04:31.870 --> 00:04:41.640
hand side or the actual equations for it then there's also the rectified linear unit or R E L U and

64
00:04:41.640 --> 00:04:45.630
this is a really common activation function but it's actually relatively simple.

65
00:04:45.750 --> 00:04:48.370
And the way it works is the following method.

66
00:04:48.750 --> 00:04:52.350
You basically passengers the value into this Max equation.

67
00:04:52.350 --> 00:04:55.180
So you say Max of 0 or Z.

68
00:04:55.500 --> 00:04:56.710
So what does that actually mean.

69
00:04:56.760 --> 00:05:01.340
Well the rectified linear unit that actual activation function works the following manner.

70
00:05:01.530 --> 00:05:06.830
It says they suffer z value and zero just return the maximum value.

71
00:05:07.200 --> 00:05:13.440
And that actually means that if your value of z is negative then the max between 0 and any negative

72
00:05:13.440 --> 00:05:15.050
number is always going to be zero.

73
00:05:15.240 --> 00:05:17.910
So we can see here that is z.

74
00:05:18.000 --> 00:05:21.640
If it's negative the function always returns 0.

75
00:05:21.840 --> 00:05:28.530
And then between Max 0 comma Z if z is positive it's always going to return the value of z.

76
00:05:28.710 --> 00:05:32.130
So essentially of just y is equal to Z here.

77
00:05:32.130 --> 00:05:35.960
So on the right hand side that's where you get that sort of straight linear line.

78
00:05:35.970 --> 00:05:41.280
So again relatively simple function but this is kind of considered at certain points in time state of

79
00:05:41.280 --> 00:05:41.840
the art.

80
00:05:42.030 --> 00:05:46.940
So we're going to definitely be seeing this activation function a lot throughout our lectures.

81
00:05:46.980 --> 00:05:51.700
So as I mentioned rectified linear units and the hyperbolic tangent tend to have the best performance.

82
00:05:51.720 --> 00:05:54.400
So we're really going to be focusing on these two.

83
00:05:54.480 --> 00:05:59.250
Lucky for us the deep learning libraries we're working with specifically carriers already have all of

84
00:05:59.250 --> 00:06:01.580
these activation functions built in.

85
00:06:01.620 --> 00:06:04.470
So it's going to be just simply adding in a string code.

86
00:06:04.570 --> 00:06:07.780
Are you for rectified linear unit or so on.

87
00:06:07.980 --> 00:06:12.090
Now as we continue we'll also talk about some more of the state of the art activation functions and

88
00:06:12.090 --> 00:06:13.730
how it can play around them.

89
00:06:13.730 --> 00:06:18.420
Up next we're going to be discussing cost functions which allow us to measure how well these neurons

90
00:06:18.420 --> 00:06:19.550
are actually performing.

91
00:06:19.740 --> 00:06:24.430
So we're kind of halfway through the general understanding and fundamentals of deep learning.

92
00:06:24.620 --> 00:06:26.660
But let's continue by talking about cost functions.

93
00:06:26.700 --> 00:06:29.070
And then after that grading the sense and back propagation.
