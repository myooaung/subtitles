WEBVTT
1
00:00:05.540 --> 00:00:07.820
Welcome back everyone in this lecture.

2
00:00:07.820 --> 00:00:10.820
We're going to go over Karris basics.

3
00:00:10.850 --> 00:00:14.840
We're going to learn how to create a machine learning model with the cursed library with tensor flow

4
00:00:15.090 --> 00:00:18.640
back and we'll start with some data on currency banknotes.

5
00:00:18.740 --> 00:00:22.760
Some of these bank notes were forgeries and others were legitimate.

6
00:00:22.760 --> 00:00:28.670
The researchers created a data set from these bank notes by taking 400 by 400 pixel images of the notes

7
00:00:28.910 --> 00:00:35.270
and then extracting various numerical features based off the wavelets of those images something that's

8
00:00:35.270 --> 00:00:40.190
very important to note here is for this particular lecturer the data we're working with is actually

9
00:00:40.190 --> 00:00:42.220
not image data directly.

10
00:00:42.260 --> 00:00:47.090
We're focusing right now and how to use carriers for general machine learning instead of reusing features

11
00:00:47.090 --> 00:00:49.100
that were extracted from images.

12
00:00:50.160 --> 00:00:54.840
Once we learn about convolutional neural that works later on in the section then we can expand on carriers

13
00:00:54.930 --> 00:00:59.690
to feed and image data that is actual pixel images that are rectally into a network.

14
00:00:59.700 --> 00:01:04.860
So right now we're just focused on how to use keras that general syntax and how to perform a machine

15
00:01:04.860 --> 00:01:11.130
learning task such as getting data reading it in splitting it into a training set and a test set fitting

16
00:01:11.130 --> 00:01:16.130
the model to that training data and then predicting a new unseen data such as the test set.

17
00:01:16.460 --> 00:01:18.330
OK let's get started.

18
00:01:18.330 --> 00:01:18.690
All right.

19
00:01:18.690 --> 00:01:24.440
I'm going to begin by importing some pie as an MP and then also from some pie.

20
00:01:24.630 --> 00:01:29.310
I'm going to import this function called Jeon from text.

21
00:01:29.340 --> 00:01:33.040
And what this does is it generates an array from a text file.

22
00:01:33.090 --> 00:01:38.520
The data we have for you has already been organized as what's known as a CSB file or a comma separated

23
00:01:38.520 --> 00:01:39.480
file.

24
00:01:39.600 --> 00:01:44.140
So we will say Gen from text and then from the data folder.

25
00:01:44.190 --> 00:01:52.470
Go ahead and read in bank note data that TSC but just make sure you passen the delimiter parameter with

26
00:01:52.470 --> 00:01:54.660
a string and then a comma inside that string.

27
00:01:54.690 --> 00:01:58.750
That basically indicates that the actual features are separated by commas.

28
00:01:59.220 --> 00:02:00.640
OK so now we have our data.

29
00:02:00.660 --> 00:02:06.510
If we take a look that we have these various columns of features and you'll notice here on the end we

30
00:02:06.510 --> 00:02:12.520
have zeros and ones where that basically indicates whether or not it was an actual authentic note.

31
00:02:12.540 --> 00:02:15.090
So if it's zero that means it was a forgery.

32
00:02:15.090 --> 00:02:17.100
If it is one that means it was an authentic.

33
00:02:17.100 --> 00:02:19.680
So this is known as the label or class.

34
00:02:19.680 --> 00:02:25.080
So we're going to classify or build a machine learning model that can classify these bank notes just

35
00:02:25.080 --> 00:02:27.600
based off of these features right here.

36
00:02:27.600 --> 00:02:32.190
So we're going to feed in these features and then later on we're going to be able to predict the zero

37
00:02:32.280 --> 00:02:39.750
or one class in order to do that or we're going to do is start by separating out the label from the

38
00:02:39.750 --> 00:02:40.830
actual features.

39
00:02:41.910 --> 00:02:48.390
So the labels themselves that is the last column they're going to say give me all the rows of that last

40
00:02:48.390 --> 00:02:54.510
column index for so if you check out the labels now have an array of all the zeros and ones.

41
00:02:54.650 --> 00:02:57.250
So that's important here is it looks like these are sorted.

42
00:02:57.440 --> 00:03:02.940
So we're going to take care of that later on a train test split but I also want to grab the features.

43
00:03:03.200 --> 00:03:05.200
So my features are right here.

44
00:03:05.210 --> 00:03:09.530
So these feature columns so I will say features equal to data.

45
00:03:10.250 --> 00:03:12.110
And I don't regret everything but the fourth column.

46
00:03:12.120 --> 00:03:15.360
So go 0 to 4 so not including four.

47
00:03:15.550 --> 00:03:17.060
I take a look at those features.

48
00:03:17.330 --> 00:03:20.610
It's just the numerical features it no longer has that label.

49
00:03:20.930 --> 00:03:23.410
So I have my labels or my glasses there.

50
00:03:23.420 --> 00:03:25.010
Those terms are interchangeable.

51
00:03:25.250 --> 00:03:27.340
And then I also have my features.

52
00:03:27.590 --> 00:03:32.510
It's time to actually split the data into a training and test set by convention.

53
00:03:32.510 --> 00:03:38.760
We use a capital X for features and a lowercase y for labels.

54
00:03:38.970 --> 00:03:43.100
That's just because that's the mathematical notation used in machine learning papers.

55
00:03:43.100 --> 00:03:49.460
We have a capital X due to the fact that the features is usually a to d matrix and Y is then a singular

56
00:03:49.520 --> 00:03:52.040
array vector but it's up to you.

57
00:03:52.040 --> 00:03:56.210
You can keep features and labels if you want as your variable names but we'll go ahead and keep capital

58
00:03:56.210 --> 00:03:57.950
X and lowercase y for now.

59
00:03:59.210 --> 00:04:03.390
The next step as I mentioned is to split the data into a training and test set.

60
00:04:03.500 --> 00:04:10.420
We're going to do this with the sikat learn library will say from S.K. learn the model selection and

61
00:04:10.430 --> 00:04:17.440
you can tab out a complete that import the train test split function.

62
00:04:17.690 --> 00:04:24.260
And what this does is it's going to split up the features and the labels into a training set and a test

63
00:04:24.260 --> 00:04:24.670
set.

64
00:04:24.800 --> 00:04:28.970
And what's really nice about this train test split is that also those randomized shuffling.

65
00:04:28.970 --> 00:04:33.850
So we don't need to worry about this concern that the labels happen to be in sorted order.

66
00:04:33.950 --> 00:04:36.520
This will automatically shuffle them for us.

67
00:04:36.890 --> 00:04:38.980
So we're going to do the following.

68
00:04:39.080 --> 00:04:42.790
We'll start typing out train to split and then do shift tab here.

69
00:04:42.950 --> 00:04:47.340
And if you scroll all the way to the bottom you get this nice example of how to use it.

70
00:04:47.660 --> 00:04:53.570
And if you keep scrolling down you get this example right here trying to split x y to size in random

71
00:04:53.570 --> 00:04:54.250
state.

72
00:04:54.440 --> 00:05:02.600
Go ahead and copy that and then just replace what's in your cell with that line and then go ahead and

73
00:05:03.320 --> 00:05:06.320
Luray those ellipses is three dots there.

74
00:05:06.350 --> 00:05:06.760
All right.

75
00:05:06.860 --> 00:05:08.840
So let me explain what's going on here.

76
00:05:08.840 --> 00:05:15.320
All we're saying is passen your X features and then passing your y labels choose your test size.

77
00:05:15.310 --> 00:05:20.870
So 30 percent of the data here 33 percent a third of that data is going to go to the test set.

78
00:05:20.870 --> 00:05:26.480
So 33 percent of the features will be an X test 33 percent of the labels will be in white test and then

79
00:05:26.480 --> 00:05:33.380
this random state 42 as I mentioned this data is going to be shuffled before we actually split it into

80
00:05:33.380 --> 00:05:35.080
a training set and a test set.

81
00:05:35.190 --> 00:05:41.360
So to make sure you get the same shuffle every time you can set a random state and 42 is just an arbitrary

82
00:05:41.360 --> 00:05:42.140
value.

83
00:05:42.140 --> 00:05:46.430
You'll see a lot of programs use 42 because of the Hitchhiker's Guide To The Galaxy book.

84
00:05:46.430 --> 00:05:51.020
That's the answer to everything in the universe so if you're ever curious about why 42 is a default

85
00:05:51.240 --> 00:05:53.580
in so many Python things that's the answer.

86
00:05:54.050 --> 00:05:54.340
OK.

87
00:05:54.350 --> 00:06:00.170
Now that we've done that we can go ahead and check out our data so we can check out if I can spell it

88
00:06:00.420 --> 00:06:01.310
X train.

89
00:06:02.260 --> 00:06:07.230
So here we have extreme And let's take a look at the length of X train.

90
00:06:07.540 --> 00:06:09.550
It's nine hundred and nineteen.

91
00:06:09.550 --> 00:06:15.530
If I take a look at the length of X before with the split It was 1972.

92
00:06:15.760 --> 00:06:21.490
Which means if I check the length of X test now it's the remaining 33 percent.

93
00:06:21.490 --> 00:06:24.830
So 60 percent of my data for the features is now an X train.

94
00:06:25.000 --> 00:06:29.710
Thirty three percent of it is now an X test so x train an X test.

95
00:06:29.710 --> 00:06:34.750
Those are the features that else have y train and y tests which took a look at as well.

96
00:06:34.750 --> 00:06:42.950
If I'm curious we have y train and it's just a bunch of ones and zeros and then we also have Y test

97
00:06:43.180 --> 00:06:43.480
again.

98
00:06:43.490 --> 00:06:50.370
Also a bunch of ones and zeros and they correspond based off their index to the rows of X test or X

99
00:06:50.370 --> 00:06:53.210
train now.

100
00:06:53.220 --> 00:07:00.450
Typically when working with neural networks it's a good idea to standardize or scale your data and we

101
00:07:00.450 --> 00:07:03.370
can do that with a convenient function from sikat learn.

102
00:07:03.750 --> 00:07:15.210
We will say from Eski learn pre-processing import min max scaler and all of this is going to do is it's

103
00:07:15.210 --> 00:07:22.140
going to force all the feature data to fall within a certain range and this can help the neural network

104
00:07:22.230 --> 00:07:24.860
actually perform better for this small data set.

105
00:07:24.870 --> 00:07:28.310
It's actually probably not necessary the range doesn't seem to be that high.

106
00:07:28.470 --> 00:07:32.310
And you can actually test for this by searching for the max.

107
00:07:32.310 --> 00:07:38.070
So the max is 17 that's about the same order of magnitude as the minimum.

108
00:07:38.070 --> 00:07:40.670
So the minimum is around 13.

109
00:07:40.680 --> 00:07:45.000
Now if you add a max value a something like a million and a minimum value of something like negative

110
00:07:45.000 --> 00:07:48.490
13 then you have orders of magnitude off in certain features.

111
00:07:48.510 --> 00:07:51.590
So then it would definitely be a good idea to do a min max scale.

112
00:07:51.630 --> 00:07:56.930
In our case it's probably not super necessary but it's always a good idea and it is free.

113
00:07:56.940 --> 00:07:59.430
You know the computers doing the work here so might as well do it.

114
00:07:59.520 --> 00:08:05.840
So we have been Max Giller and the way this works is you create a scalar object.

115
00:08:06.030 --> 00:08:10.790
So you say minimax Galer open and close print sees and now the scalar object.

116
00:08:10.790 --> 00:08:17.430
What we're going to do is we're going to fit it to our training data.

117
00:08:17.630 --> 00:08:24.150
We will say scalar objects that fit in person X train.

118
00:08:24.360 --> 00:08:30.360
So what this does is it basically just finds the minimum value and the maximum value then what it's

119
00:08:30.360 --> 00:08:37.500
going to do is it's going to transform whatever array you pasand such as X tests and x train based off

120
00:08:37.590 --> 00:08:40.590
the min and max that it just calculated during the fit.

121
00:08:40.710 --> 00:08:44.110
So it just lets it know what the minimax is.

122
00:08:44.130 --> 00:08:47.530
The next thing we need to do is actually do a transform.

123
00:08:47.760 --> 00:08:56.910
So will say scalar object thought transform and then pass an X train and this is going to return back

124
00:08:56.960 --> 00:09:06.280
the scaled versions will say scaled X train is equal to transform X train and then we'll do the same

125
00:09:06.280 --> 00:09:08.320
for the test data.

126
00:09:08.410 --> 00:09:17.270
So I'll say scalar object transform X test now a really common question here is why did we only fit

127
00:09:17.690 --> 00:09:18.780
to X train.

128
00:09:18.920 --> 00:09:24.860
Why did I not fit to all my data why they are not fit to X that is because we want to make sure that

129
00:09:24.860 --> 00:09:29.060
this scalar object doesn't get to peek at any test data.

130
00:09:29.120 --> 00:09:34.940
Otherwise it's kind of like cheating because we're not going to transform the X test having only fitted

131
00:09:35.060 --> 00:09:36.410
on X train.

132
00:09:36.410 --> 00:09:39.890
So if you were to on the entire data set that's known as data leakage.

133
00:09:39.890 --> 00:09:40.990
And that's essentially cheating.

134
00:09:41.000 --> 00:09:46.690
So I usually just want to fit to your training data and then transform both the train and the test.

135
00:09:46.790 --> 00:09:52.460
Having only fit to the training data otherwise you're assuming some knowledge of the test data that

136
00:09:52.550 --> 00:09:55.590
in real life you're not going to have OK.

137
00:09:55.840 --> 00:10:04.960
So now we're going to do is check out the max of these data sets so we can say scale x train and check

138
00:10:04.960 --> 00:10:10.690
out the max here and those the max has now been fitted to be one or pretty much close to one.

139
00:10:10.690 --> 00:10:12.840
This is usually just a floating point air.

140
00:10:13.150 --> 00:10:20.470
And if I take a look at my x train the original looks something like this now and if I take a look at

141
00:10:20.890 --> 00:10:27.630
the scale version it's now pretty much all between 0 and 1 are probably negative 1.

142
00:10:27.640 --> 00:10:28.790
Let's check them at home.

143
00:10:29.170 --> 00:10:29.420
Yep.

144
00:10:29.470 --> 00:10:31.630
So it's all between 0 and 1.

145
00:10:31.900 --> 00:10:32.670
OK.

146
00:10:32.680 --> 00:10:35.160
So we're able to successfully scale those.

147
00:10:35.170 --> 00:10:37.740
Both the training set and the test set.

148
00:10:37.750 --> 00:10:40.590
Now it's time to actually build a simple network with cameras.

149
00:10:40.620 --> 00:10:42.610
It is actually pretty straightforward.

150
00:10:42.910 --> 00:10:48.160
We say from Cara's thought models import sequential

151
00:10:52.020 --> 00:10:56.890
layers in poor dense and densely connected layer.

152
00:10:57.010 --> 00:11:01.750
And once you were in both of these you sometimes get a little warning sign that says using tensor flowback

153
00:11:01.780 --> 00:11:09.770
and if you happen to be using tensor or flow next I'm going to say model is equal to sequential.

154
00:11:10.130 --> 00:11:11.940
So this actually creates the model.

155
00:11:12.410 --> 00:11:14.100
And then I add in my layers.

156
00:11:14.360 --> 00:11:19.440
So I'm going to add in a dense layer that's that densely connected layer.

157
00:11:19.560 --> 00:11:22.770
And I'm going to say that it expects for features.

158
00:11:22.800 --> 00:11:25.220
Remember we have four columns in the feature right.

159
00:11:25.590 --> 00:11:30.490
And then I will say the input dimensions is equal to 4 as well.

160
00:11:31.910 --> 00:11:35.460
And the activation we're going to be using is the rectified linear unit.

161
00:11:35.700 --> 00:11:42.300
So right now we have four neurons in the Slayer the input dimension is four and the activation is rectified

162
00:11:42.300 --> 00:11:43.370
leading a unit.

163
00:11:43.380 --> 00:11:50.150
Let's go ahead and add another then Slayer are densely connected Lehre will say dense.

164
00:11:50.220 --> 00:11:52.700
And here is where you can play around the neurons.

165
00:11:52.710 --> 00:11:55.730
So right now this is the layer right in the middle of the neural network.

166
00:11:55.860 --> 00:11:57.450
It's up to you what how many neurons.

167
00:11:57.450 --> 00:12:02.900
One of the side if you go too large you will probably end up getting bad results if you also go to small.

168
00:12:02.970 --> 00:12:08.060
Here it's probably a good idea to go somewhere between 1 x and 2 x the input dimensions.

169
00:12:08.070 --> 00:12:10.840
So it's going to go with two X and say eight.

170
00:12:11.220 --> 00:12:14.310
And we're going to say activation is equal to.

171
00:12:14.310 --> 00:12:16.910
Also rectified linear unit.

172
00:12:17.040 --> 00:12:21.800
We don't need to provide an input then mentioned here because this is the layer that's the input layer.

173
00:12:21.810 --> 00:12:24.000
This is now kind of a hidden layer.

174
00:12:24.150 --> 00:12:30.960
And then finally we're going to have a last layer the output layer and this is going to be dense.

175
00:12:30.960 --> 00:12:34.740
And this is actually just going to be one year on because this is one year on.

176
00:12:34.740 --> 00:12:43.310
It only has one output either 0 or 1 and then the activation here will be sigmoid because remember the

177
00:12:43.310 --> 00:12:49.860
sigmoid function as described will be fit between 0 and 1 OK.

178
00:12:50.080 --> 00:12:55.000
So once you've set up the model as a sequential model and we've added all or layers that we want if

179
00:12:55.000 --> 00:12:58.940
you really wanted to you could add a bunch of hidden layers but that's unnecessary for this problem.

180
00:12:58.960 --> 00:13:04.660
Then we compile the model and for the compilation the model we need to choose a loss in optimizer and

181
00:13:04.660 --> 00:13:07.290
the metrics that we're concerned with during fitting.

182
00:13:07.300 --> 00:13:16.640
So we're going to do here same model compile and we're going to set our loss equal to binary underscore

183
00:13:17.450 --> 00:13:18.430
cross entropy

184
00:13:21.690 --> 00:13:33.620
and then set the optimizer equal to Adam and then set the metrics equal to just a list of a single string

185
00:13:33.740 --> 00:13:35.360
accuracy.

186
00:13:36.090 --> 00:13:36.650
OK.

187
00:13:36.650 --> 00:13:37.710
So go ahead and run that.

188
00:13:37.720 --> 00:13:39.290
That should have compiled the model.

189
00:13:39.290 --> 00:13:43.250
Now it's time to actually fit or train the model so we'll do the following.

190
00:13:43.250 --> 00:13:48.710
We say model fit and then we're going to fit the scaled training data.

191
00:13:48.710 --> 00:13:53.770
So we say scale x train and then we also provided the correct labels to train on.

192
00:13:53.810 --> 00:13:59.660
So we'll give it white train and then you can choose the number of e POCs one e Poch means you've gone

193
00:13:59.660 --> 00:14:02.030
through all the training data one time.

194
00:14:02.030 --> 00:14:06.920
So let's go ahead and do 50 epochs depending on how fast your computers you may want to choose a lower

195
00:14:06.920 --> 00:14:07.870
number.

196
00:14:08.210 --> 00:14:14.480
And then for both I like verbose too it gives you enough information basically verbose in parameters.

197
00:14:14.510 --> 00:14:16.930
Usually it means that it's reporting back it's going to print.

198
00:14:16.940 --> 00:14:18.740
Long as it's training.

199
00:14:18.740 --> 00:14:23.400
So let's go ahead and run this a should begin to see IPAC 1 out of 50.

200
00:14:23.480 --> 00:14:25.460
And eventually it'll start training.

201
00:14:25.490 --> 00:14:30.070
Keep in mind I have a really fast computer so yours may take slightly longer.

202
00:14:30.410 --> 00:14:31.900
So this ran for 50 epochs.

203
00:14:31.940 --> 00:14:36.940
And at the end of the day it looks like it was hovering almost around 99 percent accuracy.

204
00:14:36.950 --> 00:14:39.930
Remember this is the accuracy on the training set.

205
00:14:39.950 --> 00:14:45.010
We still don't know how it's going to do when it tries to predict on data that it hasn't seen before.

206
00:14:45.080 --> 00:14:46.930
This data is our test data.

207
00:14:47.360 --> 00:14:51.530
So remember we have our scaled X test data.

208
00:14:51.530 --> 00:14:55.250
This is essentially brand new data that the model hasn't seen before.

209
00:14:55.250 --> 00:15:00.410
So once we train the model the next question is how do we actually predict on new data.

210
00:15:00.530 --> 00:15:04.330
Well we can predict all the tests because the Tesa is essentially new data.

211
00:15:04.430 --> 00:15:06.520
It was never trained on the test set.

212
00:15:06.530 --> 00:15:13.250
So let's go ahead and show you how you can do that the way you do it is just saying model that predict

213
00:15:14.690 --> 00:15:20.480
predict underscore classes and then Passons scaled X test.

214
00:15:20.600 --> 00:15:25.340
And when you actually read that it's going to produce this array of zeros and ones that it predicts

215
00:15:25.670 --> 00:15:27.920
for the scaled X test.

216
00:15:27.920 --> 00:15:31.850
So that's nice for the predictions but how well did it do.

217
00:15:31.940 --> 00:15:33.460
Do we actually perform well.

218
00:15:33.710 --> 00:15:36.610
Well what we're going to do now is evaluate our model.

219
00:15:36.950 --> 00:15:44.730
So let me comment this out what I will say is the following model and I'm going to call metrics underscore

220
00:15:44.760 --> 00:15:50.450
names and the two metrics are actually the ones appear loss and accuracy.

221
00:15:50.640 --> 00:15:57.360
And we're going to be using those metrics to evaluate the model performance and we'll say from a Skillern

222
00:15:57.930 --> 00:16:06.770
metrics import confusion matrix and classification report and we're going to actually grab the predictions

223
00:16:06.770 --> 00:16:08.050
we showed last time.

224
00:16:08.150 --> 00:16:14.610
So I'm going to copy and paste this and set this equal to my predictions.

225
00:16:16.130 --> 00:16:20.900
So now that I have a list of predictions on the test set we already know the right answer.

226
00:16:20.930 --> 00:16:32.910
The right answer were the why test so I can say confusion matrix passen my why test and my predictions.

227
00:16:33.180 --> 00:16:34.680
And now I have my confusion matrix.

228
00:16:34.680 --> 00:16:39.090
And you can see it's only misidentifying seven big nodes and if you actually want to get things like

229
00:16:39.090 --> 00:16:44.260
precision recall an EF 1 score all you do is say Prince.

230
00:16:44.430 --> 00:16:52.450
The classification report on why test and predictions and if you run that now we can see that we're

231
00:16:52.450 --> 00:16:57.950
performing really well pretty much 90 percent precision recalling EF 1 score across the board.

232
00:16:58.090 --> 00:17:05.350
And if you want to say this model all you need to do is grab your model say save and then save it to

233
00:17:05.350 --> 00:17:06.070
whatever you want.

234
00:17:06.070 --> 00:17:12.160
You can say my super model and give it the extension dot h 5.

235
00:17:12.250 --> 00:17:19.550
And if you want to load the model or you need to do is say from Kurus dot models.

236
00:17:19.750 --> 00:17:27.710
Import load model and then say new model is equal to load model.

237
00:17:27.740 --> 00:17:30.220
And then you in wherever the model will save.

238
00:17:30.230 --> 00:17:36.860
So in this case we'll just say my super model H5 and then now that we have this new model and it's loaded

239
00:17:36.860 --> 00:17:39.780
in it loads and all the weights and you're ready to go.

240
00:17:39.800 --> 00:17:49.950
You can then call pre-date classes again on the scaled X test and then we have our classes.

241
00:17:50.220 --> 00:17:50.700
OK.

242
00:17:50.920 --> 00:17:52.550
So that's all there is to it.

243
00:17:52.660 --> 00:17:55.650
If you have any questions feel free to post the Q&amp;A forums.

244
00:17:55.750 --> 00:18:00.570
But that is the very basics of running cares and running a machine learning process.

245
00:18:00.640 --> 00:18:01.930
We'll see you at the next lecture.
