WEBVTT
1
00:00:04.380 --> 00:00:06.720
Alone will come back to the intro to this course.

2
00:00:06.840 --> 00:00:12.490
And today we're going to talk about deep learning and get to know what this whole field is all about.

3
00:00:12.810 --> 00:00:17.860
Learning is quite an advanced area of this science one of the most advanced areas.

4
00:00:18.070 --> 00:00:26.310
And originally deplorably came to be somewhere in the 70s and 80s and then it kind of faded away because

5
00:00:26.310 --> 00:00:32.220
computers were not powerful enough and now it's resurfaced again because computers are indeed much more

6
00:00:32.220 --> 00:00:37.140
powerful than back then and we can process lots more data and therefore deeply is relevant again.

7
00:00:37.510 --> 00:00:43.320
So here we've got a gentleman whose name is Jeffrey Hinton and he is also known as the godfather of

8
00:00:43.410 --> 00:00:44.760
deep learning.

9
00:00:45.210 --> 00:00:52.230
He worked on this problem back in the 80s and now that it's rough surface again he is one of the leading

10
00:00:52.230 --> 00:00:54.150
experts in the world of deep learning.

11
00:00:54.220 --> 00:00:56.490
And Jeffrey Hinton currently works for Google.

12
00:00:56.490 --> 00:01:00.750
You can find lots of talks with him on YouTube lots of his presentations.

13
00:01:00.750 --> 00:01:02.330
He's very good at explaining.

14
00:01:02.350 --> 00:01:09.030
Please if you ever need to reference something my personal first point of call is to check what this

15
00:01:09.060 --> 00:01:11.120
Jeffrey Intan has to say about it.

16
00:01:11.220 --> 00:01:15.480
And then if I don't find anything by him then I continue searching.

17
00:01:15.630 --> 00:01:16.920
So that's Jeffrey Hinton.

18
00:01:16.950 --> 00:01:23.220
And so what he is deploring all that well in order to understand the pattern we have to take a step

19
00:01:23.220 --> 00:01:26.170
back and actually delve a little bit into neuroscience.

20
00:01:26.170 --> 00:01:33.160
So we've got an image which portrays neurons smeared onto glass with some coloring applied.

21
00:01:33.420 --> 00:01:36.390
And specifically in the right image you can actually see them quite well.

22
00:01:36.540 --> 00:01:40.430
These are a specific type of neuron they're called motor in your hands.

23
00:01:40.560 --> 00:01:45.360
The reason they're used in this image is because they're generally bigger than typical neurons in the

24
00:01:45.360 --> 00:01:46.790
brain and therefore they're easier to see.

25
00:01:46.800 --> 00:01:51.740
But the neurons in the brain still have the same structure same concepts apply.

26
00:01:51.880 --> 00:01:59.280
And as you can see they have arms and legs of their own they have a body you have a nucleus inside you

27
00:01:59.280 --> 00:02:00.870
can clearly see the nucleus.

28
00:02:00.960 --> 00:02:07.950
And while we don't know what exactly happens the human brain and it's still being studied is lots of

29
00:02:07.980 --> 00:02:11.910
research being put into the human brain at the same time.

30
00:02:12.030 --> 00:02:19.800
We do know what it consists of and we do know what the overall pattern kind of is and the overall pattern

31
00:02:19.800 --> 00:02:25.470
is that there's tons and tons and tons of these neurons and they're all interconnected and they're sending

32
00:02:25.470 --> 00:02:28.940
electrical impulses to each other and somehow through.

33
00:02:28.950 --> 00:02:31.360
That's what we get is learning.

34
00:02:31.380 --> 00:02:37.050
And here is an example of a cross-section of the human brain.

35
00:02:37.230 --> 00:02:46.350
And they're in you can really see that there are tons of little neurons in there and in the human brain

36
00:02:46.350 --> 00:02:53.160
is actually a hundred billion neurons if you stop to think about that number for a second that's a hundred

37
00:02:53.310 --> 00:02:59.490
billion neurons or even a million or ten billion 100 billion and imagine how many connections there

38
00:02:59.490 --> 00:03:06.090
is between them when every neuron can connect to multiple different neurons to like hundreds or thousands

39
00:03:06.090 --> 00:03:07.070
different neurons.

40
00:03:07.350 --> 00:03:13.200
And this cross-sections actually of that part of the brain at the bottom which is responsible for your

41
00:03:13.200 --> 00:03:20.490
motor function and for keeping balance and staying staying upright and upon among among other things

42
00:03:20.490 --> 00:03:21.270
of course.

43
00:03:21.300 --> 00:03:25.440
And so yeah that's how the human brain structure there's lots of these neurons are interconnected and

44
00:03:25.440 --> 00:03:33.300
somehow through passing signals to each other we eventually what we get is human learning and memory

45
00:03:33.300 --> 00:03:34.160
retention.

46
00:03:34.320 --> 00:03:41.190
And so the idea behind deep learning is to take what evolution has given us to take this concept of

47
00:03:41.250 --> 00:03:44.370
interconnected neurons and recreate it artificially.

48
00:03:44.370 --> 00:03:49.920
And that's why they're called artificial neural networks as well because we're recreating these neural

49
00:03:49.920 --> 00:03:51.190
networks artificially.

50
00:03:51.300 --> 00:03:59.400
And the idea is to it like in through a programming structure create these neurons that are interconnected

51
00:03:59.940 --> 00:04:05.630
and come up with a way for them to pass signals to each other and so that they learn.

52
00:04:05.800 --> 00:04:10.980
And the difference between deep learning and machine learning is drastic in machine learning.

53
00:04:10.980 --> 00:04:20.190
We have pretty coded algorithms coded procedures methods and ways that these algorithms work and then

54
00:04:20.250 --> 00:04:22.530
we get we get to train them up and so on.

55
00:04:22.530 --> 00:04:28.260
So the structures are quite fixed there whereas in deep learning all we create is a this artificial

56
00:04:28.260 --> 00:04:32.060
neural network and then we let it learn on its own.

57
00:04:32.070 --> 00:04:33.520
That's a core difference.

58
00:04:33.540 --> 00:04:41.370
We we set up the rules of the game on how the signals will be passed what back refrigeration is which

59
00:04:41.650 --> 00:04:46.090
like or how the inputs get into the neural and how they go out of the network.

60
00:04:46.860 --> 00:04:50.100
But then we leave it alone and we let it just learn on its own.

61
00:04:50.100 --> 00:04:55.770
And that's the key difference between machine learning and deep learning that's why people are is usually

62
00:04:55.770 --> 00:04:58.220
gets much better results than machine.

63
00:04:58.320 --> 00:05:03.960
The drawback though is that deeper learning requires a lot of input a lot of data in order to be for

64
00:05:03.960 --> 00:05:14.550
us to train a deploring and to finish off here are a couple of examples of deep learning types of deep

65
00:05:14.550 --> 00:05:16.710
learn they're going to look at six.

66
00:05:16.720 --> 00:05:21.720
They're broken down into two categories supervised deploring and unsupervised supervised learning is

67
00:05:21.720 --> 00:05:25.020
when we have training day and when we know the answer.

68
00:05:25.200 --> 00:05:31.620
And then we train algorithm there and then we can apply it in the future to new data and then figure

69
00:05:31.620 --> 00:05:37.620
out the answer there unsupervised is when we don't know what we're actually looking for when we want

70
00:05:37.620 --> 00:05:42.030
to kind of like in clustering we want to want to apply the algorithm.

71
00:05:42.060 --> 00:05:46.650
So it tells us the answers even though we don't really have the answers.

72
00:05:46.790 --> 00:05:48.820
Even in the past already for ourselves.

73
00:05:49.050 --> 00:05:49.910
So let's have a look.

74
00:05:50.010 --> 00:05:55.170
I've got some interesting animations prepared by artificial neural networks.

75
00:05:55.170 --> 00:05:58.850
Corrosion on your own networks and recurrent neural networks.

76
00:05:58.860 --> 00:06:03.460
So those are the three core types of deep learning of supervised learning.

77
00:06:03.600 --> 00:06:08.790
Artificial neural networks are the base ones that you will need to know in order to know any other type

78
00:06:08.790 --> 00:06:10.100
of deporting.

79
00:06:10.220 --> 00:06:16.380
In conclusion all that usually deal with images and image recognition and recurrent neural networks

80
00:06:16.380 --> 00:06:24.900
usually deal with memory retention so so assessing sequences so when you have for instance for instance

81
00:06:24.900 --> 00:06:29.350
in language translation you would reply a recurrence to your own network or a specific type called an

82
00:06:29.430 --> 00:06:34.090
Alice T.M. long short term memory network.

83
00:06:34.370 --> 00:06:41.080
And so here are some examples of the top Amen's or those are the short names and then CNN Arnon and

84
00:06:41.090 --> 00:06:45.720
ends are used for regression and classification CNN's are used for computer vision and RNA as use for

85
00:06:45.720 --> 00:06:49.570
time series analysis unsupervised learning.

86
00:06:49.570 --> 00:06:50.620
All right.

87
00:06:50.880 --> 00:06:51.710
Fire.

88
00:06:51.770 --> 00:06:58.830
We've not self-organizing maps deep Boltzmann machines and honor and Coras self-organizing maps are

89
00:06:58.830 --> 00:07:01.110
perhaps the easiest type of deep learning.

90
00:07:01.290 --> 00:07:09.240
And they are used for feature detection depost machines are probably one of the most difficult types

91
00:07:09.240 --> 00:07:10.550
of deep learning.

92
00:07:10.740 --> 00:07:18.270
And they're not actually used in practice because they've been proven extremely confutation all heavy.

93
00:07:18.360 --> 00:07:21.860
Theoretically they're very interesting and they they have merit.

94
00:07:21.930 --> 00:07:27.660
But for now in practice they're not really used our own Choros are a interesting type of unsupervised

95
00:07:27.780 --> 00:07:33.150
learning and they are used for recommender systems in practice.

96
00:07:33.240 --> 00:07:34.310
So let's have a look.

97
00:07:34.380 --> 00:07:38.670
Here's an example of future direction recommendations this relations system.

98
00:07:38.670 --> 00:07:40.940
So there we go there's a quick overview of learning.

99
00:07:40.940 --> 00:07:44.880
Of course this world is much richer and broader than what we just discussed here.

100
00:07:45.030 --> 00:07:47.220
And if you're interested to dive in.

101
00:07:47.220 --> 00:07:49.320
There are multiple resources.

102
00:07:49.500 --> 00:07:55.590
For instance this specific structure is the one we believe in and that's why we structure our course

103
00:07:55.650 --> 00:08:00.570
on deep learning in this specific order about the six parts to it.

104
00:08:00.870 --> 00:08:09.720
At the same time there are other resources which you can find on any any specific type of the this or

105
00:08:09.840 --> 00:08:12.320
these neural networks were there.

106
00:08:12.390 --> 00:08:14.470
And as CNN's ordnance.

107
00:08:14.620 --> 00:08:17.540
So I hope you enjoyed this Tauriel and I look forward to seeing you next.

108
00:08:17.700 --> 00:08:19.620
Until then enjoy datasets.
