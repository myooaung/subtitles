1
00:00:00,630 --> 00:00:03,170
Hello and welcome to this Python tutorial.

2
00:00:03,170 --> 00:00:03,500
All right.

3
00:00:03,510 --> 00:00:09,120
Now in the next function that we're about implementing We will train the deep neural network that is

4
00:00:09,120 --> 00:00:11,470
inside our artificial intelligence.

5
00:00:11,490 --> 00:00:16,980
So basically we're going to do the whole process for propagation and then back propagation.

6
00:00:16,980 --> 00:00:19,140
So it is we're going to get our output.

7
00:00:19,200 --> 00:00:20,850
We're going to get the target.

8
00:00:20,890 --> 00:00:26,020
We'll compare the output of the target to compute the last error then we're going to back propagate

9
00:00:26,130 --> 00:00:28,410
this last error into the new network.

10
00:00:28,680 --> 00:00:33,960
And using suggested gradient descent who will have to wait according to how much they contributed to

11
00:00:33,960 --> 00:00:34,780
the last error.

12
00:00:35,190 --> 00:00:39,030
So let's do all this for those of you coming from the deep Marine Corps.

13
00:00:39,030 --> 00:00:43,850
This will be good stuff but for the others don't worry I'm going to expand that again.

14
00:00:44,160 --> 00:00:51,630
So we're going to call this new function learn and learn function is going to take several arguments.

15
00:00:51,630 --> 00:00:57,350
First self of course which will refer to the object of the degree in class.

16
00:00:57,510 --> 00:01:08,770
Then we're going to take our batched state for the current state then our batch next state then our

17
00:01:08,860 --> 00:01:15,570
That reward and finally Arbat action.

18
00:01:15,590 --> 00:01:16,960
So why do we take this.

19
00:01:16,970 --> 00:01:19,770
You probably recognized what is this series.

20
00:01:19,880 --> 00:01:26,350
Well that's of course a transition a transition of the market decision process that is at the base.

21
00:01:26,360 --> 00:01:27,440
Did you learn.

22
00:01:27,500 --> 00:01:30,420
And why do we all take them into some batches.

23
00:01:30,560 --> 00:01:36,200
Well that's because you know remember we don't consider the transitions by a series of the top or current

24
00:01:36,200 --> 00:01:39,630
state next state current reward and current action.

25
00:01:39,650 --> 00:01:44,050
We created some simple batches here thanks to the simple function.

26
00:01:44,210 --> 00:01:48,430
And so now our transitions are in the form of first batch for this state.

27
00:01:48,500 --> 00:01:53,520
A second batch for the next date a batch for the reward and a batch for the action.

28
00:01:53,510 --> 00:01:59,090
That's the form of our transition's now and they're all well aligned with respect to time thanks to

29
00:01:59,090 --> 00:02:04,150
this concatenation that we made here with respect to the first dimension.

30
00:02:04,160 --> 00:02:10,460
So the point is now we have these transition of batches one batch for each of the state next days we

31
00:02:10,450 --> 00:02:15,400
watch it in action and we do all this because we're using this experience replay trick.

32
00:02:15,530 --> 00:02:18,580
So that's our deep neural network can learn something.

33
00:02:18,590 --> 00:02:24,190
Remember if you only had a transition's by themselves what it would be some instant learning.

34
00:02:24,260 --> 00:02:28,930
Or if you want some very short memory learning and therefore the mole wouldn't learn anything.

35
00:02:29,180 --> 00:02:35,690
So we have to take these batches from the memory which become our transitions and then eventually we

36
00:02:35,690 --> 00:02:40,940
will get the different outputs for each of the states of the input states and we will do this for the

37
00:02:41,160 --> 00:02:45,620
states and for the next States because we will need both to compute the loss.

38
00:02:45,680 --> 00:02:51,860
I will soon remind the balance equation that is at the heart of the learning algorithm.

39
00:02:51,860 --> 00:02:57,120
So now let's go into the function and let's first get the outputs of the box state.

40
00:02:57,170 --> 00:03:04,740
So I'm going to call this first viable outputs and then we're going to say of course our self-talk not

41
00:03:04,740 --> 00:03:07,670
all so self start.

42
00:03:07,740 --> 00:03:14,160
No because we want to get our model outputs of the input states of the state.

43
00:03:14,510 --> 00:03:19,110
And since our model is actually expecting a batch of input states.

44
00:03:19,310 --> 00:03:25,330
Well we can totally input that state right now for the input of them all.

45
00:03:25,340 --> 00:03:31,550
That's exactly how we initialized the states that are going into the network with torture answer with

46
00:03:31,550 --> 00:03:35,080
this vague dimension for the batch that's perfect.

47
00:03:35,180 --> 00:03:37,720
We now get the outputs of them all.

48
00:03:37,880 --> 00:03:45,680
But then there is another technical trick if we only do sell that model state well we will get the outputs

49
00:03:45,860 --> 00:03:50,980
of all the possible actions you know 0 1 and 2 but that's not what we want.

50
00:03:50,990 --> 00:03:54,580
We are only interested in the actions that were chosen.

51
00:03:54,740 --> 00:04:01,190
The actions that were decided by the network to play at each time and so to get these actually interested

52
00:04:01,190 --> 00:04:09,320
in that is the actions played well we have to use this gather function in which we input one because

53
00:04:09,320 --> 00:04:16,840
we only want the action that was chosen and then we add that action with this one.

54
00:04:16,880 --> 00:04:23,600
And in that section we will gather each time the best action to play for each of the input states of

55
00:04:23,600 --> 00:04:24,560
the state.

56
00:04:24,830 --> 00:04:30,690
We don't want the action that is played the action that is chosen and we get this with this gather one

57
00:04:30,840 --> 00:04:31,770
that action.

58
00:04:32,130 --> 00:04:33,390
But then be careful.

59
00:04:33,450 --> 00:04:40,190
The state here has this fake diamonds and grease on into the batch and that section doesn't have it.

60
00:04:40,190 --> 00:04:46,190
Backstay has it because we used to Unsworth with here but we haven't used any arm squeeze for the actions

61
00:04:46,430 --> 00:04:52,880
so we have to add it here so that the Bache action has the exact same dimension as the state.

62
00:04:53,150 --> 00:05:02,710
So we're going to add a dot and squeeze your right here and actually this is not zero but one because

63
00:05:03,200 --> 00:05:09,120
zero response to Faith is not the state and one will correspond to the examination of the actions.

64
00:05:09,370 --> 00:05:16,540
And finally the last thing we need to do here is we need to kill this fake batch with a squeeze.

65
00:05:16,600 --> 00:05:18,010
Why do we need to do that.

66
00:05:18,130 --> 00:05:20,130
Because now we are out of the neural network.

67
00:05:20,200 --> 00:05:22,820
We have our outputs but we don't want them back.

68
00:05:22,900 --> 00:05:24,080
We want them.

69
00:05:24,190 --> 00:05:26,260
And the simple answer is simple vector.

70
00:05:26,260 --> 00:05:31,540
A vector of output the batches just when we work in the neural network because the neural network is

71
00:05:31,540 --> 00:05:34,880
expecting the format of sensors into a batch.

72
00:05:34,990 --> 00:05:40,510
But now we have our outputs and in the next baluns equation of the deep learning we won't need them

73
00:05:40,510 --> 00:05:41,530
into a batch.

74
00:05:41,530 --> 00:05:48,130
So I'm killing that here and killing the faith dimension to get back the simple form of our outputs.

75
00:05:48,160 --> 00:05:54,530
So I'm just adding here Dot and then squeeze and then since I want to kill the fake lamination corresponding

76
00:05:54,540 --> 00:05:56,120
to the back of the action.

77
00:05:56,250 --> 00:06:01,430
Well since the spacetime engine has index one I'm adding one here.

78
00:06:01,560 --> 00:06:02,050
All right.

79
00:06:02,100 --> 00:06:05,480
And now there we go we have our outputs.

80
00:06:05,490 --> 00:06:05,910
OK.

81
00:06:06,000 --> 00:06:11,100
We have a little warning what is that local variable output is assigned but never used.

82
00:06:11,190 --> 00:06:11,860
That's OK.

83
00:06:11,880 --> 00:06:13,510
We will use it very quickly.

84
00:06:13,920 --> 00:06:15,540
So that's our outputs.

85
00:06:15,600 --> 00:06:23,660
And now we want to get our next outwits So now you might be thinking why do we need the next outputs.

86
00:06:23,840 --> 00:06:29,160
Well to understand this we need to go back to the deep learning algorithm which is right here that is

87
00:06:29,180 --> 00:06:31,670
part of the letak handbook.

88
00:06:31,850 --> 00:06:33,820
So that's the whole diffusion process.

89
00:06:33,860 --> 00:06:39,180
At the beginning we were initializing all the key values and then at each time t.

90
00:06:39,440 --> 00:06:44,770
Well there we go we select the action with soughed Max that's what we did with the select action function.

91
00:06:44,870 --> 00:06:51,050
Then we opened the transition and then as you can see we get the prediction we get the target and we

92
00:06:51,050 --> 00:06:52,120
can be the last.

93
00:06:52,190 --> 00:06:54,330
So why do we need the next output as well.

94
00:06:54,350 --> 00:07:01,640
That's because of the target the target is equal to Ghana times the next output plus the we want and

95
00:07:01,640 --> 00:07:06,060
we will compute the targets right after that since we need the next output of the target.

96
00:07:06,200 --> 00:07:07,790
Let's compute this first.

97
00:07:07,820 --> 00:07:14,060
So again to get the next update very simple the next output is going to be the result of our neural

98
00:07:14,060 --> 00:07:19,180
network when the batched next state is entering it as input.

99
00:07:19,190 --> 00:07:27,110
So very simply we take our model that is our neural network and then this time the input of the neural

100
00:07:27,110 --> 00:07:33,020
network is going to be the batched next state that batched next date.

101
00:07:33,200 --> 00:07:41,290
But now remember if we go back to the early algorithm Well you can see that the next output is the maximum

102
00:07:41,290 --> 00:07:45,530
of the q values for the next state with respect to all the actions.

103
00:07:45,730 --> 00:07:51,790
So right now to get the next output we need to get the maximum of this q values and therefore I'm going

104
00:07:51,790 --> 00:07:52,720
to do here.

105
00:07:52,840 --> 00:08:00,550
Detach you know to detach all the outputs of the model because we have several states in this batch

106
00:08:00,550 --> 00:08:06,730
next States that's the batch of all the next states in all the transitions taken from the random sample

107
00:08:06,730 --> 00:08:07,620
of our memory.

108
00:08:07,840 --> 00:08:14,530
So I'm detaching all of them using the detach function and then I'm taking the max of all these two

109
00:08:14,530 --> 00:08:15,250
values.

110
00:08:15,340 --> 00:08:20,920
And since we're taking the maximum of these two values with respect to the action Well we have to specify

111
00:08:20,920 --> 00:08:23,080
that it is with respect to the action.

112
00:08:23,080 --> 00:08:26,310
And since the action is represented by the index one.

113
00:08:26,530 --> 00:08:32,770
Well again we have to put the next one here and then we have to specify that we're taking the cue values

114
00:08:33,290 --> 00:08:41,140
as T plus 1 that is the next stage and the next state is represented by 0 because the index zero corresponds

115
00:08:41,380 --> 00:08:47,370
to the states and therefore here we need to add brackets with index 0.

116
00:08:47,770 --> 00:08:54,400
That way we get the maximum of the key values of the next states represented by the next zero according

117
00:08:54,400 --> 00:09:01,490
to all the actions that are represented by the index one and now perfect we get our next outputs.

118
00:09:01,570 --> 00:09:02,860
These are a new set.

119
00:09:02,870 --> 00:09:04,320
That's when we had the warning.

120
00:09:04,320 --> 00:09:05,050
But that's fine.

121
00:09:05,080 --> 00:09:07,950
We will use it right now to compute the target.

122
00:09:08,470 --> 00:09:12,510
And speaking of the target that's the next step of this known function.

123
00:09:12,520 --> 00:09:13,210
So there we go.

124
00:09:13,220 --> 00:09:15,460
Target equals.

125
00:09:15,670 --> 00:09:18,220
Now let's get back to our AI AI handbook.

126
00:09:18,400 --> 00:09:24,800
The target is equal to the word plus gamma times the next output is the maximum of the cube values of

127
00:09:24,800 --> 00:09:25,710
the next day.

128
00:09:25,930 --> 00:09:29,290
According to the actions that we can compute that.

129
00:09:29,340 --> 00:09:35,590
So that is equal to self that gamma and self that Gamma was initialized.

130
00:09:35,590 --> 00:09:45,100
Here it is introduce a that's a variable the Virgin Q An object self get to Times the next outputs as

131
00:09:45,100 --> 00:09:49,910
we just said plus Do we want that is the best want.

132
00:09:49,940 --> 00:09:57,400
We're working with batches here so plus batch we want and that's the target.

133
00:09:57,520 --> 00:10:03,820
In one sample of the memory gamma multiplied by the next outputs Plus the reward.

134
00:10:03,900 --> 00:10:04,480
All right.

135
00:10:04,480 --> 00:10:05,080
Perfect.

136
00:10:05,110 --> 00:10:07,100
So now we have our outputs.

137
00:10:07,180 --> 00:10:13,270
We also have our targets and therefore we can compute the loss the loss that is representing the error

138
00:10:13,270 --> 00:10:14,260
of the prediction.

139
00:10:14,500 --> 00:10:21,000
So let's call this last the last two is of course for the temporal difference.

140
00:10:21,040 --> 00:10:28,720
That is again at the heart of Q learning and this tiddy us is going to be equal to the release that

141
00:10:28,720 --> 00:10:30,330
improves much the Cunanan.

142
00:10:30,520 --> 00:10:34,730
That's the last function we will choose for our artificial intelligence.

143
00:10:34,800 --> 00:10:38,640
For those of you coming from the deep green course that's really the last I recommend.

144
00:10:38,680 --> 00:10:43,280
If you want to implement the Coonerty and so how are we going to get this.

145
00:10:43,510 --> 00:10:50,380
Well again we're going to take a function from the functional module represented by F and therefore

146
00:10:50,380 --> 00:10:57,970
here I'm going to use our functional module f ducks and the Hubble us can be obtained thanks to the

147
00:10:57,970 --> 00:11:02,420
function Smoots L-1 loves that one.

148
00:11:02,420 --> 00:11:03,850
So pressing enter.

149
00:11:03,850 --> 00:11:07,910
And that's really the best lost function I recommend for deep learning.

150
00:11:07,930 --> 00:11:09,680
It really improves the Culin.

151
00:11:09,820 --> 00:11:12,670
But this is a function so I'm adding some parenthesis.

152
00:11:12,850 --> 00:11:14,740
And now there is nothing more simple.

153
00:11:14,830 --> 00:11:19,340
The arguments we need to input are the predictions and the targets.

154
00:11:19,420 --> 00:11:24,010
So the predictions of course are outputs because that's the output of the neural network.

155
00:11:24,190 --> 00:11:27,600
No the output of the neural network is what neural network predicts.

156
00:11:27,730 --> 00:11:29,030
So that's the prediction.

157
00:11:29,260 --> 00:11:35,900
So the first argument here is outputs and then the second argument is of course the target.

158
00:11:36,100 --> 00:11:40,110
The thing we're trying to get and it's already computed perfect.

159
00:11:40,150 --> 00:11:43,150
We can directly input target.

160
00:11:43,150 --> 00:11:43,630
Perfect.

161
00:11:43,650 --> 00:11:48,200
Now we have told us just we've got a little tea here.

162
00:11:48,220 --> 00:11:48,760
There we go.

163
00:11:48,760 --> 00:11:50,470
Now the warning should disappear.

164
00:11:50,890 --> 00:11:51,420
Yes.

165
00:11:51,430 --> 00:11:52,110
Perfect.

166
00:11:52,180 --> 00:11:58,090
And now that we have the last error we can back propagators error back into the network to update the

167
00:11:58,090 --> 00:12:03,230
weights with stochastic gradient descent and that's exactly what we're going to do in the next step.

168
00:12:03,490 --> 00:12:12,040
So of course now what we have to do as you might guess is take our optimizing our optimizer which again

169
00:12:12,190 --> 00:12:14,850
we introduce you initialized it.

170
00:12:15,030 --> 00:12:21,430
And that's an atom optimizer which is actually an object of the Atom class and it is already fitted

171
00:12:21,580 --> 00:12:23,720
with the parameters of our model.

172
00:12:23,810 --> 00:12:31,480
And we already chose a learning rate of 0.1 percent to perfect our optimizer is ready but now we need

173
00:12:31,480 --> 00:12:37,150
to play it on the last error to perform stochastic grid in the sense and data weights.

174
00:12:37,180 --> 00:12:43,540
So when working with fighters the first thing we need to do is re-initialize it at each iteration of

175
00:12:43,540 --> 00:12:44,150
the loop.

176
00:12:44,200 --> 00:12:50,620
We must re-initialize the optimizer from one interaction to the other in the loop of this to get the

177
00:12:50,660 --> 00:12:53,310
grid in the set and to re-initialize it.

178
00:12:53,350 --> 00:12:54,820
And each iteration of the loop.

179
00:12:55,200 --> 00:12:59,410
Well we're going to use the following method which is zero.

180
00:12:59,940 --> 00:13:00,400
Here we go.

181
00:13:00,410 --> 00:13:05,180
Zero grad will re-initialize the optimizer at each iteration of the loop.

182
00:13:05,230 --> 00:13:07,300
Then let's not forget the parenthesis.

183
00:13:07,390 --> 00:13:08,180
Perfect.

184
00:13:08,200 --> 00:13:14,850
And now that it is re-initialize Well we can perform backward propagation with our optimizer.

185
00:13:15,190 --> 00:13:16,380
And how do we do that.

186
00:13:16,540 --> 00:13:22,660
Well we take our to the laws and we're going to back propagate it back into the network and to back

187
00:13:22,660 --> 00:13:24,330
propagated into the network.

188
00:13:24,460 --> 00:13:33,190
We need to use the backward function and inside this backward function I recommend to input retain underscore

189
00:13:33,220 --> 00:13:37,180
variables and set it equal to true.

190
00:13:37,240 --> 00:13:40,990
I recommend to do this because this will improve back propagation.

191
00:13:41,200 --> 00:13:46,420
The use of written variables equals true is to free some memory and we need to free the memory because

192
00:13:46,420 --> 00:13:52,730
we are going to go several times on the last so that will definitely improve the training performance.

193
00:13:52,900 --> 00:13:58,980
And finally last step of this learned function is to have data weights according to the back propagation.

194
00:13:58,980 --> 00:14:02,570
That is according to how much the weights contributed to the error.

195
00:14:02,800 --> 00:14:11,680
And to do this we take our optimizer again which was initialized in re-initialize and we use the step

196
00:14:12,250 --> 00:14:17,560
function and simply with this line of code by using the step function.

197
00:14:17,560 --> 00:14:19,450
This will update the weights.

198
00:14:19,480 --> 00:14:21,860
That's this line of code that updates the weights.

199
00:14:21,910 --> 00:14:28,750
This line of code that propagates the error into the neural network and this line of code uses the optimizer

200
00:14:28,930 --> 00:14:31,510
to update the weights and there we go.

201
00:14:31,510 --> 00:14:36,800
We have a learning neural network all right so congratulations.

202
00:14:36,840 --> 00:14:42,320
This was probably the most technical and difficult part of all this dispute or morrow.

203
00:14:42,450 --> 00:14:47,880
I know my torch can be tricky sometimes with ease and squeeze and squeeze but in the end I promise you

204
00:14:47,880 --> 00:14:54,900
will get a very functional neural network and therefore peculiarly model and eventually a great artificial

205
00:14:54,960 --> 00:14:56,580
intelligence.

206
00:14:56,580 --> 00:15:03,150
So now let's move on to the next step or teacher and model which will be the update function that will

207
00:15:03,210 --> 00:15:07,170
obviously update when the AI will discover the new state.

208
00:15:07,170 --> 00:15:11,420
So you know it will discover the new state and then it will receive the reward.

209
00:15:11,520 --> 00:15:16,860
Depending on the action that is displayed and this new state will take care of this with the update

210
00:15:16,860 --> 00:15:19,510
function and will do this in the next tutorial.

211
00:15:19,710 --> 00:15:21,000
Until then enjoy.

212
00:15:21,020 --> 00:15:21,300
I.
