1
00:00:00,300 --> 00:00:02,000
Hello and welcome to this tutorial.

2
00:00:02,160 --> 00:00:05,090
Congratulations again for being done with it.

3
00:00:05,150 --> 00:00:08,200
See we made it we made the brains and we trained them.

4
00:00:08,400 --> 00:00:15,600
But now we still have to make a test agent which will not a date tomorrow at all but it will just use

5
00:00:15,780 --> 00:00:19,430
the share model to make its own explorations.

6
00:00:19,530 --> 00:00:26,080
And of course in this code we will record some videos and these will be the videos test agents break

7
00:00:26,090 --> 00:00:28,750
out with a certain score.

8
00:00:28,810 --> 00:00:30,840
So let's go through this code.

9
00:00:30,850 --> 00:00:35,860
The most important is done so as I told you we're not going to code it line by line but I think it's

10
00:00:35,860 --> 00:00:38,580
important that you understand what's going on here.

11
00:00:38,800 --> 00:00:42,680
So there we go with this code in the first section as you noticed.

12
00:00:42,790 --> 00:00:49,480
We import the libraries and then we did find this test function which will make this test agent to do

13
00:00:49,480 --> 00:00:52,390
its own exploration and play the breakout game.

14
00:00:52,600 --> 00:00:58,610
So we get this test function takes three arguments the first one is ranking that is still to do synchronize

15
00:00:58,850 --> 00:01:02,060
the test agent as we did for the training agents.

16
00:01:02,230 --> 00:01:05,380
Then we have our parameters of course because you need some.

17
00:01:05,380 --> 00:01:12,220
And of course we have the shared model because this test agent will use a shared model to do its own

18
00:01:12,220 --> 00:01:13,360
exploration.

19
00:01:13,360 --> 00:01:18,790
All right then we go inside the function and this line of code we synchronize to test agent.

20
00:01:18,820 --> 00:01:23,120
Exactly as we did before then we import the environment.

21
00:01:23,170 --> 00:01:27,070
So I remind that in the main code which will be in the next tutorial.

22
00:01:27,250 --> 00:01:33,970
Well and name here will be replaced by break of zero so that we can go into the break zero environment

23
00:01:34,060 --> 00:01:40,180
and play the game and the Red Cross Trumans that will get the videos of our evening break out.

24
00:01:40,180 --> 00:01:46,680
So basically this line of code in total means that we run one environment with video.

25
00:01:47,080 --> 00:01:54,520
Then at the next line of code we synchronize this environment so exact same principle as the Trend function.

26
00:01:54,700 --> 00:02:02,170
Then we get our model and to do this we create an object of the activity class and we put the input

27
00:02:02,170 --> 00:02:08,920
shape with our environment observation space and shape zeroes are exactly like the train function and

28
00:02:09,190 --> 00:02:13,690
our outputs which are the actions with an action space.

29
00:02:13,900 --> 00:02:19,770
So exactly like before then something new here since we're done with the training.

30
00:02:19,870 --> 00:02:25,450
We don't want to put the model in train mode because simply we don't want it to train we want to put

31
00:02:25,450 --> 00:02:26,880
it in development.

32
00:02:27,160 --> 00:02:29,740
And that's what we do here with model that evolved.

33
00:02:29,890 --> 00:02:36,940
So that's just basically to put the test agent in a mode that will basically test it tested performance

34
00:02:37,030 --> 00:02:38,350
evaluated.

35
00:02:38,720 --> 00:02:45,680
Then here we get our input states which are the input images from the game which at this point are an

36
00:02:45,680 --> 00:02:46,790
entire race.

37
00:02:46,840 --> 00:02:49,360
Then we convert them into torch dancers.

38
00:02:49,480 --> 00:02:52,810
Here we initialize some of the words here.

39
00:02:52,840 --> 00:02:54,980
We initialize down to true.

40
00:02:55,200 --> 00:03:03,430
So still just like last time then something new again we introduce this third viable with a time function

41
00:03:03,850 --> 00:03:05,990
to measure the time of computations.

42
00:03:06,190 --> 00:03:08,680
And that's because you want to get the starting point.

43
00:03:08,890 --> 00:03:15,160
Then here the actions we use a very practical type of cue that allows to add an element to the cue from

44
00:03:15,160 --> 00:03:16,550
the right or from the left.

45
00:03:16,600 --> 00:03:21,960
So that's very practical and I'll give you the reference I think in decremented version of the code.

46
00:03:22,180 --> 00:03:27,320
So you'll have a look at what this dequeue is and that's what allows to do that.

47
00:03:27,490 --> 00:03:33,370
Then we initialize the length of an episode with zero of course and then we will increment the size

48
00:03:33,400 --> 00:03:34,690
in this well loop.

49
00:03:34,870 --> 00:03:36,480
So we use the same trick here.

50
00:03:36,680 --> 00:03:42,290
While true and in the loop we increments the length of the episode by one.

51
00:03:42,490 --> 00:03:49,700
When the game is done when the game is over we reload the last set of the shared model the share model

52
00:03:49,730 --> 00:03:51,460
that the dated by the other models.

53
00:03:51,460 --> 00:03:55,610
Remember that here the shared model is no longer dated then.

54
00:03:55,660 --> 00:04:04,030
Still if the game is over if the game is done we Reinette we re-initialize the cell states see X and

55
00:04:04,190 --> 00:04:13,840
then States H x and else if the game is not over well we keep the same cell States and in states.

56
00:04:13,840 --> 00:04:18,030
But to make sure they are taught variable so that they can be attached to a gradient.

57
00:04:18,170 --> 00:04:25,240
OK so that's something we already dead in the trend function and then still in the while loop and after

58
00:04:25,240 --> 00:04:30,870
having a data that states in the hidden states the right way depending on the two cases here.

59
00:04:31,060 --> 00:04:34,360
Well what do we do we get the predictions of tomorrow.

60
00:04:34,450 --> 00:04:37,380
That's exactly what we do here with this line of code.

61
00:04:37,750 --> 00:04:43,360
So we get the value which is the output of the critic the actual value which is the output of the actor.

62
00:04:43,600 --> 00:04:49,750
And then it's up all of the hidden states H x and the cell States the X then we generate a distribution

63
00:04:49,750 --> 00:04:54,130
of probabilities of the actions that is at the Q values action value here.

64
00:04:54,270 --> 00:04:56,380
And we do this with the next function.

65
00:04:56,470 --> 00:05:01,230
And of course we don't need to get the luck probabilities here because this is just for the training

66
00:05:01,480 --> 00:05:02,650
for the test agent.

67
00:05:02,650 --> 00:05:09,130
It will just play the actions we will just use it you know like doom a certain activity to play it but

68
00:05:09,190 --> 00:05:10,920
we're not doing any training here.

69
00:05:10,960 --> 00:05:19,040
So we have just a prop and from this we play the action by taking directly to RMX of these probabilities

70
00:05:19,040 --> 00:05:22,720
that is it takes the action that has the highest probability.

71
00:05:22,810 --> 00:05:26,860
And the reason is that the test agent doesn't do any exploration.

72
00:05:26,860 --> 00:05:32,830
Remember that we want have a chance to pick up some actions that have low probabilities when you want

73
00:05:32,830 --> 00:05:38,170
to do some exploration of these other actions and you know not taking each time the action that has

74
00:05:38,170 --> 00:05:44,260
the highest probability but here test agent can do any exploration and therefore that's why we directly

75
00:05:44,260 --> 00:05:50,800
take the action that has the maximum probability again then once we play the action we reach the next

76
00:05:50,800 --> 00:05:53,340
state and we get the next word.

77
00:05:53,470 --> 00:05:56,920
And that is a day dated whether or not the game is over.

78
00:05:57,160 --> 00:06:03,700
So this we get all this with this line of code by playing the action after having selected it with our

79
00:06:03,790 --> 00:06:04,630
Max here.

80
00:06:04,840 --> 00:06:13,000
So we play the action here and we get the state we get the reward and done it again and then since we

81
00:06:13,090 --> 00:06:18,950
just got a new reward We're going to update some of the reward by simply adding this new word.

82
00:06:19,180 --> 00:06:21,480
And finally whenever the game is done.

83
00:06:21,490 --> 00:06:28,510
So if that means when the game is done when I finishes playing the game well we're going to print the

84
00:06:28,510 --> 00:06:31,210
results with the time the opposite.

85
00:06:31,230 --> 00:06:36,100
We wanted the length of the episode that is how much time did it last.

86
00:06:36,100 --> 00:06:42,890
Playing great out and this is just how we print all these variables using these tiny tricks.

87
00:06:42,910 --> 00:06:46,280
That's for the time then we want some it's just a variable.

88
00:06:46,340 --> 00:06:51,960
Some of the words and ideas at length is the of the length of the present OK.

89
00:06:52,160 --> 00:06:57,860
And then once we printed all the results well since the game is over and we want to start a new game

90
00:06:58,220 --> 00:06:59,930
we're going to re-initialize everything.

91
00:06:59,930 --> 00:07:04,170
That is the sum of two words zero the length of an episode to zero.

92
00:07:04,230 --> 00:07:10,100
We're going to re-enact all the actions by using this key function resets the input images you know

93
00:07:10,100 --> 00:07:13,360
by repeating all the breaks altogether.

94
00:07:13,610 --> 00:07:21,980
And finally we use this time that sleep 60 seconds to do a break of one minute to let the other agents

95
00:07:21,980 --> 00:07:22,840
practice.

96
00:07:22,850 --> 00:07:24,810
And that if the game is over.

97
00:07:25,210 --> 00:07:25,840
OK.

98
00:07:25,940 --> 00:07:32,210
And finally we have this last line of code which will get us the new state and then we can move forward.

99
00:07:32,240 --> 00:07:34,550
We can continue in this new game.

100
00:07:34,550 --> 00:07:35,840
So there we go.

101
00:07:35,870 --> 00:07:37,430
That's the test function.

102
00:07:37,430 --> 00:07:40,550
Things to which you will see the videos in one or two tutorials.

103
00:07:40,550 --> 00:07:45,310
I hope you will be altogether like last time to watch the results that is with you.

104
00:07:45,350 --> 00:07:47,360
Curial and me that will be fun.

105
00:07:47,480 --> 00:07:48,400
And I'm telling you.

106
00:07:48,440 --> 00:07:50,330
Expect to see good results.

107
00:07:50,360 --> 00:07:55,130
But keep in mind that this breakout game was super challenging.

108
00:07:55,130 --> 00:07:58,430
We thought it was a simple game to play first but not at all.

109
00:07:58,430 --> 00:08:01,480
It actually turned out to be much more difficult than doom.

110
00:08:01,670 --> 00:08:03,890
And that's why we put it in the last module.

111
00:08:04,190 --> 00:08:09,510
But anyway let's make this main function in the next tutorial.

112
00:08:09,590 --> 00:08:11,770
Same This is not the most important here.

113
00:08:11,780 --> 00:08:18,860
Now that the A-380 is demented we will not code it line by line it will expand code and very quickly

114
00:08:18,980 --> 00:08:20,570
we will get the results.

115
00:08:20,570 --> 00:08:22,130
Until then enjoy AI.
