1
00:00:00,530 --> 00:00:02,950
Hello and welcome to this Python tutorial.

2
00:00:02,990 --> 00:00:08,520
So now the next step is to make that count neurons function which will give us what we want.

3
00:00:08,540 --> 00:00:14,130
That is this number of neurons in this huge vector after the convolutions are applied.

4
00:00:14,300 --> 00:00:19,640
That's the only missing information we need right now and we are going to get it with the function.

5
00:00:19,700 --> 00:00:23,930
So let's make this function we are going to call it counts.

6
00:00:24,280 --> 00:00:27,570
And those core neurons very simply.

7
00:00:27,830 --> 00:00:32,220
And what is this count neurons function going to take as arguments.

8
00:00:32,510 --> 00:00:39,450
Well it is going to take the object self but then it's going to take something else because this number

9
00:00:39,450 --> 00:00:44,540
of output neurons in the flattening layer actually only depends on one thing.

10
00:00:44,660 --> 00:00:50,840
It depends on the dominations of the original input image the one that goes at the very beginning of

11
00:00:50,840 --> 00:00:51,930
the neural network.

12
00:00:52,160 --> 00:00:58,370
And so the only argument we need right now is actually the time mentions the timing of the input images.

13
00:00:58,370 --> 00:01:03,410
Therefore let's give a name to this argument representing the diamond and of the input image.

14
00:01:03,520 --> 00:01:06,790
And we're going to call it image.

15
00:01:07,220 --> 00:01:07,690
All right.

16
00:01:07,850 --> 00:01:14,720
And I can tell you right now that the actual dimensions of the input images coming from Dume are going

17
00:01:14,720 --> 00:01:16,850
to be 80 by 80.

18
00:01:16,850 --> 00:01:23,330
We're going to reduce the size of the original images to 80 by 80 and that's going to be the format

19
00:01:23,390 --> 00:01:26,270
of the images going into the neural network.

20
00:01:26,270 --> 00:01:32,300
So Image them is actually going to be one 80 80 and the one corresponds to the fact that we're working

21
00:01:32,300 --> 00:01:33,750
with black and white images.

22
00:01:33,830 --> 00:01:41,040
That is with only one channel so image them is going to be correlated to the total one eighty and 80.

23
00:01:41,280 --> 00:01:41,560
All right.

24
00:01:41,560 --> 00:01:43,590
So that's the only argument we need.

25
00:01:43,660 --> 00:01:45,670
And now let's scan the neurons.

26
00:01:45,850 --> 00:01:47,280
So how are you going to do that.

27
00:01:47,500 --> 00:01:51,110
Well first of all we actually don't have any input image right now.

28
00:01:51,250 --> 00:01:54,070
We don't have any doom image that we can import.

29
00:01:54,070 --> 00:01:55,330
We're going to do that later.

30
00:01:55,540 --> 00:02:01,240
So the first thing we have to do is create a fake image but that has done introns 80 over 80.

31
00:02:01,240 --> 00:02:06,160
We're going to create that fake image with fake pixels and that will still give us eventually the number

32
00:02:06,160 --> 00:02:12,280
that we want because that number only depends on the dimensions and not on the pixels that are inside

33
00:02:12,280 --> 00:02:13,320
the images.

34
00:02:13,330 --> 00:02:15,640
So this just creates a fake image to start.

35
00:02:15,730 --> 00:02:18,520
And then we will compute the number of neurons that we want.

36
00:02:18,520 --> 00:02:23,860
So the trick to create a fake image is well we are going to call it x.

37
00:02:23,890 --> 00:02:30,700
First of all and then we were going to use the torch that Rand because you know we're going to put some

38
00:02:30,700 --> 00:02:37,200
random pixels in these images who we're using these random functions from torche which is the rand function.

39
00:02:37,250 --> 00:02:42,740
Then inside we're going to input as you can see the damnations of the images.

40
00:02:42,850 --> 00:02:44,400
That is one 80 80.

41
00:02:44,560 --> 00:02:50,500
But since we're going to put this image into the neural network and as you remember the neural network

42
00:02:50,500 --> 00:02:55,170
can only accept batches of input states that is here batches of input images.

43
00:02:55,370 --> 00:03:00,600
We are going to create that fake diamonds in which we can directly do this run function.

44
00:03:00,670 --> 00:03:05,720
We actually just start with the one that will correspond to the batch and then we can just put it up

45
00:03:05,730 --> 00:03:11,070
all 188 the corresponding to the dominations of the input image.

46
00:03:11,110 --> 00:03:16,980
And as you understood these nominations are contained in this image them argument which represents that

47
00:03:16,980 --> 00:03:19,330
table 1 80 80.

48
00:03:19,420 --> 00:03:23,290
So we just need to add image them.

49
00:03:23,620 --> 00:03:29,410
But in order to pass the elements of the table because you know right now Image them is a double as

50
00:03:29,410 --> 00:03:31,940
a list of arguments of a function.

51
00:03:32,020 --> 00:03:39,460
We need to add here before image them there is before that Apple store the store will allow to pass

52
00:03:39,550 --> 00:03:44,100
the elements of the imaged him to Apple as a list of arguments for function.

53
00:03:44,170 --> 00:03:49,600
And as you can see that's exactly what is specified here with the store and the diamond.

54
00:03:49,930 --> 00:03:56,640
All right so that will create an image of fake pixels that will have nothing to do with the images.

55
00:03:56,800 --> 00:04:01,720
But again we'll still be able to get the final number of neurons and now the last thing that we need

56
00:04:01,720 --> 00:04:11,080
to do remember is to convert this input vector into a torch variable because this is going to go into

57
00:04:11,380 --> 00:04:13,060
the neural network.

58
00:04:13,060 --> 00:04:13,380
All right.

59
00:04:13,390 --> 00:04:20,650
So this now represents an input image of random pixels that was just converted into a viable and that

60
00:04:20,650 --> 00:04:22,440
will now go into the neural network.

61
00:04:22,570 --> 00:04:28,330
And more specifically the convolutional layers of the neural network because since we only need the

62
00:04:28,330 --> 00:04:34,100
number of neurons after the convolutions are applied we will just go up to the convolutions 3.

63
00:04:34,210 --> 00:04:36,440
So right up to the third convolutional layer.

64
00:04:36,580 --> 00:04:39,630
And we will not go into the two full connections here.

65
00:04:39,850 --> 00:04:45,490
And that's because the number of neurons that we want is between convolution 3 and f.c 1.

66
00:04:45,520 --> 00:04:52,120
All right so now that we have one input image with the right mentions Well it's time to propagate this

67
00:04:52,180 --> 00:04:58,330
image into the neural network to reach the flattening layer then we're going to get the neurons in the

68
00:04:58,330 --> 00:05:03,850
flattened layer and we'll just get the information that we want that is the number of neurons in this

69
00:05:03,850 --> 00:05:04,750
flattening layer.

70
00:05:04,990 --> 00:05:08,950
So now we have to do is exactly what we do in a forward function.

71
00:05:08,950 --> 00:05:14,560
We need to propagate the signals into the neural network but only in the convolutional layers until

72
00:05:14,560 --> 00:05:16,120
we reach the flooding layer.

73
00:05:16,330 --> 00:05:17,360
So let's do this.

74
00:05:17,360 --> 00:05:25,660
We're going to update x now x is the input image and with the second X here X will become well the first

75
00:05:25,660 --> 00:05:27,050
convolutional layer.

76
00:05:27,310 --> 00:05:32,930
And now what we have to do is the three steps process first step we apply deconvolution to the input

77
00:05:32,950 --> 00:05:33,790
images.

78
00:05:33,850 --> 00:05:40,960
Then the second step we apply mix pruning to the convoluted images and inserts that we activate the

79
00:05:40,960 --> 00:05:48,160
neurons in this puled convoluted images and the x will become this first convolutional they're composed

80
00:05:48,160 --> 00:05:50,590
of all these pools convoluted Energis.

81
00:05:51,010 --> 00:05:56,840
So let's do this first step apply the first convolution convolution one to the input images.

82
00:05:57,040 --> 00:06:07,540
So what we do is take our convolution one self that convo Lucian one that we go we apply it to our input

83
00:06:07,550 --> 00:06:11,360
images which so far are represented by x.

84
00:06:11,500 --> 00:06:14,290
So that's the first that first that done now.

85
00:06:14,320 --> 00:06:22,630
Second step we are going to apply spooling to our convoluted images returned by convolution when X and

86
00:06:22,640 --> 00:06:26,880
two climax pulling Well we're going to take a function from the functional module.

87
00:06:26,920 --> 00:06:34,610
So we take the short cut than that and then we're going to use the function Max pool to D.

88
00:06:34,810 --> 00:06:42,340
That's the one we put self convolution one X in the parenthesis of the max pull to the because we play

89
00:06:42,360 --> 00:06:45,220
next pull into the convoluted images.

90
00:06:45,750 --> 00:06:53,140
But this next function takes additional arguments which are first the kernel size.

91
00:06:53,190 --> 00:06:59,190
So again that's the size of the window sliding through your images and that will take the maximum of

92
00:06:59,190 --> 00:07:00,710
the pixels in each slide.

93
00:07:00,830 --> 00:07:06,750
So that will still detect the features because the features are associated to a high value of the pixel

94
00:07:06,840 --> 00:07:07,750
in the arrays.

95
00:07:07,890 --> 00:07:09,580
As you say intuition lectures.

96
00:07:09,780 --> 00:07:14,020
So this first documented human need to input is this kernel size.

97
00:07:14,100 --> 00:07:15,560
And we're going to take three.

98
00:07:15,660 --> 00:07:17,650
That's a common choice for the kernel size.

99
00:07:17,940 --> 00:07:25,200
And then we need to put the strides you know by how many pixels it's going to slide in the images.

100
00:07:25,200 --> 00:07:27,560
And we are going to take a stride of two.

101
00:07:27,610 --> 00:07:29,400
Again that's a common choice.

102
00:07:29,820 --> 00:07:30,670
So there we go.

103
00:07:30,690 --> 00:07:32,530
Now the second step is done.

104
00:07:32,620 --> 00:07:38,910
And now let's move on to the third step which is to activate all the neurons in this pool and convoluted

105
00:07:38,910 --> 00:07:39,580
images.

106
00:07:39,610 --> 00:07:46,090
And this first convolution layer and to do this again we are going to apply a function to all this.

107
00:07:46,170 --> 00:07:51,960
And so here and taking again because we're going to take another function which as you might have guessed

108
00:07:52,050 --> 00:07:55,170
is going to be an activation function but which one.

109
00:07:55,230 --> 00:08:01,410
As usual it's going to be rectified activation function and maybe you remember the name for that is

110
00:08:01,620 --> 00:08:02,330
really.

111
00:08:02,790 --> 00:08:03,410
There we go.

112
00:08:03,450 --> 00:08:04,290
That's the one.

113
00:08:04,290 --> 00:08:12,540
And so we apply really to our pooled convoluted images that is all this.

114
00:08:12,540 --> 00:08:12,940
All right.

115
00:08:12,990 --> 00:08:14,270
And that's it.

116
00:08:14,370 --> 00:08:15,320
Three steps done.

117
00:08:15,330 --> 00:08:16,370
That was very quick.

118
00:08:16,500 --> 00:08:23,460
So remember the way we have to look at this is first we apply the convolution to our input images then

119
00:08:23,460 --> 00:08:28,830
we apply Max pulling to our convoluted images obtained with a convolution.

120
00:08:28,950 --> 00:08:35,730
And then we activate the neurons in all this pool convolutional layer with the rectifier activation

121
00:08:35,730 --> 00:08:43,960
function so perfect we get our first convolutional layer on which was a climax pulling and in which

122
00:08:44,050 --> 00:08:46,200
the neurons are now activated.

123
00:08:46,260 --> 00:08:51,640
And so basically what it does is that it propagates the signals from the first convolutional layer to

124
00:08:51,640 --> 00:08:52,500
the next one.

125
00:08:52,630 --> 00:08:56,580
And speaking of the next one that's exactly what we're going to take care of right now.

126
00:08:56,570 --> 00:09:01,660
We're going to do the same thing as we just did on the first convolutional there to the second convolutional

127
00:09:01,660 --> 00:09:08,170
layer to again propagate the signals further into the neural network by activating the neurons of the

128
00:09:08,170 --> 00:09:09,660
second convolutional layer.

129
00:09:09,850 --> 00:09:12,910
But before doing this we need to get this convolutional layer.

130
00:09:13,120 --> 00:09:18,330
And so we are going to apply convolution to X that is now the first convolutional layer.

131
00:09:18,460 --> 00:09:24,070
Well we are going to apply convolution to 2 x to obtain the second convolutional layer after which will

132
00:09:24,070 --> 00:09:25,120
be Max pulling it.

133
00:09:25,240 --> 00:09:27,860
And then finally activating Sirat.

134
00:09:27,970 --> 00:09:29,070
So let's do this.

135
00:09:29,170 --> 00:09:35,350
It's actually very easy we just to copy that and pasting that below.

136
00:09:35,350 --> 00:09:39,240
Now of course we need to replace convolution one by convolution too.

137
00:09:39,520 --> 00:09:40,460
And there we go.

138
00:09:40,480 --> 00:09:43,650
That's actually ready see very easy.

139
00:09:43,900 --> 00:09:50,200
And now with this line we propagate the signals from the second convolutional there to the next one

140
00:09:50,260 --> 00:09:52,580
which is going to be the third convolutional there.

141
00:09:52,720 --> 00:09:57,220
And to get this third convolutional will will need to apply that again.

142
00:09:57,220 --> 00:10:04,630
So I'm copying this pasting that below and replacing convolution too by convolution 3 and that's done

143
00:10:04,870 --> 00:10:06,340
isn't it so practical.

144
00:10:06,340 --> 00:10:11,490
We propagate the signals in the three convolutional letters in a flashlight.

145
00:10:11,500 --> 00:10:13,100
Thanks to this awesome structure.

146
00:10:14,020 --> 00:10:15,340
All right so perfect.

147
00:10:15,340 --> 00:10:21,250
Now we have our signals propagated up to the third convolutional layer and after.

148
00:10:21,360 --> 00:10:24,780
And speaking of after that leads us to what we're looking for.

149
00:10:24,820 --> 00:10:28,510
What we're interested in that is the flattening there.

150
00:10:28,510 --> 00:10:33,620
All right so now that we have our third convolutional there that's the last X here.

151
00:10:33,790 --> 00:10:36,120
It's time to get our flattening out.

152
00:10:36,490 --> 00:10:37,990
And so that's exactly what we're going to do.

153
00:10:37,990 --> 00:10:44,200
Now we're going to flatten all the pixels of this third convolutional layer that is we're going to take

154
00:10:44,290 --> 00:10:48,430
all the pixels of all the channels of the third convolutional layer.

155
00:10:48,640 --> 00:10:51,920
We're going to put them one after the other in a huge vector.

156
00:10:52,150 --> 00:10:56,920
And of course this huge vector is going to be nothing else than the flattening layer and at the same

157
00:10:56,920 --> 00:11:02,000
time we will use a trick to get the number of neurons in this flattening there.

158
00:11:02,070 --> 00:11:03,790
That is what we're looking for.

159
00:11:03,790 --> 00:11:09,370
That's the number of neurons we're missing and therefore let's directly return what we want and in this

160
00:11:09,370 --> 00:11:15,130
return we're going to flatten the third convolutional layer and get at the same and the number of neurons

161
00:11:15,130 --> 00:11:16,330
in this flattening layer.

162
00:11:16,630 --> 00:11:20,150
So we're going to take X. which is our third convolutional there.

163
00:11:20,320 --> 00:11:25,150
We're going to take all the channels of the third convolutional there and we're going to use a function

164
00:11:25,150 --> 00:11:32,160
which is the size function to flatten all the pixels of all these channels in one same huge vector.

165
00:11:32,230 --> 00:11:35,930
And so the trick you can find it in the pite watch the toil.

166
00:11:36,130 --> 00:11:42,400
Well first we take the data of X because X is a special structure you know it's a torch Voivode So he

167
00:11:42,400 --> 00:11:44,250
has a pretty complex structure.

168
00:11:44,320 --> 00:11:51,920
But first we need to access it with data here then we need to view what's inside of it.

169
00:11:52,090 --> 00:11:57,700
So we use this view function and now we need to access what we're looking for and that is even with

170
00:11:57,700 --> 00:12:01,980
the arguments 1 and minus 1.

171
00:12:02,230 --> 00:12:04,390
You have to understand what's inside the structure.

172
00:12:04,510 --> 00:12:10,060
But you can just understand that this is how we're going to get this number of neurons and then to finish

173
00:12:10,150 --> 00:12:15,560
we need to add size in parenthesis and inside inputs 1.

174
00:12:15,880 --> 00:12:21,610
So basically what we do here what we do is we take all the pixels of all the channels and we put them

175
00:12:21,610 --> 00:12:27,180
one after the other in this huge vector which will be the input of the fully connected network.

176
00:12:27,190 --> 00:12:29,250
That's basically what the size does.

177
00:12:29,410 --> 00:12:34,060
And with this we can get this number of new ones that we're looking for.

178
00:12:34,060 --> 00:12:36,460
All right so now we get what we want.

179
00:12:36,490 --> 00:12:44,350
And so finally we can replace number neurons here by what is returned by this function when it is applied

180
00:12:44,530 --> 00:12:47,400
to the format of the images.

181
00:12:47,440 --> 00:12:50,140
That is one by 80 by 80.

182
00:12:50,170 --> 00:13:00,040
So what we have to do now is replace number neurons by we take the count neurons function which we apply

183
00:13:00,250 --> 00:13:09,040
to the format of the images which will be the total one 80 and 80.

184
00:13:09,460 --> 00:13:10,400
And there we go.

185
00:13:10,510 --> 00:13:17,140
And of course we don't forget self because Count neuron is actually a method of the CNN test.

186
00:13:17,170 --> 00:13:18,490
So we need to add the.

187
00:13:18,700 --> 00:13:21,190
And now the warning should disappear.

188
00:13:21,190 --> 00:13:22,540
And there we go.

189
00:13:22,540 --> 00:13:23,890
Now everything is good.

190
00:13:23,980 --> 00:13:29,930
We get the architecture of the neural network with nothing missing and we have this countersuits function

191
00:13:30,140 --> 00:13:35,290
in case you know you want to try some other architectures and you don't want to count the number of

192
00:13:35,290 --> 00:13:36,210
neurons manually.

193
00:13:36,340 --> 00:13:39,940
You just use this function you play it to the format of your images.

194
00:13:40,180 --> 00:13:42,130
And this will get you directly what you want.

195
00:13:42,130 --> 00:13:45,810
That is the number of neurons in the flooding layer without having to do anything.

196
00:13:45,820 --> 00:13:49,450
And wherever the architecture is that's critical.

197
00:13:49,480 --> 00:13:57,040
And now we're done with the first big important step of this brain that we're making and we have one

198
00:13:57,040 --> 00:13:57,720
last step.

199
00:13:57,790 --> 00:14:02,500
That is one last function to make which is going to be the main forward function.

200
00:14:02,590 --> 00:14:07,730
So we are going to propagate the signals from the beginning of the brain that is from the eyes of the

201
00:14:07,730 --> 00:14:12,160
eye to the output layer that is after the second connection.

202
00:14:12,460 --> 00:14:15,850
So we'll do that in the next two toile and until then our AI.
