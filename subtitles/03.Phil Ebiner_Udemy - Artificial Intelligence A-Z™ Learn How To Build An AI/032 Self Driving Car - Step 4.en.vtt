WEBVTT
1
00:00:00.520 --> 00:00:02.690
Hello and welcome to this Python tutorial.

2
00:00:02.860 --> 00:00:05.680
All right so we have very exciting to soil's ahead of us.

3
00:00:05.680 --> 00:00:10.690
We are going to start by creating the architecture of the neural network that is we will make the neural

4
00:00:10.690 --> 00:00:16.740
network that will be at the heart of our AI and that will return the action to play at age 90.

5
00:00:16.930 --> 00:00:18.060
So let's do this.

6
00:00:18.130 --> 00:00:23.130
So since we want our neural network to be objective we're going to make it class.

7
00:00:23.200 --> 00:00:25.100
And that's because it's much more convenient.

8
00:00:25.180 --> 00:00:28.370
You know the class is the model of something we want to build.

9
00:00:28.540 --> 00:00:34.000
We want to build a neural network and we need to make some kind of instructions which will all be contained

10
00:00:34.090 --> 00:00:35.080
in the class.

11
00:00:35.140 --> 00:00:37.910
And in this class we're going to make two functions.

12
00:00:37.960 --> 00:00:42.810
First the init function which is the function that comes up all the time when making class.

13
00:00:43.000 --> 00:00:47.770
And that basically defines the variable of your object that is the neural network.

14
00:00:47.920 --> 00:00:52.600
You know the variables attached to the object as opposed to the global variables.

15
00:00:52.840 --> 00:00:57.910
And so this is in this function that will define the architecture of the new network you know defining

16
00:00:57.910 --> 00:01:03.460
the input layer which will be composed of five input neurons because we have five dimensions for the

17
00:01:03.790 --> 00:01:05.680
encoded vector of input states.

18
00:01:05.890 --> 00:01:08.080
Then we will define some hidden layers.

19
00:01:08.110 --> 00:01:12.940
Maybe you will start with one hidden layer and then you will be welcome to try some other architectures

20
00:01:12.940 --> 00:01:13.920
of the neural network.

21
00:01:14.200 --> 00:01:20.080
And then of course we will end up with the output layer that will contain the possible actions that

22
00:01:20.110 --> 00:01:21.940
we can play at each time.

23
00:01:22.180 --> 00:01:24.730
So that's exactly what we'll do in this function.

24
00:01:24.850 --> 00:01:30.460
And then we will make another function still inside the class which will be the forward function and

25
00:01:30.460 --> 00:01:34.870
that will be the function that will activate the neurons in the neural network.

26
00:01:34.870 --> 00:01:40.570
You know this will activate the signals and so we will use a rectified activation function because of

27
00:01:40.570 --> 00:01:46.920
course we're dealing with a purely nonlinear problem and this rectified function breaks the linearity.

28
00:01:47.200 --> 00:01:53.340
But mostly we're making this Ford function to return the q values which are the outputs of the network.

29
00:01:53.500 --> 00:01:56.110
But we have one key value for each action.

30
00:01:56.200 --> 00:02:02.710
And later on we'll be returning the final action by either taking the max of the key values or using

31
00:02:02.710 --> 00:02:03.880
a soft Max method.

32
00:02:04.030 --> 00:02:05.270
We will see that afterwards.

33
00:02:05.440 --> 00:02:10.330
So in this tutorial we're going to start by implementing the init function and then the next one will

34
00:02:10.330 --> 00:02:12.880
be implementing the forward function.

35
00:02:12.880 --> 00:02:13.810
So let's do this.

36
00:02:13.810 --> 00:02:17.060
First we need to introduce our class.

37
00:02:17.200 --> 00:02:22.930
So we start with class and we give a name to our class which is where we can call it network.

38
00:02:23.650 --> 00:02:29.980
And then in this network class I'm going to use an object programming technique which is called inheritance

39
00:02:30.310 --> 00:02:35.120
and that is just to inherit from all the tools of a parent class.

40
00:02:35.200 --> 00:02:41.550
So our network class that we're about to make is a child test of a larger class which is.

41
00:02:41.770 --> 00:02:44.110
And that module.

42
00:02:44.470 --> 00:02:50.790
So that's just to inherit from all the tools of this module class which of course the tools to implement

43
00:02:50.790 --> 00:02:51.940
a neural network.

44
00:02:51.940 --> 00:02:57.880
So that's a very powerful and evolution trick in object oriented programming that's going heritance.

45
00:02:57.910 --> 00:03:02.670
And right now we are inheriting from this module parent class.

46
00:03:02.800 --> 00:03:06.000
All right and now we're ready to go inside the class.

47
00:03:06.130 --> 00:03:12.910
So I'm pressing enter twice actually because we'll be making two functions and we're starting with the

48
00:03:13.140 --> 00:03:14.280
end function.

49
00:03:14.350 --> 00:03:20.470
So the init function we have to name it this way with two on this course then in it.

50
00:03:20.650 --> 00:03:25.750
And then again to underscores that's just Python syntax that is just how we have to do it.

51
00:03:25.930 --> 00:03:28.460
And then we need to put the arguments.

52
00:03:28.720 --> 00:03:30.340
So we have three arguments.

53
00:03:30.340 --> 00:03:35.800
The first one is a compulsory argument that is actually self and Self.

54
00:03:35.800 --> 00:03:42.250
There is no mystery about it that refers to the object that will be created from this class that we're

55
00:03:42.250 --> 00:03:42.810
about to make.

56
00:03:42.820 --> 00:03:44.400
You know we're making this class.

57
00:03:44.470 --> 00:03:48.980
It's like some instruction some model of this neural network we want to build.

58
00:03:49.180 --> 00:03:53.620
And then once the class is ready we can make as many with networks as we want.

59
00:03:53.620 --> 00:04:00.130
And each of these new networks will be some object of this class and since we will be using the object

60
00:04:00.400 --> 00:04:07.630
for some other purposes we need to but what are the variables of the object and to Spudis we're using

61
00:04:07.630 --> 00:04:11.900
this self here to specify that we're referring to the object.

62
00:04:12.100 --> 00:04:18.430
So whenever I want to use available from my object I will use self before the variable to specify that

63
00:04:18.430 --> 00:04:21.340
this is a variable of the object.

64
00:04:21.340 --> 00:04:25.680
All right so that's the first argument and then we have two other arguments which are of course the

65
00:04:25.680 --> 00:04:30.240
number of input neurons and the number of output neurons.

66
00:04:30.550 --> 00:04:38.220
So the number of input neurons we're going to call it input size and that's actually five because our

67
00:04:38.310 --> 00:04:45.890
input vectors have five dimensions to three signals plus orientation plus minus orientation that are

68
00:04:45.900 --> 00:04:51.020
vectors of encoded values that describe one state of the environment.

69
00:04:51.210 --> 00:04:54.960
These five values are enough to describe a state of the environment.

70
00:04:54.960 --> 00:05:00.600
We could have thought of Less values or more values but that's what I tried and it actually makes sense

71
00:05:00.600 --> 00:05:05.520
because we actually need one signal from the left one in front of us and one at the right.

72
00:05:05.520 --> 00:05:10.860
You know when we're driving a car we could have gone for a 360 signal you know like the signals at the

73
00:05:10.860 --> 00:05:17.370
top of the Google cars that we can totally self-drive with three sensors and then we have this orientation

74
00:05:17.400 --> 00:05:22.200
and minus orientation to you know keep track of the goal that you were trying to reach.

75
00:05:22.530 --> 00:05:29.160
And then we have of course the output neurons of our network which correspond to the actions and we

76
00:05:29.160 --> 00:05:32.840
have three possible actions going left going straight or going right.

77
00:05:32.880 --> 00:05:38.520
And therefore I'm going to call it and the action and there will be three of them.

78
00:05:38.520 --> 00:05:39.030
All right.

79
00:05:39.120 --> 00:05:44.850
So far we only have to give names to the inputs and then we'll use these volleyballs to do the conditions

80
00:05:44.940 --> 00:05:46.140
inside the neural network.

81
00:05:47.090 --> 00:05:55.010
All right then and you start by using another by torch trick this trick is a superfunction that's a

82
00:05:55.010 --> 00:05:59.310
function that actually inherits from the module.

83
00:05:59.390 --> 00:06:02.730
So that's why we had to use inheritance to inherited the module too.

84
00:06:02.750 --> 00:06:04.440
This is the first to use.

85
00:06:04.520 --> 00:06:11.120
And so basically we're only using this super trick superfunction to be able to use the tools of module

86
00:06:11.580 --> 00:06:13.320
that's much more efficient.

87
00:06:13.670 --> 00:06:18.620
And inside the superfunction I just need to specify first the network.

88
00:06:18.650 --> 00:06:25.100
So that's our network chul class you know because this is inheriting from the module parent class and

89
00:06:25.550 --> 00:06:27.360
then our object.

90
00:06:27.380 --> 00:06:35.220
And then I'm just adding that and are in a function like this exactly how we named it.

91
00:06:35.570 --> 00:06:39.350
All right so that's just a trick that's just to use all the tools I have.

92
00:06:39.360 --> 00:06:46.270
And in that module then we can move on to the next step which is to specify the input layer.

93
00:06:46.550 --> 00:06:53.300
So basically what I have to do is introduce a new variable that will be attached to the object and this

94
00:06:53.300 --> 00:06:57.120
variable will contain the number of input neurons.

95
00:06:57.170 --> 00:07:05.140
So not to be confused with input size input size is the argument of the end function.

96
00:07:05.180 --> 00:07:09.710
But that's not the variable that is attached to the object yet the variable that is attached to the

97
00:07:09.710 --> 00:07:10.360
object.

98
00:07:10.520 --> 00:07:16.520
Well as I just mentioned we need to specify that it is instead attached to the object so we use a self

99
00:07:17.230 --> 00:07:22.130
taught and now we give a name to this first variable attached to the object.

100
00:07:22.190 --> 00:07:24.870
And so we can simply give the same name as the input.

101
00:07:24.920 --> 00:07:33.650
We can call it input size and we will say it is equal to the arguments a function that is input size.

102
00:07:33.680 --> 00:07:34.080
All right.

103
00:07:34.130 --> 00:07:39.900
Each time I'm creating an object from the network class and I'm specifying the input size like for example

104
00:07:39.900 --> 00:07:41.170
I'm putting 5.

105
00:07:41.180 --> 00:07:47.330
There will be of 5 here and therefore the input size variable of our object will have the value of 5

106
00:07:47.690 --> 00:07:54.110
because this input size here will be 5 and therefore our new network will have 5 input neurons in the

107
00:07:54.110 --> 00:07:55.470
input layer.

108
00:07:55.490 --> 00:07:55.790
All right.

109
00:07:55.790 --> 00:08:02.180
And then that's the same for the other variable that we want to attach to objects.

110
00:08:02.210 --> 00:08:08.100
And as you might have guessed this is going to be a variable for the number of output neurons.

111
00:08:08.330 --> 00:08:15.030
And to say we take our object self and then we give a name to this second variable of the object we

112
00:08:15.040 --> 00:08:17.740
are going to call it and the action.

113
00:08:18.170 --> 00:08:23.600
And this will be equal to this argument here given the number of actions that is the number of output

114
00:08:23.600 --> 00:08:24.250
neurons.

115
00:08:24.530 --> 00:08:30.850
And so we set it equal to the action actually and the action will be equal to three.

116
00:08:30.890 --> 00:08:37.930
Therefore the variable and the action attached to our object to a network will get the value of three.

117
00:08:38.240 --> 00:08:41.680
Actually we can see warning here it says undefined name.

118
00:08:41.720 --> 00:08:44.180
And then well that's because here we use the.

119
00:08:44.230 --> 00:08:46.030
And then shortcut.

120
00:08:46.350 --> 00:08:48.530
And we need to use a shortcut here.

121
00:08:48.590 --> 00:08:52.780
And for our torche start and in Mudgal and then it will disappear.

122
00:08:52.880 --> 00:08:53.670
Here we go.

123
00:08:53.690 --> 00:08:54.580
Perfect.

124
00:08:54.590 --> 00:09:00.800
Right now we have new warnings all the warnings here are just to specify that what we import is not

125
00:09:00.800 --> 00:09:01.520
yet used.

126
00:09:01.580 --> 00:09:04.660
That's ok we will be using them afterwards.

127
00:09:04.670 --> 00:09:10.010
All right then we have another two variables we want to define for object.

128
00:09:10.190 --> 00:09:15.820
And this will be the full connections the full connections between the different layers of our neural

129
00:09:15.830 --> 00:09:16.810
network.

130
00:09:16.820 --> 00:09:21.800
So since right now we want to make a neural network composed of only one head in their world there will

131
00:09:21.800 --> 00:09:23.440
be two full connections.

132
00:09:23.570 --> 00:09:27.740
There will be one first full connection between the input layer and the hidden layer.

133
00:09:27.980 --> 00:09:32.450
And one second full connection between the hill there and the output layer.

134
00:09:32.480 --> 00:09:34.770
So let's start with the first full connection.

135
00:09:34.890 --> 00:09:43.310
We're going to call it SE1 And again I use self here to specify that FC one is a variable of my object

136
00:09:43.780 --> 00:09:44.530
to solve that.

137
00:09:44.530 --> 00:09:47.490
FC one which will be equal to.

138
00:09:47.630 --> 00:09:55.160
And now we use the N in module and we're going to use a function called linear R and that's exactly

139
00:09:55.160 --> 00:10:02.080
to make this full connection between the neurons and the input layer to the neurons of the hidden there.

140
00:10:02.180 --> 00:10:04.090
And what do I mean by full connection.

141
00:10:04.160 --> 00:10:09.190
That means that all the neurons of the input layer will all be connected to all the neurons of the Here

142
00:10:09.190 --> 00:10:09.920
in there.

143
00:10:10.190 --> 00:10:16.140
And so to make this connection we use this linear function to which we need to put some arguments.

144
00:10:16.190 --> 00:10:19.880
And as you can see these arguments are in features.

145
00:10:19.880 --> 00:10:25.370
So that is the number of neurons of the first law you want to connect them out features that is the

146
00:10:25.370 --> 00:10:30.110
number of neurons of the second layer you want to connect that is the layer at the right that is the

147
00:10:30.110 --> 00:10:32.360
hidden layer and bicycle's true.

148
00:10:32.420 --> 00:10:38.850
So bicycle's true we will keep the default value that's in order to have a bias and not only some weight

149
00:10:38.900 --> 00:10:43.350
attached to the run we'll have to wait and one bias for each layer.

150
00:10:43.610 --> 00:10:46.140
And so well let's see what we need to input.

151
00:10:46.280 --> 00:10:51.850
So the first argument in features is the number of input neurons in the input layer.

152
00:10:52.000 --> 00:10:52.930
And so where is it.

153
00:10:53.030 --> 00:10:55.080
Well that's actually imprecise.

154
00:10:55.100 --> 00:11:01.930
That's the argument of our init function which later we'll be able to fight the three signals orientation

155
00:11:02.200 --> 00:11:04.150
and Mannus orientation.

156
00:11:04.160 --> 00:11:05.020
So here we go.

157
00:11:05.190 --> 00:11:14.300
When the first arguments and put size and then the second argument is out features that is that is the

158
00:11:14.300 --> 00:11:17.090
number of neurons we want to have in the second layer.

159
00:11:17.180 --> 00:11:20.450
The second layer that will be fully connected to the first layer.

160
00:11:20.450 --> 00:11:24.960
And so now the question is how many neurons do we want in this hidden layer.

161
00:11:25.220 --> 00:11:27.420
Well I did a lot of parameter training.

162
00:11:27.440 --> 00:11:29.110
I did a lot of experimenting.

163
00:11:29.210 --> 00:11:31.940
That's what we do or that's what we do in deeper.

164
00:11:31.940 --> 00:11:38.270
In general we do a lot of experimenting to see what would be the best neural network for our specific

165
00:11:38.270 --> 00:11:39.170
problem.

166
00:11:39.170 --> 00:11:45.950
And so I try many values and I ended up choosing 30 30 runs in a hidden layer and you will see that

167
00:11:45.950 --> 00:11:50.750
with this number we will get some pretty good results but then feel free to change the architecture

168
00:11:50.750 --> 00:11:51.580
of the neural network.

169
00:11:51.580 --> 00:11:53.120
Feel free to play with it.

170
00:11:53.180 --> 00:11:58.730
You can not only change the number of neurons in the here and there but also you can add some more layers

171
00:11:59.150 --> 00:12:05.000
so that maybe you get an even better car but 30 hinna neurons will get us a good neural network and

172
00:12:05.000 --> 00:12:06.000
a good cause.

173
00:12:06.020 --> 00:12:07.390
So that's what we go for.

174
00:12:07.520 --> 00:12:08.410
And there we go.

175
00:12:08.420 --> 00:12:13.500
We have our first full connection really with this linear function.

176
00:12:13.520 --> 00:12:16.910
We make the connection between the input layer and the hidden leg.

177
00:12:17.360 --> 00:12:23.270
And now time to make the second full connection that is the full connection between the hidden layer

178
00:12:23.600 --> 00:12:25.180
and the output layer.

179
00:12:25.490 --> 00:12:26.750
So there we go.

180
00:12:26.750 --> 00:12:31.380
We're going to call this second full connection at C2.

181
00:12:31.490 --> 00:12:32.280
There we go.

182
00:12:32.360 --> 00:12:36.400
And still this is available for more objects using Saphir.

183
00:12:36.650 --> 00:12:38.330
And then again we use.

184
00:12:38.450 --> 00:12:45.310
Well actually we can copy this because we're going to use the N in module and then the linear function.

185
00:12:45.530 --> 00:12:49.250
But then we need to change the arguments of course first.

186
00:12:49.280 --> 00:12:55.050
That's the same first is the number of neurons we're going to have in the first layer of the connection.

187
00:12:55.190 --> 00:12:56.510
So that's the hidden there.

188
00:12:56.720 --> 00:13:03.810
And therefore that is 30 and then second argument is the number of neurons in the second layer of the

189
00:13:04.010 --> 00:13:08.810
connection and that corresponds to the output layer and the output there has.

190
00:13:08.980 --> 00:13:15.020
And the actual neurons which later will be three because we have three possible actions but so far we

191
00:13:15.020 --> 00:13:16.930
have to use the names we defined.

192
00:13:17.050 --> 00:13:23.990
That is the name of the argument of the init function and therefore we input here and the action and

193
00:13:23.990 --> 00:13:24.950
there we go.

194
00:13:24.950 --> 00:13:27.760
First of all our tuple connections re.

195
00:13:27.920 --> 00:13:30.980
And second of all are any function Israel.

196
00:13:31.400 --> 00:13:36.940
So that's what we'll initialize our object whenever we create an object from the network class.

197
00:13:37.130 --> 00:13:44.300
And so as soon as we create an object well all these variables for variables here input size and reaction.

198
00:13:44.380 --> 00:13:46.980
You an and two will be defined.

199
00:13:47.180 --> 00:13:52.060
And that's how we'll get the architecture of our animal network for each object that we create.

200
00:13:52.160 --> 00:13:59.450
Each object will correspond to a neural network of five input neurons 30 hidden neurons and three output

201
00:13:59.450 --> 00:14:00.440
neurons.

202
00:14:00.470 --> 00:14:01.430
So there we go.

203
00:14:01.430 --> 00:14:06.980
We are done with this first function and now we can move on to the second function which is the forward

204
00:14:06.980 --> 00:14:13.100
function and that will be used to activate the neurons in the neural network using the rectifier activation

205
00:14:13.100 --> 00:14:19.500
function and mostly to eventually return to cube values which are the outputs of only one network.

206
00:14:19.580 --> 00:14:23.420
So I can't wait to do this in the next tutorial and until then I.
