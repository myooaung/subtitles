1
00:00:00,300 --> 00:00:02,930
Hello and welcome to this Python tutorial.

2
00:00:02,940 --> 00:00:08,700
All right so we just build the architecture of a neural network with the function of our network class

3
00:00:09,030 --> 00:00:13,650
and then we're going to make a second function which is going to be the forward function and that's

4
00:00:13,650 --> 00:00:15,810
the function that will activate the neurons.

5
00:00:15,840 --> 00:00:19,770
That is the function that will perform for propagation.

6
00:00:19,830 --> 00:00:27,240
So let's do this let's make this function let's call it forward as we just said and this function is

7
00:00:27,240 --> 00:00:29,550
going to take two arguments.

8
00:00:29,610 --> 00:00:35,070
First is as usual self you know to be able to use the variables of the object because we're going to

9
00:00:35,070 --> 00:00:37,400
use SE1 NFC to.

10
00:00:37,500 --> 00:00:43,440
So we need the self to be able to use these variables and then we're going to need a second argument

11
00:00:43,590 --> 00:00:50,100
which is our input and we're going to call it state because state is exactly the input of our neural

12
00:00:50,100 --> 00:00:50,580
networks.

13
00:00:50,580 --> 00:00:51,980
You know that's the States.

14
00:00:51,990 --> 00:00:57,320
There are other inputs entering the neural network and then as outputs will have the q values of the

15
00:00:57,330 --> 00:01:00,560
three possible actions go left go straight or go right.

16
00:01:00,690 --> 00:01:06,210
But we don't need to put it as an argument here because that's exactly what we want to return to this

17
00:01:06,210 --> 00:01:11,880
forward function is not only going to activate the neurons but also and mostly it will return the cube

18
00:01:11,880 --> 00:01:16,640
values for each possible action depending on the input state here.

19
00:01:16,650 --> 00:01:17,060
All right.

20
00:01:17,100 --> 00:01:19,000
So that's the two arguments we need.

21
00:01:19,140 --> 00:01:24,440
And now let's go inside the function and let's specify what we wanted to do.

22
00:01:24,740 --> 00:01:29,940
OK so the first thing we're going to do is activate the hidden neurons and we we're going to call the

23
00:01:29,940 --> 00:01:34,690
hidden neurons by the variable x so x represents the hidden neurons.

24
00:01:34,800 --> 00:01:36,750
And so how to activate them.

25
00:01:36,930 --> 00:01:39,990
Well of course we're going to take our input neurons.

26
00:01:40,170 --> 00:01:45,360
We're going to use our first full connection if you want to get the hidden neurons and then we're going

27
00:01:45,360 --> 00:01:49,570
to apply inactivation function on them which will be the rectified function.

28
00:01:49,860 --> 00:01:51,450
So how are you going to do that.

29
00:01:51,450 --> 00:01:54,680
Remember we imported the torch.

30
00:01:54,730 --> 00:02:01,450
And it's that functional module that contains all the functions in order to implement the neural network.

31
00:02:01,560 --> 00:02:02,960
And we gave it the shortcut.

32
00:02:03,450 --> 00:02:08,130
So actually what we're going to do now is we're going to use one of these functions from the functional

33
00:02:08,130 --> 00:02:11,240
module and this function is to really function.

34
00:02:11,430 --> 00:02:16,490
So what is really relevant is the rectifier function that you saw in the intuition lectures.

35
00:02:16,620 --> 00:02:19,040
That's just and then given to the rectifier function.

36
00:02:19,170 --> 00:02:26,130
But since this function is taken from and in that functional which was given the shortcut we need to

37
00:02:26,130 --> 00:02:31,860
type here first f thoughts and then that's where we can take this function.

38
00:02:31,990 --> 00:02:36,610
And actually if I type are we here we go we have the real function.

39
00:02:36,630 --> 00:02:41,980
So that's directed by a function that will activate the hidden neurons that is x.

40
00:02:42,030 --> 00:02:47,640
So in this really function now we understand perfectly what we have to input that's the neurons that

41
00:02:47,640 --> 00:02:48,920
we want to activate.

42
00:02:48,990 --> 00:02:54,300
That is the hidden neurons and so to get these hidden neurons we are going to take our first full connection

43
00:02:54,520 --> 00:03:01,540
one which we will apply to our input neurons to go from the input neurons to the neurons.

44
00:03:01,800 --> 00:03:04,450
So let's take our first full connection one.

45
00:03:04,620 --> 00:03:08,070
But our first full connection is a variable of our object.

46
00:03:08,130 --> 00:03:12,070
Therefore we need to type here first self.

47
00:03:12,120 --> 00:03:13,550
FC one here we go.

48
00:03:13,620 --> 00:03:16,660
That's the first full connection of our neural network.

49
00:03:16,980 --> 00:03:22,890
And inside this personal connection we are going to input our input states to go from the input neurons

50
00:03:23,010 --> 00:03:24,210
to the hidden neurons.

51
00:03:24,480 --> 00:03:30,680
And so since we gave it the name STATE Well here we have two input state and there we go.

52
00:03:30,720 --> 00:03:34,360
We now get activated hidden neurons.

53
00:03:34,510 --> 00:03:35,000
All right.

54
00:03:35,040 --> 00:03:40,610
And now that we have the hidden neurons we are going to return the output neurons.

55
00:03:40,640 --> 00:03:46,240
So next line and as you understood the output neurons correspond to our actions.

56
00:03:46,370 --> 00:03:48,170
But these are not the actions directly.

57
00:03:48,290 --> 00:03:55,760
These are the q values because we're building a digital model that combines a deeper model to Q learning

58
00:03:56,020 --> 00:04:02,090
and therefore we use q learning here to get the q values for each of our actions and then later using

59
00:04:02,090 --> 00:04:09,560
soughed Max or in our Max we will get the final action so here the Voivode I'm about to introduce will

60
00:04:09,560 --> 00:04:13,560
correspond to the APA nuance and since the output neurons are the key values.

61
00:04:13,640 --> 00:04:15,590
Well I'm going to call this variable.

62
00:04:15,590 --> 00:04:24,630
Q That means that we go to q values and now we directly take our food connection which is the variable.

63
00:04:24,810 --> 00:04:27,010
To put a variable from our object.

64
00:04:27,020 --> 00:04:33,830
So we take here self taught F C to and of course here we input the neurons of the left side of this

65
00:04:34,010 --> 00:04:34,860
connection.

66
00:04:34,910 --> 00:04:38,050
That is what we got from the first line which is x.

67
00:04:38,270 --> 00:04:40,770
So x There we go.

68
00:04:40,790 --> 00:04:42,650
We now get our Q values.

69
00:04:42,680 --> 00:04:45,790
That's the output neurons of our neural network.

70
00:04:46,100 --> 00:04:46,800
OK.

71
00:04:47,060 --> 00:04:53,050
And then the last line of code of course this forward function is used to return these values.

72
00:04:53,210 --> 00:05:02,900
So we just have to add a return and simply Q values and that will return the key values for each possible

73
00:05:02,900 --> 00:05:06,310
action go left go straight or go right.

74
00:05:06,370 --> 00:05:06,760
All right.

75
00:05:06,760 --> 00:05:08,370
So congratulations.

76
00:05:08,370 --> 00:05:13,720
We're done with our first class and actually we were done making the architecture of the neural network.

77
00:05:13,720 --> 00:05:15,550
Remember this is not finished job.

78
00:05:15,550 --> 00:05:19,890
You can always improve the architecture of the neural network by trying different ones.

79
00:05:20,020 --> 00:05:22,780
So feel free to do that by adding more neurons here.

80
00:05:22,780 --> 00:05:28,270
For example if you want to add 50 hidden neurons you can just replace the 30 here and the 30 here by

81
00:05:28,270 --> 00:05:34,860
50 50 and 50 and then you can add some more hidden layers by making some useful connections.

82
00:05:34,990 --> 00:05:37,630
Well that's really the job of an artist.

83
00:05:37,660 --> 00:05:41,980
There is no general rule of what would be the best architecture in each situation.

84
00:05:42,040 --> 00:05:43,720
So that's why we have to experiment.

85
00:05:43,940 --> 00:05:50,140
Let's let's try with that you will see that we'll get eventually a pretty good self-driving car.

86
00:05:50,170 --> 00:05:50,590
All right.

87
00:05:50,620 --> 00:05:55,960
And now we're going to make the next class which is about experience replay and we will be making that

88
00:05:55,990 --> 00:05:57,450
in the next three to two hours.

89
00:05:57,610 --> 00:05:59,080
Until then enjoy AI.
