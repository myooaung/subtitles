1
00:00:00,450 --> 00:00:05,400
Hello and welcome to this path into Torro now we're going to make the forward function that will forward

2
00:00:05,400 --> 00:00:11,040
propagate the signal through all the brain from the very beginning with the input images up to the outputs

3
00:00:11,130 --> 00:00:16,860
which will contain the key values for the actor and the de-value that is value taken by the function

4
00:00:17,040 --> 00:00:17,950
for the critic.

5
00:00:18,240 --> 00:00:23,820
So it's going to be quite similar as what we did for Dume but this time something is going to change.

6
00:00:23,880 --> 00:00:25,550
Amending is going to change that.

7
00:00:25,560 --> 00:00:27,510
Now we have an illustration in the brain.

8
00:00:27,510 --> 00:00:32,470
So we all have to do something more to propagate the signal and be careful with that.

9
00:00:32,850 --> 00:00:38,280
And the other thing less important but still that changes compared to before is that we're not going

10
00:00:38,280 --> 00:00:44,220
to use a real activation function as you know the nonlinear activation function but we're going to use

11
00:00:44,220 --> 00:00:48,880
the loo which is kind of a more sophisticated really function.

12
00:00:48,980 --> 00:00:51,390
You will see that in the past we're talking mentation.

13
00:00:51,780 --> 00:00:55,970
So let's do this let's make this function we start with a death.

14
00:00:56,040 --> 00:00:59,190
It's actually the last function of this execrated class.

15
00:00:59,280 --> 00:01:06,900
So we're going to call it forward like Einstein and this fourth function is going to take self the object

16
00:01:06,900 --> 00:01:09,930
because we're going to use the objects and the inputs.

17
00:01:10,170 --> 00:01:13,520
So important to understand what will these inputs be.

18
00:01:13,590 --> 00:01:19,170
This will not only be the input images these inputs will also contain the hidden nodes and the cell

19
00:01:19,170 --> 00:01:20,980
nodes of the same.

20
00:01:21,090 --> 00:01:25,120
So that's why I wanted to highlight that some things are going to change now.

21
00:01:25,180 --> 00:01:30,820
Basically we're considering the forward function the Internet and cell nodes of the list.

22
00:01:31,200 --> 00:01:38,900
And speaking of them now what we're going to do is separate these two inputs of this argument in putting

23
00:01:38,900 --> 00:01:41,540
the forward function and how can we separate them.

24
00:01:41,700 --> 00:01:46,160
Well we can recall a new Voivode which will be the inputs images.

25
00:01:46,230 --> 00:01:55,620
So that's designed input images and we separate them with the topple H x and C X which is a temple of

26
00:01:55,620 --> 00:01:59,090
the hidden states and the cell States at the Coliseum.

27
00:01:59,370 --> 00:02:04,470
So H x other states and C X are the South states.

28
00:02:04,540 --> 00:02:09,580
All right and that will be equal to the input that is this argument here.

29
00:02:09,580 --> 00:02:14,950
So now we made that separation and therefore we can start to propagate the signal throughout the brain

30
00:02:15,220 --> 00:02:21,400
and to do that we are going to get successively different layers from the first one to the last one

31
00:02:21,700 --> 00:02:28,690
by using our connections that is the convolutions the LACMA connection and the linear connection here

32
00:02:29,020 --> 00:02:30,300
the full connections.

33
00:02:30,490 --> 00:02:31,400
So let's do this.

34
00:02:31,450 --> 00:02:33,620
Now it's going to be the same as before.

35
00:02:33,750 --> 00:02:40,420
We're going to get our first layer that we're going to call X and get this first layer we need to propagate

36
00:02:40,510 --> 00:02:46,240
the signal from the inputs to this first layer and therefore we need to use the first convolution because

37
00:02:46,240 --> 00:02:51,770
it is the first convolution that propagates the signal from the input images to the first layer.

38
00:02:52,030 --> 00:02:58,450
So what we're going to do now is copy this because this is the first convolution we had here and we

39
00:02:58,450 --> 00:03:06,100
apply this first convolution to our input images which are now the right inputs and that we get that

40
00:03:06,100 --> 00:03:09,560
propagates the signal from the input images to the first layer.

41
00:03:09,820 --> 00:03:16,390
But now remember we have to use a nonlinear activation function to break the linearity in order to be

42
00:03:16,390 --> 00:03:23,170
able to learn the non-linear relationships inside images and to do this we are going to use as we said

43
00:03:23,500 --> 00:03:28,050
the activation function that we're about to see right now and that by touch stuck.

44
00:03:28,180 --> 00:03:30,110
But before that let's get it.

45
00:03:30,130 --> 00:03:36,430
So to get it it's like really we take the functional module which has a shortcut and then that and then

46
00:03:36,490 --> 00:03:45,790
a loop and then we put all this in parentheses because we want to nonlinearly activate the neurons.

47
00:03:45,880 --> 00:03:51,160
This first layer here that we obtained by applying the first convolution on the inputs.

48
00:03:51,160 --> 00:03:55,310
So now let's go to the PI torch Doug to understand what it is.

49
00:03:55,360 --> 00:03:56,110
Here it is.

50
00:03:56,200 --> 00:04:00,320
So you can access it to torch stuck or slash slash.

51
00:04:00,340 --> 00:04:06,940
And then that H G and L and then you have to find non-linear activations and then the nonlinear activation

52
00:04:06,940 --> 00:04:08,080
functions you will find.

53
00:04:08,170 --> 00:04:14,040
Well really that's the classic when we know it's just a max of zero and X you have the graph in mind.

54
00:04:14,260 --> 00:04:17,160
Then you have six which is this one.

55
00:04:17,160 --> 00:04:19,350
So a little more sophisticated.

56
00:04:19,570 --> 00:04:21,190
And then we go we have a look.

57
00:04:21,550 --> 00:04:26,470
And as you can see blue is a redo plus an additional element.

58
00:04:26,620 --> 00:04:29,280
So it's like a more sophisticated really.

59
00:04:29,340 --> 00:04:34,180
And so that's the one we use to nonlinearly activate the neurons and the different layers.

60
00:04:34,180 --> 00:04:39,420
And by the way this new activation function is called the exponential in their unit.

61
00:04:39,490 --> 00:04:40,250
So there we go.

62
00:04:40,270 --> 00:04:44,230
We apply the elu on the first convolutional layer.

63
00:04:44,470 --> 00:04:46,300
And now things are going to be easy.

64
00:04:46,370 --> 00:04:52,270
We're going to proceed to the next for propagation of the signal which is from the first convolutional

65
00:04:52,270 --> 00:04:59,440
layer to the second convolutional layer which we are going to call X because basically we're just adding

66
00:04:59,470 --> 00:05:02,190
X now x is the first convolutional there.

67
00:05:02,290 --> 00:05:07,990
And by propagating the signal from the first convolutional layer to the next one X will become the next

68
00:05:07,990 --> 00:05:09,190
convolutional layer.

69
00:05:09,520 --> 00:05:15,130
And so to propagate the signal from the first convolutional layer to the second one we can't simply

70
00:05:15,220 --> 00:05:20,490
copy this and paste it here and replace one by two.

71
00:05:20,510 --> 00:05:27,340
And now of course the second convolution is not applied to the input images but to X that is the first

72
00:05:27,340 --> 00:05:29,530
convolutional layer which is right here.

73
00:05:29,830 --> 00:05:30,790
All right perfect.

74
00:05:30,790 --> 00:05:32,800
Now we get our second convolutional there.

75
00:05:33,010 --> 00:05:38,230
And now let's propagate the signal again from the second convolutional there to the third one.

76
00:05:38,230 --> 00:05:46,000
And therefore we can directly copy this and paste it here and replace two by three.

77
00:05:46,000 --> 00:05:46,840
There we go.

78
00:05:46,990 --> 00:05:47,900
And last one.

79
00:05:47,920 --> 00:05:53,770
Now to propagate the signal from the third convolutional there to the fourth one and last one we can

80
00:05:53,770 --> 00:05:58,770
just copy this again here and replace three by come four.

81
00:05:58,840 --> 00:05:59,790
There we go.

82
00:06:00,220 --> 00:06:01,870
So let's recap.

83
00:06:01,870 --> 00:06:03,310
We start with our inputs.

84
00:06:03,460 --> 00:06:07,170
We apply the first convolution to get the first convolutional there.

85
00:06:07,360 --> 00:06:12,130
Then we apply the second convolution to the first convolution layer to obtain the second convolutional

86
00:06:12,130 --> 00:06:12,850
layer.

87
00:06:12,970 --> 00:06:17,830
Then we apply this third convolution to the second convolutional there to obtain the third convolutional

88
00:06:17,830 --> 00:06:18,510
layer.

89
00:06:18,550 --> 00:06:24,650
And finally we apply the fourth convolution to the third convolutional there to obtain the fourth convolutional

90
00:06:24,650 --> 00:06:25,180
there.

91
00:06:25,510 --> 00:06:29,900
And that's how the signal is propagated throughout the eyes of the eye.

92
00:06:30,130 --> 00:06:35,620
So there we go we have now the output signal after the four convolutions and now we know what to do

93
00:06:35,620 --> 00:06:40,790
we need to expand this whole output signal in one dimensional vector.

94
00:06:40,900 --> 00:06:42,640
That's the flattening step.

95
00:06:42,880 --> 00:06:45,440
So we're going to update x again x.

96
00:06:45,490 --> 00:06:49,010
Now will become this flattened one dimensional vector.

97
00:06:49,230 --> 00:06:57,070
And to do this that's the same we need to take X which is so far the fourth convolutional layer X but

98
00:06:57,520 --> 00:07:05,660
then we use a view function and we first put minus one to say that we want one dimensional vector and

99
00:07:05,660 --> 00:07:11,810
then as a second argument we need to put the number of elements in the vector and that is remember three

100
00:07:11,880 --> 00:07:13,560
two times three times three.

101
00:07:13,670 --> 00:07:20,270
And therefore we can put here 32 times three times three.

102
00:07:20,270 --> 00:07:20,720
There we go.

103
00:07:20,720 --> 00:07:24,570
Now we have our flattened vector and the flattening step is done.

104
00:07:24,680 --> 00:07:25,580
Perfect.

105
00:07:25,580 --> 00:07:28,030
Now let's take care of the LCN part.

106
00:07:28,280 --> 00:07:33,030
So as you understood the LSD takes as input the flattened vector.

107
00:07:33,200 --> 00:07:37,190
This one dimensional vector of three two times three times three elements.

108
00:07:37,190 --> 00:07:40,310
So it's already ready and well-prepared for the icn.

109
00:07:40,320 --> 00:07:49,070
The team is now ready to take this Flaten vector as inputs and therefore we can take our LACMA and inputs

110
00:07:49,190 --> 00:07:52,010
as argument first X are ready.

111
00:07:52,010 --> 00:07:55,960
FLATOW And vector that is this x ray here that we just expanded.

112
00:07:56,270 --> 00:08:00,810
But also and that's where the trouble comes into play.

113
00:08:00,890 --> 00:08:04,540
We need to put H x and z x.

114
00:08:04,550 --> 00:08:10,820
And we can use ha-Satan see X here because we made that separations from the original input arguments

115
00:08:11,060 --> 00:08:12,980
of the forward function.

116
00:08:12,980 --> 00:08:20,460
So LACMA X the platens output vector after the four convolutions and all of the hidden in the cell note.

117
00:08:20,480 --> 00:08:21,390
So there we go.

118
00:08:21,440 --> 00:08:27,530
Then we must not forget the self because LSD is a variable or function so a variable attached to the

119
00:08:27,530 --> 00:08:35,210
object itself and a CM and this will actually return to outputs a total of two outputs which will be

120
00:08:35,210 --> 00:08:37,850
the output nodes and the output sounds.

121
00:08:37,880 --> 00:08:42,250
So it's actually topical and therefore we can update H.

122
00:08:42,260 --> 00:08:48,890
X y and z x the cell nodes because that's exactly the output of this NCM here.

123
00:08:50,030 --> 00:08:50,500
Great.

124
00:08:50,510 --> 00:08:52,750
So we're almost done now.

125
00:08:52,790 --> 00:08:58,850
Now that we have the outputs of the illustration we need to get the useful output because actually only

126
00:08:58,850 --> 00:09:05,570
the hidden notes are useful and therefore we going to get it by then X again and X will now be equal

127
00:09:05,570 --> 00:09:12,490
to atax the first element of the output sample of the US an X equals.

128
00:09:12,680 --> 00:09:14,270
And we're almost done.

129
00:09:14,270 --> 00:09:18,590
Remember that we have two brains one brain for the actor in one room for the critic.

130
00:09:18,860 --> 00:09:24,380
And therefore we have to output signals to return the output signal of the actor and the output signal

131
00:09:24,380 --> 00:09:25,480
of the critic.

132
00:09:25,580 --> 00:09:30,730
And therefore now what we're going to do is return these two output signals and how can we do that.

133
00:09:30,830 --> 00:09:32,010
Well that's very easy.

134
00:09:32,030 --> 00:09:38,690
We simply need to take our linear connections but separately that is a linear connection of the critic

135
00:09:38,770 --> 00:09:40,740
and that the full connection of the actor.

136
00:09:41,180 --> 00:09:47,870
And we applied each of these connections to the output X that is a useful output of the LACMA and that

137
00:09:47,870 --> 00:09:50,470
will be all that will be the output signal.

138
00:09:50,480 --> 00:09:50,990
So there we go.

139
00:09:50,990 --> 00:09:51,840
Let's do it.

140
00:09:51,900 --> 00:09:59,660
We first take self or object then we get the linear connection of the critic which is critic and this

141
00:09:59,690 --> 00:10:08,090
colinear which we apply to X the output signal the CM and then same we take self again then that and

142
00:10:08,090 --> 00:10:15,290
then we take the linear connection of the actor which is actor and it's colinear are which same we apply

143
00:10:15,290 --> 00:10:16,200
to x.

144
00:10:16,310 --> 00:10:19,580
There we go so that's the main thing we need.

145
00:10:19,710 --> 00:10:26,040
But then we're also going to return the top of Ajax to a node and see X to sell node because we will

146
00:10:26,040 --> 00:10:29,490
be using them later and the retro look of the LCN.

147
00:10:29,520 --> 00:10:30,430
All right perfect.

148
00:10:30,480 --> 00:10:32,960
So now we're done with the brain.

149
00:10:32,970 --> 00:10:38,040
Or should I say the brains because we actually made two brains one for the actor and one critic.

150
00:10:38,250 --> 00:10:41,990
So congratulations for making the eighth with the brains.

151
00:10:42,180 --> 00:10:47,940
I hope that wasn't too overwhelming to combine at CNN and in Anniston but at least the good news is

152
00:10:47,940 --> 00:10:51,190
that we're really working with the best and most powerful model.

153
00:10:51,300 --> 00:10:53,010
So there we go.

154
00:10:53,010 --> 00:10:55,950
We're actually done with this first family model that.

155
00:10:56,010 --> 00:10:56,810
Y.

156
00:10:56,880 --> 00:11:02,040
And so in the next two toile we'll take care of the optimizer because we're going to make a separate

157
00:11:02,190 --> 00:11:03,300
optimizer.

158
00:11:03,300 --> 00:11:08,850
We're not going to cut each line of code because a lot of it comes from the research papers and this

159
00:11:08,850 --> 00:11:10,520
is actually pretty specific.

160
00:11:10,530 --> 00:11:16,470
And if we go into the deep details of what's going on with this optimizer this might be a little too

161
00:11:16,470 --> 00:11:22,410
overwhelming for what's going to happen next because we still have the train function to make which

162
00:11:22,410 --> 00:11:26,890
will be a huge function and that contains the algorithm that we see.

163
00:11:27,030 --> 00:11:29,240
So trust me you want to keep some energy for that.

164
00:11:29,310 --> 00:11:32,300
And therefore we will not spend too much time on this.

165
00:11:32,490 --> 00:11:37,830
But still I will expand the code and you will understand the whole idea behind this optimizing.

166
00:11:38,190 --> 00:11:44,740
So congrats again for making this activity class and I'll see you in the next Statoil 290 optimizer.

167
00:11:44,760 --> 00:11:46,140
Until then enjoy I.
