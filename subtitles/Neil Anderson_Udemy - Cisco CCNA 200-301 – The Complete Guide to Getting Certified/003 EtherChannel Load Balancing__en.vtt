WEBVTT
1
00:00:06.360 --> 00:00:14.130
In this lecture, you'll learn about how etherchannel load balancing works, and I'm going to use the

2
00:00:14.130 --> 00:00:17.970
diagram that you see on the slide here throughout this lecture.

3
00:00:18.390 --> 00:00:24.990
So I've got two switches which have got four links between them that have been grouped into an etherchannel,

4
00:00:25.170 --> 00:00:31.140
and each of those four links is 1Gbps Ethernet interfaces. Just starting with gi0/1

5
00:00:31.140 --> 00:00:38.920
on the left going to gi0/4 on the right. In the bottom switch,

6
00:00:38.940 --> 00:00:41.700
I've got some PCs plugged in there.

7
00:00:41.700 --> 00:00:43.530
I've got PC-1 and PC-2.

8
00:00:43.770 --> 00:00:45.000
And then the top switch,

9
00:00:45.000 --> 00:00:49.110
I've got some servers. There's Server-1 and Server-2.

10
00:00:49.440 --> 00:00:57.780
So in this lecture, we're going to cover how etherchannel load balances the different flows that are going across

11
00:00:57.780 --> 00:00:59.520
the links between the switches.

12
00:00:59.730 --> 00:01:08.250
A flow is a communication from a client to a server using a particular application. If PC-1, in our example

13
00:01:08.250 --> 00:01:15.780
example, opens a web session to Server-1 and PC-2 opens an FTP session to Server-2, we'd have

14
00:01:15.780 --> 00:01:18.270
two flows going through our switches.

15
00:01:18.690 --> 00:01:26.220
And with etherchannel, a single flow is load balanced onto a single port channel interface.

16
00:01:26.580 --> 00:01:33.600
For example, all packets in the flow from PC-1 to Server-1, always go over interface gi0/1.

17
00:01:33.600 --> 00:01:41.340
All packets and flow from PC-2 to Server-2 always go over interface gi0/2.

18
00:01:42.390 --> 00:01:48.600
So, looking at that with an animation, the first packet and flow from PC-1 to Server-1,

19
00:01:49.020 --> 00:01:50.970
it hits the first switch.

20
00:01:51.150 --> 00:01:56.640
The switch decides which interface it's going to load balance it over.

21
00:01:56.820 --> 00:02:01.740
It chooses gi0/1 in our example and that goes to the server.

22
00:02:02.040 --> 00:02:06.690
The next packet in the flow will also go over to the same interface.

23
00:02:06.690 --> 00:02:12.780
So it comes in the switch, it load balances it to the same interface again and then it goes up to the server.

24
00:02:13.730 --> 00:02:21.830
On the second flow, from PC-2 to Server-2 that comes into the switch, the switch will use this algorithm

25
00:02:21.830 --> 00:02:27.520
to decide which interface to load balance it onto - gi0/2 in our example,

26
00:02:27.650 --> 00:02:33.020
and it goes to the server. When the second and third and fourth and so on packets come in from that flow,

27
00:02:33.020 --> 00:02:37.130
they will all be load balanced onto the same interface.

28
00:02:38.360 --> 00:02:44.570
Packets from the same flow are always load balanced on the same interface. They're not load balanced

29
00:02:44.570 --> 00:02:51.230
round-robin across all the interfaces in the port channel. For example, we don't load balance

30
00:02:51.230 --> 00:02:57.410
the first packet from PC-1 to Server-1 on interface gi0/1 and then the second packet

31
00:02:57.410 --> 00:03:00.620
on that same flow to gi0/2.

32
00:03:01.010 --> 00:03:07.990
The reason for that is that round-robin load balancing could cause packets to arrive out of order at

33
00:03:08.000 --> 00:03:11.390
the destination and that would break some applications.

34
00:03:11.390 --> 00:03:13.340
So it makes sure that doesn't happen.

35
00:03:13.520 --> 00:03:19.220
We always load balance packets from the same flow on to the same interface, so they're always going to arrive

36
00:03:19.220 --> 00:03:19.700
in order.

37
00:03:21.030 --> 00:03:26.670
So this does not happen, you see the first packet on the flow went over to interface gi0/1,

38
00:03:26.940 --> 00:03:31.190
the second packet in the same flow over gi0/2, we don't do that.

39
00:03:32.850 --> 00:03:39.240
So because the way this works, because a single flow always gets load balanced onto the same interface,

40
00:03:39.690 --> 00:03:46.320
any single flow receives a bandwidth of a single link in the port channel as its maximum.

41
00:03:46.770 --> 00:03:52.980
That's a maximum of 1Gbps bandwidth per flow in our example where we were using 1Gbps links

42
00:03:52.980 --> 00:03:54.090
between our switches.

43
00:03:54.450 --> 00:03:59.610
But there's an aggregate bandwidth of 4Gbps across all available flows.

44
00:04:00.540 --> 00:04:04.910
So you can think of a port channel as a multi-lane motorway.

45
00:04:05.250 --> 00:04:11.700
The cars always stay in their own lane and a single lane, but because there are multiple lanes, the

46
00:04:11.700 --> 00:04:14.130
overall traffic gets there quicker.

47
00:04:14.310 --> 00:04:20.220
Obviously an odd example, if we only had one uplink rather than four, we don't have so much overall

48
00:04:20.220 --> 00:04:23.190
bandwidth available between the switches. Etherchannel

49
00:04:23.190 --> 00:04:29.620
provides redundancy as well as load balancing. If a link fails, the flows will be load

50
00:04:29.630 --> 00:04:31.700
balanced to the remaining links.

51
00:04:31.920 --> 00:04:35.310
OK, so that's how load balancing and redundancy works.

52
00:04:35.660 --> 00:04:39.150
See you in the next lecture for more etherchannel.

