WEBVTT
1
00:00:00.900 --> 00:00:04.170
Hello, welcome back to the course on deep natural language processing.

2
00:00:04.200 --> 00:00:06.690
Today, we're looking at the bag of words model.

3
00:00:07.560 --> 00:00:12.480
First thing I'd like us to look at is an email, an email I received just a few days ago.

4
00:00:12.900 --> 00:00:13.680
So here we go.

5
00:00:13.890 --> 00:00:17.890
The e-mail is about to catch up and my friend is asking.

6
00:00:17.940 --> 00:00:18.580
Hello, Carol.

7
00:00:18.660 --> 00:00:21.630
Checking if you're back in Oz Oz sense for Australia.

8
00:00:22.020 --> 00:00:25.500
Let me know if you're around and keen to sync on how things are going.

9
00:00:25.740 --> 00:00:30.720
I defo, as it definitely could use some of your creative thinking to help with mine.

10
00:00:30.780 --> 00:00:31.180
Cheers.

11
00:00:32.310 --> 00:00:38.850
And so what I'd like us to pay attention to first of all, of course, you can see that I sent this

12
00:00:38.850 --> 00:00:43.660
email to myself, but that's just because I wanted to keep my friend.

13
00:00:45.990 --> 00:00:49.080
Actually, it is because I read your reply to the email and then I wanted to send it.

14
00:00:49.110 --> 00:00:52.680
And also I wanted to keep my friend, keep his privacy.

15
00:00:53.040 --> 00:00:54.170
But this is a real email.

16
00:00:54.240 --> 00:00:58.490
This is the exact text that I got literally a couple of days ago.

17
00:00:58.920 --> 00:01:01.520
And the title is different, but I just called.

18
00:01:01.640 --> 00:01:02.770
I changed to catch up.

19
00:01:03.330 --> 00:01:11.520
And so what is interesting about this, we're going to be looking at how we can apply natural language

20
00:01:11.520 --> 00:01:17.100
processing to this email in the next couple of tutorials, and it will help us work with a real life

21
00:01:17.100 --> 00:01:17.640
example.

22
00:01:18.120 --> 00:01:28.860
And then the other thing is that here you can see in Google, the G.M. app for iPhone, you can see

23
00:01:28.860 --> 00:01:31.350
that is giving me some suggestions.

24
00:01:31.800 --> 00:01:32.910
Very interesting.

25
00:01:32.910 --> 00:01:36.270
And saying I was really giving you some quick replies that I can use.

26
00:01:36.270 --> 00:01:39.980
It can be, yes, Emerald, I'm back or sorry, I'm not very interesting.

27
00:01:39.990 --> 00:01:43.890
So let's keep that in mind and we will come back to this later.

28
00:01:44.400 --> 00:01:46.520
In the meantime, text of the email is here.

29
00:01:46.530 --> 00:01:48.210
What can we do with it?

30
00:01:48.630 --> 00:01:48.900
All right.

31
00:01:48.930 --> 00:01:50.070
So first things.

32
00:01:50.130 --> 00:01:51.480
We're going to start off simple.

33
00:01:51.780 --> 00:01:53.700
We're going to create a model.

34
00:01:53.730 --> 00:01:58.280
We're going to look at how we can create a model that will give us an A yes.

35
00:01:58.320 --> 00:02:00.390
No response, because that's one of those questions.

36
00:02:01.140 --> 00:02:03.570
The question is, are you back in Australia?

37
00:02:03.600 --> 00:02:05.180
Let me know if you're around and keen to see.

38
00:02:05.270 --> 00:02:09.150
So, yes, no, of course, it's better to have a long response.

39
00:02:09.150 --> 00:02:12.120
And that's that's the social norm.

40
00:02:12.120 --> 00:02:18.060
And it's it's added the etiquette to like converse with people and just say yes, no.

41
00:02:18.330 --> 00:02:21.050
But even let's try to get a yes or no response.

42
00:02:21.060 --> 00:02:24.510
Let's see how would go about that, because that's a first step into an LP.

43
00:02:24.870 --> 00:02:28.680
And then further on, we will see how we can expand that even more.

44
00:02:29.610 --> 00:02:29.870
All right.

45
00:02:29.910 --> 00:02:40.230
So we're going to start off with with a vector, a vector or a just like an hour a a full of zeros,

46
00:02:40.500 --> 00:02:41.460
let's call it vectors.

47
00:02:41.530 --> 00:02:42.090
These like that.

48
00:02:42.360 --> 00:02:44.230
So just zero zero zero zero.

49
00:02:44.310 --> 00:02:45.180
How many zeros?

50
00:02:45.540 --> 00:02:47.370
Well, a lot of zeros.

51
00:02:47.550 --> 00:02:50.480
Twenty thousand elements in total.

52
00:02:50.490 --> 00:02:51.220
Twenty thousand.

53
00:02:51.240 --> 00:02:51.690
Why is that?

54
00:02:52.380 --> 00:02:55.620
Well, it's because of the way that we're building as well.

55
00:02:56.010 --> 00:03:04.140
Twenty thousand is the number of words that are commonly used by the average native English language

56
00:03:04.140 --> 00:03:04.560
speakers.

57
00:03:04.590 --> 00:03:07.350
So here's a quick search on Google.

58
00:03:07.380 --> 00:03:08.790
How many words in the English?

59
00:03:08.980 --> 00:03:10.360
So that's the search I took.

60
00:03:10.440 --> 00:03:13.710
I came up with how many words are there in the English language?

61
00:03:13.980 --> 00:03:16.530
Hundred seventy one thousand point seventy six words.

62
00:03:16.890 --> 00:03:22.770
That's how many entries in the Oxford Dictionary, plus some obsolete words plus derivative words and

63
00:03:22.770 --> 00:03:23.100
so on.

64
00:03:23.130 --> 00:03:31.980
But also people also you can see Google's giving a suggestion that more subtle adult native test takers

65
00:03:31.980 --> 00:03:37.410
range from 20 to 30, trained to thirty five thousand words, average native test takers of age eight

66
00:03:37.410 --> 00:03:44.940
or ten thousand words, average native test takers or, you know, 5000 words as an adult native test

67
00:03:45.150 --> 00:03:50.670
takers learn almost white, whatever this is going into detail.

68
00:03:51.480 --> 00:03:59.680
But the interesting thing here is that Hami, like what I wanted to point out, first of all, twenty

69
00:03:59.680 --> 00:04:00.020
thousand.

70
00:04:00.020 --> 00:04:02.220
And we will see why exactly is this number not more.

71
00:04:03.400 --> 00:04:08.040
What I wanted to point out is how many words are there in the English language?

72
00:04:08.130 --> 00:04:12.290
Even this in its own is actually Google is applying natural language processing.

73
00:04:12.360 --> 00:04:14.850
It's it's looking at what we wrote.

74
00:04:14.900 --> 00:04:19.020
And then it's also checking other similar answers.

75
00:04:19.050 --> 00:04:22.950
How many boards in the English language does as other person, the average person?

76
00:04:23.010 --> 00:04:23.470
No.

77
00:04:23.520 --> 00:04:24.570
So that's not the question I ask.

78
00:04:24.600 --> 00:04:25.500
But it came up with that.

79
00:04:25.800 --> 00:04:27.510
Then it came up of many other questions.

80
00:04:27.540 --> 00:04:33.210
So you can see that the irony is that even in this search on its own, we're ready.

81
00:04:33.600 --> 00:04:36.240
Falling victim of natural language processing.

82
00:04:37.050 --> 00:04:38.400
Even though that wasn't our intention.

83
00:04:38.430 --> 00:04:39.870
That's not what we're gonna be talking about.

84
00:04:40.140 --> 00:04:42.450
But it's just funny that it came up anyway.

85
00:04:42.510 --> 00:04:43.690
So 20000 words.

86
00:04:43.740 --> 00:04:54.030
And fun fact is that we actually use about 3000 words out of those one hundred seventy one thousand

87
00:04:54.030 --> 00:04:55.100
four hundred seventy six words.

88
00:04:55.110 --> 00:04:59.850
We only use 3000 words, not just in conversational language.

89
00:05:00.050 --> 00:05:06.650
But you can see here a vocabulary of just 3000 words provides coverage for around 95 percent of common

90
00:05:06.650 --> 00:05:07.130
texts.

91
00:05:07.760 --> 00:05:13.430
Ninety five percent of common text that I got, assuming that's including books and stuff like that.

92
00:05:13.820 --> 00:05:15.870
So if you do the math, it's one.

93
00:05:16.070 --> 00:05:21.470
Only use one point seventy five percent of the total number of words in the English language.

94
00:05:21.800 --> 00:05:28.490
So as you can see, even that three, like our twenty thousand, is more than even the three thousand

95
00:05:28.490 --> 00:05:31.580
that covers 95 percent of the situation.

96
00:05:31.590 --> 00:05:33.140
So we're pretty good.

97
00:05:33.170 --> 00:05:37.800
We're definitely covered if we say that our vocabulary.

98
00:05:38.930 --> 00:05:43.730
All possible words that we can encounter is going to fit into a vector of 20.

99
00:05:43.880 --> 00:05:46.910
So every basically what we're saying, this is important.

100
00:05:47.270 --> 00:05:52.690
What we're saying is that every word in the English language has a position somewhere on the spectrum.

101
00:05:52.730 --> 00:05:55.850
So, for example, this the word if could have this position.

102
00:05:55.880 --> 00:06:01.830
So if you count one, two, three, four, five, six, the seventh position in our custom made a vector

103
00:06:02.900 --> 00:06:04.070
is that word.

104
00:06:04.100 --> 00:06:07.670
If it's always gonna be on that position, that's very crucial for this.

105
00:06:08.000 --> 00:06:09.460
For instance, the word badminton.

106
00:06:09.470 --> 00:06:12.830
Let's just say like that we can construct this vector any way we want.

107
00:06:13.220 --> 00:06:15.020
The word badminton could be on this position.

108
00:06:15.020 --> 00:06:18.440
There's always gonna be on this position and the word table is gonna be on this position.

109
00:06:18.560 --> 00:06:21.360
And this is like how this bag of words model works.

110
00:06:21.380 --> 00:06:28.460
So just keep in mind that once you like was we've taken over twenty thousand words and then we've assigned

111
00:06:28.460 --> 00:06:29.060
them a space.

112
00:06:29.060 --> 00:06:34.650
That's where they that's what they will this base in this vector will be associated with.

113
00:06:34.660 --> 00:06:35.900
It will be associated with the word we have.

114
00:06:36.590 --> 00:06:38.340
This will be associate with badminton.

115
00:06:38.450 --> 00:06:41.030
This will this position will be associated to a table.

116
00:06:42.830 --> 00:06:47.300
And the other thing is here you can see a great out the first two in the last one.

117
00:06:47.420 --> 00:06:50.970
First two are going to be reserved for Source and AOS.

118
00:06:51.200 --> 00:06:54.320
So stance for start of sentence, us stands for.

119
00:06:54.410 --> 00:06:55.340
End of sentence.

120
00:06:55.860 --> 00:06:58.430
And the last one will be reserved for special words.

121
00:06:58.780 --> 00:07:01.220
And that's for those words that you're wondering about.

122
00:07:01.220 --> 00:07:01.490
Second.

123
00:07:01.700 --> 00:07:04.240
I can hear your brain churning right now.

124
00:07:04.550 --> 00:07:09.440
What about those other one hundred and fifty thousand words that we didn't take into account?

125
00:07:09.470 --> 00:07:10.310
What if they come up?

126
00:07:10.580 --> 00:07:15.860
Well, if they come up, we're gonna just associate them with this with this lost thing.

127
00:07:15.920 --> 00:07:18.080
This last element where you just throw them all in there.

128
00:07:18.080 --> 00:07:20.720
Any kind of words that we kind of recognized in the twenty thousand.

129
00:07:21.260 --> 00:07:24.110
Let's throw them into that last element.

130
00:07:25.190 --> 00:07:25.460
All right.

131
00:07:25.490 --> 00:07:27.380
So let's go back to our e-mail text.

132
00:07:27.680 --> 00:07:28.100
Here it is.

133
00:07:28.130 --> 00:07:28.800
Hello, Carol.

134
00:07:28.880 --> 00:07:30.230
Checking if you're back in Oz.

135
00:07:30.230 --> 00:07:33.050
Let me know if you are around, et cetera, et cetera, et cetera.

136
00:07:33.350 --> 00:07:33.800
Cheers.

137
00:07:36.080 --> 00:07:41.840
And so let's see how this can be put into our bag of words.

138
00:07:41.870 --> 00:07:46.580
If you've probably noticed by now that this is our bag of words that we're constructing here.

139
00:07:46.610 --> 00:07:50.510
So now we're going to throw the text into this bag of words.

140
00:07:50.950 --> 00:07:51.710
How is that going to happen?

141
00:07:51.740 --> 00:07:54.930
I'm just going to throw it in and then I'll just I'll explain how it happened.

142
00:07:54.980 --> 00:07:56.090
So there it is.

143
00:07:56.120 --> 00:07:56.900
That's the result.

144
00:07:58.650 --> 00:08:02.170
That's all, of course, depends on how we construct our victory.

145
00:08:02.200 --> 00:08:05.140
But this is our result in the way we construct our victory.

146
00:08:05.290 --> 00:08:07.560
Let's let's look at this way.

147
00:08:07.600 --> 00:08:13.630
So we've as we've discussed previously, we took the 20000 thousand words and we associate each position

148
00:08:13.630 --> 00:08:14.740
with a word.

149
00:08:15.400 --> 00:08:23.560
And now we go through our text and find and then like increase the counter in each position of the associate

150
00:08:23.560 --> 00:08:23.710
word.

151
00:08:23.740 --> 00:08:24.400
So hello.

152
00:08:24.790 --> 00:08:30.520
Let's say in our viqueira, it is in position number five because we only have one.

153
00:08:30.520 --> 00:08:33.060
Hello in this whole email.

154
00:08:33.160 --> 00:08:34.600
We're going to put a one here.

155
00:08:35.080 --> 00:08:40.570
Carol is definitely not an English language work, so we're gonna have to put it into there.

156
00:08:40.930 --> 00:08:48.640
And the reason why there is three here is because we have Carol, then OR's and these those are non

157
00:08:48.640 --> 00:08:51.160
English language words, not among twenty thousand.

158
00:08:51.310 --> 00:08:52.480
They're all gonna go here.

159
00:08:53.800 --> 00:08:56.440
Then we've got the comma surprise.

160
00:08:56.440 --> 00:08:57.990
The comma also has a position.

161
00:08:58.000 --> 00:09:02.320
Let's say it was in position number said three six, seven, eight, nine.

162
00:09:02.380 --> 00:09:06.740
So the ninth position is associated with a comma because we have one comma in our e-mail.

163
00:09:06.850 --> 00:09:08.680
Oh, actually, we have two commas, OK.

164
00:09:08.770 --> 00:09:09.550
So this should be a two.

165
00:09:09.730 --> 00:09:11.350
But let's there's something about that comma.

166
00:09:11.380 --> 00:09:13.210
Let's let's forget about that comma.

167
00:09:14.100 --> 00:09:14.840
I didn't notice it.

168
00:09:14.980 --> 00:09:19.390
So assuming we have one comment or email, this is a one checking.

169
00:09:19.960 --> 00:09:24.610
So let's say that this this element is associated with the word checking.

170
00:09:24.620 --> 00:09:31.390
This is a one because it's only one word checking if it's a two, because we have two ifs in our email.

171
00:09:31.720 --> 00:09:38.260
So it's going to be it to you two because we have to use in our email, including the rest of the text.

172
00:09:38.350 --> 00:09:39.670
I don't think there's any more use in there.

173
00:09:40.000 --> 00:09:40.440
And so on.

174
00:09:40.480 --> 00:09:45.370
So that's basically how we fill this bag of words we just put in.

175
00:09:45.430 --> 00:09:50.300
The the quantity of words for every position is pretty straightforward.

176
00:09:50.320 --> 00:09:52.690
We're just filling in this vector.

177
00:09:52.810 --> 00:09:55.060
As you can see, it's gonna be quite a spa specter.

178
00:09:55.060 --> 00:09:59.930
There's gonna be lots of zeros, almost twenty thousand zeros, and some of the words are gonna be filled

179
00:09:59.930 --> 00:10:00.070
in.

180
00:10:01.950 --> 00:10:02.160
OK.

181
00:10:02.320 --> 00:10:03.280
So what is our goal?

182
00:10:03.710 --> 00:10:10.450
So our goal, as we discussed before, is to come up with a reply yes or no to this email, which is

183
00:10:10.450 --> 00:10:12.430
now in the form of a vector.

184
00:10:13.060 --> 00:10:14.290
And how are we going to do that?

185
00:10:14.380 --> 00:10:16.390
Well, we're going to do it through training data.

186
00:10:16.480 --> 00:10:22.150
So we're going to look at all the e-mails that I have replied to, because this is us training a model

187
00:10:22.360 --> 00:10:28.780
to reply to my e-mails or in your case, in anybody's case, is going to be training a model to reply

188
00:10:28.780 --> 00:10:29.490
to their emails.

189
00:10:29.860 --> 00:10:30.900
We got to look at training data.

190
00:10:30.910 --> 00:10:31.980
We're going to need some trained it.

191
00:10:32.020 --> 00:10:34.630
I'm going to fish it out of the inbox or outbox.

192
00:10:35.710 --> 00:10:37.480
So let's say let's look at a couple.

193
00:10:37.540 --> 00:10:38.320
So here we've got.

194
00:10:38.620 --> 00:10:39.230
Hey, mate.

195
00:10:39.280 --> 00:10:42.400
Have you read about Hinton's capsule networks?

196
00:10:42.460 --> 00:10:43.930
And General replied to that.

197
00:10:43.960 --> 00:10:44.350
No.

198
00:10:45.100 --> 00:10:47.110
So we're going to use that as a training example.

199
00:10:47.770 --> 00:10:48.180
Next one.

200
00:10:48.190 --> 00:10:50.050
Did you like that recipe essential last week?

201
00:10:50.760 --> 00:10:52.120
The answer was yes.

202
00:10:52.830 --> 00:10:54.070
It was a good recipe, I guess.

203
00:10:54.100 --> 00:10:55.350
So there we go.

204
00:10:55.390 --> 00:10:57.430
So now we have two, three.

205
00:10:57.760 --> 00:10:58.390
Hi, Carol.

206
00:10:58.390 --> 00:10:59.560
Are you coming to dinner tonight?

207
00:10:59.770 --> 00:11:01.040
Yes, dear.

208
00:11:01.100 --> 00:11:03.280
Carol, would you like to search your car with us again?

209
00:11:03.760 --> 00:11:04.150
No.

210
00:11:04.780 --> 00:11:06.730
Are you coming to Australia in December?

211
00:11:06.970 --> 00:11:07.450
Yes.

212
00:11:07.780 --> 00:11:08.350
And so on.

213
00:11:08.500 --> 00:11:14.600
So ideally, we would have tens or hundreds of thousands of e-mails like that and responses like that.

214
00:11:14.650 --> 00:11:14.830
Yes.

215
00:11:14.860 --> 00:11:15.550
No responses.

216
00:11:17.140 --> 00:11:21.520
Of course, would be like a lot of groundwork to get that data because we usually don't just respond.

217
00:11:21.550 --> 00:11:21.730
Yes.

218
00:11:21.760 --> 00:11:21.910
No.

219
00:11:21.910 --> 00:11:22.340
Two emails.

220
00:11:22.340 --> 00:11:27.680
So we'd have to look at this answer and understand what was the sentiment.

221
00:11:27.730 --> 00:11:28.850
The sentiment was no.

222
00:11:28.890 --> 00:11:30.670
What was the overall?

223
00:11:30.970 --> 00:11:31.910
Was it a yes or no?

224
00:11:31.930 --> 00:11:32.320
No.

225
00:11:32.350 --> 00:11:32.800
Yes or no?

226
00:11:32.800 --> 00:11:33.080
It's OK.

227
00:11:34.360 --> 00:11:36.760
Of course, it is kind of more of a theoretical example.

228
00:11:36.940 --> 00:11:38.830
Nobody is going to do this for their own inbox.

229
00:11:38.860 --> 00:11:40.570
But nevertheless, the point stands.

230
00:11:40.990 --> 00:11:42.370
So how would we train?

231
00:11:42.490 --> 00:11:43.840
How would we use the stream data?

232
00:11:44.560 --> 00:11:49.650
We would use a similar principle and convert each one of those emails to a vector in this.

233
00:11:49.690 --> 00:11:53.920
And again, each vector would be 20000 elements long.

234
00:11:53.980 --> 00:11:59.470
So, you know, I just threw some numbers in here, too, to get the point across.

235
00:11:59.710 --> 00:12:05.080
It's not exactly accurate, but so we have these vectors like lots of lots and lots of pictures, lots

236
00:12:05.080 --> 00:12:06.340
and lots and lots of responses.

237
00:12:06.370 --> 00:12:06.970
Yes and no.

238
00:12:08.020 --> 00:12:13.150
And yeah, so now what we're going to do is we're going to.

239
00:12:15.770 --> 00:12:16.600
Apply and model.

240
00:12:16.870 --> 00:12:18.590
Once we have all this data, we're going to apply model.

241
00:12:18.630 --> 00:12:25.270
So one of the models we can apply to create our backup words or one of the algorithms we can apply to

242
00:12:25.270 --> 00:12:28.420
create a backboards model is the logistic regression.

243
00:12:28.900 --> 00:12:31.600
So we apply the logistic regression to our yes.

244
00:12:31.630 --> 00:12:35.170
No responses to these to this information that we have.

245
00:12:36.670 --> 00:12:38.050
And then.

246
00:12:39.860 --> 00:12:45.770
Once we have that model, once we've separated, so we know we kind of like we've modeled what goes

247
00:12:46.460 --> 00:12:53.400
what goes into a yes, like what's what is likely to yield a yes, what is likely to yield know the

248
00:12:55.070 --> 00:12:56.180
border between them.

249
00:12:56.450 --> 00:13:05.420
Then we can feed our actual email that we got into this bottle and then get a response service, for

250
00:13:05.420 --> 00:13:05.750
instance.

251
00:13:05.780 --> 00:13:06.080
Yes.

252
00:13:06.110 --> 00:13:06.710
And that's it.

253
00:13:06.770 --> 00:13:09.230
So we use all the training data to create a model.

254
00:13:09.290 --> 00:13:16.090
We feed in our actual e-mail, which this is important, which has exactly the same format.

255
00:13:16.100 --> 00:13:22.320
So you can see that every input here, every every time we would train the data.

256
00:13:23.360 --> 00:13:30.050
The independent variable, the independent variable vector always had the same length.

257
00:13:30.080 --> 00:13:31.980
Twenty thousand and always had the same format.

258
00:13:32.030 --> 00:13:35.480
We know that this position always corresponds to a certain word.

259
00:13:35.990 --> 00:13:37.640
This position is always a certain word.

260
00:13:39.090 --> 00:13:42.850
This position, let's say, one, two, three, which I wish, where was it, one, two, three, four,

261
00:13:42.850 --> 00:13:45.960
five, six, seven, eight, nine.

262
00:13:46.000 --> 00:13:47.010
So this was what?

263
00:13:47.220 --> 00:13:47.460
No.

264
00:13:47.530 --> 00:13:48.430
This one is the if.

265
00:13:48.430 --> 00:13:48.700
Right.

266
00:13:48.730 --> 00:13:51.100
So this corresponds to it or something.

267
00:13:51.280 --> 00:13:53.320
So we know that it's it's the same format.

268
00:13:53.320 --> 00:13:54.380
It's always the same length.

269
00:13:54.440 --> 00:13:55.030
Twenty thousand.

270
00:13:55.060 --> 00:13:58.030
So we can safely feed in this vector into there.

271
00:13:58.390 --> 00:13:59.920
It's got the same number of features.

272
00:14:00.700 --> 00:14:01.450
We get an answer.

273
00:14:01.660 --> 00:14:02.720
So for instance, we get.

274
00:14:02.740 --> 00:14:03.070
Yes.

275
00:14:03.140 --> 00:14:03.500
So.

276
00:14:03.710 --> 00:14:04.950
And then we can like look back.

277
00:14:05.000 --> 00:14:05.140
Oh.

278
00:14:05.140 --> 00:14:06.440
What did actual e-mail say.

279
00:14:06.460 --> 00:14:06.870
It said hello.

280
00:14:06.960 --> 00:14:07.420
Carol check.

281
00:14:07.700 --> 00:14:08.110
OK.

282
00:14:08.170 --> 00:14:14.110
So based on my training, I would have most likely reply to this with a yes.

283
00:14:14.560 --> 00:14:15.070
Interesting.

284
00:14:15.520 --> 00:14:20.280
The other approach we can take here, first of all, let's put this on our diagram is our diagram.

285
00:14:20.650 --> 00:14:26.170
And that's a natural language processing algorithm, which is called back reports sits over there.

286
00:14:27.010 --> 00:14:32.680
The other approach we could apply here or here is we could instead of a logistic regression, we could

287
00:14:32.770 --> 00:14:35.500
use a neural network.

288
00:14:35.890 --> 00:14:37.600
We could because we have a vector.

289
00:14:37.810 --> 00:14:37.950
Right.

290
00:14:38.020 --> 00:14:39.430
So we have all these vectors.

291
00:14:39.850 --> 00:14:46.780
We could feed them into as an input layer, like over twenty thousand neurons into how our neural network.

292
00:14:46.870 --> 00:14:53.020
They would go through one hidden layer to analyze Azmy Hitler's as we want our own decision on how to

293
00:14:53.020 --> 00:14:53.620
structure it.

294
00:14:53.650 --> 00:14:55.810
And then bam, we've got an output layer.

295
00:14:55.840 --> 00:14:56.830
And tells us yes or no.

296
00:14:56.950 --> 00:15:02.020
And so we again, would use all this data that we have here, all our millions and millions and millions

297
00:15:02.020 --> 00:15:04.000
of emails and responses.

298
00:15:04.520 --> 00:15:06.890
We'd use that to train on your own network.

299
00:15:07.090 --> 00:15:12.760
All of all, through back propagation and stochastic gradient descent, all the weights would be updated

300
00:15:12.760 --> 00:15:14.440
and bam, we have an answer.

301
00:15:14.650 --> 00:15:15.340
So not bad.

302
00:15:15.340 --> 00:15:15.850
We have an answer.

303
00:15:15.940 --> 00:15:18.590
So we would use these answers here to train that.

304
00:15:18.610 --> 00:15:20.710
I would use the pairs like the vector.

305
00:15:20.740 --> 00:15:22.050
And the answer vector answer.

306
00:15:22.120 --> 00:15:27.400
So to minimize the Aristarchus a gradient descent back propagation, updated weights, bam, we have

307
00:15:27.400 --> 00:15:28.140
a neural network.

308
00:15:28.150 --> 00:15:28.990
It's all trained up.

309
00:15:29.470 --> 00:15:35.350
Now we feed in our vector here, which represents our new e-mail into the neural network.

310
00:15:35.440 --> 00:15:43.090
And voila, we get our answer, as in this case might also be, yes, they might yield different cells,

311
00:15:43.120 --> 00:15:50.320
but if the models are constructed well, more or less, it should be you coming up with similar or the

312
00:15:50.320 --> 00:15:53.360
same answers most of the time as in this case.

313
00:15:53.380 --> 00:15:55.960
We've got a deep natural language processing.

314
00:15:56.970 --> 00:15:58.120
Then put the emphasis right there.

315
00:15:58.300 --> 00:16:00.880
We've got a deep natural language processing algorithm.

316
00:16:01.210 --> 00:16:01.400
Right.

317
00:16:01.450 --> 00:16:03.250
Because we're using a neural network.

318
00:16:03.670 --> 00:16:05.550
And that is different.

319
00:16:05.570 --> 00:16:11.430
So in both cases, the bag of words model in one case is in an LP bag go Gabor's, in other cases a

320
00:16:11.440 --> 00:16:14.050
deep and ILP bag of words.

321
00:16:15.160 --> 00:16:19.600
But in both cases, it is still a bag of words and it has its own limitations.

322
00:16:19.720 --> 00:16:26.140
And it has its own limitations and issues that are not that great.

323
00:16:26.200 --> 00:16:29.790
And so I'll point out one of them right now is that that response is very simple.

324
00:16:29.800 --> 00:16:31.000
It's just a yes or no.

325
00:16:31.680 --> 00:16:31.930
Right.

326
00:16:31.960 --> 00:16:33.990
Like, we want something more sophisticated.

327
00:16:34.000 --> 00:16:37.780
We want like a conversation can really have a conversation, can really build a chadbourne if you're

328
00:16:37.780 --> 00:16:39.340
just going to be saying yes no all the time.

329
00:16:39.730 --> 00:16:41.050
So that's one of the limitations.

330
00:16:41.080 --> 00:16:45.100
We'll talk about some more of them in upcoming tutorial.

331
00:16:45.160 --> 00:16:51.820
And we'll also see how to overcome those limitations and what models await us in the future.

332
00:16:52.270 --> 00:16:53.890
And I hope you enjoyed this, Taurel.

333
00:16:53.890 --> 00:16:56.530
I really enjoyed going through all this with you together.

334
00:16:56.950 --> 00:16:59.920
And I can't wait to see it again until then.

335
00:17:00.150 --> 00:17:02.740
Enjoy it in natural language processing.
