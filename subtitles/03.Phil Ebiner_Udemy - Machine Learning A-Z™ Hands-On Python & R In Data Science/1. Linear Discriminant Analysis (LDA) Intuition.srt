1
00:00:00,750 --> 00:00:06,620
Hi everyone and welcome to the intuition lecture linear discriminant analysis or LDI.

2
00:00:06,930 --> 00:00:12,830
Now for those of you who are coming from the previous section on principle component analysis or PCA

3
00:00:13,170 --> 00:00:15,000
this may seem a bit similar.

4
00:00:15,240 --> 00:00:19,680
But there is a difference between the two and we're getting it started by taking a look at overall what

5
00:00:19,680 --> 00:00:20,810
LDA entails.

6
00:00:20,970 --> 00:00:26,490
It's a pretty brief and straightforward intuition lecture but we'll get to the main takeaways between

7
00:00:26,490 --> 00:00:33,360
PCa and LDI LDA is commonly used as a dimensionality reduction technique and we've heard that before

8
00:00:34,020 --> 00:00:36,070
with PCa.

9
00:00:36,120 --> 00:00:42,410
It's used in the pre-processing step for pattern classification and machine learning algorithms.

10
00:00:42,810 --> 00:00:46,880
And its goal is to project a data set onto a lower dimensional space.

11
00:00:48,140 --> 00:00:56,780
Sounds similar to PCA but LDA differs because in addition to finding the component axes with LDK we

12
00:00:57,020 --> 00:01:02,870
are interested in the axes that maximize the separation between multiple classes.

13
00:01:02,990 --> 00:01:08,960
And that is the main takeaway or the main point is where PCA we are with that distinction and working

14
00:01:08,960 --> 00:01:14,280
with the principle component analysis with the axes the principal components within the data.

15
00:01:14,330 --> 00:01:21,910
But where as we're looking in LDA we are looking for the separation of those classes within the data

16
00:01:23,480 --> 00:01:25,170
and to break it down further.

17
00:01:25,280 --> 00:01:33,680
The goal of LDA is to project a feature space onto a small subspace while maintaining the class discriminatory

18
00:01:33,680 --> 00:01:34,560
information.

19
00:01:35,000 --> 00:01:43,200
And we have both PCa and LDH as linear transformation techniques used for dimensionality reduction PCa

20
00:01:43,530 --> 00:01:52,080
is a unsupervised algorithm but LVH is supervised because of the relation to the dependent variable.

21
00:01:52,250 --> 00:01:57,140
And we can see here from this visualization the main operations and main differences between PCa and

22
00:01:57,140 --> 00:01:58,640
L.D. at PCA.

23
00:01:58,640 --> 00:02:04,370
Again we're looking in that the subspace and the dimension reduction technique of the data to examine

24
00:02:04,370 --> 00:02:07,200
how the principle KOHONA axes are in relation.

25
00:02:07,280 --> 00:02:12,020
Whereas in LDI we're looking for that class separation and I think this visualization kind of makes

26
00:02:12,020 --> 00:02:14,040
it the most clear between the two.

27
00:02:14,060 --> 00:02:17,680
If you want some additional information you can always take a look at the following link.

28
00:02:17,870 --> 00:02:24,740
But here we have the PCA and LDA and the main operations related to each again.

29
00:02:25,000 --> 00:02:28,860
LDI supervised because of the relation to the dependent variable.

30
00:02:28,860 --> 00:02:33,390
And I think when you start working through this in the upcoming lecture and the hands on part it's going

31
00:02:33,390 --> 00:02:37,230
to make more sense but that's the main takeaway to focus on LDA.

32
00:02:37,770 --> 00:02:41,160
And you can accomplish this by five main steps.

33
00:02:41,160 --> 00:02:43,260
Similar again to PCA.

34
00:02:43,470 --> 00:02:47,280
The five main steps for LDI include the following.

35
00:02:47,370 --> 00:02:54,780
The computation of the D dimensional mean vectors the computation of the scatter matrices you have to

36
00:02:54,780 --> 00:03:03,620
also compute the eigenvectors sort the eigenvectors by decreasing eigenvalues and use the D times K

37
00:03:03,740 --> 00:03:09,370
eigenvector matrix to transform the samples onto the new subspace overall.

38
00:03:09,480 --> 00:03:10,950
Very similar to PCA.

39
00:03:11,070 --> 00:03:16,890
Two different types of dimensional reduction techniques one being unsupervised and one being supervised.

40
00:03:16,890 --> 00:03:21,600
But the main distinction with LDA to take away is that we're looking for that class separation within

41
00:03:21,600 --> 00:03:22,630
the data.

42
00:03:22,770 --> 00:03:26,670
Overall if you're coming from PCA this should seem familiar for the majority of operations if you're

43
00:03:26,670 --> 00:03:29,850
new to this I advise you to go take a look at PCA as well.

44
00:03:30,180 --> 00:03:33,920
But when you are starting work through the next comp coming park it should make more sense.

45
00:03:33,930 --> 00:03:40,110
But just keep in mind that the main takeaway for LDA is that class separation and it is a supervised

46
00:03:40,440 --> 00:03:41,230
learning technique.

47
00:03:42,870 --> 00:03:47,290
If you have any questions as always please feel free to share them and enjoy machine learning.
