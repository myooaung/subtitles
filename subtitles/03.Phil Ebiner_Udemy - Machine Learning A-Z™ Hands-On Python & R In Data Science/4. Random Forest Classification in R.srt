1

00:00:00,180  -->  00:00:05,700
Hello and welcome to our tutorial we are finally getting to the final round of our classification adventure

2

00:00:05,700  -->  00:00:11,080
in our machine learning journey because today we're making the very last classifier the random forest

3

00:00:11,100  -->  00:00:12,000
classifier.

4

00:00:12,270  -->  00:00:17,640
So this classifiers basically the daddy of the previous class for you the decision tree classifier because

5

00:00:17,730  -->  00:00:22,520
very simply a random forest classifier is a team of decision trees.

6

00:00:22,650  -->  00:00:27,840
So you will see that we will pick a number of trees to compose our Random forest and then when it is

7

00:00:27,840  -->  00:00:33,720
time to make a prediction then each of the trees of the random forest will each one make a prediction

8

00:00:33,900  -->  00:00:39,150
whether or not they think that the user will buy yes or no the SUV and then the random forest will look

9

00:00:39,150  -->  00:00:44,610
at each of the votes of its trees and then we'll count the number of votes for each class that has the

10

00:00:44,610  -->  00:00:50,460
number of votes for Yes to use about the SUV and the number of votes for no they use or didn't buy the

11

00:00:50,460  -->  00:00:56,160
SUV and simply based on a majority vote run them for us classify you will choose a class that received

12

00:00:56,430  -->  00:01:02,300
the majority vote that has the most votes from its trees to make it as its ultimate prediction.

13

00:01:02,310  -->  00:01:04,480
So that's the idea about run for us.

14

00:01:04,650  -->  00:01:09,390
And therefore if you understood correctly decision trees then you would have no problem to understand

15

00:01:09,490  -->  00:01:10,570
running for us.

16

00:01:10,590  -->  00:01:10,890
OK.

17

00:01:10,890  -->  00:01:15,590
So let's make our very last chance for you right now and let's find out about the results.

18

00:01:15,690  -->  00:01:20,370
So let's start with the basics We'll get to our machinery as you folder to set the rightful the rest

19

00:01:20,370  -->  00:01:24,520
working directory so this rifle there is and boy 3 classification.

20

00:01:24,630  -->  00:01:27,260
And here it is section when you're running for classification.

21

00:01:27,270  -->  00:01:28,810
That's the right folder.

22

00:01:29,040  -->  00:01:34,800
And as I was telling you we are reaching the end of part three classification because the next section

23

00:01:34,800  -->  00:01:40,670
will be dedicated to evaluating classification models performance in order to improve the models.

24

00:01:40,800  -->  00:01:42,830
And therefore this is our last gasifier.

25

00:01:43,020  -->  00:01:44,250
So let's go to this folder.

26

00:01:44,370  -->  00:01:48,060
That's the folder we want to set as a working directory make sure that you have the social network ads

27

00:01:48,140  -->  00:01:49,170
as you file.

28

00:01:49,230  -->  00:01:53,900
And if that's the case click on this more button here and then set as a working directory.

29

00:01:54,180  -->  00:01:59,970
All good here and now let's go to our classification templates to build around them for us classifier

30

00:02:00,240  -->  00:02:01,770
with the best efficiency.

31

00:02:01,770  -->  00:02:10,710
So we'll just take everything from here to the bottom copy and then will paste it here.

32

00:02:11,010  -->  00:02:14,630
All right so I'll just give a quick reminder about what this template is doing.

33

00:02:14,640  -->  00:02:16,790
So the first step is to import the data set.

34

00:02:16,860  -->  00:02:22,830
We are putting the social network ads as we file that contains some data about users of a social network

35

00:02:23,220  -->  00:02:26,750
and their information like their age and their estimated salary.

36

00:02:26,910  -->  00:02:32,760
So this car company has just launched its brand new SUV at a ridiculous price puts ads on the social

37

00:02:32,760  -->  00:02:37,060
network and the last column of this social network ads as he felt tells.

38

00:02:37,100  -->  00:02:38,210
If yes or no.

39

00:02:38,220  -->  00:02:41,120
Some users of the social network bought the SUV.

40

00:02:41,190  -->  00:02:47,970
So zero if no one if yes then we encode here the target features factor because we sometimes need to

41

00:02:47,970  -->  00:02:50,340
do it for the class Faiers that we build.

42

00:02:50,360  -->  00:02:55,740
We are just doing this to specify to declassify that our last call them purchased is a categorical variable

43

00:02:55,740  -->  00:02:58,380
with factor levels 0 and 1.

44

00:02:58,590  -->  00:03:03,570
So we don't need to do it for all the classifiers but we do need to do it for sun like the Navy base

45

00:03:03,570  -->  00:03:04,860
class very as we saw.

46

00:03:05,040  -->  00:03:09,840
So it's good to keep this code section in the template then we split the data set into the training

47

00:03:09,840  -->  00:03:11,220
set and the test set.

48

00:03:11,220  -->  00:03:16,780
So as a reminder the training set is where our machine learning model that is are running for US disfiguration

49

00:03:16,830  -->  00:03:20,920
model will learn the correlations between the age and the estimated salary.

50

00:03:21,120  -->  00:03:25,890
And the question whether the user will buy yes or no the SUV and then based on these informations the

51

00:03:25,890  -->  00:03:28,990
machine learning model learns how to make some future predictions.

52

00:03:29,220  -->  00:03:34,810
And so we build this test set here to test the performance of our model in terms of predictions.

53

00:03:34,950  -->  00:03:39,570
And then in the last section of the pre-processing phase we apply fut. getting to the data and we are

54

00:03:39,570  -->  00:03:45,360
only doing this because at the end of this template we make this very cool graph for both the trends

55

00:03:45,360  -->  00:03:50,300
that result in the test results that will plot the prediction regions and the prediction boundary.

56

00:03:50,520  -->  00:03:54,820
OK so right now we need to change a few things in this template but some very few things.

57

00:03:54,930  -->  00:04:02,310
First let's not forget to change the title of the plot here will replace classify it by random forest

58

00:04:02,930  -->  00:04:04,500
classification.

59

00:04:04,890  -->  00:04:08,790
We'll copy this because we'll do the same in the test results.

60

00:04:08,790  -->  00:04:09,570
Here it is.

61

00:04:09,630  -->  00:04:10,560
Perfect.

62

00:04:11,000  -->  00:04:11,500
OK.

63

00:04:11,490  -->  00:04:17,700
And now what we only need to do is to create our classifier here and then we'll be ready.

64

00:04:17,700  -->  00:04:20,120
So I'll just paste the same thing here.

65

00:04:20,150  -->  00:04:22,340
Run them for us gasification to the trainset.

66

00:04:22,620  -->  00:04:25,270
And now let's create our classifier.

67

00:04:25,290  -->  00:04:26,520
All right let's do this.

68

00:04:26,520  -->  00:04:27,960
So it's very simple.

69

00:04:27,960  -->  00:04:31,470
We're going to use a library that is called random forest.

70

00:04:31,620  -->  00:04:37,260
So let's do this let's first install the package with the command install that package.

71

00:04:37,590  -->  00:04:43,410
So that's for those of you who are using this package for the first time then you won't have it installed

72

00:04:43,500  -->  00:04:45,040
on your back just is here.

73

00:04:45,420  -->  00:04:49,330
As you can see mine should be already here because I used it several times.

74

00:04:49,360  -->  00:04:51,490
There it is running forest.

75

00:04:51,630  -->  00:04:55,900
So if it's not the case for you then you need to install the package here.

76

00:04:56,070  -->  00:05:02,430
And so in this install the packages function you just need to input the name of the library in quotes

77

00:05:02,440  -->  00:05:02,560
.

78

00:05:02,760  -->  00:05:09,670
So it's random Forest be careful with the capital F here and not capital are here.

79

00:05:09,670  -->  00:05:13,540
All right so that's what you need to impart Anyway that will install the package.

80

00:05:13,560  -->  00:05:18,610
I won't do it because I already have mine installed with what you just need to do is to select this

81

00:05:18,810  -->  00:05:21,340
and press command and control plus enter to execute.

82

00:05:21,340  -->  00:05:24,570
And this will install the package without any issues.

83

00:05:24,570  -->  00:05:29,550
All right so here I'll just press command per shift Plessey to make it as a comment.

84

00:05:29,560  -->  00:05:36,330
But then what I need to do is to add the command here library and then input the name of the Ranum for

85

00:05:36,330  -->  00:05:39,190
us library because as you can see here it's not selected.

86

00:05:39,310  -->  00:05:45,040
So I need to add this library around and for us command to select it automatically especially if I want

87

00:05:45,040  -->  00:05:48,160
to make some automated scripts in the future.

88

00:05:48,340  -->  00:05:50,970
So that's the thing to do that's very practical.

89

00:05:51,220  -->  00:05:54,390
And now we're all good let's create our classifier.

90

00:05:54,690  -->  00:06:00,200
OK so as usual we'll start by creating the Voivode classifier that will be around them for us class

91

00:06:00,210  -->  00:06:01,300
for itself.

92

00:06:01,300  -->  00:06:05,350
And now we will use the random forest function to build our classifier.

93

00:06:05,350  -->  00:06:11,110
So here I'll just take the function random forest and now let's see what arguments we need to input

94

00:06:11,110  -->  00:06:11,410
.

95

00:06:11,410  -->  00:06:17,860
So we'll just go here and press one to get some info about these functions or here we need to click

96

00:06:18,100  -->  00:06:23,940
here and here are some info about the random forest library and function.

97

00:06:23,940  -->  00:06:27,150
So let's look at the arguments which are what we're interested in.

98

00:06:27,510  -->  00:06:27,780
OK.

99

00:06:27,790  -->  00:06:32,270
So the first argument is data we will need it as you can see it's a national data frame.

100

00:06:32,410  -->  00:06:36,640
So it's not an argument that we need to build around him for his classifier.

101

00:06:36,680  -->  00:06:37,550
Same for subset.

102

00:06:37,560  -->  00:06:42,210
And they do action we actually don't need the three first parameters to build around for us.

103

00:06:42,220  -->  00:06:49,980
However we of course need the x and y arguments here which as you can guess will be the matrix of features

104

00:06:50,130  -->  00:06:52,340
and the dependent variable vector.

105

00:06:52,440  -->  00:06:56,770
Indeed as you can see X is a data frame or a matrix of predictors.

106

00:06:56,800  -->  00:06:58,000
So that's pretty clear.

107

00:06:58,030  -->  00:07:04,270
X is our matrix of features a matrix of predictors which are our independent variables age an estimated

108

00:07:04,260  -->  00:07:05,070
salary.

109

00:07:05,290  -->  00:07:07,930
And then why is that to be a response vector.

110

00:07:07,940  -->  00:07:12,950
So of course why is the dependent variable vector that is the purchased column.

111

00:07:13,400  -->  00:07:14,350
Okay perfect.

112

00:07:14,350  -->  00:07:19,690
And then the last argument that will need to build around for us classifier is of course the number

113

00:07:19,690  -->  00:07:26,350
of trees we want to have in the forest and this number of trees is given by this entry argument here

114

00:07:26,710  -->  00:07:30,110
which as you can see is the number of trees to grow.

115

00:07:30,340  -->  00:07:36,040
So to grow is a nice way of telling that the trees will learn from dataset how to make the predictions

116

00:07:36,050  -->  00:07:36,240
.

117

00:07:36,490  -->  00:07:39,400
And basically this argument is the number of trees.

118

00:07:39,460  -->  00:07:42,390
So remember in the pattern to toile we chose sentry's.

119

00:07:42,460  -->  00:07:43,880
Well let's do the same here.

120

00:07:43,890  -->  00:07:44,590
Let's pick.

121

00:07:44,620  -->  00:07:45,940
Entry equals 10.

122

00:07:46,300  -->  00:07:52,620
And that is actually everything we need to build around him for us classify you only X the matrix of

123

00:07:52,690  -->  00:07:55,620
featurism matrix of independent variables y.

124

00:07:55,750  -->  00:08:00,720
The response vector of the dependent variable vector and entry the number of troops.

125

00:08:00,730  -->  00:08:02,930
OK so let's do this let's end with the arguments.

126

00:08:03,120  -->  00:08:04,270
Let's go back to our.

127

00:08:04,480  -->  00:08:05,250
And here we are.

128

00:08:05,380  -->  00:08:05,650
OK.

129

00:08:05,640  -->  00:08:10,840
So let's end with the arguments as you remember the first argument was the matrix A features the matrix

130

00:08:10,840  -->  00:08:12,220
of independent variables.

131

00:08:12,390  -->  00:08:20,320
And that is trainset excluding the last column which has indexed 3 because as you remember in the training

132

00:08:20,320  -->  00:08:26,590
set we have the first two columns which are independent variables age an estimated salary with therefore

133

00:08:26,760  -->  00:08:28,200
indexes 1 and 2.

134

00:08:28,360  -->  00:08:33,660
And we have the third column indexed by three which is are dependent variable vector purchased.

135

00:08:33,730  -->  00:08:35,150
So here minus three.

136

00:08:35,320  -->  00:08:39,720
Then what was the next argument the next argument was why the dependent variable vector.

137

00:08:39,930  -->  00:08:46,820
And then here will take trainset and let's pick it this way to specify the name of the in the bin invariable

138

00:08:46,840  -->  00:08:49,930
$2 here and purchased.

139

00:08:49,920  -->  00:08:53,250
Purchased is the name of our dependent variable column.

140

00:08:53,250  -->  00:08:55,620
All right so we almost have everything we need.

141

00:08:55,620  -->  00:09:02,990
The last thing we need now is of course the number of trees and that is entry equals 10.

142

00:09:03,000  -->  00:09:08,080
You can play around with the entry arguments you can choose many more trees in the forests you'll observe

143

00:09:08,080  -->  00:09:09,300
some interesting results.

144

00:09:09,350  -->  00:09:15,840
That's interesting to see what different teams of trees can do to predict the response of your users

145

00:09:15,930  -->  00:09:19,160
in the social network whether they buy yes or no the SUV.

146

00:09:19,390  -->  00:09:24,900
But if you do this make sure to pay attention to overfitting which you want to avoid.

147

00:09:24,900  -->  00:09:27,550
You don't want to overfit run them for his class.

148

00:09:27,560  -->  00:09:28,770
Very true the training said.

149

00:09:28,890  -->  00:09:33,490
Because if you do this then it might make some poor predictions on a new set.

150

00:09:33,490  -->  00:09:39,660
You can actually check it out with the test but he will choose 10 trees and we'll see what happens.

151

00:09:39,660  -->  00:09:40,020
All right.

152

00:09:40,010  -->  00:09:42,510
So actually we're done with the templates.

153

00:09:42,510  -->  00:09:49,000
We changed everything we had to change and now we can just you know select everything and execute to

154

00:09:49,330  -->  00:09:50,550
make everything ready.

155

00:09:50,560  -->  00:09:56,280
You can actually take some coffee or tea and you can just select everything and execute to watch the

156

00:09:56,290  -->  00:09:57,110
results.

157

00:09:57,410  -->  00:10:04,060
But let's rather do it step by step we'll just do the first preprocessing step all in once here so I

158

00:10:04,060  -->  00:10:09,730
just selected the preprocessing phase and now I'll press command control press enter to execute all

159

00:10:09,730  -->  00:10:14,890
right all good we have our data set our training set and our test set.

160

00:10:15,000  -->  00:10:21,990
So everything looks fine we have 400 observations in total 300 observations that went into the trainset

161

00:10:22,280  -->  00:10:29,020
and 100 observations I went into the test set as you can see the training set and the test set are scaled

162

00:10:29,380  -->  00:10:33,960
because in the end we were plotting some graphic results with a resolution of open 0 1.

163

00:10:33,970  -->  00:10:40,540
So in order for our code to execute faster and not actually break our code we need to apply features

164

00:10:40,550  -->  00:10:43,250
just getting our training set and our test set.

165

00:10:43,360  -->  00:10:48,780
Otherwise we wouldn't need to do that because the running for US classification is not based on Euclidean

166

00:10:48,780  -->  00:10:53,550
distances but it's based on you know conditions on the independent variables.

167

00:10:53,740  -->  00:10:59,880
But because of this code here that is compute intensive we need to apply feature scaling so that everything

168

00:10:59,880  -->  00:11:01,100
is well executed.

169

00:11:01,440  -->  00:11:04,290
All right so let's do this let's watch the results.

170

00:11:04,290  -->  00:11:08,810
We just need to create our classifier here by executing the section.

171

00:11:08,820  -->  00:11:11,980
So I'll just do this all right.

172

00:11:12,000  -->  00:11:18,220
Again now let's predict the test results then we have the confusion matrix which will tell us in a flashlight

173

00:11:18,340  -->  00:11:20,270
how many incorrect predictions we have.

174

00:11:20,400  -->  00:11:22,370
So let's actually do it directly.

175

00:11:22,440  -->  00:11:27,360
It will be faster to see how our Ranum for us classify it well on the predictions.

176

00:11:27,450  -->  00:11:29,160
So let's execute this.

177

00:11:29,640  -->  00:11:35,500
And now let's enter CME here in the council press enter.

178

00:11:35,860  -->  00:11:38,130
And here we have our confusion matrix.

179

00:11:38,250  -->  00:11:42,620
OK we have seven plus 10 equals 17 incorrect predictions.

180

00:11:42,960  -->  00:11:44,210
Well that's not too bad.

181

00:11:44,250  -->  00:11:52,560
Just for fun let's let's just pick another number of trees like for example let's pick 500 trees 500

182

00:11:52,560  -->  00:11:53,240
trees is a lot.

183

00:11:53,250  -->  00:11:57,220
That's a really big army of trees to make some predictions.

184

00:11:57,450  -->  00:11:59,940
And now just for fun let's take this.

185

00:11:59,980  -->  00:12:05,020
I don't need to do this because my library was already selected from the previous execution of this

186

00:12:05,010  -->  00:12:05,760
code section here.

187

00:12:05,760  -->  00:12:10,110
So let's rebuild a new classifier with 500 trees.

188

00:12:10,130  -->  00:12:13,210
Now let's look at the confusion matrix.

189

00:12:13,370  -->  00:12:18,630
But before let's build our vector of prediction because right now the Y protector of prediction is the

190

00:12:18,630  -->  00:12:21,130
one given by the run and for us with 10 trees.

191

00:12:21,450  -->  00:12:23,120
So let's really execute this.

192

00:12:23,130  -->  00:12:30,120
Right now we have y spread as the vector of predictions predicted by the Ranum for us with 500 trees

193

00:12:30,120  -->  00:12:30,660
.

194

00:12:30,660  -->  00:12:32,620
And now let's look at the matrix of predictions.

195

00:12:32,620  -->  00:12:36,320
Remember with Gentry's we had 17 incorrect predictions.

196

00:12:36,340  -->  00:12:37,390
And now let's see.

197

00:12:37,470  -->  00:12:41,130
Select executes C and enter.

198

00:12:41,160  -->  00:12:44,470
And now we have 15 incorrect predictions.

199

00:12:44,490  -->  00:12:45,000
Great.

200

00:12:45,000  -->  00:12:48,880
We invested 490 more trees to win to correct predictions.

201

00:12:49,090  -->  00:12:53,420
So definitely that means that there are a lot of useful trees in the team.

202

00:12:53,610  -->  00:13:01,930
OK so as you want let's maybe go back to centuries here because obviously 500 trees is not very useful

203

00:13:01,920  -->  00:13:02,660
.

204

00:13:02,670  -->  00:13:05,700
All right so I'll just take that again.

205

00:13:05,930  -->  00:13:06,860
That as well.

206

00:13:06,880  -->  00:13:09,370
And that as well.

207

00:13:09,370  -->  00:13:10,090
All right.

208

00:13:10,260  -->  00:13:12,610
And now let's look at the training search results.

209

00:13:12,630  -->  00:13:17,520
So everything is fine here we changed the title here with random first classification.

210

00:13:17,520  -->  00:13:18,810
So it's all good.

211

00:13:18,810  -->  00:13:21,520
We are ready to look at the graphic result.

212

00:13:21,690  -->  00:13:26,350
And by the way you can puzzle the video now and try to guess what you about to see.

213

00:13:26,520  -->  00:13:30,810
Try to guess the shape of the prediction Regence and of the prediction boundary.

214

00:13:30,810  -->  00:13:35,550
If you understood correctly the decision trees and you would have no problem guessing what's about to

215

00:13:35,550  -->  00:13:36,390
happen.

216

00:13:36,390  -->  00:13:39,240
All right so I'm going to execute right now.

217

00:13:39,250  -->  00:13:47,090
Commander control presented to execute and Showtime.

218

00:13:47,110  -->  00:13:47,720
All right.

219

00:13:47,740  -->  00:13:49,700
Wow that's quite something here.

220

00:13:49,950  -->  00:13:50,220
OK.

221

00:13:50,220  -->  00:13:53,700
So first of all give a very short and quick reminder of what this is about.

222

00:13:53,700  -->  00:13:57,100
So the points here are the real observation points.

223

00:13:57,120  -->  00:14:03,250
There is the real users of the social network the red points are the users who didn't buy the SUV and

224

00:14:03,250  -->  00:14:05,790
the green points other users who bought the SUV.

225

00:14:06,100  -->  00:14:10,080
And then we have the regions here the red region and the green region which are the prediction regions

226

00:14:10,100  -->  00:14:10,280
.

227

00:14:10,440  -->  00:14:15,660
So the red region is the region where I run them for us classify predicts that the U.S doesn't buy the

228

00:14:15,660  -->  00:14:20,690
SUV and the green region where around for us Kaspar predicts that the user buys the SUV.

229

00:14:20,860  -->  00:14:25,380
OK so in short the points are the truth and the regions are the predictions.

230

00:14:25,530  -->  00:14:26,250
And now let's see.

231

00:14:26,250  -->  00:14:26,530
OK.

232

00:14:26,550  -->  00:14:32,950
So the Ranum for us classifiers definitely catches most of the users that didn't buy the SUV in the

233

00:14:32,940  -->  00:14:33,640
right category.

234

00:14:33,630  -->  00:14:34,500
That is the red region.

235

00:14:34,500  -->  00:14:35,860
So that means that it classified.

236

00:14:35,860  -->  00:14:40,770
Well most of the users who didn't buy the SUV and same for the green users who are other users.

237

00:14:40,780  -->  00:14:48,210
But yes we in reality because as we can see most of them are in the right green region and it's desperately

238

00:14:48,210  -->  00:14:51,450
trying to catch some outliers.

239

00:14:51,450  -->  00:14:52,550
We can call them this way.

240

00:14:52,550  -->  00:14:58,500
For example this guy here is a user that didn't buy the SUV in reality because this is a red point and

241

00:14:58,500  -->  00:15:03,780
it is way into the green region here and we can see but the random first class fare managed to make

242

00:15:03,780  -->  00:15:08,820
this little rectangle part of the region in red to catch this.

243

00:15:08,830  -->  00:15:11,770
Use it I didn't buy the SUV and classify it well.

244

00:15:12,030  -->  00:15:13,620
But is it the smart way of doing it.

245

00:15:13,620  -->  00:15:19,290
Because what will tell you that for some new observations we'll have you know some users who didn't

246

00:15:19,290  -->  00:15:22,350
buy the SUV in this red rectangle here.

247

00:15:22,380  -->  00:15:27,330
So that looks like overfilling because it made this red rectangle here because we had this user indeed

248

00:15:27,340  -->  00:15:33,460
who didn't buy the SUV but nothing tells us that for some new observations we will have some users who

249

00:15:33,450  -->  00:15:36,520
didn't buy the SUV in this red rectangle here.

250

00:15:36,720  -->  00:15:38,680
So we should be careful with that.

251

00:15:38,830  -->  00:15:44,320
And same for this user here as you can see at this user isn't some sort of irregular red region here

252

00:15:44,620  -->  00:15:50,450
but fortunately around for us classify was not too obsessed at making all the predictions correct.

253

00:15:50,460  -->  00:15:53,890
Because as we can see this red user here is in the green region.

254

00:15:53,970  -->  00:15:57,670
So that means that it's still paid attention to or fitting but not too much.

255

00:15:57,660  -->  00:15:59,440
And we should be careful with that.

256

00:15:59,430  -->  00:16:05,140
So speaking of overfishing Let's check that right now let's look at the test results right now to see

257

00:16:05,400  -->  00:16:08,820
how this region here because you know the regions won't change.

258

00:16:08,830  -->  00:16:11,100
These are the regions built by our model.

259

00:16:11,110  -->  00:16:16,980
So when we look at the test results we will have the same red region here with this rectangle here and

260

00:16:16,980  -->  00:16:18,180
green region here.

261

00:16:18,420  -->  00:16:22,890
But what will change will be the test set observation point that is all the red points and the green

262

00:16:22,890  -->  00:16:23,230
points.

263

00:16:23,230  -->  00:16:28,920
This will change and we will see if we have some red points here in this rectangle here and actually

264

00:16:28,920  -->  00:16:34,710
probably not because this looks like overfitting that occurred because our class was fitted too much

265

00:16:34,840  -->  00:16:37,680
to the trainset So let's find out about that right now.

266

00:16:37,680  -->  00:16:43,560
Let's select this section dedicated to visualize the test results.

267

00:16:43,600  -->  00:16:53,230
So I'll just select everything and press command control press enter to execute.

268

00:16:53,220  -->  00:16:53,580
All right.

269

00:16:53,590  -->  00:16:55,910
So what is the first thing you see here.

270

00:16:56,130  -->  00:17:02,690
Well yes indeed this red rectangle here is totally useful for some observations.

271

00:17:02,700  -->  00:17:09,040
So that was clearly a red rectangle region to catch some uses of the trainset because our class was

272

00:17:09,030  -->  00:17:14,970
too much fitted to the trainset and this red rectangle actually doesn't make any sense here because

273

00:17:14,970  -->  00:17:18,830
indeed we don't have any red user in this rectangle region here.

274

00:17:18,850  -->  00:17:23,130
Well it's it's not that it doesn't make any sense but it's totally unuseful here.

275

00:17:23,490  -->  00:17:28,890
And besides you know we have this green point here and this green point could have been in this region

276

00:17:28,890  -->  00:17:31,030
here and it would make an incorrect prediction.

277

00:17:31,030  -->  00:17:35,850
We were lucky on this one that this could have happened because these are new observations and they

278

00:17:35,860  -->  00:17:41,190
are random for us classification machine learning model didn't learn anything from this new observation

279

00:17:41,190  -->  00:17:41,610
point.

280

00:17:41,730  -->  00:17:46,870
So this guy could totally have ended up here so lucky on this one.

281

00:17:46,920  -->  00:17:50,740
And by the way same for this region here we don't have any red user.

282

00:17:50,760  -->  00:17:53,380
That is some user who didn't buy the SUV in this red region.

283

00:17:53,380  -->  00:17:56,400
So this red region is totally useful as well.

284

00:17:56,880  -->  00:18:01,780
OK so that's the idea but most of all it did a pretty good job because of course got most of the red

285

00:18:01,780  -->  00:18:07,200
users here with a low age and the low estimate salary and therefore users who didn't buy it yesterday

286

00:18:07,190  -->  00:18:07,360
.

287

00:18:07,650  -->  00:18:13,500
And most of the green users who are quite all that with a higher estimated salary who bought this awesome

288

00:18:13,680  -->  00:18:16,280
cheap luxury as you.

289

00:18:16,420  -->  00:18:18,090
And now what is the conclusion of all this.

290

00:18:18,100  -->  00:18:21,580
Because we reached the end of our classification adventure.

291

00:18:21,610  -->  00:18:27,780
We built all our classifiers So according to you what is the best gasifier for this particular business

292

00:18:27,780  -->  00:18:28,450
problem.

293

00:18:28,440  -->  00:18:29,770
What is the best one.

294

00:18:29,880  -->  00:18:32,520
It should be a classified that classified correctly.

295

00:18:32,530  -->  00:18:39,420
The users who didn't buy the SUV and the users who bought the SUV and at the same time prevented overfilling

296

00:18:39,660  -->  00:18:44,730
in the training set to be able to make some good new predictions of some new observations.

297

00:18:44,740  -->  00:18:51,450
So in my opinion the best class would be the kernel as VM in terms of the balance between the percentage

298

00:18:51,510  -->  00:18:55,210
of incorrect predictions and the fact that we want to prevent overfitting.

299

00:18:55,440  -->  00:19:00,940
Well if we look at them again in my opinion the kernel as being classified would be the best one.

300

00:19:00,930  -->  00:19:02,990
All right so that's the end of this tutorial.

301

00:19:03,010  -->  00:19:09,840
And now I have to say congratulations because you built a great deal of classifiers from simple classifiers

302

00:19:09,850  -->  00:19:17,220
was logistic regression to more sophisticated and more complex classifiers like kernel SVM or Ranum

303

00:19:17,240  -->  00:19:18,700
forest classifiers.

304

00:19:18,970  -->  00:19:24,220
But that's not the end of the journey in the next section we will be talking about how to evaluate the

305

00:19:24,220  -->  00:19:28,110
performance of our models and how we can improve them.

306

00:19:28,240  -->  00:19:33,850
And then eventually we will have a homework on a real life data set where we will combine what we learned

307

00:19:33,850  -->  00:19:38,790
here about how to build some classifiers and all the next concept that we will learn to evaluate the

308

00:19:38,800  -->  00:19:44,160
model of performance in order to find the best model for this real life business problem data set that

309

00:19:44,160  -->  00:19:49,540
you will be given and we will do the job as a data scientist or a machine or any scientists would do

310

00:19:49,620  -->  00:19:50,500
in reality.

311

00:19:50,670  -->  00:19:52,240
So congratulations again.

312

00:19:52,240  -->  00:19:54,210
I look forward to seeing you in the next section.

313

00:19:54,240  -->  00:19:56,180
And until then enjoy machine learning.
