WEBVTT
1
00:00:00.450 --> 00:00:01.390
Hello my friends.

2
00:00:01.410 --> 00:00:03.060
I hope you are feeling fantastic now.

3
00:00:03.180 --> 00:00:08.070
After this first date of depressing phase that we tackled in a flashlight together.

4
00:00:08.190 --> 00:00:13.650
Now we are ready for the exciting step which is to train the multiple in our regression or on the training

5
00:00:13.650 --> 00:00:16.970
set that we now collected thanks to the previous step.

6
00:00:17.040 --> 00:00:21.860
But first before we do this I would like to answer two important questions.

7
00:00:21.900 --> 00:00:29.760
The first question is do we have to avoid you know to do something to avoid the dummy variable trap.

8
00:00:29.760 --> 00:00:37.380
And the answer is no because indeed the multiple in our regression class that we are about to import

9
00:00:37.470 --> 00:00:43.080
and which will build the multiple in our regression model itself and actually also train it because

10
00:00:43.080 --> 00:00:48.870
remember that a class actually can complete several actions including building it and training it and

11
00:00:48.870 --> 00:00:55.920
well this class will automatically avoid this trap which means that indeed you know one of the three

12
00:00:55.920 --> 00:01:01.770
first columns here because remember Carol explained that one is a redundant well will automatically

13
00:01:01.860 --> 00:01:02.580
be outcast.

14
00:01:02.610 --> 00:01:06.090
So you have nothing to worry about regarding the dummy variable trap.

15
00:01:06.090 --> 00:01:10.370
You don't have to remove one of the columns here and that's the beauty of classes.

16
00:01:10.440 --> 00:01:16.320
You know these are advanced implementations that allow you to build a moral and machinery model in just

17
00:01:16.320 --> 00:01:20.670
a few lines of code and this will take care of the dummy variable trap for you.

18
00:01:20.670 --> 00:01:28.710
Now second question Do we have to work on the features to select the best ones with you know the techniques

19
00:01:28.710 --> 00:01:32.880
that Carol introduced to you like for example backward elimination.

20
00:01:32.880 --> 00:01:38.100
Do we have to deploy the backward elimination technique in order to select the features that have the

21
00:01:38.100 --> 00:01:42.380
highest p values and and there are the most artistically significant.

22
00:01:42.600 --> 00:01:45.330
And the answer is once again no.

23
00:01:45.390 --> 00:01:46.140
Why is that.

24
00:01:46.260 --> 00:01:52.980
Well for the exact same reason as the dummy variable trap the class that we're about to call to build

25
00:01:53.100 --> 00:02:00.120
are multiple in our regression model will automatically identify the best features of the features that

26
00:02:00.120 --> 00:02:05.940
have the highest p values or that are the most statistically significant to figure out how to predict

27
00:02:06.060 --> 00:02:07.130
the dependent variable.

28
00:02:07.130 --> 00:02:10.060
You know the profit with the highest accuracy.

29
00:02:10.080 --> 00:02:12.820
So once again you don't have to worry about this.

30
00:02:12.940 --> 00:02:18.810
The class of psychic learn you know this amazing Data Science Library will take care of everything for

31
00:02:18.810 --> 00:02:18.990
you.

32
00:02:19.210 --> 00:02:19.770
Okay.

33
00:02:19.920 --> 00:02:22.270
So I'm glad I answered these two questions.

34
00:02:22.380 --> 00:02:27.390
You know the purpose of building machinery most today is to be efficient because in your career you

35
00:02:27.390 --> 00:02:32.970
know I can't tell you how many times I had to test several machinery models on my data sets and select

36
00:02:32.970 --> 00:02:33.740
the best one.

37
00:02:33.750 --> 00:02:38.820
Well you know if I had to deploy the backward elimination technique on my dataset Well I would have

38
00:02:38.820 --> 00:02:40.010
lost a lot of time.

39
00:02:40.020 --> 00:02:42.540
And here we have a class that takes care of everything.

40
00:02:42.570 --> 00:02:47.970
You just have to deploy your class on your data set and then you will get an accuracy and you will compare

41
00:02:47.970 --> 00:02:51.030
that accuracy with the accuracy of the other models.

42
00:02:51.090 --> 00:02:53.580
And you know that process is called model selection.

43
00:02:53.580 --> 00:02:56.940
That's what we actually cover and the parts and of this course.

44
00:02:56.940 --> 00:03:01.430
So really I want you to be efficient with you know your implementations and your machine learning tool

45
00:03:01.430 --> 00:03:05.550
kit so that you can optimize your model selection process.

46
00:03:05.580 --> 00:03:06.090
All right.

47
00:03:06.090 --> 00:03:12.630
So now that we said this Well let's build together the multiple in our regression model and now actually

48
00:03:12.630 --> 00:03:19.110
I have some good news because you know we're still doing linear regression and in the previous section

49
00:03:19.110 --> 00:03:24.390
we actually did simple in our regression with which we therefore had one feature in our dataset and

50
00:03:24.390 --> 00:03:28.800
now we're doing multiple in our regression which is exactly the same as simple in a Russian except that

51
00:03:28.800 --> 00:03:30.690
we have several features.

52
00:03:30.690 --> 00:03:36.690
So the good news is actually that the class that we're about to use to build and train this multiple

53
00:03:36.690 --> 00:03:41.700
in our regression model is actually the exact same class as for the simple in our regression model it

54
00:03:41.700 --> 00:03:45.810
will just recognize that there are several features and therefore that we are doing multiple in our

55
00:03:45.810 --> 00:03:48.600
regression but the rest will be exactly the same.

56
00:03:48.600 --> 00:03:53.550
It will be trained to understand the correlations between all your features actually I can open it here

57
00:03:53.910 --> 00:03:58.890
all your features and the profit which has really been invaluable.

58
00:03:58.890 --> 00:04:03.600
And then it will take care of the dummy variable trap and it will also take care of selecting the best

59
00:04:03.660 --> 00:04:06.660
features are the most statistically significant.

60
00:04:07.200 --> 00:04:12.570
So that's the good news but still I just want to implement this again from scratch to make sure it is

61
00:04:12.570 --> 00:04:14.760
best integrated in your head.

62
00:04:14.760 --> 00:04:19.260
All right so let's close this and let's implement are multiple in our regression model.

63
00:04:19.510 --> 00:04:23.850
Actually tried to do it faster than me try to do it before me you know you can press both on this video

64
00:04:23.850 --> 00:04:27.870
and do it and me I'm gonna do it pretty efficiently.

65
00:04:27.870 --> 00:04:34.440
So remember we have to start from the SDK learn the cycle learn library from which we're going gonna

66
00:04:34.440 --> 00:04:38.690
get access to this specific module which is linear model.

67
00:04:38.700 --> 00:04:39.330
There we go.

68
00:04:39.360 --> 00:04:41.730
Google cool app guesses it perfectly.

69
00:04:41.730 --> 00:04:50.310
And from this module we're going to import Well the linear regression class Google collab guess is again

70
00:04:50.310 --> 00:04:51.570
perfectly all right.

71
00:04:51.570 --> 00:04:54.150
So that's the exact same class as in the produce section.

72
00:04:54.180 --> 00:04:56.490
Simple in our regression and there you go.

73
00:04:56.490 --> 00:05:01.120
We're gonna do just exactly the same you know this is exactly the same as what we did in the previous

74
00:05:01.120 --> 00:05:06.550
section because now we're going to create a new variable which will be are multiple in our regression

75
00:05:06.550 --> 00:05:11.180
model which will be created as an object of this linear regression class.

76
00:05:11.200 --> 00:05:16.030
So let's introduce this new variable regress our equals.

77
00:05:16.030 --> 00:05:22.700
Well since this will be an object of the linear regression class I'm just copying this class and pasting

78
00:05:22.700 --> 00:05:25.350
in here and adding some parenthesis.

79
00:05:25.390 --> 00:05:30.790
All right so that regressive is created as an instance of this linear regression class.

80
00:05:30.790 --> 00:05:34.080
Now the question is do we have to enter any parameters here.

81
00:05:34.090 --> 00:05:37.850
Well just like simple in our regression the answer is no.

82
00:05:37.900 --> 00:05:43.330
We're just gonna keep the default values of the parameters inside this linear regression class and there

83
00:05:43.330 --> 00:05:48.280
is absolutely no need to understand them you can check them out in the documentation if you want but

84
00:05:48.610 --> 00:05:54.500
usually when we're building a multiple in our regression mural there is not much parameter to tune.

85
00:05:54.520 --> 00:05:57.520
I will explain in a whole section parameter tuning.

86
00:05:57.520 --> 00:06:01.150
You know when you can improve your model but for linear regression it's pretty simple.

87
00:06:01.150 --> 00:06:03.640
So usually we don't have anything to input here.

88
00:06:03.670 --> 00:06:04.390
So all good.

89
00:06:04.780 --> 00:06:10.060
And now final step you know with this line of code we actually built the multiple in our regression

90
00:06:10.060 --> 00:06:15.460
model so we had them all already on only right now it is done you know it is not trained yet on the

91
00:06:15.460 --> 00:06:16.200
data set.

92
00:06:16.210 --> 00:06:22.550
So now we're gonna make it smart by training it's indeed on our training set composed of remember extra

93
00:06:22.610 --> 00:06:23.760
and NY train.

94
00:06:23.770 --> 00:06:24.310
So there we go.

95
00:06:24.310 --> 00:06:33.780
Let's take our regress or object from which we're going to call the fit method which takes us input.

96
00:06:33.780 --> 00:06:34.140
Right.

97
00:06:34.140 --> 00:06:35.720
Exactly the same as before.

98
00:06:35.730 --> 00:06:39.740
First the matrix of feature is X train.

99
00:06:39.960 --> 00:06:45.390
And second the dependent variable vector Y train of the training set of course.

100
00:06:45.390 --> 00:06:46.230
All right.

101
00:06:46.230 --> 00:06:47.030
And that's it.

102
00:06:47.040 --> 00:06:50.440
This line of code will build our multiple in the regression model.

103
00:06:50.490 --> 00:06:54.420
And this line of code will train it on our training set.

104
00:06:54.450 --> 00:06:54.830
All right.

105
00:06:54.840 --> 00:06:55.260
Perfect.

106
00:06:55.260 --> 00:06:57.310
So now let's just execute the sale.

107
00:06:57.300 --> 00:07:03.120
So let me check the sales that we didn't execute before so we executed everything up to here.

108
00:07:03.120 --> 00:07:07.260
So we still have to execute this sell you know to split.

109
00:07:07.260 --> 00:07:09.960
Indeed the data sent into the training set and tested.

110
00:07:10.110 --> 00:07:15.660
And now we had the training set and therefore we can proceed to this next step to train the multiple

111
00:07:15.660 --> 00:07:17.910
in our regression model on the training set.

112
00:07:17.910 --> 00:07:18.440
So here we go.

113
00:07:18.450 --> 00:07:19.870
Let's do it.

114
00:07:19.920 --> 00:07:22.050
Clicking play and there we go.

115
00:07:22.050 --> 00:07:27.440
We now have a fully trained linear regression model on this data set.

116
00:07:27.440 --> 00:07:32.520
Now we have a model that was trained to understand the correlations between different types of spends

117
00:07:32.790 --> 00:07:39.960
by 50 startups and their profits so that now investors can deploy this model on new startups in order

118
00:07:39.960 --> 00:07:44.270
to break what profit they will generate based on these informations.

119
00:07:44.280 --> 00:07:45.180
All right.

120
00:07:45.180 --> 00:07:45.590
Perfect.

121
00:07:45.590 --> 00:07:46.850
So congratulations.

122
00:07:46.860 --> 00:07:49.080
Now let's recap you know three things.

123
00:07:49.080 --> 00:07:54.160
Thanks to this amazing linear regression class you don't have to worry about the dummy variable trap.

124
00:07:54.330 --> 00:07:59.760
You don't either have to worry about selecting the best feature as meaning the fishers with the highest

125
00:07:59.920 --> 00:08:02.460
p value or the most statistically significant.

126
00:08:02.560 --> 00:08:05.080
The linear regression class will take care of that as well.

127
00:08:05.190 --> 00:08:11.130
And also you know how to build and train a multiple in your regression model on a dataset which you

128
00:08:11.130 --> 00:08:13.380
also know how to pre process like a pro.

129
00:08:13.620 --> 00:08:14.360
All right.

130
00:08:14.430 --> 00:08:19.530
So we're now going to move on to the last step which will be to predict the test results.

131
00:08:19.530 --> 00:08:24.990
Because remember that so far our model was trained on the training set and therefore we still need to

132
00:08:24.990 --> 00:08:30.050
check its performance on new observations which are contained in the test set.

133
00:08:30.060 --> 00:08:34.350
However here we have to understand something important you know compared to what we did in the previous

134
00:08:34.350 --> 00:08:39.920
section on simple in our regression this time we actually have several features right.

135
00:08:39.930 --> 00:08:46.020
We actually have four features instead of one like before and therefore we can not actually plot a graph

136
00:08:46.260 --> 00:08:51.690
like we did in simple in our regression where you know we have the features in the x axis and the dependent

137
00:08:51.690 --> 00:08:56.280
variable in the y axis because simply there are four features we would need actually a five dimensional

138
00:08:56.280 --> 00:08:59.100
graph which is not possible to visualize as humans.

139
00:08:59.100 --> 00:09:06.030
So what we're gonna do instead you know instead of visualizing the test results on a plot well we will

140
00:09:06.210 --> 00:09:08.940
actually display two vectors.

141
00:09:08.940 --> 00:09:13.830
The first one is the vectors of the real profits in the test set.

142
00:09:13.890 --> 00:09:19.380
And remember that the test that actually counts for 20 percent of the whole dataset and therefore sits

143
00:09:19.380 --> 00:09:24.620
here there are no actually 50 observations corresponding to the 50 stories.

144
00:09:24.720 --> 00:09:28.700
Well 20 percent of 50 will be 10 observations in the test.

145
00:09:29.070 --> 00:09:33.630
So we will actually have 10 samples in a test and we will display two vectors.

146
00:09:33.630 --> 00:09:38.070
First the vector of the 10 real profits from the test set.

147
00:09:38.070 --> 00:09:44.110
You know the 10 the ground truth and a second vector of the 10 predicted profits of the same test set

148
00:09:44.190 --> 00:09:47.720
so that we can compare for each startup of the test set.

149
00:09:47.760 --> 00:09:53.340
If our predicted profit is close to the real profit and that's how we will you know evaluate our model

150
00:09:53.370 --> 00:10:00.000
and then later on in this part you will actually learn about some evaluation techniques to measure better

151
00:10:00.060 --> 00:10:03.570
the performance of your regression models with some relevant metrics.

152
00:10:03.570 --> 00:10:05.660
But so far this is how we will do it.

153
00:10:05.670 --> 00:10:10.260
We will clearly see if you know our model performs well on your observations you know on the test set

154
00:10:10.440 --> 00:10:15.090
because we will clearly see if the predictions are closed to the real results.

155
00:10:15.090 --> 00:10:15.630
Okay.

156
00:10:15.810 --> 00:10:21.530
So you can try to do it yourself at least the first step which is to get the vector of predicted profits.

157
00:10:21.540 --> 00:10:25.560
But then in order to display the two vectors next to each other.

158
00:10:25.890 --> 00:10:30.690
Well we'll have to use some 7 tricks which are not necessarily obvious so we'll do that together.

159
00:10:30.690 --> 00:10:31.040
All right.

160
00:10:31.500 --> 00:10:34.600
So as soon as you're ready let us proceed to next sartorial.

161
00:10:34.650 --> 00:10:36.600
And until then enjoy machine learning.
