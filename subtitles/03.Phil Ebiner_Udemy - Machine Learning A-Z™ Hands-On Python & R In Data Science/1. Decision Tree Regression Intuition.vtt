WEBVTT
1

00:00:01.110  -->  00:00:05.880
Hello and welcome back to the course on machine learning interested Tauriel we're talking about decision

2

00:00:05.880  -->  00:00:08.160
trees and the intuition behind them.

3

00:00:08.190  -->  00:00:08.610
All right.

4

00:00:08.640  -->  00:00:12.940
So you may have heard the term cart which stands for classification and regression trees.

5

00:00:13.170  -->  00:00:19.070
And this is an umbrella term that encompasses two types of decisions trees and as if croaky guessed

6

00:00:19.140  -->  00:00:23.070
they are classification trees and regression trees.

7

00:00:23.070  -->  00:00:28.800
And in this course we're going to talk about both types but specifically in this section we're focusing

8

00:00:28.830  -->  00:00:30.690
on the regression trees.

9

00:00:31.020  -->  00:00:36.030
And I wanted to mention right away that regression trees are a bit more complex than classification

10

00:00:36.030  -->  00:00:40.570
trees and that's why this is going to be a bit longer and is going to require some additional attention

11

00:00:40.580  -->  00:00:40.820
.

12

00:00:40.950  -->  00:00:48.060
But nevertheless we're still going to break this kind of somewhat complex topic into a very simple bite

13

00:00:48.060  -->  00:00:51.960
sized elements of information so it will all make sense.

14

00:00:51.960  -->  00:00:54.870
And towards the end of it you'll be quite comfortable with regression reas.

15

00:00:54.900  -->  00:00:56.750
So let's get straight into it.

16

00:00:56.760  -->  00:00:57.240
All right.

17

00:00:57.300  -->  00:01:02.760
So here we've got a scatterplot which represent our Darcis sardars that that has been given to us.

18

00:01:02.790  -->  00:01:07.950
And the interesting thing about the scatterplot is that we've got to indep been wearables x1 and x2

19

00:01:07.960  -->  00:01:08.120
.

20

00:01:08.310  -->  00:01:15.210
And what we're predicting is a third variable a dependent variable which is white and you cannot actually

21

00:01:15.210  -->  00:01:16.270
see one in the.

22

00:01:16.280  -->  00:01:21.030
And that is because this is a simply a two dimensional chart only fit the two variables.

23

00:01:21.030  -->  00:01:22.710
Why is the third dimension.

24

00:01:22.710  -->  00:01:27.440
And if you think about it's like sticking out of your screen that's where that dimension is.

25

00:01:27.440  -->  00:01:31.560
And this is just a projection of all the points on the x1 x2 plane.

26

00:01:31.560  -->  00:01:35.070
And so if I add a dimension I would look something like that.

27

00:01:35.220  -->  00:01:37.730
But once again we can see why right now.

28

00:01:37.890  -->  00:01:43.320
And the interesting thing is that we don't actually need to see why because we need to work with this

29

00:01:43.840  -->  00:01:47.260
scatterplot for us for a little bit to build our decision tree.

30

00:01:47.370  -->  00:01:49.610
And then once we've built it will return to y.

31

00:01:49.830  -->  00:01:56.790
Now a quick point I wanted to make here is that I've seen decision trees explained with just one independent

32

00:01:56.940  -->  00:02:04.020
variable so x 1 or just X and Y and then and in that case yes you can you can just put X-1 over here

33

00:02:04.050  -->  00:02:04.450
and then.

34

00:02:04.470  -->  00:02:10.080
Why would go here and you would have a bit of a different kind of diagram and you'd be able to explain

35

00:02:10.080  -->  00:02:10.470
it that way.

36

00:02:10.470  -->  00:02:16.380
But at the same time I think it might not really drive the point home and it can be a bit confusing

37

00:02:16.380  -->  00:02:20.540
when it's explained like that although sometimes it is done.

38

00:02:20.670  -->  00:02:26.880
Nevertheless I thought would go the full way would do the full monty and would look at this problem

39

00:02:26.990  -->  00:02:32.010
with two independent variables because it'll be a more robust explanation so it will make it a bit more

40

00:02:32.010  -->  00:02:37.800
complex but is definitely worth it in the long run because that way will understand the decision tree

41

00:02:37.830  -->  00:02:42.360
regression and bit or Ashlynn I would say quite a bit better.

42

00:02:42.360  -->  00:02:42.620
All right.

43

00:02:42.630  -->  00:02:48.630
So let's continue we've got the X1 and x2 these are independent variables the dependent variable we

44

00:02:48.630  -->  00:02:54.510
can't see it adds the third dimension and we're actually going to forget about it for a little while

45

00:02:54.510  -->  00:02:54.530
.

46

00:02:54.540  -->  00:02:59.940
Right so we're going to just forget about it because we need to work with this scatterplot to see how

47

00:02:59.940  -->  00:03:01.550
our decision is going to be great.

48

00:03:01.800  -->  00:03:10.230
So once you run the regression tree or decision tree algorithm in regression sense of it what will happen

49

00:03:10.260  -->  00:03:18.240
is your scatterplot will be split up into segments and let's have a look at how an algorithm could go

50

00:03:18.240  -->  00:03:18.990
about doing that.

51

00:03:18.990  -->  00:03:24.410
So an algorithm would create a split or here for example at somewhere around 20.

52

00:03:24.660  -->  00:03:29.090
So it would basically split your diagram or your scatterplot into two parts.

53

00:03:29.100  -->  00:03:30.240
Everything else less than 20.

54

00:03:30.240  -->  00:03:34.860
Everything that's great and 20 for the X1 variable then another split would happen here.

55

00:03:34.860  -->  00:03:41.780
So for all of the elements in this side they would be compared to 170 greater or less then had been

56

00:03:41.780  -->  00:03:44.910
on the split here and then maybe another split over here.

57

00:03:44.940  -->  00:03:51.630
Now how and where these splits are conducted is determined by the algorithm.

58

00:03:51.780  -->  00:04:00.570
And it is actually involves looking at something called the information entropy and it is a mathematical

59

00:04:00.570  -->  00:04:02.800
concept it is quite complex.

60

00:04:03.030  -->  00:04:11.040
So it basically means when I perform this split right is this split increasing amount of information

61

00:04:11.040  -->  00:04:12.930
that we have about our points.

62

00:04:12.930  -->  00:04:20.670
Is it actually adding some value to our the way that we want to group our points and the algorithm knows

63

00:04:20.670  -->  00:04:26.920
when to stop is when there is a certain minimum for the information that needs to be added.

64

00:04:27.180  -->  00:04:35.640
And once like it cannot add any more information to our set up by splitting these leaves they're called

65

00:04:35.640  -->  00:04:39.660
leaves so each one of the splits is called the leaf by splitting these leaves.

66

00:04:40.270  -->  00:04:45.730
Ones it can add more information than stops or in or the algorithm could let's say stop when you have

67

00:04:45.730  -->  00:04:50.950
less than 5 percent well if you were to conduct a split then you would have less than 5 percent of your

68

00:04:51.010  -->  00:04:54.340
total points in that leaf and then that leaf wouldn't be created.

69

00:04:54.340  -->  00:04:58.660
So there are different variations of different options for that to happen.

70

00:04:58.690  -->  00:05:02.330
And but look the most important thing is of course where the splits are happening.

71

00:05:02.560  -->  00:05:07.570
And if you'd like to learn more about that you'd you'd need to study a bit more about information entropy

72

00:05:07.570  -->  00:05:07.990
.

73

00:05:07.990  -->  00:05:12.850
We're not going to go into that mathematical depth right now for us it's sufficient to know that the

74

00:05:12.850  -->  00:05:19.600
algorithm can handle this and that it is finding the optimal splits of our data set into these leaves

75

00:05:19.600  -->  00:05:19.620
.

76

00:05:19.630  -->  00:05:25.810
And the final These are called terminal leaves and then we're going to focus on the practical application

77

00:05:25.810  -->  00:05:33.190
of this algorithm how and why we're using these decision trees and how this regression is going to work

78

00:05:33.190  -->  00:05:33.700
.

79

00:05:33.700  -->  00:05:35.920
All right so hopefully we're on the same page.

80

00:05:35.950  -->  00:05:36.510
Let's continue.

81

00:05:36.510  -->  00:05:42.970
So we're going to rewind all of this a little bit and we're going to create these splits one by one

82

00:05:42.970  -->  00:05:46.630
and alongside we're going to actually start drawing our decision tree.

83

00:05:46.930  -->  00:05:49.280
So there's our diagram brand new and fresh.

84

00:05:49.510  -->  00:05:51.340
And there goes our For a split.

85

00:05:51.520  -->  00:05:54.300
So now we're going to start creating our decision tree.

86

00:05:54.560  -->  00:05:57.640
The split happened at 20 so let's start drawing.

87

00:05:57.790  -->  00:06:01.260
There is our first decision and we have two options.

88

00:06:01.270  -->  00:06:02.940
Yes and No.

89

00:06:03.250  -->  00:06:05.410
All right so let's have let's see what happens next.

90

00:06:05.440  -->  00:06:11.530
Next happens a split to split to happens at 170 and only happens for the points that are greater than

91

00:06:11.530  -->  00:06:12.170
20.

92

00:06:12.430  -->  00:06:17.480
So that means you would check this condition X-1 is less than 20 meaning you checked no.

93

00:06:17.500  -->  00:06:18.360
The answer's no.

94

00:06:18.490  -->  00:06:25.630
And then you check if x 2 is less than 170 X-News less than once and then a split 3 happens on the other

95

00:06:25.630  -->  00:06:29.200
side and it checks if x 2 is alyssum 200.

96

00:06:29.320  -->  00:06:37.840
Let's add that here X to this and it and then split 4 happens at 40 and it checks if x 1 is greater

97

00:06:37.900  -->  00:06:43.430
or less than 40 and split for only happens for the point that answered to split one they answered.

98

00:06:43.520  -->  00:06:45.120
No it's not less than 20.

99

00:06:45.130  -->  00:06:47.300
And to split two they answered no.

100

00:06:47.300  -->  00:06:50.240
It's yes it's actually less than 170.

101

00:06:50.530  -->  00:06:52.950
So no not less than 20 years.

102

00:06:52.960  -->  00:06:53.920
It's less than 170.

103

00:06:53.920  -->  00:06:58.490
And then this split will four happens X-1 is less than 40.

104

00:06:58.510  -->  00:06:59.240
Yes.

105

00:06:59.530  -->  00:07:01.140
All right so that's our decision tree.

106

00:07:01.150  -->  00:07:02.840
It's done it's drawn.

107

00:07:03.070  -->  00:07:04.450
And so what happens next.

108

00:07:04.450  -->  00:07:07.620
How what do we actually populate into those boxes.

109

00:07:07.840  -->  00:07:11.320
Well this is where we need to remember about our dependent variable.

110

00:07:11.410  -->  00:07:22.630
The third dimension and what we need to check here is how are we going to predict the value of y for

111

00:07:22.690  -->  00:07:28.130
a new observation that gets added to our scatterplot or our daughters.

112

00:07:28.150  -->  00:07:35.320
So let's say we add a observation which is has x 1 equal to 30 and x 2 equals to 50.

113

00:07:35.410  -->  00:07:37.140
It did fall somewhere over here.

114

00:07:37.300  -->  00:07:40.660
And 50 is somewhere we're headed for some or are here.

115

00:07:40.780  -->  00:07:45.060
So obviously it falls into this terminal leaf.

116

00:07:45.070  -->  00:07:47.880
And how does that information.

117

00:07:47.890  -->  00:07:53.390
So as you can see we've been adding these plates we've added information into our system.

118

00:07:53.410  -->  00:07:58.450
So how does that information that now we know that it falls into this terminal leaf.

119

00:07:58.510  -->  00:08:04.060
How does that information help us in terms of predicting the value of wife for that new element that

120

00:08:04.060  -->  00:08:04.960
we're going to add.

121

00:08:05.230  -->  00:08:08.550
Well the way it works is it's actually pretty straightforward.

122

00:08:08.620  -->  00:08:14.860
The way it works is you just take the averages of each of your terminal leaves.

123

00:08:14.950  -->  00:08:21.790
So you take the average of y for all of these points and that'll be the value that will be assigned

124

00:08:21.790  -->  00:08:23.990
to any new point that falls in this terminal.

125

00:08:23.990  -->  00:08:26.860
The same for this terminal leave Same for this terminal.

126

00:08:26.880  -->  00:08:28.530
Same for this one and same for us.

127

00:08:28.680  -->  00:08:29.290
So let's have a look.

128

00:08:29.290  -->  00:08:34.480
Let's say the average for why here are sixty five point seven The average for wise here is three hundred

129

00:08:34.480  -->  00:08:36.930
point five 1023 here.

130

00:08:36.970  -->  00:08:39.840
Minus sixty four point one zero points on here.

131

00:08:40.030  -->  00:08:48.460
So for that point that we just discussed with X Wanny Consoli and x 2 x equals 50 the predicted value

132

00:08:48.460  -->  00:08:49.300
of y.

133

00:08:49.390  -->  00:08:54.280
The regression tree algorithm would predict a value of minus sixty four point one.

134

00:08:54.490  -->  00:08:59.620
If it were to fall in any other terminal if then that's what the value there would predict.

135

00:08:59.620  -->  00:09:01.950
So as you can see it's actually pretty straightforward.

136

00:09:01.960  -->  00:09:10.480
It's it's very simple as just taking averages but you do need to remember that we are working on the

137

00:09:10.480  -->  00:09:18.060
whole point of this exercise is to add more information into our chart into our system to better predict

138

00:09:18.070  -->  00:09:18.280
why.

139

00:09:18.280  -->  00:09:25.510
Because if you think about it what was our other option what is our default option if the default option

140

00:09:25.510  -->  00:09:31.270
were far running any machine learning on this data set is to just take all of the points and take the

141

00:09:31.270  -->  00:09:38.230
average across all of the points and whatever that is wherever you point the new element of data that

142

00:09:38.530  -->  00:09:40.300
is added to our data set wherever it falls.

143

00:09:40.300  -->  00:09:41.330
We just assign.

144

00:09:41.410  -->  00:09:45.960
It's always that's average for all of the points that we had existing previously.

145

00:09:46.000  -->  00:09:51.490
What we did now is we've split our diagram up into these terminal leaves the machine learning algorithm

146

00:09:51.490  -->  00:09:54.050
has added information nurture into our system.

147

00:09:54.100  -->  00:10:02.200
And so now we can more accurately predict the value or assign the value of y to a new coming element

148

00:10:02.200  -->  00:10:02.550
.

149

00:10:02.560  -->  00:10:09.700
And as you can see now it's averages across all of them averages taken into specific parts or segments

150

00:10:09.760  -->  00:10:14.810
of our scatterplot and therefore it is or it's supposed to be more accurate.

151

00:10:14.830  -->  00:10:17.820
That's the whole point of the regression tree.

152

00:10:17.950  -->  00:10:24.640
And now the last time we are left to do is to add the values into our decision tree so basically we

153

00:10:24.640  -->  00:10:29.040
just add those values in here and now whenever we have a new value.

154

00:10:29.650  -->  00:10:36.670
What would happen is the algorithm which is go through this these checks ended with check where it falls

155

00:10:36.670  -->  00:10:38.110
and assign the value.

156

00:10:38.440  -->  00:10:39.340
And that's pretty much it.

157

00:10:39.340  -->  00:10:45.520
So the scatterplot is more for like physical Zeshan conceptual purposes so you can maybe drive some

158

00:10:45.520  -->  00:10:46.390
insights from there.

159

00:10:46.480  -->  00:10:52.900
But the core of decision tree is actually held here and that's why the algorithm is called a regression

160

00:10:52.990  -->  00:10:53.810
tree.

161

00:10:53.920  -->  00:10:59.890
I hope you enjoyed today's Statoil and hopefully we did break down this quite complex topic into some

162

00:10:59.950  -->  00:11:01.900
simple and actionable steps.

163

00:11:02.020  -->  00:11:04.070
And I look forward to seeing you next.

164

00:11:04.120  -->  00:11:05.950
Until then in Germany in learning
