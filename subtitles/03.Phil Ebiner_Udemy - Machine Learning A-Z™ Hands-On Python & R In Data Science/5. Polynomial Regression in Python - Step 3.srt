1
00:00:00,090 --> 00:00:01,110
Hello my friends.

2
00:00:01,110 --> 00:00:06,300
So are you ready to visualize a linear regression results and the polynomial regression results to indeed

3
00:00:06,330 --> 00:00:12,750
understand that the polynomial regression model is way more adapted to nonlinear datasets meaning two

4
00:00:12,750 --> 00:00:15,140
datasets with non-linear relationships.

5
00:00:15,140 --> 00:00:16,350
All right let's do this.

6
00:00:16,350 --> 00:00:18,080
Let's create a new coat cell.

7
00:00:18,120 --> 00:00:19,890
And now you actually know what to do.

8
00:00:19,890 --> 00:00:22,790
I should've told you in the proofs tutorial to try to do it yourself first.

9
00:00:23,040 --> 00:00:25,260
Because we're going to code exactly the same as.

10
00:00:25,260 --> 00:00:30,630
Remember this piece of code we implemented to visualize the results of the simple linear regression

11
00:00:30,630 --> 00:00:31,260
model.

12
00:00:31,260 --> 00:00:31,840
But it's okay.

13
00:00:31,860 --> 00:00:33,680
Let's do it together now quickly.

14
00:00:33,750 --> 00:00:39,510
So remember the first thing that we have to do is call the matte plot lib dot by plot module which has

15
00:00:39,510 --> 00:00:41,470
a shortcut P L T.

16
00:00:41,580 --> 00:00:48,660
Then from this module we're going to call first the scatter function which will allow to first you know

17
00:00:48,660 --> 00:00:54,890
display on a 2D plot the different points of coordinates containing the real results.

18
00:00:54,900 --> 00:00:58,930
You know the real position level's going from 1 to 10.

19
00:00:59,010 --> 00:01:00,230
And the real salaries.

20
00:01:00,240 --> 00:01:03,870
So that's what we are planning first and then we'll plot the predictions.

21
00:01:03,870 --> 00:01:09,990
And so while here we have two input for the coordinates of these real points containing the real position

22
00:01:09,990 --> 00:01:14,070
levels in real salaries and on the x axis we have of course the position levels.

23
00:01:14,070 --> 00:01:20,010
Therefore the x coordinates will be X you know all the position levels contained in X and then on the

24
00:01:20,010 --> 00:01:25,500
y axis will have the salaries the real ones and therefore the y coordinates will be Y.

25
00:01:25,740 --> 00:01:27,060
All right.

26
00:01:27,060 --> 00:01:32,590
And then remember we need to add a color and we'll choose like last time red.

27
00:01:32,610 --> 00:01:33,170
Perfect.

28
00:01:33,180 --> 00:01:34,560
So that's for the real results.

29
00:01:34,560 --> 00:01:38,040
And now we're gonna plot the predictions and to do that.

30
00:01:38,100 --> 00:01:45,180
Well we're gonna call P LTE again motility type plot from which we're gonna call the plot method because

31
00:01:45,180 --> 00:01:51,390
this time we're actually going to plot that regression line you know in blue like in simple in our regression

32
00:01:51,660 --> 00:01:56,490
and then for polynomial regression you will see that it's not aligned but it will be actually a curve

33
00:01:56,730 --> 00:01:59,510
and still we will use the plot function for that.

34
00:01:59,520 --> 00:02:06,260
So again we have to enter the coordinates of the different points of this line and for these coordinates

35
00:02:06,270 --> 00:02:12,870
Well first the x coordinates are still X the position levels but then the y coordinates will be of course

36
00:02:12,870 --> 00:02:18,900
the predicted salaries you know instead of the real salaries and to get them well we simply to call

37
00:02:18,900 --> 00:02:25,380
our line rank object Nuttall in red to you know Limerick to his for the polynomial regression model

38
00:02:25,500 --> 00:02:27,810
Lynn regs for the linear regression model.

39
00:02:27,990 --> 00:02:35,540
And so Lynn rank from which we're gonna call that predict method applied of course to X right.

40
00:02:35,550 --> 00:02:39,010
Containing the position levels of the matrix of features X..

41
00:02:39,180 --> 00:02:44,810
All right and then we add a color and like last time will choose the color blue.

42
00:02:44,820 --> 00:02:51,720
All right blue and then we'll just improve or enhance our graph by adding a title and x level and a

43
00:02:51,720 --> 00:02:52,280
Y label.

44
00:02:52,290 --> 00:02:55,460
And finally displayed so you know how to do this.

45
00:02:55,470 --> 00:02:59,790
We call first party then the title function.

46
00:02:59,790 --> 00:03:07,740
So here we're going to add a funny title like in quotes of course truth or fluff.

47
00:03:07,740 --> 00:03:08,010
Right.

48
00:03:08,010 --> 00:03:13,710
This is the simple scenario I invented for this case study and then we're going to precise that it's

49
00:03:13,800 --> 00:03:15,220
linear regression.

50
00:03:15,220 --> 00:03:17,020
You know the linear regression model.

51
00:03:17,560 --> 00:03:17,940
Okay.

52
00:03:17,960 --> 00:03:21,270
Good good title now an X label.

53
00:03:21,270 --> 00:03:30,130
So here again fealty that X label and we'll choose you know something simple like position level.

54
00:03:30,450 --> 00:03:31,440
Okay perfect.

55
00:03:31,440 --> 00:03:33,240
Then a y label.

56
00:03:33,390 --> 00:03:40,650
So I'm calling the Y label function and we're going to enter salary because on the y axis are all the

57
00:03:40,650 --> 00:03:43,420
salaries the real ones or the predictive ones.

58
00:03:43,440 --> 00:03:51,260
And finally remember that we have to show the graph by using this show function perfect.

59
00:03:51,540 --> 00:03:52,210
And there we go.

60
00:03:52,210 --> 00:03:55,210
Where are you ready to visualize a linear regression results.

61
00:03:55,230 --> 00:03:55,860
Are you ready.

62
00:03:55,920 --> 00:04:00,090
I'm going to close this so that we can see it very well.

63
00:04:00,090 --> 00:04:02,370
All right let's make sure I didn't make any mistake.

64
00:04:02,370 --> 00:04:04,080
Yeah it seems all good.

65
00:04:04,080 --> 00:04:05,880
So now let's press play.

66
00:04:05,910 --> 00:04:07,170
And here we go.

67
00:04:07,170 --> 00:04:10,130
This is the linear regression results.

68
00:04:10,140 --> 00:04:12,200
Let's scroll down a bit.

69
00:04:12,210 --> 00:04:12,830
All right.

70
00:04:12,870 --> 00:04:21,090
So first of all to recap the red points here are the real salaries you know going from zero to I think

71
00:04:21,180 --> 00:04:23,310
one million for the CEO.

72
00:04:23,310 --> 00:04:23,660
Yes.

73
00:04:23,690 --> 00:04:27,210
Okay so these are all the real salaries of this column.

74
00:04:27,530 --> 00:04:28,100
Okay.

75
00:04:28,140 --> 00:04:34,440
And then the blue line is of course the regression line containing all our predictions but the predictions

76
00:04:34,470 --> 00:04:36,390
of the linear regression model.

77
00:04:37,080 --> 00:04:44,600
So first of all we can see that indeed the linear regression model is not well adapted to this dataset

78
00:04:44,700 --> 00:04:50,250
because indeed remember that in simple in our regression it was well adapted because each time you know

79
00:04:50,250 --> 00:04:55,860
for each of the values of the features Well the prediction was close to the real result.

80
00:04:56,220 --> 00:05:01,000
But here for many position levels you know for many values of the future.

81
00:05:01,030 --> 00:05:04,120
Well the prediction is far from the real result here.

82
00:05:04,180 --> 00:05:06,940
For example it is quite far from the real salary.

83
00:05:06,940 --> 00:05:13,030
Imagine if we wanted to use that model to predict if someone is saying the truth or bluffing about a

84
00:05:13,030 --> 00:05:14,470
salary that is contained here.

85
00:05:14,740 --> 00:05:19,290
Well you know we would have offered a way higher salary than we would be supposed to.

86
00:05:19,300 --> 00:05:21,630
So that would not be the best negotiation.

87
00:05:21,640 --> 00:05:21,920
OK.

88
00:05:21,940 --> 00:05:24,890
So that linear regression model is not well adapted.

89
00:05:24,890 --> 00:05:25,630
And same here.

90
00:05:25,660 --> 00:05:27,160
It is far from the prediction.

91
00:05:27,220 --> 00:05:29,820
Same here far from the prediction here it's okay.

92
00:05:29,830 --> 00:05:32,350
But you know that's only for two points.

93
00:05:32,350 --> 00:05:33,490
Two salaries.

94
00:05:33,550 --> 00:05:36,760
But then here it's super far from operation and here as well.

95
00:05:36,760 --> 00:05:38,890
So clearly not well adapted.

96
00:05:38,890 --> 00:05:43,840
And that's why I want to show you this because now you're going to see that the pulling the mere regression

97
00:05:43,840 --> 00:05:46,670
results are gonna be much better.

98
00:05:46,750 --> 00:05:48,770
And now I'm gonna show this to you right away.

99
00:05:48,790 --> 00:05:54,640
I'm going to close this and we're going to visualize binomial regression results efficiently by taking

100
00:05:54,640 --> 00:05:58,960
this code because you're going to see that it's going to be almost the same.

101
00:05:58,960 --> 00:06:06,850
So I'm going to scroll down ask crawled back up to compare create a new coat cell basing that here and

102
00:06:06,850 --> 00:06:10,800
now according to you what do we have to change here.

103
00:06:10,870 --> 00:06:12,490
Actually something important.

104
00:06:12,490 --> 00:06:18,050
Please press buzz on the video now and try to figure out before I give you the solution.

105
00:06:18,100 --> 00:06:22,030
What you have to replace here in order to visualize polynomial regression results.

106
00:06:22,330 --> 00:06:25,750
I really want you to think because it's not that easy it is easy.

107
00:06:25,750 --> 00:06:28,840
But you know it's not like what you would think at first sight.

108
00:06:29,110 --> 00:06:34,060
So please press pause in the video and make that change in order to make it work for polynomial regression

109
00:06:34,060 --> 00:06:35,610
results.

110
00:06:35,860 --> 00:06:36,450
All right.

111
00:06:36,520 --> 00:06:36,840
Okay.

112
00:06:36,850 --> 00:06:38,560
Now I'm going to tell you the solution.

113
00:06:38,590 --> 00:06:44,950
Well first of course we have to change our regressive here because we want to use our polynomial regression

114
00:06:44,950 --> 00:06:48,580
model which is based on Lin rig too.

115
00:06:48,880 --> 00:06:52,090
So that's the first change Limerick to yes.

116
00:06:52,090 --> 00:06:59,110
And now in the British method do you think we need to keep X or change that by something else.

117
00:06:59,110 --> 00:07:05,290
Well of course we can't keep X because remember X is only that matrix of single feature containing the

118
00:07:05,290 --> 00:07:06,540
position levels.

119
00:07:06,550 --> 00:07:12,610
Remember that when we use the Lin rect to regress her which is the linear aggressor it has to be applied

120
00:07:12,610 --> 00:07:19,390
to the transformed matrix of features X into this matrix of features at the different powers you know

121
00:07:19,570 --> 00:07:21,910
position level then squared position level.

122
00:07:21,940 --> 00:07:27,760
And then the other powers here we only chose N equals two so far so we only have the squared position

123
00:07:27,760 --> 00:07:28,450
levels.

124
00:07:28,450 --> 00:07:30,910
But that's exactly what you need to change here.

125
00:07:30,910 --> 00:07:33,370
You can't keep X because it is the single feature.

126
00:07:33,370 --> 00:07:40,330
So you need to input here that transformed matrix of features X containing the different powers of that

127
00:07:40,330 --> 00:07:42,070
single feature position level.

128
00:07:42,070 --> 00:07:50,260
And that's of course exactly this pulley rag veterans for method applied to this matrix of single feature

129
00:07:50,550 --> 00:07:51,180
x.

130
00:07:51,240 --> 00:07:57,460
So I'm copying this and that's exactly what we have to input inside this predict method.

131
00:07:57,580 --> 00:07:58,550
You understand.

132
00:07:58,550 --> 00:08:01,370
So that was the little thing not to forget.

133
00:08:01,450 --> 00:08:02,620
And now you're all good.

134
00:08:02,620 --> 00:08:06,970
You're ready to have a beautiful visualization of the polynomial regression results.

135
00:08:06,970 --> 00:08:11,840
Let's just replace this linear here by Bob Lee normal.

136
00:08:12,250 --> 00:08:14,200
And now 100 percent ready.

137
00:08:14,330 --> 00:08:18,250
Let's visualize the polynomial regression results.

138
00:08:18,250 --> 00:08:19,570
And there you go.

139
00:08:19,570 --> 00:08:27,790
Now we have indeed a way more adaptive regression curve coming indeed much closer to the real result.

140
00:08:27,790 --> 00:08:31,440
You know the real salaries indeed if we compare two.

141
00:08:31,750 --> 00:08:35,740
I'm going to zoom out a bit so we can see both of them at the same time.

142
00:08:35,740 --> 00:08:36,790
There we go.

143
00:08:36,790 --> 00:08:41,890
So if we compare these points where we had issues previously with the linear regression model well we

144
00:08:41,890 --> 00:08:48,880
can clearly see now that the issue is resolved because indeed the predictions on this blue curve come

145
00:08:48,970 --> 00:08:53,890
way closer to the real salaries and this is only with N equals two.

146
00:08:53,890 --> 00:08:58,690
I'm going to show you then that with higher powers and equals three or four.

147
00:08:58,930 --> 00:09:03,820
Well we will get even better results and I'm going to actually show this to you right away.

148
00:09:03,880 --> 00:09:09,580
So now what we're going to do is well we're going to keep this but we're going to remove this because

149
00:09:09,610 --> 00:09:14,340
we're going to retrain the polynomial regression model with a higher degree.

150
00:09:14,530 --> 00:09:18,940
Let's take for example you can try with three but we're going to directly try with four.

151
00:09:19,240 --> 00:09:20,050
So there you go.

152
00:09:20,050 --> 00:09:25,660
I'm going to remove the output retrain the pulling them a regression model on the whole dataset with

153
00:09:25,660 --> 00:09:32,260
therefore this time a degree of 4 which means that the polynomial regression equation will be salary

154
00:09:32,290 --> 00:09:39,970
equals B zero plus B one times position level plus B two times position level square plus B three times

155
00:09:39,970 --> 00:09:45,220
position level all of the power of three plus before times position level the power for that will be

156
00:09:45,220 --> 00:09:51,250
the new Polynomial regression equation And so therefore let's retrain its let's build this new Polynomial

157
00:09:51,250 --> 00:09:54,430
regression moral by just running this cell again.

158
00:09:54,430 --> 00:09:54,820
All right.

159
00:09:54,830 --> 00:10:01,000
Now we have it and now very simply we're going to visualize the new Polynomial Russian results by clicking

160
00:10:01,000 --> 00:10:07,330
this sell here and now as you can see now the polynomial regression model is perfectly fitting.

161
00:10:07,390 --> 00:10:08,110
This data set.

162
00:10:08,110 --> 00:10:10,940
So here we clearly have over filling but that's okay.

163
00:10:10,960 --> 00:10:16,450
Only in this situation because we want to have a perfect prediction of the salary between position level

164
00:10:16,690 --> 00:10:17,720
6 and 7.

165
00:10:18,450 --> 00:10:18,900
Okay.

166
00:10:18,910 --> 00:10:23,680
And now one final thing because I really want you to have the best results and best visualizations.

167
00:10:23,680 --> 00:10:30,280
Indeed as you can see here what happened is that only some straight lines were plotted between each

168
00:10:30,280 --> 00:10:33,470
consecutive points of the data set right.

169
00:10:33,580 --> 00:10:37,950
And therefore that makes this curve not as smooth as we would hope for.

170
00:10:37,960 --> 00:10:43,360
So I actually prepared another code but which we want code together because this is just for the sake

171
00:10:43,360 --> 00:10:45,000
of having a more beautiful curve.

172
00:10:45,190 --> 00:10:49,090
So we're going to get it from the original implementation.

173
00:10:49,080 --> 00:10:54,130
It is actually right here you see visualizing the pulling them a regression results for a higher resolution

174
00:10:54,190 --> 00:10:55,380
and smoother curve.

175
00:10:55,420 --> 00:11:03,830
So I'm going to take all this code and then paste it in our copy of the implementation right here in

176
00:11:03,830 --> 00:11:09,970
New Kotel and you will see that we will get indeed a much smoother and more beautiful curve as you can

177
00:11:09,970 --> 00:11:10,860
see right.

178
00:11:10,870 --> 00:11:16,540
And the trick to plot this curve I'm going to explain this quickly is just to instead of taking you

179
00:11:16,540 --> 00:11:20,550
know the integer is zero one two three four five six seven eight nine ten.

180
00:11:20,590 --> 00:11:25,990
Well we increase the density of these points by taking not only these integers but you know one one

181
00:11:25,990 --> 00:11:30,640
point one one point two one point three one point four up to you know nine point one nine point two

182
00:11:30,880 --> 00:11:32,550
nine point eight nine point nine ten.

183
00:11:32,590 --> 00:11:34,350
That's what this 0.01 means.

184
00:11:34,360 --> 00:11:36,130
That's what we call this step.

185
00:11:36,130 --> 00:11:36,430
All right.

186
00:11:36,430 --> 00:11:38,330
So you have to understand this.

187
00:11:38,410 --> 00:11:42,940
You can if you want but this is something you will probably do only once in your life because I remind

188
00:11:42,940 --> 00:11:45,990
that usually your datasets will have many features.

189
00:11:46,030 --> 00:11:50,680
And here I only took one picture in order to show you the results on a graph because you know if we

190
00:11:50,680 --> 00:11:55,010
had many features I couldn't show this to you because we would have way too many dimensions.

191
00:11:55,120 --> 00:11:56,710
So don't worry too much about it.

192
00:11:56,740 --> 00:11:57,170
OK.

193
00:11:57,280 --> 00:11:59,030
But just appreciate the result.

194
00:11:59,050 --> 00:11:59,290
Right.

195
00:11:59,290 --> 00:12:05,130
We have a very well trained and therefore very well fitted but over fitted model for this data set.

196
00:12:05,320 --> 00:12:11,800
But that's fine because then indeed we will be able to get an amazing and accurate prediction to figure

197
00:12:11,800 --> 00:12:14,410
out if there is truth or bluff.

198
00:12:14,410 --> 00:12:20,590
So that's what we'll do in this next and last tutorial we will first predict that salary of the position

199
00:12:20,590 --> 00:12:26,980
level six point five with linear regression and then with polynomial regression I really encourage you

200
00:12:26,980 --> 00:12:31,280
to try to do it on your own first because it's actually not that difficult.

201
00:12:31,300 --> 00:12:36,430
The only new thing that you'll have to figure out will be to well figure out how to predict a single

202
00:12:36,430 --> 00:12:40,610
observation instead of a whole matrix of features of the test set.

203
00:12:40,720 --> 00:12:45,460
But really if at least you try well you will definitely improve your python skills.

204
00:12:46,030 --> 00:12:47,260
So good luck.

205
00:12:47,350 --> 00:12:51,900
And I can't wait to see you in the next material to find out about that prediction.

206
00:12:51,900 --> 00:12:53,740
And until then enjoy machine learning.
