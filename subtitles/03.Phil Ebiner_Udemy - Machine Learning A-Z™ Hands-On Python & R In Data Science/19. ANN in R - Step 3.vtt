WEBVTT
1
00:00:00.330 --> 00:00:02.520
Hello and welcome to this art to Tauriel.

2
00:00:02.700 --> 00:00:07.340
So we just connected to our to instance and now things are going to get easy.

3
00:00:07.590 --> 00:00:13.140
That's another reason why this age tour package is great because in a few lines of code we're going

4
00:00:13.140 --> 00:00:19.750
to be able to build a complex DPN model and it will be very simple We'll just use one function and then

5
00:00:19.750 --> 00:00:21.210
input different parameters.

6
00:00:21.240 --> 00:00:27.730
I'm going to first define our classifier that I called classifier then equals then that's where we used

7
00:00:27.730 --> 00:00:29.420
the H to function.

8
00:00:29.430 --> 00:00:36.000
So to take this function we need to take first to 0 package and then the function that we're going to

9
00:00:36.000 --> 00:00:38.140
use right now is one of the functions here.

10
00:00:38.280 --> 00:00:44.010
And you are going to like the name of this function sensitives called Deep learning so that here and

11
00:00:44.010 --> 00:00:45.380
then d d.

12
00:00:45.750 --> 00:00:46.650
And here it is.

13
00:00:46.650 --> 00:00:50.770
Deep learning that's the function we'll use to build our deep learning model.

14
00:00:50.970 --> 00:00:51.990
Pressing enter.

15
00:00:52.230 --> 00:00:53.160
And here we go.

16
00:00:53.160 --> 00:00:56.820
Now we just need to input the different arguments.

17
00:00:56.850 --> 00:01:04.080
So what I'm going to do now is go here and press one to have a better look at all the arguments of this

18
00:01:04.440 --> 00:01:06.950
age to deep learning function.

19
00:01:06.960 --> 00:01:10.910
So let's see let's scroll down here to find the arguments.

20
00:01:11.100 --> 00:01:12.120
And here we go.

21
00:01:12.420 --> 00:01:17.850
So the first argument is x vector containing the character names of the predictors in the model.

22
00:01:17.850 --> 00:01:19.960
That's actually not a required argument.

23
00:01:20.130 --> 00:01:25.680
We don't need to input it so we'll move on to the next one to the next one is why the name of the response

24
00:01:25.740 --> 00:01:27.300
variable in the mall.

25
00:01:27.330 --> 00:01:32.640
So we will have to use this one because actually the next argument is training frame and this training

26
00:01:32.650 --> 00:01:35.190
frame argument is basically the training set.

27
00:01:35.250 --> 00:01:41.250
That means that for this training for arguments we can put our training said here and since the training

28
00:01:41.250 --> 00:01:46.410
set contains both the independent variables and the dependent variable we need to specify with this

29
00:01:46.530 --> 00:01:47.800
Y argument here.

30
00:01:47.910 --> 00:01:51.020
What is the dependent variable in this training frame.

31
00:01:51.120 --> 00:01:52.630
That is our trainset.

32
00:01:52.710 --> 00:01:58.500
And so with these two informations training friend why the model will then understand what is the training

33
00:01:58.500 --> 00:01:58.940
set.

34
00:01:59.010 --> 00:02:02.480
What are the independent variables and what is the dependent variable.

35
00:02:02.490 --> 00:02:06.080
So let's start by putting these two arguments first.

36
00:02:06.150 --> 00:02:06.960
Why.

37
00:02:07.200 --> 00:02:13.560
Because And as you can see it is just asking for the name of the response very well that has been invaluable.

38
00:02:13.650 --> 00:02:20.460
So here we just need two inputs in quotes the name of the dependent variable which is exited.

39
00:02:20.610 --> 00:02:21.150
All right.

40
00:02:21.150 --> 00:02:26.250
Done for the first argument comma and then moving on to the second argument.

41
00:02:26.370 --> 00:02:31.810
So the second argument is training underscore frame.

42
00:02:31.810 --> 00:02:32.650
Here we go.

43
00:02:32.850 --> 00:02:34.340
And so that's our training set.

44
00:02:34.480 --> 00:02:38.290
And therefore we in here training sets.

45
00:02:38.350 --> 00:02:43.890
All right but now be careful because as it is said in this training frame argument info here.

46
00:02:44.050 --> 00:02:47.090
This is an age to frame objects.

47
00:02:47.170 --> 00:02:50.500
And right now our training set is not an age to frame object.

48
00:02:50.500 --> 00:02:55.200
It is just a training set that is a data frame but that's not the age to frame.

49
00:02:55.420 --> 00:03:02.590
So we need to convert this data from training set into an age to frame and to do this there is one very

50
00:03:02.590 --> 00:03:11.650
simple way it is to use the as Dot age to O function that is a function that will convert its input.

51
00:03:11.650 --> 00:03:18.880
Here training said that so far as a data frame into nature to frame and therefore this train frame argument

52
00:03:18.880 --> 00:03:23.860
is getting what it is expecting great second argument doesn't come up.

53
00:03:23.860 --> 00:03:25.970
Moving on to the third argument.

54
00:03:26.050 --> 00:03:27.690
So what is the argument now.

55
00:03:27.820 --> 00:03:33.910
Well we have other arguments like model ID but we won't use it override with best model so that's some

56
00:03:33.910 --> 00:03:34.550
other options.

57
00:03:34.570 --> 00:03:40.570
But let's not focus on that right now that's not the most important than simple validation frame checkpoint

58
00:03:40.690 --> 00:03:43.410
020 encoder pre-trained our encoder.

59
00:03:43.450 --> 00:03:45.700
That's again not the most important.

60
00:03:45.860 --> 00:03:53.380
Then moving on to what's most important we get to activation which of course corresponds to the activation

61
00:03:53.380 --> 00:03:54.120
function.

62
00:03:54.220 --> 00:03:55.880
You want to use for your network.

63
00:03:56.200 --> 00:04:02.860
And so that argument is very important because you saw incurables intuition to Tauriel that some activation

64
00:04:02.860 --> 00:04:08.260
functions are better than others and actually ask really explained the best activation function to use

65
00:04:08.260 --> 00:04:14.450
to train an artificial neural network is to rectify a function which has this function here on the slide.

66
00:04:14.530 --> 00:04:19.020
And that's the function we'll use right now for artificial neural network.

67
00:04:19.020 --> 00:04:26.170
All right so let's input activation equals and then quotes rectifier.

68
00:04:26.170 --> 00:04:29.050
Make sure to type capital are so perfect.

69
00:04:29.110 --> 00:04:33.970
We have one of the best choices of activation functions for artificial neural network.

70
00:04:34.420 --> 00:04:35.120
Okay great.

71
00:04:35.200 --> 00:04:36.510
And then come up.

72
00:04:36.520 --> 00:04:38.770
Moving on to the fourth argument.

73
00:04:38.830 --> 00:04:44.920
So the fourth argument is actually the one just below him and that's the hidden layer sizes.

74
00:04:45.040 --> 00:04:51.820
That is it's a two in one argument because specifying this argument will allow you to specify two parameters

75
00:04:51.850 --> 00:04:53.840
of your network at the same time.

76
00:04:53.860 --> 00:04:56.330
The first parameter is the number of hidden layers.

77
00:04:56.470 --> 00:05:00.420
And the second parameter is the number of nodes in each hidden layer.

78
00:05:00.410 --> 00:05:04.930
And so how do we put these two parameters in this only one argument here.

79
00:05:05.080 --> 00:05:11.140
Well we are doing this with a vector the number of elements of this vector will be the number of hidden

80
00:05:11.140 --> 00:05:17.140
layers and the value of each element of this vector will be the number of nodes in the hidden layer.

81
00:05:17.380 --> 00:05:23.620
So for example here we have C so you know C is the way to define a vector and are then first element

82
00:05:23.620 --> 00:05:27.770
of the vector 100 and then the second element of the vector 100 again.

83
00:05:28.000 --> 00:05:33.250
So that means that here we have a vector of two elements and therefore will have two have layers and

84
00:05:33.330 --> 00:05:39.990
in the first layer will have 100 neurons and in the second hidden layer we'll have 100 neurons again.

85
00:05:40.330 --> 00:05:45.640
And now time to ask ourselves a big question a question that comes very often when building a deep learning

86
00:05:45.640 --> 00:05:46.260
model.

87
00:05:46.420 --> 00:05:48.900
It's how many layers do we want to choose.

88
00:05:49.000 --> 00:05:55.150
That is how many elements in this vector and then how many neurons we want to put in each layer.

89
00:05:55.150 --> 00:06:00.790
So unfortunately the bad news is that there is no rule of thumb to choose this optimal number of hidden

90
00:06:00.790 --> 00:06:04.330
layers and these optimal numbers of neurons in the hidden layers.

91
00:06:04.420 --> 00:06:09.880
But there is a tip that we can use and this tip is not really based on research but rather based on

92
00:06:09.880 --> 00:06:11.820
experiments we observed.

93
00:06:11.860 --> 00:06:18.280
That's a convenient choice of number of hidden nodes and the hidden layers not the optimal choice but

94
00:06:18.460 --> 00:06:24.670
more a convenient choice is to have the average of the number of input nodes and the number of output

95
00:06:24.670 --> 00:06:25.250
node.

96
00:06:25.540 --> 00:06:31.360
And as you saw in Carol's intuition tutorial Well the number of input nodes in the input layer is the

97
00:06:31.360 --> 00:06:32.900
number of independent variables.

98
00:06:33.040 --> 00:06:38.410
So the number of input nodes in the input layer is 10 because we have 10 independent variables and the

99
00:06:38.410 --> 00:06:41.770
number of output nodes in the output layer is one.

100
00:06:41.770 --> 00:06:47.030
Because again you saw on Carol's intuition tutorial that's when our depende viable has a binary outcome.

101
00:06:47.170 --> 00:06:49.910
While there is only one output node in the output layer.

102
00:06:50.040 --> 00:06:56.140
So that means that the number of neurons will choose in the hidden layer is going to be 10 Plus 1 divided

103
00:06:56.140 --> 00:06:58.630
by two that is five point five.

104
00:06:58.660 --> 00:07:00.670
But of course we need to take a round number.

105
00:07:00.850 --> 00:07:04.320
So we'll go for 6 6 neurons in the hidden layer.

106
00:07:04.540 --> 00:07:10.060
And so now to be honest we are not working with some complex data set like for example image just to

107
00:07:10.060 --> 00:07:11.950
find some patterns in the pixels.

108
00:07:11.950 --> 00:07:17.590
Here we have a simple data set with some independent variables and one in the Bible and there is no

109
00:07:17.590 --> 00:07:21.550
spatial structure and this dataset like it is the case for images.

110
00:07:21.550 --> 00:07:24.160
So to be honest we don't need many hidden layers.

111
00:07:24.160 --> 00:07:29.430
In fact I'm pretty sure our ancient model would work very well with only one hidden layer.

112
00:07:29.890 --> 00:07:35.520
But hey since we are deep learning part let's go for two hidden layers that will not be this kind of

113
00:07:35.520 --> 00:07:38.220
deeper any model but that will be a start.

114
00:07:38.220 --> 00:07:42.090
All right so let's end put these layers with the neurons inside.

115
00:07:42.300 --> 00:07:46.220
So I'm going to add the hidden argument.

116
00:07:46.530 --> 00:07:47.290
Here we go.

117
00:07:47.400 --> 00:07:53.070
And so as you understood we need to specify these numbers of layers and these numbers of neurons in

118
00:07:53.070 --> 00:07:55.220
the layers with a vector.

119
00:07:55.410 --> 00:08:01.180
So see parenthesis then the first element of this vector is the number of neurons in the first layer.

120
00:08:01.290 --> 00:08:07.290
So we said 6 then come up and then the second element of this vector is the number of neurons in the

121
00:08:07.290 --> 00:08:08.530
second hidden layer.

122
00:08:08.560 --> 00:08:10.970
And so let's pick six as well.

123
00:08:10.980 --> 00:08:16.650
Keep in mind that in part We will be able to improve these choices of parameters things to parameter

124
00:08:16.650 --> 00:08:19.610
chinning techniques like K4 cross-validation.

125
00:08:19.830 --> 00:08:22.530
But for now let's just focus on deep learning.

126
00:08:22.860 --> 00:08:23.410
OK good.

127
00:08:23.420 --> 00:08:26.650
So that's it for this hidden argument.

128
00:08:26.730 --> 00:08:33.420
And so now let's move onto the next argument the next argument is going to be the one just below hidden

129
00:08:33.760 --> 00:08:35.240
that is epochs.

130
00:08:35.260 --> 00:08:40.110
So that is of course the number of epochs in the stochastic great in descent algorithm.

131
00:08:40.200 --> 00:08:46.290
We actually have a good description here the number of books is how many times the dataset should be

132
00:08:46.290 --> 00:08:47.020
iterated.

133
00:08:47.220 --> 00:08:52.710
If we go back to our intuition slide about this to get a great and decent algorithm well we can see

134
00:08:52.710 --> 00:08:58.230
this number of epochs here and step 7 when the whole training set completed all the steps from 1 to

135
00:08:58.230 --> 00:09:04.260
6 passing through the area and then well that makes it back and to many more epochs.

136
00:09:04.470 --> 00:09:10.560
So this number Ebix here is this number of times we are repeating the whole process of steps 1 to 6

137
00:09:10.800 --> 00:09:16.410
for the whole training set either by updating the weights after each observation or by adding the weights

138
00:09:16.410 --> 00:09:18.270
after each batch of observations.

139
00:09:18.300 --> 00:09:19.940
For example 10 observations.

140
00:09:20.040 --> 00:09:23.230
So let's add this epochs argument here.

141
00:09:23.220 --> 00:09:24.050
Here we go.

142
00:09:24.180 --> 00:09:28.820
And as in Python we're going to put 100 bucks.

143
00:09:28.840 --> 00:09:29.490
Great.

144
00:09:29.700 --> 00:09:36.240
And now we have one final argument input that is actually the one just below again train samples per

145
00:09:36.240 --> 00:09:37.230
iteration.

146
00:09:37.230 --> 00:09:38.250
So what is that.

147
00:09:38.250 --> 00:09:39.930
You might have guessed what it is.

148
00:09:39.930 --> 00:09:45.060
Well first the description says that it's the number of turning samples per map reduce iteration but

149
00:09:45.060 --> 00:09:48.080
more simply that is your batch size.

150
00:09:48.270 --> 00:09:51.970
That is the number of observations after which you want to update the weights.

151
00:09:51.990 --> 00:09:56.670
So this number can be one if you want to update the weights after each observation passing through the

152
00:09:56.670 --> 00:10:02.460
area and then and in that case that's called reinforcement learning exactly as we saw in part 6 reinforcement

153
00:10:02.460 --> 00:10:07.710
learning or this number can be more than one that is for example when the weights are dead after each

154
00:10:07.710 --> 00:10:13.730
batch of 10 observations passing through the area and then and in that case that is called Bache learning.

155
00:10:14.050 --> 00:10:16.380
OK so let's end with this argument.

156
00:10:16.380 --> 00:10:18.650
Train symbols per iteration.

157
00:10:18.650 --> 00:10:20.280
Here it is pressing enter.

158
00:10:20.520 --> 00:10:26.820
And now the good thing is that we don't even need to choose a batch size because as you can see we have

159
00:10:27.210 --> 00:10:31.530
these three parameters zero minus one and minus two.

160
00:10:31.560 --> 00:10:38.220
And as you can see this minus two value is very practical because by specifying minus two here this

161
00:10:38.220 --> 00:10:45.270
will all Choon your artificial neural network and that corresponds to the third reason why age 2 was

162
00:10:45.270 --> 00:10:50.790
to me one of the best packages it's because it gives you the option to already apply some parameter

163
00:10:50.790 --> 00:10:53.780
tuning parameter and it can be way more complicated.

164
00:10:53.880 --> 00:10:58.420
But here we have this great tool that will already help us to improve the model.

165
00:10:58.770 --> 00:11:07.770
So let's definitely put minus two here and actually now artificial neural network is ready that all

166
00:11:07.770 --> 00:11:11.830
the parameters we need to build our artificial neural network.

167
00:11:12.000 --> 00:11:14.060
And we even have some parameter tuning.

168
00:11:14.130 --> 00:11:18.350
So we have everything we need and even more than we need therefore let's do it.

169
00:11:18.420 --> 00:11:25.260
I'm going to select all these lines here that will create the classifier and now I just want to say

170
00:11:25.260 --> 00:11:26.310
something important.

171
00:11:26.400 --> 00:11:31.790
For those of you who follow the Python tutorials Well you saw that training and then took quite a while.

172
00:11:31.800 --> 00:11:37.630
It took about 1 minute if I remember correctly and that was because we used our CPQ on our system.

173
00:11:37.830 --> 00:11:39.860
But here we are connected to a server.

174
00:11:40.020 --> 00:11:43.790
And so we have access to a powerful system using this H-2 instance.

175
00:11:43.950 --> 00:11:48.850
And therefore you will see how the model is going to be trained much faster.

176
00:11:48.840 --> 00:11:50.130
We're going to see that right now.

177
00:11:50.220 --> 00:11:51.570
I'm going to execute.

178
00:11:51.660 --> 00:11:52.200
Ready.

179
00:11:52.200 --> 00:11:54.730
Three two one go.

180
00:11:57.670 --> 00:12:00.020
And that's it that's already ready.

181
00:12:00.020 --> 00:12:02.400
It took about five seconds.

182
00:12:02.540 --> 00:12:05.660
So that's pretty exciting to work with such powerful tools.

183
00:12:05.840 --> 00:12:11.240
I was very happy to show you this and I hope that now you're convinced that with this age to back it

184
00:12:11.420 --> 00:12:12.860
you are in good hands.

185
00:12:13.010 --> 00:12:13.850
So great.

186
00:12:13.850 --> 00:12:15.590
And now that our model is trained.

187
00:12:15.590 --> 00:12:16.780
Time for the next step.

188
00:12:16.850 --> 00:12:24.110
Making predictions on the test and that will be very interesting to do to see the accuracy of new observations

189
00:12:24.230 --> 00:12:25.870
on which the model wasn't trained.

190
00:12:26.030 --> 00:12:27.830
So we'll see that in the next tutorial.

191
00:12:27.860 --> 00:12:29.630
And until then and Gerrish learning.
