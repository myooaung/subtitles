1

00:00:00,270  -->  00:00:02,390
Hello and welcome to this art.

2

00:00:02,640  -->  00:00:06,140
So today we're going to implement our second non linear regression model.

3

00:00:06,210  -->  00:00:09,720
It's going to be that as we our support vector for regression.

4

00:00:09,720  -->  00:00:10,640
So let's do it.

5

00:00:10,650  -->  00:00:15,710
We're going to use our regression template and you're going to see how it's going to be so easy.

6

00:00:15,750  -->  00:00:17,290
So let's start by doing the basics.

7

00:00:17,310  -->  00:00:23,280
Set the right folder as working directory so we'll go to machine running a Z or two regression and then

8

00:00:23,370  -->  00:00:25,620
suport vector regression.

9

00:00:25,920  -->  00:00:26,220
Right.

10

00:00:26,220  -->  00:00:28,830
That's the rifle third position sorry we.

11

00:00:28,920  -->  00:00:29,630
All good.

12

00:00:29,820  -->  00:00:34,460
Let's click on set as a working directory to set the folder as a working directory.

13

00:00:34,460  -->  00:00:34,920
Great.

14

00:00:35,040  -->  00:00:36,190
And now we can start.

15

00:00:36,420  -->  00:00:42,690
So we're going to take our regression template and we're going to take everything from here to here

16

00:00:42,690  -->  00:00:43,140
.

17

00:00:43,200  -->  00:00:47,000
Copy and paste it in our SVR model.

18

00:00:47,310  -->  00:00:48,080
Here we go.

19

00:00:48,180  -->  00:00:50,160
And now we just need to change a few things.

20

00:00:50,220  -->  00:00:53,270
So let's start by changing the basics.

21

00:00:53,350  -->  00:00:56,270
We're going to replace regression model here by as we are.

22

00:00:56,730  -->  00:00:57,690
Okay.

23

00:00:58,230  -->  00:01:02,780
And same here as your results.

24

00:01:03,100  -->  00:01:04,010
Okay.

25

00:01:04,110  -->  00:01:05,940
And same here.

26

00:01:05,940  -->  00:01:06,960
All right.

27

00:01:07,290  -->  00:01:08,470
That was the easy step.

28

00:01:08,580  -->  00:01:10,890
And now let's get to the interesting step.

29

00:01:10,890  -->  00:01:16,230
So this interesting step is of course to create are as we are the aggressor and we are creating it here

30

00:01:16,250  -->  00:01:16,340
.

31

00:01:16,440  -->  00:01:18,330
So I am going to remove this.

32

00:01:18,500  -->  00:01:20,580
And now let's create this regressors.

33

00:01:21,150  -->  00:01:23,490
It's going to take three or four lines as usual.

34

00:01:23,490  -->  00:01:29,030
It's going to be very simple we're going to use a function which is the SVM function very simply because

35

00:01:29,150  -->  00:01:34,770
as we are you know is a support vector machine algorithm but for regression That's why we call it as

36

00:01:34,770  -->  00:01:35,460
you are.

37

00:01:35,760  -->  00:01:41,030
And therefore we are just taking it from the SBM function and you will understand why perfectly.

38

00:01:41,490  -->  00:01:47,610
So first let's import the required package because this function is contained in a package which is

39

00:01:47,610  -->  00:01:49,160
the intent 71 package.

40

00:01:49,160  -->  00:01:53,870
So let's go to our packages here and let's check if we have it.

41

00:01:54,300  -->  00:01:57,860
So I have it because of course I used it before but you might not have it.

42

00:01:57,850  -->  00:02:05,550
So in case you don't have it I'm going to type this line here that you will need to execute to install

43

00:02:05,640  -->  00:02:06,550
this package.

44

00:02:06,630  -->  00:02:12,540
So you install that package here and then in the parenthesis and then the quotes you type the name of

45

00:02:12,540  -->  00:02:16,010
the package which is 10 71.

46

00:02:16,110  -->  00:02:16,560
All right.

47

00:02:16,560  -->  00:02:19,480
And then you just select this line and execute.

48

00:02:19,710  -->  00:02:21,230
And this will install the package.

49

00:02:21,330  -->  00:02:26,730
Not going to do it because mine is already installed and therefore I'm going to put that in comment

50

00:02:26,940  -->  00:02:29,580
just pressed command shift plus seat.

51

00:02:29,820  -->  00:02:30,620
All right.

52

00:02:30,740  -->  00:02:39,720
And now let's actually also add this line library and in parenthesis Eatons 71 not in quotes.

53

00:02:39,960  -->  00:02:46,440
And this will select automatically the package here intensively one because this will not always be

54

00:02:46,440  -->  00:02:47,100
selected.

55

00:02:47,310  -->  00:02:49,980
But by including that in your script you will be fine.

56

00:02:49,980  -->  00:02:51,520
This will always be selected.

57

00:02:51,720  -->  00:02:52,290
OK.

58

00:02:52,500  -->  00:02:56,360
And now let's start creating our SVR aggressor.

59

00:02:56,430  -->  00:03:04,140
So as usual we are going to start by defining our aggressor regressors and then you equals and then

60

00:03:04,170  -->  00:03:09,170
as I mentioned before we're going to use the SVM function which is actually this.

61

00:03:09,330  -->  00:03:10,320
And then in parenthesis.

62

00:03:10,330  -->  00:03:17,080
Now let's press here for one to have a look at the arguments and see what we need to input.

63

00:03:17,100  -->  00:03:19,040
So the first argument is formula.

64

00:03:19,170  -->  00:03:21,110
So you start to know perfectly what it is.

65

00:03:21,110  -->  00:03:30,170
Of course the formula is our dependent variable which is remember salary and then tell the.

66

00:03:30,300  -->  00:03:36,480
I just pressed out and and then it does here and that specifies that we are taking all the independent

67

00:03:36,480  -->  00:03:39,530
variable of our dataset.

68

00:03:39,810  -->  00:03:43,570
We actually have one independent variable which is the level independent variable.

69

00:03:43,620  -->  00:03:48,990
So we could also type here level but most of the time we used the dot because of course we have a lot

70

00:03:48,990  -->  00:03:50,910
more than one independent viable.

71

00:03:51,300  -->  00:03:56,310
But as a reminder I'm taking one of the variable here so that we can clearly see the visual graphic

72

00:03:56,310  -->  00:04:00,170
results of the different non-linear models that we're building.

73

00:04:00,560  -->  00:04:03,160
OK so now let's add the second argument.

74

00:04:03,420  -->  00:04:06,600
So I'm adding a comma here and then next argument.

75

00:04:06,600  -->  00:04:08,810
So the next argument is data.

76

00:04:09,070  -->  00:04:13,530
Ok I guess you know what this data argument will be.

77

00:04:13,530  -->  00:04:20,940
It's going to be of course our dataset dataset here because we did not create any training set or test

78

00:04:20,940  -->  00:04:23,880
sets you know before we used the training set.

79

00:04:23,880  -->  00:04:24,900
Here is the data.

80

00:04:25,080  -->  00:04:29,550
But here we don't have any training set so of course we're going to take the whole data set because

81

00:04:29,550  -->  00:04:31,640
we want to make very accurate predictions.

82

00:04:31,860  -->  00:04:32,320
OK.

83

00:04:32,490  -->  00:04:35,080
And now finally most important arguments.

84

00:04:35,100  -->  00:04:41,310
Well all the arguments are important but the most relevant for this section about as we are is actually

85

00:04:41,310  -->  00:04:43,490
the next arguments we are about inputs.

86

00:04:43,530  -->  00:04:48,440
So this argument is actually not X or Y these are optional arguments.

87

00:04:48,450  -->  00:04:55,860
The most important argument I was just talking about is this one the type because this argument type

88

00:04:55,860  -->  00:05:02,570
will actually specify if you're making an ass VM model which is used for classification or as we are

89

00:05:02,580  -->  00:05:04,960
model which is used for regression.

90

00:05:04,980  -->  00:05:11,250
So here since we're building a non linear regression model we will choose the EPA s regression type

91

00:05:11,990  -->  00:05:16,610
we could use actually regression or new regression as you can see you want to take the most common one

92

00:05:16,610  -->  00:05:16,700
.

93

00:05:16,760  -->  00:05:18,170
The EPA has regression.

94

00:05:18,320  -->  00:05:24,950
And if we were making as a model for classification then we would have chosen C classification here

95

00:05:24,950  -->  00:05:29,250
as you can see sophistication is the default type for classification.

96

00:05:29,540  -->  00:05:34,820
So in short if you want to do regression you choose time equals regression.

97

00:05:34,990  -->  00:05:39,860
And if you want to do classification you choose type you cause C classification and actually and the

98

00:05:39,860  -->  00:05:43,180
next part about classification will make an SVM model.

99

00:05:43,220  -->  00:05:47,290
And for this specific model we will choose the C classification type.

100

00:05:47,840  -->  00:05:51,540
Ok but here we are doing regression so we choose the regression type.

101

00:05:51,540  -->  00:05:59,150
So let's put it into type here type equals EPEAT as regression.

102

00:05:59,450  -->  00:05:59,700
Right.

103

00:05:59,720  -->  00:06:01,410
And you need to put that in quotes.

104

00:06:01,460  -->  00:06:07,910
Actually don't forget the quotes to prevent any error from happening again.

105

00:06:07,910  -->  00:06:13,820
That's actually the final arguments if you want we can add the kernel argument here just to specify

106

00:06:13,820  -->  00:06:19,490
that we want a gaussian kernel because here we're using Galchen kernel but it's the kernel selected

107

00:06:19,490  -->  00:06:25,820
by default here so we don't actually need to put it but just keep in mind that we are here using the

108

00:06:26,060  -->  00:06:28,290
Gulshan kernel far as we are model.

109

00:06:28,580  -->  00:06:33,520
Ok but here we're all fine and actually our model is ready to be created.

110

00:06:33,770  -->  00:06:40,880
So actually this is all we had to change in this regression template and now we are ready to execute

111

00:06:40,880  -->  00:06:46,430
the sections one by one to create our model and find out about the final result and the final verdict

112

00:06:46,820  -->  00:06:48,500
whether it's truth or bluff.

113

00:06:48,530  -->  00:06:50,470
So let's do it.

114

00:06:50,780  -->  00:06:53,780
I'm going to import the data set first.

115

00:06:53,810  -->  00:06:54,680
All right here we go.

116

00:06:54,820  -->  00:06:56,470
Well-reported we can have a look.

117

00:06:56,480  -->  00:07:01,520
So these are days that with the levels as the independent variable and the salaries as the dependent

118

00:07:01,520  -->  00:07:08,710
variable get great and no need to split the deficit into the training set and said no need for future

119

00:07:08,740  -->  00:07:13,490
scanning because we're using the very popular Eatons of any one library that includes it.

120

00:07:13,550  -->  00:07:15,670
And now let's create our model.

121

00:07:15,710  -->  00:07:22,160
So I'm going to select this whole section here for us command control center to the truth done regrets

122

00:07:22,160  -->  00:07:23,900
are created great.

123

00:07:24,080  -->  00:07:28,610
And now let's break down your results would you like to start by visualizing that as we are results

124

00:07:28,640  -->  00:07:34,610
first or predicting the new result well let's actually predict the result because this is the most exciting

125

00:07:34,610  -->  00:07:35,160
step.

126

00:07:35,300  -->  00:07:36,830
And let's keep the best for the end.

127

00:07:37,130  -->  00:07:41,780
But this step is actually very exciting also because we're getting the final prediction we are getting

128

00:07:41,780  -->  00:07:43,540
the final predicted salaries.

129

00:07:43,550  -->  00:07:48,110
So you know it says you won't if you're doing actually this code by yourself before me.

130

00:07:48,110  -->  00:07:50,030
That's actually a very good exercise.

131

00:07:50,060  -->  00:07:51,800
I really encourage you to do this.

132

00:07:51,830  -->  00:07:55,570
Choose whatever you want to execute first it's ready anyway.

133

00:07:55,580  -->  00:07:57,490
You don't need to change anything more now.

134

00:07:57,620  -->  00:08:00,020
So I'm going to execute that right now.

135

00:08:00,380  -->  00:08:06,180
And actually we obtain a predicted salary of a hundred and seventy seven thousand dollars.

136

00:08:06,230  -->  00:08:07,010
Great.

137

00:08:07,310  -->  00:08:12,650
So remember this employee who we are negotiating the future salary right now said that it's pretty salary

138

00:08:12,650  -->  00:08:14,130
was 160 Okay.

139

00:08:14,330  -->  00:08:19,200
And our model predicted that it's pretty salary with a hundred and seventy seven.

140

00:08:19,190  -->  00:08:19,580
OK.

141

00:08:19,790  -->  00:08:23,490
So first of all that's actually close to what this employee said.

142

00:08:23,570  -->  00:08:26,410
And besides it's on the good side of the negotiation.

143

00:08:26,480  -->  00:08:27,850
So that's actually pretty good.

144

00:08:27,950  -->  00:08:31,570
And then we can be satisfied with this result and our model.

145

00:08:31,580  -->  00:08:38,060
But to be really satisfied Let's see what's happening with the graphic results so I'm going to execute

146

00:08:38,060  -->  00:08:44,990
the section and let's have a look at the SVR model and here it is.

147

00:08:45,230  -->  00:08:47,990
I'm going to zoom on it and better.

148

00:08:48,140  -->  00:08:48,810
Okay.

149

00:08:49,040  -->  00:08:51,750
So first of all this model fits very well.

150

00:08:51,770  -->  00:08:57,380
Most of the data points as a reminder the real observation points other red points here and all the

151

00:08:57,380  -->  00:09:02,670
points in this blue curve here which is the as your model itself our prediction point.

152

00:09:02,660  -->  00:09:08,600
So for example if we're taking this red observation point here well the prediction is perfect because

153

00:09:08,660  -->  00:09:12,530
the prediction point is the projection of this red point on the blue curve.

154

00:09:12,650  -->  00:09:17,690
So it's actually the repoint itself because the red point is on the blue curve so that makes a perfect

155

00:09:17,690  -->  00:09:22,790
prediction but here for example if we take a less accurate prediction but still a very good one then

156

00:09:22,790  -->  00:09:28,580
we take the real observation point here the red point we projected on the blue curve and that's the

157

00:09:28,580  -->  00:09:33,150
difference between the real salary and the predicted sorry which is here.

158

00:09:33,160  -->  00:09:38,780
If you project it back on the y axis which contains the salaries but you can see that for all of these

159

00:09:38,780  -->  00:09:44,750
points from this one to actually this one the blue curve is actually getting very close to the real

160

00:09:44,750  -->  00:09:46,780
observation points the red points.

161

00:09:46,820  -->  00:09:53,390
And so the predictions are very close to real results but that's for all these points of the data set

162

00:09:53,510  -->  00:09:55,330
except for this one here.

163

00:09:55,370  -->  00:09:59,400
This one is left alone which by the way corresponds to see.

164

00:09:59,420  -->  00:10:05,690
So I'm sorry for the CEO but the reason for what is happening here is that this is actually an outlier

165

00:10:05,920  -->  00:10:11,990
not to call this year not liar but this is an outlier because as you can see this point is actually

166

00:10:11,990  -->  00:10:15,380
far from the other points in terms of salaries.

167

00:10:15,410  -->  00:10:19,810
The CEO has a much higher salary than the previous positions.

168

00:10:19,820  -->  00:10:24,040
So for that as your model that's an outlier and therefore he actually didn't consider it.

169

00:10:24,050  -->  00:10:30,440
It's like it excludes at this point no moral by not looking at it and making its predictions on these

170

00:10:30,440  -->  00:10:31,250
points here.

171

00:10:31,550  -->  00:10:37,340
So that specifically due to the your algorithm itself but there are a lot of parameters and you can

172

00:10:37,340  -->  00:10:43,200
actually play with the parameters to change the way as your model perceives outliers.

173

00:10:43,310  -->  00:10:49,070
So for example these parameters are the penalty parameters the regularization parameters it's most of

174

00:10:49,070  -->  00:10:54,780
the time well described in the description and of course Eaton 71 library includes such techniques.

175

00:10:55,340  -->  00:11:00,710
But we're not going to do it here in this tutorial because we actually don't need to get a good prediction

176

00:11:00,710  -->  00:11:02,200
of the CEO's salary.

177

00:11:02,210  -->  00:11:06,740
And remember what we actually needed was a good prediction of the previous salary of this employee we

178

00:11:06,730  -->  00:11:10,440
were negotiating with and this employee has a level 6.5.

179

00:11:10,610  -->  00:11:15,830
And around this point we can see that the model fits very well to our dataset and we actually get a

180

00:11:15,830  -->  00:11:23,510
prediction of 177 K quite close to the real or the mentioned salary of this employee which is a 160

181

00:11:23,510  -->  00:11:23,930
k.

182

00:11:24,200  -->  00:11:25,400
So that's actually pretty good.

183

00:11:25,460  -->  00:11:32,520
And therefore the verdict of truth of bluff according to our your model is rather truth.

184

00:11:32,750  -->  00:11:35,890
OK so not bad as you are made a good job here.

185

00:11:35,900  -->  00:11:40,610
Now let's see how decision tree regression and run them for us regression will do we'll see that in

186

00:11:40,610  -->  00:11:41,600
the next sections.

187

00:11:41,690  -->  00:11:42,520
So I'll see you there.

188

00:11:42,560  -->  00:11:44,210
And until then enjoy machine learning
