WEBVTT
1

00:00:00.420  -->  00:00:06.750
Hello and welcome to this art Tauriel today we're going to implement the K nearest neighbors classifier

2

00:00:06.830  -->  00:00:07.520
on our.

3

00:00:07.710  -->  00:00:09.300
So let's start right now.

4

00:00:09.300  -->  00:00:11.790
The first thing we're going to do is to set to work directory.

5

00:00:11.970  -->  00:00:17.490
So right now I'm on my desktop we're going to go to machine learning a folder Part 3 classification

6

00:00:17.790  -->  00:00:20.460
and then S.K. nearest neighbors.

7

00:00:20.460  -->  00:00:24.660
All right that's the full Do we want to set as working directory make sure that you have this social

8

00:00:24.660  -->  00:00:26.400
network and see as you file.

9

00:00:26.610  -->  00:00:31.290
And if that's the case you're ready to click on this more button here to set the folder as a working

10

00:00:31.290  -->  00:00:32.520
directory.

11

00:00:32.520  -->  00:00:33.060
Here we go.

12

00:00:33.060  -->  00:00:33.780
That's done.

13

00:00:33.990  -->  00:00:40.890
And now we're going to use our classification template to be efficient and generate all the results

14

00:00:40.980  -->  00:00:41.990
in a flashlight.

15

00:00:42.300  -->  00:00:47.430
So I'm going to select everything from here to the top.

16

00:00:47.430  -->  00:00:52.640
There you go copy and paste it here.

17

00:00:52.830  -->  00:00:53.370
Right.

18

00:00:53.550  -->  00:00:55.930
And now we need to change a very few things.

19

00:00:55.980  -->  00:01:02.700
First we need to create the classify here and we just need to change the titles of our plots in the

20

00:01:02.700  -->  00:01:04.810
trainings that results in the test results.

21

00:01:04.810  -->  00:01:07.210
Let's do this right now so that we don't forget.

22

00:01:07.860  -->  00:01:09.490
I'm going to reply because you by.

23

00:01:09.570  -->  00:01:10.610
So it's OK.

24

00:01:10.920  -->  00:01:19.050
And so we're just doing this to specify the algorithm that we're planning and here as well classifier

25

00:01:19.350  -->  00:01:22.970
is replaced by K and then right.

26

00:01:23.010  -->  00:01:27.750
And now what we only need to do is to create our classifier.

27

00:01:28.050  -->  00:01:31.570
But first let's select and execute the preprocessing steps.

28

00:01:31.620  -->  00:01:38.710
So I just need to track everything from here and then press command control to plus that plus enter

29

00:01:38.730  -->  00:01:41.490
to execute that's done.

30

00:01:41.520  -->  00:01:43.860
All right so we have our data set.

31

00:01:43.860  -->  00:01:44.670
We can have a look.

32

00:01:44.730  -->  00:01:46.130
That's our data set.

33

00:01:46.350  -->  00:01:51.600
Our training set and our test set which are well scaled.

34

00:01:51.820  -->  00:01:52.210
Okay.

35

00:01:52.220  -->  00:01:59.250
So as a reminder the data set contains information about users in the Social Network The Social Network

36

00:01:59.250  -->  00:02:06.240
has a business client which is a car company that puts ads on the social network and the social network

37

00:02:06.240  -->  00:02:13.070
gathered not only eatings formations like the age and the estimated salary of those users but also to

38

00:02:13.080  -->  00:02:14.750
get the answer.

39

00:02:14.760  -->  00:02:23.550
The response to these users to the ad that is zero if the user didn't buy the product the car and one

40

00:02:23.640  -->  00:02:25.520
if the user bought the product.

41

00:02:25.530  -->  00:02:32.310
So you know it's a very cool luxury SUV launched at a ridiculously low price.

42

00:02:32.340  -->  00:02:38.070
So a lot of people when they saw the ad said let's do this let's get the car and we can see there a

43

00:02:38.070  -->  00:02:39.050
lot of buyers.

44

00:02:39.240  -->  00:02:42.730
So yeah as you can see it must be a very cool car and cheap.

45

00:02:42.900  -->  00:02:48.260
So now let's go back to KNM and creates our classifier.

46

00:02:48.390  -->  00:02:56.540
So the classifiers AK and then gasifier and to create this we are going to first import the right library

47

00:02:56.550  -->  00:03:00.150
we need a library for this which is by the way called class.

48

00:03:00.420  -->  00:03:03.540
So let's do this.

49

00:03:03.540  -->  00:03:09.960
So if by any chance the class library is not in your packages list you need to install it using this

50

00:03:09.960  -->  00:03:15.470
command install that packages and in parenthesis and in quotes the name of the class which is class

51

00:03:15.470  -->  00:03:15.870
.

52

00:03:16.200  -->  00:03:21.720
But I think you might have it by default so but if that's not the case please write this command with

53

00:03:21.720  -->  00:03:25.300
class inside then execute and it will install it.

54

00:03:25.330  -->  00:03:34.080
But so here we're just going to write a library and invented this class that selects automatically that

55

00:03:34.080  -->  00:03:34.410
class.

56

00:03:34.410  -->  00:03:38.070
You see that right now the class package is not selected.

57

00:03:38.340  -->  00:03:42.790
And once this is executed it will select it automatically.

58

00:03:42.800  -->  00:03:43.480
All right.

59

00:03:43.710  -->  00:03:52.470
And now now we're going to do something a little different because usually you know we create or classify

60

00:03:52.470  -->  00:03:58.500
here and then we create this vector of prediction y spread using this predict function that we apply

61

00:03:58.500  -->  00:04:02.240
on our classifier and our new observations in a test set.

62

00:04:02.280  -->  00:04:09.090
But here it's like we're going to do these two steps all at once because we are going to directly.

63

00:04:09.090  -->  00:04:20.510
We're actually going to remove this line and we are actually going to replace discomfiting K and the

64

00:04:20.520  -->  00:04:24.060
training sets and predicting

65

00:04:26.790  -->  00:04:35.850
the test set results because all of the ones we're going to fit into our training set and predicts if

66

00:04:35.850  -->  00:04:39.030
the users of our test set by yes or no.

67

00:04:39.080  -->  00:04:39.800
Yes.

68

00:04:40.020  -->  00:04:44.550
So you're going to understand why we're going to directly create our vector of prediction y create here

69

00:04:44.550  -->  00:04:45.530
.

70

00:04:45.570  -->  00:04:53.040
And then you know before we had to classify your equals then the as V.M. or logistic regression here

71

00:04:53.280  -->  00:04:55.830
it's going to be directly ype radicals.

72

00:04:55.920  -->  00:05:02.400
And because actually this k n n function here will return the predictions of the tests and observations

73

00:05:02.410  -->  00:05:02.860
.

74

00:05:03.300  -->  00:05:07.340
OK so now let's have a look at this K and then function and you'll get it.

75

00:05:07.650  -->  00:05:15.140
So press 1 ok so can your neighbor classification Let's first input the arguments.

76

00:05:15.270  -->  00:05:17.680
The first argument is train.

77

00:05:17.760  -->  00:05:21.560
So you can guess that by train they mean the training sets.

78

00:05:21.660  -->  00:05:25.470
So here we need to specify what the training set is.

79

00:05:25.540  -->  00:05:32.740
And actually since we call it trainset it's trains and so trains that here all right turn set.

80

00:05:33.000  -->  00:05:39.810
However the training set as you can see contains the independent variables and the dependent variable

81

00:05:39.810  -->  00:05:40.070
.

82

00:05:40.140  -->  00:05:43.230
And we only need to take the independent variables.

83

00:05:43.230  -->  00:05:47.490
We want to include the dependent variable in the Cain function here.

84

00:05:47.490  -->  00:05:49.930
So here we need to take only the first two columns.

85

00:05:50.010  -->  00:05:56.400
So we're going to add a bracket here and then come up because on the left of this come all the lines

86

00:05:56.400  -->  00:06:02.160
that I'm taking all the observations and not the right of this come up is the columns I want to include

87

00:06:02.280  -->  00:06:03.360
in the training set.

88

00:06:03.630  -->  00:06:06.050
So it's all the columns except for the last one.

89

00:06:06.150  -->  00:06:13.240
So I'm going to put here minus three which means I'm removing the last column of my training center

90

00:06:13.240  -->  00:06:13.990
.

91

00:06:14.000  -->  00:06:14.680
All right.

92

00:06:14.850  -->  00:06:18.030
So that's done that's what we had to import pre-trained here.

93

00:06:18.140  -->  00:06:24.840
So train is your training set but without the dependent variable then come and then let's add the second

94

00:06:24.840  -->  00:06:25.410
argument.

95

00:06:25.590  -->  00:06:27.450
So the second argument is test.

96

00:06:27.690  -->  00:06:33.690
So you can guess what it is it's going to be the same test equals test set of course.

97

00:06:33.710  -->  00:06:34.160
Right.

98

00:06:34.170  -->  00:06:40.260
Test sets and then same training said we're going to remove the dependent Roybal because way we are

99

00:06:40.260  -->  00:06:44.430
supposed not to know the results we want to predict the observations of the tests.

100

00:06:44.430  -->  00:06:45.940
So anyway we need to remove it.

101

00:06:46.200  -->  00:06:51.870
So come on to take all the lines and minus three to remove the last column.

102

00:06:52.560  -->  00:06:55.360
OK so we have our trending set and our test set.

103

00:06:55.440  -->  00:06:57.050
And now what is the next parameter.

104

00:06:57.090  -->  00:07:02.260
OK the next parameter is C L factor of true classifications of turning sets.

105

00:07:02.280  -->  00:07:05.340
So can you guess what it's going to be.

106

00:07:05.920  -->  00:07:06.750
OK let's see.

107

00:07:06.840  -->  00:07:10.710
C L equals in your opinion what is it going to be.

108

00:07:11.100  -->  00:07:17.900
Well you know to train a classifier the class really needs to have OK the independent variables but

109

00:07:17.910  -->  00:07:24.300
it also needs to have that dependent variable because it needs to have the results to you know find

110

00:07:24.300  -->  00:07:30.420
the correlations between the information of the independent variables and the information contained

111

00:07:30.420  -->  00:07:31.620
in the dependent variable.

112

00:07:31.710  -->  00:07:37.470
So here since we only have the info about the independent variables we also need to include somewhere

113

00:07:37.610  -->  00:07:40.800
the info of the dependent variable and that's what we had here.

114

00:07:40.800  -->  00:07:44.590
That's the C-L So factor of true classifications of trends.

115

00:07:44.640  -->  00:07:47.040
There is the categorical dependent variable.

116

00:07:47.310  -->  00:07:50.630
So let's do this so to take this vector actually.

117

00:07:50.630  -->  00:07:54.120
So as you can see that's the last column of the training set.

118

00:07:54.120  -->  00:08:00.400
So it's going to be trainset taking all the lines all the observations and then the one to three.

119

00:08:00.420  -->  00:08:03.920
So the third index of the column per chased.

120

00:08:03.930  -->  00:08:06.530
So let's take that trainset

121

00:08:10.230  -->  00:08:17.790
brackets come up and then three because the column we want is indexed by three.

122

00:08:17.790  -->  00:08:23.300
All right so that's the third argument and then we have one more argument which is the number of neighbors

123

00:08:23.300  -->  00:08:23.680
.

124

00:08:23.820  -->  00:08:25.340
So let's add this one.

125

00:08:25.380  -->  00:08:28.910
So remember in Bison We took five neighbors.

126

00:08:28.980  -->  00:08:30.510
That's actually the default parameter.

127

00:08:30.510  -->  00:08:35.810
So here we're going to take the same that will allow us to compare the results we obtained on Python

128

00:08:35.820  -->  00:08:36.510
and our.

129

00:08:36.720  -->  00:08:38.160
So it will be interesting.

130

00:08:38.340  -->  00:08:41.720
So let's say K equals 5 neighbors.

131

00:08:42.120  -->  00:08:42.570
All right.

132

00:08:42.580  -->  00:08:44.150
Now we have everything we need.

133

00:08:44.220  -->  00:08:46.210
We can select this.

134

00:08:46.410  -->  00:08:48.460
And here is why read all good.

135

00:08:48.690  -->  00:08:51.370
So now let's have a look at white bread.

136

00:08:51.900  -->  00:08:57.000
We can have a look here why KRAD and pressing why put it in the console and press enter to have a look

137

00:08:57.000  -->  00:08:57.460
at it.

138

00:08:57.750  -->  00:08:59.640
And here all the predictions for the test.

139

00:08:59.640  -->  00:09:02.740
So remember the test it contains 100 observations.

140

00:09:02.740  -->  00:09:10.140
Here we have a hundred predictions corresponding to the same observations as these guys here.

141

00:09:10.140  -->  00:09:14.970
So for example let's take the first observation to the first users.

142

00:09:14.970  -->  00:09:18.590
So let's take the one two three four five.

143

00:09:18.600  -->  00:09:26.400
So the five first users these five first users didn't buy this in reality because the per chaced variable

144

00:09:26.490  -->  00:09:28.380
equals zero here and that's the truth.

145

00:09:28.380  -->  00:09:30.960
That's what actually happens in reality.

146

00:09:30.960  -->  00:09:39.000
And what does our predictions say 1 2 3 4 5 5 0 here so correct predictions for the 5 for us users.

147

00:09:39.150  -->  00:09:40.250
Okay perfect.

148

00:09:40.440  -->  00:09:49.650
Then then we have 4 once the the 6 7 8 and 9 users actually but yes.

149

00:09:49.890  -->  00:09:55.600
So great for the 6 1 7 1 grades correct prediction 81 correct prediction as well.

150

00:09:55.620  -->  00:09:59.490
But the classify you made a little mistake here that's fine.

151

00:09:59.490  -->  00:10:04.120
It looks like it's making some correct prediction most of the time and we are going to check that on

152

00:10:04.140  -->  00:10:05.480
the confusion matrix.

153

00:10:05.490  -->  00:10:07.190
That will be faster.

154

00:10:07.540  -->  00:10:11.340
Was just to you know understand to explain what white print was.

155

00:10:11.340  -->  00:10:12.660
But I think you get it.

156

00:10:12.930  -->  00:10:20.980
So here we just it is like this and executes here C.M. we can have a look at it in the console see and

157

00:10:21.280  -->  00:10:23.130
that's the predictions.

158

00:10:23.140  -->  00:10:23.550
OK.

159

00:10:23.560  -->  00:10:28.940
So we have six plus five incorrect predictions that say 11 and predictions.

160

00:10:29.020  -->  00:10:30.610
So that's not too bad.

161

00:10:30.620  -->  00:10:36.940
And now what we are most interested to see is the prediction regions how they behave and especially

162

00:10:36.940  -->  00:10:41.930
the prediction boundary to see if it's going to be a straight line or something else.

163

00:10:41.980  -->  00:10:47.470
And actually you're going to see that the K and then is a non-linear classifiers So we will get something

164

00:10:47.470  -->  00:10:50.580
different than what we got for logistic regression.

165

00:10:50.620  -->  00:10:58.060
So I know it's supposed to be a template that work for every classifier but the Canon and classifier

166

00:10:58.210  -->  00:11:04.510
will be the only classify we will need to change something in the template besides the title I mean

167

00:11:04.510  -->  00:11:05.320
.

168

00:11:05.320  -->  00:11:11.230
And that's because of this structure we change here you know for all the classify as we had this first

169

00:11:12.730  -->  00:11:18.070
section where we create or classify there you know classify equals and then we create or classify with

170

00:11:18.070  -->  00:11:18.940
the function.

171

00:11:18.940  -->  00:11:25.810
And then in another section we create a widespread using the classifier we build in the section above

172

00:11:25.820  -->  00:11:26.100
.

173

00:11:26.350  -->  00:11:32.440
But here since we do that here since we did both at the same time we just need to change some little

174

00:11:32.440  -->  00:11:34.130
thing here in the template.

175

00:11:34.210  -->  00:11:35.020
So can you.

176

00:11:35.080  -->  00:11:36.350
Can you guess what it is.

177

00:11:36.580  -->  00:11:43.660
OK so it's it's actually in this slide because as you can see here we are using to predict function

178

00:11:44.020  -->  00:11:45.490
on our classifier.

179

00:11:45.570  -->  00:11:51.690
But here since we didn't create a classifier separately from the vector of prediction of white.

180

00:11:52.120  -->  00:11:55.900
Well that means that this product class very doesn't mean anything here.

181

00:11:55.900  -->  00:12:01.800
So actually it's really simple what we want to do is to replace this predictive classify a new data

182

00:12:02.320  -->  00:12:09.410
set by this code here because this is actually like to predict function needs creating the classifier

183

00:12:09.410  -->  00:12:13.350
and making the predictions at the same time so there is not to predict function here.

184

00:12:13.360  -->  00:12:18.730
So it's very simple where we only need to do is to take this Kamensky.

185

00:12:19.060  -->  00:12:24.760
And here you know since it's all the predictions of the big points in the grid we will need to change

186

00:12:24.760  -->  00:12:24.990
this.

187

00:12:25.000  -->  00:12:30.510
New data equals test set so this testicles set by grid set.

188

00:12:30.640  -->  00:12:37.640
So basically it will be very simple we will remove this predict function here and paste that.

189

00:12:37.660  -->  00:12:47.320
And here we're just going to replace test set by grid set because we want to predict if those imaginary

190

00:12:47.320  -->  00:12:54.430
pixel point that we picture as imaginary users of the social network will buy yes or no the SUV according

191

00:12:54.430  -->  00:12:59.170
to their coordinates and the graph that is their age and their estimated salary.

192

00:12:59.260  -->  00:13:04.690
And then once the prediction is done it will be colorized in the proper color green.

193

00:13:04.720  -->  00:13:10.480
If this imaginary user pixel point is predicted to buy the SUV and red if it's predicted not to buy

194

00:13:10.490  -->  00:13:10.580
.

195

00:13:11.380  -->  00:13:12.850
And that's it that's all we need.

196

00:13:12.850  -->  00:13:22.360
So we're going to select this as well and paste it here to do the same for the test.

197

00:13:22.360  -->  00:13:24.230
Of course it's the same.

198

00:13:25.390  -->  00:13:26.210
And here we go.

199

00:13:26.440  -->  00:13:29.360
And then we were all good to plot the results.

200

00:13:29.470  -->  00:13:34.620
So let's do this we're going to select this and let's see what we obtain.

201

00:13:34.660  -->  00:13:38.710
Command and control plus entity executes the graph.

202

00:13:38.710  -->  00:13:39.460
The plot is coming.

203

00:13:39.460  -->  00:13:46.150
You're going to see how the I can manage is to classify the two classes of users those who didn't buy

204

00:13:46.150  -->  00:13:47.620
the SUV and those who bought it.

205

00:13:47.640  -->  00:13:48.490
And here we go.

206

00:13:48.640  -->  00:13:52.000
That's a graph that's the prediction boundary here.

207

00:13:52.060  -->  00:13:57.460
So you can see there are a lot of irregularities but that's because of the way the K and model works

208

00:13:57.460  -->  00:14:03.780
because it's taking each time the five nearest neighbors that's all the prediction boundary.

209

00:14:03.790  -->  00:14:05.720
These are two prediction regions.

210

00:14:05.730  -->  00:14:11.090
The red one the green one of these points here the real observation points are real users of the data

211

00:14:11.100  -->  00:14:13.120
set and their real results for the day.

212

00:14:13.130  -->  00:14:17.850
But yes if it's green and no they don't buy that it's red.

213

00:14:18.490  -->  00:14:24.040
So we can see that most of our predictions are correct because all the red points are in the red region

214

00:14:24.040  -->  00:14:26.640
and most of the green points are in the green region.

215

00:14:26.860  -->  00:14:34.150
We also see that the Canan classify it makes a much better job at classifying those uses here that their

216

00:14:34.150  -->  00:14:40.300
logistic regression can classify correctly because it was a straight line and some green points were

217

00:14:40.300  -->  00:14:42.820
falling on the red region.

218

00:14:42.820  -->  00:14:45.410
So here is a much better job.

219

00:14:45.580  -->  00:14:50.950
And all those users with an age about the average and in Estimate salary below the average are well

220

00:14:50.950  -->  00:14:52.910
classified in a green region.

221

00:14:53.020  -->  00:14:53.680
So very good.

222

00:14:53.680  -->  00:14:54.480
Very good.

223

00:14:54.580  -->  00:15:00.400
But let's not scream victory until we find out about the results on the test set because since this

224

00:15:00.400  -->  00:15:04.810
is a training set and the cane and transfer was built on a training said we might get different results

225

00:15:04.810  -->  00:15:05.470
from that set.

226

00:15:05.470  -->  00:15:06.530
So let's see.

227

00:15:06.560  -->  00:15:13.790
We're going to select this the test set results are ready to execute.

228

00:15:15.220  -->  00:15:16.550
And these are great results.

229

00:15:16.570  -->  00:15:21.310
Most of the red points are in the red region most of the green points are in the green region and of

230

00:15:21.310  -->  00:15:25.820
course we had a few incorrect predictions like these two guys here.

231

00:15:25.900  -->  00:15:27.420
These two guys here.

232

00:15:27.640  -->  00:15:29.000
Here it looks very good.

233

00:15:29.290  -->  00:15:33.050
And we have made nice prediction regions.

234

00:15:33.060  -->  00:15:33.450
OK.

235

00:15:33.450  -->  00:15:35.550
So that's it for this tutorial.

236

00:15:35.550  -->  00:15:39.110
I hope you enjoy discovering this new type of class very.

237

00:15:39.220  -->  00:15:43.560
We are going to discover new kinds of classifiers in the next sections.

238

00:15:43.570  -->  00:15:45.120
I can't wait to show you this.

239

00:15:45.130  -->  00:15:47.040
And until then enjoy machine learning.
