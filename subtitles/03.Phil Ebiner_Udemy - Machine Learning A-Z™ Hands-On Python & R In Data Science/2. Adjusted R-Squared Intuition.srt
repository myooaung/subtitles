1

00:00:00,570  -->  00:00:03,560
All right so this is the fun part adjusted r squared.

2

00:00:03,840  -->  00:00:07,120
So here we've got a simple inner.

3

00:00:07,290  -->  00:00:12,330
We've talked about how the R-Squared is derived so what the formula for our secret is it's equal to

4

00:00:12,330  -->  00:00:19,170
one minus sum of squares of residuals divided by total sum of squares where sum of squares of residuals

5

00:00:19,710  -->  00:00:28,470
is the sum of squares of the distances from the observations to your fitted line so we don't see it

6

00:00:28,500  -->  00:00:35,880
here that's your linear symbol in your regression line that we fit onto our data and the sum of the

7

00:00:35,890  -->  00:00:42,600
whole sums whereas we can see it here so that's the total sum of those squares of those red lines we

8

00:00:42,600  -->  00:00:47,640
can see here so the difference between your observations and the average Sobieski R-Squared is telling

9

00:00:47,640  -->  00:00:50,220
us how well is your model fitted to the daughter.

10

00:00:50,390  -->  00:00:55,620
In fact how much better is it fitted than the average line that she can just draw through her daughter

11

00:00:55,860  -->  00:00:57,180
very easily.

12

00:00:57,180  -->  00:01:02,620
So let's get rid of all of that and put all our secret formula no left for a second.

13

00:01:02,630  -->  00:01:10,170
Now we talked about R-squared for a simple linear regression while the same concepts apply for a multiple

14

00:01:10,170  -->  00:01:10,930
in your regression.

15

00:01:10,920  -->  00:01:16,590
So for instance we have two variables R-squared would be calculated in the same way.

16

00:01:16,590  -->  00:01:22,920
We're just not going to go through all these derivations right now because it's just easier to visualize

17

00:01:22,920  -->  00:01:30,120
the way we've done it and that's exactly the same concepts apply to a multiple linear regression and

18

00:01:30,120  -->  00:01:31,570
still.

19

00:01:31,980  -->  00:01:36,080
So what that means is that all ordinary least squares method is used.

20

00:01:36,090  -->  00:01:43,800
And the best fitting is a multiple linear regression is the one that has the least sum of squares of

21

00:01:43,940  -->  00:01:47,290
residuals So you're trying to minimize the sum of squares of residual.

22

00:01:47,290  -->  00:01:50,100
And when I bring that up well we'll need that in a second.

23

00:01:50,310  -->  00:01:56,130
So we want to use R-squared as we discussed as a goodness of fit parameters so the bigger it is the

24

00:01:56,130  -->  00:01:59,070
better the current closer it is to one the better model.

25

00:01:59,140  -->  00:01:59,930
And that's awesome.

26

00:01:59,940  -->  00:02:06,630
We can totally try doing that but there is one problem and the problem is or starts to occur when you

27

00:02:06,690  -->  00:02:09,780
add more variables to your model.

28

00:02:09,780  -->  00:02:13,410
So here you can see we've got two variables in our multiple integration.

29

00:02:13,560  -->  00:02:17,350
What will happen if we add a third variable to our model.

30

00:02:17,440  -->  00:02:22,470
We think of it as an example let's look at that example with a salary.

31

00:02:22,470  -->  00:02:30,990
So your salary equals to a constant 30000 plus 10000 times the years of experience plus for instance

32

00:02:31,020  -->  00:02:38,460
B2 could be a constant hour coefficient times the number of qualifications you have.

33

00:02:38,550  -->  00:02:46,260
And I don't know B-3 could be how much how much money you brought the company in the previous year so

34

00:02:46,350  -->  00:02:51,600
if you're a salesman for instance so a coefficient times the amount of dollars that you brought the

35

00:02:51,600  -->  00:02:53,910
company in the previous year something like that.

36

00:02:53,970  -->  00:03:01,410
So you can keep adding on variables that you think are actually impacting the outcome so are impacting

37

00:03:01,410  -->  00:03:07,380
the dependent variable and you want to fit in model and see if it's better or not.

38

00:03:07,380  -->  00:03:09,270
So in this case we're adding a third variable.

39

00:03:09,270  -->  00:03:11,880
We already have a model with two variables and it works okay.

40

00:03:12,010  -->  00:03:17,580
And now we want to add a third variable and see if maybe using a third variable we can fit our model

41

00:03:17,610  -->  00:03:21,250
even better so we can create an even better model with three variables.

42

00:03:21,300  -->  00:03:22,730
So how can we judge that.

43

00:03:22,740  -->  00:03:25,450
So we've added all bearable and we've run the model.

44

00:03:25,500  -->  00:03:28,400
We can look at the R-squared did the R-squared get better or worse.

45

00:03:28,410  -->  00:03:34,760
So in fact did the R-squared increase or did it decrease or stay the same.

46

00:03:34,770  -->  00:03:41,060
So the problem here is that R-squared because of these two things are spread will never decrease.

47

00:03:41,100  -->  00:03:43,410
And let's go through that in a in a bit more detail.

48

00:03:43,450  -->  00:03:47,460
I'm going to use my mouse here because this is quite an important part.

49

00:03:47,460  -->  00:03:54,180
So R-squared over here is equal to one minus somewhat squared of residual divide by some square of the

50

00:03:54,180  -->  00:03:55,250
total.

51

00:03:55,800  -->  00:04:00,570
So once you add a new variable to your model.

52

00:04:00,690  -->  00:04:06,450
Right it's going to somehow affect what the model looks like.

53

00:04:06,450  -->  00:04:15,090
And the fact that we're trying to minimize the sum of squares of residual what that means is that either

54

00:04:15,090  -->  00:04:21,090
this new variable will help minimize the somewhat square of residual somehow the regression process

55

00:04:21,090  -->  00:04:27,930
will find a way to give it a coefficient that will help minimize the sum of square residuals.

56

00:04:27,930  -->  00:04:32,920
And then and in that case R-squared will happen to us squared.

57

00:04:33,180  -->  00:04:37,310
It'll be one minus something that is less than it used to be.

58

00:04:37,410  -->  00:04:38,110
Right.

59

00:04:38,280  -->  00:04:44,550
Divided by the same value because we by adding a new variable we're not affecting the observations we're

60

00:04:44,550  -->  00:04:47,190
not affecting the averages of observations right.

61

00:04:47,190  -->  00:04:48,580
This does not change.

62

00:04:48,690  -->  00:04:55,410
So by adding new variable the regression process through this condition we'll definitely try to minimize

63

00:04:55,470  -->  00:04:58,910
this value make it even smaller than it is currently.

64

00:04:59,190  -->  00:05:05,400
And that way this whole part will decrease and this whole part will increase.

65

00:05:05,400  -->  00:05:11,190
So you are going to increase if it so happens that the new variable that's added whatever coefficient

66

00:05:11,220  -->  00:05:17,630
you give it a new coefficient you give it if you cannot decrease SS residual.

67

00:05:17,640  -->  00:05:17,910
Right.

68

00:05:17,930  -->  00:05:18,980
What will happen.

69

00:05:18,990  -->  00:05:20,610
Well this coefficient will just become zero.

70

00:05:20,610  -->  00:05:26,470
It very rarely happens I don't think I've ever seen a coefficient like B exactly zero.

71

00:05:26,570  -->  00:05:28,440
And I'll tell you just now why.

72

00:05:28,500  -->  00:05:33,390
But in the worst case scenario the regression process will say this variable is you know completely

73

00:05:33,870  -->  00:05:38,840
just making the world model definitely worse so I'll just put a 0 instead of this coefficient.

74

00:05:39,060  -->  00:05:39,980
And forget about it.

75

00:05:39,990  -->  00:05:43,720
And that way as this residual won't change and R-squared won't change.

76

00:05:43,830  -->  00:05:49,160
So you only have two options either R-squared will increase or it won't change at all.

77

00:05:49,170  -->  00:05:51,010
So R-squared will never decrease.

78

00:05:51,150  -->  00:05:58,770
If you add variables and why say that B-3 is never equal to exactly zero because they can always be

79

00:05:58,830  -->  00:06:06,840
or there will always be at least a slight random correlation between the independent label and the dependent

80

00:06:06,840  -->  00:06:07,370
variable.

81

00:06:07,450  -->  00:06:08,860
Doesn't matter what you put in here.

82

00:06:08,860  -->  00:06:16,290
So even in the example when we're looking at salary so salary equals years of experience plus how many

83

00:06:16,620  -->  00:06:19,230
how much I mean qualifications a person has.

84

00:06:19,380  -->  00:06:24,570
And then we can add in what is the last digit of the law of the person's mobile number.

85

00:06:24,600  -->  00:06:24,940
Right.

86

00:06:25,050  -->  00:06:27,940
Of course that is not going to.

87

00:06:27,940  -->  00:06:33,270
It doesn't have any correlation with the independent variable.

88

00:06:33,270  -->  00:06:37,160
What sort of that will there is no causative factor there's no association between the two.

89

00:06:37,170  -->  00:06:41,480
The lossage of your mobile number has no effect on your salary.

90

00:06:41,580  -->  00:06:46,870
But if we added in and they'll we'll randomly be a slight correlation you know.

91

00:06:46,890  -->  00:06:51,570
It'll just be a random correlation and the regression process will pick it up and I'll give it a go

92

00:06:51,570  -->  00:06:58,050
efficient and R-squared will probably decrease by that tiny little or increase by that tiny little bit

93

00:06:58,050  -->  00:06:58,340
.

94

00:06:58,410  -->  00:07:01,400
And so that is why there's a problem with R-squared.

95

00:07:01,530  -->  00:07:07,170
You can add variables and you will not know if those variables are helping your model or if they're

96

00:07:07,170  -->  00:07:12,540
not helping because you're R-Squared is going to is biased is basically always increasing regardless

97

00:07:12,630  -->  00:07:15,690
of the actual improvement or non-proven fit.

98

00:07:15,840  -->  00:07:23,910
So we've got to come up with different parameter to measure goodness of fit and that is where adjusted

99

00:07:23,940  -->  00:07:25,170
R-squared comes in.

100

00:07:25,170  -->  00:07:27,460
And this is the former for just that R-squared.

101

00:07:27,480  -->  00:07:29,640
Here is the number of regressors.

102

00:07:29,850  -->  00:07:34,870
So the number of independent variables and is the sample size so it's over here.

103

00:07:35,430  -->  00:07:39,230
Now they say that adjusted R-squared has a penalization factor.

104

00:07:39,330  -->  00:07:45,000
It penalizes you for adding independent variables that don't help your model.

105

00:07:45,000  -->  00:07:46,290
And let's talk about that.

106

00:07:46,290  -->  00:07:50,820
That's the important bit about just R-squared so p is a number of endpin verbals.

107

00:07:50,820  -->  00:07:55,490
Let's look at the piece here is in the bottom in the denominator.

108

00:07:55,590  -->  00:07:58,980
That means that when PPI increases when you increase the number of independent variables.

109

00:07:59,160  -->  00:08:01,800
This whole part decreases.

110

00:08:02,000  -->  00:08:08,070
And when this whole part decreases the denominator decreases the ratio increases and as the ratio increases

111

00:08:08,220  -->  00:08:10,480
this whole bid increases as well.

112

00:08:10,520  -->  00:08:14,920
And as this whole increases one minuses hold decreases.

113

00:08:15,030  -->  00:08:22,140
So as you can see as you're adding more regressors the adjusted R-squared is decreasing is going further

114

00:08:22,140  -->  00:08:23,810
away from 1.

115

00:08:24,000  -->  00:08:25,730
And that is the important part.

116

00:08:25,730  -->  00:08:33,260
Also you can see here what happens when R-squared increases when just normal R-squared increases R-squared

117

00:08:33,270  -->  00:08:34,180
increases.

118

00:08:34,260  -->  00:08:37,110
So this part decreases right.

119

00:08:37,110  -->  00:08:42,220
One might R-squared decreases and that means that this whole part increases.

120

00:08:42,360  -->  00:08:48,480
So you've got a battle here on one hand R-squared by adding a new variable you're increasing R-squared

121

00:08:49,560  -->  00:08:52,250
So you're increasing a justed R-squared.

122

00:08:52,260  -->  00:08:57,650
But on the other hand by adding numerable you increase P So you're decreasing adjusted R-squared.

123

00:08:57,900  -->  00:09:03,100
And in that sense it's a fair like it's a fair battle here.

124

00:09:03,300  -->  00:09:10,590
If your variable is not good is not helping the model then adjusted R-squared this will be an insignificant

125

00:09:10,590  -->  00:09:11,240
increase.

126

00:09:11,460  -->  00:09:17,530
And this penalization factor will actually drive R-squared down adjusted R-squared down.

127

00:09:17,790  -->  00:09:23,610
If on the other hand your new variable that you added is helping to model a lot then the increase in

128

00:09:23,610  -->  00:09:30,000
R-squared will be substantial and it will overwhelm this penalization factor so even though you'll still

129

00:09:30,000  -->  00:09:36,660
get penalized for adding a variable that increase the benefit to the model will be so great that even

130

00:09:36,660  -->  00:09:39,100
the justed R-squared will go up.

131

00:09:39,190  -->  00:09:41,910
And so that's how the adjusted R-squared basically works.

132

00:09:41,910  -->  00:09:47,910
It's it's a very good metric it helps you understand whether you're adding good variables to model or

133

00:09:47,910  -->  00:09:54,690
not and we'll be using adjusted R-squared throughout this course to make sure our models are robust

134

00:09:54,690  -->  00:09:55,530
when we're building them
