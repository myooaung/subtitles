1

00:00:00,390  -->  00:00:06,480
Hello and welcome to this art tutorial in the British Statoil we imported our mall dataset and we prepared

2

00:00:06,540  -->  00:00:11,820
our data correctly by taking the two columns we are interested in the annual income and spending score

3

00:00:11,830  -->  00:00:11,990
.

4

00:00:12,150  -->  00:00:17,520
So we created this variable x that contains these two columns and now things are going to get much more

5

00:00:17,520  -->  00:00:23,910
interesting because in this tutorial we are going to build our DNA program and we will use it to find

6

00:00:23,910  -->  00:00:30,120
the optimal number of clusters exactly like we did in the Cayman section where as you remember in step

7

00:00:30,120  -->  00:00:33,960
two we use the elbow method chart to find the optimal number of clusters.

8

00:00:34,140  -->  00:00:40,050
Well here in hierarchical clustering Step two we will also look for this optimal number of clusters

9

00:00:40,050  -->  00:00:40,320
.

10

00:00:40,320  -->  00:00:42,510
Only this time we're not going to use the other method.

11

00:00:42,630  -->  00:00:45,060
We are going to use the then program.

12

00:00:45,060  -->  00:00:47,010
So let's do that right now.

13

00:00:47,010  -->  00:00:51,910
The very cool thing about it is that we only need one line of code to build this program.

14

00:00:52,260  -->  00:00:54,880
So let's write it let's write this line of code.

15

00:00:54,900  -->  00:01:02,020
We start by creating a variable then program then equals and then we're going to use a class H class

16

00:01:02,040  -->  00:01:02,210
.

17

00:01:02,370  -->  00:01:06,860
So let's type each class here and then let's press F 1.

18

00:01:07,170  -->  00:01:10,780
And here we have all the info of this age class class.

19

00:01:10,950  -->  00:01:14,880
So let's look at the arguments here we only need the first two arguments.

20

00:01:15,090  -->  00:01:20,520
The first argument is a dissimilarity structure as produced by this and in our case this parameter is

21

00:01:20,520  -->  00:01:26,850
going to be the distance matrix of our dataset X which is a matrix that tells for each pair of customers

22

00:01:27,170  -->  00:01:29,170
the Euclidean distance between the two.

23

00:01:29,280  -->  00:01:34,500
So that means that for each pair of customers we take the two coordinates annual income and spending

24

00:01:34,500  -->  00:01:38,300
score and we compute the Euclidean distance between the two.

25

00:01:38,310  -->  00:01:40,010
Based on these coordinates.

26

00:01:40,320  -->  00:01:44,070
OK so that was just to explain the first parameter of the H class class.

27

00:01:44,080  -->  00:01:46,260
And so let's put it in our code.

28

00:01:46,260  -->  00:01:55,650
We input d'Este and in parenthesis X come up and then method equals Euclidean so that specifies that

29

00:01:55,650  -->  00:01:59,780
we want to compute the unity and distance matrix for our data X..

30

00:02:00,300  -->  00:02:02,710
OK so that's the first parameter.

31

00:02:02,730  -->  00:02:07,100
This distance matrix and now the second parameter is the method.

32

00:02:07,350  -->  00:02:14,250
So this method is simply the method used to find the clusters and like in Python we're going to choose

33

00:02:14,250  -->  00:02:17,100
the most common method which is the word method.

34

00:02:17,100  -->  00:02:23,730
Here it's called word dot D and it's actually a method that is trying to minimize the variance within

35

00:02:23,790  -->  00:02:25,260
each cluster.

36

00:02:25,290  -->  00:02:30,030
Kind of like would we didn't k when we were trying to minimize the willing cluster sum of squares.

37

00:02:30,330  -->  00:02:34,530
Well here it's based on the same idea but instead of trying to minimize the within close the sum of

38

00:02:34,530  -->  00:02:39,750
squares we are trying to minimize the within cluster variance to find our clusters.

39

00:02:40,350  -->  00:02:44,790
So here we write method equals word darty.

40

00:02:44,880  -->  00:02:48,390
So that is the end of the line to build this dense program.

41

00:02:48,480  -->  00:02:50,170
And now we just need to plot it.

42

00:02:50,310  -->  00:02:58,290
So just below we are going to write plot then in parenthesis then program then let's give it a title

43

00:02:58,290  -->  00:03:05,640
by typing main equals paste parenthesis then program in quotes.

44

00:03:05,640  -->  00:03:10,800
Then let's give a name to the x axis by adding x love equals customers.

45

00:03:10,980  -->  00:03:16,170
Because in the dead program all our customers are going to be on the x axis and then finally let's give

46

00:03:16,170  -->  00:03:18,370
a name to our White Label.

47

00:03:18,460  -->  00:03:23,580
We're going to call it Euclidean distances and that's because in the dental gram The vertical lines

48

00:03:23,580  -->  00:03:29,430
that we're going to see are actually the Euclidean distances of the closer's that is between the centroid

49

00:03:29,430  -->  00:03:30,670
of the clusters.

50

00:03:30,690  -->  00:03:33,060
OK so we're good to go with our A-plot.

51

00:03:33,090  -->  00:03:35,390
So and our dental gram actually.

52

00:03:35,390  -->  00:03:39,300
So let's select all this code section here executes.

53

00:03:39,420  -->  00:03:41,470
And here is our den program.

54

00:03:41,700  -->  00:03:43,530
So let's have a look at it.

55

00:03:43,530  -->  00:03:46,020
I'm clicking on zoom here to make it bigger.

56

00:03:46,260  -->  00:03:47,200
OK.

57

00:03:47,550  -->  00:03:50,830
And now let's try to find the optimal number of clusters.

58

00:03:50,880  -->  00:03:52,520
Thanks to this tent program.

59

00:03:52,740  -->  00:03:59,880
So as Cuil explains in the intuition section to find this optimal number of clusters we need to find

60

00:03:59,880  -->  00:04:07,510
the largest vertical distance that we can make without crossing any other horizontal line.

61

00:04:08,400  -->  00:04:12,220
And then we just need to count the number of vertical lines at this level.

62

00:04:12,600  -->  00:04:12,840
OK.

63

00:04:12,840  -->  00:04:16,340
So let's start by finding the largest vertical distance.

64

00:04:16,560  -->  00:04:22,950
So it's not here obviously then maybe this one it's quite a large distance that would actually give

65

00:04:22,950  -->  00:04:27,500
us three clusters because as you can see here I'm crossing three vertical lines.

66

00:04:27,990  -->  00:04:29,720
Definitely not this one.

67

00:04:29,760  -->  00:04:36,900
And here we have another large distance you see from this point to this point is quite a large distance

68

00:04:37,260  -->  00:04:40,120
and then below obviously we don't have any large distance.

69

00:04:40,140  -->  00:04:45,650
So now the question is what is the largest distance between this distance and this distance.

70

00:04:45,900  -->  00:04:51,240
Well if you have a better look at it we can see that the largest distance is actually this distance

71

00:04:51,250  -->  00:04:51,560
.

72

00:04:52,050  -->  00:04:54,930
And how many vertical lines do we have at this level.

73

00:04:54,930  -->  00:04:58,470
Let's see one two three four and five.

74

00:04:58,470  -->  00:05:04,470
So that means the optimal number of clusters is five clusters and that's of course a relief because

75

00:05:04,470  -->  00:05:08,820
that's what we obtained with the Kamins algorithm using the elbow method.

76

00:05:08,820  -->  00:05:12,170
So everything is fine everything is perfectly coherent.

77

00:05:12,270  -->  00:05:18,810
So we welcome our second step and now we are ready to move on to the next step which is to fit our hierarchical

78

00:05:18,810  -->  00:05:23,990
clustering algorithm to our data X. And that's what we will be doing in the next tutorial.
