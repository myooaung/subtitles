1
00:00:00,090 --> 00:00:00,900
Hello, my friends.

2
00:00:00,960 --> 00:00:03,900
Welcome back to this moral selection section.

3
00:00:04,140 --> 00:00:09,600
In the previous editorial, we implemented the K4 cross-validation technique, which is a new tool you

4
00:00:09,600 --> 00:00:14,670
now have in order to measure the most relevant way, the performance of your moral.

5
00:00:15,030 --> 00:00:21,090
And now I'm giving you an extra tool, which is grid search and which will allow you to find the optimal

6
00:00:21,150 --> 00:00:27,870
values of the hyper parameters in any model to therefore get an even higher accuracy than, you know,

7
00:00:27,870 --> 00:00:29,760
the standard version of the model.

8
00:00:30,210 --> 00:00:33,760
So we're going to work on the exact same data set Social Network ad.

9
00:00:33,760 --> 00:00:35,670
So I'm not going to remind this again.

10
00:00:35,910 --> 00:00:42,950
And therefore, let's open our grid search implementation either in Google Collaboratory or Dupere and

11
00:00:42,950 --> 00:00:43,440
Notebook.

12
00:00:43,710 --> 00:00:44,310
As you wish.

13
00:00:44,640 --> 00:00:50,400
And actually, this grid search implementation is exactly the same as this one.

14
00:00:50,670 --> 00:00:57,210
Only we're just going to implement one extra cell, which will be, you know, the cell applying grid

15
00:00:57,210 --> 00:01:03,600
search to find the best model and the best parameter, the best model version, actually, of the kernel

16
00:01:03,750 --> 00:01:04,200
as yet.

17
00:01:04,830 --> 00:01:05,250
All right.

18
00:01:05,490 --> 00:01:06,420
Are you ready for this?

19
00:01:06,600 --> 00:01:07,440
I am sure you are.

20
00:01:07,530 --> 00:01:13,100
So let's create right away a copy, because this file is in read only mode and we're gonna, you know,

21
00:01:13,140 --> 00:01:17,070
reimplement that cell building the grid search technique.

22
00:01:17,820 --> 00:01:23,040
Okay, let's now remove this because we're going to work with our copies and there we go.

23
00:01:23,120 --> 00:01:27,870
Now let's crawl down, you know, to find that cell implementing grid search.

24
00:01:28,290 --> 00:01:29,970
Let's remove it.

25
00:01:30,000 --> 00:01:31,290
You know, put it in the trash.

26
00:01:31,590 --> 00:01:32,590
And there we go.

27
00:01:32,830 --> 00:01:37,020
Let's apply grid search to find best model and the best parameters.

28
00:01:37,650 --> 00:01:37,840
All right.

29
00:01:37,890 --> 00:01:39,280
So let's create a new coattail.

30
00:01:39,510 --> 00:01:45,240
And now the first thing I want to do is actually go to dislike it, learn API.

31
00:01:45,330 --> 00:01:51,510
There we go to show you actually again the class that implements the kernel as your model, because

32
00:01:51,810 --> 00:01:58,140
I actually want to show you the hyper parameters which we can tune to find the best version of the kernel

33
00:01:58,230 --> 00:01:58,860
as your model.

34
00:01:58,890 --> 00:02:00,150
So here, take the first link.

35
00:02:00,460 --> 00:02:01,110
That's the psychic.

36
00:02:01,110 --> 00:02:01,990
You're in Web site.

37
00:02:02,400 --> 00:02:06,150
Then please go to API here, you know, to find all the classes and the functions.

38
00:02:06,570 --> 00:02:11,590
And remember, Colonel, as VM is actually in the as v m section.

39
00:02:11,940 --> 00:02:17,190
So we have to scroll down to find the S letter and we should find it very soon.

40
00:02:17,270 --> 00:02:23,430
We support vector machines as Kaylor and DOT as VM as V a module by the Cycler Library.

41
00:02:23,790 --> 00:02:27,590
And remember, the class is this one as VM dot as we see.

42
00:02:28,310 --> 00:02:28,600
Right.

43
00:02:28,740 --> 00:02:35,880
If we check on our implementation, you know, we import this SBC class to build this kernel as VM moral

44
00:02:35,880 --> 00:02:37,950
with a radial basis function kernell.

45
00:02:38,160 --> 00:02:38,490
Okay.

46
00:02:39,090 --> 00:02:39,390
All right.

47
00:02:39,420 --> 00:02:41,520
So K4 cross-validation grid search.

48
00:02:41,610 --> 00:02:44,490
There we go back in the cycle and API.

49
00:02:44,790 --> 00:02:52,140
Well, the thing I wanted to show you was that indeed the kernel as VM model has a lot of hyper parameters.

50
00:02:52,400 --> 00:02:57,990
I remind that hyper parameters are the parameters which are not learned during the training process.

51
00:02:58,050 --> 00:03:02,220
You know, these are different parameters than the weights or, you know, the coefficient of the model.

52
00:03:02,670 --> 00:03:05,760
And these hyper parameters are well, you know, there are many of them.

53
00:03:05,850 --> 00:03:13,110
You have the C parameter, which is the regularization parameter that you can tune to actually improve

54
00:03:13,110 --> 00:03:16,050
the training performance by reducing overfitting.

55
00:03:16,280 --> 00:03:22,200
I reminded Overfitting is a situation where you have a high accuracy on the training set, but a poor

56
00:03:22,200 --> 00:03:27,210
one on the test set, which means that your model was overtrained on the training set.

57
00:03:27,240 --> 00:03:28,560
You know, two well trained.

58
00:03:28,920 --> 00:03:32,850
And we definitely want to avoid this so that Perama is actually very important.

59
00:03:33,150 --> 00:03:36,270
And we will tune it to find the best value.

60
00:03:36,570 --> 00:03:40,470
The default value is 1.0, but we will try several values.

61
00:03:40,490 --> 00:03:41,010
We will try.

62
00:03:41,190 --> 00:03:44,680
Oh point twenty five point five or point seventy five and one.

63
00:03:44,760 --> 00:03:47,370
Because as you can see, it's important to notice this.

64
00:03:47,560 --> 00:03:52,300
The strength of the regularization is inversely proportional to see.

65
00:03:52,620 --> 00:03:57,960
Meaning that the lower is C, the stronger will be the regularization.

66
00:03:58,170 --> 00:04:02,310
So, you know, we're going to try the value of O point twenty five, which will be therefore a strong

67
00:04:02,310 --> 00:04:03,320
regularization.

68
00:04:03,570 --> 00:04:05,130
We'll try open five as well.

69
00:04:05,220 --> 00:04:08,320
Still quite a strong regularization O point seventy five.

70
00:04:08,340 --> 00:04:10,640
Less strong regularization and one.

71
00:04:10,740 --> 00:04:11,750
And we can even try.

72
00:04:11,790 --> 00:04:12,020
No.

73
00:04:12,080 --> 00:04:12,870
Open twenty.

74
00:04:13,260 --> 00:04:14,760
Open five as you want.

75
00:04:14,790 --> 00:04:15,330
Feel free.

76
00:04:15,580 --> 00:04:17,400
I will actually improvise this a bit.

77
00:04:17,730 --> 00:04:19,920
I'll see where I end up choosing as the values of.

78
00:04:20,410 --> 00:04:21,040
Experiment.

79
00:04:21,750 --> 00:04:23,220
And then we have the kernel also.

80
00:04:23,220 --> 00:04:26,280
Of course, that's a very important hyper parameter.

81
00:04:26,330 --> 00:04:27,720
And we will actually tune it.

82
00:04:27,990 --> 00:04:33,120
Meaning we will find the best kernel of the kernel as a model among these ones.

83
00:04:33,190 --> 00:04:37,890
So we'll actually try both the linear one and Arbie F one.

84
00:04:37,920 --> 00:04:39,120
But you're free to also try.

85
00:04:39,140 --> 00:04:41,400
For example, the pulley or sigmoid one.

86
00:04:42,060 --> 00:04:48,330
And then I would also like to tune this parameter gamma, which is a parameter you only enter when you

87
00:04:48,330 --> 00:04:52,920
choose either arbi f kernel or polynomial kernel or a sigmoid kernel.

88
00:04:53,070 --> 00:04:58,170
And since we're gonna tune the linear and IBF kernels, well, for the RBA of kernel, I would like

89
00:04:58,170 --> 00:04:59,880
to tune, you know, different values.

90
00:05:00,080 --> 00:05:01,910
Gamma to actually find the best one.

91
00:05:02,240 --> 00:05:07,400
And actually we will try all the gamma values from zero point one to zero point nine.

92
00:05:07,600 --> 00:05:07,920
OK.

93
00:05:08,060 --> 00:05:13,460
And Gamma is, of course, the coefficient in, you know, the formula of the kernel, which Carol and

94
00:05:13,460 --> 00:05:16,400
I have already showed you in part three classification.

95
00:05:17,090 --> 00:05:17,540
All right.

96
00:05:17,630 --> 00:05:20,330
So let's do these three parameters.

97
00:05:20,570 --> 00:05:24,560
And if you want to tune, more of them will feel totally free or if you want to tune more values of

98
00:05:24,560 --> 00:05:24,890
them.

99
00:05:25,070 --> 00:05:29,930
But you will see that already with this tuning, we will get an excellent performance.

100
00:05:30,350 --> 00:05:36,350
And speaking of performance, actually, it's important to highlight that each time we will try these

101
00:05:36,650 --> 00:05:39,630
different combinations of these hyper parameter values.

102
00:05:39,920 --> 00:05:46,100
Well, the accuracy of the model will be evaluated through careful trust validation and not on a single

103
00:05:46,100 --> 00:05:46,610
test set.

104
00:05:46,840 --> 00:05:47,010
OK.

105
00:05:47,150 --> 00:05:52,970
So we will have super relevant measures of the accuracy's for each of the combinations of these parameters.

106
00:05:53,270 --> 00:05:58,550
And therefore, we will be confident that the best set, you know, the best combination of parameters

107
00:05:58,550 --> 00:06:00,920
we end up with is indeed the right one.

108
00:06:01,550 --> 00:06:02,090
All right.

109
00:06:02,210 --> 00:06:03,050
So let's do this.

110
00:06:03,170 --> 00:06:05,830
Let's implement grid search now.

111
00:06:06,230 --> 00:06:10,700
And of course, as usual, we're going to start from Saikat.

112
00:06:10,700 --> 00:06:12,350
Learn from cyclone.

113
00:06:12,470 --> 00:06:18,560
By the way, let's upload a notebook now so that we can, you know, benefit from Google cloud assistance.

114
00:06:18,950 --> 00:06:21,680
Let's click upload here and there we go.

115
00:06:21,710 --> 00:06:26,510
We already have the right folder and we select social network ads, dot CSC.

116
00:06:26,720 --> 00:06:30,200
That's the data set with which we're going to experiment grid search.

117
00:06:30,680 --> 00:06:31,280
And so there we go.

118
00:06:31,310 --> 00:06:33,760
Now we should get the help of Google Kulab.

119
00:06:33,800 --> 00:06:39,920
So we're going to start from Saikat here and and then saying we're gonna get access to the model selection.

120
00:06:40,010 --> 00:06:40,520
There we go.

121
00:06:40,530 --> 00:06:48,080
Module from which we're going to import this time a class which is a grid search civic class.

122
00:06:48,260 --> 00:06:48,770
There we go.

123
00:06:49,700 --> 00:06:50,000
Okay.

124
00:06:50,420 --> 00:06:56,120
And now, already in the next step, we're going to enter the different combinations of parameters.

125
00:06:56,180 --> 00:06:57,350
We want to experiment.

126
00:06:57,710 --> 00:07:03,800
And so I'm going to create a new variable here, which I'm going to call parameters, and then I'm going

127
00:07:03,800 --> 00:07:06,110
to enter these combinations of hyper parameters.

128
00:07:06,110 --> 00:07:08,690
We want to experiment within a list.

129
00:07:08,840 --> 00:07:13,340
So, you know, this parameter is viable here will simply be the list of the different combinations

130
00:07:13,340 --> 00:07:14,420
of hybrid parameters.

131
00:07:14,630 --> 00:07:15,470
We want to test.

132
00:07:15,980 --> 00:07:22,790
And then within that list, I'm actually going to create two dictionaries y two, because actually I

133
00:07:22,790 --> 00:07:24,660
want to test two different kernel.

134
00:07:24,680 --> 00:07:27,920
So in one dictionary we will have the linear kernel.

135
00:07:28,220 --> 00:07:32,350
And in a separate dictionary, we'll have the RPF kernel.

136
00:07:32,600 --> 00:07:38,030
And the reason why I have to separate these two dictionaries is because, you know, the gamma parameter,

137
00:07:38,030 --> 00:07:43,520
which we want to tune, can only be used with the RBA kernel and not with the linear kernel.

138
00:07:43,790 --> 00:07:48,350
If it could be used with the linear kernel, then I wouldn't have to make two dictionaries.

139
00:07:48,560 --> 00:07:49,700
Only one would be fine.

140
00:07:49,700 --> 00:07:54,230
And I would just enter the different values of all the parameters which we want to experiment.

141
00:07:54,500 --> 00:07:55,380
But that's not the case.

142
00:07:55,400 --> 00:08:00,530
So indeed, I have to create these two dictionaries, which I'm gonna put, you know, below each other

143
00:08:00,590 --> 00:08:01,310
just like that.

144
00:08:01,640 --> 00:08:02,330
And there we go.

145
00:08:02,420 --> 00:08:04,670
Now let's enter the first dictionary.

146
00:08:05,270 --> 00:08:06,740
So how are you gonna do this?

147
00:08:06,830 --> 00:08:09,110
Well, just the way dictionaries work.

148
00:08:09,340 --> 00:08:14,300
First, we have to enter the key and the key is actually the parameter itself.

149
00:08:14,570 --> 00:08:21,230
So remember, we want to tune first, see the regularization parameter, then the kernel, and then

150
00:08:21,320 --> 00:08:23,750
in the case of the RBA girl, the degree.

151
00:08:24,140 --> 00:08:29,360
And so in my dictionary here, I'm going to enter as the first key in quotes.

152
00:08:29,720 --> 00:08:37,100
Well, this parameter C and then I'm adding here some Cullin and right after the colon, I have to enter

153
00:08:37,100 --> 00:08:38,600
the value of that key.

154
00:08:38,720 --> 00:08:42,590
And the value of that key is exactly the values of C.

155
00:08:42,680 --> 00:08:48,110
We want to experiment and we have to enter them in a new list in a pair of square brackets.

156
00:08:48,470 --> 00:08:54,530
And as we said, we want to try the following different values of this regularization parameter C,

157
00:08:54,950 --> 00:09:01,460
which are, as we said, first four point twenty five, then O point five.

158
00:09:01,970 --> 00:09:04,130
Then O point seventy five.

159
00:09:04,610 --> 00:09:05,690
And then one.

160
00:09:06,140 --> 00:09:06,410
Right.

161
00:09:06,430 --> 00:09:08,990
So here on the left, we have strong regularization.

162
00:09:09,080 --> 00:09:13,010
And the closer we get to one, the less strong is the regularization.

163
00:09:13,620 --> 00:09:13,960
All right.

164
00:09:14,060 --> 00:09:20,510
So that's for this parameter C, then we can enter a second key within this dictionary.

165
00:09:20,570 --> 00:09:21,480
And we have to separate.

166
00:09:21,500 --> 00:09:22,240
With a comma.

167
00:09:22,760 --> 00:09:28,010
And that second key will be, of course, that other parameter which we want to tune, which is the

168
00:09:28,010 --> 00:09:28,530
kernel.

169
00:09:28,580 --> 00:09:31,260
And therefore, in quotes here, I'm going to enter Colonel.

170
00:09:32,240 --> 00:09:35,450
And then for the value of that key, which has to be separate.

171
00:09:35,600 --> 00:09:36,280
By Colin.

172
00:09:36,590 --> 00:09:39,080
Well, this value will be once again in the list.

173
00:09:39,170 --> 00:09:42,730
Therefore, in a pair of square brackets, the different values of the kernel.

174
00:09:42,770 --> 00:09:44,030
We want to experiment.

175
00:09:44,360 --> 00:09:50,450
However, since we have these two dictionaries to separate the case of the linear kernel and the RBA

176
00:09:50,510 --> 00:09:53,060
kernel, because, you know, if this gamma parameter.

177
00:09:53,360 --> 00:09:58,430
Well, here in this COL list, I will only enter in quotes once again.

178
00:09:58,580 --> 00:09:59,430
The Lynnae are.

179
00:09:59,830 --> 00:10:00,240
Colonel.

180
00:10:00,790 --> 00:10:02,660
And now check what I'm gonna do.

181
00:10:02,830 --> 00:10:06,190
I'm gonna copy all of these, you know, inside this first dictionary.

182
00:10:06,880 --> 00:10:10,690
I'm going to paste that inside this second dictionary.

183
00:10:11,110 --> 00:10:15,480
And then here we're gonna test the same values of the parameters, see?

184
00:10:15,760 --> 00:10:19,980
But we're gonna test them with an RB if, Colonel.

185
00:10:20,320 --> 00:10:23,420
And therefore, since now we're experimenting the army.

186
00:10:23,560 --> 00:10:24,070
Colonel.

187
00:10:24,310 --> 00:10:31,840
Well, then we can add this extra hyper parameter to tune, which is the gamma parameter, which is

188
00:10:31,840 --> 00:10:34,360
a Newquay of this second dictionary.

189
00:10:34,780 --> 00:10:40,840
So Gamma and the value of that key will be the list of the different values of Gamma.

190
00:10:40,870 --> 00:10:42,220
We want to try.

191
00:10:42,460 --> 00:10:42,820
Right.

192
00:10:42,910 --> 00:10:45,880
And as we said, these are 0.01.

193
00:10:46,150 --> 00:10:55,380
Then oh point two, then four point three and point four point five or point six or point seven.

194
00:10:56,010 --> 00:10:58,720
Open eight and open nine.

195
00:10:59,110 --> 00:10:59,650
All right.

196
00:10:59,770 --> 00:11:00,190
Good.

197
00:11:00,310 --> 00:11:01,480
So you see how this works.

198
00:11:01,750 --> 00:11:04,210
We have to enter our parameters into a list.

199
00:11:04,330 --> 00:11:05,680
And then inside each list.

200
00:11:05,800 --> 00:11:10,360
Well, either we don't have to separate some cases and we can put all the hyper parameters and their

201
00:11:10,360 --> 00:11:16,750
values into a same dictionary, or we have to separate some cases and we enter them with two separate

202
00:11:16,840 --> 00:11:17,380
dictionaries.

203
00:11:17,410 --> 00:11:17,800
This way.

204
00:11:18,810 --> 00:11:19,140
All right.

205
00:11:19,230 --> 00:11:19,770
Excellent.

206
00:11:19,830 --> 00:11:26,370
And now that we prepared correctly are parameters, you know, our hyper parameters, combinations.

207
00:11:26,760 --> 00:11:33,360
Well, time to call that grid search silly class, because indeed, as you can guess, this list of

208
00:11:33,360 --> 00:11:39,780
parameters will be one of the inputs of that grid search class or, you know, the instance of that

209
00:11:39,780 --> 00:11:40,830
class we will create.

210
00:11:41,070 --> 00:11:45,720
And well, speaking of the instance of that class, that's exactly our next step here.

211
00:11:45,900 --> 00:11:50,650
And so I'm going to create a new object, a new variable, which will be an object of this grid search

212
00:11:50,650 --> 00:11:55,380
theory class and which I'm going to call simply grid underscore search.

213
00:11:55,770 --> 00:11:56,370
All right.

214
00:11:56,550 --> 00:11:59,940
Then, of course, we have to call a class to create such an instance.

215
00:12:00,270 --> 00:12:01,500
So grid search.

216
00:12:01,770 --> 00:12:02,400
There we go.

217
00:12:02,490 --> 00:12:03,930
Was waiting for Google Kulab.

218
00:12:04,320 --> 00:12:07,590
Then in parentheses, let's enter the parameters.

219
00:12:07,590 --> 00:12:09,690
You can find them on the cyclone API.

220
00:12:09,870 --> 00:12:11,280
But I'll just give them to you now.

221
00:12:11,290 --> 00:12:13,440
So the first one is estimated.

222
00:12:14,370 --> 00:12:14,820
All right.

223
00:12:15,090 --> 00:12:18,300
And this will be equal, of course, to your classifier.

224
00:12:18,390 --> 00:12:21,900
You know, just like the estimated argument in the Cross Vänskä function.

225
00:12:22,320 --> 00:12:26,440
So classifier and which is, of course, your kernel as yem classifier.

226
00:12:27,090 --> 00:12:28,830
Then next argument.

227
00:12:29,070 --> 00:12:31,820
Well, next argument is actually our parameters.

228
00:12:31,830 --> 00:12:34,650
You know, the different combinations of the hybrid parameters.

229
00:12:34,650 --> 00:12:37,320
We want to experiment and try to find the best value.

230
00:12:37,800 --> 00:12:42,030
And so the name of that argument is Boram Underscore.

231
00:12:42,190 --> 00:12:42,780
Great.

232
00:12:43,110 --> 00:12:49,440
And this will be of course equal to or parameter is variable, that list of all the combinations of

233
00:12:49,530 --> 00:12:50,880
hybrid parameter values.

234
00:12:51,450 --> 00:12:53,070
Then next argument.

235
00:12:53,580 --> 00:12:55,410
Well, the next argument is an important one.

236
00:12:55,440 --> 00:12:56,670
That's the scoring.

237
00:12:56,880 --> 00:13:01,920
And that's basically the metric with which you want to evaluate the performance of your model for each

238
00:13:02,100 --> 00:13:05,070
of these combinations of hyper parameter values.

239
00:13:05,430 --> 00:13:07,830
And well, since now we're doing classification.

240
00:13:08,100 --> 00:13:12,060
Well, the value for this argument will be in, quote, accuracy, of course.

241
00:13:12,630 --> 00:13:15,090
All right, then, next argument.

242
00:13:15,480 --> 00:13:22,320
So as I told you, you know, each of these combinations of hyper parameters will be evaluated through

243
00:13:22,380 --> 00:13:26,280
K4 cross-validation and not on a single test set.

244
00:13:26,670 --> 00:13:31,710
And therefore, here, the next argument is just to choose the number of trained test force that will

245
00:13:31,710 --> 00:13:35,520
have when applying careful cross-validation for each of the combinations.

246
00:13:35,730 --> 00:13:38,660
And that parameter, his name is the same as right here.

247
00:13:38,700 --> 00:13:40,100
It is S.V..

248
00:13:40,590 --> 00:13:45,350
And once again, we will choose the very common value of ten trained test force.

249
00:13:46,200 --> 00:13:50,430
And finally, a very optional parameter, which you don't have too much worry about.

250
00:13:50,460 --> 00:13:54,090
But I'm adding it here in case you run this code on your machine.

251
00:13:54,510 --> 00:13:59,040
This is a parameter that will set how to run your processor, as you know, in your machine.

252
00:13:59,460 --> 00:14:06,330
And I'm just adding it here with the parameter value minus one, which means that all your processors

253
00:14:06,420 --> 00:14:10,860
will be used, you know, in your machine, which will optimize the grid search process, because you're

254
00:14:10,860 --> 00:14:15,000
going to see that it can be a very long process, you know, to test all these combinations.

255
00:14:15,240 --> 00:14:16,740
Well, it can actually take some time.

256
00:14:17,120 --> 00:14:17,290
OK.

257
00:14:17,460 --> 00:14:20,100
So that's it for this grid search study class.

258
00:14:20,220 --> 00:14:21,750
We now have our object.

259
00:14:21,870 --> 00:14:27,630
And as you might guess, the next step will be to connect this object to our training set.

260
00:14:27,810 --> 00:14:33,690
I remind that grid search has to be applied on to training set only because the test set once again

261
00:14:33,690 --> 00:14:39,240
is something you want to have separated whenever you want to test your moral or, you know, your tuned

262
00:14:39,330 --> 00:14:39,660
model.

263
00:14:39,700 --> 00:14:45,210
You know, after you tune it on a set of new observations to see how it performs on a data set like

264
00:14:45,210 --> 00:14:45,870
in production.

265
00:14:46,170 --> 00:14:46,430
OK.

266
00:14:46,520 --> 00:14:51,060
So we will apply now grid search to the training set only and to do this.

267
00:14:51,150 --> 00:14:58,350
Well, we're going to call our object grid search from which we're going to call once again that very

268
00:14:58,350 --> 00:15:03,540
famous method, which you're so familiar with now, which is the fit method.

269
00:15:04,080 --> 00:15:09,510
And of course, this fit method takes two arguments as input, which are, first, the features of the

270
00:15:09,510 --> 00:15:16,280
training set, meaning X train and then the dependent, viable vector of the training set.

271
00:15:16,560 --> 00:15:20,370
That is why train just like a classic training ride.

272
00:15:20,480 --> 00:15:22,890
Because actually now we're going to do a lot of training.

273
00:15:23,000 --> 00:15:27,510
You know, a training for each of these combinations of hyper parameter values.

274
00:15:28,150 --> 00:15:28,480
OK.

275
00:15:28,590 --> 00:15:30,960
So that's basically the multiple training.

276
00:15:31,170 --> 00:15:32,820
And then there we go.

277
00:15:33,300 --> 00:15:34,920
This multiple training is done.

278
00:15:35,040 --> 00:15:41,010
Well, we'll get all the final results, meaning all the different accuracy's resulting from all these

279
00:15:41,010 --> 00:15:42,720
combinations of happy parameters.

280
00:15:43,080 --> 00:15:48,000
And therefore, after getting all these accuracy's will, we'll get the highest of them and we'll get

281
00:15:48,000 --> 00:15:52,560
mostly the set of parameters that led to that highest accuracy.

282
00:15:53,070 --> 00:15:58,080
And so first, let's get that best accuracy, which we're going to put into a new variable, which we're

283
00:15:58,080 --> 00:16:00,900
going to call best accuracy.

284
00:16:01,410 --> 00:16:07,080
And to get this, well, we're actually gonna get it from an attribute of the great search civic class,

285
00:16:07,410 --> 00:16:13,860
which is called best score, I reminded an attribute is just a value we can get from a specific object.

286
00:16:14,250 --> 00:16:17,880
And that value here is, of course, the best accuracy taken from that.

287
00:16:18,110 --> 00:16:19,030
Research objects.

288
00:16:19,150 --> 00:16:28,400
I'm gonna copy this and just below gonna paste it from which I'm gonna call that best on the score score

289
00:16:28,540 --> 00:16:30,490
and then we have to add an underscore again.

290
00:16:30,550 --> 00:16:35,320
That's usually how our named attributes are to best score, meaning best accuracy.

291
00:16:35,350 --> 00:16:37,750
Because we chose an accuracy scoring.

292
00:16:38,440 --> 00:16:40,720
And now now that we have the best accuracy.

293
00:16:40,750 --> 00:16:44,510
Well, let's see which parameters led to that best accuracy.

294
00:16:44,830 --> 00:16:49,390
And to do this well, we're going to put that best set of parameters once we get into a new variable,

295
00:16:49,420 --> 00:16:53,710
which we're gonna call best underscore parameters.

296
00:16:55,080 --> 00:16:55,620
All right.

297
00:16:56,040 --> 00:16:57,630
And once again, to get them.

298
00:16:57,870 --> 00:17:02,790
We're going to call another attribute of our grid search city class or, you know, our grid search

299
00:17:02,940 --> 00:17:04,140
instance of that class.

300
00:17:04,590 --> 00:17:06,030
And therefore, I just copied this.

301
00:17:06,030 --> 00:17:14,040
And we'll be that here because this time the name of that attribute is best, perhaps best Baran's.

302
00:17:14,250 --> 00:17:14,670
OK.

303
00:17:15,090 --> 00:17:19,200
And now we're going to finish this with two beautiful prints.

304
00:17:19,590 --> 00:17:24,480
And these prints are actually very similar to the ones we did for K4 cross-validation.

305
00:17:24,490 --> 00:17:25,830
So I'm just copying this.

306
00:17:26,250 --> 00:17:28,650
And right here, basting them.

307
00:17:29,160 --> 00:17:33,060
And now instead of printing, you know, accuracy as string, we're gonna print.

308
00:17:33,210 --> 00:17:34,920
Best accuracy.

309
00:17:35,430 --> 00:17:40,040
Then we're going to keep the same format of, you know, the two decimals after the comma in a float.

310
00:17:40,380 --> 00:17:45,420
And for the variable that will get us this, well, of course, this won't be the mean of the accuracy's

311
00:17:45,420 --> 00:17:50,310
here, but simply that best accuracy, variable value.

312
00:17:50,640 --> 00:17:50,970
Right.

313
00:17:51,090 --> 00:17:56,370
This gives directly the best accuracy resulting from the best combination of hyper parameters.

314
00:17:56,760 --> 00:18:03,840
So let's just take this and let's just replace that accuracy's dot mean, you know, the main function

315
00:18:04,110 --> 00:18:06,840
by this best accuracy variable.

316
00:18:06,900 --> 00:18:11,280
And then we have to multiply it by 100 in order to get the accuracy in this format, you know, with

317
00:18:11,280 --> 00:18:14,160
the precent format, which we have here once again.

318
00:18:14,760 --> 00:18:20,820
And then we don't want to print the standard deviation, but the best parameters.

319
00:18:21,630 --> 00:18:27,210
And then I'm just gonna print these best parameters the classic way, you know, the simple way of printing

320
00:18:27,210 --> 00:18:27,670
something.

321
00:18:27,720 --> 00:18:32,760
So I'm just gonna remove everything here, you know, that format function and that variable.

322
00:18:32,760 --> 00:18:34,710
So because it will be a different variable.

323
00:18:35,070 --> 00:18:36,900
And then, look, I'm just going to do this.

324
00:18:37,110 --> 00:18:43,290
This is the string that will be printed and then to print the value of that strain or the best parameters

325
00:18:43,290 --> 00:18:43,830
themselves.

326
00:18:44,100 --> 00:18:49,680
I'm just gonna separate that string with a comma and then adding the variable that will give us these

327
00:18:49,680 --> 00:18:50,520
best parameters.

328
00:18:50,550 --> 00:18:55,770
That's just a classic way of printing something with, you know, a string indicating what we're printing.

329
00:18:56,280 --> 00:19:00,180
And so here we just need to add best parameters.

330
00:19:01,220 --> 00:19:02,420
And there we go, my friends.

331
00:19:02,780 --> 00:19:04,670
We're done with this implementation.

332
00:19:04,760 --> 00:19:05,650
Well, we're almost done.

333
00:19:05,660 --> 00:19:10,490
We just forgot to remove one parenthesis, which was, I think, coming from this format function.

334
00:19:10,820 --> 00:19:11,690
But there we go.

335
00:19:11,780 --> 00:19:13,970
Now, the implementation of grid search is done.

336
00:19:14,390 --> 00:19:17,030
So we're going to experiment this now.

337
00:19:17,240 --> 00:19:24,020
And I look forward to see which combination of all these hyper parameters will lead to the best accuracy.

338
00:19:24,050 --> 00:19:30,290
And of course, we'll compare that best accuracy to the relevant accuracy we got with the standard version

339
00:19:30,560 --> 00:19:36,620
of the kernel as your model, you know, with the RB of kernel and the default value of one for that

340
00:19:36,800 --> 00:19:41,420
regularization parameter C and then the same default value of the gamma parameter.

341
00:19:41,840 --> 00:19:44,450
So let's see, we have our data set.

342
00:19:44,540 --> 00:19:46,220
So we are ready to do this.

343
00:19:46,310 --> 00:19:47,270
Restart and run.

344
00:19:47,350 --> 00:19:47,640
Oh.

345
00:19:47,900 --> 00:19:48,830
So there we go.

346
00:19:49,010 --> 00:19:52,480
Three, two, one, go.

347
00:19:53,240 --> 00:19:54,650
And yes.

348
00:19:56,150 --> 00:19:56,480
All right.

349
00:19:56,480 --> 00:19:56,900
All right.

350
00:19:56,910 --> 00:19:59,000
So now the sales will be running.

351
00:19:59,030 --> 00:19:59,720
There we go.

352
00:20:00,020 --> 00:20:03,950
Grid search is now running and we are about to get.

353
00:20:04,520 --> 00:20:04,990
OK.

354
00:20:05,080 --> 00:20:07,370
So very, very interesting.

355
00:20:07,850 --> 00:20:15,770
We get a best accuracy of ninety point sixty seven percent, which is slightly better than this ninety

356
00:20:15,770 --> 00:20:16,850
point thirty three percent.

357
00:20:16,880 --> 00:20:23,120
But you will see in your future machinery projects that even a slight improvement of the accuracy can

358
00:20:23,120 --> 00:20:23,930
make a difference.

359
00:20:24,440 --> 00:20:32,630
And the best combination of parameters that led to that best accuracy is the following first regularization

360
00:20:32,630 --> 00:20:39,980
parameter C of all point five, meaning that indeed it was necessary to reduce a bit that hyper parameter

361
00:20:39,980 --> 00:20:42,320
C to reduce overfitting.

362
00:20:42,650 --> 00:20:49,010
Then of course, we got the best accuracy with an RB F colonel, meaning with the kernel as VM model

363
00:20:49,010 --> 00:20:51,120
instead of the classic linear as.

364
00:20:51,890 --> 00:20:57,410
And the best value of this gamma parameter, you know, when we have a Arby F kernel isn't it.

365
00:20:57,530 --> 00:20:58,400
Or point six.

366
00:20:58,670 --> 00:20:58,940
All right.

367
00:20:58,970 --> 00:21:01,100
So it was actually good to test all of them.

368
00:21:01,460 --> 00:21:03,950
And that's what we get as the best model.

369
00:21:05,030 --> 00:21:11,180
So now feel free to try some other values, you know, if you want, and maybe some other hyper parameters.

370
00:21:11,570 --> 00:21:17,240
And if you get a better accuracy than ninety point sixty seven percent, which I'm sure you can get

371
00:21:17,270 --> 00:21:20,030
easily, you know, I haven't tried all the combinations.

372
00:21:20,390 --> 00:21:25,160
Well, once again, feel free to share this in the Q&amp;A section or in private message.

373
00:21:25,190 --> 00:21:29,420
But Q&amp;A section is best so that other students can see how you did it.

374
00:21:29,990 --> 00:21:30,600
All right.

375
00:21:30,710 --> 00:21:35,570
And now we're going to move on to the very exciting final section of this caution of the bonus section,

376
00:21:35,810 --> 00:21:41,930
which is about extra boost and which will basically give you an extra super powerful machinery model

377
00:21:42,140 --> 00:21:46,370
that you can actually both apply to classification and regression.

378
00:21:46,490 --> 00:21:50,990
So you definitely want this as the final tool of your huge machine learning tool kit.

379
00:21:51,260 --> 00:21:54,050
So I look forward to implementing this final model with you.

380
00:21:54,320 --> 00:21:56,450
And until then, enjoy machine learning.
