WEBVTT
1
00:00:00.390 --> 00:00:01.290
Hello my friends.

2
00:00:01.290 --> 00:00:01.790
All right.

3
00:00:01.800 --> 00:00:06.780
Are you ready for that last tutorial of the hierarchical clustering implementation.

4
00:00:06.780 --> 00:00:07.470
Here we go.

5
00:00:07.470 --> 00:00:13.290
We now have the Dan program which gave us the optimal number of clusters which turned out to be five.

6
00:00:13.350 --> 00:00:20.760
And so now we're going to build train and run the hierarchical clustering to indeed identify five clusters.

7
00:00:20.760 --> 00:00:22.200
All right so let's do this.

8
00:00:22.200 --> 00:00:28.780
Let's create a new code cell here to build train and run this hierarchical clustering model.

9
00:00:28.830 --> 00:00:29.270
All right.

10
00:00:29.280 --> 00:00:36.510
So remember to build our Daniel gram we actually used the site by library because it contained this

11
00:00:36.570 --> 00:00:41.220
Daniel Graham function which directly returned the Daniel Graham which was perfect.

12
00:00:41.220 --> 00:00:46.610
But now in order to build the REIT eco classroom model with five clusters.

13
00:00:46.740 --> 00:00:52.410
Well we're going to go back to our best friend Sackett here and because indeed psychedelia and contains

14
00:00:52.620 --> 00:00:59.790
if you remember the cluster module which contains the ugly Marathi clustering class which is exactly

15
00:01:00.000 --> 00:01:06.420
you know the classic version of hierarchical clustering the one that you studied in the intuition lectures.

16
00:01:06.420 --> 00:01:10.470
All right so we're going to start from psychic learn.

17
00:01:10.650 --> 00:01:14.130
There we go from which we're gonna get access to cluster.

18
00:01:14.130 --> 00:01:21.630
The cluster module from which we're going to import that Anglo narrative clustering class.

19
00:01:21.630 --> 00:01:22.230
Perfect.

20
00:01:22.230 --> 00:01:24.310
Thank you so much Google collab.

21
00:01:24.390 --> 00:01:25.010
All right.

22
00:01:25.050 --> 00:01:32.160
Now the next natural step as usual as most of the time is to create an object or an instance of this

23
00:01:32.160 --> 00:01:32.640
class.

24
00:01:32.670 --> 00:01:39.630
And we're going to call it H C because this object will be nothing else than the hierarchical clustering

25
00:01:39.630 --> 00:01:40.420
model itself.

26
00:01:40.440 --> 00:01:43.110
You know with all its algorithm inside.

27
00:01:43.410 --> 00:01:50.940
So H.C. and therefore now we're gonna call that class to indeed create an instance of this class and

28
00:01:50.940 --> 00:01:52.980
then adding some parenthesis.

29
00:01:53.040 --> 00:01:56.470
And now let's see what we have to input.

30
00:01:56.490 --> 00:01:56.850
All right.

31
00:01:56.880 --> 00:01:59.120
So can you guess the first parameter.

32
00:01:59.130 --> 00:02:00.230
It's pretty obvious.

33
00:02:00.270 --> 00:02:06.120
It's actually the same as in the Caymans class the first parameter is the number of clusters we want

34
00:02:06.120 --> 00:02:10.280
to identify in our dataset and we know that it's five.

35
00:02:10.310 --> 00:02:15.900
But you know I'm very curious about that three you know that number three is the other option of the

36
00:02:16.020 --> 00:02:17.640
optimal number of clusters.

37
00:02:17.760 --> 00:02:20.220
So you know we'll try that at the end we will see what we get.

38
00:02:20.280 --> 00:02:26.020
But let's start first with n clusters equals five.

39
00:02:26.040 --> 00:02:27.760
All right five clusters.

40
00:02:27.840 --> 00:02:30.180
And now we need to add two more arguments.

41
00:02:30.240 --> 00:02:37.410
The second one is affinity which is simply the type of distance that will be computed in order to measure

42
00:02:37.410 --> 00:02:42.840
the variance within your clusters because then you're going to see that we're using in this ward method

43
00:02:42.870 --> 00:02:47.090
which corresponds to the minimization of the variance within your clusters.

44
00:02:47.100 --> 00:02:53.870
So for affinity here we're going to choose well the U CLI D and distance.

45
00:02:54.030 --> 00:02:58.560
And that last parameter that we need to add is of course that method.

46
00:02:58.590 --> 00:03:03.890
But this time the name of the parameter is not method it is directly linked cage.

47
00:03:03.900 --> 00:03:04.550
All right.

48
00:03:04.590 --> 00:03:10.770
And so linkage here should be equal you know there are several options but the one we recommend is the

49
00:03:10.770 --> 00:03:14.990
ward method which corresponds to the minimum variance method.

50
00:03:15.000 --> 00:03:15.600
All right.

51
00:03:15.600 --> 00:03:16.290
And that's it.

52
00:03:16.290 --> 00:03:24.000
So now we have are hierarchical classroom model but of course it is not yet trained or fitted to the

53
00:03:24.270 --> 00:03:25.190
data set.

54
00:03:25.260 --> 00:03:27.480
And so that's exactly our next step here.

55
00:03:27.480 --> 00:03:33.600
But remember that at the same time we want to create this dependent variable which contains for each

56
00:03:33.600 --> 00:03:39.310
customer the future class they will belong to you know the future cluster they will belong to.

57
00:03:39.540 --> 00:03:45.600
And therefore instead of only using the fit method which you know usually trains your machinery models

58
00:03:45.630 --> 00:03:46.730
on your data set.

59
00:03:46.800 --> 00:03:52.200
Well we're going to use the fit predict method which will not only train your hierarchical classroom

60
00:03:52.200 --> 00:03:58.140
model on your data set but also will create at the same time this dip in invaluable continue for each

61
00:03:58.140 --> 00:04:01.260
of the customers the cluster they belong to.

62
00:04:01.260 --> 00:04:01.790
All right.

63
00:04:01.800 --> 00:04:05.680
And speaking of this future created dependent variable.

64
00:04:05.760 --> 00:04:11.910
Well we're going to introduce here a new variable which we're going to call y underscore H.S. and this

65
00:04:11.910 --> 00:04:17.670
is exactly you know that dependent variable you see here with the five clusters.

66
00:04:17.670 --> 00:04:21.210
All right White H.S. and let's go back.

67
00:04:21.210 --> 00:04:29.400
That's why H.C. variable well will be equal to what is returned by this fit predict method not only

68
00:04:29.400 --> 00:04:35.760
training the hierarchical classroom model on the data set but also returning the clusters to which each

69
00:04:35.760 --> 00:04:37.420
customer belongs to.

70
00:04:37.470 --> 00:04:37.830
All right.

71
00:04:37.860 --> 00:04:44.750
And therefore what we have to do here is just take our H.S. object because that's from this object that

72
00:04:44.760 --> 00:04:53.960
we have to call this fit underscore predict method and inside we of course input x just x.

73
00:04:54.060 --> 00:04:54.300
Right.

74
00:04:54.300 --> 00:04:55.870
Because we just need to connect.

75
00:04:55.890 --> 00:05:04.420
We just need to fit r h the object a high IQ cluster model to the data set which is exactly x but only

76
00:05:04.570 --> 00:05:08.180
you know remember containing the two last features.

77
00:05:08.200 --> 00:05:10.520
The annual income and spending score.

78
00:05:10.570 --> 00:05:10.820
All right.

79
00:05:10.830 --> 00:05:14.190
So exactly the same as with came It's okay.

80
00:05:14.200 --> 00:05:15.070
And that's it.

81
00:05:15.070 --> 00:05:18.910
Once again thanks to our best friend Sackett learned well in only three lines of code.

82
00:05:18.910 --> 00:05:24.690
We build a train and run the hierarchical classroom model to identify five clusters.

83
00:05:24.850 --> 00:05:25.600
So let's do this.

84
00:05:25.600 --> 00:05:29.780
Let's run this cell and done.

85
00:05:29.830 --> 00:05:32.530
We have our model and it is already trained.

86
00:05:32.710 --> 00:05:42.820
So now let's actually do a little print to see you know that created dependent variable Y H C and let's

87
00:05:42.940 --> 00:05:45.720
play this cell and we'll see what we get.

88
00:05:45.730 --> 00:05:46.060
All right.

89
00:05:46.090 --> 00:05:53.500
So remember that the class numbers don't go from 1 to 5 but from 0 to 4 because indexes in Python start

90
00:05:53.500 --> 00:05:54.440
from zero.

91
00:05:54.460 --> 00:05:55.560
So let's see.

92
00:05:55.560 --> 00:05:57.280
Let's open this again.

93
00:05:57.280 --> 00:06:03.580
What these numbers mean is that well the first customer customer lady number one belongs to the last

94
00:06:03.600 --> 00:06:05.730
cluster you know cluster number 5.

95
00:06:05.770 --> 00:06:09.090
Then the second customer belongs to cluster number 4.

96
00:06:09.250 --> 00:06:13.930
Third customer belongs to class number 5 or cluster of index 4 as you want.

97
00:06:13.930 --> 00:06:17.170
Then customize number 4 belongs to cluster number 4.

98
00:06:17.230 --> 00:06:19.300
All right so this is how you should read it.

99
00:06:19.300 --> 00:06:22.320
And the last customer in this data set.

100
00:06:22.450 --> 00:06:26.930
You know the customer actually number 200.

101
00:06:26.950 --> 00:06:33.670
This one of age 30 and earning a high salary and spending a lot actually in the mall belongs to the

102
00:06:33.910 --> 00:06:36.780
third cluster or cluster of index 2.

103
00:06:37.090 --> 00:06:37.330
All right.

104
00:06:37.360 --> 00:06:38.810
So that's how you should read it.

105
00:06:38.870 --> 00:06:39.650
And now.

106
00:06:39.670 --> 00:06:45.490
Now let's visualize the final cluster as you know now that we have this dependent variable that we just

107
00:06:45.490 --> 00:06:49.060
created through the hierarchical clustering process.

108
00:06:49.300 --> 00:06:50.350
And so there you go.

109
00:06:50.350 --> 00:06:51.840
I'm going to close this.

110
00:06:51.910 --> 00:06:55.010
We're going to run that sale too indeed.

111
00:06:55.060 --> 00:06:55.760
Fine.

112
00:06:55.780 --> 00:07:03.790
Actually you know the same clusters as with k means with remember this cluster representing the customers

113
00:07:03.790 --> 00:07:09.520
that earn a low salary and don't spend much in the mall and therefore we should just not target them

114
00:07:09.520 --> 00:07:14.710
too much because we want to be socially responsible and don't push them to consume too much.

115
00:07:14.770 --> 00:07:15.480
Right.

116
00:07:15.490 --> 00:07:22.180
However this cluster is the cluster of the customers having a high annual income but a low spending

117
00:07:22.180 --> 00:07:28.300
score and therefore we want to target these customers to offer them some more attractive deals in order

118
00:07:28.300 --> 00:07:33.810
to incentivize them to spend more and more because otherwise the more is missing out.

119
00:07:33.940 --> 00:07:40.390
Then this cluster is the cluster of customers having a low annual income or high spending score and

120
00:07:40.390 --> 00:07:46.180
therefore with these customers you know you want to be the maximum socially responsible and maybe protect

121
00:07:46.180 --> 00:07:50.260
them from spending too much and potentially more than they could afford.

122
00:07:50.260 --> 00:07:55.210
So to these customers we for example want to reduce any kind of advertising.

123
00:07:55.210 --> 00:07:59.680
Then we have this cluster which is the best cluster you know the one we want to target the most because

124
00:07:59.680 --> 00:08:05.190
it is a cluster of the customers having a high annual income and at the same time spending a lot.

125
00:08:05.200 --> 00:08:11.680
So we definitely want to target these customers to you know offer them the new products new deals because

126
00:08:11.680 --> 00:08:15.910
we know that we have a high chance to have a high conversion rates with them all right.

127
00:08:16.270 --> 00:08:20.680
And then we have this cluster which is the average cluster you know average annual income and average

128
00:08:20.680 --> 00:08:21.540
spending score.

129
00:08:21.810 --> 00:08:25.650
And for this cluster Well we don't have much specific to do all right.

130
00:08:25.750 --> 00:08:31.990
So these are the same five classes as with k means but now I'm very curious to see what we get with

131
00:08:32.110 --> 00:08:39.940
three clusters and therefore what we're gonna do is try now and clusters call three here but then be

132
00:08:39.940 --> 00:08:45.820
careful we need to actually remove two lines here when you know visualizing the clusters because each

133
00:08:45.820 --> 00:08:51.370
scatter plot here corresponds to one cluster and therefore now since we're about to have three clusters

134
00:08:51.610 --> 00:08:53.850
well we need to remove two classes here.

135
00:08:53.860 --> 00:08:58.140
So we're going to remove cluster 4 and cluster 5 right.

136
00:08:58.300 --> 00:09:03.400
And therefore we're just going to end up with cluster one cluster two interest the three of colors red

137
00:09:03.400 --> 00:09:04.450
blue and green.

138
00:09:04.510 --> 00:09:05.350
All right.

139
00:09:05.350 --> 00:09:08.650
So let's just run this again.

140
00:09:08.710 --> 00:09:15.190
You know we can leave the previous cells and just rerun this one to indeed get a new hierarchical classroom

141
00:09:15.190 --> 00:09:21.400
model this time identifying three clusters we can print this again in order to get the new dependent

142
00:09:21.400 --> 00:09:27.730
variable with this time indeed three clusters the cluster of index 0 which seems to contain most of

143
00:09:27.730 --> 00:09:34.420
the first customers then the cluster of the next one the second closer and the cluster index to this

144
00:09:34.420 --> 00:09:35.290
third cluster.

145
00:09:35.290 --> 00:09:35.910
Okay.

146
00:09:36.040 --> 00:09:39.990
And now I'm really curious to see what we get when visualizing the clusters.

147
00:09:40.000 --> 00:09:40.750
Here we go.

148
00:09:40.810 --> 00:09:46.730
We just have to play this cell again and let's see what we get all right.

149
00:09:46.740 --> 00:09:47.130
OK.

150
00:09:47.130 --> 00:09:48.480
So yeah really.

151
00:09:48.600 --> 00:09:54.750
Five clusters was actually a better number because here were three clusters well tomorrow just put all

152
00:09:54.750 --> 00:10:01.110
these customers you know actually the low income customers with both a low spending score or high spinning

153
00:10:01.110 --> 00:10:02.900
score into a same cluster.

154
00:10:02.910 --> 00:10:08.390
Also taking the average one and then we have these two other clusters the high spending score with the

155
00:10:08.390 --> 00:10:13.320
high annual income and the low spending score with the high annual income.

156
00:10:13.320 --> 00:10:19.830
And you know this still actually makes some sense because remember that the clusters of customers that

157
00:10:19.830 --> 00:10:26.550
we really want to target after all are this one and this one and this you know is something we don't

158
00:10:26.550 --> 00:10:32.600
really want to target but maybe protect you know you know as per our social responsibility.

159
00:10:32.610 --> 00:10:34.880
So this actually still makes kind of sense.

160
00:10:34.920 --> 00:10:41.940
And we indeed end up with the same focus of targeting these two important customers that can boost indeed

161
00:10:42.120 --> 00:10:43.410
the sales.

162
00:10:43.410 --> 00:10:43.860
All right.

163
00:10:43.890 --> 00:10:45.490
So that was very interesting.

164
00:10:45.520 --> 00:10:50.250
I I didn't expect actually to show you the result with three clusters.

165
00:10:50.250 --> 00:10:56.220
I was just curious to see and that's very interesting because indeed we end up with kind of the same

166
00:10:56.400 --> 00:11:00.670
final marketing decisions of targeting our customers.

167
00:11:00.690 --> 00:11:01.140
All right.

168
00:11:01.140 --> 00:11:07.140
So I hope you enjoyed clustering and I hope you are convinced that indeed you should have these two

169
00:11:07.320 --> 00:11:12.960
clustering models key means in hierarchical clustering because indeed by trying to two you can get some

170
00:11:13.110 --> 00:11:19.200
extra insights which is exactly what happened here when building hierarchical clustering with which

171
00:11:19.410 --> 00:11:24.600
we got this extra insight of trying the moral with three clusters.

172
00:11:24.600 --> 00:11:30.570
And so if by any chance you're wondering which model should I choose between K Means and hierarchical

173
00:11:30.570 --> 00:11:31.440
clustering.

174
00:11:31.440 --> 00:11:36.930
Well just run the two you know it would just take a few minutes and let's see if you get the same optimal

175
00:11:36.930 --> 00:11:40.500
number of clusters and if not well you can try the two options.

176
00:11:40.500 --> 00:11:45.810
You know if you get to optimal numbers of clusters between which you hesitate you try the two and then

177
00:11:46.200 --> 00:11:49.450
you get the final insights to help you make the right decision.

178
00:11:49.470 --> 00:11:50.520
Idiot.

179
00:11:50.610 --> 00:11:51.390
All right.

180
00:11:51.390 --> 00:11:53.070
So good for clustering.

181
00:11:53.070 --> 00:11:55.380
Now we're going to move on to the next part.

182
00:11:55.380 --> 00:12:01.860
Part Five on association rule learning it's gonna be pretty exciting we're gonna work on two new models

183
00:12:02.010 --> 00:12:08.700
priory and ELA and so I will either meet you in this next part or if you want to learn or as well I

184
00:12:08.700 --> 00:12:13.940
will meet you in the next section to build the hierarchical clustering model in R.

185
00:12:14.070 --> 00:12:17.920
And either way I look forward to building another model with you.

186
00:12:17.940 --> 00:12:19.800
And until then enjoy machine learning.
