WEBVTT
1
00:00:00.150 --> 00:00:00.990
Hello, my friends.

2
00:00:01.110 --> 00:00:06.720
All right, are you ready to start this big implementation of your very first artificial brain?

3
00:00:06.990 --> 00:00:08.730
Well, I'm definitely ready.

4
00:00:08.790 --> 00:00:09.720
So let's do this.

5
00:00:09.810 --> 00:00:11.070
Let's mash this together.

6
00:00:11.700 --> 00:00:12.030
All right.

7
00:00:12.060 --> 00:00:17.940
So we're going to start with the data repricing phase, which we will tackle in the same tutorial because

8
00:00:18.060 --> 00:00:20.400
we quickly want to get to the interesting stuff.

9
00:00:20.520 --> 00:00:22.620
So let's do this efficiently.

10
00:00:22.670 --> 00:00:27.430
Thanks to our data pricing template, but also our data pricing tool kit.

11
00:00:27.840 --> 00:00:32.520
And therefore, the first thing we're gonna do here is to import the libraries.

12
00:00:32.660 --> 00:00:35.250
So we're going to create a new Kotal here.

13
00:00:35.610 --> 00:00:42.690
We're gonna go into our data pricing template to steal the cell we want, meaning this one and get it

14
00:00:42.690 --> 00:00:44.970
back into our implementation.

15
00:00:45.000 --> 00:00:45.780
The first cell.

16
00:00:46.140 --> 00:00:46.440
All right.

17
00:00:46.500 --> 00:00:47.160
That's the first thing.

18
00:00:47.280 --> 00:00:50.040
However, I just want to show you something extra.

19
00:00:50.250 --> 00:00:52.840
It is about the beauty of Google Kulab.

20
00:00:53.130 --> 00:00:58.890
I want to show you that indeed, Tensor Flow 2.0 is already preinstalled in Google.

21
00:00:58.890 --> 00:01:03.060
Kulab You know, in any Google Kulab notebook you will ever open.

22
00:01:03.120 --> 00:01:08.970
So the way for me to show you this is to first import tends to flow because, OK.

23
00:01:09.140 --> 00:01:14.370
It is already preinstalled as a library inside a notebook, but we still need to import it.

24
00:01:14.700 --> 00:01:21.000
And in fact, here, since we will actually won't use matplotlib, we're just gonna delete this import

25
00:01:21.000 --> 00:01:21.750
in this library.

26
00:01:22.050 --> 00:01:26.160
And then just add as a third library here will tend to flow.

27
00:01:26.400 --> 00:01:31.080
And the way to import tenses, who is dealt with indeed import then the name of the library, which

28
00:01:31.080 --> 00:01:32.700
is Tenzer flow of course.

29
00:01:33.180 --> 00:01:37.150
And then we add a shortcut, simple one like T.F..

30
00:01:37.320 --> 00:01:38.010
The classic one.

31
00:01:38.510 --> 00:01:38.900
OK.

32
00:01:39.240 --> 00:01:41.760
And now I'm gonna create a new code cell.

33
00:01:42.060 --> 00:01:50.750
And inside I'm going to enter the following T.F. Dot double underscore version, double underscore again.

34
00:01:51.090 --> 00:01:57.010
And this will simply print the version of tensor flow we're using, which I just want to show you is

35
00:01:57.010 --> 00:01:59.940
indeed Tenzer flow to write the brand new tensor flow.

36
00:02:00.420 --> 00:02:01.560
So let's do this.

37
00:02:01.740 --> 00:02:03.960
First, we need to execute the sale on this one.

38
00:02:04.230 --> 00:02:09.210
But remember, if we execute the sale now, this will take some time because actually the notebook is

39
00:02:09.210 --> 00:02:10.590
not running yet.

40
00:02:10.710 --> 00:02:13.990
And the way to run this is just to click this folder here.

41
00:02:14.100 --> 00:02:14.520
Right.

42
00:02:14.580 --> 00:02:19.640
And it is now that it will connect to a runtime to enable file browsing, but mostly to, you know,

43
00:02:19.890 --> 00:02:21.570
start running the notebook.

44
00:02:21.690 --> 00:02:22.010
OK.

45
00:02:22.350 --> 00:02:26.390
And at the same time, let's take the opportunity to upload the data set.

46
00:02:26.580 --> 00:02:28.470
So please go to your machine learning.

47
00:02:28.520 --> 00:02:33.630
Is that code and datasets folder which had to download either in the previous tutorial or at the beginning

48
00:02:33.630 --> 00:02:35.430
of each practical activity.

49
00:02:35.670 --> 00:02:37.530
So there we go inside then port.

50
00:02:37.590 --> 00:02:42.720
A deepening almost close to the end, by the way, must be very excited, almost seeing the tip of the

51
00:02:42.720 --> 00:02:43.200
tunnel.

52
00:02:43.630 --> 00:02:46.650
Then let's go through this section on artificial neural network.

53
00:02:46.950 --> 00:02:50.490
Let's go to Python and let's select this dataset churn modelling.

54
00:02:50.580 --> 00:02:52.080
That's USV open.

55
00:02:52.610 --> 00:02:53.180
Okay.

56
00:02:53.430 --> 00:02:54.510
And there we go.

57
00:02:54.630 --> 00:02:55.500
Now we have everything.

58
00:02:55.500 --> 00:02:56.240
We had a dataset.

59
00:02:56.460 --> 00:02:58.980
And besides, our notebook is already running.

60
00:02:59.310 --> 00:03:03.450
As you can notice, it takes a little bit more time than usual because, you know, the dataset is this

61
00:03:03.450 --> 00:03:05.790
time more real world and mostly bigger.

62
00:03:05.910 --> 00:03:06.200
OK.

63
00:03:06.780 --> 00:03:06.930
All right.

64
00:03:06.960 --> 00:03:07.510
So let's do this.

65
00:03:07.530 --> 00:03:10.720
Let's run this first cell here to important libraries.

66
00:03:10.770 --> 00:03:13.260
This time, none by pendas intensive flow.

67
00:03:13.590 --> 00:03:20.760
And now let's play this cell to indeed reassure ourselves that the tensor flow version we're gonna be

68
00:03:20.760 --> 00:03:28.070
working with is two point 2.0, basically tensor flow two, which is so much better than tensor flow

69
00:03:28.110 --> 00:03:28.350
one.

70
00:03:28.500 --> 00:03:30.150
I'm so happy about their new version.

71
00:03:30.750 --> 00:03:31.380
OK, great.

72
00:03:31.530 --> 00:03:33.180
So it's good to have the confirmation.

73
00:03:33.310 --> 00:03:33.840
And now.

74
00:03:33.930 --> 00:03:36.620
Now let's tackle this part one data preprocessing.

75
00:03:36.660 --> 00:03:38.340
We're gonna do this efficiently.

76
00:03:38.520 --> 00:03:41.060
Thanks to our data progressing template and toolkit.

77
00:03:41.340 --> 00:03:46.080
So let's create first and you coattail to import the data set, which we already have in a notebook.

78
00:03:46.470 --> 00:03:47.010
Perfect.

79
00:03:47.280 --> 00:03:54.570
So let's go to our data pricing template and let's now steal this second cell to import the data set.

80
00:03:54.930 --> 00:03:55.440
All right.

81
00:03:55.530 --> 00:03:58.230
Let's go back here and let's face that inside.

82
00:03:58.380 --> 00:04:01.860
And now, of course, the question is, what do we need to replace?

83
00:04:02.100 --> 00:04:08.100
Well, the obvious first change we need to do is the name of the dataset, which this time is not data

84
00:04:08.190 --> 00:04:12.620
that C as V, but churn on score modeling.

85
00:04:13.170 --> 00:04:13.850
That's easy.

86
00:04:14.220 --> 00:04:14.700
Great.

87
00:04:15.150 --> 00:04:16.360
Then let's look at the rows.

88
00:04:16.380 --> 00:04:17.100
One by one.

89
00:04:17.460 --> 00:04:18.930
This one is OK.

90
00:04:19.260 --> 00:04:20.700
Now what about this one?

91
00:04:21.120 --> 00:04:26.910
This line of code creates the matrix features X and the way it does this is it takes all the columns

92
00:04:26.940 --> 00:04:28.080
except the last one.

93
00:04:28.560 --> 00:04:31.960
But let's actually have a look again at our data.

94
00:04:32.550 --> 00:04:33.000
Right.

95
00:04:33.390 --> 00:04:40.140
We noticed when I described to you the dataset that the first columns are actually irrelevant in the

96
00:04:40.140 --> 00:04:44.340
sense that they won't help to predict the outcome of the dependent variable.

97
00:04:44.610 --> 00:04:49.760
And these columns are, you know, the non helpful columns are obviously this one.

98
00:04:49.770 --> 00:04:52.040
This just gives the row number of this dataset.

99
00:04:52.170 --> 00:04:56.250
So we clearly don't want to include it then customer idea as well.

100
00:04:56.340 --> 00:04:56.680
Right.

101
00:04:56.970 --> 00:04:59.850
The customer idea is just a key identifier of.

102
00:04:59.970 --> 00:05:04.980
Each customer, because, you know, each row corresponds to a different customer.

103
00:05:05.070 --> 00:05:10.320
So, of course, the customer I.D. has absolutely no impact on the dependent variable exited.

104
00:05:10.590 --> 00:05:13.600
So we will also exclude that column.

105
00:05:13.770 --> 00:05:16.950
We don't have to, you know, the new one that work, we'll just figure it out.

106
00:05:17.010 --> 00:05:20.730
But let's just ease the learning process of our future new network.

107
00:05:20.790 --> 00:05:21.120
Right.

108
00:05:21.210 --> 00:05:22.620
We're all on the same boat here.

109
00:05:23.160 --> 00:05:25.020
OK, then what about the surname?

110
00:05:25.200 --> 00:05:30.840
Does the surname have an impact on whether the customer will stay in or leave the bank?

111
00:05:31.070 --> 00:05:32.460
Well, absolutely not.

112
00:05:32.550 --> 00:05:32.910
Right.

113
00:05:32.930 --> 00:05:38.430
Surname, of course, has no impact on the decision of a customer to stay in or leave the bank.

114
00:05:38.700 --> 00:05:44.190
So we will also exclude this column and then all the rest and all the other features here.

115
00:05:44.340 --> 00:05:44.700
Look.

116
00:05:44.730 --> 00:05:45.180
Fine.

117
00:05:45.210 --> 00:05:51.750
They might have an impact on the dependent variable, meaning they might help to predict if each customer

118
00:05:51.750 --> 00:05:54.870
will stay in the bank or leave the bank.

119
00:05:55.050 --> 00:05:55.300
OK.

120
00:05:55.380 --> 00:06:01.070
So we will definitely keep all the other ones, meaning all the features starting from this one two

121
00:06:01.110 --> 00:06:01.770
credit score.

122
00:06:02.490 --> 00:06:09.570
And so here in our implementation, instead of taking all the columns except the last one, well, we

123
00:06:09.570 --> 00:06:14.280
will take all the columns starting from this one except the last one.

124
00:06:14.670 --> 00:06:18.330
Meaning all the columns from credit score up to estimated salary.

125
00:06:18.750 --> 00:06:25.230
And a way to do this is still to keep that upper bound of the range, you know, finishing at the one

126
00:06:25.230 --> 00:06:26.730
before last column.

127
00:06:26.940 --> 00:06:27.240
Right.

128
00:06:27.270 --> 00:06:29.130
You know, that's exactly the upper bound.

129
00:06:29.370 --> 00:06:30.270
That's the range.

130
00:06:30.540 --> 00:06:36.940
But at the left of this range, we won't specify nothing, which means the first goal in the first index.

131
00:06:37.320 --> 00:06:41.760
But instead, we will specify the index of the column.

132
00:06:41.970 --> 00:06:42.870
We want to stop with.

133
00:06:42.900 --> 00:06:44.520
Which is the credit score.

134
00:06:44.760 --> 00:06:45.060
Right.

135
00:06:45.090 --> 00:06:46.380
We know we want to start from here.

136
00:06:46.380 --> 00:06:49.920
And therefore, now the question is, what is the index of that column?

137
00:06:50.160 --> 00:06:51.270
Well, let's see.

138
00:06:51.350 --> 00:06:52.950
Indexes and bitin start from zero.

139
00:06:52.980 --> 00:06:55.230
So this has index zero then?

140
00:06:55.230 --> 00:06:56.340
This has indexed one.

141
00:06:56.550 --> 00:06:57.430
This has index two.

142
00:06:57.480 --> 00:06:59.130
And this has the next three.

143
00:06:59.310 --> 00:07:04.140
And therefore, here, instead of specifying nothing here as a lower bound of the range.

144
00:07:04.170 --> 00:07:10.710
Well, we will specify the index three so that we can take all the columns starting from the column

145
00:07:10.710 --> 00:07:16.220
of index three up to the one before last and taking all the rows, all the values of the dataset.

146
00:07:16.650 --> 00:07:19.770
And this will create a relevant matrix of features.

147
00:07:20.370 --> 00:07:20.910
Perfect.

148
00:07:21.030 --> 00:07:22.440
So this line of code is done.

149
00:07:22.800 --> 00:07:24.150
Now, what about the next one?

150
00:07:24.330 --> 00:07:26.280
Well, obviously the next one is fine.

151
00:07:26.550 --> 00:07:32.430
It will just take the last column of this dataset, which is exactly what we want for our dependent

152
00:07:32.430 --> 00:07:33.740
variable exited.

153
00:07:34.170 --> 00:07:35.130
So I'll go here.

154
00:07:35.220 --> 00:07:36.240
Nothing to change.

155
00:07:36.450 --> 00:07:42.810
We can just play this cell and we will have our data set, matrix of features and our dependent, viable

156
00:07:42.810 --> 00:07:43.260
vector.

157
00:07:43.530 --> 00:07:44.220
Let's check it out.

158
00:07:44.220 --> 00:07:47.450
Actually, let's create two new code cells, right.

159
00:07:47.760 --> 00:07:55.740
One where we will print the matrix of features X and one where we will print the dependent in viable

160
00:07:55.740 --> 00:07:56.400
vector Y.

161
00:07:56.790 --> 00:07:57.300
Perfect.

162
00:07:57.630 --> 00:07:57.840
All right.

163
00:07:57.840 --> 00:07:58.650
So let's do this now.

164
00:07:58.650 --> 00:08:01.750
Actually, let's play for this self print.

165
00:08:02.160 --> 00:08:04.260
The matrix of features X and there we go.

166
00:08:04.560 --> 00:08:08.040
We have indeed all the features starting from the credit score.

167
00:08:08.100 --> 00:08:09.240
This is a great score.

168
00:08:09.570 --> 00:08:11.450
Then you know, the country of residence.

169
00:08:11.730 --> 00:08:14.820
Then the gender and all the other ones, you know, has credit card.

170
00:08:14.850 --> 00:08:16.440
Yes or no is active.

171
00:08:16.650 --> 00:08:19.650
And the last one is the estimated salary.

172
00:08:19.850 --> 00:08:20.160
OK.

173
00:08:20.580 --> 00:08:21.770
So we have all these features.

174
00:08:21.780 --> 00:08:22.340
Perfect.

175
00:08:22.470 --> 00:08:27.460
And of course, we don't have the dependent variable values because they are right here in Y.

176
00:08:27.900 --> 00:08:28.920
And there we go.

177
00:08:29.220 --> 00:08:33.420
These are all the decisions of the customers to stay or leave in the bank.

178
00:08:33.440 --> 00:08:40.050
So, of course, this one here corresponds to this customer here, which obviously has decided to leave

179
00:08:40.050 --> 00:08:40.500
the bank.

180
00:08:40.560 --> 00:08:40.830
Right.

181
00:08:40.860 --> 00:08:44.190
This is actually this same one here, exited one.

182
00:08:44.730 --> 00:08:51.720
And then, well, this second customer has decided to stay in the bank and corresponds to this one.

183
00:08:52.050 --> 00:08:52.350
Right.

184
00:08:52.410 --> 00:08:54.510
Which is exactly this one as well.

185
00:08:54.660 --> 00:08:54.990
OK.

186
00:08:55.260 --> 00:08:55.980
This customer.

187
00:08:56.730 --> 00:08:57.060
All right.

188
00:08:57.150 --> 00:08:58.170
All good so far.

189
00:08:58.280 --> 00:09:02.310
First step of the data processing phase done successfully.

190
00:09:02.580 --> 00:09:08.490
And now let's move on to the more advanced steps of our data pricing phase, which is about encoding

191
00:09:08.730 --> 00:09:09.960
the category called data.

192
00:09:10.200 --> 00:09:10.620
Right.

193
00:09:10.770 --> 00:09:14.340
Of course, we noticed that there are two category variables.

194
00:09:14.640 --> 00:09:18.660
This first one, giving the country of residence of the customers.

195
00:09:18.840 --> 00:09:21.660
And the second one giving the gender of the customers.

196
00:09:21.930 --> 00:09:28.890
So we'll have to do some encoding work here to encode these categorical data in either simple labels,

197
00:09:29.130 --> 00:09:35.550
you know, zero and one for the gender or some one had encoding for this categorical variables in which

198
00:09:35.610 --> 00:09:41.010
indeed there is no relationship order between these values, you know, between these categories, France,

199
00:09:41.100 --> 00:09:42.180
Spain and Germany.

200
00:09:42.390 --> 00:09:42.640
OK.

201
00:09:43.080 --> 00:09:43.830
So let's do this.

202
00:09:43.950 --> 00:09:47.700
Let's start first with the label, including of the gender column.

203
00:09:47.820 --> 00:09:49.190
So let's create a new coattail.

204
00:09:49.590 --> 00:09:53.900
And now, of course, to do it efficiently, we're gonna go into our data repricing.

205
00:09:53.920 --> 00:09:54.320
Took it.

206
00:09:54.810 --> 00:09:57.270
We're going to scroll down to find.

207
00:09:57.330 --> 00:09:59.690
By the way, there is no missing data in the data said.

208
00:09:59.870 --> 00:10:03.150
I check them, and in reality, you would also have to check them.

209
00:10:03.390 --> 00:10:04.020
But all good.

210
00:10:04.080 --> 00:10:09.690
We don't have to take care of any missing data so we can directly move to encoding categorical data.

211
00:10:10.020 --> 00:10:16.440
And now, since we're taking care of label and coding the gender column, well, we're going to take

212
00:10:16.440 --> 00:10:16.860
this.

213
00:10:17.040 --> 00:10:20.650
That's exactly the tool we need to perform label encoding.

214
00:10:20.650 --> 00:10:25.170
So I'm sealing this Kotel now and adding it inside our notebook.

215
00:10:25.170 --> 00:10:26.520
Here are implementation.

216
00:10:26.880 --> 00:10:32.130
But remember that in our data processing tool kit, we did this on the deben viable vector.

217
00:10:32.340 --> 00:10:38.670
But now we want to do it on this specific column of the matrix of Features X and therefore, what we

218
00:10:38.730 --> 00:10:45.570
only need to replace here is this Y by that specific column of the matrix of features, X to which we

219
00:10:45.570 --> 00:10:47.430
want to apply label encoding.

220
00:10:47.820 --> 00:10:48.180
And so.

221
00:10:48.230 --> 00:10:51.120
Well, now the question is, how can we get this column?

222
00:10:51.330 --> 00:10:55.290
Well, we just need to get the index and then call X with that index.

223
00:10:55.500 --> 00:10:56.850
And so while there we go.

224
00:10:57.060 --> 00:10:58.540
That's the first column of X.

225
00:10:58.590 --> 00:10:59.530
It hasn't Exito.

226
00:10:59.820 --> 00:11:01.200
That's the second column of X.

227
00:11:01.200 --> 00:11:02.060
It has indexed one.

228
00:11:02.370 --> 00:11:05.280
And that's the third column of X, which has index two.

229
00:11:05.700 --> 00:11:12.780
And therefore, here we simply tend to replace Y by R matrix of features, X of which we're gonna take

230
00:11:12.930 --> 00:11:13.770
all the rows.

231
00:11:13.890 --> 00:11:18.030
And I'm taking them with this column, you know, which means a range in Python.

232
00:11:18.300 --> 00:11:22.680
And then to take the column we want, meaning the gender column, which has index two.

233
00:11:23.010 --> 00:11:27.160
Well, I just need to add here after the comma, the index too.

234
00:11:27.390 --> 00:11:30.900
So there it will take all the rows, but only the column of index two.

235
00:11:31.320 --> 00:11:38.040
And now, of course, we need to take this and paste that inside the fit transfer method called from

236
00:11:38.070 --> 00:11:43.170
our L Object, which is an instance of the label encoder class and done.

237
00:11:43.440 --> 00:11:49.290
We just performed successfully label, including to the gender column of our Matrix of Features X..

238
00:11:49.380 --> 00:11:57.090
Let's make sure it's the case by creating a new code so here and do a new print of the matrix of features

239
00:11:57.120 --> 00:11:57.450
X.

240
00:11:57.870 --> 00:12:01.020
Let's run the cell, you know first.

241
00:12:01.440 --> 00:12:02.910
All right, good.

242
00:12:03.240 --> 00:12:08.160
And now let's print X and let's just make sure that we no longer see female, female, female, female,

243
00:12:08.160 --> 00:12:08.880
male, female.

244
00:12:09.150 --> 00:12:14.880
But whatever encoding there was, which probably will be one for female or, you know, zero for female

245
00:12:15.000 --> 00:12:16.910
and zero for male or one for male.

246
00:12:16.950 --> 00:12:17.940
Let's see what they did.

247
00:12:18.260 --> 00:12:18.770
All right.

248
00:12:18.900 --> 00:12:19.620
And there we go.

249
00:12:19.740 --> 00:12:19.980
Right.

250
00:12:20.010 --> 00:12:22.260
That's the new column after the label encoding.

251
00:12:22.500 --> 00:12:28.230
And so female was encoded into zero and male was encoded into one.

252
00:12:28.410 --> 00:12:31.770
That's, of course, a random decision of the machine to choose this.

253
00:12:31.830 --> 00:12:33.200
Integers, associations.

254
00:12:33.330 --> 00:12:34.020
So all good.

255
00:12:34.440 --> 00:12:37.020
Now, this column is, well, label encoded.

256
00:12:37.320 --> 00:12:42.720
And now we're going to proceed to the one hot encoding of the geography column.

257
00:12:42.990 --> 00:12:47.820
And this time, we have indeed to perform one hutting coding because there is no auto relationship between

258
00:12:47.820 --> 00:12:49.230
France, Spain and Germany.

259
00:12:49.260 --> 00:12:54.150
So we couldn't, you know, encode France into zero than Spain into one and Jimin into three.

260
00:12:54.330 --> 00:12:56.610
We have to perform one HUTZ encoding instead.

261
00:12:56.940 --> 00:12:58.050
And so let's do this.

262
00:12:58.410 --> 00:13:01.460
Let's go back toward data pricing tool kit.

263
00:13:01.830 --> 00:13:08.610
Let's take that cell this time, which is executive cell that perform one hertz encoding.

264
00:13:08.940 --> 00:13:14.730
And let's based it inside a new code cell to one hutting code.

265
00:13:14.850 --> 00:13:15.990
The geography column.

266
00:13:16.800 --> 00:13:17.250
All right.

267
00:13:17.460 --> 00:13:24.390
Now, the question is, of course, what do we have to replace or change in that cell to indeed perform

268
00:13:24.390 --> 00:13:26.880
one hutz, including on the geography column?

269
00:13:27.240 --> 00:13:33.960
Well, remember, the only thing that you have to change inside this code is that index of the column

270
00:13:33.960 --> 00:13:36.270
you want to apply one hot encoding on.

271
00:13:36.540 --> 00:13:36.840
Right.

272
00:13:37.260 --> 00:13:39.480
And remember that in our dataset.

273
00:13:39.510 --> 00:13:39.720
Yes.

274
00:13:39.720 --> 00:13:42.080
Which fell of our 41 day pricing.

275
00:13:42.390 --> 00:13:46.380
Well, the category called variable with the three different states was in the first column.

276
00:13:46.410 --> 00:13:48.630
That's why we had index zero here.

277
00:13:48.720 --> 00:13:52.290
But this time, this column is actually the second column.

278
00:13:52.380 --> 00:13:53.640
Therefore, it has indexed one.

279
00:13:53.940 --> 00:13:58.860
And therefore, very simply, we just need to replace zero here by one.

280
00:13:59.610 --> 00:14:00.000
OK.

281
00:14:00.210 --> 00:14:00.810
And that's it.

282
00:14:01.050 --> 00:14:02.970
All the rest will be done automatically.

283
00:14:03.240 --> 00:14:04.170
Let me show you this.

284
00:14:04.380 --> 00:14:05.970
Let's play that cell.

285
00:14:06.390 --> 00:14:10.570
And now let's create a new code cell to print again X.

286
00:14:11.130 --> 00:14:12.210
All right, good.

287
00:14:12.390 --> 00:14:16.740
And now let's play that cell and see what X has become.

288
00:14:17.130 --> 00:14:22.440
And indeed, well, remember, when we perform one hutting coding while the dummy variables are actually

289
00:14:22.440 --> 00:14:28.170
moved to the first columns of the Matrix features, we have them exactly here, you know, in the three

290
00:14:28.170 --> 00:14:28.980
first columns.

291
00:14:29.310 --> 00:14:32.370
So let's see let's see how the one hutting going was done.

292
00:14:32.730 --> 00:14:38.530
This is the first combination of dummy variables which corresponds to France.

293
00:14:38.580 --> 00:14:40.470
You know, these are the same rows here.

294
00:14:40.740 --> 00:14:45.180
And therefore, France was encoded into one zero zero.

295
00:14:45.660 --> 00:14:50.160
Now, Spain was encoded in two zero zero one.

296
00:14:50.490 --> 00:14:57.090
And finally, Germany was encoded into, well, this one zero one zero.

297
00:14:57.440 --> 00:14:57.740
OK.

298
00:14:58.200 --> 00:14:59.340
So that's our one.

299
00:14:59.600 --> 00:15:03.860
Coding, then we no longer see the gender column, but nor is it is still here.

300
00:15:04.070 --> 00:15:04.910
And so perfect.

301
00:15:05.090 --> 00:15:09.800
One Hutting coding was not only done successfully, but also went on efficiently.

302
00:15:09.860 --> 00:15:13.100
Thanks to our delivery pricing tool kit and template.

303
00:15:13.730 --> 00:15:14.090
Good.

304
00:15:14.120 --> 00:15:18.910
Now let's move onto the next step, which is to split the dataset into the training set and test it.

305
00:15:19.240 --> 00:15:21.950
And once again, we're gonna do that so efficiently.

306
00:15:22.280 --> 00:15:25.100
Thanks to this time or data pre pricing template.

307
00:15:25.490 --> 00:15:31.370
Indeed, we have to steal now this cell that splits the dataset into the training set and test set.

308
00:15:31.820 --> 00:15:38.390
So let's base that back into our implementation in a new code cell right here.

309
00:15:38.750 --> 00:15:39.960
And then we can just justice.

310
00:15:40.000 --> 00:15:40.760
Hundred percent.

311
00:15:40.780 --> 00:15:45.260
We will just play that cell and we don't have to do a print of these four entities.

312
00:15:45.410 --> 00:15:48.740
We perfectly understand how they work, but feel free to do it if you want.

313
00:15:49.070 --> 00:15:52.880
You're free to do any modification in this copy, of course, of the notebook.

314
00:15:53.600 --> 00:15:58.940
And finally, we have a final step of our data repricing phase, which is feature scaling.

315
00:15:59.240 --> 00:16:01.790
And now I want to say something very, very important.

316
00:16:02.120 --> 00:16:06.470
Feature scaling is absolutely compulsory for deep learning.

317
00:16:06.500 --> 00:16:11.210
Whenever you build an artificial neural network, you have to apply feature scaling.

318
00:16:11.250 --> 00:16:12.740
That's absolutely fundamental.

319
00:16:13.240 --> 00:16:19.180
And it is so fundamental that we will actually apply feature skilling to all our features, you know,

320
00:16:19.190 --> 00:16:24.080
regardless of whether they already have some values of zero and one, you know, like the dummy variables

321
00:16:24.110 --> 00:16:30.350
and sympathy's ones who will just scale everything because it is so important to do it for deep learning.

322
00:16:30.500 --> 00:16:33.860
So the feature scaling step here will be very simple.

323
00:16:34.130 --> 00:16:36.860
We will just take our deal repressing took it.

324
00:16:37.120 --> 00:16:40.700
We will go right at the end because I think this is our last tool.

325
00:16:40.730 --> 00:16:41.570
Yes, there we go.

326
00:16:41.960 --> 00:16:49.760
We will take that full sale and we will based it right back in a new code sale just below Fichus killing

327
00:16:50.050 --> 00:16:56.460
will pace it here and now instead of selecting some specific indexes here, we'll just take everything.

328
00:16:56.470 --> 00:17:00.470
So I'm just removing all our index selections here.

329
00:17:00.890 --> 00:17:03.590
Right, so that we can just scale everything.

330
00:17:03.590 --> 00:17:06.800
And that's the way it should be for neural network.

331
00:17:06.830 --> 00:17:09.650
You know, for building and training a new network.

332
00:17:10.220 --> 00:17:10.490
All right.

333
00:17:10.490 --> 00:17:11.180
So perfect.

334
00:17:11.480 --> 00:17:18.110
This will just apply features, killing to all the features of both the training set and the test set.

335
00:17:18.200 --> 00:17:22.610
But, of course, AR scale object is only fitted to the training set.

336
00:17:22.850 --> 00:17:23.120
Right.

337
00:17:23.150 --> 00:17:25.400
Remember, it's to avoid information like it.

338
00:17:25.670 --> 00:17:26.600
That doesn't change.

339
00:17:26.720 --> 00:17:27.650
But there you go.

340
00:17:27.830 --> 00:17:30.340
Now we have the code to perform features killing.

341
00:17:30.620 --> 00:17:31.610
All ready.

342
00:17:31.790 --> 00:17:32.610
So let's do this.

343
00:17:32.630 --> 00:17:39.290
Let's run this final sale and then the data repressing phase will be over.

344
00:17:39.860 --> 00:17:40.970
So congratulations.

345
00:17:41.330 --> 00:17:43.880
I hope we did it efficiently enough for you.

346
00:17:44.030 --> 00:17:45.200
That's the way it should be.

347
00:17:45.320 --> 00:17:51.440
I'd like to remind, by the way, that, you know, the data pricing face counts for 70 percent of the

348
00:17:51.440 --> 00:17:52.820
work of a data scientist.

349
00:17:53.090 --> 00:17:58.090
So that's why it was really important for me to give you some very efficient data, be pressing template

350
00:17:58.090 --> 00:18:03.380
and took it so that, as you can see, we can do it efficiently in less than 20 minutes.

351
00:18:03.490 --> 00:18:07.610
You know, in less than 20 minutes with my explanation, but without the explanation, even in less

352
00:18:07.610 --> 00:18:08.360
than 10 minutes.

353
00:18:08.750 --> 00:18:11.000
So I hope you understand and appreciate the importance.

354
00:18:11.320 --> 00:18:12.180
And now, my friends.

355
00:18:12.440 --> 00:18:13.970
It is time for the exciting step.

356
00:18:14.000 --> 00:18:15.980
The exciting part of this implementation.

357
00:18:16.140 --> 00:18:19.410
I'm talking, of course, about point to building the then.

358
00:18:19.550 --> 00:18:20.540
So there we go.

359
00:18:20.770 --> 00:18:22.670
Recharge yourself with good energy.

360
00:18:22.820 --> 00:18:27.830
And as soon as you're ready, let's tackle together part two, where we're going to build for the first

361
00:18:27.830 --> 00:18:32.120
time an artificial brain leveraging tensor flow 2.0.

362
00:18:32.630 --> 00:18:34.030
I can't wait to see you in the next.

363
00:18:34.030 --> 00:18:34.640
It's oil.

364
00:18:34.760 --> 00:18:36.620
And until then, enjoy machine learning.
