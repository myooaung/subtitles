WEBVTT
1

00:00:01.140  -->  00:00:03.490
Hello and welcome back to the course of machine learning.

2

00:00:03.570  -->  00:00:06.100
Today we're talking about decision trees.

3

00:00:06.120  -->  00:00:07.920
All right let's get started.

4

00:00:08.130  -->  00:00:10.240
You might have heard the term cart.

5

00:00:10.320  -->  00:00:17.040
It stands for classification and regression trees and this term is an umbrella term for two types of

6

00:00:17.040  -->  00:00:21.450
trees which are what we see calcifications trees and regression trees.

7

00:00:21.450  -->  00:00:26.430
Now the difference is that classification trees they help you classify your data so they won't give

8

00:00:26.430  -->  00:00:31.260
categorical variables such as male or female apple or orange or different types of colors and variables

9

00:00:31.350  -->  00:00:32.410
of that sort.

10

00:00:32.520  -->  00:00:37.740
Whereas aggression trees they are designed to help you predict outcomes which can be real numbers so

11

00:00:38.100  -->  00:00:43.380
for instance the salary of a person or the temperature that's going to be outside and things like that

12

00:00:43.380  -->  00:00:43.410
.

13

00:00:43.410  -->  00:00:48.030
So those are the two different types and we're going to be talking about classification trees in this

14

00:00:48.030  -->  00:00:49.300
section of the course.

15

00:00:49.350  -->  00:00:54.540
So here we've got an example with lots of points on our two dimensional scatterplot.

16

00:00:54.720  -->  00:00:56.360
Now how does a decision tree work.

17

00:00:56.370  -->  00:01:00.950
So it is going to do is cut it up into slices in several iterations so let's have a look.

18

00:01:01.110  -->  00:01:02.670
So they'll be split one.

19

00:01:02.790  -->  00:01:11.690
They'll be split two so split one split our data at X to equal 60 split to split our daughter X1 equals

20

00:01:11.700  -->  00:01:13.730
50 split three.

21

00:01:13.740  -->  00:01:14.490
But our.

22

00:01:14.580  -->  00:01:19.660
Exotic or 70 and split for split our data at x 2.

23

00:01:19.710  -->  00:01:21.810
It's not shown here it's about 20.

24

00:01:21.810  -->  00:01:26.520
So that is how a decision tree works and the basis for the split.

25

00:01:26.520  -->  00:01:32.860
So how are these splits selected how does the algorithm know where to select the splits.

26

00:01:33.050  -->  00:01:39.300
Well basically if you have a look at it now and then the split is done in such a way to maximize the

27

00:01:39.300  -->  00:01:47.310
number of ason category in each of these splits so to maximize For instance we want maximum Reade's

28

00:01:47.310  -->  00:01:52.680
categories here and here is why it's still the same but then the next split maximizes the number of

29

00:01:52.680  -->  00:01:54.430
green here and them more red here.

30

00:01:54.450  -->  00:01:56.590
It's a very basic way to explain it.

31

00:01:56.610  -->  00:01:59.780
In reality there's some complex mathematics happening in background.

32

00:01:59.790  -->  00:02:03.120
The split is trying to minimize entropy.

33

00:02:03.350  -->  00:02:08.310
And so it's informational entropy it's a very interesting term.

34

00:02:08.420  -->  00:02:12.440
It would take hours and hours and hours for us to go through all of that right now.

35

00:02:12.600  -->  00:02:18.450
And so if you want to get into the deeper mathematics behind this algorithm then you certainly can research

36

00:02:18.450  -->  00:02:18.620
that.

37

00:02:18.630  -->  00:02:23.400
But for us it's sufficient that we're just looking for the optimal split or the algorithms going to

38

00:02:23.400  -->  00:02:30.450
find optimal splits that are going to maximize the number of different points in each one of these new

39

00:02:30.480  -->  00:02:35.490
pockets are actually called leaves So we've got the starting scatterplot and then at the end you've

40

00:02:35.490  -->  00:02:39.230
got these leaves and the final leaves are actually called a terminal lease.

41

00:02:39.300  -->  00:02:40.360
So that's how this will occur.

42

00:02:40.360  -->  00:02:44.480
Now let's rewind a bit and let's do that whole procedure again.

43

00:02:44.490  -->  00:02:49.410
But while we're performing the splits we're going to start constructing a decision tree an actual decision

44

00:02:49.410  -->  00:02:49.610
tree.

45

00:02:49.620  -->  00:02:50.280
Let's have a look.

46

00:02:50.460  -->  00:02:52.190
So there is our split number one.

47

00:02:52.470  -->  00:02:56.620
And what it's doing is it's splitting our daughter at the 60 level.

48

00:02:56.700  -->  00:03:00.710
So now let's construct a decision tree that's going to ask that question.

49

00:03:00.720  -->  00:03:07.410
So is X too greater than 60 or less than 60 so if is great and 6 it falls into one branch if it's less

50

00:03:07.410  -->  00:03:09.590
than 60 it will fall into the next range.

51

00:03:09.600  -->  00:03:12.160
So there we go X-2 is less than 60.

52

00:03:12.260  -->  00:03:13.930
No yes and no.

53

00:03:13.950  -->  00:03:21.300
Next is split two only splits the Dahdah that is above 60 x to verbal.

54

00:03:21.600  -->  00:03:22.960
So let's have a look at that.

55

00:03:22.980  -->  00:03:29.100
We're only dealing with data that is above X-2 So it's over here and now we're checking.

56

00:03:29.100  -->  00:03:30.010
So I'm going back now.

57

00:03:30.010  -->  00:03:34.340
Now we're checking as split to happens at 50 for the X1 variable.

58

00:03:34.590  -->  00:03:37.710
So here we go x X1 is less than 50 yes or no.

59

00:03:37.710  -->  00:03:43.080
So if and here you can see that right away this split already you can tell us whether something is green

60

00:03:43.110  -->  00:03:43.740
or red.

61

00:03:43.920  -->  00:03:51.840
So if it's less so if we're already above 60 and then below 50 then it's green which we can see here

62

00:03:52.560  -->  00:03:55.710
if we are above 50 then it's red which we can see here.

63

00:03:55.710  -->  00:03:59.430
So that's how this classification works and all let's deal of the remainder.

64

00:03:59.430  -->  00:04:02.520
So here we've got a split 3 happening at 70.

65

00:04:02.730  -->  00:04:07.500
If you're below 70 you're obviously going to be red otherwise we're going to need to do another split

66

00:04:07.500  -->  00:04:07.770
.

67

00:04:07.800  -->  00:04:10.580
So below 70 then it's red.

68

00:04:10.590  -->  00:04:16.200
Now we've got to do on the split split for if it's above 20 then it's green.

69

00:04:16.200  -->  00:04:22.740
If it's below 20 then it's red if it's above 20 then it's green so that's a no if it's below then yes

70

00:04:22.740  -->  00:04:23.190
.

71

00:04:23.190  -->  00:04:27.410
So with these decision trees are a good way to structure them is to always keep.

72

00:04:27.410  -->  00:04:28.890
Yes yes yes yes.

73

00:04:28.920  -->  00:04:35.010
On one side so like if you if you're looking for yesses they're always going to the left looking for

74

00:04:35.010  -->  00:04:36.740
those go into the right or vice versa.

75

00:04:36.810  -->  00:04:37.920
Just don't mix them up.

76

00:04:37.980  -->  00:04:45.680
And then the terminal leaves will predict exactly what color or what Clauss is left.

77

00:04:45.930  -->  00:04:49.950
But at the same time even if you don't get to the terminal leef because it's a very simple tree.

78

00:04:49.950  -->  00:04:51.930
Trees can be very very long.

79

00:04:52.050  -->  00:04:57.000
And so sometimes you might not even get to the bottom so if you want to classify new observation and

80

00:04:57.000  -->  00:05:03.640
for example this observation falls into this section away here then it would go down this road than

81

00:05:03.660  -->  00:05:05.000
here and there you go here.

82

00:05:05.090  -->  00:05:08.830
But let's say you don't even get to the end you get to somewhere right here.

83

00:05:08.920  -->  00:05:14.220
Then in these boxes that don't that have still they still have a mixture here as is a mix of green or

84

00:05:14.220  -->  00:05:14.870
red.

85

00:05:14.940  -->  00:05:21.450
Then the rule here is that realistic classification occurs so here we know instead of checking this

86

00:05:21.450  -->  00:05:25.490
last condition we'll just check what is the likelihood of it being green and red.

87

00:05:25.500  -->  00:05:30.000
So here we see that there's more green and red so if we're just going to leave it at this box then we'll

88

00:05:30.000  -->  00:05:36.720
just say that it's it's a green dot whereas if we leave it at this hold box if we just just do this

89

00:05:36.720  -->  00:05:39.970
part so we only go check the first condition and then we'll leave it here.

90

00:05:40.080  -->  00:05:42.820
Then we would automatically say that it's a red dot.

91

00:05:42.840  -->  00:05:48.030
If we don't go down the decision tree and check more conditions so that's another way of using a dictionary

92

00:05:48.360  -->  00:05:50.520
instead of going to a down to the very end.

93

00:05:50.520  -->  00:05:55.620
You can stop at any point and then just use the probabilities to predict your classification.

94

00:05:55.620  -->  00:05:59.670
And another thing is that it doesn't always have to be the two variables.

95

00:05:59.670  -->  00:06:05.490
So for instance in a decision tree just like any other machine learning algorithm you can have a multidimensional

96

00:06:05.520  -->  00:06:10.500
data set which has lots of different columns so in our case we only have two but you might have lots

97

00:06:10.500  -->  00:06:16.200
and lots of columns and then you could have a mix of questions being asked here and that's up to the

98

00:06:16.200  -->  00:06:18.290
algorithm to come up with those questions.

99

00:06:18.450  -->  00:06:23.550
And as a final note I wanted to mention a little bit about the history of decision trees suden trees

100

00:06:23.550  -->  00:06:30.810
have been around for very very long time and in fact they are so old that they have recently kind of

101

00:06:30.810  -->  00:06:31.790
started to die off.

102

00:06:31.800  -->  00:06:38.250
There were still poppier about 23 years ago but recently more sophisticated methods have come to replace

103

00:06:38.250  -->  00:06:38.730
them.

104

00:06:38.850  -->  00:06:41.750
And soon she's been so popular.

105

00:06:42.090  -->  00:06:43.820
And that's continued for a while.

106

00:06:43.830  -->  00:06:52.560
Until recently they were reborn with new upgrades so to speak and those upgrades are additional methods

107

00:06:52.650  -->  00:07:00.180
that build on top of decision trees and such methods are random forest gradient boosting and other methods

108

00:07:00.180  -->  00:07:00.570
.

109

00:07:00.570  -->  00:07:05.150
And in this part of the course we look at least at one of those other methods.

110

00:07:05.160  -->  00:07:10.030
The point is that decision trees are though a very simple tool.

111

00:07:10.110  -->  00:07:16.860
They aren't very powerful on their own but they're used in other methods that leverage their simplicity

112

00:07:17.160  -->  00:07:23.340
and create some very powerful machine learning algorithms and such algorithms even are used to perform

113

00:07:23.340  -->  00:07:29.070
facial recognition like on your iPhone you get you have facial recognition and also some games such

114

00:07:29.070  -->  00:07:34.620
as Kinect which is kind of like the we but you can play it without actually holding a control.

115

00:07:34.620  -->  00:07:40.840
So it's like a game for your addition to your X-Box and you can play as we followed have any control

116

00:07:40.850  -->  00:07:46.430
controlling your hands so it kind of recognizes where you're moving your arms and legs and that method

117

00:07:46.440  -->  00:07:53.750
Microsoft decided to use around forests for that method and run it for us in Boak decision trees.

118

00:07:53.820  -->  00:07:55.690
So hopefully this today's tutorial.

119

00:07:55.750  -->  00:08:01.440
It is quite a simple method but at the same time it lies in the foundation of some of the more modern

120

00:08:01.500  -->  00:08:04.410
and more powerful methods in machine learning.

121

00:08:04.410  -->  00:08:05.730
I look forward to seeing you next time.

122

00:08:05.730  -->  00:08:07.440
And until then in your machine learning
