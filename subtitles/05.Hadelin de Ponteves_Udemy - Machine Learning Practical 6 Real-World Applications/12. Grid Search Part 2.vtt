WEBVTT

00:01.420 --> 00:03.160
Welcome back everybody.

00:03.160 --> 00:07.270
In today's video we're going to continue our process of grid search that we started in the previous

00:07.270 --> 00:08.080
video.

00:08.140 --> 00:11.350
We're going to again continue with the criterion of entropy.

00:11.500 --> 00:17.430
And we're going to leverage our previous results of the optimal parameters to do another round.

00:17.740 --> 00:26.370
So let's begin quickly by looking at the cons. We can see the results from the previous run.

00:26.590 --> 00:28.870
So just temporarily copy here.

00:29.240 --> 00:36.890
Now let's go back to the EDR let's copy and paste it again because we're going to do another round.

00:37.180 --> 00:39.150
It's going to be around two.

00:40.120 --> 00:44.740
And we're going to base our new assaults on the previous results.

00:45.070 --> 00:48.440
So we found that none is the most optimal max depth.

00:48.690 --> 00:50.830
So we're going to get rid of pre and just run none.

00:50.820 --> 00:53.040
So there's nothing else to be done here.

00:53.140 --> 00:55.940
The max features gave birth a value of five.

00:55.950 --> 00:56.510
Correct.

00:56.680 --> 01:03.640
So instead of one in 10 on the ages we're going to try it actually forensics to see if there is a preferred

01:04.840 --> 01:09.730
feature size we actually could try five and seven or three five and seven.

01:10.000 --> 01:11.750
So to cover more range.

01:11.960 --> 01:13.080
OK let's do that.

01:13.240 --> 01:17.910
And again these two issues that you have to make on your own based on what you're trying to test then

01:17.940 --> 01:19.240
for me samples please.

01:19.240 --> 01:22.320
We saw that our results keep us A10.

01:22.630 --> 01:27.790
So what we can do here is stride an ape.

01:28.030 --> 01:35.980
This is six and eight and a top and see if that gives us better results than many samples.

01:35.980 --> 01:38.520
Leaf was stuck on one.

01:38.530 --> 01:41.550
So we're going to use one two and three.

01:42.480 --> 01:43.350
Awesome.

01:43.510 --> 01:45.370
Bootstrap was true.

01:45.460 --> 01:50.050
So we're going to stick to truth or weight or false and we're going to keep the entropy.

01:50.110 --> 01:50.850
And that's it.

01:51.160 --> 01:54.650
And now we're ready to run the second round of grid search.

01:54.850 --> 01:56.830
So let's do this.

01:57.040 --> 01:59.060
We're ready for a second round.

01:59.590 --> 02:02.210
Everything is set.

02:02.530 --> 02:03.310
And there we go.

02:03.550 --> 02:06.830
And now we're going to run all this again.

02:06.850 --> 02:08.750
Again this is going to take a few minutes.

02:08.920 --> 02:13.040
So we're going to stop that here and come back when it's all done.

02:15.290 --> 02:15.620
OK.

02:15.630 --> 02:18.140
So the model has finished running.

02:18.630 --> 02:20.860
And then we got to cancel and see the results.

02:20.880 --> 02:26.140
We realize that the results are the exact same that we got in the first round.

02:26.580 --> 02:31.950
So what this tells us this tells us that the results we got in the first round are just the optimal

02:32.040 --> 02:33.450
are all the ones we have try.

02:33.540 --> 02:39.410
So we're going to stick to that and run our tests on that particular set of results so as to that.

02:39.420 --> 02:47.490
Now the final part here is to apply our model to our test set so that we can see if this set of results

02:47.520 --> 02:49.920
actually improves our accuracy or not.

02:50.130 --> 02:54.960
So we're going to copy the code from before we're going to paste it here.

02:55.650 --> 03:01.710
And the one thing we're going to add here is that instead of having to classify a DDX this we're going

03:01.710 --> 03:05.890
to have our research pretty easy access to the body.

03:05.900 --> 03:12.000
This is going to run the accuracy and going to create a more resource that is going to detail those

03:12.000 --> 03:22.110
accuracies and it's going to be run the is Plus grid search times to plus entropy.

03:22.410 --> 03:24.260
So this is the entropy version.

03:24.750 --> 03:28.490
So let's run this great.

03:28.500 --> 03:29.940
This is accurately.

03:30.090 --> 03:38.060
And now finally we're going to again copy code from before and put this resoled into the ressort stage.

03:38.550 --> 03:42.330
And if we look at the results table the console.

03:42.330 --> 03:49.410
Now we have our linear model here that has a slightly higher accuracy.

03:49.830 --> 03:56.130
Same precision for the most part a slightly higher recall and much higher.

03:56.210 --> 03:58.750
One score of almost two points.

03:58.800 --> 04:04.330
So this grid search model is actually performing better than the original run of force model.

04:04.560 --> 04:09.480
So this grid search was definitely watched attempting the second round didn't give us any lift.

04:09.600 --> 04:16.470
But the first one the now we attempted this grid search on the Entropia version of front for us.

04:16.580 --> 04:24.530
We can do the exact same but you seen Guinea version so to do that we just have to copy everything we

04:24.530 --> 04:36.260
have done beta right here and this is going to be around one guinea and we're run to Guinea then we're

04:36.290 --> 04:42.910
also going to change the actual criteria beginning the entropy here.

04:43.230 --> 04:45.340
Finally the very end.

04:45.480 --> 04:52.020
We're going to change the fact that this is no longer a great search plus two times entropy is going

04:52.020 --> 04:57.360
to be concert times two plus getting and that's about it.

04:57.600 --> 05:04.500
So first we're going to run the first version of this Guinea grid search and see what we get.

05:06.940 --> 05:09.850
This going to run for a few minutes in those minutes.

05:09.850 --> 05:11.490
I want to go over one thing.

05:11.770 --> 05:14.800
The difference between Guinea and entropy.

05:15.190 --> 05:22.300
You have to do a lot with what these criterial things to this great hearing is the splitting criteria.

05:22.660 --> 05:29.380
So that means that when the parent leave is partitioned into two chyle regions in the tree and the decision

05:29.380 --> 05:35.180
tree that is run the force is made out of we need a particular criterion for these.

05:35.620 --> 05:38.170
And the entropy is a criterion.

05:38.350 --> 05:45.280
There's a whole equation behind this that is meant to maximize the informational content that are random

05:45.280 --> 05:46.700
for us has.

05:46.780 --> 05:52.270
So the question makes it so that we're maximizing the information that we keep at every split.

05:52.610 --> 05:57.700
The Guinea version and the other hand minimizes the probability of mislabelling.

05:57.700 --> 05:59.530
So when does this leading.

05:59.560 --> 06:05.330
It does so in a way that it values not mislabelling our lives.

06:05.430 --> 06:06.080
OK.

06:06.550 --> 06:08.060
So that is the only difference.

06:08.110 --> 06:10.700
You can go and do some research on the equations.

06:10.710 --> 06:12.780
There's a lot of math involved in those.

06:12.970 --> 06:18.070
But that is like the main difference that you should know if somebody ask you about Ghanian and now

06:18.160 --> 06:19.760
this is going to run for a few more minutes.

06:19.810 --> 06:22.710
I'll be back when it's done and we'll see the results.

06:24.830 --> 06:25.700
So great.

06:25.800 --> 06:27.980
The model has finished running.

06:28.530 --> 06:30.260
So here we look at the console.

06:30.270 --> 06:38.100
We have our accuracy of sixty three six Aransas 3.6 percent which is a tiny bit better but not much.

06:38.190 --> 06:47.220
The cameras are bootstrapped true Guinea Mac step of non Max feature of 10 min samples for 10 samples.

06:47.670 --> 06:48.090
OK.

06:48.330 --> 06:50.000
So we're going to copy this.

06:50.160 --> 06:53.620
We're going to paste it here the Berrien so we can see it.

06:54.210 --> 06:57.280
And now let's set up the round two for this grid search.

06:57.630 --> 07:03.560
So if we look a little bit higher up here we're going to realize that Max there still remains none.

07:05.180 --> 07:08.890
Max features should now be around 10.

07:08.930 --> 07:20.120
So we're going to start with maybe an 8 here a 10 and a 12 men samples as a result of two.

07:20.270 --> 07:26.960
So we're going to make sure that we get numbers around that two three and four to see if we get better

07:26.960 --> 07:28.020
results that way.

07:28.020 --> 07:31.430
I mean sample leaf gave as a result of 10.

07:31.610 --> 07:40.720
So we're going to make this around 10 8 10 and 12 bootstrap history of course.

07:40.820 --> 07:43.040
And the criterion is getting and that's it.

07:43.040 --> 07:47.450
So this is a new set of features privater sorry that we're going to try our great search.

07:47.460 --> 07:52.030
Number two we're going to run it and see what we get out.

07:52.100 --> 07:52.660
Get back.

07:52.670 --> 07:56.980
The moment is finished running okay.

07:57.070 --> 07:59.040
So the model has finished running.

07:59.290 --> 08:05.920
And here the results the results are an accuracy of sixty three point eight just slightly higher.

08:05.920 --> 08:08.300
And the parameters are true.

08:08.370 --> 08:08.810
Okay.

08:08.830 --> 08:12.020
Guinea none Max features of 12.

08:12.160 --> 08:16.360
I mean samples off of 12 as well and mean samples.

08:16.780 --> 08:18.480
So those are the ones that we're going to use.

08:18.520 --> 08:19.500
So what next.

08:19.510 --> 08:22.530
Now with that we have finalized the second round of research.

08:22.660 --> 08:25.210
We're actually going to run that model on it.

08:25.510 --> 08:27.740
Again this is the exact same lines that we were on before.

08:27.910 --> 08:30.010
So we're going to run the model.

08:30.160 --> 08:37.210
We're going to create a new vector of results in appendant we're so state for so let's do it.

08:38.930 --> 08:39.750
Excellent.

08:39.750 --> 08:41.000
So now let's look at our results

08:44.090 --> 08:45.420
and here we go.

08:45.560 --> 08:51.440
So we see the random forays we grid search apply two times Angelini give gibbous a accuracy of sixty

08:51.440 --> 08:57.490
three point five on that test set a procession of sixty four point nine.

08:57.860 --> 09:03.980
A recall of 70 percent and the EF 1 score of sixty three point six seven point three.

09:04.370 --> 09:07.840
So random forays into being guinea are not that different.

09:07.850 --> 09:13.820
We can assume that the difference here are just the randomness but overall we're going to just stick

09:13.820 --> 09:18.960
to random forest and Trape and realized that this is the best model we can possibly use.

09:19.340 --> 09:24.050
And it's definitely an improvement over the previous model so you can see the original random foras

09:24.070 --> 09:28.230
the SBM the SBM linear and logistic regression.

09:28.640 --> 09:32.120
So this is said This is the final dispersion of our model.

09:32.210 --> 09:36.930
Thanks to great search and we have done everything we need to do to build our model.

09:37.010 --> 09:39.190
So thank you very much for watching.

09:39.440 --> 09:44.930
And the next video we're going to append these results with a user identifier.

09:45.320 --> 09:47.930
And we're going to finalize our script.

09:47.930 --> 09:49.330
See you in the next one.
