WEBVTT

00:03.880 --> 00:11.440
Let's take a look on how to improve our motto we just saw this Jupiter notebook that our model accuracy

00:11.740 --> 00:13.680
wasn't that great actually terrible.

00:13.720 --> 00:16.270
And let's take a look on how can we improve our model.

00:16.540 --> 00:19.740
The first step that we need to perform is all data normalization.

00:19.840 --> 00:20.380
OK.

00:20.710 --> 00:26.200
So as you guys can recall for example here I plotted the Mean area versus the mean smoothness.

00:26.200 --> 00:30.790
As you can see here that means smoothness is let's say these are actually very small numbers from point

00:30.850 --> 00:35.520
0 6 let's say point six over the mean areas and looks twenty five hunters.

00:35.610 --> 00:36.170
OK.

00:36.430 --> 00:42.190
This data is actually going to cause a lot of problems when we train our machine learning model.

00:42.370 --> 00:49.090
And that's why one of the key elements in here is try to kind of normalize our data which is in very

00:49.090 --> 00:50.090
simple terms.

00:50.080 --> 00:54.400
We're going to use what we call it unity unity base normalization which is we wanted to get all the

00:54.400 --> 00:56.640
data to be from between 0 and 1.

00:56.710 --> 00:58.140
That's the whole objective.

00:58.180 --> 01:00.880
So we're going to show you how can we do that in just a notebook and a bit.

01:00.900 --> 01:02.780
But just a quick overview.

01:02.780 --> 01:04.130
This is again the area.

01:04.130 --> 01:08.920
This means smoothness these are just larger values in here here as you can see after normalization you'll

01:08.920 --> 01:11.120
find that the mean areas from between 0 and 1.

01:11.320 --> 01:13.760
And again that means more this again between zero and one.

01:13.760 --> 01:14.670
All right.

01:15.220 --> 01:20.310
And again with the zero indicates the malignant case the malignant cancer.

01:20.320 --> 01:22.810
And if it's one that means it's benign.

01:22.920 --> 01:23.550
All right.

01:23.560 --> 01:26.630
How can we do the United Beys normalization.

01:26.790 --> 01:33.400
Robert call it features killing we simply subtract each of the sample points by the minimum value and

01:33.400 --> 01:38.900
we divide by the range or the X Max minus X-Men that will tell us that would give us kind of normal

01:38.980 --> 01:40.920
the data between 0 and 1.

01:41.260 --> 01:46.510
It's really simple and we'll see how can we apply that normalization and can improve the results of

01:46.510 --> 01:47.740
the model dramatically.

01:47.740 --> 01:48.390
All right.

01:48.850 --> 01:49.230
OK.

01:49.240 --> 01:52.630
So the next optimization now can do.

01:52.630 --> 01:59.470
Can we do in the SVM in when we use the SVM we use kind of the default parameters.

01:59.470 --> 02:01.230
OK we didn't hear any of the parameters.

02:01.240 --> 02:05.980
We just called as SVM and it was just you know it was just fine.

02:06.030 --> 02:06.550
All right.

02:06.790 --> 02:10.410
Obviously the results were great because they were optimized right.

02:10.600 --> 02:16.510
So we have two key parameters that we can actually optimize during the during that process which we

02:16.510 --> 02:19.410
call it see parameters and gamma parameters.

02:19.410 --> 02:20.120
Okay.

02:20.920 --> 02:23.300
So the first prime minister will call the see parameter.

02:23.440 --> 02:24.110
OK.

02:24.280 --> 02:30.310
The C parameter simply controlled the tradeoff between classifying taining points correctly and having

02:30.310 --> 02:32.150
kind of a smooth decision boundary.

02:32.250 --> 02:32.850
OK.

02:33.110 --> 02:33.420
OK.

02:33.430 --> 02:37.010
It might seem a little bit like strange but again it's really simple.

02:37.270 --> 02:43.840
So the whole idea is we can use a parameter which is an emitter that can kind of specifies the penalty.

02:43.900 --> 02:49.510
OK but how can we penalize the model if it misclassified as a data point.

02:49.510 --> 02:50.000
OK.

02:50.440 --> 02:52.990
So if we use a small c that.

02:53.160 --> 02:53.550
OK.

02:53.560 --> 02:58.510
That means we are assuming that our model or our penalty is kind of loose.

02:58.570 --> 03:00.430
It's kind of a cool teacher in a way.

03:00.460 --> 03:01.210
OK.

03:01.210 --> 03:10.060
So if if the model classifies our data set misclassified or data set we to assume that OK we're not

03:10.060 --> 03:11.390
going to penalize you that much.

03:11.410 --> 03:17.200
That means we can hived kind of a smoother boundary between the two classification points between the

03:17.200 --> 03:18.020
two classes.

03:18.200 --> 03:18.850
OK.

03:19.120 --> 03:20.900
That's when we have a small value of c..

03:21.130 --> 03:27.310
However if we have a large value of c that means we can have a very strict model that means the cost

03:27.310 --> 03:32.530
of misclassification can be very high which means if we do something wrong if we misclassify any of

03:32.530 --> 03:39.310
these points OK we need to kind of bend our boundary a little bit more which means we can have kind

03:39.310 --> 03:43.600
of more detailed or more fine line in here not a line anymore.

03:43.600 --> 03:45.250
And I even have kind of more

03:47.820 --> 03:53.960
strict or more decision line that kind of fitted specifically for this training dataset.

03:54.360 --> 03:59.520
So this is kind of when we have a smaller value of c and this when we have a larger value of seats we

03:59.520 --> 04:05.160
have larger value of c we can actually might be prone to overfitting which means we are overfitting

04:05.280 --> 04:10.620
our training specifically for the training data set which means if we expose that model to training

04:10.680 --> 04:15.930
to data which is kind of testing data that the model hasn't seen before then it might actually be prone

04:15.930 --> 04:22.340
to misclassified which means Y because the model is not trained specifically or to generalize on any

04:22.410 --> 04:23.150
dataset.

04:23.580 --> 04:29.820
As you can see here if we try to change for example the C value from C let's say 1 to 10 to a hundred

04:29.880 --> 04:35.070
and thousand you will see that the boundary layer starts from being very smooth which is when we have

04:35.070 --> 04:40.530
a small value of C which is kind of we have a very low penalty if you do something wrong that's fine.

04:40.530 --> 04:43.720
We're just going to keep it kind of smooth as much as we can.

04:43.790 --> 04:44.420
OK.

04:44.850 --> 04:48.960
To actually moving to a larger value of C which is a very strict boundary.

04:49.190 --> 04:49.560
OK.

04:49.590 --> 04:54.600
Which means that OK if we do something wrong we're going to penalize really high and that's why we're

04:54.600 --> 05:01.210
going to come up with this kind of you know more overfit it or stricter boundary line correct.

05:01.230 --> 05:03.280
So that's the first parameter is quality parameter.

05:03.370 --> 05:03.800
Right.

05:04.020 --> 05:07.950
Let's take a look at the other parameters called a gamma parameter.

05:07.950 --> 05:14.040
So again a parameter control is how far the influence of a single taining set reaches.

05:14.040 --> 05:14.550
OK.

05:14.760 --> 05:19.370
We just kind of specify the spread of our influence of the data points.

05:19.560 --> 05:25.110
If we use a large gamma large value of gamma that means we're going to have a close reach.

05:25.110 --> 05:25.910
What do you mean.

05:26.010 --> 05:30.390
That means we're going to focus only on the points very close to the hyperplane.

05:30.600 --> 05:34.860
That means these points in here are our points of interest.

05:34.860 --> 05:41.120
We have much higher weight which means that OK if we have a larger gamma What do you expect are going

05:41.140 --> 05:46.020
to be more generalized or more overfit it actually going to be way more overfit it's.

05:46.050 --> 05:53.100
Which means that our lines in here we can focus on these points and ignoring the other way points from

05:53.100 --> 05:54.860
the from the hyperplane.

05:54.870 --> 05:55.510
Right.

05:56.010 --> 05:56.530
OK.

05:56.820 --> 05:57.880
That's the first part.

05:57.980 --> 06:02.460
If we have larger gamma and that's why we can end up with like combine like this.

06:02.460 --> 06:08.180
Basically if we have washerman However if we have a small gamma that means we're going to have far far

06:08.190 --> 06:08.850
small gamma.

06:08.880 --> 06:13.490
That means we have four each we'd have kind of a more generalized solution.

06:13.890 --> 06:19.110
As you can see here the points or the inference of the points are going to be kind of widespread.

06:19.110 --> 06:23.310
So we're not going to concede that only the pounds the only the point on the boundary actually going

06:23.310 --> 06:25.200
to consider way more points.

06:25.260 --> 06:30.180
OK we're going to cover a lot more space so we can achieve more generalized solution.

06:30.420 --> 06:33.440
So the key question is how can we optimize these parameters.

06:33.450 --> 06:34.440
Normalization is fine.

06:34.440 --> 06:37.750
We can do normalization are going to show you how can we do no motivation.

06:37.770 --> 06:41.360
The question is how can we optimize the C and gamma parameter OK.

06:41.700 --> 06:47.910
Luckily we're going to show you that there is a technique that's really easy you can use kind of a grid

06:47.910 --> 06:48.690
search.

06:48.690 --> 06:52.230
We're not going to go and actually try you know like by trial and error we're just going to run kind

06:52.230 --> 06:58.630
of a grid search of tell us OK this is the best the best game for our our our model or specifically

06:58.620 --> 06:59.510
for our data.

06:59.610 --> 07:01.010
And then we're good to go.

07:01.110 --> 07:06.000
And then you can have a way better accuracy moving forward compared to the terrible actually accuracy

07:06.000 --> 07:08.470
that we had before as we saw.

07:08.970 --> 07:12.610
We're going to implement maybe two improvements within the model.

07:12.630 --> 07:17.830
The first improvement they're going to perform normalization which is simply scaling back the data to

07:18.460 --> 07:22.250
be all the data to be very useful ranging from zero to one.

07:22.590 --> 07:30.330
And then the next improvement that we're going to Turenne the VC or support vector classifier to simply

07:30.960 --> 07:34.570
tune the gamma parameter and the parameter as we saw earlier.

07:34.660 --> 07:38.820
OK let's take a look at the first model improvement first.

07:38.820 --> 07:42.300
So the first step is that we're going to do that we're going to perform normalization.

07:42.380 --> 07:43.110
Okay.

07:43.200 --> 07:45.060
So I'm going to do that first.

07:45.090 --> 07:51.030
We're going to get the men value of all the men of Xchange first.

07:51.030 --> 07:57.630
So we're going to normalize the X-raying data so explain that are going to the myth method method men

07:57.940 --> 07:59.720
that just simply get us the minimum value.

07:59.730 --> 08:00.980
OK.

08:01.130 --> 08:05.750
And then the next step we're going to get there within the range which is simply after we're putting

08:05.760 --> 08:11.490
that age out there were the men then we can subtract the training data minus Minimed value and divided

08:11.490 --> 08:16.790
by the range that will pretty much kill our our our future.

08:17.100 --> 08:21.980
The next step we're going to define OK range underscore trained obviously can do it in one step here

08:21.990 --> 08:24.000
I'm just doing it in multiple steps.

08:24.070 --> 08:30.970
So we're going to do that we're going to subtract extreme minus men on the train which is what defined

08:31.030 --> 08:35.430
earlier we're all going to do that because we need to get the range which is going to do the best method

08:35.440 --> 08:38.130
Max to simply give me the maximum value.

08:38.280 --> 08:40.170
OK so now we have the maximum value.

08:40.180 --> 08:42.920
Now we have the range simply now have the minimum value.

08:43.060 --> 08:44.280
Let's actually do the scaling.

08:44.470 --> 08:49.370
So we're going to do X underskirt train but we're going to call it scaled scaled.

08:49.370 --> 08:51.400
We can call it whatever you want obviously.

08:51.580 --> 08:59.350
And then afterwards we're going to do that we're going to subtract X train which is any data point minus

08:59.500 --> 09:04.810
minimum of the cochain and we get divided by the range on the score.

09:05.410 --> 09:07.390
Let's run this game and we're good.

09:07.400 --> 09:10.450
So now we have Eckstine scale which is perfect.

09:10.480 --> 09:12.650
Let's make sure that we are on the right page.

09:12.650 --> 09:12.970
OK.

09:13.000 --> 09:19.960
So the first step is that we are going to simply know that scatterplot which is s n s is simply Seaborn.

09:20.290 --> 09:25.690
We plotted before the mean area and the smooth smoothness before the scatterplot.

09:25.690 --> 09:27.740
The visualization section.

09:27.830 --> 09:28.240
All right.

09:28.260 --> 09:32.160
So let's make sure that we actually did the scaling correctly.

09:32.350 --> 09:32.670
OK.

09:32.680 --> 09:35.340
Or did the normalization correctly the first step.

09:35.380 --> 09:39.950
We're going to first plot the data OK using seaboard.

09:40.030 --> 09:42.160
So we're going to do again I'm just copying the comment here.

09:42.160 --> 09:43.090
It's really simple.

09:43.150 --> 09:44.780
So we're just going to either scatter plots.

09:44.830 --> 09:45.410
OK.

09:45.660 --> 09:50.590
What we did that we were in the main area versus means smoothness when you selected the US to being

09:50.660 --> 09:51.260
white.

09:51.380 --> 09:53.430
OK let's srong this.

09:53.460 --> 09:56.630
That would show us the actual trailing data without the scaling.

09:56.790 --> 10:03.310
OK what we could do is that we again as you can see here the value of the areas ranging down two hundred

10:03.490 --> 10:07.180
twenty five hundred which is not acceptable.

10:07.180 --> 10:12.760
Are we going to do they're going to run again the same value or the same command but without extreme

10:13.030 --> 10:19.300
rishonim extremes killed and Y and X extreme scaled.

10:19.970 --> 10:20.570
OK.

10:21.070 --> 10:22.330
That means killing data.

10:22.360 --> 10:27.160
So as you guys can see here is the the data is exactly similar.

10:27.160 --> 10:29.250
However the range here is totally different.

10:29.410 --> 10:31.810
So everything all the data is going from zero to 1.

10:31.870 --> 10:39.760
Ok so now we just did like a feature scaling or we did unity normalization and now we're pretty much

10:39.760 --> 10:45.890
ready to actually train the data set or change the model again using the newer normalized training data.

10:45.890 --> 10:46.550
All right.

10:46.700 --> 10:47.040
Right.

10:47.110 --> 10:51.470
Let's perform the same features scaling or the same mobilization again.

10:51.470 --> 10:53.310
But on the testing data set.

10:53.330 --> 10:53.860
All right.

10:54.130 --> 10:59.230
So we're just going to pretty much copy the same commands again here.

10:59.530 --> 11:04.000
So we will see that there is access again in the minimum that will give me the test.

11:04.120 --> 11:10.090
Again you can do it in one one one step and then we get the X test minus the minimum value and then

11:10.090 --> 11:15.130
we get the maximum out of all this which is why the range and then we get to that scale the testing

11:15.330 --> 11:20.060
dataset X test minus men divided by the range will give me the x scale.

11:20.310 --> 11:22.470
And again if we on it then we're good.

11:22.480 --> 11:28.840
And the next step is that we wanted to again turn the model but instead of changing it again with the

11:29.230 --> 11:35.740
data or the of data we're going to use these killed data or normalized data set to change one model.

11:36.160 --> 11:37.940
All right let's run it again.

11:38.110 --> 11:42.900
So we're getting it right as we see that model which is the exact same one that we had before.

11:43.340 --> 11:43.900
OK.

11:44.110 --> 11:47.670
Actually as we see underskirt a model and Dot fits.

11:47.680 --> 11:54.420
But instead we actually not use x underscore train underscores scaled right along with why underscore

11:54.480 --> 11:57.480
tree that would simply fit our model.

11:57.520 --> 11:59.700
OK so we fit the model now we're good.

11:59.770 --> 12:03.130
The next step is that we want to again do the predictions.

12:03.160 --> 12:11.270
The confusion matrix and the heat map let's go up and let's copy the commands so we have our Y predicts.

12:11.480 --> 12:16.640
OK so now within the model it's a new company cells.

12:16.820 --> 12:17.170
Right.

12:17.320 --> 12:21.990
So let's evaluate our new model using the newer dataset that we all play before.

12:22.330 --> 12:23.910
So we're going to do that when right.

12:24.040 --> 12:25.340
Underscored predicts.

12:25.430 --> 12:25.960
OK.

12:26.260 --> 12:34.990
And we're going to use the again predict method on our object as VCM the model Daut predicts But instead

12:35.140 --> 12:37.270
this time of using the X test.

12:37.270 --> 12:43.870
We actually can write X tests skil datasets like that from this day.

12:43.970 --> 12:46.870
Now what happens actually this creates some nuisance.

12:47.180 --> 12:50.890
The next step I'm going to use the confusion matrix that we have exactly the same as we had them before.

12:50.930 --> 13:00.680
When I see them equal to confusion and the score matrix and they're going to use our true value which

13:00.680 --> 13:02.630
is weightiest are two values.

13:02.840 --> 13:08.180
And then we're going to use y underscore predict which is simply the predictions that we obtain here.

13:08.180 --> 13:08.710
All right.

13:09.170 --> 13:10.010
Let's run it.

13:10.010 --> 13:11.310
So now we're good.

13:11.330 --> 13:16.540
The next step is that we wanted to plot our heating up so we can use the seaborne library s and s dot

13:16.730 --> 13:17.500
map.

13:17.870 --> 13:18.280
Right.

13:18.470 --> 13:22.800
And all we're going to do they're going to write We're going to select our confusion matrix and I'm

13:22.820 --> 13:25.980
going to enable the annotations to be simply true.

13:26.060 --> 13:28.930
So we can actually view the numbers right.

13:29.820 --> 13:34.190
OK I think we are good let's give it a shot.

13:34.300 --> 13:34.890
OK.

13:34.900 --> 13:35.860
Perfect.

13:35.860 --> 13:39.070
Now we actually have way better results if you guys can see.

13:39.070 --> 13:43.150
We have 43 correct classification.

13:43.180 --> 13:48.550
We have 66 Coretta's litigation and we have only classified or misclassified 5 samples only.

13:48.550 --> 13:49.760
Which is great.

13:49.780 --> 13:52.520
That means we are doing absolutely great.

13:52.540 --> 13:55.200
The next step is we're going to do.

13:55.300 --> 13:59.470
They're going to plot OKALIK classification report that what kind of class that could give us like you

13:59.470 --> 14:02.310
know kind of a summary of our performance.

14:02.350 --> 14:03.060
Right.

14:03.110 --> 14:07.960
Going to do it again like Prince and they're going to use classification board just to class.

14:08.290 --> 14:13.990
Just to remind you guys too we had our when we imported the metrics we imported confusion matrix and

14:13.990 --> 14:16.330
we put our classification report as well.

14:16.330 --> 14:16.720
All right.

14:16.750 --> 14:22.420
If you go down here which is going to print our classification report and then we're going to use our

14:22.750 --> 14:28.520
many true values along with the predictions going the right way and the tests and.

14:28.630 --> 14:32.520
Why underscore predicts which is our predictions.

14:32.740 --> 14:39.260
Let's give it a shot that will tell me our accuracy is on point 6 which is great which is kind of you

14:39.260 --> 14:42.210
know like a summary of our performance which is 96 percent.

14:42.220 --> 14:43.280
That's actually not bad.

14:43.540 --> 14:45.130
Actually we haven't done anything different.

14:45.130 --> 14:49.020
We haven't even tuned the parameters of the support that they're machine.

14:49.120 --> 14:52.380
What we did is we just we did normalization.

14:52.390 --> 14:53.900
That's all what we wanted.

14:53.920 --> 14:55.830
That's kind of the first improvement to our model.

14:56.080 --> 14:58.920
I hope you guys enjoy that section and see you in the next section.

14:59.200 --> 15:05.080
The next improvement that we're going to do to the model that will try to optimize our C and gamma parameters

15:05.170 --> 15:06.940
of the support vector machines.

15:07.030 --> 15:07.650
Right.

15:07.890 --> 15:13.820
The only problem is how can we actually search for the best values are the best parameters of C and

15:13.840 --> 15:14.590
gamma.

15:15.050 --> 15:21.970
Luckily Skillern actually had a kind of a method that tells us or provides all the grid search for us

15:22.250 --> 15:23.960
its search for the best parameters.

15:24.040 --> 15:28.040
And you don't have to worry about it which is going to tell you okay that we're best this or best estimate

15:28.040 --> 15:29.550
or it can just use it.

15:29.620 --> 15:35.560
And let's give let's give it a shot and let's see how can we search for the best parameters.

15:35.560 --> 15:43.110
The first step is we're going to call simply we're going to define our range so we're going find we'll

15:43.110 --> 15:47.770
call it program grids and then we're going to define OK we're going to look into the sea will not try

15:47.770 --> 15:50.900
to optimize our perimeter and our gamma parameter.

15:51.170 --> 15:51.550
OK.

15:51.550 --> 15:52.260
And what do you.

15:52.280 --> 15:58.480
Colonel RCF or radial basis function and here we're going to specify simply our range so here we get

15:58.480 --> 16:01.640
a look from Pinpoint 1 110 and hundred and gamma.

16:01.660 --> 16:06.380
We're going to be that simply we're going to create our grids to look to search for to look for it.

16:06.500 --> 16:11.380
You looking at between let's say point 1 1.1 point or 1 and point 0 1.

16:11.530 --> 16:13.450
Right OK.

16:13.630 --> 16:20.990
The next step is we're going to import the grid search C-v from escolar so going to right from from

16:21.430 --> 16:34.090
scaler dot model selection and we're going to click Import grid search CV.

16:34.200 --> 16:35.080
All right.

16:35.700 --> 16:37.280
OK so now we're good.

16:37.290 --> 16:44.990
The next step is we're going to use grid search C-v to simply search within our grids right.

16:45.060 --> 16:49.830
So we're going to do it that we're going to simply use our as we see objects.

16:50.010 --> 16:50.270
Right.

16:50.280 --> 16:54.520
Which is support for stand for and support the vector classifier and we're going to do with going and

16:54.520 --> 16:59.010
use our grade that were created before which is parameter underskirt grades which is our trade that

16:59.010 --> 16:59.680
we had.

17:00.000 --> 17:07.590
And here we're just going to write refits equates to and for very Beauce we're going to specify 2 4

17:07.600 --> 17:15.150
5 or whatever simply that we're just going to show us how many values that we want to display while

17:15.300 --> 17:16.410
searching for our great.

17:16.500 --> 17:17.210
OK.

17:17.680 --> 17:19.590
And I'm going to show you exactly what you mean by this.

17:19.880 --> 17:20.170
OK.

17:20.190 --> 17:22.360
That would simply return our grids.

17:22.560 --> 17:23.160
OK.

17:23.460 --> 17:30.750
The next step is that we're going to simply use our grids to fit the training data which is in this

17:30.750 --> 17:36.750
case I'm going to use my skill training data to actually get even better values compared to the data

17:36.790 --> 17:38.040
that original data that we have.

17:38.080 --> 17:38.660
All right.

17:39.030 --> 17:39.790
So we're going to do.

17:39.830 --> 17:41.420
They're going to use grids.

17:41.450 --> 17:48.070
These are objects that we had when he used the fit method on our grids simply using the X underscore

17:48.120 --> 17:50.270
train the scores scaled

17:53.070 --> 17:55.410
along with y underscored.

17:55.830 --> 17:56.470
Right.

17:56.850 --> 17:57.770
Okay great.

17:57.780 --> 17:58.920
That's right.

17:58.920 --> 17:59.310
All right.

17:59.310 --> 18:01.580
So that's Ronit so going around Preem great.

18:01.710 --> 18:06.460
And then we're on the import and then we're going to run the grid search.

18:06.570 --> 18:11.200
And then afterwards we're going to run the fit simply using the extreme scaled and the Y.

18:11.550 --> 18:12.420
All right.

18:12.420 --> 18:18.960
So if we scroll down here it will show you that it has been searching for the best you know values of

18:18.990 --> 18:28.220
gammoned see and what we need to do that we need to get our best value is simply using grids.

18:28.410 --> 18:36.210
OK so we're going to do we're going to use grids Daut best underskirt pram prims.

18:36.340 --> 18:36.680
All right.

18:36.690 --> 18:42.210
That would show us our best value section and tell me OK the best values to choose is actually if you

18:42.210 --> 18:45.110
see of 10 and gamma of point 1.

18:45.240 --> 18:45.510
Right.

18:45.570 --> 18:47.710
That's the best that is right.

18:48.030 --> 18:49.320
Okay great.

18:49.320 --> 18:50.490
Let's keep going.

18:50.910 --> 18:59.820
So what we could do afterwards we're going to use our grid dot products to simply use the kind of predictions

19:00.030 --> 19:04.390
Plus the confusion matrix with the best values are the best parameters that we had already.

19:04.390 --> 19:04.710
All right.

19:04.710 --> 19:07.520
So we don't need to actually go and rerun it again.

19:07.530 --> 19:10.810
We can actually run it directly on our grid object.

19:10.920 --> 19:11.930
So we're going to do it right.

19:11.940 --> 19:12.590
Great.

19:12.650 --> 19:19.970
Dodd predicts and we're going to use going to use our X underscored test underscores scale values right.

19:19.980 --> 19:24.710
That well should tell me or my grade predictions.

19:24.820 --> 19:26.650
You can call it whatever you want.

19:26.670 --> 19:27.040
OK.

19:27.060 --> 19:28.380
That would give me my good prediction.

19:28.380 --> 19:32.110
That's kind of the output using the best or optimized values.

19:32.150 --> 19:32.580
All right.

19:33.950 --> 19:34.440
All right.

19:34.460 --> 19:36.030
So now we have our great predictions.

19:36.080 --> 19:44.450
The next step that we're going to compute our confusion matrix Metsi and equals to confusion confusion

19:44.510 --> 19:46.610
on the score matrix.

19:46.670 --> 19:52.140
And we're going to specify again our why tests and our great predictions.

19:52.490 --> 19:55.080
The exact same fashion as we had earlier.

19:55.080 --> 19:55.700
All right.

19:56.000 --> 19:56.920
Let's turn this.

19:56.920 --> 19:57.750
We're good.

19:57.890 --> 20:02.780
The next step we're going to create again our heat maps we're going to use seaborne Asinus dot heat

20:02.780 --> 20:11.420
map and we're going to use our confusion metric CM but we use a notation equals two right.

20:11.660 --> 20:12.220
Right.

20:12.230 --> 20:13.730
Let's run this as well.

20:14.050 --> 20:14.910
OK.

20:14.990 --> 20:16.930
That's actually even better.

20:16.970 --> 20:21.930
So as you guys can see here we found that we only misclassified only three points.

20:21.970 --> 20:22.590
OK.

20:22.760 --> 20:28.220
Which is obviously better than misclassifying only 5 points and that means we are actually on the right

20:28.220 --> 20:28.430
track.

20:28.430 --> 20:36.200
That means we are using are optimizing the values of the of the machine and setting it to seek withstand

20:36.200 --> 20:39.290
gamma equal point one that actually dramatically improve the results.

20:39.290 --> 20:42.230
All right let's plot the classification report.

20:42.320 --> 20:47.380
So if we go up here if you recall we had our classification report.

20:47.550 --> 20:52.370
So just going to print our classification report when instead of why predict we're just going to use

20:52.370 --> 20:56.190
our wide basically great predictions instead.

20:56.280 --> 20:56.990
Right.

20:57.240 --> 20:58.970
So let's try it.

20:59.050 --> 21:04.860
Actually we reached 97 percent accuracy in sort of the previous 96 percent accuracy which is even greater.

21:05.150 --> 21:08.810
And we only again misclassified three samples right.

21:09.020 --> 21:17.130
One last point that I wanted to summarize is that here we had the misclassification points.

21:17.150 --> 21:21.290
It's actually type 1 error which is perfect which is obvious not perfect.

21:21.290 --> 21:22.790
We would love to have on the percent.

21:23.060 --> 21:29.060
But here we only showed that we only have 3 misclassified samples of type 1 error.

21:29.060 --> 21:29.840
Why.

21:29.840 --> 21:37.070
Because here the my predictions said that the cancer was malignant which is close to zero.

21:37.070 --> 21:43.490
However it was simply the patient was fine was just benign He had a benign cancer which means we are

21:43.550 --> 21:45.430
even if we misclassified free samples.

21:45.440 --> 21:49.420
It's kind of you know type 1 error which is not terrible or not too bad.

21:49.760 --> 21:54.730
I hope you guys enjoyed that section and I hope you enjoyed the case study and see you in the next section.
