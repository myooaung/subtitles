WEBVTT

00:01.350 --> 00:02.780
Welcome back everybody.

00:02.970 --> 00:06.160
In this video we're going to continue our model building process.

00:06.180 --> 00:11.100
We're going to use the same code that we use in the previous video to apply different models to our

00:11.170 --> 00:16.440
dataset and see what results we get as a barrier and we're going to compare those results.

00:16.440 --> 00:18.350
So let's just get started.

00:18.750 --> 00:25.740
Now if you guys remember from the last video we wrote all these lines to create our model and to evaluate

00:25.740 --> 00:26.490
it.

00:26.790 --> 00:30.720
I decide to save this resource to a resource Barabara here.

00:31.170 --> 00:38.370
And what we're going to do to apply the other models is just copy and paste this code.

00:38.610 --> 00:43.790
Now of course this is not only going to be linear logistic regression this is going to be support back

00:43.800 --> 00:49.460
to machine and to actually use the support back the machine we've got.

00:49.650 --> 00:57.170
So it's imported from a skill learned that SBM and we're going to import the function SPC or the classes

00:57.200 --> 01:01.520
wesay which represents the Sopore vector machine or a sportbike to classify.

01:01.520 --> 01:07.500
In this case because this is a classification model so the classifiers going to be as we see.

01:08.220 --> 01:13.770
Of course there's not going to be a pinata here but instead there's going to be a kernel because we're

01:13.770 --> 01:16.590
going to be using kernels for a support vector machines.

01:16.950 --> 01:23.550
And because of that I'm actually going to label this section SBM linear because that is the kernel that

01:23.550 --> 01:26.760
I'm going to be using for the support commission.

01:26.980 --> 01:28.830
So let's call it linear.

01:28.910 --> 01:33.730
As you can see here then we actually train the model.

01:35.580 --> 01:37.540
That's one take a few seconds to run.

01:37.710 --> 01:42.660
Meanwhile we realize that everything else in here is going to remain the same.

01:42.680 --> 01:48.630
Nothing is changing in these fuel lines right here that we copy and we're going to call the result of

01:48.660 --> 01:52.220
our actual model car model results.

01:53.520 --> 01:55.060
And we're going to run.

01:55.620 --> 01:57.340
So actually run it together.

01:57.670 --> 01:58.290
Great.

01:58.830 --> 02:00.820
And now we have our model results.

02:01.070 --> 02:07.470
Now let's not forget who came in the name of the model to actual support back to machines linear

02:10.910 --> 02:12.280
and there we go.

02:12.320 --> 02:17.360
Now we just have to run this line create a model.

02:17.420 --> 02:21.520
And finally we need to append these to the initial results table.

02:21.860 --> 02:31.860
So we're going to use results that were sold at auction rate and we're going to append tomorrow.

02:32.690 --> 02:34.200
Excellent.

02:35.330 --> 02:42.290
And finally this is just something that I do to reset in access when I'm working is to use the argument

02:42.860 --> 02:46.430
you know X equals truth.

02:46.520 --> 02:49.430
This is not needed for a particular task.

02:49.430 --> 02:50.770
So let's run this.

02:51.050 --> 02:51.900
Great.

02:51.980 --> 02:56.390
Now it's good to see our results in the con..

02:56.390 --> 03:04.580
So in the console's we're going to type in results and then we we have now the Superbike to machine

03:04.580 --> 03:06.170
linear model.

03:06.170 --> 03:11.080
And as you can see here the accuracy is almost identical so is the precision.

03:11.150 --> 03:14.270
The recall is pretty high it's even higher than the one previously.

03:14.300 --> 03:16.880
So obviously there's still bias in this linear.

03:16.890 --> 03:23.030
So we're back to machine model and the F1 score is 64 a little bit higher but not that much better.

03:23.060 --> 03:27.200
So now let's do the same for our different cardinal in this case.

03:27.200 --> 03:30.190
We're going to use the RPF.

03:30.830 --> 03:36.840
So they use it for word machine RBA.

03:36.850 --> 03:40.340
The Colonel is not going to be of course here.

03:40.700 --> 03:42.070
And we run this.

03:42.150 --> 03:47.810
One last thing the change is right here in the title and that's it.

03:47.810 --> 03:51.200
And then we combine all of this and explore the results again.

03:51.500 --> 03:57.530
So this is going to take a few more seconds to run but once that finish we head up to the console and

03:57.530 --> 04:00.960
we run the results the different.

04:02.330 --> 04:03.690
So we have artier.

04:04.010 --> 04:09.750
So our b f actually gives us a better accuracy by almost 3 points or a percent.

04:09.770 --> 04:13.630
The precision increases as well and the recall goes down a little bit.

04:13.840 --> 04:14.410
Okay.

04:14.690 --> 04:15.940
So the accuracy is higher.

04:15.940 --> 04:16.860
So that's a positive.

04:16.860 --> 04:22.360
This tells us is a better model now although the recall has gone down so has a precision going up.

04:22.400 --> 04:25.870
So the Oberle one score which is the balance of these two.

04:26.000 --> 04:33.560
It's still similar to say SBM linear which is sixty four point five almost sixty five percent.

04:34.040 --> 04:38.850
So this slightly better is less and balance and has higher accuracy.

04:38.900 --> 04:43.220
So finally the last model that we're going to use is going to be the run for US model.

04:43.220 --> 04:46.180
So let's see we copy and paste again.

04:46.610 --> 04:49.580
We change the title to random forest

04:54.320 --> 05:01.220
and we're going to select anti-course 100 because this is going to be a random for us model with 100

05:01.700 --> 05:03.370
tricks.

05:03.480 --> 05:04.090
OK.

05:04.220 --> 05:05.830
So so far so good.

05:05.990 --> 05:08.620
Then let's get rid of all this.

05:08.720 --> 05:14.750
Obviously we are not going to actually keep here but we are not going to be using SBM we're going to

05:14.750 --> 05:21.390
import it a friend function is going to come from a scalar and some blood and some.

05:21.500 --> 05:22.500
There you go.

05:22.610 --> 05:25.130
And then we're going to import run sports classifier

05:29.230 --> 05:30.990
of course clarifier excellent.

05:31.010 --> 05:37.950
And now when we called the classifier we're going to call them right here with the function run once

05:37.990 --> 05:39.170
classified.

05:39.440 --> 05:45.020
Now there's a couple of arguments that this classify has to take the random state.

05:45.020 --> 05:46.390
Of course we're going to live in a.

05:46.550 --> 05:48.560
We're going to get a number of trees.

05:48.560 --> 05:49.580
Number of estimators.

05:49.640 --> 05:50.030
Right.

05:50.180 --> 05:56.990
So this is given Boody and estimators argument and we're going to say equal to 100 or going to 100 trees

05:56.990 --> 06:02.720
like we put into the description then we're going to let a criteria and this criterion is going to be

06:02.780 --> 06:05.910
entropy for now.

06:06.200 --> 06:11.360
So there should be as the classifiers build and we could actually run this entire line and see what

06:11.360 --> 06:13.080
we get.

06:13.130 --> 06:15.730
So this is going to run for a few seconds as well.

06:15.770 --> 06:20.340
We're going to head to the console and see the results.

06:21.650 --> 06:22.540
And there we go.

06:22.550 --> 06:26.210
Now this results are much different innocence.

06:26.420 --> 06:33.950
So the run for us with 100 trees has given us a 62 percent which is almost almost exactly actually 3

06:33.950 --> 06:37.310
percent higher than the SBM which was the highest accuracy we had.

06:37.310 --> 06:39.920
So this is definitely the most accurate model.

06:39.910 --> 06:44.320
Furthermore we can see the precision has gone up and the recall has remained the same.

06:44.320 --> 06:51.170
What is actually gone down by two points but the actual F-1 score has gone up meaning that it is more

06:51.170 --> 06:51.990
balance.

06:52.130 --> 06:57.400
It is best performing within these two scores as you can see 64 and 67 is not that much different.

06:57.410 --> 07:04.560
I like the 57 and 70 of the original linear logistic regression model Sorry I should relabel this is

07:04.650 --> 07:06.340
logistic regression work.

07:06.950 --> 07:13.280
Regardless this is why we are going to end up using because you guys can see that is a much better performing

07:13.280 --> 07:14.360
model.

07:14.360 --> 07:15.570
Excellent.

07:15.680 --> 07:21.590
So now that we have decided which model to use the last step that we're going to care about is to actually

07:21.800 --> 07:26.200
balladry these models to see if performs like he says he does right.

07:26.480 --> 07:33.700
So to do that we're going to apply as a K4 course validation to the actual Trinian's it.

07:34.040 --> 07:35.320
So let's take a look at that.

07:36.710 --> 07:45.100
The first part being the section is focused by the patient.

07:46.440 --> 07:47.720
Great.

07:47.850 --> 07:49.560
We're going to import it from Eski learn

07:58.130 --> 08:06.670
great and we're going to call accuracy's as the set of results from this cross-pollination.

08:06.920 --> 08:12.320
So the estimator for these K-4 is going to be the classified that we just built which is the random

08:12.320 --> 08:14.690
for one with 100 trees.

08:15.020 --> 08:21.820
X is going to be extreme of course Y is going to be y trend

08:24.950 --> 08:30.360
and the number of faults on this coast validation is going to be.

08:30.460 --> 08:32.560
So it's going to be 10 for quite awhile.

08:33.110 --> 08:34.260
Great.

08:34.370 --> 08:41.570
So finally we're going to create a print statement to show us our results a little bit more clearly.

08:41.570 --> 08:47.340
I just pasted it right here which is just around the fourth classifier accuracy and skewness two numbers.

08:47.390 --> 08:53.030
The accuracy is going to be here plus or minus the standard deviation times 2 is actually going to be

08:53.040 --> 08:54.440
distant deviation things too.

08:54.740 --> 08:59.750
So there's going to of course comes from the accuracy's mean which is the average of the resource that

08:59.750 --> 09:03.210
we're going to get above and then the deviation of the response times too.

09:03.470 --> 09:08.090
So let's just run all of this and see what results we get.

09:08.120 --> 09:11.460
This is going to run for a few minutes depending on your machine.

09:11.480 --> 09:15.720
So as soon as this is over I'm going to come back.

09:16.070 --> 09:16.700
OK.

09:16.720 --> 09:17.760
So it's done.

09:17.800 --> 09:25.330
And I run this printout line and see what we do in the console the accuracy that we got for Kafer cross-pollination

09:25.350 --> 09:31.900
was 63 percent study creation around three point meaning that it could be 160 to 66.

09:31.990 --> 09:37.280
This is even higher than the run we did in the results there.

09:37.450 --> 09:40.590
So if we look at results we see that we got around 60 percent here right.

09:40.780 --> 09:46.810
So if we do give our gross validation we are guaranteeing that this model is or we're almost guaranteeing

09:47.020 --> 09:52.470
that this model is consistent throughout all of the data that exists in the train and said Okay.

09:52.480 --> 09:54.640
So in a way was strain.

09:54.730 --> 09:59.300
There was no particular subset of people who were just way off the model.

09:59.320 --> 09:59.980
So this is good.

09:59.980 --> 10:05.200
So then we can finally guarantee that run the force is the best option we have to run here and we're

10:05.200 --> 10:07.400
going to be using it going forward.

10:07.900 --> 10:10.960
Thank you very much for watching and see you in the next media.
