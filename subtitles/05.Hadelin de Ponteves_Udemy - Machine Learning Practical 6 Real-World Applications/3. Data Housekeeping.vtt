WEBVTT

00:01.080 --> 00:02.260
Welcome back everybody.

00:02.580 --> 00:06.450
And this video we're going to begin the first step of the idea process.

00:06.600 --> 00:08.370
We're going to import the data set.

00:08.370 --> 00:12.770
We're going to do a couple of the exploration and we're going to clean up a little bit.

00:12.990 --> 00:18.590
All this is going to be some housekeeping before we're ready for probably the histograms.

00:18.690 --> 00:21.260
So let's begin now.

00:21.570 --> 00:22.260
The first person.

00:22.320 --> 00:27.450
Every script that we write here is going to be important libraries of course.

00:27.450 --> 00:32.230
So we're going to import Pandurs map least leave Nampa and see if we're good.

00:32.430 --> 00:35.210
Now let's worry about importing the actual data.

00:35.610 --> 00:39.120
So we create a new data set variable.

00:39.390 --> 00:44.490
We are going to import this using read the C S B be great.

00:44.940 --> 00:52.880
And finally the data set is actually called financial data that see as we exit and so let's run it Larry

00:52.920 --> 00:55.370
or we have data data set import.

00:55.620 --> 00:58.330
So now let's begin the actual FDA process.

00:58.330 --> 01:00.040
So we're going a little CDA.

01:00.450 --> 01:07.800
And of course not every data set that we're going to have is going to be in a CSP format sometimes when

01:07.800 --> 01:10.980
we do these projects where we're carrying data from a database.

01:11.130 --> 01:13.120
So we always have to do.

01:13.230 --> 01:21.670
And I'd recommend this for a project just to we see it on your actual interpreter the con..

01:21.690 --> 01:25.080
So here we see the data as we've seen in the previous video.

01:25.110 --> 01:26.540
Everything that we expected.

01:26.730 --> 01:27.630
That's good.

01:27.720 --> 01:33.170
We actually looked at the columns again.

01:33.390 --> 01:37.080
Pretty straightforward everything that we have exploring the Prue's video and what actually matters

01:37.080 --> 01:46.080
here and what I want to do that we haven't done before so far is to look at the descrive function.

01:46.080 --> 01:52.100
So we applied the describe method to dataset and Sukant gibbers a description of all the market calls.

01:52.260 --> 01:59.380
So it didn't really matter because it's just a randomized user id h actually does matter.

01:59.620 --> 02:00.090
OK.

02:00.210 --> 02:03.550
So we see that the minimum age is 18 years so which is what we expect.

02:03.900 --> 02:06.110
And we see that the maximum age 96 years old.

02:06.110 --> 02:08.240
So there's somebody who is pretty old.

02:08.610 --> 02:15.690
Now the media which is the 50 percentile is on around to your source again 480 evenly distributed.

02:15.740 --> 02:17.530
It's nothing crazy on this.

02:17.540 --> 02:23.970
And if we jump into the next feels home owner seems to be pretty good the medium still 0 but the 75

02:23.970 --> 02:26.160
percentile is now one.

02:26.160 --> 02:31.160
So this tells us that it's somewhat evenly split because the mean is 45 42 percent.

02:31.160 --> 02:31.680
All right.

02:32.100 --> 02:39.990
So the income the lowest monthly income is 905 the high ones we have is almost $10000 per month.

02:40.350 --> 02:47.140
Now four months employ obviously the smallest one and the one to be zero which is months zero 1 1.

02:47.430 --> 02:52.740
And this one's going to be month 11 get the same four years employed.

02:52.770 --> 02:59.000
Your employer is going to be the minium in 0 years employee meaning that you have less in the year employed

02:59.040 --> 03:01.420
and the maximum went up to 16.

03:01.680 --> 03:08.560
And we see something similar in corrent address if we just jump into personal account again months.

03:08.650 --> 03:13.370
Our upper bounded by 11 years are not upper bound or Also we don't have anything here.

03:13.520 --> 03:20.070
Fuller has that we see that the average is 75 79 percent meaning there's 79 percent of these people

03:20.070 --> 03:27.780
right here have some that amount requested it's on the lower lower end 350 the highest then now and

03:27.780 --> 03:29.130
again pretty pretty straightforward.

03:29.130 --> 03:35.340
And then we have the racecourse the center scores inquiries the higher amount increases thirty's So

03:35.340 --> 03:40.860
we have up to a person who has had 30 inquiries in the last month which is pretty crazy actually but

03:40.860 --> 03:47.760
the median and the 75 percent range from 6 to 8 so this third is probably just an aberration and we

03:47.760 --> 03:50.820
have the response Berryville with 52 percent.

03:50.820 --> 03:56.110
We can conclude that almost half of them are esign and I'm happy with them are not esign So they're

03:56.130 --> 03:59.440
pretty balanced when it comes to we're good.

03:59.760 --> 04:01.870
So now just do a little more housekeeping.

04:02.010 --> 04:09.140
So we're going to begin with cleaning the data and cleaning the data and we're going to do this as follows.

04:09.150 --> 04:15.060
We're going to apply the east and they function and we're going to play the any function but both methods

04:15.060 --> 04:16.190
to this dataset.

04:16.260 --> 04:25.350
And what is going to return is all the columns that have any kind of any and here we can see that none

04:25.350 --> 04:26.100
of the columns.

04:26.100 --> 04:30.810
There is no particular field here that has any missing values which is pretty good.

04:30.810 --> 04:35.100
This is not always going to be the case with datasets that we work with infinity or any other industry

04:35.610 --> 04:42.470
but because these sort of people are users who are coming from a intermediary market place then.

04:42.600 --> 04:48.150
Then we can believe that the data that we're getting from them is accurate whatever data cleaning they

04:48.150 --> 04:53.670
do whatever regular data they exist that shouldn't be used.

04:53.730 --> 04:56.500
They're probably going to clean it before it even arrives to us.

04:56.670 --> 05:03.240
So we have a very good expectation that the data we're getting from their P2P marketplace is clean.

05:03.390 --> 05:06.350
So this is just a housekeeping to make sure that our data is clean.

05:06.350 --> 05:08.360
This is something they should always check.

05:08.520 --> 05:14.040
Even though we're not going to do anything with this in this video because it's already clean you should

05:14.040 --> 05:17.610
always check this just to make sure that you're sure this is the case.

05:17.640 --> 05:18.080
OK.

05:18.240 --> 05:19.930
So that's about it for this video.

05:19.980 --> 05:23.620
Just a quick housekeeping analyzing what we're working with.

05:23.730 --> 05:29.280
And now we can actually get ready to work on the histograms and see how the data is Bulos better through

05:29.340 --> 05:30.820
histogram plots.

05:30.840 --> 05:33.410
Thank you very much for watching and see you in the next Meteo.
