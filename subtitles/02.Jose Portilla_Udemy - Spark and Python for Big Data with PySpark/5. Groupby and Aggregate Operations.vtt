WEBVTT
1
00:00:05.540 --> 00:00:11.360
Welcome to this lecture on group by an aggregate functions with SPARK data frames GROUP BY allows you

2
00:00:11.360 --> 00:00:14.420
to group browes together based off some column value.

3
00:00:14.540 --> 00:00:19.970
For example you could group together sales data by the day the sale occurred or group repeat customer

4
00:00:19.970 --> 00:00:22.220
data based off the name of the customer.

5
00:00:22.340 --> 00:00:26.570
Once you performed that group by operation you can then use some sort of aggregate method or function

6
00:00:26.630 --> 00:00:33.380
off of that data on an aggregate function does is it combines or aggregates multiple rows of data into

7
00:00:33.380 --> 00:00:34.820
a single output.

8
00:00:34.820 --> 00:00:41.460
An example would be taking the sum of a bunch of numerical inputs or counting the number of inputs etc..

9
00:00:41.570 --> 00:00:49.350
Let's see some examples of this by playing an example dataset in Egypt or an output in this new book

10
00:00:49.380 --> 00:00:57.450
I'm going save from PI spark sequel import a spark session and I will quickly create a spark session

11
00:00:57.880 --> 00:01:00.550
so I can do this all with just tab autocomplete.

12
00:01:00.880 --> 00:01:01.960
So let's go through that.

13
00:01:03.250 --> 00:01:11.640
And we'll call this something like agues and then we'll say or create.

14
00:01:11.890 --> 00:01:14.050
And then after this we're going to actually read some data.

15
00:01:14.050 --> 00:01:21.160
So we'll say Sparke that read that CSFB and we'll be working with sales info CSC and I will also infer

16
00:01:21.160 --> 00:01:22.690
the schema to be true.

17
00:01:22.900 --> 00:01:25.640
And I also will declare that the header is true.

18
00:01:27.180 --> 00:01:29.980
And Altes actually see what this data looks like.

19
00:01:30.220 --> 00:01:35.710
So I can see you have a company column with the actual company logo or name Google Microsoft Facebook

20
00:01:35.760 --> 00:01:36.430
Apple.

21
00:01:36.550 --> 00:01:41.250
The person that works at that company and then their sales may be how much software they've sold to

22
00:01:41.260 --> 00:01:42.210
customers.

23
00:01:42.370 --> 00:01:47.470
And I can also check out the schema by saying print schema and I can see that I have two strings and

24
00:01:47.470 --> 00:01:52.920
a double so only one numerical column and that's the sales column.

25
00:01:52.920 --> 00:01:59.440
Now let's play around with actually grouping together so let's grouped together by the company column.

26
00:01:59.630 --> 00:02:08.270
So I will say the math and I can do it by and then pass in the company or the actual name of the column

27
00:02:08.270 --> 00:02:09.660
in this case it's company.

28
00:02:09.770 --> 00:02:15.350
And if I run this I get back a group that grouped data object and actually shows you its location in

29
00:02:15.350 --> 00:02:16.220
memory.

30
00:02:16.220 --> 00:02:21.650
So often this grouped data object I can call a variety of methods and you can check out the link in

31
00:02:21.650 --> 00:02:27.140
the lecture notebook so you can actually see a variety of methods available to you if you actually click

32
00:02:27.140 --> 00:02:27.600
on the link.

33
00:02:27.650 --> 00:02:33.200
Looks something like this or like this really and you can scroll down and see all the various functions

34
00:02:33.200 --> 00:02:34.210
that are available to you.

35
00:02:34.220 --> 00:02:36.540
But let's go back to the notebook for now.

36
00:02:37.360 --> 00:02:44.470
And actually walk through some of the most common examples so I'll say the f group by company and then

37
00:02:44.470 --> 00:02:50.590
I'm going to call mean and this will actually return to frame with company string and then average sales

38
00:02:50.590 --> 00:02:52.570
so let's actually show the result.

39
00:02:54.250 --> 00:02:58.210
And here I can see when you group it by company what the average sales are.

40
00:02:58.210 --> 00:03:03.700
So Apple average Google average Facebook average etc. and let me walk through some of the most common

41
00:03:03.700 --> 00:03:07.250
ones we can take the sum of all the sales per company.

42
00:03:07.300 --> 00:03:09.670
Can also do things like Max and Min.

43
00:03:09.670 --> 00:03:11.560
So what is the max sale.

44
00:03:11.560 --> 00:03:12.810
What is the minimum sale.

45
00:03:13.740 --> 00:03:14.370
Per company.

46
00:03:14.370 --> 00:03:17.070
So you can see those examples there and you can even do things like count.

47
00:03:17.070 --> 00:03:19.740
So how many rows are there per company.

48
00:03:19.770 --> 00:03:22.930
So Apple has four employees in this state of frames.

49
00:03:22.950 --> 00:03:25.200
If you come back here you can see Apple has four rows.

50
00:03:25.200 --> 00:03:27.350
Facebook has to come back down here.

51
00:03:27.360 --> 00:03:32.620
We can see that Facebook has to now there's a variety of methods that are available to you.

52
00:03:32.620 --> 00:03:36.610
I just showed some of the most common ones you can definitely check out the link in the lecture notebook

53
00:03:36.970 --> 00:03:41.200
that will show you the variety of methods you can call Offut group data object.

54
00:03:41.200 --> 00:03:43.840
Now not all methods needed group by call.

55
00:03:43.840 --> 00:03:49.570
Instead you could just call a generalized thought AGM method that will aggregate across all rows in

56
00:03:49.570 --> 00:03:50.430
the data frame.

57
00:03:50.440 --> 00:03:51.550
So what does that mean.

58
00:03:51.550 --> 00:03:55.180
Maybe you don't actually need the group by Maybe you just want the sum of all sales.

59
00:03:55.180 --> 00:04:00.950
You don't really care about something like per company so you can do that with the AG or that method

60
00:04:01.570 --> 00:04:04.870
and that actually takes a really interesting argument and takes in a dictionary.

61
00:04:04.870 --> 00:04:11.670
So the issue with that looks like we'll say DFA A.G. And then a dictionary will have the key be the

62
00:04:11.670 --> 00:04:12.990
name of the column.

63
00:04:12.990 --> 00:04:17.750
So in this case sales number one not doing group by we're just aggregating across everything.

64
00:04:18.090 --> 00:04:25.030
And then we'll do something like some and then we'll say that show and now this returns the sum of all

65
00:04:25.030 --> 00:04:26.540
the sales in the data frame.

66
00:04:26.770 --> 00:04:32.020
So again we pass in the column that we want to aggregate and then the actual function we want to use

67
00:04:32.200 --> 00:04:32.800
as a string.

68
00:04:32.790 --> 00:04:34.410
So maybe I want the max sale.

69
00:04:34.540 --> 00:04:39.450
So across every single row in that data from what actually happened there the max sale was 870.

70
00:04:39.460 --> 00:04:45.370
So if we go back to our original we can see here that the highest sale was in fact a 70.

71
00:04:45.430 --> 00:04:48.140
Now we could have also done this on a group by.

72
00:04:48.140 --> 00:04:51.910
So if you end up liking the sort of dictionary syntax you can do that as well.

73
00:04:51.910 --> 00:04:58.120
Let me show you how we can kind of achieve that effect so I'll create something like group underscore

74
00:04:58.450 --> 00:05:08.110
data and I'll set the sequel to DMF the group by and I will group by company and then I'm going to run

75
00:05:08.110 --> 00:05:13.460
that and then I'll grab that group data and I can call the AG Methot off that.

76
00:05:13.510 --> 00:05:18.930
And then again pasan something like sales and then maybe Maxiell.

77
00:05:19.150 --> 00:05:25.530
And this is going to be the exact same thing as if I just use that group by Max and there it is.

78
00:05:25.530 --> 00:05:29.700
So this kind of nice because it allows you to if you're doing a for loop or something.

79
00:05:29.730 --> 00:05:33.110
Insert keys or grab a dictionary from somewhere else etc..

80
00:05:33.120 --> 00:05:38.700
So a little bit more of a generalized format than just calling something like if we go up here Doc count

81
00:05:38.730 --> 00:05:40.430
or Max et cetera.

82
00:05:40.440 --> 00:05:43.770
So again kind of a nice little format with this dictionary.

83
00:05:43.770 --> 00:05:48.160
You can check out the various functions available for this in the lecture notebook.

84
00:05:48.210 --> 00:05:53.390
Speaking of functions let's show you how you can import functions from Sparke.

85
00:05:53.760 --> 00:06:01.800
So in order to do this you'll save from Paice spark thought sequel functions import and then you can

86
00:06:01.800 --> 00:06:06.070
hit tab here and it will show you some various functions as you begin to scroll down here.

87
00:06:06.120 --> 00:06:12.300
So we have things like concat core for correllation create map etc. and there's a lot of them more than

88
00:06:12.300 --> 00:06:14.050
we actually need to go through here.

89
00:06:14.070 --> 00:06:16.730
So throughout the course we'll show you the more important ones.

90
00:06:16.740 --> 00:06:25.290
But I want to kind of go over some of these so we'll do count distinct EVGA for average and then CD

91
00:06:25.380 --> 00:06:27.910
DV for standard deviation.

92
00:06:27.910 --> 00:06:36.220
And what I can do these functions is combine them with a select call so I can say ADF that select in

93
00:06:36.220 --> 00:06:42.220
whatever function I want in this case we'll do something like count distinct and then passen the column

94
00:06:42.220 --> 00:06:44.610
that I want to apply it to just a string SALES

95
00:06:47.290 --> 00:06:49.130
So what is actually doing when it returns.

96
00:06:49.150 --> 00:06:52.480
Well it counts the distinct number of sales values.

97
00:06:52.480 --> 00:06:54.280
So here we have 11.

98
00:06:54.520 --> 00:06:56.110
And what does it actually look like.

99
00:06:56.110 --> 00:07:00.860
So maybe I want to see a more clear example maybe average would be a little clearer.

100
00:07:01.000 --> 00:07:04.290
Here we can see the average sales was three sixty point five E-3.

101
00:07:04.300 --> 00:07:09.190
So this is more just the general way to grab a function import it.

102
00:07:09.340 --> 00:07:14.920
And then when you called the select you can apply that function to whatever column you want and show

103
00:07:14.920 --> 00:07:16.000
or collect the results.

104
00:07:16.000 --> 00:07:21.040
We're going to be seeing a lot more of this later on especially when we deal with dates and time stamps

105
00:07:21.040 --> 00:07:25.960
that sort of thing but just kind of get used to this format of a function from Paice spark that sequel

106
00:07:25.960 --> 00:07:29.880
functions and you pass that in within a select call.

107
00:07:29.890 --> 00:07:32.830
Now maybe you don't want this sort of notation of average sales.

108
00:07:32.980 --> 00:07:35.850
In that case you can actually use what's known as an alias.

109
00:07:35.920 --> 00:07:44.310
So inside of this select call you'll say alias and give this whatever column name you want so you can

110
00:07:44.340 --> 00:07:47.870
call this I don't know average sales.

111
00:07:47.880 --> 00:07:49.600
Maybe that's a little clearer to you.

112
00:07:49.820 --> 00:07:54.130
And now when you show that you can see the alias name for that column has been displayed.

113
00:07:54.180 --> 00:07:56.760
So it's kind of a nice little feature of that.

114
00:07:56.800 --> 00:08:01.750
And finally let's deal with the standard deviation function because I want to show you how you can actually

115
00:08:01.750 --> 00:08:03.260
format the data itself.

116
00:08:03.310 --> 00:08:13.810
So I will say DFI select and let's select the standard deviation of the sales column and let's show

117
00:08:13.810 --> 00:08:15.330
the result there.

118
00:08:15.430 --> 00:08:20.950
So you don't notice that standard deviation has this kind of weird column name as PDV CMP sales and

119
00:08:20.950 --> 00:08:22.700
then has a really long number there.

120
00:08:22.840 --> 00:08:26.770
I mentioned before that you can actually format these things to look a little nicer.

121
00:08:26.770 --> 00:08:28.320
Let me show you how you can actually do that.

122
00:08:29.780 --> 00:08:37.640
You'll save from Paice spark that sequel that functions and the one we want is called format underscore

123
00:08:37.700 --> 00:08:38.670
number.

124
00:08:38.840 --> 00:08:40.040
So we'll import that.

125
00:08:40.070 --> 00:08:45.050
And we're going to end up doing is passing in format number inside of that select call.

126
00:08:45.350 --> 00:08:47.210
But we can do this in a couple of steps.

127
00:08:47.270 --> 00:08:52.110
And I also want to fix this column name issue so let's do two things here.

128
00:08:52.130 --> 00:08:56.240
I already selected the standard deviation of the sales column but there's two things I don't like.

129
00:08:56.270 --> 00:09:00.520
I don't like this really long name and I don't like how many significant digits are here.

130
00:09:00.650 --> 00:09:02.450
So I'm going to fix this in a couple of steps.

131
00:09:02.450 --> 00:09:11.510
First I'll say sales SDD is equal to DSF select and I'm going to call standard deviation function on

132
00:09:11.510 --> 00:09:12.450
my sales column.

133
00:09:12.470 --> 00:09:19.430
But as we already showed I will give an alias of something simple like SDD And then when I want to do

134
00:09:19.430 --> 00:09:21.200
is just show you the results there.

135
00:09:21.200 --> 00:09:25.060
So here is a study for standard deviation show.

136
00:09:25.490 --> 00:09:26.320
And there it is.

137
00:09:26.330 --> 00:09:29.290
So I have this nice column name but I still have this number.

138
00:09:29.420 --> 00:09:30.830
So now let's fix that.

139
00:09:30.830 --> 00:09:37.450
So the fix that significant digits problem I can say the SDD dot and then I'm going to call select off

140
00:09:37.460 --> 00:09:45.170
offer this and then I passen whoops with a lowercase format number and it takes in the name of the column

141
00:09:45.170 --> 00:09:45.970
as the first argument.

142
00:09:45.980 --> 00:09:47.270
In this case it's already been changed.

143
00:09:47.270 --> 00:09:48.050
SDD.

144
00:09:48.380 --> 00:09:52.380
And then as a second argument how many decimals you want to show.

145
00:09:52.400 --> 00:09:58.250
So I just want to show two decimal places beyond that point and then say that show I run this and I

146
00:09:58.250 --> 00:10:03.440
can see now I have format number acidy to 250 point 0 9.

147
00:10:03.530 --> 00:10:08.270
So you can see here that I again had this issue of the alias actually have to call alias twice in order

148
00:10:08.270 --> 00:10:12.770
to fix this or just wait until the very end to call Aliase kind of up to you.

149
00:10:12.770 --> 00:10:17.570
So if we really want to fix this I can call Aliase and then say something like.

150
00:10:17.910 --> 00:10:19.800
Final.

151
00:10:19.960 --> 00:10:21.500
Again I could just call SDD.

152
00:10:21.510 --> 00:10:22.380
So we run that.

153
00:10:22.500 --> 00:10:26.500
And then this gives me kind of this nice result as Saidy 250 points or nine.

154
00:10:26.520 --> 00:10:28.610
Much nicer than what I had up here.

155
00:10:28.650 --> 00:10:30.580
So we called Aliase twice.

156
00:10:30.600 --> 00:10:33.280
You could've just called Aliase once and kind of skipped this step.

157
00:10:33.340 --> 00:10:34.680
Would would've had to change the name here.

158
00:10:34.690 --> 00:10:38.130
SDD to match up this kind of a longer column title.

159
00:10:38.130 --> 00:10:40.980
So kind of up to you or how you actually want to approach that.

160
00:10:40.980 --> 00:10:43.890
Now let's finally discuss how you can order and sort things.

161
00:10:43.980 --> 00:10:46.380
It's actually really easy to just use ORDER BY.

162
00:10:46.380 --> 00:10:49.650
So we come up to the state of frameless sure that data from one more time.

163
00:10:49.770 --> 00:10:55.130
So the data frame has this company the person and sales but let's try to sort things by that sales column.

164
00:10:55.140 --> 00:10:57.210
Right now it's kind of jumbled and mixed up.

165
00:10:57.270 --> 00:10:58.900
So they're put together by company.

166
00:10:59.010 --> 00:11:00.840
But if I want to order it it's no problem.

167
00:11:00.840 --> 00:11:04.870
I can just use the order by and then passing the column I WANT TO ORDER BY.

168
00:11:04.890 --> 00:11:10.550
So in this case an order by sales and let's show the results there in here.

169
00:11:10.560 --> 00:11:14.850
I've ordered from the lowest sales number all the way to the highest sales number.

170
00:11:14.850 --> 00:11:18.270
So this goes in ascending order based off the sales column.

171
00:11:18.270 --> 00:11:21.740
So just a simple order by if you want to call descending.

172
00:11:21.900 --> 00:11:24.430
Well that's a little more complicated but not so bad.

173
00:11:24.720 --> 00:11:25.940
You have to say.

174
00:11:26.970 --> 00:11:35.210
ORDER BY and then you pass in the actual column not just the column name you'll say DLF sales and then

175
00:11:35.210 --> 00:11:42.610
off of this column you'll say dot D E S C close parentheses and then show the results of this.

176
00:11:43.040 --> 00:11:45.820
So it's a little bit of a weird format but that's the way it works.

177
00:11:45.830 --> 00:11:52.310
If SPARC data frames and now I can see I have a descending value so goes from 870 all the way to the

178
00:11:52.310 --> 00:11:52.970
lowest.

179
00:11:53.120 --> 00:11:56.960
So you can kind of compare these two methods so let's kind of show them both in screening and a copy

180
00:11:56.960 --> 00:12:00.450
and paste this and show them both to you at same time.

181
00:12:00.470 --> 00:12:05.580
So this is for ascending order this very first one and the second bottom one is for the ascending order.

182
00:12:05.660 --> 00:12:10.600
So for ascending which is the default all you have to do is pass in the string for the column name for

183
00:12:10.610 --> 00:12:11.200
descending.

184
00:12:11.200 --> 00:12:15.650
A little more complicated you have to passen the column itself using that sort of index call and then

185
00:12:15.650 --> 00:12:17.800
called DGSE.

186
00:12:17.840 --> 00:12:23.120
All right those are the basics about using group by an aggregate functions as well as using order by

187
00:12:23.130 --> 00:12:24.620
and sorting your data.

188
00:12:24.620 --> 00:12:26.290
Thanks and I'll see you at the next lecture.
