WEBVTT
1
00:00:05.420 --> 00:00:08.510
Welcome back to the solutions lecture for the project.

2
00:00:08.510 --> 00:00:10.470
Exercise for spark data frames.

3
00:00:10.700 --> 00:00:13.230
Let's jump right to the Jupiter notebook and get started.

4
00:00:15.080 --> 00:00:15.390
All right.

5
00:00:15.410 --> 00:00:17.900
So let's get started here I am at the notebook.

6
00:00:17.900 --> 00:00:22.440
First thing I wanted to do was actually start a simple Spark's session.

7
00:00:22.520 --> 00:00:28.830
I'll start by saying from Paice park sequel import Spark's session.

8
00:00:29.020 --> 00:00:39.130
And then after that I will say Sparke is equal to lips Sparke session Daut builder app name.

9
00:00:39.130 --> 00:00:49.160
And let's just give this something like a so-l for solutions and then get or create.

10
00:00:49.180 --> 00:00:54.840
Now we want to load the Wal-Mart stock CSFB file and we want to have Sparc and further data types.

11
00:00:54.970 --> 00:01:02.640
So we'll have DMF SPARC read CSFB and the first thing I want to do is pass in that Wal-Mart stock C

12
00:01:02.640 --> 00:01:09.040
S V and I also want to say the header is true which hopefully are able to figure out.

13
00:01:09.160 --> 00:01:15.730
And then we also want to say infer schema is equal to true and we can put this on multiple lines to

14
00:01:15.730 --> 00:01:17.080
make it a little more readable.

15
00:01:17.380 --> 00:01:20.560
So run that and then we want to know what are the column names.

16
00:01:20.560 --> 00:01:27.550
Hopefully you remember that you can just call up columns to get a list of the column names to get what

17
00:01:27.550 --> 00:01:28.480
the schema looks like.

18
00:01:28.480 --> 00:01:33.640
All you have to do is say print schema and run that method and you can see the schema.

19
00:01:33.640 --> 00:01:34.960
We have a time stamp.

20
00:01:35.140 --> 00:01:40.350
Double double double integer for volume that makes sense and you can only trade a certain unit of stock

21
00:01:40.360 --> 00:01:45.250
you can't trade points seven stock given the way that the state assets presented to us.

22
00:01:46.160 --> 00:01:51.110
Next we want to actually say print out the first five columns so you don't actually need a for loop

23
00:01:51.110 --> 00:01:52.290
for this in the solutions.

24
00:01:52.290 --> 00:01:53.780
No but I should have for a loop.

25
00:01:53.780 --> 00:01:58.970
But the other way you could just do this really quickly is just say head of five and this will eventually

26
00:01:58.970 --> 00:02:02.270
show you the first five columns since this is a little messy.

27
00:02:02.270 --> 00:02:12.170
What I ended up doing was saying for row in D-Nev. had five print the row and then we can see a little

28
00:02:12.170 --> 00:02:13.940
bit of the same thing.

29
00:02:13.970 --> 00:02:17.170
So to clean it up I just added a new line.

30
00:02:17.510 --> 00:02:21.600
So that goes in as a string new and that breaks into line.

31
00:02:21.620 --> 00:02:23.210
So that's how we got the solution.

32
00:02:23.270 --> 00:02:26.360
If you just said DFA had a 5 that's totally okay to.

33
00:02:26.380 --> 00:02:29.460
Now want to use the scribe to learn about the data frame.

34
00:02:29.480 --> 00:02:35.030
Hopefully we're able to figure out that you had to say scribe and then show as well and then we can

35
00:02:35.030 --> 00:02:40.550
see a summary it's going to look kind of weird to us but we can see the summary the low count etc..

36
00:02:40.580 --> 00:02:42.170
And if you zoom out this will look a little nicer.

37
00:02:42.170 --> 00:02:46.380
So just to kind of clarify as you zoom out more ill make more sense.

38
00:02:46.380 --> 00:02:49.080
So zooming back in so you guys can see clearly.

39
00:02:49.160 --> 00:02:52.050
Let's continue on.

40
00:02:52.050 --> 00:02:52.470
All right.

41
00:02:52.470 --> 00:02:54.090
Up next was the bonus question.

42
00:02:54.090 --> 00:02:55.480
Now this is pretty tricky.

43
00:02:55.500 --> 00:02:57.570
And there's a specific reason for that.

44
00:02:57.570 --> 00:03:03.930
But the basic idea is there's too many decimal places if you go back up on this describe data frame.

45
00:03:04.110 --> 00:03:07.570
So a way too many significant figures are just past that small.

46
00:03:07.800 --> 00:03:10.910
And what we want to do is try to reduce that number.

47
00:03:10.980 --> 00:03:15.200
Previously we showed you how to use format underscore number to do that but there's going to be an issue.

48
00:03:15.210 --> 00:03:18.780
And the reason for that is because everything is a string.

49
00:03:18.780 --> 00:03:28.750
So if I say DMF describe print schema and show the results of that notice the open is a string.

50
00:03:28.750 --> 00:03:30.890
So is high so is low etc..

51
00:03:31.000 --> 00:03:36.600
So now I have this issue that if I got something like describe how it actually format the numbers something

52
00:03:36.610 --> 00:03:40.930
I could do is format strings but maybe I actually want to get numbers at the very end.

53
00:03:40.930 --> 00:03:42.300
So how do you actually do that.

54
00:03:42.460 --> 00:03:46.690
Well if he clicked on the link eventually you would see that you can actually cast stuff.

55
00:03:46.690 --> 00:03:49.670
So let's walk through one possible way of doing this.

56
00:03:49.690 --> 00:03:54.920
There's other possible ways so if you figure out another way feel free to share in the Q&amp;A forums.

57
00:03:55.200 --> 00:04:01.010
There is definitely more than one way to do this but one simple way early is maybe a little more straightforward

58
00:04:02.360 --> 00:04:08.750
is import format number from PI sparked that sequel that functions and then we'll say result is equal

59
00:04:08.750 --> 00:04:19.010
to the F describes so that actual description data frame and then eventually we say results selects.

60
00:04:19.170 --> 00:04:26.710
I grab the summary column from results and then when I see him do all the columns is a call format number

61
00:04:28.320 --> 00:04:29.950
passing the column I want to format.

62
00:04:29.940 --> 00:04:38.150
So something like open and then cast it to something like a fluke or an integer.

63
00:04:38.360 --> 00:04:47.350
And then I also want to clarify that during the cast I want to say to decimal places so that goes inside

64
00:04:47.350 --> 00:04:51.100
this format numbers format number result cast floats.

65
00:04:51.100 --> 00:04:53.090
That's the first argument instead of format number.

66
00:04:53.170 --> 00:04:55.370
Second argument instead of format numbers too.

67
00:04:55.570 --> 00:05:03.550
And then after all of that I want to make sure I give it an alias so will say alias open that way it

68
00:05:03.550 --> 00:05:06.220
looks nicer instead of saying something like format number.

69
00:05:06.220 --> 00:05:07.750
Open flow etc..

70
00:05:08.080 --> 00:05:10.090
And that's basically how to do for all of them.

71
00:05:10.090 --> 00:05:15.670
So I'm going to copy and paste from the solutions notebook just so we can see this little bit clear.

72
00:05:15.700 --> 00:05:18.760
And I don't have to basically walk through a bunch of typing for you.

73
00:05:18.820 --> 00:05:24.430
Let me zoom out and scroll up a little bit and zoom out one more time so you can see everything in one

74
00:05:24.430 --> 00:05:25.100
cell.

75
00:05:25.330 --> 00:05:26.090
So there it is.

76
00:05:26.110 --> 00:05:31.930
All you have to do is say select and then format number Pessin the column they are trying to format

77
00:05:32.500 --> 00:05:37.530
cast it to either a float or for the case of the volume and integer.

78
00:05:37.630 --> 00:05:43.600
And then you're able to easily call the decimal places inside of format number and then I also use an

79
00:05:43.600 --> 00:05:45.640
alias so it looked a little nicer.

80
00:05:45.640 --> 00:05:51.170
So when I run this show at the end I get the sort of nice formatted data frame.

81
00:05:51.220 --> 00:05:54.080
Now maybe the bonus question was super clear to you.

82
00:05:54.100 --> 00:05:55.380
Again don't worry about it.

83
00:05:55.450 --> 00:05:58.670
If he did it some other way or interpreted it differently.

84
00:05:58.900 --> 00:05:59.720
Don't worry about it.

85
00:05:59.830 --> 00:06:04.090
It wasn't a big deal and we won't have to do anything like this for the machine learning section of

86
00:06:04.090 --> 00:06:04.840
the course.

87
00:06:05.950 --> 00:06:11.500
Up next we wanted to create a new data frame with a column called HIV ratio which is the ratio of high

88
00:06:11.500 --> 00:06:14.270
price versus volume stock traded for our day.

89
00:06:14.320 --> 00:06:18.460
So let's zoom in and kind of handle this a little more then.

90
00:06:18.490 --> 00:06:31.530
OK there we go and we'll say DFI 2 is equal to DMF with column H V ratio and that's going to be equal

91
00:06:31.530 --> 00:06:39.630
to T.F. high divided by IDF volume.

92
00:06:39.680 --> 00:06:44.780
So during the lectures in the previous section of the course we saw how we could kind of divide and

93
00:06:44.780 --> 00:06:50.630
multiply numbers or a scalar against an entire column but you can do definitely the same thing with

94
00:06:50.870 --> 00:06:56.280
column and columns so column divided by column or columns added together etc..

95
00:06:56.390 --> 00:07:04.010
So after that we can just say the left to select and I'm just going to select that HIV ratio column

96
00:07:04.010 --> 00:07:08.610
to show it.

97
00:07:08.790 --> 00:07:12.000
And there it is DHV ratio column.

98
00:07:12.010 --> 00:07:15.720
Now the next question was what they had the peak high in price.

99
00:07:15.940 --> 00:07:20.320
There's different ways you can do this but one easy way to do this is just have order by.

100
00:07:20.320 --> 00:07:29.020
I can say if ordered by an order by the high price but I want to make sure I'm ordering it in the Suddenlink

101
00:07:29.020 --> 00:07:31.540
order which is why I passed it in as a column there.

102
00:07:31.630 --> 00:07:34.940
So if I just run that I can see that I have this data frame.

103
00:07:35.050 --> 00:07:40.570
So then if I show that data frame I can see I have all the information ordered and all I really want

104
00:07:40.660 --> 00:07:41.890
is that very first one.

105
00:07:42.100 --> 00:07:47.170
So what I'm going to do is instead of saying that show I will just grab the head.

106
00:07:47.170 --> 00:07:48.100
So let's do that

107
00:07:51.990 --> 00:07:56.260
and pass and one run that and now I can see the data.

108
00:07:56.290 --> 00:08:03.310
So it was 2015 January 13th if actually just really want this date time I can index it out so I can

109
00:08:03.310 --> 00:08:08.890
say grab that first row objects from that list and then grab the first item there which is date.

110
00:08:08.950 --> 00:08:11.310
And here we have that daytime object.

111
00:08:11.410 --> 00:08:13.090
Just more clearly.

112
00:08:13.690 --> 00:08:15.880
Now what does the mean of the close column.

113
00:08:15.880 --> 00:08:17.710
So that's pretty simple as well.

114
00:08:17.920 --> 00:08:25.570
We'll save from PI spark thought sequel that functions import mean and then all you do is say select

115
00:08:26.200 --> 00:08:30.340
mean from close and then show the results.

116
00:08:31.600 --> 00:08:36.880
And here we have the select and there was the average close value a seventy two point thirty eight and

117
00:08:36.880 --> 00:08:40.890
you can always format a number to format that more.

118
00:08:40.890 --> 00:08:46.450
Now what is the max and min of the volume column Well again that's just an important function.

119
00:08:46.600 --> 00:08:53.350
We'll say from Paice park functions import Maximin you could have also use the Agam method if you wanted

120
00:08:53.350 --> 00:08:54.130
to.

121
00:08:54.350 --> 00:09:03.250
But we're going to say if select the max volume and then we'll also say the volume

122
00:09:07.170 --> 00:09:08.500
and then show the results here.

123
00:09:10.260 --> 00:09:12.910
And then we get the exact same things.

124
00:09:12.940 --> 00:09:16.830
The next question was how many days was the close lower than $60.

125
00:09:16.840 --> 00:09:22.960
Lots of ways you can do this all the two main ways you could do something like filter in which case

126
00:09:22.960 --> 00:09:31.660
you can use either the pure sequence syntax so close less than 60 and then account that or you could

127
00:09:31.660 --> 00:09:41.780
say the filter and then do the python way which is close and then less than 60 count that so that's

128
00:09:41.780 --> 00:09:43.430
using filter.

129
00:09:43.430 --> 00:09:47.390
Now you could also use count as a function which again would use filter.

130
00:09:47.390 --> 00:09:49.500
But let's show you how we can actually do that.

131
00:09:49.500 --> 00:10:01.100
So we'll say from PI sparked sequel the functions import counts and a new cell say the result is equal

132
00:10:01.100 --> 00:10:10.360
to DFT filter the close less than 60.

133
00:10:10.680 --> 00:10:14.940
And then let's say you forgot that you couldn't do a count method directly after that.

134
00:10:15.060 --> 00:10:19.260
Well you could always just imported that count and then say result.

135
00:10:19.580 --> 00:10:26.250
So like the Count of close and then this should show pretty much the same result.

136
00:10:26.270 --> 00:10:30.410
But in the form of a data frame and there it is Count close 81.

137
00:10:30.450 --> 00:10:35.220
So in case you forgot that you could use the count method write off a filtered data frame.

138
00:10:35.250 --> 00:10:37.880
You could always just import count as a function itself.

139
00:10:37.920 --> 00:10:44.580
Kind of up to you however you want to approach that next question was What percentage of the time was

140
00:10:44.580 --> 00:10:46.620
the high greater than $80 dollars.

141
00:10:46.620 --> 00:10:51.170
So in other words the number of days the highest grade and 80 divided by the total days in the data

142
00:10:51.170 --> 00:10:51.480
set.

143
00:10:51.600 --> 00:10:55.760
So lots and lots of ways to do this are kind of go through a simpler one.

144
00:10:56.040 --> 00:11:05.420
So I'll say the filter by when the high is greater than 80 and I'm going to then count the number of

145
00:11:05.420 --> 00:11:11.570
days that occurred and then I'm going to divide this by DFI thought count.

146
00:11:11.570 --> 00:11:18.110
So if I just do this I'll get some errors and I'm going to explain why in a second I get 0.09 since

147
00:11:18.110 --> 00:11:24.700
I want the actual percentage what I need to do is multiply this by 100 Howard.

148
00:11:24.750 --> 00:11:26.640
I don't want to multiply just the denominator.

149
00:11:26.640 --> 00:11:33.960
I want to multiply this entire thing and if I run this now I get the actual percentage 9.1 for in some

150
00:11:34.050 --> 00:11:40.410
earlier versions of SPARC you may get a 0 here and if you do it's because Spark's perform the classic

151
00:11:40.410 --> 00:11:44.760
division instead of true the vision in which case you need to just multiply one of these by a floating

152
00:11:44.760 --> 00:11:45.570
point number.

153
00:11:45.590 --> 00:11:48.750
So one point zero and this will work exactly the same.

154
00:11:48.750 --> 00:11:50.940
So keep that in mind for getting zero here.

155
00:11:50.940 --> 00:11:54.380
Try multiplying by 1.0 on top.

156
00:11:54.390 --> 00:11:54.780
All right.

157
00:11:54.780 --> 00:12:00.040
So 9.4 percent of the time the price was higher than $80.

158
00:12:00.040 --> 00:12:04.060
Now we wanted to say what does the Pearson correlation between high end volume.

159
00:12:04.060 --> 00:12:05.090
There was a hint here.

160
00:12:05.110 --> 00:12:08.530
But all you had to do was say from Paice Park

161
00:12:11.320 --> 00:12:19.630
from Paice sparked that sequel that functions import that correlation function and then just select

162
00:12:19.630 --> 00:12:31.960
that we select core and then passen high as well as volume and then show the results and there we go.

163
00:12:32.140 --> 00:12:37.260
Now the next thing I want to say is what is the wups Max high per year.

164
00:12:37.300 --> 00:12:38.920
So lots of ways we can do this.

165
00:12:38.920 --> 00:12:43.980
We actually walk through a really similar process on this but we'll kind of go through it again.

166
00:12:44.910 --> 00:12:52.550
We'll import year as a function and then say you're DMF is equal to death with column.

167
00:12:52.920 --> 00:13:00.190
And I'm going to say with year pass and the year function along with the actual dates column note in

168
00:13:00.200 --> 00:13:03.590
passing the date column not just the date string.

169
00:13:03.860 --> 00:13:12.100
And then when I'm going to do after that is WIPs looks like we misspelled functions run that perfect.

170
00:13:12.410 --> 00:13:23.080
And let me actually put a new cell below and we'll say Max DMF is equal to your DMF dot group by and

171
00:13:23.080 --> 00:13:26.840
then we'll group by year and then also call Max with it.

172
00:13:28.170 --> 00:13:38.140
And once that's done I'll say Max T.F. selects and we'll say year and I'm going to put in as a string

173
00:13:38.140 --> 00:13:45.310
Max high because that's what it's going to show up after you say that Max on one of these columns and

174
00:13:45.310 --> 00:13:50.730
then we'll say show it once that is then running we can see the max here.

175
00:13:50.750 --> 00:13:56.520
So here's the year and here's the max high value 90 81 88 77 75.

176
00:13:56.590 --> 00:14:01.550
And if you want to you could continue this by ordering by year in order to kind of see the results or

177
00:14:01.550 --> 00:14:02.150
the trends.

178
00:14:02.150 --> 00:14:07.760
If that was something that interested you then we have what does the average close for each calendar

179
00:14:07.760 --> 00:14:08.200
month.

180
00:14:08.210 --> 00:14:09.390
The final question.

181
00:14:09.410 --> 00:14:14.030
In other words across all the years what's the average close price for January February March etc..

182
00:14:14.180 --> 00:14:20.480
So let's kind of break this down into four steps first that is to say from Weissberg sequel that functions

183
00:14:22.040 --> 00:14:28.850
import the month function and then after that we want to say create a month DSF and that's going to

184
00:14:28.850 --> 00:14:36.280
be DSF with column that's going to take in the month column and then we'll pass in the month function

185
00:14:36.340 --> 00:14:37.560
with date.

186
00:14:37.660 --> 00:14:41.290
And in this case we can just put in data as a string.

187
00:14:41.440 --> 00:14:42.940
So we'll run that as well.

188
00:14:43.450 --> 00:14:51.760
And then we'll say month averages is equal to month DMF select and I'm going to select the month column

189
00:14:52.360 --> 00:14:55.020
and then let's actually grab the close column as well.

190
00:14:56.270 --> 00:15:02.510
And you don't have to put them in the list so some versions of Sparke require you to put them in a list

191
00:15:02.510 --> 00:15:03.120
this way.

192
00:15:03.170 --> 00:15:04.780
Other versions don't.

193
00:15:04.790 --> 00:15:09.380
Newer versions are usually more flexible than the older versions but I always like to play it safe and

194
00:15:09.390 --> 00:15:17.590
Passons a list so the total do there and then I'm willing to buy and I'll get by month and then play

195
00:15:17.590 --> 00:15:20.190
call that mean at the end of this group by.

196
00:15:20.310 --> 00:15:21.110
So kind of a lot.

197
00:15:21.130 --> 00:15:23.500
Let me zoom out so we can kind of see the whole thing.

198
00:15:23.530 --> 00:15:31.150
So let's call that month averages is equal to month DFI select Here's month and close group by a month

199
00:15:31.270 --> 00:15:32.900
and then grabbed the mean of that.

200
00:15:32.920 --> 00:15:34.290
So let's run that.

201
00:15:34.420 --> 00:15:44.420
And then finally grab those month averages and I'm going to select the month column and the average

202
00:15:44.420 --> 00:15:50.990
close call so note here that I'm not passing in the list and this should still work and then I'm also

203
00:15:50.990 --> 00:15:56.810
going to order by the month just so it kind of looks in order and then let's show this.

204
00:15:56.810 --> 00:15:59.810
And we should get our final result and there it is.

205
00:15:59.870 --> 00:16:02.250
The average closing price per month.

206
00:16:02.330 --> 00:16:05.880
So it looks like it doesn't matter what month it is.

207
00:16:05.990 --> 00:16:12.470
Maybe there's a little higher average closing price or in certain months like July here July has 74

208
00:16:12.760 --> 00:16:18.350
but it looks like the general trend is around all around the year you get an average closing price that's

209
00:16:18.350 --> 00:16:21.800
roughly the same which hopefully makes sense.

210
00:16:21.800 --> 00:16:22.380
All right.

211
00:16:22.640 --> 00:16:26.470
Hopefully those exercises were OK some of them should have been a little challenging.

212
00:16:26.470 --> 00:16:29.990
There were some hints there through in a couple of things you hadn't seen before but we're hopefully

213
00:16:29.990 --> 00:16:33.530
able to figure out as far as these sort of functionalities.

214
00:16:33.530 --> 00:16:37.660
We won't actually have to deal with them too often throughout the machine learning section of the course.

215
00:16:37.670 --> 00:16:41.960
In fact SPARC is actually going to do a lot of the heavy lifting for us and make our lives a lot easier

216
00:16:41.960 --> 00:16:46.580
by taking care of a lot of these functionalities for us within the actual machine learning algorithm

217
00:16:46.580 --> 00:16:47.310
calls.

218
00:16:47.530 --> 00:16:48.210
OK.

219
00:16:48.380 --> 00:16:52.440
I hope we're excited to start with machine learning and we'll see at the next section in the course.
