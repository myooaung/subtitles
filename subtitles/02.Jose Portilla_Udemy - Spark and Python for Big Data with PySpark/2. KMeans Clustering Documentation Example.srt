1
00:00:05,730 --> 00:00:10,020
Hello everyone and welcome back in this lecture we're going to be working through the documentation

2
00:00:10,020 --> 00:00:17,320
example for k means clustering So pay close attention to how we don't need the actual label column which

3
00:00:17,320 --> 00:00:22,590
makes sense because we're doing clustering which is an unsupervised learning algorithm.

4
00:00:22,610 --> 00:00:26,420
Now the documentation's example is a bit peculiar in its choice of a data set.

5
00:00:26,420 --> 00:00:30,890
They basically have a really small data set and they actually do provide you if the label columns which

6
00:00:30,890 --> 00:00:31,880
are unnecessary.

7
00:00:32,000 --> 00:00:33,800
So we're going to explain that along the way.

8
00:00:33,980 --> 00:00:38,450
And hopefully our own custom code along will clarify things further in the next lecture.

9
00:00:38,460 --> 00:00:40,080
But for now let's get started.

10
00:00:40,100 --> 00:00:45,910
Open up a new Jupiter notebook and walk through the documentation's own example of Kamins clustering.

11
00:00:45,910 --> 00:00:46,180
All right.

12
00:00:46,190 --> 00:00:50,360
So I've started in you know book and as a quick note you can find all the code that I'm going to be

13
00:00:50,360 --> 00:00:55,280
working through at this note book right here clustering code exampled IPY and B.

14
00:00:55,580 --> 00:00:56,980
So let's get started.

15
00:00:56,990 --> 00:01:03,230
First thing you have to do is actually start a Paice perk sessions will say Place perks equal import

16
00:01:03,230 --> 00:01:10,640
Sparke session and then I will create a spark session so will say spark session builder name and I'll

17
00:01:10,640 --> 00:01:11,910
call this cluster.

18
00:01:12,170 --> 00:01:15,330
And then I will say Get or create.

19
00:01:15,560 --> 00:01:19,900
So we will run that and then I'm actually going to import our model.

20
00:01:19,930 --> 00:01:22,040
So we'll save from Paice sparked off.

21
00:01:22,280 --> 00:01:24,800
And we have to provide what a family this belongs to.

22
00:01:24,800 --> 00:01:26,930
And in this case this is a clustering algorithm.

23
00:01:27,170 --> 00:01:34,310
So from here I can import Kamins then we'll upload the data set the data set.

24
00:01:34,320 --> 00:01:39,210
I've already downloaded it for you and it's under the clustering folder under Sparke for machine learning.

25
00:01:39,480 --> 00:01:43,090
But it's this kind of esoteric lib SVM format.

26
00:01:43,170 --> 00:01:49,650
So when you say Sparke read that format lib SVM and then you'll say load and it's called Nico's just

27
00:01:49,650 --> 00:01:53,310
tab autocomplete this sample case means data that text.

28
00:01:53,370 --> 00:01:58,010
So I will run that the next thing I want to do is actually check up data set.

29
00:01:58,080 --> 00:02:02,240
And this is kind of why it's a weird choice given that we're doing Kamins clustering.

30
00:02:02,250 --> 00:02:08,820
But if you show the dataset it's actually just this it's six rows of data and it has both a label and

31
00:02:08,820 --> 00:02:09,680
features.

32
00:02:09,750 --> 00:02:14,610
So I'm not sure if they're trying to imply that there's six different clusters to be expected here.

33
00:02:14,640 --> 00:02:18,220
The documentation example actually sets K equal to 2.

34
00:02:18,240 --> 00:02:20,840
So that's something that's a little bit peculiar.

35
00:02:20,940 --> 00:02:24,600
So just keep that in mind as we continue on throughout this project.

36
00:02:24,600 --> 00:02:29,580
What I really want to do is prove to you that it only needs features because this is an unsupervised

37
00:02:29,580 --> 00:02:30,730
learning algorithm.

38
00:02:30,840 --> 00:02:39,900
So I will say final data is equal to this dataset and I'm going to select only the features column.

39
00:02:40,110 --> 00:02:46,160
So I'm going to run that and then I'm going to show you the final data and the final data only has those

40
00:02:46,160 --> 00:02:46,710
features.

41
00:02:46,730 --> 00:02:51,470
So that's what we're going to be using just to basically prove to you what a more realistic scenario

42
00:02:51,470 --> 00:02:52,260
would look like.

43
00:02:52,580 --> 00:03:00,650
And then I'm going to create my model will say k means is equal to and it's k means.

44
00:03:00,650 --> 00:03:05,430
And then you can call off this set K and this is where you can set your K value.

45
00:03:05,450 --> 00:03:07,940
In this case a follow along to the documentation.

46
00:03:08,000 --> 00:03:12,910
They set their K equal to 2 and then something else you can do is set a seed of value.

47
00:03:12,920 --> 00:03:17,010
Now a seed value is just a seed value for a random number generator.

48
00:03:17,030 --> 00:03:23,540
So remember back in the theory lecture where you actually start the centroid is randomly chosen.

49
00:03:23,600 --> 00:03:26,660
Well you can do is a seed number here that way.

50
00:03:26,660 --> 00:03:30,830
In case you want to come back to this experiment and make sure you have the same quote unquote random

51
00:03:30,830 --> 00:03:35,890
setting that C number is going to provide the same set of random numbers every time.

52
00:03:35,900 --> 00:03:38,760
As long as you have the same number here.

53
00:03:38,810 --> 00:03:42,890
So if you want to follow along with me and make sure you get the exact same results set your speed equal

54
00:03:42,890 --> 00:03:44,270
to 1.

55
00:03:44,270 --> 00:03:46,000
So that's our Kamins model.

56
00:03:46,280 --> 00:03:52,310
And now let's fit the model so we'll say model is equal to key means and I'm going to fit it to that

57
00:03:52,340 --> 00:03:56,290
final data dataset so now we run that.

58
00:03:56,410 --> 00:04:01,900
And while it's fitting notice that it had no problems so final data only contains features and has no

59
00:04:01,900 --> 00:04:04,990
problem fitting that because it's an unsupervised learning algorithm.

60
00:04:05,050 --> 00:04:09,550
If you try to do this on another dataset it would give you back an error saying it's looking for that

61
00:04:09,550 --> 00:04:10,420
label.

62
00:04:10,420 --> 00:04:15,400
So again means only needs features and you can even do shift tab here on this means and notice that

63
00:04:15,400 --> 00:04:19,070
it only expects features call it has no label column input.

64
00:04:19,390 --> 00:04:25,930
OK so let's continue on and actually try to evaluate our clustering algorithm.

65
00:04:25,930 --> 00:04:31,630
So something we can do is check out the within set sum of squared errors.

66
00:04:31,690 --> 00:04:36,390
So within set sum of squared errors or W S S S E.

67
00:04:36,730 --> 00:04:38,970
And then I will say that's equal to.

68
00:04:39,310 --> 00:04:44,770
And the way you do this is a little different than our evaluation metrics earlier but we do as you say

69
00:04:45,160 --> 00:04:47,530
model dot compute cost.

70
00:04:47,530 --> 00:04:50,620
And then you passen your data.

71
00:04:50,620 --> 00:04:54,880
So we compute the cost of our actual dataset and you're passing in the whole thing you're not doing

72
00:04:54,880 --> 00:04:56,350
any test train splits.

73
00:04:56,350 --> 00:04:58,510
That doesn't make sense because we don't have labels.

74
00:04:58,510 --> 00:05:01,330
We're just kind of dealing with the entire data set as is.

75
00:05:01,420 --> 00:05:08,740
And then you can just prints the within some square errors or thinset some squirters.

76
00:05:08,740 --> 00:05:13,500
So for us it happens to be 0.1 1:9 repeating.

77
00:05:13,630 --> 00:05:16,700
And what we can actually do is check out the cluster centers.

78
00:05:16,720 --> 00:05:24,320
So if we look at our final data one more time and the show this notice that we essentially had three

79
00:05:24,320 --> 00:05:25,590
dimensions of features.

80
00:05:25,730 --> 00:05:30,890
So we had this item and to pull this vector of items in the tuple and then another one.

81
00:05:31,070 --> 00:05:36,650
And if we see here the first one is actually empty or nothing on the first or the second two of the

82
00:05:36,650 --> 00:05:37,700
first row.

83
00:05:37,790 --> 00:05:42,950
So we can end up doing is checking out the cluster centers and because we know we have essentially three

84
00:05:42,950 --> 00:05:49,460
values in this tuple for the features we should expect the centers to be in three dimensions.

85
00:05:49,910 --> 00:05:56,480
So in order to actually get the centers we can say create a new object called centers and model and

86
00:05:56,480 --> 00:06:01,150
it's going to be cluster centers and this is a method so you have close princes there.

87
00:06:01,250 --> 00:06:05,920
So if you end up typing out centers it will return back the actual centers.

88
00:06:05,960 --> 00:06:12,880
In this case it returns back to centers at 9.1 9.1 and put it on is like x y z and then another one

89
00:06:12,880 --> 00:06:14,920
is 0.1 0.1 0.1.

90
00:06:15,110 --> 00:06:16,150
So what does that actually mean.

91
00:06:16,160 --> 00:06:24,680
Well it means since we set K equal to 2 it computed that the actual centers are at these locations.

92
00:06:24,680 --> 00:06:29,870
Now generally speaking especially if you're unfamiliar if the data set it's a little hard to actually

93
00:06:29,870 --> 00:06:32,900
interpret the within set some of squared errors.

94
00:06:32,930 --> 00:06:35,780
This number right here or even the Centers.

95
00:06:35,780 --> 00:06:39,780
I mean we understand that this is some Center and some three of them mention all space.

96
00:06:39,920 --> 00:06:49,270
But truly what we want to know is if I look at my final data and say that show I have essentially five

97
00:06:49,270 --> 00:06:54,560
rows and what I really want to know is which group does each row belong to.

98
00:06:54,580 --> 00:06:58,990
Essentially I want to know the label that I created for it.

99
00:06:58,990 --> 00:07:06,310
And the way you do that is you call your model then say transform and you actually then pass in the

100
00:07:06,310 --> 00:07:07,330
data.

101
00:07:07,450 --> 00:07:12,400
And again we're not doing a test train split which makes sense because this is unsupervised learning.

102
00:07:12,400 --> 00:07:15,880
We don't have any real labels to do the testing on.

103
00:07:16,000 --> 00:07:20,260
So we just have to pass in all the data during the fitting process and we're going to pass and all the

104
00:07:20,260 --> 00:07:24,170
data during the transformation process because this is unsupervised learning.

105
00:07:24,280 --> 00:07:30,910
That's kind of part of the issues with it that it's difficult to know for sure how well your model actually

106
00:07:30,910 --> 00:07:36,740
performed because since the very beginning we never knew the true label to compare it to.

107
00:07:36,760 --> 00:07:40,930
Now there's definitely method such as I mentioned though within set some A-squared errors that try to

108
00:07:40,930 --> 00:07:46,070
evaluate that especially try to evaluate if you chose the right k but let's run this now.

109
00:07:46,090 --> 00:07:52,730
So actually let's set this as results and then if I check out results.

110
00:07:52,840 --> 00:07:58,990
So notice now I can say show and now I have the actual prediction.

111
00:07:59,050 --> 00:08:02,400
So this predicts that this is the first group.

112
00:08:02,470 --> 00:08:08,980
So these first three rows belong to the first group and then these last three rows belong to the second

113
00:08:08,980 --> 00:08:09,670
group.

114
00:08:09,670 --> 00:08:13,110
Now let's run this again with a higher value.

115
00:08:13,180 --> 00:08:19,690
So I'll set this K equal to 3 and let's run this again so we'll say Kamins is this Kamins fit to the

116
00:08:19,690 --> 00:08:22,210
model or the final data I should say.

117
00:08:22,210 --> 00:08:27,220
And if we compute the COS now we get a different within set some A-squared errors and it makes sense

118
00:08:27,220 --> 00:08:33,280
that it goes down as you increase k then we're going to check out the cluster centers and note that

119
00:08:33,280 --> 00:08:38,290
we now have three cluster centers and one of the clusters is slightly off.

120
00:08:38,290 --> 00:08:45,100
The other one looks like it took the 9.1 group from earlier and split it into two and it kept the other

121
00:08:45,100 --> 00:08:46,420
group the same.

122
00:08:46,420 --> 00:08:48,060
So now pay attention here.

123
00:08:48,070 --> 00:08:50,380
One of these groups should probably be remaining the same.

124
00:08:50,380 --> 00:08:55,830
And another group will be split so we'll say results is modeled transform final data.

125
00:08:55,870 --> 00:08:57,010
Run that again.

126
00:08:57,010 --> 00:08:59,950
And what I'm going to do is check out the results.

127
00:08:59,950 --> 00:09:04,510
So remember this first three were ones last three were zeros for one that again.

128
00:09:04,660 --> 00:09:05,910
Well look what happened.

129
00:09:05,920 --> 00:09:09,490
It looks like one of these groups stayed intact and it split off.

130
00:09:09,490 --> 00:09:15,610
The other group now has two zeros and ones two which makes sense given that we saw one of the centers

131
00:09:15,940 --> 00:09:17,910
is essentially split off from each other.

132
00:09:18,150 --> 00:09:18,920
OK.

133
00:09:19,120 --> 00:09:22,260
It's not always going to be that obvious especially if you have higher dimensions.

134
00:09:22,270 --> 00:09:25,810
But in this case the data is pretty simple if just five rows.

135
00:09:25,810 --> 00:09:29,980
Hopefully that makes sense if you have any questions on that explanation please feel free to post the

136
00:09:29,980 --> 00:09:31,310
Kewaunee forms.

137
00:09:31,330 --> 00:09:31,870
OK.

138
00:09:32,080 --> 00:09:33,790
So this was a really small data set.

139
00:09:33,820 --> 00:09:38,380
It was kind of strange in the fact that it provided labels for us already but we really didn't need

140
00:09:38,380 --> 00:09:38,930
them.

141
00:09:39,160 --> 00:09:43,570
And what we're going to do is in the next lecture look at a more realistic dataset.

142
00:09:43,570 --> 00:09:47,890
In fact that's a real data set from UC Irvine machine learning repository.

143
00:09:47,890 --> 00:09:48,870
So let's get started.

144
00:09:48,910 --> 00:09:50,700
I'll see you at the next lecture.
