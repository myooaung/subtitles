WEBVTT
1
00:00:05.310 --> 00:00:10.100
Hello everyone and welcome to an example solution for logistic regression consulting project.

2
00:00:10.110 --> 00:00:12.260
Let's open up the notebook and get started.

3
00:00:12.660 --> 00:00:14.220
OK here I am at the notebook.

4
00:00:14.220 --> 00:00:19.000
So we have this customer churn see as we filed we want to build a model off of.

5
00:00:19.170 --> 00:00:23.610
And then later on we also want to compare it to this new customers see Espey file.

6
00:00:23.610 --> 00:00:26.320
So we're going to run this.

7
00:00:26.490 --> 00:00:28.340
And let's start a spark session.

8
00:00:28.340 --> 00:00:35.820
So from Paice sparked that sequel we'll import a spark session and then let's create one spark spark

9
00:00:35.820 --> 00:00:43.800
session builder app name and I'm going to call this law Greg consult and then get or create that spark

10
00:00:43.800 --> 00:00:44.920
session.

11
00:00:45.030 --> 00:00:46.160
So we run that cell.

12
00:00:46.170 --> 00:00:52.160
And now it's load in our data our data is just say see every file so we can say SPARC that read cxxviii

13
00:00:52.720 --> 00:00:58.320
and I'm going to grab the customer churn data so it's customer turn that C S V.

14
00:00:58.370 --> 00:01:06.760
And I'm also going to infer schema and I will say that header is equal to true and shook up the schema

15
00:01:06.760 --> 00:01:09.380
of actual data by saying print schema.

16
00:01:10.700 --> 00:01:12.680
And running this.

17
00:01:12.680 --> 00:01:17.390
So here we have the schema we already went over in the previous lecture explaining it so we can also

18
00:01:17.390 --> 00:01:20.890
do say the scribe if you want to see some more information about it.

19
00:01:21.140 --> 00:01:27.200
We're pretty zoomed in right now so we probably get some weird behavior but let's see it out.

20
00:01:27.240 --> 00:01:32.310
So they describe that show because we have close princes here and we can zoom out just a little bit.

21
00:01:32.310 --> 00:01:38.490
See this and we can see here we have 900 counts and it looks like we have 900 counts on all the columns

22
00:01:38.490 --> 00:01:43.680
so we shouldn't be missing any data as long as everything is there we can continue along.

23
00:01:43.950 --> 00:01:49.690
So we're going to do is check out the columns we're going to be working with data that columns we can

24
00:01:49.810 --> 00:01:57.210
zoom in one more wups data that columns and then we want to actually select stuff for the vector assembler.

25
00:01:57.280 --> 00:02:05.700
So I will say from Paice park M-L that feature import the vector assembler and then I'll create my assembler

26
00:02:05.700 --> 00:02:15.990
object and L said equal to vector assembler loops and I want my input columns to be the following.

27
00:02:15.990 --> 00:02:18.700
I'm not going to use names because it's pretty arbitrary.

28
00:02:18.720 --> 00:02:23.360
The age of the customer it's a numeric so might be pretty useful as well as the total purchase.

29
00:02:23.400 --> 00:02:25.140
Maybe whether or not they have an account manager.

30
00:02:25.170 --> 00:02:30.540
I don't expect that to actually help because remember in the data that's randomly assigned but it's

31
00:02:30.540 --> 00:02:32.850
easy enough to include numerically so we can include it.

32
00:02:33.090 --> 00:02:34.950
And then years and number of sites.

33
00:02:34.980 --> 00:02:36.810
So let's select those.

34
00:02:36.960 --> 00:02:42.000
Copy them and then paste them in here and then we make sure that's a close string.

35
00:02:42.000 --> 00:02:48.360
So there are my input columns and the next thing I want to do is select this all to be the output column

36
00:02:48.530 --> 00:02:50.120
of features.

37
00:02:50.360 --> 00:02:56.730
So we'll run that and now that we have our assembler object let's create the output to be equal to assembler.

38
00:02:57.180 --> 00:03:01.020
And then we're going to transform our data.

39
00:03:01.360 --> 00:03:08.110
So we have this output and let's create our final dataset which is just going to be that output.

40
00:03:08.110 --> 00:03:11.220
And all I want to do is select the features and the turn column

41
00:03:14.280 --> 00:03:19.260
so we run that in our ready to actually split this into our training data and our test data.

42
00:03:19.380 --> 00:03:29.650
So we'll call this train churn and then we'll say test chern and set that equal to this final data and

43
00:03:29.890 --> 00:03:33.920
let's do a random split and we'll do a 17:30 split

44
00:03:36.660 --> 00:03:43.410
run that and hour ready to actually perform or logistic regression we'll see say from Paice spark M-L

45
00:03:43.630 --> 00:03:51.580
classification import a logistic regression and then what I'm going to do is create an model for religious

46
00:03:51.580 --> 00:03:52.320
expression.

47
00:03:52.570 --> 00:03:57.580
We'll say L.R. churn is equal to an instance of logistic regression.

48
00:03:57.580 --> 00:04:03.700
And the thing I have to clarify is that my label column will be equal to the chern column.

49
00:04:03.700 --> 00:04:04.760
So we have that going.

50
00:04:04.780 --> 00:04:08.230
The next thing I want to do is actually fit this model to the data.

51
00:04:08.230 --> 00:04:19.260
So we'll call this fitted chern model and I will grab L.R. chern and fit this to my train data which

52
00:04:19.260 --> 00:04:20.840
is trained in.

53
00:04:21.270 --> 00:04:22.880
So that's fitting right now.

54
00:04:22.920 --> 00:04:25.570
And what I can do after this is check out a summary.

55
00:04:25.950 --> 00:04:32.100
So we'll create some variable called training some for training summary and this will be equal to this

56
00:04:32.100 --> 00:04:35.610
fit a term model summary object off of it.

57
00:04:35.670 --> 00:04:37.210
And let's check it out.

58
00:04:37.380 --> 00:04:44.250
I'll call my training some and then call the predictions off of this and then call describe a show and

59
00:04:44.250 --> 00:04:46.470
this is just to explore a little bit you don't have to do this.

60
00:04:46.470 --> 00:04:52.260
You don't want to but I can see here that my prediction mean prediction standard deviation versus my

61
00:04:52.260 --> 00:04:54.430
turn mean insurance standard deviation.

62
00:04:54.660 --> 00:04:55.740
So it would be nice to do.

63
00:04:55.740 --> 00:04:59.140
Here is actually to some evaluation metrics.

64
00:04:59.190 --> 00:05:05.260
So we'll do these evaluation metrics against the M-L live evaluation and our test data.

65
00:05:05.430 --> 00:05:11.820
So we actually do this and is safe from Paice park the M-L evaluation and I'm going to import since

66
00:05:11.820 --> 00:05:13.850
as the binary classification evaluation.

67
00:05:13.860 --> 00:05:21.720
I'll import that and we'll call this new object pred and labels for a prediction labels and that's going

68
00:05:21.720 --> 00:05:30.840
to be equal to our fitted churn model evaluating the test churn data.

69
00:05:30.950 --> 00:05:37.360
Now that I have this press and labels object I can actually grab the predictions off of it so we can

70
00:05:37.360 --> 00:05:39.470
say Pridden label's predictions.

71
00:05:39.640 --> 00:05:41.000
And then I can show the results here.

72
00:05:41.020 --> 00:05:46.060
Let me zoom out so you can see that entire data frame when I run this and I can zoom out one more time

73
00:05:46.600 --> 00:05:48.260
so we can see the whole features.

74
00:05:48.280 --> 00:05:53.620
So have my features call them the chern value that's the true value the actual label the raw prediction

75
00:05:53.680 --> 00:05:56.850
the probability and then the prediction itself.

76
00:05:56.890 --> 00:06:01.260
So we can see here a lot of these predictions are matching up to be zeros which is good.

77
00:06:01.270 --> 00:06:04.690
All of these turn values zero but it looks like we're already missing one here.

78
00:06:04.690 --> 00:06:09.100
So what I want to do is actually use this binary classification evaluator to check the area under the

79
00:06:09.100 --> 00:06:09.780
curve.

80
00:06:09.910 --> 00:06:10.930
So let's do that.

81
00:06:11.170 --> 00:06:12.950
I'm going to continue on here.

82
00:06:13.090 --> 00:06:15.960
Scroll down a little bit and let's zoom back in.

83
00:06:16.670 --> 00:06:21.650
Scroll down and then let's call that binary classification evaluator.

84
00:06:21.650 --> 00:06:31.570
So let's create an object call turn evil and set that equal to binary classification evaluator where

85
00:06:31.580 --> 00:06:38.750
my label column is equal to churn and I can set my prediction column.

86
00:06:38.880 --> 00:06:46.000
So a prediction column equal to prediction so have these two columns here.

87
00:06:46.110 --> 00:06:50.730
Once I have this evaluator object created all I have to do is passen that prediction is data frame we

88
00:06:50.730 --> 00:06:52.010
just worked with.

89
00:06:52.020 --> 00:06:54.550
So the default is area under curve.

90
00:06:54.570 --> 00:07:00.630
So that's going to be my variable name and I will grab this Cherney Valuator object and I will evaluates

91
00:07:01.320 --> 00:07:09.230
the predictions are present and labels and then call predictions off of this run that and then let's

92
00:07:09.230 --> 00:07:13.000
check our Acey to see how we did or a U.S. excuse me.

93
00:07:13.010 --> 00:07:18.050
So looks like we have a point 6:08 value for a you see that's not so bad it's not super great either.

94
00:07:18.050 --> 00:07:22.200
Remember that a value point five will just be as good as randomly guessing.

95
00:07:22.220 --> 00:07:26.660
So now that we see the area under the curve the next step is to try to predict on new data.

96
00:07:26.720 --> 00:07:32.350
So if we scroll all the way back up remember that we have this new customers cxxviii file.

97
00:07:32.540 --> 00:07:36.650
So the area under curve is performing well enough that we can evaluate on this sort of like hold out

98
00:07:36.650 --> 00:07:40.400
data set which essentially doesn't really have label so it's not truly a hold out data.

99
00:07:40.430 --> 00:07:42.470
It's more like a model deployment.

100
00:07:42.470 --> 00:07:44.170
So we'll scroll all the way down here.

101
00:07:44.540 --> 00:07:50.660
And what we're going to do is let's just create this and say predict on new data.

102
00:07:51.860 --> 00:07:54.920
Run that so it will create some sort of final model.

103
00:07:54.920 --> 00:08:03.350
So Final our model and set that equal to our L.R. churn model and I'm going to fit this on all the data

104
00:08:03.440 --> 00:08:08.610
not just my training data or my test data but what I'm going to do is to scroll back up here.

105
00:08:08.840 --> 00:08:15.690
I'm going to fit this on all my final data so I will copy that and then paste it in here.

106
00:08:16.500 --> 00:08:21.960
So are all our final data and then we're going to end up doing is read in that new customer data.

107
00:08:22.410 --> 00:08:31.110
So we'll say new customers is equal to spark read see a city and that's new customers and I'm going

108
00:08:31.110 --> 00:08:35.950
to infer schema to be true as well as headers to be equal to true.

109
00:08:36.350 --> 00:08:41.480
So we're run that and the next thing one do is make sure that new customers is in the same format.

110
00:08:41.850 --> 00:08:48.410
So I can say in your customers the print schema check that out and it looks like everything is exactly

111
00:08:48.410 --> 00:08:50.160
the same format as before.

112
00:08:50.180 --> 00:08:51.700
So that means we're ready to go.

113
00:08:51.710 --> 00:09:00.860
So what I can do is say this test on new customers is going to be equal to my old similar object and

114
00:09:00.860 --> 00:09:04.940
I'm going to transform this new customers object.

115
00:09:04.940 --> 00:09:06.610
So here's my new customers test.

116
00:09:06.740 --> 00:09:12.080
And then the other thing I have to do now is confirm that it has that feature column so I can say test

117
00:09:12.080 --> 00:09:13.440
new customers.

118
00:09:13.730 --> 00:09:17.870
And then we can print schema and they should see a feature column all the way at the end.

119
00:09:17.870 --> 00:09:22.910
And there it is features it's a vector of those features from my assembler object in the beginning and

120
00:09:23.040 --> 00:09:25.620
and next thing I want to do is transform this.

121
00:09:25.880 --> 00:09:39.490
So I'll say final results is equal to that final LR model and I will transform that final data.

122
00:09:39.710 --> 00:09:44.160
Well actually our new data so this new customers CXVII.

123
00:09:44.210 --> 00:09:46.190
So let's make sure that we have that.

124
00:09:46.400 --> 00:09:53.320
It was tested to customers excuse me so test new customers were going to transform that dataset.

125
00:09:53.710 --> 00:09:56.340
Run that and then let's check out the final results.

126
00:09:57.890 --> 00:10:03.840
So if we show these and we can zoom out a little bit you can see here we have all the information.

127
00:10:03.860 --> 00:10:12.420
But what I really care about is the churn prediction which in this case is just called prediction.

128
00:10:12.580 --> 00:10:18.940
And if I wanted to format this nicely I can also grab the names so maybe we can grab the company name

129
00:10:19.150 --> 00:10:19.800
as well.

130
00:10:20.950 --> 00:10:25.120
So showing that here are basically my predictions for the company.

131
00:10:25.210 --> 00:10:27.040
And then these guys.

132
00:10:27.040 --> 00:10:30.700
So my actual data set for this new customers wasn't so big.

133
00:10:30.700 --> 00:10:37.710
In fact we can see from test new customers if we ask for a described method on it and then show the

134
00:10:37.710 --> 00:10:40.580
results there there's actually only six customers here.

135
00:10:40.590 --> 00:10:44.820
So these are our predictions for these customers and whether or not they will churn.

136
00:10:44.820 --> 00:10:47.440
So looks like I predict for these customers are turned.

137
00:10:47.610 --> 00:10:51.920
Meaning these are the customers that I should assign account managers to.

138
00:10:51.930 --> 00:10:54.840
Now if you didn't get the exact same results that's totally okay.

139
00:10:54.840 --> 00:10:57.310
It really depends on the random split.

140
00:10:57.360 --> 00:11:02.720
Hopefully you've got similar results at least half of these should match up to what I have here.

141
00:11:02.760 --> 00:11:06.960
Do you have any questions feel free to post the Q&amp;A forums but also definitely check out the example

142
00:11:06.960 --> 00:11:10.920
solutions notebook which goes into a lot more detail than what we've shown here.

143
00:11:10.920 --> 00:11:11.740
Thanks everyone.

144
00:11:11.760 --> 00:11:12.700
I'll see you at the next section.

145
00:11:12.700 --> 00:11:13.170
Of course.
