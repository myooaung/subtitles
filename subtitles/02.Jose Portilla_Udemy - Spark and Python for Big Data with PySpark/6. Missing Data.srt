1
00:00:05,310 --> 00:00:11,480
Welcome back often data sources are incomplete which means you will have missing data.

2
00:00:11,520 --> 00:00:14,730
You have three basic options for filling in missing data.

3
00:00:14,760 --> 00:00:20,190
Either keep the missing data points as just Noll's drop the missing data points including the entire

4
00:00:20,190 --> 00:00:23,120
row or fill them in with some other value.

5
00:00:23,160 --> 00:00:28,050
We're grooming to cover how to actually execute these various methods using Python and Sparke.

6
00:00:28,050 --> 00:00:30,490
Let's hop over to a Jupiter notebook and get started.

7
00:00:31,780 --> 00:00:35,890
In this Jupiter note book I've already started a spark session and I've called that just miss you can

8
00:00:35,890 --> 00:00:41,740
feel free to call it whatever you want and we're going to create a data frame by saying Sparke read

9
00:00:41,970 --> 00:00:49,810
C S V and we'll use the contains no thought see as we file include two of the notes and I'll say the

10
00:00:49,810 --> 00:00:54,630
header is true as well as in for schema is equal to true.

11
00:00:54,760 --> 00:00:56,320
But it's one equal sign.

12
00:00:56,320 --> 00:00:59,060
Let's put that on another line that way it's a little more readable.

13
00:01:00,280 --> 00:01:06,940
So there's our data frame let's showed it looks like and we can see here we have three columns in employee

14
00:01:06,940 --> 00:01:13,030
ID column where there are no missing values and name column we have two missing values null and NULL

15
00:01:13,030 --> 00:01:13,730
in the middle.

16
00:01:13,870 --> 00:01:18,000
And then we have a sales column at the end with the first two being nulls.

17
00:01:18,010 --> 00:01:24,760
So if we actually look at it at a row basis we have one row where we're missing both a name and a sales

18
00:01:25,180 --> 00:01:28,920
and then two rows were missing either a name or a sales value.

19
00:01:29,050 --> 00:01:31,830
And the very last row has all the information we need.

20
00:01:32,110 --> 00:01:34,800
Let's talk about just dropping the missing data.

21
00:01:34,840 --> 00:01:40,810
So for any data frame you can use the dot and Ã¡n methods either to drop fill or do something with that

22
00:01:40,810 --> 00:01:41,730
missing data.

23
00:01:42,620 --> 00:01:44,700
We're going to show you what that looks like.

24
00:01:44,700 --> 00:01:50,490
I can say DFA N.A. and if I hit tab here I can see the various options I have and I'll start with just

25
00:01:50,490 --> 00:01:51,180
dropping.

26
00:01:51,330 --> 00:01:56,730
So you can use the any functions to drop missing data in if I just call that drop and then show the

27
00:01:56,730 --> 00:01:57,930
results here.

28
00:01:57,960 --> 00:02:01,850
This is going to drop any row that contains any amount of missing data.

29
00:02:02,100 --> 00:02:06,960
However you can also specify a threshold and if you specify threshold argument.

30
00:02:06,960 --> 00:02:11,480
So if I come back here to drop and specify what's known as thresh.

31
00:02:11,940 --> 00:02:12,870
So it's not the whole world.

32
00:02:12,930 --> 00:02:14,320
Word is just thresh.

33
00:02:14,500 --> 00:02:20,720
And if you specify this it drops rows that have less than this certain number of non-null values.

34
00:02:20,820 --> 00:02:27,380
So for example I can type in for two as my threshold and then show the result.

35
00:02:27,390 --> 00:02:28,910
So what does sexy mean.

36
00:02:28,920 --> 00:02:34,680
This means that the road needs to have at least two non-nil values in order to pass the threshold of

37
00:02:34,680 --> 00:02:35,940
being dropped.

38
00:02:36,000 --> 00:02:44,390
So I can see here all these rows have at least two non-null values and 1 John employee 3 3 4 5.

39
00:02:44,550 --> 00:02:45,960
And then this one has all of it.

40
00:02:46,140 --> 00:02:51,870
So if we come back here to the original dataset we can see that this row only had one non-null value

41
00:02:51,990 --> 00:02:54,270
and didn't meet the threshold so it got dropped.

42
00:02:55,330 --> 00:02:55,890
OK.

43
00:02:56,020 --> 00:03:01,070
There is another argument you put in here and set a threshold and that is the parameter how.

44
00:03:01,330 --> 00:03:05,060
So by default the parameter How is set on any.

45
00:03:05,260 --> 00:03:09,580
And that just means if you have any null values we'll just drop that row.

46
00:03:09,580 --> 00:03:16,210
However you can also specify all which means only dropped a row if all the values are no values.

47
00:03:16,270 --> 00:03:21,100
And if you do that in our case not there's not a single row that has everything all all of them have

48
00:03:21,100 --> 00:03:22,380
at least the ID column.

49
00:03:22,510 --> 00:03:24,250
So nothing actually gets dropped there.

50
00:03:24,460 --> 00:03:28,320
So you have the threshold and the how parameters that you can play around with.

51
00:03:28,540 --> 00:03:33,520
You can also play around with the subset parameter and the subset parameter just gives you an optional

52
00:03:33,520 --> 00:03:35,300
list of column names to consider.

53
00:03:35,440 --> 00:03:40,210
So if you only want to consider a certain column as far as missing data you can clarify that with the

54
00:03:40,210 --> 00:03:40,690
subset.

55
00:03:40,690 --> 00:03:46,740
So I can say something like subset is equal to and you pass it in as a list and we'll say sales.

56
00:03:46,870 --> 00:03:52,000
So now I'm going to run that and now it considered any on subset sales.

57
00:03:52,000 --> 00:03:54,380
So I can see here that it doesn't matter if your name is missing.

58
00:03:54,490 --> 00:03:56,520
All that matters is if your sales are missing.

59
00:03:56,530 --> 00:03:59,080
So we see three four five four five six here.

60
00:03:59,080 --> 00:04:03,670
Looking back at the original data it dropped anything that had no value on the sales call and that's

61
00:04:03,670 --> 00:04:06,750
how you can use the subset argument.

62
00:04:06,760 --> 00:04:09,250
Now let's say we don't want to actually drop the missing values.

63
00:04:09,250 --> 00:04:11,230
You want to fill in the missing values.

64
00:04:11,230 --> 00:04:15,020
Well luckily DFA N.A. also has a Phil method we can call off of it.

65
00:04:15,790 --> 00:04:22,600
Coming back here also at the FBI N.A. and I will fill and I'll just show you what happens because it's

66
00:04:22,600 --> 00:04:23,940
actually really interesting.

67
00:04:23,980 --> 00:04:27,400
SPARC is actually smart enough to fill in and match up data types.

68
00:04:27,400 --> 00:04:29,160
So let's imagine that I put in.

69
00:04:29,350 --> 00:04:33,820
Well before I actually do this I want to print out the schema just so we can see what's going on here.

70
00:04:33,820 --> 00:04:37,920
So if we look at the schema I have a string a string and a double.

71
00:04:38,290 --> 00:04:44,470
So then if I say DFT and a thought fill and let's pasan a string so we'll say fill value.

72
00:04:44,470 --> 00:04:47,090
Something really obvious and then show the results here.

73
00:04:48,470 --> 00:04:53,430
Know what happens it fills in this value to any column that's a string column.

74
00:04:53,450 --> 00:04:58,930
In this case name had two missing values so said Phil value fill value.

75
00:04:59,000 --> 00:04:59,590
Great.

76
00:04:59,600 --> 00:05:02,060
Now let's imagine that I put in a number.

77
00:05:02,240 --> 00:05:06,030
So for example I just put in zero and then I say show.

78
00:05:06,200 --> 00:05:08,660
Now it knows that this is a numeric type.

79
00:05:08,780 --> 00:05:13,280
So it only fills in numeric values so sales since it was double it was numeric.

80
00:05:13,280 --> 00:05:19,370
So if filled in zeros they're actually smart enough to not fill in the zero for the string or fill in

81
00:05:19,370 --> 00:05:22,360
the string for the sales which is kind of interesting.

82
00:05:22,370 --> 00:05:25,780
Usually however you should probably specify what columns you want to fill.

83
00:05:25,790 --> 00:05:29,620
You wouldn't just depend on Spark to be smart enough to get it for you.

84
00:05:29,620 --> 00:05:31,300
So we end up doing something like this.

85
00:05:31,340 --> 00:05:33,560
You would pass in the value you want to fill.

86
00:05:33,890 --> 00:05:38,710
So we'll say no name and then using a subset parameter.

87
00:05:38,750 --> 00:05:43,130
Again we'll just pass in a list of the actual column we want to target in this case will say name.

88
00:05:43,400 --> 00:05:45,540
So now let's run this.

89
00:05:45,750 --> 00:05:48,120
And here I can see No-Name has been filled.

90
00:05:48,300 --> 00:05:53,850
If I didn't clarify this subset it would have also worked as we saw earlier but you should actually

91
00:05:54,060 --> 00:05:58,680
declare the subset always because it's just it's nicer that way and you make sure that you know what

92
00:05:58,680 --> 00:05:59,100
you're doing.

93
00:05:59,100 --> 00:06:04,020
So if someone comes back and reviews your work it's really obvious what you are trying to do.

94
00:06:04,020 --> 00:06:09,150
So let's finally go over a really common practice and that is to fill valleys with the mean value for

95
00:06:09,150 --> 00:06:10,140
the column.

96
00:06:10,140 --> 00:06:17,100
So for example I want to fill in any missing values for sales with the average sale value to do that.

97
00:06:17,100 --> 00:06:19,550
I need to import some functions from Paice park.

98
00:06:19,920 --> 00:06:27,150
So we'll say from PI spark thought sequel that functions import mean.

99
00:06:27,390 --> 00:06:39,360
And then I'll say my mean value is equal to DNF select the mean of of sales and I'm going to collect

100
00:06:39,360 --> 00:06:43,310
this to actually get the object back instead of just showing it.

101
00:06:43,320 --> 00:06:45,750
Now if you actually look at what mean value is.

102
00:06:46,200 --> 00:06:49,180
It's this list real object which is a little weird.

103
00:06:49,210 --> 00:06:55,310
We actually just want to grab the single number which means what I'm going to do is say zero here and

104
00:06:55,310 --> 00:06:56,270
then I have row.

105
00:06:56,480 --> 00:06:59,530
And I want to grab the first item in that row object.

106
00:06:59,630 --> 00:07:01,600
And luckily I can actually index this.

107
00:07:01,610 --> 00:07:07,040
We also saw earlier how we could translate this into a dictionary but I can use indexing to grab that

108
00:07:07,040 --> 00:07:07,970
as well.

109
00:07:08,000 --> 00:07:13,300
So let's say means sales is equal to mean value 0 0.

110
00:07:13,760 --> 00:07:21,410
So that actual number there then what I can do is say if any fill and then I'll say fill in that means

111
00:07:21,410 --> 00:07:25,450
sales value and I want to subset sales.

112
00:07:25,640 --> 00:07:29,990
Note here that I'm not actually passing in the parameter names but I'm passing them in the right order

113
00:07:29,990 --> 00:07:31,250
so it actually works.

114
00:07:31,250 --> 00:07:37,430
And I when I run this I can see the FDA that fill me in sales and I have that mean value there.

115
00:07:37,430 --> 00:07:41,540
Now let me show you quickly how you could do this all with one line that's going to be a little ugly

116
00:07:41,570 --> 00:07:46,070
but you may see this in the real world the finding on who your coworkers are.

117
00:07:46,070 --> 00:07:46,990
But we'll see.

118
00:07:47,030 --> 00:07:59,200
The FDA does fill all select that mean from DFA sales and then make up this to collect that value zero.

119
00:07:59,230 --> 00:08:05,690
Those index calls and then at the end of this I want to say the subset was on sales.

120
00:08:05,700 --> 00:08:06,210
Whoops.

121
00:08:06,280 --> 00:08:11,350
Let's make sure that's correct and then show the result.

122
00:08:11,350 --> 00:08:14,550
So let me zoom out a bit so we can see this whole thing.

123
00:08:14,710 --> 00:08:16,760
Run this and I get the same results.

124
00:08:16,780 --> 00:08:19,290
This is everything we just did in one line.

125
00:08:19,330 --> 00:08:20,220
Pretty ugly.

126
00:08:20,260 --> 00:08:23,100
Not very readable so I wouldn't suggest you do it this way.

127
00:08:23,110 --> 00:08:26,500
But if you find yourself needing to do it it's definitely possible.

128
00:08:26,500 --> 00:08:27,070
All right.

129
00:08:27,070 --> 00:08:28,330
That's really all we need to know.

130
00:08:28,330 --> 00:08:33,100
Basically the personal decision of whether you should feel something or drop something is going to have

131
00:08:33,100 --> 00:08:35,640
to be evaluated on a case by case basis.

132
00:08:35,650 --> 00:08:41,200
There's no general rules on when it's good to fill or when it's good to just drop.

133
00:08:41,200 --> 00:08:47,830
It really depends on what algorithm you're using or what's the general objective of the data analysis

134
00:08:47,830 --> 00:08:52,280
so keep that in mind whenever you're working with data that has missing data.

135
00:08:52,340 --> 00:08:53,080
OK.

136
00:08:53,170 --> 00:08:54,730
Thanks and I'll see you at the next lecture.
