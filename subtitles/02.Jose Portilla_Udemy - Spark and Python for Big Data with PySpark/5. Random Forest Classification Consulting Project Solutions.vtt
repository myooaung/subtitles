WEBVTT
1
00:00:05.730 --> 00:00:12.370
Hello everyone and welcome to the solutions lecture for the tree methods consulting project again.

2
00:00:12.390 --> 00:00:18.480
Our main task was to figure out which preservative chemical A B C or D was having an effect on the dog

3
00:00:18.480 --> 00:00:20.130
food being spoiled.

4
00:00:20.130 --> 00:00:24.080
So we actually use machine learning models to solve this issue.

5
00:00:24.360 --> 00:00:29.250
Many machine learning models produced some sort of coefficient value for each feature involved indicating

6
00:00:29.250 --> 00:00:33.780
their importance or predictive power relative to the actual label.

7
00:00:33.780 --> 00:00:39.140
Remember back to the documentation lecture for the section of the course we mentioned that these tree

8
00:00:39.140 --> 00:00:43.380
method classifiers have a feature importances attribute available.

9
00:00:43.550 --> 00:00:48.410
So what we can do is we can create a model fit on all the data and then check which feature.

10
00:00:48.440 --> 00:00:53.750
In this case our preservative was causing the spoilage or had the most predictive power to the label

11
00:00:54.410 --> 00:00:59.660
when using feature importances you're going to get something like this back and return a sparse vector

12
00:00:59.930 --> 00:01:05.570
with the first number indicating how many features were used and then the second entry is almost like

13
00:01:05.570 --> 00:01:11.360
a dictionary where you have the actual index number of the feature so index number 0 and then it's actual

14
00:01:11.360 --> 00:01:11.940
important.

15
00:01:11.940 --> 00:01:18.530
So here we can see that the most important feature was by far feature index to giving a value of zero

16
00:01:18.530 --> 00:01:20.320
point nine 6 8 6.

17
00:01:20.330 --> 00:01:24.590
Now the corresponding features column would look something like this you would have a row the features.

18
00:01:24.590 --> 00:01:26.820
Remember we have to input it as a dense vector.

19
00:01:26.990 --> 00:01:32.740
And then these are some particular values so these are actual preservative values with 4 to 12 and 3.

20
00:01:32.810 --> 00:01:35.330
And then finally you have a labeled column spoiled.

21
00:01:35.330 --> 00:01:37.620
So we're going to go over how this actually works.

22
00:01:37.640 --> 00:01:42.170
And an example of it later on in the Jupiter notebook.

23
00:01:42.220 --> 00:01:46.300
Now there are many different ways to solve the actual problem including just using something like Peirce's

24
00:01:46.310 --> 00:01:48.980
mystic's instead of a machine learning model so keep that in mind.

25
00:01:49.120 --> 00:01:54.340
This is definitely not the only way to solve this problem and hopefully his consulting project shows

26
00:01:54.340 --> 00:01:57.790
how we can apply machine learning in a different way from previous examples.

27
00:01:57.790 --> 00:02:02.200
For example in this case we don't really care about train test splits or even deployments.

28
00:02:02.200 --> 00:02:07.370
All we really care about is the actual relationship between a feature and its label.

29
00:02:08.630 --> 00:02:13.850
So again what we really want to fundamentally understand is which feature is important for predicting

30
00:02:13.970 --> 00:02:15.500
the label.

31
00:02:15.510 --> 00:02:16.120
All right.

32
00:02:16.290 --> 00:02:19.910
Let's jump over to Jupiter notebook and get started.

33
00:02:19.920 --> 00:02:24.760
OK so here we have a new notebook and I've created a spark session and the next thing I want to do is

34
00:02:24.760 --> 00:02:26.280
actually read in the data.

35
00:02:26.280 --> 00:02:33.880
I will say data spark read C S V and I'm going to read in dog food CXXVI file.

36
00:02:33.880 --> 00:02:36.910
I will say in first schema and also say header is true.

37
00:02:36.910 --> 00:02:42.970
And as always there's a notebook underneath the tree methods folder that has all the code that is going

38
00:02:42.970 --> 00:02:44.450
to be working through here.

39
00:02:44.470 --> 00:02:50.020
So it's called Tree methods consulting project solution and the basic idea of our approach here is we're

40
00:02:50.020 --> 00:02:55.870
going to perform a random forest classifier model on the data itself.

41
00:02:55.870 --> 00:02:59.900
So if I say data ahead 1 just so I can see what the data looks like.

42
00:03:00.040 --> 00:03:05.560
I have these four rows or four columns I should say these four columns for this particular row of the

43
00:03:05.560 --> 00:03:08.510
preservative percentage amounts A B C and D.

44
00:03:08.620 --> 00:03:10.870
And then I have whether or not it was spoiled.

45
00:03:10.870 --> 00:03:16.360
So what I want to do for this basic approach is build that random for classifier model that is able

46
00:03:16.360 --> 00:03:22.080
to predict whether or not this particular batch of dog food is spoiled based off their preservative

47
00:03:22.080 --> 00:03:22.600
amounts.

48
00:03:22.630 --> 00:03:28.660
Then I'm going to request the feature importance off that model and see if a particular preservative

49
00:03:28.690 --> 00:03:29.630
is really important.

50
00:03:29.710 --> 00:03:33.610
In that case I can figure out which preservative is causing the issue.

51
00:03:33.940 --> 00:03:36.900
So let's continue along in order to do this.

52
00:03:37.150 --> 00:03:39.820
The data needs to be in the right format.

53
00:03:39.820 --> 00:03:48.630
So I need to say from Paice park the M-L that feature a vector assembler.

54
00:03:48.840 --> 00:03:53.290
So I will run that and then the next thing I want to do is check my data columns.

55
00:03:53.730 --> 00:03:58.400
So I have these for preservative columns as well as the spoiled label column.

56
00:03:58.440 --> 00:04:04.950
So then I will create an assembler object and set it equal to vector assembler and my input columns

57
00:04:05.520 --> 00:04:10.750
are going to be a B see indeed.

58
00:04:11.060 --> 00:04:17.900
So the four preservatives and I'm going to create that as an output column just called features.

59
00:04:18.030 --> 00:04:18.930
So there it is.

60
00:04:18.990 --> 00:04:21.560
And then the next step is actually transform the data I'll say.

61
00:04:21.570 --> 00:04:24.220
Output is equal to that assembler object.

62
00:04:24.220 --> 00:04:25.270
I just made.

63
00:04:25.470 --> 00:04:32.070
And then I'm going to transform my actual data source transform my data and then we'll save from Paice

64
00:04:32.070 --> 00:04:34.390
spark MLR.

65
00:04:34.390 --> 00:04:41.450
Cl. I'm going to import my ran the forest classifier and we can also do this if a decision tree classifier.

66
00:04:41.490 --> 00:04:42.660
Either way it's fine.

67
00:04:42.960 --> 00:04:51.550
So I will say for classifier and I'll say RAFC is equal to a random force classifier.

68
00:04:51.720 --> 00:04:54.270
And in the solutions book we use a decision tree classifier.

69
00:04:54.270 --> 00:04:56.310
So either way it's fine.

70
00:04:56.310 --> 00:05:03.850
I'm going to say my label column is equal to spoiled and then I'm going to say my features column is

71
00:05:03.850 --> 00:05:05.960
equal to features.

72
00:05:07.370 --> 00:05:13.820
Next one I'm going to do is check out my output so my output file print the schema.

73
00:05:13.870 --> 00:05:18.880
Notice here I have the spoiled column and the features vector column which means for my final data I

74
00:05:18.880 --> 00:05:22.050
really just need these two pass to the random forest classifier.

75
00:05:22.420 --> 00:05:29.860
So I'll say final data is equal to output dot select and anoints select the features column as well

76
00:05:29.890 --> 00:05:31.060
as the spoiled column.

77
00:05:31.090 --> 00:05:37.450
And you can test this with a list or without a list if you get errors without a list then just pass

78
00:05:37.510 --> 00:05:38.220
in with a list.

79
00:05:38.230 --> 00:05:42.190
But the latest versions of Sparke should have no problems with list or no list.

80
00:05:42.190 --> 00:05:46.070
So you can see here I'm passing in the brackets and it still works OK.

81
00:05:46.090 --> 00:05:47.270
So there's my final data.

82
00:05:47.410 --> 00:05:53.930
If I just show my final data I can hear the features and then that label column.

83
00:05:53.930 --> 00:05:58.220
So now let's train our classifier on that actual data.

84
00:05:58.220 --> 00:06:00.270
So we're going to take her data.

85
00:06:00.620 --> 00:06:01.510
Well let's create a model.

86
00:06:01.520 --> 00:06:09.650
I should say our of model and say r f c whatever happened to call it fit on that final data.

87
00:06:09.750 --> 00:06:13.860
We'll run that and that may take a little bit of time of the pinning on how many trees decided.

88
00:06:13.890 --> 00:06:15.750
I just use the default plente trees.

89
00:06:15.900 --> 00:06:20.640
Now that we have a fitted model what I can do is request the feature importance of this model.

90
00:06:20.850 --> 00:06:26.060
But let me just show you something real quick if we take out the actual final data and then show the

91
00:06:26.060 --> 00:06:28.750
head say head of one.

92
00:06:28.790 --> 00:06:34.820
I can see here that I have the dense vector of the features and each of these is at a certain index

93
00:06:34.820 --> 00:06:38.350
locations is 0 1 2 and then 3.

94
00:06:38.690 --> 00:06:46.790
So if I actually call RAFC model and then request the feature importances remember that from the documentation

95
00:06:46.790 --> 00:06:52.000
example if I run this I can see the actual feature importance of each of these.

96
00:06:52.010 --> 00:07:00.410
So zero correlates to a 1 correlates to be 2 correlates to C and then 3 correlates to D.

97
00:07:00.540 --> 00:07:05.380
And you'll notice one of these particular letters is by far the most important feature and that's chemical

98
00:07:05.400 --> 00:07:07.010
C index too.

99
00:07:07.280 --> 00:07:11.470
So that's by far the most important feature meaning it's actually causing the early spoilage.

100
00:07:11.720 --> 00:07:16.400
So hopefully this was a pretty interesting example of using machine learning models in an alternative

101
00:07:16.400 --> 00:07:17.260
way.

102
00:07:17.540 --> 00:07:20.300
So we don't actually need to do a test train split here.

103
00:07:20.330 --> 00:07:26.620
All we really wanted to know was what feature is really driving the causation of whether or not something

104
00:07:26.620 --> 00:07:27.870
is being spoiled.

105
00:07:27.870 --> 00:07:34.430
Now you could definitely see other models could do this but for actual Just a quick easy attribute.

106
00:07:34.730 --> 00:07:38.620
Definitely random forest or decision tree classifiers make it really easy for you.

107
00:07:38.630 --> 00:07:41.940
So feature importance you can call that off a decision tree as well.

108
00:07:41.980 --> 00:07:46.340
And if you call it off at Decision Tree It would also indicate that this is by far the most important

109
00:07:46.340 --> 00:07:47.550
feature.

110
00:07:47.570 --> 00:07:48.100
OK.

111
00:07:48.350 --> 00:07:52.700
Hopefully that made sense to you if you have any questions since I know this was a bit of a different

112
00:07:52.700 --> 00:07:54.480
approach than what we'd done in the past.

113
00:07:54.500 --> 00:07:56.670
Feel free to ask them at the Q&amp;A forums.

114
00:07:56.930 --> 00:07:57.570
Okay.

115
00:07:57.570 --> 00:07:59.670
Thanks and I'll see at the next section of the course.
