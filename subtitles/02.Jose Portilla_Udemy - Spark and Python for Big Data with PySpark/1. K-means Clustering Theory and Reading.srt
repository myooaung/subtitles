1
00:00:05,500 --> 00:00:10,860
Hello everyone and welcome to the clustering section of the course we've seen how to deal with labeled

2
00:00:10,870 --> 00:00:11,480
data.

3
00:00:11,560 --> 00:00:13,300
But what about unlabelled data.

4
00:00:13,360 --> 00:00:18,340
Often you'll find yourself trying to create groups from data instead of trying to predict classes or

5
00:00:18,340 --> 00:00:19,640
continuous values.

6
00:00:20,680 --> 00:00:22,900
This sort of problem is known as clustering.

7
00:00:22,900 --> 00:00:26,230
You can think of it almost as an attempt to create labels.

8
00:00:26,230 --> 00:00:30,700
You input some unlabeled data and the unsupervised learning algorithm returns back.

9
00:00:30,700 --> 00:00:33,370
Possible clusters of the data.

10
00:00:33,400 --> 00:00:37,960
This means you have data that only contains features and you want to see if there are patterns in the

11
00:00:37,960 --> 00:00:41,200
data that would allow you to create groupings or clusters.

12
00:00:42,000 --> 00:00:46,770
This is a key distinction from our previous supervised learning tasks where we had historical labelled

13
00:00:46,770 --> 00:00:47,550
data.

14
00:00:47,550 --> 00:00:52,620
Now we're going to try to do is have unlabeled data and attempt to discover possible labels through

15
00:00:52,620 --> 00:00:56,250
clustering by the nature of this very problem.

16
00:00:56,280 --> 00:01:00,970
It can be difficult to evaluate the groups or clusters for quote unquote correctness.

17
00:01:00,990 --> 00:01:05,730
A large part of being able to interpret the clusters assigned comes down to domain knowledge.

18
00:01:06,940 --> 00:01:11,570
So for example maybe have some customer data and then you clustered them into distinct groups.

19
00:01:11,590 --> 00:01:16,330
It's going to be up to you and your domain knowledge to decide what those groups actually represent.

20
00:01:16,330 --> 00:01:19,700
Sometimes this is really easy and other times it's really hard.

21
00:01:20,580 --> 00:01:26,280
For example you can cluster tumors into two groups hoping to separate between benign tumors and malignant

22
00:01:26,280 --> 00:01:31,200
tumors but there's no guarantee that the clusters will fall along those lines it will just be split

23
00:01:31,200 --> 00:01:33,130
into the two most separable groups.

24
00:01:34,170 --> 00:01:39,390
Also depending on the clustering algorithm it may be up to you to decide beforehand how many clusters

25
00:01:39,390 --> 00:01:45,720
you expect to create a lot of clustering problems have no 100 percent correct approach or answer.

26
00:01:45,810 --> 00:01:48,360
And that's just the nature of unsupervised learning in general.

27
00:01:48,450 --> 00:01:51,420
Since you never had those labels to check against.

28
00:01:51,420 --> 00:01:57,450
So let's continue on by discussing Kamins clustering a specific clustering algorithm which is unsupervised

29
00:01:57,450 --> 00:02:03,660
learning so if you want more background and Kamins clustering and the mathematics behind it read Chapter

30
00:02:03,660 --> 00:02:10,550
10 of an introduction to Siskel learning Kamins clustering is an unsupervised learning algorithm that

31
00:02:10,550 --> 00:02:13,760
will attempt to group similar clusters together in your data.

32
00:02:13,760 --> 00:02:16,710
So what do typical clustering problems actually look like.

33
00:02:16,710 --> 00:02:21,920
You can cluster similar documents cluster customers based off their features do market segmentation

34
00:02:21,990 --> 00:02:28,690
identify similar physical groups and much more the overall goal is to divide data into distinct groups

35
00:02:28,690 --> 00:02:31,890
such that observations within each group are similar.

36
00:02:31,930 --> 00:02:37,030
So we can see here on the left what an example of unlabeled data would look like if you plotted it on

37
00:02:37,030 --> 00:02:42,940
two features one features our next one features on y Kamins clustering is going to attempt to cluster

38
00:02:42,940 --> 00:02:45,840
that data into distinct groups so we can see on the right.

39
00:02:45,840 --> 00:02:48,690
So you can see here is almost discovering those labels.

40
00:02:48,790 --> 00:02:52,290
In this case we're represented by colors.

41
00:02:52,430 --> 00:02:55,570
So the Kamins algorithm does the following steps.

42
00:02:55,670 --> 00:03:00,530
You as the user have to choose a number of clusters k so you have to choose that before you actually

43
00:03:00,530 --> 00:03:01,850
initiate the model.

44
00:03:01,880 --> 00:03:06,890
That means you're going to have to use domain knowledge to choose a reasonable value then it's going

45
00:03:06,890 --> 00:03:11,770
to randomly assigned each point to a cluster until clusters stop changing.

46
00:03:11,810 --> 00:03:14,320
It's going to repeat the following for each cluster.

47
00:03:14,340 --> 00:03:19,400
It's going to compute the cluster centroid by taking the mean vector of points in the cluster then it

48
00:03:19,400 --> 00:03:24,290
will assign each data point to the cluster for which the centroid is the closest.

49
00:03:24,290 --> 00:03:28,550
So let's actually see this in action on the very first panel on the top left.

50
00:03:28,550 --> 00:03:29,430
We have our data.

51
00:03:29,540 --> 00:03:33,610
It's unlabelled and it's totally ready to go for clustering.

52
00:03:33,650 --> 00:03:35,540
Then we have Step 1.

53
00:03:35,540 --> 00:03:38,620
In this case we've chosen to be equal to 3.

54
00:03:38,690 --> 00:03:46,340
So we randomly assign each point to one of the groupings of k then we're going to do on our first iteration

55
00:03:46,610 --> 00:03:48,170
which we'll call Step 2.

56
00:03:48,350 --> 00:03:51,400
We randomly place the send choice on top of our data.

57
00:03:51,530 --> 00:03:59,090
Then on iteration once that to be that's on that bottom row we assign the individual data points to

58
00:03:59,090 --> 00:04:02,390
the actual grouping for the centroid they're closest to.

59
00:04:02,390 --> 00:04:08,990
Then on iteration to that to a well we end up doing is moving those centroid to the average of all the

60
00:04:08,990 --> 00:04:13,090
vectors from connecting from that data point to the actual centroid.

61
00:04:13,160 --> 00:04:17,690
And then we keep repeating this result until the centroid stop moving and then eventually you have your

62
00:04:17,690 --> 00:04:20,420
actual cluster groups.

63
00:04:20,430 --> 00:04:24,660
So again choosing a value this is going to be pretty difficult decision.

64
00:04:24,810 --> 00:04:26,450
Or maybe a really easy decision.

65
00:04:26,460 --> 00:04:30,810
It really depends on the domain knowledge and what the data looks like depending on what K-Ville you

66
00:04:30,810 --> 00:04:31,470
choose.

67
00:04:31,470 --> 00:04:36,870
You may get various groupings and some of them may be obvious or intuitive choices and other ones may

68
00:04:36,870 --> 00:04:38,370
be completely unclear.

69
00:04:38,410 --> 00:04:40,480
Here you can see some similar data.

70
00:04:40,560 --> 00:04:42,330
We have different values chosen.

71
00:04:42,360 --> 00:04:44,550
Some of these look to be making sense.

72
00:04:44,550 --> 00:04:46,970
Other ones don't look to make sense at all.

73
00:04:46,980 --> 00:04:49,110
It really depends on what your data looks like.

74
00:04:49,110 --> 00:04:54,600
So as I've mentioned before kind of the shadow of unsupervised learning algorithms is that there aren't

75
00:04:54,600 --> 00:04:55,800
really easy answers.

76
00:04:55,800 --> 00:05:01,200
And in this particular case there's no easy answer for choosing a best value without specific domain

77
00:05:01,200 --> 00:05:01,980
knowledge.

78
00:05:02,010 --> 00:05:06,200
One mathematical method you could use is called the elbow method.

79
00:05:06,210 --> 00:05:11,520
So what you end up doing is you compute the sum of squared errors otherwise known as S S E for some

80
00:05:11,520 --> 00:05:16,340
values of k for example KCSM two equals four KCl six etc..

81
00:05:16,470 --> 00:05:22,620
The SCC that sum of squared error is defined as the sum of the squared distance between each member

82
00:05:22,620 --> 00:05:30,180
of the cluster to its actual centroid then what you end up doing is you plot K against SSD and you will

83
00:05:30,180 --> 00:05:33,320
see that the error decreases as K gets larger.

84
00:05:33,510 --> 00:05:36,090
And this is because when the number of clusters increases.

85
00:05:36,210 --> 00:05:40,250
They should be smaller so each distortion is going to be smaller as well.

86
00:05:40,260 --> 00:05:46,020
The idea of the elbow method is to choose the K at which the SSD decreases abruptly.

87
00:05:46,020 --> 00:05:48,900
So this for this is what's known as an elbow effect in the graph.

88
00:05:48,960 --> 00:05:50,770
So you can see this in the following picture.

89
00:05:51,510 --> 00:05:56,720
So here we can see we have the number of clusters chosen and the within group sum of squares.

90
00:05:56,790 --> 00:06:04,410
And you can see that around 5 or 6 you get the abrupt drop so that's known as the Elbel method because

91
00:06:04,410 --> 00:06:06,630
it kind of looks like the Elbel of an arm.

92
00:06:06,750 --> 00:06:12,150
So that indicates that choosing around that point is a good value and hopefully also have domain knowledge

93
00:06:12,420 --> 00:06:15,260
that helps drive that decision forward.

94
00:06:16,930 --> 00:06:22,120
So as far as plotting Paice spark by itself doesn't support a plotting mechanism but could always use

95
00:06:22,120 --> 00:06:25,960
collects to get back a Python object such as a list or an array.

96
00:06:26,110 --> 00:06:32,080
And then you can plot the results with MAP lib or other Python visualization libraries.

97
00:06:32,090 --> 00:06:36,900
OK so as far as the elbow method don't take that as a strict rule when choosing a k value.

98
00:06:37,140 --> 00:06:41,030
As I keep mentioning a lot of it depends more on the context of the exact situation.

99
00:06:41,040 --> 00:06:42,760
Otherwise known as domain knowledge.

100
00:06:42,840 --> 00:06:47,460
So we'll try our best to get a feel for this with the examples in the consulting projects.

101
00:06:47,460 --> 00:06:52,040
All right let's get started by walking through the documentation example for Kamins clustering.

102
00:06:52,050 --> 00:06:53,130
I'll see you at the next lecture.
