1
00:00:06,770 --> 00:00:11,570
Welcome back in this lecture We'll discuss the basics of working with Sparke data frames.

2
00:00:11,750 --> 00:00:14,010
Let's jump to Jupiter notebook and get started.

3
00:00:15,000 --> 00:00:21,410
So in order to actually work with Sparke data frames we first need to start a spark session to do that.

4
00:00:21,450 --> 00:00:29,920
I can say from Paice spark that sequel in poor sparks session and note the capitalization here.

5
00:00:30,090 --> 00:00:32,640
You can always just use tab to complete this.

6
00:00:32,640 --> 00:00:37,660
So I'm going to run that and then we need to actually start the spark session by applying it.

7
00:00:37,680 --> 00:00:44,610
So annoying create a variable by default or convention I should say you call it spark and then we'll

8
00:00:44,610 --> 00:00:47,950
say spark session dots builder.

9
00:00:48,420 --> 00:00:57,300
App name and let's give this app name something like basics and then I will call get or create and notice

10
00:00:57,320 --> 00:01:01,060
capitalization there and then do shift enter to run this.

11
00:01:01,070 --> 00:01:04,970
Now most likely if you just open up the notebook and you're running this command for the first time

12
00:01:05,240 --> 00:01:07,020
it's going to take maybe a minute or two.

13
00:01:07,040 --> 00:01:10,530
I already ran this previously so that's why it's working so fast right now.

14
00:01:10,760 --> 00:01:16,080
So maybe if you're working on a W-S elastic map reduce it's also going to run really fast.

15
00:01:16,280 --> 00:01:20,900
Something like a local virtual box or SETI might take a little while that very first time.

16
00:01:21,050 --> 00:01:25,230
So don't worry if it takes a little bit of time just give it one or two minutes.

17
00:01:25,240 --> 00:01:25,870
All right.

18
00:01:25,870 --> 00:01:30,670
The next thing I want to do is actually read a data set we'll talk a lot more about various data inputs

19
00:01:30,730 --> 00:01:31,440
and outputs.

20
00:01:31,450 --> 00:01:36,610
But for right now we're going to be using the people that chase on a file that's included with the course

21
00:01:36,610 --> 00:01:43,420
notes so we'll say the F is equal to spark and then we'll call the read method and then from that will

22
00:01:43,420 --> 00:01:45,940
say dot and then if I hit tab here.

23
00:01:46,090 --> 00:01:51,070
Luckily Jupiter notebook can show me all the various ways I can read stuff but also keep it simple for

24
00:01:51,070 --> 00:01:54,340
now and just say thought Jason and then I'm going to read.

25
00:01:54,360 --> 00:01:57,100
And people thought Jason.

26
00:01:57,220 --> 00:02:02,380
Now I recommend that you start this Jupiter note book in the same directory where this file is located.

27
00:02:02,500 --> 00:02:05,640
So you don't need to worry about providing the full file path name.

28
00:02:05,770 --> 00:02:11,650
If this notebook is in a different directory than this data then in that case you do need to post the

29
00:02:11,650 --> 00:02:14,450
whole file pathname as a string here.

30
00:02:14,590 --> 00:02:20,560
And keep in mind sometimes Sparke can be a little tricky when working the file paths to have spaces

31
00:02:20,560 --> 00:02:21,220
in them.

32
00:02:21,250 --> 00:02:25,780
Sometimes it doesn't like that so you may need to put underscores or get rid of those spaces so keep

33
00:02:25,780 --> 00:02:28,160
that in mind as you work with Sparke.

34
00:02:28,210 --> 00:02:31,030
So I will run this and then I have my data frame.

35
00:02:31,030 --> 00:02:36,400
So let's actually show some of this data in order to show the data and show it data frame you just call

36
00:02:36,400 --> 00:02:40,540
the DOT show and it's a method so we call a closed parentheses there.

37
00:02:40,720 --> 00:02:42,400
And now it's actually missing some data.

38
00:02:42,420 --> 00:02:44,020
But SPARC can handle it.

39
00:02:44,020 --> 00:02:48,490
So we have an age column and a name column and we have a null value for this age.

40
00:02:48,520 --> 00:02:53,380
Spark has no problem dealing that it automatically replaces missing data with null and we'll have a

41
00:02:53,380 --> 00:02:56,750
whole another lecture on working of missing data with SPARK data frames.

42
00:02:56,770 --> 00:03:00,180
But right now notice how it's missing and how it still works.

43
00:03:00,370 --> 00:03:06,490
If you want to know the schema of a data frame you can use the DFT print schema method and then just

44
00:03:06,490 --> 00:03:10,900
using tabs to autocomplete there close parentheses because this is a method to run it.

45
00:03:11,470 --> 00:03:16,930
And here I get the roots and then the name of the column age and name and the data type.

46
00:03:16,930 --> 00:03:21,120
So long is just kind of another way of saying a certain numeric integer data type.

47
00:03:21,250 --> 00:03:23,920
If you haven't encountered it forward don't worry too much about it.

48
00:03:23,980 --> 00:03:25,770
And then we have this string data type.

49
00:03:25,770 --> 00:03:33,010
So luckily SPARC was able to infer that we had age as some sort of numeric long integer data type because

50
00:03:33,010 --> 00:03:36,430
of 30 and 19 even that missing data and really stop it there.

51
00:03:36,610 --> 00:03:40,510
And it was able to clarify that name is some sort of string data type.

52
00:03:40,510 --> 00:03:45,880
Now sometimes depending on your data source it may just default everything to a string and we'll talk

53
00:03:45,880 --> 00:03:47,340
about that in just a little bit.

54
00:03:47,410 --> 00:03:50,700
But I want to show you two more methods one of them is actually an attribute.

55
00:03:50,800 --> 00:03:56,560
If you just want the column names you can use DFT columns and this is an attribute meaning you don't

56
00:03:56,560 --> 00:03:59,360
need to actually have those close parentheses there.

57
00:03:59,560 --> 00:04:06,220
And luckily it just returns back a python list of all the column names and then we can call this scribe

58
00:04:06,670 --> 00:04:09,470
to get a statistical summary of the data frame.

59
00:04:09,760 --> 00:04:16,320
And if you called the described by itself can just run this you get back a data frame.

60
00:04:16,430 --> 00:04:17,730
So what does it actually mean.

61
00:04:17,930 --> 00:04:23,360
Well it says hey I returned that data frame with a column called summary which has some strings in it

62
00:04:23,810 --> 00:04:27,270
and a column called age which also has some strings in it.

63
00:04:27,320 --> 00:04:33,440
If you actually want to see the state of frame then we need a call that show and then actually the data

64
00:04:33,440 --> 00:04:35,740
framed that was the description data frame.

65
00:04:35,750 --> 00:04:37,190
So what does the scribe actually do.

66
00:04:37,190 --> 00:04:41,060
It's just a statistical summary of the numeric columns in your data frame.

67
00:04:41,240 --> 00:04:43,700
So the only numeric column we had was the age column.

68
00:04:43,700 --> 00:04:45,000
So it gives you a count.

69
00:04:45,080 --> 00:04:47,060
There was two values in there.

70
00:04:47,130 --> 00:04:51,810
Note that we just had a scroll back up 13:19 it won't count that null value there.

71
00:04:51,830 --> 00:04:56,390
It gives you the average some standard deviation and then the min and max.

72
00:04:56,420 --> 00:05:00,860
Now later on we'll also talk about how you can edit the output so you don't see so many significant

73
00:05:00,860 --> 00:05:01,270
figures.

74
00:05:01,280 --> 00:05:06,170
But for right now just know that that describe and that show are way to quickly see a statistical summary

75
00:05:06,470 --> 00:05:07,820
of the numeric columns.

76
00:05:08,770 --> 00:05:12,650
Now I want to take a little break and discuss schema.

77
00:05:12,730 --> 00:05:17,950
Luckily we're dealing for pretty small data set and the schema was inferred correctly some sort of long

78
00:05:17,950 --> 00:05:20,420
integer value and then a string.

79
00:05:20,650 --> 00:05:23,130
Often if you're not dealing with data that's really nice.

80
00:05:23,130 --> 00:05:28,050
Or maybe from a particular source you need to actually clarify what the schema is.

81
00:05:28,210 --> 00:05:32,650
So in order to do certain operations the schema has to be correct there has to know what columns are

82
00:05:32,650 --> 00:05:35,720
strings or columns are integers etc..

83
00:05:36,160 --> 00:05:41,500
So let's walk you through how to actually do this SPARC has all the tools you need but it just requires

84
00:05:41,500 --> 00:05:43,280
kind of a very specific structure.

85
00:05:43,360 --> 00:05:49,240
So the documentation actually isn't so great on showing this so I want to walk you through it myself.

86
00:05:49,390 --> 00:05:52,960
The first thing to do is actually import these type tools.

87
00:05:52,960 --> 00:06:00,890
So we'll say from Paice spark the sequel dot types is important.

88
00:06:01,090 --> 00:06:04,030
And you can actually use tabs help auto complete some of this.

89
00:06:04,030 --> 00:06:10,780
So I'm going to import a struct field which is the search field and then I can import a string type

90
00:06:11,830 --> 00:06:17,260
and I'm also going to import integer type and then if I hit tab here I can just see wups Let's hit tab

91
00:06:17,260 --> 00:06:21,320
over here so I can see the list of everything that's available for me here.

92
00:06:21,340 --> 00:06:21,860
It's a lot.

93
00:06:21,880 --> 00:06:24,720
So we'll just kind of stick with the ones we need.

94
00:06:24,880 --> 00:06:30,190
And I also want structure type so you can always refer to the notes if you want to just copy and paste

95
00:06:30,190 --> 00:06:35,760
this but we imported structure field string type integer type and struct type in as a quick no.

96
00:06:35,800 --> 00:06:39,820
If you're ever importing stuff and you don't want everything just in one long line like this I'm a little

97
00:06:39,820 --> 00:06:40,620
zoomed in.

98
00:06:40,810 --> 00:06:47,290
So you can do is have a princes here and then break it up into two lines and just wrap all the imports

99
00:06:47,350 --> 00:06:49,200
in these print princes and all still work.

100
00:06:49,480 --> 00:06:55,210
So I imported these four types and basically Next we need to actually create a list of structure fields

101
00:06:55,690 --> 00:07:01,360
and the structure fields takes in kind of these three parameters takes in the name the data type and

102
00:07:01,360 --> 00:07:02,500
some sort of nullable.

103
00:07:02,500 --> 00:07:09,190
So let me walk through that kind of explain it all create a variable called data schema.

104
00:07:11,140 --> 00:07:17,770
And this is going to be a list here and the list is going to take in structure field.

105
00:07:18,370 --> 00:07:25,270
And I want to pass in the column which is going to be age and I want to pass in an instance of integer

106
00:07:25,270 --> 00:07:30,440
type close parentheses and then I want to specify true.

107
00:07:30,550 --> 00:07:32,090
So what does this actually mean.

108
00:07:32,180 --> 00:07:35,120
Are you going to pass in name into this field.

109
00:07:35,230 --> 00:07:39,400
So that just means hey the column that this relates to is called age.

110
00:07:39,400 --> 00:07:42,010
Then you pass in some sort of class instance.

111
00:07:42,010 --> 00:07:45,940
In this case integer type meaning I want age to be integer type.

112
00:07:46,000 --> 00:07:52,150
So I'm actually going to be changing from long to straight integer and then the third parameter is a

113
00:07:52,150 --> 00:07:52,900
boolean.

114
00:07:52,900 --> 00:07:58,450
So whether or not the field can be NULL if you don't type in true here if you have a missing value they'll

115
00:07:58,450 --> 00:07:59,530
show an error.

116
00:07:59,530 --> 00:08:06,170
So right now I have true here saying hey it's OK that it's null and that's one structure field anoints

117
00:08:06,220 --> 00:08:07,380
create one more.

118
00:08:07,510 --> 00:08:09,710
So it's actually kind of start this on a new line.

119
00:08:09,970 --> 00:08:15,880
So I'll create one more structure field all in the same list just the new line and then this clarifies

120
00:08:15,880 --> 00:08:20,410
that the Name column or name field should be a string type.

121
00:08:20,410 --> 00:08:24,240
Which kind of matches with exactly what spark inferred on its own.

122
00:08:24,430 --> 00:08:28,460
And then I'll say true that it can be null even though we didn't actually have any.

123
00:08:28,780 --> 00:08:29,960
So that's my data schema.

124
00:08:29,980 --> 00:08:32,920
That's how you set up the schema that you're expecting.

125
00:08:32,920 --> 00:08:38,920
And then I'm going to set up a variable called Final struck for final structure and you pass in that

126
00:08:38,920 --> 00:08:42,960
data schema into the struct type.

127
00:08:42,960 --> 00:08:44,390
So structure type.

128
00:08:44,750 --> 00:08:49,270
And this takes in this Field's argument which is just this data schema.

129
00:08:49,270 --> 00:08:56,710
So you're going to say Fields is equal to data schema so we're going to run that.

130
00:08:56,860 --> 00:09:03,190
And now I'm actually going to read in that case on file but I'm going to clarify that I want the schema

131
00:09:03,190 --> 00:09:04,800
to be my final structure.

132
00:09:05,020 --> 00:09:08,770
So no what happens here when say the F is equal to spark.

133
00:09:08,830 --> 00:09:12,820
Read that Jason same file people that Jason.

134
00:09:12,940 --> 00:09:20,470
Except I'm going to add in schema as a parameter argument and pass in my final stroke.

135
00:09:20,740 --> 00:09:24,490
So let's run this and see what happens when we show the schema.

136
00:09:24,490 --> 00:09:27,820
So let's actually say DFA print schema.

137
00:09:28,760 --> 00:09:33,890
And now I can see that it's clarified age to be an integer instead of a long.

138
00:09:34,160 --> 00:09:40,040
So if you ever have issues with your own particular file formats where everything's being read into

139
00:09:40,070 --> 00:09:45,560
as a string you can use this methodology to actually clarify your particular schema type.

140
00:09:45,560 --> 00:09:50,150
So for esoteric data types this is kind of an issue sometimes.

141
00:09:50,180 --> 00:09:55,370
Luckily it's getting less and less of an issue and SPARC is getting better and better at inferring schema

142
00:09:55,370 --> 00:09:57,830
types so I want to show this here.

143
00:09:57,870 --> 00:10:04,100
But as a quick note we'll never have to do this for the course all the course materials are in C S ves

144
00:10:04,160 --> 00:10:09,740
and SPARC has no problem inferring the schema types but I do want you to be aware that this does exist

145
00:10:09,740 --> 00:10:11,010
for you as a tool.

146
00:10:11,250 --> 00:10:17,540
And honestly just frankly speaking right now that documentation I don't think does a very good job of

147
00:10:17,540 --> 00:10:22,430
showing this process but hopefully with this notebook and this video you now have a clear idea of what

148
00:10:22,430 --> 00:10:27,020
you would need to do in case she ever need it to define your own schema.

149
00:10:27,030 --> 00:10:32,010
That's all I wanted discuss for the first part of day to frame basics in the second part of day to frame

150
00:10:32,010 --> 00:10:32,770
basics.

151
00:10:32,880 --> 00:10:36,420
Kind of get a little more with the nitty gritty and the stuff that you'll actually be doing on a day

152
00:10:36,420 --> 00:10:41,570
to day basis things like grabbing the data creating new columns and even possibly using sequel with

153
00:10:41,580 --> 00:10:42,870
your data frames.

154
00:10:42,870 --> 00:10:47,460
So coming up next we're going to see some more realistic use cases for actually interacting with your

155
00:10:47,460 --> 00:10:48,560
data frame.

156
00:10:48,570 --> 00:10:50,310
Thanks and I'll see you at the next lecture.
