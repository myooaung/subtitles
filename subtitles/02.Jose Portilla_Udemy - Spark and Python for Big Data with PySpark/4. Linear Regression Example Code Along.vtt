WEBVTT
1
00:00:05.450 --> 00:00:10.850
Welcome back to our custom code example for linear regression with Python and Sparke what we're going

2
00:00:10.850 --> 00:00:13.990
to do is walk through a more realistic example of linear regression.

3
00:00:14.060 --> 00:00:19.820
We saw the basic workflow using documentation example but let's examine some e-commerce customer data

4
00:00:19.850 --> 00:00:22.970
for companies web site and a mobile app.

5
00:00:23.000 --> 00:00:28.280
The main idea is using various features we're going to try to predict it customers total expenditure

6
00:00:28.370 --> 00:00:33.840
which is just a continuous money value and we'll also discuss how to convert realistic data into a format

7
00:00:33.840 --> 00:00:39.090
accepted by sparks and Madlib that documentation example already had formatted that way.

8
00:00:39.110 --> 00:00:43.400
Realistically you're not going to have it in that format so we'll show you how to convert it into the

9
00:00:43.400 --> 00:00:48.890
spark and live format the relevant notebook for this lecture is called linear regression code along.

10
00:00:49.010 --> 00:00:53.340
Let's get started going to hop over to my browser with a new notebook with access to the data.

11
00:00:54.530 --> 00:00:56.120
All right let's get started.

12
00:00:56.120 --> 00:01:02.870
First we need to do is say from Paice spark thought sequel import a spark session and will actually

13
00:01:02.870 --> 00:01:04.490
create that spark session now.

14
00:01:04.730 --> 00:01:07.810
Hopefully by now you're kind of used to doing this.

15
00:01:08.240 --> 00:01:16.910
We just create an example and then get or create shift enter or Spark's ready to go and we're going

16
00:01:16.910 --> 00:01:24.200
to do is right off the top say from Paice sparked the M-L that regression in poor linear regression

17
00:01:24.320 --> 00:01:28.190
won't ease this right away but we will use it in just a little bit.

18
00:01:28.190 --> 00:01:29.390
Now let's get our data.

19
00:01:29.390 --> 00:01:33.350
So our data is going to be Sparke read that CXXVIII.

20
00:01:33.650 --> 00:01:38.810
And I'm going to be using the e-commerce underscore customers that see every file that's available to

21
00:01:38.810 --> 00:01:41.840
you in the notes under the linear regression folder.

22
00:01:41.880 --> 00:01:44.530
I also say in first schema is equal to true.

23
00:01:44.750 --> 00:01:47.300
And I'm also going to say that the header is true.

24
00:01:50.110 --> 00:01:54.010
And then once we've done that may take a little bit of time to read the pinning how fast your computer

25
00:01:54.010 --> 00:01:59.050
is what I want to do is just print the schema so we can explore the data just a little bit and I can

26
00:01:59.050 --> 00:02:03.720
see it has an email which has a string and address which is also a string some sort of avatar.

27
00:02:03.720 --> 00:02:06.970
We'll see if that isn't a second average session length that's a number.

28
00:02:06.970 --> 00:02:12.040
Time on app also number time on the Web site also number length of the memberships how much time has

29
00:02:12.040 --> 00:02:13.480
this customer been a member.

30
00:02:13.660 --> 00:02:17.560
And then your yearly amount spent on the service for the site.

31
00:02:17.560 --> 00:02:21.630
And that's the kind of variable that we're going to be trying to predict now for your own custom examples

32
00:02:21.640 --> 00:02:24.240
you're going to have to clarify what you need predicts.

33
00:02:24.250 --> 00:02:28.180
Right now I'm just telling you that this is the actual value we're trying to predict.

34
00:02:28.180 --> 00:02:32.830
So let's show that it actually looks like if I just hit show it's probably going to be too zoomed in

35
00:02:32.830 --> 00:02:34.510
for us to kind of get a good idea.

36
00:02:34.510 --> 00:02:39.670
So it's a little messy since that what I'm going to do is just show you head of one so we can see what

37
00:02:39.670 --> 00:02:41.080
the value actually looks like.

38
00:02:41.290 --> 00:02:46.170
And it's actually Same for will say zero.

39
00:02:46.210 --> 00:02:53.030
That's the actual role and I'll say for item in this print item.

40
00:02:53.040 --> 00:02:54.690
All right so here we can see the actual value.

41
00:02:54.690 --> 00:02:56.420
So this is the e-mail value.

42
00:02:56.430 --> 00:02:59.180
We can see here that there is actually an address as well.

43
00:02:59.190 --> 00:03:00.870
So Frank Tunnell whatever.

44
00:03:00.870 --> 00:03:04.050
Then there is the avatar and the avatar is actually just a color.

45
00:03:04.080 --> 00:03:11.910
So if I take a look at another example such as here had two loops section I wanted to one you can see

46
00:03:12.000 --> 00:03:12.630
dark green.

47
00:03:12.660 --> 00:03:14.100
So there just various colors.

48
00:03:14.220 --> 00:03:17.640
And then we have actual numeric values for these things right here.

49
00:03:17.640 --> 00:03:22.880
The average session length time spent on the app time spent on the Web site the length of the membership

50
00:03:22.890 --> 00:03:25.380
so that's in years and then yearly amounts spent.

51
00:03:25.380 --> 00:03:27.020
So this is all fake data.

52
00:03:27.030 --> 00:03:29.310
I generated this myself so don't worry.

53
00:03:29.310 --> 00:03:30.850
These aren't real e-mails or anything.

54
00:03:31.020 --> 00:03:32.450
Well hopefully they aren't real.

55
00:03:32.760 --> 00:03:33.280
OK.

56
00:03:33.390 --> 00:03:36.280
So moving along we're going to set up the data frame for machine learning.

57
00:03:36.300 --> 00:03:41.070
And this is one of the most important features of using and understanding how to use M-L Madlib especially

58
00:03:41.070 --> 00:03:42.130
of python and SPARC.

59
00:03:42.270 --> 00:03:44.270
So this is something you're really going to want to get down.

60
00:03:44.280 --> 00:03:48.570
The first thing you have to do is to import the vector assembler and vectors.

61
00:03:48.720 --> 00:03:55.260
So I'll say from Paice barked M-L Lynn Ouch import vector's.

62
00:03:55.290 --> 00:04:00.460
Now we don't actually use this directly but often in the documentation you see imported anyways.

63
00:04:00.480 --> 00:04:04.450
So I always like to do it just in case it's a requirement.

64
00:04:04.680 --> 00:04:08.940
So I'll say from Paice sparked the MLV up feature and this is where we're actually going to be using

65
00:04:08.940 --> 00:04:09.850
something.

66
00:04:09.840 --> 00:04:13.200
We're going to say vector assembler.

67
00:04:13.200 --> 00:04:14.040
All right.

68
00:04:14.040 --> 00:04:15.390
So this is what we need to do.

69
00:04:15.390 --> 00:04:20.220
We need to convert all the numerical columns that are going to be using won't work with the actual categorical

70
00:04:20.220 --> 00:04:23.630
columns who won't work with the address or the email or this avatar.

71
00:04:23.790 --> 00:04:26.490
We'll see how to work with those later on and future examples.

72
00:04:26.490 --> 00:04:28.390
Right now I just care about this numerical data.

73
00:04:28.410 --> 00:04:33.980
So I want to grab this numerical data as the features and I want to grab this as the label to predict.

74
00:04:34.040 --> 00:04:38.120
So let's continue on and show you how to use vector assembler.

75
00:04:38.120 --> 00:04:43.970
So again if I say data columns and show the column names that I want the only things I'm interested

76
00:04:43.970 --> 00:04:48.590
in are the average session length the time on the app time on the Web site.

77
00:04:48.590 --> 00:04:53.820
The length of the membership and this yearly amount spent that's going to be in my label column.

78
00:04:53.840 --> 00:04:57.220
So let's continue on by creating an assembler object.

79
00:04:57.260 --> 00:04:59.450
So we'll call it assembler.

80
00:04:59.450 --> 00:05:04.910
You can really call it whatever you want but it's going to be a vector assembler and it takes in the

81
00:05:04.910 --> 00:05:12.410
following arguments whoops takes in input calls and input calls is a list of strings of the actual columns

82
00:05:12.440 --> 00:05:13.310
you want to take in.

83
00:05:13.370 --> 00:05:17.390
So in this case this is going to be all our feature columns.

84
00:05:17.390 --> 00:05:19.550
So we say input columns.

85
00:05:19.550 --> 00:05:21.460
Now we can just copy and paste that here.

86
00:05:21.710 --> 00:05:28.270
So we have average session length time on App time on the Web site length of membership and if you can

87
00:05:28.270 --> 00:05:30.610
format this looks a little nicer than what I have here.

88
00:05:30.820 --> 00:05:33.500
So it's kind of put this all in one line.

89
00:05:33.570 --> 00:05:36.310
I'm zoomed in which is why it looks a little strange here.

90
00:05:37.770 --> 00:05:38.920
But there we go.

91
00:05:38.980 --> 00:05:44.220
And now after this we're going to do is say OK so impot calls.

92
00:05:44.220 --> 00:05:45.580
That's one of the arguments.

93
00:05:45.630 --> 00:05:52.350
The other argument we're going to be doing is this is the actual output column so output call is equal

94
00:05:52.350 --> 00:05:55.620
to features or whatever you want to call.

95
00:05:55.620 --> 00:05:59.760
But basically the idea behind vector assembler is I can zoom out just a little bit so we can see the

96
00:05:59.760 --> 00:06:05.520
whole thing is you have a list of input columns and those are the input columns you want to be the features

97
00:06:06.030 --> 00:06:09.270
and then you have an output column that's just a single feature.

98
00:06:09.270 --> 00:06:13.260
So it's going to grab all these columns and turn them into a single vector.

99
00:06:13.320 --> 00:06:17.430
So we have our assembler there which is basically just saying OK this is what to expect.

100
00:06:17.610 --> 00:06:23.580
The next thing I want to do is actually transform the data so I create an object called output and this

101
00:06:23.580 --> 00:06:29.850
will be that assembler object that it just made and I will call the transform method and I pass in my

102
00:06:29.850 --> 00:06:32.300
original data and I'm passing it all my data.

103
00:06:32.310 --> 00:06:34.760
I haven't done the train test split yet that's important.

104
00:06:34.920 --> 00:06:41.560
So I'm passing it all my data and if I may take a look at what the print schema of my output I have

105
00:06:41.560 --> 00:06:47.270
everything that I used to have and I have this new features column to get a better idea of what the

106
00:06:47.320 --> 00:06:49.190
actual features Roe looks like.

107
00:06:49.360 --> 00:06:50.510
Let's select it.

108
00:06:50.800 --> 00:07:00.380
So I'm going to say output had one and I can see here that this features object is a dense vector consisting

109
00:07:00.380 --> 00:07:02.340
of zoom in so little bit here.

110
00:07:03.180 --> 00:07:04.490
Of the values that I input.

111
00:07:04.510 --> 00:07:10.120
So we can see here we have essentially looks like a vector or array and it's a combination of all these

112
00:07:10.120 --> 00:07:11.780
numerical values that I wanted.

113
00:07:11.800 --> 00:07:12.160
Perfect.

114
00:07:12.160 --> 00:07:18.790
That's exactly what we want and that's the kind of format that SPARC is going to be expecting for linear

115
00:07:18.790 --> 00:07:22.020
regression or really any machine learning algorithm.

116
00:07:22.030 --> 00:07:26.080
So now the next step of what I want to do is select my final data.

117
00:07:26.140 --> 00:07:30.260
You don't actually have to do this but I recommend that cause it kind of tightens everything.

118
00:07:30.390 --> 00:07:30.970
So I'll say.

119
00:07:30.970 --> 00:07:33.840
Final data is output select.

120
00:07:33.850 --> 00:07:40.300
And all I care about is that features column that contains all the features I want and then the yearly

121
00:07:40.390 --> 00:07:43.710
amount spent column.

122
00:07:43.720 --> 00:07:49.860
All right so I have my final data and if I show my final data I can see yours that features column and

123
00:07:49.860 --> 00:07:52.870
then what is essentially my labels column yearly amounts spent.

124
00:07:53.070 --> 00:07:59.260
So now it's split this up into a train test split so say training data Khama test data.

125
00:07:59.270 --> 00:08:07.190
So using kind of tuple impacting here and I'll say my final data Colerain them split on it and by convention

126
00:08:07.250 --> 00:08:14.210
a 70 30 split is really common 60:40 is also really common as well as 80:20 depending on what kind of

127
00:08:14.210 --> 00:08:17.200
data you have how easy it is to train on that data.

128
00:08:17.340 --> 00:08:20.890
There's no real right answer on what you should do a split.

129
00:08:20.990 --> 00:08:25.040
But if you just want to kind of choose a default split 17:30 is a pretty good choice.

130
00:08:25.460 --> 00:08:26.760
So that's what we're going to do.

131
00:08:26.970 --> 00:08:34.610
Who do train test and or want to do is see and confirm that everything actually looks good.

132
00:08:34.610 --> 00:08:41.450
So it's kind of show the training data co-writes we have 357 in our training data and now we're going

133
00:08:41.460 --> 00:08:42.500
to do is copy and test.

134
00:08:42.540 --> 00:08:46.630
Place this for the test data.

135
00:08:46.660 --> 00:08:46.900
All right.

136
00:08:46.900 --> 00:08:48.960
And we have 1 4 3 in our test data.

137
00:08:49.050 --> 00:08:51.090
Are trained train split worked perfectly.

138
00:08:51.160 --> 00:08:53.320
Now create our linear regression model.

139
00:08:53.530 --> 00:08:57.970
So I'm going to create an object called L-R and this is going to be my linear regression model.

140
00:08:58.390 --> 00:09:04.270
And remember if you look at linear regression model we define the features column the label column and

141
00:09:04.270 --> 00:09:09.460
the prediction column The defaults are fine to use as long as you match up with the defaults.

142
00:09:09.460 --> 00:09:11.800
Now our label column isn't called label.

143
00:09:11.830 --> 00:09:13.990
Instead it's called the yearly amount spent.

144
00:09:13.990 --> 00:09:24.330
So let's actually adjust this by saying the label column is going to be called yearly amount spent.

145
00:09:24.480 --> 00:09:29.080
If we come back up here to when we're looking or features call column that is the same name as the default

146
00:09:29.080 --> 00:09:29.460
value.

147
00:09:29.470 --> 00:09:32.390
So that's totally okay believe that the same.

148
00:09:32.390 --> 00:09:35.690
So we have yearly amount spent as our label column.

149
00:09:35.950 --> 00:09:42.610
And then what I'm going to do is create a linear regression model and I'm just going to zoom in here.

150
00:09:42.790 --> 00:09:50.220
All I'm going to do is say L.R. fit and I'm going to fit to my training data so we'll fit your training

151
00:09:50.220 --> 00:09:53.330
data let that run again depending on what kind of computer we have.

152
00:09:53.430 --> 00:09:58.960
It may take a little bit of time and outside to see how well our model actually performed.

153
00:09:59.060 --> 00:09:59.920
So hard to do that.

154
00:09:59.940 --> 00:10:02.810
We need to evaluate it again some test data.

155
00:10:03.060 --> 00:10:12.490
So create test results is equal to L.R. model evaluated or evaluate on the test data.

156
00:10:12.510 --> 00:10:18.800
So now I have these test results and if I take a look at the test results and their residuals I can

157
00:10:18.800 --> 00:10:20.960
show these residuals and the residuals.

158
00:10:20.960 --> 00:10:25.840
It's just a difference between the predictive value and the actual label from the test data.

159
00:10:25.880 --> 00:10:27.230
So let's scroll down here.

160
00:10:27.350 --> 00:10:32.970
Continue on and get some of those regression evaluation metrics that we discussed earlier.

161
00:10:33.010 --> 00:10:36.680
Well I'm going to do that just by calling test results.

162
00:10:36.800 --> 00:10:41.570
And then what I can do is call the root mean squared error.

163
00:10:41.930 --> 00:10:43.980
So that's my root mean squared error.

164
00:10:44.300 --> 00:10:48.230
And then we can also call from test results.

165
00:10:48.340 --> 00:10:52.470
You can call our squared and that's 0.9.

166
00:10:52.520 --> 00:10:56.450
So this kind of indicates that we're explaining a lot of the variance.

167
00:10:56.480 --> 00:11:01.520
This is a very good model just from the root mean squared error and the residuals because if you look

168
00:11:01.520 --> 00:11:03.580
at the actual original data.

169
00:11:03.740 --> 00:11:12.810
So if you look at final data describe that show lips describes our show as we begin to finish up this

170
00:11:12.810 --> 00:11:13.440
lecture.

171
00:11:13.440 --> 00:11:15.130
There's two more things I want to do.

172
00:11:15.180 --> 00:11:21.360
One is to actually go over these results this root mean squared error square compared to that final

173
00:11:21.360 --> 00:11:27.270
data and the second thing I want to do is just quickly review how you could deploy this model on some

174
00:11:27.390 --> 00:11:28.620
totally unlabeled data.

175
00:11:28.630 --> 00:11:32.770
So not just test data but only data that you have features for.

176
00:11:32.790 --> 00:11:34.810
So let's quickly review what's going on here.

177
00:11:34.920 --> 00:11:36.950
This number the root mean squared error.

178
00:11:36.990 --> 00:11:42.580
Remember that's the difference between your test value that true value versus the predictive value.

179
00:11:42.600 --> 00:11:48.360
So you take that difference you square it which in turn punishes larger errors and to turn it back into

180
00:11:48.360 --> 00:11:52.590
the same units in this case yearly amount spent let's say dollar spent.

181
00:11:52.680 --> 00:11:54.920
We take the square root of that whole thing.

182
00:11:55.140 --> 00:12:01.080
So I can see that my root mean squared error is around 10 and a half dollars by itself that number is

183
00:12:01.110 --> 00:12:02.010
pretty meaningless.

184
00:12:02.040 --> 00:12:07.500
We have to do is compare it to the actual column value that actual label you're trying to predict in

185
00:12:07.500 --> 00:12:13.170
this case it's yearly amounts spent and the average yearly amount spent is $500 with a standard deviation

186
00:12:13.170 --> 00:12:17.620
of $80 minimum was 2 5 6 maximum was 7 6 5.

187
00:12:17.700 --> 00:12:23.730
So I can see here in comparison being off on that root mean squared error by 10 and a half dollars versus

188
00:12:23.730 --> 00:12:30.930
the actual average being $500 and the standard deviation being $80 is actually pretty darn good to confirm

189
00:12:30.930 --> 00:12:35.520
that I have a good fitting model I can also check out the R-squared value where the R-squared value

190
00:12:35.520 --> 00:12:40.630
is saying I am my model explains 98 percent of the variance in the data.

191
00:12:40.650 --> 00:12:42.510
That's also very good.

192
00:12:42.540 --> 00:12:47.460
Now having such a good root mean squared error and also having such a good R-squared value to kind of

193
00:12:47.460 --> 00:12:51.750
Ping's something in the back of your mind saying okay I should double check this data double check the

194
00:12:51.750 --> 00:12:56.290
way I fit in my model make sure that X accidentally fit your model and the training data.

195
00:12:56.310 --> 00:12:58.340
And then also evaluate your model and training data.

196
00:12:58.350 --> 00:13:00.030
That's kind of a common mistake.

197
00:13:00.060 --> 00:13:05.170
However you can rest assured here because I artificially created this data it was designed to have a

198
00:13:05.170 --> 00:13:10.920
very good fit by a linear regression model you can expect more advanced or more complex models to have

199
00:13:11.120 --> 00:13:15.390
a better fit but for something as simple as linear regression you should definitely double check if

200
00:13:15.390 --> 00:13:18.670
you get results this good on a real data set.

201
00:13:18.940 --> 00:13:19.340
OK.

202
00:13:19.350 --> 00:13:21.120
So that was revealing the results.

203
00:13:21.120 --> 00:13:26.070
Let me just quickly review how you would then essentially deploy this model on some data.

204
00:13:26.090 --> 00:13:28.380
You only had the features for.

205
00:13:28.450 --> 00:13:33.140
So let's say we have some customers where we only have their features but we don't know how much they

206
00:13:33.140 --> 00:13:36.220
are actually going to spend in a year to mimic that.

207
00:13:36.220 --> 00:13:42.860
What I'm going to do is say I have some unlabeled data and that's going to be equal to my test data.

208
00:13:43.850 --> 00:13:47.270
However I'm only going to select the features column from it.

209
00:13:47.270 --> 00:13:54.540
So if I look at my own label data I can see only has features it doesn't actually have the yearly amount

210
00:13:54.540 --> 00:13:55.070
spent.

211
00:13:55.080 --> 00:14:00.170
So we can treat this as some new incoming data that we want to deploy my model on.

212
00:14:00.180 --> 00:14:09.710
So scroll down here and I'm going to say grab my model and I will transform that unlabeled data and

213
00:14:09.710 --> 00:14:18.530
then set that equal to predictions and what predictions is as a reminder it is a data frame where it

214
00:14:18.530 --> 00:14:22.700
shows the incoming features and the predicted yearly amount spent.

215
00:14:22.700 --> 00:14:28.970
So for this first customer with this specific feature or this vector of features I predict in a year

216
00:14:29.030 --> 00:14:32.430
they will spend $330 for the second customer.

217
00:14:32.450 --> 00:14:38.090
I predict they'll spend this much etc. and if a check against test data which is exactly what we did

218
00:14:38.090 --> 00:14:44.120
up here when we said evaluate Kree-Skrull appear when we said evaluate what this ends up doing is it

219
00:14:44.240 --> 00:14:49.730
has that residual information that root mean squared error information et cetera all from comparing

220
00:14:50.060 --> 00:14:54.310
that prediction versus what it knew to be the true value.

221
00:14:54.490 --> 00:14:58.810
All right I hope you enjoy that discussion and I hope it gave you a good idea of how you can apply these

222
00:14:58.810 --> 00:15:01.380
techniques to a more realistic looking dataset.

223
00:15:01.390 --> 00:15:04.160
Coming up next is going to be a consulting project.

224
00:15:04.270 --> 00:15:08.800
So we'll start out by giving you a little bit of background on the project itself with the data and

225
00:15:08.800 --> 00:15:10.460
what the expected output is.

226
00:15:10.720 --> 00:15:12.330
Thanks and I'll see at the next lecture.
