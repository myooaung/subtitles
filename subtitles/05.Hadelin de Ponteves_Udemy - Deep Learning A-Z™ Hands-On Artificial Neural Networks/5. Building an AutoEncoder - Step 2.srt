1
1

00:00:00,260  -->  00:00:02,720
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,720  -->  00:00:05,270
So in the previous tutorial we imported the data set,
3

3

00:00:05,270  -->  00:00:07,540
and now we're going to prepare the training set
4

4

00:00:07,540  -->  00:00:08,740
and the test set.
5

5

00:00:08,740  -->  00:00:12,120
So the training set will be to train our encoders
6

6

00:00:12,120  -->  00:00:14,130
and the test set will be used to evaluate
7

7

00:00:14,130  -->  00:00:15,400
their performance.
8

8

00:00:15,400  -->  00:00:18,800
So let's do this, let's start with the training set.
9

9

00:00:18,800  -->  00:00:21,010
So we're first going to go to file explorer
10

10

00:00:21,010  -->  00:00:22,720
to see what we are going to take,
11

11

00:00:22,720  -->  00:00:24,730
and, as I said in the previous tutorial,
12

12

00:00:24,730  -->  00:00:28,130
we are going to take the 1000 ratings dataset
13

13

00:00:28,130  -->  00:00:30,650
which is in this folder, and in fact what we
14

14

00:00:30,650  -->  00:00:33,670
are seeing here in the ml-100k folder
15

15

00:00:33,670  -->  00:00:37,090
is five train-test splits of the whole dataset
16

16

00:00:37,090  -->  00:00:39,310
composed of 100 thousand ratings.
17

17

00:00:39,310  -->  00:00:41,790
So as you can see we have u1.base, u1.test,
18

18

00:00:41,790  -->  00:00:44,610
u2.base, u2.test, u3.base, u3.test,
19

19

00:00:44,610  -->  00:00:47,420
up to u5.base, u5.test, and so,
20

20

00:00:47,420  -->  00:00:49,450
each one of those pairs of sets
21

21

00:00:49,450  -->  00:00:51,160
are actually some separate training set
22

22

00:00:51,160  -->  00:00:54,110
and test set, so base means training set,
23

23

00:00:54,110  -->  00:00:56,060
and test means test set.
24

24

00:00:56,060  -->  00:00:59,590
So u1.base and u1.test is one train-test split
25

25

00:00:59,590  -->  00:01:00,970
of the whole dataset composed
26

26

00:01:00,970  -->  00:01:02,580
of one hundred thousand ratings,
27

27

00:01:02,580  -->  00:01:05,780
and the other pairs are other train-test splits.
28

28

00:01:05,780  -->  00:01:09,000
And so what's the use of having several train-test splits?
29

29

00:01:09,000  -->  00:01:11,540
Well, that's to perform careful cross-validations
30

30

00:01:11,540  -->  00:01:14,240
manually afterwards, you know, to be able to train
31

31

00:01:14,240  -->  00:01:16,160
our model on several training sets
32

32

00:01:16,160  -->  00:01:18,050
and test it on several test sets
33

33

00:01:18,050  -->  00:01:20,200
exactly as we did when we performed careful
34

34

00:01:20,200  -->  00:01:22,670
cross-validation with ten-fold, since indeed
35

35

00:01:22,670  -->  00:01:24,130
we were training our neural network
36

36

00:01:24,130  -->  00:01:25,824
on ten training folds, and we were testing it
37

37

00:01:25,824  -->  00:01:28,400
on ten associated test folds.
38

38

00:01:28,400  -->  00:01:31,470
And here, instead of having ten train-test folds,
39

39

00:01:31,470  -->  00:01:33,720
we actually have five train-test folds
40

40

00:01:33,720  -->  00:01:36,800
and therefore that's to do five-fold cross-validation.
41

41

00:01:36,800  -->  00:01:39,430
But we're not going to perform five-fold cross-validation
42

42

00:01:39,430  -->  00:01:42,030
in this section, because I like to focus on
43

43

00:01:42,030  -->  00:01:44,220
the auto-encoders themselves, and therefore
44

44

00:01:44,220  -->  00:01:46,461
we will only take one train-test split
45

45

00:01:46,461  -->  00:01:48,280
and we will take the first one
46

46

00:01:48,280  -->  00:01:51,090
composed of u1.base, that is the training set,
47

47

00:01:51,090  -->  00:01:53,220
and u1.test, that is the test set.
48

48

00:01:53,220  -->  00:01:55,740
So let's import these two sets.
49

49

00:01:55,740  -->  00:01:57,750
All right, so we're going to start with the training set
50

50

00:01:57,750  -->  00:02:00,520
which is u1.base, and therefore we're going to create
51

51

00:02:00,520  -->  00:02:02,307
a new variable training_set.
52

52

00:02:03,650  -->  00:02:05,610
Here we go, and now we're going to use
53

53

00:02:05,610  -->  00:02:07,970
the pandas library, still, because we want
54

54

00:02:07,970  -->  00:02:11,130
to import u1.base, but then we will convert
55

55

00:02:11,130  -->  00:02:13,120
this training set into an array because
56

56

00:02:13,120  -->  00:02:16,090
by importing u1.base with pandas we will get
57

57

00:02:16,090  -->  00:02:17,980
a data frame, and remember with Python
58

58

00:02:17,980  -->  00:02:19,610
we would rather work with arrays.
59

59

00:02:19,610  -->  00:02:22,420
So let's first import it with pandas as a data frame,
60

60

00:02:22,420  -->  00:02:24,040
and then let's convert it.
61

61

00:02:24,040  -->  00:02:27,060
So, the same, we're gonna use the shortcut pd
62

62

00:02:27,060  -->  00:02:31,110
and dot, then the read_csv function,
63

63

00:02:31,110  -->  00:02:33,860
then parentheses, and for the first argument
64

64

00:02:33,860  -->  00:02:36,470
we need to input the path that will take
65

65

00:02:36,470  -->  00:02:39,920
u1.base in this ml-100k folder.
66

66

00:02:39,920  -->  00:02:40,810
So let's do it.
67

67

00:02:40,810  -->  00:02:44,020
We have to put the path in quotes, like before,
68

68

00:02:44,020  -->  00:02:47,160
and so we start with the folder that contains
69

69

00:02:47,160  -->  00:02:49,050
u1.base, that is the training set,
70

70

00:02:49,050  -->  00:02:53,480
and this folder is ml-100k, so we start
71

71

00:02:53,480  -->  00:02:58,480
by typing ml-100k, then /, and then simply
72

72

00:02:59,360  -->  00:03:03,570
the name of the training set which is u1.base.
73

73

00:03:03,570  -->  00:03:06,460
So u1, dot, base.
74

74

00:03:06,460  -->  00:03:08,980
And here we go, that should be okay to import
75

75

00:03:08,980  -->  00:03:11,620
the training set, however, I pre-checked
76

76

00:03:11,620  -->  00:03:14,619
the separator for this u1.base file
77

77

00:03:14,619  -->  00:03:18,470
and this time it happens to be not a double colon
78

78

00:03:18,470  -->  00:03:21,460
but a tab, so you can check it out by opening,
79

79

00:03:21,460  -->  00:03:23,820
for example, u1.base on a text editor
80

80

00:03:23,820  -->  00:03:25,970
and you'll see that the separator is a tab,
81

81

00:03:25,970  -->  00:03:28,140
and therefore you will need to specify it
82

82

00:03:28,140  -->  00:03:30,300
otherwise it will think the separator is a comma
83

83

00:03:30,300  -->  00:03:32,730
because the default separator is a comma.
84

84

00:03:32,730  -->  00:03:34,180
So we're gonna add the argument
85

85

00:03:34,180  -->  00:03:38,492
delimiter equals tab, and to specify tab
86

86

00:03:38,492  -->  00:03:42,900
we need to input \t, like this.
87

87

00:03:42,900  -->  00:03:44,760
And by the way, the delimiter, tab,
88

88

00:03:44,760  -->  00:03:47,920
should rather be taken with this delimiter argument
89

89

00:03:47,920  -->  00:03:50,120
rather than this set argument.
90

90

00:03:50,120  -->  00:03:52,510
So it's good to know that, and, actually,
91

91

00:03:52,510  -->  00:03:56,060
now I think we're ready to import the training set.
92

92

00:03:56,060  -->  00:03:57,160
So let's check it out.
93

93

00:03:57,160  -->  00:04:00,930
I'm going to select this line and execute.
94

94

00:04:00,930  -->  00:04:04,140
Here we go, yes, the training set is well imported.
95

95

00:04:04,140  -->  00:04:06,910
Perfect, let's have a look in the variable explorer,
96

96

00:04:06,910  -->  00:04:07,850
and here it is.
97

97

00:04:07,850  -->  00:04:10,060
So, first of all, what is the split?
98

98

00:04:10,060  -->  00:04:12,360
I mean, what is the proportion of the training set
99

99

00:04:12,360  -->  00:04:14,080
compared to the whole set?
100

100

00:04:14,080  -->  00:04:16,200
Well, we can already see that.
101

101

00:04:16,200  -->  00:04:20,510
Remember, the whole original data set in this ml-100k folder
102

102

00:04:20,510  -->  00:04:22,700
contains one hundred thousand ratings,
103

103

00:04:22,700  -->  00:04:26,670
and since each observation corresponds to one rating...
104

104

00:04:26,670  -->  00:04:28,170
Well, since we can see here that we
105

105

00:04:28,170  -->  00:04:30,510
have 80 thousand observations that means
106

106

00:04:30,510  -->  00:04:32,660
that we have 80 thousand ratings,
107

107

00:04:32,660  -->  00:04:35,780
and therefore the training set is 80%
108

108

00:04:35,780  -->  00:04:37,550
of the original dataset composed of
109

109

00:04:37,550  -->  00:04:39,310
the one hundred thousand ratings,
110

110

00:04:39,310  -->  00:04:42,140
so that will be an 80% 20% train-test split,
111

111

00:04:42,140  -->  00:04:44,400
and I think that's the optimal train-test split
112

112

00:04:44,400  -->  00:04:46,110
to train a model.
113

113

00:04:46,110  -->  00:04:48,800
Okay, so now let's open the training set to see
114

114

00:04:48,800  -->  00:04:50,050
what it looks like.
115

115

00:04:50,050  -->  00:04:51,960
So, remember that's exactly the same
116

116

00:04:51,960  -->  00:04:54,890
as in this ratings dataset that we imported
117

117

00:04:54,890  -->  00:04:56,340
in the previous tutorial.
118

118

00:04:56,340  -->  00:04:58,840
The first column corresponds to users,
119

119

00:04:58,840  -->  00:05:01,130
the second column corresponds to the movies,
120

120

00:05:01,130  -->  00:05:04,060
and the third column corresponds to the ratings,
121

121

00:05:04,060  -->  00:05:06,720
and then the fourth column corresponds to the timestamps,
122

122

00:05:06,720  -->  00:05:08,270
but that, as we said, we don't care,
123

123

00:05:08,270  -->  00:05:11,250
because this won't be relevant to train the model.
124

124

00:05:11,250  -->  00:05:13,880
And so here, for example, at the fourth index,
125

125

00:05:13,880  -->  00:05:17,070
that is the fifth observation because it starts at 0,
126

126

00:05:17,070  -->  00:05:19,141
well, that corresponds to the user number one
127

127

00:05:19,141  -->  00:05:23,870
that rated the movie number seven and gave it four stars.
128

128

00:05:23,870  -->  00:05:26,020
Then, if we take another example...
129

129

00:05:26,020  -->  00:05:29,640
Well, at the 261st observation,
130

130

00:05:29,640  -->  00:05:31,910
that corresponds to the user number five
131

131

00:05:31,910  -->  00:05:36,646
that rated the movie number 373 and gave it three stars.
132

132

00:05:36,646  -->  00:05:39,280
All right, and so, now what's gonna happen
133

133

00:05:39,280  -->  00:05:41,500
is that we will train our auto-encoders
134

134

00:05:41,500  -->  00:05:43,005
on this training set, and so it will try
135

135

00:05:43,005  -->  00:05:46,250
to identify some patterns to find some groups
136

136

00:05:46,250  -->  00:05:49,600
of movies that are liked by similar segments of users,
137

137

00:05:49,600  -->  00:05:51,070
and so it will find these patterns,
138

138

00:05:51,070  -->  00:05:53,910
it will find some specific features of the movies
139

139

00:05:53,910  -->  00:05:56,790
which will be the hidden nodes in the auto-encoders,
140

140

00:05:56,790  -->  00:05:58,980
and these specific features can be some genders,
141

141

00:05:58,980  -->  00:06:02,650
some actors that are in the movies, or some directors.
142

142

00:06:02,650  -->  00:06:05,940
For example, one hidden node can be a specific director,
143

143

00:06:05,940  -->  00:06:08,780
like Tarantino, for example, or it can identify
144

144

00:06:08,780  -->  00:06:12,040
some movies with a great actor that is liked
145

145

00:06:12,040  -->  00:06:14,270
by the same group of people because they gave
146

146

00:06:14,270  -->  00:06:16,060
the same high rating.
147

147

00:06:16,060  -->  00:06:18,050
So basically it will identify some patterns,
148

148

00:06:18,050  -->  00:06:19,528
it will identify some features,
149

149

00:06:19,528  -->  00:06:21,780
based on which, in the future,
150

150

00:06:21,780  -->  00:06:24,430
the model will be able to predict the rating
151

151

00:06:24,430  -->  00:06:27,520
of a movie that one user hasn't seen yet,
152

152

00:06:27,520  -->  00:06:29,550
and it will be able to predict that rating
153

153

00:06:29,550  -->  00:06:32,410
based on the features that the auto-encoder's
154

154

00:06:32,410  -->  00:06:35,750
detected, and based on the history of this user.
155

155

00:06:35,750  -->  00:06:37,610
That is, you know the auto-encoders will take
156

156

00:06:37,610  -->  00:06:39,480
the features that it detected,
157

157

00:06:39,480  -->  00:06:42,760
and it will take also the ratings of that one same user
158

158

00:06:42,760  -->  00:06:45,410
to predict the rating of the new movie
159

159

00:06:45,410  -->  00:06:47,060
that the same user hasn't seen,
160

160

00:06:47,060  -->  00:06:49,120
and therefore has not rated yet.
161

161

00:06:49,120  -->  00:06:51,130
So that's how it will work, and you will see
162

162

00:06:51,130  -->  00:06:54,010
how the auto-encoders will make a brilliant job
163

163

00:06:54,010  -->  00:06:55,810
at identifying these patterns,
164

164

00:06:55,810  -->  00:06:58,300
identifying these features, and eventually
165

165

00:06:58,300  -->  00:06:59,737
predict the ratings.
166

166

00:06:59,737  -->  00:07:02,020
Okay, so that is the training set,
167

167

00:07:02,020  -->  00:07:04,580
it is so far imported as a data frame,
168

168

00:07:04,580  -->  00:07:06,580
and therefore, as I just told you,
169

169

00:07:06,580  -->  00:07:08,480
we have to convert it into an array
170

170

00:07:08,480  -->  00:07:09,792
because then you will see that you will use
171

171

00:07:09,792  -->  00:07:12,770
Torch sensors, and for this we need arrays
172

172

00:07:12,770  -->  00:07:14,420
instead of data frames.
173

173

00:07:14,420  -->  00:07:17,270
So, let's quickly convert this training set
174

174

00:07:17,270  -->  00:07:19,240
into an array, that's very simple.
175

175

00:07:19,240  -->  00:07:22,920
We need to take our training set variable again
176

176

00:07:24,070  -->  00:07:26,240
and give it a new value, and so now,
177

177

00:07:26,240  -->  00:07:27,640
basically what we're going to do is use
178

178

00:07:27,640  -->  00:07:30,450
the function that will convert a data frame
179

179

00:07:30,450  -->  00:07:32,700
into an array, and this function is, of course,
180

180

00:07:32,700  -->  00:07:35,834
a non-bide function that is called array.
181

181

00:07:35,834  -->  00:07:39,670
We take our shortcut np, because we're taking
182

182

00:07:39,670  -->  00:07:41,760
this function from non-bide, and now we take
183

183

00:07:41,760  -->  00:07:45,740
this function array and parentheses.
184

184

00:07:45,740  -->  00:07:47,840
So that's the best way of converting a data frame
185

185

00:07:47,840  -->  00:07:48,870
into an array.
186

186

00:07:48,870  -->  00:07:51,110
Now, inside this function we have to input
187

187

00:07:51,110  -->  00:07:54,560
our training set but the old one, this one,
188

188

00:07:54,560  -->  00:07:57,267
that is a data frame, so training_set.
189

189

00:07:58,180  -->  00:07:59,970
And, as a second argument we will need
190

190

00:07:59,970  -->  00:08:02,500
to specify the type of this new array
191

191

00:08:02,500  -->  00:08:05,070
that we are creating, and since we only have
192

192

00:08:05,070  -->  00:08:09,040
user IDs, movie IDs, and ratings that are all integers,
193

193

00:08:09,040  -->  00:08:10,920
then we will convert this whole array
194

194

00:08:10,920  -->  00:08:13,380
into an array of integers, and to do this,
195

195

00:08:13,380  -->  00:08:15,900
we need to use this argument here, that you see,
196

196

00:08:15,900  -->  00:08:19,290
dtype, and that will specify the type of the array.
197

197

00:08:19,290  -->  00:08:23,710
And so we'll add dtype equals, and then in quotes,
198

198

00:08:23,710  -->  00:08:28,210
we will specify the type which is int for integers.
199

199

00:08:28,210  -->  00:08:29,043
Perfect.
200

200

00:08:29,043  -->  00:08:32,240
So, as you can see, if I select this line,
201

201

00:08:32,240  -->  00:08:33,800
now we can see that that the training set
202

202

00:08:33,800  -->  00:08:36,560
is a data frame of this size,
203

203

00:08:36,560  -->  00:08:38,050
and if I execute this...
204

204

00:08:38,050  -->  00:08:40,130
The training set is now an array,
205

205

00:08:40,130  -->  00:08:43,470
that means array actually of integers, 64,
206

206

00:08:43,470  -->  00:08:44,890
and of the same size.
207

207

00:08:44,890  -->  00:08:47,830
So perfect, and if we open it, well we get
208

208

00:08:47,830  -->  00:08:51,053
the same values, but this time into an array.
209

209

00:08:51,950  -->  00:08:55,270
Okay, so now we're going to do the same for the test set,
210

210

00:08:55,270  -->  00:08:57,090
we're going to prepare the test set,
211

211

00:08:57,090  -->  00:08:58,860
and actually this is going to be very easy
212

212

00:08:58,860  -->  00:09:00,970
because we will use the same techniques
213

213

00:09:00,970  -->  00:09:03,750
to import and convert our test set
214

214

00:09:03,750  -->  00:09:06,770
into an array, so I'm pasting that here.
215

215

00:09:06,770  -->  00:09:11,080
I'm going to replace training set by test set,
216

216

00:09:11,080  -->  00:09:14,920
same here, training set, test set.
217

217

00:09:14,920  -->  00:09:17,910
And here as well, and of course, after that
218

218

00:09:17,910  -->  00:09:22,327
we'll need to replace this u1.base by u1.test
219

219

00:09:23,240  -->  00:09:25,610
because we're taking out the test set
220

220

00:09:25,610  -->  00:09:28,440
which is u1.test.
221

221

00:09:28,440  -->  00:09:31,950
Here we go, perfect, and that will take the test set.
222

222

00:09:31,950  -->  00:09:34,360
The delimiter is also a tab, so basically,
223

223

00:09:34,360  -->  00:09:37,560
we're already ready to import the test set.
224

224

00:09:37,560  -->  00:09:39,110
So let's do it step by step.
225

225

00:09:39,110  -->  00:09:40,850
I'm going to select this line first,
226

226

00:09:40,850  -->  00:09:43,730
and, execute, here we go, test set imported,
227

227

00:09:43,730  -->  00:09:45,110
we can have a quick look.
228

228

00:09:45,110  -->  00:09:48,210
So that's the test set, and same, this is exactly
229

229

00:09:48,210  -->  00:09:49,500
the same structure.
230

230

00:09:49,500  -->  00:09:51,529
We have the users in the first column,
231

231

00:09:51,529  -->  00:09:53,700
then the movies in the second column,
232

232

00:09:53,700  -->  00:09:56,150
and the ratings in the third column.
233

233

00:09:56,150  -->  00:09:57,900
And so what we have to understand is that
234

234

00:09:57,900  -->  00:09:59,737
the training set and the test set
235

235

00:09:59,737  -->  00:10:01,490
have different ratings.
236

236

00:10:01,490  -->  00:10:03,120
You know, there is no common rating
237

237

00:10:03,120  -->  00:10:05,280
of the same movie by the same user
238

238

00:10:05,280  -->  00:10:07,180
between the training sets and the test set,
239

239

00:10:07,180  -->  00:10:09,830
however, we have the same users, here indeed
240

240

00:10:09,830  -->  00:10:12,230
we start with user one, as in the training set.
241

241

00:10:12,230  -->  00:10:16,390
But, for this same user one, we won't have the same movies
242

242

00:10:16,390  -->  00:10:18,360
because the ratings are different.
243

243

00:10:18,360  -->  00:10:20,690
So here, we have the answers in this third column
244

244

00:10:20,690  -->  00:10:22,980
that contains the ratings, but it's just to get
245

245

00:10:22,980  -->  00:10:25,280
the real results on one's side, and so what
246

246

00:10:25,280  -->  00:10:27,720
will happen is that we will make our predictions
247

247

00:10:27,720  -->  00:10:31,232
using our auto-encoder's model to predict
248

248

00:10:31,232  -->  00:10:33,960
the rating, for example, of movie 14 by user one.
249

249

00:10:33,960  -->  00:10:35,940
This will give a prediction here,
250

250

00:10:35,940  -->  00:10:38,460
so it will give, for example, four, and then,
251

251

00:10:38,460  -->  00:10:42,040
we will measure the error between the real rating, five,
252

252

00:10:42,040  -->  00:10:45,130
and the prediction, four, and that will allow us
253

253

00:10:45,130  -->  00:10:48,050
to measure the MSE that is the Mean Squared Error.
254

254

00:10:48,050  -->  00:10:49,820
So, we're just keeping this column
255

255

00:10:49,820  -->  00:10:52,200
to measure the MSE in the end, in order
256

256

00:10:52,200  -->  00:10:54,710
to evaluate our test set performance,
257

257

00:10:54,710  -->  00:10:56,640
and I will tell soon where the medals are,
258

258

00:10:56,640  -->  00:10:58,570
for the challenge of this section.
259

259

00:10:58,570  -->  00:11:01,420
Okay, so, perfect, that's the test set,
260

260

00:11:01,420  -->  00:11:03,470
but right now it is as a data frame,
261

261

00:11:03,470  -->  00:11:05,540
and we need to convert it into an array,
262

262

00:11:05,540  -->  00:11:07,390
so that's what we're gonna do now.
263

263

00:11:07,390  -->  00:11:09,280
And actually we don't have anything to do,
264

264

00:11:09,280  -->  00:11:12,056
the line of code is ready, so we just have to select it
265

265

00:11:12,056  -->  00:11:15,330
and press command or control plus enter
266

266

00:11:15,330  -->  00:11:18,520
to execute, and here we go, now the test set
267

267

00:11:18,520  -->  00:11:23,030
is an array of integers 64 of 20 thousand ratings
268

268

00:11:23,030  -->  00:11:26,270
that corresponds to 20% of the original dataset
269

269

00:11:26,270  -->  00:11:29,070
composed of the one hundred thousand ratings.
270

270

00:11:29,070  -->  00:11:31,330
Perfect, so that's done for this tutorial,
271

271

00:11:31,330  -->  00:11:33,410
we prepared correctly the training set,
272

272

00:11:33,410  -->  00:11:35,940
and the test set, and now in the next tutorial
273

273

00:11:35,940  -->  00:11:38,677
we will get the maximum number of users
274

274

00:11:38,677  -->  00:11:42,080
and the maximum number of movies in two separate variables
275

275

00:11:42,080  -->  00:11:43,860
because then we will need these two variables
276

276

00:11:43,860  -->  00:11:46,070
to prepare our auto-encoders.
277

277

00:11:46,070  -->  00:11:47,720
So we'll do that in the next tutorial,
278

278

00:11:47,720  -->  00:11:49,553
and until then, enjoy deep learning!
