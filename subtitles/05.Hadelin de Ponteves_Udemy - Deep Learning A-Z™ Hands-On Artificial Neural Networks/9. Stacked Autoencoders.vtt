WEBVTT
1
1

00:00:00.380  -->  00:00:02.530
<v ->Hello, and welcome back to the course on Deep Learning.</v>
2

2

00:00:02.530  -->  00:00:05.290
Today we're talking about stacked autoencoders.
3

3

00:00:05.290  -->  00:00:08.750
All right. So here is a normal autoencoder.
4

4

00:00:08.750  -->  00:00:10.660
So what is a stacked autoencoder?
5

5

00:00:10.660  -->  00:00:12.670
Well, a stacked autoencoder is
6

6

00:00:12.670  -->  00:00:17.230
if we add another hidden layer into our autoencoder,
7

7

00:00:17.230  -->  00:00:18.200
and there we go.
8

8

00:00:18.200  -->  00:00:20.790
So we have two stages of encoding
9

9

00:00:20.790  -->  00:00:23.130
and one stage of decoding here.
10

10

00:00:23.130  -->  00:00:27.920
And what we get is a very, very powerful algorithm.
11

11

00:00:27.920  -->  00:00:31.920
In fact it's been shown that sometimes
12

12

00:00:31.920  -->  00:00:36.300
this model can supersede
13

13

00:00:36.300  -->  00:00:39.110
the results that are achieved
14

14

00:00:39.110  -->  00:00:40.630
by deep belief networks.
15

15

00:00:40.630  -->  00:00:43.220
So DBNs which we talked about previously,
16

16

00:00:43.220  -->  00:00:45.190
and that's a very important breakthrough
17

17

00:00:45.190  -->  00:00:50.190
because DBNs are a whole different type of model.
18

18

00:00:51.710  -->  00:00:56.710
They're based on stacked restricted boltzmann machines,
19

19

00:00:57.200  -->  00:00:59.200
and restricted boltzmann machines
20

20

00:00:59.200  -->  00:01:01.760
are undirected neural networks
21

21

00:01:01.760  -->  00:01:04.720
whereas stacked autoencoders are directed neural networks.
22

22

00:01:04.720  -->  00:01:09.280
So it's very big breakthrough that stacked autoencoders
23

23

00:01:09.280  -->  00:01:13.790
can sometime supersede the results of deep believe networks,
24

24

00:01:13.790  -->  00:01:17.410
and a great paper to look into on this topic is
25

25

00:01:17.410  -->  00:01:19.650
by Pascal Vincent and others is called
26

26

00:01:19.650  -->  00:01:21.480
Stacked Denoising Autoencoders:
27

27

00:01:21.480  -->  00:01:23.940
Learning Useful Representations in a Deep Network
28

28

00:01:23.940  -->  00:01:26.807
with a Local Denoising Criterion.
29

29

00:01:26.807  -->  00:01:28.720
It's quite a large paper.
30

30

00:01:28.720  -->  00:01:31.250
Also has Bengio in it.
31

31

00:01:31.250  -->  00:01:33.880
So Yoshua Bengio was one of the co-authors
32

32

00:01:33.880  -->  00:01:35.690
on this paper as well,
33

33

00:01:35.690  -->  00:01:39.080
and it builds upon their 2008 paper
34

34

00:01:39.080  -->  00:01:41.950
which we have already referenced before.
35

35

00:01:41.950  -->  00:01:44.690
So there you go. I hope you enjoyed today's tutorial,
36

36

00:01:44.690  -->  00:01:45.640
and if you would like
37

37

00:01:45.640  -->  00:01:47.330
to learn more about stacked autoencoders
38

38

00:01:47.330  -->  00:01:50.550
then definitely this paper is a great place to start.
39

39

00:01:50.550  -->  00:01:52.110
And I will look forward to seeing you next time.
40

40

00:01:52.110  -->  00:01:53.893
Until then, enjoy deep learning.
