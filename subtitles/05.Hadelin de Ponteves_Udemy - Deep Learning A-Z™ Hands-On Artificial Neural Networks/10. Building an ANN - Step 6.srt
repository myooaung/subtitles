1
1

00:00:00,140  -->  00:00:02,600
<v ->Hello, and welcome to this Python tutorial.</v>
2

2

00:00:02,600  -->  00:00:05,930
So, in the previous tutorial we created our first layers
3

3

00:00:05,930  -->  00:00:07,250
of our neural network.
4

4

00:00:07,250  -->  00:00:10,290
This is the input layer and the first hidden layer
5

5

00:00:10,290  -->  00:00:12,400
and now in this tutorial we're gonna add
6

6

00:00:12,400  -->  00:00:14,720
the second hidden layer.
7

7

00:00:14,720  -->  00:00:17,080
All right, so this is going to be a very quick tutorial
8

8

00:00:17,080  -->  00:00:20,010
because what we'll basically do is just
9

9

00:00:20,010  -->  00:00:22,360
to copy this line.
10

10

00:00:22,360  -->  00:00:24,850
All right, copy and paste it here
11

11

00:00:24,850  -->  00:00:27,840
because we will use of course the same method
12

12

00:00:27,840  -->  00:00:29,270
which is this add method
13

13

00:00:29,270  -->  00:00:31,400
and the same parameter inside this method
14

14

00:00:31,400  -->  00:00:33,640
which is the dense function
15

15

00:00:33,640  -->  00:00:36,770
with the same parameters except one, which,
16

16

00:00:36,770  -->  00:00:37,870
as you might've guessed,
17

17

00:00:37,870  -->  00:00:39,950
is the input_dim parameter.
18

18

00:00:39,950  -->  00:00:40,783
And why is that?
19

19

00:00:40,783  -->  00:00:43,424
It's because, as I explained in the previous tutorial,
20

20

00:00:43,424  -->  00:00:46,205
we need to specify this input_dim parameter
21

21

00:00:46,205  -->  00:00:47,740
for the first hidden layer
22

22

00:00:47,740  -->  00:00:50,710
because no layer was created yet and therefore
23

23

00:00:50,710  -->  00:00:53,390
this hidden layer here didn't know what to expect
24

24

00:00:53,390  -->  00:00:54,900
as input nodes.
25

25

00:00:54,900  -->  00:00:56,240
So that's why we had to specify
26

26

00:00:56,240  -->  00:00:58,856
that it should expect 11 input nodes
27

27

00:00:58,856  -->  00:01:01,610
and these 11 input nodes are nothing else
28

28

00:01:01,610  -->  00:01:03,820
than our 11 independent variables.
29

29

00:01:03,820  -->  00:01:05,650
But here for the second hidden layer,
30

30

00:01:05,650  -->  00:01:07,060
well it knows what to expect
31

31

00:01:07,060  -->  00:01:09,940
because the first hidden layer was created
32

32

00:01:09,940  -->  00:01:13,810
so we don't need to specify any input_dim parameter here
33

33

00:01:13,810  -->  00:01:16,260
so we can remove it.
34

34

00:01:16,260  -->  00:01:17,093
All right.
35

35

00:01:17,093  -->  00:01:18,400
And then what do we have left?
36

36

00:01:18,400  -->  00:01:21,050
We have our output_dim parameter
37

37

00:01:21,050  -->  00:01:23,670
and the tip that I gave you in the previous tutorial
38

38

00:01:23,670  -->  00:01:26,240
to choose the number of nodes in the hidden layer
39

39

00:01:26,240  -->  00:01:28,850
if we don't wanna be an artist still applies
40

40

00:01:28,850  -->  00:01:30,150
for this second hidden layer.
41

41

00:01:30,150  -->  00:01:31,860
So we will keep this average
42

42

00:01:31,860  -->  00:01:34,290
of the number of nodes in the input layer
43

43

00:01:34,290  -->  00:01:36,400
and the number of nodes in the output layer.
44

44

00:01:36,400  -->  00:01:39,890
That is one plus 11 divided by two, that is six.
45

45

00:01:39,890  -->  00:01:41,380
So we keep six here.
46

46

00:01:41,380  -->  00:01:43,410
Same we need to initialize the weights
47

47

00:01:43,410  -->  00:01:44,940
for this second hidden layer
48

48

00:01:44,940  -->  00:01:46,432
and we will keep this uniform method
49

49

00:01:46,432  -->  00:01:49,410
that initializes the weights randomly
50

50

00:01:49,410  -->  00:01:50,680
and makes sure that the weights
51

51

00:01:50,680  -->  00:01:52,940
are given small numbers close to zero
52

52

00:01:52,940  -->  00:01:56,040
and eventually, we have this activation parameter
53

53

00:01:56,040  -->  00:01:58,080
and remember in the previous tutorial I explained
54

54

00:01:58,080  -->  00:02:00,580 line:15% 
that the activation function is recommended
55

55

00:02:00,580  -->  00:02:03,490 line:15% 
to be the rectifier function for the hidden layers
56

56

00:02:03,490  -->  00:02:06,380 line:15% 
and the sigmoid function for the output layer.
57

57

00:02:06,380  -->  00:02:09,070
So here since we're dealing with the second hidden layer,
58

58

00:02:09,070  -->  00:02:12,240
we are still choosing to rectify our activation function.
59

59

00:02:12,240  -->  00:02:13,100
So that's all.
60

60

00:02:13,100  -->  00:02:15,350
We are ready to add the second hidden layer
61

61

00:02:15,350  -->  00:02:19,147
so I'm going to select this line and execute.
62

62

00:02:19,147  -->  00:02:20,920
All right, perfect.
63

63

00:02:20,920  -->  00:02:23,580
We successfully added the second hidden layer.
64

64

00:02:23,580  -->  00:02:25,670
So great, that's done and we are ready
65

65

00:02:25,670  -->  00:02:26,990
to move on to the next step.
66

66

00:02:26,990  -->  00:02:28,640
And what is the next step gonna be?
67

67

00:02:28,640  -->  00:02:30,000
Well, as you might've guessed,
68

68

00:02:30,000  -->  00:02:32,610
since you know we are only adding two hidden layers
69

69

00:02:32,610  -->  00:02:34,450
in our artificial neural network,
70

70

00:02:34,450  -->  00:02:37,580
well the next step will be about adding the final layer,
71

71

00:02:37,580  -->  00:02:40,130
that is, the output layer.
72

72

00:02:40,130  -->  00:02:41,940
So we'll do that in the next tutorial
73

73

00:02:41,940  -->  00:02:44,040
and until then, enjoy enjoy deep learning.
