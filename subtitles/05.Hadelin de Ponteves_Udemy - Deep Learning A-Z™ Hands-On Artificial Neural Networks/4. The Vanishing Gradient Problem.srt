1
1

00:00:00,450  -->  00:00:01,370
<v Instructor>Hello and welcome back</v>
2

2

00:00:01,370  -->  00:00:03,190
to the course on deep learning.
3

3

00:00:03,190  -->  00:00:05,550
I hope you enjoyed the introduction into RNNs
4

4

00:00:05,550  -->  00:00:08,700
and today right away off the bat we're going to jump
5

5

00:00:08,700  -->  00:00:12,510
into a huge problem that exists with RNNs.
6

6

00:00:12,510  -->  00:00:14,720
All right, so, what is this problem
7

7

00:00:14,720  -->  00:00:17,270
called the vanishing gradient?
8

8

00:00:17,270  -->  00:00:22,020
It was first discovered by Sepp Hochreiter
9

9

00:00:22,020  -->  00:00:24,970
and I hope I'm pronouncing that right.
10

10

00:00:24,970  -->  00:00:26,430
I know we have students from Germany,
11

11

00:00:26,430  -->  00:00:29,960
so, you can comment if the pronunciation is incorrect,
12

12

00:00:29,960  -->  00:00:34,960
but he, Sepp or Josef is his full, is a genius scientist
13

13

00:00:37,160  -->  00:00:41,453
who back in the '90s while he was still,
14

14

00:00:42,850  -->  00:00:44,270
you know, not a professor.
15

15

00:00:44,270  -->  00:00:46,730
This is recent photo, so he was much younger
16

16

00:00:46,730  -->  00:00:48,650
when he actually came up with this concept.
17

17

00:00:48,650  -->  00:00:51,160
He found that there was a big, big problem,
18

18

00:00:51,160  -->  00:00:53,923
and we'll talk about the problem itself just now.
19

19

00:00:53,923  -->  00:00:57,010
I just wanted to point out who are the people
20

20

00:00:57,010  -->  00:01:00,020
that spotted this, and basically as you'll learn
21

21

00:01:00,020  -->  00:01:01,720
from the further tutorials Sepp is one
22

22

00:01:01,720  -->  00:01:06,420
of the founding people in the modern
23

23

00:01:06,420  -->  00:01:09,160
way that we use RNNs and LSTMs
24

24

00:01:09,160  -->  00:01:10,330
and we'll talk about that further down,
25

25

00:01:10,330  -->  00:01:11,900
but this is Sepp Hochreiter.
26

26

00:01:11,900  -->  00:01:14,020
So, I just wanted to make sure you know
27

27

00:01:14,020  -->  00:01:16,170
who is behind all this.
28

28

00:01:16,170  -->  00:01:19,110
And the second person is Yoshua Bengio.
29

29

00:01:19,110  -->  00:01:23,323
He's a professor at the University of Montreal.
30

30

00:01:24,240  -->  00:01:26,390
He also discovered that this, this problem,
31

31

00:01:27,350  -->  00:01:29,360
I think a bit later in 1994.
32

32

00:01:29,360  -->  00:01:32,737
So, Sepp was, discovered this in 1991.
33

33

00:01:32,737  -->  00:01:35,950
Yoshua wrote about this in 1994, but again,
34

34

00:01:35,950  -->  00:01:38,213
this is another person driving this whole,
35

35

00:01:39,570  -->  00:01:42,160
pushing the envelope in the space of RNNs
36

36

00:01:42,160  -->  00:01:44,417
and if you go to the University of Montreal
37

37

00:01:44,417  -->  00:01:48,905
and you look up Yoshua's profile you will find
38

38

00:01:48,905  -->  00:01:53,905
that Yoshua has over 500 research papers,
39

39

00:01:54,780  -->  00:01:57,020
and by the way there's Yoshua Bengio
40

40

00:01:57,020  -->  00:01:58,800
just hanging out with (mumbles).
41

41

00:01:58,800  -->  00:02:00,270
As you can see, they all know each other.
42

42

00:02:00,270  -->  00:02:01,640
It's a very tight knit community,
43

43

00:02:01,640  -->  00:02:04,280
and it does sound like a conspiracy,
44

44

00:02:04,280  -->  00:02:06,950
these people that are driving or pushing the envelope
45

45

00:02:06,950  -->  00:02:08,760
in terms of deep learning.
46

46

00:02:08,760  -->  00:02:10,990
These are just a select group of people
47

47

00:02:10,990  -->  00:02:13,440
who are all, all in it together.
48

48

00:02:13,440  -->  00:02:14,420
They all know what's going on.
49

49

00:02:14,420  -->  00:02:17,070
They've all been doing it since the '80s or '90s
50

50

00:02:17,070  -->  00:02:19,260
and now it's all popping out into the world.
51

51

00:02:19,260  -->  00:02:20,093
So, there you go.
52

52

00:02:20,093  -->  00:02:21,130
That's just to give you a quick idea
53

53

00:02:21,130  -->  00:02:24,220
of who's behind all of this, and today,
54

54

00:02:24,220  -->  00:02:26,450
and now we're proceeding to the vanishing
55

55

00:02:26,450  -->  00:02:28,030
gradient problem itself.
56

56

00:02:28,030  -->  00:02:31,180
So, as you remember, this is the gradient descent algorithm.
57

57

00:02:31,180  -->  00:02:34,210
We're trying to find the global minimum
58

58

00:02:34,210  -->  00:02:36,290
of your cost function, and that's gonna be
59

59

00:02:36,290  -->  00:02:38,570
the optimal solution, optimal setup
60

60

00:02:38,570  -->  00:02:40,502
for your neural network.
61

61

00:02:40,502  -->  00:02:44,430
As you also recall, your gradient,
62

62

00:02:44,430  -->  00:02:48,320
or your information travels through your neural network
63

63

00:02:48,320  -->  00:02:50,880
to your answer, to get your output,
64

64

00:02:50,880  -->  00:02:52,780
and then the error is calculated
65

65

00:02:52,780  -->  00:02:54,620
and is propagated back through the network
66

66

00:02:54,620  -->  00:02:56,780
to update the weights.
67

67

00:02:56,780  -->  00:02:58,560
And here we've got some weights written out.
68

68

00:02:58,560  -->  00:03:02,100
So, what happens in a recurrent neural network
69

69

00:03:02,100  -->  00:03:05,660
is a similar thing, but here you've got
70

70

00:03:05,660  -->  00:03:06,960
a bit more going on, right?
71

71

00:03:06,960  -->  00:03:09,550
So, when your information travels through the network
72

72

00:03:09,550  -->  00:03:10,720
it travels like that.
73

73

00:03:10,720  -->  00:03:13,910
You've got lots of, it travels through time
74

74

00:03:13,910  -->  00:03:16,590
and information from previous time points goes,
75

75

00:03:16,590  -->  00:03:17,820
keeps coming through the network,
76

76

00:03:17,820  -->  00:03:19,410
and remember that every node here,
77

77

00:03:19,410  -->  00:03:21,070
it's always very important to remember
78

78

00:03:21,070  -->  00:03:23,740
for neural networks that every single node here
79

79

00:03:23,740  -->  00:03:27,920
is not just a node, it's a representation
80

80

00:03:27,920  -->  00:03:29,990
of a whole layer of nodes.
81

81

00:03:29,990  -->  00:03:31,480
Remember, we were looking at from,
82

82

00:03:31,480  -->  00:03:33,260
from the top or from the bottom.
83

83

00:03:33,260  -->  00:03:36,580
They are, they actually extend through this chart
84

84

00:03:36,580  -->  00:03:40,710
down there into this presentation.
85

85

00:03:40,710  -->  00:03:42,950
You can (mumbles), there's lots more
86

86

00:03:42,950  -->  00:03:45,140
neurons behind the ones that we can actually see
87

87

00:03:45,140  -->  00:03:47,000
because each one represents a layer.
88

88

00:03:47,000  -->  00:03:48,350
Very important to remember that.
89

89

00:03:48,350  -->  00:03:51,060
And so, at each point in time you can calculate
90

90

00:03:51,060  -->  00:03:54,060
your cost function, or your error.
91

91

00:03:54,060  -->  00:03:56,840
So, basically your cost function compares your output
92

92

00:03:56,840  -->  00:03:59,990
which is in the red circle to your desired output,
93

93

00:03:59,990  -->  00:04:03,060
to what you should be getting, and this happens
94

94

00:04:03,060  -->  00:04:04,300
during the training.
95

95

00:04:04,300  -->  00:04:06,890
And so, and you have these values for,
96

96

00:04:06,890  -->  00:04:08,710
throughout the time series.
97

97

00:04:08,710  -->  00:04:12,670
So, for every single one of these red circles
98

98

00:04:12,670  -->  00:04:15,000
you can calculate the cost function,
99

99

00:04:15,000  -->  00:04:17,190
and let's focus on one, just to understand
100

100

00:04:17,190  -->  00:04:18,023
what's going on.
101

101

00:04:18,023  -->  00:04:20,900
Let's focus on this one specifically at time T.
102

102

00:04:20,900  -->  00:04:24,430
So you've calculated the cost function epsilon T
103

103

00:04:24,430  -->  00:04:27,360
and now you want to propagate your cost function back
104

104

00:04:27,360  -->  00:04:28,193
through the network.
105

105

00:04:28,193  -->  00:04:29,100
Well, how is this going to look?
106

106

00:04:29,100  -->  00:04:30,420
Because you need to update the weight.
107

107

00:04:30,420  -->  00:04:32,560
So, every single neuron which participated
108

108

00:04:32,560  -->  00:04:34,880
in the calculation of the output associated
109

109

00:04:34,880  -->  00:04:36,540
with this cost function should be,
110

110

00:04:36,540  -->  00:04:39,920
should have their weight updated to,
111

111

00:04:39,920  -->  00:04:42,510
in order to help it better calculate the output,
112

112

00:04:42,510  -->  00:04:43,970
to minimize that error.
113

113

00:04:43,970  -->  00:04:47,020
And the thing here is that it's not just the neurons in,
114

114

00:04:47,020  -->  00:04:50,650
directly below epsilon T, directly below this red circle.
115

115

00:04:50,650  -->  00:04:52,500
It's all the neurons that contributed,
116

116

00:04:52,500  -->  00:04:53,800
and there's many more of them.
117

117

00:04:53,800  -->  00:04:56,560
There's all of these neurons as far back as you go.
118

118

00:04:56,560  -->  00:04:58,910
Depending on how many time steps you take,
119

119

00:04:58,910  -->  00:05:00,450
you might take one time step, you might take two,
120

120

00:05:00,450  -->  00:05:02,680
you might take 50 time steps, depending on how
121

121

00:05:02,680  -->  00:05:05,980
you set up your network, and this is where
122

122

00:05:05,980  -->  00:05:08,800
the problem lies, that you have to update
123

123

00:05:08,800  -->  00:05:11,261
or you have to propagate all the way back
124

124

00:05:11,261  -->  00:05:13,770
through time to these neurons.
125

125

00:05:13,770  -->  00:05:15,670
And when we talk about time it's not that
126

126

00:05:15,670  -->  00:05:17,470
the problem is that we can't travel through time.
127

127

00:05:17,470  -->  00:05:18,357
Not at all.
128

128

00:05:18,357  -->  00:05:21,370
We've unraveled this network, so this whole propagation
129

129

00:05:21,370  -->  00:05:22,466
can be facilitated.
130

130

00:05:22,466  -->  00:05:24,380
The problem lies in something else,
131

131

00:05:24,380  -->  00:05:26,870
and as much as I don't like putting math
132

132

00:05:26,870  -->  00:05:28,260
on the slides, on the intuition slides,
133

133

00:05:28,260  -->  00:05:29,790
we're not gonna go through this math,
134

134

00:05:29,790  -->  00:05:31,370
but I will point out one thing here.
135

135

00:05:31,370  -->  00:05:33,310
So, this is the math behind RNNs
136

136

00:05:33,310  -->  00:05:37,550
and we'll definitely direct you to more additional reading
137

137

00:05:37,550  -->  00:05:40,700
which you can do to up-skill on all of these maths.
138

138

00:05:40,700  -->  00:05:43,440
So, here we've got W rec, and W rec stands
139

139

00:05:43,440  -->  00:05:47,210
for weight recurring, and that is the weight that is used
140

140

00:05:47,210  -->  00:05:52,210
to connect the hidden layers to themselves
141

141

00:05:52,430  -->  00:05:55,861
in the unrolled temporal loop.
142

142

00:05:55,861  -->  00:05:59,090
And as you can see here on the left your,
143

143

00:05:59,090  -->  00:06:02,650
in order to get from X T minus three from this
144

144

00:06:02,650  -->  00:06:05,030
layer, remember, this is a layer, to X T minus two
145

145

00:06:05,030  -->  00:06:07,720
you need to apply W rec.
146

146

00:06:07,720  -->  00:06:09,650
Then from here to here apply W rec,
147

147

00:06:09,650  -->  00:06:12,450
and in simple terms, and intuitive terms
148

148

00:06:12,450  -->  00:06:15,760
what you're doing is you're simply multiplying
149

149

00:06:15,760  -->  00:06:18,500
the values here as you remember to get from layer
150

150

00:06:18,500  -->  00:06:19,333
to the next.
151

151

00:06:19,333  -->  00:06:20,860
We multiply the output by the weight,
152

152

00:06:20,860  -->  00:06:22,070
and then we get to the next layer,
153

153

00:06:22,070  -->  00:06:23,790
and then we multiply the output by the weight
154

154

00:06:23,790  -->  00:06:24,970
and get to the next, and then we multiply
155

155

00:06:24,970  -->  00:06:27,110
the output from here by the weight you get here,
156

156

00:06:27,110  -->  00:06:28,720
and the thing here is that we're multiplying
157

157

00:06:28,720  -->  00:06:31,460
by the same exact weight multiple times,
158

158

00:06:31,460  -->  00:06:34,050
many times, as many times as we need to go through
159

159

00:06:34,050  -->  00:06:35,830
this temporal loop, and this can be set.
160

160

00:06:35,830  -->  00:06:37,850
This, you can set this in your network.
161

161

00:06:37,850  -->  00:06:38,910
Do you wanna do it once?
162

162

00:06:38,910  -->  00:06:40,210
Do you wanna look back one step?
163

163

00:06:40,210  -->  00:06:41,100
Do you wanna look back three steps?
164

164

00:06:41,100  -->  00:06:43,140
Do you wanna look back 50 steps?
165

165

00:06:43,140  -->  00:06:44,480
But as many times as we do it,
166

166

00:06:44,480  -->  00:06:46,990
we have to multiply by the weight.
167

167

00:06:46,990  -->  00:06:49,034
And this is where the problem arises,
168

168

00:06:49,034  -->  00:06:53,572
because when you multiply by something small
169

169

00:06:53,572  -->  00:06:55,900
your value decreases very quickly,
170

170

00:06:55,900  -->  00:06:59,900
and this, the multiplication comes from this P here.
171

171

00:06:59,900  -->  00:07:02,610
You can see that P is, stands for multiplication.
172

172

00:07:02,610  -->  00:07:04,810
So, we have to multiply, and that's what it's representing,
173

173

00:07:04,810  -->  00:07:07,010
that as far as you go back you multiply,
174

174

00:07:07,010  -->  00:07:09,540
and as you remember weights are
175

175

00:07:09,540  -->  00:07:12,970
assigned at the start of your neural network.
176

176

00:07:12,970  -->  00:07:14,380
As you see in the practical tutorials
177

177

00:07:14,380  -->  00:07:17,420
your weights are assigned at a random value,
178

178

00:07:17,420  -->  00:07:18,717
but random values close to zero,
179

179

00:07:18,717  -->  00:07:21,200
and from there the network trains them up
180

180

00:07:21,200  -->  00:07:23,600
and identifies what they should be.
181

181

00:07:23,600  -->  00:07:26,050
But if you start with random double,
182

182

00:07:26,050  -->  00:07:29,930
a random W rec close to zero then because you're multiplying
183

183

00:07:29,930  -->  00:07:32,830
by it many times the more you multiply
184

184

00:07:32,830  -->  00:07:34,100
the lower the value gets.
185

185

00:07:34,100  -->  00:07:37,540
So, if you start off you might have a certain gradient
186

186

00:07:37,540  -->  00:07:39,890
going through your network being back propagated
187

187

00:07:39,890  -->  00:07:41,410
through your network.
188

188

00:07:41,410  -->  00:07:43,590
Then you move backwards.
189

189

00:07:43,590  -->  00:07:45,100
Your gradient becomes less.
190

190

00:07:45,100  -->  00:07:46,410
Then your gradient becomes less,
191

191

00:07:46,410  -->  00:07:48,170
and then your gradient becomes even less,
192

192

00:07:48,170  -->  00:07:49,710
and what does that mean for the network?
193

193

00:07:49,710  -->  00:07:52,660
And this is the important part to kind of get your head
194

194

00:07:52,660  -->  00:07:56,320
around, that what does a vanishing gradient like that,
195

195

00:07:56,320  -->  00:07:57,410
why is it bad for the network?
196

196

00:07:57,410  -->  00:07:59,917
Well, because the gradient as it goes back through
197

197

00:07:59,917  -->  00:08:02,690
the network, it is used to update the weights,
198

198

00:08:02,690  -->  00:08:04,070
and we know that already.
199

199

00:08:04,070  -->  00:08:07,530
Well, the lower the gradient is the harder it is
200

200

00:08:07,530  -->  00:08:09,380
for the network to update the weights.
201

201

00:08:09,380  -->  00:08:14,380
It cannot, it can still understand what kind of contribution
202

202

00:08:15,270  -->  00:08:19,030
each of these outputs has towards the error,
203

203

00:08:19,030  -->  00:08:20,390
and therefore it can update the weights,
204

204

00:08:20,390  -->  00:08:23,260
but the lower the gradient the slower it's going
205

205

00:08:23,260  -->  00:08:25,283
to update the weights, and the higher the gradient
206

206

00:08:25,283  -->  00:08:27,490
the faster it's going to update the weights
207

207

00:08:27,490  -->  00:08:28,970
and get you the final result.
208

208

00:08:28,970  -->  00:08:32,970
And so, therefore if you have for instance 1,000 epochs
209

209

00:08:32,970  -->  00:08:37,032
these weights for this layer might be updated
210

210

00:08:37,032  -->  00:08:39,260
towards the end of the 1,000 and they'll,
211

211

00:08:39,260  -->  00:08:40,330
you'll have some final results.
212

212

00:08:40,330  -->  00:08:42,930
But these weights, because the gradient's so,
213

213

00:08:42,930  -->  00:08:47,650
so much smaller, they're gonna be updated slower
214

214

00:08:47,650  -->  00:08:49,880
and therefore by the end of the 1,000 epochs
215

215

00:08:49,880  -->  00:08:52,214
you might not have the final results there,
216

216

00:08:52,214  -->  00:08:54,557
and therefore this part of the network is trained.
217

217

00:08:54,557  -->  00:08:56,620
This part of the network is not trained,
218

218

00:08:56,620  -->  00:08:58,640
based on this cost function.
219

219

00:08:58,640  -->  00:09:00,930
But the problem here is not just that half
220

220

00:09:00,930  -->  00:09:03,060
your network is not trained properly,
221

221

00:09:03,060  -->  00:09:08,060
but also that these weights, or this layer,
222

222

00:09:08,390  -->  00:09:12,070
its outputs are being used as inputs for further layers.
223

223

00:09:12,070  -->  00:09:16,160
So, the training here has been happening all along
224

224

00:09:16,160  -->  00:09:19,390
based on, based off of inputs that are coming
225

225

00:09:19,390  -->  00:09:21,860
from untrained neurons, untrained layers.
226

226

00:09:21,860  -->  00:09:24,130
So, it's kind of a vicious cycle.
227

227

00:09:24,130  -->  00:09:26,560
You're training here and you think you're getting
228

228

00:09:26,560  -->  00:09:29,130
good results, but because you're gradient's so small here
229

229

00:09:29,130  -->  00:09:31,350
this is training so slow and the outputs it's giving
230

230

00:09:31,350  -->  00:09:36,050
is so, are incorrect, are not final outputs,
231

231

00:09:36,050  -->  00:09:38,090
and therefore you're training on the non-final output.
232

232

00:09:38,090  -->  00:09:40,810
So, because of this balance because of vanishing
233

233

00:09:40,810  -->  00:09:45,810
gradient you have a problem in your network
234

234

00:09:45,870  -->  00:09:48,030
which is not just that these weights
235

235

00:09:48,030  -->  00:09:48,863
are not being trained properly.
236

236

00:09:48,863  -->  00:09:51,040
The whole network is not being trained properly
237

237

00:09:51,040  -->  00:09:53,370
because these weights are not being trained properly,
238

238

00:09:53,370  -->  00:09:57,020
and that's, there's like a domino effect like this
239

239

00:09:57,020  -->  00:09:58,780
wherever you look in the network.
240

240

00:09:58,780  -->  00:10:00,550
Always the ones at the very back
241

241

00:10:00,550  -->  00:10:01,880
are going to have that problem,
242

242

00:10:01,880  -->  00:10:04,640
and this is in a nutshell what the vanishing gradient
243

243

00:10:04,640  -->  00:10:08,065
problem for recurrent neural networks is,
244

244

00:10:08,065  -->  00:10:11,824
and that's, that was kind of the main roadblock
245

245

00:10:11,824  -->  00:10:15,910
to using recurrent neural networks,
246

246

00:10:15,910  -->  00:10:18,650
and let's summarize this in a short slide.
247

247

00:10:18,650  -->  00:10:23,100
So, if W rec is small, then your,
248

248

00:10:23,950  -->  00:10:25,340
you have a vanishing gradient problem.
249

249

00:10:25,340  -->  00:10:28,480
If W rec is large you have an exploding gradient problem,
250

250

00:10:28,480  -->  00:10:31,120
because same thing, but then it's just gonna explode.
251

251

00:10:31,120  -->  00:10:34,600
And here an important thing to point out here
252

252

00:10:34,600  -->  00:10:36,810
is that of course there's more to it, right?
253

253

00:10:36,810  -->  00:10:39,890
There's, as you can see, there's in this formula
254

254

00:10:39,890  -->  00:10:42,360
there's other things like the activation function,
255

255

00:10:42,360  -->  00:10:43,208
and so on.
256

256

00:10:43,208  -->  00:10:44,910
So, it's not as simple as small or large,
257

257

00:10:44,910  -->  00:10:47,060
or less than one, greater than one, but as a rule
258

258

00:10:47,060  -->  00:10:49,030
of thumb if you have very small weights
259

259

00:10:49,030  -->  00:10:49,863
you're gonna have a vanishing gradient,
260

260

00:10:49,863  -->  00:10:53,010
and if you have very large weights, you put 100 in there,
261

261

00:10:53,010  -->  00:10:55,660
the value of 100, then you're gonna, by step two
262

262

00:10:55,660  -->  00:10:57,570
you're gonna have 10,000.
263

263

00:10:57,570  -->  00:11:00,090
By step three you're going to have 1,000,000.
264

264

00:11:00,090  -->  00:11:03,820
So, then you have an exploding gradient problem.
265

265

00:11:03,820  -->  00:11:06,710
So, hopefully this summarizes the exploding gradient
266

266

00:11:06,710  -->  00:11:10,010
problem on an intuitive level, because, so,
267

267

00:11:10,010  -->  00:11:13,700
in short, because you're unraveling the temporal loop
268

268

00:11:13,700  -->  00:11:16,470
the further you go through your network
269

269

00:11:16,470  -->  00:11:18,590
the lower your gradient is and the harder it is
270

270

00:11:18,590  -->  00:11:21,010
to train the weights which has a domino effect on all
271

271

00:11:21,010  -->  00:11:24,350
of the further weights throughout the network as well.
272

272

00:11:24,350  -->  00:11:27,070
And so, how do you combat the vanishing gradient problem?
273

273

00:11:27,070  -->  00:11:29,020
There's a couple of solutions.
274

274

00:11:29,020  -->  00:11:30,870
For instance, for the exploding gradient
275

275

00:11:30,870  -->  00:11:33,120
you can have truncated back propagation.
276

276

00:11:33,120  -->  00:11:36,080
So, you stop back propagating after a certain point,
277

277

00:11:36,080  -->  00:11:38,260
but as you can imagine that's probably not optimal
278

278

00:11:38,260  -->  00:11:41,130
because then you're not updating all the weights.
279

279

00:11:41,130  -->  00:11:43,750
But if you don't stop, then you're just going to have
280

280

00:11:43,750  -->  00:11:45,590
a completely irrelevant network.
281

281

00:11:45,590  -->  00:11:48,673
So, it's better than the original approach.
282

282

00:11:48,673  -->  00:11:50,020
And then you can have penalties.
283

283

00:11:50,020  -->  00:11:53,920
So, you could have the gradient being penalized
284

284

00:11:53,920  -->  00:11:56,570
and being artificially reduced.
285

285

00:11:56,570  -->  00:11:57,970
You can have gradient clipping.
286

286

00:11:57,970  -->  00:12:01,780
So, you could have a maximum limit for the gradient.
287

287

00:12:01,780  -->  00:12:04,300
You could say never, never have the gradient go over
288

288

00:12:04,300  -->  00:12:06,570
this value, and then if it's over that value
289

289

00:12:06,570  -->  00:12:08,780
it just stays at that value as it propagates
290

290

00:12:08,780  -->  00:12:10,620
further down through a network.
291

291

00:12:10,620  -->  00:12:12,900
You can have, for the vanishing gradient problem
292

292

00:12:12,900  -->  00:12:15,369
you have certain other solutions.
293

293

00:12:15,369  -->  00:12:19,500
You have weight initialization, where you are
294

294

00:12:19,500  -->  00:12:21,250
smart about how you initialize your weights
295

295

00:12:21,250  -->  00:12:25,142
to minimize the potential for vanishing gradient.
296

296

00:12:25,142  -->  00:12:29,240
You can have, there's a type of network
297

297

00:12:29,240  -->  00:12:32,400
called the echo state networks, and we're not
298

298

00:12:32,400  -->  00:12:35,750
going to talk about that, but they do somehow
299

299

00:12:35,750  -->  00:12:37,940
solve that, or they are designed to solve
300

300

00:12:37,940  -->  00:12:39,450
the vanishing gradient problem,
301

301

00:12:39,450  -->  00:12:41,280
but there's also a different type of network
302

302

00:12:41,280  -->  00:12:44,560
called the long short-term memory networks, or the LSTMs
303

303

00:12:44,560  -->  00:12:47,891
for short, which are extremely popular,
304

304

00:12:47,891  -->  00:12:52,040
which are considered to be the go to network
305

305

00:12:52,040  -->  00:12:55,010
for implementing recurrent neural networks,
306

306

00:12:55,010  -->  00:12:57,790
and that's exactly what we're going to be talking about
307

307

00:12:57,790  -->  00:12:59,400
in the rest of this section.
308

308

00:12:59,400  -->  00:13:01,391
So, that's gonna be very exciting.
309

309

00:13:01,391  -->  00:13:03,810
We're going to look at a brand new architecture
310

310

00:13:03,810  -->  00:13:06,060
for recurrent neural networks.
311

311

00:13:06,060  -->  00:13:07,830
I really can't wait to get started on that.
312

312

00:13:07,830  -->  00:13:11,804
It's a very interesting topic to get into.
313

313

00:13:11,804  -->  00:13:15,367
But for now, this is, this brings us to the end
314

314

00:13:15,367  -->  00:13:17,830
of the vanishing gradient problem tutorial,
315

315

00:13:17,830  -->  00:13:19,590
and some additional reading.
316

316

00:13:19,590  -->  00:13:22,470
So, additional reading, you can definitely reference
317

317

00:13:22,470  -->  00:13:26,220
the original works by Sepp Hochreiter and Yoshua Bengio.
318

318

00:13:26,220  -->  00:13:30,123
So, this is Sepp's paper in 1991, from 1991.
319

319

00:13:31,070  -->  00:13:33,330
It's completely in German, so,
320

320

00:13:33,330  -->  00:13:34,910
I'm not even going to read the name,
321

321

00:13:34,910  -->  00:13:36,970
but if you understand and can read German,
322

322

00:13:36,970  -->  00:13:39,622
then definitely this could be a good read for you.
323

323

00:13:39,622  -->  00:13:42,050
Then there's Yoshua Bengio's paper
324

324

00:13:42,050  -->  00:13:44,440
which is called Learning Long Term Dependencies
325

325

00:13:44,440  -->  00:13:47,297
with Gradient Descent is Difficult, 1994.
326

326

00:13:47,297  -->  00:13:51,360
Also you can reference that, but what I would recommend
327

327

00:13:51,360  -->  00:13:53,860
looking into is a paper called On the difficulty
328

328

00:13:53,860  -->  00:13:56,350
of training recurrent neural networks
329

329

00:13:56,350  -->  00:13:57,980
by Razvan Pascanu.
330

330

00:13:57,980  -->  00:13:58,880
It's just newer.
331

331

00:13:58,880  -->  00:14:01,740
It's 2013 and it's also got Yoshua Bengio
332

332

00:14:01,740  -->  00:14:02,940
as the co-author.
333

333

00:14:02,940  -->  00:14:05,270
So, probably he was supervising some of this research,
334

334

00:14:05,270  -->  00:14:07,290
and here it summarizes a lot of the things
335

335

00:14:07,290  -->  00:14:10,980
that Yoshua Bengio talks about in his paper from 1994.
336

336

00:14:12,430  -->  00:14:14,880
So, why not look at something newer.
337

337

00:14:14,880  -->  00:14:18,540
I would recommend checking this paper out.
338

338

00:14:18,540  -->  00:14:19,540
So, there we go.
339

339

00:14:19,540  -->  00:14:22,100
That's the vanishing gradient problem.
340

340

00:14:22,100  -->  00:14:23,060
Hope this was fun.
341

341

00:14:23,060  -->  00:14:24,520
Can't wait to see you on the next tutorial,
342

342

00:14:24,520  -->  00:14:26,363
and until then, enjoy deep learning.
