1
1

00:00:00,560  -->  00:00:01,393
<v Instructor>Hello and welcome back,</v>
2

2

00:00:01,393  -->  00:00:02,999
to the course on Deep Learning. Today we're talking
3

3

00:00:02,999  -->  00:00:07,010
about Contrastive Divergence. This is the algorithm
4

4

00:00:07,010  -->  00:00:11,090
that actually allows Restricted Boltzmann Machines to learn.
5

5

00:00:11,090  -->  00:00:12,600
So let's have a look at this.
6

6

00:00:12,600  -->  00:00:15,078
Here we've got a diagrammatic representation of our
7

7

00:00:15,078  -->  00:00:18,840
Restricted Boltzmann Machines. Here we've got the input
8

8

00:00:18,840  -->  00:00:20,940
nodes, and we've got the hidden nodes in red.
9

9

00:00:20,940  -->  00:00:23,010
And what are we going to be covering off today,
10

10

00:00:23,010  -->  00:00:26,220
is a specific part of the learning process.
11

11

00:00:26,220  -->  00:00:28,410
So we've already talked about the learning process
12

12

00:00:28,410  -->  00:00:30,960
for Restricted Boltzmann Machines in the previous tutorial,
13

13

00:00:30,960  -->  00:00:35,960
where we discussed exactly how we feed in different values,
14

14

00:00:36,030  -->  00:00:38,530
different roles into our Restricted Boltzmann Machines
15

15

00:00:38,530  -->  00:00:42,460
and how it looks at them and looks for features
16

16

00:00:42,460  -->  00:00:45,440
and then assign certain nodes to those features
17

17

00:00:45,440  -->  00:00:50,070
to further understand our system and better represent
18

18

00:00:50,070  -->  00:00:53,080
our system. But the question that we still have is,
19

19

00:00:53,080  -->  00:00:56,530
how does the Restricted Boltzmann Machines adjust
20

20

00:00:56,530  -->  00:01:00,590
its weights. Because previously in the other neural networks
21

21

00:01:00,590  -->  00:01:04,020
that we've looked at, we had the gradient descent process,
22

22

00:01:04,020  -->  00:01:06,870
which allowed us to back propagate the error through
23

23

00:01:06,870  -->  00:01:09,935
the network, and therefore adjust the weights accordingly
24

24

00:01:09,935  -->  00:01:11,640
to minimize that error.
25

25

00:01:11,640  -->  00:01:15,420
But in this time, we don't have a directed network.
26

26

00:01:15,420  -->  00:01:18,140
We have an undirected network. And the question is,
27

27

00:01:18,140  -->  00:01:20,240
how do the weights get adjusted here.
28

28

00:01:20,240  -->  00:01:22,640
And this is where the Contrastive Divergence comes in.
29

29

00:01:22,640  -->  00:01:24,930
So let's have a look at this process what's going on.
30

30

00:01:24,930  -->  00:01:25,922
We're going to look at it in two ways.
31

31

00:01:25,922  -->  00:01:27,970
We're going to look at a diagrammatically like this
32

32

00:01:27,970  -->  00:01:31,880
and then we're going to look at it through a chart,
33

33

00:01:31,880  -->  00:01:34,380
through an energy chart, which will help us understand,
34

34

00:01:34,380  -->  00:01:36,070
we're going to tackle this problem from two ways.
35

35

00:01:36,070  -->  00:01:40,591
So let's go. So here we've got our input nodes.
36

36

00:01:40,591  -->  00:01:44,200
Once you put them into the network, using some randomly
37

37

00:01:44,200  -->  00:01:46,520
assigned weights, at the very start,
38

38

00:01:46,520  -->  00:01:50,250
the system or the Restricted Boltzmann Machine calculates
39

39

00:01:50,250  -->  00:01:52,945
the hidden nodes. Then what's going to happen is
40

40

00:01:52,945  -->  00:01:56,030
those hidden nodes are going to use the exact same weights
41

41

00:01:56,030  -->  00:01:59,830
to calculate the input nodes, or to reconstruct,
42

42

00:01:59,830  -->  00:02:03,631
the correct term is to reconstruct the input nodes here.
43

43

00:02:03,631  -->  00:02:06,480
The key point here is that they weigh exactly the same
44

44

00:02:06,480  -->  00:02:09,810
they don't change. What is also important to understand
45

45

00:02:09,810  -->  00:02:14,170
is that the reconstructed inputs are not going to equal
46

46

00:02:14,170  -->  00:02:17,330
the original inputs even though the weight is the same.
47

47

00:02:17,330  -->  00:02:20,160
Let's have a look at our network as our
48

48

00:02:20,160  -->  00:02:22,240
Restricted Boltzmann Machine in a bit more depth
49

49

00:02:22,240  -->  00:02:25,680
or in a bit more detail to understand this specific thing.
50

50

00:02:25,680  -->  00:02:26,970
Why that is.
51

51

00:02:26,970  -->  00:02:29,310
So here we've got our Restricted Boltzmann Machine,
52

52

00:02:29,310  -->  00:02:31,946
we've got visible nodes, our hidden nodes.
53

53

00:02:31,946  -->  00:02:36,946
The question is, once we've reconstructed are visible modes,
54

54

00:02:37,010  -->  00:02:41,160
how come they're not identical to the original visible nodes
55

55

00:02:41,160  -->  00:02:42,660
even though we're using the same weights.
56

56

00:02:42,660  -->  00:02:45,710
Well, the reason for that is because these nodes
57

57

00:02:45,710  -->  00:02:47,910
are not initially interconnected.
58

58

00:02:47,910  -->  00:02:51,020
There's no specific connection, not necessarily
59

59

00:02:51,020  -->  00:02:52,280
is there a specific connection between them,
60

60

00:02:52,280  -->  00:02:54,520
and there's no formula or equation that's connecting.
61

61

00:02:54,520  -->  00:02:56,740
Let's listen to stand this on an example.
62

62

00:02:56,740  -->  00:02:59,740
Let's look at this node, node number two over here.
63

63

00:02:59,740  -->  00:03:01,340
How does it get reconstructed?
64

64

00:03:01,340  -->  00:03:04,300
Well, it gets it gets reconstructed based on the values
65

65

00:03:04,300  -->  00:03:06,500
that all of these hidden nodes, all of these five
66

66

00:03:06,500  -->  00:03:11,117
hidden nodes have in them. So once we first run this RBM,
67

67

00:03:11,117  -->  00:03:14,910
these initial values will assign or will initiate
68

68

00:03:14,910  -->  00:03:16,710
some values in your hidden nodes.
69

69

00:03:16,710  -->  00:03:19,700
Then once we run it backwards, these hidden nodes
70

70

00:03:19,700  -->  00:03:23,810
will reconstruct, all of these nodes including this node.
71

71

00:03:23,810  -->  00:03:27,610
But the thing is, each one of these nodes wasn't constructed
72

72

00:03:27,610  -->  00:03:29,550
just based on this node. If that was the case,
73

73

00:03:29,550  -->  00:03:32,640
if this node was on its own, and it constructed this node
74

74

00:03:32,640  -->  00:03:35,221
constructed this node, the value from this node was used
75

75

00:03:35,221  -->  00:03:38,640
to create the value in this node. It was then used to create
76

76

00:03:38,640  -->  00:03:41,768
this node and then it was used to create this node,
77

77

00:03:41,768  -->  00:03:42,601
it was used to create this node, was used to create
78

78

00:03:42,601  -->  00:03:44,197
this node, and then we ran everything backwards,
79

79

00:03:44,197  -->  00:03:47,838
then yes, this node would be identical to what it was
80

80

00:03:47,838  -->  00:03:52,600
initially. But the way the RBM works is that this node
81

81

00:03:52,600  -->  00:03:55,942
during the forward pause, this node was constructed
82

82

00:03:55,942  -->  00:03:58,136
from this node, from this node, from this node,
83

83

00:03:58,136  -->  00:03:59,336
from this node, from this node.
84

84

00:03:59,336  -->  00:04:02,270
Then this node was constructed from this node,
85

85

00:04:02,270  -->  00:04:03,760
from this node, from this node, from this node,
86

86

00:04:03,760  -->  00:04:04,720
from this node and from this node.
87

87

00:04:04,720  -->  00:04:07,380
So every single node here, was constructed
88

88

00:04:07,380  -->  00:04:12,160
from all six of these. And so therefore, all of these nodes
89

89

00:04:12,160  -->  00:04:15,290
have values that didn't initially come from here.
90

90

00:04:15,290  -->  00:04:17,200
They came from the other nodes, and therefore
91

91

00:04:17,200  -->  00:04:20,790
when you reconstruct this node even though using the same
92

92

00:04:20,790  -->  00:04:23,930
weights, the values in here came from other nodes
93

93

00:04:23,930  -->  00:04:26,700
as well as this node and therefore the reconstructed
94

94

00:04:26,700  -->  00:04:28,810
value in this node is not going to be equal
95

95

00:04:28,810  -->  00:04:31,498
to what you had in here initially.
96

96

00:04:31,498  -->  00:04:34,800
That's very important to understand, that's why this whole
97

97

00:04:34,800  -->  00:04:39,800
Contrastive Divergence process exists, that's why from here
98

98

00:04:39,860  -->  00:04:42,075
we're going to keep going and these two don't equal
99

99

00:04:42,075  -->  00:04:44,151
to each other. So there's a first forward pause
100

100

00:04:44,151  -->  00:04:48,620
backward pause, now we're going to do another one.
101

101

00:04:48,620  -->  00:04:50,900
We're going to again feed these values
102

102

00:04:50,900  -->  00:04:55,870
the reconstructed node values of our inputs into the RBM
103

103

00:04:55,870  -->  00:04:58,890
and we're going to get some outputs or some hidden values.
104

104

00:04:58,890  -->  00:05:00,058
Then based on these hidden values, we're going to
105

105

00:05:00,058  -->  00:05:02,740
reconstruct the inputs again, and again,
106

106

00:05:02,740  -->  00:05:03,840
they're not going to equal.
107

107

00:05:03,840  -->  00:05:08,630
Then we're going to construct the hidden values and so on.
108

108

00:05:08,630  -->  00:05:12,730
And this whole process is called Gibbs sampling.
109

109

00:05:12,730  -->  00:05:15,620
And towards the end, finally, at some point,
110

110

00:05:15,620  -->  00:05:19,750
we're going to get some reconstructed input values
111

111

00:05:19,750  -->  00:05:23,223
which are such that when we feed them into the RBM,
112

112

00:05:24,770  -->  00:05:27,570
and then we try to reconstruct them again,
113

113

00:05:27,570  -->  00:05:28,830
we will get those same values.
114

114

00:05:28,830  -->  00:05:31,610
So from here we don't keep going forward from here,
115

115

00:05:31,610  -->  00:05:33,710
this once it goes into the RBM and then it get
116

116

00:05:33,710  -->  00:05:36,110
reconstruction we get exactly that.
117

117

00:05:36,110  -->  00:05:41,100
So in this final scenario, what happens is yes, our network
118

118

00:05:41,100  -->  00:05:44,748
is modeling exactly our inputs. So basically,
119

119

00:05:44,748  -->  00:05:49,313
we can input in and we will always get the output.
120

120

00:05:50,655  -->  00:05:55,200
So in essence, this process has finally converged
121

121

00:05:55,200  -->  00:05:58,960
and our network is finally a great
122

122

00:05:58,960  -->  00:06:03,550
model to model our inputs. To model that specific input.
123

123

00:06:03,550  -->  00:06:06,110
And in terms of the curve, what does this look like?
124

124

00:06:06,110  -->  00:06:08,510
So we've discussed this process step by step
125

125

00:06:08,510  -->  00:06:10,440
in terms of diagrammatically.
126

126

00:06:10,440  -->  00:06:12,260
Now let's have a look what this looks like
127

127

00:06:12,260  -->  00:06:14,730
in terms of curve, and also what it means for us.
128

128

00:06:14,730  -->  00:06:16,560
In terms of the curve, this is what it looks like.
129

129

00:06:16,560  -->  00:06:19,570
So we've got two parts here, we'll start with the formula.
130

130

00:06:19,570  -->  00:06:24,300
We've got a formula, and this is a gradient formula.
131

131

00:06:24,300  -->  00:06:26,240
So you can see the gradient on the left here.
132

132

00:06:26,240  -->  00:06:29,570
And we've got the gradient of the log probability
133

133

00:06:29,570  -->  00:06:34,264
of a certain state of our system, based on the weights
134

134

00:06:34,264  -->  00:06:35,910
in the system.
135

135

00:06:35,910  -->  00:06:38,180
And remember, through this whole process, through
136

136

00:06:38,180  -->  00:06:40,490
this whole process, the weights we're constant.
137

137

00:06:40,490  -->  00:06:42,280
They we're not changing the weights, we're just using
138

138

00:06:42,280  -->  00:06:43,610
weights whichever weights twe have.
139

139

00:06:43,610  -->  00:06:45,870
And for now, we're also the weight is constant is very
140

140

00:06:45,870  -->  00:06:46,920
important to remember.
141

141

00:06:46,920  -->  00:06:50,904
But here, what it's telling us is how the weights affect
142

142

00:06:50,904  -->  00:06:53,590
the log probability. How changing the weights will change
143

143

00:06:53,590  -->  00:06:55,500
the log probability, and the way it'll change
144

144

00:06:55,500  -->  00:06:58,640
is this value minus value. And so what are these values?
145

145

00:06:58,640  -->  00:07:01,119
Well, this is your initial state of the system visible,
146

146

00:07:01,119  -->  00:07:03,750
hidden, visible vector, hidden vector,
147

147

00:07:03,750  -->  00:07:06,740
visible layer, hidden layer, visible layer, hidden layer.
148

148

00:07:06,740  -->  00:07:09,570
Without going into too much detail on this formula,
149

149

00:07:09,570  -->  00:07:12,430
we're going to look at it on the curve, because we'll supply
150

150

00:07:12,430  -->  00:07:15,190
some additional reading, which you can look at in your free
151

151

00:07:15,190  -->  00:07:17,982
time. But this let's go through the intuitive understanding.
152

152

00:07:17,982  -->  00:07:20,400
It will help us understand what's going on here.
153

153

00:07:20,400  -->  00:07:23,400
So we've got initial state, that's this one,
154

154

00:07:23,400  -->  00:07:26,380
visible hidden layer. That's our data space
155

155

00:07:26,380  -->  00:07:27,530
and this is our energy.
156

156

00:07:27,530  -->  00:07:29,600
But actually, before we put these on, let's talk
157

157

00:07:29,600  -->  00:07:31,620
about the energy. Where does the energy come from?
158

158

00:07:31,620  -->  00:07:34,820
We've talked about energy based systems
159

159

00:07:34,820  -->  00:07:38,820
and we said that in the Restricted Boltzmann Machines
160

160

00:07:38,820  -->  00:07:41,130
the way we define energy is through weights.
161

161

00:07:41,130  -->  00:07:44,220
So the weights are fixed as we saw through the whole
162

162

00:07:44,220  -->  00:07:47,680
previous process the way to fixed, and we're going to define
163

163

00:07:47,680  -->  00:07:51,300
this energy curve is based on the weight.
164

164

00:07:51,300  -->  00:07:55,380
So weights dictate the shape of this energy curve,
165

165

00:07:55,380  -->  00:08:00,100
and now we place our initial inputs
166

166

00:08:00,100  -->  00:08:03,940
visible and hidden layer. So our first pause through the RBM
167

167

00:08:03,940  -->  00:08:06,610
and that's where we happen to be, just because the weights
168

168

00:08:06,610  -->  00:08:09,910
are initialized randomly. For example, we end up some where
169

169

00:08:09,910  -->  00:08:14,580
here. Then after, so that's over there, after that pause,
170

170

00:08:14,580  -->  00:08:17,026
now we have this pause, visible, hidden.
171

171

00:08:17,026  -->  00:08:20,640
after the second pause, what happens is we end up somewhere
172

172

00:08:21,638  -->  00:08:25,540
here. So as we've discussed before, a system
173

173

00:08:25,540  -->  00:08:30,540
which is governed by its energy will always try to end up in
174

174

00:08:30,670  -->  00:08:33,490
the lowest energy state possible.
175

175

00:08:33,490  -->  00:08:36,410
So as you can see, this ball is rolling towards the bottom
176

176

00:08:36,410  -->  00:08:38,550
over here. And that's exactly what's happening through
177

177

00:08:38,550  -->  00:08:42,310
that divergent process, through that process which
178

178

00:08:42,310  -->  00:08:45,130
we discussed through our diagrams as we go through
179

179

00:08:45,130  -->  00:08:47,510
the Contrastive Divergence process basically.
180

180

00:08:47,510  -->  00:08:50,070
What's happening is, we're going closer and closer
181

181

00:08:50,070  -->  00:08:52,600
to our lowest energy state.
182

182

00:08:52,600  -->  00:08:55,780
And a good way to think about it is like if you're feeling
183

183

00:08:55,780  -->  00:08:58,190
a bit lost right now, just think of it as, remember
184

184

00:08:58,190  -->  00:08:59,800
that example of the gas.
185

185

00:08:59,800  -->  00:09:04,030
You have a room and then you let out some gas in one corner.
186

186

00:09:04,030  -->  00:09:06,910
For instance, if you got a big room, and then you let out
187

187

00:09:06,910  -->  00:09:08,740
some gas in the top right corner,
188

188

00:09:08,740  -->  00:09:12,150
just based on the architecture of the room based
189

189

00:09:12,150  -->  00:09:15,920
on the conditions that the gas has been put into,
190

190

00:09:15,920  -->  00:09:18,930
hence the weights. So the the architecture of the room
191

191

00:09:18,930  -->  00:09:22,470
or the design of the room, that is not the lowest energy
192

192

00:09:22,470  -->  00:09:24,610
state for that gas at that point in time.
193

193

00:09:24,610  -->  00:09:28,260
So the gas could be in a much lower energy state than that.
194

194

00:09:28,260  -->  00:09:31,350
And so what the gas starts doing is it starts expanding
195

195

00:09:31,350  -->  00:09:34,040
into the whole room, this is what's happening here.
196

196

00:09:34,040  -->  00:09:36,720
Through the Contrastive Divergence process, we're finding
197

197

00:09:36,720  -->  00:09:41,180
what's the values in our system, the inputs and the hidden
198

198

00:09:41,180  -->  00:09:43,770
layers, what they should be for the system
199

199

00:09:43,770  -->  00:09:48,160
to be in the lowest energy state possible. So here it is.
200

200

00:09:48,160  -->  00:09:51,150
So basically, at the very end, we get certain values.
201

201

00:09:51,150  -->  00:09:53,040
Again, the weights are not changed, the weights haven't
202

202

00:09:53,040  -->  00:09:55,230
changed, but we get certain values, certain reconstructed
203

203

00:09:55,230  -->  00:09:58,320
input values and certain hidden values, that bring
204

204

00:09:58,320  -->  00:10:01,730
the system to the minimal energy state at the end
205

205

00:10:01,730  -->  00:10:04,470
of this Contrastive Divergence process, and in terms
206

206

00:10:04,470  -->  00:10:08,220
of the diagram, this means that this bowl ends up over here.
207

207

00:10:08,220  -->  00:10:11,424
And from there, what this formula is telling us is,
208

208

00:10:11,424  -->  00:10:15,200
once you have that state, once you are ball is over here
209

209

00:10:15,200  -->  00:10:18,170
at the very end, if you subtract this value,
210

210

00:10:18,170  -->  00:10:20,860
and again, just check the additional reading for more
211

211

00:10:20,860  -->  00:10:24,320
information is, but basically if you subtract this value
212

212

00:10:24,320  -->  00:10:28,750
from this value, it will tell you how adjusting your weights
213

213

00:10:28,750  -->  00:10:31,740
will affect the log probability
214

214

00:10:33,840  -->  00:10:35,630
of the system being in this state.
215

215

00:10:35,630  -->  00:10:39,050
So basically, this is a recipe, this formula
216

216

00:10:39,050  -->  00:10:42,500
is a recipe for adjusting your curve for shifting
217

217

00:10:42,500  -->  00:10:47,010
or for modifying your energy curve, so that you can make
218

218

00:10:47,010  -->  00:10:51,100
sure that this state is inside an energy minimum,
219

219

00:10:51,100  -->  00:10:53,050
so that you can get any desired effect.
220

220

00:10:54,596  -->  00:10:58,680
Right now it's like getting towards a certain
221

221

00:10:58,680  -->  00:11:01,750
minimal energy state but the inputs are completely different
222

222

00:11:01,750  -->  00:11:04,040
to our inputs, we want to change that.
223

223

00:11:04,040  -->  00:11:06,524
We want to use this formula to adjust our curve, so this,
224

224

00:11:06,524  -->  00:11:10,360
the energy minimum is actually next to our inputs
225

225

00:11:10,360  -->  00:11:13,730
rather than some random reconstructed inputs which
226

226

00:11:13,730  -->  00:11:16,960
are defined by the randomly initialized weights.
227

227

00:11:16,960  -->  00:11:18,543
That's what we're doing here.
228

228

00:11:19,470  -->  00:11:21,620
So this is a long process, as you can imagine
229

229

00:11:21,620  -->  00:11:23,140
it takes very long.
230

230

00:11:23,140  -->  00:11:25,918
But in 1998, Jeffrey Hinton discovered a shortcut
231

231

00:11:25,918  -->  00:11:28,700
which I like to call, I'm probably going to call
232

232

00:11:28,700  -->  00:11:31,275
on this tutorial Hinton's shortcut.
233

233

00:11:31,275  -->  00:11:34,520
What happens is he says that, "Even if you take just the
234

234

00:11:34,520  -->  00:11:37,697
first two pauses", you don't wait until it convergences to
235

235

00:11:37,697  -->  00:11:41,768
the end, "This is sufficient to understand how to adjust
236

236

00:11:41,768  -->  00:11:45,290
your curve as far as is the initial stage."
237

237

00:11:45,290  -->  00:11:49,750
So this is a CD one, Contrastive Divergence one pause.
238

238

00:11:49,750  -->  00:11:52,947
You might hear that term CD one, CD three, CD five, CD nine
239

239

00:11:52,947  -->  00:11:53,883
and so on.
240

240

00:11:56,775  -->  00:11:58,970
So if you do a CD one, Contrastive Divergence one pause,
241

241

00:11:58,970  -->  00:12:02,320
what happens is, so you know from here, you've got this as
242

242

00:12:02,320  -->  00:12:04,110
your green ball. This is your red ball,
243

243

00:12:04,110  -->  00:12:06,320
your green ball is over here, your red ball is over here
244

244

00:12:06,320  -->  00:12:08,890
and you know that and that's enough for you
245

245

00:12:08,890  -->  00:12:10,320
to know which way is rolling.
246

246

00:12:10,320  -->  00:12:12,820
So like similar to what we had in gradient descent
247

247

00:12:12,820  -->  00:12:15,880
that's why it's downhill. But you now want to adjust
248

248

00:12:15,880  -->  00:12:18,537
your curve. In gradient decent we just had a curve
249

249

00:12:18,537  -->  00:12:20,540
and we were like finding the minimum.
250

250

00:12:20,540  -->  00:12:24,550
Here we have control over the curve, we are adjusting
251

251

00:12:24,550  -->  00:12:27,160
the weights because it's an energy based process
252

252

00:12:27,160  -->  00:12:32,160
we're adjusting the weights so that minimum is actually
253

253

00:12:32,170  -->  00:12:35,259
going to be here rather than here. So let's look at that.
254

254

00:12:35,259  -->  00:12:37,420
So we know that the balls are going rolling downhill.
255

255

00:12:37,420  -->  00:12:41,250
So what we actually want is want to pull this curve down
256

256

00:12:41,250  -->  00:12:43,350
here and we want to push it up over here.
257

257

00:12:43,350  -->  00:12:47,620
That way you'll see what happens viola.
258

258

00:12:47,620  -->  00:12:51,600
So you can see your ball is already inside the minimum,
259

259

00:12:51,600  -->  00:12:56,170
and that way you don't even have to go through the long
260

260

00:12:56,170  -->  00:13:01,020
process of sampling to get to that recipe of how to adjust
261

261

00:13:01,020  -->  00:13:03,206
the curve, but you can just adjust the weights.
262

262

00:13:03,206  -->  00:13:06,770
Basically what I'd like to you to take away from here.
263

263

00:13:06,770  -->  00:13:10,220
We have an energy curve and the shape of this energy curve,
264

264

00:13:10,220  -->  00:13:12,760
is governed by the weights in the system, that's just
265

265

00:13:12,760  -->  00:13:14,130
how we design it.
266

266

00:13:14,130  -->  00:13:16,380
We also know that the way we've designed the system,
267

267

00:13:16,380  -->  00:13:18,820
is that the Restricted Boltzmann Machine,
268

268

00:13:18,820  -->  00:13:23,820
will aim to always get to the minimal energy state,
269

269

00:13:24,610  -->  00:13:26,810
the state with the minimal energy possible
270

270

00:13:26,810  -->  00:13:29,120
and what do we mean by get to that state?
271

271

00:13:29,120  -->  00:13:31,060
What's the process of getting to that state?
272

272

00:13:31,060  -->  00:13:33,300
Well, that process we just looked at that sampling
273

273

00:13:33,300  -->  00:13:36,440
that Gibbs sampling process, where we input the inputs
274

274

00:13:36,440  -->  00:13:38,830
then we get the hidden values and where we construct
275

275

00:13:38,830  -->  00:13:40,220
the inputs then we get the hidden values,
276

276

00:13:40,220  -->  00:13:41,360
reconstruct inputs and so on.
277

277

00:13:41,360  -->  00:13:44,680
Through that process, the system will always aim to get
278

278

00:13:44,680  -->  00:13:47,790
to values and its nodes which represent the lowest energy
279

279

00:13:47,790  -->  00:13:52,030
state possible. Now, what we want is we want to redesign
280

280

00:13:52,030  -->  00:13:54,504
the system hence, redesign the energy curve,
281

281

00:13:54,504  -->  00:13:59,383
so that the energy curve reflects also that the system
282

282

00:13:59,383  -->  00:14:04,383
is such that when we input our values, our training values,
283

283

00:14:05,230  -->  00:14:08,500
the system is already going to be in the lowest
284

284

00:14:08,500  -->  00:14:11,440
energy possible. And for that we use this formula.
285

285

00:14:11,440  -->  00:14:14,343
So we start with some randomly initialize weights,
286

286

00:14:15,477  -->  00:14:19,400
we input a value like one of our rows into the RBM,
287

287

00:14:19,400  -->  00:14:22,560
we go through this process of Gibbs sampling, we calculate
288

288

00:14:22,560  -->  00:14:25,670
this value, we find out how to adjust our curve.
289

289

00:14:25,670  -->  00:14:28,280
Plus on top of all of that, there's a shortcut.
290

290

00:14:28,280  -->  00:14:30,350
There's a shortcut that we don't actually have to go through
291

291

00:14:30,350  -->  00:14:32,260
to the very end of the sampling process,
292

292

00:14:32,260  -->  00:14:35,410
we can just do two pauses, we go first pause, second pause
293

293

00:14:35,410  -->  00:14:38,270
and so we do a CD one, Contrastive Divergence one,
294

294

00:14:38,270  -->  00:14:40,474
and that will tell us how to adjust the curve.
295

295

00:14:40,474  -->  00:14:44,400
That's the essence of it all,
296

296

00:14:44,400  -->  00:14:48,558
I can appreciate it's a very complex process,
297

297

00:14:48,558  -->  00:14:51,110
Contrastive Divergence, there's math behind it,
298

298

00:14:51,110  -->  00:14:54,300
there's steps involved and there's lots of understanding.
299

299

00:14:54,300  -->  00:14:57,140
But in an intuitive way, that is what's happening.
300

300

00:14:57,140  -->  00:15:00,970
We're trying to adjust the energy curve by modifying
301

301

00:15:00,970  -->  00:15:05,330
the weights, in order to facilitate a system,
302

302

00:15:05,330  -->  00:15:08,870
or create a system which
303

303

00:15:08,870  -->  00:15:11,100
in the best way possible resembles
304

304

00:15:11,100  -->  00:15:14,810
our input values, our training values and we do that,
305

305

00:15:14,810  -->  00:15:17,890
using this recipe formula over here.
306

306

00:15:17,890  -->  00:15:21,800
And if you'd like to get into more depth on this topic,
307

307

00:15:21,800  -->  00:15:24,116
on Contrastive Divergence, it's very interesting,
308

308

00:15:24,116  -->  00:15:26,870
there's a couple of papers that you can look at.
309

309

00:15:26,870  -->  00:15:30,547
The first paper is a great paper by Jeffrey Hinton
310

310

00:15:30,547  -->  00:15:34,720
and others 2006, it's called a fast learning algorithm
311

311

00:15:34,720  -->  00:15:37,590
for deep belief nets. And you can see exactly the diagram
312

312

00:15:37,590  -->  00:15:39,014
here which we discussed.
313

313

00:15:39,014  -->  00:15:43,500
It's a good paper to get you started into the process.
314

314

00:15:43,500  -->  00:15:47,700
Another paper if you'd like to get a bit more mathematical
315

315

00:15:47,700  -->  00:15:50,700
on the Contrastive Divergence and really understand the math
316

316

00:15:50,700  -->  00:15:53,730
behind it, and what's exactly going on with the gradients
317

317

00:15:53,730  -->  00:15:56,470
and so on, a good paper to look at is called
318

318

00:15:56,470  -->  00:15:59,480
Notes on Contrastive Divergence, it's not actually a paper
319

319

00:15:59,480  -->  00:16:03,163
it's just some notes is a three pager by Oliver Woodford
320

320

00:16:03,163  -->  00:16:06,780
I'm not sure I think it's 2012 but I couldn't find out
321

321

00:16:06,780  -->  00:16:09,310
exactly the date, but it looks like it's 2012.
322

322

00:16:09,310  -->  00:16:13,250
So Notes on Contrastive Divergence by Oliver Woodford date
323

323

00:16:13,250  -->  00:16:17,000
might be 2012 or not and of course the link is over there
324

324

00:16:17,000  -->  00:16:19,530
and I'll include this in the additional resources.
325

325

00:16:19,530  -->  00:16:21,850
So have a look at those if you'd like to dig
326

326

00:16:21,850  -->  00:16:24,140
into Contrastive Divergence a bit more.
327

327

00:16:24,140  -->  00:16:25,850
Hope you enjoyed today's tutorial and I look forward
328

328

00:16:25,850  -->  00:16:28,500
to see you next time, until then enjoy Deep Learning.
