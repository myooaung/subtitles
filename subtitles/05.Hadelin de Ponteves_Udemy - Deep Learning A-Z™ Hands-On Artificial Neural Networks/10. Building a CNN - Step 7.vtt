WEBVTT
1
1

00:00:00.120  -->  00:00:02.570
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.570  -->  00:00:04.560
Three steps down, one more to go
3

3

00:00:04.560  -->  00:00:06.480
and this one final step that we have to do
4

4

00:00:06.480  -->  00:00:09.550
is the full connection step, which basically consists
5

5

00:00:09.550  -->  00:00:11.390
of making a classic ANN
6

6

00:00:11.390  -->  00:00:13.760
composed of some fully-connected layers.
7

7

00:00:13.760  -->  00:00:15.780
And so why do we need to finish by this?
8

8

00:00:15.780  -->  00:00:19.090
Well, that's because we managed to convert our input image
9

9

00:00:19.090  -->  00:00:20.577
into this one-dimensional vector
10

10

00:00:20.577  -->  00:00:24.060
that contains some informations of the spatial structure
11

11

00:00:24.060  -->  00:00:26.960
or of some pixel patterns in the image
12

12

00:00:26.960  -->  00:00:28.510
and now what we have to do, of course,
13

13

00:00:28.510  -->  00:00:31.970
is to use this input vector as the input layer
14

14

00:00:31.970  -->  00:00:34.180
of a classic artificial neural network
15

15

00:00:34.180  -->  00:00:36.420
because a classic artificial neural network,
16

16

00:00:36.420  -->  00:00:38.590
as we saw it, can be a great classifier
17

17

00:00:38.590  -->  00:00:40.180
for nonlinear problems.
18

18

00:00:40.180  -->  00:00:43.410
And since image classifications is a nonlinear problem,
19

19

00:00:43.410  -->  00:00:46.900
well, it will make a perfect job here to classify the images
20

20

00:00:46.900  -->  00:00:50.540
and tell if each image is the image of a cat or of a dog.
21

21

00:00:50.540  -->  00:00:53.240
So right now, since we already have our input layer,
22

22

00:00:53.240  -->  00:00:54.367
the only thing that we have to do left
23

23

00:00:54.367  -->  00:00:56.720
is to create a hidden layer,
24

24

00:00:56.720  -->  00:00:59.050
that is what we call a fully-connected layer.
25

25

00:00:59.050  -->  00:01:01.050
So we will do that exactly as we did it
26

26

00:01:01.050  -->  00:01:03.520
when we built our classic artificial neural network
27

27

00:01:03.520  -->  00:01:04.930
in the previous section.
28

28

00:01:04.930  -->  00:01:07.140
And then we will need to add the output layer,
29

29

00:01:07.140  -->  00:01:08.530
just composed of one node
30

30

00:01:08.530  -->  00:01:11.070
because this is a binary outcome, cat or dog.
31

31

00:01:11.070  -->  00:01:13.630
And same, we will do it exactly as we did it
32

32

00:01:13.630  -->  00:01:15.050
in the previous section.
33

33

00:01:15.050  -->  00:01:15.980
So let's do it.
34

34

00:01:15.980  -->  00:01:18.710
We are gonna start by adding the hidden layer,
35

35

00:01:18.710  -->  00:01:20.800
that is, our fully-connected layer.
36

36

00:01:20.800  -->  00:01:23.987
And so again, we take our classifier, paste,
37

37

00:01:26.090  -->  00:01:28.730
and then dot, and then we use the add method,
38

38

00:01:28.730  -->  00:01:32.270
then parenthesis, and then remember what we have to do.
39

39

00:01:32.270  -->  00:01:35.050
We need to use the Dense function.
40

40

00:01:35.050  -->  00:01:36.590
That is the function that we use
41

41

00:01:36.590  -->  00:01:38.820
to add a fully-connected layer.
42

42

00:01:38.820  -->  00:01:40.970
So parenthesis again,
43

43

00:01:40.970  -->  00:01:43.060
and now remember what we need to start with.
44

44

00:01:43.060  -->  00:01:45.400
The first parameter that we need to input here
45

45

00:01:45.400  -->  00:01:49.226
is output underscore dim, then equals,
46

46

00:01:49.226  -->  00:01:53.480
and then, and now remember, this outputting parameter
47

47

00:01:53.480  -->  00:01:55.830
is the number of nodes in the hidden layer.
48

48

00:01:55.830  -->  00:01:57.810
So now there is this big question again,
49

49

00:01:57.810  -->  00:02:00.060
how many nodes do we need to input here?
50

50

00:02:00.060  -->  00:02:01.740
So remember in the previous section we saw
51

51

00:02:01.740  -->  00:02:03.436
that there was no rule of thumb
52

52

00:02:03.436  -->  00:02:06.410
to choose a number of nodes in the hidden layer,
53

53

00:02:06.410  -->  00:02:08.160
but we saw that a common practice
54

54

00:02:08.160  -->  00:02:10.440
that leads to good results most of the time
55

55

00:02:10.440  -->  00:02:12.580
is to choose a number of hidden nodes
56

56

00:02:12.580  -->  00:02:14.270
between the number of input nodes
57

57

00:02:14.270  -->  00:02:16.060
and the number of output nodes.
58

58

00:02:16.060  -->  00:02:18.450
So here we have tons of input nodes
59

59

00:02:18.450  -->  00:02:21.230
because, you know, we built 32 feature maps
60

60

00:02:21.230  -->  00:02:23.297
and each feature map contains many cells,
61

61

00:02:23.297  -->  00:02:25.480
so even if then we apply max pooling
62

62

00:02:25.480  -->  00:02:27.300
to reduce the size of our feature maps
63

63

00:02:27.300  -->  00:02:29.050
and dividing by two, well then,
64

64

00:02:29.050  -->  00:02:31.300
when we proceed to flattening to put all the cells
65

65

00:02:31.300  -->  00:02:33.710
of the feature maps into this one single vector
66

66

00:02:33.710  -->  00:02:36.240
that is the input layer of our fully-connected layer,
67

67

00:02:36.240  -->  00:02:39.110
well we end up with a lot of input nodes.
68

68

00:02:39.110  -->  00:02:42.040
So we are not gonna count all of them right now,
69

69

00:02:42.040  -->  00:02:44.890
we just know that we shouldn't take a too-small number,
70

70

00:02:44.890  -->  00:02:48.260
and therefore we are gonna choose here 128.
71

71

00:02:48.260  -->  00:02:50.150
Remember this choice of numbers results
72

72

00:02:50.150  -->  00:02:52.650
from experimentation and we need to choose a number
73

73

00:02:52.650  -->  00:02:55.620
that is not too small to make the classifier a good model
74

74

00:02:55.620  -->  00:02:57.540
and also not too big to not make it
75

75

00:02:57.540  -->  00:02:59.240
too highly compute-intensive.
76

76

00:02:59.240  -->  00:03:01.000
And therefore we need to experiment.
77

77

00:03:01.000  -->  00:03:04.340
But by experimenting on this outputting parameter,
78

78

00:03:04.340  -->  00:03:07.380
we realize that a number around 100 is a good choice. `
79

79

00:03:07.380  -->  00:03:09.860
We could have picked 100, but it is a common practice
80

80

00:03:09.860  -->  00:03:12.060
to pick a power of two.
81

81

00:03:12.060  -->  00:03:15.300
So we will go with this 128 hidden nodes
82

82

00:03:15.300  -->  00:03:17.560
in the hidden layer, and so now let's move on
83

83

00:03:17.560  -->  00:03:20.320
to the second and last parameter that we need to input
84

84

00:03:20.320  -->  00:03:23.850
in this Dense function to add this fully-connected layer
85

85

00:03:23.850  -->  00:03:25.120
as the hidden layer.
86

86

00:03:25.120  -->  00:03:29.489
And this second parameter is the activation function,
87

87

00:03:29.489  -->  00:03:33.270
of course, because the nodes in this hidden layer
88

88

00:03:33.270  -->  00:03:35.420
that we are adding are like neurons
89

89

00:03:35.420  -->  00:03:36.930
that we need to activate
90

90

00:03:36.930  -->  00:03:39.540
according to how much they can pass on the signal.
91

91

00:03:39.540  -->  00:03:42.160
That is, according to how much it is relevant for them
92

92

00:03:42.160  -->  00:03:44.670
to pass on their contribution to the final vote
93

93

00:03:44.670  -->  00:03:46.280
of the predicted class.
94

94

00:03:46.280  -->  00:03:47.800
And as in the previous section,
95

95

00:03:47.800  -->  00:03:49.530
since this is the hidden layer,
96

96

00:03:49.530  -->  00:03:51.600
well, the activation function that we'll use
97

97

00:03:51.600  -->  00:03:54.520
is going to be the rectifier activation function
98

98

00:03:54.520  -->  00:03:55.780
that is called ReLU.
99

99

00:03:56.870  -->  00:03:57.703
And that's it.
100

100

00:03:57.703  -->  00:03:59.630
That adds the fully-connected layer,
101

101

00:03:59.630  -->  00:04:02.260
that is the hidden layer, in our CNN.
102

102

00:04:02.260  -->  00:04:03.990
And then the last layer that we need
103

103

00:04:03.990  -->  00:04:06.230
to add now is the output layer.
104

104

00:04:06.230  -->  00:04:08.070
And to add this output layer efficiently,
105

105

00:04:08.070  -->  00:04:10.719
we need to take this line here, copy it,
106

106

00:04:10.719  -->  00:04:14.410
paste it below, and now we just need to change the value
107

107

00:04:14.410  -->  00:04:17.000
of output_dim, and the activation function.
108

108

00:04:17.000  -->  00:04:18.540
Because now we are not using
109

109

00:04:18.540  -->  00:04:20.430
the rectifier activation function.
110

110

00:04:20.430  -->  00:04:23.080
To return the probabilities of each class,
111

111

00:04:23.080  -->  00:04:26.270
now it's going to be the sigmoid function.
112

112

00:04:26.270  -->  00:04:27.850
Remember this is the sigmoid function
113

113

00:04:27.850  -->  00:04:31.250
because we have a binary outcome, cats or dog.
114

114

00:04:31.250  -->  00:04:33.850
And if we had an outcome with more than two categories,
115

115

00:04:33.850  -->  00:04:36.730
well, we would need to use the softmax activation function.
116

116

00:04:36.730  -->  00:04:38.220
But here we have a binary outcome,
117

117

00:04:38.220  -->  00:04:41.740
so it's the sigmoid activation function.
118

118

00:04:41.740  -->  00:04:44.380
Perfect, all good for the activation function.
119

119

00:04:44.380  -->  00:04:45.930
And now of course, we must not forget
120

120

00:04:45.930  -->  00:04:48.290
to change the outputting parameter,
121

121

00:04:48.290  -->  00:04:51.640
because of course we're not expecting 128 nodes
122

122

00:04:51.640  -->  00:04:52.860
in the output layer.
123

123

00:04:52.860  -->  00:04:54.300
We are just expecting one node
124

124

00:04:54.300  -->  00:04:56.614
that is going to be the predicted probability
125

125

00:04:56.614  -->  00:05:01.250
of one class, and this class can be the dog or the cat.
126

126

00:05:01.250  -->  00:05:04.120
So let's replace 128 by one.
127

127

00:05:04.120  -->  00:05:06.190
Here we go, and now that's done.
128

128

00:05:06.190  -->  00:05:08.340
We are done with the full connection step.
129

129

00:05:08.340  -->  00:05:10.520
Because this full connection step only consisted
130

130

00:05:10.520  -->  00:05:12.010
of adding the fully-connected layer,
131

131

00:05:12.010  -->  00:05:14.870
that is the hidden layer, and then the output layer
132

132

00:05:14.870  -->  00:05:16.720
to get the final predictions.
133

133

00:05:16.720  -->  00:05:17.750
So we are ready.
134

134

00:05:17.750  -->  00:05:22.730
We can select these two lines here and execute.
135

135

00:05:22.730  -->  00:05:26.040
And now here we go, the full connection step is done.
136

136

00:05:26.040  -->  00:05:29.190
And now eventually all our layers were correctly added
137

137

00:05:29.190  -->  00:05:31.750
to our convolutional neural network.
138

138

00:05:31.750  -->  00:05:35.530
And so now let's have a quick look at what we created.
139

139

00:05:35.530  -->  00:05:36.363
Here it is.
140

140

00:05:36.363  -->  00:05:39.160
That's the beautiful convolutional neural network
141

141

00:05:39.160  -->  00:05:42.180
that we just built through these four steps.
142

142

00:05:42.180  -->  00:05:44.210
So now we just need to compile the whole model,
143

143

00:05:44.210  -->  00:05:47.100
and that's what we'll do in the next tutorial.
144

144

00:05:47.100  -->  00:05:48.863
Until then, enjoy deep learning.
