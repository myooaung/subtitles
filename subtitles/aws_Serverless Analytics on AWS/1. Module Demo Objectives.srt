1
00:00:00,000 --> 00:00:00,604
Hi.

2
00:00:00,604 --> 00:00:03,019
It's Kim Schmidt again.

3
00:00:03,019 --> 00:00:08,277
In the last module, we went over AWS Glue in detail.

4
00:00:08,277 --> 00:00:11,143
This module is primary a demo module,

5
00:00:11,143 --> 00:00:17,739
where you're really going to get hands-on experience using AWS Glue.

6
00:00:17,739 --> 00:00:22,881
This module's objectives, in order to teach you all about AWS Glue,

7
00:00:22,881 --> 00:00:24,057
include the following,

8
00:00:24,057 --> 00:00:28,188
we're going to upload a Python script that contains the

9
00:00:28,188 --> 00:00:34,100
transformation code we'll use during an AWS Glue job that will

10
00:00:34,100 --> 00:00:37,035
transform the data from MySQL to Parquet format,

11
00:00:37,035 --> 00:00:43,011
so the data is ready for analysis at any time in the future.

12
00:00:43,011 --> 00:00:47,062
We're going to create another CloudFormation stack.

13
00:00:47,062 --> 00:00:48,653
This time we'll call it GlueStack,

14
00:00:48,653 --> 00:00:53,819
that will configure and provision our AWS Glue infrastructure.

15
00:00:53,819 --> 00:00:58,418
We're going to use two AWS Glue crawlers,

16
00:00:58,418 --> 00:01:01,854
one will crawl the Amazon Aurora database we created earlier,

17
00:01:01,854 --> 00:01:07,024
and populate the Glue Data Catalog with the MySQL data schema.

18
00:01:07,024 --> 00:01:12,066
The second crawler will craw the output of the AWS Glue jobs

19
00:01:12,066 --> 00:01:15,144
transformations that the Python script created.

20
00:01:15,144 --> 00:01:20,856
The second crawler takes the Parquet-formatted data in our Amazon S3

21
00:01:20,856 --> 00:01:26,244
processed zone bucket and populates the Glue Data Catalog with a new

22
00:01:26,244 --> 00:01:29,912
table in Parquet format versus MySQL format,

23
00:01:29,912 --> 00:01:34,955
and that also has the same join of the films and

24
00:01:34,955 --> 00:01:39,085
films_category tables just like we did when we used a

25
00:01:39,085 --> 00:01:42,184
third-party SQL client in a previous module.

26
00:01:42,184 --> 00:01:46,562
The intent here is to show how much easier it is to use AWS

27
00:01:46,562 --> 00:01:52,032
Glue to keep a central metadata repository with all your raw

28
00:01:52,032 --> 00:01:54,806
data versus having to log in remotely.

29
00:01:54,806 --> 00:01:59,013
Let the Glue crawlers perform their magic by crawling your

30
00:01:59,013 --> 00:02:01,681
data and populating the Glue Data Catalog.

31
00:02:01,681 --> 00:02:06,915
We're going to work with an AWS Glue job that performs the ETL

32
00:02:06,915 --> 00:02:13,799
transformations using the Python script that we upload to Amazon S3.

33
00:02:13,799 --> 00:02:17,509
We're going to make two separate entries in the Glue data catalog,

34
00:02:17,509 --> 00:02:23,675
one from each crawler differentiated by prefixes to the table names.

35
00:02:23,675 --> 00:02:24,546
And lastly,

36
00:02:24,546 --> 00:02:30,265
we'll use the data in the S3 processed zone bucket that's in

37
00:02:30,265 --> 00:02:33,568
Parquet format loaded into Amazon Athena,

38
00:02:33,568 --> 00:02:37,802
and use standard SQL to run some queries.

39
00:02:37,802 --> 00:02:47,000
In the next section, we'll be creating and configuring an S3 bucket to upload our Python script to.

