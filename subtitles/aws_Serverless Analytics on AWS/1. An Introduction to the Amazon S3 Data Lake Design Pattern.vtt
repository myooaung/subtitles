WEBVTT
1
00:00:00.000 --> 00:00:01.137
Hi everyone.

2
00:00:01.137 --> 00:00:03.411
It's Kim Schmidt again.

3
00:00:03.411 --> 00:00:07.689
In the last module, we covered Amazon Athena in detail.

4
00:00:07.689 --> 00:00:08.958
In this module,

5
00:00:08.958 --> 00:00:14.667
I'm going to introduce you to the Amazon S3 Data

6
00:00:14.667 --> 00:00:16.717
Lake log-centric design pattern,

7
00:00:16.717 --> 00:00:22.458
so you can AI and ML your apps and business processes while

8
00:00:22.458 --> 00:00:25.608
simultaneously future-proofing your data.

9
00:00:25.608 --> 00:00:31.907
In this section, I'll introduce you to the Amazon S3 Data Lake design pattern.

10
00:00:31.907 --> 00:00:36.981
On this slide is a bare schematic of an S3 data lake design pattern.

11
00:00:36.981 --> 00:00:42.299
S3 data lakes emphasize using immutable logs and materialized views,

12
00:00:42.299 --> 00:00:46.936
the definitions of which I'll explain in a subsequent slide.

13
00:00:46.936 --> 00:00:51.087
You use interactive and batch predictions on slow-moving data,

14
00:00:51.087 --> 00:00:55.092
and you use real-time predictions on fast-moving data.

15
00:00:55.092 --> 00:00:58.454
If you're building real-time analytic systems that

16
00:00:58.454 --> 00:01:03.273
use real-time or streaming data, the general architecture is like this.

17
00:01:03.273 --> 00:01:08.372
You take your streaming data and put it into Amazon Kinesis data streams.

18
00:01:08.372 --> 00:01:13.847
You can also use Amazon Kinesis Analytics to do streaming ETL,

19
00:01:13.847 --> 00:01:18.337
and then put the results into another Kinesis data stream or

20
00:01:18.337 --> 00:01:22.368
Kinesis Firehose for further downstream processing.

21
00:01:22.368 --> 00:01:24.727
In order to analyze streaming data,

22
00:01:24.727 --> 00:01:30.958
you're going to use either an Amazon Kinesis client library application,

23
00:01:30.958 --> 00:01:37.955
AWS Lambda, or Spark Streaming on Amazon Elastic Map Reduce based on your needs.

24
00:01:37.955 --> 00:01:42.274
If you do real-time predictions using your machine-learning models on the fly,

25
00:01:42.274 --> 00:01:46.333
you're using Amazon AI, and if the models see something wrong,

26
00:01:46.333 --> 00:01:50.229
you can send alerts to notify the appropriate people,

27
00:01:50.229 --> 00:01:54.569
and you can persist the data to S3 with a lambda

28
00:01:54.569 --> 00:01:57.848
function for further processing.

29
00:01:57.848 --> 00:01:59.224
Note the word log here.

30
00:01:59.224 --> 00:02:03.754
On the right in the tall green rectangle are storage services

31
00:02:03.754 --> 00:02:08.135
where you're decoupling processing from storage.

32
00:02:08.135 --> 00:02:10.856
If you have downstream applications presenting

33
00:02:10.856 --> 00:02:13.682
visualizations or other downstream applications,

34
00:02:13.682 --> 00:02:18.956
usually the data will come from a database or Elasticsearch.

35
00:02:18.956 --> 00:02:22.396
Note the words app state or materialized view here.

36
00:02:22.396 --> 00:02:23.072
In some cases,

37
00:02:23.072 --> 00:02:26.620
you might want to fan out your Kinesis streams for other

38
00:02:26.620 --> 00:02:30.769
downstream processing or to send to other consumers.

39
00:02:30.769 --> 00:02:34.976
Similarly, if you're doing batch or interactive analysis,

40
00:02:34.976 --> 00:02:39.880
you put your data into Kinesis Firehose and transport

41
00:02:39.880 --> 00:02:45.259
that data to Amazon Elasticsearch, or an Amazon Redshift data warehouse,

42
00:02:45.259 --> 00:02:48.781
or you can put your data into files,

43
00:02:48.781 --> 00:02:53.109
and then have Amazon Elastic Map Reduce or Amazon

44
00:02:53.109 --> 00:02:55.703
Athena process it and consume it,

45
00:02:55.703 --> 00:03:01.798
or you can run classic Hive or Pig on top of S3 for batch analytics.

46
00:03:01.798 --> 00:03:02.699
Putting this together,

47
00:03:02.699 --> 00:03:05.965
you can have interactive and batch predictions that

48
00:03:05.965 --> 00:03:09.330
include Amazon AI in the pipeline,

49
00:03:09.330 --> 00:03:13.045
and those advanced insights can be incorporated into the

50
00:03:13.045 --> 00:03:15.498
answers you give to your customers.

51
00:03:15.498 --> 00:03:16.696
In the next section,

52
00:03:16.696 --> 00:03:31.000
I'll get more specific into what immutable logs and materialized views mean, as well as putting together batch and real-time or streaming processing.

