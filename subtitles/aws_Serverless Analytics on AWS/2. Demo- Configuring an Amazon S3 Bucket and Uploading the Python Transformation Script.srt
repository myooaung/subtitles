1
00:00:01,393 --> 00:00:06,799
Log into your AWS Console and navigate to the Amazon S3 Console.

2
00:00:06,799 --> 00:00:09,024
What we're going to do is we're going to create a bucket

3
00:00:09,024 --> 00:00:11,184
and upload the Python script to that.

4
00:00:11,184 --> 00:00:12,957
So click on Create bucket.

5
00:00:12,957 --> 00:00:17,972
If you created your Aurora stack in the US East Ohio region,

6
00:00:17,972 --> 00:00:24,931
name your bucket like you see here, aurora-glue-etl-script-east2,

7
00:00:24,931 --> 00:00:29,573
and the reason for doing this is because it's hardcoded in the YAML script,

8
00:00:29,573 --> 00:00:33,015
which in practice isn't a good idea, but for this course,

9
00:00:33,015 --> 00:00:36,660
just use the naming conventions that I suggest.

10
00:00:36,660 --> 00:00:42,968
If you created your Aurora stack in US West 2 Oregon region,

11
00:00:42,968 --> 00:00:46,293
just leave off the -east2.

12
00:00:46,293 --> 00:00:51,452
Filter your S3 buckets b Date created so that your most

13
00:00:51,452 --> 00:00:54,591
recently-created bucket appears on the top.

14
00:00:54,591 --> 00:01:00,366
Click on it, and then click on Upload and Add files.

15
00:01:00,366 --> 00:01:05,343
Click the sakila- etl.py script, and click Upload.

16
00:01:05,343 --> 00:01:09,091
Once the object appears, click on it, and go to Properties,

17
00:01:09,091 --> 00:01:10,317
and then click on metadata.

18
00:01:10,317 --> 00:01:14,242
If you don't have any metadata in the Properties

19
00:01:14,242 --> 00:01:19,386
folder of your S3 object Python script, you would add metadata,

20
00:01:19,386 --> 00:01:24,185
but since I already have something there and it's the wrong value,

21
00:01:24,185 --> 00:01:27,587
I'm going to select it and say Edit.

22
00:01:27,587 --> 00:01:34,601
You can choose from quite a few different pre-populated keys that AWS provides,

23
00:01:34,601 --> 00:01:37,620
but the value here is wrong, we want content type,

24
00:01:37,620 --> 00:01:41,791
but we want the value to be binary/octet-stream,

25
00:01:41,791 --> 00:01:44,043
and then click Save.

26
00:01:44,043 --> 00:01:48,004
At this point, we have our S3 bucket configured properly,

27
00:01:48,004 --> 00:01:52,713
and it contains the Python object sakila_etl.

28
00:01:52,713 --> 00:01:57,286
We're ready to create the second CloudFormation script Glue script.

29
00:01:57,286 --> 00:01:59,595
Before we do the next demo,

30
00:01:59,595 --> 00:02:03,124
I thought it would be helpful to walk through the Python

31
00:02:03,124 --> 00:02:07,739
script that we just uploaded to the S3 bucket,

32
00:02:07,739 --> 00:02:15,000
so you fully understand the transformations that are going to be done and how you write scripts to do that.

