1
00:00:00,994 --> 00:00:01,507
In this section,

2
00:00:01,507 --> 00:00:05,850
I'm going to walk you through what we're going to be doing in the next demo.

3
00:00:05,850 --> 00:00:11,482
Now that we have the Parquet-formatted data in the S3 target zone bucket,

4
00:00:11,482 --> 00:00:16,366
it needs to be crawled in order to populate the Glue Data Catalog

5
00:00:16,366 --> 00:00:19,954
with the data that has the transformations.

6
00:00:19,954 --> 00:00:23,162
The first step that we need to do is add a new crawler to

7
00:00:23,162 --> 00:00:25,321
crawl the transformed Parquet data.

8
00:00:25,321 --> 00:00:31,363
You're looking at the Add crawler window where you give the crawler a name.

9
00:00:31,363 --> 00:00:36,047
I called mine s3_film_cat_crawler because to me,

10
00:00:36,047 --> 00:00:39,633
that's descriptive of what the crawler is going to do.

11
00:00:39,633 --> 00:00:44,131
On the left are the series of steps that are necessary to add a new crawler.

12
00:00:44,131 --> 00:00:47,385
We have to set a data source for the crawler to crawl.

13
00:00:47,385 --> 00:00:50,464
So in the Add a data store window,

14
00:00:50,464 --> 00:00:53,881
we're going to choose S3 from the drop-down list,

15
00:00:53,881 --> 00:00:59,337
and we're going to choose for the Include Path when you choose the little

16
00:00:59,337 --> 00:01:03,658
folder icon in the left image surrounded by a red rectangle,

17
00:01:03,658 --> 00:01:09,671
and you choose S3, all of your S3 buckets will appear in a popup.

18
00:01:09,671 --> 00:01:13,163
The screenshot on the right is the popup that appears

19
00:01:13,163 --> 00:01:15,512
where you choose your S3 bucket.

20
00:01:15,512 --> 00:01:20,068
And when it asks you if you want to add another data store, say no.

21
00:01:20,068 --> 00:01:24,478
The reason that you're asked that is because a crawler can crawl

22
00:01:24,478 --> 00:01:27,211
multiple different data stores simultaneously,

23
00:01:27,211 --> 00:01:29,375
even with disparate data.

24
00:01:29,375 --> 00:01:31,595
Next you choose an IAM role,

25
00:01:31,595 --> 00:01:36,071
and we're going to choose the IAM role that AWS created for us.

26
00:01:36,071 --> 00:01:42,810
It will have the name GlueStack-AWSGlueRole in the name, so choose that one.

27
00:01:42,810 --> 00:01:46,370
Crawlers can be configured to run on a schedule.

28
00:01:46,370 --> 00:01:51,942
In the screenshot you see is a partial listing of the drop-down list that

29
00:01:51,942 --> 00:01:57,473
shows various time intervals to run your crawler regularly.

30
00:01:57,473 --> 00:02:01,665
And this screenshot shows the rest of the drop-down choices that you have.

31
00:02:01,665 --> 00:02:04,930
And the red arrow is pointing to custom,

32
00:02:04,930 --> 00:02:11,118
so you can create a custom schedule for this crawler or any crawler to run.

33
00:02:11,118 --> 00:02:14,923
This is a screenshot of the crawler's output configuration.

34
00:02:14,923 --> 00:02:17,758
Notice that the database is sakiladb,

35
00:02:17,758 --> 00:02:23,240
and we're going to add a prefix of S3_ to differentiate the

36
00:02:23,240 --> 00:02:26,803
tables that were created by the first crawler,

37
00:02:26,803 --> 00:02:32,204
which had a prefix of Glue_ from the results of this crawler,

38
00:02:32,204 --> 00:02:36,610
and it will have the prefix of S3_.

39
00:02:36,610 --> 00:02:39,412
This is like the review page for the crawler,

40
00:02:39,412 --> 00:02:43,450
where you are looking at the Name, the Database,

41
00:02:43,450 --> 00:02:47,481
the prefix, the Service role, the Data store,

42
00:02:47,481 --> 00:02:52,538
and the Include path, and now this is a screenshot of the crawler running.

43
00:02:52,538 --> 00:02:56,855
I ran the crawler previously to get these screenshots for you.

44
00:02:56,855 --> 00:03:00,774
The next section is a pure demo section,

45
00:03:00,774 --> 00:03:03,473
where we'll create the new crawler, and we'll run it,

46
00:03:03,473 --> 00:03:09,743
and then look at the Glue Data Catalog to see the Parquet formatted results.

47
00:03:09,743 --> 00:03:12,436
And since it's in a great columnar format,

48
00:03:12,436 --> 00:03:21,000
we'll take a quick peek at Amazon Athena because Parquet format is perfect for running analytics.

