1
00:00:01,127 --> 00:00:02,029
In this section,

2
00:00:02,029 --> 00:00:05,535
we're going to have another demo where we'll run the

3
00:00:05,535 --> 00:00:09,240
Glue crawler and the Glue ETL job.

4
00:00:09,240 --> 00:00:14,721
What we'll be doing in this demo includes running our Glue crawler

5
00:00:14,721 --> 00:00:20,094
on the Amazon Aurora sakila relational database,

6
00:00:20,094 --> 00:00:24,322
review the table results in the AWS Glue database

7
00:00:24,322 --> 00:00:31,483
sakiladb after the crawler runs, confirm the sakila data has tables in data,

8
00:00:31,483 --> 00:00:34,223
then we'll run the Glue job to perform the ETL

9
00:00:34,223 --> 00:00:37,318
transformations on the data in the Glue Data Catalog,

10
00:00:37,318 --> 00:00:43,786
and then we'll look at the Amazon CloudWatch logs after the Glue job runs,

11
00:00:43,786 --> 00:00:47,231
and then we'll look at our S3 bucket that we specified was

12
00:00:47,231 --> 00:00:51,509
the target destination for the Glue job to put the results

13
00:00:51,509 --> 00:00:54,596
of the transformations into.

14
00:00:54,596 --> 00:00:59,227
Log into your AWS Management Console, and start typing glue.

15
00:00:59,227 --> 00:01:01,724
When you see it, click on it.

16
00:01:01,724 --> 00:01:04,460
In the database view of the Glue Console,

17
00:01:04,460 --> 00:01:08,016
if we click on our sakiladb and then click on tables,

18
00:01:08,016 --> 00:01:09,715
we don't have any tables there yet,

19
00:01:09,715 --> 00:01:13,918
so let's go down to crawler and click the little

20
00:01:13,918 --> 00:01:17,236
checkbox to the left of the Aurora crawler,

21
00:01:17,236 --> 00:01:20,421
and then you have the option to run the crawler,

22
00:01:20,421 --> 00:01:21,229
click that.

23
00:01:21,229 --> 00:01:23,640
You can see that it's attempting to run the crawler by

24
00:01:23,640 --> 00:01:26,294
reading the message at the top of the screen.

25
00:01:26,294 --> 00:01:31,943
Now your message is that the crawler is running, and the Status is Starting.

26
00:01:31,943 --> 00:01:36,397
If you click on the name of the crawler, the status here is Running.

27
00:01:36,397 --> 00:01:37,396
It goes pretty quick,

28
00:01:37,396 --> 00:01:40,623
so let's go back to the main console and just keep refreshing the page

29
00:01:40,623 --> 00:01:43,301
until it's finished crawling the Aurora database.

30
00:01:43,301 --> 00:01:49,589
And you can see now that it shows a status of 3 minutes elapsed,

31
00:01:49,589 --> 00:01:53,000
and now you can see that the status is stopping.

32
00:01:53,000 --> 00:01:57,156
The crawler status now says Ready, which means it's complete,

33
00:01:57,156 --> 00:02:02,413
and if you read the message, it says 23 tables were created,

34
00:02:02,413 --> 00:02:05,126
0 tables were updated, and right from here,

35
00:02:05,126 --> 00:02:10,604
you can click on the sakiladb database link in the message,

36
00:02:10,604 --> 00:02:14,321
which is a shortcut versus using the links in the left menu.

37
00:02:14,321 --> 00:02:15,620
So let's click on it.

38
00:02:15,620 --> 00:02:18,550
And look at that, we have data.

39
00:02:18,550 --> 00:02:21,642
Toto, I don't think we're in Kansas anymore.

40
00:02:21,642 --> 00:02:26,604
So we have the 23 tables in our sakila database right now,

41
00:02:26,604 --> 00:02:32,066
which are the same tables that we saw when using the third-party SQL client.

42
00:02:32,066 --> 00:02:36,540
The sakila database has the columns of Name of the tables,

43
00:02:36,540 --> 00:02:40,016
the Database that they belong to, the Location,

44
00:02:40,016 --> 00:02:46,174
the Classification, and Last updated, as well as a column for Deprecated.

45
00:02:46,174 --> 00:02:51,284
The classification for all these tables are MySQL because they all came

46
00:02:51,284 --> 00:02:55,866
from the Amazon Aurora MySQL relational database.

47
00:02:55,866 --> 00:02:58,938
Let's click on one of these to view the table details.

48
00:02:58,938 --> 00:03:04,465
When we click on film, we see the details about this particular table,

49
00:03:04,465 --> 00:03:07,603
and notice you can edit the table from here,

50
00:03:07,603 --> 00:03:12,057
delete the table, view properties, and edit the schema.

51
00:03:12,057 --> 00:03:16,480
The main window is composed of separate sections,

52
00:03:16,480 --> 00:03:20,728
it shows various properties, various statistics,

53
00:03:20,728 --> 00:03:23,845
and it shows the schema of the table.

54
00:03:23,845 --> 00:03:27,145
Now if any of these had nested tables underneath them,

55
00:03:27,145 --> 00:03:32,542
it would be a link, and you could click on it to see the nested table.

56
00:03:32,542 --> 00:03:36,640
Notice here you can View the properties of this particular table,

57
00:03:36,640 --> 00:03:37,893
let's click on it.

58
00:03:37,893 --> 00:03:40,201
In the Properties popup that comes up,

59
00:03:40,201 --> 00:03:44,565
notice that it has the name of every column in the table,

60
00:03:44,565 --> 00:03:48,347
it also has the location, the format, any SerDes,

61
00:03:48,347 --> 00:03:53,451
and it used a default SerDes because we didn't specify one,

62
00:03:53,451 --> 00:03:58,791
it has information about parameters and about the crawlers,

63
00:03:58,791 --> 00:04:00,935
classification, and the type of table.

64
00:04:00,935 --> 00:04:03,803
Now that we have our first entry into the Glue Data Catalog,

65
00:04:03,803 --> 00:04:10,453
it's time to run our Glue job to perform ETL transformations.

66
00:04:10,453 --> 00:04:13,112
Click on Jobs in the left Glue menu,

67
00:04:13,112 --> 00:04:16,250
and when you see our GlueStack-aurora-etljob,

68
00:04:16,250 --> 00:04:20,384
click on the checkbox next to its row.

69
00:04:20,384 --> 00:04:24,827
From the Actions drop-down, choose Run job.

70
00:04:24,827 --> 00:04:30,235
From here, you can choose to add more properties before the job is run,

71
00:04:30,235 --> 00:04:33,463
but I'm just going to choose Run job.

72
00:04:33,463 --> 00:04:38,818
You can see the message that reads the GlueStack-aurora-etljob is running.

73
00:04:38,818 --> 00:04:41,638
Check the checkbox next to the job name again,

74
00:04:41,638 --> 00:04:46,340
and you can see that this job has a status of Running.

75
00:04:46,340 --> 00:04:53,734
Notice currently that it's running, and so far its Execution time is 16 seconds.

76
00:04:53,734 --> 00:04:57,316
Keep clicking the Refresh button from time to time,

77
00:04:57,316 --> 00:05:01,739
and right now the job Run status is Succeeded.

78
00:05:01,739 --> 00:05:05,749
Let's look at the Amazon CloudWatch logs for this job run.

79
00:05:05,749 --> 00:05:11,202
Look at all the free CloudWatch metrics that you get when you run AWS Glue jobs.

80
00:05:11,202 --> 00:05:15,329
Each of these are expandable with more information,

81
00:05:15,329 --> 00:05:19,592
and you can filter through here by words or by time,

82
00:05:19,592 --> 00:05:22,694
and you can also do a custom filter.

83
00:05:22,694 --> 00:05:25,670
Let's close this and go back to the Glue Console.

84
00:05:25,670 --> 00:05:29,856
Now that the job is done, let's go to the S3 Management Console.

85
00:05:29,856 --> 00:05:31,276
Filter by Date created,

86
00:05:31,276 --> 00:05:36,193
and navigate to whatever the name was that you input into the

87
00:05:36,193 --> 00:05:41,505
GlueStack as the S3 destination target bucket for the job to

88
00:05:41,505 --> 00:05:43,488
put the processed data into.

89
00:05:43,488 --> 00:05:44,866
Click on it.

90
00:05:44,866 --> 00:05:50,836
What you're looking at are all of the partitions that the Glue ETL job created,

91
00:05:50,836 --> 00:05:54,109
and notice they all have snappy compression,

92
00:05:54,109 --> 00:05:56,667
and they're all in Parquet format.

93
00:05:56,667 --> 00:06:01,992
So what we need to do now is create another crawler to crawl this target

94
00:06:01,992 --> 00:06:13,000
data source and put the results in the Glue Data Catalog. In the next section, I'll summarize exactly what happened in this demo.

