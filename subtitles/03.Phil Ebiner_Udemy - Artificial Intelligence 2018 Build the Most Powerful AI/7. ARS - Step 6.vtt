WEBVTT
ï»¿1
00:00:00.300 --> 00:00:06.140
Hello and welcome to this final step of building the normalizer class.

2
00:00:06.210 --> 00:00:12.120
So in the previous two tutorials we made the first two methods of this normalizer class the first method

3
00:00:12.120 --> 00:00:19.200
that initializes all the variables of the future normalizer objects two vectors of zeros with the same

4
00:00:19.200 --> 00:00:23.410
number of zeros as number of inputs in the vector of input states.

5
00:00:23.670 --> 00:00:30.750
Then a second method observed which each time we observe a new state will do an online update of the

6
00:00:30.750 --> 00:00:32.200
mean and variance.

7
00:00:32.310 --> 00:00:34.360
After they were initialized to zeros.

8
00:00:34.680 --> 00:00:41.290
And finally now that we have the mean and variance we are ready to normalize our state.

9
00:00:41.310 --> 00:00:43.670
Each time we observe a new state as well.

10
00:00:43.980 --> 00:00:49.670
So still we have to understand that we are doing an on line normalization of the states but the online

11
00:00:49.680 --> 00:00:54.110
part of the consideration in the normalization of the state was already done.

12
00:00:54.180 --> 00:00:56.040
Thanks to this observed method.

13
00:00:56.340 --> 00:01:04.200
So let's do this let's define the final function the final method of this normalized class which will

14
00:01:04.200 --> 00:01:10.220
be normalize and which is going to take as input self.

15
00:01:10.530 --> 00:01:17.890
Because we're going to use the variables of the object here self and this time the values of our inputs

16
00:01:18.390 --> 00:01:25.260
because indeed we'll take the values of our input into the input state vector we'll subtract them by

17
00:01:25.260 --> 00:01:31.470
the mean which was computed here through an online data the mean and divide them by the square root

18
00:01:31.560 --> 00:01:33.100
of the variance.

19
00:01:33.120 --> 00:01:38.460
So indeed we need to take again our inputs values and that's it.

20
00:01:38.460 --> 00:01:42.900
Colin Let's get the observed mean first.

21
00:01:43.080 --> 00:01:50.770
So I'm going to make any variable here so that nothing is lost or you know erased abs mean meaning the

22
00:01:50.790 --> 00:01:52.650
observed mean.

23
00:01:52.650 --> 00:01:58.340
Which is of course self that mean which was computed previously in the observed method.

24
00:01:59.340 --> 00:02:05.390
Then I'm going to get to observe standard deviation which is the root.

25
00:02:05.460 --> 00:02:11.160
The square root of the variance you know you have to subtract by the mean and then divided by the standard

26
00:02:11.160 --> 00:02:14.180
deviation which is the square root of the variance.

27
00:02:14.520 --> 00:02:20.610
And in order to take the square root of something you can use non-pay which has a shortcut and P and

28
00:02:20.610 --> 00:02:27.780
then take the s q r t function which will take as input the variable you want to take the world and

29
00:02:27.780 --> 00:02:30.240
that is of course our variance.

30
00:02:30.330 --> 00:02:35.610
Self-taught var which was already updated previously in the observed method.

31
00:02:35.830 --> 00:02:36.730
Self-taught var.

32
00:02:36.810 --> 00:02:43.130
So now we have you observed mean where we have our new input when we reach a new state.

33
00:02:43.500 --> 00:02:46.200
And we also have the observe standard deviation.

34
00:02:46.470 --> 00:02:54.720
So we have everything we need to normalize our states and since we want to normalize method to return

35
00:02:55.050 --> 00:02:58.690
the normalized values at the state.

36
00:02:58.940 --> 00:03:01.620
Well we are going to return that directly.

37
00:03:01.890 --> 00:03:08.380
And these are our values of the states and counted at this specific time T where we're at.

38
00:03:08.430 --> 00:03:19.650
And that's our input minus the mean of the values of the states divided by the standard deviation observed

39
00:03:19.710 --> 00:03:26.390
SDD at this specific time t that we've reached and that normalizes our states.

40
00:03:26.550 --> 00:03:29.170
As I told you this was very easy.

41
00:03:29.220 --> 00:03:36.960
Now that we did the most difficult part meaning the 0 9 update on an condition of the mean and variance.

42
00:03:36.960 --> 00:03:37.590
All right perfect.

43
00:03:37.630 --> 00:03:39.700
So that's a good first step done.

44
00:03:39.720 --> 00:03:46.310
We are done with this normalizer class which means that we are also done with this part of the paper

45
00:03:46.320 --> 00:03:47.580
we did this part.

46
00:03:47.790 --> 00:03:53.940
So we did the first optimization of the future performance and now we're going to move on to the heart

47
00:03:54.180 --> 00:03:56.130
of the algorithm which is this one.

48
00:03:56.130 --> 00:04:02.120
And to do that we'll make a policy class inside which will apply some of the algorithm here.

49
00:04:02.420 --> 00:04:06.960
But what you have to understand is that the future object of this policy class that we're about to make

50
00:04:06.960 --> 00:04:13.440
in the future to Toyo's will represent the policy and it will have some method which will allow to date

51
00:04:13.530 --> 00:04:20.220
the weight of the policy of you know the perception representing the policy after applying some perturbations

52
00:04:20.310 --> 00:04:24.750
in some directions in opposite directions in order to increase the reward.

53
00:04:24.750 --> 00:04:25.800
The best way.

54
00:04:25.800 --> 00:04:28.240
So let's do all this in the next tutorial.

55
00:04:28.320 --> 00:04:29.910
And until then enjoy AI.

