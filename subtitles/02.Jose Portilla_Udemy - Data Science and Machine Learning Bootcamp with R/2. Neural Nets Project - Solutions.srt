1

00:00:00,480  -->  00:00:05,240
Hello everyone and welcome to the neural networks project solutions lecture this lecture.

2

00:00:05,250  -->  00:00:09,510
We're going to go over the solutions for the neural network project and explain the code as we go along

3

00:00:09,510  -->  00:00:09,990
.

4

00:00:09,990  -->  00:00:13,510
Let's go ahead and jump to our studio to get started.

5

00:00:13,840  -->  00:00:14,160
OK.

6

00:00:14,160  -->  00:00:21,010
Here in our studio we're going to need to do is read in the data into a data frame.

7

00:00:21,210  -->  00:00:25,560
Go ahead and make sure you are in the right location and the data we're going to be using is the bank

8

00:00:25,590  -->  00:00:30,660
underscore no underscore data that see every file and that's located under the sea as we files for M-L

9

00:00:30,660  -->  00:00:33,570
projects folder may go ahead and read that data.

10

00:00:33,580  -->  00:00:39,690
And then again to check the head of the data frame just make sure everything looks OK and it's looking

11

00:00:39,690  -->  00:00:39,860
good.

12

00:00:39,860  -->  00:00:45,900
We have the image variants the skewness the kurtosis the entropy of the image and then we have the class

13

00:00:45,930  -->  00:00:49,550
which indicates whether or not a bank note is authentic.

14

00:00:49,620  -->  00:00:55,530
We're going to go ahead and do is skip the edia the exploratory data analysis because you by now can

15

00:00:55,530  -->  00:00:57,930
create whatever visualizations you're interested in.

16

00:00:58,110  -->  00:01:02,060
We'll go ahead and skip this step and move on to the train test split.

17

00:01:02,280  -->  00:01:03,900
Hopefully by now you're pretty familiar.

18

00:01:04,020  -->  00:01:08,280
The train split her perform it will just go ahead and review it one last time.

19

00:01:08,310  -->  00:01:13,560
Go ahead and call CAA tools and then if you want your results to match mine as far as the random split

20

00:01:13,740  -->  00:01:25,680
you go out and set the sea to 101 then we'll say split sample that split pass in our data frame and

21

00:01:25,680  -->  00:01:30,420
then we want to pass in a column I always pass in the column that we're going to be predicting which

22

00:01:30,420  -->  00:01:38,340
is the class of our data frame and then go ahead and set the split ratio to 0.7 then we'll go ahead

23

00:01:38,370  -->  00:01:45,670
and do the train test split by calling subset on our data frame and saying split equals true.

24

00:01:45,720  -->  00:01:55,400
And we'll go ahead and say test subset data frame split equals false.

25

00:01:55,410  -->  00:01:56,250
All right.

26

00:01:56,250  -->  00:01:57,540
So that's the train to split.

27

00:01:57,540  -->  00:02:00,740
Let's go ahead and run the source.

28

00:02:00,810  -->  00:02:02,540
We have our trained test now.

29

00:02:02,760  -->  00:02:07,020
Let's go ahead and actually start training our neural net in a push just a little over to the right

30

00:02:07,350  -->  00:02:15,420
and clear the council member we got a call library neural nets and then what I encourage to do is actually

31

00:02:15,420  -->  00:02:20,930
just check out the help documentation for the neural net function.

32

00:02:20,940  -->  00:02:25,020
There's a lot of really good stuff in the documentation and I always recommend taking the time to read

33

00:02:25,020  -->  00:02:32,550
it especially because these sort of topics weren't covered in the introduction to discoloring book.

34

00:02:32,580  -->  00:02:32,820
All right.

35

00:02:32,820  -->  00:02:37,740
Once you went ahead and browsed the documentation Well we'll go ahead and do is train our neural net

36

00:02:39,040  -->  00:02:41,290
and clear this consul and say N-N

37

00:02:44,040  -->  00:02:54,990
neural nets and I'm trying to predict the class based off of the other features which is the image variance

38

00:02:56,940  -->  00:02:58,080
the image skew

39

00:03:01,160  -->  00:03:05,340
the image kurtosis and then the entropy.

40

00:03:05,370  -->  00:03:11,790
And keep in mind you could use that trick that I showed you in the previous lecture in order to create

41

00:03:11,790  -->  00:03:16,680
this formula automatically since there's only four features I went ahead and just did it manually.

42

00:03:16,680  -->  00:03:18,420
I want to pass in my data.

43

00:03:18,600  -->  00:03:24,750
My data is just my training data for now and then we can go ahead and pasan a certain amount of hidden

44

00:03:24,930  -->  00:03:26,160
neurons.

45

00:03:26,160  -->  00:03:32,970
It doesn't it's hard to tell what the correct number of hidden neurons is or if you show multiple levels

46

00:03:32,970  -->  00:03:33,720
.

47

00:03:33,720  -->  00:03:39,100
I'm going to go ahead and just do what we did last time which was five and three.

48

00:03:39,300  -->  00:03:44,860
And then lastly we want to say linear output and in this case since this classification it's not linear

49

00:03:44,860  -->  00:03:44,880
.

50

00:03:44,880  -->  00:03:49,020
So we're going to go ahead and say false.

51

00:03:49,180  -->  00:03:49,440
All right.

52

00:03:49,440  -->  00:03:50,650
Now that's been trained.

53

00:03:50,700  -->  00:03:55,110
Let's go ahead and grab the predictions on the test dataset.

54

00:03:55,430  -->  00:03:59,340
When I say predicted and n values computes

55

00:04:02,400  -->  00:04:07,380
and then I'm just going to pass in the features columns 1 through 4 member column 5 is the actual class

56

00:04:08,180  -->  00:04:08,900
to compute.

57

00:04:09,000  -->  00:04:11,170
And there I have my predictive values.

58

00:04:11,640  -->  00:04:15,660
All right now that we have those predicted values we can go ahead and do is check.

59

00:04:15,660  -->  00:04:25,200
Head of the predicted and n values and then grab the net result which you notice is that they're all

60

00:04:25,200  -->  00:04:26,820
still probabilities.

61

00:04:26,820  -->  00:04:33,600
And this is basically the probability of belonging to the zero class or the one class where we can go

62

00:04:33,600  -->  00:04:37,570
ahead and do is apply the round function to the predicted values.

63

00:04:37,620  -->  00:04:41,790
So you only see zeros and ones is your predicted classes.

64

00:04:41,790  -->  00:04:55,500
Let's go ahead and say predictions and we'll use a supply on the predicted end and values net results

65

00:04:57,510  -->  00:05:01,580
and I'm going to apply the round function.

66

00:05:01,860  -->  00:05:04,170
Now if I check the head of those predictions

67

00:05:10,470  -->  00:05:14,490
we have all zeros and set of these small probabilities.

68

00:05:14,580  -->  00:05:20,580
Something to note here is that we didn't actually scale or normalized our data in this case our data

69

00:05:20,880  -->  00:05:27,270
as far as the actual columns were relatively at the same order of magnitude entropy was a little smaller

70

00:05:27,270  -->  00:05:33,450
but it would still usually around either negative 5 up to 5.

71

00:05:33,790  -->  00:05:38,430
And that means it wasn't a huge deal to not normalizer data you should normalize your data.

72

00:05:38,430  -->  00:05:45,590
However if there's large ranges of min and max values between the column features in this case they

73

00:05:45,600  -->  00:05:47,480
were all relatively similar.

74

00:05:47,550  -->  00:05:52,380
So I didn't actually perform the normalization but I just wanted to make it clear that you should usually

75

00:05:52,380  -->  00:05:57,480
perform some sort of scaling or normalization on your data before putting it into that neural net.

76

00:05:57,750  -->  00:06:00,890
But we're about to see why that wasn't actually a huge deal.

77

00:06:00,900  -->  00:06:07,010
Let's go ahead and use table to essentially create a confusion matrix on a say table

78

00:06:09,360  -->  00:06:18,960
predictions and grab the test class column and you'll notice that we actually predicted perfectly from

79

00:06:18,960  -->  00:06:21,320
the tests that we did extremely well.

80

00:06:21,570  -->  00:06:25,970
And as a data scientist you should always be very suspicious of perfect results.

81

00:06:25,980  -->  00:06:30,930
Now we're going to do now is compare against a random force model just to make sure we're not going

82

00:06:30,930  -->  00:06:36,000
crazy here because one we didn't actually scale normalize the data and we still got perfect results

83

00:06:36,000  -->  00:06:37,270
.

84

00:06:37,320  -->  00:06:43,830
Let's go ahead and create a random forest library or model to compare it to a call the random forest

85

00:06:43,830  -->  00:06:45,010
library.

86

00:06:46,110  -->  00:06:53,250
And then what I'm going to go ahead and do is run the code that's in your notebook and it sets the class

87

00:06:53,510  -->  00:06:56,370
column as a factor column.

88

00:06:56,640  -->  00:06:59,990
And basically I'm going to paste it here.

89

00:07:00,420  -->  00:07:02,790
Again this is from your notebook

90

00:07:05,600  -->  00:07:11,880
and what it does is it takes a factor version of the day of class and then it goes ahead and read redoes

91

00:07:11,880  -->  00:07:13,590
the train test split for that.

92

00:07:13,590  -->  00:07:18,870
And that's going to be for the random forest when you go out and run that we already know that our neural

93

00:07:18,870  -->  00:07:20,640
net predicted perfectly well the test set.

94

00:07:20,640  -->  00:07:26,070
Let's see how our Random forest does when we go ahead and do control L here.

95

00:07:26,070  -->  00:07:35,310
Going to creates the RF model the random force model cholerae in the forest pass in class

96

00:07:40,970  -->  00:07:48,360
and I want to predict off of everything and then say data equals train.

97

00:07:48,440  -->  00:07:51,170
All right our random force has been built and trained.

98

00:07:51,180  -->  00:07:57,150
Let's go ahead and use the Predict function to get the predicted values from our RF model and a call

99

00:07:57,210  -->  00:08:06,060
RF credit for the random force predictions predict pass in my model and then passes the test set.

100

00:08:06,080  -->  00:08:07,530
And now let's call a table

101

00:08:10,950  -->  00:08:15,760
and pass in the actual class.

102

00:08:15,780  -->  00:08:16,090
OK.

103

00:08:16,110  -->  00:08:21,780
What we notice here is that the random forest model performed very well but not perfectly well and that's

104

00:08:21,780  -->  00:08:23,880
a great reality check for us.

105

00:08:23,880  -->  00:08:29,010
It makes sense now that the neural net performed perfectly well because a random force was also able

106

00:08:29,010  -->  00:08:34,470
to perform extremely well with just a little bit of a type 1 error and type 2 error.

107

00:08:34,470  -->  00:08:39,780
All right I hope you enjoy the project and I sincerely hope you enjoy the entire machine learning section

108

00:08:40,080  -->  00:08:41,390
of this course.

109

00:08:41,400  -->  00:08:47,700
What I would suggest doing now is go ahead and go back to the other old projects you've done and compare

110

00:08:47,820  -->  00:08:53,640
the performance of the machine learning algorithm discussed for that project versus a neural net or

111

00:08:53,640  -->  00:08:59,280
maybe even go to the UC Irvine machine learning repository and test out your various skills for machine

112

00:08:59,280  -->  00:09:03,170
learning algorithms on data sets that we haven't discussed yet.

113

00:09:03,610  -->  00:09:06,330
Okay hopefully enjoying the machine learning project.

114

00:09:06,330  -->  00:09:10,380
Thanks so much for completing the section of the course.

115

00:09:10,380  -->  00:09:11,410
Thanks everyone.
