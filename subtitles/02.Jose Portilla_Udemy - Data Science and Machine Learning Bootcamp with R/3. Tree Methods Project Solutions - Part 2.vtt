WEBVTT
1

00:00:00.600  -->  00:00:06.320
Hello everyone and welcome to the tree methods project solutions part 2 video lecture in part 1.

2

00:00:06.330  -->  00:00:12.690
We went ahead and got data perform some exploratory data analysis on that college data set using cheesy

3

00:00:12.690  -->  00:00:13.550
plot too.

4

00:00:13.680  -->  00:00:18.360
And then we actually built a decision tree model using that data as a training set.

5

00:00:18.360  -->  00:00:21.950
Now we're going to go ahead and move on to using a random forced model.

6

00:00:22.200  -->  00:00:24.810
Let's go ahead and jump to our studio and get started.

7

00:00:24.810  -->  00:00:26.420
OK here we are our studio.

8

00:00:26.460  -->  00:00:32.030
In order to get started make sure we have the following lines of code actually calling the college data

9

00:00:32.030  -->  00:00:36.720
set and then actually having the train test split.

10

00:00:36.720  -->  00:00:41.220
You should already have all this code from part 1 of the video lecture series.

11

00:00:41.220  -->  00:00:42.210
If not go ahead.

12

00:00:42.210  -->  00:00:43.380
Go back to Part 1.

13

00:00:43.410  -->  00:00:45.870
In case you need a refresher on any of this code.

14

00:00:45.930  -->  00:00:47.270
But now that we're at part two.

15

00:00:47.280  -->  00:00:50.880
Let's go ahead and train some random forest models.

16

00:00:50.880  -->  00:00:56.670
First thing we have to do is called the random forest library and go ahead and push this on the console

17

00:00:56.670  -->  00:00:58.380
.

18

00:00:58.650  -->  00:01:02.670
And we're going to go ahead a car ran the forest library and now that we've called the random forest

19

00:01:02.670  -->  00:01:07.530
library we can just use the random force function to build out the model and then create a variable

20

00:01:07.590  -->  00:01:12.150
called our model called the random forest function.

21

00:01:12.240  -->  00:01:14.590
And then to pasan a formula.

22

00:01:14.850  -->  00:01:23.070
I want to predict pry it off of all the features and my data is going to be my training data that train

23

00:01:23.270  -->  00:01:24.230
variable.

24

00:01:24.480  -->  00:01:33.320
Something else I told you to do in the assignment was to say importance equal to true and the importance

25

00:01:33.320  -->  00:01:39.180
is just the extractor function for variable importance measures that are produced by the random forest

26

00:01:39.990  -->  00:01:46.350
function and go ahead and look up the reading and the notes for what the actual important measurement

27

00:01:46.350  -->  00:01:47.070
is.

28

00:01:47.070  -->  00:01:53.250
It's something called the genie in spirit or a genie impurity index and it's essentially a measure of

29

00:01:53.490  -->  00:01:58.830
how much information you get when you do a certain split on a certain value and how you actually judge

30

00:01:58.830  -->  00:01:59.730
that.

31

00:02:00.220  -->  00:02:05.970
You can go ahead and train the random forest model and now since it's trained we can just directly call

32

00:02:06.600  -->  00:02:12.300
it confusion matrix off the model by saying R.F. thought model and then call confusion.

33

00:02:12.300  -->  00:02:14.720
Remember this is just a confusion on the training data.

34

00:02:14.730  -->  00:02:19.960
Not on our test data we have to actually make predictions in order to do the true confusion matrix offer

35

00:02:19.970  -->  00:02:21.210
test data.

36

00:02:21.360  -->  00:02:26.270
But here we can get a good idea of the classification error just based off the training data.

37

00:02:26.400  -->  00:02:27.980
Looks like we're doing okay.

38

00:02:28.170  -->  00:02:30.620
And but it also looks like it's easier to predict that.

39

00:02:30.630  -->  00:02:33.810
Yes it is a private school versus.

40

00:02:33.810  -->  00:02:35.790
No it's not private school.

41

00:02:35.790  -->  00:02:40.110
And that makes sense because there was a lot more private school data in the actual data set.

42

00:02:40.680  -->  00:02:48.180
And because we saw the importance equal to true what we can also do now is say our model and call importance

43

00:02:48.240  -->  00:02:55.640
offer the model and this will give you the importance based off of the Genie impurity index values.

44

00:02:55.680  -->  00:03:00.840
Again go ahead and read in the introduction system learning sections for random forest and tree methods

45

00:03:00.860  -->  00:03:01.000
.

46

00:03:01.170  -->  00:03:06.300
If you want more background that how to actually interpret these numbers here since it's outside of

47

00:03:06.300  -->  00:03:10.590
the scope of this particular lecture as far as the mathematics of how to interpret these values we'll

48

00:03:10.650  -->  00:03:12.100
go ahead and just move on.

49

00:03:12.180  -->  00:03:17.070
So you're actually getting predictions which is a little more practical.

50

00:03:17.280  -->  00:03:25.110
I'm going to go ahead and say our f creds for random for his predictions.

51

00:03:25.350  -->  00:03:32.970
I'm going to call predict pass in y our model and then pass in my test set.

52

00:03:33.600  -->  00:03:43.540
And then after that I could just call table Hassen RF predictions and from my test data and the private

53

00:03:43.550  -->  00:03:44.520
.

54

00:03:45.030  -->  00:03:46.480
And there we have it.

55

00:03:46.900  -->  00:03:52.830
And if we compare this to the confusion matrix for that decision tree it performed just slightly better

56

00:03:52.890  -->  00:03:59.040
on certain aspect and the pending on your definition of better weather trying to minimize type 1 or

57

00:03:59.040  -->  00:04:04.800
Type 2 error the random forest model performed either better or maybe slightly worse than the decision

58

00:04:04.800  -->  00:04:05.600
tree.

59

00:04:05.670  -->  00:04:12.750
But depending on what the cost is associated versus a type 1 Type 2 error or precision recon accuracy

60

00:04:13.080  -->  00:04:19.410
it kind of depends on how to actually interpret the goodness of fit or the evaluation of your model

61

00:04:19.420  -->  00:04:19.770
.

62

00:04:20.100  -->  00:04:24.580
But we can see that overall it did perform pretty well.

63

00:04:25.140  -->  00:04:25.730
Okay.

64

00:04:25.860  -->  00:04:27.190
That's it for the random forest.

65

00:04:27.200  -->  00:04:29.040
Part 2 section is pretty short.

66

00:04:29.210  -->  00:04:34.680
But hopefully you realize that it just makes it very easy to create ran the forest models or any other

67

00:04:34.680  -->  00:04:37.980
model for that matter or just passing in the formula.

68

00:04:37.980  -->  00:04:41.670
The training data and then getting predictions off your model.

69

00:04:41.800  -->  00:04:43.290
OK thanks Ron.

70

00:04:43.290  -->  00:04:46.150
Hope you enjoy the project and I'll see at the next lecture.
