WEBVTT
1

00:00:01.050  -->  00:00:06.330
Hello everyone and welcome to the introduction to try methods lecture in this lecture we'll discuss

2

00:00:06.330  -->  00:00:10.970
decision trees and ran them for us as machine learning models.

3

00:00:11.280  -->  00:00:16.380
If you want the mathematics behind these topics go ahead and read Chapter eight of an introduction to

4

00:00:16.380  -->  00:00:18.290
statistical learning.

5

00:00:19.110  -->  00:00:23.850
Let's start off with a thought experiment to give a little bit of motivation behind why we would use

6

00:00:23.910  -->  00:00:26.380
a decision tree method.

7

00:00:26.430  -->  00:00:31.710
Imagine that I play tennis every Saturday and I always invite a friend to come play with me.

8

00:00:31.740  -->  00:00:35.340
Sometimes a friend shows up sometimes not for him.

9

00:00:35.370  -->  00:00:41.590
It depends on a variety of factors such as weather temperature humidity wind etc..

10

00:00:41.640  -->  00:00:47.940
I start keeping track of these features and whether or not he showed up to play with me and my data

11

00:00:47.940  -->  00:00:49.500
looks something like this.

12

00:00:49.500  -->  00:00:54.620
Each of these columns represents a feature and each of the rows represents a certain day.

13

00:00:54.660  -->  00:00:58.450
I go ahead and for each row or a day take note of features.

14

00:00:58.470  -->  00:01:01.330
And finally at the final column I have a label.

15

00:01:01.470  -->  00:01:07.290
Whether or not my friend came to play with me so he can see on the first day I have temperature mild

16

00:01:07.560  -->  00:01:08.570
a sunny outlook.

17

00:01:08.610  -->  00:01:09.860
Humidity of 80.

18

00:01:09.950  -->  00:01:10.440
Windy.

19

00:01:10.450  -->  00:01:13.540
It was not and my friend came out to play.

20

00:01:13.540  -->  00:01:19.380
However in Day 2 it was a hot temperature sunny outlook 75 humidity.

21

00:01:19.380  -->  00:01:22.500
It was windy and my friend did not comply with me.

22

00:01:22.560  -->  00:01:26.070
I CAN BUILD IT decision tree off of this process.

23

00:01:26.070  -->  00:01:31.830
I want to use this data to predict whether or not my friend is going to show up to play in an intuitive

24

00:01:31.830  -->  00:01:34.200
way to do this is through a decision tree.

25

00:01:34.200  -->  00:01:40.390
And this is what a decision tree may look like or the data I just showed in this tree.

26

00:01:40.410  -->  00:01:47.580
We have notes and notes split for the value of a certain attribute we can he can see here we have Outlook

27

00:01:47.580  -->  00:01:48.020
nodes.

28

00:01:48.030  -->  00:01:50.050
Humidity nodes and windy nodes.

29

00:01:50.070  -->  00:01:56.280
So each of these attributes are nodes and then we have edges edges are the outcome of a split to the

30

00:01:56.280  -->  00:01:58.080
next node.

31

00:01:59.000  -->  00:02:01.020
In this true we also have a root node.

32

00:02:01.290  -->  00:02:03.880
That's the note that performs the first split.

33

00:02:03.990  -->  00:02:07.720
So in this particular example the root node is the outlook feature.

34

00:02:07.810  -->  00:02:13.860
I performed the first split and I split into the possible categories of sunny overcast and rain and

35

00:02:13.860  -->  00:02:17.750
then those will lead to other nodes such as humidity and windy.

36

00:02:17.760  -->  00:02:24.150
The coloured nodes which are yeses and nos are leaves and those with terminal nose that predict the

37

00:02:24.300  -->  00:02:26.020
outcome.

38

00:02:27.150  -->  00:02:31.620
Let's go ahead and discuss intuition behind splitting on these nodes.

39

00:02:31.620  -->  00:02:39.240
Imagine now that I have this data with three features X Y and Z and I'm trying to predict two possible

40

00:02:39.240  -->  00:02:39.870
classes.

41

00:02:39.870  -->  00:02:47.970
Class A or Class B splitting on y gives us a clear separation between classes.

42

00:02:48.000  -->  00:02:54.210
So I asked Does Weichel one I get perfect separation of the two class instances of class A and the two

43

00:02:54.210  -->  00:02:56.620
instances of class B.

44

00:02:56.670  -->  00:02:57.810
Again if I go back.

45

00:02:57.810  -->  00:02:59.820
Check out this imaginary data.

46

00:02:59.820  -->  00:03:07.950
Note how the Y feature split the classes perfectly and also note how the x and z features don't split

47

00:03:07.950  -->  00:03:13.620
the classes perfectly meaning I could have also tried splitting on other features.

48

00:03:13.620  -->  00:03:21.430
First if I split on X first or Z first I don't get proper separation of those classes.

49

00:03:22.020  -->  00:03:26.780
Entropy and information gain are the mathematical methods of choosing the best split.

50

00:03:26.910  -->  00:03:31.920
Go ahead to refer to the reading assignment if you're interested in the mathematics behind the intuition

51

00:03:32.670  -->  00:03:38.520
but the actual intuition is just trying to choose the features that best split your data.

52

00:03:38.520  -->  00:03:43.090
This is called trying to maximize your information gain off of this split.

53

00:03:43.250  -->  00:03:48.840
Again refer to chapter 8 if you want more information on the mathematics behind these topics of entropy

54

00:03:48.850  -->  00:03:50.490
in information gain.

55

00:03:50.490  -->  00:03:56.520
Now let's talk about random force as a way to improve performance off single decision trees.

56

00:03:56.520  -->  00:04:02.010
The primary weakness of decision trees is that they don't tend to have the best predictive accuracy

57

00:04:02.010  -->  00:04:02.520
.

58

00:04:02.520  -->  00:04:07.740
This is partially due to the high variance meaning that different splits in the training data can lead

59

00:04:07.740  -->  00:04:09.750
to very different trees.

60

00:04:09.750  -->  00:04:15.660
Bagging is a general purpose procedure for reducing the variance of machine learning method that's discussed

61

00:04:15.750  -->  00:04:18.570
in Chapter 8 of the reading assignment.

62

00:04:18.570  -->  00:04:24.800
We can build off the idea of bagging by utilizing random force random force.

63

00:04:24.810  -->  00:04:29.090
It's just a slight variation of these Bagga trees that has even better performance.

64

00:04:29.100  -->  00:04:34.710
Which is what we're going to be discussing and coding out in our what we do have around the forest as

65

00:04:34.710  -->  00:04:41.460
we create an ensemble of decision trees using bootstrap samples of the training set bootstrap samples

66

00:04:41.460  -->  00:04:46.310
of the training set just means sampling from the training set with replacement.

67

00:04:46.710  -->  00:04:54.750
However we're building each tree each time a split is considered a random sample of M features is chosen

68

00:04:54.780  -->  00:04:59.710
as a split candidate from the full set of p features.

69

00:04:59.730  -->  00:05:06.630
The split is only allowed to use one of those m features a new random sample of features is chosen for

70

00:05:06.720  -->  00:05:11.470
every single tree every single split for classification.

71

00:05:11.610  -->  00:05:18.960
This M. random sample of M features is typically chosen to be the square root of P where P is the full

72

00:05:18.960  -->  00:05:22.040
set of features.

73

00:05:22.170  -->  00:05:25.910
You may be wondering what's the point of using random force.

74

00:05:26.280  -->  00:05:32.250
Well suppose there is one very strong feature in the dataset a feature that's really strong at predicting

75

00:05:32.280  -->  00:05:36.040
a certain class when using bagged trees.

76

00:05:36.220  -->  00:05:38.310
That's that bootstrap sampling.

77

00:05:38.310  -->  00:05:44.280
Most of the trees will use that feature as the top split that's going to result in ensemble of really

78

00:05:44.280  -->  00:05:51.330
similar trees that are highly correlated which is something you want to avoid averaging highly correlated

79

00:05:51.330  -->  00:05:54.930
quantities does not significantly reduce variance.

80

00:05:54.930  -->  00:06:02.040
I randomly leaving out candidate features from each split random force are going to d correlate to trees

81

00:06:02.040  -->  00:06:02.220
.

82

00:06:02.400  -->  00:06:07.800
Meaning making the trees in the pendant of each other such that that averaging process can reduce the

83

00:06:07.800  -->  00:06:10.270
variance of the resulting model.

84

00:06:10.500  -->  00:06:17.540
So we won't be affected by features that really strongly predict the class data.

85

00:06:17.550  -->  00:06:22.740
Let's go ahead and jump to our studio and begin to explore some examples of building out the sentries

86

00:06:22.740  -->  00:06:27.500
and ran the course then you'll have a project to test your own understanding.

87

00:06:27.510  -->  00:06:29.580
Thanks everyone and I'll see you at the next lecture
