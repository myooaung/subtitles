WEBVTT
1

00:00:00.690  -->  00:00:06.210
Hello everyone and welcome to part two of the linear regression with our lectures series in the previous

2

00:00:06.210  -->  00:00:11.790
lecture we went ahead and got our data and checked it out structure and summary the data and even did

3

00:00:11.790  -->  00:00:14.370
some visualizations off the data in part.

4

00:00:14.370  -->  00:00:19.920
We're going to focus on splitting that data into a training set and a testing set training a linear

5

00:00:19.940  -->  00:00:23.150
regression model and then interpret seeing the results of that model.

6

00:00:23.370  -->  00:00:26.070
Let's go ahead and jump to our studio and get started.

7

00:00:26.100  -->  00:00:28.450
OK so here we're our studio.

8

00:00:28.830  -->  00:00:30.930
Let's go ahead and continue on.

9

00:00:30.930  -->  00:00:37.410
Remember I've already read the C S V file the student dasht met that C S V separators a semi-colon and

10

00:00:37.410  -->  00:00:39.270
I say that as a variable d f.

11

00:00:39.300  -->  00:00:41.450
So that is my data frame for my data.

12

00:00:41.950  -->  00:00:47.500
I need to continue on and I want to split the data into a training set and a testing's set.

13

00:00:47.580  -->  00:00:52.740
The reason for this is that we're going to train our model on the training set and then we're going

14

00:00:52.740  -->  00:00:59.540
to use the test set to have predictions from the train model in order to do this.

15

00:00:59.550  -->  00:01:05.010
You're going to need to install the CA tools package if you don't already have it in order to do that

16

00:01:05.020  -->  00:01:05.080
.

17

00:01:05.130  -->  00:01:12.090
You just need to say install the packages see a capital T O L S C A tools.

18

00:01:12.090  -->  00:01:13.440
I've already installed it.

19

00:01:13.470  -->  00:01:17.760
Go ahead and run that line in your console to install that package.

20

00:01:17.850  -->  00:01:21.010
Let's go ahead and continue on.

21

00:01:21.060  -->  00:01:27.120
This package makes it really easy to randomly split up your data into a training set and a testing set

22

00:01:27.120  -->  00:01:28.240
.

23

00:01:28.580  -->  00:01:37.200
First thing you need to do is call library C-8 tools and then I'm going to go ahead and set a seed.

24

00:01:37.200  -->  00:01:44.160
The reason I'm setting a seed is that since the splits are going to be random I want you to be able

25

00:01:44.160  -->  00:01:47.040
to follow along and have the same results as me.

26

00:01:47.040  -->  00:01:52.590
In order to do that you can go ahead and set a random seed and with this random seed does is it make

27

00:01:52.590  -->  00:01:58.740
sure that the random numbers I have on my session will be the same as the random numbers of yours.

28

00:01:58.920  -->  00:02:04.080
So that however this dataset gets split randomly will actually occur the same for you.

29

00:02:04.080  -->  00:02:11.400
If you set your seeds 1 0 1 throughout the course whenever we're doing random splits on our data I'll

30

00:02:11.390  -->  00:02:13.360
always set the seed to 101.

31

00:02:13.410  -->  00:02:17.640
That way you can follow along with the exact same code.

32

00:02:17.640  -->  00:02:21.580
Next up we're going to split up the sample.

33

00:02:22.290  -->  00:02:25.450
And this is the main way you're going to use C-8 tools.

34

00:02:25.740  -->  00:02:32.790
You'll create a variable called sample and then what you're going to do is called the sample dot split

35

00:02:33.240  -->  00:02:33.820
function.

36

00:02:33.900  -->  00:02:36.040
And notice that it's in the C-8 tools.

37

00:02:36.040  -->  00:02:44.340
And what this does it just splits the data from a vector into two sets from a predefined ratio and then

38

00:02:44.340  -->  00:02:49.270
what you do is you take in your data frame and you pass in a column of your data frame.

39

00:02:49.560  -->  00:02:52.440
It'll actually work for any column in your data frame.

40

00:02:52.440  -->  00:02:57.880
But just by convention I like to pass in the column that we're trying to predict in this case.

41

00:02:57.900  -->  00:03:02.870
We're trying to predict the final third period score or grade of the student.

42

00:03:02.880  -->  00:03:04.600
So g 3.

43

00:03:04.770  -->  00:03:07.550
So I go ahead and pass in the column that I'm trying to predict.

44

00:03:07.680  -->  00:03:12.130
Again any column works for this but just by convention I like this because it's easier to read and I

45

00:03:12.120  -->  00:03:13.230
want to come back to this code.

46

00:03:13.230  -->  00:03:16.940
I remember what I'm actually trying to predict.

47

00:03:17.190  -->  00:03:24.870
Then we put in the split ratio the split ratio is going to be the percentage of the stadia that we want

48

00:03:24.870  -->  00:03:26.980
to use for training.

49

00:03:27.000  -->  00:03:34.740
I'll go ahead and use a split ratio of 0.7 which means I'll eventually use 70 percent of this as training

50

00:03:34.740  -->  00:03:39.870
data and 30 percent of it as testing data.

51

00:03:39.870  -->  00:03:47.250
What that does is it basically just creates a new column called sample on your data frame and it randomly

52

00:03:47.370  -->  00:03:51.380
puts in a boolean or logical value of either true or false.

53

00:03:51.510  -->  00:03:56.030
And you can use that to split into training and test data.

54

00:03:56.400  -->  00:04:03.600
That means I'm going to create a new variable called Train I will call the subset function pass in my

55

00:04:03.600  -->  00:04:08.250
data frame and my data frame after running the sample that split.

56

00:04:08.520  -->  00:04:14.160
What I'm going to do is say sample is equal to true

57

00:04:16.980  -->  00:04:22.020
which means that's the 70 percent of my data

58

00:04:25.320  -->  00:04:34.530
goes to training and 30 percent of my data will be test data going to make a new variable called test

59

00:04:36.350  -->  00:04:42.660
a subset of my data frame where the sample is equal to false.

60

00:04:42.660  -->  00:04:43.210
All right.

61

00:04:44.280  -->  00:04:47.610
And that's how you can split your data into a training and test set.

62

00:04:47.790  -->  00:04:49.850
Let's go ahead and review this real quickly.

63

00:04:49.920  -->  00:04:56.790
I call C.A. tools because it has the sampled out Split function I pass in a column of my data frame

64

00:04:56.790  -->  00:04:56.950
.

65

00:04:57.060  -->  00:04:59.460
I like the pass in the column that we're trying to predict.

66

00:04:59.550  -->  00:05:04.870
I set the split ratio that's going to be the actual ratio of sample equal to true.

67

00:05:05.130  -->  00:05:11.130
I said train equal to the subset of the data were samples equal to true and 30 the subset of data frame

68

00:05:11.130  -->  00:05:13.470
are samples equal to falls.

69

00:05:13.540  -->  00:05:18.660
Make sure you really understand these five lines of code because you're going to be using them really

70

00:05:18.660  -->  00:05:23.160
all the time when we're working with machine learning projects or on your own machine learning project

71

00:05:23.250  -->  00:05:29.280
exercises will basically always be splitting our data into training sets and testing sets and these

72

00:05:29.280  -->  00:05:33.080
five lines are going to be the lines you use over and over again to do it.

73

00:05:33.150  -->  00:05:37.030
Make sure you know where they are in the notebook so you can reference them.

74

00:05:37.080  -->  00:05:42.840
OK let's go ahead and move along and actually train our model here in this little script in this new

75

00:05:42.840  -->  00:05:43.370
tab.

76

00:05:43.440  -->  00:05:50.370
I have the general model formula and what it looks like in our We've actually already gone over this

77

00:05:50.370  -->  00:05:51.770
code and this formatting.

78

00:05:51.930  -->  00:05:57.160
Well let's go ahead and quickly remind ourselves how to actually build a linear regression model and

79

00:05:57.180  -->  00:06:01.210
are the first that is you decide what you're going to call your model.

80

00:06:01.230  -->  00:06:04.600
In this case I just call that model but you can choose any variable name you want.

81

00:06:04.830  -->  00:06:11.040
We have then our assignment operator and for a linear regression model you called the l m function linear

82

00:06:11.100  -->  00:06:12.350
model function.

83

00:06:12.480  -->  00:06:17.300
Then you pass in the feature column that you're trying to predict the tilde sign.

84

00:06:17.640  -->  00:06:22.160
And you can add up the features you want to use in order to make that prediction.

85

00:06:22.380  -->  00:06:28.500
And then you pass in the data source or if you want to use all the features in your data frame you just

86

00:06:28.520  -->  00:06:33.320
pasan y tilde and then a period and that will end up using all the features.

87

00:06:33.360  -->  00:06:35.680
That way you don't have to have a bunch of addition signs.

88

00:06:35.850  -->  00:06:41.510
If you have like 20 columns in your data frame you don't want to put 6 1 6 2 plus x 3 and so on.

89

00:06:41.520  -->  00:06:45.120
You can just write it period and that will take care of all the features for you.

90

00:06:45.120  -->  00:06:48.210
Let's go ahead and do this on our training data.

91

00:06:48.270  -->  00:06:54.000
So now we're out the build a model phase and actually believe I have that right here so let's go in

92

00:06:54.000  -->  00:06:56.180
and train and build

93

00:06:58.780  -->  00:07:00.750
model.

94

00:07:01.410  -->  00:07:11.820
That means I'm going to say model call L M I'm trying to predict the third period score so G-3 and I'm

95

00:07:11.820  -->  00:07:18.500
going to use all my data to predict this and I'm going to pass in my training data.

96

00:07:18.600  -->  00:07:19.760
So train.

97

00:07:20.040  -->  00:07:22.240
You can just pass and train as a second argument.

98

00:07:22.260  -->  00:07:25.930
Or if you want to do it more formally You can say data equals train.

99

00:07:25.980  -->  00:07:31.760
That's the same thing that's actually going to train and create the model.

100

00:07:31.890  -->  00:07:39.610
And then after that I'm going to go ahead and run the model and then interpret the model so I'll call

101

00:07:39.610  -->  00:07:46.010
a summary on the model and ask our studio to go ahead and print that out for me.

102

00:07:46.580  -->  00:07:50.240
And there's going to be a ton of mathematical information when we actually asked for the summary.

103

00:07:50.240  -->  00:07:54.900
The model will go over the few key points that we really want to know about.

104

00:07:54.930  -->  00:07:59.560
Let's go and run this control shift s make sure it all worked out OK.

105

00:07:59.580  -->  00:08:00.850
Looks like we're good here.

106

00:08:01.020  -->  00:08:05.970
Let's go ahead and check out what the summary actually looks like when it squeezes over the right.

107

00:08:06.300  -->  00:08:07.520
And let me run this one more times.

108

00:08:07.580  -->  00:08:12.120
Take care of all that space and push this up.

109

00:08:12.120  -->  00:08:18.720
So we ran our model on the training set and we get this nice summary out of our model and you do that

110

00:08:18.720  -->  00:08:20.640
when you call at the very end.

111

00:08:20.730  -->  00:08:26.310
Summary models we printed out that summary of the model you get all this useful mathematical information

112

00:08:26.310  -->  00:08:26.710
.

113

00:08:26.940  -->  00:08:32.550
You want the full background of what each of these basically residuals coefficients and other mathematical

114

00:08:32.550  -->  00:08:33.360
terms mean.

115

00:08:33.510  -->  00:08:38.530
You can check out the introduction to statistical learning book and you can check out the note book

116

00:08:38.850  -->  00:08:45.690
which basically has a huge table of model interpretation with all the important residuals coefficients

117

00:08:45.720  -->  00:08:49.560
and other terms in the description and how to actually interpret them.

118

00:08:50.150  -->  00:08:56.970
For now I'm going to just briefly walk you through the important parts of this summary call first you

119

00:08:56.970  -->  00:09:01.560
get the call line and the call line just tells you what the formula actually was and what the model

120

00:09:01.560  -->  00:09:02.240
was.

121

00:09:02.250  -->  00:09:07.470
This case is essentially what you just typed in tells you your formula was G-3 re-addicted using all

122

00:09:07.470  -->  00:09:09.960
the features in your data equal string.

123

00:09:10.260  -->  00:09:11.680
And that's what's really nice about our.

124

00:09:11.700  -->  00:09:15.930
That's all you have to do to actually build a linear regression model which is kind of crazy can do

125

00:09:15.930  -->  00:09:19.800
this all in one line and have it be so intuitive.

126

00:09:19.800  -->  00:09:24.850
The next line are the residuals and the actual statistical summary for the residuals.

127

00:09:25.050  -->  00:09:29.580
But the residuals are it's the difference between the actual values of the variable you're predicting

128

00:09:29.760  -->  00:09:35.980
and the predicting values of your regression the predicted values of your Russian for most regressions

129

00:09:36.340  -->  00:09:40.690
you want your residuals to look like a normal distribution when plotted and will actually show you how

130

00:09:40.690  -->  00:09:43.250
to plot out those residuals in just a moment.

131

00:09:43.510  -->  00:09:48.940
But for right now you can basically just think of it as a dart board a good model is going to hit the

132

00:09:48.950  -->  00:09:51.410
bullseye some of the time but not every time.

133

00:09:51.730  -->  00:09:57.100
And when it doesn't hit the bull's eye it's missing in all of the other buckets evenly not just hitting

134

00:09:57.100  -->  00:10:02.830
in a certain bucket like the 16 bucket of it are bored and it also misses close to the bull's eye as

135

00:10:02.830  -->  00:10:05.080
opposed to missing all over the dartboard.

136

00:10:05.290  -->  00:10:09.940
And that's how you can essentially interpret the residuals it's just a difference between the actual

137

00:10:09.940  -->  00:10:16.000
data points and then the regression line and this residual information is just this to school information

138

00:10:16.000  -->  00:10:16.780
off of it.

139

00:10:16.810  -->  00:10:23.080
So the minimum the residual value the maximum residual value in the median and the first and third quintiles

140

00:10:23.760  -->  00:10:28.900
will go ahead and explore this later because we'll actually discover something interesting for a model

141

00:10:29.230  -->  00:10:32.010
after plotting out the residuals.

142

00:10:32.020  -->  00:10:38.020
Let's go ahead and move along with the coefficients here.

143

00:10:38.020  -->  00:10:44.800
We can see under the Kleefisch section we have every single feature or column variable that was in our

144

00:10:44.800  -->  00:10:45.670
data frame.

145

00:10:45.670  -->  00:10:47.850
Now we have a bunch of values for them.

146

00:10:47.890  -->  00:10:55.030
We have the estimated coefficient the standard error of that coefficient estimate the T value of the

147

00:10:55.030  -->  00:10:58.520
coefficient estimate and then the variable p value.

148

00:10:58.930  -->  00:11:04.340
Let's go ahead and just briefly explain what each of these are the estimate coefficient.

149

00:11:04.340  -->  00:11:10.150
Here is the value of slope calculated by the regression and it might seem a little confusing at first

150

00:11:10.150  -->  00:11:12.720
that this intercept also has a value.

151

00:11:12.940  -->  00:11:16.530
But just think of it as a slope that's always multiplied by 1.

152

00:11:16.570  -->  00:11:22.680
This number will obviously vary the Pentagon how large the magnitude of the variable was put in.

153

00:11:22.810  -->  00:11:29.470
Which means let's say you're predicting housing prices and one variable is square feet of the house

154

00:11:29.530  -->  00:11:34.510
and another variable is number of rooms of the House that's going to affect the actual number of the

155

00:11:34.510  -->  00:11:35.860
estimate here.

156

00:11:35.920  -->  00:11:40.520
So don't worry too much about trying to interpret this variable directly.

157

00:11:40.520  -->  00:11:46.090
If you don't have normalized data with normalized data you'll be able to compare each of these estimates

158

00:11:46.090  -->  00:11:47.140
to each other.

159

00:11:47.140  -->  00:11:50.000
But in this case we didn't actually normalize our data.

160

00:11:50.120  -->  00:11:53.870
It'll be difficult to compare one coefficient to another.

161

00:11:53.890  -->  00:11:58.630
Here we have the standard error which is just a measure of the variability in the estimate of the coefficient

162

00:11:59.290  -->  00:12:00.780
lower means better usually.

163

00:12:00.790  -->  00:12:05.740
But again those numbers relative to the value of the coefficient as a rule of thumb you'd like this

164

00:12:05.740  -->  00:12:11.080
value to be at least an order of magnitude less than the coefficient estimate.

165

00:12:11.080  -->  00:12:15.950
In our case we don't quite have that but we'll later on see that it's not a super big deal.

166

00:12:16.120  -->  00:12:22.300
Then we have the value of the coefficient estimate which just scores it's a score that measures whether

167

00:12:22.300  -->  00:12:25.860
or not that coefficient for the variable is meaningful for the model.

168

00:12:25.960  -->  00:12:31.450
You probably won't actually use this value itself but know that it's used to calculate the p value and

169

00:12:31.450  -->  00:12:33.330
the significance level.

170

00:12:33.700  -->  00:12:38.560
And then you have the probability or the variable p value.

171

00:12:38.650  -->  00:12:46.090
This is the probability that this variable over here is not relevance and you want this number to be

172

00:12:46.090  -->  00:12:47.830
as small as possible.

173

00:12:48.010  -->  00:12:52.240
If the number is super small our will display it with scientific notation.

174

00:12:52.270  -->  00:12:56.740
So either the power of negative 10 or something like that.

175

00:12:56.740  -->  00:13:01.060
Let's talk a little more in-depth about this variable P-value because this is the one that's actually

176

00:13:01.060  -->  00:13:06.790
really easy to interpret right out of the bat and are makes it easy because it actually has significant

177

00:13:06.790  -->  00:13:10.300
stars or the smaller p values.

178

00:13:10.300  -->  00:13:15.700
If we scroll down here you'll notice that some of these columns are right next to the P-value half stars

179

00:13:15.700  -->  00:13:18.100
or Asterix next to them we have two stars here.

180

00:13:18.190  -->  00:13:23.770
And if you keep scrolling down you'll see that we have 1 star 3 stars a period and then three stars

181

00:13:23.770  -->  00:13:24.190
.

182

00:13:24.190  -->  00:13:29.650
And these are significant stars and are putting the stars here because they're basically shorthand for

183

00:13:29.650  -->  00:13:36.550
significance levels of the number of Asterix displayed according to the P-value computed and the way

184

00:13:36.550  -->  00:13:43.960
you interpret these star values or these significant level values is that the more stars the higher

185

00:13:43.960  -->  00:13:45.390
significance we have.

186

00:13:45.670  -->  00:13:51.160
And if you don't have any stars and there isn't really that much significance and you have one star

187

00:13:51.160  -->  00:13:58.450
or a period for lower significance in this case three stars indicates that it's unlikely that no relationship

188

00:13:58.450  -->  00:14:03.510
exists between the feature and what we're trying to predict.

189

00:14:03.510  -->  00:14:10.410
I mean if we look at a particular example for instance absences here we have a very low P-value.

190

00:14:10.510  -->  00:14:15.890
Remember the P-value here is the probability that the variable is not relevant.

191

00:14:16.180  -->  00:14:24.220
That means there's an extremely low chance that absences was not a relevant feature when trying to predict

192

00:14:24.310  -->  00:14:25.770
G-3 scores.

193

00:14:26.660  -->  00:14:31.610
And you can easily tell that because you just come down side and check out that this has three stars

194

00:14:31.610  -->  00:14:32.020
.

195

00:14:32.180  -->  00:14:36.520
And essentially what you can do is kind of eyeball your summary model.

196

00:14:36.560  -->  00:14:42.980
Check out which variables have stars on them and you'll see which ones are have a low probability of

197

00:14:42.980  -->  00:14:45.160
not being significant.

198

00:14:45.200  -->  00:14:49.440
Sometimes it's a little confusing about interpreting this model is they use of that double negative

199

00:14:49.450  -->  00:14:49.610
.

200

00:14:49.850  -->  00:14:56.570
But that's basically just because of the way the t test works or the probability of this coefficient

201

00:14:56.570  -->  00:14:57.950
works.

202

00:14:57.950  -->  00:15:05.280
Again the stars indicate the probability that the variable is not relevance.

203

00:15:05.300  -->  00:15:10.610
So you want these numbers to be as small as possible and the stars will indicate really small numbers

204

00:15:10.610  -->  00:15:11.050
.

205

00:15:11.090  -->  00:15:19.010
For example we can tell here that looks like it's really likely that G-2 and absences are significance

206

00:15:19.470  -->  00:15:26.060
to predicting the G-3 score and that they're pretty relevant and intuitively this makes sense if you

207

00:15:26.060  -->  00:15:30.740
have a lot of absences you're probably not going to score really well in the test and it makes sense

208

00:15:30.740  -->  00:15:36.520
that you're G-2 your second period score is extremely relevant to your final period score.

209

00:15:36.540  -->  00:15:42.530
What's kind of surprising is that the Jiwon score was not as relevant as a g to score by a lot.

210

00:15:42.530  -->  00:15:44.900
It only has a dot here instead of three stars.

211

00:15:45.130  -->  00:15:49.850
And again go ahead and check out the notebook for a full rundown of the rest of this.

212

00:15:49.850  -->  00:15:56.330
A final thing I want to explain here in the summary is the R-squared value the R-squared values just

213

00:15:56.330  -->  00:16:03.020
the metric for evaluating the goodness of fit of your model hires better with one being the best and

214

00:16:03.020  -->  00:16:07.250
it corresponds to the amount of variability in what you're predicting that is explained by the model

215

00:16:07.250  -->  00:16:07.800
.

216

00:16:07.820  -->  00:16:14.210
However you should know that while high R-squared indicates good correlation correlation does not always

217

00:16:14.210  -->  00:16:15.840
imply causation.

218

00:16:15.860  -->  00:16:20.900
If you're already familiar mystics sticks and you can also check out the f statistic and the resulting

219

00:16:20.890  -->  00:16:26.120
p value for it as well as a residual standard error and the degrees of freedom will go ahead.

220

00:16:26.120  -->  00:16:28.030
Just skip those for now.

221

00:16:28.100  -->  00:16:33.260
All I want you to be aware of as far as I interpret in this model is being able to quickly understand

222

00:16:33.250  -->  00:16:39.530
the R-squared value and note that whether or not you have a lot of the variants explained or if you

223

00:16:39.530  -->  00:16:44.450
don't have the variance explained very well and then there's also the significance codes as far as the

224

00:16:44.450  -->  00:16:50.900
stars and you can go ahead and quickly go through your model and check out which features have a low

225

00:16:50.890  -->  00:16:53.600
probability of not being relevant.

226

00:16:53.740  -->  00:16:58.170
Again that double negative sometimes confuses people so keep that in mind.

227

00:16:58.310  -->  00:17:02.170
It's just the nature of the way the probability of the t test works.

228

00:17:02.240  -->  00:17:07.140
Let's go ahead and try to visualize our model by just plotting out the residuals.

229

00:17:07.250  -->  00:17:09.440
We're going to go out and clear the cons..

230

00:17:09.560  -->  00:17:12.430
You want to actually grab the residuals of your model.

231

00:17:12.470  -->  00:17:14.870
It's essentially a quick two step process.

232

00:17:14.990  -->  00:17:23.180
You can call the residuals function and passing in your model and that will give you.

233

00:17:23.200  -->  00:17:26.920
We check out the class of the rez.

234

00:17:26.920  -->  00:17:31.870
It gives you a numeric vector and you can go ahead and do in order to plot it out using cheesy plot

235

00:17:32.420  -->  00:17:36.740
is just call as data frame on residuals.

236

00:17:36.770  -->  00:17:41.990
And then you can check the head of the residuals data from there and this will allow us to plot this

237

00:17:41.990  -->  00:17:44.360
out using cheesy plus two.

238

00:17:44.380  -->  00:17:49.190
Let's go ahead and plot out the residuals remember the residuals are just the difference between the

239

00:17:49.180  -->  00:17:57.840
actual data points and our predicted linear regression model results so that actual line we drew in

240

00:17:57.860  -->  00:18:03.990
a central are always trying to do is minimize that residual value.

241

00:18:04.100  -->  00:18:06.710
Let's go ahead and scale Philly it blue.

242

00:18:06.700  -->  00:18:07.810
Put a little Alpha on it.

243

00:18:07.930  -->  00:18:13.320
So here I'm just going to plot a histogram using cheesy plot two of the residuals.

244

00:18:13.610  -->  00:18:19.620
Let's go ahead and do that and we can choose Ben's later on.

245

00:18:19.720  -->  00:18:25.130
They notice I actually have this here actually had this out in the beginning of the lecture but let's

246

00:18:25.120  -->  00:18:26.210
go ahead and interpret it.

247

00:18:26.220  -->  00:18:31.640
Now what's interesting here is that we would like the residuals to be normally distributed.

248

00:18:31.630  -->  00:18:37.430
Again that goes back to that dartboard analogy for most regressions we want the residuals to look like

249

00:18:37.420  -->  00:18:41.930
a normal distribution or plotted out because for residuals are normally distributed.

250

00:18:41.920  -->  00:18:47.120
This indicates the mean of the difference between our predictions and the actual values is close to

251

00:18:47.140  -->  00:18:47.820
zero.

252

00:18:48.160  -->  00:18:53.510
Which means that when we miss we're missing both the short and long of the actual value and the likelihood

253

00:18:53.510  -->  00:18:59.870
of a miss being far from the actual value gets smaller as the distance from the actual value gets larger

254

00:18:59.880  -->  00:19:00.220
.

255

00:19:00.490  -->  00:19:06.260
Well you should note here is that there are suspicious suspicious activity going on with residuals that

256

00:19:06.260  -->  00:19:08.470
are large negative values.

257

00:19:08.470  -->  00:19:14.570
Notice that we actually have some significant counts over here of negative residuals.

258

00:19:14.620  -->  00:19:20.620
And the reason for that is because we're plotting out a linear regression model the models actually

259

00:19:20.620  -->  00:19:25.670
predicting negative test scores for really poor performing students.

260

00:19:25.700  -->  00:19:30.440
However the lowest score possible on a test is zero.

261

00:19:30.530  -->  00:19:36.550
And that's why we're getting these predicted or large negative residual values because essentially the

262

00:19:36.560  -->  00:19:40.730
models is saying these students are going to perform so poorly on the test.

263

00:19:40.760  -->  00:19:42.770
I will predict a negative score.

264

00:19:42.860  -->  00:19:47.970
However with the real training data the lowest possible score was a zero.

265

00:19:48.050  -->  00:19:52.520
Which is why we're getting these weird set of large negative values and we're going to go ahead and

266

00:19:52.520  -->  00:19:56.390
see how to deal with that in part three of this lecture series.

267

00:19:56.420  -->  00:19:57.340
OK.

268

00:19:57.860  -->  00:20:00.430
Hopefully found that informative and fun again.

269

00:20:00.430  -->  00:20:04.490
Go ahead and check out the notebook if you want to reference any of this code and you want some more

270

00:20:04.580  -->  00:20:08.530
in-depth explanations of how to interpret the summary of the model.

271

00:20:08.540  -->  00:20:11.150
Thanks everyone and I'll see you at the next lecture.
