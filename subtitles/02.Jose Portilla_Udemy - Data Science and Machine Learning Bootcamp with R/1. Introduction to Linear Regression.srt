1

00:00:01,170  -->  00:00:06,540
Hello everyone and welcome to an introduction to linear regression in this lecture We'll discuss the

2

00:00:06,540  -->  00:00:09,210
history and background of linear regression.

3

00:00:09,210  -->  00:00:14,730
The basic concept behind the linear regression and will end with showing how models can actually be

4

00:00:14,730  -->  00:00:16,750
built in are.

5

00:00:16,920  -->  00:00:20,350
If you want the mathematical background behind linear regression.

6

00:00:20,520  -->  00:00:25,940
Go ahead and read chapters 2 and 3 of an introduction to statistical learning before actually approaching

7

00:00:25,950  -->  00:00:28,110
this lecture.

8

00:00:28,110  -->  00:00:28,910
Moving along.

9

00:00:28,950  -->  00:00:32,310
Let's go ahead and start the history of linear regression.

10

00:00:32,310  -->  00:00:36,600
This all started in the 1800s of a guy named Francis Galton.

11

00:00:36,720  -->  00:00:41,180
Galton was studying the relationship between parents and their children particular.

12

00:00:41,210  -->  00:00:45,930
He investigated the relationship between the heights of fathers and their sons.

13

00:00:46,440  -->  00:00:51,210
What he discovered was that a man's son tended to be roughly as tall as his father.

14

00:00:51,540  -->  00:00:57,720
However Galton's breakthrough was at the son's height tended to be closer to the overall average height

15

00:00:57,810  -->  00:00:59,540
of all people.

16

00:01:00,450  -->  00:01:03,520
Let's go ahead and take Shaquille O'Neal as an example.

17

00:01:03,570  -->  00:01:06,180
He's a very famous basketball player.

18

00:01:06,180  -->  00:01:09,120
Shaq is really tall seven feet one inch.

19

00:01:09,120  -->  00:01:12,380
Or if you using the metric system that's 2.2 meters ish.

20

00:01:12,810  -->  00:01:17,060
If Shaq has a son chances are the sun's going to be pretty tall too.

21

00:01:17,070  -->  00:01:23,070
However Shaq is such an anomaly or an outlier that there's also a very good chance that his son will

22

00:01:23,160  -->  00:01:26,350
not be as tall as Shaq.

23

00:01:26,790  -->  00:01:28,260
Turns out that is the case.

24

00:01:28,260  -->  00:01:34,160
Shaq's son is pretty tall six feet seven inches but not nearly as tall as his dad.

25

00:01:34,350  -->  00:01:41,910
Galton called this particular phenomenon regression as in a father son's height tends to regress or

26

00:01:41,910  -->  00:01:45,140
drift towards the mean which is the average height.

27

00:01:45,330  -->  00:01:48,690
And that's where the term regression comes from.

28

00:01:48,690  -->  00:01:52,020
Let's go out and take the simplest example possible.

29

00:01:52,020  -->  00:01:55,670
We'll go ahead and calculate a regression with only two data points.

30

00:01:55,980  -->  00:02:00,980
Here I have two little tiny data points on a G-G plot graph.

31

00:02:01,200  -->  00:02:06,300
All we're trying to do when we calculate a regression line is to draw a line that's as close to every

32

00:02:06,300  -->  00:02:07,740
dot as possible.

33

00:02:07,830  -->  00:02:10,640
Here it's very easy because we just have two dots.

34

00:02:10,690  -->  00:02:17,160
Is the simplest example we can do for classically regression or the least squares method that you read

35

00:02:17,160  -->  00:02:22,080
about in the reading in the introduction to us to skal learning you only measure the closeness in the

36

00:02:22,170  -->  00:02:23,950
up and down the reaction.

37

00:02:23,970  -->  00:02:29,940
So how far in the up and down the action or that y axis you are from the line you drew that regression

38

00:02:29,940  -->  00:02:33,190
line to the rest of the points.

39

00:02:33,450  -->  00:02:38,840
Now wouldn't it be great if we could apply this same concept to a graph of more than just two data points

40

00:02:38,840  -->  00:02:39,530
.

41

00:02:39,780  -->  00:02:43,950
More likely our data is going to look something like this on the right here on the y axis we have a

42

00:02:43,950  -->  00:02:48,120
Sun site and on the x axis we have a father site.

43

00:02:48,150  -->  00:02:53,880
By doing this we could actually take multiple men and their sons heights and do things like tell a man

44

00:02:53,880  -->  00:03:00,720
how tall we would expect his son to be before he even has a son who would try to predict that Sun's

45

00:03:00,720  -->  00:03:04,390
height based off of the father's type.

46

00:03:04,470  -->  00:03:10,190
Our goal of linear regression is to minimize that vertical distance between all the data points and

47

00:03:10,190  -->  00:03:10,980
our line.

48

00:03:11,250  -->  00:03:16,740
So in determining the best line we are attempting to minimize the distance between all the points and

49

00:03:16,740  -->  00:03:19,700
their distance to our line.

50

00:03:19,890  -->  00:03:25,620
There's lots of different ways to minimize this sum of squared errors some of absolute errors etc. but

51

00:03:25,650  -->  00:03:30,720
all of these methods have a general goal of minimizing the distance between those points and the regression

52

00:03:30,720  -->  00:03:37,500
line formulas that are take the form why tilde x.

53

00:03:37,860  -->  00:03:43,180
If you see the diagram here below this is how we actually build models in our.

54

00:03:43,500  -->  00:03:47,790
If you want to add more predicter variables you just go ahead and use the plus sign.

55

00:03:47,790  -->  00:03:51,870
Let's go out and break down what's written here all the way on the left.

56

00:03:51,870  -->  00:03:55,200
We have a variable called Model that's the R object.

57

00:03:55,200  -->  00:04:01,500
We're going to save our result and then we have our assignment operator the arrow and we begin by saying

58

00:04:01,530  -->  00:04:03,360
linear regression modeling command.

59

00:04:03,720  -->  00:04:08,250
So that linear regression modeling command is going to be the actual modeling function.

60

00:04:08,250  -->  00:04:13,770
Now we're going to be replacing that actual function depending on what machine learning model for linear

61

00:04:13,770  -->  00:04:17,910
regression will use l m meaning linear model.

62

00:04:18,150  -->  00:04:22,150
Then the first thing we pass is the quantity we want to predict.

63

00:04:22,200  -->  00:04:27,330
In this example they're trying to predict some sort of log quantity but as we'll see later on we can

64

00:04:27,330  -->  00:04:32,470
actually put any quantity or any feature from the variable as what we want to predict.

65

00:04:32,490  -->  00:04:34,700
Then you have the tilt sign and your actual formula.

66

00:04:34,830  -->  00:04:38,870
So you put in the variables that are available to make the prediction.

67

00:04:38,910  -->  00:04:46,620
Then finally you add in data equals and then your data frame to use for training your model.

68

00:04:46,620  -->  00:04:49,770
Once you've trained your model you can predict off your model.

69

00:04:50,070  -->  00:04:56,970
You'll go ahead and Storm Prediction as some sort of new column using the Predict command this predict

70

00:04:56,970  -->  00:04:58,160
function.

71

00:04:58,230  -->  00:05:03,990
All I will do is pass in the linear regression model or later on will see that it can pass in any other

72

00:05:03,990  -->  00:05:09,690
model and then we pass a new data test set the data to use in prediction.

73

00:05:09,690  -->  00:05:14,910
We can also do the same operation on a training data.

74

00:05:14,910  -->  00:05:19,150
Let's go ahead and go to our studio in the next lecture and begin to explore an example.

75

00:05:19,260  -->  00:05:22,550
Then you'll have a project to test your own understanding.

76

00:05:22,950  -->  00:05:25,010
Thanks everyone and I'll see you at the next lecture
