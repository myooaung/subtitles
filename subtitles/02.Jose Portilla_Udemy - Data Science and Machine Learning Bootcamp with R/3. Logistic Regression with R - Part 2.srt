1

00:00:00,600  -->  00:00:06,780
Hello everyone and welcome to logistic regression with our part two in the previous part one of this

2

00:00:06,780  -->  00:00:11,630
lecture series we went ahead and explored the data use Jiyu plot to visualize it.

3

00:00:11,880  -->  00:00:18,510
And we explored how we could replace the missing age values with an imputed age value based on the average

4

00:00:18,510  -->  00:00:20,910
age of the passengers class.

5

00:00:20,910  -->  00:00:25,830
Let's go ahead and continue on right where we left off and then jump to our studio now.

6

00:00:25,830  -->  00:00:26,200
All right.

7

00:00:26,220  -->  00:00:31,380
This is where we left off last time we created this imputes underscore age function which took in the

8

00:00:31,470  -->  00:00:39,180
age and class vectors and then basically created an output vector which is the fixed ages.

9

00:00:39,180  -->  00:00:49,680
Let's make sure this works by re-assigning the train age to the fixed ages

10

00:00:52,620  -->  00:00:58,340
and then what we're going to go ahead and do is use that mismatch function again again to do that.

11

00:00:58,350  -->  00:01:07,990
I'm just going to say this map pasand my data the if that train the main it's just the title here.

12

00:01:08,310  -->  00:01:13,340
We'll go ahead and say imputation check.

13

00:01:13,680  -->  00:01:19,500
As the title and C O L stands for the color vector first color is what you want.

14

00:01:19,500  -->  00:01:25,960
Missing values to be or any values second colors what you want absurd variables to be elements.

15

00:01:25,990  -->  00:01:31,030
Call it black and then we'll say legend is equal to false.

16

00:01:31,080  -->  00:01:37,320
If you say legend is equal to true you get kind of a super large legend that takes too much room and

17

00:01:37,320  -->  00:01:41,410
then we'll go ahead and print this out.

18

00:01:42,030  -->  00:01:48,960
Let me expand this window before I actually run the script so we have enough room to see the plot.

19

00:01:50,460  -->  00:01:54,150
All right let me go and I can run this control ship us.

20

00:01:54,640  -->  00:01:56,590
OK we have an amputation check.

21

00:01:56,610  -->  00:01:59,290
Everything is black which means we no longer have any.

22

00:01:59,340  -->  00:02:02,800
And a data in our data frame looks like it work.

23

00:02:02,940  -->  00:02:03,900
Let's go ahead and continue.

24

00:02:03,900  -->  00:02:10,590
By building up our model so that it's time to build our model or we're going to go ahead and do is begin

25

00:02:10,590  -->  00:02:15,660
by doing a final clean up of raw data by removing the features we won't be using and making sure that

26

00:02:15,660  -->  00:02:18,250
the features are of the correct data type.

27

00:02:18,270  -->  00:02:25,530
If we get quickly check here under structure I'm going to go ahead and remove all the plots I've been

28

00:02:25,530  -->  00:02:29,120
using.

29

00:02:29,370  -->  00:02:31,100
Let me clear the console.

30

00:02:31,200  -->  00:02:34,020
Move this over to the rights to won't be visualizing.

31

00:02:34,020  -->  00:02:40,240
For now I'm going to check the structure of DFI thought train.

32

00:02:41,070  -->  00:02:48,300
And if you notice here we have some variables or features that we're not going to use the ones we're

33

00:02:48,300  -->  00:02:49,010
not going to use.

34

00:02:49,020  -->  00:02:57,270
Our passenger ID the name of the passenger the ticket of the passenger and the cabin of the passenger

35

00:02:57,690  -->  00:03:01,160
because it just have way too many levels and factors.

36

00:03:01,170  -->  00:03:04,470
You'd have to do some feature engineering to actually make use of these.

37

00:03:04,540  -->  00:03:06,170
Something you could do fichu engineering.

38

00:03:06,210  -->  00:03:11,550
For instance the name we actually already mentioned you can try to grab titles as a feature and something

39

00:03:11,550  -->  00:03:13,400
else you could do is from cabin.

40

00:03:13,470  -->  00:03:15,600
Try to grab the actual cabin level.

41

00:03:15,710  -->  00:03:17,980
I noticed that a lot of these cabins start off a letter.

42

00:03:18,120  -->  00:03:23,490
You could just try to grab that letter and use the cabin level as a feature.

43

00:03:23,490  -->  00:03:25,650
This case we won't bother using any of those.

44

00:03:25,710  -->  00:03:28,370
We'll just go ahead and remove those features.

45

00:03:28,560  -->  00:03:34,430
I'm going to call to the plier library to remove these features.

46

00:03:34,500  -->  00:03:38,590
All right so we've have the the plier library ready to go.

47

00:03:38,670  -->  00:03:48,930
I'm going to say DMF train and I'll use the select function from the plier and for the select function

48

00:03:49,140  -->  00:03:56,790
function you pasan DFA train and we have two choices here either passen all the features or column names

49

00:03:56,790  -->  00:04:03,080
that you want to keep or put a negative sign and passing all the features variables or columns that

50

00:04:03,090  -->  00:04:04,810
it don't want to keep this case.

51

00:04:04,830  -->  00:04:06,970
I'll do it the negative way.

52

00:04:07,230  -->  00:04:09,280
I'm not going to keep passenger ID.

53

00:04:09,390  -->  00:04:15,780
I'm not going to keep name ID I want to get rid of ticket ID or a ticket and then want to get rid of

54

00:04:15,780  -->  00:04:17,370
carbon.

55

00:04:17,940  -->  00:04:22,780
So we're getting rid of passenger ID name ticket cabin and we could do that with a negative sign using

56

00:04:22,790  -->  00:04:24,920
the select from the train.

57

00:04:25,650  -->  00:04:30,990
Let's go ahead and check out the head of the train make sure it works.

58

00:04:31,890  -->  00:04:33,390
And it worked out perfect.

59

00:04:33,390  -->  00:04:39,270
This is exactly what we want what I'm going to go ahead and do is not check the structure of the train

60

00:04:41,790  -->  00:04:46,350
Note that I'm kind of hopping back and forth between the script and the cons. You should be careful

61

00:04:46,350  -->  00:04:49,710
if you're doing this to make sure you're using the same DFI train.

62

00:04:49,710  -->  00:04:54,300
I would suggest that you actually type this all out through steps to create our script and organize

63

00:04:54,300  -->  00:04:54,860
it.

64

00:04:54,870  -->  00:05:01,250
I'm doing this so that it's easier to show step by step or actually doing that we have the structure

65

00:05:01,270  -->  00:05:04,730
of the frame I want to make sure that survive is not an integer.

66

00:05:04,730  -->  00:05:07,110
It should be a factor so class.

67

00:05:07,110  -->  00:05:08,410
It's also not an integer.

68

00:05:08,640  -->  00:05:17,040
And we saw from our visualizations that sibling spouse these are low enough numbers that let's go ahead

69

00:05:17,070  -->  00:05:21,990
and group these as a factor instead of putting it as a continuous variable.

70

00:05:21,990  -->  00:05:27,360
This is a choice you can make and you can also choose to not make it since it is a continuous value

71

00:05:27,390  -->  00:05:31,980
of number of siblings or spouses from 0 to technically infinite siblings.

72

00:05:32,100  -->  00:05:37,500
But I'm going to go ahead and group these because it makes sense given the small actual amounts of siblings

73

00:05:37,500  -->  00:05:39,690
or spouses or parents or children.

74

00:05:39,690  -->  00:05:46,440
So let's go ahead and say survives as a factor P class as a factor this sibling spouse is a factor and

75

00:05:46,440  -->  00:05:48,180
this parent child is a factor.

76

00:05:48,360  -->  00:05:53,390
I'm going to go ahead and copy and paste the code for the note book that allows us to do this.

77

00:05:53,730  -->  00:05:59,320
In this case it's just these four lines Central just calling each of those four columns and then re-assigning

78

00:05:59,320  -->  00:06:06,300
that with a factor in a run that knowledge check the structure of the train one more time.

79

00:06:06,330  -->  00:06:09,680
Note that I don't have to type in Prince because I'm right in the con..

80

00:06:10,110  -->  00:06:14,310
It looks like we were able to convert those all to factors perfect.

81

00:06:14,640  -->  00:06:19,210
Something to note here is that embarked actually has four levels a blank string.

82

00:06:19,210  -->  00:06:21,410
C q and s.

83

00:06:21,450  -->  00:06:22,290
That's OK.

84

00:06:22,470  -->  00:06:27,870
Basically you can just treat this blank as an unknown and that may be an important feature if they don't

85

00:06:27,870  -->  00:06:30,570
even know what court they embarked on.

86

00:06:30,690  -->  00:06:34,700
It may or may not be but it shouldn't affect the model too much to just leave it like that.

87

00:06:35,010  -->  00:06:38,400
That's something that you may or may not want to take care of when you're actually clean in that data

88

00:06:38,410  -->  00:06:38,830
.

89

00:06:39,240  -->  00:06:42,060
Let's go to continue on by training the model.

90

00:06:42,240  -->  00:06:53,220
I'm going to go ahead and say Lagat model and to use a logistic regression model in our you say G L

91

00:06:53,280  -->  00:06:59,630
M which stands for generalized linear model you pass in the formula.

92

00:06:59,820  -->  00:07:06,690
In this case I want to predict these survived column based off of everything or every feature every

93

00:07:06,690  -->  00:07:08,360
column in my data frame.

94

00:07:08,430  -->  00:07:10,870
I'll go ahead and write period to do that.

95

00:07:12,000  -->  00:07:17,670
And then what you have to denote is family and family is basically a description of the earth distribution

96

00:07:17,670  -->  00:07:27,470
of we're going to be using this case will say Binomial and we'll say link equals logic.

97

00:07:27,930  -->  00:07:36,130
And this basically just says use the logistic regression function and we'll go in and say data DFT train

98

00:07:36,130  -->  00:07:37,110
.

99

00:07:38,160  -->  00:07:43,230
Again when you're using generalized linear models function here in order to perform a logistic regression

100

00:07:43,290  -->  00:07:48,730
you just pass in your formula like we did for linear regression earlier but you also pass in this argument

101

00:07:48,730  -->  00:07:48,750
.

102

00:07:48,750  -->  00:07:50,580
Family equals binomial link.

103

00:07:50,580  -->  00:07:51,540
Watch it.

104

00:07:51,540  -->  00:07:57,630
You can go ahead explore help on Jhelum to see what other families you can use this case for using TLM

105

00:07:57,690  -->  00:07:59,060
for logistic regression.

106

00:07:59,250  -->  00:08:01,630
Let's go ahead and train the model.

107

00:08:02,070  -->  00:08:05,670
Now it's going a summary of that log model.

108

00:08:07,590  -->  00:08:08,340
OK.

109

00:08:08,700  -->  00:08:14,880
Note that we have a bunch coefficients and a bunch of data here to interpret just like in the summary

110

00:08:14,880  -->  00:08:23,610
results for linear regression we can use D-Star notation to see what has a low probability of not being

111

00:08:23,610  -->  00:08:24,470
significant.

112

00:08:24,480  -->  00:08:33,240
Basically three stars you can kind of interpret this as important features unsurprisingly second class

113

00:08:33,330  -->  00:08:40,800
and third class are very important features and so is the age and sex of the passenger especially the

114

00:08:40,800  -->  00:08:45,810
fact that if they were a male or not male seems to be extremely important.

115

00:08:45,870  -->  00:08:51,510
Now I say extremely important but the correct statistical way of saying this is that it's very likely

116

00:08:51,510  -->  00:08:53,760
to not be important.

117

00:08:54,110  -->  00:08:58,050
But we'll just go ahead and use the terminology for semantics here.

118

00:08:58,110  -->  00:09:02,010
I'm just saying that this is a pretty significant feature age.

119

00:09:02,040  -->  00:09:04,870
What else was significant having either three or four siblings.

120

00:09:04,890  -->  00:09:09,610
A little bit but definitely not as much as the age sex or class.

121

00:09:09,630  -->  00:09:11,670
And those were the most important ones.

122

00:09:11,670  -->  00:09:14,110
Intuitively this makes sense.

123

00:09:14,430  -->  00:09:18,870
Maybe you've seen the movie Titanic where it kind of gives the impression that everyone in third class

124

00:09:18,870  -->  00:09:23,930
died but really the most important thing was whether you were a male or not.

125

00:09:25,670  -->  00:09:31,680
OK go ahead and check out help on summary for logistic regression if you want more information.

126

00:09:31,670  -->  00:09:36,650
Everything that we've done here or check the previous notebooks for more information on how to actually

127

00:09:36,640  -->  00:09:41,850
interpret this summary what I'm going to go ahead and do is start with predictions.

128

00:09:42,070  -->  00:09:51,490
Let's go ahead and try to predict off of test data as an exercise for you to finish off this lecture

129

00:09:51,500  -->  00:09:51,650
.

130

00:09:51,640  -->  00:09:58,340
I'm going to go ahead and leave it as an exercise for you to use the Titanic underscore test that C

131

00:09:58,340  -->  00:10:03,680
S V file in order to predict new survived or not survived.

132

00:10:03,700  -->  00:10:07,300
Classes offer that test that cxxviii file.

133

00:10:07,450  -->  00:10:11,750
That's an assignment for you because I want you to practice everything we just did here because you're

134

00:10:11,750  -->  00:10:15,970
going to have to clean it the same way we cleaned the training data.

135

00:10:15,980  -->  00:10:20,630
You're going to have to impute the age and the average age based off a class.

136

00:10:20,620  -->  00:10:25,010
Now I want to leave that for practice for you so we won't do it in this lecture and said what we'll

137

00:10:25,060  -->  00:10:33,650
do is split up our larger training set into a kind of a mini test mini train dataset and redo the models

138

00:10:33,700  -->  00:10:34,700
that way.

139

00:10:34,810  -->  00:10:36,190
Again go ahead.

140

00:10:36,230  -->  00:10:41,710
And as an exercise after this lecture to make sure you fully understood everything we just did try to

141

00:10:41,710  -->  00:10:46,150
predict off Titanic underscored tests that C S V.

142

00:10:46,510  -->  00:10:52,560
For now let's go ahead and splits by making tests out of our training set.

143

00:10:52,910  -->  00:10:56,060
NASA library s.a.a tools.

144

00:10:56,750  -->  00:11:02,780
And this is also good because it's always great to practice using this going to set seed to 100 once

145

00:11:02,770  -->  00:11:05,110
you get the same random splits I do.

146

00:11:05,650  -->  00:11:16,930
And then a set split to sample that split and say DFI train pass in a column you can pass in any column

147

00:11:16,950  -->  00:11:16,980
.

148

00:11:17,000  -->  00:11:20,720
But like I mentioned earlier I like to pass in the column that we're trying to predict.

149

00:11:20,800  -->  00:11:25,480
We're trying to predict whether they survived or not and that I'm going to go ahead and say split ratio

150

00:11:25,500  -->  00:11:26,000
.

151

00:11:26,360  -->  00:11:28,180
And also at the 0.7

152

00:11:30,900  -->  00:11:42,450
then I'm going to say final train as the subset of the train where split is equal to true.

153

00:11:42,860  -->  00:11:54,010
And then we'll say final the first sequel to subset the if the train splits equal to false.

154

00:11:55,120  -->  00:11:55,640
All right.

155

00:11:55,630  -->  00:12:02,680
Now let's rerun our model using only this final training set and then we'll test it using this final

156

00:12:02,680  -->  00:12:03,870
test set.

157

00:12:04,000  -->  00:12:10,870
Again I'm not using the Titanic underscore test CSP file that's for you to work out as an additional

158

00:12:10,900  -->  00:12:16,780
exercise just to make sure you understood that basically the imputation of age based off of class.

159

00:12:16,930  -->  00:12:21,190
That's what I really wanted to test out so you can test that out on the test.

160

00:12:21,520  -->  00:12:31,300
Continuing on what we're going to do is say final log that model will say G L M pasan a formula which

161

00:12:31,300  -->  00:12:47,950
is parodic survive off of all the columns say family loops is equal to binomial and we'll say link equals

162

00:12:49,780  -->  00:12:52,130
logic for logistic regression.

163

00:12:52,120  -->  00:12:58,300
And then finally we'll passenger data and our data is equal to final the train now.

164

00:12:59,130  -->  00:13:06,670
And let's call a summary of the final the log model since it's already been trained.

165

00:13:06,670  -->  00:13:11,860
And here again we see that third class extremely important predictors.

166

00:13:11,910  -->  00:13:12,820
If they were male.

167

00:13:12,860  -->  00:13:15,400
Also very important and their age is important.

168

00:13:15,430  -->  00:13:19,900
Let's go ahead and practice predicting values off of this.

169

00:13:20,050  -->  00:13:23,360
We have a final test if we scroll back up here.

170

00:13:23,370  -->  00:13:25,030
Notice we have the final thought.

171

00:13:25,210  -->  00:13:29,030
Let's try to predict using our final dot log model.

172

00:13:29,330  -->  00:13:36,880
I can go ahead and say fitted the probabilities you go in and clear this so it shows up in the middle

173

00:13:37,620  -->  00:13:42,220
we'll say fitted the probabilities

174

00:13:47,260  -->  00:13:48,920
and we'll call the Predict function.

175

00:13:48,940  -->  00:13:55,490
Remember for the Predict function you pass and your model will say final that Lague model you pasan

176

00:13:55,540  -->  00:13:56,420
your new data.

177

00:13:56,440  -->  00:13:59,570
In this case it's final thought test.

178

00:14:00,280  -->  00:14:06,770
And since we're actually doing a classification problem or trying to predict declasse 0 or 1 as to whether

179

00:14:06,760  -->  00:14:07,820
or not they survived.

180

00:14:08,090  -->  00:14:15,530
But you end up typing here as type equals response.

181

00:14:15,520  -->  00:14:18,290
And now we can calculate from the predicted values.

182

00:14:18,320  -->  00:14:20,440
So when I say fitted that results

183

00:14:23,750  -->  00:14:31,450
and what you're going to do is one quick way to do this is put in if else passen fitted up probabilities

184

00:14:32,290  -->  00:14:37,190
greater than 0.5 Kamo 1 comma zero.

185

00:14:37,390  -->  00:14:42,570
And this is just a very shorthand way instead of having to write a whole function to do this.

186

00:14:42,590  -->  00:14:49,810
This is essentially a shortcut way of saying OK if my fitted probability is greater than 0.5 set it

187

00:14:49,850  -->  00:14:56,560
equal to 1 meaning they had a greater than 50 percent probability of surviving I will set it equal to

188

00:14:56,570  -->  00:14:57,280
1.

189

00:14:57,570  -->  00:14:59,910
Otherwise I will set it equal to zero.

190

00:14:59,920  -->  00:15:06,220
This sort of if else kind of shorthand function is just shorthand and sort of having to write a whole

191

00:15:06,910  -->  00:15:10,060
function with if else statement in it.

192

00:15:10,330  -->  00:15:11,820
Right.

193

00:15:12,090  -->  00:15:16,500
And you can type in help if else if you need more information on that.

194

00:15:16,510  -->  00:15:23,730
Now let's go ahead and get my misclassification air or say Mis classification.

195

00:15:23,920  -->  00:15:27,580
We'll just call it miss class error

196

00:15:30,700  -->  00:15:40,570
and that's going to be the mean or the average of the fitted the results that are not equal to the actual

197

00:15:40,580  -->  00:15:46,190
results and the actual results are in final test survived.

198

00:15:46,230  -->  00:15:52,220
Colum let's make sure we have that.

199

00:15:52,660  -->  00:15:54,910
Go ahead and print that one minus

200

00:15:58,120  -->  00:15:59,300
MS Glasser.

201

00:15:59,330  -->  00:16:00,370
There you go.

202

00:16:00,880  -->  00:16:03,620
And that is your accuracy.

203

00:16:03,670  -->  00:16:09,360
Let's go in and break this down real quick will we actually just did we use the Predict function past

204

00:16:09,380  -->  00:16:15,880
then our logistic regression model passed in our test data not case it was the final test data we split

205

00:16:15,880  -->  00:16:20,960
from our training set and we put in type equals response because we're trying to predict an actual class

206

00:16:20,950  -->  00:16:23,100
response 0 or 1.

207

00:16:23,380  -->  00:16:26,400
Then we use this little if else shortcut.

208

00:16:26,470  -->  00:16:31,990
And this is kind of a trick for you for future reference when you're doing logistic regression that

209

00:16:32,000  -->  00:16:38,740
we can just say if the fitted probabilities are greater than 0.5 Stethem equal to 1 otherwise equal

210

00:16:38,750  -->  00:16:39,390
to zero.

211

00:16:39,620  -->  00:16:45,410
And we say that as the fitted results so that's just a vector of zeros and ones that we want in the

212

00:16:45,400  -->  00:16:47,040
misclassification air.

213

00:16:47,050  -->  00:16:54,880
So we want to the number or average amount of fit adult results not equal to their actual test values

214

00:16:54,880  -->  00:16:56,710
which was final that test survived.

215

00:16:56,720  -->  00:17:03,090
Kala we grabbed the meat of that and then one minus the misclassification ere is your accuracy.

216

00:17:03,130  -->  00:17:06,670
So we got an accuracy of about 80 percent.

217

00:17:06,880  -->  00:17:13,310
So we have 80 percent accuracy in what we can do here is actually create a confusion matrix A confusion

218

00:17:13,300  -->  00:17:20,230
matrix is basically going to be your main standard of evaluating your model because you can get specificity

219

00:17:20,510  -->  00:17:22,900
or recall or precision or accuracy.

220

00:17:22,960  -->  00:17:28,360
Don't just use accuracy as a score of your model use all the different rates that we learn about in

221

00:17:28,370  -->  00:17:31,840
the logistic regression overview lecture.

222

00:17:31,900  -->  00:17:34,660
What I'm going to go ahead and do is show you how to create this confusion matrix.

223

00:17:34,660  -->  00:17:37,400
Let me go type in confusion matrix

224

00:17:40,480  -->  00:17:46,100
as a comments and use a table function to actually create this confusion matrix.

225

00:17:46,340  -->  00:17:50,800
You put in a table and you pass in your actual data.

226

00:17:51,260  -->  00:17:59,830
So I'll say final test survive and then you just pass in fit it probabilities that are greater than

227

00:17:59,840  -->  00:18:01,950
0.5.

228

00:18:03,130  -->  00:18:06,260
And here we have our confusion matrix.

229

00:18:06,280  -->  00:18:06,960
OK.

230

00:18:07,220  -->  00:18:08,870
I hope that was help for you.

231

00:18:09,070  -->  00:18:11,210
That's basically the end of this assignment.

232

00:18:11,240  -->  00:18:17,330
What I want you to do as an exercise to just as it's optional you can go ahead and not do it but it

233

00:18:17,320  -->  00:18:22,550
would be nice to really test your understanding by using the Titanic underscore the underscore Tessy

234

00:18:22,550  -->  00:18:29,210
a city in order to actually test your model against this totally unknown test that see c file if you

235

00:18:29,200  -->  00:18:35,650
want it can even try submitting your results to Kaggle competition Web site by their submission by their

236

00:18:35,650  -->  00:18:37,100
online submission.

237

00:18:37,170  -->  00:18:38,690
OK thanks everyone.

238

00:18:38,720  -->  00:18:40,780
Hope you find that useful and I'll see you at the next lecture
