WEBVTT
1

00:00:01.440  -->  00:00:05.070
Hello everyone and welcome to the nearest neighbors project.

2

00:00:05.070  -->  00:00:10.260
In this lecture we're going to be going over a brief overview of what the project entails.

3

00:00:10.540  -->  00:00:15.930
Nerds do this let's go ahead and jump to the nearest neighbors project notebook.

4

00:00:16.740  -->  00:00:18.860
OK here we are at the notebook.

5

00:00:19.020  -->  00:00:23.820
Since Kinnear's neighbors is such a simple algorithm we're going to be using this project more as a

6

00:00:23.820  -->  00:00:28.260
simple exercise to test your understanding of the different steps you have to take when implementing

7

00:00:28.590  -->  00:00:32.650
cannoneers neighbors in our will be using the iris data center.

8

00:00:32.760  -->  00:00:38.460
It's actually quite a famous data set and a lot of times it's a first dataset when machine learning

9

00:00:38.460  -->  00:00:40.920
students are learning about classification.

10

00:00:40.920  -->  00:00:44.440
It's a pretty small dataset only has about 100 or 50 observations.

11

00:00:44.640  -->  00:00:50.040
The basic idea and you can actually click the link to the Wikipedia page for this iris flower dataset

12

00:00:50.490  -->  00:00:56.520
is that this biologist took a bunch of measurements of iris flowers.

13

00:00:56.880  -->  00:01:02.100
He took measurements other Sipple length Sipple with pedal length and pedal with and then noted what

14

00:01:02.100  -->  00:01:03.290
species they were.

15

00:01:03.450  -->  00:01:08.130
And the idea here is that you're going to be trying to use Kinnear's neighbor's algorithm to attempt

16

00:01:08.130  -->  00:01:15.090
to predict what species of flower the iris is based off of its physical features in order to do that

17

00:01:15.180  -->  00:01:17.590
you just need to call the Eisele our library.

18

00:01:17.670  -->  00:01:20.720
And then the lowercase Iris dataset will be there.

19

00:01:21.210  -->  00:01:26.580
Once you've done that you'll go ahead and standardize the data and you'll use skil function to do that

20

00:01:27.870  -->  00:01:33.330
then you'll do a train test split on your data you will build a nearest neighbors model we'll use that

21

00:01:33.330  -->  00:01:37.370
came nears neighbors function to predict the species of the test set.

22

00:01:37.380  -->  00:01:42.930
You start off with Cahill's to one and then you'll check what you're misclassification rate was or your

23

00:01:42.930  -->  00:01:51.150
error rate then you'll choose a key value using the Elbel method by plotting out the range of the misclassification

24

00:01:51.150  -->  00:01:55.450
rate for k values from 1 to 10 just like we did in the previous lecture.

25

00:01:55.620  -->  00:01:58.270
You should get a plot that looks something like this.

26

00:01:58.290  -->  00:02:02.790
Notice it looks a little bit funky because the error goes down and then it starts coming back up again

27

00:02:02.810  -->  00:02:02.990
.

28

00:02:03.160  -->  00:02:05.480
And that's because of the size of the data.

29

00:02:05.550  -->  00:02:11.160
So it's only about 100 of 50 features if you're only looking at the training or testing set that's even

30

00:02:11.160  -->  00:02:16.700
less features excuse me observations and you'll get a little jump in error.

31

00:02:16.710  -->  00:02:21.600
Overall the error rate should be pretty small and there's a further explanation of why this looks like

32

00:02:21.600  -->  00:02:25.120
this in the solutions video lecture.

33

00:02:25.140  -->  00:02:30.510
After that you have an optional assignment which is to check out the machine learning repository for

34

00:02:30.600  -->  00:02:32.020
UCI.

35

00:02:32.100  -->  00:02:36.420
If you click on that link in the notebook it will take you to the UCLA repository for machine learning

36

00:02:36.430  -->  00:02:37.010
.

37

00:02:37.530  -->  00:02:40.000
And once that page is loaded it looks like this.

38

00:02:40.050  -->  00:02:43.240
Then you can go ahead and click here it says you may view all data sets.

39

00:02:43.230  -->  00:02:48.870
There's a little hyperlink to view all the data sets and you can actually sort by the default task.

40

00:02:48.990  -->  00:02:53.430
In this case for cannoneers neighbors you look for classification and go ahead and explore the various

41

00:02:53.430  -->  00:02:54.490
data sets here.

42

00:02:54.780  -->  00:02:59.580
A lot of the data sets that we've been using have been just directly taken from this machine learning

43

00:02:59.580  -->  00:03:00.780
repository.

44

00:03:00.780  -->  00:03:06.210
So this is a great source just like Kaggle for data sets where you can just practice what you know and

45

00:03:06.210  -->  00:03:08.730
practice your machine learning skills with our.

46

00:03:09.280  -->  00:03:09.990
OK.

47

00:03:10.200  -->  00:03:13.190
Go ahead and get started on the Kinnear's Neighbors Project.

48

00:03:13.260  -->  00:03:15.720
I will see it the next lecture for the solutions.

49

00:03:15.720  -->  00:03:16.910
Thanks everyone.
