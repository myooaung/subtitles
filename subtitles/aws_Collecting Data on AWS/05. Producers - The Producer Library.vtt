WEBVTT
1
00:00:01.090 --> 00:00:04.220
Now let's take a closer look at the Kinesis Producer Library

2
00:00:04.220 --> 00:00:07.600
and how to use it to send data into Kinesis.

3
00:00:07.600 --> 00:00:10.240
So what is the Kinesis Producer Library?

4
00:00:10.240 --> 00:00:12.970
It's a library for Java applications that is used to

5
00:00:12.970 --> 00:00:15.400
help send data to Amazon Kinesis.

6
00:00:15.400 --> 00:00:16.450
Specifically,

7
00:00:16.450 --> 00:00:19.890
it's commonly used to collect and aggregate record data in order to

8
00:00:19.890 --> 00:00:23.340
optimize the way you send it to Kinesis more efficiently.

9
00:00:23.340 --> 00:00:27.410
This library is also especially useful when dealing with high‑rate producers,

10
00:00:27.410 --> 00:00:30.230
as it can help optimize the sending of data over to

11
00:00:30.230 --> 00:00:32.640
Kinesis, and how we package that together.

12
00:00:32.640 --> 00:00:34.870
So let's look at an example of this.

13
00:00:34.870 --> 00:00:36.060
To start,

14
00:00:36.060 --> 00:00:38.610
let's imagine we have a very similar scenario to the

15
00:00:38.610 --> 00:00:41.070
Kinesis agent example I explained earlier.

16
00:00:41.070 --> 00:00:44.330
Let's imagine we have a virtual machine somewhere in AWS,

17
00:00:44.330 --> 00:00:47.930
and inside of the virtual machine we have a Java application.

18
00:00:47.930 --> 00:00:51.920
Now this Java application will use the Producer Library to help collect and

19
00:00:51.920 --> 00:00:55.870
aggregate records and then send them over to the Kinesis stream.

20
00:00:55.870 --> 00:00:59.220
In a normal scenario, each record might be sent individually,

21
00:00:59.220 --> 00:00:59.650
however,

22
00:00:59.650 --> 00:01:03.460
the Producer Library can take disparate records and then batch them

23
00:01:03.460 --> 00:01:06.610
up together before sending them over to Kinesis.

24
00:01:06.610 --> 00:01:10.640
This allows us to optimize the ingestion of data by the Kinesis stream.

25
00:01:10.640 --> 00:01:12.890
So after this data is batched together,

26
00:01:12.890 --> 00:01:18.040
it can be sent over to the Kinesis data stream as a single aggregate record.

27
00:01:18.040 --> 00:01:19.600
And when it's sent over,

28
00:01:19.600 --> 00:01:22.890
then any consumer of the Kinesis data stream could then

29
00:01:22.890 --> 00:01:25.920
read that particular aggregated record using something

30
00:01:25.920 --> 00:01:28.640
like the Kinesis Client Library, for example,

31
00:01:28.640 --> 00:01:33.090
and then process that stream data and retrieve the independent parts of

32
00:01:33.090 --> 00:01:36.840
that aggregate record and process them individually.

33
00:01:36.840 --> 00:01:39.850
There are two kinds of batching that the Kinesis Producer Library

34
00:01:39.850 --> 00:01:43.190
can accomplish, aggregation and collection.

35
00:01:43.190 --> 00:01:45.700
These two techniques can be used to reduce the network

36
00:01:45.700 --> 00:01:48.590
requirements and improve ingestion performance.

37
00:01:48.590 --> 00:01:51.340
So let's take a look at what each of these does.

38
00:01:51.340 --> 00:01:54.100
First, let's look at aggregation.

39
00:01:54.100 --> 00:01:57.700
Aggregation is when the Kinesis Producer Library puts multiple

40
00:01:57.700 --> 00:02:00.340
data records in the same Kinesis data record.

41
00:02:00.340 --> 00:02:03.610
You might start with a data record inside of your application,

42
00:02:03.610 --> 00:02:06.860
maybe some sold item or something else like that.

43
00:02:06.860 --> 00:02:10.610
Now normally you might put this sold item inside of a Kinesis data

44
00:02:10.610 --> 00:02:13.480
record and then send it over to the stream, however,

45
00:02:13.480 --> 00:02:14.350
in this case,

46
00:02:14.350 --> 00:02:18.910
maybe we take that record and aggregate it with other records inside the same

47
00:02:18.910 --> 00:02:23.590
Kinesis data record, so in this Kinesis data record maybe we had three

48
00:02:23.590 --> 00:02:26.900
different items that were sold in that particular example.

49
00:02:26.900 --> 00:02:29.980
Now all of this is possible because the Kinesis data record

50
00:02:29.980 --> 00:02:33.250
data blob where we store the information in Kinesis data

51
00:02:33.250 --> 00:02:36.350
records can contain any arbitrary data,

52
00:02:36.350 --> 00:02:38.760
so you can stuff multiple records in there as long as

53
00:02:38.760 --> 00:02:41.160
they conform to the size limitations.

54
00:02:41.160 --> 00:02:46.570
Now doing this helps avoid the 1 MB or 1000 records per second, per shard

55
00:02:46.570 --> 00:02:50.340
limits by reducing the overall size of data passed through,

56
00:02:50.340 --> 00:02:53.380
because then it requires less metadata, and by reducing the

57
00:02:53.380 --> 00:02:57.120
overall number of records by including multiple would‑be records

58
00:02:57.120 --> 00:03:00.240
inside of the same Kinesis data record.

59
00:03:00.240 --> 00:03:01.740
In addition to aggregation,

60
00:03:01.740 --> 00:03:05.410
you can also use collection with the Kinesis Producer Library in order

61
00:03:05.410 --> 00:03:08.660
to reduce the overall number of network requests.

62
00:03:08.660 --> 00:03:09.950
In order to do this,

63
00:03:09.950 --> 00:03:13.570
you might have two Kinesis data records already created. Now rather

64
00:03:13.570 --> 00:03:16.340
than sending them over to Kinesis one at a time,

65
00:03:16.340 --> 00:03:21.340
you could collect several at once to send them over in the same HTTP request.

66
00:03:21.340 --> 00:03:24.750
Now what this would mean is that you would make a single network request

67
00:03:24.750 --> 00:03:29.040
sending this collection of records over to a Kinesis data stream.

68
00:03:29.040 --> 00:03:32.230
This will reduce the network overhead and improve the performance of the

69
00:03:32.230 --> 00:03:36.350
producer overall by reducing the number of HTTP requests you need to make

70
00:03:36.350 --> 00:03:40.780
for the same number of records. You can also use both collection and

71
00:03:40.780 --> 00:03:43.220
aggregation together simultaneously.

72
00:03:43.220 --> 00:03:47.170
Let's imagine we have a similar Amazon EC2 instance with a Java app inside

73
00:03:47.170 --> 00:03:50.680
of it for this purpose. We can start with the aggregation of these data

74
00:03:50.680 --> 00:03:54.240
records into one single Kinesis data record.

75
00:03:54.240 --> 00:03:58.430
Now, we can also collect multiple of these aggregated Kinesis data

76
00:03:58.430 --> 00:04:02.850
records into a single request that we want to send over, so this

77
00:04:02.850 --> 00:04:07.280
collection of multiple Kinesis data records, each containing other pieces

78
00:04:07.280 --> 00:04:11.720
of information, and then the Producer Library will send it over a

79
00:04:11.720 --> 00:04:16.210
aggregate record of this collection of multiple Kinesis data records over

80
00:04:16.210 --> 00:04:18.880
to the Kinesis data stream.

81
00:04:18.880 --> 00:04:20.970
So in addition to what we've seen so far,

82
00:04:20.970 --> 00:04:24.050
why would we want to use the Producer Library? To start, it will

83
00:04:24.050 --> 00:04:27.910
automatically handle error and retry logic behind the scenes for you.

84
00:04:27.910 --> 00:04:31.080
You'll also get lots of built‑in functionality to help you batch

85
00:04:31.080 --> 00:04:34.450
data together in ways that will optimize your network requests and

86
00:04:34.450 --> 00:04:37.140
avoid Kinesis data streams limitations.

87
00:04:37.140 --> 00:04:41.440
The Producer Library also automatically sends data into Amazon CloudWatch,

88
00:04:41.440 --> 00:04:44.230
which is a service that can help you monitor and review

89
00:04:44.230 --> 00:04:46.380
what's happening in your application.

90
00:04:46.380 --> 00:04:47.160
Finally,

91
00:04:47.160 --> 00:04:50.120
the Producer Library works effectively with the Kinesis Client

92
00:04:50.120 --> 00:04:56.000
Libraries to help you build out consumers that can leverage some of these benefits effectively.

