WEBVTT
1
00:00:00.970 --> 00:00:03.480
So now that we understand a little bit more about

2
00:00:03.480 --> 00:00:06.100
Kinesis Data Streams and Kinesis Firehose,

3
00:00:06.100 --> 00:00:09.270
let's compare the two different options to see when we

4
00:00:09.270 --> 00:00:11.530
might want to use one over the other.

5
00:00:11.530 --> 00:00:14.920
This will be a really important part if you end up deciding to use

6
00:00:14.920 --> 00:00:18.180
these in your own applications or you end up taking an AWS

7
00:00:18.180 --> 00:00:21.590
certification exam that compares the two options.

8
00:00:21.590 --> 00:00:22.410
First,

9
00:00:22.410 --> 00:00:25.370
you might want to use Kinesis Firehose if you're okay with

10
00:00:25.370 --> 00:00:27.550
having a one‑minute delay for your data.

11
00:00:27.550 --> 00:00:31.050
Now it's possible the data might be delivered more rapidly

12
00:00:31.050 --> 00:00:34.730
than that if you quickly fill up the buffer inside of the

13
00:00:34.730 --> 00:00:37.380
Kinesis Firehose with that buffer size.

14
00:00:37.380 --> 00:00:38.060
However,

15
00:00:38.060 --> 00:00:42.770
it is possible that even if you set the minimum interval for Kinesis Firehose,

16
00:00:42.770 --> 00:00:47.680
the data does take 60 seconds to process through your Kinesis Firehose system.

17
00:00:47.680 --> 00:00:52.130
With Kinesis Data Streams, however, you might want the data in near real time,

18
00:00:52.130 --> 00:00:55.540
and you could use Kinesis Data Streams to help process the

19
00:00:55.540 --> 00:00:58.740
data in that near real‑time fashion.

20
00:00:58.740 --> 00:01:00.220
With Kinesis Firehose,

21
00:01:00.220 --> 00:01:03.920
you're going to get a more managed service by AWS where you don't have

22
00:01:03.920 --> 00:01:05.910
to deal with any of the underlying infrastructure.

23
00:01:05.910 --> 00:01:09.380
Now this is somewhat applicable to Kinesis Data Streams,

24
00:01:09.380 --> 00:01:12.380
but you're going to be able to control the capacity for

25
00:01:12.380 --> 00:01:14.840
yourself when you're using that service.

26
00:01:14.840 --> 00:01:16.960
When you're dealing with the shards and the

27
00:01:16.960 --> 00:01:19.220
sharding of your Kinesis Data Streams,

28
00:01:19.220 --> 00:01:22.470
you're effectively determining how much capacity you want and

29
00:01:22.470 --> 00:01:25.120
when you want to increase or decrease that.

30
00:01:25.120 --> 00:01:29.930
Kinesis Firehose also only supports specific destinations.

31
00:01:29.930 --> 00:01:33.200
And if you're not able to use those destinations in a

32
00:01:33.200 --> 00:01:37.240
way that suits your application, it might not be the best fit for you.

33
00:01:37.240 --> 00:01:39.030
Now with Kinesis Data Streams,

34
00:01:39.030 --> 00:01:42.060
you can support multiple data consumers and process that

35
00:01:42.060 --> 00:01:44.780
data in really any way you'd like.

36
00:01:44.780 --> 00:01:47.950
Now this isn't to say that Kinesis Firehose can't deliver data to other

37
00:01:47.950 --> 00:01:50.920
places if you add other layers of your application.

38
00:01:50.920 --> 00:01:54.580
For example, if you send data from Kinesis Firehose to S3,

39
00:01:54.580 --> 00:01:58.610
you could then trigger events based on that data landing in S3 and use

40
00:01:58.610 --> 00:02:03.210
things like Lambda functions or EC2 services to move that data basically

41
00:02:03.210 --> 00:02:05.840
anywhere you want and do anything you want with it.

42
00:02:05.840 --> 00:02:10.040
But it's still limited in terms of its configured destinations.

43
00:02:10.040 --> 00:02:11.730
So in addition to this,

44
00:02:11.730 --> 00:02:15.960
Kinesis Firehose can only use Lambda for data transformations,

45
00:02:15.960 --> 00:02:18.770
unless you write your own application layer after

46
00:02:18.770 --> 00:02:21.790
everything's ended up in something like S3.

47
00:02:21.790 --> 00:02:23.430
Kinesis Data Streams, however,

48
00:02:23.430 --> 00:02:26.480
will allow multiple different kinds of consumers

49
00:02:26.480 --> 00:02:28.240
that can process the data streams.

50
00:02:28.240 --> 00:02:31.650
And they can process them in many different languages and aren't bound to

51
00:02:31.650 --> 00:02:36.290
be only inside of the AWS Lambda execution environment.

52
00:02:36.290 --> 00:02:37.690
With Kinesis Firehose,

53
00:02:37.690 --> 00:02:42.150
it'll also automatically retry its deliveries to your destinations,

54
00:02:42.150 --> 00:02:45.500
contrasting the Kinesis Data Streams policy of having 24

55
00:02:45.500 --> 00:02:47.830
hours to 7 days of data retention,

56
00:02:47.830 --> 00:02:51.600
where you'll be able to go back and update your own consumers in order to

57
00:02:51.600 --> 00:02:54.540
make sure that they're processing the data correctly.

58
00:02:54.540 --> 00:02:57.790
Now keep in mind, if you're processing for Kinesis Firehose fails,

59
00:02:57.790 --> 00:03:01.320
you'll also be pretty limited in how long you have to fix that.

60
00:03:01.320 --> 00:03:04.680
So make sure that your processing is set up correctly and you're monitoring

61
00:03:04.680 --> 00:03:08.240
any errors when you're using either of these options.

62
00:03:08.240 --> 00:03:11.860
Now let's take a look at everything we've done inside of this module.

63
00:03:11.860 --> 00:03:15.920
We started by learning some basic concepts about Kinesis Firehose.

64
00:03:15.920 --> 00:03:21.240
Then we learned about Firehose's producers and the destinations that it offers.

65
00:03:21.240 --> 00:03:22.030
From there,

66
00:03:22.030 --> 00:03:27.140
we learned how we could transform data using Kinesis Firehose and AWS Lambda.

67
00:03:27.140 --> 00:03:29.230
After that, we actually did a demo,

68
00:03:29.230 --> 00:03:32.420
and we learned how to use Kinesis Data Streams to send data

69
00:03:32.420 --> 00:03:35.930
into Kinesis Firehose and then have that be processed by

70
00:03:35.930 --> 00:03:38.740
AWS Lambda and output to S3.

71
00:03:38.740 --> 00:03:43.010
Now this let us change the JSON file into a CSV file for any

72
00:03:43.010 --> 00:03:46.200
people in our financial part of our organization who might be

73
00:03:46.200 --> 00:03:47.990
more comfortable with that format.

74
00:03:47.990 --> 00:03:48.950
And, finally,

75
00:03:48.950 --> 00:03:52.930
we just looked at how Firehose and Data Streams differ and what

76
00:03:52.930 --> 00:03:56.140
cases we might want to use one over the other.

77
00:03:56.140 --> 00:03:57.000
So, hopefully,

78
00:03:57.000 --> 00:04:00.770
you'll stay tuned for the next module where we'll be looking more at using

79
00:04:00.770 --> 00:04:11.000
data transfer inside of AWS with Snowball and Direct Connect, two good options for when you need to transfer larger amounts of data into AWS.

