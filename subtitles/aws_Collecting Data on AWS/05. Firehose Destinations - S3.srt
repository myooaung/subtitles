1
00:00:01,100 --> 00:00:03,000
Let's look at different destinations that we can

2
00:00:03,000 --> 00:00:06,040
configure with Kinesis Data Firehose.

3
00:00:06,040 --> 00:00:09,840
First, you can send your data along to Amazon S3. Now,

4
00:00:09,840 --> 00:00:14,030
this would act as a long‑term object storage for any data that you have.

5
00:00:14,030 --> 00:00:14,670
However,

6
00:00:14,670 --> 00:00:17,000
if you wanted something a bit more flexible that you could

7
00:00:17,000 --> 00:00:19,290
still interact with and maybe searched through,

8
00:00:19,290 --> 00:00:23,240
you could send it to something like the Amazon Elasticsearch Service.

9
00:00:23,240 --> 00:00:26,120
You could also send it to something like Amazon Redshift,

10
00:00:26,120 --> 00:00:29,060
which would allow you to perform more data analytics‑style

11
00:00:29,060 --> 00:00:32,040
queries rather than search‑based queries.

12
00:00:32,040 --> 00:00:35,750
Another option for a destination is to send it along to Splunk,

13
00:00:35,750 --> 00:00:39,840
another tool that might help you visualize and interact with your data.

14
00:00:39,840 --> 00:00:41,470
Now, with all of these destinations,

15
00:00:41,470 --> 00:00:44,710
there's some configuration that we can do to determine how

16
00:00:44,710 --> 00:00:47,370
we're going to send the data along and a few different options

17
00:00:47,370 --> 00:00:49,120
that we might want to take with it.

18
00:00:49,120 --> 00:00:52,540
So let's look at each of these scenarios in more detail.

19
00:00:52,540 --> 00:00:57,140
First, we'll look at sending data to Amazon S3 using Kinesis Firehose.

20
00:00:57,140 --> 00:01:01,170
We'll start with just Kinesis Firehose and a destination bucket we want to

21
00:01:01,170 --> 00:01:06,190
send the data to. As data comes in to Kinesis Firehose, either the buffer

22
00:01:06,190 --> 00:01:09,360
interval or the buffer size will eventually be hit,

23
00:01:09,360 --> 00:01:14,090
and at that point we would send our data over to a destination S3 bucket.

24
00:01:14,090 --> 00:01:17,870
Now, there is also another option where we might process the data.

25
00:01:17,870 --> 00:01:21,430
So, as it comes into the Kinesis Firehose delivery stream,

26
00:01:21,430 --> 00:01:25,720
it would then be either the buffer interval or buffer size would then trigger

27
00:01:25,720 --> 00:01:30,910
it to go over to Lambda. From there, Lambda would then process all this data

28
00:01:30,910 --> 00:01:34,450
and either turn it into successfully processed data,

29
00:01:34,450 --> 00:01:36,780
dropped records, or failed records.

30
00:01:36,780 --> 00:01:37,800
Now, with this,

31
00:01:37,800 --> 00:01:40,920
we would then send the failed records and the successfully

32
00:01:40,920 --> 00:01:44,370
processed records to Kinesis Firehose to be then moved back

33
00:01:44,370 --> 00:01:47,240
along to a destination S3 bucket,

34
00:01:47,240 --> 00:01:50,230
and we could split up the successful and failed records to isolate

35
00:01:50,230 --> 00:01:52,810
them a little bit better and review them later for more details

36
00:01:52,810 --> 00:01:54,940
about why they failed the processing.

37
00:01:54,940 --> 00:01:58,690
Another option with Amazon S3 is to do a pretty similar case,

38
00:01:58,690 --> 00:02:02,960
except with a backup bucket where we store all of our original data.

39
00:02:02,960 --> 00:02:05,790
So when data comes into the Kinesis Firehose and hits its

40
00:02:05,790 --> 00:02:08,220
interval or buffer size limitations,

41
00:02:08,220 --> 00:02:13,400
then it gets moved over to both Lambda for processing and the optional backup

42
00:02:13,400 --> 00:02:17,920
bucket with an initial unprocessed copy of that data in case we want to keep

43
00:02:17,920 --> 00:02:21,810
that around for the long term so we have another thing to replay if we need to

44
00:02:21,810 --> 00:02:24,690
reprocess it or analyze it in a different way.

45
00:02:24,690 --> 00:02:25,850
Now from there, of course,

46
00:02:25,850 --> 00:02:28,530
Lambda then processes the data and does the exact

47
00:02:28,530 --> 00:02:32,270
same thing it was doing earlier, where it takes the successful records,

48
00:02:32,270 --> 00:02:35,010
the dropped records, and the failed records and treats

49
00:02:35,010 --> 00:02:37,810
them differently. The dropped records it's going to leave

50
00:02:37,810 --> 00:02:39,280
alone and not do anything to,

51
00:02:39,280 --> 00:02:42,380
whereas with the successful and the failed records, it'll move over to

52
00:02:42,380 --> 00:02:45,640
the destination bucket where we can then review it.

53
00:02:45,640 --> 00:02:53,000
So these are some of the options we can configure with Kinesis Firehose when we're using the destination of Amazon S3.

