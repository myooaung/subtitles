WEBVTT
1
00:00:00.130 --> 00:00:02.590
So now that we've defined the put_to_stream function,

2
00:00:02.590 --> 00:00:06.010
let's run these few lines of code here to run the function over and

3
00:00:06.010 --> 00:00:09.460
over again with a little bit of time in between it in order to start

4
00:00:09.460 --> 00:00:11.950
sending data into our Kinesis data stream.

5
00:00:11.950 --> 00:00:13.320
So, let's hit Enter there.

6
00:00:13.320 --> 00:00:14.560
Now this will keep running,

7
00:00:14.560 --> 00:00:17.470
and it's not going to stop or provide any input to us,

8
00:00:17.470 --> 00:00:20.840
but as long as it's paused here, that means we're sending data into that stream.

9
00:00:20.840 --> 00:00:24.530
And make sure that you run this code after you've already created the

10
00:00:24.530 --> 00:00:26.990
stream and it's in the active state like we just saw.

11
00:00:26.990 --> 00:00:29.980
Otherwise, the data won't go in and you might get an error here,

12
00:00:29.980 --> 00:00:31.540
or you might need to restart it.

13
00:00:31.540 --> 00:00:36.110
So now that we've finished this, we'll go over to the analytics.sql portion here.

14
00:00:36.110 --> 00:00:36.340
Now,

15
00:00:36.340 --> 00:00:40.730
I'm going to use this SQL code inside of analytics.sql to create

16
00:00:40.730 --> 00:00:44.820
a Kinesis Data Analytics application that processes all the data

17
00:00:44.820 --> 00:00:46.650
inside of our Kinesis data stream.

18
00:00:46.650 --> 00:00:51.080
So let's go back over to that and take a look inside of the AWS console.

19
00:00:51.080 --> 00:00:53.810
Now I already have my sales stream created here,

20
00:00:53.810 --> 00:00:56.900
so next we need to go to Analytics applications.

21
00:00:56.900 --> 00:01:00.250
You can get there directly from this side window here or from the higher

22
00:01:00.250 --> 00:01:03.840
level dashboard by clicking on Amazon Kinesis Data Analytics.

23
00:01:03.840 --> 00:01:05.670
Now, if you've already created an application,

24
00:01:05.670 --> 00:01:08.870
it might look different here, but we're going to start with Create application.

25
00:01:08.870 --> 00:01:10.600
I'm going to give this application a name.

26
00:01:10.600 --> 00:01:14.870
Let's call it salesapp, so it'll let us see all sales data.

27
00:01:14.870 --> 00:01:18.500
And we're going to use, in this case, SQL to process the data that's coming in,

28
00:01:18.500 --> 00:01:20.260
although we could also use Apache Flink,

29
00:01:20.260 --> 00:01:23.040
which is another tool for doing this sort of work.

30
00:01:23.040 --> 00:01:26.460
Now that we're done selecting the runtime and giving the application a name,

31
00:01:26.460 --> 00:01:29.440
we can scroll over and click Create application.

32
00:01:29.440 --> 00:01:31.080
So now we've created the application,

33
00:01:31.080 --> 00:01:33.800
we have to configure it with some streaming data.

34
00:01:33.800 --> 00:01:36.240
So I'm going to click Connect streaming data here,

35
00:01:36.240 --> 00:01:40.360
and if we go back up, there's a few different options for how we configure this.

36
00:01:40.360 --> 00:01:43.070
We could have a source that we already have in existence,

37
00:01:43.070 --> 00:01:46.630
or we could create a new stream by configuring a new stream.

38
00:01:46.630 --> 00:01:47.150
In this case,

39
00:01:47.150 --> 00:01:49.810
I'm going to stick with the source that we've already had

40
00:01:49.810 --> 00:01:51.810
auto‑selected here of Choose source,

41
00:01:51.810 --> 00:01:55.390
and then I'm going to leave the Kinesis data stream here selected as well.

42
00:01:55.390 --> 00:01:56.730
Now, to make this easier to see,

43
00:01:56.730 --> 00:01:59.070
let's close out this window up at the top there so we

44
00:01:59.070 --> 00:02:01.340
have a fuller view of what's going on.

45
00:02:01.340 --> 00:02:01.610
Now,

46
00:02:01.610 --> 00:02:04.540
the Kinesis data stream we want to use should appear for us

47
00:02:04.540 --> 00:02:06.790
automatically because we already created it.

48
00:02:06.790 --> 00:02:09.020
So I'm going to select the sales stream here,

49
00:02:09.020 --> 00:02:11.770
and instead of creating a new one or anything else like that,

50
00:02:11.770 --> 00:02:14.700
we'll just leave it selected and continue scrolling down.

51
00:02:14.700 --> 00:02:15.100
Now,

52
00:02:15.100 --> 00:02:18.880
we could pre‑process data with AWS Lambda or change some of the

53
00:02:18.880 --> 00:02:20.610
ways that permissions are happening here,

54
00:02:20.610 --> 00:02:23.230
but we're going to leave all these defaults the same and

55
00:02:23.230 --> 00:02:24.730
scroll down to the Schema section.

56
00:02:24.730 --> 00:02:26.420
Before we save this application,

57
00:02:26.420 --> 00:02:29.760
we need to discover what the schema of the incoming data looks like,

58
00:02:29.760 --> 00:02:33.250
and Kinesis Data Analytics should do this for us automatically.

59
00:02:33.250 --> 00:02:33.430
Now,

60
00:02:33.430 --> 00:02:36.850
it might take a moment or two for this data to go into the stream

61
00:02:36.850 --> 00:02:39.010
that we were sending it in from our Python script,

62
00:02:39.010 --> 00:02:42.340
and then allow that to be processed and discovered in Schema.

63
00:02:42.340 --> 00:02:42.670
So,

64
00:02:42.670 --> 00:02:45.350
now you'll see this is about what the schema looks like

65
00:02:45.350 --> 00:02:47.600
going into our Kinesis data stream.

66
00:02:47.600 --> 00:02:50.160
We're creating a unique ID for the sale_id,

67
00:02:50.160 --> 00:02:52.990
then we also have this COL_timestamp value,

68
00:02:52.990 --> 00:02:56.240
which is an epoch integer timestamp for when this actually

69
00:02:56.240 --> 00:02:58.210
happened and was sent into the stream.

70
00:02:58.210 --> 00:03:01.370
From there, we have the product_sold, a raincoat,

71
00:03:01.370 --> 00:03:03.840
jeans, a shirt or any of those other items,

72
00:03:03.840 --> 00:03:06.840
and the number of items that were sold in a particular instance.

73
00:03:06.840 --> 00:03:09.140
Now, if we wanted to start processing this data,

74
00:03:09.140 --> 00:03:11.490
we can go down and click Save and continue,

75
00:03:11.490 --> 00:03:15.910
and then our application will allow us to run some SQL against this data.

76
00:03:15.910 --> 00:03:16.310
So,

77
00:03:16.310 --> 00:03:19.420
let's go over to the SQL editor so we can start running our SQL

78
00:03:19.420 --> 00:03:22.570
code against our Kinesis Data Analytics application and the

79
00:03:22.570 --> 00:03:24.580
incoming data from the Kinesis data stream.

80
00:03:24.580 --> 00:03:27.340
We'll need to click Yes, start application.

81
00:03:27.340 --> 00:03:31.520
Now we'll be given a SQL editor that we can write our own SQL in,

82
00:03:31.520 --> 00:03:34.760
and there's a variety of different templates we could choose from that AWS

83
00:03:34.760 --> 00:03:38.090
offers for us against the data that's inside of our stream.

84
00:03:38.090 --> 00:03:41.390
However, these are all examples that pertain to different datasets,

85
00:03:41.390 --> 00:03:43.820
and we need to modify them for our purposes.

86
00:03:43.820 --> 00:03:44.750
So, in this case,

87
00:03:44.750 --> 00:03:48.200
we're going to avoid using these examples and return to the editor,

88
00:03:48.200 --> 00:03:50.970
and we'll be pasting our own code into the SQL editor here.

89
00:03:50.970 --> 00:03:54.340
So let me go back to my VS Code editor and grab the

90
00:03:54.340 --> 00:03:55.890
code that we're using in this case.

91
00:03:55.890 --> 00:03:57.670
I'm going to copy the entire thing,

92
00:03:57.670 --> 00:04:00.580
and we'll go back over here to the SQL editor and paste it

93
00:04:00.580 --> 00:04:02.180
over everything that's currently in here,

94
00:04:02.180 --> 00:04:04.590
so that we can look at it before we get started.

95
00:04:04.590 --> 00:04:07.360
So now that we've pasted this in, let's click Save and run,

96
00:04:07.360 --> 00:04:11.040
and then review what's going on. At the very top, we're creating or

97
00:04:11.040 --> 00:04:13.950
replacing a stream called DESTINATION_SQL_STREAM,

98
00:04:13.950 --> 00:04:17.790
and we're creating two different columns inside of this creation,

99
00:04:17.790 --> 00:04:19.500
the product_sold and the total_items,

100
00:04:19.500 --> 00:04:23.010
and we're getting both of these from the incoming data.

101
00:04:23.010 --> 00:04:25.940
So when we created that Python script that had the

102
00:04:25.940 --> 00:04:29.440
product_sold value all the way up in the top here,

103
00:04:29.440 --> 00:04:32.940
where it was randomly choosing between socks and jackets and other things,

104
00:04:32.940 --> 00:04:36.360
this is getting directly piped into our Kinesis data stream, and

105
00:04:36.360 --> 00:04:39.650
ends up showing up in our Kinesis Data Analytics application so

106
00:04:39.650 --> 00:04:41.510
we can use that value coming in.

107
00:04:41.510 --> 00:04:44.160
Then we're also going to want to see the total items for each of

108
00:04:44.160 --> 00:04:46.910
these products, and in order to get all this data,

109
00:04:46.910 --> 00:04:49.810
we'll have this new section here which uses a bunch of

110
00:04:49.810 --> 00:04:52.440
Kinesis Data Analytics specific syntax.

111
00:04:52.440 --> 00:04:54.660
I'm not going to dive into too much detail there.

112
00:04:54.660 --> 00:04:55.210
Essentially,

113
00:04:55.210 --> 00:04:57.860
all you really need to know is that we're processing that

114
00:04:57.860 --> 00:05:00.420
incoming data from the Kinesis data stream,

115
00:05:00.420 --> 00:05:03.330
and we're selecting particular values from it.

116
00:05:03.330 --> 00:05:03.990
In this case,

117
00:05:03.990 --> 00:05:07.480
we're looking at the product_sold and a sum of the number

118
00:05:07.480 --> 00:05:09.740
of items for that particular product.

119
00:05:09.740 --> 00:05:12.880
And then we're looking at that from the SOURCE_SQL_STREAM_001

120
00:05:12.880 --> 00:05:17.450
that is by default what we're calling the input source for our

121
00:05:17.450 --> 00:05:19.840
Kinesis Data Analytics application.

122
00:05:19.840 --> 00:05:22.560
From there, we're grouping by the name of the product sold,

123
00:05:22.560 --> 00:05:27.040
and we're also taking a window of 10 seconds and looking at all the

124
00:05:27.040 --> 00:05:29.900
products sold within those 10 seconds. Finally,

125
00:05:29.900 --> 00:05:32.520
we're ordering by that same window so we can see these

126
00:05:32.520 --> 00:05:34.350
chronologically as they go through,

127
00:05:34.350 --> 00:05:37.310
and also ordering by the number of items so we can see what the

128
00:05:37.310 --> 00:05:40.100
top items within these 10‑second windows are.

129
00:05:40.100 --> 00:05:43.100
Now there's a lot of flexibility to what you could do with SQL.

130
00:05:43.100 --> 00:05:46.680
This is just one example that will help us visualize that data coming in.

131
00:05:46.680 --> 00:05:48.940
So, let's scroll down and see what happened.

132
00:05:48.940 --> 00:05:50.460
Now I've been talking for a little while,

133
00:05:50.460 --> 00:05:52.690
so we have a bunch of data already in here.

134
00:05:52.690 --> 00:05:53.640
Now, as you can see,

135
00:05:53.640 --> 00:05:59.230
we have these 10‑second windows starting with 5‑27‑20 where we have

136
00:05:59.230 --> 00:06:03.470
all this data right here, and this is going to show us that the top

137
00:06:03.470 --> 00:06:06.320
most sold item in this period was a jacket,

138
00:06:06.320 --> 00:06:08.700
where we sold 16 jackets in this period.

139
00:06:08.700 --> 00:06:12.330
And if we scroll a little further down, we can look at the next period,

140
00:06:12.330 --> 00:06:15.110
which has the top sold item as a raincoat.

141
00:06:15.110 --> 00:06:17.700
And if we keep scrolling through these different periods,

142
00:06:17.700 --> 00:06:22.040
you'll see that they each have their own top items for the 10‑second period.

143
00:06:22.040 --> 00:06:23.790
And we just got a bunch more data added,

144
00:06:23.790 --> 00:06:26.290
so that scooted everything over, but let's look at this period

145
00:06:26.290 --> 00:06:28.320
here where we have the shirt at the top.

146
00:06:28.320 --> 00:06:30.880
And there's lots of different aggregations we could do,

147
00:06:30.880 --> 00:06:33.530
this is just one example of how you can manipulate some of

148
00:06:33.530 --> 00:06:36.750
this data and get the results that you want to see out of all

149
00:06:36.750 --> 00:06:38.350
of the streaming data coming in.

150
00:06:38.350 --> 00:06:39.930
Now, these are sampled results,

151
00:06:39.930 --> 00:06:43.130
but if we wanted to output the actual results to an application,

152
00:06:43.130 --> 00:06:46.300
we could then hook this up to a destination.

153
00:06:46.300 --> 00:06:48.760
Now this would involve having our application send data

154
00:06:48.760 --> 00:06:51.520
over to a Kinesis data stream or a Kinesis delivery

155
00:06:51.520 --> 00:06:54.340
stream or an AWS Lambda function.

156
00:06:54.340 --> 00:06:56.840
We're not going to do this right now, because it will add a little bit

157
00:06:56.840 --> 00:06:59.220
more complexity for the cleanup process for this demo,

158
00:06:59.220 --> 00:07:02.580
but now you should know how to send in streaming data from a Kinesis

159
00:07:02.580 --> 00:07:05.940
data stream to a Kinesis Data Analytics application.

160
00:07:05.940 --> 00:07:07.880
Now in order to make sure you're not charged for

161
00:07:07.880 --> 00:07:09.910
continuing to use these resources,

162
00:07:09.910 --> 00:07:15.000
hop over to the next video to see how to remove them and save yourself some money.

