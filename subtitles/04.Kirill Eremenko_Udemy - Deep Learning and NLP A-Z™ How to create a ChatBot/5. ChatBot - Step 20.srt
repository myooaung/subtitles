1
00:00:00,560 --> 00:00:06,440
Hello and welcome to this new very exciting tutorial and this one we're going to start creating the

2
00:00:06,530 --> 00:00:10,030
architecture eventually of our set to take model.

3
00:00:10,160 --> 00:00:11,800
This architecture is twofold.

4
00:00:11,900 --> 00:00:18,260
We'll first create the encoder Arnon layer because this what comes first in the architecture of the

5
00:00:18,260 --> 00:00:22,820
sect to morrow and then we'll create the decoder Arnon layer.

6
00:00:22,820 --> 00:00:23,900
So you got this right.

7
00:00:23,960 --> 00:00:29,870
We have the neural network for both the anchor and the decoder will be an Arnon.

8
00:00:29,870 --> 00:00:36,170
It will not be a simple iron and rest assured it will be stacked episteme which reply drop out to improve

9
00:00:36,170 --> 00:00:40,620
as much as we can the accuracy and we'll create all this in the next tutorials.

10
00:00:40,850 --> 00:00:44,400
And today we're going to start with the encoder Arnon there.

11
00:00:44,450 --> 00:00:50,540
And so we're going to make a function that will define in some way the architecture of this anchor RNA

12
00:00:50,570 --> 00:00:51,100
layer.

13
00:00:51,290 --> 00:00:56,560
I hesitated to include either LACMA or Recker and unit.

14
00:00:56,570 --> 00:01:03,490
In fact I first implemented a child in PI torch and this implementation I included BGR you get redirector

15
00:01:03,520 --> 00:01:07,030
in the unit I will share with you this model at the end of this course.

16
00:01:07,190 --> 00:01:12,560
But in this one with tons of flow we are going to include an LSM instead of a jar.

17
00:01:12,830 --> 00:01:16,790
And for this we're going to use the basic LACMA Cell class by tens of flow.

18
00:01:17,060 --> 00:01:17,890
So let's do this.

19
00:01:17,900 --> 00:01:19,220
I can't wait to start.

20
00:01:19,340 --> 00:01:21,170
Let's make this function happen.

21
00:01:21,170 --> 00:01:23,150
So as I said it's a function.

22
00:01:23,150 --> 00:01:25,410
So we start with Def to define it.

23
00:01:25,580 --> 00:01:31,070
And now we need to give a name to this function and still in the spirit of making everything crystal

24
00:01:31,070 --> 00:01:39,500
clear we are going to call it and coater are and then later and coureur aren't in there and now be careful.

25
00:01:39,500 --> 00:01:40,370
Get ready.

26
00:01:40,370 --> 00:01:43,160
This function is going to take several arguments.

27
00:01:43,160 --> 00:01:49,740
The first one is the Arnon inputs so that corresponds to the model inputs.

28
00:01:49,790 --> 00:01:56,150
Remember we made this function that prepare the inputs of the models not to be confused with the questions

29
00:01:56,150 --> 00:02:01,500
inputs but inputs like the input questions the target answers the learning rate to keep prob..

30
00:02:01,640 --> 00:02:09,410
So that's the model inputs for the oron and basically then the next argument is going to be the Arnon

31
00:02:10,130 --> 00:02:18,110
size and the arland size is not to be confused with the number of layers in the Arnon the arland size

32
00:02:18,200 --> 00:02:23,070
is the number of input tensors of the encoder are in and they are we're making right now.

33
00:02:23,360 --> 00:02:30,000
So arland size then the next argument is this time the number of layers which I'm counting.

34
00:02:30,290 --> 00:02:32,840
None of those core layers.

35
00:02:33,050 --> 00:02:41,690
All right then the next argument is going to be our keep problem parameter and that we need to do that

36
00:02:41,690 --> 00:02:49,560
because we are going to apply a dropout regularisation to our Ellas GM will first make an industry and

37
00:02:49,640 --> 00:02:51,150
then we will apply dropout.

38
00:02:51,200 --> 00:02:53,950
So we will have a stacked LACMA with drop about.

39
00:02:54,020 --> 00:02:56,690
And this again is to improve the accuracy.

40
00:02:56,690 --> 00:03:06,260
So keep that control the dropout rate and then a final argument sequence underscore length which is

41
00:03:06,440 --> 00:03:10,440
the list of the length of each question in the batch.

42
00:03:10,730 --> 00:03:13,470
All right so that's the list of the lengths of the questions.

43
00:03:13,560 --> 00:03:18,860
Inside each batch and that's it our function is ready to be defined.

44
00:03:19,210 --> 00:03:22,330
OK so first we will create the LSD.

45
00:03:22,550 --> 00:03:23,740
How will we create it.

46
00:03:23,780 --> 00:03:29,120
Well thanks to a sense of where we have an amazing class that allows us to create it in a flashlight

47
00:03:29,150 --> 00:03:33,950
and actually only one line of code and that's the basic LACMA cell class.

48
00:03:33,950 --> 00:03:39,800
Therefore what I'm going to do right now is introduce a new variable that I'm going to call t m but

49
00:03:39,860 --> 00:03:46,940
that will actually represent an object that will be an object of the basic stemcell class and therefore

50
00:03:46,970 --> 00:03:52,290
I'm going to do right now is get this basic LACMA cell class from tensor flow.

51
00:03:52,520 --> 00:03:57,980
And so we need to start by getting tensor for here which has a shocker T.F. and then from terms of flow

52
00:03:58,250 --> 00:04:06,380
well we can directly get the class in terms of flow we need to go through first the contrib module then

53
00:04:06,620 --> 00:04:13,430
the Arnon submodule and then from there we can get the basic LACMA cell class.

54
00:04:13,460 --> 00:04:14,450
So let's get it.

55
00:04:14,450 --> 00:04:15,500
It is felt this way.

56
00:04:15,500 --> 00:04:17,390
Be careful with the capital letters.

57
00:04:17,390 --> 00:04:18,310
Capital B.

58
00:04:19,220 --> 00:04:27,430
Basic LSD em all in capital and capital C D L L bezique LC himself.

59
00:04:27,860 --> 00:04:33,740
And in parenthesis we need to put just one argument which is our Arlen's size which I remain corresponds

60
00:04:33,740 --> 00:04:38,060
to the number of input tensors in the layers.

61
00:04:38,180 --> 00:04:42,110
Let's put this r n n size as we called it.

62
00:04:42,110 --> 00:04:45,710
This is one of our arguments arland size and there we go.

63
00:04:45,710 --> 00:04:48,840
We now have our LACMA perfect.

64
00:04:48,950 --> 00:04:52,560
So now we're going to apply dropout to this.

65
00:04:52,850 --> 00:04:59,150
So busy going on what we'll get is the same but on which drop out was applied and therefore the new

66
00:04:59,150 --> 00:05:04,740
variable I'm going to introduce here is T.M. underscore dropout.

67
00:05:04,880 --> 00:05:10,970
Well you can call it however you want but with this name of the variable it's quite clear that we are

68
00:05:10,970 --> 00:05:15,340
just getting the same re-use as GM but with drop out applied.

69
00:05:15,650 --> 00:05:18,590
So I left him drop out and to apply rappelled.

70
00:05:18,590 --> 00:05:20,420
Well again that can be very simple.

71
00:05:20,420 --> 00:05:26,300
We're going to use another class which is a drop out wrapper class and which is some kind of wrapper

72
00:05:26,300 --> 00:05:31,200
that can wraps an already existing LACMA which is the one we just created.

73
00:05:31,310 --> 00:05:32,760
But with drop out applied.

74
00:05:32,840 --> 00:05:37,490
So I remind that dropout is the technique of dropping out.

75
00:05:37,490 --> 00:05:43,370
That is deactivating a certain percentage of the neurons during the training iterations.

76
00:05:43,370 --> 00:05:50,660
So the usual dropout rate is 20 percent which would mean that during the training iteration 20 percent

77
00:05:50,660 --> 00:05:55,160
of the neurons are like nonexisting their weights are not updated.

78
00:05:55,310 --> 00:06:01,880
That's why they are in some way deactivated so that's to drop out and so to apply it well we'll create

79
00:06:01,970 --> 00:06:08,460
a new object of the drop out wrapper class which will of course get as one of its arguments.

80
00:06:08,480 --> 00:06:10,700
The elist object we just created.

81
00:06:10,860 --> 00:06:11,720
So let's do it.

82
00:06:11,720 --> 00:06:14,510
Let's get it from sense of flow of course.

83
00:06:14,700 --> 00:06:15,740
Sense of flow.

84
00:06:15,740 --> 00:06:19,640
Then again we need to go to the module contrat.

85
00:06:19,670 --> 00:06:21,470
Then Arne again.

86
00:06:21,630 --> 00:06:32,840
And from this Arlon module we get the drop out there with capital D and then capital W wrapper class.

87
00:06:32,890 --> 00:06:33,220
Right.

88
00:06:33,240 --> 00:06:35,880
And this class takes several arguments.

89
00:06:35,880 --> 00:06:40,950
The first one is the record a neural network you want to apply dropout on.

90
00:06:41,040 --> 00:06:46,190
And that's of course our LACMA object that which is created here.

91
00:06:46,180 --> 00:06:54,060
So LCN and then the next argument is going to be our key prob of course because keep drub remind us

92
00:06:54,060 --> 00:06:56,230
to control the dropout rate.

93
00:06:56,370 --> 00:06:58,240
And since now we're dealing with drop out.

94
00:06:58,320 --> 00:07:02,030
Well that's the time to use it because it will control the dropout rate.

95
00:07:02,220 --> 00:07:14,300
So I'm adding here keep track but I will specify that keep prob is the input underscore keep prob parameter.

96
00:07:14,460 --> 00:07:18,930
So that's the name of one fixed argument of drab wrapper.

97
00:07:19,020 --> 00:07:21,520
This is not an arbitrary name I'm choosing.

98
00:07:21,540 --> 00:07:22,830
This is part of the class.

99
00:07:23,070 --> 00:07:28,390
But this argument should be held to that keep from argument.

100
00:07:28,500 --> 00:07:33,390
That is the argument we will put later of our encoder Arnon layer function.

101
00:07:33,420 --> 00:07:36,580
So keep this core prob here.

102
00:07:36,660 --> 00:07:37,570
Perfect.

103
00:07:37,600 --> 00:07:42,790
Now we have our LCN with drop out applied.

104
00:07:42,900 --> 00:07:43,490
Great.

105
00:07:43,500 --> 00:07:45,620
Now next step the next step.

106
00:07:45,630 --> 00:07:52,740
Now that we have our LSM with drop out applied and I see him drop out we are ready to create the encoder

107
00:07:52,750 --> 00:07:59,130
cell and we can only create it now because we are going to create it through again a very useful class

108
00:07:59,160 --> 00:08:04,820
by sense of flow which is the multiday Arnon Cell class and which takes an argument.

109
00:08:04,890 --> 00:08:12,980
Well of course the LACMA with drop out applied this LACMA drop out object by the drop out wrapper class.

110
00:08:13,200 --> 00:08:18,240
So what we need to do now is introduce a new variable and call your cell.

111
00:08:18,510 --> 00:08:24,000
And this core cell that will be the cell of the encoder not to be confused with the state that will

112
00:08:24,000 --> 00:08:25,370
create right afterwards.

113
00:08:25,380 --> 00:08:26,580
So that's the cell.

114
00:08:26,790 --> 00:08:28,260
And to get it.

115
00:08:28,380 --> 00:08:34,980
Well this will actually be an object of this Mahlzeit Arnon cell which we'll get again from sense of

116
00:08:34,980 --> 00:08:43,320
flow and then from the contrib module and then from the Arnon submodule And then there we go we can

117
00:08:43,320 --> 00:08:48,150
get the non-Thai are and and sell.

118
00:08:48,180 --> 00:08:49,910
Be careful with the capital letters.

119
00:08:50,130 --> 00:08:53,200
And then this class is going to take several arguments.

120
00:08:53,340 --> 00:09:00,060
Well first to asked him drop out later we created year times the number of layers to specify how many

121
00:09:00,060 --> 00:09:05,950
layers we want and our encoder Arnon and therefore how do we put this.

122
00:09:06,060 --> 00:09:12,750
Well we first need to put our as Jim drop out later in square brackets like this.

123
00:09:12,750 --> 00:09:21,390
So in square brackets I'm including our previews LSD and drop out object and then to make it several

124
00:09:21,390 --> 00:09:22,030
layers.

125
00:09:22,110 --> 00:09:28,470
Well I multiply that by another argument that we have and that gives us exactly the number of layers

126
00:09:28,770 --> 00:09:37,770
we plan to have in our encoded Arnon and which is none on the score layers underscore layers which will

127
00:09:37,770 --> 00:09:43,530
create the anchor Rousselle I remind If you have not followed the intuition lectures or if this is pretty

128
00:09:43,530 --> 00:09:44,350
new for you.

129
00:09:44,490 --> 00:09:49,750
Well the encoder cell is composed of several layers.

130
00:09:49,890 --> 00:09:54,900
And besides we applied dropout to each of these and I asked him there's that Sinco to sell.

131
00:09:54,900 --> 00:10:01,140
And again if this is pretty new for you I highly recommend to have a look at the annexes and especially

132
00:10:01,140 --> 00:10:07,530
the second annex at the end of this course because we explain the whole theory of Arnon and that will

133
00:10:07,530 --> 00:10:11,550
help you understand the difference between cells and states.

134
00:10:11,550 --> 00:10:12,810
All right so perfect.

135
00:10:12,810 --> 00:10:20,680
Now we have our ankers cell and now of course we'll take care of the anchor state and to get it.

136
00:10:20,740 --> 00:10:27,180
We're going to get it from the bi directional dynamic Arlon function by the end and the Mudgal this

137
00:10:27,180 --> 00:10:31,330
time it's a new module I a torch to an end module.

138
00:10:31,380 --> 00:10:35,320
I tend to flow and this will get us exactly what we want that is the output.

139
00:10:35,340 --> 00:10:38,980
One of the output of the Arnon which is the state.

140
00:10:39,180 --> 00:10:45,460
And so we are going to call this state and coater state.

141
00:10:45,460 --> 00:10:47,670
So we again for the long names of the variable.

142
00:10:47,670 --> 00:10:54,390
But that's because we will have then the decoder cell and state so I just want to make the clear difference

143
00:10:54,390 --> 00:10:56,750
between those two and go to state.

144
00:10:56,790 --> 00:11:03,120
However the interstate is going to be returned by the directional dynamic Arnon function by tens of

145
00:11:03,120 --> 00:11:11,430
flow but which returns two elements one of them is and go to state but any state is the second element

146
00:11:11,520 --> 00:11:17,080
returned by this bidirectional dynamic Arnon function and therefore to only get it because we only need

147
00:11:17,080 --> 00:11:24,930
it we have to add here underscore and then come to specify We only want the second element returned

148
00:11:25,320 --> 00:11:30,250
by is future directional dynamic and function of tensor flow.

149
00:11:30,520 --> 00:11:35,070
And speaking of tons of flow that's what we need to get first to get function.

150
00:11:35,200 --> 00:11:40,840
So the TFT out there remember we get it from the end and Mudgal inside the tent of the library.

151
00:11:41,050 --> 00:11:52,090
And then from this and then Munjal we get the B D Rex no underscore dynamic underscore our end and function.

152
00:11:52,300 --> 00:11:58,090
And this creates a dynamic version of a bidirectional record renewal network.

153
00:11:58,090 --> 00:12:03,580
Remember I told you we're not going to make a simple record a neural network we want to make our job

154
00:12:03,610 --> 00:12:09,010
but as powerful as possible and creating this dynamic version of a bidirectional record Renu and that

155
00:12:09,010 --> 00:12:11,140
work will definitely help.

156
00:12:11,140 --> 00:12:18,280
So this dynamic version of directional Arnon basically takes your input that we're about to answer right

157
00:12:18,280 --> 00:12:25,900
now and will build independent forward and backward Arnett's but be careful when you build this this

158
00:12:25,900 --> 00:12:32,680
kind of bidirectional dynamic Arnon we have to make sure that the input size of the forward cell and

159
00:12:32,680 --> 00:12:34,790
the backwards cell must match.

160
00:12:34,810 --> 00:12:38,390
That's the first thing we'll make sure of when entering these arguments.

161
00:12:38,410 --> 00:12:39,310
So let's do it.

162
00:12:39,490 --> 00:12:42,180
Let's take your first of the four Arnon.

163
00:12:42,190 --> 00:12:47,220
The argument for it is cell underscore f w forward.

164
00:12:47,500 --> 00:12:55,360
And this will be equal to r and code hersel object from the not Ireland cell that we just created in

165
00:12:55,360 --> 00:12:56,210
the previous line.

166
00:12:56,440 --> 00:13:04,370
Therefore the cell forward here is going to be equal to and Khoder cell then next argument is going

167
00:13:04,370 --> 00:13:13,130
to be for the backward Arnon and the name of the argument for that is sell underscore B.-W backward.

168
00:13:13,340 --> 00:13:19,100
And this one is going to be equal to again the encoder cell because it's like a B directional Recker

169
00:13:19,100 --> 00:13:22,370
a neural network that we're making meaning we have both directions.

170
00:13:22,490 --> 00:13:25,070
But on the same encoder cell.

171
00:13:25,130 --> 00:13:31,350
All right so the cell backwood here is going to be equal to the encoder cell again.

172
00:13:32,070 --> 00:13:39,680
Then the next argument is going to be the sequence and the score lengths which I remind is the list

173
00:13:39,680 --> 00:13:42,280
of the length of each question in the batch.

174
00:13:42,510 --> 00:13:49,410
So sequence length which this one is the name of the argument inside the existing bidirectional dynamic

175
00:13:49,500 --> 00:13:50,570
and function.

176
00:13:50,730 --> 00:13:58,170
But we have to set equal to the sequence argument which is one of our arguments a VAR function that

177
00:13:58,170 --> 00:13:59,120
we're making.

178
00:13:59,490 --> 00:14:07,310
OK then next argument is going to be our model inputs which are one of the VAR arguments here that are

179
00:14:07,320 --> 00:14:08,270
in an input.

180
00:14:08,370 --> 00:14:15,150
So inputs which is the name of the argument of the bidirectional dynamic or in function are you going

181
00:14:15,150 --> 00:14:19,170
to be equal to or are an input argument.

182
00:14:19,500 --> 00:14:30,130
And then finally one last argument to decide and this has to be equal to the float 32 type.

183
00:14:30,150 --> 00:14:33,360
I tend to follow T.F. that float 32.

184
00:14:33,360 --> 00:14:36,850
It's important to include it to make sure we are dealing with floats.

185
00:14:36,930 --> 00:14:37,230
All right.

186
00:14:37,230 --> 00:14:45,600
So congratulations you made the first pillar of this sector sic architecture to anchor the Arnon and

187
00:14:45,600 --> 00:14:51,660
so now of course we simply need to return the output of this and Goodere which is of course I remind

188
00:14:51,930 --> 00:14:58,530
the state and not the cell the cell will be part of the loop in the Arnon but will not be the output.

189
00:14:58,530 --> 00:14:59,790
The output is the.

190
00:14:59,840 --> 00:15:03,210
And go to state that we got in our last line here.

191
00:15:03,480 --> 00:15:06,390
So we have to return it and return it.

192
00:15:06,390 --> 00:15:13,200
There is nothing more simple we simply need to add return and coater on this core state.

193
00:15:13,200 --> 00:15:14,220
And there we go.

194
00:15:14,220 --> 00:15:19,800
We have our first function created that creates the encoder Arnon layer.

195
00:15:19,800 --> 00:15:20,660
Perfect.

196
00:15:20,670 --> 00:15:25,530
So now we're going to move on to the decoder and it's not going to be that simple.

197
00:15:25,570 --> 00:15:31,950
Will do it in three tutorials because we're going to make some cross-validation to separate some training

198
00:15:31,950 --> 00:15:39,000
data and then some Krus validated data so that cross-validation that's very important to reduce overfilling

199
00:15:39,030 --> 00:15:42,090
and improve the accuracy of new observations.

200
00:15:42,090 --> 00:15:47,340
So we're going to start by making that separation between the training data and the cross validation

201
00:15:47,340 --> 00:15:52,740
data and then we'll be ready to create our decoder Arnon layer.

202
00:15:52,980 --> 00:15:55,190
So let's do that in the three next it's.

203
00:15:55,320 --> 00:15:56,890
I'm going to execute this.

204
00:15:56,970 --> 00:15:59,550
And until then and then I'll be.
