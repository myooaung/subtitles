1
00:00:00,330 --> 00:00:03,250
Hello welcome to the course and deep natural language processing.

2
00:00:03,270 --> 00:00:06,730
And in two days the story we're going to talk about the plan of attack.

3
00:00:06,840 --> 00:00:12,810
We've got quite a diverse and involved intuition session coming up ahead.

4
00:00:12,810 --> 00:00:16,320
Lots of concept to grasp so it will be good for us too.

5
00:00:16,590 --> 00:00:20,400
At the start have a bird's eye view of the things that we're going to discuss.

6
00:00:20,640 --> 00:00:21,040
All right.

7
00:00:21,120 --> 00:00:25,590
Here's what we will learn in this section on intuition.

8
00:00:25,590 --> 00:00:29,890
First of all we'll talk about the types of natural language processing this will lay the foundation

9
00:00:29,890 --> 00:00:37,410
and we will discuss a Venn diagram which demonstrates this deep learning which is deep in our and shows

10
00:00:37,410 --> 00:00:42,810
us where the sequence to sequence what lies in that diagram will keep coming up because it will be very

11
00:00:42,810 --> 00:00:49,700
helpful for us to keep track of how we're progressing into the world of world of natural language processing.

12
00:00:49,890 --> 00:00:53,960
Then we'll talk about classical versus a deep learning models.

13
00:00:54,300 --> 00:00:57,990
Look at some actual examples of applications of a.p.

14
00:00:58,170 --> 00:01:04,500
And there's also be useful just to show us that natural language processing is not limited simply to

15
00:01:04,620 --> 00:01:05,410
chat boards.

16
00:01:05,460 --> 00:01:11,930
It's actually covers a huge range of different applications and we'll see a couple of them here.

17
00:01:11,940 --> 00:01:16,740
There will talk about entry into deep learning models and how they're different to enter in deep learning

18
00:01:16,740 --> 00:01:24,890
models and what they're advantages and then we'll talk about the bag of words and our model.

19
00:01:25,170 --> 00:01:33,770
We'll see two of its variations and this will help us gradually progress to the most advanced model.

20
00:01:34,030 --> 00:01:40,360
The sequence sequence which will start discussing in the next tutorial in the secrecy was architecture.

21
00:01:40,380 --> 00:01:42,190
This is the model that we're actually after.

22
00:01:42,190 --> 00:01:45,310
This is what we're implementing in our chat.

23
00:01:45,360 --> 00:01:47,330
Then we'll talk about the sequence of sequence training.

24
00:01:47,330 --> 00:01:50,730
So it's intentionally placed in this order.

25
00:01:50,730 --> 00:01:57,380
That first bit of architecture and we'll actually see it in action as if it was trained because that

26
00:01:57,380 --> 00:02:04,950
will help us better understand what is is desired from the training and then it will be easier to cover

27
00:02:04,950 --> 00:02:06,890
all the training afterwards.

28
00:02:07,080 --> 00:02:08,800
And that's why they're in that order.

29
00:02:08,890 --> 00:02:16,110
Then we'll talk about search decoding which is the way that the sequence secrecy secret architecture

30
00:02:16,440 --> 00:02:18,000
actually comes up.

31
00:02:18,000 --> 00:02:24,330
Wherefore the model comes up with the outputs that we want to see you know put the different.

32
00:02:24,480 --> 00:02:30,640
Very subtle very very interesting conceptual approach here for research decoding.

33
00:02:30,900 --> 00:02:33,680
And finally we'll talk about at mechanisms.

34
00:02:33,680 --> 00:02:41,400
So this is an additional augmentation of the SEC to sec model which helps with a longer term memory

35
00:02:41,640 --> 00:02:44,310
for the algorithm.

36
00:02:44,310 --> 00:02:48,630
Again a very interesting and quite new concept.

37
00:02:48,750 --> 00:02:52,920
And finally on top of all of that there's two Annexes.

38
00:02:52,920 --> 00:02:54,390
Make sure to check them out.

39
00:02:54,460 --> 00:02:58,440
Next one is artificial neural networks and to recurrent neural networks.

40
00:02:58,440 --> 00:03:05,230
So we included these tutorials easily intuitions tutorials only from our deeper recourse.

41
00:03:05,460 --> 00:03:10,460
And the reason that they're here is because you will need a good understanding of both of them.

42
00:03:10,620 --> 00:03:15,750
Once you dive into them more deeper insight out of things on this course.

43
00:03:15,840 --> 00:03:23,280
So for these Janis's you should make sure that you've covered them somewhere over here before you proceed

44
00:03:23,280 --> 00:03:28,650
to see what sort of architecture you once you proceed you will need to know in an hour and ends and

45
00:03:28,680 --> 00:03:31,310
LSD and which are covered in this part.

46
00:03:31,470 --> 00:03:35,130
So the first couple of stressors What is your final piece.

47
00:03:35,550 --> 00:03:41,490
But then once you go into these areas just make sure that you are familiar with Anand's and recurrent

48
00:03:41,520 --> 00:03:47,190
neural networks because that will be the foundation of our discussion.

49
00:03:47,790 --> 00:03:52,760
And there we go that's the plan of attack for the core intuition for this course.

50
00:03:52,800 --> 00:03:56,510
I really look forward to seeing you on the tutorials.

51
00:03:56,850 --> 00:04:00,460
And until next time enjoy natural language processing.
