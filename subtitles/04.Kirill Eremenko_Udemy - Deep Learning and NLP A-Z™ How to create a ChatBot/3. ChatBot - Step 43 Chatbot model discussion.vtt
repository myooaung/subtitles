WEBVTT
1
00:00:01.240 --> 00:00:05.190
Hi everyone and welcome back in this video we're going to be examining the code.

2
00:00:05.230 --> 00:00:11.020
And besides running and visualizing the chap by intensive board coming up I wanted to take the time

3
00:00:11.020 --> 00:00:16.930
with this code to discuss a few key elements more of it intuition approach that we can examine the code

4
00:00:16.930 --> 00:00:18.040
some of the structure.

5
00:00:18.190 --> 00:00:23.770
And besides working in an updated version of tensor flow and some changes to the architecture I wanted

6
00:00:23.770 --> 00:00:25.810
to point out three main points.

7
00:00:25.930 --> 00:00:30.510
So we're going to go ahead and discuss these to help give you some ideas on improving your chat bots.

8
00:00:30.580 --> 00:00:35.590
If you want to load the files into tensor flow just remember to open into your working directory.

9
00:00:35.740 --> 00:00:38.720
You can load it the file and the folder here.

10
00:00:38.860 --> 00:00:39.860
And we're working.

11
00:00:39.950 --> 00:00:43.030
We're going to be examining the vocabulary PI file and that end.

12
00:00:43.030 --> 00:00:48.250
The chap honors were modeled on PI file and remember if you are going to launch a chap or make any changes

13
00:00:48.250 --> 00:00:55.690
to run the files and you have installed the tens or full version the Jason Pickel version and are working

14
00:00:55.690 --> 00:00:58.330
with the chap on environment originally created.

15
00:00:58.330 --> 00:01:04.750
Remember when you launch your a kind of prompt or your terminal to activate the chat by environment.

16
00:01:04.840 --> 00:01:13.180
Mac and Linux its source activate the environment name and on Windows is activity with the environment

17
00:01:13.180 --> 00:01:16.910
name and then you could run the commands that you want to use that specific environment.

18
00:01:16.910 --> 00:01:17.290
All right.

19
00:01:17.440 --> 00:01:18.830
So jumping into the code.

20
00:01:19.000 --> 00:01:20.620
Let's take a look in the vocabulary.

21
00:01:20.630 --> 00:01:22.000
Py file.

22
00:01:22.000 --> 00:01:26.770
If you explore the entire file it has some really useful information that has some similarities to our

23
00:01:26.770 --> 00:01:30.610
original code that we can see some data preprocessing and cleansing.

24
00:01:30.610 --> 00:01:39.640
We want to look at the compiled definition so let's scroll or let's search for compile.

25
00:01:40.220 --> 00:01:40.980
Here we go.

26
00:01:41.060 --> 00:01:49.910
Just took a couple returns to get to initially as some of you have noticed in the main Chaput in the

27
00:01:49.910 --> 00:01:55.250
course when the two dictionaries were initially created they were exactly the same because the previous

28
00:01:55.250 --> 00:01:59.560
answers of the previous questions become the next questions of the next answers.

29
00:01:59.570 --> 00:02:04.040
It's very normal which happens to have these dictionaries at the beginning but it's recommended to set

30
00:02:04.040 --> 00:02:09.210
up two different dictionaries if you want to apply different thresholds to filter out non frequent words.

31
00:02:10.230 --> 00:02:12.720
We can see with this code and arguments with compile.

32
00:02:12.960 --> 00:02:18.540
But the main takeaway is the threshold or the minimum number of times the word must appear in sequences

33
00:02:18.870 --> 00:02:21.510
in order to be included in the vocabulary.

34
00:02:21.510 --> 00:02:25.950
This is useful for filtering out rarely used words in order to reduce the size of the vocabulary and

35
00:02:25.950 --> 00:02:30.440
reduces the size of the embedding matrices and dimensionality of the outputs off Max.

36
00:02:30.540 --> 00:02:33.510
It's something that needs to be thought about when dealing with your data.

37
00:02:33.510 --> 00:02:37.760
Because think about how non frequent words are highly frequent words play a role in the chapter.

38
00:02:37.890 --> 00:02:42.990
So it's a great method to try whose results you can follow along with some of the arguments and notes

39
00:02:42.990 --> 00:02:44.360
that Abraham added in here.

40
00:02:44.370 --> 00:02:51.060
Great little tips and tricks to understand what is going on with each line of code and overall just

41
00:02:51.060 --> 00:02:54.600
the use of the threshold and how it can help improve our results with the chap.

42
00:02:54.600 --> 00:03:01.830
Now if we jump back into the chat bot model PI file we can also see the following.

43
00:03:01.830 --> 00:03:03.650
I'm going to look for real quick so we could pull it up.

44
00:03:03.660 --> 00:03:13.220
The Define build encoder and we go want to just exit at this

45
00:03:16.470 --> 00:03:24.240
and we have the setup or part of the architecture referring to the bidirectional encoder now bidirectional

46
00:03:24.390 --> 00:03:30.240
encoding designees one or more art and cells to read the sequence forward in one or more Arnon cells

47
00:03:30.240 --> 00:03:32.190
to read the sequence backward.

48
00:03:32.190 --> 00:03:36.420
The resulting states are concatenated before sending them onto the decoder.

49
00:03:36.420 --> 00:03:40.230
And the purpose and the main takeaway of this is that we're using the cell forward and cell backward

50
00:03:40.430 --> 00:03:45.120
to cells to handle the forward and backward direction where in the original code in the course the same

51
00:03:45.120 --> 00:03:49.950
cell was used for both the bidirectional encoder is in an alternative that would eliminate the need

52
00:03:50.220 --> 00:03:55.140
for input reversal make the order of the padding irrelevant and remove the need for bucketing.

53
00:03:55.140 --> 00:04:01.410
And we can actually see here the encoder cells the self-ordained the cell backward being created with

54
00:04:01.410 --> 00:04:07.920
encoder cell forward with self-doubt and cell and the self-model hyper pro-ams aren't incise being used.

55
00:04:07.920 --> 00:04:13.950
The number of byte layers and the key prob. and the same thing for the encoder cell backward to separate

56
00:04:13.950 --> 00:04:14.370
cells.

57
00:04:14.370 --> 00:04:17.070
Remember in a directional and quarter within the architecture.

58
00:04:17.520 --> 00:04:21.000
And again thanks to Abraham for adding those notes on that.

59
00:04:21.000 --> 00:04:30.600
And we can also move on to the following of the Predict batch because what the Define of predict batch

60
00:04:30.600 --> 00:04:37.790
here and there are references earlier but I do like the notes on this argument as well.

61
00:04:39.430 --> 00:04:44.530
And one of the key takeaways here and I hope that you explore implementing this in your chat bots in

62
00:04:44.530 --> 00:04:45.090
the future.

63
00:04:45.130 --> 00:04:47.310
Is the implementation of Bheem search.

64
00:04:47.410 --> 00:04:53.020
It's an excellent idea to use in the main course we can see that sometimes when training or having a

65
00:04:53.020 --> 00:04:57.110
conversation with the chat but it would deviate a bit with the output.

66
00:04:57.130 --> 00:05:02.620
So enter beam's search in our main model and it's a little tough to show since the architecture and

67
00:05:02.620 --> 00:05:07.480
updated models deviate quite a bit from the original architecture was using what some would refer to

68
00:05:07.510 --> 00:05:12.970
as a vanilla sect to sect model that relies on the encoder decoder paradigm.

69
00:05:12.970 --> 00:05:18.230
We have the encoder handling the input sequences with the decoder and the target sequences.

70
00:05:18.240 --> 00:05:20.160
This should be familiar from the Course.

71
00:05:20.500 --> 00:05:25.210
This brings us to the use of greedy decoding as it is the most natural approach by feeding the next

72
00:05:25.210 --> 00:05:25.800
step.

73
00:05:25.870 --> 00:05:32.200
The most likely word predicted at the previous step with Bheem search instead of predicting the token

74
00:05:32.200 --> 00:05:38.410
with the best score we can keep track of the say K hypotheses and let's say K equals 5 and refer to

75
00:05:38.410 --> 00:05:41.290
k as the beam size at each new timestep.

76
00:05:41.290 --> 00:05:48.280
For these five hypotheses we have the new possible tokens and it would make a total of five hypotheses.

77
00:05:48.280 --> 00:05:51.440
Then you'd want to keep the five best and so.

78
00:05:51.810 --> 00:05:56.140
And to explore a little bit in the code we actually see the establishment of the beam length penalty

79
00:05:56.140 --> 00:05:56.580
weight.

80
00:05:56.740 --> 00:06:01.780
So when we are using Bheem search with this architecture and the decoding this penalty weight influences

81
00:06:01.780 --> 00:06:07.180
how beams are ranked large negative values rank very short beam's first while large positive values

82
00:06:07.180 --> 00:06:13.270
rank very long beam's first a value of zero will not influence the ranking for the Chaput model positive

83
00:06:13.270 --> 00:06:18.580
values between 0 and 2 can be beneficial to help the avoid short generic answers.

84
00:06:18.580 --> 00:06:24.700
If you want to examine some notes on Bheem search within tensor flow I advise you to take a look at

85
00:06:24.700 --> 00:06:26.380
the tensor flow documentation.

86
00:06:26.560 --> 00:06:29.410
You can see the arguments used and the returns to pass in.

87
00:06:29.490 --> 00:06:35.060
It's always helpful to pull that up and also if we jump back into the repository.

88
00:06:36.840 --> 00:06:38.840
We can scroll down and we see here.

89
00:06:38.870 --> 00:06:41.800
I mean enhances a bit so we could read it a little more clearly.

90
00:06:42.030 --> 00:06:45.830
We have the options if we throw the flag of the length penalty.

91
00:06:45.870 --> 00:06:50.070
You could actually establish the mean length penalty when you're chatting with the model on the train

92
00:06:50.070 --> 00:06:50.670
weights.

93
00:06:50.730 --> 00:06:56.050
Or if you train it further and you can set the beam length penalty weight to the number that you want.

94
00:06:56.050 --> 00:07:02.520
I highly advise you to experiment with it see if you can notice any obvious differences when establishing

95
00:07:02.520 --> 00:07:07.890
the beam length penalty and think about how that would be applied and how it can affect the data and

96
00:07:07.920 --> 00:07:13.020
on a last note I want to also mention if we scroll down through the code and Abraham did such an awesome

97
00:07:13.020 --> 00:07:19.140
job writing this code we can see that we have this if an else statement we have.

98
00:07:19.140 --> 00:07:23.910
If the beam is enabled or if it's greater than zero we have our beam for being searched the coding here.

99
00:07:24.060 --> 00:07:28.640
Otherwise we have greedy in our sampling decoding remember we talked about earlier the vanilla sect

100
00:07:28.640 --> 00:07:31.020
who Seck model the implementation of being search.

101
00:07:31.020 --> 00:07:35.220
So it's just a nother great addition of the code related to search.

102
00:07:35.520 --> 00:07:38.580
Those are the three main points that I wanted to touch on.

103
00:07:38.580 --> 00:07:40.310
I hope that you find this information useful.

104
00:07:40.320 --> 00:07:45.150
And there is quite a difference in the architecture so if you have any questions please feel free to

105
00:07:45.150 --> 00:07:46.510
share them in the Q&amp;A.

106
00:07:46.590 --> 00:07:51.840
With that we are going to move on and in the next video we were going to use tensor board as a visualization

107
00:07:51.840 --> 00:07:56.020
tool on the data and then we'll get into running and chatting with the Chapa.

108
00:07:56.340 --> 00:07:58.230
All right I will see you next video.
