1
00:00:00,480 --> 00:00:05,880
Hello welcome back to the course on deep natural language processing in today's tutorial we're going

2
00:00:05,880 --> 00:00:10,770
to look at how the sequence to sequence and models are trained.

3
00:00:10,860 --> 00:00:12,520
We were talking about the architecture.

4
00:00:12,640 --> 00:00:17,210
And so this is what we've come up with that this is our picture.

5
00:00:17,330 --> 00:00:26,210
What an encoder decoder which reasonable to record neural networks our inputs are fed in.

6
00:00:26,220 --> 00:00:37,210
And then once we have our inputs here then what happens is the decoder starts decoding the values and

7
00:00:37,210 --> 00:00:41,830
the city values back into the decoder as their video.

8
00:00:41,880 --> 00:00:50,640
Now the difference between training and actually apply is that in this case this is the end result.

9
00:00:50,640 --> 00:00:55,770
This is what we want that after our network has been trained once we feed in the sense that is Herrell.

10
00:00:55,800 --> 00:01:01,470
Hello Carol chikin if you're back in Oz once we get in the sentence then we get this as an output.

11
00:01:01,470 --> 00:01:03,250
Yes I'm back end of sentence.

12
00:01:03,420 --> 00:01:06,350
Now we're going to this is the end result.

13
00:01:06,360 --> 00:01:07,890
Now we're going to look at training.

14
00:01:07,900 --> 00:01:12,210
So for training we're going to look at a different set this is going to take one of the sentences from

15
00:01:12,210 --> 00:01:14,830
a different like will take a different email.

16
00:01:15,030 --> 00:01:21,470
And in this case we will already have the full response that was provided in that case.

17
00:01:21,480 --> 00:01:22,640
So this is how it works.

18
00:01:22,850 --> 00:01:28,680
The email was like the recipe I sent you last week and the actual response that for instance this case

19
00:01:28,710 --> 00:01:31,080
I sent like this is hypothetical.

20
00:01:31,080 --> 00:01:37,280
It's never happened but if I had said the response would have been yes it was great and of sense hypothetically.

21
00:01:37,350 --> 00:01:40,470
So if it was good if it was a good rest.

22
00:01:40,590 --> 00:01:42,930
So here we've got.

23
00:01:42,930 --> 00:01:44,740
Did you like that.

24
00:01:44,920 --> 00:01:45,810
And so on and so on.

25
00:01:45,810 --> 00:01:49,880
So that recipe that I said you I said last week end of sentence.

26
00:01:49,930 --> 00:01:55,840
So that's our inputs to the model that's out in.

27
00:01:55,870 --> 00:01:58,500
And then for the decoder we already know that.

28
00:01:58,490 --> 00:02:02,310
So the difference why I highlighted this in red is because we didn't know this.

29
00:02:02,350 --> 00:02:10,270
And the model generated in the training part we actually know the end result and now that will allow

30
00:02:10,270 --> 00:02:11,050
us to train the model.

31
00:02:11,060 --> 00:02:12,600
That's what we're going to look at now.

32
00:02:12,870 --> 00:02:16,380
So here you can see that there is a lot more letters a lot more happening.

33
00:02:16,690 --> 00:02:25,910
So here we've got use We've got double use We've got you twos double twos and these and so in different

34
00:02:25,980 --> 00:02:29,880
literature you might find different notations which is going to stick with this one.

35
00:02:29,940 --> 00:02:40,110
And basically those represent the parameters of our network such as weights such as the weights of the

36
00:02:42,090 --> 00:02:45,420
neural network that's we need two of.

37
00:02:45,420 --> 00:02:55,500
So basically we need to update these parameters through our our training in order for the network to

38
00:02:56,010 --> 00:03:00,440
learn from the training data from the training and how like what does that imply.

39
00:03:00,440 --> 00:03:02,100
That's the main question we're going to answer.

40
00:03:02,100 --> 00:03:07,650
So we're not going to go into the detail of the mathematics or on what weights are where what these

41
00:03:07,650 --> 00:03:13,140
parameters mean and so the main key is we want to get the intuition behind those who want to listen.

42
00:03:13,140 --> 00:03:15,440
What does that exactly mean.

43
00:03:16,170 --> 00:03:19,270
And so there are two main points that we're going to look at.

44
00:03:19,970 --> 00:03:26,540
First one the main one is one that question is like what does it mean that the network is going to be

45
00:03:26,540 --> 00:03:26,870
lower.

46
00:03:26,990 --> 00:03:28,630
Well let's look at it.

47
00:03:28,640 --> 00:03:37,830
So once we get in this sentence or this e-mail says Did you like that recipe that I said last week end

48
00:03:37,830 --> 00:03:38,830
of sentence.

49
00:03:39,020 --> 00:03:46,370
Then the network through training was going to happen in an iterative process going to look at the different

50
00:03:46,730 --> 00:03:48,230
options that you can spit out.

51
00:03:48,230 --> 00:03:54,490
So for instance you will start learning OK what work can I we know the correct board is the correct

52
00:03:54,500 --> 00:03:56,360
pursers word it is yes.

53
00:03:56,360 --> 00:04:02,760
But the network actually has 20000 different options to pick from 20000 different words.

54
00:04:03,020 --> 00:04:13,490
And so through the process of back propagation this network this sequence to sequence model is going

55
00:04:13,490 --> 00:04:18,740
to make sure that the probability of getting the word yes is the highest.

56
00:04:18,740 --> 00:04:25,160
So it's going to update the weights of the parameters of the network in such a way that when the word

57
00:04:25,160 --> 00:04:30,210
yes comes up it will have the highest probability so that indeed it is yes.

58
00:04:30,580 --> 00:04:40,000
Then what what happens next is it moves on to the next where this part of the network is considered

59
00:04:41,050 --> 00:04:47,220
done and then now given that the output here is yes.

60
00:04:47,530 --> 00:04:56,860
And this information is passed on our way here in this information is passed on between this segment

61
00:04:56,860 --> 00:05:05,040
of the Edric and this segment and we're feeding back the value yes into our layer of neurons.

62
00:05:05,080 --> 00:05:07,790
Now we want the output to be it.

63
00:05:07,970 --> 00:05:12,070
This we know that this is the final output that has to be or place number two.

64
00:05:12,100 --> 00:05:21,880
So the network given that this is the input and given that the first word is yes is going to aim to

65
00:05:21,880 --> 00:05:30,490
maximize the chances of or the probability that it or the like is going to maximize the probability

66
00:05:30,490 --> 00:05:34,190
all the word it for some or two.

67
00:05:34,330 --> 00:05:36,880
So it's going to update itself.

68
00:05:36,880 --> 00:05:42,320
It's going to like through the process of stochastic radiation and back propagation.

69
00:05:42,450 --> 00:05:43,220
It's like.

70
00:05:43,280 --> 00:05:52,510
This this error is going to be back later and it is going to update itself in order to maximize the

71
00:05:53,230 --> 00:05:58,590
score that is assigned to the word it in this case out of 20000 words.

72
00:05:59,110 --> 00:06:04,590
Once that is done you move on and will do the same thing for the third place.

73
00:06:04,600 --> 00:06:10,070
Given that we have that input the first word is yes the second one is it it will maximize what you want

74
00:06:10,070 --> 00:06:10,800
about this for now.

75
00:06:10,800 --> 00:06:18,140
Ill maximize the score that is assigned to the word was.

76
00:06:18,190 --> 00:06:19,990
So again backwardation and so on.

77
00:06:20,180 --> 00:06:29,170
And this is we just look at one example as you know in neural networks there there's going to be like

78
00:06:29,170 --> 00:06:32,870
tons of training data tons of data is going through there.

79
00:06:33,010 --> 00:06:38,800
It's like going through every one of them is multiple abox many many many airports it's all happening

80
00:06:39,250 --> 00:06:45,880
like and they're very true to Prose's and the network is adjusting itself to match the match all the

81
00:06:47,500 --> 00:06:53,940
inputs and outputs and so therefore it looks very simple but in reality these are all layers of neurons

82
00:06:54,310 --> 00:06:57,350
and it can actually have more even more levels.

83
00:06:57,350 --> 00:07:00,170
So it could be a deeper network as we saw before.

84
00:07:00,580 --> 00:07:03,260
And so is because it's not that simple.

85
00:07:03,280 --> 00:07:10,270
There is room for it to adjust its weights to accommodate all different options of input e-mails and

86
00:07:10,300 --> 00:07:11,110
outputs.

87
00:07:11,900 --> 00:07:14,170
Answers that were sent.

88
00:07:14,430 --> 00:07:25,300
So the goal for the network is to make sure that it can model the logic and behavior that was observed

89
00:07:25,360 --> 00:07:33,640
in the emails so that like it looks at an email it understands the meaning behind it and then it creates

90
00:07:33,640 --> 00:07:37,420
a response and that should be in line with all the training it has.

91
00:07:37,420 --> 00:07:42,070
So that's how it goes like one by one because that and the other thing.

92
00:07:42,070 --> 00:07:47,910
The second important thing that we need to observe here is that the Wace So a natural question that

93
00:07:47,920 --> 00:07:48,770
you might have is.

94
00:07:48,970 --> 00:07:49,270
OK.

95
00:07:49,270 --> 00:07:59,620
So we've trained up this network for all that say in this case we have a question of 1 2 3 4 5 6 7 8

96
00:07:59,680 --> 00:08:11,980
9 10 11 words including the question mark plus us 12 and 12 here and then 1 2 3 4 5 on Outwood.

97
00:08:12,220 --> 00:08:17,590
And so this is kind of like the The question this question is related to the fact that we have variable

98
00:08:17,590 --> 00:08:19,520
input and variable outputs.

99
00:08:19,740 --> 00:08:20,590
What ear.

100
00:08:20,680 --> 00:08:28,330
So this looks like a very specific natural 12:5 What if we had 20 and you know what if we like what

101
00:08:28,330 --> 00:08:36,810
if we have 20 here and then were and the response that the net generates is going to be like three words.

102
00:08:36,820 --> 00:08:38,410
So how is it.

103
00:08:38,590 --> 00:08:39,960
It's not ready for that right.

104
00:08:39,970 --> 00:08:43,920
It feels like it's only trained up to work of 12 and 5.

105
00:08:44,030 --> 00:08:46,060
What how can it work with variable.

106
00:08:46,060 --> 00:08:53,590
Like how can a candidate even explore options like a can explore emails that have a different slant

107
00:08:53,650 --> 00:08:59,500
to anything seen before and responses that have might have a different than anything seen before.

108
00:08:59,590 --> 00:09:00,740
The answer is yes.

109
00:09:01,000 --> 00:09:10,690
And the reason for that is as you can see the weights they are actually the same as you go through the

110
00:09:10,690 --> 00:09:13,150
network as you go through.

111
00:09:13,370 --> 00:09:20,800
Like if you add on more parts to the network you add on another like we have more than 12 here if you

112
00:09:20,800 --> 00:09:27,250
add on or remove what will happen is just like it replicates itself every element here has the same

113
00:09:27,250 --> 00:09:32,250
parameters you want everyone here to to an output you have these.

114
00:09:32,380 --> 00:09:37,750
So basically every time like even if you add more more like say you didn't come up end of sentence here

115
00:09:37,920 --> 00:09:38,860
it just kept going.

116
00:09:38,950 --> 00:09:43,990
You'd add more and more and more so that basically means like whatever is trained during the training

117
00:09:43,990 --> 00:09:51,730
process you will have all these parameters and then you'll have one w want you to delve into the.

118
00:09:51,790 --> 00:09:57,690
And then basically that is going to be those that are going to be the foundations of this sequence in

119
00:09:57,690 --> 00:10:03,740
sequence model and regardless of how big it is likes bigger or small is just going to be recreated.

120
00:10:03,850 --> 00:10:10,120
We would have a problem if every time you add a new segment you know like it has its own weights it

121
00:10:10,120 --> 00:10:13,240
has its own parameters.

122
00:10:13,360 --> 00:10:16,450
But in this case because it's replicated we don't care about that.

123
00:10:16,450 --> 00:10:25,430
That means that these these weights are going to be trained through the normal process of backwardation.

124
00:10:25,480 --> 00:10:26,690
That's a great design.

125
00:10:26,890 --> 00:10:29,430
And then regardless of how long.

126
00:10:29,650 --> 00:10:35,770
So however long they are we're training the same thing however long the output is going to be using

127
00:10:35,890 --> 00:10:39,070
those values or those weights that we trained up.

128
00:10:39,100 --> 00:10:43,370
So there we go that's how the sequence the sequence network is trained.

129
00:10:43,390 --> 00:10:50,960
Very similar to any sort of back propagation just the standard back progression and stochastic gray

130
00:10:51,010 --> 00:10:59,860
in the sense that we know about and though the only concepts new concepts are that you know with the

131
00:10:59,870 --> 00:11:08,510
variable and variable inputs and outputs and that is already taken into account so all we had to have

132
00:11:08,510 --> 00:11:16,430
a look at is how does it actually take into consideration the different responses that we have to the

133
00:11:16,430 --> 00:11:17,910
DUPERE emails.

134
00:11:17,960 --> 00:11:21,770
So I hope you enjoy this tural and I'll see you next time.

135
00:11:21,890 --> 00:11:25,210
Until then enjoy the natural language processing.
