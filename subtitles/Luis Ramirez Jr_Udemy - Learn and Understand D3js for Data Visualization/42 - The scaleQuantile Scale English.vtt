WEBVTT
﻿1
00:00:00.300 --> 00:00:05.730
‫In this lecture, we are going to learn about the quantum scale, it's another scale that can perform

2
00:00:05.730 --> 00:00:11.670
‫a similar task to the quantize scale, we are going to use it for mapping our data to colors.

3
00:00:12.240 --> 00:00:15.870
‫The quantum scale will accept a continuous input domain.

4
00:00:16.170 --> 00:00:22.720
‫It'll transform the data into discrete data since we're transforming the data into discrete data.

5
00:00:22.740 --> 00:00:25.880
‫We need to provide a list of possible values.

6
00:00:26.340 --> 00:00:29.340
‫We are going to be using three colors like last time.

7
00:00:29.640 --> 00:00:31.920
‫They will be white, pink and red.

8
00:00:32.280 --> 00:00:37.040
‫Overall, it seems like the quantum scale is similar to the quantize scale.

9
00:00:37.350 --> 00:00:41.660
‫The difference between the two scales is how data is divided in two buckets.

10
00:00:42.180 --> 00:00:45.890
‫The input domain for a quantum scale is different than before.

11
00:00:46.200 --> 00:00:50.050
‫We need to tell the scale how many items are in the data set array.

12
00:00:50.400 --> 00:00:53.450
‫In our case, we have one hundred incomes.

13
00:00:53.700 --> 00:00:57.600
‫Therefore, the input domain for the scale will be one hundred.

14
00:00:58.320 --> 00:01:02.390
‫After receiving the domain, it needs a list of possible values.

15
00:01:02.550 --> 00:01:05.550
‫We are transforming the data into three colors.

16
00:01:05.790 --> 00:01:08.310
‫Therefore we will have three buckets.

17
00:01:08.820 --> 00:01:13.410
‫The quantize scale will evenly distribute the numbers across all buckets.

18
00:01:13.680 --> 00:01:18.630
‫It'll divide the number of items in the domain by the number of items in the output range.

19
00:01:18.810 --> 00:01:21.690
‫So basically one hundred divided by three.

20
00:01:21.990 --> 00:01:26.280
‫It'll determine that each bucket can represent thirty three values.

21
00:01:26.280 --> 00:01:30.870
‫Each one hundred divided by three will result in a decimal value.

22
00:01:31.170 --> 00:01:36.570
‫If the domain cannot be evenly divided, it'll put the remainder into the final bucket.

23
00:01:37.080 --> 00:01:42.810
‫For example, the first bucket for the color white will be used for the lowest thirty three numbers

24
00:01:42.810 --> 00:01:43.700
‫in the data set.

25
00:01:44.310 --> 00:01:49.590
‫The second bucket for the color pink will be used for the next thirty three numbers in the data set.

26
00:01:50.160 --> 00:01:56.070
‫Lastly, the third bucket for the color red will be used for the highest thirty four numbers in the

27
00:01:56.070 --> 00:01:56.700
‫data set.

28
00:01:57.420 --> 00:02:00.780
‫Let's look at another example for further clarification.

29
00:02:01.020 --> 00:02:05.790
‫In this example, we will pretend we have three hundred incomes in our data set.

30
00:02:06.450 --> 00:02:09.930
‫The lowest one hundred values will be placed in the first bucket.

31
00:02:10.290 --> 00:02:13.630
‫The next one hundred values will be placed in the second bucket.

32
00:02:13.980 --> 00:02:18.600
‫Lastly, the highest one hundred values will be placed in the third bucket.

33
00:02:18.990 --> 00:02:23.760
‫The quantize scale is perfect for evenly distributing values in two buckets.

34
00:02:25.140 --> 00:02:31.760
‫It's time to dig into some code, let's add another element in the HTML document for the new image,

35
00:02:31.770 --> 00:02:34.710
‫we will copy the H1 and div tags.

36
00:02:37.160 --> 00:02:41.420
‫The texturally H1 tag will be the following quanti scale.

37
00:02:43.940 --> 00:02:48.080
‫The idea of the div tag will be changed to heat map three.

38
00:02:50.600 --> 00:02:53.450
‫Next, let's update the script file.

39
00:02:55.930 --> 00:03:02.380
‫At the bottom of the script, we are going to call the draw function a third time, the ID of the element

40
00:03:02.380 --> 00:03:03.600
‫is heat map three.

41
00:03:03.940 --> 00:03:06.010
‫The name of the scale will be quantize.

42
00:03:08.680 --> 00:03:14.880
‫The last thing we need to do is update the draw function with the quintile scale inside the function

43
00:03:14.890 --> 00:03:21.040
‫search for where we created these conditional blocks, we will add a new conditional block with the

44
00:03:21.050 --> 00:03:22.210
‫live statement.

45
00:03:24.680 --> 00:03:29.060
‫The condition will check if the skill argument is equal to quintile.

46
00:03:31.540 --> 00:03:38.050
‫If it is, we will assign the color scale variable to the D 3.0 scale quanti or function.

47
00:03:40.590 --> 00:03:43.890
‫We need to change the domain function to add the domain.

48
00:03:46.520 --> 00:03:49.970
‫The value for the domain is going to be different than before.

49
00:03:50.180 --> 00:03:55.700
‫Instead of providing the lowest and highest values in the array, we're going to pass him the entire

50
00:03:55.700 --> 00:03:56.360
‫data set.

51
00:03:59.000 --> 00:04:05.030
‫The quanti scale requires the data set, it needs to know how to distribute the data evenly.

52
00:04:05.450 --> 00:04:10.130
‫This means it needs to know how many items are in the array and the range of data.

53
00:04:10.790 --> 00:04:13.740
‫The next thing we will set is the output range.

54
00:04:13.760 --> 00:04:15.800
‫We will change the range function.

55
00:04:18.290 --> 00:04:25.070
‫The range function will need an array of colors to transform the data to we will pass in white, pink

56
00:04:25.070 --> 00:04:25.700
‫and red.

57
00:04:28.870 --> 00:04:29.990
‫Our skill is ready.

58
00:04:30.130 --> 00:04:34.000
‫It's time to test the shark, refresh the page in the browser.

59
00:04:36.530 --> 00:04:42.740
‫The chart is functioning as expected, there's an even distribution of colors in the grid, we are being

60
00:04:42.740 --> 00:04:44.030
‫told a different story.

61
00:04:44.300 --> 00:04:49.100
‫The quantize scale is perfect for transforming data evenly into buckets.

62
00:04:49.310 --> 00:04:55.160
‫In the next lecture, we will explore one more scale before discussing which is better for our data.

