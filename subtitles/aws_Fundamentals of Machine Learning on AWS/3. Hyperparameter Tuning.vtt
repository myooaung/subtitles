WEBVTT
1
00:00:01.540 --> 00:00:03.160
[Autogenerated] the process of tuning the model

2
00:00:03.160 --> 00:00:05.840
involves what are called hyper parameters,

3
00:00:05.840 --> 00:00:08.460
and you can think of these as sort of the knobs that you can

4
00:00:08.460 --> 00:00:11.240
use to control the behavior of an algorithm.

5
00:00:11.240 --> 00:00:12.950
Out here in the Sage Maker Developer guide,

6
00:00:12.950 --> 00:00:16.940
I'm under the Xcode Boost algorithm hyper parameters,

7
00:00:16.940 --> 00:00:19.990
and here on the page you'll see the hyper parameter names,

8
00:00:19.990 --> 00:00:25.940
a short description of what they do and whether they're required or not.

9
00:00:25.940 --> 00:00:27.210
You'll see there's a lot here.

10
00:00:27.210 --> 00:00:29.640
Luckily, a lot of them are optional.

11
00:00:29.640 --> 00:00:31.720
You'll also see these when you go to create a new

12
00:00:31.720 --> 00:00:33.940
training job through the console.

13
00:00:33.940 --> 00:00:37.040
So if I come over here into the sage maker dashboard

14
00:00:37.040 --> 00:00:40.230
and create a new training job, I'll just scroll down.

15
00:00:40.230 --> 00:00:43.050
Until we get to that section here,

16
00:00:43.050 --> 00:00:49.140
I will choose these algorithm of Xcode boost and

17
00:00:49.140 --> 00:00:50.840
depending on the algorithm you choose, Obviously,

18
00:00:50.840 --> 00:00:52.760
the hyper parameters will change.

19
00:00:52.760 --> 00:00:57.940
But if we scroll all the way down here to hyper parameters,

20
00:00:57.940 --> 00:01:01.740
you can see there displayed here, and you can enter values for them.

21
00:01:01.740 --> 00:01:04.740
So that's when you set up the job originally.

22
00:01:04.740 --> 00:01:07.150
But tuning hyper parameters is slightly different.

23
00:01:07.150 --> 00:01:10.530
If I go back to the boost documentation here.

24
00:01:10.530 --> 00:01:14.140
I actually want to come down to model tuning,

25
00:01:14.140 --> 00:01:15.910
and there are basically three parts to this.

26
00:01:15.910 --> 00:01:18.100
You want to choose your objective metric,

27
00:01:18.100 --> 00:01:22.660
which is the thing you're trying to optimize for this particular algorithm.

28
00:01:22.660 --> 00:01:26.840
If we scroll down here, you'll see the options.

29
00:01:26.840 --> 00:01:29.100
Maybe you want to optimize for accuracy,

30
00:01:29.100 --> 00:01:31.340
and over here on the right, you'll see that to optimize.

31
00:01:31.340 --> 00:01:33.240
You should be maximizing this.

32
00:01:33.240 --> 00:01:34.150
That's your objective.

33
00:01:34.150 --> 00:01:41.120
Metric and there are others as well, and then you want to look atyour tunable.

34
00:01:41.120 --> 00:01:47.440
Hyper parameters Down here, you can see the full list put up on the top.

35
00:01:47.440 --> 00:01:51.570
There's a note about the five that have the greatest impact now.

36
00:01:51.570 --> 00:01:54.620
You probably have no idea what the value should be for each of these,

37
00:01:54.620 --> 00:01:56.660
but what you can do is specify a range.

38
00:01:56.660 --> 00:01:59.240
You'll see the minimum and maximum values.

39
00:01:59.240 --> 00:02:02.270
And then sage maker will run a bunch of training jobs using those

40
00:02:02.270 --> 00:02:06.350
ranges of values until it finds the right combination that optimizes

41
00:02:06.350 --> 00:02:09.140
these Objective-C metric that we chose.

42
00:02:09.140 --> 00:02:11.860
When those jobs are done in the sage maker console,

43
00:02:11.860 --> 00:02:13.920
you'll see the best training job and the hyper

44
00:02:13.920 --> 00:02:17.060
parameters that it landed on and Of course,

45
00:02:17.060 --> 00:02:18.630
you could do all of this through code as well,

46
00:02:18.630 --> 00:02:22.440
but I think it's a little bit easier to see through the console.

47
00:02:22.440 --> 00:02:25.610
You remember this slide from earlier, but just to call it out again,

48
00:02:25.610 --> 00:02:28.470
the data that you set aside for validation is what you want to

49
00:02:28.470 --> 00:02:31.640
use during this hyper parameter tuning step.

50
00:02:31.640 --> 00:02:32.460
So to recap,

51
00:02:32.460 --> 00:02:35.620
the main points of hyper parameter tuning first you want to choose your

52
00:02:35.620 --> 00:02:38.840
objective metric or the thing you're trying to optimize.

53
00:02:38.840 --> 00:02:42.340
Then you'll define the ranges for your various hyper parameters.

54
00:02:42.340 --> 00:02:44.620
And finally, sage maker will run the training jobs,

55
00:02:44.620 --> 00:02:47.140
using the ranges you specified until it finds the right

56
00:02:47.140 --> 00:02:51.760
combination of values that optimize that objective metric now is

57
00:02:51.760 --> 00:02:53.570
you're going through and evaluating the model.

58
00:02:53.570 --> 00:02:56.440
How do you know how well it's performing?

59
00:02:56.440 --> 00:03:01.060
For example, here we have the true values sent in by the dataset,

60
00:03:01.060 --> 00:03:04.670
and we see that the model got some of them right in green.

61
00:03:04.670 --> 00:03:08.440
But some of them are wrong, the ones in gray.

62
00:03:08.440 --> 00:03:11.470
The confusion matrix is a great way to visualize this.

63
00:03:11.470 --> 00:03:15.470
Let's walk through an example using fraud detection over on the left.

64
00:03:15.470 --> 00:03:17.840
We have the actual values that we passed in on our

65
00:03:17.840 --> 00:03:21.010
dataset of fraud or not fraud on top.

66
00:03:21.010 --> 00:03:25.320
We have, with the model predicted again, fraud, positive or not, Fraud.

67
00:03:25.320 --> 00:03:26.440
Negative.

68
00:03:26.440 --> 00:03:29.140
And then the results are in the middle.

69
00:03:29.140 --> 00:03:31.910
You definitely need to understand these concepts and even

70
00:03:31.910 --> 00:03:34.640
fill one of these matrices out for the exam.

71
00:03:34.640 --> 00:03:38.090
You'll need to know your true positives, your falsy negatives and so forth.

72
00:03:38.090 --> 00:03:40.740
So let me show you how I do it.

73
00:03:40.740 --> 00:03:44.230
I take what the prediction did either positive or negative,

74
00:03:44.230 --> 00:03:47.740
and then I moved those down into the middle of the Matrix.

75
00:03:47.740 --> 00:03:52.540
So for positive predictive value, I move those down below positive and positive.

76
00:03:52.540 --> 00:03:56.140
And then I do the same thing for negative where we predicted negative.

77
00:03:56.140 --> 00:03:59.240
I move those down into the middle of the matrix.

78
00:03:59.240 --> 00:04:01.260
Now, for each of the four squares in the middle,

79
00:04:01.260 --> 00:04:03.910
we need to figure out if the prediction was correct.

80
00:04:03.910 --> 00:04:08.270
True or not, falsy starting with this one in the middle,

81
00:04:08.270 --> 00:04:13.350
we predicted positive for fraud and the actual value was positive as well.

82
00:04:13.350 --> 00:04:16.560
So a prediction was true or correct.

83
00:04:16.560 --> 00:04:19.240
This is a true positive.

84
00:04:19.240 --> 00:04:20.610
Next over to the right,

85
00:04:20.610 --> 00:04:26.450
we predicted negative meaning not fraud and the actual value was positive,

86
00:04:26.450 --> 00:04:29.340
which means we were wrong or falsy in our prediction.

87
00:04:29.340 --> 00:04:33.180
So this was a false negative to get some practice with this

88
00:04:33.180 --> 00:04:35.280
and encourage you to walk through similar steps in the

89
00:04:35.280 --> 00:04:37.540
lower two boxes of the Matrix.

90
00:04:37.540 --> 00:04:38.800
Once you feel the matrix out,

91
00:04:38.800 --> 00:04:42.640
you can start calculating some metrics to see how the model is doing.

92
00:04:42.640 --> 00:04:46.740
The first is accuracy, and you'll see the formula here.

93
00:04:46.740 --> 00:04:50.320
Accuracy measures these percentage of predictions that were correct so true

94
00:04:50.320 --> 00:04:54.640
positives and true negatives divided by all predictions.

95
00:04:54.640 --> 00:04:57.520
And while having an accurate model sounds great,

96
00:04:57.520 --> 00:05:00.010
this actually is not very helpful in cases where

97
00:05:00.010 --> 00:05:02.780
you have a lot of true negatives, such as fraud,

98
00:05:02.780 --> 00:05:04.420
where you correctly predicted,

99
00:05:04.420 --> 00:05:08.040
not fraud a lot because there's just not a lot of fraud data.

100
00:05:08.040 --> 00:05:11.030
You don't know for sure that when you do see fraud that

101
00:05:11.030 --> 00:05:13.940
it's accurately going to pick that up.

102
00:05:13.940 --> 00:05:15.290
Next is precision.

103
00:05:15.290 --> 00:05:18.740
This is the percentage of positive predictions that were correct.

104
00:05:18.740 --> 00:05:22.740
This is a metric you want to use when the cost of false positives is high.

105
00:05:22.740 --> 00:05:27.590
For instance, an important legitimate email got flagged and deleted a spam,

106
00:05:27.590 --> 00:05:29.440
and it shouldn't have been.

107
00:05:29.440 --> 00:05:33.420
And finally we have recall this is the percentage of actual

108
00:05:33.420 --> 00:05:35.940
positives that were correctly identified.

109
00:05:35.940 --> 00:05:39.140
And you want to use this when the cost of falsy negatives are high.

110
00:05:39.140 --> 00:05:40.710
For example, you have cancer,

111
00:05:40.710 --> 00:05:45.130
but screening didn't find it ID recommend going through these and

112
00:05:45.130 --> 00:05:48.140
getting some practice with the concepts and formulas because they're

113
00:05:48.140 --> 00:05:54.000
all sort of similar but slightly nuanced, and you are definitely going to see them on the exam.

