WEBVTT
1
00:00:01.440 --> 00:00:02.800
[Autogenerated] Now that we've covered the concepts of

2
00:00:02.800 --> 00:00:08.340
fetching and preparing your data, let's see them in action in Sagemaker studio.

3
00:00:08.340 --> 00:00:11.320
At the beginning of this course, I showed you how to set up Sage Maker studio.

4
00:00:11.320 --> 00:00:15.970
So go ahead and navigate their in my file structure here on the left.

5
00:00:15.970 --> 00:00:17.870
I'm going to go under Amazon's sage maker.

6
00:00:17.870 --> 00:00:25.050
Examples to AWS Sage maker studio getting started and

7
00:00:25.050 --> 00:00:26.350
then you'll notice this Jupyter.

8
00:00:26.350 --> 00:00:29.740
Notebook here ending in studio.

9
00:00:29.740 --> 00:00:32.340
Let me click into that.

10
00:00:32.340 --> 00:00:34.300
If you prompted to start up the kernel at this point,

11
00:00:34.300 --> 00:00:37.480
go ahead and cancel out of that, we aren't actually gonna be using this one,

12
00:00:37.480 --> 00:00:39.440
but it will point us to the one that we need.

13
00:00:39.440 --> 00:00:43.410
Let me just rearrange my screen a little bit here, up here on top.

14
00:00:43.410 --> 00:00:44.970
It gives you a little bit of information about the

15
00:00:44.970 --> 00:00:47.120
different features and sage maker studio.

16
00:00:47.120 --> 00:00:48.870
Feel free to look through this on your own,

17
00:00:48.870 --> 00:00:54.300
but I'm going to scroll down past background and passed

18
00:00:54.300 --> 00:00:57.840
the first couple blocks of code here.

19
00:00:57.840 --> 00:01:00.580
You'll see that we're using a publicly available dataset.

20
00:01:00.580 --> 00:01:03.450
But this particular notebook that comes with the examples has

21
00:01:03.450 --> 00:01:06.220
actually done some of the pre processing work for us,

22
00:01:06.220 --> 00:01:08.040
and we want to do that ourselves.

23
00:01:08.040 --> 00:01:09.710
So to get to the notebook that starts with the

24
00:01:09.710 --> 00:01:13.040
original dataset click on this link here,

25
00:01:13.040 --> 00:01:15.140
this will open up GitHub.

26
00:01:15.140 --> 00:01:16.870
And if you scroll down just a little bit,

27
00:01:16.870 --> 00:01:21.650
go ahead and click on raw and then on the screen

28
00:01:21.650 --> 00:01:26.450
right-click do a save as we need to save.

29
00:01:26.450 --> 00:01:28.390
This is an IP y and be file.

30
00:01:28.390 --> 00:01:30.840
So put it in quotes.

31
00:01:30.840 --> 00:01:33.560
Otherwise it'll save it as a text file and then I'm going

32
00:01:33.560 --> 00:01:37.260
to append underscore pre processing.

33
00:01:37.260 --> 00:01:39.320
Just so I can distinguish it from that workbook that's

34
00:01:39.320 --> 00:01:43.440
already in sagemaker Studio Save.

35
00:01:43.440 --> 00:01:45.160
Now back in Sagemaker studio,

36
00:01:45.160 --> 00:01:48.230
we need to upload that workbook that we just downloaded and

37
00:01:48.230 --> 00:01:52.040
we can do that here with the arrow upload files,

38
00:01:52.040 --> 00:01:55.940
select the workbook and then open.

39
00:01:55.940 --> 00:01:58.650
I'll close out of this original notebook that we had open

40
00:01:58.650 --> 00:02:00.740
and over here on the left make sure that you're opening

41
00:02:00.740 --> 00:02:03.540
the one called pre Processing.

42
00:02:03.540 --> 00:02:07.720
This time we do want to select the kernel and we'll be using the

43
00:02:07.720 --> 00:02:13.540
python three data science kernel So like that.

44
00:02:13.540 --> 00:02:15.640
And now we can get started.

45
00:02:15.640 --> 00:02:19.730
We move this over a little bit close this out on the bottom here again,

46
00:02:19.730 --> 00:02:21.630
you can read through some of the background,

47
00:02:21.630 --> 00:02:24.790
but I'm going to skip down to this set up section here where we start

48
00:02:24.790 --> 00:02:28.440
getting into the code scrolling down a little bit more.

49
00:02:28.440 --> 00:02:31.450
You see, the first thing we define here is these S3 bucket.

50
00:02:31.450 --> 00:02:33.640
This is where we're going to store our data.

51
00:02:33.640 --> 00:02:35.900
We do need to manually go create that and then

52
00:02:35.900 --> 00:02:38.240
enter the name of the bucket here.

53
00:02:38.240 --> 00:02:40.900
To do that, I'll go into the AWS management console,

54
00:02:40.900 --> 00:02:44.340
which I have up here and under services.

55
00:02:44.340 --> 00:02:50.970
I'll type in S3 and then create bucket her name.

56
00:02:50.970 --> 00:02:56.650
I will do PS for Pluralsight AWS XML S3 bucket names

57
00:02:56.650 --> 00:02:58.160
do need to be globally unique.

58
00:02:58.160 --> 00:03:01.660
So I will go ahead and enter a date and my initials.

59
00:03:01.660 --> 00:03:04.940
You might need to try some different things here to get yours unique.

60
00:03:04.940 --> 00:03:07.720
And from here, I'm basically just going to accept all the defaults.

61
00:03:07.720 --> 00:03:14.220
Go next, next, next, and create the bucket that worked.

62
00:03:14.220 --> 00:03:21.040
So I'll click into this, grab the name and copy that to my clipboard.

63
00:03:21.040 --> 00:03:26.040
Now come back over to Sage Maker studio and fill that in right here,

64
00:03:26.040 --> 00:03:28.740
leaving the single quotes on either end.

65
00:03:28.740 --> 00:03:30.540
That's the only update you need to make here.

66
00:03:30.540 --> 00:03:32.760
We've also got some code that's going to get the

67
00:03:32.760 --> 00:03:34.670
execution role that we're running under,

68
00:03:34.670 --> 00:03:37.740
and that was specified when you set up sage maker studio.

69
00:03:37.740 --> 00:03:39.280
So we want to run this block of code.

70
00:03:39.280 --> 00:03:42.540
You could do that either with the run button up here on top,

71
00:03:42.540 --> 00:03:44.420
or you can also hit Shift Enter.

72
00:03:44.420 --> 00:03:45.980
Looks like that ran just fine.

73
00:03:45.980 --> 00:03:47.640
So let's scroll down.

74
00:03:47.640 --> 00:03:49.380
This next bit of code is going to import all the

75
00:03:49.380 --> 00:03:53.140
different Python libraries we need, so I'll click into here.

76
00:03:53.140 --> 00:03:58.280
Do shift enter to run the code and then scrolling down.

77
00:03:58.280 --> 00:03:59.940
There's a little bit more background here about the

78
00:03:59.940 --> 00:04:02.340
business problem and the data.

79
00:04:02.340 --> 00:04:05.940
I'll zoom through that and get to the next block of code.

80
00:04:05.940 --> 00:04:08.590
This next code is going out and getting the dataset,

81
00:04:08.590 --> 00:04:10.490
which is a publicly available dataset,

82
00:04:10.490 --> 00:04:13.940
and it's going to unzip it into our current working directory,

83
00:04:13.940 --> 00:04:15.530
so I'll go ahead and run this shift.

84
00:04:15.530 --> 00:04:17.360
Enter now.

85
00:04:17.360 --> 00:04:19.810
There is a known issue with the Unzip Command

86
00:04:19.810 --> 00:04:21.760
currently here in Sage Maker Studio.

87
00:04:21.760 --> 00:04:25.540
It's been reported to the team, but if you do see an are about unzip.

88
00:04:25.540 --> 00:04:27.440
There is a work around for it.

89
00:04:27.440 --> 00:04:34.450
So coming up here before you do unzip, you want to enter this code exclamation?

90
00:04:34.450 --> 00:04:40.340
Kanda, Install dash y dash c condo, dash, forge and unzip.

91
00:04:40.340 --> 00:04:42.450
I had loaded that up previously, so minds working.

92
00:04:42.450 --> 00:04:45.840
But you might need to run that at least until they get that bug fixed.

93
00:04:45.840 --> 00:04:48.600
If you had that are, insert this line of code,

94
00:04:48.600 --> 00:04:51.340
Run the block again and you should be good.

95
00:04:51.340 --> 00:04:52.840
Assuming everything works, you'll see.

96
00:04:52.840 --> 00:04:57.890
Over here, we've got our data sets folder and in here we've got churned txt,

97
00:04:57.890 --> 00:04:59.840
which is our dataset.

98
00:04:59.840 --> 00:05:03.140
So let me scroll down a little bit more.

99
00:05:03.140 --> 00:05:07.340
This next block of code is going to read that dataset these turn txt

100
00:05:07.340 --> 00:05:09.790
and it displays the first five and last five rows.

101
00:05:09.790 --> 00:05:14.940
So let me run this block scrolling down just a little bit.

102
00:05:14.940 --> 00:05:20.540
You'll see that we have 3333 rows and 21 columns.

103
00:05:20.540 --> 00:05:23.690
We've got features up on the top here, like account length,

104
00:05:23.690 --> 00:05:27.780
international plan, voicemail plans and so forth and then,

105
00:05:27.780 --> 00:05:32.110
very importantly, to scroll around a little bit here.

106
00:05:32.110 --> 00:05:34.050
This last column we've got churn.

107
00:05:34.050 --> 00:05:35.440
And that, of course, is the target.

108
00:05:35.440 --> 00:05:38.450
The thing that we're trying to predict for values we've got

109
00:05:38.450 --> 00:05:40.930
true on falsy in this case they're all falsy,

110
00:05:40.930 --> 00:05:42.260
the ones that are being displayed,

111
00:05:42.260 --> 00:05:46.440
which means this is what type of classification.

112
00:05:46.440 --> 00:05:50.180
It's binary, right, true or false binary classification.

113
00:05:50.180 --> 00:05:52.230
And because it's labeled data,

114
00:05:52.230 --> 00:05:58.740
we know that we're working with a supervised learning problem.

115
00:05:58.740 --> 00:06:00.660
So that's the data were working with.

116
00:06:00.660 --> 00:06:03.240
That's these fetch part of the process.

117
00:06:03.240 --> 00:06:06.870
And like I said earlier, Azure getting started It might be just this simple.

118
00:06:06.870 --> 00:06:09.550
You're working with a single CSV file rather than

119
00:06:09.550 --> 00:06:11.650
Data Lakes and TL tools and such.

120
00:06:11.650 --> 00:06:12.400
Keeping it simple.

121
00:06:12.400 --> 00:06:15.970
For now, this data is actually fairly clean.

122
00:06:15.970 --> 00:06:17.700
It's a publicly available dataset,

123
00:06:17.700 --> 00:06:20.150
but you can imagine if we were fetching 10 or 20

124
00:06:20.150 --> 00:06:22.020
different files from different sources.

125
00:06:22.020 --> 00:06:26.140
You probably need to spend some time cleaning up before moving on.

126
00:06:26.140 --> 00:06:30.940
But moving on, let's do some feature engineering scrolling down.

127
00:06:30.940 --> 00:06:33.660
You'll see some additional data about what each feature is.

128
00:06:33.660 --> 00:06:37.140
I'll let you spend some time with this if you'd like,

129
00:06:37.140 --> 00:06:39.100
but down to the next block of code,

130
00:06:39.100 --> 00:06:41.840
what we're gonna do is start exploring the data.

131
00:06:41.840 --> 00:06:42.950
This code will displace um,

132
00:06:42.950 --> 00:06:46.450
frequency tables for each categorical feature and then

133
00:06:46.450 --> 00:06:49.240
histograms for each numeric feature.

134
00:06:49.240 --> 00:06:52.600
I'll click into this and run scrolling down.

135
00:06:52.600 --> 00:06:54.160
We've got our frequency tables here,

136
00:06:54.160 --> 00:06:58.680
so this just shows how frequently each of these values occur as a percentage.

137
00:06:58.680 --> 00:07:02.240
And we could visualize these, but just scrolling down.

138
00:07:02.240 --> 00:07:04.220
They seem to be fairly well distributed.

139
00:07:04.220 --> 00:07:04.730
In other words,

140
00:07:04.730 --> 00:07:08.220
I'm not seeing any huge numbers or any really tiny numbers for any

141
00:07:08.220 --> 00:07:11.470
particular state going down a little bit more.

142
00:07:11.470 --> 00:07:13.820
We've also done the same thing for phone here.

143
00:07:13.820 --> 00:07:17.370
This is not gonna be very useful because phone numbers are unique and

144
00:07:17.370 --> 00:07:20.770
presumably each number appears the same percentage of time.

145
00:07:20.770 --> 00:07:25.440
And that seems to be the case here, scrolling down some more.

146
00:07:25.440 --> 00:07:29.180
You'll see the majority of customers don't have an international plan.

147
00:07:29.180 --> 00:07:32.840
90% of them, in fact, don't have an international plan.

148
00:07:32.840 --> 00:07:36.940
And for voicemail plans, 72% don't have voice mail.

149
00:07:36.940 --> 00:07:42.330
And then for churn, you can see that about 14.5% of customers churned or left,

150
00:07:42.330 --> 00:07:46.040
which is pretty significant, and that's the problem we're trying to solve.

151
00:07:46.040 --> 00:07:49.390
Then we have some descriptive statistics about our data here.

152
00:07:49.390 --> 00:07:53.550
Things like count mean standard deviation men.

153
00:07:53.550 --> 00:07:55.710
This just helps us better understand the data.

154
00:07:55.710 --> 00:07:57.830
We could even use some of these values to fill in

155
00:07:57.830 --> 00:08:00.040
missing values if we had them now.

156
00:08:00.040 --> 00:08:02.640
Anderson visualizations.

157
00:08:02.640 --> 00:08:05.350
We've got the histograms here for the numeric features,

158
00:08:05.350 --> 00:08:08.010
and a lot of these show fairly even distribution,

159
00:08:08.010 --> 00:08:10.340
meaning we have that bell curve.

160
00:08:10.340 --> 00:08:11.580
There was one up at the top,

161
00:08:11.580 --> 00:08:17.640
though scrolling back up customer service calls that has a positive skew,

162
00:08:17.640 --> 00:08:20.010
meaning that most of the distribution is on the left of

163
00:08:20.010 --> 00:08:22.530
the histogram customer service calls.

164
00:08:22.530 --> 00:08:26.350
And whether somebody is a happy customer could definitely be related.

165
00:08:26.350 --> 00:08:29.180
There's a couple others that we might want to dig on here,

166
00:08:29.180 --> 00:08:31.840
for instance, see international calls.

167
00:08:31.840 --> 00:08:32.370
But first,

168
00:08:32.370 --> 00:08:34.620
let's do some dimensionality reduction to get rid of

169
00:08:34.620 --> 00:08:38.240
the things that we really don't need, like phone number.

170
00:08:38.240 --> 00:08:42.340
The next block of code down here will do that,

171
00:08:42.340 --> 00:08:45.310
so we're going to drop phone and we'll also convert the

172
00:08:45.310 --> 00:08:47.350
area code to a non numeric value.

173
00:08:47.350 --> 00:08:49.640
I'll run this block of code,

174
00:08:49.640 --> 00:08:51.420
and the next we'll take a look at the relationship

175
00:08:51.420 --> 00:08:54.040
between features and our target variable,

176
00:08:54.040 --> 00:08:56.910
which is churn this block of code.

177
00:08:56.910 --> 00:09:01.310
Let's run and these tables are going to look similar to what we just saw.

178
00:09:01.310 --> 00:09:04.750
Except here we've added the churn variable so we can see the at

179
00:09:04.750 --> 00:09:10.040
relationship scroll down to something more interesting than state.

180
00:09:10.040 --> 00:09:12.690
You'll see that people who churn or more likely,

181
00:09:12.690 --> 00:09:20.340
to have an international plan 28% versus 6% and are less likely

182
00:09:20.340 --> 00:09:25.910
to have a voicemail plan 16% versus 29% down below.

183
00:09:25.910 --> 00:09:27.200
We've got our histograms.

184
00:09:27.200 --> 00:09:28.840
I'll just scroll through some of these.

185
00:09:28.840 --> 00:09:34.540
We'll find the customer service calls, which were pretty interesting.

186
00:09:34.540 --> 00:09:37.080
And here you'll see under true the people who turned.

187
00:09:37.080 --> 00:09:39.970
There's a higher number of customer service calls than for those who

188
00:09:39.970 --> 00:09:44.540
didn't turn now scrolling down to our next block of code.

189
00:09:44.540 --> 00:09:48.640
Here, we're going to look at the scatter plot matrix will hit shift enter.

190
00:09:48.640 --> 00:09:51.980
This kind of combines these scatter plot with the correlation

191
00:09:51.980 --> 00:09:54.880
matrix that we saw on the slide and plots all the values of

192
00:09:54.880 --> 00:09:57.040
one variable against another.

193
00:09:57.040 --> 00:10:00.670
I'll scroll down to the visuals and let me just zoom on my

194
00:10:00.670 --> 00:10:03.840
browser just a little bit to help see all of this.

195
00:10:03.840 --> 00:10:04.960
Alright, not perfect,

196
00:10:04.960 --> 00:10:07.100
but hopefully you can see that we've got some linear

197
00:10:07.100 --> 00:10:08.770
relationships or correlations.

198
00:10:08.770 --> 00:10:11.850
Here you'll see a few of these apps sloping lines,

199
00:10:11.850 --> 00:10:15.140
and these are 100% positive correlation,

200
00:10:15.140 --> 00:10:16.460
and that might seem exciting.

201
00:10:16.460 --> 00:10:17.900
But if you look at the details,

202
00:10:17.900 --> 00:10:22.010
you'll see that these are actually day minutes and day a charge,

203
00:10:22.010 --> 00:10:24.510
which just means that as you use more minutes,

204
00:10:24.510 --> 00:10:27.250
your charge more, which we would expect for everybody.

205
00:10:27.250 --> 00:10:29.590
And that probably doesn't have a lot of predictive value

206
00:10:29.590 --> 00:10:31.460
for the problem we're trying to solve.

207
00:10:31.460 --> 00:10:33.940
It's also making our chart really difficult to read.

208
00:10:33.940 --> 00:10:36.140
We have similar outcomes for eve minutes,

209
00:10:36.140 --> 00:10:38.110
and ive charge night minutes,

210
00:10:38.110 --> 00:10:42.040
a night charge and international minutes and international charge.

211
00:10:42.040 --> 00:10:45.540
So what we're going to do is dropped these features.

212
00:10:45.540 --> 00:10:47.740
We've got some code here for this.

213
00:10:47.740 --> 00:10:49.140
I'll run this.

214
00:10:49.140 --> 00:10:53.910
But if I go back up and re create this scatter matrix by

215
00:10:53.910 --> 00:10:58.640
running this code again now scrolling back down,

216
00:10:58.640 --> 00:11:02.560
you'll see that we don't have any clear linear relationships like we did before.

217
00:11:02.560 --> 00:11:07.440
And you should be able to confirm that by looking at the numbers above.

218
00:11:07.440 --> 00:11:09.240
I'll increase my browser size again,

219
00:11:09.240 --> 00:11:11.740
so here you can see that we have very low numbers.

220
00:11:11.740 --> 00:11:15.040
Remember that a one means perfect correlation.

221
00:11:15.040 --> 00:11:17.820
And other than that diagonal line where we would expect one,

222
00:11:17.820 --> 00:11:20.280
you could see everything else has very small numbers,

223
00:11:20.280 --> 00:11:22.240
very close to zero.

224
00:11:22.240 --> 00:11:24.330
Okay, so we've done some analysis on our data.

225
00:11:24.330 --> 00:11:25.740
It's already clean.

226
00:11:25.740 --> 00:11:29.640
Now we can start thinking about which algorithm to pick for our model,

227
00:11:29.640 --> 00:11:31.260
but that will be in the next module.

228
00:11:31.260 --> 00:11:34.460
If you're going to pause for a little bit before continuing to the next module,

229
00:11:34.460 --> 00:11:36.870
you want to shut down the kernel and instance,

230
00:11:36.870 --> 00:11:40.340
this notebook is running on so you aren't incurring charges.

231
00:11:40.340 --> 00:11:44.640
The way to do that, we come over here and back up.

232
00:11:44.640 --> 00:11:47.090
If you right click on the notebook that you're working on,

233
00:11:47.090 --> 00:11:48.860
you'll see that you can shut down the kernel.

234
00:11:48.860 --> 00:11:50.280
But that's not the whole story.

235
00:11:50.280 --> 00:11:53.530
What you want to do instead is click on this round button with the square on it,

236
00:11:53.530 --> 00:11:59.240
which is for running terminals and colonels and then shut things down here.

237
00:11:59.240 --> 00:12:01.820
Just note that if you do decide to shut things down now,

238
00:12:01.820 --> 00:12:04.140
you'll need to run all of that beginning code again

239
00:12:04.140 --> 00:12:09.000
before picking up in the next module. And with that, let's summarize what we've learned

