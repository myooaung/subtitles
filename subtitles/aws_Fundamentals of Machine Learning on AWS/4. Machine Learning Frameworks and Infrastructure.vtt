WEBVTT
1
00:00:01.340 --> 00:00:04.240
[Autogenerated] machine learning frameworks and infrastructure.

2
00:00:04.240 --> 00:00:06.440
Now to the foundational layer,

3
00:00:06.440 --> 00:00:08.990
this layer is for the machine learning practitioners who really

4
00:00:08.990 --> 00:00:10.840
want to get into the nitty gritty details,

5
00:00:10.840 --> 00:00:13.540
using different frameworks and having more control of the

6
00:00:13.540 --> 00:00:16.220
underlying infrastructure for frameworks.

7
00:00:16.220 --> 00:00:19.130
There are many out there, and Amazon supports tensor flow.

8
00:00:19.130 --> 00:00:23.410
Mxnet and pytorch Tensorflow was created by Google.

9
00:00:23.410 --> 00:00:27.290
It's very popular today and has good community support for languages.

10
00:00:27.290 --> 00:00:32.780
We have support for python C++ and are mxnet came from Apache.

11
00:00:32.780 --> 00:00:35.590
It also supports multiple languages, including python.

12
00:00:35.590 --> 00:00:41.910
C++ are Julia Perl and more, and Facebook gave us pytorch,

13
00:00:41.910 --> 00:00:44.430
and it's grown in popularity in recent years.

14
00:00:44.430 --> 00:00:45.970
This is a python library,

15
00:00:45.970 --> 00:00:50.700
and it's really favored for its flexibility and speed and then for interfaces.

16
00:00:50.700 --> 00:00:51.490
We have glue on,

17
00:00:51.490 --> 00:00:55.940
and care is the frameworks we saw a minute ago were built more for developers.

18
00:00:55.940 --> 00:00:58.240
But these interfaces are built more for researchers

19
00:00:58.240 --> 00:01:00.160
and other non developer types.

20
00:01:00.160 --> 00:01:01.440
They're basically an interface,

21
00:01:01.440 --> 00:01:05.340
or API to the frameworks we just talked about glue On was a joint

22
00:01:05.340 --> 00:01:09.400
venture between AWS and Microsoft and works with Mxnet and then

23
00:01:09.400 --> 00:01:15.140
caress created by Francoise Chalet, currently with Google Works on tensorflow.

24
00:01:15.140 --> 00:01:18.200
These infrastructure bucket here when you're using Sage maker.

25
00:01:18.200 --> 00:01:20.510
Most of this is abstracted away from you,

26
00:01:20.510 --> 00:01:22.680
but if you're working in the lower layer of the stack,

27
00:01:22.680 --> 00:01:25.740
you have more control of your instances and containers.

28
00:01:25.740 --> 00:01:27.970
There are many easy to instances that are optimized

29
00:01:27.970 --> 00:01:29.840
for machine learning these days.

30
00:01:29.840 --> 00:01:30.690
Inferential.

31
00:01:30.690 --> 00:01:35.440
That second icon over is a chip designed to speed up the inference process.

32
00:01:35.440 --> 00:01:37.100
And then containers, as you've seen,

33
00:01:37.100 --> 00:01:39.940
are used for training and inference with the model.

34
00:01:39.940 --> 00:01:42.890
Elastic inference is away to lower the cost of your inference by

35
00:01:42.890 --> 00:01:46.740
attaching GPU powered acceleration to those instances.

36
00:01:46.740 --> 00:01:48.970
And I won't get into too many details here about green grass.

37
00:01:48.970 --> 00:01:51.590
But just know that this is used for running i o T devices

38
00:01:51.590 --> 00:01:54.740
such as cameras or sensors in the field.

39
00:01:54.740 --> 00:01:57.800
Green Grass allows these devices to act locally with

40
00:01:57.800 --> 00:01:59.530
the data they capture locally.

41
00:01:59.530 --> 00:02:00.650
And this is important,

42
00:02:00.650 --> 00:02:03.940
especially if your devices aren't always connected to the Internet.

43
00:02:03.940 --> 00:02:04.480
But of course,

44
00:02:04.480 --> 00:02:09.000
they can still published data to the cloud for storage and analytics at a later point

