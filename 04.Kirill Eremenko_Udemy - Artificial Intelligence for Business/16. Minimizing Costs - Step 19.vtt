WEBVTT

00:00.480 --> 00:06.170
All right here we go let's build the architecture of our neural network inside this method.

00:06.390 --> 00:11.820
So I insist on the fact that we are going to build indeed the architecture because I remind that the

00:11.820 --> 00:15.930
class only defines the instructions of what you're building.

00:16.110 --> 00:18.990
And right now we're building the model of a brain.

00:19.140 --> 00:26.120
But in order to have the real brain itself and use it well we will have to create an object of the brain

00:26.140 --> 00:28.440
class after it is fully defined.

00:28.590 --> 00:35.310
So what we're about to do now is just to create this fool architecture composed of an input layer of

00:35.310 --> 00:36.820
three input neurons.

00:36.840 --> 00:43.280
Then the first layer of Sixty-Four hidden neurons the second hidden layer of 32 neurons and an output

00:43.280 --> 00:45.070
layer of five neurons.

00:45.240 --> 00:50.070
So let's do this let's build this neural network architecture.

00:50.130 --> 00:56.010
All right so before we build this architecture Well remember that the learning rate we provided here

00:56.070 --> 00:57.640
is only an argument.

00:57.810 --> 01:00.600
So exactly as what happened for the environment.

01:00.780 --> 01:06.780
Well we will have to create also a learning rate object variable which of course will initialized with

01:06.780 --> 01:08.820
the value provided in the argument.

01:08.820 --> 01:09.820
Learning right here.

01:10.010 --> 01:10.610
OK.

01:10.800 --> 01:12.830
So let's start by doing this.

01:12.890 --> 01:22.350
So all of that learning rate which will be initialized as of course the learning rate provided in the

01:22.350 --> 01:29.170
arguments you create then that's when eventually we started building that architecture.

01:29.280 --> 01:35.820
And so we're simply going to do this step by step from left to right meaning that we'll first start

01:35.820 --> 01:42.060
by creating the input layer composed of three neurons and we'll do that by simply creating an object

01:42.150 --> 01:46.980
of the input class then we'll simply create these two here and there.

01:46.980 --> 01:49.850
Here the first one composed of 64 neurons.

01:49.860 --> 01:56.550
And the second one composed of 32 neurons and we'll do that again by creating two objects of the dance

01:56.550 --> 01:57.310
class.

01:57.480 --> 02:03.810
And then again using the dance class we will create the output layer composed of the five possible output

02:03.990 --> 02:10.080
which I remind our for is going to be the q values and then you'll see in the training and the simulation

02:10.350 --> 02:17.400
will use an arc Max method to return the final action which will have basically the maximum of the output

02:17.400 --> 02:20.410
Q values and then eventually.

02:20.580 --> 02:25.530
Well we will create a new object of the middle class with the architecture that we're about to build

02:25.530 --> 02:32.230
here and then we'll compare everything with I mean squared error last and an atom optimizer.

02:32.460 --> 02:40.080
So let's do this let's start with the first step creating the input layer composed of three input neurons.

02:40.440 --> 02:41.820
So how are you going to do this.

02:41.820 --> 02:48.390
Well first we're going to introduce a variable which we're going to call states and that's to specify

02:48.390 --> 02:54.900
that indeed we're starting by building the input layer which contains exactly the state right the states

02:54.970 --> 02:59.390
are the inputs of the neural network and each state is composed of three elements.

02:59.470 --> 03:02.940
The server temperature number of users and the rate of data.

03:03.270 --> 03:05.390
And so this day is horrible here.

03:05.400 --> 03:11.820
As we said it's going to be created as an object of this input class which is a class of the layers

03:11.820 --> 03:13.360
module by Cara's.

03:13.380 --> 03:14.690
And so there we go.

03:14.710 --> 03:21.240
I'm calling this into the class which is taking on the one argument that we actually need and which

03:21.240 --> 03:29.670
is the shape and the shape will be of course the dimensions of the input state which is a simple vector

03:29.670 --> 03:36.240
of three elements and to write this well in parenthesis we're going to enter three here and then a comma

03:36.450 --> 03:42.920
and that creates an input vector of three elements like it has three rows and only one column.

03:43.140 --> 03:44.250
OK great.

03:44.250 --> 03:45.430
We already have inputs.

03:45.480 --> 03:51.870
We already created the input layer here and now we're going to make some fool connections to connect

03:52.050 --> 04:00.660
this input layer here in the states with the first hidden layer composed of 64 neurons.

04:00.900 --> 04:01.780
So let's do this.

04:01.800 --> 04:08.430
And that's where we're going to use the dance class because indeed we're going to create this first

04:08.430 --> 04:17.370
hidden layer that I'm going to call X as an object of the dance class which will create the full connections

04:17.700 --> 04:21.080
between the input layer and this first thin layer.

04:21.300 --> 04:25.360
And so what we have to put in is dense glasses arguments.

04:25.510 --> 04:30.570
Well of course we have to say how many neurons we want in this first layer.

04:30.620 --> 04:33.570
Now remind us we want 64 neurons.

04:33.750 --> 04:36.110
So where does that 64 come from.

04:36.240 --> 04:42.240
Well that comes from my experimentation you know of course before teaching this case that I experimented

04:42.240 --> 04:45.400
with the AI and I tried several architectures.

04:45.510 --> 04:51.570
And this one with 64 neurons in the first layer and thirty two neurons and the second one led to some

04:51.570 --> 04:52.240
good results.

04:52.290 --> 04:57.590
So maybe there is a better one we can actually do some parameter attuning to find the best architecture.

04:57.600 --> 05:03.680
But you can imagine that this is a job because there is an infinite number of architectures that you

05:03.680 --> 05:04.130
can try.

05:04.130 --> 05:10.100
So I try to classic one sixty four neurons in the first layer and third two neurons and the second layer

05:10.370 --> 05:13.500
is an architecture you will see in many examples.

05:13.500 --> 05:15.370
So that's where it comes from.

05:15.440 --> 05:17.140
And we're going to choose this one.

05:17.240 --> 05:24.590
And so to specify that we want indeed 64 hidden neurons in the first hill and later we'll hear the arguments

05:24.590 --> 05:27.500
we will input a number of units.

05:27.500 --> 05:29.480
So this is the name of the arguments here.

05:29.510 --> 05:36.410
I'm not choosing to call it this way it's what was defined in the Cara's library units equals 64.

05:36.440 --> 05:41.280
And with that only you create your hidden layer of 64 neurons.

05:41.450 --> 05:49.680
But then I also want to specify an activation function which will be used to break the linearity of

05:49.680 --> 05:53.920
the operations happening in the neural network that's crucial to understand.

05:54.110 --> 05:59.110
Whenever you build a neural network is always to solve a nonlinear problem.

05:59.210 --> 06:01.040
Otherwise you don't have to build a neural network.

06:01.040 --> 06:03.700
You can just use a simple inner moral.

06:03.830 --> 06:08.720
Of course when we were doing some AI and we have to break some actions the operations will never be

06:08.720 --> 06:09.270
linear.

06:09.320 --> 06:11.290
No it will never be a linear problem.

06:11.510 --> 06:17.990
And in order to break the linearity of the operations happening all these connections are between the

06:17.990 --> 06:22.110
neurons and the inputs the neurons with each other and the neurons with outputs.

06:22.270 --> 06:24.680
For that we use an activation function.

06:24.750 --> 06:28.910
We have several activation functions that you can use for that purpose.

06:29.000 --> 06:35.960
You have to really function rectifiable any unit or you have the sigmoid function which is this function.

06:35.990 --> 06:37.390
So that's what you have to know about it.

06:37.400 --> 06:42.350
But if you need more details on this if you want to get some intuition in more depth.

06:42.470 --> 06:48.140
Well keep in mind that idea of discourse we have some annex sections which provide the theory of deep

06:48.140 --> 06:54.890
learning and which you can check out to get some more solid understanding of the deeper in theory and

06:54.890 --> 06:59.630
we explain in detail how the neural networks work from start to finish.

06:59.660 --> 07:01.010
So feel free to check it out.

07:01.010 --> 07:03.710
We of course talk about the sigmoid function.

07:04.100 --> 07:08.880
And so speaking of the sigmoid function that's exactly what we have to improve here.

07:09.080 --> 07:15.890
And so this activation argument which is again an argument by Chris Well we're going to put in quote

07:16.250 --> 07:24.980
sigmoid get perfect and then to make the full connection between the input layer here which are the

07:24.980 --> 07:27.180
states that is the input state vector.

07:27.380 --> 07:30.370
And this first layer here.

07:30.530 --> 07:38.180
Well we simply have to input here the state and that will make the full connection between the input

07:38.180 --> 07:41.190
layer here and the first hit here.

07:41.420 --> 07:48.440
And so now we are ready to move on to the next step in our architecture which is to build a second hidden

07:48.440 --> 07:55.100
layer and make again a full connection between the first hidden layer composed of the 64 neurons and

07:55.100 --> 07:57.940
the second is composed of 32 neurons.

07:58.160 --> 08:02.150
And to do this we're going to do exactly the same thing as what we just did.

08:02.340 --> 08:09.650
But I'm going to introduce a new marble Y which will be exactly the second layer which will be created

08:09.650 --> 08:16.880
again as an object of the dance class which is a class you have to use anytime you want to build a fully

08:16.880 --> 08:20.770
connected layer and make some connections with the previous layer.

08:20.800 --> 08:25.070
And so here again of course we're going to impute the same arguments.

08:25.330 --> 08:32.000
And I'm going to use the same activation function to break the linearity but this time we're not going

08:32.000 --> 08:36.680
to create 64 neurons but 32 neurons.

08:36.980 --> 08:41.560
Then as we said we keep the same activation function to signal the activation function.

08:41.720 --> 08:47.600
And in the end of course to make that full connection between the second hit in there and the first

08:47.600 --> 08:54.480
hit and they are here well we have two input here that first here and they are perfect.

08:54.500 --> 08:57.990
Now we're making progress in building that architecture.

08:58.130 --> 08:59.800
Let's see what the next step is.

09:00.020 --> 09:06.780
Well the next step is of course to create the final output layer composed of the five actions here.

09:07.070 --> 09:10.030
And to do this we're going to use the dense glass again.

09:10.160 --> 09:10.900
Why.

09:10.910 --> 09:15.830
That's because we have to make the full connections between the second hit and they are composed of

09:15.830 --> 09:17.080
the 32 neurons.

09:17.330 --> 09:23.900
And this output layer composed of the five actions or the five core values of the five actions.

09:23.900 --> 09:24.310
All right.

09:24.320 --> 09:31.300
So let's do this this time I'm not going to call it set since we're about to create the output layer.

09:31.490 --> 09:38.640
And since this output contains exactly the q values Well I'm going to create exact here and a very well

09:38.660 --> 09:46.280
which will be the q values arise so that we clearly remember that the output of our neural network are

09:46.280 --> 09:51.730
the q values and not directly the actions will get the actions later in the training and the testing.

09:51.830 --> 09:55.240
When we have to play exactly the action from zero to five.

09:55.400 --> 09:57.780
OK so two values.

09:57.860 --> 10:02.870
And as we said since we need to make some food things here and as we said since we need to make some

10:02.980 --> 10:11.450
full connections again between the second layer and the output layer Well we're going to use dance class

10:11.450 --> 10:19.880
again so that our output is created as an abject of this class which will take the same arguments here

10:19.910 --> 10:26.720
but with different values so I'm copying everything here and we'll just replace their values by the

10:26.720 --> 10:27.460
right ones.

10:27.650 --> 10:29.470
And so what I'm going to be the right ones.

10:29.630 --> 10:33.610
Well first the units what is going to be the number of units.

10:33.830 --> 10:39.380
Well as we can clearly see here the number of units corresponds to the number of actions that can be

10:39.380 --> 10:40.020
returned.

10:40.130 --> 10:44.110
And we have five actions so the number of units here would be five.

10:44.300 --> 10:50.930
But we specified number of actions of the argument in case you know you want to experiment with different

10:50.930 --> 10:55.400
AI that can play more than five actions or less well in that case.

10:55.400 --> 10:59.300
We want to use this so that we can plug things more directly.

10:59.630 --> 11:05.170
OK and finally what activation function are we going to use here to break inheriting.

11:05.360 --> 11:11.690
Well for the output layer it is recommended and you will see that most of the time is not a sigmoid

11:11.720 --> 11:19.490
activation function but soughed Max activation function and again soughed Knox is clearly explained

11:19.820 --> 11:22.770
in the Annex deploying lectures at the end of this course.

11:22.790 --> 11:26.480
So if you don't know what's up next is I highly recommend to check it out.

11:26.810 --> 11:34.250
And then again to make the full connections between the output letter containing the q values and the

11:34.250 --> 11:37.510
previous hidden layer which is a Y variable here.

11:37.610 --> 11:40.100
Well of course we have to hear why.

11:40.400 --> 11:41.350
And there we go.

11:41.430 --> 11:44.880
We're done building this whole architecture.

11:45.140 --> 11:45.900
Perfect.

11:45.950 --> 11:47.460
So clearly that was easy.

11:47.480 --> 11:50.990
Really this is the easiest way you can do this with Garrus.

11:51.110 --> 11:55.940
You have some other great deep libraries like by torche or even sense of flow.

11:56.110 --> 12:02.400
But with these frameworks you would not build a neural network like this one as easily as we just did.

12:02.450 --> 12:03.110
All right.

12:03.200 --> 12:03.950
Perfect.

12:03.950 --> 12:09.260
Now we're going to move onto the next step which is to assemble everything into a model so we will create

12:09.260 --> 12:15.770
a new model our here which will be an object of the middle class and then from this model will compel

12:15.830 --> 12:24.160
everything with a quit errorless and our atom optimized are created as a nudge X of this and less but

12:24.170 --> 12:26.730
we'll do and explain all this in the next story.

12:26.840 --> 12:27.670
And until then.

12:27.740 --> 12:28.530
Enjoy AI.
