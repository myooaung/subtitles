WEBVTT

00:00.600 --> 00:06.180
Hello and welcome back to the course on a I in the previous part we talked about the deep learning Killary

00:06.360 --> 00:13.170
intuition we started there and in fact we actually got all the way to this part and where we talked

00:13.170 --> 00:18.150
about learning and now we're going to move on to the actual acting part.

00:18.150 --> 00:22.200
So there's there's two parts to distinct parts that we have to remember.

00:22.200 --> 00:25.420
So that's the learning part but now he actually he's done all of this.

00:25.450 --> 00:26.340
That's beautiful.

00:26.340 --> 00:30.450
Now he actually has to take an action he has to decide what is he going to do is going to do action

00:30.450 --> 00:31.670
one two three or four.

00:31.690 --> 00:32.810
And so how does he do that.

00:32.970 --> 00:39.330
Well the way he does it is now given those same values so the values don't change after we've we have

00:39.330 --> 00:43.350
these values of compared them with Calicut the last two by arrogated arrow update at the weights but

00:43.410 --> 00:45.850
the key values don't change in that whole process.

00:45.860 --> 00:47.350
Also have got the cube values there.

00:47.370 --> 00:48.280
They're fixed.

00:48.330 --> 00:53.040
We know what they are sold all this happens though networks updated and out using those same values

00:53.040 --> 00:53.740
that we had.

00:53.910 --> 00:58.430
What we're going to do is we're going to post them through a soft max function.

00:58.560 --> 01:01.920
And again a soft Max is described and I think an annex 2.

01:02.160 --> 01:05.120
And we'll talk a bit more about soft max.

01:05.130 --> 01:12.020
Further down in or we'll talk about this action selection policy further down in the rest of this section.

01:12.090 --> 01:13.680
So just in a few tutorials.

01:13.680 --> 01:17.220
But for now we're just going to say we're passing it through a soft next function.

01:17.220 --> 01:22.200
Basically what it does is it allows it helps select the best one it selects the best action possible.

01:22.200 --> 01:23.610
And there's a small caveat to that.

01:23.610 --> 01:26.060
It's not just the best one possible.

01:26.070 --> 01:30.420
We'll talk about that in the action section policy tutorial but for knowledge just say it selects the

01:30.420 --> 01:33.440
best action from here it says OK so Q1.

01:33.690 --> 01:35.800
You know the likelihood.

01:36.090 --> 01:41.890
Basically we know that q values has predicted the Q value so it can look at them and say OK so the highest

01:41.890 --> 01:46.200
Q value out of these just as we did in the simple Q learning algorithm.

01:46.200 --> 01:50.100
Ill just look at all these for say the highest values this one I'm going to select that action.

01:50.100 --> 01:52.140
We're going to take those and that's pretty much it.

01:52.170 --> 01:57.240
That's how he chooses which action take takes takes action and then all of this process happens again.

01:57.240 --> 02:02.070
For for the next state the agent ends up in in all case and the next square of the maze.

02:02.070 --> 02:04.530
But generally speaking in the next state.

02:04.590 --> 02:05.370
So there we go.

02:05.370 --> 02:14.610
That's how we feed in a reinforcement learning problem into a neural network through a vector describing

02:14.610 --> 02:15.980
the state that we're in.

02:16.110 --> 02:22.170
And once we feed it and there's two parts of the process that happen Part one is the learning.

02:22.350 --> 02:26.790
So remember that part where we compare each of the cube values with the target and then we back propagate

02:26.790 --> 02:32.310
the loss through the network to update the weights so that our network is learning as we go through

02:32.310 --> 02:37.980
this maze or through this environment and also the second part is of course we have to act we have to

02:37.980 --> 02:45.360
select an action and that is where we postes the key values through a soft max function and or basically

02:45.360 --> 02:48.290
an action selection policy which we'll talk about further down.

02:48.420 --> 02:52.800
And then we simply select the action that we want to take and we perform that action.

02:52.800 --> 02:57.120
And then this whole process starts again and then maybe the agent gets then maybe the agent doesn't

02:57.250 --> 02:59.580
Pozza the game.

02:59.580 --> 03:01.040
In any case the game ends.

03:01.200 --> 03:08.220
And then once again the whole process repeats the agent plays the whole game again and then that stops

03:08.220 --> 03:14.360
so basically that's that's another airpark every time the agent you know every time the game ends with

03:14.360 --> 03:16.630
a favor beyond February that's the end of an airport.

03:16.650 --> 03:19.740
And then he starts again and then he starts again and then he starts again.

03:19.740 --> 03:20.370
And so on.

03:20.370 --> 03:26.760
So that happens and this process happens for every single time the agent is in you in a new state so

03:26.760 --> 03:32.190
the state is encoded here so that's important not just for every single game that he plays but for every

03:32.190 --> 03:32.970
single state.

03:32.970 --> 03:38.080
So he's in a state it goes through his process dates and so on and happens every single time.

03:38.110 --> 03:41.330
And so the learning happens and the acting happens as well.

03:41.640 --> 03:47.010
So that is deep learning in the intuition behind deep learning.

03:47.010 --> 03:49.460
We've got lots more to cover off.

03:49.710 --> 03:55.620
And then of course we get practical and in the meantime if you'd like to get some additional information

03:55.650 --> 03:56.670
on the learning.

03:56.670 --> 04:05.150
We've got a recommended reading so we've already spoken about Arthur Giuliani's series of blog posts.

04:05.160 --> 04:12.510
If you look at simple informal learning Lifton's flow part 4 you will find the part that's relevant

04:12.540 --> 04:14.010
to what we discussed today.

04:14.220 --> 04:21.120
Note that here he talks about convolutions we are not covering revolutions in this section we're going

04:21.120 --> 04:23.600
to be talking about them in the next section of the course.

04:23.670 --> 04:28.830
So the difference here is that it's just kind of skip the conclusions part for now and we'll talk about

04:28.830 --> 04:32.770
them in the next part of the course but the difference is in evolutions.

04:32.790 --> 04:37.230
You're like looking your agent is looking at the image and.

04:37.300 --> 04:42.300
And therefore he has to process an image an additional complication for now where we're slowly gradually

04:42.300 --> 04:43.500
building up to that.

04:43.500 --> 04:50.250
For now we're encoding our environment through social care we're encoding our environment or maybe like

04:50.310 --> 04:57.900
look at this one probably in coding our environment as a or in according to state the agent is in as

04:57.930 --> 04:58.650
a vector.

04:58.650 --> 05:04.530
So in our case it was a very a vector of values sometimes people even in that in that simple may sometimes

05:04.530 --> 05:10.140
or as you'll see from this blog post sometimes you'll prefer the one hot and coded version of that state.

05:10.130 --> 05:13.310
So basically where every single box of the maze has a.

05:13.560 --> 05:17.680
So you have like a vector of for a null case would be 12 values three by four.

05:17.700 --> 05:22.040
This isn't like either either one or a zero depending on which elements and which box you're in.

05:22.060 --> 05:23.010
In the environment.

05:23.010 --> 05:29.850
So in whichever way you decide to code your environment and the state of your environment that's how

05:29.850 --> 05:31.470
in coding It's basically a vector.

05:31.470 --> 05:36.360
The key here is that it's not a convolution So it's not like an image and there's no convolution volts

05:36.360 --> 05:37.760
so this part will come later.

05:37.770 --> 05:43.360
For us it starts over here and that just simplifies the process for us to graduate and then better.

05:43.500 --> 05:49.880
And of course don't forget that this post is really to flow and we're using pi torche in our tutorials.

05:50.040 --> 05:51.840
So hopefully you enjoy this.

05:51.840 --> 05:59.110
A quick intro into deep convolutional deep convolutional yet deep book learning.

05:59.250 --> 06:02.860
And on that note I look forward to seeing you next time.

06:02.880 --> 06:05.560
And until then enjoy artificial intelligence.
