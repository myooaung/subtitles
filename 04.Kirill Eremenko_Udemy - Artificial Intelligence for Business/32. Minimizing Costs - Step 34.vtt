WEBVTT

00:00.690 --> 00:04.800
Hello and welcome to the final part of this whole implementation.

00:04.800 --> 00:08.470
Congratulations for already reaching that far.

00:08.520 --> 00:14.700
And now we're about to tackle the last remaining steps which are about implementing the testing that

00:14.700 --> 00:20.490
is the simulation over one year of our AAA regulating the temperature of the.

00:20.670 --> 00:27.330
And we're going to compute the whole energy spent that is accumulated energy spent by the AI and compare

00:27.330 --> 00:33.100
it of course with the energy that would have been spent by this alternative system.

00:33.240 --> 00:36.450
And that of course on the same simulation.

00:36.490 --> 00:37.940
So that's all this is about.

00:37.950 --> 00:43.340
We already have the training done which means we already have our weight in model at age 5.

00:43.340 --> 00:46.700
So this was populated at the very end.

00:46.770 --> 00:48.370
This is just the bike.

00:48.420 --> 00:49.950
You can remove it.

00:49.950 --> 00:54.210
We actually don't need it for what we're going to do meaning just running this simulation.

00:54.330 --> 00:55.560
So I'm deleting it.

00:55.800 --> 00:56.730
And here we go.

00:56.760 --> 01:00.850
Now we're ready to implement that simulation.

01:00.870 --> 01:04.090
All right so we're going to do this the most efficient way of course.

01:04.320 --> 01:10.140
And this will consist of taking everything from the training and just adapting because you're going

01:10.140 --> 01:13.250
to see that it's going to be almost the same.

01:13.470 --> 01:19.630
So I'm going to take everything from here to the bottom.

01:20.220 --> 01:24.290
And in the testing file I'm going to replace that.

01:24.390 --> 01:28.040
And now let's see what we have to change or remove.

01:28.050 --> 01:28.350
All right.

01:28.350 --> 01:33.550
And let's start with the first section in the libraries and the other python files.

01:33.900 --> 01:41.210
So first of all what we can already notice is that we will need the brain and we will need the dequeue

01:41.210 --> 01:41.870
an object.

01:42.030 --> 01:45.260
Because this was only used for the training.

01:45.270 --> 01:50.370
Now you're going to say by the way how come do we not need the brain if we're going to use our brain

01:50.630 --> 01:52.440
that was just trained previously.

01:52.590 --> 01:59.070
Well that's because we're going to use the load method by the pre-build model class of Cara's which

01:59.070 --> 02:06.210
will very simply load this model that H5 which contains all the weight of our pre-trained neural network

02:06.540 --> 02:13.020
and therefore we don't need that brain Vihear so we can just remove it and then for TQM that's obvious

02:13.380 --> 02:15.400
it's clearly only for the training.

02:15.510 --> 02:17.250
So we don't need this as well.

02:17.330 --> 02:24.480
And now however we will import something new which is exactly what I've just told you about which is

02:24.480 --> 02:33.990
that load model method from cares and therefore from Paris and more specifically from the models by

02:33.990 --> 02:34.740
Kerith.

02:34.920 --> 02:40.160
Well I'm going to import that load model function.

02:40.170 --> 02:41.100
All right perfect.

02:41.100 --> 02:45.080
We have a warning because we haven't used it yet but no worries we're about to.

02:45.210 --> 02:51.120
And then regarding the environment of course we have to import that because we will of course run our

02:51.120 --> 02:53.310
simulation inside the environment.

02:53.310 --> 02:57.010
And of course the same one as the one that was used for the training.

02:57.120 --> 03:03.120
Meaning with that optimal range of temperature that initial month that initial number of users in that

03:03.210 --> 03:04.470
initial rate of data.

03:04.470 --> 03:09.500
So that means that for example this line of code we won't have to change anything.

03:09.510 --> 03:10.280
All right perfect.

03:10.290 --> 03:12.320
Now let's move onto the next section.

03:12.330 --> 03:15.540
Sending the seeds for reproducibility purposes.

03:15.600 --> 03:17.190
So we're going to keep that.

03:17.190 --> 03:21.510
And again that's only if you want to get the same results as mine.

03:21.630 --> 03:27.110
You know the ones that will get at the very end of this condition when we execute the simulation.

03:27.120 --> 03:29.440
So I suggest that you keep the same.

03:29.460 --> 03:34.770
And of course if you want to play with some other seeds I'm sure you will get good results as well.

03:34.770 --> 03:35.100
All right.

03:35.100 --> 03:39.690
Now moving on to the next section here we have some things to change.

03:39.780 --> 03:44.340
So let's have a look at them one by one to make sure we don't forget anything.

03:44.340 --> 03:47.880
All right so that Epsilon parameter remember what it was used for.

03:47.900 --> 03:50.310
It was used for exploration purposes.

03:50.310 --> 03:57.480
Remember 30 percent of the time we did some exploration during the training and therefore that's only

03:57.480 --> 03:58.960
related to the training.

03:59.010 --> 04:03.990
We don't need to do any exploration during simulation and therefore we don't need that.

04:04.350 --> 04:05.020
OK.

04:05.100 --> 04:10.620
Now then the number of actions of course we need it because we're going to play with the same number

04:10.620 --> 04:11.860
of possible actions.

04:12.060 --> 04:15.390
So let's keep that direction boundary of course we'll need it.

04:15.390 --> 04:22.280
Because remember this was used to compute directly in one formula the energy spent and of course since

04:22.340 --> 04:26.830
we're going to compute again the energy spent by the AI during our simulation.

04:27.000 --> 04:32.370
Well we will need to keep that direction boundary parameter then number of books.

04:32.460 --> 04:33.650
Well this one is obvious.

04:33.720 --> 04:36.350
Of course the number of epochs is only for the training.

04:36.400 --> 04:44.510
So let's get rid of this then max memory again very obvious these belong to the DGU and implementation.

04:44.550 --> 04:47.340
So that's only related to training as well.

04:47.340 --> 04:49.980
Let's get rid of this then the batch size.

04:49.980 --> 04:50.950
Good question.

04:50.970 --> 04:53.190
Do we have to keep that batch size parameter.

04:53.210 --> 04:56.720
Are we going to have to use some batches during the simulation.

04:56.910 --> 04:58.470
Well absolutely not.

04:58.470 --> 05:00.730
Remember the purpose of our batches.

05:00.730 --> 05:07.990
You know our previous batches of inputs and target was only to do mini batch lernen you know when we

05:08.120 --> 05:10.240
they do weights to reduce the loss.

05:10.360 --> 05:16.750
Well we performed many batch gradient descent which is a combination of stochastic grillin descent and

05:16.810 --> 05:22.630
batch gradient descent and therefore that's again only for the training and we get rid of this.

05:22.690 --> 05:24.880
And then finally the temperature step.

05:24.880 --> 05:32.260
So again this is another element that allows us to compute the energy spent by the AI in one single

05:32.260 --> 05:38.800
formula and since at each time step of the simulation we will have to compute again the energy spent

05:38.800 --> 05:39.670
by the AI.

05:39.820 --> 05:42.250
Well we'll need that temperature step.

05:42.550 --> 05:45.580
All right so perfect for that section setting the parameters.

05:45.640 --> 05:48.180
Now moving on to the next one.

05:48.280 --> 05:52.920
So that's where we build the environment by simply creating an object of the environment class.

05:52.960 --> 05:58.360
And as I've just told you we're going to run of course the simulation on the same environment that was

05:58.360 --> 06:04.510
used for the training and therefore we keep the same range of optimal temperature and same initial number

06:04.510 --> 06:07.330
of users an initial rate of data transmission.

06:07.330 --> 06:09.460
So nothing to change here now.

06:09.490 --> 06:10.430
Interesting.

06:10.450 --> 06:14.880
The next good section is about building the brain by simply creating an object of the brain.

06:15.160 --> 06:19.750
Well you know already the answer that of course we have to get rid of this because that's where we create

06:19.750 --> 06:21.120
the brain object itself.

06:21.120 --> 06:23.520
That is the neural network of our AI.

06:23.710 --> 06:29.440
But as I've told you at the beginning of this tutorial this time we're not going to create a brain because

06:29.440 --> 06:34.250
we already have a pre-trained brain and therefore we don't have to create a new brain.

06:34.270 --> 06:36.510
We can just load the pre-trained brain.

06:36.580 --> 06:46.780
And so I'm going to change the name of that code section and well let's just call it Lowton pre trained

06:47.350 --> 06:48.080
brain.

06:48.160 --> 06:48.760
Right.

06:48.760 --> 06:53.360
Our pre-trained brain that was trained before and in size.

06:53.470 --> 06:54.660
Now what are we going to do.

06:54.670 --> 06:59.530
How are we going to load that brain that we've just trained in the previous tutorial.

06:59.530 --> 07:04.270
Well first we're going to have to introduce a new Voivode which we can call either you know brain or

07:04.270 --> 07:10.390
even moral because what we're going to lose actually is not all that H5 so let's call this variable

07:10.390 --> 07:14.150
model and load the weight of our brain.

07:14.170 --> 07:20.890
We're going to use of course what we imported here the load model function from the Cara's models and

07:21.280 --> 07:25.400
I'm going to base that here and it's going to take as inputs.

07:25.480 --> 07:30.020
Of course all the weight saved into this fout model that H5.

07:30.040 --> 07:38.580
And so in this Lomo function we need inputs in double quote model that H5 perfect.

07:38.590 --> 07:41.700
All right so that will lower our brain resemble.

07:41.800 --> 07:44.060
Once again thanks to Karris.

07:44.080 --> 07:44.490
All right.

07:44.500 --> 07:48.480
Perfect So we've made some good progression but we're not there yet.

07:48.520 --> 07:53.980
Let's move on to the next section which is about building the Dejuan moral of the declaring class.

07:53.980 --> 07:54.850
This one is of use.

07:54.850 --> 07:56.250
We have to remove it.

07:56.260 --> 07:58.480
This is only for the training.

07:58.480 --> 07:59.200
Here we go.

07:59.200 --> 08:00.980
And finally choosing the mode.

08:01.120 --> 08:02.370
This one is obvious as well.

08:02.370 --> 08:08.560
This time we're not going to have to train mode but the inference mode and therefore as we explained

08:08.560 --> 08:13.170
in the environment we have to set it to false right.

08:13.210 --> 08:16.680
Training equals false meaning that we are in inference mode.

08:16.980 --> 08:17.690
All right.

08:17.690 --> 08:24.340
And then let's do the last final thing in this tutorial before we move on to that you know big loop

08:24.520 --> 08:30.390
iterating on all the time steps in one full year money on all the minutes in one full year.

08:30.430 --> 08:42.370
And so here is how I'm going to call that code section let's say running one year simulation and we

08:42.370 --> 08:46.220
can add an inference note.

08:46.510 --> 08:47.050
Great.

08:47.050 --> 08:47.520
Perfect.

08:47.520 --> 08:54.640
So now basically what we only have to do is make that big for loop so we can you know started from scratch

08:54.640 --> 09:00.420
because we are going to re-implemented from scratch and it will be easier for us this way because Anyway

09:00.430 --> 09:02.530
there is a lot to remove here in the training.

09:02.680 --> 09:04.700
So let's just start from scratch again.

09:04.720 --> 09:11.470
However I'm just going to remove everything up to here because indeed indeed we will print all the final

09:11.470 --> 09:16.510
results with these same prints and we'll keep the same variable names.

09:16.540 --> 09:23.860
So I'm just going to remember this and then I'm going to put all these prints just below in a new code

09:23.860 --> 09:29.950
section which of course won't be to any for loop because they're simply going to be the very final results

09:29.960 --> 09:35.830
you know the total energy spent at the end of the full year both by the AI and that alternative cooling

09:35.830 --> 09:39.170
system and then of course this we can remove it.

09:39.310 --> 09:42.520
That was just to save them all after the training.

09:42.520 --> 09:49.510
All right so basically we're ready to make that final implementation where we run this whole 1 year

09:49.510 --> 09:54.880
simulation in inference mode and you'll see that it's going to be pretty easy thanks to what we've done

09:54.880 --> 10:01.180
before you know we were kind of vaccinated with all this if you understood this you will have absolutely

10:01.180 --> 10:05.190
no problem to understand the implementation of the simulation.

10:05.350 --> 10:07.230
So let's do this in a next toil.

10:07.240 --> 10:08.910
And until then enjoy AI.
