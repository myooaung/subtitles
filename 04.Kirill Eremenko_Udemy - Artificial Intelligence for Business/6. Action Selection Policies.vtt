WEBVTT

00:00.540 --> 00:04.020
Hello and welcome back to the course on artificial intelligence.

00:04.020 --> 00:05.360
I hope you're enjoying the course so far.

00:05.370 --> 00:09.000
And today we're talking about action the selection policies.

00:09.000 --> 00:09.350
All right.

00:09.360 --> 00:10.890
Let's dive straight into it.

00:10.950 --> 00:17.880
Previously we talked about adding a neural network to our simple learning and so far we are getting

00:18.000 --> 00:21.180
quite into deep learning.

00:21.180 --> 00:26.560
We've talked about the learning part quite a bit including adding some elements to it.

00:26.580 --> 00:29.940
And today we're talking about this part we're talking about the acting.

00:29.970 --> 00:31.240
So let's have a look.

00:31.260 --> 00:38.640
So here we've got what we discussed about acting that once you input the values the parameters or the

00:38.640 --> 00:45.180
vector describing the state the agent is currently in that environment then that is after all the learning

00:45.180 --> 00:47.240
is done or even before the learning is done.

00:47.370 --> 00:49.460
Basically we get all the q values.

00:49.470 --> 00:53.880
So we're not interested in the learning right now we insist on acting so once we have these key values

00:53.880 --> 00:57.100
how do we understand which one we need to use.

00:57.300 --> 00:58.860
Well if you think about it.

00:58.860 --> 01:01.840
Q values are simply the predictions for the cube.

01:01.860 --> 01:08.580
So as we did in the simple learning algorithm what did we do we just selected the one with the best

01:09.100 --> 01:10.380
of the highest value.

01:10.380 --> 01:15.330
Once we have the one with the highest IQ value we just take that action because it just brings us the

01:15.330 --> 01:20.580
highest value and that we know that Duval's calculus is immediate reward that we expect to receive Plus

01:20.580 --> 01:22.330
the DK factor or times of value.

01:22.350 --> 01:23.000
The next day.

01:23.030 --> 01:28.550
And it's a recursive calculation so why not why wouldn't you take the best value.

01:28.560 --> 01:30.650
That's kind of the end of it.

01:30.750 --> 01:35.310
But as you can see here it's not as simple here we're using a soft max function and this is where we're

01:35.310 --> 01:37.870
going to talk about action selection policies.

01:37.890 --> 01:41.250
So here in reality we don't have to have just a soft max function.

01:41.250 --> 01:44.730
We can have different action selection policies.

01:44.730 --> 01:51.840
For example we've got Epsilon greedy Epsilon's soft and we've got the soft Max and those are kind of

01:51.840 --> 01:57.030
like the most commonly used action selection policies of course there are others for instance the most

01:57.030 --> 02:00.070
basic one is a very simple actions.

02:00.080 --> 02:03.730
Suppose you just select the best the one with the highest Q value.

02:03.930 --> 02:09.420
But why doesn't that action pulse you fly and why do we have different types of action pulse action

02:09.420 --> 02:10.430
selection pulls.

02:10.470 --> 02:15.200
Well it all boils down to exploration versus exploitation.

02:15.510 --> 02:22.590
And that is the core of reinforcement learning because we already talked about this a little bit that

02:22.830 --> 02:28.740
your agent when it's operating in an environment it might predict certain Q values which might be good

02:29.010 --> 02:34.920
and it might turn out great it might turn out that those valves are bad and will be forced to explore.

02:34.920 --> 02:40.590
So if we for instance in this case predict that Q2 is the best one and then it takes Q To takes action

02:40.590 --> 02:43.870
too and it is so from here it takes action too.

02:44.010 --> 02:46.740
And then it gets it gets a very negative reward.

02:46.830 --> 02:51.930
Then the environment is forcing the agent to go and explode because now he's going to learn that oh

02:51.930 --> 02:56.680
actually I thought Q2 was going to be very good but it turned out very bad.

02:56.740 --> 02:58.320
So the results are not very bad.

02:58.320 --> 03:02.940
So the networks can update itself so next time he's in the state he's going to probably eat my soldiers

03:02.970 --> 03:03.940
get to it.

03:04.140 --> 03:09.420
You know like if it was a very very favorable so you might think that that's like you know you might

03:09.420 --> 03:14.940
need a couple of times a couple of penalties or punishments in order to learn that future is about action.

03:14.940 --> 03:20.220
But maybe he'll already soon learn that I'm going to take a different action and take the action because

03:20.220 --> 03:22.070
now it has the best value.

03:22.110 --> 03:27.340
So sometimes the environment forces the agent to take different to explore different actions.

03:27.540 --> 03:35.960
But sometimes the agent might get it find itself stuck in a local maximum it might find that it followed

03:36.180 --> 03:40.530
through through its initial exploration and found that this is a pretty cool action like I'm going to

03:40.530 --> 03:42.050
go right here.

03:42.150 --> 03:43.840
And that d'esprit collection.

03:43.890 --> 03:49.710
But the problem is that it thinks is the best action simply because it hasn't explored is explored going

03:49.710 --> 03:55.770
up his nose or going left is explore going right but it hasn't explored going down from that specific

03:56.310 --> 04:01.610
state that it's in and now that it's kind of like biased towards this action and thinks good action

04:01.610 --> 04:03.610
is going to keep taking it is going to keep getting.

04:03.790 --> 04:06.540
He's going to keep taking is actually going to keep getting a good reward.

04:06.540 --> 04:13.960
But what if this action would have been even better if this action would have been so much better that

04:14.010 --> 04:19.260
if it knew about this action it would actually switch to this action but because it got stuck in a local

04:19.260 --> 04:23.540
maximum is getting these good rewards is just going to be reinforced.

04:23.580 --> 04:27.690
This is going to keep reinforcing itself that or the violence going to reinforce it that this is a good

04:27.690 --> 04:29.240
action to take keep doing that.

04:29.480 --> 04:35.730
But the reality is that there's this other action that it hasn't found yet or hasn't even explored that

04:35.730 --> 04:40.410
would have been much better as a what do you want to do is we want to come up with an actual selection

04:40.410 --> 04:45.750
policy that allows our agent not to get stuck in a local maximum.

04:45.750 --> 04:50.130
Yes it's important to you know keep doing the good actions that's the exploitation part.

04:50.130 --> 04:51.970
We won't exploit what we've found.

04:52.050 --> 04:56.670
But at the same time we still want to explore we never want to stop exploring as like in life you never

04:56.670 --> 04:58.960
want to stop learning you stop learning you die.

04:58.960 --> 05:03.280
That's There's a saying like that that when you're not growing you're dying or something.

05:03.320 --> 05:07.520
So you want to keep learning and your agent wants to keep learning.

05:07.700 --> 05:10.150
And that's where these action selection policies come in.

05:10.340 --> 05:13.910
So we've got three you listed here so the first one is excellent.

05:13.950 --> 05:20.270
Really it's a very simple one it sounds pretty complex in the sense that like it's got a cool name and

05:20.270 --> 05:22.260
usually things with surgical names are complex.

05:22.310 --> 05:23.120
It's actually not.

05:23.120 --> 05:31.340
So basically what it does is it will select the one with the best Q value and epsilon like Epsilon 3

05:31.340 --> 05:34.940
you might hear in other places it's just like selection policy.

05:35.180 --> 05:36.920
So in this case we're using it too slick.

05:36.920 --> 05:43.640
So our out of our cube values are by action sales like the one with the highest Q value all the time

05:43.640 --> 05:45.850
except for Epsilon percent of the time.

05:45.950 --> 05:53.240
So for instance if you set epsilon to 10 percent then you're going towards 0.1 than 10 percent of the

05:53.240 --> 05:56.680
time that the action is going to be selected at random.

05:56.690 --> 06:01.930
So 90 percent of the time you're still going to be selecting the best action based on the highest value.

06:02.060 --> 06:07.670
But 10 percent of the time is going to be selecting a random action uniform it is going to be absolutely

06:07.670 --> 06:13.160
randomly taking an action or if you set epsilon to zero point five for 0.05.

06:13.160 --> 06:19.130
That means that 95 percent of the time the agent is going to be taking the action with the highest value

06:19.160 --> 06:22.410
but 5 percent of the time it's still going to be selecting and a random action.

06:22.430 --> 06:25.640
So it's going to be going out there and exploring.

06:25.730 --> 06:31.160
So epsilons solved is a very similar way the way that that's kind of like why it's called Epsilon greedy

06:31.160 --> 06:39.750
because then you're greedily selecting the action the good action except for that little episode.

06:39.750 --> 06:40.250
Some of the time.

06:40.250 --> 06:46.910
So the lower the abseil they'll lower the Lepp Epsilon the more greedily you're selecting that kind

06:46.910 --> 06:53.840
of the action that is the optimal action and the less you're leaving the less chances you leaving for

06:53.840 --> 06:54.680
exploration.

06:54.710 --> 06:55.940
Epsilon soft is the opposite.

06:55.940 --> 07:02.240
So basically you're selecting at random you're selecting one minus Epsilon cent of the time so if you

07:02.240 --> 07:08.500
Epsilon's like 0.1 to 10 percent then only 10 percent of the time you're taking this action.

07:08.570 --> 07:12.130
And 90 percent of the time you're selecting a random action.

07:12.350 --> 07:19.080
So very very simple just inverted algorithms and a soft Max is kind of like the next step from or it's

07:19.140 --> 07:24.320
it's a more advanced version I would say over epsilon of epsilon greedy algorithm although they both

07:24.320 --> 07:30.420
have merit and they both have place where we want to be using self-mastery in our coding in or practical

07:30.420 --> 07:30.800
sort of thing.

07:30.800 --> 07:35.200
So that's what we're going to talk in a bit more detail about soft max.

07:35.300 --> 07:36.320
So let's have a look.

07:36.320 --> 07:38.390
So let's move on to soft next hopefully.

07:38.420 --> 07:40.670
It's pretty clear about Epson's greedy.

07:40.770 --> 07:42.760
It's a pretty straightforward algorithm.

07:42.770 --> 07:44.970
Select this one.

07:45.170 --> 07:47.730
Most of the time except for sometimes go and explore.

07:47.750 --> 07:53.780
And now we also see why it's important to do that exploration so that we don't end up in local maximums

07:53.780 --> 07:58.690
in our in our optimization process so we're going to look a bit more about soft Max.

07:58.820 --> 08:02.630
There's a tutorial on soft Max at the end of the course.

08:02.720 --> 08:09.500
I think it's an annex number two where we talk about the concept behind sauf maxims to refresh a little

08:09.500 --> 08:14.180
bit here so there we're talking about conflictual neural networks and by the way we're all going to

08:14.180 --> 08:18.590
be covering convolutional we're not covering evolution neural networks in this section of the course

08:19.010 --> 08:21.430
in this section we're still using a vector.

08:21.740 --> 08:27.710
But in the next section of the course when we're we're creating an AI to play Doom we are going to be

08:27.710 --> 08:32.810
using convolutional neural network so it could be beneficial for you to look at in relational neural

08:32.810 --> 08:39.200
networks and then take the soft max function or you can learn a bit more Bazoft Max after you take the

08:39.200 --> 08:43.000
convolutional neural networks and of course later on.

08:43.190 --> 08:46.060
But here's a quick refresher So here we go.

08:46.100 --> 08:48.890
Can relational neural network which decides whether it's a dog or cat.

08:48.890 --> 08:55.940
So here we've got the voting process between these neurons and this one that says that it's it's got

08:55.940 --> 08:58.190
the features you know the fluffy ears.

08:58.190 --> 09:02.050
What's the point to point that face type of thing.

09:02.210 --> 09:09.500
And the kind of features that are the types of eyes the eye with eyes look all these features that belong

09:09.500 --> 09:13.670
to dogs so it's a 95 percent chance that it's a dog and the 5 percent chance that cat.

09:13.850 --> 09:19.400
But the question is how did we get in that Tauriel we're talking about how did we get these values to

09:19.430 --> 09:20.470
add up to one.

09:20.810 --> 09:27.600
Well whatever convolutional all our whole neural networks are the can relational neural network plus

09:27.600 --> 09:31.720
the fully connected Lares whatever it's bad out whatever the values it is about.

09:31.790 --> 09:33.850
We apply a soft next function here.

09:33.950 --> 09:37.710
This is where we introduced and found a formula for soft next function.

09:37.770 --> 09:38.630
Is what it looks like.

09:38.720 --> 09:40.560
And then we got these values.

09:40.560 --> 09:43.400
And so basically that's a quick refresher.

09:43.400 --> 09:46.030
This is the formula for the soft knocks.

09:46.040 --> 09:50.650
It's what it does is it takes however many outputs you have doesn't matter.

09:50.840 --> 09:58.070
It will take them and it will squash them all into values between 0 and 1 regardless of how big they

09:58.070 --> 09:58.850
are.

09:59.430 --> 10:04.420
It's for me you can see that there's a total sum at the bottom so this is going to be zero bitches are

10:04.450 --> 10:04.810
in one.

10:04.820 --> 10:08.660
And also the all these values are going to add up to one always.

10:08.670 --> 10:16.710
And so that's that's very beneficial for us because when we're using the soft max function what happens

10:16.770 --> 10:24.330
is we get these new values we select this best value but in reality what happens is these values that

10:24.330 --> 10:26.660
we get there are actual numbers right.

10:26.700 --> 10:28.700
So this is some kind of numbers.

10:28.860 --> 10:31.670
They don't have to all add up to one and it's twins are in one.

10:31.670 --> 10:33.120
Just some numbers.

10:33.120 --> 10:38.490
But when we apply soft Max we don't just select the best one we actually get numbers like that so we

10:38.490 --> 10:44.250
get our numbers in the range between 0 and 1 and that are also that also add up to 1.

10:44.260 --> 10:47.190
And so what other thing do we know that up to one.

10:47.280 --> 10:52.980
Well probabilities we know that probabilities always have to add up to 1 so that is why we can say here

10:52.980 --> 10:58.530
we've got key values but here all of a sudden we've got soft or we've got probabilities so we can say

10:58.530 --> 11:02.790
that the likelihood of this being the best action is 90 percent.

11:02.790 --> 11:08.570
This lesbian bisexual is 5 percent 2 percent 3 percent because we know the higher your q value the better

11:08.580 --> 11:09.290
the action.

11:09.330 --> 11:14.870
So if we squash them to 0 to 1 then these become probabilities and we can deal with them as such.

11:15.060 --> 11:22.580
And therefore now is when the action is selected and that's how we come up with Q2.

11:22.860 --> 11:28.520
But if you look at it closely this isn't a strict 100 percent and these are not 0 percent.

11:28.530 --> 11:30.740
So this is a 5 percent to 3 percent.

11:30.750 --> 11:41.070
So the the most natural way to apply the soft Max in order to preserve exploration in the algorithm

11:41.370 --> 11:48.540
is to use these exact probabilities as how often we're going to be taking that action.

11:48.540 --> 11:55.650
So these probabilities actually present the distribution all of these actions that we're taking so basically

11:55.830 --> 12:01.670
soft Max makes it very easy for us to come up with a way to combine exploitation and exploration.

12:01.680 --> 12:07.350
So the best the best action will always have the high probability because it has high value and therefore

12:07.350 --> 12:10.530
here we're going to be just going to use these as our distribution.

12:10.530 --> 12:15.390
We're going to say OK we're going to be taking Q2 90 percent of the time but 5 percent of the time we

12:15.390 --> 12:20.010
still get to be taking Q1 and 2 percent of the time we get to 3 and 3 percent of the time we're going

12:20.010 --> 12:21.110
to be taking Q4.

12:21.360 --> 12:27.030
And the beauty here is also that as these values update as and as the agent goes through the network

12:27.030 --> 12:35.150
more and more and more it becomes more familiar with with the environment and therefore these updates

12:35.160 --> 12:41.600
so this value for instance might become like it might might ascertain that this value is actually less

12:41.600 --> 12:47.000
or this actually is higher and so these probabilities will also change as an agent goes through.

12:47.010 --> 12:52.920
So even though here we've got you to nobody is to say that sometimes 5 percent of the time to be more

12:52.920 --> 12:59.590
precise we'll be selecting Q1 as the action to take and sometimes or action one will be taking action

12:59.590 --> 12:59.980
one.

13:00.100 --> 13:05.220
Sometimes we'll be taking action through a two action three two percent of the time and action for will

13:05.220 --> 13:06.330
be taking about 3 percent.

13:06.360 --> 13:13.560
So every action has a chance to play in this process as long as we have enough iterations and the agent

13:13.560 --> 13:17.870
goes through lots and lots of times through these states that they're in.

13:17.880 --> 13:23.850
And that's that's how this that's how any kind of deep learning algorithm works that you want to do

13:23.850 --> 13:29.970
this many many times so that you learn from experience and therefore as you can see here it's a very

13:29.970 --> 13:31.700
natural transition to.

13:31.800 --> 13:37.530
We're not just randomly like an Epson angry algorithm and not just randomly selecting the actions we're

13:37.530 --> 13:44.050
selecting them based on their soft max values which makes it makes it like has some logic behind it

13:44.130 --> 13:48.720
not just not just that random 10 percent of the time we're selecting a random action but there's some

13:48.720 --> 13:52.900
logic behind how we're doing it and based on their values that we've explored.

13:53.220 --> 13:58.550
And so that's the action selection policy that we're going to be using in this course.

13:58.560 --> 14:03.710
You're welcome to definitely check out the Ebsen greedy action section Polsce if you like.

14:03.960 --> 14:10.260
But we're going to be predominately using the soft Max action section policy and I've got an interesting

14:10.350 --> 14:11.390
reading for you.

14:11.430 --> 14:17.400
So this is called adaptive Epsilon greedy exploration in reinforcement learning based on value differences.

14:17.400 --> 14:24.420
It's a 2010 article and it's interesting because Mike Michel I'm not sure how to pronounce and Michelle

14:24.420 --> 14:33.660
and Miquel toxic introduces a different type of Algren's so and adjusted only greedy algorithm and called

14:33.660 --> 14:39.950
the VDB VDB algorithm or epsilon greedy VDB algorithm you can see it over here.

14:40.350 --> 14:46.530
And he actually compares compares it to the Ebsen greedy and soft Max and it's an abseil agree algorithm

14:46.590 --> 14:55.680
which basically the main idea behind it is to adjust the value of epsilon depending on the state the

14:55.680 --> 14:56.490
agent is in.

14:56.490 --> 15:01.780
So if if the agent is very certain about the it there and then Epsilon's should be smaller so they should

15:01.780 --> 15:03.970
be less exploration if the agent is uncertain.

15:04.070 --> 15:06.270
Epson's should be higher should be more expression.

15:06.310 --> 15:09.030
So it is a 2010 article.

15:09.220 --> 15:17.890
I'm not sure if it's if this new proposed algorithm is widely used or is as been accepted in the community

15:17.910 --> 15:24.400
or or if artificial Times has moved kind of away from this this suggestion but nevertheless it will

15:24.640 --> 15:30.110
definitely help you reinforce your knowledge about action selection policies which we discussed the

15:30.110 --> 15:32.240
Epsom ingredient the soft Naxal help you.

15:32.290 --> 15:37.810
Going to put you in by sight and also see in which direction people actually think when they want to

15:37.810 --> 15:44.440
improve artificial intelligence so if you're ever planning on creating really interesting algorithms

15:44.440 --> 15:50.860
that are pushing the edge of an artificial intelligence and pushing the envelope in this space then

15:50.860 --> 15:56.800
this could be a good way for you to see in which direction people think sometimes when they're trying

15:56.800 --> 16:03.660
to improve the norms of artificial intelligence or the norms that existed back then in 2010.

16:04.000 --> 16:04.750
So there we go.

16:04.750 --> 16:11.010
Hopefully you enjoyed today's tutorial about the action selection policies and we learned about abseil

16:11.020 --> 16:18.190
greedy Epsilon soft and the soft Macs and now you are even more prepared for the practical side of things.

16:18.250 --> 16:20.770
And on that note I'll look for CNN.

16:20.800 --> 16:22.510
And until then enjoy AI.
