WEBVTT

00:01.730 --> 00:05.290
Hello and welcome back to the course on machine learning.

00:05.390 --> 00:08.690
Today we talking about the multi armed bandit problem.

00:08.720 --> 00:14.510
Don't you just love these names and they come up such cool names for machine learning algorithms and

00:14.510 --> 00:15.460
problems.

00:15.830 --> 00:23.690
Well today we're indeed talking about this problem and it is the example that we're going to be using

00:23.690 --> 00:25.800
in this whole section on reinforcement learning.

00:25.790 --> 00:31.370
We're going to be looking at different ways that we can solve the multi armed bandit problem and comparing

00:31.370 --> 00:32.470
the results.

00:32.600 --> 00:37.910
But before we can do you I just wanted to mention that the Malte armband The problem is not the only

00:38.390 --> 00:43.250
problem that can be solved with reinforcement learning reinforcement learning is actually really really

00:43.250 --> 00:49.220
cool reinforcement learning for instance is used to train robot dogs to walk.

00:49.220 --> 00:55.610
And I'll give you a quick example for instance you can once you've created a robot dog you can implement

00:55.610 --> 01:00.030
an algorithm inside the robot dog which will tell it how to walk you can tell it.

01:00.170 --> 01:00.410
All right.

01:00.410 --> 01:05.480
Move your front right forward and then move your left back foot and then Front Left Forward right back

01:05.480 --> 01:10.040
foot and so on you can actually give the sequence of actions that it needs to take in order to accomplish

01:10.040 --> 01:17.180
a task which is walking or you can implement arean Forsman learning algorithm which will train the dog

01:17.210 --> 01:20.420
to walk in a very very interesting way.

01:20.420 --> 01:25.710
So basically what it will do is they'll say hey dog here are all the actions you can take.

01:25.880 --> 01:29.050
You can move your legs like this you can move your legs like that.

01:29.060 --> 01:34.240
And your goal is to make a step forward every time you make a step forward.

01:34.250 --> 01:40.790
You are given a reward every time you fall over you're given a punishment and a reward is basically

01:40.790 --> 01:47.300
a one in Dahlgren you don't actually have to give it a carrot or a you know something to eat you just

01:47.330 --> 01:50.810
give it a one room and a punishment is zero.

01:50.900 --> 01:54.970
And basically every time he takes a step forward it knows it's got a reward and it will.

01:55.400 --> 01:56.260
Yes that's good for it.

01:56.260 --> 02:02.210
So basically will try all these random sets of actions and see what they lead to.

02:02.210 --> 02:03.620
Every time it takes a step forward.

02:03.620 --> 02:09.260
Remember that those were good actions and will try to repeat them more and more and actually dogs like

02:09.260 --> 02:11.740
that can learn to walk.

02:11.810 --> 02:17.690
So you don't have to program an actual walking algorithm into it Ill figure out the steps it needs to

02:17.690 --> 02:18.800
take on its own.

02:18.830 --> 02:26.980
I think that's really mind blowing and really cool but unfortunately that is a more a topic more of

02:27.260 --> 02:31.130
on the side of artificial intelligence rather than just machine learning.

02:31.130 --> 02:34.970
And that is you know that can be a whole course on its own.

02:34.970 --> 02:40.880
We're not going to delve into the training robot dogs to walk inside this section inside this section.

02:40.910 --> 02:46.400
We are going to talk about the multi armed bandit problem which is a bit of a different application

02:46.730 --> 02:51.740
of this machine learning branch of reinforcement learning.

02:52.040 --> 02:56.760
And plus of course there's other lots of other applications of reinforcement learning as well.

02:56.900 --> 03:00.830
So moving on to our multi armed bandit problem.

03:00.830 --> 03:05.730
So first of all what on earth is a multi armed bandit right.

03:05.740 --> 03:11.930
So the first thing that comes to mind is like a robber going into a bank and so on and or somebody for

03:11.930 --> 03:19.780
gun but actually a bandit or a one armed band the sort of thing let's simplify things.

03:19.940 --> 03:23.800
A one armed bandit is a slot machine.

03:23.810 --> 03:24.110
Right.

03:24.110 --> 03:25.080
It's one of these.

03:25.130 --> 03:28.070
And why is it called the warm one arm bandit.

03:28.070 --> 03:30.690
Well he's got a bit of a history there.

03:30.710 --> 03:34.390
Back in the day they used to have this handle on the right.

03:34.400 --> 03:39.620
You could still see their movies and maybe some places you can still find these slot machines where

03:39.620 --> 03:43.970
you actually have to pull the handle because now they're all electronic and you just press a button

03:43.970 --> 03:46.460
right there push push push slot machines.

03:46.730 --> 03:57.050
Whereas back in there you had to pull the lever to make it work to like initiate the game.

03:57.170 --> 03:59.970
And so hence the army.

03:59.990 --> 04:01.040
But why is it called the bandit.

04:01.040 --> 04:09.260
Well because these machines they would actually you know these are this is one of the quickest way to

04:09.260 --> 04:12.720
lose your money in a casino.

04:12.770 --> 04:14.360
They would take.

04:14.840 --> 04:21.500
I think it was like a 50 percent chance that they would take away your money back in the day so they

04:21.500 --> 04:25.780
would of course you earn less than your.

04:26.090 --> 04:27.590
You're actually winning.

04:27.590 --> 04:34.700
And it was about you know a 50/50 chance whether or not you actually make a or you get a win or you

04:34.700 --> 04:37.900
lose money but then they put a bug into them.

04:38.050 --> 04:42.890
They read up a little bit online they put a bug into them that people who are playing them were losing

04:42.890 --> 04:48.950
even faster than or even more frequent than 50 percent so hence the name bandit because they're basically

04:48.950 --> 04:50.340
robbing you of your money.

04:50.480 --> 04:56.720
And you know one of the quickest way to ways to lose your money hands the multi That's what's called

04:56.720 --> 04:58.420
the one armed bandit.

04:58.820 --> 05:00.530
And what does a multi-hour bandit.

05:00.530 --> 05:09.050
Well the a multi armed bandit problem is kind of the challenge that a person is faced when he comes

05:09.050 --> 05:15.830
up to a whole set of these machines when he doesn't have just one has like five or ten hour programming

05:15.830 --> 05:21.660
examples will have an example of 10 but we won't be talking specifically about these machines of course

05:21.740 --> 05:28.820
is this is the historic problem you'll see just now we'll see that there are many many other applications

05:29.720 --> 05:36.860
that even though it's called the multi armband problem it's actually used to solve many other problems

05:36.890 --> 05:37.660
as well.

05:38.030 --> 05:42.540
So basically here you're faced with challenge you've got five of these machines right.

05:42.560 --> 05:52.070
And how do you actually play them to maximize your return from the number of games that you can actually

05:52.070 --> 05:52.280
play.

05:52.280 --> 05:57.380
So you've you know you decided you're going to play you know a hundred times or a thousand times and

05:57.380 --> 05:58.760
you want to maximize return.

05:58.760 --> 06:04.010
How did you figure out which ones of them to play in order to maximize your returns.

06:04.010 --> 06:11.990
Well the problem to describe the problem in more detail we've got to mention that's the assumption here

06:11.990 --> 06:20.150
is that each one of these machines has a distribution behind it so there's a distribution of numbers

06:20.280 --> 06:24.730
out of which or outcomes out of which the machine picks results right.

06:24.740 --> 06:31.270
So it has it has like H1-Bs machines has its own distribution and it picks out a result.

06:31.360 --> 06:36.290
You pull the trigger and it just picks all randomly out of its distribution a result an outcome you

06:36.290 --> 06:41.780
know whether you win or whether you lose and how much you win how much use or could you lose the same

06:41.780 --> 06:43.650
money you just put in the coin.

06:43.760 --> 06:49.920
But basically it tells you whether you win or lose based on the distribution that's built into the machine.

06:49.940 --> 06:53.720
But the problem here is that you don't know these distribution right.

06:53.720 --> 06:59.060
You don't know in advance what the distributions are and they are assumed to be different for these

06:59.060 --> 06:59.780
machines.

06:59.780 --> 07:05.020
Sometimes it can be similar or the same in some of the machines but by by default they are different

07:06.070 --> 07:12.820
and your goal is to figure out which of these distributions is the best one for you.

07:12.820 --> 07:14.620
So let's have a look.

07:14.620 --> 07:16.590
So there are these the submissions right.

07:16.600 --> 07:21.110
So for example we've got these five machines the five distributions.

07:21.130 --> 07:26.740
And as you can see right away just by looking at this which is the best machine right away obviously

07:26.950 --> 07:28.130
the one on the right.

07:28.150 --> 07:35.380
The orange one is the best machine because it's got the best you know it's the most left skewed left

07:35.380 --> 07:36.910
skewed because of tails on the left.

07:36.910 --> 07:41.250
So it's got the most favorable outcomes got the highest mean median and mode.

07:41.350 --> 07:48.220
And you if if you knew these distributions and you would obviously just go to the fifth machine and

07:48.220 --> 07:54.250
you would bet on the with machine just on the fifth machine all the time because it's got the best distribution

07:54.250 --> 07:54.460
right.

07:54.460 --> 07:58.940
So on average you would get the best results but you don't know that you don't know that in advance.

07:58.960 --> 08:05.290
And your goal is to figure out you know this is like a it's like a mind game.

08:05.320 --> 08:12.000
You know how there's all these movies about machine learning and really cool almost cool mathematics

08:12.030 --> 08:13.680
on how they're using it.

08:13.680 --> 08:14.360
They're cool.

08:14.380 --> 08:23.350
Really good movie was an imitation game right about Alan Turing and and how he was solving the enigma

08:23.350 --> 08:23.750
and so on.

08:23.770 --> 08:29.260
But the similar kind of concept you don't know which one of these is the best you're going to figure

08:29.260 --> 08:29.520
it out.

08:29.530 --> 08:33.760
But at the same time you are already spending your money doing this right.

08:33.760 --> 08:39.240
You can just you know the longer you take to figure it out there is a tradeoff right.

08:39.250 --> 08:46.480
The longer you take to figure it out the more money you will probably spend on the wrong ones.

08:46.930 --> 08:49.480
And therefore you have to figure out very quickly.

08:49.480 --> 08:53.120
So there are these two factors that are in play exploration and exploitation.

08:53.140 --> 08:58.580
So you need to explore the machines to find out which one of them is the best one.

08:58.930 --> 09:05.680
And at the same time you need to as soon as you can already start exploiting exploiting these machines

09:05.680 --> 09:09.000
exploiting your findings to make the maximum return.

09:09.310 --> 09:15.640
So basically and there's another methodical concept behind all of this which is called regret and regret

09:15.640 --> 09:17.860
is that it is mathematically defined.

09:17.860 --> 09:21.160
And if you want read more about this is a goal is a really good white paper on it.

09:21.160 --> 09:30.190
It's called using confidence bounds for exploitation and exploration are tradeoffs and it is by Pyrrha

09:30.790 --> 09:38.340
a lawyer or a PR from Graz University of Technology in Austria really like the white paper.

09:38.410 --> 09:43.540
It goes into a lot of detail like I didn't you read the whole thing but the first couple of chapters

09:43.540 --> 09:44.310
are pretty good.

09:44.590 --> 09:52.290
If you want to go into detail but basically regrets is is when you suffered when you're using an unknown

09:52.320 --> 09:54.520
utterance alternative and not optimal method.

09:54.520 --> 09:54.760
Right.

09:54.760 --> 10:01.660
So the one on the right is the optimal or the one on the right the optimal machine whenever using the

10:01.660 --> 10:09.130
non-optimum machine you have a regret which can be quantified as like as the difference between the

10:10.060 --> 10:16.520
best outcome and the best outcome and you know all of those sums of the money that you put like your

10:17.560 --> 10:20.690
opportunity cost of actually exploring other machines.

10:20.820 --> 10:25.500
And so the longer you explore there are non-optimal machines the higher regret.

10:25.510 --> 10:29.640
But at the same time if you don't explore for long enough right.

10:29.680 --> 10:37.480
If you explore if you don't explore for long enough then your and a suboptimal machine might might appear

10:37.480 --> 10:38.310
as an optimal machine.

10:38.310 --> 10:44.890
So for instance this machine here writes If we explore explore explore but we don't spend enough time

10:44.890 --> 10:49.600
exploring we might think that this is the best machine because it's got quite a good return right close

10:49.600 --> 10:53.650
to this one and we might start exploiting this one for the rest of the time.

10:53.740 --> 10:56.390
But in reality this one was the best one.

10:56.410 --> 11:05.620
So the goal is to find the best one and exploit the best one but spend the least amount of time exploring

11:05.650 --> 11:06.500
all of them.

11:06.550 --> 11:10.680
And while you're exploring you're still earning money but not from the optimal machinery.

11:10.750 --> 11:12.130
So that's the goal.

11:12.130 --> 11:18.270
That's the point of this whole exercise and it's borne to understand here that there is the best one.

11:18.280 --> 11:25.860
So there is where even though these machines you know they have like jackpots sometimes and so on.

11:25.870 --> 11:32.920
But we are assuming that there's just way that these distributions are finite there and out of them

11:32.980 --> 11:35.070
there is a best one that you are looking for.

11:35.070 --> 11:42.550
That's kind of the pre emphasis or the whole assumption on this problem if they are more complex options

11:42.550 --> 11:43.710
and versions of this problem.

11:43.720 --> 11:49.640
And again check out some additional reading on that topic.

11:49.640 --> 11:54.680
That's more even more advanced but what we're going to be using this for is that's going to be sufficient.

11:54.730 --> 11:56.580
And why is it going to be sufficient for us.

11:56.740 --> 12:03.370
Because the most common modern application of this that we can think of is the one that we're going

12:03.370 --> 12:07.100
to be exploring is advertising voila.

12:07.110 --> 12:09.790
So let's have a look at some ads going to be fun.

12:09.870 --> 12:13.530
So just a disclaimer this there is no affiliation of Coca-Cola.

12:13.530 --> 12:15.900
Examples are used just for educational purposes.

12:15.900 --> 12:17.720
All right so let's have a look.

12:17.880 --> 12:25.890
We have let's say Coca-Cola or some company wants to run a campaign and it's going to be called Welcome

12:25.890 --> 12:27.970
to the Coke Side of Life campaign.

12:28.230 --> 12:32.460
And if you search those campaign online you'll see that they had you know hundreds of different ads

12:32.770 --> 12:35.330
that they came up with for this campaign.

12:35.470 --> 12:37.720
And here's here's one example them.

12:37.740 --> 12:42.570
These are just some images I pulled from Google so maybe these are even drawn by people but we're going

12:42.570 --> 12:48.450
to assume that these are legitimate ads that we're going to go into the campaign as well we want to

12:48.450 --> 12:51.300
find out which is the best ad which is the ad that it works.

12:51.300 --> 12:52.170
So we've got options.

12:52.170 --> 12:53.180
Number one.

12:53.220 --> 12:55.490
Number two three.

12:55.490 --> 12:56.460
Number four.

12:56.460 --> 12:57.280
Number five.

12:57.510 --> 13:03.750
And so now our goal is to find out which ad works to maximize our returns.

13:03.750 --> 13:05.890
But right now we don't know which has the best right.

13:05.910 --> 13:13.590
So there's no there is a distribution behind it but that distribution will only become known after thousands

13:13.590 --> 13:15.720
and thousands and thousands of people.

13:15.810 --> 13:18.300
Look at these ads and click or not click on these ads.

13:18.300 --> 13:21.870
And this is actually going to be very similar to the example that we're going to be looking at.

13:21.870 --> 13:26.700
The example that had LUNs going to be walking you through in the programming tutorials in that example

13:26.720 --> 13:27.810
actually did have 10 ads.

13:27.800 --> 13:29.160
So even more.

13:29.170 --> 13:31.160
And so what can you do here.

13:31.170 --> 13:34.490
Well one way to approach the problem is just run an AB test right.

13:34.500 --> 13:44.730
So take your five or 50 or 500 ads and run a huge AB test with multiple AB test and wait until you have

13:44.730 --> 13:51.350
enough of a large enough sample and then conclude which side is the best right.

13:51.650 --> 13:53.250
With certain confidence.

13:53.250 --> 13:59.690
But the problem of that is that you would spend a lot of time and money doing that right.

13:59.730 --> 14:02.780
So an AB test is pure exploration right.

14:02.790 --> 14:08.220
You're not exploiting the best option you are exploiting the best option but to the same extent as you're

14:08.220 --> 14:10.400
exploiting the non optimal options right.

14:10.400 --> 14:16.130
So if if we go by our previous distributions if this is the best one if you just run an AB test then

14:16.260 --> 14:23.390
you are uniformly distributing or you uniformly using these 5 options and therefore as much as using

14:23.400 --> 14:27.480
this when you you're using all four all four of them so basically all five of them.

14:27.480 --> 14:35.160
So basically you are exploiting it a bit but on unconsciously you arrive in a random way and therefore

14:35.340 --> 14:38.410
AB tests are just for exploration.

14:38.410 --> 14:46.070
So the challenge is to find out which is the best one but do it while you're exploring.

14:46.120 --> 14:52.940
While you're not to exploit the best one while you're exploring for it right so find out which of them

14:52.940 --> 14:57.160
is the best one in the process of.

14:57.810 --> 14:59.450
Hold on fire.

14:59.450 --> 15:03.150
Find out which is the best one in the process of the actual launch campaign.

15:03.160 --> 15:04.970
So not don't have two phases.

15:04.960 --> 15:07.060
You do the AB test and then use them.

15:07.080 --> 15:13.230
The best one but actually find out the best one in the quickest way possible and start exploiting it

15:13.260 --> 15:14.140
along the way.

15:14.220 --> 15:17.710
So that's the challenge here and that's what we're going to be solving.

15:17.880 --> 15:22.540
And that's the modern application of the multi armed bandit problem.

15:22.740 --> 15:24.510
So hopefully you're excited about this.

15:24.510 --> 15:27.570
We've got two great algorithms coming up.

15:27.570 --> 15:29.160
Can't wait to get started.

15:29.160 --> 15:30.950
I look for you in the next tutorial.

15:30.990 --> 15:33.370
And until then enjoy machine learning.
