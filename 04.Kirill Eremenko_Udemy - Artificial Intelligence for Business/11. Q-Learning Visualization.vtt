WEBVTT

00:00.570 --> 00:03.960
Hello and welcome back to the course on artificial intelligence.

00:03.960 --> 00:05.890
In today's tutorial we're going to have some fun.

00:05.910 --> 00:11.850
We're going to have a look and artificial intelligence actually going through that maze that we've been

00:11.850 --> 00:18.360
talking about so long and is going to use kill learning to navigate its way and find the way out.

00:18.360 --> 00:24.300
And we'll see what happens to the q values was going to happen to the policy and so on.

00:24.300 --> 00:26.280
So let's have a look.

00:26.280 --> 00:31.860
We're going to be using some materials kindly provided by the Berkeley University.

00:31.860 --> 00:41.280
So if you go to a dot Burke Lee the R K E L E Why don't you just go to that link or you'll see this

00:41.280 --> 00:48.600
Web site and hear what we're going to be looking at is the need to go to we're going to go to PacMan

00:48.600 --> 00:49.070
projects.

00:49.070 --> 00:57.980
I think Pacman projects and here if you scroll down and you look at have been forced learning this is

00:57.980 --> 00:59.130
what we're working with.

00:59.150 --> 01:03.520
So here you can download the zip archive So that's if you want to.

01:03.540 --> 01:08.270
So you don't have to this is we're not going to go through the solution together in this trial just

01:08.270 --> 01:13.880
letting you know where this is all from because we're very like we really appreciate that UC Berkeley

01:14.210 --> 01:16.120
has made these materials available.

01:16.130 --> 01:19.230
But if you do wish to experiment with this on your own.

01:19.340 --> 01:20.630
Just bear in mind this is not part.

01:20.670 --> 01:23.230
It's going to be part of our courses as part of the Berkeley course.

01:23.270 --> 01:24.950
I'm going sure how it works.

01:24.950 --> 01:28.620
For illustration purposes but if you do want to experiment with this you can find it here.

01:28.820 --> 01:31.130
The zip archive and all the instructions as well.

01:31.370 --> 01:34.900
And we're just going to go into Python right away.

01:35.050 --> 01:42.230
And first thing I want to show you is that here we've got the licensing information so this is what

01:42.230 --> 01:42.730
I mean.

01:42.830 --> 01:47.660
We're very lucky that they said we are free to use or extend these projects for educational purposes

01:47.660 --> 01:51.040
provided you know distribution publish solutions which we're not going to do.

01:51.140 --> 01:56.720
You retain this notice which we have and you provide a clear archbishop to UC Berkeley including a link

01:56.720 --> 01:57.800
to which we also have.

01:57.800 --> 02:00.740
So once again if you'd like to learn more there is a link.

02:00.740 --> 02:06.170
You can have a look and thank you very much all these people who have worked on this project so here's

02:06.650 --> 02:09.350
the great world there we're going to be working if there's a solution there.

02:09.440 --> 02:15.060
You would have to in order to make it work you'd have to solve it yourself or possibly find a solution.

02:15.050 --> 02:18.910
Maybe some of your some people somebody you know might help you out with that.

02:19.100 --> 02:25.100
If again you want to you don't have to because we're just going to look at it on this screen right now.

02:25.100 --> 02:29.430
So after we've created all those files we could just want you over here.

02:29.660 --> 02:36.650
So there are some parameters that are involved in this whole world and we're not going to just show

02:36.650 --> 02:39.030
you what it looks like if we want it.

02:39.020 --> 02:41.510
So let's try to launch it in manual mode.

02:41.510 --> 02:47.020
So if I go minus one of these panoramas your manual so I can command the control agent.

02:47.030 --> 02:53.330
So here you can see all grid so I can go up up so you can see that it's taking action starting and started

02:53.330 --> 02:54.940
in states where I was.

02:55.070 --> 03:00.620
And then you saw that I pressed up took action Norf and first time I ended up in zero once I did go

03:00.620 --> 03:01.010
up.

03:01.430 --> 03:04.990
But second time I took action Norf and I ended in the same sad didn't move.

03:04.990 --> 03:08.380
So something happened you know the randomness happened I either went left or right.

03:08.720 --> 03:10.850
And by default the parameters are set.

03:10.850 --> 03:16.850
You can see here by default they're set to exactly what we discussed that that how often action results

03:16.880 --> 03:18.220
in unintended direction.

03:18.230 --> 03:21.100
Twenty percent of the time to 10 percent to the left since the right.

03:21.170 --> 03:23.440
So if I go up and say I went up I go right.

03:23.440 --> 03:25.040
I went right right.

03:25.100 --> 03:26.540
Now didn't happen.

03:26.750 --> 03:29.120
Right again and right.

03:29.120 --> 03:34.290
And I'm finished but in this implementation you have to click again to get out of this final outputs

03:34.290 --> 03:35.520
are you out of there.

03:35.570 --> 03:36.980
Just like again and you're finished.

03:37.130 --> 03:40.680
That's a terminal state so we can run our manual.

03:40.700 --> 03:45.580
You can see that if I go right right right left up.

03:45.710 --> 03:50.000
So here what we saw previously that the agent wouldn't go straight up right.

03:50.000 --> 03:53.260
What's the point of getting up if there's a chance of going into the pit.

03:53.260 --> 03:54.500
So let's see what the agent would do.

03:54.520 --> 03:58.930
It would go left and go west here so go west and you see I clicked left but it went up.

03:59.180 --> 04:04.250
And here I would click right and I end up in that final exit say and you actually got a reward equal

04:04.250 --> 04:05.330
to one.

04:05.330 --> 04:07.120
So that's what it looks like manually.

04:07.130 --> 04:12.450
Now let's actually hook up an AI to this and let it go through.

04:12.440 --> 04:16.790
So let's do an H here and let's add some Brandner.

04:16.790 --> 04:22.510
So let me just see what I typed here so hopefully you can see PI's and grid world.

04:22.630 --> 04:25.280
Why then here minus our means.

04:25.310 --> 04:27.920
That's the reward for living.

04:27.920 --> 04:31.720
So I've got two of them so I probably should remove this one.

04:32.060 --> 04:35.000
So minus k is how many iterations.

04:35.000 --> 04:36.630
That's way too many iterations.

04:36.630 --> 04:39.890
Let's do less Let's do like 10 iterations.

04:39.890 --> 04:40.770
That should be enough.

04:41.120 --> 04:42.640
Minus a is Agent.

04:42.650 --> 04:46.970
What type of agent don't want to do honor and damage and some value agent or Q.

04:47.030 --> 04:48.380
Q So I want to.

04:48.380 --> 04:57.050
Q q learning agent doing this minus s is what is s speed so that's way too large a force that just used

04:57.050 --> 05:04.760
the full speed for now minus our is living penalty's so by default is 0.

05:04.760 --> 05:10.390
So remember at the very start we started 0 living penances so let's call it also 0 0.

05:10.400 --> 05:11.780
Can just remove this parameter.

05:12.020 --> 05:15.980
And d is what is the discount.

05:15.980 --> 05:18.190
So I just kind of factor so let's keep it at zero point.

05:18.220 --> 05:23.030
And so very similar to what we are starting off in this section of the course so let's run that.

05:23.510 --> 05:25.430
OK way too fast again.

05:26.470 --> 05:27.430
Oh actually so.

05:27.430 --> 05:30.520
OK so you can see how he's exploring.

05:30.520 --> 05:35.470
And so so far he's hit negative three times and you can see how the cube values are being updated in

05:35.470 --> 05:36.630
these squares.

05:36.640 --> 05:39.090
So these are cube they sort of 0.

05:39.170 --> 05:40.770
You can see now the cube values.

05:40.830 --> 05:41.920
He's learned that.

05:42.060 --> 05:45.490
Again this one is a bit different the implemented because once you get to the final stage you have to

05:45.490 --> 05:46.290
get out of it.

05:46.600 --> 05:48.930
You have to just click one more button to exit.

05:48.940 --> 05:51.430
And so it's very close to one but not exactly one.

05:51.700 --> 05:57.600
But at the same time you can see that here you know the value slowly kind of crystallizing and zero

05:57.620 --> 06:03.250
point a slowly getting somewhere but so far they're kind of zeroes because he doesn't have enough information

06:03.250 --> 06:04.910
to understand what's going on.

06:05.420 --> 06:05.670
OK.

06:05.690 --> 06:08.670
So let's see let's see what happens here.

06:10.130 --> 06:13.600
Exploring exploring exploring what's going to happen.

06:13.730 --> 06:15.340
Well it's been a while.

06:15.620 --> 06:17.730
And don't forget there's some randomness involved here.

06:18.050 --> 06:20.060
So there is that good one a few times.

06:20.060 --> 06:22.460
Now he only gets 10 iterations.

06:22.460 --> 06:24.050
So he's got to learn fast.

06:25.260 --> 06:26.710
OK I need you there.

06:27.100 --> 06:29.230
Let's see what's going on.

06:29.260 --> 06:30.000
Come on.

06:30.000 --> 06:32.820
Get out of that maze already.

06:32.910 --> 06:40.370
And yes 10 episodes so average it turns out that that's not really interested in that.

06:40.410 --> 06:41.710
So here let's see.

06:41.710 --> 06:43.040
I've never seen enough of a click.

06:43.050 --> 06:43.390
Right.

06:43.390 --> 06:43.770
There we go.

06:43.770 --> 06:47.780
So you can see this is the policy that he came up with.

06:47.940 --> 06:50.560
Even through just 10 episodes he's already got a pulse.

06:50.570 --> 06:50.870
OK.

06:50.880 --> 06:55.770
We're going to go up a bump and here I'm going to go down here I'm going to go down here I'm going to

06:55.770 --> 06:58.250
go into the wall and then I'm going to bounce and we're here.

06:58.500 --> 06:59.540
That's pretty cool.

06:59.960 --> 07:00.200
OK.

07:00.210 --> 07:02.270
So now let's increase the speed.

07:02.580 --> 07:04.160
What was a parameter s there.

07:04.170 --> 07:06.190
And it's like double lawlessness.

07:06.210 --> 07:13.020
That's quadruple the speed and that's increase the number of iterations so let's say 20 to ration this

07:13.030 --> 07:16.390
time and let's see if he can get through a bit more now.

07:16.740 --> 07:18.650
So you can see he's going a bit faster.

07:19.540 --> 07:25.840
And he's learning he's learning that it's not really you know out of this state there's not many good

07:25.840 --> 07:30.200
actions or you know these actions that the right and straight are not that good.

07:30.220 --> 07:32.360
Definitely this was definitely not good.

07:32.380 --> 07:34.630
He still needs to learn that so from here is also good.

07:34.640 --> 07:36.790
You can see that this action is pretty good.

07:36.790 --> 07:37.250
All right.

07:37.270 --> 07:38.470
What did he get.

07:38.470 --> 07:42.140
OK so interesting policy here you really decide to go up.

07:42.280 --> 07:51.330
Just not enough information so let's let's do that and let's increase the speed to like 100 super fast

07:51.360 --> 07:57.690
and the number of iterations will give him 100 iterations this time it's run that seems like crazy fast

07:58.110 --> 08:01.170
and you can see that because there's so many more iterations.

08:01.170 --> 08:07.380
He's got more information more opportunity to experiment and to actually build out this this matrix

08:07.380 --> 08:10.190
or matrix these values for every single state.

08:10.200 --> 08:13.170
He now knows OK you can see that zero point eighty nine.

08:13.200 --> 08:15.970
Where did we say in our zero point 86.

08:16.050 --> 08:20.670
Another thing to remember is that the value of any given state.

08:20.670 --> 08:24.180
Remember that formula we had is the maximum of the q values.

08:24.180 --> 08:27.090
Remember that thing that we came up with short cut for.

08:27.120 --> 08:30.660
So what is what with the value in this state be the V of this state.

08:30.870 --> 08:36.960
It would be 0.18 because that's the highest out of the four here the value of this state 0.7 you want

08:37.080 --> 08:40.320
the value of this day is there is point sixty one and so on.

08:40.320 --> 08:41.440
So that's something to remember.

08:41.450 --> 08:45.550
I remember when I was up I think we had like zero point eighty six or something so praecox.

08:45.720 --> 08:55.000
And so if we go next year I'll just disappear or disappear again and this could make it come back.

08:55.110 --> 08:55.700
OK.

08:55.710 --> 08:56.160
OK.

08:56.160 --> 09:00.600
Slowly slowly slowly filling up some spaces.

09:00.940 --> 09:01.400
I see.

09:01.440 --> 09:06.120
And it's also pretty random because not only the environment has randomness but also the way he explores

09:06.120 --> 09:10.770
that the star really doesn't know the policy is he's exploring at random.

09:11.140 --> 09:12.110
Just keeps disappearing.

09:12.120 --> 09:13.320
I don't understand why.

09:13.620 --> 09:18.760
Anyways we'll see what happens if we increase the number here and here should pretty much take the same

09:18.760 --> 09:23.020
amount of time if the speed doesn't have a cap on it.

09:23.410 --> 09:27.550
OK so he's like he has more opportunity to explore things.

09:27.630 --> 09:30.790
OK let's see how it all goes.

09:31.210 --> 09:34.960
And you can see the values are converging they go up and down depending you know because there is some

09:34.960 --> 09:38.470
randomness and he might end up like in the pit even though he goes like this way.

09:38.590 --> 09:44.830
But at the same time they're slowly starting to converge to some sort of values and q values.

09:44.910 --> 09:48.490
OK probably a thousand is a bit too much in terms of time.

09:48.500 --> 09:53.210
It doesn't look like the speed is proportionally increasing as well.

09:53.560 --> 09:55.530
So it might cut that part.

09:55.600 --> 10:02.000
I mean like Regis's speed while this is very long you don't have to watch to the end of this tutorial

10:02.010 --> 10:03.340
I just want to experiment with quite a bit.

10:03.340 --> 10:09.400
So to give you some like examples of what we've been working through but you get the point that it goes

10:09.400 --> 10:14.750
through all of this it has some randomness like Rambler's built into his behavior.

10:14.760 --> 10:20.670
So even when it has like a policy it will still continue exploring so it won't just like once it has

10:20.670 --> 10:23.380
a basic policy it won't just continue following its policy.

10:23.410 --> 10:29.080
It will still experiment with other variations once in a while in order to enhance its policy maybe

10:29.080 --> 10:31.270
it hasn't found the best policy already right away.

10:31.270 --> 10:37.840
Maybe it can improve the policy and that's why even after so many iterations you can still see some

10:37.930 --> 10:43.090
random effects is sometimes jumps in to random states not just because of the randomness in the environment

10:43.450 --> 10:48.880
but also because there is some level like a parameter which you could control which you could set up

10:48.880 --> 10:54.880
for your agent saying that you know most of the time 80 percent of the time do whatever your policy

10:54.880 --> 10:59.170
tells you to do but 20 percent of the time you just have some fun experiment and see what happens and

10:59.650 --> 11:03.330
use that information that you gather to update your policy.

11:03.360 --> 11:05.260
OK this is taking way too long.

11:05.260 --> 11:06.290
Lets try that again.

11:06.480 --> 11:11.590
Yeah so thats how the agent learns in different states.

11:11.590 --> 11:14.230
Maybe its just one one more just out of curiosity.

11:14.230 --> 11:16.540
So is there anything else we can change about it.

11:18.370 --> 11:20.060
Iterations.

11:21.600 --> 11:22.930
Okay okay.

11:22.960 --> 11:24.080
Have a look.

11:24.520 --> 11:26.620
Yeah well we could change the discussion for example.

11:26.630 --> 11:39.860
So in this case we could say K minus 100 minus a q minus two and minus are OK two thousand.

11:39.860 --> 11:41.290
So reward.

11:41.330 --> 11:45.840
We want to keep it maybe let's keep it at 0.04 But let's say set again.

11:46.040 --> 11:49.220
Let's keep the reward at zero point zero for every time.

11:49.220 --> 11:58.290
And then here we're going to say that the discount is not zero point nine but it's like zero point point

11:58.290 --> 11:58.980
five.

11:59.010 --> 12:02.240
So it gets discounted quite a lot as you go through the game.

12:02.550 --> 12:08.910
So it actually now it'll be incentivized to be closer to the finish rather than further route the states

12:08.910 --> 12:12.810
close to finish will get high value so you can see that the values really drops off.

12:12.810 --> 12:15.340
It's not as as green as it was before.

12:16.330 --> 12:20.090
So here you can see that this is the policy now.

12:20.320 --> 12:26.020
So it goes like that like that like I like that very similar to what we saw before just probably only

12:26.020 --> 12:28.760
differences from here jumping straight into here.

12:28.780 --> 12:29.900
So that's one.

12:29.950 --> 12:32.440
And OK let's just run one more.

12:32.440 --> 12:33.350
This is so much fun.

12:33.520 --> 12:34.490
Let's just run one more.

12:34.490 --> 12:45.070
So K minus a k a 100 A Q discount keep it as it was original so like just run this basic vanilla set

12:45.110 --> 12:48.890
up ok ok ok.

12:49.060 --> 12:51.060
It's going to see if it will show us the policy.

12:51.160 --> 12:54.770
And yes we got the policy.

12:54.790 --> 12:55.120
Yes.

12:55.120 --> 12:56.030
Good finish.

12:56.320 --> 12:58.870
So here we've got the policy.

12:58.880 --> 12:59.770
You know this is familiar.

12:59.770 --> 13:02.730
Remember that time when we saw that the AI outsmarted the human.

13:02.740 --> 13:08.500
When boom into the wall to go there and boom into the wall to go like that to increase that problem.

13:08.500 --> 13:09.210
So there we go.

13:09.220 --> 13:13.770
That is an example of artificial intelligence in action.

13:13.780 --> 13:18.270
Very very basic simple kill earnings so no deep learning at this stage.

13:18.550 --> 13:21.840
But at the same time it's already pretty smart.

13:22.060 --> 13:27.100
And I hope you enjoyed tonight's the Tauriel and once again thank you to UC Berkeley and I hope you

13:27.100 --> 13:29.590
enjoyed today's tutorial and I look forward to seeing you next time.

13:29.590 --> 13:30.840
Until then enjoy.

13:30.840 --> 13:31.100
I.
