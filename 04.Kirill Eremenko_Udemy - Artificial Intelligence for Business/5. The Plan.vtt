WEBVTT

00:00.880 --> 00:03.810
Hello and welcome back to the course on artificial intelligence.

00:03.940 --> 00:08.470
So we've talked about the Belman equation and we've analyzed our little maze.

00:08.470 --> 00:12.340
Let's have a look at the plan what is the plan.

00:12.700 --> 00:14.590
Well here is our main analysis.

00:14.620 --> 00:19.480
And we know that we can see actually the states the values of each state.

00:19.480 --> 00:26.410
We can see what the value of being in every single state is and therefore the AI can or the agent can

00:26.410 --> 00:27.780
navigate this maze.

00:27.790 --> 00:28.750
So what is the plan.

00:28.750 --> 00:35.590
Well the plan is simply like a treasure map for artificial intelligence instead of looking at these

00:35.680 --> 00:41.380
values that just replace them move arrows which indicate in which direction the agent should go.

00:41.440 --> 00:43.300
Because of those because it knows those values.

00:43.300 --> 00:47.180
So an ideal scenario after it's explored this environment.

00:47.200 --> 00:51.640
It knows the values of being in each state and therefore it can come up with this map so let's have

00:51.640 --> 00:52.280
a look again.

00:52.330 --> 00:58.360
We know that here values one so if you are here out of the two the better one is this Once you go right

00:58.780 --> 01:01.920
from here out of the two this one is a better one this was a better one.

01:01.930 --> 01:02.700
This one is a better one.

01:02.710 --> 01:04.690
Or actually from here you have two options right.

01:04.710 --> 01:11.320
So he kind of like a tie so just pick one at random doesn't matter which one because the value in these

01:11.380 --> 01:16.540
in either case is the same and more so even if you look through it will take the same amount of steps

01:16.750 --> 01:18.350
same number of steps to get to the end.

01:18.640 --> 01:22.480
From here you've got three options but this one is the better value from here.

01:22.480 --> 01:24.290
This one is a better value from here.

01:24.310 --> 01:28.690
Obviously this was a better value because you're you know you just get it minus 1 reward right away

01:29.380 --> 01:34.600
and from here you have like three actually so but this one is the best one of the best value of the

01:34.590 --> 01:35.200
state.

01:35.350 --> 01:41.140
And so therefore if we replace them with arrows it makes sense that this is how the agent would go if

01:41.140 --> 01:44.520
it stars here or solve for some reason it ends up in this square.

01:44.530 --> 01:48.210
It knows how to get out of here and the stars in this square knows how to get on here.

01:48.490 --> 01:51.370
And so and so that is what a plan is.

01:51.370 --> 01:56.700
And don't confuse plan with policy because we're going to be talking about policies for them on forces

01:56.700 --> 01:58.010
of very similar plans.

01:58.180 --> 02:02.360
But they have a little trick to them because the environment's going to be a bit different.

02:02.380 --> 02:07.590
It's going to be stochastic and that's what we're going to talk about in the next tutorial.

02:07.890 --> 02:09.970
So a call awaits you on the next one.

02:09.970 --> 02:12.020
And until then enjoy I.
