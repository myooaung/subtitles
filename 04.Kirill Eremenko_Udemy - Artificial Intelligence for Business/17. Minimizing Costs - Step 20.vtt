WEBVTT

00:00.750 --> 00:06.360
OK so now that the full architecture of the neural network is clearly defined you know with the input

00:06.360 --> 00:11.730
layer the first layer of the 64 neurons and the second hidden layer of the 32 neurons.

00:11.850 --> 00:15.460
And finally the output layer composed of the five possible actions.

00:15.670 --> 00:22.650
Well we have to assemble everything and to do that we're going to create an object of the model class

00:22.980 --> 00:24.810
which will take two arguments.

00:24.810 --> 00:28.010
The first one will be the input which will be our states here.

00:28.080 --> 00:33.660
And the second one will be the output which will be our Q values and that's only what we have to do

00:33.930 --> 00:41.250
to assemble everything since indeed the q values here are the result from its full connection with the

00:41.250 --> 00:42.520
previous hidden layer.

00:42.530 --> 00:44.480
Why that was created here.

00:44.610 --> 00:52.200
Which itself is the result from the full connection with the first layer X and eventually same since

00:52.260 --> 00:57.110
x is the result of a full connection with the states.

00:57.110 --> 01:03.860
No our input states which are going to be the first input of this middle class well this will assemble

01:03.960 --> 01:06.930
everything inside this model object.

01:06.930 --> 01:12.190
And speaking of this model object that's exactly our next step here to create it.

01:12.240 --> 01:17.220
And so we're going to introduce a new variable here that we're going to call Norrell and that will exactly

01:17.460 --> 01:25.860
be this model itself but be careful since now we're about to get the final not all that you know will

01:25.860 --> 01:28.400
be the brain itself the artificial brain.

01:28.590 --> 01:36.300
Well of course this model has to be an object variable and that's why I'm using our model here as an

01:36.540 --> 01:38.040
object variable.

01:38.040 --> 01:45.540
That is a variable of our future brain's object which will be created afterwards for the training actually

01:45.540 --> 01:47.130
will only create one.

01:47.190 --> 01:51.430
But if we're doing the same for another server we can create another brain.

01:51.720 --> 01:53.880
That's what is important to understand.

01:53.880 --> 01:58.240
Know what we only want to use later is dismal itself.

01:58.290 --> 02:05.490
And these were just to create the final architecture of this model and therefore we didn't have to introduce

02:05.490 --> 02:07.950
these variables as Object Variables.

02:07.950 --> 02:13.000
So in other words these are just some local variables only used here to build a model.

02:13.230 --> 02:20.310
OK so solve that model which will be an object of the model class which will take as arguments as we

02:20.310 --> 02:20.900
said.

02:21.060 --> 02:29.980
First the inputs which will be of course are states and the outputs which as we said arguing to be the

02:29.980 --> 02:40.830
q values and perfect that assembles everything are right now next and final step indeed after we assemble

02:40.830 --> 02:45.630
everything we have to compel the model with two essential elements.

02:45.630 --> 02:52.010
The first one is the last function that will compute the error between the predictions that is the cube

02:52.010 --> 02:59.660
values and the toilet which will compute in the deep learning process in that specific document byte

02:59.680 --> 03:00.590
and foul.

03:00.780 --> 03:04.540
And then the second essential element will be of course.

03:04.620 --> 03:08.440
Well the only thing missing here is that we haven't used yet.

03:08.490 --> 03:13.360
I'm talking of course about the optimizer which will be an atom optimizer.

03:13.770 --> 03:19.440
That's the only remaining two things we have to do and to do this well we're going to take our model

03:19.440 --> 03:21.000
that we just created.

03:21.000 --> 03:27.660
And in fact from this model last you know since middle class it has some method and one of these methods

03:27.660 --> 03:35.880
is called compile and it is a method that will connect your model to the last function and an optimizer

03:36.400 --> 03:41.360
and to get that method we just need to add a dot here and then compile.

03:41.490 --> 03:47.820
And since this compound that will just connect your model to the last function and the optimizer and

03:47.820 --> 03:49.290
will not return anything.

03:49.440 --> 03:52.810
Well that's why we don't have to create a new variable here.

03:52.890 --> 03:55.200
As a result of what's going to happen here.

03:55.200 --> 04:00.770
No we were just doing this to attach to our model a last function in an atom optimizer.

04:00.840 --> 04:07.980
And so let's start with the last function for which the argument name is less and important thing to

04:07.980 --> 04:08.810
understand here.

04:08.850 --> 04:12.990
What is going to be the last to figure out what it is we need to understand.

04:12.990 --> 04:15.770
What are new and that work will return.

04:15.810 --> 04:21.960
Remember it will return the q values for each of the five possible actions here it will not return these

04:21.990 --> 04:23.460
indexes themselves.

04:23.490 --> 04:29.910
Instead it will return their q values and their q values are actually float numbers and therefore what

04:29.910 --> 04:37.400
we're doing here with this neural network is not classification you know returning a category or a class.

04:37.620 --> 04:44.700
We're doing regression because we are returning a continuous numerical outcome and for that reason the

04:44.700 --> 04:52.140
best laws that we have to use here and connect to our model is called the mean square error and very

04:52.140 --> 04:58.710
simply it will be the mean of the squares of the differences between the predictions and the target

04:58.930 --> 05:04.640
and the targets will be that in our next degree when implementation fall where we will implement the

05:04.640 --> 05:07.730
deep learning process an algorithm.

05:07.960 --> 05:09.000
But here's the deal.

05:09.070 --> 05:15.260
Since we are predicting some continuous outcomes because the queue values our float numbers Well we

05:15.260 --> 05:23.200
are dealing with regression and therefore we have to use a mean square errorless which has the key identifier.

05:23.200 --> 05:27.420
MSE All right so this is all part of the Cara's API.

05:27.610 --> 05:32.680
And this is the most classic but very effective to use for regression.

05:33.010 --> 05:34.940
OK and then that's it for the last.

05:34.960 --> 05:39.340
Now the optimizer which will be an atom optimizer.

05:39.490 --> 05:44.620
So there is a great section in the carries the condition that explains and gives more details and what

05:44.620 --> 05:46.110
is at an optimizer.

05:46.270 --> 05:51.190
What's most important to understand is the role of this optimizer that is what it will do.

05:51.400 --> 05:56.970
Well what it will do is that it will back propagate this last area between the operations and the target

05:57.280 --> 06:01.870
back into the neural network you know through back propagation from right to left.

06:01.870 --> 06:07.600
So after we get the predictions we generate the last area between the operations and target the last

06:07.600 --> 06:14.200
error is back propagated through the neural network and then the atom optimizer will update each of

06:14.200 --> 06:19.040
these wait through stochastic grid in the sense to reduce the loss.

06:19.120 --> 06:24.770
And therefore since reducing the loss implies some predictions closer to the target.

06:24.970 --> 06:31.180
This will improve the predictions and iteration by iteration our AI will play better and better actions

06:31.180 --> 06:35.640
and therefore doing some better and better regulations of the surface temperature.

06:35.890 --> 06:37.680
So that's how it works in a few words.

06:37.870 --> 06:44.010
And that's the role of the optimizer and to create this atom optimizer.

06:44.130 --> 06:48.630
Well as we said we going to created as an object of the Atom class.

06:48.760 --> 06:51.040
And so that's why here I'm taking the Atom class.

06:51.100 --> 06:57.370
And since this is a class it's going to take some arguments here just one Can you guess what it is.

06:57.580 --> 07:02.710
Well there is one thing we haven't used yet is of course the learning rate which again would be the

07:02.710 --> 07:07.210
speed at which your model will learn how to perform better and better actions.

07:07.360 --> 07:12.670
So the higher the learning rate the higher we'll be updated the ways by you know this at an optimizer

07:12.970 --> 07:14.670
and therefore the faster it will learn.

07:14.670 --> 07:16.330
But maybe not the best way.

07:16.480 --> 07:21.910
And the lowers the learning rate the lower will be the update of the weight by this Annam optimizer

07:22.360 --> 07:24.390
slower this model is going to learn.

07:24.640 --> 07:28.590
And so good value here is this one open or one.

07:28.750 --> 07:33.130
Again you can choose that learning parameter and test some other values.

07:33.130 --> 07:36.610
But for our specific case study this will work very well.

07:36.610 --> 07:40.820
So we're going to improve that single argument year.

07:40.960 --> 07:46.660
And the name of the argument the name of the parameter in this Atom class for the learning right is

07:47.050 --> 07:55.430
an R and so r will be cool to our learning rate entered as the argument here and perfect.

07:55.420 --> 08:01.360
We're done with this brain class creating our artificial brain.

08:01.360 --> 08:02.680
Congratulations.

08:02.680 --> 08:08.920
Basically you just built this and I'm sure now you know how to create any other architectures you know

08:08.920 --> 08:11.080
if you want to add a third layer.

08:11.170 --> 08:17.200
You have to make a third full connection here by creating that variable before you link them to the

08:17.210 --> 08:19.860
final output containing the values.

08:20.110 --> 08:24.170
And then you know very easily you can change the number of neurons in the hidden layers.

08:24.200 --> 08:25.150
That's very easy.

08:25.210 --> 08:28.510
And that's thanks to this amazing Karris API.

08:28.810 --> 08:29.800
So congrats again.

08:29.800 --> 08:32.550
Now you know how to build an artificial brain.

08:32.620 --> 08:39.220
And now that we have our artificial brain for our specific case City Well we are ready to implement

08:39.580 --> 08:45.730
the deep learning process and we'll build the advanced version of deep learning because we will integrate

08:46.030 --> 08:48.940
experience play which will improve the performance.

08:49.120 --> 08:55.000
So that's our next step in this general AI framework and we'll take care of that next step in the next

08:55.000 --> 08:56.470
couple of tutorials.

08:56.500 --> 08:57.960
Until then enjoy AI.
