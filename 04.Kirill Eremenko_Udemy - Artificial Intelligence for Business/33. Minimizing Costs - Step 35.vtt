WEBVTT

00:01.100 --> 00:07.280
All right here we go let's do this let's run this one year simulation and in France mode.

00:07.280 --> 00:14.420
So speaking about inference mode the first thing we have to do is make sure that the train variable

00:14.540 --> 00:20.010
of our environment object which was created here is equal to force.

00:20.060 --> 00:23.460
And since this is how we initialize the train variable here.

00:23.600 --> 00:27.970
Well like before we can just train here then.

00:28.160 --> 00:34.610
Well we're actually almost ready to start that big loop that is going to iterate on all the minutes

00:34.880 --> 00:36.650
happening in one full year.

00:36.770 --> 00:41.620
But before that remember we have to initialize the state meaning we have to start somewhere.

00:41.620 --> 00:49.410
In know start in a specific state and for this we have our great observer method from the environment.

00:49.490 --> 00:53.180
So we're going to call that method but actually we've already done this before.

00:53.240 --> 00:54.960
It's in the training.

00:55.010 --> 01:01.670
Remember just before starting the big while loop iterating on all the five month period for each book.

01:01.850 --> 01:11.030
Well we initialized the state with the current state extracted from the observed method of our environment

01:11.160 --> 01:11.610
object.

01:11.620 --> 01:19.710
So I'm copying this and I'm going to base that here and now we can start the big full loop to run that

01:19.710 --> 01:22.000
one your simulation inference mode.

01:22.220 --> 01:25.800
So why did I say for loop and not while loop as before.

01:25.940 --> 01:30.630
Well that's because you know this time we won't have to deal with that.

01:30.680 --> 01:36.350
Game over feature because I remind that this game of feature is only used for the training.

01:36.350 --> 01:41.380
You know in case our AI is doing pretty bad well we want to restart from scratch the training.

01:41.510 --> 01:46.370
But now we're just doing some simulation and therefore we're just going to loop over all the time steps

01:46.730 --> 01:48.490
and therefore we don't need a well here.

01:48.620 --> 01:54.800
So we're going to do this with for you but I can already take this because we're going to look over

01:54.800 --> 02:01.820
this time not five months but 12 months because the whole simulation is over one full year.

02:02.060 --> 02:10.670
So let's start this for loop which is going to loop over all the time step in the range starting from

02:10.820 --> 02:19.130
0 to well the total number of minutes in one year meaning not five months but 12 month.

02:19.130 --> 02:22.860
All right so we have the total number of minutes in one year here.

02:22.930 --> 02:29.900
And so for all the time steps meaning all the minutes in one year what are we going to do.

02:30.170 --> 02:33.680
Well you know how I love to code in efficiency.

02:33.800 --> 02:38.990
And so that's exactly what we're going to be efficient and effective because indeed what we have to

02:38.990 --> 02:45.400
do now is just for each time that here of this for loop Frigg the values then get the action that has

02:45.400 --> 02:52.640
a maximum value then compute the energy spent by the AI then a deadly environment and then finally update

02:52.730 --> 02:56.170
the current state so that it doesn't stay initialized like that.

02:56.450 --> 03:04.070
And therefore what we only need to do now is just take what we have already done in the training in

03:04.220 --> 03:11.240
that inference part because indeed here was we only do is some inference which means we're predicting

03:11.480 --> 03:15.410
the outcome when some input states feed the neural network.

03:15.500 --> 03:18.200
And this is exactly what happens with the simulation.

03:18.200 --> 03:22.130
So basically we only have to get all of this.

03:22.130 --> 03:25.720
So let's take this for now but then you're going to see that even afterwards.

03:25.850 --> 03:27.680
Well here it's going to be the same.

03:27.800 --> 03:31.650
And here it's going to be the same when we have to update the current state.

03:31.760 --> 03:35.480
But let's take care of this verse so I'm going to go back to testing.

03:35.620 --> 03:40.150
And now let's be careful we're going to have some alignment issue after right.

03:40.400 --> 03:41.140
Yes indeed.

03:41.150 --> 03:41.880
Here we go.

03:42.050 --> 03:43.250
But no worries.

03:43.250 --> 03:45.720
I'm going to take care of this very quickly.

03:45.740 --> 03:47.600
So I'd like to do it this way.

03:47.600 --> 03:53.340
Here we go then the F is now well aligned.

03:53.380 --> 03:54.800
Then we have to direction.

03:54.940 --> 03:55.350
Yes.

03:55.360 --> 04:01.700
All good then else is going to go below the IF THEN direction again.

04:01.780 --> 04:03.250
I just need to do it this way.

04:03.280 --> 04:12.610
And finally the energy has to go below the s right because the if else is only to get the right direction.

04:12.610 --> 04:19.030
All right see we've already done the heart of the simulation where Let's recap still where we first

04:19.030 --> 04:26.200
get the q values by predicting the outcome that we get when we feed the neural network of our AI with

04:26.200 --> 04:33.060
the current state then we select the action that has the maximum of these values that we've got here.

04:33.060 --> 04:38.680
Then depending on the value of the action you know whether it's below the direction boundary to or above

04:38.680 --> 04:44.830
the duration boundary to where we get the direction which is going to be minus one if the action is

04:44.830 --> 04:52.840
zero or 1 meaning that is cooling down the server or plus 1 if the action is 3 or 4 meaning that the

04:52.900 --> 04:54.920
AI is hitting up this.

04:55.150 --> 05:01.210
And once again I remind that we need to get that direction because it is required argument of the update

05:01.270 --> 05:03.780
of method which will be our next step here.

05:03.820 --> 05:05.340
After computing the energy.

05:05.560 --> 05:12.280
Very quickly here of course that energy is computed the same way by this formula which we verified in

05:12.280 --> 05:13.090
the training.

05:13.350 --> 05:15.750
And so now speaking of next steps.

05:15.850 --> 05:22.180
Well the next step is of course to update the environment and to reach the next date.

05:22.360 --> 05:29.350
So I'm going to you know simply copy this line again because it's exactly the same only we won't use

05:29.350 --> 05:34.870
the word and the game over Viceregal any time but you know we can just still get them or replace them

05:34.870 --> 05:36.360
by some underscores.

05:36.370 --> 05:41.680
So let's go back to the testing implementation and netspace that just below.

05:41.680 --> 05:42.570
Here we go.

05:42.670 --> 05:45.970
So now we get the next date and all of this is correct.

05:46.000 --> 05:54.310
So we can again move onto the next step which is this time to update that current state variable by

05:54.310 --> 05:57.150
sending it equal to that next date.

05:57.160 --> 06:02.950
We've just reached because indeed after we obtained the environment and reached the next date well the

06:02.950 --> 06:09.730
new occurrence date becomes that next date reached that's exactly what it's going to be the final step

06:09.820 --> 06:16.150
of the simulation because indeed of course all this is related to the training you know related to the

06:16.150 --> 06:19.760
ditch an object used for the deep learning process.

06:19.830 --> 06:24.220
And of course this is the last which is reduce thanks to the training but function well.

06:24.340 --> 06:27.060
All this belongs obviously to the training.

06:27.070 --> 06:28.740
Therefore here we go.

06:28.780 --> 06:32.040
We are confident that this is our final step.

06:32.100 --> 06:34.400
Current state is next state.

06:34.520 --> 06:35.150
All right.

06:35.150 --> 06:39.320
So see that's how we can code things very efficiently.

06:39.320 --> 06:47.630
And so now good news we're very soon about to get the final result whether we are above or below 50

06:47.630 --> 06:49.600
percent of energy saving.

06:49.790 --> 06:55.320
So I can't wait to test because remember that I tested on my site with 1000 bucks.

06:55.490 --> 07:01.840
And right now with you I'm testing with you know one hundred epochs in the training.

07:01.940 --> 07:06.270
So we're going to see if we manage to get at least 50 percent.

07:06.320 --> 07:11.690
So let's do this final step in the next little while you know where we will print the final result.

07:11.690 --> 07:17.750
So we will print it all energy spent by the AI it's all energy spent by the service out in a live system

07:17.750 --> 07:19.050
when there is no AI.

07:19.100 --> 07:25.220
Of course we won't have to print this because this was for the training so we can already remove this

07:25.580 --> 07:31.390
and we'll have to do a final print here which will be of course to print the final outcome that we're

07:31.400 --> 07:39.530
expecting so bad which is the energy saved by the AI vs disturbers alternative cooling system.

07:39.530 --> 07:42.540
So let's reveal that final outcome in the next tutorial.

07:42.560 --> 07:44.270
And until then enjoy AI.
