WEBVTT

00:00.590 --> 00:06.150
All right let's do this let's kick off the plan of attack for this case study number two.

00:06.180 --> 00:09.290
Minimizing the energy consumption in a server.

00:09.630 --> 00:15.510
And we're going to start by defining the environment which as for any AI we build is a process and three

00:15.510 --> 00:16.260
steps.

00:16.260 --> 00:22.220
First defining the state second define the actions and third defining the reward.

00:22.230 --> 00:25.350
All right so let's do this and let's start with the states.

00:25.680 --> 00:28.500
So what I'm going to be the states for are AI.

00:28.770 --> 00:34.770
Well first of all I remind that the states are actually going to be the input of your artificial intelligence.

00:34.800 --> 00:40.560
And as you understood this time the artificial intelligence will be an artificial brain you know an

00:40.680 --> 00:47.190
artificial neural network and this artificial neural network will take some input and these inputs are

00:47.190 --> 00:50.370
exactly the states that we're about to define.

00:50.580 --> 00:52.930
So let's see where I'm going to be these inputs.

00:52.950 --> 00:59.970
Well the input state is t at time t you know at a specific minute is composed of the following three

00:59.970 --> 01:00.810
elements.

01:00.810 --> 01:03.810
First the temperature of the server at time t.

01:03.810 --> 01:10.380
And of course this will be the temperature when we have an AI when we have our AI regulating the temperature

01:10.380 --> 01:11.510
of the server.

01:11.760 --> 01:17.340
Then a second element which will be the number of users in the server at a specific time the number

01:17.340 --> 01:22.180
of active users you know using the server to do whatever they want on their machine.

01:22.410 --> 01:27.140
And third the rate of data transmission in the server at time t.

01:27.270 --> 01:30.090
And so these three elements will be inside a vector.

01:30.100 --> 01:37.080
A vector of three elements which will be the input state that is the input vector of this artificial

01:37.080 --> 01:39.040
brain that will be our AI.

01:39.090 --> 01:44.290
As you can see the input state will be an input vector of these three elements and our future AI.

01:44.300 --> 01:50.580
I will take this vector as input and will return the action to play at each time T after the signals

01:50.610 --> 01:54.310
pass inside the two fully connected letters.

01:54.330 --> 01:55.490
All right so that's for the states.

01:55.490 --> 01:58.660
Breezy just a vector of the three elements.

01:58.700 --> 02:00.980
Now let's define the actions.

02:01.230 --> 02:04.750
So this time we're going to have five actions.

02:04.830 --> 02:11.490
And of course these actions will correspond to some certain temperature changes that the AI will apply

02:11.550 --> 02:15.070
onto the server to regulate its temperature.

02:15.120 --> 02:21.180
So indeed we just need to read here the actions are simply the temperature changes that AI can cause

02:21.510 --> 02:25.110
inside a server in order to heat it up or cool it down.

02:25.380 --> 02:28.570
And of course we are dealing with discrete actions.

02:28.680 --> 02:34.360
So what we consider is five possible temperature changes from minus three degrees to plus three degrees.

02:34.470 --> 02:41.440
And also considering minus 1.5 and plus 1.5 and so we end up with these five possible actions.

02:41.610 --> 02:46.740
And it's important that we keep in mind the indexes of these actions because you know when we start

02:46.740 --> 02:52.320
working with the neural network and applied predictions for the training world we will need to know

02:52.320 --> 02:57.480
this mapping because we will of course give the indexes just as we did in the previous case with the

02:57.480 --> 02:58.530
previous AI.

02:58.750 --> 03:05.210
And so these indexes are index zero corresponds to the situation where it cools down the 7 by 3 degrees.

03:05.460 --> 03:06.670
When the action is 1.

03:06.690 --> 03:13.590
The AI cools down the server by 1.5 degrees when the action is too the AI does not transfer any heat

03:13.800 --> 03:19.680
to the Saroo that is there is no temperature change then when the action is 3 the heats up the server

03:19.680 --> 03:27.480
by 1.5 degrees and finally the final and fifth action when the action is for while the AI heats up the

03:27.480 --> 03:30.030
server by three degrees.

03:30.030 --> 03:30.410
All right.

03:30.410 --> 03:32.990
So again very simple actions.

03:33.150 --> 03:34.560
All clear to understand.

03:34.630 --> 03:38.000
And so now we're ready to define what we want.

03:38.050 --> 03:42.090
So you might have guessed how the words are defined.

03:42.270 --> 03:49.670
Well of course since we want to compare the performance of our AI with the servers integrate including

03:49.710 --> 03:57.030
system well at a specific time the reward will be of course the difference in energy between these two

03:57.030 --> 03:58.070
systems.

03:58.070 --> 04:04.770
So as you can see I'm saying that the reward at a specific time t is the energy spent at that specific

04:04.770 --> 04:11.610
time when there is no AI minus the energy spent at a specific time when there is an area that is the

04:11.610 --> 04:13.650
energy spent by the AI.

04:13.740 --> 04:20.490
And that's because the energy spent by the server is integrated cooling system is exactly the energy

04:20.490 --> 04:27.190
spent when there is no AI's so for notations implicity purposes we are noting that this way.

04:27.450 --> 04:34.440
And remember that we will measure these two energies separately in two separate simulations but with

04:34.440 --> 04:37.710
the same users reiterations and data fluctuations.

04:37.830 --> 04:43.560
Because indeed we want to compare how the two systems do separately but on the same temperatures for

04:43.560 --> 04:48.370
tuition's that isn't the same intrinsic temperatures for two of the server.

04:48.840 --> 04:56.340
And then since Remember assumption to saying that the energy spent by system is exactly equal to the

04:56.400 --> 05:01.670
absolute value of the temperature change that is Delta the cost onto the server.

05:01.800 --> 05:08.580
Well we get that reward at a specific time t is the absolute value of the temperature change applied

05:08.580 --> 05:14.640
to the Saroo when there is no AAE minus the absolute value of the temperature change cost onto this

05:14.650 --> 05:16.200
or when we have any.

05:16.230 --> 05:23.190
When the AI is activated and that simply the reward will have to get at each country because indeed

05:23.460 --> 05:29.370
when doing reinforcement learning or deep reinforcement learning Well we have to get the reward at each

05:29.370 --> 05:34.880
time T because of course the way the AI trains itself is by trial and error.

05:34.980 --> 05:40.740
So the higher the reward the better the AI AI is doing and the lower is the reward the worst the AI

05:40.770 --> 05:45.160
is doing all right and that's it we define the environment.

05:45.170 --> 05:47.260
It was quite simple and easy.

05:47.370 --> 05:52.890
And so now we are already ready to move on to the next step which is to understand the solution that

05:52.890 --> 05:53.840
we're going to build.

05:54.060 --> 05:59.580
And you're going to understand this with Kiril because in the next couple of tutorials you'll get his

05:59.700 --> 06:07.450
intuition lectures and for more theory and more math details you can have a look at the book where indeed

06:07.470 --> 06:09.830
we provide the theory in more detail.

06:09.840 --> 06:15.930
In fact you'll get the full deep Q learning algorithm including experience replay.

06:16.260 --> 06:22.860
So I'll leave you with curial for intuition and theory of declaring and I'll see you in part 3 for this

06:22.860 --> 06:29.250
huge implementation in Python that will do all together from scratch and step by step.

06:29.370 --> 06:30.830
Until then enjoy a.
