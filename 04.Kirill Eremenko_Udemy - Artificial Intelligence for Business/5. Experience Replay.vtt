WEBVTT

00:00.890 --> 00:04.100
Hello welcome back to the course on artificial intelligence.

00:04.100 --> 00:09.020
All right so I hope you're enjoying the tutorial so far we're nearly done with the intuition you soon

00:09.020 --> 00:13.340
very soon get to the practical side of things we've just got a few little things that we need to cover.

00:13.460 --> 00:20.300
All right so previously we talked about how we add neural networks into this whole equation of CULE

00:20.300 --> 00:25.600
learning and take ular into the next step and turn it into deep learning.

00:25.640 --> 00:33.080
And today we're going to add a an extra important feature which will be coding in the practical side

00:33.080 --> 00:38.870
of things so a headline and I decided that it's important for us to cover it often in the intuition

00:38.870 --> 00:42.380
side of things so that you're more prepared for it when it comes in the coding side of things.

00:42.380 --> 00:49.090
So as we discussed we've got the network there's two parts that happen first of all it's the learning

00:49.100 --> 00:53.100
so the network actually learns with every new status.

00:53.190 --> 00:58.790
It slowly updates its waits to get better and better and better at dealing with this environment.

00:58.820 --> 01:06.860
And then there is the acting inside the state so after the q values have been counted in the state then

01:06.890 --> 01:08.170
once you value selected.

01:08.180 --> 01:14.750
So today we're still going to talk about the learning part we're going to come up with a interesting

01:14.750 --> 01:20.000
feature that's going to help in undergrads to come up with this feature ourselves but we'll talk about

01:20.030 --> 01:29.650
a feature that is very important for a deep learning and that feature is called experience or replay.

01:29.660 --> 01:29.990
All right.

01:29.990 --> 01:34.520
So here is our network so we've just copied it over here.

01:34.520 --> 01:36.860
We've got that lost that is Calcott.

01:36.860 --> 01:43.070
The bottom is back propagate through a network and let's have a look at an example of what happens to

01:43.070 --> 01:45.620
understand the problem that we're dealing with a bit better.

01:45.620 --> 01:53.210
So here is an example actually from the course this is a screen shot shot exactly from this course this

01:53.210 --> 01:54.770
is what you'll be programming.

01:54.770 --> 02:02.900
This is a self-driving car that is driving through this along this road and it has to learn how to navigate

02:02.900 --> 02:03.740
this road.

02:03.770 --> 02:09.230
And so what it is as we discussed previously What is this in the state.

02:09.270 --> 02:12.100
And of course the state is not going to be x1 x2.

02:12.130 --> 02:18.260
Lundell will just describe it in a lot more detail what the state is it is going to be a couple of parameters

02:18.860 --> 02:26.300
which relate to the angle of the car and some relative parameters what the sensors are reading and so

02:26.300 --> 02:26.430
on.

02:26.430 --> 02:29.780
So there's going to be more parameters than that to describe the state.

02:29.780 --> 02:34.070
But nevertheless it's going to be a vector of values going to go through a neural network and then on

02:34.070 --> 02:36.480
the output you're going to have some ACU values.

02:36.500 --> 02:39.800
Again there'll be a difference depending on the environment.

02:39.800 --> 02:44.220
They can be a different number of actions possible actions.

02:44.390 --> 02:49.610
But it just going to for simplicity sake leave it at for just for us to be able to understand better

02:49.610 --> 02:50.750
what's going on here.

02:50.750 --> 02:55.550
So in this case what is the question is so far what is this.

02:55.570 --> 03:03.470
This inputs into this neural network or more specifically how often do we trigger this neural network.

03:03.470 --> 03:05.030
How often does this neural net growth.

03:05.060 --> 03:11.360
Well every time the car ends up in a new state so the car makes a move it ends up in a new state and

03:11.480 --> 03:12.610
then everything go.

03:12.620 --> 03:17.350
All that data all that information from about the state goes through the network give Alice the calculated

03:17.590 --> 03:18.120
errors.

03:18.200 --> 03:22.860
This error is calculated based on what we discussed in previous tutorials.

03:22.910 --> 03:26.020
This error is back propagated through and the weights are updated.

03:26.030 --> 03:32.540
Then the car selects which action was to take makes that move ends up in a new state in the new state.

03:32.540 --> 03:34.340
Everything starts over again.

03:34.400 --> 03:39.760
And so basically this happens every time the car is in and you said well have a look at this example

03:39.760 --> 03:46.070
and I specifically took the screen shot because it looks it's very well illustrates the problem that

03:46.070 --> 03:51.380
is addressed through experience replay and means replace not just something that we use in this course

03:51.380 --> 03:52.670
or in this specific problem.

03:52.760 --> 03:57.130
It is something that you will see used all.

03:57.290 --> 04:04.430
On and on and over and over again in artificial intelligence algorithms because it is so powerful and

04:04.430 --> 04:05.090
it's so important.

04:05.090 --> 04:11.060
So look at this car this car in this problem or in this environment its goal is to come from go from

04:11.060 --> 04:12.360
here to here and back.

04:12.360 --> 04:17.500
Its goal is to navigate its way here here without crossing these walls which are made of sand.

04:17.740 --> 04:24.380
And so the car started over here it went down and like its reward is based on you know how close it

04:24.380 --> 04:25.060
is to start.

04:25.070 --> 04:29.840
So the car went from here it went down and kept going like this like this like this like this or along

04:29.840 --> 04:31.440
this wall along the seawall.

04:31.520 --> 04:34.950
And what is it going to do next is going to turn is going to keep going.

04:34.960 --> 04:37.370
Well what we wanted to do is keep going here.

04:37.640 --> 04:39.530
But let's think about it for a second.

04:39.530 --> 04:45.290
Once it got to this wall every single time it moves forward it moves forward it moves forward it moves

04:45.290 --> 04:48.530
forward moves forward moves forward moves forward and so on it moves forward.

04:48.530 --> 04:53.720
So they might be like depending on the structure environment could be like a hundred moves here or 50

04:53.720 --> 04:54.670
moves here.

04:54.940 --> 04:57.480
It just keeps moving forward forward forward forward forward.

04:57.710 --> 05:02.920
And nothing changes nothing really changes it gets way further away from this started closer to this

05:02.920 --> 05:03.260
started.

05:03.270 --> 05:04.060
That's lovely.

05:04.140 --> 05:09.930
But in terms of the surrounding environment not many things are changing it's still that same wall.

05:10.020 --> 05:15.420
If you are sitting in the car you probably seen the situation when you're driving in the whatever you're

05:15.420 --> 05:21.150
seeing is like the environment is so monotonous that you're just seeing kind of the same thing as just

05:21.150 --> 05:26.220
passing by but like Imagine you're driving through a desert and you're just seeing the same thing it's

05:26.220 --> 05:30.300
the same sound it's the same sound nothing is happening nothing is changing.

05:30.480 --> 05:36.780
And so base but every single time we're putting that state new state into here.

05:36.930 --> 05:41.970
Yes of course something might be changing for us as you're driving the car and your GPS is showing you're

05:41.970 --> 05:43.490
closer to your destination.

05:43.500 --> 05:49.230
So one of these inputs is strange but a lot of these other inputs the sensors for instance which are

05:49.230 --> 05:53.280
on the car they're not changing and therefore as you're driving.

05:53.280 --> 05:59.800
So in this day to put in put in put into New Orleans we're here here here here here here here and here

05:59.800 --> 06:03.060
and here all the time the inputs are pretty much the same.

06:03.210 --> 06:11.100
And so if you keep inputting the same inputs same values in vector or very similar vectors into your

06:11.100 --> 06:14.250
network because there is no variety.

06:14.250 --> 06:16.810
The car will learn very well.

06:16.830 --> 06:21.460
One thing you'll learn very well how to drive along this wall which is on its right.

06:21.690 --> 06:26.760
And so that's how the network will update and it will get rewarded will slowly start getting rewarded

06:26.760 --> 06:32.640
for driving so well it'll be like oh I saw from here will be so learning Oh I'm doing so good I'm doing

06:32.640 --> 06:34.020
better I'm doing it better.

06:34.070 --> 06:34.350
It.

06:34.440 --> 06:41.870
It will have this this false perception that it's actually doing very well even though it only learns

06:41.880 --> 06:47.490
how to drive along as well as other neural networks will become very adapted to driving along this wall

06:47.490 --> 06:51.030
and then all of a sudden there's this curve and the car doesn't know what to do.

06:51.270 --> 06:55.200
And it completely doesn't fit in with this neural network.

06:55.380 --> 07:01.620
And even if it does it just somehow let's hypothetically say passes the spot and then it ends up on

07:01.620 --> 07:02.180
this wall.

07:02.190 --> 07:05.160
Same thing is going to happen is going to drive from here here here.

07:05.250 --> 07:10.820
OK now the neural network is restructuring itself to adapt to this wall and then bam this thing happens.

07:10.830 --> 07:16.220
And then even if somehow get past that it'll drive past this thing and then same thing along these lines.

07:16.220 --> 07:23.520
So basically this is a very vivid example of the problem that we are what we have is that because the

07:23.520 --> 07:29.720
way we're using the neural net updating it every single state once we have lots of consecutive stuff

07:29.720 --> 07:30.840
they don't even have to be the same.

07:30.840 --> 07:39.690
But there is in environments it's normal that is consecutive states are somehow correlated or are somehow

07:39.690 --> 07:45.510
interdependent and we don't want that interdependency to bias our network.

07:45.510 --> 07:52.620
We don't want the car to just learn how to drive along like a straight line or a long curved line or

07:54.020 --> 08:01.710
like anything that you think you can think of in in life where an agent would be Navigant environment

08:01.710 --> 08:10.560
where we can think of correlated or interdependent states that come after another that can really mess

08:10.560 --> 08:12.110
up your neural network.

08:12.150 --> 08:15.390
If you're just going to let the agent learn from that.

08:15.390 --> 08:17.570
And that's where experience replay comes in.

08:17.580 --> 08:24.810
What what happens in experience replay is these experiences so these states that it's in one two three

08:24.810 --> 08:31.330
however many 50 states here in IRL they don't get pushed through the network right away.

08:31.410 --> 08:35.450
They are actually saved into memory of the agent.

08:36.120 --> 08:41.400
And so for instance it saves these and saves all these and some at some point once it reaches a certain

08:41.520 --> 08:44.800
threshold which you'll be able to code and Atlanta will show you how to do that.

08:45.060 --> 08:50.300
Once it reaches a certain threshold then the agent decides for itself.

08:50.310 --> 08:51.240
OK it's time to learn.

08:51.240 --> 08:56.810
I have I have this batch of experiences that I have and now I'm going to learn from that batch and sewed

08:56.820 --> 09:03.870
randomly selects a uniformly distributed and uniform these key is important here because that's something

09:03.870 --> 09:06.460
we'll we'll talk about on the next slide.

09:06.750 --> 09:08.090
We'll book will mention that.

09:08.100 --> 09:12.330
But it takes a uniformly distributed sample.

09:12.390 --> 09:15.620
So basically all experiences are considered to be equal.

09:15.630 --> 09:23.370
It takes a uniformly distributed sample from that batch of experiences that it has and then it goes

09:23.370 --> 09:27.620
through them and learns from them so it doesn't take all the experience it just takes and uniformly

09:27.630 --> 09:33.090
distribute samples it might take a couple from here a couple from here a couple from here and each experience

09:33.090 --> 09:39.870
is characterized by the state it was in the action that it took the state it ended up in and the reward

09:39.940 --> 09:44.760
is it achieved through that action in that specific state.

09:44.760 --> 09:50.110
So four elements in each experience state one action state two and reward.

09:50.110 --> 09:54.720
And so it takes all those experiences and then it passes them through the network and it learns and

09:54.720 --> 10:05.030
that way it it breaks the pattern of that bias which comes from the sequential nature of the experience

10:05.060 --> 10:08.180
as if you were to put them through the network one after the other.

10:08.280 --> 10:11.870
So that's the main focus of experience we play.

10:11.880 --> 10:14.330
That's that's what the problem it addresses.

10:14.330 --> 10:20.100
And another benefit of experience replay is that sometimes in an environment like this you might have

10:20.100 --> 10:22.370
very valuable rare experiences.

10:22.380 --> 10:28.270
So for instance I don't know let's say let's look at this corner right this is this is a right corner.

10:28.400 --> 10:28.640
Right.

10:28.680 --> 10:35.330
And a very sharp one is sharp so it'll be coming from here assuming it's going to be hugging this corner.

10:35.580 --> 10:40.470
So having you sharp right corners do we have in this in this whole new army only have one right corner

10:40.470 --> 10:42.800
here and one right corner here.

10:43.620 --> 10:43.880
Right.

10:43.880 --> 10:46.210
So when it's coming this way that's the right corner.

10:46.350 --> 10:48.600
And then when it's going back it's a sharp right corner here.

10:48.590 --> 10:53.010
So and this one's not sharp this in the shop so there's only one opportunity in the whole environment

10:53.580 --> 10:56.920
to learn from a sharp right corner.

10:56.920 --> 11:03.030
And that's a very important experience because it might get really good at driving along straight lines

11:03.030 --> 11:06.390
get really good doing like soft corners like that like that.

11:06.390 --> 11:13.860
But and then it'll keep messing up this sharp right corner simply because simply because it doesn't

11:13.860 --> 11:17.900
have that much opportunity to learn from it and so therefore it will learn everything else very quickly.

11:17.910 --> 11:20.110
But it'll take a long time to learn or learn the right course.

11:20.120 --> 11:25.950
It's a very simplified example is a very simplified explanation but it illustrates the concept that

11:26.220 --> 11:30.060
sometimes there are rare experiences which are which can be valuable.

11:30.210 --> 11:35.820
And if you're just doing a simple neural network where you're putting in your values here and you know

11:35.820 --> 11:40.900
they're going through and you know like even if you forget about that problem of the sequential nature

11:40.900 --> 11:46.080
of experiences and how they can be interdependent or correlated info even forget about that for a second

11:46.740 --> 11:52.080
what happens is once you put an experience in it goes through Netflix updated then you instantly forget

11:52.080 --> 11:52.210
it.

11:52.260 --> 11:53.310
Forget about their experience.

11:53.310 --> 11:54.360
You move on to the next one.

11:54.390 --> 11:56.160
That's just how the neural network works.

11:56.190 --> 12:00.180
Then you move onto the next date the next day the next day the next experience experience experience

12:00.510 --> 12:01.060
and so on.

12:01.140 --> 12:05.700
So this right corner as soon as it goes through a network is gone there and you don't have any memory

12:05.700 --> 12:11.010
of that valuable experience whereas we've experienced replay because you're putting these experiences

12:11.010 --> 12:12.980
into batches.

12:13.080 --> 12:19.080
You can organize your bash as a rolling window so for instance you could have like 100 batches So hundred

12:19.350 --> 12:20.290
experiences in your batch.

12:20.290 --> 12:27.160
So when it's coming back from here it's as soon as it has this recorded this experience in its batch.

12:27.330 --> 12:33.740
Then like at some point it runs it takes a uniform distribution from its batch of experiences.

12:33.900 --> 12:37.920
And then there's a rolling window so it forgets these experiences but then it keeps these experiences

12:37.930 --> 12:42.340
and then again and it learns from once it's here it learns from this batch.

12:42.540 --> 12:45.350
And then once it's here it forgets all the way up to here.

12:45.360 --> 12:50.590
But then it has a batch of experiences like that so therefore now learn from these experiences.

12:50.670 --> 12:58.350
And that way what you are getting is that this right hand corner might come up several times in its

12:58.350 --> 13:03.420
learning process because it was in that batch when the batch was like this around there than it was

13:03.420 --> 13:08.910
in the batch here in about here so it came up in several batches because of abash might be updated as

13:08.910 --> 13:11.400
a rolling window of experience.

13:11.400 --> 13:15.630
So the older experiences get kicked out the newer experiences are added and then again older experience

13:15.620 --> 13:16.260
get it.

13:16.380 --> 13:22.980
So in experience it stays in the batch for quite some time and the car or agent can learn from that

13:22.980 --> 13:24.150
experience several times.

13:24.150 --> 13:27.410
So that's another advantage of experience replay.

13:27.510 --> 13:33.420
And of course the final advantage is experience replay gives you an opportunity to learn from more experiences

13:34.150 --> 13:39.240
than if you're just learning for one at a time because you have that batch and therefore And it's a

13:39.240 --> 13:46.650
rolling window and therefore even if your environment is limited to experience your experience replay

13:46.650 --> 13:49.350
approach can help you learn faster.

13:49.350 --> 13:55.170
And instead of just redoing there are many many many times you can learn fast because you don't have

13:55.170 --> 13:55.680
to redo it.

13:55.670 --> 13:57.680
You have those experiences saved.

13:57.750 --> 13:59.850
So those are the main advantages of ex-Marines replay.

13:59.850 --> 14:01.610
Let's recap on that we've got that.

14:01.680 --> 14:09.800
We're breaking that pattern of independence and correlation of sequential experiences we save rare experiences

14:09.800 --> 14:16.760
which might be important therefore we can learn from them more often and we can learn in environments

14:16.850 --> 14:21.220
we can learn Foster environments which are experience.

14:21.470 --> 14:27.260
We have a shortage of experiences which don't have that many experiences that the agent goes through

14:27.260 --> 14:29.120
and still we can be able to learn.

14:29.330 --> 14:32.420
So that is what experience replay is all about.

14:32.420 --> 14:34.470
If you'd like to read a bit more than this.

14:34.580 --> 14:41.240
There's an interesting article published by deep mind in 2016 is called prioritized experience replay

14:41.510 --> 14:50.110
and it talks about why why are we using a uniform distribution to select our experiences from the experience

14:50.140 --> 14:55.310
Bachche why don't we find a better way to select our experiences and prioritize some of the experiences

14:55.310 --> 14:57.100
which we feel that are important.

14:57.160 --> 15:03.800
It's quite an interesting thing though in this case you will be able to not only reinforce or not only

15:03.800 --> 15:11.750
reinforce your knowledge on experience replay but you will actually be able to move with the cutting

15:11.750 --> 15:12.590
edge of technology.

15:12.590 --> 15:15.090
So this is 2016 and published by deep minds.

15:15.090 --> 15:21.530
It's a very recent very powerful paper so you'll be able to actually explore the limits or explore even

15:21.530 --> 15:24.490
further this algorithm and take it to the next level.

15:24.500 --> 15:30.830
So I'll leave it up to you to find out why and how we can change the uniform solution to a different

15:30.830 --> 15:33.730
approach to experience really from this paper if you'd like.

15:33.890 --> 15:39.200
And I hope you answer to this Tauriel and now we know what experience really is and we can confidently

15:39.200 --> 15:41.350
use it in our practical Sorrells.

15:41.420 --> 15:44.500
And I look for see an extent until then enjoy a.
