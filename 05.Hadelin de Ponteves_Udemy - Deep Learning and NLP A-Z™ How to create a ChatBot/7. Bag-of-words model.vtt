WEBVTT
1
00:00:00.770 --> 00:00:04.280
Well I will go back to the course on deep natural language processing.

2
00:00:04.280 --> 00:00:07.500
Today we're looking at a bag of words model.

3
00:00:07.610 --> 00:00:12.750
First thing I'd like us to look at is an email an email I received just a few days ago.

4
00:00:12.950 --> 00:00:13.880
So here we go.

5
00:00:13.910 --> 00:00:18.010
The e-mail is about to catch up and my friend is asking.

6
00:00:18.020 --> 00:00:18.740
Hello Carol.

7
00:00:18.740 --> 00:00:24.770
Checking if you're back in Oz hands for Australia let me know if you're around and keen to see on how

8
00:00:24.770 --> 00:00:25.690
things are going.

9
00:00:25.790 --> 00:00:30.820
I definitely definitely could use some of your creative thinking to help of mine.

10
00:00:30.830 --> 00:00:32.180
Cheers B.

11
00:00:32.360 --> 00:00:36.680
And so what I'd like us to pay attention to.

12
00:00:36.680 --> 00:00:42.290
First of all of course you can see that I sent this email to myself but that is because I wanted to

13
00:00:42.290 --> 00:00:46.050
keep my friend.

14
00:00:46.070 --> 00:00:49.850
Actually it is because I read your reply to the email and then I wanted to send it and also wanted to

15
00:00:49.850 --> 00:00:54.320
keep my friend keep his privacy but this is a real email.

16
00:00:54.320 --> 00:01:01.370
This is the exact text that I got literally a couple of days ago and the title was different but I just

17
00:01:01.380 --> 00:01:04.950
called a change to catch up and so on.

18
00:01:05.080 --> 00:01:11.960
What is interesting about this we're going to be looking at how we can apply natural language processing

19
00:01:11.960 --> 00:01:18.770
to this email in the next couple tutorials and it will help us work with a real life example and then

20
00:01:18.790 --> 00:01:29.480
the other thing is that it is here you can see in Google the gmail app for iPhone you can see that is

21
00:01:29.560 --> 00:01:31.790
give me some suggestions.

22
00:01:31.850 --> 00:01:37.520
Very interesting it's saying I was requiring some quick replies that I can use it can be yes I'm around.

23
00:01:37.520 --> 00:01:44.410
I'm back so I am not very interesting so let's keep that in mind and we will come back to this later.

24
00:01:44.450 --> 00:01:46.700
In the meantime text of the email is here.

25
00:01:46.700 --> 00:01:48.630
What can we do with it.

26
00:01:48.680 --> 00:01:48.980
All right.

27
00:01:48.980 --> 00:01:51.830
So first things were going to start off simple.

28
00:01:51.840 --> 00:01:53.780
We're going to create a model.

29
00:01:53.780 --> 00:01:59.540
We're going to look at how we can create a model that will give us an a yes no response because that's

30
00:01:59.540 --> 00:02:00.950
one of those questions.

31
00:02:01.190 --> 00:02:03.640
The question is are you back in Australia.

32
00:02:03.650 --> 00:02:05.510
Let me know if you're around and keen to say so.

33
00:02:05.530 --> 00:02:05.950
Yes.

34
00:02:05.960 --> 00:02:09.190
No of course it's better to have a long response.

35
00:02:09.200 --> 00:02:17.600
And that's that's the social norm and it's it's the etiquette to converse with people and they just

36
00:02:17.600 --> 00:02:22.820
say you know what even Let's try to get a yes no response and see how would you go about that because

37
00:02:22.820 --> 00:02:29.190
that's a first step into an LP and then further on we will see how we can expand that and more.

38
00:02:29.630 --> 00:02:40.030
All right so we're going to start off with we're vector a vector or a just like an hour a full of zeros

39
00:02:40.170 --> 00:02:41.530
let's call it vectors.

40
00:02:41.530 --> 00:02:42.410
These are like that.

41
00:02:42.410 --> 00:02:44.340
So just 0 0 0 0.

42
00:02:44.360 --> 00:02:45.430
How many zeros.

43
00:02:45.620 --> 00:02:47.520
Well a lot of zeros.

44
00:02:47.610 --> 00:02:51.290
Twenty thousand elements in total twenty thousand.

45
00:02:51.290 --> 00:02:52.120
Why is that.

46
00:02:52.460 --> 00:03:00.530
Well it's because of the way that we're building a small 20000 is the number of words that are commonly

47
00:03:00.530 --> 00:03:04.670
used by the average native English language speakers.

48
00:03:04.670 --> 00:03:07.450
So here's a quick search on Google.

49
00:03:07.450 --> 00:03:09.080
How many words in the English.

50
00:03:09.140 --> 00:03:14.060
That's the search I took I came up with how many words are there in English language.

51
00:03:14.060 --> 00:03:16.950
Hundred seventy one thousand pardons seventy six words.

52
00:03:16.950 --> 00:03:23.180
That's how many entries in our tradition plus some obsolete words plus derivative words and so on.

53
00:03:23.180 --> 00:03:32.060
But also people also you can see Google's giving a suggestion that more subtle adult native test takers

54
00:03:32.060 --> 00:03:38.090
range from 20 to 30 trade thirty five thousand words average native test takers of age eight or ten

55
00:03:38.090 --> 00:03:45.610
thousand words average age of test takers or you know 5000 or that it's an adult native test takers

56
00:03:45.630 --> 00:03:51.380
low almost white whatever this is going to is what's.

57
00:03:51.530 --> 00:03:53.720
But the interesting thing here is that

58
00:03:56.370 --> 00:03:59.050
Harmy like what I wanted to point out.

59
00:03:59.050 --> 00:04:03.210
First of all 20000 and we will see why exactly resisted a lot more.

60
00:04:03.430 --> 00:04:08.250
What I wanted to point out is how many words are there in the English language.

61
00:04:08.290 --> 00:04:12.440
Even this in its own is actually Google's applying natural language processing.

62
00:04:12.450 --> 00:04:19.090
It's it's looking at what we wrote and and then it's also checking other similar answers.

63
00:04:19.090 --> 00:04:23.560
How many words in the English language does that other person the average person know.

64
00:04:23.560 --> 00:04:25.780
So that's a question I ask but it came up with that.

65
00:04:25.870 --> 00:04:27.580
Then you came up with many other questions.

66
00:04:27.580 --> 00:04:34.870
So you can see that the irony is that even in this search on its own We're really falling victim of

67
00:04:34.900 --> 00:04:39.190
natural language processing even though that wasn't our intention and that's not what we're going to

68
00:04:39.190 --> 00:04:40.060
be talking about.

69
00:04:40.210 --> 00:04:42.530
But it's just funny that it came up anyway.

70
00:04:42.550 --> 00:04:53.680
So 20000 words and fun fact is that we actually use about 3000 words out of those hundred seventy one

71
00:04:53.680 --> 00:05:00.760
thousand four is only six words we only use 3000 words not just in conversational language but you can

72
00:05:00.760 --> 00:05:08.890
see here vocabulary of just 3000 words provides coverage for around 95 percent of common texts 95 percent

73
00:05:08.950 --> 00:05:13.870
of common text that I like I'm assuming that's including books and stuff like that.

74
00:05:13.900 --> 00:05:20.820
So if you do the math it's I only use one point seventy five percent of the total number of words in

75
00:05:20.830 --> 00:05:21.880
English language.

76
00:05:21.880 --> 00:05:30.220
So as you can see even that 3000 like our 20000 is more than even the 3000 that covers any facts of

77
00:05:30.250 --> 00:05:33.220
the situation so we're pretty good.

78
00:05:33.220 --> 00:05:41.980
We're definitely covered if we say that our vocabulary all possible words that we can encounter is going

79
00:05:41.980 --> 00:05:43.900
to fit into a vector of tweets.

80
00:05:43.910 --> 00:05:49.120
So every Basically what we're saying this is important what we're saying is that every word in the English

81
00:05:49.120 --> 00:05:52.750
language has a position somewhere on this vector.

82
00:05:52.750 --> 00:05:55.930
So for example this the word if could have this position.

83
00:05:55.940 --> 00:06:04.090
So if count one two three four five six the seventh position in our custom made vector is that word

84
00:06:04.120 --> 00:06:06.060
events always going to be on that position.

85
00:06:06.060 --> 00:06:07.930
That's very crucial for this.

86
00:06:08.050 --> 00:06:13.540
For instance the word badminton is just like that we can construct this vector any way we want the word

87
00:06:13.540 --> 00:06:17.590
Badman's and could be on this position is always going to be on this position and the word table is

88
00:06:17.590 --> 00:06:18.530
going to be on this position.

89
00:06:18.610 --> 00:06:24.990
And this is like how this bag of words model works so just keep in mind that once you like what we've

90
00:06:25.000 --> 00:06:31.820
taken out 20000 words then we've assigned them a space that's where they that's what they will.

91
00:06:31.870 --> 00:06:37.670
This base in this vector will be associated with will be associated with we this will be associate with

92
00:06:37.670 --> 00:06:38.510
or badminton.

93
00:06:38.550 --> 00:06:46.030
This will this position will be associated or table and other thing is here you can see a great out

94
00:06:46.210 --> 00:06:53.470
first and the last one first two are going to be reserved for solace and iOS stands for start of sentence.

95
00:06:53.480 --> 00:06:59.860
U.S. stands for end of sentence and the last one will be reserved for special words and that's for those

96
00:06:59.860 --> 00:07:01.320
words that you're wondering about.

97
00:07:01.320 --> 00:07:04.490
Second I can hear your brain churning right now.

98
00:07:04.630 --> 00:07:09.510
What about those other hundred and fifty thousand words that we didn't take into account.

99
00:07:09.520 --> 00:07:10.510
What if they come up.

100
00:07:10.630 --> 00:07:15.980
Well if they come up we're going to just associate them with this with this last thing.

101
00:07:16.000 --> 00:07:20.080
This last element would be just throw them all in there any kind of words that we can recognize of the

102
00:07:20.080 --> 00:07:24.680
20000 and throw them into that last element.

103
00:07:25.210 --> 00:07:27.660
All right so let's go back to our e-mail text.

104
00:07:27.730 --> 00:07:28.190
Here it is.

105
00:07:28.210 --> 00:07:28.950
Hello Carol.

106
00:07:28.950 --> 00:07:31.880
Checking if you're back in let me know if you are around.

107
00:07:31.900 --> 00:07:33.290
Etc etc. etc..

108
00:07:33.400 --> 00:07:35.210
Cheers V.

109
00:07:36.130 --> 00:07:41.940
And so let's see how this can be put into our bag of words.

110
00:07:41.940 --> 00:07:47.170
If you've probably noticed by now that this is our bag of words that we are constructing here so now

111
00:07:47.170 --> 00:07:51.790
we get to throw the text into this bag of words as they are going to happen.

112
00:07:51.790 --> 00:07:56.190
I'm just going to throw it in and then I'll explain how it happened so there it is.

113
00:07:56.190 --> 00:08:00.020
That's the result it that's it.

114
00:08:00.030 --> 00:08:02.220
And of course depends on how we construct our victory.

115
00:08:02.240 --> 00:08:03.790
But this is our result.

116
00:08:03.800 --> 00:08:11.110
And in the way we construct a vector and let's let's look at it this way so we as we discussed previously

117
00:08:11.140 --> 00:08:18.710
we took the 20000 words and we associate each position with a work and now we go through our text and

118
00:08:18.710 --> 00:08:24.560
find and then like increase the counter in each position of the associated words so.

119
00:08:24.830 --> 00:08:32.210
Let's say in our lecture it is in position number five because we only have one hello in this whole

120
00:08:32.520 --> 00:08:33.200
e-mail.

121
00:08:33.240 --> 00:08:35.130
We're going to put a one here.

122
00:08:35.150 --> 00:08:38.090
Cairo is definitely not an English language word.

123
00:08:38.210 --> 00:08:40.850
So we're going to have to put it into there.

124
00:08:41.000 --> 00:08:46.610
And the reason why there's three here is because we have curial than ours.

125
00:08:46.820 --> 00:08:51.340
And the those are not English language words not among or 20000.

126
00:08:51.360 --> 00:08:52.680
They're all going to go here.

127
00:08:53.840 --> 00:08:58.040
Then we've got the comma surprise the comma also has a position.

128
00:08:58.040 --> 00:09:02.420
Let's say it was in position number 3 6 7 8 9.

129
00:09:02.420 --> 00:09:06.880
So the ninth position is associated with a comma because we have one comma in our e-mail.

130
00:09:06.890 --> 00:09:08.600
Oh actually we have two commas.

131
00:09:08.600 --> 00:09:11.570
OK so this should be a two but let's listen something about that.

132
00:09:11.650 --> 00:09:14.040
Let's forget about that comma.

133
00:09:14.040 --> 00:09:15.020
I didn't notice it.

134
00:09:15.020 --> 00:09:23.450
So assuming we have one comment or email this is a one checking let's say that this this element is

135
00:09:23.450 --> 00:09:29.720
associated with or checking this a 1 because it's only one word checking if it's a two because we have

136
00:09:29.730 --> 00:09:31.750
two ifs in our e-mail.

137
00:09:31.790 --> 00:09:38.410
So it's going to be up to you too because we have to use in our email including the rest of the text.

138
00:09:38.420 --> 00:09:39.870
I don't think there's any more use in there.

139
00:09:40.070 --> 00:09:44.180
And so and so that's basically how we feel this bag of words.

140
00:09:44.210 --> 00:09:52.130
We just put in the quantity of words for every positions pretty straightforward we're just filling in

141
00:09:52.130 --> 00:09:52.830
this vector.

142
00:09:52.880 --> 00:09:56.920
As you can see it's going to be quite a sparse electorate is going to be lots of zeros.

143
00:09:57.290 --> 00:10:03.580
Almost 20000 zeros and some of the words are going to be folding So what is our goal.

144
00:10:03.770 --> 00:10:11.000
So our goal as we discussed before is to come up with replies Yes or no to this e-mail which is now

145
00:10:11.090 --> 00:10:14.410
in the form of a vector and how we're going to do that.

146
00:10:14.450 --> 00:10:16.490
Well we're going to do it through training day.

147
00:10:16.550 --> 00:10:23.120
So we're going to look at all the e-mails that I have replied to because US training model to reply

148
00:10:23.120 --> 00:10:29.180
to my e-mails or in your case in anybody's case is going to be training the model to reply to their

149
00:10:29.180 --> 00:10:32.050
e-mails are going to look at training data we're going to need some change.

150
00:10:32.060 --> 00:10:35.650
I'm going to fish it out of the inbox outbox.

151
00:10:35.750 --> 00:10:37.580
So let's say let's look at a couple.

152
00:10:37.580 --> 00:10:44.030
So here we've got Hey mate have you read about Hinton's capsule networks and general reply to that.

153
00:10:44.030 --> 00:10:44.760
No.

154
00:10:45.140 --> 00:10:47.760
So we're going to use that as a training example.

155
00:10:47.840 --> 00:10:50.710
Next one did you like that recipe I sent you last week.

156
00:10:50.790 --> 00:10:52.610
The answer was yes.

157
00:10:52.910 --> 00:10:55.080
It was a good recipe I guess so.

158
00:10:55.430 --> 00:10:57.640
So now we have two three.

159
00:10:57.800 --> 00:10:59.730
Hi Carol are you coming to dinner tonight.

160
00:10:59.810 --> 00:11:03.580
Yes dear Carol would you like to search your car with us again.

161
00:11:03.830 --> 00:11:04.510
No.

162
00:11:04.820 --> 00:11:06.900
Are you coming to Australia in December.

163
00:11:07.010 --> 00:11:07.740
Yes.

164
00:11:07.820 --> 00:11:08.570
And so on.

165
00:11:08.570 --> 00:11:14.690
So ideally we would have tens or hundreds of thousands of e-mails like that and responses like that

166
00:11:14.690 --> 00:11:20.420
yes or no responses of course would be like a lot of ground work to get that data because we usually

167
00:11:20.420 --> 00:11:27.850
don't just respond to emails so we have to look at this answer and understand what was the sentiment.

168
00:11:27.890 --> 00:11:31.980
Sentiment was no know what was the overall was it a yes or no.

169
00:11:31.980 --> 00:11:36.990
No yes at all and so of course it's kind of more of a theoretical example.

170
00:11:36.990 --> 00:11:39.020
Nobody is going to do this for their own inbox.

171
00:11:39.080 --> 00:11:40.870
But nevertheless the point.

172
00:11:41.060 --> 00:11:42.530
So how would we train.

173
00:11:42.530 --> 00:11:44.070
How would we use the data.

174
00:11:44.600 --> 00:11:49.640
We would use a similar principle and convert each one of those e-mails to a vector.

175
00:11:49.720 --> 00:11:54.000
And again each vector would be 20000 elements long.

176
00:11:54.020 --> 00:11:59.760
So you know I just threw some numbers in here too to get the point across.

177
00:11:59.780 --> 00:12:05.150
It's not exactly accurate but so we have these vectors like lots and lots and lots of pictures lots

178
00:12:05.150 --> 00:12:07.540
and lots and lots of responses yes and no.

179
00:12:08.090 --> 00:12:13.260
And yeah so now what we're going to do is we go into

180
00:12:15.840 --> 00:12:18.650
apply and model once we have all this data we're going to apply model.

181
00:12:18.660 --> 00:12:25.340
So one of the models we can apply to create a bag of words or one of the algorithms we can apply to

182
00:12:25.340 --> 00:12:28.780
create a bag of words model is the logistic regression.

183
00:12:28.980 --> 00:12:36.280
So we apply the logistic regression to our Yes no responses to these to this information that we have.

184
00:12:36.720 --> 00:12:42.300
And then once we have that model once we've separated.

185
00:12:42.430 --> 00:12:50.890
So we know we kind of like we've modeled what goes like what goes into a yes like what's what is likely

186
00:12:50.890 --> 00:13:01.300
to yield a yes it is like to table no border between them then we can feed our actual e-mail that we

187
00:13:01.300 --> 00:13:05.020
got into this model and then get a response.

188
00:13:05.020 --> 00:13:06.810
So for instance yes and that's it.

189
00:13:06.850 --> 00:13:09.350
So we use all the training data to create a model.

190
00:13:09.370 --> 00:13:16.660
We see it in our actual email which is important which has exactly the same format so you can see that

191
00:13:16.660 --> 00:13:28.240
every input here every every time we train the data the independent variable of the independent variable

192
00:13:28.240 --> 00:13:32.070
vector always has the same length 20000 and always had the same format.

193
00:13:32.080 --> 00:13:35.910
We know that this position always corresponds to a certain word.

194
00:13:35.940 --> 00:13:42.480
This position is always a certain word this position let's say 1 to 3 which is where it was at one two

195
00:13:42.480 --> 00:13:46.020
three four five six seven eight.

196
00:13:46.020 --> 00:13:48.760
So this was what we know this one as if right.

197
00:13:48.780 --> 00:13:51.320
So this corresponds to it or something.

198
00:13:51.330 --> 00:13:57.180
So we know that it's the same format it's always the same length 20000 so we can safely feed in this

199
00:13:57.180 --> 00:13:58.240
victory into there.

200
00:13:58.440 --> 00:14:00.230
It's got the same number of features.

201
00:14:00.280 --> 00:14:01.740
BAM we get an answer.

202
00:14:01.740 --> 00:14:02.790
So for instance we get.

203
00:14:02.820 --> 00:14:03.180
Yes.

204
00:14:03.180 --> 00:14:06.810
So and then we can like look back on what the actual emails say it said.

205
00:14:06.810 --> 00:14:07.670
Hello Carol.

206
00:14:07.920 --> 00:14:13.670
OK so based on my training I would have most likely replied to this.

207
00:14:13.780 --> 00:14:14.470
Yes.

208
00:14:14.640 --> 00:14:15.510
Interesting.

209
00:14:15.570 --> 00:14:19.490
The other approach we can take here are first of all let's put this on our diagram.

210
00:14:19.650 --> 00:14:25.290
Our diagram and that's a natural language processing algorithm which is called back of words.

211
00:14:25.490 --> 00:14:26.950
So it's over there.

212
00:14:27.090 --> 00:14:33.330
The other approach we could apply here to here is we could sort of logistic regression we could use

213
00:14:33.420 --> 00:14:38.020
a neural network we could because we have a vector right.

214
00:14:38.100 --> 00:14:44.970
So we have all these vectors we could feed them into as an input layer like over 20000 neurons into

215
00:14:45.090 --> 00:14:46.930
our neural network.

216
00:14:46.930 --> 00:14:53.070
They would go through we want to learn to analyze as me layers as we want our own decision on how to

217
00:14:53.070 --> 00:14:56.960
structure it and then bam we've got an output layer and tell us yes or no.

218
00:14:57.000 --> 00:15:02.100
And so we again would use all this data that we have here all our millions and millions and millions

219
00:15:02.100 --> 00:15:07.390
of emails and responses would use that to train on your own networks.

220
00:15:07.560 --> 00:15:13.500
All through back propagation and sarcastic gradient descent all the weights would be updated and bam

221
00:15:13.560 --> 00:15:14.740
we have an answer.

222
00:15:14.750 --> 00:15:19.740
So not that we have an answer or so we would use these answers here to train that I would use the pairs

223
00:15:19.740 --> 00:15:22.160
like the vector and the answer vector answer.

224
00:15:22.170 --> 00:15:27.720
So to minimize the errors because a great and decent back propagation of the ways bam we have a neural

225
00:15:27.720 --> 00:15:29.540
network it's all trained up.

226
00:15:29.600 --> 00:15:37.620
Now we feed in our vector here which represents our new email into the neural network and voila we get

227
00:15:37.710 --> 00:15:43.100
our answer as in this case might also be yes they might reveal different.

228
00:15:43.170 --> 00:15:51.360
But if their models are circuit Well moralists should be coming up with similar or the same answers

229
00:15:51.370 --> 00:15:56.590
most of the time as in this case we have got a deep natural language processing.

230
00:15:56.990 --> 00:15:58.340
I didn't put emphasis there either.

231
00:15:58.350 --> 00:16:01.110
We got a deep natural language processing algorithm.

232
00:16:01.260 --> 00:16:08.280
Right because we're using a neural network and that is different so in both cases a bag of words model

233
00:16:08.590 --> 00:16:10.830
one case is an old bag of words.

234
00:16:10.830 --> 00:16:15.200
In other cases the deep and a the bag of words.

235
00:16:15.210 --> 00:16:23.310
But in both cases it is still a bag of words and it has its own limitations and it has its own limitations

236
00:16:23.310 --> 00:16:26.230
and issues that are not that great.

237
00:16:26.250 --> 00:16:30.780
And so I'll point out one of them right now is that that response is very simple it's just a yes or

238
00:16:30.810 --> 00:16:31.390
no.

239
00:16:31.740 --> 00:16:32.020
Right.

240
00:16:32.010 --> 00:16:36.630
Like we want something more sophisticated we want like a conversation can really have a conversation

241
00:16:36.630 --> 00:16:39.620
can really build the Chugwater is going to be saying yes all the time.

242
00:16:39.810 --> 00:16:41.150
So that's one of the limitations.

243
00:16:41.160 --> 00:16:48.540
We'll talk about some more of them in upcoming tutorial and we'll also see how to overcome those limitations

244
00:16:48.570 --> 00:16:52.150
and what models await us in the future.

245
00:16:52.320 --> 00:16:53.940
And I hope you enjoy this.

246
00:16:53.940 --> 00:16:56.780
I really enjoyed going through all this with you together.

247
00:16:57.000 --> 00:16:59.270
And I can't wait to see an accent.

248
00:16:59.340 --> 00:17:02.960
Until then enjoy natural language processing.
