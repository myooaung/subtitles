1
00:00:00,930 --> 00:00:05,940
Hello and welcome back to the course on deep natural language processing and philippic talk about the

2
00:00:05,940 --> 00:00:08,330
types of natural language processing.

3
00:00:08,370 --> 00:00:16,620
So we've got two Venn diagrams here or we got a Venn diagram of two circles in it and we're going to

4
00:00:16,710 --> 00:00:24,940
look at the different areas of natural language processing that are going to come up in this course.

5
00:00:25,140 --> 00:00:33,720
So on the left of natural language processing overall and this refers to the whole circle on the left.

6
00:00:33,720 --> 00:00:38,620
So the reason why we've called in just this great part is because that's not overlapping part.

7
00:00:38,620 --> 00:00:43,170
So we know that anything here is just natural language processing.

8
00:00:43,170 --> 00:00:46,230
We evolved with disregard to the second circle.

9
00:00:46,390 --> 00:00:51,180
But natural language processing is indeed everything that is in this circle.

10
00:00:51,450 --> 00:00:54,730
Then we've got on the right deep learning.

11
00:00:54,750 --> 00:01:01,640
So these are all algorithms that have something to do with neural networks deep learning.

12
00:01:01,890 --> 00:01:04,860
Basically anything that's called a deeper or an algorithm falls in here.

13
00:01:04,860 --> 00:01:08,040
They don't have to be natural language processing.

14
00:01:08,040 --> 00:01:14,880
They can be classification they can be anything so they can be that's deeper here and natural language

15
00:01:14,880 --> 00:01:20,760
processing is any algorithm any mortal that has something to do with processing of natural language

16
00:01:21,420 --> 00:01:24,620
into machine terms.

17
00:01:25,380 --> 00:01:28,920
And then finally in the overlap we have deep and all.

18
00:01:29,280 --> 00:01:35,970
So these are models which have to do with natural language processing but also which are deep learning

19
00:01:35,970 --> 00:01:37,910
more which are neural networks.

20
00:01:38,220 --> 00:01:46,140
And so that's the part that we're going to be aiming for but it's also very good to have visibility

21
00:01:46,200 --> 00:01:47,380
of all three.

22
00:01:47,400 --> 00:01:53,030
Because in this course we will be talking about some models that fall just in here and then we'll be

23
00:01:53,040 --> 00:01:58,080
talking about those here and it'll be good to compare and see how the world has changed over time and

24
00:01:58,470 --> 00:02:02,050
why these models are often better than these models.

25
00:02:02,760 --> 00:02:10,380
And the other thing to note here is that the size of these diagrams is not reflective of the importance

26
00:02:10,410 --> 00:02:17,640
or the volumes of these different fields so I just said circles on the same size simply because we want

27
00:02:17,640 --> 00:02:24,180
a visual representation of all the overlap and that these fields exist but don't take size into account.

28
00:02:24,210 --> 00:02:26,770
It's not to scale at all.

29
00:02:27,270 --> 00:02:36,390
And finally there is a another part another part of this event diagram which is very important to us

30
00:02:36,840 --> 00:02:46,100
and it is this part over here a sub section of the deep and Piccola sequence to sequence so sequences

31
00:02:46,100 --> 00:02:52,470
sequence models of the most cutting edge the most powerful models that exist right now for natural language

32
00:02:52,470 --> 00:02:52,940
processing.

33
00:02:52,950 --> 00:02:55,890
And that's what we are going to be looking at.

34
00:02:56,370 --> 00:03:00,960
So as you'll see throughout this course we will make our way through the natural language processing

35
00:03:00,960 --> 00:03:05,700
side of things and to deepen O.P. and then we'll go into sequence to sequence.

36
00:03:05,700 --> 00:03:08,000
It'll be a fun and exciting journey.

37
00:03:08,400 --> 00:03:13,470
And the other thing that I wanted to mention is you will also notice that throughout this course even

38
00:03:13,470 --> 00:03:17,820
though it's focused on Chadwell so we won't be talking about just chat bots.

39
00:03:17,850 --> 00:03:24,870
We'll be looking at different examples of how these models from here or from here and from here can

40
00:03:24,870 --> 00:03:29,260
be applied to different things because the applications are huge.

41
00:03:29,270 --> 00:03:37,190
We think we can apply them in a natural neuro machine translation we can apply them in image captioning

42
00:03:37,200 --> 00:03:44,010
we can apply them in speech recognition questions and answers summarization lots and lots of models

43
00:03:44,010 --> 00:03:49,650
so we will be looking at different ones and they will be of different types.

44
00:03:49,650 --> 00:03:54,570
So this map will come in handy as we go through the course and it will be popping up here and there.

45
00:03:54,690 --> 00:04:01,840
So I think it was very important for us to set the foundation right so that now we're ready to proceed.

46
00:04:02,100 --> 00:04:07,080
And I can't wait to see you on the next tutorial and until then enjoy the deep and natural language

47
00:04:07,080 --> 00:04:08,030
processing.
