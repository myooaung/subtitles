1
00:00:00,820 --> 00:00:06,550
Hello and welcome back to the course on deep learning now that we know a bit more about LACMA and how

2
00:00:06,550 --> 00:00:08,590
they work and what their architecture is like.

3
00:00:08,590 --> 00:00:15,190
Today we're going to look at some practical applications or specifically we're going to look at how

4
00:00:15,190 --> 00:00:21,910
Ellis DMD's work inside practical applications and really quite interesting and at the same time a big

5
00:00:21,910 --> 00:00:30,490
magical I would say so let's get started here we've got the system architecture and we are going to

6
00:00:30,490 --> 00:00:37,510
be to start if we're going to be looking at this tangent function and how it fires up.

7
00:00:37,540 --> 00:00:45,830
So according to some of the images we're going to be using are all from Andray card party's blog.

8
00:00:45,830 --> 00:00:52,590
So thank you very much to Andre for doing all of this amazing research so you can see here it's just

9
00:00:52,710 --> 00:00:58,290
incredible so there got some examples where they created some bogusly in your algebra using trained

10
00:00:58,290 --> 00:01:00,010
up LSD.

11
00:01:00,390 --> 00:01:03,480
And then lots and lots of stuff.

12
00:01:03,480 --> 00:01:06,340
So here these are the images we're going to be looking at just now.

13
00:01:06,570 --> 00:01:08,250
These ones as well.

14
00:01:08,490 --> 00:01:12,390
And there's lots of information here lots of comments so we check it out.

15
00:01:12,390 --> 00:01:16,750
We'll link to these at the end of the tutorial.

16
00:01:16,800 --> 00:01:20,250
This blog is called than reasonable effectiveness of recurrent neural networks.

17
00:01:20,490 --> 00:01:27,080
And the paper that Andrii published along with that will also link to that paper as well.

18
00:01:27,090 --> 00:01:30,480
So basically we're looking at the tangent function to start off with.

19
00:01:30,480 --> 00:01:37,160
And according to the paper minus one means it's going to be red plus one is going to be blue.

20
00:01:37,170 --> 00:01:39,330
So let's get started.

21
00:01:39,330 --> 00:01:47,730
All right so here's some text which was given to an R N N which learned to read text and kind of create

22
00:01:47,730 --> 00:01:50,220
tests and predict what text is coming.

23
00:01:50,220 --> 00:01:55,120
And here this is a snippet from the.

24
00:01:55,260 --> 00:02:01,040
War and Peace the huge novel by Tolstoy.

25
00:02:01,270 --> 00:02:05,810
And so here you can see that this neuron is activating.

26
00:02:06,180 --> 00:02:10,980
Well it is sensitive to as it says here sensitive to position in line so you can see that when you get

27
00:02:10,980 --> 00:02:14,580
towards the end of the line it's activating and how does it know when it's the end of the line.

28
00:02:14,580 --> 00:02:25,170
Well in this novel you have about 80 symbols per line approximately So it's counting how many symbols

29
00:02:25,170 --> 00:02:26,160
have passed.

30
00:02:26,340 --> 00:02:30,910
And that way it's trying to predict when the new line character is coming up because the new line newline

31
00:02:30,980 --> 00:02:32,350
is just a character.

32
00:02:32,420 --> 00:02:36,820
It's an invisible character and trying to predict where that character is going to appear.

33
00:02:36,870 --> 00:02:39,670
Then you've got a cell that turns on inside quotes.

34
00:02:39,810 --> 00:02:44,970
Well this is inside because I think it's actually outside quotes because here you see it says you mean

35
00:02:45,000 --> 00:02:47,700
to imply that I have nothing to eat out of.

36
00:02:47,700 --> 00:02:49,500
On the contrary I can supply.

37
00:02:49,530 --> 00:02:53,410
So basically somebody is talking and then say warmly replied Chicago.

38
00:02:53,600 --> 00:02:55,320
She shrugged.

39
00:02:56,850 --> 00:02:57,500
Chicago.

40
00:02:57,540 --> 00:02:59,580
Haven't read that in a long time.

41
00:03:00,000 --> 00:03:03,070
Who tried to buy who tried by every word.

42
00:03:03,070 --> 00:03:06,430
So basically this is the inside the quotes is outside the quotes.

43
00:03:06,450 --> 00:03:11,970
I'm not sure if this is correct but either case one way or another it's activating either inside the

44
00:03:11,970 --> 00:03:16,680
quotes or outside the quotes and you can see how this one is keeping track of what's happening and so

45
00:03:16,680 --> 00:03:23,340
very similar to what we discussed previously where we were keeping track of the subject when that would

46
00:03:23,340 --> 00:03:29,850
help us understand if the subject is male or female or we would be able to understand things like if

47
00:03:29,850 --> 00:03:33,640
it's a singular or plural and that would affect all verbs in our translation.

48
00:03:33,750 --> 00:03:38,400
Similar things so it's important to know if you're inside or outside of course because that affects

49
00:03:39,750 --> 00:03:40,570
the rest of the text.

50
00:03:40,600 --> 00:03:45,960
And for instance the easiest way you can think of that effective text that if you're inside the quotes

51
00:03:45,970 --> 00:03:51,390
then there has to be another quote to close the quotes so you anticipating another quote something that

52
00:03:52,860 --> 00:04:01,730
here's another one where the what the input was the code of the Linux operating system.

53
00:04:01,740 --> 00:04:10,710
And here you can see that a cell activates inside if statement so it's completely dormant everywhere

54
00:04:10,710 --> 00:04:10,910
else.

55
00:04:10,920 --> 00:04:15,510
But as soon as you have an if statement it activates if statement activates if statement it activates

56
00:04:16,010 --> 00:04:25,530
so and if it is active you can say it stops being active over here at the next actual body of the if

57
00:04:25,560 --> 00:04:32,580
statement so it's only active for the main part or for the condition of the if statement and you know

58
00:04:32,580 --> 00:04:38,700
that that can be important because you are anticipating that the condition is going to close off a bracket

59
00:04:38,700 --> 00:04:43,990
and then it's going to be a bracket curly brace to open up the body of the if statement.

60
00:04:44,370 --> 00:04:51,330
That's pretty cool and then here you've got another one where this sensitive cell is sensitive to how

61
00:04:51,330 --> 00:04:55,170
deep you are inside of a nested expression.

62
00:04:55,180 --> 00:05:01,450
So as you go deeper and you get the expression because more and more nested this cell keeps track of

63
00:05:01,450 --> 00:05:07,400
that so it's using this memory to keep trying and it's very important to remember that.

64
00:05:07,600 --> 00:05:11,730
And none of this is actually hard coded into the neural network.

65
00:05:11,740 --> 00:05:14,450
All of this is learned by the network itself.

66
00:05:14,450 --> 00:05:16,990
There are thousands and thousands thousands of iterations.

67
00:05:17,170 --> 00:05:26,710
It learns that OK I have this many hidden states I have and out of them I need to identify what's important

68
00:05:26,710 --> 00:05:31,310
in the text to keep track of and the identifies for instance in this case that it has.

69
00:05:31,340 --> 00:05:38,890
That's being understanding how deep you are inside and that statement is important and therefore it's

70
00:05:39,190 --> 00:05:40,400
a science.

71
00:05:40,400 --> 00:05:48,580
A One of its hidden states to keep track of that is so it has these resources hidden States or the actual

72
00:05:49,030 --> 00:05:55,250
memory cells and it assigns them to keep track of certain things based on what it thinks is important.

73
00:05:55,250 --> 00:06:01,120
So it's like it's really evolving like that on its own and deciding what's important what's not and

74
00:06:01,120 --> 00:06:04,010
how to allocate its resources to best complete the task.

75
00:06:04,030 --> 00:06:07,410
I think that's very fascinating.

76
00:06:07,630 --> 00:06:11,790
And then here's an example of a cell that you can't really understand what it's doing.

77
00:06:11,800 --> 00:06:16,840
And according to drake or Pathi about 95 percent of the cells are like this.

78
00:06:16,840 --> 00:06:22,170
They're doing something but it's just not not obvious to us what is happening.

79
00:06:22,180 --> 00:06:29,050
And it's like that example of CNN's where the filters or the features that we're looking out for there

80
00:06:29,380 --> 00:06:33,670
by the time they get to the last Lehre they are completely unrecognizable to the human eye.

81
00:06:33,670 --> 00:06:35,020
But they make sense to the machine.

82
00:06:35,020 --> 00:06:35,620
Same thing here.

83
00:06:35,620 --> 00:06:39,770
So most of the time line of time you can really tell what's going on.

84
00:06:39,790 --> 00:06:41,560
But those five some of the time those were the exam.

85
00:06:41,560 --> 00:06:49,870
So we looked at and they should be helpful to better understand what is kind of going on in in the neural

86
00:06:49,870 --> 00:06:54,220
network when it's processing for instance text.

87
00:06:54,910 --> 00:06:59,010
So again now we're back at our architecture.

88
00:06:59,230 --> 00:07:05,290
And now what we're going to be looking at is are we going to be looking at the actual outputs so we're

89
00:07:05,290 --> 00:07:06,720
going to be looking at this value.

90
00:07:06,730 --> 00:07:12,730
So after it's gone through the tangent or evolve or the output gate and now we're going to be looking

91
00:07:12,730 --> 00:07:14,990
at what's being produced over here.

92
00:07:15,040 --> 00:07:23,230
So here's an example again from car parties or undertake her panties blog and we're looking at a neural

93
00:07:23,320 --> 00:07:25,180
network that is reading.

94
00:07:25,180 --> 00:07:31,930
So this is a bit this is a bit more detailed so here it's not just showing us if it's active or not.

95
00:07:31,930 --> 00:07:35,820
You can see that you've got at the top if it's active or not.

96
00:07:35,860 --> 00:07:44,340
Every first line and then another five lines it is saying what's the neural network is predicting that's

97
00:07:44,350 --> 00:07:45,480
going to happen next.

98
00:07:45,520 --> 00:07:48,490
What the letter is going to come next what symbols are going to come next.

99
00:07:48,490 --> 00:07:50,700
So you've got the combination of the two here.

100
00:07:50,920 --> 00:07:53,640
And just by looking at this what what do you think is predicting.

101
00:07:53,640 --> 00:08:00,250
So I'm going to I'm going to like outline the colors green means for the top line.

102
00:08:00,250 --> 00:08:06,910
Green means active and blue means not active and red means a very likely prediction and lighted means

103
00:08:07,180 --> 00:08:08,250
like prediction.

104
00:08:08,260 --> 00:08:10,810
So let's talk about the top line.

105
00:08:10,810 --> 00:08:16,630
What do you think this specific hidden state in the neural network is looking out for when do you think

106
00:08:16,630 --> 00:08:17,530
it's being activated.

107
00:08:17,710 --> 00:08:19,310
Well I guess this one is pretty obvious.

108
00:08:19,330 --> 00:08:21,910
It's activating inside your else.

109
00:08:21,940 --> 00:08:29,530
So here you can see that inside w w w the hidden state is B is being activated just like we saw in the

110
00:08:29,530 --> 00:08:34,180
previous examples foran piece and the Linux kernel.

111
00:08:34,180 --> 00:08:38,860
Here you can see that it's been activated inside your house.

112
00:08:38,890 --> 00:08:41,830
But now we have some additional stuff to look at.

113
00:08:41,830 --> 00:08:45,190
Now we can see what it's actually predicting that's going to be next character.

114
00:08:45,190 --> 00:08:51,250
So for instance under this w you can see that it's predicting that the next character is going to be

115
00:08:51,250 --> 00:08:54,630
a W of the highest probability and it's correct.

116
00:08:54,640 --> 00:08:59,860
It is doubly that under this bill you can see that predicting another W and it is correct.

117
00:08:59,860 --> 00:09:03,940
And then under this w the whole this is what the whole neural network is predicting.

118
00:09:04,970 --> 00:09:11,120
Under this w what you're seeing is a dot and that is correct.

119
00:09:11,130 --> 00:09:12,010
It is predicted.

120
00:09:12,030 --> 00:09:20,010
So that's that's how it's gone up to here and then after this dot if it thinks the next letter letter

121
00:09:20,190 --> 00:09:20,750
will be a B.

122
00:09:20,760 --> 00:09:21,530
But it's actual.

123
00:09:21,540 --> 00:09:26,140
But now knowing that it's a why it thinks they're going to be in a now knowing it's an A and B.

124
00:09:26,170 --> 00:09:29,000
And you can see that these are less.

125
00:09:29,130 --> 00:09:32,790
These are not as red as these because it's not sure about this.

126
00:09:32,810 --> 00:09:33,600
That's fair enough.

127
00:09:33,600 --> 00:09:35,250
It could be absolutely any website.

128
00:09:35,250 --> 00:09:36,550
Why would it be a B.

129
00:09:36,640 --> 00:09:37,780
Could be any upside can't.

130
00:09:37,800 --> 00:09:38,740
You can't tell.

131
00:09:38,850 --> 00:09:39,540
Maybe it could.

132
00:09:39,540 --> 00:09:42,990
Based on the context but still it's very hard.

133
00:09:43,040 --> 00:09:50,450
But then when it gets to wide net new and E.W. it predicts and as if a very high likelihood and it is

134
00:09:50,450 --> 00:09:56,450
indeed an ass because it kind of knows that the word after you have new very likely you will have an

135
00:09:56,450 --> 00:10:04,430
s or based on everything it's learned before that in this particular type of text it's news is mentioned

136
00:10:04,430 --> 00:10:10,160
quite often because this is actually as far as I understand this is Wikipedia text and then you have

137
00:10:10,160 --> 00:10:12,280
a dot then after dot.

138
00:10:12,420 --> 00:10:13,280
It predicts this.

139
00:10:13,280 --> 00:10:21,390
See it is a C after a c prefix and oh it's an O After all pricks and it is an M and a slash and a slash.

140
00:10:21,390 --> 00:10:22,110
So beautiful.

141
00:10:22,110 --> 00:10:25,630
So and then it's done its job and it's not active anymore.

142
00:10:26,190 --> 00:10:31,140
And then this is what but this is what the neuron that we're looking at is not active on anymore but

143
00:10:31,140 --> 00:10:32,920
the neural network is still predicting things.

144
00:10:32,920 --> 00:10:34,890
So here we can see then.

145
00:10:34,980 --> 00:10:36,260
And so it's incorrect.

146
00:10:36,270 --> 00:10:39,350
And then after the and after that is incorrect at the end.

147
00:10:39,360 --> 00:10:44,240
So after he has eaten it predicts a G it's a G and then it's becomes more confident that it's a word.

148
00:10:44,250 --> 00:10:50,550
English language you can see that the predictions become more confident so after the L it didn't predict

149
00:10:50,600 --> 00:10:55,500
correctly but after the L.A. It's already predicting and G and so on and so on so on so it can actually

150
00:10:55,500 --> 00:11:01,180
predict the whole word based on just a few first letters website on and so on to see the actual neurons

151
00:11:01,200 --> 00:11:02,910
still dormant still dormant.

152
00:11:02,910 --> 00:11:08,140
And then here you've got again off we go W.W. is predicting.

153
00:11:08,670 --> 00:11:09,960
And this one is getting activated.

154
00:11:09,960 --> 00:11:15,060
This one was pretty interesting because it goes see oh and then present and but it's not an M it's a

155
00:11:15,060 --> 00:11:24,240
dot and it's like the neural network or this is the neural network is probably a bit upset at this stage

156
00:11:24,240 --> 00:11:26,330
thinking Whoa Where's my M.

157
00:11:26,460 --> 00:11:29,260
And then then it takes another shot it says OK.

158
00:11:29,280 --> 00:11:35,580
So probably you because that code that you k that's that's UK websites.

159
00:11:36,150 --> 00:11:46,110
But here instead of you it's got an eye and an L for Israel that code i l and therefore is kind of like

160
00:11:46,120 --> 00:11:48,950
didn't guess this then but it probably is.

161
00:11:48,980 --> 00:11:53,130
It's not even trying to guess because he's already been trying to guess and I because from what it's

162
00:11:53,130 --> 00:11:58,650
learned that called that you k are made much more likely to come up than dot co.

163
00:11:58,740 --> 00:12:04,220
I l and we're not even looking at these other ones here which you could look at them.

164
00:12:04,800 --> 00:12:09,360
They are you know the second best guess a third best guess fourth and fifth which you can see that they

165
00:12:09,360 --> 00:12:10,330
are not that red.

166
00:12:10,380 --> 00:12:20,450
And what it would it will always put the highest likeliest guess at the top on the second line overall.

167
00:12:20,520 --> 00:12:25,650
So there we go this is how to look at these pictures that Andray has created.

168
00:12:25,650 --> 00:12:29,190
And for more check out his blog Carpathian get harder.

169
00:12:29,190 --> 00:12:34,980
I know there's a couple more of these examples there and more of the previous examples I looked at.

170
00:12:35,360 --> 00:12:43,620
And so hopefully this is helpful to show you what's going on inside the neural network when it's when

171
00:12:43,620 --> 00:12:50,400
it's thinking and when it's processing information in terms of references and additional reading we've

172
00:12:50,400 --> 00:12:57,240
got Andre her party blog and we'll link to it in the resources for this course.

173
00:12:57,300 --> 00:13:00,660
This is this is your L otherwise.

174
00:13:00,840 --> 00:13:09,450
And also we've got and raker Party and others research paper which was published in 2015 it's called

175
00:13:09,460 --> 00:13:13,020
visualizing and understanding recurrent networks.

176
00:13:13,020 --> 00:13:20,520
Very interesting read actually because like you if you're there's not too much math then you can probably

177
00:13:20,520 --> 00:13:20,960
skip it.

178
00:13:20,970 --> 00:13:29,630
But even the insights parts and chapters are very interesting they make you kind of feel that the.

179
00:13:29,790 --> 00:13:31,290
And they say this in the paper that they like.

180
00:13:31,290 --> 00:13:37,260
Neuroscientists trying to understand what's going on so they open up the brain of the neural network

181
00:13:37,290 --> 00:13:44,080
and monitor what's happening in one specific neuron or other different or different neurons.

182
00:13:44,250 --> 00:13:51,180
And you actually feel from the language the way that it's written that it's as if they're exploring

183
00:13:51,180 --> 00:13:57,270
some alien as if they're exploring some kind of extraterrestrial being and how it thinks.

184
00:13:57,270 --> 00:14:03,030
Because if you think about it humans created these LSD atoms and are and and these are just things that

185
00:14:03,030 --> 00:14:10,600
work on our computers but because they are so advanced because they involve so many different elements

186
00:14:10,600 --> 00:14:18,090
so many different parts to them and they're so complex we now need to after we've created them now we

187
00:14:18,090 --> 00:14:25,200
need to study them as if they're separate beings separate something something that exists on its own

188
00:14:25,270 --> 00:14:31,680
and it's just reading through it's kind of like if you think of it that way gives you this mysterious

189
00:14:31,680 --> 00:14:36,290
feeling or magic if you feel like it's something magical is happening and sometimes you feel that a

190
00:14:36,360 --> 00:14:41,160
few more years or maybe a decade from now these things are going to be able to think completely on their

191
00:14:41,160 --> 00:14:41,890
own.

192
00:14:41,910 --> 00:14:49,250
So if if you want to have some fun reading a research paper this one's pretty cool I think maybe you

193
00:14:49,250 --> 00:14:51,460
know you don't have to read the math skip the math.

194
00:14:51,470 --> 00:14:54,320
I didn't I didn't really dig into the math myself.

195
00:14:55,160 --> 00:14:57,040
Yeah so that's pretty much it.

196
00:14:57,070 --> 00:15:02,550
Hope you enjoyed today's tutorial and hopefully that gives you a bit of a better understanding of how

197
00:15:02,940 --> 00:15:06,390
the architecture actually plays out in practice.

198
00:15:06,720 --> 00:15:08,260
And I look forward to seeing you next then.

199
00:15:08,280 --> 00:15:09,900
Until then enjoy deep learning.
