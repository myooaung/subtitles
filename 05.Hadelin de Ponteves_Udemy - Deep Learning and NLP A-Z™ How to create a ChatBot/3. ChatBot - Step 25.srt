1
00:00:00,680 --> 00:00:07,040
Hello and welcome to part three training the SEC to sec Merrill we just built a brain in the previous

2
00:00:07,040 --> 00:00:12,980
part part to building the success moral and now it's time to train this brain and this part three to

3
00:00:12,980 --> 00:00:17,270
make it smart or at least able to talk able to chat with us.

4
00:00:17,270 --> 00:00:19,640
So for this tutorial everything's going to be fine.

5
00:00:19,640 --> 00:00:25,400
We're going to set some values for the hyper parameters that will be used during the training obviously

6
00:00:25,700 --> 00:00:31,520
and that you will be able to change if you want to experiment more and try to improve the results.

7
00:00:31,520 --> 00:00:39,080
I remind that machinery in deep learning AI and also of course deep in our be a lot of the work is experimentation

8
00:00:39,320 --> 00:00:44,680
tweaking the parameters to try to find the best model that will lead to the best accuracy.

9
00:00:44,840 --> 00:00:50,020
And eventually in our case the best shot but so we're going to choose some values here.

10
00:00:50,030 --> 00:00:53,060
I'm not saying these are the best values at the end of this course.

11
00:00:53,060 --> 00:00:59,690
We will actually try to improve the model by either tweaking the parameters or improving the data processing

12
00:00:59,930 --> 00:01:03,710
or even improving the architecture of the sector sick brain of our chat.

13
00:01:03,710 --> 00:01:10,490
But but these are at least relevant values that are most of the time the values used in the architecture

14
00:01:10,490 --> 00:01:13,220
of the neural networks you can find online.

15
00:01:13,220 --> 00:01:20,600
So let's do this let's set these parameters and we're going to start with the number of epochs.

16
00:01:20,660 --> 00:01:27,680
So I remind in that book is the whole process of getting the batches of input into the networks then

17
00:01:27,680 --> 00:01:33,920
for propagating them inside the uncurse get the states then for propagating the uncurse States with

18
00:01:33,920 --> 00:01:40,490
the target inside the Dakota record renewal that work to get the final output first if I don't output

19
00:01:40,490 --> 00:01:47,630
scores with then the final answer as predicted by the chat but and then back propagating the lust generated

20
00:01:47,630 --> 00:01:53,930
by the outputs and the target back into the neural network and updating the weights towards the direction

21
00:01:53,930 --> 00:01:57,800
of a better ability for the chatbot to speak like a human.

22
00:01:57,980 --> 00:02:04,040
So a netbook is basically one whole iteration of the training and we're going to choose one hundred

23
00:02:04,050 --> 00:02:04,850
epochs.

24
00:02:04,850 --> 00:02:08,000
However if the training takes too long on your computer.

25
00:02:08,090 --> 00:02:15,140
Well I recommend to choose less books like 50 but not too low because we really need a lot of books

26
00:02:15,410 --> 00:02:18,270
to get some correct results.

27
00:02:18,320 --> 00:02:20,280
So airbags equals 100.

28
00:02:20,390 --> 00:02:28,510
Then we'll take care of the batch size and we'll choose a batch size to start with 64.

29
00:02:28,850 --> 00:02:32,740
And again if the training takes too long you can try 128.

30
00:02:32,780 --> 00:02:42,000
This will get basically more questions and get answers in your batches to train the new network to batch

31
00:02:42,010 --> 00:02:43,640
size across 64.

32
00:02:43,640 --> 00:02:54,020
Then we have to choose a value for the Arnon size the size of the Arnon and we'll choose 512 then the

33
00:02:54,020 --> 00:02:56,500
number of layers.

34
00:02:56,570 --> 00:03:03,140
The number of layers which is how many layers you have in the end co-direct renew that work and also

35
00:03:03,140 --> 00:03:05,320
the decoder Recker a new network.

36
00:03:05,450 --> 00:03:10,790
The number of layers and we're going to choose three layers three layers and we'll probably change that

37
00:03:10,880 --> 00:03:14,600
at the end of the implementation when trying to improve the model.

38
00:03:14,630 --> 00:03:25,790
So non-players all three then we have to choose the encoding embedding size which I remind is the number

39
00:03:25,790 --> 00:03:32,570
of columns in your embeddings matrix that is the number of columns you want to have for the Billings

40
00:03:32,600 --> 00:03:33,500
values.

41
00:03:33,530 --> 00:03:40,000
Where in this matrix each line corresponds to each token in the whole corpus of the questions.

42
00:03:40,160 --> 00:03:41,790
So encoding and building size.

43
00:03:41,870 --> 00:03:49,460
And we're going to choose a size of 512 meaning that we're going to have 512 columns in this embeddings

44
00:03:49,460 --> 00:03:50,370
matrix.

45
00:03:50,600 --> 00:03:57,110
Then I'm going to copy this because our next hyper parameter is going to be the not the encoding embedding

46
00:03:57,110 --> 00:04:06,800
size but the decoding and building size for which will also choose 512 decoding embeddings size then

47
00:04:07,190 --> 00:04:08,290
their learning rate.

48
00:04:08,300 --> 00:04:10,920
Very important hyper parameter.

49
00:04:10,930 --> 00:04:15,000
The learning rate is quite tricky We never know exactly what to choose.

50
00:04:15,020 --> 00:04:17,440
It must not be too high it must not be too low.

51
00:04:17,510 --> 00:04:18,480
If it is too high.

52
00:04:18,510 --> 00:04:23,330
Basically your model will learn too fast and will therefore not learn how to speak properly.

53
00:04:23,480 --> 00:04:24,850
And if it's too low.

54
00:04:24,860 --> 00:04:30,540
Well basically it will take ages for the model to learn how to speak properly so learning rate.

55
00:04:30,650 --> 00:04:31,810
That's a tricky one.

56
00:04:31,820 --> 00:04:34,620
We're going to start with 0.01.

57
00:04:34,850 --> 00:04:40,760
I'll do a lot of experimenting so make sure to check out you know the checkpoint lectures which are

58
00:04:40,760 --> 00:04:45,920
actually articles but the checkpoint lectures at the end of each part where I share the code because

59
00:04:45,920 --> 00:04:51,040
basically I give the last version of the code that I always try to improve.

60
00:04:51,110 --> 00:04:58,190
So in these checkpoint lectures you will always have the best version of the code with the best hyper

61
00:04:58,190 --> 00:05:03,030
parameters and the best improvement the best corrections because at the time I'm speaking this is the

62
00:05:03,030 --> 00:05:04,850
first time I implement this.

63
00:05:04,920 --> 00:05:09,670
And therefore you can imagine that by trying to improve it and improve it and improve it again while

64
00:05:09,690 --> 00:05:13,780
the code and therefore the child but will definitely improve over time.

65
00:05:14,010 --> 00:05:20,400
So I'm starting here with a learning rate of 0.1 but don't be surprised if you find another learning

66
00:05:20,400 --> 00:05:23,560
rate in the implementation given in the checkpoint.

67
00:05:23,580 --> 00:05:26,030
So learning raising calls 0.01.

68
00:05:26,160 --> 00:05:30,780
And now we're going to have to choose learning rates.

69
00:05:31,950 --> 00:05:39,450
So that's basically by which present age the learning rate is reduced over the iterations of the training

70
00:05:39,720 --> 00:05:44,910
because you know we want to start with a certain learning rate but then want to play decay over the

71
00:05:44,910 --> 00:05:51,330
iterations of the training to reduce the learning rate so that it can learn in more depth the logic

72
00:05:51,450 --> 00:05:53,230
of human conversations.

73
00:05:53,250 --> 00:05:57,790
In our case or in general the correlations identified in the data set.

74
00:05:57,990 --> 00:06:01,380
So learning read decay very important parameter as well.

75
00:06:01,590 --> 00:06:07,130
And we're going to choose a common value that you will find in most implementations.

76
00:06:07,130 --> 00:06:11,190
We're going to choose a decay of 90 percent 0.9.

77
00:06:11,190 --> 00:06:17,190
If you look at the implementations and get Herb or on any other open source platforms you will either

78
00:06:17,190 --> 00:06:24,210
find a decay of 0.9 or one meaning that there is no decay but we're going to apply a decay here which

79
00:06:24,210 --> 00:06:32,190
will be 90 percent then next type of parameter is a minimum of the learning rates we want to apply because

80
00:06:32,490 --> 00:06:37,740
since we are applying the decay the learning rate will be reduced over the iterations and we have many

81
00:06:37,740 --> 00:06:42,470
iterations so we don't want the learning rate to reach a too low value.

82
00:06:42,630 --> 00:06:49,560
And for this we're going to set a minimum learning rate and this hybrid parameter for that will be men

83
00:06:50,280 --> 00:06:57,730
learning rates which we will set to be called to 0 point 0 or 1.

84
00:06:57,840 --> 00:07:02,310
So that's a very low minimum learning rate but at least the training will end for sure.

85
00:07:02,310 --> 00:07:09,630
And besides we will try to add some early stopping in case there is literally no improvement during

86
00:07:09,630 --> 00:07:10,620
the training.

87
00:07:10,620 --> 00:07:14,800
So no worries with that and early stopping the training will come to an end.

88
00:07:15,180 --> 00:07:22,300
And finally we have one last type hyper parameter to include here which is the key prob. hyper parameter.

89
00:07:22,540 --> 00:07:24,870
And for this I'd like to show you something.

90
00:07:25,050 --> 00:07:31,740
So I'm going to go to Google now and the thing I wanted to show you is this paper by the best of the

91
00:07:31,740 --> 00:07:32,420
best.

92
00:07:32,460 --> 00:07:37,380
Jeffrey Hinton Jeffrey Henzen is the guru of artificial intelligence.

93
00:07:37,380 --> 00:07:42,860
He is the top one reference in deep learning artificial intelligence and all the research around it.

94
00:07:42,860 --> 00:07:48,570
And actually the new very promising capsule networks are invented by Geoffrey Hinton.

95
00:07:48,570 --> 00:07:54,690
So this is a paper by Geoffrey Hinton called drop out a simple way to prevent neural networks from overfishing

96
00:07:55,140 --> 00:08:01,320
and we're going to trust the value recommended for the drop out rate in this paper.

97
00:08:01,470 --> 00:08:07,380
If we scroll down a little bit well first you have some nice explanation on what it is about and especially

98
00:08:07,380 --> 00:08:14,790
here you see you implying dropout that neuron is present with a certain probability p which is of course

99
00:08:14,850 --> 00:08:20,050
our key prob. probability the parameter that controls to drop out rate.

100
00:08:20,070 --> 00:08:23,810
But mostly that is equal to one minus the dropout rate.

101
00:08:23,820 --> 00:08:30,350
And that is for the training time there is we only deactivate the ruins with a probability one man's.

102
00:08:30,690 --> 00:08:36,000
During the training time and then a test time of course all the neurons are always present.

103
00:08:36,000 --> 00:08:41,610
That's why I remember when getting our test predictions we didn't apply drop out.

104
00:08:41,640 --> 00:08:44,380
That's because the neurons are always present.

105
00:08:44,400 --> 00:08:51,060
So basically this keep drub parameter is one minus the dropout rate it's the probability of a neuron

106
00:08:51,270 --> 00:08:53,520
to be present during the training.

107
00:08:53,550 --> 00:09:00,450
And so if we scroll down even more we will find a recommended value for this dropout rate right here.

108
00:09:00,450 --> 00:09:11,280
Page 1133 we see that dropping out 20 percent of the input units and 50 percent of the hidden units

109
00:09:11,550 --> 00:09:14,070
was often found to be optimal.

110
00:09:14,070 --> 00:09:20,100
So this keep up hyper parameter for which we have to choose value is actually for the Hittleman it's

111
00:09:20,220 --> 00:09:27,390
not input units and therefore we're going to trust Jeffrey Henton for the first try of our model.

112
00:09:27,400 --> 00:09:34,350
We're going to choose a dropout rate of 50 percent which therefore is a probability of 50 percent as

113
00:09:34,350 --> 00:09:38,340
well because the probability is 1 minus the drop out rate.

114
00:09:38,520 --> 00:09:45,030
So that means that during the training the neurons of our record a neural networks will be present with

115
00:09:45,090 --> 00:09:48,690
a probability of 50 percent and we'll see how it goes.

116
00:09:48,780 --> 00:09:53,100
But I feel safe using Jeffrey Hinton's research results.

117
00:09:53,270 --> 00:10:02,820
So let's go back to Python and let's include this keep probability high parameters like that keep probability

118
00:10:03,060 --> 00:10:09,030
on purpose you know instead of keep prob because this is the name in the tensor of API and therefore

119
00:10:09,030 --> 00:10:13,320
you must introduce it this way keep probability keep prob won't work.

120
00:10:13,560 --> 00:10:14,840
So keep probability.

121
00:10:15,000 --> 00:10:22,260
And this we will choose as we said 50 percent that is 0 point five and there we go.

122
00:10:22,260 --> 00:10:29,220
We are done with our hyper parameters we could choose some more happy parameters if we want to develop

123
00:10:29,250 --> 00:10:30,510
even more the morals.

124
00:10:30,540 --> 00:10:36,110
These are not the only hyper parameters but that's definitely the most important ones for our model.

125
00:10:36,120 --> 00:10:38,200
And again feel free to tweak them.

126
00:10:38,310 --> 00:10:44,070
I will tweak them even more I will do some research to try to improve the chatbot as much as possible.

127
00:10:44,160 --> 00:10:48,800
But for starters these should be rather than the values and the way we will test them in the end.

128
00:10:48,810 --> 00:10:51,980
In part for testing the second set model.

129
00:10:52,020 --> 00:10:56,250
So now let's select all these and execute.

130
00:10:56,250 --> 00:10:57,000
There we go.

131
00:10:57,000 --> 00:11:01,860
We now have our hyper parameters and so we were ready to move on to the next steps.

132
00:11:01,860 --> 00:11:04,630
So the next steps are going to be quite quick and easy.

133
00:11:04,680 --> 00:11:08,110
We'll just basically prepare the training so that we are ready for it.

134
00:11:08,190 --> 00:11:09,600
That's going to be a big part.

135
00:11:09,690 --> 00:11:15,330
Try to get some good energy for it because we are going to have a quite long journey until then and

136
00:11:15,330 --> 00:11:16,080
already been on the.
