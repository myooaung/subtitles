WEBVTT
1
00:00:00.450 --> 00:00:05.670
Hello and welcome to this new to toile I'm pretty excited about this one because as you can see by the

2
00:00:05.670 --> 00:00:11.490
title of the code section we're going to get very close to the training because indeed in this tutorial

3
00:00:11.500 --> 00:00:16.800
we're going to set up the lust for the optimizer and we're going to apply some great unclipping great

4
00:00:16.830 --> 00:00:19.110
tipping for those of you who don't know what that is.

5
00:00:19.110 --> 00:00:25.230
This is a technique actually some operations that will cap the gradient in the graph between a minimum

6
00:00:25.230 --> 00:00:31.240
value and a maximum value and that's to avoid some exploding or vanishing gradient issues.

7
00:00:31.290 --> 00:00:36.670
And so we're going to make sure to avoid these issues by applying gradient tipping to our optimizer.

8
00:00:36.780 --> 00:00:43.800
And so we are going to define a new scope here which will contain two elements to final ultimate elements

9
00:00:43.800 --> 00:00:49.460
that we'll use for the training which are going to be the last error and the optimizer with grading

10
00:00:49.500 --> 00:00:50.130
tipping.

11
00:00:50.250 --> 00:00:55.380
The last error is going to be based on a weighted cross entropy loss error which is the most relevant

12
00:00:55.380 --> 00:01:01.320
loss to use when dealing with sequences as we are doing right now and in general when dealing with deep

13
00:01:01.350 --> 00:01:07.680
and LP and the optimizer that we're going to use will be first an atom optimizer which is one of the

14
00:01:07.680 --> 00:01:10.380
best optimizers for us to cast a great descent.

15
00:01:10.620 --> 00:01:17.730
And then we will apply gradiently ping to that optimizer to avoid exploding or vanishing great issues.

16
00:01:17.730 --> 00:01:21.180
And so let's do this let's define the scope.

17
00:01:21.270 --> 00:01:22.890
You already know how to define a scope.

18
00:01:22.890 --> 00:01:25.460
We did that for the decoding scope.

19
00:01:25.590 --> 00:01:34.230
So we need to take our sense of library and then the name scope function to which we need to input the

20
00:01:34.230 --> 00:01:36.350
name of this scope in quotes.

21
00:01:36.450 --> 00:01:43.390
And we're going to choose the name up Tim zation are right that our optimization scope and inside the

22
00:01:43.390 --> 00:01:48.810
scope we're going to set up these two essential tools for the training that is the last error.

23
00:01:48.900 --> 00:01:51.560
And the optimizer with great unclipping applied.

24
00:01:51.870 --> 00:01:54.170
So let's start with the last error.

25
00:01:54.390 --> 00:01:56.220
We're going to introduce a new variable.

26
00:01:56.220 --> 00:02:03.690
Last error and that we will get it things to the sequence last function by tenths of a.

27
00:02:03.900 --> 00:02:08.350
So let's get this model T.F. then we get it from contrib.

28
00:02:08.490 --> 00:02:17.540
And then there we go we get our Sec 2 sec Mudgal from which we will get that sequence last function.

29
00:02:17.550 --> 00:02:22.500
All right so the sequences function and that's going to take several arguments the first two arguments

30
00:02:22.560 --> 00:02:29.000
as usual when dealing with the last function is going to be first the predictions and the target.

31
00:02:29.070 --> 00:02:34.230
And since right now we're preparing the training these are going to be our training predictions that

32
00:02:34.230 --> 00:02:36.190
we just got in the previous toil.

33
00:02:36.270 --> 00:02:44.580
So training predictions first and then the second argument is of course going to be the target and I'm

34
00:02:44.580 --> 00:02:48.830
going to put them right below so that you can see them better the targets.

35
00:02:48.990 --> 00:02:53.910
And that's of course because we're going to measure the last error between the training predictions

36
00:02:54.030 --> 00:02:55.450
and the targets.

37
00:02:55.530 --> 00:02:58.330
Right so that's the two most essential arguments.

38
00:02:58.440 --> 00:03:02.090
But then we have some new arguments which will correspond to the weight.

39
00:03:02.160 --> 00:03:06.660
And so we have to prepare this tensor of weights initialize them all to once.

40
00:03:06.810 --> 00:03:12.800
And of course with the right time engines so to initialize the weight as intensive ones.

41
00:03:12.960 --> 00:03:15.260
Well we're going to use the tens of the library.

42
00:03:15.300 --> 00:03:21.060
And then one of its function very useful function for that which is the one function the one function

43
00:03:21.060 --> 00:03:27.360
that takes on one argument that we have to put in square brackets and that's of course the shape as

44
00:03:27.360 --> 00:03:28.080
we can see here.

45
00:03:28.080 --> 00:03:29.900
The other ones are optional.

46
00:03:30.120 --> 00:03:32.260
That's of course the shape of this tensor.

47
00:03:32.280 --> 00:03:34.200
That is the dimensions of this tensor.

48
00:03:34.290 --> 00:03:39.100
And so we're going to enter first the number of lines and then the number of columns.

49
00:03:39.120 --> 00:03:46.530
So let's go with the number of lines the number of lines of distances should be our input shape of index

50
00:03:46.850 --> 00:03:47.750
0.

51
00:03:47.760 --> 00:03:49.130
That's the first dimension.

52
00:03:49.260 --> 00:03:55.560
And then the second dimension is going to be the sequence length and that's perfect.

53
00:03:55.560 --> 00:04:01.900
We already got previously the input shape right here when getting the shape of the input sensor.

54
00:04:02.130 --> 00:04:05.600
And we got the sequence length just before our eyes.

55
00:04:05.730 --> 00:04:11.940
And that should be the diamond sions of the center of weights all initialized to once perfect.

56
00:04:11.940 --> 00:04:14.030
That gives us the last error.

57
00:04:14.220 --> 00:04:17.130
So that's our first essential tool for the training.

58
00:04:17.160 --> 00:04:22.190
And now we're going to prepare the second essential tool which is the optimizer.

59
00:04:22.470 --> 00:04:25.390
And so we're going to do that in three steps.

60
00:04:25.440 --> 00:04:31.410
First we'll get the atom optimizer which will be an object of the atom optimizer class.

61
00:04:31.410 --> 00:04:38.370
Then second step we will prepare gradiently being that we will cap our gradients between the minimum

62
00:04:38.370 --> 00:04:45.420
value and the maximum value and then eventually we will apply gradiently being to our optimizer.

63
00:04:45.420 --> 00:04:45.980
All right.

64
00:04:46.110 --> 00:04:47.370
So let's go with the first step.

65
00:04:47.370 --> 00:04:50.310
The first step is to get the atom optimizer.

66
00:04:50.440 --> 00:04:57.450
And therefore I am introducing a new variable here up to Mizer and this optimizer is going to be an

67
00:04:57.570 --> 00:05:06.570
object at an optimizer class which is a class by tensor flow and taken from the Mudgal train ride train

68
00:05:06.870 --> 00:05:15.540
from which we get the atom optimizer which is right here atom optimizer and this atom optimizer as every

69
00:05:15.540 --> 00:05:21.810
optimizer class takes one fundamental element which is the learning rates the learning rate that we

70
00:05:21.880 --> 00:05:28.900
are already prepared before in the hyper parameters right here learning rate equals 0.01.

71
00:05:28.920 --> 00:05:34.190
We're starting with this value and if this doesn't work well we will tweak some other values.

72
00:05:34.230 --> 00:05:43.420
So let's put the learning rate inside the arguments of our optimizer learning rate and there we go.

73
00:05:43.440 --> 00:05:51.240
We have our optimizer and now next up next step as I said is to clip all our gradients.

74
00:05:51.240 --> 00:05:56.970
You know we have one gradient per neuron in the neural networks and for each of these neuron we compute

75
00:05:57.120 --> 00:06:03.930
the gradient of the last error with respect to the weight of that neuron and all these gradients are

76
00:06:03.930 --> 00:06:07.070
into a graph and are attached to a variable.

77
00:06:07.080 --> 00:06:07.580
All right.

78
00:06:07.680 --> 00:06:13.530
So now the first thing that we need to do is compute all these gradients and to compute them that's

79
00:06:13.530 --> 00:06:15.600
where our optimizer comes in.

80
00:06:15.600 --> 00:06:20.700
We're going to use a method from that optimizer which is the compute gradient method.

81
00:06:20.780 --> 00:06:27.240
And that will compute these gradients of the last error with respect to the weights of each of the neurons.

82
00:06:27.240 --> 00:06:30.990
So let's get them let's get our gradients.

83
00:06:30.990 --> 00:06:37.080
Just introducing a new variable here Graydon's and we're going to get them as we said thanks to our

84
00:06:37.410 --> 00:06:45.540
optimizer from which we apply to compute grades and function and this computes graden function takes

85
00:06:45.780 --> 00:06:52.530
one essential argument which is the last error of which we want to compute the gradients with respect

86
00:06:52.530 --> 00:06:56.430
to the weights and computing gradients is always with respect to the weight.

87
00:06:56.550 --> 00:07:01.170
So we don't have to put the weights as argument but just the last area of which we want to compute the

88
00:07:01.170 --> 00:07:09.150
gradient and therefore I'm entering here this last error which we got in the previous step last area.

89
00:07:09.540 --> 00:07:11.730
So that computes all the gradients of the graph.

90
00:07:11.760 --> 00:07:15.330
And now let's cap each one of these gradients.

91
00:07:15.510 --> 00:07:22.290
And so I'm going to introduce a new viable that I'm going to call clipped gradients clipped gradients

92
00:07:22.470 --> 00:07:29.520
that are going to be our gradients clipped by value and to show you what this means I'm going to walk

93
00:07:29.520 --> 00:07:35.310
you through the Tenterfield documentation right now to show you two pages gradiently being that I will

94
00:07:35.310 --> 00:07:41.040
share with you and the resources of this lecture and that explains to you what gradiently been is in

95
00:07:41.040 --> 00:07:41.900
more details.

96
00:07:42.150 --> 00:07:47.350
And this clip by value function that we're about to use to keep our gradients.

97
00:07:47.430 --> 00:07:53.730
So as you can see this function takes three arguments the first one is the tensor of gradient that is

98
00:07:53.820 --> 00:07:57.730
each of the tens of gradient among all the gradients in the graph.

99
00:07:57.780 --> 00:08:02.640
Then the second argument is a minimum value below which the Graylands value cannot go.

100
00:08:02.760 --> 00:08:07.980
And the third argument is the maximum value above which to Graydon's value cannot go will choose the

101
00:08:07.980 --> 00:08:10.800
minimum to be minus five and the maximum to be five.

102
00:08:10.950 --> 00:08:16.530
And that will keep our Graydon's in a way to avoid vanishing or exploding green issues.

103
00:08:16.770 --> 00:08:24.320
So let's do this let's go back to Python and let's flip all our gradients so clipped gradients equals.

104
00:08:24.430 --> 00:08:29.910
And now since we want to do that for each of the gradients of the graph we're going to have to make

105
00:08:29.910 --> 00:08:33.100
a full up you know to loop over each one of these gradients.

106
00:08:33.180 --> 00:08:38.250
And as I said a gradient in the graph is attached to a variable and therefore we are going to have two

107
00:08:38.250 --> 00:08:43.010
new variables to the tensor itself and the variable attached to it.

108
00:08:43.110 --> 00:08:47.250
And we're not going to make a full loop as we did before you know we are going to make this for loop

109
00:08:47.250 --> 00:08:53.550
into some square brackets which is a trick technique to get these gradients directly inside a list and

110
00:08:53.550 --> 00:08:59.430
therefore as we said we're going to start a for loop for we're going to loop over the two elements of

111
00:08:59.490 --> 00:09:05.730
each of the gradients in the graph which are first the gradient tensor which I'm calling the great tensor

112
00:09:06.240 --> 00:09:13.530
and it's variable that I'm calling the grad viable Gretton so Gred variable which basically means for

113
00:09:13.590 --> 00:09:22.530
each gradient in all our gradients that we got in the previous line it was Graydon's here that we computed

114
00:09:22.620 --> 00:09:24.660
things to compute graden function.

115
00:09:24.780 --> 00:09:26.580
Thanks to our optimizer.

116
00:09:26.580 --> 00:09:29.880
So for each of these gradients among all the gradients.

117
00:09:29.970 --> 00:09:35.670
Well we're going to do something but before we do that we have to add some safety here to make sure

118
00:09:36.150 --> 00:09:45.110
that the sensor of the gradient is existing and therefore I'm adding If the answer is not None.

119
00:09:45.270 --> 00:09:46.980
That is if it is existing.

120
00:09:47.070 --> 00:09:49.320
So now we have to loop perfect.

121
00:09:49.320 --> 00:09:55.290
We make sure that the great intenser is existing and therefore let's specify what we want to do for

122
00:09:55.380 --> 00:10:01.280
each of these gradients and we have to specify this before the loop and that's well we use the clip

123
00:10:01.370 --> 00:10:09.430
by value function to clip each one of our great intenser but to keep track of each gradient.

124
00:10:09.630 --> 00:10:15.380
We're going to form a couple here and this couple will actually be the gradient as a whole in the graph

125
00:10:15.560 --> 00:10:20.120
which you know is composed of two elements to grade intenser and the variable and therefore And this

126
00:10:20.120 --> 00:10:25.970
couple of parenthesis here to introduce first the tensor that we're going to clip things to the by value

127
00:10:25.970 --> 00:10:31.610
function and then the variable which is the other variable in our loop.

128
00:10:31.610 --> 00:10:39.290
So first let's get the first element of the gradient as a whole which is the grad tensor and that we

129
00:10:39.290 --> 00:10:48.290
want to clip by value applying the clip by value function that we take from tensor for of course T.F.

130
00:10:48.410 --> 00:10:58.780
that Kibei value that will clip by value this great tensor from as we said minus five to plus five.

131
00:10:58.970 --> 00:11:01.370
And it's better to use floats here.

132
00:11:01.370 --> 00:11:09.920
So I'm adding a dot decimal from minus five point zero to plus five point zero and this clip's the tensor

133
00:11:10.190 --> 00:11:15.230
of the gradient we're dealing with right now in the loop but which will be done for all the great answers

134
00:11:15.380 --> 00:11:17.090
of the gradients in the graph.

135
00:11:17.120 --> 00:11:17.420
All right.

136
00:11:17.420 --> 00:11:23.600
And now the second element of the gradient is its variable that we called Gred variable and therefore

137
00:11:23.600 --> 00:11:25.560
I'm including the second element here.

138
00:11:25.650 --> 00:11:27.500
Great variable.

139
00:11:27.500 --> 00:11:33.560
All right so basically in this loop here we're just clipping the tensors of each of the gradients in

140
00:11:33.560 --> 00:11:38.960
the graph that we computed before thanks to the complete Graydon's method of our optimizer.

141
00:11:38.960 --> 00:11:44.660
All right so great so now we have our clipped gradients and we have one final step to do which is to

142
00:11:44.660 --> 00:11:48.500
apply gradient tipping to our optimizer.

143
00:11:48.500 --> 00:11:57.420
So let's do this let's get our new optimizer that we're going to call optimizer gradient clipping which

144
00:11:57.510 --> 00:12:02.770
exactly is the optimizer on which grading clipping was applied and to apply it.

145
00:12:02.850 --> 00:12:09.510
Well we need to take our previous optimizer the atom optimizer object that we created from the atom

146
00:12:09.570 --> 00:12:15.210
optimizer class and then from this object we're going to apply one of the methods of the optimizer class

147
00:12:15.210 --> 00:12:17.890
which is this one Plag regions.

148
00:12:18.090 --> 00:12:23.370
And of course this is going to take one arguments which are going to be the new gradients that we got

149
00:12:23.400 --> 00:12:31.030
and that we kept in the previous line and therefore I'm entering here are clipped gradients.

150
00:12:31.320 --> 00:12:32.370
Perfect.

151
00:12:32.370 --> 00:12:36.890
We just set up the last error the optimizer and gradiently.

152
00:12:37.020 --> 00:12:39.910
And that's a very good thing done for the training.

153
00:12:39.930 --> 00:12:43.410
So now we have a few steps left to do before the training.

154
00:12:43.500 --> 00:12:49.110
And in two or three tutorials we'll start the big four loop of the training.

155
00:12:49.130 --> 00:12:53.710
So now I'm going to leave you with this code section that I'm going to execute.

156
00:12:53.940 --> 00:12:55.710
And until then and Gelperin I'll be.
