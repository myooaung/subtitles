1
00:00:00,940 --> 00:00:04,080
Whether you're working with standard web applications or

2
00:00:04,080 --> 00:00:07,280
microserviced‑out serverless applications,

3
00:00:07,280 --> 00:00:11,240
basic log‑level monitoring usually leaves a lot to be desired.

4
00:00:11,240 --> 00:00:12,870
In surplus applications,

5
00:00:12,870 --> 00:00:16,090
you have the added task of having to find which

6
00:00:16,090 --> 00:00:18,700
services are calling which services.

7
00:00:18,700 --> 00:00:21,970
As you get more distributed with your architecture,

8
00:00:21,970 --> 00:00:24,820
monitoring only becomes more difficult.

9
00:00:24,820 --> 00:00:29,000
One solution to make things easier is to aggregate all your

10
00:00:29,000 --> 00:00:31,700
logs from each service to a common location.

11
00:00:31,700 --> 00:00:34,110
While this is always recommended,

12
00:00:34,110 --> 00:00:40,200
now with AWS X‑Ray, you have a new way to monitor your applications.

13
00:00:40,200 --> 00:00:44,880
X‑Ray collects data from your application as requests are handled and

14
00:00:44,880 --> 00:00:50,280
replied to. As a service or an application sends this data, X‑Ray uses a

15
00:00:50,280 --> 00:00:55,200
trace ID to build a picture of what requests are doing as a pass through

16
00:00:55,200 --> 00:00:59,490
different parts of your architecture. Because each request is using the same

17
00:00:59,490 --> 00:01:03,730
trace ID, X‑Ray can draw a map of how things are processed in your

18
00:01:03,730 --> 00:01:09,620
application and provide very specific data at each step. The data available

19
00:01:09,620 --> 00:01:14,830
are things like how often the step resulted in a success or failure, the

20
00:01:14,830 --> 00:01:19,960
total length of execution including a breakdown into child calls, and how

21
00:01:19,960 --> 00:01:24,610
many similar requests were made. When your code sends this data to X‑Ray,

22
00:01:24,610 --> 00:01:28,870
it doesn't send it directly to the service, but instead sends the

23
00:01:28,870 --> 00:01:33,550
data to an X‑Ray daemon process that buffers the data before

24
00:01:33,550 --> 00:01:36,440
uploading it to the actual X‑Ray service.

25
00:01:36,440 --> 00:01:41,240
This daemon comes preinstalled on Lambda and Elastic Beanstalk, but you

26
00:01:41,240 --> 00:01:45,340
will need to download it from AWS and run it if your application is

27
00:01:45,340 --> 00:01:49,940
running on a different service like EC2 or ECS.

28
00:01:49,940 --> 00:01:52,500
X‑Ray doesn't use every single request,

29
00:01:52,500 --> 00:01:56,640
but instead applies a sampling algorithm to only take a sample of the

30
00:01:56,640 --> 00:02:00,880
requests that are coming. By default, it includes one request per

31
00:02:00,880 --> 00:02:05,040
second and then 5% of the subsequent requests.

32
00:02:05,040 --> 00:02:08,810
This keeps costs low and has a good likelihood of accurately

33
00:02:08,810 --> 00:02:12,740
reflecting what's going on in your application as a whole.

34
00:02:12,740 --> 00:02:16,320
You can edit these sampling rules in the X‑Ray dashboard and even

35
00:02:16,320 --> 00:02:19,140
set different rules according to the service type.

36
00:02:19,140 --> 00:02:22,070
You might not ever need to change this configuration,

37
00:02:22,070 --> 00:02:25,890
but it's nice to have control at that level. Tracing is an

38
00:02:25,890 --> 00:02:32,000
important concept to understand in X‑Ray, so let's take a look at that in the next video.

