1

00:00:00,870  -->  00:00:05,000
Welcome back to the ultimate data science course and I am super excited about today.

2

00:00:05,000  -->  00:00:10,560
I've got an incredible tutorial prepared for you I'm going to cover off a very important topic and that

3

00:00:10,560  -->  00:00:13,680
is how to build models step by step.

4

00:00:13,680  -->  00:00:15,170
I couldn't hold myself.

5

00:00:15,170  -->  00:00:21,570
I had to add that extra line step by step to the name of the tutorial because that is exactly what we're

6

00:00:21,570  -->  00:00:22,760
going to be looking at.

7

00:00:22,980  -->  00:00:28,140
I'm going to give you a framework for several different methods and it's all going to be step by step

8

00:00:28,140  -->  00:00:28,380
.

9

00:00:28,380  -->  00:00:30,980
So let's jump straight into it.

10

00:00:31,050  -->  00:00:36,410
Do you remember the good old days when we had one dependent variable and one independent variable.

11

00:00:36,450  -->  00:00:41,630
Everything was easy and we just had a simple linear regression that we had to build.

12

00:00:41,640  -->  00:00:43,550
And everything worked great.

13

00:00:43,710  -->  00:00:48,210
But now in our Datto we have all these columns.

14

00:00:48,210  -->  00:00:54,360
Those easy days are gone now all of these columns are potential predictors for our dependent variable

15

00:00:55,170  -->  00:01:01,740
and there's just so many of them and we need to decide which ones we want to keep and which ones we

16

00:01:01,740  -->  00:01:08,090
want to throw out and you'll ask why do we need to throw out columns or do we need to get rid of data

17

00:01:08,100  -->  00:01:11,090
why can we just use everything in our model.

18

00:01:11,190  -->  00:01:14,100
Well I can think of two reasons of the top of my head.

19

00:01:14,100  -->  00:01:17,210
Number one is garbage in garbage out.

20

00:01:17,280  -->  00:01:25,020
If you throw in a lot of stuff in your model then your model will not be a good model it won't be reliable

21

00:01:25,020  -->  00:01:26,950
it won't be doing what it's supposed to be doing.

22

00:01:26,970  -->  00:01:28,490
It's going to be a.

23

00:01:28,560  -->  00:01:35,070
So to speak garbage model and number two at the end of the day you're going to have to explain these

24

00:01:35,070  -->  00:01:42,090
variables and understand the not just the math behind them but actually what it means that certain variables

25

00:01:42,090  -->  00:01:48,420
predict the behavior of your dependent variable and you will have to explain that to your executives

26

00:01:48,420  -->  00:01:51,360
to your boss to people you're presenting to.

27

00:01:51,360  -->  00:01:56,600
So if you have a thousand variables is not going to be practical to try and explain that so you want

28

00:01:56,600  -->  00:02:02,010
to keep only the very important ones the ones that actually predict something.

29

00:02:02,010  -->  00:02:04,470
So how do we construct a model.

30

00:02:04,460  -->  00:02:09,220
This is the process of building the model selecting the right variables.

31

00:02:09,240  -->  00:02:10,710
So how do we construct a model.

32

00:02:10,710  -->  00:02:15,840
Well there are five methods that we're going to discuss of building models.

33

00:02:15,840  -->  00:02:20,130
Number one is all in the Mattew is backward elimination.

34

00:02:20,130  -->  00:02:22,550
Number three is forward selection.

35

00:02:22,680  -->  00:02:25,550
And before is bi directional an elimination.

36

00:02:25,560  -->  00:02:28,350
And number five is score comparison.

37

00:02:28,350  -->  00:02:33,420
We're going to talk about each one of them just now before we do I wanted to say that sometimes you'll

38

00:02:33,420  -->  00:02:42,310
hear stepwise regression so stepwise regression actually refers to number two three and four because

39

00:02:42,310  -->  00:02:46,040
those are really the true step by step methods.

40

00:02:46,200  -->  00:02:51,960
But sometimes you will hear people say stepwise or stepwise regression in reference to just not before

41

00:02:51,960  -->  00:02:58,920
so there will replays be bidirectional elimination of stepwise regression and that's fine that's that's

42

00:02:58,920  -->  00:03:04,950
normal that's just because that's the more as you'll see from what we discussed by directional admonition

43

00:03:05,250  -->  00:03:07,120
is kind of the more general approach.

44

00:03:07,140  -->  00:03:15,470
And when people say stepwise regression they kind of by default infer imply imply that is bidirectional

45

00:03:15,570  -->  00:03:18,540
illumination and you have to infer it from there.

46

00:03:18,630  -->  00:03:21,940
OK so let's move on to our methods.

47

00:03:22,050  -->  00:03:23,690
Method number one all in.

48

00:03:23,700  -->  00:03:29,160
It's not a technical term I just call it all in basically what it means is just throw in all your variables

49

00:03:29,160  -->  00:03:29,280
.

50

00:03:29,280  -->  00:03:31,240
Something we just discussed the Russian too.

51

00:03:31,260  -->  00:03:32,290
When would you do that.

52

00:03:32,310  -->  00:03:38,250
One is if you have prior knowledge if you know that these exact variables are the ones are your true

53

00:03:38,250  -->  00:03:43,550
predictors you don't have to build anything you already know that this is the case.

54

00:03:43,560  -->  00:03:49,680
You might know it from domain knowledge or you might know it because you've done this model before somebody

55

00:03:49,680  -->  00:03:52,730
just gave you these variables and said please build a model.

56

00:03:52,740  -->  00:03:54,370
Well then you don't really have a choice.

57

00:03:54,420  -->  00:03:58,980
You just build the model that the one is you have to perhaps like.

58

00:03:59,130  -->  00:04:06,570
I can't really think of good examples here but maybe there is some framework in your company that says

59

00:04:06,570  -->  00:04:08,440
that you have to use these variables right.

60

00:04:08,460  -->  00:04:14,970
So it's kind of similar to prior knowledge but it's not a it's not a it's not your decision it's you

61

00:04:14,970  -->  00:04:20,150
might want to do it differently but there is a framework you know like maybe a bank.

62

00:04:20,160  -->  00:04:26,350
And to predict credit like or the likelihood of somebody defaulting on something they have to use these

63

00:04:26,510  -->  00:04:27,540
variables.

64

00:04:27,870  -->  00:04:32,700
Once again I'm not sure in which industries that would be the case but perhaps that could be the case

65

00:04:32,700  -->  00:04:33,360
.

66

00:04:33,390  -->  00:04:40,560
And number three you would use this method if you're preparing for a backward elimination type of construction

67

00:04:40,560  -->  00:04:43,470
of regression which is our next type.

68

00:04:43,470  -->  00:04:46,800
So let's move onto backward elimination.

69

00:04:47,040  -->  00:04:52,410
Okay so here comes a step by step stuff you might you might want to get your pens out and write these

70

00:04:52,410  -->  00:04:56,620
things down because we're going to have here truly step by step method now.

71

00:04:56,790  -->  00:04:57,470
All right.

72

00:04:57,480  -->  00:05:00,090
Backward elimination first thing.

73

00:05:00,090  -->  00:05:04,880
Step one you have to select a significance level to stay in the model.

74

00:05:04,980  -->  00:05:09,360
So by default we're going to go with 5 percent so 0.05.

75

00:05:09,720  -->  00:05:15,030
And we're going to use it in the next step so it's at the beginning you decide on this significance

76

00:05:15,030  -->  00:05:15,570
level.

77

00:05:15,780  -->  00:05:20,850
Step 2 you fit the full model of all possible predicter So you kind of do that all in approach which

78

00:05:20,850  -->  00:05:27,470
we just talked about and you put all of your variables into your model and now we're going to start

79

00:05:27,470  -->  00:05:28,690
getting rid of them.

80

00:05:28,730  -->  00:05:32,480
Step three you considered the predicter with the highest P-value.

81

00:05:32,510  -->  00:05:37,670
So remember those P-value that we talked about in for instance in Grettel or in any software or you

82

00:05:37,670  -->  00:05:38,620
can see them.

83

00:05:38,620  -->  00:05:42,840
So after you fitted the model you'll see the one with the highest P value.

84

00:05:43,070  -->  00:05:52,310
So if p is greater than your significance level then you go to Step 4 and Step 4 is you have to remove

85

00:05:52,310  -->  00:05:56,310
that predict to remove basically the variable that has the highest P value.

86

00:05:56,570  -->  00:06:02,690
And from Step 4 you fit the model before this variable so there's a star here because I just wanted

87

00:06:02,700  -->  00:06:11,050
to remind myself to tell you that if you just remove the variable obviously you can just say that.

88

00:06:11,060  -->  00:06:16,180
OK now now I've got the new model you have to actually refit them all you have to re recreate them all

89

00:06:16,190  -->  00:06:21,950
rebuild it with the fewer number of variables so if you had maybe I don't know a hundred variables and

90

00:06:21,950  -->  00:06:24,500
you removed them one of them may have 99 now.

91

00:06:24,560  -->  00:06:27,800
Well you have to rebuild it so the coefficients are going to be different.

92

00:06:27,830  -->  00:06:29,680
The constant is going to be different.

93

00:06:29,810  -->  00:06:36,070
And you you need to perform that step because once you remove a variable it affects all the other variables

94

00:06:36,110  -->  00:06:38,270
in your whole regression.

95

00:06:38,750  -->  00:06:41,920
And so after Step 5 you go back to Step 3.

96

00:06:41,930  -->  00:06:46,860
Once again you look for the variable with the highest P-value in your new model.

97

00:06:47,000  -->  00:06:49,310
You take it out you remove it.

98

00:06:49,310  -->  00:06:51,250
So basically step 4 you remove it.

99

00:06:51,250  -->  00:06:57,110
You fit the model again with one less variable and so on and you keep doing that until you come to a

100

00:06:57,320  -->  00:07:05,330
point where the even the variable of the highest P value that p value is still less than your significance

101

00:07:05,330  -->  00:07:05,630
level.

102

00:07:05,620  -->  00:07:11,940
So if that condition P is greater than s.l is not correct then you don't just have 4 anymore.

103

00:07:11,960  -->  00:07:16,030
You go to fin and this case ffin is the finish.

104

00:07:16,060  -->  00:07:17,760
And your model is ready.

105

00:07:17,780  -->  00:07:25,130
So as soon as all of the variables that you have left in your model are there p values are less than

106

00:07:25,370  -->  00:07:26,600
the significance level.

107

00:07:26,600  -->  00:07:28,460
Your models prepared.

108

00:07:29,000  -->  00:07:31,220
So that's how the backward elimination method works.

109

00:07:31,340  -->  00:07:32,750
Let's move on to the next one.

110

00:07:32,840  -->  00:07:35,100
Next method is the forward selection.

111

00:07:35,240  -->  00:07:40,100
This is it sounds like the opposite and the picture in the top right corner does illustrate the opposite

112

00:07:40,120  -->  00:07:40,310
.

113

00:07:40,550  -->  00:07:46,380
But it's a much more complex than just simply reversing the procedure.

114

00:07:46,400  -->  00:07:50,360
You will see that it's a much larger procedure.

115

00:07:50,420  -->  00:07:52,170
We start with step 1.

116

00:07:52,340  -->  00:07:55,580
Select the significance level to enter the model.

117

00:07:55,880  -->  00:07:59,750
And in this case once again we're going to select 5 percent.

118

00:07:59,840  -->  00:08:00,880
Then we go to Step 2.

119

00:08:00,880  -->  00:08:04,840
We fit all possible simple regression models.

120

00:08:04,850  -->  00:08:12,050
So we take the dependent variable and we create a regression model with every single independent variable

121

00:08:12,050  -->  00:08:13,220
that we have.

122

00:08:13,430  -->  00:08:20,020
And then we select out of all those models we select the one which has the lowest p value for the independent

123

00:08:20,030  -->  00:08:21,460
variable.

124

00:08:21,680  -->  00:08:25,180
So you can already see that that is in itself a lot of work.

125

00:08:25,250  -->  00:08:27,520
Then what we do is we move to step three.

126

00:08:27,590  -->  00:08:36,340
We keep this variable that we've just chosen and we fit all other possible models with one extra predicter

127

00:08:36,950  -->  00:08:39,410
added to the one we usually have.

128

00:08:39,400  -->  00:08:41,210
So what does that mean.

129

00:08:41,210  -->  00:08:47,590
That means we've selected a simple linear regression with one variable.

130

00:08:47,600  -->  00:08:55,150
Now we need to construct all possible linear regressions with two variables where one of those two variables

131

00:08:55,150  -->  00:08:56,410
is the one of various like.

132

00:08:56,430  -->  00:09:03,200
So basically we add on all possible all of the other variables one by one so we choose OK let's add

133

00:09:03,200  -->  00:09:06,550
on this variable and then let's add on the next one like.

134

00:09:06,620  -->  00:09:14,240
But separately so we construct all possible 2 variable linear regressions and just keeping definitely

135

00:09:14,240  -->  00:09:16,290
keeping the variable that we're very selected.

136

00:09:16,550  -->  00:09:18,890
So what do we do after that.

137

00:09:18,880  -->  00:09:26,120
Out of all of these possible two variable regressions we consider the one where the new variable that

138

00:09:26,120  -->  00:09:29,670
we added had the lowest p value.

139

00:09:29,780  -->  00:09:36,980
So if that p value is less than our significance level meaning that you know that variables a good one

140

00:09:37,210  -->  00:09:41,030
it's a significant variable then we moved back to Step 3.

141

00:09:41,120  -->  00:09:41,800
What does that mean.

142

00:09:41,810  -->  00:09:47,720
Means that now we have a regression with two variables and now we will add a third variable We'll try

143

00:09:47,720  -->  00:09:54,950
all possible variables that we have left as our third variable and then out of all of those models with

144

00:09:54,950  -->  00:10:00,760
three variables we will go to Step 4 and we'll select again the one of the lowest p value for that third

145

00:10:00,770  -->  00:10:01,360
variable.

146

00:10:01,390  -->  00:10:02,050
We added.

147

00:10:02,270  -->  00:10:03,190
And I'll keep doing that.

148

00:10:03,200  -->  00:10:08,300
So basically we'll be keep growing the regression model but not just randomly we'll be actually selecting

149

00:10:08,300  -->  00:10:15,300
out of the all all of the possible combinations every single time and growing at one variable at a time

150

00:10:15,310  -->  00:10:15,520
.

151

00:10:16,330  -->  00:10:24,370
And then we will only stop when the variable that we've added It has a p value that is greater than

152

00:10:24,640  -->  00:10:25,840
our significance level.

153

00:10:25,840  -->  00:10:32,520
So when this condition is less than SL is not true then we don't go to Step Three we finished the regression

154

00:10:32,530  -->  00:10:38,060
y well because that variable that we just added is no longer is not significant anymore.

155

00:10:38,140  -->  00:10:43,030
And we also know that we selected the one with the lowest P-value So there is no other variable that

156

00:10:43,030  -->  00:10:51,250
we can add that its p value will be greater or will be less than s.l any any regression which is from

157

00:10:51,550  -->  00:10:56,340
then onwards it will the Arab the new variable will always be insignificant.

158

00:10:56,380  -->  00:11:02,530
And so here we finish the regression and the trick here is that you keep not the current model but the

159

00:11:02,530  -->  00:11:08,410
previous one and that makes sense because you've just added a variable which is insignificant So what's

160

00:11:08,410  -->  00:11:10,720
the point of that variable just move a step back.

161

00:11:11,020  -->  00:11:12,700
So that's how forward selection works.

162

00:11:12,700  -->  00:11:17,140
I know it's a bit confusing but just try to wrap your head around and maybe read through these instructions

163

00:11:17,140  -->  00:11:18,080
one more time.

164

00:11:18,110  -->  00:11:24,670
It does make a lot of sense when you and it's like kind of picture what is it exactly is going on.

165

00:11:25,570  -->  00:11:29,080
And next we're moving on onto the bi directional limit elimination.

166

00:11:29,230  -->  00:11:35,430
So this one as you can assume or perhaps guess it combines the two step one.

167

00:11:35,440  -->  00:11:40,960
Select a significant level to stay or to enter and a significant level to stay.

168

00:11:40,960  -->  00:11:47,500
So we're going to select in both cases 5 percent but it's up to you what you select set to perform the

169

00:11:47,500  -->  00:11:54,510
next step of the forward selection meaning that's the one that we just discussed.

170

00:11:54,520  -->  00:12:01,360
So where new variables when they enter in order to enter they have to be less than the significance

171

00:12:01,370  -->  00:12:03,160
level to enter.

172

00:12:03,160  -->  00:12:07,810
So basically add on a new variable based on the forward selection method.

173

00:12:07,810  -->  00:12:11,440
Step 3 perform all of the steps of the backward elimination process.

174

00:12:11,440  -->  00:12:16,390
So now if you have two variables start getting rid of them and see if you can get rid of any of them

175

00:12:17,050  -->  00:12:21,490
and then move back to Step 2 so then grow it by another variable.

176

00:12:21,520  -->  00:12:27,070
And every time you go by verbal say let's say you were at five variables and went to six since you went

177

00:12:27,070  -->  00:12:31,030
to six you have to form all the steps of backward elimination so you don't just eliminate one variable

178

00:12:31,030  -->  00:12:34,410
if you can eliminate one to three however many you can.

179

00:12:34,750  -->  00:12:37,370
And then from there you go back to Step 2.

180

00:12:37,390  -->  00:12:39,190
And so this is a very iterative process.

181

00:12:39,190  -->  00:12:45,940
You keep doing that until at some point you cannot add new variables no variables can enter or no old

182

00:12:45,940  -->  00:12:48,820
variables can exit as soon as you get there.

183

00:12:48,820  -->  00:12:52,790
Then you proceed to the finish because your model is ready to call.

184

00:12:52,780  -->  00:12:57,010
You can add anything you can take anything out that means you've you've created the model.

185

00:12:57,160  -->  00:13:00,490
So this is one of the more tedious methods.

186

00:13:00,520  -->  00:13:06,820
Of course you would have to get a computer to do this for you because otherwise you'd go insane putting

187

00:13:06,820  -->  00:13:08,370
variables in and taking them out.

188

00:13:08,380  -->  00:13:11,840
But that's how bidirectional elimination works.

189

00:13:11,850  -->  00:13:17,370
And once again some people call it stepwise regression.

190

00:13:17,680  -->  00:13:20,880
And finally all possible models.

191

00:13:20,890  -->  00:13:29,440
So this is the most probably Saro approach but also the most resource consuming approach because you

192

00:13:29,440  -->  00:13:34,940
select a criterion of goodness of fit for instance that caky criterion can be the R-squared lots of

193

00:13:34,930  -->  00:13:38,630
different criterions then you construct all possible regression model.

194

00:13:38,620  -->  00:13:44,680
So if you had and variables and all they'll be a two to the power of and minus one total combinations

195

00:13:45,040  -->  00:13:51,700
of these variables and that that's exactly how many models there can possibly be and then step three

196

00:13:51,700  -->  00:13:56,800
you select the one of these models with the best criterion that you're looking at.

197

00:13:56,800  -->  00:13:58,350
There you go your model is ready.

198

00:13:58,360  -->  00:14:03,880
So sounds easy but let's have a look an example even if you have 10 columns and a daughter you'll have

199

00:14:03,890  -->  00:14:05,570
1023 models.

200

00:14:05,600  -->  00:14:06,700
That's insane.

201

00:14:06,700  -->  00:14:11,010
That's an insane amount of morals and we're not talking about columns that you've already filtered out

202

00:14:11,020  -->  00:14:12,280
so call them that.

203

00:14:12,620  -->  00:14:18,030
You know that's like in our example you might have five or six columns.

204

00:14:18,040  -->  00:14:24,460
Now we're talking about when you get a data set that you you just is pretty much a rule and it has like

205

00:14:24,760  -->  00:14:32,530
maybe 100 columns like I've worked with data sets were around that maybe 50 to 100 maybe more columns

206

00:14:32,800  -->  00:14:36,910
and instead of going through them there's what this method is suggesting is that we're going through

207

00:14:36,910  -->  00:14:39,640
them and picking out the ones that you think should be in the model.

208

00:14:39,640  -->  00:14:40,780
You just throw everything in.

209

00:14:40,780  -->  00:14:47,930
Well that's not a good approach because basically it's is growing exponentially the number of models

210

00:14:47,920  -->  00:14:49,110
is growing exponentially.

211

00:14:49,300  -->  00:14:56,270
And it's it's very resource consuming to get a result from this approach.

212

00:14:56,650  -->  00:15:02,070
And finally so where how we come to we've come to our conclusion.

213

00:15:02,090  -->  00:15:07,720
We have five methods of building model models all in backwords elimination for selection by directional

214

00:15:07,720  -->  00:15:09,950
ammunition and the score comparison.

215

00:15:10,000  -->  00:15:16,990
So the one we are going to be looking at in these tutorials in order to get our head around how to build

216

00:15:16,990  -->  00:15:22,480
models step by step and get some practice is the backward elimination process because it is the fastest

217

00:15:22,480  -->  00:15:27,960
one or all of them and you will still get to see exactly how the step by step method works.

218

00:15:28,150  -->  00:15:35,470
And plus we'll throw in a few extra tricks along the way to make sure our models are very robust Can't wait

219

00:15:35,620  -->  00:15:36,880
to get started.

220

00:15:36,910  -->  00:15:37,840
Lots of theory.

221

00:15:37,880  -->  00:15:39,160
Let's get to the practice.

222

00:15:39,250  -->  00:15:40,450
Forcing in next time
