1
00:00:05,000 --> 00:00:10,160
Hello everyone and welcome to the documentation example lecture for the principal component analysis

2
00:00:10,160 --> 00:00:16,610
section of the course in this lecture we will briefly walk you through the is official example of how

3
00:00:16,610 --> 00:00:19,340
to implement PCa with Scalia Scotland spark.

4
00:00:19,610 --> 00:00:22,240
Let's jump to that Scala script now.

5
00:00:22,250 --> 00:00:22,550
All right.

6
00:00:22,550 --> 00:00:27,550
Here I am at the Adam text editor and I have opened up the documentation example.

7
00:00:27,770 --> 00:00:31,880
All the scripts for this section of the Course can be found under the machine learning instructions

8
00:00:31,880 --> 00:00:34,150
folder under the PCA folder.

9
00:00:34,160 --> 00:00:40,810
And in this case I've opened up these PCA underscore doc underscore example dot Skala script lets you

10
00:00:40,840 --> 00:00:47,190
simply walk through this documentation an example of how to actually create a principal component analysis.

11
00:00:47,210 --> 00:00:51,620
The main thing you'll notice here is that you started Spark's session but more importantly you have

12
00:00:51,620 --> 00:01:02,000
to import from M-L dot feature PCA PCA is technically in Scala and SPARC thought of as a feature processing

13
00:01:02,250 --> 00:01:03,210
out for them.

14
00:01:03,230 --> 00:01:08,810
It's not really thought of as a unsupervised machine learning algorithm that is not to say that principal

15
00:01:08,810 --> 00:01:14,360
component analysis is not an unsupervised machine learning algorithm it just letting you know that don't

16
00:01:14,360 --> 00:01:17,780
expect to find it under M-L dog clustering or something.

17
00:01:17,840 --> 00:01:25,680
PCa is found of the features section of Spark's machine learning library but principal component analysis

18
00:01:25,770 --> 00:01:33,210
is an unsupervised task meaning we only expect to see features not a label when working with PCa and

19
00:01:33,210 --> 00:01:38,170
the other thing we want to import is from M-L Butland alledged vectors.

20
00:01:38,340 --> 00:01:42,630
Then we start a spark session here we're calling it PCa example.

21
00:01:42,630 --> 00:01:48,540
We create some data documentation instead of using some lib SVM format just creates some simple data

22
00:01:48,540 --> 00:01:49,520
right here.

23
00:01:49,530 --> 00:01:52,420
Notice that it's just some array of vectors.

24
00:01:52,440 --> 00:01:55,830
And here we can see that it's 2 0 3 4 5.

25
00:01:56,010 --> 00:02:03,350
So right now we actually have what are essentially five features for each of these vectors.

26
00:02:03,570 --> 00:02:08,910
And we're going to end up doing is saying sparks create data frame with some operations to transform

27
00:02:09,180 --> 00:02:14,580
this array of vectors to a data frame and the frame just has a single column.

28
00:02:14,720 --> 00:02:20,010
This single column is just called features which makes sense just as we saw in the Kamins clustering

29
00:02:20,010 --> 00:02:20,830
lectures.

30
00:02:20,970 --> 00:02:27,150
We just have a single column of that array of features when dealing with unsupervised machine learning

31
00:02:27,180 --> 00:02:27,990
algorithms.

32
00:02:27,990 --> 00:02:32,310
This case we're dealing with PCa to actually implement PCA It's quite simple.

33
00:02:33,100 --> 00:02:39,730
Basically treat this just as you would some vector assembler object or a string indexer or other objects

34
00:02:39,730 --> 00:02:46,370
that you find under the if I scroll up here and that feature section of Spark's machine learning capabilities.

35
00:02:46,660 --> 00:02:51,700
So all you have to do is say Vau PCA is equal to a new PC object.

36
00:02:51,700 --> 00:02:56,140
You set the input column to be that features column so the output column.

37
00:02:56,140 --> 00:03:00,880
You can call it whatever you want but we usually just want to call it something simple like PCA features

38
00:03:00,880 --> 00:03:02,300
those principal components.

39
00:03:02,410 --> 00:03:05,280
And then you have this important set K value.

40
00:03:05,500 --> 00:03:12,160
So the set K don't mix this up with K means all this is saying is how many Princeville principal components

41
00:03:12,160 --> 00:03:13,150
do you want.

42
00:03:13,390 --> 00:03:18,160
There's no real correct answer here as far as what value you should choose.

43
00:03:18,340 --> 00:03:22,510
It's the value that you want to reduce the number of features too.

44
00:03:22,510 --> 00:03:26,140
So for example here we're dealing with five features.

45
00:03:26,500 --> 00:03:29,560
And here we're going to reduce it to three principal components.

46
00:03:29,740 --> 00:03:35,710
And as I mentioned in the theory lecture it's pretty hard to interpret these components directly but

47
00:03:35,710 --> 00:03:40,910
basically you can think of these principal components as just combinations of all the features.

48
00:03:40,930 --> 00:03:43,920
Again there's no real right answer of how to choose this k value.

49
00:03:43,930 --> 00:03:49,240
It really depends on your data your situation and what you're trying to achieve by how much do you actually

50
00:03:49,240 --> 00:03:51,640
want to reduce the number of features you have.

51
00:03:52,620 --> 00:03:59,700
Then in this case we're going to put everything in once that by saying that fit to the data frame.

52
00:03:59,760 --> 00:04:04,920
So he's saying new PCA set the input column set the output column so that the number of principal components

53
00:04:04,920 --> 00:04:10,650
you want and then fit this to the data frame and then you can just transform and check out the results.

54
00:04:10,740 --> 00:04:17,220
You say Val PCA death is equal to that PCA object that transform on that data frame.

55
00:04:17,220 --> 00:04:19,090
So note that there's basically two steps here.

56
00:04:19,260 --> 00:04:25,470
You have to create this PCA object fit it to the data fring and then to actually get the result transform

57
00:04:25,470 --> 00:04:30,350
that data frame and then you can just select that column PCA features that you just created.

58
00:04:30,600 --> 00:04:32,280
And you can show that off.

59
00:04:32,310 --> 00:04:38,520
Let's run this whole script and see if it works you will say load PCA doc.

60
00:04:38,520 --> 00:04:43,210
The example that Skala enter and let it run.

61
00:04:43,380 --> 00:04:44,040
Up everything.

62
00:04:44,040 --> 00:04:49,960
Let me expand this so we can actually see this results so noticed now we have this PC features column

63
00:04:50,050 --> 00:04:53,680
and if we take a look at our original data frame number it's called the death.

64
00:04:53,830 --> 00:05:02,710
I will say DFI show and we can also say if or actually we have all three rows here so we can see that

65
00:05:02,710 --> 00:05:06,850
we have more than three principal components in these features.

66
00:05:06,850 --> 00:05:15,280
If I say the result does show we have this PC features in effect call the first row of that by saying

67
00:05:15,280 --> 00:05:17,410
results head.

68
00:05:17,660 --> 00:05:20,180
We notice that we just have three principal components.

69
00:05:20,210 --> 00:05:21,350
Here's the first one.

70
00:05:21,470 --> 00:05:22,330
Here's the second one.

71
00:05:22,370 --> 00:05:23,520
And there's the third one.

72
00:05:23,540 --> 00:05:29,150
So we successfully reduce the number of features this has two just three principal components where

73
00:05:29,180 --> 00:05:34,750
each principal component is essentially representative of a combination of features.

74
00:05:34,790 --> 00:05:38,360
So that's really all there is two principal components analysis.

75
00:05:38,360 --> 00:05:43,580
You just basically treat this as you would for some vector assembler or some string indexer or really

76
00:05:43,610 --> 00:05:48,470
any other sort of feature object you would get from this and Malkoff feature light.

77
00:05:48,950 --> 00:05:49,490
OK.

78
00:05:49,760 --> 00:05:52,400
If you have any questions feel free to post them to the Kewney forums.

79
00:05:52,400 --> 00:05:58,380
But coming up next we're going to go over what you can expect to do in your PC project exercise.

80
00:05:58,430 --> 00:06:00,360
Thanks everyone and I'll see you at the next lecture.
