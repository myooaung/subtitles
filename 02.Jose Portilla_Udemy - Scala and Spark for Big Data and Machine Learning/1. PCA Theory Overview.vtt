WEBVTT
1
00:00:05.940 --> 00:00:12.840
Hello everyone and welcome to the principal component analysis lecture if you want a more mathematical

2
00:00:12.840 --> 00:00:16.580
background and a deeper dive into principle component analysis.

3
00:00:16.740 --> 00:00:22.880
Go ahead and read Section ten point two of an introduction to statistical learning.

4
00:00:22.880 --> 00:00:27.920
Now let's discuss the basic idea behind principle component analysis or PCA.

5
00:00:28.370 --> 00:00:35.030
PCa is an unsupervised school technique used to examine the interrelations among a set of variables

6
00:00:35.300 --> 00:00:39.320
in order to identify the underlying structure of those variables.

7
00:00:39.320 --> 00:00:47.230
It is also known sometimes as a general factor analysis where regression determines a line of best fit

8
00:00:47.230 --> 00:00:48.370
to a data set.

9
00:00:48.430 --> 00:00:54.900
Factor Analysis determines several orthogonal lines of best fit to the data set orthogonal just means

10
00:00:54.910 --> 00:00:56.230
at right angles.

11
00:00:56.260 --> 00:00:58.940
Actually those lines are perpendicular to each other.

12
00:00:58.990 --> 00:01:04.790
In the end dimensional space where the end dimensional space is the variable sample space.

13
00:01:05.020 --> 00:01:07.720
There are as many dimensions as there are variables.

14
00:01:07.720 --> 00:01:15.240
So in a dataset with four variables the sample space for them in Shanel Let's go ahead and see an example.

15
00:01:15.370 --> 00:01:21.940
Here we have some data plotted along to features X and Y and we want to head and put a regression line

16
00:01:21.970 --> 00:01:30.130
of best fit what we can do now is add an orthogonal line which is just at a right angle to that first

17
00:01:30.190 --> 00:01:30.820
line.

18
00:01:31.020 --> 00:01:37.890
And now we can begin to understand the components of our dataset components are a linear transformation

19
00:01:38.160 --> 00:01:44.100
that chooses a variable system for the data set such that the greatest variance of the data set comes

20
00:01:44.100 --> 00:01:51.010
to lie on the first axes and then likewise the second greatest variance on the second axes and so on.

21
00:01:51.060 --> 00:01:56.840
And this process allows us to reduce the number of variables used in an analysis and if we look back

22
00:01:56.840 --> 00:02:02.940
on our figure we can see that that first principle line explains the 70 percent of the variation and

23
00:02:02.980 --> 00:02:07.320
the line orthogonal to that explains 28 percent of the variation.

24
00:02:07.380 --> 00:02:13.510
And right now we have 2 percent of the variation remaining unexplained and note that the components

25
00:02:13.510 --> 00:02:14.700
are uncorrelated.

26
00:02:14.770 --> 00:02:19.690
Since in the sample space they are orthogonal to each other or at direkte right angles.

27
00:02:20.380 --> 00:02:23.810
And we can continue this analysis into higher dimensions.

28
00:02:23.820 --> 00:02:29.010
Here we now have three components component one that principal component explaining most of the variance

29
00:02:29.250 --> 00:02:32.120
and then component to component 3 and a higher mention.

30
00:02:32.310 --> 00:02:39.290
As we go into the z axes if we use this technique on a dataset with a large number of variables we can

31
00:02:39.290 --> 00:02:43.790
actually compress the amount of explained variation to just a few components.

32
00:02:43.790 --> 00:02:47.900
However the most challenging part of PCa is interpreting the components.

33
00:02:47.900 --> 00:02:53.660
All right so that's the theory on principal component analysis or PCA for the rest of this section of

34
00:02:53.660 --> 00:02:54.410
the course.

35
00:02:54.440 --> 00:02:59.060
We're going to walk through the documentation example and then after that goes straight into a project

36
00:02:59.060 --> 00:03:06.080
with exercises the project will involve using PCA to take in a cancer data set on tumors with 30 features

37
00:03:06.140 --> 00:03:10.430
and reducing it to a dataset with only four principal components.

38
00:03:10.530 --> 00:03:12.020
OK let's get started.

39
00:03:12.080 --> 00:03:13.190
I'll see you in the next lecture.
