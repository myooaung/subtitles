1
00:00:05,650 --> 00:00:09,370
Hello everyone and welcome to the machine learning will spark lecture.

2
00:00:10,350 --> 00:00:15,990
In this lecture we will be discussing the basics of how to use Spark's machine learning library sometimes

3
00:00:15,990 --> 00:00:19,980
known as MLA lib and other times shortened to just sparks m-L.

4
00:00:20,310 --> 00:00:24,740
We will explore its documentation and discuss her approach to learning how to use m-L.

5
00:00:24,810 --> 00:00:30,620
In this course let's get a quick review of The Fool machine learning process before we begin.

6
00:00:30,640 --> 00:00:33,720
Remember that the machine learning process has several steps.

7
00:00:33,730 --> 00:00:40,210
First you need to acquire your data then you need to manipulate and clean that data to the correct format.

8
00:00:40,210 --> 00:00:46,150
Afterwards you split that data into a test set and use your training set to actually train your model

9
00:00:46,180 --> 00:00:47,440
and build your model.

10
00:00:47,560 --> 00:00:52,600
Then you will test that model against your test set and then reevaluate what you need to fix in your

11
00:00:52,600 --> 00:00:53,270
model.

12
00:00:53,350 --> 00:00:58,180
And once that loop is complete your model is ready for the placement and in between the model testing

13
00:00:58,180 --> 00:00:59,690
and model deployment phase.

14
00:00:59,720 --> 00:01:06,070
There is also some times what is known as a hold out set that is a third set of data not your training

15
00:01:06,070 --> 00:01:06,380
set.

16
00:01:06,400 --> 00:01:12,280
Not your test set that a hold out set as a last measure of how well your model perform before you actually

17
00:01:12,290 --> 00:01:14,710
the point.

18
00:01:14,720 --> 00:01:20,160
Now let's talk about Spark's MLA Sparke currently has two machine learning API.

19
00:01:20,330 --> 00:01:23,750
It has the r d d API and the data from an API.

20
00:01:23,990 --> 00:01:28,880
The r d d API will soon be no longer supported and is considered outdated.

21
00:01:28,880 --> 00:01:33,820
Which is why we focused so much on learning how to work with data frames in the earlier section.

22
00:01:33,840 --> 00:01:40,370
The course the documentation for Spark's MLA is actually very good it's much better than its data frame

23
00:01:40,370 --> 00:01:45,270
documentation which I'm sure you're happy to hear because the documentation is so good.

24
00:01:45,320 --> 00:01:47,670
Let's take a quick tour of the documentation.

25
00:01:47,690 --> 00:01:53,540
I'm going to jump to my browser now and show you the documentation for Spark's M-L it or it here and

26
00:01:53,600 --> 00:01:59,090
at the documentation for the machine learning library guide we will be visiting the documentation often

27
00:01:59,210 --> 00:02:01,880
as we go throughout the machine learning section of the course.

28
00:02:01,880 --> 00:02:06,560
Right now I just want to give you a quick overview of what it looks like at this current state.

29
00:02:06,560 --> 00:02:11,840
You can find this documentation that sparked that Apache the org slash doc slash latest slash M-L bash

30
00:02:11,840 --> 00:02:15,330
guide the HTL or you can just come here to programming guides.

31
00:02:15,350 --> 00:02:21,530
Go to the version of sparkie white and then click down to MLM machine learning when you come to this

32
00:02:21,530 --> 00:02:24,450
page you'll notice that there's essentially two guides.

33
00:02:24,500 --> 00:02:30,150
One is the main guide which is the data frame guide and the other is the RTD based API guide.

34
00:02:30,320 --> 00:02:38,500
Now right now they are still showing the RTT based API guide but by the time SPARC 3.0 is released the

35
00:02:38,560 --> 00:02:45,050
d based API is expected to just be removed from the latest versions of SPARC and once SPARC reaches

36
00:02:45,050 --> 00:02:50,990
2.2 that version 2.2 the Ardi the base API will just be considered outdated.

37
00:02:50,990 --> 00:02:53,240
Right now it's kind of on the cusp of being outdated.

38
00:02:53,270 --> 00:02:55,400
So they're still showing a lot of that information.

39
00:02:55,490 --> 00:03:00,470
Since a lot of people still use it but really the future of machine learning was SPARC is really with

40
00:03:00,470 --> 00:03:01,370
data frames.

41
00:03:01,370 --> 00:03:06,760
Now there's lots of reasons for this why we're switching to the data frame base the API for one reason.

42
00:03:06,770 --> 00:03:11,300
It's just much much easier to work with the data frames than it is with the DS.

43
00:03:11,330 --> 00:03:17,780
There's lots of benefits including getting Sparke data sources of data frame's a lot of optimizations

44
00:03:17,840 --> 00:03:20,780
and a uniform API across multiple languages.

45
00:03:20,840 --> 00:03:26,890
That way the data from API looks relatively the same whether you are working in Java Scala or Python.

46
00:03:26,990 --> 00:03:29,600
So you can quickly jump using the same skillset.

47
00:03:29,810 --> 00:03:36,600
The data frame based API also allows for uniform API across M-L algorithms and across multiple languages.

48
00:03:36,610 --> 00:03:42,830
They just noted and it also facilitates the use of pipelines which we will discuss later on.

49
00:03:42,860 --> 00:03:46,910
You can read more about the general information about the machine learning library guide here on this

50
00:03:46,940 --> 00:03:51,680
main page but I also want to just show you that what we're going to be going over as we explore the

51
00:03:51,680 --> 00:03:56,600
documentation throughout this section of the Course is right here under M-L main guide.

52
00:03:56,660 --> 00:04:01,910
There's pipeline's extracting transforming and selecting features classification regression clustering

53
00:04:02,240 --> 00:04:07,280
collaborative filtering for recommendation systems model selection tuning and then advanced topics.

54
00:04:07,280 --> 00:04:09,280
We're going to start off with just simple regression.

55
00:04:09,320 --> 00:04:14,880
So if you come over here a classification or regression you'll notice that there is a regression here

56
00:04:14,880 --> 00:04:20,910
section and basically we are going to be focusing on this main topic and an example algorithm.

57
00:04:20,970 --> 00:04:26,070
Once you understand the main topic regression in a sample algorithm of regression it's very easy to

58
00:04:26,160 --> 00:04:29,480
switch out that machine learning model for another algorithm.

59
00:04:29,490 --> 00:04:34,590
So if you already understand how to use linear regression for the regression type problem you can easily

60
00:04:34,590 --> 00:04:38,990
switch out your linear regression for something like a random forced regression depending on your data.

61
00:04:39,000 --> 00:04:43,590
That may be a better fit and once you understand had due classification of logistic regression.

62
00:04:43,620 --> 00:04:48,800
Again it's very easy to change it out for something like a naive base or decision tree classifier etc..

63
00:04:49,020 --> 00:04:54,300
That's why in this course we're just focusing on the main big picture ideas so that you with your extra

64
00:04:54,300 --> 00:04:58,790
knowledge or particular datasets can come in and choose whatever algorithm is right for you.

65
00:04:58,890 --> 00:05:02,600
Once you understand how to do the main concept of spark.

66
00:05:02,810 --> 00:05:07,250
Now once we learn how to actually implement some of the machine learning models we'll also talk about

67
00:05:07,550 --> 00:05:10,000
extracting transforming and selecting features.

68
00:05:10,060 --> 00:05:15,740
There's a ton of ways to actually manipulate your data using Spark's API for data frames.

69
00:05:15,860 --> 00:05:20,540
And this is just a few examples of them we'll be exploring these as we go on throughout the course.

70
00:05:20,540 --> 00:05:22,010
Then there are also pipelines.

71
00:05:22,040 --> 00:05:23,810
We will also be exploring pipelines.

72
00:05:23,810 --> 00:05:28,940
It basically allows you to put in multiple steps together into a single pipeline object and then train

73
00:05:28,940 --> 00:05:30,170
that pipeline object.

74
00:05:30,200 --> 00:05:36,740
You may be familiar pipelines with Python's like learn and then we'll also talk about model selection

75
00:05:36,800 --> 00:05:38,750
and tuning model selection.

76
00:05:38,750 --> 00:05:42,210
Tuning is a little different than most other languages.

77
00:05:42,230 --> 00:05:47,830
It does have a train validations split but the easiest way to use this is with that pipeline object

78
00:05:47,840 --> 00:05:51,320
so we will show you how to use that later on this course.

79
00:05:51,320 --> 00:05:53,380
OK that's really all I wanted to show you for now.

80
00:05:53,390 --> 00:05:58,010
We're coming into the documentation later as you may have noticed this documentation is much much better

81
00:05:58,070 --> 00:06:02,590
than the data frame documentation only that data from documentation gets better.

82
00:06:02,600 --> 00:06:06,150
But you can rest easy knowing that you have a lot of examples here.

83
00:06:06,150 --> 00:06:10,460
Free to explore under the machine learning library documentation.

84
00:06:10,460 --> 00:06:14,710
For example if you click here on classification go to random forest classifier.

85
00:06:15,080 --> 00:06:21,250
Click on that it will take you down and it has full code examples with actual example data sets and

86
00:06:21,250 --> 00:06:22,490
it will actually also show you.

87
00:06:22,490 --> 00:06:27,250
If you scroll down here where to find it in your installation of SPARC.

88
00:06:27,260 --> 00:06:32,450
Now these file paths are pretty long so I include a lot of these documentations already in the downloaded

89
00:06:32,450 --> 00:06:33,980
zip file that you have.

90
00:06:33,980 --> 00:06:37,100
But we'll discuss that later on in the same lecture.

91
00:06:37,100 --> 00:06:40,990
OK let's go ahead and go back to the presentation.

92
00:06:40,990 --> 00:06:41,350
All right.

93
00:06:41,360 --> 00:06:41,780
We're back.

94
00:06:41,780 --> 00:06:44,360
We just had a quick tour of the documentation.

95
00:06:44,360 --> 00:06:48,130
Now let's go over an example of process to use Spark's M-L lib.

96
00:06:48,290 --> 00:06:50,290
And again don't worry about memorizing any of this.

97
00:06:50,300 --> 00:06:54,890
We're going to get plenty of practice and review all of this when we actually start coding and subsequent

98
00:06:54,890 --> 00:07:02,050
lectures First you need to get all your data into a data frame using what you've already learned.

99
00:07:02,050 --> 00:07:06,460
Next you need to deal if your features and find your label and supervised learning you'll find your

100
00:07:06,460 --> 00:07:11,560
label for unsupervised learning you will actually just use your features but you already know how to

101
00:07:11,560 --> 00:07:13,870
grab data from a variety of sources.

102
00:07:13,910 --> 00:07:14,700
You have to do next.

103
00:07:14,700 --> 00:07:16,390
There's actually a deal of these features.

104
00:07:16,390 --> 00:07:22,240
And if using a supervised learning algorithm find your label and this is where sparks and Madlib library

105
00:07:22,570 --> 00:07:24,770
is a little different than other libraries.

106
00:07:25,890 --> 00:07:31,440
Usually before you even get to your features and your label you need to manipulate your data that raw

107
00:07:31,440 --> 00:07:35,520
data and spark separates this process into three categories.

108
00:07:35,670 --> 00:07:42,970
Extraction transformation and selection extraction is just extracting features from the raw data selecting

109
00:07:42,970 --> 00:07:48,610
what columns you want to use then transformation is actually either scaling and converting or modifying

110
00:07:48,610 --> 00:07:55,120
certain features of your data then selection again is selecting that subset from a larger set of features.

111
00:07:55,120 --> 00:08:00,570
So you have these three main street steps extraction getting from that raw data the features you want.

112
00:08:00,760 --> 00:08:06,430
Transforming this through scaling converting or modifying features and then selecting the transform

113
00:08:06,520 --> 00:08:07,010
objects.

114
00:08:07,010 --> 00:08:12,550
Everything that you want to keep from that subset of transformations once the data is in the correct

115
00:08:12,550 --> 00:08:16,810
format you need to select your machine learning model and you can check out the research links in this

116
00:08:16,810 --> 00:08:20,040
lecture for guides on how to actually choose the correct model.

117
00:08:20,200 --> 00:08:26,730
Well we will show you later on different models for classification regression etc. and once you've chosen

118
00:08:26,730 --> 00:08:30,760
that model you need to get it into the correct format as far as your data is concerned.

119
00:08:30,780 --> 00:08:35,340
For Spark's M-L lib and this is it works works pretty different from other languages.

120
00:08:35,340 --> 00:08:40,290
This is a data frame with two columns a label column and an array of features column.

121
00:08:40,290 --> 00:08:41,950
Let me explain what that actually means.

122
00:08:42,890 --> 00:08:48,620
This formatting means you will convert all your columns of features into a single column consisting

123
00:08:48,620 --> 00:08:50,540
of an array of all those features.

124
00:08:50,600 --> 00:08:53,830
And as I mentioned this formatting isn't very common for most other languages.

125
00:08:53,930 --> 00:08:56,080
They have machine learning capabilities.

126
00:08:56,180 --> 00:09:00,740
That means at the end of the day once your data is ready to go into the machine learning library model

127
00:09:00,950 --> 00:09:07,580
it actually just has two columns a single label column and then a features column of arrays of all the

128
00:09:07,580 --> 00:09:08,600
actual features.

129
00:09:08,600 --> 00:09:11,000
Now we'll show you what that looks like when we actually program this out.

130
00:09:11,000 --> 00:09:12,260
And that's for supervised learning.

131
00:09:12,260 --> 00:09:16,880
I should clarify if you're using an unsupervised learning algorithm then you actually just have one

132
00:09:16,880 --> 00:09:21,340
column of just a features or a column.

133
00:09:21,390 --> 00:09:25,530
The main reason for this sort of formatting is that it actually allows you to immediately distribute

134
00:09:25,800 --> 00:09:27,570
these models across your cluster.

135
00:09:27,640 --> 00:09:30,780
And we're going to be covering how to create clusters later on.

136
00:09:30,780 --> 00:09:33,010
For now we will still stay on your local computer.

137
00:09:33,060 --> 00:09:35,340
And this also saves the money that we want to pay.

138
00:09:35,400 --> 00:09:41,110
Amazon Web Services are a data Brick's to actually create your clusters once you have all your data

139
00:09:41,140 --> 00:09:44,650
in the correct format you need to train your model and evaluate your model.

140
00:09:44,650 --> 00:09:48,560
You can also do this of just a simple test train split which will show you later on.

141
00:09:48,610 --> 00:09:53,860
But Sparke has a full model selection suite of tools that we showed you a little bit of in the documentation

142
00:09:54,190 --> 00:09:59,630
and we will show you how to use the train test validation splits later on we're going start off with

143
00:09:59,630 --> 00:10:02,850
the basics a model evaluation and then slowly introduce train to split.

144
00:10:02,900 --> 00:10:09,750
As I just mentioned as well is the full train validation split with holdout data afterwards we're going

145
00:10:09,750 --> 00:10:14,400
to show you how does set up this entire process into a pipeline object.

146
00:10:14,460 --> 00:10:18,840
This is going to it to easily set up a pipeline of data transformations model selection's parameter

147
00:10:18,840 --> 00:10:24,450
tuning etc. so that you just train your pipeline at the end of the day and save all your work in certain

148
00:10:24,450 --> 00:10:25,120
steps.

149
00:10:25,140 --> 00:10:29,820
This allows for a much cleaner syntax which is why it's so important to begin to learn the data from

150
00:10:29,850 --> 00:10:32,290
API for Sparc.

151
00:10:32,420 --> 00:10:37,340
The best way to show any of this is just to dive straight into some examples throughout the machine

152
00:10:37,340 --> 00:10:38,580
learning section for Sparc.

153
00:10:38,660 --> 00:10:40,620
We're going to stick to a general format.

154
00:10:41,440 --> 00:10:46,630
We'll start off with a general discussion of that machine learning concept and show an example algorithm

155
00:10:46,630 --> 00:10:48,070
for that concept.

156
00:10:48,310 --> 00:10:53,020
Then we will go over the official documentation example so you can get a quick taste of what the actual

157
00:10:53,020 --> 00:10:54,490
code will look like.

158
00:10:54,490 --> 00:11:00,380
Then we'll walk you through a more realistic example with data from a real data source such as Kaggle.

159
00:11:00,520 --> 00:11:05,110
And then I'll give you a project exercise for us complete where you actually have to type in and fill

160
00:11:05,110 --> 00:11:10,730
out the code on your own and then check it against a solutions lecture this course is going to stay

161
00:11:10,730 --> 00:11:14,510
like a mathematical theory and there's basically two reasons for that.

162
00:11:14,510 --> 00:11:16,920
One is that not everyone has the same background.

163
00:11:17,060 --> 00:11:21,970
And two is that sparks and the Lib library actually makes it very easy to adjust parameters.

164
00:11:22,100 --> 00:11:26,810
So if you have that mathematical experience for a particular algorithm you can quickly set options for

165
00:11:26,810 --> 00:11:30,270
particular parameters and if you don't have that background that's OK too.

166
00:11:30,320 --> 00:11:34,520
You can just leave the default options as set.

167
00:11:34,590 --> 00:11:37,860
OK so I hope you're excited start using SPARC for machine learning.

168
00:11:37,890 --> 00:11:42,820
This is really the latest in Sparc and the latest in big data technology.

169
00:11:43,050 --> 00:11:46,920
So this is going to be a very great tool to have in your toolset.

170
00:11:46,920 --> 00:11:48,350
All right thanks everyone.

171
00:11:48,440 --> 00:11:49,650
And I will see you at the next lecture.
