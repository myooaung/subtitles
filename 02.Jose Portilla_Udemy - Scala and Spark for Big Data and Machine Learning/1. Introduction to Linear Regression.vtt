WEBVTT
1
00:00:05.870 --> 00:00:10.690
Hello everyone and welcome to the regression section of machine learning part of the course.

2
00:00:11.050 --> 00:00:16.050
Our first type of machine learning task is a regression test where we try to create a model to predict

3
00:00:16.050 --> 00:00:17.510
a continuous value.

4
00:00:17.510 --> 00:00:23.460
For example the price of a house the most fundamental type of regression algorithm is a linear regression.

5
00:00:23.690 --> 00:00:27.660
Let's discuss the background of a linear regression and how it all works.

6
00:00:27.730 --> 00:00:30.900
Do you want any deeper mathematical understanding of linear regression.

7
00:00:30.950 --> 00:00:37.450
You can go ahead and check out chapters 2 and 3 of an introduction to statistical learning.

8
00:00:37.490 --> 00:00:40.700
Let's discuss the history behind linear regression.

9
00:00:40.700 --> 00:00:47.990
This entire idea all started back in the 1800s a man named Francis Galton Galton was studying the relationship

10
00:00:48.050 --> 00:00:53.660
between parents and their children and in particular he investigated the relationship between the heights

11
00:00:53.750 --> 00:00:57.480
of fathers and the heights of their sons.

12
00:00:57.590 --> 00:01:02.900
What he discovered was that a man's son tended to be roughly as tall as his father.

13
00:01:02.990 --> 00:01:09.620
However Galton's breakthrough was at the son's height tended to be closer to the overall average height

14
00:01:09.740 --> 00:01:14.780
of all people let's go ahead and take an example of this.

15
00:01:14.830 --> 00:01:18.540
Shaquille O'Neal was a famous NBA player is very tall.

16
00:01:18.550 --> 00:01:22.270
He's 7 foot 1 inch or 2.2 metres tall.

17
00:01:22.330 --> 00:01:27.550
If Shaq as he's known has a son chances are he'll be pretty tall too.

18
00:01:27.550 --> 00:01:33.430
However Czech is such an anomaly in height that there's also a very good chance that his son is not

19
00:01:33.430 --> 00:01:38.820
going to be as tall as Shaq or and it turns out this is the case.

20
00:01:38.860 --> 00:01:40.750
Shaquille O'Neal son is pretty tall.

21
00:01:40.750 --> 00:01:47.410
He's 6 foot seven inches but he's not nearly as tall as his dad who was 7 at one Galt who calls this

22
00:01:47.410 --> 00:01:55.630
phenomenon regression as in a fathers sons height tends to regress or drift towards the mean or average

23
00:01:55.630 --> 00:01:57.310
height of everyone else.

24
00:01:58.560 --> 00:02:04.680
Let's go ahead and begin to plot out the sort of example let's cut to the progression with only two

25
00:02:04.800 --> 00:02:07.830
datapoints which is the simplest possible example.

26
00:02:07.980 --> 00:02:09.610
Here we have two data points.

27
00:02:09.660 --> 00:02:12.110
X equals 2 and white was 4.

28
00:02:12.170 --> 00:02:17.350
As one data point and that X sequel's 5 and y equals 10 is another data point.

29
00:02:17.370 --> 00:02:23.940
These two little black gawks all we're trying to do when we calculate our regression line is draw a

30
00:02:23.940 --> 00:02:30.370
line that's as close to every dot as possible for classic linear regression or the least squares method.

31
00:02:30.450 --> 00:02:33.800
You only measure the closeness in the up and down their action.

32
00:02:33.810 --> 00:02:37.680
Here we have a perfectly fitted line because it only had two points.

33
00:02:39.230 --> 00:02:45.370
Now wouldn't it be great if we could apply this same concept to grasp with more than just two data points.

34
00:02:45.470 --> 00:02:50.660
By doing this we could take multiple men and their sons heights and do things like tell Amanda how tall

35
00:02:50.660 --> 00:02:54.070
we expect the sun to be before he even has a son.

36
00:02:54.200 --> 00:02:56.580
And this is the idea behind supervised learning.

37
00:02:56.720 --> 00:03:00.620
We're going to have a bunch of labeled data points create a model.

38
00:03:00.620 --> 00:03:07.040
In this case in theory aggression and try to take unlabeled data such as a father's height and spit

39
00:03:07.040 --> 00:03:14.300
back out labelled data are prediction of the sun's height our goal if linear regression is to minimize

40
00:03:14.300 --> 00:03:18.090
the vertical distance between all the data points in our line.

41
00:03:18.320 --> 00:03:24.350
So in determining the best line we are attempting to minimize the distance between all the points and

42
00:03:24.530 --> 00:03:27.350
distance to our line.

43
00:03:27.400 --> 00:03:30.110
There are lots of actually different ways to minimize this.

44
00:03:30.110 --> 00:03:35.740
The sum of squares error some of absolute errors etc. but all of these methods have a general goal of

45
00:03:35.740 --> 00:03:40.230
minimizing the distance between your line and the rest of the data points.

46
00:03:41.100 --> 00:03:45.810
For example one of the most popular methods that we've just described is the least squares method.

47
00:03:45.810 --> 00:03:52.140
Here we have a couple of blue data points along an x and y axes and we want to fit a linear regression

48
00:03:52.140 --> 00:03:52.660
line.

49
00:03:52.860 --> 00:03:57.960
And the question is how do we decide which line is the best fitting one we can go ahead and use the

50
00:03:57.960 --> 00:04:00.660
least squares method which we discussed earlier.

51
00:04:00.870 --> 00:04:08.070
This method is spitted by minimizing the sum of the squares of the residuals the residuals for an observation

52
00:04:08.190 --> 00:04:14.990
is the difference between the observation the y value and the fitted line in this image the residuals

53
00:04:15.020 --> 00:04:17.060
are marked by the red line.

54
00:04:17.060 --> 00:04:23.150
The difference between the true data point in blue and your fitted model line the black Bagenal line.

55
00:04:23.150 --> 00:04:25.950
Now let's shift our focus to regression metrics.

56
00:04:26.000 --> 00:04:31.070
There are many regression metrics you can use to evaluate your regression model including things such

57
00:04:31.070 --> 00:04:38.210
as your R-squared value mean absolute error or maybe mean squared error MSE in root mean squared error

58
00:04:38.390 --> 00:04:39.390
are MSE.

59
00:04:39.650 --> 00:04:45.730
Let's discuss some of them since We'll show you how to call them off a trained model in Sparke they

60
00:04:45.730 --> 00:04:52.030
mean absolute error or E is the mean of the absolute value of the errors and the error is just the difference

61
00:04:52.030 --> 00:04:58.030
between the value predicted versus the value you actually had as a true label and that's what that y

62
00:04:58.060 --> 00:04:59.560
minus y hat represents.

63
00:04:59.560 --> 00:05:02.650
The difference between the predicted value and the true value.

64
00:05:02.740 --> 00:05:09.430
Then you just take the absolute value of that and the average so that sum divided by how many actual

65
00:05:09.550 --> 00:05:14.710
points you predicted for the mean absolute error is the easiest to understand because it's basically

66
00:05:14.710 --> 00:05:19.790
just the average error on average how far off were you.

67
00:05:20.000 --> 00:05:24.520
Then there is the mean square error or MC and it's the mean of the squared error.

68
00:05:24.710 --> 00:05:29.370
You take your errors squared them and then you take the average at that or the mean.

69
00:05:29.480 --> 00:05:36.530
The MC is more popular than the mean absolute error because MSE will punish larger errors which tend

70
00:05:36.530 --> 00:05:38.450
to be more useful in the real world.

71
00:05:38.720 --> 00:05:44.750
And then one of the most popular ones is the root mean squared air or R and SE and so is the square

72
00:05:44.750 --> 00:05:46.370
root of the mean of the square airs.

73
00:05:46.400 --> 00:05:51.410
Basically the square root of what we just previously saw and the root mean square is more popular than

74
00:05:51.410 --> 00:05:57.860
the MSEE because the root means square air is interpretable in the White units that label units that

75
00:05:57.860 --> 00:05:59.480
you were actually trying to predict.

76
00:06:01.120 --> 00:06:06.500
Now let's begin our understanding of regression with SPARC actually showing you a documentation example.

77
00:06:06.500 --> 00:06:11.970
Afterwards we'll move on to a more realistic example and the exercise project freed fill out.

78
00:06:12.040 --> 00:06:14.110
Thanks everyone and I will see you at the next lecture.
