1
00:00:05,340 --> 00:00:10,860
Hello everyone and welcome to the walk through example lecture in the regression where sparks section

2
00:00:10,860 --> 00:00:16,470
of this course in this lecture we're going to walk you through an example of that actually implement

3
00:00:16,520 --> 00:00:18,870
Linaria Gresham line by line.

4
00:00:18,900 --> 00:00:21,590
Let's jump to our text editor to get started.

5
00:00:23,020 --> 00:00:23,450
All right.

6
00:00:23,450 --> 00:00:28,760
Here I am at the text editor I've opened up the Lin Regg walk through that Scala script.

7
00:00:28,790 --> 00:00:33,950
This is the script we are going to be walking you through will set up everything and explain line by

8
00:00:33,950 --> 00:00:36,440
line what's actually happening here.

9
00:00:36,770 --> 00:00:40,780
The text data will be working with is this USA housing data.

10
00:00:41,030 --> 00:00:46,730
This contains various columns such as average area income average area housing age number of rooms.

11
00:00:46,730 --> 00:00:50,290
Number of average bedrooms the population address of the house.

12
00:00:50,300 --> 00:00:55,670
And finally this price column is what we will try to predict with a regression model based off of all

13
00:00:55,670 --> 00:00:56,780
these other features.

14
00:00:56,780 --> 00:00:59,110
What does the actual price of the house.

15
00:00:59,180 --> 00:01:02,200
Let me start a new script and start coding this out.

16
00:01:03,140 --> 00:01:04,810
I will send you file.

17
00:01:04,830 --> 00:01:11,380
You will call this LR Scala and let's begin.

18
00:01:11,420 --> 00:01:22,570
First thing we want to do is import or Apache that spark that sequel and say spark session that's going

19
00:01:22,570 --> 00:01:29,280
to allow us to actually build or get or create that spark session and then we also need to import Apache

20
00:01:29,300 --> 00:01:37,110
the spark M-L regression and we will be using linear regression and later on if you want.

21
00:01:37,220 --> 00:01:41,060
You can use different regression models and one less import.

22
00:01:41,090 --> 00:01:48,380
I will do a peer is say or that Apache log for J underscore.

23
00:01:48,410 --> 00:01:57,380
Import everything there and that will say logger get logger person org here.

24
00:01:57,780 --> 00:02:06,780
And then we will set level 2 level dot error in all caps and that's just going to allow us to see a

25
00:02:06,780 --> 00:02:11,280
little bit less as far as the reporting back of the warnings.

26
00:02:11,280 --> 00:02:13,770
Now let's start a simple Sparke session.

27
00:02:14,930 --> 00:02:23,070
We'll say vowel Sparke Sparke session builder and then gets or creates.

28
00:02:23,070 --> 00:02:25,700
Keeping it simple right now not even giving an app name.

29
00:02:25,890 --> 00:02:30,120
And notice I'm not even putting all of this into a function because I want us to really play around

30
00:02:30,120 --> 00:02:31,880
with this to understand that.

31
00:02:32,010 --> 00:02:38,070
Now the next step is to actually prepare the training and test data something to know is the way that

32
00:02:38,160 --> 00:02:43,800
these machine learning algorithms take in data is in a very specific format.

33
00:02:43,820 --> 00:02:49,220
It's essentially just a data frame of two columns with the first column being the label column and then

34
00:02:49,220 --> 00:02:54,360
the second column being a column consisting of arrays of all the feature values and we will show you

35
00:02:54,750 --> 00:02:57,500
how to actually do that with your data.

36
00:02:57,870 --> 00:03:05,740
So I will say valid data is equal to spark that read this option.

37
00:03:07,080 --> 00:03:12,210
And I will set the header to be true and it can pass and strength's here should be fine.

38
00:03:13,160 --> 00:03:20,620
Now that option in first schema to also be true that what we actually get the data types we expect

39
00:03:23,280 --> 00:03:27,240
and then say format's we expect they see as we form it.

40
00:03:27,660 --> 00:03:31,180
And then finally we want to actually passen where the file is located.

41
00:03:32,170 --> 00:03:36,970
In this case the file is actually located in the same regression folder that we're in.

42
00:03:36,970 --> 00:03:45,810
So I can just say load USA underscore housing CXXVI and you'll have to check the pending on the file

43
00:03:46,020 --> 00:03:48,940
whether or not to pass and that extension.

44
00:03:48,940 --> 00:03:49,300
All right.

45
00:03:49,300 --> 00:03:50,940
So we have our data loaded up.

46
00:03:50,950 --> 00:03:58,550
Let's go ahead and just print the schema we'll say print schema save this and now let's run this mixture

47
00:03:58,560 --> 00:03:59,920
everything works.

48
00:03:59,930 --> 00:04:06,120
So to run this I will load all our thoughts Skala and then hit enter.

49
00:04:06,140 --> 00:04:12,040
All right we've imported everything and we can see now the printed schema we have average area income

50
00:04:12,050 --> 00:04:15,560
the housing age average number of rooms average number of bedrooms.

51
00:04:15,680 --> 00:04:21,380
The area's population the price of the house to double and then the address something we are going to

52
00:04:21,380 --> 00:04:24,960
want to do is to make sure that our formatting is correct.

53
00:04:25,010 --> 00:04:31,460
So we will have to make all these new airco columns B in a single features column with an array of each

54
00:04:31,460 --> 00:04:32,250
of those values.

55
00:04:32,280 --> 00:04:38,510
Ill probably have to drop off address since we dont know how to actually use text data just to see an

56
00:04:38,510 --> 00:04:41,130
example of what the data looks like.

57
00:04:41,150 --> 00:04:45,980
Let's walk through how to actually print out each row of data.

58
00:04:46,820 --> 00:04:50,740
So we will print out an example row instead of printing up scheme.

59
00:04:50,770 --> 00:05:00,380
If you say data head and then say one saving that and then loading this up again.

60
00:05:02,450 --> 00:05:08,900
You'll notice that you get an example here and what we can do is actually loop through the columns of

61
00:05:08,900 --> 00:05:09,490
the data.

62
00:05:09,500 --> 00:05:15,560
I will go in and just copy that for loop script so we don't spend a bunch of time walking it here.

63
00:05:15,880 --> 00:05:27,300
All we can say if we scroll up here this little for loop is going to just nicely print out a row in

64
00:05:27,300 --> 00:05:28,490
all the values for it.

65
00:05:28,800 --> 00:05:33,070
So we'll save this you know what we're doing here is just grabbing columns.

66
00:05:33,180 --> 00:05:39,190
Grab that first row in the column and then just print out that column and then that first row value.

67
00:05:39,220 --> 00:05:44,520
This is just a nice way of displaying an example of everything that's in the data frame.

68
00:05:44,550 --> 00:05:49,810
As far as what a typical Roe would look like so we can see an address typically looks like this.

69
00:05:49,920 --> 00:05:54,960
The price for this particular house is this area population for this house is this average number of

70
00:05:54,960 --> 00:06:00,870
bedrooms in the area of the house has four bedrooms etc. rooms and this is just a nicer way of presenting

71
00:06:00,870 --> 00:06:08,220
that data than the head that we saw earlier which is kind of ugly but it's up to you whether or not

72
00:06:08,220 --> 00:06:09,510
you want to use that.

73
00:06:09,570 --> 00:06:12,120
This is just a little example script for you.

74
00:06:12,120 --> 00:06:12,810
Copy and paste.

75
00:06:12,810 --> 00:06:17,190
In case you ever want it now more importantly we need actually discuss how to set up a data frame for

76
00:06:17,190 --> 00:06:18,200
machine learning.

77
00:06:18,570 --> 00:06:21,060
As I mentioned it needs to be in a very specific format.

78
00:06:21,120 --> 00:06:24,600
It needs to be in the format of a data frame.

79
00:06:24,600 --> 00:06:25,800
It looks like this.

80
00:06:25,830 --> 00:06:29,480
Labels or label is one column.

81
00:06:30,210 --> 00:06:35,110
And then features is another column where features is just an array of values.

82
00:06:35,160 --> 00:06:40,920
So let's use vector assembler in order to actually create this.

83
00:06:40,920 --> 00:06:42,240
Now I will need to import this.

84
00:06:42,240 --> 00:06:47,250
Usually you would do this all at the top of the script but I'm separating out my imports to the relevant

85
00:06:47,250 --> 00:06:47,910
sections.

86
00:06:47,910 --> 00:06:50,670
That way when you review this it's easier for you to understand.

87
00:06:51,030 --> 00:07:00,050
So we need to say that Apache that sparked the M-L feature dot and then say vector assembler and one

88
00:07:00,060 --> 00:07:05,790
other thing I want to do is say or at Apache that spark that m-L.

89
00:07:05,930 --> 00:07:11,520
Lynn Owch an import vector's.

90
00:07:11,750 --> 00:07:15,190
All right first thing we need to do is actually grab the label column.

91
00:07:15,190 --> 00:07:17,930
Now the label we are trying to predict is the price of the house.

92
00:07:18,040 --> 00:07:23,980
So let's go ahead and rename the price column to label column for naming convention and also only want

93
00:07:23,980 --> 00:07:26,470
to grab the numerical columns from the data.

94
00:07:26,500 --> 00:07:27,480
So here's a quick trick.

95
00:07:27,490 --> 00:07:33,280
In order to do this rumor that since we're using everything not an object we can actually call in SPARC

96
00:07:33,280 --> 00:07:35,740
shell data columns.

97
00:07:37,290 --> 00:07:42,050
And get back the columns themselves which is going to be nice when we actually need to type these out.

98
00:07:42,210 --> 00:07:45,980
So I will say Val DSF is equal to data.

99
00:07:46,140 --> 00:07:55,680
Select and then I will select the price column as label

100
00:07:58,690 --> 00:08:01,300
and then everything that's numerical.

101
00:08:01,300 --> 00:08:06,700
So I want the average area income average house age average number of rooms average number of bedrooms.

102
00:08:06,690 --> 00:08:07,990
Area population.

103
00:08:07,990 --> 00:08:12,130
I don't need the price because that's already here label and I don't want the address because we can't

104
00:08:12,130 --> 00:08:14,900
use this text radio for a linear regression model.

105
00:08:15,100 --> 00:08:20,260
We will copy that pasted in and then we will just edit it.

106
00:08:20,770 --> 00:08:23,570
Let's make sure it's actually lines up correctly.

107
00:08:24,640 --> 00:08:29,700
And I will wrap this in parentheses so I can use this under multiple lines.

108
00:08:29,780 --> 00:08:31,620
So let's start making some lines here.

109
00:08:32,550 --> 00:08:37,580
Won't say tab and then I just need a raptus in dollar sign quotes.

110
00:08:38,090 --> 00:08:40,280
So let's do that.

111
00:08:41,550 --> 00:08:46,560
For all of these column values so you'll know as you're working more and more with Sparke data frames

112
00:08:46,890 --> 00:08:50,890
it's actually pretty tedious to make sure everything's in the correct format.

113
00:08:51,270 --> 00:08:58,830
As you'll notice that I'm doing here this isn't exactly very fun but the main advantage is that once

114
00:08:58,830 --> 00:09:05,280
you have everything set up it's going to be instantly parallelizable across a distributed set of data.

115
00:09:05,280 --> 00:09:10,200
And the reason I have these outside princes is just to wrap everything in multiple lines.

116
00:09:10,200 --> 00:09:15,240
If I don't do that I need to have everything in a long single line which isn't really good practice.

117
00:09:15,390 --> 00:09:18,900
So we will go onto the next step.

118
00:09:19,100 --> 00:09:20,630
So I've selected my data.

119
00:09:20,630 --> 00:09:22,050
Let me save this.

120
00:09:22,070 --> 00:09:24,000
Run this and make sure it all worked.

121
00:09:24,350 --> 00:09:28,040
So let's load L.R. that Skala run this.

122
00:09:28,040 --> 00:09:34,140
And here we see DFAC So if I say DMF print schema then you can use.

123
00:09:34,220 --> 00:09:36,030
You can just use tab autocomplete there.

124
00:09:36,110 --> 00:09:39,320
You'll notice now I have label and then my numerical columns.

125
00:09:39,470 --> 00:09:41,280
That's the data frame we are going to be working with.

126
00:09:41,280 --> 00:09:48,030
Now now I need to use an assembler and that's the vector assembler what the assembler does.

127
00:09:48,030 --> 00:09:54,960
It will convert the input values to a single vector which is necessary because a vector or an array

128
00:09:55,230 --> 00:10:00,330
is what the machine learning algorithm needs to actually read to train the model.

129
00:10:00,330 --> 00:10:04,980
So we will set the input columns from which we are supposed to read the values and then set the name

130
00:10:04,980 --> 00:10:08,310
of the column or the vector will be stored.

131
00:10:08,420 --> 00:10:12,810
So we will say Val is assembler.

132
00:10:14,840 --> 00:10:18,070
Wups is equal to new.

133
00:10:18,410 --> 00:10:20,520
And here's where we will call vector assembler.

134
00:10:20,570 --> 00:10:27,670
So create a new vector assembler object and then you will set input columns end here you pass in an

135
00:10:27,670 --> 00:10:31,980
array of the input columns you expect to receive.

136
00:10:32,200 --> 00:10:35,360
In this case I expect to receive all my feature columns.

137
00:10:35,410 --> 00:10:40,290
So you don't include label here and in that case I can actually just copy these from the slide here

138
00:10:40,630 --> 00:10:43,070
so I can copy this paste it in.

139
00:10:43,090 --> 00:10:44,840
And this actually just except strings.

140
00:10:44,860 --> 00:10:49,330
If you read the documentation it will tell you it just needs the street names which means you don't

141
00:10:49,330 --> 00:10:53,200
actually need these dollar sign notations for Skala.

142
00:10:53,440 --> 00:10:56,370
And I'll put this all in one line even though that's a little bit about practice.

143
00:10:56,380 --> 00:11:00,500
But just to make it maybe a little more recognizable as far as what we're doing.

144
00:11:00,520 --> 00:11:03,710
So I'm saying vector Sembler set the input columns.

145
00:11:03,730 --> 00:11:08,880
I'm sending an array of all the feature columns as my input columns here.

146
00:11:09,280 --> 00:11:14,530
And the last thing I want to do is actually set the output column and in this case the output column

147
00:11:14,770 --> 00:11:16,660
should be called just features.

148
00:11:16,870 --> 00:11:19,230
So let me put this at the very end.

149
00:11:20,100 --> 00:11:31,710
Here so I will say that sets out put call and just in lowercase features and then I will put a closing

150
00:11:31,710 --> 00:11:34,740
parentheses here to actually split this up into multiple lines.

151
00:11:34,740 --> 00:11:39,340
Now that we have everything written out and then I will put the closing Quincy's there.

152
00:11:40,440 --> 00:11:44,220
And then let's format everything a little nicer.

153
00:11:46,860 --> 00:11:50,370
And then put that in one more line and there we have it.

154
00:11:50,370 --> 00:11:51,970
All right so what's actually happening here.

155
00:11:51,990 --> 00:11:57,970
I'm creating a new vector assembly assembler object set my input columns what goes in there.

156
00:11:58,110 --> 00:12:01,550
Well it's an array of all the string column names.

157
00:12:01,740 --> 00:12:07,740
And then I need a set output columns to just features what this actually does is it takes in all those

158
00:12:07,740 --> 00:12:13,560
columns in your data frame puts them into an array of a single column called features.

159
00:12:13,560 --> 00:12:28,640
Now let's go ahead and say Val output is equal to assembler that transform transform that data frame.

160
00:12:28,860 --> 00:12:38,340
And then I will selects the label column and the features column.

161
00:12:38,370 --> 00:12:39,370
Let's run this.

162
00:12:39,390 --> 00:12:40,360
Make sure it all works.

163
00:12:40,380 --> 00:12:47,220
And then we will explain why we're just selecting labels and features helps load L.R. Scala

164
00:12:50,010 --> 00:12:52,820
expand this a little bit and see what we have.

165
00:12:53,190 --> 00:12:53,730
All right.

166
00:12:53,880 --> 00:12:58,100
So we went ahead and imported vector Sembler got vectors there.

167
00:12:58,150 --> 00:13:02,470
We created this data frame which is just a numerical information with the label.

168
00:13:02,520 --> 00:13:09,930
Then we said assembler is this vector assembler and this output here if I check out what the output

169
00:13:10,020 --> 00:13:11,280
actually looks like.

170
00:13:13,240 --> 00:13:20,480
So I say output show notice here that the format is label which in this case is the price and trying

171
00:13:20,480 --> 00:13:24,480
to predict and features and features is an array.

172
00:13:24,770 --> 00:13:30,050
So all the features that I have are now under a single column as an array of features.

173
00:13:30,080 --> 00:13:36,410
And this is the formatting we need to actually work with the machine learning library in SPARC.

174
00:13:36,410 --> 00:13:43,550
Now if I happen to have not said select here so we will comment that out let me show you what happens

175
00:13:43,670 --> 00:13:46,190
if I just say assembler that transformed the f.

176
00:13:46,250 --> 00:13:52,590
In fact since we are dealing with everything I can actually just copy and paste this into spark shell

177
00:13:53,060 --> 00:13:53,970
and this should work

178
00:13:57,490 --> 00:13:57,950
OK.

179
00:13:58,120 --> 00:13:59,140
So notice what we have here.

180
00:13:59,140 --> 00:14:04,720
We actually keep in all the remaining columns and just add a features column at the end which is why

181
00:14:04,720 --> 00:14:08,210
we need to select only label and features.

182
00:14:08,320 --> 00:14:14,290
So if I say come at this out save this run this again.

183
00:14:14,690 --> 00:14:15,910
Whoops.

184
00:14:16,070 --> 00:14:26,720
LR load that Skala expand my terminal until this shows and then I say output that show that will happens

185
00:14:26,740 --> 00:14:27,050
here.

186
00:14:27,070 --> 00:14:28,870
I get all the old columns.

187
00:14:28,990 --> 00:14:36,420
So not only do I have label and then all my old columns and features I need to just have label and features

188
00:14:36,430 --> 00:14:38,530
which is why we use that select statement.

189
00:14:38,530 --> 00:14:41,160
So vector assembler creates that new column.

190
00:14:41,200 --> 00:14:46,690
But that doesn't mean it gets rid of all these old columns which is a reminder to you to remember after

191
00:14:46,690 --> 00:14:51,750
you do the assembler transform that old data you need to select only the columns you need in that case

192
00:14:51,770 --> 00:14:53,770
its label and features.

193
00:14:53,770 --> 00:14:54,310
All right.

194
00:14:54,490 --> 00:14:56,210
So we have that ready to go.

195
00:14:56,230 --> 00:15:01,270
Now it's time for the fun part which is actually creating our linear regression model object training

196
00:15:01,270 --> 00:15:03,590
and fitting it and then seeing the summary.

197
00:15:03,790 --> 00:15:06,460
I'm going to go ahead and stop the video here.

198
00:15:06,460 --> 00:15:11,890
We'll quickly review what we've done and in part two will go ahead and create the model and finish off

199
00:15:11,890 --> 00:15:15,370
with this lecture that as a reminder what have we done so far.

200
00:15:15,520 --> 00:15:19,870
Well we imported the spark session imported the linear regression model we haven't used that yet.

201
00:15:20,140 --> 00:15:25,690
The These two lines just to see less errors or less warnings I should say went ahead and created the

202
00:15:25,690 --> 00:15:30,550
spark session read in the see every file which was true inferring the schema.

203
00:15:30,550 --> 00:15:34,960
And then this is probably the most important part of what we've done so far this vector assembler and

204
00:15:34,960 --> 00:15:36,690
setting up the data as we need it.

205
00:15:36,760 --> 00:15:39,460
And this label and features array.

206
00:15:39,910 --> 00:15:41,740
You select the data.

207
00:15:41,740 --> 00:15:46,930
In this case you want to make sure that whatever you're trying to predict that label is called label

208
00:15:46,930 --> 00:15:52,730
as a column name then you grab all the numerical columns in this case.

209
00:15:52,780 --> 00:15:55,810
Later on we're going to explore how to deal with categorical columns.

210
00:15:55,900 --> 00:16:01,260
String information but right now we're dealing with simplified data where everything's just numerical.

211
00:16:01,270 --> 00:16:08,530
Once you've done that you go ahead and say create a new vector assembler set the input columns as an

212
00:16:08,530 --> 00:16:11,760
array and then set the output columns as features.

213
00:16:11,890 --> 00:16:15,330
And you just passen your features as an array.

214
00:16:15,730 --> 00:16:22,930
Then you say assembler transform this data frame that you have here that old data and you only want

215
00:16:22,930 --> 00:16:26,940
to select that label column and this newly created features column.

216
00:16:27,130 --> 00:16:31,480
Then you have everything set up for your machine learning model which we're about to cover in the next

217
00:16:31,480 --> 00:16:32,320
lecture.

218
00:16:32,320 --> 00:16:33,190
Have any questions.

219
00:16:33,190 --> 00:16:37,450
Go ahead and review the notes or feel free to post to the Kewney forums.

220
00:16:37,450 --> 00:16:39,500
Thanks everyone and I'll see you at the next lecture.
