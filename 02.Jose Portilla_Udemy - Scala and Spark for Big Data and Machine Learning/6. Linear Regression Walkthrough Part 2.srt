1
00:00:06,870 --> 00:00:12,950
Hello everyone and welcome to part two of the walk through example for regression with Sparke.

2
00:00:13,050 --> 00:00:18,240
Let's jump right back in where we left off last time and jumped to the editor for a script.

3
00:00:18,540 --> 00:00:18,870
All right.

4
00:00:18,870 --> 00:00:22,050
Here I am at the script right where we left off last time.

5
00:00:22,080 --> 00:00:27,840
Let's quickly review what we did last time we went ahead and imported sparks session in linear regression

6
00:00:28,230 --> 00:00:31,880
set the level of the warning to just error.

7
00:00:31,950 --> 00:00:33,490
That way we saw less output.

8
00:00:33,840 --> 00:00:35,370
Then we created a separate session.

9
00:00:35,370 --> 00:00:41,220
We went ahead and read in our CXXVI data and since the formatting was C S V We need to go ahead and

10
00:00:41,220 --> 00:00:43,620
make sure that everything's in the correct format.

11
00:00:43,620 --> 00:00:48,700
That means our data frame for the machine learning algorithm needs to have only two columns in it.

12
00:00:48,720 --> 00:00:52,960
One column is the label column which is just the actual label you're trying to predict.

13
00:00:53,130 --> 00:00:59,640
And then the next one is the features column that's going to be an array of all the features in order

14
00:00:59,640 --> 00:01:00,930
to do that.

15
00:01:00,990 --> 00:01:03,400
We essentially did three steps here.

16
00:01:03,540 --> 00:01:08,800
We said our original data grab that price column which is what we're trying to predict.

17
00:01:08,850 --> 00:01:13,880
Rename it the label column and grab all the other columns that I want to treat as features.

18
00:01:14,040 --> 00:01:19,290
In our case since we haven't learned how to do all of categorical data we will later on we're just grabbing

19
00:01:19,320 --> 00:01:27,480
numerical values then we needed to use vector assembler to select all those input columns as an array

20
00:01:27,990 --> 00:01:32,140
and then set an output column as a single column called features.

21
00:01:32,190 --> 00:01:38,380
After that we went ahead and said assembler transformed that data frame that we selected earlier.

22
00:01:38,610 --> 00:01:44,610
And since this vector assembler object creates a new column with this array of all the features we just

23
00:01:44,610 --> 00:01:48,960
want to grab those two columns so we say select label and then features.

24
00:01:48,960 --> 00:01:53,010
Now it's time to actually move on and create the linear regression model object.

25
00:01:53,010 --> 00:01:55,620
Train it and explore the results.

26
00:01:56,290 --> 00:01:59,320
To create a linear regression model object you just to save Val.

27
00:01:59,610 --> 00:02:05,120
LR is equal to new and then linear regression.

28
00:02:05,360 --> 00:02:06,690
Close parentheses.

29
00:02:06,710 --> 00:02:10,490
Now there's a lot more stuff you can add onto this such as different parameters.

30
00:02:10,490 --> 00:02:14,510
We will explore that later on in the model evaluation section.

31
00:02:14,510 --> 00:02:18,900
Keeping things simple will just create a base linear regression.

32
00:02:18,950 --> 00:02:23,450
The next thing we need to do is actually fit our model to our training data.

33
00:02:23,450 --> 00:02:29,910
So I will say well our model is equal to L-R that fit.

34
00:02:29,910 --> 00:02:33,460
And in this case I'm going to fit to the output.

35
00:02:33,630 --> 00:02:39,710
And as I mentioned in the model evaluation section will go over how to actually do a train test split.

36
00:02:39,750 --> 00:02:44,160
Right now we're doing what you usually should not do which is not split up your data and then train

37
00:02:44,610 --> 00:02:50,090
and get results from an entire singular dataset training and test splits are coming up.

38
00:02:50,160 --> 00:02:57,590
But we're keeping things as simple as possible to not jump to a higher level too fast once we've fitted

39
00:02:57,620 --> 00:03:11,910
our model to the actual data I can say Val training summary equal to L-R model summary and off of that

40
00:03:11,910 --> 00:03:19,110
summary object that training summary I can call the residuals and the residuals are the difference between

41
00:03:19,480 --> 00:03:22,600
our predictive value and the actual true value.

42
00:03:22,620 --> 00:03:27,270
So I'll say training that some rethought residuals and we can show that it should just be a data Frane

43
00:03:27,690 --> 00:03:28,960
Let's save this.

44
00:03:28,980 --> 00:03:35,390
Make sure everything ran correctly and then explore the output of the model summary I will load L.R.

45
00:03:35,420 --> 00:03:44,300
that score and then expand this terminal so we can see everything.

46
00:03:44,430 --> 00:03:48,630
All right here's the output we requested in this case we just wanted to show the residuals.

47
00:03:48,630 --> 00:03:53,010
And this is the residual for each label and each predicted value.

48
00:03:53,010 --> 00:03:57,040
Now we can actually explore a lot more information off of the training summary.

49
00:03:57,070 --> 00:04:02,700
Remember the training summary was just the model after it's been fit you call summary off of it.

50
00:04:02,700 --> 00:04:10,070
So I will say training summary and as I noted earlier what's nice about doing this all outside of an

51
00:04:10,070 --> 00:04:16,580
object is that in Sparke shell I can just say dot and then hit tab here and I will see all the methods

52
00:04:16,580 --> 00:04:20,070
or attributes available off this summary object.

53
00:04:20,180 --> 00:04:26,150
In this case I can ask for a lot of information such as what were my predictions if I want the predictions

54
00:04:26,150 --> 00:04:27,110
themselves.

55
00:04:27,110 --> 00:04:30,530
I can just say that predictions and it returns this data frame.

56
00:04:30,530 --> 00:04:37,010
I actually want to see if I can say that show or if I want to collect that as something I can say that

57
00:04:37,010 --> 00:04:37,640
collect.

58
00:04:37,640 --> 00:04:43,190
But keep in mind this sort of action can be dangerous if it's a extremely large dataset.

59
00:04:43,220 --> 00:04:47,710
So I'll say just not show up because it only shows the top 20 rows and what happens when you say that

60
00:04:47,750 --> 00:04:53,930
show Ill say the label column the features column and then show the actual predicted value and the residual

61
00:04:53,930 --> 00:04:57,350
is just a difference between this label and this prediction.

62
00:04:57,350 --> 00:05:02,570
If you want some more statistical information about how the summary actually went you can call.

63
00:05:02,600 --> 00:05:08,830
Go ahead and say that tab again here called the R-squared value which is just how much variances explained

64
00:05:08,830 --> 00:05:09,790
in our model.

65
00:05:09,920 --> 00:05:14,490
In our case we got about 91 percent or nine point nine one R-squared value.

66
00:05:14,590 --> 00:05:20,550
It's not so bad but remember we are taking that value off the same data we train those.

67
00:05:20,690 --> 00:05:24,830
So later on we'll need to check how to actually split up the data into a training set and a testing

68
00:05:24,830 --> 00:05:25,350
set.

69
00:05:25,520 --> 00:05:26,940
But keeping things simple right now.

70
00:05:26,960 --> 00:05:30,350
Hopefully you can see just how powerful the training summary is.

71
00:05:31,730 --> 00:05:38,780
Then we can call training summary and call stuff like the root mean square in our lips tab autocomplete

72
00:05:38,840 --> 00:05:41,980
that and it will report back the root mean squared error.

73
00:05:42,080 --> 00:05:47,260
Or you can even just get the means Square if you want it to.

74
00:05:47,270 --> 00:05:51,830
So there's a lot of stuff here that you can explore and rat information off of.

75
00:05:51,980 --> 00:05:53,990
But that's basically all there is to it.

76
00:05:54,050 --> 00:06:00,620
As far as actually training your model the most difficult part when you are creating a machine learning

77
00:06:00,830 --> 00:06:05,090
algorithm in Scala and in Sparke is your data set up.

78
00:06:05,210 --> 00:06:10,460
And there's a really common saying in data science and data engineering in general that 90 percent of

79
00:06:10,460 --> 00:06:14,270
your time is manipulating the data getting in the right format.

80
00:06:14,270 --> 00:06:19,100
Cleaning that data data manging as it's called and then 10 percent of your time is actually just setting

81
00:06:19,100 --> 00:06:19,940
up the algorithm.

82
00:06:19,940 --> 00:06:25,130
So unless you're some sort of machine learning algorithm expert where you are actually creating your

83
00:06:25,130 --> 00:06:30,170
own machine learning algorithms most of the time is going to be set up on these more tedious tasks of

84
00:06:30,170 --> 00:06:34,090
cleaning up your data making sure everything's in the correct format.

85
00:06:34,130 --> 00:06:39,650
So we will focus much more on that aspect of machine learning due to the fact that it's a little more

86
00:06:39,650 --> 00:06:41,920
tedious when you're working with Scala and SPARC.

87
00:06:42,050 --> 00:06:45,920
But the advantage comes into play when you're dealing with a large distributed data set.

88
00:06:45,980 --> 00:06:47,470
You essentially need to change your code.

89
00:06:47,510 --> 00:06:53,630
It's going to automatically scale to as large of a data set you want it will show how to do that later

90
00:06:53,630 --> 00:07:00,410
on on systems such as data breaks data Brick's excuse me or Amazon Web Services.

91
00:07:00,440 --> 00:07:05,280
All right go ahead and check out the rest of the walk through that Scala script.

92
00:07:05,300 --> 00:07:09,310
If you scroll down I show the print summarization.

93
00:07:09,320 --> 00:07:15,390
Things such as the number of iterations the history main square squared the residuals etc..

94
00:07:15,560 --> 00:07:19,750
If you have any questions on what we just did here feel free to post the Q&amp;A forums.

95
00:07:19,760 --> 00:07:24,500
Thanks everyone and I'll see you at the next lecture Uller where we will be going over your project

96
00:07:24,620 --> 00:07:26,260
for this section of the course.
