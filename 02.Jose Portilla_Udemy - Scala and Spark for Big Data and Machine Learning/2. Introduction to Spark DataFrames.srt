1
00:00:05,630 --> 00:00:09,250
Hello everyone and welcome to an introduction to spark data frames.

2
00:00:09,320 --> 00:00:13,790
We finally reached this section of the course where we're not going to be dealing with Skala by itself

3
00:00:14,010 --> 00:00:20,630
but we'll also be working with SPARK and capability is the spark specifically data frame's in the second

4
00:00:20,630 --> 00:00:24,680
or the course we're going to discussing what is a spark data frame we're going to talk about that in

5
00:00:24,680 --> 00:00:28,950
this lecture and then later on we'll talk about how to use smart data frames and how to program with

6
00:00:28,970 --> 00:00:34,820
data frames discussing things such as aggregate or grouped by operations how to index or select columns

7
00:00:34,820 --> 00:00:39,060
from a data frame or even how to deal with time stamp information and much more.

8
00:00:39,140 --> 00:00:43,550
Then at the end of this section will have some data frame exercises dealing of some financial data to

9
00:00:43,550 --> 00:00:49,710
test your understanding so Sparke data frames are now the standard way of dealing with data.

10
00:00:49,730 --> 00:00:56,150
For Skala and Sparke as well as other languages such as are in Sparc or Piscean spark spark is moving

11
00:00:56,150 --> 00:01:02,260
away from the traditional D syntax that it used to use in favor of the simpler to understand data framed

12
00:01:02,260 --> 00:01:03,120
syntax.

13
00:01:03,170 --> 00:01:08,660
And if you've worked in other programming languages that use data frames such as Python's pandas library

14
00:01:08,660 --> 00:01:13,910
or just the our language you probably already have a good familiarity of what a data frame is going

15
00:01:13,910 --> 00:01:14,700
to look like.

16
00:01:14,840 --> 00:01:19,640
If you haven't you can basically think of a data frame as kind of this large Excel spreadsheet where

17
00:01:19,640 --> 00:01:26,420
it has rows and then columns spark data frames are also now going to be the standard way of using Spark's

18
00:01:26,420 --> 00:01:31,490
machine learning capabilities with the Lib library or machine learning library.

19
00:01:31,490 --> 00:01:36,080
In fact if you actually go visit the documentation for Spark's machine learning capabilities you'll

20
00:01:36,080 --> 00:01:37,430
notice that there's two sections.

21
00:01:37,460 --> 00:01:43,050
One for the outdated RTD version and then one for the newer SPARC data frame version.

22
00:01:43,250 --> 00:01:46,930
So all upcoming developments will occur for Sparc data frames.

23
00:01:46,940 --> 00:01:52,640
Now that we've reached SPARC 2.0 dispersed data frame and general documentation is actually still pretty

24
00:01:52,640 --> 00:01:54,150
new and can be sparse.

25
00:01:54,320 --> 00:01:58,750
So let's go ahead and get a brief tour of that documentation to teach you how to best utilize it.

26
00:01:59,920 --> 00:02:05,020
Go ahead and go to spark that Apache org and explore the documentation there.

27
00:02:05,050 --> 00:02:07,160
So I'm going to go out and jump to my browser right now.

28
00:02:07,270 --> 00:02:11,130
So you have find that documentation and discuss how to best utilize it.

29
00:02:11,130 --> 00:02:13,290
I'm going to go ahead and jump to my browser now.

30
00:02:13,640 --> 00:02:17,770
OK so here we are at Sparks that Apache the org or let's go ahead and show you where you can find the

31
00:02:17,770 --> 00:02:20,070
documentation for Sparke data frames.

32
00:02:20,260 --> 00:02:25,310
Just come over here to the documentation tab and you should see the latest release documentation.

33
00:02:25,510 --> 00:02:26,680
Go ahead and click on it.

34
00:02:26,980 --> 00:02:32,000
And then here and there programming guides go ahead and scroll down to you see data frames data sets

35
00:02:32,050 --> 00:02:33,280
and sequel.

36
00:02:33,280 --> 00:02:37,910
Click on that and you should see the spark sequel data frames and data sets guides.

37
00:02:37,990 --> 00:02:42,610
This is just a general overview guide and if you actually click here in the time you'll get an overview

38
00:02:42,610 --> 00:02:47,500
explanation of how sequel data sets and data frames all relate to each other.

39
00:02:47,500 --> 00:02:54,400
That basically sparked sequel is a spark module that allows you to use sequel like syntax or exact sequel

40
00:02:54,400 --> 00:03:00,700
syntax so you queery structured data and then when you do that sort of query you can get a data set

41
00:03:00,700 --> 00:03:07,260
back a dataset is a distributed collection of data and then a data frame is a data set organized into

42
00:03:07,330 --> 00:03:12,590
named columns data frames is the main data type but we're going to be working with here.

43
00:03:12,610 --> 00:03:18,640
But go ahead and on your own time read about this overview here to totally understand how data sets

44
00:03:18,640 --> 00:03:24,400
relate to data frames and how secure can be use to actually query a data frame or data set.

45
00:03:24,400 --> 00:03:29,670
Later on we'll actually show you how you can use sequel syntax for clearing a data frame.

46
00:03:29,680 --> 00:03:34,150
Now we won't focus on that particular aspect because that assumes you have sequel knowledge.

47
00:03:34,150 --> 00:03:39,470
Instead we'll actually show you how to do most of this stuff just through basic Skala if you keep scrolling

48
00:03:39,470 --> 00:03:44,480
down here you'll see stuff like basic getting started documentation and then creating data frames.

49
00:03:44,780 --> 00:03:51,230
Now here under the creating data frames you have a very simple example of reading H.A. some file showing

50
00:03:51,230 --> 00:03:51,890
that.

51
00:03:51,890 --> 00:03:56,750
And then you'll see something that says the columns of data frame with some rows of data and then if

52
00:03:56,750 --> 00:04:00,020
you come down here you'll see three basic operations.

53
00:04:00,020 --> 00:04:07,160
Print schema select the column select on the increment one column by a certain value do a little bit

54
00:04:07,160 --> 00:04:13,280
of filtering and then do it by and you'll notice that's actually it really for the documentation as

55
00:04:13,280 --> 00:04:19,730
far as basic programming guides everything else is just topics on other things such as data sets or

56
00:04:19,730 --> 00:04:21,400
interpreting these etc..

57
00:04:21,410 --> 00:04:26,240
But as far as working to spark data frame as a programming guide this is really all there is at the

58
00:04:26,240 --> 00:04:27,200
moment.

59
00:04:27,200 --> 00:04:30,600
Now you can come down here to the bottom of this.

60
00:04:30,670 --> 00:04:33,210
If you go here you'll have data from operations.

61
00:04:33,260 --> 00:04:38,280
So if you scroll down here at the bottom you'll see that there's a complete API documentation.

62
00:04:38,450 --> 00:04:39,780
That's what I really want you to know.

63
00:04:39,800 --> 00:04:44,840
And there's a link in the resource section to this API documentation directly.

64
00:04:44,900 --> 00:04:46,190
Go ahead and click on that.

65
00:04:46,490 --> 00:04:52,760
OK so this is the full API documentation page for data sets and remember data frames are just data sets

66
00:04:53,000 --> 00:04:55,530
organized into columns of column names.

67
00:04:55,670 --> 00:05:00,800
So they're very similar but here you can scroll down see a lot more code examples of actually create

68
00:05:00,820 --> 00:05:05,630
in work with these data frames or data sets and then more importantly as you come and scroll down you'll

69
00:05:05,630 --> 00:05:11,820
see all the various actions and transformations that are available to you on this data frame object.

70
00:05:11,830 --> 00:05:18,350
So this is really important to know and visit this page if you're looking for a specific action or transformation

71
00:05:18,350 --> 00:05:22,580
you want to perform on a specific dataset or data frame.

72
00:05:22,580 --> 00:05:26,750
Now keep in mind we're going to be going over a lot of these transformations and actions especially

73
00:05:26,750 --> 00:05:28,610
the most common or useful ones.

74
00:05:28,610 --> 00:05:33,030
Things such as order by or filtering etc..

75
00:05:33,370 --> 00:05:35,190
And in case you're looking for something specific.

76
00:05:35,330 --> 00:05:40,490
So is a day to come here to the API page and see if you can find that yourself and you can also just

77
00:05:40,490 --> 00:05:43,010
search the API documentation here.

78
00:05:43,010 --> 00:05:48,590
Now keep in mind a lot of this stuff is still new and if it's really new you'll see something like experimental

79
00:05:48,590 --> 00:05:52,540
attached to it which means it's still experimental There may be some bugs in it.

80
00:05:52,550 --> 00:05:54,770
So if you're working with production code.

81
00:05:54,830 --> 00:06:01,630
Keep that in mind as far as how stable you need your environment to be and your code to be all right.

82
00:06:01,640 --> 00:06:07,520
So go ahead and explore the documentation not just this API documentation but also the previous page

83
00:06:07,550 --> 00:06:12,050
in order to learn a little bit more about the very basics of data frames and data sets.

84
00:06:12,110 --> 00:06:17,270
Coming up next we're going to give you an actual programming overview of how to create a spark session

85
00:06:17,300 --> 00:06:20,150
how to create a data frame and how to work for data frame.

86
00:06:20,150 --> 00:06:22,640
All right I hope you're excited for this section of the Course.

87
00:06:22,670 --> 00:06:26,700
This is going to be some of the latest and greatest for Skala and Sparke.

88
00:06:26,750 --> 00:06:28,700
Thanks everyone and I'll see you at the next lecture.
