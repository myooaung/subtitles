1
00:00:05,280 --> 00:00:09,870
Hello everyone and welcome to an introduction to Kamins clustering in this lecture.

2
00:00:09,870 --> 00:00:16,380
We're going to discuss the K means clustering algorithm which will allow us to cluster and label data

3
00:00:16,440 --> 00:00:19,630
in an unsupervised machine learning algorithm.

4
00:00:19,710 --> 00:00:22,130
You want the full mathematics behind this algorithm.

5
00:00:22,200 --> 00:00:24,660
Go ahead and read Chapter 10 of an introduction.

6
00:00:24,750 --> 00:00:32,860
Just learning k means clustering is an unsupervised learning algorithm meaning it takes an unlabeled

7
00:00:32,920 --> 00:00:37,940
data it's going to attempt to group similar clusters together in your data.

8
00:00:37,960 --> 00:00:40,750
So what is the typical clustering problem look like.

9
00:00:40,750 --> 00:00:46,000
Well a few examples may be things like clustering similar documents together clustering customers based

10
00:00:46,000 --> 00:00:52,030
off of their features trying to perform market segmentation or even trying to identify similar physical

11
00:00:52,030 --> 00:00:53,050
groups.

12
00:00:53,840 --> 00:01:00,650
The overall goal is to divide data into distinct groups such observations within each group are similar.

13
00:01:00,650 --> 00:01:08,330
Here you can see a diagram of unlabelled training data and k means clustering trying to attempt to cluster

14
00:01:08,330 --> 00:01:12,650
the data into five distinct clock colored clusters here.

15
00:01:14,200 --> 00:01:15,980
How does the algorithm actually work.

16
00:01:16,220 --> 00:01:23,090
Well you first choose the number of clusters K and we'll discuss how to choose k later on then you randomly

17
00:01:23,090 --> 00:01:29,690
assign each point in your data to a specific cluster then until the clusters stop changing.

18
00:01:29,690 --> 00:01:35,360
You repeat the following steps for each cluster you're going to compute the cluster centroid by taking

19
00:01:35,360 --> 00:01:38,200
the mean vector of points in the cluster.

20
00:01:38,270 --> 00:01:43,710
Then you assign each data point to the cluster for which the centroid is the closest.

21
00:01:44,000 --> 00:01:50,000
Let's go ahead and take a look at the state of visualization or this plotted out visualization of k

22
00:01:50,000 --> 00:01:55,990
means clustering algorithm you first start off of just the observations that's shown on the top left

23
00:01:56,530 --> 00:01:59,160
then in top center we have Step One of the algorithm.

24
00:01:59,410 --> 00:02:06,490
Each observation is randomly assigned to a cluster in the top right of step to a the cluster centroid

25
00:02:06,580 --> 00:02:08,080
are then computed.

26
00:02:08,080 --> 00:02:10,730
These are shown as large colored disks.

27
00:02:10,780 --> 00:02:15,970
Initially these centroid are almost completely overlapping because the initial cluster assignments were

28
00:02:15,970 --> 00:02:23,560
chosen at random in the bottom left and step to be each observation is assigned to the nearest centroid

29
00:02:24,240 --> 00:02:25,270
in the bottom center.

30
00:02:25,270 --> 00:02:27,590
Step two is a once again perform.

31
00:02:27,670 --> 00:02:34,240
You can see that we've led to new cluster centroid and we basically keep repeating the steps until there's

32
00:02:34,270 --> 00:02:40,750
no new clusters meaning data points aren't being reassigned to a new cluster centroid at the bottom

33
00:02:40,750 --> 00:02:40,960
right.

34
00:02:40,960 --> 00:02:46,140
We have the results obtained after about 10 iterations.

35
00:02:46,360 --> 00:02:50,270
Let's discuss choosing a k value as we just saw before.

36
00:02:50,270 --> 00:02:51,550
K means clustering.

37
00:02:51,580 --> 00:02:54,790
We also decide how many clusters we expect in the data.

38
00:02:54,790 --> 00:02:59,330
The problem of selecting K isn't actually that simple.

39
00:02:59,360 --> 00:03:02,320
There is no easy answer for choosing a best case value.

40
00:03:02,510 --> 00:03:07,310
However one way to try to do it is using something known as the elbow method.

41
00:03:07,340 --> 00:03:12,560
First of all you compute the sum of squared error otherwise known as s s e.

42
00:03:12,560 --> 00:03:13,960
For some values of k.

43
00:03:13,960 --> 00:03:17,460
For example 2 4 6 8 etc..

44
00:03:17,480 --> 00:03:22,940
The sum of squares heir's is defined as the sum of the square distance between each member of the cluster

45
00:03:23,030 --> 00:03:25,440
and that centroid.

46
00:03:25,460 --> 00:03:32,900
If you plot K against the SS e you will see that the error decreases as Kagiso larger just because of

47
00:03:32,900 --> 00:03:34,360
the number of clusters increases.

48
00:03:34,360 --> 00:03:37,950
They should be smaller so distortion is also smaller.

49
00:03:37,970 --> 00:03:43,800
The idea of the Elbel method is to choose the K at which the s s e decreases abruptly.

50
00:03:44,000 --> 00:03:48,920
This produces an elbow effect in the graph as you can see in the following picture.

51
00:03:48,950 --> 00:03:54,580
Here we can see an example of they plotted out Elbot method on the X horizontal axis.

52
00:03:54,590 --> 00:04:01,190
We have the K value which stands for the number of clusters you choose for the algorithm on the y axis

53
00:04:01,190 --> 00:04:08,690
you have the within groups sum of squares you want to try to choose a K where you won't get that much

54
00:04:08,690 --> 00:04:12,990
more information by increasing the number of clusters.

55
00:04:13,090 --> 00:04:19,220
Are not going to significantly decrease the within groups sum of squared error by increasing number

56
00:04:19,220 --> 00:04:20,430
of clusters.

57
00:04:20,510 --> 00:04:22,730
And that's why this is called the elbow method.

58
00:04:22,880 --> 00:04:28,750
As you can see an elbow occurring around the number let's say six to seven clusters.

59
00:04:28,760 --> 00:04:32,040
That means that's a good number to choose for your clusters.

60
00:04:32,070 --> 00:04:34,540
Again there's no correct value.

61
00:04:34,610 --> 00:04:39,260
So you want to use your domain experience or domain knowledge in whatever field you're working with

62
00:04:39,590 --> 00:04:43,820
in order to try to choose a correct or reasonable case value.

63
00:04:43,820 --> 00:04:49,370
Now that we know the theory behind K means clustering we're going to explore the documentation example

64
00:04:49,760 --> 00:04:54,320
in this particular case the documentation example is actually straightforward enough that we don't have

65
00:04:54,320 --> 00:04:56,590
to show our own custom example.

66
00:04:56,720 --> 00:05:01,650
So after seeing the documentation example afterwards you're going to go straight into an exercise project

67
00:05:01,650 --> 00:05:02,750
to complete.

68
00:05:02,780 --> 00:05:04,730
Thanks everyone and I will see at the next lecture.
