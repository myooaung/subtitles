1
00:00:05,320 --> 00:00:10,060
Hello everyone and welcome to the walk through example lecture where we walk you through how to perform

2
00:00:10,060 --> 00:00:15,550
logistic regression for the classification section for machine learning with SPARK.

3
00:00:15,630 --> 00:00:19,950
We're going to walk through a more complicated classification example using the classic Titanic data

4
00:00:19,950 --> 00:00:21,440
set from Kaggle.

5
00:00:21,450 --> 00:00:26,560
This basically is a data set that contains passenger information for passengers on the Titanic.

6
00:00:26,670 --> 00:00:32,580
And we want to try to classify or predict what passengers survived the Titanic and what passengers didn't

7
00:00:32,580 --> 00:00:34,800
survive the actual crash of the Titanic.

8
00:00:34,800 --> 00:00:37,370
You can check this all out at Kaggle dot com.

9
00:00:37,440 --> 00:00:42,650
And just google searching for the Titanic data set there if you want more of a background on that actual

10
00:00:42,650 --> 00:00:43,420
data set.

11
00:00:43,650 --> 00:00:48,360
We will go ahead and walk through an example on how to actually deal with this more complicated and

12
00:00:48,360 --> 00:00:50,130
actual real data.

13
00:00:50,130 --> 00:00:53,680
Let's get started by jumping to the Adam text editor.

14
00:00:53,730 --> 00:00:54,000
OK.

15
00:00:54,000 --> 00:00:55,940
Here I am at the text editor.

16
00:00:55,980 --> 00:01:01,820
If you go to your classification folder you should see an example script called log Reggy example Scala.

17
00:01:01,980 --> 00:01:08,040
And this is basically a simplified version of a more complicated logistic regression script.

18
00:01:08,070 --> 00:01:13,920
And the reason for that is because I'm not going to use an object layout due to objects not allowing

19
00:01:13,920 --> 00:01:17,070
us to really play around the data inside the spark shell.

20
00:01:17,070 --> 00:01:19,470
I'm going to just put it in line by line.

21
00:01:19,470 --> 00:01:23,920
The imports the value statements the functions everything we're going to be doing.

22
00:01:24,360 --> 00:01:31,980
OK I'm going to leave this for you to reference and I'm going to start a new file here called log Reg

23
00:01:32,010 --> 00:01:37,870
X Skala and actually walk through line by line type all this out and explain along the way.

24
00:01:38,100 --> 00:01:42,450
So you can always just reference everything I'm doing here and then log examples call a script.

25
00:01:42,450 --> 00:01:45,800
I've created a new one called log Reg export Skala.

26
00:01:46,140 --> 00:01:50,710
First thing I want to do is some of our imports import stuff along the way.

27
00:01:50,940 --> 00:01:55,620
As I've mentioned many times before I usually want to do all your imports at the top of a script but

28
00:01:55,620 --> 00:02:01,490
for now just for learning purposes I'm going to just put them all as we need them.

29
00:02:01,560 --> 00:02:06,910
So we want org got a patch that sparked the M-L the classification.

30
00:02:07,050 --> 00:02:09,060
And this is where we choose a specific algorithm.

31
00:02:09,060 --> 00:02:17,100
In this case I want logistic regression and then up next I want to import or Apache that spark that

32
00:02:17,100 --> 00:02:25,040
sequel spark session and there's an optional script here you can type in.

33
00:02:25,110 --> 00:02:26,670
This is just the logger.

34
00:02:26,850 --> 00:02:30,320
So I'm going to just copy and paste this from that script.

35
00:02:30,420 --> 00:02:35,390
And this basically allows us to not see a whole bunch of warnings.

36
00:02:35,570 --> 00:02:36,440
All right.

37
00:02:36,440 --> 00:02:39,100
First thing I want to do is actually create the spark session.

38
00:02:39,140 --> 00:02:40,530
Keeping things simple right now.

39
00:02:40,720 --> 00:02:46,640
We'll just say Val spark is equal to spark session builder.

40
00:02:47,380 --> 00:02:52,110
And then we will get or create that spark session then that's quite a few times during the spark data

41
00:02:52,120 --> 00:02:57,160
frame lecturers not doing anything fancy right now by saying how many cores you are or what we want

42
00:02:57,160 --> 00:02:58,300
the app to be named.

43
00:02:58,330 --> 00:03:02,230
Keeping things simple so we can focus on the logistic regression aspects.

44
00:03:02,230 --> 00:03:06,510
Up next we want to use Sparke to read in the actual Titanic CACP file.

45
00:03:06,640 --> 00:03:12,520
If you look at your classification folder you should notice that there is a way expand this out a bit

46
00:03:12,730 --> 00:03:14,650
Titanic that see every file.

47
00:03:14,830 --> 00:03:20,470
If you open that up you'll notice that there is a passenger ID column you survived the column indicating

48
00:03:20,520 --> 00:03:21,090
zero.

49
00:03:21,080 --> 00:03:22,090
If they did not survive.

50
00:03:22,120 --> 00:03:25,270
What if they did survive the passenger class.

51
00:03:25,270 --> 00:03:30,970
So passengers were on three separate class and the Titanic first second or third class the name of that

52
00:03:30,970 --> 00:03:35,870
passenger which isn't actually going to be super super useful for us at this point in time.

53
00:03:35,890 --> 00:03:41,270
The sex of that passenger male or female the age of the passengers some number age.

54
00:03:41,320 --> 00:03:47,760
Sid espie that's the number of siblings and spouses they had on board this par S.H..

55
00:03:47,800 --> 00:03:49,230
That's a number of parents are children.

56
00:03:49,240 --> 00:03:52,050
They also had on board the ticket.

57
00:03:52,090 --> 00:03:56,110
So they have some sort of ticket number the fare that they paid for that ticket.

58
00:03:56,110 --> 00:04:01,630
The cabin number that they were in and then embarked and embarked you'll notice is also just a letter

59
00:04:01,630 --> 00:04:11,450
string where it's S or C or q which designate what actual town they embarked on OK.

60
00:04:11,510 --> 00:04:13,840
You'll notice that we have some categorical data that we want to deal with.

61
00:04:13,880 --> 00:04:19,170
We haven't actually delved categorical data before so we'll explore how to do that in this lecture.

62
00:04:19,170 --> 00:04:22,120
Now let's print out the schema of this data frame.

63
00:04:22,310 --> 00:04:27,870
Well first let's actually grab this and create a data frame from the Titanic cxxviii file so say valid

64
00:04:27,870 --> 00:04:29,990
data spark.

65
00:04:30,020 --> 00:04:33,380
We're going to do that before read option.

66
00:04:33,440 --> 00:04:35,580
I want that header to be true.

67
00:04:36,660 --> 00:04:44,520
Just pressing the strings here and then the other option I want is to infer schema I will also say

68
00:04:47,880 --> 00:04:50,980
and then I'm going to say format c s v.

69
00:04:51,030 --> 00:04:59,330
We know at c a c file and then just say load Titanic the CSP OK.

70
00:04:59,430 --> 00:05:02,600
And you can use parentheses to split this up into multiple lines if you want.

71
00:05:02,670 --> 00:05:06,600
It's not really good style to have everything in one long line but sometimes it's easier for you to

72
00:05:06,600 --> 00:05:07,760
understand what's going on.

73
00:05:07,890 --> 00:05:12,270
If you can see everything builds off of each other totally up to you how you want to approach this.

74
00:05:12,720 --> 00:05:19,160
OK moving along now it's actually print that schema so we can get an idea of what it actually looks

75
00:05:19,160 --> 00:05:19,880
like.

76
00:05:20,060 --> 00:05:27,920
We're going to save this blog Wrex open up a terminal here already have a terminal that's running Skala

77
00:05:28,370 --> 00:05:30,540
let's make sure it's actually working.

78
00:05:31,000 --> 00:05:33,610
And I'm going to load log.

79
00:05:34,070 --> 00:05:37,800
Reg XT sculler enter.

80
00:05:38,010 --> 00:05:41,990
We should see it load up that data and there we have the schema.

81
00:05:42,020 --> 00:05:47,930
So again here the calls we have passed already survived the class name and you notice that we have some

82
00:05:47,930 --> 00:05:54,080
of these as strings and things such as the name of the passenger aren't going to be especially useful

83
00:05:54,080 --> 00:05:55,730
for us right now.

84
00:05:55,730 --> 00:06:00,710
For instance a machine learning algorithm isn't going to be able to detect more accurately whether someone

85
00:06:00,710 --> 00:06:04,860
survives if their name is Charlie or if their name is John.

86
00:06:05,150 --> 00:06:07,810
Well you can do is something called feature engineering.

87
00:06:07,820 --> 00:06:12,090
We try to engineer new features off a feature column.

88
00:06:12,170 --> 00:06:21,350
If you go back to the file Titanic TSB you'll notice that under the name column scroll to the left some

89
00:06:21,350 --> 00:06:25,860
of these people have titles such as Mr. MS. Mrs. M..

90
00:06:25,940 --> 00:06:31,210
And later on he keeps rolling down and see things such as Dr again Master etc..

91
00:06:31,280 --> 00:06:36,070
Now that's a feature that may or may not be useful and it's something you may want to explore.

92
00:06:36,140 --> 00:06:39,890
We'll keep things simple right now and we'll just drop the name column.

93
00:06:39,980 --> 00:06:41,720
But there are certain columns we don't want to drop.

94
00:06:41,720 --> 00:06:46,850
There are still strings such as the sex column or even the embarked column.

95
00:06:46,970 --> 00:06:52,460
These are categorical data columns that we're going to have to deal with so we'll have to focus on how

96
00:06:52,460 --> 00:06:53,980
to actually do that.

97
00:06:54,710 --> 00:06:55,290
OK.

98
00:06:55,610 --> 00:07:02,240
Let's go ahead and display the data instead of typing this all out I'm going to just copy and paste

99
00:07:02,380 --> 00:07:04,220
the display data.

100
00:07:04,220 --> 00:07:05,800
We've done this before many times.

101
00:07:05,810 --> 00:07:13,940
It's basically just a way of printing out an example row and going to copy this and paste this into

102
00:07:13,940 --> 00:07:15,220
my file.

103
00:07:15,440 --> 00:07:16,260
Save this.

104
00:07:16,310 --> 00:07:21,710
And I will press up on this key and run the script again and this will print out an example row for

105
00:07:21,710 --> 00:07:22,300
us.

106
00:07:22,520 --> 00:07:28,190
OK so we have cut Cabane no embarked s fer $7 me five cents.

107
00:07:28,330 --> 00:07:31,740
Some sort of ticket number 0 parents were children on board.

108
00:07:31,820 --> 00:07:37,310
One sibling or spouse the age the sex is male and this guy's name is Mr Owen Harris.

109
00:07:37,310 --> 00:07:39,750
He was in third class did not survive.

110
00:07:39,770 --> 00:07:40,900
Sorry.

111
00:07:40,910 --> 00:07:45,620
Now that we understand what the data more or less looks like we want to be in setting up a data frame

112
00:07:45,620 --> 00:07:46,850
for machine learning.

113
00:07:46,910 --> 00:07:49,900
The first step in doing this is to grab the actual data.

114
00:07:49,910 --> 00:07:51,350
We want to use.

115
00:07:51,500 --> 00:08:03,650
I'm going to create a value called log Reg data all and they all set equal to data that selects an I'm

116
00:08:03,650 --> 00:08:05,450
going to select my label column.

117
00:08:05,450 --> 00:08:08,210
Remember we're trying to predict how someone survived or not.

118
00:08:08,210 --> 00:08:17,930
So I will say select data survived and I'll set this as lower case label number how important it is

119
00:08:17,930 --> 00:08:23,470
to actually have the label and feature orientation for data transfer machine learning.

120
00:08:23,690 --> 00:08:33,240
And I'm going to use open print CS here and a close parentheses here so I can actually start writing

121
00:08:33,240 --> 00:08:34,780
this on multiple lines.

122
00:08:35,870 --> 00:08:36,450
OK.

123
00:08:36,470 --> 00:08:40,100
Next up we want to actually grab the columns we want.

124
00:08:40,310 --> 00:08:45,920
I'm going to end up using the past year class column the sex column the age column the sibling and spouse

125
00:08:45,950 --> 00:08:50,000
parent child column the Fair column and the barked column the other columns.

126
00:08:50,000 --> 00:08:56,270
I don't want to use in order to kind of quickly type this all out a simple trick you can do if you're

127
00:08:56,270 --> 00:09:03,600
using no objects and spark shell in the way I'm doing it say data columns enter and you can be into

128
00:09:03,620 --> 00:09:05,360
just copy and paste some of these.

129
00:09:05,420 --> 00:09:13,530
So I once not passenger ID we really don't survive want P.E. class name sex age parent child ticket.

130
00:09:13,940 --> 00:09:19,160
And then the fair and the embarked and I'm going to delete some of these that actually don't want in

131
00:09:19,160 --> 00:09:19,800
a little bit.

132
00:09:21,360 --> 00:09:28,160
So let's highlight this right click copy and then right click paste.

133
00:09:28,170 --> 00:09:32,520
This should save you some time when you're trying to actually grab features.

134
00:09:32,520 --> 00:09:35,680
Remember these need to have the sort of dollar notation.

135
00:09:35,830 --> 00:09:49,480
So I'm going to set this all up I want peak class I want name enter here I do want the sex of the individual

136
00:09:50,620 --> 00:09:58,450
age aunt's sibling spouse put this back up here.

137
00:09:58,710 --> 00:10:00,220
How many parent child they have.

138
00:10:00,300 --> 00:10:05,320
I don't want a ticket that's not going to be very useful to me because it's just some sort of information

139
00:10:05,350 --> 00:10:06,440
that I don't understand.

140
00:10:06,600 --> 00:10:13,360
Maybe later for future engineering maybe useful Cabane we can just ignore that for now and then embarked.

141
00:10:13,380 --> 00:10:15,910
We will also have that what city they embark on.

142
00:10:16,280 --> 00:10:16,930
OK.

143
00:10:17,100 --> 00:10:18,370
So those are all the columns.

144
00:10:18,390 --> 00:10:19,760
Let's close up these princes.

145
00:10:19,800 --> 00:10:20,880
Make sure they match

146
00:10:23,820 --> 00:10:31,810
and check that they match so survived sear labels their data's selecting their looks like I have one

147
00:10:31,810 --> 00:10:34,920
too many parentheses going to delete that.

148
00:10:35,020 --> 00:10:35,530
OK.

149
00:10:35,530 --> 00:10:37,300
Now that should all work.

150
00:10:37,630 --> 00:10:42,910
I'm going to save this and run it just to make sure that all worked out.

151
00:10:44,130 --> 00:10:48,680
Let's load it up and make sure we have log rig data all OK.

152
00:10:48,740 --> 00:10:50,060
We have our data frame.

153
00:10:50,090 --> 00:10:55,070
Next thing I want to do is actually drop missing values in this case.

154
00:10:55,070 --> 00:11:02,810
We have a few We want to drop and I can do this just by saying log Reg data is going to be able to log

155
00:11:02,880 --> 00:11:07,120
data all day and a drop.

156
00:11:07,150 --> 00:11:11,480
Keep in mind at this point in time this is technically still experimental but it's been around since

157
00:11:11,480 --> 00:11:15,760
Sparke around one point three or 1.6 I want to say.

158
00:11:15,950 --> 00:11:18,050
So it should work just fine.

159
00:11:18,050 --> 00:11:22,020
Now there's a few things we need to deal with before Spahr can actually accept this data.

160
00:11:22,190 --> 00:11:27,340
And this is one of the main focus of the lecture is how to actually deal with these categorical columns.

161
00:11:27,380 --> 00:11:31,140
And we're going to use vector assemblers and vectors.

162
00:11:31,160 --> 00:11:36,260
I want to take a quick tour of the documentation and then in the next lecture we'll actually show you

163
00:11:36,530 --> 00:11:39,480
how to build out these vector assemblers and vectors.

164
00:11:39,680 --> 00:11:41,720
I'm going to save the script as I have it.

165
00:11:41,730 --> 00:11:43,750
Now jump to the documentation.

166
00:11:43,790 --> 00:11:46,790
Explore some of these vector assemblers and vectors with you.

167
00:11:46,790 --> 00:11:51,950
And then we will in the next lecture actually implement this and want to open up my browser and jump

168
00:11:51,950 --> 00:11:53,900
to spark that Apache the org.

169
00:11:54,210 --> 00:11:58,400
All right here Amyt sparked that Apache the org go to documentation.

170
00:11:58,400 --> 00:12:05,690
Latest release programming guides machine learning and then hit here under extracting transforming and

171
00:12:05,690 --> 00:12:07,660
selecting features.

172
00:12:07,670 --> 00:12:12,770
This is where we can actually see all the tools that SPARC has for us for extracting transforming and

173
00:12:12,770 --> 00:12:19,100
selecting features and one of the transformations we have to do is dealing with categorical columns.

174
00:12:19,100 --> 00:12:22,380
We've already seen how to work with vector assemblers and vectors.

175
00:12:22,490 --> 00:12:29,030
But now we need to work with string data that's things such as the gender which was male or female or

176
00:12:29,030 --> 00:12:36,080
the city of embarkation which was s q or C and if you scroll down here you will see that there is a

177
00:12:36,080 --> 00:12:37,600
string indexer.

178
00:12:37,700 --> 00:12:43,070
And basically what the string Incs index or does it encodes a string column of labels to a column of

179
00:12:43,070 --> 00:12:44,130
label in the.

180
00:12:44,150 --> 00:12:46,960
And there's actually an example of how this kind of works here.

181
00:12:47,060 --> 00:12:54,380
You can say the category and then if you have some idea zero it's in Category A B and C you can kind

182
00:12:54,380 --> 00:13:00,050
of think of these as strings of male female or strings of those cities that the passengers were getting

183
00:13:00,050 --> 00:13:01,190
on the ship at.

184
00:13:01,370 --> 00:13:08,350
Then you can use string index to create some sort of category index to convert that to a numerical value.

185
00:13:08,830 --> 00:13:09,630
OK.

186
00:13:09,710 --> 00:13:15,230
And then eventually you can drop the category column because your machine learning algorithm isn't going

187
00:13:15,230 --> 00:13:21,100
to understand how to actually take a letter but it will understand how to take a number.

188
00:13:21,110 --> 00:13:26,540
Now that's one way of doing this but you can actually expand on this by then converting this category

189
00:13:26,660 --> 00:13:30,280
index into what's known as one hot encoding.

190
00:13:30,620 --> 00:13:32,030
And there's real quick.

191
00:13:32,030 --> 00:13:38,030
An example of how to use string indexer which is pretty nice you can refer to this in the documentation

192
00:13:38,030 --> 00:13:39,970
for more details on that API.

193
00:13:39,980 --> 00:13:44,150
But the next one I want to show you that we're going to use in the next lecture is if you scroll back

194
00:13:44,150 --> 00:13:44,940
up here.

195
00:13:44,990 --> 00:13:51,770
One hot encoder and if you're familiar with dummy variables maybe from some of my other courses or just

196
00:13:51,860 --> 00:13:57,620
machine learning or using excel in general you know about 100 coding already because it's pretty similar

197
00:13:57,620 --> 00:13:59,090
to a dummy variable.

198
00:13:59,120 --> 00:14:03,440
Basically what one encoding does is it maps a column of label indices.

199
00:14:03,530 --> 00:14:09,970
Basically the result of the string indexer to a column of binary vectors with at most a single one value.

200
00:14:10,100 --> 00:14:13,640
This encoding allows algorithms which expect continuous features.

201
00:14:13,670 --> 00:14:19,090
In our case logistic regression to use categorical features and that's basically we're going to be doing.

202
00:14:19,190 --> 00:14:23,760
And you can see here that this is exactly the steps we are going to follow.

203
00:14:23,780 --> 00:14:28,930
We'll start with a string indexer that will take in this category ABC etc..

204
00:14:28,990 --> 00:14:34,910
Set some sort of output for it and then fit that data frame then we will transform that data frame and

205
00:14:34,910 --> 00:14:42,980
after that we will use one hot encoder to convert that into columns of binary vectors zeros or ones.

206
00:14:43,040 --> 00:14:47,210
So you'll end up having a column called a with zero or one values.

207
00:14:47,210 --> 00:14:52,250
Was this passenger coming from City 0 for false 1 for true.

208
00:14:52,430 --> 00:14:58,850
Then you'll have another column B which if they show up in a as true B and C will be false etc..

209
00:14:58,870 --> 00:15:04,310
Ok so I encourage you to actually read the docs if you want more details on that API or you can actually

210
00:15:04,310 --> 00:15:05,090
just click here.

211
00:15:05,090 --> 00:15:11,180
One Hutten coding this links straight to the Wikipedia article on what hot if it's a little confusing

212
00:15:11,180 --> 00:15:11,860
for you.

213
00:15:11,870 --> 00:15:16,550
Keep in mind one hot sometimes refers to digital circuits so don't let this sort of stuff confuse you

214
00:15:16,550 --> 00:15:16,940
here.

215
00:15:17,000 --> 00:15:21,950
Basically just think of it as creating what's known as a dummy variables and we'll explain that a little

216
00:15:21,950 --> 00:15:25,890
more than we actually see the output of this one Hotton decoder.

217
00:15:25,910 --> 00:15:27,040
It's not so complicated.

218
00:15:27,080 --> 00:15:34,100
You're just converting string columns into columns of binary zero or one true for that particular string

219
00:15:34,100 --> 00:15:34,720
value.

220
00:15:34,910 --> 00:15:37,160
OK so that's enough of me talking.

221
00:15:37,160 --> 00:15:40,020
Let's move on and actually implement all of this.

222
00:15:40,040 --> 00:15:44,450
I will see you at the next lecture where we will jump right back into the code and actually implement

223
00:15:44,450 --> 00:15:47,150
string indexers in one hot encoders.

224
00:15:47,150 --> 00:15:48,930
Thanks everyone and I'll see you at the next lecture.
