1
00:00:00,420 --> 00:00:02,860
Hi and welcome back to this NABJ toil.

2
00:00:03,000 --> 00:00:05,390
We have finally builds our transformer.

3
00:00:05,550 --> 00:00:08,480
So we are now ready to get to training.

4
00:00:08,700 --> 00:00:14,850
These pots might be longer than usual because we will have to do a lot of custom training as it is explained

5
00:00:14,850 --> 00:00:22,080
in the article that somewhere we would just have to follow the instructions in the article and we will

6
00:00:22,080 --> 00:00:24,090
be ready to finally train our models.

7
00:00:24,090 --> 00:00:37,230
So let us start by well creating our model will first clean the session just to make sure session and

8
00:00:37,230 --> 00:00:39,080
define all hyperbolic images.

9
00:00:39,090 --> 00:00:44,860
So let's define them parameters.

10
00:00:45,010 --> 00:00:51,930
First we need to define the dimensions of our model model.

11
00:00:52,380 --> 00:00:57,090
And let's pick one hundred twenty eight.

12
00:00:57,230 --> 00:01:03,420
We'll write on the rights what they used in the article we will use most of the time lower numbers because

13
00:01:03,630 --> 00:01:05,270
just for the sake of the example.

14
00:01:05,280 --> 00:01:09,230
And so that for your first training it's shorter.

15
00:01:09,540 --> 00:01:15,660
We will use lower Hopper barometers but if you want to try even better results and use the ones they

16
00:01:15,660 --> 00:01:18,660
use in the article feel free to do it.

17
00:01:18,840 --> 00:01:25,650
I used those once it makes the training faster and I had the results I was expecting so I decided to

18
00:01:25,650 --> 00:01:27,000
stick with those.

19
00:01:27,150 --> 00:01:36,940
Then we need the number of layers let's say for again it's six in the article the number of units in

20
00:01:36,940 --> 00:01:49,430
the FIT forward but let's puts 512 in the article they used to forty eights.

21
00:01:49,500 --> 00:01:51,840
Well two thousand and forty eight.

22
00:01:52,260 --> 00:01:54,070
Now the number of projection.

23
00:01:54,140 --> 00:02:01,170
This is what they call the number of heads in the article and we will use the same ones as.

24
00:02:01,170 --> 00:02:07,970
Sorry the same one as in the article which is eight.

25
00:02:08,120 --> 00:02:10,170
And finally the drop outs.

26
00:02:10,330 --> 00:02:12,380
Right.

27
00:02:12,560 --> 00:02:20,810
Which again will be exactly the same thing as in the article so you will point 1.

28
00:02:21,140 --> 00:02:25,630
Now that we have all our high profile images.

29
00:02:25,910 --> 00:02:38,050
Let's finally build the transformer transform a equals transformer and let's be lazy

30
00:02:40,860 --> 00:02:44,780
we'll copy this busted the

31
00:02:50,500 --> 00:02:53,290
make its head a bit more clean

32
00:02:57,680 --> 00:03:10,430
and finally drop outs so equals capsize English because this is how we called it's in the very beginning

33
00:03:10,430 --> 00:03:26,930
after the data after processing equals vocab size French equals D model equals no layers because if

34
00:03:26,950 --> 00:03:43,060
and units equals number of projections and equals dropouts who are transform is ready to be used.

35
00:03:43,690 --> 00:03:51,160
So now will in this breach builds all the viable as all the functions that are related to the loss and

36
00:03:51,160 --> 00:03:51,910
the accuracy.

37
00:03:51,910 --> 00:04:00,700
So first let's create a class objects as we are doing categorical work because basically we output some

38
00:04:00,700 --> 00:04:05,320
kind of probability for each word to be the next one.

39
00:04:05,470 --> 00:04:13,510
Each word inside all vocabulary so we want all lost objects to be T F that US that's losses.

40
00:04:13,510 --> 00:04:24,760
So less objects and these will be sparse category call cross entropy.

41
00:04:24,760 --> 00:04:34,330
So this is the standard loss objects in this case and we want to specify form objects equals true which

42
00:04:34,330 --> 00:04:40,840
means that the outputs of our model are just real numbers that are ready to become probabilities by

43
00:04:40,840 --> 00:04:43,360
applying later s Max for instance.

44
00:04:43,360 --> 00:04:53,770
And second thing we want to do is to apply no prediction at all what this means is that usually the

45
00:04:53,770 --> 00:05:02,260
objects will some of the all the losses of all the batches for instance in order to have just one number

46
00:05:02,320 --> 00:05:04,150
which is the loss for the whole batch.

47
00:05:04,150 --> 00:05:10,350
But we don't want to do that because we will first want to get rid of the losses from the putting tokens.

48
00:05:10,360 --> 00:05:15,430
So we will do it by ourself and actually we will do it right now.

49
00:05:15,430 --> 00:05:26,530
In this last function that we declare to the targets which is what we want the module to find and the

50
00:05:26,530 --> 00:05:30,570
predictions we eat which is actually what we get from the transformer.

51
00:05:30,640 --> 00:05:34,290
So others said we wanted to well to mask the padding tokens.

52
00:05:34,300 --> 00:05:44,800
When we compute the loss so let us create this mask to have that math that's logical not see if that's

53
00:05:44,860 --> 00:05:51,050
math that's equal targets zero.

54
00:05:51,060 --> 00:05:58,650
So this way we have a mask that will get rid of all the losses that correspond to a zero in the targets.

55
00:05:58,650 --> 00:06:11,480
So putting padding token and let's say that loss equals less objects of targets and pred.

56
00:06:11,640 --> 00:06:17,850
So he would just compute the standard loss that we get from sports category called gross entropy with

57
00:06:17,850 --> 00:06:19,190
the target and the prediction.

58
00:06:19,290 --> 00:06:25,350
But without any predictions so we can still apply the mask and then send over all the bunches.

59
00:06:25,500 --> 00:06:32,340
So let's just make sure that we can multiply the mask to the loss that we computed just before.

60
00:06:33,030 --> 00:06:39,930
And the best way to do that is to make sure they are all the same time with gusts.

61
00:06:39,930 --> 00:06:49,280
So we don't change the values just the type of the mask and lowest a d type.

62
00:06:49,300 --> 00:06:58,180
So now we are sure that the values inside loss and and mask are of the same type.

63
00:06:58,180 --> 00:07:07,400
So we can just multiply loss by by the mask.

64
00:07:07,830 --> 00:07:14,070
And this way we just put as you will at the loss where we have the putting tokens.

65
00:07:14,100 --> 00:07:21,430
So so we only use the projections for the real sentence and not just the zeros we add that the ends

66
00:07:21,660 --> 00:07:24,090
in order to compute the loss.

67
00:07:24,420 --> 00:07:32,790
And now we are ready to return what we gets after having a son of over all the matches actually all

68
00:07:32,790 --> 00:07:33,980
dimensions.

69
00:07:34,170 --> 00:07:37,830
So we have a fully working class function.

70
00:07:38,340 --> 00:07:46,230
Let's just creates triangles which will keep track of the losses during the training.

71
00:07:46,230 --> 00:07:50,160
We do that by calling to add that q.

72
00:07:50,160 --> 00:08:01,900
That's that's matrix that's mean and let's call it's train loss here we are.

73
00:08:01,960 --> 00:08:04,510
Yeah name equals train loss.

74
00:08:05,540 --> 00:08:11,020
And finally with the last we also wants to get the accuracy of the training so let's say.

75
00:08:11,020 --> 00:08:14,680
Train accuracy equals t efforts cue us.

76
00:08:15,200 --> 00:08:30,080
That's metrics that's sparse sparse categorical accuracy and again means let's simply name its train

77
00:08:30,860 --> 00:08:31,640
accuracy.

78
00:08:33,040 --> 00:08:33,510
Here we are.

79
00:08:33,940 --> 00:08:38,070
We have the lost the accuracy.

80
00:08:38,130 --> 00:08:45,400
Now here comes the important part from the algorithm so let's just have a quick look at what they said

81
00:08:45,400 --> 00:08:51,730
for the training to use it optimizer of course to indicate how we will use the gradients to optimize

82
00:08:51,730 --> 00:08:56,010
the model and the optimizer that they used in the article is quite standards.

83
00:08:56,020 --> 00:09:02,410
It's the other optimizer but they used a custom learning rates that just don't like the add an algorithm

84
00:09:03,130 --> 00:09:04,750
do the learning rate by itself.

85
00:09:04,750 --> 00:09:09,400
So let's just compute the learning rates that they said they used right here.

86
00:09:11,630 --> 00:09:21,800
So we will need to create a class that we will call custom schedule that inherits from.

87
00:09:21,820 --> 00:09:34,470
That's optimizer is thus schedule does Ning rates schedule.

88
00:09:34,480 --> 00:09:43,220
So this is just how we need to process if we want to create our own custom learning rates.

89
00:09:43,360 --> 00:09:50,260
This is the class that inherits from another one so we have first to call in its

90
00:09:53,380 --> 00:10:02,070
and of course we need the dimension of the module at the number of one of steps that we set by default

91
00:10:02,100 --> 00:10:06,310
to what they use in the paper.

92
00:10:06,390 --> 00:10:14,940
So this is just explained hey you have a number of one of steps equals to four thousands.

93
00:10:14,980 --> 00:10:22,030
This just means that during the first 4000 steps this is the second part he that will be lower than

94
00:10:22,030 --> 00:10:22,920
the first parts.

95
00:10:23,020 --> 00:10:30,340
So we will follow this curve which is basically just a linear curve and after we reach the step in the

96
00:10:30,390 --> 00:10:36,970
4000 this will become lower than this one so we will follow this one which is just a standard decreasing

97
00:10:36,970 --> 00:10:38,960
curve like that.

98
00:10:38,980 --> 00:10:49,300
So let's write this one called super custom schedule self.

99
00:10:50,260 --> 00:10:59,920
That's called in it's as per usual we will need to have access to the dimension of the model.

100
00:10:59,960 --> 00:11:02,430
And so let's define it right there.

101
00:11:02,680 --> 00:11:08,440
And the fact is that later we will apply as quotes to this number.

102
00:11:08,440 --> 00:11:12,470
So actually we don't want this to be an integer.

103
00:11:12,610 --> 00:11:22,960
That's why we call T evidence casts and we make it's the floats to the photos to all right.

104
00:11:22,960 --> 00:11:37,140
So now sending for the warm up steps that we want to access later equals warm up steps.

105
00:11:37,260 --> 00:11:39,430
So let's define the call function.

106
00:11:39,930 --> 00:11:43,700
In this case we are not dealing with layers or model anymore.

107
00:11:43,710 --> 00:11:48,570
The call function has to be written that way with the underscores.

108
00:11:48,690 --> 00:11:56,940
And of course it uses self and step which is not viable that we declare by ourself or that we use or

109
00:11:56,940 --> 00:11:57,390
whatever.

110
00:11:57,390 --> 00:12:02,120
This is something that is inside the objects that we create.

111
00:12:02,310 --> 00:12:06,180
And it keeps memory of the step we are in at the moment.

112
00:12:06,180 --> 00:12:14,010
So this is always opponents of the coal methods in any learning rate schedule and absolutely the whole

113
00:12:14,160 --> 00:12:15,850
object will deal with it by itself.

114
00:12:17,050 --> 00:12:24,490
So we created the variables that we will need to implement the custom learning rates from the article

115
00:12:25,060 --> 00:12:32,920
we apply square roots which means instead of using the power zero point five we use minus zero open

116
00:12:32,940 --> 00:12:36,440
five as in the article of step.

117
00:12:37,600 --> 00:12:45,390
The second argument for the minimum will be step times set.

118
00:12:45,410 --> 00:12:51,420
That's 1 up steps forward.

119
00:12:53,060 --> 00:12:59,480
Minus one point five and now we just have to return the ETF.

120
00:12:59,480 --> 00:13:05,120
That's math that's once again we use the power minus zero point five.

121
00:13:05,650 --> 00:13:22,960
So as to our t to sell that model model and we multiply it by TAF that's math that's mini minimum of

122
00:13:23,020 --> 00:13:25,310
argon and R2.

123
00:13:25,350 --> 00:13:32,130
Here we are exactly the customer learning rate that was explained in the article.

124
00:13:32,180 --> 00:13:35,660
Now we can just create one.

125
00:13:37,140 --> 00:13:48,780
So learning rates will be cost to schedule with the model.

126
00:13:48,900 --> 00:13:49,610
Here we are.

127
00:13:49,690 --> 00:13:51,930
And we can finally create all optimizer.

128
00:13:51,940 --> 00:13:55,730
So the other optimizer with all customer learning rates T.

129
00:13:55,780 --> 00:13:56,500
That's cue us.

130
00:13:56,500 --> 00:13:58,130
That's up.

131
00:13:58,300 --> 00:13:59,130
Sorry.

132
00:13:59,180 --> 00:14:01,630
That's optimizer.

133
00:14:01,630 --> 00:14:06,510
That's Adam with all lending rates.

134
00:14:06,510 --> 00:14:09,080
They said that's better.

135
00:14:09,180 --> 00:14:25,520
One should be equal to zero point nine but the two should be zero point zero points ninety eight Epsilon

136
00:14:26,270 --> 00:14:36,430
Epsilon should be equal to 1 e minus 9.

137
00:14:36,460 --> 00:14:36,910
Perfect.

138
00:14:36,940 --> 00:14:40,960
We have the whole optimizer that has been created.

139
00:14:41,010 --> 00:14:43,350
Now we are really close to the end.

140
00:14:43,390 --> 00:14:49,650
Let us just before creating our training loop gets to the checkpoint phase because the training may

141
00:14:49,950 --> 00:14:56,070
take some time so we wanted to create checkpoints and to store the model at the end of each.

142
00:14:56,130 --> 00:15:06,490
So let's do it by creating our checkpoint path equals and we will actually store it in all google drive

143
00:15:06,490 --> 00:15:23,080
that we mounted at the very beginning and the path was my drive projects transformer and C K Piti of

144
00:15:23,080 --> 00:15:28,810
course you will have to create this folder in your personal drive if it's not or it's done.

145
00:15:29,380 --> 00:15:36,610
Also before I forget the drives you mounted at the very beginning may have been dismantled because I

146
00:15:36,610 --> 00:15:42,250
don't really know why but Google collab usually lose the connection to your google drive if you don't

147
00:15:42,490 --> 00:15:43,840
use it enough.

148
00:15:43,840 --> 00:15:49,680
So just before you do this and more importantly just before you train the the model.

149
00:15:49,780 --> 00:15:56,340
Don't forget to once again run the cell that is about mounting the drive in the server.

150
00:15:56,410 --> 00:15:59,510
Otherwise saving the checkpoint will not work.

151
00:15:59,530 --> 00:16:05,980
So make sure this is not the case for me right now but make sure that queue in the files you see the

152
00:16:06,340 --> 00:16:12,280
drive appearing and that's if you go to the X power you have the the right files in it so that you are

153
00:16:12,280 --> 00:16:15,220
sure that's your notebook has access to your drive.

154
00:16:16,000 --> 00:16:16,250
Okay.

155
00:16:16,270 --> 00:16:25,810
So we have our checkpoint path so let's create the checkpoint objects with T F the train that checkpoint.

156
00:16:26,000 --> 00:16:36,290
We need to give him the module we want to to save agents finder and also an object optimizer which is

157
00:16:37,250 --> 00:16:41,830
exactly our optimizer transform and optimizer.

158
00:16:42,020 --> 00:16:43,150
So here are checkpoints.

159
00:16:43,180 --> 00:16:49,700
We'll know that it has to store the status of the transformer and the optimizer.

160
00:16:49,700 --> 00:16:51,130
That's perfect.

161
00:16:51,200 --> 00:16:55,180
Let's create a manager for all checkpoints.

162
00:16:55,530 --> 00:17:08,350
So see if the train that's checkpoints manager and it will have to manage the checkpoints in this path.

163
00:17:08,390 --> 00:17:09,530
Checkpoint path.

164
00:17:09,710 --> 00:17:15,930
And let's say that we want to keep them well.

165
00:17:16,100 --> 00:17:17,300
Five checkpoints.

166
00:17:17,330 --> 00:17:21,080
So that means that's in order for the right here.

167
00:17:21,200 --> 00:17:27,620
We will be able to store five different checkpoints so the five last checkpoints that we that we have

168
00:17:27,620 --> 00:17:28,360
created.

169
00:17:28,490 --> 00:17:33,590
If we have more than five checkpoints if we just did it the older one and create a new one to replace

170
00:17:33,590 --> 00:17:41,500
its last thing we want to do is that we want to check if the checkpoints already exists and if so we

171
00:17:41,500 --> 00:17:42,690
want to restore it.

172
00:17:42,690 --> 00:17:52,770
So if S.K. BDC manager that s latest checkpoints.

173
00:17:52,860 --> 00:17:55,970
So this is available from the checkpoint manager.

174
00:17:56,110 --> 00:18:04,360
And basically if a checkpoints exists in the folder then the latest checkpoints will contain this status

175
00:18:04,450 --> 00:18:06,910
of the model and the optimizer.

176
00:18:06,910 --> 00:18:12,990
Otherwise of course it would be known so if latest checkpoints will behave like true.

177
00:18:13,090 --> 00:18:17,410
If there is already a checkpoints in the folder otherwise it will behave like force.

178
00:18:17,410 --> 00:18:22,620
So if there was one Sikh APC that we store

179
00:18:25,590 --> 00:18:32,800
and we want to restore this specific checkpoint which is C.J. Beatty manager.

180
00:18:32,950 --> 00:18:38,520
That's latest checkpoint.

181
00:18:38,550 --> 00:18:45,840
Well of course you don't have to do that if you want to train your model from scratch once again you

182
00:18:45,930 --> 00:18:51,540
just have to commence these last lines and you will let's restore the checkpoints annual starts from

183
00:18:51,540 --> 00:18:53,230
the very beginning.

184
00:18:53,340 --> 00:19:01,710
It's just things a fancy latest checkpoint.

185
00:19:01,820 --> 00:19:07,900
Start just to be aware of whether we are training a new model or if we are starting from an existing

186
00:19:07,900 --> 00:19:08,610
one.

187
00:19:08,800 --> 00:19:09,220
Okay.

188
00:19:09,220 --> 00:19:10,410
I think everything's ready.

189
00:19:10,420 --> 00:19:14,400
We have the laws the accuracy the optimizer.

190
00:19:14,490 --> 00:19:18,710
We also have checkpoints if we want to keep track of the training.

191
00:19:18,750 --> 00:19:23,540
So finally we just have to create our loop in order to train the model.

192
00:19:23,550 --> 00:19:30,500
So let's say that we want to go through 10 epochs.

193
00:19:30,570 --> 00:19:33,540
So for epoch in

194
00:19:36,180 --> 00:19:38,470
branch e-books

195
00:19:42,270 --> 00:19:49,650
let's prints that you are starting the approach then the county gets.

196
00:19:49,650 --> 00:19:50,630
Is this how you say it.

197
00:19:50,880 --> 00:20:00,040
Anyway we apply formats epoch plus 1 and let's just keep track of the time it takes to go through the

198
00:20:00,040 --> 00:20:03,070
whole e-book so time that's time

199
00:20:07,340 --> 00:20:13,250
let's reset the loss at the beginning of each epoch.

200
00:20:13,270 --> 00:20:23,510
So states and same thing for the accuracy we want to clean the history of the losses and the accuracy

201
00:20:23,960 --> 00:20:26,990
at the very beginning of each epoch.

202
00:20:27,530 --> 00:20:30,450
States.

203
00:20:30,440 --> 00:20:30,910
Yes sorry.

204
00:20:30,920 --> 00:20:33,210
This was the train losses but train less.

205
00:20:33,950 --> 00:20:34,930
OK.

206
00:20:35,210 --> 00:20:38,390
So now we're ready to each race of each batch.

207
00:20:38,390 --> 00:20:53,520
So for each and call the inputs and targets in enumerates data sets.

208
00:20:53,690 --> 00:21:01,550
So we use the enumerates tools so that we get each item plus an index accounts of where we are through

209
00:21:01,550 --> 00:21:04,830
the whole process which is stored in the best viable.

210
00:21:04,880 --> 00:21:11,050
And if you remember well that I said we build contents the inputs and outputs.

211
00:21:11,060 --> 00:21:18,590
So this is why we have to get them like this as a couple so let's declare the inputs for the decoder.

212
00:21:18,620 --> 00:21:30,650
That's our basically the target's minus the last words and the targets of the outputs we are aiming

213
00:21:30,740 --> 00:21:32,720
at the end of the decoder.

214
00:21:32,750 --> 00:21:40,670
So they are they are just targets once again but this time they are shifted towards the right.

215
00:21:40,670 --> 00:21:47,330
So we just get rid of the firsts elements which anyway is just the starting token if you remember well.

216
00:21:47,480 --> 00:21:54,530
And so in the outputs the what no I is the projection for the word that follows the word and then the

217
00:21:54,530 --> 00:21:57,370
eye in the decode the inputs.

218
00:21:57,620 --> 00:22:02,540
And that is why in the look ahead process we masked any words that is in the future.

219
00:22:02,560 --> 00:22:09,200
So in position I you can have access to all the words from zero to buy but not the following ones.

220
00:22:09,200 --> 00:22:11,060
So that's exactly what he wants.

221
00:22:11,120 --> 00:22:17,960
And so let's apply the transformer keeping track of what happens and the best way to do this is to call

222
00:22:17,960 --> 00:22:25,720
the transformer into the block defined by the objects crashing tape us tape.

223
00:22:25,720 --> 00:22:32,180
So tape will record everything that happens when we do predictions.

224
00:22:32,180 --> 00:22:44,740
Equal equals transformer applied to and codec inputs decoding inputs and the training will be true because

225
00:22:44,740 --> 00:22:45,720
we are in the training phase.

226
00:22:45,730 --> 00:22:55,220
So we wanted to apply the dropouts and he always just computes the loss that we get from the predictions.

227
00:22:55,480 --> 00:23:00,490
Outputs real and predictions.

228
00:23:01,150 --> 00:23:01,660
Perfect.

229
00:23:02,650 --> 00:23:13,630
So we can get out of this block and compute the gradients so gradients will be equal to tape.

230
00:23:13,640 --> 00:23:15,050
That's great.

231
00:23:15,060 --> 00:23:21,830
So this is a method from the tape that we created thanks to great tape that records everything that

232
00:23:21,830 --> 00:23:28,700
happens so it knows everything about the training the trainable waits inside the transformer and how

233
00:23:28,700 --> 00:23:31,610
they work together in order to come through the predictions.

234
00:23:31,610 --> 00:23:35,420
And so we can get gradients out of this.

235
00:23:35,480 --> 00:23:47,620
We just have to give it the less and the list of transformer trainable weights all valuables actually

236
00:23:47,630 --> 00:23:48,140
in this case.

237
00:23:48,140 --> 00:23:52,180
This is exactly the same trainable valuables.

238
00:23:52,250 --> 00:24:06,390
Now let's just apply the gradients according to our optimizer so optimizer that apply gradients to zip

239
00:24:07,260 --> 00:24:11,540
gradients because we have to send everything in a zip format.

240
00:24:11,550 --> 00:24:17,480
The graduates and again the least of the valuables that we want to train.

241
00:24:17,500 --> 00:24:23,720
So transformer that's trainable valuables.

242
00:24:23,790 --> 00:24:24,270
Here we are.

243
00:24:25,270 --> 00:24:36,790
Let's just add all us to the train less object that keep track of the loss and do the same thing with

244
00:24:36,810 --> 00:24:38,990
train accuracy.

245
00:24:39,090 --> 00:24:46,920
Train accuracy the outputs real and predictions the effects.

246
00:24:47,340 --> 00:24:55,960
Let's just prints the loss and the accuracy from time to time so let's say if Buch can be divided by

247
00:24:56,110 --> 00:25:05,930
50 so it's a multiple of 50 let's prints something like epoch and then the

248
00:25:09,350 --> 00:25:10,640
and much none the

249
00:25:13,170 --> 00:25:14,720
loss.

250
00:25:14,750 --> 00:25:16,270
Here we space Fi.

251
00:25:16,280 --> 00:25:17,350
How many decimals.

252
00:25:17,360 --> 00:25:27,300
We wanted to print from our flood so let's say for an accuracy same thing.

253
00:25:27,740 --> 00:25:37,900
That's not for F and let's call formats and say that's the first one we want to print a park plus one

254
00:25:40,080 --> 00:25:46,360
then batch then train loss that's results.

255
00:25:46,980 --> 00:25:54,130
Then same thing with train accuracy train accuracy that's results.

256
00:25:54,470 --> 00:26:03,880
The effects finally we can get out of the end of the loop of all the batches we are at the end of an

257
00:26:03,880 --> 00:26:04,180
epoch.

258
00:26:04,180 --> 00:26:16,600
So as we said we want to save our check points so we get the safe path by calling C.K. BDC manager that

259
00:26:16,610 --> 00:26:34,040
save and inform the user so ourselves that saving check points for epoch appropriate number that's appropriate

260
00:26:34,050 --> 00:26:40,450
path that formats and we wanted to prints.

261
00:26:40,510 --> 00:26:45,330
Once again epoch plus 1 and C.K. Beatty save.

262
00:26:45,410 --> 00:26:47,230
But here we are.

263
00:26:47,230 --> 00:26:55,540
And finally let's print how long it took time taken for one epoch.

264
00:27:00,080 --> 00:27:12,180
Appropriate number of seconds that formats and time the time so current time minus time we it at the

265
00:27:12,180 --> 00:27:17,070
beginning of the of the epoch starts the effect.

266
00:27:17,070 --> 00:27:20,150
So the whole training phase is now ready to be used.

267
00:27:20,190 --> 00:27:26,070
I will not run this cell myself because in my current session the whole took advising and cleaning phase

268
00:27:26,070 --> 00:27:30,390
for the data has not been done and it will take a long time to do that.

269
00:27:30,390 --> 00:27:38,070
So the training candidates operate right now but just lets you discover by yourself the fancy printings

270
00:27:38,130 --> 00:27:43,330
that will come with the training and it should take about a few hours.

271
00:27:43,380 --> 00:27:48,000
If you use the same parameters as I did you can also use less epochs.

272
00:27:48,000 --> 00:27:53,240
I think that we have maybe only five bucks you can have decent translations.

273
00:27:53,430 --> 00:28:01,440
But anyway how fun to you in a few hours and we will finally be able to evaluate and to see if all transformer

274
00:28:01,470 --> 00:28:05,100
is able to make good translations so as using.
