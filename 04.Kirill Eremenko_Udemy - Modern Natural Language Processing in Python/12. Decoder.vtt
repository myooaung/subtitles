WEBVTT
1
00:00:00.700 --> 00:00:02.980
Hi guys and welcome back to this email course.

2
00:00:03.130 --> 00:00:09.760
So right before we wrote the first part of the encoder of the transformer sorry which is the encoder

3
00:00:10.120 --> 00:00:14.630
and now let's start to right the second part the decoder again.

4
00:00:14.680 --> 00:00:18.620
Let's have a quick look at the architecture of the transformer.

5
00:00:18.640 --> 00:00:20.110
We already wrote these parts.

6
00:00:20.110 --> 00:00:22.190
Now let's make this one.

7
00:00:22.240 --> 00:00:24.580
This has pretty much the same architecture.

8
00:00:24.580 --> 00:00:25.870
It's quite similar.

9
00:00:25.900 --> 00:00:29.790
The only difference is that's in the decoder layer.

10
00:00:29.800 --> 00:00:37.060
We will use the at the attention mechanism twice first time it's self attention so again we will only

11
00:00:37.060 --> 00:00:44.830
use the input of the decoder by the second time we will use both the produce vectors that we have inside

12
00:00:44.860 --> 00:00:48.700
the decoder and the fruits of the encoder.

13
00:00:49.030 --> 00:00:52.120
So yeah it will be quite similar.

14
00:00:52.120 --> 00:00:53.200
Not that hard.

15
00:00:53.200 --> 00:00:56.370
Now that we already wrote the encoded parts.

16
00:00:56.500 --> 00:00:59.470
So let's get into it right now.

17
00:00:59.470 --> 00:01:08.680
So same thing as for the encoding we start by creating the sub layer which we we call the code layer

18
00:01:10.450 --> 00:01:24.420
which inherits from layers that player and of course defining the in its function using self again.

19
00:01:24.610 --> 00:01:32.640
The number of units for the feet forward but the number of projections for the Met head.

20
00:01:32.650 --> 00:01:39.480
Attention sub layer and the dual part the drop out rates so let's call super.

21
00:01:40.600 --> 00:01:52.210
As usual you call the layer self that's in it's who we are.

22
00:01:52.870 --> 00:02:00.880
Let's make sure that we can use the number of units as well as the number of projects since and the

23
00:02:00.880 --> 00:02:04.480
dropout rates as where in all class.

24
00:02:06.220 --> 00:02:15.750
So self-taught no thrush equals an approach with the dots approach not with the dots.

25
00:02:15.780 --> 00:02:25.120
That was right the first time and said That's dropouts equals dropouts.

26
00:02:25.200 --> 00:02:33.910
Now like we did before we will define the core core methods and we will see along the way.

27
00:02:34.020 --> 00:02:40.290
What we will need to define later in the union function or maybe in in the build method.

28
00:02:40.290 --> 00:02:43.330
So let's go through the process.

29
00:02:43.420 --> 00:02:44.800
Def.

30
00:02:45.150 --> 00:02:47.080
Of course we would need self.

31
00:02:47.130 --> 00:02:52.470
We will have some inputs and we will also have the outputs from the encoder.

32
00:02:52.830 --> 00:02:56.790
Let's call them and each outputs two masks.

33
00:02:56.790 --> 00:03:04.260
This time the mask for the first self attention layer and the second one for the attention layer that

34
00:03:04.260 --> 00:03:09.860
uses the outputs of the encoder so mask 2.

35
00:03:10.170 --> 00:03:16.220
And the training boolean just to see if we apply the dropouts on that.

36
00:03:16.220 --> 00:03:24.540
So first phase is in this sub layer is to apply the first attention layer.

37
00:03:24.540 --> 00:03:28.390
So melty at attention.

38
00:03:30.010 --> 00:03:39.910
Number one and this is a set of attentional yes so we apply it to inputs inputs and inputs with the

39
00:03:39.940 --> 00:03:40.930
first mask.

40
00:03:41.980 --> 00:03:55.370
So here we are and we apply as we will many times the first drop outs to attention and we specify if

41
00:03:55.370 --> 00:03:57.090
we are in training phase or not.

42
00:03:57.830 --> 00:04:04.740
And finally this small sum plus normalization.

43
00:04:04.740 --> 00:04:18.400
So no no one to attention plus inputs so this was the first phase of this decoder that we see right

44
00:04:18.400 --> 00:04:19.200
here.

45
00:04:19.420 --> 00:04:22.270
Then we will have to do pretty much the same thing here.

46
00:04:22.840 --> 00:04:31.050
And finally the feed forward but let's declare right now the layers that we used in the build methods

47
00:04:31.890 --> 00:04:43.410
which use self as before the input shape which will be the shape of the valuable inputs right here so

48
00:04:43.440 --> 00:04:55.550
we will need later to get the model so equals input shape of course this is along the last dimension.

49
00:04:55.550 --> 00:05:03.550
So for this first phase we has to use self that melty head's attention.

50
00:05:03.740 --> 00:05:17.900
Number one which is equal to multi head tension and we all we just need to give it the number of projection

51
00:05:17.900 --> 00:05:19.730
that we want to to have.

52
00:05:20.630 --> 00:05:33.860
Then we used to jump out the first one its equals two layer starts drop outs and the rate is exactly

53
00:05:34.720 --> 00:05:35.200
safe.

54
00:05:35.210 --> 00:05:39.790
That's dropouts that we declared in the in its methods.

55
00:05:39.800 --> 00:05:47.630
And finally last part of this First's process is to apply the normalization so let's declare Norm one

56
00:05:47.660 --> 00:05:55.620
equals layers that layer not malign ization.

57
00:05:55.670 --> 00:06:04.390
And again we will apply this Epsilon equals one e minus six.

58
00:06:04.420 --> 00:06:05.020
Here we are.

59
00:06:05.350 --> 00:06:12.730
Let's go to next phase which is the attention layer using the outputs of the encoder.

60
00:06:12.760 --> 00:06:15.720
So let's create our attention.

61
00:06:15.720 --> 00:06:17.560
Number two equals self.

62
00:06:17.570 --> 00:06:22.690
That's multi heads attention to.

63
00:06:23.530 --> 00:06:26.440
And we apply to attention

64
00:06:29.320 --> 00:06:35.170
the outputs of the the outputs of the encoder

65
00:06:37.710 --> 00:06:47.800
and the second mask so in this motel attention layer we will have a look at the paper to see how this

66
00:06:47.800 --> 00:06:55.250
is explained in order to see what will be the values the keys and the queries.

67
00:06:55.270 --> 00:07:02.260
So they expanded right here in the encoder decoder attention layers which is the one we are focusing

68
00:07:02.260 --> 00:07:03.160
on right now.

69
00:07:03.190 --> 00:07:09.370
The queries come from the previous decoder layer and the memory keys and values come from the outputs

70
00:07:09.430 --> 00:07:11.040
of the encoder.

71
00:07:11.050 --> 00:07:18.910
So this is why he we say that the queries for the multi had attention sub layer come from the attention

72
00:07:18.910 --> 00:07:25.060
that we just computed right here and the keys and the values just come from the outputs of the encoder.

73
00:07:25.060 --> 00:07:29.080
So this is just what is explained in the paper.

74
00:07:29.130 --> 00:07:37.890
Now let's finish this process by applying as before a little bit of drop outs.

75
00:07:37.890 --> 00:07:39.630
So if that's dropouts.

76
00:07:39.800 --> 00:07:51.010
Number two applied to in to saying if we are training on that's and attention number two equals self

77
00:07:51.030 --> 00:08:00.990
that's known to applied to attention to plus attention.

78
00:08:00.990 --> 00:08:01.650
Here we are.

79
00:08:02.190 --> 00:08:06.040
The second part of the decoder has been written.

80
00:08:06.040 --> 00:08:10.910
So we just need to declare all the layers that we that we used there.

81
00:08:11.880 --> 00:08:15.650
So actually this is really similar to the first phase.

82
00:08:15.650 --> 00:08:21.090
So let's just be lazy once again don't do this at home.

83
00:08:21.090 --> 00:08:21.920
This is quite dangerous.

84
00:08:21.930 --> 00:08:28.140
If you have a small mistake a typo or whatever you will copy it and sometimes it can lead to days of

85
00:08:28.140 --> 00:08:33.230
debugging but I'm a professional and actually I'm kind of cheating because I already know what the code

86
00:08:33.240 --> 00:08:34.770
of course.

87
00:08:34.770 --> 00:08:43.950
But yeah I allowed myself to be a little bit lazy right here and just change the ones with two and that

88
00:08:43.950 --> 00:08:46.520
should do the job.

89
00:08:46.610 --> 00:08:51.130
Now let's write the last part which is as for the encoder layer.

90
00:08:51.220 --> 00:08:53.110
A simple feed forward sequence.

91
00:08:53.120 --> 00:08:57.410
So let's say that outputs will be equal to itself.

92
00:08:57.440 --> 00:08:59.810
That's tense.

93
00:09:00.140 --> 00:09:14.250
One applied to attention to outputs equal to self dance to the plate to itself.

94
00:09:14.420 --> 00:09:20.170
Then let's apply the last drop outs dropouts.

95
00:09:20.180 --> 00:09:23.610
So that's the third outputs.

96
00:09:23.930 --> 00:09:31.970
Training and finally self that's known for the third.

97
00:09:32.330 --> 00:09:40.810
The third time outputs plus attention to and here we are.

98
00:09:41.470 --> 00:09:44.760
Let's once again declare what we use right there.

99
00:09:44.830 --> 00:09:55.180
So self that's tense one equals layers that dance the number of units has been declared before.

100
00:09:55.210 --> 00:10:10.320
So self that's f f and it's of course it's units equals f f and units and we use the activation function

101
00:10:12.310 --> 00:10:17.450
being revalued that's for the first one self.

102
00:10:17.480 --> 00:10:25.360
That tends to will again be a dense dense layer.

103
00:10:25.360 --> 00:10:27.220
Units equals.

104
00:10:27.220 --> 00:10:32.860
We just go back to the dimension of our model.

105
00:10:32.860 --> 00:10:35.700
Then we applied the drop outs.

106
00:10:35.710 --> 00:10:37.850
The third one equals layers.

107
00:10:37.880 --> 00:10:41.650
That's dropouts and the rate is self.

108
00:10:41.650 --> 00:10:43.900
That's dropouts.

109
00:10:44.410 --> 00:10:57.490
And the last layer that we use normalization layer third one equals layers that layer normalized station

110
00:10:58.420 --> 00:11:08.410
effects same Epsilon course one e minus six.

111
00:11:08.430 --> 00:11:16.620
Here we are making making the code look it's clean up and well I think the last thing we have to do

112
00:11:16.620 --> 00:11:21.150
with this sub layer is just to return the results.

113
00:11:21.160 --> 00:11:25.090
So return outputs perfect.

114
00:11:25.190 --> 00:11:31.170
We have the decoder layer that we needed in order to build the whole decoder.

115
00:11:32.220 --> 00:11:33.120
Let's start right now.

116
00:11:33.120 --> 00:11:51.430
So let's declare the class code that inherits from layers that layer define the unit function self then

117
00:11:51.460 --> 00:11:58.750
the layers number of units for the feet for its parts.

118
00:11:59.070 --> 00:12:09.270
The number of projections the dropouts rates the size of the vocabulary.

119
00:12:09.280 --> 00:12:15.970
This is pretty much similar to what we did for the encoder.

120
00:12:16.180 --> 00:12:25.600
Again the dimension of the model and let's again be crazy and given name to this whole layer deep code.

121
00:12:27.640 --> 00:12:36.230
OK let's call super decode the self that's in it.

122
00:12:36.260 --> 00:12:40.700
And as we use a name we have to give it to this method.

123
00:12:41.470 --> 00:12:49.990
And before creating any valuable let's try to write the the code methods and to see what we will need

124
00:12:49.990 --> 00:12:50.420
later.

125
00:12:51.010 --> 00:12:54.660
So self it will get of course inputs for the decoder.

126
00:12:54.820 --> 00:13:00.430
It will get the outputs of the encoder so and the outputs.

127
00:13:00.430 --> 00:13:10.560
We will use two masks that we will receive from the whole transformer code and get training to see if

128
00:13:10.560 --> 00:13:12.730
we need to apply the Joe part on that.

129
00:13:12.990 --> 00:13:15.840
So quick look to be sure.

130
00:13:15.950 --> 00:13:20.160
The architecture right here.

131
00:13:20.220 --> 00:13:28.290
First we need to make an embedding then to add the positional encoding and finally to apply our decoder

132
00:13:28.320 --> 00:13:32.130
sub layer and times and being number layers.

133
00:13:32.850 --> 00:13:34.110
So let's do that right now.

134
00:13:34.110 --> 00:13:37.110
Let's start with outputs equals self.

135
00:13:37.110 --> 00:13:41.820
That's embedding applied to inputs.

136
00:13:41.820 --> 00:13:46.050
Then as before we do a little bit of normalization.

137
00:13:46.170 --> 00:13:51.210
Multiplying that by T F that math that s cute.

138
00:13:51.300 --> 00:14:02.720
Square root of 2 you have that casts because we wants the D model which is an integer to be floats in

139
00:14:02.720 --> 00:14:09.270
order to apply the square root to its we see that we use the D model.

140
00:14:09.280 --> 00:14:17.500
So let's just define it right here so we don't forget later set a D model equals t model.

141
00:14:17.520 --> 00:14:18.060
Here we are.

142
00:14:18.510 --> 00:14:22.920
After that we saw that we have to apply a positional encoding layer.

143
00:14:22.980 --> 00:14:26.910
So pose encoding applied to outputs.

144
00:14:27.300 --> 00:14:39.850
And before we get to the sub layers let's apply the base of the dropouts drop drop outs to outputs with

145
00:14:39.850 --> 00:14:40.940
the training permits.

146
00:14:42.110 --> 00:14:42.480
Okay.

147
00:14:42.550 --> 00:14:47.250
Right before we apply decode those sub layers let's define what we used.

148
00:14:47.250 --> 00:14:47.650
Right there.

149
00:14:47.680 --> 00:14:53.440
So self that's embedding equals layers.

150
00:14:53.440 --> 00:15:01.000
That's embedding and we give it the vocab size that we get as parameter.

151
00:15:01.000 --> 00:15:04.340
Same thing with the dimension of the model.

152
00:15:04.360 --> 00:15:11.600
Then we used a positional encoding layer let's define it right here.

153
00:15:12.100 --> 00:15:19.550
Should Know and cutting we don't in any perimeter for this one.

154
00:15:19.550 --> 00:15:38.910
And we applied drop outs equals there's that's dropouts and rates is the viable dropouts drop outs that's

155
00:15:38.910 --> 00:15:39.190
it.

156
00:15:39.630 --> 00:15:47.160
So now just as we did for the encoder we are ready to apply multiple decoders of layers and the way

157
00:15:47.160 --> 00:15:50.220
we will do this again is with the list of layers.

158
00:15:50.220 --> 00:15:53.750
So for I in range.

159
00:15:54.120 --> 00:15:57.450
Self that's number layers

160
00:16:00.550 --> 00:16:02.530
outputs equals self.

161
00:16:02.530 --> 00:16:08.290
That's because the layers I

162
00:16:11.040 --> 00:16:15.000
outputs applied to outputs.

163
00:16:15.000 --> 00:16:23.850
We also receive the outputs of the encoder so and coordinate outputs we give the two masks.

164
00:16:23.850 --> 00:16:28.130
That's we've been given.

165
00:16:28.230 --> 00:16:33.480
And finally of course the training parameter.

166
00:16:33.640 --> 00:16:34.240
Here we are.

167
00:16:34.300 --> 00:16:38.570
So we applied the decoder sub layer as many times as we need.

168
00:16:38.590 --> 00:16:44.190
So we have to create them First's self.

169
00:16:44.260 --> 00:16:45.710
That's.

170
00:16:46.360 --> 00:16:59.560
Layers equals let's create lists of those layers decoder layer we give it's the number of units for

171
00:16:59.560 --> 00:17:13.080
the feet forward thoughts f f and units the number of pros and the drop out rates and this is for we

172
00:17:13.080 --> 00:17:23.960
don't care in branch nonetheless and finally as we have seen right here we will use in a number of layers

173
00:17:24.260 --> 00:17:26.070
outside of the input method.

174
00:17:26.070 --> 00:17:28.460
So we need to create self.

175
00:17:28.460 --> 00:17:35.240
That's number layers equals number layers.

176
00:17:35.240 --> 00:17:36.340
Here we are.

177
00:17:36.350 --> 00:17:38.870
Thing we all done with the decoder.

178
00:17:38.870 --> 00:17:44.890
Of course we need this final thing which is to return the outputs.

179
00:17:44.940 --> 00:17:49.700
That sets all the code is really to be used to build a transformer.

180
00:17:49.700 --> 00:17:51.320
All the huge bricks have been built.

181
00:17:51.530 --> 00:17:59.630
We just have to put them together to add a little bit of a dense layer at the end and I guess we will

182
00:17:59.630 --> 00:18:02.240
have a fully performing transformer.

183
00:18:02.240 --> 00:18:06.280
Really excited to finish this mother building and hope you are too.

184
00:18:06.290 --> 00:18:07.010
So it's using.
