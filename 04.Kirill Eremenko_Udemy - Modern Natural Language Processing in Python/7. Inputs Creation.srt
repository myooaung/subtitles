1
00:00:00,270 --> 00:00:02,430
Hello and welcome back to this tutorial.

2
00:00:02,430 --> 00:00:03,120
Here we are.

3
00:00:03,120 --> 00:00:05,540
Final step of all I propose processing.

4
00:00:05,640 --> 00:00:08,680
We are almost ready to start building our translator.

5
00:00:08,700 --> 00:00:10,620
We just have to do two final things.

6
00:00:10,620 --> 00:00:16,440
First one is to pad all inputs and outputs which means that we want each sentence to the same length.

7
00:00:16,470 --> 00:00:22,020
And the second one is just to create the data set to shuffle back and do all the things that are necessary

8
00:00:22,020 --> 00:00:25,200
before using them as training sets.

9
00:00:25,200 --> 00:00:25,950
So let's go.

10
00:00:25,950 --> 00:00:31,480
We start by bypassing the inputs using TFA to cure us.

11
00:00:31,520 --> 00:00:40,910
That's true processing that sequence that part sequences.

12
00:00:40,990 --> 00:00:48,610
We want to put the inputs we want to use the value zero which is a value that is not used by the two

13
00:00:48,610 --> 00:00:50,490
can either encoding.

14
00:00:50,530 --> 00:00:54,770
So we are sure that it doesn't really corresponds to a word.

15
00:00:54,790 --> 00:01:01,000
And anyway you will see later that the algorithm will not have access to those values because we will

16
00:01:01,000 --> 00:01:07,410
use a mask that doesn't allow the attention mechanism to look at those spending values.

17
00:01:07,430 --> 00:01:16,520
Now we just tell how we wanted to pad each sentence and the way we want to do this is costs which means

18
00:01:16,520 --> 00:01:20,750
that we want to add the zero at the end of each sentence.

19
00:01:20,780 --> 00:01:28,780
And finally Max Len will tell how long our sentences will be which is exactly what we used before.

20
00:01:28,930 --> 00:01:32,070
Max van here we are.

21
00:01:32,610 --> 00:01:35,270
Small spaces to make it clean.

22
00:01:35,310 --> 00:01:40,500
And now that our inputs our is let's do the same thing with the outputs

23
00:01:43,900 --> 00:01:48,280
outputs small white spaces to make it cleaner.

24
00:01:48,820 --> 00:01:55,840
And of course this is the outputs that we want but so that's for the putting step.

25
00:01:56,140 --> 00:01:59,610
And now final step let's create 0 data sets.

26
00:02:01,100 --> 00:02:07,180
We will first decide of the budget size that we want to use.

27
00:02:07,390 --> 00:02:14,890
Let's say 64 again this is something you can play with when you try to improve your translator emphasize

28
00:02:14,920 --> 00:02:18,070
these would be Father shuffling of the data sets.

29
00:02:18,070 --> 00:02:28,960
Let's set it to 20 thousands and this time to create our data sets data sets equals T F dots data that

30
00:02:29,680 --> 00:02:31,200
data sets.

31
00:02:31,200 --> 00:02:34,320
Woops that's that from.

32
00:02:34,330 --> 00:02:44,340
That's from 10 so slices and the slice we use is simply inputs outputs.

33
00:02:44,380 --> 00:02:45,050
Here we are.

34
00:02:45,160 --> 00:02:46,410
We have a data sets.

35
00:02:46,450 --> 00:02:53,110
Now in order to optimize it and to make it fully ready to be used by our training phase we will first

36
00:02:54,280 --> 00:03:00,660
use these small two which is called data sets that cash.

37
00:03:00,740 --> 00:03:05,650
This is just something that improves the way the data sets is stored and the way we can have access

38
00:03:05,650 --> 00:03:07,160
to the data during the training.

39
00:03:07,210 --> 00:03:12,520
These just increases the speed of the training but has nothing to do with the performance by itself

40
00:03:12,620 --> 00:03:14,160
by the accuracy of the model.

41
00:03:14,170 --> 00:03:19,370
So even if you don't use it it will have no impact on the performances.

42
00:03:19,370 --> 00:03:28,730
Now let's just shuffle and batch these data sets so Let's shuffle with the buffer size we created before

43
00:03:29,300 --> 00:03:37,500
and batch everything with this batch size and finally one last little thing that we can add to make

44
00:03:37,500 --> 00:03:49,070
it even faster is these two data sets that fetch and stand out a way to use it is to use t after data

45
00:03:49,140 --> 00:03:54,060
the experimental that's the tune.

46
00:03:54,080 --> 00:04:00,860
So again this is the same thing as the cash two lines before it will just improve the speed of the training

47
00:04:00,890 --> 00:04:04,610
because we can access to the data faster but that's all it does.

48
00:04:04,910 --> 00:04:05,600
So here we are.

49
00:04:05,690 --> 00:04:11,150
Now finally people says the data and have a data set that is ready to be used for training phase.

50
00:04:11,150 --> 00:04:15,500
So now let's get to the fun parts and start building all our translator popularity.

51
00:04:15,500 --> 00:04:15,980
See you soon.
