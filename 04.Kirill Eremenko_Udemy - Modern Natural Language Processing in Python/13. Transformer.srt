1
00:00:00,180 --> 00:00:01,260
Hi guys and welcome back.

2
00:00:01,260 --> 00:00:07,970
This now because we are finally ready to build a transformer now that we have all big bricks.

3
00:00:07,980 --> 00:00:10,830
That's the encoder and the decoder.

4
00:00:10,860 --> 00:00:14,160
We are ready to finalize this model before that.

5
00:00:14,160 --> 00:00:17,020
Let's have a quick last look at the architecture.

6
00:00:17,410 --> 00:00:20,370
So this is the whole transformer.

7
00:00:20,370 --> 00:00:25,490
We have already made these parts which is the encoder and this box which is the decoder.

8
00:00:25,500 --> 00:00:29,640
So what we just have to do you can divide it in three phases.

9
00:00:29,640 --> 00:00:34,100
The first one is just to assemble those two elements.

10
00:00:34,110 --> 00:00:42,230
We also have to make this last phase which is basically to apply in our function at the end of our decoder.

11
00:00:42,240 --> 00:00:48,850
And last thing that we have not done yet is to implement the masks that we apply inside the Motorhead

12
00:00:48,900 --> 00:00:53,030
attention so we will have to create two masking functions.

13
00:00:53,040 --> 00:00:59,550
The first one to mask the padding tokens at the end of the sentences and the second one is to create

14
00:00:59,550 --> 00:01:07,110
the head mask which makes sure that's the decoder can have access to future words.

15
00:01:07,230 --> 00:01:08,480
So let's start right now.

16
00:01:09,860 --> 00:01:21,230
By defining our class transformer and this time as it a it will be a model and not a layer so it will

17
00:01:21,230 --> 00:01:25,160
inherits from T F that cure us.

18
00:01:25,160 --> 00:01:29,240
That's model but it works the same way.

19
00:01:29,240 --> 00:01:36,920
We still have to define the innards function so let's make a list of all the valuables that we will

20
00:01:36,920 --> 00:01:37,660
need.

21
00:01:37,670 --> 00:01:43,310
First we will have to make to embed things the embedding for the encoder and for the decoder.

22
00:01:43,320 --> 00:01:55,100
So for that we need the vocab size of the encoder the vocab size of the decoder.

23
00:01:55,340 --> 00:02:01,810
We also need the dimension of the model we wants to have at the end of the embedding.

24
00:02:02,150 --> 00:02:08,960
We need the number of layers which is basically the number of time we will apply the encoder layer and

25
00:02:08,960 --> 00:02:13,300
the decoder layer inside of the huge encoder and decoder bricks.

26
00:02:13,310 --> 00:02:20,870
So in the paper they say six we might want to just 4 for the tests to make it a bit lower but we'll

27
00:02:20,870 --> 00:02:22,550
see later in the training.

28
00:02:22,560 --> 00:02:31,580
So we also need the number of units in the in the FIT forwards box at the end of each encoder and decoder

29
00:02:31,580 --> 00:02:33,890
layer next.

30
00:02:33,900 --> 00:02:40,920
We need the number of projections insides of the much head attentional layer the drop dropout rates

31
00:02:41,880 --> 00:02:50,400
and let's give a name to our module which will be transformed.

32
00:02:51,210 --> 00:02:51,800
Right there.

33
00:02:53,310 --> 00:03:07,990
So as before we need to call super transformer self called in its methods and tell its the name of our

34
00:03:07,990 --> 00:03:09,320
model.

35
00:03:09,620 --> 00:03:10,020
Okay.

36
00:03:10,510 --> 00:03:18,010
So before starting to create the call methods we will first build all all masking functions.

37
00:03:18,010 --> 00:03:24,090
So the first one we said that we need a function to create a mask for the pageant tokens.

38
00:03:24,100 --> 00:03:34,030
So let's create this and it will just take a sequence as inputs at these points of the model a sequence

39
00:03:34,030 --> 00:03:39,440
is still just a list of numbers each number corresponding to awards.

40
00:03:39,520 --> 00:03:41,280
The editing has not been done yet.

41
00:03:41,530 --> 00:03:48,270
So what we have is simply the inputs from the data sets and absent the encoding from the token NISA

42
00:03:48,610 --> 00:03:49,050
awards.

43
00:03:49,060 --> 00:03:49,960
It's just a number.

44
00:03:49,960 --> 00:03:54,860
So the sequence right here it's just a list of numbers.

45
00:03:55,090 --> 00:04:09,040
But as it has been botched the shape of SEK is this one batch size sic length OK.

46
00:04:09,440 --> 00:04:19,970
So let's look for the zeros in the sequence which corresponds to the pattern token let's call mask equals

47
00:04:20,330 --> 00:04:37,100
see if that's not that's equal SEK 0 and we want it to be represented as floats so that we can perform

48
00:04:37,520 --> 00:04:44,240
anything we want later see if that casts.

49
00:04:44,320 --> 00:04:47,560
And so this has the same shape as before.

50
00:04:48,070 --> 00:05:00,230
And we want to return mask after adding to New Dimensions new access to efforts.

51
00:05:00,700 --> 00:05:01,960
Access.

52
00:05:02,200 --> 00:05:04,060
And we keep the mask.

53
00:05:04,540 --> 00:05:10,970
So in order to understand why we do this we have to go back to where we apply this mask.

54
00:05:11,850 --> 00:05:17,080
If you remember well attention computation we apply the mask.

55
00:05:18,060 --> 00:05:23,820
In these nerds had attention sub layer and more precisely in the sketch that put it attention.

56
00:05:23,910 --> 00:05:29,930
So first thing to remember is that before applying this get the attention right here.

57
00:05:30,000 --> 00:05:36,780
We have splits the dimension of the embedding which means that we have an additional dimension.

58
00:05:36,780 --> 00:05:39,750
This is what we just wrote.

59
00:05:39,750 --> 00:05:40,310
Why here.

60
00:05:40,320 --> 00:05:41,490
The number of projection.

61
00:05:41,700 --> 00:05:44,940
So this is the reason why we added this new axis.

62
00:05:44,970 --> 00:05:48,780
This is because the mask will be applied for each of the projection.

63
00:05:48,790 --> 00:05:49,100
OK.

64
00:05:49,740 --> 00:05:58,010
And the reason why we added this one is because where we apply the mask is just after the scales project.

65
00:05:58,020 --> 00:06:00,720
So it's inside these parenthesis.

66
00:06:00,720 --> 00:06:09,660
So basically this is a matrix of shape sequence length sequence length and the clever thing is that's

67
00:06:10,290 --> 00:06:15,000
when we do this even if the mask is not a matrix that fits the shape.

68
00:06:15,240 --> 00:06:20,970
If it's just a vector instead of being a matrix it will cleverly repeat the mask in the dimension that

69
00:06:20,970 --> 00:06:21,960
is missing.

70
00:06:21,960 --> 00:06:26,930
So basically what we sent here.

71
00:06:26,930 --> 00:06:29,630
Because this is in kind of an empty dimension.

72
00:06:29,630 --> 00:06:31,460
This is just a row.

73
00:06:31,670 --> 00:06:46,870
And thanks to this empty dimension the row will be repeated as many times as we need.

74
00:06:46,880 --> 00:06:56,320
So now let's explain why it does what we want so to do that I have this little figure that explains

75
00:06:56,320 --> 00:07:04,360
how much exposure application works in our case this metric right here is the attention waits.

76
00:07:04,360 --> 00:07:09,750
So this is what we get after multiplying the queries by the keys.

77
00:07:10,000 --> 00:07:17,410
And basically as a reminder what it means is that it just has all the information about how let's say

78
00:07:17,590 --> 00:07:21,960
words e is related to words J.

79
00:07:22,090 --> 00:07:27,190
And he this second matrix just represents a sentence.

80
00:07:27,190 --> 00:07:30,660
Actually it's not really a sentence because at the very beginning.

81
00:07:30,700 --> 00:07:31,420
Yes.

82
00:07:31,420 --> 00:07:38,400
Each row corresponds to a words but after a lot of processing inside the model it's not the case anymore.

83
00:07:38,410 --> 00:07:43,980
Each row represents the global information about the whole sentence we had at the beginning.

84
00:07:44,260 --> 00:07:52,570
But anyway what is important is that here this metric says how the row number I in the results must

85
00:07:52,570 --> 00:07:55,200
be related to the row.

86
00:07:55,220 --> 00:08:05,050
No change in the values and what we created with all padding masks is that we just said that in one

87
00:08:05,050 --> 00:08:10,350
row the last let's say and numbers will be equal to zero.

88
00:08:10,680 --> 00:08:13,780
And being the number of putting tokens at the end.

89
00:08:13,780 --> 00:08:17,810
Of course and this will be repeated in each row.

90
00:08:17,810 --> 00:08:25,670
What that means is that if the input sentence hides and putting tokens at the end this matrix right

91
00:08:25,670 --> 00:08:26,000
here.

92
00:08:26,060 --> 00:08:32,870
So this attention weights will have zeros everywhere in the lists and columns which means if we have

93
00:08:32,870 --> 00:08:39,740
a look at how the matrix multiplication works if we want to see how this number at the end of the computation

94
00:08:40,190 --> 00:08:46,970
is made it's just a combination of multiplications of those numbers of those pairs and numbers and we

95
00:08:46,970 --> 00:08:51,640
some of them but the last n numbers here being equal to zero.

96
00:08:51,710 --> 00:08:54,070
Those lists and numbers will not be used.

97
00:08:54,260 --> 00:08:57,210
And this is the case everywhere in this results.

98
00:08:57,260 --> 00:09:02,540
So that just means that the lists and rows right here are not used.

99
00:09:02,600 --> 00:09:05,260
And that's actually exactly what we wanted to perform.

100
00:09:05,270 --> 00:09:12,420
So this siding mask kind of disables those lists and rows and that's exactly what we wanted to do.

101
00:09:12,710 --> 00:09:16,160
So that's for the putting mask.

102
00:09:16,640 --> 00:09:29,690
Let's go to the second one def create look ahead mask which takes against self and sequence what we

103
00:09:29,690 --> 00:09:32,450
want to do now is not create just a vector.

104
00:09:32,450 --> 00:09:34,470
We want to create a full matrix.

105
00:09:34,670 --> 00:09:41,060
And the goal of this matrix is to say that we can't have access to future words and the best way to

106
00:09:41,060 --> 00:09:44,600
do that is to use triangular matrix.

107
00:09:44,600 --> 00:09:48,220
So for those of you who are not familiar with this concept it's really simple.

108
00:09:48,320 --> 00:09:55,790
If you take a square matrix a triangle matrix means that's only let's say the half top right of the

109
00:09:55,790 --> 00:09:56,360
matrix.

110
00:09:56,360 --> 00:10:01,230
So this is in a triangle shape will not have zeros.

111
00:10:01,550 --> 00:10:07,490
And the best way to do that is to use the little bend parts methods intensive low.

112
00:10:07,910 --> 00:10:11,120
So we will have a look at this in a few seconds.

113
00:10:11,120 --> 00:10:17,300
First we need to declare sequence length equals t f the shape of our sequence.

114
00:10:17,300 --> 00:10:27,940
So we will have the the shape of our matrix and what we do is look the head mask equals t efforts Lee

115
00:10:27,980 --> 00:10:41,320
now that's bend thoughts to f that once here we just created a match we landed with once sec line minus

116
00:10:41,320 --> 00:10:42,730
1 0.

117
00:10:43,060 --> 00:10:43,810
So let me explain

118
00:10:47,410 --> 00:10:55,090
the size of sequence length and what we want to do now is to add zeros in the top right parts of this

119
00:10:55,090 --> 00:10:55,820
matrix.

120
00:10:55,930 --> 00:10:56,980
And what.

121
00:10:57,460 --> 00:11:04,360
BEN POTTS does is that it adds condition on the indices of all metrics and put zeros.

122
00:11:04,360 --> 00:11:12,400
If those conditions are completed and basically this first part says that's in the row I and the column

123
00:11:12,730 --> 00:11:13,000
J.

124
00:11:13,000 --> 00:11:19,480
In all metrics this adds a condition about how I is supposed to be higher than J and these does the

125
00:11:19,480 --> 00:11:19,960
opposite.

126
00:11:19,960 --> 00:11:27,400
This adds a condition about how Shay has to be higher than I so I can hear a negative number say is

127
00:11:27,400 --> 00:11:29,140
that we don't want to use this one.

128
00:11:29,140 --> 00:11:34,850
And here it just says that we want to change to be greater or equal to I.

129
00:11:34,960 --> 00:11:38,840
Let me just show you what it does.

130
00:11:40,790 --> 00:11:53,390
If we apply this to let's say the metrics of size 5 will actually prints buffer for them so it's easier

131
00:11:53,390 --> 00:11:54,260
to see what happens.

132
00:11:56,720 --> 00:11:58,040
Let's print this one

133
00:12:00,750 --> 00:12:02,160
and that one too.

134
00:12:03,840 --> 00:12:09,280
So here we just created a square matrix field with once and here.

135
00:12:09,300 --> 00:12:16,020
Thanks to these methods we added a condition about how a J should be higher than I in order to apply

136
00:12:16,050 --> 00:12:23,580
0 0 0 if you want to see the other way we added the condition about how I should be higher than J.

137
00:12:23,590 --> 00:12:30,530
In order to keep the original values so we have this bottom left triangular matrix.

138
00:12:30,750 --> 00:12:36,650
But what we want to do actually is to have a strict upper right triangular matrix.

139
00:12:36,660 --> 00:12:39,500
This is how we will say that the words.

140
00:12:39,640 --> 00:12:43,000
No I will not have access to the word no J.

141
00:12:43,230 --> 00:12:46,410
We want to mask the words that will be in the future.

142
00:12:46,890 --> 00:12:55,460
So we just have to add one minus this whole matrix and we get exactly what we want.

143
00:12:55,470 --> 00:13:02,820
So again we can afford to keep those dimensions because we will mix the Look AHEAD mask with the padding

144
00:13:02,820 --> 00:13:09,900
mask and so tensor flow will cleverly know that it needs to add those two dimensions to the look ahead

145
00:13:09,900 --> 00:13:17,320
mask so we can simply return look ahead mask.

146
00:13:17,340 --> 00:13:26,340
And so just to illustrate what are said right now we will actually use those functions to see how it

147
00:13:26,340 --> 00:13:28,120
works.

148
00:13:28,290 --> 00:13:29,840
2 3 4.

149
00:13:30,510 --> 00:13:33,740
And of course gets rid of self.

150
00:13:34,230 --> 00:13:35,310
Do the same thing here.

151
00:13:40,380 --> 00:13:42,060
Gets rid of self

152
00:13:44,870 --> 00:13:57,360
remove the tops so let's define those functions and let's define the FH sequence.

153
00:13:57,360 --> 00:14:03,960
Let's say it's equal to T abducts casts and we need to simulate a batch.

154
00:14:04,110 --> 00:14:13,880
So that's why I add this array into another array and it's random number zero.

155
00:14:13,950 --> 00:14:21,990
We don't want to have a look at this one and let's say three zeros at the end and this will be t f that's

156
00:14:22,140 --> 00:14:24,220
int 32.

157
00:14:24,480 --> 00:14:24,830
Okay.

158
00:14:25,140 --> 00:14:32,070
And just to illustrate what I said before when we will apply the self attention mechanism at the beginning

159
00:14:32,070 --> 00:14:38,640
of the decoder we will use the look ahead mask again so that we don't have access to future words but

160
00:14:38,670 --> 00:14:50,970
also the padding masks and the way we will do that is just if we do t f that's maximum of it's copy

161
00:14:50,990 --> 00:14:53,180
busts creates buying mask.

162
00:14:56,280 --> 00:14:56,760
And

163
00:14:59,410 --> 00:15:00,130
creates

164
00:15:03,020 --> 00:15:05,330
look at head masks.

165
00:15:05,970 --> 00:15:08,140
We can see that we get exactly what we want.

166
00:15:08,200 --> 00:15:15,680
Tenth floor understood that it had to create the first and the second axis for the look ahead the mask

167
00:15:15,690 --> 00:15:16,200
also.

168
00:15:16,320 --> 00:15:18,560
So that's why we get the right Sharpie.

169
00:15:18,600 --> 00:15:24,030
This is for the batch size number of projections inside the self attention sub layer.

170
00:15:24,690 --> 00:15:29,370
And finally the size of the mattress for the attention weights.

171
00:15:29,460 --> 00:15:34,430
And if you have a closer look at where we have the one so each one means that we apply a mask.

172
00:15:34,650 --> 00:15:42,300
So that means that we don't get this into accounts we have here this full column of once which corresponds

173
00:15:42,300 --> 00:15:43,250
to the zero.

174
00:15:43,470 --> 00:15:50,790
And of course the last we once and we keep the top right parts of the mattress as wants.

175
00:15:50,880 --> 00:15:53,240
As for the word no.

176
00:15:53,670 --> 00:15:56,280
Let's say one two three four.

177
00:15:56,340 --> 00:15:57,220
We don't have to.

178
00:15:57,240 --> 00:15:58,890
We don't want to have access to all the words.

179
00:15:58,920 --> 00:16:04,070
After that we don't want to have access towards 5 6 and 7.

180
00:16:04,200 --> 00:16:09,720
So that's why we don't have to add the axis for the first and the second dimensions.

181
00:16:09,720 --> 00:16:15,810
This will do it by itself because anyway we will add a padding mass to the look ahead mask so the maximum

182
00:16:15,810 --> 00:16:17,090
will do all the job.

183
00:16:17,880 --> 00:16:18,170
OK.

184
00:16:18,210 --> 00:16:24,810
So that was kind of the tricky parts dealing with the all the masks but now we have done that.

185
00:16:24,810 --> 00:16:34,320
We basically just have to combine the encoder and the decoder in the call method so let's define call

186
00:16:36,150 --> 00:16:37,670
with itself.

187
00:16:37,800 --> 00:16:46,770
Of course we get in the transformer inputs for the encoder inputs for the decoder and information about

188
00:16:46,830 --> 00:16:49,480
whether we are training or not.

189
00:16:49,830 --> 00:16:56,070
Let's create the masks right now so the first one is the mask that we will use in the encoder.

190
00:16:56,100 --> 00:17:05,260
This is a simple putting mask because here we don't need the look ahead mask so self that creates padding

191
00:17:05,260 --> 00:17:05,800
mask

192
00:17:08,530 --> 00:17:18,600
for the encoder inputs then the first mask of the decoder is this time with the LUCA mask because this

193
00:17:18,600 --> 00:17:21,650
is exactly the moment where we don't want to have a look at the future.

194
00:17:21,660 --> 00:17:40,140
So deck mask and then the one equals as we did before maximum of set of dots creates putting mask the

195
00:17:40,390 --> 00:17:53,160
flight to of course the decode the inputs and self that creates Luke had mask applied to the Dakota

196
00:17:53,190 --> 00:17:55,880
inputs as well.

197
00:17:56,130 --> 00:18:08,650
And the last mask that we want is the second mask of the Dakota which is a simple parsing mask and we

198
00:18:08,650 --> 00:18:15,550
don't want our decoder to have access to the purging tokens that come from the encoder.

199
00:18:15,790 --> 00:18:22,430
So here we just add as parameter encoding inputs.

200
00:18:22,450 --> 00:18:28,150
Quick explanation about why here we have the encoder inputs and that the decoder inputs because you

201
00:18:28,150 --> 00:18:30,730
may think that's as we are inside the decoder.

202
00:18:31,420 --> 00:18:35,430
This is the decoder inputs that we want to use here in order to have all putting mask.

203
00:18:35,440 --> 00:18:36,640
But that's not.

204
00:18:36,650 --> 00:18:43,330
And the reason is that let's have a look at actually we'll use both those things so you can see right

205
00:18:43,330 --> 00:18:45,340
here that in this layer.

206
00:18:45,370 --> 00:18:52,270
So this is in this layer that we want to apply or mask the queries come from the previous layer of the

207
00:18:52,270 --> 00:18:52,920
decoder.

208
00:18:53,560 --> 00:19:02,750
But the values and the keys come from the outputs of the encoder and if we have another look right here.

209
00:19:03,400 --> 00:19:09,790
What we do in the attention mechanism is that we compute this using the queries and the keys.

210
00:19:09,790 --> 00:19:12,330
So in the case we are right now.

211
00:19:12,670 --> 00:19:18,970
This is using the previous layer of the decoder and the outputs of the encoder.

212
00:19:19,180 --> 00:19:26,800
So this part says how they are related to each other how it's words of the decoder is related to the

213
00:19:26,800 --> 00:19:34,150
words of the outputs of the encoder and we use this to kind of re compose the values that now are from

214
00:19:34,150 --> 00:19:35,580
the encoder.

215
00:19:35,620 --> 00:19:43,300
So the result of all this operation is a real regiments of the outputs of the encoder following the

216
00:19:43,300 --> 00:19:49,480
way sentences from the decoder are related to the sentences from the decoder.

217
00:19:49,930 --> 00:19:57,370
So in this process in this combination of the outputs of the encoder This is the putting tokens of the

218
00:19:57,370 --> 00:20:04,270
encoder that we don't want to have a look at because this is the output of the encoder that we are mixing

219
00:20:04,270 --> 00:20:09,880
together in order to have the new representation of all information so far.

220
00:20:10,570 --> 00:20:10,890
Yes.

221
00:20:10,900 --> 00:20:17,740
The final results of our Motorhead attentional error will have the same shape the same form as what

222
00:20:17,740 --> 00:20:19,100
we have inside the decoder.

223
00:20:19,180 --> 00:20:26,440
But with the way we achieve that is by making combination of the information from the encoder and of

224
00:20:26,440 --> 00:20:33,520
course the way we perform those combinations is given by the relations between what we have inside the

225
00:20:33,520 --> 00:20:34,890
encoder and the decoder.

226
00:20:35,560 --> 00:20:41,530
That's why for all fighting mask in the second melty head attention layer of the decoder we use the

227
00:20:41,530 --> 00:20:53,450
shape of the input for the encoder now we really simply have to define the outputs of the encoder as

228
00:20:54,680 --> 00:21:05,530
the results of the encoder layer giving its the inputs the mask and the information about the training

229
00:21:07,460 --> 00:21:07,710
now.

230
00:21:07,710 --> 00:21:19,300
Same thing we just apply our decoder layer decoder and we give it the inputs for the decoder.

231
00:21:19,920 --> 00:21:30,830
The outputs from the enclosure outputs the first mask of the decoder.

232
00:21:31,130 --> 00:21:32,610
The second one.

233
00:21:32,960 --> 00:21:36,880
And finally whether we are in a training phase or not.

234
00:21:39,630 --> 00:21:46,940
So let's apply the last phase which is a really simple linear function.

235
00:21:46,960 --> 00:21:57,520
So a dense layer to the outputs and we turn the results and of course we have to define the layers that

236
00:21:57,520 --> 00:21:58,920
we used right there.

237
00:21:58,920 --> 00:22:01,890
So let's do it he said.

238
00:22:01,960 --> 00:22:07,660
That's called a will be and then coda.

239
00:22:08,480 --> 00:22:13,820
And we simply add all the parameters that we need.

240
00:22:14,030 --> 00:22:22,610
The number of layers of the number of times we apply the encoder sub layer the number of units for the

241
00:22:22,610 --> 00:22:31,850
fit forward part number of projections inside the motel attentional layer the drop out rates the size

242
00:22:31,850 --> 00:22:46,840
of the vocabulary for the encoder and D model so that the embedding is done well and same thing for

243
00:22:46,900 --> 00:22:47,820
the decoder.

244
00:22:48,640 --> 00:23:03,460
Let's define decode a number layers number of units and then the projections show about

245
00:23:07,120 --> 00:23:12,840
vocab size decoder and the dimension of the model.

246
00:23:12,950 --> 00:23:15,410
And finally the last thing we used

247
00:23:18,240 --> 00:23:27,500
a little bit of a dense layer Lee now equals layers that's dense.

248
00:23:28,150 --> 00:23:36,350
And of course we wanted to have as outputs something of the size of the vocabulary of the decoder.

249
00:23:36,550 --> 00:23:42,730
And so yeah of course we want the number of units here to be equal to the vocab size of the decoder

250
00:23:42,820 --> 00:23:48,340
because we want to get the probability for each possible words in our vocab to be the next one.

251
00:23:48,490 --> 00:23:50,350
So well here we are.

252
00:23:50,410 --> 00:23:53,200
This is done the whole transformer has been made.

253
00:23:53,200 --> 00:23:58,420
Guess we are ready to trade outs so let's build the training pot right now and see you soon.
