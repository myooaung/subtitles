1
00:00:00,450 --> 00:00:07,050
Hi and welcome back to this insult because so we have seen how we used to address the secrets to secret

2
00:00:07,050 --> 00:00:13,970
task by using our names and how we improved them by adding these attention mechanism that was every

3
00:00:13,980 --> 00:00:20,040
powerful tool but it seems that we might be able to go even further in that direction.

4
00:00:20,190 --> 00:00:29,040
And the reason why we needed to go further and to even more improve it is because the thing about Armenians

5
00:00:29,100 --> 00:00:35,130
is that it's a sequential process so it doesn't have a global behavior concerning the the input sequence.

6
00:00:35,130 --> 00:00:39,020
And so we lose information along the encoding process.

7
00:00:39,160 --> 00:00:45,090
The fact that's the last hidden State of the encoder which is the final output of the encoder has not

8
00:00:45,090 --> 00:00:49,130
seen the beginning of the sentence for a long time could be quite disturbing.

9
00:00:49,140 --> 00:00:55,710
So for very long sentences very long sequences we lose a lot of information and that's a huge weakness

10
00:00:55,710 --> 00:00:57,200
for our audience.

11
00:00:57,210 --> 00:01:01,470
So what the Google research teams have been thinking about is OK.

12
00:01:01,500 --> 00:01:08,610
So the attention mechanism added global behavior to the decoding phase but the audience still has this

13
00:01:08,700 --> 00:01:11,100
weakness about not being global enough.

14
00:01:11,280 --> 00:01:18,780
So what if actually all we need in order to perform well in this sequence to secret process is attention.

15
00:01:18,780 --> 00:01:24,850
And so all we need is attention is actually the name of the paper that Google lists when they introduced

16
00:01:24,850 --> 00:01:25,860
this transformer.

17
00:01:26,010 --> 00:01:31,890
And that just illustrates the fact that that transformer only uses the attention mechanism in order

18
00:01:31,890 --> 00:01:37,520
to encode and decode sequences and comfortably gets read off the ordinance.

19
00:01:37,560 --> 00:01:42,960
So let's have a look at the architect's figure that they provide in this paper.

20
00:01:43,230 --> 00:01:45,770
So there is a lot of stuff going on here.

21
00:01:45,800 --> 00:01:50,270
We will go of course into the details of each step of each layer later.

22
00:01:50,580 --> 00:01:56,910
But let's start by trying to have the global understanding of what's happening here so we can clearly

23
00:01:56,910 --> 00:02:00,040
see as before that we have two main blocks.

24
00:02:00,060 --> 00:02:06,300
The first one on the left will be the encoder and the second one on the right will be the decoder.

25
00:02:06,300 --> 00:02:08,970
There are some ideas that stay the same.

26
00:02:08,970 --> 00:02:16,230
Like for instance the fact that we have inputs for the encoder and after that we get outputs from this

27
00:02:16,320 --> 00:02:16,920
and Coda.

28
00:02:17,010 --> 00:02:21,000
And those outputs will be part of the inputs of decoder.

29
00:02:21,360 --> 00:02:26,610
And from that we will get all awkward sequence so this doesn't change but one of the main things that

30
00:02:26,640 --> 00:02:32,190
all difference is that's our inputs for the encoder will be the whole sentence at once.

31
00:02:32,220 --> 00:02:38,130
So we don't feed the encoder one word at a time we just give it the whole sequence and it will process

32
00:02:38,130 --> 00:02:39,660
the whole sequence at once.

33
00:02:39,660 --> 00:02:45,360
And this is important to keep the global aspect of the processing that we wanted to have.

34
00:02:45,360 --> 00:02:50,120
Second thing that we wanted to notice is the inputs for the decoder.

35
00:02:50,160 --> 00:02:52,470
You can see that's here to say outputs.

36
00:02:52,470 --> 00:02:59,240
It's actually because we use the outputs of our decoder as new inputs for the decoder as we did before.

37
00:02:59,400 --> 00:03:02,240
But once again we do it with the whole sentence.

38
00:03:02,250 --> 00:03:09,720
So let's say for instance that's all transformer has been made to build a chat bots and the inputs sequence

39
00:03:09,720 --> 00:03:12,270
will be how are you doing for instance.

40
00:03:12,270 --> 00:03:18,210
So first time we apply our transformer we will have as inputs for the encoder the whole sentence.

41
00:03:18,210 --> 00:03:19,340
How are you doing.

42
00:03:19,380 --> 00:03:26,820
And we will give here as inputs just a single token that we will call the starts talking and it doesn't

43
00:03:26,820 --> 00:03:27,780
convey any meaning.

44
00:03:27,780 --> 00:03:33,960
It's just to tell the agents found out that this is the beginning of a sentence and giving that's after

45
00:03:33,970 --> 00:03:41,250
the first application of the transformer will get us outputs just one element that will be I.

46
00:03:41,270 --> 00:03:47,810
So the first word of our answer and we will add this word eye to the output sequence that we had before.

47
00:03:48,120 --> 00:03:51,020
And this will become start talking.

48
00:03:51,030 --> 00:03:56,640
I then will repeat the process keeping the same inputs here so how are you doing.

49
00:03:56,700 --> 00:04:01,520
And key we will use as inputs father decoder start to can I.

50
00:04:01,890 --> 00:04:04,050
And we will get our outputs.

51
00:04:04,050 --> 00:04:09,960
I am for instance and repeating this we will add new words to the output of the decoder to finally get

52
00:04:09,960 --> 00:04:12,570
something like I am fine thank you.

53
00:04:12,570 --> 00:04:15,120
This is where this shift to the right comes from.

54
00:04:15,120 --> 00:04:20,700
It's just that by adding this taut token at the beginning of the input sequence for the decoder you

55
00:04:20,700 --> 00:04:22,540
shift the whole sentence right.

56
00:04:22,620 --> 00:04:29,250
And with that we know that the last word from the output sequence he will be the predicted next word

57
00:04:29,370 --> 00:04:30,750
for our sequence.

58
00:04:30,750 --> 00:04:34,740
But that's an implementation detail and we'll also see that later.

59
00:04:35,040 --> 00:04:42,450
So that was the first thing to have in mind the fact that we use whole sequences at once as inputs and

60
00:04:42,540 --> 00:04:43,380
that's key.

61
00:04:43,380 --> 00:04:50,280
The sequence we use as inputs for all decoder is the one that we already had at the previous iteration

62
00:04:50,850 --> 00:04:55,480
plus the new words that we will predict at the end of this sequence.

63
00:04:55,500 --> 00:04:55,710
Right.

64
00:04:55,710 --> 00:05:02,320
He said something that is really important when we have a look to the architecture are those orange

65
00:05:02,380 --> 00:05:05,510
cells right there where you can see attention everywhere.

66
00:05:05,590 --> 00:05:09,790
And that's the most important thing about these transformer.

67
00:05:09,820 --> 00:05:12,610
That's actually what we'll do the whole job.

68
00:05:12,940 --> 00:05:19,250
And that is exactly based on the attention mechanism that we so earlier applies to our events.

69
00:05:19,270 --> 00:05:26,860
So just to summarize how it works the idea is when you have two sequences it's just we compose the first

70
00:05:26,860 --> 00:05:33,610
sentence according to how each elements of this first sentence is related to the elements of the second

71
00:05:33,610 --> 00:05:40,350
sentence but you might be wondering then why here as inputs we use three times the same sentence.

72
00:05:40,450 --> 00:05:45,820
And that's actually what they call self attention which is the key of the encoder and the transformer.

73
00:05:46,090 --> 00:05:51,360
So what we did before with the ordinance is that we used a standard new network.

74
00:05:51,430 --> 00:05:54,390
We had information about the beginning of the sentence.

75
00:05:54,400 --> 00:05:59,650
We are the new words we make some computation and we get new information about the sentence.

76
00:05:59,740 --> 00:06:06,340
But what we do here is that we have a whole sentence and we will see how each words of its sentence

77
00:06:06,520 --> 00:06:13,150
is related to the other words of the same sentence and then we will read compose this sentence according

78
00:06:13,150 --> 00:06:13,510
to that.

79
00:06:13,510 --> 00:06:20,210
So so here is for instance an illustration of the self attention mechanism.

80
00:06:20,290 --> 00:06:25,310
So if we take the first sentence the animal didn't cross the streets because it was too times.

81
00:06:25,480 --> 00:06:31,480
When we apply the self attentional mechanism it just computes how each words of the sentence is related

82
00:06:31,480 --> 00:06:32,110
to the others.

83
00:06:32,110 --> 00:06:38,500
So here it understands that that it's right here refers to the animal it's the animal that was too tired.

84
00:06:38,680 --> 00:06:43,370
But if we take another example like the animal didn't cross the streets because it was too wide.

85
00:06:43,570 --> 00:06:48,180
We understand that it refers to the streets because it was the street that was too wide of course but

86
00:06:48,180 --> 00:06:51,150
the animal and the self attention mechanism.

87
00:06:51,210 --> 00:06:59,430
Understand that's so if we feed this sentence to a self attentional layer like we have in the encoder

88
00:06:59,800 --> 00:07:08,170
we will have as inputs just this sequence of words is words being of course doing the implementation

89
00:07:08,330 --> 00:07:17,300
of vector and as the outputs of these attentional layer we will have Q Instead of the words it's we

90
00:07:17,320 --> 00:07:24,360
would have a combination of those words according to how strong the relation with the words is.

91
00:07:24,370 --> 00:07:33,010
So we will have a lot of the vector animal a little bits off of streets of the maybe it's these points

92
00:07:33,130 --> 00:07:35,270
and we will have nothing from the other woods.

93
00:07:35,350 --> 00:07:43,570
And for this sentence instead of it as outputs of all self attentional layer we will have lots of the

94
00:07:43,570 --> 00:07:49,960
vectors streets and a little bits of those lighter blue ones and none of those ones.

95
00:07:49,990 --> 00:07:58,660
So it's just that instead of having just a sequence of words we have a new representation of the information

96
00:07:58,660 --> 00:08:06,760
that can convey a sentence because as inputs we have the lowest possible way to have information about

97
00:08:07,000 --> 00:08:13,300
a sentence which is just one words after the other and the goal of applying these self attention mechanism

98
00:08:13,600 --> 00:08:20,500
is to combine everything so that each element doesn't only represent the words but it represents relations

99
00:08:20,530 --> 00:08:22,920
which in the differing words of a sentence.

100
00:08:22,930 --> 00:08:29,830
So that was kind of an illustration of how self attention works and these self attention mechanism is

101
00:08:29,890 --> 00:08:33,350
how we will encode sentences in the transformer.

102
00:08:33,490 --> 00:08:38,770
So that is used of course in the quarter and that's the main thing that we use.

103
00:08:38,800 --> 00:08:41,980
We also use fully connected network which is to learn more.

104
00:08:41,980 --> 00:08:49,870
We apply this whole thing seven times and we get all final and coded version of the input sequence and

105
00:08:50,350 --> 00:08:53,020
about the decoder we start doing the same thing.

106
00:08:53,020 --> 00:09:00,520
So with a sequence as input here we apply a self attentional layer just to have more information about

107
00:09:00,550 --> 00:09:06,430
the relations between the words and after that we apply an attention mechanism between the outputs of

108
00:09:06,430 --> 00:09:10,420
the encoder and the sequence that we just had.

109
00:09:10,420 --> 00:09:19,330
So that means mixing information from the end closer according to how each element is related to the

110
00:09:19,330 --> 00:09:25,780
sequence from the decoder but we'll see that's into more details later when we will go deeper into the

111
00:09:25,780 --> 00:09:27,230
attention mechanism.

112
00:09:27,310 --> 00:09:34,120
One last thing I want to show you a small illustration that could help understanding how it works.

113
00:09:34,120 --> 00:09:39,550
Is this cool illustration out I've found that's kind of show an animation of how everything works.

114
00:09:39,670 --> 00:09:46,420
So this sees the encoding phase with the input sequence and you can see that what happens is that we

115
00:09:46,420 --> 00:09:52,990
mix all the words together all the relations to get new representation of the input sequence and then

116
00:09:52,990 --> 00:09:55,680
as I said we starts the decoder.

117
00:09:55,790 --> 00:10:03,390
Just one token and we apply several time the decoder.

118
00:10:04,660 --> 00:10:11,340
With also the information from the encoder in order to have a new words at each step like this.

119
00:10:13,810 --> 00:10:23,940
Let's have a second Lukasz so let's get all whole input sequence right there and makes each word according

120
00:10:23,940 --> 00:10:29,610
to how they are related to the others and get new information and we repeat that same old time and get

121
00:10:30,090 --> 00:10:33,240
a new representation of the out input sequence.

122
00:10:33,240 --> 00:10:40,830
And yet now we just apply the decoder the other new word each time and using the information from the

123
00:10:40,830 --> 00:10:50,370
previous words and the whole encoding sentence we can needs little by little step by step predicts new

124
00:10:50,380 --> 00:10:51,300
words.

125
00:10:51,370 --> 00:10:55,200
And finally just get our final answer.

126
00:10:55,210 --> 00:11:04,240
So here the answers the sentences our nuts complete of course it's just an illustration but I find it

127
00:11:04,240 --> 00:11:09,020
pretty cool and it's and it's easier to understand what happens this way.

128
00:11:09,040 --> 00:11:13,720
So that was it for the general understanding of the transformer.

129
00:11:13,720 --> 00:11:18,000
Of course we will go into more details for each element of this model.

130
00:11:18,130 --> 00:11:25,790
But to summarize what we have to understand key is that there is still an encoder and decoder and we

131
00:11:25,810 --> 00:11:30,770
computes whole sequences at once instead of one words after the other.

132
00:11:31,060 --> 00:11:39,250
And the way we do that is by re composing sequences making sums of difference items from each sequence

133
00:11:39,700 --> 00:11:42,730
according to how they are related to each other.

134
00:11:42,760 --> 00:11:48,630
So that's just a way to get global information about the whole sentence.

135
00:11:48,700 --> 00:11:56,760
Just an intuition thing actually that's in my opinion would be how the brain processes sentences when

136
00:11:56,760 --> 00:11:58,150
you have a sentence in mind.

137
00:11:58,270 --> 00:12:03,330
We don't process the meaning of it sentence by going word after the other.

138
00:12:03,340 --> 00:12:10,300
We actually have the whole sentence in mind and we capture the meaning of it's at once and if you think

139
00:12:10,300 --> 00:12:14,210
about it that's actually easy to see when you think about reading.

140
00:12:14,230 --> 00:12:21,220
I don't know if you like me a slow reader but if you read slowly you may experience that you have to

141
00:12:21,220 --> 00:12:27,610
go back many times when you read a single sentence and that's not only because you maybe are not concentrating

142
00:12:27,610 --> 00:12:34,540
enough that's also because if you read too slow would afterwords it's not how your brain is optimized

143
00:12:34,600 --> 00:12:40,780
to understand a sentence when you arrive at the end of the sentence you have already forgotten the beginning

144
00:12:40,840 --> 00:12:43,860
and you are losing the whole sense of a sentence.

145
00:12:43,870 --> 00:12:49,450
Actually a sentence is made to be understood globally as one big block.

146
00:12:49,480 --> 00:12:56,170
Actually it's kind of a burden that we have to use words and that we have when we read or when you write

147
00:12:56,410 --> 00:13:03,760
to get them one at a time because if you think about that you might realize that when you hear someone

148
00:13:03,760 --> 00:13:10,420
speaking you wait for the end of a sentence and then you get the whole idea of the sentence at once.

149
00:13:10,420 --> 00:13:15,550
When you process the meaning of it's and that is actually what the first readers do.

150
00:13:15,550 --> 00:13:22,210
Those people that can read a page in a few seconds and have trained a lot to improve their reading speeds

151
00:13:22,480 --> 00:13:28,990
what they do instead of reading word by word is that they try to use the global vision to get the biggest

152
00:13:28,990 --> 00:13:35,230
group of words at once that they can and to process the whole meaning of it at once and you can do that

153
00:13:35,260 --> 00:13:38,500
because your brain is made to understand language like that.

154
00:13:38,830 --> 00:13:45,550
And so my understanding of this transformer is that it goes in the same direction instead of processing

155
00:13:45,550 --> 00:13:46,480
word by word.

156
00:13:46,510 --> 00:13:54,010
We try to capture the meaning of the whole sentence at once by combining each item with the others and

157
00:13:54,130 --> 00:14:00,400
I find it so interesting that we naturally get to those kind of models and that they are often the ones

158
00:14:00,400 --> 00:14:01,450
that we used before.

159
00:14:01,930 --> 00:14:05,280
So that's it for the general understanding of the transformer.

160
00:14:05,290 --> 00:14:12,850
Now we can go into more details and see exactly how the attention mechanism works and how they use it

161
00:14:12,910 --> 00:14:19,450
and all the other details that they used to make it even more performance popular I'm excited to go

162
00:14:19,450 --> 00:14:21,120
into those details and see you soon.
