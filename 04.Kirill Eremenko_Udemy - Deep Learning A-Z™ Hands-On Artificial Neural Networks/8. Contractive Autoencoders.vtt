WEBVTT
1
1

00:00:00.038  -->  00:00:02.670
<v ->Hello and welcome back to the course on Deep Learning.</v>
2

2

00:00:02.670  -->  00:00:04.790
Today we're talking about Contractive Autoencoders.
3

3

00:00:04.790  -->  00:00:08.060
This is going to be a very quick tutorial just to introduce
4

4

00:00:08.060  -->  00:00:11.220
the concept and then we'll advise some further reading where
5

5

00:00:11.220  -->  00:00:12.949
you can dig deeper into it.
6

6

00:00:12.949  -->  00:00:14.877
So, contractive autoencoders.
7

7

00:00:14.877  -->  00:00:17.660
This is another regularization technique
8

8

00:00:17.660  -->  00:00:21.420
just like sparse autoencoders and denoising autoencoders
9

9

00:00:21.420  -->  00:00:24.750
which is designed to combat the problem when we have
10

10

00:00:24.750  -->  00:00:28.010
an over-complete hidden layer in the autoencoder
11

11

00:00:28.010  -->  00:00:30.030
like we have here so that the problem that we have
12

12

00:00:30.030  -->  00:00:32.320
is that the autoencoder can just simply take the inputs,
13

13

00:00:32.320  -->  00:00:33.370
copy them into the hidden layer
14

14

00:00:33.370  -->  00:00:34.540
and then copy them into the output
15

15

00:00:34.540  -->  00:00:38.420
without actually learning any valuable
16

16

00:00:38.420  -->  00:00:40.640
feature extraction in its training
17

17

00:00:40.640  -->  00:00:42.110
and that's what we want it to do.
18

18

00:00:42.110  -->  00:00:44.760
We want it to be able to extract features.
19

19

00:00:44.760  -->  00:00:47.160
And, so, what the contractive autoencoder does
20

20

00:00:47.160  -->  00:00:49.300
is it leverages the whole process,
21

21

00:00:49.300  -->  00:00:50.890
this whole training process where
22

22

00:00:50.890  -->  00:00:52.960
information goes through the autoencoder,
23

23

00:00:52.960  -->  00:00:53.990
then we get the outputs
24

24

00:00:53.990  -->  00:00:55.580
and then they are compared to the inputs.
25

25

00:00:55.580  -->  00:01:00.580
Well, contractive autoencoders, they add a penalty
26

26

00:01:00.800  -->  00:01:04.220
into this loss-function that's going back through the,
27

27

00:01:04.220  -->  00:01:06.020
has been propagated back through the network
28

28

00:01:06.020  -->  00:01:09.700
and it specifically doesn't allow the autoencoder
29

29

00:01:09.700  -->  00:01:12.520
to just simply just copy these values across.
30

30

00:01:12.520  -->  00:01:15.910
How it works, we're not going to go into detail on that
31

31

00:01:15.910  -->  00:01:17.726
because it is very complex,
32

32

00:01:17.726  -->  00:01:20.450
there's quite a lot of math behind it,
33

33

00:01:20.450  -->  00:01:25.450
and we're just going to recommend a wonderful article
34

34

00:01:25.510  -->  00:01:28.200
here by Salah Rifai,
35

35

00:01:28.200  -->  00:01:29.680
it's called "Contractive Auto-Encoders:
36

36

00:01:29.680  -->  00:01:33.270
Explicit Invariance During Feature Extraction".
37

37

00:01:33.270  -->  00:01:35.210
And what I also wanted to mention
38

38

00:01:35.210  -->  00:01:37.580
is that this article has quite a lot of authors.
39

39

00:01:37.580  -->  00:01:39.140
It has I think five authors,
40

40

00:01:39.140  -->  00:01:41.170
and one of them is Yoshua Benjio,
41

41

00:01:41.170  -->  00:01:43.651
so quite an in-depth study into the topic.
42

42

00:01:43.651  -->  00:01:46.413
So from this tutorial the takeaway is that
43

43

00:01:46.413  -->  00:01:49.370
there is such a thing as contractive autoencoders
44

44

00:01:49.370  -->  00:01:51.770
and based actually on this article
45

45

00:01:51.770  -->  00:01:55.096
they say that they can even get better results
46

46

00:01:55.096  -->  00:01:59.790
than denoising autoencoders on some certain data sets.
47

47

00:01:59.790  -->  00:02:01.550
And so it's just important to be aware
48

48

00:02:01.550  -->  00:02:04.040
of the fact that contractive autoencoders exist
49

49

00:02:04.040  -->  00:02:06.510
and if you ever need to dig deeper
50

50

00:02:06.510  -->  00:02:07.820
1into contractive autoencoders,
51

51

00:02:07.820  -->  00:02:09.350
or if you would like to learn more about them,
52

52

00:02:09.350  -->  00:02:12.140
this is the best place to get started,
53

53

00:02:12.140  -->  00:02:15.570
this specific paper by Salah Rifai,
54

54

00:02:15.570  -->  00:02:18.500
and others including Yoshua Benjio.
55

55

00:02:18.500  -->  00:02:20.150
And that's it for today,
56

56

00:02:20.150  -->  00:02:21.400
I look forward to seeing you next time.
57

57

00:02:21.400  -->  00:02:23.103
Until then, enjoy Deep Learning.
