WEBVTT
1
1

00:00:00.130  -->  00:00:02.800
<v ->Hello and welcome to this Python tutorial,</v>
2

2

00:00:02.800  -->  00:00:05.235
so that's it we built our model we fitted the
3

3

00:00:05.235  -->  00:00:08.360
logistic regression model to our training set
4

4

00:00:08.360  -->  00:00:10.860
and then we predicted the test that resolved.
5

5

00:00:10.860  -->  00:00:14.235
We used the confusion matrix to evaluate the predictive
6

6

00:00:14.235  -->  00:00:17.327
power of our logistic regression model, that was actually
7

7

00:00:17.327  -->  00:00:21.380
quite fine, we found sixty five plus twenty four equals
8

8

00:00:21.380  -->  00:00:24.450
eighty nine correct predictions and eight plus three equals
9

9

00:00:24.450  -->  00:00:27.820
eleven incorrect predictions but that's not really
10

10

00:00:27.820  -->  00:00:31.350
the fun way of visualizing the predictive power.
11

11

00:00:31.350  -->  00:00:34.190
What is much more fun and much more exciting is to look
12

12

00:00:34.190  -->  00:00:37.100
at the result on a graph, so that's what we're going to do
13

13

00:00:37.100  -->  00:00:39.750
in this tutorial; we're going to make a graph
14

14

00:00:39.750  -->  00:00:42.730
where we will clearly see the regions where our logistic
15

15

00:00:42.730  -->  00:00:45.560
regression model predicts, yes the user is going to buy
16

16

00:00:45.560  -->  00:00:49.260
the product and no the user is not going to buy the product.
17

17

00:00:49.260  -->  00:00:51.080
So lets make this graph.
18

18

00:00:51.080  -->  00:00:54.200
So unfortunately we cannot make this graph
19

19

00:00:54.200  -->  00:00:57.800
in just one line or two lines; I tried to make it
20

20

00:00:57.800  -->  00:01:00.870
the simplest as possible but by doing that I ended
21

21

00:01:00.870  -->  00:01:03.940
up with a little more than ten lines of code.
22

22

00:01:03.940  -->  00:01:06.300
So what I'm gonna do, I'm not going to write the whole
23

23

00:01:06.300  -->  00:01:09.235
code right now because you might fall asleep.
24

24

00:01:09.235  -->  00:01:11.891
So what I'm gonna do is I'm going to take the code
25

25

00:01:11.891  -->  00:01:15.390
I prepared to visualize the training set results
26

26

00:01:15.390  -->  00:01:19.320
in my text editor; I'm going to copy it, paste it here
27

27

00:01:19.320  -->  00:01:22.050
then I will select this code and execute it.
28

28

00:01:22.050  -->  00:01:24.300
We will look at our results, interpret them
29

29

00:01:24.300  -->  00:01:25.950
and then for those of you interested
30

30

00:01:25.950  -->  00:01:29.140
in how we made such a graph, at the end of the tutorial
31

31

00:01:29.140  -->  00:01:31.820
I will explain how it's made in the graph.
32

32

00:01:31.820  -->  00:01:35.690
Okay, so I copied it I'm gonna paste it now; here it is
33

33

00:01:35.690  -->  00:01:39.360
here is the code so that's fifteen lines of code
34

34

00:01:39.360  -->  00:01:42.280
as you can see, and so since I can't wait to show you
35

35

00:01:42.280  -->  00:01:46.040
the results we're going to select the code now
36

36

00:01:46.040  -->  00:01:48.047
and I'm going to press Command and Control
37

37

00:01:48.047  -->  00:01:50.830
+ Enter to execute and we will see
38

38

00:01:50.830  -->  00:01:53.920
the whole thing behind the logistic regression model
39

39

00:01:53.920  -->  00:01:55.610
we gonna see the true results
40

40

00:01:55.610  -->  00:01:58.810
under regions of predictions, so let's do it.
41

41

00:01:58.810  -->  00:02:02.810
Command and Control + Enter to execute and here we go,
42

42

00:02:02.810  -->  00:02:06.150
that's the graph, so I'm going to enlarge this
43

43

00:02:08.180  -->  00:02:11.050
and now I'll explain what to interpret.
44

44

00:02:11.050  -->  00:02:14.970
Okay, so let's analyze this graph step by step
45

45

00:02:14.970  -->  00:02:17.920
first let's focus on all the points here; we can see that
46

46

00:02:17.920  -->  00:02:21.064
we have some red points and some green points.
47

47

00:02:21.064  -->  00:02:24.790
So all these points that we see on this graph are the
48

48

00:02:24.790  -->  00:02:28.550
observation points of our training set, that is these
49

49

00:02:28.550  -->  00:02:32.460
are all the users of the social network that were selected
50

50

00:02:32.460  -->  00:02:36.500
to go to the training set and each of these users here
51

51

00:02:36.500  -->  00:02:39.887
is characterized by its age here in the x-axis
52

52

00:02:39.887  -->  00:02:44.060
and its estimated salary here on the y-axis.
53

53

00:02:44.060  -->  00:02:48.270
Now we can see that there's some red points here
54

54

00:02:48.270  -->  00:02:50.820
and some green points here.
55

55

00:02:50.820  -->  00:02:53.800
The red points are the training set observations
56

56

00:02:53.800  -->  00:02:56.570
for which the dependent variable purchased
57

57

00:02:56.570  -->  00:03:00.240
is equal to zero and the green points are the training
58

58

00:03:00.240  -->  00:03:03.420
set observations for which the dependent variable
59

59

00:03:03.420  -->  00:03:05.860
purchased is equal to one.
60

60

00:03:05.860  -->  00:03:08.440
That means that the red points here are
61

61

00:03:08.440  -->  00:03:12.490
the users who didn't buy the SUV and the green points
62

62

00:03:12.490  -->  00:03:16.120
here are the users who bought, who actually bought the SUV.
63

63

00:03:16.120  -->  00:03:18.980
So now as a first step of analysis
64

64

00:03:18.980  -->  00:03:21.787
lets give an interpretation of what we observe here
65

65

00:03:21.787  -->  00:03:25.950
with these users, okay so first we can see that
66

66

00:03:25.950  -->  00:03:30.090
the users who are young with a low estimated salary
67

67

00:03:30.090  -->  00:03:33.407
so these users here actually didn't buy the SUV because
68

68

00:03:33.407  -->  00:03:37.100
these points are the real observation points they're red
69

69

00:03:37.100  -->  00:03:39.690
and red corresponds to zero here so that means that
70

70

00:03:39.690  -->  00:03:42.840
all these points here are the users who didn't buy the SUV
71

71

00:03:42.840  -->  00:03:44.373
the users in the training set.
72

72

00:03:45.622  -->  00:03:50.220
Then if we look at the users who are older and with a higher
73

73

00:03:50.220  -->  00:03:53.480
estimated salary; well we can see that most of these users
74

74

00:03:53.480  -->  00:03:56.020
actually bought the SUV and it actually makes sense
75

75

00:03:56.020  -->  00:03:59.540
because the SUV is more like a family car and therefore
76

76

00:03:59.540  -->  00:04:01.830
more interesting for these older users here with the
77

77

00:04:01.830  -->  00:04:05.570
high estimated salary, besides we can also see that some
78

78

00:04:05.570  -->  00:04:09.390
older people even with the low estimated salary actually
79

79

00:04:09.390  -->  00:04:12.750
bought the SUV as we can see that we have some green points
80

80

00:04:12.750  -->  00:04:16.700
here that correspond to an age above the average
81

81

00:04:16.700  -->  00:04:19.850
the average is here, but an estimated salary below
82

82

00:04:19.850  -->  00:04:22.480
the average because the average is here.
83

83

00:04:22.480  -->  00:04:25.896
Okay, so these guys these older guys although they have
84

84

00:04:25.896  -->  00:04:28.380
the lowest estimated salary actually bought the SUV
85

85

00:04:28.380  -->  00:04:30.330
probably because they've been saving up some money
86

86

00:04:30.330  -->  00:04:32.740
or maybe they finished paying off their mortgage
87

87

00:04:32.740  -->  00:04:35.210
I don't know but what's for sure is that they couldn't
88

88

00:04:35.210  -->  00:04:39.050
resist buying this very cool luxury SUV offered
89

89

00:04:39.050  -->  00:04:41.260
at a ridiculously low price
90

90

00:04:42.450  -->  00:04:44.500
and on the other hand we can also see that
91

91

00:04:44.500  -->  00:04:46.880
there's some young people here
92

92

00:04:46.880  -->  00:04:51.410
with a high estimated salary who actually bought the SUV
93

93

00:04:51.410  -->  00:04:53.680
you know maybe because it's a very cool SUV and they
94

94

00:04:53.680  -->  00:04:56.500
want to impress their friends and take them in to road trips
95

95

00:04:56.500  -->  00:04:59.410
or maybe they already have a family I don't know.
96

96

00:04:59.410  -->  00:05:02.740
Anyway, they bought the SUV actually there are a lot
97

97

00:05:02.740  -->  00:05:06.870
of buyers so this must be a very cool and cheap SUV.
98

98

00:05:06.870  -->  00:05:10.530
Okay and now what is the goal of classification,
99

99

00:05:10.530  -->  00:05:12.730
now we're talking machine learning;
100

100

00:05:12.730  -->  00:05:15.210
why are we making some classifiers
101

101

00:05:15.210  -->  00:05:17.410
and what will classifiers will do,
102

102

00:05:17.410  -->  00:05:19.470
at least what are we trying to make them do
103

103

00:05:19.470  -->  00:05:21.790
for this particular business problem.
104

104

00:05:21.790  -->  00:05:25.860
Well the goal here is to classify the right users in to
105

105

00:05:25.860  -->  00:05:29.280
the right categories, that is we are trying to make a
106

106

00:05:29.280  -->  00:05:33.070
classifier that will catch the right users in to the right
107

107

00:05:33.070  -->  00:05:37.090
category which are, yes they buy the SUV and no they don't
108

108

00:05:37.090  -->  00:05:40.590
buy the SUV and we represented they way our classifier
109

109

00:05:40.590  -->  00:05:43.600
catches these users by plotting what are called
110

110

00:05:43.600  -->  00:05:47.410
the prediction regions and so the prediction regions are
111

111

00:05:47.410  -->  00:05:51.270
the two regions that we see on this graph, this red one here
112

112

00:05:51.270  -->  00:05:54.720
and this green one here and the red prediction region
113

113

00:05:54.720  -->  00:05:58.390
is the region where are classifier catches all the users
114

114

00:05:58.390  -->  00:06:02.010
that don't buy the SUV and the green prediction region is
115

115

00:06:02.010  -->  00:06:03.670
the region where our classifier
116

116

00:06:03.670  -->  00:06:06.040
catches all the users that buy the SUV.
117

117

00:06:06.040  -->  00:06:09.760
But be careful this is according to the classifier
118

118

00:06:09.760  -->  00:06:13.920
that is for each user of this red prediction region here
119

119

00:06:13.920  -->  00:06:17.610
our logistic regression classifier predicts that the user
120

120

00:06:17.610  -->  00:06:20.860
doesn't buy the SUV and for each user of this
121

121

00:06:20.860  -->  00:06:24.540
green prediction region here our classifier will predict
122

122

00:06:24.540  -->  00:06:27.930
that the user buys the SUV; even if that's not the case
123

123

00:06:27.930  -->  00:06:30.840
in real life that's just a prediction but that's what
124

124

00:06:30.840  -->  00:06:33.690
our classifier believes will happen.
125

125

00:06:33.690  -->  00:06:38.330
It is the classifier prediction compared to the truth here
126

126

00:06:38.330  -->  00:06:41.980
which is the point; the point is the truth and the region
127

127

00:06:41.980  -->  00:06:43.750
here is the prediction.
128

128

00:06:43.750  -->  00:06:47.840
So that makes an awesome tool because for each new user
129

129

00:06:47.840  -->  00:06:50.873
of the social network well our classifier, our logistic
130

130

00:06:50.873  -->  00:06:55.030
regression classifier will tell based on its age and its
131

131

00:06:55.030  -->  00:06:57.920
estimated salary if this user belongs
132

132

00:06:57.920  -->  00:07:00.200
to this red prediction region here
133

133

00:07:00.200  -->  00:07:03.120
and therefore doesn't buy the SUV or if this user
134

134

00:07:03.120  -->  00:07:06.070
belongs to this green prediction region here and therefore
135

135

00:07:06.070  -->  00:07:09.930
buys the SUV and that way this business client car company
136

136

00:07:09.930  -->  00:07:13.340
can substantially optomize their marketing campaign
137

137

00:07:13.340  -->  00:07:16.680
by targeting the social network ads to the the users
138

138

00:07:16.680  -->  00:07:20.040
in the green region because these are the users that are
139

139

00:07:20.040  -->  00:07:24.340
predicted to buy the SUV according to our classifier.
140

140

00:07:24.340  -->  00:07:27.910
Now the other very important thing to understand is that
141

141

00:07:27.910  -->  00:07:31.310
these are two prediction regions separated by
142

142

00:07:31.310  -->  00:07:34.400
a straight line which is this straight line here
143

143

00:07:34.400  -->  00:07:37.560
and this straight line is called the prediction boundary.
144

144

00:07:37.560  -->  00:07:41.230
Because it's the boundary between the two prediction regions
145

145

00:07:41.230  -->  00:07:44.980
and the fact that it's a straight line is not random
146

146

00:07:44.980  -->  00:07:47.630
it is for a particular reason and that's the thing very
147

147

00:07:47.630  -->  00:07:50.840
important to understand because that's the essence
148

148

00:07:50.840  -->  00:07:54.070
of logistic regression If the prediction boundary
149

149

00:07:54.070  -->  00:07:56.300
is a straight line here that's because
150

150

00:07:56.300  -->  00:08:00.890
our logistic progression classifier is a linear classifier
151

151

00:08:00.890  -->  00:08:03.350
that means that here since we're in two dimensions
152

152

00:08:03.350  -->  00:08:05.140
you know because we have two independent variables
153

153

00:08:05.140  -->  00:08:08.190
the age and the estimated salary so we are in two dimensions
154

154

00:08:08.190  -->  00:08:11.440
then since the logistic regression classifier is a linear
155

155

00:08:11.440  -->  00:08:15.110
classifier then the prediction boundary separator here
156

156

00:08:15.110  -->  00:08:19.640
can only be a straight line; if we were in three dimensions
157

157

00:08:19.640  -->  00:08:23.360
then is would be a straight plain separating two spaces
158

158

00:08:23.360  -->  00:08:26.070
but here in two dimensions it's a straight line and it will
159

159

00:08:26.070  -->  00:08:29.050
always be a straight line if your classifier is a linear
160

160

00:08:29.050  -->  00:08:32.830
classifier but you will see later that when we build
161

161

00:08:32.830  -->  00:08:36.070
nonlinear classifiers then the prediction boundaries
162

162

00:08:36.070  -->  00:08:39.790
separator won't be a straight line anymore; I won't tell you
163

163

00:08:39.790  -->  00:08:42.880
more right now and I'll let you wait for the surprise.
164

164

00:08:42.880  -->  00:08:46.280
So here we can clearly see that our logistic regression
165

165

00:08:46.280  -->  00:08:50.260
classifier manages to catch most of the users who didn't
166

166

00:08:50.260  -->  00:08:53.870
buy the SUV in the red region here and most of the users
167

167

00:08:53.870  -->  00:08:56.930
who bought the SUV in the green region here.
168

168

00:08:56.930  -->  00:09:00.570
So it actually did a pretty good job however it seems to
169

169

00:09:00.570  -->  00:09:03.960
have trouble catching some green users here who
170

170

00:09:03.960  -->  00:09:07.900
in spite of their low salary bought the luxury SUV
171

171

00:09:07.900  -->  00:09:11.190
as well as those other green users here who also bought the
172

172

00:09:11.190  -->  00:09:14.990
luxury SUV because as you can see these green points here
173

173

00:09:14.990  -->  00:09:18.670
and those here are in the red region which is the region
174

174

00:09:18.670  -->  00:09:22.370
where our classifier predicts that the users don't buy
175

175

00:09:22.370  -->  00:09:26.300
the SUV and those incorrect predictions are due
176

176

00:09:26.300  -->  00:09:29.860
specifically to the fact that our classifier is a linear
177

177

00:09:29.860  -->  00:09:32.160
classifier and because our users
178

178

00:09:32.160  -->  00:09:34.640
are not linearly distributed
179

179

00:09:34.640  -->  00:09:35.473
if they were
180

180

00:09:35.473  -->  00:09:38.270
linearly distributed then we would have all the green points
181

181

00:09:38.270  -->  00:09:41.820
here in this space and all the red points here in this space
182

182

00:09:41.820  -->  00:09:44.150
and then a liner classifier with a straight line could
183

183

00:09:44.150  -->  00:09:46.650
perfectly separate all the red points here
184

184

00:09:46.650  -->  00:09:48.210
and all the green points here,
185

185

00:09:48.210  -->  00:09:50.910
but here we have some rebellious points who are
186

186

00:09:50.910  -->  00:09:53.390
not in the wanted linear regions and because
187

187

00:09:53.390  -->  00:09:55.730
our linear classifier has a linear straight line
188

188

00:09:55.730  -->  00:09:58.740
separator that's why is has trouble catching those
189

189

00:09:58.740  -->  00:10:00.900
users here and those here.
190

190

00:10:00.900  -->  00:10:02.500
You can clearly see that even if you
191

191

00:10:02.500  -->  00:10:06.070
tried to rotate this straight line here well you will
192

192

00:10:06.070  -->  00:10:09.520
always have some green points in the wrong category.
193

193

00:10:09.520  -->  00:10:12.412
For example if we tried to rotate here this way like
194

194

00:10:12.412  -->  00:10:16.640
putting it down; well okay we will catch these green points
195

195

00:10:16.640  -->  00:10:20.380
here in the right green region here but since we rotated
196

196

00:10:20.380  -->  00:10:24.160
down we will take more green users here because this
197

197

00:10:24.160  -->  00:10:27.750
will go up and more green users here will be in the
198

198

00:10:27.750  -->  00:10:31.960
red region; so that's the best separator that the logistic
199

199

00:10:31.960  -->  00:10:35.070
regression classifier could find and it couldn't do better
200

200

00:10:35.070  -->  00:10:38.010
because it can only be a straight line separating these
201

201

00:10:38.010  -->  00:10:40.680
two regions because to catch those users
202

202

00:10:40.680  -->  00:10:42.540
the green users here and the green users here
203

203

00:10:42.540  -->  00:10:44.760
in the right categories that is the green region
204

204

00:10:44.760  -->  00:10:47.070
our classifier will need to make some kind of a curve
205

205

00:10:47.070  -->  00:10:51.810
here to you know classify correctly those green users here
206

206

00:10:51.810  -->  00:10:53.950
and here and place them in the green region
207

207

00:10:53.950  -->  00:10:56.150
and that would prevent our classifier from making
208

208

00:10:56.150  -->  00:10:59.030
this incorrect predictions here because it is
209

209

00:10:59.030  -->  00:11:01.910
a straight line; with a curve here we would catch
210

210

00:11:01.910  -->  00:11:04.610
all the red users properly in the red region and
211

211

00:11:04.610  -->  00:11:06.910
all the green users in the green region.
212

212

00:11:06.910  -->  00:11:09.410
So that would make an awesome classifier
213

213

00:11:09.410  -->  00:11:11.980
and you will see how our nonlinear classifiers
214

214

00:11:11.980  -->  00:11:14.210
will make a terrific job at doing this
215

215

00:11:14.210  -->  00:11:16.410
I can't wait to show you this.
216

216

00:11:16.410  -->  00:11:19.030
Okay and now eventually the last thing very important
217

217

00:11:19.030  -->  00:11:23.070
to understand is that this is the training set; this is
218

218

00:11:23.070  -->  00:11:25.940
a training set so that means that our classifier learnt
219

219

00:11:25.940  -->  00:11:29.220
how to classify based on this information here
220

220

00:11:29.220  -->  00:11:31.510
so I would hold my breath a few more seconds
221

221

00:11:31.510  -->  00:11:34.420
until I find out if our logistic regression classifier
222

222

00:11:34.420  -->  00:11:37.900
can manage to make good predictions of new observations
223

223

00:11:37.900  -->  00:11:41.020
that is to classify new users in to the right regions
224

224

00:11:41.020  -->  00:11:44.250
which by the way are fixed regions here because these
225

225

00:11:44.250  -->  00:11:46.760
are the regions generated by the learning experience
226

226

00:11:46.760  -->  00:11:49.810
of our logistic regression classifier and therefore
227

227

00:11:49.810  -->  00:11:52.500
won't change if we look at some new observations
228

228

00:11:52.500  -->  00:11:56.420
that is new social network users and that's what we're
229

229

00:11:56.420  -->  00:11:59.363
about to find out on the test set so hold on.
230

230

00:12:01.070  -->  00:12:04.550
So what I'm gonna do now I'm going to copy this
231

231

00:12:04.550  -->  00:12:08.110
because we don't wanna, we want to be efficient
232

232

00:12:09.300  -->  00:12:12.610
and base it here and now I'm just going to replace
233

233

00:12:12.610  -->  00:12:16.773
training here by test,
234

234

00:12:17.660  -->  00:12:21.570
all right, and here I just have to replace x-train
235

235

00:12:22.510  -->  00:12:23.540
by x-test
236

236

00:12:24.490  -->  00:12:27.670
and y-train by y-test
237

237

00:12:27.670  -->  00:12:29.610
that's all that's all I need to do, oh and maybe
238

238

00:12:29.610  -->  00:12:34.390
the title I'm going to change the title of the graph test.
239

239

00:12:34.390  -->  00:12:38.540
Okay so let's now see how our logistic regression classifier
240

240

00:12:38.540  -->  00:12:41.730
predicts the new observations on the test set
241

241

00:12:41.730  -->  00:12:45.547
on which our model wasn't built, so Command and Control
242

242

00:12:45.547  -->  00:12:50.190
+ enter to execute and lets see.
243

243

00:12:50.190  -->  00:12:53.620
Alright not bad, not bad, I'm going to enlarge this
244

244

00:12:54.940  -->  00:12:59.400
here we go, so this looks good this looks very good
245

245

00:12:59.400  -->  00:13:01.780
these are the real results in red that means the users
246

246

00:13:01.780  -->  00:13:04.680
that didn't buy the SUV in reality and these are the
247

247

00:13:04.680  -->  00:13:07.340
customers that bought it and we can see that the prediction
248

248

00:13:07.340  -->  00:13:11.040
regions predict well those real values because
249

249

00:13:11.040  -->  00:13:13.670
all those red points here in the red region
250

250

00:13:13.670  -->  00:13:15.490
so that's the correct prediction
251

251

00:13:15.490  -->  00:13:17.830
all those green points here in the green region
252

252

00:13:17.830  -->  00:13:19.750
so that's some other correct predictions
253

253

00:13:19.750  -->  00:13:22.410
and of course since this is a linear classifier
254

254

00:13:22.410  -->  00:13:25.190
the logistic regression makes a few mistakes here
255

255

00:13:25.190  -->  00:13:26.730
but that's fine and that's actually
256

256

00:13:26.730  -->  00:13:29.040
the incorrect predictions that we saw on the
257

257

00:13:29.040  -->  00:13:32.730
confusion matrix remember there was eleven incorrect
258

258

00:13:32.730  -->  00:13:34.570
predictions we can count them here.
259

259

00:13:34.570  -->  00:13:39.570
This green point should be here so it's one then two
260

260

00:13:39.570  -->  00:13:43.700
three, four, five, six, seven, eight, and then we have
261

261

00:13:43.700  -->  00:13:46.240
to count the red points in the green region
262

262

00:13:46.240  -->  00:13:51.240
that was incorrectly predicted so nine, ten and eleven.
263

263

00:13:52.040  -->  00:13:54.870
Okay, so yes that's the eleven incorrect predictions
264

264

00:13:54.870  -->  00:13:59.600
that we found in the confusion matrix; alright so
265

265

00:13:59.600  -->  00:14:03.080
that's the first classification model we built I hope you
266

266

00:14:03.080  -->  00:14:06.650
like the graph, okay so congratulations.
267

267

00:14:06.650  -->  00:14:09.380
You implemented your first classification model
268

268

00:14:09.380  -->  00:14:11.330
and now for those of you who are interested
269

269

00:14:11.330  -->  00:14:14.730
in understanding how the graph was made
270

270

00:14:14.730  -->  00:14:18.300
stay with me and I will explain right now how it works.
271

271

00:14:18.300  -->  00:14:23.287
Okay, so the best way to explain it is to reselect this
272

272

00:14:24.740  -->  00:14:27.150
Command and Control + Enter to execute
273

273

00:14:27.150  -->  00:14:30.270
and here's the graph, so what is the idea how
274

274

00:14:30.270  -->  00:14:34.010
how did we plot those prediction regions
275

275

00:14:34.010  -->  00:14:37.080
well the idea is actually pretty cool,
276

276

00:14:37.080  -->  00:14:41.660
because we took all the pixels of this frame here
277

277

00:14:41.660  -->  00:14:43.900
that means this pixel, this pixel all the pixels
278

278

00:14:43.900  -->  00:14:47.050
well actually with a oh point oh one resolution
279

279

00:14:47.050  -->  00:14:50.050
so we took all the pixel points of this framework
280

280

00:14:50.050  -->  00:14:52.840
and we applied our classifier on it
281

281

00:14:52.840  -->  00:14:55.170
and so you know it's like each of these pixel points
282

282

00:14:55.170  -->  00:14:58.290
is a user of the social network with a salary
283

283

00:14:58.290  -->  00:15:01.280
and an age, so it's like, it's like an observation
284

284

00:15:01.280  -->  00:15:04.260
only it's not the observations we have in our data set.
285

285

00:15:04.260  -->  00:15:07.470
It's a new observation we create but we have to picture
286

286

00:15:07.470  -->  00:15:10.110
this like a user in the social network with an
287

287

00:15:10.110  -->  00:15:11.910
estimated salary and an age.
288

288

00:15:11.910  -->  00:15:15.860
So for each of these new pixel points these new pixel
289

289

00:15:15.860  -->  00:15:20.220
observation points; we applied our logistic regression model
290

290

00:15:20.220  -->  00:15:23.790
to predict if this pixel observation point
291

291

00:15:23.790  -->  00:15:27.690
has value zero or one, and if our classifier predicts
292

292

00:15:27.690  -->  00:15:32.160
zero then it's going to colorize this pixel point in red
293

293

00:15:32.160  -->  00:15:35.390
and if the classifier predicts one it's going to colorize
294

294

00:15:35.390  -->  00:15:37.630
the pixel point in green.
295

295

00:15:37.630  -->  00:15:40.412
So by doing this on all the pixel
296

296

00:15:40.412  -->  00:15:45.412
points in the frame here it colorizes all the pixel points
297

297

00:15:45.850  -->  00:15:49.410
that have the zero predictions, the zero predictions here
298

298

00:15:49.410  -->  00:15:53.170
and all the pixel points that have the one prediction
299

299

00:15:53.170  -->  00:15:56.010
and since the logistic regression is a classifier
300

300

00:15:56.010  -->  00:16:00.700
the limits between those sets of points is a straight line
301

301

00:16:00.700  -->  00:16:03.860
because as I told earlier the logistic regression is
302

302

00:16:03.860  -->  00:16:06.930
a liner classifier which means it's a straight line
303

303

00:16:06.930  -->  00:16:10.020
in two dimensions and now that you understand that
304

304

00:16:10.020  -->  00:16:12.020
you will understand this code very well.
305

305

00:16:14.000  -->  00:16:17.160
Okay, so now let's go through the lines one by one
306

306

00:16:17.160  -->  00:16:19.820
we start by importing the listed call map class that
307

307

00:16:19.820  -->  00:16:23.490
will help us colorize all the data points
308

308

00:16:23.490  -->  00:16:26.560
then we create this line to just create some local variables
309

309

00:16:26.560  -->  00:16:30.080
x-set so that we can replace x-train and y-train easily
310

310

00:16:30.080  -->  00:16:33.400
by x-test and y-test instead of having to replace it
311

311

00:16:33.400  -->  00:16:36.560
everywhere because you know we use x-set all the time
312

312

00:16:36.560  -->  00:16:40.260
so you know it's just a shortcut to avoid having to
313

313

00:16:40.260  -->  00:16:43.900
replace too many times the x-test and x-train here.
314

314

00:16:43.900  -->  00:16:47.550
Okay here with this two lines of code x=one and x-two
315

315

00:16:47.550  -->  00:16:50.980
equals np.mesh grid we prepare the grid you know
316

316

00:16:50.980  -->  00:16:54.720
with all the pixel points that I just talked to you about.
317

317

00:16:54.720  -->  00:16:58.540
So here as you can see we take the minimum values of the
318

318

00:16:58.540  -->  00:17:02.160
age values minus one because we don't want our points
319

319

00:17:02.160  -->  00:17:05.520
to be squeezed on the axis here and the maximum values
320

320

00:17:05.520  -->  00:17:08.940
of the age, you know to get the range of pixels we want to
321

321

00:17:08.940  -->  00:17:13.780
include in the frame and same for the salary we're taking
322

322

00:17:13.780  -->  00:17:17.850
the minimum salary minus one and the maximum salary plus
323

323

00:17:17.850  -->  00:17:21.360
one to get all the estimated salary range and minus one
324

324

00:17:21.360  -->  00:17:23.850
and plus one so that all the points aren't squeezed;
325

325

00:17:23.850  -->  00:17:26.580
and here we're choosing the steps 0.01
326

326

00:17:26.580  -->  00:17:29.770
that means that there is an 0.01 resolution
327

327

00:17:29.770  -->  00:17:33.120
so for example if I had chosen 0.5 this
328

328

00:17:33.120  -->  00:17:36.070
wouldn't have been that dense we would actually see
329

329

00:17:36.070  -->  00:17:38.240
the pixel points but that's better to take
330

330

00:17:38.240  -->  00:17:39.850
this resolution because these are nice
331

331

00:17:39.850  -->  00:17:43.250
prediction regions like it's continuous.
332

332

00:17:43.250  -->  00:17:46.420
Okay and then with this line of code that's where
333

333

00:17:46.420  -->  00:17:48.920
the whole magic happens, because this is where
334

334

00:17:48.920  -->  00:17:53.680
we apply the classifier on all the pixel observation points
335

335

00:17:53.680  -->  00:17:57.100
and by doing that it colorizes all the red pixel points
336

336

00:17:57.100  -->  00:17:59.300
and all the green pixel points and we use this
337

337

00:17:59.300  -->  00:18:03.310
contour function too actually make the contour between
338

338

00:18:03.310  -->  00:18:06.560
the two prediction regions the red one and the green one.
339

339

00:18:06.560  -->  00:18:09.060
Then as you can see here we use the predict function
340

340

00:18:09.060  -->  00:18:12.110
of our classifier, the logistic regression classifier,
341

341

00:18:12.110  -->  00:18:15.130
to predict if each of the pixel points belongs
342

342

00:18:15.130  -->  00:18:18.250
to class zero or class one then if the pixel point
343

343

00:18:18.250  -->  00:18:21.050
belongs to class zero it will be colorized in red
344

344

00:18:21.050  -->  00:18:23.150
and if the pixel point belongs to class one
345

345

00:18:23.150  -->  00:18:25.320
it will be colorized in green.
346

346

00:18:25.320  -->  00:18:28.770
Okay so here we plot the limits of x the age and
347

347

00:18:28.770  -->  00:18:31.700
y the estimated salary then with this loop here we
348

348

00:18:31.700  -->  00:18:35.180
plot all the data points that are the real values
349

349

00:18:35.180  -->  00:18:38.310
so all the red data points here and the green data points
350

350

00:18:38.310  -->  00:18:40.700
so we're using this PLT.scatter.
351

351

00:18:40.700  -->  00:18:43.160
Which is what we use for matplotlib to make a scatter
352

352

00:18:43.160  -->  00:18:46.260
plot and then very simply here we add the title logistic
353

353

00:18:46.260  -->  00:18:49.264
regression training set, the x label age
354

354

00:18:49.264  -->  00:18:52.920
the y label estimated salary, we plot the legend
355

355

00:18:52.920  -->  00:18:55.610
to specify that the red points correspond to zero
356

356

00:18:55.610  -->  00:18:57.710
that is the user didn't buy and the green point
357

357

00:18:57.710  -->  00:19:01.960
corresponds to one that is the users bought the product
358

358

00:19:01.960  -->  00:19:05.860
and PLT show to display the graph and specify that this is
359

359

00:19:05.860  -->  00:19:08.710
the end of the graph, all right so congratulations if
360

360

00:19:08.710  -->  00:19:12.050
you're still here congratulations for having the curiosity
361

361

00:19:12.050  -->  00:19:15.230
of wondering how we can build such a plot
362

362

00:19:15.230  -->  00:19:17.140
so that's a lot of things to learn in this section
363

363

00:19:17.140  -->  00:19:19.450
because you learned how to make a logistic regression
364

364

00:19:19.450  -->  00:19:21.970
model and how to make an awesome chart
365

365

00:19:21.970  -->  00:19:25.210
so congratulations, in the next section things are
366

366

00:19:25.210  -->  00:19:28.090
gonna get even more exciting because we will
367

367

00:19:28.090  -->  00:19:30.710
build even more powerful classifiers
368

368

00:19:30.710  -->  00:19:32.680
so I look forward to showing that to you
369

369

00:19:32.680  -->  00:19:34.580
and until then enjoy machine learning.
