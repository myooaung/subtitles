1
1

00:00:00,400  -->  00:00:01,233
<v Instructor>Hello and welcome back</v>
2

2

00:00:01,233  -->  00:00:02,370
to the course on Deep Learning.
3

3

00:00:02,370  -->  00:00:04,220
In today's tutorial we're going to walk through the steps
4

4

00:00:04,220  -->  00:00:05,770
that are included in the process
5

5

00:00:05,770  -->  00:00:08,060
of training an Auto Encoder,
6

6

00:00:08,060  -->  00:00:10,100
so let's get started.
7

7

00:00:10,100  -->  00:00:11,640
Here we've got an Auto Encoder,
8

8

00:00:11,640  -->  00:00:13,990
and obviously we've got some inputs,
9

9

00:00:13,990  -->  00:00:15,030
and then we'll get some outputs.
10

10

00:00:15,030  -->  00:00:16,220
In our case,
11

11

00:00:16,220  -->  00:00:20,520
as inputs will get the ratings of lots and lots of users
12

12

00:00:20,520  -->  00:00:22,200
for these six movies,
13

13

00:00:22,200  -->  00:00:24,610
and based on those ratings it'll come up
14

14

00:00:24,610  -->  00:00:27,510
with a way to compress that information
15

15

00:00:27,510  -->  00:00:30,365
and come up with those weights that will allow it to encode
16

16

00:00:30,365  -->  00:00:33,650
or to basically set itself up
17

17

00:00:33,650  -->  00:00:36,100
to be able to then encode it and decode it
18

18

00:00:36,100  -->  00:00:37,220
in the future.
19

19

00:00:37,220  -->  00:00:40,130
So here we go, these are our training steps.
20

20

00:00:40,130  -->  00:00:42,400
Step one, we need to start with,
21

21

00:00:42,400  -->  00:00:44,442
you start with an array where the lines
22

22

00:00:44,442  -->  00:00:47,400
correspond to users and columns correspond to features,
23

23

00:00:47,400  -->  00:00:49,860
the movies, and each cell contains a rating
24

24

00:00:49,860  -->  00:00:50,710
from one to five,
25

25

00:00:50,710  -->  00:00:53,600
zero if there's no rating by that user u.
26

26

00:00:53,600  -->  00:00:55,000
So, important to understand that
27

27

00:00:55,000  -->  00:00:57,090
every single row in your data set
28

28

00:00:57,090  -->  00:01:00,710
is a unique user who rated those movies.
29

29

00:01:00,710  -->  00:01:03,700
And by the way, these steps will be very valuable for you
30

30

00:01:03,700  -->  00:01:06,250
when you are performing them
31

31

00:01:06,250  -->  00:01:08,070
or when you're doing practical tutorials,
32

32

00:01:08,070  -->  00:01:09,940
because you will be actually working with
33

33

00:01:09,940  -->  00:01:11,520
exactly this situation,
34

34

00:01:11,520  -->  00:01:15,930
you'll be creating a recommender system for movies.
35

35

00:01:15,930  -->  00:01:18,530
Step two, the first user goes into the network.
36

36

00:01:18,530  -->  00:01:21,350
The input vector contains all its ratings
37

37

00:01:21,350  -->  00:01:22,900
for all the movies.
38

38

00:01:22,900  -->  00:01:27,000
So all the ratings of that user for all of the movies.
39

39

00:01:27,000  -->  00:01:29,150
Step three, the input vector is encoded
40

40

00:01:29,150  -->  00:01:30,979
into vector z of lower dimensions
41

41

00:01:30,979  -->  00:01:33,020
by mapping function,
42

42

00:01:33,020  -->  00:01:34,880
usually you'd use a sigmoid or
43

43

00:01:34,880  -->  00:01:37,460
a hyperbolic tangent as we mentioned.
44

44

00:01:37,460  -->  00:01:40,340
And that's the equation that you would have.
45

45

00:01:40,340  -->  00:01:43,870
Then, step four, z is then decoded into the output
46

46

00:01:43,870  -->  00:01:46,150
vector of the same dimension as x aiming to
47

47

00:01:46,150  -->  00:01:47,300
replicate the input vector.
48

48

00:01:47,300  -->  00:01:49,240
So, this is all pretty straight forward stuff.
49

49

00:01:49,240  -->  00:01:51,940
The reconstruction error, which is how
50

50

00:01:51,940  -->  00:01:53,860
different is the output compared to the input
51

51

00:01:53,860  -->  00:01:56,130
is computed and the goal is to minimize
52

52

00:01:56,130  -->  00:01:57,690
your reconstruction error.
53

53

00:01:57,690  -->  00:02:01,080
And then, all this error is back-propagated from
54

54

00:02:01,080  -->  00:02:02,770
right to left, through your network,
55

55

00:02:02,770  -->  00:02:04,722
and the weights are updated accordingly
56

56

00:02:04,722  -->  00:02:06,980
depending on how much they are responsible
57

57

00:02:06,980  -->  00:02:08,550
for that, so you've got a gradient decent
58

58

00:02:08,550  -->  00:02:09,880
process happening there.
59

59

00:02:09,880  -->  00:02:12,520
And, the learning rate describes how much
60

60

00:02:12,520  -->  00:02:13,728
we update the weight.
61

61

00:02:13,728  -->  00:02:15,934
That's a perimeter you can tune
62

62

00:02:15,934  -->  00:02:19,560
during your, you will be tuning or you can tune
63

63

00:02:19,560  -->  00:02:22,570
during decoding and you will see that
64

64

00:02:22,570  -->  00:02:24,160
in the practical tutorials.
65

65

00:02:24,160  -->  00:02:26,100
And step seven, repeats steps one to six,
66

66

00:02:26,100  -->  00:02:28,160
and update the weights after each observation,
67

67

00:02:28,160  -->  00:02:30,070
that's reinforcement learning if it's done after
68

68

00:02:30,070  -->  00:02:31,730
each observation or if you do it
69

69

00:02:31,730  -->  00:02:33,750
after batches that will be batch learning.
70

70

00:02:33,750  -->  00:02:36,380
And finally, step eight, when the whole training
71

71

00:02:36,380  -->  00:02:38,874
passes through the neural network,
72

72

00:02:38,874  -->  00:02:42,240
that's an epoch, and then just redo more epochs.
73

73

00:02:42,240  -->  00:02:44,140
So, there we go, those are our steps.
74

74

00:02:44,140  -->  00:02:46,110
Let's have a look in a more visual way.
75

75

00:02:46,110  -->  00:02:48,000
So, step one, that's our input.
76

76

00:02:48,000  -->  00:02:51,410
In this case, this input is the same file as
77

77

00:02:51,410  -->  00:02:53,880
we used for the Bolster machine, or same example,
78

78

00:02:53,880  -->  00:02:58,050
you can have ratings from one to five, as we
79

79

00:02:58,050  -->  00:03:00,270
mentioned in the example, one to five means
80

80

00:03:00,270  -->  00:03:03,160
basically, a more advanced reccomender system,
81

81

00:03:03,160  -->  00:03:05,030
and then a zero would mean empty.
82

82

00:03:05,030  -->  00:03:07,500
In our case we've got one zero and empty,
83

83

00:03:07,500  -->  00:03:09,990
we can easily change them, we can change these.
84

84

00:03:09,990  -->  00:03:12,740
One minus one and put a zero in the empty cell.
85

85

00:03:12,740  -->  00:03:15,380
So, it's very easy to adjust and also you can
86

86

00:03:15,380  -->  00:03:18,840
build a recommender system with minor ratings,
87

87

00:03:18,840  -->  00:03:21,450
one and zero, or one minus one,
88

88

00:03:21,450  -->  00:03:22,890
depending on how you wanna structure it,
89

89

00:03:22,890  -->  00:03:24,510
it'll be a good reccomender system,
90

90

00:03:24,510  -->  00:03:26,080
but of course, it will be a more powerful
91

91

00:03:26,080  -->  00:03:27,800
recommender system if you build it with
92

92

00:03:27,800  -->  00:03:30,020
ratings from one to five.
93

93

00:03:30,020  -->  00:03:32,300
In the practical tutorials, good news, you will
94

94

00:03:32,300  -->  00:03:35,170
be building one with ratings from one to five.
95

95

00:03:35,170  -->  00:03:36,360
Okay, so there we go.
96

96

00:03:36,360  -->  00:03:39,660
That's our input, then,
97

97

00:03:39,660  -->  00:03:43,660
step two, we take the first row, we put it into
98

98

00:03:43,660  -->  00:03:46,621
our Auto Encoder, and then step three,
99

99

00:03:46,621  -->  00:03:50,470
we calculate the hidden nodes, voila.
100

100

00:03:50,470  -->  00:03:52,530
Again, at the very start you'll have some randomly
101

101

00:03:52,530  -->  00:03:56,770
initialized weights, then that process called encoding.
102

102

00:03:56,770  -->  00:03:59,150
Step four, we're going to calculate our visible
103

103

00:03:59,150  -->  00:04:01,525
output nodes, again initially it will be
104

104

00:04:01,525  -->  00:04:04,280
some random starting weights.
105

105

00:04:04,280  -->  00:04:06,300
That's a process called decoding.
106

106

00:04:06,300  -->  00:04:08,680
And then, so basically, this is a summary
107

107

00:04:08,680  -->  00:04:09,890
that yellow arrow is a summary.
108

108

00:04:09,890  -->  00:04:12,720
We've just put the information, the data through
109

109

00:04:12,720  -->  00:04:14,630
our Auto Encoder from left to right.
110

110

00:04:14,630  -->  00:04:16,840
And step five, we're going to compare
111

111

00:04:16,840  -->  00:04:19,800
the results to the actual ratings for those movies.
112

112

00:04:19,800  -->  00:04:21,760
Movie one, two, three, four, five, six.
113

113

00:04:21,760  -->  00:04:24,345
And then we'll calculate the error, step six.
114

114

00:04:24,345  -->  00:04:26,280
And then we will propagate it back
115

115

00:04:26,280  -->  00:04:29,920
through the network, we'll adjust the weights accordingly.
116

116

00:04:29,920  -->  00:04:33,750
And, next we will take the next row in our data set.
117

117

00:04:33,750  -->  00:04:36,050
And we'll do the same thing and so on
118

118

00:04:36,050  -->  00:04:37,080
and so on and so on we'll continue
119

119

00:04:37,080  -->  00:04:38,320
through all the rows.
120

120

00:04:38,320  -->  00:04:39,950
And then once we're done, we'll move on to
121

121

00:04:39,950  -->  00:04:42,510
step eight, that means we've finished a whole epoch.
122

122

00:04:42,510  -->  00:04:43,850
And then we just repeat these epochs,
123

123

00:04:43,850  -->  00:04:45,760
we continue doing that with our data set
124

124

00:04:45,760  -->  00:04:47,810
depending on how many times we wanna do that.
125

125

00:04:47,810  -->  00:04:51,094
Hopefully, our training converges to some sort of value.
126

126

00:04:51,094  -->  00:04:54,620
There we go, that's how you train an Auto Encoder,
127

127

00:04:54,620  -->  00:04:56,070
very straight forward steps,
128

128

00:04:56,070  -->  00:04:59,190
and as I mentioned at the start of this section
129

129

00:04:59,190  -->  00:05:01,660
that simply because you've already done
130

130

00:05:01,660  -->  00:05:03,302
all of the previous parts to this course,
131

131

00:05:03,302  -->  00:05:06,860
all of this probably sounds very trivial to you.
132

132

00:05:06,860  -->  00:05:11,240
If you look back at how you were starting out
133

133

00:05:11,240  -->  00:05:12,990
into this course this probably would have been
134

134

00:05:12,990  -->  00:05:14,160
a bit more advanced.
135

135

00:05:14,160  -->  00:05:16,570
But, if this sounds trivial that's good news,
136

136

00:05:16,570  -->  00:05:18,410
that just means that all of the
137

137

00:05:18,410  -->  00:05:20,050
knowledge that you've acquired throughout
138

138

00:05:20,050  -->  00:05:23,230
the course before is benefiting you now.
139

139

00:05:23,230  -->  00:05:25,390
And that's great, that's why we are able to
140

140

00:05:25,390  -->  00:05:27,600
fly through this part very quickly.
141

141

00:05:27,600  -->  00:05:30,600
And, we can move onto some more interesting things.
142

142

00:05:30,600  -->  00:05:32,470
And by the way, additional reading,
143

143

00:05:32,470  -->  00:05:34,280
we've got some additional reading today.
144

144

00:05:34,280  -->  00:05:38,210
Here is a blog by Francois Chollet,
145

145

00:05:38,210  -->  00:05:39,690
the creator of Keras,
146

146

00:05:39,690  -->  00:05:41,548
it's called building Autoencoders in Keras,
147

147

00:05:41,548  -->  00:05:43,970
check it out, it's got some actual
148

148

00:05:43,970  -->  00:05:47,050
practical applications so even if you want to get
149

149

00:05:47,050  -->  00:05:49,560
some hands on experience before you start to
150

150

00:05:49,560  -->  00:05:51,880
practice (mumbles), this could be a good place to
151

151

00:05:51,880  -->  00:05:55,740
do that, there is some quick exercises which you can follow
152

152

00:05:55,740  -->  00:05:58,660
and get some cool insights already
153

153

00:05:58,660  -->  00:06:00,150
and then you'll be even more prepared
154

154

00:06:00,150  -->  00:06:02,120
when you go onto the practical tutorials.
155

155

00:06:02,120  -->  00:06:05,940
So check it out, it's called Building Autoencoders in Keras.
156

156

00:06:05,940  -->  00:06:07,290
And that's it for today, I'll look forward
157

157

00:06:07,290  -->  00:06:08,200
to seeing you next time.
158

158

00:06:08,200  -->  00:06:10,123
And until then, enjoy deep learning.
