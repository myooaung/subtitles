1
1

00:00:00,230  -->  00:00:02,940
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02,940  -->  00:00:04,710
So in the previous tutorial we imported
3

3

00:00:04,710  -->  00:00:06,520
the required libraries to build
4

4

00:00:06,520  -->  00:00:08,970
our future artificial neural network.
5

5

00:00:08,970  -->  00:00:10,230
And so now we're gonna start
6

6

00:00:10,230  -->  00:00:12,690
building our deep learning model.
7

7

00:00:12,690  -->  00:00:15,450
And we're gonna build this model in two parts.
8

8

00:00:15,450  -->  00:00:17,590
Part one will be data preprocessing,
9

9

00:00:17,590  -->  00:00:19,030
which I've prepared here.
10

10

00:00:19,030  -->  00:00:23,200
And part two will only be about creating the ANN model.
11

11

00:00:23,200  -->  00:00:24,760
So let's start with part one.
12

12

00:00:24,760  -->  00:00:26,470
And you will understand in this tutorial
13

13

00:00:26,470  -->  00:00:29,550
why it's so important to prepare the data correctly
14

14

00:00:29,550  -->  00:00:32,450
to build a future deep learning model.
15

15

00:00:32,450  -->  00:00:35,100
So, as you understood in the business problem description
16

16

00:00:35,100  -->  00:00:36,510
explained by career.
17

17

00:00:36,510  -->  00:00:39,890
We are in front of a classification problem.
18

18

00:00:39,890  -->  00:00:41,900
So we have some independent variables
19

19

00:00:41,900  -->  00:00:45,030
which are some informations about customers in a bank
20

20

00:00:45,030  -->  00:00:46,360
and we are trying to predict
21

21

00:00:46,360  -->  00:00:48,830
a binary outcome for the dependent variable,
22

22

00:00:48,830  -->  00:00:51,460
which is either one if the customer leaves the bank
23

23

00:00:51,460  -->  00:00:54,060
and zero if the customer stays in the bank.
24

24

00:00:54,060  -->  00:00:55,990
So this is a classification problem.
25

25

00:00:55,990  -->  00:00:58,090
And therefore, I've prepared here
26

26

00:00:58,090  -->  00:00:59,890
our classification template
27

27

00:00:59,890  -->  00:01:01,620
which is explained in great details
28

28

00:01:01,620  -->  00:01:04,510
in the last annex section at the end of this course
29

29

00:01:04,510  -->  00:01:06,390
get the machine learning basics.
30

30

00:01:06,390  -->  00:01:07,950
So basically, what we're gonna do now
31

31

00:01:07,950  -->  00:01:10,280
is take everything in this template
32

32

00:01:10,280  -->  00:01:12,230
except the visualizing parts
33

33

00:01:12,230  -->  00:01:14,790
because we know we have many independent variables.
34

34

00:01:14,790  -->  00:01:17,240
We have more than 10 independent variables.
35

35

00:01:17,240  -->  00:01:21,140
So we take everything from importing the libraries to here
36

36

00:01:21,140  -->  00:01:24,070
because in the end we'll have a look at the confusion matrix
37

37

00:01:24,070  -->  00:01:27,350
to evaluate the performance of our ANN model.
38

38

00:01:27,350  -->  00:01:29,160
So we will take this whole section
39

39

00:01:29,160  -->  00:01:34,160
and go back to our ANN model and paste everything here.
40

40

00:01:34,330  -->  00:01:36,870
All right, so let's handle
41

41

00:01:36,870  -->  00:01:39,400
each of the sections here one by one.
42

42

00:01:39,400  -->  00:01:41,930
Let's start with the first one, importing the libraries.
43

43

00:01:41,930  -->  00:01:45,040
So let's take this code section here and execute.
44

44

00:01:45,040  -->  00:01:45,873
Here we go.
45

45

00:01:45,873  -->  00:01:47,210
Libraries well imported.
46

46

00:01:47,210  -->  00:01:51,295
Great, now second code section, importing the dataset.
47

47

00:01:51,295  -->  00:01:52,565
So let's do it.
48

48

00:01:52,565  -->  00:01:53,667
The data set is not social_Networks_Ads.csv.
49

49

00:01:56,207  -->  00:01:57,040
It is now
50

50

00:01:57,040  -->  00:02:02,040
Churn_Modelling.csv
51

51

00:02:02,110  -->  00:02:03,030
All right, perfect.
52

52

00:02:03,030  -->  00:02:04,143
So let's import it.
53

53

00:02:05,780  -->  00:02:06,850
Great, well imported.
54

54

00:02:06,850  -->  00:02:08,780
Now let's go to variable explorer
55

55

00:02:08,780  -->  00:02:11,143
to have a quick look, here it is.
56

56

00:02:12,640  -->  00:02:14,350
That's our data set.
57

57

00:02:14,350  -->  00:02:16,590
So quick reminder, this data set
58

58

00:02:16,590  -->  00:02:18,600
are informations of customers in a bank.
59

59

00:02:18,600  -->  00:02:20,590
The bank gathered these informations
60

60

00:02:20,590  -->  00:02:23,770
and at the same time observed if the customer stayed
61

61

00:02:23,770  -->  00:02:27,070
or left the bank within a six month period of time.
62

62

00:02:27,070  -->  00:02:30,270
And so the key thing to understand here is that
63

63

00:02:30,270  -->  00:02:33,510
all these variables here are independent variables
64

64

00:02:33,510  -->  00:02:36,900
and the last column here, exited is our dependent variable.
65

65

00:02:36,900  -->  00:02:39,840
And so, we are trying to predict the results
66

66

00:02:39,840  -->  00:02:42,100
of this exited dependent variable
67

67

00:02:42,100  -->  00:02:45,050
from the informations of these independent variables.
68

68

00:02:45,050  -->  00:02:46,310
And we're going to do that
69

69

00:02:46,310  -->  00:02:48,610
within our artificial neural network.
70

70

00:02:48,610  -->  00:02:50,360
All right, so let's press okay.
71

71

00:02:50,360  -->  00:02:52,370
And now the next thing that we have to do
72

72

00:02:52,370  -->  00:02:55,550
is to create the matrix X of features
73

73

00:02:55,550  -->  00:02:58,840
and to do that we need to input the right indexes here.
74

74

00:02:58,840  -->  00:03:01,010
So to have an idea of what these indexes are
75

75

00:03:01,010  -->  00:03:03,750
we need to go back to our data set and see
76

76

00:03:03,750  -->  00:03:05,500
which independent variables
77

77

00:03:05,500  -->  00:03:08,140
we want to include in our matrix of features
78

78

00:03:08,140  -->  00:03:10,560
because our matrix of features is nothing else
79

79

00:03:10,560  -->  00:03:13,710
than a matrix containing the independent variables.
80

80

00:03:13,710  -->  00:03:15,960
All right, so let's do it.
81

81

00:03:15,960  -->  00:03:17,680
We have all our variables here
82

82

00:03:17,680  -->  00:03:19,630
and let's look at them one by one.
83

83

00:03:19,630  -->  00:03:21,600
First column is row number,
84

84

00:03:21,600  -->  00:03:24,860
row number has of course, no impact on the decision
85

85

00:03:24,860  -->  00:03:27,280
of the customer to leave or stay in the bank.
86

86

00:03:27,280  -->  00:03:31,520
So we won't include this variable in our matrix of features.
87

87

00:03:31,520  -->  00:03:34,090
Second variable, customer ID same.
88

88

00:03:34,090  -->  00:03:37,290
Absolutely no impact on the dependent variable.
89

89

00:03:37,290  -->  00:03:40,840
All right, third column, surname well that's the same.
90

90

00:03:40,840  -->  00:03:42,880
It's not because your name is Andrews
91

91

00:03:42,880  -->  00:03:44,810
that you have more chance to leave the bank
92

92

00:03:44,810  -->  00:03:47,230
than if your name is McDonald.
93

93

00:03:47,230  -->  00:03:49,130
All right, that has absolutely no impact
94

94

00:03:49,130  -->  00:03:51,040
on the dependent variable and therefore,
95

95

00:03:51,040  -->  00:03:54,360
we won't include the surname variable either.
96

96

00:03:54,360  -->  00:03:56,370
Now next variable, credit score.
97

97

00:03:56,370  -->  00:03:57,960
Well, credit score, yes.
98

98

00:03:57,960  -->  00:03:59,520
The credit score is very likely
99

99

00:03:59,520  -->  00:04:01,830
to have an impact on the decision of the customer
100

100

00:04:01,830  -->  00:04:04,170
to stay in the bank or leave the bank.
101

101

00:04:04,170  -->  00:04:06,230
If we think about it, we might expect
102

102

00:04:06,230  -->  00:04:09,070
that customers with a low credit score are more likely
103

103

00:04:09,070  -->  00:04:12,720
to leave the bank than customers with a high credit score.
104

104

00:04:12,720  -->  00:04:15,380
So definitely, that's the first independent variable
105

105

00:04:15,380  -->  00:04:17,940
that we will include in our matrix of features.
106

106

00:04:17,940  -->  00:04:20,740
All right, then geography, geography, yes.
107

107

00:04:20,740  -->  00:04:23,150
Perhaps customers in France are more likely
108

108

00:04:23,150  -->  00:04:25,630
to leave the bank than customers in Germany.
109

109

00:04:25,630  -->  00:04:27,340
So we'll include that as well.
110

110

00:04:27,340  -->  00:04:28,890
Gender, yes, same.
111

111

00:04:28,890  -->  00:04:31,750
Perhaps men are more likely to leave the bank than women.
112

112

00:04:31,750  -->  00:04:34,260
We never know, we have to check it out.
113

113

00:04:34,260  -->  00:04:35,400
Then age.
114

114

00:04:35,400  -->  00:04:37,660
Age, well, of course, young people might be
115

115

00:04:37,660  -->  00:04:40,040
more likely to leave the bank than older people
116

116

00:04:40,040  -->  00:04:42,310
that not only have more stability,
117

117

00:04:42,310  -->  00:04:43,690
but also might have been in the bank
118

118

00:04:43,690  -->  00:04:45,100
for a longer period of time
119

119

00:04:45,100  -->  00:04:47,300
and therefore, are more likely to stay.
120

120

00:04:47,300  -->  00:04:48,600
All right, and tenure.
121

121

00:04:48,600  -->  00:04:51,100
Tenure is how long the customer has been in the bank.
122

122

00:04:51,100  -->  00:04:52,730
Of course, this might have an impact
123

123

00:04:52,730  -->  00:04:55,470
on the decision to stay or leave the bank
124

124

00:04:55,470  -->  00:04:57,800
so we include that as well.
125

125

00:04:57,800  -->  00:04:59,070
Balance, same.
126

126

00:04:59,070  -->  00:05:02,310
Of course, the customer with a zero balance has more chance
127

127

00:05:02,310  -->  00:05:05,840
to leave the bank than a customer with a high balance.
128

128

00:05:05,840  -->  00:05:07,590
Number of products, we never know
129

129

00:05:07,590  -->  00:05:08,740
it might have an impact
130

130

00:05:08,740  -->  00:05:10,070
we'll include that as well.
131

131

00:05:10,070  -->  00:05:12,050
Has credit card, yes, maybe.
132

132

00:05:12,050  -->  00:05:14,100
We might expect that customers
133

133

00:05:14,100  -->  00:05:16,830
with a credit card are more likely to stay in the bank
134

134

00:05:16,830  -->  00:05:19,180
than customers who don't have a credit card.
135

135

00:05:19,180  -->  00:05:20,940
So we have to check it out as well.
136

136

00:05:20,940  -->  00:05:24,040
Is active member, well, same logic as here.
137

137

00:05:24,040  -->  00:05:25,530
If a customer is active,
138

138

00:05:25,530  -->  00:05:28,360
then he might be more likely to stay in the bank.
139

139

00:05:28,360  -->  00:05:30,920
And eventually, estimated salary,
140

140

00:05:30,920  -->  00:05:33,270
well, that's the same logic as balance,
141

141

00:05:33,270  -->  00:05:35,760
customers with a high estimated salary
142

142

00:05:35,760  -->  00:05:37,490
might be more likely to stay in the bank
143

143

00:05:37,490  -->  00:05:40,720
than customers with low estimated salaries.
144

144

00:05:40,720  -->  00:05:43,160
So with our own logic, we are able to say
145

145

00:05:43,160  -->  00:05:46,040
which independent variables might have impact
146

146

00:05:46,040  -->  00:05:49,100
on the dependent variable but what we don't know is
147

147

00:05:49,100  -->  00:05:51,940
which independent variable has the most impact
148

148

00:05:51,940  -->  00:05:53,120
on the dependent variable.
149

149

00:05:53,120  -->  00:05:56,640
And that's what our artificial neural network will spot
150

150

00:05:56,640  -->  00:05:58,260
and we'll perfectly see by looking
151

151

00:05:58,260  -->  00:06:00,290
at the correlations here and therefore,
152

152

00:06:00,290  -->  00:06:02,980
it will give the bigger weight in the neural network
153

153

00:06:02,980  -->  00:06:05,930
to those independent variables that have the most impact.
154

154

00:06:05,930  -->  00:06:08,390
All right, so now we know which independent variables
155

155

00:06:08,390  -->  00:06:10,550
to include in our matrix of features.
156

156

00:06:10,550  -->  00:06:13,750
That's all the independent variables from credit score
157

157

00:06:13,750  -->  00:06:15,750
to the last one, estimated salary.
158

158

00:06:15,750  -->  00:06:18,340
And so let's see what the indexes
159

159

00:06:18,340  -->  00:06:20,460
of these independent variables are.
160

160

00:06:20,460  -->  00:06:22,900
So let's see indexes and bison start at zero,
161

161

00:06:22,900  -->  00:06:25,910
so that's zero here, one, two, three.
162

162

00:06:25,910  -->  00:06:28,730
Okay, so we'll go from three to three, four,
163

163

00:06:28,730  -->  00:06:32,753
five, six, seven, eight, nine, 10, 11 and 12.
164

164

00:06:32,753  -->  00:06:36,840
All right, so we must include the indexes from three to 12.
165

165

00:06:36,840  -->  00:06:38,040
So let's do it.
166

166

00:06:38,040  -->  00:06:39,440
Let's press okay here.
167

167

00:06:39,440  -->  00:06:43,400
All right, so we need to take the indexes from three to 12.
168

168

00:06:43,400  -->  00:06:46,670
This takes the indexes two and three.
169

169

00:06:46,670  -->  00:06:49,640
And so to take indexes from three to 12,
170

170

00:06:49,640  -->  00:06:53,860
we need to add the lower bound here, index three colon
171

171

00:06:53,860  -->  00:06:55,980
and then the upper bound, but remember,
172

172

00:06:55,980  -->  00:06:58,350
the upper bound is excluded in a range.
173

173

00:06:58,350  -->  00:07:01,660
So instead of putting 12 we need to put 13.
174

174

00:07:01,660  -->  00:07:04,610
And that takes all the indexes from three to 12.
175

175

00:07:04,610  -->  00:07:05,443
All right, good.
176

176

00:07:05,443  -->  00:07:08,150
So now we're ready to create our matrix of features x.
177

177

00:07:08,150  -->  00:07:11,780
And so let's select this line and execute.
178

178

00:07:11,780  -->  00:07:14,530
Great, matrix of features x created.
179

179

00:07:14,530  -->  00:07:16,600
All right, and now let's create
180

180

00:07:16,600  -->  00:07:18,140
our dependent variable vector.
181

181

00:07:18,140  -->  00:07:19,320
So that's very easy.
182

182

00:07:19,320  -->  00:07:22,020
You know, remember our independent variables are taking
183

183

00:07:22,020  -->  00:07:24,760
indexes from three to 12 and therefore,
184

184

00:07:24,760  -->  00:07:27,780
the index of our dependent variable is index number 13.
185

185

00:07:27,780  -->  00:07:32,040
And therefore, instead of four here we input 13.
186

186

00:07:32,040  -->  00:07:33,800
AlL right, so we are ready to build
187

187

00:07:33,800  -->  00:07:34,860
a dependent variable vectors.
188

188

00:07:34,860  -->  00:07:37,760
So let's select this line and execute.
189

189

00:07:37,760  -->  00:07:38,593
All right, here we go.
190

190

00:07:38,593  -->  00:07:39,740
We can have a look.
191

191

00:07:39,740  -->  00:07:43,650
Y contains the binary outcomes, zero or one
192

192

00:07:45,050  -->  00:07:48,950
for all the 10,000 customers of the sample of this bank.
193

193

00:07:48,950  -->  00:07:50,490
All right, so let's press okay.
194

194

00:07:50,490  -->  00:07:52,790
We're done with importing the data set.
195

195

00:07:52,790  -->  00:07:54,680
Everything is well prepared.
196

196

00:07:54,680  -->  00:07:56,780
Now let's move on to the next code section.
197

197

00:07:56,780  -->  00:07:59,480
The next code section is about splitting the dataset
198

198

00:07:59,480  -->  00:08:01,800
into the training set and the test set.
199

199

00:08:01,800  -->  00:08:04,740
But before we do that, we must pay attention to something.
200

200

00:08:04,740  -->  00:08:08,270
We must fix something before splitting the dataset
201

201

00:08:08,270  -->  00:08:10,130
to the training set and the test set.
202

202

00:08:10,130  -->  00:08:11,530
Can you guess what it is?
203

203

00:08:11,530  -->  00:08:13,670
Well, it's, of course, related to the fact that
204

204

00:08:13,670  -->  00:08:15,630
we have some categorical variables
205

205

00:08:15,630  -->  00:08:17,220
in our matrix of features.
206

206

00:08:17,220  -->  00:08:20,277
And therefore, we need to encode them exactly as we did.
207

207

00:08:20,277  -->  00:08:23,150
And in the next section at the end of this course,
208

208

00:08:23,150  -->  00:08:25,050
get the machine learning basics.
209

209

00:08:25,050  -->  00:08:26,840
And that is why, I've also prepared
210

210

00:08:26,840  -->  00:08:30,010
the categorical data that PY file right here.
211

211

00:08:30,010  -->  00:08:31,340
And therefore, in this file,
212

212

00:08:31,340  -->  00:08:33,490
we will take this code section here
213

213

00:08:33,490  -->  00:08:36,380
that encodes any categorical data
214

214

00:08:36,380  -->  00:08:37,810
that you have in your dataset.
215

215

00:08:37,810  -->  00:08:41,840
So by the way, our dependent variable is categorical,
216

216

00:08:41,840  -->  00:08:44,430
but it takes values zero or one.
217

217

00:08:44,430  -->  00:08:45,820
So it takes numerical values.
218

218

00:08:45,820  -->  00:08:48,110
So we don't need to apply label encoder
219

219

00:08:48,110  -->  00:08:51,610
to, you know, encode text into numbers.
220

220

00:08:51,610  -->  00:08:53,550
But we have our independent variables
221

221

00:08:53,550  -->  00:08:56,180
which have categories that are strings and therefore,
222

222

00:08:56,180  -->  00:08:57,900
we need to take this code section here
223

223

00:08:57,900  -->  00:09:01,440
that will encode the categorical independent variables.
224

224

00:09:01,440  -->  00:09:03,110
All right, so I'm taking this,
225

225

00:09:03,110  -->  00:09:06,330
copy and going back to my ANN file
226

226

00:09:06,330  -->  00:09:10,090
and we will encode the categorical data before splitting
227

227

00:09:10,090  -->  00:09:12,040
the dataset into the training set and the test set.
228

228

00:09:12,040  -->  00:09:13,860
Because basically, when you do this split
229

229

00:09:13,860  -->  00:09:15,410
your matrix of features x
230

230

00:09:15,410  -->  00:09:18,600
and your dependent variable y must be already encoded.
231

231

00:09:18,600  -->  00:09:20,800
All right, so I'm pasting that here.
232

232

00:09:20,800  -->  00:09:21,633
Here we go.
233

233

00:09:21,633  -->  00:09:22,710
I'm going to remove that.
234

234

00:09:22,710  -->  00:09:23,960
That was just to make the distinction
235

235

00:09:23,960  -->  00:09:26,910
between independent variable and dependent variable.
236

236

00:09:26,910  -->  00:09:28,150
And now we will encode
237

237

00:09:28,150  -->  00:09:30,470
our categorical independent variables.
238

238

00:09:30,470  -->  00:09:31,690
So let's see what they are.
239

239

00:09:31,690  -->  00:09:32,600
We will have a look
240

240

00:09:32,600  -->  00:09:35,980
at our matrix of features from the console.
241

241

00:09:35,980  -->  00:09:39,370
So I'm pressing X here and let's have a look.
242

242

00:09:39,370  -->  00:09:42,700
All right, so these are all our independent variables.
243

243

00:09:42,700  -->  00:09:44,960
We can very clearly and quickly see here
244

244

00:09:44,960  -->  00:09:48,530
that we only have two categorical independent variables
245

245

00:09:48,530  -->  00:09:52,730
which are the country which contains three categories
246

246

00:09:52,730  -->  00:09:56,560
France, Spain, and Germany and the gender variable
247

247

00:09:56,560  -->  00:09:59,830
which contains two categories female and male.
248

248

00:09:59,830  -->  00:10:01,600
So these are the only two variables
249

249

00:10:01,600  -->  00:10:04,910
we need to encode in our matrix of features.
250

250

00:10:04,910  -->  00:10:05,743
So let's do it.
251

251

00:10:05,743  -->  00:10:08,060
Let's take care of the first one, the country.
252

252

00:10:08,060  -->  00:10:11,070
All right so, since we only need to encode two variables,
253

253

00:10:11,070  -->  00:10:14,630
we will need to create two label encoder object here.
254

254

00:10:14,630  -->  00:10:17,953
And therefore, we'll call the first one, labelencoder_x_1.
255

255

00:10:20,000  -->  00:10:22,770
All right, so here we create the object.
256

256

00:10:22,770  -->  00:10:24,300
And then in the next line, we apply
257

257

00:10:24,300  -->  00:10:28,200
the fit_transform method to encode this variable.
258

258

00:10:28,200  -->  00:10:30,350
That is, it will convert the strings here,
259

259

00:10:30,350  -->  00:10:34,740
France, Spain and Germany into numbers zero, one and two.
260

260

00:10:34,740  -->  00:10:36,650
So that's the first step of the encoding.
261

261

00:10:36,650  -->  00:10:37,640
So let's do it.
262

262

00:10:37,640  -->  00:10:40,580
We need to put the right index here,
263

263

00:10:40,580  -->  00:10:43,190
which is not zero, but of course one
264

264

00:10:43,190  -->  00:10:45,450
because indexes in Python start at zero.
265

265

00:10:45,450  -->  00:10:47,250
So that's index zero and that's index one.
266

266

00:10:47,250  -->  00:10:48,660
That's the index we want.
267

267

00:10:48,660  -->  00:10:49,493
All right, great.
268

268

00:10:49,493  -->  00:10:52,860
And same here we replace zero by one.
269

269

00:10:52,860  -->  00:10:53,693
All right, good.
270

270

00:10:53,693  -->  00:10:56,750
Let's not forget to add this underscore one here
271

271

00:10:56,750  -->  00:10:59,350
to take this object created here.
272

272

00:10:59,350  -->  00:11:03,830
And so I'm going to select these three lines here.
273

273

00:11:03,830  -->  00:11:06,930
And this will convert the strings into three numbers.
274

274

00:11:06,930  -->  00:11:08,400
All right, so well executed.
275

275

00:11:08,400  -->  00:11:12,160
Now let's have a look at x, here we go and that's it.
276

276

00:11:12,160  -->  00:11:14,840
So as you can see, France became zero,
277

277

00:11:14,840  -->  00:11:18,630
Spain became two and Germany became one.
278

278

00:11:18,630  -->  00:11:19,920
All right and now let's take care
279

279

00:11:19,920  -->  00:11:22,850
of the second categorical variable, the gender variable.
280

280

00:11:22,850  -->  00:11:25,270
So we will do exactly the same as we did here,
281

281

00:11:25,270  -->  00:11:27,940
but only with a new object.
282

282

00:11:27,940  -->  00:11:32,940
So I'm going to copy this, copy and paste it here.
283

283

00:11:33,110  -->  00:11:35,470
And here, I will not only change
284

284

00:11:35,470  -->  00:11:37,800
the name of the labelencoder object,
285

285

00:11:37,800  -->  00:11:42,370
which I will replace by labelencoder_x_2, right?
286

286

00:11:42,370  -->  00:11:45,510
And let's not forget to replace it here as well.
287

287

00:11:45,510  -->  00:11:48,240
And of course, we need to change the index here
288

288

00:11:48,240  -->  00:11:49,720
which corresponds to the index
289

289

00:11:49,720  -->  00:11:52,420
of the independent variable we want to encode
290

290

00:11:52,420  -->  00:11:53,530
and therefore, that's, of course,
291

291

00:11:53,530  -->  00:11:56,600
the index number two, zero, one and two.
292

292

00:11:56,600  -->  00:12:01,590
All right, so we replace one by two here and same as well.
293

293

00:12:01,590  -->  00:12:02,800
All right, and that's done.
294

294

00:12:02,800  -->  00:12:06,620
We just need to select this and execute.
295

295

00:12:06,620  -->  00:12:10,380
All right, now let's have a look at x, x here.
296

296

00:12:10,380  -->  00:12:12,500
Here we go and as you can see,
297

297

00:12:12,500  -->  00:12:16,090
female became zero and male became one.
298

298

00:12:16,090  -->  00:12:18,530
I hope the ladies of this course won't be offended.
299

299

00:12:18,530  -->  00:12:20,650
But that's pure random.
300

300

00:12:20,650  -->  00:12:22,690
All right, so now we need to do one more thing,
301

301

00:12:22,690  -->  00:12:24,880
which is, of course, related to the fact
302

302

00:12:24,880  -->  00:12:28,360
that our categorical variables are not ordinal.
303

303

00:12:28,360  -->  00:12:30,580
That means that there is no relational order
304

304

00:12:30,580  -->  00:12:33,470
between the categories of our categorical variables.
305

305

00:12:33,470  -->  00:12:35,710
That is, you know, France is not higher than Germany
306

306

00:12:35,710  -->  00:12:38,100
and Germany isn't higher than France, and same
307

307

00:12:38,100  -->  00:12:41,340
between Spain and Germany and Spain and France.
308

308

00:12:41,340  -->  00:12:43,140
All right, so exactly as we did in part one,
309

309

00:12:43,140  -->  00:12:45,220
we need to create dummy variables
310

310

00:12:45,220  -->  00:12:46,980
for this categorical variable.
311

311

00:12:46,980  -->  00:12:50,090
And we will only do it for this categorical variable
312

312

00:12:50,090  -->  00:12:53,390
of the country because it contains three categories
313

313

00:12:53,390  -->  00:12:55,600
and this one contains two categories.
314

314

00:12:55,600  -->  00:12:57,550
So since we will then remove one column
315

315

00:12:57,550  -->  00:12:59,740
to avoid the dummy variable trap.
316

316

00:12:59,740  -->  00:13:01,340
Well, it will be no use to create
317

317

00:13:01,340  -->  00:13:02,910
dummy variables for this one.
318

318

00:13:02,910  -->  00:13:04,440
So let's just do it for this one.
319

319

00:13:04,440  -->  00:13:06,610
And we already have everything prepared here
320

320

00:13:06,610  -->  00:13:08,300
with the OneHotEncoder class
321

321

00:13:08,300  -->  00:13:11,190
that will create these semi variables.
322

322

00:13:11,190  -->  00:13:13,090
We don't even need to change the index
323

323

00:13:13,090  -->  00:13:15,080
because that corresponds to the right index
324

324

00:13:15,080  -->  00:13:18,440
of the column we want to create the dummy variables for.
325

325

00:13:18,440  -->  00:13:20,180
Indeed, that is index one.
326

326

00:13:20,180  -->  00:13:21,970
All right, so basically, we're ready
327

327

00:13:21,970  -->  00:13:24,640
and we can just select that here
328

328

00:13:24,640  -->  00:13:27,910
to create the dummy variables.
329

329

00:13:27,910  -->  00:13:30,370
All right, done, well executed.
330

330

00:13:30,370  -->  00:13:32,070
Now we can have a look at x.
331

331

00:13:32,070  -->  00:13:35,470
And actually, since now all the columns have the same type.
332

332

00:13:35,470  -->  00:13:37,120
We can have a look at x from here
333

333

00:13:37,120  -->  00:13:41,110
because the type is no longer an object, but float 64.
334

334

00:13:41,110  -->  00:13:42,360
So let's have a look.
335

335

00:13:42,360  -->  00:13:45,580
And we can clearly see all the independent variables
336

336

00:13:45,580  -->  00:13:48,350
of our matrix of features.
337

337

00:13:48,350  -->  00:13:50,780
All right, so we now have 12 independent variables
338

338

00:13:50,780  -->  00:13:53,070
because we have our three new dummy variables.
339

339

00:13:53,070  -->  00:13:57,580
I'm gonna change the format very quickly, zero, okay.
340

340

00:13:57,580  -->  00:13:59,450
Alright, so these are our three dummy variables
341

341

00:13:59,450  -->  00:14:00,980
corresponding to the country.
342

342

00:14:00,980  -->  00:14:02,850
And then here we have the credit score.
343

343

00:14:02,850  -->  00:14:04,760
Then here, the categorical variable
344

344

00:14:04,760  -->  00:14:06,210
corresponding to the gender,
345

345

00:14:06,210  -->  00:14:08,280
and then the other independent variables.
346

346

00:14:08,280  -->  00:14:09,180
So that's great.
347

347

00:14:09,180  -->  00:14:12,840
But now we will remove one dummy variable here
348

348

00:14:12,840  -->  00:14:16,030
to avoid falling into the dummy variable trap.
349

349

00:14:16,030  -->  00:14:17,350
All right, so let's do that right now .
350

350

00:14:17,350  -->  00:14:19,420
It's going to be very quick and easy.
351

351

00:14:19,420  -->  00:14:23,460
We take our matrix of features X and we will update it
352

352

00:14:23,460  -->  00:14:26,690
by taking all the lines of this matrix
353

353

00:14:26,690  -->  00:14:30,130
and all the columns except the first one.
354

354

00:14:30,130  -->  00:14:31,850
And to do this, I'm taking all the columns
355

355

00:14:31,850  -->  00:14:35,400
from index one which corresponds to the second column
356

356

00:14:35,400  -->  00:14:37,530
until the last index of the last column.
357

357

00:14:37,530  -->  00:14:39,000
All right, and that will do it.
358

358

00:14:39,000  -->  00:14:41,690
I'm going to select this and execute
359

359

00:14:41,690  -->  00:14:43,580
and now as you can see,
360

360

00:14:43,580  -->  00:14:46,660
we only have two dummy variables for the country
361

361

00:14:46,660  -->  00:14:47,590
which is great.
362

362

00:14:47,590  -->  00:14:49,410
So no more dummy variable trap.
363

363

00:14:49,410  -->  00:14:51,230
All right, so we're good.
364

364

00:14:51,230  -->  00:14:53,950
We are ready to split the dataset
365

365

00:14:53,950  -->  00:14:56,360
into the training set and the test set.
366

366

00:14:56,360  -->  00:14:58,470
So that's exactly what we do in this section.
367

367

00:14:58,470  -->  00:14:59,750
Everything is ready.
368

368

00:14:59,750  -->  00:15:02,150
We will maybe change the test size to 0.2
369

369

00:15:03,120  -->  00:15:06,510
to you know, train the ANN on 8,000 observations
370

370

00:15:06,510  -->  00:15:09,690
and test its performance on 2,000 observations.
371

371

00:15:09,690  -->  00:15:10,820
So let's do it.
372

372

00:15:10,820  -->  00:15:14,790
I'm going to select these three lines here and execute.
373

373

00:15:14,790  -->  00:15:16,350
And here we get something new .
374

374

00:15:16,350  -->  00:15:18,880
We get a deprecation warning that is fine.
375

375

00:15:18,880  -->  00:15:20,900
It's just because spider was a updated
376

376

00:15:20,900  -->  00:15:24,240
to its latest version and in this latest version,
377

377

00:15:24,240  -->  00:15:27,930
well, this cross validation package here
378

378

00:15:27,930  -->  00:15:31,120
was replaced by model selection.
379

379

00:15:31,120  -->  00:15:33,920
So basically, to avoid this warning, this is fine,
380

380

00:15:33,920  -->  00:15:36,880
this perfectly created the training set and the test set,
381

381

00:15:36,880  -->  00:15:39,540
but to avoid the warning, we just need to replace
382

382

00:15:39,540  -->  00:15:42,927
cross_validation here by model_selection.
383

383

00:15:44,940  -->  00:15:49,190
All right, and now if I reselect that
384

384

00:15:49,190  -->  00:15:52,930
and execute as you can see, the warning disappears.
385

385

00:15:52,930  -->  00:15:54,350
All right, so that's good to know.
386

386

00:15:54,350  -->  00:15:57,980
And that can be useful to avoid future errors.
387

387

00:15:57,980  -->  00:16:00,050
All right, and now time for feature scaling.
388

388

00:16:00,050  -->  00:16:01,630
Do we need to apply feature scaling
389

389

00:16:01,630  -->  00:16:05,140
to artificial neural networks and in general, deep learning?
390

390

00:16:05,140  -->  00:16:07,730
Well, the answer is yes, absolutely.
391

391

00:16:07,730  -->  00:16:10,050
That is thoroughly compulsory.
392

392

00:16:10,050  -->  00:16:11,150
And that is because there's going
393

393

00:16:11,150  -->  00:16:13,620
to be a lot of computations, you know,
394

394

00:16:13,620  -->  00:16:15,830
highly compute intensive calculations.
395

395

00:16:15,830  -->  00:16:17,730
And besides parallel computations,
396

396

00:16:17,730  -->  00:16:19,920
so we need to apply feature scaling
397

397

00:16:19,920  -->  00:16:21,810
to ease all these calculations.
398

398

00:16:21,810  -->  00:16:23,300
And besides, we don't wanna have
399

399

00:16:23,300  -->  00:16:25,570
one independent variable dominating another one.
400

400

00:16:25,570  -->  00:16:28,690
So very important, we need to apply feature scaling
401

401

00:16:28,690  -->  00:16:30,490
that's really compulsory.
402

402

00:16:30,490  -->  00:16:31,490
So let's do it.
403

403

00:16:31,490  -->  00:16:32,730
Everything's ready here.
404

404

00:16:32,730  -->  00:16:35,310
I don't need to change anything.
405

405

00:16:35,310  -->  00:16:39,560
So let's execute and all good, no warning,
406

406

00:16:39,560  -->  00:16:40,820
features getting done properly.
407

407

00:16:40,820  -->  00:16:44,110
We can have a quick look at x_train here.
408

408

00:16:44,110  -->  00:16:46,640
As you can see, all the independent variables are scaled
409

409

00:16:46,640  -->  00:16:49,540
and that's the same for the test set.
410

410

00:16:49,540  -->  00:16:52,600
Everything is scaled properly, we are all good.
411

411

00:16:52,600  -->  00:16:55,330
Our data is not well preprocessed.
412

412

00:16:55,330  -->  00:16:58,240
And now I'm super excited because we are about
413

413

00:16:58,240  -->  00:17:02,230
to start building the artificial neural network.
414

414

00:17:02,230  -->  00:17:04,900
This is going to be something, we are now approaching
415

415

00:17:04,900  -->  00:17:07,890
one of the most powerful tools in machine learning
416

416

00:17:07,890  -->  00:17:09,520
that I'm sure will be very useful
417

417

00:17:09,520  -->  00:17:11,700
if you need to apply them for your work.
418

418

00:17:11,700  -->  00:17:14,100
So we're gonna do that in the next tutorial.
419

419

00:17:14,100  -->  00:17:16,353
And until then, enjoy deep learning.
