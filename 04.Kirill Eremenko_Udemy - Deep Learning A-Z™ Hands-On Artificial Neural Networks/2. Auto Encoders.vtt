WEBVTT
1
1

00:00:00.520  -->  00:00:02.050
<v Instructor>Hello, and welcome back</v>
2

2

00:00:02.050  -->  00:00:03.600
to the course on Deep Learning.
3

3

00:00:03.600  -->  00:00:07.340
We're finally at the part on Auto Encoders.
4

4

00:00:07.340  -->  00:00:10.020
We've gone through quite a lot of different models
5

5

00:00:10.020  -->  00:00:11.540
in this course, we've already talked
6

6

00:00:11.540  -->  00:00:13.320
about Artificial Neural Networks,
7

7

00:00:13.320  -->  00:00:15.770
Convolution Neural Networks and Recurrent Neural Networks,
8

8

00:00:15.770  -->  00:00:18.000
and that summarized the Supervised
9

9

00:00:18.000  -->  00:00:20.010
Deep Learning side of things.
10

10

00:00:20.010  -->  00:00:22.720
Now, we are in Unsupervised branch of Deep Learning.
11

11

00:00:22.720  -->  00:00:25.090
And we've already talked about Self-Organizing Maps,
12

12

00:00:25.090  -->  00:00:26.730
Boltzmann Machines and now,
13

13

00:00:26.730  -->  00:00:28.864
we're proceeding Auto Encoders.
14

14

00:00:28.864  -->  00:00:32.980
Congratulations that you've gotten this far in the course,
15

15

00:00:32.980  -->  00:00:34.910
and it's been a very exciting journey,
16

16

00:00:34.910  -->  00:00:37.000
and now we're proceeding to the final stage.
17

17

00:00:37.000  -->  00:00:39.000
So, let's get started.
18

18

00:00:39.000  -->  00:00:41.120
What is an Auto Encoder?
19

19

00:00:41.120  -->  00:00:44.570
Well, this is what an Auto Encoder looks like.
20

20

00:00:44.570  -->  00:00:46.920
Right away by the colors, you can tell the good news
21

21

00:00:46.920  -->  00:00:51.920
that we're back to the directed types of neural networks.
22

22

00:00:52.030  -->  00:00:54.600
And this is a director type neural network.
23

23

00:00:54.600  -->  00:00:56.860
The blue lines don't have arrows on the ends,
24

24

00:00:56.860  -->  00:01:00.670
but we'll just agree that it is a directed type of network
25

25

00:01:00.670  -->  00:01:03.370
and everything's moving from left to right.
26

26

00:01:03.370  -->  00:01:06.670
How does Auto Encoder work, and what's the whole purpose
27

27

00:01:06.670  -->  00:01:07.853
of an Auto Encoder?
28

28

00:01:08.866  -->  00:01:12.069
Well, just as the name suggests, an Auto Encoder
29

29

00:01:12.069  -->  00:01:16.160
encodes itself, that's the purpose of what is doing,
30

30

00:01:16.160  -->  00:01:18.850
or the whole philosophy behind the Auto Encoder.
31

31

00:01:18.850  -->  00:01:21.220
That it takes some sort of inputs,
32

32

00:01:21.220  -->  00:01:22.930
put some through a hidden layer,
33

33

00:01:22.930  -->  00:01:26.960
and then it gets outputs, but it aims for the outputs
34

34

00:01:26.960  -->  00:01:28.890
to be identical to the inputs.
35

35

00:01:28.890  -->  00:01:30.590
That's what it's trying to do,
36

36

00:01:30.590  -->  00:01:34.140
that's how you are going to be training Auto Encoders
37

37

00:01:34.140  -->  00:01:38.960
to set them up in such a way that on the output you get
38

38

00:01:38.960  -->  00:01:42.510
values which are equivalent to your inputs.
39

39

00:01:42.510  -->  00:01:46.230
From that you can tell that Auto Encoders are not a pure
40

40

00:01:46.230  -->  00:01:50.300
type of Unsupervised Deep Learning algorithm.
41

41

00:01:50.300  -->  00:01:54.450
They are actually a self-supervised Deep Learning algorithm,
42

42

00:01:54.450  -->  00:01:57.000
because they are comparing to something on the end.
43

43

00:01:57.000  -->  00:01:59.690
Remember, in Boltzmann Machines we didn't even have outputs.
44

44

00:01:59.690  -->  00:02:03.910
We didn't have to compare to any kind of labels or anything.
45

45

00:02:03.910  -->  00:02:07.350
And in Self-Organizing Maps, the same thing we didn't have
46

46

00:02:07.350  -->  00:02:10.410
anything to compare to, we're just looking for features.
47

47

00:02:10.410  -->  00:02:11.910
We're just looking for things.
48

48

00:02:11.910  -->  00:02:14.430
Whereas here we are looking for things, we're looking
49

49

00:02:14.430  -->  00:02:19.070
for this hidden layer, which is also called the coding layer
50

50

00:02:19.070  -->  00:02:20.130
or the bottleneck.
51

51

00:02:20.130  -->  00:02:23.100
We're looking for how to structure it, but at the same time,
52

52

00:02:23.100  -->  00:02:26.960
we are comparing the outputs to certain values,
53

53

00:02:26.960  -->  00:02:27.793
which are the inputs.
54

54

00:02:27.793  -->  00:02:31.310
So, it's kind of on the verge between supervised
55

55

00:02:31.310  -->  00:02:33.830
and unsupervised, but we'll stick to unsupervised
56

56

00:02:33.830  -->  00:02:36.780
for Auto Encoders, and that's pretty much how it works.
57

57

00:02:36.780  -->  00:02:39.540
So, you have inputs, they get encoded, and then they get
58

58

00:02:39.540  -->  00:02:41.170
decoded and they're compared,
59

59

00:02:41.170  -->  00:02:43.590
and through that the training happens.
60

60

00:02:43.590  -->  00:02:46.467
As you'll see, because of all the things we already know,
61

61

00:02:46.467  -->  00:02:49.960
from the previous parts of this course,
62

62

00:02:49.960  -->  00:02:52.060
it'll be very easy for you to understand Auto Encoders.
63

63

00:02:52.060  -->  00:02:55.160
They're a quite a simple model when you combine
64

64

00:02:55.160  -->  00:02:57.940
all the things that you already know and right away now
65

65

00:02:57.940  -->  00:02:59.870
we just talked about the process,
66

66

00:02:59.870  -->  00:03:01.340
how they compare the output the inputs,
67

67

00:03:01.340  -->  00:03:03.270
and you can already imagine how information
68

68

00:03:03.270  -->  00:03:06.090
is propagated this way, and then you have gradient descent
69

69

00:03:06.090  -->  00:03:06.923
going the other way.
70

70

00:03:06.923  -->  00:03:09.720
We'll talk about all those things, but just be prepared
71

71

00:03:09.720  -->  00:03:12.540
that this part of the course is gonna be pretty easy
72

72

00:03:12.540  -->  00:03:15.170
for you, and you're going to most likely breeze through it,
73

73

00:03:15.170  -->  00:03:17.260
especially if you've been through the other
74

74

00:03:17.260  -->  00:03:18.660
parts of the course already.
75

75

00:03:19.530  -->  00:03:21.100
One more thing on the slide
76

76

00:03:21.100  -->  00:03:23.640
is what are Auto Encoders used for?
77

77

00:03:23.640  -->  00:03:25.910
Well, there's a couple of things that they can be used for.
78

78

00:03:25.910  -->  00:03:27.600
They can be used for feature detection.
79

79

00:03:27.600  -->  00:03:31.300
So, once you've encoded your data, these nodes
80

80

00:03:31.300  -->  00:03:34.358
will represent certain features which are important
81

81

00:03:34.358  -->  00:03:36.410
in your data, and then you can just look at them
82

82

00:03:36.410  -->  00:03:38.330
and get those features out of them,
83

83

00:03:38.330  -->  00:03:41.610
or basically use those features in the future.
84

84

00:03:41.610  -->  00:03:44.200
They can also be used to build powerful recommender systems,
85

85

00:03:44.200  -->  00:03:47.200
as you'll see from the practical tutorials in this course.
86

86

00:03:47.200  -->  00:03:50.010
And they can be used for encoding, basically,
87

87

00:03:50.010  -->  00:03:53.940
as they're designed, they are designed to encode data
88

88

00:03:53.940  -->  00:03:57.080
and you could take data with lots and lots of values,
89

89

00:03:57.080  -->  00:03:59.720
encoded into a smaller representation,
90

90

00:03:59.720  -->  00:04:01.380
and then all you'll have to carry around
91

91

00:04:01.380  -->  00:04:04.610
is this auto encoder or this decoder part
92

92

00:04:04.610  -->  00:04:08.110
and your encoded data, so it would take up less space.
93

93

00:04:08.110  -->  00:04:11.090
That's another use case for Auto Encoders.
94

94

00:04:11.090  -->  00:04:13.440
All right, that was a quick breakdown of the architecture
95

95

00:04:13.440  -->  00:04:16.840
of Auto Encoders, and now let's have have a look
96

96

00:04:16.840  -->  00:04:19.420
at an example of how they actually work,
97

97

00:04:19.420  -->  00:04:22.280
so we can understand them better on an intuitive level.
98

98

00:04:22.280  -->  00:04:25.120
There's a simplified auto encoder,
99

99

00:04:25.120  -->  00:04:26.780
with just four input nodes,
100

100

00:04:26.780  -->  00:04:29.090
and two nodes in the hidden layer.
101

101

00:04:29.090  -->  00:04:32.750
As we can see, we've got four movies over here
102

102

00:04:32.750  -->  00:04:34.070
and four movies right here.
103

103

00:04:34.070  -->  00:04:35.000
And what are these movies?
104

104

00:04:35.000  -->  00:04:37.660
Well, these are just movies that a person has watched,
105

105

00:04:37.660  -->  00:04:41.940
and we're going to be encoding the rating
106

106

00:04:41.940  -->  00:04:43.900
or people have left for those movies.
107

107

00:04:43.900  -->  00:04:46.903
One means a person liked that movie, and zero means a person
108

108

00:04:46.903  -->  00:04:49.128
didn't like that movie.
109

109

00:04:49.128  -->  00:04:52.570
That's how it basically works, and now let's have a look
110

110

00:04:52.570  -->  00:04:55.470
at how this information can be encoded through
111

111

00:04:55.470  -->  00:04:56.800
the auto encoder.
112

112

00:04:56.800  -->  00:04:58.880
Before we start, I just wanted to say that this example
113

113

00:04:58.880  -->  00:05:02.131
actually comes from this blog, "PROBABLY DANCE".
114

114

00:05:02.131  -->  00:05:05.350
It's a great blog, it's by a person who's actually
115

115

00:05:05.350  -->  00:05:09.560
a programmer who isn't a Deep Learning scientist,
116

116

00:05:09.560  -->  00:05:13.800
as I understand but he really broke it down into good steps.
117

117

00:05:13.800  -->  00:05:16.600
We'll link to this at the end as additional reading,
118

118

00:05:16.600  -->  00:05:18.340
very great read here,
119

119

00:05:18.340  -->  00:05:20.900
but for now, let's just walk through this example.
120

120

00:05:20.900  -->  00:05:23.290
Here we go, let's have a look at these connections.
121

121

00:05:23.290  -->  00:05:26.520
Well, first of all, we first training the auto encoder.
122

122

00:05:26.520  -->  00:05:29.780
We're just going to come up with certain connections
123

123

00:05:29.780  -->  00:05:32.690
right away, certain weights just to prove.
124

124

00:05:32.690  -->  00:05:35.150
This whole example is to prove that it is possible.
125

125

00:05:35.150  -->  00:05:39.300
It is possible to take four values and encode them into
126

126

00:05:39.300  -->  00:05:42.890
actually two values, and carry your data around,
127

127

00:05:42.890  -->  00:05:46.954
and basically save space and extract those features.
128

128

00:05:46.954  -->  00:05:49.460
This example just to show that this whole
129

129

00:05:49.460  -->  00:05:50.650
situation is possible.
130

130

00:05:50.650  -->  00:05:52.130
We're not gonna worry about the training,
131

131

00:05:52.130  -->  00:05:53.540
we're not gonna worry about anything else,
132

132

00:05:53.540  -->  00:05:55.757
we're just going to see how is this possible,
133

133

00:05:55.757  -->  00:05:57.920
this sounds like magic, right?
134

134

00:05:57.920  -->  00:05:59.760
So, let's see how this is possible.
135

135

00:05:59.760  -->  00:06:02.650
First of all we're going to color our synapses
136

136

00:06:02.650  -->  00:06:05.140
in two different colors, blue and black.
137

137

00:06:05.140  -->  00:06:09.100
Or light blue and black, were light blue is basically
138

138

00:06:09.100  -->  00:06:12.560
a multiplication by one, so your weight is plus one,
139

139

00:06:12.560  -->  00:06:15.130
and black is a multiplication by minus one.
140

140

00:06:15.130  -->  00:06:16.961
So your weight is minus one.
141

141

00:06:16.961  -->  00:06:20.110
The other thing I wanted to mention here, in auto encoders,
142

142

00:06:20.110  -->  00:06:21.061
you normally use
143

143

00:06:21.061  -->  00:06:25.023
the hyperbolic tangent activation function here and here.
144

144

00:06:25.023  -->  00:06:26.499
We're not going to be doing that,
145

145

00:06:26.499  -->  00:06:29.600
in this specific example, we're going to just worry
146

146

00:06:29.600  -->  00:06:31.250
about the weights and we're just going to forget
147

147

00:06:31.250  -->  00:06:33.520
about activation function for now completely.
148

148

00:06:33.520  -->  00:06:35.400
This will just help us understand everything
149

149

00:06:35.400  -->  00:06:37.240
on an intuitive level. So there we go.
150

150

00:06:37.240  -->  00:06:41.470
That's how we're going to structure this specific network.
151

151

00:06:41.470  -->  00:06:44.240
This specific auto encoder, we've already predefined
152

152

00:06:44.240  -->  00:06:46.620
the weights as such, and this will help us understand
153

153

00:06:46.620  -->  00:06:47.800
that everything is possible,
154

154

00:06:47.800  -->  00:06:49.955
this is just a specific example.
155

155

00:06:49.955  -->  00:06:52.780
Let's have a look at an input.
156

156

00:06:52.780  -->  00:06:55.887
Let's say as an input, we've got one, zero, zero, zero.
157

157

00:06:55.887  -->  00:06:58.190
The person just like movie number one,
158

158

00:06:58.190  -->  00:07:00.190
and they dislike the rest of the movies.
159

159

00:07:00.190  -->  00:07:03.470
What will the hidden nodes look like in that case?
160

160

00:07:03.470  -->  00:07:07.210
Well, in that case, hidden nodes will be, this one
161

161

00:07:07.210  -->  00:07:10.040
will turn into one here, and this one
162

162

00:07:10.040  -->  00:07:12.788
will turn into one here, because blue is multiplying by one.
163

163

00:07:12.788  -->  00:07:16.110
These zeros, they will always just add zero, basically,
164

164

00:07:16.110  -->  00:07:18.665
they're not going to contribute to the hidden nodes.
165

165

00:07:18.665  -->  00:07:22.340
Once you have that in now, let's calculate each one
166

166

00:07:22.340  -->  00:07:23.220
of the output nodes.
167

167

00:07:23.220  -->  00:07:26.283
This is going to be a tedious process, but it's worth it.
168

168

00:07:27.300  -->  00:07:31.910
There's our input going into the hidden nodes,
169

169

00:07:31.910  -->  00:07:33.550
and now let's see what happens.
170

170

00:07:33.550  -->  00:07:37.190
For the top nodes we multiply by a plus one by a plus one
171

171

00:07:37.190  -->  00:07:41.370
and we get two, for this node we get one times one equals
172

172

00:07:41.370  -->  00:07:43.950
one, one times minus one equals minus one,
173

173

00:07:43.950  -->  00:07:45.490
you add them up, you get zero.
174

174

00:07:45.490  -->  00:07:48.680
Here, you get minus one plus one equals zero, and here
175

175

00:07:48.680  -->  00:07:51.310
you get minus one minus one equals minus two.
176

176

00:07:51.310  -->  00:07:53.450
So those are your outputs but those
177

177

00:07:53.450  -->  00:07:54.770
are actually preliminary outputs.
178

178

00:07:54.770  -->  00:07:59.230
In Auto Encoder we also have a softmax function on the end.
179

179

00:07:59.230  -->  00:08:01.340
we have a tutorial on the softmax function
180

180

00:08:01.340  -->  00:08:04.940
in one of the very early parts of the course,
181

181

00:08:04.940  -->  00:08:07.650
it's a bonus tutorial, now is the time to probably
182

182

00:08:07.650  -->  00:08:10.510
go revisit that tutorial if you skipped it at this stage.
183

183

00:08:10.510  -->  00:08:14.050
Basically what the softmax function does in this case,
184

184

00:08:14.050  -->  00:08:17.660
is it takes the highest value, so in this case are two
185

185

00:08:17.660  -->  00:08:20.440
and it turns that into one and everything else into zero.
186

186

00:08:20.440  -->  00:08:23.150
If you apply a softMax, you will get one
187

187

00:08:23.150  -->  00:08:25.580
where you see the highest value and then zeros
188

188

00:08:25.580  -->  00:08:27.400
where you see everything else.
189

189

00:08:27.400  -->  00:08:30.720
As you can see, this result is indeed identical
190

190

00:08:30.720  -->  00:08:34.320
to what we input into our network.
191

191

00:08:34.320  -->  00:08:35.690
All right, so that's the start.
192

192

00:08:35.690  -->  00:08:38.060
Let's have a look at some other cases.
193

193

00:08:38.060  -->  00:08:39.270
We're not going to be as detailed,
194

194

00:08:39.270  -->  00:08:41.020
we're just going to look at the results.
195

195

00:08:41.020  -->  00:08:43.249
If you input zero, one, zero, zero,
196

196

00:08:43.249  -->  00:08:46.030
you will see that you get a two over here.
197

197

00:08:46.030  -->  00:08:48.780
Zero with two minus two zero, and when you apply
198

198

00:08:48.780  -->  00:08:52.190
the softmax, you get zero, one, zero, zero again, identical.
199

199

00:08:52.190  -->  00:08:55.990
If you've input a one over here, you get one over here.
200

200

00:08:55.990  -->  00:09:00.483
If you input a one over here, you get a one over here.
201

201

00:09:00.483  -->  00:09:04.500
As you can see, as long as in our data set, we've got rows
202

202

00:09:04.500  -->  00:09:08.620
with just three zeros on one, one, we can encode them
203

203

00:09:08.620  -->  00:09:12.100
into a small format where we just have two value.
204

204

00:09:12.100  -->  00:09:13.660
So we just have to have these hidden nodes.
205

205

00:09:13.660  -->  00:09:15.280
As you can see every state,
206

206

00:09:15.280  -->  00:09:17.020
is represented by a hidden node.
207

207

00:09:17.020  -->  00:09:20.250
So you have one, one, you have one minus one,
208

208

00:09:20.250  -->  00:09:23.246
you have minus one, one, and you have minus one minus one
209

209

00:09:23.246  -->  00:09:26.156
every state is represented by a hidden layer.
210

210

00:09:26.156  -->  00:09:30.620
Then you just need these weights, the knowledge about
211

211

00:09:30.620  -->  00:09:33.490
these weights to reconstruct your output.
212

212

00:09:33.490  -->  00:09:35.073
Is a very simplified example,
213

213

00:09:37.571  -->  00:09:41.200
now looking back it is even quite straightforward
214

214

00:09:41.200  -->  00:09:43.790
that if you have four states here, four possible states,
215

215

00:09:43.790  -->  00:09:46.020
and you have to by two combinations,
216

216

00:09:46.020  -->  00:09:49.060
you have four in total, but this gives a overview
217

217

00:09:49.060  -->  00:09:53.473
of how Auto Encoders work again, in a very simplified way.
218

218

00:09:53.473  -->  00:09:56.500
Of course are much more complex than that,
219

219

00:09:56.500  -->  00:09:59.520
so here you can see a much more sophisticated example
220

220

00:09:59.520  -->  00:10:01.350
where you have way more.
221

221

00:10:01.350  -->  00:10:04.300
So you have one, two, three, four, five, six, seven,
222

222

00:10:04.300  -->  00:10:07.600
eight, nine, 10, 11 inputs, two hidden nodes,
223

223

00:10:07.600  -->  00:10:10.480
and 11 outputs and it still works totally fine.
224

224

00:10:10.480  -->  00:10:13.840
And this is a reference for additional reading to that same
225

225

00:10:13.840  -->  00:10:17.127
blog that we already mentioned it's called,
226

226

00:10:17.127  -->  00:10:19.700
"Neural Networks Are Impressively Are Good At Compression"
227

227

00:10:19.700  -->  00:10:24.300
by Malte Skarupke and will include it in the additional
228

228

00:10:24.300  -->  00:10:26.700
resources of course, so definitely check it out.
229

229

00:10:28.070  -->  00:10:32.366
Nicely written very easy introduction into Auto Encoders.
230

230

00:10:32.366  -->  00:10:35.510
In fact, I don't think it's even actually mentioned
231

231

00:10:35.510  -->  00:10:38.060
that it's Auto Encoders from the very start,
232

232

00:10:38.060  -->  00:10:40.540
just Neural Networks, and then in the comments you can read
233

233

00:10:40.540  -->  00:10:44.388
that indeed they were talking about Auto Encoders.
234

234

00:10:44.388  -->  00:10:45.670
Have a look at that,
235

235

00:10:45.670  -->  00:10:48.050
and I look forward to seeing you next time.
236

236

00:10:48.050  -->  00:10:50.003
Until then, enjoy Deep Learning.
