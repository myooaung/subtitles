1
1

00:00:00,330  -->  00:00:02,700
<v Instructor>Hello, and welcome to this new tutorial,</v>
2

2

00:00:02,700  -->  00:00:06,050
about adding the extra LSTM layers,
3

3

00:00:06,050  -->  00:00:07,490
and of course, to each of them,
4

4

00:00:07,490  -->  00:00:09,660
some dropout regularization.
5

5

00:00:09,660  -->  00:00:10,520
So as you can see,
6

6

00:00:10,520  -->  00:00:13,010
I already prepared what we're going to add,
7

7

00:00:13,010  -->  00:00:15,070
a second LSTM layer,
8

8

00:00:15,070  -->  00:00:16,960
with some Dropout regularization.
9

9

00:00:16,960  -->  00:00:18,780
A third LSTM layer,
10

10

00:00:18,780  -->  00:00:21,190
with some Dropout regularization again,
11

11

00:00:21,190  -->  00:00:23,840
and a fourth LSTM layer,
12

12

00:00:23,840  -->  00:00:25,930
with some Dropout regularization.
13

13

00:00:25,930  -->  00:00:29,170
So we're going to have a total of four LSTM layers
14

14

00:00:29,170  -->  00:00:31,750
in our robust structure,
15

15

00:00:31,750  -->  00:00:34,730
of our LSTM recurrent neural network.
16

16

00:00:34,730  -->  00:00:37,740
So let's start with the second LSTM layer,
17

17

00:00:37,740  -->  00:00:39,660
and the good news is that it's gonna be
18

18

00:00:39,660  -->  00:00:41,080
very, very easy,
19

19

00:00:41,080  -->  00:00:43,640
because we simply need to copy
20

20

00:00:43,640  -->  00:00:45,180
these two lines of code,
21

21

00:00:45,180  -->  00:00:46,867
because of course we're gonna use again
22

22

00:00:46,867  -->  00:00:49,910
.add method from the Sequential class,
23

23

00:00:49,910  -->  00:00:52,170
to add a new LSTM layer,
24

24

00:00:52,170  -->  00:00:53,660
and some dropout regularization
25

25

00:00:53,660  -->  00:00:55,230
to our regressor,
26

26

00:00:55,230  -->  00:00:58,030
but be careful, we only need to do one change
27

27

00:00:58,030  -->  00:01:00,280
which is about the input_shape.
28

28

00:01:00,280  -->  00:01:03,680
We had to specify the input_shape argument here,
29

29

00:01:03,680  -->  00:01:05,630
because this was our first LSTM layer,
30

30

00:01:05,630  -->  00:01:08,340
and we had to specify the shape of the input,
31

31

00:01:08,340  -->  00:01:10,180
with the last two dimensions here,
32

32

00:01:10,180  -->  00:01:12,800
first pointing to the timesteps and the predictors,
33

33

00:01:12,800  -->  00:01:15,730
the indicators, but now things are slightly different.
34

34

00:01:15,730  -->  00:01:18,280
We're just adding a second LSTM layer,
35

35

00:01:18,280  -->  00:01:20,680
and therefor we don't need to specify
36

36

00:01:20,680  -->  00:01:23,550
the input layer anymore,
37

37

00:01:23,550  -->  00:01:26,340
and that's why I am just removing this,
38

38

00:01:26,340  -->  00:01:27,173
and there we go,
39

39

00:01:27,173  -->  00:01:28,740
these two lines of code,
40

40

00:01:28,740  -->  00:01:30,750
will add your second LSTM layer,
41

41

00:01:30,750  -->  00:01:32,780
with some dropout regularization,
42

42

00:01:32,780  -->  00:01:34,400
and for those of you who are wondering,
43

43

00:01:34,400  -->  00:01:36,650
why we don't have to still specify
44

44

00:01:36,650  -->  00:01:39,560
the input shape of the previous LSTM layer,
45

45

00:01:39,560  -->  00:01:42,310
well that's because it's recognized automatically,
46

46

00:01:42,310  -->  00:01:44,610
thanks to this units argument here,
47

47

00:01:44,610  -->  00:01:47,440
which tells exactly that we have 50 neurons
48

48

00:01:47,440  -->  00:01:48,720
in the previous layer.
49

49

00:01:48,720  -->  00:01:51,430
So no need to specify any input shape here,
50

50

00:01:51,430  -->  00:01:54,300
when you're adding your next LSTM layers,
51

51

00:01:54,300  -->  00:01:55,820
after the first one,
52

52

00:01:55,820  -->  00:01:57,800
and so, we're gonna keep this same
53

53

00:01:57,800  -->  00:02:00,860
number of neurons in this second LSTM layer,
54

54

00:02:00,860  -->  00:02:03,070
50 neurons, and that's the same.
55

55

00:02:03,070  -->  00:02:05,740
That's to have some high dimensional NT
56

56

00:02:05,740  -->  00:02:08,250
in our model, to be able to handle
57

57

00:02:08,250  -->  00:02:10,000
the complexity of the problem.
58

58

00:02:10,000  -->  00:02:12,390
Augmenting the dimensionality of our model,
59

59

00:02:12,390  -->  00:02:14,740
will augment at the same time the complexity,
60

60

00:02:14,740  -->  00:02:16,750
and therefor will respond better
61

61

00:02:16,750  -->  00:02:18,420
to the complexity of the problem
62

62

00:02:18,420  -->  00:02:21,470
and that will eventually lead us to better results.
63

63

00:02:21,470  -->  00:02:22,970
So 50 neurons again,
64

64

00:02:22,970  -->  00:02:26,270
and same, we're keeping a 20% dropout
65

65

00:02:26,270  -->  00:02:28,130
for the regularization,
66

66

00:02:28,130  -->  00:02:29,260
that should do it,
67

67

00:02:29,260  -->  00:02:31,260
since that's a relevant choice.
68

68

00:02:31,260  -->  00:02:32,610
Alright, and now,
69

69

00:02:32,610  -->  00:02:34,390
I have some even better news.
70

70

00:02:34,390  -->  00:02:36,440
It is that, in order to add,
71

71

00:02:36,440  -->  00:02:38,680
the third LSTM layer here,
72

72

00:02:38,680  -->  00:02:40,700
well the only thing that we have to do,
73

73

00:02:40,700  -->  00:02:44,070
is just copy-paste these two lines of code here,
74

74

00:02:44,070  -->  00:02:46,250
that's out of the second LSTM layer,
75

75

00:02:46,250  -->  00:02:48,910
because adding the third LSTM layer,
76

76

00:02:48,910  -->  00:02:52,200
is exactly the same as adding the second LSTM layer.
77

77

00:02:52,200  -->  00:02:55,160
We simply need to specify the number of neurons
78

78

00:02:55,160  -->  00:02:56,400
in the LSTM layer,
79

79

00:02:56,400  -->  00:02:57,800
and we're keeping 50 neurons
80

80

00:02:57,800  -->  00:03:00,080
to have the same same goal
81

81

00:03:00,080  -->  00:03:02,210
of having a high dimensionality.
82

82

00:03:02,210  -->  00:03:05,240
We still need to keep return_sequence is equal true,
83

83

00:03:05,240  -->  00:03:06,170
because we're going to add
84

84

00:03:06,170  -->  00:03:08,260
another LSTM layer after that,
85

85

00:03:08,260  -->  00:03:11,910
and we're keeping the 20% dropout regularization.
86

86

00:03:11,910  -->  00:03:14,120
So that's done, just by copy-pasting
87

87

00:03:14,120  -->  00:03:15,330
these two lines of code,
88

88

00:03:15,330  -->  00:03:18,240
we now have our third LSTM layer,
89

89

00:03:18,240  -->  00:03:21,350
however, now for the fourth LSTM layer,
90

90

00:03:21,350  -->  00:03:22,670
things are going to change,
91

91

00:03:22,670  -->  00:03:24,290
but very slightly.
92

92

00:03:24,290  -->  00:03:26,280
We will only have one change to make,
93

93

00:03:26,280  -->  00:03:28,050
try to guess before I do it,
94

94

00:03:28,050  -->  00:03:30,110
what exactly is this change gonna be?
95

95

00:03:30,110  -->  00:03:31,030
Well, let's see.
96

96

00:03:31,030  -->  00:03:33,860
I'm going to paste what I copied before,
97

97

00:03:33,860  -->  00:03:35,750
and now, according to you,
98

98

00:03:35,750  -->  00:03:37,420
what do I need to change here?
99

99

00:03:37,420  -->  00:03:38,410
Well, let's see.
100

100

00:03:38,410  -->  00:03:40,720
The number of units equal to 50 here,
101

101

00:03:40,720  -->  00:03:42,010
do we need to change this?
102

102

00:03:42,010  -->  00:03:44,680
Well, no, we're going to keep 50 neurons
103

103

00:03:44,680  -->  00:03:46,330
in this fourth LSTM layer,
104

104

00:03:46,330  -->  00:03:48,740
because be careful, this is not the final layer
105

105

00:03:48,740  -->  00:03:50,400
of our recurrent neural network,
106

106

00:03:50,400  -->  00:03:52,250
this is the fourth LSTM layer,
107

107

00:03:52,250  -->  00:03:54,910
but after that, comes the output layer,
108

108

00:03:54,910  -->  00:03:56,900
with the output dimension,
109

109

00:03:56,900  -->  00:03:57,900
which will be one of course,
110

110

00:03:57,900  -->  00:04:00,460
because we're predicting just one value,
111

111

00:04:00,460  -->  00:04:02,910
the value of the stock price at time T+1,
112

112

00:04:02,910  -->  00:04:05,800
but here, we're adding the fourth LSTM layer,
113

113

00:04:05,800  -->  00:04:08,670
and since this is the last LSTM layer
114

114

00:04:08,670  -->  00:04:09,680
that we're adding,
115

115

00:04:09,680  -->  00:04:12,410
well, this is the change that we need to make,
116

116

00:04:12,410  -->  00:04:14,610
we need to set it equal to false,
117

117

00:04:14,610  -->  00:04:16,510
because we're not going to return
118

118

00:04:16,510  -->  00:04:18,460
anymore sequences,
119

119

00:04:18,460  -->  00:04:20,590
and therefor, since the default value
120

120

00:04:20,590  -->  00:04:23,440
of the return sequence's parameter is false,
121

121

00:04:23,440  -->  00:04:26,700
I am just removing this parameter,
122

122

00:04:26,700  -->  00:04:27,810
and there we go,
123

123

00:04:27,810  -->  00:04:29,420
that's what we had to do for
124

124

00:04:29,420  -->  00:04:31,150
the fourth LSTM layer,
125

125

00:04:31,150  -->  00:04:32,740
just add in the LSTM class
126

126

00:04:32,740  -->  00:04:34,970
with 50 units, and saying
127

127

00:04:34,970  -->  00:04:38,940
we're keeping 20% dropout regularization.
128

128

00:04:38,940  -->  00:04:42,370
Perfect, so now let's add our different LSTM layers,
129

129

00:04:42,370  -->  00:04:43,930
by executing the codes.
130

130

00:04:43,930  -->  00:04:47,430
So let's start with the second LSTM layer
131

131

00:04:47,430  -->  00:04:49,460
of our RNN, execute.
132

132

00:04:49,460  -->  00:04:52,070
There we go, second LSTM layer added.
133

133

00:04:52,070  -->  00:04:54,120
Now let's take care of the third one,
134

134

00:04:54,120  -->  00:04:57,570
so I'm selecting these two lines and executing,
135

135

00:04:57,570  -->  00:04:58,403
there we go,
136

136

00:04:58,403  -->  00:05:01,688
and finally the fourth and final LSTM layer,
137

137

00:05:01,688  -->  00:05:05,410
there we go, fourth and final LSTM well added.
138

138

00:05:05,410  -->  00:05:07,470
So perfect, now congratulations,
139

139

00:05:07,470  -->  00:05:10,160
you are done with the LSTM part
140

140

00:05:10,160  -->  00:05:11,740
of the recurrent neural network.
141

141

00:05:11,740  -->  00:05:12,990
Now we just need to add
142

142

00:05:12,990  -->  00:05:15,870
one final layer, which is of course the output layer,
143

143

00:05:15,870  -->  00:05:18,450
and that's exactly what we'll do in the next tutorial.
144

144

00:05:18,450  -->  00:05:20,050
Until then, enjoy deep learning.
