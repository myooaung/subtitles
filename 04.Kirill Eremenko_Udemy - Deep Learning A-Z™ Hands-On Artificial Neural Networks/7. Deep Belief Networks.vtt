WEBVTT
1
1

00:00:00.000  -->  00:00:01.220
<v Narrator>Hello, and welcome back</v>
2

2

00:00:01.220  -->  00:00:02.690
to the course on Deep Learning.
3

3

00:00:02.690  -->  00:00:05.290
Today we are talking about Deep Belief Networks.
4

4

00:00:05.290  -->  00:00:07.520
So, what is a Deep Belief Network?
5

5

00:00:07.520  -->  00:00:10.850
Well, a Deep Belief Network comes to be if you
6

6

00:00:10.850  -->  00:00:13.280
stack on top of each other several
7

7

00:00:13.280  -->  00:00:15.520
Restricted Boltzmann Machines or RBMs.
8

8

00:00:15.520  -->  00:00:17.700
So, here you can see the first RBM,
9

9

00:00:17.700  -->  00:00:20.700
here you can see the second RBM, the third RBM.
10

10

00:00:20.700  -->  00:00:23.740
So, basically the outputs, or the hidden layer
11

11

00:00:23.740  -->  00:00:27.710
of the first RBM, is the input of the second RBM
12

12

00:00:27.710  -->  00:00:30.300
and then the hidden layer of the second RBM
13

13

00:00:30.300  -->  00:00:32.190
is the input of the third RBM.
14

14

00:00:32.190  -->  00:00:35.510
And, in a Deep Belief Network,
15

15

00:00:35.510  -->  00:00:38.020
so you've stacked up these RBMs,
16

16

00:00:38.020  -->  00:00:42.310
and then also another thing that happens is
17

17

00:00:42.310  -->  00:00:47.050
you make sure that you're directionality is in place
18

18

00:00:47.050  -->  00:00:50.670
for all of the layers except for the top two.
19

19

00:00:50.670  -->  00:00:54.480
So, basically you make sure that these layers,
20

20

00:00:54.480  -->  00:00:56.750
layers one, two and three, and the connections between them,
21

21

00:00:56.750  -->  00:00:58.421
they are directed and they are directed downwards.
22

22

00:00:58.421  -->  00:01:01.910
Whereas there is no direction in the top layers.
23

23

00:01:01.910  -->  00:01:05.340
It's quite hard to explain what's going on here
24

24

00:01:05.340  -->  00:01:07.830
because this is a very complex,
25

25

00:01:07.830  -->  00:01:10.440
a very advanced type of network.
26

26

00:01:10.440  -->  00:01:13.760
We'll just say that this is the network,
27

27

00:01:13.760  -->  00:01:18.180
these Deep Belief Networks, is what revived
28

28

00:01:18.180  -->  00:01:21.350
the interest in Deep Learning in the 2000's.
29

29

00:01:21.350  -->  00:01:25.810
This coming up when Hinton and his team, as for as I know,
30

30

00:01:25.810  -->  00:01:27.290
it was Hinton and his team who came up
31

31

00:01:27.290  -->  00:01:29.260
with these Deep Belief Networks.
32

32

00:01:29.260  -->  00:01:32.910
When that happened that's when the whole interest
33

33

00:01:32.910  -->  00:01:36.460
in Deep Learning got revived.
34

34

00:01:36.460  -->  00:01:38.520
And, the other thing that I'll mention here,
35

35

00:01:38.520  -->  00:01:41.930
is that there's two types of training.
36

36

00:01:41.930  -->  00:01:44.200
So, there will be quite a bit of additional reading
37

37

00:01:44.200  -->  00:01:46.900
which you can do to really familiarize yourself with
38

38

00:01:46.900  -->  00:01:48.910
Deep Belief Networks, and how they are used,
39

39

00:01:48.910  -->  00:01:52.570
and what kind of mathematics are behind them.
40

40

00:01:52.570  -->  00:01:55.100
But, in terms of training, there's two types of algorithms
41

41

00:01:55.100  -->  00:01:56.191
that you'll come across.
42

42

00:01:56.191  -->  00:01:59.960
There's Greedy layer-wise training algorithm
43

43

00:01:59.960  -->  00:02:03.670
which basically trains these RBMs.
44

44

00:02:03.670  -->  00:02:06.770
So, the directionality you set it up
45

45

00:02:06.770  -->  00:02:08.120
after you've trained up the weights.
46

46

00:02:08.120  -->  00:02:11.262
So, first you train the Restricted Bolzmann Machines, RBMs,
47

47

00:02:11.262  -->  00:02:15.180
with the undirected connections.
48

48

00:02:15.180  -->  00:02:18.730
And the Greedy layer-wise training is, you train this layer,
49

49

00:02:18.730  -->  00:02:20.390
you train this layer, you train this layer.
50

50

00:02:20.390  -->  00:02:22.930
So, you train them layer by layer as RBMs
51

51

00:02:22.930  -->  00:02:25.400
and then there's also the wake-sleep algorithm.
52

52

00:02:25.400  -->  00:02:27.030
It's another thing you should look out for
53

53

00:02:27.030  -->  00:02:28.780
in the papers you're going to be reading.
54

54

00:02:28.780  -->  00:02:30.930
The wake-sleep algorithm is basically
55

55

00:02:30.930  -->  00:02:33.730
you train all the way up, then you train all the way down.
56

56

00:02:33.730  -->  00:02:36.357
So, your connections there awake.
57

57

00:02:36.357  -->  00:02:38.710
All the ones that going up are awake.
58

58

00:02:38.710  -->  00:02:40.340
The ones that are going down are asleep.
59

59

00:02:40.340  -->  00:02:41.290
And then the other way around.
60

60

00:02:41.290  -->  00:02:43.710
That's where the wake-sleep term comes from.
61

61

00:02:43.710  -->  00:02:45.440
So, basically that's how it works.
62

62

00:02:45.440  -->  00:02:48.300
You stack up RBMs, you train them up,
63

63

00:02:48.300  -->  00:02:50.020
then you, once you've got the weights,
64

64

00:02:50.020  -->  00:02:52.740
you make sure that these connections only work downwards.
65

65

00:02:52.740  -->  00:02:55.190
And that's pretty much it and, unfortunately,
66

66

00:02:55.190  -->  00:02:57.490
we won't go into too much detail on this
67

67

00:02:57.490  -->  00:02:59.410
because this is a very, very advanced topic.
68

68

00:02:59.410  -->  00:03:02.670
I just wanted to make sure that you are familiar
69

69

00:03:02.670  -->  00:03:05.600
with what a Deep Belief Network looks like
70

70

00:03:05.600  -->  00:03:07.710
and what it entails.
71

71

00:03:07.710  -->  00:03:09.360
If you ever do need to come across,
72

72

00:03:09.360  -->  00:03:11.480
if you ever do need to use it in practice,
73

73

00:03:11.480  -->  00:03:14.430
you inevitably will have to design your own
74

74

00:03:14.430  -->  00:03:17.590
and therefore you will have to do some research
75

75

00:03:17.590  -->  00:03:19.710
and read some papers
76

76

00:03:20.868  -->  00:03:23.810
and understand the design that goes onto it.
77

77

00:03:23.810  -->  00:03:27.360
And that is just going beyond the scope
78

78

00:03:27.360  -->  00:03:28.740
of these initial tutorials.
79

79

00:03:28.740  -->  00:03:30.410
But, nevertheless, we are going to give you
80

80

00:03:30.410  -->  00:03:32.830
some additional things you can look into.
81

81

00:03:32.830  -->  00:03:37.830
So, the Greedy layer-wise training of Deep Belief Networks
82

82

00:03:39.310  -->  00:03:41.270
you can find it in this article over here,
83

83

00:03:41.270  -->  00:03:43.200
by Yoshua Bengio and others.
84

84

00:03:43.200  -->  00:03:45.990
While actually this article is very popular,
85

85

00:03:45.990  -->  00:03:47.360
it's been cited over a thousand times,
86

86

00:03:47.360  -->  00:03:48.470
but the article that we mentioned
87

87

00:03:48.470  -->  00:03:50.900
in the previous tutorial by Hinton,
88

88

00:03:50.900  -->  00:03:54.070
where we were talking about how the training
89

89

00:03:54.070  -->  00:03:57.111
of an RBM happens. Well that article actually also has,
90

90

00:03:57.111  -->  00:04:00.110
that's where the Greedy layer-wise is described
91

91

00:04:00.110  -->  00:04:03.410
before it was used in Bengio's article.
92

92

00:04:03.410  -->  00:04:05.920
So, then that article by Hinton was cited like
93

93

00:04:05.920  -->  00:04:07.130
over 5,000 times.
94

94

00:04:07.130  -->  00:04:09.370
So, make sure to check out that article first,
95

95

00:04:09.370  -->  00:04:10.458
if you haven't yet.
96

96

00:04:10.458  -->  00:04:12.010
Or that paper first.
97

97

00:04:12.010  -->  00:04:14.690
And then move onto this paper by Bengio
98

98

00:04:14.690  -->  00:04:17.340
and here they explore the whole concept
99

99

00:04:17.340  -->  00:04:19.440
of Greedy layer-wise training further.
100

100

00:04:19.440  -->  00:04:24.250
And, they understand how it enhances the training.
101

101

00:04:24.250  -->  00:04:28.220
How it makes things faster
102

102

00:04:28.220  -->  00:04:29.500
and like they just basically test it.
103

103

00:04:29.500  -->  00:04:30.780
I think it's an emperical test
104

104

00:04:30.780  -->  00:04:32.520
on the Greedy layer-wise training.
105

105

00:04:32.520  -->  00:04:35.700
And so, if you do want to get into DBNs more
106

106

00:04:35.700  -->  00:04:38.130
then those two articles, those two papers,
107

107

00:04:38.130  -->  00:04:41.440
will help you get, actually do exactly that.
108

108

00:04:41.440  -->  00:04:44.130
And, also in addition, you will learn about this
109

109

00:04:44.130  -->  00:04:46.450
Greedy layer-wise training and how it's happening.
110

110

00:04:46.450  -->  00:04:51.230
Another one is the wake-sleep algorithm by Hinton.
111

111

00:04:51.230  -->  00:04:55.380
So it's another popular article, another popular paper.
112

112

00:04:55.380  -->  00:04:59.111
This one was written in, published in 1995.
113

113

00:04:59.111  -->  00:05:01.140
This one is talking about the wake-sleep algorithm.
114

114

00:05:01.140  -->  00:05:05.120
So, again, if you feel that you need to get into DBNs,
115

115

00:05:05.120  -->  00:05:07.330
and it's something that you need for your work
116

116

00:05:07.330  -->  00:05:08.420
or something you need for a project
117

117

00:05:08.420  -->  00:05:09.670
or you want to explore further,
118

118

00:05:09.670  -->  00:05:14.380
these are the 3 papers that are a good start
119

119

00:05:14.380  -->  00:05:17.980
to get you into the world of Deep Belief Networks.
120

120

00:05:17.980  -->  00:05:18.813
There we go.
121

121

00:05:18.813  -->  00:05:20.970
I hope you enjoyed today's tutorial and I look forward
122

122

00:05:20.970  -->  00:05:21.820
to seeing you next time.
123

123

00:05:21.820  -->  00:05:23.523
Until then enjoy Deep Learning.
