1
00:00:00,000 --> 00:00:02,873
Hello, I'm Fernando Medina Corey,

2
00:00:02,873 --> 00:00:07,971
and welcome to this module of my course on the serverless framework.

3
00:00:07,971 --> 00:00:08,616
In this module,

4
00:00:08,616 --> 00:00:12,682
we'll look at where a serverless is moving with

5
00:00:12,682 --> 00:00:14,392
multi-platform serverless architectures.

6
00:00:14,392 --> 00:00:19,606
We'll start this module with some context by looking IaaS Outages and Risks.

7
00:00:19,606 --> 00:00:23,645
We'll look at cloud provider failures, including notable failures of Azure,

8
00:00:23,645 --> 00:00:25,896
Google Cloud Platform, and AWS.

9
00:00:25,896 --> 00:00:30,353
We'll review these failures and look at some key takeaways from all of them.

10
00:00:30,353 --> 00:00:33,148
Then we'll move into looking directly at multi-platform architectures.

11
00:00:33,148 --> 00:00:36,098
We'll overview what multi-platform architectures are,

12
00:00:36,098 --> 00:00:38,084
and the components that they're comprised of.

13
00:00:38,084 --> 00:00:41,514
Then we'll move into other forms of resilient architectures and provide

14
00:00:41,514 --> 00:00:45,733
examples of how we can add fault tolerance to our applications.

15
00:00:45,733 --> 00:00:46,185
After that,

16
00:00:46,185 --> 00:00:49,607
we'll review a few of the basic best practices in this area before we

17
00:00:49,607 --> 00:00:52,723
look at what we can do with our newfound skills.

18
00:00:52,723 --> 00:00:54,721
So let's get started.

19
00:00:54,721 --> 00:00:57,148
Using any technology has benefits and risks.

20
00:00:57,148 --> 00:01:00,093
The same is true with infrastructure as a service providers.

21
00:01:00,093 --> 00:01:02,526
While you gain a huge amount of benefit in terms of functionality,

22
00:01:02,526 --> 00:01:06,282
you do give up some control to the internals of your systems.

23
00:01:06,282 --> 00:01:07,973
And when a serious provider happens,

24
00:01:07,973 --> 00:01:12,067
you end up only being able to report the problem and twiddle your thumbs.

25
00:01:12,067 --> 00:01:15,365
So here are a few examples of some of the things that could

26
00:01:15,365 --> 00:01:17,215
go wrong with infrastructure providers.

27
00:01:17,215 --> 00:01:18,777
In November of 2014,

28
00:01:18,777 --> 00:01:21,783
Microsoft Azure made a configuration change to improve

29
00:01:21,783 --> 00:01:23,142
the performance of Azure storage.

30
00:01:23,142 --> 00:01:27,568
Usually these kinds of changes are flighted and incrementally rolled out in

31
00:01:27,568 --> 00:01:30,590
different areas to try to preemptively spot any issues.

32
00:01:30,590 --> 00:01:31,839
Unfortunately in this case,

33
00:01:31,839 --> 00:01:35,302
an engineer inadvertently skipped the flighting process and deployed a bug.

34
00:01:35,302 --> 00:01:36,675
Before this could be fixed,

35
00:01:36,675 --> 00:01:39,812
it caused connections to Azure Storage and VMs to fail and

36
00:01:39,812 --> 00:01:41,632
took several hours to fully resolve.

37
00:01:41,632 --> 00:01:44,486
In a more recent example, in April of 2016,

38
00:01:44,486 --> 00:01:47,273
Google Cloud platform was making a configuration change

39
00:01:47,273 --> 00:01:49,477
to traffic routing on its network.

40
00:01:49,477 --> 00:01:50,730
This change caused an issue,

41
00:01:50,730 --> 00:01:54,541
because it was made in only one configuration file and not another.

42
00:01:54,541 --> 00:01:56,187
Because of this inconsistency,

43
00:01:56,187 --> 00:01:58,229
the system was supposed to notice this error with the canary

44
00:01:58,229 --> 00:02:00,544
test and fall back to the previous state.

45
00:02:00,544 --> 00:02:03,499
But while the canary test noticed an unsaved configuration,

46
00:02:03,499 --> 00:02:07,840
a bug in the management software ignored it and deployed the change anyway.

47
00:02:07,840 --> 00:02:11,337
At this point, connections began to fail across Google compute instances,

48
00:02:11,337 --> 00:02:14,063
which launched a series of internal alerts at Google.

49
00:02:14,063 --> 00:02:17,837
These alerts were investigated and then fixed by the engineering team there.

50
00:02:17,837 --> 00:02:18,757
And here's one final example,

51
00:02:18,757 --> 00:02:21,581
just in case you think I'm showing favoritism to AWS.

52
00:02:21,581 --> 00:02:23,631
On February 28, 2017,

53
00:02:23,631 --> 00:02:27,996
an engineer in the AWS billing department tried to remove a

54
00:02:27,996 --> 00:02:30,400
few servers related to work they were doing.

55
00:02:30,400 --> 00:02:33,346
Unfortunately, the command used by the engineer had a typo,

56
00:02:33,346 --> 00:02:37,502
which dramatically changed the outcome and removed many servers,

57
00:02:37,502 --> 00:02:41,582
including servers used by Core AWS simple storage service subsystems.

58
00:02:41,582 --> 00:02:44,286
This removal caused connections to S3 to fail,

59
00:02:44,286 --> 00:02:47,225
which also cascaded down into errors for services

60
00:02:47,225 --> 00:02:49,920
that relied on the AWS S3 service,

61
00:02:49,920 --> 00:02:51,976
which was a substantial number of AWS services.

62
00:02:51,976 --> 00:02:53,871
So why do I show you all these examples?

63
00:02:53,871 --> 00:02:56,006
It's not to convince you that using infrastructure

64
00:02:56,006 --> 00:02:58,531
providers is a bad idea; in fact,

65
00:02:58,531 --> 00:03:00,789
the vast majority of companies that aren't massive

66
00:03:00,789 --> 00:03:03,272
enterprises are probably better off outsourcing their cloud

67
00:03:03,272 --> 00:03:04,870
needs to an infrastructure provider.

68
00:03:04,870 --> 00:03:07,071
The main reason I show you this is to make sure

69
00:03:07,071 --> 00:03:08,510
that you realize that bugs happen,

70
00:03:08,510 --> 00:03:11,702
and you need to take appropriate precautions for your organization.

71
00:03:11,702 --> 00:03:14,922
That might mean planning for cases like these and it might not.

72
00:03:14,922 --> 00:03:17,312
Another important detail here is that the principle of

73
00:03:17,312 --> 00:03:19,213
least privilege makes for safer systems.

74
00:03:19,213 --> 00:03:20,925
In many of these examples,

75
00:03:20,925 --> 00:03:24,542
reducing the ability of humans or programs to take certain

76
00:03:24,542 --> 00:03:26,417
actions may have avoided this issue entirely.

77
00:03:26,417 --> 00:03:29,727
And this is an important thing to bring to your own organizations.

78
00:03:29,727 --> 00:03:32,713
In general, it's also important to have a plan for failures.

79
00:03:32,713 --> 00:03:33,209
For some,

80
00:03:33,209 --> 00:03:35,841
this might simply mean that you're ready to send a tweet

81
00:03:35,841 --> 00:03:38,103
or email about your infrastructure provider having issues

82
00:03:38,103 --> 00:03:39,941
and apologize to your users.

83
00:03:39,941 --> 00:03:42,560
It also might mean that you need to plan a place to

84
00:03:42,560 --> 00:03:44,400
mitigate issues like this entirely, and finally,

85
00:03:44,400 --> 00:03:47,597
it's important to realize that even IaaS providers can fail.

86
00:03:47,597 --> 00:03:51,000
It doesn't happen very often, but it does happen.

87
00:03:51,000 --> 00:03:51,203
Next,

88
00:03:51,203 --> 00:04:01,000
we'll take a look at how we can try and design systems within the serverless framework that avoid these issues when they do come up.

