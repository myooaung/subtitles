1
00:00:01,140 --> 00:00:03,960
[Autogenerated] we'll start at the beginning of the process with fetching data.

2
00:00:03,960 --> 00:00:07,940
As you already know, without data, there is no machine learning,

3
00:00:07,940 --> 00:00:10,390
and these first three steps in the process fetch,

4
00:00:10,390 --> 00:00:14,240
clean and prepare are the most time consuming part of the process.

5
00:00:14,240 --> 00:00:15,910
And these are usually iterative as well,

6
00:00:15,910 --> 00:00:18,540
where you'll fetch some data clean and analyze.

7
00:00:18,540 --> 00:00:22,740
Go back and get more data and keep iterating until you have what you need.

8
00:00:22,740 --> 00:00:25,980
The goal of all three of these steps is to get your data from one or more

9
00:00:25,980 --> 00:00:30,240
sources into sage maker so you can train the model,

10
00:00:30,240 --> 00:00:32,540
starting with this first step of fetch.

11
00:00:32,540 --> 00:00:35,740
This is also commonly referred to as data ingestion.

12
00:00:35,740 --> 00:00:39,640
Your data is probably in many places in many different formats.

13
00:00:39,640 --> 00:00:41,060
You've got your structured data.

14
00:00:41,060 --> 00:00:45,840
Things like database is where the data is stored in tables of rows and columns.

15
00:00:45,840 --> 00:00:47,940
You might also have unstructured data.

16
00:00:47,940 --> 00:00:51,760
Unlike database is this data doesn't have a defined scheme up here.

17
00:00:51,760 --> 00:00:55,740
We're talking about things like images, PDFs or videos,

18
00:00:55,740 --> 00:00:59,390
and then you might also have semi structured data that's kind of in between.

19
00:00:59,390 --> 00:01:02,540
There's a little bit of structure, but not enough to go on a database.

20
00:01:02,540 --> 00:01:07,440
This includes things like no SQL CSV files and JSON code,

21
00:01:07,440 --> 00:01:09,940
and all of that needs to get into sage maker.

22
00:01:09,940 --> 00:01:12,840
But how exactly does that part work?

23
00:01:12,840 --> 00:01:16,140
Well, you actually need to get your data into S3 first,

24
00:01:16,140 --> 00:01:20,340
and then S3 serves up that data as input into sage maker.

25
00:01:20,340 --> 00:01:22,500
Just a quick word about the asterisk here.

26
00:01:22,500 --> 00:01:26,620
In late 2019, we got the ability to use FSX for luster,

27
00:01:26,620 --> 00:01:28,740
which is a file system services.

28
00:01:28,740 --> 00:01:31,240
This will serve a pure S3 data at higher speeds.

29
00:01:31,240 --> 00:01:33,240
So you get better performance,

30
00:01:33,240 --> 00:01:36,550
and then if your data is already in elastic file system or AD FS,

31
00:01:36,550 --> 00:01:39,040
you can use that Azure data sources well,

32
00:01:39,040 --> 00:01:42,040
but we're going to be using S3 for this course.

33
00:01:42,040 --> 00:01:47,740
So this piece about getting your data in to S3, how exactly does that part work?

34
00:01:47,740 --> 00:01:52,040
Well, if you have a lot of different data formats, a data lake is the way to go.

35
00:01:52,040 --> 00:01:55,330
AWS Lake Formation was released in August 2019,

36
00:01:55,330 --> 00:01:58,040
and it's Amazons Data Lake solution.

37
00:01:58,040 --> 00:02:00,330
It's actually built on top of AWS glue.

38
00:02:00,330 --> 00:02:02,550
If you're familiar with that which is an extract

39
00:02:02,550 --> 00:02:06,840
transform and load or TLS solution, we'll see more about that later,

40
00:02:06,840 --> 00:02:10,470
but with like formation, you can fetch, organize and clean your data,

41
00:02:10,470 --> 00:02:12,910
and you can use S3 as storage for the data,

42
00:02:12,910 --> 00:02:15,440
which can then feed into sage maker.

43
00:02:15,440 --> 00:02:18,180
And the information is good for large enterprise solutions,

44
00:02:18,180 --> 00:02:21,140
where you have data in a lot of formats and a lot of places,

45
00:02:21,140 --> 00:02:23,040
but you don't have to use it.

46
00:02:23,040 --> 00:02:25,380
If you're working with a symbol or small dataset,

47
00:02:25,380 --> 00:02:27,190
maybe a single CSV file,

48
00:02:27,190 --> 00:02:31,790
then just upload that directly to S3 and runs age maker against it.

49
00:02:31,790 --> 00:02:34,250
There are also other services you could use instead

50
00:02:34,250 --> 00:02:36,540
or in addition to lake formation.

51
00:02:36,540 --> 00:02:40,690
And the service depends on the type of ingestion you need to do there.

52
00:02:40,690 --> 00:02:41,940
Two types.

53
00:02:41,940 --> 00:02:45,260
The first is batch processing with batch processing.

54
00:02:45,260 --> 00:02:48,520
UWP periodically collect and send data in batches.

55
00:02:48,520 --> 00:02:50,140
As the name implies,

56
00:02:50,140 --> 00:02:54,840
You could do this ingestion based on some triggering event or on a set schedule,

57
00:02:54,840 --> 00:02:56,870
And this is the recommended approach when you don't really

58
00:02:56,870 --> 00:02:59,240
have a need for real time processing.

59
00:02:59,240 --> 00:03:02,690
In other words, if you get your data every couple hours or once a day,

60
00:03:02,690 --> 00:03:04,140
then that's just fine.

61
00:03:04,140 --> 00:03:08,940
This option is generally going to be cheaper and easier than stream processing.

62
00:03:08,940 --> 00:03:11,610
Now, if you've worked with AWS for any length of time,

63
00:03:11,610 --> 00:03:13,550
you know there are multiple ways of doing things,

64
00:03:13,550 --> 00:03:15,440
and that's definitely true here.

65
00:03:15,440 --> 00:03:19,340
There are several different services you can use for batch processing.

66
00:03:19,340 --> 00:03:23,740
The first is A W s glue, which is a fully managed detail services.

67
00:03:23,740 --> 00:03:25,850
We'll talk more about the transform part later,

68
00:03:25,850 --> 00:03:28,190
but this could be a great way to get the data from wherever

69
00:03:28,190 --> 00:03:32,540
it is in to S3 more to other AWS services.

70
00:03:32,540 --> 00:03:36,440
This is a serve URL is solution, and it runs on Apache Spark.

71
00:03:36,440 --> 00:03:38,740
And it works by setting up crawlers that will crawl your

72
00:03:38,740 --> 00:03:41,320
data source and try to infer the schema.

73
00:03:41,320 --> 00:03:41,940
Then it stores.

74
00:03:41,940 --> 00:03:45,540
Those schema is in what's called the data catalog.

75
00:03:45,540 --> 00:03:50,540
Data Pipeline is a managed orchestration service for data driven work flows.

76
00:03:50,540 --> 00:03:53,770
This lets you move your data between AWS compute and storage.

77
00:03:53,770 --> 00:03:58,140
Resource is, and you can also move things from on Prem to AWS.

78
00:03:58,140 --> 00:04:01,460
This lets you store data and dynamodb AD DS,

79
00:04:01,460 --> 00:04:03,450
which is the relational database service,

80
00:04:03,450 --> 00:04:06,940
or Red Shift, which is a data warehouse and S3.

81
00:04:06,940 --> 00:04:09,280
Finally, we have the database Migration Service or D M.

82
00:04:09,280 --> 00:04:11,390
s as you might guess from the name.

83
00:04:11,390 --> 00:04:16,480
This is used to migrate data between databases either in AWS or on Prem.

84
00:04:16,480 --> 00:04:18,740
With this, you can do homogeneous migrations,

85
00:04:18,740 --> 00:04:21,720
say an on Prem Oracle database to Amazon.

86
00:04:21,720 --> 00:04:26,930
RDS for Oracle or heterogeneous jobs like Oracle to Aurora and D.

87
00:04:26,930 --> 00:04:29,740
M s manages the infrastructure for you.

88
00:04:29,740 --> 00:04:32,040
The second type of data ingestion you could do is stream

89
00:04:32,040 --> 00:04:34,460
processing rather than running this once a day.

90
00:04:34,460 --> 00:04:35,090
For instance,

91
00:04:35,090 --> 00:04:40,260
this is real time processing data is loaded and manipulated as it's recognized,

92
00:04:40,260 --> 00:04:43,740
and the way that's possible is through constant monitoring.

93
00:04:43,740 --> 00:04:48,340
You want to use this one real time data is required, such as for stock prices.

94
00:04:48,340 --> 00:04:49,960
This is gonna be the more expensive option,

95
00:04:49,960 --> 00:04:53,240
though, as compared to batch processing.

96
00:04:53,240 --> 00:04:56,860
Any time you think of stream processing, think of Amazon Kinesis.

97
00:04:56,860 --> 00:04:59,140
That's really what it's all about.

98
00:04:59,140 --> 00:05:01,130
Therefore, services in the can is is family,

99
00:05:01,130 --> 00:05:03,480
which you'll see here and let me highlight just a few

100
00:05:03,480 --> 00:05:06,130
things about each of them video streams.

101
00:05:06,130 --> 00:05:07,210
As the name implies.

102
00:05:07,210 --> 00:05:12,140
This is for ingesting processing and storing streaming video and audio.

103
00:05:12,140 --> 00:05:14,950
You can also stream video and audio into an application with this

104
00:05:14,950 --> 00:05:17,750
service and this will automatically provisioned and scale the

105
00:05:17,750 --> 00:05:21,360
infrastructure for you with data streams.

106
00:05:21,360 --> 00:05:25,940
You're also working with streaming data and here the data is broken into shards.

107
00:05:25,940 --> 00:05:30,410
One key differentiator with data streams is that the data has to be processed,

108
00:05:30,410 --> 00:05:35,040
maybe using a lambda function or data analytics before it could be stored.

109
00:05:35,040 --> 00:05:38,060
You can't use data streams to store directly into S3,

110
00:05:38,060 --> 00:05:41,070
for instance, and storage it all is optional.

111
00:05:41,070 --> 00:05:44,940
You might just want to run a lamb to function on the data and then toss it out.

112
00:05:44,940 --> 00:05:50,640
Data retention is 24 hours by default, so you can increase that up to seven days.

113
00:05:50,640 --> 00:05:53,080
Data firehose is similar to data streams,

114
00:05:53,080 --> 00:05:55,840
but here you aren't dealing with shards.

115
00:05:55,840 --> 00:05:59,060
And with this you can stream the data directly into storage,

116
00:05:59,060 --> 00:06:01,940
and the processing part is optional.

117
00:06:01,940 --> 00:06:05,410
You do get automatic re tries here if your data delivery fails,

118
00:06:05,410 --> 00:06:09,340
but if it fails for more than 24 hours, the data is discarded.

119
00:06:09,340 --> 00:06:10,840
So if data attention's important,

120
00:06:10,840 --> 00:06:13,680
this is probably not the right option for you and

121
00:06:13,680 --> 00:06:17,050
Data Analytics with this service, your analyzing streaming data.

122
00:06:17,050 --> 00:06:19,070
So you have a bunch of stores around the world that

123
00:06:19,070 --> 00:06:22,880
are streaming their sales data in, and you need to analyze that in real times.

124
00:06:22,880 --> 00:06:26,040
You can make adjustments to promotions throughout the day.

125
00:06:26,040 --> 00:06:28,800
Here again, the underlying infrastructure is handled for you,

126
00:06:28,800 --> 00:06:32,080
and you're able to use SQL queries on the data and or

127
00:06:32,080 --> 00:06:34,760
build Custom Java applications building.

128
00:06:34,760 --> 00:06:38,420
On top of that last point of being able to query your data using SQL,

129
00:06:38,420 --> 00:06:41,940
there's two other services you should know about for the same thing.

130
00:06:41,940 --> 00:06:42,290
Amazon,

131
00:06:42,290 --> 00:06:47,640
Athena and Red Shift Spectrum both let you run SQL queries on your data in S3.

132
00:06:47,640 --> 00:06:50,800
The main difference between the two is that with red shift spectrum,

133
00:06:50,800 --> 00:06:53,240
you'll need to set up a red shift cluster.

134
00:06:53,240 --> 00:06:56,490
So if you're already using red shift for other things, this is a good option.

135
00:06:56,490 --> 00:06:58,440
But otherwise it's going to be overkill,

136
00:06:58,440 --> 00:07:02,480
and you'll want to go with Athena and then one other service to be familiar

137
00:07:02,480 --> 00:07:07,110
with for the exam is Amazon elastic Map Reduce or EMR e.

138
00:07:07,110 --> 00:07:07,230
M.

139
00:07:07,230 --> 00:07:10,190
R is based on Apache Hadoop and allows you to process

140
00:07:10,190 --> 00:07:12,430
massive amounts of data across multiple E.

141
00:07:12,430 --> 00:07:12,600
C.

142
00:07:12,600 --> 00:07:13,800
Two instances.

143
00:07:13,800 --> 00:07:16,440
It's all about big, big data.

144
00:07:16,440 --> 00:07:18,670
It works with various open source frameworks like

145
00:07:18,670 --> 00:07:21,680
Apache spark hive Presto and more.

146
00:07:21,680 --> 00:07:25,740
And there's tight integration for sparks ml lib capabilities.

147
00:07:25,740 --> 00:07:27,960
If your data is already in an EMR cluster,

148
00:07:27,960 --> 00:07:33,000
you can use some of those other services we've talked about to get the data and to S3.

