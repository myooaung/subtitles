1
00:00:01,340 --> 00:00:02,290
[Autogenerated] Well, hi there.

2
00:00:02,290 --> 00:00:04,150
You've made it to the next module in the course.

3
00:00:04,150 --> 00:00:05,290
Thanks for sticking with me.

4
00:00:05,290 --> 00:00:08,720
I'm Amber is real JSON In the last module,

5
00:00:08,720 --> 00:00:11,030
we got a good look at how to fetch and prepare your data.

6
00:00:11,030 --> 00:00:16,440
We cleaned, we analyzed, we transformed, visualized and feature engineered.

7
00:00:16,440 --> 00:00:19,960
The next step is feeding that data to an algorithm to train and then

8
00:00:19,960 --> 00:00:24,260
we'll evaluate how we did in this module will tackle the next two

9
00:00:24,260 --> 00:00:26,440
steps of the machine learning process.

10
00:00:26,440 --> 00:00:27,320
To train the model,

11
00:00:27,320 --> 00:00:31,540
we first need to talk about the algorithms to use and when to use them.

12
00:00:31,540 --> 00:00:34,200
After training the model, we need to see how accurate it was.

13
00:00:34,200 --> 00:00:36,370
And we'll tweak things until it's just right.

14
00:00:36,370 --> 00:00:39,240
And we do that through hyper parameter tuning.

15
00:00:39,240 --> 00:00:39,880
As always,

16
00:00:39,880 --> 00:00:42,370
we'll go through all of these steps conceptually and then see

17
00:00:42,370 --> 00:00:46,140
them in action in a demo in Sage Maker studio.

18
00:00:46,140 --> 00:00:49,880
Let's dive right into the training part in the machine learning process.

19
00:00:49,880 --> 00:00:52,320
We're moving right along to train a model.

20
00:00:52,320 --> 00:00:56,060
We need an algorithm and let me pause just right there To make this point,

21
00:00:56,060 --> 00:00:59,650
you'll sometimes hear people using the word algorithm and model interchangeably.

22
00:00:59,650 --> 00:01:04,450
But that's not totally right going back to this diagram we've seen a few times.

23
00:01:04,450 --> 00:01:07,950
We, of course, start with our training data and then the algorithm.

24
00:01:07,950 --> 00:01:12,040
The code is going to run on that data and try to identify patterns.

25
00:01:12,040 --> 00:01:16,380
The output of all of that is called the model, so the model is really the rules.

26
00:01:16,380 --> 00:01:18,960
The numbers that data structures required to make

27
00:01:18,960 --> 00:01:22,140
predictions is far as algorithms go.

28
00:01:22,140 --> 00:01:23,320
There are a lot of them.

29
00:01:23,320 --> 00:01:26,140
I've listed just a few here.

30
00:01:26,140 --> 00:01:28,310
If you go out to the sage maker Developer guide,

31
00:01:28,310 --> 00:01:29,990
you'll see the section for algorithms here.

32
00:01:29,990 --> 00:01:31,290
Choose an algorithm.

33
00:01:31,290 --> 00:01:34,340
There are built in algorithms in sage maker.

34
00:01:34,340 --> 00:01:37,740
You'll notice there are quite a few to choose from Here.

35
00:01:37,740 --> 00:01:40,650
You can also use machine learning frameworks and use

36
00:01:40,650 --> 00:01:43,340
your own algorithms or models.

37
00:01:43,340 --> 00:01:46,660
You definitely won't need to know every detail of everything here for the exam.

38
00:01:46,660 --> 00:01:49,970
But I would recommend going through these built in algorithms and

39
00:01:49,970 --> 00:01:54,440
understanding what they do at a high level and how they work.

40
00:01:54,440 --> 00:01:56,250
These algorithm you choose is going to depend on the

41
00:01:56,250 --> 00:01:58,340
type of problem you're trying to solve.

42
00:01:58,340 --> 00:02:00,520
Going back to this diagram from earlier.

43
00:02:00,520 --> 00:02:02,440
They're gonna be different algorithms for supervised

44
00:02:02,440 --> 00:02:04,640
learning and unsupervised learning.

45
00:02:04,640 --> 00:02:08,370
You saw that full list a minute ago in the Developer Guide for our purposes.

46
00:02:08,370 --> 00:02:10,840
I'm just going to hit on these five built in sage maker

47
00:02:10,840 --> 00:02:13,890
algorithms that are fairly common for a variety of machine

48
00:02:13,890 --> 00:02:17,070
learning problems on the supervised learning side,

49
00:02:17,070 --> 00:02:19,740
let's start with linear learner.

50
00:02:19,740 --> 00:02:23,970
Linear learner is used for classification and regression problems as an

51
00:02:23,970 --> 00:02:28,240
example for a classifications problem for a given value X,

52
00:02:28,240 --> 00:02:32,100
what is why or based on past shopping habits?

53
00:02:32,100 --> 00:02:34,940
Will this customer buy this product?

54
00:02:34,940 --> 00:02:35,970
And over on the left here,

55
00:02:35,970 --> 00:02:39,620
you'll see that we have some values for X and y and the algorithms going

56
00:02:39,620 --> 00:02:43,630
to try to figure out what why is when X equals three or try to figure

57
00:02:43,630 --> 00:02:48,220
out a formula like this one here for a regression problems something

58
00:02:48,220 --> 00:02:52,040
like this for a given X Predict why,

59
00:02:52,040 --> 00:02:54,510
like the example we saw earlier in the course about predicting

60
00:02:54,510 --> 00:02:57,740
rent for a three bedroom vacation rental.

61
00:02:57,740 --> 00:02:58,910
In both of these cases,

62
00:02:58,910 --> 00:03:02,130
these algorithm is trying to fit the data to a line and is

63
00:03:02,130 --> 00:03:04,710
trying to minimize the error or minimize the distance

64
00:03:04,710 --> 00:03:07,240
between the data points on the line.

65
00:03:07,240 --> 00:03:11,640
One way it could do that is using what's called stochastic radiant descent or S.

66
00:03:11,640 --> 00:03:11,870
G.

67
00:03:11,870 --> 00:03:13,440
D.

68
00:03:13,440 --> 00:03:15,840
Grady in dissent is used frequently a machine learning.

69
00:03:15,840 --> 00:03:18,640
So let me at least give you an intuition for it.

70
00:03:18,640 --> 00:03:20,800
You'll hear this thing called a loss function,

71
00:03:20,800 --> 00:03:22,700
or sometimes called a cost function,

72
00:03:22,700 --> 00:03:26,040
which is basically how good your predictions are.

73
00:03:26,040 --> 00:03:26,610
In other words,

74
00:03:26,610 --> 00:03:30,670
how far away are your predicted values from the actual values and

75
00:03:30,670 --> 00:03:34,540
you want to minimize thedc ost or the errors.

76
00:03:34,540 --> 00:03:38,080
So for this cost function, the idea is to find the optimal point,

77
00:03:38,080 --> 00:03:41,040
which is going to be the lowest point on the curve.

78
00:03:41,040 --> 00:03:44,160
An ingredient descent algorithm is going to do just that.

79
00:03:44,160 --> 00:03:48,130
It will descend this Grady int iterating until it finds the lowest point,

80
00:03:48,130 --> 00:03:52,140
the point where you've minimized the errors in your predictions

81
00:03:52,140 --> 00:03:53,730
to tie that to something in the real world.

82
00:03:53,730 --> 00:03:55,690
If you're a hiker, you'll understand the concept.

83
00:03:55,690 --> 00:03:57,610
Here you're on top of a mountain,

84
00:03:57,610 --> 00:04:00,430
a storm moves in and you need to get to the bottom of the mountain.

85
00:04:00,430 --> 00:04:01,310
A SAP.

86
00:04:01,310 --> 00:04:04,150
So you're going to descend, you're gonna find the Grady into,

87
00:04:04,150 --> 00:04:07,340
or the slope that'll get you down the fastest.

88
00:04:07,340 --> 00:04:10,440
This is the idea behind radiant descent.

89
00:04:10,440 --> 00:04:10,990
Next up,

90
00:04:10,990 --> 00:04:13,530
we have the Xcode boost algorithm and this is the one that

91
00:04:13,530 --> 00:04:15,980
we're gonna be using in our upcoming demo.

92
00:04:15,980 --> 00:04:19,740
This is also used for classification and regression problems.

93
00:04:19,740 --> 00:04:23,540
And this is an implementation of Grady int boosted trees.

94
00:04:23,540 --> 00:04:26,780
It's designed for speed and performance and is very popular today,

95
00:04:26,780 --> 00:04:29,540
kind of a multipurpose type of algorithm.

96
00:04:29,540 --> 00:04:30,120
Essentially,

97
00:04:30,120 --> 00:04:32,740
this tries to make predictions by attempting to correct

98
00:04:32,740 --> 00:04:35,740
the mistakes of the models before it.

99
00:04:35,740 --> 00:04:37,620
You're going to start with a training dataset,

100
00:04:37,620 --> 00:04:40,640
some positive points and some negative points.

101
00:04:40,640 --> 00:04:43,640
The first classifier is going to say Okay,

102
00:04:43,640 --> 00:04:46,950
all these things here with a green background are positive

103
00:04:46,950 --> 00:04:48,560
and everything on the blue background.

104
00:04:48,560 --> 00:04:52,240
It's negative, but obviously that's not quite right.

105
00:04:52,240 --> 00:04:56,240
These three plus is here are incorrectly identified.

106
00:04:56,240 --> 00:04:59,670
So those three points are going to be boosted or given more

107
00:04:59,670 --> 00:05:03,240
wait and passed on to the second classifier.

108
00:05:03,240 --> 00:05:04,270
This one says okay,

109
00:05:04,270 --> 00:05:06,840
things on the blue background or negative and things on

110
00:05:06,840 --> 00:05:09,460
the green background are positive again,

111
00:05:09,460 --> 00:05:11,840
we got it wrong in a few cases,

112
00:05:11,840 --> 00:05:16,440
so those are going to get boosted and we continue to the next classifier.

113
00:05:16,440 --> 00:05:19,060
We have a couple of more incorrect points,

114
00:05:19,060 --> 00:05:21,540
but we're eventually going to correct all the mistakes and come

115
00:05:21,540 --> 00:05:24,340
up with something that's correct in the end.

116
00:05:24,340 --> 00:05:27,840
So that's the idea behind the algorithm.

117
00:05:27,840 --> 00:05:31,290
The next algorithm is also used for classification and regression problems,

118
00:05:31,290 --> 00:05:35,540
and this is the K nearest neighbors algorithm or K N n.

119
00:05:35,540 --> 00:05:40,040
This makes predictions based on the K points closest to the sample point.

120
00:05:40,040 --> 00:05:44,640
For example, in this diagram, the sample point is the great question mark.

121
00:05:44,640 --> 00:05:49,000
If K equals to this algorithm, will search for the two closest neighbors,

122
00:05:49,000 --> 00:05:51,040
and these are orange dots.

123
00:05:51,040 --> 00:05:53,080
If we increase Kate equal five, though,

124
00:05:53,080 --> 00:05:55,620
it's gonna look at the five closest neighbors to that point,

125
00:05:55,620 --> 00:06:00,240
in which case that makes us a blue dot Can N is commonly used for concept,

126
00:06:00,240 --> 00:06:05,140
search, image classification and recommendation systems.

127
00:06:05,140 --> 00:06:07,230
So that does it for the common supervised learning

128
00:06:07,230 --> 00:06:10,200
algorithms now onto supervised over on the right.

129
00:06:10,200 --> 00:06:14,430
Starting with K means this one is used for clustering problems

130
00:06:14,430 --> 00:06:17,340
where it groups data based on similarity.

131
00:06:17,340 --> 00:06:19,040
The K here is a number of groups.

132
00:06:19,040 --> 00:06:20,780
It'll place things into,

133
00:06:20,780 --> 00:06:24,640
and you have to supply the attributes that indicates similarity.

134
00:06:24,640 --> 00:06:28,040
Let's walk through an example to give you some intuition on this one.

135
00:06:28,040 --> 00:06:31,740
Let's say you've been tasked with organizing a pile of books.

136
00:06:31,740 --> 00:06:37,240
You have three bookshelves or groups, so RK here equals three.

137
00:06:37,240 --> 00:06:39,280
We're going to say that they should be grouped by genre.

138
00:06:39,280 --> 00:06:41,630
So that's these similarity attribute,

139
00:06:41,630 --> 00:06:46,540
as opposed to grouping by size or number of pages or something else.

140
00:06:46,540 --> 00:06:49,940
So we're going to take our first book and place it on the shelf.

141
00:06:49,940 --> 00:06:54,140
Turns out that book is nonfiction, so that's going to be the name of the shelf.

142
00:06:54,140 --> 00:06:56,490
Note that we didn't name the shelf first,

143
00:06:56,490 --> 00:07:00,740
but we got the name based on the genre of the first book that we placed on it.

144
00:07:00,740 --> 00:07:03,430
We'll take the next book, Place it on the second shelf.

145
00:07:03,430 --> 00:07:05,940
This one is fiction, so that's sitting with that shelf,

146
00:07:05,940 --> 00:07:08,440
and the third book will be biography.

147
00:07:08,440 --> 00:07:12,240
So we have our three or K groups.

148
00:07:12,240 --> 00:07:13,930
Then we'll take the rest of the books and place them into

149
00:07:13,930 --> 00:07:17,140
the group where they belong by genre.

150
00:07:17,140 --> 00:07:17,960
Earlier in the course,

151
00:07:17,960 --> 00:07:22,240
we saw this slide that helped us cluster our customers into different segments.

152
00:07:22,240 --> 00:07:22,410
Well,

153
00:07:22,410 --> 00:07:28,490
this is the algorithm that does that work K means the next and final algorithm

154
00:07:28,490 --> 00:07:30,860
we're going to talk about is principal component analysis,

155
00:07:30,860 --> 00:07:32,500
or PCA.

156
00:07:32,500 --> 00:07:34,960
This is used on unsupervised learning problems to

157
00:07:34,960 --> 00:07:37,440
solve the curse of dimensionality.

158
00:07:37,440 --> 00:07:39,270
That's a phrase you'll sometimes hear to mean that

159
00:07:39,270 --> 00:07:43,240
you just have to many features, and it's hard to know what's important.

160
00:07:43,240 --> 00:07:46,030
So this algorithm will identify important relationships in

161
00:07:46,030 --> 00:07:49,320
the data and quantifies the importance so that you know

162
00:07:49,320 --> 00:07:51,840
what to keep and what to drop.

163
00:07:51,840 --> 00:07:52,380
In short,

164
00:07:52,380 --> 00:07:54,830
you're going from something with lots and lots of features and

165
00:07:54,830 --> 00:07:58,640
whittling it down to only the most important.

166
00:07:58,640 --> 00:08:01,190
All right, so we've walked through some common algorithms,

167
00:08:01,190 --> 00:08:05,140
and now we want to move on to training so we could get a model.

168
00:08:05,140 --> 00:08:07,680
But wait, there is a very important step here,

169
00:08:07,680 --> 00:08:10,230
and that's to split your data into training,

170
00:08:10,230 --> 00:08:13,240
validation and test sets.

171
00:08:13,240 --> 00:08:15,270
The reason for doing this is because if you passed in

172
00:08:15,270 --> 00:08:17,940
100% of your data and trained on that,

173
00:08:17,940 --> 00:08:20,130
you have no data left to test the model and see if

174
00:08:20,130 --> 00:08:23,040
it's actually accurate on new data.

175
00:08:23,040 --> 00:08:25,770
Now here I'm using a split of 70 2010,

176
00:08:25,770 --> 00:08:29,990
but you'll sometimes see 80 10 10 or 70 15 15.

177
00:08:29,990 --> 00:08:32,860
But regardless, you need a set of data to train the model on.

178
00:08:32,860 --> 00:08:35,600
Initially, that's our 70%.

179
00:08:35,600 --> 00:08:38,500
The validation set comes into play Azure doing hyper parameter

180
00:08:38,500 --> 00:08:41,040
tuning and trying to optimize the model.

181
00:08:41,040 --> 00:08:43,770
And then that third set the test set is to test the

182
00:08:43,770 --> 00:08:47,340
final model that you decided to go with.

183
00:08:47,340 --> 00:08:49,780
Okay, so we've clean, prepared and split the data.

184
00:08:49,780 --> 00:08:54,140
We know what algorithm to use now we can create the training job.

185
00:08:54,140 --> 00:08:55,210
To do that in sage Maker,

186
00:08:55,210 --> 00:08:59,520
there's three steps you need to specify the girls for the S3 bucket,

187
00:08:59,520 --> 00:09:01,340
where your training data lives,

188
00:09:01,340 --> 00:09:04,850
as well as the S3 bucket to store output or artifacts that

189
00:09:04,850 --> 00:09:07,740
are created during the training process.

190
00:09:07,740 --> 00:09:11,480
Next, specify the algorithm you want to use and the compute or E C.

191
00:09:11,480 --> 00:09:13,940
Two instances to use for the training.

192
00:09:13,940 --> 00:09:17,120
And finally, you need to specify the path to the training code in E.

193
00:09:17,120 --> 00:09:17,350
C.

194
00:09:17,350 --> 00:09:17,690
R.

195
00:09:17,690 --> 00:09:20,440
Or Elastic container Registry.

196
00:09:20,440 --> 00:09:22,710
You can do all of this through the sage maker console,

197
00:09:22,710 --> 00:09:25,540
and it kind of walks you through what you need to do.

198
00:09:25,540 --> 00:09:28,020
But you can also do it through code and a Jupyter notebook,

199
00:09:28,020 --> 00:09:35,000
which is what we're going to be doing in a demo. For now, let's take a look at evaluating our model

