1
00:00:02,040 --> 00:00:02,430
[Autogenerated] Okay,

2
00:00:02,430 --> 00:00:05,050
Now that we've walked through how to train and evaluate your model,

3
00:00:05,050 --> 00:00:08,640
let's go do that in Sage Maker studio.

4
00:00:08,640 --> 00:00:09,510
Just a reminder.

5
00:00:09,510 --> 00:00:11,810
If you shut down your notebook kernel on instance,

6
00:00:11,810 --> 00:00:14,710
after the last demo, you'll need to start it back up and run the code.

7
00:00:14,710 --> 00:00:16,340
Up to this point,

8
00:00:16,340 --> 00:00:18,960
I'm about halfway down in the notebook just below the

9
00:00:18,960 --> 00:00:21,150
scatter matrix that we did last time.

10
00:00:21,150 --> 00:00:25,600
And right down here in this paragraph, we are determining which algorithm to use.

11
00:00:25,600 --> 00:00:27,680
So let's pick up there.

12
00:00:27,680 --> 00:00:30,300
This example uses these Xcode boost algorithm,

13
00:00:30,300 --> 00:00:32,770
which we talked about a little bit earlier in the module.

14
00:00:32,770 --> 00:00:35,700
This is a good general purpose algorithm that handles nonlinear

15
00:00:35,700 --> 00:00:39,340
relationships like we have if we go back to the developer guide

16
00:00:39,340 --> 00:00:43,250
and look at the documentation for this algorithm on the top level

17
00:00:43,250 --> 00:00:44,350
page for the algorithm.

18
00:00:44,350 --> 00:00:48,980
If you scroll down to input and output,

19
00:00:48,980 --> 00:00:53,440
you'll see the is takes in data in CSV or lib S V M format.

20
00:00:53,440 --> 00:00:55,900
We have CSV, so that'll work great.

21
00:00:55,900 --> 00:00:58,540
So back to our notebook,

22
00:00:58,540 --> 00:01:00,940
there's actually one more step here that we need to do to convert

23
00:01:00,940 --> 00:01:03,160
our categorical features into numeric features.

24
00:01:03,160 --> 00:01:04,630
We did some of that last time.

25
00:01:04,630 --> 00:01:07,030
But let's run this one block of code here to finish that

26
00:01:07,030 --> 00:01:10,990
work apps and then scrolling down.

27
00:01:10,990 --> 00:01:14,680
This next block of code is splitting the data into training data,

28
00:01:14,680 --> 00:01:17,520
validation data and test data like we talked about.

29
00:01:17,520 --> 00:01:19,540
So we'll run this.

30
00:01:19,540 --> 00:01:21,480
And then the final thing we need to do before we can

31
00:01:21,480 --> 00:01:23,560
train is to get our data into S3.

32
00:01:23,560 --> 00:01:26,740
So I'll run this block of code, which is doing that.

33
00:01:26,740 --> 00:01:30,140
And now we can move down to the training section.

34
00:01:30,140 --> 00:01:32,300
You recall from the slides that there's a few things we have

35
00:01:32,300 --> 00:01:34,940
to specify when we create the training job.

36
00:01:34,940 --> 00:01:37,690
The first thing we're doing here is getting the location of the container

37
00:01:37,690 --> 00:01:40,940
in the elastic container registry to run Xcode boost.

38
00:01:40,940 --> 00:01:43,940
So I'll run this next block of code.

39
00:01:43,940 --> 00:01:46,690
You might get a warning here about there being a more up to date version,

40
00:01:46,690 --> 00:01:48,190
but for what we're doing, that's fine.

41
00:01:48,190 --> 00:01:54,640
We can move on next 22.2 the CSV files that we just uploaded to S3.

42
00:01:54,640 --> 00:01:56,890
These two lines of code here are doing that for us,

43
00:01:56,890 --> 00:02:00,540
one for training data and one for validation data.

44
00:02:00,540 --> 00:02:03,360
Next up, we scroll down here.

45
00:02:03,360 --> 00:02:07,440
Here, you'll see a little bit more information about some key hyper parameters,

46
00:02:07,440 --> 00:02:10,940
but let me scroll down to the code section here.

47
00:02:10,940 --> 00:02:13,170
Some set up work here on top with estimator.

48
00:02:13,170 --> 00:02:15,420
This is where we're specifying the instance.

49
00:02:15,420 --> 00:02:18,020
We want to train on how many and where the output

50
00:02:18,020 --> 00:02:21,340
should do and then our hyper parameters.

51
00:02:21,340 --> 00:02:23,580
There's obviously a lot more than what we see here for

52
00:02:23,580 --> 00:02:24,980
anything that you don't specify.

53
00:02:24,980 --> 00:02:28,820
Sage maker will use the default values, and you'll also notice the objective.

54
00:02:28,820 --> 00:02:32,540
Metric, we're specifying, is binary logistic?

55
00:02:32,540 --> 00:02:35,050
This tells the algorithm a kind of problem or trying to solve.

56
00:02:35,050 --> 00:02:36,450
And remember, ours is binary.

57
00:02:36,450 --> 00:02:40,740
We're trying to solve customer churn, true or false.

58
00:02:40,740 --> 00:02:42,800
And then the fit function is the thing that actually

59
00:02:42,800 --> 00:02:48,040
creates and runs these training job, using both our training and validation data.

60
00:02:48,040 --> 00:02:51,040
So let's run this block of code.

61
00:02:51,040 --> 00:02:51,220
Now.

62
00:02:51,220 --> 00:02:55,310
Sage maker is going out and launching the instance we specified handling

63
00:02:55,310 --> 00:02:58,040
everything on the back end that we need to run the job.

64
00:02:58,040 --> 00:03:03,240
This will likely take a few minutes, so I'll pause the video and come back.

65
00:03:03,240 --> 00:03:03,600
Okay,

66
00:03:03,600 --> 00:03:07,150
you'll see the training job has completed now Let's scroll

67
00:03:07,150 --> 00:03:09,940
up and see exactly what happened here.

68
00:03:09,940 --> 00:03:13,840
Go up to where we started.

69
00:03:13,840 --> 00:03:18,400
You'll see starting the training job, launching the M L instances,

70
00:03:18,400 --> 00:03:21,540
preparing them for training, downloading the input data,

71
00:03:21,540 --> 00:03:24,470
getting that out of S3 and then downloading the training

72
00:03:24,470 --> 00:03:27,140
image from the container registry.

73
00:03:27,140 --> 00:03:29,410
And then when it starts running, you'll see a bunch of output.

74
00:03:29,410 --> 00:03:33,000
Here in blue, I'll call out just a couple of things here,

75
00:03:33,000 --> 00:03:36,640
you'll see Train are and validation error.

76
00:03:36,640 --> 00:03:38,860
These are evaluation metrics that are computed by the

77
00:03:38,860 --> 00:03:41,670
algorithm as its training is calculating.

78
00:03:41,670 --> 00:03:44,370
This on both the training data and the validation data,

79
00:03:44,370 --> 00:03:47,830
and it's trying to minimize that error you'll see appear at the top

80
00:03:47,830 --> 00:03:52,500
for train are we have 0.45 for validation error,

81
00:03:52,500 --> 00:03:58,340
0.73 and then as we go, those errors get a little bit smaller.

82
00:03:58,340 --> 00:04:04,720
You'll see at the bottom 0.39 and 0.67 And that continues until

83
00:04:04,720 --> 00:04:09,040
we get down to the very bottom scrolling here.

84
00:04:09,040 --> 00:04:15,810
And the last values that were calculated are 0.24 and 0.66 In other words,

85
00:04:15,810 --> 00:04:21,440
training are is about 2% and validation are is about 6%.

86
00:04:21,440 --> 00:04:24,310
If you do one minus that are accuracy on.

87
00:04:24,310 --> 00:04:29,240
The training is about 98% and for validation, about 94%.

88
00:04:29,240 --> 00:04:32,640
So we did a little bit better on the training data than the validation data.

89
00:04:32,640 --> 00:04:34,150
If this was wildly different,

90
00:04:34,150 --> 00:04:39,330
say you had a 98% accuracy on training but 70% accuracy on validation.

91
00:04:39,330 --> 00:04:42,280
That could definitely be a signal that the model has over fitted.

92
00:04:42,280 --> 00:04:45,010
And we need to make some modifications that we talked about earlier.

93
00:04:45,010 --> 00:04:47,520
Like getting more data or removing features or

94
00:04:47,520 --> 00:04:49,730
stopping early source could be better.

95
00:04:49,730 --> 00:04:51,820
But for the level of evaluation we're doing,

96
00:04:51,820 --> 00:04:54,900
we're going to say we're happy with that and move on and

97
00:04:54,900 --> 00:04:57,520
incidently all of this logging that we see up above.

98
00:04:57,520 --> 00:04:59,430
This is all logged into CloudWatch as well,

99
00:04:59,430 --> 00:05:02,940
so you could go back and see what happened during the training job.

100
00:05:02,940 --> 00:05:06,370
At the end, you'll notice it uploaded the model and completed the training job.

101
00:05:06,370 --> 00:05:07,990
Overall, it took 47 seconds,

102
00:05:07,990 --> 00:05:12,760
and that's how much I'll be billed for this next section here for compile.

103
00:05:12,760 --> 00:05:16,800
This is about sage maker Neo I'm not gonna get too deep into neo in this course.

104
00:05:16,800 --> 00:05:20,200
Just know that neocon optimize your models for target hardware.

105
00:05:20,200 --> 00:05:24,440
This means you can run the same model on multiple platforms,

106
00:05:24,440 --> 00:05:27,940
so I'll run this block of code and let it do its thing.

107
00:05:27,940 --> 00:05:30,110
We're telling it to optimize for the MLM,

108
00:05:30,110 --> 00:05:31,530
for instance, family,

109
00:05:31,530 --> 00:05:34,250
and we're also giving it the output path to Are S3

110
00:05:34,250 --> 00:05:37,140
bucket to store the compiled model.

111
00:05:37,140 --> 00:05:39,490
Now let's go back and do a short summary of the module.

112
00:05:39,490 --> 00:05:46,000
If you don't plan to continue to the next model right away again, I'd recommend you shut down the kernel and instance for this notebook.

