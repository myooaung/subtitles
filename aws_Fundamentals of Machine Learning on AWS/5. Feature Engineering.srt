1
00:00:01,840 --> 00:00:04,840
[Autogenerated] Now we're up to a topic called Feature Engineering.

2
00:00:04,840 --> 00:00:08,240
This is the process of transforming raw data into features

3
00:00:08,240 --> 00:00:10,720
that better represent the underlying problem.

4
00:00:10,720 --> 00:00:12,750
You could do this by adding removing,

5
00:00:12,750 --> 00:00:16,330
combining features and encoding the data in those features.

6
00:00:16,330 --> 00:00:19,840
For instance, changing text into numerical values.

7
00:00:19,840 --> 00:00:23,440
And the goal here is to increase the predictive power of the model.

8
00:00:23,440 --> 00:00:27,140
I've got a sample dataset here for approval of auto loans.

9
00:00:27,140 --> 00:00:29,120
One thing to address during this feature engineering

10
00:00:29,120 --> 00:00:32,940
process is removal of unnecessary features.

11
00:00:32,940 --> 00:00:35,330
Now there are actually algorithms that can help with this.

12
00:00:35,330 --> 00:00:36,190
But up front,

13
00:00:36,190 --> 00:00:37,930
just go through your features and ask if there's

14
00:00:37,930 --> 00:00:39,480
anything that doesn't seem relevant.

15
00:00:39,480 --> 00:00:40,740
Prediction.

16
00:00:40,740 --> 00:00:42,840
Generally speaking, the simpler your dataset,

17
00:00:42,840 --> 00:00:44,720
the better performance you're going to get when it

18
00:00:44,720 --> 00:00:47,820
comes to training and predictions, for instance,

19
00:00:47,820 --> 00:00:50,640
with the size of the car really play a role in whether

20
00:00:50,640 --> 00:00:53,040
somebody was approved for the loan or not?

21
00:00:53,040 --> 00:00:54,640
Perhaps not.

22
00:00:54,640 --> 00:00:57,610
When in doubt, you want to check with the domain expert for the data,

23
00:00:57,610 --> 00:01:01,140
and they should be able to give you input on what to keep or what to drop.

24
00:01:01,140 --> 00:01:01,650
In our case,

25
00:01:01,650 --> 00:01:03,270
I'm actually gonna keep this one because I want to

26
00:01:03,270 --> 00:01:06,440
show you how to in code that column, assuming that you need it.

27
00:01:06,440 --> 00:01:11,240
But in general, make that first pass and remove anything that's unnecessary.

28
00:01:11,240 --> 00:01:14,140
Another thing you need to do is handle scale.

29
00:01:14,140 --> 00:01:14,790
For instance,

30
00:01:14,790 --> 00:01:19,740
you might have measurement data that is in inches kilometers and yards

31
00:01:19,740 --> 00:01:22,740
and scales for those are going to be very different.

32
00:01:22,740 --> 00:01:25,610
Or let's say you have features for age and income.

33
00:01:25,610 --> 00:01:31,570
Age might only be between 21 65 but salary could be 100,000 plus so

34
00:01:31,570 --> 00:01:34,940
very different scales or magnitudes for those numbers.

35
00:01:34,940 --> 00:01:37,310
Some algorithms don't perform well with big numbers like

36
00:01:37,310 --> 00:01:40,540
that and Italy to inaccurate results.

37
00:01:40,540 --> 00:01:41,690
They're a couple of ways to handle.

38
00:01:41,690 --> 00:01:44,150
This one is called Normalization,

39
00:01:44,150 --> 00:01:48,940
where you re scale the data so that your values are between zero and one.

40
00:01:48,940 --> 00:01:50,580
I won't get into all the formulas here,

41
00:01:50,580 --> 00:01:53,230
but there are built in functions for things like this in the machine

42
00:01:53,230 --> 00:01:57,800
learning libraries like scikit-learn for python and then standardization is

43
00:01:57,800 --> 00:02:01,710
where your re scaling thebe S3 BUE shin of the data so that the mean is

44
00:02:01,710 --> 00:02:05,170
zero and standard deviation is one again.

45
00:02:05,170 --> 00:02:07,020
The point to all of this is to ensure that you're

46
00:02:07,020 --> 00:02:09,640
treating your variables the same.

47
00:02:09,640 --> 00:02:11,840
We also need to encode the data.

48
00:02:11,840 --> 00:02:15,450
A lot of machine learning algorithms can only work with numerical data,

49
00:02:15,450 --> 00:02:18,750
not the different things that we have here in our dataset.

50
00:02:18,750 --> 00:02:19,740
We have our target,

51
00:02:19,740 --> 00:02:22,730
which is the thing that we're ultimately trying to predict in this case,

52
00:02:22,730 --> 00:02:25,040
whether the auto loan was approved or not.

53
00:02:25,040 --> 00:02:27,940
In that column, we have yes or no.

54
00:02:27,940 --> 00:02:31,560
We have another column of yes or no, which is these e used column,

55
00:02:31,560 --> 00:02:34,240
meaning it was a used automobile or not.

56
00:02:34,240 --> 00:02:37,170
And these are examples of binary, categorical values.

57
00:02:37,170 --> 00:02:41,240
There's two choices, yes or no, true or false, that kind of thing.

58
00:02:41,240 --> 00:02:43,800
So if you have binary values like in this example,

59
00:02:43,800 --> 00:02:46,780
it's really easy to change them into numbers like here.

60
00:02:46,780 --> 00:02:51,540
We've changed the yes to one and no has been changed to zero.

61
00:02:51,540 --> 00:02:53,780
Now that word I used a minute ago categorical.

62
00:02:53,780 --> 00:02:55,840
That's an important one to know.

63
00:02:55,840 --> 00:02:59,940
Categorical data basically describes categories or groups,

64
00:02:59,940 --> 00:03:02,940
and within that definition there's two types.

65
00:03:02,940 --> 00:03:06,240
Nominal data means that order doesn't matter.

66
00:03:06,240 --> 00:03:11,340
I like to remember that one by remembering the end means not matter

67
00:03:11,340 --> 00:03:14,190
examples of nominal data would be things like red,

68
00:03:14,190 --> 00:03:17,240
yellow, blue or yes and no.

69
00:03:17,240 --> 00:03:21,640
On the flip side of that, we have ordinal data, and here the order does matter.

70
00:03:21,640 --> 00:03:23,060
I remember this using the O.

71
00:03:23,060 --> 00:03:27,240
R D in ordinal to mean org That order does matter.

72
00:03:27,240 --> 00:03:29,550
And here's some examples of ordinal small,

73
00:03:29,550 --> 00:03:33,040
medium, large, hot, hotter and hottest.

74
00:03:33,040 --> 00:03:34,750
Now that we know what categorical means and the

75
00:03:34,750 --> 00:03:39,540
difference between ordinal a nominal, let's look at these next features for size.

76
00:03:39,540 --> 00:03:42,200
We have values of medium, small and large,

77
00:03:42,200 --> 00:03:45,740
and we want to convert those into numbers as well.

78
00:03:45,740 --> 00:03:47,120
When the order matters,

79
00:03:47,120 --> 00:03:50,840
what we do is basically a mapping between the word and a number.

80
00:03:50,840 --> 00:03:55,230
So it maintains that order of going from small values to larger values.

81
00:03:55,230 --> 00:03:59,040
And now it's in a format that the algorithm can understand.

82
00:03:59,040 --> 00:04:00,980
These features are already numerical values,

83
00:04:00,980 --> 00:04:02,030
so we're good with these,

84
00:04:02,030 --> 00:04:06,340
though we might wanna handle scale like we talked about earlier,

85
00:04:06,340 --> 00:04:09,110
and finally we have the type of automobile truck,

86
00:04:09,110 --> 00:04:12,320
SUV sedan or coupe here.

87
00:04:12,320 --> 00:04:15,760
Order doesn't matter like it did with small medium large,

88
00:04:15,760 --> 00:04:19,770
so these are nominal values, and generally it's not recommended that,

89
00:04:19,770 --> 00:04:23,340
um, code nominal values like the mapping we use before.

90
00:04:23,340 --> 00:04:23,850
For instance,

91
00:04:23,850 --> 00:04:28,740
how would you decide if the truck was 10 or the coop was 20 and so forth?

92
00:04:28,740 --> 00:04:30,980
The solution for this is one hot encoding,

93
00:04:30,980 --> 00:04:32,780
and this is definitely a concept you'll need to

94
00:04:32,780 --> 00:04:35,740
understand for the exam with one hot encoding.

95
00:04:35,740 --> 00:04:38,770
You basically take each value for the type of automobile and

96
00:04:38,770 --> 00:04:41,040
create a new feature or column for it.

97
00:04:41,040 --> 00:04:44,620
So type underscore truck type, underscore SUV type,

98
00:04:44,620 --> 00:04:47,140
underscore sedan and so forth.

99
00:04:47,140 --> 00:04:51,440
And then you're gonna use zeros and ones to denote what type it is.

100
00:04:51,440 --> 00:04:54,440
For instance, this top row here for truck.

101
00:04:54,440 --> 00:04:57,910
We have a one in the type underscore truck column and

102
00:04:57,910 --> 00:04:59,740
then zeros for everything else.

103
00:04:59,740 --> 00:05:02,020
And you'll see it continues that way for the remaining rows or

104
00:05:02,020 --> 00:05:06,140
observations so that just about does it for feature engineering.

105
00:05:06,140 --> 00:05:08,670
But you might be wondering how you actually do this

106
00:05:08,670 --> 00:05:10,640
cleaning and preparing of the data.

107
00:05:10,640 --> 00:05:14,340
It's obviously not done in PowerPoint slides, right?

108
00:05:14,340 --> 00:05:17,740
Depending on the size of your data, you have a few different options.

109
00:05:17,740 --> 00:05:19,150
Azure starting out in learning.

110
00:05:19,150 --> 00:05:21,530
Sometimes all you need is an Excel workbook,

111
00:05:21,530 --> 00:05:23,580
but for doing anything more serious,

112
00:05:23,580 --> 00:05:27,180
you want to Use Jupyter notebooks in sage Maker There's several

113
00:05:27,180 --> 00:05:30,140
libraries that make this work really easy.

114
00:05:30,140 --> 00:05:34,480
AWS glue is also a great option as a fully managed detail solution.

115
00:05:34,480 --> 00:05:37,640
It's a little bit more robust and repeatable.

116
00:05:37,640 --> 00:05:40,420
And if you're more on the B I or analyst side of things

117
00:05:40,420 --> 00:05:44,740
and you don't want to write any code, you can use a tool called quicksight.

118
00:05:44,740 --> 00:05:46,960
One other service to know about when it comes to preparing

119
00:05:46,960 --> 00:05:50,170
your data is SATA Maker Ground Truth Ground Truth can

120
00:05:50,170 --> 00:05:51,970
automatically label a lot of your data,

121
00:05:51,970 --> 00:05:55,780
but there are times when you really do need to have a human step in.

122
00:05:55,780 --> 00:06:00,740
In cases where a human workforce is needed, Amazon engages Mechanical Turk.

123
00:06:00,740 --> 00:06:03,850
These folks will manually label your data and or validate

124
00:06:03,850 --> 00:06:06,440
that the automatic labeling was correct.

125
00:06:06,440 --> 00:06:09,260
Ground truth can also be set up to allow labeling from your own

126
00:06:09,260 --> 00:06:15,000
private workforce or from other third-party vendors. Now let's get into a demo

