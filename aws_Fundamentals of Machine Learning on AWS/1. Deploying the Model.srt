1
00:00:01,440 --> 00:00:04,170
[Autogenerated] Greetings machine learners, and welcome to this next module.

2
00:00:04,170 --> 00:00:06,740
In the course, I'm embrace real JSON.

3
00:00:06,740 --> 00:00:07,950
We're moving right along.

4
00:00:07,950 --> 00:00:10,520
In the last module we trained and evaluated the model,

5
00:00:10,520 --> 00:00:14,840
and now the time has come to deploy it and make it available to the world.

6
00:00:14,840 --> 00:00:18,740
In this module will tackle the next two steps of the machine learning process.

7
00:00:18,740 --> 00:00:20,410
When deploying the model we need to take into

8
00:00:20,410 --> 00:00:22,320
account architecture er and security.

9
00:00:22,320 --> 00:00:25,990
Best practices will review those and for monitoring.

10
00:00:25,990 --> 00:00:29,040
We'll take a look at CloudWatch and CloudTrail.

11
00:00:29,040 --> 00:00:29,680
As always,

12
00:00:29,680 --> 00:00:31,770
we'll go through all of these things conceptually and

13
00:00:31,770 --> 00:00:37,240
then also see them in action in a demo, starting with deploying the model,

14
00:00:37,240 --> 00:00:40,940
which is here in the overall process almost to the end.

15
00:00:40,940 --> 00:00:43,140
Now, if you already know the basics of AWS,

16
00:00:43,140 --> 00:00:45,390
I'll assume you're generally familiar with some architecture.

17
00:00:45,390 --> 00:00:49,940
Best practices like building for high availability and fault tolerance.

18
00:00:49,940 --> 00:00:52,290
And I'll just point out that those best practices and

19
00:00:52,290 --> 00:00:53,880
others apply to machine learning,

20
00:00:53,880 --> 00:00:58,840
just like they do any other type of application you're building on top of AWS.

21
00:00:58,840 --> 00:01:01,150
One way to achieve those is through loose coupling.

22
00:01:01,150 --> 00:01:04,490
Another concept you've no doubt heard about this just means you're

23
00:01:04,490 --> 00:01:07,620
separating the components in the system so that if one fails,

24
00:01:07,620 --> 00:01:09,440
it doesn't take down the rest.

25
00:01:09,440 --> 00:01:12,780
Amazon Simple Cue Service or SQs and AWS Step

26
00:01:12,780 --> 00:01:15,340
functions are two ways you could do that.

27
00:01:15,340 --> 00:01:18,560
Sqs is a messaging service, and in this example,

28
00:01:18,560 --> 00:01:21,430
you could do something like get your videos into an S3

29
00:01:21,430 --> 00:01:25,090
bucket and then Q them into sqs in your code,

30
00:01:25,090 --> 00:01:26,340
grab them from the Q.

31
00:01:26,340 --> 00:01:30,340
Do some kind of processing and then send them to recognition video.

32
00:01:30,340 --> 00:01:35,160
So that s que s component just decouple Azure code from the S3 bucket If

33
00:01:35,160 --> 00:01:37,150
something goes wrong with your code or the bucket.

34
00:01:37,150 --> 00:01:42,440
Sqs is there with retry logic and such to keep you going until things RBAC up.

35
00:01:42,440 --> 00:01:46,010
Some other best practices here for sage maker include deploying your

36
00:01:46,010 --> 00:01:49,140
sage maker endpoints to multiple availability zones.

37
00:01:49,140 --> 00:01:51,940
Arrays ease and use auto scaling.

38
00:01:51,940 --> 00:01:55,340
This will protect you if one of the A's C's goes down.

39
00:01:55,340 --> 00:01:55,910
And secondly,

40
00:01:55,910 --> 00:01:59,510
it's a good idea to provide separate Docker images for training code

41
00:01:59,510 --> 00:02:03,520
and inference code and keep them separated is far as.

42
00:02:03,520 --> 00:02:04,870
Security goes with sage maker.

43
00:02:04,870 --> 00:02:06,640
There's a few things to know.

44
00:02:06,640 --> 00:02:07,520
First, it supports.

45
00:02:07,520 --> 00:02:11,420
I am role based access, and we've seen a little bit of that in the demos.

46
00:02:11,420 --> 00:02:12,540
This means, for instance,

47
00:02:12,540 --> 00:02:14,940
you can control access to the S3 buckets that you're

48
00:02:14,940 --> 00:02:19,740
using to your notebooks to training, job execution and more.

49
00:02:19,740 --> 00:02:22,970
Your data is encrypted at rest with either kms the key

50
00:02:22,970 --> 00:02:27,120
management service or a transient key, and it's also encrypted in transit.

51
00:02:27,120 --> 00:02:30,240
Using to s 1.2 encryption.

52
00:02:30,240 --> 00:02:33,210
Your training data is stored and transferred within your account,

53
00:02:33,210 --> 00:02:35,840
not the sage maker managed account.

54
00:02:35,840 --> 00:02:36,340
And finally,

55
00:02:36,340 --> 00:02:39,610
your E c two instances can do launched inside of a VPC that you

56
00:02:39,610 --> 00:02:42,960
manage to deploy your model in sage Maker.

57
00:02:42,960 --> 00:02:44,350
You need three things.

58
00:02:44,350 --> 00:02:47,840
First, you need the model that you previously created.

59
00:02:47,840 --> 00:02:52,340
Next, you'll need to set up an endpoint configuration for an HTTPS endpoint.

60
00:02:52,340 --> 00:02:55,940
And then finally, you create the HTTPS endpoint itself,

61
00:02:55,940 --> 00:02:57,230
and I should note, this is the steps.

62
00:02:57,230 --> 00:02:59,500
When you deploy the model using the console,

63
00:02:59,500 --> 00:03:01,210
it'll look a little bit different through code,

64
00:03:01,210 --> 00:03:03,140
which you'll see in a few minutes.

65
00:03:03,140 --> 00:03:06,520
Once it's been deployed, a client application will call your endpoint,

66
00:03:06,520 --> 00:03:12,000
using the invoke endpoint method in passing in the data for which they want a prediction

