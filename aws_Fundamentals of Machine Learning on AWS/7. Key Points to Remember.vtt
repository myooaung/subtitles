WEBVTT
1
00:00:01.540 --> 00:00:03.480
[Autogenerated] Let's recap the important points to remember

2
00:00:03.480 --> 00:00:06.270
from this module for fetching your data.

3
00:00:06.270 --> 00:00:09.480
You want to get your data from various sources in to S3

4
00:00:09.480 --> 00:00:11.640
so Saint Maker could work with it.

5
00:00:11.640 --> 00:00:14.920
We talked about various AWS services that you can use for this,

6
00:00:14.920 --> 00:00:16.500
and I won't rehash all of them here.

7
00:00:16.500 --> 00:00:19.040
But just a couple reminders for each.

8
00:00:19.040 --> 00:00:19.830
Like formacion,

9
00:00:19.830 --> 00:00:21.840
this is the data lake that pulls together all your

10
00:00:21.840 --> 00:00:24.040
data in its different formats.

11
00:00:24.040 --> 00:00:28.110
AWS glue is a fully managed DTL tool that can not only be used for

12
00:00:28.110 --> 00:00:30.950
ingestion but for transforming your data as well.

13
00:00:30.950 --> 00:00:33.940
And link formacion is built on top of glue.

14
00:00:33.940 --> 00:00:38.160
Data Pipeline lets you move data between AWS storage and compute services,

15
00:00:38.160 --> 00:00:41.340
as well as on prim to AWS.

16
00:00:41.340 --> 00:00:46.440
The database Migration service, or GMs, lets you move data between databases,

17
00:00:46.440 --> 00:00:48.220
Kenny says is all about streaming.

18
00:00:48.220 --> 00:00:50.240
Refer to the slide earlier in the course for the

19
00:00:50.240 --> 00:00:53.560
differences in those four services E.

20
00:00:53.560 --> 00:00:53.690
M.

21
00:00:53.690 --> 00:00:53.920
R.

22
00:00:53.920 --> 00:00:56.660
Or elastic map reduces used for processing massive

23
00:00:56.660 --> 00:00:59.940
amounts of data in a distributed way.

24
00:00:59.940 --> 00:01:05.420
Athena is for doing SQL queries on your S3 data, as is red shift spectrum.

25
00:01:05.420 --> 00:01:07.380
But this only makes sense if you've got a red shift

26
00:01:07.380 --> 00:01:09.640
cluster set up for something else,

27
00:01:09.640 --> 00:01:11.680
and quick site helps you get insight into your data

28
00:01:11.680 --> 00:01:13.550
with queries and visualizations.

29
00:01:13.550 --> 00:01:16.340
This is more geared towards by professionals,

30
00:01:16.340 --> 00:01:19.120
and finally, ground truth helps with labeling your data.

31
00:01:19.120 --> 00:01:23.340
There's an automatic component as well as a human workforce component.

32
00:01:23.340 --> 00:01:25.940
After fetching your data, you'll want to clean it up.

33
00:01:25.940 --> 00:01:29.840
You'll need to handle missing values, outliers and formatting.

34
00:01:29.840 --> 00:01:33.150
We saw some common data visualizations to help make sense of your data,

35
00:01:33.150 --> 00:01:37.140
things like scatter plots, correlation matrices,

36
00:01:37.140 --> 00:01:39.270
Instagram's and box plots.

37
00:01:39.270 --> 00:01:43.470
We also saw some of those in action in our demo and then finally,

38
00:01:43.470 --> 00:01:45.710
feature engineering is the process of adding,

39
00:01:45.710 --> 00:01:50.040
removing or combining features to ensure you have the most predictive model.

40
00:01:50.040 --> 00:01:52.550
Some of the concepts related to this were how to scale your

41
00:01:52.550 --> 00:01:55.440
data so that every value is treated the same.

42
00:01:55.440 --> 00:01:59.740
We talked about categorical data, which is used to describe categories or groups,

43
00:01:59.740 --> 00:02:01.390
and remember that with nominal data,

44
00:02:01.390 --> 00:02:05.940
order does not matter and with ordinal data order does matter.

45
00:02:05.940 --> 00:02:09.440
We also saw one hot encoding where you essentially explode

46
00:02:09.440 --> 00:02:13.520
nominal data into multiple features and then use zeros and ones

47
00:02:13.520 --> 00:02:16.840
to indicate what is true for a given row,

48
00:02:16.840 --> 00:02:23.000
and with that, let's move on to the next module, where we'll talk about training and evaluating our model.

