1
00:00:01,240 --> 00:00:01,680
[Autogenerated] okay,

2
00:00:01,680 --> 00:00:04,670
we've talked about how to fetch your data from various places.

3
00:00:04,670 --> 00:00:07,740
Now let's talk about how to get it ready to use.

4
00:00:07,740 --> 00:00:09,830
I'm going to lump these next two faces together,

5
00:00:09,830 --> 00:00:12,360
cleaning and preparing because there's some overlap and

6
00:00:12,360 --> 00:00:14,840
also iteration that happens here.

7
00:00:14,840 --> 00:00:15,430
In a nutshell.

8
00:00:15,430 --> 00:00:20,740
Though data is messy with just this little dataset, we have several problems.

9
00:00:20,740 --> 00:00:23,380
The column headings, for instance, aren't consistent.

10
00:00:23,380 --> 00:00:27,330
One uses underscores, and one doesn't in the first name.

11
00:00:27,330 --> 00:00:31,740
We have punctuation not necessarily wrong as these are legitimate names,

12
00:00:31,740 --> 00:00:34,440
but we need to make sure we can handle that.

13
00:00:34,440 --> 00:00:38,690
In the state and country columns, we have inconsistent naming some abbreviations,

14
00:00:38,690 --> 00:00:41,840
some blanks, some Knowles and some dashes.

15
00:00:41,840 --> 00:00:43,820
Similar problem for the Zip code column,

16
00:00:43,820 --> 00:00:48,430
where we have some blanks and zip codes that aren't the same length for age.

17
00:00:48,430 --> 00:00:52,550
We have different decimal formats and any n or not a number

18
00:00:52,550 --> 00:00:56,040
and likely a typo with the age of 450.

19
00:00:56,040 --> 00:00:56,630
Or actually,

20
00:00:56,630 --> 00:01:01,240
maybe that ages in months rather than years for gender here again

21
00:01:01,240 --> 00:01:03,790
were inconsistent in our naming and abbreviations,

22
00:01:03,790 --> 00:01:06,640
and we have blank values and for salary.

23
00:01:06,640 --> 00:01:11,140
We have formatting issues, different currencies and missing values,

24
00:01:11,140 --> 00:01:12,260
and it's not shown here.

25
00:01:12,260 --> 00:01:16,040
But it's also common to have duplicate records or rows.

26
00:01:16,040 --> 00:01:18,000
So we need to get these sorts of things cleaned up

27
00:01:18,000 --> 00:01:20,040
before the data will be useful.

28
00:01:20,040 --> 00:01:22,160
First, let's tackle missing data.

29
00:01:22,160 --> 00:01:24,340
There's a few ways to handle this.

30
00:01:24,340 --> 00:01:28,340
First, you could remove the rows or columns that have the missing data.

31
00:01:28,340 --> 00:01:30,580
This is generally only a good idea, though.

32
00:01:30,580 --> 00:01:34,340
If the missing data is a very small percentage of the overall dataset.

33
00:01:34,340 --> 00:01:34,920
Otherwise,

34
00:01:34,920 --> 00:01:39,140
you could be losing some of the richness and predictive power of the data.

35
00:01:39,140 --> 00:01:43,440
You could also fill in the missing values, taking a few different approaches.

36
00:01:43,440 --> 00:01:46,100
You could look at the data that is filled in and

37
00:01:46,100 --> 00:01:48,390
then take the mean or median value,

38
00:01:48,390 --> 00:01:51,780
and use that to fill in the missing values you could fill in.

39
00:01:51,780 --> 00:01:55,670
The missing data was zeros or Knowles, and finally,

40
00:01:55,670 --> 00:01:59,740
you could use imputation, which basically means your best guess.

41
00:01:59,740 --> 00:02:03,740
This could be a best guess for manually looking at and knowing the data.

42
00:02:03,740 --> 00:02:06,100
Or you could actually run the data through a machine

43
00:02:06,100 --> 00:02:08,440
learning model to predict the value,

44
00:02:08,440 --> 00:02:13,140
kind of like doing ML on the data as a way to prepare it for doing ML.

45
00:02:13,140 --> 00:02:15,730
This option is typically going to give you better results

46
00:02:15,730 --> 00:02:18,440
and has become a more popular approach.

47
00:02:18,440 --> 00:02:20,570
Next, you'll need to handle outliers.

48
00:02:20,570 --> 00:02:24,010
While dataset SSHD definitely do contain valid outliers,

49
00:02:24,010 --> 00:02:27,510
they could also be a sign of typos or mistakes in the data.

50
00:02:27,510 --> 00:02:30,340
And they can see cue the overall dataset.

51
00:02:30,340 --> 00:02:33,840
If this happens, it becomes harder to get accurate predictions.

52
00:02:33,840 --> 00:02:36,150
Generally speaking, you want to remove outliers,

53
00:02:36,150 --> 00:02:38,100
but be cautious with this, because again,

54
00:02:38,100 --> 00:02:41,040
you might be losing some of the richness of the data.

55
00:02:41,040 --> 00:02:41,670
When in doubt,

56
00:02:41,670 --> 00:02:44,910
you want to check with somebody who's an expert on that particular dataset,

57
00:02:44,910 --> 00:02:50,350
so they can tell you how important or common the outliers are handling format.

58
00:02:50,350 --> 00:02:53,170
This includes things like spacing, casing,

59
00:02:53,170 --> 00:02:56,640
punctuation, decimal points, special characters,

60
00:02:56,640 --> 00:02:59,340
currencies and abbreviations.

61
00:02:59,340 --> 00:03:01,560
There's really no hard and fast rules here other than

62
00:03:01,560 --> 00:03:07,000
you need to get things consistent, pick whatever the convention is and stick with it

