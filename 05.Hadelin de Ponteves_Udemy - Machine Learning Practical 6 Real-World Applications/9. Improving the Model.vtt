WEBVTT

00:03.060 --> 00:05.630
So let's take a look at how can we improve our model.

00:05.700 --> 00:06.260
OK.

00:06.540 --> 00:14.160
So we will improve our model using either by increasing number of filters or number of kernels or feature

00:14.160 --> 00:16.120
detectors are going to be using.

00:16.140 --> 00:23.870
So as we saw before that we only used 32 by 32 kernels or feature detectors.

00:24.030 --> 00:28.640
We're going to increase the numbers to let's say 64 maybe 128 and we'll see what's going to happen.

00:28.640 --> 00:29.070
OK.

00:29.250 --> 00:33.380
That's kind of one idea the idea of improving the model.

00:33.420 --> 00:36.810
The other element that we could do with all college dropouts.

00:36.810 --> 00:44.960
Ok took out technique is actually very interesting and it can be used to simply drop out units out of

00:44.970 --> 00:47.230
on your network to this network.

00:47.260 --> 00:47.610
OK.

00:47.610 --> 00:52.790
And what we do with that we keep learning or teaching our new networks by basically adjusting these

00:52.800 --> 00:54.300
weights as we saw.

00:54.620 --> 00:55.070
OK.

00:55.350 --> 01:01.530
What we do is during training we can actually take some of the samples and actually disconnect them

01:01.530 --> 01:02.880
or destroy them in a way.

01:02.940 --> 01:03.600
OK.

01:03.660 --> 01:08.730
It's kind of the art are destroying the learning process to make it better moving forward.

01:08.730 --> 01:14.490
That's the whole intuition of running or performing dropouts.

01:14.520 --> 01:17.560
So I guys can see here we actually drop down some neurons.

01:17.580 --> 01:22.950
OK again we preserve some some some neurons some connections still but we drop out some neurons when

01:22.950 --> 01:23.890
we do this.

01:24.000 --> 01:27.510
We actually make the network in a way generalized more.

01:27.510 --> 01:30.620
So it's not kind of fitted specifically to the training data.

01:30.660 --> 01:37.170
It can actually generalize and look more into kind of images or features that had hasn't seen before.

01:37.260 --> 01:37.780
OK.

01:38.160 --> 01:42.930
So again neurons can develop co-dependancy among each other during training.

01:42.930 --> 01:46.800
And that's why we want to overcome this by running or performing dropouts.

01:47.160 --> 01:52.540
So job out again is kind of what we call the regularisation technique for using overfitting within your

01:52.590 --> 01:53.640
networks.

01:53.640 --> 01:56.810
It enables training to occur on several architecture of the network.

01:56.800 --> 01:59.600
It's kind of you are because you're dropping some neurons.

01:59.670 --> 02:03.790
It's kind of you are trying different architectures while training you or your network.

02:03.800 --> 02:04.650
OK.

02:05.190 --> 02:10.500
Again we're going to see we're not we don't need to go and actually like drop neurons when we are running

02:10.510 --> 02:11.970
of the actual model.

02:11.970 --> 02:17.580
It's literally like just like you know very very simple commands just drop and then you specify the

02:17.580 --> 02:19.730
percentage of neurons you wanted to drop.

02:19.800 --> 02:20.460
OK.

02:20.770 --> 02:21.520
All right.

02:21.910 --> 02:22.220
OK.

02:22.230 --> 02:25.710
That's kind of how can we improve our model per se.

02:25.710 --> 02:29.310
Let's take a look at the any of these strategies work or not.

02:29.340 --> 02:29.990
OK.

02:30.010 --> 02:31.090
Propagations I enjoyed it.

02:31.110 --> 02:32.380
And we'll see you in the next section.
