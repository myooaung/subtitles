1
00:00:01,760 --> 00:00:07,520
Let's now go through a demo and do data preparation using AWS SageMaker.

2
00:00:07,520 --> 00:00:10,430
It's time to get our hands dirty.

3
00:00:10,430 --> 00:00:15,110
And now we are back to our previous Ames housing price

4
00:00:15,110 --> 00:00:18,520
dataset we visualized in the last module.

5
00:00:18,520 --> 00:00:22,730
It is now that time to proceed with the data preparation steps.

6
00:00:22,730 --> 00:00:25,310
Since our dataset is quite large,

7
00:00:25,310 --> 00:00:30,630
the first thing I would like to do is to adjust pandas print settings to

8
00:00:30,630 --> 00:00:38,540
change the maximum number of rows to display to a large number.

9
00:00:38,540 --> 00:00:42,380
Let's now try to detect missing values in our dataset.

10
00:00:42,380 --> 00:00:46,020
For that, we can run a pandas command called isnull,

11
00:00:46,020 --> 00:00:50,990
and then we will sum the numbers of null values in each column.

12
00:00:50,990 --> 00:00:55,600
You will notice that pandas detected certain null values.

13
00:00:55,600 --> 00:00:58,900
For example, Alley column.

14
00:00:58,900 --> 00:01:05,710
The reason is that this column uses the value NA as one of its categories,

15
00:01:05,710 --> 00:01:11,080
while pandas' default behavior is to consider NA as not a number,

16
00:01:11,080 --> 00:01:15,040
and, hence, it will be detected in isnull.

17
00:01:15,040 --> 00:01:18,790
Let's look at what we have in the original dataset.

18
00:01:18,790 --> 00:01:24,840
And as you can see, Alley column uses the value NA as one of its categories,

19
00:01:24,840 --> 00:01:27,450
which indicates that pandas will be confused,

20
00:01:27,450 --> 00:01:32,140
and it will consider it not a number.

21
00:01:32,140 --> 00:01:36,340
We can validate that by looking at our pandas dataset.

22
00:01:36,340 --> 00:01:37,380
And, as you can see,

23
00:01:37,380 --> 00:01:44,090
pandas identified the NA values as not a number in Alley column.

24
00:01:44,090 --> 00:01:49,040
The reason is that pandas is using a specific configured list of values as an

25
00:01:49,040 --> 00:01:53,100
indicator of missing values whenever it reads a CSV file.

26
00:01:53,100 --> 00:01:57,940
And you can see this list below.

27
00:01:57,940 --> 00:01:58,850
So,

28
00:01:58,850 --> 00:02:03,380
the solution to this problem would be to reconfigure pandas when reading

29
00:02:03,380 --> 00:02:07,640
to exclude the value NA as a measure of missing values.

30
00:02:07,640 --> 00:02:11,330
So I will replace the default setting of pandas by removing

31
00:02:11,330 --> 00:02:15,540
the NA value to make it fit our scenario.

32
00:02:15,540 --> 00:02:17,770
And I will load the CSV again,

33
00:02:17,770 --> 00:02:23,260
and let's have a look at our pandas DataFrame one more time.

34
00:02:23,260 --> 00:02:27,920
And as you can see, the Alley column reads the value properly as NA,

35
00:02:27,920 --> 00:02:30,490
rather than not a number.

36
00:02:30,490 --> 00:02:35,000
Let's now do some analysis over what we see in the dataset.

37
00:02:35,000 --> 00:02:39,900
If you can see the PID column, it's not a really useful column for us,

38
00:02:39,900 --> 00:02:43,510
and it's just used as a unique generated identifier.

39
00:02:43,510 --> 00:02:46,290
We can drop that column.

40
00:02:46,290 --> 00:02:52,840
Let's now have a look at the descriptive statistics we have in our dataset.

41
00:02:52,840 --> 00:02:55,740
For that, I use describe and I call transpose.

42
00:02:55,740 --> 00:03:00,020
Transpose will switch the rows and columns of our pandas DataFrame.

43
00:03:00,020 --> 00:03:06,140
Here it will help us to easily read the descriptive statistics.

44
00:03:06,140 --> 00:03:08,920
It would be worth time to check the maximum and

45
00:03:08,920 --> 00:03:12,240
minimum columns for each feature,

46
00:03:12,240 --> 00:03:16,520
because if we see a feature that has equal minimum and maximum values,

47
00:03:16,520 --> 00:03:20,390
it would mean that this feature has no change across that column,

48
00:03:20,390 --> 00:03:25,230
and we can drop it, since it doesn't add much information to the analysis.

49
00:03:25,230 --> 00:03:31,340
And as we can see, there is no such case, so all is good, and we can proceed.

50
00:03:31,340 --> 00:03:34,450
Let's now start reading our missing values.

51
00:03:34,450 --> 00:03:38,390
As you have seen previously, when we call pandas isnull,

52
00:03:38,390 --> 00:03:42,200
we identified some features with missing values.

53
00:03:42,200 --> 00:03:44,790
Let's now try to calculate the percentage of the

54
00:03:44,790 --> 00:03:47,470
features and see how significant they are.

55
00:03:47,470 --> 00:03:49,600
In the first line of the code,

56
00:03:49,600 --> 00:03:54,500
I calculate the missing percentage of entries by dividing the number of

57
00:03:54,500 --> 00:03:58,300
missing entries in each feature by the total length of the dataset and

58
00:03:58,300 --> 00:04:02,640
multiplying it by 100 to get it in the percentage terms.

59
00:04:02,640 --> 00:04:03,110
Then,

60
00:04:03,110 --> 00:04:06,450
I construct a new pandas DataFrame containing each column

61
00:04:06,450 --> 00:04:09,400
and the corresponding missing percent.

62
00:04:09,400 --> 00:04:13,240
Finally, I sort the values in a descending fashion.

63
00:04:13,240 --> 00:04:14,530
And as you can see,

64
00:04:14,530 --> 00:04:18,540
the feature with the largest number of missing values is the Lot Frontage,

65
00:04:18,540 --> 00:04:23,040
with 16.7% missing values.

66
00:04:23,040 --> 00:04:26,540
When dealing with missing values, you can consider the following rule of thumb.

67
00:04:26,540 --> 00:04:31,340
If the percentage of missing features is greater than 80%,

68
00:04:31,340 --> 00:04:34,470
you need to consider dropping that feature at all,

69
00:04:34,470 --> 00:04:36,620
since we might not have much to do.

70
00:04:36,620 --> 00:04:40,090
Fortunately, this is not the case here in our dataset.

71
00:04:40,090 --> 00:04:42,820
Then you need to consider imputing the features using

72
00:04:42,820 --> 00:04:46,830
different data imputation techniques.

73
00:04:46,830 --> 00:04:49,700
And now let's see what strategy we are going to do

74
00:04:49,700 --> 00:04:52,440
to fill in our missing values.

75
00:04:52,440 --> 00:04:56,540
The first two variables, Lot Frontage and Garage Yr Blt,

76
00:04:56,540 --> 00:04:58,680
have the highest amount of missing values,

77
00:04:58,680 --> 00:05:02,590
which are 16.7% and 5%, respectively,

78
00:05:02,590 --> 00:05:08,640
while the other variables have missing percentage less than 1%.

79
00:05:08,640 --> 00:05:10,820
I will use median for numerical values,

80
00:05:10,820 --> 00:05:15,030
and the most commonly occurring value for the categorical variables.

81
00:05:15,030 --> 00:05:16,660
Let's see that in action.

82
00:05:16,660 --> 00:05:23,010
And here for every numerical value I have with missing values less than 1%,

83
00:05:23,010 --> 00:05:26,540
I replace missing values with the median of that feature.

84
00:05:26,540 --> 00:05:31,740
This is accomplished using a pandas function called fillna.

85
00:05:31,740 --> 00:05:34,740
And here, for every categorical variable I have,

86
00:05:34,740 --> 00:05:39,340
I replace the missing values with the most commonly used value.

87
00:05:39,340 --> 00:05:41,570
You can think about it as amount.

88
00:05:41,570 --> 00:05:48,040
This can be obtained from pandas by using a function called value_counts.

89
00:05:48,040 --> 00:05:52,980
The value_counts function returns a list of all unique values in the

90
00:05:52,980 --> 00:05:56,700
column and how many times each value occurred,

91
00:05:56,700 --> 00:05:59,400
and sorting them in a descending fashion,

92
00:05:59,400 --> 00:06:02,570
which means if we take the index 0,

93
00:06:02,570 --> 00:06:07,040
we will get the value of the most frequently occurred value that we

94
00:06:07,040 --> 00:06:10,940
can use to adjust the missing categorical variables.

95
00:06:10,940 --> 00:06:14,950
Let's have a look once again on how our missing values look like.

96
00:06:14,950 --> 00:06:19,960
And as you can see, only Lot Frontage and Garage Yr Blt are with missing values.

97
00:06:19,960 --> 00:06:21,340
Let's handle that.

98
00:06:21,340 --> 00:06:27,010
The strategy we will use to impute Lot Frontage and Garage Yr Blt will rely on

99
00:06:27,010 --> 00:06:30,380
estimating their values using machine learning techniques.

100
00:06:30,380 --> 00:06:31,840
In other words,

101
00:06:31,840 --> 00:06:35,600
we will consider the missing values as target unknown values that

102
00:06:35,600 --> 00:06:38,540
we would like to estimate from a nonâ€‘value,

103
00:06:38,540 --> 00:06:41,840
which are the other values in the dataset.

104
00:06:41,840 --> 00:06:46,500
Fortunately, we don't need to develop a machine learning pipeline for that,

105
00:06:46,500 --> 00:06:50,140
since already Python provides that functionality out of the box.

106
00:06:50,140 --> 00:06:52,940
To use that functionality, firstly,

107
00:06:52,940 --> 00:06:56,720
we will need to make sure that we updated our Python version,

108
00:06:56,720 --> 00:07:01,840
since AWS SageMaker might load an older version of Python

109
00:07:01,840 --> 00:07:04,670
while the imputer is a new feature.

110
00:07:04,670 --> 00:07:07,940
And let's validate Python version.

111
00:07:07,940 --> 00:07:08,840
Good.

112
00:07:08,840 --> 00:07:13,820
Now we are using version 0.22.2 of scikitâ€‘learn.

113
00:07:13,820 --> 00:07:20,170
And now we will import the experimental imputer.

114
00:07:20,170 --> 00:07:24,400
I have imported Python subâ€‘package impute,

115
00:07:24,400 --> 00:07:28,640
which contains IterativeImputer method that we will use to fit

116
00:07:28,640 --> 00:07:32,090
against our features and impute missing values.

117
00:07:32,090 --> 00:07:36,540
Notice that I imported enable_iterative_imputer.

118
00:07:36,540 --> 00:07:39,840
This is because the imputer is an experimental feature

119
00:07:39,840 --> 00:07:43,160
and has to be imported explicitly.

120
00:07:43,160 --> 00:07:46,640
The strategy for imputing missing values is by modeling

121
00:07:46,640 --> 00:07:49,690
each feature with missing values as a function of other

122
00:07:49,690 --> 00:07:51,870
features in a round robin fashion.

123
00:07:51,870 --> 00:07:55,850
There are many details behind the scene regarding the IterativeImputer.

124
00:07:55,850 --> 00:07:59,210
You can read about them in scikitâ€‘learn documentation.

125
00:07:59,210 --> 00:08:04,430
And here I am separating my features from the target predict,

126
00:08:04,430 --> 00:08:08,340
as IterativeImputer expects only features.

127
00:08:08,340 --> 00:08:11,470
I will only apply the imputer on the numerical features,

128
00:08:11,470 --> 00:08:15,280
since the imputer does not support categorical features.

129
00:08:15,280 --> 00:08:20,380
Categorical features can be detected by checking that pipe dtype of the values.

130
00:08:20,380 --> 00:08:24,990
And I prepare the imputer with a random_state of 100.

131
00:08:24,990 --> 00:08:29,740
This is just to make it easy for you to replicate the same results.

132
00:08:29,740 --> 00:08:34,940
Then I will call imputer.fit, which will fit the imputer to our features.

133
00:08:34,940 --> 00:08:38,300
In other words, it trains on the features.

134
00:08:38,300 --> 00:08:44,340
And then I will impute the missing values.

135
00:08:44,340 --> 00:08:44,780
Then,

136
00:08:44,780 --> 00:08:50,140
I concatenate the newly imputed features with categorical features and

137
00:08:50,140 --> 00:08:53,950
sale price to get the new fully imputed dataset.

138
00:08:53,950 --> 00:08:59,460
Notice that I called reset_index on the DataFrames before concatenation.

139
00:08:59,460 --> 00:09:02,400
This is to avoid a tricky behavior by pandas,

140
00:09:02,400 --> 00:09:07,400
where it assigns not a number if there are two DataFrames that

141
00:09:07,400 --> 00:09:12,280
don't have the same index during concatenation.

142
00:09:12,280 --> 00:09:17,840
And now let's validate that we don't have any missing values.

143
00:09:17,840 --> 00:09:18,950
All values are 0.

144
00:09:18,950 --> 00:09:24,000
Very good. We are done with handling our missing values.

