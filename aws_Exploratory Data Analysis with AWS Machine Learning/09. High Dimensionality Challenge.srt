1
00:00:01,040 --> 00:00:05,630
Another common challenge we are going to discuss is the high dimensionality.

2
00:00:05,630 --> 00:00:07,220
The high dimensionality challenge,

3
00:00:07,220 --> 00:00:10,690
or what is so called the curse of dimensionality,

4
00:00:10,690 --> 00:00:14,350
occurs when we have too many dimensions in our dataset. A

5
00:00:14,350 --> 00:00:17,240
common example would be a video stream.

6
00:00:17,240 --> 00:00:20,570
Having too many dimensions in the dataset is extremely bad,

7
00:00:20,570 --> 00:00:26,260
due to the following reasons: Your data will become more difficult to visualize.

8
00:00:26,260 --> 00:00:30,670
Visualizing three dimensions is hard, let alone hundreds.

9
00:00:30,670 --> 00:00:32,340
The more complex your dataset,

10
00:00:32,340 --> 00:00:35,780
the more complex your model, and the more complex your model, the

11
00:00:35,780 --> 00:00:41,460
higher risk of overfitting. The machine learning training phase will

12
00:00:41,460 --> 00:00:45,360
be more expensive since you will need to tune more parameters and that

13
00:00:45,360 --> 00:00:50,220
will take more time. And let's now discuss the common techniques to

14
00:00:50,220 --> 00:00:52,090
reduce dimensionality.

15
00:00:52,090 --> 00:00:57,630
The first technique is feature engineering, and this consists of creating new,

16
00:00:57,630 --> 00:01:00,740
useful features from the currently existing ones.

17
00:01:00,740 --> 00:01:01,860
For example,

18
00:01:01,860 --> 00:01:05,390
transforming the death year and birth year to a single

19
00:01:05,390 --> 00:01:09,420
feature column called Life Span or Age.

20
00:01:09,420 --> 00:01:12,630
The second technique is called feature selection, and this

21
00:01:12,630 --> 00:01:16,100
consists of selecting a subset of features from the currently

22
00:01:16,100 --> 00:01:19,730
existing ones based on certain criteria.

23
00:01:19,730 --> 00:01:22,920
The third technique would be what is so called dimensionality

24
00:01:22,920 --> 00:01:27,620
reduction, and this technique consists of creating new dimensions (not

25
00:01:27,620 --> 00:01:31,360
necessarily from the existing features) that better capture the

26
00:01:31,360 --> 00:01:34,510
underlying relationships in the dataset.

27
00:01:34,510 --> 00:01:38,880
A detailed discussion of each technique is outside the scope of the course.

28
00:01:38,880 --> 00:01:41,530
But we will discuss one technique of dimensionality

29
00:01:41,530 --> 00:01:45,470
reduction called principal component analysis.

30
00:01:45,470 --> 00:01:49,720
Let's see what principal component analysis is.

31
00:01:49,720 --> 00:01:53,810
Imagine that we have the following dataset distributed across the Y and

32
00:01:53,810 --> 00:01:59,430
X axes, two dimensions. Let's assume that the green line is the

33
00:01:59,430 --> 00:02:02,810
best‑fitting line we found to fit our dataset.

34
00:02:02,810 --> 00:02:03,570
As you can see,

35
00:02:03,570 --> 00:02:10,020
most of the dataset variation is across that line. And here, we have the same

36
00:02:10,020 --> 00:02:15,500
dataset as previous. And this time, we will make the best‑fitting line we

37
00:02:15,500 --> 00:02:18,700
found with what is so called principal component,

38
00:02:18,700 --> 00:02:22,440
which means that we will make it our new dimension.

39
00:02:22,440 --> 00:02:23,460
As you can see,

40
00:02:23,460 --> 00:02:27,610
we have not lost much information by removing the two dimensions and

41
00:02:27,610 --> 00:02:31,540
introducing the principal component across a single dimension.

42
00:02:31,540 --> 00:02:33,030
So in practice,

43
00:02:33,030 --> 00:02:36,690
what we have done enabled us to convert a dataset that's

44
00:02:36,690 --> 00:02:39,870
described in two dimensions to a dataset that's described in

45
00:02:39,870 --> 00:02:42,910
terms of one dimension. This is big.

46
00:02:42,910 --> 00:02:45,150
Since we did a dimensionality reduction,

47
00:02:45,150 --> 00:02:48,780
we reduced the number of dimensions while maintaining useful

48
00:02:48,780 --> 00:02:52,900
relationships in the dataset. And, in fact,

49
00:02:52,900 --> 00:02:54,620
that can be generalized.

50
00:02:54,620 --> 00:02:59,870
It would be possible to convert a 3D dataset to a 2D dataset or even 1D

51
00:02:59,870 --> 00:03:04,730
dataset. And the same goes for 4D dataset and so on.

52
00:03:04,730 --> 00:03:07,010
And from here, we are at an excellent position to

53
00:03:07,010 --> 00:03:10,600
define the principal component analysis.

54
00:03:10,600 --> 00:03:15,520
The objective of principal component analysis is to reduce the number from

55
00:03:15,520 --> 00:03:21,960
n‑dimensions dataset to a k‑dimensions dataset by finding k vectors onto which

56
00:03:21,960 --> 00:03:26,740
to project the data so to minimize the projection error.

57
00:03:26,740 --> 00:03:30,400
The k vectors are called principal components,

58
00:03:30,400 --> 00:03:34,340
and they are ranked by their explained variance.

59
00:03:34,340 --> 00:03:39,570
Think about the best‑fitting line we've seen in the last slides.

60
00:03:39,570 --> 00:03:42,870
I have borrowed this definition from the lead machine learning scientist,

61
00:03:42,870 --> 00:03:48,750
Andrew Ng. And if you are interested to read more around mathematical

62
00:03:48,750 --> 00:03:52,140
derivations of principal component analysis,

63
00:03:52,140 --> 00:03:55,340
feel free to hit the following Bitly link.

64
00:03:55,340 --> 00:04:00,620
One word about PCA is that you will end up with two or three principal

65
00:04:00,620 --> 00:04:04,600
components within your machine learning model that are often just

66
00:04:04,600 --> 00:04:09,820
mathematical vectors and are not explainable in logical terms.

67
00:04:09,820 --> 00:04:10,910
Therefore,

68
00:04:10,910 --> 00:04:21,000
PCA, or principal component analysis, makes it difficult to communicate your data to an external nontechnical machine learning audience.

