WEBVTT
1
00:00:01.190 --> 00:00:03.990
Let's start with the problem of the imbalanced data,

2
00:00:03.990 --> 00:00:07.840
which significantly affects the classification problems.

3
00:00:07.840 --> 00:00:12.170
Let's discuss it by taking a real‑life example.

4
00:00:12.170 --> 00:00:16.860
Suppose that your boss asks you to design a fraud detection system.

5
00:00:16.860 --> 00:00:22.970
You trained the model with life data where 98% of cases are legit

6
00:00:22.970 --> 00:00:27.840
and only 2% of the cases are fraudulent.

7
00:00:27.840 --> 00:00:31.140
Your model is doing good, and you are happy with it and

8
00:00:31.140 --> 00:00:34.910
decide to deploy it to the production.

9
00:00:34.910 --> 00:00:38.980
The first legitimate case comes, and your model successfully

10
00:00:38.980 --> 00:00:45.490
categorizes it as legit. And the second legit case comes, and your

11
00:00:45.490 --> 00:00:50.790
model again successfully categorizes it as legitimate.

12
00:00:50.790 --> 00:00:54.990
However, when the eighth fraudulent request comes,

13
00:00:54.990 --> 00:00:59.650
the model failed to identify it correctly as fraudulent and made it legit.

14
00:00:59.650 --> 00:01:00.250
Hence,

15
00:01:00.250 --> 00:01:06.140
this machine learning model has put the organization under a security threat.

16
00:01:06.140 --> 00:01:09.890
But why did that happen in the first place?

17
00:01:09.890 --> 00:01:10.760
Well,

18
00:01:10.760 --> 00:01:15.520
since the natural life data is that most of the cases are okay and very

19
00:01:15.520 --> 00:01:19.710
few cases are fraudulent, our model did not learn enough about

20
00:01:19.710 --> 00:01:24.370
fraudulent cases, and in that scenario, we say that our data is

21
00:01:24.370 --> 00:01:29.490
imbalanced. So let's discuss the possible solutions to handle the

22
00:01:29.490 --> 00:01:34.760
imbalanced data. The first strategy would be that we under sample the

23
00:01:34.760 --> 00:01:37.770
majority of classes in our data.

24
00:01:37.770 --> 00:01:39.440
In our previous example,

25
00:01:39.440 --> 00:01:43.880
that means we take less samples from the legitimate cases.

26
00:01:43.880 --> 00:01:46.860
The second strategy would be to over sample the minority

27
00:01:46.860 --> 00:01:50.460
classes of the data by replicating some problem instances

28
00:01:50.460 --> 00:01:52.970
of the less‑occurring category.

29
00:01:52.970 --> 00:01:54.720
In our previous example,

30
00:01:54.720 --> 00:01:58.740
that would mean we will duplicate fraudulent records and, hence,

31
00:01:58.740 --> 00:02:02.140
increase their weight in the training base.

32
00:02:02.140 --> 00:02:06.340
The third strategy would involve generating scientific data.

33
00:02:06.340 --> 00:02:10.040
Scientific data is data that's generated based on the current

34
00:02:10.040 --> 00:02:14.350
characteristics of our dataset. In our previous example,

35
00:02:14.350 --> 00:02:18.890
it would be if we know that most fraudulent cases occur within a certain

36
00:02:18.890 --> 00:02:23.600
time frame from a specific geographical area, we can generate similar

37
00:02:23.600 --> 00:02:29.100
instances by looking at these characteristics. All these approaches aim at

38
00:02:29.100 --> 00:02:32.640
rebalancing (partially or fully) the dataset.

39
00:02:32.640 --> 00:02:33.380
However,

40
00:02:33.380 --> 00:02:39.000
a detailed mathematical discussion about them is outside the scope of our course.

