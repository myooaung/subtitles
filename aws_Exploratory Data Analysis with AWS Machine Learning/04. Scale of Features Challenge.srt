1
00:00:00,840 --> 00:00:04,760
The second problem I would like to discuss is the scale of features,

2
00:00:04,760 --> 00:00:07,940
and that problem can take two forms.

3
00:00:07,940 --> 00:00:08,990
Firstly,

4
00:00:08,990 --> 00:00:12,580
we might have some feature columns that have inconsistent scales

5
00:00:12,580 --> 00:00:16,940
across the dataset for different instances.

6
00:00:16,940 --> 00:00:18,240
For example,

7
00:00:18,240 --> 00:00:21,030
we might have some data entries that are in USD

8
00:00:21,030 --> 00:00:23,890
while others are in British pound.

9
00:00:23,890 --> 00:00:29,070
And even though if we have all instances of the dataset with the same scale,

10
00:00:29,070 --> 00:00:30,760
there is another challenge,

11
00:00:30,760 --> 00:00:33,740
which is that many machine learning algorithms are

12
00:00:33,740 --> 00:00:36,010
sensitive to the data magnitudes.

13
00:00:36,010 --> 00:00:40,360
And one common example is the Kâ€‘Means clustering,

14
00:00:40,360 --> 00:00:42,100
which uses the Euclidean Distance,

15
00:00:42,100 --> 00:00:46,710
and the Euclidean Distance is affected by variable magnitudes.

16
00:00:46,710 --> 00:00:48,110
For example,

17
00:00:48,110 --> 00:00:53,260
a dataset that's entered with a specific feature in centimeters would give

18
00:00:53,260 --> 00:00:57,840
different results than a dataset with the same feature in inches.

19
00:00:57,840 --> 00:01:03,470
It's an inherent limitation by design on some machine learning algorithms.

20
00:01:03,470 --> 00:01:09,100
Let's now discuss the solution for data with multiple scales issue.

21
00:01:09,100 --> 00:01:12,840
Let's assume that we have the following dataset with sales price,

22
00:01:12,840 --> 00:01:19,140
where the first two items are in USD, while the third one is in British pound.

23
00:01:19,140 --> 00:01:23,040
This is clearly a problematic case since the British pound is in

24
00:01:23,040 --> 00:01:26,270
a different unit scale than the US dollar.

25
00:01:26,270 --> 00:01:30,500
The solution would be to multiply the British pound with a correct scale,

26
00:01:30,500 --> 00:01:32,650
in this case, the exchange rate.

27
00:01:32,650 --> 00:01:35,800
Let's say that it is 1.25.

28
00:01:35,800 --> 00:01:42,360
And here we have the new dataset with one scale across all features when

29
00:01:42,360 --> 00:01:49,440
we scaled the 30 British pound to 37.53 US dollars.

30
00:01:49,440 --> 00:01:53,640
There are several techniques to solve the feature magnitudes challenge.

31
00:01:53,640 --> 00:01:56,540
We will discuss the most commonly used ones.

32
00:01:56,540 --> 00:02:01,110
Standardization removes the mean and scales the data to unit variance.

33
00:02:01,110 --> 00:02:05,350
MinMax rescales the dataset such that all features

34
00:02:05,350 --> 00:02:07,640
are in a range between 0 and 1.

35
00:02:07,640 --> 00:02:13,760
And normalization scales the vector for each sample to unit norm,

36
00:02:13,760 --> 00:02:18,700
independently of the distribution of samples.

37
00:02:18,700 --> 00:02:23,400
The core theory behind these approaches is that they are representing the data

38
00:02:23,400 --> 00:02:27,070
in relative magnitudes greater than absolute magnitudes and,

39
00:02:27,070 --> 00:02:30,790
hence, removing any scale effect from the dataset.

40
00:02:30,790 --> 00:02:32,370
To sum up,

41
00:02:32,370 --> 00:02:36,680
always remember to make sure that all feature columns

42
00:02:36,680 --> 00:02:39,590
have the same scale across the dataset.

43
00:02:39,590 --> 00:02:43,530
This is done by multiplying by the right scaling factor.

44
00:02:43,530 --> 00:02:52,000
And always scale your features using a standardization technique if the underlying machine learning algorithm calculates distance.

