WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:00.330
Hi,

00:00:00.330 --> 00:00:00.790
everybody.

00:00:00.790 --> 00:00:01.960
And welcome to this lesson.

00:00:01.960 --> 00:00:05.750
I'm looking at the different training models that are offered by AWS.

00:00:05.750 --> 00:00:10.860
So the machine learning supports three different types off models.

00:00:10.860 --> 00:00:12.740
You have been a reclassification,

00:00:12.740 --> 00:00:16.050
multi class classification and regression.

00:00:16.050 --> 00:00:24.400
So models for Bonaly binary classification problems predict a binary outcome one off to

00:00:24.400 --> 00:00:28.230
possible classes and to train these models,

00:00:28.230 --> 00:00:32.910
it uses the industry standard learning a longer than known as logistic regression,

00:00:32.910 --> 00:00:34.080
for example.

00:00:34.080 --> 00:00:35.330
Some examples are.

00:00:35.330 --> 00:00:37.140
Is this email spam or not?

00:00:37.140 --> 00:00:37.580
Which again,

00:00:37.580 --> 00:00:38.500
is a yes or no?

00:00:38.500 --> 00:00:39.560
Will the customer buy?

00:00:39.560 --> 00:00:42.010
This product is a product,

00:00:42.010 --> 00:00:44.720
a book or a farm animal?

00:00:44.720 --> 00:00:47.680
Is this review written by a customer or a robot again,

00:00:47.680 --> 00:00:50.650
and either or in the multi class,

00:00:50.650 --> 00:00:55.740
it allows you to generate predictions for multiple classes.

00:00:55.740 --> 00:00:59.370
Just predict one off more than two outcomes.

00:00:59.370 --> 00:01:07.200
And for this AWS uses and longer than known as the multi a no meal logistic regression.

00:01:07.200 --> 00:01:07.660
So,

00:01:07.660 --> 00:01:08.330
for example,

00:01:08.330 --> 00:01:09.630
is this product a book,

00:01:09.630 --> 00:01:11.240
movie or clothing?

00:01:11.240 --> 00:01:13.940
Is this movie a romantic comedy,

00:01:13.940 --> 00:01:16.450
a documentary or thriller or horror?

00:01:16.450 --> 00:01:21.420
Which category of products is most interesting to this customer and you would have a list

00:01:21.420 --> 00:01:24.460
off products haven't finally,

00:01:24.460 --> 00:01:25.640
the regression model,

00:01:25.640 --> 00:01:28.570
and that basically predicts a numerical value.

00:01:28.570 --> 00:01:33.720
And for this AWS uses the longer them linear regression.

00:01:33.720 --> 00:01:34.030
So,

00:01:34.030 --> 00:01:34.800
for example,

00:01:34.800 --> 00:01:38.530
what is with the temperature in Dubai tomorrow?

00:01:38.530 --> 00:01:46.190
Or how much will the sell for on what the price of this house so so on the three main model

00:01:46.190 --> 00:01:48.950
types that are used by Amazon machine learning.

00:01:48.950 --> 00:01:51.750
So to train the machine learning model,

00:01:51.750 --> 00:01:54.080
you need to specify a few things.

00:01:54.080 --> 00:01:55.180
First of all,

00:01:55.180 --> 00:01:58.200
you need to do it in put training data source.

00:01:58.200 --> 00:02:04.510
Name off the data attribute that contains the target to be predicted the required data

00:02:04.510 --> 00:02:09.950
transformation instructions and then finally training primaries to control the learning a

00:02:09.950 --> 00:02:10.670
logger them.

00:02:10.670 --> 00:02:12.630
And during the training process,

00:02:12.630 --> 00:02:17.420
the machine learning automatically selects the correct learning longer before you based on

00:02:17.420 --> 00:02:21.690
the type off target that you specified in the training data source.

00:02:21.690 --> 00:02:24.300
So it's a very automated and easy way off,

00:02:24.300 --> 00:02:29.240
starting out and doing your machine learning and typically machine learning longer than

00:02:29.240 --> 00:02:34.440
accept parameters that could be used to control certain properties of the training process

00:02:34.440 --> 00:02:35.950
and of the resulting model.

00:02:35.950 --> 00:02:39.910
Not in AWS thes air called training parameters,

00:02:39.910 --> 00:02:43.350
and you can set these parameters using the console,

00:02:43.350 --> 00:02:46.080
the A P I or even the command line interface.

00:02:46.080 --> 00:02:48.430
If you don't set any parameters,

00:02:48.430 --> 00:02:48.870
don't worry,

00:02:48.870 --> 00:02:55.040
because the AWS will use default values that are known toe work well for a large range off

00:02:55.040 --> 00:03:01.080
machine learning tasks so you can specify values for the following training parameters that

00:03:01.080 --> 00:03:01.830
you guys see.

00:03:01.830 --> 00:03:04.550
You can specify parameters for the model size,

00:03:04.550 --> 00:03:07.450
the max number of passes to shuffle type,

00:03:07.450 --> 00:03:09.950
the regionalization type and the amount.

00:03:09.950 --> 00:03:13.820
Not all of these parameters are set by default,

00:03:13.820 --> 00:03:18.150
and the default settings are adequate for most machine learning problems.

00:03:18.150 --> 00:03:18.960
But again,

00:03:18.960 --> 00:03:21.010
depending on your specific business case,

00:03:21.010 --> 00:03:26.850
you are able to choose and define your own values and fine tune them based on your data.

00:03:26.850 --> 00:03:30.400
Not to discuss a few of these in a little bit more detail.

00:03:30.400 --> 00:03:30.900
I won't get,

00:03:30.900 --> 00:03:31.370
too.

00:03:31.370 --> 00:03:33.290
I won't discuss all of them for you,

00:03:33.290 --> 00:03:35.690
but I'll discuss a few of them for you.

00:03:35.690 --> 00:03:39.270
So what do we mean by saying Maxim model size?

00:03:39.270 --> 00:03:44.570
That's basically and the units of bites the total size of your model.

00:03:44.570 --> 00:03:47.160
So by default,

00:03:47.160 --> 00:03:53.330
AWS creates 100 meg model and you construct it to create a smaller or larger model bus,

00:03:53.330 --> 00:03:55.170
pacifying a different size.

00:03:55.170 --> 00:03:57.420
If it,

00:03:57.420 --> 00:03:58.260
for example,

00:03:58.260 --> 00:04:01.190
cannot find enough patterns to fill the model size,

00:04:01.190 --> 00:04:03.840
it creates a small model for you automatically.

00:04:03.840 --> 00:04:04.280
So,

00:04:04.280 --> 00:04:05.010
for example,

00:04:05.010 --> 00:04:07.850
if you specify a maximum model size of 100 megs,

00:04:07.850 --> 00:04:11.120
but it finds patterns that are only 50 megs,

00:04:11.120 --> 00:04:13.290
the resulting model only be 50 megs,

00:04:13.290 --> 00:04:16.190
so it's not going to enlarge the data for you.

00:04:16.190 --> 00:04:17.870
It'll only reduce it.

00:04:17.870 --> 00:04:21.960
Choosing the right model basically allows you to control the trade off between the

00:04:21.960 --> 00:04:24.610
predictive quality and the cost of use,

00:04:24.610 --> 00:04:30.100
because small models can cause the total BS to remove many patterns to fit within a maximum

00:04:30.100 --> 00:04:32.760
size limit affecting the quality of predictions.

00:04:32.760 --> 00:04:36.540
But the smaller models also cost less.

00:04:36.540 --> 00:04:37.070
So again,

00:04:37.070 --> 00:04:42.100
there's trade off that you need to decide the maximum number of passes and or for best

00:04:42.100 --> 00:04:42.800
results,

00:04:42.800 --> 00:04:47.500
it may need to make multiple passes over your data to discover the patterns,

00:04:47.500 --> 00:04:48.810
because most of the time,

00:04:48.810 --> 00:04:51.190
by this going over through one pass,

00:04:51.190 --> 00:04:55.310
it will not be able to correctly predict and discover all patterns,

00:04:55.310 --> 00:04:57.840
so by default it makes 10 passes,

00:04:57.840 --> 00:05:02.850
but you can change the default by setting number upto a maximum of 100 passes.

00:05:02.850 --> 00:05:03.490
So,

00:05:03.490 --> 00:05:03.920
for example,

00:05:03.920 --> 00:05:09.070
if you set the number of passes just to 20 but AWS discovers that no new patterns can be

00:05:09.070 --> 00:05:10.870
found by the end of 15 passes,

00:05:10.870 --> 00:05:11.760
then it will stop.

00:05:11.760 --> 00:05:18.640
Even if you specify a maximum of 100 it will stop when it stops discovering new patterns.

00:05:18.640 --> 00:05:22.880
You can also shuffle your training data and what that basically does.

00:05:22.880 --> 00:05:27.210
It mixes up the order of your data so that the longer them doesn't in cholera.

00:05:27.210 --> 00:05:30.310
One type of data for too many observations in succession.

00:05:30.310 --> 00:05:31.200
For example,

00:05:31.200 --> 00:05:35.500
if you're training a model to predict a product type your training data includes,

00:05:35.500 --> 00:05:36.530
let's see a movie,

00:05:36.530 --> 00:05:38.250
a toy video game,

00:05:38.250 --> 00:05:39.060
product types.

00:05:39.060 --> 00:05:43.130
And if you sort of the data by the product type column before uploading it,

00:05:43.130 --> 00:05:46.950
the longer term sees the data alphabetically by the product type,

00:05:46.950 --> 00:05:51.250
and it sees all of your data for movies first,

00:05:51.250 --> 00:05:54.310
and your model begins to learn patterns for movies.

00:05:54.310 --> 00:05:54.680
Then,

00:05:54.680 --> 00:05:57.030
when your model encounters the next option,

00:05:57.030 --> 00:05:58.580
which is toys in this example,

00:05:58.580 --> 00:06:05.080
every update that they log in the makes would fit the model to the toy product type.

00:06:05.080 --> 00:06:08.830
Even if those updates degrade the patterns that fit movies.

00:06:08.830 --> 00:06:13.640
So this sudden switch from movie the toy type can produce a model that doesn't really learn

00:06:13.640 --> 00:06:16.300
how to predict product types accurately.

00:06:16.300 --> 00:06:23.410
So shuffling basically shuffles all those up and allows a mile to predict it more by using

00:06:23.410 --> 00:06:25.280
by doing random shuffling.

00:06:25.280 --> 00:06:26.280
Now,

00:06:26.280 --> 00:06:26.730
lastly,

00:06:26.730 --> 00:06:29.140
the regularization both type in a mountain.

00:06:29.140 --> 00:06:35.420
Most predictive performance of very large or complex models suffer when the data contains

00:06:35.420 --> 00:06:36.400
too many patterns.

00:06:36.400 --> 00:06:38.690
As the number of patterns increases,

00:06:38.690 --> 00:06:43.790
so does the likely hold that the model learned unintentional data artifacts rather than

00:06:43.790 --> 00:06:44.850
true patterns.

00:06:44.850 --> 00:06:46.290
When that happens,

00:06:46.290 --> 00:06:52.180
the model does very well on the training data but doesn't really farewell in the

00:06:52.180 --> 00:06:54.140
generalization off new data.

00:06:54.140 --> 00:06:59.310
And this is basically what a lot of people over the industry of first to as over fitting

00:06:59.310 --> 00:07:00.350
the training data.

00:07:00.350 --> 00:07:06.060
So regular list helps prevent linear model from over fitting training data examples by

00:07:06.060 --> 00:07:08.740
penalizing extreme Vaid values.

00:07:08.740 --> 00:07:13.580
And I'm not gonna get into too much of the details off the regularization because it does

00:07:13.580 --> 00:07:15.560
get a bit outside of the scope of this course.

00:07:15.560 --> 00:07:20.510
But I just want you familiar as you guys with the different parameters that are used by

00:07:20.510 --> 00:07:23.750
eight of US machine learning and times off training

