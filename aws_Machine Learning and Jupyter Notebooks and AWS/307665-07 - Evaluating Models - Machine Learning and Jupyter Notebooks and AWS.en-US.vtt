WEBVTT
X-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:0

00:00:00.000 --> 00:00:00.830
everybody.

00:00:00.830 --> 00:00:02.080
And welcome to his lesson.

00:00:02.080 --> 00:00:06.650
I'm looking at evaluating the machine learning models developed by AWS,

00:00:06.650 --> 00:00:08.920
so you should always as a good practice.

00:00:08.920 --> 00:00:13.380
You're always evaluate a model to determine if it will do a good job of predicting the

00:00:13.380 --> 00:00:16.410
target on a new or and future data.

00:00:16.410 --> 00:00:19.880
Because Fisher instances have unknown target values,

00:00:19.880 --> 00:00:24.640
you need to check the accuracy metric off the model on data for which you already know the

00:00:24.640 --> 00:00:25.050
target.

00:00:25.050 --> 00:00:27.440
Answer and the news is assessment.

00:00:27.440 --> 00:00:30.750
As a proxy for predictive accuracy on future data,

00:00:30.750 --> 00:00:33.460
not a properly valued a model.

00:00:33.460 --> 00:00:38.770
You hold out a sample of data that has been labeled with a target from the training data

00:00:38.770 --> 00:00:39.270
source.

00:00:39.270 --> 00:00:44.580
Evaluating the predictive accuracy with the same data that was used for training is not

00:00:44.580 --> 00:00:49.410
really useful because it just kind of rewards the model for remembering the training data

00:00:49.410 --> 00:00:51.250
rather than generalizing from it.

00:00:51.250 --> 00:00:52.060
Now,

00:00:52.060 --> 00:00:53.530
in Amazon machine learning,

00:00:53.530 --> 00:00:59.380
you can evaluate a model by using or by creating an evaluation and to create an evaluation

00:00:59.380 --> 00:00:59.380
.

00:00:59.380 --> 00:00:59.950
For a model,

00:00:59.950 --> 00:01:05.850
you need a model that you want to evaluate and you need the label data that was not used

00:01:05.850 --> 00:01:06.480
for training.

00:01:06.480 --> 00:01:12.470
So let's go ahead and look at how we can evaluate the different types of models from the

00:01:12.470 --> 00:01:16.540
insights love and you value in a model it provides.

00:01:16.540 --> 00:01:21.400
Our Amazon machine Learning provides an industry standard metric and a number of insights

00:01:21.400 --> 00:01:23.370
to review the predictive accuracy off.

00:01:23.370 --> 00:01:27.900
The model of outcome of evaluation contains a number of insights.

00:01:27.900 --> 00:01:28.460
For example,

00:01:28.460 --> 00:01:34.010
contains a prediction accuracy metric to report on the overall success visualizations To

00:01:34.010 --> 00:01:39.900
help explore the accuracy of your model beyond the prediction accuracy metric the ability

00:01:39.900 --> 00:01:43.740
to review the impact of setting a score threshold.

00:01:43.740 --> 00:01:49.780
Just keep in mind that's Onley for binary classification and alerts on criteria to check

00:01:49.780 --> 00:01:51.730
the validity of the evaluation.

00:01:51.730 --> 00:01:56.950
Now the choice of the metric and reservation depends on the type of model that you're

00:01:56.950 --> 00:01:57.780
evaluating.

00:01:57.780 --> 00:02:02.590
So it's important to review these visualizations to decide if your model is performing.

00:02:02.590 --> 00:02:06.340
Roll enough to match your business requirements and your expectations.

00:02:06.340 --> 00:02:11.950
How the actual output of many Byner classifications longer them's is what's called a

00:02:11.950 --> 00:02:13.090
prediction score.

00:02:13.090 --> 00:02:18.490
The score indicates the system certainty that the given observation belongs to the positive

00:02:18.490 --> 00:02:26.070
class by reclassification models output a score that ranges from 0 to 1 and as a consumer

00:02:26.070 --> 00:02:31.310
of the score to make the decision about whether the observation should be classified as one

00:02:31.310 --> 00:02:31.840
or zero.

00:02:31.840 --> 00:02:37.210
You interpret the score by picking classifications threshold or what's called a cut off and

00:02:37.210 --> 00:02:38.670
compare the score against it.

00:02:38.670 --> 00:02:44.710
And the observations would scores higher than the cut off are predicted as target one and

00:02:44.710 --> 00:02:46.440
scores lore are predicted.

00:02:46.440 --> 00:02:50.980
A zero of the default cut off is 00.5 for Amazon,

00:02:50.980 --> 00:02:55.210
so you can choose to update this cut off to match your business needs.

00:02:55.210 --> 00:02:58.820
You can also use the visualizations in the console,

00:02:58.820 --> 00:03:03.580
which will look at in the demonstrations to understand how the choice off cut off will

00:03:03.580 --> 00:03:05.160
affect your application.

00:03:05.160 --> 00:03:12.110
Now it provides an industry standard accuracy metric for buying reclassification models,

00:03:12.110 --> 00:03:15.730
called a U C or our area under the curve.

00:03:15.730 --> 00:03:21.320
That measures the ability of the model to predict a higher score for positive examples as

00:03:21.320 --> 00:03:22.700
compared to negative ones.

00:03:22.700 --> 00:03:26.050
Because it is independent of the score cut off,

00:03:26.050 --> 00:03:31.220
you can get a sense of the prediction accuracy of your model from the au symmetric without

00:03:31.220 --> 00:03:32.370
picking a threshold.

00:03:32.370 --> 00:03:39.840
It usually returns a decimal value from 01 and values near one indicate a model that's

00:03:39.840 --> 00:03:40.890
highly accurate.

00:03:40.890 --> 00:03:43.720
Values near 10.5 indicated model.

00:03:43.720 --> 00:03:49.500
That's no better than guessing at random and values that zero are unusual to see and

00:03:49.500 --> 00:03:52.040
typically indicate a problem with the data.

00:03:52.040 --> 00:03:52.750
So,

00:03:52.750 --> 00:03:53.220
essentially,

00:03:53.220 --> 00:03:58.410
an A you see near zero says that the model has learned the correct patterns but is using

00:03:58.410 --> 00:04:06.590
them to make predictions that are opposite from reality and just keep in mind the baseline

00:04:06.590 --> 00:04:07.050
for the a.

00:04:07.050 --> 00:04:07.390
U C.

00:04:07.390 --> 00:04:07.760
Metric.

00:04:07.760 --> 00:04:08.230
For Biden,

00:04:08.230 --> 00:04:09.930
remodel is 0.5,

00:04:09.930 --> 00:04:11.330
so it is a value for ah,

00:04:11.330 --> 00:04:13.100
hypothetical machine learning model.

00:04:13.100 --> 00:04:16.570
Let randomly predicts a one or zero answer.

00:04:16.570 --> 00:04:19.960
Explore the accuracy of the model you can review,

00:04:19.960 --> 00:04:25.310
and we will review the graphs in evaluation through the console and a model that is good

00:04:25.310 --> 00:04:30.460
predictive accuracy will predict higher scores to the actual ones and lower scores to the

00:04:30.460 --> 00:04:31.380
actual zeros.

00:04:31.380 --> 00:04:31.940
Ah,

00:04:31.940 --> 00:04:38.340
perfect model will have to history grams at two different ends of the X axis showing the

00:04:38.340 --> 00:04:39.630
actual positives.

00:04:39.630 --> 00:04:43.840
All got high scores and actual negatives all got lost course.

00:04:43.840 --> 00:04:44.850
However,

00:04:44.850 --> 00:04:47.010
Hammel model makes mistakes,

00:04:47.010 --> 00:04:47.670
obviously,

00:04:47.670 --> 00:04:54.280
and a typical graph will show that the two Instagram's overlap at certain scores an

00:04:54.280 --> 00:04:59.140
extremely poor performing model will be unable to distinguish between the positive and

00:04:59.140 --> 00:05:00.590
medic negative classes,

00:05:00.590 --> 00:05:04.380
and both classes will have mostly overlapping history,

00:05:04.380 --> 00:05:04.990
grams,

00:05:04.990 --> 00:05:11.450
even the examples you guys can see on your screen in terms of the good chart and the bad

00:05:11.450 --> 00:05:12.760
chart on the right hand.

00:05:12.760 --> 00:05:14.710
Now for correct prediction,

00:05:14.710 --> 00:05:17.900
there's what's called a true positive or a teepee,

00:05:17.900 --> 00:05:21.580
which predicted the value as one and the true value is one where,

00:05:21.580 --> 00:05:22.870
as a true negative,

00:05:22.870 --> 00:05:25.630
the model predicted the value of zero,

00:05:25.630 --> 00:05:27.350
and the true value is zero.

00:05:27.350 --> 00:05:29.600
And then you have Aaron s predictions,

00:05:29.600 --> 00:05:30.420
which is a false,

00:05:30.420 --> 00:05:32.310
positive or false negative.

00:05:32.310 --> 00:05:37.460
And then you can also adjust the score cut off by changing the score cut off.

00:05:37.460 --> 00:05:39.530
You adjust the models behavior.

00:05:39.530 --> 00:05:40.950
When it makes a mistake,

00:05:40.950 --> 00:05:44.580
just keep in mind when you're moving the cut off to the left,

00:05:44.580 --> 00:05:46.700
it will capture more true positives.

00:05:46.700 --> 00:05:50.770
But the trade off is an increase in the number of false positive airs.

00:05:50.770 --> 00:05:54.850
Moving into the right captures less of the false positive errors,

00:05:54.850 --> 00:05:55.540
but again,

00:05:55.540 --> 00:05:58.550
there's a trade off that it will miss some true positives.

00:05:58.550 --> 00:06:00.510
So for your predictive applications,

00:06:00.510 --> 00:06:01.600
you can make a decision.

00:06:01.600 --> 00:06:02.750
You have to make a decision,

00:06:02.750 --> 00:06:07.550
which kind of air is more tolerable by selecting an appropriate cut off score.

00:06:07.550 --> 00:06:11.210
And as I mentioned in the very beginning of this course,

00:06:11.210 --> 00:06:13.510
machine learning is an iterative process,

00:06:13.510 --> 00:06:18.420
so most likely you will have to test out a different cut off point to see which ones will

00:06:18.420 --> 00:06:20.700
work better for your business case,

00:06:20.700 --> 00:06:23.420
then we have the multi class.

00:06:23.420 --> 00:06:23.550
On.

00:06:23.550 --> 00:06:28.180
The actual output of this class is a set of prediction scores,

00:06:28.180 --> 00:06:33.410
so the scores indicate the models certainty that the certain observation belongs to each of

00:06:33.410 --> 00:06:34.240
the classes.

00:06:34.240 --> 00:06:35.810
And unlike binary,

00:06:35.810 --> 00:06:39.450
you do not need to choose a score cut off to make predictions.

00:06:39.450 --> 00:06:41.570
The predictive answer is the class.

00:06:41.570 --> 00:06:42.140
For example,

00:06:42.140 --> 00:06:45.050
the label with the highest predicted score.

00:06:45.050 --> 00:06:51.110
So typical metrics used in multi class are the same as a metrics used in Byron

00:06:51.110 --> 00:06:55.810
classifications case after averaging them over all classes,

00:06:55.810 --> 00:06:58.040
not in AWS,

00:06:58.040 --> 00:06:58.750
the Mac er,

00:06:58.750 --> 00:07:05.830
average F one score is used to evaluate the predictive accuracy off a multi class metric of

00:07:05.830 --> 00:07:06.190
the effort.

00:07:06.190 --> 00:07:11.120
Score is basically a binary classification metric that considers both binary metrics,

00:07:11.120 --> 00:07:13.280
precision and recall.

00:07:13.280 --> 00:07:17.270
It is the harmonic mean between precision and recall,

00:07:17.270 --> 00:07:20.960
and again the ranges from 0 to 1 are larger.

00:07:20.960 --> 00:07:23.200
Value indicates better predictive accuracy,

00:07:23.200 --> 00:07:28.970
and you guys can see the mathematical calculation for the F one score are the macro average

00:07:28.970 --> 00:07:28.970
.

00:07:28.970 --> 00:07:34.870
F one is an unweighted average off the F one score over all classes in the case.

00:07:34.870 --> 00:07:40.320
It does not take into account the frequency of occurrence of the classes and the data set,

00:07:40.320 --> 00:07:43.760
so a larger value indicates better predictive accuracy.

00:07:43.760 --> 00:07:44.820
Well,

00:07:44.820 --> 00:07:49.660
keep in mind that eight Elvis does provide baseline metric four multi class models.

00:07:49.660 --> 00:07:51.070
For example,

00:07:51.070 --> 00:07:55.870
if you were predicting a genre off movie and the most common genre in your training,

00:07:55.870 --> 00:07:57.890
that data set was romance.

00:07:57.890 --> 00:07:58.690
For example,

00:07:58.690 --> 00:08:03.240
the baseline model would always predict the genre as romance.

00:08:03.240 --> 00:08:08.680
You would compare your model against the baseline to validate if you're ML model is better

00:08:08.680 --> 00:08:12.150
than the ML model that predicts this constant answer,

00:08:12.150 --> 00:08:17.520
and this is where you can use the performance visualization that you guys see on the right

00:08:17.520 --> 00:08:18.190
hand side.

00:08:18.190 --> 00:08:23.590
It basically provides a confusion matrix as a way to visualize the accuracy off the multi

00:08:23.590 --> 00:08:29.820
class and the confusion matrix illustrates in a table form the number or percentage of

00:08:29.820 --> 00:08:36.160
correct and incorrect predictions for each class by comparing and observations,

00:08:36.160 --> 00:08:37.130
predicted class.

00:08:37.130 --> 00:08:38.180
And it's true class.

00:08:38.180 --> 00:08:38.850
So,

00:08:38.850 --> 00:08:39.540
for example,

00:08:39.540 --> 00:08:44.190
going back to the movies if you're trying to classify a movie into a genre,

00:08:44.190 --> 00:08:47.700
the predictive model might predict that this genre is romance.

00:08:47.700 --> 00:08:48.630
However,

00:08:48.630 --> 00:08:51.200
it's true genre might actually be a thriller.

00:08:51.200 --> 00:08:56.190
So when you evaluate theocracy off a multi class classification model,

00:08:56.190 --> 00:09:02.520
AWS identifies this miss classifications and displays the results in the confusion matrix.

00:09:02.520 --> 00:09:04.750
As you guys can see in that illustration,

00:09:04.750 --> 00:09:09.820
Sebesta displays a number of features.

00:09:09.820 --> 00:09:12.790
The number of correct and incorrect predictions freeze class,

00:09:12.790 --> 00:09:19.060
the class wise F one score true class frequencies in the evaluation data and the predicted

00:09:19.060 --> 00:09:21.550
class frequencies for the evaluation data.

00:09:21.550 --> 00:09:24.530
So it does provide a visual display,

00:09:24.530 --> 00:09:29.700
and it can accommodate up to 10 classes in the confusion Matrix listed in the order of most

00:09:29.700 --> 00:09:35.460
frequent to the least frequent class in the evaluation data of the output of a regression

00:09:35.460 --> 00:09:38.130
model is a numeric value for the model's prediction.

00:09:38.130 --> 00:09:38.590
So,

00:09:38.590 --> 00:09:39.120
for example,

00:09:39.120 --> 00:09:40.080
if you're predicting,

00:09:40.080 --> 00:09:41.500
let's say housing prices,

00:09:41.500 --> 00:09:50.250
the prediction of the model could be a value such as 300,000 or 357,004 and 55 or and so on

00:09:50.250 --> 00:09:50.250
.

00:09:50.250 --> 00:09:52.390
So for regression tasks,

00:09:52.390 --> 00:09:57.440
it'll be us uses the industry standard root mean square air or R M s E.

00:09:57.440 --> 00:10:01.870
Because the distance measure between their predicted in America Target and the actual

00:10:01.870 --> 00:10:02.470
answer.

00:10:02.470 --> 00:10:07.400
So the smaller the RMS see the better is the predictive accuracy of the model.

00:10:07.400 --> 00:10:12.480
And a model with a perfectly correct predictions would have a value off zero.

00:10:12.480 --> 00:10:15.540
Now again,

00:10:15.540 --> 00:10:16.450
like with most things,

00:10:16.450 --> 00:10:20.060
AWS does provide a baseline metric or regressive models.

00:10:20.060 --> 00:10:24.300
And it's the arrest and me for a hypothetical regression model that would always predict

00:10:24.300 --> 00:10:26.970
the mean off the target as the answer.

00:10:26.970 --> 00:10:27.180
So,

00:10:27.180 --> 00:10:27.860
for example,

00:10:27.860 --> 00:10:32.700
if you're predicting the age of our house buyer and the mean age for the observations in

00:10:32.700 --> 00:10:40.780
your training data set was 35 the baseline model always predict the answer as 35 and again

00:10:40.780 --> 00:10:40.780
,

00:10:40.780 --> 00:10:41.630
just like with the other ones.

00:10:41.630 --> 00:10:44.240
You can also use the performance visualization with you guys.

00:10:44.240 --> 00:10:45.380
See on the right hand side.

00:10:45.380 --> 00:10:46.090
Basically,

00:10:46.090 --> 00:10:49.480
is your history graham off the residuals on the evaluation data?

00:10:49.480 --> 00:10:53.430
One distributed in a bell shape and centered at zero,

00:10:53.430 --> 00:10:58.410
indicating that the model makes mistakes in a random manner and does not systematically

00:10:58.410 --> 00:11:02.680
over or under predict any particular range of target values.

00:11:02.680 --> 00:11:05.620
Then we have cross validation,

00:11:05.620 --> 00:11:07.300
which is a technique for evaluating,

00:11:07.300 --> 00:11:13.550
modeled by training several models on subsets off the available input data and evaluating

00:11:13.550 --> 00:11:16.350
them on the complimentary subset of the data.

00:11:16.350 --> 00:11:17.510
So basically,

00:11:17.510 --> 00:11:22.600
you would use cross validation to detect over fitting or also known as failing to

00:11:22.600 --> 00:11:24.030
generalize a pattern.

00:11:24.030 --> 00:11:25.200
So basically,

00:11:25.200 --> 00:11:29.540
the diagram that you guys see shows an example for training subsets and complementary

00:11:29.540 --> 00:11:35.040
evaluation subsets generated for each of the four models that are created and trained

00:11:35.040 --> 00:11:38.270
during during a four fold cross validation.

00:11:38.270 --> 00:11:45.280
So Model the first model uses the 1st 25% off of the data for evaluation and the remaining

00:11:45.280 --> 00:11:45.970
for training.

00:11:45.970 --> 00:11:50.930
The 2nd 1 uses 25% for evaluation and then so on.

00:11:50.930 --> 00:11:56.380
Each model is trained and evaluated using complementary data sources and the data in the

00:11:56.380 --> 00:12:00.530
evaluation data source includes and is limited toe all of the data.

00:12:00.530 --> 00:12:03.360
That is not the training data source.

00:12:03.360 --> 00:12:05.630
The biscuit About performing this validation,

00:12:05.630 --> 00:12:11.270
you would generate four different models for data sources to train the models fourty valued

00:12:11.270 --> 00:12:19.020
them and four evaluations for each model and then finally AWS does provide insights to help

00:12:19.020 --> 00:12:22.180
you validate whether you evaluated the model correctly.

00:12:22.180 --> 00:12:26.330
And if any of the validation criteria are not met by the valuation.

00:12:26.330 --> 00:12:32.060
Hato Bs will alert you by displaying and let you know that a validation criteria has been

00:12:32.060 --> 00:12:37.650
violated and it will do so according to these five metric city.

00:12:37.650 --> 00:12:43.270
I see either the valuation model is done on held out data for now.

00:12:43.270 --> 00:12:48.070
But if you use the same data source for training and evaluation if sufficient data was used

00:12:48.070 --> 00:12:50.680
for evaluation off the predictive model,

00:12:50.680 --> 00:12:51.380
for example,

00:12:51.380 --> 00:12:56.350
if the number off records in your evaluation data is less than 10% of the number of

00:12:56.350 --> 00:13:02.410
observations you have in your training for DAPA or if the schema is matched or mismatched.

00:13:02.410 --> 00:13:08.800
All records from evaluation files were used for predictive model performance evaluation and

00:13:08.800 --> 00:13:11.110
finally the distribution off the target variable.

00:13:11.110 --> 00:13:16.580
So I because got a good overview in this lesson on how AWS values to different models and

00:13:16.580 --> 00:13:22.740
how we can use the metrics provided by AWS to evaluate if the models that we have developed

00:13:22.740 --> 00:13:28.360
for our data and for our business case are working properly or if we to change them and

00:13:28.360 --> 00:13:33.350
then see how in which options are available for us to use based on the binary or multi

00:13:33.350 --> 00:13:34.570
class classifications.

00:13:34.570 --> 00:13:39.360
To see how we can find tune the machine learning model Istres to help us predict the

00:13:39.360 --> 00:13:40.260
correct answers.

