1
00:00:02,440 --> 00:00:05,700
Hi there, and welcome this module in our Pluralsight course,

2
00:00:05,700 --> 00:00:09,740
Monitoring and Event Response on AWS for DevOps Engineers.

3
00:00:09,740 --> 00:00:10,790
My name is Mike Brown.

4
00:00:10,790 --> 00:00:12,200
In this module,

5
00:00:12,200 --> 00:00:16,190
we're going to be looking at AWS CloudWatch log retention and archiving.

6
00:00:16,190 --> 00:00:21,130
In this module, we're going to learn how to store logs for long‑term retention.

7
00:00:21,130 --> 00:00:23,440
By storing logs long term,

8
00:00:23,440 --> 00:00:26,640
we can keep coming back to those logs and asking different

9
00:00:26,640 --> 00:00:29,140
business questions against the data inside there.

10
00:00:29,140 --> 00:00:31,810
There may also be different compliance reasons that we

11
00:00:31,810 --> 00:00:33,660
have to retain logs longer term.

12
00:00:33,660 --> 00:00:36,210
So we're going to discuss the importance of long‑term log

13
00:00:36,210 --> 00:00:40,300
retention and demonstrate how to archive logs from

14
00:00:40,300 --> 00:00:43,940
CloudWatch into products like S3.

15
00:00:43,940 --> 00:00:47,710
So let's begin by discussing log retention and archiving logs.

16
00:00:47,710 --> 00:00:51,940
So with CloudWatch log retention, we have two options.

17
00:00:51,940 --> 00:00:54,130
The default is never expire.

18
00:00:54,130 --> 00:00:59,000
So if you do nothing, all your log data will be stored indefinitely.

19
00:00:59,000 --> 00:01:01,340
The second option is expire after.

20
00:01:01,340 --> 00:01:03,360
So if you want your long information to expire,

21
00:01:03,360 --> 00:01:07,280
you can configure settings from 1 day to 10 years.

22
00:01:07,280 --> 00:01:09,940
And there are several factors to consider,

23
00:01:09,940 --> 00:01:14,240
which will guide us towards either never expire or expire after.

24
00:01:14,240 --> 00:01:21,140
So one of the first things to know is we get 5GB of logs for free per month.

25
00:01:21,140 --> 00:01:23,940
This 5GB includes log storage,

26
00:01:23,940 --> 00:01:28,060
but also actions like data routing. After that 5GB,

27
00:01:28,060 --> 00:01:32,300
we are charged a per‑GB fee for log collection,

28
00:01:32,300 --> 00:01:35,240
storage, and analysis.

29
00:01:35,240 --> 00:01:37,950
That analysis covers features like Log Insights.

30
00:01:37,950 --> 00:01:41,980
So the more you store and analyze above that 5GB,

31
00:01:41,980 --> 00:01:44,370
the more expensive CloudWatch logs gets.

32
00:01:44,370 --> 00:01:46,940
In a region like eu‑west‑2 then,

33
00:01:46,940 --> 00:01:51,960
we are paying 0.0315 per gigabyte for log storage.

34
00:01:51,960 --> 00:01:53,770
Now that might seem like a small amount,

35
00:01:53,770 --> 00:01:55,950
but these fees still start to add up,

36
00:01:55,950 --> 00:02:00,520
particularly when you consider how much data we're collecting in logs every day.

37
00:02:00,520 --> 00:02:02,320
Logs from AWS services,

38
00:02:02,320 --> 00:02:06,750
logs from our EC2 instances will soon mean that this 0.0315

39
00:02:06,750 --> 00:02:09,500
per‑gigabyte fee starts to add up to serious money.

40
00:02:09,500 --> 00:02:14,290
In comparison, if we were to store the same information in Glacier storage,

41
00:02:14,290 --> 00:02:19,570
our per‑gigabyte fee would be 0.0045 per gigabyte.

42
00:02:19,570 --> 00:02:21,900
So it's much cheaper to store logs longer term in

43
00:02:21,900 --> 00:02:24,640
Glacier than it would in CloudWatch.

44
00:02:24,640 --> 00:02:26,640
There's lots of questions to ask here.

45
00:02:26,640 --> 00:02:29,010
How quickly do you need access to log information?

46
00:02:29,010 --> 00:02:32,360
What actions are you performing against those logs on a regular basis?

47
00:02:32,360 --> 00:02:34,550
But if we're talking purely about archival,

48
00:02:34,550 --> 00:02:37,070
then Glacier is a more cost‑effective option for

49
00:02:37,070 --> 00:02:39,120
storing your logs than CloudWatch is.

50
00:02:39,120 --> 00:02:41,140
When working with CloudWatch Logs,

51
00:02:41,140 --> 00:02:44,840
one of the things we can do is export our log data to Amazon S3.

52
00:02:44,840 --> 00:02:48,400
Amazon S3 itself supports some really powerful tools for

53
00:02:48,400 --> 00:02:51,040
working with and storing logging information.

54
00:02:51,040 --> 00:02:54,550
We can also stream our CloudWatch log data to Elasticsearch.

55
00:02:54,550 --> 00:02:57,140
This gives a more powerful search environment,

56
00:02:57,140 --> 00:03:00,140
plus integration with visualization tools like Kibana.

57
00:03:00,140 --> 00:03:03,530
We can also stream log data to Amazon Kinesis.

58
00:03:03,530 --> 00:03:06,470
This gives you the ability to work with that data using features

59
00:03:06,470 --> 00:03:10,190
like Kinesis Analytics and other Kinesis applications. By

60
00:03:10,190 --> 00:03:13,900
archiving our CloudWatch log data to S3, we get the ability to

61
00:03:13,900 --> 00:03:15,870
work with S3 storage classes.

62
00:03:15,870 --> 00:03:19,170
This allows us to choose a storage class for our information with the

63
00:03:19,170 --> 00:03:22,070
correct level of service and at the right price for us.

64
00:03:22,070 --> 00:03:26,780
Once in S3, log data can be used by products like Elastic MapReduce,

65
00:03:26,780 --> 00:03:31,060
EMR, Amazon Redshift, our data warehouse service, as well as search

66
00:03:31,060 --> 00:03:35,520
against that log data using Amazon Athena. It might also be easier to

67
00:03:35,520 --> 00:03:38,980
find third‑party applications that integrate directly with S3

68
00:03:38,980 --> 00:03:42,220
compared to finding third‑party applications that integrate directly

69
00:03:42,220 --> 00:03:43,240
with CloudWatch.

70
00:03:43,240 --> 00:03:46,080
So by getting that CloudWatch data into S3, it could be

71
00:03:46,080 --> 00:03:48,540
used by a wider range of services.

72
00:03:48,540 --> 00:03:51,180
It's also true that CloudWatch is pretty regional.

73
00:03:51,180 --> 00:03:52,680
Whenever we're looking at CloudWatch,

74
00:03:52,680 --> 00:03:56,110
we're looking at CloudWatch for a specific region. Integration with

75
00:03:56,110 --> 00:03:59,220
S3 means that we can take CloudWatch logs from multiple regions,

76
00:03:59,220 --> 00:04:02,680
store them in a single bucket, and have this centralized information

77
00:04:02,680 --> 00:04:04,840
from all of our different regions.

78
00:04:04,840 --> 00:04:08,480
When exporting log data to Amazon S3, we need to create an

79
00:04:08,480 --> 00:04:12,040
S3 bucket to store the information in.

80
00:04:12,040 --> 00:04:14,580
And although the buckets can be encrypted, encryption

81
00:04:14,580 --> 00:04:16,840
with a KMS key is not supported.

82
00:04:16,840 --> 00:04:20,230
We need to use S3 zone encryption on the buckets that we're going to

83
00:04:20,230 --> 00:04:25,280
archive our CloudWatch log data into. Logs from multiple log groups can

84
00:04:25,280 --> 00:04:29,530
be exported to the same bucket, but log data can take up to 12 hours to

85
00:04:29,530 --> 00:04:31,240
become available for export.

86
00:04:31,240 --> 00:04:35,730
So when we export data to Amazon S3, this is not a real‑time service. Log

87
00:04:35,730 --> 00:04:39,510
data will be available CloudWatch almost straightaway, and a some later time,

88
00:04:39,510 --> 00:04:44,190
it's available for export to S3. If you want near real‑time access to log

89
00:04:44,190 --> 00:04:47,670
information, consider using Log Insights. So almost immediately as

90
00:04:47,670 --> 00:04:51,380
information is logged to CloudWatch, Log Insights can view it or consider

91
00:04:51,380 --> 00:04:53,840
integration with Amazon Kinesis.

92
00:04:53,840 --> 00:04:57,850
One other thing to consider is automation. So we can manual export our logs

93
00:04:57,850 --> 00:05:03,680
to S3, or we can set up a scheduled CloudWatch event to trigger a Lambda

94
00:05:03,680 --> 00:05:07,940
function or a Step Function for daily exports to S3.

95
00:05:07,940 --> 00:05:13,140
Remember, automation is key. When we do export log data to S3,

96
00:05:13,140 --> 00:05:15,140
our logs become objects.

97
00:05:15,140 --> 00:05:18,680
Each object has a storage class, and we can manually set the storage

98
00:05:18,680 --> 00:05:21,870
class for each object, or we can use lifecycle rules.

99
00:05:21,870 --> 00:05:24,650
Lifecycle rules allow us to automatically transition objects

100
00:05:24,650 --> 00:05:27,640
between one storage class and another, as well as allowing

101
00:05:27,640 --> 00:05:29,940
us to expire objects as well.

102
00:05:29,940 --> 00:05:32,880
S3 is very feature‑rich, but a couple of features you might want to

103
00:05:32,880 --> 00:05:35,950
consider using are object lock and replication.

104
00:05:35,950 --> 00:05:39,620
Object lock has been configured when your S3 bucket is created. With

105
00:05:39,620 --> 00:05:43,290
object lock, we can put in rules that guarantee that objects cannot be

106
00:05:43,290 --> 00:05:46,340
deleted or altered for a given time period.

107
00:05:46,340 --> 00:05:50,050
This will help us satisfy various compliance standards that we might be trying

108
00:05:50,050 --> 00:05:53,420
to achieve. Replication allows us to replicate objects from one region to

109
00:05:53,420 --> 00:05:58,200
another. We might do this to guard against regional failure or to get our S3

110
00:05:58,200 --> 00:06:05,000
data closer to the people that need it. Let's take a look at some of this integration action then.

