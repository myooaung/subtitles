WEBVTT
1
00:00:02.140 --> 00:00:02.960
Hi there,

2
00:00:02.960 --> 00:00:05.530
and welcome to this module in our Pluralsight course Monitoring

3
00:00:05.530 --> 00:00:08.290
and Event Response on AWS for DevOps Engineers.

4
00:00:08.290 --> 00:00:09.640
My name's Mike Brown,

5
00:00:09.640 --> 00:00:12.960
and in this module we're going to be working with CloudWatch logs,

6
00:00:12.960 --> 00:00:15.440
alarms, and metrics.

7
00:00:15.440 --> 00:00:18.910
In this module we're going to start off by taking a close look at CloudWatch

8
00:00:18.910 --> 00:00:22.960
metrics and alarms before discussing how we can gather custom log

9
00:00:22.960 --> 00:00:26.740
information and system metrics from our EC2 instances.

10
00:00:26.740 --> 00:00:28.530
We're going to discuss CloudTrail events,

11
00:00:28.530 --> 00:00:31.490
building upon what we discussed about CloudTrail in a previous module.

12
00:00:31.490 --> 00:00:34.200
And we'll finish off this module by discussing ways that we

13
00:00:34.200 --> 00:00:37.540
can query log data inside CloudWatch.

14
00:00:37.540 --> 00:00:41.240
Let's begin then by taking a close look at CloudWatch metrics and alarms.

15
00:00:41.240 --> 00:00:43.740
So fundamentally metrics are data,

16
00:00:43.740 --> 00:00:47.010
data about the performance of our systems that are

17
00:00:47.010 --> 00:00:49.130
either run in AWS or on‑premise.

18
00:00:49.130 --> 00:00:53.030
Most AWS services provide metric information into CloudWatch and most

19
00:00:53.030 --> 00:00:57.470
services that do offer a free tier that monitors basic information while

20
00:00:57.470 --> 00:01:00.670
other services offer a free tier and detailed monitoring.

21
00:01:00.670 --> 00:01:03.870
An example of this is EC2 where you can enable detailed

22
00:01:03.870 --> 00:01:05.940
monitoring for each of your EC2 instances,

23
00:01:05.940 --> 00:01:08.390
the caution here being that detailed monitoring

24
00:01:08.390 --> 00:01:10.340
will always cost you extra money.

25
00:01:10.340 --> 00:01:13.870
Once we have metric information inside CloudWatch we can

26
00:01:13.870 --> 00:01:18.140
search through that metric information, build graphs from our search results,

27
00:01:18.140 --> 00:01:21.930
and generate alarms based on metrics that can inform us when a particular

28
00:01:21.930 --> 00:01:26.540
event occurs or start off a whole series of automation events.

29
00:01:26.540 --> 00:01:29.290
Metric data in CloudWatch is kept for 15 months.

30
00:01:29.290 --> 00:01:31.550
This allows us to keep going back to our metric data and

31
00:01:31.550 --> 00:01:34.740
working with it in different ways.

32
00:01:34.740 --> 00:01:38.270
When viewing metric information inside CloudWatch we should

33
00:01:38.270 --> 00:01:41.330
be aware that metrics are grouped first by a namespace and

34
00:01:41.330 --> 00:01:43.070
then by dimension combinations.

35
00:01:43.070 --> 00:01:44.280
So for example,

36
00:01:44.280 --> 00:01:47.700
if you've deployed EC2 instances the metrics of those EC2

37
00:01:47.700 --> 00:01:50.590
instances will be grouped under the EC2 namespace,

38
00:01:50.590 --> 00:01:54.900
but then beneath that EC2 namespace we can view metrics for individual EC2

39
00:01:54.900 --> 00:01:57.870
instances or metrics for an entire auto scale group.

40
00:01:57.870 --> 00:02:01.840
In this case, the entire auto scale group or individual EC2 instances,

41
00:02:01.840 --> 00:02:02.950
they're your dimensions.

42
00:02:02.950 --> 00:02:05.680
Now AWS offers lots and lots of services,

43
00:02:05.680 --> 00:02:07.830
most of which you're probably not even using.

44
00:02:07.830 --> 00:02:11.410
So be aware that when working with CloudWatch only AWS services that

45
00:02:11.410 --> 00:02:13.680
you're using actually send metrics to CloudWatch.

46
00:02:13.680 --> 00:02:16.560
So if you haven't deployed the elastic load balancer service

47
00:02:16.560 --> 00:02:19.140
you will not see metrics for that service.

48
00:02:19.140 --> 00:02:22.100
When working with CloudWatch you can view metrics and the metric

49
00:02:22.100 --> 00:02:25.640
details through the AWS console or through the AWS CLI.

50
00:02:25.640 --> 00:02:27.730
It's also possible to access the CloudWatch metrics

51
00:02:27.730 --> 00:02:30.580
programmatically using the CloudWatch APIs,

52
00:02:30.580 --> 00:02:34.170
as well as metrics published by the AWS services that you've deployed.

53
00:02:34.170 --> 00:02:36.640
We can also publish custom metrics to CloudWatch.

54
00:02:36.640 --> 00:02:41.800
We can use either the AWS CLI or the CloudWatch APIs to publish custom metrics.

55
00:02:41.800 --> 00:02:45.230
When we do publish our custom metrics we get to choose a resolution,

56
00:02:45.230 --> 00:02:48.720
either standard that can publish metrics at one‑minute intervals or

57
00:02:48.720 --> 00:02:52.040
high resolution that can publish metrics every second.

58
00:02:52.040 --> 00:02:56.760
To add custom metrics from the AWS CLI use cloudwatch

59
00:02:56.760 --> 00:03:01.210
put‑metric‑data to place a custom metric into CloudWatch.

60
00:03:01.210 --> 00:03:03.940
When you publish custom metrics in CloudWatch it's

61
00:03:03.940 --> 00:03:05.960
recommended that you set dimensions.

62
00:03:05.960 --> 00:03:08.740
Dimensions give us more insight into the metrics,

63
00:03:08.740 --> 00:03:12.240
what the metrics contain and how the metrics can be used.

64
00:03:12.240 --> 00:03:17.240
To view your custom metrics on the CLI use cloudwatch get‑metric‑statistics.

65
00:03:17.240 --> 00:03:19.900
Now you'll be able to see your custom metrics and how they're configured.

66
00:03:19.900 --> 00:03:23.070
We'll take a look at different types of metrics during our demonstration.

67
00:03:23.070 --> 00:03:26.420
Another really powerful component of CloudWatch is CloudWatch alarms.

68
00:03:26.420 --> 00:03:28.910
Unlike metrics that are enabled by default in AWS,

69
00:03:28.910 --> 00:03:31.240
there are no default CloudWatch alarms.

70
00:03:31.240 --> 00:03:34.770
But we can create CloudWatch alarms to monitor for changes in

71
00:03:34.770 --> 00:03:37.540
any of the metrics registered with CloudWatch.

72
00:03:37.540 --> 00:03:40.010
When we create an alarm we can integrate that alarm

73
00:03:40.010 --> 00:03:42.840
with the Simple Notification Service, SNS,

74
00:03:42.840 --> 00:03:45.640
we can integrate that alarm with auto scale,

75
00:03:45.640 --> 00:03:49.570
or, depending on the metric, we can also integrate with EC2 actions.

76
00:03:49.570 --> 00:03:53.040
So if we create an alarm that's monitored in an EC2 metric and the alarm goes

77
00:03:53.040 --> 00:03:56.530
into the alarm state we can have an action that shuts down,

78
00:03:56.530 --> 00:03:58.940
reboots, or terminates an EC2 instance.

79
00:03:58.940 --> 00:04:01.060
The integrate with auto scale means that based on a

80
00:04:01.060 --> 00:04:04.040
metric we can trigger an auto scale event,

81
00:04:04.040 --> 00:04:06.440
so you either scale out or scale in.

82
00:04:06.440 --> 00:04:09.230
And by integrating with the Simple Notification Service,

83
00:04:09.230 --> 00:04:11.850
which is our published subscriber service in AWS,

84
00:04:11.850 --> 00:04:14.950
an alarm can send a message to an SNS topic which in

85
00:04:14.950 --> 00:04:19.440
turn can integrate with email, lambda functions,

86
00:04:19.440 --> 00:04:23.240
and a whole heap of other AWS and non‑AWS services.

87
00:04:23.240 --> 00:04:27.840
So an alarm can be one of the starting off points for automation inside AWS.

88
00:04:27.840 --> 00:04:32.370
When working with alarms we can configure metric alarms or composite alarms.

89
00:04:32.370 --> 00:04:34.740
Metric alarms work with a single metric.

90
00:04:34.740 --> 00:04:36.590
So let's say CPU utilization.

91
00:04:36.590 --> 00:04:40.440
If CPU goes up, generate an alarm.

92
00:04:40.440 --> 00:04:44.840
Composite metrics work with several metric alarms.

93
00:04:44.840 --> 00:04:49.420
So let's say you've got metric alarms alerting you to issues

94
00:04:49.420 --> 00:04:51.440
with different parts of your application,

95
00:04:51.440 --> 00:04:53.740
maybe metric arms that are monitoring your web tier,

96
00:04:53.740 --> 00:04:56.440
your application tier, your database tier.

97
00:04:56.440 --> 00:05:00.680
We can then create a composite alarm that only kicks in if

98
00:05:00.680 --> 00:05:03.240
several other alarms are breached first.

99
00:05:03.240 --> 00:05:06.540
This way different teams and events can be notified

100
00:05:06.540 --> 00:05:08.740
when specific alarms are triggered.

101
00:05:08.740 --> 00:05:11.370
But if multiple alarms are triggered that can trigger a

102
00:05:11.370 --> 00:05:15.440
composite alarm that escalates the problem to a different team

103
00:05:15.440 --> 00:05:17.540
or instigates different work flows.

104
00:05:17.540 --> 00:05:17.790
Now,

105
00:05:17.790 --> 00:05:20.020
when we configure an alarm and it goes into alarm

106
00:05:20.020 --> 00:05:22.560
state it triggers its actions once.

107
00:05:22.560 --> 00:05:24.450
The exception is auto scale.

108
00:05:24.450 --> 00:05:26.730
When we configure an alarm that triggers an auto scale event

109
00:05:26.730 --> 00:05:30.110
that can keep triggering the auto scale until the maximum size

110
00:05:30.110 --> 00:05:34.130
of our auto scale group is reached or, since the metric's changed,

111
00:05:34.130 --> 00:05:37.540
until the alarm is no longer in the alarm state.

112
00:05:37.540 --> 00:05:41.240
When working with alarms you'll find that alarms have one of three states,

113
00:05:41.240 --> 00:05:42.530
either OK,

114
00:05:42.530 --> 00:05:44.740
this means the metric you are monitoring is below the

115
00:05:44.740 --> 00:05:47.060
threshold you've set to monitor, ALARM,

116
00:05:47.060 --> 00:05:50.280
that means the metric you are monitoring is above the threshold

117
00:05:50.280 --> 00:05:52.840
that you configured and action should be taken,

118
00:05:52.840 --> 00:05:54.450
or INSUFFICIENT_DATA.

119
00:05:54.450 --> 00:05:58.340
So if the alarm was just started, the metric's no longer available,

120
00:05:58.340 --> 00:06:01.330
or if it's been running but it hasn't gathered enough information yet to make

121
00:06:01.330 --> 00:06:05.340
a decision then the alarm will show us INSUFFICIENT_DATA.

122
00:06:05.340 --> 00:06:07.330
This is Globomantics' application.

123
00:06:07.330 --> 00:06:09.140
This application consists of three tiers,

124
00:06:09.140 --> 00:06:12.040
a web tier, application, and database tier.

125
00:06:12.040 --> 00:06:16.440
So the database tier is running MySQL and using Multi‑AZ replication.

126
00:06:16.440 --> 00:06:18.690
The web tier and application tier machines are split

127
00:06:18.690 --> 00:06:20.940
across multiple availability zones.

128
00:06:20.940 --> 00:06:23.740
Now you can see they're both part of auto scale groups.

129
00:06:23.740 --> 00:06:25.940
Then we have a public‑facing load balancer.

130
00:06:25.940 --> 00:06:27.660
The load balancer requests the web tier,

131
00:06:27.660 --> 00:06:30.500
and the web tier and application tier communicate with each

132
00:06:30.500 --> 00:06:32.760
other with the use of standard SQS queues.

133
00:06:32.760 --> 00:06:35.340
Given this application,

134
00:06:35.340 --> 00:06:38.860
what would you suggest Globomantics monitor so the application can respond

135
00:06:38.860 --> 00:06:42.420
to changes in demand and react to availability issues?

136
00:06:42.420 --> 00:06:45.110
Take a minute, have a think, pause the video if you need,

137
00:06:45.110 --> 00:06:47.240
and see if you can come up with some ideas.

138
00:06:47.240 --> 00:06:50.240
So, one set of possible monitoring options,

139
00:06:50.240 --> 00:06:54.070
for the EC2 instances we could monitor things like CPU,

140
00:06:54.070 --> 00:06:56.940
disk metrics, SQS queue length.

141
00:06:56.940 --> 00:07:00.050
All of these can be used as part of alarms to monitor for when our

142
00:07:00.050 --> 00:07:03.340
web tier and application tier is under strain.

143
00:07:03.340 --> 00:07:06.240
Our alarms will then be integrated with auto scale,

144
00:07:06.240 --> 00:07:09.340
triggering scale‑out and scale‑in events.

145
00:07:09.340 --> 00:07:11.930
We might also want to monitor AWS health events.

146
00:07:11.930 --> 00:07:15.100
So individual services like the load balancer service in SQS

147
00:07:15.100 --> 00:07:17.740
generate events that we can monitor for in CloudWatch.

148
00:07:17.740 --> 00:07:21.980
Using CloudWatch Events or Amazon EventBridge we can be notified when

149
00:07:21.980 --> 00:07:25.780
there's performance issues with the ELB service or SQS service or

150
00:07:25.780 --> 00:07:27.810
indeed if the entire region's having issues.

151
00:07:27.810 --> 00:07:30.440
We can then use the information to automate responses.

152
00:07:30.440 --> 00:07:31.150
So for example,

153
00:07:31.150 --> 00:07:33.720
if the load balancer service and SQS service are having issues

154
00:07:33.720 --> 00:07:37.250
in our region we might decide to integrate with lambda that in

155
00:07:37.250 --> 00:07:40.520
turn can integrate with CloudFormation to launch our application

156
00:07:40.520 --> 00:07:41.570
in a different region,

157
00:07:41.570 --> 00:07:44.080
giving us another environment that we can fail over

158
00:07:44.080 --> 00:07:46.440
to if problems get more severe.

159
00:07:46.440 --> 00:07:50.110
We'll also want to monitor for RDS metrics and database events,

160
00:07:50.110 --> 00:07:53.740
building alarms that can indicate to us when our database is under strain

161
00:07:53.740 --> 00:07:57.340
or watching for key events from RDS which can give us information about

162
00:07:57.340 --> 00:08:01.740
service status that we can use to scale our service or again migrate our

163
00:08:01.740 --> 00:08:03.940
service to a different region.

164
00:08:03.940 --> 00:08:06.320
There's lots of information that we can monitor in CloudWatch

165
00:08:06.320 --> 00:08:09.240
for our deployed applications and services,

166
00:08:09.240 --> 00:08:11.940
but the key to success is automation,

167
00:08:11.940 --> 00:08:13.770
choosing the appropriate metrics to monitor,

168
00:08:13.770 --> 00:08:21.000
the right alarms to configure, the right events to watch for, and using those results to trigger remediation action.

