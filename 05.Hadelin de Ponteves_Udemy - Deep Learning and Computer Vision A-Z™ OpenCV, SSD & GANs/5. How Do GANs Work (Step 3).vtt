WEBVTT
1
00:00:00.390 --> 00:00:06.470
Crais final step that we're going to look at noise goes into the generator generator generates dogs

2
00:00:06.540 --> 00:00:13.090
with eyes this time and we can look more three dimensional I have ears for different.

3
00:00:13.100 --> 00:00:15.680
Too bad they have too many eyes.

4
00:00:15.680 --> 00:00:22.990
But some of them but already you know it's learned from the mistakes it made previously.

5
00:00:23.160 --> 00:00:28.560
And so now we want to train the discriminator and we're going to need another batch of dog images we're

6
00:00:28.560 --> 00:00:30.000
going to put them in.

7
00:00:30.000 --> 00:00:33.360
And there you go see what outputs you've got.

8
00:00:33.360 --> 00:00:44.180
We've got some valuable top to bottom and so what we can see now is that these values are slowly converging

9
00:00:44.180 --> 00:00:51.470
so the value of getting closer the values for the dogs or the dogs are getting bigger with time and

10
00:00:51.470 --> 00:00:57.550
it's harder it is becoming harder for the discriminator to discriminate between dogs and dogs.

11
00:00:57.550 --> 00:01:03.230
I also noticed that in the Paduan retraining the general and that's good that means these images are

12
00:01:03.230 --> 00:01:05.170
getting closer to what we want to.

13
00:01:05.210 --> 00:01:07.130
More realistic dogs.

14
00:01:07.130 --> 00:01:12.590
And again we're going to take these values Kellow can calculate the errors based on what we know they

15
00:01:12.590 --> 00:01:15.410
should be they should be zeros at the top ones at the bottom.

16
00:01:15.740 --> 00:01:20.300
And that error is going to be back propagated through the network of the discriminator.

17
00:01:20.320 --> 00:01:26.350
Weights are going to be updated and then then it's time to train the generators.

18
00:01:26.360 --> 00:01:28.930
Now we're going to train the generator.

19
00:01:28.940 --> 00:01:32.800
We're going to have to put these images into the network.

20
00:01:32.990 --> 00:01:36.500
And what happens next is we get an output.

21
00:01:36.500 --> 00:01:43.700
You can see this output is lower than it was just now but also as you can see it slowly.

22
00:01:43.760 --> 00:01:46.470
It's not as low as at the very start.

23
00:01:46.560 --> 00:01:55.100
Again this is a sign it's showing us that these images are actually getting closer to realistic dog

24
00:01:55.130 --> 00:01:56.050
images.

25
00:01:56.140 --> 00:02:01.500
Again that these valves are going to be we're going to get the values collate the error and get back

26
00:02:01.510 --> 00:02:08.260
Burkitt the error through the network of the generator and the weights there.

27
00:02:08.750 --> 00:02:10.970
So there we go that's how it works.

28
00:02:10.970 --> 00:02:16.550
These are just three steps in training in reality these like hundreds and thousands of these steps and

29
00:02:16.550 --> 00:02:19.670
then multiple airports as well.

30
00:02:19.670 --> 00:02:24.410
And as you can imagine through many many many many iterations these images are going to get better and

31
00:02:24.410 --> 00:02:31.790
better and better and better through this struggle through this confrontation between the generator

32
00:02:31.850 --> 00:02:34.100
and the discriminant.

33
00:02:34.340 --> 00:02:37.740
Additional information is definitely available.

34
00:02:37.760 --> 00:02:43.400
And one of the best place to go to is the original paper by in good fellow it's called generative adversarial

35
00:02:43.490 --> 00:02:48.420
Annetts 2014 paper which you can find archive.

36
00:02:48.530 --> 00:02:48.880
Yeah.

37
00:02:48.890 --> 00:02:51.110
And he explains everything there.

38
00:02:51.500 --> 00:02:54.570
And one more thing I wanted to mention here.

39
00:02:54.620 --> 00:02:58.850
When we were talking about the generator we just said it's a neural network.

40
00:02:58.850 --> 00:03:03.850
But this type of neural network we haven't discussed it is not even discussed in the annex.

41
00:03:03.950 --> 00:03:10.300
It's called a deconvolution or neural network in that case you'll find artificial neural networks and

42
00:03:10.300 --> 00:03:14.070
can we do illusional neural networks but not deconvolution all neural networks.

43
00:03:14.240 --> 00:03:20.090
So I just wanted to make a quick note here so there is this illusional neural network what a deconvolution

44
00:03:20.090 --> 00:03:27.350
all neural network is if you flip this upside down or run it back to front and then run it this way

45
00:03:28.460 --> 00:03:30.660
instead of the normal way you run the other way.

46
00:03:30.680 --> 00:03:35.870
Basically you start with the vector of values which is our Random signal.

47
00:03:35.880 --> 00:03:38.180
Then at the end you'll get the image.

48
00:03:38.420 --> 00:03:43.940
And if you'd like to learn more about decompositional neural networks there is some additional reading

49
00:03:43.940 --> 00:03:44.700
over here.

50
00:03:44.750 --> 00:03:52.730
We're not discussing it here because it's not the main concept of for us to focus on one focus on Ganns

51
00:03:52.760 --> 00:03:59.450
But if you'd like to learn more about D-Conn. finance then this is a paper which talks about them is

52
00:03:59.450 --> 00:04:06.260
called adaptive deconvolution networks for mid and high level feature learning by Mathieu's and others.

53
00:04:06.500 --> 00:04:14.780
So yeah that's a go to people are forged continents hopefully you enjoyed this tutorial and you now

54
00:04:14.780 --> 00:04:20.840
know how Ganns work in the background and then report seeing you back here next time.

55
00:04:20.850 --> 00:04:22.710
And until then enjoy computer vision.
