1
1

00:00:00,260  -->  00:00:01,237
<v ->Hello and welcome</v>
2

2

00:00:01,237  -->  00:00:05,430
to the final step of part two, building the RNN,
3

3

00:00:05,430  -->  00:00:09,240
about fitting the RNN to the training set, of course.
4

4

00:00:09,240  -->  00:00:13,240
Now, so far we have a highly robust wrecker renewal network,
5

5

00:00:13,240  -->  00:00:15,930
but we have not made the connection yet
6

6

00:00:15,930  -->  00:00:17,570
with our training set.
7

7

00:00:17,570  -->  00:00:19,760
So that's exactly the last thing we have to do.
8

8

00:00:19,760  -->  00:00:22,119
And besides, it is in this tutorial
9

9

00:00:22,119  -->  00:00:24,150
that the training will happen.
10

10

00:00:24,150  -->  00:00:27,960
Because once we execute this last line of code of part two,
11

11

00:00:27,960  -->  00:00:31,920
of course, well, the training will take place and in the end
12

12

00:00:31,920  -->  00:00:34,770
we will not only have a robust neural network,
13

13

00:00:34,770  -->  00:00:37,330
but mostly, we will have a smart one.
14

14

00:00:37,330  -->  00:00:40,460
That is an intelligent, trained, neural network,
15

15

00:00:40,460  -->  00:00:44,020
to be able to predict in some way, the upward trends,
16

16

00:00:44,020  -->  00:00:46,250
and downward trends of the Google set price.
17

17

00:00:46,250  -->  00:00:49,100
So it will be specifically intelligent.
18

18

00:00:49,100  -->  00:00:52,800
But there we go, it's the last step of this part two.
19

19

00:00:52,800  -->  00:00:54,610
And let's tackle this.
20

20

00:00:54,610  -->  00:00:57,070
So, you actually know how to do it,
21

21

00:00:57,070  -->  00:00:59,376
if you followed ANN and CNN,
22

22

00:00:59,376  -->  00:01:01,640
we need to take, of course this time,
23

23

00:01:01,640  -->  00:01:02,700
our regressor,
24

24

00:01:02,700  -->  00:01:04,410
and not our test fire.
25

25

00:01:04,410  -->  00:01:06,760
And then dot and then use of course
26

26

00:01:06,760  -->  00:01:09,890
the fit method, which will, not only
27

27

00:01:09,890  -->  00:01:13,410
connect our neural network to the training set
28

28

00:01:13,410  -->  00:01:16,470
and will also execute the training over
29

29

00:01:16,470  -->  00:01:18,040
a certain number of epochs
30

30

00:01:18,040  -->  00:01:20,780
that we will chose in the same fit method.
31

31

00:01:20,780  -->  00:01:23,520
So fit and now, we need to input
32

32

00:01:23,520  -->  00:01:26,340
four arguments, which are going to be
33

33

00:01:26,340  -->  00:01:29,400
first the input of the training set
34

34

00:01:29,400  -->  00:01:32,340
that features, or if you want the independent variables
35

35

00:01:32,340  -->  00:01:33,430
of the training set.
36

36

00:01:33,430  -->  00:01:35,760
Which will be the input of the neural network
37

37

00:01:35,760  -->  00:01:37,490
and will be forward propagated
38

38

00:01:37,490  -->  00:01:40,410
to the output, which will be the prediction
39

39

00:01:40,410  -->  00:01:44,170
and that will compare to the ground truth that is contained
40

40

00:01:44,170  -->  00:01:45,570
in Y train.
41

41

00:01:45,570  -->  00:01:47,200
And speaking of the ground truth,
42

42

00:01:47,200  -->  00:01:49,500
that is exactly the next argument we need to input
43

43

00:01:49,500  -->  00:01:50,600
after X train.
44

44

00:01:50,600  -->  00:01:51,750
That is Y train.
45

45

00:01:51,750  -->  00:01:54,390
So lets start inputting these first two arguments.
46

46

00:01:54,390  -->  00:01:57,490
First one is X train, then input.
47

47

00:01:57,490  -->  00:02:01,220
And the second one is Y train, the ground truth.
48

48

00:02:01,220  -->  00:02:04,700
Alright, and then we need to add two more arguments.
49

49

00:02:04,700  -->  00:02:06,960
Which are going to be the number of epochs.
50

50

00:02:06,960  -->  00:02:09,690
That is on how many iterations do you want your
51

51

00:02:09,690  -->  00:02:11,630
wrecker renewal network to be trained.
52

52

00:02:11,630  -->  00:02:14,440
Or if you want how many times do you want the whole
53

53

00:02:14,440  -->  00:02:17,590
data, the whole training data to be forward propagated
54

54

00:02:17,590  -->  00:02:18,660
inside the neural network,
55

55

00:02:18,660  -->  00:02:21,410
and then back propagated to update the wait.
56

56

00:02:21,410  -->  00:02:25,070
Well that number you choose it in the epochs parameter.
57

57

00:02:25,070  -->  00:02:26,977
Spelled this way epcohs.
58

58

00:02:28,060  -->  00:02:30,330
And the number were going to choose is going to be
59

59

00:02:30,330  -->  00:02:32,430
100. And again
60

60

00:02:32,430  -->  00:02:36,460
you can choose several numbers, but I experimented before
61

61

00:02:36,460  -->  00:02:39,740
and you know I tried with first 25 epochs
62

62

00:02:39,740  -->  00:02:42,180
I noticed that there wasn't a convergence of the loss.
63

63

00:02:42,180  -->  00:02:45,080
So I tried with more epochs, 50
64

64

00:02:45,080  -->  00:02:46,680
still not some convergence.
65

65

00:02:46,680  -->  00:02:50,000
Then I tried 100, and there I observed some convergence.
66

66

00:02:50,000  -->  00:02:53,110
So I think 100 is the right number of epochs.
67

67

00:02:53,110  -->  00:02:55,120
For this kind of problem, besides
68

68

00:02:55,120  -->  00:02:56,720
we don't have that much data.
69

69

00:02:56,720  -->  00:02:59,209
We only have the Google stock prize for 5 years
70

70

00:02:59,209  -->  00:03:01,740
and by the way if you want you can train
71

71

00:03:01,740  -->  00:03:05,100
this wrecker renewal network on even more than 5 years.
72

72

00:03:05,100  -->  00:03:08,033
The data is available online, it's the real
73

73

00:03:08,033  -->  00:03:10,310
Google stock prize that you can take from
74

74

00:03:10,310  -->  00:03:11,920
many financial sources.
75

75

00:03:11,920  -->  00:03:13,570
One of them can be for example,
76

76

00:03:13,570  -->  00:03:14,740
Yahoo finance.
77

77

00:03:14,740  -->  00:03:16,000
So feel free to play with it
78

78

00:03:16,000  -->  00:03:18,470
and experiment even more to create some
79

79

00:03:18,470  -->  00:03:21,720
maybe even more robust wrecker renewal networks.
80

80

00:03:21,720  -->  00:03:24,480
But anyway with 5 years of training data
81

81

00:03:24,480  -->  00:03:26,560
well 100 epochs turned out to be
82

82

00:03:26,560  -->  00:03:28,870
a good choice, with some convergence.
83

83

00:03:28,870  -->  00:03:31,610
So 100 epochs, and now we have
84

84

00:03:31,610  -->  00:03:34,230
one final argument to input.
85

85

00:03:34,230  -->  00:03:37,560
And actually this will be the final element we will add
86

86

00:03:37,560  -->  00:03:40,100
for this part two, building the RNN.
87

87

00:03:40,100  -->  00:03:41,920
And that is, the batch size.
88

88

00:03:41,920  -->  00:03:44,770
So our network is going to be trained.
89

89

00:03:44,770  -->  00:03:46,733
Not on single observations
90

90

00:03:46,733  -->  00:03:48,286
going into the neural network.
91

91

00:03:48,286  -->  00:03:52,787
It's going to be trained on batches of observations
92

92

00:03:52,787  -->  00:03:54,300
that is batches of stock prices
93

93

00:03:54,300  -->  00:03:55,940
going into the neural network.
94

94

00:03:55,940  -->  00:03:57,550
So instead of updating the wait,
95

95

00:03:57,550  -->  00:04:01,060
every stock price being forward propagated into
96

96

00:04:01,060  -->  00:04:03,250
the neural network, and then generating an error
97

97

00:04:03,250  -->  00:04:05,630
that is back propagated into the neural network.
98

98

00:04:05,630  -->  00:04:09,460
We will do that, every 32 stock prices
99

99

00:04:09,460  -->  00:04:11,250
because as you might just guess
100

100

00:04:11,250  -->  00:04:14,287
we're going to choose a batch size of 32.
101

101

00:04:14,287  -->  00:04:16,770
There we go, that's our last argument.
102

102

00:04:16,770  -->  00:04:17,800
Batch
103

103

00:04:17,800  -->  00:04:19,660
underscore size
104

104

00:04:19,660  -->  00:04:21,351
equals 32
105

105

00:04:21,351  -->  00:04:24,600
perfect, and now, you made it.
106

106

00:04:24,600  -->  00:04:28,180
Not only you build a super robust wrecker renewal network
107

107

00:04:28,180  -->  00:04:31,870
but also you're ready to train it on 5 years
108

108

00:04:31,870  -->  00:04:33,420
of the Google stock price.
109

109

00:04:33,420  -->  00:04:36,120
So let's do this, I'm going to
110

110

00:04:36,120  -->  00:04:40,750
select this line, and let's make our wrecker renewal network
111

111

00:04:40,750  -->  00:04:43,053
the smartest stock price predictor.
112

112

00:04:44,210  -->  00:04:46,357
There we go, the training is launched.
113

113

00:04:46,357  -->  00:04:49,330
It is on it's way, so
114

114

00:04:49,330  -->  00:04:50,270
and there we go.
115

115

00:04:50,270  -->  00:04:52,190
Starting with the first epoch.
116

116

00:04:52,190  -->  00:04:55,700
Epoch of 1 out of 100
117

117

00:04:55,700  -->  00:04:58,830
and what we can see there are the different batches
118

118

00:04:58,830  -->  00:05:00,330
going into the neural network.
119

119

00:05:00,330  -->  00:05:02,440
Generating some errors and then the errors
120

120

00:05:02,440  -->  00:05:04,810
back propagated into the neural network.
121

121

00:05:04,810  -->  00:05:08,341
And that's every 32 stock prices.
122

122

00:05:08,341  -->  00:05:10,970
And we know because we choose a batch size of 32.
123

123

00:05:10,970  -->  00:05:13,050
So that's why you see in the epoch,
124

124

00:05:13,050  -->  00:05:16,620
these numbers of the batches out of 1198.
125

125

00:05:16,620  -->  00:05:17,980
That's exactly what you see
126

126

00:05:17,980  -->  00:05:20,030
and on the right you can observe the loss.
127

127

00:05:20,030  -->  00:05:23,400
That is going to be reduced at each iteration.
128

128

00:05:23,400  -->  00:05:25,700
Hopefully, well that is what I observed.
129

129

00:05:25,700  -->  00:05:27,860
And in the end you should be able to
130

130

00:05:27,860  -->  00:05:29,050
observe some convergence.
131

131

00:05:29,050  -->  00:05:31,892
That is in the last 20 or 10 epochs
132

132

00:05:31,892  -->  00:05:34,930
the loss shouldn't be decreasing much.
133

133

00:05:34,930  -->  00:05:35,763
So there we go,
134

134

00:05:35,763  -->  00:05:38,080
the training is happening.
135

135

00:05:38,080  -->  00:05:39,910
And now as you can see it's going to take
136

136

00:05:39,910  -->  00:05:42,860
a while it's actually going to take around 20 minutes
137

137

00:05:42,860  -->  00:05:43,800
on my computer.
138

138

00:05:43,800  -->  00:05:46,430
So what I'm going to do is have a little
139

139

00:05:46,430  -->  00:05:48,550
break here, I'm going to accelerate the video.
140

140

00:05:48,550  -->  00:05:50,690
And I'll see you at the end of the training.
141

141

00:05:50,690  -->  00:05:51,773
[Piano Music]
142

142

00:07:13,803  -->  00:07:16,280
<v ->Alright we have just reached the final epoch</v>
143

143

00:07:16,280  -->  00:07:18,240
out of the 100 epochs in total.
144

144

00:07:18,240  -->  00:07:20,120
I just had time to make a quick nap.
145

145

00:07:20,120  -->  00:07:23,400
It's perfect, and now the training is about to end.
146

146

00:07:23,400  -->  00:07:24,233
There we go,
147

147

00:07:24,233  -->  00:07:26,090
the training is over.
148

148

00:07:26,090  -->  00:07:29,950
So let's have a look at the final loses and see
149

149

00:07:29,950  -->  00:07:31,830
if I was right about the convergence.
150

150

00:07:31,830  -->  00:07:34,500
So we started with a loss of 5%,
151

151

00:07:34,500  -->  00:07:36,740
0.0531.
152

152

00:07:36,740  -->  00:07:39,220
Then of course after the first update of the wait
153

153

00:07:39,220  -->  00:07:41,510
to improve the predictive power
154

154

00:07:41,510  -->  00:07:44,110
well the loss immediately reduced.
155

155

00:07:44,110  -->  00:07:45,120
So that's normal.
156

156

00:07:45,120  -->  00:07:46,700
And then, epoch by epoch
157

157

00:07:46,700  -->  00:07:49,090
it was reduced more and more.
158

158

00:07:49,090  -->  00:07:51,370
So here we go, from 47 to 43
159

159

00:07:51,370  -->  00:07:53,520
from 43 to 41
160

160

00:07:53,520  -->  00:07:55,870
then it went back up to 46.
161

161

00:07:55,870  -->  00:07:59,150
Then went back down to 41, then 38.
162

162

00:07:59,150  -->  00:08:02,490
46 again, then 37, 36.
163

163

00:08:02,490  -->  00:08:05,610
And progressively, like every 10 epoch,
164

164

00:08:05,610  -->  00:08:08,130
we clearly see the loss decreasing.
165

165

00:08:08,130  -->  00:08:10,780
And lets see when we were halfway there
166

166

00:08:10,780  -->  00:08:13,190
at epoch 50 out of 100
167

167

00:08:13,190  -->  00:08:15,680
we reached 0.0022
168

168

00:08:15,680  -->  00:08:18,573
and then it decreased even more 0.0021
169

169

00:08:19,790  -->  00:08:21,340
0.0019,
170

170

00:08:21,340  -->  00:08:22,173
0.0018,
171

171

00:08:23,607  -->  00:08:24,440
0.0017
172

172

00:08:25,700  -->  00:08:27,617
18 again, 15
173

173

00:08:27,617  -->  00:08:29,630
16, 17, 15.
174

174

00:08:29,630  -->  00:08:31,840
And as you can see, in the end
175

175

00:08:31,840  -->  00:08:33,900
in the last 20 epochs,
176

176

00:08:33,900  -->  00:08:36,420
well it's staying around the same level
177

177

00:08:36,420  -->  00:08:39,440
that is around 0.0015.
178

178

00:08:39,440  -->  00:08:41,090
So if you want you could try to
179

179

00:08:41,090  -->  00:08:43,460
do it with more epochs, I didn't try it.
180

180

00:08:43,460  -->  00:08:45,580
I didn't try with 200 epochs,
181

181

00:08:45,580  -->  00:08:47,740
but I'm sure you wouldn't get a loss
182

182

00:08:47,740  -->  00:08:50,740
that with the drop out regularization were added.
183

183

00:08:50,740  -->  00:08:53,430
Well we have prevented over fitting enough
184

184

00:08:53,430  -->  00:08:55,900
to not decrease this loss even more.
185

185

00:08:55,900  -->  00:08:59,030
You know if you obtain a loss too small in the end
186

186

00:08:59,030  -->  00:09:00,760
well you might get some over fitting.
187

187

00:09:00,760  -->  00:09:02,530
You're predictions will be close
188

188

00:09:02,530  -->  00:09:04,520
to the real Google stock price.
189

189

00:09:04,520  -->  00:09:06,130
But in the training set
190

190

00:09:06,130  -->  00:09:08,240
in the training data, which I remind
191

191

00:09:08,240  -->  00:09:09,570
is the data of the past.
192

192

00:09:09,570  -->  00:09:11,750
So not the data we're interested in
193

193

00:09:11,750  -->  00:09:12,970
to make predictions.
194

194

00:09:12,970  -->  00:09:14,760
So you might get some great loss
195

195

00:09:14,760  -->  00:09:16,350
on the training data.
196

196

00:09:16,350  -->  00:09:18,010
But then some really bad loss
197

197

00:09:18,010  -->  00:09:19,170
on the test data.
198

198

00:09:19,170  -->  00:09:21,750
And that's exactly what over fitting is about.
199

199

00:09:21,750  -->  00:09:24,470
So that's why when you do the training
200

200

00:09:24,470  -->  00:09:26,920
on the training set, well you must be careful
201

201

00:09:26,920  -->  00:09:28,440
not to obtain over fitting
202

202

00:09:28,440  -->  00:09:30,241
and therefore not to try to decrease
203

203

00:09:30,241  -->  00:09:32,730
the loss as much as possible.
204

204

00:09:32,730  -->  00:09:34,710
And that's why here it seems that we get
205

205

00:09:34,710  -->  00:09:36,060
really good results.
206

206

00:09:36,060  -->  00:09:38,240
But anyway we're going to observe
207

207

00:09:38,240  -->  00:09:40,580
the predictions then in part 3.
208

208

00:09:40,580  -->  00:09:43,390
Making the Predictions and Visualizing the Results.
209

209

00:09:43,390  -->  00:09:45,230
So we'll clearly see, in the end
210

210

00:09:45,230  -->  00:09:47,280
we will clearly have a chart,
211

211

00:09:47,280  -->  00:09:49,203
Showing the graph of the prediction.
212

212

00:09:49,203  -->  00:09:51,750
That is, the predicted Google stock price.
213

213

00:09:51,750  -->  00:09:54,770
And on the same chart, the real Google stock price.
214

214

00:09:54,770  -->  00:09:57,134
So we will see how it manages to follow the trends
215

215

00:09:57,134  -->  00:09:59,200
of the Google stock price.
216

216

00:09:59,200  -->  00:10:00,033
We will see that in the end,
217

217

00:10:00,033  -->  00:10:02,448
it will be very exciting to visualize.
218

218

00:10:02,448  -->  00:10:05,200
And now, congratulations
219

219

00:10:05,200  -->  00:10:07,210
you are now done with part 2.
220

220

00:10:07,210  -->  00:10:08,350
And you have just reached
221

221

00:10:08,350  -->  00:10:11,565
part 3, Making the Predictions and Visualizing the Results.
222

222

00:10:11,565  -->  00:10:16,060
So we are just a few tutorials left from visualizing
223

223

00:10:16,060  -->  00:10:18,100
these final results, and conclude
224

224

00:10:18,100  -->  00:10:19,156
on the power of RRNN.
225

225

00:10:19,156  -->  00:10:23,400
So I can't wait to start this part 3 with you.
226

226

00:10:23,400  -->  00:10:25,100
And of course I can't wait to reach
227

227

00:10:25,100  -->  00:10:26,320
the very exciting moment
228

228

00:10:26,320  -->  00:10:27,760
of visualizing the results.
229

229

00:10:27,760  -->  00:10:29,994
We will get to is very fast, and until then.
230

230

00:10:29,994  -->  00:10:31,153
Enjoy deep learning.
