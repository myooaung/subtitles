WEBVTT
1
1

00:00:00.200  -->  00:00:02.720
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.720  -->  00:00:05.250
Alright, so we are done adding the layers of our
3

3

00:00:05.250  -->  00:00:08.250
artificial neural network, and now what we're gonna do
4

4

00:00:08.250  -->  00:00:11.110
is to compile the whole artificial neural network,
5

5

00:00:11.110  -->  00:00:14.280
that is basically applying stochastic gradient descent
6

6

00:00:14.280  -->  00:00:16.630
on the whole artificial neural network.
7

7

00:00:16.630  -->  00:00:18.510
That's what compiling means, and that's
8

8

00:00:18.510  -->  00:00:20.890
what we're gonna do with this single
9

9

00:00:20.890  -->  00:00:22.770
line of code that we're about to add.
10

10

00:00:22.770  -->  00:00:24.810
Alright, so let's add it, we are gonna start
11

11

00:00:24.810  -->  00:00:27.550
by taking our classifier object,
12

12

00:00:27.550  -->  00:00:29.870
as we did when we added the different layers
13

13

00:00:29.870  -->  00:00:32.310
in the a and n, and now we're gonna use
14

14

00:00:32.310  -->  00:00:34.300
another method, exactly as we did here
15

15

00:00:34.300  -->  00:00:36.220
with the add method, but this time
16

16

00:00:36.220  -->  00:00:37.940
it's not gonna be the add method,
17

17

00:00:37.940  -->  00:00:39.500
you can guess what it's gonna be,
18

18

00:00:39.500  -->  00:00:42.170
it's going to be the compile method.
19

19

00:00:42.170  -->  00:00:43.620
As simple as that.
20

20

00:00:43.620  -->  00:00:45.470
So we're gonna add a dot here,
21

21

00:00:45.470  -->  00:00:48.080
and then compile, which is right here,
22

22

00:00:48.080  -->  00:00:50.280
pressing enter, alright.
23

23

00:00:50.280  -->  00:00:52.270
And now of course this compile method
24

24

00:00:52.270  -->  00:00:55.350
contains several parameters, so let's add
25

25

00:00:55.350  -->  00:00:57.240
some parentheses here, and so now
26

26

00:00:57.240  -->  00:00:58.310
I'm going to explain to you
27

27

00:00:58.310  -->  00:00:59.960
what those parameters are,
28

28

00:00:59.960  -->  00:01:01.860
and we'll add them one by one.
29

29

00:01:01.860  -->  00:01:03.980
Alright, so let's check out the arguments
30

30

00:01:03.980  -->  00:01:07.090
by pressing here, command i,
31

31

00:01:07.090  -->  00:01:08.650
and here we go, we have some info
32

32

00:01:08.650  -->  00:01:10.700
about this compile method.
33

33

00:01:10.700  -->  00:01:12.670
And the informations we are interested in
34

34

00:01:12.670  -->  00:01:14.350
are of course the arguments.
35

35

00:01:14.350  -->  00:01:17.320
Alright, so let's look at the first argument.
36

36

00:01:17.320  -->  00:01:19.860
The first argument is optimizer.
37

37

00:01:19.860  -->  00:01:22.770
So what is optimizer, optimizer is simply
38

38

00:01:22.770  -->  00:01:25.030
the algorithm you wanna use to find
39

39

00:01:25.030  -->  00:01:28.230
the optimal set of weights in the neural networks.
40

40

00:01:28.230  -->  00:01:30.640
Because you know we defined our neural networks,
41

41

00:01:30.640  -->  00:01:32.490
it is built with the different layers,
42

42

00:01:32.490  -->  00:01:36.260
but the weights are still only initialized.
43

43

00:01:36.260  -->  00:01:39.220
So now we have to apply some sort of algorithm
44

44

00:01:39.220  -->  00:01:42.840
to find the best weights that will make
45

45

00:01:42.840  -->  00:01:45.050
our neural network the most powerful.
46

46

00:01:45.050  -->  00:01:46.713
And so that's why in this compile method here
47

47

00:01:46.713  -->  00:01:49.140
we need to add an algorithm,
48

48

00:01:49.140  -->  00:01:51.490
and this algorithm is going to be nothing else
49

49

00:01:51.490  -->  00:01:54.140
than the stochastic gradient descent algorithm,
50

50

00:01:54.140  -->  00:01:57.300
and there are several types of stochastic gradient descent
51

51

00:01:57.300  -->  00:01:59.530
algorithm, and a very efficient one
52

52

00:01:59.530  -->  00:02:01.830
is called Adam, and that's exactly what's
53

53

00:02:01.830  -->  00:02:05.380
going to be the input of this optimizer parameter.
54

54

00:02:05.380  -->  00:02:07.230
Alright, so let's add it right now,
55

55

00:02:07.230  -->  00:02:10.970
optimizer equals, and so now we need to
56

56

00:02:10.970  -->  00:02:13.360
add in quotes, Adam.
57

57

00:02:13.360  -->  00:02:14.650
Here we go.
58

58

00:02:14.650  -->  00:02:16.660
Great, then comma, and now let's
59

59

00:02:16.660  -->  00:02:18.060
add the second parameter.
60

60

00:02:18.060  -->  00:02:20.070
So the second parameter is loss,
61

61

00:02:20.070  -->  00:02:22.410
and this corresponds to the loss function
62

62

00:02:22.410  -->  00:02:25.840
within the stochastic gradient descent algorithm,
63

63

00:02:25.840  -->  00:02:27.870
that is within the Adam algorithm.
64

64

00:02:27.870  -->  00:02:30.090
Because if you go deeper into the mathematical
65

65

00:02:30.090  -->  00:02:32.520
details of stochastic gradient descent,
66

66

00:02:32.520  -->  00:02:35.330
you will see that it is based on a loss function
67

67

00:02:35.330  -->  00:02:37.140
that you need to optimize to find
68

68

00:02:37.140  -->  00:02:39.220
the optimal weights, and so that's what
69

69

00:02:39.220  -->  00:02:40.540
we have to specify right now,
70

70

00:02:40.540  -->  00:02:42.460
it's this loss function,
71

71

00:02:42.460  -->  00:02:45.000
and so you know I can give you some examples,
72

72

00:02:45.000  -->  00:02:46.620
you saw the loss function when you studied
73

73

00:02:46.620  -->  00:02:50.290
linear regression, remember in Carol's intuition tutorials,
74

74

00:02:50.290  -->  00:02:53.180
the loss function was the sum of the squared errors,
75

75

00:02:53.180  -->  00:02:55.390
that is the sum of the squared differences
76

76

00:02:55.390  -->  00:02:57.870
between the real value and the predicted value.
77

77

00:02:57.870  -->  00:02:59.050
Well that was the loss function
78

78

00:02:59.050  -->  00:03:01.490
that we used to optimize the parameters
79

79

00:03:01.490  -->  00:03:03.700
of the regression model, and here that's
80

80

00:03:03.700  -->  00:03:05.860
exactly the same, we have some parameters,
81

81

00:03:05.860  -->  00:03:07.790
those parameters are the weights in their
82

82

00:03:07.790  -->  00:03:09.890
neural network, and so we need to specify
83

83

00:03:09.890  -->  00:03:11.550
a loss function that we will then
84

84

00:03:11.550  -->  00:03:13.690
optimize through stochastic gradient descent
85

85

00:03:13.690  -->  00:03:15.930
to eventually find the optimal weights.
86

86

00:03:15.930  -->  00:03:17.870
And so what is going to be the loss function
87

87

00:03:17.870  -->  00:03:19.550
for the neural networks?
88

88

00:03:19.550  -->  00:03:21.180
Well again, if you wanna go deeper
89

89

00:03:21.180  -->  00:03:23.000
into the mathematical details of
90

90

00:03:23.000  -->  00:03:25.520
stochastic gradient descent, well this loss function
91

91

00:03:25.520  -->  00:03:26.880
is going to be kind of the same
92

92

00:03:26.880  -->  00:03:28.750
as for logistic regression,
93

93

00:03:28.750  -->  00:03:30.840
because when you take a simple neural network
94

94

00:03:30.840  -->  00:03:32.910
with only one neuron, which makes it
95

95

00:03:32.910  -->  00:03:34.980
a perceptron model, and if you use
96

96

00:03:34.980  -->  00:03:38.200
a sigmoid activation function for this perceptron,
97

97

00:03:38.200  -->  00:03:41.030
well you obtain a logistic regression model.
98

98

00:03:41.030  -->  00:03:43.060
And then if you go deeper into the mathematical
99

99

00:03:43.060  -->  00:03:45.890
details of stochastic gradient descent
100

100

00:03:45.890  -->  00:03:47.750
for the logistic regression model,
101

101

00:03:47.750  -->  00:03:49.990
well you will find out that the loss function
102

102

00:03:49.990  -->  00:03:51.900
is not the sum of squared errors
103

103

00:03:51.900  -->  00:03:53.390
like for linear regression,
104

104

00:03:53.390  -->  00:03:55.640
but is going to be a logarithmic function
105

105

00:03:55.640  -->  00:03:57.573
that is called the logarithmic loss.
106

106

00:04:00.060  -->  00:04:01.790
And so since the activation function
107

107

00:04:01.790  -->  00:04:03.880
for our output layer is nothing else
108

108

00:04:03.880  -->  00:04:05.240
than the sigmoid function,
109

109

00:04:05.240  -->  00:04:07.210
well the loss function that we're gonna use here
110

110

00:04:07.210  -->  00:04:09.890
to compile our a and n and on which
111

111

00:04:09.890  -->  00:04:12.450
our stochastic gradient descent Adam algorithm
112

112

00:04:12.450  -->  00:04:14.720
is based on, well the loss function used
113

113

00:04:14.720  -->  00:04:16.420
to compile our a and n here,
114

114

00:04:16.420  -->  00:04:19.210
is going to be the logarithmic loss as well.
115

115

00:04:19.210  -->  00:04:20.820
And what is the name of this logarithmic
116

116

00:04:20.820  -->  00:04:22.040
loss function here?
117

117

00:04:22.040  -->  00:04:23.450
Well, if your dependent variable
118

118

00:04:23.450  -->  00:04:26.020
has a binary outcome, then this logarithmic
119

119

00:04:26.020  -->  00:04:28.640
loss function is called binary underscore
120

120

00:04:28.640  -->  00:04:31.050
cross entropy, and if your dependent variable
121

121

00:04:31.050  -->  00:04:33.960
has more than two outcomes, like three categories,
122

122

00:04:33.960  -->  00:04:36.320
then this logarithmic loss function is called
123

123

00:04:36.320  -->  00:04:39.230
categorical underscore cross entropy.
124

124

00:04:39.230  -->  00:04:40.390
Alright, so that's good to know,
125

125

00:04:40.390  -->  00:04:41.800
and now we know what to use,
126

126

00:04:41.800  -->  00:04:43.830
what to input here for the loss function,
127

127

00:04:43.830  -->  00:04:45.640
in case you're working with a dependent variable
128

128

00:04:45.640  -->  00:04:47.990
that has more than two categories.
129

129

00:04:47.990  -->  00:04:49.750
But here we have a binary outcome,
130

130

00:04:49.750  -->  00:04:53.110
so we'll input here for the loss parameter,
131

131

00:04:53.110  -->  00:04:58.110
in quotes, binary underscore cross entropy.
132

132

00:04:58.320  -->  00:04:59.860
Alright, and then quotes.
133

133

00:04:59.860  -->  00:05:02.970
Great, perfect, and now we need to add
134

134

00:05:02.970  -->  00:05:05.250
a third and final argument.
135

135

00:05:05.250  -->  00:05:07.770
It's this metrics argument here.
136

136

00:05:07.770  -->  00:05:10.030
So, what is this metrics argument?
137

137

00:05:10.030  -->  00:05:11.590
Well that's very simple, that is just
138

138

00:05:11.590  -->  00:05:14.450
a criterion that you choose to evaluate your model,
139

139

00:05:14.450  -->  00:05:17.280
and typically we use the accuracy criterion
140

140

00:05:17.280  -->  00:05:19.610
that we already saw many times in this course.
141

141

00:05:19.610  -->  00:05:21.350
So basically what happens is that,
142

142

00:05:21.350  -->  00:05:23.040
when the weights are updated after each
143

143

00:05:23.040  -->  00:05:26.140
observation or after each batch of many observations,
144

144

00:05:26.140  -->  00:05:28.970
the algorithm uses this accuracy criterion
145

145

00:05:28.970  -->  00:05:31.290
to improve the model's performance.
146

146

00:05:31.290  -->  00:05:33.200
You're gonna see that, when we fit
147

147

00:05:33.200  -->  00:05:35.250
the a and n to our training set,
148

148

00:05:35.250  -->  00:05:37.570
well the accuracy is going to increase
149

149

00:05:37.570  -->  00:05:40.960
little by little, until reaching a top accuracy.
150

150

00:05:40.960  -->  00:05:42.980
And that will happen because we choose here
151

151

00:05:42.980  -->  00:05:44.730
the accuracy metric.
152

152

00:05:44.730  -->  00:05:46.860
Alright, so let's add this argument here,
153

153

00:05:46.860  -->  00:05:50.710
metrics equals, and since actually this metrics
154

154

00:05:50.710  -->  00:05:53.430
argument is expecting a list of metrics,
155

155

00:05:53.430  -->  00:05:55.080
but here we only use one metric,
156

156

00:05:55.080  -->  00:05:56.670
which is the accuracy metric,
157

157

00:05:56.670  -->  00:06:00.560
we need to add this accuracy metric in brackets,
158

158

00:06:00.560  -->  00:06:02.750
because by adding these brackets like that,
159

159

00:06:02.750  -->  00:06:04.240
we are creating a list.
160

160

00:06:04.240  -->  00:06:06.740
But this list will only contain one element,
161

161

00:06:06.740  -->  00:06:10.330
which is the accuracy metric.
162

162

00:06:10.330  -->  00:06:12.600
Alright, perfect, and now we're ready
163

163

00:06:12.600  -->  00:06:14.800
to compile the a and n.
164

164

00:06:14.800  -->  00:06:17.310
So let's do it, we will select this line
165

165

00:06:17.310  -->  00:06:21.460
and execute, alright, perfectly well compiled.
166

166

00:06:21.460  -->  00:06:23.780
Perfect, so that step is done,
167

167

00:06:23.780  -->  00:06:26.130
and actually the next step is the most
168

168

00:06:26.130  -->  00:06:28.920
exciting step of this whole section,
169

169

00:06:28.920  -->  00:06:30.290
because this is in this step
170

170

00:06:30.290  -->  00:06:32.440
that we are gonna see the algorithm in action.
171

171

00:06:32.440  -->  00:06:34.610
We are going to choose a number of epochs,
172

172

00:06:34.610  -->  00:06:36.050
that as you know the number of times
173

173

00:06:36.050  -->  00:06:37.730
we are training our a and n on the whole
174

174

00:06:37.730  -->  00:06:39.810
training set, and so we're going to see the
175

175

00:06:39.810  -->  00:06:41.330
stochastic gradient descent in action,
176

176

00:06:41.330  -->  00:06:43.820
that is we're gonna see how our a and n model
177

177

00:06:43.820  -->  00:06:47.050
is trained, and how it is improving its accuracy
178

178

00:06:47.050  -->  00:06:49.500
at each round, that is at each epoch.
179

179

00:06:49.500  -->  00:06:51.130
So that's gonna be very exciting,
180

180

00:06:51.130  -->  00:06:53.870
and that will be the show of the next tutorial.
181

181

00:06:53.870  -->  00:06:55.553
Until then, enjoy deep learning.
