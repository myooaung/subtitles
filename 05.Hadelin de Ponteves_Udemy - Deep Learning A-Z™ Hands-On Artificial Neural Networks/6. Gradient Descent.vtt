WEBVTT
1
1

00:00:00.580  -->  00:00:01.820
<v Narrator>Hello and welcome back to the course</v>
2

2

00:00:01.820  -->  00:00:03.490
on Deep Learning.
3

3

00:00:03.490  -->  00:00:06.780
In today's tutorial we're talking about gradient descent.
4

4

00:00:06.780  -->  00:00:09.360
What we learnt previously was that
5

5

00:00:09.360  -->  00:00:12.150
in order for a neural network to learn
6

6

00:00:12.150  -->  00:00:14.260
what needs to happen is back propagation,
7

7

00:00:14.260  -->  00:00:17.880
and that is when the error, the difference,
8

8

00:00:17.880  -->  00:00:22.135
or the sum of square differences between 天 and y
9

9

00:00:22.135  -->  00:00:25.730
is back propagated through the neural network
10

10

00:00:25.730  -->  00:00:28.400
and the weights are adjusted accordingly.
11

11

00:00:28.400  -->  00:00:31.210
So we saw that and today we're going to learn
12

12

00:00:31.210  -->  00:00:34.300
exactly how these weights are adjusted.
13

13

00:00:34.300  -->  00:00:36.010
So let's have a look.
14

14

00:00:36.010  -->  00:00:41.010
This is our very simple version of neural network.
15

15

00:00:41.030  -->  00:00:43.690
A perceptron or a single layer of feed forward
16

16

00:00:43.690  -->  00:00:47.290
neural network and what we can see here is this
17

17

00:00:47.290  -->  00:00:50.480
whole processing action where we've got
18

18

00:00:50.480  -->  00:00:53.800
some input value, then we've got a weight,
19

19

00:00:53.800  -->  00:00:56.920
and then a activation functions applied,
20

20

00:00:56.920  -->  00:00:59.100
we get 天 and then we compare it to
21

21

00:00:59.100  -->  00:01:01.740
the actual value we calculate the cost function.
22

22

00:01:01.740  -->  00:01:05.310
So how can we minimize the cost function?
23

23

00:01:05.310  -->  00:01:07.270
What can we do about it?
24

24

00:01:07.270  -->  00:01:10.500
Well one approach to do it is a brute force approach
25

25

00:01:10.500  -->  00:01:14.120
where we just take all lots of different
26

26

00:01:14.120  -->  00:01:16.740
possible weights and look at them
27

27

00:01:16.740  -->  00:01:18.120
and see which one looks best.
28

28

00:01:18.120  -->  00:01:20.974
And what we do is, for instance, we'd try out,
29

29

00:01:20.974  -->  00:01:23.260
let's say for example a thousand weights,
30

30

00:01:23.260  -->  00:01:26.750
and we try them out, we'd get something like this
31

31

00:01:26.750  -->  00:01:30.320
for the cost function and this is a chart of
32

32

00:01:30.320  -->  00:01:32.040
on the y axis you have cost function
33

33

00:01:32.040  -->  00:01:33.680
on the vertical axis, on the horizontal axis
34

34

00:01:33.680  -->  00:01:36.940
you have 天 and because you can see the formula is
35

35

00:01:36.940  -->  00:01:40.240
天 minus y squared, this is what the cost function
36

36

00:01:40.240  -->  00:01:44.860
would look something like that and basically
37

37

00:01:44.860  -->  00:01:47.830
you'd find the best one is over here.
38

38

00:01:47.830  -->  00:01:50.950
So very simple, very intuitive approach.
39

39

00:01:50.950  -->  00:01:53.120
Why not do this brute force method?
40

40

00:01:53.120  -->  00:01:55.480
Why not just try out a thousand different
41

41

00:01:59.595  -->  00:02:01.590
parameters or inputs for weights
42

42

00:02:01.590  -->  00:02:02.930
and see which one works the best
43

43

00:02:02.930  -->  00:02:04.300
you'll find the best one that way
44

44

00:02:04.300  -->  00:02:06.700
Well if you have just one way to optimize
45

45

00:02:06.700  -->  00:02:09.290
this might work, but as you increase
46

46

00:02:09.290  -->  00:02:10.970
the number of weights, increase the number
47

47

00:02:10.970  -->  00:02:14.530
of synapses in your network you have to face
48

48

00:02:14.530  -->  00:02:16.520
the curse of dimensionality.
49

49

00:02:16.520  -->  00:02:19.370
And so what is the curse of dimensionality?
50

50

00:02:19.370  -->  00:02:21.600
The best way to describe this, or explain it,
51

51

00:02:21.600  -->  00:02:24.520
is to just look at a practical example.
52

52

00:02:24.520  -->  00:02:26.320
So remember this example we had
53

53

00:02:26.320  -->  00:02:27.950
when we were talking about how
54

54

00:02:27.950  -->  00:02:29.970
neural networks actually work?
55

55

00:02:29.970  -->  00:02:32.720
Where we were building, or running,
56

56

00:02:32.720  -->  00:02:37.020
a neural network for a appropriate valuation.
57

57

00:02:37.020  -->  00:02:38.610
So this is what it looked like
58

58

00:02:38.610  -->  00:02:40.660
when it was trained up already.
59

59

00:02:40.660  -->  00:02:43.220
Well when it's not trained, before it's trained,
60

60

00:02:43.220  -->  00:02:45.420
before we know which what are the weights
61

61

00:02:45.420  -->  00:02:47.610
the actual neural network looks like this.
62

62

00:02:47.610  -->  00:02:50.493
Right, because we have all these different,
63

63

00:02:51.360  -->  00:02:53.810
possible synapses there and we still
64

64

00:02:53.810  -->  00:02:55.130
have to train up the weights.
65

65

00:02:55.130  -->  00:02:57.160
And here we have a total of 25 weights.
66

66

00:02:57.160  -->  00:03:00.170
So four times five at the start plus five more
67

67

00:03:00.170  -->  00:03:03.540
from the hidden layer to the outward layer 25 weights total.
68

68

00:03:03.540  -->  00:03:08.540
And let's see how we could possibly brute force 25 weights.
69

69

00:03:09.160  -->  00:03:12.520
It's a very simple neural network right here.
70

70

00:03:12.520  -->  00:03:14.550
Very simple just one hidden layer,
71

71

00:03:14.550  -->  00:03:18.810
and how could we brute force our way through
72

72

00:03:18.810  -->  00:03:21.240
a neural network of this size?
73

73

00:03:21.240  -->  00:03:24.300
Well let's do some simple mathematical calculations.
74

74

00:03:24.300  -->  00:03:26.380
We have 25 weights so that means
75

75

00:03:26.380  -->  00:03:28.430
if we a thousand combinations that
76

76

00:03:28.430  -->  00:03:29.880
we're going to test out for every weight
77

77

00:03:29.880  -->  00:03:31.220
the total number of combinations
78

78

00:03:31.220  -->  00:03:32.895
is a thousand to the power of 25
79

79

00:03:32.895  -->  00:03:37.700
or 10 to the power of 75 different combinations.
80

80

00:03:37.700  -->  00:03:41.040
Now let's see how Sunway TaihuLight,
81

81

00:03:41.970  -->  00:03:46.703
the worlds fastest super computer as of June 2016,
82

82

00:03:47.920  -->  00:03:49.590
how would it approach this problem?
83

83

00:03:49.590  -->  00:03:53.667
Right so Sunway TaihuLight looks like this.
84

84

00:03:53.667  -->  00:03:57.890
It's a whole, huge building pretty much for this one
85

85

00:03:57.890  -->  00:04:02.370
super computer and it got the Guinness world record
86

86

00:04:02.370  -->  00:04:05.100
for being the fastest super computer,
87

87

00:04:05.100  -->  00:04:07.260
right now it is the fastest super computer
88

88

00:04:07.260  -->  00:04:10.800
in the world and Sunway TaihuLight
89

89

00:04:10.800  -->  00:04:14.427
can operate at a speed of 93 petaflops.
90

90

00:04:15.420  -->  00:04:19.830
Flop stand for floating operation per second.
91

91

00:04:19.830  -->  00:04:24.750
So it can do 93 to the power, times 10 to the power of 15
92

92

00:04:26.030  -->  00:04:27.980
floating operations per second.
93

93

00:04:27.980  -->  00:04:29.493
That's how quick it is.
94

94

00:04:31.180  -->  00:04:34.550
In comparison average computers right now
95

95

00:04:34.550  -->  00:04:38.100
they do like just over several gigaflops and so on,
96

96

00:04:38.100  -->  00:04:40.970
so like kind of those ranges,
97

97

00:04:40.970  -->  00:04:44.280
way less than Sunway TaihuLight.
98

98

00:04:44.280  -->  00:04:48.270
So Sunway TaihuLight is in the forefront of technology
99

99

00:04:48.270  -->  00:04:53.270
and let's say hypothetically that it can do one, test one,
100

100

00:04:55.990  -->  00:04:59.720
combination for our neural network in one flop,
101

101

00:04:59.720  -->  00:05:01.590
basically in one floating operation.
102

102

00:05:01.590  -->  00:05:03.463
That is not possible that is not practical
103

103

00:05:03.463  -->  00:05:06.310
because you need multiple floating operations
104

104

00:05:06.310  -->  00:05:09.410
to test out a single weight in your neural network
105

105

00:05:09.410  -->  00:05:11.190
but even let's give it a headstart.
106

106

00:05:11.190  -->  00:05:14.980
Let's say that it can do that, in an ideal world,
107

107

00:05:14.980  -->  00:05:17.330
it can do that in one floating operation.
108

108

00:05:17.330  -->  00:05:20.000
It can do one test per one floating operation.
109

109

00:05:20.000  -->  00:05:23.930
That means it will still require 10 to the power of 75
110

110

00:05:23.930  -->  00:05:27.530
divided by 93 times 10 to the power of 15
111

111

00:05:27.530  -->  00:05:32.140
seconds to run all of those tests,
112

112

00:05:32.140  -->  00:05:34.010
to brute force through that network.
113

113

00:05:34.010  -->  00:05:36.230
So that means one, or approximately
114

114

00:05:36.230  -->  00:05:38.250
10 to the power of 58 seconds and that
115

115

00:05:38.250  -->  00:05:42.040
is the same as 10 to the power of 50 years.
116

116

00:05:42.040  -->  00:05:45.200
That is a huge number that is longer
117

117

00:05:46.190  -->  00:05:48.110
than the universe has existed
118

118

00:05:48.110  -->  00:05:50.850
and that is definitely not going to...
119

119

00:05:50.850  -->  00:05:52.800
simply this number is so huge that
120

120

00:05:52.800  -->  00:05:55.780
it's just definitely not going to work for us
121

121

00:05:55.780  -->  00:05:59.030
at all in our optimization.
122

122

00:05:59.030  -->  00:06:01.990
So there we go this is a no no even on
123

123

00:06:01.990  -->  00:06:05.360
the worlds fastest super computer Sunway TaihuLight.
124

124

00:06:05.360  -->  00:06:07.530
So we have to come up with a very different approach
125

125

00:06:07.530  -->  00:06:10.230
how we're going to find the optimal weight.
126

126

00:06:10.230  -->  00:06:13.500
By the way, this, our neural network is very simple
127

127

00:06:13.500  -->  00:06:15.040
what about if the neural network
128

128

00:06:15.040  -->  00:06:16.800
looks like something like this?
129

129

00:06:16.800  -->  00:06:18.520
Right? Or even greater than that?
130

130

00:06:18.520  -->  00:06:22.640
Then, yeah. It's just not going to happen at all, ever.
131

131

00:06:22.640  -->  00:06:24.190
So the method we're going to be looking at
132

132

00:06:24.190  -->  00:06:26.630
is called gradient descent and you
133

133

00:06:26.630  -->  00:06:28.450
may have heard of it already.
134

134

00:06:28.450  -->  00:06:30.620
If not we'll find out what it is right now.
135

135

00:06:30.620  -->  00:06:35.620
So, there's our cost function and now
136

136

00:06:36.380  -->  00:06:39.480
we're going to see how we can faster,
137

137

00:06:39.480  -->  00:06:43.100
come up with a faster way to find the best option.
138

138

00:06:43.100  -->  00:06:45.840
So, let's say we start somewhere, you gotta start somewhere,
139

139

00:06:45.840  -->  00:06:50.110
so we start over there and from that point
140

140

00:06:50.110  -->  00:06:52.080
in the top left what we're going to do
141

141

00:06:52.080  -->  00:06:55.630
is we're going to look at the angle
142

142

00:06:55.630  -->  00:06:58.520
of our cost function at that point.
143

143

00:06:58.520  -->  00:06:59.990
So we're just going to, basically that's why
144

144

00:06:59.990  -->  00:07:02.030
it's called gradient because you have to differentiate,
145

145

00:07:02.030  -->  00:07:04.150
we're not going to look at the mathematical equations.
146

146

00:07:04.150  -->  00:07:07.420
We will provide some tips on additional reading
147

147

00:07:07.420  -->  00:07:09.690
at the end of the next lecture.
148

148

00:07:09.690  -->  00:07:12.950
But basically you just need to differentiate,
149

149

00:07:12.950  -->  00:07:16.830
find out what the slope is in that specific point
150

150

00:07:16.830  -->  00:07:19.210
and find out if the slope is positive or negative.
151

151

00:07:19.210  -->  00:07:21.800
If the slope is negative like in this case,
152

152

00:07:21.800  -->  00:07:23.870
means that you're going downhill.
153

153

00:07:23.870  -->  00:07:27.220
So to the right is downhill to the left is uphill.
154

154

00:07:27.220  -->  00:07:29.660
And from there it means you need to go right
155

155

00:07:29.660  -->  00:07:31.570
basically you need to go downhill
156

156

00:07:31.570  -->  00:07:32.990
and that's what we're going to do.
157

157

00:07:32.990  -->  00:07:35.380
Vroom. Takes a step right.
158

158

00:07:35.380  -->  00:07:36.940
The ball rolls down.
159

159

00:07:36.940  -->  00:07:39.480
Again same thing you calculate the slope
160

160

00:07:39.480  -->  00:07:40.750
this time the slope is positive
161

161

00:07:40.750  -->  00:07:43.460
meaning right is uphill left is downhill
162

162

00:07:43.460  -->  00:07:44.950
and you need to go left.
163

163

00:07:44.950  -->  00:07:46.690
And you roll the ball down.
164

164

00:07:46.690  -->  00:07:49.940
And again you calculate the slope
165

165

00:07:50.780  -->  00:07:52.180
and you roll the ball right.
166

166

00:07:53.060  -->  00:07:56.060
There we go so that's how you find, in simple terms,
167

167

00:07:56.060  -->  00:08:00.810
that's how you find the best weights,
168

168

00:08:00.810  -->  00:08:04.470
the best situation that minimizes your cost function.
169

169

00:08:04.470  -->  00:08:06.810
Of course it's not going to be, like a ball rolling
170

170

00:08:06.810  -->  00:08:08.730
is going to be a very zig zaggy type of approach
171

171

00:08:08.730  -->  00:08:11.820
but it's easier to remember a kind of...
172

172

00:08:11.820  -->  00:08:14.880
It's more fun to look at it as a ball rolling
173

173

00:08:14.880  -->  00:08:16.610
but in reality yes you just,
174

174

00:08:16.610  -->  00:08:18.670
it is going to be like a step by step approach
175

175

00:08:18.670  -->  00:08:21.020
so it's going to be a zig zaggy type of method.
176

176

00:08:21.960  -->  00:08:25.190
Yeah and also there's loads of other elements to it.
177

177

00:08:25.190  -->  00:08:27.723
There's things like, for instance,
178

178

00:08:28.720  -->  00:08:31.550
why does it go down?
179

179

00:08:31.550  -->  00:08:35.040
Why does it not go way over the lines?
180

180

00:08:35.040  -->  00:08:37.540
So it could have jumped out of this,
181

181

00:08:37.540  -->  00:08:39.340
gone upwards instead of downwards
182

182

00:08:39.340  -->  00:08:41.150
and things like that so there are parameters
183

183

00:08:41.150  -->  00:08:43.920
that you can tweak and again we will mention
184

184

00:08:43.920  -->  00:08:45.470
where you can find out more on that
185

185

00:08:45.470  -->  00:08:47.620
And plus we'll have this in the practical application.
186

186

00:08:47.620  -->  00:08:50.440
But in the simplest intuitive approach
187

187

00:08:50.440  -->  00:08:53.850
this is what is happening, we are getting to the bottom
188

188

00:08:53.850  -->  00:08:56.590
Why just understanding which way we need to go
189

189

00:08:56.590  -->  00:08:58.370
instead of brute forcing through
190

190

00:08:58.370  -->  00:09:01.000
thousands and thousands and millions and billions
191

191

00:09:01.000  -->  00:09:02.920
and quadrillions of combinations,
192

192

00:09:02.920  -->  00:09:05.140
we can just simply every time have a look
193

193

00:09:05.140  -->  00:09:08.830
at where is, which way is is sloping?
194

194

00:09:08.830  -->  00:09:10.540
So right, like you imagine you're
195

195

00:09:10.540  -->  00:09:12.200
standing on a hill, which way
196

196

00:09:12.200  -->  00:09:13.720
does it feel that it's going downwards?
197

197

00:09:13.720  -->  00:09:15.180
And whichever way it's going downwards
198

198

00:09:15.180  -->  00:09:16.430
you just keep walking that way
199

199

00:09:16.430  -->  00:09:18.010
you like take 50 steps that way
200

200

00:09:18.010  -->  00:09:19.340
and then you assess again.
201

201

00:09:19.340  -->  00:09:20.980
Okay, which way is it going downward?
202

202

00:09:20.980  -->  00:09:21.813
This way. Okay.
203

203

00:09:21.813  -->  00:09:24.570
Now take 50 steps, or less take 40 steps, that way
204

204

00:09:24.570  -->  00:09:28.410
so it gets less and less and less as you get closer.
205

205

00:09:28.410  -->  00:09:30.900
So here's an example of gradient descent
206

206

00:09:30.900  -->  00:09:32.610
applied in a two dimensional space.
207

207

00:09:32.610  -->  00:09:35.433
So that was a one dimensional example.
208

208

00:09:36.500  -->  00:09:40.200
Here we have a two dimensional space for a gradient descent.
209

209

00:09:40.200  -->  00:09:42.960
As you can see it's getting closer to the minimum.
210

210

00:09:42.960  -->  00:09:44.710
And it's also called gradient descent because
211

211

00:09:44.710  -->  00:09:49.580
you're descending into the minimum of the cost function.
212

212

00:09:49.580  -->  00:09:51.530
And, finally, here's a gradient descent
213

213

00:09:51.530  -->  00:09:53.310
applied in three dimensions,
214

214

00:09:53.310  -->  00:09:54.590
this is what it looks like if you
215

215

00:09:54.590  -->  00:09:56.430
project it onto two dimensions.
216

216

00:09:56.430  -->  00:09:59.570
You can see it zig zagging its way into the minimum.
217

217

00:09:59.570  -->  00:10:01.760
So there you go that was gradient descent.
218

218

00:10:01.760  -->  00:10:02.930
In the next tutorial we will talk about
219

219

00:10:02.930  -->  00:10:04.970
stochastic gradient descent,
220

220

00:10:04.970  -->  00:10:06.900
will be a continuation of this tutorial
221

221

00:10:06.900  -->  00:10:08.880
and I look forward to seeing you there.
222

222

00:10:08.880  -->  00:10:10.730
Until next time, enjoy Deep Learning.
