1
1

00:00:00,210  -->  00:00:02,880
<v Instructor>Hello and Welcome to this Python tutorial.</v>
2

2

00:00:02,880  -->  00:00:05,210
So, in the previous tutorial, we initialized our
3

3

00:00:05,210  -->  00:00:08,350
Artificial Neural Network by defining it as a sequence
4

4

00:00:08,350  -->  00:00:11,360
of layers and now in this new tutorial, we are gonna
5

5

00:00:11,360  -->  00:00:14,360
add the first layers of our A.N.N.
6

6

00:00:14,360  -->  00:00:18,090
which is the input layer and the first hidden layer.
7

7

00:00:18,090  -->  00:00:19,610
Alright, but before we do that,
8

8

00:00:19,610  -->  00:00:22,890
let's refresh our memories to remind us about the different
9

9

00:00:22,890  -->  00:00:27,170
steps of how an Artificial Neural Network is built.
10

10

00:00:27,170  -->  00:00:28,410
So here is the slide.
11

11

00:00:28,410  -->  00:00:30,800
This is the slide from the intuition tutorial
12

12

00:00:30,800  -->  00:00:33,680
and as we can see an Artificial Neural Network
13

13

00:00:33,680  -->  00:00:35,630
is built in seven steps.
14

14

00:00:35,630  -->  00:00:39,410
The first step is to randomly initialize the weights
15

15

00:00:39,410  -->  00:00:43,050
of each of the nodes to small numbers close to zero.
16

16

00:00:43,050  -->  00:00:45,480
So that will be done thanks to the Dense function
17

17

00:00:45,480  -->  00:00:48,130
which we take from this Dense module here
18

18

00:00:48,130  -->  00:00:49,800
that we imported previously.
19

19

00:00:49,800  -->  00:00:52,570
So basically, this Dense function that we're gonna use
20

20

00:00:52,570  -->  00:00:55,160
is going to take care of this first step.
21

21

00:00:55,160  -->  00:00:58,780
Then, next step, our first observation row goes into
22

22

00:00:58,780  -->  00:01:01,580
the neural network and as we can see in the step two
23

23

00:01:01,580  -->  00:01:04,110
each feature is going to one input node.
24

24

00:01:04,110  -->  00:01:06,190
So we already know the number of nodes we'll have
25

25

00:01:06,190  -->  00:01:08,740
in our input layer, and this number is nothing else
26

26

00:01:08,740  -->  00:01:10,470
than the number of independent variables
27

27

00:01:10,470  -->  00:01:12,400
we have in our matrix of features.
28

28

00:01:12,400  -->  00:01:13,550
So what is this number?
29

29

00:01:13,550  -->  00:01:16,190
That is 11 independent variables.
30

30

00:01:16,190  -->  00:01:18,080
So that means that in our input layer,
31

31

00:01:18,080  -->  00:01:20,330
we'll have 11 input nodes.
32

32

00:01:20,330  -->  00:01:22,910
Then the third step is forward-propagation,
33

33

00:01:22,910  -->  00:01:25,710
so from left to right the neurons are activated by the
34

34

00:01:25,710  -->  00:01:28,810
activation function in such a way that the higher the value
35

35

00:01:28,810  -->  00:01:31,360
of the activation function is for the neuron,
36

36

00:01:31,360  -->  00:01:34,650
the more impact this neuron is going to have in the network.
37

37

00:01:34,650  -->  00:01:37,450
That means the more it will pass on the signal
38

38

00:01:37,450  -->  00:01:40,240
from the nodes on the left to the nodes on the right,
39

39

00:01:40,240  -->  00:01:43,040
and so speaking of activation function, what we'll have
40

40

00:01:43,040  -->  00:01:45,810
to do in this tutorial to define the first hidden layer
41

41

00:01:45,810  -->  00:01:48,820
is to choose an activation function and as a reminder,
42

42

00:01:48,820  -->  00:01:51,840
there are several activation functions and the best one
43

43

00:01:51,840  -->  00:01:54,500
based on experiments and based on research is
44

44

00:01:54,500  -->  00:01:57,460
the rectifier function, which you can see right here.
45

45

00:01:57,460  -->  00:02:01,110
We also have this sigmoid function which is really good
46

46

00:02:01,110  -->  00:02:04,530
for output layer since using the sigmoid function
47

47

00:02:04,530  -->  00:02:08,120
for the output layer will allow us to get the probabilities
48

48

00:02:08,120  -->  00:02:10,190
of the different class, that means that will get the
49

49

00:02:10,190  -->  00:02:12,040
probability that the class equals one
50

50

00:02:12,040  -->  00:02:15,140
for each of the observation and even the new observations
51

51

00:02:15,140  -->  00:02:17,830
of the test set when we'll make our predictions.
52

52

00:02:17,830  -->  00:02:20,300
So that means that for each observations of the test set
53

53

00:02:20,300  -->  00:02:22,990
we'll get the probability that the customer leaves the bank
54

54

00:02:22,990  -->  00:02:25,430
and the probability that the customer stays in the bank
55

55

00:02:25,430  -->  00:02:28,200
and so that's great because as Kirill explain when he
56

56

00:02:28,200  -->  00:02:31,120
described the model, we are trying to build a segmentation
57

57

00:02:31,120  -->  00:02:33,543
model and by getting the probabilities thanks to the
58

58

00:02:33,543  -->  00:02:36,940
sigmoid activation function for the output layer,
59

59

00:02:36,940  -->  00:02:40,130
well we will be able to see which customers have the highest
60

60

00:02:40,130  -->  00:02:43,530
probabilities to leave the bank, so we will even be able to
61

61

00:02:43,530  -->  00:02:47,900
make a ranking of the customers ranked by their probability
62

62

00:02:47,900  -->  00:02:50,410
to leave the bank so that can be very useful and then you
63

63

00:02:50,410  -->  00:02:53,600
can segment your customers according to their probability
64

64

00:02:53,600  -->  00:02:55,910
to leave the bank and according to what you decide to do
65

65

00:02:55,910  -->  00:02:58,830
in terms of business constraints and business goals.
66

66

00:02:58,830  -->  00:03:00,090
Alright, so to summarize,
67

67

00:03:00,090  -->  00:03:02,570
we'll choose the rectifier activation function
68

68

00:03:02,570  -->  00:03:04,460
for the hidden layers and we'll choose
69

69

00:03:04,460  -->  00:03:07,770
the sigmoid activation function for the output layer.
70

70

00:03:07,770  -->  00:03:10,800
Alright, so then step four, in step four the algorithm
71

71

00:03:10,800  -->  00:03:14,030
compares the predicted result to the actual result
72

72

00:03:14,030  -->  00:03:16,840
and that generates an error and then in the step five
73

73

00:03:16,840  -->  00:03:19,580
this error is back-propagated in the neural network
74

74

00:03:19,580  -->  00:03:22,210
from right to left and all the weights are updated
75

75

00:03:22,210  -->  00:03:24,680
according to how much they are responsible
76

76

00:03:24,680  -->  00:03:26,280
for this generated error.
77

77

00:03:26,280  -->  00:03:28,710
And by the way, there are several way of updating these
78

78

00:03:28,710  -->  00:03:31,230
weights and this way is defined by the learning rate
79

79

00:03:31,230  -->  00:03:34,880
perimeter which decides by how much the weights are updated.
80

80

00:03:34,880  -->  00:03:37,510
Alright, and then we have step six, where we repeat
81

81

00:03:37,510  -->  00:03:40,620
steps one to five either after each observation or
82

82

00:03:40,620  -->  00:03:43,530
either after a batch of observations like for example
83

83

00:03:43,530  -->  00:03:45,840
we update the weights every ten observations going
84

84

00:03:45,840  -->  00:03:48,520
into the network and finally step seven,
85

85

00:03:48,520  -->  00:03:50,970
when the whole training set passed through the A.N.N.
86

86

00:03:50,970  -->  00:03:54,900
that makes an epoch and re-repeat many more epoch.
87

87

00:03:54,900  -->  00:03:57,610
And we will see how many epoch we choose to do in the end.
88

88

00:03:57,610  -->  00:03:59,200
Alright, so that was a quick refresher
89

89

00:03:59,200  -->  00:04:01,590
of how we can train an Artificial Neural Network
90

90

00:04:01,590  -->  00:04:03,540
using Stochastic Gradient Descent
91

91

00:04:03,540  -->  00:04:07,030
and so now let's do this step here to add the
92

92

00:04:07,030  -->  00:04:09,710
input layer and the first hidden layer.
93

93

00:04:09,710  -->  00:04:11,980
Alright, so the first thing we're gonna do is take
94

94

00:04:11,980  -->  00:04:15,130
our classifier, that's the classifier we initialized
95

95

00:04:15,130  -->  00:04:17,390
in the previous step by creating an object
96

96

00:04:17,390  -->  00:04:20,780
of the sequential class and now we're gonna use a method
97

97

00:04:20,780  -->  00:04:24,500
of this object which is the add method and as you might
98

98

00:04:24,500  -->  00:04:27,720
have guessed, this add method is the method that we use
99

99

00:04:27,720  -->  00:04:30,980
to add the different layers in our neural network.
100

100

00:04:30,980  -->  00:04:34,050
So remember, an object is separated from the method
101

101

00:04:34,050  -->  00:04:37,840
by a dot, here we go, and as you can see, the first method
102

102

00:04:37,840  -->  00:04:41,010
here on this list is add, so I just need to press Enter.
103

103

00:04:41,010  -->  00:04:44,790
Here we go, and now let's add some parenthesis and see
104

104

00:04:44,790  -->  00:04:47,320
what we need to add in this add method.
105

105

00:04:47,320  -->  00:04:49,500
So as you saw the yellow rectangle appear here,
106

106

00:04:49,500  -->  00:04:53,170
there is only one argument and this argument is the layer
107

107

00:04:53,170  -->  00:04:56,410
that we want to add to our A.N.N.
108

108

00:04:56,410  -->  00:04:58,580
So as you can see in this title of this section,
109

109

00:04:58,580  -->  00:05:00,850
right now we're gonna add two layers,
110

110

00:05:00,850  -->  00:05:03,150
the input layer and the first hidden layer
111

111

00:05:03,150  -->  00:05:06,083
and we are gonna do this thanks to the Dense function
112

112

00:05:06,083  -->  00:05:09,640
I've mentioned earlier and so right now we're going to input
113

113

00:05:09,640  -->  00:05:13,580
Dense and then some parenthesis and as you can see now
114

114

00:05:13,580  -->  00:05:16,490
there are a lot of arguments and so we'll add the different
115

115

00:05:16,490  -->  00:05:19,140
arguments of this Dense function and these arguments
116

116

00:05:19,140  -->  00:05:21,130
are going to be all the perimeters that we can see
117

117

00:05:21,130  -->  00:05:24,200
on this slide that is how the weights are updated,
118

118

00:05:24,200  -->  00:05:26,260
what is the activation function we use
119

119

00:05:26,260  -->  00:05:28,640
and what is the number of nodes we want to chose
120

120

00:05:28,640  -->  00:05:31,530
in the layer and what is the number of input nodes?
121

121

00:05:31,530  -->  00:05:35,110
Well, everything happens here in this Dense function.
122

122

00:05:35,110  -->  00:05:38,700
So, let's have a better look at what we need to input
123

123

00:05:38,700  -->  00:05:42,523
by pressing here command I to get into the help
124

124

00:05:42,523  -->  00:05:44,900
and that gives us all the different perimeters
125

125

00:05:44,900  -->  00:05:46,390
that we need to input.
126

126

00:05:46,390  -->  00:05:48,590
Alright, so let's have a look at the first perimeter.
127

127

00:05:48,590  -->  00:05:51,320
The first perimeter is output_dim,
128

128

00:05:51,320  -->  00:05:53,090
okay so what is output_dim?
129

129

00:05:53,090  -->  00:05:55,790
That is simply the number of nodes you want to add
130

130

00:05:55,790  -->  00:05:58,610
in this hidden layer, because more precisely,
131

131

00:05:58,610  -->  00:06:02,110
what this add function does is not to add the input
132

132

00:06:02,110  -->  00:06:04,960
layer in the first hidden layer, what it really does
133

133

00:06:04,960  -->  00:06:08,540
is add this hidden layer, and by adding this hidden layer
134

134

00:06:08,540  -->  00:06:11,750
we're specifying the number of inputs in the previous
135

135

00:06:11,750  -->  00:06:13,500
layer which is the input layer.
136

136

00:06:13,500  -->  00:06:17,230
So, the output_dim here is just the number of nodes
137

137

00:06:17,230  -->  00:06:20,590
of the layer we are adding in this add function.
138

138

00:06:20,590  -->  00:06:23,490
So, that's the number of nodes of the hidden layer,
139

139

00:06:23,490  -->  00:06:26,510
and now the big question is how many nodes are we gonna
140

140

00:06:26,510  -->  00:06:28,720
add in this hidden layer?
141

141

00:06:28,720  -->  00:06:30,230
That's a big question in Deep Learning.
142

142

00:06:30,230  -->  00:06:32,700
You'll get a lot of questions like that in forums
143

143

00:06:32,700  -->  00:06:35,550
or Deep Learning discussions and the answer that comes
144

144

00:06:35,550  -->  00:06:38,120
most of the time is that it's art.
145

145

00:06:38,120  -->  00:06:40,910
Indeed, there is no rule of thumbs on what would be
146

146

00:06:40,910  -->  00:06:43,880
the optimal number of nodes in this hidden layer.
147

147

00:06:43,880  -->  00:06:46,450
We can give some rules, like for example if your data
148

148

00:06:46,450  -->  00:06:49,310
is linearly separable, well you don't even need a hidden
149

149

00:06:49,310  -->  00:06:52,330
layer and in fact, you don't even need a neural network,
150

150

00:06:52,330  -->  00:06:54,010
but we have better than that.
151

151

00:06:54,010  -->  00:06:57,400
I would not characterize it as a rule, but rather as a tip
152

152

00:06:57,400  -->  00:06:59,390
and this tip is not based on theory,
153

153

00:06:59,390  -->  00:07:01,540
but rather based on experiment.
154

154

00:07:01,540  -->  00:07:04,130
This tip is to choose the number of nodes in the hidden
155

155

00:07:04,130  -->  00:07:07,140
layer as the average of the number of nodes in the input
156

156

00:07:07,140  -->  00:07:10,130
layer and the number of nodes in the output layer.
157

157

00:07:10,130  -->  00:07:12,500
That is the tip you want to use if you don't want to be
158

158

00:07:12,500  -->  00:07:14,980
an artist and if you want to be an artist,
159

159

00:07:14,980  -->  00:07:18,140
well what you have to do is experimenting with a technique
160

160

00:07:18,140  -->  00:07:20,000
called perimeter tuning.
161

161

00:07:20,000  -->  00:07:22,590
Perimeter tuning is about using some techniques,
162

162

00:07:22,590  -->  00:07:25,340
like k-fold cross validation for example which we'll
163

163

00:07:25,340  -->  00:07:28,618
study in part 10, model selection and ensemble model
164

164

00:07:28,618  -->  00:07:31,700
and these k-fold cross validation technique consists
165

165

00:07:31,700  -->  00:07:35,100
on creating a separate set in your data set besides
166

166

00:07:35,100  -->  00:07:37,560
the training set and the test that is called a cross
167

167

00:07:37,560  -->  00:07:40,330
validation set and basically in this set,
168

168

00:07:40,330  -->  00:07:43,170
you experiment different perimeters of your model,
169

169

00:07:43,170  -->  00:07:45,560
so typically the number of hidden layers and the number
170

170

00:07:45,560  -->  00:07:48,090
of nodes in the hidden layers and then you test
171

171

00:07:48,090  -->  00:07:49,790
the performance of your different models
172

172

00:07:49,790  -->  00:07:51,420
with the different perimeters.
173

173

00:07:51,420  -->  00:07:54,150
So we won't do it now obviously, because we want to focus
174

174

00:07:54,150  -->  00:07:57,320
on our Artificial Neural Network, but keep that in mind
175

175

00:07:57,320  -->  00:07:59,800
and keep in mind that we'll do this in part 10
176

176

00:07:59,800  -->  00:08:02,700
and that it will help us choose the optimal perimeters
177

177

00:08:02,700  -->  00:08:05,660
of our model, but for now we're gonna take this tip
178

178

00:08:05,660  -->  00:08:07,730
which is to take the average of the number of nodes
179

179

00:08:07,730  -->  00:08:10,360
in the input layer and the number of nodes in the output
180

180

00:08:10,360  -->  00:08:13,660
layer so that is, if I go back to variable explorer,
181

181

00:08:13,660  -->  00:08:16,440
well the number of nodes in the input layer is 11,
182

182

00:08:16,440  -->  00:08:18,590
because the number of nodes in the input layer is
183

183

00:08:18,590  -->  00:08:21,300
the number of independent variables, so 11,
184

184

00:08:21,300  -->  00:08:23,910
and the number of nodes in the output layer is one
185

185

00:08:23,910  -->  00:08:26,450
because as you saw in Kurill's intuition tutorial
186

186

00:08:26,450  -->  00:08:28,850
when the dependent variable has a binary outcome
187

187

00:08:28,850  -->  00:08:32,270
one or zero well there is only one node in the output layer
188

188

00:08:32,270  -->  00:08:35,440
and so the average is 11 plus one divided by two,
189

189

00:08:35,440  -->  00:08:37,870
that is six nodes in the hidden layer.
190

190

00:08:37,870  -->  00:08:41,250
So that's what we'll choose for the output_dim perimeter
191

191

00:08:41,250  -->  00:08:43,900
here and therefore the first argument
192

192

00:08:43,900  -->  00:08:46,377
that we'll input here is output_dim
193

193

00:08:47,910  -->  00:08:49,990
equals six.
194

194

00:08:49,990  -->  00:08:54,560
Alright, next argument, the next argument is init,
195

195

00:08:54,560  -->  00:08:58,270
so that corresponds to the step one of the
196

196

00:08:58,270  -->  00:09:00,830
Stochastic Gradient Descent and so what we have to do
197

197

00:09:00,830  -->  00:09:02,880
now is we have to randomly initialize the weight
198

198

00:09:02,880  -->  00:09:06,520
as small numbers close to zero and so we can randomly
199

199

00:09:06,520  -->  00:09:08,910
initialize them with a uniform function.
200

200

00:09:08,910  -->  00:09:11,379
Here we see that we have the glorot_uniform
201

201

00:09:11,379  -->  00:09:14,690
this is kind of a uniform function to initialize the weight
202

202

00:09:14,690  -->  00:09:17,610
and we even have more simple, there is the simple uniform
203

203

00:09:17,610  -->  00:09:20,410
function which will initialize the weights according
204

204

00:09:20,410  -->  00:09:23,330
to a uniform distribution and the sciences will make sure
205

205

00:09:23,330  -->  00:09:25,860
that the weights are small numbers close to zero.
206

206

00:09:25,860  -->  00:09:28,870
Alright so let's input the second argument init.
207

207

00:09:28,870  -->  00:09:33,410
Here we go, init equals in quotes uniform.
208

208

00:09:33,410  -->  00:09:36,280
Great, now come out to input the third argument
209

209

00:09:36,280  -->  00:09:39,620
and the third argument is the activation argument
210

210

00:09:39,620  -->  00:09:41,960
which of course is the activation function we want
211

211

00:09:41,960  -->  00:09:44,170
to choose in our hidden layer.
212

212

00:09:44,170  -->  00:09:46,130
So as I just explained, we're gonna chose
213

213

00:09:46,130  -->  00:09:49,330
the rectifier activation function for the hidden layers
214

214

00:09:49,330  -->  00:09:52,680
and the sigmoid activation function for the output layer.
215

215

00:09:52,680  -->  00:09:54,740
So here, since we are in the hidden layer,
216

216

00:09:54,740  -->  00:09:59,270
we're gonna input here activation equals, and actually
217

217

00:09:59,270  -->  00:10:01,010
the perimeter that corresponds to this
218

218

00:10:01,010  -->  00:10:05,240
rectifier function is called relu, r-e-l-u.
219

219

00:10:05,240  -->  00:10:08,380
Alright, so let's see if we have all the arguments here.
220

220

00:10:08,380  -->  00:10:10,840
We specified the number of nodes in the hidden layer,
221

221

00:10:10,840  -->  00:10:13,620
we specified the initialization method,
222

222

00:10:13,620  -->  00:10:16,160
and we specified the activation function.
223

223

00:10:16,160  -->  00:10:20,580
If I select this line right now and press Enter to Execute,
224

224

00:10:20,580  -->  00:10:23,820
I get this raise Exception error, and why's that?
225

225

00:10:23,820  -->  00:10:25,470
It's because there is another perimeter
226

226

00:10:25,470  -->  00:10:29,010
that is kind of hidden here, that we must input as well.
227

227

00:10:29,010  -->  00:10:31,120
Well I think you might guess what it is.
228

228

00:10:31,120  -->  00:10:34,400
It is actually the input_dim perimeter.
229

229

00:10:34,400  -->  00:10:37,160
If init appears as one of the last arguments here
230

230

00:10:37,160  -->  00:10:40,010
this is actually a compulsory argument because
231

231

00:10:40,010  -->  00:10:41,260
what is this argument?
232

232

00:10:41,260  -->  00:10:44,250
This argument is the number of nodes in the input layer
233

233

00:10:44,250  -->  00:10:46,860
that is the number of independent variables.
234

234

00:10:46,860  -->  00:10:50,360
And why is it compulsory to add this argument at this stage?
235

235

00:10:50,360  -->  00:10:53,130
It's because so far our Artificial Neural Network
236

236

00:10:53,130  -->  00:10:56,770
is simply initialized, we haven't created any layer yet
237

237

00:10:56,770  -->  00:11:00,790
and that's why it doesn't know here which nodes this hidden
238

238

00:11:00,790  -->  00:11:05,000
layer here that we are creating is expecting as inputs.
239

239

00:11:05,000  -->  00:11:07,760
So that's why we have to specify this argument here
240

240

00:11:07,760  -->  00:11:10,170
but then when we'll add some other hidden layers
241

241

00:11:10,170  -->  00:11:13,730
well since this first hidden layer will already be created
242

242

00:11:13,730  -->  00:11:16,620
we won't need to specify this input_dim argument
243

243

00:11:16,620  -->  00:11:18,480
for the next hidden layers because
244

244

00:11:18,480  -->  00:11:20,470
since the first one will already be created,
245

245

00:11:20,470  -->  00:11:23,770
the next hidden layer will know what to expect.
246

246

00:11:23,770  -->  00:11:27,210
Alright so let's add this last compulsory argument here
247

247

00:11:27,210  -->  00:11:28,560
which is
248

248

00:11:28,560  -->  00:11:30,100
the input
249

249

00:11:30,100  -->  00:11:31,390
underscore dim
250

250

00:11:31,390  -->  00:11:33,670
and that is equal to 11
251

251

00:11:33,670  -->  00:11:36,270
because we have 11 independent variables.
252

252

00:11:36,270  -->  00:11:38,520
Alright, so now we are ready to execute,
253

253

00:11:38,520  -->  00:11:41,550
it should work fine, so we'll select this line
254

254

00:11:41,550  -->  00:11:45,380
and execute and as you can see, our first hidden layer
255

255

00:11:45,380  -->  00:11:49,000
and at the same time, our input layer were very well added
256

256

00:11:49,000  -->  00:11:51,040
in our Artificial Neural Network.
257

257

00:11:51,040  -->  00:11:54,010
So perfect, now we're ready to move on to the next step,
258

258

00:11:54,010  -->  00:11:57,400
which is to add a new hidden layer, which to be honest
259

259

00:11:57,400  -->  00:11:59,618
is not necessarily useful for our dataset
260

260

00:11:59,618  -->  00:12:02,250
but we're gonna add it anyway for two reasons.
261

261

00:12:02,250  -->  00:12:04,560
First of all because this part is called Deep Learning
262

262

00:12:04,560  -->  00:12:07,760
and Deep Learning is defined as an Artificial Neural Network
263

263

00:12:07,760  -->  00:12:10,470
with many hidden layers and the second reason is simply
264

264

00:12:10,470  -->  00:12:12,860
that if you need to add more hidden layers in your
265

265

00:12:12,860  -->  00:12:15,870
neural networks, well you need to know how to do it.
266

266

00:12:15,870  -->  00:12:18,340
And that's what you'll learn how to do in the next tutorial,
267

267

00:12:18,340  -->  00:12:20,153
and until then, enjoy Deep Learning.
