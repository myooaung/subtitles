1
1

00:00:00,540  -->  00:00:01,750
<v Instructor>Hello and welcome back to the course</v>
2

2

00:00:01,750  -->  00:00:02,890
on Deep Learning.
3

3

00:00:02,890  -->  00:00:05,560
Today we're talking about deep autoencoders.
4

4

00:00:05,560  -->  00:00:09,610
In fact this is a quick tutorial to introduce
5

5

00:00:09,610  -->  00:00:10,870
deep autoencoders.
6

6

00:00:10,870  -->  00:00:12,980
So why is this tutorial important?
7

7

00:00:12,980  -->  00:00:15,090
Because I wanted to point out one thing
8

8

00:00:15,090  -->  00:00:16,430
that you should be aware of,
9

9

00:00:16,430  -->  00:00:19,170
stacked autoencoders are not the same thing
10

10

00:00:19,170  -->  00:00:21,270
as deep autoencoders.
11

11

00:00:21,270  -->  00:00:23,810
And this will become extremely obvious when you see
12

12

00:00:23,810  -->  00:00:25,260
the full in picture.
13

13

00:00:25,260  -->  00:00:26,730
Are you ready for it?
14

14

00:00:26,730  -->  00:00:28,160
And here we go.
15

15

00:00:28,160  -->  00:00:30,130
This is a deep autoencoder.
16

16

00:00:30,130  -->  00:00:32,390
And right away, just because based on the colors
17

17

00:00:32,390  -->  00:00:33,830
and the way we've structured this course
18

18

00:00:33,830  -->  00:00:37,950
you can tell that this is a blast from the past.
19

19

00:00:37,950  -->  00:00:40,580
These are restricted Boltzmann machine.
20

20

00:00:40,580  -->  00:00:44,590
So stacked pre-trained, layer by layer RBMs,
21

21

00:00:44,590  -->  00:00:46,600
this is what we have here.
22

22

00:00:46,600  -->  00:00:49,720
These are, this is our stacked autoencoder.
23

23

00:00:49,720  -->  00:00:52,880
So basically it's RBMs that are stacked.
24

24

00:00:52,880  -->  00:00:54,800
Then they're pre-trained layer by layer.
25

25

00:00:54,800  -->  00:00:55,830
Then they're unrolled.
26

26

00:00:55,830  -->  00:00:57,800
Then they're fine tuned with back propagation.
27

27

00:00:57,800  -->  00:01:01,000
So then you do get directionality, and then you,
28

28

00:01:01,000  -->  00:01:02,880
in your network and then you have back propagation.
29

29

00:01:02,880  -->  00:01:06,640
But in essence a deep autoencoder,
30

30

00:01:06,640  -->  00:01:07,770
comes from RBMs.
31

31

00:01:07,770  -->  00:01:10,810
Stacked autoencoders are just normal autoencoders stacked.
32

32

00:01:10,810  -->  00:01:14,420
A deep autoencoder is RBMs stacked on top of each other
33

33

00:01:14,420  -->  00:01:17,450
and then certain things are done with them in order
34

34

00:01:17,450  -->  00:01:20,940
to achieve a autoencoding mechanism.
35

35

00:01:20,940  -->  00:01:21,773
There we go.
36

36

00:01:21,773  -->  00:01:23,120
Just wanted to make sure you're aware of that
37

37

00:01:23,120  -->  00:01:26,800
and that if you hear one of these terms, or the other,
38

38

00:01:26,800  -->  00:01:29,800
that you know exactly what is being referenced.
39

39

00:01:29,800  -->  00:01:32,960
And if you'd like to learn more about deep autoencoders,
40

40

00:01:32,960  -->  00:01:35,900
a great paper by Geoffrey Hinton and others called,
41

41

00:01:35,900  -->  00:01:40,020
Reducing the Dimensionality of Data with Neural Networks.
42

42

00:01:40,020  -->  00:01:42,350
So check it out if you'd like to learn more about
43

43

00:01:42,350  -->  00:01:43,610
deep autoencoders.
44

44

00:01:43,610  -->  00:01:46,470
And remember that they're based on RBMs.
45

45

00:01:46,470  -->  00:01:48,340
And I look forward to seeing you next time.
46

46

00:01:48,340  -->  00:01:50,293
Until then enjoy Deep Learning.
