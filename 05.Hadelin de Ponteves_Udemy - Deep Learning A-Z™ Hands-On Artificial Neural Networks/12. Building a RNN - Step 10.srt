1
1

00:00:00,520  -->  00:00:02,930
<v Narrator>Hello and welcome to this new tutorial.</v>
2

2

00:00:02,930  -->  00:00:05,420
In this one we are going to compile the RNN
3

3

00:00:05,420  -->  00:00:07,580
with the right optimizer
4

4

00:00:07,580  -->  00:00:09,210
and the right loss function.
5

5

00:00:09,210  -->  00:00:11,400
So, we have done this before.
6

6

00:00:11,400  -->  00:00:14,640
We first start by taking our regressor,
7

7

00:00:14,640  -->  00:00:15,890
this time regressor because we are
8

8

00:00:15,890  -->  00:00:17,900
predicting a continuous value
9

9

00:00:17,900  -->  00:00:19,940
and, we add a dot and then, of course,
10

10

00:00:19,940  -->  00:00:22,460
we use the compile
11

11

00:00:22,460  -->  00:00:24,780
method, which is another method
12

12

00:00:24,780  -->  00:00:26,650
of the sequential class.
13

13

00:00:26,650  -->  00:00:28,350
And in this compile method
14

14

00:00:28,350  -->  00:00:31,390
well, this time for our regression problem here,
15

15

00:00:31,390  -->  00:00:33,210
we need to input two arguments
16

16

00:00:33,210  -->  00:00:34,670
and these are gonna be of course,
17

17

00:00:34,670  -->  00:00:37,520
the optimizer and the loss function.
18

18

00:00:37,520  -->  00:00:39,960
So, let's start with the optimizer,
19

19

00:00:39,960  -->  00:00:44,450
op-ti-mi-zer like that is the name of the parameter
20

20

00:00:44,450  -->  00:00:46,540
and this will be equal to
21

21

00:00:46,540  -->  00:00:48,890
well, here we have a choice.
22

22

00:00:48,890  -->  00:00:52,470
Because you will see that for rec renewal networks
23

23

00:00:52,470  -->  00:00:55,630
in general, and especially in the keras documentation,
24

24

00:00:55,630  -->  00:00:59,900
that an RMS prop optimizer is recommended.
25

25

00:00:59,900  -->  00:01:03,360
So, if we look at the keras documentation,
26

26

00:01:03,360  -->  00:01:06,170
which is on the SIMS site as what we looked at
27

27

00:01:06,170  -->  00:01:08,010
for our input shapes.
28

28

00:01:08,010  -->  00:01:09,710
Well, let's try and look for it.
29

29

00:01:09,710  -->  00:01:12,780
We're looking for the optimizer and I think,
30

30

00:01:12,780  -->  00:01:15,550
that if we scroll down here we will find optimizers.
31

31

00:01:15,550  -->  00:01:16,450
There we go,
32

32

00:01:16,450  -->  00:01:18,410
So, as you can see you have all
33

33

00:01:18,410  -->  00:01:20,720
the optimizers you can use in keras
34

34

00:01:20,720  -->  00:01:23,900
and here is the RMS prop that I just talked about.
35

35

00:01:23,900  -->  00:01:25,520
So it is some kind of an advanced
36

36

00:01:25,520  -->  00:01:27,740
stochastic gradient descent optimizer.
37

37

00:01:27,740  -->  00:01:29,050
And as you can see here,
38

38

00:01:29,050  -->  00:01:31,500
this optimizer is usually a good choice
39

39

00:01:31,500  -->  00:01:33,730
for recurrent neural networks.
40

40

00:01:33,730  -->  00:01:35,860
So, that's the first option you can try.
41

41

00:01:35,860  -->  00:01:37,630
Try with an RMS prop.
42

42

00:01:37,630  -->  00:01:39,700
However, with our implementation
43

43

00:01:39,700  -->  00:01:41,600
we're not gonna use an RMS prop
44

44

00:01:41,600  -->  00:01:44,900
and that's for the simple reason that I have experimented
45

45

00:01:44,900  -->  00:01:47,690
with another optimizer which is
46

46

00:01:47,690  -->  00:01:49,400
the one you're very familiar with;
47

47

00:01:49,400  -->  00:01:51,950
it is the Adam optimizer.
48

48

00:01:51,950  -->  00:01:54,980
The Adam optimizer is always a safe choice.
49

49

00:01:54,980  -->  00:01:57,790
It's always a good choice because it is very powerful
50

50

00:01:57,790  -->  00:01:59,740
and it always performs some
51

51

00:01:59,740  -->  00:02:01,910
relevant updates of the weights.
52

52

00:02:01,910  -->  00:02:04,360
And therefore, in our implementation we are gonna use
53

53

00:02:04,360  -->  00:02:05,850
the Adam optimizer.
54

54

00:02:05,850  -->  00:02:08,390
Which, if you scroll down even more
55

55

00:02:08,390  -->  00:02:09,750
you should be able to find.
56

56

00:02:09,750  -->  00:02:10,600
Here we go,
57

57

00:02:10,600  -->  00:02:12,730
Adam, Adam optimizer.
58

58

00:02:12,730  -->  00:02:14,420
And you can by the way see the arguments
59

59

00:02:14,420  -->  00:02:15,540
that you have to input.
60

60

00:02:15,540  -->  00:02:18,440
If by any chance you are forgetting about them
61

61

00:02:18,440  -->  00:02:20,190
so there we go, Adam optimizer
62

62

00:02:20,190  -->  00:02:22,320
and then you can find some other ones like
63

63

00:02:22,320  -->  00:02:24,670
the Nadam or TFOptimizer.
64

64

00:02:24,670  -->  00:02:28,550
But the most relevant ones for ANN's are
65

65

00:02:28,550  -->  00:02:30,110
RMS prop or Adam.
66

66

00:02:30,110  -->  00:02:32,030
And as far as we're concerned,
67

67

00:02:32,030  -->  00:02:33,720
with our problem we are going to choose
68

68

00:02:33,720  -->  00:02:35,290
the Adam optimizer.
69

69

00:02:35,290  -->  00:02:36,900
So let's go back to python
70

70

00:02:36,900  -->  00:02:39,380
and let's put here in quotes,
71

71

00:02:39,380  -->  00:02:42,460
adam, no capital letters, just like that
72

72

00:02:42,460  -->  00:02:43,800
exactly like before.
73

73

00:02:43,800  -->  00:02:44,650
Perfect.
74

74

00:02:44,650  -->  00:02:46,450
So now we have our Adam optimizer
75

75

00:02:46,450  -->  00:02:49,320
and now we need to input the second argument
76

76

00:02:49,320  -->  00:02:50,500
of this compile method,
77

77

00:02:50,500  -->  00:02:51,890
which is the loss.
78

78

00:02:51,890  -->  00:02:54,610
And the name of the parameter for the loss is
79

79

00:02:54,610  -->  00:02:56,460
loss; just like that.
80

80

00:02:56,460  -->  00:02:57,310
And now,
81

81

00:02:57,310  -->  00:02:59,430
this time things are gonna change.
82

82

00:02:59,430  -->  00:03:01,750
We're not doing classification anymore.
83

83

00:03:01,750  -->  00:03:04,860
So the loss is not gonna be binary cross entropy.
84

84

00:03:04,860  -->  00:03:07,530
This time, we're dealing with a regression problem
85

85

00:03:07,530  -->  00:03:09,850
because we have to predict a continuous value
86

86

00:03:09,850  -->  00:03:12,430
and the loss for this kind of problem is
87

87

00:03:12,430  -->  00:03:14,000
the mean squared error.
88

88

00:03:14,000  -->  00:03:15,460
Which makes sense because
89

89

00:03:15,460  -->  00:03:17,030
the error can be measured this time
90

90

00:03:17,030  -->  00:03:19,810
by the mean of the squared differences
91

91

00:03:19,810  -->  00:03:22,460
between the predictions and the targets.
92

92

00:03:22,460  -->  00:03:23,610
And by targets I mean
93

93

00:03:23,610  -->  00:03:24,870
the real values.
94

94

00:03:24,870  -->  00:03:27,520
So that's what the loss is for regression
95

95

00:03:27,520  -->  00:03:30,570
and it is called the mean squared error and very simply
96

96

00:03:30,570  -->  00:03:32,840
the name that we have to type here
97

97

00:03:32,840  -->  00:03:33,730
for this loss
98

98

00:03:33,730  -->  00:03:34,944
is mean
99

99

00:03:34,944  -->  00:03:35,827
underscore
100

100

00:03:35,827  -->  00:03:36,718
squared
101

101

00:03:36,718  -->  00:03:37,784
underscore
102

102

00:03:37,784  -->  00:03:39,280
error.
103

103

00:03:39,280  -->  00:03:40,470
Just like that.
104

104

00:03:40,470  -->  00:03:42,040
Mean squared error,
105

105

00:03:42,040  -->  00:03:44,470
also sometimes called MSE.
106

106

00:03:44,470  -->  00:03:47,670
Alright perfect so let's execute this.
107

107

00:03:47,670  -->  00:03:50,310
I'm selecting this and, execute.
108

108

00:03:50,310  -->  00:03:51,390
Alright perfect.
109

109

00:03:51,390  -->  00:03:55,350
Now our regressor is compiled with a powerful optimizer
110

110

00:03:55,350  -->  00:03:57,540
and the right loss function.
111

111

00:03:57,540  -->  00:04:01,160
And there you go, we are done with compiling the RNN.
112

112

00:04:01,160  -->  00:04:02,520
Another step done.
113

113

00:04:02,520  -->  00:04:04,090
And now we're gonna move on to
114

114

00:04:04,090  -->  00:04:06,880
the final step of this part two;
115

115

00:04:06,880  -->  00:04:09,910
which will be about of course fitting the RNN
116

116

00:04:09,910  -->  00:04:11,240
to the training set
117

117

00:04:11,240  -->  00:04:13,290
which I remind is composed of
118

118

00:04:13,290  -->  00:04:15,210
X train and Y train.
119

119

00:04:15,210  -->  00:04:17,260
So that's exactly what we'll do in the next
120

120

00:04:17,260  -->  00:04:19,800
and final tutorial of part two
121

121

00:04:19,800  -->  00:04:21,550
and until then, enjoy Deep Learning
