WEBVTT
1
1

00:00:00.250  -->  00:00:02.770
<v Instructor>Hello and welcome to this Python tutorial.</v>
2

2

00:00:02.770  -->  00:00:05.920
Alright, so now let's make the second function we need
3

3

00:00:05.920  -->  00:00:07.740
for our RBM class.
4

4

00:00:07.740  -->  00:00:11.020
The second function is about sampling the hidden nodes
5

5

00:00:11.020  -->  00:00:14.400
according to the probabilities, p h given v
6

6

00:00:14.400  -->  00:00:17.730
where h is a hidden node and v is a visible node
7

7

00:00:17.730  -->  00:00:19.790
and as you saw in the intuition lectures,
8

8

00:00:19.790  -->  00:00:22.120
this probability p h given v
9

9

00:00:22.120  -->  00:00:25.720
is nothing else than the sigmoid activation function.
10

10

00:00:25.720  -->  00:00:27.020
And so most importantly,
11

11

00:00:27.020  -->  00:00:29.230
why do we need this sample h function?
12

12

00:00:29.230  -->  00:00:30.870
Well that's because during the training
13

13

00:00:30.870  -->  00:00:34.060
we will approximate the log likelihood gradient
14

14

00:00:34.060  -->  00:00:36.550
and we will do that through Gibbs sampling
15

15

00:00:36.550  -->  00:00:38.870
because you know the trick is to use Gibbs sampling
16

16

00:00:38.870  -->  00:00:41.410
to approximate the log likelihood gradients.
17

17

00:00:41.410  -->  00:00:43.410
And to apply Gibbs sampling,
18

18

00:00:43.410  -->  00:00:46.260
we need to compute the probabilities of the hidden nodes
19

19

00:00:46.260  -->  00:00:47.550
given the visible nodes.
20

20

00:00:47.550  -->  00:00:49.330
And then once we have this probability,
21

21

00:00:49.330  -->  00:00:53.390
we can sample the activations of the hidden nodes.
22

22

00:00:53.390  -->  00:00:54.610
Alright, so let's do this.
23

23

00:00:54.610  -->  00:00:59.610
We're gonna call this function sample underscore h
24

24

00:00:59.760  -->  00:01:02.170
because it will return some samples
25

25

00:01:02.170  -->  00:01:04.930
of the different hidden nodes of our RBM.
26

26

00:01:04.930  -->  00:01:07.860
Suppose we have 100 hidden nodes in our RBM.
27

27

00:01:07.860  -->  00:01:10.380
Well, this function will sample the activations
28

28

00:01:10.380  -->  00:01:11.213
of these hidden nodes,
29

29

00:01:11.213  -->  00:01:13.860
that is for each of these 100 hidden nodes,
30

30

00:01:13.860  -->  00:01:17.610
it will activate them according to a certain probability
31

31

00:01:17.610  -->  00:01:19.870
that we will compute in this same function
32

32

00:01:19.870  -->  00:01:21.220
And for each of this hidden node,
33

33

00:01:21.220  -->  00:01:24.490
this probability is p h given v.
34

34

00:01:24.490  -->  00:01:26.750
That, is the probability that this hidden node
35

35

00:01:26.750  -->  00:01:30.580
equals one given v that is given the value of v.
36

36

00:01:30.580  -->  00:01:33.000
And that is this probability that is equal to
37

37

00:01:33.000  -->  00:01:34.223
the activation function.
38

38

00:01:35.130  -->  00:01:39.070
Alright, so sample h and this sample h function
39

39

00:01:39.070  -->  00:01:40.580
takes two arguments.
40

40

00:01:40.580  -->  00:01:43.110
The first one is of course self
41

41

00:01:43.110  -->  00:01:44.530
that corresponds to the object,
42

42

00:01:44.530  -->  00:01:46.980
because to make this sample h function,
43

43

00:01:46.980  -->  00:01:49.900
we're gonna use the variables that we defined here
44

44

00:01:49.900  -->  00:01:53.740
and to take these variables, we need to take our object,
45

45

00:01:53.740  -->  00:01:55.410
which is identified by itself.
46

46

00:01:55.410  -->  00:01:56.830
So we're taking self here
47

47

00:01:56.830  -->  00:01:59.580
to be able to access these variables.
48

48

00:01:59.580  -->  00:02:02.470
Then the second arguments will be x
49

49

00:02:02.470  -->  00:02:05.820
and x will correspond to the visible neurons, v
50

50

00:02:05.820  -->  00:02:08.213
in the probabilities, p h given v.
51

51

00:02:09.182  -->  00:02:12.490
Then let's go inside the function.
52

52

00:02:12.490  -->  00:02:13.870
So what do we have to do first?
53

53

00:02:13.870  -->  00:02:18.040
First we have to compute the probability of h given v.
54

54

00:02:18.040  -->  00:02:21.140
That is the probability that the hidden neuron equals one
55

55

00:02:21.140  -->  00:02:23.800
given the values of the visible neurons.
56

56

00:02:23.800  -->  00:02:26.740
That is actually our input vector of observations
57

57

00:02:26.740  -->  00:02:28.110
with all the ratings.
58

58

00:02:28.110  -->  00:02:30.100
So if you remember the intuition lectures
59

59

00:02:30.100  -->  00:02:32.730
or even if you look at this paper I gave you
60

60

00:02:32.730  -->  00:02:35.710
about this introduction of restricted Boltzmann machines,
61

61

00:02:35.710  -->  00:02:39.250
well, this probability of h given v
62

62

00:02:39.250  -->  00:02:42.400
is nothing else than the sigmoid activation function.
63

63

00:02:42.400  -->  00:02:45.010
But the sigmoid activation function applied to what?
64

64

00:02:45.010  -->  00:02:46.563
Well, applied to wx.
65

65

00:02:47.793  -->  00:02:51.560
That is the product of w the vector of weights times x,
66

66

00:02:51.560  -->  00:02:53.180
the vector of visible neurons
67

67

00:02:53.180  -->  00:02:56.840
plus the bias, a ,because a responds to the bias
68

68

00:02:56.840  -->  00:02:58.070
of the hidden nodes.
69

69

00:02:58.070  -->  00:03:00.430
Then b corresponds to the bias of the visible nodes.
70

70

00:03:00.430  -->  00:03:03.090
And we will use b then to define the sample function,
71

71

00:03:03.090  -->  00:03:04.950
but for the visible nodes.
72

72

00:03:04.950  -->  00:03:06.910
But right now we're dealing with the hidden nodes.
73

73

00:03:06.910  -->  00:03:09.480
And so we need to take the bias of the hidden nodes.
74

74

00:03:09.480  -->  00:03:10.410
That is, a.
75

75

00:03:10.410  -->  00:03:13.510
So first what we're gonna do is compute the product
76

76

00:03:13.510  -->  00:03:16.520
of the weights times the neurons, that is x.
77

77

00:03:16.520  -->  00:03:19.600
And so let's define wx like this.
78

78

00:03:19.600  -->  00:03:21.290
So it's a variable.
79

79

00:03:21.290  -->  00:03:23.630
And then let's use torch because you know,
80

80

00:03:23.630  -->  00:03:25.690
we're working with torch tensors.
81

81

00:03:25.690  -->  00:03:27.530
And so now since we are about to make
82

82

00:03:27.530  -->  00:03:29.430
a product of two tensors,
83

83

00:03:29.430  -->  00:03:32.080
well we have to take torch to make that product.
84

84

00:03:32.080  -->  00:03:34.250
And so, to make that product we need to use a function
85

85

00:03:34.250  -->  00:03:36.140
that is called nm.
86

86

00:03:36.140  -->  00:03:38.770
That will make the product of two tensors,
87

87

00:03:38.770  -->  00:03:39.950
two torch tensors.
88

88

00:03:39.950  -->  00:03:42.290
And inside this function, we of course input
89

89

00:03:42.290  -->  00:03:46.040
our two matrices, you know, matrix one and matrix two.
90

90

00:03:46.040  -->  00:03:50.230
So, we said that of course we want to make the product of x,
91

91

00:03:50.230  -->  00:03:53.730
the visible neuron nw, the tensor of weights.
92

92

00:03:53.730  -->  00:03:55.960
But remember w is attached to the object
93

93

00:03:55.960  -->  00:03:58.730
because it's the tensor of weights of the object
94

94

00:03:58.730  -->  00:04:01.400
that will be initialized by this in its function.
95

95

00:04:01.400  -->  00:04:05.040
So instead of taking only w, we'll take self.w
96

96

00:04:06.410  -->  00:04:09.760
which we'll input inside this nm function.
97

97

00:04:09.760  -->  00:04:13.280
Alright, so that will make the product of x divisible nodes
98

98

00:04:13.280  -->  00:04:15.300
and the tensor of weights.
99

99

00:04:15.300  -->  00:04:17.280
But then to make things mathematically correct,
100

100

00:04:17.280  -->  00:04:20.080
we actually need to take the transpose
101

101

00:04:20.080  -->  00:04:22.060
of this matrix of weights.
102

102

00:04:22.060  -->  00:04:24.307
And now this product is consistent.
103

103

00:04:24.307  -->  00:04:27.740
So now we get our product wx.
104

104

00:04:27.740  -->  00:04:30.760
Now let's compute what is going to be inside
105

105

00:04:30.760  -->  00:04:32.300
the activation function.
106

106

00:04:32.300  -->  00:04:34.520
That is the sigmoid activation function.
107

107

00:04:34.520  -->  00:04:37.660
So that's very simple, what it's going to be inside
108

108

00:04:37.660  -->  00:04:42.100
this activation function is going to be wx plus the bias.
109

109

00:04:42.100  -->  00:04:45.090
You know that's the same as for every differing model,
110

110

00:04:45.090  -->  00:04:48.110
what's inside the activation function is a linear function
111

111

00:04:48.110  -->  00:04:50.820
of the neurons where the coefficients are the weights.
112

112

00:04:50.820  -->  00:04:53.600
And then you have the bias that is a.
113

113

00:04:53.600  -->  00:04:57.060
So let's compute simply this wx plus a
114

114

00:04:57.060  -->  00:04:59.360
and we'll call this activation.
115

115

00:04:59.360  -->  00:05:01.150
You know, because that's what's inside
116

116

00:05:01.150  -->  00:05:02.720
the activation function.
117

117

00:05:02.720  -->  00:05:07.070
So activation equals wx plus.
118

118

00:05:07.070  -->  00:05:10.210
And then we want to take the bias that is a,
119

119

00:05:10.210  -->  00:05:12.630
and again, since a, is attached
120

120

00:05:12.630  -->  00:05:16.120
to the object that will be created by the RBM class,
121

121

00:05:16.120  -->  00:05:18.210
we need to take self.a
122

122

00:05:18.210  -->  00:05:21.909
to specify that a, is a variable of the object.
123

123

00:05:21.909  -->  00:05:25.230
So self.a, but then we need to do something more.
124

124

00:05:25.230  -->  00:05:26.900
Remember that each input vector
125

125

00:05:26.900  -->  00:05:28.720
will not be treated individually,
126

126

00:05:28.720  -->  00:05:30.730
but inside batches.
127

127

00:05:30.730  -->  00:05:33.030
That is this new dimension that we created here
128

128

00:05:33.030  -->  00:05:34.720
with these two ones here.
129

129

00:05:34.720  -->  00:05:37.730
And even if the batch contains one input vector
130

130

00:05:37.730  -->  00:05:39.360
or you know, one vector of bias,
131

131

00:05:39.360  -->  00:05:41.400
well that input vector is still in the batch.
132

132

00:05:41.400  -->  00:05:44.070
And in that case we call it a mini batch.
133

133

00:05:44.070  -->  00:05:46.310
And here when we add the bias,
134

134

00:05:46.310  -->  00:05:47.810
the bias of the hidden nodes,
135

135

00:05:47.810  -->  00:05:50.560
well we want to make sure that this bias
136

136

00:05:50.560  -->  00:05:53.400
is applied to each line of the mini batch.
137

137

00:05:53.400  -->  00:05:55.470
That is to each line of this dimension.
138

138

00:05:55.470  -->  00:05:58.400
And to make sure of that, we need to use a function
139

139

00:05:58.400  -->  00:06:00.880
that will again, add a new dimension
140

140

00:06:00.880  -->  00:06:02.610
for these bias that we're adding.
141

141

00:06:02.610  -->  00:06:06.950
And this function is called expand underscore as.
142

142

00:06:06.950  -->  00:06:08.970
And in parenthesis, we need to say
143

143

00:06:08.970  -->  00:06:11.677
as what we want to expand the bias
144

144

00:06:11.677  -->  00:06:14.720
and we want to expand it as wx.
145

145

00:06:14.720  -->  00:06:17.370
So inside we input wx
146

146

00:06:17.370  -->  00:06:19.110
and that way we make sure that
147

147

00:06:19.110  -->  00:06:22.620
the bias are applied to each line of the mini batch.
148

148

00:06:22.620  -->  00:06:23.540
Alright, perfect.
149

149

00:06:23.540  -->  00:06:26.850
So now we have what's inside the activation function,
150

150

00:06:26.850  -->  00:06:29.670
that is this linear combinations of the neurons,
151

151

00:06:29.670  -->  00:06:30.590
the visible neurons,
152

152

00:06:30.590  -->  00:06:33.120
because x corresponds to the visible neurons.
153

153

00:06:33.120  -->  00:06:35.180
Where the coefficients are the weights
154

154

00:06:35.180  -->  00:06:37.800
and to which is added the bias.
155

155

00:06:37.800  -->  00:06:39.550
Alright, so now that we have that,
156

156

00:06:39.550  -->  00:06:41.710
we can compute the activation function
157

157

00:06:41.710  -->  00:06:44.240
that will activate the hidden node.
158

158

00:06:44.240  -->  00:06:46.480
But remember this activation function
159

159

00:06:46.480  -->  00:06:48.310
represents a probability.
160

160

00:06:48.310  -->  00:06:50.720
It will be the probability that the hidden node
161

161

00:06:50.720  -->  00:06:53.280
will be activated according to the value
162

162

00:06:53.280  -->  00:06:55.060
of the visible node.
163

163

00:06:55.060  -->  00:06:58.470
So let's compute this activation function
164

164

00:06:58.470  -->  00:07:00.980
and so to clearly explain what's happening,
165

165

00:07:00.980  -->  00:07:04.260
we're gonna call this function p of h.
166

166

00:07:04.260  -->  00:07:09.060
So p underscore, h underscore given v.
167

167

00:07:09.060  -->  00:07:11.050
So you know that clearly shows what it is.
168

168

00:07:11.050  -->  00:07:14.200
It is a probability that the hidden node is activated
169

169

00:07:14.200  -->  00:07:17.060
given the value of the visible node.
170

170

00:07:17.060  -->  00:07:19.520
So now maybe it's time to take a little step back.
171

171

00:07:19.520  -->  00:07:22.100
Well, for example, let's say that we have a user
172

172

00:07:22.100  -->  00:07:24.490
that likes only dramatic movies.
173

173

00:07:24.490  -->  00:07:26.720
Well, if there is a hidden node
174

174

00:07:26.720  -->  00:07:28.960
that detected a specific feature corresponding
175

175

00:07:28.960  -->  00:07:31.880
to that drama genre of the movies,
176

176

00:07:31.880  -->  00:07:34.640
well for this user who has high ratings,
177

177

00:07:34.640  -->  00:07:36.680
that is one ratings for all the movies
178

178

00:07:36.680  -->  00:07:38.720
that are drama movies.
179

179

00:07:38.720  -->  00:07:41.030
Well the probability of that node
180

180

00:07:41.030  -->  00:07:44.100
that is specific to this drama, feature genre,
181

181

00:07:44.100  -->  00:07:46.390
given the visible node of that user
182

182

00:07:46.390  -->  00:07:49.120
who has all the nodes of the drama movies is equal to one.
183

183

00:07:49.120  -->  00:07:51.740
This probability will be very high
184

184

00:07:51.740  -->  00:07:54.030
because v equals one for the drama movies
185

185

00:07:54.030  -->  00:07:57.350
and h corresponds to the drama movie genre.
186

186

00:07:57.350  -->  00:07:59.943
So p h given v will be very high.
187

187

00:08:01.070  -->  00:08:03.510
Alright, so now let's dive back into the math.
188

188

00:08:03.510  -->  00:08:06.030
So p h given v, what did we say?
189

189

00:08:06.030  -->  00:08:09.260
Well that's the sigmoid of this activation.
190

190

00:08:09.260  -->  00:08:11.420
This linear combination of the neurons.
191

191

00:08:11.420  -->  00:08:13.660
And so whereas the sigmoid function was torch.
192

192

00:08:13.660  -->  00:08:18.660
Well, we need to take torch. and then the sigmoid function.
193

193

00:08:18.930  -->  00:08:20.670
Very simply, so sigmoid
194

194

00:08:20.670  -->  00:08:23.523
and inside of course we input activation.
195

195

00:08:24.480  -->  00:08:29.450
Alright, so now we have our probability p h given v
196

196

00:08:29.450  -->  00:08:30.600
we have a little warning.
197

197

00:08:30.600  -->  00:08:34.110
That's just to say that p h given v is not used yet,
198

198

00:08:34.110  -->  00:08:34.943
but that's fine.
199

199

00:08:34.943  -->  00:08:36.100
We will use it very quickly.
200

200

00:08:37.058  -->  00:08:39.890
So now next step, well actually, here we are
201

201

00:08:39.890  -->  00:08:41.040
at the final step.
202

202

00:08:41.040  -->  00:08:44.530
The final step is to return not only this probability,
203

203

00:08:44.530  -->  00:08:47.070
but of course a sample of h.
204

204

00:08:47.070  -->  00:08:49.460
That is a sample of all the hidden nodes,
205

205

00:08:49.460  -->  00:08:50.900
of all the hidden neurons,
206

206

00:08:50.900  -->  00:08:54.290
according to this probability p h given v.
207

207

00:08:54.290  -->  00:08:56.280
So let's return these two elements
208

208

00:08:56.280  -->  00:08:59.210
and then I'll explain what will be returned exactly.
209

209

00:08:59.210  -->  00:09:03.770
So return, first we return p,
210

210

00:09:03.770  -->  00:09:07.233
well, let's copy this p h given v.
211

211

00:09:08.080  -->  00:09:10.880
That's the first element we return.
212

212

00:09:10.880  -->  00:09:13.060
But then we return something else.
213

213

00:09:13.060  -->  00:09:16.020
And that's some samples of the hidden neurons
214

214

00:09:16.020  -->  00:09:18.010
according to that probability.
215

215

00:09:18.010  -->  00:09:20.890
So what does that mean and how are we going to do that?
216

216

00:09:20.890  -->  00:09:22.640
Well, right now what's important to understand
217

217

00:09:22.640  -->  00:09:25.430
is that we're making a Bernoulli RBM.
218

218

00:09:25.430  -->  00:09:27.570
We're making a Bernoulli RBM because
219

219

00:09:27.570  -->  00:09:29.600
we're just predicting a binary outcome.
220

220

00:09:29.600  -->  00:09:32.270
That is that the users like yes or no, a movie.
221

221

00:09:32.270  -->  00:09:34.660
So we predict zero or one.
222

222

00:09:34.660  -->  00:09:36.250
And so what we'll return also
223

223

00:09:36.250  -->  00:09:40.000
is some Bernoulli samples of that distribution.
224

224

00:09:40.000  -->  00:09:42.470
Of that probabilities of h given v.
225

225

00:09:42.470  -->  00:09:43.500
So what does that mean?
226

226

00:09:43.500  -->  00:09:48.430
Right now p h given v is a vector of nh elements.
227

227

00:09:48.430  -->  00:09:50.590
That is, suppose we have 100 hidden nodes,
228

228

00:09:50.590  -->  00:09:54.726
while this p h given vector is a vector of 100 elements.
229

229

00:09:54.726  -->  00:09:58.150
Each of these 100 elements corresponds to
230

230

00:09:58.150  -->  00:10:00.060
each of the 100 hidden nodes
231

231

00:10:00.060  -->  00:10:02.470
and each of these elements is the probability
232

232

00:10:02.470  -->  00:10:04.490
that the hidden node is activated.
233

233

00:10:04.490  -->  00:10:08.370
So for example, let's take the if element of this vector.
234

234

00:10:08.370  -->  00:10:11.030
Well the if element of this vector,
235

235

00:10:11.030  -->  00:10:16.030
is the probability that the if hidden node is activated.
236

236

00:10:16.100  -->  00:10:18.680
But remember that's given the values
237

237

00:10:18.680  -->  00:10:19.570
of the visible nodes.
238

238

00:10:19.570  -->  00:10:23.060
There is given the ratings of the user we're dealing with.
239

239

00:10:23.060  -->  00:10:26.490
So that's what we have in this p h given v vector.
240

240

00:10:26.490  -->  00:10:28.890
And so now the idea is to use these probabilities
241

241

00:10:28.890  -->  00:10:32.100
to sample the activation of each of this
242

242

00:10:32.100  -->  00:10:33.720
100 hundred hidden nodes.
243

243

00:10:33.720  -->  00:10:36.110
That is for each of these 100 hidden nodes,
244

244

00:10:36.110  -->  00:10:38.920
while depending on that probability that we have
245

245

00:10:38.920  -->  00:10:42.090
for these hidden nodes which is in p h given v
246

246

00:10:42.090  -->  00:10:45.200
we will activate yes or no, this hidden neuron.
247

247

00:10:45.200  -->  00:10:47.440
And so how are we going to activate it?
248

248

00:10:47.440  -->  00:10:50.610
Well, let's suppose that for a hidden neuron, i,
249

249

00:10:50.610  -->  00:10:53.050
the probability corresponding to that hidden neuron
250

250

00:10:53.050  -->  00:10:58.050
in this p h given v vector is 0.7, 70%.
251

251

00:10:58.140  -->  00:10:59.480
Well then, what happens is that
252

252

00:10:59.480  -->  00:11:02.010
we take a random number between zero and one,
253

253

00:11:02.010  -->  00:11:06.210
If this random number is below 0.7, 70%
254

254

00:11:06.210  -->  00:11:08.400
then we will activate the neuron.
255

255

00:11:08.400  -->  00:11:11.730
And if this random number is larger than 0.7
256

256

00:11:11.730  -->  00:11:14.400
then we will not activate the neuron.
257

257

00:11:14.400  -->  00:11:16.250
That's how Bernoulli sampling works.
258

258

00:11:16.250  -->  00:11:18.800
And so we do that for each of the hidden nodes
259

259

00:11:18.800  -->  00:11:20.470
of our 100 hundred nodes.
260

260

00:11:20.470  -->  00:11:24.510
And then in the end we get a vector of zeros and ones.
261

261

00:11:24.510  -->  00:11:26.380
The zeros correspond to the hidden nodes
262

262

00:11:26.380  -->  00:11:29.130
that were not activated after the sampling,
263

263

00:11:29.130  -->  00:11:30.710
and the ones correspond to the neurons
264

264

00:11:30.710  -->  00:11:33.203
that were activated by the sampling.
265

265

00:11:34.166  -->  00:11:35.680
And so now the question is,
266

266

00:11:35.680  -->  00:11:39.480
how do we return that sampling of the hidden neurons?
267

267

00:11:39.480  -->  00:11:40.330
Well, that's very simple.
268

268

00:11:40.330  -->  00:11:42.060
We have a torch function for this
269

269

00:11:42.060  -->  00:11:43.390
and this is,
270

270

00:11:43.390  -->  00:11:45.460
so I'm taking my torch library
271

271

00:11:45.460  -->  00:11:47.660
and this is the Bernoulli function.
272

272

00:11:47.660  -->  00:11:51.430
Here we go, Bernoulli and inside this Bernoulli function
273

273

00:11:51.430  -->  00:11:54.070
according to you, what do we have to input?
274

274

00:11:54.070  -->  00:11:56.150
Well, of course we have to input the distribution
275

275

00:11:56.150  -->  00:11:58.580
of which we are making that sampling.
276

276

00:11:58.580  -->  00:12:00.873
And that is p h given v.
277

277

00:12:03.170  -->  00:12:07.050
So I'm pasting p h, given v here and here we go.
278

278

00:12:07.050  -->  00:12:10.080
Our sample h function is done.
279

279

00:12:10.080  -->  00:12:10.913
Perfect.
280

280

00:12:10.913  -->  00:12:13.020
So that will return all the probabilities
281

281

00:12:13.020  -->  00:12:14.120
of the hidden neurons,
282

282

00:12:14.120  -->  00:12:16.160
given the values of the visible nodes.
283

283

00:12:16.160  -->  00:12:17.530
That is the ratings,
284

284

00:12:17.530  -->  00:12:21.750
and it will return also that sampling of the hidden neurons.
285

285

00:12:21.750  -->  00:12:24.700
So that's the first function we need for Gibbs sampling.
286

286

00:12:24.700  -->  00:12:26.030
But then we need another function,
287

287

00:12:26.030  -->  00:12:29.700
which is sample v and basically we will do the same,
288

288

00:12:29.700  -->  00:12:32.440
but this time for the visible neurons.
289

289

00:12:32.440  -->  00:12:34.120
So we'll do that in the next tutorial.
290

290

00:12:34.120  -->  00:12:35.920
And until then, enjoy deep learning.
