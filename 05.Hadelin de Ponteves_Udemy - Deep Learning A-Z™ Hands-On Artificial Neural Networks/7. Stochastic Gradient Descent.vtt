WEBVTT
1
1

00:00:01.010  -->  00:00:03.280
<v ->Hello and welcome back to the course on Deep Learning.</v>
2

2

00:00:03.280  -->  00:00:06.822
Today we're talking about stochastic gradient descent.
3

3

00:00:06.822  -->  00:00:10.120
Previously we learned about gradient descent
4

4

00:00:10.120  -->  00:00:14.230
and we found out that it is a very efficient method
5

5

00:00:14.230  -->  00:00:16.610
to solve our optimization problem
6

6

00:00:16.610  -->  00:00:19.500
where we're trying to minimize the cost function.
7

7

00:00:19.500  -->  00:00:22.220
It basically takes us from
8

8

00:00:22.220  -->  00:00:25.410
10 to the parallel 57 years
9

9

00:00:25.410  -->  00:00:28.573
to solving a problem within minutes or hours
10

10

00:00:28.573  -->  00:00:32.692
or within a day or so and it really helps speed things up
11

11

00:00:32.692  -->  00:00:35.903
because we can see which way is downhill
12

12

00:00:35.903  -->  00:00:38.150
and we can just go in that direction
13

13

00:00:38.150  -->  00:00:41.540
and take steps and get to the minimum faster.
14

14

00:00:41.540  -->  00:00:44.968
But the thing with the gradient descent
15

15

00:00:44.968  -->  00:00:49.597
is that this method requires for the cost function
16

16

00:00:49.597  -->  00:00:51.940
to be convex and as you can see here
17

17

00:00:51.940  -->  00:00:55.180
we've specifically chosen a convex cost function.
18

18

00:00:55.180  -->  00:00:59.410
Basically convex means that the function looks similar
19

19

00:00:59.410  -->  00:01:00.770
to what we are seeing now
20

20

00:01:00.770  -->  00:01:05.450
that it's just convexed into one direction
21

21

00:01:05.450  -->  00:01:08.726
and that it in essence has one global minimum
22

22

00:01:08.726  -->  00:01:11.086
and that's the one that we're going to find.
23

23

00:01:11.086  -->  00:01:14.060
But what if our function is not convexed?
24

24

00:01:14.060  -->  00:01:16.040
What if our cost function is not convexed?
25

25

00:01:16.040  -->  00:01:17.910
What if it looks something like this?
26

26

00:01:17.910  -->  00:01:19.770
Well first of all how could that happen?
27

27

00:01:19.770  -->  00:01:23.610
Well that could happen because if we first of all
28

28

00:01:23.610  -->  00:01:27.930
choose a cost function which is not the squared difference
29

29

00:01:27.930  -->  00:01:29.690
between Y-hat and Y
30

30

00:01:29.690  -->  00:01:32.610
or if we do choose the cost function
31

31

00:01:32.610  -->  00:01:35.410
which is like that but then in the multidimensional
32

32

00:01:35.410  -->  00:01:37.232
space it can actually turn into something
33

33

00:01:37.232  -->  00:01:39.660
that is not convex.
34

34

00:01:39.660  -->  00:01:41.340
And so what will happen in this case
35

35

00:01:41.340  -->  00:01:44.039
if we just try to apply our normal gradient descent method
36

36

00:01:44.039  -->  00:01:46.400
something like this could happen.
37

37

00:01:46.400  -->  00:01:49.770
We could find a local minimum of the cost function
38

38

00:01:49.770  -->  00:01:53.170
rather than the global one so this one was the best one
39

39

00:01:53.170  -->  00:01:55.390
and we found the wrong one and therefor
40

40

00:01:55.390  -->  00:01:57.740
we won't have the correct weight.
41

41

00:01:57.740  -->  00:01:59.639
We don't have an optimized neural network.
42

42

00:01:59.639  -->  00:02:02.051
We have a subpar neural network.
43

43

00:02:02.051  -->  00:02:04.580
And so what do we do in this case?
44

44

00:02:04.580  -->  00:02:09.580
Well the answer here is stochastic gradient descent
45

45

00:02:09.940  -->  00:02:12.190
and it turns out the stochastic gradient descent
46

46

00:02:12.190  -->  00:02:15.280
doesn't require for the cost function to be convex.
47

47

00:02:15.280  -->  00:02:17.290
So let's have a look at the two differences
48

48

00:02:17.290  -->  00:02:20.090
between the normal gradient descent that we talked about
49

49

00:02:20.090  -->  00:02:21.750
and the stochastic gradient descent.
50

50

00:02:21.750  -->  00:02:24.855
So normal gradient descent is when we take all of our rows
51

51

00:02:24.855  -->  00:02:27.905
we plug them into our neural network and once again
52

52

00:02:27.905  -->  00:02:31.920
here we've got the neural network copied over several times
53

53

00:02:31.920  -->  00:02:34.087
but the rows are being plugged into that same
54

54

00:02:34.087  -->  00:02:35.920
neural network every time
55

55

00:02:35.920  -->  00:02:37.120
so there's only one neural network.
56

56

00:02:37.120  -->  00:02:39.210
This is just for visualization purposes.
57

57

00:02:39.210  -->  00:02:40.620
And then once we've plugged them in
58

58

00:02:40.620  -->  00:02:42.100
we've calculated our cost function
59

59

00:02:42.100  -->  00:02:43.330
based on the formula on the right
60

60

00:02:43.330  -->  00:02:45.003
and looking at the chart at the bottom
61

61

00:02:45.003  -->  00:02:47.720
and then we adjust the weights then.
62

62

00:02:47.720  -->  00:02:49.650
This is called the gradient descent method
63

63

00:02:49.650  -->  00:02:52.027
or this also, the proper term is
64

64

00:02:52.027  -->  00:02:54.390
the batch gradient descent method.
65

65

00:02:54.390  -->  00:02:57.453
So we take the whole batch from our sample
66

66

00:02:57.453  -->  00:03:00.204
we apply it and then we run that.
67

67

00:03:00.204  -->  00:03:03.680
The stochastic gradient method is a bit different.
68

68

00:03:03.680  -->  00:03:05.860
Here we take the rows one by one
69

69

00:03:05.860  -->  00:03:07.070
so we take this row
70

70

00:03:07.070  -->  00:03:11.528
we run our neural network and then we adjust the weights.
71

71

00:03:11.528  -->  00:03:13.470
Then we move onto the second row.
72

72

00:03:13.470  -->  00:03:14.530
We take the second row
73

73

00:03:14.530  -->  00:03:15.993
we run our neural network
74

74

00:03:15.993  -->  00:03:17.770
we look at the cost function
75

75

00:03:17.770  -->  00:03:19.724
and we adjust the weights again.
76

76

00:03:19.724  -->  00:03:20.557
(gently chiming bell)
77

77

00:03:20.557  -->  00:03:21.874
then we take another row, take row three
78

78

00:03:21.874  -->  00:03:23.680
we run our neural network
79

79

00:03:23.680  -->  00:03:24.650
we look at the cost function
80

80

00:03:24.650  -->  00:03:25.483
we adjust the weights.
81

81

00:03:25.483  -->  00:03:27.591
So basically we're looking at
82

82

00:03:27.591  -->  00:03:31.180
we're adjusting the weights after every single row
83

83

00:03:31.180  -->  00:03:32.970
rather than doing everything together
84

84

00:03:32.970  -->  00:03:34.089
and then adjusting weights.
85

85

00:03:34.089  -->  00:03:36.080
Two different approaches
86

86

00:03:36.080  -->  00:03:39.630
and now we're going to just compare the two side-by-side.
87

87

00:03:39.630  -->  00:03:40.520
So here they are
88

88

00:03:40.520  -->  00:03:42.390
this is to visually remember them.
89

89

00:03:42.390  -->  00:03:44.590
So you've got the batch gradient descent
90

90

00:03:44.590  -->  00:03:47.690
where you are adjusting the weights
91

91

00:03:47.690  -->  00:03:49.030
after you've run them
92

92

00:03:49.030  -->  00:03:52.890
after you've run all of the rows in your neural network
93

93

00:03:52.890  -->  00:03:54.920
and then basically you adjust the weights
94

94

00:03:54.920  -->  00:03:55.880
and then you run the whole thing again
95

95

00:03:55.880  -->  00:03:57.470
iteration, iteration, iteration.
96

96

00:03:57.470  -->  00:03:59.390
In the stochastic gradient descent method
97

97

00:03:59.390  -->  00:04:01.460
you run one row at a time
98

98

00:04:01.460  -->  00:04:03.782
and you adjust the weights.
99

99

00:04:03.782  -->  00:04:04.615
You adjust the weights.
100

100

00:04:04.615  -->  00:04:06.650
You adjust the weights and then you do everything again
101

101

00:04:06.650  -->  00:04:09.350
and again and that is called
102

102

00:04:09.350  -->  00:04:10.783
the stochastic gradient descent method.
103

103

00:04:10.783  -->  00:04:12.382
The main two differences are
104

104

00:04:12.382  -->  00:04:15.750
that this stochastic gradient descent method
105

105

00:04:15.750  -->  00:04:19.180
helps you avoid the problem
106

106

00:04:19.180  -->  00:04:22.486
where you find those local extraneural
107

107

00:04:22.486  -->  00:04:23.654
or local minimums
108

108

00:04:23.654  -->  00:04:28.577
rather than the overall global minimum.
109

109

00:04:28.577  -->  00:04:32.900
And the reason for that in simple terms is that the SGD
110

110

00:04:32.900  -->  00:04:34.712
or the stochastic gradient descent method
111

111

00:04:34.712  -->  00:04:38.100
has much higher fluctuation because it can afford them.
112

112

00:04:38.100  -->  00:04:41.840
It's doing one iteration or one row at at time
113

113

00:04:41.840  -->  00:04:43.360
and therefor the fluctuations are much higher
114

114

00:04:43.360  -->  00:04:46.976
and it is more likely to find the global minimum
115

115

00:04:46.976  -->  00:04:49.330
rather than just the local minimum
116

116

00:04:49.330  -->  00:04:50.960
and the other thing
117

117

00:04:50.960  -->  00:04:52.870
about the stochastic gradient descent method
118

118

00:04:52.870  -->  00:04:56.410
compares to the batch gradient is that it's faster.
119

119

00:04:56.410  -->  00:04:58.080
Like the first impression that you might have
120

120

00:04:58.080  -->  00:05:00.760
is because it's doing every single row one at a time
121

121

00:05:00.760  -->  00:05:03.640
it is slower but actually in fact it is faster
122

122

00:05:03.640  -->  00:05:08.139
because it doesn't have to load up all the data into memory
123

123

00:05:08.139  -->  00:05:12.600
and run and wait until all those rows are run altogether.
124

124

00:05:12.600  -->  00:05:14.060
You can just run them one by one
125

125

00:05:14.060  -->  00:05:15.460
so it's a much lighter algorithm.
126

126

00:05:15.460  -->  00:05:16.750
It's much faster in that sense.
127

127

00:05:16.750  -->  00:05:19.826
So it has way more
128

128

00:05:19.826  -->  00:05:22.730
in those senses you have more advantages
129

129

00:05:22.730  -->  00:05:24.867
over the batch gradient descent method.
130

130

00:05:24.867  -->  00:05:26.506
The main advantage of
131

131

00:05:26.506  -->  00:05:29.070
or the main kind of like pro
132

132

00:05:29.070  -->  00:05:30.550
for the batch gradient descent method
133

133

00:05:30.550  -->  00:05:32.420
is that it is a deterministic algorithm
134

134

00:05:32.420  -->  00:05:34.871
rather than a stochastic gradient descent
135

135

00:05:34.871  -->  00:05:36.880
being a stochastic algorithm
136

136

00:05:36.880  -->  00:05:37.713
meaning it's random
137

137

00:05:37.713  -->  00:05:40.368
and with the batch gradient descent method
138

138

00:05:40.368  -->  00:05:42.964
as long as you have the same starting weights
139

139

00:05:42.964  -->  00:05:46.016
for your neural network every time you run
140

140

00:05:46.016  -->  00:05:47.830
the batch gradient descent method
141

141

00:05:47.830  -->  00:05:51.330
you will get the same iterations
142

142

00:05:51.330  -->  00:05:55.220
the same results for the way your weights are being updated.
143

143

00:05:55.220  -->  00:05:57.895
First for the stochastic gradient descent method
144

144

00:05:57.895  -->  00:06:01.080
you won't get that because it is a stochastic method.
145

145

00:06:01.080  -->  00:06:03.244
You are picking your rows possible at random
146

146

00:06:03.244  -->  00:06:05.835
and you are updating your neural network
147

147

00:06:05.835  -->  00:06:08.000
in a stochastic manner and
148

148

00:06:08.000  -->  00:06:11.300
therefor you're just going to every single time
149

149

00:06:11.300  -->  00:06:12.950
you run the stochastic gradient descent method
150

150

00:06:12.950  -->  00:06:14.808
even if you have the same weights at the start
151

151

00:06:14.808  -->  00:06:17.281
you're going to have a different process
152

152

00:06:17.281  -->  00:06:20.670
a different iterations to get there.
153

153

00:06:20.670  -->  00:06:25.670
So that's in a nutshell what stochastic gradient descent is.
154

154

00:06:26.030  -->  00:06:28.020
Also there is a method in between the two
155

155

00:06:28.020  -->  00:06:30.140
called the mini batch gradient descent method
156

156

00:06:30.140  -->  00:06:33.704
where you combine the two and you basically run
157

157

00:06:33.704  -->  00:06:37.447
rather than running a whole batch or run one at a time
158

158

00:06:37.447  -->  00:06:40.136
you run batches of rows maybe 5, 10, 100
159

159

00:06:40.136  -->  00:06:42.680
however many rows you decided to set.
160

160

00:06:42.680  -->  00:06:45.160
You run that number of rows at a time
161

161

00:06:45.160  -->  00:06:46.486
then you update your weights and
162

162

00:06:46.486  -->  00:06:47.561
you update your weights and so on.
163

163

00:06:47.561  -->  00:06:50.460
That's called the mini batch gradient descent method.
164

164

00:06:50.460  -->  00:06:52.930
If you'd like to learn more about gradient descent
165

165

00:06:52.930  -->  00:06:56.234
there's a great article which you can have a look at.
166

166

00:06:56.234  -->  00:07:00.310
It's called A Neural Network in 13 lines of Python
167

167

00:07:00.310  -->  00:07:03.200
Part two Gradient Descent by Andrew Trask
168

168

00:07:04.450  -->  00:07:05.710
and the link's below
169

169

00:07:05.710  -->  00:07:08.383
it's on GitHub 2015 article.
170

170

00:07:09.480  -->  00:07:12.520
Very well written in very simple terms.
171

171

00:07:12.520  -->  00:07:16.530
It's got some interesting philosophical
172

172

00:07:16.530  -->  00:07:18.292
or just interesting thoughts on
173

173

00:07:18.292  -->  00:07:21.420
how to apply gradient descent.
174

174

00:07:21.420  -->  00:07:24.090
What are the advantages and disadvantages
175

175

00:07:24.090  -->  00:07:27.565
and how to do things in certain situations.
176

176

00:07:27.565  -->  00:07:30.106
So it's got some very cool tips and tricks and hacks.
177

177

00:07:30.106  -->  00:07:32.999
Very easy read so definitely check that out
178

178

00:07:32.999  -->  00:07:36.556
and another one a bit more heavier read
179

179

00:07:36.556  -->  00:07:39.310
for those of you who are into mathematics
180

180

00:07:39.310  -->  00:07:41.480
who want to get to the bottom of the mathematics.
181

181

00:07:41.480  -->  00:07:45.190
Why gradient descent that specific?
182

182

00:07:45.190  -->  00:07:47.200
What are the formulas that are driving gradient descent?
183

183

00:07:47.200  -->  00:07:48.130
How is it calculated?
184

184

00:07:48.130  -->  00:07:49.160
And so on.
185

185

00:07:49.160  -->  00:07:51.580
Check out the article or actually the book
186

186

00:07:51.580  -->  00:07:52.500
it's a free online book
187

187

00:07:52.500  -->  00:07:54.410
called Neural Networks and Deep Learning
188

188

00:07:54.410  -->  00:07:57.050
by Michael Nielsen, 2015 book.
189

189

00:07:57.050  -->  00:07:58.126
It's just basically
190

190

00:07:58.126  -->  00:08:02.113
it's all online. You can go ahead and check it out there.
191

191

00:08:03.318  -->  00:08:05.706
Again, very soft introduction to the mathematics
192

192

00:08:05.706  -->  00:08:10.706
but then the mathematics are pretty heavy as you go along
193

193

00:08:11.480  -->  00:08:14.616
as you read through the article but at the same time
194

194

00:08:14.616  -->  00:08:17.280
it gets you into that mood.
195

195

00:08:17.280  -->  00:08:20.070
I think it has like a warmup chapter
196

196

00:08:20.070  -->  00:08:21.730
where you first warm up with the math
197

197

00:08:21.730  -->  00:08:22.563
and then you jump in.
198

198

00:08:22.563  -->  00:08:24.140
If you're interested in math then
199

199

00:08:24.140  -->  00:08:25.478
this is the article to go to.
200

200

00:08:25.478  -->  00:08:27.140
And there you go.
201

201

00:08:27.140  -->  00:08:28.141
So that's in a nutshell
202

202

00:08:28.141  -->  00:08:30.707
the difference between gradient descent
203

203

00:08:30.707  -->  00:08:31.540
and stochastic gradient descent
204

204

00:08:35.356  -->  00:08:36.310
and how the two work
205

205

00:08:36.310  -->  00:08:39.356
and on that note we're going to wrap up today's tutorial.
206

206

00:08:39.356  -->  00:08:41.910
I look forward to seeing you on the next one
207

207

00:08:41.910  -->  00:08:44.173
and until then enjoy Deep Learning.
