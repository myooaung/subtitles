1
1

00:00:00,700  -->  00:00:01,533 line:15% 
<v Narrator>Hello and welcome back</v>
2

2

00:00:01,533  -->  00:00:03,610 line:15% 
to the course on Deep Learning.
3

3

00:00:03,610  -->  00:00:06,890 line:15% 
Now that we know a bit more about LSTMs and how they work
4

4

00:00:06,890  -->  00:00:08,520 line:15% 
and what their architecture is like,
5

5

00:00:08,520  -->  00:00:12,080 line:15% 
today we're going to look at some practical applications
6

6

00:00:12,080  -->  00:00:16,840 line:15% 
or specifically we're gonna look at how LSTMs work
7

7

00:00:16,840  -->  00:00:18,840 line:15% 
inside practical applications.
8

8

00:00:18,840  -->  00:00:21,090 line:15% 
And it's gonna be quite interesting and at the same time
9

9

00:00:21,090  -->  00:00:23,905 line:15% 
a bit magical I'd say so let's get started.
10

10

00:00:23,905  -->  00:00:28,905
Here we've got LSTM architecture.
11

11

00:00:29,530  -->  00:00:31,920
And we're going to be, to start off we're gonna be looking
12

12

00:00:31,920  -->  00:00:36,433
at the standard function and how it fires up.
13

13

00:00:37,400  -->  00:00:41,850
So according to, the images that we're gonna be using
14

14

00:00:41,850  -->  00:00:45,750
are all from Andrej Karpathy's blog,
15

15

00:00:45,750  -->  00:00:49,690
so thank you very much to Andrej for doing all of this
16

16

00:00:49,690  -->  00:00:50,550
amazing research.
17

17

00:00:50,550  -->  00:00:53,740
So you can see here, it's just incredible.
18

18

00:00:53,740  -->  00:00:55,620
They've got some examples where they've created
19

19

00:00:55,620  -->  00:01:00,270
some bogus algebra using trained up LSTMs.
20

20

00:01:00,270  -->  00:01:03,464
And then lots and lots of stuff.
21

21

00:01:03,464  -->  00:01:06,440
These are the images we're gonna be looking at just now.
22

22

00:01:06,440  -->  00:01:08,200
These ones as well.
23

23

00:01:08,200  -->  00:01:10,510
And there's lots of information here.
24

24

00:01:10,510  -->  00:01:12,320
Lots of comments so definitely check it out.
25

25

00:01:12,320  -->  00:01:16,422
We'll link to these at the end of the tutorial.
26

26

00:01:16,422  -->  00:01:18,460
This blog is called The Unreasonable Effectiveness
27

27

00:01:18,460  -->  00:01:20,380
of Recurrent Neural Networks.
28

28

00:01:20,380  -->  00:01:25,140
And the paper that Andrej published along with that,
29

29

00:01:25,140  -->  00:01:26,990
we'll also link to that paper as well.
30

30

00:01:26,990  -->  00:01:29,338
So basically we're looking at the standard function
31

31

00:01:29,338  -->  00:01:30,261
to start off with.
32

32

00:01:30,261  -->  00:01:35,200
According to the paper, minus one means, is gonna be red,
33

33

00:01:35,200  -->  00:01:37,070
plus one is gonna be blue.
34

34

00:01:37,070  -->  00:01:38,120
So let's get started.
35

35

00:01:39,200  -->  00:01:42,770
All right, so here's some text, which is given to an RNN,
36

36

00:01:42,770  -->  00:01:47,770
which learned to read text and kind of create text
37

37

00:01:48,010  -->  00:01:50,070
and predict what text is coming.
38

38

00:01:50,070  -->  00:01:55,070
And here, this is a snippet from the War and Peace,
39

39

00:01:56,830  -->  00:02:01,150
the huge novel by Tolstoy,
40

40

00:02:01,150  -->  00:02:05,403
and so here you can see that this neuron is activating,
41

41

00:02:05,403  -->  00:02:08,170
well it is sensitive to as it says here,
42

42

00:02:08,170  -->  00:02:09,330
sensitive to position in line.
43

43

00:02:09,330  -->  00:02:11,740
So you can see that when you get towards the end of the line
44

44

00:02:11,740  -->  00:02:12,573
it's activating.
45

45

00:02:12,573  -->  00:02:14,500
And how does it know when it's the end of the line?
46

46

00:02:14,500  -->  00:02:18,400
Well, in this novel,
47

47

00:02:18,400  -->  00:02:22,910
you have about 80 symbols per line
48

48

00:02:22,910  -->  00:02:25,895
approximately, so it's counting how many symbols have passed
49

49

00:02:25,895  -->  00:02:28,500
and that way it's trying to predict when the new line
50

50

00:02:28,500  -->  00:02:29,333
character is coming up.
51

51

00:02:29,333  -->  00:02:32,120
Because the new line, new line is just a character.
52

52

00:02:32,120  -->  00:02:34,290
It's an invisible character and it's trying to predict
53

53

00:02:34,290  -->  00:02:36,750
where that character's gonna appear.
54

54

00:02:36,750  -->  00:02:39,730
Then you've got a cell that turn on inside quotes.
55

55

00:02:39,730  -->  00:02:40,880
Well this says inside quotes.
56

56

00:02:40,880  -->  00:02:42,584
I think it's actually outside quotes,
57

57

00:02:42,584  -->  00:02:45,631
because here you see it says, you mean to imply that I have
58

58

00:02:45,631  -->  00:02:49,420
noting to eat out of, on the contrary, I can supply.
59

59

00:02:49,420  -->  00:02:52,418
So basically, somebody's talking and they say warmly replied
60

60

00:02:52,418  -->  00:02:57,418
Chichagov, haven't read that in a long time,
61

61

00:02:59,436  -->  00:03:02,990
who tried to by, who tried by every word.
62

62

00:03:02,990  -->  00:03:05,380
So basically, this is the inside the quotes,
63

63

00:03:05,380  -->  00:03:06,330
this is outside the quotes.
64

64

00:03:06,330  -->  00:03:09,550
I'm not sure if this is correct, but either case,
65

65

00:03:09,550  -->  00:03:12,110
one way or another, it's activating either inside the quotes
66

66

00:03:12,110  -->  00:03:14,320
or outside the quotes, and you can see, so this one
67

67

00:03:14,320  -->  00:03:15,973
is keeping track of what's happening.
68

68

00:03:15,973  -->  00:03:18,500
So very similar to what we discusses previously,
69

69

00:03:18,500  -->  00:03:22,070
where we were keeping track of the subject,
70

70

00:03:22,070  -->  00:03:24,290
when that would help us understand.
71

71

00:03:24,290  -->  00:03:28,680
And if the subject is male or female, we would be able
72

72

00:03:28,680  -->  00:03:31,510
to understand things like if it's a singular or plural,
73

73

00:03:31,510  -->  00:03:33,650
that would effect our verbs, and our translation,
74

74

00:03:33,650  -->  00:03:34,483
similar things.
75

75

00:03:34,483  -->  00:03:37,160
So it's important to know if you're inside or outside quotes
76

76

00:03:37,160  -->  00:03:40,480
because that effects the rest of the text.
77

77

00:03:40,480  -->  00:03:43,054
And for instance, the easiest way you can think of
78

78

00:03:43,054  -->  00:03:45,880
that affects text that if you're inside the quotes
79

79

00:03:45,880  -->  00:03:48,070
then there has to be another quote to close the quote.
80

80

00:03:48,070  -->  00:03:51,651
So you're anticipating another quote, on something.
81

81

00:03:51,651  -->  00:03:56,651
Here's another one where the input
82

82

00:03:57,420  -->  00:04:01,630
was the code of the Linux operating system.
83

83

00:04:01,630  -->  00:04:06,630
And here, you can see that a cell activates
84

84

00:04:06,930  -->  00:04:07,940
inside if statements.
85

85

00:04:07,940  -->  00:04:10,850
So it's completely dormant everywhere else.
86

86

00:04:10,850  -->  00:04:13,180
But as soon as you have an if statement, it activates.
87

87

00:04:13,180  -->  00:04:14,340
If statement, activates.
88

88

00:04:14,340  -->  00:04:15,890
If statement, activates.
89

89

00:04:15,890  -->  00:04:18,350
So, and it is active.
90

90

00:04:18,350  -->  00:04:20,240
You can see it stops being active over here
91

91

00:04:20,240  -->  00:04:25,240
at the next, at the actual body of the if statement.
92

92

00:04:25,930  -->  00:04:30,030
So it's only active for the main part, or for the condition
93

93

00:04:30,030  -->  00:04:31,390
of the if statement.
94

94

00:04:31,390  -->  00:04:35,790
You know that can be important because you are anticipating
95

95

00:04:35,790  -->  00:04:38,630
that condition's going to close if a bracket.
96

96

00:04:38,630  -->  00:04:40,660
And then it's gonna be a bracket current embrace
97

97

00:04:40,660  -->  00:04:43,647
to open up the body of the if statement.
98

98

00:04:43,647  -->  00:04:45,030
That's pretty cool.
99

99

00:04:45,030  -->  00:04:50,030
And here, you've got another one where this cell
100

100

00:04:50,460  -->  00:04:52,670
is sensitive to how deep you are inside
101

101

00:04:52,670  -->  00:04:54,203
of a nested expression.
102

102

00:04:55,070  -->  00:04:58,610
So as you go deeper and the expression gets more
103

103

00:04:58,610  -->  00:05:01,530
and more nested, this cell keeps track of that.
104

104

00:05:01,530  -->  00:05:04,570
So it's using this memory to keep track of.
105

105

00:05:04,570  -->  00:05:08,250
So it's very important to remember that none of this
106

106

00:05:08,250  -->  00:05:11,650
is actually hard coded into the neural network.
107

107

00:05:11,650  -->  00:05:14,340
All of this is learned by network itself.
108

108

00:05:14,340  -->  00:05:17,040
Through thousands and thousands and thousands of iterations.
109

109

00:05:17,040  -->  00:05:22,040
It learns that okay, I have this many hidden states,
110

110

00:05:22,460  -->  00:05:25,840
I have, and out of them, I need to identify
111

111

00:05:25,840  -->  00:05:27,980
what's important in the text to keep track of.
112

112

00:05:27,980  -->  00:05:30,370
And it identifies for instance, in this case,
113

113

00:05:30,370  -->  00:05:35,370
that being, understanding how deep you are
114

114

00:05:35,390  -->  00:05:38,350
inside a nested statement is important and therefore,
115

115

00:05:38,350  -->  00:05:42,470
it assigns a, one of its hidden states
116

116

00:05:42,470  -->  00:05:43,650
to keep track of that.
117

117

00:05:43,650  -->  00:05:46,240
So it has these resources, hidden states,
118

118

00:05:46,240  -->  00:05:51,240
or the actual memory cells, and it assigns them
119

119

00:05:52,090  -->  00:05:54,630
to keep track of certain things based on what it thinks
120

120

00:05:54,630  -->  00:05:55,463
is important.
121

121

00:05:55,463  -->  00:05:59,060
So it's really evolving like that on its own
122

122

00:05:59,060  -->  00:06:00,720
and deciding what's important and what's not,
123

123

00:06:00,720  -->  00:06:03,890
and how to allocate its resources to best complete the task.
124

124

00:06:03,890  -->  00:06:06,600
I think that's very fascinating.
125

125

00:06:06,600  -->  00:06:09,150
And then here's an example of a cell
126

126

00:06:09,150  -->  00:06:11,680
that you can't really understand what it's doing.
127

127

00:06:11,680  -->  00:06:16,200
And according to Andrej Karpathy, about 95% of the cells
128

128

00:06:16,200  -->  00:06:17,033
are like this.
129

129

00:06:17,033  -->  00:06:21,380
They're doing something, but it's just not obvious to us
130

130

00:06:21,380  -->  00:06:22,213
what is happening.
131

131

00:06:22,213  -->  00:06:26,850
And it's like that example of CNNs where the filters
132

132

00:06:26,850  -->  00:06:28,557
or the features we're looking out for,
133

133

00:06:28,557  -->  00:06:31,340
by the time they get to the last layer,
134

134

00:06:31,340  -->  00:06:33,580
they're completely unrecognizable to the human eye.
135

135

00:06:33,580  -->  00:06:34,960
But they make sense to the machines.
136

136

00:06:34,960  -->  00:06:35,921
Same thing here.
137

137

00:06:35,921  -->  00:06:39,370
Most the time, 95% of the time, you can't really tell
138

138

00:06:39,370  -->  00:06:40,203
what's going on.
139

139

00:06:40,203  -->  00:06:41,036
But those five percent of the time,
140

140

00:06:41,036  -->  00:06:42,700
those were the examples we looked at,
141

141

00:06:44,155  -->  00:06:46,530
and they should be helpful to better understand
142

142

00:06:46,530  -->  00:06:50,670
what is kind of going on in the neural network
143

143

00:06:50,670  -->  00:06:53,663
when it's processing text.
144

144

00:06:54,800  -->  00:06:59,100
So again, now we're back at our architecture,
145

145

00:06:59,100  -->  00:07:00,386
and now what we're gonna be looking at
146

146

00:07:00,386  -->  00:07:04,730
is we're going to be looking at the actual output.
147

147

00:07:04,730  -->  00:07:06,660
So we're gonna be looking at this value.
148

148

00:07:06,660  -->  00:07:08,270
So after it's going through the tangent,
149

149

00:07:08,270  -->  00:07:12,650
the valve or the output gate and now were gonna be looking
150

150

00:07:12,650  -->  00:07:14,438
at what's being produced over here.
151

151

00:07:14,438  -->  00:07:18,503
So, here's an example again, from Karpathy's,
152

152

00:07:18,503  -->  00:07:21,920
Andrej Karpathy's blog, and we're looking at
153

153

00:07:21,920  -->  00:07:25,070
a neural network, that is reading,
154

154

00:07:25,070  -->  00:07:27,870
so this is a bit more detailed.
155

155

00:07:27,870  -->  00:07:31,495
So here, it's not just showing us if it's active or not.
156

156

00:07:31,495  -->  00:07:35,270
You can see that you've got, at the top, if it's active
157

157

00:07:35,270  -->  00:07:40,130
or not every first line, and then in the other five lines
158

158

00:07:40,130  -->  00:07:43,130
it is saying what's the neural network is predicting,
159

159

00:07:43,130  -->  00:07:45,430
that's going to happen next.
160

160

00:07:45,430  -->  00:07:46,970
What letter is going to come next.
161

161

00:07:46,970  -->  00:07:48,400
What symbol is going to come next.
162

162

00:07:48,400  -->  00:07:50,790
So, you got the combination of the two here.
163

163

00:07:50,790  -->  00:07:53,040
And just by looking at this, what do you think
164

164

00:07:53,040  -->  00:07:53,873
it's predicting.
165

165

00:07:53,873  -->  00:07:58,430
So I'm gonna, I'm going to outline the colors.
166

166

00:07:58,430  -->  00:08:02,000
Green means, for the top line, green means active
167

167

00:08:02,000  -->  00:08:05,110
and blue means not active, and red means a very likely
168

168

00:08:05,110  -->  00:08:08,140
prediction, and light red means unlikely prediction.
169

169

00:08:08,140  -->  00:08:10,740
So, let's talk about the top line.
170

170

00:08:10,740  -->  00:08:13,930
What do you think this specific hidden state
171

171

00:08:13,930  -->  00:08:16,030
in the neural network is looking out for?
172

172

00:08:16,030  -->  00:08:17,600
When do you think it's being activated?
173

173

00:08:17,600  -->  00:08:19,220
Well I guess this one is pretty obvious.
174

174

00:08:19,220  -->  00:08:21,830
It's activating inside URLs.
175

175

00:08:21,830  -->  00:08:26,830
So here you can see, that inside w w w, the hidden state
176

176

00:08:27,170  -->  00:08:29,791
is being activated just like we saw in the previous
177

177

00:08:29,791  -->  00:08:34,080
examples of War and Peace and Linux kernel.
178

178

00:08:34,080  -->  00:08:36,760
Here you can see it's being, it's activated
179

179

00:08:36,760  -->  00:08:40,900
inside URLs, but now we have some additional stuff
180

180

00:08:40,900  -->  00:08:41,733
to look at.
181

181

00:08:41,733  -->  00:08:43,930
Now we can see what it's actually predicting
182

182

00:08:43,930  -->  00:08:45,100
that's going to be the next character.
183

183

00:08:45,100  -->  00:08:48,681
So for instance, under this W, you can see that
184

184

00:08:48,681  -->  00:08:51,650
it's predicting that the next character's going to be a W
185

185

00:08:51,650  -->  00:08:54,550
with the highest probability, and it's correct.
186

186

00:08:54,550  -->  00:08:55,540
It is a W.
187

187

00:08:55,540  -->  00:08:57,670
Then under this W, you can see that it's predicting
188

188

00:08:57,670  -->  00:08:59,740
another W, and it is correct.
189

189

00:08:59,740  -->  00:09:03,050
And under this W, this is what the whole neural network
190

190

00:09:03,050  -->  00:09:08,050
is predicting, under this W, what you are seeing
191

191

00:09:09,040  -->  00:09:11,940
is a dot, and that is correct, it is predicting a dot.
192

192

00:09:11,940  -->  00:09:16,940
So that's how its gone up to here, and then after this dot,
193

193

00:09:17,835  -->  00:09:20,650
it thinks the next letter will be a B,
194

194

00:09:20,650  -->  00:09:21,483
but it's actually a Y.
195

195

00:09:21,483  -->  00:09:23,150
But now knowing that it's a Y,
196

196

00:09:23,150  -->  00:09:24,280
it thinks there's gonna be an A.
197

197

00:09:24,280  -->  00:09:26,137
Now knowing it's an A, it thinks it's going to be an N.
198

198

00:09:26,137  -->  00:09:29,090
And you can see that these are less,
199

199

00:09:29,090  -->  00:09:32,100
these are not as red as these, because it's not sure
200

200

00:09:32,100  -->  00:09:33,490
about these predictions, and fair enough,
201

201

00:09:33,490  -->  00:09:35,160
it could be absolutely any website.
202

202

00:09:35,160  -->  00:09:35,993
Why would it be a B?
203

203

00:09:35,993  -->  00:09:38,730
It could be an N, you can't tell.
204

204

00:09:38,730  -->  00:09:41,390
Maybe it could, based on the context, but still
205

205

00:09:41,390  -->  00:09:42,910
it's very hard.
206

206

00:09:42,910  -->  00:09:47,910
But then when it gets to Y net new, N E W, it predicts an S
207

207

00:09:48,033  -->  00:09:50,920
with a very high likelihood, and it is indeed an S
208

208

00:09:50,920  -->  00:09:55,170
because it kind of knows that the word after you have new
209

209

00:09:55,170  -->  00:09:56,760
very likely you'll have an S.
210

210

00:09:56,760  -->  00:09:59,280
Or based on everything it's learned before,
211

211

00:09:59,280  -->  00:10:01,089
that in this particular type of text,
212

212

00:10:01,089  -->  00:10:04,810
it's, news is mentioned quite often,
213

213

00:10:04,810  -->  00:10:08,703
because as far as I understand, this is Wikipedia text.
214

214

00:10:09,610  -->  00:10:11,729
And then you have the dot, and after dot,
215

215

00:10:11,729  -->  00:10:14,220
it predicts a C, it is a C.
216

216

00:10:14,220  -->  00:10:17,020
After C it predicts an O, it's an O.
217

217

00:10:17,020  -->  00:10:19,510
After O it predicts an M, it is am M,
218

218

00:10:19,510  -->  00:10:21,280
and then a slash and it is a slash.
219

219

00:10:21,280  -->  00:10:22,113
So beautiful.
220

220

00:10:22,113  -->  00:10:24,695
So then it's done its job, and it's not active anymore.
221

221

00:10:24,695  -->  00:10:29,695
And then this is what, but the neuron that we're looking at
222

222

00:10:30,030  -->  00:10:30,880
is not active anymore.
223

223

00:10:30,880  -->  00:10:32,850
But the neural network is still predicting things.
224

224

00:10:32,850  -->  00:10:36,180
So here you can see E and the N, so it's incorrect.
225

225

00:10:36,180  -->  00:10:38,730
And then after the N, oh no, after the E it's incorrect.
226

226

00:10:38,730  -->  00:10:42,150
After N, so after you have E N, it predicts a G, it's a G.
227

227

00:10:42,150  -->  00:10:43,670
And then it becomes more confident
228

228

00:10:43,670  -->  00:10:45,270
that it's the word English.
229

229

00:10:45,270  -->  00:10:46,890
Language you can see that the predictions
230

230

00:10:46,890  -->  00:10:47,854
become more confident.
231

231

00:10:47,854  -->  00:10:50,810
After the L it didn't predict correctly,
232

232

00:10:50,810  -->  00:10:54,550
but after the L A, it's really predicting N G and so on.
233

233

00:10:54,550  -->  00:10:56,300
So it can actually predict the whole word,
234

234

00:10:56,300  -->  00:10:58,260
based on just a few first letters.
235

235

00:10:58,260  -->  00:10:59,945
Websites, and so on and so on.
236

236

00:10:59,945  -->  00:11:02,410
The actual neuron is still dormant, still dormant.
237

237

00:11:02,410  -->  00:11:06,120
And then here, you've got again, off we go.
238

238

00:11:06,120  -->  00:11:09,890
W W W it's predicting and this one is getting active.
239

239

00:11:09,890  -->  00:11:13,130
This one is pretty interesting, because it goes C O
240

240

00:11:13,130  -->  00:11:13,963
and then predicts an M,
241

241

00:11:13,963  -->  00:11:16,030
but it's not an M, it's a dot.
242

242

00:11:16,030  -->  00:11:20,010
And it's like, the neural network, or this,
243

243

00:11:20,010  -->  00:11:23,520
yeah this is a neural network is probably a bit upset
244

244

00:11:23,520  -->  00:11:26,350
at this stage, thinking whoa, where's my M?
245

245

00:11:26,350  -->  00:11:29,190
And then it takes another shot, it's says okay,
246

246

00:11:29,190  -->  00:11:32,380
so probably U because dot co, dot U K.
247

247

00:11:32,380  -->  00:11:35,113
That's the UK websites.
248

248

00:11:36,120  -->  00:11:40,513
But here, instead of a U, it's got an I and an L
249

249

00:11:40,513  -->  00:11:45,513
for Israel dot C O, dot I L, and therefore
250

250

00:11:45,530  -->  00:11:47,870
it didn't guess this time.
251

251

00:11:47,870  -->  00:11:49,977
But it's not even trying to guess.
252

252

00:11:49,977  -->  00:11:51,590
You can see it's not even trying to guess an I
253

253

00:11:51,590  -->  00:11:55,040
because from what's it's learned, dot C O, dot U K,
254

254

00:11:55,040  -->  00:11:58,992
are much more likely to come up than dot C O dot I L.
255

255

00:11:58,992  -->  00:12:01,650
And we're not even looking at these other ones here,
256

256

00:12:01,650  -->  00:12:03,100
which you could look at them.
257

257

00:12:04,009  -->  00:12:07,620
They are the second best guess, the third best guess,
258

258

00:12:07,620  -->  00:12:09,620
fourth and fifth, but you can see that they're not
259

259

00:12:09,620  -->  00:12:14,620
that red and it will always put it's highest likeliest
260

260

00:12:16,710  -->  00:12:19,463
guess at the top on the second line overall.
261

261

00:12:20,420  -->  00:12:21,253
So there you go.
262

262

00:12:21,253  -->  00:12:23,850
This is how to look at these pictures that Andrej
263

263

00:12:23,850  -->  00:12:25,550
has created.
264

264

00:12:25,550  -->  00:12:29,393
And for more, check out his blog, karpathy.github.io.
265

265

00:12:29,393  -->  00:12:31,550
There's a couple more of these examples there
266

266

00:12:31,550  -->  00:12:33,676
and more of the previous examples that we looked at.
267

267

00:12:33,676  -->  00:12:38,350
And yeah, so hopefully this is helpful to show you
268

268

00:12:38,350  -->  00:12:42,770
what's going on inside the neural network
269

269

00:12:42,770  -->  00:12:46,948
when it's thinking and when it's processing information.
270

270

00:12:46,948  -->  00:12:49,786
In terms of references and additional reading,
271

271

00:12:49,786  -->  00:12:53,860
we've got the Andrej Karpathy blog and we'll link to it
272

272

00:12:53,860  -->  00:12:56,566
in the resources for this course.
273

273

00:12:56,566  -->  00:13:01,566
This is the URL, otherwise, and also,
274

274

00:13:01,800  -->  00:13:05,937
we've got Andrej Karpathy and others research paper
275

275

00:13:05,937  -->  00:13:09,030
which was published in 2015.
276

276

00:13:09,030  -->  00:13:11,220
It's called Visualizing and Understanding
277

277

00:13:11,220  -->  00:13:12,910
Recurrent Networks.
278

278

00:13:12,910  -->  00:13:15,660
Very interesting read actually,
279

279

00:13:15,660  -->  00:13:19,700
because there's not too much math
280

280

00:13:19,700  -->  00:13:24,460
and you can probably skip it, but even the insights
281

281

00:13:24,460  -->  00:13:26,680
and parts and chapters are very interesting.
282

282

00:13:26,680  -->  00:13:30,330
They make you kind of feel that, and they say this
283

283

00:13:30,330  -->  00:13:31,926
in the paper, that they're like neuroscientists
284

284

00:13:31,926  -->  00:13:33,720
trying to understand what's going on.
285

285

00:13:33,720  -->  00:13:37,200
So they open up the brain of the neural network
286

286

00:13:37,200  -->  00:13:40,261
and monitor what's happening in one specific neuron,
287

287

00:13:40,261  -->  00:13:43,850
or different neurons.
288

288

00:13:43,850  -->  00:13:47,100
And you actually feel, from the language,
289

289

00:13:47,100  -->  00:13:50,170
they way that's it's written, that it's as if they're
290

290

00:13:50,170  -->  00:13:54,230
exploring some alien, as if they're exploring some kind
291

291

00:13:54,230  -->  00:13:57,190
of extra-terrestrial being and how it thinks.
292

292

00:13:57,190  -->  00:14:01,267
Because if you think about it, humans created these LSTMs
293

293

00:14:01,267  -->  00:14:03,749
and RNNs, these are just things that work on our computers.
294

294

00:14:03,749  -->  00:14:08,480
But because they are so advanced, because they involve
295

295

00:14:08,480  -->  00:14:12,004
so many different elements, so many different parts to them,
296

296

00:14:12,004  -->  00:14:16,402
they're so complex, we now need,
297

297

00:14:16,402  -->  00:14:19,240
we've created them, now we need to study them
298

298

00:14:19,240  -->  00:14:23,480
as if they're separate beings, separate something
299

299

00:14:23,480  -->  00:14:25,150
that exists on its own.
300

300

00:14:25,150  -->  00:14:27,183
And it's just reading through it, it's kinda like
301

301

00:14:27,183  -->  00:14:29,486
if you think of it that way, it gives you
302

302

00:14:29,486  -->  00:14:32,480
this mysterious feeling or magic,
303

303

00:14:32,480  -->  00:14:34,360
if you feel like it's something magical happening.
304

304

00:14:34,360  -->  00:14:37,110
At the same time you feel that a few more years
305

305

00:14:37,110  -->  00:14:39,420
or maybe a decade from now, these things
306

306

00:14:39,420  -->  00:14:41,800
are going to be able to think completely on their own.
307

307

00:14:41,800  -->  00:14:46,250
So if you want to have some fun reading a research paper
308

308

00:14:46,250  -->  00:14:47,800
this one's pretty cool I think.
309

309

00:14:48,720  -->  00:14:51,310
Maybe you don't have to read the math, skip the math.
310

310

00:14:51,310  -->  00:14:53,544
I didn't really dig into the math myself.
311

311

00:14:53,544  -->  00:14:56,880
Yeah, so that's pretty much it.
312

312

00:14:56,880  -->  00:14:58,210
I hope you enjoyed today's tutorial.
313

313

00:14:58,210  -->  00:15:00,910
And hopefully it give you a bit of a better understanding
314

314

00:15:00,910  -->  00:15:05,910
of how the architecture actually plays out in practice.
315

315

00:15:06,580  -->  00:15:08,160
And I look forward to seeing you next time.
316

316

00:15:08,160  -->  00:15:09,983
Until then, enjoy Deep Learning.
