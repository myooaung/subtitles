WEBVTT
1
1

00:00:00.600  -->  00:00:01.433
<v Narrator>Hello, and welcome back</v>
2

2

00:00:01.433  -->  00:00:02.970
to the course on Deep Learning.
3

3

00:00:02.970  -->  00:00:06.000
Today we're talking about sparse autoencoders.
4

4

00:00:06.000  -->  00:00:08.210
So as we previously discussed,
5

5

00:00:08.210  -->  00:00:10.850
we are aiming to create an autoencoder where
6

6

00:00:10.850  -->  00:00:13.930
the hidden layer is actually greater than the input layer.
7

7

00:00:13.930  -->  00:00:17.450
And the reason for that is we want to extract more features.
8

8

00:00:17.450  -->  00:00:19.660
And we know that autoencorders are designed
9

9

00:00:19.660  -->  00:00:21.230
as a great feature extraction tool.
10

10

00:00:21.230  -->  00:00:22.880
And the whole concept is pretty solid
11

11

00:00:22.880  -->  00:00:26.150
when we want our outputs to equate to our inputs.
12

12

00:00:26.150  -->  00:00:28.600
But in this case, we want to extract more features
13

13

00:00:28.600  -->  00:00:30.310
than just three, or just two,
14

14

00:00:30.310  -->  00:00:35.310
or we want to extract an unrestricted amount of features.
15

15

00:00:35.320  -->  00:00:38.900
And here, what we're seeing is that basically,
16

16

00:00:38.900  -->  00:00:41.280
as soon as we do that, the autoencoder can cheat,
17

17

00:00:41.280  -->  00:00:43.650
it can basically just take these inputs,
18

18

00:00:43.650  -->  00:00:46.020
put them through the hidden layer and and input them
19

19

00:00:46.020  -->  00:00:50.350
as outputs and without actually creating any
20

20

00:00:50.350  -->  00:00:52.990
valuable feature extraction tool within itself
21

21

00:00:52.990  -->  00:00:55.520
during a training process just because it's going
22

22

00:00:55.520  -->  00:00:57.680
to be easier for autoencoder to do this.
23

23

00:00:57.680  -->  00:01:00.440
And so we have to come up with a way to prevented
24

24

00:01:00.440  -->  00:01:01.450
it from doing that.
25

25

00:01:01.450  -->  00:01:03.448
And the first way we're looking at is called,
26

26

00:01:03.448  -->  00:01:05.682
the sparse autoencoder.
27

27

00:01:05.682  -->  00:01:10.210
And what I wanted to mention here is that you will hear
28

28

00:01:10.210  -->  00:01:14.370
sparse autoencoder a lot, like a lot, a lot.
29

29

00:01:14.370  -->  00:01:15.740
It is used everywhere,
30

30

00:01:15.740  -->  00:01:18.330
or pretty much everywhere in literature.
31

31

00:01:18.330  -->  00:01:22.810
And very often you will just hear sparse encoder
32

32

00:01:22.810  -->  00:01:26.180
and people won't even explain why it's a sparse autoencoder?
33

33

00:01:26.180  -->  00:01:27.480
what that means?
34

34

00:01:27.480  -->  00:01:31.441
And it is sometimes even used interchangeably
35

35

00:01:31.441  -->  00:01:32.908
within autoencoder.
36

36

00:01:32.908  -->  00:01:36.010
As soon as you notice that you have to be aware
37

37

00:01:36.010  -->  00:01:36.870
of what you're dealing with.
38

38

00:01:36.870  -->  00:01:39.458
If you're dealing with nodes, the autoencoder
39

39

00:01:39.458  -->  00:01:41.736
that we were working with before but this type
40

40

00:01:41.736  -->  00:01:44.120
of autoencoder and that's why this tutorial so important.
41

41

00:01:44.120  -->  00:01:45.860
So what is a sparse autoencoder?
42

42

00:01:45.860  -->  00:01:48.300
A sparse autoencoder is an autoencoder
43

43

00:01:48.300  -->  00:01:51.030
which looks like this, where the hidden layer is greater
44

44

00:01:51.030  -->  00:01:52.260
than the input layer.
45

45

00:01:52.260  -->  00:01:56.860
But a regularization technique which introduces sparsity
46

46

00:01:56.860  -->  00:01:57.693
has been applied.
47

47

00:01:57.693  -->  00:02:00.480
A regularization technique basically means something
48

48

00:02:00.480  -->  00:02:03.520
that helps prevent over-fitting or stabilizes the algorithm.
49

49

00:02:03.520  -->  00:02:07.940
In this case, if it was just sending the values through,
50

50

00:02:07.940  -->  00:02:11.130
it would be over-fitting in a way.
51

51

00:02:11.130  -->  00:02:13.860
And therefore, we need regularization techniques
52

52

00:02:13.860  -->  00:02:15.970
and this sparse autoencoder is one of them.
53

53

00:02:15.970  -->  00:02:18.463
And basically, what it does is it just says
54

54

00:02:18.463  -->  00:02:22.580
it introduces a constraint on the loss function,
55

55

00:02:22.580  -->  00:02:23.830
or a penalty of a loss function,
56

56

00:02:23.830  -->  00:02:26.690
and which doesn't allow the autoencoder to use
57

57

00:02:26.690  -->  00:02:29.020
all of its hidden layer every single time.
58

58

00:02:29.020  -->  00:02:32.230
So every time or anytime, anytime at all.
59

59

00:02:32.230  -->  00:02:37.230
So, at any time the autoencoder can only use a certain
60

60

00:02:37.482  -->  00:02:40.050
number of nodes from it's hidden layer.
61

61

00:02:40.050  -->  00:02:42.670
For instance, it can use two nodes in this case.
62

62

00:02:42.670  -->  00:02:45.840
And so, when the values go through these nodes
63

63

00:02:45.840  -->  00:02:48.660
are out putting very very small values,
64

64

00:02:48.660  -->  00:02:53.100
so or very tiny values which are insignificant.
65

65

00:02:53.100  -->  00:02:55.110
so it's not inputting insignificant values
66

66

00:02:55.110  -->  00:02:58.930
and therefore only these nodes are actually participating.
67

67

00:02:58.930  -->  00:03:02.930
Then in another pass these nodes are really participating.
68

68

00:03:02.930  -->  00:03:04.990
Another pass these nodes are really participating.
69

69

00:03:04.990  -->  00:03:07.197
So you are extracting features,
70

70

00:03:07.197  -->  00:03:10.360
the essences that you are at the end of the day,
71

71

00:03:10.360  -->  00:03:12.630
because you have so many rows in your data set
72

72

00:03:12.630  -->  00:03:13.463
that are going through it,
73

73

00:03:13.463  -->  00:03:15.520
at the end of the day you are training this whole layer.
74

74

00:03:15.520  -->  00:03:18.910
So you are extracting features from each one of these nodes
75

75

00:03:18.910  -->  00:03:19.860
through each one of these nodes.
76

76

00:03:19.860  -->  00:03:22.982
But at the same time, not at any given pass,
77

77

00:03:22.982  -->  00:03:24.720
you're not using all of these.
78

78

00:03:24.720  -->  00:03:27.830
So the autoencoder cannot cheat because even though
79

79

00:03:27.830  -->  00:03:29.470
it has more nodes in the hidden layer
80

80

00:03:29.470  -->  00:03:30.766
than in the input layer,
81

81

00:03:30.766  -->  00:03:34.330
it is not able to use all of them at any given pass.
82

82

00:03:34.330  -->  00:03:37.360
And that's how the sparse autoencoder works.
83

83

00:03:37.360  -->  00:03:38.505
Pretty straightforward technique.
84

84

00:03:38.505  -->  00:03:40.448
Of course, it has mathematics behind it,
85

85

00:03:40.448  -->  00:03:43.790
and we'll just now look through some further reading
86

86

00:03:43.790  -->  00:03:44.623
that you can do.
87

87

00:03:44.623  -->  00:03:47.680
But in essence, it's quite a simple idea,
88

88

00:03:47.680  -->  00:03:51.326
quite a straightforward idea to still keep the size,
89

89

00:03:51.326  -->  00:03:55.155
the large size of the hidden layer, but at the same time,
90

90

00:03:55.155  -->  00:03:58.940
not allow the autoencoder to bypass the proper training
91

91

00:03:58.940  -->  00:04:00.950
that we wanted to and we will.
92

92

00:04:00.950  -->  00:04:02.810
And that's what is sparse autoencoder is.
93

93

00:04:02.810  -->  00:04:04.930
So whenever you hear sparse autoencoder,
94

94

00:04:04.930  -->  00:04:07.440
just remember that it's this kind of thing.
95

95

00:04:07.440  -->  00:04:10.470
And in reality when you think about it,
96

96

00:04:10.470  -->  00:04:12.880
it is still compressing the information
97

97

00:04:12.880  -->  00:04:15.220
but just every time is using different nodes.
98

98

00:04:15.220  -->  00:04:18.730
Okay, so let's have a look at the additional reading.
99

99

00:04:18.730  -->  00:04:23.160
So we've got a interesting tutorial here by Chris McCormick
100

100

00:04:23.160  -->  00:04:25.360
is called Deep Learning tutorial Sparse Autoencoder.
101

101

00:04:25.360  -->  00:04:28.520
And this is a MATLAB tutorial.
102

102

00:04:28.520  -->  00:04:31.070
So if there's any fans of MATLAB,
103

103

00:04:31.070  -->  00:04:33.670
if you're a fan of MATLAB you might find this interesting.
104

104

00:04:33.670  -->  00:04:35.520
He will walk you through how to do it in MATLAB.
105

105

00:04:35.520  -->  00:04:38.240
But other than that, there's just, I like it because there's
106

106

00:04:38.240  -->  00:04:41.235
some introductory mathematics, not too complex,
107

107

00:04:41.235  -->  00:04:45.030
but some basic level formulas which allow you
108

108

00:04:45.030  -->  00:04:48.504
to gently get introduced to the mathematics behind
109

109

00:04:48.504  -->  00:04:50.530
the sparse autoencoders.
110

110

00:04:50.530  -->  00:04:53.133
If you want to understand how that whole
111

111

00:04:53.133  -->  00:04:56.634
how the equations work which don't allow the autoencoders
112

112

00:04:56.634  -->  00:05:00.320
to switch on all of its nodes at the same time,
113

113

00:05:00.320  -->  00:05:02.667
at any given point, at any given pass.
114

114

00:05:02.667  -->  00:05:06.840
The next one is called, Deep Learning Sparse Autoencoders
115

115

00:05:06.840  -->  00:05:07.871
by Eric Wilkinson.
116

116

00:05:07.871  -->  00:05:12.330
It's a very short blog post on the essence
117

117

00:05:12.330  -->  00:05:13.915
of sparse autoencoders.
118

118

00:05:13.915  -->  00:05:17.356
It's literally like five to 10 minute read.
119

119

00:05:17.356  -->  00:05:20.585
And just another gentle introduction to the world
120

120

00:05:20.585  -->  00:05:22.600
of sparse autoencoders.
121

121

00:05:22.600  -->  00:05:26.586
And finally a very strong powerful, heavy artillery paper
122

122

00:05:26.586  -->  00:05:28.460
on sparse autoencoders.
123

123

00:05:28.460  -->  00:05:32.670
It's called, K-Sparse Autoencoders by Alireza Makhzani, 2014
124

124

00:05:33.831  -->  00:05:38.831
and it basically takes his whole sparse autoencoders concept
125

125

00:05:39.010  -->  00:05:43.740
to a whole new level where it talks about different
126

126

00:05:43.740  -->  00:05:45.750
like a parameter K which allows you to control
127

127

00:05:45.750  -->  00:05:47.090
the sparsity of the auto encoder
128

128

00:05:47.090  -->  00:05:50.470
and you got some examples over here on the right.
129

129

00:05:50.470  -->  00:05:52.860
You might find this paper interesting if you want
130

130

00:05:52.860  -->  00:05:54.880
to learn more about sparse encoders.
131

131

00:05:54.880  -->  00:05:57.467
I actually would encourage you to learn a bit more
132

132

00:05:57.467  -->  00:06:01.550
because it is a tool that is used all over the place
133

133

00:06:01.550  -->  00:06:02.960
and it's constantly mentioned.
134

134

00:06:02.960  -->  00:06:05.330
So you will definitely come across sparse autoencoders
135

135

00:06:05.330  -->  00:06:07.804
if you are going to be dealing with autoencoders.
136

136

00:06:07.804  -->  00:06:10.880
okay, so that's sparse autoencoders
137

137

00:06:10.880  -->  00:06:13.290
and I look forward to seeing you next time.
138

138

00:06:13.290  -->  00:06:14.953
Until then, enjoy Deep Learning.
