WEBVTT
1
1

00:00:00.360  -->  00:00:02.595
<v ->Hello, and welcome to the second part</v>
2

2

00:00:02.595  -->  00:00:06.360
of our implementation of the recurrent neural network.
3

3

00:00:06.360  -->  00:00:08.790
Part 2: Building the RNN
4

4

00:00:08.790  -->  00:00:10.770
That's in this part where we gonna build the whole
5

5

00:00:10.770  -->  00:00:12.620
architecture of this neural network.
6

6

00:00:12.620  -->  00:00:14.894
A robust architecture, because we're not only gonna make
7

7

00:00:14.894  -->  00:00:16.850
a simple LSTM.
8

8

00:00:16.850  -->  00:00:19.202
We're gonna make a stacked LSTM with some dropout
9

9

00:00:19.202  -->  00:00:22.350
regularization to prevent over fitting.
10

10

00:00:22.350  -->  00:00:24.639
And speaking of dropout, as you can see,
11

11

00:00:24.639  -->  00:00:27.778
what I included in the imports of the keras libraries
12

12

00:00:27.778  -->  00:00:29.280
and packages.
13

13

00:00:29.280  -->  00:00:32.280
Not only am I importing the sequential class
14

14

00:00:32.280  -->  00:00:34.721
that will allow us to create a neural network object
15

15

00:00:34.721  -->  00:00:38.310
representing a sequence of layers, but also the dense
16

16

00:00:38.310  -->  00:00:40.350
class to add the output layer.
17

17

00:00:40.350  -->  00:00:43.025
The LSTM class, to add the LSTM layers.
18

18

00:00:43.025  -->  00:00:48.025
And, the dropout class to add some dropout regularization.
19

19

00:00:48.056  -->  00:00:49.120
So there we go.
20

20

00:00:49.120  -->  00:00:53.136
That's all we will need to build this powerful RNN.
21

21

00:00:53.136  -->  00:00:56.510
Therefore lets do this now, so that we get,
22

22

00:00:56.510  -->  00:00:57.343
the right tools.
23

23

00:00:57.343  -->  00:00:58.176
Execute
24

24

00:00:58.176  -->  00:01:00.421
There we go, using TenserFlow backend.
25

25

00:01:00.421  -->  00:01:01.762
And the import
26

26

00:01:01.762  -->  00:01:02.640
should
27

27

00:01:02.640  -->  00:01:03.473
be
28

28

00:01:03.473  -->  00:01:04.440
done
29

29

00:01:04.440  -->  00:01:05.273
in any
30

30

00:01:05.273  -->  00:01:07.170
second.
31

31

00:01:07.170  -->  00:01:08.850
And there we go, perfect.
32

32

00:01:08.850  -->  00:01:09.780
We have the right classes.
33

33

00:01:09.780  -->  00:01:12.490
And now we gonna initialize our RNN.
34

34

00:01:12.490  -->  00:01:15.610
That's the first step of this big part 2.
35

35

00:01:15.610  -->  00:01:16.740
Building the RNN.
36

36

00:01:16.740  -->  00:01:19.460
We gonna initialize our Recurrent Neural Network.
37

37

00:01:19.460  -->  00:01:21.750
As a sequence of layers, as opposed to a
38

38

00:01:21.750  -->  00:01:23.220
computational graph.
39

39

00:01:23.220  -->  00:01:26.160
We gonna build later on in this course, in volume two,
40

40

00:01:26.160  -->  00:01:27.760
some computational graphs.
41

41

00:01:27.760  -->  00:01:29.640
And we're gonna do this with firetorch, because for this,
42

42

00:01:29.640  -->  00:01:31.540
firetorch is much more powerful
43

43

00:01:31.540  -->  00:01:33.070
thanks to the dynamic graphs.
44

44

00:01:33.070  -->  00:01:35.651
But for now, we're making a Recurrent Neural Network.
45

45

00:01:35.651  -->  00:01:38.380
And Keras is perfectly fine and so we're gonna use
46

46

00:01:38.380  -->  00:01:42.000
the sequential class from keras to introduce our.
47

47

00:01:42.000  -->  00:01:42.833
This time...
48

48

00:01:42.833  -->  00:01:43.873
Regressor...
49

49

00:01:43.873  -->  00:01:45.630
As...
50

50

00:01:45.630  -->  00:01:46.750
a sequence...
51

51

00:01:46.750  -->  00:01:47.583
of...
52

52

00:01:47.583  -->  00:01:48.416
layers.
53

53

00:01:48.416  -->  00:01:52.134
Regressor is now an object of the sequential class
54

54

00:01:52.134  -->  00:01:55.870
which represents exactly the sequence of layers.
55

55

00:01:55.870  -->  00:01:58.560
And why did I call it regressor this time?
56

56

00:01:58.560  -->  00:02:01.560
As opposed to classifier like in the first two parts.
57

57

00:02:01.560  -->  00:02:02.960
ANN and CNN.
58

58

00:02:02.960  -->  00:02:05.710
Well that because this time we're predicting a continuous
59

59

00:02:05.710  -->  00:02:06.543
output.
60

60

00:02:06.543  -->  00:02:08.580
A continuous value, which is the google stock price,
61

61

00:02:08.580  -->  00:02:09.620
twenty plus one.
62

62

00:02:09.620  -->  00:02:12.719
And therefore we are doing some regression.
63

63

00:02:12.719  -->  00:02:15.720
A reminder, regression is about predicting the
64

64

00:02:15.720  -->  00:02:16.900
continuous value.
65

65

00:02:16.900  -->  00:02:19.474
And classification is about creating a (mumble), a class.
66

66

00:02:19.474  -->  00:02:22.580
So, this time we're predicting a continuous value.
67

67

00:02:22.580  -->  00:02:25.910
And therefore I called my neural network, my recurrent
68

68

00:02:25.910  -->  00:02:27.417
neural network regressor.
69

69

00:02:27.417  -->  00:02:29.110
Alright, and that's it.
70

70

00:02:29.110  -->  00:02:31.300
That initializes the RNN.
71

71

00:02:31.300  -->  00:02:34.120
So we are going to execute this line.
72

72

00:02:34.120  -->  00:02:35.020
There we go.
73

73

00:02:35.020  -->  00:02:37.410
Regressor equals sequential.
74

74

00:02:37.410  -->  00:02:38.700
And congratulations.
75

75

00:02:38.700  -->  00:02:40.521
You have initialized your regressor.
76

76

00:02:40.521  -->  00:02:43.020
And now we gonna add the different layers,
77

77

00:02:43.020  -->  00:02:45.153
to make it a powerful stacked LSTM.
78

78

00:02:45.153  -->  00:02:48.370
And we're gonna do that in the next tutorials.
79

79

00:02:48.370  -->  00:02:50.637
Until then, enjoy the training.
