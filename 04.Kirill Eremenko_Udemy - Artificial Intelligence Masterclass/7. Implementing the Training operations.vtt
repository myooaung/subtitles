WEBVTT

00:00.210 --> 00:07.230
Hello and welcome to this new tutorial now that we've given the ability to dream Well we have to implement

00:07.440 --> 00:13.610
the training operations because indeed what we've just done so far is you know build a brain for the

00:13.620 --> 00:16.850
eye and then at some point of course we have to train the AI.

00:16.950 --> 00:23.820
And what we're about to implement are exactly the training operations that you know will reduce the

00:23.820 --> 00:27.450
loss between the predictions and the target.

00:27.480 --> 00:29.890
So the predictions are exactly self-taught.

00:29.940 --> 00:30.300
Why.

00:30.300 --> 00:37.620
Because these are the output of the V8 immoral and the targets are of course self-taught X because they

00:37.620 --> 00:39.450
are the real images.

00:39.450 --> 00:45.490
And in order to reconstruct some images there are the closest to reality.

00:45.600 --> 00:52.760
Well during the training of the Wii will have to reduce that loss between the predictions the reconstructed

00:52.770 --> 00:58.250
images solve that Y and the targets are the real images subtopics.

00:58.410 --> 01:02.880
So that's exactly what we'll do in this new code section.

01:02.880 --> 01:06.280
But the laws will actually be very complex.

01:06.300 --> 01:08.270
It is actually the sum of two losses.

01:08.430 --> 01:14.160
The mean square error last and the callback libelous Caylus.

01:14.190 --> 01:15.050
So here we go.

01:15.090 --> 01:21.240
If you're ready let's do this let's implement the train operations and the further we're going to do

01:21.270 --> 01:28.260
is make sure that we are in training mode and the way we're going to do that is of course by using our

01:28.330 --> 01:33.920
is training variable which is indeed a variable of our object.

01:34.170 --> 01:41.550
And so the way we're going to start here is with an F and then self no we're calling the object and

01:41.550 --> 01:45.900
we're taking that is training variable of our object.

01:46.020 --> 01:52.540
And then Colonne because this means exactly if self that training equals equals.

01:52.770 --> 01:53.430
All right.

01:53.440 --> 01:58.860
And so if that's the case if we are indeed in training mode Well here we go.

01:58.890 --> 02:02.880
Let's implement one by one the training operations.

02:03.040 --> 02:08.910
And so the first thing we're going to do now is define a new variable which is once again an object

02:09.000 --> 02:17.070
variable which is going to be a global step of gradient descent you know a step of the update by the

02:17.160 --> 02:23.940
optimizer to reduce that loss basically with stochastic gradient isn't the optimizer that will reduce

02:23.940 --> 02:24.560
the loss.

02:24.570 --> 02:31.590
For this we will be in advanced greatness and technique and it's a global step and to define that step

02:31.860 --> 02:39.000
of great in the sense we will do that with that sense of variable object which we can create by first

02:39.000 --> 02:44.180
call intensively of course and then by going from sense of flow the variable class.

02:44.320 --> 02:44.810
Here we go.

02:44.850 --> 02:50.880
First one variable inside which as you can see we can input many arguments.

02:50.880 --> 02:58.710
The first one is the initial value an initial value will be zero as the first index of the first step.

02:58.800 --> 03:04.760
Then we're going to give a name to that variable object by tensor flow.

03:04.890 --> 03:07.790
And this name will be simply in quotes.

03:07.950 --> 03:11.150
Global step right.

03:11.260 --> 03:20.230
And then third argument which will be this one trainable and which will be initialized to false.

03:20.220 --> 03:23.470
All right so that was for the first step.

03:23.470 --> 03:27.260
Setting the global state variable object by tenths of flow.

03:27.460 --> 03:29.710
And now here we go with the losses.

03:29.770 --> 03:35.840
And so we're going to start with the simplest one which is the very well known mean square error last.

03:36.040 --> 03:41.100
But we're going to compute that from scratch in two steps using the tensor functions.

03:41.110 --> 03:45.020
So here we go first let's create a variable for that last.

03:45.160 --> 03:50.570
So we're going to call this our core loss and that will be the mean square.

03:50.590 --> 03:54.720
Errorless And so we're going to compute it in two steps.

03:54.730 --> 04:01.480
The first step will be to calculate the sum of the square differences between the predictions and the

04:01.480 --> 04:02.330
target.

04:02.420 --> 04:08.320
And remember the predictions are set that why the reconstructed images and the targets are set that

04:08.370 --> 04:10.860
X the original real images.

04:11.140 --> 04:16.330
And so what we're going to do here is first Coltons a flow then from tens of through we're going to

04:16.330 --> 04:19.570
take to reduce on this score some.

04:19.600 --> 04:20.490
Here we go.

04:20.510 --> 04:25.350
Function inside which of course we're going to put the square differences.

04:25.420 --> 04:28.290
And so how do we take first squares.

04:28.450 --> 04:36.190
Well we take them with a sense of flow and using the square function inside which of course we are going

04:36.190 --> 04:45.310
to input the difference between the target so that x the original real images minus the predictions

04:45.640 --> 04:50.000
which are the reconstructed images self-taught Why perfect.

04:50.110 --> 04:58.240
But then that's not all we have to specify on which indices we want to operate these sums and actually

04:58.240 --> 05:06.880
know since x and y both have three dimensions corresponding to remember the width the height and the

05:06.880 --> 05:08.010
R.G. dimension.

05:08.020 --> 05:10.520
Because we're dealing with colored images.

05:10.810 --> 05:17.170
Well right now we'll have to specify that we are something along the three axis corresponding to these

05:17.500 --> 05:18.680
three dimensions.

05:18.920 --> 05:26.830
And therefore what we're going to add here as the second argument of to reduce some function is reduction

05:27.760 --> 05:31.540
indices equals in square brackets.

05:31.540 --> 05:35.720
The three indexes of the three dimensions the three axis.

05:35.830 --> 05:37.860
And remember there is trap here.

05:37.870 --> 05:44.040
The first index corresponds to the batched So that's not where we want to start with the first axis.

05:44.050 --> 05:45.350
We'll have the next one.

05:45.400 --> 05:51.540
The second axis will have the next two and the third axis corresponding to the R.G. dimension.

05:51.580 --> 05:52.860
We'll have the next three.

05:52.960 --> 05:53.840
And here we go.

05:53.860 --> 05:59.670
Now we are summing the square differences along the three axis is perfect.

05:59.710 --> 06:04.540
But that's the first part of that can be taken to mean square errorless.

06:04.540 --> 06:08.440
Now of course we have to take the mean of these sums.

06:08.440 --> 06:11.760
And again we're going to use that sense of the function.

06:11.830 --> 06:19.430
And so I'm going to take that again because we're actually going to update our laws by taking its meaning

06:19.630 --> 06:21.320
from the tense of your function.

06:21.330 --> 06:25.670
Reduce mean and so right here I'm taking a sense of flow again.

06:25.930 --> 06:28.670
Then reduce mean.

06:28.840 --> 06:29.700
Here we go.

06:29.860 --> 06:36.700
And inside we we're going to input very simply self that our loss and this will just compute the mean

06:37.060 --> 06:42.690
of these sums as it is exactly that can be taken of the mean square airless.

06:42.910 --> 06:43.760
Perfect.

06:43.810 --> 06:45.790
So that was the easy step.

06:45.790 --> 06:48.320
Now comes the pretty difficult step.

06:48.370 --> 06:56.890
It is the accumulation of the K L plus callback libelous based of course on the callback labor divergence.

06:57.070 --> 07:01.720
So we're not going to explain the formula we're just going to implement it.

07:01.750 --> 07:03.110
It is very complex.

07:03.130 --> 07:06.670
It is actually so complex that were implemented in three steps.

07:06.850 --> 07:08.710
So here we go with the first step.

07:08.770 --> 07:10.690
We're going to call this this time.

07:10.730 --> 07:11.660
Self-doubt.

07:11.680 --> 07:11.970
K.

07:12.010 --> 07:22.900
L last and the first step will be to some one plus the values in the Lovemore dense layer minus the

07:22.900 --> 07:31.280
squares of the values in the new dense layer minus the exponential of the values and a lot more dense.

07:31.480 --> 07:34.860
Then all this multiplied by minus 0.5.

07:34.930 --> 07:44.170
Actually we're going to start with this minus 0.5 times and then we take again from tensor flow to reduce

07:45.010 --> 07:48.560
some function just to you know computer some.

07:48.790 --> 07:53.000
And now we want to compute the sum of the three elements that I've mentioned.

07:53.110 --> 07:57.110
One plus self that locavore.

07:57.220 --> 08:06.420
So basically the values in the dense layer lover are then minus the squares of the myu dense layer.

08:06.430 --> 08:13.690
So here I'm taking first the square function by tens of flow inside which I'm putting the values in

08:13.690 --> 08:22.230
the self that new dense there and then as we said minus the exponential function by sense of flow.

08:22.300 --> 08:30.910
So I'm taking that exp function as we took before of the LUGG var Denslow.

08:31.150 --> 08:31.680
All right.

08:31.700 --> 08:39.220
All this is what we are or something inside the Kalos but therefore we have to put this in one argument

08:39.220 --> 08:45.660
because then comes the indices of the axes along which we want to do that some.

08:45.790 --> 08:47.220
And so we have to specify this.

08:47.230 --> 08:52.800
But this time it's going to be simple because the dense layer actually has two dimensions.

08:52.840 --> 08:58.990
The first one corresponding to the Bache and the second one corresponding to the single time mention

08:59.050 --> 09:01.570
of this one dimensional vertical tensor.

09:01.670 --> 09:03.400
You know it's like a vertical vector.

09:03.430 --> 09:08.410
So here the reduction in the indices is nothing else than what.

09:08.800 --> 09:09.610
Okay perfect.

09:09.610 --> 09:12.890
So we have done the toughest part.

09:12.940 --> 09:18.370
Now what we're going to do is the second part of the competition of the K L us nor is it going to be

09:18.520 --> 09:25.490
more simple because we're just going to take the maximum of that care to have just compute it.

09:25.550 --> 09:27.240
You know this formula here.

09:27.490 --> 09:35.350
And the key l tolerance which is one of our arguments here and also a variable object multiplied by

09:35.560 --> 09:36.350
the set size.

09:36.370 --> 09:44.470
So we're taking this maximum do entrancingly to the K L tolerance that we want to use to cap diskless

09:44.670 --> 09:50.740
and that makes sense that's because if the loss is too small then the maximum of the last we computed

09:50.740 --> 09:57.650
before and that K.L. tolerance will be equal to the K L tolerance and therefore is the less is too small.

09:57.700 --> 10:03.190
Well we won't have to apply the gradients and you know a date the ways to reduce that part of the last

10:03.550 --> 10:06.530
because that maximum will just be a constant.

10:06.640 --> 10:12.910
So basically this is just a trick to save some computations of the gradient in case the loss is too

10:12.910 --> 10:13.300
small.

10:13.300 --> 10:19.720
And therefore in case there is not too much to optimize and so right now we're just going to update

10:19.990 --> 10:32.350
the K.L. loss by taking the maximum function by tens of flow of that same K.L. loss that we've just

10:32.350 --> 10:33.320
computed.

10:33.460 --> 10:43.540
And as we've just said the K L tolerance multiplied by that that size.

10:43.540 --> 10:44.190
Here we go.

10:44.200 --> 10:44.650
Perfect.

10:44.650 --> 10:49.040
So second part of the K L can be done.

10:49.420 --> 10:51.050
And now third part.

10:51.160 --> 10:52.480
The easiest one.

10:52.480 --> 10:53.370
That's the good news.

10:53.480 --> 10:59.830
We're just going to compute the mean again of that sum we can Puran the first step just like what we

10:59.830 --> 11:06.420
did with the means quit air but of course the maximum of these two is this one here.

11:06.520 --> 11:09.740
Well the mean will be equal to that same value here.

11:09.890 --> 11:17.080
And if however the maximum is the key plus as we computed here well it will just take the mean of the

11:17.080 --> 11:20.980
sum of all the values in these dense layers.

11:20.980 --> 11:21.790
All right.

11:21.790 --> 11:25.740
So let's do that quickly and efficiently actually we can do it this way.

11:25.810 --> 11:34.750
I'm going to copy this then they say here and then just replace our loss by Kalos and say Here are those

11:34.780 --> 11:36.230
by Kalos.

11:36.490 --> 11:37.240
All right.

11:37.360 --> 11:38.170
We did it.

11:38.230 --> 11:40.610
We computed the Colback libelous.

11:40.630 --> 11:43.850
Congratulations if you did that for the first time.

11:44.040 --> 11:51.670
And now now that we both have the mean square error plus the plastic one and the Colback libelers Well

11:51.760 --> 11:58.510
we're going to sum these two because actually our global us that will want to reduce in the global step

11:58.780 --> 12:01.490
of great in this sense will be that some of these too.

12:01.750 --> 12:02.770
Let's do this.

12:02.800 --> 12:09.830
We're going to call that cell blood loss which will be the son of first the main square.

12:09.860 --> 12:18.530
Our laws are laws and the callback labor laws this one perfect.

12:18.610 --> 12:20.890
And now the rest will be very easy.

12:20.890 --> 12:29.230
We'll just put our learning rate on and choosing a new variable called are what will set their rates

12:29.290 --> 12:31.890
as sense of flow variable.

12:31.930 --> 12:35.950
Again you know as an object of the voluble class I tend to flow.

12:36.010 --> 12:36.930
Here we go.

12:37.150 --> 12:46.120
And this is going to take of course our self learning rate which is already an existing viable object

12:46.380 --> 12:47.560
learning rate.

12:47.680 --> 12:55.600
And again we don't want yet the training will cause true value for the trainable argument but as for

12:55.600 --> 13:03.850
the global step here will initialize that to falls perfect then we're going to create a variable for

13:04.270 --> 13:08.440
optimizer which will be the atom optimizer.

13:08.440 --> 13:12.670
You know I told you that it was B is the category in the sense optimizer.

13:12.830 --> 13:18.260
And one of the best versions of the Kastigar innocent is the Adam optimizer.

13:18.470 --> 13:21.490
So that's exactly what we're going to create.

13:21.710 --> 13:28.490
And we can create it as an object of the atom optimized class but at an optimized class is a class of

13:28.790 --> 13:32.110
the train module violence of flow.

13:32.120 --> 13:38.950
And here we go from the train module buttons of we can get the atom optimized class as you can see here

13:39.800 --> 13:45.890
and this and an optimizer class takes on the one argument which you might guess what it is that's why

13:46.250 --> 13:48.330
we took the learning right before.

13:48.350 --> 13:55.310
But we've made sure to set as a of your variable and that's because the optimizer class expects as its

13:55.400 --> 13:59.870
argument the learning rate as an intensive variable.

13:59.870 --> 14:02.300
So I'm basing that here and here we go.

14:02.360 --> 14:05.460
Now we have our Adam optimizer.

14:05.480 --> 14:06.230
All right.

14:06.230 --> 14:10.130
So basically now we have everything to compute the gradients.

14:10.160 --> 14:14.780
And speaking of computing the gradients Well that's exactly what we're going to do now.

14:14.990 --> 14:24.470
So we're going to call our gradients grads and they will be the results of the compute Gretchen's method

14:24.740 --> 14:26.790
by the atom optimizer.

14:27.110 --> 14:33.380
So let's do this let's call this method from our self-taught optimizer object.

14:33.500 --> 14:38.100
And from this object we're calling the method compute gradients.

14:38.120 --> 14:41.260
And of course this method will take as arguments.

14:41.420 --> 14:45.320
Well the last of which it wants to compute the gradients.

14:45.470 --> 14:48.670
And this is of course Arliss here self.

14:48.850 --> 14:51.320
That looks perfect.

14:51.320 --> 14:51.860
All right.

14:51.860 --> 14:59.930
So this computed the gradients but now we're going to apply our gradients on the US to indeed reduce

14:59.930 --> 15:07.970
it by updating the weights in our variational our encode neural network according to how they contributed

15:08.150 --> 15:09.270
to that last error.

15:09.470 --> 15:16.100
And the way we're going to do this is by creating a new variable which will be exactly what we're doing

15:16.100 --> 15:21.770
right now which is to implement the training operations because the Voivode we're going to create now

15:22.130 --> 15:27.470
is exactly to train up volleyball which is exactly that step.

15:27.470 --> 15:34.520
Upgrading does an operation which reduces that loss and updates to wait in this direction of less reduction.

15:34.520 --> 15:41.510
And so to do that well we'd say first our optimizer But then this time from the optimizer we're going

15:41.510 --> 15:49.130
to apply here it is the Apply gradient method which is going to take his arguments first to grad's and

15:49.130 --> 15:55.890
Varas which are here the grads are gradients that we computed with the compute gradient method.

15:56.120 --> 16:03.260
So Gretz first then as you can see the next one is global step and that's of course what we've created

16:03.260 --> 16:08.090
here as a sense of flow of our will which represents the global step of great interest and with our

16:08.180 --> 16:09.310
atom optimizer.

16:09.380 --> 16:20.960
So let's just put you know global step equals self does global step and then finally as usual we can

16:20.960 --> 16:29.300
give a name to that operation and we are going to call this in quote train step right a training step

16:29.490 --> 16:32.560
of gradient descent all right.

16:32.600 --> 16:39.710
And lastly and we have to get out of this if condition because we're done with the train operations.

16:39.830 --> 16:47.580
But what I want to do is just some initialization of all the global variables we'll have for the.

16:47.780 --> 16:53.620
And that's just because whenever we start the training we'll have to initialize everything with an initializer

16:53.630 --> 16:55.100
that we're about to create.

16:55.100 --> 17:02.310
And speaking of which we're going to call this self that minute which is going to be the result of a

17:02.480 --> 17:10.460
sense of function that is global variables initialiser And let's not forget the parenthesis.

17:10.520 --> 17:11.290
All right.

17:11.300 --> 17:16.280
And that closes the section on the build graph method.

17:16.460 --> 17:22.970
So the most important thing in this bill graph method is of course all this bar here where we build

17:23.360 --> 17:26.090
the full architecture of the V.

17:26.390 --> 17:32.570
Then this is just the train operations which I've wanted to include here so that you can see two losses

17:32.600 --> 17:38.120
that you want to reduce and especially so that you understand that the targets are the original real

17:38.120 --> 17:44.620
images of the X and the predictions are the reconstructed images of the Y.

17:44.630 --> 17:51.860
So that gives you the essential but the most important for you is that you understand how we built this

17:52.040 --> 17:59.720
essential component of the full well know the Wii and then three steps after you see some more theory

17:59.960 --> 18:04.370
about the record a neural network and the mixture density network.

18:04.490 --> 18:10.680
While same will build together that second essential element of the full world model which the NTN Arnon

18:11.230 --> 18:14.000
So now you'll take a break with the code.

18:14.170 --> 18:16.240
You have a lot more theory ahead of you.

18:16.240 --> 18:20.260
So get some good energy because you want to stay focused on the essential.

18:20.470 --> 18:25.700
And as soon as you're ready for more code well we'll implement together the NTN are in it.

18:25.840 --> 18:31.610
So I can't wait to see you back on Step 8 when implementing the NBN Arnon.

18:31.660 --> 18:34.330
And until then enjoy artificial intelligence.
