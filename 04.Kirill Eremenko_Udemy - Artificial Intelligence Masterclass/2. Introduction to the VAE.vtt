WEBVTT

00:00.580 --> 00:04.850
Hello welcome back to the artificial intelligence master class.

00:05.050 --> 00:06.480
This is Carol Eremenko.

00:06.480 --> 00:11.820
And in this section we're talking about variational Ardern colors.

00:12.030 --> 00:18.880
Now in this first tutorial I wanted us to take a quick pause from intuition and actually understand

00:18.940 --> 00:22.480
the purpose of variational also kowtows.

00:22.480 --> 00:25.540
In this model what's the port what purpose do they serve.

00:25.540 --> 00:32.320
Why are we introducing variational encoders into this model and assigning the purpose will help us go

00:32.320 --> 00:37.960
through the rest of the operational decoder tutorials much faster and get the most out of them including

00:38.140 --> 00:40.040
the practical side of things literally.

00:40.390 --> 00:46.040
So here we are at the article that we recommend for this course.

00:46.270 --> 00:48.250
World models that get how did I know.

00:48.250 --> 00:56.890
A wonderful paper or blog that explains a lot of the things that we're doing with this model.

00:57.280 --> 01:03.880
And so let's scroll down and actually have a look at the model itself.

01:03.880 --> 01:09.550
So this is where they start talking about the variational encoders and we'll get back here in a second.

01:09.820 --> 01:14.430
But for now I just wanted to go all the way to the appendix or here the appendix.

01:14.430 --> 01:16.340
I'll take you down to the appendix.

01:16.380 --> 01:25.450
So here you can see this model which you are no doubt already familiar with where we have our inputs

01:25.450 --> 01:26.140
here.

01:26.440 --> 01:34.450
Then we perform all our convolutions and other operations and then finally we get to this part here

01:34.450 --> 01:35.690
which looks like a bottleneck.

01:35.710 --> 01:37.420
And in fact it is a bottleneck.

01:37.420 --> 01:39.790
And then we go back to that reconstruction.

01:39.790 --> 01:50.410
So this is the variational out in court it will look in more detail into this specific structure in

01:50.410 --> 01:50.800
the fall.

01:50.800 --> 01:53.310
Falling to Charles and it will be very clear after that.

01:53.440 --> 01:56.920
But what purpose does this serve here.

01:56.950 --> 02:02.860
We've got the inputs and then we've got out what is this variation on to and quarter doing well what

02:02.860 --> 02:09.730
it's actually doing it is the variational outturn coder is the component of our model that creates the

02:09.730 --> 02:11.070
dream.

02:11.110 --> 02:18.250
We've already talked and for lunch has mentioned this many times that the idea behind this model is

02:18.250 --> 02:26.940
that we are allowing our artificial intelligence to dream and then come up with solutions and in streams.

02:26.950 --> 02:29.110
So here you actually see some examples.

02:29.140 --> 02:34.150
This is one of the clearest examples here that this is the actual observation from environment.

02:34.240 --> 02:37.180
And this is what we get encoded in the end.

02:37.180 --> 02:44.000
So this is the dream that artificial intelligence is going through and somehow we need to create it.

02:44.140 --> 02:46.730
That's where the variational out uncolored it comes in.

02:46.990 --> 02:49.160
So now we can look at it in a bit more detail.

02:49.480 --> 02:53.780
Now that we know you know overall that's that's the purpose to create the dream.

02:53.890 --> 02:55.960
How does it create the dream now.

02:56.080 --> 03:01.040
So if you look here you'll see that we've got the image it's coming in.

03:01.060 --> 03:07.150
Then it's been encoded with our auto encoder which is actually a variational and quarter.

03:07.150 --> 03:10.870
But for now we can just think of this as a normal auto and core has been encoded.

03:10.990 --> 03:16.570
Then we have the bottleneck or the middle of the ATA encoder or it's also called the latent factor in

03:16.570 --> 03:25.540
the middle and then that's information that compressed information is used to is decoded into a reconstructed

03:25.630 --> 03:29.280
frame as you can see it's the quality has dropped that here.

03:29.380 --> 03:36.240
Now we could do this with a normal auto encoder and indeed we could code encoded and then decode it.

03:36.400 --> 03:37.940
And that's what we would have.

03:38.170 --> 03:43.390
And we would have a compressed environment so that's one of the benefits we'd have like a compressed

03:43.390 --> 03:47.860
version of our original environment.

03:47.860 --> 03:54.040
And that's one of the benefits of using an auto and color is that we can take a environment with millions

03:54.040 --> 03:58.050
or sometimes in some cases it goes up to billions of parameters.

03:58.210 --> 04:06.580
I think somewhere here in the notes the comments are not sure which one it was but basically an outturn

04:06.580 --> 04:15.280
encoder allows you to compress this environment into from billions of parameters really reduce the parameters

04:15.280 --> 04:24.530
and extract extract the features that really matter that really matter for this task at hand.

04:24.550 --> 04:30.310
You know perhaps the color of the wall doesn't matter or the the texture of the wall does not or the

04:30.310 --> 04:31.650
color of the ceiling doesn't matter.

04:31.650 --> 04:39.610
But how these these monsters in this case how these monsters are moving and where the missile is flying

04:39.610 --> 04:41.140
it showed which speed that does matter.

04:41.150 --> 04:46.570
So extracting the features that actually matter so trading that out in court in a way to check the features

04:46.570 --> 04:47.700
that actually matter.

04:47.830 --> 04:49.240
That's already a step forward.

04:49.240 --> 04:56.740
So and encoder quarter would allow us to construct a compressed environment or compressed or a compressed

04:56.740 --> 05:05.700
version of compressed reality where we train our model and then take that trained model into the real

05:05.820 --> 05:07.230
into the real world.

05:07.230 --> 05:08.880
So that's what an auto encoder would do.

05:08.910 --> 05:10.790
But why do we have a rational.

05:11.040 --> 05:13.170
So that's where the dream comes in.

05:13.170 --> 05:19.100
When we dream as humans we don't actually see the world exactly as it is sometimes but probably rarely.

05:19.110 --> 05:20.810
Most of the time it looks a little bit different.

05:20.820 --> 05:26.390
So here's a screen shot here is perhaps a dream like a dream version of what we might see in a dream

05:26.790 --> 05:31.860
or it might see something that we might see something got so something slightly different.

05:31.920 --> 05:33.280
A modified version.

05:33.420 --> 05:39.930
And this is where the variational are and color comes in because in the middle of a variational all

05:39.930 --> 05:40.630
in color.

05:40.680 --> 05:45.920
It has a stochastic component here.

05:45.980 --> 05:47.550
Actually some more examples.

05:47.550 --> 05:49.410
So there is a screenshot.

05:49.410 --> 05:51.320
There's original There's a reconstruction.

05:51.330 --> 05:56.700
So it can there's many ways that it can reconstruct it and that depends on how this latest picture was

05:56.700 --> 05:57.360
sampled.

05:57.510 --> 06:03.230
So let's go back to what we're talking about the stochastic components here.

06:04.200 --> 06:14.080
As we will see in addition to others in a variation of our encoder this component here has some 660

06:14.130 --> 06:15.690
So some randomness.

06:15.690 --> 06:22.710
The way we encode and decode is not fixed it's not just like one vector every time the suppression happens

06:23.410 --> 06:30.390
some sampling occurs and that allows us to create slightly varying dreams every time slightly varying

06:30.390 --> 06:33.000
representations or reconstructions.

06:33.060 --> 06:40.410
And that allows for more diverse training allows us to allows artificial intelligence to go through

06:40.980 --> 06:47.490
various Even with the same inputs go through various different iterations So for instance the inputs

06:47.530 --> 06:53.370
but they are to feel intelligence actually dreaming lots of different stuff all lots of different scenarios

06:53.400 --> 06:55.630
and it's learning from all of them.

06:55.680 --> 07:02.370
So not only are we compressing the input space like for instance we're getting rid of these colors and

07:02.370 --> 07:09.900
the green for example is an example that might not be important to the decision making process.

07:10.050 --> 07:16.170
Not only are we compressing the input into space but we're actually creating slight variations of the

07:16.170 --> 07:24.450
input space all the time and effectively we're increasing our possibility for training our opportunity

07:24.450 --> 07:26.610
for artificial intelligence to learn.

07:26.790 --> 07:31.890
That's how the dreams are more powerful than the actual.

07:32.140 --> 07:37.530
And we can be more powerful than just the actual reality that we have.

07:37.530 --> 07:43.290
So here we have an example once again of the actual observation of what's happening in the dream or

07:43.340 --> 07:44.430
directions.

07:45.150 --> 07:50.400
So that's in a nutshell the purpose of introducing Mauritian and quoters as you can see it's a very

07:50.400 --> 07:58.500
very exciting idea very exciting approach to solving this challenge and to helping our AI learn.

07:58.590 --> 08:04.200
And in the falling tutorials we learn intuition behind version all that and coders and then you will

08:04.350 --> 08:08.600
code them in practical practical tutorials together for lunch.

08:08.790 --> 08:12.840
On that note I hope enjoyed today's tutorial and I look forward to seeing you back here next.

08:12.960 --> 08:14.650
Until then enjoy.

08:14.690 --> 08:15.000
I.
