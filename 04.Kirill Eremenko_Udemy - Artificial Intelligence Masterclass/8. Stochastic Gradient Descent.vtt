WEBVTT

00:01.050 --> 00:03.310
Hello and welcome back to the course on deep learning.

00:03.330 --> 00:07.150
Today we talk about stochastic gradient descent.

00:07.170 --> 00:14.400
Previously we learned about gradient descent and we found out that it is a very efficient method to

00:14.400 --> 00:19.560
solve our optimization problem where we are trying to minimize the cost function.

00:19.590 --> 00:28.980
It basically takes us from 10 to the power of 57 years to solving a problem within minutes or hours

00:29.430 --> 00:30.870
or within a day or so.

00:31.020 --> 00:37.440
And it really helps speed things up because we can see which way is downhill and we can just go in that

00:37.440 --> 00:41.350
direction and take steps and get to the minimum faster.

00:41.550 --> 00:49.980
But the thing with the stick with gradient descent is that this method requires for the cost function

00:49.980 --> 00:50.930
to be convex.

00:51.060 --> 00:57.660
And as you can see here we've specifically chosen a convex cost function basically convex means that

00:58.110 --> 01:05.460
the function looks similar to what we are seeing now that it's just kind of vext into one direction

01:05.460 --> 01:09.170
and that in essence has one global minimum.

01:09.330 --> 01:11.550
And that's the one that we're going to find.

01:11.580 --> 01:14.010
But what if our function is not convex.

01:14.010 --> 01:16.240
What if our cost function is not convex.

01:16.320 --> 01:17.750
What if it looks like this.

01:17.970 --> 01:19.610
Well first of all how could that happen.

01:19.830 --> 01:27.900
Well that could happen because if we first of all choose a cost function which is not the squared difference

01:27.930 --> 01:33.800
between why how and why or if we do choose the cost function which is like that.

01:33.810 --> 01:39.290
But then in a multi dimensional space it can actually turn into something that is not convex.

01:39.720 --> 01:45.360
And so what would happen in this case if we just tried to apply our normal gradient decent method something

01:45.360 --> 01:46.320
like this could happen.

01:46.440 --> 01:51.180
We could find a local minimum of the cost function rather than the global one.

01:51.180 --> 01:57.680
So this one was the best one and we found the wrong one and therefore we don't have the correct weight.

01:57.690 --> 02:00.010
We don't have an optimized neural network.

02:00.180 --> 02:02.420
We have a subpar neural network.

02:02.560 --> 02:04.420
And so what do we do in this case.

02:04.620 --> 02:09.990
Well the answer here is stochastic gradient descent.

02:10.020 --> 02:15.150
And it turns out the sarcastic gradient descent doesn't require for the cause function to be convex.

02:15.330 --> 02:20.070
So let's have a look at the two differences between the normal gradient descent that we talked about

02:20.100 --> 02:21.560
and the stochastic range.

02:21.810 --> 02:27.870
So normal green descent is when we take all of our rows we plug them into our whole network and once

02:27.870 --> 02:33.840
again here we've got the neural network copied over several times but the rows are being plugged into

02:33.840 --> 02:35.940
that same neural network every time.

02:35.970 --> 02:39.160
So there's only one year old and this is just for Weasel's action purposes.

02:39.300 --> 02:43.830
And then once we plug them in we've calculated our cost function based on the formula right and looking

02:43.830 --> 02:49.350
at the chart on the at the bottom and then we adjust the weights then this is called the gradient descent

02:49.350 --> 02:54.420
method or it's also the proper term is the Bachche gradient descent method.

02:54.420 --> 03:01.890
So we take the whole batch of from our sample we apply it and then we run that the stochastic gradient

03:01.890 --> 03:03.650
descent method is a bit different.

03:03.750 --> 03:07.030
Here we take the rose one by one so we take this row.

03:07.140 --> 03:14.010
We run our neural network and then we adjust the weights then we move onto the second row we take the

03:14.010 --> 03:16.270
second row we run our neural network.

03:16.530 --> 03:21.530
We look at the cost function and then we adjust the weights again and then we take another Rohtak or

03:21.530 --> 03:22.350
three.

03:22.620 --> 03:25.370
We run our neural network will look at the cost function we adjust just it.

03:25.380 --> 03:32.610
So basically we're looking at we're adjusting the weights after every single row rather than doing everything

03:32.610 --> 03:36.180
together and then adjusting weights to different approaches.

03:36.180 --> 03:39.660
And now we're going to just compare the two side by side.

03:39.660 --> 03:42.860
So here they are this is how to visually remember them.

03:42.870 --> 03:49.440
So you've got bashed gradient descent where you are adjusting the weights after you've run them after

03:49.440 --> 03:55.080
you've run all of the rows in your neural network and then basically you adjust the weights and you

03:55.080 --> 03:59.940
run the whole thing again iteration iteration iteration in the sixth grade in December that you run

03:59.940 --> 04:06.270
one row at a time and you just the way it's just a way to adjust the weights and then you do everything

04:06.270 --> 04:07.550
again and again.

04:07.770 --> 04:09.990
And that is called discussing.

04:10.030 --> 04:16.530
And you said that the main two differences are that the sarcastic gradient descent method helps you

04:16.530 --> 04:27.420
avoid the problem where you find those local extremities or local minimums rather than the overall overall

04:27.420 --> 04:28.890
global minimum.

04:28.980 --> 04:34.770
And the reason for that in simple terms is that the video of the stochastic gradient descent method

04:35.100 --> 04:38.150
has much higher fluctuations because it can afford them.

04:38.160 --> 04:43.590
It's doing one iteration or one row at a time and therefore the fluctuations are much higher and it

04:43.590 --> 04:49.390
is much more likely to find the global minimum rather than just the local minimum.

04:49.410 --> 04:56.420
And the other thing about the stochastic gradient descent I think is a batch gradient is that it's faster

04:56.430 --> 05:01.610
like the first impression you might have is because it's doing every single role one at a time is slower

05:01.700 --> 05:09.020
but actually in fact it is faster because it is it doesn't have to load up all the data into memory

05:09.050 --> 05:12.550
and run and wait until all those roles are run altogether.

05:12.650 --> 05:16.760
You can just roll around them one by one so it's a much lighter algorithm is much faster in that sense

05:16.760 --> 05:24.060
so though it has way more and that's in those senses it has more advantages over the bad.

05:24.060 --> 05:25.280
Gradient descent method.

05:25.370 --> 05:31.190
The main advantage of or domain kind of like profer the badge gradient descent method is that it is

05:31.190 --> 05:37.400
a deterministic algorithm or than a sixth grade in dissent being a sarcastic algorithm meaning it's

05:37.400 --> 05:44.630
random and with the best gradient decent method as long as you have the same starting weights for your

05:44.630 --> 05:51.340
neural network every time you run the batch gradient descent method you will get the same iterations

05:51.350 --> 05:57.230
the same results for you for the way your weights are being updated for us to have for the sarcastic

05:57.230 --> 06:02.350
gradient descent method you won't get that because it is a stochastic method you're picking your roles

06:02.360 --> 06:08.690
it possibly are random and you are updating your neural network in a sarcastic manner and therefore

06:09.740 --> 06:13.730
you're just going to every single time you run the sixth grader and do some method even if you have

06:13.730 --> 06:15.250
the same weights at the start.

06:15.260 --> 06:20.700
You're going to have a different process different iterations to get there.

06:20.720 --> 06:28.040
So that's in a nutshell what's to castigate and dissent is also there's a method in-between the two

06:28.040 --> 06:34.460
called the Mini Bachche gradient descent method where you combine the two and you basically run rather

06:34.460 --> 06:37.590
than running a whole batch of running one at a time.

06:37.610 --> 06:44.990
You run batches of rows maybe 5 10 or 100 over Munro's you decide to set those number of rows at a time

06:45.230 --> 06:47.640
then you update your way single digits and so on.

06:47.870 --> 06:52.610
And that's called the Mini Bachche gradient descent method if you'd like to learn more about gradient

06:52.610 --> 06:52.960
descent.

06:52.970 --> 06:56.590
There's a great article which you can have a look at.

06:56.600 --> 07:04.880
It's called in neural network in 13 lines of Python part two gradient descent by Andrew Trask and the

07:04.880 --> 07:12.690
links below it's an good hub 12:15 article very well-written very very simple terms.

07:12.830 --> 07:22.400
It's got some interesting philosophical or just interesting thoughts on how to apply green decent water.

07:22.400 --> 07:28.290
You know that vantages and disadvantage and how to be how to do things in certain situations so you

07:28.290 --> 07:32.000
got some very cool tips tricks and hacks very easy read.

07:32.000 --> 07:33.720
So definitely check that out.

07:33.740 --> 07:36.960
And another one a bit more heavier read.

07:36.980 --> 07:42.260
For those of you who are intimate thematics who want to get to the bottom of the mathematics why gradient

07:42.260 --> 07:44.980
descent is that specific.

07:45.210 --> 07:49.130
What are the formulas that are driving gradients and how is it calculate and so on.

07:49.160 --> 07:51.550
Check out the article or actually the book.

07:51.590 --> 07:57.050
It's a free online book called neural networks and deep learning by Michael Nielsen 2050 book.

07:57.050 --> 08:04.760
It's just basically it's all on line you can go ahead and check it out there and there again very soft

08:04.760 --> 08:05.810
introduction to the mathematics.

08:05.810 --> 08:11.900
But then for Mother Earth the math but the mathematics are pretty heavy as you go along as you read

08:11.900 --> 08:13.430
through the article.

08:13.550 --> 08:20.210
But at the same time it gets you into into that mood I think you mean has like a warm up chapter where

08:20.210 --> 08:25.310
you first warm up for the math and then you jump into I'm so interested in math and this is the article

08:25.310 --> 08:32.330
to go to and there we go so that's in a nutshell the difference between Graney sense to cast a gradient

08:32.330 --> 08:36.330
descent and how to work.

08:36.380 --> 08:39.840
And on that note we're going to wrap up today's tutorial.

08:39.860 --> 08:41.950
I look forward to seeing you on the next one.

08:41.960 --> 08:44.030
And until then enjoy deep learning.
