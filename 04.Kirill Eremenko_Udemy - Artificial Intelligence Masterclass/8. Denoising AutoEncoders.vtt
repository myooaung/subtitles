WEBVTT

00:01.030 --> 00:06.010
Hello and welcome back to the course on deep learning today we're talking about invoicing out and to

00:06.320 --> 00:15.010
nosing out and coder's is another regularisation technique which is here to combat the problem of when

00:15.100 --> 00:21.190
we have more nodes in the hidden lair than in the input there and therefore the outer encoder can simply

00:21.370 --> 00:27.010
just copy these values across without finding any meaningful features and undergoing the training that

00:27.040 --> 00:28.280
we wanted to undergo.

00:28.480 --> 00:32.800
And so what are we going to do here is we're going to take these input values and we're going to move

00:32.800 --> 00:35.380
them to the left and replace them with something else.

00:35.620 --> 00:39.310
And this something else is a modified version of our input values.

00:39.340 --> 00:43.330
So let's say we have input values X1 sixty three and x 4.

00:43.450 --> 00:49.360
Well what are we going to do is we're going to take these inputs and randomly out of them.

00:49.360 --> 00:53.120
We're going to turn some of them into zeros just like that.

00:53.200 --> 00:59.620
And it's a parameter you can specify in your in the setup of your own code or it can be for instance

00:59.620 --> 01:03.900
half of your inputs that you have are turned into zeros every single time.

01:03.910 --> 01:05.220
And it happens randomly.

01:05.230 --> 01:08.430
So it's every single possible.

01:08.440 --> 01:10.320
It can be different variables.

01:10.510 --> 01:16.870
And then once you put this data through your Ardern quarter which you do in the end is you compare the

01:16.870 --> 01:21.040
output not with the modified values but with the original values.

01:21.040 --> 01:28.180
And that prevents the order encoder from simply just copying those that data or those inputs all the

01:28.180 --> 01:32.980
way through to the outputs because it's actually comparing the output not with the noisy but with the

01:32.980 --> 01:34.640
original inputs.

01:34.780 --> 01:38.160
And that helps combat the problem that we are facing.

01:38.380 --> 01:44.560
And also it's important to note here that because this happens randomly this type of auto encoder is

01:44.560 --> 01:46.270
a sarcastic or encoder.

01:46.360 --> 01:52.030
So basically it depends on this random generation or random selection of which values are going to be

01:52.030 --> 01:58.240
zeroed out and so it just becomes a stochastic type of oughtnt quarter.

01:58.450 --> 02:02.560
So there we go that's how that's how noisy out and coder's work.

02:02.560 --> 02:08.020
Also quite a popular technique you will hear about it and you will come across it if you're going to

02:08.050 --> 02:14.650
delve into the world of out and Connors and in terms of additional reading here is a great paper by

02:14.650 --> 02:17.320
Pascal even sent and others 2008.

02:17.380 --> 02:22.540
It's called extracting and composing robust features with invoicing out and coders exactly as you can

02:22.540 --> 02:26.400
see from the image exactly what we spoke about.

02:26.410 --> 02:30.280
So there we go that's annoying out in coder's and I look forward to seeing you next time.

02:30.280 --> 02:31.960
Until then enjoy learning.
