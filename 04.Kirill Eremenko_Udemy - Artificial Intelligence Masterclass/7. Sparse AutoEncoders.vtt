WEBVTT

00:00.640 --> 00:05.760
Hello and welcome back to the course on deep learning today we're talking about sparse adn cotters.

00:06.040 --> 00:12.610
So as we previously discussed we are aiming to create an encoder where the hidden Lair is actually greater

00:12.610 --> 00:13.810
than the input layer.

00:14.020 --> 00:19.810
And the reason for that is we want to extract more features and we know that in colors our design is

00:19.810 --> 00:24.970
a great feature extraction tool and the whole concept is pretty solid when we want our outputs to equate

00:24.970 --> 00:26.140
to our inputs.

00:26.200 --> 00:32.470
But in this case we want to extract more features than just three or just two or we want to text and

00:32.560 --> 00:35.200
and an unrestricted amount of features.

00:35.410 --> 00:41.560
And here what we're seeing is that basically as soon as we do that the art encoder can cheat it can

00:41.560 --> 00:48.190
basically just take these inputs put them through the hidden layer and and put them as outputs and without

00:48.640 --> 00:53.500
actually creating any valuable feature extraction tool within itself.

00:53.500 --> 00:57.580
During a training process just because it's going to be easier for doubt and coder to do this.

00:57.720 --> 01:01.460
And so we have to come up with a way to prevent it from doing that.

01:01.510 --> 01:06.010
And the first way we're looking at is called the sparse or encoder.

01:06.100 --> 01:14.440
And what I wanted to mention here is that you will hear sparse or Ann Coulter a lot like a lot a lot.

01:14.440 --> 01:22.330
It is used everywhere or pretty much everywhere in literature and very often you will just hear sparser

01:22.330 --> 01:27.420
and coater and people won't even explain why it's a sparser encoder quarter what that means.

01:27.550 --> 01:34.540
And it just is sometimes even used interchangeably with an out and quarter as soon as you notice that

01:34.570 --> 01:39.120
you have to be aware of what you're dealing with if you're dealing with not doubt and color that we're

01:39.170 --> 01:39.870
working with before.

01:39.880 --> 01:44.090
But this type of art and color and that's why this tutorial so important.

01:44.170 --> 01:49.390
So what is a sparse art in color as sparser and color is an ordered color which looks like this where

01:49.390 --> 01:56.710
the hidden layer is greater than the input Lehre but a regularisation technique which introduces sparsity

01:56.950 --> 02:02.290
has been applied a regularisation technique basically means something that helps prevent overfitting

02:02.290 --> 02:04.440
or stabilizes the algorithm in this case.

02:04.620 --> 02:11.980
If it was just sending the values through it would be overfitting in you know in a way and therefore

02:12.440 --> 02:16.150
we need regularisation techniques and this sparse out and is one of them.

02:16.210 --> 02:23.140
And basically what it does is it just says it introduces a constraint on the last function or a penalty

02:23.140 --> 02:28.750
in the last function and which doesn't allow the Ardern code to use all of its hidden layer every single

02:28.750 --> 02:29.040
time.

02:29.050 --> 02:37.850
So every time or any time any time at all so any at any time the outin code can only use a certain number

02:37.850 --> 02:42.700
a number of nodes from its hidden lair for instance it can use two nodes in this case.

02:42.730 --> 02:50.680
And so when the values go through these nodes are outputting very very small values so or very tiny

02:50.680 --> 02:53.110
values which are not which are insignificant.

02:53.110 --> 02:58.630
So in sound putting in significant values and therefore only these nodes are actually participating

02:58.990 --> 03:05.020
then and then in another pass these knowability participate in other parts these nodes will be participating.

03:05.020 --> 03:07.550
So you are extracting features.

03:07.550 --> 03:12.760
The essence is that you are at the end of the day because you have so many rows in your data set that

03:12.760 --> 03:16.960
are going through at the end of the day you are training this whole there so you are extracting features

03:16.960 --> 03:19.860
from each one of these nodes through each one of these notes.

03:19.870 --> 03:26.800
But at the same time not at at any given pass you're not using all of these so our encoder cannot cheat

03:26.860 --> 03:32.410
because even though it has more nodes in the hidden lair than in the input layer it is not able to use

03:32.410 --> 03:34.240
all of them at any given postes.

03:34.390 --> 03:37.300
And that's how the spores are in code it works.

03:37.390 --> 03:38.800
Pretty straightforward technique.

03:38.870 --> 03:40.740
Of course it has mathematics behind it.

03:40.840 --> 03:44.430
And we'll just now look through some further reading that you can do.

03:44.440 --> 03:52.090
But in essence it's quite a simple idea quite a straightforward idea to still keep the size the large

03:52.090 --> 03:58.960
size of the hidden lair but at the same time not allow the Aughton corner to bypass the proper training

03:58.960 --> 04:00.640
that we wanted to undergo.

04:01.030 --> 04:05.140
And that's that's what US posture our encounter is so whenever you hear splosh our own kowtows Just

04:05.140 --> 04:07.370
remember that it's this kind of thing.

04:07.510 --> 04:12.910
And in reality when you think about it it is still compressing the information.

04:12.910 --> 04:15.080
But just every time it's using different notes.

04:15.280 --> 04:15.770
OK.

04:15.910 --> 04:23.190
So let's have a look at the additional reading so we've got a interesting Tauriel here by Chris McCormick.

04:23.200 --> 04:29.880
It's called deep learning tutorial spots are and coater and this is a Matlab tutorial so if there's

04:29.920 --> 04:33.500
any fans of matlab if you're a fan of Matlab you might find this interesting.

04:33.670 --> 04:38.240
Who walks you through how to do it in matlab but other than that there's just I like it because there's

04:38.260 --> 04:46.240
some introductory mathematics not too complex but some basic level formulas which allow you to gently

04:46.240 --> 04:50.540
get introduced to the mathematics behind the sparse out and.

04:50.560 --> 04:56.860
If you want to and understand how that whole how the equations work when we don't allow the R and quarter

04:56.860 --> 05:04.250
to switch on all of its nose at old and at the same at any given point at any given past the next one

05:04.250 --> 05:08.700
is called Deep learning sparse out in chorus by Eric Wilkinson.

05:08.720 --> 05:14.270
It's a very short blog post on the essence of sparser and courters.

05:14.300 --> 05:21.440
It's literally like a five to 10 minute read and just another gentle introduction to the world of sparse

05:21.440 --> 05:22.520
our own cotters.

05:22.670 --> 05:29.160
And finally a very strong powerful heavy artillery paper on sparse on its own in kowtows is called case

05:29.490 --> 05:31.300
out and called her by Alireza.

05:31.340 --> 05:41.270
Mike zuÃ±i 2014 and it basically takes his whole sparse out and color concept to a whole new level where

05:41.360 --> 05:47.070
it talks about different like a parameter K which allows you to control the sparsity of the out encoder

05:47.120 --> 05:50.250
and you got some examples here on the right.

05:50.510 --> 05:55.550
You might find this paper interesting if you want to learn more about parson courters actually would

05:55.550 --> 06:02.660
encourage you to learn a bit more because it is a tool that is used all over the place and is constantly

06:02.660 --> 06:07.100
mentioned so you will definitely come across sparser and coder's if you are going to be dealing with

06:07.190 --> 06:08.230
Aaton cars.

06:08.510 --> 06:13.310
OK so that's sparser and kowtows and I look farseeing and XM.

06:13.360 --> 06:15.010
Until then enjoy learning.
