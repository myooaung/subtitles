WEBVTT

00:01.790 --> 00:05.330
Hello and welcome back to the course on deep learning today we're going to wrap up.

00:05.330 --> 00:07.260
We're back propagation.

00:07.270 --> 00:11.410
All right so we already know pretty much everything we need to know about what happens in the neural

00:11.400 --> 00:11.850
net.

00:12.050 --> 00:18.560
We know that there's a process called for propagation where information is entered into the input layer

00:18.590 --> 00:25.100
and then it's propagated forward to get our white hats our output values and then we compare those to

00:25.100 --> 00:28.980
the actual values that we have in our training set.

00:29.180 --> 00:36.050
And then we calculate the errors then the errors are back propagated through the network in the opposite

00:36.050 --> 00:41.540
direction and that allows us to train the network by adjusting the weights.

00:41.600 --> 00:49.610
So the one key important thing to remember here is that back propagation is an advanced algorithm driven

00:49.610 --> 00:58.850
by very interesting and sophisticated mathematics which allows us to adjust the weights.

00:58.970 --> 01:02.440
All of them at the same time all the weights are just it's a multi-year sea.

01:02.480 --> 01:08.930
So if we were doing this manually or if we were coming up a very different type of algorithm than Even

01:08.930 --> 01:14.090
if we calculated the error and then we were trying to understand what effect each of the weights has

01:14.090 --> 01:20.910
on the error we'd have to somehow adjust each of the weights independent independently or individually.

01:21.930 --> 01:28.900
The huge advantage of backwardation and this is a key thing to remember is that during the process of

01:28.900 --> 01:35.990
back propagation simply because of the way the algorithm is structured.

01:36.800 --> 01:43.940
You are able to adjust all the way to the same time so you basically know which part of the error each

01:43.940 --> 01:47.310
of your weights in the neural network is responsible for.

01:47.370 --> 01:54.160
Now that is the key fundamental underlying principle of back propagation.

01:54.170 --> 02:02.610
And this was why it picked up so rapidly in the 1980s and this was a major breakthrough.

02:02.720 --> 02:08.840
And if you'd like to learn more about that and how exactly the mathematics works in the background then

02:09.140 --> 02:14.750
a good article which we've already mentioned is the neural networks and deep learning is actually a

02:14.750 --> 02:21.890
book by Michael Nielsen The You'll find the mathematics written out and it will help you understand

02:22.220 --> 02:23.590
how exactly this is possible.

02:23.600 --> 02:30.500
But for now for our purposes if from an intuition point of view the important part is to remember that

02:31.190 --> 02:33.260
that's what backwardation does.

02:33.260 --> 02:36.370
It adjusts all of the weights at the same time.

02:36.890 --> 02:43.250
And now we're going to just wrap everything up with a step by step walkthrough of what happens in the

02:43.250 --> 02:45.320
training of a neural network.

02:45.320 --> 02:50.960
All right so step one we randomly initialized the weights to small numbers close to zero but not zero.

02:50.960 --> 02:56.780
We didn't really focus on the initialization of weights during the initial tutorials but then we have

02:56.780 --> 03:02.570
to start somewhere and they are initialized with random values near zero.

03:02.570 --> 03:09.650
And from there through the process for propagation by propagation these weights are adjusted until the

03:09.650 --> 03:11.460
error is minimized.

03:11.920 --> 03:13.490
So the cost function is minimized.

03:13.770 --> 03:19.250
Then step two inputs the first observation all your data sets to the first row into the input Lehre

03:19.430 --> 03:21.390
each feature is one input now.

03:21.380 --> 03:27.440
So basically take the columns and put them into the input nodes separately for propagation from left

03:27.440 --> 03:27.860
to right.

03:27.860 --> 03:32.570
The neurons are activated in a way that they pick in our vision neurons activation is limited by the

03:32.570 --> 03:39.110
weights the weights to basically determine how important each neurons activation is then propagate the

03:39.110 --> 03:43.000
activation until getting the predicted result y hat.

03:43.070 --> 03:43.800
In this case.

03:43.940 --> 03:46.590
So basically you propagate from left to right.

03:46.640 --> 03:49.910
You go all the way until you get to the and you get your y hat.

03:50.270 --> 03:52.660
Then compare the result to the actual result.

03:52.700 --> 03:58.580
Measure the generated error and then you do the backwardation from right to left the air is back propagated.

03:58.580 --> 04:02.000
Update the weights according to how much they are responsible for the error.

04:02.180 --> 04:08.450
Again you are able to calculate that because of the way the back propagated perturbation algorithm is

04:08.450 --> 04:09.220
structured.

04:09.470 --> 04:13.850
The learning rate decides by how much we update the weights the learning rate as parameter.

04:13.940 --> 04:17.570
You can control in your neural network.

04:17.660 --> 04:23.080
Step 6 repeat steps 1 2 5 and update the weights after each observation.

04:23.270 --> 04:24.830
That is called reinforcement learning.

04:24.830 --> 04:31.440
And in our case that was stochastic gradient descent or repeat steps 1 to 5.

04:31.450 --> 04:37.790
But the way it's only after Basho observation so batched learning it's either full gradient descent

04:37.820 --> 04:39.570
or badge green Nissan or mini batch.

04:39.570 --> 04:45.290
Gradient descent and step seven when the whole train had passed through an artificial neural network

04:45.710 --> 04:48.960
that makes an epoch redo more epochs.

04:48.980 --> 04:51.890
So basically just keep doing that and doing that and doing that.

04:52.370 --> 04:58.370
And to allow your neural network to train better and better and better and constantly adjust itself

05:00.230 --> 05:02.470
as you minimize the cost function.

05:02.690 --> 05:04.360
So there we go.

05:04.370 --> 05:09.730
Those are the steps you need to take to build your artificial neural networks and train it.

05:09.980 --> 05:15.950
And these are the steps that you will be taken to have had on in the practical tutorials.

05:16.070 --> 05:19.490
We should the best of luck and I look forward to seeing you next time.

05:19.490 --> 05:21.230
Until then enjoy deep learning.
