WEBVTT

00:00.660 --> 00:06.060
Hello and welcome back to the A.I.M. class in today's tutorial we're talking about the memorization

00:06.060 --> 00:08.690
trick for rational Ardern coders.

00:08.880 --> 00:10.510
So let's have a look.

00:10.590 --> 00:12.810
Here we have our variational or Ann Coulter.

00:12.900 --> 00:20.830
And as we discussed previously effectivity this is an online coder where we replaced the bottleneck

00:20.940 --> 00:26.060
which is sort of one latent act on its own with a distribution.

00:26.100 --> 00:32.340
So now we're taking our inputs and rather than mapping them straight onto the latent vector we're mapping

00:32.340 --> 00:35.830
them onto a mean vector in a standard deviation vector from.

00:35.830 --> 00:40.750
So which which are actually which constitute our distribution.

00:40.820 --> 00:47.080
And from this distribution we sample the late vector so every time we sample laserdiscs or we get a

00:47.130 --> 00:52.610
slightly varying result and then we use the sample Laken vector to perform the decoding.

00:52.770 --> 01:00.470
So that allows us to accomplish that took a city or randomness that we are looking for in the variational

01:00.480 --> 01:01.680
or encoder.

01:01.890 --> 01:08.700
Now how do you actually train a variation on code or what will happen when we run back for progression.

01:08.700 --> 01:14.070
Well this is what will happen when we're running backwardation goes here and then right away as soon

01:14.070 --> 01:20.230
as it gets to the simple plate and Vector it's going to encounter a problem because there is still a

01:20.610 --> 01:29.520
in this latent vector and so basically we're sampling it and we cannot run back propagation through

01:29.610 --> 01:32.580
this the caustic latent vector.

01:32.580 --> 01:33.600
So that's a problem.

01:33.600 --> 01:36.320
And how do we resolve this problem.

01:36.600 --> 01:40.870
Well this is where the reaper ization trick comes in handy.

01:40.950 --> 01:43.910
What is suggested is quite a simple solution.

01:44.100 --> 01:49.120
What we're going to do is we're going to take this latent vector sample data vector and we're going

01:49.120 --> 01:51.850
to we're presented in the following way.

01:52.170 --> 01:59.520
Zed our sample data vector is actually going to be a combination of the mean vector the myu plus the

01:59.520 --> 02:05.360
standard deviation of Sigma Wasfi by Epsilon where Epsilon is actually going to be the stochastic node.

02:05.550 --> 02:12.480
So rather than performing a 660 here or performing the sampling here in this latent vector itself we're

02:12.480 --> 02:21.570
going to separate out the sampling into its own separate node and that way is going to allow us to run

02:21.570 --> 02:22.420
by provision.

02:22.590 --> 02:29.880
So let's have a look in a schematic or at a schematic or a presentation of this so we can so it becomes

02:29.880 --> 02:30.980
a bit clearer.

02:31.320 --> 02:34.960
So here is our original version of our.

02:35.100 --> 02:43.620
As you can see we're taking the mean vector and standard deviation vector we're using them inside the

02:45.420 --> 02:47.600
latent vector of the sample plate and vector.

02:47.600 --> 02:53.190
We're using them inside here to perform the sampling of the latent vector and then we're proceeding

02:53.280 --> 02:54.580
with the decoding.

02:54.930 --> 03:01.230
Whereas in the now so that's where the problem occurs we cannot go back for progression can go through

03:01.230 --> 03:02.330
this sticker system.

03:02.700 --> 03:09.980
So this is where the repurposed ization trick comes in handy comes to the rescue because what we're

03:09.990 --> 03:18.180
doing now is rather than doing the sampling within this node within this vector we're actually separating

03:18.180 --> 03:21.880
out the sampling into its own separate node.

03:22.050 --> 03:30.790
So Epsilon is just going to be a way for us to introduce randomness into this equation so it's just

03:30.800 --> 03:34.080
great to be a standard deviation is that a distribution offer.

03:34.150 --> 03:36.750
I mean all the zeros and division of one.

03:36.900 --> 03:42.030
There's nothing special about it we don't actually need to train it whereas everything else that we

03:42.030 --> 03:48.390
do want to train is easily accessible tabac propagations about arrogation just simply runs through it

03:48.420 --> 03:50.430
because there is no randomness here.

03:50.580 --> 03:56.850
In fact the randomness is separate out in its own node and as we can see here we don't need to worry

03:56.850 --> 04:03.590
about this we don't want to train it anyway whereas everything we do want to train is very very straightforward

04:03.600 --> 04:05.070
for backwardation.

04:05.190 --> 04:08.710
So that's how the repair mineralization trick works in a nutshell.

04:08.880 --> 04:13.770
You'll actually see all this in the practical tutorial so when you'll be flying alone along with our

04:13.800 --> 04:21.610
love to the code you will create this move vector the Sigma vector the epsilon vector and then all of

04:21.760 --> 04:26.670
the code that you put it together you'll just remember this diagram and it will all it all come together

04:26.670 --> 04:28.450
in your mind as well.

04:29.280 --> 04:33.110
And so that's the intuition behind them ization.

04:33.150 --> 04:39.420
If you'd like to learn more about it then there's a paper on archive which is called auto encoding variational

04:39.420 --> 04:42.700
Beys by Kingma and Wehling.

04:42.870 --> 04:45.460
And this is where there are privatization trick.

04:45.450 --> 04:48.320
Originally first was introduced.

04:48.540 --> 04:50.340
So check it out if you want to learn more.

04:50.370 --> 04:52.820
And I look forward to seeing you back here next time.

04:52.830 --> 04:55.510
Until then enjoy artificial intelligence.
