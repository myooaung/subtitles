WEBVTT

00:00.210 --> 00:02.580
Hello and welcome back to the course on deep learning.

00:02.670 --> 00:05.080
All right today we're talking about the activation function.

00:05.130 --> 00:06.740
Let's get straight into it.

00:06.990 --> 00:11.970
So this is where we left off previously we talked about the structure of one neuron.

00:11.970 --> 00:16.710
So there it is in the middle we know that it has some inputs values coming in it's got some weights

00:17.070 --> 00:21.810
then it adds up the way to calculate the way that some of those inputs.

00:22.080 --> 00:28.800
And then unpleased activation function in step 3 it passes on the signal to the next year and and that's

00:28.800 --> 00:32.790
what we're talking about today we're talking about the value that is going to be passed over.

00:32.790 --> 00:35.910
So we're talking about the activation function that's being applied.

00:36.330 --> 00:39.240
So what options do we have for the activation function.

00:39.240 --> 00:43.380
Well we're going to look at four different types of activation functions that you can choose from.

00:43.380 --> 00:47.340
Of course there are more different types of activation function but these are the predominate ones that

00:47.340 --> 00:50.330
you'll be hearing about and that we'll be using in this course.

00:50.340 --> 00:53.000
So here is the threshold function.

00:53.010 --> 00:54.240
This is what it looks like.

00:54.240 --> 01:02.250
So on the x axis you have the way that some of inputs on the y axis you have just you know the values

01:02.670 --> 01:10.800
from 0 to 1 and basically the threshold function is a very simple type of function where if the value

01:10.800 --> 01:16.600
is less than zero then the fractional function passes on zero.

01:16.830 --> 01:22.890
If the value is more than zero or equal to zero then threshold function is on a 1.

01:22.890 --> 01:26.880
So it's basically kind of like yes no type of function.

01:26.880 --> 01:33.480
Very very straightforward very kind of like rigid type of function either yes or no.

01:33.480 --> 01:34.960
No other options.

01:35.010 --> 01:35.460
So there you go.

01:35.460 --> 01:36.150
That's how it works.

01:36.150 --> 01:37.290
Very simple function.

01:37.410 --> 01:40.420
Let's move on to something a bit more complex now.

01:40.610 --> 01:48.660
This sigmoid function very interesting formula that we have here you'll see just now there is one divide

01:48.660 --> 01:49.910
by one plus each.

01:49.920 --> 01:58.280
The power of minus X whereas in this case of course X is the value of the summed of the way that sums.

01:58.590 --> 02:00.500
And so yeah.

02:00.510 --> 02:02.550
So this is what the sigmoid looks like.

02:02.550 --> 02:06.440
It's a function which is used in the logistic regression.

02:06.450 --> 02:09.220
If you recall from the machine learning course.

02:09.480 --> 02:14.910
So what is good about this function is that it is smooth unlike the virtual function.

02:14.910 --> 02:21.660
This one doesn't have those kinks in its curve and therefore it's just nice and smooth gradual progression.

02:21.660 --> 02:26.290
So anything below 0 is just like drops off above zero.

02:26.290 --> 02:35.190
It acts approximates towards one and this sigmoid function is very useful in the final area in the output

02:35.190 --> 02:38.840
layer especially when you're trying to predict probabilities.

02:38.860 --> 02:40.860
And we'll see that throughout this course.

02:41.160 --> 02:47.310
And then we've got the rectifier function rectifier function even though it has a kink is one of the

02:47.310 --> 02:55.050
most popular functions for artificial neural networks so it goes all the way to zero it is zero.

02:55.050 --> 03:02.400
And then from there it's gradually progresses as the input value increases as well and we'll see that

03:02.400 --> 03:07.080
throughout the course we'll see that in other intuition tutorials and we also see that how we use this

03:07.080 --> 03:12.990
function in the practical side of the course and I will comment on this a bit more in a few slides from

03:12.990 --> 03:13.330
now.

03:13.530 --> 03:18.930
So just remember the direct fire function is one of the most used functions in artificial neural networks.

03:18.990 --> 03:22.710
And finally we've got one more function that you will probably hear about.

03:22.800 --> 03:25.180
It's the hyperbolic tangent function.

03:25.230 --> 03:32.360
It's very similar to the sigmoid function but here the hyperbolic tangent function goes below zero.

03:32.360 --> 03:39.560
So the values go from 0 to 1 or approximate to 1 and go from zero to minus one on the other side.

03:39.690 --> 03:42.130
And that can be useful in some applications.

03:42.330 --> 03:45.750
So we're not going to go into too much depth on each one of these functions.

03:45.750 --> 03:51.630
I just wanted to acquaint you with them so that you know what they look like and what they're called.

03:51.720 --> 04:02.140
If you'd like to get some additional reading then check out this paper by a 75 year lot of you lot called

04:02.140 --> 04:05.670
a deep sparse rectifying neural networks 2011 paper.

04:05.730 --> 04:14.670
And there you will find out exactly why the rectifier function is such a valuable function why it's

04:14.910 --> 04:16.290
so popularly used.

04:16.290 --> 04:20.610
But nevertheless for now you don't really need to know all of those things.

04:20.610 --> 04:24.480
For now we just going to start applying them when you start using them more and more and more and so

04:24.480 --> 04:31.560
when you feel comfortable with the practical side of things then you can go and refer to this paper

04:31.560 --> 04:37.240
and then you will be able to soak in that knowledge much quicker and you will make much more sense.

04:37.320 --> 04:41.930
But just keep this in mind that when you're ready when you feel that you're ready then you can go and

04:41.930 --> 04:45.080
refer to this paper and get some valuable knowledge from there.

04:45.480 --> 04:53.040
So just to quickly recap we have the threshold activation function which looks like this the sigmoid

04:53.070 --> 04:55.390
activation function which looks like this.

04:55.660 --> 05:00.230
We have the rectifier function and we have the hyperbolic tangent from.

05:00.460 --> 05:07.210
And now to finish off this tutorial Let's quickly do a few exercise so just do two quick exercises to

05:07.720 --> 05:09.100
help that knowledge sink in.

05:09.100 --> 05:15.090
So first one is we've got an example here of a neural network of just one neuron and that right away

05:15.110 --> 05:20.410
the outputs there and the question is assuming that your dependent variable is binary So it's either

05:20.410 --> 05:23.710
0 or 1 which threshold function would you use.

05:23.710 --> 05:31.930
So out of the ones that we've discussed we have a threshold function the sigmoid function the rectifier

05:31.930 --> 05:39.430
function and we've got the hyperbolic tangent function in it's in their roll forms which ones would

05:39.430 --> 05:43.410
you be able to use for a binary variable.

05:43.890 --> 05:44.350
OK.

05:44.440 --> 05:49.300
So the answers here are there's two options that we can approach this with.

05:49.300 --> 05:55.740
So one is the threshold activation function because we know that it's between 0 and 1 and it gives us

05:55.750 --> 06:00.580
0 under certain values and then otherwise it gives you once it only can give you two values it fits

06:00.580 --> 06:10.330
perfectly fits this requirement perfectly and therefore you can you say y equals the threshold function

06:10.360 --> 06:12.960
of your psuedo some.

06:13.000 --> 06:13.730
And that's it.

06:13.960 --> 06:18.390
And in the second case which you could use is the sigmoid activation function.

06:18.390 --> 06:21.660
It is actually also between 0 and 1 just what we need.

06:21.700 --> 06:28.930
But at the same time you want just one right so you it's not exactly what we need.

06:28.930 --> 06:37.440
But in this case which you could use it as is the probability of Y being yes or no.

06:37.480 --> 06:46.120
So we want Y to be 0 1 but instead we'll say that the sigmoid function Simoun activation function tells

06:46.120 --> 06:51.800
us whether it tells the probability of Y being equal to 1.

06:51.820 --> 06:59.080
So basically the closer you get to the top the more likely it is that this is indeed a one or a yes

06:59.110 --> 07:00.340
rather than a no.

07:00.750 --> 07:07.570
And yes so that's very similar to the logistic regression approach and those are just two examples.

07:07.600 --> 07:09.750
If you have a binary variable.

07:10.080 --> 07:12.760
Now let's have a look at another practical application.

07:12.760 --> 07:17.130
Let's have a look at how all this would play out if we had in your all natural like this.

07:17.380 --> 07:20.950
So in the first layer we have some inputs.

07:20.980 --> 07:26.010
They are sent off to our first hidden layer and then an activation function is applied.

07:26.020 --> 07:30.310
And usually what you would apply here and what you will see throughout this course is we would apply

07:30.310 --> 07:34.460
a rectifier activation function so it would look something like that.

07:34.480 --> 07:40.930
We apply the rectifier activation function and then from there the signals would be passed on to the

07:40.930 --> 07:46.770
output layer where the sigmoid activation function would be applied and that would be our final output.

07:46.780 --> 07:51.220
And that could predict a probability for instance so this combination is going to be quite common where

07:51.550 --> 07:58.820
in the hidden layers we apply the rectifier function and then output there we apply the sigmoid function.

07:58.840 --> 07:59.800
So there we go.

07:59.800 --> 08:04.990
Hope you enjoyed this tutorial now you are quite well versed in four different types of activation functions

08:04.990 --> 08:10.840
and you will get some hands on practical experience with them throughout this course we'll be using

08:10.840 --> 08:15.670
them all over the place so you'll get to know them quite intimately and you should be quite comfortable

08:15.670 --> 08:16.250
with them.

08:16.450 --> 08:22.180
But for now this is the knowledge that you need to progress and understand what he's going to be happening

08:22.210 --> 08:23.540
further down in this course.

08:23.890 --> 08:26.860
And on that note I will look forward to seeing you next time.

08:26.860 --> 08:28.510
Until then enjoy learning.
