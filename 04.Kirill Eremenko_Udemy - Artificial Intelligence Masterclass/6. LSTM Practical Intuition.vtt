WEBVTT

00:00.750 --> 00:06.510
Hello and welcome back to the course on deep learning now that we know a bit more about LACMA and how

00:06.510 --> 00:08.550
they work and what their architecture is like.

00:08.550 --> 00:15.150
Today we're going to look at some practical applications or specifically we're going to look at how

00:15.150 --> 00:20.300
Ellis DMD's work inside practical applications and is going to be quite interesting.

00:20.310 --> 00:28.950
And at the same time a bit magical I would say so let's get started here we've got the LSD architecture

00:29.610 --> 00:35.910
and we're going to be to start off we're going to be looking at this tangent function and how it fires

00:36.000 --> 00:37.440
up.

00:37.480 --> 00:45.780
So according to some of the images we're going to be using are all from Andre card party's blog.

00:45.780 --> 00:52.530
So thank you very much to Andre for doing all of this amazing research so you can see here it's just

00:52.680 --> 00:58.260
incredible so there got some examples where they created some bogusly in your algebra using trained

00:58.260 --> 01:03.420
up Alice DMD's and then lots and lots of stuff.

01:03.420 --> 01:07.970
So here these are the images we're going to be looking at just now these ones as well.

01:08.280 --> 01:12.330
And there's lots of information here lots of comments so definitely check it out.

01:12.330 --> 01:16.710
We'll link to these at the end of the tutorial.

01:16.740 --> 01:23.130
This blog is called the unreasonable effectiveness of recurrent neural networks and the paper that Andrii

01:23.130 --> 01:27.050
published along with that will also link to that paper as well.

01:27.060 --> 01:30.420
So basically we're looking at this tangent function to start off with.

01:30.420 --> 01:37.070
And according to the paper minus one means is going to be read plus one is going to be blue.

01:37.110 --> 01:38.620
So let's get started.

01:39.270 --> 01:47.670
All right so here's some text which was given to an R N N which learned to read text and kind of create

01:47.670 --> 01:49.980
text and predict what text is coming.

01:50.160 --> 01:56.900
And here this is a snippet from though War and Peace.

01:57.150 --> 02:01.140
Huge novel by Tolstoy.

02:01.230 --> 02:05.700
And so here you can see that this neuron is activating.

02:06.100 --> 02:10.920
Well it is sensitive to as it says here sensitive to position in line so you can see that when you get

02:10.920 --> 02:14.520
towards the end of the line it's activating and how does it know when it's end of the line.

02:14.520 --> 02:25.140
Well in this novel you have about 80 symbols per line approximately So it's counting how many symbols

02:25.140 --> 02:25.990
have passed.

02:26.280 --> 02:30.600
And that way it's trying to predict when the new line character is coming up because the New Line New

02:30.690 --> 02:32.360
Line is a just a character right.

02:32.380 --> 02:36.760
It's an invisible character and trying to predict where that character is going to appear.

02:36.810 --> 02:39.620
Then you've got a cell that turns on inside quotes.

02:39.780 --> 02:44.910
While this is inside cause I think it's actually outside quotes because here you see it says you mean

02:44.940 --> 02:47.480
to imply that I have nothing to eat out of.

02:47.670 --> 02:49.380
On the contrary I can supply.

02:49.500 --> 02:53.370
So basically somebody is talking and then warmly replied Chicago.

02:53.550 --> 02:55.210
She shrugged.

02:56.680 --> 02:59.410
She chag of Haven't read that in a long time.

02:59.940 --> 03:03.030
Who tried to buy who tried by every word.

03:03.030 --> 03:06.390
So basically this is the inside the courses outside the quotes.

03:06.390 --> 03:11.910
I'm not sure if this is correct but either case one way or another it's activating either inside the

03:11.910 --> 03:16.620
quotes or outside the quotes and you can see how this one is keeping track of what's happening and so

03:16.620 --> 03:23.310
very similar to what we discussed previously where we are keeping track of the subject when that would

03:23.310 --> 03:29.790
help us understand if the subject is male or female or we would be able to understand things like if

03:29.790 --> 03:33.590
it's a singular or plural and that would affect all verbs in our translation.

03:33.690 --> 03:39.780
Similar thing so it's important to know if you're inside or outside of course because that affects the

03:39.780 --> 03:40.520
rest of the text.

03:40.570 --> 03:45.900
And for instance the easiest way you can think of that effective text that if you're inside the quotes

03:45.900 --> 03:51.180
then there has to be another quote to close the quotes are you anticipating another quote something

03:51.180 --> 03:58.990
that here's another one where the what the input was the code.

03:59.000 --> 04:01.670
All of the Linux operating system.

04:01.680 --> 04:10.650
And here you can see that a cell activates inside if statements so it's completely dormant everywhere

04:10.650 --> 04:10.870
else.

04:10.890 --> 04:17.120
But as soon as you have an if statement it activates if statement to its statement it activates so and

04:17.250 --> 04:18.380
it is active.

04:18.390 --> 04:26.790
You can see it stops being active over here at the next actual A body of the if statement so it's only

04:26.790 --> 04:31.220
active for the main part or for the condition of the if statement.

04:31.230 --> 04:37.470
And you know that that can be important because you are anticipating that the condition is going to

04:37.740 --> 04:44.280
close the bracket and then it's going to be a bracket curly brace to open up the body of the if statement.

04:44.310 --> 04:51.270
That's pretty cool and then here you've got another one where this sensitive cell is sensitive to how

04:51.270 --> 04:54.240
deep you are inside of a nested expression.

04:55.130 --> 05:01.550
So as you go deeper and you get the expression we more and more nested this cell keeps track of that

05:01.550 --> 05:04.670
so it's using this memory to keep trying.

05:04.670 --> 05:11.690
And it's very important to remember that and none of this is actually hard coded into the neural network.

05:11.690 --> 05:14.440
All of this is learned by the network itself.

05:14.440 --> 05:17.120
There are thousands and thousands thousands of iterations.

05:17.120 --> 05:23.390
It learns that OK I have this many hidden states I have.

05:23.470 --> 05:29.110
And out of them I need to identify what's important in the text to keep track of and the identifies

05:29.120 --> 05:31.260
for instance in this case that it has.

05:31.280 --> 05:38.840
That's being understanding how deep you are inside and that statement is important and therefore it

05:39.000 --> 05:43.670
a science a one of its hidden states to keep track of that.

05:43.690 --> 05:52.790
So it has these resources hidden States or the actual the memory cells and it assigns them to keep track

05:52.790 --> 05:55.200
of certain things based on what it thinks is important.

05:55.200 --> 06:01.070
So it's like it's really evolving like that on its own and deciding what's important what's not and

06:01.070 --> 06:03.960
how to allocate its resources to best complete the task.

06:03.990 --> 06:07.360
I think that's very fascinating.

06:07.550 --> 06:11.710
And then here's an example of a cell that you can't really understand what it's doing.

06:11.720 --> 06:16.790
And according to drake or Pathi about 95 percent of the cells are like this.

06:16.790 --> 06:23.300
They're doing something but it's just not not obvious to us what is happening and it's like that example

06:23.300 --> 06:29.190
of CNN's where the filters or the features are looking out for there.

06:29.330 --> 06:33.890
By the time they get to the last layer they are completely unrecognizable to the human eye but they

06:33.890 --> 06:34.970
make sense to the machine.

06:34.970 --> 06:35.570
Same thing here.

06:35.570 --> 06:39.720
So most of the time line of time you can really tell what's going on.

06:39.740 --> 06:44.720
But those five some of the time those were the exam exams so we looked at and they should be helpful

06:44.720 --> 06:53.090
to better understand what is kind of going on in in the neural network when it's processing for instance

06:53.090 --> 06:54.820
text.

06:54.860 --> 06:58.940
So again now we're back at our architecture.

06:59.180 --> 07:05.240
And now what we're going to be looking at is are we going to be looking at the actual outputs so we're

07:05.240 --> 07:06.670
going to be looking at this value.

07:06.680 --> 07:12.680
So after it's gone through the tangent or evolve or the output gate and now we're going to be looking

07:12.680 --> 07:14.920
at what's being produced here.

07:14.990 --> 07:23.180
So here's an example again from car parties or undertake her panties blog and we're looking at a neural

07:23.270 --> 07:25.130
network that is reading.

07:25.130 --> 07:31.880
So this is a bit this is a bit more detailed so here it's not just showing us if it's active or not.

07:31.880 --> 07:35.790
You can see that you've got at the top if it's active or not.

07:35.810 --> 07:44.290
Every first line and then another five lines it is saying what's the neural network is predicting that's

07:44.300 --> 07:45.430
going to happen next.

07:45.470 --> 07:48.440
What the letter is going to come next what symbols is going to come next.

07:48.440 --> 07:50.650
So you've got the combination of the two here.

07:50.870 --> 07:57.950
And just by looking at this what what do you think is predicting so I'm going to I'm going to like outline

07:57.980 --> 08:04.550
the colors green means for the topline green means active and blue means not active and red means a

08:04.580 --> 08:05.840
very likely prediction.

08:05.990 --> 08:08.180
And light red means like prediction.

08:08.210 --> 08:10.750
So let's talk about the topline.

08:10.760 --> 08:16.580
What do you think this specific hidden state in the neural network is looking out for when do you think

08:16.580 --> 08:17.660
it's being activated.

08:17.660 --> 08:19.280
Well I guess this one is pretty obvious.

08:19.280 --> 08:21.830
It's activating inside your else.

08:21.890 --> 08:29.480
So here you can see that inside w w w the hidden state is B is being activated just like we saw in the

08:29.480 --> 08:34.130
previous examples foreign piece and the Linux kernel.

08:34.140 --> 08:38.810
Here you can see that it's been activated inside your else.

08:38.840 --> 08:41.780
But now we have some additional stuff to look at.

08:41.780 --> 08:45.140
Now we can see what it's actually predicting that's going to be the next character.

08:45.140 --> 08:51.170
So for instance under this w you can see that it's predicting that the next character is going to be

08:51.170 --> 08:54.590
a W of the highest probability and it's correct.

08:54.590 --> 08:59.670
It is doubly that under this bill you can see that it's predicting another W and it is correct.

08:59.810 --> 09:08.320
And then under this w the whole this is what the whole neural network is predicting this w what you're

09:08.530 --> 09:11.070
seeing is a dot and that is correct.

09:11.080 --> 09:11.970
It is predicted.

09:11.990 --> 09:20.130
So that's that's how it's gone up to here and then after that if it thinks the next letter letter B

09:20.140 --> 09:21.480
will b a b but it's actually a Y.

09:21.490 --> 09:25.860
But now knowing that it's a y it thinks they're going to be in a now knowing it's an age thing is going

09:25.870 --> 09:26.190
to be.

09:26.270 --> 09:28.980
And you can see that these are less.

09:29.080 --> 09:33.540
These are not as red as these because it's not sure about these predictions that's fair enough.

09:33.550 --> 09:35.190
It could be absolutely any website.

09:35.200 --> 09:36.490
Why would it be a B.

09:36.580 --> 09:38.790
Could be any upside can't you can't tell.

09:38.800 --> 09:42.930
Maybe it could based on the context but still is very hard.

09:42.960 --> 09:50.100
But then when it gets to wide net and new and E.W. it predicts and as if a very high likelihood and

09:50.100 --> 09:56.280
it is indeed an ass because it kind of knows that the word after you have new very likely you will have

09:56.280 --> 09:59.600
an s or based on everything it's learned before that.

09:59.640 --> 10:06.630
In this particular type of text its news is mentioned quite often because this is actually as far as

10:06.630 --> 10:12.370
I understand this is Wikipedia text and then you have a dot then after dot.

10:12.370 --> 10:13.220
It predicts this.

10:13.250 --> 10:20.950
See it is a C or A C prefix and O it's an O after Albrechtsen and it is an M and a slash and is a slash

10:21.340 --> 10:22.310
so beautiful so.

10:22.330 --> 10:25.840
And then it's done its job and it's not active anymore.

10:26.140 --> 10:27.430
And then this is what.

10:27.640 --> 10:32.110
But this is the neuron that we're looking at is not active on anymore but the neural network is still

10:32.110 --> 10:32.860
predicting things.

10:32.860 --> 10:34.810
So here you can see then.

10:34.980 --> 10:36.190
And so it's incorrect.

10:36.190 --> 10:39.300
And then after the and after that is incorrect at the end.

10:39.320 --> 10:44.170
So after he has and it predicts a G it's a G and then it's becomes more confident that it's the word

10:44.200 --> 10:50.500
English language you can see that the predictions become more confident so after the L you can't predict

10:50.500 --> 10:50.850
correctly.

10:50.860 --> 10:55.690
But after the L.A. It's really predicting and G and so on and so on so on so it can actually predict

10:55.690 --> 11:01.360
the whole word based on just a few first letters websites on and so on to see the actual neuron is still

11:01.360 --> 11:02.860
dormant still dormant.

11:02.860 --> 11:09.910
And then here you've got again off we go W.W. is predicting and this one is getting activated.

11:09.910 --> 11:13.110
This one was pretty interesting because it goes see.

11:13.180 --> 11:21.130
And then present and but it's not an M it's a dot and it's like the neural network or this is the neural

11:21.130 --> 11:26.210
network is probably a bit upset at this stage thinking Whoa Where's my arm.

11:26.410 --> 11:29.220
And then then it takes another shot it says OK.

11:29.230 --> 11:35.510
So probably you because dot Kodachi OK that's that's the UK website.

11:36.100 --> 11:45.910
But here instead of you it's got an eye and an owl for Israel that code i l and therefore it kind of

11:45.910 --> 11:48.900
like didn't guess this time but it probably is.

11:48.940 --> 11:53.080
It's not even trying to guess because he's already been trying to guess and I because from what it's

11:53.080 --> 12:00.010
learned that called that you k are made much more likely to come up than dot co that i l and we're not

12:00.010 --> 12:04.210
even looking at these are the ones here which you could look at them.

12:04.750 --> 12:09.320
They are you know the second best guess a third best guess fourth and fifth which you can see that they

12:09.320 --> 12:10.280
are not that red.

12:10.330 --> 12:20.420
And it would always put the highest likeliest guess at the top on the second line or all.

12:20.470 --> 12:25.590
So there we go this is how to look at these pictures that Andray has created.

12:25.590 --> 12:29.140
And for more check out his blog Carpathia get harder.

12:29.140 --> 12:34.870
I know there was a couple more of these examples there and more of the previous examples I looked at.

12:35.300 --> 12:43.330
And so hopefully this is helpful to show you what's what's going on inside the neural network when it's

12:43.420 --> 12:50.080
when it's thinking and when it's processing information in terms of references and additional reading

12:50.200 --> 12:57.250
we've got Andre Karpovsky blog and we'll link to it in the resources for this course.

12:57.250 --> 13:00.580
This is this is your L otherwise.

13:00.790 --> 13:09.400
And also we've got and raker Party and others research paper which was published in 2015 it's called

13:09.420 --> 13:12.910
visualizing and understanding recurrent networks.

13:12.970 --> 13:20.470
Very interesting read actually because like you if you're there's not too much math then you can probably

13:20.470 --> 13:20.920
skip it.

13:20.920 --> 13:26.740
But even the insights parts and chapters are very interesting.

13:26.740 --> 13:29.600
They make you kind of feel that way.

13:29.740 --> 13:33.910
And they say this in the paper that they like neuroscientists trying to understand what's going on so

13:33.910 --> 13:41.410
they open up the brain of the neural network and monitor what's happening in one specific neuron or

13:42.250 --> 13:43.880
other different or different neurons.

13:43.900 --> 13:51.130
And and you actually feel from the language the way that it's written that it's as if they're exploring

13:51.130 --> 13:57.220
some alien as if they're exploring some kind of extraterrestrial being and how it thinks.

13:57.220 --> 14:02.980
Because if you think about it humans created these Ls DMAs and are and and these are just things that

14:02.980 --> 14:10.510
work on our computers but because they are so advanced because they involve so many different elements

14:10.510 --> 14:18.040
so many different parts to them and they're so complex we now need to after we've created them now we

14:18.040 --> 14:25.150
need to study them as if they're separate beings separate something something that exists on its own

14:25.210 --> 14:31.630
and it's just reading through it's kind of like if you think of it that way gives you this mysterious

14:31.630 --> 14:35.500
feeling or magic if you feel like it's something magical is happening at the same time you feel that

14:35.850 --> 14:40.840
fume a few more years or maybe a decade from now these things are going to be able to think completely

14:40.840 --> 14:41.840
on their own.

14:41.860 --> 14:47.570
So if you want to have some fun reading a research paper this one's pretty cool I think.

14:48.750 --> 14:51.400
Maybe you know you don't have to read the math skip the math.

14:51.400 --> 14:54.820
I didn't I didn't really dig into the math myself.

14:55.110 --> 14:57.030
Yeah so that's pretty much it.

14:57.030 --> 15:02.490
Hope you enjoyed today's tutorial and hopefully that gives you a bit of a better understanding of how

15:02.880 --> 15:06.660
the architecture actually plays out in practice.

15:06.660 --> 15:08.130
And I look forward to seeing you next step.

15:08.250 --> 15:10.030
Until then enjoy deep learning.
