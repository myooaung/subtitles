WEBVTT

00:00.330 --> 00:06.870
Hello and welcome back to the course on deep learning to do it just to quickly cover all of the variations

00:06.870 --> 00:09.780
of long term memory architectures.

00:09.780 --> 00:16.980
I think it's important so that you released aware that there are other alternatives to the Ellis Tim

00:16.980 --> 00:20.870
version that we looked at and that you might encounter them sometime in your work.

00:20.880 --> 00:27.690
So here is the standard LACMA which we discussed it should be it should be pretty comfortable fit by

00:27.690 --> 00:28.250
now.

00:28.480 --> 00:32.630
And now let's have a look at a couple of variations some variations.

00:32.640 --> 00:34.900
One is when you add peepholes.

00:34.920 --> 00:44.070
So I'm going to go back again so you see these lines are added connecting these sigmoid activation functions

00:44.070 --> 00:53.610
or these layer neural network layer operations additional input is being provided it's information about

00:53.610 --> 00:56.050
the current state of the memory cell.

00:56.220 --> 01:02.970
And that basically allows kind of like a Is that's what's called peephole allowance allows these decisions

01:02.970 --> 01:09.160
about the valves to be made with taking into account what is actually sitting there in the memory.

01:09.780 --> 01:15.810
Verification number two is when you take these two valves The Forget valve and the memory valve and

01:15.810 --> 01:16.830
you connect them.

01:16.830 --> 01:23.970
So let's go back instead of having a separate decision for the memory valve.

01:24.000 --> 01:29.190
Now you have a combined decision for the forget forgetful and the memory valve and basically when whenever

01:29.190 --> 01:34.620
you add something into memory so whenever you close the memory off when ever this is zero this becomes

01:34.800 --> 01:38.060
the opposite one month zero becomes a 1 so you have to put something in.

01:38.160 --> 01:43.740
If you put this if you make it a one or close to one it becomes zero and therefore you don't put anything

01:43.740 --> 01:44.630
in here.

01:44.640 --> 01:45.730
You just keep the old memory.

01:45.810 --> 01:49.470
So it just makes sense to combine them sometimes.

01:49.830 --> 01:58.020
And then another modification a very popular modification called the good recurring units G.R. use for

01:58.020 --> 01:59.780
short.

01:59.820 --> 02:05.550
They what they do is they completely get rid of the C pipeline as you can see the memory problem and

02:05.550 --> 02:11.390
they replace it with the H which is the hidden pipeline which we had before at the bottom.

02:11.490 --> 02:12.900
They just combined basically the two.

02:12.900 --> 02:19.470
So now instead of having two separate values one for the memory and one for the hidden state.

02:19.500 --> 02:27.180
Now you have one and make it simplifies things so it's probably a bit less flexible but in terms of

02:27.270 --> 02:32.180
how many things are being controlled and monitored there there's actually less.

02:32.180 --> 02:34.090
And and that's it.

02:34.110 --> 02:39.120
And that's this is harder to get a recurring unit works as you can see it might look a bit more can

02:39.150 --> 02:45.030
do but in reality it's a bit simpler you only have you know this this valve.

02:45.720 --> 02:50.180
Then you have another valve here another valve here but these two are currently connected as well.

02:50.190 --> 02:53.090
So this is in essence the get recurring unit.

02:53.100 --> 03:01.410
The concept behind it is to get rid of the memory cell and just have this one pipeline going through

03:01.410 --> 03:04.070
there that takes care of everything.

03:04.080 --> 03:04.590
So there we go.

03:04.590 --> 03:08.680
That's the three Alycia variations.

03:08.680 --> 03:10.750
There's there's lots more other ones.

03:10.930 --> 03:19.670
A good paper to check out is called LSD him a search space odyssey by Claus graph and others 2015.

03:19.980 --> 03:26.710
So there they compared quite a few different LACMA you might like this research that they did.

03:27.060 --> 03:27.540
So there you go.

03:27.540 --> 03:34.200
That's a short intro into the alternative architectures of the LS team and I look forward to seeing

03:34.200 --> 03:34.980
you next time.

03:34.980 --> 03:36.410
Until then enjoy deep learning.
