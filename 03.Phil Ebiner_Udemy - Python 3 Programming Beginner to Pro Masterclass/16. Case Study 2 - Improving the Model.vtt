WEBVTT
1
00:00:03.850 --> 00:00:06.390
Let's take a look on how to improve our armada.

2
00:00:06.740 --> 00:00:13.600
We just started as Jupiter notebook our model accuracy wasn't that great was actually terrible.

3
00:00:13.770 --> 00:00:16.470
Let's slicker look on how can we improve our model.

4
00:00:16.510 --> 00:00:18.200
The first step that we need to perform.

5
00:00:18.200 --> 00:00:19.740
It's been a normalization.

6
00:00:19.840 --> 00:00:20.450
Okay.

7
00:00:20.710 --> 00:00:26.290
So as guess Candy corn for example you know part of the Mean area or versed in the mean smoothness as

8
00:00:26.290 --> 00:00:30.790
you can see here the mean smoothness is let's say is that actually but it's more numbers from point

9
00:00:30.850 --> 00:00:32.850
0 6 let's say to point 1 6.

10
00:00:33.040 --> 00:00:36.100
However the mean is in 25 countries OK.

11
00:00:36.400 --> 00:00:42.120
This data is actually going to cause a lot of problems when we train our machine learning model.

12
00:00:42.340 --> 00:00:49.090
And that's why one of the key elements in here is to try to kind of normalize our data which is in study

13
00:00:49.090 --> 00:00:50.090
simple terms.

14
00:00:50.080 --> 00:00:51.460
We're going to use what we call it Unity.

15
00:00:51.500 --> 00:00:56.760
Unity is normalization which is we wanted to get all the data to be from between zero and one.

16
00:00:56.770 --> 00:00:58.150
So all objective.

17
00:00:58.180 --> 00:01:02.810
So I'm going to show you how can you do then inject an old book in a bit but just a quick overview.

18
00:01:02.810 --> 00:01:04.130
This is again the main area.

19
00:01:04.130 --> 00:01:06.590
This means more than this these are just larger values in here.

20
00:01:06.700 --> 00:01:11.120
Here as you can see after normalization you'll find them in areas from between 0 and 1.

21
00:01:11.320 --> 00:01:17.830
And again that means more this again between 0 and 1 or 8 and again zero indicates the most malignant

22
00:01:17.830 --> 00:01:20.510
case that malignant cancer.

23
00:01:20.530 --> 00:01:23.330
And if it's one that means it's benign.

24
00:01:23.560 --> 00:01:26.620
How can we do with the unitaid normalization.

25
00:01:26.800 --> 00:01:33.430
Well we call it features kealing we simply subtract each of the sample points by the minimum value and

26
00:01:33.430 --> 00:01:36.470
we divide by the range or the X Max minus X min.

27
00:01:36.510 --> 00:01:40.920
That would tell us that would give us kind of norm by the data between 0 and 1.

28
00:01:41.230 --> 00:01:46.570
It's really simple and we'll see how can we apply them idealisation you can improve the results of the

29
00:01:46.570 --> 00:01:48.410
model dramatically correct.

30
00:01:48.950 --> 00:01:52.630
Okay so the next optimization now can do.

31
00:01:52.630 --> 00:01:55.640
Can you do in the as the ATM.

32
00:01:55.930 --> 00:01:59.470
When we use the as VM we use kind of the default premise.

33
00:01:59.480 --> 00:02:05.530
Okay we didn't use any of the pretender's we just call as far as VM it was just you know it was just

34
00:02:05.530 --> 00:02:06.560
fine.

35
00:02:06.760 --> 00:02:09.920
Obviously the results were there great because we were an optimist.

36
00:02:09.920 --> 00:02:10.370
All right.

37
00:02:10.600 --> 00:02:16.770
So we have to keep put Eminem is actually optimized during the during the process which we're excited

38
00:02:16.770 --> 00:02:19.320
to see premiers and gamma premiers.

39
00:02:19.420 --> 00:02:20.490
OK.

40
00:02:20.890 --> 00:02:23.290
So the first payment is we'll call it the see Potamia.

41
00:02:23.470 --> 00:02:30.310
Okay premiered simply control the trade off between classifying taining points correctly and having

42
00:02:30.310 --> 00:02:32.150
kind of a spoof decision bounty.

43
00:02:32.260 --> 00:02:33.350
Okay okay.

44
00:02:33.430 --> 00:02:37.240
It's might seem a little bit like it's changed but again it's really simple.

45
00:02:37.240 --> 00:02:44.270
So the whole idea is we can use a mirror which is an image that can kind of specifies the penalty okay

46
00:02:44.530 --> 00:02:49.990
well how can we penalize the model if it misclassify is a data point OK.

47
00:02:50.440 --> 00:02:53.500
So if we use is small C then you cocky.

48
00:02:53.530 --> 00:02:58.520
That means we are assuming that our model or our penalty is kind of loose.

49
00:02:58.570 --> 00:03:00.430
It's kind of a cool teacher in a way.

50
00:03:00.460 --> 00:03:01.050
OK.

51
00:03:01.210 --> 00:03:09.910
So if if the model classifies the asset misclassify or dataset we're going to assume that okay we're

52
00:03:09.910 --> 00:03:15.640
not going to penalize you that much that means we can have kind of a smaller boundary between the two

53
00:03:15.640 --> 00:03:18.020
classification points between the two classes.

54
00:03:18.160 --> 00:03:18.690
OK.

55
00:03:19.100 --> 00:03:21.080
That's when we have a small value of c..

56
00:03:21.130 --> 00:03:26.800
However if we have a lot of the value of C that means we going have a very strict model that means the

57
00:03:26.800 --> 00:03:32.230
cost of misclassification going to be very high which means if we do something wrong if we misclassify

58
00:03:32.260 --> 00:03:33.560
any of these points.

59
00:03:33.850 --> 00:03:40.810
We need to kind of bend our boundary a little bit more which means we're going have kind of more detailed

60
00:03:40.820 --> 00:03:42.580
or more defined line in here.

61
00:03:42.790 --> 00:03:46.390
Not a line anymore kind of you have kind of more.

62
00:03:47.820 --> 00:03:53.980
Strict or more decision line they kind of fitted specifically for this training data set.

63
00:03:54.360 --> 00:03:59.400
So this is kind of when we have a smaller value of c and this when they have a larger venue of seats

64
00:03:59.410 --> 00:04:05.160
and have a larger value of c we can actually be prone to overfitting which means we are overfilling

65
00:04:05.280 --> 00:04:10.600
our training specifically for the training data set which means if we expose that model to it training

66
00:04:10.840 --> 00:04:15.570
with data which is kind of testing data that the mother hasn't seen before then it might actually be

67
00:04:15.570 --> 00:04:18.390
prone to misclassifying which means why.

68
00:04:18.390 --> 00:04:23.260
Because the model is not trained specifically or to generalize on that in a dataset.

69
00:04:23.580 --> 00:04:30.840
As you can see here if we try to change for example the C value from it's a 1 to 10 to 100 and a thousand.

70
00:04:30.930 --> 00:04:35.910
You see that the boundary layer starts from being very smooth which is when we have a small value of

71
00:04:35.910 --> 00:04:40.890
c which is kind of we had a very low penalty if you do something wrong that's fine we're just going

72
00:04:40.890 --> 00:04:47.070
to keep it kind of small that as much as we can get to actually more eight or larger value of c which

73
00:04:47.070 --> 00:04:48.980
is a very strict boundary.

74
00:04:49.190 --> 00:04:49.560
OK.

75
00:04:49.590 --> 00:04:53.730
Which means that okay if we do something wrong we're going to penalize very high.

76
00:04:53.940 --> 00:05:00.310
And that's why you're going to come up with this kind of more of a fit at a stricter boundary line.

77
00:05:00.510 --> 00:05:00.910
Right.

78
00:05:01.200 --> 00:05:03.200
So that's the first pre-eminent quality parameter.

79
00:05:03.360 --> 00:05:03.780
Right.

80
00:05:03.990 --> 00:05:05.490
Let's take a look at the other Premier.

81
00:05:05.490 --> 00:05:07.420
It's called again an MRI.

82
00:05:07.950 --> 00:05:13.910
So the game a parameter controls how far the influence of a single teaching said teachers.

83
00:05:14.020 --> 00:05:14.450
OK.

84
00:05:14.730 --> 00:05:21.060
We'll just kind of specify the spread of our influence of the data points if we use a large get the

85
00:05:21.690 --> 00:05:22.920
larger value of gamma.

86
00:05:22.980 --> 00:05:24.990
That means we've got to have a close reach.

87
00:05:24.990 --> 00:05:25.980
What do you mean.

88
00:05:25.980 --> 00:05:30.610
That means we're going to focus only on the points very close to the hype really claim.

89
00:05:30.730 --> 00:05:34.850
Me mean these points in here are our points of interest.

90
00:05:34.850 --> 00:05:36.280
They have much higher weight.

91
00:05:36.360 --> 00:05:36.930
OK.

92
00:05:37.320 --> 00:05:42.330
Which means that okay if we have a larger gamma what do you expect are going to be more generalized

93
00:05:42.360 --> 00:05:48.360
or more a fit it actually going to be way more of a fit which means that a lot of lines in here will

94
00:05:48.360 --> 00:05:54.720
get you focussed on these points and ignoring the other away points from the from the hyperbolic.

95
00:05:54.870 --> 00:05:55.520
Right.

96
00:05:56.010 --> 00:05:56.490
OK.

97
00:05:56.800 --> 00:05:57.880
That's the first part.

98
00:05:57.990 --> 00:06:02.450
If we have larger gamma and that's why we can end up with like a line like vitz.

99
00:06:02.460 --> 00:06:07.800
Basically if we have bloche name however if we have a small gamma that means we're going to have far

100
00:06:07.880 --> 00:06:13.520
if we have small game that means we have far each we'd have kind of a more generalised solution.

101
00:06:13.890 --> 00:06:18.920
As you can see here the points all of the influence of the points are going to be kind of widespread.

102
00:06:19.110 --> 00:06:23.340
So we're not going to consider only the point that only the point of the boundary actually going to

103
00:06:23.340 --> 00:06:25.190
consider way more points.

104
00:06:25.230 --> 00:06:25.390
OK.

105
00:06:25.410 --> 00:06:30.220
We're going to cover a lot more space so we can achieve more generalized solution.

106
00:06:30.480 --> 00:06:35.460
But the key question is how can we optimize these polymers normalization is fine we can do normalization

107
00:06:35.460 --> 00:06:37.690
are going to show you how can we do not know zation.

108
00:06:37.740 --> 00:06:41.380
The question is how can we optimize the sea ingame upon MLA.

109
00:06:41.670 --> 00:06:47.880
Luckily we're going to show you that there is a technique that's really easy you can use kind of a grid

110
00:06:47.880 --> 00:06:48.660
search.

111
00:06:48.660 --> 00:06:51.310
We're not going to go and actually try you know like by trial and error.

112
00:06:51.450 --> 00:06:53.000
We're just going to write in kind of a grid search.

113
00:06:53.040 --> 00:06:53.620
Get us okay.

114
00:06:53.640 --> 00:06:59.560
This is the best see best game for our weather for our model specifically for our data.

115
00:06:59.580 --> 00:07:00.870
And then we're good to go.

116
00:07:01.080 --> 00:07:05.980
And then you can have way better accuracy moving forward compared to the terrible actually accuracy

117
00:07:06.000 --> 00:07:08.900
that we had before as we saw.

118
00:07:08.940 --> 00:07:12.570
We're going to implement merely two improvements within the model.

119
00:07:12.600 --> 00:07:17.850
The first improvement they're going to perform analyzation which is simply scaling back the data to

120
00:07:18.390 --> 00:07:22.400
be all the data to be values for ranging from zilah to 1.

121
00:07:22.560 --> 00:07:31.350
And then the next improvement we're going to tune the we see or supported to classify to simply tune

122
00:07:31.350 --> 00:07:34.540
the game a parameter and see parameters as we saw earlier.

123
00:07:34.670 --> 00:07:38.750
Okay let's take a look at the first model improvement first.

124
00:07:38.790 --> 00:07:42.310
So the first step is that we're going to do if we're going to perform normalization.

125
00:07:42.380 --> 00:07:43.190
Okay.

126
00:07:43.200 --> 00:07:45.080
So I'm going to do that first.

127
00:07:45.090 --> 00:07:50.890
We're going to give the men value of all the men off exchange first.

128
00:07:51.000 --> 00:07:57.210
So going to normalize the exchange rate are so extreme that men are going to the myth method method

129
00:07:57.210 --> 00:07:59.730
men that just simply give us the minimum value.

130
00:07:59.730 --> 00:08:00.900
Okay.

131
00:08:01.110 --> 00:08:07.050
And then the next step is to get in the range which is simply to opt in the range as they were paying

132
00:08:07.050 --> 00:08:12.870
the men then you can subtract the training data minus minimum value and divide by the range that we

133
00:08:12.900 --> 00:08:17.000
pretty much scale our our our objection.

134
00:08:17.100 --> 00:08:22.080
The next step we're going to define reach underscored clean obvious who can do it in one step here I'm

135
00:08:22.080 --> 00:08:24.040
just doing it in multiple steps.

136
00:08:24.090 --> 00:08:27.070
We're going to do it that we're going to subtract extreme.

137
00:08:27.940 --> 00:08:33.640
Minus men underscored which is what defined earlier in Oregon to do with that because we need to get

138
00:08:33.640 --> 00:08:36.290
the range which is going to you the message method Max.

139
00:08:36.310 --> 00:08:38.640
To simply give me the maximum value.

140
00:08:38.920 --> 00:08:43.480
So now we have the maximum value now we have the range simply now have the minimum value let's actually

141
00:08:43.480 --> 00:08:44.440
do this kealing.

142
00:08:44.440 --> 00:08:49.150
So going to do X underscore mean we're going to call it scale and scale.

143
00:08:49.320 --> 00:08:51.090
You can call it whatever you want obviously.

144
00:08:51.550 --> 00:08:58.270
And then afterwards we're going to do that we're going to subtract X train which has ended up point

145
00:08:58.810 --> 00:09:06.940
minus minimum with the squad team and we get divided by the range of discounting right strong this gain

146
00:09:06.940 --> 00:09:07.400
out good.

147
00:09:07.410 --> 00:09:12.950
So now we have X team scale which is perfect let's make sure that we are on the right pitch.

148
00:09:13.000 --> 00:09:19.210
So the first step is that we are going to simply we know the scatterplot which is as an S or simply

149
00:09:19.240 --> 00:09:25.640
seaborne we plotted before the mean area and the smooth means smoothness before the scatterplot.

150
00:09:25.660 --> 00:09:27.530
The visualization section.

151
00:09:27.850 --> 00:09:32.170
All right so let's me showed that we actually did this kealing correctly.

152
00:09:32.320 --> 00:09:32.640
Okay.

153
00:09:32.660 --> 00:09:34.720
Or did the normalizations correctly.

154
00:09:34.720 --> 00:09:39.790
The first step we were going to first plot the data using seaboard.

155
00:09:40.000 --> 00:09:40.880
So were going to do again.

156
00:09:40.900 --> 00:09:43.040
I'm just copying the command here to see this simple.

157
00:09:43.150 --> 00:09:44.780
So just going to the scatterplot.

158
00:09:44.830 --> 00:09:45.340
Okay.

159
00:09:45.640 --> 00:09:51.270
Well we did that we were in the Amine area versus mean smoothness and selected hue as being Whiting.

160
00:09:51.370 --> 00:09:51.860
Okay.

161
00:09:52.210 --> 00:09:53.460
Let's run this.

162
00:09:53.480 --> 00:09:56.640
Let's show us the actual training data with this killing.

163
00:09:56.760 --> 00:09:57.400
OK.

164
00:09:57.760 --> 00:10:02.620
What we could do is that we again you can see here the value of the areas that are inching your way

165
00:10:02.630 --> 00:10:02.880
around.

166
00:10:02.890 --> 00:10:07.100
Two hundred twenty five hundred which is not acceptable.

167
00:10:07.150 --> 00:10:12.760
What we're going to do with we're going to run again the same value the same man but without extreme

168
00:10:13.060 --> 00:10:15.180
missioning extreme scale.

169
00:10:15.770 --> 00:10:20.300
And why an extreme skill.

170
00:10:21.040 --> 00:10:22.330
That means scaling.

171
00:10:22.390 --> 00:10:24.310
So as you can see here is peanuts.

172
00:10:24.350 --> 00:10:26.990
The data is exactly similar.

173
00:10:27.170 --> 00:10:29.230
However the range is totally different.

174
00:10:29.380 --> 00:10:31.800
So really all the data is gone from zero to one.

175
00:10:31.840 --> 00:10:32.400
OK.

176
00:10:32.710 --> 00:10:39.760
So now we just did like a feature scaling or we did not unity normalization and now we're pretty much

177
00:10:39.760 --> 00:10:45.420
ready to actually change the dataset or change the model again using the new normalized training.

178
00:10:45.790 --> 00:10:46.300
Right.

179
00:10:46.550 --> 00:10:47.080
Right.

180
00:10:47.110 --> 00:10:51.470
Let's perform the same features scaling or at the same normalization again.

181
00:10:51.470 --> 00:10:53.350
But on the testing dataset.

182
00:10:53.350 --> 00:10:53.830
All right.

183
00:10:54.100 --> 00:10:59.240
So I'm just going to pretty much copy the same comments again here.

184
00:10:59.500 --> 00:11:04.660
So we'll see there is excessed again upping the minimum value give me the mint us again you can do it

185
00:11:04.660 --> 00:11:10.960
in one one one step and then we get the extras minus the minimum value and then we get the maximum out

186
00:11:10.960 --> 00:11:16.710
of all this was shippable I mean the range and then we're going to scale testing the asset next test

187
00:11:16.720 --> 00:11:17.770
minus menteri to buy.

188
00:11:17.800 --> 00:11:22.330
Angel give me the x deste scale and again if we're on it then we're good.

189
00:11:22.450 --> 00:11:25.740
And the next step is that we want to again team the model.

190
00:11:25.840 --> 00:11:32.170
But instead of turning it again with the data we're going to have data we're going to use these killed

191
00:11:32.970 --> 00:11:35.740
normalized the asset to 10 or more.

192
00:11:36.120 --> 00:11:37.950
All right let's run it again.

193
00:11:38.110 --> 00:11:42.910
So again it is see model which is the exact same model that we had before.

194
00:11:43.360 --> 00:11:44.040
Okay.

195
00:11:44.080 --> 00:11:51.230
Actually as we see underscoring model and it fits but instead we actually use x underscored clean underscores

196
00:11:51.240 --> 00:11:51.990
scale.

197
00:11:52.280 --> 00:11:57.470
Right along with y and just courting gay that will simply fit our model.

198
00:11:57.520 --> 00:11:57.740
OK.

199
00:11:57.760 --> 00:12:03.660
So we fit the model now what good the next step is that we wanted to again do the predictions to confusion

200
00:12:03.670 --> 00:12:05.330
matrix and to the heat map.

201
00:12:05.470 --> 00:12:11.850
Let's go up and let's copy the comments so we have our why predicts.

202
00:12:12.220 --> 00:12:13.570
So now we're in the model.

203
00:12:14.240 --> 00:12:17.300
It's a new couple of new cells right.

204
00:12:17.350 --> 00:12:23.260
So let's evaluate our new model using the new data set that we came before so we're going to do they're

205
00:12:23.260 --> 00:12:29.980
going to write why and just sort of predict and we're going to use the again predict method on our object

206
00:12:30.040 --> 00:12:33.930
as we see underscored model dot predict.

207
00:12:34.420 --> 00:12:41.220
Instead this time of using the XT test we're actually going to 8 x test scale dataset.

208
00:12:41.410 --> 00:12:42.440
Right.

209
00:12:42.700 --> 00:12:44.940
So this know what to get.

210
00:12:45.070 --> 00:12:46.910
Actually this creates some new cells.

211
00:12:47.170 --> 00:12:50.830
The next step I'm going to use a confusing matrix that we have exactly the same as we've done before

212
00:12:50.940 --> 00:12:53.960
and I think equates to confusion.

213
00:12:55.450 --> 00:12:57.720
Underscore matrix.

214
00:12:57.950 --> 00:13:02.540
And they're going to use our tool value which is why test is our tool values.

215
00:13:02.750 --> 00:13:08.070
And then going to use y underscore to predict which are simply the predictions that we'll put in here.

216
00:13:08.180 --> 00:13:11.150
All right let's Granites So now we're good.

217
00:13:11.330 --> 00:13:16.490
The next step that we wanted to plot our heat map we're going to the seaborne library s and astarte

218
00:13:16.500 --> 00:13:17.840
HEAT MAP.

219
00:13:17.850 --> 00:13:18.380
Right.

220
00:13:18.620 --> 00:13:19.510
What we're going to do.

221
00:13:19.660 --> 00:13:22.670
We're going to right going to select our confusion matrix.

222
00:13:22.690 --> 00:13:28.900
We're going to enable the innovations to be simply too so we can actually view the numbers right.

223
00:13:30.740 --> 00:13:32.700
I think we are good.

224
00:13:32.710 --> 00:13:33.960
Let's give it a shot.

225
00:13:34.300 --> 00:13:35.830
Okay perfect.

226
00:13:35.830 --> 00:13:43.150
Now we actually have way better results if you guys can see we have 43 correct classification.

227
00:13:43.180 --> 00:13:49.090
We have 66 correct classification and we've only classified as misclassify five samples ONDI which is

228
00:13:49.180 --> 00:13:49.750
great.

229
00:13:49.750 --> 00:13:52.540
That means we are doing absolutely great.

230
00:13:52.540 --> 00:13:57.450
The next step is we're going to do it the way to plot classification.

231
00:13:57.570 --> 00:13:58.760
All kind of clustered.

232
00:13:58.800 --> 00:14:04.790
Give us like you know kind of a summary of our performance right and then do like print.

233
00:14:04.870 --> 00:14:07.980
And they're going to use classification board just took less.

234
00:14:08.290 --> 00:14:13.990
Just to remind you goes to we had our when we imported the metrics we imported confusion meetings and

235
00:14:13.990 --> 00:14:16.270
we put our classification to report as well.

236
00:14:16.270 --> 00:14:16.720
All right.

237
00:14:16.750 --> 00:14:17.900
If you go down here.

238
00:14:18.250 --> 00:14:24.490
We're just going to print our classification report and they're going to use our mainly to values along

239
00:14:24.490 --> 00:14:26.130
with the predictions.

240
00:14:26.170 --> 00:14:32.510
Going to why underscored tests and why underscore predict which is our predictions.

241
00:14:32.730 --> 00:14:38.450
Let's give it a shot that would tell me our accuracy is that one point nine six which is great.

242
00:14:38.740 --> 00:14:43.350
Which is kind of you know like a summary of our performance which is 96 percent this action or.

243
00:14:43.540 --> 00:14:45.100
Actually we haven't done anything different.

244
00:14:45.100 --> 00:14:49.060
We haven't even tuned the parameters of the support to the machine.

245
00:14:49.090 --> 00:14:52.380
What we did is we just we did normalization.

246
00:14:52.390 --> 00:14:53.830
That's all what we wanted.

247
00:14:53.920 --> 00:14:55.820
That's kind of a first improvement to our model.

248
00:14:56.050 --> 00:14:58.900
I hope you get in touch with our section and see you in the next section.

249
00:14:59.140 --> 00:15:04.300
The next improvement that we're going to do to the model there will try to optimize our See and get

250
00:15:04.300 --> 00:15:06.860
more prone areas of the supported victim machine.

251
00:15:07.040 --> 00:15:07.540
Right.

252
00:15:07.810 --> 00:15:13.690
So the only problem is how can we actually search for the best values of the best premolars obviously

253
00:15:13.750 --> 00:15:14.380
and.

254
00:15:14.980 --> 00:15:21.970
Luckily Escuela actually had a kind of a method that tells us or provides all the great search for us

255
00:15:22.160 --> 00:15:25.240
it's search for the best premières and you don't have to worry about it.

256
00:15:25.240 --> 00:15:28.340
We're just going to tell yawkey this that we're best this is our best estimate.

257
00:15:28.510 --> 00:15:29.740
It can just use it.

258
00:15:29.760 --> 00:15:35.650
And let's give a sh let's give it a shot and let's see how can we search for the best pramila is the

259
00:15:35.650 --> 00:15:42.150
first step is we're going to call simply I'm going to define our range.

260
00:15:42.370 --> 00:15:44.180
So going to the final college program.

261
00:15:44.500 --> 00:15:49.030
And we're going to defy okay we're going to look into the sea will now try to optimize or see preliminary

262
00:15:49.830 --> 00:15:50.880
game parameter.

263
00:15:51.170 --> 00:15:51.540
Okay.

264
00:15:51.550 --> 00:15:56.770
And he what are you as a colonel before radio regular basis function and he we're going to specify simply

265
00:15:57.130 --> 00:16:01.630
our range talking going to look from point 1 1 10 and hundred and gamma.

266
00:16:01.660 --> 00:16:06.240
We're going to be simply we're going to create our grid to look so forth to look for.

267
00:16:06.490 --> 00:16:12.150
You're looking at between let's say point one one point one point or one point or one rate.

268
00:16:12.590 --> 00:16:13.200
Okay.

269
00:16:13.600 --> 00:16:19.560
The next step is we're going to import the grid search CB from escolar.

270
00:16:19.570 --> 00:16:28.940
So going to right from from scaler duct model selection we're going to click Import.

271
00:16:29.140 --> 00:16:31.710
Great search.

272
00:16:32.760 --> 00:16:35.370
CV all right.

273
00:16:35.760 --> 00:16:37.290
Okay so now we're good.

274
00:16:37.290 --> 00:16:45.210
The next step is they're going to use great search CV to simply search within our great Alright so I'm

275
00:16:45.210 --> 00:16:49.680
gonna do that we're going to simply use our VCR object.

276
00:16:49.980 --> 00:16:50.270
Right.

277
00:16:50.280 --> 00:16:54.100
Which is a stand for that support to classify an organ.

278
00:16:54.100 --> 00:16:58.380
Do you think any use our grade that were created before which is premiered it underscored grids which

279
00:16:58.380 --> 00:16:59.860
is our great that we had.

280
00:17:00.020 --> 00:17:06.160
And you were just going to light fettes equates to and verbose.

281
00:17:06.180 --> 00:17:13.170
We're going to specify two for fibre or whatever simply that we're just going to show us how many values

282
00:17:13.170 --> 00:17:16.430
that we wanted to display while searching for our Great.

283
00:17:16.500 --> 00:17:17.150
Okay.

284
00:17:17.650 --> 00:17:19.600
And I'm going to show you exactly what I mean by this.

285
00:17:19.850 --> 00:17:20.130
OK.

286
00:17:20.170 --> 00:17:22.380
That would simply return our grid.

287
00:17:22.530 --> 00:17:23.250
Okay.

288
00:17:23.460 --> 00:17:30.990
The next step is that we've got to simply use our grid to fit the taining data which is in this case.

289
00:17:30.990 --> 00:17:37.230
I'm going to use my scale training data to actually get even better values compared to the original

290
00:17:37.230 --> 00:17:38.080
data that we had.

291
00:17:38.130 --> 00:17:38.660
All right.

292
00:17:39.030 --> 00:17:43.200
So we're going to do they're going to use grids is our object that we had.

293
00:17:43.200 --> 00:17:43.550
Great.

294
00:17:43.560 --> 00:17:50.470
When you see fit method on our grid simply using the x underscored train just discourse scale.

295
00:17:53.070 --> 00:17:55.410
Along with y underscored T.

296
00:17:55.690 --> 00:17:56.460
All right.

297
00:17:56.850 --> 00:17:57.750
Okay great.

298
00:17:57.750 --> 00:17:58.900
Let's run it.

299
00:17:58.930 --> 00:18:01.680
All right so let's run it's going to run Prem great.

300
00:18:01.680 --> 00:18:06.510
And then what on the import and then I'm going to run the grid search.

301
00:18:06.570 --> 00:18:11.470
And then afterwards we're going to run the fit simply using the 16 scale and the lighting.

302
00:18:11.470 --> 00:18:12.190
All right.

303
00:18:12.420 --> 00:18:18.960
So if we scroll down here it would show you Dave has been searching for the best you know values of

304
00:18:18.960 --> 00:18:28.240
gamma and C and what we need to do is we need to get our best values simply using grids.

305
00:18:28.410 --> 00:18:28.650
OK.

306
00:18:28.680 --> 00:18:37.710
So we're going to do we're going to use grid dots best underscored prism frames that would show us our

307
00:18:37.710 --> 00:18:44.200
best value so you're telling me okay the best values to choose is actually if you see of 10 and gamma

308
00:18:44.220 --> 00:18:45.420
of point one.

309
00:18:45.570 --> 00:18:47.710
That's the best that is right.

310
00:18:48.030 --> 00:18:49.280
OK great.

311
00:18:49.290 --> 00:18:50.910
Let's keep going.

312
00:18:50.910 --> 00:18:52.420
So what we could do afterwards.

313
00:18:52.620 --> 00:19:01.260
We're going to use our grid dot predicts to simply use the kind of predictions plus the confusion matrix

314
00:19:01.470 --> 00:19:04.220
with the best values are the best parameters that we had already.

315
00:19:04.420 --> 00:19:06.420
Alright so we don't need to actually go in.

316
00:19:06.420 --> 00:19:07.460
We run it again.

317
00:19:07.500 --> 00:19:10.750
We can actually run it directly on our grid object.

318
00:19:10.940 --> 00:19:16.630
Talking to do you going to write great does predict not going to use going to use our X underscored

319
00:19:16.690 --> 00:19:19.420
test underscores scale values.

320
00:19:19.730 --> 00:19:24.820
That that should tell me where my grid predictions go.

321
00:19:24.840 --> 00:19:26.400
You can call it whatever you want.

322
00:19:26.670 --> 00:19:26.990
OK.

323
00:19:27.030 --> 00:19:28.380
That will give me my good prediction.

324
00:19:28.380 --> 00:19:29.500
That's kind of the output.

325
00:19:29.520 --> 00:19:32.630
Using the best or optimized values are.

326
00:19:33.860 --> 00:19:34.440
All right.

327
00:19:34.460 --> 00:19:36.040
So now we have all great predictions.

328
00:19:36.050 --> 00:19:40.130
The next step is that we're going to compute our confuser matrix.

329
00:19:40.370 --> 00:19:40.910
Let's see.

330
00:19:40.950 --> 00:19:46.760
Equal to confusion confusion underscored a matrix.

331
00:19:46.910 --> 00:19:49.880
And we're going to specify again what or why tests.

332
00:19:50.090 --> 00:19:55.430
And our great predictions the exact same fashion as we had earlier.

333
00:19:56.000 --> 00:19:56.940
Let's say this.

334
00:19:56.950 --> 00:19:57.810
Well good.

335
00:19:57.840 --> 00:20:04.800
The next step we're going to create again what he's going to use seaborne as an S dart map or whatever

336
00:20:04.800 --> 00:20:12.140
and to use our confusion Matrix C.M. but we use annotation equals to drag it.

337
00:20:12.170 --> 00:20:13.750
Let's run this as well.

338
00:20:14.120 --> 00:20:14.900
Okay.

339
00:20:14.990 --> 00:20:16.940
That's actually even better.

340
00:20:16.970 --> 00:20:21.900
So as you can see here we found that we only misclassify only three points.

341
00:20:21.950 --> 00:20:27.890
Okay which is obviously better than misclassifying only five points and that's mean we are actually

342
00:20:27.890 --> 00:20:28.410
on the right track.

343
00:20:28.420 --> 00:20:35.330
That means we are using are optimizing the values of the of the sport in the machine then setting it

344
00:20:35.330 --> 00:20:39.300
to sequence 10 and again make the point one that actually dramatically improved the results.

345
00:20:39.300 --> 00:20:42.210
All right let's plot the classification report.

346
00:20:42.320 --> 00:20:47.350
So if we go up here if you recall we had our classification report.

347
00:20:47.500 --> 00:20:52.100
So we just going to print out a classification report when instead of Y predict we're just going to

348
00:20:52.100 --> 00:20:56.300
use our y basically grid predictions instead.

349
00:20:56.430 --> 00:20:57.540
Right.

350
00:20:57.920 --> 00:20:58.790
Let's run it.

351
00:20:58.890 --> 00:21:04.310
And actually we reached 97 percent accuracy instead of the previous 96 percent accuracy which is even

352
00:21:04.310 --> 00:21:05.120
greater.

353
00:21:05.120 --> 00:21:08.740
And we only again misclassify three samples alright.

354
00:21:08.990 --> 00:21:13.510
One last point that I wanted to summarize here.

355
00:21:13.640 --> 00:21:17.120
We had the misclassification points.

356
00:21:17.120 --> 00:21:22.110
It's actually type 1 error which is perfect which is obviously not perfect I would love to have under

357
00:21:22.110 --> 00:21:22.810
a percent.

358
00:21:23.000 --> 00:21:29.040
But here we only showed that we only have three misclassify temples of type 1 error.

359
00:21:29.060 --> 00:21:29.780
Why.

360
00:21:29.810 --> 00:21:36.890
Because here the my predictions said their cancer was malignant which is classic zero.

361
00:21:37.070 --> 00:21:40.250
However it was simply the patient was fine.

362
00:21:40.250 --> 00:21:45.430
He would just benign He had a benign cancer which means we are even if we misclassify a few samples.

363
00:21:45.440 --> 00:21:51.170
It's kind of you know type 1 error which is not terrible or not but I hope you guys enjoyed that section

364
00:21:51.200 --> 00:21:54.750
and I hope you enjoyed the case study and see you in the next section.
