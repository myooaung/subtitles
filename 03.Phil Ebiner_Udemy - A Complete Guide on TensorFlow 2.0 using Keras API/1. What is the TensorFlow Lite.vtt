WEBVTT
1
00:00:01.040 --> 00:00:07.640
Hello everyone and welcome to the tens of low light section of the course the tens of low light is an

2
00:00:07.640 --> 00:00:15.110
extension of the Thames a floor library that is used to convert and optimize models for mobile devices.

3
00:00:15.110 --> 00:00:21.650
And the goal of this section is to see how to train a model and optimize it for any mobile device either

4
00:00:21.740 --> 00:00:26.290
Android or IRS when we talk about the deep learning.

5
00:00:26.290 --> 00:00:31.540
The first thing that pops up is the amount of processing resources that we need.

6
00:00:31.540 --> 00:00:37.720
And now we have a mobile device on the other side that has a problem even to run that inference and

7
00:00:37.720 --> 00:00:40.490
not to train the whole model on it.

8
00:00:40.510 --> 00:00:42.860
That is one part of the problem.

9
00:00:42.880 --> 00:00:45.490
The other part is the model size.

10
00:00:45.640 --> 00:00:51.310
Sometimes the same model could be up to 700 megabytes or even more.

11
00:00:51.310 --> 00:00:59.680
Imagine the net is 30 megabytes but to use it there is additional 700 megabytes of additional resources

12
00:00:59.680 --> 00:01:00.990
to be downloaded.

13
00:01:01.180 --> 00:01:03.400
That is just a bad user experience.

14
00:01:04.500 --> 00:01:10.830
But now we have a solution that makes the whole process of handling these problems much easier.

15
00:01:10.890 --> 00:01:16.600
As always we train our model on the computer after their training process is done.

16
00:01:16.640 --> 00:01:17.770
We save the model.

17
00:01:17.930 --> 00:01:25.470
Then we use our tens of low light library to optimize the Save the model for the mobile device.

18
00:01:25.490 --> 00:01:29.800
We'll talk about model optimization in a second after this step.

19
00:01:29.810 --> 00:01:37.770
We end up with the new Save the model smaller and optimized for a mobile device and at the end we put

20
00:01:37.770 --> 00:01:44.860
that optimized model on a mobile device to perform inference on it having the inference time on a mobile

21
00:01:44.860 --> 00:01:50.530
device is not essential although it provides a much better user experience.

22
00:01:50.680 --> 00:01:53.460
The app doesn't need the internet for that.

23
00:01:53.560 --> 00:02:01.040
No data is shared with the server and let's explain the workflow right now in the more details.

24
00:02:01.280 --> 00:02:09.200
We started by training the model in terms of flow 2.0 then we save the train model on a filesystem.

25
00:02:09.240 --> 00:02:15.570
The next step is to define the terms of flow light converter that takes the same model and optimize

26
00:02:15.570 --> 00:02:20.390
it for a mobile device and save the optimized model as a new file.

27
00:02:20.400 --> 00:02:27.060
Finally we put the train model on a mobile device and by using the terms of flow light interpreter we

28
00:02:27.060 --> 00:02:32.620
can efficiently perform inference on the mobile device so far.

29
00:02:32.620 --> 00:02:35.920
We mentioned that there is an optimization process.

30
00:02:35.920 --> 00:02:41.280
Now let's take a few minutes to explain what the optimization is and how it works.

31
00:02:41.290 --> 00:02:48.130
There are several ways to optimize a model for a mobile device one way and maybe the most obvious way

32
00:02:48.160 --> 00:02:55.390
is to optimize the model topology decrease numbers of layers faster operations less fully connected

33
00:02:55.390 --> 00:02:57.400
layers and more.

34
00:02:57.400 --> 00:03:04.570
In most cases this strategy will work but in this way we sacrifice the accuracy over speed and in most

35
00:03:04.570 --> 00:03:07.390
cases we don't want to do that.

36
00:03:07.440 --> 00:03:14.570
The second one is the network parameters precision reduction with quantification the continuation is

37
00:03:14.570 --> 00:03:21.230
a technique where we reduce the precision spaces after the floating numbers from a regular floating

38
00:03:21.230 --> 00:03:21.890
numbers.

39
00:03:21.890 --> 00:03:26.810
We are converting them into 8 bit numbers in most cases.

40
00:03:26.810 --> 00:03:33.170
This technique won't influence our model accuracy and there are two ways to perform quantized version

41
00:03:33.170 --> 00:03:34.220
of the model.

42
00:03:34.310 --> 00:03:41.210
One is pre training quantized Asian and another one is boss training quantized Asian and just to mention

43
00:03:41.210 --> 00:03:41.790
one more.

44
00:03:41.810 --> 00:03:49.400
There is a parameter count reduction as an optimization strategy and finally here is a link to excellent

45
00:03:49.400 --> 00:03:52.600
blog post about the model monetization process.

46
00:03:52.730 --> 00:03:58.630
If you want to learn more about the continuation techniques this is the place to start and that's it

47
00:03:58.630 --> 00:03:59.520
for now.

48
00:03:59.590 --> 00:04:04.160
If you have any questions or comments so far please post them in the comments section.

49
00:04:04.300 --> 00:04:06.190
Otherwise I've seen the next tutorial.
