1
00:00:00,830 --> 00:00:03,090
Hello and welcome back to the course and deep learning.

2
00:00:03,230 --> 00:00:08,420
Now that we've seen neural networks in action it's time for us to find out how they learn.

3
00:00:08,420 --> 00:00:10,430
So let's dive into it.

4
00:00:10,430 --> 00:00:16,190
There are two fundamentally different approaches to getting a program to do what you wanted to do.

5
00:00:16,190 --> 00:00:25,030
One is hardcoded coding where you actually tell the program's specific rules and what outcomes you want

6
00:00:25,050 --> 00:00:31,190
and you just guided throughout the whole way and you account for all the possible options that the program

7
00:00:31,190 --> 00:00:33,150
has to deal with.

8
00:00:33,260 --> 00:00:41,240
On the other hand you have neural networks where you create a facility for the program to be able to

9
00:00:41,750 --> 00:00:47,480
understand what it needs to join it's on to basically create this neural network where you provided

10
00:00:47,630 --> 00:00:53,570
inputs you tell it what you want as outputs and then you let it figure everything out on its own two

11
00:00:53,570 --> 00:00:55,780
fundamentally different approaches.

12
00:00:55,970 --> 00:01:00,780
And that is something to keep in mind as we go through these tutorials.

13
00:01:00,800 --> 00:01:09,530
Our goal is to create this network which then learns on its own we are going to avoid trying to put

14
00:01:09,530 --> 00:01:15,650
in the rules and a good example that I can give you right now is this will come further in the course

15
00:01:15,680 --> 00:01:18,650
but it's just a very visual example for instance.

16
00:01:18,650 --> 00:01:25,640
How do you distinguish between a dog and cat four on the left side on the process depicted on the left

17
00:01:25,640 --> 00:01:32,960
you would program things in like the cat's years have to be like this look out for whiskers look out

18
00:01:32,960 --> 00:01:39,200
for this type of nose look out for this type of shape of a face look out for these colors you kind of

19
00:01:39,200 --> 00:01:44,840
you describe all of these things and you'd have conditions like if if the years appointee then cad if

20
00:01:44,840 --> 00:01:49,450
the years are sloping down then possibly dog and so on.

21
00:01:49,540 --> 00:01:54,560
On the other hand for in your network you'd just code the neural networks you called the architecture

22
00:01:54,890 --> 00:02:00,740
and then you point the neural network at a full the reform these cats and dogs with images of cats and

23
00:02:00,740 --> 00:02:03,170
dogs which are already categorized and you tell it.

24
00:02:03,200 --> 00:02:09,890
OK I've got you I've got some images of cats and dogs go and learn what a cat is go and learn what a

25
00:02:09,890 --> 00:02:15,920
dog is and the neural network will on its own understand everything it is to understand and then further

26
00:02:15,920 --> 00:02:21,080
down once it's trained up when you give it a new image of a cat or dog it'll be able to understand what

27
00:02:21,080 --> 00:02:21,560
it was.

28
00:02:21,560 --> 00:02:23,090
So they're there they are.

29
00:02:23,120 --> 00:02:28,490
Those are the two fundamentally different approaches and today we're going to slowly start getting into

30
00:02:28,640 --> 00:02:31,060
how that second approach works.

31
00:02:31,070 --> 00:02:31,450
All right.

32
00:02:31,520 --> 00:02:33,230
So let's get straight to it.

33
00:02:33,320 --> 00:02:39,380
Here we have a very basic neural network with a one layer is this is called a single layer of feed forward

34
00:02:39,460 --> 00:02:42,710
a neural network and it is also called a perception.

35
00:02:42,720 --> 00:02:47,200
Now before we proceed one thing that we do need to adjust is that output value.

36
00:02:47,330 --> 00:02:49,270
Right now you can see that it's just a Y.

37
00:02:49,280 --> 00:02:51,080
We need to put a white hat in there.

38
00:02:51,140 --> 00:02:56,150
And the reason for that is usually why stands for the actual value and that's what we're going to be

39
00:02:56,150 --> 00:02:56,450
using.

40
00:02:56,450 --> 00:03:04,460
So why is gonna be the actual value which we see in reality output value is the predicted value by the

41
00:03:04,460 --> 00:03:10,670
algorithm by the neural network white hat is the output value basically that's the denomination for

42
00:03:10,670 --> 00:03:18,470
the output value and the perception that was first invented in 1947 by Frank Rosenblat and his whole

43
00:03:18,710 --> 00:03:25,020
idea was to create something that can actually learn and adjust itself.

44
00:03:25,160 --> 00:03:27,680
And this is what we're going to be looking at now.

45
00:03:27,950 --> 00:03:30,170
So we've got our perception.

46
00:03:30,200 --> 00:03:35,900
Let's see how perception learns so let's say we have some input values that have been supplied to the

47
00:03:35,900 --> 00:03:44,120
perception and or basically to our neural network then the activation function is applied.

48
00:03:44,120 --> 00:03:45,080
We have an output.

49
00:03:45,530 --> 00:03:49,160
And now we're going to plot the output on a chart.

50
00:03:49,160 --> 00:03:51,690
So there it is our output y hat.

51
00:03:51,740 --> 00:03:57,470
Now what we need to do is in order to be able to learn we need to compare the output value to the actual

52
00:03:57,470 --> 00:04:00,760
value that we want the neural network to get.

53
00:04:00,770 --> 00:04:01,040
Right.

54
00:04:01,520 --> 00:04:04,590
And that is the y value Y.

55
00:04:04,760 --> 00:04:08,150
And so if we plotted here you'll see that there's a bit of a difference.

56
00:04:08,270 --> 00:04:13,460
Now we're going to calculate a function called the cost function is calculated as one half of the difference

57
00:04:13,460 --> 00:04:17,120
of the squared difference between the actual value and output value.

58
00:04:17,120 --> 00:04:20,450
Now there are many ways you can come up with a class function.

59
00:04:20,450 --> 00:04:23,260
There are many different cost functions that you can use.

60
00:04:23,270 --> 00:04:30,230
This is probably the most commonly used cost function and why it is specifically this function that

61
00:04:30,230 --> 00:04:34,210
we use we'll find out further down when we're talking about a gradient descent.

62
00:04:34,220 --> 00:04:39,050
But for now we're just going to agree that this is the cost function and basically what the cost function

63
00:04:39,050 --> 00:04:44,210
is telling us is what is the error that you have in your prediction.

64
00:04:44,240 --> 00:04:50,180
And our goal is to minimize the cost function because the the lower the cost function the closer that

65
00:04:50,180 --> 00:04:51,350
y hat is to wait.

66
00:04:52,100 --> 00:04:54,380
Okay so as long as we agree on that let's proceed.

67
00:04:54,380 --> 00:04:58,350
So basically from here what happens is there is our cost function.

68
00:04:58,430 --> 00:05:05,630
And from here what happens is now we're going to once we've compared now we're going to feed this information

69
00:05:05,630 --> 00:05:08,660
back into the neural network.

70
00:05:08,930 --> 00:05:14,330
So there we go there's information going back into the neural network and it goes the weights and the

71
00:05:14,330 --> 00:05:15,650
weights get updated.

72
00:05:15,650 --> 00:05:20,840
Basically the only thing that we have control of in this very simple neural network are the weights

73
00:05:20,840 --> 00:05:23,100
W1 W2 all the way to W..

74
00:05:23,930 --> 00:05:26,720
And our goal is to minimize the cost function.

75
00:05:26,720 --> 00:05:33,890
So all we can do is update the weights so we update the weights and tweak them a little bit.

76
00:05:33,890 --> 00:05:38,250
And how exactly we'll find out for the done but for now we we agree that we have.

77
00:05:38,910 --> 00:05:40,020
And then we continue.

78
00:05:40,050 --> 00:05:40,270
So.

79
00:05:40,290 --> 00:05:48,780
But here I've put up this screenshot of the data just to make some one point very clear that right now

80
00:05:48,870 --> 00:05:53,930
throughout this whole experiment everything we're doing right now we're dealing with just the one roll.

81
00:05:53,940 --> 00:05:54,970
So we're dealing with.

82
00:05:55,140 --> 00:05:59,780
We have a dataset of one row where we have for instance we're dealing with.

83
00:06:00,000 --> 00:06:07,380
How long you study it like the variable that we are predicting is what what's results you're gonna get

84
00:06:07,380 --> 00:06:12,420
on an exam and the dependent independent variables that we have is how many hours did you study for

85
00:06:12,660 --> 00:06:13,850
how many hours did you sleep.

86
00:06:13,950 --> 00:06:15,380
And what did you get on the quiz.

87
00:06:15,390 --> 00:06:19,830
In the mid semester or so in the middle of the semester is a quiz what percentage did you get there.

88
00:06:19,830 --> 00:06:25,890
So based on those variables we're trying to predict what score you'll get for the exam exam.

89
00:06:25,980 --> 00:06:27,960
Ninety three percent that's the actual value.

90
00:06:27,960 --> 00:06:29,120
So that's why.

91
00:06:29,550 --> 00:06:29,840
So.

92
00:06:30,630 --> 00:06:33,430
So we feed these three values into our neural network.

93
00:06:33,450 --> 00:06:38,760
Again for the second time now and then we're going to be comparing the result to white.

94
00:06:39,090 --> 00:06:40,590
So let's see how this works.

95
00:06:40,710 --> 00:06:43,650
We feed these values into the neural network.

96
00:06:43,800 --> 00:06:49,680
Everything gets adjusted and weights get it just so as you can see this is again we're going to feed

97
00:06:49,680 --> 00:06:50,250
the values.

98
00:06:50,250 --> 00:06:53,120
Again the point here is that we're feeding in these same values.

99
00:06:53,130 --> 00:06:56,340
So we only have one role we're trying to we're training on one row.

100
00:06:56,340 --> 00:06:59,550
This is because this is just a very simple basic example.

101
00:06:59,550 --> 00:07:01,380
Then we'll see what happens when there's more rows.

102
00:07:01,740 --> 00:07:09,060
So again we feed these rows in our cost function get adjusted as you can see everything happens along

103
00:07:09,690 --> 00:07:10,470
those lines again.

104
00:07:10,470 --> 00:07:16,110
So as you see every time our white hat is changing because we've tweak the weights all I had is changing

105
00:07:16,470 --> 00:07:17,550
our cost function is changing.

106
00:07:17,550 --> 00:07:18,300
Let's have a look again.

107
00:07:18,300 --> 00:07:20,300
So we feed those in.

108
00:07:20,460 --> 00:07:22,800
Why had is changing cost function is changing.

109
00:07:22,860 --> 00:07:26,960
We get information back feedback to the weights so that the weights get adjusted again.

110
00:07:26,970 --> 00:07:31,820
We feed in the same values every time everything gets adjusted goes back to the weights.

111
00:07:31,830 --> 00:07:33,040
And one more time.

112
00:07:33,140 --> 00:07:33,720
Feed in.

113
00:07:33,960 --> 00:07:35,620
OK.

114
00:07:35,670 --> 00:07:36,620
And another time.

115
00:07:36,630 --> 00:07:39,960
So we've adjust the weight adjust to the ways we feed and the information.

116
00:07:40,010 --> 00:07:40,660
Aha.

117
00:07:40,770 --> 00:07:41,280
And there we go.

118
00:07:41,310 --> 00:07:45,900
So now this time the Y had is equal to y cross-functional zero.

119
00:07:45,930 --> 00:07:50,720
Usually you won't get cost function equal to zero but this is a very simple example.

120
00:07:50,760 --> 00:07:56,070
So hopefully all that made sense every time we feed in exactly that same row.

121
00:07:56,220 --> 00:08:03,060
Because just in this case we're just dealing with that one row into our neural network where then the

122
00:08:03,060 --> 00:08:08,020
weights get the value get multiplied the ways the activation function is applied we get why had why

123
00:08:08,190 --> 00:08:12,210
y it is compared to why then we see how the cost function has changed.

124
00:08:12,300 --> 00:08:16,830
FEEDBACK The feed that information backwards the neural network and adjusts adjust the weights again

125
00:08:17,820 --> 00:08:21,370
and then we repeat the same process again with the same exact row.

126
00:08:21,540 --> 00:08:23,280
We're trying to minimize that cost function.

127
00:08:23,460 --> 00:08:26,900
So up until now we've been dealing with just that one rule.

128
00:08:26,970 --> 00:08:29,400
Let's see what happens when you have multiple roles.

129
00:08:29,430 --> 00:08:31,250
So here's the full data set.

130
00:08:31,260 --> 00:08:38,550
We have eight rows of how many hours you slept or maybe these are different students in day taking the

131
00:08:38,550 --> 00:08:43,890
same exam how many other hours they studied how many hours they slept before the exam what to get on

132
00:08:43,890 --> 00:08:47,430
the quiz and their final result on the test.

133
00:08:47,430 --> 00:08:52,570
And as you can see here on the left I've got eight of these perceptions actually.

134
00:08:53,040 --> 00:08:55,980
They are all the same perceptions so this is also important to understand.

135
00:08:55,980 --> 00:09:04,260
I just multiplied it all I duplicated eight times just so that we can conception is that.

136
00:09:04,260 --> 00:09:09,990
But the important thing here is the same neural network we're going to be feeding these into one Samuel

137
00:09:09,990 --> 00:09:10,350
network.

138
00:09:10,350 --> 00:09:11,590
So let's go let's get started.

139
00:09:11,580 --> 00:09:20,700
So one epoch as you'll hear had Lun mentioning one epoch is when we go through our whole dataset and

140
00:09:20,700 --> 00:09:26,210
we train our neural network on on all of these roles.

141
00:09:26,220 --> 00:09:27,360
So let's go let's get Sara.

142
00:09:27,390 --> 00:09:34,500
So there is our first row and there is y had for the first row there is a second row there's Y had for

143
00:09:34,500 --> 00:09:35,220
the second row.

144
00:09:35,220 --> 00:09:41,120
So again is being fed into the same neural network every time I've just copied them several times.

145
00:09:41,130 --> 00:09:44,580
So we can visually see how this is happening.

146
00:09:45,000 --> 00:09:47,710
Then again it's happening again.

147
00:09:47,850 --> 00:09:50,320
That's third row fourth row.

148
00:09:50,550 --> 00:09:52,950
There is our way ahead for the fourth row and so on.

149
00:09:52,980 --> 00:09:56,550
Basically then we get the same values for the remaining four hours as well.

150
00:09:56,550 --> 00:10:06,060
So every time we just feed in a row into our neural network we get a value and then we compare to the

151
00:10:06,090 --> 00:10:06,900
actual value.

152
00:10:06,930 --> 00:10:08,660
So there are the actual values.

153
00:10:08,720 --> 00:10:11,060
So for every single row we have an actual value.

154
00:10:11,580 --> 00:10:18,420
And now based on all of these differences between y had and why we can calculate the cost function which

155
00:10:18,420 --> 00:10:27,120
is the sum of all of those squared differences between why it has and why and how all of that is halved

156
00:10:28,170 --> 00:10:29,860
and there's our cost function.

157
00:10:30,300 --> 00:10:36,720
And basically now what we do after we have the full cost function we go back and we update the weights

158
00:10:37,110 --> 00:10:44,970
we update W1 W2 W3 and the important thing to remember here is that all of these perceptions all of

159
00:10:44,970 --> 00:10:47,280
these new networks is actually one neural network.

160
00:10:47,310 --> 00:10:49,620
So there's not eight of them there's just one.

161
00:10:49,620 --> 00:10:54,450
And when we update the weights we're going to update the weights in that one neural network.

162
00:10:54,450 --> 00:10:57,850
So basically the weights are going to be the same for all of the rows.

163
00:10:57,870 --> 00:11:00,470
So it's not the case that every row has its own weights.

164
00:11:00,480 --> 00:11:02,850
Now all the rows share the weights.

165
00:11:02,850 --> 00:11:11,100
And so that's why we looked at the cost function which is the sum of the square differences and then

166
00:11:11,100 --> 00:11:15,210
we updated the weights and now from here that was just one iteration.

167
00:11:15,210 --> 00:11:22,890
Next we're going to run this whole thing again we're going to feed every single row into the neural

168
00:11:22,890 --> 00:11:23,560
network.

169
00:11:23,670 --> 00:11:26,310
Find out our cost function and do this whole process again.

170
00:11:26,340 --> 00:11:32,010
So just as we saw previously where we had just one row and we were doing everything again and again

171
00:11:32,130 --> 00:11:32,740
and again.

172
00:11:32,850 --> 00:11:33,530
Same thing here.

173
00:11:33,540 --> 00:11:37,670
But now we're going to be doing it eight rows or eight hundred rows or eight thousand rows home.

174
00:11:37,800 --> 00:11:43,320
I mean you rows you have in your data set you do this process and then you calculate the cost function

175
00:11:44,130 --> 00:11:51,240
and the goal here is to minimize the cost function and to get as soon as you've found the minimum the

176
00:11:51,240 --> 00:11:57,840
cost function that is your final neural network that means your weights have been adjusted and you have

177
00:11:59,430 --> 00:12:08,610
found the optimal weights for this dataset that you're training on and you're ready to proceed to the

178
00:12:08,610 --> 00:12:14,930
testing phase or to the application phase and this whole process is called back propagation.

179
00:12:14,930 --> 00:12:20,410
So some additional reading that you might want to do for the cost function.

180
00:12:20,450 --> 00:12:24,800
And I know we just talked about one and there are many different ones.

181
00:12:24,800 --> 00:12:28,220
A good article is located on cross validated.

182
00:12:28,700 --> 00:12:33,020
It's called a list of course functions used in your own networks alongside applications.

183
00:12:33,020 --> 00:12:39,990
So the URLs there but you can just google for that exact search term or a search phrase and you will.

184
00:12:40,030 --> 00:12:42,020
This one will be the first one that pops up.

185
00:12:42,110 --> 00:12:48,590
It's actually got some good examples and application or use cases for different cost functions so if

186
00:12:48,590 --> 00:12:51,790
you're interested to learn more about cost functions check out this article.

187
00:12:51,920 --> 00:12:54,380
And on that note I hope you enjoyed this material.

188
00:12:54,380 --> 00:12:55,910
I look forward to seeing you next then.

189
00:12:56,000 --> 00:12:57,970
Until then enjoy deep learning.
