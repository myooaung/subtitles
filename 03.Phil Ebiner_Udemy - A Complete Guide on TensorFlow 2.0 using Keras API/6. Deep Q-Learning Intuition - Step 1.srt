1
00:00:00,610 --> 00:00:03,880
Hello and welcome back to the course on artificial intelligence.

2
00:00:03,880 --> 00:00:06,960
And finally we're on to the fun stuff.

3
00:00:06,970 --> 00:00:09,330
We're onto deep learning.

4
00:00:09,370 --> 00:00:10,630
All right so let's have a look.

5
00:00:10,630 --> 00:00:14,060
Previously we spoke about kill learning and what it's all about.

6
00:00:14,080 --> 00:00:20,830
And we learned about the age and the environment and how the agent will look at the state here or she

7
00:00:20,830 --> 00:00:27,670
is in take an action get a reward enter into a new state and based on that feedback loop they will continue

8
00:00:27,670 --> 00:00:29,450
taking actions and they'll learn from that.

9
00:00:29,440 --> 00:00:32,230
Understand what are the better actions to take.

10
00:00:32,230 --> 00:00:34,990
And so we looked at this basic example of a maze.

11
00:00:34,990 --> 00:00:40,030
We understood that as the agent explores the environment and understands what the values of the states

12
00:00:40,030 --> 00:00:40,510
are.

13
00:00:40,510 --> 00:00:45,100
Then we moved on from dealing with the values of the states to dealing with the values of the actions

14
00:00:45,100 --> 00:00:51,880
or the Q values and then based on that we understood how plans in an answer caustic environments work

15
00:00:51,880 --> 00:00:56,520
and how policies work in the casting environments and this is an example of policy.

16
00:00:57,040 --> 00:01:01,370
So that's a quick recap of everything we discussed in the basic learning.

17
00:01:01,400 --> 00:01:07,150
And now let's have a look at how this can be taken to the next level through deep learning through adding

18
00:01:07,180 --> 00:01:07,870
deep learning.

19
00:01:08,250 --> 00:01:16,030
OK so this is our environment and what we're going to do now is we're going to add instead of just doing

20
00:01:16,030 --> 00:01:21,780
basic calculations in this matrix that we have which is pretty simple.

21
00:01:21,790 --> 00:01:26,920
What are we going to do is we're going to add two axes which add an x and y axis or we'll call them

22
00:01:26,920 --> 00:01:30,420
X1 and X2 just to make things even more general.

23
00:01:30,430 --> 00:01:34,330
And here we've got two will number the columns one two three four.

24
00:01:34,340 --> 00:01:36,490
You'll hear world number at the rows 1 2 3.

25
00:01:36,910 --> 00:01:43,750
And so now every single state can be described by a pair of two values x1 and x2.

26
00:01:43,750 --> 00:01:51,460
So any one of these squares in which the agent can possibly be in can be described by x1 x2 So for instance

27
00:01:51,460 --> 00:02:00,310
right now he's in the square with X1 equal to 1 and x 2 equal to 2 and therefore that's in the same

28
00:02:00,380 --> 00:02:03,380
that same way we can escape any square meaning we can describe any state.

29
00:02:03,430 --> 00:02:08,860
Then of course this is a very simplified version of an environment of describing states but nevertheless

30
00:02:08,860 --> 00:02:17,140
it works in this case and that means that now we can feed this these states into a neural network.

31
00:02:17,380 --> 00:02:21,790
And by the way here I would just like to mention that at the end of the course of good annexes we've

32
00:02:21,790 --> 00:02:25,910
got an X number one and an X and but two in order to proceed successfully.

33
00:02:25,910 --> 00:02:31,270
This section is highly advisable that you check out an X number one which is on artificial new neural

34
00:02:31,270 --> 00:02:33,850
network so you understand how they work so that we can.

35
00:02:34,180 --> 00:02:39,400
We don't have to delve into that here and we can just use the benefits of the knowledge of how artificial

36
00:02:39,400 --> 00:02:40,670
neural networks work.

37
00:02:40,750 --> 00:02:49,000
And so we feed in this information on the state into a neural network and then it will process this

38
00:02:49,240 --> 00:02:54,490
information so x1 x2 depending on the structure of the neural network good might have multiple hidden

39
00:02:54,490 --> 00:02:55,330
layers and so on.

40
00:02:55,330 --> 00:03:00,850
So that's something that you you'll figure out in their practical tutorials but at the end we will structure

41
00:03:00,850 --> 00:03:06,520
in such a way that it spits out four values and these four values are actually going to be our cue values.

42
00:03:06,550 --> 00:03:11,320
So the values which dictate which action we need to take and further down in this tutorial we'll see

43
00:03:11,320 --> 00:03:15,130
exactly how these key values are used to decide which action is taken.

44
00:03:15,160 --> 00:03:22,490
But the main point here is that we no longer look at just this maze from a cube learning perspective.

45
00:03:22,600 --> 00:03:29,680
We're now taking the states of the maze and we're feeding them into a deep neural network in order to

46
00:03:29,740 --> 00:03:31,260
get these key values.

47
00:03:31,330 --> 00:03:35,050
And at the end of the day we're still going to come up with an action we're still going to understand

48
00:03:35,050 --> 00:03:38,910
how what action we need to take and we'll discuss all this in more detail.

49
00:03:38,920 --> 00:03:43,990
But the question right now is why why are we doing all of this why we complete why we're making things

50
00:03:43,990 --> 00:03:49,540
so much more complicated when that initial approach of Q learning was working already well the reason

51
00:03:49,540 --> 00:03:55,330
for that is the Q learning was working in this very simplistic environment and we're continuing to deal

52
00:03:55,330 --> 00:03:59,670
for now with this very simplistic environment in order to better understand the concepts.

53
00:03:59,950 --> 00:04:06,790
But at the same time that simple Q learning will no longer work in more complex environments and we're

54
00:04:06,790 --> 00:04:13,390
talking about for instance the self-driving cars we shall be creating or playing Doom when the artificial

55
00:04:13,390 --> 00:04:21,190
intelligence is playing Doom or other Atari games like breakout or even self-driving cars and more advanced

56
00:04:21,580 --> 00:04:27,190
reinforcement learning things such as like robots walking around and performing actions in all those

57
00:04:27,190 --> 00:04:34,000
cases basic learning is insufficient is not strong is not powerful enough to be able to master those

58
00:04:34,000 --> 00:04:39,550
challenges and just like we've seen in the deep learning course if you've been in our discipline course

59
00:04:39,550 --> 00:04:45,280
or if you've done the annexed sections and X number one and angst do you will where you know that deep

60
00:04:45,280 --> 00:04:52,060
learning is by far superior to any type of machine learning let alone simple Q learning and that's why

61
00:04:52,060 --> 00:04:56,530
we're leveraging the power of deep learning here so we're feeding in the information about the environment

62
00:04:56,950 --> 00:05:02,220
as a vector of values so in this case just to values into a deep neural network and then we're using

63
00:05:02,220 --> 00:05:06,990
that to perform the actions that we want to decide which actions are agents going to take.

64
00:05:07,350 --> 00:05:11,830
So that's kind of like a high level overview of why we're doing this.

65
00:05:11,880 --> 00:05:13,960
Now let's have a look at it a bit more detail.

66
00:05:14,010 --> 00:05:20,250
What happens to the concepts of kill learning when we transfer when we make this transformation from.

67
00:05:20,310 --> 00:05:23,640
Or transition from simple learning into deep Hillary.

68
00:05:24,060 --> 00:05:31,710
So as you saw in the previous intuition tutorials we had a slide like this which is the foundation of

69
00:05:31,920 --> 00:05:33,570
temporal difference learning.

70
00:05:33,660 --> 00:05:35,730
This is the formula for temporal difference.

71
00:05:35,730 --> 00:05:37,400
And basically so let's go through it.

72
00:05:37,400 --> 00:05:45,930
So basically we had an agent who was in this state over here which is indicated the blue arrow and we

73
00:05:45,930 --> 00:05:51,140
were understanding how Temporal Difference works for this Q value of for instance going up.

74
00:05:51,720 --> 00:05:56,790
And so what we saw here was before this is in the simple coloring not the Depew learning this in the

75
00:05:56,790 --> 00:06:04,500
simple Hillary what we saw was before the agent had a certain and create value that he had learned about

76
00:06:04,770 --> 00:06:06,300
this action of going up.

77
00:06:06,300 --> 00:06:11,730
And so then he decides to take deception to go up and right after he takes his action he gets a reward

78
00:06:11,790 --> 00:06:14,610
for taking this action in this state.

79
00:06:14,760 --> 00:06:21,000
And that is that reward plus now he can evaluate the value of the current state he is in which is the

80
00:06:21,000 --> 00:06:27,720
maximum of all of the new Q values of all of the Q values of the new actions he can take a prime in

81
00:06:27,720 --> 00:06:32,350
the new state as prime and read multiplied by the decay factor of gamma.

82
00:06:32,400 --> 00:06:41,340
So that is essentially the Q The new Q value or kind of like the the empirical Q value that he has just

83
00:06:41,340 --> 00:06:43,230
received for taking that action.

84
00:06:43,230 --> 00:06:49,170
And ideally these two two should be the same so that actually the Q value that he had in his memory

85
00:06:49,170 --> 00:06:55,920
about this action in this state should equate to the actual reward plus the gamma times the value of

86
00:06:55,920 --> 00:07:00,480
the state that he ended up in and therefore that's how we calculate that Temporal Difference we take

87
00:07:00,480 --> 00:07:06,060
what he got after minus what he got what he had in mind what he was expecting you'd subtract one from

88
00:07:06,060 --> 00:07:12,600
the other that's a Temporal Difference and then you use your learning rate alpha to adjust your Q value

89
00:07:12,640 --> 00:07:18,750
your your new Q value by the Temporal Difference but with a coefficient of alpha so that is the essence

90
00:07:18,810 --> 00:07:20,400
of the simple Q learning.

91
00:07:20,400 --> 00:07:24,120
Now let's have a look at how it changes in deep color.

92
00:07:24,510 --> 00:07:29,220
And so we're still going to work with the slide but we're going to just see exactly what's happening.

93
00:07:29,550 --> 00:07:35,410
So in a deep Q learning the neural network will predict for roles as we saw in the previously.

94
00:07:35,430 --> 00:07:41,160
And as we'll see for a donor's tutorial the neural network will predict for values or it might predict

95
00:07:41,160 --> 00:07:44,730
more values if there's more possible actions in a given state.

96
00:07:44,730 --> 00:07:48,320
But in this case we know we know that there's only four actions upright left down.

97
00:07:48,630 --> 00:07:53,150
And so the neural network will predict four of these values.

98
00:07:53,190 --> 00:07:57,190
So there will be no in in a deep learning situation is important.

99
00:07:57,190 --> 00:07:58,650
Understand there is no before or after.

100
00:07:58,920 --> 00:08:01,680
And this is how we we'll get to know this a bit better.

101
00:08:01,680 --> 00:08:08,040
So the neural network will predict four of these values and it will compare not to what will happen

102
00:08:08,070 --> 00:08:08,940
after.

103
00:08:08,940 --> 00:08:15,210
But the neural network will compared to this exact value but it was this value which was calculated

104
00:08:15,360 --> 00:08:17,700
in the previous step.

105
00:08:17,700 --> 00:08:22,980
So in the previous time when the agent was in this exact square.

106
00:08:23,040 --> 00:08:24,950
So let's say I don't know.

107
00:08:24,950 --> 00:08:34,140
Some time ago the agent was again was in this exact square as well and it calculated this value previously.

108
00:08:34,350 --> 00:08:38,640
So in the previous time long time ago the agent calculated this value.

109
00:08:38,640 --> 00:08:41,970
Then the agents stored this value for the future.

110
00:08:41,970 --> 00:08:43,640
And now the future has come.

111
00:08:43,650 --> 00:08:48,750
So now he's in the square again and now he's got these Q values which is predicted and one of them is

112
00:08:48,750 --> 00:08:50,280
for the for going up.

113
00:08:50,640 --> 00:08:57,150
So now what he's going to do is going to compare the predicted value of Q to this value which he had

114
00:08:57,150 --> 00:08:58,460
recorded from the previous.

115
00:08:59,130 --> 00:09:03,690
And we'll understand exactly why this is important right now so just important understand here is there's

116
00:09:03,720 --> 00:09:04,940
no before and after.

117
00:09:04,940 --> 00:09:11,520
In this specific square this specific time we're taking the cue value that he's predicted using the

118
00:09:11,520 --> 00:09:19,440
neural network this time and we're comparing it to this value which he had from the previous time from

119
00:09:19,440 --> 00:09:25,170
the previous time he was in this square assessing all of the situation and likely the previous time

120
00:09:25,440 --> 00:09:28,200
he actually performed this action.

121
00:09:28,200 --> 00:09:29,240
So there we go.

122
00:09:29,250 --> 00:09:35,310
Now let's have a look at how this all works out in the neural network and why why is it like I know

123
00:09:35,310 --> 00:09:39,000
it sounds a bit complicated right now but we'll break it down into simple terms right.

124
00:09:39,270 --> 00:09:39,960
Just in a second.

125
00:09:39,960 --> 00:09:44,340
So this on your own network we're sitting in the states of the environment into the neural network is

126
00:09:44,340 --> 00:09:49,890
going through the hidden layers then it's coming out with these outputs Q1 Q2 Q3 Q4 in that specific

127
00:09:49,890 --> 00:09:50,760
state.

128
00:09:50,760 --> 00:09:57,320
These are the Q values that the neural network is predicting for the possible actions.

129
00:09:57,330 --> 00:09:58,310
Those are the key values.

130
00:09:58,350 --> 00:10:03,520
So then we're comparing to target and these targets is exactly so if we go back here.

131
00:10:03,520 --> 00:10:04,660
This is the target.

132
00:10:04,660 --> 00:10:10,420
So this is the value that was predicted and then but also we know that we have a target from the last

133
00:10:10,420 --> 00:10:11,710
time we were in the square.

134
00:10:11,740 --> 00:10:16,340
We have a target for this same action which is up for instance.

135
00:10:16,600 --> 00:10:20,820
So here we've got a target and we're going to compare so we compare Q1 versus that target.

136
00:10:20,980 --> 00:10:22,990
We're comparing Q2 versus that target.

137
00:10:22,990 --> 00:10:28,320
The target we had from previously Q3 versus the target Q4 versus the target.

138
00:10:28,360 --> 00:10:36,550
And so this is the part where the neural network or the agent is now learning through deep learning

139
00:10:36,550 --> 00:10:38,600
how to better go through.

140
00:10:38,620 --> 00:10:45,130
And the key point here is that we're still applying Q learning but the concepts in set in simple killer

141
00:10:45,130 --> 00:10:49,270
and you learn through temporal differences which are pretty straight forward which we've already discussed

142
00:10:49,270 --> 00:10:54,640
and we know quite well by now but at the same time in deep learning how do neural networks learn while

143
00:10:54,700 --> 00:10:56,940
neural networks learn through adjusting their weight.

144
00:10:56,950 --> 00:11:07,030
So we have to adapt the concepts of reinforced the concepts of simple kill learning to the way neural

145
00:11:07,030 --> 00:11:10,920
networks actually work and that is through updating their weights.

146
00:11:10,920 --> 00:11:14,890
And so this is what we're trying to figure out here how do we adapt that concept of Temporal Difference

147
00:11:15,310 --> 00:11:21,210
to a neural network so that we can leverage the full power of neural networks.

148
00:11:21,230 --> 00:11:22,170
So far we've gotten this.

149
00:11:22,160 --> 00:11:28,630
So we enter our environment state here as a vector goes through a neural network we get predictions

150
00:11:28,630 --> 00:11:35,560
of Q values and then from the previous time the agent was in that state we have these Q target to target

151
00:11:35,560 --> 00:11:39,410
one two three and four for each of these respective actions.

152
00:11:39,460 --> 00:11:42,880
And so now we're up to Okay let's compare each one with each one.

153
00:11:43,510 --> 00:11:50,440
And from here it's it becomes pretty straightforward if you're up to speed with neural networks.

154
00:11:50,440 --> 00:11:52,360
Once again that's on a and.

155
00:11:52,510 --> 00:11:59,020
Number one we're going to calculate a loss which is l here and we're going to be a huge target this

156
00:11:59,020 --> 00:12:01,640
one minus Q minus this one.

157
00:12:01,810 --> 00:12:06,100
We're going to square that so the square difference of the of each one of these and we're going to sum

158
00:12:06,100 --> 00:12:11,950
them so we can take the sum of the squared differences of these Q values and their targets and we're

159
00:12:11,950 --> 00:12:13,960
going to send them up and that's going to be a loss.

160
00:12:13,960 --> 00:12:19,250
And so ideally just as we had into in the temporal difference learning so if we go back for a second.

161
00:12:19,330 --> 00:12:22,920
Remember we said ideally we want this to be equal to this.

162
00:12:22,930 --> 00:12:25,120
So we want the temporal difference to be zero.

163
00:12:25,120 --> 00:12:31,720
So that's that means basically the agent is predicting exactly correctly what you know the Q values

164
00:12:31,720 --> 00:12:37,840
that the agent is predicting are exactly or that he has a memory are exactly descriptive of the environment

165
00:12:38,560 --> 00:12:44,130
and therefore the agent can navigate navigate the environ pretty well right there's no surprises there's

166
00:12:44,140 --> 00:12:45,470
no there's no.

167
00:12:45,580 --> 00:12:51,270
So as long as this temporal difference is a highly positive or highly negative then we've got some surprises.

168
00:12:51,280 --> 00:12:55,410
But if the temporal difference is zero then he knows environment so well that he can predict what's

169
00:12:55,420 --> 00:13:01,100
going on and he can and therefore his policy is going to be very good and he's going to be able to navigate.

170
00:13:01,300 --> 00:13:02,110
So here.

171
00:13:02,140 --> 00:13:02,650
Same thing.

172
00:13:02,680 --> 00:13:09,070
So we want this loss to be as close to zero suppose as small as possible and that's why now we're going

173
00:13:09,070 --> 00:13:15,910
to this is the part where we are going to leverage the real true power of neural networks so we could

174
00:13:15,910 --> 00:13:22,900
take this loss we're going to use back propagation or stochastic gradient descent to take this loss

175
00:13:23,200 --> 00:13:28,510
and pass it through the network pass it back or back propagate it through a network and through stochastic

176
00:13:28,510 --> 00:13:36,100
gradient descent date the weights of these synopses in the network so that next time we go through this

177
00:13:36,100 --> 00:13:39,960
network the way it's already a bit better descriptive of the environment.

178
00:13:39,970 --> 00:13:41,010
And that's exactly hard work.

179
00:13:41,020 --> 00:13:47,560
So here you have if we go back this is calculated losses calculate and it gets per prove propagated

180
00:13:47,560 --> 00:13:51,160
for the network the weights are updated then the next time we get here.

181
00:13:51,160 --> 00:13:56,950
This happens again and we get here and this happens again and so on and so and it keeps happening and

182
00:13:56,950 --> 00:14:03,670
that's how this agent learns or basically now it's the neural network which is the brain of the agent

183
00:14:03,730 --> 00:14:10,690
is learning is becoming more and more descriptive of the environment and therefore the agent is able

184
00:14:10,690 --> 00:14:15,610
to navigate the environment when we say descriptive environment basically means that when we put in

185
00:14:15,730 --> 00:14:23,200
the states of the environment that the agent is in we are more likely to get closer and closer to the

186
00:14:23,260 --> 00:14:24,940
actual cue values.

187
00:14:24,940 --> 00:14:29,290
And that happens because the key values that we want to find the right action.

188
00:14:29,290 --> 00:14:34,810
And that happens because these new targets are actually empirically derived so he every.

189
00:14:35,020 --> 00:14:36,820
How does he find these new targets.

190
00:14:36,850 --> 00:14:38,400
That's because that's actually there.

191
00:14:38,400 --> 00:14:40,030
So he actually observes.

192
00:14:40,060 --> 00:14:40,260
Okay.

193
00:14:40,270 --> 00:14:42,870
So once I do take this step what's the reward I get.

194
00:14:43,000 --> 00:14:44,970
And then what's the value of this state.

195
00:14:45,010 --> 00:14:49,960
So same thing as we saw previously in Q learning in the simple Q learning intuition so he learns this

196
00:14:49,960 --> 00:14:54,820
through trial and error and then he constructs his network or updates the way it serves his network

197
00:14:54,820 --> 00:15:01,310
in such a way that the predicted Q values are close and close up proximity to the target.

198
00:15:01,310 --> 00:15:07,310
Q values so very similar to the concept we discussed here in the simple Temporal Difference learning

199
00:15:07,370 --> 00:15:09,860
of the simple Q learning algorithm.

200
00:15:09,860 --> 00:15:10,410
So there we go.

201
00:15:10,410 --> 00:15:12,450
That's that's how the agent learns.

202
00:15:12,500 --> 00:15:14,330
So we're up to here.

203
00:15:14,350 --> 00:15:15,230
That's the learning part.
