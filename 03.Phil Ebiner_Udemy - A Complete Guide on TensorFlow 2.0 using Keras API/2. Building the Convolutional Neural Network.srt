1
00:00:00,420 --> 00:00:00,720
All right.

2
00:00:00,720 --> 00:00:01,500
Here we go.

3
00:00:01,500 --> 00:00:07,420
Let us build our convolution old news network with tens flow 2.0.

4
00:00:07,500 --> 00:00:13,330
And first we're going to start the exact same way as for the artificial neural network.

5
00:00:13,360 --> 00:00:19,440
We're gonna start by building the model object as an instance of the sequential class which is still

6
00:00:19,440 --> 00:00:23,870
taken from the morals module by the caris library taken from tensor flow.

7
00:00:24,210 --> 00:00:31,320
So as a reminder the sequential class allows you to build a new network which is a sequence of layers

8
00:00:31,590 --> 00:00:37,890
as opposed to computational graph like for example both man machines and so you initialize your model

9
00:00:37,890 --> 00:00:43,890
like this and then as before we're going to add the different layers which this time are going to be

10
00:00:43,890 --> 00:00:47,670
some convolution of layers and Max putting layers.

11
00:00:47,700 --> 00:00:48,240
All right.

12
00:00:48,240 --> 00:00:49,180
So are you ready.

13
00:00:49,380 --> 00:00:56,600
Let's add the first convolution or layer so as before I gathered here the hybrid parameters that we're

14
00:00:56,610 --> 00:01:02,000
gonna have 30 to filters a kernel size of three so that's actually the size of the filters.

15
00:01:02,010 --> 00:01:04,530
You know a filter can also be called the kernel.

16
00:01:04,530 --> 00:01:11,810
So we're gonna have 32 filters of size three by three and that as a reminder is exactly what allows

17
00:01:11,820 --> 00:01:18,240
to do the convolution operations from the input images to the convolution layer and then from the convolution

18
00:01:18,240 --> 00:01:22,580
layer to the next convolution layer or Max pulley layer.

19
00:01:22,650 --> 00:01:26,850
So you have your filters then padding is going to be the same.

20
00:01:26,880 --> 00:01:32,160
And there is actually something to understand here with the padding because the padding can neither

21
00:01:32,160 --> 00:01:37,230
take most of the time the same value or the valid value.

22
00:01:37,230 --> 00:01:41,260
And so I wanted to explain the difference between same embedding.

23
00:01:41,360 --> 00:01:47,640
There are actually a lot of questions on this online and I looked at all the sources and the best explanation

24
00:01:47,640 --> 00:01:53,170
I found where we can clearly visualize this is from this blog core will come.

25
00:01:53,220 --> 00:01:57,040
So thank you so much for providing such a great explanation.

26
00:01:57,120 --> 00:02:01,980
We can clearly see it at the end the difference between valid and same.

27
00:02:01,980 --> 00:02:07,890
So imagine your array has an odd number and you know the size of your filter is an even number.

28
00:02:08,190 --> 00:02:14,490
Well as we can see you're adding this column of zeros here so that when you're is trading you are taking

29
00:02:14,640 --> 00:02:20,510
this column of zeros and you see that here the max is five then the max is six.

30
00:02:20,520 --> 00:02:25,230
We're gonna see six then the max here is gonna be eleven and then the max is twelve.

31
00:02:25,230 --> 00:02:32,810
So basically the size of the output is the same as the input and that's why it is called same.

32
00:02:32,820 --> 00:02:34,590
However and valid.

33
00:02:34,590 --> 00:02:41,160
Well you're not taking this column of zeros and so instead you're losing information because you're

34
00:02:41,280 --> 00:02:47,870
basically ignoring this column and you're just taking the maximums each time of the squares here.

35
00:02:47,880 --> 00:02:51,140
And that's why we only end up with five and eleven.

36
00:02:51,330 --> 00:02:51,570
Right.

37
00:02:51,600 --> 00:02:54,150
That's difference between same and valid.

38
00:02:54,150 --> 00:03:00,000
It's good that you know it because indeed we're gonna see that we will provide a different patterns

39
00:03:00,270 --> 00:03:02,550
in our convolution all layers.

40
00:03:02,550 --> 00:03:09,270
So for our first convolution all layered Well the bearing is going to be same meaning we're adding this

41
00:03:09,270 --> 00:03:15,960
column of zeros which will be including when trading the filter in the tensor.

42
00:03:16,080 --> 00:03:19,790
So same pairing and then rectify activation function.

43
00:03:19,850 --> 00:03:21,110
That's the same as before.

44
00:03:21,330 --> 00:03:28,380
Usually in the hidden layer as well we use a rectified activation function which has the parameter name.

45
00:03:28,860 --> 00:03:34,650
And then the input shape well that's the shape of our input images which have a..

46
00:03:34,650 --> 00:03:36,630
Dimension 32 by 32.

47
00:03:36,690 --> 00:03:39,950
And the three channels for the colors red green and blue.

48
00:03:40,440 --> 00:03:45,900
And that's what you have to provide when you have colored images you have to include that final dimension

49
00:03:46,170 --> 00:03:47,480
for the color.

50
00:03:47,490 --> 00:03:47,860
All right.

51
00:03:47,850 --> 00:03:48,840
And so where do we get.

52
00:03:48,840 --> 00:03:56,150
Well as usual we use the ad method to add a new layer whether it is a fully connected layer or drop

53
00:03:56,150 --> 00:03:58,200
out layer or convolution or layer.

54
00:03:58,200 --> 00:04:05,910
You use this add method from your model object and to which you input Well the layer you want which

55
00:04:05,910 --> 00:04:11,070
this time account to these layer meaning a convolution in two dimensions.

56
00:04:11,070 --> 00:04:16,300
And the cons to this is actually a class taken from the layers module by the carer's library from tensor

57
00:04:16,320 --> 00:04:16,830
flow.

58
00:04:16,900 --> 00:04:19,060
But once again you don't have to remember all this.

59
00:04:19,140 --> 00:04:24,600
This is a notebook that you can keep close to you whenever you're working on your project so that you

60
00:04:24,600 --> 00:04:33,120
can just take this and paste it by making sure to change the right input shape for your particular application.

61
00:04:33,130 --> 00:04:38,520
Right then as we say we have 32 filters that's all the arguments you have to input into comms through

62
00:04:38,520 --> 00:04:38,970
the class.

63
00:04:38,970 --> 00:04:43,450
We have 32 filters of size three by three that's the kernel size.

64
00:04:43,570 --> 00:04:52,110
Same padding erected via activation function and the input shape you know the shape of our input images

65
00:04:52,440 --> 00:04:54,510
which are 32 by 32.

66
00:04:54,600 --> 00:04:57,210
And the three channels because they are colored images.

67
00:04:57,660 --> 00:04:57,920
All right.

68
00:04:57,930 --> 00:05:05,400
So very very simple now let's add the second convolution layer and actually the max pooling layer Max

69
00:05:05,400 --> 00:05:13,230
pooling consist of only keeping the highest value when striding with a filter so as to reduce the size

70
00:05:13,230 --> 00:05:17,700
of the output tensor and therefore reduce the complexity in the training.

71
00:05:17,700 --> 00:05:24,620
But however we saw that we still keep the information so that our model can still be able to learn.

72
00:05:24,810 --> 00:05:25,040
Right.

73
00:05:25,050 --> 00:05:28,320
So let's do this let's add the second convolution layer.

74
00:05:28,320 --> 00:05:34,290
Here are the hyper parameters we're gonna use again 32 filters of size three by three.

75
00:05:34,350 --> 00:05:41,470
Again the same padding and erected fire activation function and then for the max pool layer we're going

76
00:05:41,470 --> 00:05:47,600
to have the following hyper parameters a pool size of 2 astride of 2 so it's going to stride every two

77
00:05:47,600 --> 00:05:48,140
cells.

78
00:05:48,300 --> 00:05:56,250
And this time a valid bearing which I remind is when you don't consider a column of zeroes and therefore

79
00:05:56,250 --> 00:06:01,050
you only take you know all the columns except the last one and therefore you lose information of the

80
00:06:01,050 --> 00:06:01,790
last one.

81
00:06:01,810 --> 00:06:09,330
You don't add a column of zeros here and so there we go when going back to our second convolution layer

82
00:06:09,480 --> 00:06:10,700
we use this valid bearing.

83
00:06:10,710 --> 00:06:13,710
And so where do we get in terms of coding.

84
00:06:13,710 --> 00:06:19,350
Well once again we use the ad method from our model which for the second convolution will layer will

85
00:06:19,350 --> 00:06:23,290
input again the comes to the class and which takes as input.

86
00:06:23,310 --> 00:06:31,710
Well all these parameters here the hyper parameters 32 filters of size 3 by 3 same padding and rectify

87
00:06:31,740 --> 00:06:33,300
activation function.

88
00:06:33,300 --> 00:06:39,720
And then once the second convolution layer is added Well we choose to add a max pulling layer and to

89
00:06:39,720 --> 00:06:44,700
do so we use to add methods once again and then the max pool to the class.

90
00:06:44,700 --> 00:06:51,660
That's a class that will create a max pooling layer with Max pooling applied on your last layer.

91
00:06:52,080 --> 00:06:58,830
And so as we saw well we have a pool size of 2 as tries to and this time a valid padding.

92
00:06:58,830 --> 00:06:59,370
All right.

93
00:06:59,370 --> 00:07:04,310
So now you know how to add a convolution layer and you know how to add a max balloon layer.

94
00:07:04,330 --> 00:07:07,770
And now let's add a third convolution or layer.

95
00:07:07,770 --> 00:07:11,790
So in case you're wondering how did we choose such an architecture.

96
00:07:11,940 --> 00:07:14,520
Well you know the data set is so classic.

97
00:07:14,610 --> 00:07:21,390
So for 10 data set that there are actually many architectures which were already identified as good

98
00:07:21,390 --> 00:07:23,760
ones and this is just one of them.

99
00:07:23,790 --> 00:07:28,560
This is a good architecture for the C for 10 data set that allow us to have good results.

100
00:07:28,590 --> 00:07:36,090
However I just changed some little things inside so that you can then practice and try to improve the

101
00:07:36,090 --> 00:07:39,950
final accuracy that will get in this section.

102
00:07:40,020 --> 00:07:44,520
So let's add the third convolution layer where I'm gonna be the hyper parameters this time.

103
00:07:44,610 --> 00:07:52,860
Well this time we're going to add 64 filters of size three by three a same batting rectify activation

104
00:07:52,860 --> 00:07:55,120
function and nothing.

105
00:07:55,200 --> 00:08:02,070
This is actually something I forgot to remove from the copy paste of course for the second or third

106
00:08:02,070 --> 00:08:04,090
or next convolution layers.

107
00:08:04,110 --> 00:08:09,780
Well you don't have the input shape argument because this is only for the input images at the beginning

108
00:08:10,130 --> 00:08:10,860
and so there you go.

109
00:08:10,860 --> 00:08:16,120
As you can see very practical and Google collab you can just modify your text very easily.

110
00:08:16,290 --> 00:08:21,930
And so when we add the third convolution layer we use the N method once again that comes to the class

111
00:08:21,930 --> 00:08:29,900
once again which takes as arguments 64 filters of size three by three the kernel size same patting and

112
00:08:29,940 --> 00:08:32,340
erect rectify activation function.

113
00:08:32,340 --> 00:08:33,120
All right.

114
00:08:33,120 --> 00:08:39,300
And now we're going to add a final fourth convolution layer and a final Max pulling there.

115
00:08:39,300 --> 00:08:45,810
And that will be the end of the convolution all part of the CNN because then remember we have the fully

116
00:08:45,810 --> 00:08:48,630
connected part with the fully connected layers.

117
00:08:48,630 --> 00:08:50,900
And so let's scroll down a bit.

118
00:08:51,210 --> 00:08:55,970
Here is what we get for the hyper parameters of this fourth convolution layer.

119
00:08:55,980 --> 00:09:04,210
Well once again 64 filters of side 3 by 3 a same pairing once again and a rectify activation function.

120
00:09:04,470 --> 00:09:06,450
And then for the max pooling layer.

121
00:09:06,450 --> 00:09:12,850
Well we had the same hyper parameters with the pool size of 2 astride of two and a valid padding which

122
00:09:12,870 --> 00:09:18,720
can remember if you like rule of thumb is that you're going to use the same padding for the simple convolution

123
00:09:18,720 --> 00:09:22,780
of layers and valid padding for the max pulling layers.

124
00:09:22,800 --> 00:09:29,100
And the reason for this is that it's OK to lose the information when doing Max pulling but you want

125
00:09:29,100 --> 00:09:35,250
to keep the maximum information inside your convolution all layers and since the same padding includes

126
00:09:35,280 --> 00:09:43,170
this zero columns which allows to take all the different cells in your tensor well you're keeping the

127
00:09:43,170 --> 00:09:47,760
maximum information as opposed to the valid padding.

128
00:09:47,760 --> 00:09:48,450
All right.

129
00:09:48,450 --> 00:09:49,850
So now.

130
00:09:49,860 --> 00:09:50,980
Well that's the same.

131
00:09:51,000 --> 00:09:57,290
We used to add method comes to the class for the convolution layer 64 filters of side 3 by 3.

132
00:09:57,360 --> 00:10:01,700
Same padding and rectify activation function and for the max bullying there.

133
00:10:01,800 --> 00:10:06,060
Well we use the pro size of to strive to an invalid padding.

134
00:10:06,080 --> 00:10:06,720
All right.

135
00:10:06,720 --> 00:10:09,280
So now that's the end of the convolution all part.

136
00:10:09,310 --> 00:10:17,250
We're now ready to flatten the final output of all these convolutions and Max pulling into a one dimensional

137
00:10:17,370 --> 00:10:18,210
layer.

138
00:10:18,390 --> 00:10:20,250
And that's exactly what we do here.

139
00:10:20,280 --> 00:10:25,950
Adding the flattening layer and to do it well sense of flow 2.0 will do everything for you.

140
00:10:25,980 --> 00:10:33,150
You just have to use the add method once again and then from the caris layers module by tensor flow

141
00:10:33,560 --> 00:10:36,140
you're going to take the flatten class.

142
00:10:36,450 --> 00:10:42,210
And you know you don't have anything to input here it will automatically recognize what was here before

143
00:10:42,810 --> 00:10:48,860
and therefore understand what to flatten you the output of all the operations that happen here.

144
00:10:48,870 --> 00:10:51,110
So very easy to add the flattening layer.

145
00:10:51,120 --> 00:10:55,280
And now you're ready for the full connections.

146
00:10:55,290 --> 00:11:01,170
Now you're going to recognize everything because this is just basically the same as what we saw before

147
00:11:01,230 --> 00:11:03,000
with artificial neural network.

148
00:11:03,090 --> 00:11:10,590
Basically since the flattening layer is now a one dimensional input it can go into a new fully connected

149
00:11:10,890 --> 00:11:11,940
neural network.

150
00:11:11,940 --> 00:11:19,530
And so that's exactly why we're adding a first fully connected layer of 128 hidden neurons rectify activation

151
00:11:19,530 --> 00:11:22,530
function because this is still not the output layer.

152
00:11:22,530 --> 00:11:23,440
And so there we go.

153
00:11:23,460 --> 00:11:24,810
You recognize this.

154
00:11:24,810 --> 00:11:31,590
We used to add method from our model object and we use again the dance class which is used to build

155
00:11:31,590 --> 00:11:34,180
indeed a fully connected layer.

156
00:11:34,290 --> 00:11:41,360
And it takes as input 128 headed neurons the unit and rectify activation function.

157
00:11:41,490 --> 00:11:47,490
And we don't have to specify the input shape because we have something before which is the flattening

158
00:11:47,490 --> 00:11:48,010
layer.

159
00:11:48,030 --> 00:11:54,630
You know we already used this add method so that sense of flow understands what's here before because

160
00:11:54,630 --> 00:12:00,330
as opposed to the previous section it's not something we provide as inputs like the input images.

161
00:12:00,330 --> 00:12:07,710
So you only use the input shape when nothing was added yet into your model object.

162
00:12:07,710 --> 00:12:08,160
All right.

163
00:12:08,160 --> 00:12:14,460
And finally we add the output layer which is going to have 10 output neurons of course because we have

164
00:12:14,760 --> 00:12:22,530
10 classes in total and a soft Max activation function to get the probabilities for each of the classes.

165
00:12:22,710 --> 00:12:26,330
And so well you know how to do this you use the add method.

166
00:12:26,340 --> 00:12:32,850
Once again the dance class once again because we want to establish full connections between the previous

167
00:12:33,120 --> 00:12:39,360
layer you know the hidden layer here and the output layer and a soft Max activation function indeed

168
00:12:39,690 --> 00:12:41,270
for the probabilities.

169
00:12:41,490 --> 00:12:47,280
And so well now we're going to get a pretty cool summary of our model because it's actually pretty big

170
00:12:47,280 --> 00:12:48,570
compared to before.

171
00:12:48,570 --> 00:12:51,020
Let's see here it is.

172
00:12:51,020 --> 00:12:53,520
Here is a model we've just built with.

173
00:12:53,550 --> 00:12:56,410
Indeed first convolution all layer.

174
00:12:56,520 --> 00:13:02,160
Then a second convolution layer then a first Max pulling layer.

175
00:13:02,160 --> 00:13:08,360
Then a third convolution layer a fourth convolution layer and then a second Max pulling layer.

176
00:13:08,490 --> 00:13:15,380
Then we flatten the whole output of all these operations that happen here which therefore gives us a

177
00:13:15,390 --> 00:13:22,740
one dimensional layer which we can fully connect to a first dance hidden layer and then finally which

178
00:13:22,740 --> 00:13:29,730
we connect to the output layer through other full connections and so well we have in total five hundred

179
00:13:29,730 --> 00:13:35,490
and ninety one thousand two hundred seventy four weights there are trainable you know there are gonna

180
00:13:35,510 --> 00:13:41,130
be a data that's where we had the same number through the iterations and once again no hyper parameters

181
00:13:41,280 --> 00:13:43,560
inside the neural network.

182
00:13:43,590 --> 00:13:44,400
So there we go.

183
00:13:44,400 --> 00:13:50,660
We can clearly call it a deep learning model it's actually pretty deep with many hidden layers.

184
00:13:50,820 --> 00:13:53,670
And that's usually the case for CNN.

185
00:13:53,910 --> 00:14:00,330
And luckily we have today powerful enough computing resources to be able to train such a model to classify

186
00:14:00,330 --> 00:14:02,450
some images.

187
00:14:02,460 --> 00:14:03,090
All right.

188
00:14:03,090 --> 00:14:04,590
So we're going to take a little break now.

189
00:14:04,620 --> 00:14:10,740
After having built this huge deep learning model and I'll see you in the next tutorial to of course

190
00:14:10,890 --> 00:14:16,380
train it after which we'll get some first accuracy of the training but it's not a victory yet.

191
00:14:16,380 --> 00:14:22,140
Remember what we want to get is a great accuracy on the evaluation set as well you know on the test

192
00:14:22,140 --> 00:14:22,810
set.

193
00:14:22,860 --> 00:14:28,170
So we'll see what we'll get and then no worries if you're not satisfied about the result you'll get

194
00:14:28,170 --> 00:14:33,360
the opportunity to practice and try to improve the accuracy and we'll of course provide an improvement

195
00:14:33,360 --> 00:14:34,230
solution for this.
