1
00:00:00,880 --> 00:00:05,800
Hello everyone and welcome to this python tutorial in this video.

2
00:00:05,850 --> 00:00:13,850
We will complete the whole dataset pre processing project but before we jump into the code we need to

3
00:00:13,850 --> 00:00:21,680
discuss a few things that the tensor flow transform library lays down on a Apache beam backend.

4
00:00:21,910 --> 00:00:28,750
And the reason for that is simply because they want to create scalable data set transform pre processing

5
00:00:28,750 --> 00:00:37,010
library they achieve that by using Apache beam the Apache Beam is a whole infrastructure on its own

6
00:00:37,820 --> 00:00:40,670
and it can get really complex.

7
00:00:40,760 --> 00:00:45,220
So we are not going to explain it in great detail since it's not the cause.

8
00:00:45,230 --> 00:00:53,060
The main on the other hand I encourage you to visit the documentation site and seek more readings on

9
00:00:53,060 --> 00:00:55,260
the topic of the Apache beam.

10
00:00:55,400 --> 00:01:00,590
You can build a lot of exciting applications around the Apache beam itself.

11
00:01:00,750 --> 00:01:06,360
Let's go together for these notes that I provided for you and see what we have there.

12
00:01:06,380 --> 00:01:14,030
We have a few arguments dict features as you know that is our dataset converted to Python dictionaries

13
00:01:14,480 --> 00:01:24,110
and again interlaced then we have a data metadata which represents a meta information about each column.

14
00:01:24,110 --> 00:01:27,350
With that we have in our dataset.

15
00:01:27,560 --> 00:01:35,020
And lastly we have our pre processing function that we created in the previous video and we are going

16
00:01:35,020 --> 00:01:42,150
to use that on top of our dict features to create our normalized data.

17
00:01:42,170 --> 00:01:49,380
Now we will talk about the special syntax that is used because of the Apache being as you can see here

18
00:01:49,440 --> 00:01:53,490
the result is equal to data to pass.

19
00:01:53,490 --> 00:02:01,890
So some data that we have to pass to serve the function and then it is followed by this pipe which is

20
00:02:01,950 --> 00:02:09,440
the special syntax that we are talking about here and after that we need to specify where to parse the

21
00:02:09,440 --> 00:02:09,920
data.

22
00:02:10,280 --> 00:02:15,190
So in most cases it is going to be some function.

23
00:02:15,210 --> 00:02:22,120
There are a lot of reasons why they choose this syntax instead of standard Python syntax one of the

24
00:02:22,120 --> 00:02:29,620
reasons is because this transformation of the data needs to be a scalable away and it can be really

25
00:02:29,620 --> 00:02:31,610
complex really fast.

26
00:02:32,450 --> 00:02:35,420
There are a lot of things that can be stacked together.

27
00:02:35,450 --> 00:02:42,650
For example you can normalize data from 0 to 1 and then again normalize it by extracting meaning from

28
00:02:42,650 --> 00:02:42,800
it.

29
00:02:42,800 --> 00:02:48,370
For example let's break down our case that we are going to do in this video.

30
00:02:49,850 --> 00:02:57,730
We have our results that are transformed data set and transform function after death.

31
00:02:57,770 --> 00:03:00,630
We have to specify what data to pass.

32
00:03:00,770 --> 00:03:09,560
Somewhere that data in our case is the features which is our data set and the information about the

33
00:03:09,560 --> 00:03:10,960
data set itself.

34
00:03:11,030 --> 00:03:13,660
So our metadata.

35
00:03:13,810 --> 00:03:21,540
Lastly we need to specify where to parse the data set from the Apache being back and we are using this

36
00:03:21,690 --> 00:03:23,670
analyze and transform data set.

37
00:03:23,670 --> 00:03:26,020
Function it has two parts.

38
00:03:26,020 --> 00:03:31,060
As you can see analyzed pirate is there because we are using metadata as well.

39
00:03:31,960 --> 00:03:39,820
And then this transform is the part where we use our pre processing function to create normalized data

40
00:03:39,820 --> 00:03:41,260
set.

41
00:03:41,280 --> 00:03:47,940
The only argument in this function is the pre processing function that we wrote.

42
00:03:48,040 --> 00:03:49,930
Here is the final syntax.

43
00:03:49,930 --> 00:03:58,480
If we follow the syntax they choose the first output or result are transform data then transform function

44
00:03:59,200 --> 00:04:02,470
the transform function is our pre processing function.

45
00:04:02,470 --> 00:04:07,620
So we don't need it here at all then the data to pass.

46
00:04:07,650 --> 00:04:11,670
In our case the features and data metadata.

47
00:04:11,880 --> 00:04:19,230
Then we have this pipe which is our special syntax and where to parse the data as we said where to pester

48
00:04:19,250 --> 00:04:19,830
data.

49
00:04:20,090 --> 00:04:27,470
Well in our case analyze and transform data set build around our pre processing function.

50
00:04:27,560 --> 00:04:34,660
I know it's a very weird syntax and to be honest I never seen it before it took some time for me to

51
00:04:34,660 --> 00:04:35,850
get my head around it.

52
00:04:36,010 --> 00:04:37,430
So be patient.

53
00:04:37,630 --> 00:04:41,010
Try to practice it as much as possible and you will get there.

54
00:04:42,200 --> 00:04:46,410
The final function is called data transform.

55
00:04:46,510 --> 00:04:55,090
It doesn't have any arguments and we are merely going to write with DFT beam that we imported in the

56
00:04:55,090 --> 00:04:56,830
very beginning.

57
00:04:56,830 --> 00:05:04,160
Then create context in this context is very important because of the Apache beam.

58
00:05:04,240 --> 00:05:06,340
This is the way Apache being works.

59
00:05:06,340 --> 00:05:12,580
It creates context and tries to pre process or prepare data in that context.

60
00:05:12,580 --> 00:05:17,270
As you can see it's optional but it's a good practice to put them up.

61
00:05:17,310 --> 00:05:19,770
Their argument for it.

62
00:05:19,780 --> 00:05:24,070
We are going to use this temp file library that imported at the very beginning.

63
00:05:24,820 --> 00:05:36,810
So right from that library M.K. the temp which is shorter for make directory temporary and call it it

64
00:05:36,810 --> 00:05:43,930
is going to create temporary directory for the context in store all warnings in all logs while the data

65
00:05:43,930 --> 00:05:49,340
set pre processing is happening in that context.

66
00:05:49,360 --> 00:05:56,290
We are going to call our weird syntax line from our notes so you can simply copy that line from the

67
00:05:56,290 --> 00:06:03,900
notes and paste here our output is transform data and our transform function.

68
00:06:04,120 --> 00:06:12,280
But as we said we don't need these function anymore and that is all equal to our inputs since we have

69
00:06:12,280 --> 00:06:14,520
defined that above those inputs.

70
00:06:14,560 --> 00:06:18,700
We don't need to provide them as an argument for to this function.

71
00:06:18,700 --> 00:06:21,760
We could obviously do that but it's not necessary.

72
00:06:22,670 --> 00:06:29,480
Followed by this special syntax pipe and then we are going to call our analyze and transform data set

73
00:06:29,480 --> 00:06:36,590
function that is going to be built around our pre processing function and that's it.

74
00:06:36,620 --> 00:06:40,960
Now we are done with our context and our pre processing.

75
00:06:41,300 --> 00:06:49,370
And as you can see one part of our return data is called transform data set it consists actually from

76
00:06:49,370 --> 00:06:58,100
two parts our data and metadata so you can split it to two different parts transform data and transform

77
00:06:58,250 --> 00:06:59,710
metadata.

78
00:06:59,720 --> 00:07:05,970
You can send that to be equal to our transform data set and that's pretty much it.

79
00:07:06,010 --> 00:07:13,840
We unpacked it and we can now print it out to how well we scale our data since for the printing part

80
00:07:13,840 --> 00:07:14,740
of this function.

81
00:07:14,740 --> 00:07:16,730
We are simply using a for loop.

82
00:07:16,900 --> 00:07:21,650
I'm going to skip to the part where I've already written that.

83
00:07:21,850 --> 00:07:23,310
And welcome back.

84
00:07:23,320 --> 00:07:28,800
This is our for loop which is going to print out all of our outputs.

85
00:07:28,800 --> 00:07:33,690
We are going to run for each example in our data set at this point.

86
00:07:33,690 --> 00:07:41,490
Doesn't matter if you use landfall transform data or dict features since it has the same number of samples

87
00:07:42,620 --> 00:07:51,380
then we are going to say Okay print out the current one of the raw data from our dict features and transform

88
00:07:51,380 --> 00:07:52,520
data.

89
00:07:52,520 --> 00:07:54,200
That's pretty much it.

90
00:07:54,260 --> 00:07:58,510
If this was the part of the pipeline you would return your transform data.

91
00:07:58,790 --> 00:08:06,080
But now since we are just doing the pre processing we are going to simply print it out let's call it.

92
00:08:06,410 --> 00:08:12,040
And are you ready execute a cell and as you can see it is working.

93
00:08:12,050 --> 00:08:14,780
There are a lot of warnings happening in the background.

94
00:08:14,960 --> 00:08:18,730
Everything is scaling and now we have our results printed out.

95
00:08:19,800 --> 00:08:24,950
Here for example we have a raw data for forty four point thirty eight.

96
00:08:25,290 --> 00:08:32,480
But here transform data is minus eighteen point fifty eight then we have this zero point twenty eight.

97
00:08:32,630 --> 00:08:40,080
And so on at this point everything is automatic so we don't have to do anything manually anymore.

98
00:08:40,120 --> 00:08:42,090
I hope you understand everything here.

99
00:08:42,100 --> 00:08:44,100
If you have any questions please.

100
00:08:44,260 --> 00:08:51,790
I do encourage you to ask us if you need any help on your project for example and also I encourage you

101
00:08:51,790 --> 00:08:59,350
to read more about Apache BMC index since it is going to take some time to get used to this index that

102
00:08:59,350 --> 00:09:01,270
we used in this video.

103
00:09:01,270 --> 00:09:06,000
If you have any questions or comments so far please post them in the comments section.

104
00:09:06,200 --> 00:09:08,100
Otherwise I've seen the next video.
