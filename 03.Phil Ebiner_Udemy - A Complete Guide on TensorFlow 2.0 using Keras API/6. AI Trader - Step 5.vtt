WEBVTT
1
00:00:00.550 --> 00:00:03.940
Hello everyone and welcome to this biking tutorial.

2
00:00:03.940 --> 00:00:10.420
In the previous video we have implemented the trade function that takes a state as an input and returns

3
00:00:10.420 --> 00:00:12.850
an action based on that state.

4
00:00:13.000 --> 00:00:20.940
And now in this video we will implement the last piece of the AA trade model and that is the custom

5
00:00:20.940 --> 00:00:22.990
training function.

6
00:00:23.080 --> 00:00:30.070
It will take a batch of saved data and train the model based on that let's define it.

7
00:00:30.190 --> 00:00:38.880
Def that train and this function takes only one argument and that is the badge size this argument could

8
00:00:38.880 --> 00:00:47.150
be anywhere from 32 to 256 or even more and this is an additional hyper parameter that we want to play

9
00:00:47.150 --> 00:00:47.910
with.

10
00:00:47.990 --> 00:00:54.050
The first thing that we have to do is to select the data from the experience replay memory so right

11
00:00:54.490 --> 00:00:57.020
batteries equal to an empty list.

12
00:00:57.050 --> 00:00:59.350
Then we have to iterate through the memory.

13
00:00:59.750 --> 00:01:09.830
For i in range then Len of self-doubt memory and remember this memory is defined up here as the Q data

14
00:01:09.830 --> 00:01:10.880
structure.

15
00:01:11.240 --> 00:01:19.460
Then Right mine is batch size plus 1 because we are dealing with a time constraint data.

16
00:01:19.670 --> 00:01:23.570
We don't want to randomly select samples from the memory.

17
00:01:23.800 --> 00:01:30.170
We will always sample from the end of the memory and this indexing method will help us to get the exact

18
00:01:30.290 --> 00:01:33.230
number of points inside the batch of data.

19
00:01:33.220 --> 00:01:39.980
Let's complete the for loop statement by defining the end index to be the land of self-taught memory

20
00:01:41.700 --> 00:01:48.090
now that the for loop is done inside it call the batch list and append that element from the memory

21
00:01:48.090 --> 00:01:51.140
itself at this point.

22
00:01:51.160 --> 00:01:57.310
We have the bedroom data and now it's time to iterate through it and to train the model for each sample

23
00:01:57.310 --> 00:02:07.410
from that batch so let's write for state comma action comma reward Comma next state.

24
00:02:07.520 --> 00:02:15.620
Come on done in batch with this four statement we are iterating through that all information stored

25
00:02:15.710 --> 00:02:19.940
inside the batch of data just be very careful here.

26
00:02:19.940 --> 00:02:25.360
The order of variables is very important and inside the for loop right.

27
00:02:25.410 --> 00:02:28.120
Reward is equal to reward.

28
00:02:28.160 --> 00:02:31.990
Let's make sure that our agent is not in the terminal state.

29
00:02:32.180 --> 00:02:40.660
So let's write if not the done we are doing this very simple check to make sure that our agent is not

30
00:02:40.660 --> 00:02:47.330
in the terminal state and there are a few more actions to be plate based on this information alone.

31
00:02:47.360 --> 00:02:54.620
We can calculate our reward in a different ways if the agent is in a terminal state we will use the

32
00:02:54.620 --> 00:02:56.570
current reward as a reward.

33
00:02:57.410 --> 00:03:02.810
But if it's not in the terminal state and there are a few more anxious to be played we are going to

34
00:03:02.810 --> 00:03:10.740
calculate this count the total leave all as the current reward so in this if statement just right the

35
00:03:10.760 --> 00:03:15.900
reward is equal to reward plus self-doubt gamma.

36
00:03:16.200 --> 00:03:24.720
And you know the formula multiplied by MP dot a max and these function returns are the maximum value

37
00:03:25.050 --> 00:03:29.240
from an input array and that is exactly what we want.

38
00:03:29.340 --> 00:03:33.780
We want to return the highest value of predictions.

39
00:03:33.780 --> 00:03:43.230
And inside that function provide cell dot model but predict of next state and said that to zero because

40
00:03:43.230 --> 00:03:44.820
of the output side.

41
00:03:45.120 --> 00:03:51.960
And that is our discount that total reward after they've defined the target variable which is predicted

42
00:03:51.960 --> 00:04:02.520
by the model as well so right target is able to sell the model that predict of state at this point.

43
00:04:02.550 --> 00:04:09.760
It is just an action and we want to modify it with our current reward and this is exactly why we use

44
00:04:09.760 --> 00:04:18.380
a means squared error instead of cross entropy loss to complete the target right target of zero because

45
00:04:18.380 --> 00:04:21.940
of the output shape and then action.

46
00:04:22.030 --> 00:04:27.390
And this is the action performed and that is all equal to reward.

47
00:04:27.430 --> 00:04:30.440
Now we have our target and our state.

48
00:04:30.490 --> 00:04:36.060
Now it's time to feed the model just write some dot model dot fit.

49
00:04:36.880 --> 00:04:45.040
And as always provide state for the features and target for our target epochs just set to one because

50
00:04:45.040 --> 00:04:49.660
we will train the model very often on each sample from our batch.

51
00:04:49.960 --> 00:04:52.840
We don't want to print all these training results.

52
00:04:52.900 --> 00:04:58.550
So just put verbose is equal to zero at the end of this function.

53
00:04:58.590 --> 00:05:06.950
Let's decrease the Epsilon parameter so we can stop performing random actions at one point right.

54
00:05:07.010 --> 00:05:15.490
If self-taught Epsilon is bigger than self-taught Epsilon final and if that is correct led decrease

55
00:05:15.500 --> 00:05:23.620
it by multiplying it with Epsilon decay and execute a cell and with this function we are all done with

56
00:05:23.710 --> 00:05:28.740
a traitor class if you have any questions or comments so far.

57
00:05:28.910 --> 00:05:33.380
Please post them in the comments section otherwise I'll see in the next tutorial.
