WEBVTT
1
00:00:00.630 --> 00:00:01.880
Hello and welcome back to the course.

2
00:00:01.890 --> 00:00:08.310
On a I in the previous part we talked about the deep learning Hillary intuition we started there.

3
00:00:08.310 --> 00:00:14.460
And in fact we actually got all the way to this part and where we talked about learning.

4
00:00:14.460 --> 00:00:18.110
And now we're going to move on to the actual acting part.

5
00:00:18.150 --> 00:00:22.190
So there's in there's two parts two distinct parts that we have to remember.

6
00:00:22.220 --> 00:00:25.440
So that's the learning part but now he actually he's done all of this.

7
00:00:25.440 --> 00:00:26.310
That's beautiful.

8
00:00:26.310 --> 00:00:27.830
No he actually has to take an action.

9
00:00:27.840 --> 00:00:31.500
He has to decide what he's going to do is going to do action one two three or four.

10
00:00:31.680 --> 00:00:32.970
And so how does he do that.

11
00:00:32.970 --> 00:00:37.100
Well the way he does it is now given those same cue values.

12
00:00:37.100 --> 00:00:38.930
So the key values don't change after we've.

13
00:00:38.970 --> 00:00:40.960
We have these key values of compared them with calico.

14
00:00:40.980 --> 00:00:45.330
The laws we bear protected area we've updated the weights but the key values don't change in that whole

15
00:00:45.330 --> 00:00:45.660
process.

16
00:00:45.670 --> 00:00:48.220
So after we've got the key values there they're fixed.

17
00:00:48.300 --> 00:00:49.590
We know what they are sold.

18
00:00:49.590 --> 00:00:53.880
This happens though networks updated and using those same key values that we had.

19
00:00:53.910 --> 00:00:58.560
What are we going to do is we're going to pass them through a soft max function.

20
00:00:58.560 --> 00:01:05.460
And again a soft Max is described in I think in Annex 2 and we'll talk a bit more about soft Max further

21
00:01:05.460 --> 00:01:12.090
down in or we'll talk about this action selection policy further down in the rest of this section.

22
00:01:12.090 --> 00:01:17.220
So just in a few tutorials but for now we're just going to say we're passing it through a soft max function.

23
00:01:17.220 --> 00:01:20.100
Basically what it does is it allows it helps select the best one.

24
00:01:20.120 --> 00:01:23.610
It's select the best action possible and there's a small caveat to that.

25
00:01:23.610 --> 00:01:25.990
It's not just the best on possible.

26
00:01:26.100 --> 00:01:28.880
We'll talk about that in the action selection policy tutorial.

27
00:01:28.890 --> 00:01:32.070
But for now let's just say it selects the best action from here it says.

28
00:01:32.080 --> 00:01:32.290
Okay.

29
00:01:32.310 --> 00:01:35.560
So Q1 you know the likelihood.

30
00:01:35.610 --> 00:01:37.580
Yes basically we know the queue values.

31
00:01:37.620 --> 00:01:40.910
So it's predicted the queue value so it's it can look at them and say okay.

32
00:01:40.920 --> 00:01:46.710
So the highest value out of these just as we did in the simple queue learning algorithm it'll just look

33
00:01:46.710 --> 00:01:50.090
at all these for say the highest key values this one and I'm going to select that action.

34
00:01:50.100 --> 00:01:52.050
We're going to take those and that's pretty much it.

35
00:01:52.140 --> 00:01:57.000
That's how it chooses which action take takes it takes the action and then all of this process happens

36
00:01:57.000 --> 00:02:02.070
again for for the next state the add agent ends up in our case in the next square of the maze.

37
00:02:02.070 --> 00:02:04.560
But generally speaking it's the next state.

38
00:02:04.560 --> 00:02:05.330
So there we go.

39
00:02:05.340 --> 00:02:14.610
That's how we feed in a reinforcement learning problem into a neural network through a vector describing

40
00:02:14.610 --> 00:02:16.070
the state that we're in.

41
00:02:16.110 --> 00:02:20.520
And once we feed it there's two parts of the process that happen.

42
00:02:20.520 --> 00:02:22.350
Part one is the learning.

43
00:02:22.350 --> 00:02:26.760
So remember that part where we compare each of the Q values with the targets and then we back propagate

44
00:02:26.760 --> 00:02:32.310
the loss through the network to update the weights so that our network is learning as we go through

45
00:02:32.310 --> 00:02:37.980
this maze or through this environment and also the second part is of course we have to act we have to

46
00:02:37.980 --> 00:02:45.360
select an action and that is where we pass the key values through soft max function and or basically

47
00:02:45.360 --> 00:02:48.420
an action selection policy which we'll talk about further down.

48
00:02:48.420 --> 00:02:52.790
And then we simply select the action that we want to take and we perform that action.

49
00:02:52.800 --> 00:02:57.120
And then this whole process starts again and then maybe the agent gets then maybe the agent doesn't

50
00:02:57.120 --> 00:02:59.490
pass the the game.

51
00:02:59.580 --> 00:03:05.760
In any case the game ends and then once again the whole whole process repeats the agent plays the whole

52
00:03:05.760 --> 00:03:08.220
game again and then that stopped.

53
00:03:08.220 --> 00:03:14.490
So basically that's that's another epoch every time the agent you know every time the game ends whether

54
00:03:14.490 --> 00:03:16.640
favorably on February that's the end of an epoch.

55
00:03:16.650 --> 00:03:20.370
And then he starts again and then he starts again and then he starts again and so on.

56
00:03:20.370 --> 00:03:26.490
So that happens and this process happens for every single time the agent is a new in a new state.

57
00:03:26.520 --> 00:03:31.920
So the state is encoded here so that's important not just for every single game that he plays but for

58
00:03:31.920 --> 00:03:32.970
every single state.

59
00:03:32.970 --> 00:03:34.040
So he's in a state.

60
00:03:34.050 --> 00:03:38.090
It goes through this process dates and so on and happens every single time.

61
00:03:38.100 --> 00:03:41.100
And so the learning happens and the acting happens as well.

62
00:03:41.670 --> 00:03:42.810
So that is deep.

63
00:03:42.810 --> 00:03:47.040
Q learning in the intuition behind deep learning.

64
00:03:47.040 --> 00:03:49.610
We've got lots more to cover off.

65
00:03:49.710 --> 00:03:55.620
And then of course we get the practical and in the meantime if you'd like to get some additional information

66
00:03:55.650 --> 00:04:04.200
on deep learning we've got a recommended reading so we've already spoken about Arthur Giuliani's series

67
00:04:04.200 --> 00:04:05.110
of blog posts.

68
00:04:05.130 --> 00:04:12.510
If you look at simple reinforcement learning with test flow Part 4 you will find the part that's relevant

69
00:04:12.510 --> 00:04:14.200
to what we discussed today.

70
00:04:14.220 --> 00:04:21.110
Note that here he talks about convolutions we are not covering revolutions in this section we're going

71
00:04:21.110 --> 00:04:23.660
to be talking about them in the next section of the course.

72
00:04:23.670 --> 00:04:28.800
So the difference here is that it's just kind of skip the Coalition's part for now and we'll talk about

73
00:04:28.800 --> 00:04:34.500
them in the next part of the course but the difference is in convolutions You're like looking your agent

74
00:04:34.530 --> 00:04:38.880
is looking at the image and and therefore he has to process an image.

75
00:04:38.880 --> 00:04:43.410
An additional complication for now where we're slowly gradually building up to that.

76
00:04:43.500 --> 00:04:47.510
For now we're encoding our environment through.

77
00:04:47.520 --> 00:04:53.190
So if you look here we're encoding our environment or maybe like look at this one probably encoding

78
00:04:53.190 --> 00:04:58.650
our environment as a or in calling a state the agent is in as a vector.

79
00:04:58.650 --> 00:05:01.380
So in our case was the very simple vector of two values.

80
00:05:01.440 --> 00:05:06.570
Sometimes people even in that in that simple may sometimes or as you'll see from this blog post sometimes

81
00:05:06.570 --> 00:05:10.140
we will prefer the one hot and coded version of that state.

82
00:05:10.140 --> 00:05:13.480
So basically where every single box of the maze has a.

83
00:05:13.570 --> 00:05:17.740
So you have like a vector of four in our case would be 12 values three by four.

84
00:05:17.740 --> 00:05:22.950
So it's like either 1 or a 0 depending on which elements in which box you're in in the environment.

85
00:05:22.980 --> 00:05:29.850
So in whichever way you decide to encode your environment and the state of your environment that's how

86
00:05:29.850 --> 00:05:31.500
encoding it is basically a vector.

87
00:05:31.500 --> 00:05:36.360
The key here is that it's not a convolution so it's not like an image and there's no convolution vote.

88
00:05:36.360 --> 00:05:37.720
So this part will come later.

89
00:05:37.770 --> 00:05:43.500
For us it starts over here and that just simplifies the process for us to gradually understand better.

90
00:05:43.500 --> 00:05:49.080
And of course don't forget that this blog post is an intensive flow and we're using pi torch in our

91
00:05:49.090 --> 00:05:56.800
tutorials so hopefully you enjoyed this quick intro into a deep convolution all deep not convulsion

92
00:05:57.240 --> 00:05:59.130
yet deeper Q learning.

93
00:05:59.250 --> 00:06:02.880
And on that note I look forward to seeing you next time.

94
00:06:02.890 --> 00:06:05.390
And until then enjoy our artificial intelligence.
