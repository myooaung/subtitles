1
00:00:00,360 --> 00:00:02,980
Hello and welcome back to the course on the deep learning.

2
00:00:03,060 --> 00:00:06,000
So we've learned quite a lot in this section of the course.

3
00:00:06,000 --> 00:00:08,520
Let's summarize what we've talked about.

4
00:00:08,550 --> 00:00:08,960
All right.

5
00:00:08,970 --> 00:00:10,050
So here we go.

6
00:00:10,050 --> 00:00:16,140
We started with an input image to which we applied multiple different feature detectors or also called

7
00:00:16,140 --> 00:00:19,080
filters to create these feature maps.

8
00:00:19,080 --> 00:00:21,420
And this comprises our convolution layer.

9
00:00:21,540 --> 00:00:28,920
Then on top of that crucial layer we applied the red blue or rectified linear unit to remove any linearity

10
00:00:28,920 --> 00:00:31,950
or increase non linearity in our images.

11
00:00:31,980 --> 00:00:36,900
Then we applied pooling layer to our can relational there.

12
00:00:36,930 --> 00:00:42,810
So from every single feature map we created a pooled feature map.

13
00:00:42,810 --> 00:00:45,810
And basically the pooling layer has lots of advantages.

14
00:00:45,810 --> 00:00:53,760
The main purpose of the pulling layer is to make sure that we have a special special in variance in

15
00:00:53,760 --> 00:00:54,630
our images.

16
00:00:54,630 --> 00:01:01,830
So basically if something tilts or twists or is a bit different to the ideal scenario then we can still

17
00:01:01,830 --> 00:01:07,220
pick up that feature plus pooling significantly reduces the size of our images.

18
00:01:07,230 --> 00:01:15,330
Also pooling helps with avoiding any kind of overfishing of our data or of our model to the data because

19
00:01:15,330 --> 00:01:18,250
it just simply gets rid of a lot of that data.

20
00:01:18,390 --> 00:01:23,040
But at the same time pooling preserves the main features that we were after.

21
00:01:23,040 --> 00:01:28,590
Just because the way instruction and the pulling we used was Max pooling then we flattened all of the

22
00:01:28,590 --> 00:01:37,200
pooled images into one along a vector or column of all of these values and we input that into an artificial

23
00:01:37,200 --> 00:01:38,240
neural network.

24
00:01:38,280 --> 00:01:44,430
And that was step three flattening and step four is the fully connected artificial neural network where

25
00:01:44,940 --> 00:01:50,850
all of these features are processed through a network and then we have this final layer final fully

26
00:01:50,850 --> 00:01:57,480
connected layer which performs of voting towards the classes that we're after and then all of this is

27
00:01:57,480 --> 00:02:04,440
trained through a forward propagation and back propagation process and lots of lots of iterations and

28
00:02:04,440 --> 00:02:09,520
epochs and in the end we have a very well defined neural network.

29
00:02:09,690 --> 00:02:14,010
And the important another important thing is not only the weights are trained in artificial neural network

30
00:02:14,010 --> 00:02:22,050
part but also the feature detectors are trained and adjusted in that same gradient descent process and

31
00:02:22,050 --> 00:02:26,810
that allows us to come up with the best feature maps and in the end we get a fully trained evolutionary

32
00:02:26,910 --> 00:02:31,290
neural network which can recognize images and classify them.

33
00:02:31,710 --> 00:02:32,300
So there we go.

34
00:02:32,310 --> 00:02:35,600
That's how can the volitional neural networks work.

35
00:02:35,700 --> 00:02:42,000
And now you should be totally comfortable with this concept and ready to proceed to the practical applications

36
00:02:42,270 --> 00:02:44,440
if you'd like to do some additional reading.

37
00:02:44,550 --> 00:02:52,680
Then there's a great blog by alluded to Deshpande there from 2016 you can see the link over there at

38
00:02:52,680 --> 00:02:57,420
the bottom so the blog is called The Nine Deep Learning papers you need to know about understanding

39
00:02:57,420 --> 00:03:04,080
CNN's Part 3 and this blog actually gives you a short overview of nine different CNN ads that have been

40
00:03:04,080 --> 00:03:10,530
created by people like young kun and others which you can then go ahead and study further.

41
00:03:10,530 --> 00:03:17,940
So there will be a lot of new things that will be totally new to you and that you will have to get your

42
00:03:17,940 --> 00:03:23,820
head around but just keep this blog in mind or these nine papers in mind and even if you're not ready

43
00:03:23,820 --> 00:03:29,100
to go through them right now maybe after the Practical Tutorials maybe after you do some additional

44
00:03:29,430 --> 00:03:36,120
training in the space of deep learning slowly you can then reference these works and ideally I think

45
00:03:36,120 --> 00:03:41,190
you will get a lot of value through looking through other people's neural networks and how they structured

46
00:03:41,480 --> 00:03:46,560
their can relational nets and that will help you understand what are the best practices and why people

47
00:03:46,560 --> 00:03:48,050
did certain things in a certain way.

48
00:03:48,390 --> 00:03:53,520
And that will help you with your architecture of neural networks because neural networks and kind of

49
00:03:53,520 --> 00:03:57,930
delusional neural networks are not an exception.

50
00:03:57,930 --> 00:04:05,610
They are like an architecture challenge you have to come up with a idea and then structure it and then

51
00:04:05,610 --> 00:04:11,690
adjusted and tweak it to get the best possible design and the best possible and optimal performance.

52
00:04:11,730 --> 00:04:12,420
So there we go.

53
00:04:12,420 --> 00:04:13,380
That's us for today.

54
00:04:13,380 --> 00:04:17,670
I hope you enjoyed today's tutorial and this whole section and I look forward to see you next time.

55
00:04:17,670 --> 00:04:19,410
Until then enjoy deep learning.
