WEBVTT
1

00:00:05.490 --> 00:00:08.010

Welcome everyone for the solution and for design of experiments.



2

00:00:08.010 --> 00:00:10.280

Question number three.



3

00:00:10.400 --> 00:00:14.820

So for this question Facebook was testing different designs of the user home page and they've come up



4

00:00:14.820 --> 00:00:18.060

with those 50 variations and they like to test all of them and choose the best.



5

00:00:18.060 --> 00:00:19.680

So how do you set this experiment.



6

00:00:19.680 --> 00:00:22.890

What metrics would you calculate and how would you report the results.



7

00:00:22.890 --> 00:00:26.220

This is different than some of the other questions we've seen because this is not just the simple A-B



8

00:00:26.220 --> 00:00:26.850

test.



9

00:00:26.850 --> 00:00:29.280

We're actually now dealing with 50 variations of something.



10

00:00:29.340 --> 00:00:35.070

So the approach is going to be a little different so I should just mention slightly different than AB



11

00:00:35.070 --> 00:00:39.320

test and there's actually multiple statistical techniques that exist for this sort of problem.



12

00:00:39.360 --> 00:00:41.100

So let's go ahead and discuss some of them.



13

00:00:42.820 --> 00:00:49.320

So there is something known as a t test among pairs of treatment and this is similar to an AB test except



14

00:00:49.330 --> 00:00:54.160

we end up doing is we randomly assigned all 50 versions of this page to users.



15

00:00:54.310 --> 00:00:58.870

And there are a few things to consider with this approach because we're doing a t test among multiple



16

00:00:58.870 --> 00:01:05.070

pairs of treatments not just a single pair for one thing when you're doing this T-test among pairs of



17

00:01:05.070 --> 00:01:09.840

treatment with so many versions you're going to need to make sure you have enough users to get statistically



18

00:01:09.840 --> 00:01:11.350

significant results.



19

00:01:11.370 --> 00:01:16.490

Typically this is actually a really big problem because as you'll see in just a little bit the P-value



20

00:01:16.500 --> 00:01:23.160

you have to use for a T-test among pairs of treatment ends up becoming really small as far as what value



21

00:01:23.190 --> 00:01:29.340

you need to actually get past to achieve statistical significance for really large companies however



22

00:01:29.340 --> 00:01:31.120

with really large user bases.



23

00:01:31.260 --> 00:01:33.750

We're talking things like Facebook and Gmail.



24

00:01:33.750 --> 00:01:38.750

That's actually not an issue which is really convenient because they can run experiments all the time



25

00:01:38.760 --> 00:01:43.820

and in fact Facebook is running thousands of experiments on any time because they have such a huge population



26

00:01:43.820 --> 00:01:49.090

a sample from they can get statistical specifically significant results a lot quicker than other people.



27

00:01:50.810 --> 00:01:54.590

So let's talk about that for a little bit with multiple hypothesis tests.



28

00:01:54.590 --> 00:01:58.820

We end up doing is you increase the likelihood of a rare event occurring.



29

00:01:58.820 --> 00:02:04.290

Meaning you need to adjust your alpha value used to compare your p value against significance accordingly.



30

00:02:04.490 --> 00:02:08.330

So as I previously mentioned you're going to have to adjust that Alpha value which is the one you're



31

00:02:08.330 --> 00:02:11.710

testing your P-value against.



32

00:02:11.710 --> 00:02:16.300

Now this is a multiple an inference problem where you can do in order to correct your alpha value you



33

00:02:16.300 --> 00:02:18.610

can use the Bonferroni correction method.



34

00:02:18.610 --> 00:02:20.800

So let's go ahead and talk about that how that works.



35

00:02:21.740 --> 00:02:25.360

So typically what we end up doing is we say Alpha to zero point zero or five.



36

00:02:25.380 --> 00:02:29.910

And if you ever done any sort of reading on T-test or hypothesis testing that value is going to be really



37

00:02:29.910 --> 00:02:34.980

familiar to you but with the Bonferroni correction we divide this by the number of tests we're actually



38

00:02:34.980 --> 00:02:35.910

performing.



39

00:02:35.910 --> 00:02:40.590

So in our case if we're doing 50 different versions of this test so we're doing a t test among pairs



40

00:02:40.610 --> 00:02:44.940

and 50 of these pairs we're gonna end up doing 0.05 divided by 50.



41

00:02:45.150 --> 00:02:47.580

So that's going to be equal to 0.00 one.



42

00:02:47.790 --> 00:02:52.410

So now that becomes the alpha value that we're going to test or P-value against.



43

00:02:52.410 --> 00:02:55.180

And clearly we're going to need a much larger population for testing.



44

00:02:55.230 --> 00:02:59.370

Otherwise we're just never going to achieve statistical significance for anything with such a small



45

00:02:59.370 --> 00:03:00.140

Alpha value.



46

00:03:00.140 --> 00:03:05.040

So this is why this sort of experimentation really only works when you have very large populations.



47

00:03:05.070 --> 00:03:10.260

And luckily more and more companies are able to have these huge user bases to test this against.



