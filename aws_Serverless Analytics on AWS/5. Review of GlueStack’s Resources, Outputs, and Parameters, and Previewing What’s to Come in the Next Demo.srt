1
00:00:01,359 --> 00:00:04,145
In this section, we're going to review the resources,

2
00:00:04,145 --> 00:00:07,225
outputs, and parameters from the GlueStack,

3
00:00:07,225 --> 00:00:12,257
and look at some of the things we're going to do in the subsequent demo.

4
00:00:12,257 --> 00:00:18,061
You saw this screen in the demo, this is the resources tab of the GlueStack.

5
00:00:18,061 --> 00:00:19,884
And you saw this in the demo as well,

6
00:00:19,884 --> 00:00:24,511
where it shows all of the outputs created by the GlueStack.

7
00:00:24,511 --> 00:00:31,444
And lastly, you saw this in the demo as well, showing all the parameters.

8
00:00:31,444 --> 00:00:35,968
The top-left screenshot shows the AWS Glue interface for databases,

9
00:00:35,968 --> 00:00:40,885
and you can see our sakila database is there.

10
00:00:40,885 --> 00:00:43,017
Moving to the right screenshot on the top,

11
00:00:43,017 --> 00:00:46,537
we're looking at the sakila database,

12
00:00:46,537 --> 00:00:50,790
and surrounded in purple at the bottom is a link to view

13
00:00:50,790 --> 00:00:54,136
the tables just for the sakila database.

14
00:00:54,136 --> 00:00:58,181
Looking at the lower screenshot, when you click on that link to view the tables,

15
00:00:58,181 --> 00:01:03,599
the lower-right blue rectangle surrounds the words that reads you

16
00:01:03,599 --> 00:01:07,219
don't have any tables defined in your data catalog,

17
00:01:07,219 --> 00:01:11,712
add tables using a crawler, and that's just what we'll do.

18
00:01:11,712 --> 00:01:16,730
The screenshots on this slide show the crawler details for both

19
00:01:16,730 --> 00:01:19,017
crawlers that we're going to run in this module.

20
00:01:19,017 --> 00:01:24,376
The primary differences are that the first crawler will scrape the Amazon

21
00:01:24,376 --> 00:01:29,957
Aurora database and populate the sakiladb database with tables,

22
00:01:29,957 --> 00:01:33,916
using a prefix of glue_ for every table,

23
00:01:33,916 --> 00:01:36,639
whereas the second crawler will run,

24
00:01:36,639 --> 00:01:42,752
will crawl the S3 destination bucket that holds the contents of the ETL job,

25
00:01:42,752 --> 00:01:49,943
where the data is transformed into Parquet format and has a prefix of S3_.

26
00:01:49,943 --> 00:01:53,640
The first crawler's data store is a JDBC data store.

27
00:01:53,640 --> 00:01:57,238
The second crawler's data store is Amazon S3.

28
00:01:57,238 --> 00:02:01,126
The first crawler's Include path is sakila/%,

29
00:02:01,126 --> 00:02:06,809
and it will crawl everything under the Amazon Aurora sakila database,

30
00:02:06,809 --> 00:02:10,497
whereas the second crawler's Include path is only going to

31
00:02:10,497 --> 00:02:15,232
crawl one bucket underneath Amazon S3.

32
00:02:15,232 --> 00:02:21,257
On this screen is a screenshot of the AWS Glue job,

33
00:02:21,257 --> 00:02:25,432
GlueStack-aurora-etljob, that we'll be using the in the next demo.

34
00:02:25,432 --> 00:02:32,057
Notice in the Glue Job interface the lower blue rectangle surrounds four tabs.

35
00:02:32,057 --> 00:02:34,405
The first one is the History tab,

36
00:02:34,405 --> 00:02:40,329
where it keeps track of all of the runs that the job has done.

37
00:02:40,329 --> 00:02:43,848
The next tab is the Details tab where you can drill

38
00:02:43,848 --> 00:02:46,858
down into the configuration details.

39
00:02:46,858 --> 00:02:53,058
The next tab is the Script tab, where you can view and edit the ETL script.

40
00:02:53,058 --> 00:02:56,657
And the last tab is the Metrics tab,

41
00:02:56,657 --> 00:03:01,834
where you can look at all of the metrics for that particular job.

42
00:03:01,834 --> 00:03:10,000
The next section is a demo section, where we're going to run our Glue crawler and run the Glue ETL job.

