1
00:00:05,235 --> 00:00:10,248
There are a few best practices to optimize your data for analysis.

2
00:00:10,248 --> 00:00:14,532
Let's take a very short look at those now.

3
00:00:14,532 --> 00:00:19,776
It's important to ingest data in its raw format before any transformations,

4
00:00:19,776 --> 00:00:24,835
analyses, manipulations, or doing anything with it at all.

5
00:00:24,835 --> 00:00:26,628
Once you have the raw data in S3,

6
00:00:26,628 --> 00:00:30,211
you have to prepare the raw data for consumption.

7
00:00:30,211 --> 00:00:31,584
When you do this,

8
00:00:31,584 --> 00:00:35,680
the raw datasets in S3 become your immutable source of record

9
00:00:35,680 --> 00:00:38,741
that you can always go back to if needed.

10
00:00:38,741 --> 00:00:40,254
To use raw data,

11
00:00:40,254 --> 00:00:45,001
you abstract out the complexities of how the data is stored through

12
00:00:45,001 --> 00:00:50,080
the Glue Data Catalog and Athena SerDes are used to serialize and

13
00:00:50,080 --> 00:00:55,952
desterilize the data for analyzing the data with Amazon Athena and

14
00:00:55,952 --> 00:00:57,723
other analytics services.

15
00:00:57,723 --> 00:01:00,897
In S3 data lakes, the datasets are curated,

16
00:01:00,897 --> 00:01:05,209
meaning they're query optimized for consumption across a wide

17
00:01:05,209 --> 00:01:08,510
range of analytics and machine-learning tools.

18
00:01:08,510 --> 00:01:12,400
Optimizing the data means that the data is normalized,

19
00:01:12,400 --> 00:01:17,859
partitioned, compressed, and optimized for storage.

20
00:01:17,859 --> 00:01:22,628
Due to time constraints, I won't be able to dive into each of these individually,

21
00:01:22,628 --> 00:01:25,917
but they're very important, and I encourage you to do so.

22
00:01:25,917 --> 00:01:32,484
I'll just point out a couple of them in so far as cost savings are concerned.

23
00:01:32,484 --> 00:01:38,066
Before I show you the cost savings that can be had when using best practices

24
00:01:38,066 --> 00:01:42,228
of preparing your raw data for downstream consumption,

25
00:01:42,228 --> 00:01:47,439
you need to know that Amazon Athena and many,

26
00:01:47,439 --> 00:01:53,610
many other advanced analytics work most efficiently on

27
00:01:53,610 --> 00:01:58,205
columnar data formats versus row format.

28
00:01:58,205 --> 00:02:06,977
Columnar data formats most often used are Apache Parquet or Apache ORC.

29
00:02:06,977 --> 00:02:13,749
Parquet is very good at compressing data and is used heavily in

30
00:02:13,749 --> 00:02:18,574
large-scale distributed Spark deployments on big data.

31
00:02:18,574 --> 00:02:21,678
Columnar storage formats have the following characteristics

32
00:02:21,678 --> 00:02:26,151
that make them suitable for using with Athena.

33
00:02:26,151 --> 00:02:28,921
It has compression by column,

34
00:02:28,921 --> 00:02:33,566
with compression algorithms selected for the column data

35
00:02:33,566 --> 00:02:38,840
type to save storage space in Amazon S3 and reduce disk

36
00:02:38,840 --> 00:02:43,403
space in I/O during query processing.

37
00:02:43,403 --> 00:02:46,910
Parquet and ORC have predicate pushdown,

38
00:02:46,910 --> 00:02:52,696
which enables Athena queries to fetch only the blocks it needs,

39
00:02:52,696 --> 00:02:54,564
improving query performance.

40
00:02:54,564 --> 00:03:00,026
When an Athena query obtains specific column values from your data,

41
00:03:00,026 --> 00:03:06,102
it uses statistics from data block predicates such as min/max values and

42
00:03:06,102 --> 00:03:11,830
so on to determine whether to read or skip that block.

43
00:03:11,830 --> 00:03:18,048
Parquet and ORC formats allow Athena to split the reading of data to multiple

44
00:03:18,048 --> 00:03:23,470
readers and increase parallelism during its query processing.

45
00:03:23,470 --> 00:03:29,810
Notice on this slide that by using Parquet format versus CSV format,

46
00:03:29,810 --> 00:03:34,718
there's an 87% reduction in size on S3.

47
00:03:34,718 --> 00:03:38,730
The speed of querying is 34 times faster.

48
00:03:38,730 --> 00:03:44,981
That gives 99% less data to scan, and price wise,

49
00:03:44,981 --> 00:03:51,044
you spend 99.7 % less by simply using Parquet format.

50
00:03:51,044 --> 00:03:53,562
These are significant improvements.

51
00:03:53,562 --> 00:03:57,958
Glue ETL supports partitions when working with

52
00:03:57,958 --> 00:04:00,981
DynamicFrames or semi-structured data.

53
00:04:00,981 --> 00:04:05,494
DynamicFrames represent a distributed collection of data

54
00:04:05,494 --> 00:04:08,367
without requiring you to specify a schema.

55
00:04:08,367 --> 00:04:14,428
Instead of reading the entire dataset and then filtering in a DynamicFrame,

56
00:04:14,428 --> 00:04:20,788
you can apply a filter to prune unnecessary Amazon S3

57
00:04:20,788 --> 00:04:25,245
partitions in Parquet and ORC formats directly on the

58
00:04:25,245 --> 00:04:30,795
partition metadata in the data catalog, reducing processing time.

59
00:04:30,795 --> 00:04:34,231
Crawlers not only infer file types and schemas,

60
00:04:34,231 --> 00:04:39,768
they also automatically identify the partition structure of your

61
00:04:39,768 --> 00:04:43,596
dataset when they populate the AWS Glue Data Catalog.

62
00:04:43,596 --> 00:04:49,330
The resulting partition columns are available for querying in AWS

63
00:04:49,330 --> 00:04:55,025
Glue ETL jobs or query engines like Amazon Athena.

64
00:04:55,025 --> 00:04:59,535
By default, a DynamicFrame is not partitioned when it's written.

65
00:04:59,535 --> 00:05:02,746
All of the output files are written at the top

66
00:05:02,746 --> 00:05:05,624
level of the specified output path.

67
00:05:05,624 --> 00:05:06,244
Until recently,

68
00:05:06,244 --> 00:05:10,505
the only way to write a DynamicFrame into partitions was to

69
00:05:10,505 --> 00:05:15,502
convert it to a Spark SQL data frame before writing.

70
00:05:15,502 --> 00:05:19,335
DynamicFrames now support native partitioning using a

71
00:05:19,335 --> 00:05:23,459
sequence of keys and using the partition keys option when

72
00:05:23,459 --> 00:05:27,342
you write to other destinations.

73
00:05:27,342 --> 00:05:31,007
Source data sometimes is already fully or partially

74
00:05:31,007 --> 00:05:34,756
compressed to reduce storage and disk I/O,

75
00:05:34,756 --> 00:05:40,471
thereby improving query performance, as well as taking less space to store.

76
00:05:40,471 --> 00:05:46,037
The table on this slide conceptually illustrates the differences between

77
00:05:46,037 --> 00:05:54,266
data stored in an uncompressed CSV format versus simply GZIP-ing the CSV

78
00:05:54,266 --> 00:05:58,081
file versus storing the data in Parquet format.

79
00:05:58,081 --> 00:06:02,414
The size on S3 is large for plain CSV format,

80
00:06:02,414 --> 00:06:07,763
but for both the GZIP CSV and the Parquet formats have

81
00:06:07,763 --> 00:06:10,437
reduced this size by one-quarter.

82
00:06:10,437 --> 00:06:14,696
However, looking at the amount of data that has to be scanned,

83
00:06:14,696 --> 00:06:20,821
GZIP CSV files only need one-quarter of the data

84
00:06:20,821 --> 00:06:25,632
scanned versus plain CSV format, and when the data is stored in Parquet format,

85
00:06:25,632 --> 00:06:30,785
that's one-quarter of the data that needs to be

86
00:06:30,785 --> 00:06:33,193
scanned then for GZIP CSV format.

87
00:06:33,193 --> 00:06:37,467
The reason for this is that Parquet format is columnar,

88
00:06:37,467 --> 00:06:45,261
and as you can see, your costs are reduced as well as the size.

89
00:06:45,261 --> 00:06:49,608
Glue storage optimization is achieved via S3 lifecycle policies,

90
00:06:49,608 --> 00:06:54,356
where the data automatically transfers to a different storage

91
00:06:54,356 --> 00:06:58,719
class without any changes to your apps.

92
00:06:58,719 --> 00:07:04,166
In the Functionality column, the information represents frequency of access,

93
00:07:04,166 --> 00:07:10,817
retrieval time, availability zones, and price at the time of making this slide.

94
00:07:10,817 --> 00:07:13,846
If there's a plus sign after the cost,

95
00:07:13,846 --> 00:07:15,226
that indicates that there are some stipulations,

96
00:07:15,226 --> 00:07:29,000
so you'd need to look up what those are. In the next section, I'll cover versioning in the Glue Data Catalog.

