1
00:00:01,386 --> 00:00:09,378
In this section, I'll explain exactly what AWS Glue is and how it works.

2
00:00:09,378 --> 00:00:11,351
So what is AWS Glue?

3
00:00:11,351 --> 00:00:15,129
The emergence of unstructured and semi-structured data produced

4
00:00:15,129 --> 00:00:20,531
new problems of performing ETL on non-relational data that's

5
00:00:20,531 --> 00:00:23,116
necessary to do prior to analysis.

6
00:00:23,116 --> 00:00:27,845
Glue is a fully-managed ETL service that solves that business problem.

7
00:00:27,845 --> 00:00:33,915
As a side note, I'm sure many of you have heard of not ETL, but ELT.

8
00:00:33,915 --> 00:00:39,674
That's the way data is transformed today using a service like AWS Glue.

9
00:00:39,674 --> 00:00:41,945
Today, we don't extract, transform, and load,

10
00:00:41,945 --> 00:00:48,342
but in an S3 data lake, you keep raw data so you can go back to it at any time,

11
00:00:48,342 --> 00:00:52,112
thus what's happening now is extract, load,

12
00:00:52,112 --> 00:00:54,322
and then transform.

13
00:00:54,322 --> 00:00:55,702
Looking at this diagram,

14
00:00:55,702 --> 00:01:00,284
you can see some of the many different types of data

15
00:01:00,284 --> 00:01:01,701
sources we're working with today.

16
00:01:01,701 --> 00:01:03,637
With Glue's ETL on the fly,

17
00:01:03,637 --> 00:01:08,987
you now can load the raw data regardless of the type or where the physical

18
00:01:08,987 --> 00:01:14,782
location of the underlying data store is into Amazon S3.

19
00:01:14,782 --> 00:01:18,659
You connect to your various heterogenous data stores,

20
00:01:18,659 --> 00:01:22,555
you can run Glue ETL at any time in this workflow,

21
00:01:22,555 --> 00:01:27,387
which in this diagram is after data arrives in S3,

22
00:01:27,387 --> 00:01:30,862
but it can be performed at any point in this workflow.

23
00:01:30,862 --> 00:01:34,069
Glue crawlers crawl all your data stores,

24
00:01:34,069 --> 00:01:35,210
determines the schema,

25
00:01:35,210 --> 00:01:40,431
then populates the Glue Data Catalog with that schema and

26
00:01:40,431 --> 00:01:44,156
other metadata necessary to reconstruct the database when

27
00:01:44,156 --> 00:01:47,588
it's ready to be used for analytics.

28
00:01:47,588 --> 00:01:51,416
Most often, after you have the results of your data analysis,

29
00:01:51,416 --> 00:01:54,641
it's common to visualize the information.

30
00:01:54,641 --> 00:01:58,258
In this diagram, the visualization is done with Amazon Quicksight,

31
00:01:58,258 --> 00:02:03,595
but you can use any visualization tool you like.

32
00:02:03,595 --> 00:02:05,993
Let's now look at how Glue works.

33
00:02:05,993 --> 00:02:11,352
To crawl the data source, you simply point a Glue crawler at the source data,

34
00:02:11,352 --> 00:02:15,547
and the crawlers populate your Glue Data Catalog with enough

35
00:02:15,547 --> 00:02:19,831
metadata and statistics to reconstruct the source data when you

36
00:02:19,831 --> 00:02:23,656
request that data to be used in analytics.

37
00:02:23,656 --> 00:02:30,761
Glue jobs perform the ETL on the raw data in an Amazon S3 data lake.

38
00:02:30,761 --> 00:02:36,014
During the ETL process, many different types of transformations can be done.

39
00:02:36,014 --> 00:02:38,210
Once the ETL is complete,

40
00:02:38,210 --> 00:02:42,839
the Glue job puts the transform data into wherever you specify,

41
00:02:42,839 --> 00:02:45,775
which is called a target data source.

42
00:02:45,775 --> 00:02:48,514
You can run jobs on demand,

43
00:02:48,514 --> 00:02:52,581
or you can set up triggers to automatically start a job

44
00:02:52,581 --> 00:02:54,694
whenever certain criteria are met.

45
00:02:54,694 --> 00:02:58,543
Triggers can be time based or event based.

46
00:02:58,543 --> 00:03:00,126
When your job runs,

47
00:03:00,126 --> 00:03:04,921
an automated script written in either Python or Scala

48
00:03:04,921 --> 00:03:07,547
extracts the data from your data source,

49
00:03:07,547 --> 00:03:14,197
transforms it, and loads the transformed data into the data target.

50
00:03:14,197 --> 00:03:17,870
AWS Glue solves the business problems that come with the

51
00:03:17,870 --> 00:03:20,380
need for analyzing heterogenous data types,

52
00:03:20,380 --> 00:03:25,714
which in the past was laborious and very time consuming.

53
00:03:25,714 --> 00:03:30,744
Glue provides one centralized location for literally all your company data,

54
00:03:30,744 --> 00:03:37,252
thereby solving the common business problem of globally scattered data silos.

55
00:03:37,252 --> 00:03:43,036
The AWS Glue Data Catalog is a central repository to store structural

56
00:03:43,036 --> 00:03:47,255
and operational metadata for all your data assets,

57
00:03:47,255 --> 00:03:48,922
even from on premises.

58
00:03:48,922 --> 00:03:54,237
The AWS Glue crawlers connect to data stores to extract the

59
00:03:54,237 --> 00:03:57,302
schema and other statistics about the data.

60
00:03:57,302 --> 00:04:02,092
Then it populates the Glue Data Catalog with this metadata.

61
00:04:02,092 --> 00:04:03,990
Glue eliminates dark data.

62
00:04:03,990 --> 00:04:08,104
Dark data is information assets organizations collect,

63
00:04:08,104 --> 00:04:11,212
process, and store during regular business activities,

64
00:04:11,212 --> 00:04:15,499
but generally failed to use it for other purposes.

65
00:04:15,499 --> 00:04:21,194
It's not used in any manner to drive insights or for decision making,

66
00:04:21,194 --> 00:04:25,323
but hiding in those modes of dark data might be pivotal

67
00:04:25,323 --> 00:04:28,385
to a solution today or in the future.

68
00:04:28,385 --> 00:04:31,130
Glue automatically generates the ETL code necessary

69
00:04:31,130 --> 00:04:36,441
to extract data from the source, transform the data to match the target schema,

70
00:04:36,441 --> 00:04:41,961
and load it into the target by pointing to your data source and target,

71
00:04:41,961 --> 00:04:45,573
making it easy to prepare and load data for analytics.

72
00:04:45,573 --> 00:04:48,014
There are many other types of transformations that

73
00:04:48,014 --> 00:04:51,167
can be done during the ETL process.

74
00:04:51,167 --> 00:04:54,615
Glue has a flexible job scheduler.

75
00:04:54,615 --> 00:04:57,102
Glue jobs perform the ETL processing.

76
00:04:57,102 --> 00:05:02,822
They can be invoked on a schedule, on demand, or based on an event.

77
00:05:02,822 --> 00:05:07,494
You can start multiple jobs in parallel or specify dependencies

78
00:05:07,494 --> 00:05:12,320
across jobs to build complex ETL pipelines.

79
00:05:12,320 --> 00:05:15,560
Glue handles all inter-job dependencies,

80
00:05:15,560 --> 00:05:19,611
filters out bad data, and retries jobs if they fail.

81
00:05:19,611 --> 00:05:22,477
Glue has monitoring built in,

82
00:05:22,477 --> 00:05:27,197
all logs and notifications are pushed to Amazon CloudWatch,

83
00:05:27,197 --> 00:05:34,336
so you can monitor and get alerts from one central service.

84
00:05:34,336 --> 00:05:36,736
So what's so cool about AWS Glue?

85
00:05:36,736 --> 00:05:40,454
The title on this slide gives you one primary advantage,

86
00:05:40,454 --> 00:05:45,801
it allows different people to work on the same data at the same time,

87
00:05:45,801 --> 00:05:49,129
but using different tools of their choice for

88
00:05:49,129 --> 00:05:51,747
analytics to solve different problems,

89
00:05:51,747 --> 00:05:54,063
and since you're working on the metadata for the

90
00:05:54,063 --> 00:05:56,260
database in the Glue Data Catalog,

91
00:05:56,260 --> 00:06:00,557
the underlying source database isn't touched at all.

92
00:06:00,557 --> 00:06:04,605
One of AWS Glue's core values is not only having

93
00:06:04,605 --> 00:06:07,259
all data in one central location,

94
00:06:07,259 --> 00:06:11,777
but that there's native integration with all AWS' and

95
00:06:11,777 --> 00:06:15,081
third-party business intelligence tools.

96
00:06:15,081 --> 00:06:19,198
There's native integration with all of AWS' and

97
00:06:19,198 --> 00:06:21,690
third-party artificial intelligence, machine-learning,

98
00:06:21,690 --> 00:06:23,968
and deep-learning tools.

99
00:06:23,968 --> 00:06:28,996
This benefits the company as a whole because anyone with any role

100
00:06:28,996 --> 00:06:32,442
that works with company data can use their favorite business

101
00:06:32,442 --> 00:06:38,146
intelligence and/or advanced analytics tools on the same data in an

102
00:06:38,146 --> 00:06:41,638
Amazon S3 data lake simultaneously.

103
00:06:41,638 --> 00:06:52,000
In the next section, I'll go over what Amazon Athena is, what it does, and the business problems it solves.

