WEBVTT
1
00:00:01.248 --> 00:00:04.776
What we're going to be doing in this demo includes

2
00:00:04.776 --> 00:00:08.135
creating a new AWS Glue crawler,

3
00:00:08.135 --> 00:00:13.668
run the crawler on the process data in the S3 processed zone,

4
00:00:13.668 --> 00:00:21.832
review the table results in AWS Glue database sakiladb in the Glue Data Catalog,

5
00:00:21.832 --> 00:00:25.541
confirm that the two tables were joined,

6
00:00:25.541 --> 00:00:32.601
confirmed the sakiladb data catalog entry has a new Parquet-formatted table,

7
00:00:32.601 --> 00:00:37.884
I'll introduce you to the Amazon Athena query editor interface,

8
00:00:37.884 --> 00:00:40.598
and we'll run some queries on the Parquet-formatted

9
00:00:40.598 --> 00:00:43.643
data in the Glue Data Catalog.

10
00:00:43.643 --> 00:00:45.798
Let's start this demo now.

11
00:00:45.798 --> 00:00:52.220
Navigate to the Crawlers link in the left AWS Glue Console's menu,

12
00:00:52.220 --> 00:00:54.567
and then click on Add crawler.

13
00:00:54.567 --> 00:00:56.659
Give your crawler a name,

14
00:00:56.659 --> 00:01:06.382
I named mine S3-film-cat-crawler2 because I already had S3-film-cat-crawler,

15
00:01:06.382 --> 00:01:07.758
but call yours whatever you'd like.

16
00:01:07.758 --> 00:01:14.005
In the specified Crawler source type, choose data stores, and then choose Next.

17
00:01:14.005 --> 00:01:18.773
From the drop-down list, choose S3,

18
00:01:18.773 --> 00:01:24.532
and set the radio button to crawl data in a specified path in my account,

19
00:01:24.532 --> 00:01:27.262
and then click on the little folder icon.

20
00:01:27.262 --> 00:01:31.981
This brings up all your S3 buckets that you have in your account.

21
00:01:31.981 --> 00:01:38.709
Navigate to the bucket name that you specified for the AWS Glue job

22
00:01:38.709 --> 00:01:43.272
to put the results of the ETL transformations in.

23
00:01:43.272 --> 00:01:49.817
For me, it's uniquebucketnamedestination, and select it, and then click Next.

24
00:01:49.817 --> 00:01:53.790
This is where it's asking if you want to add another data store because

25
00:01:53.790 --> 00:01:58.489
one crawler can crawl many data stores simultaneously,

26
00:01:58.489 --> 00:02:01.882
but we're just going to say no because the chosen data

27
00:02:01.882 --> 00:02:06.765
store is one bucket in Amazon S3, and choose Next.

28
00:02:06.765 --> 00:02:13.466
In the Choose an IAM role, click the option to Choose an existing IAM role,

29
00:02:13.466 --> 00:02:19.281
and click on the drop-down, and you will see GlueStack-AWSGlueRole as an option.

30
00:02:19.281 --> 00:02:22.427
So click that, and then click Next.

31
00:02:22.427 --> 00:02:25.142
This is where you can create a schedule for the crawler,

32
00:02:25.142 --> 00:02:28.619
and we'll get into that when we do automation,

33
00:02:28.619 --> 00:02:31.815
but remember that as you create a new crawler,

34
00:02:31.815 --> 00:02:36.602
you can do that right off the bat, but I'm going to leave it at Run on demand,

35
00:02:36.602 --> 00:02:37.286
and click Next.

36
00:02:37.286 --> 00:02:41.245
In the next page entitled Configure the crawler's

37
00:02:41.245 --> 00:02:45.367
output for the Database option, We want to choose sakiladb,

38
00:02:45.367 --> 00:02:51.467
and for a Prefix, we want that to be S3_.

39
00:02:51.467 --> 00:02:54.995
Review the final page, and then click Finish.

40
00:02:54.995 --> 00:02:57.818
So you can see the crawler is now running,

41
00:02:57.818 --> 00:03:02.067
and currently it shows a Status of Starting.

42
00:03:02.067 --> 00:03:06.428
Keep hitting the Refresh button, and now it has a status of Stopping.

43
00:03:06.428 --> 00:03:09.022
Now the Status is Ready.

44
00:03:09.022 --> 00:03:11.992
In the sakiladb tables, scroll down to the bottom,

45
00:03:11.992 --> 00:03:18.874
and we have a new entry that has the Classification of Parquet.

46
00:03:18.874 --> 00:03:22.930
So the Glue job transformation that took the MySQL data format and

47
00:03:22.930 --> 00:03:27.291
turned it into Parquet format was successful.

48
00:03:27.291 --> 00:03:31.230
Let's click on the bucket name and scroll down.

49
00:03:31.230 --> 00:03:34.778
The join transformation was successful as well.

50
00:03:34.778 --> 00:03:37.475
Here we have the two timestamp columns,

51
00:03:37.475 --> 00:03:40.530
one from the film table and one from the film_category table,

52
00:03:40.530 --> 00:03:43.218
and we have the category_id column,

53
00:03:43.218 --> 00:03:48.338
which is what we wanted to get by doing the join of the

54
00:03:48.338 --> 00:03:50.994
film table and the film_category table.

55
00:03:50.994 --> 00:03:54.043
So both transformations were successful.

56
00:03:54.043 --> 00:03:56.077
Let's view the Properties.

57
00:03:56.077 --> 00:04:00.378
Here are all the columns, here's the location,

58
00:04:00.378 --> 00:04:07.251
the input format, the output format, we didn't use any kind of compression,

59
00:04:07.251 --> 00:04:14.047
and it chose a default SerDes for Parquet format.

60
00:04:14.047 --> 00:04:18.620
It gives you more information about the parameters,

61
00:04:18.620 --> 00:04:21.205
including the classification of Parquet,

62
00:04:21.205 --> 00:04:26.356
and in contrast to the last time we looked at the properties of a

63
00:04:26.356 --> 00:04:31.088
table that came from the Amazon Aurora database,

64
00:04:31.088 --> 00:04:33.602
the type of data is not table, it's file.

65
00:04:33.602 --> 00:04:35.674
And let's close this.

66
00:04:35.674 --> 00:04:38.793
Let's now go up to Services and start typing Athena,

67
00:04:38.793 --> 00:04:43.349
and when it shows up, click on it, and you might have to click Refresh,

68
00:04:43.349 --> 00:04:49.165
and make sure that the Database that's selected is the sakila database,

69
00:04:49.165 --> 00:04:54.651
and the Table is the uniquebucketnamedestination or whatever you chose to

70
00:04:54.651 --> 00:05:00.089
be the name of your target destination for the Glue job.

71
00:05:00.089 --> 00:05:05.771
We expand this, you have all of the columns in the table,

72
00:05:05.771 --> 00:05:08.388
and here's category_id and the two last updates.

73
00:05:08.388 --> 00:05:10.571
So let's run a query.

74
00:05:10.571 --> 00:05:18.394
What we're doing is selecting everything from the database and the table,

75
00:05:18.394 --> 00:05:22.216
and we're limiting it to 10 rows just for the example.

76
00:05:22.216 --> 00:05:25.411
So you highlight it and run the query.

77
00:05:25.411 --> 00:05:30.642
And you can see it's running, and here we have data.

78
00:05:30.642 --> 00:05:35.005
ET, phone home because this is a whole new world.

79
00:05:35.005 --> 00:05:39.753
And what you can do, you can do a Create table as,

80
00:05:39.753 --> 00:05:44.267
you can save the query, you can format the query,

81
00:05:44.267 --> 00:05:48.924
and any queries that you save will be under this tab.

82
00:05:48.924 --> 00:05:49.997
There's also the History tab,

83
00:05:49.997 --> 00:05:56.313
and you can go to the AWS Glue Data Catalog by simply clicking on that link,

84
00:05:56.313 --> 00:05:58.513
and here we are.

85
00:05:58.513 --> 00:06:03.662
Going back to the Athena Console, if we click on Create table,

86
00:06:03.662 --> 00:06:08.435
you have the option to create a table from S3 bucket data,

87
00:06:08.435 --> 00:06:11.002
from a Glue crawler,

88
00:06:11.002 --> 00:06:17.418
and also using SQL templates of CREATE TABLE or CREATE TABLE AS SELECT.

89
00:06:17.418 --> 00:06:22.588
Notice if you choose that you want to create a table from a Glue crawler,

90
00:06:22.588 --> 00:06:27.370
you're given this popup that reads that you will go to the

91
00:06:27.370 --> 00:06:32.552
AWS Glue Console to set up a crawler, how easy is that,

92
00:06:32.552 --> 00:06:33.051
right?

93
00:06:33.051 --> 00:06:37.691
It states the crawler connects to your data store and automatically

94
00:06:37.691 --> 00:06:42.449
determines its structure to create the metadata for your table.

95
00:06:42.449 --> 00:06:46.824
So remember this option when you're working with Amazon Athena.

96
00:06:46.824 --> 00:06:53.117
I uploaded a CSV file to an S3 bucket that lists the NBA

97
00:06:53.117 --> 00:06:55.617
players and if they have tattoos or not.

98
00:06:55.617 --> 00:07:00.061
This is the perfect kind of NBA trivia that

99
00:07:00.061 --> 00:07:02.898
Globomantics customers like to read about.

100
00:07:02.898 --> 00:07:03.396
Quite frankly,

101
00:07:03.396 --> 00:07:09.053
I'd like to know what the tattoos are of versus a yes or no answer,

102
00:07:09.053 --> 00:07:10.398
but that's what we have to work with.

103
00:07:10.398 --> 00:07:13.923
So from the Athena Query Editor Console,

104
00:07:13.923 --> 00:07:18.660
I'm going to create a table from S3 bucket data.

105
00:07:18.660 --> 00:07:22.133
So when you choose Create table from S3 bucket data,

106
00:07:22.133 --> 00:07:24.798
in the Add table dialog,

107
00:07:24.798 --> 00:07:31.132
choose the sakila database because everything in the sakiladb data

108
00:07:31.132 --> 00:07:33.883
catalog database has to do with NBA players.

109
00:07:33.883 --> 00:07:37.553
And I gave the table a name of nba_tattoos.

110
00:07:37.553 --> 00:07:43.508
So if you read the text underneath the location of input dataset,

111
00:07:43.508 --> 00:07:49.991
AWS gives you examples of the format to use if you're going to load

112
00:07:49.991 --> 00:07:53.793
certain partitions or everything under a certain bucket.

113
00:07:53.793 --> 00:08:00.522
Let me show you the S3 bucket that I uploaded without showing you.

114
00:08:00.522 --> 00:08:03.432
If you filter by date, if I filter by Date created,

115
00:08:03.432 --> 00:08:08.128
I have nba-tattoos, and if I click on it,

116
00:08:08.128 --> 00:08:11.811
it has the CSV file named nbatattoos.csv,

117
00:08:11.811 --> 00:08:17.396
you have that in your demo folder as an asset.

118
00:08:17.396 --> 00:08:23.001
So the way that this would be represented when we are adding a

119
00:08:23.001 --> 00:08:31.354
table to the database is s3://nba-tattoos/.

120
00:08:31.354 --> 00:08:37.505
And all Athena tables have to be external, and then click Next.

121
00:08:37.505 --> 00:08:42.739
It asks you about the Data Format, the data format is CSV, and click on Next.

122
00:08:42.739 --> 00:08:49.139
So this CSV file only has two columns, the first column is playername,

123
00:08:49.139 --> 00:08:51.591
and it's of type string.

124
00:08:51.591 --> 00:08:58.043
The second column has the name tattoos, and it's also a string.

125
00:08:58.043 --> 00:08:58.768
Click Next.

126
00:08:58.768 --> 00:09:03.664
You could configure partitions here, but I'm just going to say Create the table.

127
00:09:03.664 --> 00:09:06.973
And what this does is it creates a new external table.

128
00:09:06.973 --> 00:09:12.708
If we run the query, it's successful, so let's drop this down,

129
00:09:12.708 --> 00:09:13.929
we have two columns.

130
00:09:13.929 --> 00:09:17.902
So now that the create external table ran,

131
00:09:17.902 --> 00:09:21.073
let's run a select statement on the nba-tattoos table.

132
00:09:21.073 --> 00:09:26.627
So type in the select query that you see here on the screen where

133
00:09:26.627 --> 00:09:29.332
you're selecting everything from the schema.table,

134
00:09:29.332 --> 00:09:35.032
and limit it to 600 because the listings are ordered with

135
00:09:35.032 --> 00:09:38.092
all the no's at the top of the table,

136
00:09:38.092 --> 00:09:41.648
and all the yes's at the bottom of the table.

137
00:09:41.648 --> 00:09:44.819
So let's run this query, select it, and choose Run.

138
00:09:44.819 --> 00:09:47.803
Bring this up so we can see the results,

139
00:09:47.803 --> 00:09:51.582
and we go through all of the no tattoos at the top,

140
00:09:51.582 --> 00:09:55.513
but if we scroll down far enough, we start seeing yes.

141
00:09:55.513 --> 00:10:03.571
And examples of those NBA players that do have tattoos includes Kobe Bryant,

142
00:10:03.571 --> 00:10:08.223
LeBron James, Shaquille O'Neal.

143
00:10:08.223 --> 00:10:11.143
So I'll let you explore this on your own time,

144
00:10:11.143 --> 00:10:17.350
and who knows, you might become one of the millions of Globomantics fans.

145
00:10:17.350 --> 00:10:20.630
And you can say I fixed their technical problems

146
00:10:20.630 --> 00:10:24.037
with AWS Glue and Amazon Athena.

147
00:10:24.037 --> 00:10:25.764
In the next section,

148
00:10:25.764 --> 00:10:39.000
I'll quickly review the join ETL transformation and the data type transformation, and a little bit of detail on Amazon Athena.

