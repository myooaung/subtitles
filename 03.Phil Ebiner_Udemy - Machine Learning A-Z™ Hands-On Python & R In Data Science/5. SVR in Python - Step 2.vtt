WEBVTT
1
00:00:00.270 --> 00:00:01.290
Hello my friends.

2
00:00:01.290 --> 00:00:01.640
All right.

3
00:00:01.710 --> 00:00:02.280
Excellent.

4
00:00:02.280 --> 00:00:07.920
We already did the essentials with therapy pricing by imploring the libraries and importing the data

5
00:00:07.920 --> 00:00:08.550
set.

6
00:00:08.670 --> 00:00:15.200
And now I'd like us to particularly focus on this feature scaling that we're about to do so.

7
00:00:15.420 --> 00:00:21.780
I've already touched you feature scaling in the data processing part you know part 1 where indeed we

8
00:00:21.780 --> 00:00:28.920
implemented together this feature scaling tool at the very end you feature scaling where we learned

9
00:00:28.950 --> 00:00:34.240
how to scale with standardization the training set and the test set separately.

10
00:00:34.310 --> 00:00:35.910
Okay so that's what we did.

11
00:00:35.910 --> 00:00:43.230
Now we are in a slightly different situation in the sense that indeed we do not have a split of the

12
00:00:43.230 --> 00:00:48.360
data set between a training set and a test and that I remind us because we want to leverage the maximum

13
00:00:48.360 --> 00:00:53.350
data in order to learn the correlations between the business levels and the salaries.

14
00:00:53.370 --> 00:01:00.120
So the first thing is we will actually apply exceptionally features killing on the whole matrix of features

15
00:01:00.150 --> 00:01:07.110
X then the second important point and the second difference with what I taught you before and the same

16
00:01:07.310 --> 00:01:14.520
day will be pricing tools implementation is the fact that in this situation we only applied feature

17
00:01:14.520 --> 00:01:17.390
scaling to the features Right.

18
00:01:17.400 --> 00:01:22.830
We only applied feature scaling to extreme an excess which are the features of respectively the training

19
00:01:22.830 --> 00:01:24.310
set and the test.

20
00:01:24.540 --> 00:01:30.100
And we did not apply feature scaling to the dependent variable vector y but why was that.

21
00:01:30.120 --> 00:01:34.400
Why did and we apply here feature scaling to the dependent variable vector Y.

22
00:01:34.440 --> 00:01:40.590
Well that's because remember the data set for our data processing tools implementation was a data set

23
00:01:40.650 --> 00:01:45.050
where the dependent variable was taking values 0 or 1 right.

24
00:01:45.090 --> 00:01:47.250
We can scroll up to have a look again.

25
00:01:47.280 --> 00:01:53.250
Remember these were the poor chaste decisions of this retail company and that's the dependent variable

26
00:01:53.250 --> 00:01:55.730
taking the following values 0 4.

27
00:01:55.770 --> 00:01:58.110
The customer didn't buy the product and won.

28
00:01:58.160 --> 00:02:05.610
The customer bought the product and therefore since it took value 0 or 1 then we did not have to apply

29
00:02:05.700 --> 00:02:12.450
feature scaling here because exactly the same as a dummy variables here the values 0 and 1 are already

30
00:02:12.540 --> 00:02:17.530
in the same range as the one resulting from applying feature scaling onto the features.

31
00:02:17.580 --> 00:02:22.780
All right but now we are in a different situation indeed.

32
00:02:22.920 --> 00:02:29.910
This is our new dataset position salaries where the features are indeed the levels going from one to

33
00:02:29.910 --> 00:02:36.310
10 and the dependent variable taking values from 45000 to 1 million.

34
00:02:36.420 --> 00:02:38.240
And so now I have a question for you.

35
00:02:38.460 --> 00:02:43.520
And after answering this question you will know what to do in any future scaling situation.

36
00:02:43.560 --> 00:02:48.780
So according to you do we have to apply features gearing to this dependent variable the salary.

37
00:02:48.900 --> 00:02:53.970
And the answer is Well yes I am sure you get that right.

38
00:02:53.970 --> 00:02:59.880
Indeed we have to apply feature scaling because same we don't want this feature you know which takes

39
00:02:59.940 --> 00:03:07.230
value is much lower than the values of the dependent variable to be neglected by the SVR model.

40
00:03:07.230 --> 00:03:13.590
And that even if there is not an explicit equation you know like in multiple in our regression this

41
00:03:13.590 --> 00:03:20.580
is an explicit equation because Y is explicitly resulting from a linear combination of features and

42
00:03:20.670 --> 00:03:21.810
for as your model.

43
00:03:21.810 --> 00:03:27.510
This is not the case we have an implicit equation but still through this implicit equation well if the

44
00:03:27.510 --> 00:03:32.660
salary is way higher than the features and here this is absolutely the case well.

45
00:03:32.730 --> 00:03:36.510
Accordingly the future might be neglected by the model.

46
00:03:36.630 --> 00:03:42.930
And actually I of course try to build this as we are moral without applying feature scaling and you

47
00:03:42.930 --> 00:03:45.750
can check that this absolutely does not work.

48
00:03:45.750 --> 00:03:52.170
Indeed if we don't apply feature scaling for or as we are implementation and training on this data set

49
00:03:52.440 --> 00:03:56.000
well you will see that there as your model will not work at all.

50
00:03:56.190 --> 00:04:02.760
So we have to apply features going here on both the feature where the values go from 1 to 10 and the

51
00:04:02.760 --> 00:04:07.470
dependent variable taking values from 45000 to 1 million.

52
00:04:07.650 --> 00:04:08.060
All right.

53
00:04:08.070 --> 00:04:13.230
And so now you know everything about just getting you know what to do in any situation you know that

54
00:04:13.290 --> 00:04:18.990
you don't apply features giving to some dummy variables resulting from one hertz encoding you know that

55
00:04:19.230 --> 00:04:24.120
when a dependent variable takes binary values like 0 and 1 you don't have to apply for just gaining

56
00:04:24.150 --> 00:04:29.130
either because the values are already in the right range you also know that when the dependent variable

57
00:04:29.130 --> 00:04:33.990
takes super high values with respect to the other features then you have to apply feature scaling to

58
00:04:33.990 --> 00:04:37.320
put all the features and it had been available in the same range.

59
00:04:37.650 --> 00:04:42.840
And finally you also know that whenever you want to split your dataset into the training set and the

60
00:04:42.840 --> 00:04:46.770
test set while you have to apply feature scaling after this split.

61
00:04:46.770 --> 00:04:47.400
All right.

62
00:04:47.400 --> 00:04:51.960
So now you know everything about features killing and by the end of this implementation you will also

63
00:04:51.960 --> 00:04:57.300
know something very important to do with features killing but you know on the practical side which will

64
00:04:57.300 --> 00:05:03.000
be the inverse transformation of features killing you know when you scale your features or your dependent

65
00:05:03.000 --> 00:05:07.180
variable at some point you know to get the final prediction and to visualize the result.

66
00:05:07.260 --> 00:05:12.900
You need to inverse that features going inverse that transformation to go back to the original scale

67
00:05:12.960 --> 00:05:18.450
and I will of course teach you how to do it so that we can not only get a very relevant prediction at

68
00:05:18.450 --> 00:05:25.710
the end in this step here and also a super nice visualization where indeed we have the x axis and the

69
00:05:25.710 --> 00:05:28.170
y axis back in their original scale.

70
00:05:28.170 --> 00:05:34.260
You know these scales of the levels going from one to 10 and d scale the salaries going from 45000 to

71
00:05:34.260 --> 00:05:35.100
1 million.

72
00:05:35.180 --> 00:05:36.670
All right so you will know everything.

73
00:05:36.690 --> 00:05:42.540
And now let's implement this new step feature scaling but just before since we're going to apply a lot

74
00:05:42.540 --> 00:05:45.120
of transformations on both X and Y.

75
00:05:45.360 --> 00:05:50.760
Well we're gonna do some print to see the before and after the transformation.

76
00:05:50.760 --> 00:05:51.240
All right.

77
00:05:51.390 --> 00:05:53.550
So let's create two new code cells here.

78
00:05:53.550 --> 00:06:02.130
Let's first do a print of the matrix of features X then a print of the dependent variable vector y.

79
00:06:02.130 --> 00:06:07.370
Now we're going to run the cell to print x and x contains of course in a 2D array.

80
00:06:07.380 --> 00:06:12.660
As you can see with a double pair of square brackets all the position levels going from one to 10.

81
00:06:12.660 --> 00:06:14.210
That was totally expected.

82
00:06:14.210 --> 00:06:21.360
Then let's print Y which will this time contain in a one dimensional vector all the salaries corresponding

83
00:06:21.360 --> 00:06:23.210
to these position levels.

84
00:06:23.220 --> 00:06:24.350
So all good there.

85
00:06:24.420 --> 00:06:30.790
But now we will have to do a first transformation and I'm not even talking about feature scaling.

86
00:06:30.900 --> 00:06:37.280
That first transformation will be to reshape this Y into actually an array.

87
00:06:37.290 --> 00:06:43.350
You know a two dimensional array where you have same the salaries displayed vertically meaning that

88
00:06:43.350 --> 00:06:47.500
instead of having you know a horizontal vector of the salaries here.

89
00:06:47.700 --> 00:06:53.610
Well we want to have the salaries vertically in a two dimensional array and so now the question is why

90
00:06:53.610 --> 00:06:54.360
do we want.

91
00:06:54.390 --> 00:06:56.850
Why do we have such a format you know and to the array.

92
00:06:57.120 --> 00:07:03.690
Well that's because you know the standard scale class that will perform standardization meaning feature

93
00:07:03.700 --> 00:07:04.420
scaling.

94
00:07:04.690 --> 00:07:08.610
Expect one unique format in its input.

95
00:07:08.610 --> 00:07:14.610
You know when you apply the fit transfer method which is a 2D array if you input here a one dimensional

96
00:07:14.610 --> 00:07:17.340
vector like what we have here.

97
00:07:17.340 --> 00:07:24.720
This will return an error simply because well this standard scalar class expects it to the array as

98
00:07:24.810 --> 00:07:25.820
its input.

99
00:07:25.830 --> 00:07:32.400
So now we just have to transform this into a 2D array and you actually know exactly how to do that because

100
00:07:32.400 --> 00:07:33.340
we already did it.

101
00:07:33.630 --> 00:07:34.450
I'll give you a hint.

102
00:07:34.470 --> 00:07:37.280
This was in the multiple in our regression section.

103
00:07:37.470 --> 00:07:38.100
And so now.

104
00:07:38.100 --> 00:07:43.350
Well of course I would like you to press pause on this video and try to transform y.

105
00:07:43.380 --> 00:07:50.490
Or you know reshape way into this 2D array with the salaries displayed vertically.

106
00:07:50.490 --> 00:07:50.950
All right.

107
00:07:51.270 --> 00:07:55.770
So please press pause and now I'm going to give you the solution.

108
00:07:55.810 --> 00:07:56.100
All right.

109
00:07:56.120 --> 00:07:59.890
So we're going to create a new code cell to do this.

110
00:08:00.090 --> 00:08:06.930
And so well for us we want to update y and therefore you know we will start with this Y equals and then

111
00:08:07.050 --> 00:08:11.060
we'll do the necessary transformation to return this new way.

112
00:08:11.070 --> 00:08:12.330
And so how do we do this.

113
00:08:12.750 --> 00:08:20.160
So the first thing to do is to take Y again from which we're gonna call that reach shape function into

114
00:08:20.160 --> 00:08:21.980
which we're going to input.

115
00:08:22.110 --> 00:08:27.060
Well you know the new shape that we would like Y to have and remember how we have to enter this new

116
00:08:27.060 --> 00:08:27.590
shape.

117
00:08:27.600 --> 00:08:29.580
Well we have to input here two elements.

118
00:08:29.610 --> 00:08:34.840
The first one being the number of rows of this new y you know this new format of y want to have.

119
00:08:34.950 --> 00:08:37.030
And then the number of columns.

120
00:08:37.080 --> 00:08:43.360
So that's easy since we want to have the salaries to split vertically you know in different rows actually.

121
00:08:43.380 --> 00:08:50.100
Well what we want to have for the number of rows is of course Len of y you know the length of Y meaning

122
00:08:50.100 --> 00:08:53.730
the number of elements in y meaning the number of salaries.

123
00:08:53.950 --> 00:08:54.390
Okay.

124
00:08:54.660 --> 00:08:59.670
So that's the number of rows and then the second element here is the number of columns.

125
00:08:59.670 --> 00:09:05.040
And of course we want one column because we want to display the salaries vertically in a 2D array of

126
00:09:05.040 --> 00:09:05.600
course.

127
00:09:05.640 --> 00:09:09.140
And therefore here we input 1 as in 1 column.

128
00:09:09.150 --> 00:09:14.520
So we're going to have actually since we have 10 salaries we're going to have 10 rows and one column.

129
00:09:15.110 --> 00:09:15.650
Okay.

130
00:09:15.780 --> 00:09:17.000
So it was good to do it again.

131
00:09:17.010 --> 00:09:21.170
Now you become more familiar with this reshape trick.

132
00:09:21.660 --> 00:09:21.940
Okay.

133
00:09:21.970 --> 00:09:26.850
Now of course we're going to print this to see and to check that everything's all right and mostly that

134
00:09:26.850 --> 00:09:33.180
we have the right format expected by this standard scale class which we'll use then to apply feature

135
00:09:33.180 --> 00:09:33.700
scaling.

136
00:09:33.930 --> 00:09:40.670
So there we go print y and let us first execute this to reshape y.

137
00:09:40.770 --> 00:09:46.410
And now let's print y to and check two things that first we have a 2D array we can clearly see that

138
00:09:46.410 --> 00:09:53.520
with the double pair of square brackets and also the salaries displayed vertically just like this matrix

139
00:09:53.670 --> 00:09:56.410
of features which is also add to the array of course.

140
00:09:57.050 --> 00:09:57.390
All right.

141
00:09:57.390 --> 00:10:03.620
So now everything's perfect we're ready play feature scaling and so we're going to do that right away

142
00:10:04.070 --> 00:10:08.300
starting by creating a new code cell here.

143
00:10:08.410 --> 00:10:09.010
All right.

144
00:10:09.110 --> 00:10:11.950
So now how are we going to do this efficiently.

145
00:10:11.960 --> 00:10:14.250
We always want to be efficient when we code.

146
00:10:14.300 --> 00:10:18.700
So of course we're going to grab our tool in our data pricing toolkit.

147
00:10:18.740 --> 00:10:23.480
I'm talking of course about a feature scaling tool and we'll have to adapt this a bit because this was

148
00:10:23.660 --> 00:10:30.010
applied on the training and test sets but no worries we will adapt it very quickly and efficiently.

149
00:10:30.020 --> 00:10:38.540
So I'm copying this and pasting that right here and now of course I would like you to please press pause

150
00:10:38.540 --> 00:10:45.830
again and try to figure out on your own what we have to modify here to make this feature scaling work

151
00:10:45.950 --> 00:10:47.580
for situation.

152
00:10:47.600 --> 00:10:48.210
All right.

153
00:10:48.320 --> 00:10:52.710
So pause and now let me give you the solution.

154
00:10:52.730 --> 00:10:53.060
All right.

155
00:10:53.090 --> 00:10:58.550
So first we don't have a training set of tests here so I'm going to remove this line of code.

156
00:10:58.620 --> 00:11:03.860
I'm going to you know remove that as well all the index elections.

157
00:11:03.860 --> 00:11:05.440
And here as well.

158
00:11:05.510 --> 00:11:08.930
And I'm just keeping X and then let's see what we have to do.

159
00:11:08.930 --> 00:11:09.110
All right.

160
00:11:09.110 --> 00:11:14.190
So first that already more clean now with what we have here.

161
00:11:14.210 --> 00:11:19.940
You know these three lines of code we are indeed applying feature scaling to the matrix of features

162
00:11:19.970 --> 00:11:22.620
X which is one of the things we have to do indeed.

163
00:11:22.640 --> 00:11:23.150
That's good.

164
00:11:23.420 --> 00:11:30.410
However as we've just explained we also have to scale the dependent variable vector the salaries and

165
00:11:30.410 --> 00:11:32.480
of course and that's the important thing.

166
00:11:32.480 --> 00:11:39.020
You have to understand and figure out we're not going to use the same standard scalar object on both

167
00:11:39.020 --> 00:11:42.510
the matrix of features x and the dependent variable vector Y.

168
00:11:42.530 --> 00:11:43.340
Why is that.

169
00:11:43.340 --> 00:11:48.920
That's because you know when you fit your object a C on your data.

170
00:11:49.130 --> 00:11:53.620
Well it is going to compute the mean and the standard deviation of that same variable.

171
00:11:53.720 --> 00:11:58.580
And therefore since of course we don't have the same mean and same standard deviation for our levels

172
00:11:58.580 --> 00:11:59.910
here and our salaries.

173
00:11:59.930 --> 00:12:06.650
Well obviously we have to create two standard scalar object one that will be fitted to X in order to

174
00:12:06.650 --> 00:12:09.420
compute the mean and standard deviation of the position levels.

175
00:12:09.470 --> 00:12:15.470
And one that will be fitted to Y to indeed compute the mean and the standard deviation of the salaries.

176
00:12:15.470 --> 00:12:20.690
All right so that was the only important thing to understand and therefore here I'm actually going to

177
00:12:20.690 --> 00:12:28.010
call this as the object is C X in order to say that it's the scalar of the matrix of features x and

178
00:12:28.340 --> 00:12:30.770
here as C X as well.

179
00:12:30.770 --> 00:12:37.270
And now I'm going to create a new standard scalar object the one that will be used on our dependent

180
00:12:37.270 --> 00:12:45.890
variable vector y and therefore here I am replacing XY X by we're gonna call it a C Y so that it's perfectly

181
00:12:45.890 --> 00:12:49.380
clear that this is a scalar of X and this is a scalar of Y.

182
00:12:49.490 --> 00:12:58.250
And now we're going to copy this base that right below and here we'll just have three little replacements

183
00:12:58.250 --> 00:13:07.780
to do which are first replacing X here by Y then SC X here by SC y and x here by Y.

184
00:13:07.880 --> 00:13:14.690
So that now we're ready to scale both R matrix of features x and R deep and viable vector Y.

185
00:13:14.900 --> 00:13:15.850
Let's check it out.

186
00:13:15.860 --> 00:13:23.690
Let's run this cell and now we're going to add two new code cells to print the new X and the new Y and

187
00:13:23.690 --> 00:13:25.430
see what they have become.

188
00:13:25.430 --> 00:13:35.630
All right so two new code cells and let's start here with a print of X good and then a print of Y.

189
00:13:36.320 --> 00:13:41.400
And now let's run these two cells here starting with print x ended.

190
00:13:41.420 --> 00:13:47.720
Well we have some perfectly scaled values of the position level's going from minus one point five and

191
00:13:47.720 --> 00:13:53.150
that corresponds of course to position level number one and one point fifty six which corresponds to

192
00:13:53.150 --> 00:13:54.920
position level number 10.

193
00:13:54.920 --> 00:13:55.250
All right.

194
00:13:55.250 --> 00:14:00.060
Now let's print why and this will be interesting here.

195
00:14:00.110 --> 00:14:05.570
The values will go from minus 0 point 7 which corresponds of course to the salary of forty five thousand

196
00:14:05.580 --> 00:14:06.100
per year.

197
00:14:06.290 --> 00:14:10.180
And two point sixty four which corresponds to the one million dollar salary.

198
00:14:10.250 --> 00:14:15.440
And as you can see here the standard values are in the range from minus one to plus three.

199
00:14:15.440 --> 00:14:21.680
That's why in the data pricing part I told you that usually standardization transform your values between

200
00:14:21.770 --> 00:14:23.720
minus three and plus three.

201
00:14:23.750 --> 00:14:24.350
All right.

202
00:14:24.410 --> 00:14:25.550
So there you go.

203
00:14:25.550 --> 00:14:26.480
Congratulations.

204
00:14:26.480 --> 00:14:32.360
Now you successfully applied feature scaling for the as we are moral and especially for this data set

205
00:14:32.630 --> 00:14:36.180
where it clearly features getting had to be applied on the dependent variable.

206
00:14:36.230 --> 00:14:41.320
And now I'm really glad for you because you now know how to handle any situation of future scaling.

207
00:14:41.330 --> 00:14:46.430
You just need to know how to inverse the transformation but that will come very quickly because then

208
00:14:46.700 --> 00:14:49.790
we will actually trained as your model on the whole data set.

209
00:14:49.790 --> 00:14:55.970
And then when we get our smart model after the training well we will predict that salary of the position

210
00:14:55.970 --> 00:14:57.260
level number is six point five.

211
00:14:57.260 --> 00:15:00.910
And that's where you will learn how to inverse transformation.

212
00:15:00.940 --> 00:15:01.990
All right so I can't wait.

213
00:15:02.070 --> 00:15:03.620
Let us build and train for it.

214
00:15:03.690 --> 00:15:05.490
As VR moral on the whole data set.

215
00:15:05.770 --> 00:15:07.010
And then we'll get together.

216
00:15:07.060 --> 00:15:08.260
The final results.

217
00:15:08.260 --> 00:15:10.000
And until then enjoy machine learning.
